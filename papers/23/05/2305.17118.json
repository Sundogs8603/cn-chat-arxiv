{
    "title": "Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time. (arXiv:2305.17118v2 [cs.LG] UPDATED)",
    "abstract": "Large language models(LLMs) have sparked a new wave of exciting AI applications. Hosting these models at scale requires significant memory resources. One crucial memory bottleneck for the deployment stems from the context window. It is commonly recognized that model weights are memory hungry; however, the size of key-value embedding stored during the generation process (KV cache) can easily surpass the model size. The enormous size of the KV cache puts constraints on the inference batch size, which is crucial for high throughput inference workload. Inspired by an interesting observation of the attention scores, we hypothesize the persistence of importance: only pivotal tokens, which had a substantial influence at one step, will significantly influence future generations. Based on our empirical verification and theoretical analysis around this hypothesis, we propose Scissorhands, a system that maintains the memory usage of the KV cache at a fixed budget without finetuning the model. In ",
    "link": "http://arxiv.org/abs/2305.17118",
    "context": "Title: Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time. (arXiv:2305.17118v2 [cs.LG] UPDATED)\nAbstract: Large language models(LLMs) have sparked a new wave of exciting AI applications. Hosting these models at scale requires significant memory resources. One crucial memory bottleneck for the deployment stems from the context window. It is commonly recognized that model weights are memory hungry; however, the size of key-value embedding stored during the generation process (KV cache) can easily surpass the model size. The enormous size of the KV cache puts constraints on the inference batch size, which is crucial for high throughput inference workload. Inspired by an interesting observation of the attention scores, we hypothesize the persistence of importance: only pivotal tokens, which had a substantial influence at one step, will significantly influence future generations. Based on our empirical verification and theoretical analysis around this hypothesis, we propose Scissorhands, a system that maintains the memory usage of the KV cache at a fixed budget without finetuning the model. In ",
    "path": "papers/23/05/2305.17118.json",
    "total_tokens": 900,
    "translated_title": "剪刀手：利用重要性持久性假设在测试时对LLM KV缓存进行压缩",
    "translated_abstract": "大型语言模型（LLMs）引发了一波新的令人兴奋的人工智能应用。大规模托管这些模型需要大量的内存资源。部署过程中一个关键的内存瓶颈来自于上下文窗口。众所周知，模型权重占用大量内存；然而，在生成过程中存储的键值嵌入大小（KV缓存）往往超过了模型的大小。巨大的KV缓存大小对于关键字批处理大小的推理产生约束，这对于高吞吐量的推理工作负载至关重要。受到注意力分数的有趣观察的启发，我们提出了持久性重要性的假设：只有具有重要影响的关键标记，在一步中有实质性影响，才会在未来的生成中产生重大影响。基于对这一假设的经验验证和理论分析，我们提出了剪刀手，一个可以在不微调模型的情况下将KV缓存的内存使用维持在固定预算内的系统。",
    "tldr": "Scissorhands是一个可以在不对模型进行微调的情况下，通过利用重要性持久性假设将LLM KV缓存的内存使用维持在固定预算内的系统。"
}