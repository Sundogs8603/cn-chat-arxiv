{
    "title": "Inverse Reinforcement Learning with the Average Reward Criterion. (arXiv:2305.14608v1 [cs.LG])",
    "abstract": "We study the problem of Inverse Reinforcement Learning (IRL) with an average-reward criterion. The goal is to recover an unknown policy and a reward function when the agent only has samples of states and actions from an experienced agent. Previous IRL methods assume that the expert is trained in a discounted environment, and the discount factor is known. This work alleviates this assumption by proposing an average-reward framework with efficient learning algorithms. We develop novel stochastic first-order methods to solve the IRL problem under the average-reward setting, which requires solving an Average-reward Markov Decision Process (AMDP) as a subproblem. To solve the subproblem, we develop a Stochastic Policy Mirror Descent (SPMD) method under general state and action spaces that needs $\\mathcal{{O}}(1/\\varepsilon)$ steps of gradient computation. Equipped with SPMD, we propose the Inverse Policy Mirror Descent (IPMD) method for solving the IRL problem with a $\\mathcal{O}(1/\\varepsi",
    "link": "http://arxiv.org/abs/2305.14608",
    "context": "Title: Inverse Reinforcement Learning with the Average Reward Criterion. (arXiv:2305.14608v1 [cs.LG])\nAbstract: We study the problem of Inverse Reinforcement Learning (IRL) with an average-reward criterion. The goal is to recover an unknown policy and a reward function when the agent only has samples of states and actions from an experienced agent. Previous IRL methods assume that the expert is trained in a discounted environment, and the discount factor is known. This work alleviates this assumption by proposing an average-reward framework with efficient learning algorithms. We develop novel stochastic first-order methods to solve the IRL problem under the average-reward setting, which requires solving an Average-reward Markov Decision Process (AMDP) as a subproblem. To solve the subproblem, we develop a Stochastic Policy Mirror Descent (SPMD) method under general state and action spaces that needs $\\mathcal{{O}}(1/\\varepsilon)$ steps of gradient computation. Equipped with SPMD, we propose the Inverse Policy Mirror Descent (IPMD) method for solving the IRL problem with a $\\mathcal{O}(1/\\varepsi",
    "path": "papers/23/05/2305.14608.json",
    "total_tokens": 985,
    "translated_abstract": "本文研究了平均奖励标准下的逆强化学习（IRL）问题。研究的目标是当一个智能体只有来自于一个经验智能体的状态和行为样本时，恢复一个未知的策略和奖励函数。先前的IRL方法假设专家是在打折的环境中训练的，并且折扣因子是已知的。而本文提出了一个采用平均奖励框架和高效学习算法的方法来缓解这个假设。我们提出了新颖的随机一阶方法来解决平均奖励设定下的IRL问题，这需要将一个平均奖励马尔可夫决策过程（AMDP）作为子问题来解决。我们开发了一种随机政策镜像下降（SPMD）方法来解决这个子问题，该方法需要 $\\mathcal{{O}}(1/\\varepsilon)$ 步梯度计算。利用SPMD，我们提出了逆政策镜像下降（IPMD）方法来解决具有 $\\mathcal{O}(1/\\varepsilon)$ 效率的IRL问题。",
    "tldr": "本文研究了平均奖励下的逆强化学习问题，提出了随机政策镜像下降方法并解决了该问题，避免了原方法对折扣因子已知的假设。",
    "en_tdlr": "This paper proposes inverse reinforcement learning (IRL) with average-reward criterion, develops a novel stochastic first-order method called Stochastic Policy Mirror Descent (SPMD) to solve the IRL problem, and solves the Average-reward Markov Decision Process (AMDP) as a subproblem without the assumption of a known discount factor."
}