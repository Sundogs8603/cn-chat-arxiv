{
    "title": "SELFOOD: Self-Supervised Out-Of-Distribution Detection via Learning to Rank. (arXiv:2305.14696v1 [cs.CL])",
    "abstract": "Deep neural classifiers trained with cross-entropy loss (CE loss) often suffer from poor calibration, necessitating the task of out-of-distribution (OOD) detection. Traditional supervised OOD detection methods require expensive manual annotation of in-distribution and OOD samples. To address the annotation bottleneck, we introduce SELFOOD, a self-supervised OOD detection method that requires only in-distribution samples as supervision. We cast OOD detection as an inter-document intra-label (IDIL) ranking problem and train the classifier with our pairwise ranking loss, referred to as IDIL loss. Specifically, given a set of in-distribution documents and their labels, for each label, we train the classifier to rank the softmax scores of documents belonging to that label to be higher than the scores of documents that belong to other labels. Unlike CE loss, our IDIL loss function reaches zero when the desired confidence ranking is achieved and gradients are backpropagated to decrease probab",
    "link": "http://arxiv.org/abs/2305.14696",
    "context": "Title: SELFOOD: Self-Supervised Out-Of-Distribution Detection via Learning to Rank. (arXiv:2305.14696v1 [cs.CL])\nAbstract: Deep neural classifiers trained with cross-entropy loss (CE loss) often suffer from poor calibration, necessitating the task of out-of-distribution (OOD) detection. Traditional supervised OOD detection methods require expensive manual annotation of in-distribution and OOD samples. To address the annotation bottleneck, we introduce SELFOOD, a self-supervised OOD detection method that requires only in-distribution samples as supervision. We cast OOD detection as an inter-document intra-label (IDIL) ranking problem and train the classifier with our pairwise ranking loss, referred to as IDIL loss. Specifically, given a set of in-distribution documents and their labels, for each label, we train the classifier to rank the softmax scores of documents belonging to that label to be higher than the scores of documents that belong to other labels. Unlike CE loss, our IDIL loss function reaches zero when the desired confidence ranking is achieved and gradients are backpropagated to decrease probab",
    "path": "papers/23/05/2305.14696.json",
    "total_tokens": 904,
    "translated_title": "SELFOOD: 基于自学习排序的无监督外部样本检测",
    "translated_abstract": "采用交叉熵损失训练的深度神经分类器常常存在校准不良的问题，需要进行外部样本检测。传统的有监督外部样本检测方法需要昂贵的手动注释内部和外部样本。为了解决注释瓶颈问题，我们介绍了SELFOOD，这是一种仅需要内部样本作为监督的自监督外部样本检测方法。我们将外部样本检测看作一个文档之间标签内部排序问题，并使用比较排序损失（IDIL loss）训练分类器。具体来说，对于内部样本文档及其标签，我们训练分类器将属于该标签的文档的softmax分数排名排在其他标签文档的分数之上。与交叉熵损失不同，当达到期望的置信度排名时，我们的IDIL损失函数将达到零并进行反向传播来降低概率。",
    "tldr": "提出了一种自监督的外部样本检测方法SELFOOD，通过将外部样本检测视为文档标签内部排序问题，使用比较排序损失对分类器进行训练，实现无监督的外部样本检测。",
    "en_tdlr": "SELFOOD proposes a self-supervised out-of-distribution detection method that frames the task as an inter-document intra-label ranking problem and trains the classifier using pairwise ranking loss. The method achieves unsupervised OOD detection by using only in-distribution samples as supervision."
}