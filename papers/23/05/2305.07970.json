{
    "title": "Investigating Emergent Goal-Like Behaviour in Large Language Models Using Experimental Economics. (arXiv:2305.07970v1 [cs.GT])",
    "abstract": "In this study, we investigate the capacity of large language models (LLMs), specifically GPT-3.5, to operationalise natural language descriptions of cooperative, competitive, altruistic, and self-interested behavior in social dilemmas. Our focus is on the iterated Prisoner's Dilemma, a classic example of a non-zero-sum interaction, but our broader research program encompasses a range of experimental economics scenarios, including the ultimatum game, dictator game, and public goods game. Using a within-subject experimental design, we instantiated LLM-generated agents with various prompts that conveyed different cooperative and competitive stances. We then assessed the agents' level of cooperation in the iterated Prisoner's Dilemma, taking into account their responsiveness to the cooperative or defection actions of their partners. Our results provide evidence that LLMs can translate natural language descriptions of altruism and selfishness into appropriate behaviour to some extent, but e",
    "link": "http://arxiv.org/abs/2305.07970",
    "context": "Title: Investigating Emergent Goal-Like Behaviour in Large Language Models Using Experimental Economics. (arXiv:2305.07970v1 [cs.GT])\nAbstract: In this study, we investigate the capacity of large language models (LLMs), specifically GPT-3.5, to operationalise natural language descriptions of cooperative, competitive, altruistic, and self-interested behavior in social dilemmas. Our focus is on the iterated Prisoner's Dilemma, a classic example of a non-zero-sum interaction, but our broader research program encompasses a range of experimental economics scenarios, including the ultimatum game, dictator game, and public goods game. Using a within-subject experimental design, we instantiated LLM-generated agents with various prompts that conveyed different cooperative and competitive stances. We then assessed the agents' level of cooperation in the iterated Prisoner's Dilemma, taking into account their responsiveness to the cooperative or defection actions of their partners. Our results provide evidence that LLMs can translate natural language descriptions of altruism and selfishness into appropriate behaviour to some extent, but e",
    "path": "papers/23/05/2305.07970.json",
    "total_tokens": 1087,
    "translated_title": "利用实验经济学研究大型语言模型中出现的类似目标行为",
    "translated_abstract": "本研究探讨了大型语言模型（LLMs），特别是GPT-3.5，实现合作、竞争、利他和自私行为的自然语言描述在社会困境下的能力。我们聚焦于迭代囚徒困境，这是一个非零和互动的经典例子，但我们的更广泛研究计划包括一系列实验经济学场景，包括最后通牒博弈、独裁者博弈和公共物品游戏。使用被试内实验设计，我们运用不同的提示信息实例化由LLM生成的智能体，表达不同的合作和竞争立场。我们评估了智能体在迭代囚徒困境中的合作水平，同时考虑到它们对合作或出尔反尔的伙伴行动的响应。我们的结果表明，LLMs在某种程度上可以将利他和自私的自然语言描述转化为适当的行为，但展示出区分合作和竞争水平的能力有限。总体而言，我们的研究为在人类决策制定的背景下使用LLMs的伦理意义和局限性提供了证据。",
    "tldr": "本研究探讨了大型语言模型的能力，发现其可以将自然语言描述转化为适当的行为，但在区分细微的合作和竞争水平方面的能力受到限制，为使用LLMs在人类决策制定背景下的伦理意义和局限性做出了贡献。",
    "en_tdlr": "This study investigates the capacity of large language models to operationalize natural language descriptions of cooperative, competitive, altruistic, and self-interested behavior in social dilemmas and found that while they can translate natural language descriptions to appropriate behavior, their ability to differentiate between subtle differences in levels of cooperation and competition is limited, contributing to the ethical implications and limitations of using LLMs in human decision-making contexts."
}