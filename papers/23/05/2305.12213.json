{
    "title": "Taming Resource Heterogeneity In Distributed ML Training With Dynamic Batching. (arXiv:2305.12213v1 [cs.LG])",
    "abstract": "Current techniques and systems for distributed model training mostly assume that clusters are comprised of homogeneous servers with a constant resource availability. However, cluster heterogeneity is pervasive in computing infrastructure, and is a fundamental characteristic of low-cost transient resources (such as EC2 spot instances). In this paper, we develop a dynamic batching technique for distributed data-parallel training that adjusts the mini-batch sizes on each worker based on its resource availability and throughput. Our mini-batch controller seeks to equalize iteration times on all workers, and facilitates training on clusters comprised of servers with different amounts of CPU and GPU resources. This variable mini-batch technique uses proportional control and ideas from PID controllers to find stable mini-batch sizes. Our empirical evaluation shows that dynamic batching can reduce model training times by more than 4x on heterogeneous clusters.",
    "link": "http://arxiv.org/abs/2305.12213",
    "context": "Title: Taming Resource Heterogeneity In Distributed ML Training With Dynamic Batching. (arXiv:2305.12213v1 [cs.LG])\nAbstract: Current techniques and systems for distributed model training mostly assume that clusters are comprised of homogeneous servers with a constant resource availability. However, cluster heterogeneity is pervasive in computing infrastructure, and is a fundamental characteristic of low-cost transient resources (such as EC2 spot instances). In this paper, we develop a dynamic batching technique for distributed data-parallel training that adjusts the mini-batch sizes on each worker based on its resource availability and throughput. Our mini-batch controller seeks to equalize iteration times on all workers, and facilitates training on clusters comprised of servers with different amounts of CPU and GPU resources. This variable mini-batch technique uses proportional control and ideas from PID controllers to find stable mini-batch sizes. Our empirical evaluation shows that dynamic batching can reduce model training times by more than 4x on heterogeneous clusters.",
    "path": "papers/23/05/2305.12213.json",
    "total_tokens": 894,
    "translated_title": "用动态分批技术平衡分布式机器学习培训中的资源异质性",
    "translated_abstract": "目前分布式模型训练的技术和系统大多都假设集群由具有恒定资源可用性的同构服务器组成，然而集群异质性在计算基础设施中是普遍存在的，并且是低廉的瞬态资源（例如 EC2 spot 实例）的基本特性。在本文中，我们开发了一种动态分批技术，用于分布式数据并行训练，根据每个工作节点的资源可用性和吞吐量调整小批量大小。我们的小批量控制器旨在平衡所有工作节点的迭代时间，并促进使用具有不同数量 CPU 和 GPU 资源的服务器组成的集群进行训练。该变量小批量技术使用比例控制和 PID 控制器的思想来找到稳定的小批量大小。我们的实证评估表明，动态分批可以将异构集群上的模型训练时间缩短超过 4 倍。",
    "tldr": "本文提出一种动态分批技术，用于分布式数据并行训练，可以根据每个工作节点的资源可用性和吞吐量调整小批量大小，从而平衡所有工作节点的迭代时间，减少分布式机器学习模型训练时间。",
    "en_tdlr": "This paper proposes a dynamic batching technique for distributed data-parallel training that adjusts the mini-batch sizes on each worker based on its resource availability and throughput, which reduces the model training times by more than 4x on heterogeneous clusters."
}