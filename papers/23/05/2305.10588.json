{
    "title": "A Better Way to Do Masked Language Model Scoring. (arXiv:2305.10588v1 [cs.CL])",
    "abstract": "Estimating the log-likelihood of a given sentence under an autoregressive language model is straightforward: one can simply apply the chain rule and sum the log-likelihood values for each successive token. However, for masked language models, there is no direct way to estimate the log-likelihood of a sentence. To address this issue, Salazar et al. (2020) propose to estimate sentence pseudo-log-likelihood (PLL) scores, computed by successively masking each sentence token, retrieving its score using the rest of the sentence as context, and summing the resulting values. Here, we demonstrate that the original PLL method yields inflated scores for out-of-vocabulary words and propose an adapted metric, in which we mask not only the target token, but also all within-word tokens to the right of the target. We show that our adapted metric (PLL-word-l2r) outperforms both the original PLL metric and a PLL metric in which all within-word tokens are masked. In particular, it better satisfies theore",
    "link": "http://arxiv.org/abs/2305.10588",
    "context": "Title: A Better Way to Do Masked Language Model Scoring. (arXiv:2305.10588v1 [cs.CL])\nAbstract: Estimating the log-likelihood of a given sentence under an autoregressive language model is straightforward: one can simply apply the chain rule and sum the log-likelihood values for each successive token. However, for masked language models, there is no direct way to estimate the log-likelihood of a sentence. To address this issue, Salazar et al. (2020) propose to estimate sentence pseudo-log-likelihood (PLL) scores, computed by successively masking each sentence token, retrieving its score using the rest of the sentence as context, and summing the resulting values. Here, we demonstrate that the original PLL method yields inflated scores for out-of-vocabulary words and propose an adapted metric, in which we mask not only the target token, but also all within-word tokens to the right of the target. We show that our adapted metric (PLL-word-l2r) outperforms both the original PLL metric and a PLL metric in which all within-word tokens are masked. In particular, it better satisfies theore",
    "path": "papers/23/05/2305.10588.json",
    "total_tokens": 946,
    "translated_title": "一种更好的掩码语言模型评分方法",
    "translated_abstract": "估计自回归语言模型下给定句子的对数似然很简单：可以直接应用链式法则并对每个连续标记的对数似然值求和。但对于掩码语言模型，没有直接的方法来估计一个句子的对数似然。为了解决这个问题，Salazar等人（2020）提出了估计句子伪对数似然（PLL）分数的方法，该方法通过依次屏蔽每个句子标记，使用其余的句子作为上下文检索其得分，并总结结果值。本文针对原PLL方法中字汇外单词产生的得分夸大的问题提出了一种改进的度量方法，其中我们不仅屏蔽目标标记，而且还屏蔽目标标记右侧所有的标记。我们展示了我们改进的度量方法（PLL-word-l2r）优于原始PLL度量方法和一个屏蔽所有单词标记的PLL合成方式。特别地，它更好地符合理论。",
    "tldr": "本文提出了一种更好的掩码语言模型评分方法，即PLL-word-l2r，用于估计句子的伪对数似然得分，相对于原PLL方法和屏蔽所有单词标记的PLL评分方法，改进的度量方法更好地针对字汇外单词得分问题进行了解决。",
    "en_tdlr": "This paper proposes a better scoring method, PLL-word-l2r, for masked language models to estimate pseudo-log-likelihood scores of sentences. It outperforms the original PLL method and a method masking all within-word tokens, particularly by better addressing the inflated scores of out-of-vocabulary words."
}