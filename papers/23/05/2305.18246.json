{
    "title": "Provable and Practical: Efficient Exploration in Reinforcement Learning via Langevin Monte Carlo",
    "abstract": "arXiv:2305.18246v2 Announce Type: replace  Abstract: We present a scalable and effective exploration strategy based on Thompson sampling for reinforcement learning (RL). One of the key shortcomings of existing Thompson sampling algorithms is the need to perform a Gaussian approximation of the posterior distribution, which is not a good surrogate in most practical settings. We instead directly sample the Q function from its posterior distribution, by using Langevin Monte Carlo, an efficient type of Markov Chain Monte Carlo (MCMC) method. Our method only needs to perform noisy gradient descent updates to learn the exact posterior distribution of the Q function, which makes our approach easy to deploy in deep RL. We provide a rigorous theoretical analysis for the proposed method and demonstrate that, in the linear Markov decision process (linear MDP) setting, it has a regret bound of $\\tilde{O}(d^{3/2}H^{3/2}\\sqrt{T})$, where $d$ is the dimension of the feature mapping, $H$ is the plannin",
    "link": "https://arxiv.org/abs/2305.18246",
    "context": "Title: Provable and Practical: Efficient Exploration in Reinforcement Learning via Langevin Monte Carlo\nAbstract: arXiv:2305.18246v2 Announce Type: replace  Abstract: We present a scalable and effective exploration strategy based on Thompson sampling for reinforcement learning (RL). One of the key shortcomings of existing Thompson sampling algorithms is the need to perform a Gaussian approximation of the posterior distribution, which is not a good surrogate in most practical settings. We instead directly sample the Q function from its posterior distribution, by using Langevin Monte Carlo, an efficient type of Markov Chain Monte Carlo (MCMC) method. Our method only needs to perform noisy gradient descent updates to learn the exact posterior distribution of the Q function, which makes our approach easy to deploy in deep RL. We provide a rigorous theoretical analysis for the proposed method and demonstrate that, in the linear Markov decision process (linear MDP) setting, it has a regret bound of $\\tilde{O}(d^{3/2}H^{3/2}\\sqrt{T})$, where $d$ is the dimension of the feature mapping, $H$ is the plannin",
    "path": "papers/23/05/2305.18246.json",
    "total_tokens": 873,
    "translated_title": "可证明且实用：通过Langevin Monte Carlo在强化学习中的有效探索",
    "translated_abstract": "我们提出了一种基于Thompson采样的可伸缩且有效的强化学习探索策略。现有Thompson采样算法的一个关键缺点是需要对后验分布进行高斯逼近，在大多数实际场景中并不是一个很好的替代方法。我们使用Langevin Monte Carlo直接从后验分布中采样Q函数，这是一种高效的马尔可夫链蒙特卡洛(MCMC)方法。我们的方法只需执行带噪梯度下降更新就可以学习Q函数的精确后验分布，这使得我们的方法在深度强化学习中易于部署。我们为所提出的方法进行了严格的理论分析，并证明在线性马尔可夫决策过程(linear MDP)设置中，它的遗憾界约为$\\tilde{O}(d^{3/2}H^{3/2}\\sqrt{T})$，其中 $d$ 是特征映射的维度，$H$ 是计划的长度。",
    "tldr": "通过Langevin Monte Carlo直接采样Q函数后验分布，避免了高斯逼近，实现了在强化学习中高效的探索策略。",
    "en_tdlr": "Efficient exploration strategy in reinforcement learning achieved by directly sampling Q function from its posterior distribution using Langevin Monte Carlo, avoiding Gaussian approximation."
}