{
    "title": "KGA: A General Machine Unlearning Framework Based on Knowledge Gap Alignment. (arXiv:2305.06535v1 [cs.CL])",
    "abstract": "Recent legislation of the \"right to be forgotten\" has led to the interest in machine unlearning, where the learned models are endowed with the function to forget information about specific training instances as if they have never existed in the training set. Previous work mainly focuses on computer vision scenarios and largely ignores the essentials of unlearning in NLP field, where text data contains more explicit and sensitive personal information than images. In this paper, we propose a general unlearning framework called KGA to induce forgetfulness. Different from previous work that tries to recover gradients or forces models to perform close to one specific distribution, KGA maintains distribution differences (i.e., knowledge gap). This relaxes the distribution assumption. Furthermore, we first apply the unlearning method to various NLP tasks (i.e., classification, translation, response generation) and propose several unlearning evaluation metrics with pertinence. Experiments on l",
    "link": "http://arxiv.org/abs/2305.06535",
    "context": "Title: KGA: A General Machine Unlearning Framework Based on Knowledge Gap Alignment. (arXiv:2305.06535v1 [cs.CL])\nAbstract: Recent legislation of the \"right to be forgotten\" has led to the interest in machine unlearning, where the learned models are endowed with the function to forget information about specific training instances as if they have never existed in the training set. Previous work mainly focuses on computer vision scenarios and largely ignores the essentials of unlearning in NLP field, where text data contains more explicit and sensitive personal information than images. In this paper, we propose a general unlearning framework called KGA to induce forgetfulness. Different from previous work that tries to recover gradients or forces models to perform close to one specific distribution, KGA maintains distribution differences (i.e., knowledge gap). This relaxes the distribution assumption. Furthermore, we first apply the unlearning method to various NLP tasks (i.e., classification, translation, response generation) and propose several unlearning evaluation metrics with pertinence. Experiments on l",
    "path": "papers/23/05/2305.06535.json",
    "total_tokens": 949,
    "translated_title": "KGA: 基于知识差异对齐的通用机器遗忘框架",
    "translated_abstract": "近期《被遗忘权》立法引起了机器遗忘的研究兴趣，其中学习模型具有忘记有关特定训练实例信息的功能，就像它们从未存在于训练集中一样。以往的工作主要关注于计算机视觉场景，并且在NLP领域中忽略了遗忘的要素，文本数据比图像包含更多明确且敏感的个人信息。本文提出了一个通用的遗忘框架KGA来诱发遗忘。与以往试图恢复梯度或强制模型执行接近于一个特定分布的方法不同，KGA维护分布差异（即，知识差异）。这放宽了分布假设。此外，我们首次将遗忘方法应用于各种NLP任务（即，分类，翻译，响应生成）并提出了几个具有相关性的遗忘评估指标。在语言建模和文本分类任务上的实验表明，我们提出的KGA框架在NLP场景中实现了适当的遗忘。",
    "tldr": "本文提出了基于知识差异对齐的通用机器遗忘框架KGA，实现了文本场景中的遗忘功能，并在多个NLP任务中进行了实验验证。",
    "en_tdlr": "The paper proposes a general machine unlearning framework called KGA which utilizes knowledge gap alignment to induce forgetfulness in text data. Experiments on language modeling and text classification tasks demonstrate the effectiveness of KGA in achieving proper forgetting in NLP scenarios."
}