{
    "title": "Recursions Are All You Need: Towards Efficient Deep Unfolding Networks. (arXiv:2305.05505v1 [cs.CV])",
    "abstract": "The use of deep unfolding networks in compressive sensing (CS) has seen wide success as they provide both simplicity and interpretability. However, since most deep unfolding networks are iterative, this incurs significant redundancies in the network. In this work, we propose a novel recursion-based framework to enhance the efficiency of deep unfolding models. First, recursions are used to effectively eliminate the redundancies in deep unfolding networks. Secondly, we randomize the number of recursions during training to decrease the overall training time. Finally, to effectively utilize the power of recursions, we introduce a learnable unit to modulate the features of the model based on both the total number of iterations and the current iteration index. To evaluate the proposed framework, we apply it to both ISTA-Net+ and COAST. Extensive testing shows that our proposed framework allows the network to cut down as much as 75% of its learnable parameters while mostly maintaining its per",
    "link": "http://arxiv.org/abs/2305.05505",
    "context": "Title: Recursions Are All You Need: Towards Efficient Deep Unfolding Networks. (arXiv:2305.05505v1 [cs.CV])\nAbstract: The use of deep unfolding networks in compressive sensing (CS) has seen wide success as they provide both simplicity and interpretability. However, since most deep unfolding networks are iterative, this incurs significant redundancies in the network. In this work, we propose a novel recursion-based framework to enhance the efficiency of deep unfolding models. First, recursions are used to effectively eliminate the redundancies in deep unfolding networks. Secondly, we randomize the number of recursions during training to decrease the overall training time. Finally, to effectively utilize the power of recursions, we introduce a learnable unit to modulate the features of the model based on both the total number of iterations and the current iteration index. To evaluate the proposed framework, we apply it to both ISTA-Net+ and COAST. Extensive testing shows that our proposed framework allows the network to cut down as much as 75% of its learnable parameters while mostly maintaining its per",
    "path": "papers/23/05/2305.05505.json",
    "total_tokens": 968,
    "translated_title": "递归是你需要的全部：朝着高效深度展开网络迈进",
    "translated_abstract": "深度展开网络在压缩感知方面的应用非常成功，因为它们提供了简单性和可解释性。然而，由于大多数深度展开网络是迭代的，这会在网络中产生较大的冗余。在这项工作中，我们提出了一个新的基于递归的框架，以增强深度展开模型的效率。首先，使用递归有效地消除深度展开网络中的冗余。其次，我们在训练过程中随机递归的数量，以减少整体训练时间。最后，为了有效地利用递归的能力，我们引入了一个可学习单元，根据迭代总数和当前迭代索引调节模型的特征。为评估所提出的框架，我们将其应用于ISTA-Net+和COAST。广泛的测试显示，我们的提出的框架允许网络削减高达75%的可学习参数，同时大多数维持其性能。",
    "tldr": "本文提出了一个基于递归的框架来提高深度展开网络的效率，在训练过程中随机递归数量，以减少整体训练时间，同时引入了一个可学习单元来调节模型特征，可以使网络削减高达75%的可学习参数。",
    "en_tdlr": "This paper proposes a recursion-based framework to enhance the efficiency of deep unfolding models. Randomizing the number of recursions during training decreases overall training time, and a learnable unit is introduced to modulate the model's features based on the total number of iterations and the current iteration index. Extensive testing shows that the proposed framework allows the network to cut down as much as 75% of its learnable parameters while mostly maintaining its performance."
}