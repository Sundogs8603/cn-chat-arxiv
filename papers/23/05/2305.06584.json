{
    "title": "Active Learning in the Predict-then-Optimize Framework: A Margin-Based Approach. (arXiv:2305.06584v1 [cs.LG])",
    "abstract": "We develop the first active learning method in the predict-then-optimize framework. Specifically, we develop a learning method that sequentially decides whether to request the \"labels\" of feature samples from an unlabeled data stream, where the labels correspond to the parameters of an optimization model for decision-making. Our active learning method is the first to be directly informed by the decision error induced by the predicted parameters, which is referred to as the Smart Predict-then-Optimize (SPO) loss. Motivated by the structure of the SPO loss, our algorithm adopts a margin-based criterion utilizing the concept of distance to degeneracy and minimizes a tractable surrogate of the SPO loss on the collected data. In particular, we develop an efficient active learning algorithm with both hard and soft rejection variants, each with theoretical excess risk (i.e., generalization) guarantees. We further derive bounds on the label complexity, which refers to the number of samples who",
    "link": "http://arxiv.org/abs/2305.06584",
    "context": "Title: Active Learning in the Predict-then-Optimize Framework: A Margin-Based Approach. (arXiv:2305.06584v1 [cs.LG])\nAbstract: We develop the first active learning method in the predict-then-optimize framework. Specifically, we develop a learning method that sequentially decides whether to request the \"labels\" of feature samples from an unlabeled data stream, where the labels correspond to the parameters of an optimization model for decision-making. Our active learning method is the first to be directly informed by the decision error induced by the predicted parameters, which is referred to as the Smart Predict-then-Optimize (SPO) loss. Motivated by the structure of the SPO loss, our algorithm adopts a margin-based criterion utilizing the concept of distance to degeneracy and minimizes a tractable surrogate of the SPO loss on the collected data. In particular, we develop an efficient active learning algorithm with both hard and soft rejection variants, each with theoretical excess risk (i.e., generalization) guarantees. We further derive bounds on the label complexity, which refers to the number of samples who",
    "path": "papers/23/05/2305.06584.json",
    "total_tokens": 925,
    "tldr": "本文开发了首个在预测-优化框架中进行主动学习的方法，通过符合退化距离的边缘准则最小化SPO损失的可计算替代品，在决策制定过程中取得了良好的效果。",
    "en_tdlr": "This paper develops the first active learning method in the predict-then-optimize framework, which minimizes the SPO loss through a margin-based criterion utilizing the concept of distance to degeneracy, resulting in a successful decision-making process."
}