{
    "title": "Monitoring Algorithmic Fairness. (arXiv:2305.15979v1 [cs.CY])",
    "abstract": "Machine-learned systems are in widespread use for making decisions about humans, and it is important that they are fair, i.e., not biased against individuals based on sensitive attributes. We present runtime verification of algorithmic fairness for systems whose models are unknown, but are assumed to have a Markov chain structure. We introduce a specification language that can model many common algorithmic fairness properties, such as demographic parity, equal opportunity, and social burden. We build monitors that observe a long sequence of events as generated by a given system, and output, after each observation, a quantitative estimate of how fair or biased the system was on that run until that point in time. The estimate is proven to be correct modulo a variable error bound and a given confidence level, where the error bound gets tighter as the observed sequence gets longer. Our monitors are of two types, and use, respectively, frequentist and Bayesian statistical inference techniqu",
    "link": "http://arxiv.org/abs/2305.15979",
    "context": "Title: Monitoring Algorithmic Fairness. (arXiv:2305.15979v1 [cs.CY])\nAbstract: Machine-learned systems are in widespread use for making decisions about humans, and it is important that they are fair, i.e., not biased against individuals based on sensitive attributes. We present runtime verification of algorithmic fairness for systems whose models are unknown, but are assumed to have a Markov chain structure. We introduce a specification language that can model many common algorithmic fairness properties, such as demographic parity, equal opportunity, and social burden. We build monitors that observe a long sequence of events as generated by a given system, and output, after each observation, a quantitative estimate of how fair or biased the system was on that run until that point in time. The estimate is proven to be correct modulo a variable error bound and a given confidence level, where the error bound gets tighter as the observed sequence gets longer. Our monitors are of two types, and use, respectively, frequentist and Bayesian statistical inference techniqu",
    "path": "papers/23/05/2305.15979.json",
    "total_tokens": 879,
    "translated_title": "监控算法公正性",
    "translated_abstract": "机器学习系统被广泛应用于对人类做出决策，因此确保它们是公正的（即不会基于敏感属性歧视个人）非常重要。本文介绍了针对模型未知但假定具有马尔可夫链结构的系统的算法公正性的运行时验证。我们引入了一种规范语言，可以建模许多常见的算法公正性属性，例如人口平等，等同机会和社会负担。我们构建了监视器，观察给定系统生成的一系列事件，并在每次观察之后输出该系统在到目前为止的运行中有多公正或有多倾向于偏见的定量估计。该估计值在给定的置信水平和变量误差范围下被证明是正确的，其中误差范围随着观察序列的长度而变得更紧。我们的监视器有两种类型，分别使用频率派和贝叶斯统计推断技术。",
    "tldr": "本论文介绍了一种对机器学习系统进行算法公正性监控的方法，该方法可以监测公正性属性，如人口平等和等同机会，为确保机器学习系统不基于敏感属性歧视个人提供有用的工具。",
    "en_tdlr": "This paper presents a method for monitoring algorithmic fairness in machine learning systems, which can detect fairness properties such as demographic parity and equal opportunity, providing useful tools to ensure that machine learning systems do not discriminate against individuals based on sensitive attributes."
}