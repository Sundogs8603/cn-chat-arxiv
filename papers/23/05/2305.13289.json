{
    "title": "Achieving Minimax Optimal Sample Complexity of Offline Reinforcement Learning: A DRO-Based Approach. (arXiv:2305.13289v2 [cs.LG] UPDATED)",
    "abstract": "Offline reinforcement learning aims to learn from pre-collected datasets without active exploration. This problem faces significant challenges, including limited data availability and distributional shifts. Existing approaches adopt a pessimistic stance towards uncertainty by penalizing rewards of under-explored state-action pairs to estimate value functions conservatively. In this paper, we show that the distributionally robust optimization (DRO) based approach can also address these challenges and is minimax optimal. Specifically, we directly model the uncertainty in the transition kernel and construct an uncertainty set of statistically plausible transition kernels. We then find the policy that optimizes the worst-case performance over this uncertainty set. We first design a metric-based Hoeffding-style uncertainty set such that with high probability the true transition kernel is in this set. We prove that to achieve a sub-optimality gap of $\\epsilon$, the sample complexity is $\\mat",
    "link": "http://arxiv.org/abs/2305.13289",
    "context": "Title: Achieving Minimax Optimal Sample Complexity of Offline Reinforcement Learning: A DRO-Based Approach. (arXiv:2305.13289v2 [cs.LG] UPDATED)\nAbstract: Offline reinforcement learning aims to learn from pre-collected datasets without active exploration. This problem faces significant challenges, including limited data availability and distributional shifts. Existing approaches adopt a pessimistic stance towards uncertainty by penalizing rewards of under-explored state-action pairs to estimate value functions conservatively. In this paper, we show that the distributionally robust optimization (DRO) based approach can also address these challenges and is minimax optimal. Specifically, we directly model the uncertainty in the transition kernel and construct an uncertainty set of statistically plausible transition kernels. We then find the policy that optimizes the worst-case performance over this uncertainty set. We first design a metric-based Hoeffding-style uncertainty set such that with high probability the true transition kernel is in this set. We prove that to achieve a sub-optimality gap of $\\epsilon$, the sample complexity is $\\mat",
    "path": "papers/23/05/2305.13289.json",
    "total_tokens": 958,
    "translated_title": "实现离线强化学习的极小极大样本复杂性：一种基于DRO的方法。",
    "translated_abstract": "离线强化学习旨在从先前收集的数据集中学习，而无需主动探索。该问题面临着诸多挑战，包括有限的数据可用性和分布转移。现有方法采用一种悲观的态度对待不确定性，通过惩罚未充分探索的状态-行为对的奖励来保守估计值函数。在本文中，我们展示了分布鲁棒优化（DRO）基于方法也可以应对这些挑战，并且是极小极大最优的。具体而言，我们直接建模转移核的不确定性，并构建一个统计合理的转移核不确定性集合。然后我们寻找在该不确定性集合上最优化最坏情况下的性能的策略。我们首先设计了一种基于度量的霍夫丁风格不确定性集合，这样真实的转移核以高概率位于该集合中。我们证明了为了实现$\\epsilon$的次最优性差距，样本复杂度为$\\mat。",
    "tldr": "本文提出了一种分布鲁棒优化 (DRO)的方法，用于解决离线强化学习中的数据有限性和分布转移问题。通过直接建模转移核的不确定性，并寻找在不确定性集合中最优化最坏情况下的性能的策略，实现了极小极大最优性。",
    "en_tdlr": "This paper proposes a distributionally robust optimization (DRO) approach to address the limited data availability and distributional shifts in offline reinforcement learning. By modeling the uncertainty in the transition kernel and finding the policy that optimizes worst-case performance over an uncertainty set, the paper achieves minimax optimality."
}