{
    "title": "Backpack Language Models. (arXiv:2305.16765v1 [cs.CL])",
    "abstract": "We present Backpacks: a new neural architecture that marries strong modeling performance with an interface for interpretability and control. Backpacks learn multiple non-contextual sense vectors for each word in a vocabulary, and represent a word in a sequence as a context-dependent, non-negative linear combination of sense vectors in this sequence. We find that, after training, sense vectors specialize, each encoding a different aspect of a word. We can interpret a sense vector by inspecting its (non-contextual, linear) projection onto the output space, and intervene on these interpretable hooks to change the model's behavior in predictable ways. We train a 170M-parameter Backpack language model on OpenWebText, matching the loss of a GPT-2 small (124Mparameter) Transformer. On lexical similarity evaluations, we find that Backpack sense vectors outperform even a 6B-parameter Transformer LM's word embeddings. Finally, we present simple algorithms that intervene on sense vectors to perfo",
    "link": "http://arxiv.org/abs/2305.16765",
    "context": "Title: Backpack Language Models. (arXiv:2305.16765v1 [cs.CL])\nAbstract: We present Backpacks: a new neural architecture that marries strong modeling performance with an interface for interpretability and control. Backpacks learn multiple non-contextual sense vectors for each word in a vocabulary, and represent a word in a sequence as a context-dependent, non-negative linear combination of sense vectors in this sequence. We find that, after training, sense vectors specialize, each encoding a different aspect of a word. We can interpret a sense vector by inspecting its (non-contextual, linear) projection onto the output space, and intervene on these interpretable hooks to change the model's behavior in predictable ways. We train a 170M-parameter Backpack language model on OpenWebText, matching the loss of a GPT-2 small (124Mparameter) Transformer. On lexical similarity evaluations, we find that Backpack sense vectors outperform even a 6B-parameter Transformer LM's word embeddings. Finally, we present simple algorithms that intervene on sense vectors to perfo",
    "path": "papers/23/05/2305.16765.json",
    "total_tokens": 1057,
    "translated_title": "背包语言模型",
    "translated_abstract": "我们提出了背包（Backpacks）：一种将强大的建模性能与可解释性和控制接口结合的新型神经架构。背包学习词汇表中每个单词的多个非上下文感知向量，并将序列中的单词表示为上下文依赖的非负线性组合。我们发现，在训练后，感知向量专门化了，每个感知向量编码了单词的不同方面。我们可以通过检查感知向量在输出空间上的（非上下文，线性）投影来解释一个感知向量，并干预这些可解释的钩子以可预测的方式改变模型的行为。我们在OpenWebText上训练了一个包含170M参数的背包语言模型，与124M参数的GPT-2小型Transformer的损失相匹配。在词汇相似性评估中，我们发现背包感知向量甚至优于一个6B参数的Transformer语言模型的单词嵌入。最后，我们提出了一些简单的算法来干预感知向量，以实现特定的行为。",
    "tldr": "背包语言模型是一种结合了强大的建模性能和可解释性的神经架构。它学习每个单词的多个非上下文感知向量，并将单词表示为上下文依赖的非负线性组合。感知向量可以被解释为单词不同的方面，并可以通过干预这些可解释的钩子以可预测的方式改变模型的行为。该模型在词汇相似性评估中表现优越，甚至优于一个6B参数的Transformer语言模型的单词嵌入。",
    "en_tdlr": "Backpack language model is a neural architecture that combines strong modeling performance with interpretability and control. It learns multiple non-contextual sense vectors for each word in a vocabulary, representing words as a context-dependent, non-negative linear combination of these vectors. Sense vectors specialize, each encoding a different aspect of a word, and can be interpreted and intervened upon in a predictable manner to change the model's behavior. The model outperforms a 6B-parameter Transformer LM's embeddings on lexical similarity evaluations."
}