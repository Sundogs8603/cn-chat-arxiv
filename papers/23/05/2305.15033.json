{
    "title": "SmartTrim: Adaptive Tokens and Attention Pruning for Efficient Vision-Language Models",
    "abstract": "arXiv:2305.15033v2 Announce Type: replace  Abstract: Despite achieving remarkable performance on various vision-language tasks, Transformer-based Vision-Language Models (VLMs) suffer from redundancy in inputs and parameters, significantly hampering their efficiency in real-world applications. Moreover, the degree of redundancy in token representations and model parameters, such as attention heads, varies significantly for different inputs. In light of the challenges, we propose SmartTrim, an adaptive acceleration framework for VLMs, which adjusts the computational overhead per instance. Specifically, we integrate lightweight modules into the original backbone to identify and prune redundant token representations and attention heads within each layer. Furthermore, we devise a self-distillation strategy to enhance the consistency between the predictions of the pruned model and its fully-capacity counterpart. Experimental results across various vision-language tasks consistently demonstra",
    "link": "https://arxiv.org/abs/2305.15033",
    "context": "Title: SmartTrim: Adaptive Tokens and Attention Pruning for Efficient Vision-Language Models\nAbstract: arXiv:2305.15033v2 Announce Type: replace  Abstract: Despite achieving remarkable performance on various vision-language tasks, Transformer-based Vision-Language Models (VLMs) suffer from redundancy in inputs and parameters, significantly hampering their efficiency in real-world applications. Moreover, the degree of redundancy in token representations and model parameters, such as attention heads, varies significantly for different inputs. In light of the challenges, we propose SmartTrim, an adaptive acceleration framework for VLMs, which adjusts the computational overhead per instance. Specifically, we integrate lightweight modules into the original backbone to identify and prune redundant token representations and attention heads within each layer. Furthermore, we devise a self-distillation strategy to enhance the consistency between the predictions of the pruned model and its fully-capacity counterpart. Experimental results across various vision-language tasks consistently demonstra",
    "path": "papers/23/05/2305.15033.json",
    "total_tokens": 771,
    "translated_title": "SmartTrim：用于高效的视觉-语言模型的自适应标记和注意力修剪",
    "translated_abstract": "尽管基于Transformer的视觉-语言模型（VLMs）在各种视觉-语言任务上取得了显著的性能，但输入和参数中存在冗余，严重影响了它们在实际应用中的效率。鉴于挑战，我们提出了SmartTrim，这是一种自适应加速框架，用于调整每个实例的计算开销。具体而言，我们将轻量级模块集成到原始主干中，以识别并修剪每个层中的冗余标记表示和注意力头。此外，我们设计了自我蒸馏策略，以增强修剪模型和其完全容量对应物之间预测的一致性。在各种视觉-语言任务中的实验结果不断展示",
    "tldr": "SmartTrim 提出了一种自适应加速框架，通过识别并修剪每个层中的冗余标记表示和注意力头来提升视觉-语言模型的效率。",
    "en_tdlr": "SmartTrim proposes an adaptive acceleration framework to enhance the efficiency of Vision-Language Models by identifying and pruning redundant token representations and attention heads within each layer."
}