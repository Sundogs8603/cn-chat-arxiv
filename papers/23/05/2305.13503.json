{
    "title": "Asynchronous Multi-Model Federated Learning over Wireless Networks: Theory, Modeling, and Optimization. (arXiv:2305.13503v1 [cs.LG])",
    "abstract": "Federated learning (FL) has emerged as a key technique for distributed machine learning (ML). Most literature on FL has focused on systems with (i) ML model training for a single task/model, (ii) a synchronous setting for uplink/downlink transfer of model parameters, which is often unrealistic. To address this, we develop MA-FL, which considers FL with multiple downstream tasks to be trained over an asynchronous model transmission architecture. We first characterize the convergence of ML model training under MA-FL via introducing a family of scheduling tensors to capture the scheduling of devices. Our convergence analysis sheds light on the impact of resource allocation (e.g., the mini-batch size and number of gradient descent iterations), device scheduling, and individual model states (i.e., warmed vs. cold initialization) on the performance of ML models. We then formulate a non-convex mixed integer optimization problem for jointly configuring the resource allocation and device schedu",
    "link": "http://arxiv.org/abs/2305.13503",
    "context": "Title: Asynchronous Multi-Model Federated Learning over Wireless Networks: Theory, Modeling, and Optimization. (arXiv:2305.13503v1 [cs.LG])\nAbstract: Federated learning (FL) has emerged as a key technique for distributed machine learning (ML). Most literature on FL has focused on systems with (i) ML model training for a single task/model, (ii) a synchronous setting for uplink/downlink transfer of model parameters, which is often unrealistic. To address this, we develop MA-FL, which considers FL with multiple downstream tasks to be trained over an asynchronous model transmission architecture. We first characterize the convergence of ML model training under MA-FL via introducing a family of scheduling tensors to capture the scheduling of devices. Our convergence analysis sheds light on the impact of resource allocation (e.g., the mini-batch size and number of gradient descent iterations), device scheduling, and individual model states (i.e., warmed vs. cold initialization) on the performance of ML models. We then formulate a non-convex mixed integer optimization problem for jointly configuring the resource allocation and device schedu",
    "path": "papers/23/05/2305.13503.json",
    "total_tokens": 1083,
    "translated_title": "无线网络中的异步多模型联邦学习：理论、建模与优化(arXiv:2305.13503v1 [cs.LG])",
    "translated_abstract": "联邦学习是一种分布式机器学习的重要技术。目前，联邦学习的大部分文献都关注单一任务/模型的机器学习模型训练，并采用同步模型参数传输设置。为了解决这个问题，本文提出了MA-FL，它考虑利用异步模型传输体系结构，实现有多个下游任务需要训练的联邦学习。我们首先通过引入一族调度张量来捕捉设备的调度，并对MA-FL下的机器学习模型训练收敛性进行了表征。我们的收敛性分析揭示了资源分配（例如，小批量大小和梯度下降迭代次数）、设备调度和个体模型状态（即预热与冷启动初始化）对机器学习模型性能的影响。最后，我们为MA-FL制定了一个非凸混整数优化问题，用于共同配置资源分配和设备调度。对合成和真实数据集的数值实验表明，MA-FL在收敛速度和模型精度方面优于现有的联邦学习方法。",
    "tldr": "本文提出了MA-FL，应用异步模型传输体系结构来实现有多个下游任务需要训练的联邦学习。本文的收敛性分析揭示了资源分配、设备调度和个体模型状态对机器学习模型性能的影响。实验表明，MA-FL在收敛速度和模型精度方面优于现有的联邦学习方法。",
    "en_tdlr": "This paper proposes MA-FL, which applies an asynchronous model transmission architecture to enable federated learning with multiple downstream tasks. The convergence analysis reveals the impact of resource allocation, device scheduling, and individual model states on the performance of machine learning models. Experimental results demonstrate that MA-FL outperforms existing federated learning methods in terms of convergence speed and model accuracy."
}