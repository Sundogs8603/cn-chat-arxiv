{
    "title": "Simple yet Effective Code-Switching Language Identification with Multitask Pre-Training and Transfer Learning. (arXiv:2305.19759v1 [cs.CL])",
    "abstract": "Code-switching, also called code-mixing, is the linguistics phenomenon where in casual settings, multilingual speakers mix words from different languages in one utterance. Due to its spontaneous nature, code-switching is extremely low-resource, which makes it a challenging problem for language and speech processing tasks. In such contexts, Code-Switching Language Identification (CSLID) becomes a difficult but necessary task if we want to maximally leverage existing monolingual tools for other tasks. In this work, we propose two novel approaches toward improving language identification accuracy on an English-Mandarin child-directed speech dataset. Our methods include a stacked Residual CNN+GRU model and a multitask pre-training approach to use Automatic Speech Recognition (ASR) as an auxiliary task for CSLID. Due to the low-resource nature of code-switching, we also employ careful silver data creation using monolingual corpora in both languages and up-sampling as data augmentation. We f",
    "link": "http://arxiv.org/abs/2305.19759",
    "context": "Title: Simple yet Effective Code-Switching Language Identification with Multitask Pre-Training and Transfer Learning. (arXiv:2305.19759v1 [cs.CL])\nAbstract: Code-switching, also called code-mixing, is the linguistics phenomenon where in casual settings, multilingual speakers mix words from different languages in one utterance. Due to its spontaneous nature, code-switching is extremely low-resource, which makes it a challenging problem for language and speech processing tasks. In such contexts, Code-Switching Language Identification (CSLID) becomes a difficult but necessary task if we want to maximally leverage existing monolingual tools for other tasks. In this work, we propose two novel approaches toward improving language identification accuracy on an English-Mandarin child-directed speech dataset. Our methods include a stacked Residual CNN+GRU model and a multitask pre-training approach to use Automatic Speech Recognition (ASR) as an auxiliary task for CSLID. Due to the low-resource nature of code-switching, we also employ careful silver data creation using monolingual corpora in both languages and up-sampling as data augmentation. We f",
    "path": "papers/23/05/2305.19759.json",
    "total_tokens": 779,
    "translated_title": "简单而有效的多任务预训练与迁移学习识别混合语言的方法",
    "translated_abstract": "本文针对语言混合现象（code-switching）提出了两种新的方法来提高在英汉儿童语音数据集上的语言识别精度，包括Residual CNN+GRU模型和多任务预训练方法使用自动语音识别作为CSLID的辅助任务。由于语言混合的低资源性，我们还使用了单语语料库进行银数据创建和上采样进行数据增强。",
    "tldr": "本文提出了两个新方法来提高在混合语音数据上的语言识别精度，包括使用Residual CNN+GRU模型和多任务预训练方法。由于语言混合的低资源性，我们还使用了单语语料库进行银数据创建和上采样。",
    "en_tdlr": "This paper proposes two novel methods to improve language identification accuracy on mixed-language speech data, including using a stacked Residual CNN+GRU model and a multitask pre-training approach. Due to the low-resource nature of code-switching, silver data creation using monolingual corpora and upsampling for data augmentation is employed."
}