{
    "title": "FITNESS: A Causal De-correlation Approach for Mitigating Bias in Machine Learning Software. (arXiv:2305.14396v1 [cs.LG])",
    "abstract": "Software built on top of machine learning algorithms is becoming increasingly prevalent in a variety of fields, including college admissions, healthcare, insurance, and justice. The effectiveness and efficiency of these systems heavily depend on the quality of the training datasets. Biased datasets can lead to unfair and potentially harmful outcomes, particularly in such critical decision-making systems where the allocation of resources may be affected. This can exacerbate discrimination against certain groups and cause significant social disruption. To mitigate such unfairness, a series of bias-mitigating methods are proposed. Generally, these studies improve the fairness of the trained models to a certain degree but with the expense of sacrificing the model performance. In this paper, we propose FITNESS, a bias mitigation approach via de-correlating the causal effects between sensitive features (e.g., the sex) and the label. Our key idea is that by de-correlating such effects from a ",
    "link": "http://arxiv.org/abs/2305.14396",
    "context": "Title: FITNESS: A Causal De-correlation Approach for Mitigating Bias in Machine Learning Software. (arXiv:2305.14396v1 [cs.LG])\nAbstract: Software built on top of machine learning algorithms is becoming increasingly prevalent in a variety of fields, including college admissions, healthcare, insurance, and justice. The effectiveness and efficiency of these systems heavily depend on the quality of the training datasets. Biased datasets can lead to unfair and potentially harmful outcomes, particularly in such critical decision-making systems where the allocation of resources may be affected. This can exacerbate discrimination against certain groups and cause significant social disruption. To mitigate such unfairness, a series of bias-mitigating methods are proposed. Generally, these studies improve the fairness of the trained models to a certain degree but with the expense of sacrificing the model performance. In this paper, we propose FITNESS, a bias mitigation approach via de-correlating the causal effects between sensitive features (e.g., the sex) and the label. Our key idea is that by de-correlating such effects from a ",
    "path": "papers/23/05/2305.14396.json",
    "total_tokens": 909,
    "translated_title": "FITNESS：一种减轻机器学习软件中偏见的因果去相关方法",
    "translated_abstract": "建立在机器学习算法上的软件在包括大学招生、医疗保健、保险和司法在内的各个领域中越来越普遍。这些系统的有效性和效率在很大程度上取决于训练数据集的质量。偏见数据集可能导致不公平和潜在有害的结果，特别是在这样的关键决策系统中，资源的分配可能会受到影响。这可能加剧对某些群体的歧视，并造成重大的社会动荡。为了减少这样的不公平，提出了一系列减轻偏见的方法。一般来说，这些研究在一定程度上提高了训练模型的公正性，但代价是牺牲了模型的性能。在本文中，我们提出了FITNESS，一种通过去相关敏感特征（例如性别）和标签之间的因果影响来减少偏见的方法。我们的关键思想是通过从特征中去相关获得更清晰的因果信息，同时维持模型的性能和准确性。",
    "tldr": "本文提出了一种名为FITNESS的方法，通过去除敏感特征和标签之间的因果影响来减少偏见，从而获得更公平的结果，同时保持模型的性能。",
    "en_tdlr": "This paper proposes FITNESS, a bias mitigation approach that de-correlates sensitive features and labels to achieve fairer results while maintaining the model's performance."
}