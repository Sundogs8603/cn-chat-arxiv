{
    "title": "PIP: Parse-Instructed Prefix for Syntactically Controlled Paraphrase Generation. (arXiv:2305.16701v1 [cs.CL])",
    "abstract": "Syntactically controlled paraphrase generation requires language models to generate paraphrases for sentences according to specific syntactic structures. Existing fine-tuning methods for this task are costly as all the parameters of the model need to be updated during the training process. Inspired by recent studies on parameter-efficient learning, we propose Parse-Instructed Prefix (PIP), a novel adaptation of prefix-tuning to tune large pre-trained language models on syntactically controlled paraphrase generation task in a low-data setting with significantly less training cost. We introduce two methods to instruct a model's encoder prefix to capture syntax-related knowledge: direct initiation (PIP-Direct) and indirect optimization (PIP-Indirect). In contrast to traditional fine-tuning methods for this task, PIP is a compute-efficient alternative with 10 times less learnable parameters. Compared to existing prefix-tuning methods, PIP excels at capturing syntax control information, ach",
    "link": "http://arxiv.org/abs/2305.16701",
    "context": "Title: PIP: Parse-Instructed Prefix for Syntactically Controlled Paraphrase Generation. (arXiv:2305.16701v1 [cs.CL])\nAbstract: Syntactically controlled paraphrase generation requires language models to generate paraphrases for sentences according to specific syntactic structures. Existing fine-tuning methods for this task are costly as all the parameters of the model need to be updated during the training process. Inspired by recent studies on parameter-efficient learning, we propose Parse-Instructed Prefix (PIP), a novel adaptation of prefix-tuning to tune large pre-trained language models on syntactically controlled paraphrase generation task in a low-data setting with significantly less training cost. We introduce two methods to instruct a model's encoder prefix to capture syntax-related knowledge: direct initiation (PIP-Direct) and indirect optimization (PIP-Indirect). In contrast to traditional fine-tuning methods for this task, PIP is a compute-efficient alternative with 10 times less learnable parameters. Compared to existing prefix-tuning methods, PIP excels at capturing syntax control information, ach",
    "path": "papers/23/05/2305.16701.json",
    "total_tokens": 930,
    "translated_title": "PIP：语法控制的释义生成的解析指导前缀",
    "translated_abstract": "语法控制的释义生成需要语言模型根据特定的语法结构为句子生成释义。现有的fine-tuning方法需要更新模型的所有参数，成本高昂。在参数有效学习的最新研究的启发下，我们提出了Parse-Instructed Prefix (PIP),这是一种新颖的前缀调整方法，可在低数据设置中调整大型预训练语言模型，显着降低训练成本。 我们介绍了两种方法来指导模型的编码器前缀捕获语法相关知识：直接初始化（PIP-Direct）和间接优化（PIP-Indirect）。与传统的fine-tuning方法相比，PIP是一种低计算成本的替代方法，具有10倍更少的可学习参数。与现有的前缀调整方法相比，PIP在捕获语法控制信息方面表现出色，这得益于将语法解析树作为指导前缀。实验结果表明，PIP在两个语法控制的释义生成基准测试中实现了最先进的性能，同时只需要很少的训练数据和时间。",
    "tldr": "使用Parse-Instructed Prefix的语法控制释义生成的计算成本降低了10倍，并在两个benchmark上达到了最先进的性能表现。"
}