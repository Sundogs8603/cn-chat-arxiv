{
    "title": "MPE4G: Multimodal Pretrained Encoder for Co-Speech Gesture Generation. (arXiv:2305.15740v1 [cs.CV])",
    "abstract": "When virtual agents interact with humans, gestures are crucial to delivering their intentions with speech. Previous multimodal co-speech gesture generation models required encoded features of all modalities to generate gestures. If some input modalities are removed or contain noise, the model may not generate the gestures properly. To acquire robust and generalized encodings, we propose a novel framework with a multimodal pre-trained encoder for co-speech gesture generation. In the proposed method, the multi-head-attention-based encoder is trained with self-supervised learning to contain the information on each modality. Moreover, we collect full-body gestures that consist of 3D joint rotations to improve visualization and apply gestures to the extensible body model. Through the series of experiments and human evaluation, the proposed method renders realistic co-speech gestures not only when all input modalities are given but also when the input modalities are missing or noisy.",
    "link": "http://arxiv.org/abs/2305.15740",
    "context": "Title: MPE4G: Multimodal Pretrained Encoder for Co-Speech Gesture Generation. (arXiv:2305.15740v1 [cs.CV])\nAbstract: When virtual agents interact with humans, gestures are crucial to delivering their intentions with speech. Previous multimodal co-speech gesture generation models required encoded features of all modalities to generate gestures. If some input modalities are removed or contain noise, the model may not generate the gestures properly. To acquire robust and generalized encodings, we propose a novel framework with a multimodal pre-trained encoder for co-speech gesture generation. In the proposed method, the multi-head-attention-based encoder is trained with self-supervised learning to contain the information on each modality. Moreover, we collect full-body gestures that consist of 3D joint rotations to improve visualization and apply gestures to the extensible body model. Through the series of experiments and human evaluation, the proposed method renders realistic co-speech gestures not only when all input modalities are given but also when the input modalities are missing or noisy.",
    "path": "papers/23/05/2305.15740.json",
    "total_tokens": 952,
    "translated_title": "MPE4G: 多模态预训练编码器用于同时语音手势生成",
    "translated_abstract": "当虚拟代理人与人类进行交互时，手势对于通过语音传递其意图至关重要。先前的多模态同时语音手势生成模型需要对所有模态的编码特征进行编码，才能生成手势。如果某些输入模态被移除或包含噪音，则模型可能无法正确生成手势。为了获得稳健和广义的编码，我们提出了一种新的框架，使用多模态预先训练的编码器进行共同语音手势生成。在所提出的方法中，基于多头注意力的编码器使用自我监督学习进行训练，以包含每种模态的信息。此外，我们收集由3D关节旋转组成的全身手势以改善可视化，并将手势应用于可扩展身体模型。通过一系列的实验和人工评估，所提出的方法在给定所有输入模态以及缺少或包含嘈杂模态时，都可以生成逼真的共同语音手势。",
    "tldr": "本文提出了一种名为MPE4G的多模态预训练编码器框架，用于共同语音手势生成。该方法通过自我监督学习来训练基于多头注意力的编码器以包含每种模态的信息，并且可以生成逼真的共同语音手势，即使存在缺失或嘈杂的输入模态。",
    "en_tdlr": "This paper proposes a multimodal pretrained encoder framework named MPE4G for co-speech gesture generation. The method trains a multi-head-attention-based encoder with self-supervised learning to contain the information on each modality, and can generate realistic co-speech gestures even when there are missing or noisy input modalities."
}