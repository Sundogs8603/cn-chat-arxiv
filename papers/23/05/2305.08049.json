{
    "title": "A Surprisingly Simple Continuous-Action POMDP Solver: Lazy Cross-Entropy Search Over Policy Trees. (arXiv:2305.08049v1 [cs.AI])",
    "abstract": "The Partially Observable Markov Decision Process (POMDP) provides a principled framework for decision making in stochastic partially observable environments. However, computing good solutions for problems with continuous action spaces remains challenging. To ease this challenge, we propose a simple online POMDP solver, called Lazy Cross-Entropy Search Over Policy Trees (LCEOPT). At each planning step, our method uses a lazy Cross-Entropy method to search the space of policy trees, which provide a simple policy representation. Specifically, we maintain a distribution on promising finite-horizon policy trees. The distribution is iteratively updated by sampling policies, evaluating them via Monte Carlo simulation, and refitting them to the top-performing ones. Our method is lazy in the sense that it exploits the policy tree representation to avoid redundant computations in policy sampling, evaluation, and distribution update. This leads to computational savings of up to two orders of magn",
    "link": "http://arxiv.org/abs/2305.08049",
    "context": "Title: A Surprisingly Simple Continuous-Action POMDP Solver: Lazy Cross-Entropy Search Over Policy Trees. (arXiv:2305.08049v1 [cs.AI])\nAbstract: The Partially Observable Markov Decision Process (POMDP) provides a principled framework for decision making in stochastic partially observable environments. However, computing good solutions for problems with continuous action spaces remains challenging. To ease this challenge, we propose a simple online POMDP solver, called Lazy Cross-Entropy Search Over Policy Trees (LCEOPT). At each planning step, our method uses a lazy Cross-Entropy method to search the space of policy trees, which provide a simple policy representation. Specifically, we maintain a distribution on promising finite-horizon policy trees. The distribution is iteratively updated by sampling policies, evaluating them via Monte Carlo simulation, and refitting them to the top-performing ones. Our method is lazy in the sense that it exploits the policy tree representation to avoid redundant computations in policy sampling, evaluation, and distribution update. This leads to computational savings of up to two orders of magn",
    "path": "papers/23/05/2305.08049.json",
    "total_tokens": 1118,
    "translated_title": "令人惊讶的简单连续行动POMDP求解器：基于策略树的懒惰交叉熵搜索",
    "translated_abstract": "部分可观察马尔可夫决策过程（POMDP）提供了在随机部分可观察环境中进行决策的原则性框架。但对于具有连续动作空间的问题提供良好解决方案仍然具有挑战性。为了简化这个挑战，我们提出了一种称为Lazy Cross-Entropy Search Over Policy Trees (LCEOPT) 的简单在线POMDP求解器。我们的方法在每个计划步骤中使用懒惰交叉熵方法来搜索策略树空间，该树提供了一种简单的策略表示。具体而言，我们维护一个分布在有前途的有限时间策略树上的分布。通过抽样策略、通过蒙特卡罗模拟评估它们并将它们重新拟合到表现最佳的策略上，迭代更新此分布。我们的方法是懒惰的，因为它利用策略树表示来避免策略抽样、评估和分布更新中的冗余计算。与以前针对连续行动空间的最先进的POMDP求解器相比，这导致可节省高达两个数量级的计算。我们的实验表明，LCEOPT可以高精度和高效地解决具有挑战性的连续行动POMDP。",
    "tldr": "提出了一种称为LCEOPT的简单在线连续动作POMDP求解器，利用基于策略树的懒惰交叉熵搜索实现了高效解决具有连续动作空间的挑战性POMDP问题，相较于以往最先进的POMDP求解器可实现高达两个数量级的计算节省。",
    "en_tdlr": "Proposed a simple online POMDP solver called LCEOPT using lazy Cross-Entropy method to search policy trees and achieved high accuracy and efficiency for challenging continuous action POMDP problems, with up to two orders of magnitude computational saving compared to previous state-of-the-art POMDP solvers."
}