{
    "title": "Fast global convergence of gradient descent for low-rank matrix approximation. (arXiv:2305.19206v1 [math.OC])",
    "abstract": "This paper investigates gradient descent for solving low-rank matrix approximation problems. We begin by establishing the local linear convergence of gradient descent for symmetric matrix approximation. Building on this result, we prove the rapid global convergence of gradient descent, particularly when initialized with small random values. Remarkably, we show that even with moderate random initialization, which includes small random initialization as a special case, gradient descent achieves fast global convergence in scenarios where the top eigenvalues are identical. Furthermore, we extend our analysis to address asymmetric matrix approximation problems and investigate the effectiveness of a retraction-free eigenspace computation method. Numerical experiments strongly support our theory. In particular, the retraction-free algorithm outperforms the corresponding Riemannian gradient descent method, resulting in a significant 29\\% reduction in runtime.",
    "link": "http://arxiv.org/abs/2305.19206",
    "context": "Title: Fast global convergence of gradient descent for low-rank matrix approximation. (arXiv:2305.19206v1 [math.OC])\nAbstract: This paper investigates gradient descent for solving low-rank matrix approximation problems. We begin by establishing the local linear convergence of gradient descent for symmetric matrix approximation. Building on this result, we prove the rapid global convergence of gradient descent, particularly when initialized with small random values. Remarkably, we show that even with moderate random initialization, which includes small random initialization as a special case, gradient descent achieves fast global convergence in scenarios where the top eigenvalues are identical. Furthermore, we extend our analysis to address asymmetric matrix approximation problems and investigate the effectiveness of a retraction-free eigenspace computation method. Numerical experiments strongly support our theory. In particular, the retraction-free algorithm outperforms the corresponding Riemannian gradient descent method, resulting in a significant 29\\% reduction in runtime.",
    "path": "papers/23/05/2305.19206.json",
    "total_tokens": 920,
    "translated_title": "低秩矩阵逼近的梯度下降全局快速收敛性",
    "translated_abstract": "本文研究了使用梯度下降求解低秩矩阵逼近问题。我们首先证明了对于对称矩阵逼近，梯度下降的局部线性收敛性。在此基础上，我们证明了梯度下降在使用随机小值初始化时具有非常快的全局收敛性。值得注意的是，我们表明，即使是在中等大小的随机初始化情况下，包括使用小随机初始化的特殊情况在内，当最大特征值相同时，梯度下降在快速收敛方面也具有很好的效果。此外，我们扩展了我们的分析以解决非对称矩阵逼近问题，并调查了一种不依赖投影的特征空间计算方法的有效性。数值实验强烈支持我们的理论。特别地，退化自由算法优于对应的Riemannian梯度下降方法，导致运行时间显着减少了29％。",
    "tldr": "本文证明了对于低秩矩阵逼近问题，使用小随机初始化的梯度下降方法具有非常快的全局收敛性，尤其是当最大特征值相同时。退化自由算法比Riemannian梯度下降方法更有效，在运行时间上可以减少29％。",
    "en_tdlr": "This paper proves that using small random initialization, gradient descent method has fast global convergence for low-rank matrix approximation, especially when the top eigenvalues are identical. The retraction-free algorithm is more efficient than the Riemannian gradient descent method, with a significant 29% reduction in runtime."
}