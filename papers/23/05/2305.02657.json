{
    "title": "Statistical Optimality of Deep Wide Neural Networks. (arXiv:2305.02657v1 [stat.ML])",
    "abstract": "In this paper, we consider the generalization ability of deep wide feedforward ReLU neural networks defined on a bounded domain $\\mathcal X \\subset \\mathbb R^{d}$. We first demonstrate that the generalization ability of the neural network can be fully characterized by that of the corresponding deep neural tangent kernel (NTK) regression. We then investigate on the spectral properties of the deep NTK and show that the deep NTK is positive definite on $\\mathcal{X}$ and its eigenvalue decay rate is $(d+1)/d$. Thanks to the well established theories in kernel regression, we then conclude that multilayer wide neural networks trained by gradient descent with proper early stopping achieve the minimax rate, provided that the regression function lies in the reproducing kernel Hilbert space (RKHS) associated with the corresponding NTK. Finally, we illustrate that the overfitted multilayer wide neural networks can not generalize well on $\\mathbb S^{d}$.",
    "link": "http://arxiv.org/abs/2305.02657",
    "context": "Title: Statistical Optimality of Deep Wide Neural Networks. (arXiv:2305.02657v1 [stat.ML])\nAbstract: In this paper, we consider the generalization ability of deep wide feedforward ReLU neural networks defined on a bounded domain $\\mathcal X \\subset \\mathbb R^{d}$. We first demonstrate that the generalization ability of the neural network can be fully characterized by that of the corresponding deep neural tangent kernel (NTK) regression. We then investigate on the spectral properties of the deep NTK and show that the deep NTK is positive definite on $\\mathcal{X}$ and its eigenvalue decay rate is $(d+1)/d$. Thanks to the well established theories in kernel regression, we then conclude that multilayer wide neural networks trained by gradient descent with proper early stopping achieve the minimax rate, provided that the regression function lies in the reproducing kernel Hilbert space (RKHS) associated with the corresponding NTK. Finally, we illustrate that the overfitted multilayer wide neural networks can not generalize well on $\\mathbb S^{d}$.",
    "path": "papers/23/05/2305.02657.json",
    "total_tokens": 1005,
    "translated_title": "深度宽松弛神经网络的统计优化性",
    "translated_abstract": "本文研究了定义在有界域$\\mathcal X \\subset \\mathbb R^{d}$上的深度宽松弛ReLU神经网络的泛化能力。首先证明了神经网络的泛化能力可以被相应的深度神经切向核回归所完全描绘。然后，我们研究了深度神经切向核的谱特性，并证明了深度神经切向核在$\\mathcal{X}$上为正定，其特征值衰减率为$(d+1)/d$。由于核回归中已经建立的理论，我们得出结论，适当早停的梯度下降训练的多层宽神经网络可以实现最小极大率，前提是回归函数在对应的NTK相关的再生核希尔伯特空间中。最后，我们证明过度拟合的多层宽神经网络在$\\mathbb S^{d}$上不能很好地泛化。",
    "tldr": "本文研究了深度宽松弛ReLU神经网络的泛化能力，证明适当早停的梯度下降训练的多层宽神经网络可以实现最小极大率，前提是回归函数在对应的NTK相关的再生核希尔伯特空间中，但过度拟合的多层宽神经网络在$\\mathbb S^{d}$上不能很好地泛化。",
    "en_tdlr": "This paper explores the generalization ability of deep wide feedforward ReLU neural networks, characterizing it through the corresponding deep neural tangent kernel (NTK) regression and demonstrating that properly trained multilayer wide neural networks can achieve the minimax rate on a bounded domain, as long as the regression function lies in the reproducing kernel Hilbert space associated with the corresponding NTK. However, the paper also shows that overfitted multilayer wide neural networks do not generalize well on $\\mathbb S^{d}$."
}