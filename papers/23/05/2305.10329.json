{
    "title": "G-Adapter: Towards Structure-Aware Parameter-Efficient Transfer Learning for Graph Transformer Networks. (arXiv:2305.10329v1 [cs.LG])",
    "abstract": "It has become a popular paradigm to transfer the knowledge of large-scale pre-trained models to various downstream tasks via fine-tuning the entire model parameters. However, with the growth of model scale and the rising number of downstream tasks, this paradigm inevitably meets the challenges in terms of computation consumption and memory footprint issues. Recently, Parameter-Efficient Fine-Tuning (PEFT) (e.g., Adapter, LoRA, BitFit) shows a promising paradigm to alleviate these concerns by updating only a portion of parameters. Despite these PEFTs having demonstrated satisfactory performance in natural language processing, it remains under-explored for the question of whether these techniques could be transferred to graph-based tasks with Graph Transformer Networks (GTNs). Therefore, in this paper, we fill this gap by providing extensive benchmarks with traditional PEFTs on a range of graph-based downstream tasks. Our empirical study shows that it is sub-optimal to directly transfer ",
    "link": "http://arxiv.org/abs/2305.10329",
    "context": "Title: G-Adapter: Towards Structure-Aware Parameter-Efficient Transfer Learning for Graph Transformer Networks. (arXiv:2305.10329v1 [cs.LG])\nAbstract: It has become a popular paradigm to transfer the knowledge of large-scale pre-trained models to various downstream tasks via fine-tuning the entire model parameters. However, with the growth of model scale and the rising number of downstream tasks, this paradigm inevitably meets the challenges in terms of computation consumption and memory footprint issues. Recently, Parameter-Efficient Fine-Tuning (PEFT) (e.g., Adapter, LoRA, BitFit) shows a promising paradigm to alleviate these concerns by updating only a portion of parameters. Despite these PEFTs having demonstrated satisfactory performance in natural language processing, it remains under-explored for the question of whether these techniques could be transferred to graph-based tasks with Graph Transformer Networks (GTNs). Therefore, in this paper, we fill this gap by providing extensive benchmarks with traditional PEFTs on a range of graph-based downstream tasks. Our empirical study shows that it is sub-optimal to directly transfer ",
    "path": "papers/23/05/2305.10329.json",
    "total_tokens": 898,
    "translated_title": "G-Adapter: 面向图形Transformer网络的结构感知参数高效迁移学习",
    "translated_abstract": "将大规模预训练模型的知识通过微调整个模型参数传递到各个下游任务已成为一种流行的范例。然而，随着模型规模的增长和下游任务数量的增加，这种范例不可避免地面临着计算消耗和内存占用问题。最近，参数高效微调（如Adapter、LoRA、BitFit）展示了一种有望通过仅更新一部分参数来缓解这些问题的范例。尽管这些技术已经在自然语言处理方面展示出了令人满意的性能，但它在图形Transformer网络（GTNs）下的适用性仍然不够广泛。因此，在本文中，我们通过在一系列基于图形的下游任务上进行广泛的基准测试来填补这一空白。我们的实证研究表明，直接将PEFT技术应用于GTNs并不是最优的解决方案。",
    "tldr": "本文提出一种名为G-Adapter的面向图形Transformer网络的结构感知参数高效迁移学习算法，通过对一组下游图形任务进行广泛测试，证明了将PEFT技术应用于GTNs并非最佳解决方案。",
    "en_tdlr": "This paper proposes a structure-aware parameter-efficient transfer learning algorithm called G-Adapter for graph Transformer networks (GTNs). Extensive benchmark tests on a range of downstream graph-based tasks show that directly applying parameter-efficient fine-tuning (PEFT) techniques to GTNs is sub-optimal."
}