{
    "title": "Generalization Bounds for Neural Belief Propagation Decoders. (arXiv:2305.10540v1 [cs.IT])",
    "abstract": "Machine learning based approaches are being increasingly used for designing decoders for next generation communication systems. One widely used framework is neural belief propagation (NBP), which unfolds the belief propagation (BP) iterations into a deep neural network and the parameters are trained in a data-driven manner. NBP decoders have been shown to improve upon classical decoding algorithms. In this paper, we investigate the generalization capabilities of NBP decoders. Specifically, the generalization gap of a decoder is the difference between empirical and expected bit-error-rate(s). We present new theoretical results which bound this gap and show the dependence on the decoder complexity, in terms of code parameters (blocklength, message length, variable/check node degrees), decoding iterations, and the training dataset size. Results are presented for both regular and irregular parity-check matrices. To the best of our knowledge, this is the first set of theoretical results on ",
    "link": "http://arxiv.org/abs/2305.10540",
    "context": "Title: Generalization Bounds for Neural Belief Propagation Decoders. (arXiv:2305.10540v1 [cs.IT])\nAbstract: Machine learning based approaches are being increasingly used for designing decoders for next generation communication systems. One widely used framework is neural belief propagation (NBP), which unfolds the belief propagation (BP) iterations into a deep neural network and the parameters are trained in a data-driven manner. NBP decoders have been shown to improve upon classical decoding algorithms. In this paper, we investigate the generalization capabilities of NBP decoders. Specifically, the generalization gap of a decoder is the difference between empirical and expected bit-error-rate(s). We present new theoretical results which bound this gap and show the dependence on the decoder complexity, in terms of code parameters (blocklength, message length, variable/check node degrees), decoding iterations, and the training dataset size. Results are presented for both regular and irregular parity-check matrices. To the best of our knowledge, this is the first set of theoretical results on ",
    "path": "papers/23/05/2305.10540.json",
    "total_tokens": 915,
    "translated_title": "神经置信传播译码器的泛化边界",
    "translated_abstract": "越来越多的采用基于机器学习的方法来设计下一代通信系统的解码器。一个广泛使用的框架是神经置信传播（NBP），它将置信传播（BP）迭代展开为深度神经网络，参数以数据驱动方式进行训练。已经证明，NBP解码器相较于传统的解码算法有所改进。本文研究了NBP解码器的泛化能力。具体而言，解码器的泛化间隙是经验和期望误码率之间的差异。我们提出了新的理论结果，界定了这种差距，并表明它与解码器的复杂程度（即代码参数（块长度、消息长度、变量/检查节点度数）、解码迭代次数和训练数据集大小）有关。我们还展示了常规和不规则奇偶校验矩阵的结果。据我们所知，这是第一组关于神经置信传播译码器的理论结果。",
    "tldr": "本文研究了神经置信传播译码器的泛化能力，提出了一组新的理论结果，界定了解码器的泛化间隙，结果与解码器的复杂程度有关。"
}