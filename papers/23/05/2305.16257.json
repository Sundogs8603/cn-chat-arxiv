{
    "title": "Fast Online Node Labeling for Very Large Graphs. (arXiv:2305.16257v2 [cs.LG] UPDATED)",
    "abstract": "This paper studies the online node classification problem under a transductive learning setting. Current methods either invert a graph kernel matrix with $\\mathcal{O}(n^3)$ runtime and $\\mathcal{O}(n^2)$ space complexity or sample a large volume of random spanning trees, thus are difficult to scale to large graphs. In this work, we propose an improvement based on the \\textit{online relaxation} technique introduced by a series of works (Rakhlin et al.,2012; Rakhlin and Sridharan, 2015; 2017). We first prove an effective regret $\\mathcal{O}(\\sqrt{n^{1+\\gamma}})$ when suitable parameterized graph kernels are chosen, then propose an approximate algorithm FastONL enjoying $\\mathcal{O}(k\\sqrt{n^{1+\\gamma}})$ regret based on this relaxation. The key of FastONL is a \\textit{generalized local push} method that effectively approximates inverse matrix columns and applies to a series of popular kernels. Furthermore, the per-prediction cost is $\\mathcal{O}(\\text{vol}({\\mathcal{S}})\\log 1/\\epsilon)$",
    "link": "http://arxiv.org/abs/2305.16257",
    "context": "Title: Fast Online Node Labeling for Very Large Graphs. (arXiv:2305.16257v2 [cs.LG] UPDATED)\nAbstract: This paper studies the online node classification problem under a transductive learning setting. Current methods either invert a graph kernel matrix with $\\mathcal{O}(n^3)$ runtime and $\\mathcal{O}(n^2)$ space complexity or sample a large volume of random spanning trees, thus are difficult to scale to large graphs. In this work, we propose an improvement based on the \\textit{online relaxation} technique introduced by a series of works (Rakhlin et al.,2012; Rakhlin and Sridharan, 2015; 2017). We first prove an effective regret $\\mathcal{O}(\\sqrt{n^{1+\\gamma}})$ when suitable parameterized graph kernels are chosen, then propose an approximate algorithm FastONL enjoying $\\mathcal{O}(k\\sqrt{n^{1+\\gamma}})$ regret based on this relaxation. The key of FastONL is a \\textit{generalized local push} method that effectively approximates inverse matrix columns and applies to a series of popular kernels. Furthermore, the per-prediction cost is $\\mathcal{O}(\\text{vol}({\\mathcal{S}})\\log 1/\\epsilon)$",
    "path": "papers/23/05/2305.16257.json",
    "total_tokens": 981,
    "translated_title": "面向超大规模图的快速在线节点分类算法",
    "translated_abstract": "本文研究了转导学习背景下的在线节点分类问题。当前方法要么需要在$\\mathcal{O}(n^3)$的时间复杂度和$\\mathcal{O}(n^2)$的空间复杂度内求解图核矩阵的逆，要么需要采样大量的随机生成树，这使得这些方法难以处理大规模图。我们提出了一种基于在线松弛技术的改进算法。当适当选择参数化的图核时，我们首先证明了有效的遗憾值为$\\mathcal{O}(\\sqrt{n^{1+\\gamma}})$。然后，我们基于该松弛提出了一种近似算法FastONL，其遗憾值为$\\mathcal{O}(k\\sqrt{n^{1+\\gamma}})$。FastONL的关键是一种广义局部推送方法，它能有效地近似逆矩阵列并应用于一系列流行的图核。此外，每个预测的成本为$\\mathcal{O}(\\text{vol}({\\mathcal{S}})\\log 1/\\epsilon)$",
    "tldr": "本文提出了一种适用于超大规模图的在线节点分类算法FastONL，它基于广义局部推送方法，能有效近似逆矩阵列并应用于一系列流行的图核，具有较低的遗憾值和每个预测的较低成本。",
    "en_tdlr": "This paper proposes an online node classification algorithm FastONL for very large graphs based on the generalized local push method, which effectively approximates inverse matrix columns and applies to a series of popular kernels. FastONL has lower regret and lower cost per prediction."
}