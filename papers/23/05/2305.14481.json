{
    "title": "FOCUS: Effective Embedding Initialization for Specializing Pretrained Multilingual Models on a Single Language. (arXiv:2305.14481v1 [cs.CL])",
    "abstract": "Using model weights pretrained on a high-resource language as a warm start can reduce the need for data and compute to obtain high-quality language models in low-resource languages. To accommodate the new language, the pretrained vocabulary and embeddings need to be adapted. Previous work on embedding initialization for such adapted vocabularies has mostly focused on monolingual source models. In this paper, we investigate the multilingual source model setting and propose FOCUS - Fast Overlapping Token Combinations Using Sparsemax, a novel embedding initialization method that outperforms previous work when adapting XLM-R. FOCUS represents newly added tokens as combinations of tokens in the overlap of the pretrained and new vocabularies. The overlapping tokens are selected based on semantic similarity in an auxiliary token embedding space. Our implementation of FOCUS is publicly available on GitHub.",
    "link": "http://arxiv.org/abs/2305.14481",
    "context": "Title: FOCUS: Effective Embedding Initialization for Specializing Pretrained Multilingual Models on a Single Language. (arXiv:2305.14481v1 [cs.CL])\nAbstract: Using model weights pretrained on a high-resource language as a warm start can reduce the need for data and compute to obtain high-quality language models in low-resource languages. To accommodate the new language, the pretrained vocabulary and embeddings need to be adapted. Previous work on embedding initialization for such adapted vocabularies has mostly focused on monolingual source models. In this paper, we investigate the multilingual source model setting and propose FOCUS - Fast Overlapping Token Combinations Using Sparsemax, a novel embedding initialization method that outperforms previous work when adapting XLM-R. FOCUS represents newly added tokens as combinations of tokens in the overlap of the pretrained and new vocabularies. The overlapping tokens are selected based on semantic similarity in an auxiliary token embedding space. Our implementation of FOCUS is publicly available on GitHub.",
    "path": "papers/23/05/2305.14481.json",
    "total_tokens": 840,
    "translated_title": "FOCUS：基于单语言的预训练多语言模型的有效嵌入初始化方法",
    "translated_abstract": "在低资源语言中获得高质量的语言模型需要大量的数据和计算。使用在高资源语言上预训练的模型权重作为温启动，可以减少此需求。为了适应新语言，需要对预训练的词汇表和嵌入进行调整。在以前的工作中，针对适应后的词汇表的嵌入初始化大多聚焦于单语言源模型。本文研究了多语言源模型设置，并提出了FOCUS-快速重叠标记组合使用Sparsemax的新型嵌入初始化方法，当适应XLM-R时，它的表现优于以前的工作。FOCUS将新增的标记表示为预训练和新词汇表之间的重叠标记组合。这些重叠标记是基于辅助标记嵌入空间中的语义相似性进行选择的。我们实现的FOCUS公开在GitHub上。",
    "tldr": "本文提出了FOCUS，在多语言源模型设置下，该方法使用重叠标记组合有效地初始化预训练的模型权重，提高了这种方法在适应新语言时的性能表现。",
    "en_tdlr": "This paper proposes FOCUS, a novel embedding initialization method that effectively initializes pretrained model weights using overlapping token combinations in multilingual source model setting and outperforms previous work when adapting XLM-R to new languages."
}