{
    "title": "Syntactic Knowledge via Graph Attention with BERT in Machine Translation. (arXiv:2305.13413v1 [cs.CL])",
    "abstract": "Although the Transformer model can effectively acquire context features via a self-attention mechanism, deeper syntactic knowledge is still not effectively modeled. To alleviate the above problem, we propose Syntactic knowledge via Graph attention with BERT (SGB) in Machine Translation (MT) scenarios. Graph Attention Network (GAT) and BERT jointly represent syntactic dependency feature as explicit knowledge of the source language to enrich source language representations and guide target language generation. Our experiments use gold syntax-annotation sentences and Quality Estimation (QE) model to obtain interpretability of translation quality improvement regarding syntactic knowledge without being limited to a BLEU score. Experiments show that the proposed SGB engines improve translation quality across the three MT tasks without sacrificing BLEU scores. We investigate what length of source sentences benefits the most and what dependencies are better identified by the SGB engines. We al",
    "link": "http://arxiv.org/abs/2305.13413",
    "context": "Title: Syntactic Knowledge via Graph Attention with BERT in Machine Translation. (arXiv:2305.13413v1 [cs.CL])\nAbstract: Although the Transformer model can effectively acquire context features via a self-attention mechanism, deeper syntactic knowledge is still not effectively modeled. To alleviate the above problem, we propose Syntactic knowledge via Graph attention with BERT (SGB) in Machine Translation (MT) scenarios. Graph Attention Network (GAT) and BERT jointly represent syntactic dependency feature as explicit knowledge of the source language to enrich source language representations and guide target language generation. Our experiments use gold syntax-annotation sentences and Quality Estimation (QE) model to obtain interpretability of translation quality improvement regarding syntactic knowledge without being limited to a BLEU score. Experiments show that the proposed SGB engines improve translation quality across the three MT tasks without sacrificing BLEU scores. We investigate what length of source sentences benefits the most and what dependencies are better identified by the SGB engines. We al",
    "path": "papers/23/05/2305.13413.json",
    "total_tokens": 876,
    "translated_title": "基于BERT和图注意力机制的机器翻译中的句法知识",
    "translated_abstract": "虽然Transformer模型通过自注意机制可以有效地获取上下文特征，但较深的句法知识仍然未被有效地建模。为解决这个问题，我们提出了一种在机器翻译场景中使用图注意力和BERT来表示句法依赖特征的句法知识（SGB）方法。该方法可以丰富源语言表示并引导目标语言生成。我们的实验使用了金标注句子和质量估计模型来获得关于句法知识对翻译质量改善的解释性。实验结果表明，所提出的SGB引擎可以在三个机器翻译任务中改善翻译质量，而不会牺牲BLEU分数。我们研究了哪些源句长度受益最大，以及哪些依赖关系被SGB引擎更好地识别。",
    "tldr": "该论文提出了一种在机器翻译中使用图注意力和BERT来表示句法依赖关系的方法，可以丰富源语言表示并引导目标语言生成，经实验证实该方法能够在不影响BLEU分数的情况下改善翻译质量。"
}