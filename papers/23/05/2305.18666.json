{
    "title": "BiSLS/SPS: Auto-tune Step Sizes for Stable Bi-level Optimization. (arXiv:2305.18666v2 [cs.LG] UPDATED)",
    "abstract": "The popularity of bi-level optimization (BO) in deep learning has spurred a growing interest in studying gradient-based BO algorithms. However, existing algorithms involve two coupled learning rates that can be affected by approximation errors when computing hypergradients, making careful fine-tuning necessary to ensure fast convergence. To alleviate this issue, we investigate the use of recently proposed adaptive step-size methods, namely stochastic line search (SLS) and stochastic Polyak step size (SPS), for computing both the upper and lower-level learning rates. First, we revisit the use of SLS and SPS in single-level optimization without the additional interpolation condition that is typically assumed in prior works. For such settings, we investigate new variants of SLS and SPS that improve upon existing suggestions in the literature and are simpler to implement. Importantly, these two variants can be seen as special instances of general family of methods with an envelope-type ste",
    "link": "http://arxiv.org/abs/2305.18666",
    "context": "Title: BiSLS/SPS: Auto-tune Step Sizes for Stable Bi-level Optimization. (arXiv:2305.18666v2 [cs.LG] UPDATED)\nAbstract: The popularity of bi-level optimization (BO) in deep learning has spurred a growing interest in studying gradient-based BO algorithms. However, existing algorithms involve two coupled learning rates that can be affected by approximation errors when computing hypergradients, making careful fine-tuning necessary to ensure fast convergence. To alleviate this issue, we investigate the use of recently proposed adaptive step-size methods, namely stochastic line search (SLS) and stochastic Polyak step size (SPS), for computing both the upper and lower-level learning rates. First, we revisit the use of SLS and SPS in single-level optimization without the additional interpolation condition that is typically assumed in prior works. For such settings, we investigate new variants of SLS and SPS that improve upon existing suggestions in the literature and are simpler to implement. Importantly, these two variants can be seen as special instances of general family of methods with an envelope-type ste",
    "path": "papers/23/05/2305.18666.json",
    "total_tokens": 915,
    "translated_title": "BiSLS/SPS：稳定双层优化的自动调整步长",
    "translated_abstract": "双层优化（BO）在深度学习中的流行引发了对基于梯度的BO算法的兴趣。然而，现有算法涉及两个耦合的学习率，当计算超梯度时可能受到近似误差的影响，因此需要仔细微调以确保快速收敛。为了缓解这个问题，我们研究了最近提出的自适应步长方法——随机线搜索（SLS）和随机Polyak步长（SPS），用于计算上层和下层的学习率。首先，我们重新审视了在没有通常在先前研究中假设的额外插值条件的情况下在单层优化中使用SLS和SPS的方法。对于这样的设置，我们研究了SLS和SPS的新变体，改进了文献中的现有建议，并且更容易实现。重要的是，这两个变体可以看作是一类具有包络型结构方法的特殊实例。",
    "tldr": "本文研究了稳定双层优化中自动调整步长的方法，并提出了两个变体的改进版本来替代现有的算法。这些方法使用了最近提出的自适应步长方法进行计算，以缓解计算超梯度时可能引起的近似误差问题。"
}