{
    "title": "Error Feedback Shines when Features are Rare. (arXiv:2305.15264v1 [math.OC])",
    "abstract": "We provide the first proof that gradient descent $\\left({\\color{green}\\sf GD}\\right)$ with greedy sparsification $\\left({\\color{green}\\sf TopK}\\right)$ and error feedback $\\left({\\color{green}\\sf EF}\\right)$ can obtain better communication complexity than vanilla ${\\color{green}\\sf GD}$ when solving the distributed optimization problem $\\min_{x\\in \\mathbb{R}^d} {f(x)=\\frac{1}{n}\\sum_{i=1}^n f_i(x)}$, where $n$ = # of clients, $d$ = # of features, and $f_1,\\dots,f_n$ are smooth nonconvex functions. Despite intensive research since 2014 when ${\\color{green}\\sf EF}$ was first proposed by Seide et al., this problem remained open until now. We show that ${\\color{green}\\sf EF}$ shines in the regime when features are rare, i.e., when each feature is present in the data owned by a small number of clients only. To illustrate our main result, we show that in order to find a random vector $\\hat{x}$ such that $\\lVert {\\nabla f(\\hat{x})} \\rVert^2 \\leq \\varepsilon$ in expectation, ${\\color{green}\\sf",
    "link": "http://arxiv.org/abs/2305.15264",
    "context": "Title: Error Feedback Shines when Features are Rare. (arXiv:2305.15264v1 [math.OC])\nAbstract: We provide the first proof that gradient descent $\\left({\\color{green}\\sf GD}\\right)$ with greedy sparsification $\\left({\\color{green}\\sf TopK}\\right)$ and error feedback $\\left({\\color{green}\\sf EF}\\right)$ can obtain better communication complexity than vanilla ${\\color{green}\\sf GD}$ when solving the distributed optimization problem $\\min_{x\\in \\mathbb{R}^d} {f(x)=\\frac{1}{n}\\sum_{i=1}^n f_i(x)}$, where $n$ = # of clients, $d$ = # of features, and $f_1,\\dots,f_n$ are smooth nonconvex functions. Despite intensive research since 2014 when ${\\color{green}\\sf EF}$ was first proposed by Seide et al., this problem remained open until now. We show that ${\\color{green}\\sf EF}$ shines in the regime when features are rare, i.e., when each feature is present in the data owned by a small number of clients only. To illustrate our main result, we show that in order to find a random vector $\\hat{x}$ such that $\\lVert {\\nabla f(\\hat{x})} \\rVert^2 \\leq \\varepsilon$ in expectation, ${\\color{green}\\sf",
    "path": "papers/23/05/2305.15264.json",
    "total_tokens": 784,
    "translated_title": "当特征稀少时，误差反馈更为出色",
    "translated_abstract": "本文研究了分布式优化问题中，当特征稀少时，利用梯度下降（GD）与贪心的稀疏算法（TopK），结合误差反馈（EF）可以比纯GD更好地解决问题的情况。我们证明了EF在特征稀少的情况下更为出色。本文首次证明了这一问题。我们还阐述了寻找使梯度的二范数的期望小于一个给定值$\\varepsilon$ 的随机向量的实例。",
    "tldr": "本文证明了误差反馈算法在分布式优化问题中当特征稀少时更为出色。",
    "en_tdlr": "This paper proves that error feedback algorithm is better than vanilla GD in solving distributed optimization problems with rare features."
}