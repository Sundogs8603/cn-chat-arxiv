{
    "title": "Neural Exploitation and Exploration of Contextual Bandits. (arXiv:2305.03784v1 [cs.LG])",
    "abstract": "In this paper, we study utilizing neural networks for the exploitation and exploration of contextual multi-armed bandits. Contextual multi-armed bandits have been studied for decades with various applications. To solve the exploitation-exploration trade-off in bandits, there are three main techniques: epsilon-greedy, Thompson Sampling (TS), and Upper Confidence Bound (UCB). In recent literature, a series of neural bandit algorithms have been proposed to adapt to the non-linear reward function, combined with TS or UCB strategies for exploration. In this paper, instead of calculating a large-deviation based statistical bound for exploration like previous methods, we propose, ``EE-Net,'' a novel neural-based exploitation and exploration strategy. In addition to using a neural network (Exploitation network) to learn the reward function, EE-Net uses another neural network (Exploration network) to adaptively learn the potential gains compared to the currently estimated reward for exploration",
    "link": "http://arxiv.org/abs/2305.03784",
    "context": "Title: Neural Exploitation and Exploration of Contextual Bandits. (arXiv:2305.03784v1 [cs.LG])\nAbstract: In this paper, we study utilizing neural networks for the exploitation and exploration of contextual multi-armed bandits. Contextual multi-armed bandits have been studied for decades with various applications. To solve the exploitation-exploration trade-off in bandits, there are three main techniques: epsilon-greedy, Thompson Sampling (TS), and Upper Confidence Bound (UCB). In recent literature, a series of neural bandit algorithms have been proposed to adapt to the non-linear reward function, combined with TS or UCB strategies for exploration. In this paper, instead of calculating a large-deviation based statistical bound for exploration like previous methods, we propose, ``EE-Net,'' a novel neural-based exploitation and exploration strategy. In addition to using a neural network (Exploitation network) to learn the reward function, EE-Net uses another neural network (Exploration network) to adaptively learn the potential gains compared to the currently estimated reward for exploration",
    "path": "papers/23/05/2305.03784.json",
    "total_tokens": 729,
    "translated_title": "多臂赌博机的上下文利用与探索的神经网络研究",
    "translated_abstract": "本文研究利用神经网络进行上下文多臂赌博机的利用和探索。我们提出了一个名为\"EE-Net\"的新型神经网络利用和探索策略，它使用一个神经网络（利用网络）来学习奖励函数，另一个神经网络（探索网络）来适应性地学习相对于当前估计奖励的潜在收益。",
    "tldr": "本文提出了一种新型的神经网络策略\"EE-Net\"，它用于多臂赌博机的利用和探索，在学习奖励函数的同时也适应性地学习潜在收益。",
    "en_tdlr": "This paper proposes a novel neural-based strategy, named \"EE-Net,\" for exploitation and exploration of contextual multi-armed bandits. It utilizes two neural networks to learn the reward function and potential gains adaptively, respectively."
}