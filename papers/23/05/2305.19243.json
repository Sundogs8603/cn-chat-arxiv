{
    "title": "Auto-tune: PAC-Bayes Optimization over Prior and Posterior for Neural Networks. (arXiv:2305.19243v1 [stat.ML])",
    "abstract": "It is widely recognized that the generalization ability of neural networks can be greatly enhanced through carefully designing the training procedure. The current state-of-the-art training approach involves utilizing stochastic gradient descent (SGD) or Adam optimization algorithms along with a combination of additional regularization techniques such as weight decay, dropout, or noise injection. Optimal generalization can only be achieved by tuning a multitude of hyperparameters through grid search, which can be time-consuming and necessitates additional validation datasets. To address this issue, we introduce a practical PAC-Bayes training framework that is nearly tuning-free and requires no additional regularization while achieving comparable testing performance to that of SGD/Adam after a complete grid search and with extra regularizations. Our proposed algorithm demonstrates the remarkable potential of PAC training to achieve state-of-the-art performance on deep neural networks wit",
    "link": "http://arxiv.org/abs/2305.19243",
    "context": "Title: Auto-tune: PAC-Bayes Optimization over Prior and Posterior for Neural Networks. (arXiv:2305.19243v1 [stat.ML])\nAbstract: It is widely recognized that the generalization ability of neural networks can be greatly enhanced through carefully designing the training procedure. The current state-of-the-art training approach involves utilizing stochastic gradient descent (SGD) or Adam optimization algorithms along with a combination of additional regularization techniques such as weight decay, dropout, or noise injection. Optimal generalization can only be achieved by tuning a multitude of hyperparameters through grid search, which can be time-consuming and necessitates additional validation datasets. To address this issue, we introduce a practical PAC-Bayes training framework that is nearly tuning-free and requires no additional regularization while achieving comparable testing performance to that of SGD/Adam after a complete grid search and with extra regularizations. Our proposed algorithm demonstrates the remarkable potential of PAC training to achieve state-of-the-art performance on deep neural networks wit",
    "path": "papers/23/05/2305.19243.json",
    "total_tokens": 841,
    "translated_title": "Auto-tune: 神经网络的先验与后验PAC-Bayes优化",
    "translated_abstract": "通过精心设计训练过程，可以显著提高神经网络的泛化能力。目前最先进的训练方法涉及使用随机梯度下降或Adam优化算法，以及额外的正则化技术，如权重衰减、Dropout或噪声注入。通过网格搜索调整数量众多的超参数才能达到最优泛化，这可能耗时，并需要额外的验证数据集。为解决这个问题，我们引入了一个切实可行的PAC-Bayes训练框架，几乎是无需调整，也不需要额外的正则化，而在完成网格搜索和加入额外正则化后，达到了与SGD/Adam可比较的测试性能。我们提出的算法展示了PAC训练在深度神经网络上实现最先进性能的显著潜力。",
    "tldr": "通过提出一种PAC-Bayes训练框架，无需额外正则化和网格搜索调整超参数即可达到与传统方法相媲美的测试性能，显著提高神经网络泛化能力并具有实际应用价值。",
    "en_tdlr": "By proposing a PAC-Bayes training framework, which requires no additional regularization or grid search for tuning hyperparameters, our algorithm achieves comparable testing performance to traditional methods, significantly enhancing the generalization ability of neural networks and providing practical value."
}