{
    "title": "Self-Reinforcement Attention Mechanism For Tabular Learning. (arXiv:2305.11684v1 [cs.LG])",
    "abstract": "Apart from the high accuracy of machine learning models, what interests many researchers in real-life problems (e.g., fraud detection, credit scoring) is to find hidden patterns in data; particularly when dealing with their challenging imbalanced characteristics. Interpretability is also a key requirement that needs to accompany the used machine learning model. In this concern, often, intrinsically interpretable models are preferred to complex ones, which are in most cases black-box models. Also, linear models are used in some high-risk fields to handle tabular data, even if performance must be sacrificed. In this paper, we introduce Self-Reinforcement Attention (SRA), a novel attention mechanism that provides a relevance of features as a weight vector which is used to learn an intelligible representation. This weight is then used to reinforce or reduce some components of the raw input through element-wise vector multiplication. Our results on synthetic and real-world imbalanced data s",
    "link": "http://arxiv.org/abs/2305.11684",
    "context": "Title: Self-Reinforcement Attention Mechanism For Tabular Learning. (arXiv:2305.11684v1 [cs.LG])\nAbstract: Apart from the high accuracy of machine learning models, what interests many researchers in real-life problems (e.g., fraud detection, credit scoring) is to find hidden patterns in data; particularly when dealing with their challenging imbalanced characteristics. Interpretability is also a key requirement that needs to accompany the used machine learning model. In this concern, often, intrinsically interpretable models are preferred to complex ones, which are in most cases black-box models. Also, linear models are used in some high-risk fields to handle tabular data, even if performance must be sacrificed. In this paper, we introduce Self-Reinforcement Attention (SRA), a novel attention mechanism that provides a relevance of features as a weight vector which is used to learn an intelligible representation. This weight is then used to reinforce or reduce some components of the raw input through element-wise vector multiplication. Our results on synthetic and real-world imbalanced data s",
    "path": "papers/23/05/2305.11684.json",
    "total_tokens": 914,
    "translated_title": "自我强化注意机制用于表格学习",
    "translated_abstract": "除了机器学习模型的高精度外，许多研究者在处理现实中的问题（如欺诈检测、信用评分）时，更关注于发现数据中的隐藏模式，特别是当面对具有挑战性的不平衡特征时。解释能力也是一个关键要求，必须伴随使用的机器学习模型。在这方面，通常会优先选择本质上具有可解释性的模型，而不是那些在大多数情况下都是黑匣子模型的复杂模型。即使需要牺牲性能，一些高风险领域也使用线性模型来处理表格数据。本文介绍了自我强化注意机制（Self-Reinforcement Attention, SRA），它提供了一种将相关性转化为特征权重向量的新颖注意机制，该权重向量用于学习可理解的表达。然后，该权重用于通过逐元素向量乘法来增强或减少原始输入的某些组件。我们在合成和真实的不平衡数据集上得到了实验结果。",
    "tldr": "本文提出了自我强化注意机制（SRA）用于处理表格学习，该机制通过逐元素向量乘法以加强或减弱原始输入的某些组件来学习可理解的特征表示。",
    "en_tdlr": "This paper proposes a Self-Reinforcement Attention (SRA) mechanism for tabular learning, which learns an interpretable feature representation by providing relevance of features as a weight vector and reinforcing or reducing some components of the raw input through element-wise vector multiplication."
}