{
    "title": "Interpretation of Time-Series Deep Models: A Survey. (arXiv:2305.14582v1 [cs.LG])",
    "abstract": "Deep learning models developed for time-series associated tasks have become more widely researched nowadays. However, due to the unintuitive nature of time-series data, the interpretability problem -- where we understand what is under the hood of these models -- becomes crucial. The advancement of similar studies in computer vision has given rise to many post-hoc methods, which can also shed light on how to explain time-series models. In this paper, we present a wide range of post-hoc interpretation methods for time-series models based on backpropagation, perturbation, and approximation. We also want to bring focus onto inherently interpretable models, a novel category of interpretation where human-understandable information is designed within the models. Furthermore, we introduce some common evaluation metrics used for the explanations, and propose several directions of future researches on the time-series interpretability problem. As a highlight, our work summarizes not only the well",
    "link": "http://arxiv.org/abs/2305.14582",
    "context": "Title: Interpretation of Time-Series Deep Models: A Survey. (arXiv:2305.14582v1 [cs.LG])\nAbstract: Deep learning models developed for time-series associated tasks have become more widely researched nowadays. However, due to the unintuitive nature of time-series data, the interpretability problem -- where we understand what is under the hood of these models -- becomes crucial. The advancement of similar studies in computer vision has given rise to many post-hoc methods, which can also shed light on how to explain time-series models. In this paper, we present a wide range of post-hoc interpretation methods for time-series models based on backpropagation, perturbation, and approximation. We also want to bring focus onto inherently interpretable models, a novel category of interpretation where human-understandable information is designed within the models. Furthermore, we introduce some common evaluation metrics used for the explanations, and propose several directions of future researches on the time-series interpretability problem. As a highlight, our work summarizes not only the well",
    "path": "papers/23/05/2305.14582.json",
    "total_tokens": 939,
    "translated_title": "时间序列深度模型的解释：一项调查",
    "translated_abstract": "近年来，针对时间序列任务开发的深度学习模型受到了广泛的研究。然而，由于时间序列数据的不直观性，解释性问题——我们如何理解这些模型的内部原理——变得至关重要。类似于计算机视觉领域的类似研究推动了许多事后方法的发展，这些方法也可以阐明如何解释时间序列模型。本文介绍了一系列基于反向传播、扰动、逼近的时间序列模型事后解释方法。我们还想着重介绍内在可解释模型，这是一种将可人理解的信息设计到模型中的新颖解释类别。此外，我们介绍了用于解释的常见评估指标，并提出了关于时间序列可解释性问题未来研究的几个方向。值得一提的是，我们的工作不仅总结了解释时间序列深度模型的众所周知的事后方法，还凸显了发展内在可解释模型的新兴趋势。",
    "tldr": "本文调查了解释时间序列深度模型的事后方法，并介绍了一种新颖的解释类别 - 内在可解释模型。此外，本文介绍了常见的解释评估指标，提出了时间序列可解释性问题的未来研究方向。",
    "en_tdlr": "This survey discusses post-hoc methods for interpreting time-series deep models and introduces a novel category of interpretation - inherently interpretable models. Additionally, it presents common evaluation metrics for explanations and proposes future research directions on the time-series interpretability problem."
}