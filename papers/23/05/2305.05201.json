{
    "title": "Exploration of Language Dependency for Japanese Self-Supervised Speech Representation Models. (arXiv:2305.05201v1 [cs.CL])",
    "abstract": "Self-supervised learning (SSL) has been dramatically successful not only in monolingual but also in cross-lingual settings. However, since the two settings have been studied individually in general, there has been little research focusing on how effective a cross-lingual model is in comparison with a monolingual model. In this paper, we investigate this fundamental question empirically with Japanese automatic speech recognition (ASR) tasks. First, we begin by comparing the ASR performance of cross-lingual and monolingual models for two different language tasks while keeping the acoustic domain as identical as possible. Then, we examine how much unlabeled data collected in Japanese is needed to achieve performance comparable to a cross-lingual model pre-trained with tens of thousands of hours of English and/or multilingual data. Finally, we extensively investigate the effectiveness of SSL in Japanese and demonstrate state-of-the-art performance on multiple ASR tasks. Since there is no c",
    "link": "http://arxiv.org/abs/2305.05201",
    "context": "Title: Exploration of Language Dependency for Japanese Self-Supervised Speech Representation Models. (arXiv:2305.05201v1 [cs.CL])\nAbstract: Self-supervised learning (SSL) has been dramatically successful not only in monolingual but also in cross-lingual settings. However, since the two settings have been studied individually in general, there has been little research focusing on how effective a cross-lingual model is in comparison with a monolingual model. In this paper, we investigate this fundamental question empirically with Japanese automatic speech recognition (ASR) tasks. First, we begin by comparing the ASR performance of cross-lingual and monolingual models for two different language tasks while keeping the acoustic domain as identical as possible. Then, we examine how much unlabeled data collected in Japanese is needed to achieve performance comparable to a cross-lingual model pre-trained with tens of thousands of hours of English and/or multilingual data. Finally, we extensively investigate the effectiveness of SSL in Japanese and demonstrate state-of-the-art performance on multiple ASR tasks. Since there is no c",
    "path": "papers/23/05/2305.05201.json",
    "total_tokens": 928,
    "translated_title": "探究语言依赖性在日语自监督语音表示模型中的应用",
    "translated_abstract": "自监督学习在单语言和跨语言环境中都取得了巨大的成功。然而，由于这两种情况通常是分开研究的，对于跨语言模型和单语言模型的比较效果的研究却很少。在本文中，我们通过日语自动语音识别任务的实证研究来探讨这个基本问题。首先，在尽可能保持声学域相同的情况下，比较跨语言和单语言模型在两种不同语言任务的ASR性能。然后，我们研究日语中收集到的多少未标注数据可以达到与预先通过数万小时的英语和/或多语言数据预训练的跨语言模型相当的性能。最后，我们广泛研究了自监督学习在日语中的有效性，并在多个ASR任务中展示了最先进的性能。",
    "tldr": "本文研究了跨语言和单语言模型在日语自动语音识别任务中的表现，研究了日语中未标注数据相对于跨语言预先训练模型的需求，同时研究了自监督学习在日语中的有效性，并展示了最先进的性能。"
}