{
    "title": "IDEA: Invariant Causal Defense for Graph Adversarial Robustness. (arXiv:2305.15792v1 [cs.LG])",
    "abstract": "Graph neural networks (GNNs) have achieved remarkable success in various tasks, however, their vulnerability to adversarial attacks raises concerns for the real-world applications. Existing defense methods can resist some attacks, but suffer unbearable performance degradation under other unknown attacks. This is due to their reliance on either limited observed adversarial examples to optimize (adversarial training) or specific heuristics to alter graph or model structures (graph purification or robust aggregation). In this paper, we propose an Invariant causal DEfense method against adversarial Attacks (IDEA), providing a new perspective to address this issue. The method aims to learn causal features that possess strong predictability for labels and invariant predictability across attacks, to achieve graph adversarial robustness. Through modeling and analyzing the causal relationships in graph adversarial attacks, we design two invariance objectives to learn the causal features. Extens",
    "link": "http://arxiv.org/abs/2305.15792",
    "context": "Title: IDEA: Invariant Causal Defense for Graph Adversarial Robustness. (arXiv:2305.15792v1 [cs.LG])\nAbstract: Graph neural networks (GNNs) have achieved remarkable success in various tasks, however, their vulnerability to adversarial attacks raises concerns for the real-world applications. Existing defense methods can resist some attacks, but suffer unbearable performance degradation under other unknown attacks. This is due to their reliance on either limited observed adversarial examples to optimize (adversarial training) or specific heuristics to alter graph or model structures (graph purification or robust aggregation). In this paper, we propose an Invariant causal DEfense method against adversarial Attacks (IDEA), providing a new perspective to address this issue. The method aims to learn causal features that possess strong predictability for labels and invariant predictability across attacks, to achieve graph adversarial robustness. Through modeling and analyzing the causal relationships in graph adversarial attacks, we design two invariance objectives to learn the causal features. Extens",
    "path": "papers/23/05/2305.15792.json",
    "total_tokens": 975,
    "translated_title": "IDEA：图形对抗鲁棒性的不变因果防御",
    "translated_abstract": "图神经网络在各种任务中取得了显著的成功，然而，它们对于对抗性攻击的脆弱性引起了真实世界应用的关注。现有的防御方法可以抵抗一些攻击，但在其他未知攻击下会遭受难以承受的性能下降。这是由于它们依赖于有限的观察到的对抗性示例来进行优化（对抗性训练）或特定启发式来改变图形或模型结构（图纯化或鲁棒聚合）。本文提出了一种不变因果防御方法来对抗对抗性攻击（IDEA），为解决这个问题提供了一个新的视角。该方法旨在学习具有强预测标签的因果特征，并且跨攻击具有不变的预测能力，以实现图形对抗鲁棒性。通过对图形对抗攻击中的因果关系进行建模和分析，我们设计了两个不变性目标来学习因果特征。在各种数据集上进行了广泛的模拟实验和综合比较，证明了IDEA的有效性和泛化性。",
    "tldr": "IDEA提出了一种通过学习具有强预测性和跨攻击不变性的因果特征来实现图形对抗鲁棒性的不变因果防御方法，相对于其他方法具有更高的效果和泛化性。",
    "en_tdlr": "IDEA proposes an invariant causal defense method for achieving graph adversarial robustness by learning causal features with strong predictability for labels and invariant predictability across attacks, which outperforms other methods with higher effectiveness and generalization."
}