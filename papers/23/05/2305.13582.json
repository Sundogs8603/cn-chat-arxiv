{
    "title": "Better Low-Resource Entity Recognition Through Translation and Annotation Fusion. (arXiv:2305.13582v1 [cs.CL])",
    "abstract": "Pre-trained multilingual language models have enabled significant advancements in cross-lingual transfer. However, these models often exhibit a performance disparity when transferring from high-resource languages to low-resource languages, especially for languages that are underrepresented or not in the pre-training data. Motivated by the superior performance of these models on high-resource languages compared to low-resource languages, we introduce a Translation-and-fusion framework, which translates low-resource language text into a high-resource language for annotation using fully supervised models before fusing the annotations back into the low-resource language. Based on this framework, we present TransFusion, a model trained to fuse predictions from a high-resource language to make robust predictions on low-resource languages. We evaluate our methods on two low-resource named entity recognition (NER) datasets, MasakhaNER2.0 and LORELEI NER, covering 25 languages, and show consist",
    "link": "http://arxiv.org/abs/2305.13582",
    "context": "Title: Better Low-Resource Entity Recognition Through Translation and Annotation Fusion. (arXiv:2305.13582v1 [cs.CL])\nAbstract: Pre-trained multilingual language models have enabled significant advancements in cross-lingual transfer. However, these models often exhibit a performance disparity when transferring from high-resource languages to low-resource languages, especially for languages that are underrepresented or not in the pre-training data. Motivated by the superior performance of these models on high-resource languages compared to low-resource languages, we introduce a Translation-and-fusion framework, which translates low-resource language text into a high-resource language for annotation using fully supervised models before fusing the annotations back into the low-resource language. Based on this framework, we present TransFusion, a model trained to fuse predictions from a high-resource language to make robust predictions on low-resource languages. We evaluate our methods on two low-resource named entity recognition (NER) datasets, MasakhaNER2.0 and LORELEI NER, covering 25 languages, and show consist",
    "path": "papers/23/05/2305.13582.json",
    "total_tokens": 976,
    "translated_title": "通过翻译和注释融合改进低资源实体识别",
    "translated_abstract": "预训练的多语言语言模型已经在跨语言转移方面实现了重大进展。然而，这些模型在从高资源语言转移至低资源语言时，通常表现出性能差异，特别是对于未被充分训练或未包含在预训练数据中的语言。受这些模型在高资源语言上表现优秀的启发，我们介绍了一个翻译和融合框架，该框架将低资源语言文本翻译成高资源语言进行注释，然后将注释融合回低资源语言。基于该框架，我们提出了TransFusion模型，该模型训练用于融合来自高资源语言的预测结果，以在低资源语言上进行强大的预测。我们在两个低资源命名实体识别（NER）数据集MasakhaNER2.0和LORELEI NER上评估了我们的方法，并展示了与最先进的跨语言NER基线的一致优秀性能。",
    "tldr": "本研究提出了一种通过翻译和注释融合的框架，可以改进低资源语言文本的命名实体识别。通过TransFusion模型，可以在不同语言之间进行强大的预测，且在两个低资源命名实体识别数据集上表现一致优秀。",
    "en_tdlr": "This paper proposes a Translation-and-fusion framework for low-resource entity recognition, which translates the low-resource language text into high-resource language for annotation and fuses the annotations back. The TransFusion model is presented to make robust predictions on languages with fully supervised models. The methods were evaluated on two low-resource NER datasets with consistent superior performance against state-of-the-art cross-lingual NER baselines."
}