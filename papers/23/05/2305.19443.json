{
    "title": "OWAdapt: An adaptive loss function for deep learning using OWA operators. (arXiv:2305.19443v1 [cs.LG])",
    "abstract": "In this paper, we propose a fuzzy adaptive loss function for enhancing deep learning performance in classification tasks. Specifically, we redefine the cross-entropy loss to effectively address class-level noise conditions, including the challenging problem of class imbalance. Our approach introduces aggregation operators, leveraging the power of fuzzy logic to improve classification accuracy. The rationale behind our proposed method lies in the iterative up-weighting of class-level components within the loss function, focusing on those with larger errors. To achieve this, we employ the ordered weighted average (OWA) operator and combine it with an adaptive scheme for gradient-based learning. Through extensive experimentation, our method outperforms other commonly used loss functions, such as the standard cross-entropy or focal loss, across various binary and multiclass classification tasks. Furthermore, we explore the influence of hyperparameters associated with the OWA operators and ",
    "link": "http://arxiv.org/abs/2305.19443",
    "context": "Title: OWAdapt: An adaptive loss function for deep learning using OWA operators. (arXiv:2305.19443v1 [cs.LG])\nAbstract: In this paper, we propose a fuzzy adaptive loss function for enhancing deep learning performance in classification tasks. Specifically, we redefine the cross-entropy loss to effectively address class-level noise conditions, including the challenging problem of class imbalance. Our approach introduces aggregation operators, leveraging the power of fuzzy logic to improve classification accuracy. The rationale behind our proposed method lies in the iterative up-weighting of class-level components within the loss function, focusing on those with larger errors. To achieve this, we employ the ordered weighted average (OWA) operator and combine it with an adaptive scheme for gradient-based learning. Through extensive experimentation, our method outperforms other commonly used loss functions, such as the standard cross-entropy or focal loss, across various binary and multiclass classification tasks. Furthermore, we explore the influence of hyperparameters associated with the OWA operators and ",
    "path": "papers/23/05/2305.19443.json",
    "total_tokens": 902,
    "translated_title": "OWAdapt：使用OWA算子的深度学习自适应损失函数",
    "translated_abstract": "本文提出了一种模糊自适应损失函数，用于提升分类任务中深度学习的性能。具体而言，我们重新定义了交叉熵损失，以有效应对类别级别噪声条件，包括类别不平衡这一难题。我们的方法引入了聚合算子，利用模糊逻辑的能力提高了分类精度。我们提出的方法的理论基础在于损失函数内类别级别组件的迭代加权，重点关注那些存在较大误差的组件。为此，我们采用了有序加权平均（OWA）算子，并将其与基于梯度的学习的自适应方案结合在一起。通过大量实验，我们的方法在各种二元和多元分类任务中优于其他常用的损失函数，如标准交叉熵或聚焦损失。此外，我们还探讨了与OWA算子相关的超参数的影响。",
    "tldr": "本文提出了一种基于OWA算子的模糊自适应损失函数，通过迭代加权策略应对类别级别噪声条件，提高深度学习在分类任务中的性能。实验证明该方法在各种分类任务中均优于传统的常用损失函数。",
    "en_tdlr": "This paper proposes a fuzzy adaptive loss function using OWA operators, which iteratively up-weights class-level components within the loss function to address class-level noise conditions. The approach improves deep learning performance in classification tasks and outperforms other commonly used loss functions in various binary and multiclass classification tasks."
}