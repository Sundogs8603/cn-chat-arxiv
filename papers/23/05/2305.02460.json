{
    "title": "Tensorizing flows: a tool for variational inference. (arXiv:2305.02460v1 [cs.LG])",
    "abstract": "Fueled by the expressive power of deep neural networks, normalizing flows have achieved spectacular success in generative modeling, or learning to draw new samples from a distribution given a finite dataset of training samples. Normalizing flows have also been applied successfully to variational inference, wherein one attempts to learn a sampler based on an expression for the log-likelihood or energy function of the distribution, rather than on data. In variational inference, the unimodality of the reference Gaussian distribution used within the normalizing flow can cause difficulties in learning multimodal distributions. We introduce an extension of normalizing flows in which the Gaussian reference is replaced with a reference distribution that is constructed via a tensor network, specifically a matrix product state or tensor train. We show that by combining flows with tensor networks on difficult variational inference tasks, we can improve on the results obtained by using either tool",
    "link": "http://arxiv.org/abs/2305.02460",
    "context": "Title: Tensorizing flows: a tool for variational inference. (arXiv:2305.02460v1 [cs.LG])\nAbstract: Fueled by the expressive power of deep neural networks, normalizing flows have achieved spectacular success in generative modeling, or learning to draw new samples from a distribution given a finite dataset of training samples. Normalizing flows have also been applied successfully to variational inference, wherein one attempts to learn a sampler based on an expression for the log-likelihood or energy function of the distribution, rather than on data. In variational inference, the unimodality of the reference Gaussian distribution used within the normalizing flow can cause difficulties in learning multimodal distributions. We introduce an extension of normalizing flows in which the Gaussian reference is replaced with a reference distribution that is constructed via a tensor network, specifically a matrix product state or tensor train. We show that by combining flows with tensor networks on difficult variational inference tasks, we can improve on the results obtained by using either tool",
    "path": "papers/23/05/2305.02460.json",
    "total_tokens": 795,
    "translated_title": "张量流：一种变分推断工具。",
    "translated_abstract": "基于深度神经网络的表达能力，标准化流在生成建模中取得了巨大的成功。标准化流还成功地应用于变分推断，其中我们尝试基于分布的对数似然或能量函数学习采样器，而不是基于数据。在变分推断中，正态流中使用的参考高斯分布的单峰性可能导致学习多峰分布时出现困难。我们引入了一个标准流的扩展，在这个扩展中，高斯参考被一个通过张量网络（特别是矩阵积态或张量列车）构造的参考分布所取代。我们展示了通过在困难的变分推断任务中结合流和张量网络，我们可以改善使用任一工具所获得的结果。",
    "tldr": "张量流是一种扩展标准化流的工具，可以通过结合流与张量网络来改善在学习多峰分布的困难变分推断任务中的结果。",
    "en_tdlr": "Tensorizing flows is a tool for extending normalizing flows in variational inference by replacing the unimodal Gaussian reference with a reference distribution constructed via a tensor network. It has shown promise in improving results on difficult variational inference tasks involving learning multimodal distributions."
}