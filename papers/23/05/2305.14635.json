{
    "title": "CMOT: Cross-modal Mixup via Optimal Transport for Speech Translation. (arXiv:2305.14635v1 [cs.CL])",
    "abstract": "End-to-end speech translation (ST) is the task of translating speech signals in the source language into text in the target language. As a cross-modal task, end-to-end ST is difficult to train with limited data. Existing methods often try to transfer knowledge from machine translation (MT), but their performances are restricted by the modality gap between speech and text. In this paper, we propose Cross-modal Mixup via Optimal Transport CMOT to overcome the modality gap. We find the alignment between speech and text sequences via optimal transport and then mix up the sequences from different modalities at a token level using the alignment. Experiments on the MuST-C ST benchmark demonstrate that CMOT achieves an average BLEU of 30.0 in 8 translation directions, outperforming previous methods. Further analysis shows CMOT can adaptively find the alignment between modalities, which helps alleviate the modality gap between speech and text. Code is publicly available at https://github.com/ic",
    "link": "http://arxiv.org/abs/2305.14635",
    "context": "Title: CMOT: Cross-modal Mixup via Optimal Transport for Speech Translation. (arXiv:2305.14635v1 [cs.CL])\nAbstract: End-to-end speech translation (ST) is the task of translating speech signals in the source language into text in the target language. As a cross-modal task, end-to-end ST is difficult to train with limited data. Existing methods often try to transfer knowledge from machine translation (MT), but their performances are restricted by the modality gap between speech and text. In this paper, we propose Cross-modal Mixup via Optimal Transport CMOT to overcome the modality gap. We find the alignment between speech and text sequences via optimal transport and then mix up the sequences from different modalities at a token level using the alignment. Experiments on the MuST-C ST benchmark demonstrate that CMOT achieves an average BLEU of 30.0 in 8 translation directions, outperforming previous methods. Further analysis shows CMOT can adaptively find the alignment between modalities, which helps alleviate the modality gap between speech and text. Code is publicly available at https://github.com/ic",
    "path": "papers/23/05/2305.14635.json",
    "total_tokens": 920,
    "translated_title": "CMOT: 通过最优传输进行跨模态Mixup，用于语音翻译",
    "translated_abstract": "端到端语音翻译（ST）是将源语言中的语音信号翻译成目标语言文本的任务。作为一项跨模态任务，端到端ST在有限数据下进行训练非常困难。现有的方法通常尝试从机器翻译（MT）中转移知识，但其性能由于语音和文本之间的模态差距受到限制。本文提出了Cross-modal Mixup via Optimal Transport（CMOT）来克服模态差距。我们通过最优传输找到语音和文本序列之间的对齐，然后使用对齐在标记级别上混合来自不同模态的序列。在MuST-C ST基准测试上的实验表明，CMOT在8个翻译方向上实现了平均BLEU值为30.0，超过了以前的方法。进一步的分析表明，CMOT可以自适应地找到模态之间的对齐，有助于缓解语音和文本之间的模态差距。代码公开可用于 https://github.com/ic",
    "tldr": "CMOT是一种用于跨模态语音翻译的方法，通过最优传输找到语音和文本序列之间的对齐，并在标记级别上混合不同模态的序列，实现了在有限数据下更好的性能表现。",
    "en_tdlr": "CMOT is a cross-modal method for speech translation that uses optimal transport to find the alignment between sequences from different modalities and mixes them up at a token level, achieving better performance on limited data."
}