{
    "title": "How to Prompt LLMs for Text-to-SQL: A Study in Zero-shot, Single-domain, and Cross-domain Settings. (arXiv:2305.11853v1 [cs.CL])",
    "abstract": "Large language models (LLMs) with in-context learning have demonstrated remarkable capability in the text-to-SQL task. Previous research has prompted LLMs with various demonstration-retrieval strategies and intermediate reasoning steps to enhance the performance of LLMs. However, those works often employ varied strategies when constructing the prompt text for text-to-SQL inputs, such as databases and demonstration examples. This leads to a lack of comparability in both the prompt constructions and their primary contributions. Furthermore, selecting an effective prompt construction has emerged as a persistent problem for future research. To address this limitation, we comprehensively investigate the impact of prompt constructions across various settings and provide insights for future work.",
    "link": "http://arxiv.org/abs/2305.11853",
    "context": "Title: How to Prompt LLMs for Text-to-SQL: A Study in Zero-shot, Single-domain, and Cross-domain Settings. (arXiv:2305.11853v1 [cs.CL])\nAbstract: Large language models (LLMs) with in-context learning have demonstrated remarkable capability in the text-to-SQL task. Previous research has prompted LLMs with various demonstration-retrieval strategies and intermediate reasoning steps to enhance the performance of LLMs. However, those works often employ varied strategies when constructing the prompt text for text-to-SQL inputs, such as databases and demonstration examples. This leads to a lack of comparability in both the prompt constructions and their primary contributions. Furthermore, selecting an effective prompt construction has emerged as a persistent problem for future research. To address this limitation, we comprehensively investigate the impact of prompt constructions across various settings and provide insights for future work.",
    "path": "papers/23/05/2305.11853.json",
    "total_tokens": 757,
    "translated_title": "如何引导LLMs进行文本到SQL的学习: 从零样本到单领域到跨领域研究",
    "translated_abstract": "具有上下文学习的大型语言模型(LLMs)在文本到SQL任务中展现了显著能力。之前的研究通过各种演示-检索策略和中间推理步骤来促使LLMs性能的提升。然而，这些工作在构建文本到SQL输入的提示文本(如数据库和演示示例)时常采用不同的策略。这导致提示文本的构建和其主要贡献的可比性不足。此外，选择有效的提示文本建设已成为未来研究中的持久问题。为了解决这个限制，我们全面调查了不同设置下提示文本结构的影响，并为未来的工作提供了见解。",
    "tldr": "本文针对引导LLMs进行文本到SQL的任务中提示文本构建问题展开了综合探究，从而为未来的研究提供了见解。"
}