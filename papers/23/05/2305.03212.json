{
    "title": "LLM2Loss: Leveraging Language Models for Explainable Model Diagnostics. (arXiv:2305.03212v1 [cs.CV])",
    "abstract": "Trained on a vast amount of data, Large Language models (LLMs) have achieved unprecedented success and generalization in modeling fairly complex textual inputs in the abstract space, making them powerful tools for zero-shot learning. Such capability is extended to other modalities such as the visual domain using cross-modal foundation models such as CLIP, and as a result, semantically meaningful representation are extractable from visual inputs.  In this work, we leverage this capability and propose an approach that can provide semantic insights into a model's patterns of failures and biases. Given a black box model, its training data, and task definition, we first calculate its task-related loss for each data point. We then extract a semantically meaningful representation for each training data point (such as CLIP embeddings from its visual encoder) and train a lightweight diagnosis model which maps this semantically meaningful representation of a data point to its task loss. We show ",
    "link": "http://arxiv.org/abs/2305.03212",
    "context": "Title: LLM2Loss: Leveraging Language Models for Explainable Model Diagnostics. (arXiv:2305.03212v1 [cs.CV])\nAbstract: Trained on a vast amount of data, Large Language models (LLMs) have achieved unprecedented success and generalization in modeling fairly complex textual inputs in the abstract space, making them powerful tools for zero-shot learning. Such capability is extended to other modalities such as the visual domain using cross-modal foundation models such as CLIP, and as a result, semantically meaningful representation are extractable from visual inputs.  In this work, we leverage this capability and propose an approach that can provide semantic insights into a model's patterns of failures and biases. Given a black box model, its training data, and task definition, we first calculate its task-related loss for each data point. We then extract a semantically meaningful representation for each training data point (such as CLIP embeddings from its visual encoder) and train a lightweight diagnosis model which maps this semantically meaningful representation of a data point to its task loss. We show ",
    "path": "papers/23/05/2305.03212.json",
    "total_tokens": 940,
    "translated_title": "LLM2Loss: 利用语言模型进行可解释的模型诊断",
    "translated_abstract": "在大量数据的训练下，大型语言模型已经在抽象空间中对相当复杂的文本输入进行建模，取得了前所未有的成功和概括能力，从而成为了零样本学习的强大工具。这种能力可以通过跨模态基础模型，如CLIP，来扩展到其他模态，如视觉域，从而可以从视觉输入中提取语义上有意义的表示。在本文中，我们利用这种能力，并提出一种方法，可以提供模型失败和偏差模式的语义见解。对于给定的黑盒模型，其训练数据和任务定义，我们首先计算其每个数据点的任务相关损失。然后，我们提取每个训练数据点的语义上有意义的表示（例如来自其视觉编码器的CLIP嵌入），并训练一个轻量级的诊断模型，将这个数据点的语义上有意义的表示映射到其任务损失。我们展示了该方法不仅可以识别对模型难以学习的数据点，而且可以识别导致模型失败和偏差的因素。",
    "tldr": "LLM2Loss利用语言模型为黑盒模型提供可解释的模型诊断，通过提取数据点的语义特征，揭示导致模型失败和偏差的因素。",
    "en_tdlr": "LLM2Loss leverages language models to provide explainable model diagnosis for black box models by extracting semantically meaningful representations of data points, revealing factors that cause model failures and biases."
}