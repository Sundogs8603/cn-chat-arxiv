{
    "title": "Few-Shot Continual Learning for Conditional Generative Adversarial Networks. (arXiv:2305.11400v1 [cs.LG])",
    "abstract": "In few-shot continual learning for generative models, a target mode must be learned with limited samples without adversely affecting the previously learned modes. In this paper, we propose a new continual learning approach for conditional generative adversarial networks (cGAN) based on a new mode-affinity measure for generative modeling. Our measure is entirely based on the cGAN's discriminator and can identify the existing modes that are most similar to the target. Subsequently, we expand the continual learning model by including the target mode using a weighted label derived from those of the closest modes. To prevent catastrophic forgetting, we first generate labeled data samples using the cGAN's generator, and then train the cGAN model for the target mode while memory replaying with the generated data. Our experimental results demonstrate the efficacy of our approach in improving the generation performance over the baselines and the state-of-the-art approaches for various standard ",
    "link": "http://arxiv.org/abs/2305.11400",
    "context": "Title: Few-Shot Continual Learning for Conditional Generative Adversarial Networks. (arXiv:2305.11400v1 [cs.LG])\nAbstract: In few-shot continual learning for generative models, a target mode must be learned with limited samples without adversely affecting the previously learned modes. In this paper, we propose a new continual learning approach for conditional generative adversarial networks (cGAN) based on a new mode-affinity measure for generative modeling. Our measure is entirely based on the cGAN's discriminator and can identify the existing modes that are most similar to the target. Subsequently, we expand the continual learning model by including the target mode using a weighted label derived from those of the closest modes. To prevent catastrophic forgetting, we first generate labeled data samples using the cGAN's generator, and then train the cGAN model for the target mode while memory replaying with the generated data. Our experimental results demonstrate the efficacy of our approach in improving the generation performance over the baselines and the state-of-the-art approaches for various standard ",
    "path": "papers/23/05/2305.11400.json",
    "total_tokens": 965,
    "translated_title": "面向有条件生成对抗网络的少样本连续学习",
    "translated_abstract": "在生成模型的少样本连续学习中，必须学习目标模式，并在不影响先前学习到的模式的情况下仅使用有限的样本。本文针对条件生成对抗网络提出了一种新的连续学习方法，基于一种新的用于生成建模的模式亲和力量度。我们的度量完全基于cGAN的判别器，可以识别最接近目标的现有模式。随后，我们通过包含基于最接近模式的加权标签来扩展连续学习模型。为了预防灾难性遗忘，我们首先使用cGAN的生成器生成带标签的数据样本，然后通过回放生成的数据来训练目标模式的cGAN模型。我们的实验结果证明了我们的方法在提高生成性能方面的有效性，超越了各种标准和最先进的方法。",
    "tldr": "本文提出了一种新的连续学习方法，适用于条件生成对抗网络，根据cGAN的判别器数据识别出最接近目标的现有模式，并通过扩展连续学习模型，使用回放生成的数据来训练目标模式的cGAN模型，以避免灾难性遗忘，提高了生成性能。",
    "en_tdlr": "This paper proposes a new few-shot continual learning approach for conditional generative adversarial networks (cGAN), based on a mode-affinity measure for generative modeling. The method expands the continual learning model by including the target mode using a weighted label derived from those of the closest modes. The approach aims to prevent catastrophic forgetting by generating labeled data samples using the cGAN's generator and then training the cGAN model for the target mode while memory replaying with the generated data. The experimental results show the efficacy of the approach in improving the generation performance over baselines and the state-of-the-art approaches for various standard tasks."
}