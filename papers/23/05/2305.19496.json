{
    "title": "Is Learning in Games Good for the Learners?. (arXiv:2305.19496v1 [cs.GT])",
    "abstract": "We consider a number of questions related to tradeoffs between reward and regret in repeated gameplay between two agents. To facilitate this, we introduce a notion of {\\it generalized equilibrium} which allows for asymmetric regret constraints, and yields polytopes of feasible values for each agent and pair of regret constraints, where we show that any such equilibrium is reachable by a pair of algorithms which maintain their regret guarantees against arbitrary opponents. As a central example, we highlight the case one agent is no-swap and the other's regret is unconstrained. We show that this captures an extension of {\\it Stackelberg} equilibria with a matching optimal value, and that there exists a wide class of games where a player can significantly increase their utility by deviating from a no-swap-regret algorithm against a no-swap learner (in fact, almost any game without pure Nash equilibria is of this form). Additionally, we make use of generalized equilibria to consider tradeo",
    "link": "http://arxiv.org/abs/2305.19496",
    "context": "Title: Is Learning in Games Good for the Learners?. (arXiv:2305.19496v1 [cs.GT])\nAbstract: We consider a number of questions related to tradeoffs between reward and regret in repeated gameplay between two agents. To facilitate this, we introduce a notion of {\\it generalized equilibrium} which allows for asymmetric regret constraints, and yields polytopes of feasible values for each agent and pair of regret constraints, where we show that any such equilibrium is reachable by a pair of algorithms which maintain their regret guarantees against arbitrary opponents. As a central example, we highlight the case one agent is no-swap and the other's regret is unconstrained. We show that this captures an extension of {\\it Stackelberg} equilibria with a matching optimal value, and that there exists a wide class of games where a player can significantly increase their utility by deviating from a no-swap-regret algorithm against a no-swap learner (in fact, almost any game without pure Nash equilibria is of this form). Additionally, we make use of generalized equilibria to consider tradeo",
    "path": "papers/23/05/2305.19496.json",
    "total_tokens": 1200,
    "translated_title": "游戏中的学习是否对学习者有益？",
    "translated_abstract": "我们考虑与奖励和后悔在两个代理之间重复玩游戏相关的一些问题。为了实现这一点，我们引入了广义均衡的概念，该概念允许不对称的后悔约束，并为每个代理和一对后悔约束派生可行值的多面体。作为核心案例，我们强调了一方是禁止交换的，另一方的后悔没有限制。我们证明了这一点，它捕获了与Stackelberg均衡的一种扩展，可匹配最优值，并且存在一大类游戏，在这些游戏中，一名玩家可以通过从禁止交换的后悔算法中偏离，显著增加自己的效用（实际上，几乎所有没有纯纳什均衡的游戏都是这种形式）。",
    "tldr": "我们提出了“广义均衡”的概念，通过学习可以在某些游戏中获得更好的结果。如果没有纯纳什均衡，则一名玩家可以从不同策略中受益，结果捕获了Stackelberg均衡的扩展。"
}