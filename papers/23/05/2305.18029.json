{
    "title": "Faithfulness Tests for Natural Language Explanations. (arXiv:2305.18029v2 [cs.CL] UPDATED)",
    "abstract": "Explanations of neural models aim to reveal a model's decision-making process for its predictions. However, recent work shows that current methods giving explanations such as saliency maps or counterfactuals can be misleading, as they are prone to present reasons that are unfaithful to the model's inner workings. This work explores the challenging question of evaluating the faithfulness of natural language explanations (NLEs). To this end, we present two tests. First, we propose a counterfactual input editor for inserting reasons that lead to counterfactual predictions but are not reflected by the NLEs. Second, we reconstruct inputs from the reasons stated in the generated NLEs and check how often they lead to the same predictions. Our tests can evaluate emerging NLE models, proving a fundamental tool in the development of faithful NLEs.",
    "link": "http://arxiv.org/abs/2305.18029",
    "context": "Title: Faithfulness Tests for Natural Language Explanations. (arXiv:2305.18029v2 [cs.CL] UPDATED)\nAbstract: Explanations of neural models aim to reveal a model's decision-making process for its predictions. However, recent work shows that current methods giving explanations such as saliency maps or counterfactuals can be misleading, as they are prone to present reasons that are unfaithful to the model's inner workings. This work explores the challenging question of evaluating the faithfulness of natural language explanations (NLEs). To this end, we present two tests. First, we propose a counterfactual input editor for inserting reasons that lead to counterfactual predictions but are not reflected by the NLEs. Second, we reconstruct inputs from the reasons stated in the generated NLEs and check how often they lead to the same predictions. Our tests can evaluate emerging NLE models, proving a fundamental tool in the development of faithful NLEs.",
    "path": "papers/23/05/2305.18029.json",
    "total_tokens": 825,
    "translated_title": "自然语言解释的真实性测试",
    "translated_abstract": "神经模型的解释旨在揭示模型预测的决策过程。然而，最近的研究表明，诸如显著性地图或反事实解释等当前的解释方法可能会误导，因为它们容易呈现与模型内部机制不一致的原因。本研究探讨了评估自然语言解释（NLEs）真实性的挑战性问题。为此，我们提出了两个测试。首先，我们提出了一种反事实输入编辑器，用于插入导致反事实预测但不被NLEs反映的原因。其次，我们根据生成的NLEs中所述的原因重建输入，并检查它们导致相同预测的频率。我们的测试可以评估新兴的NLE模型，为开发真实的NLEs提供了基本工具。",
    "tldr": "该论文研究了评估自然语言解释真实性的问题，并提出了两个测试方法：反事实输入编辑器和重建输入测试。这些测试对于评估新兴的NLE模型，对开发真实的NLEs具有重要意义。",
    "en_tdlr": "This paper investigates the challenge of evaluating the faithfulness of natural language explanations and proposes two tests: a counterfactual input editor and a reconstruction input test. These tests are important for evaluating emerging NLE models and developing faithful NLEs."
}