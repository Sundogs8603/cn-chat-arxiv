{
    "title": "Stochastic Gradient Langevin Dynamics Based on Quantization with Increasing Resolution. (arXiv:2305.18864v2 [cs.LG] UPDATED)",
    "abstract": "Stochastic learning dynamics based on Langevin or Levy stochastic differential equations (SDEs) in deep neural networks control the variance of noise by varying the size of the mini-batch or directly those of injecting noise. Since the noise variance affects the approximation performance, the design of the additive noise is significant in SDE-based learning and practical implementation. In this paper, we propose an alternative stochastic descent learning equation based on quantized optimization for non-convex objective functions, adopting a stochastic analysis perspective. The proposed method employs a quantized optimization approach that utilizes Langevin SDE dynamics, allowing for controllable noise with an identical distribution without the need for additive noise or adjusting the mini-batch size. Numerical experiments demonstrate the effectiveness of the proposed algorithm on vanilla convolution neural network(CNN) models and the ResNet-50 architecture across various data sets. Fur",
    "link": "http://arxiv.org/abs/2305.18864",
    "context": "Title: Stochastic Gradient Langevin Dynamics Based on Quantization with Increasing Resolution. (arXiv:2305.18864v2 [cs.LG] UPDATED)\nAbstract: Stochastic learning dynamics based on Langevin or Levy stochastic differential equations (SDEs) in deep neural networks control the variance of noise by varying the size of the mini-batch or directly those of injecting noise. Since the noise variance affects the approximation performance, the design of the additive noise is significant in SDE-based learning and practical implementation. In this paper, we propose an alternative stochastic descent learning equation based on quantized optimization for non-convex objective functions, adopting a stochastic analysis perspective. The proposed method employs a quantized optimization approach that utilizes Langevin SDE dynamics, allowing for controllable noise with an identical distribution without the need for additive noise or adjusting the mini-batch size. Numerical experiments demonstrate the effectiveness of the proposed algorithm on vanilla convolution neural network(CNN) models and the ResNet-50 architecture across various data sets. Fur",
    "path": "papers/23/05/2305.18864.json",
    "total_tokens": 933,
    "translated_title": "基于递增分辨率的量化随机梯度 langevin 动力学",
    "translated_abstract": "基于 langevin 或 levy 随机微分方程的随机学习动力学通过改变小批量的大小或直接注入噪声的大小来控制噪声的方差。由于噪声方差会影响近似性能，在基于 SDE 的学习和实际实现中，添加噪声的设计很重要。本文提出了一种基于量化优化的随机下降学习方程，采用随机分析的视角，用于非凸目标函数。所提出的方法采用了一种利用 langevin SDE 动力学的量化优化方法，可以实现具有相同分布的可控噪声，而无需添加噪声或调整小批量的大小。数值实验证明了所提出算法在各种数据集上对 vanilla 卷积神经网络（CNN）模型和 ResNet-50 架构的有效性。",
    "tldr": "本文提出了一种基于递增分辨率的量化随机梯度 langevin 动力学方法，通过利用 langevin 随机微分方程动力学，实现了具有可控噪声且具有相同分布的优化过程，无需添加噪声或调整小批量的大小。实验结果证明了该方法在不同数据集上对卷积神经网络模型和 ResNet-50 架构的有效性。",
    "en_tdlr": "This paper proposes a stochastic gradient Langevin dynamics based on quantization with increasing resolution, which utilizes Langevin SDE dynamics for optimization and achieves controllable noise with an identical distribution without the need for additive noise or adjusting mini-batch size. Experimental results demonstrate the effectiveness of the proposed method on convolutional neural network models and the ResNet-50 architecture across various datasets."
}