{
    "title": "Zero-shot personalized lip-to-speech synthesis with face image based voice control. (arXiv:2305.14359v1 [cs.MM])",
    "abstract": "Lip-to-Speech (Lip2Speech) synthesis, which predicts corresponding speech from talking face images, has witnessed significant progress with various models and training strategies in a series of independent studies. However, existing studies can not achieve voice control under zero-shot condition, because extra speaker embeddings need to be extracted from natural reference speech and are unavailable when only the silent video of an unseen speaker is given. In this paper, we propose a zero-shot personalized Lip2Speech synthesis method, in which face images control speaker identities. A variational autoencoder is adopted to disentangle the speaker identity and linguistic content representations, which enables speaker embeddings to control the voice characteristics of synthetic speech for unseen speakers. Furthermore, we propose associated cross-modal representation learning to promote the ability of face-based speaker embeddings (FSE) on voice control. Extensive experiments verify the eff",
    "link": "http://arxiv.org/abs/2305.14359",
    "context": "Title: Zero-shot personalized lip-to-speech synthesis with face image based voice control. (arXiv:2305.14359v1 [cs.MM])\nAbstract: Lip-to-Speech (Lip2Speech) synthesis, which predicts corresponding speech from talking face images, has witnessed significant progress with various models and training strategies in a series of independent studies. However, existing studies can not achieve voice control under zero-shot condition, because extra speaker embeddings need to be extracted from natural reference speech and are unavailable when only the silent video of an unseen speaker is given. In this paper, we propose a zero-shot personalized Lip2Speech synthesis method, in which face images control speaker identities. A variational autoencoder is adopted to disentangle the speaker identity and linguistic content representations, which enables speaker embeddings to control the voice characteristics of synthetic speech for unseen speakers. Furthermore, we propose associated cross-modal representation learning to promote the ability of face-based speaker embeddings (FSE) on voice control. Extensive experiments verify the eff",
    "path": "papers/23/05/2305.14359.json",
    "total_tokens": 937,
    "translated_title": "利用面部图像控制语音合成的零样本个性化唇语转声方法",
    "translated_abstract": "唇语转语音合成（Lip2Speech）是根据人的口型图像预测相应语音的技术，经过多项独立研究，已经在各种模型和训练策略上取得了显著进展。然而，现有的研究无法在零样本条件下实现语音控制，因为自然参考语音中需要提取额外的说话者嵌入向量，当仅给出未见过的说话者的沉默视频时，这些向量是不可用的。本文提出了一种零样本个性化Lip2Speech合成方法，其中面部图像控制说话者身份。采用变分自编码器来分离说话者身份和语言内容表示，从而使说话者嵌入向量能够控制未见过的说话者合成语音的语音特征。此外，我们提出了相关的跨模态表示学习，以提高基于面部的说话者嵌入向量（FSE）对语音控制的能力。广泛的实验验证了提出方法的有效性。",
    "tldr": "这篇论文提出了一种利用面部图像进行语音合成的方法，可以在没有参考语音时实现零样本个性化，通过分离说话者身份和语言内容，使面部特征可以控制未知说话者特征，扩展了唇语转语音合成的应用场景。",
    "en_tdlr": "This paper proposes a zero-shot personalized Lip2Speech synthesis method, using face images to control speaker identities. By disentangling the speaker identity and linguistic content, the facial features are capable of controlling the voice characteristics of unseen speakers, expanding the application scenario of lip-to-speech synthesis."
}