{
    "title": "XTab: Cross-table Pretraining for Tabular Transformers. (arXiv:2305.06090v1 [cs.LG])",
    "abstract": "The success of self-supervised learning in computer vision and natural language processing has motivated pretraining methods on tabular data. However, most existing tabular self-supervised learning models fail to leverage information across multiple data tables and cannot generalize to new tables. In this work, we introduce XTab, a framework for cross-table pretraining of tabular transformers on datasets from various domains. We address the challenge of inconsistent column types and quantities among tables by utilizing independent featurizers and using federated learning to pretrain the shared component. Tested on 84 tabular prediction tasks from the OpenML-AutoML Benchmark (AMLB), we show that (1) XTab consistently boosts the generalizability, learning speed, and performance of multiple tabular transformers, (2) by pretraining FT-Transformer via XTab, we achieve superior performance than other state-of-the-art tabular deep learning models on various tasks such as regression, binary, a",
    "link": "http://arxiv.org/abs/2305.06090",
    "context": "Title: XTab: Cross-table Pretraining for Tabular Transformers. (arXiv:2305.06090v1 [cs.LG])\nAbstract: The success of self-supervised learning in computer vision and natural language processing has motivated pretraining methods on tabular data. However, most existing tabular self-supervised learning models fail to leverage information across multiple data tables and cannot generalize to new tables. In this work, we introduce XTab, a framework for cross-table pretraining of tabular transformers on datasets from various domains. We address the challenge of inconsistent column types and quantities among tables by utilizing independent featurizers and using federated learning to pretrain the shared component. Tested on 84 tabular prediction tasks from the OpenML-AutoML Benchmark (AMLB), we show that (1) XTab consistently boosts the generalizability, learning speed, and performance of multiple tabular transformers, (2) by pretraining FT-Transformer via XTab, we achieve superior performance than other state-of-the-art tabular deep learning models on various tasks such as regression, binary, a",
    "path": "papers/23/05/2305.06090.json",
    "total_tokens": 848,
    "translated_title": "XTab: 跨表格表格变换器的交叉表预训练",
    "translated_abstract": "自我监督学习在计算机视觉和自然语言处理中的成功促使了对表格数据的预训练方法的开发。然而，大多数现有的表格自我监督学习模型未能利用多个数据表之间的信息，并且无法推广到新表中。本文介绍了XTab，这是一种用于在来自各个领域的数据集上进行交叉表预训练的框架。我们通过利用独立的特征工程师和使用联邦学习来预先训练共享组件，解决了表格之间不一致的列类型和数量的挑战。",
    "tldr": "XTab 是一个用于不同领域的跨表格预训练的框架，它通过使用独立的特征工程师和联邦学习解决表格之间不一致的列类型和数量的挑战。在测试中， XTab 可以提高多个表格变换器的泛化能力、学习速度和性能。"
}