{
    "title": "Specification-Driven Neural Network Reduction for Scalable Formal Verification. (arXiv:2305.01932v1 [cs.LG])",
    "abstract": "Formal verification of neural networks is essential before their deployment in safety-critical settings. However, existing methods for formally verifying neural networks are not yet scalable enough to handle practical problems that involve a large number of neurons. In this work, we propose a novel approach to address this challenge: A conservative neural network reduction approach that ensures that the verification of the reduced network implies the verification of the original network. Our approach constructs the reduction on-the-fly, while simultaneously verifying the original network and its specifications. The reduction merges all neurons of a nonlinear layer with similar outputs and is applicable to neural networks with any type of activation function such as ReLU, sigmoid, and tanh. Our evaluation shows that our approach can reduce a network to less than 5% of the number of neurons and thus to a similar degree the verification time is reduced.",
    "link": "http://arxiv.org/abs/2305.01932",
    "context": "Title: Specification-Driven Neural Network Reduction for Scalable Formal Verification. (arXiv:2305.01932v1 [cs.LG])\nAbstract: Formal verification of neural networks is essential before their deployment in safety-critical settings. However, existing methods for formally verifying neural networks are not yet scalable enough to handle practical problems that involve a large number of neurons. In this work, we propose a novel approach to address this challenge: A conservative neural network reduction approach that ensures that the verification of the reduced network implies the verification of the original network. Our approach constructs the reduction on-the-fly, while simultaneously verifying the original network and its specifications. The reduction merges all neurons of a nonlinear layer with similar outputs and is applicable to neural networks with any type of activation function such as ReLU, sigmoid, and tanh. Our evaluation shows that our approach can reduce a network to less than 5% of the number of neurons and thus to a similar degree the verification time is reduced.",
    "path": "papers/23/05/2305.01932.json",
    "total_tokens": 860,
    "translated_title": "基于规范的神经网络简化方法，用于大规模形式化验证",
    "translated_abstract": "在神经网络在安全关键环境中部署之前，形式验证是必不可少的。然而，现有的神经网络形式验证方法还无法处理涉及大量神经元的实际问题。本文提出了一种新方法来解决这个挑战：保守的神经网络简化方法，确保简化后的网络验证派生出原网络的验证。我们的方法同时构造简化网络，验证原始网络及其规范。简化将所有输出相似的非线性层神经元合并，适用于具有任何类型的激活函数，如ReLU，sigmoid和tanh的神经网络。我们的评估表明，我们的方法可以将网络减少到小于神经元数的5％，因此可以将验证时间相似减少。",
    "tldr": "本文提出了一种基于规范的神经网络简化方法用于大规模形式化验证。该方法采用保守的简化方法，确保简化后的网络验证与原网络验证派生等价。简化后可将网络减少到小于5％的神经元数量，从而减少了相应的验证时间。",
    "en_tdlr": "This paper proposes a specification-driven neural network reduction approach for scalable formal verification, which employs a conservative reduction to ensure that the verification of the reduced network is equivalent to that of the original network. The reduction merges neurons in nonlinear layers with similar outputs and can handle any type of activation function. The evaluation shows that the approach can reduce the network to less than 5% of the number of neurons and significantly reduce the verification time."
}