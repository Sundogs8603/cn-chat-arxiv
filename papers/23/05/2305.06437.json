{
    "title": "Self-Supervised Video Representation Learning via Latent Time Navigation. (arXiv:2305.06437v1 [cs.CV])",
    "abstract": "Self-supervised video representation learning aimed at maximizing similarity between different temporal segments of one video, in order to enforce feature persistence over time. This leads to loss of pertinent information related to temporal relationships, rendering actions such as `enter' and `leave' to be indistinguishable. To mitigate this limitation, we propose Latent Time Navigation (LTN), a time-parameterized contrastive learning strategy that is streamlined to capture fine-grained motions. Specifically, we maximize the representation similarity between different video segments from one video, while maintaining their representations time-aware along a subspace of the latent representation code including an orthogonal basis to represent temporal changes. Our extensive experimental analysis suggests that learning video representations by LTN consistently improves performance of action classification in fine-grained and human-oriented tasks (e.g., on Toyota Smarthome dataset). In ad",
    "link": "http://arxiv.org/abs/2305.06437",
    "context": "Title: Self-Supervised Video Representation Learning via Latent Time Navigation. (arXiv:2305.06437v1 [cs.CV])\nAbstract: Self-supervised video representation learning aimed at maximizing similarity between different temporal segments of one video, in order to enforce feature persistence over time. This leads to loss of pertinent information related to temporal relationships, rendering actions such as `enter' and `leave' to be indistinguishable. To mitigate this limitation, we propose Latent Time Navigation (LTN), a time-parameterized contrastive learning strategy that is streamlined to capture fine-grained motions. Specifically, we maximize the representation similarity between different video segments from one video, while maintaining their representations time-aware along a subspace of the latent representation code including an orthogonal basis to represent temporal changes. Our extensive experimental analysis suggests that learning video representations by LTN consistently improves performance of action classification in fine-grained and human-oriented tasks (e.g., on Toyota Smarthome dataset). In ad",
    "path": "papers/23/05/2305.06437.json",
    "total_tokens": 854,
    "translated_title": "自监督视频表示学习：基于潜在时间导航的方法",
    "translated_abstract": "自监督视频表示学习旨在最大程度地增加同一视频不同时间段之间的相似性，以强制特征在时间上的持续性。然而，这会导致与时间关系有关的重要信息丢失，从而使“进入”和“离开”等动作无法区分。为了缓解这个问题，我们提出了Latent Time Navigation（LTN）的时间参数对比学习策略，该策略可以捕捉到细粒度的动作。具体而言，我们通过在潜在表示代码的子空间内包含一个正交基来表示时间变化，从而最大化来自同一视频的不同视频片段之间的表示相似性，同时保持它们的时间感知。我们的大量实验分析表明，通过LTN学习视频表示可以在细粒度和以人为导向的任务（例如Toyota Smarthome数据集上）中提高动作分类的性能。",
    "tldr": "本文提出了一种自监督视频表示学习的方法，基于潜在时间导航，以捕捉细粒度的动作，大量实验证明该方法提高了动作分类的性能。",
    "en_tdlr": "This paper proposes a self-supervised video representation learning method, Latent Time Navigation (LTN), to capture fine-grained motions. LTN is a time-parameterized contrastive learning strategy that maximizes the representation similarity between different video segments from one video while maintaining their representations time-aware. Experimental results show that LTN improves the performance of action classification."
}