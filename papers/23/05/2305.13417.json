{
    "title": "Interpreting Transformer's Attention Dynamic Memory and Visualizing the Semantic Information Flow of GPT. (arXiv:2305.13417v1 [cs.CL])",
    "abstract": "Recent advances in interpretability suggest we can project weights and hidden states of transformer-based language models (LMs) to their vocabulary, a transformation that makes them human interpretable and enables us to assign semantics to what was seen only as numerical vectors. In this paper, we interpret LM attention heads and memory values, the vectors the models dynamically create and recall while processing a given input. By analyzing the tokens they represent through this projection, we identify patterns in the information flow inside the attention mechanism. Based on these discoveries, we create a tool to visualize a forward pass of Generative Pre-trained Transformers (GPTs) as an interactive flow graph, with nodes representing neurons or hidden states and edges representing the interactions between them. Our visualization simplifies huge amounts of data into easy-to-read plots that reflect why models output their results. We demonstrate the utility of our modeling by identifyi",
    "link": "http://arxiv.org/abs/2305.13417",
    "context": "Title: Interpreting Transformer's Attention Dynamic Memory and Visualizing the Semantic Information Flow of GPT. (arXiv:2305.13417v1 [cs.CL])\nAbstract: Recent advances in interpretability suggest we can project weights and hidden states of transformer-based language models (LMs) to their vocabulary, a transformation that makes them human interpretable and enables us to assign semantics to what was seen only as numerical vectors. In this paper, we interpret LM attention heads and memory values, the vectors the models dynamically create and recall while processing a given input. By analyzing the tokens they represent through this projection, we identify patterns in the information flow inside the attention mechanism. Based on these discoveries, we create a tool to visualize a forward pass of Generative Pre-trained Transformers (GPTs) as an interactive flow graph, with nodes representing neurons or hidden states and edges representing the interactions between them. Our visualization simplifies huge amounts of data into easy-to-read plots that reflect why models output their results. We demonstrate the utility of our modeling by identifyi",
    "path": "papers/23/05/2305.13417.json",
    "total_tokens": 937,
    "translated_title": "解读Transformer的注意力动态内存，可视化GPT的语义信息流",
    "translated_abstract": "最近，解释性方面的进展表明我们可以将基于transformer模型的语言模型的权重和隐藏状态投影到其词汇表中，这种转换使它们变得更容易理解，并且使我们能够将语义分配到仅作为数字向量的内容上。在本文中，我们解释了LM注意力头和内存值，这些向量是模型在处理给定输入时动态地创建和检索的。通过通过这种投影分析它们所代表的标记，我们确定了注意力机制内部信息流的模式。基于这些发现，我们创建了一个工具来将生成预训练Transformer（GPT）的前向传递可视化为交互式流图，其中结点代表神经元或隐藏状态，边代表它们之间的相互作用。我们的可视化将海量数据简化为易于阅读的图表，反映了模型为什么输出其结果的原因。我们通过确定最重要的特征并可视化其语义信息流来演示我们建模的效用。",
    "tldr": "本文提出了一种将Transformer模型的权重和隐藏状态投影到其词汇表中解释模型的方法，并分析了注意力机制内部信息流的模式。文章还介绍了一个可视化工具，将GPT的前向传递可视化为交互式流图，简化了大量数据为易于阅读的图表，展示了其语义信息流。",
    "en_tdlr": "This paper presents a method to interpret the weights and hidden states of Transformer models and analyze the information flow patterns within the attention mechanism through projecting them onto the vocabulary. The paper also introduces a visualization tool that simplifies the huge amounts of data into easy-to-read plots, demonstrating the semantic information flow of GPT through interactive flow graphs."
}