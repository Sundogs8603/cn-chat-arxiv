{
    "title": "CaseEncoder: A Knowledge-enhanced Pre-trained Model for Legal Case Encoding. (arXiv:2305.05393v1 [cs.IR])",
    "abstract": "Legal case retrieval is a critical process for modern legal information systems. While recent studies have utilized pre-trained language models (PLMs) based on the general domain self-supervised pre-training paradigm to build models for legal case retrieval, there are limitations in using general domain PLMs as backbones. Specifically, these models may not fully capture the underlying legal features in legal case documents. To address this issue, we propose CaseEncoder, a legal document encoder that leverages fine-grained legal knowledge in both the data sampling and pre-training phases. In the data sampling phase, we enhance the quality of the training data by utilizing fine-grained law article information to guide the selection of positive and negative examples. In the pre-training phase, we design legal-specific pre-training tasks that align with the judging criteria of relevant legal cases. Based on these tasks, we introduce an innovative loss function called Biased Circle Loss to ",
    "link": "http://arxiv.org/abs/2305.05393",
    "context": "Title: CaseEncoder: A Knowledge-enhanced Pre-trained Model for Legal Case Encoding. (arXiv:2305.05393v1 [cs.IR])\nAbstract: Legal case retrieval is a critical process for modern legal information systems. While recent studies have utilized pre-trained language models (PLMs) based on the general domain self-supervised pre-training paradigm to build models for legal case retrieval, there are limitations in using general domain PLMs as backbones. Specifically, these models may not fully capture the underlying legal features in legal case documents. To address this issue, we propose CaseEncoder, a legal document encoder that leverages fine-grained legal knowledge in both the data sampling and pre-training phases. In the data sampling phase, we enhance the quality of the training data by utilizing fine-grained law article information to guide the selection of positive and negative examples. In the pre-training phase, we design legal-specific pre-training tasks that align with the judging criteria of relevant legal cases. Based on these tasks, we introduce an innovative loss function called Biased Circle Loss to ",
    "path": "papers/23/05/2305.05393.json",
    "total_tokens": 1198,
    "translated_title": "CaseEncoder：一种融合法律知识的预训练模型用于法律案例编码",
    "translated_abstract": "现代法律信息系统中，法律案例检索是关键的流程。尽管近期的研究利用了基于通用领域自监督预训练范式的预训练语言模型(PLMs)构建了用于法律案例检索的模型，但是使用通用领域的PLMs作为骨干模型有其局限性。具体来说，这些模型可能无法完全捕捉法律案例文档中的潜在法律特征。为了解决这个问题，我们提出了CaseEncoder，一种法律文档编码器，它在数据采样和预训练阶段利用细粒度的法律知识。在数据采样阶段，我们利用细粒度的法律条款信息引导正负样本的选择，从而提高了训练数据的质量。在预训练阶段，我们设计了与相关法律案例的评判标准相一致的法律特定预训练任务。根据这些任务，我们引入了一种创新的损失函数——偏置圆形损失(Biased Circle Loss)来处理法律案例数据集中正负样本之间的不平衡。实验结果表明，我们的CaseEncoder在法律案例检索和法律问答任务上优于最先进的PLMs。",
    "tldr": "本文提出了一种融合法律知识的预训练模型CaseEncoder，针对法律案例的特殊领域需求，CaseEncoder在数据采样和预训练阶段中都使用了法律知识，其中包括利用细粒度的法律条款信息引导正负样本的选择，以及设计了与相关法律案例的评判标准相一致的法律特定预训练任务，该模型在法律案例检索和法律问答任务上优于最先进的PLMs。",
    "en_tdlr": "This paper proposes a pre-trained model, CaseEncoder, that incorporates fine-grained legal knowledge to address the limitations of using general pre-trained language models (PLMs) for legal case retrieval. CaseEncoder utilizes legal-specific pre-training tasks aligned with the judging criteria of relevant legal cases and a Biased Circle Loss function to handle the imbalance between positive and negative samples in legal case datasets. Experimental results demonstrate that CaseEncoder outperforms state-of-the-art PLMs in both legal case retrieval and legal question-answering tasks."
}