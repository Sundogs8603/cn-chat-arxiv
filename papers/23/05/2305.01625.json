{
    "title": "Unlimiformer: Long-Range Transformers with Unlimited Length Input. (arXiv:2305.01625v1 [cs.CL])",
    "abstract": "Transformer-based models typically have a predefined bound to their input length, because of their need to potentially attend to every token in the input. In this work, we propose Unlimiformer: a general approach that can wrap any existing pretrained encoder-decoder transformer, and offload the attention computation across all layers to a single $k$-nearest-neighbor index; this index can be kept on either the GPU or CPU memory and queried in sub-linear time. This way, we can index extremely long input sequences, while every attention head in every decoder layer retrieves its top-$k$ keys, instead of attending to every key. We demonstrate Unlimiformers's efficacy on several long-document and multi-document summarization benchmarks, showing that it can summarize even 350k token-long inputs from the BookSum dataset, without any input truncation at test time. Unlimiformer improves pretrained models such as BART and Longformer by extending them to unlimited inputs without additional learned",
    "link": "http://arxiv.org/abs/2305.01625",
    "context": "Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input. (arXiv:2305.01625v1 [cs.CL])\nAbstract: Transformer-based models typically have a predefined bound to their input length, because of their need to potentially attend to every token in the input. In this work, we propose Unlimiformer: a general approach that can wrap any existing pretrained encoder-decoder transformer, and offload the attention computation across all layers to a single $k$-nearest-neighbor index; this index can be kept on either the GPU or CPU memory and queried in sub-linear time. This way, we can index extremely long input sequences, while every attention head in every decoder layer retrieves its top-$k$ keys, instead of attending to every key. We demonstrate Unlimiformers's efficacy on several long-document and multi-document summarization benchmarks, showing that it can summarize even 350k token-long inputs from the BookSum dataset, without any input truncation at test time. Unlimiformer improves pretrained models such as BART and Longformer by extending them to unlimited inputs without additional learned",
    "path": "papers/23/05/2305.01625.json",
    "total_tokens": 734,
    "translated_title": "无限长度输入的长距离Transformer-Unlimiformer",
    "translated_abstract": "基于Transformer的模型通常对输入长度有预定义的限制，因为它们可能需要参考输入中的每个标记。本文提出了一种通用方法-Unlimiformer，可以包装任何现有的预训练编码器-解码器Transformer，并将所有层的注意计算卸载到单个k近邻索引上。我们在几个长文档和多文档摘要基准测试中证明了Unlimiformer的有效性，展示了它可以总结350k令牌长的输入而不进行测试时的截断。",
    "tldr": "Unlimiformer是一种Transformer模型的通用方法，可以将所有层的注意计算卸载到单个k近邻索引上，从而可处理无限长度的输入，而不增加额外的学习负担。",
    "en_tdlr": "Unlimiformer is a general approach for Transformer-based models that offloads the attention computation to a single k-nearest-neighbor index, allowing for processing of unlimited length inputs without additional learning burden."
}