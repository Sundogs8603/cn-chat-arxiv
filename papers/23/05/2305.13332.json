{
    "title": "Conditional Online Learning for Keyword Spotting. (arXiv:2305.13332v1 [eess.AS])",
    "abstract": "Modern approaches for keyword spotting rely on training deep neural networks on large static datasets with i.i.d. distributions. However, the resulting models tend to underperform when presented with changing data regimes in real-life applications. This work investigates a simple but effective online continual learning method that updates a keyword spotter on-device via SGD as new data becomes available. Contrary to previous research, this work focuses on learning the same KWS task, which covers most commercial applications. During experiments with dynamic audio streams in different scenarios, that method improves the performance of a pre-trained small-footprint model by 34%. Moreover, experiments demonstrate that, compared to a naive online learning implementation, conditional model updates based on its performance in a small hold-out set drawn from the training distribution mitigate catastrophic forgetting.",
    "link": "http://arxiv.org/abs/2305.13332",
    "context": "Title: Conditional Online Learning for Keyword Spotting. (arXiv:2305.13332v1 [eess.AS])\nAbstract: Modern approaches for keyword spotting rely on training deep neural networks on large static datasets with i.i.d. distributions. However, the resulting models tend to underperform when presented with changing data regimes in real-life applications. This work investigates a simple but effective online continual learning method that updates a keyword spotter on-device via SGD as new data becomes available. Contrary to previous research, this work focuses on learning the same KWS task, which covers most commercial applications. During experiments with dynamic audio streams in different scenarios, that method improves the performance of a pre-trained small-footprint model by 34%. Moreover, experiments demonstrate that, compared to a naive online learning implementation, conditional model updates based on its performance in a small hold-out set drawn from the training distribution mitigate catastrophic forgetting.",
    "path": "papers/23/05/2305.13332.json",
    "total_tokens": 867,
    "translated_title": "关键词检测的有条件在线学习方法",
    "translated_abstract": "现代关键词检测方法依赖于在具有独立同分布的大型静态数据集上训练深度神经网络。然而，当面对实际应用中数据分布发生变化时，所得模型往往会表现不佳。本文研究一种简单而有效的在线持续学习方法，该方法通过SGD在设备上更新关键词识别器，以便在新数据可用时更新模型。与之前的研究不同，本文侧重于学习相同的关键词检测任务，该任务涵盖了大多数商业应用。在不同情况下对动态音频流进行实验时，该方法将预训练的小型模型的性能提高了34％。此外，实验表明，与朴素的在线学习实现相比，基于训练分布中绘制的小型保留集中其性能进行条件模型更新可以减轻灾难性遗忘。",
    "tldr": "本文研究了一种有条件的在线持续学习方法，可以在新数据可用时更新关键词识别器，在动态音频流实验中，该方法可将预训练的小型模型的性能提高34％，并且可以减轻灾难性遗忘。",
    "en_tdlr": "This paper investigates a conditional online continual learning method for updating a keyword spotter on-device via SGD as new data becomes available. The method improves the performance of a pre-trained small-footprint model by 34% in experiments with dynamic audio streams and mitigates catastrophic forgetting."
}