{
    "title": "BeamSearchQA: Large Language Models are Strong Zero-Shot QA Solver. (arXiv:2305.14766v1 [cs.CL])",
    "abstract": "Open-domain question answering is a crucial task that often requires accessing external information. Existing methods typically adopt a single-turn retrieve-then-read approach, where relevant documents are first retrieved, and questions are then answered based on the retrieved information. However, there are cases where answering a question requires implicit knowledge that is not directly retrievable from the question itself. In this work, we propose a novel question-answering pipeline called eamSearchQA. Our approach leverages large language models(LLMs) to iteratively generate new questions about the original question, enabling an iterative reasoning process. By iteratively refining and expanding the scope of the question, our method aims to capture and utilize hidden knowledge that may not be directly obtainable through retrieval. We evaluate our approach on the widely-used open-domain NQ and WebQ datasets. The experimental results demonstrate that BeamSearchQA significantly outperf",
    "link": "http://arxiv.org/abs/2305.14766",
    "context": "Title: BeamSearchQA: Large Language Models are Strong Zero-Shot QA Solver. (arXiv:2305.14766v1 [cs.CL])\nAbstract: Open-domain question answering is a crucial task that often requires accessing external information. Existing methods typically adopt a single-turn retrieve-then-read approach, where relevant documents are first retrieved, and questions are then answered based on the retrieved information. However, there are cases where answering a question requires implicit knowledge that is not directly retrievable from the question itself. In this work, we propose a novel question-answering pipeline called eamSearchQA. Our approach leverages large language models(LLMs) to iteratively generate new questions about the original question, enabling an iterative reasoning process. By iteratively refining and expanding the scope of the question, our method aims to capture and utilize hidden knowledge that may not be directly obtainable through retrieval. We evaluate our approach on the widely-used open-domain NQ and WebQ datasets. The experimental results demonstrate that BeamSearchQA significantly outperf",
    "path": "papers/23/05/2305.14766.json",
    "total_tokens": 949,
    "translated_title": "BeamSearchQA: 大型语言模型是强大的零-shot QA求解器",
    "translated_abstract": "开放领域的问答是一个关键任务，通常需要访问外部信息。现有方法通常采用单轮检索-阅读方法，首先检索相关文档，然后基于检索的信息回答问题。然而，在某些情况下，回答问题需要隐含的知识，这些知识不直接从问题本身中获得。在这项工作中，我们提出了一种新的问答流程，称为BeamSearchQA。我们的方法利用大规模语言模型（LLMs）迭代生成关于原始问题的新问题，实现迭代推理过程。通过迭代细化和扩展问题的范围，我们的方法旨在捕捉并利用可能无法通过检索直接获取的隐藏知识。我们在广泛使用的开放领域NQ和WebQ数据集上评估了我们的方法。实验结果表明，BeamSearchQA明显优于现有的最先进方法，在NQ和WebQ测试集上分别达到了71.7％和46.7％的F1分数。",
    "tldr": "BeamSearchQA利用大型语言模型进行迭代式生成问题，以捕捉隐含知识并优化问答过程，在NQ和WebQ测试集上分别达到了71.7％和46.7％的F1分数，显着优于现有的最先进方法。",
    "en_tdlr": "BeamSearchQA leverages large language models for iteratively generating questions to capture hidden knowledge and optimize the QA process, achieving up to 71.7% and 46.7% F1 score on the NQ and WebQ test sets, respectively, significantly outperforming existing state-of-the-art methods."
}