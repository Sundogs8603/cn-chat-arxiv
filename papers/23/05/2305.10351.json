{
    "title": "BIOT: Cross-data Biosignal Learning in the Wild. (arXiv:2305.10351v1 [eess.SP])",
    "abstract": "Biological signals, such as electroencephalograms (EEG), play a crucial role in numerous clinical applications, exhibiting diverse data formats and quality profiles. Current deep learning models for biosignals are typically specialized for specific datasets and clinical settings, limiting their broader applicability. Motivated by the success of large language models in text processing, we explore the development of foundational models that are trained from multiple data sources and can be fine-tuned on different downstream biosignal tasks.  To overcome the unique challenges associated with biosignals of various formats, such as mismatched channels, variable sample lengths, and prevalent missing values, we propose a Biosignal Transformer (\\method). The proposed \\method model can enable cross-data learning with mismatched channels, variable lengths, and missing values by tokenizing diverse biosignals into unified \"biosignal sentences\". Specifically, we tokenize each channel into fixed-le",
    "link": "http://arxiv.org/abs/2305.10351",
    "context": "Title: BIOT: Cross-data Biosignal Learning in the Wild. (arXiv:2305.10351v1 [eess.SP])\nAbstract: Biological signals, such as electroencephalograms (EEG), play a crucial role in numerous clinical applications, exhibiting diverse data formats and quality profiles. Current deep learning models for biosignals are typically specialized for specific datasets and clinical settings, limiting their broader applicability. Motivated by the success of large language models in text processing, we explore the development of foundational models that are trained from multiple data sources and can be fine-tuned on different downstream biosignal tasks.  To overcome the unique challenges associated with biosignals of various formats, such as mismatched channels, variable sample lengths, and prevalent missing values, we propose a Biosignal Transformer (\\method). The proposed \\method model can enable cross-data learning with mismatched channels, variable lengths, and missing values by tokenizing diverse biosignals into unified \"biosignal sentences\". Specifically, we tokenize each channel into fixed-le",
    "path": "papers/23/05/2305.10351.json",
    "total_tokens": 1085,
    "translated_title": "BIOT：在野外跨数据源生物信号学习",
    "translated_abstract": "生物信号，如脑电图（EEG），在许多临床应用中发挥着至关重要的作用，显示出多种数据格式和质量特征。当前的生物信号深度学习模型通常专门针对特定数据集和临床设置进行了优化，从而限制了它们的应用范围。受到大型语言模型在文本处理方面的成功启发，我们探索开发基于多个数据源进行训练并可在不同基础生物信号任务上进行微调的基础模型。为了克服与各种格式的生物信号相关的独特挑战，例如不匹配的通道，可变的采样长度和普遍存在的缺失值，我们提出了一种Biosignal Transformer模型（简称\\method）。所提出的\\method模型可以通过将不同的生物信号令牌化为统一的“生物信号句子”来实现通道不匹配、长度可变和缺失值普遍存在的跨数据学习。具体来说，我们将每个通道令牌化为固定长度的特征，并在通道之间进行聚合，以形成完整的生物信号句子。在EEG分类和EEG转语音任务上的实验表明，与当前最先进的基线相比，我们的模型具有卓越的性能。",
    "tldr": "该论文提出了一种可以在多个生物信号数据源上进行训练，并可在所有生物信号任务上进行微调的基础模型BIOT。通过将不同的生物信号令牌化为统一的“生物信号句子”，该模型可以实现跨数据源的学习，处理各种格式的生物信号，具有出色的性能。",
    "en_tdlr": "The paper proposes a foundational model, BIOT, which can be trained from multiple data sources and fine-tuned on various downstream biosignal tasks. By tokenizing various biosignals into unified \"biosignal sentences\", the model enables cross-data learning, handling diverse biosignal formats, and achieves superior performance compared to state-of-the-art baselines on EEG classification and EEG-to-speech tasks."
}