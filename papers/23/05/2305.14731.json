{
    "title": "AutoDepthNet: High Frame Rate Depth Map Reconstruction using Commodity Depth and RGB Cameras. (arXiv:2305.14731v1 [cs.CV])",
    "abstract": "Depth cameras have found applications in diverse fields, such as computer vision, artificial intelligence, and video gaming. However, the high latency and low frame rate of existing commodity depth cameras impose limitations on their applications. We propose a fast and accurate depth map reconstruction technique to reduce latency and increase the frame rate in depth cameras. Our approach uses only a commodity depth camera and color camera in a hybrid camera setup; our prototype is implemented using a Kinect Azure depth camera at 30 fps and a high-speed RGB iPhone 11 Pro camera captured at 240 fps. The proposed network, AutoDepthNet, is an encoder-decoder model that captures frames from the high-speed RGB camera and combines them with previous depth frames to reconstruct a stream of high frame rate depth maps. On GPU, with a 480 x 270 output resolution, our system achieves an inference time of 8 ms, enabling real-time use at up to 200 fps with parallel processing. AutoDepthNet can estim",
    "link": "http://arxiv.org/abs/2305.14731",
    "context": "Title: AutoDepthNet: High Frame Rate Depth Map Reconstruction using Commodity Depth and RGB Cameras. (arXiv:2305.14731v1 [cs.CV])\nAbstract: Depth cameras have found applications in diverse fields, such as computer vision, artificial intelligence, and video gaming. However, the high latency and low frame rate of existing commodity depth cameras impose limitations on their applications. We propose a fast and accurate depth map reconstruction technique to reduce latency and increase the frame rate in depth cameras. Our approach uses only a commodity depth camera and color camera in a hybrid camera setup; our prototype is implemented using a Kinect Azure depth camera at 30 fps and a high-speed RGB iPhone 11 Pro camera captured at 240 fps. The proposed network, AutoDepthNet, is an encoder-decoder model that captures frames from the high-speed RGB camera and combines them with previous depth frames to reconstruct a stream of high frame rate depth maps. On GPU, with a 480 x 270 output resolution, our system achieves an inference time of 8 ms, enabling real-time use at up to 200 fps with parallel processing. AutoDepthNet can estim",
    "path": "papers/23/05/2305.14731.json",
    "total_tokens": 918,
    "translated_title": "AutoDepthNet：使用普通深度和RGB相机进行高帧率深度图重建",
    "translated_abstract": "深度相机在计算机视觉、人工智能和视频游戏等领域有着各种应用。然而，现有普通深度相机的高延迟和低帧率限制了它们的应用。我们提出了一种快速而准确的深度图重建技术，以减少延迟并提高深度相机的帧率。我们的方法仅使用混合摄像机设置中的普通深度相机和彩色相机；我们的原型使用Kinect Azure深度相机以30 fps的速度和在iPhone 11 Pro上捕获的高速RGB相机以240 fps的速度进行实现。所提出的网络AutoDepthNet是一个编码器-解码器模型，它捕获来自高速RGB相机的帧，并将它们与先前的深度帧结合起来重建一系列高帧率深度图。在GPU上，使用480 x 270输出分辨率，我们的系统实现了8毫秒的推断时间，可以通过并行处理在高达200 fps的实时使用。AutoDepthNet可以逐层精确地估计深度图。",
    "tldr": "AutoDepthNet能够利用混合摄像机设置中的深度相机和彩色相机进行深度图重建，在GPU上具有8ms的推断时间，能够实现每秒200帧的实时使用。"
}