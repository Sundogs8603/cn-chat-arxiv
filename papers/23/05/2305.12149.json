{
    "title": "Normalizing flow sampling with Langevin dynamics in the latent space. (arXiv:2305.12149v1 [stat.ML])",
    "abstract": "Normalizing flows (NF) use a continuous generator to map a simple latent (e.g. Gaussian) distribution, towards an empirical target distribution associated with a training data set. Once trained by minimizing a variational objective, the learnt map provides an approximate generative model of the target distribution. Since standard NF implement differentiable maps, they may suffer from pathological behaviors when targeting complex distributions. For instance, such problems may appear for distributions on multi-component topologies or characterized by multiple modes with high probability regions separated by very unlikely areas. A typical symptom is the explosion of the Jacobian norm of the transformation in very low probability areas. This paper proposes to overcome this issue thanks to a new Markov chain Monte Carlo algorithm to sample from the target distribution in the latent domain before transporting it back to the target domain. The approach relies on a Metropolis adjusted Langevin",
    "link": "http://arxiv.org/abs/2305.12149",
    "context": "Title: Normalizing flow sampling with Langevin dynamics in the latent space. (arXiv:2305.12149v1 [stat.ML])\nAbstract: Normalizing flows (NF) use a continuous generator to map a simple latent (e.g. Gaussian) distribution, towards an empirical target distribution associated with a training data set. Once trained by minimizing a variational objective, the learnt map provides an approximate generative model of the target distribution. Since standard NF implement differentiable maps, they may suffer from pathological behaviors when targeting complex distributions. For instance, such problems may appear for distributions on multi-component topologies or characterized by multiple modes with high probability regions separated by very unlikely areas. A typical symptom is the explosion of the Jacobian norm of the transformation in very low probability areas. This paper proposes to overcome this issue thanks to a new Markov chain Monte Carlo algorithm to sample from the target distribution in the latent domain before transporting it back to the target domain. The approach relies on a Metropolis adjusted Langevin",
    "path": "papers/23/05/2305.12149.json",
    "total_tokens": 832,
    "translated_title": "在潜空间中使用 Langevin 动力学的归一化流采样",
    "translated_abstract": "归一化流（NF）使用连续生成器将简单的潜在分布（例如高斯分布）映射到与训练数据集相关联的经验目标分布。通过最小化变分目标来训练后，学习到的映射提供了目标分布的近似生成模型。本文提出了一种新的马尔可夫链蒙特卡罗算法，将目标分布在潜域中采样，然后将其传输回目标域，以克服当应对复杂分布时可能出现的问题。该方法依赖于潜空间中的 Metropolis 调整 Langevin 动力学，并且可以轻松地融入任何 NF 结构中。我们展示了我们的方法在玩具和真实数据集上的有效性。",
    "tldr": "本文提出了一种在潜在空间中使用 Langevin 动力学的采样方法，以克服归一化流可能面临的复杂分布问题，并能够轻松地融入任何 NF 结构中。",
    "en_tdlr": "This paper proposes a sampling method using Langevin dynamics in the latent space to overcome the complex distribution problem that normalizing flows may face, and can be easily incorporated into any NF architecture."
}