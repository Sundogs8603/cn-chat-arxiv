{
    "title": "An Exploration of Conditioning Methods in Graph Neural Networks. (arXiv:2305.01933v1 [cs.LG])",
    "abstract": "The flexibility and effectiveness of message passing based graph neural networks (GNNs) induced considerable advances in deep learning on graph-structured data. In such approaches, GNNs recursively update node representations based on their neighbors and they gain expressivity through the use of node and edge attribute vectors. E.g., in computational tasks such as physics and chemistry usage of edge attributes such as relative position or distance proved to be essential. In this work, we address not what kind of attributes to use, but how to condition on this information to improve model performance. We consider three types of conditioning; weak, strong, and pure, which respectively relate to concatenation-based conditioning, gating, and transformations that are causally dependent on the attributes. This categorization provides a unifying viewpoint on different classes of GNNs, from separable convolutions to various forms of message passing networks. We provide an empirical study on th",
    "link": "http://arxiv.org/abs/2305.01933",
    "context": "Title: An Exploration of Conditioning Methods in Graph Neural Networks. (arXiv:2305.01933v1 [cs.LG])\nAbstract: The flexibility and effectiveness of message passing based graph neural networks (GNNs) induced considerable advances in deep learning on graph-structured data. In such approaches, GNNs recursively update node representations based on their neighbors and they gain expressivity through the use of node and edge attribute vectors. E.g., in computational tasks such as physics and chemistry usage of edge attributes such as relative position or distance proved to be essential. In this work, we address not what kind of attributes to use, but how to condition on this information to improve model performance. We consider three types of conditioning; weak, strong, and pure, which respectively relate to concatenation-based conditioning, gating, and transformations that are causally dependent on the attributes. This categorization provides a unifying viewpoint on different classes of GNNs, from separable convolutions to various forms of message passing networks. We provide an empirical study on th",
    "path": "papers/23/05/2305.01933.json",
    "total_tokens": 894,
    "translated_title": "图神经网络中的条件方法探究",
    "translated_abstract": "基于消息传递的图神经网络（GNN）的灵活性和有效性在图结构数据的深度学习中取得了相当的进步。在这种方法中，GNN基于其邻居递归地更新节点表示，并通过使用节点和边属性向量来获得表现力。例如，在物理和化学等计算任务中，使用边属性（如相对位置或距离）被证明是必要的。在本文中，我们关注的不是要使用什么类型的属性，而是如何在此信息的基础上进行条件处理，以提高模型性能。我们考虑了三种类型的条件处理：弱条件处理、强条件处理和纯条件处理，分别与基于连接的条件处理、门控和因果依赖于属性的变换相关。这种分类提供了一个统一的观点来看待不同类别的GNN，从可分离卷积到各种形式的消息传递网络。我们在三个常见的任务上进行了实证研究，表明GNNs的性能受到该条件方式的影响。",
    "tldr": "本文探究图神经网络中的三种条件处理方式：弱条件处理、强条件处理和纯条件处理，对于不同类别的GNNs，有不同的表现，实证研究表明这些条件处理方式对于GNN的性能有影响。",
    "en_tdlr": "This paper investigates three conditioning methods in graph neural networks, which are weak, strong, and pure conditioning. Empirical study shows that these methods have significant impacts on the performance of GNNs for different categories."
}