{
    "title": "Block-local learning with probabilistic latent representations. (arXiv:2305.14974v2 [cs.LG] UPDATED)",
    "abstract": "The ubiquitous backpropagation algorithm requires sequential updates through the network introducing a locking problem. In addition, back-propagation relies on the transpose of forward weight matrices to compute updates, introducing a weight transport problem across the network. Locking and weight transport are problems because they prevent efficient parallelization and horizontal scaling of the training process. We propose a new method to address both these problems and scale up the training of large models. Our method works by dividing a deep neural network into blocks and introduces a feedback network that propagates the information from the targets backwards to provide auxiliary local losses. Forward and backward propagation can operate in parallel and with different sets of weights, addressing the problems of locking and weight transport. Our approach derives from a statistical interpretation of training that treats output activations of network blocks as parameters of probability",
    "link": "http://arxiv.org/abs/2305.14974",
    "context": "Title: Block-local learning with probabilistic latent representations. (arXiv:2305.14974v2 [cs.LG] UPDATED)\nAbstract: The ubiquitous backpropagation algorithm requires sequential updates through the network introducing a locking problem. In addition, back-propagation relies on the transpose of forward weight matrices to compute updates, introducing a weight transport problem across the network. Locking and weight transport are problems because they prevent efficient parallelization and horizontal scaling of the training process. We propose a new method to address both these problems and scale up the training of large models. Our method works by dividing a deep neural network into blocks and introduces a feedback network that propagates the information from the targets backwards to provide auxiliary local losses. Forward and backward propagation can operate in parallel and with different sets of weights, addressing the problems of locking and weight transport. Our approach derives from a statistical interpretation of training that treats output activations of network blocks as parameters of probability",
    "path": "papers/23/05/2305.14974.json",
    "total_tokens": 816,
    "translated_title": "使用概率潜在表示的块局部学习",
    "translated_abstract": "广泛使用的反向传播算法需要通过网络进行顺序更新，导致了锁定问题。此外，反向传播依赖于前向权重矩阵的转置来计算更新，导致了网络中的权重传输问题。锁定和权重传输问题阻碍了训练过程的高效并行化和水平扩展。我们提出了一种解决这些问题并扩展大型模型训练的新方法。我们的方法将深度神经网络分为块，并引入了一个反馈网络，从目标向后传播信息以提供辅助的局部损失。前向和后向传播可以并行进行，并且使用不同的权重集，解决了锁定和权重传输的问题。我们的方法源于一种统计解释的训练方法，将网络块的输出激活视为概率参数。",
    "tldr": "该论文提出了一种使用概率潜在表示的块局部学习方法，通过将深度神经网络分为块并引入反馈网络，在解决锁定和权重传输问题的同时，实现了高效的并行化和水平扩展。",
    "en_tdlr": "This paper proposes a block-local learning method with probabilistic latent representations, which divides a deep neural network into blocks and introduces a feedback network. This method efficiently addresses the problems of locking and weight transport, allowing for parallel propagation and horizontal scaling."
}