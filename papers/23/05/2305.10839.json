{
    "title": "A Lexical-aware Non-autoregressive Transformer-based ASR Model. (arXiv:2305.10839v1 [cs.CL])",
    "abstract": "Non-autoregressive automatic speech recognition (ASR) has become a mainstream of ASR modeling because of its fast decoding speed and satisfactory result. To further boost the performance, relaxing the conditional independence assumption and cascading large-scaled pre-trained models are two active research directions. In addition to these strategies, we propose a lexical-aware non-autoregressive Transformer-based (LA-NAT) ASR framework, which consists of an acoustic encoder, a speech-text shared encoder, and a speech-text shared decoder. The acoustic encoder is used to process the input speech features as usual, and the speech-text shared encoder and decoder are designed to train speech and text data simultaneously. By doing so, LA-NAT aims to make the ASR model aware of lexical information, so the resulting model is expected to achieve better results by leveraging the learned linguistic knowledge. A series of experiments are conducted on the AISHELL-1, CSJ, and TEDLIUM 2 datasets. Acco",
    "link": "http://arxiv.org/abs/2305.10839",
    "context": "Title: A Lexical-aware Non-autoregressive Transformer-based ASR Model. (arXiv:2305.10839v1 [cs.CL])\nAbstract: Non-autoregressive automatic speech recognition (ASR) has become a mainstream of ASR modeling because of its fast decoding speed and satisfactory result. To further boost the performance, relaxing the conditional independence assumption and cascading large-scaled pre-trained models are two active research directions. In addition to these strategies, we propose a lexical-aware non-autoregressive Transformer-based (LA-NAT) ASR framework, which consists of an acoustic encoder, a speech-text shared encoder, and a speech-text shared decoder. The acoustic encoder is used to process the input speech features as usual, and the speech-text shared encoder and decoder are designed to train speech and text data simultaneously. By doing so, LA-NAT aims to make the ASR model aware of lexical information, so the resulting model is expected to achieve better results by leveraging the learned linguistic knowledge. A series of experiments are conducted on the AISHELL-1, CSJ, and TEDLIUM 2 datasets. Acco",
    "path": "papers/23/05/2305.10839.json",
    "total_tokens": 1003,
    "translated_title": "基于词汇感知的非自回归Transformer语音识别模型",
    "translated_abstract": "非自回归自动语音识别（ASR）由于其快速解码速度和令人满意的结果已经成为ASR建模的主流。为进一步提高性能，放松条件独立假设和级联大规模预训练模型是两个活跃的研究方向。除了这些策略外，我们提出了一个基于词汇感知的非自回归Transformer（LA-NAT）ASR框架，其中包括一个声学编码器、一个语音-文本共享编码器和一个语音-文本共享解码器。声学编码器像往常一样用于处理输入语音特征，而语音-文本共享编码器和解码器旨在同时训练语音和文本数据。这样做，LA-NAT旨在使ASR模型感知词汇信息，因此利用学习到的语言知识，预期产生更好的结果。我们在AISHELL-1、CSJ和TEDLIUM 2数据集上进行了一系列实验。根据实验结果，我们提出的LA-NAT模型在三个数据集上优于基线模型，并取得了最先进的性能。",
    "tldr": "此论文提出了一个基于词汇感知的非自回归Transformer语音识别模型，该模型集成了语音-文本共享编码器和解码器，并能够同时训练语音和文本数据以提高性能，实验结果显示该模型在多个数据集上拥有领先的性能表现。"
}