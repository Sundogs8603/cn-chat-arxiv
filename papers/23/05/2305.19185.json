{
    "title": "Compression with Bayesian Implicit Neural Representations. (arXiv:2305.19185v1 [cs.LG])",
    "abstract": "Many common types of data can be represented as functions that map coordinates to signal values, such as pixel locations to RGB values in the case of an image. Based on this view, data can be compressed by overfitting a compact neural network to its functional representation and then encoding the network weights. However, most current solutions for this are inefficient, as quantization to low-bit precision substantially degrades the reconstruction quality. To address this issue, we propose overfitting variational Bayesian neural networks to the data and compressing an approximate posterior weight sample using relative entropy coding instead of quantizing and entropy coding it. This strategy enables direct optimization of the rate-distortion performance by minimizing the $\\beta$-ELBO, and target different rate-distortion trade-offs for a given network architecture by adjusting $\\beta$. Moreover, we introduce an iterative algorithm for learning prior weight distributions and employ a pro",
    "link": "http://arxiv.org/abs/2305.19185",
    "context": "Title: Compression with Bayesian Implicit Neural Representations. (arXiv:2305.19185v1 [cs.LG])\nAbstract: Many common types of data can be represented as functions that map coordinates to signal values, such as pixel locations to RGB values in the case of an image. Based on this view, data can be compressed by overfitting a compact neural network to its functional representation and then encoding the network weights. However, most current solutions for this are inefficient, as quantization to low-bit precision substantially degrades the reconstruction quality. To address this issue, we propose overfitting variational Bayesian neural networks to the data and compressing an approximate posterior weight sample using relative entropy coding instead of quantizing and entropy coding it. This strategy enables direct optimization of the rate-distortion performance by minimizing the $\\beta$-ELBO, and target different rate-distortion trade-offs for a given network architecture by adjusting $\\beta$. Moreover, we introduce an iterative algorithm for learning prior weight distributions and employ a pro",
    "path": "papers/23/05/2305.19185.json",
    "total_tokens": 859,
    "translated_title": "Bayesian隐式神经表示下的压缩",
    "translated_abstract": "许多常见类型的数据可以表示为将坐标映射到信号值的函数，例如图像中的像素位置到RGB值。基于这个观点，可以通过对数据的功能表示进行超拟合，然后编码网络权重来压缩数据。然而，大多数当前的解决方案都效率低下，因为将精度量化到低比特会大幅降低重构质量。为解决这个问题，我们提出了过度拟合变分贝叶斯神经网络来压缩近似后验权重样本，而不是量化和熵编码它。该策略通过最小化 $\\beta$-ELBO 直接优化码-失真性能，并通过调整 $\\beta$ 来针对给定的网络结构实现不同的码-失真平衡。此外，我们引入了一种学习先验权重分布的迭代算法，并采用主动尺寸调整来进一步提高效率。",
    "tldr": "该论文提出了一种用Bayesian隐式神经表示来压缩数据的方法，通过最小化 $\\beta$-ELBO 直接优化码-失真性能，并通过调整 $\\beta$ 来针对给定的网络结构实现不同的码-失真平衡。"
}