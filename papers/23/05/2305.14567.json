{
    "title": "Constant Memory Attentive Neural Processes. (arXiv:2305.14567v1 [cs.LG])",
    "abstract": "Neural Processes (NPs) are efficient methods for estimating predictive uncertainties. NPs comprise of a conditioning phase where a context dataset is encoded, a querying phase where the model makes predictions using the context dataset encoding, and an updating phase where the model updates its encoding with newly received datapoints. However, state-of-the-art methods require additional memory which scales linearly or quadratically with the size of the dataset, limiting their applications, particularly in low-resource settings. In this work, we propose Constant Memory Attentive Neural Processes (CMANPs), an NP variant which only requires constant memory for the conditioning, querying, and updating phases. In building CMANPs, we propose Constant Memory Attention Block (CMAB), a novel general-purpose attention block that can compute its output in constant memory and perform updates in constant computation. Empirically, we show CMANPs achieve state-of-the-art results on meta-regression an",
    "link": "http://arxiv.org/abs/2305.14567",
    "context": "Title: Constant Memory Attentive Neural Processes. (arXiv:2305.14567v1 [cs.LG])\nAbstract: Neural Processes (NPs) are efficient methods for estimating predictive uncertainties. NPs comprise of a conditioning phase where a context dataset is encoded, a querying phase where the model makes predictions using the context dataset encoding, and an updating phase where the model updates its encoding with newly received datapoints. However, state-of-the-art methods require additional memory which scales linearly or quadratically with the size of the dataset, limiting their applications, particularly in low-resource settings. In this work, we propose Constant Memory Attentive Neural Processes (CMANPs), an NP variant which only requires constant memory for the conditioning, querying, and updating phases. In building CMANPs, we propose Constant Memory Attention Block (CMAB), a novel general-purpose attention block that can compute its output in constant memory and perform updates in constant computation. Empirically, we show CMANPs achieve state-of-the-art results on meta-regression an",
    "path": "papers/23/05/2305.14567.json",
    "total_tokens": 874,
    "translated_title": "不变存储的注意力神经过程",
    "translated_abstract": "神经过程 (Neural Processes, NPs) 是估计预测不确定性的高效方法。NPs 包含一个编码条件数据集的条件阶段、一个使用编码预测的查询阶段和一个使用新数据点更新编码的更新阶段。然而，最先进的方法需要额外的内存，这个内存随数据集的大小呈线性或二次函数增长，限制了它们在低资源环境下的应用。在这项工作中，我们提出了不变存储的注意力神经过程 (Constant Memory Attentive Neural Processes, CMANPs)，它的条件、查询和更新阶段均只需要常数内存。在构建 CMANPs 时，我们提出了一种新型的通用注意力块，称为 Constant Memory Attention Block (CMAB)，它可以在常数内存中计算输出并进行常数的更新计算。实验结果表明，CMANPs 在元回归和少样本回归任务上实现了最先进的表现，同时保持常数内存复杂度。",
    "tldr": "提出了一种不变存储的注意力神经过程 (CMANPs) 及其注意力块 CMAB，能在常数内存下进行条件、查询和更新操作，并在元回归和少样本回归任务上获得最先进的表现。",
    "en_tdlr": "Proposed a constant memory attentive neural process variant (CMANPs) and its attention block CMAB, which can perform conditioning, querying and updating operations with constant memory and achieved state-of-the-art results on meta-regression and few-shot regression tasks."
}