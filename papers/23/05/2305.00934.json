{
    "title": "Variational Inference for Bayesian Neural Networks under Model and Parameter Uncertainty. (arXiv:2305.00934v1 [stat.ML])",
    "abstract": "Bayesian neural networks (BNNs) have recently regained a significant amount of attention in the deep learning community due to the development of scalable approximate Bayesian inference techniques. There are several advantages of using a Bayesian approach: Parameter and prediction uncertainties become easily available, facilitating rigorous statistical analysis. Furthermore, prior knowledge can be incorporated. However, so far, there have been no scalable techniques capable of combining both structural and parameter uncertainty. In this paper, we apply the concept of model uncertainty as a framework for structural learning in BNNs and hence make inference in the joint space of structures/models and parameters. Moreover, we suggest an adaptation of a scalable variational inference approach with reparametrization of marginal inclusion probabilities to incorporate the model space constraints. Experimental results on a range of benchmark datasets show that we obtain comparable accuracy res",
    "link": "http://arxiv.org/abs/2305.00934",
    "context": "Title: Variational Inference for Bayesian Neural Networks under Model and Parameter Uncertainty. (arXiv:2305.00934v1 [stat.ML])\nAbstract: Bayesian neural networks (BNNs) have recently regained a significant amount of attention in the deep learning community due to the development of scalable approximate Bayesian inference techniques. There are several advantages of using a Bayesian approach: Parameter and prediction uncertainties become easily available, facilitating rigorous statistical analysis. Furthermore, prior knowledge can be incorporated. However, so far, there have been no scalable techniques capable of combining both structural and parameter uncertainty. In this paper, we apply the concept of model uncertainty as a framework for structural learning in BNNs and hence make inference in the joint space of structures/models and parameters. Moreover, we suggest an adaptation of a scalable variational inference approach with reparametrization of marginal inclusion probabilities to incorporate the model space constraints. Experimental results on a range of benchmark datasets show that we obtain comparable accuracy res",
    "path": "papers/23/05/2305.00934.json",
    "total_tokens": 865,
    "translated_title": "模型和参数不确定性下的贝叶斯神经网络变分推断研究",
    "translated_abstract": "由于可扩展的近似贝叶斯推断技术的发展，贝叶斯神经网络（BNNs）最近重新引起深度学习社区的大量关注。使用贝叶斯方法有几个优点：参数和预测的不确定性变得容易获得，从而便于进行严格的统计分析。此外，可以加入先验知识。然而，到目前为止，还没有可扩展的技术能够组合结构和参数不确定性。本文将模型不确定性的概念应用于BNNs中的结构学习框架，并因此在结构/模型和参数的联合空间中进行推断。此外，我们建议采用具有边际包含概率重参数化的可扩展变分推断方法来纳入模型空间约束。在一系列基准数据集上的实验结果表明，我们获得了相当的准确性。",
    "tldr": "本文提出了一种新的框架将模型不确定性应用于BNNs中的结构学习中并利用可扩展的变分推断方法纳入了模型空间约束，以在模型和参数的联合空间中进行推断分析。",
    "en_tdlr": "This paper proposes a new framework that applies model uncertainty to structural learning in Bayesian neural networks and incorporates model space constraints using scalable variational inference approach. The joint space of structures/models and parameters is used for inference analysis. The approach has been found to provide comparable accuracy on benchmark datasets."
}