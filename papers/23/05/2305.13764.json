{
    "title": "Mitigating Label Noise through Data Ambiguation. (arXiv:2305.13764v1 [cs.LG])",
    "abstract": "Label noise poses an important challenge in machine learning, especially in deep learning, in which large models with high expressive power dominate the field. Models of that kind are prone to memorizing incorrect labels, thereby harming generalization performance. Many methods have been proposed to address this problem, including robust loss functions and more complex label correction approaches. Robust loss functions are appealing due to their simplicity, but typically lack flexibility, while label correction usually adds substantial complexity to the training setup. In this paper, we suggest to address the shortcomings of both methodologies by \"ambiguating\" the target information, adding additional, complementary candidate labels in case the learner is not sufficiently convinced of the observed training label. More precisely, we leverage the framework of so-called superset learning to construct set-valued targets based on a confidence threshold, which deliver imprecise yet more reli",
    "link": "http://arxiv.org/abs/2305.13764",
    "context": "Title: Mitigating Label Noise through Data Ambiguation. (arXiv:2305.13764v1 [cs.LG])\nAbstract: Label noise poses an important challenge in machine learning, especially in deep learning, in which large models with high expressive power dominate the field. Models of that kind are prone to memorizing incorrect labels, thereby harming generalization performance. Many methods have been proposed to address this problem, including robust loss functions and more complex label correction approaches. Robust loss functions are appealing due to their simplicity, but typically lack flexibility, while label correction usually adds substantial complexity to the training setup. In this paper, we suggest to address the shortcomings of both methodologies by \"ambiguating\" the target information, adding additional, complementary candidate labels in case the learner is not sufficiently convinced of the observed training label. More precisely, we leverage the framework of so-called superset learning to construct set-valued targets based on a confidence threshold, which deliver imprecise yet more reli",
    "path": "papers/23/05/2305.13764.json",
    "total_tokens": 866,
    "translated_title": "通过数据模糊化缓解标签噪声",
    "translated_abstract": "标签噪声是机器学习中的一个重要挑战，特别是在深度学习中，具有高表现能力的大型模型主导了该领域。这种模型容易记忆错误的标签，从而损害泛化性能。已经提出了许多方法来解决这个问题，包括强健的损失函数和更复杂的标签校正方法。鲁棒性损失函数由于简单而具有吸引力，但通常缺乏灵活性，而标签校正通常会增加训练设置的复杂性。在本文中，我们建议通过“模糊化”目标信息来解决两种方法的缺点，在观察到的训练标签不足够可信时，添加附加的、互补的候选标签。更确切地说，我们利用所谓的超集学习框架来构建基于置信阈值的集合值目标，这些目标提供不精确但更可靠的信息。",
    "tldr": "本文提出了一种通过数据模糊化来缓解标签噪声的方法，即添加额外的、互补的候选标签，利用所谓的超集学习框架构建基于置信阈值的集合值目标。",
    "en_tdlr": "This paper proposes a method for mitigating label noise by adding additional candidate labels and leveraging the superset learning framework to construct set-valued targets based on a confidence threshold."
}