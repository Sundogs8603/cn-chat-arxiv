{
    "title": "Towards Unifying Multi-Lingual and Cross-Lingual Summarization. (arXiv:2305.09220v1 [cs.CL])",
    "abstract": "To adapt text summarization to the multilingual world, previous work proposes multi-lingual summarization (MLS) and cross-lingual summarization (CLS). However, these two tasks have been studied separately due to the different definitions, which limits the compatible and systematic research on both of them. In this paper, we aim to unify MLS and CLS into a more general setting, i.e., many-to-many summarization (M2MS), where a single model could process documents in any language and generate their summaries also in any language. As the first step towards M2MS, we conduct preliminary studies to show that M2MS can better transfer task knowledge across different languages than MLS and CLS. Furthermore, we propose Pisces, a pre-trained M2MS model that learns language modeling, cross-lingual ability and summarization ability via three-stage pre-training. Experimental results indicate that our Pisces significantly outperforms the state-of-the-art baselines, especially in the zero-shot directio",
    "link": "http://arxiv.org/abs/2305.09220",
    "context": "Title: Towards Unifying Multi-Lingual and Cross-Lingual Summarization. (arXiv:2305.09220v1 [cs.CL])\nAbstract: To adapt text summarization to the multilingual world, previous work proposes multi-lingual summarization (MLS) and cross-lingual summarization (CLS). However, these two tasks have been studied separately due to the different definitions, which limits the compatible and systematic research on both of them. In this paper, we aim to unify MLS and CLS into a more general setting, i.e., many-to-many summarization (M2MS), where a single model could process documents in any language and generate their summaries also in any language. As the first step towards M2MS, we conduct preliminary studies to show that M2MS can better transfer task knowledge across different languages than MLS and CLS. Furthermore, we propose Pisces, a pre-trained M2MS model that learns language modeling, cross-lingual ability and summarization ability via three-stage pre-training. Experimental results indicate that our Pisces significantly outperforms the state-of-the-art baselines, especially in the zero-shot directio",
    "path": "papers/23/05/2305.09220.json",
    "total_tokens": 973,
    "translated_title": "走向统一多语言和跨语言摘要",
    "translated_abstract": "为了适应多语言的世界，先前的工作提出了多语言摘要（MLS）和跨语言摘要（CLS）。然而，这两个任务因为定义的不同而被分别研究，限制了两者的兼容性和系统研究。在本文中，我们旨在将MLS和CLS统一到更通用的设置中，即多对多摘要（M2MS），其中单个模型可以处理任何语言的文档并生成它们的摘要，也可以用任何语言。作为通向M2MS的第一步，我们进行初步研究，表明M2MS可以更好地在不同语言之间传递任务知识，而不是MLS和CLS。此外，我们提出了Pisces，这是一个预先训练的M2MS模型，通过三阶段的预先训练学习语言建模、跨语言和摘要能力。实验结果表明，我们的Pisces显着优于现有的基线，特别是在零-shot方向中。",
    "tldr": "本文旨在将多语言摘要和跨语言摘要统一到更通用的多对多摘要中。我们提出了预先训练的M2MS模型“Pisces”，该模型可以处理任何语言的文档并生成摘要。实验结果表明，Pisces在零-shot方向上表现显着优于现有的基线模型。"
}