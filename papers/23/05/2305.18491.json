{
    "title": "Towards a Better Understanding of Representation Dynamics under TD-learning. (arXiv:2305.18491v1 [cs.LG])",
    "abstract": "TD-learning is a foundation reinforcement learning (RL) algorithm for value prediction. Critical to the accuracy of value predictions is the quality of state representations. In this work, we consider the question: how does end-to-end TD-learning impact the representation over time? Complementary to prior work, we provide a set of analysis that sheds further light on the representation dynamics under TD-learning. We first show that when the environments are reversible, end-to-end TD-learning strictly decreases the value approximation error over time. Under further assumptions on the environments, we can connect the representation dynamics with spectral decomposition over the transition matrix. This latter finding establishes fitting multiple value functions from randomly generated rewards as a useful auxiliary task for representation learning, as we empirically validate on both tabular and Atari game suites.",
    "link": "http://arxiv.org/abs/2305.18491",
    "context": "Title: Towards a Better Understanding of Representation Dynamics under TD-learning. (arXiv:2305.18491v1 [cs.LG])\nAbstract: TD-learning is a foundation reinforcement learning (RL) algorithm for value prediction. Critical to the accuracy of value predictions is the quality of state representations. In this work, we consider the question: how does end-to-end TD-learning impact the representation over time? Complementary to prior work, we provide a set of analysis that sheds further light on the representation dynamics under TD-learning. We first show that when the environments are reversible, end-to-end TD-learning strictly decreases the value approximation error over time. Under further assumptions on the environments, we can connect the representation dynamics with spectral decomposition over the transition matrix. This latter finding establishes fitting multiple value functions from randomly generated rewards as a useful auxiliary task for representation learning, as we empirically validate on both tabular and Atari game suites.",
    "path": "papers/23/05/2305.18491.json",
    "total_tokens": 861,
    "translated_title": "TD学习中对表示动态的更好理解",
    "translated_abstract": "TD学习是值预测的基础强化学习算法。值预测的准确性与状态表示的质量密切相关。本文考虑一个问题：端到端TD学习如何随时间影响表示？本文提供了一系列分析，进一步阐明了TD学习下的表示动态。在环境可逆的情况下，我们首先证明端到端TD学习可严格降低时间上的值逼近误差。在环境进一步的假设下，我们可以将表示动态连接到转移矩阵的谱分解。该发现证实了从随机生成的奖励中适合多个值函数作为表示学习的有用辅助任务，我们在表格和Atari游戏套件上进行了实证验证。",
    "tldr": "本文研究了TD学习中对表示动态的影响，并发现端到端TD学习在环境可逆的情况下可以严格降低值逼近误差，在环境进一步假设的情况下，我们可以将表示动态连接到转移矩阵的谱分解。从随机生成的奖励中适合多个值函数作为表示学习的有用辅助任务。",
    "en_tdlr": "This article investigates the impact of representation dynamics under TD-learning and finds that end-to-end TD-learning strictly decreases value approximation error over time when environments are reversible. In addition, the authors connect the representation dynamics with the spectral decomposition over the transition matrix and propose fitting multiple value functions from randomly generated rewards as a useful auxiliary task for representation learning."
}