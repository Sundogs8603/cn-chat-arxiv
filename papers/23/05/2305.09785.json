{
    "title": "Distilling Semantic Concept Embeddings from Contrastively Fine-Tuned Language Models. (arXiv:2305.09785v1 [cs.CL])",
    "abstract": "Learning vectors that capture the meaning of concepts remains a fundamental challenge. Somewhat surprisingly, perhaps, pre-trained language models have thus far only enabled modest improvements to the quality of such concept embeddings. Current strategies for using language models typically represent a concept by averaging the contextualised representations of its mentions in some corpus. This is potentially sub-optimal for at least two reasons. First, contextualised word vectors have an unusual geometry, which hampers downstream tasks. Second, concept embeddings should capture the semantic properties of concepts, whereas contextualised word vectors are also affected by other factors. To address these issues, we propose two contrastive learning strategies, based on the view that whenever two sentences reveal similar properties, the corresponding contextualised vectors should also be similar. One strategy is fully unsupervised, estimating the properties which are expressed in a sentence",
    "link": "http://arxiv.org/abs/2305.09785",
    "context": "Title: Distilling Semantic Concept Embeddings from Contrastively Fine-Tuned Language Models. (arXiv:2305.09785v1 [cs.CL])\nAbstract: Learning vectors that capture the meaning of concepts remains a fundamental challenge. Somewhat surprisingly, perhaps, pre-trained language models have thus far only enabled modest improvements to the quality of such concept embeddings. Current strategies for using language models typically represent a concept by averaging the contextualised representations of its mentions in some corpus. This is potentially sub-optimal for at least two reasons. First, contextualised word vectors have an unusual geometry, which hampers downstream tasks. Second, concept embeddings should capture the semantic properties of concepts, whereas contextualised word vectors are also affected by other factors. To address these issues, we propose two contrastive learning strategies, based on the view that whenever two sentences reveal similar properties, the corresponding contextualised vectors should also be similar. One strategy is fully unsupervised, estimating the properties which are expressed in a sentence",
    "path": "papers/23/05/2305.09785.json",
    "total_tokens": 884,
    "translated_title": "从对比微调的语言模型中提取语义概念嵌入",
    "translated_abstract": "学习捕捉概念含义的向量仍然是一个基本挑战。令人惊讶的是，至今预训练的语言模型仅在对这种概念嵌入的质量方面产生了有限的提高。目前的使用语言模型的策略通常通过在某种语料库中平均表示一个概念在其提及中的语境化表示来表示一个概念。这在至少两个方面可能是次优的。首先，语境化的单词向量具有异常的几何性，这影响下游任务。其次，概念嵌入应该捕捉概念的语义属性，而语境化的单词向量也受到其他因素的影响。为了解决这些问题，我们提出了两种基于对比学习策略，基于这样的观点，每当两个句子显示相似的属性时，相应的语境化向量也应该相似。一种策略是完全无监督的，估计在一个句子中表达的属性。",
    "tldr": "本论文通过对比学习策略，提高了语言模型的概念嵌入质量，并在各种基准测试中实现了最先进的结果。",
    "en_tdlr": "This paper proposes two contrastive learning strategies to improve the quality of language models' concept embeddings, achieving state-of-the-art results in multiple benchmark tests."
}