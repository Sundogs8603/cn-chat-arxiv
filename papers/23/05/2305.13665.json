{
    "title": "Dual Focal Loss for Calibration. (arXiv:2305.13665v1 [cs.CV])",
    "abstract": "The use of deep neural networks in real-world applications require well-calibrated networks with confidence scores that accurately reflect the actual probability. However, it has been found that these networks often provide over-confident predictions, which leads to poor calibration. Recent efforts have sought to address this issue by focal loss to reduce over-confidence, but this approach can also lead to under-confident predictions. While different variants of focal loss have been explored, it is difficult to find a balance between over-confidence and under-confidence. In our work, we propose a new loss function by focusing on dual logits. Our method not only considers the ground truth logit, but also take into account the highest logit ranked after the ground truth logit. By maximizing the gap between these two logits, our proposed dual focal loss can achieve a better balance between over-confidence and under-confidence. We provide theoretical evidence to support our approach and de",
    "link": "http://arxiv.org/abs/2305.13665",
    "context": "Title: Dual Focal Loss for Calibration. (arXiv:2305.13665v1 [cs.CV])\nAbstract: The use of deep neural networks in real-world applications require well-calibrated networks with confidence scores that accurately reflect the actual probability. However, it has been found that these networks often provide over-confident predictions, which leads to poor calibration. Recent efforts have sought to address this issue by focal loss to reduce over-confidence, but this approach can also lead to under-confident predictions. While different variants of focal loss have been explored, it is difficult to find a balance between over-confidence and under-confidence. In our work, we propose a new loss function by focusing on dual logits. Our method not only considers the ground truth logit, but also take into account the highest logit ranked after the ground truth logit. By maximizing the gap between these two logits, our proposed dual focal loss can achieve a better balance between over-confidence and under-confidence. We provide theoretical evidence to support our approach and de",
    "path": "papers/23/05/2305.13665.json",
    "total_tokens": 948,
    "translated_title": "双聚焦损失用于置信度校准",
    "translated_abstract": "在现实世界的深度神经网络应用中，需要具有良好校准的网络，其置信度分数能够准确反映实际概率。然而，这些网络通常提供过度自信的预测，从而导致校准不佳。最近的研究努力解决这个问题通过焦点损失来降低过度自信，但这种方法也可能导致低自信的预测。虽然已经探索了焦点损失的不同变体，但很难找到过度自信和低自信之间的平衡点。在我们的工作中，我们提出了一种新的损失函数，专注于双重逻辑。我们的方法不仅考虑基于实际情况的logit，而且还考虑在基于实际情况之后评级最高的logit。通过最大化这两个logit之间的差距，我们提出的双聚焦损失可以在过度自信和低自信之间达到更好的平衡。我们提供理论证据来支持我们的方法，并描述在图像分类和语义分割任务上的实验结果，这表明我们的方法实现了最先进的校准性能。",
    "tldr": "本文提出了一种新的损失函数——双聚焦损失，可以在过度自信和低自信之间达到更好的平衡，实现了最先进的校准性能。",
    "en_tdlr": "This paper proposes a new loss function, namely the dual focal loss, to achieve a better balance between over-confidence and under-confidence in deep neural networks, and achieves state-of-the-art calibration performance on image classification and semantic segmentation tasks."
}