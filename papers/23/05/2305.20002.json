{
    "title": "Representer Point Selection for Explaining Regularized High-dimensional Models. (arXiv:2305.20002v2 [cs.LG] UPDATED)",
    "abstract": "We introduce a novel class of sample-based explanations we term high-dimensional representers, that can be used to explain the predictions of a regularized high-dimensional model in terms of importance weights for each of the training samples. Our workhorse is a novel representer theorem for general regularized high-dimensional models, which decomposes the model prediction in terms of contributions from each of the training samples: with positive (negative) values corresponding to positive (negative) impact training samples to the model's prediction. We derive consequences for the canonical instances of $\\ell_1$ regularized sparse models, and nuclear norm regularized low-rank models. As a case study, we further investigate the application of low-rank models in the context of collaborative filtering, where we instantiate high-dimensional representers for specific popular classes of models. Finally, we study the empirical performance of our proposed methods on three real-world binary cla",
    "link": "http://arxiv.org/abs/2305.20002",
    "context": "Title: Representer Point Selection for Explaining Regularized High-dimensional Models. (arXiv:2305.20002v2 [cs.LG] UPDATED)\nAbstract: We introduce a novel class of sample-based explanations we term high-dimensional representers, that can be used to explain the predictions of a regularized high-dimensional model in terms of importance weights for each of the training samples. Our workhorse is a novel representer theorem for general regularized high-dimensional models, which decomposes the model prediction in terms of contributions from each of the training samples: with positive (negative) values corresponding to positive (negative) impact training samples to the model's prediction. We derive consequences for the canonical instances of $\\ell_1$ regularized sparse models, and nuclear norm regularized low-rank models. As a case study, we further investigate the application of low-rank models in the context of collaborative filtering, where we instantiate high-dimensional representers for specific popular classes of models. Finally, we study the empirical performance of our proposed methods on three real-world binary cla",
    "path": "papers/23/05/2305.20002.json",
    "total_tokens": 875,
    "translated_title": "解释正则化的高维模型的代表点选择",
    "translated_abstract": "我们介绍了一种新颖的基于样本的解释方法，称为高维代表点，可以用于解释正则化的高维模型对每个训练样本的重要性权重。我们提出了一个适用于一般正则化高维模型的新型代表定理，该定理将模型的预测分解为每个训练样本的贡献：正（负）值对应于正（负）影响训练样本对模型预测的影响。我们推导了$\\ell_1$正则化稀疏模型和核范数正则化低秩模型的经典实例的结果。作为一个案例研究，我们进一步研究了在协同过滤的背景下低秩模型的应用，其中我们为特定的流行模型类实例化了高维代表点。最后，我们在三个实际的二进制分类问题上研究了我们提出的方法的实证性表现。",
    "tldr": "我们提出了一种解释高维模型预测的新方法，通过代表点选择来解释每个训练样本的重要性权重。我们的方法可以适用于各种正则化模型，并在协同过滤领域中有具体应用。",
    "en_tdlr": "We propose a new method to explain predictions of high-dimensional models by selecting representative points and assigning importance weights to each training sample. Our method can be applied to various regularized models and has specific applications in collaborative filtering."
}