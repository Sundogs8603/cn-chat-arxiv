{
    "title": "The Hessian perspective into the Nature of Convolutional Neural Networks. (arXiv:2305.09088v1 [cs.LG])",
    "abstract": "While Convolutional Neural Networks (CNNs) have long been investigated and applied, as well as theorized, we aim to provide a slightly different perspective into their nature -- through the perspective of their Hessian maps. The reason is that the loss Hessian captures the pairwise interaction of parameters and therefore forms a natural ground to probe how the architectural aspects of CNN get manifested in its structure and properties. We develop a framework relying on Toeplitz representation of CNNs, and then utilize it to reveal the Hessian structure and, in particular, its rank. We prove tight upper bounds (with linear activations), which closely follow the empirical trend of the Hessian rank and hold in practice in more general settings. Overall, our work generalizes and establishes the key insight that, even in CNNs, the Hessian rank grows as the square root of the number of parameters.",
    "link": "http://arxiv.org/abs/2305.09088",
    "context": "Title: The Hessian perspective into the Nature of Convolutional Neural Networks. (arXiv:2305.09088v1 [cs.LG])\nAbstract: While Convolutional Neural Networks (CNNs) have long been investigated and applied, as well as theorized, we aim to provide a slightly different perspective into their nature -- through the perspective of their Hessian maps. The reason is that the loss Hessian captures the pairwise interaction of parameters and therefore forms a natural ground to probe how the architectural aspects of CNN get manifested in its structure and properties. We develop a framework relying on Toeplitz representation of CNNs, and then utilize it to reveal the Hessian structure and, in particular, its rank. We prove tight upper bounds (with linear activations), which closely follow the empirical trend of the Hessian rank and hold in practice in more general settings. Overall, our work generalizes and establishes the key insight that, even in CNNs, the Hessian rank grows as the square root of the number of parameters.",
    "path": "papers/23/05/2305.09088.json",
    "total_tokens": 842,
    "translated_title": "基于Hessian映射的卷积神经网络本质的新视角",
    "translated_abstract": "尽管卷积神经网络(CNNs)一直被研究、应用和理论化，我们的目的是从它们的Hessian映射的角度提供一个稍微不同的观点，因为损失的Hessian捕捉了参数的成对交互，因此形成了一个自然的基础来探索CNN的架构方面如何表现出它的结构和性质。我们开发了一个依赖于CNN的Toeplitz表示的框架，并利用它来揭示Hessian结构，特别是它的秩。我们证明了紧密的上界（使用线性激活），它们紧密地遵循了Hessian秩的经验趋势，并在更一般的设置中保持在实践中。总的来说，我们的工作概括和确认了一个关键的洞见，即即使在CNNs中，Hessian秩随着参数数量的增长而呈现出平方根增长。",
    "tldr": "本文基于Hessian映射，揭示了CNN结构和性质的本质，并证明了Hessian秩随着参数数量的增长而呈现出平方根增长。",
    "en_tdlr": "This paper provides a new perspective into the nature of Convolutional Neural Networks through their Hessian maps, proving that the Hessian rank grows as the square root of the number of parameters."
}