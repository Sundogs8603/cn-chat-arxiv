{
    "title": "Beyond Reward: Offline Preference-guided Policy Optimization. (arXiv:2305.16217v2 [cs.LG] UPDATED)",
    "abstract": "This study focuses on the topic of offline preference-based reinforcement learning (PbRL), a variant of conventional reinforcement learning that dispenses with the need for online interaction or specification of reward functions. Instead, the agent is provided with fixed offline trajectories and human preferences between pairs of trajectories to extract the dynamics and task information, respectively. Since the dynamics and task information are orthogonal, a naive approach would involve using preference-based reward learning followed by an off-the-shelf offline RL algorithm. However, this requires the separate learning of a scalar reward function, which is assumed to be an information bottleneck of the learning process. To address this issue, we propose the offline preference-guided policy optimization (OPPO) paradigm, which models offline trajectories and preferences in a one-step process, eliminating the need for separately learning a reward function. OPPO achieves this by introducin",
    "link": "http://arxiv.org/abs/2305.16217",
    "context": "Title: Beyond Reward: Offline Preference-guided Policy Optimization. (arXiv:2305.16217v2 [cs.LG] UPDATED)\nAbstract: This study focuses on the topic of offline preference-based reinforcement learning (PbRL), a variant of conventional reinforcement learning that dispenses with the need for online interaction or specification of reward functions. Instead, the agent is provided with fixed offline trajectories and human preferences between pairs of trajectories to extract the dynamics and task information, respectively. Since the dynamics and task information are orthogonal, a naive approach would involve using preference-based reward learning followed by an off-the-shelf offline RL algorithm. However, this requires the separate learning of a scalar reward function, which is assumed to be an information bottleneck of the learning process. To address this issue, we propose the offline preference-guided policy optimization (OPPO) paradigm, which models offline trajectories and preferences in a one-step process, eliminating the need for separately learning a reward function. OPPO achieves this by introducin",
    "path": "papers/23/05/2305.16217.json",
    "total_tokens": 728,
    "translated_title": "超越奖励：离线偏好引导策略优化",
    "translated_abstract": "本研究关注离线基于偏好的强化学习(PbRL)，它是传统强化学习的一种变体，不需要在线交互或指定奖励函数。相反，代理被提供了固定的离线轨迹和人类偏好之间的信息来分别提取动态和任务信息。为了解决学习过程中的信息瓶颈，我们提出了离线偏好引导策略优化(OPPO)范例，它在一个步骤中对离线轨迹和偏好进行建模，消除了分别学习奖励函数的需求。",
    "tldr": "本研究提出了离线偏好引导策略优化(OPPO)范例，它消除了传统强化学习中奖励函数的需求。",
    "en_tdlr": "This study proposes the offline preference-guided policy optimization (OPPO) paradigm, which eliminates the need for a reward function in traditional reinforcement learning."
}