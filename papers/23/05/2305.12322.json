{
    "title": "Learning Large Graph Property Prediction via Graph Segment Training. (arXiv:2305.12322v2 [cs.LG] UPDATED)",
    "abstract": "Learning to predict properties of large graphs is challenging because each prediction requires the knowledge of an entire graph, while the amount of memory available during training is bounded. Here we propose Graph Segment Training (GST), a general framework that utilizes a divide-and-conquer approach to allow learning large graph property prediction with a constant memory footprint. GST first divides a large graph into segments and then backpropagates through only a few segments sampled per training iteration. We refine the GST paradigm by introducing a historical embedding table to efficiently obtain embeddings for segments not sampled for backpropagation. To mitigate the staleness of historical embeddings, we design two novel techniques. First, we finetune the prediction head to fix the input distribution shift. Second, we introduce Stale Embedding Dropout to drop some stale embeddings during training to reduce bias. We evaluate our complete method GST-EFD (with all the techniques ",
    "link": "http://arxiv.org/abs/2305.12322",
    "context": "Title: Learning Large Graph Property Prediction via Graph Segment Training. (arXiv:2305.12322v2 [cs.LG] UPDATED)\nAbstract: Learning to predict properties of large graphs is challenging because each prediction requires the knowledge of an entire graph, while the amount of memory available during training is bounded. Here we propose Graph Segment Training (GST), a general framework that utilizes a divide-and-conquer approach to allow learning large graph property prediction with a constant memory footprint. GST first divides a large graph into segments and then backpropagates through only a few segments sampled per training iteration. We refine the GST paradigm by introducing a historical embedding table to efficiently obtain embeddings for segments not sampled for backpropagation. To mitigate the staleness of historical embeddings, we design two novel techniques. First, we finetune the prediction head to fix the input distribution shift. Second, we introduce Stale Embedding Dropout to drop some stale embeddings during training to reduce bias. We evaluate our complete method GST-EFD (with all the techniques ",
    "path": "papers/23/05/2305.12322.json",
    "total_tokens": 1049,
    "translated_title": "通过图形段训练学习大型图形属性预测",
    "translated_abstract": "学习预测大型图的属性具有挑战性，因为每个预测都需要整个图的知识，而在训练期间可用的内存量是有限的。在这里，我们提出了一种名为Graph Segment Training（GST）的通用框架，利用分治方法允许使用恒定的内存占用量来学习大型图形属性预测。 GST首先将大型图形划分为段，然后通过训练迭代中仅对几个段进行反向传播。我们通过引入历史嵌入表来改进GST范例，以有效地获取未进行反向传播的段的嵌入。为了减轻历史嵌入的过时性，我们设计了两种新技术。首先，我们微调预测头以修复输入分布移位。其次，我们引入“Stale Embedding Dropout”来在训练期间降低偏差，从而丢弃一些过时的嵌入。我们对大型图形属性预测任务进行了评估，包括化合物分类和蛋白质相互作用预测。我们提出的方法GST-EFD（包含所有技术）优于几个最先进的基准，并在准确性，存储器消耗和运行时效率方面实现了出色的性能。",
    "tldr": "本文提出了一种名为Graph Segment Training的新方法，通过分治法允许使用恒定的内存消耗来学习大型图形属性预测。该方法被评估在几项大型图形属性预测任务上，表现出优于几个最先进基准的出色性能。",
    "en_tdlr": "This paper proposes a new method called Graph Segment Training, which allows learning large graph property prediction with a constant memory footprint by utilizing divide-and-conquer approach. The proposed method outperforms several state-of-the-art baselines on large graph property prediction tasks and achieves excellent performance in terms of accuracy, memory consumption and runtime efficiency."
}