{
    "title": "GIFT: Graph-Induced Fine-Tuning for Multi-Party Conversation Understanding. (arXiv:2305.09360v1 [cs.CL])",
    "abstract": "Addressing the issues of who saying what to whom in multi-party conversations (MPCs) has recently attracted a lot of research attention. However, existing methods on MPC understanding typically embed interlocutors and utterances into sequential information flows, or utilize only the superficial of inherent graph structures in MPCs. To this end, we present a plug-and-play and lightweight method named graph-induced fine-tuning (GIFT) which can adapt various Transformer-based pre-trained language models (PLMs) for universal MPC understanding. In detail, the full and equivalent connections among utterances in regular Transformer ignore the sparse but distinctive dependency of an utterance on another in MPCs. To distinguish different relationships between utterances, four types of edges are designed to integrate graph-induced signals into attention mechanisms to refine PLMs originally designed for processing sequential texts. We evaluate GIFT by implementing it into three PLMs, and test the",
    "link": "http://arxiv.org/abs/2305.09360",
    "context": "Title: GIFT: Graph-Induced Fine-Tuning for Multi-Party Conversation Understanding. (arXiv:2305.09360v1 [cs.CL])\nAbstract: Addressing the issues of who saying what to whom in multi-party conversations (MPCs) has recently attracted a lot of research attention. However, existing methods on MPC understanding typically embed interlocutors and utterances into sequential information flows, or utilize only the superficial of inherent graph structures in MPCs. To this end, we present a plug-and-play and lightweight method named graph-induced fine-tuning (GIFT) which can adapt various Transformer-based pre-trained language models (PLMs) for universal MPC understanding. In detail, the full and equivalent connections among utterances in regular Transformer ignore the sparse but distinctive dependency of an utterance on another in MPCs. To distinguish different relationships between utterances, four types of edges are designed to integrate graph-induced signals into attention mechanisms to refine PLMs originally designed for processing sequential texts. We evaluate GIFT by implementing it into three PLMs, and test the",
    "path": "papers/23/05/2305.09360.json",
    "total_tokens": 853,
    "translated_title": "GIFT: 基于图感知微调的多方对话理解",
    "translated_abstract": "最近，关于谁与谁在多方对话中说了什么的问题已经引起了很多研究的关注。然而，现有的多方对话理解方法通常将说话者和话语嵌入到顺序信息流中，或仅利用多方对话中固有图结构的表层信息。为此，我们提出了一种名为图感知微调（GIFT）的即插即用轻量级方法，可以适应各种基于Transformer预训练语言模型（PLMs）的通用多方对话理解。具体地，在普通Transformer中，话语之间的全等连接会忽略一个话语对另一个话语的稀疏但有区别的依赖关系。为了区分话语之间的不同关系，设计了四种类型的边缘以将图感知信号集成到注意机制中，以改进最初设计用于处理顺序文本的PLMs。我们通过将GIFT实现到三个PLMs并对其进行测试来评估GIFT。",
    "tldr": "GIFT是一个适用于多方对话理解的方法，通过设计四种类型的边缘将图感知信息集成到注意力机制中，改进了原始的顺序文本处理的PLM。",
    "en_tdlr": "GIFT is a method for multi-party conversation understanding that integrates graph-induced signals into attention mechanisms through designing four types of edges, and improves the PLMs originally designed for processing sequential texts."
}