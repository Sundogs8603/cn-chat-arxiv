{
    "title": "OpenViVQA: Task, Dataset, and Multimodal Fusion Models for Visual Question Answering in Vietnamese. (arXiv:2305.04183v1 [cs.CL])",
    "abstract": "In recent years, visual question answering (VQA) has attracted attention from the research community because of its highly potential applications (such as virtual assistance on intelligent cars, assistant devices for blind people, or information retrieval from document images using natural language as queries) and challenge. The VQA task requires methods that have the ability to fuse the information from questions and images to produce appropriate answers. Neural visual question answering models have achieved tremendous growth on large-scale datasets which are mostly for resource-rich languages such as English. However, available datasets narrow the VQA task as the answers selection task or answer classification task. We argue that this form of VQA is far from human ability and eliminates the challenge of the answering aspect in the VQA task by just selecting answers rather than generating them. In this paper, we introduce the OpenViVQA (Open-domain Vietnamese Visual Question Answering",
    "link": "http://arxiv.org/abs/2305.04183",
    "context": "Title: OpenViVQA: Task, Dataset, and Multimodal Fusion Models for Visual Question Answering in Vietnamese. (arXiv:2305.04183v1 [cs.CL])\nAbstract: In recent years, visual question answering (VQA) has attracted attention from the research community because of its highly potential applications (such as virtual assistance on intelligent cars, assistant devices for blind people, or information retrieval from document images using natural language as queries) and challenge. The VQA task requires methods that have the ability to fuse the information from questions and images to produce appropriate answers. Neural visual question answering models have achieved tremendous growth on large-scale datasets which are mostly for resource-rich languages such as English. However, available datasets narrow the VQA task as the answers selection task or answer classification task. We argue that this form of VQA is far from human ability and eliminates the challenge of the answering aspect in the VQA task by just selecting answers rather than generating them. In this paper, we introduce the OpenViVQA (Open-domain Vietnamese Visual Question Answering",
    "path": "papers/23/05/2305.04183.json",
    "total_tokens": 1148,
    "translated_title": "OpenViVQA：越南语视觉问答任务、数据集和多模态融合模型",
    "translated_abstract": "在近年来，视觉问答 (VQA) 由于其高潜在应用（例如智能车上的虚拟助手、盲人辅助装置或使用自然语言作为查询的文档图像信息检索等）和挑战性而引起研究界的关注。VQA 任务需要具有从问题和图像中融合信息以生成适当答案的方法。 神经视觉问答模型在大规模数据集上的应用已经取得了巨大的发展，这些数据集多数为资源丰富的语言，如英语。然而，现有的数据集将 VQA 任务缩小为答案选择任务或答案分类任务，而这种形式的 VQA 远远不能与人类能力相提并论，通过仅选择答案而不是生成答案来消除 VQA 任务中的回答方面的挑战。在本文中，我们引入了 OpenViVQA （开放领域越南语视觉问答）任务和数据集，这旨在推广越南语和其他低资源语言中的 VQA 研究。我们还提出了两种多模态融合模型，这些模型使用注意机制组合不同的问题和图像特征以生成 OpenViVQA 任务的答案。实验结果证明了所提出模型的有效性以及 OpenViVQA 数据集在低资源语言 VQA 的未来研究中的潜力。",
    "tldr": "本文介绍了针对低资源语言的视觉问答任务和数据集——OpenViVQA，并提出了两种使用注意机制融合问题和图像特征生成答案的多模态融合模型。实验结果表明了所提出模型的有效性和OpenViVQA数据集在未来低资源语言VQA研究中的潜力。",
    "en_tdlr": "This paper introduces the OpenViVQA task and dataset for visual question answering in low-resource languages, and proposes two multimodal fusion models with attention mechanisms to generate answers. Experimental results demonstrate the effectiveness of the proposed models and the potential of the OpenViVQA dataset for future research on low-resource language VQA."
}