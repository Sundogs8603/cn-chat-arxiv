{
    "title": "Response Generation in Longitudinal Dialogues: Which Knowledge Representation Helps?. (arXiv:2305.15908v1 [cs.CL])",
    "abstract": "Longitudinal Dialogues (LD) are the most challenging type of conversation for human-machine dialogue systems. LDs include the recollections of events, personal thoughts, and emotions specific to each individual in a sparse sequence of dialogue sessions. Dialogue systems designed for LDs should uniquely interact with the users over multiple sessions and long periods of time (e.g. weeks), and engage them in personal dialogues to elaborate on their feelings, thoughts, and real-life events. In this paper, we study the task of response generation in LDs. We evaluate whether general-purpose Pre-trained Language Models (PLM) are appropriate for this purpose. We fine-tune two PLMs, GePpeTto (GPT-2) and iT5, using a dataset of LDs. We experiment with different representations of the personal knowledge extracted from LDs for grounded response generation, including the graph representation of the mentioned events and participants. We evaluate the performance of the models via automatic metrics an",
    "link": "http://arxiv.org/abs/2305.15908",
    "context": "Title: Response Generation in Longitudinal Dialogues: Which Knowledge Representation Helps?. (arXiv:2305.15908v1 [cs.CL])\nAbstract: Longitudinal Dialogues (LD) are the most challenging type of conversation for human-machine dialogue systems. LDs include the recollections of events, personal thoughts, and emotions specific to each individual in a sparse sequence of dialogue sessions. Dialogue systems designed for LDs should uniquely interact with the users over multiple sessions and long periods of time (e.g. weeks), and engage them in personal dialogues to elaborate on their feelings, thoughts, and real-life events. In this paper, we study the task of response generation in LDs. We evaluate whether general-purpose Pre-trained Language Models (PLM) are appropriate for this purpose. We fine-tune two PLMs, GePpeTto (GPT-2) and iT5, using a dataset of LDs. We experiment with different representations of the personal knowledge extracted from LDs for grounded response generation, including the graph representation of the mentioned events and participants. We evaluate the performance of the models via automatic metrics an",
    "path": "papers/23/05/2305.15908.json",
    "total_tokens": 927,
    "translated_title": "长期对话中的回应生成：哪种知识表示有助于？",
    "translated_abstract": "长期对话是人机对话系统面临的最具挑战性的类型之一。长期对话包括个人在稀疏的对话序列中回忆的事件、个人思想和情感等内容。设计用于长期对话的对话系统应该能够在多个对话会话和长时间（例如数周）内与用户进行独特交互，并让他们参与个人对话以阐述他们的感受、思想和真实生活事件。本文研究了长期对话中的回应生成任务。我们评估了通用的预训练语言模型（PLM）是否适合这个任务。我们使用LD数据集微调了两个PLM模型，GePpeTto (GPT-2)和iT5。我们尝试了从LD中提取的个人知识的不同表示形式，包括提到的事件和参与者的图形表示，以获得基于实例的回应生成。我们通过自动指标评估了模型的性能。",
    "tldr": "本文研究了长期对话中的回应生成任务，通过微调GePpeTto(GPT-2)和iT5等PLM模型，并将从LD中提取的个人知识进行不同表示，以获得基于实例的回应生成，以此来解决对话系统面临的挑战。",
    "en_tdlr": "This paper studies the task of response generation in longitudinal dialogues. The authors fine-tune two pre-trained language models, GPT-2 and iT5, using a dataset of longitudinal dialogues, and experiment with different representations of personal knowledge extracted from the dialogues to achieve grounded response generation and address the challenges faced by dialogue systems."
}