{
    "title": "Federated Learning of Gboard Language Models with Differential Privacy. (arXiv:2305.18465v1 [cs.LG])",
    "abstract": "We train language models (LMs) with federated learning (FL) and differential privacy (DP) in the Google Keyboard (Gboard). We apply the DP-Follow-the-Regularized-Leader (DP-FTRL)~\\citep{kairouz21b} algorithm to achieve meaningfully formal DP guarantees without requiring uniform sampling of client devices. To provide favorable privacy-utility trade-offs, we introduce a new client participation criterion and discuss the implication of its configuration in large scale systems. We show how quantile-based clip estimation~\\citep{andrew2019differentially} can be combined with DP-FTRL to adaptively choose the clip norm during training or reduce the hyperparameter tuning in preparation for training. With the help of pretraining on public data, we train and deploy more than twenty Gboard LMs that achieve high utility and $\\rho-$zCDP privacy guarantees with $\\rho \\in (0.2, 2)$, with two models additionally trained with secure aggregation~\\citep{bonawitz2017practical}. We are happy to announce tha",
    "link": "http://arxiv.org/abs/2305.18465",
    "context": "Title: Federated Learning of Gboard Language Models with Differential Privacy. (arXiv:2305.18465v1 [cs.LG])\nAbstract: We train language models (LMs) with federated learning (FL) and differential privacy (DP) in the Google Keyboard (Gboard). We apply the DP-Follow-the-Regularized-Leader (DP-FTRL)~\\citep{kairouz21b} algorithm to achieve meaningfully formal DP guarantees without requiring uniform sampling of client devices. To provide favorable privacy-utility trade-offs, we introduce a new client participation criterion and discuss the implication of its configuration in large scale systems. We show how quantile-based clip estimation~\\citep{andrew2019differentially} can be combined with DP-FTRL to adaptively choose the clip norm during training or reduce the hyperparameter tuning in preparation for training. With the help of pretraining on public data, we train and deploy more than twenty Gboard LMs that achieve high utility and $\\rho-$zCDP privacy guarantees with $\\rho \\in (0.2, 2)$, with two models additionally trained with secure aggregation~\\citep{bonawitz2017practical}. We are happy to announce tha",
    "path": "papers/23/05/2305.18465.json",
    "total_tokens": 1015,
    "translated_title": "《带差分隐私的Gboard语言模型联合学习》",
    "translated_abstract": "本文在谷歌键盘(Gboard)中使用联合学习和差分隐私(DP)训练语言模型(LMs)。我们应用DP-FTRL算法，在不要求对客户设备进行均匀采样的情况下实现了有意义的形式 DP 保证。为了提供有利的隐私-效用交换，我们引入了新的客户参与标准，并讨论了其在大规模系统中的配置影响。我们展示了如何将基于分位数的剪切估计与DP-FTRL相结合，以在训练过程中自适应选择剪切范数或减少超参数调整以准备训练。借助于对公共数据的预训练，我们训练并部署了超过20个Gboard LM，这些模型在$\\rho \\in (0.2, 2)$下实现了高效用和$\\rho-$zCDP隐私保证，其中两个模型还使用了安全合并。",
    "tldr": "本文讨论了在Gboard中使用联合学习和差分隐私(DP)训练语言模型(LMs)的方法，提出了新的客户参与标准，在实现有意义的形式DP保证的同时提供了有利的隐私-效用交换。在对公共数据进行预训练的基础上，我们训练并部署了超过20个LMs以实现高效用和$\\rho-$zCDP隐私保证。",
    "en_tdlr": "This paper discusses the approach of training language models (LMs) with federated learning (FL) and differential privacy (DP) in Gboard, with a new client participation criterion to provide favorable privacy-utility trade-offs. More than 20 high utility and $\\rho-$zCDP private LMs were trained and deployed using pretraining on public data and the DP-Follow-the-Regularized-Leader (DP-FTRL) algorithm."
}