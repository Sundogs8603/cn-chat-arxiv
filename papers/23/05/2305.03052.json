{
    "title": "Tracking through Containers and Occluders in the Wild. (arXiv:2305.03052v1 [cs.CV])",
    "abstract": "Tracking objects with persistence in cluttered and dynamic environments remains a difficult challenge for computer vision systems. In this paper, we introduce $\\textbf{TCOW}$, a new benchmark and model for visual tracking through heavy occlusion and containment. We set up a task where the goal is to, given a video sequence, segment both the projected extent of the target object, as well as the surrounding container or occluder whenever one exists. To study this task, we create a mixture of synthetic and annotated real datasets to support both supervised learning and structured evaluation of model performance under various forms of task variation, such as moving or nested containment. We evaluate two recent transformer-based video models and find that while they can be surprisingly capable of tracking targets under certain settings of task variation, there remains a considerable performance gap before we can claim a tracking model to have acquired a true notion of object permanence.",
    "link": "http://arxiv.org/abs/2305.03052",
    "context": "Title: Tracking through Containers and Occluders in the Wild. (arXiv:2305.03052v1 [cs.CV])\nAbstract: Tracking objects with persistence in cluttered and dynamic environments remains a difficult challenge for computer vision systems. In this paper, we introduce $\\textbf{TCOW}$, a new benchmark and model for visual tracking through heavy occlusion and containment. We set up a task where the goal is to, given a video sequence, segment both the projected extent of the target object, as well as the surrounding container or occluder whenever one exists. To study this task, we create a mixture of synthetic and annotated real datasets to support both supervised learning and structured evaluation of model performance under various forms of task variation, such as moving or nested containment. We evaluate two recent transformer-based video models and find that while they can be surprisingly capable of tracking targets under certain settings of task variation, there remains a considerable performance gap before we can claim a tracking model to have acquired a true notion of object permanence.",
    "path": "papers/23/05/2305.03052.json",
    "total_tokens": 954,
    "translated_title": "在复杂环境中跟踪含容器和遮挡物的目标",
    "translated_abstract": "在杂乱且动态的环境中跟踪具有持久性的目标仍是计算机视觉系统面临的难题。本文介绍了一个新的基准模型 $\\textbf{TCOW}$，用于在重度遮挡和容器中进行视觉跟踪。我们设定了一个任务，即在给定视频序列的情况下，分割出目标物体的投影范围以及周围的容器或遮挡物。为了研究这个任务，我们创建了一组混合的合成和真实数据集，以支持模型在各种任务变化形式下的监督学习和结构化评估。我们评估了两种最新的基于变压器的视频模型，并发现尽管它们在某些任务变化的设置下能够出人意料地跟踪目标，但在我们宣称一个跟踪模型已经获得了真正的对象恒常性概念之前，仍存在相当大的性能差距。",
    "tldr": "本文介绍了一个新的基准模型 $\\textbf{TCOW}$，用于在重度遮挡和容器中进行视觉跟踪。我们创建了一组混合的合成和真实数据集，评估了两种最新的基于变压器的视频模型，并发现它们在某些情况下能够出人意料地跟踪目标，但仍存在相当大的性能差距，必须进一步研究。",
    "en_tdlr": "This paper introduces a new benchmark model TCOW for visual tracking through heavy occlusion and containment. They evaluate two recent transformer-based video models and find a considerable performance gap before claiming the model has acquired a true notion of object permanence."
}