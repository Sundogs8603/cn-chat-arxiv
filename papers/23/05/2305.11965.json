{
    "title": "Not All Semantics are Created Equal: Contrastive Self-supervised Learning with Automatic Temperature Individualization. (arXiv:2305.11965v1 [cs.LG])",
    "abstract": "In this paper, we aim to optimize a contrastive loss with individualized temperatures in a principled and systematic manner for self-supervised learning. The common practice of using a global temperature parameter $\\tau$ ignores the fact that ``not all semantics are created equal\", meaning that different anchor data may have different numbers of samples with similar semantics, especially when data exhibits long-tails. First, we propose a new robust contrastive loss inspired by distributionally robust optimization (DRO), providing us an intuition about the effect of $\\tau$ and a mechanism for automatic temperature individualization. Then, we propose an efficient stochastic algorithm for optimizing the robust contrastive loss with a provable convergence guarantee without using large mini-batch sizes. Theoretical and experimental results show that our algorithm automatically learns a suitable $\\tau$ for each sample. Specifically, samples with frequent semantics use large temperatures to k",
    "link": "http://arxiv.org/abs/2305.11965",
    "context": "Title: Not All Semantics are Created Equal: Contrastive Self-supervised Learning with Automatic Temperature Individualization. (arXiv:2305.11965v1 [cs.LG])\nAbstract: In this paper, we aim to optimize a contrastive loss with individualized temperatures in a principled and systematic manner for self-supervised learning. The common practice of using a global temperature parameter $\\tau$ ignores the fact that ``not all semantics are created equal\", meaning that different anchor data may have different numbers of samples with similar semantics, especially when data exhibits long-tails. First, we propose a new robust contrastive loss inspired by distributionally robust optimization (DRO), providing us an intuition about the effect of $\\tau$ and a mechanism for automatic temperature individualization. Then, we propose an efficient stochastic algorithm for optimizing the robust contrastive loss with a provable convergence guarantee without using large mini-batch sizes. Theoretical and experimental results show that our algorithm automatically learns a suitable $\\tau$ for each sample. Specifically, samples with frequent semantics use large temperatures to k",
    "path": "papers/23/05/2305.11965.json",
    "total_tokens": 864,
    "translated_title": "不是所有的语义都是平等的：具有自定义温度的对比自监督学习",
    "translated_abstract": "本文旨在通过原则性和系统性的方式，优化具有个性化温度的对比损失，用于自监督学习。普遍做法是将全局温度参数τ用于所有数据，忽略了“不是所有的语义都是平等的”这个事实，特别是在数据展示长尾分布时，不同的锚点数据可能具有不同数量的类似语义的样本。我们提出了一种基于分布鲁棒性优化（DRO）的新型鲁棒对比损失，为我们提供了有关τ的影响的直觉和自动温度个性化的机制。然后，我们提出了一种有效的随机算法来优化鲁棒性对比损失，具有可证明的收敛保证，而不需要使用大型小批量大小。理论和实验结果表明，我们的算法自动学习每个样本的合适τ。具体来说，具有频繁语义的样本使用较大温度以保持难度。",
    "tldr": "本文提出了一种具有个性化温度的对比损失用于自监督学习，根据数据分布自动调整温度以使得训练更加有效。",
    "en_tdlr": "This paper proposes a contrastive loss with individualized temperatures for self-supervised learning, which automatically adjusts temperature based on data distribution to improve training effectiveness."
}