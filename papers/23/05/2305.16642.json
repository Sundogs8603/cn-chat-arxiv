{
    "title": "Improving Position Encoding of Transformers for Multivariate Time Series Classification. (arXiv:2305.16642v1 [cs.LG])",
    "abstract": "Transformers have demonstrated outstanding performance in many applications of deep learning. When applied to time series data, transformers require effective position encoding to capture the ordering of the time series data. The efficacy of position encoding in time series analysis is not well-studied and remains controversial, e.g., whether it is better to inject absolute position encoding or relative position encoding, or a combination of them. In order to clarify this, we first review existing absolute and relative position encoding methods when applied in time series classification. We then proposed a new absolute position encoding method dedicated to time series data called time Absolute Position Encoding (tAPE). Our new method incorporates the series length and input embedding dimension in absolute position encoding. Additionally, we propose computationally Efficient implementation of Relative Position Encoding (eRPE) to improve generalisability for time series. We then propose ",
    "link": "http://arxiv.org/abs/2305.16642",
    "context": "Title: Improving Position Encoding of Transformers for Multivariate Time Series Classification. (arXiv:2305.16642v1 [cs.LG])\nAbstract: Transformers have demonstrated outstanding performance in many applications of deep learning. When applied to time series data, transformers require effective position encoding to capture the ordering of the time series data. The efficacy of position encoding in time series analysis is not well-studied and remains controversial, e.g., whether it is better to inject absolute position encoding or relative position encoding, or a combination of them. In order to clarify this, we first review existing absolute and relative position encoding methods when applied in time series classification. We then proposed a new absolute position encoding method dedicated to time series data called time Absolute Position Encoding (tAPE). Our new method incorporates the series length and input embedding dimension in absolute position encoding. Additionally, we propose computationally Efficient implementation of Relative Position Encoding (eRPE) to improve generalisability for time series. We then propose ",
    "path": "papers/23/05/2305.16642.json",
    "total_tokens": 819,
    "translated_title": "改进Transformer在多元时间序列分类中的位置编码",
    "translated_abstract": "Transformer在深度学习的许多应用中表现出了出色的性能。在应用于时间序列数据时，Transformer需要有效的位置编码来捕捉时间序列数据的排序。但位置编码在时间序列分析中的效应尚未经过充分的研究，并且仍存在争议，例如，注入绝对位置编码还是相对位置编码更好，或者两者的组合更好。为了澄清这一点，我们首先回顾了现有的绝对和相对位置编码方法在时间序列分类中的应用。然后，我们提出了一种专门面向时间序列数据的绝对位置编码方法，称为时间绝对位置编码（tAPE）。我们的新方法将序列长度和输入嵌入维度纳入了绝对位置编码中。此外，我们还提出了计算上高效的相对位置编码的实现方法（eRPE），以提高时间序列的概括性。",
    "tldr": "本文提出了一种新的绝对位置编码方法tAPE，以及一种相对位置编码的计算上高效的实现方法eRPE，旨在改进Transformer在多元时间序列分类中的性能。",
    "en_tdlr": "The paper proposes a new absolute position encoding method called tAPE and a computationally efficient implementation of relative position encoding called eRPE to improve the performance of Transformers in multivariate time series classification."
}