{
    "title": "Manifold Regularization for Memory-Efficient Training of Deep Neural Networks. (arXiv:2305.17119v1 [cs.LG])",
    "abstract": "One of the prevailing trends in the machine- and deep-learning community is to gravitate towards the use of increasingly larger models in order to keep pushing the state-of-the-art performance envelope. This tendency makes access to the associated technologies more difficult for the average practitioner and runs contrary to the desire to democratize knowledge production in the field. In this paper, we propose a framework for achieving improved memory efficiency in the process of learning traditional neural networks by leveraging inductive-bias-driven network design principles and layer-wise manifold-oriented regularization objectives. Use of the framework results in improved absolute performance and empirical generalization error relative to traditional learning techniques. We provide empirical validation of the framework, including qualitative and quantitative evidence of its effectiveness on two standard image datasets, namely CIFAR-10 and CIFAR-100. The proposed framework can be sea",
    "link": "http://arxiv.org/abs/2305.17119",
    "context": "Title: Manifold Regularization for Memory-Efficient Training of Deep Neural Networks. (arXiv:2305.17119v1 [cs.LG])\nAbstract: One of the prevailing trends in the machine- and deep-learning community is to gravitate towards the use of increasingly larger models in order to keep pushing the state-of-the-art performance envelope. This tendency makes access to the associated technologies more difficult for the average practitioner and runs contrary to the desire to democratize knowledge production in the field. In this paper, we propose a framework for achieving improved memory efficiency in the process of learning traditional neural networks by leveraging inductive-bias-driven network design principles and layer-wise manifold-oriented regularization objectives. Use of the framework results in improved absolute performance and empirical generalization error relative to traditional learning techniques. We provide empirical validation of the framework, including qualitative and quantitative evidence of its effectiveness on two standard image datasets, namely CIFAR-10 and CIFAR-100. The proposed framework can be sea",
    "path": "papers/23/05/2305.17119.json",
    "total_tokens": 877,
    "translated_title": "深度神经网络的内存高效训练的流形正则化",
    "translated_abstract": "机器和深度学习领域中一种主流趋势是，采用越来越大的模型以推动最先进的性能。然而，这种趋势使普通从业者难以接触相关技术，不利于民主化知识的生产。本文提出了一种框架，通过利用诱导偏差网络设计原则和基于层的流形正则化目标，实现传统神经网络学习过程中的内存效率提高。使用该框架可以相对于传统学习技术获得更好的绝对性能和实证一般化误差。我们提供了该框架的实证验证，包括其在两个标准图像数据集，即CIFAR-10和CIFAR-100上的有效性的定性和定量证据。该提议的框架可以使用。",
    "tldr": "本文提出了一种利用流形正则化目标和诱导偏差网络设计原则的框架来实现深度神经网络的内存高效训练，相对于传统学习技术可获得更好的绝对性能和实证一般化误差，经实验验证有效。"
}