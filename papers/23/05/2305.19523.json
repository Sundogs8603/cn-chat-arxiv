{
    "title": "Explanations as Features: LLM-Based Features for Text-Attributed Graphs. (arXiv:2305.19523v1 [cs.LG])",
    "abstract": "Representation learning on text-attributed graphs (TAGs) has become a critical research problem in recent years. A typical example of a TAG is a paper citation graph, where the text of each paper serves as node attributes. Most graph neural network (GNN) pipelines handle these text attributes by transforming them into shallow or hand-crafted features, such as skip-gram or bag-of-words features. Recent efforts have focused on enhancing these pipelines with language models. With the advent of powerful large language models (LLMs) such as GPT, which demonstrate an ability to reason and to utilize general knowledge, there is a growing need for techniques which combine the textual modelling abilities of LLMs with the structural learning capabilities of GNNs. Hence, in this work, we focus on leveraging LLMs to capture textual information as features, which can be used to boost GNN performance on downstream tasks. A key innovation is our use of \\emph{explanations as features}: we prompt an LL",
    "link": "http://arxiv.org/abs/2305.19523",
    "context": "Title: Explanations as Features: LLM-Based Features for Text-Attributed Graphs. (arXiv:2305.19523v1 [cs.LG])\nAbstract: Representation learning on text-attributed graphs (TAGs) has become a critical research problem in recent years. A typical example of a TAG is a paper citation graph, where the text of each paper serves as node attributes. Most graph neural network (GNN) pipelines handle these text attributes by transforming them into shallow or hand-crafted features, such as skip-gram or bag-of-words features. Recent efforts have focused on enhancing these pipelines with language models. With the advent of powerful large language models (LLMs) such as GPT, which demonstrate an ability to reason and to utilize general knowledge, there is a growing need for techniques which combine the textual modelling abilities of LLMs with the structural learning capabilities of GNNs. Hence, in this work, we focus on leveraging LLMs to capture textual information as features, which can be used to boost GNN performance on downstream tasks. A key innovation is our use of \\emph{explanations as features}: we prompt an LL",
    "path": "papers/23/05/2305.19523.json",
    "total_tokens": 932,
    "tldr": "本文关注的问题是如何结合强大的语言模型（LLMs）和图神经网络（GNNs）的结构学习能力，从而在图上学习更好的表示。我们使用LLMs作为特征，新颖之处在于我们使用“解释作为特征”的方法。",
    "en_tdlr": "This paper focuses on how to combine the strong textual modeling capabilities of large language models (LLMs) with the structural learning abilities of graph neural networks (GNNs) to learn better representations on graphs. The authors propose using LLMs as features and a key innovation is their use of \"explanations as features\" prompt the LLM to generate additional features."
}