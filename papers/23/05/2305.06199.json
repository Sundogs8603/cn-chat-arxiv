{
    "title": "Computationally Efficient and Statistically Optimal Robust High-Dimensional Linear Regression. (arXiv:2305.06199v1 [math.ST])",
    "abstract": "High-dimensional linear regression under heavy-tailed noise or outlier corruption is challenging, both computationally and statistically. Convex approaches have been proven statistically optimal but suffer from high computational costs, especially since the robust loss functions are usually non-smooth. More recently, computationally fast non-convex approaches via sub-gradient descent are proposed, which, unfortunately, fail to deliver a statistically consistent estimator even under sub-Gaussian noise. In this paper, we introduce a projected sub-gradient descent algorithm for both the sparse linear regression and low-rank linear regression problems. The algorithm is not only computationally efficient with linear convergence but also statistically optimal, be the noise Gaussian or heavy-tailed with a finite 1 + epsilon moment. The convergence theory is established for a general framework and its specific applications to absolute loss, Huber loss and quantile loss are investigated. Compar",
    "link": "http://arxiv.org/abs/2305.06199",
    "context": "Title: Computationally Efficient and Statistically Optimal Robust High-Dimensional Linear Regression. (arXiv:2305.06199v1 [math.ST])\nAbstract: High-dimensional linear regression under heavy-tailed noise or outlier corruption is challenging, both computationally and statistically. Convex approaches have been proven statistically optimal but suffer from high computational costs, especially since the robust loss functions are usually non-smooth. More recently, computationally fast non-convex approaches via sub-gradient descent are proposed, which, unfortunately, fail to deliver a statistically consistent estimator even under sub-Gaussian noise. In this paper, we introduce a projected sub-gradient descent algorithm for both the sparse linear regression and low-rank linear regression problems. The algorithm is not only computationally efficient with linear convergence but also statistically optimal, be the noise Gaussian or heavy-tailed with a finite 1 + epsilon moment. The convergence theory is established for a general framework and its specific applications to absolute loss, Huber loss and quantile loss are investigated. Compar",
    "path": "papers/23/05/2305.06199.json",
    "total_tokens": 930,
    "translated_title": "高维稳健线性回归的计算高效和统计优化研究",
    "translated_abstract": "在重尾噪声或异常值污染下进行高维线性回归具有挑战性，无论是计算上还是统计上。凸优化方法已被证明在统计上是最优的，但通常由于鲁棒损失函数是非光滑的而产生高计算成本。最近，通过子梯度下降提出了计算速度快的非凸优化方法，但即使在子高斯噪声下，这些方法也无法提供统计一致估计。本文介绍了一种用于稀疏线性回归和低秩线性回归问题的投影子梯度下降算法。该算法不仅具有线性收敛的计算效率，而且在噪声为高斯噪声，或具有有限1 + epsilon矩的重尾噪声下，也具有统计优化的性质。收敛理论适用于一个通用框架，并研究了其对绝对值损失、Huber损失和分位数损失的具体应用。",
    "tldr": "本文介绍了一种计算高效且统计优化的稀疏线性回归和低秩线性回归方法，适用于高维数据下的异常噪声情况。",
    "en_tdlr": "This paper presents a computationally efficient and statistically optimal method for sparse and low-rank linear regression, which is robust under heavy-tailed noise or outlier corruption in high-dimensional data. The proposed projected sub-gradient descent algorithm guarantees linear convergence and statistical consistency for various robust loss functions including absolute loss, Huber loss, and quantile loss, under both Gaussian and heavy-tailed noise with finite 1 + epsilon moment."
}