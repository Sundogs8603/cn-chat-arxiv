{
    "title": "Backdoor Learning on Sequence to Sequence Models. (arXiv:2305.02424v1 [cs.CL])",
    "abstract": "Backdoor learning has become an emerging research area towards building a trustworthy machine learning system. While a lot of works have studied the hidden danger of backdoor attacks in image or text classification, there is a limited understanding of the model's robustness on backdoor attacks when the output space is infinite and discrete. In this paper, we study a much more challenging problem of testing whether sequence-to-sequence (seq2seq) models are vulnerable to backdoor attacks. Specifically, we find by only injecting 0.2\\% samples of the dataset, we can cause the seq2seq model to generate the designated keyword and even the whole sentence. Furthermore, we utilize Byte Pair Encoding (BPE) to create multiple new triggers, which brings new challenges to backdoor detection since these backdoors are not static. Extensive experiments on machine translation and text summarization have been conducted to show our proposed methods could achieve over 90\\% attack success rate on multiple ",
    "link": "http://arxiv.org/abs/2305.02424",
    "context": "Title: Backdoor Learning on Sequence to Sequence Models. (arXiv:2305.02424v1 [cs.CL])\nAbstract: Backdoor learning has become an emerging research area towards building a trustworthy machine learning system. While a lot of works have studied the hidden danger of backdoor attacks in image or text classification, there is a limited understanding of the model's robustness on backdoor attacks when the output space is infinite and discrete. In this paper, we study a much more challenging problem of testing whether sequence-to-sequence (seq2seq) models are vulnerable to backdoor attacks. Specifically, we find by only injecting 0.2\\% samples of the dataset, we can cause the seq2seq model to generate the designated keyword and even the whole sentence. Furthermore, we utilize Byte Pair Encoding (BPE) to create multiple new triggers, which brings new challenges to backdoor detection since these backdoors are not static. Extensive experiments on machine translation and text summarization have been conducted to show our proposed methods could achieve over 90\\% attack success rate on multiple ",
    "path": "papers/23/05/2305.02424.json",
    "total_tokens": 968,
    "translated_title": "序列到序列模型的后门学习",
    "translated_abstract": "后门攻击已成为构建可信机器学习系统的新兴研究领域。虽然很多研究已经研究了对图像或文本分类的后门攻击的隐藏危险，但对于输出空间为无限和离散的模型在后门攻击下的鲁棒性却了解有限。本文探讨了一个更具挑战性的问题，即测试序列到序列(seq2seq)模型是否容易受到后门攻击。具体而言，我们发现只注入0.2％的样本即可使seq2seq模型生成指定的关键词，甚至整个句子。此外，我们利用字节对编码(Byte Pair Encoding, BPE)来创建多个新触发器，这给后门检测带来了新的挑战，因为这些后门是不固定的。我们进行了大量机器翻译和文本摘要实验，展示了我们提出的方法在多个任务上可以达到90％以上的攻击成功率。",
    "tldr": "本文探讨了序列到序列模型对后门攻击的鲁棒性问题，发现只注入0.2％的样本即可让模型生成指定的关键词和整个句子，利用字节对编码创建多个新触发器对后门检测带来了新挑战，提出的方法在多个任务上可以达到90％以上的攻击成功率。",
    "en_tdlr": "This paper investigates the robustness of sequence-to-sequence models against backdoor attacks and finds that by injecting only 0.2% of the dataset, the model can generate designated keywords or even the whole sentence. The use of Byte Pair Encoding creates new challenges for backdoor detection as these backdoors are not static. The proposed methods achieve over 90% attack success rate on multiple tasks."
}