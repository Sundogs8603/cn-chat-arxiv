{
    "title": "Reward-Machine-Guided, Self-Paced Reinforcement Learning. (arXiv:2305.16505v1 [cs.LG])",
    "abstract": "Self-paced reinforcement learning (RL) aims to improve the data efficiency of learning by automatically creating sequences, namely curricula, of probability distributions over contexts. However, existing techniques for self-paced RL fail in long-horizon planning tasks that involve temporally extended behaviors. We hypothesize that taking advantage of prior knowledge about the underlying task structure can improve the effectiveness of self-paced RL. We develop a self-paced RL algorithm guided by reward machines, i.e., a type of finite-state machine that encodes the underlying task structure. The algorithm integrates reward machines in 1) the update of the policy and value functions obtained by any RL algorithm of choice, and 2) the update of the automated curriculum that generates context distributions. Our empirical results evidence that the proposed algorithm achieves optimal behavior reliably even in cases in which existing baselines cannot make any meaningful progress. It also decre",
    "link": "http://arxiv.org/abs/2305.16505",
    "context": "Title: Reward-Machine-Guided, Self-Paced Reinforcement Learning. (arXiv:2305.16505v1 [cs.LG])\nAbstract: Self-paced reinforcement learning (RL) aims to improve the data efficiency of learning by automatically creating sequences, namely curricula, of probability distributions over contexts. However, existing techniques for self-paced RL fail in long-horizon planning tasks that involve temporally extended behaviors. We hypothesize that taking advantage of prior knowledge about the underlying task structure can improve the effectiveness of self-paced RL. We develop a self-paced RL algorithm guided by reward machines, i.e., a type of finite-state machine that encodes the underlying task structure. The algorithm integrates reward machines in 1) the update of the policy and value functions obtained by any RL algorithm of choice, and 2) the update of the automated curriculum that generates context distributions. Our empirical results evidence that the proposed algorithm achieves optimal behavior reliably even in cases in which existing baselines cannot make any meaningful progress. It also decre",
    "path": "papers/23/05/2305.16505.json",
    "total_tokens": 889,
    "translated_title": "奖励机制指导下的自适应强化学习",
    "translated_abstract": "自适应强化学习旨在通过自动创建上下文概率分布序列来提高学习的数据效率。然而，现有的自适应强化学习技术在涉及时间上延长的行为的长期计划任务中失败。我们假设利用关于底层任务结构的先前知识可以提高自适应强化学习的有效性。我们开发了一种自适应强化学习算法，其基于奖励机制来进行指导。该算法将奖励机制整合到1）通过任何选择的强化学习算法获得的策略和价值函数的更新中，以及2）生成上下文分布的自动课程表的更新中。我们的实证结果表明，所提出的算法可靠地实现最优行为，即使是现有基线无法取得任何有意义的进展的情况下也可行。它还降低了在复杂任务中所需的数据量，从而提高了强化学习的数据效率。",
    "tldr": "本研究提出一种基于奖励机制来指导自适应强化学习的算法，该算法可信赖地实现最优行为，在复杂任务中提高了强化学习的数据效率。",
    "en_tdlr": "This study proposes a reward-machine-guided algorithm for self-paced reinforcement learning, which reliably achieves optimal behavior and improves the data efficiency of reinforcement learning in complex tasks."
}