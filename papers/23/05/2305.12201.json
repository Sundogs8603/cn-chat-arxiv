{
    "title": "GraVAC: Adaptive Compression for Communication-Efficient Distributed DL Training. (arXiv:2305.12201v1 [cs.LG])",
    "abstract": "Distributed data-parallel (DDP) training improves overall application throughput as multiple devices train on a subset of data and aggregate updates to produce a globally shared model. The periodic synchronization at each iteration incurs considerable overhead, exacerbated by the increasing size and complexity of state-of-the-art neural networks. Although many gradient compression techniques propose to reduce communication cost, the ideal compression factor that leads to maximum speedup or minimum data exchange remains an open-ended problem since it varies with the quality of compression, model size and structure, hardware, network topology and bandwidth. We propose GraVAC, a framework to dynamically adjust compression factor throughout training by evaluating model progress and assessing gradient information loss associated with compression. GraVAC works in an online, black-box manner without any prior assumptions about a model or its hyperparameters, while achieving the same or better",
    "link": "http://arxiv.org/abs/2305.12201",
    "context": "Title: GraVAC: Adaptive Compression for Communication-Efficient Distributed DL Training. (arXiv:2305.12201v1 [cs.LG])\nAbstract: Distributed data-parallel (DDP) training improves overall application throughput as multiple devices train on a subset of data and aggregate updates to produce a globally shared model. The periodic synchronization at each iteration incurs considerable overhead, exacerbated by the increasing size and complexity of state-of-the-art neural networks. Although many gradient compression techniques propose to reduce communication cost, the ideal compression factor that leads to maximum speedup or minimum data exchange remains an open-ended problem since it varies with the quality of compression, model size and structure, hardware, network topology and bandwidth. We propose GraVAC, a framework to dynamically adjust compression factor throughout training by evaluating model progress and assessing gradient information loss associated with compression. GraVAC works in an online, black-box manner without any prior assumptions about a model or its hyperparameters, while achieving the same or better",
    "path": "papers/23/05/2305.12201.json",
    "total_tokens": 1175,
    "translated_title": "GraVAC：适应性压缩用于高效的分布式深度学习训练",
    "translated_abstract": "分布式数据并行（DDP）训练通过多个设备在数据子集上进行训练并聚合更新来提高应用程序的整体吞吐量。每次迭代的周期性同步产生了相当大的开销，受最先进的神经网络越来越大和复杂的影响更加严重。尽管许多梯度压缩技术提出了减少通信成本的方式，但最佳压缩因子导致最大速度提升或最小数据交换的问题仍然是一个开放式的问题，因为它与压缩质量、模型大小和结构、硬件、网络拓扑和带宽有关。我们提出了GraVAC，一个动态调整压缩因子的框架，通过评估模型进展和评估与压缩相关的梯度信息损失来进行训练。GraVAC以在线的黑盒方式工作，不需要任何关于模型或其超参数的先前假设，同时实现与先前最先进的压缩方法相同或更好的翻译准确性。对于CIFAR-10和ImageNet数据集，相对于其静态压缩对应物，GraVAC将通信减少了高达87％和75％，而收敛精度相同。",
    "tldr": "GraVAC提出了一个动态调整压缩因子的框架，通过评估模型进展和评估与压缩相关的梯度信息损失来进行训练。GraVAC可以在不需要任何关于模型或其超参数的先前假设的情况下，达到与先前最先进的压缩方法相同或更好的翻译准确性。在CIFAR-10和ImageNet数据集上，相对于静态压缩对应物，GraVAC可以将通信减少高达87％和75％。",
    "en_tdlr": "GraVAC proposes an adaptive compression framework for communication-efficient distributed deep learning training, which dynamically adjusts the compression factor throughout training by evaluating model progress and assessing gradient information loss associated with compression. Without any prior assumptions about a model or its hyperparameters, GraVAC achieves the same or better translation quality as prior state-of-the-art compression methods. GraVAC reduces communication by up to 87% and 75% on the CIFAR-10 and ImageNet datasets, respectively, compared to its static compression counterparts for the same convergence accuracy."
}