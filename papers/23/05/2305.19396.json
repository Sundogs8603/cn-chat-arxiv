{
    "title": "Resource-Efficient Fine-Tuning Strategies for Automatic MOS Prediction in Text-to-Speech for Low-Resource Languages. (arXiv:2305.19396v1 [eess.AS])",
    "abstract": "We train a MOS prediction model based on wav2vec 2.0 using the open-access data sets BVCC and SOMOS. Our test with neural TTS data in the low-resource language (LRL) West Frisian shows that pre-training on BVCC before fine-tuning on SOMOS leads to the best accuracy for both fine-tuned and zero-shot prediction. Further fine-tuning experiments show that using more than 30 percent of the total data does not lead to significant improvements. In addition, fine-tuning with data from a single listener shows promising system-level accuracy, supporting the viability of one-participant pilot tests. These findings can all assist the resource-conscious development of TTS for LRLs by progressing towards better zero-shot MOS prediction and informing the design of listening tests, especially in early-stage evaluation.",
    "link": "http://arxiv.org/abs/2305.19396",
    "context": "Title: Resource-Efficient Fine-Tuning Strategies for Automatic MOS Prediction in Text-to-Speech for Low-Resource Languages. (arXiv:2305.19396v1 [eess.AS])\nAbstract: We train a MOS prediction model based on wav2vec 2.0 using the open-access data sets BVCC and SOMOS. Our test with neural TTS data in the low-resource language (LRL) West Frisian shows that pre-training on BVCC before fine-tuning on SOMOS leads to the best accuracy for both fine-tuned and zero-shot prediction. Further fine-tuning experiments show that using more than 30 percent of the total data does not lead to significant improvements. In addition, fine-tuning with data from a single listener shows promising system-level accuracy, supporting the viability of one-participant pilot tests. These findings can all assist the resource-conscious development of TTS for LRLs by progressing towards better zero-shot MOS prediction and informing the design of listening tests, especially in early-stage evaluation.",
    "path": "papers/23/05/2305.19396.json",
    "total_tokens": 945,
    "translated_title": "针对低资源语言的文本到语音自动 MOS 预测的资源有效微调策略",
    "translated_abstract": "我们使用开放数据集 BVCC 和 SOMOS 基于 wav2vec 2.0 训练了一个 MOS 预测模型。我们在低资源语言 West Frisian 上进行测试，结果表明在 SOMOS 微调前先在 BVCC 上进行预训练，可以得到最佳的微调和零样本预测精度。进一步的微调实验表明，使用超过总数据的 30% 并不会带来显著的改进。此外，使用来自单个听众的数据进行微调的实验表明，这种方法有望提高系统级精度，支持单参与者试验的可行性。这些发现可以在资源有限的情况下为低资源语言的 TTS 开发提供帮助，进一步推进零样本 MOS 预测并指导早期评估中的测试设计。",
    "tldr": "本文通过在低资源语言 West Frisian 上的测试发现，在预训练 BVCC 数据集后，微调 SOMOS 数据集能够得到最佳精度。使用超过总数据的 30% 并不能显著提高精度。使用来自单个听众的数据进行微调的实验表明，这种方法有望提高系统级精度，支持单参与者试验的可行性。",
    "en_tdlr": "This paper proposes resource-efficient fine-tuning strategies for automatic MOS prediction in text-to-speech for low-resource languages. The experiments on the low-resource language West Frisian show that pre-training on BVCC before fine-tuning on SOMOS leads to the best accuracy for both fine-tuned and zero-shot prediction. The study also suggests that using more than 30% of the total data does not lead to significant improvements and fine-tuning with data from a single listener shows promising system-level accuracy, which supports the viability of one-participant pilot tests."
}