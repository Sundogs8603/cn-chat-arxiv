{
    "title": "IMAP: Intrinsically Motivated Adversarial Policy. (arXiv:2305.02605v1 [cs.LG])",
    "abstract": "Reinforcement learning (RL) agents are known to be vulnerable to evasion attacks during deployment. In single-agent environments, attackers can inject imperceptible perturbations on the policy or value network's inputs or outputs; in multi-agent environments, attackers can control an adversarial opponent to indirectly influence the victim's observation. Adversarial policies offer a promising solution to craft such attacks. Still, current approaches either require perfect or partial knowledge of the victim policy or suffer from sample inefficiency due to the sparsity of task-related rewards. To overcome these limitations, we propose the Intrinsically Motivated Adversarial Policy (IMAP) for efficient black-box evasion attacks in single- and multi-agent environments without any knowledge of the victim policy. IMAP uses four intrinsic objectives based on state coverage, policy coverage, risk, and policy divergence to encourage exploration and discover stronger attacking skills. We also des",
    "link": "http://arxiv.org/abs/2305.02605",
    "context": "Title: IMAP: Intrinsically Motivated Adversarial Policy. (arXiv:2305.02605v1 [cs.LG])\nAbstract: Reinforcement learning (RL) agents are known to be vulnerable to evasion attacks during deployment. In single-agent environments, attackers can inject imperceptible perturbations on the policy or value network's inputs or outputs; in multi-agent environments, attackers can control an adversarial opponent to indirectly influence the victim's observation. Adversarial policies offer a promising solution to craft such attacks. Still, current approaches either require perfect or partial knowledge of the victim policy or suffer from sample inefficiency due to the sparsity of task-related rewards. To overcome these limitations, we propose the Intrinsically Motivated Adversarial Policy (IMAP) for efficient black-box evasion attacks in single- and multi-agent environments without any knowledge of the victim policy. IMAP uses four intrinsic objectives based on state coverage, policy coverage, risk, and policy divergence to encourage exploration and discover stronger attacking skills. We also des",
    "path": "papers/23/05/2305.02605.json",
    "total_tokens": 1096,
    "translated_title": "IMAP: 内在驱动的对抗策略",
    "translated_abstract": "强化学习（RL）代理在部署过程中容易受到规避攻击的影响。在单智能体环境中，攻击者可以对策略或值网络的输入或输出注入无法察觉的扰动；在多智能体环境中，攻击者可以通过控制对手间接影响受害者的观察。 对抗性策略为解决此类攻击提供了一种有前途的解决方案。然而，目前的方法要么需要受害者政策的完美或部分知识，要么由于任务相关奖励的稀疏性而导致样本效率低下。为克服这些局限性，我们提出了内在驱动的对抗政策（IMAP），用于单智能体和多智能体环境中高效的黑盒规避攻击，而不需任何关于受害者策略的知识。 IMAP利用基于状态覆盖率，策略覆盖率，风险和政策分歧的四个内在目标，以鼓励探索并发现更强的攻击技能。我们还描述了一种处理多个具有不同实力的对手的可推广算法。我们的实验表明，IMAP在单智能体和多智能体环境中均优于最先进的方法，包括两个Atari游戏，一个机器人运动任务和一个多智能体游戏。",
    "tldr": "IMAP是一个内在驱动的对抗策略，无需受害者策略的任何知识，能够高效地进行黑盒规避攻击，并且在单一和多智能体环境中优于目前最先进的方法。",
    "en_tdlr": "IMAP is an intrinsically motivated adversarial policy that efficiently performs black-box evasion attacks without requiring any knowledge of the victim's policy, and it outperforms state-of-the-art methods in both single and multi-agent environments, including various Atari games, a robot locomotion task, and a multi-agent game."
}