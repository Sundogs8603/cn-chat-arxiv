{
    "title": "On the Limitations of Simulating Active Learning. (arXiv:2305.13342v1 [cs.LG])",
    "abstract": "Active learning (AL) is a human-and-model-in-the-loop paradigm that iteratively selects informative unlabeled data for human annotation, aiming to improve over random sampling. However, performing AL experiments with human annotations on-the-fly is a laborious and expensive process, thus unrealistic for academic research. An easy fix to this impediment is to simulate AL, by treating an already labeled and publicly available dataset as the pool of unlabeled data. In this position paper, we first survey recent literature and highlight the challenges across all different steps within the AL loop. We further unveil neglected caveats in the experimental setup that can significantly affect the quality of AL research. We continue with an exploration of how the simulation setting can govern empirical findings, arguing that it might be one of the answers behind the ever posed question ``why do active learning algorithms sometimes fail to outperform random sampling?''. We argue that evaluating A",
    "link": "http://arxiv.org/abs/2305.13342",
    "context": "Title: On the Limitations of Simulating Active Learning. (arXiv:2305.13342v1 [cs.LG])\nAbstract: Active learning (AL) is a human-and-model-in-the-loop paradigm that iteratively selects informative unlabeled data for human annotation, aiming to improve over random sampling. However, performing AL experiments with human annotations on-the-fly is a laborious and expensive process, thus unrealistic for academic research. An easy fix to this impediment is to simulate AL, by treating an already labeled and publicly available dataset as the pool of unlabeled data. In this position paper, we first survey recent literature and highlight the challenges across all different steps within the AL loop. We further unveil neglected caveats in the experimental setup that can significantly affect the quality of AL research. We continue with an exploration of how the simulation setting can govern empirical findings, arguing that it might be one of the answers behind the ever posed question ``why do active learning algorithms sometimes fail to outperform random sampling?''. We argue that evaluating A",
    "path": "papers/23/05/2305.13342.json",
    "total_tokens": 919,
    "translated_title": "论模拟主动学习的限制",
    "translated_abstract": "主动学习（AL）是一种人与模型交互循环的范式，用于迭代地选择信息性未标记数据以供人类注释，旨在改善随机抽样的表现。然而，在流程中实时进行带人类注释的AL实验是一项繁琐而昂贵的过程，因此在学术研究中不切实际。解决此问题的简单方法是通过将已标记的公开可用数据集作为未标记数据的池来模拟AL。在这篇观点论文中，我们首先调查最近的文献并突出显示AL循环中所有不同步骤中的挑战。我们进一步揭示了实验设置中被忽视的注意事项，这些注意事项可能会显着影响AL研究的质量。我们接着探讨了模拟设置如何支配经验发现，认为这可能是“为什么有时主动学习算法无法胜过随机抽样”的权衡之一。我们认为仅基于模拟实验结果得出强烈结论可以导致评估AL算法的误导，因此提出谨慎。",
    "tldr": "研究提出了主动学习模拟的局限性，并警告基于模拟实验结果得出强烈结论可能导致评估AL算法的误导。",
    "en_tdlr": "The paper presents the limitations of simulating active learning and cautions against making strong conclusions solely based on simulated experiments, highlighting possible biases in the evaluation of active learning algorithms."
}