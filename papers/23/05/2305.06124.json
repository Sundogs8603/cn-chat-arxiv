{
    "title": "FedDWA: Personalized Federated Learning with Online Weight Adjustment. (arXiv:2305.06124v1 [cs.LG])",
    "abstract": "Different from conventional federated learning, personalized federated learning (PFL) is able to train a customized model for each individual client according to its unique requirement. The mainstream approach is to adopt a kind of weighted aggregation method to generate personalized models, in which weights are determined by the loss value or model parameters among different clients. However, such kinds of methods require clients to download others' models. It not only sheer increases communication traffic but also potentially infringes data privacy. In this paper, we propose a new PFL algorithm called \\emph{FedDWA (Federated Learning with Dynamic Weight Adjustment)} to address the above problem, which leverages the parameter server (PS) to compute personalized aggregation weights based on collected models from clients. In this way, FedDWA can capture similarities between clients with much less communication overhead. More specifically, we formulate the PFL problem as an optimization ",
    "link": "http://arxiv.org/abs/2305.06124",
    "context": "Title: FedDWA: Personalized Federated Learning with Online Weight Adjustment. (arXiv:2305.06124v1 [cs.LG])\nAbstract: Different from conventional federated learning, personalized federated learning (PFL) is able to train a customized model for each individual client according to its unique requirement. The mainstream approach is to adopt a kind of weighted aggregation method to generate personalized models, in which weights are determined by the loss value or model parameters among different clients. However, such kinds of methods require clients to download others' models. It not only sheer increases communication traffic but also potentially infringes data privacy. In this paper, we propose a new PFL algorithm called \\emph{FedDWA (Federated Learning with Dynamic Weight Adjustment)} to address the above problem, which leverages the parameter server (PS) to compute personalized aggregation weights based on collected models from clients. In this way, FedDWA can capture similarities between clients with much less communication overhead. More specifically, we formulate the PFL problem as an optimization ",
    "path": "papers/23/05/2305.06124.json",
    "total_tokens": 1015,
    "translated_title": "FedDWA: 个性化联邦学习与动态权重调整",
    "translated_abstract": "与传统的联邦学习不同，个性化联邦学习（PFL）能够根据每个客户端的独特需求来训练定制化模型。主流方法是采用一种加权聚合方法来生成个性化模型，其中权重是由不同客户端之间的损失值或模型参数确定的。然而，这种方法要求客户端下载其他模型，不仅增加了通信流量，而且可能侵犯数据隐私。我们在本文中提出了一种新的PFL算法，称为FedDWA（带动态权重调整的联邦学习），来解决上述问题，该算法利用参数服务器（PS）根据从客户端收集的模型计算个性化聚合权重。这样，FedDWA可以以更少的通信开销捕捉客户之间的相似性。我们将PFL问题制定为一种优化问题，并通过引入动态权重调整机制设计了一种新算法。FedDWA能够学习高精度和高效的个性化模型，同时保护数据隐私。我们在综合合成和真实数据集上进行了广泛的实验，证明了FedDWA的有效性。",
    "tldr": "本文提出了一种个性化联邦学习算法，名为FedDWA，采用动态权重调整来保护数据隐私并以更少的通信开销捕捉客户之间的相似性，能够训练高精度和高效的个性化模型。",
    "en_tdlr": "This paper proposes a personalized federated learning algorithm called FedDWA, which uses dynamic weight adjustment to protect data privacy and capture client similarities with less communication overhead. It is able to train high-accuracy and high-efficiency personalized models."
}