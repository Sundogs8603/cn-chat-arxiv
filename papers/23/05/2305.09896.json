{
    "title": "Convergence and Privacy of Decentralized Nonconvex Optimization with Gradient Clipping and Communication Compression. (arXiv:2305.09896v1 [cs.LG])",
    "abstract": "Achieving communication efficiency in decentralized machine learning has been attracting significant attention, with communication compression recognized as an effective technique in algorithm design. This paper takes a first step to understand the role of gradient clipping, a popular strategy in practice, in decentralized nonconvex optimization with communication compression. We propose PORTER, which considers two variants of gradient clipping added before or after taking a mini-batch of stochastic gradients, where the former variant PORTER-DP allows local differential privacy analysis with additional Gaussian perturbation, and the latter variant PORTER-GC helps to stabilize training. We develop a novel analysis framework that establishes their convergence guarantees without assuming the stringent bounded gradient assumption. To the best of our knowledge, our work provides the first convergence analysis for decentralized nonconvex optimization with gradient clipping and communication ",
    "link": "http://arxiv.org/abs/2305.09896",
    "context": "Title: Convergence and Privacy of Decentralized Nonconvex Optimization with Gradient Clipping and Communication Compression. (arXiv:2305.09896v1 [cs.LG])\nAbstract: Achieving communication efficiency in decentralized machine learning has been attracting significant attention, with communication compression recognized as an effective technique in algorithm design. This paper takes a first step to understand the role of gradient clipping, a popular strategy in practice, in decentralized nonconvex optimization with communication compression. We propose PORTER, which considers two variants of gradient clipping added before or after taking a mini-batch of stochastic gradients, where the former variant PORTER-DP allows local differential privacy analysis with additional Gaussian perturbation, and the latter variant PORTER-GC helps to stabilize training. We develop a novel analysis framework that establishes their convergence guarantees without assuming the stringent bounded gradient assumption. To the best of our knowledge, our work provides the first convergence analysis for decentralized nonconvex optimization with gradient clipping and communication ",
    "path": "papers/23/05/2305.09896.json",
    "total_tokens": 846,
    "tldr": "本文提出了一种名为PORTER的解决方案，它考虑了梯度截断的两个变体，使得我们能够在分布式非凸优化过程中使用通信压缩，并在不需要强有界梯度假设的情况下保证其收敛性。",
    "en_tdlr": "This paper proposes a solution called PORTER, which considers two variants of gradient clipping and enables the use of communication compression in distributed nonconvex optimization without assuming bounded gradient. It provides the first convergence analysis for such scenario."
}