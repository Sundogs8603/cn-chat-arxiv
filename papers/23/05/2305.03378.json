{
    "title": "Towards Effective Collaborative Learning in Long-Tailed Recognition. (arXiv:2305.03378v1 [cs.CV])",
    "abstract": "Real-world data usually suffers from severe class imbalance and long-tailed distributions, where minority classes are significantly underrepresented compared to the majority ones. Recent research prefers to utilize multi-expert architectures to mitigate the model uncertainty on the minority, where collaborative learning is employed to aggregate the knowledge of experts, i.e., online distillation. In this paper, we observe that the knowledge transfer between experts is imbalanced in terms of class distribution, which results in limited performance improvement of the minority classes. To address it, we propose a re-weighted distillation loss by comparing two classifiers' predictions, which are supervised by online distillation and label annotations, respectively. We also emphasize that feature-level distillation will significantly improve model performance and increase feature robustness. Finally, we propose an Effective Collaborative Learning (ECL) framework that integrates a contrastiv",
    "link": "http://arxiv.org/abs/2305.03378",
    "context": "Title: Towards Effective Collaborative Learning in Long-Tailed Recognition. (arXiv:2305.03378v1 [cs.CV])\nAbstract: Real-world data usually suffers from severe class imbalance and long-tailed distributions, where minority classes are significantly underrepresented compared to the majority ones. Recent research prefers to utilize multi-expert architectures to mitigate the model uncertainty on the minority, where collaborative learning is employed to aggregate the knowledge of experts, i.e., online distillation. In this paper, we observe that the knowledge transfer between experts is imbalanced in terms of class distribution, which results in limited performance improvement of the minority classes. To address it, we propose a re-weighted distillation loss by comparing two classifiers' predictions, which are supervised by online distillation and label annotations, respectively. We also emphasize that feature-level distillation will significantly improve model performance and increase feature robustness. Finally, we propose an Effective Collaborative Learning (ECL) framework that integrates a contrastiv",
    "path": "papers/23/05/2305.03378.json",
    "total_tokens": 880,
    "translated_title": "面向长尾识别的有效协作学习",
    "translated_abstract": "实际数据通常面临着严重的类别不平衡和长尾分布，其中少数类与多数类相比显着不足。最近的研究倾向于利用多专家体系结构来减轻在少数类中的模型不确定性，其中采用协作学习来汇总专家的知识，即在线蒸馏。在本文中，我们观察到专家之间的知识转移在类别分布方面是不平衡的，这导致少数类的性能提升有限。为了解决这个问题，我们提出了一种重新加权的蒸馏损失，通过比较在线蒸馏和标签注释分别监督的两个分类器的预测来实现。我们还强调，特征级蒸馏将显着提高模型性能并增加特征的鲁棒性。最后，我们提出了一个有效的协作学习（ECL）框架，该框架将对比学习和蒸馏机制相结合，以实现最佳表现。",
    "tldr": "本文提出了一种重新加权的蒸馏损失来处理长尾分布下的协作学习不平衡问题，并建立了一个有效的协作学习框架以提高模型的性能。",
    "en_tdlr": "This paper proposes a re-weighted distillation loss to address the imbalance problem in collaborative learning under long-tailed distributions. An effective collaborative learning framework is also established by combining contrastive learning and distillation mechanisms for optimal performance."
}