{
    "title": "Less is More! A slim architecture for optimal language translation. (arXiv:2305.10991v1 [cs.CL])",
    "abstract": "The softmax attention mechanism has emerged as a noteworthy development in the field of Artificial Intelligence research, building on the successes of Transformer-based architectures. However, their ever increasing sizes necessitate ever increasing computational memory, that limits their usage. We propose KgV, a sigmoid gating mechanism that, in conjunction with softmax attention, significantly boosts performance without increasing architecture size. To amend the size requirements, we leverage Tensor Chains to identify and prune the excess parameters. We find that such excess resides primarily within the embedding layer, and not in the output linear layer. To further improve embedding and significantly reduce parameters, we introduce H-SoftPOS, a hierarchical embedding layer which simultaneously enhances performance. Remarkably, on the WMT14 English-German validation set, our approach yields a threefold reduction in perplexity, surpassing the current state-of-the-art, while reducing pa",
    "link": "http://arxiv.org/abs/2305.10991",
    "context": "Title: Less is More! A slim architecture for optimal language translation. (arXiv:2305.10991v1 [cs.CL])\nAbstract: The softmax attention mechanism has emerged as a noteworthy development in the field of Artificial Intelligence research, building on the successes of Transformer-based architectures. However, their ever increasing sizes necessitate ever increasing computational memory, that limits their usage. We propose KgV, a sigmoid gating mechanism that, in conjunction with softmax attention, significantly boosts performance without increasing architecture size. To amend the size requirements, we leverage Tensor Chains to identify and prune the excess parameters. We find that such excess resides primarily within the embedding layer, and not in the output linear layer. To further improve embedding and significantly reduce parameters, we introduce H-SoftPOS, a hierarchical embedding layer which simultaneously enhances performance. Remarkably, on the WMT14 English-German validation set, our approach yields a threefold reduction in perplexity, surpassing the current state-of-the-art, while reducing pa",
    "path": "papers/23/05/2305.10991.json",
    "total_tokens": 952,
    "translated_title": "更少即是更好！一种优化语言翻译的轻量级架构。(arXiv:2305.10991v1 [cs.CL])",
    "translated_abstract": "Softmax 注意力机制在人工智能研究领域已经成为一个值得关注的开发，构建在 Transformer 架构的成功基础之上。然而，它们不断增长的大小需要越来越多的计算存储器，从而限制了它们的使用。我们提出了 KgV，一种 sigmoid 门控机制，与 softmax 注意力一起显著提高了性能，同时不增加架构大小。为了修正大小要求，我们利用张量链来识别和修剪多余的参数。我们发现，这样的多余主要存在于嵌入层中，而不是输出线性层中。为了进一步改进嵌入和显著减少参数，我们引入了 H-SoftPOS，一种层次嵌入层，同时增强了性能。值得注意的是，在 WMT14 英德验证集上，我们的方法使 perplexity 减少了三倍，超过了当前的最新成果，同时减少降低模型大小。",
    "tldr": "该论文提出了一种名为 KgV 的 sigmoid 门控机制，通过嵌入层剪枝减少了模型的大小，同时引入了 H-SoftPOS 层次嵌入层进一步改进嵌入以显著减少模型参数，从而在 WMT14 英德验证集上使 perplexity 减少了三倍以上。",
    "en_tdlr": "The paper proposes a sigmoid gating mechanism named KgV to boost performance without increasing model size and prunes the excess parameters in the embedding layer using Tensor Chains. The paper also introduces a hierarchical embedding layer named H-SoftPOS to enhance performance and significantly reduce model parameters. The proposed approach achieves a threefold reduction in perplexity on the WMT14 English-German validation set compared to the state-of-the-art models."
}