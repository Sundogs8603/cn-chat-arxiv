{
    "title": "People Talking and AI Listening: How Stigmatizing Language in EHR Notes Affect AI Performance. (arXiv:2305.10201v1 [cs.AI])",
    "abstract": "Electronic health records (EHRs) serve as an essential data source for the envisioned artificial intelligence (AI)-driven transformation in healthcare. However, clinician biases reflected in EHR notes can lead to AI models inheriting and amplifying these biases, perpetuating health disparities. This study investigates the impact of stigmatizing language (SL) in EHR notes on mortality prediction using a Transformer-based deep learning model and explainable AI (XAI) techniques. Our findings demonstrate that SL written by clinicians adversely affects AI performance, particularly so for black patients, highlighting SL as a source of racial disparity in AI model development. To explore an operationally efficient way to mitigate SL's impact, we investigate patterns in the generation of SL through a clinicians' collaborative network, identifying central clinicians as having a stronger impact on racial disparity in the AI model. We find that removing SL written by central clinicians is a more ",
    "link": "http://arxiv.org/abs/2305.10201",
    "context": "Title: People Talking and AI Listening: How Stigmatizing Language in EHR Notes Affect AI Performance. (arXiv:2305.10201v1 [cs.AI])\nAbstract: Electronic health records (EHRs) serve as an essential data source for the envisioned artificial intelligence (AI)-driven transformation in healthcare. However, clinician biases reflected in EHR notes can lead to AI models inheriting and amplifying these biases, perpetuating health disparities. This study investigates the impact of stigmatizing language (SL) in EHR notes on mortality prediction using a Transformer-based deep learning model and explainable AI (XAI) techniques. Our findings demonstrate that SL written by clinicians adversely affects AI performance, particularly so for black patients, highlighting SL as a source of racial disparity in AI model development. To explore an operationally efficient way to mitigate SL's impact, we investigate patterns in the generation of SL through a clinicians' collaborative network, identifying central clinicians as having a stronger impact on racial disparity in the AI model. We find that removing SL written by central clinicians is a more ",
    "path": "papers/23/05/2305.10201.json",
    "total_tokens": 1196,
    "translated_title": "人们交谈，AI倾听：电子病历中污名化语言对AI判断的影响",
    "translated_abstract": "电子病历(EHRs)是期望中的人工智能(AI)-驱动的医疗转型的重要数据来源。然而，反映在EHR笔记中的临床医师偏见可能会导致AI模型继承并放大这些偏见，从而不断加剧健康上的不平等。本研究调查了EHR笔记中污名化语言(SL)对基于Transformer的深度学习模型和可解释AI(XAI)技术进行死亡预测的影响。我们的研究发现，临床医生所写的SL不利于AI的性能表现，尤其是在黑人患者中表现更为明显，突出了SL作为AI模型发展中种族差异的一种来源。为探索一种操作上有效的缓解SL影响的方法，我们研究了临床医生协作网络中SL生成的模式，发现中央医生对AI模型中的种族差异具有更强的影响力。我们发现，删除中央临床医生撰写的SL是相对于随机选择临床医生而言，缓解SL对AI性能影响的更为有效的策略。我们的研究强调了理解反映在EHR笔记中的临床医师偏见对下游AI性能的影响的重要性，以开发更具公平和正义的医疗系统。",
    "tldr": "本文研究了电子病历中污名化语言对基于Transformer的深度学习模型和可解释AI(XAI)技术进行死亡预测的影响。发现临床医生所写的SL会对AI性能表现不利，尤其是在黑人患者中表现更为明显，强调了理解偏见对下游AI性能的影响的重要性，以开发更具公平和正义的医疗系统。"
}