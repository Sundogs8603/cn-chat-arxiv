{
    "title": "The Benefits of Bad Advice: Autocontrastive Decoding across Model Layers. (arXiv:2305.01628v1 [cs.CL])",
    "abstract": "Applying language models to natural language processing tasks typically relies on the representations in the final model layer, as intermediate hidden layer representations are presumed to be less informative. In this work, we argue that due to the gradual improvement across model layers, additional information can be gleaned from the contrast between higher and lower layers during inference. Specifically, in choosing between the probable next token predictions of a generative model, the predictions of lower layers can be used to highlight which candidates are best avoided. We propose a novel approach that utilizes the contrast between layers to improve text generation outputs, and show that it mitigates degenerative behaviors of the model in open-ended generation, significantly improving the quality of generated texts. Furthermore, our results indicate that contrasting between model layers at inference time can yield substantial benefits to certain aspects of general language model ca",
    "link": "http://arxiv.org/abs/2305.01628",
    "context": "Title: The Benefits of Bad Advice: Autocontrastive Decoding across Model Layers. (arXiv:2305.01628v1 [cs.CL])\nAbstract: Applying language models to natural language processing tasks typically relies on the representations in the final model layer, as intermediate hidden layer representations are presumed to be less informative. In this work, we argue that due to the gradual improvement across model layers, additional information can be gleaned from the contrast between higher and lower layers during inference. Specifically, in choosing between the probable next token predictions of a generative model, the predictions of lower layers can be used to highlight which candidates are best avoided. We propose a novel approach that utilizes the contrast between layers to improve text generation outputs, and show that it mitigates degenerative behaviors of the model in open-ended generation, significantly improving the quality of generated texts. Furthermore, our results indicate that contrasting between model layers at inference time can yield substantial benefits to certain aspects of general language model ca",
    "path": "papers/23/05/2305.01628.json",
    "total_tokens": 840,
    "translated_title": "坏建议的好处：模型层间自动对照解码",
    "translated_abstract": "在自然语言处理任务中，应用语言模型通常依赖于最终模型层的表示，因为假设中间隐藏层的表示是不太有用的。本文认为由于模型层之间的渐进改进，可以从更高层和更低层之间的对比中获取额外信息。具体来说，在选择生成模型的下一个可能标记的预测时，可以使用较低层的预测来突出哪些候选项是最好避免的。我们提出了一种新颖的方法，利用层之间的对比来改进文本生成输出，并表明它可以缓解模型在开放式生成中的不良行为，显著提高生成的文本质量。此外，我们的结果表明，在推断时比较模型层之间可以对一些总体语言模型能力的方面产生实质性的好处。",
    "tldr": "本文提出一种新颖的方法，利用语言模型层之间的对比来改进文本生成输出，解决模型在开放式生成中的不良行为问题，并显著提高生成文本的质量。",
    "en_tdlr": "This paper proposes a novel approach to improving text generation outputs by utilizing the contrast between language model layers, which mitigates degenerative behaviors of the model in open-ended generation and significantly improves the quality of generated texts. The contrast between model layers at inference time can yield substantial benefits to certain aspects of general language model capabilities."
}