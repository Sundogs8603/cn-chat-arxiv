{
    "title": "Which Models have Perceptually-Aligned Gradients? An Explanation via Off-Manifold Robustness",
    "abstract": "arXiv:2305.19101v2 Announce Type: replace  Abstract: One of the remarkable properties of robust computer vision models is that their input-gradients are often aligned with human perception, referred to in the literature as perceptually-aligned gradients (PAGs). Despite only being trained for classification, PAGs cause robust models to have rudimentary generative capabilities, including image generation, denoising, and in-painting. However, the underlying mechanisms behind these phenomena remain unknown. In this work, we provide a first explanation of PAGs via \\emph{off-manifold robustness}, which states that models must be more robust off- the data manifold than they are on-manifold. We first demonstrate theoretically that off-manifold robustness leads input gradients to lie approximately on the data manifold, explaining their perceptual alignment. We then show that Bayes optimal models satisfy off-manifold robustness, and confirm the same empirically for robust models trained via grad",
    "link": "https://arxiv.org/abs/2305.19101",
    "context": "Title: Which Models have Perceptually-Aligned Gradients? An Explanation via Off-Manifold Robustness\nAbstract: arXiv:2305.19101v2 Announce Type: replace  Abstract: One of the remarkable properties of robust computer vision models is that their input-gradients are often aligned with human perception, referred to in the literature as perceptually-aligned gradients (PAGs). Despite only being trained for classification, PAGs cause robust models to have rudimentary generative capabilities, including image generation, denoising, and in-painting. However, the underlying mechanisms behind these phenomena remain unknown. In this work, we provide a first explanation of PAGs via \\emph{off-manifold robustness}, which states that models must be more robust off- the data manifold than they are on-manifold. We first demonstrate theoretically that off-manifold robustness leads input gradients to lie approximately on the data manifold, explaining their perceptual alignment. We then show that Bayes optimal models satisfy off-manifold robustness, and confirm the same empirically for robust models trained via grad",
    "path": "papers/23/05/2305.19101.json",
    "total_tokens": 869,
    "translated_title": "哪些模型具有感知对齐梯度？通过离散度稳健性解释",
    "translated_abstract": "强健的计算机视觉模型的一个显著特性是它们的输入梯度通常与人类感知对齐，被文献称为感知对齐梯度（PAGs）。尽管只被训练用于分类，PAGs使得稳健模型具有基本的生成能力，包括图像生成、去噪和修复。然而，这些现象背后的机制仍未知。在这项工作中，我们通过\\emph{离散度稳健性}首次对PAGs进行解释，该理论指出模型在数据流形之外必须比在流形上更加稳健。我们首先从理论上证明离散度稳健性导致输入梯度大致位于数据流形上，从而解释它们的感知对齐性。然后我们展示贝叶斯最优模型满足离散度稳健性，并在经验上证实通过梯度训练的稳健模型也满足相同条件。",
    "tldr": "稳健计算机视觉模型的梯度通常与人类感知对齐，通过离散度稳健性解释这一现象。",
    "en_tdlr": "The gradients of robust computer vision models are often aligned with human perception, and this phenomenon is explained through off-manifold robustness."
}