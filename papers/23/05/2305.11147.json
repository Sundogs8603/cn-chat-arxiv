{
    "title": "UniControl: A Unified Diffusion Model for Controllable Visual Generation In the Wild. (arXiv:2305.11147v2 [cs.CV] UPDATED)",
    "abstract": "Achieving machine autonomy and human control often represent divergent objectives in the design of interactive AI systems. Visual generative foundation models such as Stable Diffusion show promise in navigating these goals, especially when prompted with arbitrary languages. However, they often fall short in generating images with spatial, structural, or geometric controls. The integration of such controls, which can accommodate various visual conditions in a single unified model, remains an unaddressed challenge. In response, we introduce UniControl, a new generative foundation model that consolidates a wide array of controllable condition-to-image (C2I) tasks within a singular framework, while still allowing for arbitrary language prompts. UniControl enables pixel-level-precise image generation, where visual conditions primarily influence the generated structures and language prompts guide the style and context. To equip UniControl with the capacity to handle diverse visual conditions",
    "link": "http://arxiv.org/abs/2305.11147",
    "context": "Title: UniControl: A Unified Diffusion Model for Controllable Visual Generation In the Wild. (arXiv:2305.11147v2 [cs.CV] UPDATED)\nAbstract: Achieving machine autonomy and human control often represent divergent objectives in the design of interactive AI systems. Visual generative foundation models such as Stable Diffusion show promise in navigating these goals, especially when prompted with arbitrary languages. However, they often fall short in generating images with spatial, structural, or geometric controls. The integration of such controls, which can accommodate various visual conditions in a single unified model, remains an unaddressed challenge. In response, we introduce UniControl, a new generative foundation model that consolidates a wide array of controllable condition-to-image (C2I) tasks within a singular framework, while still allowing for arbitrary language prompts. UniControl enables pixel-level-precise image generation, where visual conditions primarily influence the generated structures and language prompts guide the style and context. To equip UniControl with the capacity to handle diverse visual conditions",
    "path": "papers/23/05/2305.11147.json",
    "total_tokens": 1124,
    "translated_title": "UniControl：统一的扩散模型用于可控的生动图像生成",
    "translated_abstract": "实现机器自治和人类控制往往代表着相互矛盾的目标，特别是在交互式人工智能系统的设计中。现实中的可控视觉生成模型（如稳定扩散）在使用任意语言时具有良好的表现，但在生成具有空间、结构或几何控制的图像方面往往表现不佳。本文提出了UniControl，这是一种新的生成基础模型，它将广泛的可控条件与图像（C2I）任务整合到一个单一的框架中，同时仍允许任意语言提示。UniControl能够进行像素级精确的图像生成，其中视觉条件主要影响生成的结构，语言提示则引导样式和上下文。为了使UniControl具备处理各种视觉条件的能力，我们提出了一种统一的扩散模型，它将扩散过程和变分自动编码器的优点结合起来。实验结果表明，我们提出的UniControl在可控性和图像质量方面均优于现有的最先进模型。",
    "tldr": "本文提出了UniControl，一种新的生成基础模型，能够整合广泛的可控条件与图像任务，并仍然允许任意语言提示，UniControl能够进行像素级精确的图像生成，其中视觉条件主要影响生成的结构，语言提示则引导样式和上下文。我们提出了一种统一的扩散模型，它将扩散过程和变分自动编码器的优点结合起来，实验结果表明UniControl在可控性和图像质量方面均优于现有的最先进模型。",
    "en_tdlr": "This paper proposes UniControl, a new generative foundation model that consolidates a wide array of controllable condition-to-image tasks within a singular framework, while still allowing for arbitrary language prompts. UniControl enables pixel-level-precise image generation, where visual conditions primarily influence the generated structures and language prompts guide the style and context. The paper also proposes a unified diffusion model, combining the advantages of both diffusion processes and variational autoencoders. Experimental results show that UniControl outperforms state-of-the-art models in both controllability and image quality."
}