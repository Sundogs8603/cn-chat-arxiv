{
    "title": "Non-convex Bayesian Learning via Stochastic Gradient Markov Chain Monte Carlo. (arXiv:2305.19350v1 [stat.CO])",
    "abstract": "The rise of artificial intelligence (AI) hinges on the efficient training of modern deep neural networks (DNNs) for non-convex optimization and uncertainty quantification, which boils down to a non-convex Bayesian learning problem. A standard tool to handle the problem is Langevin Monte Carlo, which proposes to approximate the posterior distribution with theoretical guarantees. In this thesis, we start with the replica exchange Langevin Monte Carlo (also known as parallel tempering), which proposes appropriate swaps between exploration and exploitation to achieve accelerations. However, the na\\\"ive extension of swaps to big data problems leads to a large bias, and bias-corrected swaps are required. Such a mechanism leads to few effective swaps and insignificant accelerations. To alleviate this issue, we first propose a control variates method to reduce the variance of noisy energy estimators and show a potential to accelerate the exponential convergence. We also present the population-",
    "link": "http://arxiv.org/abs/2305.19350",
    "context": "Title: Non-convex Bayesian Learning via Stochastic Gradient Markov Chain Monte Carlo. (arXiv:2305.19350v1 [stat.CO])\nAbstract: The rise of artificial intelligence (AI) hinges on the efficient training of modern deep neural networks (DNNs) for non-convex optimization and uncertainty quantification, which boils down to a non-convex Bayesian learning problem. A standard tool to handle the problem is Langevin Monte Carlo, which proposes to approximate the posterior distribution with theoretical guarantees. In this thesis, we start with the replica exchange Langevin Monte Carlo (also known as parallel tempering), which proposes appropriate swaps between exploration and exploitation to achieve accelerations. However, the na\\\"ive extension of swaps to big data problems leads to a large bias, and bias-corrected swaps are required. Such a mechanism leads to few effective swaps and insignificant accelerations. To alleviate this issue, we first propose a control variates method to reduce the variance of noisy energy estimators and show a potential to accelerate the exponential convergence. We also present the population-",
    "path": "papers/23/05/2305.19350.json",
    "total_tokens": 700,
    "translated_title": "基于随机梯度马尔科夫链蒙特卡罗的非凸贝叶斯学习",
    "translated_abstract": "人工智能的兴起取决于现代深度神经网络的有效训练，这涉及到非凸优化和不确定性量化，归结为非凸贝叶斯学习问题。为了解决这个问题，本文提出了一种基于随机梯度马尔科夫链蒙特卡罗的方法，用于近似后验分布并具有理论保证。",
    "tldr": "提出了一种基于随机梯度马尔科夫链蒙特卡罗的方法来解决非凸贝叶斯学习问题，具有理论保证。",
    "en_tdlr": "A method based on stochastic gradient Markov chain Monte Carlo is proposed to solve the problem of non-convex Bayesian learning, with theoretical guarantee."
}