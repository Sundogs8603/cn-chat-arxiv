{
    "title": "Prompt-Tuning Decision Transformer with Preference Ranking. (arXiv:2305.09648v1 [cs.LG])",
    "abstract": "Prompt-tuning has emerged as a promising method for adapting pre-trained models to downstream tasks or aligning with human preferences. Prompt learning is widely used in NLP but has limited applicability to RL due to the complex physical meaning and environment-specific information contained within RL prompts. These factors require supervised learning to imitate the demonstrations and may result in a loss of meaning after learning. Additionally, directly extending prompt-tuning approaches to RL is challenging because RL prompts guide agent behavior based on environmental modeling and analysis, rather than filling in missing information, making it unlikely that adjustments to the prompt format for downstream tasks, as in NLP, can yield significant improvements. In this work, we propose the Prompt-Tuning DT algorithm to address these challenges by using trajectory segments as prompts to guide RL agents in acquiring environmental information and optimizing prompts via black-box tuning to ",
    "link": "http://arxiv.org/abs/2305.09648",
    "context": "Title: Prompt-Tuning Decision Transformer with Preference Ranking. (arXiv:2305.09648v1 [cs.LG])\nAbstract: Prompt-tuning has emerged as a promising method for adapting pre-trained models to downstream tasks or aligning with human preferences. Prompt learning is widely used in NLP but has limited applicability to RL due to the complex physical meaning and environment-specific information contained within RL prompts. These factors require supervised learning to imitate the demonstrations and may result in a loss of meaning after learning. Additionally, directly extending prompt-tuning approaches to RL is challenging because RL prompts guide agent behavior based on environmental modeling and analysis, rather than filling in missing information, making it unlikely that adjustments to the prompt format for downstream tasks, as in NLP, can yield significant improvements. In this work, we propose the Prompt-Tuning DT algorithm to address these challenges by using trajectory segments as prompts to guide RL agents in acquiring environmental information and optimizing prompts via black-box tuning to ",
    "path": "papers/23/05/2305.09648.json",
    "total_tokens": 836,
    "translated_title": "基于偏好排序的Prompt-Tuning决策变换器",
    "translated_abstract": "Prompt-tuning已成为一种很有前途的适应预训练模型到下游任务或与人类偏好对齐的方法。Prompt learning 在自然语言处理中得到广泛应用，但由于RL prompts中包含的复杂物理含义和环境特定信息，其适用性有限。这些因素需要通过监督学习来模仿演示，并在学习后可能导致意义的丧失。此外，将prompt-tuning方法直接扩展到RL是具有挑战性的，因为RL prompts是根据环境建模和分析来指导agent行为的，而不是填补缺失的信息，因此在下游任务中调整prompt格式，如在NLP中，可能不会产生显著的改进。本文提出了Prompt-Tuning DT算法来解决这些挑战，通过将轨迹段用作prompt来指导RL agent获取环境信息，并通过黑盒调整来优化prompt，从而改善了下游任务的性能。",
    "tldr": "本研究提出了Prompt-Tuning DT算法，通过使用轨迹段作为prompt来指导RL agent获取环境信息，从而在具有挑战性的任务中实现了优秀表现。",
    "en_tdlr": "The Prompt-Tuning DT algorithm is proposed in this paper to guide RL agents in acquiring environmental information by using trajectory segments as prompts and optimizing prompts via black-box tuning, achieving excellent performance in challenging tasks."
}