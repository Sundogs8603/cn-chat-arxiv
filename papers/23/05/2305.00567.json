{
    "title": "Scaling Pareto-Efficient Decision Making Via Offline Multi-Objective RL. (arXiv:2305.00567v1 [cs.LG])",
    "abstract": "The goal of multi-objective reinforcement learning (MORL) is to learn policies that simultaneously optimize multiple competing objectives. In practice, an agent's preferences over the objectives may not be known apriori, and hence, we require policies that can generalize to arbitrary preferences at test time. In this work, we propose a new data-driven setup for offline MORL, where we wish to learn a preference-agnostic policy agent using only a finite dataset of offline demonstrations of other agents and their preferences. The key contributions of this work are two-fold. First, we introduce D4MORL, (D)atasets for MORL that are specifically designed for offline settings. It contains 1.8 million annotated demonstrations obtained by rolling out reference policies that optimize for randomly sampled preferences on 6 MuJoCo environments with 2-3 objectives each. Second, we propose Pareto-Efficient Decision Agents (PEDA), a family of offline MORL algorithms that builds and extends Decision Tr",
    "link": "http://arxiv.org/abs/2305.00567",
    "context": "Title: Scaling Pareto-Efficient Decision Making Via Offline Multi-Objective RL. (arXiv:2305.00567v1 [cs.LG])\nAbstract: The goal of multi-objective reinforcement learning (MORL) is to learn policies that simultaneously optimize multiple competing objectives. In practice, an agent's preferences over the objectives may not be known apriori, and hence, we require policies that can generalize to arbitrary preferences at test time. In this work, we propose a new data-driven setup for offline MORL, where we wish to learn a preference-agnostic policy agent using only a finite dataset of offline demonstrations of other agents and their preferences. The key contributions of this work are two-fold. First, we introduce D4MORL, (D)atasets for MORL that are specifically designed for offline settings. It contains 1.8 million annotated demonstrations obtained by rolling out reference policies that optimize for randomly sampled preferences on 6 MuJoCo environments with 2-3 objectives each. Second, we propose Pareto-Efficient Decision Agents (PEDA), a family of offline MORL algorithms that builds and extends Decision Tr",
    "path": "papers/23/05/2305.00567.json",
    "total_tokens": 1208,
    "translated_title": "通过离线多目标强化学习扩展帕累托有效决策",
    "translated_abstract": "多目标强化学习（MORL）的目标是学习能同时优化多个竞争目标的策略。实际应用中，代理对目标的偏好可能不是先验已知的，因此我们需要在测试时能够适应任意偏好的策略。本文提出一种新的数据驱动离线MORL设置，我们希望只使用有限的离线演示数据集来学习一个偏好不敏感的策略代理。本文的两个主要贡献：第一，我们介绍了D4MORL，这是一组专门针对离线设置设计的MORL数据集。它包含180万个数据，是通过在6个MuJoCo环境中优化2-3个目标的参考策略的过程中获得的。第二，我们提出了帕累托有效决策代理（PEDA），它是一族离线MORL算法，通过构建和扩展决策转移（DT）方法来适用于新的MORL设置。 PEDS训练多个模型，每个模型都为不同的、随机选择的目标优化。在测试时，PEDA通过一种基于用户指定偏好的原则来选择模型。我们在六个MuJoCo环境下评估了PEDA的性能，并证明PEDA在样本效率方面优于最先进的MORL方法，并自然地扩展到更复杂的具有较大行动空间的环境中。",
    "tldr": "本文提出了一种新的数据驱动离线MORL设置和一个用于离线MORL的算法PEDA。PEDA通过在多个模型中选择最优模型来实现正交偏好。实验表明，PEDA在样本效率方面优于现有方法，同时还可扩展到具有更大动作空间的复杂环境中。",
    "en_tdlr": "This paper presents a new data-driven setup for offline multi-objective reinforcement learning (MORL) and a corresponding algorithm called Pareto-Efficient Decision Agents (PEDA). PEDA selects the optimal model among multiple models to achieve orthogonal preferences. Experimental results show that PEDA outperforms existing methods in terms of sample efficiency and can be extended to more complex environments with larger action spaces."
}