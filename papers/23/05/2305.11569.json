{
    "title": "Language-Universal Phonetic Representation in Multilingual Speech Pretraining for Low-Resource Speech Recognition. (arXiv:2305.11569v1 [eess.AS])",
    "abstract": "We improve low-resource ASR by integrating the ideas of multilingual training and self-supervised learning. Concretely, we leverage an International Phonetic Alphabet (IPA) multilingual model to create frame-level pseudo labels for unlabeled speech, and use these pseudo labels to guide hidden-unit BERT (HuBERT) based speech pretraining in a phonetically-informed manner. The experiments on the Multilingual Speech (MLS) Corpus show that the proposed approach consistently outperforms the standard HuBERT on all the target languages. Moreover, on 3 of the 4 languages, comparing to the standard HuBERT, the approach performs better, meanwhile is able to save supervised training data by 1.5k hours (75%) at most. Our approach outperforms most of the state of the arts, with much less pretraining data in terms of hours and language diversity. Compared to XLSR-53 and a retraining based multilingual method, our approach performs better with full and limited finetuning data scenarios.",
    "link": "http://arxiv.org/abs/2305.11569",
    "context": "Title: Language-Universal Phonetic Representation in Multilingual Speech Pretraining for Low-Resource Speech Recognition. (arXiv:2305.11569v1 [eess.AS])\nAbstract: We improve low-resource ASR by integrating the ideas of multilingual training and self-supervised learning. Concretely, we leverage an International Phonetic Alphabet (IPA) multilingual model to create frame-level pseudo labels for unlabeled speech, and use these pseudo labels to guide hidden-unit BERT (HuBERT) based speech pretraining in a phonetically-informed manner. The experiments on the Multilingual Speech (MLS) Corpus show that the proposed approach consistently outperforms the standard HuBERT on all the target languages. Moreover, on 3 of the 4 languages, comparing to the standard HuBERT, the approach performs better, meanwhile is able to save supervised training data by 1.5k hours (75%) at most. Our approach outperforms most of the state of the arts, with much less pretraining data in terms of hours and language diversity. Compared to XLSR-53 and a retraining based multilingual method, our approach performs better with full and limited finetuning data scenarios.",
    "path": "papers/23/05/2305.11569.json",
    "total_tokens": 955,
    "translated_title": "语言通用的国际音标表示在低资源语音识别多语种训练中的应用",
    "translated_abstract": "本文将多语言训练和自监督学习的思想融合起来，通过利用国际音标（IPA）多语种模型为无标记语音创建帧级伪标签，并以语音预训练为基础，以国际音标为依据指导隐藏单元BERT（HuBERT）的语音预训练，从而提高低资源ASR的性能。实验证明，该方法在所有目标语言上均比标准HuBERT表现更好。与标准HuBERT相比，该方法在4种语言中有3种表现更好，并能够最多节省1.5k小时（75%）的受监督训练数据。 相对于XLSR-53和基于重新训练的多语言方法，本方法在完全和限制微调数据的情况下都表现更好。",
    "tldr": "本文提出一种利用国际音标多语种模型为低资源语音识别任务创造帧级伪标签的方法，并以此指导隐藏单元BERT（HuBERT）的语音预训练。该方法在多语言语音（MLS）语料库中实验表明，相对于标准HuBERT，在所有目标语言上性能提高，且能节省大量受监督训练时间。",
    "en_tdlr": "This paper proposes a method of creating frame-level pseudo labels for low-resource speech recognition tasks using an International Phonetic Alphabet (IPA) multilingual model, and uses these labels to guide hidden-unit BERT (HuBERT) based speech pretraining in a phonetically-informed manner. Experiments on the Multilingual Speech (MLS) Corpus show that this method outperforms standard HuBERT in all target languages and saves significant supervised training time."
}