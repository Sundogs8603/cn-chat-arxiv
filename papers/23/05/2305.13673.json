{
    "title": "Physics of Language Models: Part 1, Context-Free Grammar. (arXiv:2305.13673v1 [cs.CL])",
    "abstract": "We design experiments to study $\\textit{how}$ generative language models, like GPT, learn context-free grammars (CFGs) -- diverse language systems with a tree-like structure capturing many aspects of natural languages, programs, and human logics. CFGs are as hard as pushdown automata, and can be ambiguous so that verifying if a string satisfies the rules requires dynamic programming. We construct synthetic data and demonstrate that even for very challenging CFGs, pre-trained transformers can learn to generate sentences with near-perfect accuracy and remarkable $\\textit{diversity}$.  More importantly, we delve into the $\\textit{physical principles}$ behind how transformers learns CFGs. We discover that the hidden states within the transformer implicitly and $\\textit{precisely}$ encode the CFG structure (such as putting tree node information exactly on the subtree boundary), and learn to form \"boundary to boundary\" attentions that resemble dynamic programming. We also cover some extensio",
    "link": "http://arxiv.org/abs/2305.13673",
    "context": "Title: Physics of Language Models: Part 1, Context-Free Grammar. (arXiv:2305.13673v1 [cs.CL])\nAbstract: We design experiments to study $\\textit{how}$ generative language models, like GPT, learn context-free grammars (CFGs) -- diverse language systems with a tree-like structure capturing many aspects of natural languages, programs, and human logics. CFGs are as hard as pushdown automata, and can be ambiguous so that verifying if a string satisfies the rules requires dynamic programming. We construct synthetic data and demonstrate that even for very challenging CFGs, pre-trained transformers can learn to generate sentences with near-perfect accuracy and remarkable $\\textit{diversity}$.  More importantly, we delve into the $\\textit{physical principles}$ behind how transformers learns CFGs. We discover that the hidden states within the transformer implicitly and $\\textit{precisely}$ encode the CFG structure (such as putting tree node information exactly on the subtree boundary), and learn to form \"boundary to boundary\" attentions that resemble dynamic programming. We also cover some extensio",
    "path": "papers/23/05/2305.13673.json",
    "total_tokens": 1184,
    "translated_title": "语言模型的物理学：第一部分，上下文无关文法。",
    "translated_abstract": "我们设计了实验来研究生成式语言模型（例如GPT）如何学习上下文无关文法（CFG）-具有树状结构的多样化语言系统，可捕捉许多自然语言，程序和人类逻辑的方面。CFG与下推自动机一样困难，可能是模棱两可的，因此验证字符串是否满足规则需要动态规划。我们构造了人造数据，并证明即使对于非常具有挑战性的CFG，预训练transformers也可以学会生成具有接近完美准确度和显着多样性的句子。更重要的是，我们深入探讨了transformers学习CFG背后的物理原理。我们发现transformer内部的隐藏状态隐含而精确地编码了CFG结构（如在子树边界上精确定位树节点信息），并学会形成类似动态规划的“边界到边界”的注意力。我们还涵盖了一些标准CFG的扩展，例如概率CFG和线性CFG，并展示transformers也可以学会这些扩展语法结构。我们的工作揭示了语言模型的内部工作原理，并为未来的模型设计和分析提供了启示。",
    "tldr": "本研究探究了生成式语言模型如何学习上下文无关文法（CFG），并通过构造人造数据证明了预训练transformers可以学会生成具有接近完美准确度和显着多样性的句子。研究发现transformer内部的隐藏状态隐含而精确地编码了CFG结构，学会形成类似动态规划的“边界到边界”的注意力。此外，还研究了标准CFG的扩展，例如概率CFG和线性CFG，并证明transformers也可以学会这些扩展语法结构。",
    "en_tdlr": "This study investigates how generative language models, like GPT, learn context-free grammars (CFGs), and demonstrates that pre-trained transformers can learn to generate sentences with near-perfect accuracy and remarkable diversity even for challenging CFGs. The study reveals that the hidden states within the transformer precisely encode the CFG structure and learn to form \"boundary to boundary\" attentions that resemble dynamic programming. Additionally, extensions of standard CFGs are covered, including probabilistic and linear CFGs, and transformers can learn these extended grammar structures as well."
}