{
    "title": "Component Training of Turbo Autoencoders. (arXiv:2305.09216v1 [cs.IT])",
    "abstract": "Isolated training with Gaussian priors (TGP) of the component autoencoders of turbo-autoencoder architectures enables faster, more consistent training and better generalization to arbitrary decoding iterations than training based on deep unfolding. We propose fitting the components via extrinsic information transfer (EXIT) charts to a desired behavior which enables scaling to larger message lengths ($k \\approx 1000$) while retaining competitive performance. To the best of our knowledge, this is the first autoencoder that performs close to classical codes in this regime. Although the binary cross-entropy (BCE) loss function optimizes the bit error rate (BER) of the components, the design via EXIT charts enables to focus on the block error rate (BLER). In serially concatenated systems the component-wise TGP approach is well known for inner components with a fixed outer binary interface, e.g., a learned inner code or equalizer, with an outer binary error correcting code. In this paper we ",
    "link": "http://arxiv.org/abs/2305.09216",
    "context": "Title: Component Training of Turbo Autoencoders. (arXiv:2305.09216v1 [cs.IT])\nAbstract: Isolated training with Gaussian priors (TGP) of the component autoencoders of turbo-autoencoder architectures enables faster, more consistent training and better generalization to arbitrary decoding iterations than training based on deep unfolding. We propose fitting the components via extrinsic information transfer (EXIT) charts to a desired behavior which enables scaling to larger message lengths ($k \\approx 1000$) while retaining competitive performance. To the best of our knowledge, this is the first autoencoder that performs close to classical codes in this regime. Although the binary cross-entropy (BCE) loss function optimizes the bit error rate (BER) of the components, the design via EXIT charts enables to focus on the block error rate (BLER). In serially concatenated systems the component-wise TGP approach is well known for inner components with a fixed outer binary interface, e.g., a learned inner code or equalizer, with an outer binary error correcting code. In this paper we ",
    "path": "papers/23/05/2305.09216.json",
    "total_tokens": 920,
    "translated_title": "Turbo自编码器的组件训练",
    "translated_abstract": "对Turbo自编码器结构的组件自编码器进行孤立高斯先验训练(TGP)可以实现更快、更一致的训练，并且对任意解码迭代具有更好的泛化能力。我们提出通过外部信息传递EXIT图来适应组件的期望行为，从而实现在大数据长度（k≈1000）时的扩展性，同时保持竞争性能。据我们所知，这是第一个在该区域内表现接近经典编码的自编码器。尽管二进制交叉熵(BCE)损失函数优化了组件的比特误码率(BER)，但是通过EXIT图的设计可以专注于块误码率(BLER)。在串联系统中，组件级别的TGP方法已为内部组件（具有固定外部二进制接口，例如学习的内部代码或均衡器）和外部二进制纠错码的等值器所熟知。本文提出始终采用TGP方法整个Turbo自编码器。",
    "tldr": "本文提出采用孤立训练的高斯先验Turbo自编码器的组件方法能够实现快速、一致、泛化性好的训练，并且具有竞争性能，其中借助EXIT图的设计可专注于块误码率而达到期望行为。",
    "en_tdlr": "This paper proposes an approach for training components of turbo autoencoders using isolated training with Gaussian priors, which enables faster, more consistent training and better generalization to arbitrary decoding iterations. The use of EXIT charts allows for scaling to larger message lengths while retaining competitive performance, and focusing on block error rate."
}