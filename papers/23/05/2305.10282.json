{
    "title": "Reward-agnostic Fine-tuning: Provable Statistical Benefits of Hybrid Reinforcement Learning. (arXiv:2305.10282v1 [cs.LG])",
    "abstract": "This paper studies tabular reinforcement learning (RL) in the hybrid setting, which assumes access to both an offline dataset and online interactions with the unknown environment. A central question boils down to how to efficiently utilize online data collection to strengthen and complement the offline dataset and enable effective policy fine-tuning. Leveraging recent advances in reward-agnostic exploration and model-based offline RL, we design a three-stage hybrid RL algorithm that beats the best of both worlds -- pure offline RL and pure online RL -- in terms of sample complexities. The proposed algorithm does not require any reward information during data collection. Our theory is developed based on a new notion called single-policy partial concentrability, which captures the trade-off between distribution mismatch and miscoverage and guides the interplay between offline and online data.",
    "link": "http://arxiv.org/abs/2305.10282",
    "context": "Title: Reward-agnostic Fine-tuning: Provable Statistical Benefits of Hybrid Reinforcement Learning. (arXiv:2305.10282v1 [cs.LG])\nAbstract: This paper studies tabular reinforcement learning (RL) in the hybrid setting, which assumes access to both an offline dataset and online interactions with the unknown environment. A central question boils down to how to efficiently utilize online data collection to strengthen and complement the offline dataset and enable effective policy fine-tuning. Leveraging recent advances in reward-agnostic exploration and model-based offline RL, we design a three-stage hybrid RL algorithm that beats the best of both worlds -- pure offline RL and pure online RL -- in terms of sample complexities. The proposed algorithm does not require any reward information during data collection. Our theory is developed based on a new notion called single-policy partial concentrability, which captures the trade-off between distribution mismatch and miscoverage and guides the interplay between offline and online data.",
    "path": "papers/23/05/2305.10282.json",
    "total_tokens": 857,
    "translated_title": "“无任何奖励信息的细调 Fine-tuning:基于混合增强学习的可证明统计优势”",
    "translated_abstract": "本论文研究了在混合环境中进行表格强化学习(RL)，该环境假设可以访问离线数据集并在未知环境中进行在线交互。其中一个核心问题在于如何利用在线数据收集来加强和补充离线数据集，从而实现有效的策略细调。本文借鉴了最近的无奖励探索和基于模型的离线RL 的进展，设计了一个三阶段的混合RL算法，其在样本复杂度方面优于仅使用离线RL 和仅使用在线RL 的最佳结果。所提出的算法在数据收集过程中不需要任何奖励信息。我们的理论是基于一个新概念——单策略局部集中性的，该概念捕捉了分布不匹配和覆盖错误之间的权衡，并指导离线和在线数据之间的相互作用。",
    "tldr": "本文提出一种新的三阶段混合RL算法，不需要奖励信息，有效地利用在线和离线数据，从而实现细调以获得更好的结果。"
}