{
    "title": "Consolidator: Mergeable Adapter with Grouped Connections for Visual Adaptation. (arXiv:2305.00603v1 [cs.CV])",
    "abstract": "Recently, transformers have shown strong ability as visual feature extractors, surpassing traditional convolution-based models in various scenarios. However, the success of vision transformers largely owes to their capacity to accommodate numerous parameters. As a result, new challenges for adapting large models to downstream tasks arise. On the one hand, classic fine-tuning tunes all parameters in a huge model for every task and thus easily falls into overfitting, leading to inferior performance. On the other hand, on resource-limited devices, fine-tuning stores a full copy of parameters and thus is usually impracticable for the shortage of storage space. However, few works have focused on how to efficiently and effectively transfer knowledge in a vision transformer. Existing methods did not dive into the properties of visual features, leading to inferior performance. Moreover, some of them bring heavy inference cost though benefiting storage. To tackle these problems, we propose cons",
    "link": "http://arxiv.org/abs/2305.00603",
    "context": "Title: Consolidator: Mergeable Adapter with Grouped Connections for Visual Adaptation. (arXiv:2305.00603v1 [cs.CV])\nAbstract: Recently, transformers have shown strong ability as visual feature extractors, surpassing traditional convolution-based models in various scenarios. However, the success of vision transformers largely owes to their capacity to accommodate numerous parameters. As a result, new challenges for adapting large models to downstream tasks arise. On the one hand, classic fine-tuning tunes all parameters in a huge model for every task and thus easily falls into overfitting, leading to inferior performance. On the other hand, on resource-limited devices, fine-tuning stores a full copy of parameters and thus is usually impracticable for the shortage of storage space. However, few works have focused on how to efficiently and effectively transfer knowledge in a vision transformer. Existing methods did not dive into the properties of visual features, leading to inferior performance. Moreover, some of them bring heavy inference cost though benefiting storage. To tackle these problems, we propose cons",
    "path": "papers/23/05/2305.00603.json",
    "total_tokens": 844,
    "translated_title": "Consolidator: 融合连接的可合并适配器，用于视觉领域的自适应",
    "translated_abstract": "近年来，transformer 作为视觉特征提取器表现出超越传统卷积模型的强大能力。然而，视觉 transformer 的成功在很大程度上归功于其容纳大量参数的能力。这导致将大型模型适应下游任务面临新的挑战。为了解决这些问题，本论文提出了一种名为 \"Consolidator\" 的融合连接的可合并适配器，用于视觉领域的自适应，它促进了视觉 transformer 的知识转移，实现了多个图像分类和对象检测任务的最新转移表现。",
    "tldr": "本文提出了一种名为 Consolidator 的 mergeable adapter with grouped connections for visual adaptation，促进了视觉 transformer 的知识转移，实现了多个图像分类和对象检测任务的最新转移表现。",
    "en_tdlr": "Consolidator, a mergeable adapter with grouped connections for visual adaptation, promotes knowledge transfer in vision transformers and achieves state-of-the-art transfer performance on multiple image classification and object detection tasks."
}