{
    "title": "Exploiting Explainability to Design Adversarial Attacks and Evaluate Attack Resilience in Hate-Speech Detection Models. (arXiv:2305.18585v1 [cs.CL])",
    "abstract": "The advent of social media has given rise to numerous ethical challenges, with hate speech among the most significant concerns. Researchers are attempting to tackle this problem by leveraging hate-speech detection and employing language models to automatically moderate content and promote civil discourse. Unfortunately, recent studies have revealed that hate-speech detection systems can be misled by adversarial attacks, raising concerns about their resilience. While previous research has separately addressed the robustness of these models under adversarial attacks and their interpretability, there has been no comprehensive study exploring their intersection. The novelty of our work lies in combining these two critical aspects, leveraging interpretability to identify potential vulnerabilities and enabling the design of targeted adversarial attacks. We present a comprehensive and comparative analysis of adversarial robustness exhibited by various hate-speech detection models. Our study e",
    "link": "http://arxiv.org/abs/2305.18585",
    "context": "Title: Exploiting Explainability to Design Adversarial Attacks and Evaluate Attack Resilience in Hate-Speech Detection Models. (arXiv:2305.18585v1 [cs.CL])\nAbstract: The advent of social media has given rise to numerous ethical challenges, with hate speech among the most significant concerns. Researchers are attempting to tackle this problem by leveraging hate-speech detection and employing language models to automatically moderate content and promote civil discourse. Unfortunately, recent studies have revealed that hate-speech detection systems can be misled by adversarial attacks, raising concerns about their resilience. While previous research has separately addressed the robustness of these models under adversarial attacks and their interpretability, there has been no comprehensive study exploring their intersection. The novelty of our work lies in combining these two critical aspects, leveraging interpretability to identify potential vulnerabilities and enabling the design of targeted adversarial attacks. We present a comprehensive and comparative analysis of adversarial robustness exhibited by various hate-speech detection models. Our study e",
    "path": "papers/23/05/2305.18585.json",
    "total_tokens": 883,
    "translated_title": "利用可解释性设计对抗攻击和评估仇恨言论检测模型的鲁棒性",
    "translated_abstract": "社交媒体的出现带来了许多伦理问题，其中仇恨言论是最重要的问题之一。研究人员正在尝试通过利用仇恨言论检测和使用语言模型来自动审查内容并促进文明讨论来解决这个问题。不幸的是，最近的研究表明，仇恨言论检测系统可能会被对抗攻击所误导，引起其鲁棒性的关注。尽管以前的研究已单独讨论了在对抗攻击下模型的鲁棒性和可解释性，但没有全面研究他们的交集。我们的工作的创新在于结合这两个关键方面，利用可解释性识别潜在的漏洞，从而设计有针对性的对抗攻击。我们对各种仇恨言论检测模型表现出的对抗鲁棒性进行了全面和比较分析。",
    "tldr": "本文结合可解释性和对抗攻击两个关键方面，研究了仇恨言论检测模型的鲁棒性，并提出了针对性的对抗攻击，对该领域的未来研究具有重要的意义。",
    "en_tdlr": "This paper combines interpretability and adversarial attacks to research the robustness of hate-speech detection models, presenting targeted attacks and providing important implications for future research."
}