{
    "title": "Weaker Than You Think: A Critical Look at Weakly Supervised Learning. (arXiv:2305.17442v2 [cs.CL] UPDATED)",
    "abstract": "Weakly supervised learning is a popular approach for training machine learning models in low-resource settings. Instead of requesting high-quality yet costly human annotations, it allows training models with noisy annotations obtained from various weak sources. Recently, many sophisticated approaches have been proposed for robust training under label noise, reporting impressive results. In this paper, we revisit the setup of these approaches and find that the benefits brought by these approaches are significantly overestimated. Specifically, we find that the success of existing weakly supervised learning approaches heavily relies on the availability of clean validation samples which, as we show, can be leveraged much more efficiently by simply training on them. After using these clean labels in training, the advantages of using these sophisticated approaches are mostly wiped out. This remains true even when reducing the size of the available clean data to just five samples per class, m",
    "link": "http://arxiv.org/abs/2305.17442",
    "context": "Title: Weaker Than You Think: A Critical Look at Weakly Supervised Learning. (arXiv:2305.17442v2 [cs.CL] UPDATED)\nAbstract: Weakly supervised learning is a popular approach for training machine learning models in low-resource settings. Instead of requesting high-quality yet costly human annotations, it allows training models with noisy annotations obtained from various weak sources. Recently, many sophisticated approaches have been proposed for robust training under label noise, reporting impressive results. In this paper, we revisit the setup of these approaches and find that the benefits brought by these approaches are significantly overestimated. Specifically, we find that the success of existing weakly supervised learning approaches heavily relies on the availability of clean validation samples which, as we show, can be leveraged much more efficiently by simply training on them. After using these clean labels in training, the advantages of using these sophisticated approaches are mostly wiped out. This remains true even when reducing the size of the available clean data to just five samples per class, m",
    "path": "papers/23/05/2305.17442.json",
    "total_tokens": 904,
    "translated_title": "比你想的要弱：对弱监督学习的批判性研究",
    "translated_abstract": "弱监督学习是一种在资源有限的情况下训练机器学习模型的流行方法。它允许使用从各种弱标注源获得的嘈杂标注来训练模型，而不是要求高质量但昂贵的人工标注。最近，许多精巧的方法已经被提出来在标签噪声下进行强大的训练，并报告了令人印象深刻的结果。在本文中，我们重新审视了这些方法的设置，并发现这些方法所带来的好处被显著高估了。具体来说，我们发现现有的弱监督学习方法的成功在很大程度上依赖于可用的干净验证样本，正如我们所展示的，我们可以通过简单地在其上进行训练来更有效地利用这些干净标签。在使用这些干净标签进行训练后，使用这些精巧方法的优势大部分被消除了。即使将可用的干净数据的大小减少到每类只有五个样本，这仍然成立。",
    "tldr": "这篇论文批判性地研究了弱监督学习方法，发现这些方法的好处被高估了，大多数优势可以通过简单地利用干净的训练数据实现。",
    "en_tdlr": "This paper critically examines weakly supervised learning approaches and finds that their benefits are overestimated, and most of the advantages can be achieved by simply utilizing clean training data."
}