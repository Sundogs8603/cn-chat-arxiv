{
    "title": "What is the best recipe for character-level encoder-only modelling?. (arXiv:2305.05461v1 [cs.CL])",
    "abstract": "This paper aims to benchmark recent progress in language understanding models that output contextualised representations at the character level. Many such modelling architectures and methods to train those architectures have been proposed, but it is currently unclear what the relative contributions of the architecture vs. the pretraining objective are to final model performance. We explore the design space of such models, comparing architectural innovations and a variety of different pretraining objectives on a suite of evaluation tasks with a fixed training procedure in order to find the currently optimal way to build and train character-level BERT-like models. We find that our best performing character-level model exceeds the performance of a token-based model trained with the same settings on the same data, suggesting that character-level models are ready for more widespread adoption. Unfortunately, the best method to train character-level models still relies on a subword-level toke",
    "link": "http://arxiv.org/abs/2305.05461",
    "context": "Title: What is the best recipe for character-level encoder-only modelling?. (arXiv:2305.05461v1 [cs.CL])\nAbstract: This paper aims to benchmark recent progress in language understanding models that output contextualised representations at the character level. Many such modelling architectures and methods to train those architectures have been proposed, but it is currently unclear what the relative contributions of the architecture vs. the pretraining objective are to final model performance. We explore the design space of such models, comparing architectural innovations and a variety of different pretraining objectives on a suite of evaluation tasks with a fixed training procedure in order to find the currently optimal way to build and train character-level BERT-like models. We find that our best performing character-level model exceeds the performance of a token-based model trained with the same settings on the same data, suggesting that character-level models are ready for more widespread adoption. Unfortunately, the best method to train character-level models still relies on a subword-level toke",
    "path": "papers/23/05/2305.05461.json",
    "total_tokens": 966,
    "translated_title": "字符级别编码器模型的最佳配方是什么？",
    "translated_abstract": "本文旨在对以字符级别输出上下文表示的语言理解模型中最新进展进行基准测试。已经提出了许多这样的建模结构和训练这些结构的方法，但当前仍不清楚架构与预训练目标对最终模型性能的相对贡献。作者探索了这些模型的设计空间，比较了体系结构创新和各种不同的预训练目标，在一个固定的训练过程中使用一套评估任务来寻找目前构建和训练字符级别BERT类型模型的最佳方法。作者发现，最好的字符级别模型的性能超过了在相同数据上使用相同设置训练的基于标记的模型，这表明字符级别模型已准备好更广泛的应用。不幸的是，训练字符级模型的最佳方法仍然依赖于子词级别的标记。",
    "tldr": "本文通过对字符级别BERT类型模型的设计空间和各种预训练目标的比较，找到了构建和训练字符级别模型的最佳方法。最好的字符级别模型的性能优于在相同数据上使用相同设置训练的基于标记的模型，这表明字符级别模型已准备好更广泛的应用，但训练字符级模型的最佳方法仍然依赖于子词级别的标记。"
}