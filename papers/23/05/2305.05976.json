{
    "title": "Say What You Mean! Large Language Models Speak Too Positively about Negative Commonsense Knowledge. (arXiv:2305.05976v1 [cs.CL])",
    "abstract": "Large language models (LLMs) have been widely studied for their ability to store and utilize positive knowledge. However, negative knowledge, such as \"lions don't live in the ocean\", is also ubiquitous in the world but rarely mentioned explicitly in the text. What do LLMs know about negative knowledge? This work examines the ability of LLMs to negative commonsense knowledge. We design a constrained keywords-to-sentence generation task (CG) and a Boolean question-answering task (QA) to probe LLMs. Our experiments reveal that LLMs frequently fail to generate valid sentences grounded in negative commonsense knowledge, yet they can correctly answer polar yes-or-no questions. We term this phenomenon the belief conflict of LLMs. Our further analysis shows that statistical shortcuts and negation reporting bias from language modeling pre-training cause this conflict.",
    "link": "http://arxiv.org/abs/2305.05976",
    "context": "Title: Say What You Mean! Large Language Models Speak Too Positively about Negative Commonsense Knowledge. (arXiv:2305.05976v1 [cs.CL])\nAbstract: Large language models (LLMs) have been widely studied for their ability to store and utilize positive knowledge. However, negative knowledge, such as \"lions don't live in the ocean\", is also ubiquitous in the world but rarely mentioned explicitly in the text. What do LLMs know about negative knowledge? This work examines the ability of LLMs to negative commonsense knowledge. We design a constrained keywords-to-sentence generation task (CG) and a Boolean question-answering task (QA) to probe LLMs. Our experiments reveal that LLMs frequently fail to generate valid sentences grounded in negative commonsense knowledge, yet they can correctly answer polar yes-or-no questions. We term this phenomenon the belief conflict of LLMs. Our further analysis shows that statistical shortcuts and negation reporting bias from language modeling pre-training cause this conflict.",
    "path": "papers/23/05/2305.05976.json",
    "total_tokens": 941,
    "translated_title": "说到做到! 大型语言模型在负面常识知识方面存在太过乐观的表述",
    "translated_abstract": "大型语言模型(LLMs)因能够存储和利用正面知识而被广泛研究。但是，负面知识，如“狮子不生活在海洋中”，也是世界上无处不在的，但很少在文本中明确提到。LLMs对负面常识知识了解多少？本文研究了LLMs对负面常识知识的了解能力。我们设计了一个有限制的关键词到句子生成任务(CG)和一个布尔型问答任务(QA)来探测LLMs。我们的实验揭示，LLMs经常无法生成基于负面常识知识的有效句子，但它们可以正确地回答极性的是或否问题。我们将这一现象称为LLMs的信念冲突。我们的进一步分析表明，语言建模预训练阶段的统计快捷方式和否定报告偏见引起了这种冲突。",
    "tldr": "本文研究了大型语言模型(LLMs)对负面常识知识的了解程度，发现LLMs在生成基于负面知识的有效句子方面存在困难，但在回答极性问题方面表现良好，这种信念冲突主要源于语言预训练时的统计快捷方式和否定报告偏见。",
    "en_tdlr": "This paper examines the ability of large language models (LLMs) to understand negative commonsense knowledge, revealing that LLMs struggle to generate valid sentences based on negative knowledge but perform well in polar question-answering, due to statistical shortcuts and negation reporting bias from pre-training."
}