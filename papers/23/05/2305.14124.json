{
    "title": "When Does Monolingual Data Help Multilingual Translation: The Role of Domain and Model Scale. (arXiv:2305.14124v2 [cs.CL] UPDATED)",
    "abstract": "Multilingual machine translation (MMT), trained on a mixture of parallel and monolingual data, is key for improving translation in low-resource language pairs. However, the literature offers conflicting results on the performance of different methods of including monolingual data. To resolve this, we examine how denoising autoencoding (DAE) and backtranslation (BT) impact MMT under different data conditions and model scales. Unlike prior studies, we use a realistic dataset of 100 translation directions and consider many domain combinations of monolingual and test data. We find that monolingual data generally helps MMT, but models are surprisingly brittle to domain mismatches, especially at smaller model scales. BT is beneficial when the parallel, monolingual, and test data sources are similar but can be detrimental otherwise, while DAE is less effective than previously reported. Next, we analyze the impact of scale (from 90M to 1.6B parameters) and find it is important for both methods",
    "link": "http://arxiv.org/abs/2305.14124",
    "context": "Title: When Does Monolingual Data Help Multilingual Translation: The Role of Domain and Model Scale. (arXiv:2305.14124v2 [cs.CL] UPDATED)\nAbstract: Multilingual machine translation (MMT), trained on a mixture of parallel and monolingual data, is key for improving translation in low-resource language pairs. However, the literature offers conflicting results on the performance of different methods of including monolingual data. To resolve this, we examine how denoising autoencoding (DAE) and backtranslation (BT) impact MMT under different data conditions and model scales. Unlike prior studies, we use a realistic dataset of 100 translation directions and consider many domain combinations of monolingual and test data. We find that monolingual data generally helps MMT, but models are surprisingly brittle to domain mismatches, especially at smaller model scales. BT is beneficial when the parallel, monolingual, and test data sources are similar but can be detrimental otherwise, while DAE is less effective than previously reported. Next, we analyze the impact of scale (from 90M to 1.6B parameters) and find it is important for both methods",
    "path": "papers/23/05/2305.14124.json",
    "total_tokens": 1067,
    "translated_title": "单语言数据何时有助于多语言翻译：领域和模型规模的作用",
    "translated_abstract": "多语言机器翻译（MMT）是通过混合平行和单语言数据进行训练，提高低资源语言对翻译的关键。然而，文献对于包含单语言数据的不同方法的表现存在争议。为了解决这个问题，我们研究了去噪自编码（DAE）和回译（BT）在不同数据条件和模型规模下对MMT的影响。与先前的研究不同，我们使用了一个实际数据集，包括100个翻译方向，并考虑了许多单语言和测试数据的领域组合。我们发现，单语言数据通常有助于MMT，但模型对领域不匹配的容忍性出乎意料地较差，尤其在较小的模型规模下。当平行、单语言和测试数据源相似时，回译是有益的，但在其他情况下可能是有害的，而DAE的效果不如先前报告的好。接下来，我们分析了规模（从90M到1.6B参数）的影响，发现它对两种方法都很重要。",
    "tldr": "本文研究了单语言数据在多语言翻译中的作用，发现单语言数据通常有助于多语言翻译，但模型对领域不匹配的容忍性较差，尤其在较小的模型规模下。回译在数据源相似的情况下是有益的，但在其他情况下可能是有害的，而去噪自编码的效果不如先前报告的好。规模对两种方法都很重要。"
}