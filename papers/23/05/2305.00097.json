{
    "title": "NNSplitter: An Active Defense Solution to DNN Model via Automated Weight Obfuscation. (arXiv:2305.00097v1 [cs.LG])",
    "abstract": "As a type of valuable intellectual property (IP), deep neural network (DNN) models have been protected by techniques like watermarking. However, such passive model protection cannot fully prevent model abuse. In this work, we propose an active model IP protection scheme, namely NNSplitter, which actively protects the model by splitting it into two parts: the obfuscated model that performs poorly due to weight obfuscation, and the model secrets consisting of the indexes and original values of the obfuscated weights, which can only be accessed by authorized users. NNSplitter uses the trusted execution environment to secure the secrets and a reinforcement learning-based controller to reduce the number of obfuscated weights while maximizing accuracy drop. Our experiments show that by only modifying 313 out of over 28 million (i.e., 0.001%) weights, the accuracy of the obfuscated VGG-11 model on Fashion-MNIST can drop to 10%. We also demonstrate that NNSplitter is stealthy and resilient aga",
    "link": "http://arxiv.org/abs/2305.00097",
    "context": "Title: NNSplitter: An Active Defense Solution to DNN Model via Automated Weight Obfuscation. (arXiv:2305.00097v1 [cs.LG])\nAbstract: As a type of valuable intellectual property (IP), deep neural network (DNN) models have been protected by techniques like watermarking. However, such passive model protection cannot fully prevent model abuse. In this work, we propose an active model IP protection scheme, namely NNSplitter, which actively protects the model by splitting it into two parts: the obfuscated model that performs poorly due to weight obfuscation, and the model secrets consisting of the indexes and original values of the obfuscated weights, which can only be accessed by authorized users. NNSplitter uses the trusted execution environment to secure the secrets and a reinforcement learning-based controller to reduce the number of obfuscated weights while maximizing accuracy drop. Our experiments show that by only modifying 313 out of over 28 million (i.e., 0.001%) weights, the accuracy of the obfuscated VGG-11 model on Fashion-MNIST can drop to 10%. We also demonstrate that NNSplitter is stealthy and resilient aga",
    "path": "papers/23/05/2305.00097.json",
    "total_tokens": 966,
    "translated_title": "NNSplitter：基于自动权重混淆的DNN模型主动防御方案",
    "translated_abstract": "深度神经网络模型作为一种有价值的知识产权，已经通过数字水印等技术进行保护。然而，这种被动模型保护并不能完全防止模型滥用。本研究提出了一种主动模型知识产权保护方案，即NNSplitter，通过将模型分为两部分来主动保护模型：一个表现较差的混淆模型和由混淆权重的索引和原始值组成的模型秘密，只有授权用户才能访问。NNSplitter利用可信执行环境来保护秘密，并采用基于强化学习的控制器来减少混淆权重的数量，同时最大化精度下降。我们的实验表明，仅修改超过2800万个权重的313个（即0.001％），混淆VGG-11模型在Fashion-MNIST上的精度可以降低到10％。我们还证明NNSplitter具有隐蔽性和韧性。",
    "tldr": "NNSplitter是一种主动保护深度神经网络模型知识产权的方案，通过将模型分为混淆模型和模型秘密两部分，采用可信执行环境和基于强化学习的控制器来最大化精度下降和减少混淆权重的数量。",
    "en_tdlr": "NNSplitter is an active defense solution designed to protect the intellectual property of deep neural network models. It splits the model into an obfuscated portion and a secret portion, uses trusted execution environments and reinforcement learning-based controllers to maximize accuracy drop while reducing the number of obfuscated weights."
}