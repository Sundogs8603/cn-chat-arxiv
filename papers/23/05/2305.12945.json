{
    "title": "ExplainCPE: A Free-text Explanation Benchmark of Chinese Pharmacist Examination. (arXiv:2305.12945v2 [cs.CL] UPDATED)",
    "abstract": "As ChatGPT and GPT-4 spearhead the development of Large Language Models (LLMs), more researchers are investigating their performance across various tasks. But more research needs to be done on the interpretability capabilities of LLMs, that is, the ability to generate reasons after an answer has been given. Existing explanation datasets are mostly English-language general knowledge questions, which leads to insufficient thematic and linguistic diversity. To address the language bias and lack of medical resources in generating rationales QA datasets, we present ExplainCPE (over 7k instances), a challenging medical benchmark in Simplified Chinese. We analyzed the errors of ChatGPT and GPT-4, pointing out the limitations of current LLMs in understanding text and computational reasoning. During the experiment, we also found that different LLMs have different preferences for in-context learning. ExplainCPE presents a significant challenge, but its potential for further investigation is prom",
    "link": "http://arxiv.org/abs/2305.12945",
    "context": "Title: ExplainCPE: A Free-text Explanation Benchmark of Chinese Pharmacist Examination. (arXiv:2305.12945v2 [cs.CL] UPDATED)\nAbstract: As ChatGPT and GPT-4 spearhead the development of Large Language Models (LLMs), more researchers are investigating their performance across various tasks. But more research needs to be done on the interpretability capabilities of LLMs, that is, the ability to generate reasons after an answer has been given. Existing explanation datasets are mostly English-language general knowledge questions, which leads to insufficient thematic and linguistic diversity. To address the language bias and lack of medical resources in generating rationales QA datasets, we present ExplainCPE (over 7k instances), a challenging medical benchmark in Simplified Chinese. We analyzed the errors of ChatGPT and GPT-4, pointing out the limitations of current LLMs in understanding text and computational reasoning. During the experiment, we also found that different LLMs have different preferences for in-context learning. ExplainCPE presents a significant challenge, but its potential for further investigation is prom",
    "path": "papers/23/05/2305.12945.json",
    "total_tokens": 949,
    "translated_title": "解释CPE：中国执业药师考试的自由文本解释基准。（arXiv:2305.12945v2 [cs.CL] 更新）",
    "translated_abstract": "随着ChatGPT和GPT-4引领大型语言模型（LLMs）的发展，越来越多的研究者正在研究它们在各种任务中的性能。但是，对LLMs的可解释性能力，也就是在给出答案后生成原因的能力，还需要进行更多的研究。现有的解释数据集主要是英语通用知识问题，导致主题和语言多样性不足。为了解决语言偏见和生成合理问答数据集中缺乏医疗资源的问题，我们提出了ExplainCPE（超过7k个实例），这是一个具有挑战性的简体中文医学基准。我们分析了ChatGPT和GPT-4的错误，指出了当前LLMs在理解文本和计算推理方面的局限性。在实验过程中，我们还发现不同的LLMs在上下文学习方面有不同的偏好。ExplainCPE提出了一个重大挑战，但它对进一步研究具有潜力。",
    "tldr": "ExplainCPE是一个具有挑战性的简体中文医学基准，用于解决大型语言模型的可解释性能力问题。该基准分析了ChatGPT和GPT-4的错误，并指出了当前模型在理解文本和计算推理方面的局限性。",
    "en_tdlr": "ExplainCPE is a challenging medical benchmark in Simplified Chinese that addresses the interpretability capabilities of large language models (LLMs). The benchmark analyzes the errors of ChatGPT and GPT-4, highlighting the limitations of current models in text comprehension and computational reasoning."
}