{
    "title": "Smaller3d: Smaller Models for 3D Semantic Segmentation Using Minkowski Engine and Knowledge Distillation Methods. (arXiv:2305.03188v1 [cs.CV])",
    "abstract": "There are various optimization techniques in the realm of 3D, including point cloud-based approaches that use mesh, texture, and voxels which optimize how you store, and how do calculate in 3D. These techniques employ methods such as feed-forward networks, 3D convolutions, graph neural networks, transformers, and sparse tensors. However, the field of 3D is one of the most computationally expensive fields, and these methods have yet to achieve their full potential due to their large capacity, complexity, and computation limits. This paper proposes the application of knowledge distillation techniques, especially for sparse tensors in 3D deep learning, to reduce model sizes while maintaining performance. We analyze and purpose different loss functions, including standard methods and combinations of various losses, to simulate the performance of state-of-the-art models of different Sparse Convolutional NNs. Our experiments are done on the standard ScanNet V2 dataset, and we achieved around",
    "link": "http://arxiv.org/abs/2305.03188",
    "context": "Title: Smaller3d: Smaller Models for 3D Semantic Segmentation Using Minkowski Engine and Knowledge Distillation Methods. (arXiv:2305.03188v1 [cs.CV])\nAbstract: There are various optimization techniques in the realm of 3D, including point cloud-based approaches that use mesh, texture, and voxels which optimize how you store, and how do calculate in 3D. These techniques employ methods such as feed-forward networks, 3D convolutions, graph neural networks, transformers, and sparse tensors. However, the field of 3D is one of the most computationally expensive fields, and these methods have yet to achieve their full potential due to their large capacity, complexity, and computation limits. This paper proposes the application of knowledge distillation techniques, especially for sparse tensors in 3D deep learning, to reduce model sizes while maintaining performance. We analyze and purpose different loss functions, including standard methods and combinations of various losses, to simulate the performance of state-of-the-art models of different Sparse Convolutional NNs. Our experiments are done on the standard ScanNet V2 dataset, and we achieved around",
    "path": "papers/23/05/2305.03188.json",
    "total_tokens": 839,
    "translated_title": "Smaller3d：使用Minkowski Engine和知识蒸馏方法进行3D语义分割的小型模型",
    "translated_abstract": "在3D领域，有许多优化技术，包括基于点云的方法，使用网格、纹理和体素来优化3D存储和计算。这篇论文提出了为了在保持性能的同时减小模型大小而采用的知识蒸馏技术，特别是针对稀疏张量在3D深度学习中的应用。我们分析和提出了不同的损失函数，包括标准方法和各种损失的组合，以模拟不同稀疏卷积NN的最先进模型的性能。我们的实验是在标准的ScanNet V2数据集上进行的，并且我们的结果优于现有的最佳模型。",
    "tldr": "本文提出了一种基于知识蒸馏技术的稀疏张量在3D深度学习中的小型模型，通过采用不同的损失函数，在保持性能的情况下减小模型的大小并获得优于现有模型的结果。",
    "en_tdlr": "This paper proposes a small model for 3D semantic segmentation using knowledge distillation techniques, especially for sparse tensors. By employing different loss functions, the model achieves superior performance while maintaining its small size."
}