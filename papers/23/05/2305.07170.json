{
    "title": "Towards Understanding and Improving GFlowNet Training. (arXiv:2305.07170v1 [cs.LG])",
    "abstract": "Generative flow networks (GFlowNets) are a family of algorithms that learn a generative policy to sample discrete objects $x$ with non-negative reward $R(x)$. Learning objectives guarantee the GFlowNet samples $x$ from the target distribution $p^*(x) \\propto R(x)$ when loss is globally minimized over all states or trajectories, but it is unclear how well they perform with practical limits on training resources. We introduce an efficient evaluation strategy to compare the learned sampling distribution to the target reward distribution. As flows can be underdetermined given training data, we clarify the importance of learned flows to generalization and matching $p^*(x)$ in practice. We investigate how to learn better flows, and propose (i) prioritized replay training of high-reward $x$, (ii) relative edge flow policy parametrization, and (iii) a novel guided trajectory balance objective, and show how it can solve a substructure credit assignment problem. We substantially improve sample e",
    "link": "http://arxiv.org/abs/2305.07170",
    "context": "Title: Towards Understanding and Improving GFlowNet Training. (arXiv:2305.07170v1 [cs.LG])\nAbstract: Generative flow networks (GFlowNets) are a family of algorithms that learn a generative policy to sample discrete objects $x$ with non-negative reward $R(x)$. Learning objectives guarantee the GFlowNet samples $x$ from the target distribution $p^*(x) \\propto R(x)$ when loss is globally minimized over all states or trajectories, but it is unclear how well they perform with practical limits on training resources. We introduce an efficient evaluation strategy to compare the learned sampling distribution to the target reward distribution. As flows can be underdetermined given training data, we clarify the importance of learned flows to generalization and matching $p^*(x)$ in practice. We investigate how to learn better flows, and propose (i) prioritized replay training of high-reward $x$, (ii) relative edge flow policy parametrization, and (iii) a novel guided trajectory balance objective, and show how it can solve a substructure credit assignment problem. We substantially improve sample e",
    "path": "papers/23/05/2305.07170.json",
    "total_tokens": 973,
    "translated_title": "关于理解和改善GFlowNet训练的研究",
    "translated_abstract": "生成流网络（GFlowNets）是一类算法，学习生成策略以从非负奖励x的目标分布$p^*（x）\\propto R（x）$中采样离散对象。学习目标保证GFlowNet对所有状态或轨迹全局最小化损失时，从目标分布$p^*（x）$中采样x，但在训练资源有限的情况下，其性能如何仍不清楚。我们引入了一种高效的评估策略，将所学的采样分布与目标奖励分布进行比较。由于流在给定训练数据时可能被欠定，因此我们阐明了学习流对于实践中的泛化和匹配$p^*（x）$的重要性。我们研究了如何学习更好的流，并提出了（i）优先回放训练高奖励$x$，（ii）相对边缘流策略参数化，和（iii）一种新的引导轨迹平衡目标，并展示了它如何解决子结构学分分配问题。我们显着提高了样本效率，从而提高了代理训练的效果。",
    "tldr": "该论文研究了生成流网络的训练问题，提出了三种改进方法，包括优先回放训练、相对边缘流、引导轨迹平衡目标，并在大幅度提高样本效率的同时改善了代理训练的效果。",
    "en_tdlr": "This paper studies the training problem of generative flow networks and proposes three methods for improvement, including prioritized replay training, relative edge flow, and guided trajectory balance objectives, achieving substantial improvement in sample efficiency and agent training."
}