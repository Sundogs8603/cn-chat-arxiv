{
    "title": "Multilevel Sentence Embeddings for Personality Prediction. (arXiv:2305.05748v1 [cs.CL])",
    "abstract": "Representing text into a multidimensional space can be done with sentence embedding models such as Sentence-BERT (SBERT). However, training these models when the data has a complex multilevel structure requires individually trained class-specific models, which increases time and computing costs. We propose a two step approach which enables us to map sentences according to their hierarchical memberships and polarity. At first we teach the upper level sentence space through an AdaCos loss function and then finetune with a novel loss function mainly based on the cosine similarity of intra-level pairs. We apply this method to three different datasets: two weakly supervised Big Five personality dataset obtained from English and Japanese Twitter data and the benchmark MNLI dataset. We show that our single model approach performs better than multiple class-specific classification models.",
    "link": "http://arxiv.org/abs/2305.05748",
    "context": "Title: Multilevel Sentence Embeddings for Personality Prediction. (arXiv:2305.05748v1 [cs.CL])\nAbstract: Representing text into a multidimensional space can be done with sentence embedding models such as Sentence-BERT (SBERT). However, training these models when the data has a complex multilevel structure requires individually trained class-specific models, which increases time and computing costs. We propose a two step approach which enables us to map sentences according to their hierarchical memberships and polarity. At first we teach the upper level sentence space through an AdaCos loss function and then finetune with a novel loss function mainly based on the cosine similarity of intra-level pairs. We apply this method to three different datasets: two weakly supervised Big Five personality dataset obtained from English and Japanese Twitter data and the benchmark MNLI dataset. We show that our single model approach performs better than multiple class-specific classification models.",
    "path": "papers/23/05/2305.05748.json",
    "total_tokens": 767,
    "translated_title": "多层句子嵌入用于人格预测",
    "translated_abstract": "句子嵌入模型，如Sentence-BERT（SBERT），可将文本表示为多维空间。然而，当数据具有复杂的多层结构时，训练这些模型需要单独训练类别特定的模型，增加了时间和计算成本。我们提出了一种两步法，使我们能够根据句子的层次成员资格和极性来映射句子。首先，我们通过AdaCos loss函数教授上层句子空间，然后通过一种主要基于层内成对余弦相似度的新损失函数进行微调。我们将此方法应用于三个不同的数据集：从英语和日语Twitter数据中获取的两个弱监督的Big Five人格数据集和基准MNLI数据集。我们表明，我们的单一模型方法比多个类别特定分类模型表现更好。",
    "tldr": "本文提出了一种可将文本表示为多维空间的方法，可映射具有复杂的多层结构的句子，并应用于人格预测。",
    "en_tdlr": "This paper proposes a method to represent text into a multidimensional space, which can map sentences with complex multilevel structures for personality prediction."
}