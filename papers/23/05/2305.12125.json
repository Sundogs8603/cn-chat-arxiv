{
    "title": "A Framework for Provably Stable and Consistent Training of Deep Feedforward Networks. (arXiv:2305.12125v1 [cs.LG])",
    "abstract": "We present a novel algorithm for training deep neural networks in supervised (classification and regression) and unsupervised (reinforcement learning) scenarios. This algorithm combines the standard stochastic gradient descent and the gradient clipping method. The output layer is updated using clipped gradients, the rest of the neural network is updated using standard gradients. Updating the output layer using clipped gradient stabilizes it. We show that the remaining layers are automatically stabilized provided the neural network is only composed of squashing (compact range) activations. We also present a novel squashing activation function - it is obtained by modifying a Gaussian Error Linear Unit (GELU) to have compact range - we call it Truncated GELU (tGELU). Unlike other squashing activations, such as sigmoid, the range of tGELU can be explicitly specified. As a consequence, the problem of vanishing gradients that arise due to a small range, e.g., in the case of a sigmoid activat",
    "link": "http://arxiv.org/abs/2305.12125",
    "context": "Title: A Framework for Provably Stable and Consistent Training of Deep Feedforward Networks. (arXiv:2305.12125v1 [cs.LG])\nAbstract: We present a novel algorithm for training deep neural networks in supervised (classification and regression) and unsupervised (reinforcement learning) scenarios. This algorithm combines the standard stochastic gradient descent and the gradient clipping method. The output layer is updated using clipped gradients, the rest of the neural network is updated using standard gradients. Updating the output layer using clipped gradient stabilizes it. We show that the remaining layers are automatically stabilized provided the neural network is only composed of squashing (compact range) activations. We also present a novel squashing activation function - it is obtained by modifying a Gaussian Error Linear Unit (GELU) to have compact range - we call it Truncated GELU (tGELU). Unlike other squashing activations, such as sigmoid, the range of tGELU can be explicitly specified. As a consequence, the problem of vanishing gradients that arise due to a small range, e.g., in the case of a sigmoid activat",
    "path": "papers/23/05/2305.12125.json",
    "total_tokens": 957,
    "translated_title": "深度前馈网络的稳定一致训练框架",
    "translated_abstract": "我们提出了一种新算法，用于监督（分类和回归）和非监督（强化学习）场景下的深度神经网络训练。该算法结合了标准随机梯度下降和梯度裁剪方法。输出层使用裁剪梯度更新，其余神经网络使用标准梯度更新。使用裁剪梯度更新输出层可以使其稳定。我们表明，只要神经网络仅由压缩（紧凑范围）激活函数组成，其余层将自动被稳定。我们还提出了一种新颖的压缩激活函数 - 通过修改高斯误差线性单元（GELU）来获得 - 我们称之为Truncated GELU（tGELU）。与其他压缩激活函数（如Sigmoid）不同，tGELU的范围可以明确指定。因此，在一些激活函数（例如Sigmod）的范围较小时会出现的梯度消失问题可以避免。",
    "tldr": "该论文提出了一种用于深度神经网络的稳定一致训练框架，其核心在于结合了标准随机梯度下降和梯度裁剪方法以及使用自动稳定的压缩激活函数，从而提高了深度神经网络的稳定性和训练效果。",
    "en_tdlr": "This paper proposes a framework for stable and consistent training of deep neural networks, which combines standard stochastic gradient descent and gradient clipping method, and utilizes automatically stabilized squashing activation function to improve the stability and training effectiveness of deep neural networks."
}