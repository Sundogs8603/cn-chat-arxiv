{
    "title": "Meta-learning For Vision-and-language Cross-lingual Transfer. (arXiv:2305.14843v2 [cs.CL] UPDATED)",
    "abstract": "Current pre-trained vison-language models (PVLMs) achieve excellent performance on a range of multi-modal datasets. Recent work has aimed at building multilingual models, and a range of novel multilingual multi-modal datasets have been proposed. Current PVLMs typically perform poorly on these datasets when used for multi-modal zero-shot or few-shot cross-lingual transfer, especially for low-resource languages. To alleviate this problem, we propose a novel meta-learning fine-tuning framework. Our framework makes current PVLMs rapidly adaptive to new languages in vision-language scenarios by designing MAML in a cross-lingual multi-modal manner. Experiments show that our method boosts the performance of current state-of-the-art PVLMs in both zero-shot and few-shot cross-lingual transfer on a range of vision-language understanding tasks and datasets (XVNLI, xGQA, MaRVL, xFlicker&Co)",
    "link": "http://arxiv.org/abs/2305.14843",
    "context": "Title: Meta-learning For Vision-and-language Cross-lingual Transfer. (arXiv:2305.14843v2 [cs.CL] UPDATED)\nAbstract: Current pre-trained vison-language models (PVLMs) achieve excellent performance on a range of multi-modal datasets. Recent work has aimed at building multilingual models, and a range of novel multilingual multi-modal datasets have been proposed. Current PVLMs typically perform poorly on these datasets when used for multi-modal zero-shot or few-shot cross-lingual transfer, especially for low-resource languages. To alleviate this problem, we propose a novel meta-learning fine-tuning framework. Our framework makes current PVLMs rapidly adaptive to new languages in vision-language scenarios by designing MAML in a cross-lingual multi-modal manner. Experiments show that our method boosts the performance of current state-of-the-art PVLMs in both zero-shot and few-shot cross-lingual transfer on a range of vision-language understanding tasks and datasets (XVNLI, xGQA, MaRVL, xFlicker&Co)",
    "path": "papers/23/05/2305.14843.json",
    "total_tokens": 886,
    "translated_title": "视觉与语言跨语言转移的元学习",
    "translated_abstract": "当前的预训练视觉-语言模型在一系列多模态数据集上取得了出色的性能。最近的研究旨在构建多语言模型，并提出了一系列新颖的多语言多模态数据集。但是，当这些模型用于多模态零样本或少样本的跨语言转移，尤其是对于资源有限的语言时，当前的视觉-语言模型通常表现不佳。为了缓解这个问题，我们提出了一种新颖的元学习微调框架。我们的框架通过以跨语言多模态的方式设计MAML，使得当前的视觉-语言模型能够快速适应视觉-语言场景中的新语言。实验证明，我们的方法显著提升了当前最先进的视觉-语言模型在一系列视觉-语言理解任务和数据集（XVNLI，xGQA，MaRVL，xFlicker&Co）上的零样本和少样本跨语言转移的性能。",
    "tldr": "本研究提出了一种元学习微调框架，通过设计跨语言多模态的MAML，使得当前的视觉-语言模型能够快速适应新的语言，从而在跨语言转移中显著提升了性能。",
    "en_tdlr": "This paper proposes a meta-learning fine-tuning framework that enables current vision-language models to rapidly adapt to new languages in cross-lingual transfer, significantly improving performance."
}