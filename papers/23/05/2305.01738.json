{
    "title": "Leveraging Factored Action Spaces for Efficient Offline Reinforcement Learning in Healthcare. (arXiv:2305.01738v1 [cs.LG])",
    "abstract": "Many reinforcement learning (RL) applications have combinatorial action spaces, where each action is a composition of sub-actions. A standard RL approach ignores this inherent factorization structure, resulting in a potential failure to make meaningful inferences about rarely observed sub-action combinations; this is particularly problematic for offline settings, where data may be limited. In this work, we propose a form of linear Q-function decomposition induced by factored action spaces. We study the theoretical properties of our approach, identifying scenarios where it is guaranteed to lead to zero bias when used to approximate the Q-function. Outside the regimes with theoretical guarantees, we show that our approach can still be useful because it leads to better sample efficiency without necessarily sacrificing policy optimality, allowing us to achieve a better bias-variance trade-off. Across several offline RL problems using simulators and real-world datasets motivated by healthca",
    "link": "http://arxiv.org/abs/2305.01738",
    "context": "Title: Leveraging Factored Action Spaces for Efficient Offline Reinforcement Learning in Healthcare. (arXiv:2305.01738v1 [cs.LG])\nAbstract: Many reinforcement learning (RL) applications have combinatorial action spaces, where each action is a composition of sub-actions. A standard RL approach ignores this inherent factorization structure, resulting in a potential failure to make meaningful inferences about rarely observed sub-action combinations; this is particularly problematic for offline settings, where data may be limited. In this work, we propose a form of linear Q-function decomposition induced by factored action spaces. We study the theoretical properties of our approach, identifying scenarios where it is guaranteed to lead to zero bias when used to approximate the Q-function. Outside the regimes with theoretical guarantees, we show that our approach can still be useful because it leads to better sample efficiency without necessarily sacrificing policy optimality, allowing us to achieve a better bias-variance trade-off. Across several offline RL problems using simulators and real-world datasets motivated by healthca",
    "path": "papers/23/05/2305.01738.json",
    "total_tokens": 1056,
    "translated_title": "利用因子化动作空间在医疗保健中进行高效的离线强化学习",
    "translated_abstract": "许多强化学习应用程序具有组合动作空间，其中每个动作是子动作的组合。标准强化学习方法忽略了这种固有的分解结构，导致可能对少见的子动作组合做出的推理没有意义；这在离线设置下尤其问题突出，因为数据可能受限。在这项工作中，我们提出了一种由因子化动作空间引起的线性Q函数分解的形式。我们研究了我们的方法的理论性质，确定了当用于近似Q函数时保证产生零偏差的情况。在具有理论保证的范围之外的情况下，我们表明我们的方法仍然是有用的，因为它提高了采样效率而不一定牺牲策略最优性，允许我们实现更好的偏差-方差权衡。在使用由医疗保健启示的模拟器和实际数据集进行的几个离线强化学习问题中，我们证明了我们的方法比标准方法具有更快的收敛速度、更好的性能和更高的采样效率。",
    "tldr": "本论文提出了一种利用因子化动作空间的线性Q函数分解形式的方法，用于解决离线强化学习中存在的动作组合问题，该方法在提高采样效率的同时并不牺牲策略最优性，通过模拟器和实际数据集的几个离线强化学习问题的实验表明，相较于标准方法，该方法具有更快的收敛速度、更好的性能和更高的采样效率。",
    "en_tdlr": "This paper proposes a linear Q-function decomposition induced by factored action spaces to solve the problem of action combination in offline reinforcement learning. The method improves sample efficiency without sacrificing policy optimality, and experiments on simulators and real-world datasets show that it achieves faster convergence, better performance, and higher sample efficiency compared to standard approaches."
}