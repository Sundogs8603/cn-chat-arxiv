{
    "title": "Dynamic Regularized Sharpness Aware Minimization in Federated Learning: Approaching Global Consistency and Smooth Landscape. (arXiv:2305.11584v1 [cs.LG])",
    "abstract": "In federated learning (FL), a cluster of local clients are chaired under the coordination of the global server and cooperatively train one model with privacy protection. Due to the multiple local updates and the isolated non-iid dataset, clients are prone to overfit into their own optima, which extremely deviates from the global objective and significantly undermines the performance. Most previous works only focus on enhancing the consistency between the local and global objectives to alleviate this prejudicial client drifts from the perspective of the optimization view, whose performance would be prominently deteriorated on the high heterogeneity. In this work, we propose a novel and general algorithm {\\ttfamily FedSMOO} by jointly considering the optimization and generalization targets to efficiently improve the performance in FL. Concretely, {\\ttfamily FedSMOO} adopts a dynamic regularizer to guarantee the local optima towards the global objective, which is meanwhile revised by the ",
    "link": "http://arxiv.org/abs/2305.11584",
    "context": "Title: Dynamic Regularized Sharpness Aware Minimization in Federated Learning: Approaching Global Consistency and Smooth Landscape. (arXiv:2305.11584v1 [cs.LG])\nAbstract: In federated learning (FL), a cluster of local clients are chaired under the coordination of the global server and cooperatively train one model with privacy protection. Due to the multiple local updates and the isolated non-iid dataset, clients are prone to overfit into their own optima, which extremely deviates from the global objective and significantly undermines the performance. Most previous works only focus on enhancing the consistency between the local and global objectives to alleviate this prejudicial client drifts from the perspective of the optimization view, whose performance would be prominently deteriorated on the high heterogeneity. In this work, we propose a novel and general algorithm {\\ttfamily FedSMOO} by jointly considering the optimization and generalization targets to efficiently improve the performance in FL. Concretely, {\\ttfamily FedSMOO} adopts a dynamic regularizer to guarantee the local optima towards the global objective, which is meanwhile revised by the ",
    "path": "papers/23/05/2305.11584.json",
    "total_tokens": 786,
    "translated_title": "动态正则化锐度感知联邦学习中的全局一致性和平滑场景方法",
    "translated_abstract": "在联邦学习（FL）中，一组本地客户端在全局服务器的协调下协作训练带有隐私保护的模型。由于多个本地更新和隔离的非 iid 数据集，客户端容易过度拟合到自己的自身最优解，这极大地偏离了全局目标并严重削弱了性能。本文提出了一种新颖的通用算法 FedSMOO，通过同时考虑优化和泛化目标来高效地提高 FL 中的性能。",
    "tldr": "本文提出了一种动态正则化锐度感知联邦学习方法，通过同时考虑优化和泛化目标来高效地提高联邦学习的性能。",
    "en_tdlr": "This paper proposes a novel algorithm, FedSMOO, which adopts a dynamic regularizer and a sharpness-aware mechanism to jointly improve the optimization and generalization performance in federated learning."
}