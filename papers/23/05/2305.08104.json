{
    "title": "Federated TD Learning over Finite-Rate Erasure Channels: Linear Speedup under Markovian Sampling. (arXiv:2305.08104v1 [cs.LG])",
    "abstract": "Federated learning (FL) has recently gained much attention due to its effectiveness in speeding up supervised learning tasks under communication and privacy constraints. However, whether similar speedups can be established for reinforcement learning remains much less understood theoretically. Towards this direction, we study a federated policy evaluation problem where agents communicate via a central aggregator to expedite the evaluation of a common policy. To capture typical communication constraints in FL, we consider finite capacity up-link channels that can drop packets based on a Bernoulli erasure model. Given this setting, we propose and analyze QFedTD - a quantized federated temporal difference learning algorithm with linear function approximation. Our main technical contribution is to provide a finite-sample analysis of QFedTD that (i) highlights the effect of quantization and erasures on the convergence rate; and (ii) establishes a linear speedup w.r.t. the number of agents un",
    "link": "http://arxiv.org/abs/2305.08104",
    "context": "Title: Federated TD Learning over Finite-Rate Erasure Channels: Linear Speedup under Markovian Sampling. (arXiv:2305.08104v1 [cs.LG])\nAbstract: Federated learning (FL) has recently gained much attention due to its effectiveness in speeding up supervised learning tasks under communication and privacy constraints. However, whether similar speedups can be established for reinforcement learning remains much less understood theoretically. Towards this direction, we study a federated policy evaluation problem where agents communicate via a central aggregator to expedite the evaluation of a common policy. To capture typical communication constraints in FL, we consider finite capacity up-link channels that can drop packets based on a Bernoulli erasure model. Given this setting, we propose and analyze QFedTD - a quantized federated temporal difference learning algorithm with linear function approximation. Our main technical contribution is to provide a finite-sample analysis of QFedTD that (i) highlights the effect of quantization and erasures on the convergence rate; and (ii) establishes a linear speedup w.r.t. the number of agents un",
    "path": "papers/23/05/2305.08104.json",
    "total_tokens": 927,
    "translated_title": "基于有限速抹通道的联邦 TD 学习：马尔可夫采样下的线性加速",
    "translated_abstract": "近年来，由于其在通信和隐私约束下加速监督学习任务的有效性，联邦学习 (FL)引起了广泛关注。然而，关于强化学习是否可以实现类似的加速，在理论上仍然不太清楚。针对这个方向，我们研究了一种联邦策略评估问题，在其中代理通过中央聚合器进行通信，以加快共同策略的评估。为了捕捉 FL 中的典型通信约束，我们考虑可以根据伯努利擦拭模型丢弃数据包的有限容量上行链路通道。在这种情况下，我们提出并分析了 QFedTD-一种带有线性函数逼近的量化联邦时序差分学习算法。我们的主要技术贡献是提供 QFedTD 的有限样本分析，该分析突出了量化和抹除对收敛速率的影响；并建立了与代理数量成线性的加速度。(翻译仅供参考，不代表达意完全正确)",
    "tldr": "该论文提出了一种适用于联邦学习的算法 QFedTD，在有限速抹通道下使用线性函数逼近以达到线性加速的效果，在强化学习方面具有实际应用价值。",
    "en_tdlr": "This paper proposes an algorithm QFedTD for federated learning, which achieves linear speedup using linear function approximation under the limited-rate erasure channels. It has practical value in reinforcement learning."
}