{
    "title": "A General-Purpose Multilingual Document Encoder. (arXiv:2305.07016v1 [cs.CL])",
    "abstract": "Massively multilingual pretrained transformers (MMTs) have tremendously pushed the state of the art on multilingual NLP and cross-lingual transfer of NLP models in particular. While a large body of work leveraged MMTs to mine parallel data and induce bilingual document embeddings, much less effort has been devoted to training general-purpose (massively) multilingual document encoder that can be used for both supervised and unsupervised document-level tasks. In this work, we pretrain a massively multilingual document encoder as a hierarchical transformer model (HMDE) in which a shallow document transformer contextualizes sentence representations produced by a state-of-the-art pretrained multilingual sentence encoder. We leverage Wikipedia as a readily available source of comparable documents for creating training data, and train HMDE by means of a cross-lingual contrastive objective, further exploiting the category hierarchy of Wikipedia for creation of difficult negatives. We evaluate ",
    "link": "http://arxiv.org/abs/2305.07016",
    "context": "Title: A General-Purpose Multilingual Document Encoder. (arXiv:2305.07016v1 [cs.CL])\nAbstract: Massively multilingual pretrained transformers (MMTs) have tremendously pushed the state of the art on multilingual NLP and cross-lingual transfer of NLP models in particular. While a large body of work leveraged MMTs to mine parallel data and induce bilingual document embeddings, much less effort has been devoted to training general-purpose (massively) multilingual document encoder that can be used for both supervised and unsupervised document-level tasks. In this work, we pretrain a massively multilingual document encoder as a hierarchical transformer model (HMDE) in which a shallow document transformer contextualizes sentence representations produced by a state-of-the-art pretrained multilingual sentence encoder. We leverage Wikipedia as a readily available source of comparable documents for creating training data, and train HMDE by means of a cross-lingual contrastive objective, further exploiting the category hierarchy of Wikipedia for creation of difficult negatives. We evaluate ",
    "path": "papers/23/05/2305.07016.json",
    "total_tokens": 871,
    "translated_title": "通用多语言文档编码器",
    "translated_abstract": "大规模多语言预训练转换器（MMTs）已经极大地推动了多语言NLP和特别是NLP模型的跨语言转移的最新进展。尽管许多工作利用MMTs来挖掘平行数据和诱导双语文档嵌入，但很少有工作专门用于训练通用（大规模）多语言文档编码器，可用于监督和非监督文档级任务。在本文中，我们预先训练了一种大规模多语言文档编码器，作为一个分层转换器模型（HMDE），其中一个浅层文档转换器对最先进的预先训练的多语言句子编码器产生的句子表示进行上下文处理。我们利用维基百科作为可用的可比较文件源来创建训练数据，并通过跨语言对比目标来训练HMDE，进一步利用维基百科的类别层次结构来创建难以区分的负样本。",
    "tldr": "本文提出了一种通用的大规模多语言文档编码器，使用维基百科作为数据来源，并采用跨语言对比目标进行训练，可用于监督和非监督文档级任务。",
    "en_tdlr": "This paper proposes a general-purpose multilingual document encoder, pretrained using cross-lingual contrastive objective with Wikipedia as the training data source, and can be used for both supervised and unsupervised document-level tasks."
}