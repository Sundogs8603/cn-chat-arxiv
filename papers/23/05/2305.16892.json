{
    "title": "Feature Adaptation for Sparse Linear Regression. (arXiv:2305.16892v1 [cs.DS])",
    "abstract": "Sparse linear regression is a central problem in high-dimensional statistics. We study the correlated random design setting, where the covariates are drawn from a multivariate Gaussian $N(0,\\Sigma)$, and we seek an estimator with small excess risk.  If the true signal is $t$-sparse, information-theoretically, it is possible to achieve strong recovery guarantees with only $O(t\\log n)$ samples. However, computationally efficient algorithms have sample complexity linear in (some variant of) the condition number of $\\Sigma$. Classical algorithms such as the Lasso can require significantly more samples than necessary even if there is only a single sparse approximate dependency among the covariates.  We provide a polynomial-time algorithm that, given $\\Sigma$, automatically adapts the Lasso to tolerate a small number of approximate dependencies. In particular, we achieve near-optimal sample complexity for constant sparsity and if $\\Sigma$ has few ``outlier'' eigenvalues. Our algorithm fits i",
    "link": "http://arxiv.org/abs/2305.16892",
    "context": "Title: Feature Adaptation for Sparse Linear Regression. (arXiv:2305.16892v1 [cs.DS])\nAbstract: Sparse linear regression is a central problem in high-dimensional statistics. We study the correlated random design setting, where the covariates are drawn from a multivariate Gaussian $N(0,\\Sigma)$, and we seek an estimator with small excess risk.  If the true signal is $t$-sparse, information-theoretically, it is possible to achieve strong recovery guarantees with only $O(t\\log n)$ samples. However, computationally efficient algorithms have sample complexity linear in (some variant of) the condition number of $\\Sigma$. Classical algorithms such as the Lasso can require significantly more samples than necessary even if there is only a single sparse approximate dependency among the covariates.  We provide a polynomial-time algorithm that, given $\\Sigma$, automatically adapts the Lasso to tolerate a small number of approximate dependencies. In particular, we achieve near-optimal sample complexity for constant sparsity and if $\\Sigma$ has few ``outlier'' eigenvalues. Our algorithm fits i",
    "path": "papers/23/05/2305.16892.json",
    "total_tokens": 926,
    "translated_title": "稀疏线性回归的特征适应",
    "translated_abstract": "稀疏线性回归是高维统计学中的核心问题。本文研究相关随机设计环境，其中协变量是从多元高斯分布$N(0,\\Sigma)$中绘制的，我们寻求具有小过量风险的估计器。如果真实信号是$t$-稀疏的，则在信息理论上，仅需$O(t\\log n)$个样本就可以获得强大的恢复保证。然而，具有计算效率的算法的样本复杂度是$\\Sigma$的某种变体的条件数的线性。即使在协变量中仅有单一的稀疏近似依赖关系的情况下，像Lasso这样的经典算法也可能需要比必要更多的样本。我们提供了一个多项式时间算法，可以在给定$\\Sigma$的情况下，自动适应Lasso以容忍小的近似依赖关系数量。特别地，在具有常数稀疏性并且$\\Sigma$具有少量“异常”特征值的情况下，我们实现了近乎最佳的样本复杂度。",
    "tldr": "本研究提供了一种可自适应的算法，可以在一定程度上容忍协变量中的近似依赖关系数量，从而达到在一定条件下具有近乎最佳样本复杂度的稀疏线性回归。",
    "en_tdlr": "This study provides an adaptable algorithm that can tolerate a small number of approximate dependencies among covariates, achieving near-optimal sample complexity for sparse linear regression under certain conditions."
}