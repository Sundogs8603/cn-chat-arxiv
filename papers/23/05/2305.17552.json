{
    "title": "Online Nonstochastic Model-Free Reinforcement Learning. (arXiv:2305.17552v2 [cs.LG] UPDATED)",
    "abstract": "We investigate robust model-free reinforcement learning algorithms designed for environments that may be dynamic or even adversarial. Traditional state-based policies often struggle to accommodate the challenges imposed by the presence of unmodeled disturbances in such settings. Moreover, optimizing linear state-based policies pose an obstacle for efficient optimization, leading to nonconvex objectives, even in benign environments like linear dynamical systems.  Drawing inspiration from recent advancements in model-based control, we introduce a novel class of policies centered on disturbance signals. We define several categories of these signals, which we term pseudo-disturbances, and develop corresponding policy classes based on them. We provide efficient and practical algorithms for optimizing these policies.  Next, we examine the task of online adaptation of reinforcement learning agents in the face of adversarial disturbances. Our methods seamlessly integrate with any black-box mod",
    "link": "http://arxiv.org/abs/2305.17552",
    "context": "Title: Online Nonstochastic Model-Free Reinforcement Learning. (arXiv:2305.17552v2 [cs.LG] UPDATED)\nAbstract: We investigate robust model-free reinforcement learning algorithms designed for environments that may be dynamic or even adversarial. Traditional state-based policies often struggle to accommodate the challenges imposed by the presence of unmodeled disturbances in such settings. Moreover, optimizing linear state-based policies pose an obstacle for efficient optimization, leading to nonconvex objectives, even in benign environments like linear dynamical systems.  Drawing inspiration from recent advancements in model-based control, we introduce a novel class of policies centered on disturbance signals. We define several categories of these signals, which we term pseudo-disturbances, and develop corresponding policy classes based on them. We provide efficient and practical algorithms for optimizing these policies.  Next, we examine the task of online adaptation of reinforcement learning agents in the face of adversarial disturbances. Our methods seamlessly integrate with any black-box mod",
    "path": "papers/23/05/2305.17552.json",
    "total_tokens": 861,
    "translated_title": "在线非随机无模型强化学习",
    "translated_abstract": "我们研究了针对可能是动态或者具有对抗性的环境的鲁棒无模型强化学习算法。传统的基于状态的策略常常难以适应这些环境中未建模干扰所带来的挑战。此外，优化基于线性状态的策略在效率优化方面存在困难，即使在像线性动态系统这样良好的环境中也会出现非凸的目标函数。受模型控制最新进展的启发，我们引入了一种新颖的以干扰信号为中心的策略类别。我们定义了几个这些信号的类别，并基于它们开发了相应的策略类别。我们提供了用于优化这些策略的高效和实用的算法。接下来，我们研究了面对对抗性干扰时强化学习代理的在线适应任务。我们的方法与任何黑盒模型无缝集成。",
    "tldr": "本论文研究了在线非随机无模型强化学习算法，针对动态或者具有对抗性的环境提出了一种以干扰信号为中心的策略类别，并开发了高效实用的优化算法。",
    "en_tdlr": "This paper investigates online nonstochastic model-free reinforcement learning algorithms for dynamic or adversarial environments and proposes a novel class of policies centered on disturbance signals. Efficient and practical algorithms are developed to optimize these policies."
}