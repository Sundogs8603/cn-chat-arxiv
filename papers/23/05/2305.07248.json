{
    "title": "Quantile-Based Deep Reinforcement Learning using Two-Timescale Policy Gradient Algorithms. (arXiv:2305.07248v1 [cs.LG])",
    "abstract": "Classical reinforcement learning (RL) aims to optimize the expected cumulative reward. In this work, we consider the RL setting where the goal is to optimize the quantile of the cumulative reward. We parameterize the policy controlling actions by neural networks, and propose a novel policy gradient algorithm called Quantile-Based Policy Optimization (QPO) and its variant Quantile-Based Proximal Policy Optimization (QPPO) for solving deep RL problems with quantile objectives. QPO uses two coupled iterations running at different timescales for simultaneously updating quantiles and policy parameters, whereas QPPO is an off-policy version of QPO that allows multiple updates of parameters during one simulation episode, leading to improved algorithm efficiency. Our numerical results indicate that the proposed algorithms outperform the existing baseline algorithms under the quantile criterion.",
    "link": "http://arxiv.org/abs/2305.07248",
    "context": "Title: Quantile-Based Deep Reinforcement Learning using Two-Timescale Policy Gradient Algorithms. (arXiv:2305.07248v1 [cs.LG])\nAbstract: Classical reinforcement learning (RL) aims to optimize the expected cumulative reward. In this work, we consider the RL setting where the goal is to optimize the quantile of the cumulative reward. We parameterize the policy controlling actions by neural networks, and propose a novel policy gradient algorithm called Quantile-Based Policy Optimization (QPO) and its variant Quantile-Based Proximal Policy Optimization (QPPO) for solving deep RL problems with quantile objectives. QPO uses two coupled iterations running at different timescales for simultaneously updating quantiles and policy parameters, whereas QPPO is an off-policy version of QPO that allows multiple updates of parameters during one simulation episode, leading to improved algorithm efficiency. Our numerical results indicate that the proposed algorithms outperform the existing baseline algorithms under the quantile criterion.",
    "path": "papers/23/05/2305.07248.json",
    "total_tokens": 844,
    "translated_title": "基于分位数的深度强化学习及其两时间标度策略梯度算法",
    "translated_abstract": "传统的强化学习（RL）旨在优化期望累积奖励。本文中，我们考虑了优化累积奖励分位数的RL设置。我们使用神经网络对控制动作的策略进行参数化，并提出了一种称为Quantile-Based Policy Optimization（QPO）的新型策略梯度算法及其变体Quantile-Based Proximal Policy Optimization（QPPO），用于解决具有量化目标的深度RL问题。QPO使用两个耦合迭代同时在不同的时间标度上更新分位数和策略参数，而QPPO是QPO的离线版本，允许在一个模拟回合中多次更新参数，从而提高了算法效率。我们的数值实验表明，在分位数标准下，所提出的算法优于现有基线算法。",
    "tldr": "本文探讨了优化累积奖励分位数的强化学习设置，提出了QPO和其变体QPPO算法，并使用神经网络对控制动作的策略进行参数化。实验结果表明该算法优于现有基线算法。",
    "en_tdlr": "This paper discusses the reinforcement learning setting where the goal is to optimize the quantile of the cumulative reward. QPO and its variant QPPO algorithms are proposed with neural network parameterization of the policy controlling actions. The experimental results show that the proposed algorithms outperform existing baseline algorithms under the quantile criterion."
}