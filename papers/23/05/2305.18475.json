{
    "title": "Approximation theory of transformer networks for sequence modeling. (arXiv:2305.18475v1 [cs.LG])",
    "abstract": "The transformer is a widely applied architecture in sequence modeling applications, but the theoretical understanding of its working principles is limited. In this work, we investigate the ability of transformers to approximate sequential relationships. We first prove a universal approximation theorem for the transformer hypothesis space. From its derivation, we identify a novel notion of regularity under which we can prove an explicit approximation rate estimate. This estimate reveals key structural properties of the transformer and suggests the types of sequence relationships that the transformer is adapted to approximating. In particular, it allows us to concretely discuss the structural bias between the transformer and classical sequence modeling methods, such as recurrent neural networks. Our findings are supported by numerical experiments.",
    "link": "http://arxiv.org/abs/2305.18475",
    "context": "Title: Approximation theory of transformer networks for sequence modeling. (arXiv:2305.18475v1 [cs.LG])\nAbstract: The transformer is a widely applied architecture in sequence modeling applications, but the theoretical understanding of its working principles is limited. In this work, we investigate the ability of transformers to approximate sequential relationships. We first prove a universal approximation theorem for the transformer hypothesis space. From its derivation, we identify a novel notion of regularity under which we can prove an explicit approximation rate estimate. This estimate reveals key structural properties of the transformer and suggests the types of sequence relationships that the transformer is adapted to approximating. In particular, it allows us to concretely discuss the structural bias between the transformer and classical sequence modeling methods, such as recurrent neural networks. Our findings are supported by numerical experiments.",
    "path": "papers/23/05/2305.18475.json",
    "total_tokens": 819,
    "translated_title": "序列建模的变压器网络的逼近理论",
    "translated_abstract": "变压器是序列建模应用中广泛应用的架构，但其工作原理的理论理解有限。在本文中，我们研究了变压器逼近序列关系的能力。我们首先证明了变压器假设空间的普遍逼近定理。通过推导，我们确定了一种新的规律概念，在此概念下，我们可以证明一个明确的逼近速率估计。这个估计揭示了变压器的关键结构特性，并暗示了变压器适用于逼近哪些类型的序列关系。特别地，它使我们能够具体地讨论变压器与传统序列建模方法（如循环神经网络）之间的结构偏差。我们的研究结果得到了数字实验的支持。",
    "tldr": "本文证明了变压器假设空间的普遍逼近定理，并提出了一种新的规律概念用于精确逼近速率估计，揭示了变压器适用于逼近哪些类型的序列关系，并讨论了其与传统序列建模方法之间的结构偏差。",
    "en_tdlr": "This paper proves a universal approximation theorem for the transformer hypothesis space and proposes a new notion of regularity for precise approximation rate estimates, revealing the types of sequence relationships that the transformer is adapted to approximate and discussing its structural bias compared to traditional sequence modeling methods such as recurrent neural networks."
}