{
    "title": "Understanding the Initial Condensation of Convolutional Neural Networks. (arXiv:2305.09947v1 [cs.LG])",
    "abstract": "Previous research has shown that fully-connected networks with small initialization and gradient-based training methods exhibit a phenomenon known as condensation during training. This phenomenon refers to the input weights of hidden neurons condensing into isolated orientations during training, revealing an implicit bias towards simple solutions in the parameter space. However, the impact of neural network structure on condensation has not been investigated yet. In this study, we focus on the investigation of convolutional neural networks (CNNs). Our experiments suggest that when subjected to small initialization and gradient-based training methods, kernel weights within the same CNN layer also cluster together during training, demonstrating a significant degree of condensation. Theoretically, we demonstrate that in a finite training period, kernels of a two-layer CNN with small initialization will converge to one or a few directions. This work represents a step towards a better under",
    "link": "http://arxiv.org/abs/2305.09947",
    "context": "Title: Understanding the Initial Condensation of Convolutional Neural Networks. (arXiv:2305.09947v1 [cs.LG])\nAbstract: Previous research has shown that fully-connected networks with small initialization and gradient-based training methods exhibit a phenomenon known as condensation during training. This phenomenon refers to the input weights of hidden neurons condensing into isolated orientations during training, revealing an implicit bias towards simple solutions in the parameter space. However, the impact of neural network structure on condensation has not been investigated yet. In this study, we focus on the investigation of convolutional neural networks (CNNs). Our experiments suggest that when subjected to small initialization and gradient-based training methods, kernel weights within the same CNN layer also cluster together during training, demonstrating a significant degree of condensation. Theoretically, we demonstrate that in a finite training period, kernels of a two-layer CNN with small initialization will converge to one or a few directions. This work represents a step towards a better under",
    "path": "papers/23/05/2305.09947.json",
    "total_tokens": 948,
    "translated_title": "理解卷积神经网络的初始凝结",
    "translated_abstract": "先前的研究表明，具有小初始化和基于梯度的训练方法的全连接网络在训练期间表现出一种称为凝结的现象。这种现象指的是隐层神经元的输入权重在训练期间凝聚成孤立的方向，揭示了参数空间中朝向简单解决方案的隐含偏差。然而，神经网络结构对凝聚的影响尚未得到研究。在本研究中，我们专注于卷积神经网络（CNN）的研究。我们的实验表明，当受到小初始化和基于梯度的训练方法的影响时，CNN层内的卷积核权重在训练期间也会聚集在一起，显示出显着的凝聚度。在理论上，我们证明在有限的训练时间内，小初始化的两层CNN的卷积核将会收敛到一个或几个方向。这项工作代表了更好地理解卷积神经网络在训练期间的行为的一步。",
    "tldr": "本研究揭示了小初始化和基于梯度的训练方法下卷积神经网络的凝聚现象，并在理论上证明了在有限训练期间，CNN的卷积核将会收敛到一个或几个方向。",
    "en_tdlr": "This study reveals the condensation phenomenon of convolutional neural networks with small initialization and gradient-based training methods, showing that kernel weights within the same layer cluster together during training. Theoretically, it is demonstrated that kernels of a two-layer CNN with small initialization will converge to one or a few directions, representing a step towards better understanding of the behavior of CNNs during training."
}