{
    "title": "Learning to Jump: Thinning and Thickening Latent Counts for Generative Modeling. (arXiv:2305.18375v1 [cs.LG])",
    "abstract": "Learning to denoise has emerged as a prominent paradigm to design state-of-the-art deep generative models for natural images. How to use it to model the distributions of both continuous real-valued data and categorical data has been well studied in recently proposed diffusion models. However, it is found in this paper to have limited ability in modeling some other types of data, such as count and non-negative continuous data, that are often highly sparse, skewed, heavy-tailed, and/or overdispersed. To this end, we propose learning to jump as a general recipe for generative modeling of various types of data. Using a forward count thinning process to construct learning objectives to train a deep neural network, it employs a reverse count thickening process to iteratively refine its generation through that network. We demonstrate when learning to jump is expected to perform comparably to learning to denoise, and when it is expected to perform better. For example, learning to jump is recom",
    "link": "http://arxiv.org/abs/2305.18375",
    "context": "Title: Learning to Jump: Thinning and Thickening Latent Counts for Generative Modeling. (arXiv:2305.18375v1 [cs.LG])\nAbstract: Learning to denoise has emerged as a prominent paradigm to design state-of-the-art deep generative models for natural images. How to use it to model the distributions of both continuous real-valued data and categorical data has been well studied in recently proposed diffusion models. However, it is found in this paper to have limited ability in modeling some other types of data, such as count and non-negative continuous data, that are often highly sparse, skewed, heavy-tailed, and/or overdispersed. To this end, we propose learning to jump as a general recipe for generative modeling of various types of data. Using a forward count thinning process to construct learning objectives to train a deep neural network, it employs a reverse count thickening process to iteratively refine its generation through that network. We demonstrate when learning to jump is expected to perform comparably to learning to denoise, and when it is expected to perform better. For example, learning to jump is recom",
    "path": "papers/23/05/2305.18375.json",
    "total_tokens": 1009,
    "translated_title": "学习跳跃: 薄化和加厚潜在计数用于生成建模",
    "translated_abstract": "学习去噪已成为设计最先进的深度生成模型（如扩散模型）的重要范式，用于建模连续的实值数据和分类数据已经有很好的研究。然而，本文发现学习去噪在建模某些其他类型的数据（如计数和非负连续数据）时能力有限，这些数据经常是高度稀疏、倾斜、重尾或过度分散的。为此，我们提出了学习跳跃作为各种类型数据的生成建模的通用方法。使用正向计数稀化方法构建学习目标，训练深度神经网络，通过逆向计数加厚过程迭代地改进其生成结果。我们演示了什么情况下学习跳跃与学习去噪表现相当，并且什么情况下学习跳跃表现更好。例如，建议在建模计数和非负连续数据时使用学习跳跃，这些数据往往具有稀疏性、倾斜性、重尾性或过度分散性。",
    "tldr": "本文探讨了如何使用学习跳跃方法来生成建模各种类型的数据，特别是对于计数和非负连续数据等高稀疏度、倾斜度、重尾度或过度分散度的数据，使用学习跳跃相比于学习去噪有更好的效果。"
}