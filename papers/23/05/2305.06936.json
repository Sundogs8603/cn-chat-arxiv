{
    "title": "An Option-Dependent Analysis of Regret Minimization Algorithms in Finite-Horizon Semi-Markov Decision Processes. (arXiv:2305.06936v1 [cs.LG])",
    "abstract": "A large variety of real-world Reinforcement Learning (RL) tasks is characterized by a complex and heterogeneous structure that makes end-to-end (or flat) approaches hardly applicable or even infeasible. Hierarchical Reinforcement Learning (HRL) provides general solutions to address these problems thanks to a convenient multi-level decomposition of the tasks, making their solution accessible. Although often used in practice, few works provide theoretical guarantees to justify this outcome effectively. Thus, it is not yet clear when to prefer such approaches compared to standard flat ones. In this work, we provide an option-dependent upper bound to the regret suffered by regret minimization algorithms in finite-horizon problems. We illustrate that the performance improvement derives from the planning horizon reduction induced by the temporal abstraction enforced by the hierarchical structure. Then, focusing on a sub-setting of HRL approaches, the options framework, we highlight how the a",
    "link": "http://arxiv.org/abs/2305.06936",
    "context": "Title: An Option-Dependent Analysis of Regret Minimization Algorithms in Finite-Horizon Semi-Markov Decision Processes. (arXiv:2305.06936v1 [cs.LG])\nAbstract: A large variety of real-world Reinforcement Learning (RL) tasks is characterized by a complex and heterogeneous structure that makes end-to-end (or flat) approaches hardly applicable or even infeasible. Hierarchical Reinforcement Learning (HRL) provides general solutions to address these problems thanks to a convenient multi-level decomposition of the tasks, making their solution accessible. Although often used in practice, few works provide theoretical guarantees to justify this outcome effectively. Thus, it is not yet clear when to prefer such approaches compared to standard flat ones. In this work, we provide an option-dependent upper bound to the regret suffered by regret minimization algorithms in finite-horizon problems. We illustrate that the performance improvement derives from the planning horizon reduction induced by the temporal abstraction enforced by the hierarchical structure. Then, focusing on a sub-setting of HRL approaches, the options framework, we highlight how the a",
    "path": "papers/23/05/2305.06936.json",
    "total_tokens": 963,
    "translated_title": "有限时间半马尔科夫决策进程中遗憾最小化算法的依赖于选项分析。",
    "translated_abstract": "许多真实世界中的强化学习任务都具有复杂和异构的结构，导致端到端学习方式难以应用或甚至不可行。分层强化学习（HRL）通过方便的任务多级分解提供了解决这些问题的通用解决方案，使得它们的解决变得易于访问。虽然实际应用中经常使用，但很少有作品提供理论保证有效地证明这一结果。因此，与标准平面方法相比何时更愿意使用这些方法尚不清楚。在这项工作中，我们为有限时间问题中遗憾最小化算法提供了一个依赖于选项的上界。我们展示了由分层结构引起的时间抽象所带来的计划时间减少，推动了性能的提升。然后，专注于HRL方法的一个子集要素框架，我们强调了要素如何可以有效地减少学习的计算成本和提高其鲁棒性。",
    "tldr": "本文研究了在有限时间半马尔科夫决策进程中，分层强化学习方法的优化问题，提供了一种通过降低时间分辨率来减少计划时间的选项依赖上界算法，并验证了通过要素框架实现计算成本减少和鲁棒性提高的可行性。"
}