{
    "title": "Zero is Not Hero Yet: Benchmarking Zero-Shot Performance of LLMs for Financial Tasks. (arXiv:2305.16633v1 [cs.CL])",
    "abstract": "Recently large language models (LLMs) like ChatGPT have shown impressive performance on many natural language processing tasks with zero-shot. In this paper, we investigate the effectiveness of zero-shot LLMs in the financial domain. We compare the performance of ChatGPT along with some open-source generative LLMs in zero-shot mode with RoBERTa fine-tuned on annotated data. We address three inter-related research questions on data annotation, performance gaps, and the feasibility of employing generative models in the finance domain. Our findings demonstrate that ChatGPT performs well even without labeled data but fine-tuned models generally outperform it. Our research also highlights how annotating with generative models can be time-intensive. Our codebase is publicly available on GitHub under CC BY-NC 4.0 license.",
    "link": "http://arxiv.org/abs/2305.16633",
    "context": "Title: Zero is Not Hero Yet: Benchmarking Zero-Shot Performance of LLMs for Financial Tasks. (arXiv:2305.16633v1 [cs.CL])\nAbstract: Recently large language models (LLMs) like ChatGPT have shown impressive performance on many natural language processing tasks with zero-shot. In this paper, we investigate the effectiveness of zero-shot LLMs in the financial domain. We compare the performance of ChatGPT along with some open-source generative LLMs in zero-shot mode with RoBERTa fine-tuned on annotated data. We address three inter-related research questions on data annotation, performance gaps, and the feasibility of employing generative models in the finance domain. Our findings demonstrate that ChatGPT performs well even without labeled data but fine-tuned models generally outperform it. Our research also highlights how annotating with generative models can be time-intensive. Our codebase is publicly available on GitHub under CC BY-NC 4.0 license.",
    "path": "papers/23/05/2305.16633.json",
    "total_tokens": 883,
    "translated_title": "零基础大语言模型在金融任务中的表现基准测试",
    "translated_abstract": "最近，像ChatGPT这样的大型语言模型已经展现出在许多零基础自然语言处理任务上的惊人表现。在本文中，我们研究了零基础LLMs在金融领域中的有效性。我们将ChatGPT与一些开源生成型LLM以及在注释数据上进行RoBERTa微调的性能在零基础模式下进行了比较。我们解决了关于数据注释、性能差距以及在金融领域使用生成模型的可行性的三个相关研究问题。我们的研究结果表明，即使没有标记数据，ChatGPT的表现也很好，但经过微调的模型通常表现更好。我们的研究还强调了使用生成型模型进行注释可能是耗时的。我们的代码库在CC BY-NC 4.0许可下公开在GitHub上。",
    "tldr": "研究比较了零基础LLM和RoBERTa在金融领域的性能表现，发现即使没有标记数据，ChatGPT的表现也很好，但微调后的模型通常表现更好，使用生成模型进行数据注释可能耗时",
    "en_tdlr": "This paper benchmarks the zero-shot performance of large language models (LLMs) in the financial domain and compares ChatGPT with open-source generative LLMs and RoBERTa fine-tuned on annotated data. The findings show that ChatGPT performs well even without labeled data, but fine-tuned models generally outperform it, and annotating data with generative models can be time-intensive."
}