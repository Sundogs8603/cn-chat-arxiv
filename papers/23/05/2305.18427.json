{
    "title": "GRD: A Generative Approach for Interpretable Reward Redistribution in Reinforcement Learning. (arXiv:2305.18427v1 [cs.LG])",
    "abstract": "A major challenge in reinforcement learning is to determine which state-action pairs are responsible for future rewards that are delayed. Return Decomposition offers a solution by redistributing rewards from observed sequences while preserving policy invariance. While the majority of current approaches construct the reward redistribution in an uninterpretable manner, we propose to explicitly model the contributions of state and action from a causal perspective, resulting in an interpretable return decomposition. In this paper, we start by studying the role of causal generative models in return decomposition by characterizing the generation of Markovian rewards and trajectory-wise long-term return and further propose a framework, called Generative Return Decomposition (GRD), for policy optimization in delayed reward scenarios. Specifically, GRD first identifies the unobservable Markovian rewards and causal relations in the generative process. Then, GRD makes use of the identified causal",
    "link": "http://arxiv.org/abs/2305.18427",
    "context": "Title: GRD: A Generative Approach for Interpretable Reward Redistribution in Reinforcement Learning. (arXiv:2305.18427v1 [cs.LG])\nAbstract: A major challenge in reinforcement learning is to determine which state-action pairs are responsible for future rewards that are delayed. Return Decomposition offers a solution by redistributing rewards from observed sequences while preserving policy invariance. While the majority of current approaches construct the reward redistribution in an uninterpretable manner, we propose to explicitly model the contributions of state and action from a causal perspective, resulting in an interpretable return decomposition. In this paper, we start by studying the role of causal generative models in return decomposition by characterizing the generation of Markovian rewards and trajectory-wise long-term return and further propose a framework, called Generative Return Decomposition (GRD), for policy optimization in delayed reward scenarios. Specifically, GRD first identifies the unobservable Markovian rewards and causal relations in the generative process. Then, GRD makes use of the identified causal",
    "path": "papers/23/05/2305.18427.json",
    "total_tokens": 940,
    "translated_title": "GRD: 一种在强化学习中用于可解释奖励再分配的生成方式",
    "translated_abstract": "在强化学习中，一个重要的挑战是确定哪些状态-行动对应该对未来的分步奖励负责。Return Decomposition提供了一种解决方案，通过重新分配观测序列中的奖励来保持策略不变。虽然大多数现有方法都以不可解释的方式构建奖励再分配，但我们建议采用因果透视来明确建模状态和行动的贡献，从而产生可解释的返回分解。本文首先研究了因果生成模型在返回分解中的作用，通过描述马尔可夫奖励和基于轨迹的长期回报的生成过程，并进一步提出了一种名为生成返回分解（GRD）的框架，用于延迟奖励场景中的策略优化。具体而言，GRD首先确定生成过程中不可观测的马尔可夫奖励和因果关系，然后利用确定的因果模型计算可观测奖励的期望，进而提高策略性能。",
    "tldr": "本文提出了一种可解释奖励再分配的方法，通过因果透视建模状态和行动贡献，产生可解释的返回分解。生成返回分解（GRD）框架用于延迟奖励场景中的策略优化。",
    "en_tdlr": "This paper proposes a generative approach for interpretable reward redistribution in reinforcement learning, called GRD, which models the contributions of state and action from a causal perspective. The approach utilizes a causal model to identify unobservable Markovian rewards, calculate observable rewards' expected value, and improve policy performance in a delayed reward scenario."
}