{
    "title": "Convergence of a Normal Map-based Prox-SGD Method under the KL Inequality. (arXiv:2305.05828v1 [math.OC])",
    "abstract": "In this paper, we present a novel stochastic normal map-based algorithm ($\\mathsf{norM}\\text{-}\\mathsf{SGD}$) for nonconvex composite-type optimization problems and discuss its convergence properties. Using a time window-based strategy, we first analyze the global convergence behavior of $\\mathsf{norM}\\text{-}\\mathsf{SGD}$ and it is shown that every accumulation point of the generated sequence of iterates $\\{\\boldsymbol{x}^k\\}_k$ corresponds to a stationary point almost surely and in an expectation sense. The obtained results hold under standard assumptions and extend the more limited convergence guarantees of the basic proximal stochastic gradient method. In addition, based on the well-known Kurdyka-{\\L}ojasiewicz (KL) analysis framework, we provide novel point-wise convergence results for the iterates $\\{\\boldsymbol{x}^k\\}_k$ and derive convergence rates that depend on the underlying KL exponent $\\boldsymbol{\\theta}$ and the step size dynamics $\\{\\alpha_k\\}_k$. Specifically, for the ",
    "link": "http://arxiv.org/abs/2305.05828",
    "context": "Title: Convergence of a Normal Map-based Prox-SGD Method under the KL Inequality. (arXiv:2305.05828v1 [math.OC])\nAbstract: In this paper, we present a novel stochastic normal map-based algorithm ($\\mathsf{norM}\\text{-}\\mathsf{SGD}$) for nonconvex composite-type optimization problems and discuss its convergence properties. Using a time window-based strategy, we first analyze the global convergence behavior of $\\mathsf{norM}\\text{-}\\mathsf{SGD}$ and it is shown that every accumulation point of the generated sequence of iterates $\\{\\boldsymbol{x}^k\\}_k$ corresponds to a stationary point almost surely and in an expectation sense. The obtained results hold under standard assumptions and extend the more limited convergence guarantees of the basic proximal stochastic gradient method. In addition, based on the well-known Kurdyka-{\\L}ojasiewicz (KL) analysis framework, we provide novel point-wise convergence results for the iterates $\\{\\boldsymbol{x}^k\\}_k$ and derive convergence rates that depend on the underlying KL exponent $\\boldsymbol{\\theta}$ and the step size dynamics $\\{\\alpha_k\\}_k$. Specifically, for the ",
    "path": "papers/23/05/2305.05828.json",
    "total_tokens": 952,
    "translated_title": "基于正态映射的Prox-SGD方法在KL不等式下的收敛性",
    "translated_abstract": "本文提出了一种新颖的随机正态映射算法（$\\mathsf{norM}\\text{-}\\mathsf{SGD}$）用于非凸复合型优化问题，并讨论了其收敛性质。使用基于时间窗口的策略，首先分析了$\\mathsf{norM}\\text{-}\\mathsf{SGD}$的全局收敛行为，并证明了所生成的迭代序列$\\{\\boldsymbol{x}^k\\}_k$的每个累积点几乎确定地和期望上都对应于一个稳定点。所得结果在标准假设下成立，并扩展了基本Proximal随机梯度法的更有限的收敛保证。此外，基于著名的Kurdyka-{\\L}ojasiewicz（KL）分析框架，我们为迭代序列$\\{\\boldsymbol{x}^k\\}_k$提供了新的逐点收敛结果，并得出了取决于基础KL指数$\\boldsymbol{\\theta}$和步长动态$\\{\\alpha_k\\}_k$的收敛速率。",
    "tldr": "本文提出了一种新的随机正态映射算法用于非凸复合型优化问题，并证明其收敛性质。该方法扩展了基本Proximal随机梯度法的更有限的收敛保证。",
    "en_tdlr": "This paper proposes a novel normal map-based algorithm for nonconvex composite-type optimization problems and proves its convergence properties by deriving convergence rates that depend on the underlying KL exponent and the step size dynamics. The proposed method extends the more limited convergence guarantees of the basic proximal stochastic gradient method."
}