{
    "title": "Inverse Dynamics Pretraining Learns Good Representations for Multitask Imitation. (arXiv:2305.16985v2 [cs.LG] UPDATED)",
    "abstract": "In recent years, domains such as natural language processing and image recognition have popularized the paradigm of using large datasets to pretrain representations that can be effectively transferred to downstream tasks. In this work we evaluate how such a paradigm should be done in imitation learning, where both pretraining and finetuning data are trajectories collected by experts interacting with an unknown environment. Namely, we consider a setting where the pretraining corpus consists of multitask demonstrations and the task for each demonstration is set by an unobserved latent context variable. The goal is to use the pretraining corpus to learn a low dimensional representation of the high dimensional (e.g., visual) observation space which can be transferred to a novel context for finetuning on a limited dataset of demonstrations. Among a variety of possible pretraining objectives, we argue that inverse dynamics modeling -- i.e., predicting an action given the observations appeari",
    "link": "http://arxiv.org/abs/2305.16985",
    "context": "Title: Inverse Dynamics Pretraining Learns Good Representations for Multitask Imitation. (arXiv:2305.16985v2 [cs.LG] UPDATED)\nAbstract: In recent years, domains such as natural language processing and image recognition have popularized the paradigm of using large datasets to pretrain representations that can be effectively transferred to downstream tasks. In this work we evaluate how such a paradigm should be done in imitation learning, where both pretraining and finetuning data are trajectories collected by experts interacting with an unknown environment. Namely, we consider a setting where the pretraining corpus consists of multitask demonstrations and the task for each demonstration is set by an unobserved latent context variable. The goal is to use the pretraining corpus to learn a low dimensional representation of the high dimensional (e.g., visual) observation space which can be transferred to a novel context for finetuning on a limited dataset of demonstrations. Among a variety of possible pretraining objectives, we argue that inverse dynamics modeling -- i.e., predicting an action given the observations appeari",
    "path": "papers/23/05/2305.16985.json",
    "total_tokens": 864,
    "translated_title": "逆动力学预训练为多任务模仿学习学习良好的表示",
    "translated_abstract": "近年来，自然语言处理和图像识别等领域普及了使用大型数据集预训练表示的范例，可以有效地转移到下游任务中。在本文中，我们评估了在模仿学习中应该如何进行这样的范例，其中预训练和微调数据都是由专家与未知环境交互产生的轨迹。具体而言，我们考虑了一种情况，预训练语料库由多任务演示组成，每个演示的任务由一个未观察到的潜在上下文变量设定。目标是使用预训练语料库学习高维（例如视觉）观察空间的低维表示，该表示可以转移到微调数据集，以在有限的演示数据上进行微调。在各种可能的预训练目标中，我们认为逆动力学建模是合适的，即根据观察来预测动作。",
    "tldr": "本文研究了在多任务模仿学习中的逆动力学预训练方法，通过预训练学习高维观察空间的低维表示，并将其转移到微调数据集进行任务执行。",
    "en_tdlr": "This paper investigates the use of inverse dynamics pretraining in multitask imitation learning to learn low dimensional representations of high dimensional observation space, which can be transferred for finetuning on a limited dataset."
}