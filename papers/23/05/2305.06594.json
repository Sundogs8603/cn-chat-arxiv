{
    "title": "V2Meow: Meowing to the Visual Beat via Music Generation. (arXiv:2305.06594v1 [cs.SD])",
    "abstract": "Generating high quality music that complements the visual content of a video is a challenging task. Most existing visual conditioned music generation systems generate symbolic music data, such as MIDI files, instead of raw audio waveform. Given the limited availability of symbolic music data, such methods can only generate music for a few instruments or for specific types of visual input. In this paper, we propose a novel approach called V2Meow that can generate high-quality music audio that aligns well with the visual semantics of a diverse range of video input types. Specifically, the proposed music generation system is a multi-stage autoregressive model which is trained with a number of O(100K) music audio clips paired with video frames, which are mined from in-the-wild music videos, and no parallel symbolic music data is involved. V2Meow is able to synthesize high-fidelity music audio waveform solely conditioned on pre-trained visual features extracted from an arbitrary silent vide",
    "link": "http://arxiv.org/abs/2305.06594",
    "context": "Title: V2Meow: Meowing to the Visual Beat via Music Generation. (arXiv:2305.06594v1 [cs.SD])\nAbstract: Generating high quality music that complements the visual content of a video is a challenging task. Most existing visual conditioned music generation systems generate symbolic music data, such as MIDI files, instead of raw audio waveform. Given the limited availability of symbolic music data, such methods can only generate music for a few instruments or for specific types of visual input. In this paper, we propose a novel approach called V2Meow that can generate high-quality music audio that aligns well with the visual semantics of a diverse range of video input types. Specifically, the proposed music generation system is a multi-stage autoregressive model which is trained with a number of O(100K) music audio clips paired with video frames, which are mined from in-the-wild music videos, and no parallel symbolic music data is involved. V2Meow is able to synthesize high-fidelity music audio waveform solely conditioned on pre-trained visual features extracted from an arbitrary silent vide",
    "path": "papers/23/05/2305.06594.json",
    "total_tokens": 915,
    "translated_title": "V2Meow: 通过音乐生成器跟随视觉节拍进行“喵叫”(arXiv:2305.06594v1 [cs.SD])",
    "translated_abstract": "生成与视频视觉内容相匹配的高质量音乐是一项具有挑战性的任务。大多数现有的视觉条件音乐生成系统生成符号音乐数据，如MIDI文件，而不是原始音频波形。考虑到符号音乐数据有限的情况下，这些方法只能为少数乐器或特定类型的视觉输入生成音乐。本文提出了一种名为V2Meow的新方法，它可以生成与各种类型的视频输入的视觉语义相匹配的高质量音频。具体而言，所提出的音乐生成系统是一个多阶段自回归模型，它是通过与从野生音乐视频中挖掘的O(100K)音乐音频片段配对的视频帧进行训练的，而没有涉及任何并行符号音乐数据。V2Meow能够仅在先前训练的从任意静态视频提取的视觉特征的条件下合成高保真度的音频波形。",
    "tldr": "V2Meow是一种新方法，通过与O(100K)音频片段配对的视频帧进行训练，生成与各种类型的视频输入的视觉语义相匹配的高质量音频，无需符号音乐数据。",
    "en_tdlr": "V2Meow is a novel approach that generates high-quality audio aligned with the visual semantics of a diverse range of video inputs, by training with O(100K) music audio clips paired with video frames without requiring symbolic music data."
}