{
    "title": "Human-Centered Metrics for Dialog System Evaluation. (arXiv:2305.14757v1 [cs.CL])",
    "abstract": "We present metrics for evaluating dialog systems through a psychologically-grounded \"human\" lens: conversational agents express a diversity of both states (short-term factors like emotions) and traits (longer-term factors like personality) just as people do. These interpretable metrics consist of five measures from established psychology constructs that can be applied both across dialogs and on turns within dialogs: emotional entropy, linguistic style and emotion matching, as well as agreeableness and empathy. We compare these human metrics against 6 state-of-the-art automatic metrics (e.g. BARTScore and BLEURT) on 7 standard dialog system data sets. We also introduce a novel data set, the Three Bot Dialog Evaluation Corpus, which consists of annotated conversations from ChatGPT, GPT-3, and BlenderBot. We demonstrate the proposed human metrics offer novel information, are uncorrelated with automatic metrics, and lead to increased accuracy beyond existing automatic metrics for predictin",
    "link": "http://arxiv.org/abs/2305.14757",
    "context": "Title: Human-Centered Metrics for Dialog System Evaluation. (arXiv:2305.14757v1 [cs.CL])\nAbstract: We present metrics for evaluating dialog systems through a psychologically-grounded \"human\" lens: conversational agents express a diversity of both states (short-term factors like emotions) and traits (longer-term factors like personality) just as people do. These interpretable metrics consist of five measures from established psychology constructs that can be applied both across dialogs and on turns within dialogs: emotional entropy, linguistic style and emotion matching, as well as agreeableness and empathy. We compare these human metrics against 6 state-of-the-art automatic metrics (e.g. BARTScore and BLEURT) on 7 standard dialog system data sets. We also introduce a novel data set, the Three Bot Dialog Evaluation Corpus, which consists of annotated conversations from ChatGPT, GPT-3, and BlenderBot. We demonstrate the proposed human metrics offer novel information, are uncorrelated with automatic metrics, and lead to increased accuracy beyond existing automatic metrics for predictin",
    "path": "papers/23/05/2305.14757.json",
    "total_tokens": 933,
    "translated_title": "面向人类中心的度量评估对话系统",
    "translated_abstract": "我们提出了一种通过心理学角度来评估对话系统的度量方法：对话代理人像人类一样表达了多种状态（短期因素，如情绪）和特质（更长期因素，如个性）。这些可解释的度量标准由来自已建立的心理学构造的五种度量组成，可以应用于对话和对话中的每个回合：情绪熵，语言风格和情感匹配，以及宜人性和共情。我们将这些人类度量标准与6种最先进的自动度量标准（例如BARTScore和BLEURT）在7个标准对话系统数据集上进行了比较。我们还介绍了一个新颖的数据集，即Three Bot Dialog Evaluation Corpus，其中包含来自ChatGPT、GPT-3和BlenderBot的已注释对话。我们证明了所提出的人类度量标准提供了新颖的信息，与自动度量标准不相关，并可在预测对话系统质量时超越现有的自动度量标准。",
    "tldr": "本文提出了从心理学构造中提取的可解释度量标准，用于评估对话系统，通过情绪熵、语言风格和情感匹配、宜人性和共情等五个度量，这些人类度量标准与现有的自动度量标准不相关且具有更高的预测准确度。",
    "en_tdlr": "The paper proposes interpretable metrics extracted from established psychology constructs for evaluating dialog systems, including emotional entropy, linguistic style and emotion matching, as well as agreeableness and empathy. These human-centered metrics are uncorrelated with existing automatic metrics and offer novel information, leading to increased predictability in dialog system quality."
}