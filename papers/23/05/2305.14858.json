{
    "title": "Pre-RMSNorm and Pre-CRMSNorm Transformers: Equivalent and Efficient Pre-LN Transformers. (arXiv:2305.14858v2 [cs.LG] UPDATED)",
    "abstract": "Transformers have achieved great success in machine learning applications. Normalization techniques, such as Layer Normalization (LayerNorm, LN) and Root Mean Square Normalization (RMSNorm), play a critical role in accelerating and stabilizing the training of Transformers. While LayerNorm recenters and rescales input vectors, RMSNorm only rescales the vectors by their RMS value. Despite being more computationally efficient, RMSNorm may compromise the representation ability of Transformers. There is currently no consensus regarding the preferred normalization technique, as some models employ LayerNorm while others utilize RMSNorm, especially in recent large language models. It is challenging to convert Transformers with one normalization to the other type. While there is an ongoing disagreement between the two normalization types, we propose a solution to unify two mainstream Transformer architectures, Pre-LN and Pre-RMSNorm Transformers. By removing the inherent redundant mean informat",
    "link": "http://arxiv.org/abs/2305.14858",
    "context": "Title: Pre-RMSNorm and Pre-CRMSNorm Transformers: Equivalent and Efficient Pre-LN Transformers. (arXiv:2305.14858v2 [cs.LG] UPDATED)\nAbstract: Transformers have achieved great success in machine learning applications. Normalization techniques, such as Layer Normalization (LayerNorm, LN) and Root Mean Square Normalization (RMSNorm), play a critical role in accelerating and stabilizing the training of Transformers. While LayerNorm recenters and rescales input vectors, RMSNorm only rescales the vectors by their RMS value. Despite being more computationally efficient, RMSNorm may compromise the representation ability of Transformers. There is currently no consensus regarding the preferred normalization technique, as some models employ LayerNorm while others utilize RMSNorm, especially in recent large language models. It is challenging to convert Transformers with one normalization to the other type. While there is an ongoing disagreement between the two normalization types, we propose a solution to unify two mainstream Transformer architectures, Pre-LN and Pre-RMSNorm Transformers. By removing the inherent redundant mean informat",
    "path": "papers/23/05/2305.14858.json",
    "total_tokens": 879,
    "translated_title": "Pre-RMSNorm和Pre-CRMSNorm Transformers: 等效和高效的Pre-LN Transformers",
    "translated_abstract": "Transformer模型在机器学习应用中取得了巨大成功。归一化技术，如Layer Normalization（LayerNorm，LN）和Root Mean Square Normalization（RMSNorm），在加速和稳定Transformer的训练中起着关键作用。虽然LayerNorm对输入向量进行重新中心化和重新缩放，而RMSNorm仅按其RMS值重新缩放向量。尽管RMSNorm在计算上更高效，但可能会损害Transformer的表示能力。目前关于首选归一化技术尚无共识，因为一些模型使用LayerNorm，而其他模型则使用RMSNorm，尤其是最近的大语言模型。将具有一种归一化的Transformer转换为另一种类型是一项具有挑战性的任务。虽然目前两种归一化类型之间存在争议，但我们提出了一种解决方案，即统一两种主流Transformer架构，Pre-LN和Pre-RMSNorm Transformers。通过去除固有的冗余均值信息来实现等效性和高效性。",
    "tldr": "Pre-RMSNorm和Pre-CRMSNorm Transformers是等效且高效的Pre-LN Transformers架构，可以统一使用两种主流归一化技术，LayerNorm和RMSNorm，从而加速和稳定Transformer模型的训练。"
}