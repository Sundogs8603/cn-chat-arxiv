{
    "title": "Adaptive Sparsity Level during Training for Efficient Time Series Forecasting with Transformers. (arXiv:2305.18382v1 [cs.LG])",
    "abstract": "Efficient time series forecasting has become critical for real-world applications, particularly with deep neural networks (DNNs). Efficiency in DNNs can be achieved through sparse connectivity and reducing the model size. However, finding the sparsity level automatically during training remains a challenging task due to the heterogeneity in the loss-sparsity tradeoffs across the datasets. In this paper, we propose \\enquote{\\textbf{P}runing with \\textbf{A}daptive \\textbf{S}parsity \\textbf{L}evel} (\\textbf{PALS}), to automatically seek an optimal balance between loss and sparsity, all without the need for a predefined sparsity level. PALS draws inspiration from both sparse training and during-training methods. It introduces the novel \"expand\" mechanism in training sparse neural networks, allowing the model to dynamically shrink, expand, or remain stable to find a proper sparsity level. In this paper, we focus on achieving efficiency in transformers known for their excellent time series f",
    "link": "http://arxiv.org/abs/2305.18382",
    "context": "Title: Adaptive Sparsity Level during Training for Efficient Time Series Forecasting with Transformers. (arXiv:2305.18382v1 [cs.LG])\nAbstract: Efficient time series forecasting has become critical for real-world applications, particularly with deep neural networks (DNNs). Efficiency in DNNs can be achieved through sparse connectivity and reducing the model size. However, finding the sparsity level automatically during training remains a challenging task due to the heterogeneity in the loss-sparsity tradeoffs across the datasets. In this paper, we propose \\enquote{\\textbf{P}runing with \\textbf{A}daptive \\textbf{S}parsity \\textbf{L}evel} (\\textbf{PALS}), to automatically seek an optimal balance between loss and sparsity, all without the need for a predefined sparsity level. PALS draws inspiration from both sparse training and during-training methods. It introduces the novel \"expand\" mechanism in training sparse neural networks, allowing the model to dynamically shrink, expand, or remain stable to find a proper sparsity level. In this paper, we focus on achieving efficiency in transformers known for their excellent time series f",
    "path": "papers/23/05/2305.18382.json",
    "total_tokens": 957,
    "translated_title": "适应性稀疏度训练过程中的变化，用于利用Transformer进行高效时间序列预测",
    "translated_abstract": "高效的时间序列预测对于深度神经网络应用变得至关重要。通过稀疏连接和减小模型尺寸，可以实现DNN的高效性。然而，在训练过程中自动确定稀疏度仍然是一个具有挑战性的任务，因为不同数据集中的损失稀疏度权衡是异构的。本文提出了“具有自适应稀疏度级别的修剪”(PALS)，来自动寻求损失和稀疏性之间的最佳平衡，无需预定义稀疏水平。PALS从稀疏训练和训练期间方法中吸取灵感。它在训练稀疏神经网络中引入了新颖的“扩张”机制，允许模型动态收缩、扩张或保持稳定，以找到适当的稀疏度。本文专注于在以Transformer著称的模型中实现效率, 该模型以其出色的时间序列预测表现而闻名。",
    "tldr": "本文提出了“具有自适应稀疏度级别的修剪”(PALS), 通过稀疏训练和训练期间方法中的“扩张”机制，在Transformer模型中实现高效的时间序列预测。"
}