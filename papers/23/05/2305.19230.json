{
    "title": "Controlled Text Generation with Hidden Representation Transformations. (arXiv:2305.19230v2 [cs.CL] UPDATED)",
    "abstract": "We propose CHRT (Control Hidden Representation Transformation) - a controlled language generation framework that steers large language models to generate text pertaining to certain attributes (such as toxicity). CHRT gains attribute control by modifying the hidden representation of the base model through learned transformations. We employ a contrastive-learning framework to learn these transformations that can be combined to gain multi-attribute control. The effectiveness of CHRT is experimentally shown by comparing it with seven baselines over three attributes. CHRT outperforms all the baselines in the task of detoxification, positive sentiment steering, and text simplification while minimizing the loss in linguistic qualities. Further, our approach has the lowest inference latency of only 0.01 seconds more than the base model, making it the most suitable for high-performance production environments. We open-source our code and release two novel datasets to further propel controlled l",
    "link": "http://arxiv.org/abs/2305.19230",
    "context": "Title: Controlled Text Generation with Hidden Representation Transformations. (arXiv:2305.19230v2 [cs.CL] UPDATED)\nAbstract: We propose CHRT (Control Hidden Representation Transformation) - a controlled language generation framework that steers large language models to generate text pertaining to certain attributes (such as toxicity). CHRT gains attribute control by modifying the hidden representation of the base model through learned transformations. We employ a contrastive-learning framework to learn these transformations that can be combined to gain multi-attribute control. The effectiveness of CHRT is experimentally shown by comparing it with seven baselines over three attributes. CHRT outperforms all the baselines in the task of detoxification, positive sentiment steering, and text simplification while minimizing the loss in linguistic qualities. Further, our approach has the lowest inference latency of only 0.01 seconds more than the base model, making it the most suitable for high-performance production environments. We open-source our code and release two novel datasets to further propel controlled l",
    "path": "papers/23/05/2305.19230.json",
    "total_tokens": 865,
    "translated_title": "通过隐藏表示转换实现的可控文本生成",
    "translated_abstract": "我们提出了CHRT(Control Hidden Representation Transformation)，它是一种可控语言生成框架，可以引导大型语言模型生成特定属性的文本(如有毒性文本)。CHRT通过学习表示转换从而修改基础模型的隐藏表示来获得属性控制。我们采用对比学习框架来学习这些表示转换，可以结合使用以获得多属性控制。通过在三个属性上与七个基线模型进行比较，实验证明了CHRT的有效性。CHRT在解毒、正面情感引导和文本简化任务中表现均优于所有基线模型，同时最小化了在语言质量上的损失。此外，我们的方法推断延迟仅比基础模型多0.01秒，是最适合高性能生产环境的方法。我们开放源代码并发布了两个新的数据集，以进一步推动可控文本生成的发展。",
    "tldr": "我们提出了CHRT，它是一种可控语言生成框架，通过学习表示转换来修改基础模型的隐藏表示从而获得属性控制。实验证明，CHRT在三个属性上表现均优于所有基线模型，同时最小化了在语言质量上的损失。",
    "en_tdlr": "We propose CHRT, a controlled language generation framework that modifies the hidden representation of base models through learned transformations to generate text with certain attributes. Our experiment shows CHRT outperforms all baselines in three attributes, while minimizing the loss in linguistic qualities."
}