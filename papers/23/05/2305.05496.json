{
    "title": "Exploiting Pseudo Image Captions for Multimodal Summarization. (arXiv:2305.05496v1 [cs.CL])",
    "abstract": "Cross-modal contrastive learning in vision language pretraining (VLP) faces the challenge of (partial) false negatives. In this paper, we study this problem from the perspective of Mutual Information (MI) optimization. It is common sense that InfoNCE loss used in contrastive learning will maximize the lower bound of MI between anchors and their positives, while we theoretically prove that MI involving negatives also matters when noises commonly exist. Guided by a more general lower bound form for optimization, we propose a contrastive learning strategy regulated by progressively refined cross-modal similarity, to more accurately optimize MI between an image/text anchor and its negative texts/images instead of improperly minimizing it. Our method performs competitively on four downstream cross-modal tasks and systematically balances the beneficial and harmful effects of (partial) false negative samples under theoretical guidance.",
    "link": "http://arxiv.org/abs/2305.05496",
    "context": "Title: Exploiting Pseudo Image Captions for Multimodal Summarization. (arXiv:2305.05496v1 [cs.CL])\nAbstract: Cross-modal contrastive learning in vision language pretraining (VLP) faces the challenge of (partial) false negatives. In this paper, we study this problem from the perspective of Mutual Information (MI) optimization. It is common sense that InfoNCE loss used in contrastive learning will maximize the lower bound of MI between anchors and their positives, while we theoretically prove that MI involving negatives also matters when noises commonly exist. Guided by a more general lower bound form for optimization, we propose a contrastive learning strategy regulated by progressively refined cross-modal similarity, to more accurately optimize MI between an image/text anchor and its negative texts/images instead of improperly minimizing it. Our method performs competitively on four downstream cross-modal tasks and systematically balances the beneficial and harmful effects of (partial) false negative samples under theoretical guidance.",
    "path": "papers/23/05/2305.05496.json",
    "total_tokens": 855,
    "translated_title": "利用伪图像说明进行多模态摘要",
    "translated_abstract": "视觉语言预训练中的跨模态对比学习面临（部分）误负样本的挑战。本文从互信息（MI）优化的角度研究了该问题。我们理论上证明了当存在噪声时，包括负样本的MI也很重要。在更一般的优化下界形式的指导下，我们提出了一种由逐步细化的跨模态相似度调节的对比学习策略，以更精确地优化图像/文本锚定点和其负面文本/图像之间的MI，而不是错误地将其最小化。在四个下游跨模态任务上，我们的方法表现竞争力，并在理论指导下系统地平衡了（部分）误负样本的有利和有害效果。",
    "tldr": "本文研究了跨模态对比学习中（部分）误负样本的挑战，并提出了一种从更一般下界形式的指导下调节跨模态相似度的对比学习策略，以更精确地优化图像/文本锚定点和其负面文本/图像之间的互信息。",
    "en_tdlr": "This paper studies the challenge of (partial) false negative samples in cross-modal contrastive learning and proposes a contrastive learning strategy regulated by progressively refined cross-modal similarity to more accurately optimize mutual information between an image/text anchor and its negative texts/images, under the guidance of a more general lower bound form for optimization."
}