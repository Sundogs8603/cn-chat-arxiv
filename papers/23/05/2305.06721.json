{
    "title": "Advancing Neural Encoding of Portuguese with Transformer Albertina PT-*. (arXiv:2305.06721v1 [cs.CL])",
    "abstract": "To advance the neural encoding of Portuguese (PT), and a fortiori the technological preparation of this language for the digital age, we developed a Transformer-based foundation model that sets a new state of the art in this respect for two of its variants, namely European Portuguese from Portugal (PT-PT) and American Portuguese from Brazil (PT-BR).  To develop this encoder, which we named Albertina PT-*, a strong model was used as a starting point, DeBERTa, and its pre-training was done over data sets of Portuguese, namely over a data set we gathered for PT-PT and over the brWaC corpus for PT-BR. The performance of Albertina and competing models was assessed by evaluating them on prominent downstream language processing tasks adapted for Portuguese.  Both Albertina PT-PT and PT-BR versions are distributed free of charge and under the most permissive license possible and can be run on consumer-grade hardware, thus seeking to contribute to the advancement of research and innovation in l",
    "link": "http://arxiv.org/abs/2305.06721",
    "context": "Title: Advancing Neural Encoding of Portuguese with Transformer Albertina PT-*. (arXiv:2305.06721v1 [cs.CL])\nAbstract: To advance the neural encoding of Portuguese (PT), and a fortiori the technological preparation of this language for the digital age, we developed a Transformer-based foundation model that sets a new state of the art in this respect for two of its variants, namely European Portuguese from Portugal (PT-PT) and American Portuguese from Brazil (PT-BR).  To develop this encoder, which we named Albertina PT-*, a strong model was used as a starting point, DeBERTa, and its pre-training was done over data sets of Portuguese, namely over a data set we gathered for PT-PT and over the brWaC corpus for PT-BR. The performance of Albertina and competing models was assessed by evaluating them on prominent downstream language processing tasks adapted for Portuguese.  Both Albertina PT-PT and PT-BR versions are distributed free of charge and under the most permissive license possible and can be run on consumer-grade hardware, thus seeking to contribute to the advancement of research and innovation in l",
    "path": "papers/23/05/2305.06721.json",
    "total_tokens": 909,
    "translated_title": "基于 Transformer Albertina PT-* 提升葡萄牙语的神经编码",
    "translated_abstract": "本研究旨在推进葡萄牙语（PT）的神经编码，为该语言在数字时代的技术准备打下基础。我们开发了基于 Transformer 的 Albertina PT-* 基础模型，为其两个变种（葡萄牙的欧洲葡萄牙语（PT-PT）和巴西的美洲葡萄牙语（PT-BR））的神经编码创下了新的技术水平。我们使用一种强大的模型作为起点，即DeBERTa，并使用我们收集的PT-PT数据集和brWaC语料库对其进行预训练。我们通过对适用于葡萄牙语的著名下游语言处理任务进行评估，来评估Albertina和竞争模型的性能。 Albertina PT-PT和PT-BR版本均可免费分发，并在最宽松的许可以下运行于消费级硬件。",
    "tldr": "本研究使用基于 Transformer 的 Albertina PT-* 模型进行了葡萄牙语的神经编码，创新性地提升了该语言在数字时代的技术准备水平，尤其是欧洲葡萄牙语和巴西的美洲葡萄牙语两个变种。",
    "en_tdlr": "This study developed a Transformer-based Albertina PT-* model to advance the neural encoding of Portuguese, achieving state of the art performance for both European and American variants and contributing to technological preparation for the digital age."
}