{
    "title": "Error Bounds for Learning with Vector-Valued Random Features. (arXiv:2305.17170v1 [stat.ML])",
    "abstract": "This paper provides a comprehensive error analysis of learning with vector-valued random features (RF). The theory is developed for RF ridge regression in a fully general infinite-dimensional input-output setting, but nonetheless applies to and improves existing finite-dimensional analyses. In contrast to comparable work in the literature, the approach proposed here relies on a direct analysis of the underlying risk functional and completely avoids the explicit RF ridge regression solution formula in terms of random matrices. This removes the need for concentration results in random matrix theory or their generalizations to random operators. The main results established in this paper include strong consistency of vector-valued RF estimators under model misspecification and minimax optimal convergence rates in the well-specified setting. The parameter complexity (number of random features) and sample complexity (number of labeled data) required to achieve such rates are comparable with ",
    "link": "http://arxiv.org/abs/2305.17170",
    "context": "Title: Error Bounds for Learning with Vector-Valued Random Features. (arXiv:2305.17170v1 [stat.ML])\nAbstract: This paper provides a comprehensive error analysis of learning with vector-valued random features (RF). The theory is developed for RF ridge regression in a fully general infinite-dimensional input-output setting, but nonetheless applies to and improves existing finite-dimensional analyses. In contrast to comparable work in the literature, the approach proposed here relies on a direct analysis of the underlying risk functional and completely avoids the explicit RF ridge regression solution formula in terms of random matrices. This removes the need for concentration results in random matrix theory or their generalizations to random operators. The main results established in this paper include strong consistency of vector-valued RF estimators under model misspecification and minimax optimal convergence rates in the well-specified setting. The parameter complexity (number of random features) and sample complexity (number of labeled data) required to achieve such rates are comparable with ",
    "path": "papers/23/05/2305.17170.json",
    "total_tokens": 877,
    "translated_title": "向量值随机特征学习的误差界分析",
    "translated_abstract": "本文提供了对于向量值随机特征学习的完整误差分析。该理论是针对完全通用的无限维度输入-输出设定中的RF Ridge回归而开发的，但仍适用于并改进了现有的有限维度分析。与文献中其他类似的工作相比，本文提出的方法依赖于底层风险函数的直接分析，完全避免了基于随机矩阵的显式RF Ridge回归解决方案公式的使用。这消除了随机矩阵理论中的浓度结果或其对随机算子的推广的需求。本文建立的主要结果包括在模型错误说明下向量值RF估计器的强一致性和在良好规定的情况下极小化最优收敛速率。实现这些收敛速率所需的参数复杂度(随机特征数量)和样本复杂度(标记数据数量)与",
    "tldr": "本文提供了对向量值随机特征学习的完整误差分析，包括在模型错误说明下向量值RF估计器的强一致性和在良好规定的情况下极小化最优收敛速率。",
    "en_tdlr": "This paper provides a comprehensive error analysis of learning with vector-valued random features and establishes the strong consistency of vector-valued RF estimators under model misspecification and minimax optimal convergence rates in the well-specified setting."
}