{
    "title": "Reinforcement Learning finetuned Vision-Code Transformer for UI-to-Code Generation. (arXiv:2305.14637v1 [cs.CV])",
    "abstract": "Automated HTML/CSS code generation from screenshots is an important yet challenging problem with broad applications in website development and design. In this paper, we present a novel vision-code transformer approach that leverages an Encoder-Decoder architecture as well as explore actor-critic fine-tuning as a method for improving upon the baseline. For this purpose, two image encoders are compared: Vision Transformer (ViT) and Document Image Transformer (DiT).  We propose an end-to-end pipeline that can generate high-quality code snippets directly from screenshots, streamlining the website creation process for developers. To train and evaluate our models, we created a synthetic dataset of 30,000 unique pairs of code and corresponding screenshots.  We evaluate the performance of our approach using a combination of automated metrics such as MSE, BLEU, IoU, and a novel htmlBLEU score, where our models demonstrated strong performance. We establish a strong baseline with the DiT-GPT2 mod",
    "link": "http://arxiv.org/abs/2305.14637",
    "context": "Title: Reinforcement Learning finetuned Vision-Code Transformer for UI-to-Code Generation. (arXiv:2305.14637v1 [cs.CV])\nAbstract: Automated HTML/CSS code generation from screenshots is an important yet challenging problem with broad applications in website development and design. In this paper, we present a novel vision-code transformer approach that leverages an Encoder-Decoder architecture as well as explore actor-critic fine-tuning as a method for improving upon the baseline. For this purpose, two image encoders are compared: Vision Transformer (ViT) and Document Image Transformer (DiT).  We propose an end-to-end pipeline that can generate high-quality code snippets directly from screenshots, streamlining the website creation process for developers. To train and evaluate our models, we created a synthetic dataset of 30,000 unique pairs of code and corresponding screenshots.  We evaluate the performance of our approach using a combination of automated metrics such as MSE, BLEU, IoU, and a novel htmlBLEU score, where our models demonstrated strong performance. We establish a strong baseline with the DiT-GPT2 mod",
    "path": "papers/23/05/2305.14637.json",
    "total_tokens": 978,
    "translated_title": "强化学习微调的视觉-代码Transformer用于UI-to-Code生成",
    "translated_abstract": "从屏幕截图自动生成HTML/CSS代码是一个重要且具有广泛应用的挑战性问题。本文提出了一种新颖的视觉-代码Transformer方法，利用编码器-解码器结构，同时探索actor-critic微调作为改进基线的方法。为此，比较了两个图像编码器：Vision Transformer (ViT) 和 Document Image Transformer (DiT)。我们提出了一种端到端的流水线，可以直接从屏幕截图生成高质量的代码片段，简化了开发人员的网站创建过程。为训练和评估模型，我们创建了一个包含30,000个独特的代码和对应截图的合成数据集。我们使用MSE、BLEU、IoU和一种新颖的htmlBLEU得分等自动化指标评估我们方法的性能，我们的模型表现出了强大的性能。我们用DiT-GPT2模型建立了一个强大的基准。",
    "tldr": "本文提出了一种新型的视觉-代码Transformer方法，通过actor-critic微调来改善基线，比较了Vision Transformer和Document Image Transformer这两种图像编码器，提出了一种端到端的流水线，可以直接从屏幕截图生成高质量的代码片段，创建了30,000个独特的代码和对应截图的合成数据集，并使用多种自动化指标来评估这种方法的性能，建立了一个强大的基准模型。"
}