{
    "title": "A Hierarchical Approach to Population Training for Human-AI Collaboration. (arXiv:2305.16708v1 [cs.AI])",
    "abstract": "A major challenge for deep reinforcement learning (DRL) agents is to collaborate with novel partners that were not encountered by them during the training phase. This is specifically worsened by an increased variance in action responses when the DRL agents collaborate with human partners due to the lack of consistency in human behaviors. Recent work have shown that training a single agent as the best response to a diverse population of training partners significantly increases an agent's robustness to novel partners. We further enhance the population-based training approach by introducing a Hierarchical Reinforcement Learning (HRL) based method for Human-AI Collaboration. Our agent is able to learn multiple best-response policies as its low-level policy while at the same time, it learns a high-level policy that acts as a manager which allows the agent to dynamically switch between the low-level best-response policies based on its current partner. We demonstrate that our method is able ",
    "link": "http://arxiv.org/abs/2305.16708",
    "context": "Title: A Hierarchical Approach to Population Training for Human-AI Collaboration. (arXiv:2305.16708v1 [cs.AI])\nAbstract: A major challenge for deep reinforcement learning (DRL) agents is to collaborate with novel partners that were not encountered by them during the training phase. This is specifically worsened by an increased variance in action responses when the DRL agents collaborate with human partners due to the lack of consistency in human behaviors. Recent work have shown that training a single agent as the best response to a diverse population of training partners significantly increases an agent's robustness to novel partners. We further enhance the population-based training approach by introducing a Hierarchical Reinforcement Learning (HRL) based method for Human-AI Collaboration. Our agent is able to learn multiple best-response policies as its low-level policy while at the same time, it learns a high-level policy that acts as a manager which allows the agent to dynamically switch between the low-level best-response policies based on its current partner. We demonstrate that our method is able ",
    "path": "papers/23/05/2305.16708.json",
    "total_tokens": 946,
    "translated_title": "一种基于层次思维的人工智能协同训练方法",
    "translated_abstract": "深度强化学习（DRL）代理在与未经过训练的合作伙伴协同时存在困难，特别是当代理与人类合作伙伴合作时，因人类行为的不一致性而出现行动反应的方差增加，而这加剧了这一问题。最近的研究表明，将单个代理训练为对多样化的训练伙伴做出最佳响应，可以显著提高代理与新合作伙伴的适应性。我们进一步增强了基于人口的训练方法，引入了一个基于层次强化学习（HRL）的方法来实现人工智能协同训练。我们的代理能够学习多个最佳响应策略作为其低层策略，同时学习一个作为管理者的高层策略，使代理能够根据其当前的合作伙伴动态地在低层最佳响应策略之间进行切换。我们证明了我们的方法能够",
    "tldr": "本论文提出了一种基于层次思维的人工智能协同训练方法，通过引入层次强化学习方法，代理能够根据当前合作伙伴自动切换最佳响应策略，从而显著提高了代理与新合作伙伴的适应性。",
    "en_tdlr": "This paper proposes a hierarchical approach to population training for Human-AI collaboration, which enhances the agent's robustness to novel partners by introducing a hierarchical reinforcement learning method that allows the agent to dynamically switch between multiple best-response policies based on the current partner."
}