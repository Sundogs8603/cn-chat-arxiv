{
    "title": "Evaluate What You Can't Evaluate: Unassessable Generated Responses Quality. (arXiv:2305.14658v1 [cs.CL])",
    "abstract": "LLMs (large language models) such as ChatGPT have shown remarkable language understanding and generation capabilities. Although reference-free evaluators based on LLMs show better human alignment than traditional reference-based evaluators, there are many challenges in using reference-free evaluators based on LLMs. Reference-free evaluators are more suitable for open-ended examples with different semantics responses. But not all examples are open-ended. For closed-ended examples with unique correct semantic response, reference-free evaluators will still consider it high quality when giving a response that is inconsistent with the facts and the semantic of reference. In order to comprehensively evaluate the reliability of evaluators based on LLMs, we construct two adversarial meta-evaluation dialogue generation datasets KdConv-ADV and DSTC7-ADV based on KdConv and DSTC7-AVSD, respectively. Compared to previous meta-evaluation benchmarks, KdConv-ADV and DSTC7-ADV are much more challengin",
    "link": "http://arxiv.org/abs/2305.14658",
    "context": "Title: Evaluate What You Can't Evaluate: Unassessable Generated Responses Quality. (arXiv:2305.14658v1 [cs.CL])\nAbstract: LLMs (large language models) such as ChatGPT have shown remarkable language understanding and generation capabilities. Although reference-free evaluators based on LLMs show better human alignment than traditional reference-based evaluators, there are many challenges in using reference-free evaluators based on LLMs. Reference-free evaluators are more suitable for open-ended examples with different semantics responses. But not all examples are open-ended. For closed-ended examples with unique correct semantic response, reference-free evaluators will still consider it high quality when giving a response that is inconsistent with the facts and the semantic of reference. In order to comprehensively evaluate the reliability of evaluators based on LLMs, we construct two adversarial meta-evaluation dialogue generation datasets KdConv-ADV and DSTC7-ADV based on KdConv and DSTC7-AVSD, respectively. Compared to previous meta-evaluation benchmarks, KdConv-ADV and DSTC7-ADV are much more challengin",
    "path": "papers/23/05/2305.14658.json",
    "total_tokens": 925,
    "translated_title": "无法评估的生成响应质量的评估: Evaluate What You Can't Evaluate",
    "translated_abstract": "大型语言模型（LLMs）如ChatGPT已经展现出惊人的语言理解和生成能力。虽然以LLMs为基础的无参考评估器比传统基于参考文献的评估器显示出更好的人类语义对齐度，但是在使用以LLMs为基础的无参考评估器时仍然存在很多挑战。无参考评估器更适用于具有不同语义响应的开放式例子。但并不是所有的例子都是开放式的，对于具有唯一正确语义响应的闭合式例子，如果给出与事实和参考的语义不一致的响应，无参考评估器仍然会认为其具有高质量。为了全面评估以LLMs为基础的评估器的可靠性，我们构建了两个对抗元评估对话生成数据集KdConv-ADV和DSTC7-ADV基于KdConv和DSTC7-AVSD。与以前的元评估基准相比，KdConv-ADV和DSTC7-ADV更具挑战性。",
    "tldr": "本文重点研究使用大型语言模型（LLMs）为基础的无参考评估器的局限性，并构建了两个对抗元评估对话生成数据集，以全面评估这些评估器的可靠性。",
    "en_tdlr": "This paper focuses on the limitations of reference-free evaluators based on large language models (LLMs) and constructs two adversarial meta-evaluation dialogue generation datasets to comprehensively evaluate the reliability of these evaluators."
}