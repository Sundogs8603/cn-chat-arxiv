{
    "title": "Revisiting pre-trained remote sensing model benchmarks: resizing and normalization matters. (arXiv:2305.13456v1 [cs.CV])",
    "abstract": "Research in self-supervised learning (SSL) with natural images has progressed rapidly in recent years and is now increasingly being applied to and benchmarked with datasets containing remotely sensed imagery. A common benchmark case is to evaluate SSL pre-trained model embeddings on datasets of remotely sensed imagery with small patch sizes, e.g., 32x32 pixels, whereas standard SSL pre-training takes place with larger patch sizes, e.g., 224x224. Furthermore, pre-training methods tend to use different image normalization preprocessing steps depending on the dataset. In this paper, we show, across seven satellite and aerial imagery datasets of varying resolution, that by simply following the preprocessing steps used in pre-training (precisely, image sizing and normalization methods), one can achieve significant performance improvements when evaluating the extracted features on downstream tasks -- an important detail overlooked in previous work in this space. We show that by following the",
    "link": "http://arxiv.org/abs/2305.13456",
    "context": "Title: Revisiting pre-trained remote sensing model benchmarks: resizing and normalization matters. (arXiv:2305.13456v1 [cs.CV])\nAbstract: Research in self-supervised learning (SSL) with natural images has progressed rapidly in recent years and is now increasingly being applied to and benchmarked with datasets containing remotely sensed imagery. A common benchmark case is to evaluate SSL pre-trained model embeddings on datasets of remotely sensed imagery with small patch sizes, e.g., 32x32 pixels, whereas standard SSL pre-training takes place with larger patch sizes, e.g., 224x224. Furthermore, pre-training methods tend to use different image normalization preprocessing steps depending on the dataset. In this paper, we show, across seven satellite and aerial imagery datasets of varying resolution, that by simply following the preprocessing steps used in pre-training (precisely, image sizing and normalization methods), one can achieve significant performance improvements when evaluating the extracted features on downstream tasks -- an important detail overlooked in previous work in this space. We show that by following the",
    "path": "papers/23/05/2305.13456.json",
    "total_tokens": 1048,
    "tldr": "本文研究了在遥感图像数据集上评估自监督学习预训练模型时的重要细节：通过遵循预训练中使用的精确调整大小和归一化方法，可以在评估提取的特征在下游任务上的表现时实现显著的性能改进。"
}