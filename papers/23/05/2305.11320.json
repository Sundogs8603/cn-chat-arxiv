{
    "title": "Parameter-Efficient Learning for Text-to-Speech Accent Adaptation. (arXiv:2305.11320v1 [cs.SD])",
    "abstract": "This paper presents a parameter-efficient learning (PEL) to develop a low-resource accent adaptation for text-to-speech (TTS). A resource-efficient adaptation from a frozen pre-trained TTS model is developed by using only 1.2\\% to 0.8\\% of original trainable parameters to achieve competitive performance in voice synthesis. Motivated by a theoretical foundation of optimal transport (OT), this study carries out PEL for TTS where an auxiliary unsupervised loss based on OT is introduced to maximize a difference between the pre-trained source domain and the (unseen) target domain, in addition to its supervised training loss. Further, we leverage upon this unsupervised loss refinement to boost system performance via either sliced Wasserstein distance or maximum mean discrepancy. The merit of this work is demonstrated by fulfilling PEL solutions based on residual adapter learning, and model reprogramming when evaluating the Mandarin accent adaptation. Experiment results show that the proposed",
    "link": "http://arxiv.org/abs/2305.11320",
    "context": "Title: Parameter-Efficient Learning for Text-to-Speech Accent Adaptation. (arXiv:2305.11320v1 [cs.SD])\nAbstract: This paper presents a parameter-efficient learning (PEL) to develop a low-resource accent adaptation for text-to-speech (TTS). A resource-efficient adaptation from a frozen pre-trained TTS model is developed by using only 1.2\\% to 0.8\\% of original trainable parameters to achieve competitive performance in voice synthesis. Motivated by a theoretical foundation of optimal transport (OT), this study carries out PEL for TTS where an auxiliary unsupervised loss based on OT is introduced to maximize a difference between the pre-trained source domain and the (unseen) target domain, in addition to its supervised training loss. Further, we leverage upon this unsupervised loss refinement to boost system performance via either sliced Wasserstein distance or maximum mean discrepancy. The merit of this work is demonstrated by fulfilling PEL solutions based on residual adapter learning, and model reprogramming when evaluating the Mandarin accent adaptation. Experiment results show that the proposed",
    "path": "papers/23/05/2305.11320.json",
    "total_tokens": 977,
    "translated_title": "针对语音合成口音适应的参数高效学习方法",
    "translated_abstract": "本文提出了一种参数高效学习方法（PEL）来开发低资源的语音合成口音适应。通过使用原始可训练参数的1.2％至0.8％，从已冻结的预训练TTS模型中开发出资源高效的适应模型，以实现有竞争力的语音合成性能。本研究基于最优传输（OT）的理论基础，进行TTS的PEL，引入了一个基于OT的辅助无监督损失，以最大化预训练源域和（未见过的）目标域之间的差异，除了有监督的训练损失。此外，我们利用这个无监督的损失细化来通过切片瓦瑟斯坦距离或最大均值差异来提高系统性能。本研究的优点是在评估普通话口音适应时，通过使用残差适配器学习和模型重编程来实现PEL解决方案。",
    "tldr": "本文提出了一种参数高效学习方法（PEL），利用理论基础的最优传输（OT）来实现低资源语音合成口音适应，通过引入基于OT的辅助无监督损失来最大化源域和目标域之间的差异，从而提高系统性能。"
}