{
    "title": "MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers. (arXiv:2305.07185v1 [cs.LG])",
    "abstract": "Autoregressive transformers are spectacular models for short sequences but scale poorly to long sequences such as high-resolution images, podcasts, code, or books. We proposed Megabyte, a multi-scale decoder architecture that enables end-to-end differentiable modeling of sequences of over one million bytes. Megabyte segments sequences into patches and uses a local submodel within patches and a global model between patches. This enables sub-quadratic self-attention, much larger feedforward layers for the same compute, and improved parallelism during decoding -- unlocking better performance at reduced cost for both training and generation. Extensive experiments show that Megabyte allows byte-level models to perform competitively with subword models on long context language modeling, achieve state-of-the-art density estimation on ImageNet, and model audio from raw files. Together, these results establish the viability of tokenization-free autoregressive sequence modeling at scale.",
    "link": "http://arxiv.org/abs/2305.07185",
    "context": "Title: MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers. (arXiv:2305.07185v1 [cs.LG])\nAbstract: Autoregressive transformers are spectacular models for short sequences but scale poorly to long sequences such as high-resolution images, podcasts, code, or books. We proposed Megabyte, a multi-scale decoder architecture that enables end-to-end differentiable modeling of sequences of over one million bytes. Megabyte segments sequences into patches and uses a local submodel within patches and a global model between patches. This enables sub-quadratic self-attention, much larger feedforward layers for the same compute, and improved parallelism during decoding -- unlocking better performance at reduced cost for both training and generation. Extensive experiments show that Megabyte allows byte-level models to perform competitively with subword models on long context language modeling, achieve state-of-the-art density estimation on ImageNet, and model audio from raw files. Together, these results establish the viability of tokenization-free autoregressive sequence modeling at scale.",
    "path": "papers/23/05/2305.07185.json",
    "total_tokens": 918,
    "translated_title": "MEGABYTE: 基于多尺度Transformer的百万字节序列预测",
    "translated_abstract": "自回归transformer模型在短序列上表现良好，但对于高分辨率图像、播客、代码或图书等长序列的处理能力较差。我们提出了Megabyte，一种多尺度解码器架构，能够对超过一百万字节的序列进行端到端的可微建模。Megabyte将序列分为图块，并在图块内使用局部子模型，在图块之间使用全局模型。这使得子二次自注意、更大的前馈层和更好的解码并行性得以实现，提高了训练和生成过程的性能，同时降低了成本。广泛的实验表明，Megabyte可以使基于字节的模型在长上下文语言建模方面与基于子词的模型相媲美，在ImageNet上实现了最先进的密度估计，可以模拟来自原始文件的音频。这些结果证明了在大规模上下文无需标记的自回归序列建模的可行性。",
    "tldr": "MEGABYTE是一种基于多尺度Transformer的解码器架构，能够对超过一百万字节的序列进行端到端的可微建模，在训练和生成过程中提高了性能并降低了成本，同时证明了在大规模上下文无需标记的自回归序列建模的可行性。",
    "en_tdlr": "MEGABYTE is a multi-scale decoder architecture based on Transformers, which enables end-to-end differentiable modeling of sequences of over one million bytes, improves performance and reduces cost during both training and generation. It also demonstrates the feasibility of tokenization-free autoregressive sequence modeling at scale."
}