{
    "title": "Offline Experience Replay for Continual Offline Reinforcement Learning. (arXiv:2305.13804v1 [cs.LG])",
    "abstract": "The capability of continuously learning new skills via a sequence of pre-collected offline datasets is desired for an agent. However, consecutively learning a sequence of offline tasks likely leads to the catastrophic forgetting issue under resource-limited scenarios. In this paper, we formulate a new setting, continual offline reinforcement learning (CORL), where an agent learns a sequence of offline reinforcement learning tasks and pursues good performance on all learned tasks with a small replay buffer without exploring any of the environments of all the sequential tasks. For consistently learning on all sequential tasks, an agent requires acquiring new knowledge and meanwhile preserving old knowledge in an offline manner. To this end, we introduced continual learning algorithms and experimentally found experience replay (ER) to be the most suitable algorithm for the CORL problem. However, we observe that introducing ER into CORL encounters a new distribution shift problem: the mism",
    "link": "http://arxiv.org/abs/2305.13804",
    "context": "Title: Offline Experience Replay for Continual Offline Reinforcement Learning. (arXiv:2305.13804v1 [cs.LG])\nAbstract: The capability of continuously learning new skills via a sequence of pre-collected offline datasets is desired for an agent. However, consecutively learning a sequence of offline tasks likely leads to the catastrophic forgetting issue under resource-limited scenarios. In this paper, we formulate a new setting, continual offline reinforcement learning (CORL), where an agent learns a sequence of offline reinforcement learning tasks and pursues good performance on all learned tasks with a small replay buffer without exploring any of the environments of all the sequential tasks. For consistently learning on all sequential tasks, an agent requires acquiring new knowledge and meanwhile preserving old knowledge in an offline manner. To this end, we introduced continual learning algorithms and experimentally found experience replay (ER) to be the most suitable algorithm for the CORL problem. However, we observe that introducing ER into CORL encounters a new distribution shift problem: the mism",
    "path": "papers/23/05/2305.13804.json",
    "total_tokens": 924,
    "translated_title": "离线体验重放用于连续的离线强化学习",
    "translated_abstract": "代理能够通过一系列预先收集的离线数据集不断学习新技能是理想的。然而，在资源有限的情况下，连续学习一系列离线任务很可能导致灾难性的遗忘问题。本文提出了一个新的场景——连续离线强化学习 (CORL)，代理通过一个小的回放缓冲区学习一系列离线强化学习任务，并在所有学习任务中追求良好的性能，而不探索所有顺序任务的任何环境。为了在所有顺序任务上持续学习，代理需要以离线方式获取新知识，同时保持旧知识。为此，我们引入了连续学习算法，并实验发现经验重放 (ER) 是 CORL 问题最适合的算法。然而，我们观察到将 ER 引入 CORL 会遇到新的分布偏移问题：数据集中不同任务之间的状态分布不一致。",
    "tldr": "本论文提出了一个新的场景——连续离线强化学习 (CORL)，解决了代理在离线任务序列学习中可能出现的灾难性遗忘问题。实验结果发现，经验重放 (ER) 是最适合 CORL 问题的算法，但引入 ER 后会遇到新的分布偏移问题。",
    "en_tdlr": "This paper proposes a new setting, continual offline reinforcement learning (CORL), to solve the catastrophic forgetting issue that may arise when an agent learns a sequence of offline tasks. The experiment finds that experience replay (ER) is the most suitable algorithm for the CORL problem, but introducing ER leads to a new distribution shift problem."
}