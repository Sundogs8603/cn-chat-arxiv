{
    "title": "Rethinking Graph Lottery Tickets: Graph Sparsity Matters. (arXiv:2305.02190v1 [cs.LG])",
    "abstract": "Lottery Ticket Hypothesis (LTH) claims the existence of a winning ticket (i.e., a properly pruned sub-network together with original weight initialization) that can achieve competitive performance to the original dense network. A recent work, called UGS, extended LTH to prune graph neural networks (GNNs) for effectively accelerating GNN inference. UGS simultaneously prunes the graph adjacency matrix and the model weights using the same masking mechanism, but since the roles of the graph adjacency matrix and the weight matrices are very different, we find that their sparsifications lead to different performance characteristics. Specifically, we find that the performance of a sparsified GNN degrades significantly when the graph sparsity goes beyond a certain extent. Therefore, we propose two techniques to improve GNN performance when the graph sparsity is high. First, UGS prunes the adjacency matrix using a loss formulation which, however, does not properly involve all elements of the ad",
    "link": "http://arxiv.org/abs/2305.02190",
    "context": "Title: Rethinking Graph Lottery Tickets: Graph Sparsity Matters. (arXiv:2305.02190v1 [cs.LG])\nAbstract: Lottery Ticket Hypothesis (LTH) claims the existence of a winning ticket (i.e., a properly pruned sub-network together with original weight initialization) that can achieve competitive performance to the original dense network. A recent work, called UGS, extended LTH to prune graph neural networks (GNNs) for effectively accelerating GNN inference. UGS simultaneously prunes the graph adjacency matrix and the model weights using the same masking mechanism, but since the roles of the graph adjacency matrix and the weight matrices are very different, we find that their sparsifications lead to different performance characteristics. Specifically, we find that the performance of a sparsified GNN degrades significantly when the graph sparsity goes beyond a certain extent. Therefore, we propose two techniques to improve GNN performance when the graph sparsity is high. First, UGS prunes the adjacency matrix using a loss formulation which, however, does not properly involve all elements of the ad",
    "path": "papers/23/05/2305.02190.json",
    "total_tokens": 1280,
    "translated_title": "重新思考图彩票: 图的稀疏性很重要",
    "translated_abstract": "彩票猜想 (LTH) 声称存在一种获胜的彩票 (即，一个经过适当修剪的子网络以及原始权重初始化)，它可以实现与原始密集网络相当的性能。最近的一项工作称为 UGS，扩展了 LTH 以修剪图神经网络 (GNN)，以有效加速 GNN 推理。UGS 同时使用相同的屏蔽机制修剪图邻接矩阵和模型权重，但由于图邻接矩阵和权重矩阵的角色非常不同，我们发现他们的稀疏化导致不同的性能特征。具体而言，我们发现当图的稀疏程度超过一定程度时，稀疏 GNN 的性能会显着下降。因此，我们提出了两种技术，以改善当图的稀疏程度较高时的 GNN 性能。首先，UGS 使用丢失公式修剪邻接矩阵，然而，该技术并未适当涉及邻接矩阵的所有元素。为了缓解这个问题，我们提出了一个对称的修剪技术，它可以在修剪邻接矩阵的同时保证每个节点的输入和输出特征被保留。其次，我们提出了一种新颖的稀疏化技术，它避免了直接修剪图结构，而是在保留特征图的空间位置的同时修剪了 GNN 层的特征通道，类似于图像卷积网络采用的策略。实验结果验证了我们提出的技术的有效性，在基准数据集上，在准确度和计算效率上超过了最先进的基线，分别达到了 7.0% 和 5.0x。",
    "tldr": "本文探讨了图彩票问题中图的稀疏性问题，提出了保留节点输入输出特征的对称修剪技术和保留特征图空间位置的修剪技术，并在基准数据集上取得了比现有技术更优秀的结果。",
    "en_tdlr": "This paper explores the issue of graph sparsity in the graph lottery ticket problem and proposes a symmetric pruning technique that preserves node input and output features and a pruning technique that preserves feature map spatial locations, which outperform existing techniques on benchmark datasets."
}