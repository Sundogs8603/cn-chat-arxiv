{
    "title": "How do humans perceive adversarial text? A reality check on the validity and naturalness of word-based adversarial attacks. (arXiv:2305.15587v1 [cs.CL])",
    "abstract": "Natural Language Processing (NLP) models based on Machine Learning (ML) are susceptible to adversarial attacks -- malicious algorithms that imperceptibly modify input text to force models into making incorrect predictions. However, evaluations of these attacks ignore the property of imperceptibility or study it under limited settings. This entails that adversarial perturbations would not pass any human quality gate and do not represent real threats to human-checked NLP systems. To bypass this limitation and enable proper assessment (and later, improvement) of NLP model robustness, we have surveyed 378 human participants about the perceptibility of text adversarial examples produced by state-of-the-art methods. Our results underline that existing text attacks are impractical in real-world scenarios where humans are involved. This contrasts with previous smaller-scale human studies, which reported overly optimistic conclusions regarding attack success. Through our work, we hope to positi",
    "link": "http://arxiv.org/abs/2305.15587",
    "context": "Title: How do humans perceive adversarial text? A reality check on the validity and naturalness of word-based adversarial attacks. (arXiv:2305.15587v1 [cs.CL])\nAbstract: Natural Language Processing (NLP) models based on Machine Learning (ML) are susceptible to adversarial attacks -- malicious algorithms that imperceptibly modify input text to force models into making incorrect predictions. However, evaluations of these attacks ignore the property of imperceptibility or study it under limited settings. This entails that adversarial perturbations would not pass any human quality gate and do not represent real threats to human-checked NLP systems. To bypass this limitation and enable proper assessment (and later, improvement) of NLP model robustness, we have surveyed 378 human participants about the perceptibility of text adversarial examples produced by state-of-the-art methods. Our results underline that existing text attacks are impractical in real-world scenarios where humans are involved. This contrasts with previous smaller-scale human studies, which reported overly optimistic conclusions regarding attack success. Through our work, we hope to positi",
    "path": "papers/23/05/2305.15587.json",
    "total_tokens": 1007,
    "translated_title": "人类如何感知对抗文本？对基于词语对抗攻击的有效性和自然性进行现实检验。",
    "translated_abstract": "基于机器学习的自然语言处理(NLP)模型容易受到对抗攻击——恶意算法会微小地修改输入文本，导致模型做出错误预测。然而，这些攻击的评估忽略了不可察觉性质或者在有限的情况下进行研究。这意味着对抗扰动不会通过任何人类质量测试，也不会对通过人工检查的NLP系统构成真正的威胁。为了绕过这个限制并实现NLP模型鲁棒性的适当评估（以及后来的改进），我们对378名人类参与者就当前最先进的攻击方法生产的文本对抗样本的可感知性进行了调查。我们的结果表明，现有的文本攻击在人类参与的现实世界场景中是不切实际的。这与先前规模较小的人类研究相矛盾，后者报道了攻击成功的过于乐观结论。通过我们的工作，我们希望为当前对NLP模型的对抗攻击的研究做出积极贡献，提供对其潜在影响的更现实的看法。",
    "tldr": "本研究通过调查人们对抗性文本样本的可感知性，得出现有文本攻击在人类参与的现实世界场景中是不切实际的，提供了更为现实的对NLP模型鲁棒性的评估。",
    "en_tdlr": "This study provides a more realistic assessment of the effectiveness and practicality of adversarial text attacks by surveying human perceptions of such attacks, revealing that existing attacks are impractical in real-world scenarios where humans are involved."
}