{
    "title": "Unsafe Diffusion: On the Generation of Unsafe Images and Hateful Memes From Text-To-Image Models. (arXiv:2305.13873v1 [cs.CV])",
    "abstract": "State-of-the-art Text-to-Image models like Stable Diffusion and DALLE$\\cdot$2 are revolutionizing how people generate visual content. At the same time, society has serious concerns about how adversaries can exploit such models to generate unsafe images. In this work, we focus on demystifying the generation of unsafe images and hateful memes from Text-to-Image models. We first construct a typology of unsafe images consisting of five categories (sexually explicit, violent, disturbing, hateful, and political). Then, we assess the proportion of unsafe images generated by four advanced Text-to-Image models using four prompt datasets. We find that these models can generate a substantial percentage of unsafe images; across four models and four prompt datasets, 14.56% of all generated images are unsafe. When comparing the four models, we find different risk levels, with Stable Diffusion being the most prone to generating unsafe content (18.92% of all generated images are unsafe). Given Stable ",
    "link": "http://arxiv.org/abs/2305.13873",
    "context": "Title: Unsafe Diffusion: On the Generation of Unsafe Images and Hateful Memes From Text-To-Image Models. (arXiv:2305.13873v1 [cs.CV])\nAbstract: State-of-the-art Text-to-Image models like Stable Diffusion and DALLE$\\cdot$2 are revolutionizing how people generate visual content. At the same time, society has serious concerns about how adversaries can exploit such models to generate unsafe images. In this work, we focus on demystifying the generation of unsafe images and hateful memes from Text-to-Image models. We first construct a typology of unsafe images consisting of five categories (sexually explicit, violent, disturbing, hateful, and political). Then, we assess the proportion of unsafe images generated by four advanced Text-to-Image models using four prompt datasets. We find that these models can generate a substantial percentage of unsafe images; across four models and four prompt datasets, 14.56% of all generated images are unsafe. When comparing the four models, we find different risk levels, with Stable Diffusion being the most prone to generating unsafe content (18.92% of all generated images are unsafe). Given Stable ",
    "path": "papers/23/05/2305.13873.json",
    "total_tokens": 1254,
    "translated_title": "不安全扩散：文本转图像模型生成不安全图像和令人憎恶的模因",
    "translated_abstract": "Stable Diffusion和DALLE·2等最新的文本转图像模型正在彻底改变人们的视觉内容生成方式。同时，社会对对手如何利用这些模型生成不安全图像和令人担忧的模因存在严重的担忧。本研究着眼于揭示文本转图像模型生成不安全图像和令人憎恶的模因。首先，我们构建了一个五种类别的不安全图像分类法(性暴力、暴力、令人不安、令人憎恶和政治)，然后我们使用四个提示数据集评估了四种先进的文本转图像模型生成的不安全图像比例。我们发现这些模型可以生成相当大比例的不安全图像；在四个模型和四个提示数据集中，所有生成的图像中有14.56%是不安全的。在比较这四种模型时，我们发现存在不同的风险水平，其中Stable Diffusion是生成不安全内容最容易的(所有生成的图像中有18.92%是不安全的)。鉴于Stable Diffusion的流行和不安全图像的有害影响，我们进行了对Stable Diffusion的深入分析，以揭示其生成不安全图像的倾向因素。具体而言，我们确定了一些经常导致生成不安全图像的文本提示，以及模型生成某些类型内容的倾向。我们的分析强调了需要继续研究从文本到图像模型的不安全图像生成以及开发强有力的对策的必要性。",
    "tldr": "本文研究揭示了文本转图像模型生成不安全图像和令人憎恶的模因，并且发现这些模型可以生成相当大比例的不安全图像。作者鉴定了一些文本提示因素和模型倾向因素，以揭示不安全内容的生成机理，并且凸显了需要继续研究的必要性。",
    "en_tdlr": "This paper focuses on demystifying the generation of unsafe images and hateful memes from Text-to-Image models. It uncovers that these models can generate a substantial percentage of unsafe images and identifies certain textual prompts and model's tendency as contributing factors. The analysis highlights the need for continued research on the generation of unsafe images from Text-to-Image models and the development of robust countermeasures."
}