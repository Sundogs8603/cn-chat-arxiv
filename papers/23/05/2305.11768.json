{
    "title": "Generating Visual Spatial Description via Holistic 3D Scene Understanding. (arXiv:2305.11768v1 [cs.CV])",
    "abstract": "Visual spatial description (VSD) aims to generate texts that describe the spatial relations of the given objects within images. Existing VSD work merely models the 2D geometrical vision features, thus inevitably falling prey to the problem of skewed spatial understanding of target objects. In this work, we investigate the incorporation of 3D scene features for VSD. With an external 3D scene extractor, we obtain the 3D objects and scene features for input images, based on which we construct a target object-centered 3D spatial scene graph (Go3D-S2G), such that we model the spatial semantics of target objects within the holistic 3D scenes. Besides, we propose a scene subgraph selecting mechanism, sampling topologically-diverse subgraphs from Go3D-S2G, where the diverse local structure features are navigated to yield spatially-diversified text generation. Experimental results on two VSD datasets demonstrate that our framework outperforms the baselines significantly, especially improving on",
    "link": "http://arxiv.org/abs/2305.11768",
    "context": "Title: Generating Visual Spatial Description via Holistic 3D Scene Understanding. (arXiv:2305.11768v1 [cs.CV])\nAbstract: Visual spatial description (VSD) aims to generate texts that describe the spatial relations of the given objects within images. Existing VSD work merely models the 2D geometrical vision features, thus inevitably falling prey to the problem of skewed spatial understanding of target objects. In this work, we investigate the incorporation of 3D scene features for VSD. With an external 3D scene extractor, we obtain the 3D objects and scene features for input images, based on which we construct a target object-centered 3D spatial scene graph (Go3D-S2G), such that we model the spatial semantics of target objects within the holistic 3D scenes. Besides, we propose a scene subgraph selecting mechanism, sampling topologically-diverse subgraphs from Go3D-S2G, where the diverse local structure features are navigated to yield spatially-diversified text generation. Experimental results on two VSD datasets demonstrate that our framework outperforms the baselines significantly, especially improving on",
    "path": "papers/23/05/2305.11768.json",
    "total_tokens": 895,
    "translated_title": "通过整体3D场景理解生成视觉空间描述",
    "translated_abstract": "视觉空间描述(VSD)的目标是生成描述图像中给定对象空间关系的文本。现有的VSD工作仅模拟2D几何视觉特征，因此不可避免地陷入目标对象空间理解倾斜的问题。本文研究了将3D场景特征纳入VSD的方法。通过外部3D场景提取器，我们获取输入图像的3D对象和场景特征，基于此构建目标对象为中心的3D空间场景图(Go3D-S2G)，从而模拟目标对象在整体3D场景中的空间语义。此外，我们提出一种场景子图选择机制，从Go3D-S2G中采样拓扑多样的子图，导航不同的局部结构特征以产生空间多样化的文本生成。对两个VSD数据集的实验结果表明，我们的框架显著优于基线，特别是在one-split情况下提高了性能。",
    "tldr": "本文研究将3D场景特征纳入VSD方法，构建目标对象为中心的3D空间场景图(Go3D-S2G)，提出多样化的文本生成方法，可以显著提高性能。",
    "en_tdlr": "This paper investigates incorporating 3D scene features into Visual Spatial Description (VSD), constructing a target object-centered 3D spatial scene graph (Go3D-S2G) and proposing a spatially-diversified text generation mechanism, leading to significant performance improvement."
}