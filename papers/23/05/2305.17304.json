{
    "title": "External Language Model Integration for Factorized Neural Transducers. (arXiv:2305.17304v1 [cs.CL])",
    "abstract": "We propose an adaptation method for factorized neural transducers (FNT) with external language models. We demonstrate that both neural and n-gram external LMs add significantly more value when linearly interpolated with predictor output compared to shallow fusion, thus confirming that FNT forces the predictor to act like regular language models. Further, we propose a method to integrate class-based n-gram language models into FNT framework resulting in accuracy gains similar to a hybrid setup. We show average gains of 18% WERR with lexical adaptation across various scenarios and additive gains of up to 60% WERR in one entity-rich scenario through a combination of class-based n-gram and neural LMs.",
    "link": "http://arxiv.org/abs/2305.17304",
    "context": "Title: External Language Model Integration for Factorized Neural Transducers. (arXiv:2305.17304v1 [cs.CL])\nAbstract: We propose an adaptation method for factorized neural transducers (FNT) with external language models. We demonstrate that both neural and n-gram external LMs add significantly more value when linearly interpolated with predictor output compared to shallow fusion, thus confirming that FNT forces the predictor to act like regular language models. Further, we propose a method to integrate class-based n-gram language models into FNT framework resulting in accuracy gains similar to a hybrid setup. We show average gains of 18% WERR with lexical adaptation across various scenarios and additive gains of up to 60% WERR in one entity-rich scenario through a combination of class-based n-gram and neural LMs.",
    "path": "papers/23/05/2305.17304.json",
    "total_tokens": 878,
    "translated_title": "外部语言模型在分解神经传输器中的集成",
    "translated_abstract": "我们提出了一种适用于带有外部语言模型的分解神经传输器（FNT）的适应方法。我们证明了与浅层融合相比，线性插值预测输出与神经和n-gram外部LM的结合明显增加了价值，从而确认了FNT强制预测器像常规语言模型一样工作。此外，我们提出了一种将基于类别的n-gram语言模型集成到FNT框架中的方法，可以实现类似于混合设置的准确性提高。通过词汇适应，在各种情况下，我们显示平均18%的WERR增益，并且在一个实体丰富的情况下，通过类别n-gram和神经LM的结合获得高达60%的WERR添加性增益。",
    "tldr": "该论文提出了一种外部语言模型在分解神经传输器中的集成方法，同时证明与浅层融合相比线性插值预测输出与神经和n-gram外部LM的结合可以提高准确性。结果显示平均18%的WERR增益，在一个实体丰富的情况下可以获得高达60%的WERR添加性增益。",
    "en_tdlr": "The paper proposes a method to integrate external language models into factorized neural transducers (FNT) and shows that linearly interpolating predictor output with neural and n-gram external LMs can significantly improve accuracy. By integrating class-based n-gram LMs into the FNT framework, the paper achieves accuracy gains similar to a hybrid setup and demonstrates average gains of 18% WERR with lexical adaptation and up to 60% WERR additive gains in one entity-rich scenario."
}