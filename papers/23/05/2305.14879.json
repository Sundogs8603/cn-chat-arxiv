{
    "title": "ByteSized32: A Corpus and Challenge Task for Generating Task-Specific World Models Expressed as Text Games. (arXiv:2305.14879v2 [cs.CL] UPDATED)",
    "abstract": "In this work, we investigate the capacity of language models to generate explicit, interpretable, and interactive world models of scientific and common-sense reasoning tasks. We operationalize this as a task of generating text games, expressed as hundreds of lines of Python code. To facilitate this task, we introduce ByteSized32 (Code: github.com/cognitiveailab/BYTESIZED32), a corpus of 32 reasoning-focused text games totaling 20k lines of Python code. We empirically demonstrate that GPT-4 can use these games as templates for single-shot in-context learning, successfully producing runnable games on unseen topics in 28% of cases. When allowed to self-reflect on program errors, game runnability substantially increases to 57%. While evaluating simulation fidelity is labor-intensive, we introduce a suite of automated metrics to assess game fidelity, technical validity, adherence to task specifications, and winnability, showing a high degree of agreement with expert human ratings. We pose t",
    "link": "http://arxiv.org/abs/2305.14879",
    "context": "Title: ByteSized32: A Corpus and Challenge Task for Generating Task-Specific World Models Expressed as Text Games. (arXiv:2305.14879v2 [cs.CL] UPDATED)\nAbstract: In this work, we investigate the capacity of language models to generate explicit, interpretable, and interactive world models of scientific and common-sense reasoning tasks. We operationalize this as a task of generating text games, expressed as hundreds of lines of Python code. To facilitate this task, we introduce ByteSized32 (Code: github.com/cognitiveailab/BYTESIZED32), a corpus of 32 reasoning-focused text games totaling 20k lines of Python code. We empirically demonstrate that GPT-4 can use these games as templates for single-shot in-context learning, successfully producing runnable games on unseen topics in 28% of cases. When allowed to self-reflect on program errors, game runnability substantially increases to 57%. While evaluating simulation fidelity is labor-intensive, we introduce a suite of automated metrics to assess game fidelity, technical validity, adherence to task specifications, and winnability, showing a high degree of agreement with expert human ratings. We pose t",
    "path": "papers/23/05/2305.14879.json",
    "total_tokens": 990,
    "translated_title": "ByteSized32: 一个用于生成以文字游戏形式表达的任务特定世界模型的语料库和挑战任务",
    "translated_abstract": "在这项工作中，我们研究了语言模型生成科学和常识推理任务的明确、可解释和互动世界模型的能力。我们将这个任务操作化为生成以Python代码形式表达的文字游戏的任务。为了便于完成这个任务，我们介绍了ByteSized32，一个包含32个以推理为重点的文字游戏的语料库，总共有2万行Python代码。我们经验性地证明GPT-4可以使用这些游戏作为单次上下文学习的模板，在28%的情况下成功生成未见过主题的可运行游戏。当允许自我反思程序错误时，游戏的可运行性大大提高至57%。虽然评估模拟逼真度比较费时，我们引入了一套自动评估指标来评估游戏的逼真度、技术有效性、与任务规格的一致性以及可获胜性，显示出与专家人工评级相当高的一致性。",
    "tldr": "这项工作研究了语言模型生成科学和常识推理任务的世界模型的能力，通过生成以Python代码形式表达的文字游戏来实现。实验证明GPT-4可以使用这些游戏作为模板进行上下文学习，并引入了一套自动评估指标进行模拟逼真度的评估。",
    "en_tdlr": "This work investigates the ability of language models to generate explicit and interpretable world models of scientific and common-sense reasoning tasks. It introduces ByteSized32, a corpus of reasoning-focused text games expressed as Python code, and demonstrates that GPT-4 can use these games for in-context learning. Automated metrics are introduced to evaluate game fidelity, technical validity, adherence to task specifications, and winnability."
}