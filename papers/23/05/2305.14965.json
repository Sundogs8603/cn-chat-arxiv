{
    "title": "Tricking LLMs into Disobedience: Formalizing, Analyzing, and Detecting Jailbreaks",
    "abstract": "arXiv:2305.14965v2 Announce Type: replace  Abstract: Recent explorations with commercial Large Language Models (LLMs) have shown that non-expert users can jailbreak LLMs by simply manipulating their prompts; resulting in degenerate output behavior, privacy and security breaches, offensive outputs, and violations of content regulator policies. Limited studies have been conducted to formalize and analyze these attacks and their mitigations. We bridge this gap by proposing a formalism and a taxonomy of known (and possible) jailbreaks. We survey existing jailbreak methods and their effectiveness on open-source and commercial LLMs (such as GPT-based models, OPT, BLOOM, and FLAN-T5-XXL). We further discuss the challenges of jailbreak detection in terms of their effectiveness against known attacks. For our analysis, we collect a dataset of 3700 jailbreak prompts across 4 tasks. We will make the dataset public along with the model outputs.",
    "link": "https://arxiv.org/abs/2305.14965",
    "context": "Title: Tricking LLMs into Disobedience: Formalizing, Analyzing, and Detecting Jailbreaks\nAbstract: arXiv:2305.14965v2 Announce Type: replace  Abstract: Recent explorations with commercial Large Language Models (LLMs) have shown that non-expert users can jailbreak LLMs by simply manipulating their prompts; resulting in degenerate output behavior, privacy and security breaches, offensive outputs, and violations of content regulator policies. Limited studies have been conducted to formalize and analyze these attacks and their mitigations. We bridge this gap by proposing a formalism and a taxonomy of known (and possible) jailbreaks. We survey existing jailbreak methods and their effectiveness on open-source and commercial LLMs (such as GPT-based models, OPT, BLOOM, and FLAN-T5-XXL). We further discuss the challenges of jailbreak detection in terms of their effectiveness against known attacks. For our analysis, we collect a dataset of 3700 jailbreak prompts across 4 tasks. We will make the dataset public along with the model outputs.",
    "path": "papers/23/05/2305.14965.json",
    "total_tokens": 909,
    "translated_title": "欺骗LLMs让其不遵从：正式化、分析和检测越狱行为",
    "translated_abstract": "最近对商用大规模语言模型（LLMs）的探索表明，非专家用户可以通过简单操纵他们的提示来越狱LLMs；导致退化的输出行为、隐私与安全漏洞、冒犯性输出以及违反内容监管政策。有限的研究已进行了对这些攻击及其缓解措施的正式化和分析。我们通过提出一个形式化描述和已知（及可能的）越狱分类法来填补这一差距。我们调查现有的越狱方法及其在开源和商用LLMs（如基于GPT的模型、OPT、BLOOM和FLAN-T5-XXL）上的有效性。我们进一步讨论了越狱检测在针对已知攻击方面的挑战。为了我们的分析，我们收集了4项任务的3700个越狱提示的数据集。我们将随着模型输出一起公开这一数据集。",
    "tldr": "该论文提出了正式化和已知越狱分类法以填补对商用大规模语言模型（LLMs）被越狱攻击的缺乏研究，调查现有的越狱方法及其在开源和商用LLMs上的有效性。",
    "en_tdlr": "The paper introduces formalism and a taxonomy of known jailbreaks to address the lack of research on attacks against commercial Large Language Models (LLMs), investigates existing jailbreak methods and their effectiveness on open-source and commercial LLMs."
}