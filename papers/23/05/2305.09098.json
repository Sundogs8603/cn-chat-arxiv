{
    "title": "Weight-Inherited Distillation for Task-Agnostic BERT Compression. (arXiv:2305.09098v1 [cs.CL])",
    "abstract": "Knowledge Distillation (KD) is a predominant approach for BERT compression. Previous KD-based methods focus on designing extra alignment losses for the student model to mimic the behavior of the teacher model. These methods transfer the knowledge in an indirect way. In this paper, we propose a novel Weight-Inherited Distillation (WID), which directly transfers knowledge from the teacher. WID does not require any additional alignment loss and trains a compact student by inheriting the weights, showing a new perspective of knowledge distillation. Specifically, we design the row compactors and column compactors as mappings and then compress the weights via structural re-parameterization. Experimental results on the GLUE and SQuAD benchmarks show that WID outperforms previous state-of-the-art KD-based baselines. Further analysis indicates that WID can also learn the attention patterns from the teacher model without any alignment loss on attention distributions.",
    "link": "http://arxiv.org/abs/2305.09098",
    "context": "Title: Weight-Inherited Distillation for Task-Agnostic BERT Compression. (arXiv:2305.09098v1 [cs.CL])\nAbstract: Knowledge Distillation (KD) is a predominant approach for BERT compression. Previous KD-based methods focus on designing extra alignment losses for the student model to mimic the behavior of the teacher model. These methods transfer the knowledge in an indirect way. In this paper, we propose a novel Weight-Inherited Distillation (WID), which directly transfers knowledge from the teacher. WID does not require any additional alignment loss and trains a compact student by inheriting the weights, showing a new perspective of knowledge distillation. Specifically, we design the row compactors and column compactors as mappings and then compress the weights via structural re-parameterization. Experimental results on the GLUE and SQuAD benchmarks show that WID outperforms previous state-of-the-art KD-based baselines. Further analysis indicates that WID can also learn the attention patterns from the teacher model without any alignment loss on attention distributions.",
    "path": "papers/23/05/2305.09098.json",
    "total_tokens": 927,
    "translated_title": "任务无关BERT压缩的权重继承蒸馏方法",
    "translated_abstract": "知识蒸馏（KD）是压缩BERT的主要方法。之前的KD方法侧重于为学生模型设计额外的对齐损失，以模仿教师模型的行为。这些方法以间接的方式传递知识。在本文中，我们提出了一种新颖的权重继承蒸馏（WID）方法，直接从教师模型传递知识。WID不需要额外的对齐损失，通过继承权重来训练一个紧凑的学生模型，展示了知识蒸馏的新视角。具体来说，我们将行压缩器和列压缩器设计为映射，然后通过结构重参数化压缩权重。在GLUE和SQuAD基准测试上的实验结果表明，WID优于之前最先进的基于KD的基线。进一步的分析表明，WID也可以在不需要注意力分布对齐损失的情况下学习教师模型的注意力模式。",
    "tldr": "本文提出了一种直接从教师模型传递知识的权重继承蒸馏方法，不需要额外的对齐损失就可以训练出一个紧凑的学生模型，并且在GLUE和SQuAD基准测试上优于之前最先进的基于KD的基线。",
    "en_tdlr": "This paper proposes a novel Weight-Inherited Distillation method, which directly transfers knowledge from the teacher model without extra alignment losses, and trains a compact student model. Experimental results on the GLUE and SQuAD benchmarks show that WID outperforms previous state-of-the-art KD-based baselines."
}