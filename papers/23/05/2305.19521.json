{
    "title": "Incremental Randomized Smoothing Certification. (arXiv:2305.19521v1 [cs.LG])",
    "abstract": "Randomized smoothing-based certification is an effective approach for obtaining robustness certificates of deep neural networks (DNNs) against adversarial attacks. This method constructs a smoothed DNN model and certifies its robustness through statistical sampling, but it is computationally expensive, especially when certifying with a large number of samples. Furthermore, when the smoothed model is modified (e.g., quantized or pruned), certification guarantees may not hold for the modified DNN, and recertifying from scratch can be prohibitively expensive.  We present the first approach for incremental robustness certification for randomized smoothing, IRS. We show how to reuse the certification guarantees for the original smoothed model to certify an approximated model with very few samples. IRS significantly reduces the computational cost of certifying modified DNNs while maintaining strong robustness guarantees. We experimentally demonstrate the effectiveness of our approach, showin",
    "link": "http://arxiv.org/abs/2305.19521",
    "context": "Title: Incremental Randomized Smoothing Certification. (arXiv:2305.19521v1 [cs.LG])\nAbstract: Randomized smoothing-based certification is an effective approach for obtaining robustness certificates of deep neural networks (DNNs) against adversarial attacks. This method constructs a smoothed DNN model and certifies its robustness through statistical sampling, but it is computationally expensive, especially when certifying with a large number of samples. Furthermore, when the smoothed model is modified (e.g., quantized or pruned), certification guarantees may not hold for the modified DNN, and recertifying from scratch can be prohibitively expensive.  We present the first approach for incremental robustness certification for randomized smoothing, IRS. We show how to reuse the certification guarantees for the original smoothed model to certify an approximated model with very few samples. IRS significantly reduces the computational cost of certifying modified DNNs while maintaining strong robustness guarantees. We experimentally demonstrate the effectiveness of our approach, showin",
    "path": "papers/23/05/2305.19521.json",
    "total_tokens": 889,
    "translated_title": "渐进式随机平滑认证。",
    "translated_abstract": "随机平滑认证是一种有效的方法，用于获取深度神经网络（DNN）对抗攻击的鲁棒性证书。该方法构建了一个平滑的DNN模型，并通过统计抽样来证明其鲁棒性，但计算代价较高，特别是当使用大量样本进行证明时。此外，当修改平滑模型（例如，量化或修剪）时，认证保证可能不适用于修改的DNN，并且从头开始重新认证可能代价太高。我们提出了第一种渐进式鲁棒性认证随机平滑方法（IRS）。我们展示了如何重复使用原始平滑模型的认证保证，以利用很少的样本认证近似模型。IRS显著降低了认证修改DNN的计算成本，同时保持强大的鲁棒性保证。我们通过实验验证了我们方法的有效性，展示了IRS在多个数据集上的优越性能。",
    "tldr": "本文提出了渐进式随机平滑认证方法（IRS），可通过重用原始平滑模型的认证保证来认证近似模型，从而显著降低认证修改DNN的计算成本同时保持强大的鲁棒性保证。",
    "en_tdlr": "The paper proposes a method called incremental randomized smoothing certification (IRS), which can significantly reduce the computational cost of certifying modified DNNs while maintaining strong robustness guarantees. By reusing the certification guarantees for the original smoothed model to certify an approximated model with very few samples, IRS introduces an efficient way for certifying DNNs."
}