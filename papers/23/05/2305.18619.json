{
    "title": "Likelihood-Based Diffusion Language Models. (arXiv:2305.18619v1 [cs.CL])",
    "abstract": "Despite a growing interest in diffusion-based language models, existing work has not shown that these models can attain nontrivial likelihoods on standard language modeling benchmarks. In this work, we take the first steps towards closing the likelihood gap between autoregressive and diffusion-based language models, with the goal of building and releasing a diffusion model which outperforms a small but widely-known autoregressive model. We pursue this goal through algorithmic improvements, scaling laws, and increased compute. On the algorithmic front, we introduce several methodological improvements for the maximum-likelihood training of diffusion language models. We then study scaling laws for our diffusion models and find compute-optimal training regimes which differ substantially from autoregressive models. Using our methods and scaling analysis, we train and release Plaid 1B, a large diffusion language model which outperforms GPT-2 124M in likelihood on benchmark datasets and gener",
    "link": "http://arxiv.org/abs/2305.18619",
    "context": "Title: Likelihood-Based Diffusion Language Models. (arXiv:2305.18619v1 [cs.CL])\nAbstract: Despite a growing interest in diffusion-based language models, existing work has not shown that these models can attain nontrivial likelihoods on standard language modeling benchmarks. In this work, we take the first steps towards closing the likelihood gap between autoregressive and diffusion-based language models, with the goal of building and releasing a diffusion model which outperforms a small but widely-known autoregressive model. We pursue this goal through algorithmic improvements, scaling laws, and increased compute. On the algorithmic front, we introduce several methodological improvements for the maximum-likelihood training of diffusion language models. We then study scaling laws for our diffusion models and find compute-optimal training regimes which differ substantially from autoregressive models. Using our methods and scaling analysis, we train and release Plaid 1B, a large diffusion language model which outperforms GPT-2 124M in likelihood on benchmark datasets and gener",
    "path": "papers/23/05/2305.18619.json",
    "total_tokens": 956,
    "translated_title": "基于似然的扩散语言模型",
    "translated_abstract": "尽管人们对基于扩散的语言模型越来越感兴趣，但现有的工作尚未表明这些模型可以在标准语言建模基准上获得非微不足道的似然度。在这项工作中，我们首先采取了措施来缩小自回归和扩散语言模型之间的似然差异，目标是构建和发布一个超过小但广为人知的自回归模型的扩散模型。我们通过算法改进、缩放定律和增加计算来实现这个目标。在算法前沿，我们引入了几种最大似然训练扩散语言模型的方法论改进。然后，我们研究了我们的扩散模型缩放定律，并发现计算优化的训练方案与自回归模型差别很大。使用我们的方法和缩放分析，我们训练并发布了Plaid 1B，一个大型扩散语言模型，它在基准数据集上的似然度和生成文本质量上优于GPT-2 124M。",
    "tldr": "本论文介绍了基于似然的扩散语言模型，并通过算法改进、缩放定律和增加计算，成功构建和发布了一个超过小但广为人知的自回归模型的扩散模型，优于GPT-2 124M。"
}