{
    "title": "Honey, I Shrunk the Language: Language Model Behavior at Reduced Scale. (arXiv:2305.17266v1 [cs.CL])",
    "abstract": "In recent years, language models have drastically grown in size, and the abilities of these models have been shown to improve with scale. The majority of recent scaling laws studies focused on high-compute high-parameter count settings, leaving the question of when these abilities begin to emerge largely unanswered. In this paper, we investigate whether the effects of pre-training can be observed when the problem size is reduced, modeling a smaller, reduced-vocabulary language. We show the benefits of pre-training with masked language modeling (MLM) objective in models as small as 1.25M parameters, and establish a strong correlation between pre-training perplexity and downstream performance (GLUE benchmark). We examine downscaling effects, extending scaling laws to models as small as ~1M parameters. At this scale, we observe a break of the power law for compute-optimal models and show that the MLM loss does not scale smoothly with compute-cost (FLOPs) below $2.2 \\times 10^{15}$ FLOPs. ",
    "link": "http://arxiv.org/abs/2305.17266",
    "context": "Title: Honey, I Shrunk the Language: Language Model Behavior at Reduced Scale. (arXiv:2305.17266v1 [cs.CL])\nAbstract: In recent years, language models have drastically grown in size, and the abilities of these models have been shown to improve with scale. The majority of recent scaling laws studies focused on high-compute high-parameter count settings, leaving the question of when these abilities begin to emerge largely unanswered. In this paper, we investigate whether the effects of pre-training can be observed when the problem size is reduced, modeling a smaller, reduced-vocabulary language. We show the benefits of pre-training with masked language modeling (MLM) objective in models as small as 1.25M parameters, and establish a strong correlation between pre-training perplexity and downstream performance (GLUE benchmark). We examine downscaling effects, extending scaling laws to models as small as ~1M parameters. At this scale, we observe a break of the power law for compute-optimal models and show that the MLM loss does not scale smoothly with compute-cost (FLOPs) below $2.2 \\times 10^{15}$ FLOPs. ",
    "path": "papers/23/05/2305.17266.json",
    "total_tokens": 944,
    "translated_title": "纵览语言模型：缩减规模后的行为",
    "translated_abstract": "近年来，语言模型的规模急剧增长，这些模型的能力也随着规模的扩大而得到了提高。大部分最近的规模研究都集中在高计算量，高参数的环境中，没有回答这些能力何时开始出现的问题。在本文中，我们研究了在问题规模减小的情况下是否可以观察到预训练的效果，建立了一个较小的、缩减了词汇量的语言模型。我们展示了在参数为125万的模型中使用掩码语言建模（MLM）目标预训练的好处，并建立了预训练困惑和下游性能（GLUE基准）之间的强相关性。我们研究缩小规模的影响，将缩放定律扩展到了大约100万个参数的模型中。在这个规模下，我们观察到了计算-最优模型的幂律破裂，并展示了MLM损失在低于22万亿FLOPs的计算成本下并不平滑地缩放。",
    "tldr": "本文研究了小规模语言模型的训练效果，并展示了掩码语言建模目标的预训练对性能的提高作用。同时，该研究还发现了计算成本与模型效果之间的相关性。"
}