{
    "title": "CLEME: Debiasing Multi-reference Evaluation for Grammatical Error Correction. (arXiv:2305.10819v1 [cs.CL])",
    "abstract": "It is intractable to evaluate the performance of Grammatical Error Correction (GEC) systems since GEC is a highly subjective task. Designing an evaluation metric that is as objective as possible is crucial to the development of GEC task. Previous mainstream evaluation metrics, i.e., reference-based metrics, introduce bias into the multi-reference evaluation because they extract edits without considering the presence of multiple references. To overcome the problem, we propose Chunk-LEvel Multi-reference Evaluation (CLEME) designed to evaluate GEC systems in multi-reference settings. First, CLEME builds chunk sequences with consistent boundaries for the source, the hypothesis and all the references, thus eliminating the bias caused by inconsistent edit boundaries. Then, based on the discovery that there exist boundaries between different grammatical errors, we automatically determine the grammatical error boundaries and compute F$_{0.5}$ scores in a novel way. Our proposed CLEME approach",
    "link": "http://arxiv.org/abs/2305.10819",
    "context": "Title: CLEME: Debiasing Multi-reference Evaluation for Grammatical Error Correction. (arXiv:2305.10819v1 [cs.CL])\nAbstract: It is intractable to evaluate the performance of Grammatical Error Correction (GEC) systems since GEC is a highly subjective task. Designing an evaluation metric that is as objective as possible is crucial to the development of GEC task. Previous mainstream evaluation metrics, i.e., reference-based metrics, introduce bias into the multi-reference evaluation because they extract edits without considering the presence of multiple references. To overcome the problem, we propose Chunk-LEvel Multi-reference Evaluation (CLEME) designed to evaluate GEC systems in multi-reference settings. First, CLEME builds chunk sequences with consistent boundaries for the source, the hypothesis and all the references, thus eliminating the bias caused by inconsistent edit boundaries. Then, based on the discovery that there exist boundaries between different grammatical errors, we automatically determine the grammatical error boundaries and compute F$_{0.5}$ scores in a novel way. Our proposed CLEME approach",
    "path": "papers/23/05/2305.10819.json",
    "total_tokens": 953,
    "translated_title": "CLEME: 针对语法错误修正的去偏置多参考评估方法",
    "translated_abstract": "由于Grammatical Error Correction (GEC)是一项高度主观的任务，因此评估其性能变得困难。设计尽可能客观的评估指标对于GEC任务的发展至关重要。先前的主流评估指标，即基于参考的指标，在提取编辑时未考虑多个参考的存在，从而引入偏见到多参考评估中。为了克服这个问题，我们提出了Chunk-LEvel Multi-reference Evaluation (CLEME)方法，旨在在多参考环境中评估GEC系统。首先，CLEME为源、假设和所有参考建立具有一致边界的块序列，从而消除由不一致的编辑边界引起的偏差。然后，基于发现存在不同语法错误之间的边界，我们自动确定了语法错误的边界，并以一种新颖的方式计算了F$_{0.5}$得分。我们提出的CLEME方法可以有效地去偏置多参考评估GEC系统，并提高GEC评估的可靠性。",
    "tldr": "提出了一种新的Chunk-LEvel Multi-reference Evaluation (CLEME)方法来解决在多参考环境下评估GEC系统时存在的偏差问题，其通过消除由不一致的编辑边界引起的偏差和自动确定语法错误的边界来提高了GEC评估的可靠性。"
}