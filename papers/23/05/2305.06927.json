{
    "title": "Convergence of Alternating Gradient Descent for Matrix Factorization. (arXiv:2305.06927v1 [cs.LG])",
    "abstract": "We consider alternating gradient descent (AGD) with fixed step size $\\eta > 0$, applied to the asymmetric matrix factorization objective. We show that, for a rank-$r$ matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$, $T = \\left( \\left(\\frac{\\sigma_1(\\mathbf{A})}{\\sigma_r(\\mathbf{A})}\\right)^2 \\log(1/\\epsilon)\\right)$ iterations of alternating gradient descent suffice to reach an $\\epsilon$-optimal factorization $\\| \\mathbf{A} \\mathbf{X}_T^{\\vphantom{\\intercal}} \\mathbf{Y}_T^{\\intercal} \\|_{\\rm F}^2 \\leq \\epsilon \\| \\mathbf{A} \\|_{\\rm F}^2$ with high probability starting from an atypical random initialization. The factors have rank $d>r$ so that $\\mathbf{X}_T\\in\\mathbb{R}^{m \\times d}$ and $\\mathbf{Y}_T \\in\\mathbb{R}^{n \\times d}$. Experiments suggest that our proposed initialization is not merely of theoretical benefit, but rather significantly improves convergence of gradient descent in practice. Our proof is conceptually simple: a uniform PL-inequality and uniform Lipschitz smoothne",
    "link": "http://arxiv.org/abs/2305.06927",
    "context": "Title: Convergence of Alternating Gradient Descent for Matrix Factorization. (arXiv:2305.06927v1 [cs.LG])\nAbstract: We consider alternating gradient descent (AGD) with fixed step size $\\eta > 0$, applied to the asymmetric matrix factorization objective. We show that, for a rank-$r$ matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$, $T = \\left( \\left(\\frac{\\sigma_1(\\mathbf{A})}{\\sigma_r(\\mathbf{A})}\\right)^2 \\log(1/\\epsilon)\\right)$ iterations of alternating gradient descent suffice to reach an $\\epsilon$-optimal factorization $\\| \\mathbf{A} \\mathbf{X}_T^{\\vphantom{\\intercal}} \\mathbf{Y}_T^{\\intercal} \\|_{\\rm F}^2 \\leq \\epsilon \\| \\mathbf{A} \\|_{\\rm F}^2$ with high probability starting from an atypical random initialization. The factors have rank $d>r$ so that $\\mathbf{X}_T\\in\\mathbb{R}^{m \\times d}$ and $\\mathbf{Y}_T \\in\\mathbb{R}^{n \\times d}$. Experiments suggest that our proposed initialization is not merely of theoretical benefit, but rather significantly improves convergence of gradient descent in practice. Our proof is conceptually simple: a uniform PL-inequality and uniform Lipschitz smoothne",
    "path": "papers/23/05/2305.06927.json",
    "total_tokens": 1110,
    "translated_title": "矩阵分解中交替梯度下降的收敛性分析",
    "translated_abstract": "本文考虑了应用于不对称矩阵分解目标的具有固定步长$\\eta>0$的交替梯度下降（AGD）。我们证明了，对于秩为$r$的矩阵$\\mathbf {A}\\in \\mathbb {R} ^ {m \\times n}$，$T=\\left(\\left(\\frac{\\sigma_1(\\mathbf{A})}{\\sigma_r(\\mathbf{A})}\\right)^2\\log(1/\\epsilon)\\right)$次交替梯度下降即可从非典型随机初始化高概率地达到$\\epsilon$-最优分解$\\|\\mathbf {A}\\mathbf {X}_T^{\\vphantom{\\intercal}}\\mathbf {Y}_T^{\\intercal}\\|_{\\rm F}^2\\le\\epsilon\\|\\mathbf {A}\\|_{\\rm F}^2$。分解中因子的秩为$d>r$，因此$\\mathbf{X}_T\\in\\mathbb{R}^{m \\times d}$且$\\mathbf{Y}_T\\in\\mathbb{R}^{n \\times d}$。实验表明，我们提出的初始化不仅在理论上有益，而且在实践中显著提高了梯度下降的收敛性。我们的证明概念上很简单：一致的PL不等式和一致的Lipschitz平滑性。",
    "tldr": "本文提出一种交替梯度下降算法，能够高概率地从非典型随机初始化达到一个$\\epsilon$-最优矩阵分解，实验表明该初始化不仅在理论上有益，而且在实践中显著提高了梯度下降的收敛性。",
    "en_tdlr": "This paper proposes an alternating gradient descent algorithm that can reach an $\\epsilon$-optimal matrix factorization with high probability starting from a non-typical random initialization, and experiments show that the proposed initialization significantly improves the convergence of gradient descent in practice."
}