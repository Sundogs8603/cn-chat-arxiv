{
    "title": "Efficient Storage of Fine-Tuned Models via Low-Rank Approximation of Weight Residuals. (arXiv:2305.18425v1 [cs.LG])",
    "abstract": "In this paper, we present an efficient method for storing fine-tuned models by leveraging the low-rank properties of weight residuals. Our key observation is that weight residuals in large overparameterized models exhibit even stronger low-rank characteristics. Based on this insight, we propose Efficient Residual Encoding (ERE), a novel approach that achieves efficient storage of fine-tuned model weights by approximating the low-rank weight residuals. Furthermore, we analyze the robustness of weight residuals and push the limit of storage efficiency by utilizing additional quantization and layer-wise rank allocation. Our experimental results demonstrate that our method significantly reduces memory footprint while preserving performance in various tasks and modalities. We release our code.",
    "link": "http://arxiv.org/abs/2305.18425",
    "context": "Title: Efficient Storage of Fine-Tuned Models via Low-Rank Approximation of Weight Residuals. (arXiv:2305.18425v1 [cs.LG])\nAbstract: In this paper, we present an efficient method for storing fine-tuned models by leveraging the low-rank properties of weight residuals. Our key observation is that weight residuals in large overparameterized models exhibit even stronger low-rank characteristics. Based on this insight, we propose Efficient Residual Encoding (ERE), a novel approach that achieves efficient storage of fine-tuned model weights by approximating the low-rank weight residuals. Furthermore, we analyze the robustness of weight residuals and push the limit of storage efficiency by utilizing additional quantization and layer-wise rank allocation. Our experimental results demonstrate that our method significantly reduces memory footprint while preserving performance in various tasks and modalities. We release our code.",
    "path": "papers/23/05/2305.18425.json",
    "total_tokens": 838,
    "translated_title": "通过基于权重残差的低秩逼近实现精细调整模型的高效存储",
    "translated_abstract": "本文提出了一种利用权重残差的低秩特性来高效存储精细调整模型的方法。我们的关键观察是，大型超参数模型中的权重残差表现出更强的低秩特性。基于此，我们提出了一种名为高效残差编码（ERE）的新方法，通过近似低秩权重残差来实现对精细调整模型权重的高效存储。此外，我们分析了权重残差的稳健性，并通过使用额外的量化和分层秩分配来推动存储效率的极限。我们的实验结果表明，我们的方法可以在各种任务和模态下显著减少内存占用，同时保持性能。我们公开了代码。",
    "tldr": "本论文提出了一种利用权重残差低秩特性实现精细调整模型高效存储的新方法ERE，并通过额外量化和分层秩分配来提高存储效率，实验结果表明该方法显著减少内存占用，同时保持性能。",
    "en_tdlr": "This paper proposes a novel approach, Efficient Residual Encoding (ERE), for efficient storage of fine-tuned model weights by approximating the low-rank weight residuals. The method utilizes additional quantization and layer-wise rank allocation to improve storage efficiency, and experimental results show significant reduction in memory usage while preserving performance. Code is released for public use."
}