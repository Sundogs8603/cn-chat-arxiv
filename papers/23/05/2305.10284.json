{
    "title": "Towards More Robust NLP System Evaluation: Handling Missing Scores in Benchmarks. (arXiv:2305.10284v1 [cs.CL])",
    "abstract": "The evaluation of natural language processing (NLP) systems is crucial for advancing the field, but current benchmarking approaches often assume that all systems have scores available for all tasks, which is not always practical. In reality, several factors such as the cost of running baseline, private systems, computational limitations, or incomplete data may prevent some systems from being evaluated on entire tasks. This paper formalize an existing problem in NLP research: benchmarking when some systems scores are missing on the task, and proposes a novel approach to address it. Our method utilizes a compatible partial ranking approach to impute missing data, which is then aggregated using the Borda count method. It includes two refinements designed specifically for scenarios where either task-level or instance-level scores are available. We also introduce an extended benchmark, which contains over 131 million scores, an order of magnitude larger than existing benchmarks. We validate",
    "link": "http://arxiv.org/abs/2305.10284",
    "context": "Title: Towards More Robust NLP System Evaluation: Handling Missing Scores in Benchmarks. (arXiv:2305.10284v1 [cs.CL])\nAbstract: The evaluation of natural language processing (NLP) systems is crucial for advancing the field, but current benchmarking approaches often assume that all systems have scores available for all tasks, which is not always practical. In reality, several factors such as the cost of running baseline, private systems, computational limitations, or incomplete data may prevent some systems from being evaluated on entire tasks. This paper formalize an existing problem in NLP research: benchmarking when some systems scores are missing on the task, and proposes a novel approach to address it. Our method utilizes a compatible partial ranking approach to impute missing data, which is then aggregated using the Borda count method. It includes two refinements designed specifically for scenarios where either task-level or instance-level scores are available. We also introduce an extended benchmark, which contains over 131 million scores, an order of magnitude larger than existing benchmarks. We validate",
    "path": "papers/23/05/2305.10284.json",
    "total_tokens": 890,
    "translated_title": "更鲁棒的自然语言处理系统评估方法：处理基准测试中的缺失得分问题",
    "translated_abstract": "自然语言处理系统的评估对推动该领域的发展至关重要，但目前的基准测试方法常常假设所有系统在所有任务上都有可用的得分，这并不总是切实可行的。在现实情况下，若干因素（例如运行基线，私有系统，计算限制或不完整的数据）可能会阻止某些系统在整个任务上进行评估。本文正式阐述了自然语言处理研究中的一个现有问题：如何在一些系统的任务得分缺失时进行基准测试，并提出了一种新的解决方法。我们的方法利用兼容的部分排名方法来填补缺失的数据，然后使用Borda计数方法进行聚合。它包括两个特定于任务级得分或实例级得分可用的场景的细化。我们还引入了一个扩展基准测试，其中包含超过1.31亿个得分，比现有基准测试大一个数量级。我们验证了我们的方法并证明其有效性。",
    "tldr": "本文提出了一种鲁棒的自然语言处理系统评估方法，可以解决基准测试中某些系统的得分缺失问题，并引入了一个规模更大的基准测试。"
}