{
    "title": "On the Role of Noise in the Sample Complexity of Learning Recurrent Neural Networks: Exponential Gaps for Long Sequences. (arXiv:2305.18423v1 [stat.ML])",
    "abstract": "We consider the class of noisy multi-layered sigmoid recurrent neural networks with $w$ (unbounded) weights for classification of sequences of length $T$, where independent noise distributed according to $\\mathcal{N}(0,\\sigma^2)$ is added to the output of each neuron in the network. Our main result shows that the sample complexity of PAC learning this class can be bounded by $O (w\\log(T/\\sigma))$. For the non-noisy version of the same class (i.e., $\\sigma=0$), we prove a lower bound of $\\Omega (wT)$ for the sample complexity. Our results indicate an exponential gap in the dependence of sample complexity on $T$ for noisy versus non-noisy networks. Moreover, given the mild logarithmic dependence of the upper bound on $1/\\sigma$, this gap still holds even for numerically negligible values of $\\sigma$.",
    "link": "http://arxiv.org/abs/2305.18423",
    "context": "Title: On the Role of Noise in the Sample Complexity of Learning Recurrent Neural Networks: Exponential Gaps for Long Sequences. (arXiv:2305.18423v1 [stat.ML])\nAbstract: We consider the class of noisy multi-layered sigmoid recurrent neural networks with $w$ (unbounded) weights for classification of sequences of length $T$, where independent noise distributed according to $\\mathcal{N}(0,\\sigma^2)$ is added to the output of each neuron in the network. Our main result shows that the sample complexity of PAC learning this class can be bounded by $O (w\\log(T/\\sigma))$. For the non-noisy version of the same class (i.e., $\\sigma=0$), we prove a lower bound of $\\Omega (wT)$ for the sample complexity. Our results indicate an exponential gap in the dependence of sample complexity on $T$ for noisy versus non-noisy networks. Moreover, given the mild logarithmic dependence of the upper bound on $1/\\sigma$, this gap still holds even for numerically negligible values of $\\sigma$.",
    "path": "papers/23/05/2305.18423.json",
    "total_tokens": 901,
    "translated_title": "噪音在学习循环神经网络的样本复杂度中的作用：长序列的指数差距",
    "translated_abstract": "我们考虑添加独立噪音的多层Sigmoid循环神经网络来分类长度为T的序列。我们的主要结果表明，这个类的PAC学习的样本复杂度可以被界定为$O (w\\log(T/\\sigma))$。对于相同类的非噪声版本（即$\\sigma=0$），我们证明样本复杂度的下界为$\\Omega (wT)$。我们的结果显示出在噪声和非噪声网络的样本复杂度对T的依赖性中存在指数差距。此外，考虑到上限对$1/\\sigma$的对数依赖度很小，即使针对数值上可以忽略的$\\sigma$，这个差距仍然存在。",
    "tldr": "本文研究了添加噪声的多层Sigmoid循环神经网络在学习序列分类问题上的样本复杂度问题，发现带噪声情况下样本复杂度可以用$\\log(T/\\sigma)$来界定，不存在噪声时下界为$wT$，两者存在指数级别的差距。",
    "en_tdlr": "This paper explores the sample complexity of learning sequence classification problems with noisy multi-layered Sigmoid recurrent neural networks, revealing a gap in the exponential dependence of the sample complexity on sequence length $T$ between noisy and non-noisy networks, with the former bounded by $O(w\\log(T/\\sigma))$ and the latter having a lower bound of $\\Omega(wT)$, even for numerically negligible values of $\\sigma$."
}