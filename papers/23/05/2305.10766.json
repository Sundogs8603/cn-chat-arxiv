{
    "title": "Adversarial Amendment is the Only Force Capable of Transforming an Enemy into a Friend. (arXiv:2305.10766v1 [cs.AI])",
    "abstract": "Adversarial attack is commonly regarded as a huge threat to neural networks because of misleading behavior. This paper presents an opposite perspective: adversarial attacks can be harnessed to improve neural models if amended correctly. Unlike traditional adversarial defense or adversarial training schemes that aim to improve the adversarial robustness, the proposed adversarial amendment (AdvAmd) method aims to improve the original accuracy level of neural models on benign samples. We thoroughly analyze the distribution mismatch between the benign and adversarial samples. This distribution mismatch and the mutual learning mechanism with the same learning ratio applied in prior art defense strategies is the main cause leading the accuracy degradation for benign samples. The proposed AdvAmd is demonstrated to steadily heal the accuracy degradation and even leads to a certain accuracy boost of common neural models on benign classification, object detection, and segmentation tasks. The eff",
    "link": "http://arxiv.org/abs/2305.10766",
    "context": "Title: Adversarial Amendment is the Only Force Capable of Transforming an Enemy into a Friend. (arXiv:2305.10766v1 [cs.AI])\nAbstract: Adversarial attack is commonly regarded as a huge threat to neural networks because of misleading behavior. This paper presents an opposite perspective: adversarial attacks can be harnessed to improve neural models if amended correctly. Unlike traditional adversarial defense or adversarial training schemes that aim to improve the adversarial robustness, the proposed adversarial amendment (AdvAmd) method aims to improve the original accuracy level of neural models on benign samples. We thoroughly analyze the distribution mismatch between the benign and adversarial samples. This distribution mismatch and the mutual learning mechanism with the same learning ratio applied in prior art defense strategies is the main cause leading the accuracy degradation for benign samples. The proposed AdvAmd is demonstrated to steadily heal the accuracy degradation and even leads to a certain accuracy boost of common neural models on benign classification, object detection, and segmentation tasks. The eff",
    "path": "papers/23/05/2305.10766.json",
    "total_tokens": 849,
    "translated_title": "对抗修改是将敌人转化为朋友的唯一力量。",
    "translated_abstract": "对抗攻击通常被认为对神经网络具有误导行为造成了巨大的威胁。本文提出了一种相反的观点：如果修改得当，对抗攻击可以用来改善神经模型。与旨在提高对抗鲁棒性的传统对抗防御或对抗训练方案不同，所提出的对抗修改（AdvAmd）方法旨在提高神经模型在良性样本上的原始准确性水平。我们深入分析了良性和对抗样本之间的分布不匹配。这种分布不匹配和与先前的防御策略应用相同的学习比率的相互学习机制是导致良性样本准确度下降的主要原因。所提出的AdvAmd被证明可以稳定地恢复准确度下降，甚至提高常见神经模型在良性分类、物体检测和分割任务上的准确性。",
    "tldr": "本文提出了一种对抗修改 (AdvAmd) 方法，该方法可以改善神经网络模型的准确性并提高其在良性样本上的原始准确性水平。"
}