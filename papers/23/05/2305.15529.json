{
    "title": "Editable Graph Neural Network for Node Classifications. (arXiv:2305.15529v1 [cs.LG])",
    "abstract": "Despite Graph Neural Networks (GNNs) have achieved prominent success in many graph-based learning problem, such as credit risk assessment in financial networks and fake news detection in social networks. However, the trained GNNs still make errors and these errors may cause serious negative impact on society. \\textit{Model editing}, which corrects the model behavior on wrongly predicted target samples while leaving model predictions unchanged on unrelated samples, has garnered significant interest in the fields of computer vision and natural language processing. However, model editing for graph neural networks (GNNs) is rarely explored, despite GNNs' widespread applicability. To fill the gap, we first observe that existing model editing methods significantly deteriorate prediction accuracy (up to $50\\%$ accuracy drop) in GNNs while a slight accuracy drop in multi-layer perception (MLP). The rationale behind this observation is that the node aggregation in GNNs will spread the editing e",
    "link": "http://arxiv.org/abs/2305.15529",
    "context": "Title: Editable Graph Neural Network for Node Classifications. (arXiv:2305.15529v1 [cs.LG])\nAbstract: Despite Graph Neural Networks (GNNs) have achieved prominent success in many graph-based learning problem, such as credit risk assessment in financial networks and fake news detection in social networks. However, the trained GNNs still make errors and these errors may cause serious negative impact on society. \\textit{Model editing}, which corrects the model behavior on wrongly predicted target samples while leaving model predictions unchanged on unrelated samples, has garnered significant interest in the fields of computer vision and natural language processing. However, model editing for graph neural networks (GNNs) is rarely explored, despite GNNs' widespread applicability. To fill the gap, we first observe that existing model editing methods significantly deteriorate prediction accuracy (up to $50\\%$ accuracy drop) in GNNs while a slight accuracy drop in multi-layer perception (MLP). The rationale behind this observation is that the node aggregation in GNNs will spread the editing e",
    "path": "papers/23/05/2305.15529.json",
    "total_tokens": 906,
    "translated_title": "可编辑图神经网络用于节点分类",
    "translated_abstract": "尽管图神经网络（GNN）在许多基于图的学习问题中取得了卓越的成功，例如金融网络中的信用风险评估和社交网络中的假新闻检测。然而，受训练的GNN仍然会出现错误，并且这些错误可能对社会造成严重的负面影响。 在计算机视觉和自然语言处理领域中，“模型编辑”已引起了极大的关注，该方法在纠正错误预测时不影响未被触及的预测。然而，尽管GNN具有广泛的应用领域，但GNN的模型编辑尚未得到广泛探索。为了填补这一空白，我们首先观察到现有的模型编辑方法会显著降低GNN的预测准确率（高达50％的准确率下降），而在多层感知器（MLP）中只有轻微的准确率下降。这个观察结果背后的原理是GNN中的节点聚合将传播编辑错误",
    "tldr": "该论文提出了一种可编辑的图神经网络，应用于节点分类。通过编辑模型的方式，修复预测错误，并不影响其他未受影响的预测。该方法可以显著提高GNN的预测准确率。",
    "en_tdlr": "This paper proposes an editable graph neural network for node classifications. By editing the model to correct predicted errors while leaving unaffected predictions unchanged, it significantly improves GNN's prediction accuracy, which previous model editing methods have failed."
}