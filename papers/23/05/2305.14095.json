{
    "title": "S-CLIP: Semi-supervised Vision-Language Learning using Few Specialist Captions. (arXiv:2305.14095v2 [cs.CV] UPDATED)",
    "abstract": "Vision-language models, such as contrastive language-image pre-training (CLIP), have demonstrated impressive results in natural image domains. However, these models often struggle when applied to specialized domains like remote sensing, and adapting to such domains is challenging due to the limited number of image-text pairs available for training. To address this, we propose S-CLIP, a semi-supervised learning method for training CLIP that utilizes additional unpaired images. S-CLIP employs two pseudo-labeling strategies specifically designed for contrastive learning and the language modality. The caption-level pseudo-label is given by a combination of captions of paired images, obtained by solving an optimal transport problem between unpaired and paired images. The keyword-level pseudo-label is given by a keyword in the caption of the nearest paired image, trained through partial label learning that assumes a candidate set of labels for supervision instead of the exact one. By combini",
    "link": "http://arxiv.org/abs/2305.14095",
    "context": "Title: S-CLIP: Semi-supervised Vision-Language Learning using Few Specialist Captions. (arXiv:2305.14095v2 [cs.CV] UPDATED)\nAbstract: Vision-language models, such as contrastive language-image pre-training (CLIP), have demonstrated impressive results in natural image domains. However, these models often struggle when applied to specialized domains like remote sensing, and adapting to such domains is challenging due to the limited number of image-text pairs available for training. To address this, we propose S-CLIP, a semi-supervised learning method for training CLIP that utilizes additional unpaired images. S-CLIP employs two pseudo-labeling strategies specifically designed for contrastive learning and the language modality. The caption-level pseudo-label is given by a combination of captions of paired images, obtained by solving an optimal transport problem between unpaired and paired images. The keyword-level pseudo-label is given by a keyword in the caption of the nearest paired image, trained through partial label learning that assumes a candidate set of labels for supervision instead of the exact one. By combini",
    "path": "papers/23/05/2305.14095.json",
    "total_tokens": 933,
    "translated_title": "S-CLIP: 使用少量专业字幕的半监督视觉语言学习",
    "translated_abstract": "视觉语言模型，如对比语言-图像预训练 (CLIP)，在自然图像领域展示了令人印象深刻的结果。然而，这些模型在应用于遥感等专业领域时往往遇到困难，由于训练时可用的图像-文本对数量有限，对这些领域的适应性具有挑战性。为了解决这个问题，我们提出了S-CLIP，一种用于训练CLIP的半监督学习方法，利用了额外的未配对图像。S-CLIP采用两种伪标签策略，专门针对对比学习和语言模态设计。字幕级伪标签由配对图像的字幕组合给出，通过解决未配对和配对图像之间的最优传输问题获得。关键词级伪标签由最近的配对图像字幕中的关键词给出，通过假设候选标签集合进行部分标签学习进行训练，而不是准确的标签。通过结合这两种伪标签，我们实现了在专业领域进行视觉语言学习的效果提升。",
    "tldr": "S-CLIP是一种利用少量专业字幕进行半监督视觉语言学习的方法，通过两种伪标签策略提高了模型在专业领域的适应性。",
    "en_tdlr": "S-CLIP is a semi-supervised vision-language learning method that utilizes few specialist captions, improving the adaptability of the model in specialized domains through two pseudo-labeling strategies."
}