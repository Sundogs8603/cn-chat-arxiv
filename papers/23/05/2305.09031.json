{
    "title": "AI in the Loop -- Functionalizing Fold Performance Disagreement to Monitor Automated Medical Image Segmentation Pipelines. (arXiv:2305.09031v1 [eess.IV])",
    "abstract": "Methods for automatically flag poor performing-predictions are essential for safely implementing machine learning workflows into clinical practice and for identifying difficult cases during model training. We present a readily adoptable method using sub-models trained on different dataset folds, where their disagreement serves as a surrogate for model confidence. Thresholds informed by human interobserver values were used to determine whether a final ensemble model prediction would require manual review. In two different datasets (abdominal CT and MR predicting kidney tumors), our framework effectively identified low performing automated segmentations. Flagging images with a minimum Interfold test Dice score below human interobserver variability maximized the number of flagged images while ensuring maximum ensemble test Dice. When our internally trained model was applied to an external publicly available dataset (KiTS21), flagged images included smaller tumors than those observed in ou",
    "link": "http://arxiv.org/abs/2305.09031",
    "context": "Title: AI in the Loop -- Functionalizing Fold Performance Disagreement to Monitor Automated Medical Image Segmentation Pipelines. (arXiv:2305.09031v1 [eess.IV])\nAbstract: Methods for automatically flag poor performing-predictions are essential for safely implementing machine learning workflows into clinical practice and for identifying difficult cases during model training. We present a readily adoptable method using sub-models trained on different dataset folds, where their disagreement serves as a surrogate for model confidence. Thresholds informed by human interobserver values were used to determine whether a final ensemble model prediction would require manual review. In two different datasets (abdominal CT and MR predicting kidney tumors), our framework effectively identified low performing automated segmentations. Flagging images with a minimum Interfold test Dice score below human interobserver variability maximized the number of flagged images while ensuring maximum ensemble test Dice. When our internally trained model was applied to an external publicly available dataset (KiTS21), flagged images included smaller tumors than those observed in ou",
    "path": "papers/23/05/2305.09031.json",
    "total_tokens": 1041,
    "translated_title": "AI在医学图像分割管线中的应用——使用Fold性能差异监测自动化医学图像分割管线",
    "translated_abstract": "自动标记预测不良表现的方法对于安全地将机器学习工作流纳入临床实践以及在模型训练期间识别困难病例至关重要。我们提出了一种可轻易采用的方法，使用在不同数据集折叠上训练的子模型，它们的差异可用作模型置信度的替代。使用人类互观者值确定阈值，以确定是否需要人工审查最终的集合模型预测。在两个不同的数据集（腹部CT和MR预测肾肿瘤），我们的框架有效地识别了低性能的自动分割。最小Interfold测试Dice分数低于人类互帮者可变性的图像被标记，以最大化被标记图像的数量，同时确保最大的集合测试Dice。当我们的内部训练模型应用于外部公开数据集（KiTS21）时，标记的图像包括比我们的训练数据中观察到的更小的肿瘤和先前未见的具有挑战性的病例，显示了我们方法的健壮性。我们的方法提供了一种实用的手段，以改善目前在医学图像分割的机器学习系统中的黑箱性质，以支持临床实践。",
    "tldr": "提出了一种使用在不同数据集折叠上训练的子模型的方法来确定模型的置信度，有效地标记低性能的自动分割，并在医学图像分割中实现了对机器学习系统的改进。",
    "en_tdlr": "A practical method for improving the black-box nature of machine learning systems in medical image segmentation was proposed, where sub-models trained on different dataset folds are used to determine the model's confidence and effectively flag low-performing automated segmentations."
}