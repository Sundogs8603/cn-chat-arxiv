{
    "title": "Improving CLIP Training with Language Rewrites. (arXiv:2305.20088v2 [cs.CV] UPDATED)",
    "abstract": "Contrastive Language-Image Pre-training (CLIP) stands as one of the most effective and scalable methods for training transferable vision models using paired image and text data. CLIP models are trained using contrastive loss, which typically relies on data augmentations to prevent overfitting and shortcuts. However, in the CLIP training paradigm, data augmentations are exclusively applied to image inputs, while language inputs remain unchanged throughout the entire training process, limiting the exposure of diverse texts to the same image. In this paper, we introduce Language augmented CLIP (LaCLIP), a simple yet highly effective approach to enhance CLIP training through language rewrites. Leveraging the in-context learning capability of large language models, we rewrite the text descriptions associated with each image. These rewritten texts exhibit diversity in sentence structure and vocabulary while preserving the original key concepts and meanings. During training, LaCLIP randomly s",
    "link": "http://arxiv.org/abs/2305.20088",
    "context": "Title: Improving CLIP Training with Language Rewrites. (arXiv:2305.20088v2 [cs.CV] UPDATED)\nAbstract: Contrastive Language-Image Pre-training (CLIP) stands as one of the most effective and scalable methods for training transferable vision models using paired image and text data. CLIP models are trained using contrastive loss, which typically relies on data augmentations to prevent overfitting and shortcuts. However, in the CLIP training paradigm, data augmentations are exclusively applied to image inputs, while language inputs remain unchanged throughout the entire training process, limiting the exposure of diverse texts to the same image. In this paper, we introduce Language augmented CLIP (LaCLIP), a simple yet highly effective approach to enhance CLIP training through language rewrites. Leveraging the in-context learning capability of large language models, we rewrite the text descriptions associated with each image. These rewritten texts exhibit diversity in sentence structure and vocabulary while preserving the original key concepts and meanings. During training, LaCLIP randomly s",
    "path": "papers/23/05/2305.20088.json",
    "total_tokens": 894,
    "translated_title": "改进CLIP训练的语言重写方法",
    "translated_abstract": "对比语言-图像预训练（CLIP）是使用成对的图像和文本数据进行训练可转移视觉模型的最有效和可扩展的方法之一。CLIP模型使用对比损失进行训练，通常依赖于数据增强来防止过拟合和捷径问题。然而，在CLIP训练范式中，数据增强仅应用于图像输入，而语言输入在整个训练过程中保持不变，限制了多样文本对相同图像的暴露。本文介绍了Language augmented CLIP（LaCLIP），一种简单而高效的方法，通过语言重写来增强CLIP训练。利用大型语言模型的上下文学习能力，我们重新书写与每个图像关联的文本描述。这些重新书写的文本在句子结构和词汇方面呈现多样性，同时保留了原始关键概念和意义。在训练过程中，LaCLIP随机地。",
    "tldr": "本文介绍了一种名为Language augmented CLIP（LaCLIP）的方法，通过语言重写来增强CLIP训练。利用大型语言模型的能力，重新书写与每个图像关联的文本描述，以增加多样性，同时保留原始的关键概念和意义。"
}