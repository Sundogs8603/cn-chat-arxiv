{
    "title": "Measuring and Mitigating Local Instability in Deep Neural Networks. (arXiv:2305.10625v1 [cs.LG])",
    "abstract": "Deep Neural Networks (DNNs) are becoming integral components of real world services relied upon by millions of users. Unfortunately, architects of these systems can find it difficult to ensure reliable performance as irrelevant details like random initialization can unexpectedly change the outputs of a trained system with potentially disastrous consequences. We formulate the model stability problem by studying how the predictions of a model change, even when it is retrained on the same data, as a consequence of stochasticity in the training process. For Natural Language Understanding (NLU) tasks, we find instability in predictions for a significant fraction of queries. We formulate principled metrics, like per-sample ``label entropy'' across training runs or within a single training run, to quantify this phenomenon. Intriguingly, we find that unstable predictions do not appear at random, but rather appear to be clustered in data-specific ways. We study data-agnostic regularization meth",
    "link": "http://arxiv.org/abs/2305.10625",
    "context": "Title: Measuring and Mitigating Local Instability in Deep Neural Networks. (arXiv:2305.10625v1 [cs.LG])\nAbstract: Deep Neural Networks (DNNs) are becoming integral components of real world services relied upon by millions of users. Unfortunately, architects of these systems can find it difficult to ensure reliable performance as irrelevant details like random initialization can unexpectedly change the outputs of a trained system with potentially disastrous consequences. We formulate the model stability problem by studying how the predictions of a model change, even when it is retrained on the same data, as a consequence of stochasticity in the training process. For Natural Language Understanding (NLU) tasks, we find instability in predictions for a significant fraction of queries. We formulate principled metrics, like per-sample ``label entropy'' across training runs or within a single training run, to quantify this phenomenon. Intriguingly, we find that unstable predictions do not appear at random, but rather appear to be clustered in data-specific ways. We study data-agnostic regularization meth",
    "path": "papers/23/05/2305.10625.json",
    "total_tokens": 1093,
    "translated_title": "深度神经网络中的局部不稳定性测量和减少方法",
    "translated_abstract": "深度神经网络已经成为数百万用户依赖的实际场景应用的重要组成部分。然而，这些系统的构建者往往很难确保可靠的性能，因为像随机初始化这样的无关细节可能意外地改变训练系统的输出，可能带来灾难性的后果。我们通过研究模型稳定性问题，研究模型在训练过程中的随机性对模型预测结果的影响，即使在同一数据上重新训练，预测仍然会发生变化。对于自然语言理解（NLU）任务，我们发现其中相当一部分查询的预测存在不稳定性。我们提出了一些基于原则的指标，如跨训练运行或单次训练内的每个样本的“标签熵”，来量化这种现象。有趣的是，我们发现不稳定的预测并不是随机出现的，而是以数据相关的方式聚集在一起。我们研究了数据无关正则化方法来减轻这种不稳定性，并表明一些方法可以显着提高不稳定性，甚至在某些情况下优于更广泛使用的正则化方法。",
    "tldr": "深度神经网络中，训练过程中的随机性可能导致模型的输出不稳定，作者提出了基于原则的指标来量化不稳定性并发现不稳定的预测并不是随机出现的，而是以数据相关的方式聚集在一起。作者研究了数据无关正则化方法来减轻这种不稳定性，并表明一些方法可以显着提高不稳定性，甚至在某些情况下优于更广泛使用的正则化方法。",
    "en_tdlr": "Training stochasticity can cause instability in deep neural networks, even on the same data. Authors propose principled metrics to quantify this phenomenon, and show that unstable predictions are clustered in data-specific ways. Data-agnostic regularization methods are studied for mitigating this instability, achieving better results than widely-used methods in some cases."
}