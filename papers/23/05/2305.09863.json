{
    "title": "Explaining black box text modules in natural language with language models. (arXiv:2305.09863v1 [cs.AI])",
    "abstract": "Large language models (LLMs) have demonstrated remarkable prediction performance for a growing array of tasks. However, their rapid proliferation and increasing opaqueness have created a growing need for interpretability. Here, we ask whether we can automatically obtain natural language explanations for black box text modules. A \"text module\" is any function that maps text to a scalar continuous value, such as a submodule within an LLM or a fitted model of a brain region. \"Black box\" indicates that we only have access to the module's inputs/outputs.  We introduce Summarize and Score (SASC), a method that takes in a text module and returns a natural language explanation of the module's selectivity along with a score for how reliable the explanation is. We study SASC in 3 contexts. First, we evaluate SASC on synthetic modules and find that it often recovers ground truth explanations. Second, we use SASC to explain modules found within a pre-trained BERT model, enabling inspection of the ",
    "link": "http://arxiv.org/abs/2305.09863",
    "context": "Title: Explaining black box text modules in natural language with language models. (arXiv:2305.09863v1 [cs.AI])\nAbstract: Large language models (LLMs) have demonstrated remarkable prediction performance for a growing array of tasks. However, their rapid proliferation and increasing opaqueness have created a growing need for interpretability. Here, we ask whether we can automatically obtain natural language explanations for black box text modules. A \"text module\" is any function that maps text to a scalar continuous value, such as a submodule within an LLM or a fitted model of a brain region. \"Black box\" indicates that we only have access to the module's inputs/outputs.  We introduce Summarize and Score (SASC), a method that takes in a text module and returns a natural language explanation of the module's selectivity along with a score for how reliable the explanation is. We study SASC in 3 contexts. First, we evaluate SASC on synthetic modules and find that it often recovers ground truth explanations. Second, we use SASC to explain modules found within a pre-trained BERT model, enabling inspection of the ",
    "path": "papers/23/05/2305.09863.json",
    "total_tokens": 963,
    "translated_title": "利用语言模型用自然语言解释黑盒文本模块",
    "translated_abstract": "大型语言模型已经证明在各种任务中具有出色的预测性能。然而，它们的快速增长和不透明性已经引起了对可解释性的需求。本文询问是否可以自动获取黑盒文本模块的自然语言解释。一个“文本模块”是将文本映射到标量连续值的任何函数，例如LLM内的子模块或大脑区域的拟合模型。“黑盒”表示我们只能访问模块的输入/输出。我们引入了Summarize and Score（SASC）方法，它接受文本模块并返回模块选择性的自然语言解释以及解释可靠程度的分数。我们在三个上下文中研究SASC。首先，我们在合成模块上评估SASC，并发现它经常恢复基本真相说明。其次，我们使用SASC来解释预训练BERT模型中的模块，使得检查BERT的模块成为可能。",
    "tldr": "本文介绍了一种名为Summarize and Score（SASC）的方法，该方法可以自动获取黑盒文本模块的自然语言解释以及解释可靠程度的分数。研究者们已经在合成模块和BERT模型中使用SASC，让我们可以解释模块的选择性，这对于增强大型语言模型的可解释性非常重要。",
    "en_tdlr": "This paper introduces a method called Summarize and Score (SASC) that can automatically obtain natural language explanations and a reliability score for black box text modules. SASC has been shown to be effective in synthetic modules and the pre-trained BERT model, enabling explanation of module selectivity and enhancing interpretability of large language models."
}