{
    "title": "Sliding Window Sum Algorithms for Deep Neural Networks. (arXiv:2305.16513v1 [cs.LG])",
    "abstract": "Sliding window sums are widely used for string indexing, hashing and time series analysis. We have developed a family of the generic vectorized sliding sum algorithms that provide speedup of O(P/w) for window size $w$ and number of processors P. For a sum with a commutative operator the speedup is improved to O(P/log(w)). Even more important, our algorithms exhibit efficient memory access patterns. In this paper we study the application of the sliding sum algorithms to the training and inference of the Deep Neural Networks. We demonstrate how both pooling and convolution primitives could be expressed as sliding sums and evaluated by the compute kernels with the shared structure. We show that the sliding sum convolution kernels are more efficient than the commonly used GEMM kernels on the CPU, and could even outperform their GPU counterparts.",
    "link": "http://arxiv.org/abs/2305.16513",
    "context": "Title: Sliding Window Sum Algorithms for Deep Neural Networks. (arXiv:2305.16513v1 [cs.LG])\nAbstract: Sliding window sums are widely used for string indexing, hashing and time series analysis. We have developed a family of the generic vectorized sliding sum algorithms that provide speedup of O(P/w) for window size $w$ and number of processors P. For a sum with a commutative operator the speedup is improved to O(P/log(w)). Even more important, our algorithms exhibit efficient memory access patterns. In this paper we study the application of the sliding sum algorithms to the training and inference of the Deep Neural Networks. We demonstrate how both pooling and convolution primitives could be expressed as sliding sums and evaluated by the compute kernels with the shared structure. We show that the sliding sum convolution kernels are more efficient than the commonly used GEMM kernels on the CPU, and could even outperform their GPU counterparts.",
    "path": "papers/23/05/2305.16513.json",
    "total_tokens": 812,
    "translated_title": "深度神经网络的滑动窗口求和算法",
    "translated_abstract": "滑动窗口求和广泛应用于字符串索引、哈希和时间序列分析。我们开发了一系列通用矢量化滑动和算法，对于窗口大小w和处理器数量P，提供了O（P/w）的加速。对于可交换运算符的总和，加速可以提高到O（P/log(w)）。更重要的是，我们的算法表现出高效的内存访问模式。在本文中，我们研究了将滑动和算法应用于深度神经网络的训练和推理。我们展示了如何将池化和卷积原语表达为滑动和，并通过具有共享结构的计算内核进行评估。我们表明，滑动和卷积内核比CPU上通常使用的GEMM内核更有效，甚至可以胜过它们的GPU同行。",
    "tldr": "本文介绍了一系列针对深度神经网络训练和推理的通用矢量化滑动和算法，提高了计算速度，比常用的算法更有效，能够表达池化和卷积原语。",
    "en_tdlr": "The paper introduces a family of generic vectorized sliding sum algorithms for the training and inference of Deep Neural Networks. These algorithms offer improved computational speed and are more efficient than commonly used algorithms while being able to express pooling and convolution primitives."
}