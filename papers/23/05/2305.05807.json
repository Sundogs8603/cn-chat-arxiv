{
    "title": "Even Small Correlation and Diversity Shifts Pose Dataset-Bias Issues. (arXiv:2305.05807v1 [cs.CV])",
    "abstract": "Distribution shifts are common in real-world datasets and can affect the performance and reliability of deep learning models. In this paper, we study two types of distribution shifts: diversity shifts, which occur when test samples exhibit patterns unseen during training, and correlation shifts, which occur when test data present a different correlation between seen invariant and spurious features. We propose an integrated protocol to analyze both types of shifts using datasets where they co-exist in a controllable manner. Finally, we apply our approach to a real-world classification problem of skin cancer analysis, using out-of-distribution datasets and specialized bias annotations. Our protocol reveals three findings: 1) Models learn and propagate correlation shifts even with low-bias training; this poses a risk of accumulating and combining unaccountable weak biases; 2) Models learn robust features in highand low-bias scenarios but use spurious ones if test samples have them; this",
    "link": "http://arxiv.org/abs/2305.05807",
    "context": "Title: Even Small Correlation and Diversity Shifts Pose Dataset-Bias Issues. (arXiv:2305.05807v1 [cs.CV])\nAbstract: Distribution shifts are common in real-world datasets and can affect the performance and reliability of deep learning models. In this paper, we study two types of distribution shifts: diversity shifts, which occur when test samples exhibit patterns unseen during training, and correlation shifts, which occur when test data present a different correlation between seen invariant and spurious features. We propose an integrated protocol to analyze both types of shifts using datasets where they co-exist in a controllable manner. Finally, we apply our approach to a real-world classification problem of skin cancer analysis, using out-of-distribution datasets and specialized bias annotations. Our protocol reveals three findings: 1) Models learn and propagate correlation shifts even with low-bias training; this poses a risk of accumulating and combining unaccountable weak biases; 2) Models learn robust features in highand low-bias scenarios but use spurious ones if test samples have them; this",
    "path": "papers/23/05/2305.05807.json",
    "total_tokens": 898,
    "translated_title": "即使很小的相关性和多样性变化也会导致数据集偏差问题",
    "translated_abstract": "分布变化在实际数据集中很常见，会影响深度学习模型的性能和可靠性。本文研究了两种类型的分布变化：多样性变化和相关性变化。我们提出了一个综合协议，使用同时存在这两种变化的数据集来分析它们。最后，我们将我们的方法应用于一个真实的皮肤癌分析分类问题中，使用了超出数据集和专门的偏差注释。我们的协议揭示了三个发现：1）模型即使进行了低偏差训练也会学习并传播相关性变化，这可能会累积和结合难以解释的弱偏差的风险；2）模型在高、低偏差情况下可以学习到稳健的特征，但是如果测试样本有错误的特征它们可能会使用这些特征。",
    "tldr": "本文研究了数据集中的分布变化对深度学习模型的影响，并提出了一个综合协议来分析多样性变化和相关性变化。使用皮肤癌分析分类问题的实例，发现模型不仅会学习和传播相关性变化，而且可能会使用错误的特征。"
}