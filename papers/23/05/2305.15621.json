{
    "title": "Matrix Estimation for Offline Reinforcement Learning with Low-Rank Structure. (arXiv:2305.15621v1 [cs.LG])",
    "abstract": "We consider offline Reinforcement Learning (RL), where the agent does not interact with the environment and must rely on offline data collected using a behavior policy. Previous works provide policy evaluation guarantees when the target policy to be evaluated is covered by the behavior policy, that is, state-action pairs visited by the target policy must also be visited by the behavior policy. We show that when the MDP has a latent low-rank structure, this coverage condition can be relaxed. Building on the connection to weighted matrix completion with non-uniform observations, we propose an offline policy evaluation algorithm that leverages the low-rank structure to estimate the values of uncovered state-action pairs. Our algorithm does not require a known feature representation, and our finite-sample error bound involves a novel discrepancy measure quantifying the discrepancy between the behavior and target policies in the spectral space. We provide concrete examples where our algorit",
    "link": "http://arxiv.org/abs/2305.15621",
    "context": "Title: Matrix Estimation for Offline Reinforcement Learning with Low-Rank Structure. (arXiv:2305.15621v1 [cs.LG])\nAbstract: We consider offline Reinforcement Learning (RL), where the agent does not interact with the environment and must rely on offline data collected using a behavior policy. Previous works provide policy evaluation guarantees when the target policy to be evaluated is covered by the behavior policy, that is, state-action pairs visited by the target policy must also be visited by the behavior policy. We show that when the MDP has a latent low-rank structure, this coverage condition can be relaxed. Building on the connection to weighted matrix completion with non-uniform observations, we propose an offline policy evaluation algorithm that leverages the low-rank structure to estimate the values of uncovered state-action pairs. Our algorithm does not require a known feature representation, and our finite-sample error bound involves a novel discrepancy measure quantifying the discrepancy between the behavior and target policies in the spectral space. We provide concrete examples where our algorit",
    "path": "papers/23/05/2305.15621.json",
    "total_tokens": 715,
    "translated_title": "具有低秩结构的离线强化学习矩阵估计",
    "translated_abstract": "本文提出了一种针对离线强化学习问题的矩阵估计方法，当MDP具有低秩结构时能够松弛state-action覆盖条件限制，不需要预先知道特征表示。 通过提出一种新的差异度量方法，我们给出了有限样本下的误差上界，并给出了具体例子来证明我们算法的有效性。",
    "tldr": "本文提出了离线强化学习的矩阵估计方法，当MDP具有低秩结构时可松弛覆盖条件限制，有效避免了特征表示的需要。",
    "en_tdlr": "This paper proposes a matrix estimation method for offline reinforcement learning, which relaxes the coverage condition constraint when the MDP has a low-rank structure and does not require a known feature representation. A novel discrepancy measure is used to provide finite-sample error bounds, and concrete examples are given to validate the effectiveness of the algorithm."
}