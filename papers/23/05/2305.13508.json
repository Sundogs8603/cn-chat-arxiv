{
    "title": "DeepBern-Nets: Taming the Complexity of Certifying Neural Networks using Bernstein Polynomial Activations and Precise Bound Propagation. (arXiv:2305.13508v1 [cs.LG])",
    "abstract": "Formal certification of Neural Networks (NNs) is crucial for ensuring their safety, fairness, and robustness. Unfortunately, on the one hand, sound and complete certification algorithms of ReLU-based NNs do not scale to large-scale NNs. On the other hand, incomplete certification algorithms are easier to compute, but they result in loose bounds that deteriorate with the depth of NN, which diminishes their effectiveness. In this paper, we ask the following question; can we replace the ReLU activation function with one that opens the door to incomplete certification algorithms that are easy to compute but can produce tight bounds on the NN's outputs? We introduce DeepBern-Nets, a class of NNs with activation functions based on Bernstein polynomials instead of the commonly used ReLU activation. Bernstein polynomials are smooth and differentiable functions with desirable properties such as the so-called range enclosure and subdivision properties. We design a novel algorithm, called Bern-IB",
    "link": "http://arxiv.org/abs/2305.13508",
    "context": "Title: DeepBern-Nets: Taming the Complexity of Certifying Neural Networks using Bernstein Polynomial Activations and Precise Bound Propagation. (arXiv:2305.13508v1 [cs.LG])\nAbstract: Formal certification of Neural Networks (NNs) is crucial for ensuring their safety, fairness, and robustness. Unfortunately, on the one hand, sound and complete certification algorithms of ReLU-based NNs do not scale to large-scale NNs. On the other hand, incomplete certification algorithms are easier to compute, but they result in loose bounds that deteriorate with the depth of NN, which diminishes their effectiveness. In this paper, we ask the following question; can we replace the ReLU activation function with one that opens the door to incomplete certification algorithms that are easy to compute but can produce tight bounds on the NN's outputs? We introduce DeepBern-Nets, a class of NNs with activation functions based on Bernstein polynomials instead of the commonly used ReLU activation. Bernstein polynomials are smooth and differentiable functions with desirable properties such as the so-called range enclosure and subdivision properties. We design a novel algorithm, called Bern-IB",
    "path": "papers/23/05/2305.13508.json",
    "total_tokens": 855,
    "translated_title": "DeepBern-Nets: 使用Bernstein多项式激活和精准边界传播驯化神经网络认证的复杂性",
    "translated_abstract": "神经网络（NN）的正式认证至关重要，以确保其安全、公正和鲁棒性。不幸的是，基于ReLU的NN的完整认证算法不适用于大规模NN。而基于不完整认证算法易于计算，但它们产生的界限会随着NN的深度而变得宽松，这降低了它们的有效性。本文提出了“DeepBern-Nets”，这是一类使用Bernstein多项式作为激活函数而非常用的ReLU的NN。Bernstein多项式是光滑且可微的函数，具有称为\"区间包围\"和\"细分\"属性的理想属性。我们设计了一种新算法，称为“Bern-IB”。",
    "tldr": "本文提出一种新型神经网络DeepBern-Nets，使用Bernstein多项式代替ReLU作为激活函数，可以轻松计算不完整认证算法，并能产生紧密的界限，可用于确保神经网络的安全、公正和鲁棒性。",
    "en_tdlr": "The paper proposes a new type of neural network, DeepBern-Nets, using Bernstein polynomials instead of ReLU as activation function, which can be easily computed by incomplete certification algorithms and produce tight bounds to ensure the safety, fairness and robustness of neural networks."
}