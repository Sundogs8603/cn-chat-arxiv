{
    "title": "Sketching the Future (STF): Applying Conditional Control Techniques to Text-to-Video Models. (arXiv:2305.05845v1 [cs.CV])",
    "abstract": "The proliferation of video content demands efficient and flexible neural network based approaches for generating new video content. In this paper, we propose a novel approach that combines zero-shot text-to-video generation with ControlNet to improve the output of these models. Our method takes multiple sketched frames as input and generates video output that matches the flow of these frames, building upon the Text-to-Video Zero architecture and incorporating ControlNet to enable additional input conditions. By first interpolating frames between the inputted sketches and then running Text-to-Video Zero using the new interpolated frames video as the control technique, we leverage the benefits of both zero-shot text-to-video generation and the robust control provided by ControlNet. Experiments demonstrate that our method excels at producing high-quality and remarkably consistent video content that more accurately aligns with the user's intended motion for the subject within the video. We",
    "link": "http://arxiv.org/abs/2305.05845",
    "context": "Title: Sketching the Future (STF): Applying Conditional Control Techniques to Text-to-Video Models. (arXiv:2305.05845v1 [cs.CV])\nAbstract: The proliferation of video content demands efficient and flexible neural network based approaches for generating new video content. In this paper, we propose a novel approach that combines zero-shot text-to-video generation with ControlNet to improve the output of these models. Our method takes multiple sketched frames as input and generates video output that matches the flow of these frames, building upon the Text-to-Video Zero architecture and incorporating ControlNet to enable additional input conditions. By first interpolating frames between the inputted sketches and then running Text-to-Video Zero using the new interpolated frames video as the control technique, we leverage the benefits of both zero-shot text-to-video generation and the robust control provided by ControlNet. Experiments demonstrate that our method excels at producing high-quality and remarkably consistent video content that more accurately aligns with the user's intended motion for the subject within the video. We",
    "path": "papers/23/05/2305.05845.json",
    "total_tokens": 836,
    "translated_title": "画出未来：将条件控制技术应用于文本到视频模型中",
    "translated_abstract": "视频内容的爆炸式增长对于生成新的视频内容需要高效灵活的神经网络方法。本文提出了一种新颖的方法，将零样本文本到视频生成与ControlNet相结合，从而改善模型的输出。我们的方法将多个草图帧作为输入，并生成符合这些帧流程的视频输出，基于文本到视频零架构，结合ControlNet以启用额外的输入条件。通过首先插值输入草图之间的帧，然后使用新的插值帧视频作为控制技术来运行文本到视频零，我们利用了零样本文本到视频生成和ControlNet提供的强大控制的优势。实验证明，我们的方法擅长生成高质量，非常一致的视频内容，更准确地与用户对视频中主体运动的意图相吻合。",
    "tldr": "本文提出一种将零样本文本到视频生成与ControlNet相结合的新颖方法，利用插值帧视频作为控制技术生成高质量且一致的视频内容，更准确地符合用户对视频中主体运动的意图。",
    "en_tdlr": "This paper proposes a novel approach that combines zero-shot text-to-video generation with ControlNet to generate high-quality and consistently accurate video content that accurately aligns with the user's intended motion for the subject within the video."
}