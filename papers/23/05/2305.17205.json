{
    "title": "Ghost Noise for Regularizing Deep Neural Networks. (arXiv:2305.17205v1 [cs.LG])",
    "abstract": "Batch Normalization (BN) is widely used to stabilize the optimization process and improve the test performance of deep neural networks. The regularization effect of BN depends on the batch size and explicitly using smaller batch sizes with Batch Normalization, a method known as Ghost Batch Normalization (GBN), has been found to improve generalization in many settings. We investigate the effectiveness of GBN by disentangling the induced \"Ghost Noise\" from normalization and quantitatively analyzing the distribution of noise as well as its impact on model performance. Inspired by our analysis, we propose a new regularization technique called Ghost Noise Injection (GNI) that imitates the noise in GBN without incurring the detrimental train-test discrepancy effects of small batch training. We experimentally show that GNI can provide a greater generalization benefit than GBN. Ghost Noise Injection can also be beneficial in otherwise non-noisy settings such as layer-normalized networks, provi",
    "link": "http://arxiv.org/abs/2305.17205",
    "context": "Title: Ghost Noise for Regularizing Deep Neural Networks. (arXiv:2305.17205v1 [cs.LG])\nAbstract: Batch Normalization (BN) is widely used to stabilize the optimization process and improve the test performance of deep neural networks. The regularization effect of BN depends on the batch size and explicitly using smaller batch sizes with Batch Normalization, a method known as Ghost Batch Normalization (GBN), has been found to improve generalization in many settings. We investigate the effectiveness of GBN by disentangling the induced \"Ghost Noise\" from normalization and quantitatively analyzing the distribution of noise as well as its impact on model performance. Inspired by our analysis, we propose a new regularization technique called Ghost Noise Injection (GNI) that imitates the noise in GBN without incurring the detrimental train-test discrepancy effects of small batch training. We experimentally show that GNI can provide a greater generalization benefit than GBN. Ghost Noise Injection can also be beneficial in otherwise non-noisy settings such as layer-normalized networks, provi",
    "path": "papers/23/05/2305.17205.json",
    "total_tokens": 938,
    "translated_title": "正则化深度神经网络的幽灵噪声",
    "translated_abstract": "批量归一化（BN）被广泛用于稳定深度神经网络的优化过程并提高测试性能。BN的正则化效果取决于批量大小，显式使用较小的批量大小会通过批量归一化提高泛化性，在许多设置中都具有很好的应用。本文通过将引入的“幽灵噪声”与归一化进行分离，并定量分析噪声的分布及其对模型性能的影响来研究GBN的有效性。受我们分析的启发，我们提出了一种新的正则化技术称为Ghost Noise Injection (GNI)，模仿了GBN中的噪声，而避免了小批量训练带来的有害的训练-测试差异效应。实验证明GNI可以比GBN提供更好的泛化效益。Ghost Noise Injection在其他非噪声设置中，例如层归一化网络也能够产生积极作用。",
    "tldr": "本文研究了幽灵批量归一化（GBN）中的“幽灵噪声”，提出了一种新的正则化技术Ghost Noise Injection (GNI)，该方法能够避免小批量训练带来的训练-测试差异效应，并在深度神经网络中提供更好的泛化效果。",
    "en_tdlr": "This paper investigates the \"Ghost Noise\" in Ghost Batch Normalization (GBN) and proposes a new regularization technique called Ghost Noise Injection (GNI) that imitates the noise in GBN without incurring the detrimental train-test discrepancy effects of small batch training. GNI can provide a greater generalization benefit than GBN in deep neural networks."
}