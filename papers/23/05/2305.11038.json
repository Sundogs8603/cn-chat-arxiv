{
    "title": "Learning In-context Learning for Named Entity Recognition. (arXiv:2305.11038v1 [cs.CL])",
    "abstract": "Named entity recognition in real-world applications suffers from the diversity of entity types, the emergence of new entity types, and the lack of high-quality annotations. To address the above problems, this paper proposes an in-context learning-based NER approach, which can effectively inject in-context NER ability into PLMs and recognize entities of novel types on-the-fly using only a few demonstrative instances. Specifically, we model PLMs as a meta-function $\\mathcal{ \\lambda_ {\\text{instruction, demonstrations, text}}. M}$, and a new entity extractor can be implicitly constructed by applying new instruction and demonstrations to PLMs, i.e., $\\mathcal{ (\\lambda . M) }$(instruction, demonstrations) $\\to$ $\\mathcal{F}$ where $\\mathcal{F}$ will be a new entity extractor, i.e., $\\mathcal{F}$: text $\\to$ entities. To inject the above in-context NER ability into PLMs, we propose a meta-function pre-training algorithm, which pre-trains PLMs by comparing the (instruction, demonstration)-i",
    "link": "http://arxiv.org/abs/2305.11038",
    "context": "Title: Learning In-context Learning for Named Entity Recognition. (arXiv:2305.11038v1 [cs.CL])\nAbstract: Named entity recognition in real-world applications suffers from the diversity of entity types, the emergence of new entity types, and the lack of high-quality annotations. To address the above problems, this paper proposes an in-context learning-based NER approach, which can effectively inject in-context NER ability into PLMs and recognize entities of novel types on-the-fly using only a few demonstrative instances. Specifically, we model PLMs as a meta-function $\\mathcal{ \\lambda_ {\\text{instruction, demonstrations, text}}. M}$, and a new entity extractor can be implicitly constructed by applying new instruction and demonstrations to PLMs, i.e., $\\mathcal{ (\\lambda . M) }$(instruction, demonstrations) $\\to$ $\\mathcal{F}$ where $\\mathcal{F}$ will be a new entity extractor, i.e., $\\mathcal{F}$: text $\\to$ entities. To inject the above in-context NER ability into PLMs, we propose a meta-function pre-training algorithm, which pre-trains PLMs by comparing the (instruction, demonstration)-i",
    "path": "papers/23/05/2305.11038.json",
    "total_tokens": 1020,
    "translated_title": "基于上下文学习的命名实体识别",
    "translated_abstract": "现实世界中的命名实体识别受到实体类型的多样性、新实体类型的出现和高质量标注的缺乏等问题的影响。本文提出了一种基于上下文学习的命名实体识别方法，能够将上下文NER能力有效地注入到PLMs中，并且只使用少量示意实例就能动态识别新类型的实体。具体而言，我们将PLMs建模为一个元函数 $\\mathcal{ \\lambda_ {\\text{instruction, demonstrations, text}}. M}$，并通过将新的指示和示例应用于PLMs来隐含地构建新的实体提取器，即 $\\mathcal{ (\\lambda . M) }$(instruction, demonstrations) $\\to$ $\\mathcal{F}$，其中 $\\mathcal{F}$ 将成为一个新的实体提取器，即 $\\mathcal{F}$: text $\\to$ entities。为了将上述上下文NER能力注入PLMs，我们提出了一种元函数预训练算法，该算法通过比较（指示、示例）-identity和（掩盖后的指示、示例）-identity来预训练PLMs。实验结果表明，我们的方法在几个基准数据集上达到了最先进的性能，并且能够使用少量示意实例有效地识别新类型的实体。",
    "tldr": "本文提出了一种在 PLMs 中注入上下文 NER 能力的方法，只需少量示意实例即可动态识别新类型的实体，在几个基准数据集上达到了最先进的性能。",
    "en_tdlr": "This paper proposes an approach to inject in-context NER ability into PLMs, which enables dynamic recognition of new entity types using only a few demonstrative instances, and achieves state-of-the-art performance on several benchmark datasets."
}