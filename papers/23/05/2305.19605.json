{
    "title": "Parameter-free projected gradient descent. (arXiv:2305.19605v1 [stat.ML])",
    "abstract": "We consider the problem of minimizing a convex function over a closed convex set, with Projected Gradient Descent (PGD). We propose a fully parameter-free version of AdaGrad, which is adaptive to the distance between the initialization and the optimum, and to the sum of the square norm of the subgradients. Our algorithm is able to handle projection steps, does not involve restarts, reweighing along the trajectory or additional gradient evaluations compared to the classical PGD. It also fulfills optimal rates of convergence for cumulative regret up to logarithmic factors. We provide an extension of our approach to stochastic optimization and conduct numerical experiments supporting the developed theory.",
    "link": "http://arxiv.org/abs/2305.19605",
    "context": "Title: Parameter-free projected gradient descent. (arXiv:2305.19605v1 [stat.ML])\nAbstract: We consider the problem of minimizing a convex function over a closed convex set, with Projected Gradient Descent (PGD). We propose a fully parameter-free version of AdaGrad, which is adaptive to the distance between the initialization and the optimum, and to the sum of the square norm of the subgradients. Our algorithm is able to handle projection steps, does not involve restarts, reweighing along the trajectory or additional gradient evaluations compared to the classical PGD. It also fulfills optimal rates of convergence for cumulative regret up to logarithmic factors. We provide an extension of our approach to stochastic optimization and conduct numerical experiments supporting the developed theory.",
    "path": "papers/23/05/2305.19605.json",
    "total_tokens": 806,
    "translated_title": "无参数投影梯度下降",
    "translated_abstract": "本文探讨了在闭合凸集上最小化凸函数的问题，使用投影梯度下降（PGD）进行求解。我们提出了完全无参数版本的AdaGrad，该方法能够自适应地处理初始点与最优解之间的距离以及子梯度的平方和。与经典的PGD相比，我们的算法能够处理投影步骤，无需重新启动、轨迹加权或额外的梯度评估。同时，它还满足了在对数因子下的累积遗憾的最优收敛速率。我们还提供了将该方法扩展到随机优化的方法，并进行了支持所开发的理论的数值实验。",
    "tldr": "本文提出了一种无参数的投影梯度下降算法，能够自适应地处理初始点与最优解之间的距离以及子梯度的平方和，适用于在闭合凸集上最小化凸函数的问题，并且能够在满足对数因子下的累积遗憾的最优收敛速率。",
    "en_tdlr": "This paper proposes a parameter-free projected gradient descent algorithm, which is adaptive to the distance between the initialization and the optimum, and to the sum of the square norm of the subgradients. It is suitable for minimizing a convex function over a closed convex set and achieves the optimal convergence rate for cumulative regret up to logarithmic factors. The algorithm also has an extension to stochastic optimization and is supported by numerical experiments."
}