{
    "title": "Self-Supervised Reinforcement Learning that Transfers using Random Features. (arXiv:2305.17250v1 [cs.LG])",
    "abstract": "Model-free reinforcement learning algorithms have exhibited great potential in solving single-task sequential decision-making problems with high-dimensional observations and long horizons, but are known to be hard to generalize across tasks. Model-based RL, on the other hand, learns task-agnostic models of the world that naturally enables transfer across different reward functions, but struggles to scale to complex environments due to the compounding error. To get the best of both worlds, we propose a self-supervised reinforcement learning method that enables the transfer of behaviors across tasks with different rewards, while circumventing the challenges of model-based RL. In particular, we show self-supervised pre-training of model-free reinforcement learning with a number of random features as rewards allows implicit modeling of long-horizon environment dynamics. Then, planning techniques like model-predictive control using these implicit models enable fast adaptation to problems wi",
    "link": "http://arxiv.org/abs/2305.17250",
    "context": "Title: Self-Supervised Reinforcement Learning that Transfers using Random Features. (arXiv:2305.17250v1 [cs.LG])\nAbstract: Model-free reinforcement learning algorithms have exhibited great potential in solving single-task sequential decision-making problems with high-dimensional observations and long horizons, but are known to be hard to generalize across tasks. Model-based RL, on the other hand, learns task-agnostic models of the world that naturally enables transfer across different reward functions, but struggles to scale to complex environments due to the compounding error. To get the best of both worlds, we propose a self-supervised reinforcement learning method that enables the transfer of behaviors across tasks with different rewards, while circumventing the challenges of model-based RL. In particular, we show self-supervised pre-training of model-free reinforcement learning with a number of random features as rewards allows implicit modeling of long-horizon environment dynamics. Then, planning techniques like model-predictive control using these implicit models enable fast adaptation to problems wi",
    "path": "papers/23/05/2305.17250.json",
    "total_tokens": 986,
    "translated_title": "基于随机特征的自监督增强学习实现迁移",
    "translated_abstract": "无模型强化学习算法在解决具有高维观测和长期决策方案的单任务顺序决策问题方面表现出巨大潜力，但难以横跨任务进行泛化。相比之下，有模型强化学习则学习与任务无关的世界模型，自然地实现了不同奖励函数的迁移，但由于误差的累积而难以适应复杂的环境。为了实现两者兼顾，我们提出了一种自监督增强学习方法，能够实现在具有不同奖励的任务间进行行为迁移，同时避开有模型强化学习的挑战。特别地，我们展示了模型自由强化学习的自监督预训练，用一些随机特征作为奖励，能够暗含长期环境动态模型。然后，使用这些隐式模型的规划技术（如模型预测控制）能够在短时间内适应问题。",
    "tldr": "该论文提出了一种自监督增强学习方法，能够在不同奖励的任务间进行行为迁移，同时避免有模型强化学习的挑战。使用一些随机特征作为奖励，进行自监督预训练能够暗含长期环境动态模型，然后使用这些隐式模型的规划技术能够在短时间内适应问题。"
}