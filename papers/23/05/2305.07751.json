{
    "title": "Private and Communication-Efficient Algorithms for Entropy Estimation. (arXiv:2305.07751v1 [cs.LG])",
    "abstract": "Modern statistical estimation is often performed in a distributed setting where each sample belongs to a single user who shares their data with a central server. Users are typically concerned with preserving the privacy of their samples, and also with minimizing the amount of data they must transmit to the server. We give improved private and communication-efficient algorithms for estimating several popular measures of the entropy of a distribution. All of our algorithms have constant communication cost and satisfy local differential privacy. For a joint distribution over many variables whose conditional independence is given by a tree, we describe algorithms for estimating Shannon entropy that require a number of samples that is linear in the number of variables, compared to the quadratic sample complexity of prior work. We also describe an algorithm for estimating Gini entropy whose sample complexity has no dependence on the support size of the distribution and can be implemented usi",
    "link": "http://arxiv.org/abs/2305.07751",
    "context": "Title: Private and Communication-Efficient Algorithms for Entropy Estimation. (arXiv:2305.07751v1 [cs.LG])\nAbstract: Modern statistical estimation is often performed in a distributed setting where each sample belongs to a single user who shares their data with a central server. Users are typically concerned with preserving the privacy of their samples, and also with minimizing the amount of data they must transmit to the server. We give improved private and communication-efficient algorithms for estimating several popular measures of the entropy of a distribution. All of our algorithms have constant communication cost and satisfy local differential privacy. For a joint distribution over many variables whose conditional independence is given by a tree, we describe algorithms for estimating Shannon entropy that require a number of samples that is linear in the number of variables, compared to the quadratic sample complexity of prior work. We also describe an algorithm for estimating Gini entropy whose sample complexity has no dependence on the support size of the distribution and can be implemented usi",
    "path": "papers/23/05/2305.07751.json",
    "total_tokens": 813,
    "translated_title": "用于熵估计的私有且通信高效的算法",
    "translated_abstract": "现代统计估计通常在分布式环境中进行，其中每个样本属于单个用户，该用户与中央服务器共享其数据。用户通常关心保护其样本的隐私，并尽量减少其必须向服务器传输的数据量。我们提供了改进的私有和通信高效算法，用于估计分布熵的几个流行度量。我们所有的算法通信成本固定且符合局部差分隐私。对于由树给出条件独立性的多变量联合分布，我们描述了估计Shannon熵的算法，其样本数与变量数量呈线性关系，而先前工作的样本复杂度为二次。我们还描述了一种用于估计Gini熵的算法，其样本复杂度没有依赖于分布支持大小，并且可以使用noteq",
    "tldr": "本文提出了用于熵估计的私有且通信高效的算法，对于多变量联合分布，其样本数与变量数量呈线性关系；算法可以在保证隐私的同时最小化通信成本。",
    "en_tdlr": "This paper proposes private and communication-efficient algorithms for entropy estimation. For joint distribution over many variables, the sample number needed is linear to the number of variables. The algorithms minimize the communication cost while ensuring in local differential privacy."
}