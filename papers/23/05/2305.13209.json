{
    "title": "Faster Differentially Private Convex Optimization via Second-Order Methods. (arXiv:2305.13209v1 [cs.LG])",
    "abstract": "Differentially private (stochastic) gradient descent is the workhorse of DP private machine learning in both the convex and non-convex settings. Without privacy constraints, second-order methods, like Newton's method, converge faster than first-order methods like gradient descent. In this work, we investigate the prospect of using the second-order information from the loss function to accelerate DP convex optimization. We first develop a private variant of the regularized cubic Newton method of Nesterov and Polyak, and show that for the class of strongly convex loss functions, our algorithm has quadratic convergence and achieves the optimal excess loss. We then design a practical second-order DP algorithm for the unconstrained logistic regression problem. We theoretically and empirically study the performance of our algorithm. Empirical results show our algorithm consistently achieves the best excess loss compared to other baselines and is 10-40x faster than DP-GD/DP-SGD.",
    "link": "http://arxiv.org/abs/2305.13209",
    "context": "Title: Faster Differentially Private Convex Optimization via Second-Order Methods. (arXiv:2305.13209v1 [cs.LG])\nAbstract: Differentially private (stochastic) gradient descent is the workhorse of DP private machine learning in both the convex and non-convex settings. Without privacy constraints, second-order methods, like Newton's method, converge faster than first-order methods like gradient descent. In this work, we investigate the prospect of using the second-order information from the loss function to accelerate DP convex optimization. We first develop a private variant of the regularized cubic Newton method of Nesterov and Polyak, and show that for the class of strongly convex loss functions, our algorithm has quadratic convergence and achieves the optimal excess loss. We then design a practical second-order DP algorithm for the unconstrained logistic regression problem. We theoretically and empirically study the performance of our algorithm. Empirical results show our algorithm consistently achieves the best excess loss compared to other baselines and is 10-40x faster than DP-GD/DP-SGD.",
    "path": "papers/23/05/2305.13209.json",
    "total_tokens": 905,
    "translated_title": "通过二阶方法实现更快的差分隐私凸优化",
    "translated_abstract": "差分隐私随机梯度下降是差分隐私机器学习的主要方法，在凸和非凸的情况下均可使用。如果没有隐私约束，像牛顿法这样的二阶方法比梯度下降这样的一阶方法更快地收敛。本文研究使用损失函数的二阶信息加速差分隐私凸优化的可能性。我们首先使用Nesterov和Polyak的正则化三次牛顿法开发了一种私有方法，并证明对于强凸损失函数类，我们的算法具有二次收敛速度，并实现了最优的超额损失。然后，我们为无约束逻辑回归问题设计了一个实用的二阶差分隐私算法。我们从理论和实证两方面研究了我们算法的性能，实证结果显示，与其他基线比较，我们的算法始终实现了最佳超额损失，比DP-GD/DP-SGD快10-40倍。",
    "tldr": "本文研究使用二阶信息加速差分隐私凸优化，在强凸损失函数类中提出了一种具有二次收敛速度和最优超额损失的算法。",
    "en_tdlr": "This paper investigates the acceleration of differentially private convex optimization using second-order information, and proposes an algorithm with quadratic convergence and optimal excess loss for the class of strongly convex loss functions. Empirical results show that the proposed algorithm consistently achieves the best excess loss compared to other baselines and is 10-40x faster than DP-GD/DP-SGD."
}