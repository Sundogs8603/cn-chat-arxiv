{
    "title": "Two to Five Truths in Non-Negative Matrix Factorization. (arXiv:2305.05389v1 [cs.LG])",
    "abstract": "In this paper, we explore the role of matrix scaling on a matrix of counts when building a topic model using non-negative matrix factorization. We present a scaling inspired by the normalized Laplacian (NL) for graphs that can greatly improve the quality of a non-negative matrix factorization. The results parallel those in the spectral graph clustering work of \\cite{Priebe:2019}, where the authors proved adjacency spectral embedding (ASE) spectral clustering was more likely to discover core-periphery partitions and Laplacian Spectral Embedding (LSE) was more likely to discover affinity partitions. In text analysis non-negative matrix factorization (NMF) is typically used on a matrix of co-occurrence ``contexts'' and ``terms\" counts. The matrix scaling inspired by LSE gives significant improvement for text topic models in a variety of datasets. We illustrate the dramatic difference a matrix scalings in NMF can greatly improve the quality of a topic model on three datasets where human an",
    "link": "http://arxiv.org/abs/2305.05389",
    "context": "Title: Two to Five Truths in Non-Negative Matrix Factorization. (arXiv:2305.05389v1 [cs.LG])\nAbstract: In this paper, we explore the role of matrix scaling on a matrix of counts when building a topic model using non-negative matrix factorization. We present a scaling inspired by the normalized Laplacian (NL) for graphs that can greatly improve the quality of a non-negative matrix factorization. The results parallel those in the spectral graph clustering work of \\cite{Priebe:2019}, where the authors proved adjacency spectral embedding (ASE) spectral clustering was more likely to discover core-periphery partitions and Laplacian Spectral Embedding (LSE) was more likely to discover affinity partitions. In text analysis non-negative matrix factorization (NMF) is typically used on a matrix of co-occurrence ``contexts'' and ``terms\" counts. The matrix scaling inspired by LSE gives significant improvement for text topic models in a variety of datasets. We illustrate the dramatic difference a matrix scalings in NMF can greatly improve the quality of a topic model on three datasets where human an",
    "path": "papers/23/05/2305.05389.json",
    "total_tokens": 815,
    "translated_title": "非负矩阵分解中的两到五个真相",
    "translated_abstract": "本文探讨了在使用非负矩阵分解构建主题模型时，矩阵缩放在计数矩阵上的作用。我们提出了一种受规范化拉普拉斯图（NL） 的启发的矩阵缩放方法，可以大大提高非负矩阵分解的质量。在文本分析中，非负矩阵分解 (NMF) 通常用于计数矩阵的共现“上下文”和“术语”。受 LSE 的启发，矩阵缩放对各种数据集中的文本主题模型都有显着的改进。我们在三个数据集上展示了矩阵缩放在 NMF 中的巨大影响，可以大大改善主题模型的质量。",
    "tldr": "本文提出了一种受规范化拉普拉斯图的启发的矩阵缩放方法，可以大大提高非负矩阵分解在文本主题模型中的质量。",
    "en_tdlr": "This paper proposes a matrix scaling method inspired by the normalized Laplacian, which greatly improves the quality of non-negative matrix factorization in text topic modeling."
}