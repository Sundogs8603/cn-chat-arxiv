{
    "title": "EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought. (arXiv:2305.15021v2 [cs.RO] UPDATED)",
    "abstract": "Embodied AI is a crucial frontier in robotics, capable of planning and executing action sequences for robots to accomplish long-horizon tasks in physical environments. In this work, we introduce EmbodiedGPT, an end-to-end multi-modal foundation model for embodied AI, empowering embodied agents with multi-modal understanding and execution capabilities. To achieve this, we have made the following efforts: (i) We craft a large-scale embodied planning dataset, termed EgoCOT. The dataset consists of carefully selected videos from the Ego4D dataset, along with corresponding high-quality language instructions. Specifically, we generate a sequence of sub-goals with the \"Chain of Thoughts\" mode for effective embodied planning. (ii) We introduce an efficient training approach to EmbodiedGPT for high-quality plan generation, by adapting a 7B large language model (LLM) to the EgoCOT dataset via prefix tuning. (iii) We introduce a paradigm for extracting task-related features from LLM-generated pla",
    "link": "http://arxiv.org/abs/2305.15021",
    "context": "Title: EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought. (arXiv:2305.15021v2 [cs.RO] UPDATED)\nAbstract: Embodied AI is a crucial frontier in robotics, capable of planning and executing action sequences for robots to accomplish long-horizon tasks in physical environments. In this work, we introduce EmbodiedGPT, an end-to-end multi-modal foundation model for embodied AI, empowering embodied agents with multi-modal understanding and execution capabilities. To achieve this, we have made the following efforts: (i) We craft a large-scale embodied planning dataset, termed EgoCOT. The dataset consists of carefully selected videos from the Ego4D dataset, along with corresponding high-quality language instructions. Specifically, we generate a sequence of sub-goals with the \"Chain of Thoughts\" mode for effective embodied planning. (ii) We introduce an efficient training approach to EmbodiedGPT for high-quality plan generation, by adapting a 7B large language model (LLM) to the EgoCOT dataset via prefix tuning. (iii) We introduce a paradigm for extracting task-related features from LLM-generated pla",
    "path": "papers/23/05/2305.15021.json",
    "total_tokens": 840,
    "translated_title": "EmbodiedGPT: 通过思维链预训练的视觉语言模型",
    "translated_abstract": "赋予具有多模态理解和执行能力的实体代理人，我们介绍了一种端到端的多模态基础模型EmbodiedGPT，用于实体智能。为了实现这一目标，我们做出了以下努力：（i）我们创建了一个大规模的嵌入式规划数据集，命名为EgoCOT，该数据集由Ego4D数据集中精选的视频和相应的高质量语言指令组成。具体而言，我们使用“思维链”模式生成一系列子目标，以实现有效的嵌入式规划。（ii）我们推出了一种高质量计划生成的高效训练方法，通过前缀调优将7B大型语言模型（LLM）调整到EgoCOT数据集上。（iii）我们介绍了一种从LLM生成的计划中提取与任务相关特征的范例方法。",
    "tldr": "EmbodiedGPT是一种端到端的多模态基础模型，通过思维链预训练的方式，赋予具有多模态理解和执行能力的实体代理人。"
}