{
    "title": "Rapid Adaptation in Online Continual Learning: Are We Evaluating It Right?. (arXiv:2305.09275v1 [cs.LG])",
    "abstract": "We revisit the common practice of evaluating adaptation of Online Continual Learning (OCL) algorithms through the metric of online accuracy, which measures the accuracy of the model on the immediate next few samples. However, we show that this metric is unreliable, as even vacuous blind classifiers, which do not use input images for prediction, can achieve unrealistically high online accuracy by exploiting spurious label correlations in the data stream. Our study reveals that existing OCL algorithms can also achieve high online accuracy, but perform poorly in retaining useful information, suggesting that they unintentionally learn spurious label correlations. To address this issue, we propose a novel metric for measuring adaptation based on the accuracy on the near-future samples, where spurious correlations are removed. We benchmark existing OCL approaches using our proposed metric on large-scale datasets under various computational budgets and find that better generalization can be a",
    "link": "http://arxiv.org/abs/2305.09275",
    "context": "Title: Rapid Adaptation in Online Continual Learning: Are We Evaluating It Right?. (arXiv:2305.09275v1 [cs.LG])\nAbstract: We revisit the common practice of evaluating adaptation of Online Continual Learning (OCL) algorithms through the metric of online accuracy, which measures the accuracy of the model on the immediate next few samples. However, we show that this metric is unreliable, as even vacuous blind classifiers, which do not use input images for prediction, can achieve unrealistically high online accuracy by exploiting spurious label correlations in the data stream. Our study reveals that existing OCL algorithms can also achieve high online accuracy, but perform poorly in retaining useful information, suggesting that they unintentionally learn spurious label correlations. To address this issue, we propose a novel metric for measuring adaptation based on the accuracy on the near-future samples, where spurious correlations are removed. We benchmark existing OCL approaches using our proposed metric on large-scale datasets under various computational budgets and find that better generalization can be a",
    "path": "papers/23/05/2305.09275.json",
    "total_tokens": 926,
    "translated_title": "在线持续学习中的快速适应性：我们评估得对吗？",
    "translated_abstract": "本文重新审视了在线持续学习算法适应性的常见评估方法——在线准确率度量。该度量衡量模型在接下来的几个样本上的准确性。然而，我们证明了这个度量是不可靠的。即使是空泛的盲分类器也能通过利用数据流中表面的标签相关性来实现非常高的在线准确率。我们的研究表明，现有的在线持续学习算法也能够实现高在线准确率，但是表现出较差的信息保留能力，表明它们意外地学习了表面的标签相关性。为解决这个问题，我们提出了一种基于消除表面的相关性的近期样本准确度的新度量来衡量适应性。我们使用我们提出的度量在各种计算预算下对大规模数据集中现有的OCL方法进行了基准测试，并发现更好的泛化能力可以达到。",
    "tldr": "本文重新审视了在线持续学习算法适应性的常见评估方法——在线准确率度量。该度量是不可靠的，现有算法会学习表面的标签相关性，我们提出了一种新的度量来衡量适应性并进行了基准测试。",
    "en_tdlr": "This paper re-examines the common evaluation method of the adaptation of Online Continual Learning (OCL) algorithms - online accuracy metric, and proposes a new metric based on the accuracy on the near-future samples to address the issue of unreliable online accuracy and unintentional learning of spurious label correlations. The proposed metric is used to benchmark existing OCL approaches on large-scale datasets, indicating the importance of better generalization."
}