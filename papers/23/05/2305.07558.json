{
    "title": "Measuring Progress in Fine-grained Vision-and-Language Understanding. (arXiv:2305.07558v1 [cs.CL])",
    "abstract": "While pretraining on large-scale image-text data from the Web has facilitated rapid progress on many vision-and-language (V&L) tasks, recent work has demonstrated that pretrained models lack \"fine-grained\" understanding, such as the ability to recognise relationships, verbs, and numbers in images. This has resulted in an increased interest in the community to either develop new benchmarks or models for such capabilities. To better understand and quantify progress in this direction, we investigate four competitive V&L models on four fine-grained benchmarks. Through our analysis, we find that X-VLM (Zeng et al., 2022) consistently outperforms other baselines, and that modelling innovations can impact performance more than scaling Web data, which even degrades performance sometimes. Through a deeper investigation of X-VLM, we highlight the importance of both novel losses and rich data sources for learning fine-grained skills. Finally, we inspect training dynamics, and discover that for so",
    "link": "http://arxiv.org/abs/2305.07558",
    "context": "Title: Measuring Progress in Fine-grained Vision-and-Language Understanding. (arXiv:2305.07558v1 [cs.CL])\nAbstract: While pretraining on large-scale image-text data from the Web has facilitated rapid progress on many vision-and-language (V&L) tasks, recent work has demonstrated that pretrained models lack \"fine-grained\" understanding, such as the ability to recognise relationships, verbs, and numbers in images. This has resulted in an increased interest in the community to either develop new benchmarks or models for such capabilities. To better understand and quantify progress in this direction, we investigate four competitive V&L models on four fine-grained benchmarks. Through our analysis, we find that X-VLM (Zeng et al., 2022) consistently outperforms other baselines, and that modelling innovations can impact performance more than scaling Web data, which even degrades performance sometimes. Through a deeper investigation of X-VLM, we highlight the importance of both novel losses and rich data sources for learning fine-grained skills. Finally, we inspect training dynamics, and discover that for so",
    "path": "papers/23/05/2305.07558.json",
    "total_tokens": 1139,
    "translated_title": "在细粒度视觉语言理解方面的进展量化测量",
    "translated_abstract": "虽然利用来自Web的大规模图像文本数据进行预训练已经促进了视觉语言（V＆L）任务的快速进展，但最近的研究表明预训练模型缺乏“细粒度”理解，例如在图像中识别关系、动词和数字的能力。因此，社区对开发新的基准或模型来实现这种能力的兴趣增加。为了更好地了解和量化在这个方向上的进展，我们在四个精细的基准任务上研究了四个竞争的V＆L模型。通过我们的分析，我们发现X-VLM（Zeng等人，2022）始终优于其他基线，并且模型创新可以比扩展Web数据对表现产生更大的影响，有时甚至会降低表现。通过对X-VLM的深入研究，我们强调了新颖的损失和丰富的数据源对学习细粒度技能的重要性。最后，我们检查了训练动态，发现对于某些任务（例如动词预测），模型可以在不学习感知视觉对象的情况下实现强大的性能，表明这些模型中可能存在视觉和语言之间的潜在分离。",
    "tldr": "本文旨在量化测量细粒度视觉语言理解方面的进展，探究了四个竞争的V&L模型在四个精细基准任务上的表现。研究发现X-VLM模型始终优于其他模型，并强调了新颖的损失和丰富的数据源对于学习细粒度技能的重要性。同时，该研究还揭示了部分模型在某些任务上存在视觉和语言之间的潜在分离。",
    "en_tdlr": "This paper aims to quantify progress in fine-grained vision-and-language understanding, and investigates four competitive V&L models on four fine-grained benchmarks. The study finds that X-VLM model consistently outperforms other models, highlights the importance of novel losses and rich data sources in learning fine-grained skills, and reveals a potential disconnect between vision and language within some models for certain tasks."
}