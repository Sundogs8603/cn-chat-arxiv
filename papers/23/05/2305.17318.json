{
    "title": "Radar Enlighten the Dark: Enhancing Low-Visibility Perception for Automated Vehicles with Camera-Radar Fusion. (arXiv:2305.17318v1 [cs.CV])",
    "abstract": "Sensor fusion is a crucial augmentation technique for improving the accuracy and reliability of perception systems for automated vehicles under diverse driving conditions. However, adverse weather and low-light conditions remain challenging, where sensor performance degrades significantly, exposing vehicle safety to potential risks. Advanced sensors such as LiDARs can help mitigate the issue but with extremely high marginal costs. In this paper, we propose a novel transformer-based 3D object detection model \"REDFormer\" to tackle low visibility conditions, exploiting the power of a more practical and cost-effective solution by leveraging bird's-eye-view camera-radar fusion. Using the nuScenes dataset with multi-radar point clouds, weather information, and time-of-day data, our model outperforms state-of-the-art (SOTA) models on classification and detection accuracy. Finally, we provide extensive ablation studies of each model component on their contributions to address the above-mention",
    "link": "http://arxiv.org/abs/2305.17318",
    "context": "Title: Radar Enlighten the Dark: Enhancing Low-Visibility Perception for Automated Vehicles with Camera-Radar Fusion. (arXiv:2305.17318v1 [cs.CV])\nAbstract: Sensor fusion is a crucial augmentation technique for improving the accuracy and reliability of perception systems for automated vehicles under diverse driving conditions. However, adverse weather and low-light conditions remain challenging, where sensor performance degrades significantly, exposing vehicle safety to potential risks. Advanced sensors such as LiDARs can help mitigate the issue but with extremely high marginal costs. In this paper, we propose a novel transformer-based 3D object detection model \"REDFormer\" to tackle low visibility conditions, exploiting the power of a more practical and cost-effective solution by leveraging bird's-eye-view camera-radar fusion. Using the nuScenes dataset with multi-radar point clouds, weather information, and time-of-day data, our model outperforms state-of-the-art (SOTA) models on classification and detection accuracy. Finally, we provide extensive ablation studies of each model component on their contributions to address the above-mention",
    "path": "papers/23/05/2305.17318.json",
    "total_tokens": 982,
    "translated_title": "雷达照亮黑暗：通过相机-雷达融合增强自动驾驶车辆的低能见度感知",
    "translated_abstract": "传感器融合是一种关键的增强技术，用于在不同的驾驶条件下提高自动驾驶车辆感知系统的准确性和可靠性。然而，恶劣的天气和低光照条件仍然是一个挑战，在这些条件下，传感器性能会显著下降，从而使车辆安全面临潜在风险。本文提出了一种新颖的基于Transformer的3D目标检测模型“REDFormer”，利用鸟瞰相机-雷达融合的便利和经济实用性来解决低能见度问题。在使用多雷达点云、天气信息和时间数据的nuScenes数据集上，我们的模型在分类和检测准确性方面优于最先进的模型。最后，我们对每个模型组件进行了广泛的消融研究，以了解它们对应对上述问题的贡献。",
    "tldr": "针对低能见度的自动驾驶车辆感知问题，本文提出了一种新的基于Transformer的3D目标检测模型“REDFormer”，通过鸟瞰相机-雷达融合进行实现。该模型在nuScenes数据集上表现出优异的分类和检测准确性，且相较于现有模型更经济实用。",
    "en_tdlr": "The paper proposes a novel transformer-based 3D object detection model, REDFormer, to address low-visibility perception for automated vehicles by leveraging bird's-eye-view camera-radar fusion. The model outperforms state-of-the-art models on the nuScenes dataset with multi-radar point clouds, weather information, and time-of-day data. The proposed solution is more practical and cost-effective than traditional methods such as LiDARs."
}