{
    "title": "LLMs as Factual Reasoners: Insights from Existing Benchmarks and Beyond. (arXiv:2305.14540v1 [cs.CL])",
    "abstract": "With the recent appearance of LLMs in practical settings, having methods that can effectively detect factual inconsistencies is crucial to reduce the propagation of misinformation and improve trust in model outputs. When testing on existing factual consistency benchmarks, we find that a few large language models (LLMs) perform competitively on classification benchmarks for factual inconsistency detection compared to traditional non-LLM methods. However, a closer analysis reveals that most LLMs fail on more complex formulations of the task and exposes issues with existing evaluation benchmarks, affecting evaluation precision. To address this, we propose a new protocol for inconsistency detection benchmark creation and implement it in a 10-domain benchmark called SummEdits. This new benchmark is 20 times more cost-effective per sample than previous benchmarks and highly reproducible, as we estimate inter-annotator agreement at about 0.9. Most LLMs struggle on SummEdits, with performance ",
    "link": "http://arxiv.org/abs/2305.14540",
    "context": "Title: LLMs as Factual Reasoners: Insights from Existing Benchmarks and Beyond. (arXiv:2305.14540v1 [cs.CL])\nAbstract: With the recent appearance of LLMs in practical settings, having methods that can effectively detect factual inconsistencies is crucial to reduce the propagation of misinformation and improve trust in model outputs. When testing on existing factual consistency benchmarks, we find that a few large language models (LLMs) perform competitively on classification benchmarks for factual inconsistency detection compared to traditional non-LLM methods. However, a closer analysis reveals that most LLMs fail on more complex formulations of the task and exposes issues with existing evaluation benchmarks, affecting evaluation precision. To address this, we propose a new protocol for inconsistency detection benchmark creation and implement it in a 10-domain benchmark called SummEdits. This new benchmark is 20 times more cost-effective per sample than previous benchmarks and highly reproducible, as we estimate inter-annotator agreement at about 0.9. Most LLMs struggle on SummEdits, with performance ",
    "path": "papers/23/05/2305.14540.json",
    "total_tokens": 944,
    "translated_title": "基于事实的推理：LLMs的洞见与超越",
    "translated_abstract": "随着LLMs在实际场景中的出现，拥有能够有效检测实际不一致性的方法对于减少错误信息传播并提高模型输出的信任度至关重要。通过对现有的实际一致性基准进行测试，我们发现与传统的非LLM方法相比，少数大型语言模型在事实不一致性检测分类基准方面表现竞争力。然而，更详细的分析揭示了大多数LLMs在更复杂的任务方案上失败，并揭示了现有评估基准存在的问题，影响了评估精度。为了解决这个问题，我们提出了一个新的不一致性检测基准协议，并在一个10个领域的基准测试SummEdits中实施。这个新基准测试每个样本比之前的基准测试更具有20倍的成本效益，并且高度可重复，因为我们估计人与人之间的一致性约为0.9。大多数LLMs在SummEdits上都表现不佳，其性能……（原文未完，略）",
    "tldr": "LLMs能够在基本的实际不一致性检测基准测试中表现出竞争力，但大多数LLMs不能成功完成更复杂任务方案的测试。作者提出了一个新的非一致性检测基准测试SummEdits，并表明大多数LLMs并不擅长完成此项测试。",
    "en_tdlr": "LLMs perform competitively on basic factual inconsistency detection benchmarks, but struggle with more complex formulations of the task. The authors propose a new benchmark called SummEdits and show that most LLMs do not perform well on it."
}