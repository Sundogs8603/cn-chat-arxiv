{
    "title": "Surfacing Biases in Large Language Models using Contrastive Input Decoding. (arXiv:2305.07378v1 [cs.CL])",
    "abstract": "Ensuring that large language models (LMs) are fair, robust and useful requires an understanding of how different modifications to their inputs impact the model's behaviour. In the context of open-text generation tasks, however, such an evaluation is not trivial. For example, when introducing a model with an input text and a perturbed, \"contrastive\" version of it, meaningful differences in the next-token predictions may not be revealed with standard decoding strategies. With this motivation in mind, we propose Contrastive Input Decoding (CID): a decoding algorithm to generate text given two inputs, where the generated text is likely given one input but unlikely given the other. In this way, the contrastive generations can highlight potentially subtle differences in how the LM output differs for the two inputs in a simple and interpretable manner. We use CID to highlight context-specific biases that are hard to detect with standard decoding strategies and quantify the effect of different",
    "link": "http://arxiv.org/abs/2305.07378",
    "context": "Title: Surfacing Biases in Large Language Models using Contrastive Input Decoding. (arXiv:2305.07378v1 [cs.CL])\nAbstract: Ensuring that large language models (LMs) are fair, robust and useful requires an understanding of how different modifications to their inputs impact the model's behaviour. In the context of open-text generation tasks, however, such an evaluation is not trivial. For example, when introducing a model with an input text and a perturbed, \"contrastive\" version of it, meaningful differences in the next-token predictions may not be revealed with standard decoding strategies. With this motivation in mind, we propose Contrastive Input Decoding (CID): a decoding algorithm to generate text given two inputs, where the generated text is likely given one input but unlikely given the other. In this way, the contrastive generations can highlight potentially subtle differences in how the LM output differs for the two inputs in a simple and interpretable manner. We use CID to highlight context-specific biases that are hard to detect with standard decoding strategies and quantify the effect of different",
    "path": "papers/23/05/2305.07378.json",
    "total_tokens": 856,
    "translated_title": "使用对比输入解码揭示大型语言模型中的偏见",
    "translated_abstract": "确保大型语言模型（LM）公平、强壮和有用，需要了解对它们输入的不同修改如何影响模型的行为。然而，在开放文本生成任务的背景下，这样的评估并不简单。举例来说，当引入一个带有输入文本和扰动“对照”版本的模型时，标准解码策略可能不能揭示下一个标记预测中的实质差异。因此，我们提出了对比输入解码（CID）：一种解码算法，用于生成给定两个输入的文本，其中生成的文本可能基于一个输入而不可能基于另一个输入。这样，对比生成可以以一种简单且可解释的方式突显出LM对这两个输入的输出差异可能存在的微妙差别。我们使用CID来突显标准解码策略难以检测到的上下文特定偏见，并量化不同因素的影响。",
    "tldr": "本文介绍了一种叫做对比输入解码（CID）的解码算法，用于生成给定两个输入的文本，能够揭示大型语言模型中可能存在的微妙偏见和输出差异。",
    "en_tdlr": "This paper proposes a decoding algorithm called Contrastive Input Decoding (CID) to generate text given two inputs, which can highlight potentially subtle biases and output differences in large language models."
}