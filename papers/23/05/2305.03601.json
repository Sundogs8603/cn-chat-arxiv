{
    "title": "Human Attention-Guided Explainable Artificial Intelligence for Computer Vision Models. (arXiv:2305.03601v1 [cs.CV])",
    "abstract": "We examined whether embedding human attention knowledge into saliency-based explainable AI (XAI) methods for computer vision models could enhance their plausibility and faithfulness. We first developed new gradient-based XAI methods for object detection models to generate object-specific explanations by extending the current methods for image classification models. Interestingly, while these gradient-based methods worked well for explaining image classification models, when being used for explaining object detection models, the resulting saliency maps generally had lower faithfulness than human attention maps when performing the same task. We then developed Human Attention-Guided XAI (HAG-XAI) to learn from human attention how to best combine explanatory information from the models to enhance explanation plausibility by using trainable activation functions and smoothing kernels to maximize XAI saliency map's similarity to human attention maps. While for image classification models, HAG",
    "link": "http://arxiv.org/abs/2305.03601",
    "context": "Title: Human Attention-Guided Explainable Artificial Intelligence for Computer Vision Models. (arXiv:2305.03601v1 [cs.CV])\nAbstract: We examined whether embedding human attention knowledge into saliency-based explainable AI (XAI) methods for computer vision models could enhance their plausibility and faithfulness. We first developed new gradient-based XAI methods for object detection models to generate object-specific explanations by extending the current methods for image classification models. Interestingly, while these gradient-based methods worked well for explaining image classification models, when being used for explaining object detection models, the resulting saliency maps generally had lower faithfulness than human attention maps when performing the same task. We then developed Human Attention-Guided XAI (HAG-XAI) to learn from human attention how to best combine explanatory information from the models to enhance explanation plausibility by using trainable activation functions and smoothing kernels to maximize XAI saliency map's similarity to human attention maps. While for image classification models, HAG",
    "path": "papers/23/05/2305.03601.json",
    "total_tokens": 1079,
    "translated_title": "人类注意力引导的可解释人工智能对计算机视觉模型的影响",
    "translated_abstract": "本研究探讨了将人类注意力知识嵌入基于显著性的可解释人工智能（XAI）方法用于计算机视觉模型中是否可以增强其合理性和准确性。我们首先针对目标检测模型开发了新的基于梯度的XAI方法，通过扩展当前用于图像分类模型的方法生成面向对象的解释。有趣的是，虽然这些基于梯度的方法可以很好地解释图像分类模型，但是当用于解释目标检测模型时，所得到的显著性图通常比执行同一任务的人类注意力图的准确性低。因此，我们开发了人类注意力引导的XAI（HAG-XAI）方法，通过使用可训练的激活函数和平滑核来最大化XAI显著性图与人类注意力图的相似性，从人类注意力学习如何最好地结合模型的解释信息以增强解释的合理性。对于图像分类模型，HAG-XAI和基于梯度的方法表现相似，但是对于目标检测模型，HAG-XAI生成的解释在准确性和合理性方面都显著优于基于梯度的方法。",
    "tldr": "本研究探讨了将人类注意力知识嵌入基于显著性的可解释人工智能方法用于计算机视觉模型中的效果。开发了基于梯度和人类注意力引导的两种解释方法，结果表明对于目标检测模型，人类注意力引导的方法在解释的准确性和合理性方面优于基于梯度的方法。",
    "en_tdlr": "This paper examined the effectiveness of embedding human attention knowledge into saliency-based explainable AI methods for computer vision models. Two types of explanation methods, gradient-based and human attention-guided, were developed and compared. Results showed that for object detection models, the human attention-guided method outperformed the gradient-based method in terms of explanation accuracy and plausibility."
}