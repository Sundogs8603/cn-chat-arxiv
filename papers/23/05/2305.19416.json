{
    "title": "KrADagrad: Kronecker Approximation-Domination Gradient Preconditioned Stochastic Optimization. (arXiv:2305.19416v1 [stat.ML])",
    "abstract": "Second order stochastic optimizers allow parameter update step size and direction to adapt to loss curvature, but have traditionally required too much memory and compute for deep learning. Recently, Shampoo [Gupta et al., 2018] introduced a Kronecker factored preconditioner to reduce these requirements: it is used for large deep models [Anil et al., 2020] and in production [Anil et al., 2022]. However, it takes inverse matrix roots of ill-conditioned matrices. This requires 64-bit precision, imposing strong hardware constraints. In this paper, we propose a novel factorization, Kronecker Approximation-Domination (KrAD). Using KrAD, we update a matrix that directly approximates the inverse empirical Fisher matrix (like full matrix AdaGrad), avoiding inversion and hence 64-bit precision. We then propose KrADagrad$^\\star$, with similar computational costs to Shampoo and the same regret. Synthetic ill-conditioned experiments show improved performance over Shampoo for 32-bit precision, while",
    "link": "http://arxiv.org/abs/2305.19416",
    "context": "Title: KrADagrad: Kronecker Approximation-Domination Gradient Preconditioned Stochastic Optimization. (arXiv:2305.19416v1 [stat.ML])\nAbstract: Second order stochastic optimizers allow parameter update step size and direction to adapt to loss curvature, but have traditionally required too much memory and compute for deep learning. Recently, Shampoo [Gupta et al., 2018] introduced a Kronecker factored preconditioner to reduce these requirements: it is used for large deep models [Anil et al., 2020] and in production [Anil et al., 2022]. However, it takes inverse matrix roots of ill-conditioned matrices. This requires 64-bit precision, imposing strong hardware constraints. In this paper, we propose a novel factorization, Kronecker Approximation-Domination (KrAD). Using KrAD, we update a matrix that directly approximates the inverse empirical Fisher matrix (like full matrix AdaGrad), avoiding inversion and hence 64-bit precision. We then propose KrADagrad$^\\star$, with similar computational costs to Shampoo and the same regret. Synthetic ill-conditioned experiments show improved performance over Shampoo for 32-bit precision, while",
    "path": "papers/23/05/2305.19416.json",
    "total_tokens": 1078,
    "translated_title": "KrADagrad：Kronecker近似-主导梯度预处理随机优化",
    "translated_abstract": "二阶随机优化器允许参数更新步长和方向适应损失曲率，但传统上对于深度学习而言需要太多的内存和计算资源。最近，Shampoo [Gupta et al.，2018]引入了Kronecker分解的预处理方法来减少这些要求： 它用于大型深度模型[Anil et al.，2020]并且在生产中[Anil et al.，2022]。 但是，它需要求解病态矩阵的逆矩阵根。这需要64位精度，会产生强硬件限制。本文中，我们提出了一种新的分解方法，即Kronecker近似-主导（KrAD）。 使用KrAD，我们更新一个矩阵，直接近似逆经验Fisher矩阵（类似于完整矩阵AdaGrad），避免求逆矩阵，因此不需要64位精度。我们随后提出KrADagrad$^\\star$，其计算成本与Shampoo相似并具有相同的后悔值。在32位精度下，合成的病态实验表现优于Shampoo，同时在64位精度下实现了可比较的结果。我们为KrADagrad的收敛性提供了理论分析，并在标准深度学习基准测试中展示了其有效性。",
    "tldr": "本文提出了一种名为KrAD的新的Kronecker分解预处理方法，用于降低深度学习中二阶优化器的内存和计算资源要求。通过KrADagrad方法，避免了64位精度要求，并在32位精度下表现更好。",
    "en_tdlr": "This paper proposes a new Kronecker factorization preconditioning method called KrAD to reduce the memory and computational requirements of second order stochastic optimizers in deep learning. The KrADagrad method avoids the 64-bit precision requirement and performs better in 32-bit precision."
}