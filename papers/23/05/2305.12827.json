{
    "title": "Task Arithmetic in the Tangent Space: Improved Editing of Pre-Trained Models. (arXiv:2305.12827v2 [cs.LG] UPDATED)",
    "abstract": "Task arithmetic has recently emerged as a cost-effective and scalable approach to edit pre-trained models directly in weight space: By adding the fine-tuned weights of different tasks, the model's performance can be improved on these tasks, while negating them leads to task forgetting. Yet, our understanding of the effectiveness of task arithmetic and its underlying principles remains limited. We present a comprehensive study of task arithmetic in vision-language models and show that weight disentanglement is the crucial factor that makes it effective. This property arises during pre-training and manifests when distinct directions in weight space govern separate, localized regions in function space associated with the tasks. Notably, we show that fine-tuning models in their tangent space by linearizing them amplifies weight disentanglement. This leads to substantial performance improvements across multiple task arithmetic benchmarks and diverse models. Building on these findings, we pr",
    "link": "http://arxiv.org/abs/2305.12827",
    "context": "Title: Task Arithmetic in the Tangent Space: Improved Editing of Pre-Trained Models. (arXiv:2305.12827v2 [cs.LG] UPDATED)\nAbstract: Task arithmetic has recently emerged as a cost-effective and scalable approach to edit pre-trained models directly in weight space: By adding the fine-tuned weights of different tasks, the model's performance can be improved on these tasks, while negating them leads to task forgetting. Yet, our understanding of the effectiveness of task arithmetic and its underlying principles remains limited. We present a comprehensive study of task arithmetic in vision-language models and show that weight disentanglement is the crucial factor that makes it effective. This property arises during pre-training and manifests when distinct directions in weight space govern separate, localized regions in function space associated with the tasks. Notably, we show that fine-tuning models in their tangent space by linearizing them amplifies weight disentanglement. This leads to substantial performance improvements across multiple task arithmetic benchmarks and diverse models. Building on these findings, we pr",
    "path": "papers/23/05/2305.12827.json",
    "total_tokens": 1100,
    "translated_title": "切线空间中的任务算术：预训练模型改进的方法",
    "translated_abstract": "最近，任务算术已经成为一种经济高效且可扩展的方法，可以直接在权重空间中编辑预训练模型：通过添加不同任务的微调权重，可以提高模型在这些任务上的性能，而抵消它们则会导致任务遗忘。然而，我们对任务算法的有效性和其基本原理的理解仍然有限。本文在视觉语言模型中对任务算法进行了全面研究，并表明权重分离是使其有效的关键因素。这种属性在预训练期间出现，并在权重空间中的不同方向上产生，在与任务相关的函数空间中治理独立的局部区域时体现。值得注意的是，我们发现通过将模型线性化以在切线空间中微调模型可以放大权重分离。这导致在多个任务算法基准和不同模型上实现了实质性的性能改进。基于这些发现，我们提出了一种简单而有效的基于切线空间中的任务算术技术Tan，其优于现有的最先进方法。我们的方法依赖于一种将任务权重增量投影到切线空间上的新投影，确保编辑的权重保持接近预训练流形。我们的研究为任务算术的工作原理提供了新的见解，并指出权重分离是使其成为可能的基本机制。",
    "tldr": "本文研究了在切线空间中进行任务算术的方法，发现权重分离是其有效的关键因素。我们提出了一种简单而有效的基于切线空间中的任务算术技术Tan，其优于现有的最先进方法。",
    "en_tdlr": "This paper studies the method of performing task arithmetic in the tangent space and identifies weight disentanglement as a crucial factor for its effectiveness. A new, simple and effective task arithmetic technique called Tan is proposed, which outperforms existing state-of-the-art approaches."
}