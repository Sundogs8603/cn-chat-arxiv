{
    "title": "AV-SAM: Segment Anything Model Meets Audio-Visual Localization and Segmentation. (arXiv:2305.01836v1 [cs.CV])",
    "abstract": "Segment Anything Model (SAM) has recently shown its powerful effectiveness in visual segmentation tasks. However, there is less exploration concerning how SAM works on audio-visual tasks, such as visual sound localization and segmentation. In this work, we propose a simple yet effective audio-visual localization and segmentation framework based on the Segment Anything Model, namely AV-SAM, that can generate sounding object masks corresponding to the audio. Specifically, our AV-SAM simply leverages pixel-wise audio-visual fusion across audio features and visual features from the pre-trained image encoder in SAM to aggregate cross-modal representations. Then, the aggregated cross-modal features are fed into the prompt encoder and mask decoder to generate the final audio-visual segmentation masks. We conduct extensive experiments on Flickr-SoundNet and AVSBench datasets. The results demonstrate that the proposed AV-SAM can achieve competitive performance on sounding object localization an",
    "link": "http://arxiv.org/abs/2305.01836",
    "context": "Title: AV-SAM: Segment Anything Model Meets Audio-Visual Localization and Segmentation. (arXiv:2305.01836v1 [cs.CV])\nAbstract: Segment Anything Model (SAM) has recently shown its powerful effectiveness in visual segmentation tasks. However, there is less exploration concerning how SAM works on audio-visual tasks, such as visual sound localization and segmentation. In this work, we propose a simple yet effective audio-visual localization and segmentation framework based on the Segment Anything Model, namely AV-SAM, that can generate sounding object masks corresponding to the audio. Specifically, our AV-SAM simply leverages pixel-wise audio-visual fusion across audio features and visual features from the pre-trained image encoder in SAM to aggregate cross-modal representations. Then, the aggregated cross-modal features are fed into the prompt encoder and mask decoder to generate the final audio-visual segmentation masks. We conduct extensive experiments on Flickr-SoundNet and AVSBench datasets. The results demonstrate that the proposed AV-SAM can achieve competitive performance on sounding object localization an",
    "path": "papers/23/05/2305.01836.json",
    "total_tokens": 905,
    "translated_title": "AV-SAM:音视频定位与分割任务中的万能分割模型",
    "translated_abstract": "万能分割模型（SAM）近来在视觉分割任务中表现出了卓越的效果。但是，在音视频任务中，比如视音频定位与分割，SAM的表现却鲜有实验探索。本文提出了一种简单而有效的基于SAM的音视频定位与分割框架AV-SAM，它可以生成相应于音频的声音对象掩码。具体来说，我们的AV-SAM简单地利用来自SAM中预训练图像编码器的视觉特征和音频特征的逐像素音视频融合来聚合跨模态表征。然后，将聚合后的跨模态特征馈入提示编码器和掩码解码器中生成最终的音视频分割掩码。我们在Flickr-SoundNet和AVSBench数据集上进行了广泛的实验。结果表明，与最先进的方法相比，所提出的AV-SAM在声音对象定位和分割方面可以取得竞争性的性能。",
    "tldr": "本文提出了一个音视频定位与分割的框架AV-SAM。AV-SAM基于SAM模型，能够在Flickr-SoundNet和AVSBench数据集上达到竞争性的性能，在音视频任务中具有广泛的实际应用前景。",
    "en_tdlr": "This paper proposes an AV-SAM framework for audio-visual localization and segmentation, based on the Segment Anything Model (SAM). AV-SAM achieves competitive performance on sounding object localization and segmentation compared to state-of-the-art methods, and has broad practical application prospects in audio-visual tasks."
}