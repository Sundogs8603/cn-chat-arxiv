{
    "title": "RATs-NAS: Redirection of Adjacent Trails on GCN for Neural Architecture Search. (arXiv:2305.04206v1 [cs.CV])",
    "abstract": "Various hand-designed CNN architectures have been developed, such as VGG, ResNet, DenseNet, etc., and achieve State-of-the-Art (SoTA) levels on different tasks. Neural Architecture Search (NAS) now focuses on automatically finding the best CNN architecture to handle the above tasks. However, the verification of a searched architecture is very time-consuming and makes predictor-based methods become an essential and important branch of NAS. Two commonly used techniques to build predictors are graph-convolution networks (GCN) and multilayer perceptron (MLP). In this paper, we consider the difference between GCN and MLP on adjacent operation trails and then propose the Redirected Adjacent Trails NAS (RATs-NAS) to quickly search for the desired neural network architecture. The RATs-NAS consists of two components: the Redirected Adjacent Trails GCN (RATs-GCN) and the Predictor-based Search Space Sampling (P3S) module. RATs-GCN can change trails and their strengths to search for a better neur",
    "link": "http://arxiv.org/abs/2305.04206",
    "context": "Title: RATs-NAS: Redirection of Adjacent Trails on GCN for Neural Architecture Search. (arXiv:2305.04206v1 [cs.CV])\nAbstract: Various hand-designed CNN architectures have been developed, such as VGG, ResNet, DenseNet, etc., and achieve State-of-the-Art (SoTA) levels on different tasks. Neural Architecture Search (NAS) now focuses on automatically finding the best CNN architecture to handle the above tasks. However, the verification of a searched architecture is very time-consuming and makes predictor-based methods become an essential and important branch of NAS. Two commonly used techniques to build predictors are graph-convolution networks (GCN) and multilayer perceptron (MLP). In this paper, we consider the difference between GCN and MLP on adjacent operation trails and then propose the Redirected Adjacent Trails NAS (RATs-NAS) to quickly search for the desired neural network architecture. The RATs-NAS consists of two components: the Redirected Adjacent Trails GCN (RATs-GCN) and the Predictor-based Search Space Sampling (P3S) module. RATs-GCN can change trails and their strengths to search for a better neur",
    "path": "papers/23/05/2305.04206.json",
    "total_tokens": 1041,
    "translated_title": "RATs-NAS：GCN上的相邻操作轨迹重定向用于神经结构搜索",
    "translated_abstract": "许多手工设计的卷积神经网络如VGG、ResNet、DenseNet等，在不同的任务上达到了最先进的水平。神经结构搜索（NAS）现在专注于自动找到最佳CNN架构来处理上述任务。然而，验证搜索架构非常耗时，使基于预测器的方法成为NAS的一个基本而重要的分支。建立预测器的两种常用技术是图卷积网络（GCN）和多层感知器（MLP）。本文考虑GCN和MLP在相邻操作轨迹上的差异，提出了Redirected Adjacent Trails NAS（RATs-NAS），以快速搜索所需的神经网络架构。RATs-NAS包括两个组件：Redirected Adjacent Trails GCN（RATs-GCN）和基于预测器的搜索空间抽样（P3S）模块。 RATs-GCN可以改变轨迹及其强度以搜索更好的神经网络架构，而P3S模块则对搜索空间进行抽样以提高预测架构的精度。实验结果表明，与其他最先进的方法相比，RATs-NAS可以更快地找到具有竞争力的神经网络架构。",
    "tldr": "本论文提出了一种被称为RATs-NAS的神经结构搜索方法，通过在GCN上重定向相邻操作轨迹来快速搜索最佳神经网络架构，实验结果表明这种方法比其他最先进方法更有效。",
    "en_tdlr": "This paper proposes a Neural Architecture Search method called RATs-NAS, which quickly searches for the optimal neural network architecture by redirecting adjacent operation trails on GCN. Experimental results show that this method is more efficient than other state-of-the-art approaches."
}