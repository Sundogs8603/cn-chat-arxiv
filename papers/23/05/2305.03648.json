{
    "title": "On the Effectiveness of Equivariant Regularization for Robust Online Continual Learning. (arXiv:2305.03648v1 [cs.LG])",
    "abstract": "Humans can learn incrementally, whereas neural networks forget previously acquired information catastrophically. Continual Learning (CL) approaches seek to bridge this gap by facilitating the transfer of knowledge to both previous tasks (backward transfer) and future ones (forward transfer) during training.  Recent research has shown that self-supervision can produce versatile models that can generalize well to diverse downstream tasks. However, contrastive self-supervised learning (CSSL), a popular self-supervision technique, has limited effectiveness in online CL (OCL). OCL only permits one iteration of the input dataset, and CSSL's low sample efficiency hinders its use on the input data-stream.  In this work, we propose Continual Learning via Equivariant Regularization (CLER), an OCL approach that leverages equivariant tasks for self-supervision, avoiding CSSL's limitations. Our method represents the first attempt at combining equivariant knowledge with CL and can be easily integrat",
    "link": "http://arxiv.org/abs/2305.03648",
    "context": "Title: On the Effectiveness of Equivariant Regularization for Robust Online Continual Learning. (arXiv:2305.03648v1 [cs.LG])\nAbstract: Humans can learn incrementally, whereas neural networks forget previously acquired information catastrophically. Continual Learning (CL) approaches seek to bridge this gap by facilitating the transfer of knowledge to both previous tasks (backward transfer) and future ones (forward transfer) during training.  Recent research has shown that self-supervision can produce versatile models that can generalize well to diverse downstream tasks. However, contrastive self-supervised learning (CSSL), a popular self-supervision technique, has limited effectiveness in online CL (OCL). OCL only permits one iteration of the input dataset, and CSSL's low sample efficiency hinders its use on the input data-stream.  In this work, we propose Continual Learning via Equivariant Regularization (CLER), an OCL approach that leverages equivariant tasks for self-supervision, avoiding CSSL's limitations. Our method represents the first attempt at combining equivariant knowledge with CL and can be easily integrat",
    "path": "papers/23/05/2305.03648.json",
    "total_tokens": 933,
    "translated_title": "关于等变正则化对于鲁棒在线连续学习的有效性研究",
    "translated_abstract": "人类能够增量学习，而神经网络会在学习新任务时灾难性地忘记以前学到的知识。连续学习方法试图通过在训练过程中促进知识向旧任务（向后转移）和未来任务（向前转移）的传递来弥补这个差距。最近的研究表明，自监督能够产生多才多艺的模型，可以很好地推广到不同的下游任务。然而，对比自监督学习（CSSL）是一种流行的自监督技术，在在线连续学习中的效果有限。OCL只允许输入数据集的一次迭代，而CSSL的低样本效率阻碍了它在输入数据流上的使用。在这项工作中，我们提出了一种名为等变正则化连续学习（CLER）的OCL方法，它利用等变任务进行自监督，避免了CSSL的限制。我们的方法代表了将等变知识与CL相结合的第一次尝试，并且可以很容易地集成到现有的OCL架构中。",
    "tldr": "论文提出了一种名为CLER的OCL方法，它利用等变任务进行自监督，避免了CSSL在OCL中的限制，并与连续学习相结合。",
    "en_tdlr": "The paper proposes a novel approach, CLER, for online continual learning that leverages equivariant tasks for self-supervision, avoiding the limitations of CSSL and enabling it to be integrated with the existing OCL framework."
}