{
    "title": "Evaluating Open-Domain Question Answering in the Era of Large Language Models. (arXiv:2305.06984v1 [cs.CL])",
    "abstract": "Lexical matching remains the de facto evaluation method for open-domain question answering (QA). Unfortunately, lexical matching fails completely when a plausible candidate answer does not appear in the list of gold answers, which is increasingly the case as we shift from extractive to generative models. The recent success of large language models (LLMs) for QA aggravates lexical matching failures since candidate answers become longer, thereby making matching with the gold answers even more challenging. Without accurate evaluation, the true progress in open-domain QA remains unknown. In this paper, we conduct a thorough analysis of various open-domain QA models, including LLMs, by manually evaluating their answers on a subset of NQ-open, a popular benchmark. Our assessments reveal that while the true performance of all models is significantly underestimated, the performance of the InstructGPT (zero-shot) LLM increases by nearly +60%, making it on par with existing top models, and the I",
    "link": "http://arxiv.org/abs/2305.06984",
    "context": "Title: Evaluating Open-Domain Question Answering in the Era of Large Language Models. (arXiv:2305.06984v1 [cs.CL])\nAbstract: Lexical matching remains the de facto evaluation method for open-domain question answering (QA). Unfortunately, lexical matching fails completely when a plausible candidate answer does not appear in the list of gold answers, which is increasingly the case as we shift from extractive to generative models. The recent success of large language models (LLMs) for QA aggravates lexical matching failures since candidate answers become longer, thereby making matching with the gold answers even more challenging. Without accurate evaluation, the true progress in open-domain QA remains unknown. In this paper, we conduct a thorough analysis of various open-domain QA models, including LLMs, by manually evaluating their answers on a subset of NQ-open, a popular benchmark. Our assessments reveal that while the true performance of all models is significantly underestimated, the performance of the InstructGPT (zero-shot) LLM increases by nearly +60%, making it on par with existing top models, and the I",
    "path": "papers/23/05/2305.06984.json",
    "total_tokens": 942,
    "translated_title": "在大语言模型时代评估开放领域问答",
    "translated_abstract": "词汇匹配仍是开放领域问答（QA）的事实评估方法。然而，当一个合理的候选答案未出现在金标准答案列表中时，词汇匹配完全失败，随着我们从抽取模型转向生成模型，这种情况越来越普遍。大语言模型（LLMs）在QA中的最近成功加剧了词汇匹配的失败，因为候选答案变得更长，因此与金标准答案的匹配变得更加具有挑战性。缺乏准确的评估，开放领域QA的真正进展仍然未知。本文通过在NQ-open的一个子集上手动评估各种开放领域QA模型（包括LLMs）的答案进行了彻底分析。我们的评估揭示，尽管所有模型的真实性能被显着低估，但InstructGPT（零-shot）LLM的性能增加了近60％，使其与现有的顶级模型并驾齐驱，而I",
    "tldr": "本文评估了开放领域问答中的大语言模型，发现词汇匹配作为评估方法在这些模型中的作用有限，提出了一种手动评估方法，并发现其中一个零-shot模型的性能大幅度提升。",
    "en_tdlr": "This paper evaluates open-domain question answering models, especially large language models (LLMs), and proposes a manual evaluation method since lexical matching as the traditional evaluation method fails in these models, revealing the inadequate performance estimation and a +60% enhancement of performance in one InstructGPT model."
}