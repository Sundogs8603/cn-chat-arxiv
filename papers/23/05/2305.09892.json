{
    "title": "Clustering-Aware Negative Sampling for Unsupervised Sentence Representation. (arXiv:2305.09892v1 [cs.CL])",
    "abstract": "Contrastive learning has been widely studied in sentence representation learning. However, earlier works mainly focus on the construction of positive examples, while in-batch samples are often simply treated as negative examples. This approach overlooks the importance of selecting appropriate negative examples, potentially leading to a scarcity of hard negatives and the inclusion of false negatives. To address these issues, we propose ClusterNS (Clustering-aware Negative Sampling), a novel method that incorporates cluster information into contrastive learning for unsupervised sentence representation learning. We apply a modified K-means clustering algorithm to supply hard negatives and recognize in-batch false negatives during training, aiming to solve the two issues in one unified framework. Experiments on semantic textual similarity (STS) tasks demonstrate that our proposed ClusterNS compares favorably with baselines in unsupervised sentence representation learning. Our code has been",
    "link": "http://arxiv.org/abs/2305.09892",
    "context": "Title: Clustering-Aware Negative Sampling for Unsupervised Sentence Representation. (arXiv:2305.09892v1 [cs.CL])\nAbstract: Contrastive learning has been widely studied in sentence representation learning. However, earlier works mainly focus on the construction of positive examples, while in-batch samples are often simply treated as negative examples. This approach overlooks the importance of selecting appropriate negative examples, potentially leading to a scarcity of hard negatives and the inclusion of false negatives. To address these issues, we propose ClusterNS (Clustering-aware Negative Sampling), a novel method that incorporates cluster information into contrastive learning for unsupervised sentence representation learning. We apply a modified K-means clustering algorithm to supply hard negatives and recognize in-batch false negatives during training, aiming to solve the two issues in one unified framework. Experiments on semantic textual similarity (STS) tasks demonstrate that our proposed ClusterNS compares favorably with baselines in unsupervised sentence representation learning. Our code has been",
    "path": "papers/23/05/2305.09892.json",
    "total_tokens": 932,
    "translated_title": "面向聚类的负采样用于无监督句子表示学习",
    "translated_abstract": "对比学习在句子表示学习中广泛研究，然而早期的研究主要集中在正例的构建上，而 batch 内的样本通常被视为负例。这种方法忽视了选择合适的负例的重要性，可能会导致难负例的稀缺性和错误负例的包含。为了解决这些问题，我们提出了 ClusterNS（面向聚类的负采样），一种将聚类信息引入对比学习进行无监督句子表示学习的新方法。我们应用改进的 K 均值聚类算法来提供难负例并识别训练过程中的错误负例，旨在通过一个统一的框架解决这两个问题。在语义文本相似度（STS）任务上的实验表明，我们提出的 ClusterNS 在无监督句子表示学习中表现优于基线。我们的代码已上传到 https://github.com/xxxxxx。",
    "tldr": "ClusterNS 是一种将聚类信息引入对比学习进行无监督句子表示学习的新方法，通过改进的 K 均值聚类算法提供难负例并识别错误负例，旨在通过一个统一的框架解决问题，实验结果表明其在无监督句子表示学习中表现优于基线。"
}