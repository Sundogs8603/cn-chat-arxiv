{
    "title": "Nearest Neighbor Machine Translation is Meta-Optimizer on Output Projection Layer. (arXiv:2305.13034v2 [cs.CL] UPDATED)",
    "abstract": "Nearest Neighbor Machine Translation ($k$NN-MT) has achieved great success in domain adaptation tasks by integrating pre-trained Neural Machine Translation (NMT) models with domain-specific token-level retrieval. However, the reasons underlying its success have not been thoroughly investigated. In this paper, we comprehensively analyze $k$NN-MT through theoretical and empirical studies. Initially, we provide new insights into the working mechanism of $k$NN-MT as an efficient technique to implicitly execute gradient descent on the output projection layer of NMT, indicating that it is a specific case of model fine-tuning. Subsequently, we conduct multi-domain experiments and word-level analysis to examine the differences in performance between $k$NN-MT and entire-model fine-tuning. Our findings suggest that: (1) Incorporating $k$NN-MT with adapters yields comparable translation performance to fine-tuning on in-domain test sets, while achieving better performance on out-of-domain test set",
    "link": "http://arxiv.org/abs/2305.13034",
    "context": "Title: Nearest Neighbor Machine Translation is Meta-Optimizer on Output Projection Layer. (arXiv:2305.13034v2 [cs.CL] UPDATED)\nAbstract: Nearest Neighbor Machine Translation ($k$NN-MT) has achieved great success in domain adaptation tasks by integrating pre-trained Neural Machine Translation (NMT) models with domain-specific token-level retrieval. However, the reasons underlying its success have not been thoroughly investigated. In this paper, we comprehensively analyze $k$NN-MT through theoretical and empirical studies. Initially, we provide new insights into the working mechanism of $k$NN-MT as an efficient technique to implicitly execute gradient descent on the output projection layer of NMT, indicating that it is a specific case of model fine-tuning. Subsequently, we conduct multi-domain experiments and word-level analysis to examine the differences in performance between $k$NN-MT and entire-model fine-tuning. Our findings suggest that: (1) Incorporating $k$NN-MT with adapters yields comparable translation performance to fine-tuning on in-domain test sets, while achieving better performance on out-of-domain test set",
    "path": "papers/23/05/2305.13034.json",
    "total_tokens": 944,
    "translated_title": "最近邻机器翻译是输出投影层上的元优化器",
    "translated_abstract": "最近邻机器翻译（$k$NN-MT）通过将预训练的神经机器翻译（NMT）模型与领域特定的令牌级检索相结合，在领域适应任务中取得了巨大的成功。然而，其成功的原因尚未得到深入研究。在本文中，我们通过理论和实证研究全面分析了$k$NN-MT。首先，我们对$k$NN-MT的工作机制提供了新的见解，将其视为在NMT的输出投影层上隐式执行梯度下降的高效技术，表明它是模型微调的特殊情况。随后，我们进行了多领域实验证明和词级分析，以检查$k$NN-MT和整体模型微调之间性能差异。我们的研究结果表明：（1）将$k$NN-MT与适配器相结合，在领域内测试集上具有可比较的翻译性能，同时在领域外测试集上表现更好。",
    "tldr": "最近邻机器翻译将预训练的神经机器翻译模型与领域特定的令牌级检索相结合，实现了领域适应任务中的成功，通过在NMT的输出投影层上隐式执行梯度下降，以达到模型微调的效果。",
    "en_tdlr": "Nearest Neighbor Machine Translation integrates pre-trained Neural Machine Translation models with domain-specific token-level retrieval, achieving success in domain adaptation tasks. It implicitly executes gradient descent on the output projection layer of NMT, serving as a specific case of model fine-tuning."
}