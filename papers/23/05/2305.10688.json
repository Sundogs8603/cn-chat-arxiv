{
    "title": "MolXPT: Wrapping Molecules with Text for Generative Pre-training. (arXiv:2305.10688v1 [cs.CL])",
    "abstract": "Generative pre-trained Transformer (GPT) has demonstrates its great success in natural language processing and related techniques have been adapted into molecular modeling. Considering that text is the most important record for scientific discovery, in this paper, we propose MolXPT, a unified language model of text and molecules pre-trained on SMILES (a sequence representation of molecules) wrapped by text. Briefly, we detect the molecule names in each sequence and replace them to the corresponding SMILES. In this way, the SMILES could leverage the information from surrounding text, and vice versa. The above wrapped sequences, text sequences from PubMed and SMILES sequences from PubChem are all fed into a language model for pre-training. Experimental results demonstrate that MolXPT outperforms strong baselines of molecular property prediction on MoleculeNet, performs comparably to the best model in text-molecule translation while using less than half of its parameters, and enables zero",
    "link": "http://arxiv.org/abs/2305.10688",
    "context": "Title: MolXPT: Wrapping Molecules with Text for Generative Pre-training. (arXiv:2305.10688v1 [cs.CL])\nAbstract: Generative pre-trained Transformer (GPT) has demonstrates its great success in natural language processing and related techniques have been adapted into molecular modeling. Considering that text is the most important record for scientific discovery, in this paper, we propose MolXPT, a unified language model of text and molecules pre-trained on SMILES (a sequence representation of molecules) wrapped by text. Briefly, we detect the molecule names in each sequence and replace them to the corresponding SMILES. In this way, the SMILES could leverage the information from surrounding text, and vice versa. The above wrapped sequences, text sequences from PubMed and SMILES sequences from PubChem are all fed into a language model for pre-training. Experimental results demonstrate that MolXPT outperforms strong baselines of molecular property prediction on MoleculeNet, performs comparably to the best model in text-molecule translation while using less than half of its parameters, and enables zero",
    "path": "papers/23/05/2305.10688.json",
    "total_tokens": 896,
    "translated_title": "MolXPT：使用文本包装分子进行生成性预训练",
    "translated_abstract": "生成式预训练变压器（GPT）已经在自然语言处理中取得了巨大成功，并且相关技术已经被应用到了分子建模中。考虑到文本是科学发现最重要的记录，本文提出了 MolXPT，一个在 SMILES 上预训练的统一语言模型，其中 SMILES 被文本包装。简单来说，我们检测每个序列中的分子名称，并将它们替换为相应的 SMILES。通过这种方式，SMILES 可以利用周围文本的信息，反之亦然。以上包装的序列，是由来自 PubMed 的文本序列和来自 PubChem 的 SMILES 序列组成的，它们都被输入到语言模型中进行预训练。实验结果表明，MolXPT 在 MoleculeNet 上的分子属性预测的强基准模型中表现更好，在文本-分子翻译中表现与最佳模型相当，而使用的参数不到其一半，并且能够通过文本提示零-shot生成分子。",
    "tldr": "MolXPT是一个文本包装的统一语言模型，使用SMILES作为输入，可以提高分子模型的性能表现，并且使得基于零shot的分子生成成为可能。",
    "en_tdlr": "MolXPT is a unified language model that wraps sequences of SMILES and text, pre-trained using a language model on SMILES wrapped by text. The model outperforms strong baselines of molecular property prediction on MoleculeNet and enables zero-shot generation of molecules from text prompts."
}