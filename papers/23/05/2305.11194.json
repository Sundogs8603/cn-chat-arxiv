{
    "title": "Vaxformer: Antigenicity-controlled Transformer for Vaccine Design Against SARS-CoV-2. (arXiv:2305.11194v1 [q-bio.BM])",
    "abstract": "The SARS-CoV-2 pandemic has emphasised the importance of developing a universal vaccine that can protect against current and future variants of the virus. The present study proposes a novel conditional protein Language Model architecture, called Vaxformer, which is designed to produce natural-looking antigenicity-controlled SARS-CoV-2 spike proteins. We evaluate the generated protein sequences of the Vaxformer model using DDGun protein stability measure, netMHCpan antigenicity score, and a structure fidelity score with AlphaFold to gauge its viability for vaccine development. Our results show that Vaxformer outperforms the existing state-of-the-art Conditional Variational Autoencoder model to generate antigenicity-controlled SARS-CoV-2 spike proteins. These findings suggest promising opportunities for conditional Transformer models to expand our understanding of vaccine design and their role in mitigating global health challenges. The code used in this study is available at https://git",
    "link": "http://arxiv.org/abs/2305.11194",
    "context": "Title: Vaxformer: Antigenicity-controlled Transformer for Vaccine Design Against SARS-CoV-2. (arXiv:2305.11194v1 [q-bio.BM])\nAbstract: The SARS-CoV-2 pandemic has emphasised the importance of developing a universal vaccine that can protect against current and future variants of the virus. The present study proposes a novel conditional protein Language Model architecture, called Vaxformer, which is designed to produce natural-looking antigenicity-controlled SARS-CoV-2 spike proteins. We evaluate the generated protein sequences of the Vaxformer model using DDGun protein stability measure, netMHCpan antigenicity score, and a structure fidelity score with AlphaFold to gauge its viability for vaccine development. Our results show that Vaxformer outperforms the existing state-of-the-art Conditional Variational Autoencoder model to generate antigenicity-controlled SARS-CoV-2 spike proteins. These findings suggest promising opportunities for conditional Transformer models to expand our understanding of vaccine design and their role in mitigating global health challenges. The code used in this study is available at https://git",
    "path": "papers/23/05/2305.11194.json",
    "total_tokens": 959,
    "translated_title": "Vaxformer：针对SARS-CoV-2疫苗设计的抗原性控制Transformer",
    "translated_abstract": "SARS-CoV-2大流行强调了开发一种通用疫苗以保护免受当前和未来变种病毒的重要性。本研究提出了一种新型的条件蛋白质语言模型结构，称为Vaxformer，它旨在产生外观自然的抗原性控制的SARS-CoV-2刺突蛋白。我们使用DDGun蛋白质稳定性测量、netMHCpan抗原性评分和带有AlphaFold的结构保真度评分评估了Vaxformer模型的生成蛋白质序列，以衡量其用于疫苗开发的可能性。结果表明，Vaxformer比现有的最先进的条件变分自编码器模型表现更好，可以生成抗原性受控的SARS-CoV-2刺突蛋白。这些发现表明，条件Transformer模型为扩展我们对疫苗设计的理解以及它们在缓解全球健康挑战中的作用提供了有前途的机会。此研究使用的代码可在https://git上获取。",
    "tldr": "Vaxformer是一种新型的条件蛋白质语言模型结构，超过了现有的条件变分自编码器模型，在生成抗原性受控的SARS-CoV-2刺突蛋白方面表现更好，为疫苗设计提供了有前途的机会。",
    "en_tdlr": "The Vaxformer model, a novel conditional protein Language Model architecture, outperforms the existing state-of-the-art Conditional Variational Autoencoder model in generating antigenicity-controlled SARS-CoV-2 spike proteins, providing promising opportunities for vaccine design."
}