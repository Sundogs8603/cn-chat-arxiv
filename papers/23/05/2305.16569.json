{
    "title": "Accelerating Value Iteration with Anchoring. (arXiv:2305.16569v1 [cs.LG])",
    "abstract": "Value Iteration (VI) is foundational to the theory and practice of modern reinforcement learning, and it is known to converge at a $\\mathcal{O}(\\gamma^k)$-rate, where $\\gamma$ is the discount factor. Surprisingly, however, the optimal rate for the VI setup was not known, and finding a general acceleration mechanism has been an open problem. In this paper, we present the first accelerated VI for both the Bellman consistency and optimality operators. Our method, called Anc-VI, is based on an \\emph{anchoring} mechanism (distinct from Nesterov's acceleration), and it reduces the Bellman error faster than standard VI. In particular, Anc-VI exhibits a $\\mathcal{O}(1/k)$-rate for $\\gamma\\approx 1$ or even $\\gamma=1$, while standard VI has rate $\\mathcal{O}(1)$ for $\\gamma\\ge 1-1/k$, where $k$ is the iteration count. We also provide a complexity lower bound matching the upper bound up to a constant factor of $4$, thereby establishing optimality of the accelerated rate of Anc-VI. Finally, we sh",
    "link": "http://arxiv.org/abs/2305.16569",
    "context": "Title: Accelerating Value Iteration with Anchoring. (arXiv:2305.16569v1 [cs.LG])\nAbstract: Value Iteration (VI) is foundational to the theory and practice of modern reinforcement learning, and it is known to converge at a $\\mathcal{O}(\\gamma^k)$-rate, where $\\gamma$ is the discount factor. Surprisingly, however, the optimal rate for the VI setup was not known, and finding a general acceleration mechanism has been an open problem. In this paper, we present the first accelerated VI for both the Bellman consistency and optimality operators. Our method, called Anc-VI, is based on an \\emph{anchoring} mechanism (distinct from Nesterov's acceleration), and it reduces the Bellman error faster than standard VI. In particular, Anc-VI exhibits a $\\mathcal{O}(1/k)$-rate for $\\gamma\\approx 1$ or even $\\gamma=1$, while standard VI has rate $\\mathcal{O}(1)$ for $\\gamma\\ge 1-1/k$, where $k$ is the iteration count. We also provide a complexity lower bound matching the upper bound up to a constant factor of $4$, thereby establishing optimality of the accelerated rate of Anc-VI. Finally, we sh",
    "path": "papers/23/05/2305.16569.json",
    "total_tokens": 1023,
    "translated_title": "基于锚定机制的值迭代加速算法",
    "translated_abstract": "值迭代(Value Iteration, VI)是现代强化学习领域中理论和实践的基础，已知其收敛速度为$\\mathcal{O}(\\gamma^k)$，其中$\\gamma$是折扣因子。然而，在VI设置中的最优速度尚未确定，寻求一种通用的加速机制一直是一个未解决的问题。本文提出了第一个基于锚定机制的VI加速算法，称为Anc-VI。不同于Nesterov的加速方法，Anc-VI可以加速Bellman一致性和最优性算子，还比标准VI更快地减少Bellman误差。尤其是，对于$\\gamma\\approx 1$或甚至$\\gamma=1$，Anc-VI呈现出$\\mathcal{O}(1/k)$的速度，而标准VI在$\\gamma\\ge 1-1/k$时的速度为$\\mathcal{O}(1)$，其中$k$是迭代次数。我们还提供了与上界匹配的复杂性下界，除了一个常数因子$4$，从而证明了Anc-VI的加速速度的最优性。最后，我们在实验中证明了Anc-VI的有效性。",
    "tldr": "本文提出了一种加速值迭代算法Anc-VI，采用了锚定机制，可加速Bellman一致性和最优性算子的计算。对于$\\gamma\\approx 1$或$\\gamma=1$，Anc-VI速度为$\\mathcal{O}(1/k)$，比标准VI更快。",
    "en_tdlr": "This paper introduces an accelerated value iteration algorithm Anc-VI, which uses an anchoring mechanism to speed up the computation of Bellman consistency and optimality operators. It achieves a faster convergence rate for the Bellman error compared to standard VI, especially for $\\gamma\\approx 1$ or $\\gamma=1$, with a speed of $\\mathcal{O}(1/k)$."
}