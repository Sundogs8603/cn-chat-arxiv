{
    "title": "Statistical Foundations of Prior-Data Fitted Networks. (arXiv:2305.11097v1 [stat.ML])",
    "abstract": "Prior-data fitted networks (PFNs) were recently proposed as a new paradigm for machine learning. Instead of training the network to an observed training set, a fixed model is pre-trained offline on small, simulated training sets from a variety of tasks. The pre-trained model is then used to infer class probabilities in-context on fresh training sets with arbitrary size and distribution. Empirically, PFNs achieve state-of-the-art performance on tasks with similar size to the ones used in pre-training. Surprisingly, their accuracy further improves when passed larger data sets during inference. This article establishes a theoretical foundation for PFNs and illuminates the statistical mechanisms governing their behavior. While PFNs are motivated by Bayesian ideas, a purely frequentistic interpretation of PFNs as pre-tuned, but untrained predictors explains their behavior. A predictor's variance vanishes if its sensitivity to individual training samples does and the bias vanishes only if it",
    "link": "http://arxiv.org/abs/2305.11097",
    "context": "Title: Statistical Foundations of Prior-Data Fitted Networks. (arXiv:2305.11097v1 [stat.ML])\nAbstract: Prior-data fitted networks (PFNs) were recently proposed as a new paradigm for machine learning. Instead of training the network to an observed training set, a fixed model is pre-trained offline on small, simulated training sets from a variety of tasks. The pre-trained model is then used to infer class probabilities in-context on fresh training sets with arbitrary size and distribution. Empirically, PFNs achieve state-of-the-art performance on tasks with similar size to the ones used in pre-training. Surprisingly, their accuracy further improves when passed larger data sets during inference. This article establishes a theoretical foundation for PFNs and illuminates the statistical mechanisms governing their behavior. While PFNs are motivated by Bayesian ideas, a purely frequentistic interpretation of PFNs as pre-tuned, but untrained predictors explains their behavior. A predictor's variance vanishes if its sensitivity to individual training samples does and the bias vanishes only if it",
    "path": "papers/23/05/2305.11097.json",
    "total_tokens": 1015,
    "translated_title": "先验-数据拟合网络的统计学基础",
    "translated_abstract": "先验-数据拟合网络 (PFNs) 最近被提出作为一种新的机器学习范例。与将网络训练到观察到的训练集不同，一个固定的模型在各种任务的小型模拟训练集上进行离线预训练。然后，预训练的模型用于在任意大小和分布的新训练集上推断类概率。从经验上来看，当用于与预训练所用大小相似的任务时，PFNs 实现了最先进的性能。令人惊讶的是，在推断过程中，当传递较大的数据集时，其准确性进一步提高。本文为 PFNs 建立了一个理论基础并阐明了控制它们行为的统计机制。虽然 PFNs 受贝叶斯思想的启发，但将 PFNs 完全解释为预调的未经训练的预测器的频率解释可以解释它们的行为。如果预测器对单个训练样本敏感性降低，那么其方差也会降为零，而偏差只有当误差和导致函数归于常数时才会消失。",
    "tldr": "先验-数据拟合网络是一种新的机器学习范例，通过离线预训练固定的模型，然后在任意大小和分布的新训练集上推断类概率，并在与预训练所用大小相似的任务上实现了最先进的性能, 且准确性在推断过程中进一步提高。",
    "en_tdlr": "Prior-data fitted networks (PFNs) are a new paradigm for machine learning, achieving state-of-the-art performance by pre-training a fixed model on small simulated training sets from various tasks and use it to infer class probabilities on new training sets with arbitrary size and distribution. PFNs display improved accuracy when passed larger data sets during inference, and this article establishes a theoretical foundation for PFNs by explaining the statistical mechanisms governing their behavior."
}