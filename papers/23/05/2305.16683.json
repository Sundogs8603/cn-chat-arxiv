{
    "title": "Future-conditioned Unsupervised Pretraining for Decision Transformer. (arXiv:2305.16683v1 [cs.LG])",
    "abstract": "Recent research in offline reinforcement learning (RL) has demonstrated that return-conditioned supervised learning is a powerful paradigm for decision-making problems. While promising, return conditioning is limited to training data labeled with rewards and therefore faces challenges in learning from unsupervised data. In this work, we aim to utilize generalized future conditioning to enable efficient unsupervised pretraining from reward-free and sub-optimal offline data. We propose Pretrained Decision Transformer (PDT), a conceptually simple approach for unsupervised RL pretraining. PDT leverages future trajectory information as a privileged context to predict actions during training. The ability to make decisions based on both present and future factors enhances PDT's capability for generalization. Besides, this feature can be easily incorporated into a return-conditioned framework for online finetuning, by assigning return values to possible futures and sampling future embeddings b",
    "link": "http://arxiv.org/abs/2305.16683",
    "context": "Title: Future-conditioned Unsupervised Pretraining for Decision Transformer. (arXiv:2305.16683v1 [cs.LG])\nAbstract: Recent research in offline reinforcement learning (RL) has demonstrated that return-conditioned supervised learning is a powerful paradigm for decision-making problems. While promising, return conditioning is limited to training data labeled with rewards and therefore faces challenges in learning from unsupervised data. In this work, we aim to utilize generalized future conditioning to enable efficient unsupervised pretraining from reward-free and sub-optimal offline data. We propose Pretrained Decision Transformer (PDT), a conceptually simple approach for unsupervised RL pretraining. PDT leverages future trajectory information as a privileged context to predict actions during training. The ability to make decisions based on both present and future factors enhances PDT's capability for generalization. Besides, this feature can be easily incorporated into a return-conditioned framework for online finetuning, by assigning return values to possible futures and sampling future embeddings b",
    "path": "papers/23/05/2305.16683.json",
    "total_tokens": 1009,
    "translated_title": "未来条件下的无监督预训练对决策Transformer的影响",
    "translated_abstract": "最近离线强化学习方面的研究表明，以回报为条件的监督学习是解决决策问题的一种强有力的范例。虽然很有希望，但回报条件仅适用于标记有奖励数据的训练，因此在从无监督数据中学习方面面临挑战。在本文中，我们旨在利用广义未来条件来启用从没有奖励和次优离线数据进行高效无监督预训练。我们提出了预训练决策Transformer（PDT），这是一种概念上简单的无监督RL预训练方法。 PDT利用未来轨迹信息作为预测训练期间的动作的特权上下文。基于当前和未来因素做出决策的能力增强了PDT的泛化能力。此外，可以将此功能轻松地合并到基于回报的框架中以进行在线微调，通过为可能的未来分配回报值并根据这些回报样本未来嵌入来采样。",
    "tldr": "本文提出了一种基于未来信息的无监督预训练方法PDT，使得在没有奖励和次优离线数据上的无监督学习成为可能，通过为可能的未来分配回报值和采样未来嵌入进行在线微调。实验表明，PDT优于当前的无监督方法，并与有监督的学习方法相当竞争。",
    "en_tdlr": "This paper proposes an unsupervised pretraining method called Pretrained Decision Transformer (PDT) that leverages future trajectory information as a privileged context to predict actions during training. Empirical results show that PDT outperforms current unsupervised methods and is highly competitive with supervised learning methods. PDT can also be easily incorporated into a return-conditioned framework for online finetuning."
}