{
    "title": "The Interpreter Understands Your Meaning: End-to-end Spoken Language Understanding Aided by Speech Translation. (arXiv:2305.09652v1 [cs.CL])",
    "abstract": "End-to-end spoken language understanding (SLU) remains elusive even with current large pretrained language models on text and speech, especially in multilingual cases. Machine translation has been established as a powerful pretraining objective on text as it enables the model to capture high-level semantics of the input utterance and associations between different languages, which is desired for speech models that work on lower-level acoustic frames. Motivated particularly by the task of cross-lingual SLU, we demonstrate that the task of speech translation (ST) is a good means of pretraining speech models for end-to-end SLU on both monolingual and cross-lingual scenarios.  By introducing ST, our models give higher performance over current baselines on monolingual and multilingual intent classification as well as spoken question answering using SLURP, MINDS-14, and NMSQA benchmarks. To verify the effectiveness of our methods, we also release two new benchmark datasets from both syntheti",
    "link": "http://arxiv.org/abs/2305.09652",
    "context": "Title: The Interpreter Understands Your Meaning: End-to-end Spoken Language Understanding Aided by Speech Translation. (arXiv:2305.09652v1 [cs.CL])\nAbstract: End-to-end spoken language understanding (SLU) remains elusive even with current large pretrained language models on text and speech, especially in multilingual cases. Machine translation has been established as a powerful pretraining objective on text as it enables the model to capture high-level semantics of the input utterance and associations between different languages, which is desired for speech models that work on lower-level acoustic frames. Motivated particularly by the task of cross-lingual SLU, we demonstrate that the task of speech translation (ST) is a good means of pretraining speech models for end-to-end SLU on both monolingual and cross-lingual scenarios.  By introducing ST, our models give higher performance over current baselines on monolingual and multilingual intent classification as well as spoken question answering using SLURP, MINDS-14, and NMSQA benchmarks. To verify the effectiveness of our methods, we also release two new benchmark datasets from both syntheti",
    "path": "papers/23/05/2305.09652.json",
    "total_tokens": 935,
    "translated_title": "解释器理解您的意思: 由语音翻译协助的端到端口语理解",
    "translated_abstract": "尽管当前在文本和语音上有大规模预训练的语言模型，但端到端口语理解仍然难以实现，特别是在多语言情况下。机器翻译已被确定为强大的文本预训练目标，因为它使模型能够捕捉输入语句的高级语义和不同语言之间的关联，这对于处理更低级别的声学帧的语音模型非常有用。本文特别针对跨语言口语理解任务，证明了语音翻译(ST)是预训练语音模型进行端到端口语理解的良好手段，无论是在单语言场景还是跨语言场景下。通过引入ST，我们的模型在使用SLURP、MINDS-14和NMSQA基准测试进行单语言和多语言意图分类以及口语问答时，相比当前基准测试方法均具有更高性能。为验证我们方法的有效性，我们还发布了两个新的基准数据集，分别来自合成和真实数据。",
    "tldr": "语音翻译(ST)是预训练语音模型进行端到端口语理解的良好手段。通过引入ST，我们的模型在单语言和跨语言场景下表现均好，具有更高的性能。",
    "en_tdlr": "Speech translation (ST) works as a good means of pretraining speech models for end-to-end spoken language understanding (SLU) on both monolingual and cross-lingual scenarios. By introducing ST, our models outperform the current baselines on intent classification and spoken question answering tasks."
}