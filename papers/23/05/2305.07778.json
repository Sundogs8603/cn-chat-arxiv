{
    "title": "Accelerator-Aware Training for Transducer-Based Speech Recognition. (arXiv:2305.07778v1 [cs.LG])",
    "abstract": "Machine learning model weights and activations are represented in full-precision during training. This leads to performance degradation in runtime when deployed on neural network accelerator (NNA) chips, which leverage highly parallelized fixed-point arithmetic to improve runtime memory and latency. In this work, we replicate the NNA operators during the training phase, accounting for the degradation due to low-precision inference on the NNA in back-propagation. Our proposed method efficiently emulates NNA operations, thus foregoing the need to transfer quantization error-prone data to the Central Processing Unit (CPU), ultimately reducing the user perceived latency (UPL). We apply our approach to Recurrent Neural Network-Transducer (RNN-T), an attractive architecture for on-device streaming speech recognition tasks. We train and evaluate models on 270K hours of English data and show a 5-7% improvement in engine latency while saving up to 10% relative degradation in WER.",
    "link": "http://arxiv.org/abs/2305.07778",
    "context": "Title: Accelerator-Aware Training for Transducer-Based Speech Recognition. (arXiv:2305.07778v1 [cs.LG])\nAbstract: Machine learning model weights and activations are represented in full-precision during training. This leads to performance degradation in runtime when deployed on neural network accelerator (NNA) chips, which leverage highly parallelized fixed-point arithmetic to improve runtime memory and latency. In this work, we replicate the NNA operators during the training phase, accounting for the degradation due to low-precision inference on the NNA in back-propagation. Our proposed method efficiently emulates NNA operations, thus foregoing the need to transfer quantization error-prone data to the Central Processing Unit (CPU), ultimately reducing the user perceived latency (UPL). We apply our approach to Recurrent Neural Network-Transducer (RNN-T), an attractive architecture for on-device streaming speech recognition tasks. We train and evaluate models on 270K hours of English data and show a 5-7% improvement in engine latency while saving up to 10% relative degradation in WER.",
    "path": "papers/23/05/2305.07778.json",
    "total_tokens": 905,
    "translated_title": "基于加速器的训练：用于转换器语音识别的方法",
    "translated_abstract": "在训练过程中，机器学习模型的权重和激活状态以全精度表示。然而，当在神经网络加速器芯片上进行部署时，这会导致运行时性能下降。本文中，我们在训练阶段复制了NNA运算符来考虑低精度推理带来的后向传播中的性能下降。我们的方法有效地模拟了NNA操作，避免了将数据传输到中央处理单元（CPU）中的量化错误，从而最终减少了用户感知延迟（UPL）。我们将这种方法应用于循环神经网络转录器（RNN-T）中，这是一种适用于设备上流式语音识别任务的优秀架构。我们在270K小时的英语数据上训练和评估模型，并显示出在引擎延迟方面有5-7％的改进，同时节省高达10％的WER相对下降。",
    "tldr": "本文提出的加速器感知训练方法应用于RNN-T模型，有效地模拟NNA操作，使用户感知延迟（UPL）减少，同时在引擎延迟方面有5-7％的改进，节省高达10％的WER相对下降。",
    "en_tdlr": "This paper proposes an accelerator-aware training method applied to RNN-T model, which effectively emulates NNA operations, reduces user perceived latency (UPL), shows 5-7% improvement in engine latency, and saves up to 10% relative degradation in WER."
}