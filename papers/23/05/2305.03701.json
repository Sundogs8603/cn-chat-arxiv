{
    "title": "LMEye: An Interactive Perception Network for Large Language Models. (arXiv:2305.03701v1 [cs.CV])",
    "abstract": "Training a Large Visual Language Model (LVLM) from scratch, like GPT-4, is resource-intensive. Our paper proposes an alternative method called LMEye, a play-plug-in Interactive Perception Network for Large Language Models (LLMs), aiming to improve the accuracy of image understanding for the LVLM. Previous methods that infuse visual information into LLMs utilize a static visual mapping network, but lack dynamic interaction between the LLMs and visual information. LMEye addresses this issue by allowing the LLM to incorporate the visual information that aligned with human instruction. Specifically, the LMEye network consists of a static visual mapping network to provide the basic perception of an image to LLMs. Then, it also contains additional linear layers responsible for acquiring requests from LLMs, decomposing image features, and transmitting the interleaved information to LLMs, respectively. In this way, LLMs act to be in charge of understanding human instructions, sending it to the",
    "link": "http://arxiv.org/abs/2305.03701",
    "context": "Title: LMEye: An Interactive Perception Network for Large Language Models. (arXiv:2305.03701v1 [cs.CV])\nAbstract: Training a Large Visual Language Model (LVLM) from scratch, like GPT-4, is resource-intensive. Our paper proposes an alternative method called LMEye, a play-plug-in Interactive Perception Network for Large Language Models (LLMs), aiming to improve the accuracy of image understanding for the LVLM. Previous methods that infuse visual information into LLMs utilize a static visual mapping network, but lack dynamic interaction between the LLMs and visual information. LMEye addresses this issue by allowing the LLM to incorporate the visual information that aligned with human instruction. Specifically, the LMEye network consists of a static visual mapping network to provide the basic perception of an image to LLMs. Then, it also contains additional linear layers responsible for acquiring requests from LLMs, decomposing image features, and transmitting the interleaved information to LLMs, respectively. In this way, LLMs act to be in charge of understanding human instructions, sending it to the",
    "path": "papers/23/05/2305.03701.json",
    "total_tokens": 917,
    "tldr": "本文介绍了LMEye，一种用于大型语言模型的交互式感知网络，通过允许LLM与人类指令相对齐的视觉信息，来提高LVLM的图像理解准确性。",
    "en_tdlr": "The paper proposes LMEye, an interactive perception network for large language models (LLMs) that improves the accuracy of image understanding for large visual language models (LVLMs) by allowing LLMs to incorporate visual information aligned with human instruction instead of using a static visual mapping network."
}