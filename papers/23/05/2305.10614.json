{
    "title": "Token-wise Decomposition of Autoregressive Language Model Hidden States for Analyzing Model Predictions. (arXiv:2305.10614v1 [cs.CL])",
    "abstract": "While there is much recent interest in studying why Transformer-based large language models make predictions the way they do, the complex computations performed within each layer have traditionally posed a strong bottleneck. To mitigate this shortcoming, this work presents a linear decomposition of final hidden states from autoregressive language models based on each initial input token, which is exact for virtually all contemporary Transformer architectures. This decomposition allows the definition of probability distributions that ablate the contribution of specific input tokens, which can be used to analyze their influence on model probabilities over a sequence of upcoming words with only one forward pass from the model. Using the change in next-word probability as a measure of importance, this work first examines which context words make the biggest contribution to language model predictions. Regression experiments suggest that Transformer-based language models rely primarily on co",
    "link": "http://arxiv.org/abs/2305.10614",
    "context": "Title: Token-wise Decomposition of Autoregressive Language Model Hidden States for Analyzing Model Predictions. (arXiv:2305.10614v1 [cs.CL])\nAbstract: While there is much recent interest in studying why Transformer-based large language models make predictions the way they do, the complex computations performed within each layer have traditionally posed a strong bottleneck. To mitigate this shortcoming, this work presents a linear decomposition of final hidden states from autoregressive language models based on each initial input token, which is exact for virtually all contemporary Transformer architectures. This decomposition allows the definition of probability distributions that ablate the contribution of specific input tokens, which can be used to analyze their influence on model probabilities over a sequence of upcoming words with only one forward pass from the model. Using the change in next-word probability as a measure of importance, this work first examines which context words make the biggest contribution to language model predictions. Regression experiments suggest that Transformer-based language models rely primarily on co",
    "path": "papers/23/05/2305.10614.json",
    "total_tokens": 889,
    "tldr": "该论文通过令牌分解的方式，提出了一种简化计算的方法，分析输入令牌对模型概率的影响，实验结果表明Transformer语言模型主要依赖于上下文单词。",
    "en_tdlr": "This paper proposes a token-wise decomposition method to simplify the calculations required to analyze the impact of input tokens on model predictions. The experiments showed that Transformer-based language models rely primarily on context words."
}