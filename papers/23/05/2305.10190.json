{
    "title": "Variable-length Neural Interlingua Representations for Zero-shot Neural Machine Translation. (arXiv:2305.10190v1 [cs.CL])",
    "abstract": "The language-independency of encoded representations within multilingual neural machine translation (MNMT) models is crucial for their generalization ability on zero-shot translation. Neural interlingua representations have been shown as an effective method for achieving this. However, fixed-length neural interlingua representations introduced in previous work can limit its flexibility and representation ability. In this study, we introduce a novel method to enhance neural interlingua representations by making their length variable, thereby overcoming the constraint of fixed-length neural interlingua representations. Our empirical results on zero-shot translation on OPUS, IWSLT, and Europarl datasets demonstrate stable model convergence and superior zero-shot translation results compared to fixed-length neural interlingua representations. However, our analysis reveals the suboptimal efficacy of our approach in translating from certain source languages, wherein we pinpoint the defective",
    "link": "http://arxiv.org/abs/2305.10190",
    "context": "Title: Variable-length Neural Interlingua Representations for Zero-shot Neural Machine Translation. (arXiv:2305.10190v1 [cs.CL])\nAbstract: The language-independency of encoded representations within multilingual neural machine translation (MNMT) models is crucial for their generalization ability on zero-shot translation. Neural interlingua representations have been shown as an effective method for achieving this. However, fixed-length neural interlingua representations introduced in previous work can limit its flexibility and representation ability. In this study, we introduce a novel method to enhance neural interlingua representations by making their length variable, thereby overcoming the constraint of fixed-length neural interlingua representations. Our empirical results on zero-shot translation on OPUS, IWSLT, and Europarl datasets demonstrate stable model convergence and superior zero-shot translation results compared to fixed-length neural interlingua representations. However, our analysis reveals the suboptimal efficacy of our approach in translating from certain source languages, wherein we pinpoint the defective",
    "path": "papers/23/05/2305.10190.json",
    "total_tokens": 867,
    "translated_title": "变长神经中间语表示用于零样本神经机器翻译",
    "translated_abstract": "多语种神经机器翻译（MNMT）模型中编码表示的语言无关性对其在零样本翻译上的泛化能力至关重要。神经中间语表示已被证明是实现这一目标的有效方法。然而，先前工作中引入的定长神经中间语表示可能会限制其灵活性和表示能力。本研究通过使神经中间语表示的长度变化来增强神经中间语表示的方法，从而克服了定长神经中间语表示的约束。我们在OPUS、IWSLT和Europarl数据集上进行的零样本翻译的实证结果表明，相对于固定长度神经中间语表示，我们的方法具有稳定的模型收敛性和优秀的零样本翻译结果。然而，我们的分析揭示了我们的方法在翻译某些源语言时的效果不佳。",
    "tldr": "本研究提出一种变长神经中间语表示方法，克服了先前的定长表示方法的限制。在多个数据集上，我们的方法比固定长度中间语表示方法表现更好，但在特定源语言的翻译中效果欠佳。",
    "en_tdlr": "This study proposes a variable-length neural interlingua representation method to overcome the limitations of previous fixed-length methods. Our empirical results on multiple datasets demonstrate better performance compared to fixed-length methods, but suboptimal efficacy in translating from certain source languages is observed."
}