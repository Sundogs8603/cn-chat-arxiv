{
    "title": "Mitigating Catastrophic Forgetting in Long Short-Term Memory Networks. (arXiv:2305.17244v1 [cs.LG])",
    "abstract": "Continual learning on sequential data is critical for many machine learning (ML) deployments. Unfortunately, LSTM networks, which are commonly used to learn on sequential data, suffer from catastrophic forgetting and are limited in their ability to learn multiple tasks continually. We discover that catastrophic forgetting in LSTM networks can be overcome in two novel and readily-implementable ways -- separating the LSTM memory either for each task or for each target label. Our approach eschews the need for explicit regularization, hypernetworks, and other complex methods. We quantify the benefits of our approach on recently-proposed LSTM networks for computer memory access prefetching, an important sequential learning problem in ML-based computer system optimization. Compared to state-of-the-art weight regularization methods to mitigate catastrophic forgetting, our approach is simple, effective, and enables faster learning. We also show that our proposal enables the use of small, non-r",
    "link": "http://arxiv.org/abs/2305.17244",
    "context": "Title: Mitigating Catastrophic Forgetting in Long Short-Term Memory Networks. (arXiv:2305.17244v1 [cs.LG])\nAbstract: Continual learning on sequential data is critical for many machine learning (ML) deployments. Unfortunately, LSTM networks, which are commonly used to learn on sequential data, suffer from catastrophic forgetting and are limited in their ability to learn multiple tasks continually. We discover that catastrophic forgetting in LSTM networks can be overcome in two novel and readily-implementable ways -- separating the LSTM memory either for each task or for each target label. Our approach eschews the need for explicit regularization, hypernetworks, and other complex methods. We quantify the benefits of our approach on recently-proposed LSTM networks for computer memory access prefetching, an important sequential learning problem in ML-based computer system optimization. Compared to state-of-the-art weight regularization methods to mitigate catastrophic forgetting, our approach is simple, effective, and enables faster learning. We also show that our proposal enables the use of small, non-r",
    "path": "papers/23/05/2305.17244.json",
    "total_tokens": 881,
    "tldr": "本研究提出了两种新方法，分别为每个任务或每个目标标签分离LSTM内存，可减少LSTM网络中的灾难性遗忘，而且无需显式正则化和超网络等复杂方法。"
}