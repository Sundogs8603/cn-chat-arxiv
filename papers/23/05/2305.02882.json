{
    "title": "Simple Noisy Environment Augmentation for Reinforcement Learning. (arXiv:2305.02882v1 [cs.LG])",
    "abstract": "Data augmentation is a widely used technique for improving model performance in machine learning, particularly in computer vision and natural language processing. Recently, there has been increasing interest in applying augmentation techniques to reinforcement learning (RL) problems, with a focus on image-based augmentation. In this paper, we explore a set of generic wrappers designed to augment RL environments with noise and encourage agent exploration and improve training data diversity which are applicable to a broad spectrum of RL algorithms and environments. Specifically, we concentrate on augmentations concerning states, rewards, and transition dynamics and introduce two novel augmentation techniques. In addition, we introduce a noise rate hyperparameter for control over the frequency of noise injection. We present experimental results on the impact of these wrappers on return using three popular RL algorithms, Soft Actor-Critic (SAC), Twin Delayed DDPG (TD3), and Proximal Policy",
    "link": "http://arxiv.org/abs/2305.02882",
    "context": "Title: Simple Noisy Environment Augmentation for Reinforcement Learning. (arXiv:2305.02882v1 [cs.LG])\nAbstract: Data augmentation is a widely used technique for improving model performance in machine learning, particularly in computer vision and natural language processing. Recently, there has been increasing interest in applying augmentation techniques to reinforcement learning (RL) problems, with a focus on image-based augmentation. In this paper, we explore a set of generic wrappers designed to augment RL environments with noise and encourage agent exploration and improve training data diversity which are applicable to a broad spectrum of RL algorithms and environments. Specifically, we concentrate on augmentations concerning states, rewards, and transition dynamics and introduce two novel augmentation techniques. In addition, we introduce a noise rate hyperparameter for control over the frequency of noise injection. We present experimental results on the impact of these wrappers on return using three popular RL algorithms, Soft Actor-Critic (SAC), Twin Delayed DDPG (TD3), and Proximal Policy",
    "path": "papers/23/05/2305.02882.json",
    "total_tokens": 1001,
    "translated_title": "简单的嘈杂环境增强用于强化学习",
    "translated_abstract": "数据增强是提升机器学习模型性能的一种广泛应用的技术，尤其是在计算机视觉和自然语言处理方面。最近，越来越多的人开始应用增强技术来解决强化学习（RL）问题，主要专注于基于图像的增强。本文旨在探讨一系列通用封装，设计用于噪声增强RL环境，并鼓励代理人探索和改善训练数据多样性，适用于广泛的RL算法和环境。具体而言，我们集中于涉及状态、奖励和转换动态的增强，并介绍了两种新的增强技术。此外，我们引入噪声率超参数，以控制噪声注入的频率。我们使用三种流行的RL算法，Soft Actor-Critic（SAC），Twin Delayed DDPG（TD3）和Proximal Policy进行实验，研究这些封装对回报的影响。",
    "tldr": "本论文探讨了一系列通用封装，用于噪声增强RL环境，提供了两种新的增强技术，有助于代理人探索和改善训练数据多样性。同时，介绍了控制噪声注入频率的超参数噪声率。在实验中使用了三种流行的RL算法，Soft Actor-Critic（SAC），Twin Delayed DDPG（TD3）和Proximal Policy，以研究这些封装对回报的影响。",
    "en_tdlr": "This paper explores a set of generic wrappers designed to augment RL environments with noise and encourage agent exploration and improve training data diversity, offering two novel augmentation techniques and a noise rate hyperparameter for control over the frequency of noise injection. The impact of these wrappers on return using three popular RL algorithms, Soft Actor-Critic (SAC), Twin Delayed DDPG (TD3), and Proximal Policy, are presented through experiments."
}