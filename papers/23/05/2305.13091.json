{
    "title": "Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization. (arXiv:2305.13091v2 [cs.CL] UPDATED)",
    "abstract": "With the recent undeniable advancement in reasoning abilities in large language models (LLMs) like ChatGPT and GPT-4, there is a growing trend for using LLMs on various tasks. One area where LLMs can be employed is as an alternative evaluation metric for complex generative tasks, which generally demands expensive human judges to complement the traditional automatic metrics for various evaluation dimensions such as fluency and consistency. In this work, we conduct extensive analysis to investigate the stability and reliability of LLMs as automatic evaluators for abstractive summarization. We found that while ChatGPT and GPT-4 outperform the commonly used automatic metrics, they are not ready as human replacements due to significant limitations. That is, LLM evaluators rate each candidate system inconsistently and are dimension-dependent. They also struggle to compare candidates with close performance and become more unreliable with higher-quality summaries by obtaining a lower correlati",
    "link": "http://arxiv.org/abs/2305.13091",
    "context": "Title: Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization. (arXiv:2305.13091v2 [cs.CL] UPDATED)\nAbstract: With the recent undeniable advancement in reasoning abilities in large language models (LLMs) like ChatGPT and GPT-4, there is a growing trend for using LLMs on various tasks. One area where LLMs can be employed is as an alternative evaluation metric for complex generative tasks, which generally demands expensive human judges to complement the traditional automatic metrics for various evaluation dimensions such as fluency and consistency. In this work, we conduct extensive analysis to investigate the stability and reliability of LLMs as automatic evaluators for abstractive summarization. We found that while ChatGPT and GPT-4 outperform the commonly used automatic metrics, they are not ready as human replacements due to significant limitations. That is, LLM evaluators rate each candidate system inconsistently and are dimension-dependent. They also struggle to compare candidates with close performance and become more unreliable with higher-quality summaries by obtaining a lower correlati",
    "path": "papers/23/05/2305.13091.json",
    "total_tokens": 897,
    "translated_title": "大型语言模型尚未达到人类级别的抽象摘要评估器",
    "translated_abstract": "随着ChatGPT和GPT-4等大型语言模型（LLMs）在推理能力方面的显着进展，越来越多的任务开始使用LLMs。一个可以利用LLMs的领域是作为复杂生成任务的替代评估度量，这通常需要昂贵的人工评审员来补充各种评估维度（如流畅性和一致性）的传统自动评估度量。在这项工作中，我们进行了广泛的分析，以研究LLMs作为抽象摘要评估器的稳定性和可靠性。我们发现，虽然ChatGPT和GPT-4在常用的自动评估度量上表现优异，但由于重要限制，它们尚未准备好代替人工评估。即LLM评估器对每个候选系统的评分不一致，并且与评估维度相关。它们也难以比较性能接近的候选系统，并在获取较高质量摘要时变得更加不可靠，与传统度量之间的相关性较低。",
    "tldr": "大型语言模型不适合作为抽象摘要评估器的人类替代品，由于存在限制，它们在评估稳定性和可靠性方面表现不佳。",
    "en_tdlr": "Large language models are not suitable substitutes for human evaluators in abstractive summarization due to significant limitations in evaluation stability and reliability."
}