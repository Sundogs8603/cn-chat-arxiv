{
    "title": "Quality In / Quality Out: Assessing Data quality in an Anomaly Detection Benchmark. (arXiv:2305.19770v1 [cs.LG])",
    "abstract": "Autonomous or self-driving networks are expected to provide a solution to the myriad of extremely demanding new applications in the Future Internet. The key to handle complexity is to perform tasks like network optimization and failure recovery with minimal human supervision. For this purpose, the community relies on the development of new Machine Learning (ML) models and techniques. However, ML can only be as good as the data it is fitted with. Datasets provided to the community as benchmarks for research purposes, which have a relevant impact in research findings and directions, are often assumed to be of good quality by default. In this paper, we show that relatively minor modifications on the same benchmark dataset (UGR'16, a flow-based real-traffic dataset for anomaly detection) cause significantly more impact on model performance than the specific ML technique considered. To understand this finding, we contribute a methodology to investigate the root causes for those differences,",
    "link": "http://arxiv.org/abs/2305.19770",
    "context": "Title: Quality In / Quality Out: Assessing Data quality in an Anomaly Detection Benchmark. (arXiv:2305.19770v1 [cs.LG])\nAbstract: Autonomous or self-driving networks are expected to provide a solution to the myriad of extremely demanding new applications in the Future Internet. The key to handle complexity is to perform tasks like network optimization and failure recovery with minimal human supervision. For this purpose, the community relies on the development of new Machine Learning (ML) models and techniques. However, ML can only be as good as the data it is fitted with. Datasets provided to the community as benchmarks for research purposes, which have a relevant impact in research findings and directions, are often assumed to be of good quality by default. In this paper, we show that relatively minor modifications on the same benchmark dataset (UGR'16, a flow-based real-traffic dataset for anomaly detection) cause significantly more impact on model performance than the specific ML technique considered. To understand this finding, we contribute a methodology to investigate the root causes for those differences,",
    "path": "papers/23/05/2305.19770.json",
    "total_tokens": 866,
    "translated_title": "“质量进/质量出：评估异常检测基准数据的数据质量”",
    "translated_abstract": "自主或自动驾驶网络被期望成为未来互联网中极富挑战和需求的新型应用的解决方案。处理复杂性的关键在于通过最少的人工干预执行网络优化和故障恢复的任务。为此，社区依赖于新的机器学习模型和技术的开发。然而，机器学习的好坏取决于它所拟合的数据。为研究目的提供的数据集（对研究结果和方向有重要影响的基准数据集）通常被认为是默认具有良好质量的。本文表明，对同一基准数据集（UGR'16，用于异常检测的基于流量的实时数据集）进行相对较小的修改，比所考虑的具体机器学习技术更显著地影响了模型性能。为了理解这一发现，我们提出了一种研究这些差异根本原因的方法。",
    "tldr": "本文发现，在进行异常检测基准测试时，基准数据集的质量对于机器学习模型的性能影响很大，比特定的机器学习算法更重要。",
    "en_tdlr": "This paper discovers that data quality of benchmark datasets for anomaly detection has a significant impact on the performance of machine learning models, more than the choice of specific machine learning techniques."
}