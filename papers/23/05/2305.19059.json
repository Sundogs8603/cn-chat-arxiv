{
    "title": "Rank-adaptive spectral pruning of convolutional layers during training. (arXiv:2305.19059v1 [cs.LG])",
    "abstract": "The computing cost and memory demand of deep learning pipelines have grown fast in recent years and thus a variety of pruning techniques have been developed to reduce model parameters. The majority of these techniques focus on reducing inference costs by pruning the network after a pass of full training. A smaller number of methods address the reduction of training costs, mostly based on compressing the network via low-rank layer factorizations. Despite their efficiency for linear layers, these methods fail to effectively handle convolutional filters. In this work, we propose a low-parametric training method that factorizes the convolutions into tensor Tucker format and adaptively prunes the Tucker ranks of the convolutional kernel during training. Leveraging fundamental results from geometric integration theory of differential equations on tensor manifolds, we obtain a robust training algorithm that provably approximates the full baseline performance and guarantees loss descent. A var",
    "link": "http://arxiv.org/abs/2305.19059",
    "context": "Title: Rank-adaptive spectral pruning of convolutional layers during training. (arXiv:2305.19059v1 [cs.LG])\nAbstract: The computing cost and memory demand of deep learning pipelines have grown fast in recent years and thus a variety of pruning techniques have been developed to reduce model parameters. The majority of these techniques focus on reducing inference costs by pruning the network after a pass of full training. A smaller number of methods address the reduction of training costs, mostly based on compressing the network via low-rank layer factorizations. Despite their efficiency for linear layers, these methods fail to effectively handle convolutional filters. In this work, we propose a low-parametric training method that factorizes the convolutions into tensor Tucker format and adaptively prunes the Tucker ranks of the convolutional kernel during training. Leveraging fundamental results from geometric integration theory of differential equations on tensor manifolds, we obtain a robust training algorithm that provably approximates the full baseline performance and guarantees loss descent. A var",
    "path": "papers/23/05/2305.19059.json",
    "total_tokens": 874,
    "translated_title": "训练期间的自适应秩谱剪枝卷积层",
    "translated_abstract": "深度学习模型在计算成本和内存需求方面增长迅速，因此已经发展了各种剪枝技术以减少模型参数。大多数技术侧重于通过在完整训练后对网络进行修剪以减少推理成本。少量的方法解决了减少训练成本的问题，主要是通过低秩层分解来压缩网络。尽管这些方法对于线性层是有效的，但是它们无法有效处理卷积滤波器。在这项工作中，我们提出了一种低参数训练方法，将卷积分解为张量Tucker格式，并在训练过程中自适应地修剪卷积核的Tucker秩。利用微分方程在张量流形上的几何积分理论的基本结果，我们获得了一个鲁棒的训练算法，证明能够逼近完整的基线性能并保证损失下降。",
    "tldr": "本论文提出了一种新的低参数训练方法，该方法将卷积分解为张量Tucker格式，并在训练过程中自适应地修剪卷积核的Tucker秩，可以有效地降低训练成本。",
    "en_tdlr": "This paper proposes a new low-parameter training method that decomposes convolutions into tensor Tucker format and adaptively prunes the Tucker ranks of the convolutional kernel during training, which can effectively reduce training costs."
}