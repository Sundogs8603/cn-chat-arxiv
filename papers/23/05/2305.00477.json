{
    "title": "Posterior Sampling for Deep Reinforcement Learning. (arXiv:2305.00477v1 [cs.LG])",
    "abstract": "Despite remarkable successes, deep reinforcement learning algorithms remain sample inefficient: they require an enormous amount of trial and error to find good policies. Model-based algorithms promise sample efficiency by building an environment model that can be used for planning. Posterior Sampling for Reinforcement Learning is such a model-based algorithm that has attracted significant interest due to its performance in the tabular setting. This paper introduces Posterior Sampling for Deep Reinforcement Learning (PSDRL), the first truly scalable approximation of Posterior Sampling for Reinforcement Learning that retains its model-based essence. PSDRL combines efficient uncertainty quantification over latent state space models with a specially tailored continual planning algorithm based on value-function approximation. Extensive experiments on the Atari benchmark show that PSDRL significantly outperforms previous state-of-the-art attempts at scaling up posterior sampling while being ",
    "link": "http://arxiv.org/abs/2305.00477",
    "context": "Title: Posterior Sampling for Deep Reinforcement Learning. (arXiv:2305.00477v1 [cs.LG])\nAbstract: Despite remarkable successes, deep reinforcement learning algorithms remain sample inefficient: they require an enormous amount of trial and error to find good policies. Model-based algorithms promise sample efficiency by building an environment model that can be used for planning. Posterior Sampling for Reinforcement Learning is such a model-based algorithm that has attracted significant interest due to its performance in the tabular setting. This paper introduces Posterior Sampling for Deep Reinforcement Learning (PSDRL), the first truly scalable approximation of Posterior Sampling for Reinforcement Learning that retains its model-based essence. PSDRL combines efficient uncertainty quantification over latent state space models with a specially tailored continual planning algorithm based on value-function approximation. Extensive experiments on the Atari benchmark show that PSDRL significantly outperforms previous state-of-the-art attempts at scaling up posterior sampling while being ",
    "path": "papers/23/05/2305.00477.json",
    "total_tokens": 866,
    "translated_title": "深度强化学习的后验采样",
    "translated_abstract": "尽管深度强化学习算法取得了显著的成功，但样本效率仍然较低：它们需要大量的试错来找到好的策略。基于模型的算法通过构建可以用于规划的环境模型来提高样本效率。后验采样强化学习是这样一种基于模型的算法，在表格设置中由于其性能而引起了广泛的兴趣。本文介绍了用于深度强化学习的后验采样（PSDRL），这是第一个真正可扩展的后验采样强化学习的近似方法，保留了其基于模型的本质特征。PSDRL将潜在状态空间模型上的高效不确定性量化与基于值函数逼近的特殊设计的持续规划算法相结合。对Atari基准测试的广泛实验表明，PSDRL在提高样本效率的同时，显著优于以前的最先进尝试。",
    "tldr": "本文提出了用于深度强化学习的后验采样算法PSDRL，结合了高效的不确定性量化和特殊设计的持续规划算法，使其在提高样本效率的同时显著优于之前的尝试。",
    "en_tdlr": "This paper introduces PSDRL, an approximation of Posterior Sampling for Reinforcement Learning that combines efficient uncertainty quantification and tailored continual planning algorithm, which significantly outperforms previous attempts on Atari benchmark while being more sample-efficient."
}