{
    "title": "Neural Network Reduction with Guided Regularizers. (arXiv:2305.18448v1 [cs.LG])",
    "abstract": "Regularization techniques such as $\\mathcal{L}_1$ and $\\mathcal{L}_2$ regularizers are effective in sparsifying neural networks (NNs). However, to remove a certain neuron or channel in NNs, all weight elements related to that neuron or channel need to be prunable, which is not guaranteed by traditional regularization. This paper proposes a simple new approach named \"Guided Regularization\" that prioritizes the weights of certain NN units more than others during training, which renders some of the units less important and thus, prunable. This is different from the scattered sparsification of $\\mathcal{L}_1$ and $\\mathcal{L}_2$ regularizers where the the components of a weight matrix that are zeroed out can be located anywhere. The proposed approach offers a natural reduction of NN in the sense that a model is being trained while also neutralizing unnecessary units. We empirically demonstrate that our proposed method is effective in pruning NNs while maintaining performance.",
    "link": "http://arxiv.org/abs/2305.18448",
    "context": "Title: Neural Network Reduction with Guided Regularizers. (arXiv:2305.18448v1 [cs.LG])\nAbstract: Regularization techniques such as $\\mathcal{L}_1$ and $\\mathcal{L}_2$ regularizers are effective in sparsifying neural networks (NNs). However, to remove a certain neuron or channel in NNs, all weight elements related to that neuron or channel need to be prunable, which is not guaranteed by traditional regularization. This paper proposes a simple new approach named \"Guided Regularization\" that prioritizes the weights of certain NN units more than others during training, which renders some of the units less important and thus, prunable. This is different from the scattered sparsification of $\\mathcal{L}_1$ and $\\mathcal{L}_2$ regularizers where the the components of a weight matrix that are zeroed out can be located anywhere. The proposed approach offers a natural reduction of NN in the sense that a model is being trained while also neutralizing unnecessary units. We empirically demonstrate that our proposed method is effective in pruning NNs while maintaining performance.",
    "path": "papers/23/05/2305.18448.json",
    "total_tokens": 921,
    "translated_title": "有导向性正则化的神经网络压缩",
    "translated_abstract": "正则化技术例如 $\\mathcal{L}_1$ 和 $\\mathcal{L}_2$ 正则化在删减神经网络中非常有效。然而，为了移除神经网络中的某个神经元或通道，所有与该神经元或通道相关的权重元素需要是可删减的，这不能由传统的正则化方法保证。本文提出了一种简单的新方法，称为 “导向性正则化”，在训练过程中优先考虑某些神经网络单元的权重，使一些单元不那么重要，从而可以削减神经元。这与 $\\mathcal{L}_1$ 和 $\\mathcal{L}_2$ 正则化的分散稀疏化不同，其中被置为零的权重矩阵的分量可以位于任何位置。所提出的方法以自然的方式减少神经网络，即在培训模型的同时中和不必要的单位。我们通过实验证明，我们提出的方法在保持性能的同时有效地修剪了神经网络。",
    "tldr": "本文提出了一种称为 “导向性正则化” 的新方法，通过在训练中优先考虑某些神经网络单元的权重，从而使得某些单元不那么重要，从而可以削减神经元，实现神经网络的压缩。",
    "en_tdlr": "This paper proposes a simple new approach named \"Guided Regularization\" that prioritizes the weights of certain NN units more than others during training, which renders some of the units less important and thus, prunable, achieving neural network compression."
}