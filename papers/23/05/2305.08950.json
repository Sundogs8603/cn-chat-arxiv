{
    "title": "Causal Analysis for Robust Interpretability of Neural Networks. (arXiv:2305.08950v1 [cs.LG])",
    "abstract": "Interpreting the inner function of neural networks is crucial for the trustworthy development and deployment of these black-box models. Prior interpretability methods focus on correlation-based measures to attribute model decisions to individual examples. However, these measures are susceptible to noise and spurious correlations encoded in the model during the training phase (e.g., biased inputs, model overfitting, or misspecification). Moreover, this process has proven to result in noisy and unstable attributions that prevent any transparent understanding of the model's behavior. In this paper, we develop a robust interventional-based method grounded by causal analysis to capture cause-effect mechanisms in pre-trained neural networks and their relation to the prediction. Our novel approach relies on path interventions to infer the causal mechanisms within hidden layers and isolate relevant and necessary information (to model prediction), avoiding noisy ones. The result is task-specifi",
    "link": "http://arxiv.org/abs/2305.08950",
    "context": "Title: Causal Analysis for Robust Interpretability of Neural Networks. (arXiv:2305.08950v1 [cs.LG])\nAbstract: Interpreting the inner function of neural networks is crucial for the trustworthy development and deployment of these black-box models. Prior interpretability methods focus on correlation-based measures to attribute model decisions to individual examples. However, these measures are susceptible to noise and spurious correlations encoded in the model during the training phase (e.g., biased inputs, model overfitting, or misspecification). Moreover, this process has proven to result in noisy and unstable attributions that prevent any transparent understanding of the model's behavior. In this paper, we develop a robust interventional-based method grounded by causal analysis to capture cause-effect mechanisms in pre-trained neural networks and their relation to the prediction. Our novel approach relies on path interventions to infer the causal mechanisms within hidden layers and isolate relevant and necessary information (to model prediction), avoiding noisy ones. The result is task-specifi",
    "path": "papers/23/05/2305.08950.json",
    "total_tokens": 854,
    "translated_title": "鲁棒性神经网络因果分析方法用于解释性研究",
    "translated_abstract": "神经网络内部的解释对于这些黑盒模型的可靠开发和部署至关重要。然而，以往的解释方法集中在基于相关性的度量上，以将模型决策归因于个别示例。然而，这些方法容易受到训练阶段中编码在模型中的噪声和虚假相关性的影响（例如，有偏输入，模型过拟合或错配）。此外，这个过程已经证明会产生嘈杂和不稳定的归因，从而阻碍了对模型行为的透明理解。本文开发了一种基于因果分析的鲁棒干预方法，用于捕捉预训练神经网络中的因果机制及其与预测的关系。我们的方法依赖于路径干预，以推断隐藏层中的因果机制并隔离相关和必要的信息（以进行模型预测），从而避免噪音的干扰。结果是针对特定任务的稳健且可靠的解释。",
    "tldr": "本文提出了一种基于因果分析的鲁棒干预方法，用于解释神经网络的决策，避免噪音的干扰。",
    "en_tdlr": "This paper proposes a robust interventional-based method grounded by causal analysis to interpret neural network decisions and avoid noisy interferences."
}