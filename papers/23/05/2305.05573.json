{
    "title": "An Algorithm For Adversary Aware Decentralized Networked MARL. (arXiv:2305.05573v1 [cs.LG])",
    "abstract": "Decentralized multi-agent reinforcement learning (MARL) algorithms have become popular in the literature since it allows heterogeneous agents to have their own reward functions as opposed to canonical multi-agent Markov Decision Process (MDP) settings which assume common reward functions over all agents. In this work, we follow the existing work on collaborative MARL where agents in a connected time varying network can exchange information among each other in order to reach a consensus. We introduce vulnerabilities in the consensus updates of existing MARL algorithms where agents can deviate from their usual consensus update, who we term as adversarial agents. We then proceed to provide an algorithm that allows non-adversarial agents to reach a consensus in the presence of adversaries under a constrained setting.",
    "link": "http://arxiv.org/abs/2305.05573",
    "context": "Title: An Algorithm For Adversary Aware Decentralized Networked MARL. (arXiv:2305.05573v1 [cs.LG])\nAbstract: Decentralized multi-agent reinforcement learning (MARL) algorithms have become popular in the literature since it allows heterogeneous agents to have their own reward functions as opposed to canonical multi-agent Markov Decision Process (MDP) settings which assume common reward functions over all agents. In this work, we follow the existing work on collaborative MARL where agents in a connected time varying network can exchange information among each other in order to reach a consensus. We introduce vulnerabilities in the consensus updates of existing MARL algorithms where agents can deviate from their usual consensus update, who we term as adversarial agents. We then proceed to provide an algorithm that allows non-adversarial agents to reach a consensus in the presence of adversaries under a constrained setting.",
    "path": "papers/23/05/2305.05573.json",
    "total_tokens": 803,
    "translated_title": "一种对抗感知的去中心化网络多智能体强化学习算法",
    "translated_abstract": "去中心化的多智能体强化学习算法已经在文献中变得流行，因为它允许异构体拥有自己的奖励函数，相对于假定所有智能体拥有共同奖励函数的经典多智能体马尔可夫决策过程(MDP)设置。在本文中，我们遵循现有合作MARL的工作，在一个连接的时变网络中，智能体可以相互交换信息以达成共识。我们在共识更新中引入漏洞，在现有的MARL算法中，智能体可以偏离其常规的共识更新，我们称之为对抗性智能体。然后，我们提供了一种算法，使非对抗性智能体可以在受到约束条件限制的情况下，在对抗性存在的情况下达成共识。",
    "tldr": "本文提出了一种对抗感知的去中心化网络多智能体强化学习算法，该算法允许非对抗性智能体在对抗方存在的情况下达成共识。",
    "en_tdlr": "This paper proposes an adversary aware decentralized networked multi-agent reinforcement learning (MARL) algorithm, which allows non-adversarial agents to reach a consensus in the presence of adversaries under a constrained setting."
}