{
    "title": "Eliminating Spurious Correlations from Pre-trained Models via Data Mixing. (arXiv:2305.14521v1 [cs.LG])",
    "abstract": "Machine learning models pre-trained on large datasets have achieved remarkable convergence and robustness properties. However, these models often exploit spurious correlations between certain attributes and labels, which are prevalent in the majority of examples within specific categories but are not predictive of these categories in general. The learned spurious correlations may persist even after fine-tuning on new data, which degrades models' performance on examples that do not exhibit the spurious correlation. In this work, we propose a simple and highly effective method to eliminate spurious correlations from pre-trained models. The key idea of our method is to leverage a small set of examples with spurious attributes, and balance the spurious attributes across all classes via data mixing. We theoretically confirm the effectiveness of our method, and empirically demonstrate its state-of-the-art performance on various vision and NLP tasks, including eliminating spurious correlation",
    "link": "http://arxiv.org/abs/2305.14521",
    "context": "Title: Eliminating Spurious Correlations from Pre-trained Models via Data Mixing. (arXiv:2305.14521v1 [cs.LG])\nAbstract: Machine learning models pre-trained on large datasets have achieved remarkable convergence and robustness properties. However, these models often exploit spurious correlations between certain attributes and labels, which are prevalent in the majority of examples within specific categories but are not predictive of these categories in general. The learned spurious correlations may persist even after fine-tuning on new data, which degrades models' performance on examples that do not exhibit the spurious correlation. In this work, we propose a simple and highly effective method to eliminate spurious correlations from pre-trained models. The key idea of our method is to leverage a small set of examples with spurious attributes, and balance the spurious attributes across all classes via data mixing. We theoretically confirm the effectiveness of our method, and empirically demonstrate its state-of-the-art performance on various vision and NLP tasks, including eliminating spurious correlation",
    "path": "papers/23/05/2305.14521.json",
    "total_tokens": 853,
    "translated_title": "通过数据混合消除预训练模型中的虚假相关性",
    "translated_abstract": "在大数据集上预训练的机器学习模型取得了显著的收敛性和鲁棒性。然而，这些模型往往利用了某些属性和标签之间的虚假相关性，在特定类别的大多数示例中普遍存在，但并不足以预测这些类别。学到的虚假相关性可能会在对新数据进行微调后仍然存在，这会降低模型对不展现虚假相关性的示例的性能。本文提出了一种简单而高效的方法，以消除预训练模型中的虚假相关性。我们方法的关键思想是利用一小组带有虚假属性的示例，并通过数据混合来平衡所有类别中的虚假属性。我们在理论上证实了我们的方法的有效性，并在各种视觉和NLP任务上进行了实证，包括消除虚假相关性。",
    "tldr": "本文提出了一种通过数据混合来消除预训练模型中虚假相关性的方法，来提高模型对于新样本的预测能力。这种方法经过理论证明和多种任务实验验证，可以取得良好的效果。",
    "en_tdlr": "This paper proposes a method to eliminate spurious correlations from pre-trained models by leveraging data mixing, which has been theoretically confirmed and experimentally demonstrated to be highly effective in improving models' prediction performance on new samples."
}