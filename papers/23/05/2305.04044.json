{
    "title": "Diffusion-NAT: Self-Prompting Discrete Diffusion for Non-Autoregressive Text Generation. (arXiv:2305.04044v1 [cs.CL])",
    "abstract": "Recently, continuous diffusion models (CDM) have been introduced into non-autoregressive (NAR) text-to-text generation. However, the discrete nature of text increases the difficulty of CDM to generate coherent and fluent texts, and also causes the incompatibility problem between CDM and advanced NLP techniques, especially the popular pre-trained language models~(PLMs). To solve it, we propose Diffusion-NAT, which introduces discrete diffusion models~(DDM) into NAR text-to-text generation and integrates BART to improve the performance. By revising the decoding process of BART and the typical settings of DDM, we unify the inference process of BART and the denoising process of DDM into the same NAR masked tokens recovering task. In this way, DDM can rely on BART to perform denoising, which can benefit from both the rich pre-learned knowledge of BART and the iterative refining paradigm of DDM. Besides, we also propose the iterative self-prompting strategy to further improve the generation ",
    "link": "http://arxiv.org/abs/2305.04044",
    "context": "Title: Diffusion-NAT: Self-Prompting Discrete Diffusion for Non-Autoregressive Text Generation. (arXiv:2305.04044v1 [cs.CL])\nAbstract: Recently, continuous diffusion models (CDM) have been introduced into non-autoregressive (NAR) text-to-text generation. However, the discrete nature of text increases the difficulty of CDM to generate coherent and fluent texts, and also causes the incompatibility problem between CDM and advanced NLP techniques, especially the popular pre-trained language models~(PLMs). To solve it, we propose Diffusion-NAT, which introduces discrete diffusion models~(DDM) into NAR text-to-text generation and integrates BART to improve the performance. By revising the decoding process of BART and the typical settings of DDM, we unify the inference process of BART and the denoising process of DDM into the same NAR masked tokens recovering task. In this way, DDM can rely on BART to perform denoising, which can benefit from both the rich pre-learned knowledge of BART and the iterative refining paradigm of DDM. Besides, we also propose the iterative self-prompting strategy to further improve the generation ",
    "path": "papers/23/05/2305.04044.json",
    "total_tokens": 921,
    "translated_title": "Diffusion-NAT: 自我启发离散扩散的非自回归文本生成",
    "translated_abstract": "最近，将连续扩散模型引入非自回归文本生成中，但文本的离散性增加了生成连贯和流畅文本的难度，并引起了扩散模型与NLP技术中的兼容性问题。为了解决这个问题，我们提出了Diffusion-NAT，将离散扩散模型引入非自回归文本生成中，并集成BART来提高性能。通过修改BART解码过程和DDM的典型设置，我们将BART的推理过程和DDM的去噪过程统一为相同的NAR掩码令牌还原任务。这样，DDM就可以依靠BART执行去噪，既可以从BART的丰富预学习知识中受益，也可以从DDM的迭代细化范例中受益。此外，我们还提出了迭代的自我提示策略来进一步提高生成质量。",
    "tldr": "本文提出了Diffusion-NAT，将离散扩散模型引入非自回归文本生成中，并结合BART实现统一的推理和去噪过程，在此基础上提出了迭代的自我提示策略来进一步提高生成质量。",
    "en_tdlr": "The paper introduces Diffusion-NAT, which combines discrete diffusion models with non-autoregressive text generation and integrates BART to unify inference and denoising processes. The iterative self-prompting strategy is also proposed to further improve the generation quality."
}