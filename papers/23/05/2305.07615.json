{
    "title": "What are the Desired Characteristics of Calibration Sets? Identifying Correlates on Long Form Scientific Summarization. (arXiv:2305.07615v1 [cs.CL])",
    "abstract": "Summarization models often generate text that is poorly calibrated to quality metrics because they are trained to maximize the likelihood of a single reference (MLE). To address this, recent work has added a calibration step, which exposes a model to its own ranked outputs to improve relevance or, in a separate line of work, contrasts positive and negative sets to improve faithfulness. While effective, much of this work has focused on how to generate and optimize these sets. Less is known about why one setup is more effective than another. In this work, we uncover the underlying characteristics of effective sets. For each training instance, we form a large, diverse pool of candidates and systematically vary the subsets used for calibration fine-tuning. Each selection strategy targets distinct aspects of the sets, such as lexical diversity or the size of the gap between positive and negatives. On three diverse scientific long-form summarization datasets (spanning biomedical, clinical, a",
    "link": "http://arxiv.org/abs/2305.07615",
    "context": "Title: What are the Desired Characteristics of Calibration Sets? Identifying Correlates on Long Form Scientific Summarization. (arXiv:2305.07615v1 [cs.CL])\nAbstract: Summarization models often generate text that is poorly calibrated to quality metrics because they are trained to maximize the likelihood of a single reference (MLE). To address this, recent work has added a calibration step, which exposes a model to its own ranked outputs to improve relevance or, in a separate line of work, contrasts positive and negative sets to improve faithfulness. While effective, much of this work has focused on how to generate and optimize these sets. Less is known about why one setup is more effective than another. In this work, we uncover the underlying characteristics of effective sets. For each training instance, we form a large, diverse pool of candidates and systematically vary the subsets used for calibration fine-tuning. Each selection strategy targets distinct aspects of the sets, such as lexical diversity or the size of the gap between positive and negatives. On three diverse scientific long-form summarization datasets (spanning biomedical, clinical, a",
    "path": "papers/23/05/2305.07615.json",
    "total_tokens": 1015,
    "translated_title": "标题：校准集的期望特点是什么？确定长篇科学摘要的相关因素。",
    "translated_abstract": "摘要：总结模型通常会生成与质量指标不匹配的文本，因为它们的训练是为了最大化单个参考的可能性。为了解决这个问题，最近的工作加入了一个校准步骤，让模型暴露在它自己的排名输出中，以提高相关性或改进忠实度。虽然这是有效的，但很多工作都集中在如何生成和优化这些集合上。关于为什么一种设置比另一种更有效，我们知之甚少。在这项工作中，我们揭示了有效集的基本特征。对于每个训练实例，我们形成了一个庞大、多样化的候选人池，并系统地变化了用于校准微调的子集。每种选择策略都针对集合的不同方面进行，例如词汇多样性或正负之间的差距大小。在三个不同的科学长篇摘要数据集上（涵盖生物医学、临床和COVID-19领域），我们发现最佳设置的共同特点是在校准前将正负分开，为负例选择谨慎而大胆地对待正例。",
    "tldr": "现有的总结模型由于训练过程中单个参考的可能性，生成的文本与质量指标不匹配。本文通过对不同校准集的研究，找出了最佳设置的共同特点，即在校准前将正负分开，谨慎选择负例，大胆对待正例。"
}