{
    "title": "A Simplified Framework for Contrastive Learning for Node Representations. (arXiv:2305.00623v1 [cs.LG])",
    "abstract": "Contrastive learning has recently established itself as a powerful self-supervised learning framework for extracting rich and versatile data representations. Broadly speaking, contrastive learning relies on a data augmentation scheme to generate two versions of the input data and learns low-dimensional representations by maximizing a normalized temperature-scaled cross entropy loss (NT-Xent) to identify augmented samples corresponding to the same original entity. In this paper, we investigate the potential of deploying contrastive learning in combination with Graph Neural Networks for embedding nodes in a graph. Specifically, we show that the quality of the resulting embeddings and training time can be significantly improved by a simple column-wise postprocessing of the embedding matrix, instead of the row-wise postprocessing via multilayer perceptrons (MLPs) that is adopted by the majority of peer methods. This modification yields improvements in downstream classification tasks of up ",
    "link": "http://arxiv.org/abs/2305.00623",
    "context": "Title: A Simplified Framework for Contrastive Learning for Node Representations. (arXiv:2305.00623v1 [cs.LG])\nAbstract: Contrastive learning has recently established itself as a powerful self-supervised learning framework for extracting rich and versatile data representations. Broadly speaking, contrastive learning relies on a data augmentation scheme to generate two versions of the input data and learns low-dimensional representations by maximizing a normalized temperature-scaled cross entropy loss (NT-Xent) to identify augmented samples corresponding to the same original entity. In this paper, we investigate the potential of deploying contrastive learning in combination with Graph Neural Networks for embedding nodes in a graph. Specifically, we show that the quality of the resulting embeddings and training time can be significantly improved by a simple column-wise postprocessing of the embedding matrix, instead of the row-wise postprocessing via multilayer perceptrons (MLPs) that is adopted by the majority of peer methods. This modification yields improvements in downstream classification tasks of up ",
    "path": "papers/23/05/2305.00623.json",
    "total_tokens": 854,
    "translated_title": "用于节点表示的对比学习简化框架",
    "translated_abstract": "最近对比学习已经被证明为一种提取丰富多样的数据表示的有力自监督学习框架。广义上讲，对比学习依赖于数据增强方案生成输入数据的两个版本，并通过最大化归一化温度缩放交叉熵损失（NT-Xent）来学习低维度表示以识别对应于同一原始实体的增强样本。在本文中，我们研究了将对比学习与图神经网络相结合在图中嵌入节点的潜力。具体而言，我们表明，通过简单的列处理嵌入矩阵，而不是同行处理，可以显著提高结果嵌入的质量和训练时间，而同行处理是大多数同行方法采用的多层感知器（MLPs）。这种修改可提高下游分类任务的性能。",
    "tldr": "本文研究了对比学习和图神经网络相结合的节点嵌入方法，通过简单的列处理嵌入矩阵取代同行处理，在提高结果嵌入质量和训练时间的同时，提高了下游分类任务的性能。",
    "en_tdlr": "This paper investigates the potential of deploying contrastive learning in combination with Graph Neural Networks for embedding nodes in a graph, and shows that a simple column-wise postprocessing of the embedding matrix can significantly improve the quality of the resulting embeddings and training time, yielding improvements in downstream classification tasks."
}