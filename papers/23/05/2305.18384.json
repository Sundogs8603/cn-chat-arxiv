{
    "title": "Backdoor Attacks Against Incremental Learners: An Empirical Evaluation Study. (arXiv:2305.18384v1 [cs.CR])",
    "abstract": "Large amounts of incremental learning algorithms have been proposed to alleviate the catastrophic forgetting issue arises while dealing with sequential data on a time series. However, the adversarial robustness of incremental learners has not been widely verified, leaving potential security risks. Specifically, for poisoning-based backdoor attacks, we argue that the nature of streaming data in IL provides great convenience to the adversary by creating the possibility of distributed and cross-task attacks -- an adversary can affect \\textbf{any unknown} previous or subsequent task by data poisoning \\textbf{at any time or time series} with extremely small amount of backdoor samples injected (e.g., $0.1\\%$ based on our observations). To attract the attention of the research community, in this paper, we empirically reveal the high vulnerability of 11 typical incremental learners against poisoning-based backdoor attack on 3 learning scenarios, especially the cross-task generalization effect ",
    "link": "http://arxiv.org/abs/2305.18384",
    "context": "Title: Backdoor Attacks Against Incremental Learners: An Empirical Evaluation Study. (arXiv:2305.18384v1 [cs.CR])\nAbstract: Large amounts of incremental learning algorithms have been proposed to alleviate the catastrophic forgetting issue arises while dealing with sequential data on a time series. However, the adversarial robustness of incremental learners has not been widely verified, leaving potential security risks. Specifically, for poisoning-based backdoor attacks, we argue that the nature of streaming data in IL provides great convenience to the adversary by creating the possibility of distributed and cross-task attacks -- an adversary can affect \\textbf{any unknown} previous or subsequent task by data poisoning \\textbf{at any time or time series} with extremely small amount of backdoor samples injected (e.g., $0.1\\%$ based on our observations). To attract the attention of the research community, in this paper, we empirically reveal the high vulnerability of 11 typical incremental learners against poisoning-based backdoor attack on 3 learning scenarios, especially the cross-task generalization effect ",
    "path": "papers/23/05/2305.18384.json",
    "total_tokens": 894,
    "translated_title": "增量学习器的后门攻击：一项实证评估研究",
    "translated_abstract": "已经提出了大量的增量学习算法，以缓解在时间序列上处理顺序数据时出现的灾难性遗忘问题。然而，增量学习器的对抗鲁棒性尚未得到广泛验证，存在潜在的安全风险。具体而言，对于基于中毒的后门攻击，我们认为增量学习中流式数据的本质为对手提供了很大的便利，通过数据污染，对手可以在任何时间或时间序列上创建分布式和跨任务攻击，从而影响任何未知的先前或后续任务，并且仅需要注入极小数量的后门样本 (例如，根据我们的观察，仅需要注入0.1%）。为了吸引研究社区的关注，在本文中，我们从三个学习场景出发，实证揭示了11个典型的增量学习器对基于中毒的后门攻击的高度脆弱性，特别是跨任务泛化效应。",
    "tldr": "本文提出了增量学习器的后门攻击可能存在的安全风险，并实证揭示了11个典型的增量学习器对基于中毒的后门攻击的高度脆弱性。",
    "en_tdlr": "This paper highlights the potential security risks of backdoor attacks against incremental learners and empirically demonstrates the high vulnerability of 11 typical incremental learners against poisoning-based backdoor attacks, especially the cross-task generalization effect."
}