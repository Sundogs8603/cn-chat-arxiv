{
    "title": "Mitigating Approximate Memorization in Language Models via Dissimilarity Learned Policy. (arXiv:2305.01550v1 [cs.CL])",
    "abstract": "Large Language models (LLMs) are trained on large amounts of data, which can include sensitive information that may compromise per- sonal privacy. LLMs showed to memorize parts of the training data and emit those data verbatim when an adversary prompts appropriately. Previous research has primarily focused on data preprocessing and differential privacy techniques to address memorization or prevent verbatim memorization exclusively, which can give a false sense of privacy. However, these methods rely on explicit and implicit assumptions about the structure of the data to be protected, which often results in an incomplete solution to the problem. To address this, we propose a novel framework that utilizes a reinforcement learning approach (PPO) to fine-tune LLMs to mitigate approximate memorization. Our approach utilizes a negative similarity score, such as BERTScore or SacreBLEU, as a reward signal to learn a dissimilarity policy. Our results demonstrate that this framework effectively ",
    "link": "http://arxiv.org/abs/2305.01550",
    "context": "Title: Mitigating Approximate Memorization in Language Models via Dissimilarity Learned Policy. (arXiv:2305.01550v1 [cs.CL])\nAbstract: Large Language models (LLMs) are trained on large amounts of data, which can include sensitive information that may compromise per- sonal privacy. LLMs showed to memorize parts of the training data and emit those data verbatim when an adversary prompts appropriately. Previous research has primarily focused on data preprocessing and differential privacy techniques to address memorization or prevent verbatim memorization exclusively, which can give a false sense of privacy. However, these methods rely on explicit and implicit assumptions about the structure of the data to be protected, which often results in an incomplete solution to the problem. To address this, we propose a novel framework that utilizes a reinforcement learning approach (PPO) to fine-tune LLMs to mitigate approximate memorization. Our approach utilizes a negative similarity score, such as BERTScore or SacreBLEU, as a reward signal to learn a dissimilarity policy. Our results demonstrate that this framework effectively ",
    "path": "papers/23/05/2305.01550.json",
    "total_tokens": 906,
    "translated_title": "通过学习的差异策略缓解语言模型中的近似记忆化问题",
    "translated_abstract": "大型语言模型（LLMs）是通过大量数据进行训练的，其中可能包含会危害个人隐私的敏感信息。LLMs被证明会记忆训练数据的部分内容并在遇到对应提示时直接输出该数据。先前的研究主要集中在数据预处理和差分隐私技术上以解决记忆化问题，但这些方法都依赖于对要保护数据结构的显式和隐式假设，导致问题解决不完全。为了解决这个问题，我们提出了一种新的框架，利用强化学习方法（PPO）微调LLMs以缓解近似记忆化问题。我们的方法使用负相似度评分（例如BERTScore或SacreBLEU）作为奖励信号来学习差异策略。我们的结果表明，这种框架有效地缓解了近似记忆化问题。",
    "tldr": "本论文提出了一种利用强化学习方法缓解语言模型近似记忆化问题的新框架，使用负相似度评分作为奖励信号来学习差异策略。该方法能够有效解决显式和隐式假设所导致的数据结构不完全的问题。",
    "en_tdlr": "This paper proposes a new framework to mitigate the problem of approximate memorization in language models using reinforcement learning with a negative similarity score as a reward signal. The framework effectively solves the problem caused by incomplete data structure assumptions."
}