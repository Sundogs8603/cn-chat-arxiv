{
    "title": "Zero-Shot End-to-End Spoken Language Understanding via Cross-Modal Selective Self-Training",
    "abstract": "End-to-end (E2E) spoken language understanding (SLU) is constrained by the cost of collecting speech-semantics pairs, especially when label domains change. Hence, we explore \\textit{zero-shot} E2E SLU, which learns E2E SLU without speech-semantics pairs, instead using only speech-text and text-semantics pairs. Previous work achieved zero-shot by pseudolabeling all speech-text transcripts with a natural language understanding (NLU) model learned on text-semantics corpora. However, this method requires the domains of speech-text and text-semantics to match, which often mismatch due to separate collections. Furthermore, using the entire collected speech-text corpus from any domains leads to \\textit{imbalance} and \\textit{noise} issues. To address these, we propose \\textit{cross-modal selective self-training} (CMSST). CMSST tackles imbalance by clustering in a joint space of the three modalities (speech, text, and semantics) and handles label noise with a selection network. We also introdu",
    "link": "https://arxiv.org/abs/2305.12793",
    "context": "Title: Zero-Shot End-to-End Spoken Language Understanding via Cross-Modal Selective Self-Training\nAbstract: End-to-end (E2E) spoken language understanding (SLU) is constrained by the cost of collecting speech-semantics pairs, especially when label domains change. Hence, we explore \\textit{zero-shot} E2E SLU, which learns E2E SLU without speech-semantics pairs, instead using only speech-text and text-semantics pairs. Previous work achieved zero-shot by pseudolabeling all speech-text transcripts with a natural language understanding (NLU) model learned on text-semantics corpora. However, this method requires the domains of speech-text and text-semantics to match, which often mismatch due to separate collections. Furthermore, using the entire collected speech-text corpus from any domains leads to \\textit{imbalance} and \\textit{noise} issues. To address these, we propose \\textit{cross-modal selective self-training} (CMSST). CMSST tackles imbalance by clustering in a joint space of the three modalities (speech, text, and semantics) and handles label noise with a selection network. We also introdu",
    "path": "papers/23/05/2305.12793.json",
    "total_tokens": 937,
    "translated_title": "通过跨模态选择性自训练实现零射击端到端口语理解",
    "translated_abstract": "现有的端到端口语理解 (SLU) 受到了收集语音-语义对的成本的约束，特别是当标签域发生变化时。因此，我们探索了\"零射击\"的端到端口语理解，即在没有语音-语义对的情况下学习端到端口语理解，而是仅使用语音-文本和文本-语义对。以前的工作通过使用在文本-语义语料库上学到的自然语言理解 (NLU) 模型对所有的语音-文本转录进行伪标签化来实现零射击。然而，这种方法要求语音-文本和文本-语义的领域匹配，而由于采集不同的语料库，通常会出现不匹配的情况。此外，使用从任何领域收集的整个语音-文本语料库会导致\"不平衡\"和\"噪声\"问题。为了解决这些问题，我们提出了\"跨模态选择性自训练\" (CMSST)。CMSST通过在三种模态 (语音、文本和语义) 的联合空间中进行聚类来解决不平衡问题，并借助选择网络来处理标签噪声。",
    "tldr": "本文提出了一种通过跨模态选择性自训练的方法，以解决在零射击端到端口语理解中的不平衡和标签噪声问题。",
    "en_tdlr": "This paper proposes a method of cross-modal selective self-training to address the issues of imbalance and label noise in zero-shot end-to-end spoken language understanding."
}