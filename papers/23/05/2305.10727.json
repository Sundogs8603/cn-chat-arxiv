{
    "title": "Boost Vision Transformer with GPU-Friendly Sparsity and Quantization. (arXiv:2305.10727v1 [cs.CV])",
    "abstract": "The transformer extends its success from the language to the vision domain. Because of the stacked self-attention and cross-attention blocks, the acceleration deployment of vision transformer on GPU hardware is challenging and also rarely studied. This paper thoroughly designs a compression scheme to maximally utilize the GPU-friendly 2:4 fine-grained structured sparsity and quantization. Specially, an original large model with dense weight parameters is first pruned into a sparse one by 2:4 structured pruning, which considers the GPU's acceleration of 2:4 structured sparse pattern with FP16 data type, then the floating-point sparse model is further quantized into a fixed-point one by sparse-distillation-aware quantization aware training, which considers GPU can provide an extra speedup of 2:4 sparse calculation with integer tensors. A mixed-strategy knowledge distillation is used during the pruning and quantization process. The proposed compression scheme is flexible to support superv",
    "link": "http://arxiv.org/abs/2305.10727",
    "context": "Title: Boost Vision Transformer with GPU-Friendly Sparsity and Quantization. (arXiv:2305.10727v1 [cs.CV])\nAbstract: The transformer extends its success from the language to the vision domain. Because of the stacked self-attention and cross-attention blocks, the acceleration deployment of vision transformer on GPU hardware is challenging and also rarely studied. This paper thoroughly designs a compression scheme to maximally utilize the GPU-friendly 2:4 fine-grained structured sparsity and quantization. Specially, an original large model with dense weight parameters is first pruned into a sparse one by 2:4 structured pruning, which considers the GPU's acceleration of 2:4 structured sparse pattern with FP16 data type, then the floating-point sparse model is further quantized into a fixed-point one by sparse-distillation-aware quantization aware training, which considers GPU can provide an extra speedup of 2:4 sparse calculation with integer tensors. A mixed-strategy knowledge distillation is used during the pruning and quantization process. The proposed compression scheme is flexible to support superv",
    "path": "papers/23/05/2305.10727.json",
    "total_tokens": 912,
    "translated_title": "利用GPU友好的稀疏化和量化优化Vision Transformer",
    "translated_abstract": "Transformer 模型已经在自然语言处理领域得到广泛应用，在计算机视觉领域也备受关注。由于其堆叠的自注意力和交叉注意力块，将 Vision Transformer 部署到 GPU 上加速运行是具有挑战性的，目前研究较少。本文设计了一种压缩方案，最大限度利用了基于 2:4 细粒度结构稀疏性和量化的 GPU 友好性。通过 2:4 结构化稀疏剪枝将一个原始的大模型剪枝成稀疏模型，利用 FP16 数据类型对稀疏模型进行优化，然后通过基于稀疏蒸馏感知的量化训练将浮点稀疏模型进一步量化为固定点模型，利用整数张量计算对其进行优化，实现额外的 2:4 稀疏计算加速。在稀疏化和量化过程中使用混合策略知识蒸馏。所提出的压缩方案灵活，支持监督式微调。",
    "tldr": "本论文介绍了一种针对 Vision Transformer 模型的压缩方案，通过 2:4 结构化稀疏剪枝和基于稀疏蒸馏感知的量化训练实现了GPU加速.",
    "en_tdlr": "This paper proposes a compression scheme for Vision Transformer models, which achieves GPU acceleration through 2:4 structured pruning and sparse-distillation-aware quantization aware training."
}