{
    "title": "Label Agnostic Pre-training for Zero-shot Text Classification. (arXiv:2305.16521v1 [cs.CL])",
    "abstract": "Conventional approaches to text classification typically assume the existence of a fixed set of predefined labels to which a given text can be classified. However, in real-world applications, there exists an infinite label space for describing a given text. In addition, depending on the aspect (sentiment, topic, etc.) and domain of the text (finance, legal, etc.), the interpretation of the label can vary greatly. This makes the task of text classification, particularly in the zero-shot scenario, extremely challenging. In this paper, we investigate the task of zero-shot text classification with the aim of improving the ability of pre-trained language models (PLMs) to generalize to both seen and unseen data across varying aspects and domains. To solve this we introduce two new simple yet effective pre-training strategies, Implicit and Explicit pre-training. These methods inject aspect-level understanding into the model at train time with the goal of conditioning the model to build task-l",
    "link": "http://arxiv.org/abs/2305.16521",
    "context": "Title: Label Agnostic Pre-training for Zero-shot Text Classification. (arXiv:2305.16521v1 [cs.CL])\nAbstract: Conventional approaches to text classification typically assume the existence of a fixed set of predefined labels to which a given text can be classified. However, in real-world applications, there exists an infinite label space for describing a given text. In addition, depending on the aspect (sentiment, topic, etc.) and domain of the text (finance, legal, etc.), the interpretation of the label can vary greatly. This makes the task of text classification, particularly in the zero-shot scenario, extremely challenging. In this paper, we investigate the task of zero-shot text classification with the aim of improving the ability of pre-trained language models (PLMs) to generalize to both seen and unseen data across varying aspects and domains. To solve this we introduce two new simple yet effective pre-training strategies, Implicit and Explicit pre-training. These methods inject aspect-level understanding into the model at train time with the goal of conditioning the model to build task-l",
    "path": "papers/23/05/2305.16521.json",
    "total_tokens": 895,
    "translated_title": "面向零样本文本分类的标签无关预训练",
    "translated_abstract": "传统的文本分类方法通常假设存在一组固定的预定义标签，用于将给定的文本分类。然而，在现实世界的应用中，存在着用于描述给定文本的无限标签空间。此外，根据文本的方面（情感、主题等）和领域（金融、法律等），标签的解释可能大不相同。这使得文本分类任务，特别是在零样本场景下，变得非常具有挑战性。在本文中，我们探讨了零样本文本分类的任务，旨在提高预训练语言模型（PLMs）对不同方面和领域中已知和未知数据的泛化能力。为了解决这个问题，我们引入了两种新的简单而有效的预训练策略，即隐式和显式预训练。这些方法在训练时注入了方面级别的理解，目的是让模型构建任务层次的表示。",
    "tldr": "本文旨在探究改进预训练语言模型的泛化能力，提高其在零样本情境下的文本分类表现。通过引入隐式和显式预训练策略，注入方面级别的理解，以建立任务层次的表示。",
    "en_tdlr": "This paper aims to improve the ability of pre-trained language models in generalizing to seen and unseen data for zero-shot text classification. Two new pre-training strategies, Implicit and Explicit pre-training, are introduced to inject aspect-level understanding and build task-level representations."
}