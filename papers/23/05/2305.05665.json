{
    "title": "ImageBind: One Embedding Space To Bind Them All. (arXiv:2305.05665v1 [cs.CV])",
    "abstract": "We present ImageBind, an approach to learn a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data. We show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together. ImageBind can leverage recent large scale vision-language models, and extends their zero-shot capabilities to new modalities just by using their natural pairing with images. It enables novel emergent applications 'out-of-the-box' including cross-modal retrieval, composing modalities with arithmetic, cross-modal detection and generation. The emergent capabilities improve with the strength of the image encoder and we set a new state-of-the-art on emergent zero-shot recognition tasks across modalities, outperforming specialist supervised models. Finally, we show strong few-shot recognition results outperforming prior work, and that ImageBind serves as a new way to evaluate visio",
    "link": "http://arxiv.org/abs/2305.05665",
    "context": "Title: ImageBind: One Embedding Space To Bind Them All. (arXiv:2305.05665v1 [cs.CV])\nAbstract: We present ImageBind, an approach to learn a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data. We show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together. ImageBind can leverage recent large scale vision-language models, and extends their zero-shot capabilities to new modalities just by using their natural pairing with images. It enables novel emergent applications 'out-of-the-box' including cross-modal retrieval, composing modalities with arithmetic, cross-modal detection and generation. The emergent capabilities improve with the strength of the image encoder and we set a new state-of-the-art on emergent zero-shot recognition tasks across modalities, outperforming specialist supervised models. Finally, we show strong few-shot recognition results outperforming prior work, and that ImageBind serves as a new way to evaluate visio",
    "path": "papers/23/05/2305.05665.json",
    "total_tokens": 882,
    "translated_title": "ImageBind:一个共同嵌入空间绑定所有模态的方法",
    "translated_abstract": "我们提出了ImageBind，这是一种跨越图像、文本、音频、深度、热传感和IMU数据的六种不同模态的联合嵌入方法。我们展示，不需要训练所有配对数据，只需要图像配对数据就足以将这些模态绑定在一起。ImageBind可以利用最近的大规模视觉-语言模型，并通过使用它们与图像的自然配对，将它们的零样本能力扩展到新的模态。它可以实现“开箱即用”的新型应用程序，包括跨模态检索、用算术组合模态、跨模态检测和生成。新型应用随着图像编码器的强度而不断改进，我们在跨模态的零样本识别任务上取得了新的最优成绩，超过了专家监督模型。最后，我们还展示了强的几何识别结果，超过了以前的工作，ImageBind成为了评估视觉模态联合学习的一种新方法。",
    "tldr": "ImageBind是一种新的跨模态联合嵌入方法，只需要使用图像配对数据就可以将不同模态的数据绑定在一起，并实现跨模态检索、组合和生成等多种应用。",
    "en_tdlr": "ImageBind is a novel approach for joint embedding of different modalities, which can bind them together by only using image-paired data, and enables various applications such as cross-modal retrieval and generation."
}