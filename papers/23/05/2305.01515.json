{
    "title": "MTrainS: Improving DLRM training efficiency using heterogeneous memories. (arXiv:2305.01515v1 [cs.IR])",
    "abstract": "Recommendation models are very large, requiring terabytes (TB) of memory during training. In pursuit of better quality, the model size and complexity grow over time, which requires additional training data to avoid overfitting. This model growth demands a large number of resources in data centers. Hence, training efficiency is becoming considerably more important to keep the data center power demand manageable. In Deep Learning Recommendation Models (DLRM), sparse features capturing categorical inputs through embedding tables are the major contributors to model size and require high memory bandwidth. In this paper, we study the bandwidth requirement and locality of embedding tables in real-world deployed models. We observe that the bandwidth requirement is not uniform across different tables and that embedding tables show high temporal locality. We then design MTrainS, which leverages heterogeneous memory, including byte and block addressable Storage Class Memory for DLRM hierarchicall",
    "link": "http://arxiv.org/abs/2305.01515",
    "context": "Title: MTrainS: Improving DLRM training efficiency using heterogeneous memories. (arXiv:2305.01515v1 [cs.IR])\nAbstract: Recommendation models are very large, requiring terabytes (TB) of memory during training. In pursuit of better quality, the model size and complexity grow over time, which requires additional training data to avoid overfitting. This model growth demands a large number of resources in data centers. Hence, training efficiency is becoming considerably more important to keep the data center power demand manageable. In Deep Learning Recommendation Models (DLRM), sparse features capturing categorical inputs through embedding tables are the major contributors to model size and require high memory bandwidth. In this paper, we study the bandwidth requirement and locality of embedding tables in real-world deployed models. We observe that the bandwidth requirement is not uniform across different tables and that embedding tables show high temporal locality. We then design MTrainS, which leverages heterogeneous memory, including byte and block addressable Storage Class Memory for DLRM hierarchicall",
    "path": "papers/23/05/2305.01515.json",
    "total_tokens": 899,
    "translated_title": "MTrainS: 使用异构内存提高DLRM训练效率",
    "translated_abstract": "推荐模型非常庞大，在训练时需要使用几TB的内存。为了获得更好的质量，模型的大小和复杂度随着时间的推移而增长，这需要更多的训练数据以避免过拟合。这种模型增长要求数据中心大量资源。因此，训练效率变得越来越重要，以保持数据中心的功率需求可控。在深度学习推荐模型(DLRM)中，通过嵌入表捕捉分类输入的稀疏特征是模型大小的主要贡献者，并且需要高内存带宽。在本文中，我们研究了现实中部署模型中嵌入表的带宽需求和局部性。我们观察到，不同表的带宽要求不均匀，并且嵌入表显示出高时序局部性。然后，我们设计了MTrainS，利用异构内存，包括字节和块可寻址存储类内存，用于DLRM的分层训练。",
    "tldr": "本文旨在研究现实中部署的深度学习推荐模型中嵌入表的带宽需求和局部性，并通过使用异构内存提出MTrainS来提高DLRM训练效率。",
    "en_tdlr": "This paper aims to study the bandwidth requirement and locality of embedding tables in real-world deployed deep learning recommendation models and proposes MTrainS to improve DLRM training efficiency by leveraging heterogeneous memories such as byte and block addressable Storage Class Memory for hierarchical training."
}