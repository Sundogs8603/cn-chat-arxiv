{
    "title": "Clip21: Error Feedback for Gradient Clipping. (arXiv:2305.18929v1 [cs.LG])",
    "abstract": "Motivated by the increasing popularity and importance of large-scale training under differential privacy (DP) constraints, we study distributed gradient methods with gradient clipping, i.e., clipping applied to the gradients computed from local information at the nodes. While gradient clipping is an essential tool for injecting formal DP guarantees into gradient-based methods [1], it also induces bias which causes serious convergence issues specific to the distributed setting. Inspired by recent progress in the error-feedback literature which is focused on taming the bias/error introduced by communication compression operators such as Top-$k$ [2], and mathematical similarities between the clipping operator and contractive compression operators, we design Clip21 -- the first provably effective and practically useful error feedback mechanism for distributed methods with gradient clipping. We prove that our method converges at the same $\\mathcal{O}\\left(\\frac{1}{K}\\right)$ rate as distrib",
    "link": "http://arxiv.org/abs/2305.18929",
    "context": "Title: Clip21: Error Feedback for Gradient Clipping. (arXiv:2305.18929v1 [cs.LG])\nAbstract: Motivated by the increasing popularity and importance of large-scale training under differential privacy (DP) constraints, we study distributed gradient methods with gradient clipping, i.e., clipping applied to the gradients computed from local information at the nodes. While gradient clipping is an essential tool for injecting formal DP guarantees into gradient-based methods [1], it also induces bias which causes serious convergence issues specific to the distributed setting. Inspired by recent progress in the error-feedback literature which is focused on taming the bias/error introduced by communication compression operators such as Top-$k$ [2], and mathematical similarities between the clipping operator and contractive compression operators, we design Clip21 -- the first provably effective and practically useful error feedback mechanism for distributed methods with gradient clipping. We prove that our method converges at the same $\\mathcal{O}\\left(\\frac{1}{K}\\right)$ rate as distrib",
    "path": "papers/23/05/2305.18929.json",
    "total_tokens": 878,
    "translated_title": "Clip21：梯度裁剪的误差反馈",
    "translated_abstract": "受到差分隐私（DP）约束下大规模训练的日益普及和重要性的推动，本研究研究了梯度裁剪的分布式梯度方法，即在节点处计算的梯度应用裁剪。虽然梯度裁剪是将正式DP保证注入梯度为基础的方法的重要工具，但它也会引入偏差，从而引起分布式环境下严重的收敛问题。启发于最近在误差反馈文献中取得的进展，该文献集中于驯服通信压缩运算符（例如Top - k）引入的偏差/误差，以及裁剪运算符与收缩压缩运算符之间的数学相似性，我们设计了 Clip21，这是分布式梯度裁剪的第一个能够被证明有效且实用的误差反馈机制。我们证明了我们的方法与分布式收敛速度相同。",
    "tldr": "本论文报道了Clip21，这是第一个适用于梯度裁剪的分布式训练的有效误差反馈方法，能够解决严重的收敛问题，证明了其收敛速度与分布式方法相同。",
    "en_tdlr": "This paper reports Clip21, the first effective error feedback mechanism applicable to distributed training for gradient clipping that can solve serious convergence issues, and prove its convergence rate is the same as that of distributed methods."
}