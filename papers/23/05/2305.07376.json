{
    "title": "DAISM: Digital Approximate In-SRAM Multiplier-based Accelerator for DNN Training and Inference. (arXiv:2305.07376v1 [cs.AR])",
    "abstract": "DNNs are one of the most widely used Deep Learning models. The matrix multiplication operations for DNNs incur significant computational costs and are bottlenecked by data movement between the memory and the processing elements. Many specialized accelerators have been proposed to optimize matrix multiplication operations. One popular idea is to use Processing-in-Memory where computations are performed by the memory storage element, thereby reducing the overhead of data movement between processor and memory. However, most PIM solutions rely either on novel memory technologies that have yet to mature or bit-serial computations which have significant performance overhead and scalability issues. In this work, an in-SRAM digital multiplier is proposed to take the best of both worlds, i.e. performing GEMM in memory but using only conventional SRAMs without the drawbacks of bit-serial computations. This allows the user to design systems with significant performance gains using existing techno",
    "link": "http://arxiv.org/abs/2305.07376",
    "context": "Title: DAISM: Digital Approximate In-SRAM Multiplier-based Accelerator for DNN Training and Inference. (arXiv:2305.07376v1 [cs.AR])\nAbstract: DNNs are one of the most widely used Deep Learning models. The matrix multiplication operations for DNNs incur significant computational costs and are bottlenecked by data movement between the memory and the processing elements. Many specialized accelerators have been proposed to optimize matrix multiplication operations. One popular idea is to use Processing-in-Memory where computations are performed by the memory storage element, thereby reducing the overhead of data movement between processor and memory. However, most PIM solutions rely either on novel memory technologies that have yet to mature or bit-serial computations which have significant performance overhead and scalability issues. In this work, an in-SRAM digital multiplier is proposed to take the best of both worlds, i.e. performing GEMM in memory but using only conventional SRAMs without the drawbacks of bit-serial computations. This allows the user to design systems with significant performance gains using existing techno",
    "path": "papers/23/05/2305.07376.json",
    "total_tokens": 926,
    "translated_title": "DAISM：基于多项式近似的SRAM内数字乘法器的DNN训练和推理加速器",
    "translated_abstract": "DNN是最广泛使用的深度学习模型之一。对于DNN的矩阵乘法运算会产生显著的计算成本，并受限于内存和处理单元之间的数据传输。为了优化矩阵乘法运算，提出了许多专门的加速器。一种流行的想法是使用PIM（Processing-in-Memory），其中计算是由内存存储元件执行的，从而减少了处理器和记忆体之间数据传输的开销。然而，大多数PIM解决方案要么依赖于尚未成熟的新型存储技术，要么依赖于比特串行计算，后者具有重大性能开销和可扩展性问题。在本研究中，提出了一种SRAM内数字乘法器来采用先进的GEMM计算技术，同时避免了比特串行计算的缺点。这使用户可以通过使用现有技术而实现性能显著提升的系统设计。",
    "tldr": "本论文提出了一种基于多项式近似的SRAM内数字乘法器，在不依赖于新型存储技术和避免了比特串行计算的情况下，通过内存执行GEMM计算，从而为DNN训练和推理提供了高效的加速器。",
    "en_tdlr": "This paper proposes an in-SRAM digital multiplier based on polynomial approximation to perform GEMM calculations in memory for DNN training and inference, without relying on new memory technologies and avoiding the disadvantages of bit-serial computations, providing an efficient accelerator for DNNs."
}