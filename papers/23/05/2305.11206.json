{
    "title": "LIMA: Less Is More for Alignment. (arXiv:2305.11206v1 [cs.CL])",
    "abstract": "Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43% of c",
    "link": "http://arxiv.org/abs/2305.11206",
    "context": "Title: LIMA: Less Is More for Alignment. (arXiv:2305.11206v1 [cs.CL])\nAbstract: Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43% of c",
    "path": "papers/23/05/2305.11206.json",
    "total_tokens": 914,
    "translated_title": "LIMA: 对齐的更少即为更优（Less Is More for Alignment）",
    "translated_abstract": "大型语言模型的训练通常分为两个阶段：(1)无监督的原始文本预训练，以学习通用表示；(2)大规模的指令微调和强化学习，以更好地对齐最终任务和用户偏好。我们通过训练LIMA，一个使用标准监督损失值进行的65B参数LLaMa语言模型，仅使用1000个经过筛选的提示和回复进行微调，而不使用任何强化学习或人类偏好建模，衡量了这两个阶段之间的相对重要性。 LIMA表现出了极强的性能，仅从训练数据中的少量示例中学习到如何遵循特定的响应格式，包括从规划旅行行程到推测替代历史的复杂查询。此外，该模型倾向于良好地推广到未在训练数据中出现的任务中。在一项控制的人类研究中，与GPT-4相比，LIMA的响应在43%的情况下等效或严格优先。",
    "tldr": "该论文介绍了一种使用无声调学习预训练语言模型和标准监督损失微调的方法（不使用强化学习或人类模型），并展示了在复杂任务上也有出色的表现。",
    "en_tdlr": "The paper introduces a method of using unsupervised pretraining language models and fine-tuning them with standard supervised loss without reinforcement learning or human modeling, and demonstrates remarkable performance even on complex tasks."
}