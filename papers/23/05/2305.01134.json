{
    "title": "PGrad: Learning Principal Gradients For Domain Generalization. (arXiv:2305.01134v1 [cs.LG])",
    "abstract": "Machine learning models fail to perform when facing out-of-distribution (OOD) domains, a challenging task known as domain generalization (DG). In this work, we develop a novel DG training strategy, we call PGrad, to learn a robust gradient direction, improving models' generalization ability on unseen domains. The proposed gradient aggregates the principal directions of a sampled roll-out optimization trajectory that measures the training dynamics across all training domains. PGrad's gradient design forces the DG training to ignore domain-dependent noise signals and updates all training domains with a robust direction covering main components of parameter dynamics. We further improve PGrad via bijection-based computational refinement and directional plus length-based calibrations. Our theoretical proof connects PGrad to the spectral analysis of Hessian in training neural networks. Experiments on DomainBed and WILDS benchmarks demonstrate that our approach effectively enables robust DG o",
    "link": "http://arxiv.org/abs/2305.01134",
    "context": "Title: PGrad: Learning Principal Gradients For Domain Generalization. (arXiv:2305.01134v1 [cs.LG])\nAbstract: Machine learning models fail to perform when facing out-of-distribution (OOD) domains, a challenging task known as domain generalization (DG). In this work, we develop a novel DG training strategy, we call PGrad, to learn a robust gradient direction, improving models' generalization ability on unseen domains. The proposed gradient aggregates the principal directions of a sampled roll-out optimization trajectory that measures the training dynamics across all training domains. PGrad's gradient design forces the DG training to ignore domain-dependent noise signals and updates all training domains with a robust direction covering main components of parameter dynamics. We further improve PGrad via bijection-based computational refinement and directional plus length-based calibrations. Our theoretical proof connects PGrad to the spectral analysis of Hessian in training neural networks. Experiments on DomainBed and WILDS benchmarks demonstrate that our approach effectively enables robust DG o",
    "path": "papers/23/05/2305.01134.json",
    "total_tokens": 931,
    "translated_title": "PGrad: 学习主导梯度以进行领域泛化",
    "translated_abstract": "机器学习模型在面对超出分布范围（OOD）的领域时表现不佳，这是一项具有挑战性的任务，称为领域泛化（DG）。在这项工作中，我们开发了一种新的DG训练策略，称为PGrad，以学习一个强健的梯度方向，提高模型在未知领域的泛化能力。所提出的梯度聚合了一个采样的roll-out优化轨迹的主导方向，该轨迹测量了在所有训练领域中的训练动态。PGrad梯度设计强制DG训练忽略与领域相关的噪声信号，并用覆盖参数动态主要组成部分的强健方向更新所有训练领域。我们通过双射基于计算的精炼和基于方向加长度的校准进一步改进了PGrad。我们的理论证明将PGrad与训练神经网络中Hessian的谱分析相连接。DomainBed和WILDS基准测试的实验证明，我们的方法有效地实现了强健的DG。",
    "tldr": "这篇论文提出了PGrad方法来学习主导梯度，提高模型在未知领域的泛化能力，可以忽略领域相关的噪声信号，该方法在DomainBed和WILDS基准测试中表现出较好的效果。",
    "en_tdlr": "This paper proposes the PGrad method to learn principal gradients for improving models' generalization ability on unseen domains, and can ignore domain-dependent noise signals. The method demonstrates promising performance on DomainBed and WILDS benchmarks."
}