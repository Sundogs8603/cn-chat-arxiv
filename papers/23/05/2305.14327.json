{
    "title": "Dynosaur: A Dynamic Growth Paradigm for Instruction-Tuning Data Curation. (arXiv:2305.14327v2 [cs.CL] UPDATED)",
    "abstract": "Instruction tuning has emerged to enhance the capabilities of large language models (LLMs) to comprehend instructions and generate appropriate responses. Existing methods either manually annotate or employ LLM (e.g., GPT-series) to generate data for instruction tuning. However, they often overlook associating instructions with existing annotated datasets. In this paper, we propose Dynosaur, a dynamic growth paradigm for the automatic curation of instruction-tuning data. Based on the metadata of existing datasets, we use LLMs to automatically construct instruction-tuning data by identifying relevant data fields and generating appropriate instructions.  By leveraging the existing annotated datasets, Dynosaur offers several advantages: 1) it reduces the API cost for generating instructions (e.g., it costs less than $12 USD by calling GPT-3.5-turbo for generating 800K instruction tuning samples; 2) it provides high-quality data for instruction tuning (e.g., it performs better than Alpaca a",
    "link": "http://arxiv.org/abs/2305.14327",
    "context": "Title: Dynosaur: A Dynamic Growth Paradigm for Instruction-Tuning Data Curation. (arXiv:2305.14327v2 [cs.CL] UPDATED)\nAbstract: Instruction tuning has emerged to enhance the capabilities of large language models (LLMs) to comprehend instructions and generate appropriate responses. Existing methods either manually annotate or employ LLM (e.g., GPT-series) to generate data for instruction tuning. However, they often overlook associating instructions with existing annotated datasets. In this paper, we propose Dynosaur, a dynamic growth paradigm for the automatic curation of instruction-tuning data. Based on the metadata of existing datasets, we use LLMs to automatically construct instruction-tuning data by identifying relevant data fields and generating appropriate instructions.  By leveraging the existing annotated datasets, Dynosaur offers several advantages: 1) it reduces the API cost for generating instructions (e.g., it costs less than $12 USD by calling GPT-3.5-turbo for generating 800K instruction tuning samples; 2) it provides high-quality data for instruction tuning (e.g., it performs better than Alpaca a",
    "path": "papers/23/05/2305.14327.json",
    "total_tokens": 868,
    "translated_title": "Dynosaur: 一种用于指令调优数据整理的动态增长模式",
    "translated_abstract": "指令调优已经成为增强大型语言模型（LLM）理解指令和生成适当回应能力的方法。现有方法要么手动注释，要么使用LLM（如GPT系列）生成指令调优数据。然而，它们经常忽视将指令与现有的注释数据集关联起来。在这篇论文中，我们提出了Dynosaur，一种用于自动整理指令调优数据的动态增长模式。基于现有数据集的元数据，我们使用LLM自动构建指令调优数据，通过识别相关的数据字段并生成适当的指令。通过利用现有的注释数据集，Dynosaur具有以下几个优点：1）减少了生成指令的API成本（例如，通过调用GPT-3.5-turbo生成80万个指令调优样本的成本低于12美元）；2）为指令调优提供高质量的数据（例如，表现优于Alpaca）。",
    "tldr": "Dynosaur提出了一种动态增长模式，用于自动整理指令调优数据。通过利用现有的注释数据集，Dynosaur能够以较低的API成本提供高质量的指令调优数据。",
    "en_tdlr": "Dynosaur proposes a dynamic growth paradigm for the automatic curation of instruction-tuning data. By leveraging existing annotated datasets, Dynosaur offers high-quality instruction-tuning data at a lower API cost."
}