{
    "title": "Improving the Gap in Visual Speech Recognition Between Normal and Silent Speech Based on Metric Learning. (arXiv:2305.14203v2 [eess.AS] UPDATED)",
    "abstract": "This paper presents a novel metric learning approach to address the performance gap between normal and silent speech in visual speech recognition (VSR). The difference in lip movements between the two poses a challenge for existing VSR models, which exhibit degraded accuracy when applied to silent speech. To solve this issue and tackle the scarcity of training data for silent speech, we propose to leverage the shared literal content between normal and silent speech and present a metric learning approach based on visemes. Specifically, we aim to map the input of two speech types close to each other in a latent space if they have similar viseme representations. By minimizing the Kullback-Leibler divergence of the predicted viseme probability distributions between and within the two speech types, our model effectively learns and predicts viseme identities. Our evaluation demonstrates that our method improves the accuracy of silent VSR, even when limited training data is available.",
    "link": "http://arxiv.org/abs/2305.14203",
    "context": "Title: Improving the Gap in Visual Speech Recognition Between Normal and Silent Speech Based on Metric Learning. (arXiv:2305.14203v2 [eess.AS] UPDATED)\nAbstract: This paper presents a novel metric learning approach to address the performance gap between normal and silent speech in visual speech recognition (VSR). The difference in lip movements between the two poses a challenge for existing VSR models, which exhibit degraded accuracy when applied to silent speech. To solve this issue and tackle the scarcity of training data for silent speech, we propose to leverage the shared literal content between normal and silent speech and present a metric learning approach based on visemes. Specifically, we aim to map the input of two speech types close to each other in a latent space if they have similar viseme representations. By minimizing the Kullback-Leibler divergence of the predicted viseme probability distributions between and within the two speech types, our model effectively learns and predicts viseme identities. Our evaluation demonstrates that our method improves the accuracy of silent VSR, even when limited training data is available.",
    "path": "papers/23/05/2305.14203.json",
    "total_tokens": 916,
    "translated_title": "基于度量学习提高视觉语音识别中正常与无声语音之间的差距",
    "translated_abstract": "本文提出了一种新颖的度量学习方法，以解决视觉语音识别（VSR）中正常与无声语音之间性能差距的问题。现有的VSR模型在处理无声语音时，由于两者之间的嘴唇运动差异，导致准确性下降。为了解决这个问题并解决无声语音训练数据的稀缺性，我们提出了一种基于Viseme的度量学习方法，利用正常和无声语音之间的共享字面内容。具体而言，我们的目标是在潜在空间中将两种语音类型的输入彼此靠近，如果它们具有相似的Viseme表示。通过最小化预测的Viseme概率分布之间和两种语音类型内部的Kullback-Leibler散度，我们的模型能够有效地学习和预测Viseme身份。我们的评估结果表明，即使训练数据有限，我们的方法也能提高无声VSR的准确性。",
    "tldr": "本论文提出了一种基于度量学习的方法，以缩小视觉语音识别中正常和无声语音之间的差距。通过利用正常和无声语音之间的共享内容，我们的模型能够有效地学习和预测Viseme身份，从而提高了无声VSR的准确性。",
    "en_tdlr": "This paper proposes a metric learning approach to narrow the gap in visual speech recognition between normal and silent speech by leveraging shared content and effectively learning and predicting Viseme identities."
}