{
    "title": "Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Large Language Models. (arXiv:2305.16582v1 [cs.CL])",
    "abstract": "With the widespread use of large language models (LLMs) in NLP tasks, researchers have discovered the potential of Chain-of-thought (CoT) to assist LLMs in accomplishing complex reasoning tasks by generating intermediate steps. However, human thought processes are often non-linear, rather than simply sequential chains of thoughts. Therefore, we propose Graph-of-Thought (GoT) reasoning, which models human thought processes not only as a chain but also as a graph. By representing thought units as nodes and connections between them as edges, our approach captures the non-sequential nature of human thinking and allows for a more realistic modeling of thought processes. Similar to Multimodal-CoT, we modeled GoT reasoning as a two-stage framework, generating rationales first and then producing the final answer. Specifically, we employ an additional graph-of-thoughts encoder for GoT representation learning and fuse the GoT representation with the original input representation through a gated ",
    "link": "http://arxiv.org/abs/2305.16582",
    "context": "Title: Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Large Language Models. (arXiv:2305.16582v1 [cs.CL])\nAbstract: With the widespread use of large language models (LLMs) in NLP tasks, researchers have discovered the potential of Chain-of-thought (CoT) to assist LLMs in accomplishing complex reasoning tasks by generating intermediate steps. However, human thought processes are often non-linear, rather than simply sequential chains of thoughts. Therefore, we propose Graph-of-Thought (GoT) reasoning, which models human thought processes not only as a chain but also as a graph. By representing thought units as nodes and connections between them as edges, our approach captures the non-sequential nature of human thinking and allows for a more realistic modeling of thought processes. Similar to Multimodal-CoT, we modeled GoT reasoning as a two-stage framework, generating rationales first and then producing the final answer. Specifically, we employ an additional graph-of-thoughts encoder for GoT representation learning and fuse the GoT representation with the original input representation through a gated ",
    "path": "papers/23/05/2305.16582.json",
    "total_tokens": 822,
    "translated_title": "大语言模型中的图思维推理：超越“思维链”的有力工具",
    "translated_abstract": "随着大语言模型（LLMs）在NLP任务中的广泛应用，研究人员发现“思维链”（CoT）能够通过生成中间步骤来帮助LLMs完成复杂的推理任务。然而，人类的思维过程常常是非线性的，而不只是简单的顺序思维链。因此，我们提出了“思维图”（GoT）推理，它不仅将人类思维过程建模成链式结构，而且还建模成图形结构。通过将思维单元表示为节点，它们之间的连接作为边缘，我们的方法捕捉了人类思维的非顺序性，实现了对思维过程的更加真实的建模。",
    "tldr": "通过将人类思维过程建模成图形结构，我们提出了“思维图”（GoT）推理辅助大语言模型（LLMs）来完成更加真实的、复杂的思维任务。",
    "en_tdlr": "We propose Graph-of-Thought (GoT) reasoning as an effective tool for large language models (LLMs) to accomplish more realistic and complex reasoning tasks by modeling human thought processes not only as a chain, but also as a graph."
}