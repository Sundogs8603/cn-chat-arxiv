{
    "title": "Loss Spike in Training Neural Networks. (arXiv:2305.12133v1 [cs.LG])",
    "abstract": "In this work, we study the mechanism underlying loss spikes observed during neural network training. When the training enters a region, which has a smaller-loss-as-sharper (SLAS) structure, the training becomes unstable and loss exponentially increases once it is too sharp, i.e., the rapid ascent of the loss spike. The training becomes stable when it finds a flat region. The deviation in the first eigen direction (with maximum eigenvalue of the loss Hessian ($\\lambda_{\\mathrm{max}}$) is found to be dominated by low-frequency. Since low-frequency is captured very fast (frequency principle), the rapid descent is then observed. Inspired by our analysis of loss spikes, we revisit the link between $\\lambda_{\\mathrm{max}}$ flatness and generalization. For real datasets, low-frequency is often dominant and well-captured by both the training data and the test data. Then, a solution with good generalization and a solution with bad generalization can both learn low-frequency well, thus, they hav",
    "link": "http://arxiv.org/abs/2305.12133",
    "context": "Title: Loss Spike in Training Neural Networks. (arXiv:2305.12133v1 [cs.LG])\nAbstract: In this work, we study the mechanism underlying loss spikes observed during neural network training. When the training enters a region, which has a smaller-loss-as-sharper (SLAS) structure, the training becomes unstable and loss exponentially increases once it is too sharp, i.e., the rapid ascent of the loss spike. The training becomes stable when it finds a flat region. The deviation in the first eigen direction (with maximum eigenvalue of the loss Hessian ($\\lambda_{\\mathrm{max}}$) is found to be dominated by low-frequency. Since low-frequency is captured very fast (frequency principle), the rapid descent is then observed. Inspired by our analysis of loss spikes, we revisit the link between $\\lambda_{\\mathrm{max}}$ flatness and generalization. For real datasets, low-frequency is often dominant and well-captured by both the training data and the test data. Then, a solution with good generalization and a solution with bad generalization can both learn low-frequency well, thus, they hav",
    "path": "papers/23/05/2305.12133.json",
    "total_tokens": 1251,
    "translated_title": "神经网络训练中的损失峰值研究",
    "translated_abstract": "本文研究了神经网络训练过程中出现的损失值峰值现象所背后的机制。研究发现，在具有较小损失的区域，一旦训练进入该区域，训练就会变得不稳定，损失值呈指数式增长，即出现损失峰值现象。当训练进入平坦区域时，训练会变得稳定。本研究发现，损失Hessian矩阵的最大特征值（$\\lambda_{\\mathrm{max}}$）的第一个特征向量的偏差主要由低频成分占据。由于低频成分可以非常快速地被捕获（频率原理），因此会出现急剧下降的现象。在分析损失峰值的基础上，我们重新审视了$\\lambda_{\\mathrm{max}}$平坦性和泛化能力之间的关系。对于实际数据集，低频往往占据主导地位，并且可以被训练数据和测试数据所很好地捕获。因此，具有良好泛化能力和具有劣质泛化能力的解决方案都可以很好地学习低频成分，因此它们在损失函数上的性质相似。但是，劣质泛化能力的解决方案可能会过度拟合高频成分，而良好泛化能力的解决方案具有更平滑的损失函数。",
    "tldr": "本文研究神经网络训练过程中损失值峰值现象的机制，并发现最大特征值的第一个特征向量的偏差主要受低频成分占据。低频成分可以被训练数据和测试数据很好地捕获，所以导致具有良好和劣质泛化能力的解决方案都可以很好地学习低频成分，但劣质泛化能力的解决方案可能会过度拟合高频成分，良好泛化能力的解决方案具有更平滑的损失函数。"
}