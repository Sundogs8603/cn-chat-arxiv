{
    "title": "Efficient Bound of Lipschitz Constant for Convolutional Layers by Gram Iteration. (arXiv:2305.16173v2 [cs.LG] UPDATED)",
    "abstract": "Since the control of the Lipschitz constant has a great impact on the training stability, generalization, and robustness of neural networks, the estimation of this value is nowadays a real scientific challenge. In this paper we introduce a precise, fast, and differentiable upper bound for the spectral norm of convolutional layers using circulant matrix theory and a new alternative to the Power iteration. Called the Gram iteration, our approach exhibits a superlinear convergence. First, we show through a comprehensive set of experiments that our approach outperforms other state-of-the-art methods in terms of precision, computational cost, and scalability. Then, it proves highly effective for the Lipschitz regularization of convolutional neural networks, with competitive results against concurrent approaches. Code is available at https://github.com/blaisedelattre/lip4conv.",
    "link": "http://arxiv.org/abs/2305.16173",
    "context": "Title: Efficient Bound of Lipschitz Constant for Convolutional Layers by Gram Iteration. (arXiv:2305.16173v2 [cs.LG] UPDATED)\nAbstract: Since the control of the Lipschitz constant has a great impact on the training stability, generalization, and robustness of neural networks, the estimation of this value is nowadays a real scientific challenge. In this paper we introduce a precise, fast, and differentiable upper bound for the spectral norm of convolutional layers using circulant matrix theory and a new alternative to the Power iteration. Called the Gram iteration, our approach exhibits a superlinear convergence. First, we show through a comprehensive set of experiments that our approach outperforms other state-of-the-art methods in terms of precision, computational cost, and scalability. Then, it proves highly effective for the Lipschitz regularization of convolutional neural networks, with competitive results against concurrent approaches. Code is available at https://github.com/blaisedelattre/lip4conv.",
    "path": "papers/23/05/2305.16173.json",
    "total_tokens": 946,
    "translated_title": "通过格拉姆迭代实现卷积层利普希茨常数的高效边界",
    "translated_abstract": "由于利普希茨常数的控制对神经网络的训练稳定性、泛化和鲁棒性有很大影响，因此估计这个值是目前的一个科学难题。在本文中，我们使用循环矩阵理论和一种新的功率迭代替代方法，介绍了一个精确、快速和可微分的上界，用于卷积层的谱范数。称为格拉姆迭代，我们的方法展现了一个超线性的收敛。首先，我们通过一系列全面的实验证明了我们的方法在精度、计算成本和可伸缩性方面优于其他最先进的方法。然后，我们证明了它对于卷积神经网络的利普希茨正则化非常有效，与其他方法相比具有竞争力的结果。代码可在 https://github.com/blaisedelattre/lip4conv 上获得。",
    "tldr": "本文提出了一种基于循环矩阵和格拉姆迭代的方法，用于高效估计卷积神经网络中的Lipschitz常数上界。该方法精确、快速、可微分，并展现了超线性收敛。在实验上表现出较高的精度、计算成本和可扩展性，在利普希茨正则化方面也取得了具有竞争力的结果。",
    "en_tdlr": "This paper proposes an efficient method based on circulant matrix theory and Gram iteration to estimate the upper bound of the Lipschitz constant in convolutional neural networks. It shows precise, fast, and differentiable results with superlinear convergence. The experiments demonstrate the effectiveness in terms of accuracy, computational cost, and scalability, and competitive results in Lipschitz regularization."
}