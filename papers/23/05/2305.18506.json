{
    "title": "Generalization Ability of Wide Residual Networks. (arXiv:2305.18506v1 [stat.ML])",
    "abstract": "In this paper, we study the generalization ability of the wide residual network on $\\mathbb{S}^{d-1}$ with the ReLU activation function. We first show that as the width $m\\rightarrow\\infty$, the residual network kernel (RNK) uniformly converges to the residual neural tangent kernel (RNTK). This uniform convergence further guarantees that the generalization error of the residual network converges to that of the kernel regression with respect to the RNTK. As direct corollaries, we then show $i)$ the wide residual network with the early stopping strategy can achieve the minimax rate provided that the target regression function falls in the reproducing kernel Hilbert space (RKHS) associated with the RNTK; $ii)$ the wide residual network can not generalize well if it is trained till overfitting the data. We finally illustrate some experiments to reconcile the contradiction between our theoretical result and the widely observed ``benign overfitting phenomenon''",
    "link": "http://arxiv.org/abs/2305.18506",
    "context": "Title: Generalization Ability of Wide Residual Networks. (arXiv:2305.18506v1 [stat.ML])\nAbstract: In this paper, we study the generalization ability of the wide residual network on $\\mathbb{S}^{d-1}$ with the ReLU activation function. We first show that as the width $m\\rightarrow\\infty$, the residual network kernel (RNK) uniformly converges to the residual neural tangent kernel (RNTK). This uniform convergence further guarantees that the generalization error of the residual network converges to that of the kernel regression with respect to the RNTK. As direct corollaries, we then show $i)$ the wide residual network with the early stopping strategy can achieve the minimax rate provided that the target regression function falls in the reproducing kernel Hilbert space (RKHS) associated with the RNTK; $ii)$ the wide residual network can not generalize well if it is trained till overfitting the data. We finally illustrate some experiments to reconcile the contradiction between our theoretical result and the widely observed ``benign overfitting phenomenon''",
    "path": "papers/23/05/2305.18506.json",
    "total_tokens": 994,
    "translated_title": "宽残差网络的泛化能力",
    "translated_abstract": "本文研究了在$\\mathbb{S}^{d-1}$上使用ReLU激活函数的宽残差网络的泛化能力。我们首先表明，当宽度$m\\rightarrow\\infty$时，残差网络核(RNK)统一收敛到残差神经切向核(RNTK)。这种统一收敛进一步保证了残差网络的泛化误差收敛于相对于RNTK的核回归的误差。作为直接推论，我们指出：$i$)如果目标回归函数落在与RNTK相关联的再生核希尔伯特空间(RKHS)中，采用早停策略的宽残差网络可以达到极小化速率；$ii$)如果训练到过度拟合数据，则无法很好地推广宽残差网络。最后，我们介绍一些实验来调和我们的理论结果与广泛观察到的“良性过拟合现象”之间的矛盾。",
    "tldr": "本文研究了在$\\mathbb{S}^{d-1}$上使用ReLU激活函数的宽残差网络的泛化能力，表明当宽度$m\\rightarrow\\infty$时，残差网络核(RNK)统一收敛到残差神经切向核(RNTK)，并且早停策略的宽残差网络可以达到极小化速率，但在训练过度拟合数据时无法很好地推广。",
    "en_tdlr": "This paper studies the generalization ability of the wide residual network on $\\mathbb{S}^{d-1}$ with ReLU activation function. The authors show that as the width of the network approaches infinity, the residual network kernel converges to the residual neural tangent kernel, and the wide residual network with early stopping can achieve the minimax rate, while it cannot generalize well if trained till overfitting."
}