{
    "title": "Get Back Here: Robust Imitation by Return-to-Distribution Planning. (arXiv:2305.01400v1 [cs.RO])",
    "abstract": "We consider the Imitation Learning (IL) setup where expert data are not collected on the actual deployment environment but on a different version. To address the resulting distribution shift, we combine behavior cloning (BC) with a planner that is tasked to bring the agent back to states visited by the expert whenever the agent deviates from the demonstration distribution. The resulting algorithm, POIR, can be trained offline, and leverages online interactions to efficiently fine-tune its planner to improve performance over time. We test POIR on a variety of human-generated manipulation demonstrations in a realistic robotic manipulation simulator and show robustness of the learned policy to different initial state distributions and noisy dynamics.",
    "link": "http://arxiv.org/abs/2305.01400",
    "context": "Title: Get Back Here: Robust Imitation by Return-to-Distribution Planning. (arXiv:2305.01400v1 [cs.RO])\nAbstract: We consider the Imitation Learning (IL) setup where expert data are not collected on the actual deployment environment but on a different version. To address the resulting distribution shift, we combine behavior cloning (BC) with a planner that is tasked to bring the agent back to states visited by the expert whenever the agent deviates from the demonstration distribution. The resulting algorithm, POIR, can be trained offline, and leverages online interactions to efficiently fine-tune its planner to improve performance over time. We test POIR on a variety of human-generated manipulation demonstrations in a realistic robotic manipulation simulator and show robustness of the learned policy to different initial state distributions and noisy dynamics.",
    "path": "papers/23/05/2305.01400.json",
    "total_tokens": 774,
    "translated_title": "返回分布规划：鲁棒性模仿学习",
    "translated_abstract": "我们考虑模仿学习的设置，即专家数据不是在实际部署环境下收集的，而是在另一个版本上收集的。为了解决由此导致的分布偏移问题，我们将行为克隆（BC）与一个规划器相结合，规划器的任务是在代理程序偏离演示分布时，将代理程序带回到专家访问的状态。得到的算法POIR可以离线训练，并利用在线交互来有效地优化其规划器，以逐步提高性能。我们在现实的机器人操纵模拟器中对POIR进行测试，使用各种人类生成的操作演示，并展示了学习策略对不同初始状态分布和嘈杂动态的鲁棒性。",
    "tldr": "本文提出的算法POIR将行为克隆和规划器相结合，解决了模仿学习中分布偏移的问题，模型在机器人操纵任务中表现出强大的鲁棒性。",
    "en_tdlr": "The paper proposes an algorithm, POIR, that combines behavior cloning with a planner to address the distribution shift problem in imitation learning. The resulting model shows strong robustness in robotic manipulation tasks with different initial state distributions and noisy dynamics."
}