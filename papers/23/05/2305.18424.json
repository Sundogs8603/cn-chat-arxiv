{
    "title": "Repeated Random Sampling for Minimizing the Time-to-Accuracy of Learning. (arXiv:2305.18424v1 [cs.LG])",
    "abstract": "Methods for carefully selecting or generating a small set of training data to learn from, i.e., data pruning, coreset selection, and data distillation, have been shown to be effective in reducing the ever-increasing cost of training neural networks. Behind this success are rigorously designed strategies for identifying informative training examples out of large datasets. However, these strategies come with additional computational costs associated with subset selection or data distillation before training begins, and furthermore, many are shown to even under-perform random sampling in high data compression regimes. As such, many data pruning, coreset selection, or distillation methods may not reduce 'time-to-accuracy', which has become a critical efficiency measure of training deep neural networks over large datasets. In this work, we revisit a powerful yet overlooked random sampling strategy to address these challenges and introduce an approach called Repeated Sampling of Random Subse",
    "link": "http://arxiv.org/abs/2305.18424",
    "context": "Title: Repeated Random Sampling for Minimizing the Time-to-Accuracy of Learning. (arXiv:2305.18424v1 [cs.LG])\nAbstract: Methods for carefully selecting or generating a small set of training data to learn from, i.e., data pruning, coreset selection, and data distillation, have been shown to be effective in reducing the ever-increasing cost of training neural networks. Behind this success are rigorously designed strategies for identifying informative training examples out of large datasets. However, these strategies come with additional computational costs associated with subset selection or data distillation before training begins, and furthermore, many are shown to even under-perform random sampling in high data compression regimes. As such, many data pruning, coreset selection, or distillation methods may not reduce 'time-to-accuracy', which has become a critical efficiency measure of training deep neural networks over large datasets. In this work, we revisit a powerful yet overlooked random sampling strategy to address these challenges and introduce an approach called Repeated Sampling of Random Subse",
    "path": "papers/23/05/2305.18424.json",
    "total_tokens": 1010,
    "translated_title": "重复随机抽样用于减少学习时间至准确度",
    "translated_abstract": "已经表明，仔细选择或生成一小组训练数据以进行学习，即数据修剪，核心集选择和数据精馏的方法可以有效地减少训练神经网络的成本。在此背后的成功是严格设计的策略，用于在大型数据集中识别信息性训练示例。然而，这些策略伴随着在训练开始前进行的子集选择或数据精馏所带来的额外计算成本，而且，许多筛选数据、选择核心集或提取数据的方法甚至在高数据压缩范围内也表现不佳。因此，许多数据修剪，核心集选择或精馏方法可能无法减少训练深度神经网络在大型数据集上的“时间至准确度”，这已成为衡量效率的关键指标。在本文中，我们重新审视了一种强大但被忽视的随机抽样策略，以解决这些挑战，并介绍了一种称作重复随机子集抽样（RSRS）的方法，它可以有效地减少学习时间至准确度，而不引入额外的计算开销。我们通过对基准数据集的广泛实验来理论分析RSRS的性质并证明其有效性。",
    "tldr": "本文介绍了一种称作重复随机子集抽样(RSRS)的方法，有效减少学习时间至准确度，避免了子集选择或数据精馏所带来的额外计算成本。",
    "en_tdlr": "This paper introduces a method called Repeated Sampling of Random Subsets (RSRS) that effectively reduces the 'time-to-accuracy' without introducing additional computational overhead, avoiding the additional costs associated with subset selection or data distillation."
}