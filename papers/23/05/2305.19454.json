{
    "title": "Dynamic Sparsity Is Channel-Level Sparsity Learner. (arXiv:2305.19454v1 [cs.LG])",
    "abstract": "Sparse training has received an upsurging interest in machine learning due to its tantalizing saving potential for the entire training process as well as inference. Dynamic sparse training (DST), as a leading sparse training approach, can train deep neural networks at high sparsity from scratch to match the performance of their dense counterparts. However, most if not all DST prior arts demonstrate their effectiveness on unstructured sparsity with highly irregular sparse patterns, which receives limited support in common hardware. This limitation hinders the usage of DST in practice. In this paper, we propose Channel-aware dynamic sparse (Chase), which for the first time seamlessly translates the promise of unstructured dynamic sparsity to GPU-friendly channel-level sparsity (not fine-grained N:M or group sparsity) during one end-to-end training process, without any ad-hoc operations. The resulting small sparse networks can be directly accelerated by commodity hardware, without using a",
    "link": "http://arxiv.org/abs/2305.19454",
    "context": "Title: Dynamic Sparsity Is Channel-Level Sparsity Learner. (arXiv:2305.19454v1 [cs.LG])\nAbstract: Sparse training has received an upsurging interest in machine learning due to its tantalizing saving potential for the entire training process as well as inference. Dynamic sparse training (DST), as a leading sparse training approach, can train deep neural networks at high sparsity from scratch to match the performance of their dense counterparts. However, most if not all DST prior arts demonstrate their effectiveness on unstructured sparsity with highly irregular sparse patterns, which receives limited support in common hardware. This limitation hinders the usage of DST in practice. In this paper, we propose Channel-aware dynamic sparse (Chase), which for the first time seamlessly translates the promise of unstructured dynamic sparsity to GPU-friendly channel-level sparsity (not fine-grained N:M or group sparsity) during one end-to-end training process, without any ad-hoc operations. The resulting small sparse networks can be directly accelerated by commodity hardware, without using a",
    "path": "papers/23/05/2305.19454.json",
    "total_tokens": 1062,
    "translated_title": "动态稀疏是通道级稀疏的学习者",
    "translated_abstract": "稀疏训练由于在整个训练过程和推理中具有诱人的节省能力而受到机器学习的广泛关注。动态稀疏训练(DST)作为一种领先的稀疏训练方法，可以从零开始训练深度神经网络，以达到与密集对应物性能相匹配的高稀疏性能。然而，大多数DST之前的研究都表明它们的有效性是在高度不规则的稀疏模式下的非结构化稀疏性上，这在常见硬件上得到了有限的支持。这种限制阻碍了DST在实践中的使用。在本文中，我们提出了一种名为通道感知动态稀疏（Chase）的方法，它将非结构化动态稀疏的性能转换为适合GPU友好的通道级别稀疏，在一个端到端的训练过程中实现，而不需要任何特殊的操作。所得到的小型稀疏网络可以直接通过通用硬件加速，而无需使用专用的稀疏硬件加速器。我们在各种数据集上的实验结果表明，Chase可以在深度神经网络上实现高通道级稀疏性，同时保持其性能，并显着减小模型的大小。",
    "tldr": "本文提出了一种名为Channel-aware dynamic sparse (Chase)的方法，使用端到端训练实现了GPU友好的通道级别稀疏，不需要任何特殊操作，并且可以直接在通用硬件上加速，显著减小模型大小，同时保持性能。"
}