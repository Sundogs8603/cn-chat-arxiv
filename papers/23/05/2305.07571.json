{
    "title": "Supplementing Gradient-Based Reinforcement Learning with Simple Evolutionary Ideas. (arXiv:2305.07571v1 [cs.NE])",
    "abstract": "We present a simple, sample-efficient algorithm for introducing large but directed learning steps in reinforcement learning (RL), through the use of evolutionary operators. The methodology uses a population of RL agents training with a common experience buffer, with occasional crossovers and mutations of the agents in order to search efficiently through the policy space. Unlike prior literature on combining evolutionary search (ES) with RL, this work does not generate a distribution of agents from a common mean and covariance matrix. Neither does it require the evaluation of the entire population of policies at every time step. Instead, we focus on gradient-based training throughout the life of every policy (individual), with a sparse amount of evolutionary exploration. The resulting algorithm is shown to be robust to hyperparameter variations. As a surprising corollary, we show that simply initialising and training multiple RL agents with a common memory (with no further evolutionary ",
    "link": "http://arxiv.org/abs/2305.07571",
    "context": "Title: Supplementing Gradient-Based Reinforcement Learning with Simple Evolutionary Ideas. (arXiv:2305.07571v1 [cs.NE])\nAbstract: We present a simple, sample-efficient algorithm for introducing large but directed learning steps in reinforcement learning (RL), through the use of evolutionary operators. The methodology uses a population of RL agents training with a common experience buffer, with occasional crossovers and mutations of the agents in order to search efficiently through the policy space. Unlike prior literature on combining evolutionary search (ES) with RL, this work does not generate a distribution of agents from a common mean and covariance matrix. Neither does it require the evaluation of the entire population of policies at every time step. Instead, we focus on gradient-based training throughout the life of every policy (individual), with a sparse amount of evolutionary exploration. The resulting algorithm is shown to be robust to hyperparameter variations. As a surprising corollary, we show that simply initialising and training multiple RL agents with a common memory (with no further evolutionary ",
    "path": "papers/23/05/2305.07571.json",
    "total_tokens": 905,
    "translated_abstract": "本文介绍了一种简单而样本高效的算法，用于通过使用进化算子引入大但指向性的学习步骤，提高强化学习（RL）的效率。该方法使用训练具有共同经验缓冲区的RL代理人种群，偶尔进行代理人交叉和变异，以有效地搜索策略空间。与将进化搜索（ES）与RL相结合的先前文献不同，本文不生成从相同均值和协方差矩阵的代理人分布。它也不需要在每个时间步骤评估整个政策人口。而是专注于在每个政策（个体）的整个生命周期中进行基于梯度的训练，带有少量的进化探索。结果证明，该算法对超参数变化具有鲁棒性。作为一个惊人的推论，我们表明，仅通过使用共同内存初始化和训练多个RL代理人（而不需要进一步的进化），我们也可以获得极好的结果。",
    "tldr": "本文介绍了一种新的算法，它可以通过使用进化算子引入大但指向性的学习步骤，在强化学习中提高效率。该方法不同于以往将进化搜索（ES）与RL相结合的方法，并在实验中得到了良好的结果。",
    "en_tdlr": "This paper presents a new algorithm that improves the efficiency of reinforcement learning by introducing large but directed learning steps using evolutionary operators. The method differs from previous approaches that combine evolutionary search with RL and performs well in experiments."
}