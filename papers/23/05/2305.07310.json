{
    "title": "Improving Zero-shot Multilingual Neural Machine Translation by Leveraging Cross-lingual Consistency Regularization. (arXiv:2305.07310v1 [cs.CL])",
    "abstract": "The multilingual neural machine translation (NMT) model has a promising capability of zero-shot translation, where it could directly translate between language pairs unseen during training. For good transfer performance from supervised directions to zero-shot directions, the multilingual NMT model is expected to learn universal representations across different languages. This paper introduces a cross-lingual consistency regularization, CrossConST, to bridge the representation gap among different languages and boost zero-shot translation performance. The theoretical analysis shows that CrossConST implicitly maximizes the probability distribution for zero-shot translation, and the experimental results on both low-resource and high-resource benchmarks show that CrossConST consistently improves the translation performance. The experimental analysis also proves that CrossConST could close the sentence representation gap and better align the representation space. Given the universality and s",
    "link": "http://arxiv.org/abs/2305.07310",
    "context": "Title: Improving Zero-shot Multilingual Neural Machine Translation by Leveraging Cross-lingual Consistency Regularization. (arXiv:2305.07310v1 [cs.CL])\nAbstract: The multilingual neural machine translation (NMT) model has a promising capability of zero-shot translation, where it could directly translate between language pairs unseen during training. For good transfer performance from supervised directions to zero-shot directions, the multilingual NMT model is expected to learn universal representations across different languages. This paper introduces a cross-lingual consistency regularization, CrossConST, to bridge the representation gap among different languages and boost zero-shot translation performance. The theoretical analysis shows that CrossConST implicitly maximizes the probability distribution for zero-shot translation, and the experimental results on both low-resource and high-resource benchmarks show that CrossConST consistently improves the translation performance. The experimental analysis also proves that CrossConST could close the sentence representation gap and better align the representation space. Given the universality and s",
    "path": "papers/23/05/2305.07310.json",
    "total_tokens": 978,
    "translated_title": "利用跨语言一致性规范化改进零样本多语言神经机器翻译",
    "translated_abstract": "多语言神经机器翻译（NMT）模型有很强的零样本翻译能力，能够在未经过训练的语言对之间进行直接翻译。然而，为了实现从有监督方向到零样本方向的良好转移性能，需要让多语言NMT模型学习到跨不同语言的通用表示。本文引入了跨语言一致性规范化CrossConST，以弥合不同语言之间的表示差距，并提高零样本翻译性能。理论分析表明，CrossConST隐含地最大化了零样本翻译的概率分布，并在低资源和高资源基准测试中取得了稳定的提升。实验分析还证明，CrossConST可以缩小句子表示差距并更好地对齐表示空间。鉴于所提出的方法的普适性和可扩展性，CrossConST可以方便地应用于其他多语言NMT模型中，以进一步提高零样本翻译性能。",
    "tldr": "本文提出了一种跨语言一致性规范化方法CrossConST，用于改进零样本多语言神经机器翻译模型的性能。实验结果表明，CrossConST可以缩小语言之间的表示差距，提高零样本翻译的准确性和多样性。",
    "en_tdlr": "This paper proposes a cross-lingual consistency regularization approach called CrossConST to improve the zero-shot translation performance of multilingual neural machine translation (NMT) models. The experimental results show that CrossConST can bridge the representation gap among different languages, and improve the accuracy and diversity of zero-shot translation."
}