{
    "title": "Generalization Bounds for Magnitude-Based Pruning via Sparse Matrix Sketching. (arXiv:2305.18789v2 [cs.LG] UPDATED)",
    "abstract": "In this paper, we derive a novel bound on the generalization error of Magnitude-Based pruning of overparameterized neural networks. Our work builds on the bounds in Arora et al. [2018] where the error depends on one, the approximation induced by pruning, and two, the number of parameters in the pruned model, and improves upon standard norm-based generalization bounds. The pruned estimates obtained using our new Magnitude-Based compression algorithm are close to the unpruned functions with high probability, which improves the first criteria. Using Sparse Matrix Sketching, the space of the pruned matrices can be efficiently represented in the space of dense matrices of much smaller dimensions, thereby lowering the second criterion. This leads to stronger generalization bound than many state-of-the-art methods, thereby breaking new ground in the algorithm development for pruning and bounding generalization error of overparameterized models. Beyond this, we extend our results to obtain gen",
    "link": "http://arxiv.org/abs/2305.18789",
    "context": "Title: Generalization Bounds for Magnitude-Based Pruning via Sparse Matrix Sketching. (arXiv:2305.18789v2 [cs.LG] UPDATED)\nAbstract: In this paper, we derive a novel bound on the generalization error of Magnitude-Based pruning of overparameterized neural networks. Our work builds on the bounds in Arora et al. [2018] where the error depends on one, the approximation induced by pruning, and two, the number of parameters in the pruned model, and improves upon standard norm-based generalization bounds. The pruned estimates obtained using our new Magnitude-Based compression algorithm are close to the unpruned functions with high probability, which improves the first criteria. Using Sparse Matrix Sketching, the space of the pruned matrices can be efficiently represented in the space of dense matrices of much smaller dimensions, thereby lowering the second criterion. This leads to stronger generalization bound than many state-of-the-art methods, thereby breaking new ground in the algorithm development for pruning and bounding generalization error of overparameterized models. Beyond this, we extend our results to obtain gen",
    "path": "papers/23/05/2305.18789.json",
    "total_tokens": 1022,
    "translated_title": "基于稀疏矩阵草图的幅值剪枝的泛化界限",
    "translated_abstract": "本文针对超参数神经网络的幅值剪枝，提出了一种新颖的泛化误差界限。本工作基于Arora等人（2018）的边界，其中误差取决于剪枝引起的逼近和剪枝模型中参数的数量，并改进了标准基于范数的泛化边界。使用我们新的基于幅值的压缩算法得到的剪枝估计可以高概率地接近未剪枝函数，从而改善了第一个标准。通过稀疏矩阵草图，剪枝矩阵的空间可以在远小于其维度的密集矩阵空间中高效地表示，从而降低了第二个标准。这导致了比许多最先进方法更强的泛化界限，从而在剪枝和超参数模型泛化误差边界的算法开发方面开辟了新的道路。除此之外，我们扩展了我们的结果，获得了结构剪枝的泛化界限，其中还涉及到块状权重的剪枝。我们的理论发现得到了对不同网络结构和数据集的广泛实验支持。",
    "tldr": "本文提出了基于稀疏矩阵草图的幅值剪枝的新颖泛化误差界限方法，其具有强的泛化界限，可以在剪枝和超参数模型泛化误差边界的算法开发方面开辟新的道路。",
    "en_tdlr": "This paper proposes a novel generalization error bound method for magnitude-based pruning using sparse matrix sketching, which has strong generalization bounds and opens up new paths for algorithm development in pruning and generalization error bounds of overparameterized models."
}