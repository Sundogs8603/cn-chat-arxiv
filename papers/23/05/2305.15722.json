{
    "title": "Comparative Study of Pre-Trained BERT Models for Code-Mixed Hindi-English Data. (arXiv:2305.15722v1 [cs.CL])",
    "abstract": "The term \"Code Mixed\" refers to the use of more than one language in the same text. This phenomenon is predominantly observed on social media platforms, with an increasing amount of adaptation as time goes on. It is critical to detect foreign elements in a language and process them correctly, as a considerable number of individuals are using code-mixed languages that could not be comprehended by understanding one of those languages. In this work, we focus on low-resource Hindi-English code-mixed language and enhancing the performance of different code-mixed natural language processing tasks such as sentiment analysis, emotion recognition, and hate speech identification. We perform a comparative analysis of different Transformer-based language Models pre-trained using unsupervised approaches. We have included the code-mixed models like HingBERT, HingRoBERTa, HingRoBERTa-Mixed, mBERT, and non-code-mixed models like AlBERT, BERT, and RoBERTa for comparative analysis of code-mixed Hindi-En",
    "link": "http://arxiv.org/abs/2305.15722",
    "context": "Title: Comparative Study of Pre-Trained BERT Models for Code-Mixed Hindi-English Data. (arXiv:2305.15722v1 [cs.CL])\nAbstract: The term \"Code Mixed\" refers to the use of more than one language in the same text. This phenomenon is predominantly observed on social media platforms, with an increasing amount of adaptation as time goes on. It is critical to detect foreign elements in a language and process them correctly, as a considerable number of individuals are using code-mixed languages that could not be comprehended by understanding one of those languages. In this work, we focus on low-resource Hindi-English code-mixed language and enhancing the performance of different code-mixed natural language processing tasks such as sentiment analysis, emotion recognition, and hate speech identification. We perform a comparative analysis of different Transformer-based language Models pre-trained using unsupervised approaches. We have included the code-mixed models like HingBERT, HingRoBERTa, HingRoBERTa-Mixed, mBERT, and non-code-mixed models like AlBERT, BERT, and RoBERTa for comparative analysis of code-mixed Hindi-En",
    "path": "papers/23/05/2305.15722.json",
    "total_tokens": 950,
    "translated_title": "面向代码混合的印地语-英语数据的预训练BERT模型的比较研究",
    "translated_abstract": "“代码混合”是指在同一段文本中使用多种语言的现象。这种现象在社交媒体平台上广泛存在，并随着时间的推移越来越多地被采纳。检测语言中的外来元素并正确处理它们至关重要，因为许多人使用代码混合语言，其中任一语言都无法理解。本文重点研究低资源的印地语-英语代码混合语言，并提高不同代码混合自然语言处理任务（如情感分析、情绪识别和仇恨言论识别）的性能。我们对使用无监督方法预训练的不同基于Transformer的语言模型进行了比较分析。我们包括了代码混合模型（如HingBERT、HingRoBERTa、HingRoBERTa-Mixed、mBERT）和非代码混合模型（如AlBERT、BERT、RoBERTa），进行比较分析印地语-英语代码混合。",
    "tldr": "本文比较了使用不同预训练Transformer模型的印地语-英语代码混合数据的性能表现，以提高情感分析、情绪识别和仇恨言论识别等自然语言处理任务的性能。",
    "en_tdlr": "This paper compares the performance of different pre-trained Transformer models on code-mixed Hindi-English data to enhance the performance of natural language processing tasks such as sentiment analysis, emotion recognition, and hate speech identification. Code-mixed models such as HingBERT, HingRoBERTa, HingRoBERTa-Mixed, mBERT, and non-code-mixed models such as AlBERT, BERT, and RoBERTa were included for comparative analysis."
}