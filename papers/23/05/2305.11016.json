{
    "title": "Silver Syntax Pre-training for Cross-Domain Relation Extraction. (arXiv:2305.11016v1 [cs.CL])",
    "abstract": "Relation Extraction (RE) remains a challenging task, especially when considering realistic out-of-domain evaluations. One of the main reasons for this is the limited training size of current RE datasets: obtaining high-quality (manually annotated) data is extremely expensive and cannot realistically be repeated for each new domain. An intermediate training step on data from related tasks has shown to be beneficial across many NLP tasks.However, this setup still requires supplementary annotated data, which is often not available. In this paper, we investigate intermediate pre-training specifically for RE. We exploit the affinity between syntactic structure and semantic RE, and identify the syntactic relations which are closely related to RE by being on the shortest dependency path between two entities. We then take advantage of the high accuracy of current syntactic parsers in order to automatically obtain large amounts of low-cost pre-training data. By pre-training our RE model on the ",
    "link": "http://arxiv.org/abs/2305.11016",
    "context": "Title: Silver Syntax Pre-training for Cross-Domain Relation Extraction. (arXiv:2305.11016v1 [cs.CL])\nAbstract: Relation Extraction (RE) remains a challenging task, especially when considering realistic out-of-domain evaluations. One of the main reasons for this is the limited training size of current RE datasets: obtaining high-quality (manually annotated) data is extremely expensive and cannot realistically be repeated for each new domain. An intermediate training step on data from related tasks has shown to be beneficial across many NLP tasks.However, this setup still requires supplementary annotated data, which is often not available. In this paper, we investigate intermediate pre-training specifically for RE. We exploit the affinity between syntactic structure and semantic RE, and identify the syntactic relations which are closely related to RE by being on the shortest dependency path between two entities. We then take advantage of the high accuracy of current syntactic parsers in order to automatically obtain large amounts of low-cost pre-training data. By pre-training our RE model on the ",
    "path": "papers/23/05/2305.11016.json",
    "total_tokens": 760,
    "tldr": "本文提出了利用最短依存路径上的句法关系，在关系提取任务中进行中间预训练的方法，极大地降低了人工标注数据的成本，取得了很好的效果。",
    "en_tdlr": "This paper proposes an intermediate pre-training method for relation extraction that exploits syntactic structure and semantic relations. By identifying syntactic relations on the shortest dependency path between two entities, the model can automatically obtain large amounts of low-cost pre-training data. The method shows promising results in cross-domain relation extraction tasks while significantly reducing the need for manually annotated data."
}