{
    "title": "Contextualized End-to-End Speech Recognition with Contextual Phrase Prediction Network. (arXiv:2305.12493v2 [eess.AS] UPDATED)",
    "abstract": "Contextual information plays a crucial role in speech recognition technologies and incorporating it into the end-to-end speech recognition models has drawn immense interest recently. However, previous deep bias methods lacked explicit supervision for bias tasks. In this study, we introduce a contextual phrase prediction network for an attention-based deep bias method. This network predicts context phrases in utterances using contextual embeddings and calculates bias loss to assist in the training of the contextualized model. Our method achieved a significant word error rate (WER) reduction across various end-to-end speech recognition models. Experiments on the LibriSpeech corpus show that our proposed model obtains a 12.1% relative WER improvement over the baseline model, and the WER of the context phrases decreases relatively by 40.5%. Moreover, by applying a context phrase filtering strategy, we also effectively eliminate the WER degradation when using a larger biasing list.",
    "link": "http://arxiv.org/abs/2305.12493",
    "context": "Title: Contextualized End-to-End Speech Recognition with Contextual Phrase Prediction Network. (arXiv:2305.12493v2 [eess.AS] UPDATED)\nAbstract: Contextual information plays a crucial role in speech recognition technologies and incorporating it into the end-to-end speech recognition models has drawn immense interest recently. However, previous deep bias methods lacked explicit supervision for bias tasks. In this study, we introduce a contextual phrase prediction network for an attention-based deep bias method. This network predicts context phrases in utterances using contextual embeddings and calculates bias loss to assist in the training of the contextualized model. Our method achieved a significant word error rate (WER) reduction across various end-to-end speech recognition models. Experiments on the LibriSpeech corpus show that our proposed model obtains a 12.1% relative WER improvement over the baseline model, and the WER of the context phrases decreases relatively by 40.5%. Moreover, by applying a context phrase filtering strategy, we also effectively eliminate the WER degradation when using a larger biasing list.",
    "path": "papers/23/05/2305.12493.json",
    "total_tokens": 1039,
    "translated_title": "上下文化的短语预测网络在端到端语音识别中的应用",
    "translated_abstract": "上下文信息在语音识别技术中发挥着至关重要的作用，将其融入端到端语音识别模型近年来引起了极大的兴趣。然而，先前的深度偏置方法缺乏偏置任务的显式监督。本研究引入了一个上下文短语预测网络用于基于注意力的深度偏置方法。该网络利用上下文嵌入预测发音中的上下文短语，并计算偏置损失以帮助训练上下文化模型。我们的方法在多种端到端语音识别模型上实现了显著的单词错误率(WER)降低。对LibriSpeech语料库的实验结果表明，在基线模型上，我们提出的模型相对WER提高了12.1％，上下文短语的WER相对降低了40.5％。此外，通过应用上下文短语过滤策略，我们还有效消除了使用更大的偏置列表时的WER降级现象。",
    "tldr": "本研究引入了一个上下文短语预测网络用于基于注意力的深度偏置方法，通过计算偏置损失以帮助训练上下文化模型，在多种端到端语音识别模型上实现了显著的WER降低，相对于基线模型相对WER提高了12.1％，上下文短语的WER相对降低了40.5％。",
    "en_tdlr": "This study introduces a contextual phrase prediction network for an attention-based deep bias method, which predicts context phrases in utterances using contextual embeddings and calculates bias loss to assist in the training of the contextualized model. The proposed method achieves a significant word error rate (WER) reduction across various end-to-end speech recognition models, with a 12.1% relative WER improvement over the baseline model and a 40.5% relative decrease in WER of the context phrases. The application of a context phrase filtering strategy also effectively eliminates the WER degradation when using a larger biasing list."
}