{
    "title": "Reconstruction Error-based Anomaly Detection with Few Outlying Examples. (arXiv:2305.10464v1 [cs.LG])",
    "abstract": "Reconstruction error-based neural architectures constitute a classical deep learning approach to anomaly detection which has shown great performances. It consists in training an Autoencoder to reconstruct a set of examples deemed to represent the normality and then to point out as anomalies those data that show a sufficiently large reconstruction error. Unfortunately, these architectures often become able to well reconstruct also the anomalies in the data. This phenomenon is more evident when there are anomalies in the training set. In particular when these anomalies are labeled, a setting called semi-supervised, the best way to train Autoencoders is to ignore anomalies and minimize the reconstruction error on normal data. The goal of this work is to investigate approaches to allow reconstruction error-based architectures to instruct the model to put known anomalies outside of the domain description of the normal data. Specifically, our strategy exploits a limited number of anomalous e",
    "link": "http://arxiv.org/abs/2305.10464",
    "context": "Title: Reconstruction Error-based Anomaly Detection with Few Outlying Examples. (arXiv:2305.10464v1 [cs.LG])\nAbstract: Reconstruction error-based neural architectures constitute a classical deep learning approach to anomaly detection which has shown great performances. It consists in training an Autoencoder to reconstruct a set of examples deemed to represent the normality and then to point out as anomalies those data that show a sufficiently large reconstruction error. Unfortunately, these architectures often become able to well reconstruct also the anomalies in the data. This phenomenon is more evident when there are anomalies in the training set. In particular when these anomalies are labeled, a setting called semi-supervised, the best way to train Autoencoders is to ignore anomalies and minimize the reconstruction error on normal data. The goal of this work is to investigate approaches to allow reconstruction error-based architectures to instruct the model to put known anomalies outside of the domain description of the normal data. Specifically, our strategy exploits a limited number of anomalous e",
    "path": "papers/23/05/2305.10464.json",
    "total_tokens": 816,
    "translated_title": "基于重构误差的少量异常样本检测",
    "translated_abstract": "基于重构误差的神经网络结构是一种经典的深度学习检测异常的方法，其表现出色。该方法通过训练自编码器来重构代表正常数据的样本集，然后指出那些重构误差足够大的数据为异常情况。然而，这些结构常常能够很好地重构数据中的异常情况。特别当训练集中存在异常情况时，这种现象更为明显。当这些异常情况有标签时，这种情况称为半监督，训练自编码器的最佳方法是忽略异常情况并在正常数据上最小化重构误差。本文旨在探讨让基于重构误差的结构能够让模型将已知的异常点排除在正常数据集之外的方法。具体而言，我们的策略利用了少量异常样本。",
    "tldr": "该论文探讨了在训练给定少量异常情况下，通过基于重构误差的神经网络结构，排除已知的异常点，用于检测异常情况的方法。",
    "en_tdlr": "This paper investigates the approach of using reconstruction error-based neural architectures to exclude known anomalies in a few outlying examples in order to detect anomalies when training with limited amount of abnormal data."
}