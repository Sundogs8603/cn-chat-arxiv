{
    "title": "A Momentum-Incorporated Non-Negative Latent Factorization of Tensors Model for Dynamic Network Representation. (arXiv:2305.02782v1 [cs.LG])",
    "abstract": "A large-scale dynamic network (LDN) is a source of data in many big data-related applications due to their large number of entities and large-scale dynamic interactions. They can be modeled as a high-dimensional incomplete (HDI) tensor that contains a wealth of knowledge about time patterns. A Latent factorization of tensors (LFT) model efficiently extracts this time pattern, which can be established using stochastic gradient descent (SGD) solvers. However, LFT models based on SGD are often limited by training schemes and have poor tail convergence. To solve this problem, this paper proposes a novel nonlinear LFT model (MNNL) based on momentum-incorporated SGD, which extracts non-negative latent factors from HDI tensors to make training unconstrained and compatible with general training schemes, while improving convergence accuracy and speed. Empirical studies on two LDN datasets show that compared to existing models, the MNNL model has higher prediction accuracy and convergence speed.",
    "link": "http://arxiv.org/abs/2305.02782",
    "context": "Title: A Momentum-Incorporated Non-Negative Latent Factorization of Tensors Model for Dynamic Network Representation. (arXiv:2305.02782v1 [cs.LG])\nAbstract: A large-scale dynamic network (LDN) is a source of data in many big data-related applications due to their large number of entities and large-scale dynamic interactions. They can be modeled as a high-dimensional incomplete (HDI) tensor that contains a wealth of knowledge about time patterns. A Latent factorization of tensors (LFT) model efficiently extracts this time pattern, which can be established using stochastic gradient descent (SGD) solvers. However, LFT models based on SGD are often limited by training schemes and have poor tail convergence. To solve this problem, this paper proposes a novel nonlinear LFT model (MNNL) based on momentum-incorporated SGD, which extracts non-negative latent factors from HDI tensors to make training unconstrained and compatible with general training schemes, while improving convergence accuracy and speed. Empirical studies on two LDN datasets show that compared to existing models, the MNNL model has higher prediction accuracy and convergence speed.",
    "path": "papers/23/05/2305.02782.json",
    "total_tokens": 914,
    "translated_title": "动态网络表征的动量非负张量分解模型",
    "translated_abstract": "大规模动态网络（LDN）由于其实体数量和大规模动态交互而成为许多大数据相关应用程序的数据源。它们可以被建模为包含有时间模式知识的高维不完整（HDI）张量。张量的潜在因子分解（LFT）模型可以有效地提取这些时间模式，可以使用随机梯度下降（SGD）求解器来建立。然而，基于SGD的LFT模型通常受到训练方案的限制，且收敛速度较慢。为解决这个问题，本文提出了一种基于动量SGD的非线性LFT模型（MNNL），该模型从HDI张量中提取非负潜在因子，使得训练无约束并与一般的训练方案兼容，同时提高了收敛精度和速度。两个LDN数据集上的实证研究表明，与现有模型相比，MNNL模型具有更高的预测准确性和收敛速度。",
    "tldr": "本文提出一种基于动量SGD的非线性LFT模型（MNNL）用于从高维不完整张量中提取非负潜在因子，以提高大规模动态网络的表征准确性和收敛速度。"
}