{
    "title": "Rescue Conversations from Dead-ends: Efficient Exploration for Task-oriented Dialogue Policy Optimization. (arXiv:2305.03262v1 [cs.HC])",
    "abstract": "Training a dialogue policy using deep reinforcement learning requires a lot of exploration of the environment. The amount of wasted invalid exploration makes their learning inefficient. In this paper, we find and define an important reason for the invalid exploration: dead-ends. When a conversation enters a dead-end state, regardless of the actions taken afterward, it will continue in a dead-end trajectory until the agent reaches a termination state or maximum turn. We propose a dead-end resurrection (DDR) algorithm that detects the initial dead-end state in a timely and efficient manner and provides a rescue action to guide and correct the exploration direction. To prevent dialogue policies from repeatedly making the same mistake, DDR also performs dialogue data augmentation by adding relevant experiences containing dead-end states. We first validate the dead-end detection reliability and then demonstrate the effectiveness and generality of the method by reporting experimental results",
    "link": "http://arxiv.org/abs/2305.03262",
    "context": "Title: Rescue Conversations from Dead-ends: Efficient Exploration for Task-oriented Dialogue Policy Optimization. (arXiv:2305.03262v1 [cs.HC])\nAbstract: Training a dialogue policy using deep reinforcement learning requires a lot of exploration of the environment. The amount of wasted invalid exploration makes their learning inefficient. In this paper, we find and define an important reason for the invalid exploration: dead-ends. When a conversation enters a dead-end state, regardless of the actions taken afterward, it will continue in a dead-end trajectory until the agent reaches a termination state or maximum turn. We propose a dead-end resurrection (DDR) algorithm that detects the initial dead-end state in a timely and efficient manner and provides a rescue action to guide and correct the exploration direction. To prevent dialogue policies from repeatedly making the same mistake, DDR also performs dialogue data augmentation by adding relevant experiences containing dead-end states. We first validate the dead-end detection reliability and then demonstrate the effectiveness and generality of the method by reporting experimental results",
    "path": "papers/23/05/2305.03262.json",
    "total_tokens": 878,
    "translated_title": "从死角中解救对话：面向任务型对话策略优化的高效探索",
    "translated_abstract": "使用深度强化学习训练对话策略需要对环境进行大量探索。无效的探索浪费了很多时间，使得学习效率低下。本文发现并定义了无效探索的一个重要原因：死角。当对话进入死角状态时，不管之后采取什么行动，它都将继续沿着死角轨迹，直到智能体达到终止状态或最大次数。我们提出了一种死角复活算法（DDR），及时高效地检测起始死角状态，并提供拯救行动以引导和纠正探索方向。为了防止对话策略不断犯同样的错误，DDR 还通过添加包含死角状态的相关经验进行对话数据扩充。本文首先验证了死角检测的可靠性，然后通过报告实验结果证明了该方法的有效性和普适性。",
    "tldr": "本文提出了一种死角复活（DDR）算法，可以及时高效地检测起始死角状态，并提供拯救行动以引导和纠正对话探索方向，同时防止对话策略不断犯同样的错误。",
    "en_tdlr": "This paper proposes a dead-end resurrection (DDR) algorithm which timely and efficiently detects the initial dead-end state and provides rescue action to guide and correct the exploration direction, also preventing dialogue policies from repeatedly making the same mistake. Experimental results demonstrate its effectiveness and generality."
}