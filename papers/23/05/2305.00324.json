{
    "title": "Representing Additive Gaussian Processes by Sparse Matrices. (arXiv:2305.00324v1 [stat.ML])",
    "abstract": "Among generalized additive models, additive Mat\\'ern Gaussian Processes (GPs) are one of the most popular for scalable high-dimensional problems. Thanks to their additive structure and stochastic differential equation representation, back-fitting-based algorithms can reduce the time complexity of computing the posterior mean from $O(n^3)$ to $O(n\\log n)$ time where $n$ is the data size. However, generalizing these algorithms to efficiently compute the posterior variance and maximum log-likelihood remains an open problem. In this study, we demonstrate that for Additive Mat\\'ern GPs, not only the posterior mean, but also the posterior variance, log-likelihood, and gradient of these three functions can be represented by formulas involving only sparse matrices and sparse vectors. We show how to use these sparse formulas to generalize back-fitting-based algorithms to efficiently compute the posterior mean, posterior variance, log-likelihood, and gradient of these three functions for additiv",
    "link": "http://arxiv.org/abs/2305.00324",
    "context": "Title: Representing Additive Gaussian Processes by Sparse Matrices. (arXiv:2305.00324v1 [stat.ML])\nAbstract: Among generalized additive models, additive Mat\\'ern Gaussian Processes (GPs) are one of the most popular for scalable high-dimensional problems. Thanks to their additive structure and stochastic differential equation representation, back-fitting-based algorithms can reduce the time complexity of computing the posterior mean from $O(n^3)$ to $O(n\\log n)$ time where $n$ is the data size. However, generalizing these algorithms to efficiently compute the posterior variance and maximum log-likelihood remains an open problem. In this study, we demonstrate that for Additive Mat\\'ern GPs, not only the posterior mean, but also the posterior variance, log-likelihood, and gradient of these three functions can be represented by formulas involving only sparse matrices and sparse vectors. We show how to use these sparse formulas to generalize back-fitting-based algorithms to efficiently compute the posterior mean, posterior variance, log-likelihood, and gradient of these three functions for additiv",
    "path": "papers/23/05/2305.00324.json",
    "total_tokens": 878,
    "translated_title": "用稀疏矩阵表示加性高斯过程",
    "translated_abstract": "在广义相加模型中，加性Matérn高斯过程是最受欢迎的可扩展高维问题之一。由于它们的加性结构和随机微分方程表示，基于回归的算法可以将计算后验均值的时间复杂度从$O（n^3）$减少到$O（nlogn）$时间，其中$n$是数据大小。但是，将这些算法推广到有效计算后验方差和最大对数似然仍然是一个未解决的问题。在本研究中，我们展示了对于加性Matérn高斯过程，不仅后验均值，而且后验方差、对数似然和这三个函数的梯度可以用仅涉及稀疏矩阵和稀疏向量的公式表示。我们展示了如何使用这些稀疏公式来推广回归算法，以有效计算这三个函数的后验均值、后验方差、对数似然和梯度。",
    "tldr": "本研究展示了对于加性Matérn高斯过程，通过稀疏矩阵和向量的公式可以有效地计算后验均值、后验方差、对数似然和梯度。",
    "en_tdlr": "This study demonstrates that for additive Matérn Gaussian processes, posterior mean, posterior variance, log-likelihood, and gradient of these three functions can be efficiently computed through formulas involving only sparse matrices and vectors."
}