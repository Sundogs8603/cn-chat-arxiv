{
    "title": "GATology for Linguistics: What Syntactic Dependencies It Knows. (arXiv:2305.13403v1 [cs.CL])",
    "abstract": "Graph Attention Network (GAT) is a graph neural network which is one of the strategies for modeling and representing explicit syntactic knowledge and can work with pre-trained models, such as BERT, in downstream tasks. Currently, there is still a lack of investigation into how GAT learns syntactic knowledge from the perspective of model structure. As one of the strategies for modeling explicit syntactic knowledge, GAT and BERT have never been applied and discussed in Machine Translation (MT) scenarios. We design a dependency relation prediction task to study how GAT learns syntactic knowledge of three languages as a function of the number of attention heads and layers. We also use a paired t-test and F1-score to clarify the differences in syntactic dependency prediction between GAT and BERT fine-tuned by the MT task (MT-B). The experiments show that better performance can be achieved by appropriately increasing the number of attention heads with two GAT layers. With more than two layer",
    "link": "http://arxiv.org/abs/2305.13403",
    "context": "Title: GATology for Linguistics: What Syntactic Dependencies It Knows. (arXiv:2305.13403v1 [cs.CL])\nAbstract: Graph Attention Network (GAT) is a graph neural network which is one of the strategies for modeling and representing explicit syntactic knowledge and can work with pre-trained models, such as BERT, in downstream tasks. Currently, there is still a lack of investigation into how GAT learns syntactic knowledge from the perspective of model structure. As one of the strategies for modeling explicit syntactic knowledge, GAT and BERT have never been applied and discussed in Machine Translation (MT) scenarios. We design a dependency relation prediction task to study how GAT learns syntactic knowledge of three languages as a function of the number of attention heads and layers. We also use a paired t-test and F1-score to clarify the differences in syntactic dependency prediction between GAT and BERT fine-tuned by the MT task (MT-B). The experiments show that better performance can be achieved by appropriately increasing the number of attention heads with two GAT layers. With more than two layer",
    "path": "papers/23/05/2305.13403.json",
    "total_tokens": 1046,
    "translated_title": "GATology在语言学中的应用: 它了解哪些句法依存关系？",
    "translated_abstract": "图注意力网络（GAT）是一种图形神经网络，是模拟和表示明确的句法知识的策略之一，可以与预训练模型（如BERT）一起在下游任务中使用。目前，从模型结构的角度来研究GAT如何学习句法知识仍然缺乏研究。作为模型明确句法知识的策略之一，GAT和BERT从未被应用和讨论过的机器翻译（MT）场景。我们设计了一个依赖关系预测任务，研究了GAT如何作为关注头和层数数量的函数来学习三种语言的句法知识。我们还使用配对t检验和F1分数来澄清GAT与MT-B微调之间的句法依存关系预测差异。实现表明，通过适当增加两个GAT层的关注头的数量可以实现更好的性能。在超过两层的情况下，无论关注头的数量如何，性能均没有显着提高。此外，我们的实验表明，在下游MT任务中，GAT在语法建模方面的表现略好于MT-B。",
    "tldr": "本文探究了使用GAT和BERT等预训练模型在机器翻译场景中建模句法知识的方法。实验表明，通过适当的层数和关注头数量，可以实现更好的性能。此外，相对于MT-B，GAT在下游MT任务中的语法建模方面略微更好。",
    "en_tdlr": "This paper investigates the use of pre-trained models such as GAT and BERT for modeling syntactic knowledge in the context of machine translation. The experiments show that better performance can be achieved by appropriately increasing the number of layers and attention heads. Additionally, GAT performs slightly better than MT-B in syntax modeling for the downstream MT task."
}