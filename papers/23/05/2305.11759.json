{
    "title": "Controlling the Extraction of Memorized Data from Large Language Models via Prompt-Tuning. (arXiv:2305.11759v1 [cs.CL])",
    "abstract": "Large Language Models (LLMs) are known to memorize significant portions of their training data. Parts of this memorized content have been shown to be extractable by simply querying the model, which poses a privacy risk. We present a novel approach which uses prompt-tuning to control the extraction rates of memorized content in LLMs. We present two prompt training strategies to increase and decrease extraction rates, which correspond to an attack and a defense, respectively. We demonstrate the effectiveness of our techniques by using models from the GPT-Neo family on a public benchmark. For the 1.3B parameter GPT-Neo model, our attack yields a 9.3 percentage point increase in extraction rate compared to our baseline. Our defense can be tuned to achieve different privacy-utility trade-offs by a user-specified hyperparameter. We achieve an extraction rate reduction of up to 97.7% relative to our baseline, with a perplexity increase of 16.9%.",
    "link": "http://arxiv.org/abs/2305.11759",
    "context": "Title: Controlling the Extraction of Memorized Data from Large Language Models via Prompt-Tuning. (arXiv:2305.11759v1 [cs.CL])\nAbstract: Large Language Models (LLMs) are known to memorize significant portions of their training data. Parts of this memorized content have been shown to be extractable by simply querying the model, which poses a privacy risk. We present a novel approach which uses prompt-tuning to control the extraction rates of memorized content in LLMs. We present two prompt training strategies to increase and decrease extraction rates, which correspond to an attack and a defense, respectively. We demonstrate the effectiveness of our techniques by using models from the GPT-Neo family on a public benchmark. For the 1.3B parameter GPT-Neo model, our attack yields a 9.3 percentage point increase in extraction rate compared to our baseline. Our defense can be tuned to achieve different privacy-utility trade-offs by a user-specified hyperparameter. We achieve an extraction rate reduction of up to 97.7% relative to our baseline, with a perplexity increase of 16.9%.",
    "path": "papers/23/05/2305.11759.json",
    "total_tokens": 1013,
    "translated_title": "通过Prompt-Tuning控制从大型语言模型中提取记忆数据",
    "translated_abstract": "大型语言模型（LLMs）已知会记忆其训练数据的重要部分，并且简单查询模型即可提取记忆的内容，存在隐私风险。我们提出了一种新的方法，使用Prompt-Tuning来控制LLMs中记忆内容的提取率。我们提出了两个Prompt训练策略来增加和减少提取率，分别对应攻击和防御。我们使用来自GPT-Neo系列的模型在公共基准测试中展示了我们技术的有效性。对于1.3B参数的GPT-Neo模型，我们的攻击相对于基线产生了9.3个百分点的提取率增加。通过用户指定的超参数，我们的防御可以调整以实现不同的隐私-效用权衡。我们相对于基线实现了最多97.7%的提取率减少，同时困惑度增加了16.9%。",
    "tldr": "该论文提出在大型语言模型中通过Prompt-Tuning策略来控制从中提取记忆数据的方法，提供了攻击和防御两种训练策略。在公共基准测试中展示了其在GPT-Neo模型中的有效性，攻击策略相对于基线产生了9.3%的提取率增加，而防御策略可以调整以实现不同的隐私-效用权衡，可以实现最多97.7%的提取率减少，但困惑度增加了16.9%。",
    "en_tdlr": "The paper proposes a Prompt-Tuning strategy to control the extraction of memorized data from large language models. Two prompt training strategies are presented for attack and defense purposes. The effectiveness of the techniques is demonstrated on GPT-Neo models, with a 9.3% increase in extraction rate for the attack and an up to 97.7% reduction in extraction rate for the defense with a 16.9% increase in perplexity."
}