{
    "title": "Evaluating Factual Consistency of Summaries with Large Language Models. (arXiv:2305.14069v2 [cs.CL] UPDATED)",
    "abstract": "Detecting factual errors in summaries has been an important and challenging subject in summarization research. Inspired by the emergent ability of large language models (LLMs), we explore evaluating factual consistency of summaries by directly prompting LLMs. We present a comprehensive empirical study to assess the ability of LLMs as factual consistency evaluators, which consists of (1) analyzing different LLMs such as the GPT model series and Flan-T5; (2) investigating a variety of prompting methods including vanilla prompting, chain-of-thought prompting, and a sentence-by-sentence prompting method to tackle long summaries; and (3) evaluating on diverse summaries generated by multiple summarization systems, ranging from pre-transformer methods to SOTA pretrained models. Our experiments demonstrate that prompting LLMs is able to outperform the previous best factuality systems in all settings, by up to 12.2 absolute points in terms of the binary classification accuracy on inconsistency ",
    "link": "http://arxiv.org/abs/2305.14069",
    "context": "Title: Evaluating Factual Consistency of Summaries with Large Language Models. (arXiv:2305.14069v2 [cs.CL] UPDATED)\nAbstract: Detecting factual errors in summaries has been an important and challenging subject in summarization research. Inspired by the emergent ability of large language models (LLMs), we explore evaluating factual consistency of summaries by directly prompting LLMs. We present a comprehensive empirical study to assess the ability of LLMs as factual consistency evaluators, which consists of (1) analyzing different LLMs such as the GPT model series and Flan-T5; (2) investigating a variety of prompting methods including vanilla prompting, chain-of-thought prompting, and a sentence-by-sentence prompting method to tackle long summaries; and (3) evaluating on diverse summaries generated by multiple summarization systems, ranging from pre-transformer methods to SOTA pretrained models. Our experiments demonstrate that prompting LLMs is able to outperform the previous best factuality systems in all settings, by up to 12.2 absolute points in terms of the binary classification accuracy on inconsistency ",
    "path": "papers/23/05/2305.14069.json",
    "total_tokens": 911,
    "translated_title": "用大型语言模型评估摘要的事实一致性",
    "translated_abstract": "在摘要研究中，检测事实错误一直是一个重要而具有挑战性的课题。受到大型语言模型（LLMs）新兴的能力的启发，我们探索通过直接提示LLMs来评估摘要的事实一致性。我们进行了一项全面的实证研究，评估LLMs作为事实一致性评估器的能力，其中包括(1)分析不同的LLMs，如GPT模型系列和Flan-T5;(2)研究各种提示方法，包括vanilla提示、思维链提示和逐句提示方法来处理长篇摘要;(3)评估多个摘要系统生成的多样化摘要，范围从预变压器方法到SOTA预训练模型。我们的实验表明，在所有设置中，提示LLMs能够优于先前最佳的事实性系统，对于不一致性的二分类准确性，提高了最多12.2个绝对点。",
    "tldr": "本研究通过直接提示大型语言模型（LLMs），探索评估摘要的事实一致性。实验证明，在各种设置中，提示LLMs能够在二分类准确性方面超过以前最佳的事实性系统，最高可提高12.2个绝对点。"
}