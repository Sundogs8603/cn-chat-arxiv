{
    "title": "Asking Clarification Questions to Handle Ambiguity in Open-Domain QA. (arXiv:2305.13808v1 [cs.CL])",
    "abstract": "Ambiguous questions persist in open-domain question answering, because formulating a precise question with a unique answer is often challenging. Previously, Min et al. (2020) have tackled this issue by generating disambiguated questions for all possible interpretations of the ambiguous question. This can be effective, but not ideal for providing an answer to the user. Instead, we propose to ask a clarification question, where the user's response will help identify the interpretation that best aligns with the user's intention. We first present CAMBIGNQ, a dataset consisting of 5,654 ambiguous questions, each with relevant passages, possible answers, and a clarification question. The clarification questions were efficiently created by generating them using InstructGPT and manually revising them as necessary. We then define a pipeline of tasks and design appropriate evaluation metrics. Lastly, we achieve 61.3 F1 on ambiguity detection and 40.5 F1 on clarification-based QA, providing stron",
    "link": "http://arxiv.org/abs/2305.13808",
    "context": "Title: Asking Clarification Questions to Handle Ambiguity in Open-Domain QA. (arXiv:2305.13808v1 [cs.CL])\nAbstract: Ambiguous questions persist in open-domain question answering, because formulating a precise question with a unique answer is often challenging. Previously, Min et al. (2020) have tackled this issue by generating disambiguated questions for all possible interpretations of the ambiguous question. This can be effective, but not ideal for providing an answer to the user. Instead, we propose to ask a clarification question, where the user's response will help identify the interpretation that best aligns with the user's intention. We first present CAMBIGNQ, a dataset consisting of 5,654 ambiguous questions, each with relevant passages, possible answers, and a clarification question. The clarification questions were efficiently created by generating them using InstructGPT and manually revising them as necessary. We then define a pipeline of tasks and design appropriate evaluation metrics. Lastly, we achieve 61.3 F1 on ambiguity detection and 40.5 F1 on clarification-based QA, providing stron",
    "path": "papers/23/05/2305.13808.json",
    "total_tokens": 873,
    "translated_title": "开放域QA中通过提问澄清解决歧义问题",
    "translated_abstract": "开放域中存在歧义问题，如何提出一个准确且独一无二的问题是很具挑战性的。过去，Min et al. (2020) 通过为所有可能的解释生成消除歧义的问题来解决这个问题。这种方法可能有效，但并不理想。我们提出通过提问澄清来解决这个问题，用户的回答将帮助确定最符合用户意图的解释。我们首先介绍了一个数据集CAMBIGNQ，该数据集由5,654个含有相关段落、可能的答案和澄清问题的歧义问题组成。这些澄清问题是通过使用InstructGPT生成然后进行必要的手动修订来高效创建的。然后我们定义了一系列任务和相应的评估指标。最后，我们在歧义检测上获得了61.3 F1，在澄清型QA上获得了40.5 F1，为提供强有力的答案提出了解决方案。",
    "tldr": "本文提出了一种解决开放域QA中歧义问题的方法：通过提问澄清来确定最符合用户意图的解释。",
    "en_tdlr": "This paper proposes a method to resolve ambiguity in open-domain question answering by asking clarification questions to identify the interpretation that best aligns with the user's intention. They achieve promising results on both ambiguity detection and clarification-based QA, using a dataset of 5,654 ambiguous questions with clarification questions created efficiently using InstructGPT."
}