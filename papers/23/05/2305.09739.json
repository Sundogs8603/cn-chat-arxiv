{
    "title": "Outage Performance and Novel Loss Function for an ML-Assisted Resource Allocation: An Exact Analytical Framework. (arXiv:2305.09739v1 [eess.SP])",
    "abstract": "Machine Learning (ML) is a popular tool that will be pivotal in enabling 6G and beyond communications. This paper focuses on applying ML solutions to address outage probability issues commonly encountered in these systems. In particular, we consider a single-user multi-resource greedy allocation strategy, where an ML binary classification predictor assists in seizing an adequate resource. With no access to future channel state information, this predictor foresees each resource's likely future outage status. When the predictor encounters a resource it believes will be satisfactory, it allocates it to the user. Critically, the goal of the predictor is to ensure that a user avoids an unsatisfactory resource since this is likely to cause an outage. Our main result establishes exact and asymptotic expressions for this system's outage probability. With this, we formulate a theoretically optimal, differentiable loss function to train our predictor. We then compare predictors trained using thi",
    "link": "http://arxiv.org/abs/2305.09739",
    "context": "Title: Outage Performance and Novel Loss Function for an ML-Assisted Resource Allocation: An Exact Analytical Framework. (arXiv:2305.09739v1 [eess.SP])\nAbstract: Machine Learning (ML) is a popular tool that will be pivotal in enabling 6G and beyond communications. This paper focuses on applying ML solutions to address outage probability issues commonly encountered in these systems. In particular, we consider a single-user multi-resource greedy allocation strategy, where an ML binary classification predictor assists in seizing an adequate resource. With no access to future channel state information, this predictor foresees each resource's likely future outage status. When the predictor encounters a resource it believes will be satisfactory, it allocates it to the user. Critically, the goal of the predictor is to ensure that a user avoids an unsatisfactory resource since this is likely to cause an outage. Our main result establishes exact and asymptotic expressions for this system's outage probability. With this, we formulate a theoretically optimal, differentiable loss function to train our predictor. We then compare predictors trained using thi",
    "path": "papers/23/05/2305.09739.json",
    "total_tokens": 1149,
    "translated_title": "ML辅助资源分配的失误性能和新型损失函数: 一种精确分析框架",
    "translated_abstract": "机器学习是使6G及以上通信成为可能的重要工具。本文致力于将机器学习应用于解决这些系统普遍遇到的失误概率问题。具体来说，我们考虑单用户多资源贪婪分配策略，其中一个ML二分类预测器在选择充足资源时进行辅助。当预测器遇到相信会满意的资源时，它将其分配给用户。我们的主要结果是建立了该系统失误概率的精确和渐进表达式。在此基础上，我们制定了一种理论最优的、可微的损失函数来训练预测器。我们通过大量模拟比较使用此损失函数训练的预测器和使用其他常用损失函数训练的预测器之间的性能差异。我们还探讨了所提出的损失函数对预测器透明度和无线信道结构的影响。我们的数值结果表明，所提出的损失函数在失误概率、端到端学习速度和收敛方面优于现有的基于二元交叉熵的损失。此外，我们的损失函数制定赋予了可解释的预测器训练，能够稳健地处理信道的时变性。",
    "tldr": "本文关注在无法获得未来信道状态信息时，利用机器学习解决不同资源分配对应的失误概率问题。提出了一种新型理论最优、可解释的损失函数，经过模拟验证其在失误概率、学习速度和收敛等方面的表现优于一些常见的损失函数。",
    "en_tdlr": "This paper focuses on applying machine learning to address the outage probability issues commonly encountered in communication systems beyond 6G. The proposed approach utilizes a novel, theoretically optimal and interpretable loss function to train an ML binary classification predictor which assists in selecting resources with satisfactory outage status. Results show that this approach outperforms existing methods in terms of outage probability, learning speed, and convergence."
}