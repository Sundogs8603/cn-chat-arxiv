{
    "title": "LeXFiles and LegalLAMA: Facilitating English Multinational Legal Language Model Development. (arXiv:2305.07507v1 [cs.CL])",
    "abstract": "In this work, we conduct a detailed analysis on the performance of legal-oriented pre-trained language models (PLMs). We examine the interplay between their original objective, acquired knowledge, and legal language understanding capacities which we define as the upstream, probing, and downstream performance, respectively. We consider not only the models' size but also the pre-training corpora used as important dimensions in our study. To this end, we release a multinational English legal corpus (LeXFiles) and a legal knowledge probing benchmark (LegalLAMA) to facilitate training and detailed analysis of legal-oriented PLMs. We release two new legal PLMs trained on LeXFiles and evaluate them alongside others on LegalLAMA and LexGLUE. We find that probing performance strongly correlates with upstream performance in related legal topics. On the other hand, downstream performance is mainly driven by the model's size and prior legal knowledge which can be estimated by upstream and probing ",
    "link": "http://arxiv.org/abs/2305.07507",
    "context": "Title: LeXFiles and LegalLAMA: Facilitating English Multinational Legal Language Model Development. (arXiv:2305.07507v1 [cs.CL])\nAbstract: In this work, we conduct a detailed analysis on the performance of legal-oriented pre-trained language models (PLMs). We examine the interplay between their original objective, acquired knowledge, and legal language understanding capacities which we define as the upstream, probing, and downstream performance, respectively. We consider not only the models' size but also the pre-training corpora used as important dimensions in our study. To this end, we release a multinational English legal corpus (LeXFiles) and a legal knowledge probing benchmark (LegalLAMA) to facilitate training and detailed analysis of legal-oriented PLMs. We release two new legal PLMs trained on LeXFiles and evaluate them alongside others on LegalLAMA and LexGLUE. We find that probing performance strongly correlates with upstream performance in related legal topics. On the other hand, downstream performance is mainly driven by the model's size and prior legal knowledge which can be estimated by upstream and probing ",
    "path": "papers/23/05/2305.07507.json",
    "total_tokens": 955,
    "translated_title": "LeXFiles和LegalLAMA：促进英语跨国法律语言模型的开发",
    "translated_abstract": "本文对面向法律的预训练语言模型（PLMs）的性能进行了详细分析。我们考察了它们的原始目标、获取的知识和法律语言理解能力之间的相互作用，将其定义为上游、探针和下游性能。我们不仅考虑了模型的大小，还将预训练语料库作为研究中的重要维度。为此，我们发布了一个跨国英语法律文集（LeXFiles）和一个法律知识探针基准（LegalLAMA），以促进法律导向PLMs的训练和详细分析。我们发布了两个在LeXFiles上训练的新的法律PLMs，并在LegalLAMA和LexGLUE上进行了评估。我们发现，在相关法律主题中，探针性能与上游性能强相关。另一方面，下游性能主要由模型的大小和先前的法律知识驱动，可以通过上游和探针性能来估计。",
    "tldr": "本文对面向法律的预训练语言模型进行了详细分析，发布了一个跨国英语法律文集和一个法律知识探针基准，发现探针性能与上游性能强相关，下游性能主要由模型的大小和先前的法律知识驱动。",
    "en_tdlr": "This paper conducts a detailed analysis on legal-oriented pre-trained language models (PLMs), releases a multinational English legal corpus (LeXFiles) and a legal knowledge probing benchmark (LegalLAMA), and finds that probing performance strongly correlates with upstream performance in related legal topics and downstream performance is mainly driven by the model's size and prior legal knowledge."
}