{
    "title": "Harnessing the Power of BERT in the Turkish Clinical Domain: Pretraining Approaches for Limited Data Scenarios. (arXiv:2305.03788v1 [cs.CL])",
    "abstract": "In recent years, major advancements in natural language processing (NLP) have been driven by the emergence of large language models (LLMs), which have significantly revolutionized research and development within the field. Building upon this progress, our study delves into the effects of various pre-training methodologies on Turkish clinical language models' performance in a multi-label classification task involving radiology reports, with a focus on addressing the challenges posed by limited language resources. Additionally, we evaluated the simultaneous pretraining approach by utilizing limited clinical task data for the first time. We developed four models, including TurkRadBERT-task v1, TurkRadBERT-task v2, TurkRadBERT-sim v1, and TurkRadBERT-sim v2. Our findings indicate that the general Turkish BERT model (BERTurk) and TurkRadBERT-task v1, both of which utilize knowledge from a substantial general-domain corpus, demonstrate the best overall performance. Although the task-adaptive",
    "link": "http://arxiv.org/abs/2305.03788",
    "context": "Title: Harnessing the Power of BERT in the Turkish Clinical Domain: Pretraining Approaches for Limited Data Scenarios. (arXiv:2305.03788v1 [cs.CL])\nAbstract: In recent years, major advancements in natural language processing (NLP) have been driven by the emergence of large language models (LLMs), which have significantly revolutionized research and development within the field. Building upon this progress, our study delves into the effects of various pre-training methodologies on Turkish clinical language models' performance in a multi-label classification task involving radiology reports, with a focus on addressing the challenges posed by limited language resources. Additionally, we evaluated the simultaneous pretraining approach by utilizing limited clinical task data for the first time. We developed four models, including TurkRadBERT-task v1, TurkRadBERT-task v2, TurkRadBERT-sim v1, and TurkRadBERT-sim v2. Our findings indicate that the general Turkish BERT model (BERTurk) and TurkRadBERT-task v1, both of which utilize knowledge from a substantial general-domain corpus, demonstrate the best overall performance. Although the task-adaptive",
    "path": "papers/23/05/2305.03788.json",
    "total_tokens": 1042,
    "translated_title": "在土耳其临床领域中利用BERT的力量：面向有限数据场景的预训练方法研究",
    "translated_abstract": "近年来，自然语言处理(NLP)方面的主要进展受到了大型语言模型(LLMs)的推动，这在该领域的研究和发展中有着显著的革命性影响。本研究基于这一进展，探讨了不同预训练方法对土耳其临床语言模型在包含放射学报告的多标签分类任务中表现的影响，并重点关注解决有限语言资源带来的挑战。此外，我们第一次利用有限的临床任务数据来评估同时预训练的方法。我们开发了四个模型，包括TurkRadBERT-task v1、TurkRadBERT-task v2、TurkRadBERT-sim v1和TurkRadBERT-sim v2。我们的研究结果表明，利用大量通用领域语料库知识的泛用性土耳其BERT模型(BERTurk)和TurkRadBERT-task v1表现最佳。虽然任务自适应预训练方法的性能也有所提高，但仍不如上述模型。",
    "tldr": "本研究针对土耳其临床语言模型在有限数据场景下的性能问题，探讨了不同预训练方法的影响，并指出利用大量通用领域语料库知识的BERTurk和TurkRadBERT-task v1表现最佳。",
    "en_tdlr": "This study focuses on the pre-training approaches for limited data scenarios in the Turkish clinical domain, aiming to enhance the performance of TurkRadBERT for a multi-label classification task of radiology reports. The results indicate that utilizing knowledge from a substantial general-domain corpus is essential, and the BERTurk and TurkRadBERT-task v1 show the best performance."
}