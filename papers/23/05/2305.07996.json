{
    "title": "Successive Affine Learning for Deep Neural Networks. (arXiv:2305.07996v1 [cs.LG])",
    "abstract": "This paper introduces a successive affine learning (SAL) model for constructing deep neural networks (DNNs). Traditionally, a DNN is built by solving a non-convex optimization problem. It is often challenging to solve such a problem numerically due to its non-convexity and having a large number of layers. To address this challenge, inspired by the human education system, the multi-grade deep learning (MGDL) model was recently initiated by the author of this paper. The MGDL model learns a DNN in several grades, in each of which one constructs a shallow DNN consisting of a small number of layers. The MGDL model still requires solving several non-convex optimization problems. The proposed SAL model mutates from the MGDL model. Noting that each layer of a DNN consists of an affine map followed by an activation function, we propose to learn the affine map by solving a quadratic/convex optimization problem which involves the activation function only {\\it after} the weight matrix and the bias",
    "link": "http://arxiv.org/abs/2305.07996",
    "context": "Title: Successive Affine Learning for Deep Neural Networks. (arXiv:2305.07996v1 [cs.LG])\nAbstract: This paper introduces a successive affine learning (SAL) model for constructing deep neural networks (DNNs). Traditionally, a DNN is built by solving a non-convex optimization problem. It is often challenging to solve such a problem numerically due to its non-convexity and having a large number of layers. To address this challenge, inspired by the human education system, the multi-grade deep learning (MGDL) model was recently initiated by the author of this paper. The MGDL model learns a DNN in several grades, in each of which one constructs a shallow DNN consisting of a small number of layers. The MGDL model still requires solving several non-convex optimization problems. The proposed SAL model mutates from the MGDL model. Noting that each layer of a DNN consists of an affine map followed by an activation function, we propose to learn the affine map by solving a quadratic/convex optimization problem which involves the activation function only {\\it after} the weight matrix and the bias",
    "path": "papers/23/05/2305.07996.json",
    "total_tokens": 952,
    "translated_title": "深度神经网络的连续仿射学习",
    "translated_abstract": "本文介绍了一种用于构建深度神经网络(DNNs)的连续仿射学习(SAL)模型。传统上，DNN是通过解决非凸优化问题来构建的。由于其非凸性和层数众多，通常难以在数值上解决这种问题。为了解决这一挑战，本文作者启发于人类教育系统，最近提出了多级深度学习(MGDL)模型。MGDL模型以多个年级的形式学习DNN，在每个年级中构建由少量层数组成的浅层DNN。MGDL模型仍需要解决几个非凸优化问题。提出的SAL模型是在MGDL模型基础上演变而来。发现DNN的每层都由仿射映射和激活函数组成，我们建议通过解决涉及激活函数的二次/凸优化问题来学习仿射映射，但是在权重矩阵和偏差之后。",
    "tldr": "本文提出了一种连续仿射学习（SAL）模型，用于构建深度神经网络(DNNs)。 该模型通过解决涉及激活函数的二次/凸优化问题来学习仿射映射，但是在权重矩阵和偏差之后。",
    "en_tdlr": "The paper proposes a successive affine learning (SAL) model for constructing deep neural networks (DNNs). The SAL model learns the affine map by solving a quadratic/convex optimization problem which involves the activation function only after the weight matrix and the bias."
}