{
    "title": "Is a Video worth $n\\times n$ Images? A Highly Efficient Approach to Transformer-based Video Question Answering. (arXiv:2305.09107v1 [cs.CV])",
    "abstract": "Conventional Transformer-based Video Question Answering (VideoQA) approaches generally encode frames independently through one or more image encoders followed by interaction between frames and question. However, such schema would incur significant memory use and inevitably slow down the training and inference speed. In this work, we present a highly efficient approach for VideoQA based on existing vision-language pre-trained models where we concatenate video frames to a $n\\times n$ matrix and then convert it to one image. By doing so, we reduce the use of the image encoder from $n^{2}$ to $1$ while maintaining the temporal structure of the original video. Experimental results on MSRVTT and TrafficQA show that our proposed approach achieves state-of-the-art performance with nearly $4\\times$ faster speed and only 30% memory use. We show that by integrating our approach into VideoQA systems we can achieve comparable, even superior, performance with a significant speed up for training and ",
    "link": "http://arxiv.org/abs/2305.09107",
    "context": "Title: Is a Video worth $n\\times n$ Images? A Highly Efficient Approach to Transformer-based Video Question Answering. (arXiv:2305.09107v1 [cs.CV])\nAbstract: Conventional Transformer-based Video Question Answering (VideoQA) approaches generally encode frames independently through one or more image encoders followed by interaction between frames and question. However, such schema would incur significant memory use and inevitably slow down the training and inference speed. In this work, we present a highly efficient approach for VideoQA based on existing vision-language pre-trained models where we concatenate video frames to a $n\\times n$ matrix and then convert it to one image. By doing so, we reduce the use of the image encoder from $n^{2}$ to $1$ while maintaining the temporal structure of the original video. Experimental results on MSRVTT and TrafficQA show that our proposed approach achieves state-of-the-art performance with nearly $4\\times$ faster speed and only 30% memory use. We show that by integrating our approach into VideoQA systems we can achieve comparable, even superior, performance with a significant speed up for training and ",
    "path": "papers/23/05/2305.09107.json",
    "total_tokens": 1027,
    "translated_title": "视频值得 $n\\times n$ 张图像吗? 一种基于Transformer的视频问答高效方法。",
    "translated_abstract": "传统的基于Transformer的视频问答（VideoQA）方法通常通过一个或多个图像编码器独立编码帧，并在帧和问题之间进行交互。然而，这种模式会导致显著的内存使用和训练和推理速度的不可避免的减慢。本文提出了一种基于现有视觉-语言预训练模型的高效VideoQA方法，其中我们将视频帧连接到一个 $n\\times n$ 矩阵中，然后将其转换为一张图像。通过这样做，我们将图像编码器的使用从 $n^{2}$减少到1，同时保持了原始视频的时间结构。在MSRVTT和TrafficQA上的实验结果表明，我们提出的方法以近 $4\\times$ 更快的速度和只有30％的内存使用实现了最先进的性能。我们展示了通过将我们的方法集成到VideoQA系统中，我们能够在只有很小代价的情况下实现可比甚至优异的表现，同时训练和推理速度显著加快。",
    "tldr": "本文提出了一种高效的Transformer-based Video Question Answering方法，即将视频帧连接成 $n\\times n$ 的矩阵，从而将图像编码器的使用量从 $n^{2}$ 减少到1，从而显著提高了训练和推理速度和节省了存储空间，而仍然保持了原始视频的时间结构，并在实验中取得了较好结果。"
}