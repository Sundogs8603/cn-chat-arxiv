{
    "title": "Reducing Communication for Split Learning by Randomized Top-k Sparsification. (arXiv:2305.18469v1 [cs.LG])",
    "abstract": "Split learning is a simple solution for Vertical Federated Learning (VFL), which has drawn substantial attention in both research and application due to its simplicity and efficiency. However, communication efficiency is still a crucial issue for split learning. In this paper, we investigate multiple communication reduction methods for split learning, including cut layer size reduction, top-k sparsification, quantization, and L1 regularization. Through analysis of the cut layer size reduction and top-k sparsification, we further propose randomized top-k sparsification, to make the model generalize and converge better. This is done by selecting top-k elements with a large probability while also having a small probability to select non-top-k elements. Empirical results show that compared with other communication-reduction methods, our proposed randomized top-k sparsification achieves a better model performance under the same compression level.",
    "link": "http://arxiv.org/abs/2305.18469",
    "context": "Title: Reducing Communication for Split Learning by Randomized Top-k Sparsification. (arXiv:2305.18469v1 [cs.LG])\nAbstract: Split learning is a simple solution for Vertical Federated Learning (VFL), which has drawn substantial attention in both research and application due to its simplicity and efficiency. However, communication efficiency is still a crucial issue for split learning. In this paper, we investigate multiple communication reduction methods for split learning, including cut layer size reduction, top-k sparsification, quantization, and L1 regularization. Through analysis of the cut layer size reduction and top-k sparsification, we further propose randomized top-k sparsification, to make the model generalize and converge better. This is done by selecting top-k elements with a large probability while also having a small probability to select non-top-k elements. Empirical results show that compared with other communication-reduction methods, our proposed randomized top-k sparsification achieves a better model performance under the same compression level.",
    "path": "papers/23/05/2305.18469.json",
    "total_tokens": 813,
    "translated_title": "通过随机Top-k稀疏化减少Split Learning的通信成本",
    "translated_abstract": "Split Learning是Vertical Federated Learning (VFL)的简单解决方案之一，由于其简单性和高效性，在研究和应用中受到了广泛关注。然而，通信效率仍然是Split Learning的一个重要问题。本文研究了多种Split Learning的通信量减少方法，包括减少切割层大小、Top-k稀疏化、量化和L1正则化。通过对减少切割层大小和Top-k稀疏化的分析，我们进一步提出了随机Top-k稀疏化，以使模型更好地推广和收敛。这是通过在选择Top-k元素的同时，有小概率地选择非Top-k元素来实现的。实验结果表明，与其他通信量减少方法相比，我们提出的随机Top-k稀疏化在相同压缩水平下实现了更好的模型性能。",
    "tldr": "提出了一种通过随机Top-k稀疏化减少通信效率的Split Learning方法，实验证明该方法在相同压缩水平下比其他方法有更好的模型性能。",
    "en_tdlr": "The paper proposes a communication reduction method for split learning by randomized top-k sparsification, which achieves better model performance compared to other methods at the same compression level."
}