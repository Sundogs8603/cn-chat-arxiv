{
    "title": "Differentially-Private Decision Trees and Provable Robustness to Data Poisoning. (arXiv:2305.15394v2 [cs.LG] UPDATED)",
    "abstract": "Decision trees are interpretable models that are well-suited to non-linear learning problems. Much work has been done on extending decision tree learning algorithms with differential privacy, a system that guarantees the privacy of samples within the training data. However, current state-of-the-art algorithms for this purpose sacrifice much utility for a small privacy benefit. These solutions create random decision nodes that reduce decision tree accuracy or spend an excessive share of the privacy budget on labeling leaves. Moreover, many works do not support continuous features or leak information about them. We propose a new method called PrivaTree based on private histograms that chooses good splits while consuming a small privacy budget. The resulting trees provide a significantly better privacy-utility trade-off and accept mixed numerical and categorical data without leaking information about numerical features. Finally, while it is notoriously hard to give robustness guarantees a",
    "link": "http://arxiv.org/abs/2305.15394",
    "context": "Title: Differentially-Private Decision Trees and Provable Robustness to Data Poisoning. (arXiv:2305.15394v2 [cs.LG] UPDATED)\nAbstract: Decision trees are interpretable models that are well-suited to non-linear learning problems. Much work has been done on extending decision tree learning algorithms with differential privacy, a system that guarantees the privacy of samples within the training data. However, current state-of-the-art algorithms for this purpose sacrifice much utility for a small privacy benefit. These solutions create random decision nodes that reduce decision tree accuracy or spend an excessive share of the privacy budget on labeling leaves. Moreover, many works do not support continuous features or leak information about them. We propose a new method called PrivaTree based on private histograms that chooses good splits while consuming a small privacy budget. The resulting trees provide a significantly better privacy-utility trade-off and accept mixed numerical and categorical data without leaking information about numerical features. Finally, while it is notoriously hard to give robustness guarantees a",
    "path": "papers/23/05/2305.15394.json",
    "total_tokens": 996,
    "translated_title": "差分隐私决策树与对数据篡改的可靠性证明",
    "translated_abstract": "决策树是适用于非线性学习问题的可解释模型。关于将差分隐私引入决策树学习算法的研究已经很多，差分隐私能够确保训练数据中样本的隐私性。然而，目前用于此目的的最先进算法在获得一点点隐私保护的同时牺牲了较多的模型效用。这些解决方案引入了随机决策节点，降低了决策树的准确性，或者在标记叶子节点上使用过多的隐私预算。此外，很多方法不支持连续特征或者泄露与连续特征相关的信息。我们提出了一种基于私有直方图的新方法，称为PrivaTree，它在消耗一小部分隐私预算的同时选择合适的分割点。由此产生的决策树在隐私效用权衡方面取得了显著的提升，而且能够接受混合的数值和类别数据而不泄露与数值特征相关的信息。最后，尽管给出可靠性保证一直很难，我们的方法在数据篡改方面表现出了可靠性。",
    "tldr": "本论文提出了一种名为PrivaTree的差分隐私决策树方法，通过使用私有直方图选择分割点来在隐私保护与模型效用之间取得更好的平衡。这种方法能够接收混合的数值和类别数据，并且能够在数据篡改方面表现出可靠性。",
    "en_tdlr": "This paper proposes a differentially-private decision tree method called PrivaTree, which achieves a better balance between privacy protection and model utility by using private histograms to select split points. This method can handle mixed numerical and categorical data, and exhibits robustness against data poisoning."
}