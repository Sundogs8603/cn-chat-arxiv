{
    "title": "Self-attention Dual Embedding for Graphs with Heterophily. (arXiv:2305.18385v1 [cs.LG])",
    "abstract": "Graph Neural Networks (GNNs) have been highly successful for the node classification task. GNNs typically assume graphs are homophilic, i.e. neighboring nodes are likely to belong to the same class. However, a number of real-world graphs are heterophilic, and this leads to much lower classification accuracy using standard GNNs. In this work, we design a novel GNN which is effective for both heterophilic and homophilic graphs. Our work is based on three main observations. First, we show that node features and graph topology provide different amounts of informativeness in different graphs, and therefore they should be encoded independently and prioritized in an adaptive manner. Second, we show that allowing negative attention weights when propagating graph topology information improves accuracy. Finally, we show that asymmetric attention weights between nodes are helpful. We design a GNN which makes use of these observations through a novel self-attention mechanism. We evaluate our algor",
    "link": "http://arxiv.org/abs/2305.18385",
    "context": "Title: Self-attention Dual Embedding for Graphs with Heterophily. (arXiv:2305.18385v1 [cs.LG])\nAbstract: Graph Neural Networks (GNNs) have been highly successful for the node classification task. GNNs typically assume graphs are homophilic, i.e. neighboring nodes are likely to belong to the same class. However, a number of real-world graphs are heterophilic, and this leads to much lower classification accuracy using standard GNNs. In this work, we design a novel GNN which is effective for both heterophilic and homophilic graphs. Our work is based on three main observations. First, we show that node features and graph topology provide different amounts of informativeness in different graphs, and therefore they should be encoded independently and prioritized in an adaptive manner. Second, we show that allowing negative attention weights when propagating graph topology information improves accuracy. Finally, we show that asymmetric attention weights between nodes are helpful. We design a GNN which makes use of these observations through a novel self-attention mechanism. We evaluate our algor",
    "path": "papers/23/05/2305.18385.json",
    "total_tokens": 983,
    "translated_title": "自注意力双重嵌入：适用于异质性图的图神经网络",
    "translated_abstract": "图神经网络（GNNs）在节点分类任务中取得了重大成功。GNNs通常假设图是同质的，即相邻节点很可能属于相同的类别。然而，许多真实世界的图都是异质的，这导致使用标准的GNNs时分类精度要低得多。在本文中，我们设计了一种新颖的GNN，它对异质性和同质性图都有效。我们的工作基于三个主要观察结果。首先，我们展示了在不同的图中，节点特征和图拓扑提供不同数量的信息，因此应该独立编码并以自适应方式优先级化。其次，我们展示了当传播图拓扑信息时允许负的注意权重可以提高精度。最后，我们展示了节点之间不对称的注意权重是有帮助的。我们设计了一种GNN，利用这些观察结果通过新颖的自注意力机制。我们评估了我们的算法在一些标准的节点分类数据集上，并展示了在同质性和异质性图上的最新性能。",
    "tldr": "本研究提出了一种新颖的图神经网络，采用自注意力机制，适用于异质性图和同质性图，并在许多标准数据集上展示出最先进的性能。",
    "en_tdlr": "This paper proposes a novel GNN with a self-attention mechanism, which is effective for both heterophilic and homophilic graphs. The algorithm prioritizes encoding node features and graph topology independently and adaptively and allows negative attention weights when propagating graph topology information. The paper demonstrates state-of-the-art performance on various standard datasets for node classification."
}