{
    "title": "Vanishing Activations: A Symptom of Deep Capsule Networks. (arXiv:2305.11178v1 [cs.CV])",
    "abstract": "Capsule Networks, an extension to Neural Networks utilizing vector or matrix representations instead of scalars, were initially developed to create a dynamic parse tree where visual concepts evolve from parts to complete objects. Early implementations of Capsule Networks achieved and maintain state-of-the-art results on various datasets. However, recent studies have revealed shortcomings in the original Capsule Network architecture, notably its failure to construct a parse tree and its susceptibility to vanishing gradients when deployed in deeper networks. This paper extends the investigation to a range of leading Capsule Network architectures, demonstrating that these issues are not confined to the original design. We argue that the majority of Capsule Network research has produced architectures that, while modestly divergent from the original Capsule Network, still retain a fundamentally similar structure. We posit that this inherent design similarity might be impeding the scalabilit",
    "link": "http://arxiv.org/abs/2305.11178",
    "context": "Title: Vanishing Activations: A Symptom of Deep Capsule Networks. (arXiv:2305.11178v1 [cs.CV])\nAbstract: Capsule Networks, an extension to Neural Networks utilizing vector or matrix representations instead of scalars, were initially developed to create a dynamic parse tree where visual concepts evolve from parts to complete objects. Early implementations of Capsule Networks achieved and maintain state-of-the-art results on various datasets. However, recent studies have revealed shortcomings in the original Capsule Network architecture, notably its failure to construct a parse tree and its susceptibility to vanishing gradients when deployed in deeper networks. This paper extends the investigation to a range of leading Capsule Network architectures, demonstrating that these issues are not confined to the original design. We argue that the majority of Capsule Network research has produced architectures that, while modestly divergent from the original Capsule Network, still retain a fundamentally similar structure. We posit that this inherent design similarity might be impeding the scalabilit",
    "path": "papers/23/05/2305.11178.json",
    "total_tokens": 865,
    "translated_title": "消失的激活：深度胶囊网络的症状。",
    "translated_abstract": "胶囊网络是一种使用向量或矩阵表示而不是标量的神经网络扩展。最初开发胶囊网络的目的是创建一个动态解析树，其中视觉概念从部分逐渐发展成为完整的对象。早期的胶囊网络实现在各种数据集上取得和保持最先进的结果。然而，最近的研究揭示了原始胶囊网络结构的缺陷，特别是在构建解析树和在深度网络中易受消失梯度的方面存在问题。本文将调查一系列领先的胶囊网络架构，证明这些问题不仅限于原始设计。我们认为，大部分的胶囊网络研究已经产生了架构，虽然在某种程度上有所不同，但仍保留了基本相似的结构。我们提出，这种内在的设计相似性可能会限制可扩展性。",
    "tldr": "本文探讨了胶囊网络结构的缺陷，证明这些问题不仅限于原始设计，而是存在于许多领先的胶囊网络架构中。这种内在的设计相似性可能会限制其可扩展性。",
    "en_tdlr": "The paper investigates the drawbacks of the Capsule Network architecture, showing that these issues are not confined to the original design and are present in many leading Capsule Network architectures. It suggests that the inherent design similarity might be limiting its scalability."
}