{
    "title": "Impossibility of Depth Reduction in Explainable Clustering. (arXiv:2305.02850v1 [cs.LG])",
    "abstract": "Over the last few years Explainable Clustering has gathered a lot of attention. Dasgupta et al. [ICML'20] initiated the study of explainable k-means and k-median clustering problems where the explanation is captured by a threshold decision tree which partitions the space at each node using axis parallel hyperplanes. Recently, Laber et al. [Pattern Recognition'23] made a case to consider the depth of the decision tree as an additional complexity measure of interest.  In this work, we prove that even when the input points are in the Euclidean plane, then any depth reduction in the explanation incurs unbounded loss in the k-means and k-median cost. Formally, we show that there exists a data set X in the Euclidean plane, for which there is a decision tree of depth k-1 whose k-means/k-median cost matches the optimal clustering cost of X, but every decision tree of depth less than k-1 has unbounded cost w.r.t. the optimal cost of clustering. We extend our results to the k-center objective as",
    "link": "http://arxiv.org/abs/2305.02850",
    "context": "Title: Impossibility of Depth Reduction in Explainable Clustering. (arXiv:2305.02850v1 [cs.LG])\nAbstract: Over the last few years Explainable Clustering has gathered a lot of attention. Dasgupta et al. [ICML'20] initiated the study of explainable k-means and k-median clustering problems where the explanation is captured by a threshold decision tree which partitions the space at each node using axis parallel hyperplanes. Recently, Laber et al. [Pattern Recognition'23] made a case to consider the depth of the decision tree as an additional complexity measure of interest.  In this work, we prove that even when the input points are in the Euclidean plane, then any depth reduction in the explanation incurs unbounded loss in the k-means and k-median cost. Formally, we show that there exists a data set X in the Euclidean plane, for which there is a decision tree of depth k-1 whose k-means/k-median cost matches the optimal clustering cost of X, but every decision tree of depth less than k-1 has unbounded cost w.r.t. the optimal cost of clustering. We extend our results to the k-center objective as",
    "path": "papers/23/05/2305.02850.json",
    "total_tokens": 868,
    "translated_title": "可解释聚类中深度减少的不可能性证明",
    "translated_abstract": "近年来，可解释聚类引起了许多关注。本论文在Euclidean平面中证明，对于可解释的k-means和k-median聚类问题，决策树的深度是不可避免的复杂度度量之一，无法减少而不显著降低聚类质量。我们证明了对于任何在Euclidean平面上的数据X，深度为k-1的决策树的k-means/k-median聚类代价与X的优化聚类代价相同，但是对于深度小于k-1的决策树，其聚类代价相对于最优聚类代价而言是不可接受的。我们还将结果扩展到了k-center目标。",
    "tldr": "可解释聚类中，决策树深度是无法减少的固有复杂度度量之一，减少深度会显著降低聚类质量。",
    "en_tdlr": "This paper proves the impossibility of depth reduction in explainable clustering, showing that the depth of the decision tree is an inherent complexity measure that cannot be reduced without significant loss in clustering quality. The results are extended to k-center objective as well."
}