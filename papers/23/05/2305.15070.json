{
    "title": "Annotation Imputation to Individualize Predictions: Initial Studies on Distribution Dynamics and Model Predictions. (arXiv:2305.15070v2 [cs.CL] UPDATED)",
    "abstract": "Annotating data via crowdsourcing is time-consuming and expensive. Due to these costs, dataset creators often have each annotator label only a small subset of the data. This leads to sparse datasets with examples that are marked by few annotators. The downside of this process is that if an annotator doesn't get to label a particular example, their perspective on it is missed. This is especially concerning for subjective NLP datasets where there is no single correct label: people may have different valid opinions. Thus, we propose using imputation methods to generate the opinions of all annotators for all examples, creating a dataset that does not leave out any annotator's view. We then train and prompt models, using data from the imputed dataset, to make predictions about the distribution of responses and individual annotations.  In our analysis of the results, we found that the choice of imputation method significantly impacts soft label changes and distribution. While the imputation ",
    "link": "http://arxiv.org/abs/2305.15070",
    "context": "Title: Annotation Imputation to Individualize Predictions: Initial Studies on Distribution Dynamics and Model Predictions. (arXiv:2305.15070v2 [cs.CL] UPDATED)\nAbstract: Annotating data via crowdsourcing is time-consuming and expensive. Due to these costs, dataset creators often have each annotator label only a small subset of the data. This leads to sparse datasets with examples that are marked by few annotators. The downside of this process is that if an annotator doesn't get to label a particular example, their perspective on it is missed. This is especially concerning for subjective NLP datasets where there is no single correct label: people may have different valid opinions. Thus, we propose using imputation methods to generate the opinions of all annotators for all examples, creating a dataset that does not leave out any annotator's view. We then train and prompt models, using data from the imputed dataset, to make predictions about the distribution of responses and individual annotations.  In our analysis of the results, we found that the choice of imputation method significantly impacts soft label changes and distribution. While the imputation ",
    "path": "papers/23/05/2305.15070.json",
    "total_tokens": 901,
    "translated_title": "个性化预测的注释填补：关于分布动态和模型预测的初步研究",
    "translated_abstract": "通过众包进行数据注释非常费时费钱。由于这些成本，数据集的创建者通常让每个注释者只对一小部分数据进行标注。这导致了稀疏的数据集，其中的示例只被少数注释者标记。这个过程的缺点在于，如果一个注释者没有标注一个特定的示例，他们对它的看法就会被忽视。这在主观的自然语言处理数据集中尤为令人担忧，因为没有一个正确的标签：人们可能会有不同的有效观点。因此，我们提出使用填补方法为所有示例生成所有注释者的意见，从而创建一个不排斥任何注释者观点的数据集。然后，我们使用填补数据集中的数据训练和提示模型，以预测响应和个别注释的分布。在我们对结果的分析中，我们发现填补方法的选择显著影响软标签的变化和分布。",
    "tldr": "本文提出使用填补方法为所有注释者生成所有示例的意见，从而创建一个不排斥任何注释者观点的数据集，并分析发现填补方法的选择对软标签变化和分布有显著影响。",
    "en_tdlr": "This paper proposes using imputation methods to generate the opinions of all annotators for all examples, creating a dataset that does not leave out any annotator's view, and finds that the choice of imputation method significantly impacts soft label changes and distribution."
}