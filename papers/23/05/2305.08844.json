{
    "title": "RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs. (arXiv:2305.08844v2 [cs.CL] UPDATED)",
    "abstract": "Despite their unprecedented success, even the largest language models make mistakes. Similar to how humans learn and improve using feedback, previous work proposed providing language models with natural language feedback to guide them in repairing their outputs. Because human-generated critiques are expensive to obtain, researchers have devised learned critique generators in lieu of human critics while assuming one can train downstream models to utilize generated feedback. However, this approach does not apply to black-box or limited access models such as ChatGPT, as they cannot be fine-tuned. Moreover, in the era of large general-purpose language agents, fine-tuning is neither computationally nor spatially efficient as it results in multiple copies of the network. In this work, we introduce RL4F (Reinforcement Learning for Feedback), a multi-agent collaborative framework where the critique generator is trained to maximize end-task performance of GPT-3, a fixed model more than 200 time",
    "link": "http://arxiv.org/abs/2305.08844",
    "context": "Title: RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs. (arXiv:2305.08844v2 [cs.CL] UPDATED)\nAbstract: Despite their unprecedented success, even the largest language models make mistakes. Similar to how humans learn and improve using feedback, previous work proposed providing language models with natural language feedback to guide them in repairing their outputs. Because human-generated critiques are expensive to obtain, researchers have devised learned critique generators in lieu of human critics while assuming one can train downstream models to utilize generated feedback. However, this approach does not apply to black-box or limited access models such as ChatGPT, as they cannot be fine-tuned. Moreover, in the era of large general-purpose language agents, fine-tuning is neither computationally nor spatially efficient as it results in multiple copies of the network. In this work, we introduce RL4F (Reinforcement Learning for Feedback), a multi-agent collaborative framework where the critique generator is trained to maximize end-task performance of GPT-3, a fixed model more than 200 time",
    "path": "papers/23/05/2305.08844.json",
    "total_tokens": 972,
    "translated_title": "RL4F: 使用强化学习生成自然语言反馈修复模型输出",
    "translated_abstract": "尽管最大的语言模型取得了前所未有的成功，但它们仍然会出错。与人类通过反馈学习和改进类似，先前的研究提出为语言模型提供自然语言反馈，以指导它们修复输出。由于人工生成的批评在获取上较为昂贵，研究人员设计了学习批评生成器来替代人类批评者，并假设可以训练下游模型利用生成的反馈。然而，这种方法无法适用于黑盒或仅有有限访问权限的模型，比如ChatGPT，因为它们无法进行微调。此外，在大型通用语言模型时代，微调既不具备计算效率也不具备空间效率，因为它会导致网络的多个副本。在这项工作中，我们引入了RL4F（强化学习反馈），这是一个多智能体协作框架，其中批评生成器的训练目标是最大化GPT-3的终端任务性能，GPT-3是一个固定模型，比前代模型更加优秀。",
    "tldr": "本论文提出了RL4F（Reinforcement Learning for Feedback），这是一个多智能体协作框架，通过强化学习生成自然语言反馈来修复模型输出。该方法适用于黑盒或仅有有限访问权限的模型，避免了传统微调方法的计算效率和空间效率问题。",
    "en_tdlr": "This paper introduces RL4F (Reinforcement Learning for Feedback), a multi-agent collaborative framework that repairs model outputs by generating natural language feedback through reinforcement learning. This approach is applicable to black-box or limited access models, avoiding the computational and spatial efficiency issues of traditional fine-tuning methods."
}