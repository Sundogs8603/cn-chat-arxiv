{
    "title": "Concept-Centric Transformers: Concept Transformers with Object-Centric Concept Learning for Interpretability. (arXiv:2305.15775v1 [cs.LG])",
    "abstract": "Attention mechanisms have greatly improved the performance of deep-learning models on visual, NLP, and multimodal tasks while also providing tools to aid in the model's interpretability. In particular, attention scores over input regions or concrete image features can be used to measure how much the attended elements contribute to the model inference. The recently proposed Concept Transformer (CT) generalizes the Transformer attention mechanism from such low-level input features to more abstract, intermediate-level latent concepts that better allow human analysts to more directly assess an explanation for the reasoning of the model about any particular output classification. However, the concept learning employed by CT implicitly assumes that across every image in a class, each image patch makes the same contribution to concepts that characterize membership in that class. Instead of using the CT's image-patch-centric concepts, object-centric concepts could lead to better classification",
    "link": "http://arxiv.org/abs/2305.15775",
    "context": "Title: Concept-Centric Transformers: Concept Transformers with Object-Centric Concept Learning for Interpretability. (arXiv:2305.15775v1 [cs.LG])\nAbstract: Attention mechanisms have greatly improved the performance of deep-learning models on visual, NLP, and multimodal tasks while also providing tools to aid in the model's interpretability. In particular, attention scores over input regions or concrete image features can be used to measure how much the attended elements contribute to the model inference. The recently proposed Concept Transformer (CT) generalizes the Transformer attention mechanism from such low-level input features to more abstract, intermediate-level latent concepts that better allow human analysts to more directly assess an explanation for the reasoning of the model about any particular output classification. However, the concept learning employed by CT implicitly assumes that across every image in a class, each image patch makes the same contribution to concepts that characterize membership in that class. Instead of using the CT's image-patch-centric concepts, object-centric concepts could lead to better classification",
    "path": "papers/23/05/2305.15775.json",
    "total_tokens": 750,
    "translated_title": "以概念为中心的Transformer：具有面向物体的概念学习，以实现可解释性。",
    "translated_abstract": "注意机制大大提高了深度学习模型在视觉、NLP和多模态任务上的性能，同时也提供了工具来帮助模型的可解释性。最近提出的概念Transformer（CT）将Transformer的注意力机制从低级输入特征泛化到更抽象的中间层潜在概念，更好地允许人类分析员直接评估解释关于任何特定输出分类的推理。然而，CT采用的概念学习默认假设类别中的每个图像都对表征该类别的概念作出了相同的贡献，而使用以物体为中心的概念可能会导致更好的分类结果。",
    "tldr": "本文研究了以物体为中心的概念学习，它可以提高基于概念的Transformer模型的分类性能和可解释性。"
}