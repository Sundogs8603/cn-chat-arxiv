{
    "title": "Pick your Poison: Undetectability versus Robustness in Data Poisoning Attacks against Deep Image Classification. (arXiv:2305.09671v1 [cs.CR])",
    "abstract": "Deep image classification models trained on large amounts of web-scraped data are vulnerable to data poisoning, a mechanism for backdooring models. Even a few poisoned samples seen during training can entirely undermine the model's integrity during inference. While it is known that poisoning more samples enhances an attack's effectiveness and robustness, it is unknown whether poisoning too many samples weakens an attack by making it more detectable. We observe a fundamental detectability/robustness trade-off in data poisoning attacks: Poisoning too few samples renders an attack ineffective and not robust, but poisoning too many samples makes it detectable. This raises the bar for data poisoning attackers who have to balance this trade-off to remain robust and undetectable. Our work proposes two defenses designed to (i) detect and (ii) repair poisoned models as a post-processing step after training using a limited amount of trusted image-label pairs. We show that our defenses mitigate a",
    "link": "http://arxiv.org/abs/2305.09671",
    "context": "Title: Pick your Poison: Undetectability versus Robustness in Data Poisoning Attacks against Deep Image Classification. (arXiv:2305.09671v1 [cs.CR])\nAbstract: Deep image classification models trained on large amounts of web-scraped data are vulnerable to data poisoning, a mechanism for backdooring models. Even a few poisoned samples seen during training can entirely undermine the model's integrity during inference. While it is known that poisoning more samples enhances an attack's effectiveness and robustness, it is unknown whether poisoning too many samples weakens an attack by making it more detectable. We observe a fundamental detectability/robustness trade-off in data poisoning attacks: Poisoning too few samples renders an attack ineffective and not robust, but poisoning too many samples makes it detectable. This raises the bar for data poisoning attackers who have to balance this trade-off to remain robust and undetectable. Our work proposes two defenses designed to (i) detect and (ii) repair poisoned models as a post-processing step after training using a limited amount of trusted image-label pairs. We show that our defenses mitigate a",
    "path": "papers/23/05/2305.09671.json",
    "total_tokens": 1108,
    "translated_title": "选择你的毒药：深度图像分类数据污染攻击中的检测性与鲁棒性之争",
    "translated_abstract": "在大量网络爬取数据上训练的深度图像分类模型容易受到数据污染攻击，这是一种暗藏后门的机制。即使培训过程中只有少量污染样本，也足以在推理过程中破坏模型的完整性。虽然已知污染更多的样本可以增强攻击的效果和鲁棒性，但尚不清楚污染太多样本是否会使攻击变得更易被检测到从而削弱攻击效果。我们观察到数据污染攻击中存在一个基本的检测性/鲁棒性权衡：污染太少的样本会导致攻击失效和不鲁棒，但污染太多的样本则会使攻击易被检测到。这提高了数据污染攻击者的门槛，他们必须权衡这种权衡以保持鲁棒和不易被检测。我们的工作提出了两种防御方法，旨在使用有限的信任图像标签对作为培训后的后处理步骤来检测和修复被污染的模型。我们展示了我们的防御措施可以减轻大量污染攻击，同时对逃避尝试保持抵抗力。",
    "tldr": "深度图像分类数据污染攻击存在检测性与鲁棒性之争：污染太少导致攻击失效，污染太多易被检测到。该论文提出两种防御措施，对有限的信任图像标签对进行后处理来检测和修复被污染的模型，并证明其有效性。",
    "en_tdlr": "There is a trade-off between detectability and robustness in data poisoning attacks against deep image classification, where poisoning too few samples renders an attack ineffective and not robust, but poisoning too many samples makes it detectable. This paper proposes two defenses using trusted image-label pairs to detect and repair poisoned models after training, effectively mitigating a large proportion of poisoning attacks."
}