{
    "title": "Accurate and Structured Pruning for Efficient Automatic Speech Recognition. (arXiv:2305.19549v1 [cs.CL])",
    "abstract": "Automatic Speech Recognition (ASR) has seen remarkable advancements with deep neural networks, such as Transformer and Conformer. However, these models typically have large model sizes and high inference costs, posing a challenge to deploy on resource-limited devices. In this paper, we propose a novel compression strategy that leverages structured pruning and knowledge distillation to reduce the model size and inference cost of the Conformer model while preserving high recognition performance. Our approach utilizes a set of binary masks to indicate whether to retain or prune each Conformer module, and employs L0 regularization to learn the optimal mask values. To further enhance pruning performance, we use a layerwise distillation strategy to transfer knowledge from unpruned to pruned models. Our method outperforms all pruning baselines on the widely used LibriSpeech benchmark, achieving a 50% reduction in model size and a 28% reduction in inference cost with minimal performance loss.",
    "link": "http://arxiv.org/abs/2305.19549",
    "context": "Title: Accurate and Structured Pruning for Efficient Automatic Speech Recognition. (arXiv:2305.19549v1 [cs.CL])\nAbstract: Automatic Speech Recognition (ASR) has seen remarkable advancements with deep neural networks, such as Transformer and Conformer. However, these models typically have large model sizes and high inference costs, posing a challenge to deploy on resource-limited devices. In this paper, we propose a novel compression strategy that leverages structured pruning and knowledge distillation to reduce the model size and inference cost of the Conformer model while preserving high recognition performance. Our approach utilizes a set of binary masks to indicate whether to retain or prune each Conformer module, and employs L0 regularization to learn the optimal mask values. To further enhance pruning performance, we use a layerwise distillation strategy to transfer knowledge from unpruned to pruned models. Our method outperforms all pruning baselines on the widely used LibriSpeech benchmark, achieving a 50% reduction in model size and a 28% reduction in inference cost with minimal performance loss.",
    "path": "papers/23/05/2305.19549.json",
    "total_tokens": 944,
    "translated_title": "准确和结构化剪枝用于高效的自动语音识别",
    "translated_abstract": "深度神经网络，如Transformer和Conformer在自动语音识别（ASR）方面取得了显著进展。但是，这些模型通常具有较大的模型大小和高昂的推理成本，这对于在资源有限的设备上部署构成了挑战。在本文中，我们提出了一种利用结构化剪枝和知识蒸馏来减少Conformer模型的模型大小和推理成本的新型压缩策略，同时保持较高的识别性能。我们的方法利用一组二进制掩码来指示是否保留或修剪每个Conformer模块，并采用L0正则化来学习最佳掩码值。为了进一步增强剪枝性能，我们使用逐层蒸馏策略将知识从未修剪过的模型传输到修剪过的模型。我们的方法在广泛使用的LibriSpeech基准测试中优于所有剪枝基准线，在最小性能损失的情况下，实现了50％的模型大小减少和28％的推理成本减少。",
    "tldr": "本文提出了一种新型压缩策略，结合结构化剪枝和知识蒸馏，以减少Conformer模型的模型大小和推理成本，同时保持较高的识别性能，并在LibriSpeech基准测试中优于所有剪枝基准线。"
}