{
    "title": "Efficient Detection of LLM-generated Texts with a Bayesian Surrogate Model. (arXiv:2305.16617v1 [cs.LG])",
    "abstract": "The detection of machine-generated text, especially from large language models (LLMs), is crucial in preventing serious social problems resulting from their misuse. Some methods train dedicated detectors on specific datasets but fall short in generalizing to unseen test data, while other zero-shot ones often yield suboptimal performance. Although the recent DetectGPT has shown promising detection performance, it suffers from significant inefficiency issues, as detecting a single candidate requires scoring hundreds of its perturbations with the source LLM. This paper aims to bridge this gap. Technically, we propose to incorporate a Bayesian surrogate model, which allows us to select typical samples based on Bayesian uncertainty and interpolate scores from typical samples to other ones, to improve query efficiency. Our empirical results demonstrate that our method significantly outperforms existing approaches under a low query budget. Notably, our method achieves similar performance with",
    "link": "http://arxiv.org/abs/2305.16617",
    "context": "Title: Efficient Detection of LLM-generated Texts with a Bayesian Surrogate Model. (arXiv:2305.16617v1 [cs.LG])\nAbstract: The detection of machine-generated text, especially from large language models (LLMs), is crucial in preventing serious social problems resulting from their misuse. Some methods train dedicated detectors on specific datasets but fall short in generalizing to unseen test data, while other zero-shot ones often yield suboptimal performance. Although the recent DetectGPT has shown promising detection performance, it suffers from significant inefficiency issues, as detecting a single candidate requires scoring hundreds of its perturbations with the source LLM. This paper aims to bridge this gap. Technically, we propose to incorporate a Bayesian surrogate model, which allows us to select typical samples based on Bayesian uncertainty and interpolate scores from typical samples to other ones, to improve query efficiency. Our empirical results demonstrate that our method significantly outperforms existing approaches under a low query budget. Notably, our method achieves similar performance with",
    "path": "papers/23/05/2305.16617.json",
    "total_tokens": 802,
    "translated_abstract": "检测机器生成的文本尤其是大型语言模型（LLM）的文本，对于防止由于它们的滥用造成的严重社会问题至关重要。一些方法在特定数据集上训练专用检测器，但在泛化到未见过的测试数据时效果不佳，而其他的零样本方法在性能上也常常表现不佳。本文提出了一种方法来提高检测的效率。我们提出了将贝叶斯代理模型应用到检测中，通过使用典型样本来提高效率。在经验结果中，我们的方法在低查询预算下显著优于现有方法。值得注意的是，我们的方法在效率和性能上达到了与DetectGPT类似的水平。",
    "tldr": "本文提出了一种使用贝叶斯代理模型来高效检测LLM生成文本的方法，在低查询预算下表现优异，可避免出现严重的社会问题。"
}