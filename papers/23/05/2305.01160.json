{
    "title": "Long-Tailed Recognition by Mutual Information Maximization between Latent Features and Ground-Truth Labels. (arXiv:2305.01160v1 [cs.LG])",
    "abstract": "Although contrastive learning methods have shown prevailing performance on a variety of representation learning tasks, they encounter difficulty when the training dataset is long-tailed. Many researchers have combined contrastive learning and a logit adjustment technique to address this problem, but the combinations are done ad-hoc and a theoretical background has not yet been provided. The goal of this paper is to provide the background and further improve the performance. First, we show that the fundamental reason contrastive learning methods struggle with long-tailed tasks is that they try to maximize the mutual information maximization between latent features and input data. As ground-truth labels are not considered in the maximization, they are not able to address imbalances between class labels. Rather, we interpret the long-tailed recognition task as a mutual information maximization between latent features and ground-truth labels. This approach integrates contrastive learning a",
    "link": "http://arxiv.org/abs/2305.01160",
    "context": "Title: Long-Tailed Recognition by Mutual Information Maximization between Latent Features and Ground-Truth Labels. (arXiv:2305.01160v1 [cs.LG])\nAbstract: Although contrastive learning methods have shown prevailing performance on a variety of representation learning tasks, they encounter difficulty when the training dataset is long-tailed. Many researchers have combined contrastive learning and a logit adjustment technique to address this problem, but the combinations are done ad-hoc and a theoretical background has not yet been provided. The goal of this paper is to provide the background and further improve the performance. First, we show that the fundamental reason contrastive learning methods struggle with long-tailed tasks is that they try to maximize the mutual information maximization between latent features and input data. As ground-truth labels are not considered in the maximization, they are not able to address imbalances between class labels. Rather, we interpret the long-tailed recognition task as a mutual information maximization between latent features and ground-truth labels. This approach integrates contrastive learning a",
    "path": "papers/23/05/2305.01160.json",
    "total_tokens": 1147,
    "translated_title": "最大化潜在特征和真实标签之间的互信息实现长尾识别",
    "translated_abstract": "尽管对比学习方法在各种表示学习任务中表现出了优越的性能，但当训练数据集是长尾分布时，它们会遇到困难。许多研究人员已经将对比学习和逻辑斯蒂调整技术相结合来解决这个问题，但这些组合是临时的，并没有提供理论背景。本文的目的是提供背景并进一步提高性能。首先，我们证明了对比学习方法在长尾任务中遇到困难的根本原因是它们试图最大化潜在特征和输入数据之间的互信息最大化。由于不考虑真实标签的最大化，它们无法解决类别标签之间的不平衡问题。相反，我们将长尾识别任务解释为潜在特征和真实标签之间的互信息最大化。这种方法以一种有原则的方式集成了对比学习和逻辑斯蒂调整技术。其次，我们提出了一种新方法，称为潜在类别（LC）方法，它明确地模拟了真实标签的分布，并联合最大化潜在特征和真实标签之间的互信息。对包括CIFAR-10，CIFAR-100和ImageNet在内的基准数据集进行的大量实验表明，我们提出的方法在长尾识别任务上显着优于现有方法。",
    "tldr": "本论文提出了一种名为LC的新长尾识别方法，它能够更好地模拟真实标签分布，同时解决类别标签不平衡问题，从而在CIFAR-10，CIFAR-100和ImageNet基准数据集上显着优于现有方法。"
}