{
    "title": "The Inside Story: Towards Better Understanding of Machine Translation Neural Evaluation Metrics. (arXiv:2305.11806v1 [cs.CL])",
    "abstract": "Neural metrics for machine translation evaluation, such as COMET, exhibit significant improvements in their correlation with human judgments, as compared to traditional metrics based on lexical overlap, such as BLEU. Yet, neural metrics are, to a great extent, \"black boxes\" returning a single sentence-level score without transparency about the decision-making process. In this work, we develop and compare several neural explainability methods and demonstrate their effectiveness for interpreting state-of-the-art fine-tuned neural metrics. Our study reveals that these metrics leverage token-level information that can be directly attributed to translation errors, as assessed through comparison of token-level neural saliency maps with Multidimensional Quality Metrics (MQM) annotations and with synthetically-generated critical translation errors. To ease future research, we release our code at: https://github.com/Unbabel/COMET/tree/explainable-metrics.",
    "link": "http://arxiv.org/abs/2305.11806",
    "context": "Title: The Inside Story: Towards Better Understanding of Machine Translation Neural Evaluation Metrics. (arXiv:2305.11806v1 [cs.CL])\nAbstract: Neural metrics for machine translation evaluation, such as COMET, exhibit significant improvements in their correlation with human judgments, as compared to traditional metrics based on lexical overlap, such as BLEU. Yet, neural metrics are, to a great extent, \"black boxes\" returning a single sentence-level score without transparency about the decision-making process. In this work, we develop and compare several neural explainability methods and demonstrate their effectiveness for interpreting state-of-the-art fine-tuned neural metrics. Our study reveals that these metrics leverage token-level information that can be directly attributed to translation errors, as assessed through comparison of token-level neural saliency maps with Multidimensional Quality Metrics (MQM) annotations and with synthetically-generated critical translation errors. To ease future research, we release our code at: https://github.com/Unbabel/COMET/tree/explainable-metrics.",
    "path": "papers/23/05/2305.11806.json",
    "total_tokens": 885,
    "translated_title": "内幕揭秘：更好理解机器翻译神经评估指标",
    "translated_abstract": "机器翻译评估的神经度量（如 COMET）在与传统基于词汇重叠的度量（如 BLEU）相比，与人类判断的相关性显著提高。然而，神经评估指标在很大程度上是“黑盒子”，只返回单个句子级别得分，决策过程不透明。在本研究中，我们开发和比较了多种神经可解释方法，并展示了它们用于解释最先进的微调神经度量的有效性。我们的研究揭示了这些度量利用的令人直接归因于翻译错误的令牌级信息，通过比较令牌级神经显著性图与多维质量度量（MQM）注释和合成严重翻译错误。为了方便未来的研究，我们在此发布我们的代码： https://github.com/Unbabel/COMET/tree/explainable-metrics。",
    "tldr": "本文开发多种神经可解释方法，并展示了它们用于解释最先进微调神经度量的有效性，揭示了这些度量用来利用令人直接归因于翻译错误的令牌级信息。"
}