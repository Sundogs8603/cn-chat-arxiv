{
    "title": "IMAD: IMage-Augmented multi-modal Dialogue. (arXiv:2305.10512v1 [cs.CL])",
    "abstract": "Currently, dialogue systems have achieved high performance in processing text-based communication. However, they have not yet effectively incorporated visual information, which poses a significant challenge. Furthermore, existing models that incorporate images in dialogue generation focus on discussing the image itself. Our proposed approach presents a novel perspective on multi-modal dialogue systems, which interprets the image in the context of the dialogue. By doing so, we aim to expand the capabilities of current dialogue systems and transition them from single modality (text) to multi-modality. However, there is a lack of validated English datasets that contain both images and dialogue contexts for this task. Thus, we propose a two-stage approach to automatically construct a multi-modal dialogue dataset. In the first stage, we utilize text-to-image similarity and sentence similarity to identify which utterances could be replaced with an image. In the second stage, we replace those",
    "link": "http://arxiv.org/abs/2305.10512",
    "context": "Title: IMAD: IMage-Augmented multi-modal Dialogue. (arXiv:2305.10512v1 [cs.CL])\nAbstract: Currently, dialogue systems have achieved high performance in processing text-based communication. However, they have not yet effectively incorporated visual information, which poses a significant challenge. Furthermore, existing models that incorporate images in dialogue generation focus on discussing the image itself. Our proposed approach presents a novel perspective on multi-modal dialogue systems, which interprets the image in the context of the dialogue. By doing so, we aim to expand the capabilities of current dialogue systems and transition them from single modality (text) to multi-modality. However, there is a lack of validated English datasets that contain both images and dialogue contexts for this task. Thus, we propose a two-stage approach to automatically construct a multi-modal dialogue dataset. In the first stage, we utilize text-to-image similarity and sentence similarity to identify which utterances could be replaced with an image. In the second stage, we replace those",
    "path": "papers/23/05/2305.10512.json",
    "total_tokens": 834,
    "translated_title": "IMAD: 图像增强的多模式对话",
    "translated_abstract": "目前，对话系统已经在处理基于文本的通讯方面取得了高性能。然而，它们还没有有效地融合视觉信息，这是一个重要的挑战。此外，现有的在对话生成中融合图像的模型专注于讨论图像本身。我们提出的方法提供了一种新颖的多模态对话系统视角，解释了对话中图像的上下文。通过这样做，我们旨在扩展当前对话系统的能力，从单一模式（文本）向多模态转换。然而，缺乏包含图像和对话上下文的经过验证的英文数据集是这项任务的难点。因此，我们提出了一个两阶段的方法来自动构建多模态对话数据集。在第一阶段，我们利用文本到图像的相似性和句子相似性来识别哪些话语可以用图像替换。在第二阶段，我们替换那些话语。",
    "tldr": "该论文提出了一种新颖的多模态对话系统视角，其中图像解释是基于对话上下文的，同时提出了一个两阶段观点来构建多模态对话数据集。"
}