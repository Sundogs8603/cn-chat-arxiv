{
    "title": "Sample Complexity of Variance-reduced Distributionally Robust Q-learning. (arXiv:2305.18420v1 [cs.LG])",
    "abstract": "Dynamic decision making under distributional shifts is of fundamental interest in theory and applications of reinforcement learning: The distribution of the environment on which the data is collected can differ from that of the environment on which the model is deployed. This paper presents two novel model-free algorithms, namely the distributionally robust Q-learning and its variance-reduced counterpart, that can effectively learn a robust policy despite distributional shifts. These algorithms are designed to efficiently approximate the $q$-function of an infinite-horizon $\\gamma$-discounted robust Markov decision process with Kullback-Leibler uncertainty set to an entry-wise $\\epsilon$-degree of precision. Further, the variance-reduced distributionally robust Q-learning combines the synchronous Q-learning with variance-reduction techniques to enhance its performance. Consequently, we establish that it attains a minmax sample complexity upper bound of $\\tilde O(|S||A|(1-\\gamma)^{-4}\\e",
    "link": "http://arxiv.org/abs/2305.18420",
    "context": "Title: Sample Complexity of Variance-reduced Distributionally Robust Q-learning. (arXiv:2305.18420v1 [cs.LG])\nAbstract: Dynamic decision making under distributional shifts is of fundamental interest in theory and applications of reinforcement learning: The distribution of the environment on which the data is collected can differ from that of the environment on which the model is deployed. This paper presents two novel model-free algorithms, namely the distributionally robust Q-learning and its variance-reduced counterpart, that can effectively learn a robust policy despite distributional shifts. These algorithms are designed to efficiently approximate the $q$-function of an infinite-horizon $\\gamma$-discounted robust Markov decision process with Kullback-Leibler uncertainty set to an entry-wise $\\epsilon$-degree of precision. Further, the variance-reduced distributionally robust Q-learning combines the synchronous Q-learning with variance-reduction techniques to enhance its performance. Consequently, we establish that it attains a minmax sample complexity upper bound of $\\tilde O(|S||A|(1-\\gamma)^{-4}\\e",
    "path": "papers/23/05/2305.18420.json",
    "total_tokens": 897,
    "translated_title": "方差减少的分布式鲁棒Q-learning的样本复杂度",
    "translated_abstract": "在强化学习的理论和应用中，面对分布转移的动态决策是基本问题，因为数据收集所基于的环境分布可能会不同于模型部署所基于的分布。本文提出了两种新颖的无模型算法，即分布式鲁棒Q-learning和它的方差减少对应算法，能够高效地学习鲁棒策略，尽管会面对分布变化。这些算法旨在将带有Kullback-Leibler不确定性集的无限时域$\\gamma$-折扣鲁棒马尔科夫决策过程的$q$-函数以元素$\\epsilon$-精度有效逼近。进一步地，方差减少的分布式鲁棒Q-learning将同步Q-learning与方差减少技术相结合，以增强其性能，并且我们建立了它达到$ \\tilde O(|S||A|(1-\\gamma)^{-4}\\epsilon^{-4}$的最小最大样本复杂度上界。",
    "tldr": "本文提出了两种新颖的无模型算法，为动态决策面对分布变化问题提供了鲁棒的解决方案，并通过将Q-learning与方差减少技术相结合，实现了样本复杂度的有效控制。",
    "en_tdlr": "This paper proposes two novel model-free algorithms that provide robust solutions for dynamic decision making under distributional shifts, and effectively controls sample complexity by combining Q-learning with variance-reduction techniques."
}