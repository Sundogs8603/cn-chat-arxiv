{
    "title": "Language-Guided 3D Object Detection in Point Cloud for Autonomous Driving. (arXiv:2305.15765v1 [cs.CV])",
    "abstract": "This paper addresses the problem of 3D referring expression comprehension (REC) in autonomous driving scenario, which aims to ground a natural language to the targeted region in LiDAR point clouds. Previous approaches for REC usually focus on the 2D or 3D-indoor domain, which is not suitable for accurately predicting the location of the queried 3D region in an autonomous driving scene. In addition, the upper-bound limitation and the heavy computation cost motivate us to explore a better solution. In this work, we propose a new multi-modal visual grounding task, termed LiDAR Grounding. Then we devise a Multi-modal Single Shot Grounding (MSSG) approach with an effective token fusion strategy. It jointly learns the LiDAR-based object detector with the language features and predicts the targeted region directly from the detector without any post-processing. Moreover, the image feature can be flexibly integrated into our approach to provide rich texture and color information. The cross-moda",
    "link": "http://arxiv.org/abs/2305.15765",
    "context": "Title: Language-Guided 3D Object Detection in Point Cloud for Autonomous Driving. (arXiv:2305.15765v1 [cs.CV])\nAbstract: This paper addresses the problem of 3D referring expression comprehension (REC) in autonomous driving scenario, which aims to ground a natural language to the targeted region in LiDAR point clouds. Previous approaches for REC usually focus on the 2D or 3D-indoor domain, which is not suitable for accurately predicting the location of the queried 3D region in an autonomous driving scene. In addition, the upper-bound limitation and the heavy computation cost motivate us to explore a better solution. In this work, we propose a new multi-modal visual grounding task, termed LiDAR Grounding. Then we devise a Multi-modal Single Shot Grounding (MSSG) approach with an effective token fusion strategy. It jointly learns the LiDAR-based object detector with the language features and predicts the targeted region directly from the detector without any post-processing. Moreover, the image feature can be flexibly integrated into our approach to provide rich texture and color information. The cross-moda",
    "path": "papers/23/05/2305.15765.json",
    "total_tokens": 940,
    "translated_title": "自动驾驶中基于语言指导的点云三维物体检测",
    "translated_abstract": "本文研究了在自动驾驶场景下接受自然语言指令的点云中的三维物体检测问题。以往的方法通常聚焦于二维或者三维室内场景，难以准确地预测在自动驾驶场景中所查询的三维区域。本文提出了一个新的多模态视觉定位任务，称为激光雷达视觉定位。并且，我们提出了一种多模态单端定位方法，该方法包含了一种有效的令牌融合策略，实现了与自然语言语义融合的激光雷达物体探测。我们的方法可以灵活地对图像特征进行集成，以提供更丰富的纹理和颜色信息。跨模态的实验结果证明了我们的方法具有高效的性能。",
    "tldr": "本文提出了一个基于语言指导的点云三维物体检测方法，包含了一种有效的令牌融合策略，并实现了与自然语言语义融合的激光雷达物体探测。我们的方法在跨模态实验中达到了最先进的性能，并且具有高效的优势。",
    "en_tdlr": "This paper proposes a language-guided 3D object detection approach in point cloud for autonomous driving, which jointly learns LiDAR-based object detector with language features and achieves state-of-the-art performance with high efficiency. The approach includes an effective token fusion strategy and can flexibly integrate image feature to provide rich texture and color information."
}