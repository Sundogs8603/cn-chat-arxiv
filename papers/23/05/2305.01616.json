{
    "title": "FreeLM: Fine-Tuning-Free Language Model. (arXiv:2305.01616v1 [cs.CL])",
    "abstract": "Pre-trained language models (PLMs) have achieved remarkable success in NLP tasks. Despite the great success, mainstream solutions largely follow the pre-training then finetuning paradigm, which brings in both high deployment costs and low training efficiency. Nevertheless, fine-tuning on a specific task is essential because PLMs are only pre-trained with language signal from large raw data. In this paper, we propose a novel fine-tuning-free strategy for language models, to consider both language signal and teacher signal. Teacher signal is an abstraction of a battery of downstream tasks, provided in a unified proposition format. Trained with both language and strong task-aware teacher signals in an interactive manner, our FreeLM model demonstrates strong generalization and robustness. FreeLM outperforms large models e.g., GPT-3 and InstructGPT, on a range of language understanding tasks in experiments. FreeLM is much smaller with 0.3B parameters, compared to 175B in these models.",
    "link": "http://arxiv.org/abs/2305.01616",
    "context": "Title: FreeLM: Fine-Tuning-Free Language Model. (arXiv:2305.01616v1 [cs.CL])\nAbstract: Pre-trained language models (PLMs) have achieved remarkable success in NLP tasks. Despite the great success, mainstream solutions largely follow the pre-training then finetuning paradigm, which brings in both high deployment costs and low training efficiency. Nevertheless, fine-tuning on a specific task is essential because PLMs are only pre-trained with language signal from large raw data. In this paper, we propose a novel fine-tuning-free strategy for language models, to consider both language signal and teacher signal. Teacher signal is an abstraction of a battery of downstream tasks, provided in a unified proposition format. Trained with both language and strong task-aware teacher signals in an interactive manner, our FreeLM model demonstrates strong generalization and robustness. FreeLM outperforms large models e.g., GPT-3 and InstructGPT, on a range of language understanding tasks in experiments. FreeLM is much smaller with 0.3B parameters, compared to 175B in these models.",
    "path": "papers/23/05/2305.01616.json",
    "total_tokens": 913,
    "translated_title": "FreeLM: 免调优语言模型",
    "translated_abstract": "预训练语言模型 (PLMs) 在 NLP 任务中取得了显著的成功。尽管如此，主流的解决方案仍然遵循预训练后调优的范式，这既带来了高昂的部署成本，也降低了训练效率。然而，调优特定任务是必要的，因为 PLMs 仅在大型原始数据的语言信号下进行了预训练。本文提出了一种新颖的无调优策略，即同时考虑语言信号和教师信号。教师信号是下游任务的一个抽象表示，以统一命题格式提供。我们的 FreeLM 模型在交互式地使用语言信号和强任务感知的教师信号进行训练后表现出了强大的泛化和鲁棒性。在实验中，FreeLM 在多种语言理解任务上优于大型模型，如 GPT-3 和 InstructGPT。与这些模型的 175B 参数相比，FreeLM 更小，只有 0.3B 参数。",
    "tldr": "本文提出了一种免调优策略来训练语言模型，该模型考虑了语言信号和教师信号。通过与大型模型相比，实验证明 FreeLM 模型在多种语言理解任务上表现出了更好的性能。",
    "en_tdlr": "This paper proposes a fine-tuning-free strategy for training language models, which considers both language signal and teacher signal. The FreeLM model outperforms large models on a range of language understanding tasks and demonstrates strong generalization and robustness."
}