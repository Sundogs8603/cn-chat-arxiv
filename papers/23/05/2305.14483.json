{
    "title": "Language Model Self-improvement by Reinforcement Learning Contemplation. (arXiv:2305.14483v1 [cs.CL])",
    "abstract": "Large Language Models (LLMs) have exhibited remarkable performance across various natural language processing (NLP) tasks. However, fine-tuning these models often necessitates substantial supervision, which can be expensive and time-consuming to obtain. This paper introduces a novel unsupervised method called LanguageModel Self-Improvement by Reinforcement Learning Contemplation (SIRLC) that improves LLMs without reliance on external labels. Our approach is grounded in the observation that it is simpler for language models to assess text quality than to generate text. Building on this insight, SIRLC assigns LLMs dual roles as both student and teacher. As a student, the LLM generates answers to unlabeled questions, while as a teacher, it evaluates the generated text and assigns scores accordingly. The model parameters are updated using reinforcement learning to maximize the evaluation score. We demonstrate that SIRLC can be applied to various NLP tasks, such as reasoning problems, text ",
    "link": "http://arxiv.org/abs/2305.14483",
    "context": "Title: Language Model Self-improvement by Reinforcement Learning Contemplation. (arXiv:2305.14483v1 [cs.CL])\nAbstract: Large Language Models (LLMs) have exhibited remarkable performance across various natural language processing (NLP) tasks. However, fine-tuning these models often necessitates substantial supervision, which can be expensive and time-consuming to obtain. This paper introduces a novel unsupervised method called LanguageModel Self-Improvement by Reinforcement Learning Contemplation (SIRLC) that improves LLMs without reliance on external labels. Our approach is grounded in the observation that it is simpler for language models to assess text quality than to generate text. Building on this insight, SIRLC assigns LLMs dual roles as both student and teacher. As a student, the LLM generates answers to unlabeled questions, while as a teacher, it evaluates the generated text and assigns scores accordingly. The model parameters are updated using reinforcement learning to maximize the evaluation score. We demonstrate that SIRLC can be applied to various NLP tasks, such as reasoning problems, text ",
    "path": "papers/23/05/2305.14483.json",
    "total_tokens": 873,
    "translated_title": "基于强化学习反思的语言模型自我提升方法",
    "translated_abstract": "大型语言模型在自然语言处理任务中表现出了卓越的性能。然而，对这些模型进行微调常常需要大量的监督来获取，这样很显然是耗时且昂贵的。该论文提出了一种名为语言模型强化学习反思自我提升（SIRLC）的新型无监督方法，可以在不依赖外部标签的情况下改善LLMs。该方法基于这样一个观察结果：语言模型比生成文本更容易评估文本质量。在此基础上，SIRLC给LLMs分配了双重角色，一方面作为学生生成无标签问题的答案，另一方面作为教师评估生成的文本并据此给出分数。采用强化学习来更新模型参数以最大化评估分数。我们证明了SIRLC可以应用于各种NLP任务，例如推理问题、文本分类和语言模型训练。",
    "tldr": "本文提出了一种无监督的方法 SIRLC，可以使用强化学习有效地提高语言模型的性能，而不需要使用外部标签，并且可以应用于各种NLP任务。",
    "en_tdlr": "This paper proposes an unsupervised method called SIRLC that improves language models using reinforcement learning without relying on external labels, and it can be applied to various NLP tasks, such as reasoning problems and text classification."
}