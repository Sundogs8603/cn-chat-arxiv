{
    "title": "Do We Really Need a Large Number of Visual Prompts?. (arXiv:2305.17223v1 [cs.CV])",
    "abstract": "Due to increasing interest in adapting models on resource-constrained edges, parameter-efficient transfer learning has been widely explored. Among various methods, Visual Prompt Tuning (VPT), prepending learnable prompts to input space, shows competitive fine-tuning performance compared to training of full network parameters. However, VPT increases the number of input tokens, resulting in additional computational overhead. In this paper, we analyze the impact of the number of prompts on fine-tuning performance and self-attention operation in a vision transformer architecture. Through theoretical and empirical analysis we show that adding more prompts does not lead to linear performance improvement. Further, we propose a Prompt Condensation (PC) technique that aims to prevent performance degradation from using a small number of prompts. We validate our methods on FGVC and VTAB-1k tasks and show that our approach reduces the number of prompts by ~70% while maintaining accuracy.",
    "link": "http://arxiv.org/abs/2305.17223",
    "context": "Title: Do We Really Need a Large Number of Visual Prompts?. (arXiv:2305.17223v1 [cs.CV])\nAbstract: Due to increasing interest in adapting models on resource-constrained edges, parameter-efficient transfer learning has been widely explored. Among various methods, Visual Prompt Tuning (VPT), prepending learnable prompts to input space, shows competitive fine-tuning performance compared to training of full network parameters. However, VPT increases the number of input tokens, resulting in additional computational overhead. In this paper, we analyze the impact of the number of prompts on fine-tuning performance and self-attention operation in a vision transformer architecture. Through theoretical and empirical analysis we show that adding more prompts does not lead to linear performance improvement. Further, we propose a Prompt Condensation (PC) technique that aims to prevent performance degradation from using a small number of prompts. We validate our methods on FGVC and VTAB-1k tasks and show that our approach reduces the number of prompts by ~70% while maintaining accuracy.",
    "path": "papers/23/05/2305.17223.json",
    "total_tokens": 866,
    "translated_title": "我们真的需要大量的视觉提示吗？",
    "translated_abstract": "鉴于在资源受限的边缘上适应模型的兴趣不断增加，参数高效的迁移学习已被广泛探索。在各种方法中，可视提示调整（VPT）将可学习提示加到输入空间中，与全网络参数的训练相比，显示出有竞争力的微调性能。然而，VPT增加了输入标记的数量，导致额外的计算开销。在本文中，我们分析了提示数量对视觉变换器体系结构中微调性能和自我关注操作的影响。通过理论和实证分析，我们表明添加更多提示不会导致线性性能改进。此外，我们提出了一种Prompt Condensation（PC）技术，旨在防止使用少量提示时性能下降。我们在FGVC和VTAB-1k任务上验证了我们的方法，并显示我们的方法可以将提示数量减少约70％，同时保持准确性。",
    "tldr": "本文研究了视觉提示调整（VPT）技术中提示数量对微调性能和自我关注操作的影响，并提出了Prompt Condensation（PC）技术，该技术可以将提示数量减少约70％，同时保持准确性。",
    "en_tdlr": "This paper studies the impact of the number of visual prompts on fine-tuning performance and self-attention operation in the Visual Prompt Tuning (VPT) technique, and proposes a Prompt Condensation (PC) technique that reduces the number of prompts by ~70% while maintaining accuracy."
}