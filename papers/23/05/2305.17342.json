{
    "title": "Rethinking Adversarial Policies: A Generalized Attack Formulation and Provable Defense in Multi-Agent RL. (arXiv:2305.17342v1 [cs.LG])",
    "abstract": "Most existing works consider direct perturbations of victim's state/action or the underlying transition dynamics to show vulnerability of reinforcement learning agents under adversarial attacks. However, such direct manipulation may not always be feasible in practice. In this paper, we consider another common and realistic attack setup: in a multi-agent RL setting with well-trained agents, during deployment time, the victim agent $\\nu$ is exploited by an attacker who controls another agent $\\alpha$ to act adversarially against the victim using an \\textit{adversarial policy}. Prior attack models under such setup do not consider that the attacker can confront resistance and thus can only take partial control of the agent $\\alpha$, as well as introducing perceivable ``abnormal'' behaviors that are easily detectable. A provable defense against these adversarial policies is also lacking. To resolve these issues, we introduce a more general attack formulation that models to what extent the a",
    "link": "http://arxiv.org/abs/2305.17342",
    "context": "Title: Rethinking Adversarial Policies: A Generalized Attack Formulation and Provable Defense in Multi-Agent RL. (arXiv:2305.17342v1 [cs.LG])\nAbstract: Most existing works consider direct perturbations of victim's state/action or the underlying transition dynamics to show vulnerability of reinforcement learning agents under adversarial attacks. However, such direct manipulation may not always be feasible in practice. In this paper, we consider another common and realistic attack setup: in a multi-agent RL setting with well-trained agents, during deployment time, the victim agent $\\nu$ is exploited by an attacker who controls another agent $\\alpha$ to act adversarially against the victim using an \\textit{adversarial policy}. Prior attack models under such setup do not consider that the attacker can confront resistance and thus can only take partial control of the agent $\\alpha$, as well as introducing perceivable ``abnormal'' behaviors that are easily detectable. A provable defense against these adversarial policies is also lacking. To resolve these issues, we introduce a more general attack formulation that models to what extent the a",
    "path": "papers/23/05/2305.17342.json",
    "total_tokens": 1014,
    "translated_title": "重新思考对抗策略：多智能体强化学习中的广义攻击形式和可证明的防御",
    "translated_abstract": "大多数现有的研究研究直接扰动受害者的状态/动作或基础转移动态以展示强化学习智能体在对抗攻击下的脆弱性。然而，这样的直接操纵在实践中并不总是可行的。在本文中，我们考虑另一种常见且现实的攻击设置：在经过训练的多智能体RL的设置中，在部署期间，受害代理$\\nu$被攻击者控制另一个代理$\\alpha$以敌对方式行动，使用“对抗策略”对受害代理进行攻击。尽管之前的攻击模型考虑了这种设置，但他们没有考虑到攻击者可以遇到抵抗，因此只能部分控制代理$\\alpha$，同时引入可察觉的“异常”行为，这些行为很容易被检测到。并且缺乏针对这些对抗策略的可证明的防御。为了解决这些问题，我们引入了一个更一般化的攻击形式，模拟了攻击者在何种程度上可以控制代理$\\alpha$。",
    "tldr": "本文介绍了另一种常见、现实的多智能体RL攻击设置，提出了一种模拟攻击者对代理$\\alpha$控制的更一般化攻击形式。并解决了先前攻击模型中缺乏可证明防御的问题。",
    "en_tdlr": "This paper proposes a more general attack formulation for a common and realistic multi-agent RL attack setup, where the attacker controls the adversarial behavior of another agent against the victim during deployment. The paper also provides a provable defense against such adversarial policies that models the extent of control the attacker has over the other agent."
}