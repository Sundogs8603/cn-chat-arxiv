{
    "title": "Equivariant Few-Shot Learning from Pretrained Models. (arXiv:2305.09900v1 [cs.LG])",
    "abstract": "Efficient transfer learning algorithms are key to the success of foundation models on diverse downstream tasks even with limited data. Recent works of \\cite{basu2022equi} and \\cite{kaba2022equivariance} propose group averaging (\\textit{equitune}) and optimization-based methods, respectively, over features from group-transformed inputs to obtain equivariant outputs from non-equivariant neural networks. While \\cite{kaba2022equivariance} are only concerned with training from scratch, we find that equitune performs poorly on equivariant zero-shot tasks despite good finetuning results. We hypothesize that this is because pretrained models provide better quality features for certain transformations than others and simply averaging them is deleterious. Hence, we propose $\\lambda$-\\textit{equitune} that averages the features using \\textit{importance weights}, $\\lambda$s. These weights are learned directly from the data using a small neural network, leading to excellent zero-shot and finetuned ",
    "link": "http://arxiv.org/abs/2305.09900",
    "context": "Title: Equivariant Few-Shot Learning from Pretrained Models. (arXiv:2305.09900v1 [cs.LG])\nAbstract: Efficient transfer learning algorithms are key to the success of foundation models on diverse downstream tasks even with limited data. Recent works of \\cite{basu2022equi} and \\cite{kaba2022equivariance} propose group averaging (\\textit{equitune}) and optimization-based methods, respectively, over features from group-transformed inputs to obtain equivariant outputs from non-equivariant neural networks. While \\cite{kaba2022equivariance} are only concerned with training from scratch, we find that equitune performs poorly on equivariant zero-shot tasks despite good finetuning results. We hypothesize that this is because pretrained models provide better quality features for certain transformations than others and simply averaging them is deleterious. Hence, we propose $\\lambda$-\\textit{equitune} that averages the features using \\textit{importance weights}, $\\lambda$s. These weights are learned directly from the data using a small neural network, leading to excellent zero-shot and finetuned ",
    "path": "papers/23/05/2305.09900.json",
    "total_tokens": 917,
    "translated_title": "基于预训练模型的等变小样本学习",
    "translated_abstract": "高效的迁移学习算法是基础模型在有限数据情况下在各种下游任务上取得成功的关键。最近的作品 \\cite{basu2022equi} 和 \\cite{kaba2022equivariance} 分别提出了使用从群变换输入得到的特征的群平均值（\\textit{equitune}）和基于优化的方法来从不等变的神经网络获取等变输出。虽然 \\cite{kaba2022equivariance} 只关注从头开始训练，但我们发现即使在良好的微调结果下，\\textit{equitune} 在等变零样本任务上表现不佳。我们认为这是因为预训练模型为某些转换提供了更高质量的特征，而对其进行简单平均会产生不良影响。因此，我们提出了一种使用\\textit{重要性权重}$\\lambda$对特征进行平均的$\\lambda$-\\textit{equitune} 方法。这些权重是使用一个小型神经网络直接从数据中学习的，从而导致出色的零样本和微调结果。",
    "tldr": "本文提出了一种基于预训练模型的$\\lambda$-\\textit{equitune}方法，它使用\\textit{重要性权重}$\\lambda$对特征进行平均，可以显著提高等变小样本学习的表现。"
}