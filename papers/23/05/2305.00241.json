{
    "title": "When Deep Learning Meets Polyhedral Theory: A Survey. (arXiv:2305.00241v1 [math.OC])",
    "abstract": "In the past decade, deep learning became the prevalent methodology for predictive modeling thanks to the remarkable accuracy of deep neural networks in tasks such as computer vision and natural language processing. Meanwhile, the structure of neural networks converged back to simpler representations based on piecewise constant and piecewise linear functions such as the Rectified Linear Unit (ReLU), which became the most commonly used type of activation function in neural networks. That made certain types of network structure $\\unicode{x2014}$such as the typical fully-connected feedforward neural network$\\unicode{x2014}$ amenable to analysis through polyhedral theory and to the application of methodologies such as Linear Programming (LP) and Mixed-Integer Linear Programming (MILP) for a variety of purposes. In this paper, we survey the main topics emerging from this fast-paced area of work, which bring a fresh perspective to understanding neural networks in more detail as well as to app",
    "link": "http://arxiv.org/abs/2305.00241",
    "context": "Title: When Deep Learning Meets Polyhedral Theory: A Survey. (arXiv:2305.00241v1 [math.OC])\nAbstract: In the past decade, deep learning became the prevalent methodology for predictive modeling thanks to the remarkable accuracy of deep neural networks in tasks such as computer vision and natural language processing. Meanwhile, the structure of neural networks converged back to simpler representations based on piecewise constant and piecewise linear functions such as the Rectified Linear Unit (ReLU), which became the most commonly used type of activation function in neural networks. That made certain types of network structure $\\unicode{x2014}$such as the typical fully-connected feedforward neural network$\\unicode{x2014}$ amenable to analysis through polyhedral theory and to the application of methodologies such as Linear Programming (LP) and Mixed-Integer Linear Programming (MILP) for a variety of purposes. In this paper, we survey the main topics emerging from this fast-paced area of work, which bring a fresh perspective to understanding neural networks in more detail as well as to app",
    "path": "papers/23/05/2305.00241.json",
    "total_tokens": 1070,
    "translated_title": "当深度学习遇见多面体理论：一项综述",
    "translated_abstract": "在过去的十年中，深度学习成为了预测建模的主要方法，得益于深度神经网络在计算机视觉和自然语言处理等任务中的显著准确性。与此同时，神经网络的结构回归到了基于分段常数和分段线性函数的简单表示，例如修正线性单元（ReLU），这种激活函数成为神经网络中最常用的类型。这使得某些类型的网络结构，如典型的全连接前馈神经网络，能够通过多面体理论进行分析，并应用线性规划（LP）和混合整数线性规划（MILP）等方法用于各种目的。本文综述了这个快速发展领域涌现的主要主题，为更详细地了解神经网络以及应用数学提供了新的视角。我们介绍了多面体理论的基础知识以及它与深度学习的关系，并回顾了该主题的最新进展，包括在网络修剪、鲁棒性分析和神经网络验证等任务中使用LP和MILP。最后，我们讨论了当前挑战和未来研究方向。",
    "tldr": "本文综述了深度学习与多面体理论的交叉领域。修正线性单元（ReLU）等函数使得一些神经网络结构能够通过多面体理论进行分析，应用线性和混合整数线性规划来实现网络修剪、鲁棒性分析和神经网络验证等任务。"
}