{
    "title": "Are Large Language Models Robust Zero-shot Coreference Resolvers?. (arXiv:2305.14489v1 [cs.CL])",
    "abstract": "Recent progress in domain adaptation for coreference resolution relies on continued training using annotated data from target domains. At the same time, pre-trained large language models (LMs) have exhibited strong zero- and few-shot learning abilities across a wide range of NLP tasks including pronoun resolution. While this demonstrates evidence of coreference ability, previous work has mostly studied this ability using simple sentence-level datasets such as the Winograd Schema Challenge. In this work, we assess the feasibility of zero-shot learning for coreference resolution by evaluating instruction-tuned language models on more difficult, linguistically-complex coreference benchmarks (e.g., CoNLL-2012). We demonstrate that zero-shot prompting outperforms current unsupervised coreference systems. Further investigations reveal the robust zero-shot generalization ability of instruction-tuned LMs across a wide range of domains, languages, and time periods, as well as a strong reliance ",
    "link": "http://arxiv.org/abs/2305.14489",
    "context": "Title: Are Large Language Models Robust Zero-shot Coreference Resolvers?. (arXiv:2305.14489v1 [cs.CL])\nAbstract: Recent progress in domain adaptation for coreference resolution relies on continued training using annotated data from target domains. At the same time, pre-trained large language models (LMs) have exhibited strong zero- and few-shot learning abilities across a wide range of NLP tasks including pronoun resolution. While this demonstrates evidence of coreference ability, previous work has mostly studied this ability using simple sentence-level datasets such as the Winograd Schema Challenge. In this work, we assess the feasibility of zero-shot learning for coreference resolution by evaluating instruction-tuned language models on more difficult, linguistically-complex coreference benchmarks (e.g., CoNLL-2012). We demonstrate that zero-shot prompting outperforms current unsupervised coreference systems. Further investigations reveal the robust zero-shot generalization ability of instruction-tuned LMs across a wide range of domains, languages, and time periods, as well as a strong reliance ",
    "path": "papers/23/05/2305.14489.json",
    "total_tokens": 923,
    "translated_title": "大型语言模型是否具有鲁棒的零-shot共指解析能力？",
    "translated_abstract": "最近，领域自适应的共指消解取得了进展，依靠使用目标领域的注释数据进行持续训练。同时，预训练的大型语言模型 (LMs) 在广泛的 NLP 任务中展示了强大的零和少量样本学习能力，包括代词消解。虽然这表明了共指能力的证据，但以往的研究大都使用简单的句子级别数据集 (如 Winograd Schema 挑战赛) 研究这种能力。在这项工作中，我们通过评估指令调整的语言模型在更加困难的、语言上复杂的共指基准测试 (如 CoNLL-2012) 上的可行性来评估零-shot学习进行共指消解的可行性。我们证明零-shot提示优于当前的无监督共指系统。进一步的研究揭示了指令调整 LMs 在广泛的领域、语言和时间段上具有强大的鲁棒性零 shot 普适性，以及对上下文和指示词的强烈依赖。",
    "tldr": "本文研究了零-shot共指解析技术在复杂语言环境下的应用，并证明指令调整的语言模型具有鲁棒性零 shot 普适性。",
    "en_tdlr": "This paper evaluates the feasibility of zero-shot learning for coreference resolution in linguistically-complex benchmarks, demonstrating that instruction-tuned language models exhibit robust zero-shot generalization ability across a wide range of domains, languages, and time periods."
}