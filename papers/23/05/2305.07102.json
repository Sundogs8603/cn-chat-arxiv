{
    "title": "Salient Mask-Guided Vision Transformer for Fine-Grained Classification. (arXiv:2305.07102v1 [cs.CV])",
    "abstract": "Fine-grained visual classification (FGVC) is a challenging computer vision problem, where the task is to automatically recognise objects from subordinate categories. One of its main difficulties is capturing the most discriminative inter-class variances among visually similar classes. Recently, methods with Vision Transformer (ViT) have demonstrated noticeable achievements in FGVC, generally by employing the self-attention mechanism with additional resource-consuming techniques to distinguish potentially discriminative regions while disregarding the rest. However, such approaches may struggle to effectively focus on truly discriminative regions due to only relying on the inherent self-attention mechanism, resulting in the classification token likely aggregating global information from less-important background patches. Moreover, due to the immense lack of the datapoints, classifiers may fail to find the most helpful inter-class distinguishing features, since other unrelated but distinc",
    "link": "http://arxiv.org/abs/2305.07102",
    "context": "Title: Salient Mask-Guided Vision Transformer for Fine-Grained Classification. (arXiv:2305.07102v1 [cs.CV])\nAbstract: Fine-grained visual classification (FGVC) is a challenging computer vision problem, where the task is to automatically recognise objects from subordinate categories. One of its main difficulties is capturing the most discriminative inter-class variances among visually similar classes. Recently, methods with Vision Transformer (ViT) have demonstrated noticeable achievements in FGVC, generally by employing the self-attention mechanism with additional resource-consuming techniques to distinguish potentially discriminative regions while disregarding the rest. However, such approaches may struggle to effectively focus on truly discriminative regions due to only relying on the inherent self-attention mechanism, resulting in the classification token likely aggregating global information from less-important background patches. Moreover, due to the immense lack of the datapoints, classifiers may fail to find the most helpful inter-class distinguishing features, since other unrelated but distinc",
    "path": "papers/23/05/2305.07102.json",
    "total_tokens": 899,
    "translated_title": "显著掩模引导下的视觉Transformer用于细粒度分类",
    "translated_abstract": "细粒度视觉分类是一个具有挑战性的计算机视觉问题，其任务是在亚类别中自动识别对象。其中主要的困难是捕捉那些仅在视觉上相似但在类别间有最具有区别性的差异。最近，采用视觉Transformer（ViT）的方法在细粒度视觉分类中取得了显著成果，通常通过运用自我注意机制和其他耗费资源的技术来区分具有潜在区别性的区域，而忽略其余区域。然而，这种方法只依赖内在的自我关注机制，在有效聚焦真正具有区别性的区域方面可能存在困难，导致分类令牌可能会从不重要的背景区域汇集全局信息。此外，由于缺乏数据点，分类器可能无法找到最有帮助的类间区别特征，因为其他无关但独特的特征通常会受到更多的关注。",
    "tldr": "该研究提出了一种显著掩模引导下的视觉Transformer方法，适用于细粒度分类。该方法旨在解决细粒度分类中捕捉最具有区别性的差异和忽略不相关区域的困难。",
    "en_tdlr": "The study proposes a Salient Mask-Guided Vision Transformer method for fine-grained classification, aimed at addressing the difficulty of capturing the most discriminative inter-class variances and disregarding irrelevant regions in fine-grained visual classification."
}