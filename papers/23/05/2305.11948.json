{
    "title": "Eye-SpatialNet: Spatial Information Extraction from Ophthalmology Notes. (arXiv:2305.11948v1 [cs.CL])",
    "abstract": "We introduce an annotated corpus of 600 ophthalmology notes labeled with detailed spatial and contextual information of ophthalmic entities. We extend our previously proposed frame semantics-based spatial representation schema, Rad-SpatialNet, to represent spatial language in ophthalmology text, resulting in the Eye-SpatialNet schema. The spatially-grounded entities are findings, procedures, and drugs. To accurately capture all spatial details, we add some domain-specific elements in Eye-SpatialNet. The annotated corpus contains 1715 spatial triggers, 7308 findings, 2424 anatomies, and 9914 descriptors. To automatically extract the spatial information, we employ a two-turn question answering approach based on the transformer language model BERT. The results are promising, with F1 scores of 89.31, 74.86, and 88.47 for spatial triggers, Figure, and Ground frame elements, respectively. This is the first work to represent and extract a wide variety of clinical information in ophthalmology.",
    "link": "http://arxiv.org/abs/2305.11948",
    "context": "Title: Eye-SpatialNet: Spatial Information Extraction from Ophthalmology Notes. (arXiv:2305.11948v1 [cs.CL])\nAbstract: We introduce an annotated corpus of 600 ophthalmology notes labeled with detailed spatial and contextual information of ophthalmic entities. We extend our previously proposed frame semantics-based spatial representation schema, Rad-SpatialNet, to represent spatial language in ophthalmology text, resulting in the Eye-SpatialNet schema. The spatially-grounded entities are findings, procedures, and drugs. To accurately capture all spatial details, we add some domain-specific elements in Eye-SpatialNet. The annotated corpus contains 1715 spatial triggers, 7308 findings, 2424 anatomies, and 9914 descriptors. To automatically extract the spatial information, we employ a two-turn question answering approach based on the transformer language model BERT. The results are promising, with F1 scores of 89.31, 74.86, and 88.47 for spatial triggers, Figure, and Ground frame elements, respectively. This is the first work to represent and extract a wide variety of clinical information in ophthalmology.",
    "path": "papers/23/05/2305.11948.json",
    "total_tokens": 915,
    "translated_title": "眼科笔记中的空间信息提取：Eye-SpatialNet",
    "translated_abstract": "我们介绍了一个带有详细眼科实体空间和语境信息标注的600个眼科笔记的语料库。我们扩展了我们先前提出的基于框架语义的空间表示模式Rad-SpatialNet，以表示眼科文本中的空间语言，从而得到了Eye-SpatialNet模式。空间定位的实体是发现、程序和药物。为了准确捕获所有的空间细节，我们在Eye-SpatialNet中添加了一些特定领域的元素。标注的语料库包含1715个空间触发器、7308个发现、2424个解剖学和9914个描述符。为了自动提取空间信息，我们采用了基于 transformer 语言模型 BERT 的两轮问答方法。结果很有前途，F1 得分分别为 89.31、74.86 和 88.47，用于空间触发器、图形和地面框架元素。这是第一篇在眼科领域中表示和提取各种临床信息的工作。",
    "tldr": "本研究提出了Eye-SpatialNet模式，用于表示眼科文本中的空间语言，并使用BERT语言模型自动提取空间信息，实现了对眼科实体空间和语境信息的自动化精准标注和提取。",
    "en_tdlr": "This study introduces the Eye-SpatialNet schema for representing spatial language in ophthalmology texts and employs BERT language model for automatic spatial information extraction, achieving accurate annotation and extraction of spatial and contextual information of ophthalmic entities."
}