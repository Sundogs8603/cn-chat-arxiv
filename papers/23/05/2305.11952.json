{
    "title": "Self-QA: Unsupervised Knowledge Guided Language Model Alignment. (arXiv:2305.11952v1 [cs.CL])",
    "abstract": "Large-scale language models like ChatGPT and GPT-4 have gained attention for their impressive conversational and generative capabilities. However, the creation of supervised paired question-answering data for instruction tuning presents formidable challenges. This endeavor necessitates substantial human effort for data annotation and wrestles with issues concerning data quality, diversity, accuracy, and other related factors. To overcome these obstacles, we introduce an innovative framework named Self-QA, which replaces the traditional practice of human-written instruction seeds with a vast amount of unsupervised knowledge, enabling the model to generate a larger quantity of correct and domain-specific instruction data. The effectiveness of our proposed method is demonstrated through experiments conducted on unsupervised corpora from various domains.",
    "link": "http://arxiv.org/abs/2305.11952",
    "context": "Title: Self-QA: Unsupervised Knowledge Guided Language Model Alignment. (arXiv:2305.11952v1 [cs.CL])\nAbstract: Large-scale language models like ChatGPT and GPT-4 have gained attention for their impressive conversational and generative capabilities. However, the creation of supervised paired question-answering data for instruction tuning presents formidable challenges. This endeavor necessitates substantial human effort for data annotation and wrestles with issues concerning data quality, diversity, accuracy, and other related factors. To overcome these obstacles, we introduce an innovative framework named Self-QA, which replaces the traditional practice of human-written instruction seeds with a vast amount of unsupervised knowledge, enabling the model to generate a larger quantity of correct and domain-specific instruction data. The effectiveness of our proposed method is demonstrated through experiments conducted on unsupervised corpora from various domains.",
    "path": "papers/23/05/2305.11952.json",
    "total_tokens": 789,
    "translated_title": "自我问答：无监督知识引导的语言模型对齐",
    "translated_abstract": "ChatGPT 和 GPT-4 等大规模语言模型因其出色的对话和生成能力而备受关注。然而，为指导模型调整而创建监督式配对的问答数据是一项艰巨的挑战。这需要大量的人力用于数据注释并涉及数据质量、多样性、准确性和其他相关因素的问题。为了克服这些障碍，我们引入了一个创新的框架，名为 Self-QA，它用大量的无监督知识代替传统的人工撰写的指令种子，使模型能够生成更多正确且特定于领域的指令数据。我们的方法的有效性通过在不同领域的无监督语料库上进行实验得到了证明。",
    "tldr": "Self-QA是一种无监督知识引导的语言模型对齐框架，使用大量的无监督知识来代替传统的人工撰写的指令种子，以生成更多正确且特定于领域的指令数据。",
    "en_tdlr": "Self-QA is an unsupervised knowledge-guided language model alignment framework that replaces traditional human-written instruction seeds with vast amounts of unsupervised knowledge, enabling the model to generate a larger quantity of correct and domain-specific instruction data."
}