{
    "title": "Personalized Federated Learning under Mixture of Distributions. (arXiv:2305.01068v1 [cs.LG])",
    "abstract": "The recent trend towards Personalized Federated Learning (PFL) has garnered significant attention as it allows for the training of models that are tailored to each client while maintaining data privacy. However, current PFL techniques primarily focus on modeling the conditional distribution heterogeneity (i.e. concept shift), which can result in suboptimal performance when the distribution of input data across clients diverges (i.e. covariate shift). Additionally, these techniques often lack the ability to adapt to unseen data, further limiting their effectiveness in real-world scenarios. To address these limitations, we propose a novel approach, FedGMM, which utilizes Gaussian mixture models (GMM) to effectively fit the input data distributions across diverse clients. The model parameters are estimated by maximum likelihood estimation utilizing a federated Expectation-Maximization algorithm, which is solved in closed form and does not assume gradient similarity. Furthermore, FedGMM po",
    "link": "http://arxiv.org/abs/2305.01068",
    "context": "Title: Personalized Federated Learning under Mixture of Distributions. (arXiv:2305.01068v1 [cs.LG])\nAbstract: The recent trend towards Personalized Federated Learning (PFL) has garnered significant attention as it allows for the training of models that are tailored to each client while maintaining data privacy. However, current PFL techniques primarily focus on modeling the conditional distribution heterogeneity (i.e. concept shift), which can result in suboptimal performance when the distribution of input data across clients diverges (i.e. covariate shift). Additionally, these techniques often lack the ability to adapt to unseen data, further limiting their effectiveness in real-world scenarios. To address these limitations, we propose a novel approach, FedGMM, which utilizes Gaussian mixture models (GMM) to effectively fit the input data distributions across diverse clients. The model parameters are estimated by maximum likelihood estimation utilizing a federated Expectation-Maximization algorithm, which is solved in closed form and does not assume gradient similarity. Furthermore, FedGMM po",
    "path": "papers/23/05/2305.01068.json",
    "total_tokens": 877,
    "translated_title": "混合分布下的个性化联邦学习",
    "translated_abstract": "近年来，针对每个客户训练定制模型并保持数据隐私的个性化联邦学习(PFL)趋势引起了重视。然而，当前的PFL技术主要关注条件分布异质性(即概念漂移)，当客户之间输入数据分布发散(即协变量漂移)时，可能导致性能不佳。此外，这些技术通常缺乏适应未见数据的能力，进一步限制了它们在现实场景中的有效性。为了解决这些局限性，我们提出了一种新方法FedGMM，它利用高斯混合模型(GMM)有效地适应多样化客户端的输入数据分布。模型参数通过最大似然估计，利用联邦期望最大化算法解决，该算法在闭合形式下解决且不假设渐变相似性。此外，FedGMM同时模型化条件和协变量分布异质性，提高了个性化联邦学习的性能。",
    "tldr": "该论文提出了一种新方法FedGMM，利用高斯混合模型处理了协变量漂移问题，提高了个性化联邦学习的性能。",
    "en_tdlr": "This paper proposes a novel approach, FedGMM, which utilizes Gaussian mixture models to handle covariate shift and leads to improved performance in personalized federated learning."
}