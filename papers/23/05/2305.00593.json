{
    "title": "Reliable Gradient-free and Likelihood-free Prompt Tuning. (arXiv:2305.00593v1 [cs.LG])",
    "abstract": "Due to privacy or commercial constraints, large pre-trained language models (PLMs) are often offered as black-box APIs. Fine-tuning such models to downstream tasks is challenging because one can neither access the model's internal representations nor propagate gradients through it. This paper addresses these challenges by developing techniques for adapting PLMs with only API access. Building on recent work on soft prompt tuning, we develop methods to tune the soft prompts without requiring gradient computation. Further, we develop extensions that in addition to not requiring gradients also do not need to access any internal representation of the PLM beyond the input embeddings. Moreover, instead of learning a single prompt, our methods learn a distribution over prompts allowing us to quantify predictive uncertainty. Ours is the first work to consider uncertainty in prompts when only having API access to the PLM. Finally, through extensive experiments, we carefully vet the proposed meth",
    "link": "http://arxiv.org/abs/2305.00593",
    "context": "Title: Reliable Gradient-free and Likelihood-free Prompt Tuning. (arXiv:2305.00593v1 [cs.LG])\nAbstract: Due to privacy or commercial constraints, large pre-trained language models (PLMs) are often offered as black-box APIs. Fine-tuning such models to downstream tasks is challenging because one can neither access the model's internal representations nor propagate gradients through it. This paper addresses these challenges by developing techniques for adapting PLMs with only API access. Building on recent work on soft prompt tuning, we develop methods to tune the soft prompts without requiring gradient computation. Further, we develop extensions that in addition to not requiring gradients also do not need to access any internal representation of the PLM beyond the input embeddings. Moreover, instead of learning a single prompt, our methods learn a distribution over prompts allowing us to quantify predictive uncertainty. Ours is the first work to consider uncertainty in prompts when only having API access to the PLM. Finally, through extensive experiments, we carefully vet the proposed meth",
    "path": "papers/23/05/2305.00593.json",
    "total_tokens": 897,
    "translated_title": "可靠的无梯度和无似然对话式建模API优化方法",
    "translated_abstract": "由于隐私或商业限制，大型预训练语言模型（PLMs）通常作为黑盒API提供。对这些模型进行下游任务的微调是具有挑战性的，因为既无法访问模型的内部表示，也无法通过它传播梯度。本文通过开发只有API访问权限的PLM的自适应技术来应对这些挑战。在最近的软提示调整工作的基础上，我们开发了在不需要计算梯度的情况下调整软提示的方法。此外，我们扩展了这些技术，不需要访问PLM除了输入嵌入之外的任何内部表示。我们的方法学习了提示的分布，而不是单一的提示，可以对推理不确定性进行量化，这是在仅具有API访问权限的情况下考虑提示不确定性的首次尝试。最后，通过广泛的实验，我们仔细检查了所提出的方法，并表明它们的性能与基于梯度和基于似然的方法相当，甚至更好。",
    "tldr": "本文提供一种能够应对挑战性情景，即仅具备API访问权限的情况下，建模API进行优化的方法，并能够对推理不确定性进行量化。",
    "en_tdlr": "This paper proposes a reliable method for optimizing language model APIs in challenging scenarios where only API access is available and internal representations and gradients cannot be accessed, while also allowing for quantifying predictive uncertainty."
}