{
    "title": "Lawyer LLaMA Technical Report. (arXiv:2305.15062v2 [cs.CL] UPDATED)",
    "abstract": "Large Language Models (LLMs), like LLaMA, have exhibited remarkable performance across various tasks. Nevertheless, when deployed to specific domains such as law or medicine, the models still confront the challenge of a deficiency in domain-specific knowledge and an inadequate capability to leverage that knowledge to resolve domain-related problems. In this paper, we propose a new framework to adapt LLMs to specific domains and build Lawyer LLaMA, a legal domain LLM, based on this framework. Specifically, we inject domain knowledge during the continual training stage and teach the model to learn professional skills using properly designed supervised fine-tuning tasks. Moreover, to alleviate the hallucination problem during the model's generation, we add a retrieval module and extract relevant legal articles before the model answers any queries. When learning domain-specific skills, we find that experts' experience is much more useful than experiences distilled from ChatGPT, where hundr",
    "link": "http://arxiv.org/abs/2305.15062",
    "context": "Title: Lawyer LLaMA Technical Report. (arXiv:2305.15062v2 [cs.CL] UPDATED)\nAbstract: Large Language Models (LLMs), like LLaMA, have exhibited remarkable performance across various tasks. Nevertheless, when deployed to specific domains such as law or medicine, the models still confront the challenge of a deficiency in domain-specific knowledge and an inadequate capability to leverage that knowledge to resolve domain-related problems. In this paper, we propose a new framework to adapt LLMs to specific domains and build Lawyer LLaMA, a legal domain LLM, based on this framework. Specifically, we inject domain knowledge during the continual training stage and teach the model to learn professional skills using properly designed supervised fine-tuning tasks. Moreover, to alleviate the hallucination problem during the model's generation, we add a retrieval module and extract relevant legal articles before the model answers any queries. When learning domain-specific skills, we find that experts' experience is much more useful than experiences distilled from ChatGPT, where hundr",
    "path": "papers/23/05/2305.15062.json",
    "total_tokens": 1021,
    "translated_title": "律师LLaMA技术报告",
    "translated_abstract": "大型语言模型(LLMs)，如LLaMA，在各种任务中显示出了卓越的性能。然而，当部署到特定领域，如法律或医学时，模型仍然面临领域特定知识不足和不足以解决与领域相关问题的能力的挑战。在本文中，我们提出了一种新的框架，将LLMs适应特定领域，并基于该框架构建了律师LLaMA，一种基于法律领域的LLM。具体而言，我们在持续训练阶段注入领域知识，并教授模型使用经过适当设计的有监督微调任务来学习专业技能。此外，为了解决模型在生成过程中的幻象问题，我们添加了一个检索模块，在模型回答任何查询之前提取相关的法律文章。在学习领域特定技能时，我们发现专家的经验比从ChatGPT中提炼的经验更有用，ChatGPT含有数百个例子。",
    "tldr": "提出了一种新的框架，用于将大型语言模型（LLMs）应用于特定领域，并基于该框架构建了律师LLaMA，一种针对法律领域的LLM。通过在持续训练阶段注入领域知识和设计有监督微调任务来教授专业技能，以解决模型在特定领域中遇到的问题。此外，通过添加检索模块并在生成之前提取相关法律文章，解决了模型生成过程中的幻象问题。",
    "en_tdlr": "A new framework is proposed to adapt large language models (LLMs) to specific domains, and Lawyer LLaMA, a legal domain LLM, is built based on this framework. The framework injects domain knowledge during continual training and teaches the model professional skills using supervised fine-tuning tasks. The addition of a retrieval module and extraction of relevant legal articles before model generation address the hallucination problem."
}