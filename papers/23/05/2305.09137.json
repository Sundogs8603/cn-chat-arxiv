{
    "title": "Pre-Training to Learn in Context. (arXiv:2305.09137v1 [cs.CL])",
    "abstract": "In-context learning, where pre-trained language models learn to perform tasks from task examples and instructions in their contexts, has attracted much attention in the NLP community. However, the ability of in-context learning is not fully exploited because language models are not explicitly trained to learn in context. To this end, we propose PICL (Pre-training for In-Context Learning), a framework to enhance the language models' in-context learning ability by pre-training the model on a large collection of \"intrinsic tasks\" in the general plain-text corpus using the simple language modeling objective. PICL encourages the model to infer and perform tasks by conditioning on the contexts while maintaining task generalization of pre-trained models. We evaluate the in-context learning performance of the model trained with PICL on seven widely-used text classification datasets and the Super-NaturalInstrctions benchmark, which contains 100+ NLP tasks formulated to text generation. Our expe",
    "link": "http://arxiv.org/abs/2305.09137",
    "context": "Title: Pre-Training to Learn in Context. (arXiv:2305.09137v1 [cs.CL])\nAbstract: In-context learning, where pre-trained language models learn to perform tasks from task examples and instructions in their contexts, has attracted much attention in the NLP community. However, the ability of in-context learning is not fully exploited because language models are not explicitly trained to learn in context. To this end, we propose PICL (Pre-training for In-Context Learning), a framework to enhance the language models' in-context learning ability by pre-training the model on a large collection of \"intrinsic tasks\" in the general plain-text corpus using the simple language modeling objective. PICL encourages the model to infer and perform tasks by conditioning on the contexts while maintaining task generalization of pre-trained models. We evaluate the in-context learning performance of the model trained with PICL on seven widely-used text classification datasets and the Super-NaturalInstrctions benchmark, which contains 100+ NLP tasks formulated to text generation. Our expe",
    "path": "papers/23/05/2305.09137.json",
    "total_tokens": 905,
    "tldr": "本论文提出了PICL框架，通过在大规模文本语料库上预训练模型的内在任务来增强语言模型的上下文学习能力，从而鼓励模型在依赖于上下文的情况下进行任务推导和执行。"
}