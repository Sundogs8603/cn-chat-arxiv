{
    "title": "Efficient Training of Energy-Based Models Using Jarzynski Equality. (arXiv:2305.19414v1 [cs.LG])",
    "abstract": "Energy-based models (EBMs) are generative models inspired by statistical physics with a wide range of applications in unsupervised learning. Their performance is best measured by the cross-entropy (CE) of the model distribution relative to the data distribution. Using the CE as the objective for training is however challenging because the computation of its gradient with respect to the model parameters requires sampling the model distribution. Here we show how results for nonequilibrium thermodynamics based on Jarzynski equality together with tools from sequential Monte-Carlo sampling can be used to perform this computation efficiently and avoid the uncontrolled approximations made using the standard contrastive divergence algorithm. Specifically, we introduce a modification of the unadjusted Langevin algorithm (ULA) in which each walker acquires a weight that enables the estimation of the gradient of the cross-entropy at any step during GD, thereby bypassing sampling biases induced by",
    "link": "http://arxiv.org/abs/2305.19414",
    "context": "Title: Efficient Training of Energy-Based Models Using Jarzynski Equality. (arXiv:2305.19414v1 [cs.LG])\nAbstract: Energy-based models (EBMs) are generative models inspired by statistical physics with a wide range of applications in unsupervised learning. Their performance is best measured by the cross-entropy (CE) of the model distribution relative to the data distribution. Using the CE as the objective for training is however challenging because the computation of its gradient with respect to the model parameters requires sampling the model distribution. Here we show how results for nonequilibrium thermodynamics based on Jarzynski equality together with tools from sequential Monte-Carlo sampling can be used to perform this computation efficiently and avoid the uncontrolled approximations made using the standard contrastive divergence algorithm. Specifically, we introduce a modification of the unadjusted Langevin algorithm (ULA) in which each walker acquires a weight that enables the estimation of the gradient of the cross-entropy at any step during GD, thereby bypassing sampling biases induced by",
    "path": "papers/23/05/2305.19414.json",
    "total_tokens": 858,
    "translated_title": "利用Jarzynski平等式高效训练能量基模型",
    "translated_abstract": "能量基模型是受统计物理启发的生成模型，在无监督学习中有广泛应用。模型分布相对于数据分布的交叉熵是衡量它们性能的最佳指标。然而，使用交叉熵作为训练目标挑战重重，因为它对于模型参数的梯度计算需要对模型分布进行采样。在这里，我们展示了基于Jarzynski等式的非平衡热力学结果，结合顺序蒙特卡罗采样工具，可以有效地进行计算，避免使用标准对比散度算法所产生的不可控逼近误差。具体而言，我们介绍了未调整Langevin算法的修改版本，在其中每个Walker都会获得一个权重，使得能够在任何步骤时估计交叉熵的梯度，从而规避由采样偏差导致的问题。",
    "tldr": "本文介绍了一种通过使用Jarzynski平等式和顺序蒙特卡罗采样绕过标准对比散度算法中的不可控逼近误差，有效训练能量基模型的方法。",
    "en_tdlr": "This paper introduces a method for efficiently training energy-based models by using Jarzynski equality and sequential Monte-Carlo sampling to bypass the uncontrollable approximation errors in the standard contrastive divergence algorithm."
}