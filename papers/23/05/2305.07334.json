{
    "title": "Locking and Quacking: Stacking Bayesian model predictions by log-pooling and superposition. (arXiv:2305.07334v1 [stat.ML])",
    "abstract": "Combining predictions from different models is a central problem in Bayesian inference and machine learning more broadly. Currently, these predictive distributions are almost exclusively combined using linear mixtures such as Bayesian model averaging, Bayesian stacking, and mixture of experts. Such linear mixtures impose idiosyncrasies that might be undesirable for some applications, such as multi-modality. While there exist alternative strategies (e.g. geometric bridge or superposition), optimising their parameters usually involves computing an intractable normalising constant repeatedly. We present two novel Bayesian model combination tools. These are generalisations of model stacking, but combine posterior densities by log-linear pooling (locking) and quantum superposition (quacking). To optimise model weights while avoiding the burden of normalising constants, we investigate the Hyvarinen score of the combined posterior predictions. We demonstrate locking with an illustrative examp",
    "link": "http://arxiv.org/abs/2305.07334",
    "context": "Title: Locking and Quacking: Stacking Bayesian model predictions by log-pooling and superposition. (arXiv:2305.07334v1 [stat.ML])\nAbstract: Combining predictions from different models is a central problem in Bayesian inference and machine learning more broadly. Currently, these predictive distributions are almost exclusively combined using linear mixtures such as Bayesian model averaging, Bayesian stacking, and mixture of experts. Such linear mixtures impose idiosyncrasies that might be undesirable for some applications, such as multi-modality. While there exist alternative strategies (e.g. geometric bridge or superposition), optimising their parameters usually involves computing an intractable normalising constant repeatedly. We present two novel Bayesian model combination tools. These are generalisations of model stacking, but combine posterior densities by log-linear pooling (locking) and quantum superposition (quacking). To optimise model weights while avoiding the burden of normalising constants, we investigate the Hyvarinen score of the combined posterior predictions. We demonstrate locking with an illustrative examp",
    "path": "papers/23/05/2305.07334.json",
    "total_tokens": 960,
    "translated_title": "锁定与叠层：通过对数池化和超叠加堆叠贝叶斯模型预测",
    "translated_abstract": "将来自不同模型的预测结合起来是贝叶斯推理和机器学习中的一个核心问题。目前，这些预测分布几乎仅使用线性组合进行组合，例如贝叶斯模型平均、贝叶斯堆叠和专家混合。这种线性混合可能对某些应用程序不利，例如多峰性。本文提出了两种新的贝叶斯模型组合工具。这些工具是模型堆叠的推广，但是通过对数线性汇集（锁定）和量子叠加（quacking）来合并后验密度。为了优化模型权重而避免标准化常数的负担，我们研究了组合后验预测的Hyvarinen得分。我们通过一个示例说明了锁定，并将两种方法应用于来自不同连续密度的模拟数据集，将它们与传统的模型组合工具进行比较。结果表明，这两种方法在预测准确性方面优于传统方法，同时具有高效计算的特点。",
    "tldr": "本文提出了两种新的贝叶斯模型组合工具，可以通过对数池化和超叠加来组合后验密度，避免了标准化常数的负担，并在预测准确性方面优于传统方法。"
}