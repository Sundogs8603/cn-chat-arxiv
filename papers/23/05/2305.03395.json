{
    "title": "Sparsifying Bayesian neural networks with latent binary variables and normalizing flows. (arXiv:2305.03395v1 [stat.ML])",
    "abstract": "Artificial neural networks (ANNs) are powerful machine learning methods used in many modern applications such as facial recognition, machine translation, and cancer diagnostics. A common issue with ANNs is that they usually have millions or billions of trainable parameters, and therefore tend to overfit to the training data. This is especially problematic in applications where it is important to have reliable uncertainty estimates. Bayesian neural networks (BNN) can improve on this, since they incorporate parameter uncertainty. In addition, latent binary Bayesian neural networks (LBBNN) also take into account structural uncertainty by allowing the weights to be turned on or off, enabling inference in the joint space of weights and structures. In this paper, we will consider two extensions to the LBBNN method: Firstly, by using the local reparametrization trick (LRT) to sample the hidden units directly, we get a more computationally efficient algorithm. More importantly, by using normal",
    "link": "http://arxiv.org/abs/2305.03395",
    "context": "Title: Sparsifying Bayesian neural networks with latent binary variables and normalizing flows. (arXiv:2305.03395v1 [stat.ML])\nAbstract: Artificial neural networks (ANNs) are powerful machine learning methods used in many modern applications such as facial recognition, machine translation, and cancer diagnostics. A common issue with ANNs is that they usually have millions or billions of trainable parameters, and therefore tend to overfit to the training data. This is especially problematic in applications where it is important to have reliable uncertainty estimates. Bayesian neural networks (BNN) can improve on this, since they incorporate parameter uncertainty. In addition, latent binary Bayesian neural networks (LBBNN) also take into account structural uncertainty by allowing the weights to be turned on or off, enabling inference in the joint space of weights and structures. In this paper, we will consider two extensions to the LBBNN method: Firstly, by using the local reparametrization trick (LRT) to sample the hidden units directly, we get a more computationally efficient algorithm. More importantly, by using normal",
    "path": "papers/23/05/2305.03395.json",
    "total_tokens": 1066,
    "translated_title": "用潜在二进制变量和归一化流来稀疏化贝叶斯神经网络",
    "translated_abstract": "人工神经网络（ANN）是现代许多应用中强大的机器学习方法，如面部识别、机器翻译和癌症诊断。ANN的一个常见问题是它们通常具有数百万或数十亿个可训练参数，并且因此倾向于过度拟合训练数据。这在需要可靠的不确定性估计的应用中特别有问题。贝叶斯神经网络（BNN）可以改善这一问题，因为它们包含参数不确定性。此外，潜在二进制贝叶斯神经网络（LBBNN）通过允许将权重打开或关闭，从而在权重和结构的联合空间中启用推断，也考虑了结构不确定性。本文将考虑LBBNN方法的两个扩展：首先，通过使用局部重参数化技巧（LRT）直接采样隐藏单元，我们得到了更加计算有效的算法。更重要的是，通过使用归一化流，我们可以近似潜在二进制变量的后验分布，从而在测试时实现网络的稀疏化。我们实验证明，我们提出的方法与现有的稀疏化技术相比，能够获得竞争性的结果，同时保持类似的准确性。",
    "tldr": "本论文介绍了一种新的方法来稀疏化贝叶斯神经网络，使用潜在二进制变量和归一化流，实现了网络在测试时的自动稀疏化，而且结果表明这个方法在准确性上能够与现有的稀疏化方法相媲美。",
    "en_tdlr": "This paper introduces a new method for sparsifying Bayesian neural networks using latent binary variables and normalizing flows, which enables automatic sparsification of the network at test time and achieves competitive results in accuracy compared to existing methods."
}