{
    "title": "Zero-shot Item-based Recommendation via Multi-task Product Knowledge Graph Pre-Training. (arXiv:2305.07633v1 [cs.IR])",
    "abstract": "Existing recommender systems face difficulties with zero-shot items, i.e. items that have no historical interactions with users during the training stage. Though recent works extract universal item representation via pre-trained language models (PLMs), they ignore the crucial item relationships. This paper presents a novel paradigm for the Zero-Shot Item-based Recommendation (ZSIR) task, which pre-trains a model on product knowledge graph (PKG) to refine the item features from PLMs. We identify three challenges for pre-training PKG, which are multi-type relations in PKG, semantic divergence between item generic information and relations and domain discrepancy from PKG to downstream ZSIR task. We address the challenges by proposing four pre-training tasks and novel task-oriented adaptation (ToA) layers. Moreover, this paper discusses how to fine-tune the model on new recommendation task such that the ToA layers are adapted to ZSIR task. Comprehensive experiments on 18 markets dataset ar",
    "link": "http://arxiv.org/abs/2305.07633",
    "context": "Title: Zero-shot Item-based Recommendation via Multi-task Product Knowledge Graph Pre-Training. (arXiv:2305.07633v1 [cs.IR])\nAbstract: Existing recommender systems face difficulties with zero-shot items, i.e. items that have no historical interactions with users during the training stage. Though recent works extract universal item representation via pre-trained language models (PLMs), they ignore the crucial item relationships. This paper presents a novel paradigm for the Zero-Shot Item-based Recommendation (ZSIR) task, which pre-trains a model on product knowledge graph (PKG) to refine the item features from PLMs. We identify three challenges for pre-training PKG, which are multi-type relations in PKG, semantic divergence between item generic information and relations and domain discrepancy from PKG to downstream ZSIR task. We address the challenges by proposing four pre-training tasks and novel task-oriented adaptation (ToA) layers. Moreover, this paper discusses how to fine-tune the model on new recommendation task such that the ToA layers are adapted to ZSIR task. Comprehensive experiments on 18 markets dataset ar",
    "path": "papers/23/05/2305.07633.json",
    "total_tokens": 978,
    "translated_title": "基于多任务产品知识图谱预训练的零样本基于项目推荐",
    "translated_abstract": "现有的推荐系统在处理零样本项目（即在训练阶段没有与用户进行过历史互动的项目）时面临困难。虽然最近的工作通过预训练语言模型（PLM）提取通用项目表示，但它们忽略了关键的项目关系。本文提出了一种新的方法，使用产品知识图谱（PKG）对模型进行预训练，以从PLMs中提炼出项目特征来解决零样本项目推荐（ZSIR）任务。我们确定了预训练PKG的三个挑战，即PKG中的多类型关系，项目通用信息和关系之间的语义差异以及从PKG到下游ZSIR任务的域差异。我们通过提出四个预训练任务和新颖的面向任务的适应（ToA）层来解决这些挑战。此外，本文还讨论了如何对新的推荐任务进行微调，使得ToA层适应于ZSIR任务。在18个市场数据集上进行了全面实验。",
    "tldr": "本论文提出了一种使用产品知识图谱预训练模型从预训练的语言模型提取项目特征，以解决零样本项目推荐任务。该方法通过提出四个预训练任务和任务导向的适应层来解决预训练过程中的挑战，并将模型微调到新的推荐任务中。",
    "en_tdlr": "This paper proposes a method for zero-shot item-based recommendation using pre-training on product knowledge graph to refine item features from pre-trained language models. Four pre-training tasks and task-oriented adaptation layers are introduced to address challenges in pre-training, and the model is fine-tuned for downstream recommendation task. Comprehensive experiments on 18 markets dataset are conducted."
}