{
    "title": "CPNet: Exploiting CLIP-based Attention Condenser and Probability Map Guidance for High-fidelity Talking Face Generation. (arXiv:2305.13962v1 [cs.MM])",
    "abstract": "Recently, talking face generation has drawn ever-increasing attention from the research community in computer vision due to its arduous challenges and widespread application scenarios, e.g. movie animation and virtual anchor. Although persevering efforts have been undertaken to enhance the fidelity and lip-sync quality of generated talking face videos, there is still large room for further improvements of synthesis quality and efficiency. Actually, these attempts somewhat ignore the explorations of fine-granularity feature extraction/integration and the consistency between probability distributions of landmarks, thereby recurring the issues of local details blurring and degraded fidelity. To mitigate these dilemmas, in this paper, a novel CLIP-based Attention and Probability Map Guided Network (CPNet) is delicately designed for inferring high-fidelity talking face videos. Specifically, considering the demands of fine-grained feature recalibration, a clip-based attention condenser is ex",
    "link": "http://arxiv.org/abs/2305.13962",
    "context": "Title: CPNet: Exploiting CLIP-based Attention Condenser and Probability Map Guidance for High-fidelity Talking Face Generation. (arXiv:2305.13962v1 [cs.MM])\nAbstract: Recently, talking face generation has drawn ever-increasing attention from the research community in computer vision due to its arduous challenges and widespread application scenarios, e.g. movie animation and virtual anchor. Although persevering efforts have been undertaken to enhance the fidelity and lip-sync quality of generated talking face videos, there is still large room for further improvements of synthesis quality and efficiency. Actually, these attempts somewhat ignore the explorations of fine-granularity feature extraction/integration and the consistency between probability distributions of landmarks, thereby recurring the issues of local details blurring and degraded fidelity. To mitigate these dilemmas, in this paper, a novel CLIP-based Attention and Probability Map Guided Network (CPNet) is delicately designed for inferring high-fidelity talking face videos. Specifically, considering the demands of fine-grained feature recalibration, a clip-based attention condenser is ex",
    "path": "papers/23/05/2305.13962.json",
    "total_tokens": 764,
    "translated_title": "CPNet：利用基于CLIP的关注凝聚器和概率映射指导实现高保真度的说话人脸生成",
    "translated_abstract": "最近，由于其艰巨的挑战和广泛的应用场景例如电影动画和虚拟主播，说话人脸生成引起了计算机视觉研究界的越来越多的关注。尽管已经进行了不断的努力来提高生成的说话人脸视频的保真度和口型同步质量，但是合成质量和效率仍有很大的提升空间。为了缓解这些困境，本文精心设计了一种全新的基于CLIP的关注和概率地图引导网络（CPNet）来推断高保真度的说话人脸视频。",
    "tldr": "本文提出了一种名为CPNet的网络，利用基于CLIP的关注凝聚器和概率映射指导实现高保真度的说话人脸生成。",
    "en_tdlr": "The paper proposes a novel CPNet network that exploits CLIP-based attention condenser and probability map guidance to infer high-fidelity talking face videos."
}