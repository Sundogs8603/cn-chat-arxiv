{
    "title": "Backdooring Neural Code Search. (arXiv:2305.17506v2 [cs.SE] UPDATED)",
    "abstract": "Reusing off-the-shelf code snippets from online repositories is a common practice, which significantly enhances the productivity of software developers. To find desired code snippets, developers resort to code search engines through natural language queries. Neural code search models are hence behind many such engines. These models are based on deep learning and gain substantial attention due to their impressive performance. However, the security aspect of these models is rarely studied. Particularly, an adversary can inject a backdoor in neural code search models, which return buggy or even vulnerable code with security/privacy issues. This may impact the downstream software (e.g., stock trading systems and autonomous driving) and cause financial loss and/or life-threatening incidents. In this paper, we demonstrate such attacks are feasible and can be quite stealthy. By simply modifying one variable/function name, the attacker can make buggy/vulnerable code rank in the top 11%. Our at",
    "link": "http://arxiv.org/abs/2305.17506",
    "context": "Title: Backdooring Neural Code Search. (arXiv:2305.17506v2 [cs.SE] UPDATED)\nAbstract: Reusing off-the-shelf code snippets from online repositories is a common practice, which significantly enhances the productivity of software developers. To find desired code snippets, developers resort to code search engines through natural language queries. Neural code search models are hence behind many such engines. These models are based on deep learning and gain substantial attention due to their impressive performance. However, the security aspect of these models is rarely studied. Particularly, an adversary can inject a backdoor in neural code search models, which return buggy or even vulnerable code with security/privacy issues. This may impact the downstream software (e.g., stock trading systems and autonomous driving) and cause financial loss and/or life-threatening incidents. In this paper, we demonstrate such attacks are feasible and can be quite stealthy. By simply modifying one variable/function name, the attacker can make buggy/vulnerable code rank in the top 11%. Our at",
    "path": "papers/23/05/2305.17506.json",
    "total_tokens": 1140,
    "translated_title": "神经代码搜索中的后门攻击",
    "translated_abstract": "从在线代码库中重复使用现成代码是常见的做法，它极大地提高了软件开发人员的生产力。要查找所需的代码片段，开发人员则要通过自然语言查询使用代码搜索引擎。因此，在许多这样的搜索引擎后面，都是神经代码搜索模型。这些模型基于深度学习，因其出色的性能而备受关注。然而，这些模型的安全性很少被研究。具体来说，攻击者可能会在神经代码搜索模型中注入后门，从而返回具有安全/隐私问题的错误或甚至易受攻击的代码。这可能会影响下游软件（例如股票交易系统和自动驾驶），并造成财务损失和/或危及生命的事件。在本文中，我们展示了这样的攻击是可行的，并且可能非常隐蔽。通过简单地修改一个变量/函数名称，攻击者可以使出现错误/易受攻击的代码排名前11％。我们的攻击利用了神经网络的可再训练性和代码相似度度量的宽松性，以注入后门。我们还提出了几种防御机制来缓解这种威胁，例如添加对抗性训练数据和特征压缩。我们相信我们的工作突显了研究AI系统安全方面的重要性，特别是在部署于安全关键领域时。",
    "tldr": "本文研究了神经代码搜索模型的安全性问题，指出攻击者可以注入后门来返回具有安全/隐私问题的代码，提出了几种防御机制来缓解这种威胁，该工作突显了研究AI系统安全方面的重要性，特别是在部署于安全关键领域时。",
    "en_tdlr": "This paper investigates the security issue of neural code search models and points out that attackers can inject backdoors to return code with security/privacy issues. Several defense mechanisms are proposed to mitigate this threat, emphasizing the importance of studying the security aspect of AI systems, especially in safety-critical domains."
}