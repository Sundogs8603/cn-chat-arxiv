{
    "title": "Randomized Positional Encodings Boost Length Generalization of Transformers. (arXiv:2305.16843v1 [cs.LG])",
    "abstract": "Transformers have impressive generalization capabilities on tasks with a fixed context length. However, they fail to generalize to sequences of arbitrary length, even for seemingly simple tasks such as duplicating a string. Moreover, simply training on longer sequences is inefficient due to the quadratic computation complexity of the global attention mechanism. In this work, we demonstrate that this failure mode is linked to positional encodings being out-of-distribution for longer sequences (even for relative encodings) and introduce a novel family of positional encodings that can overcome this problem. Concretely, our randomized positional encoding scheme simulates the positions of longer sequences and randomly selects an ordered subset to fit the sequence's length. Our large-scale empirical evaluation of 6000 models across 15 algorithmic reasoning tasks shows that our method allows Transformers to generalize to sequences of unseen length (increasing test accuracy by 12.0% on average",
    "link": "http://arxiv.org/abs/2305.16843",
    "context": "Title: Randomized Positional Encodings Boost Length Generalization of Transformers. (arXiv:2305.16843v1 [cs.LG])\nAbstract: Transformers have impressive generalization capabilities on tasks with a fixed context length. However, they fail to generalize to sequences of arbitrary length, even for seemingly simple tasks such as duplicating a string. Moreover, simply training on longer sequences is inefficient due to the quadratic computation complexity of the global attention mechanism. In this work, we demonstrate that this failure mode is linked to positional encodings being out-of-distribution for longer sequences (even for relative encodings) and introduce a novel family of positional encodings that can overcome this problem. Concretely, our randomized positional encoding scheme simulates the positions of longer sequences and randomly selects an ordered subset to fit the sequence's length. Our large-scale empirical evaluation of 6000 models across 15 algorithmic reasoning tasks shows that our method allows Transformers to generalize to sequences of unseen length (increasing test accuracy by 12.0% on average",
    "path": "papers/23/05/2305.16843.json",
    "total_tokens": 817,
    "translated_title": "随机位置编码提升了Transformer的长度普适性",
    "translated_abstract": "Transformer在固定长度的任务上拥有惊人的普适性，但它们无法推广到任意长度的序列，甚至是像复制字符串这样看似简单的任务也会失败。此外，由于全局注意机制的二次计算复杂度，仅仅训练更长的序列是低效的。本文表明这种失败模式与长度更长的序列（即使是相对编码）的位置编码在超出分布范围时有关，并引入一种能够克服此问题的新颖位置编码系列。具体而言，我们的随机位置编码机制模拟了更长序列的位置，并随机选择一个有序子集来适应序列的长度。我们在15个算法推理任务的6000种模型的大规模实证评估显示出，我们的方法使Transformer能够推广到未见长度的序列（平均测试准确度提高了12.0%）。",
    "tldr": "本文提出一种随机位置编码机制，能够提高Transformer的长度普适性，使其在算法推理任务中表现出色。",
    "en_tdlr": "This paper introduces a randomized positional encoding mechanism that improves the length generalization of Transformers, allowing them to perform well in algorithmic reasoning tasks by simulating longer sequences and randomly selecting an ordered subset to fit the sequence's length."
}