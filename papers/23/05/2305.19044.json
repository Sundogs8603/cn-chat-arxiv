{
    "title": "Exploring the Promise and Limits of Real-Time Recurrent Learning",
    "abstract": "arXiv:2305.19044v2 Announce Type: replace  Abstract: Real-time recurrent learning (RTRL) for sequence-processing recurrent neural networks (RNNs) offers certain conceptual advantages over backpropagation through time (BPTT). RTRL requires neither caching past activations nor truncating context, and enables online learning. However, RTRL's time and space complexity make it impractical. To overcome this problem, most recent work on RTRL focuses on approximation theories, while experiments are often limited to diagnostic settings. Here we explore the practical promise of RTRL in more realistic settings. We study actor-critic methods that combine RTRL and policy gradients, and test them in several subsets of DMLab-30, ProcGen, and Atari-2600 environments. On DMLab memory tasks, our system trained on fewer than 1.2 B environmental frames is competitive with or outperforms well-known IMPALA and R2D2 baselines trained on 10 B frames. To scale to such challenging tasks, we focus on certain wel",
    "link": "https://arxiv.org/abs/2305.19044",
    "context": "Title: Exploring the Promise and Limits of Real-Time Recurrent Learning\nAbstract: arXiv:2305.19044v2 Announce Type: replace  Abstract: Real-time recurrent learning (RTRL) for sequence-processing recurrent neural networks (RNNs) offers certain conceptual advantages over backpropagation through time (BPTT). RTRL requires neither caching past activations nor truncating context, and enables online learning. However, RTRL's time and space complexity make it impractical. To overcome this problem, most recent work on RTRL focuses on approximation theories, while experiments are often limited to diagnostic settings. Here we explore the practical promise of RTRL in more realistic settings. We study actor-critic methods that combine RTRL and policy gradients, and test them in several subsets of DMLab-30, ProcGen, and Atari-2600 environments. On DMLab memory tasks, our system trained on fewer than 1.2 B environmental frames is competitive with or outperforms well-known IMPALA and R2D2 baselines trained on 10 B frames. To scale to such challenging tasks, we focus on certain wel",
    "path": "papers/23/05/2305.19044.json",
    "total_tokens": 1065,
    "translated_title": "探索实时递归学习的潜力与限制",
    "translated_abstract": "实时递归学习（RTRL）用于序列处理的递归神经网络（RNN）相比于时间反向传播（BPTT）具有一定的概念优势。RTRL既不需要缓存过去的激活状态，也不需要截断上下文，而且支持在线学习。然而，RTRL的时间和空间复杂度使其实际应用困难。为了克服这个问题，最近关于RTRL的工作主要集中在近似理论上，而实验通常局限于诊断设置。在这里，我们在更现实的环境中探索了RTRL的实际潜力。我们研究了结合了RTRL和策略梯度的演员-评论家方法，并在DMLab-30、ProcGen和Atari-2600环境的几个子集中进行了测试。在DMLab存储任务中，我们的系统在少于1.2 B的环境帧上训练，与或优于在10 B帧上训练的著名IMPALA和R2D2基线。为了扩展到这些具有挑战性的任务，我们专注于某些方面",
    "tldr": "实时递归学习（RTRL）具有一定概念优势，不需要缓存过去的激活状态和截断上下文，支持在线学习，在演员-评论家方法中探索了其实际潜力，并在DMLab-30、ProcGen和Atari-2600环境中进行了测试，在DMLab存储任务中表现出与优于IMPALA和R2D2基线相媲美的竞争力，为了应对复杂任务，研究重点放在了某些方面",
    "en_tdlr": "Real-time recurrent learning (RTRL) has conceptual advantages over backpropagation through time (BPTT), does not require caching past activations or truncating context, supports online learning, explores its practical potential in the actor-critic method, and performs competitively or outperforms well-known baselines like IMPALA and R2D2 in DMLab memory tasks, focusing on certain aspects to scale to challenging tasks."
}