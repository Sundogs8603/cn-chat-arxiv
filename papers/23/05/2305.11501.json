{
    "title": "From Alignment to Entailment: A Unified Textual Entailment Framework for Entity Alignment. (arXiv:2305.11501v1 [cs.CL])",
    "abstract": "Entity Alignment (EA) aims to find the equivalent entities between two Knowledge Graphs (KGs). Existing methods usually encode the triples of entities as embeddings and learn to align the embeddings, which prevents the direct interaction between the original information of the cross-KG entities. Moreover, they encode the relational triples and attribute triples of an entity in heterogeneous embedding spaces, which prevents them from helping each other. In this paper, we transform both triples into unified textual sequences, and model the EA task as a bi-directional textual entailment task between the sequences of cross-KG entities. Specifically, we feed the sequences of two entities simultaneously into a pre-trained language model (PLM) and propose two kinds of PLM-based entity aligners that model the entailment probability between sequences as the similarity between entities. Our approach captures the unified correlation pattern of two kinds of information between entities, and explic",
    "link": "http://arxiv.org/abs/2305.11501",
    "context": "Title: From Alignment to Entailment: A Unified Textual Entailment Framework for Entity Alignment. (arXiv:2305.11501v1 [cs.CL])\nAbstract: Entity Alignment (EA) aims to find the equivalent entities between two Knowledge Graphs (KGs). Existing methods usually encode the triples of entities as embeddings and learn to align the embeddings, which prevents the direct interaction between the original information of the cross-KG entities. Moreover, they encode the relational triples and attribute triples of an entity in heterogeneous embedding spaces, which prevents them from helping each other. In this paper, we transform both triples into unified textual sequences, and model the EA task as a bi-directional textual entailment task between the sequences of cross-KG entities. Specifically, we feed the sequences of two entities simultaneously into a pre-trained language model (PLM) and propose two kinds of PLM-based entity aligners that model the entailment probability between sequences as the similarity between entities. Our approach captures the unified correlation pattern of two kinds of information between entities, and explic",
    "path": "papers/23/05/2305.11501.json",
    "total_tokens": 1008,
    "translated_title": "从对齐到蕴涵：面向实体对齐的统一文本蕴涵框架",
    "translated_abstract": "实体对齐旨在发现两个知识图谱之间的等效实体。现有方法通常将实体的三元组编码为嵌入并学习对齐嵌入，这导致跨知识图谱实体的原始信息无法直接交互作用。此外，它们将实体的关系三元组和属性三元组编码为异构嵌入空间，这阻止了它们互相帮助。本文将两个三元组转换为统一的文本序列，并将EA任务建模为两个实体序列之间的双向文本蕴涵任务。具体而言，我们同时将两个实体的序列馈送到预训练语言模型（PLM）中，并提出了两种基于PLM的实体对齐器，将序列之间的蕴涵概率建模为实体之间的相似度。我们的方法捕获了两种实体信息之间的统一相关模式，并用文本解释说明了对齐的实体。在两个基准数据集上的实验结果证明了我们提出的文本蕴涵框架对EA的有效性。",
    "tldr": "该论文提出了一个基于文本蕴涵的实体对齐框架，能够将实体的三元组转化为统一的文本序列，通过预训练语言模型计算实体之间的蕴涵概率进行实体对齐，能够更好地捕捉实体之间的相关信息，并且给出了文本解释说明。",
    "en_tdlr": "This paper proposes a textual entailment-based framework for entity alignment, which transforms the triples of entities into unified textual sequences and models the bi-directional entailment between the sequences as the alignment probability. The proposed framework can better capture the correlations between entities and provide textual explanations."
}