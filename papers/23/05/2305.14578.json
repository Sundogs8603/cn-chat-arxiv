{
    "title": "Connecting the Dots: What Graph-Based Text Representations Work Best for Text Classification using Graph Neural Networks?. (arXiv:2305.14578v1 [cs.CL])",
    "abstract": "Given the success of Graph Neural Networks (GNNs) for structure-aware machine learning, numerous studies have explored their application to text classification, as an alternative to traditional feature representation models. However, most studies considered just a specific domain and validated on data with particular characteristics. This work presents an extensive empirical investigation of graph-based text representation methods proposed for text classification, identifying practical implications and open challenges in the field. We compare several GNN architectures as well as BERT across five datasets, encompassing short and also long documents. The results show that: i) graph performance is highly related to the textual input features and domain, ii) despite its outstanding performance, BERT has difficulties converging when dealing with short texts, iii) graph methods are particularly beneficial for longer documents.",
    "link": "http://arxiv.org/abs/2305.14578",
    "context": "Title: Connecting the Dots: What Graph-Based Text Representations Work Best for Text Classification using Graph Neural Networks?. (arXiv:2305.14578v1 [cs.CL])\nAbstract: Given the success of Graph Neural Networks (GNNs) for structure-aware machine learning, numerous studies have explored their application to text classification, as an alternative to traditional feature representation models. However, most studies considered just a specific domain and validated on data with particular characteristics. This work presents an extensive empirical investigation of graph-based text representation methods proposed for text classification, identifying practical implications and open challenges in the field. We compare several GNN architectures as well as BERT across five datasets, encompassing short and also long documents. The results show that: i) graph performance is highly related to the textual input features and domain, ii) despite its outstanding performance, BERT has difficulties converging when dealing with short texts, iii) graph methods are particularly beneficial for longer documents.",
    "path": "papers/23/05/2305.14578.json",
    "total_tokens": 861,
    "translated_title": "连接点：基于图网络的文本表示在文本分类中的最佳表现研究",
    "translated_abstract": "鉴于图神经网络在结构感知机器学习中的成功，许多研究已经探索了它们作为传统特征表示模型的替代方法，用于文本分类。然而，大多数研究仅考虑了特定领域，并验证了具有特定特征的数据。本文对提出用于文本分类的基于图的文本表示方法进行了广泛的实证研究，确定了实际实施的含义和领域中的挑战。我们比较了五个数据集中的几种GNN架构以及BERT，涵盖了长短文档。结果表明：i）图的性能与文本输入特征和领域密切相关，ii）尽管其表现出色，但BERT在处理短文本时难以收敛， iii）图方法对于较长的文档特别有益。",
    "tldr": "本文研究了基于图的文本表示方法在文本分类中的应用，发现文本输入特征和领域要素对图的性能具有重要影响，BERT在处理短文本时难以收敛，图方法对于较长的文档特别有益。",
    "en_tdlr": "This paper investigates the application of graph-based text representation methods for text classification, and finds that the performance of graphs is highly influenced by the textual input features and domain, BERT has difficulty converging when dealing with short texts, and graph methods are particularly beneficial for longer documents."
}