{
    "title": "Communication-Efficient Graph Neural Networks with Probabilistic Neighborhood Expansion Analysis and Caching. (arXiv:2305.03152v1 [cs.LG])",
    "abstract": "Training and inference with graph neural networks (GNNs) on massive graphs has been actively studied since the inception of GNNs, owing to the widespread use and success of GNNs in applications such as recommendation systems and financial forensics. This paper is concerned with minibatch training and inference with GNNs that employ node-wise sampling in distributed settings, where the necessary partitioning of vertex features across distributed storage causes feature communication to become a major bottleneck that hampers scalability. To significantly reduce the communication volume without compromising prediction accuracy, we propose a policy for caching data associated with frequently accessed vertices in remote partitions. The proposed policy is based on an analysis of vertex-wise inclusion probabilities (VIP) during multi-hop neighborhood sampling, which may expand the neighborhood far beyond the partition boundaries of the graph. VIP analysis not only enables the elimination of th",
    "link": "http://arxiv.org/abs/2305.03152",
    "context": "Title: Communication-Efficient Graph Neural Networks with Probabilistic Neighborhood Expansion Analysis and Caching. (arXiv:2305.03152v1 [cs.LG])\nAbstract: Training and inference with graph neural networks (GNNs) on massive graphs has been actively studied since the inception of GNNs, owing to the widespread use and success of GNNs in applications such as recommendation systems and financial forensics. This paper is concerned with minibatch training and inference with GNNs that employ node-wise sampling in distributed settings, where the necessary partitioning of vertex features across distributed storage causes feature communication to become a major bottleneck that hampers scalability. To significantly reduce the communication volume without compromising prediction accuracy, we propose a policy for caching data associated with frequently accessed vertices in remote partitions. The proposed policy is based on an analysis of vertex-wise inclusion probabilities (VIP) during multi-hop neighborhood sampling, which may expand the neighborhood far beyond the partition boundaries of the graph. VIP analysis not only enables the elimination of th",
    "path": "papers/23/05/2305.03152.json",
    "total_tokens": 939,
    "translated_title": "通信高效的概率邻域扩展分析与缓存的图神经网络",
    "translated_abstract": "自图神经网络（GNNs）诞生以来，如何在规模庞大的图上训练和推断一直受到广泛关注，因为GNN在推荐系统和金融取证等应用中被广泛使用且取得了成功。本文针对在分布式设置中使用节点抽样的GNN进行小批量训练和推断，其中跨分布式存储的顶点特征的必要分区会导致特征通信成为制约可扩展性的主要瓶颈。为了显著减少通信量而不影响预测准确性，我们提出了一种策略，即缓存远程分区中与访问频率高的顶点相关的数据。所提出的策略基于顶点的多跳邻域抽样中的点包含概率（VIP）分析，这可能会将邻域扩展到图的分区边界之外。VIP分析不仅能够消除离线计算，而且能够将通信开销降到近似线性，同时保持预测准确度不变。",
    "tldr": "本文针对分布式设置中GNN的小批量训练和推断提出了一种缓存策略，基于顶点的多跳邻域抽样中的点包含概率分析（VIP），可以显著减少通信量而不影响预测准确度。",
    "en_tdlr": "This paper proposes a caching strategy for minibatch training and inference with GNNs in distributed settings, based on an analysis of vertex-wise inclusion probabilities (VIP) during multi-hop neighborhood sampling, which can significantly reduce communication volume without compromising prediction accuracy."
}