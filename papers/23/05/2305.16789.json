{
    "title": "Modulate Your Spectrum in Self-Supervised Learning. (arXiv:2305.16789v1 [cs.LG])",
    "abstract": "Whitening loss provides theoretical guarantee in avoiding feature collapse for self-supervised learning (SSL) using joint embedding architectures. One typical implementation of whitening loss is hard whitening that designs whitening transformation over embedding and imposes the loss on the whitened output. In this paper, we propose spectral transformation (ST) framework to map the spectrum of embedding to a desired distribution during forward pass, and to modulate the spectrum of embedding by implicit gradient update during backward pass. We show that whitening transformation is a special instance of ST by definition, and there exist other instances that can avoid collapse by our empirical investigation. Furthermore, we propose a new instance of ST, called IterNorm with trace loss (INTL). We theoretically prove that INTL can avoid collapse and modulate the spectrum of embedding towards an equal-eigenvalue distribution during the course of optimization. Moreover, INTL achieves 76.6% top",
    "link": "http://arxiv.org/abs/2305.16789",
    "context": "Title: Modulate Your Spectrum in Self-Supervised Learning. (arXiv:2305.16789v1 [cs.LG])\nAbstract: Whitening loss provides theoretical guarantee in avoiding feature collapse for self-supervised learning (SSL) using joint embedding architectures. One typical implementation of whitening loss is hard whitening that designs whitening transformation over embedding and imposes the loss on the whitened output. In this paper, we propose spectral transformation (ST) framework to map the spectrum of embedding to a desired distribution during forward pass, and to modulate the spectrum of embedding by implicit gradient update during backward pass. We show that whitening transformation is a special instance of ST by definition, and there exist other instances that can avoid collapse by our empirical investigation. Furthermore, we propose a new instance of ST, called IterNorm with trace loss (INTL). We theoretically prove that INTL can avoid collapse and modulate the spectrum of embedding towards an equal-eigenvalue distribution during the course of optimization. Moreover, INTL achieves 76.6% top",
    "path": "papers/23/05/2305.16789.json",
    "total_tokens": 858,
    "translated_title": "在自监督学习中调节频谱",
    "translated_abstract": "白化损失为使用联合嵌入架构进行自监督学习提供了理论保证，避免了特征崩溃。本文提出了谱变换（ST）框架，在前向传递过程中将嵌入的频谱映射到所需的分布，并在反向传递过程中通过隐式梯度更新来调制嵌入的频谱。我们证明了白化变换是ST的一个特例，还有其他实例可以避免崩溃。此外，本文提出了INTL（IterNorm with trace loss）的新实例。我们理论上证明了INTL可以避免崩溃，并在优化过程中将嵌入的频谱调节到一个等特征值分布。此外，INTL实现了76.6％的最高精度。",
    "tldr": "本文提出了谱变换（ST）框架，可以调节自监督学习的频谱，并避免特征崩溃。其中，INTL是ST的一个实例，能够在优化过程中将嵌入的频谱调节到一个等特征值分布，实现较高的准确率。",
    "en_tdlr": "This paper proposes a spectral transformation (ST) framework to modulate the spectrum of embedding and avoid feature collapse in self-supervised learning. INTL, a new instance of ST, is proven to be able to modulate the embedding spectrum towards an equal-eigenvalue distribution and achieve high accuracy."
}