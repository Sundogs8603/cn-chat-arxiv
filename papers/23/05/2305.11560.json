{
    "title": "Brain Captioning: Decoding human brain activity into images and text. (arXiv:2305.11560v1 [cs.CV])",
    "abstract": "Every day, the human brain processes an immense volume of visual information, relying on intricate neural mechanisms to perceive and interpret these stimuli. Recent breakthroughs in functional magnetic resonance imaging (fMRI) have enabled scientists to extract visual information from human brain activity patterns. In this study, we present an innovative method for decoding brain activity into meaningful images and captions, with a specific focus on brain captioning due to its enhanced flexibility as compared to brain decoding into images. Our approach takes advantage of cutting-edge image captioning models and incorporates a unique image reconstruction pipeline that utilizes latent diffusion models and depth estimation. We utilized the Natural Scenes Dataset, a comprehensive fMRI dataset from eight subjects who viewed images from the COCO dataset. We employed the Generative Image-to-text Transformer (GIT) as our backbone for captioning and propose a new image reconstruction pipeline b",
    "link": "http://arxiv.org/abs/2305.11560",
    "context": "Title: Brain Captioning: Decoding human brain activity into images and text. (arXiv:2305.11560v1 [cs.CV])\nAbstract: Every day, the human brain processes an immense volume of visual information, relying on intricate neural mechanisms to perceive and interpret these stimuli. Recent breakthroughs in functional magnetic resonance imaging (fMRI) have enabled scientists to extract visual information from human brain activity patterns. In this study, we present an innovative method for decoding brain activity into meaningful images and captions, with a specific focus on brain captioning due to its enhanced flexibility as compared to brain decoding into images. Our approach takes advantage of cutting-edge image captioning models and incorporates a unique image reconstruction pipeline that utilizes latent diffusion models and depth estimation. We utilized the Natural Scenes Dataset, a comprehensive fMRI dataset from eight subjects who viewed images from the COCO dataset. We employed the Generative Image-to-text Transformer (GIT) as our backbone for captioning and propose a new image reconstruction pipeline b",
    "path": "papers/23/05/2305.11560.json",
    "total_tokens": 968,
    "tldr": "本文介绍了一种创新的方法，利用 fMRI 数据将大脑活动解码为有意义的图像和字幕，特别关注于脑字幕生成。该方法利用了先进的图像字幕生成模型和独特的图像重建管道，部分采用潜空间扩散模型和深度估计，为实现大脑字幕生成提供了思路。"
}