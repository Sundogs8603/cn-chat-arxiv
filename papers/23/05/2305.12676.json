{
    "title": "Exploring Energy-based Language Models with Different Architectures and Training Methods for Speech Recognition. (arXiv:2305.12676v2 [cs.CL] UPDATED)",
    "abstract": "Energy-based language models (ELMs) parameterize an unnormalized distribution for natural sentences and are radically different from popular autoregressive language models (ALMs). As an important application, ELMs have been successfully used as a means for calculating sentence scores in speech recognition, but they all use less-modern CNN or LSTM networks. The recent progress in Transformer networks and large pretrained models such as BERT and GPT2 opens new possibility to further advancing ELMs. In this paper, we explore different architectures of energy functions and different training methods to investigate the capabilities of ELMs in rescoring for speech recognition, all using large pretrained models as backbones.",
    "link": "http://arxiv.org/abs/2305.12676",
    "context": "Title: Exploring Energy-based Language Models with Different Architectures and Training Methods for Speech Recognition. (arXiv:2305.12676v2 [cs.CL] UPDATED)\nAbstract: Energy-based language models (ELMs) parameterize an unnormalized distribution for natural sentences and are radically different from popular autoregressive language models (ALMs). As an important application, ELMs have been successfully used as a means for calculating sentence scores in speech recognition, but they all use less-modern CNN or LSTM networks. The recent progress in Transformer networks and large pretrained models such as BERT and GPT2 opens new possibility to further advancing ELMs. In this paper, we explore different architectures of energy functions and different training methods to investigate the capabilities of ELMs in rescoring for speech recognition, all using large pretrained models as backbones.",
    "path": "papers/23/05/2305.12676.json",
    "total_tokens": 746,
    "translated_title": "探索不同架构和训练方法下基于能量的语言模型在语音识别中的应用",
    "translated_abstract": "基于能量的语言模型（ELM）通过参数化自然语句的非归一化分布与流行的自回归语言模型（ALM）有根本性区别。作为一种重要的应用，ELM已成功地用于语音识别中计算句子得分，但它们都使用不太现代的CNN或LSTM网络。随着Transformer网络和大型预训练模型（如BERT和GPT2）的最新进展，进一步提高ELMs的能力已经成为可能。在本文中，我们探索了不同的能量函数架构和不同的训练方法，以研究在以大型预训练模型作为骨干的语音识别中，ELMs的能力。",
    "tldr": "本文探索了不同的能量函数架构和不同的训练方法，以提高基于能量的语言模型在语音识别中计算句子得分的能力。",
    "en_tdlr": "This paper explores different architectures of energy functions and different training methods to improve the capability of energy-based language models in calculating sentence scores for speech recognition using large pretrained models as backbones."
}