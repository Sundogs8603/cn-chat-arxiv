{
    "title": "Approximation and Estimation Ability of Transformers for Sequence-to-Sequence Functions with Infinite Dimensional Input",
    "abstract": "arXiv:2305.18699v1 Announce Type: cross  Abstract: Despite the great success of Transformer networks in various applications such as natural language processing and computer vision, their theoretical aspects are not well understood. In this paper, we study the approximation and estimation ability of Transformers as sequence-to-sequence functions with infinite dimensional inputs. Although inputs and outputs are both infinite dimensional, we show that when the target function has anisotropic smoothness, Transformers can avoid the curse of dimensionality due to their feature extraction ability and parameter sharing property. In addition, we show that even if the smoothness changes depending on each input, Transformers can estimate the importance of features for each input and extract important features dynamically. Then, we proved that Transformers achieve similar convergence rate as in the case of the fixed smoothness. Our theoretical results support the practical success of Transformers",
    "link": "https://arxiv.org/abs/2305.18699",
    "context": "Title: Approximation and Estimation Ability of Transformers for Sequence-to-Sequence Functions with Infinite Dimensional Input\nAbstract: arXiv:2305.18699v1 Announce Type: cross  Abstract: Despite the great success of Transformer networks in various applications such as natural language processing and computer vision, their theoretical aspects are not well understood. In this paper, we study the approximation and estimation ability of Transformers as sequence-to-sequence functions with infinite dimensional inputs. Although inputs and outputs are both infinite dimensional, we show that when the target function has anisotropic smoothness, Transformers can avoid the curse of dimensionality due to their feature extraction ability and parameter sharing property. In addition, we show that even if the smoothness changes depending on each input, Transformers can estimate the importance of features for each input and extract important features dynamically. Then, we proved that Transformers achieve similar convergence rate as in the case of the fixed smoothness. Our theoretical results support the practical success of Transformers",
    "path": "papers/23/05/2305.18699.json",
    "total_tokens": 803,
    "translated_title": "Transformer对具有无限维输入的序列到序列函数的逼近和估计能力",
    "translated_abstract": "虽然Transformer网络在自然语言处理和计算机视觉等各种应用中取得了巨大成功，但它们的理论方面尚未得到很好理解。本文研究了Transformer作为具有无限维输入的序列到序列函数的逼近和估计能力。尽管输入和输出都是无限维的，我们表明当目标函数具有各向异性平滑性时，Transformer可以通过其特征提取能力和参数共享属性避免维度诅咒。此外，我们表明即使平滑性因输入而异，Transformer仍然可以估计每个输入的特征重要性并动态提取重要特征。然后，我们证明了Transformer实现了与固定平滑性情况下相似的收敛速率。我们的理论结果支持了Transformer在实践中取得的成功。",
    "tldr": "Transformer网络作为具有无限维输入的序列到序列函数，通过其特征提取能力和参数共享属性，能够避免维度诅咒，实现对目标函数的逼近和估计。"
}