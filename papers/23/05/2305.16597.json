{
    "title": "Neural Architecture Search for Parameter-Efficient Fine-tuning of Large Pre-trained Language Models. (arXiv:2305.16597v1 [cs.CL])",
    "abstract": "Parameter-efficient tuning (PET) methods fit pre-trained language models (PLMs) to downstream tasks by either computing a small compressed update for a subset of model parameters, or appending and fine-tuning a small number of new model parameters to the pre-trained network. Hand-designed PET architectures from the literature perform well in practice, but have the potential to be improved via automated neural architecture search (NAS). We propose an efficient NAS method for learning PET architectures via structured and unstructured pruning. We present experiments on GLUE demonstrating the effectiveness of our algorithm and discuss how PET architectural design choices affect performance in practice.",
    "link": "http://arxiv.org/abs/2305.16597",
    "context": "Title: Neural Architecture Search for Parameter-Efficient Fine-tuning of Large Pre-trained Language Models. (arXiv:2305.16597v1 [cs.CL])\nAbstract: Parameter-efficient tuning (PET) methods fit pre-trained language models (PLMs) to downstream tasks by either computing a small compressed update for a subset of model parameters, or appending and fine-tuning a small number of new model parameters to the pre-trained network. Hand-designed PET architectures from the literature perform well in practice, but have the potential to be improved via automated neural architecture search (NAS). We propose an efficient NAS method for learning PET architectures via structured and unstructured pruning. We present experiments on GLUE demonstrating the effectiveness of our algorithm and discuss how PET architectural design choices affect performance in practice.",
    "path": "papers/23/05/2305.16597.json",
    "total_tokens": 796,
    "translated_title": "基于神经架构搜索的参数高效微调大型预训练语言模型",
    "translated_abstract": "参数高效微调（PET）方法通过计算部分模型参数的小型压缩更新或添加和微调少量新的模型参数到预训练网络，将预训练语言模型（PLM）适应下游任务。手工设计的PET架构在实践中表现良好，但通过自动神经架构搜索（NAS），它们有改进的潜力。我们提出了一种通过结构化和非结构化剪枝学习PET结构的有效NAS方法。我们在GLUE上进行了实验，展示了我们算法的有效性，并讨论了PET架构设计选择如何影响实际性能。",
    "tldr": "本文提出了一种基于神经架构搜索的参数高效微调大型预训练语言模型的方法，通过结构化和非结构化剪枝学习PET结构并在GLUE上进行实验验证，展示了该算法的有效性，探讨了PET架构设计选择对实际性能的影响。",
    "en_tdlr": "This paper proposes an efficient neural architecture search (NAS) method for learning parameter-efficient tuning (PET) architectures via structured and unstructured pruning, which adapt pre-trained language models (PLMs) to downstream tasks by either computing a small compressed update for a subset of model parameters or appending and fine-tuning a small number of new model parameters to the pre-trained network. Experiments on GLUE are presented to demonstrate the effectiveness of the proposed algorithm and discuss how PET architectural design choices affect performance in practice."
}