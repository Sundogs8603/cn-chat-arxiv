{
    "title": "One Network, Many Masks: Towards More Parameter-Efficient Transfer Learning. (arXiv:2305.17682v2 [cs.CL] UPDATED)",
    "abstract": "Fine-tuning pre-trained language models for multiple tasks tends to be expensive in terms of storage. To mitigate this, parameter-efficient transfer learning (PETL) methods have been proposed to address this issue, but they still require a significant number of parameters and storage when being applied to broader ranges of tasks. To achieve even greater storage reduction, we propose PROPETL, a novel method that enables efficient sharing of a single PETL module which we call prototype network (e.g., adapter, LoRA, and prefix-tuning) across layers and tasks. We then learn binary masks to select different sub-networks from the shared prototype network and apply them as PETL modules into different layers. We find that the binary masks can determine crucial information from the network, which is often ignored in previous studies. Our work can also be seen as a type of pruning method, where we find that overparameterization also exists in the seemingly small PETL modules. We evaluate PROPETL",
    "link": "http://arxiv.org/abs/2305.17682",
    "context": "Title: One Network, Many Masks: Towards More Parameter-Efficient Transfer Learning. (arXiv:2305.17682v2 [cs.CL] UPDATED)\nAbstract: Fine-tuning pre-trained language models for multiple tasks tends to be expensive in terms of storage. To mitigate this, parameter-efficient transfer learning (PETL) methods have been proposed to address this issue, but they still require a significant number of parameters and storage when being applied to broader ranges of tasks. To achieve even greater storage reduction, we propose PROPETL, a novel method that enables efficient sharing of a single PETL module which we call prototype network (e.g., adapter, LoRA, and prefix-tuning) across layers and tasks. We then learn binary masks to select different sub-networks from the shared prototype network and apply them as PETL modules into different layers. We find that the binary masks can determine crucial information from the network, which is often ignored in previous studies. Our work can also be seen as a type of pruning method, where we find that overparameterization also exists in the seemingly small PETL modules. We evaluate PROPETL",
    "path": "papers/23/05/2305.17682.json",
    "total_tokens": 905,
    "translated_title": "一网络，多任务：更高效的参数共用迁移学习方法",
    "translated_abstract": "对于多个任务来说，微调预训练的语言模型往往会占用大量存储空间，参数共用迁移学习（PETL）方法可以缓解这个问题，但在应用于更广泛的任务范围时仍需要大量的参数和存储空间。为了实现更大的存储空间减少，我们提出了PROPETL，这是一种新颖的方法，可以在不同层和任务之间使用单个PETL模块，我们称之为原型网络（例如适配器、LoRA和前缀调整）。我们然后学习二进制掩码以从共享的原型网络中选择不同的子网络，并将它们作为PETL模块应用于不同的层。我们发现，二进制掩码可以确定网络中关键的信息，这在以前的研究中经常被忽略。我们的工作也可以看作是一种修剪方法，我们发现即使在看似很小的PETL模块中也存在过度参数化。我们对PROPETL进行了评估。",
    "tldr": "本研究提出了PROPETL方法，通过原型网络和二进制掩码实现了更高效的参数共用迁移学习，解决了多任务微调预训练语言模型存储空间占用的问题。",
    "en_tdlr": "This study proposes PROPETL method, which achieves more parameter-efficient transfer learning through prototype network and binary masks, and solves the problem of storage use in fine-tuning pre-trained language models for multiple tasks."
}