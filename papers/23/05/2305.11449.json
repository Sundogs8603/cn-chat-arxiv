{
    "title": "Analyzing and Reducing the Performance Gap in Cross-Lingual Transfer with Fine-tuning Slow and Fast. (arXiv:2305.11449v1 [cs.CL])",
    "abstract": "Existing research has shown that a multilingual pre-trained language model fine-tuned with one (source) language also performs well on downstream tasks for non-source languages, even though no fine-tuning is done on these languages. However, there is a clear gap between the performance of the source language and that of the non-source languages. This paper analyzes the fine-tuning process, discovers when the performance gap changes and identifies which network weights affect the overall performance most. Additionally, the paper seeks to answer to what extent the gap can be reduced by reducing forgetting. Based on the analysis results, a method named Fine-tuning slow and fast with four training policies is proposed to address these issues. Experimental results show the proposed method outperforms baselines by a clear margin.",
    "link": "http://arxiv.org/abs/2305.11449",
    "context": "Title: Analyzing and Reducing the Performance Gap in Cross-Lingual Transfer with Fine-tuning Slow and Fast. (arXiv:2305.11449v1 [cs.CL])\nAbstract: Existing research has shown that a multilingual pre-trained language model fine-tuned with one (source) language also performs well on downstream tasks for non-source languages, even though no fine-tuning is done on these languages. However, there is a clear gap between the performance of the source language and that of the non-source languages. This paper analyzes the fine-tuning process, discovers when the performance gap changes and identifies which network weights affect the overall performance most. Additionally, the paper seeks to answer to what extent the gap can be reduced by reducing forgetting. Based on the analysis results, a method named Fine-tuning slow and fast with four training policies is proposed to address these issues. Experimental results show the proposed method outperforms baselines by a clear margin.",
    "path": "papers/23/05/2305.11449.json",
    "total_tokens": 827,
    "translated_title": "分析和减少跨语言迁移中的性能差距：缓慢和快速微调方法",
    "translated_abstract": "现有研究表明，多语言预训练语言模型在一个（源）语言上进行微调后，在非源语言的下游任务中也表现良好，尽管这些语言没有进行微调，但源语言和非源语言之间存在明显的性能差距。本文分析了微调过程，发现了性能差距何时改变，并确定哪些网络权重对整体性能影响最大。此外，本文试图回答通过减少遗忘来多大程度上可以缩小差距。基于分析结果，提出了一种名为缓慢和快速微调的方法，包括四种训练策略来解决这些问题。实验结果表明，与基线方法相比，该方法的性能有明显提高。",
    "tldr": "本文分析了微调过程，提出了缓慢和快速微调的方法来解决跨语言迁移中的性能差距问题，通过减少遗忘来弥补性能差距，实验结果表明该方法的性能比基线方法好。",
    "en_tdlr": "This paper analyzes the fine-tuning process and proposes a method called Fine-tuning slow and fast to address the performance gap in cross-lingual transfer by reducing forgetting. Experimental results show that the proposed method outperforms baselines significantly."
}