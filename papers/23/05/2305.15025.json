{
    "title": "Dior-CVAE: Pre-trained Language Models and Diffusion Priors for Variational Dialog Generation. (arXiv:2305.15025v2 [cs.CL] UPDATED)",
    "abstract": "Current variational dialog models have employed pre-trained language models (PLMs) to parameterize the likelihood and posterior distributions. However, the Gaussian assumption made on the prior distribution is incompatible with these distributions, thus restricting the diversity of generated responses. These models also suffer from posterior collapse, i.e., the decoder tends to ignore latent variables and directly access information captured in the encoder through the cross-attention mechanism. In this work, we propose Dior-CVAE, a hierarchical conditional variational autoencoder (CVAE) with diffusion priors to address these challenges. We employ a diffusion model to increase the complexity of the prior distribution and its compatibility with the distributions produced by a PLM. Also, we propose memory dropout to the cross-attention mechanism, which actively encourages the use of latent variables for response generation. Overall, experiments across two commonly used open-domain dialog ",
    "link": "http://arxiv.org/abs/2305.15025",
    "context": "Title: Dior-CVAE: Pre-trained Language Models and Diffusion Priors for Variational Dialog Generation. (arXiv:2305.15025v2 [cs.CL] UPDATED)\nAbstract: Current variational dialog models have employed pre-trained language models (PLMs) to parameterize the likelihood and posterior distributions. However, the Gaussian assumption made on the prior distribution is incompatible with these distributions, thus restricting the diversity of generated responses. These models also suffer from posterior collapse, i.e., the decoder tends to ignore latent variables and directly access information captured in the encoder through the cross-attention mechanism. In this work, we propose Dior-CVAE, a hierarchical conditional variational autoencoder (CVAE) with diffusion priors to address these challenges. We employ a diffusion model to increase the complexity of the prior distribution and its compatibility with the distributions produced by a PLM. Also, we propose memory dropout to the cross-attention mechanism, which actively encourages the use of latent variables for response generation. Overall, experiments across two commonly used open-domain dialog ",
    "path": "papers/23/05/2305.15025.json",
    "total_tokens": 938,
    "translated_title": "Dior-CVAE：预训练语言模型和扩散先验用于变分对话生成",
    "translated_abstract": "当前的变分对话模型采用了预训练语言模型（PLMs）来参数化似然和后验分布。然而，对先验分布的高斯假设与这些分布不兼容，从而限制了生成响应的多样性。这些模型还存在后验崩溃问题，即解码器倾向于通过交叉注意机制直接访问编码器中捕获的信息而忽略潜变量。在这项工作中，我们提出了Dior-CVAE，一种带有扩散先验的分层条件变分自编码器（CVAE）来解决这些挑战。我们采用扩散模型来增加先验分布的复杂性，使其与PLM产生的分布兼容。此外，我们提出了记忆丢弃机制来改进交叉注意机制，积极鼓励使用潜变量进行响应生成。总体上，在两个常用的开放领域对话实验中，我们的模型展现了优越的性能。",
    "tldr": "Dior-CVAE是一种具有扩散先验的分层条件变分自编码器，通过采用预训练语言模型和扩散模型来解决变分对话生成中的多样性和后验崩溃问题。实验表明，该模型在开放领域对话中表现出了优越的性能。",
    "en_tdlr": "Dior-CVAE is a hierarchical conditional variational autoencoder with diffusion priors that addresses the diversity and posterior collapse issues in variational dialog generation by utilizing pre-trained language models and diffusion models. Experimental results show that the model outperforms in open-domain dialogues."
}