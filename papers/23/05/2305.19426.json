{
    "title": "ScoNe: Benchmarking Negation Reasoning in Language Models With Fine-Tuning and In-Context Learning. (arXiv:2305.19426v1 [cs.CL])",
    "abstract": "A number of recent benchmarks seek to assess how well models handle natural language negation. However, these benchmarks lack the controlled example paradigms that would allow us to infer whether a model had learned how negation morphemes semantically scope. To fill these analytical gaps, we present the Scoped Negation NLI (ScoNe-NLI) benchmark, which contains contrast sets of six examples with up to two negations where either zero, one, or both negative morphemes affect the NLI label. We use ScoNe-NLI to assess fine-tuning and in-context learning strategies. We find that RoBERTa and DeBERTa models solve ScoNe-NLI after many shot fine-tuning. For in-context learning, we test InstructGPT models and find that most prompt strategies are not successful, including those using step-by-step reasoning. To better understand this result, we extend ScoNe with ScoNe-NLG, a sentence completion test set that embeds negation reasoning in short narratives. Here, InstructGPT is successful, which reveal",
    "link": "http://arxiv.org/abs/2305.19426",
    "context": "Title: ScoNe: Benchmarking Negation Reasoning in Language Models With Fine-Tuning and In-Context Learning. (arXiv:2305.19426v1 [cs.CL])\nAbstract: A number of recent benchmarks seek to assess how well models handle natural language negation. However, these benchmarks lack the controlled example paradigms that would allow us to infer whether a model had learned how negation morphemes semantically scope. To fill these analytical gaps, we present the Scoped Negation NLI (ScoNe-NLI) benchmark, which contains contrast sets of six examples with up to two negations where either zero, one, or both negative morphemes affect the NLI label. We use ScoNe-NLI to assess fine-tuning and in-context learning strategies. We find that RoBERTa and DeBERTa models solve ScoNe-NLI after many shot fine-tuning. For in-context learning, we test InstructGPT models and find that most prompt strategies are not successful, including those using step-by-step reasoning. To better understand this result, we extend ScoNe with ScoNe-NLG, a sentence completion test set that embeds negation reasoning in short narratives. Here, InstructGPT is successful, which reveal",
    "path": "papers/23/05/2305.19426.json",
    "total_tokens": 1113,
    "translated_title": "ScoNe: 用微调和上下文学习评估语言模型中的否定推理表现的基准测试",
    "translated_abstract": "最近的一些基准测试试图评估模型处理自然语言否定的能力。然而，这些基准测试缺乏受控的示例范例，无法推断模型是否已经学会了否定语素的语义作用。为了填补这些分析上的空白，我们提出了Scoped Negation NLI (ScoNe-NLI)基准测试，其中包含六个对比示例组成的集合，其中包括多达两个否定语素，其中零个、一个或两个否定语素影响NLI标签。我们使用ScoNe-NLI评估微调和上下文学习策略。我们发现RoBERTa和DeBERTa模型在进行许多次微调之后可以解决ScoNe-NLI。在上下文学习方面，我们测试了InstructGPT模型，并发现大多数提示策略都不成功，包括那些使用逐步推理的策略。为了更好地理解这个结果，我们将ScoNe扩展为ScoNe-NLG，这是一个嵌入了否定推理的短故事的句子完成测试集。在这里，InstructGPT是成功的，揭示了...",
    "tldr": "本文提出了Scoped Negation NLI (ScoNe-NLI)基准测试以评估微调和上下文学习策略对语言模型中否定推理表现的影响。研究结果表明，进行许多次微调后，RoBERTa和DeBERTa模型可以成功解决ScoNe-NLI。对于上下文学习方面，大多数提示策略都无法成功，但在嵌入否定推理的短故事的句子完成测试中，InstructGPT是成功的。",
    "en_tdlr": "This paper presents the Scoped Negation NLI (ScoNe-NLI) benchmark to evaluate the performance of language models in negation reasoning with fine-tuning and in-context learning strategies. The study found that RoBERTa and DeBERTa models can successfully solve ScoNe-NLI after many shot fine-tuning, while most prompt strategies for in-context learning, including those using step-by-step reasoning, are not successful. However, InstructGPT is successful in a sentence completion test embedding negation reasoning in short narratives."
}