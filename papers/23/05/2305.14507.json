{
    "title": "Deduction under Perturbed Evidence: Probing Student Simulation Capabilities of Large Language Models. (arXiv:2305.14507v1 [cs.CL])",
    "abstract": "We explore whether Large Language Models (LLMs) are capable of logical reasoning with distorted facts, which we call Deduction under Perturbed Evidence (DUPE). DUPE presents a unique challenge to LLMs since they typically rely on their parameters, which encode mostly accurate information, to reason and make inferences. However, in DUPE, LLMs must reason over manipulated or falsified evidence present in their prompts, which can result in false conclusions that are valid only under the manipulated evidence. Our goal with DUPE is to determine whether LLMs can arrive at these false conclusions and identify whether the dominant factor influencing the deduction process is the encoded data in the parameters or the manipulated evidence in the prompts. To evaluate the DUPE capabilities of LLMs, we create a DUPEd version of the StrategyQA dataset, where facts are manipulated to reverse the answer to the question. Our findings show that even the most advanced GPT models struggle to reason on mani",
    "link": "http://arxiv.org/abs/2305.14507",
    "context": "Title: Deduction under Perturbed Evidence: Probing Student Simulation Capabilities of Large Language Models. (arXiv:2305.14507v1 [cs.CL])\nAbstract: We explore whether Large Language Models (LLMs) are capable of logical reasoning with distorted facts, which we call Deduction under Perturbed Evidence (DUPE). DUPE presents a unique challenge to LLMs since they typically rely on their parameters, which encode mostly accurate information, to reason and make inferences. However, in DUPE, LLMs must reason over manipulated or falsified evidence present in their prompts, which can result in false conclusions that are valid only under the manipulated evidence. Our goal with DUPE is to determine whether LLMs can arrive at these false conclusions and identify whether the dominant factor influencing the deduction process is the encoded data in the parameters or the manipulated evidence in the prompts. To evaluate the DUPE capabilities of LLMs, we create a DUPEd version of the StrategyQA dataset, where facts are manipulated to reverse the answer to the question. Our findings show that even the most advanced GPT models struggle to reason on mani",
    "path": "papers/23/05/2305.14507.json",
    "total_tokens": 911,
    "translated_title": "扰动证据下的推理：探究大型语言模型的学生模拟能力",
    "translated_abstract": "本文探究了大型语言模型（LLM）是否能够处理伴有扭曲事实的逻辑推理，即扰动证据下的推理（DUPE）。由于LLM通常依赖于编码了大部分准确信息的参数进行推理和推断，DUPE对LLM提出了独特的挑战，因为在DUPE中，LLM必须在提示中存在的被操纵或伪造的证据上进行推理，这可能导致仅在被操纵证据下才有效的错误结论。我们的目标是通过DUPE确定LLM是否能够得出这些错误的结论，并确定影响推理过程的主要因素是参数中编码的数据还是提示中的被操纵证据。为了评估LLM的DUPE能力，我们创建了一个DUPE版本的StrategyQA数据集，其中事实被操纵以扭转问题的答案。我们的研究发现，即使是最先进的GPT模型在处理操纵证据下的推理时也存在困难。",
    "tldr": "本文探究了大型语言模型（LLM）是否能够在扰动证据下做出逻辑推理的结论，结果发现即使是最先进的GPT模型在处理操纵证据下的推理时也存在困难。",
    "en_tdlr": "This paper investigates whether Large Language Models (LLMs) are capable of making logical deductions under perturbed evidence, and finds that even the most advanced GPT models struggle with reasoning on manipulated evidence."
}