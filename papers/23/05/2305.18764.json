{
    "title": "When Does Optimizing a Proper Loss Yield Calibration?. (arXiv:2305.18764v1 [cs.LG])",
    "abstract": "Optimizing proper loss functions is popularly believed to yield predictors with good calibration properties; the intuition being that for such losses, the global optimum is to predict the ground-truth probabilities, which is indeed calibrated. However, typical machine learning models are trained to approximately minimize loss over restricted families of predictors, that are unlikely to contain the ground truth. Under what circumstances does optimizing proper loss over a restricted family yield calibrated models? What precise calibration guarantees does it give? In this work, we provide a rigorous answer to these questions. We replace the global optimality with a local optimality condition stipulating that the (proper) loss of the predictor cannot be reduced much by post-processing its predictions with a certain family of Lipschitz functions. We show that any predictor with this local optimality satisfies smooth calibration as defined in Kakade-Foster (2008), B{\\l}asiok et al. (2023). L",
    "link": "http://arxiv.org/abs/2305.18764",
    "context": "Title: When Does Optimizing a Proper Loss Yield Calibration?. (arXiv:2305.18764v1 [cs.LG])\nAbstract: Optimizing proper loss functions is popularly believed to yield predictors with good calibration properties; the intuition being that for such losses, the global optimum is to predict the ground-truth probabilities, which is indeed calibrated. However, typical machine learning models are trained to approximately minimize loss over restricted families of predictors, that are unlikely to contain the ground truth. Under what circumstances does optimizing proper loss over a restricted family yield calibrated models? What precise calibration guarantees does it give? In this work, we provide a rigorous answer to these questions. We replace the global optimality with a local optimality condition stipulating that the (proper) loss of the predictor cannot be reduced much by post-processing its predictions with a certain family of Lipschitz functions. We show that any predictor with this local optimality satisfies smooth calibration as defined in Kakade-Foster (2008), B{\\l}asiok et al. (2023). L",
    "path": "papers/23/05/2305.18764.json",
    "total_tokens": 926,
    "translated_title": "优化适当的损失函数是否能得到校准的预测器？",
    "translated_abstract": "优化适当的损失函数被广泛认为会得到具有良好校准特性的预测器，这是因为对于这样的损失，全局最优解是预测真实概率，这确实是校准的。但是，典型的机器学习模型是训练来近似地最小化在受限制的预测器族中的损失，这些预测器族不太可能包含真实的概率。在什么情况下，优化受限制的预测器族中适当的损失可以得到校准的模型？它提供了什么精确的校准保证？在这项工作中，我们提供了这些问题的严格答案。我们用局部最优条件替换全局最优性条件，该条件规定了预测器（适当的）损失不能通过使用一定族群的Lipschitz函数后处理其预测而降低太多。我们证明了具有这种局部最优性质的任何预测器都满足Kakade-Foster(2008)、Błasiok等人(2023)中定义的平稳校准。",
    "tldr": "研究优化适当的损失函数是否能在受限的预测器族中得到校准的模型，使用局部最优条件取代全局最优性条件并在此基础上进行了严格的证明。",
    "en_tdlr": "This paper investigates whether optimizing proper loss functions in restricted families of predictors can yield calibrated models and provides a rigorous answer by replacing global optimality with a local optimality condition and proving that any predictor with this condition satisfies smooth calibration."
}