{
    "title": "RCOT: Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought. (arXiv:2305.11499v1 [cs.CL])",
    "abstract": "Large language Models (LLMs) have achieved promising performance on arithmetic reasoning tasks by incorporating step-by-step chain-of-thought (CoT) prompting. However, LLMs face challenges in maintaining factual consistency during reasoning, exhibiting tendencies to condition overlooking, question misinterpretation, and condition hallucination over given problems. Existing methods use coarse-grained feedback (e.g., whether the answer is correct) to improve factual consistency. In this work, we propose RCoT (Reversing Chain-of-Thought), a novel method to improve LLMs' reasoning abilities by automatically detecting and rectifying factual inconsistency in LLMs' generated solutions. To detect factual inconsistency, RCoT first asks LLMs to reconstruct the problem based on generated solutions. Then fine-grained comparisons between the original problem and the reconstructed problem expose the factual inconsistency in the original solutions. To rectify the solution, RCoT formulates detected fa",
    "link": "http://arxiv.org/abs/2305.11499",
    "context": "Title: RCOT: Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought. (arXiv:2305.11499v1 [cs.CL])\nAbstract: Large language Models (LLMs) have achieved promising performance on arithmetic reasoning tasks by incorporating step-by-step chain-of-thought (CoT) prompting. However, LLMs face challenges in maintaining factual consistency during reasoning, exhibiting tendencies to condition overlooking, question misinterpretation, and condition hallucination over given problems. Existing methods use coarse-grained feedback (e.g., whether the answer is correct) to improve factual consistency. In this work, we propose RCoT (Reversing Chain-of-Thought), a novel method to improve LLMs' reasoning abilities by automatically detecting and rectifying factual inconsistency in LLMs' generated solutions. To detect factual inconsistency, RCoT first asks LLMs to reconstruct the problem based on generated solutions. Then fine-grained comparisons between the original problem and the reconstructed problem expose the factual inconsistency in the original solutions. To rectify the solution, RCoT formulates detected fa",
    "path": "papers/23/05/2305.11499.json",
    "total_tokens": 863,
    "translated_title": "RCOT：通过反转思维链条检测和纠正推理中的事实不一致性",
    "translated_abstract": "大型语言模型（LLM）通过逐步思维链（CoT）提示在算术推理任务上取得了很好的成绩。然而，LLM在推理过程中面临着维护事实一致性的挑战，表现出在给定问题上确定过度、问题误解和条件幻觉的趋势。现有方法使用粗粒度反馈（例如，答案是否正确）来提高事实一致性。本文提出RCoT（反转CoT），一种新颖的方法，通过自动检测和纠正LLM生成的解决方案中的事实不一致性来提高LLMs的推理能力。为了检测事实不一致性，RCoT首先要求LLMs基于生成的解决方案重构问题。然后，通过对比原始问题和重构问题，较为详细地揭示了原始解决方案中的事实不一致性。为了纠正解决方案，RCoT制定了检测到的fa",
    "tldr": "RCOT 提出了一个新的方法来检测和纠正 LLM 生成解决方案中的事实不一致性，以提高 LLM 推理能力。",
    "en_tdlr": "RCOT proposes a novel method to detect and rectify factual inconsistency in LLMs' generated solutions, aiming to improve LLMs' reasoning abilities."
}