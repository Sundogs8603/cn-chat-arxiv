{
    "title": "Augmenting Passage Representations with Query Generation for Enhanced Cross-Lingual Dense Retrieval. (arXiv:2305.03950v1 [cs.IR])",
    "abstract": "Effective cross-lingual dense retrieval methods that rely on multilingual pre-trained language models (PLMs) need to be trained to encompass both the relevance matching task and the cross-language alignment task. However, cross-lingual data for training is often scarcely available. In this paper, rather than using more cross-lingual data for training, we propose to use cross-lingual query generation to augment passage representations with queries in languages other than the original passage language. These augmented representations are used at inference time so that the representation can encode more information across the different target languages. Training of a cross-lingual query generator does not require additional training data to that used for the dense retriever. The query generator training is also effective because the pre-training task for the generator (T5 text-to-text training) is very similar to the fine-tuning task (generation of a query). The use of the generator does ",
    "link": "http://arxiv.org/abs/2305.03950",
    "context": "Title: Augmenting Passage Representations with Query Generation for Enhanced Cross-Lingual Dense Retrieval. (arXiv:2305.03950v1 [cs.IR])\nAbstract: Effective cross-lingual dense retrieval methods that rely on multilingual pre-trained language models (PLMs) need to be trained to encompass both the relevance matching task and the cross-language alignment task. However, cross-lingual data for training is often scarcely available. In this paper, rather than using more cross-lingual data for training, we propose to use cross-lingual query generation to augment passage representations with queries in languages other than the original passage language. These augmented representations are used at inference time so that the representation can encode more information across the different target languages. Training of a cross-lingual query generator does not require additional training data to that used for the dense retriever. The query generator training is also effective because the pre-training task for the generator (T5 text-to-text training) is very similar to the fine-tuning task (generation of a query). The use of the generator does ",
    "path": "papers/23/05/2305.03950.json",
    "total_tokens": 852,
    "translated_title": "通过查询生成增强通道表示以实现增强跨语言稠密检索",
    "translated_abstract": "有效的跨语言稠密检索方法需要依赖多语言预训练语言模型(PLMs)，同时涵盖相关性匹配和跨语言对齐任务的训练也往往面临数据匮乏的问题。本文提出了一种通过使用跨语言查询生成来增强通道表示的方法，以应对跨语言数据匮乏的问题。通过在原始通道语言之外使用其他语言的查询来增强表示，这些增强表示在推断过程中使用，以便表示可以跨越不同目标语言编码更多信息。跨语言查询生成器的训练不需要额外的训练数据，生成器的预训练任务也非常类似于微调任务(生成查询)。在推断过程中使用生成器不会增加任何计算成本。",
    "tldr": "本文提出了一种跨语言稠密检索方法，通过使用跨语言查询生成方法来增强通道表示，以应对跨语言数据较少的问题。在推断过程中使用生成器不会增加额外的计算成本。",
    "en_tdlr": "This paper proposes a solution to the problem of scarce cross-lingual data for training effective cross-lingual dense retrieval methods by introducing cross-lingual query generation to enhance passage representations. The use of the generator does not introduce any additional computational cost during inference."
}