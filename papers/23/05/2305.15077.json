{
    "title": "Contrastive Learning of Sentence Embeddings from Scratch. (arXiv:2305.15077v2 [cs.CL] UPDATED)",
    "abstract": "Contrastive learning has been the dominant approach to train state-of-the-art sentence embeddings. Previous studies have typically learned sentence embeddings either through the use of human-annotated natural language inference (NLI) data or via large-scale unlabeled sentences in an unsupervised manner. However, even in the case of unlabeled data, their acquisition presents challenges in certain domains due to various reasons. To address these issues, we present SynCSE, a contrastive learning framework that trains sentence embeddings with synthesized data. Specifically, we explore utilizing large language models to synthesize the required data samples for contrastive learning, including (1) producing positive and negative annotations given unlabeled sentences (SynCSE-partial), and (2) generating sentences along with their corresponding annotations from scratch (SynCSE-scratch). Experimental results on sentence similarity and reranking tasks indicate that both SynCSE-partial and SynCSE-",
    "link": "http://arxiv.org/abs/2305.15077",
    "context": "Title: Contrastive Learning of Sentence Embeddings from Scratch. (arXiv:2305.15077v2 [cs.CL] UPDATED)\nAbstract: Contrastive learning has been the dominant approach to train state-of-the-art sentence embeddings. Previous studies have typically learned sentence embeddings either through the use of human-annotated natural language inference (NLI) data or via large-scale unlabeled sentences in an unsupervised manner. However, even in the case of unlabeled data, their acquisition presents challenges in certain domains due to various reasons. To address these issues, we present SynCSE, a contrastive learning framework that trains sentence embeddings with synthesized data. Specifically, we explore utilizing large language models to synthesize the required data samples for contrastive learning, including (1) producing positive and negative annotations given unlabeled sentences (SynCSE-partial), and (2) generating sentences along with their corresponding annotations from scratch (SynCSE-scratch). Experimental results on sentence similarity and reranking tasks indicate that both SynCSE-partial and SynCSE-",
    "path": "papers/23/05/2305.15077.json",
    "total_tokens": 870,
    "translated_title": "从零开始对比学习句子嵌入",
    "translated_abstract": "对比学习一直是训练最先进的句子嵌入的主要方法。先前的研究通常通过使用人工标注的自然语言推理（NLI）数据或通过大规模无标签的句子以无监督的方式来学习句子嵌入。然而，即使在无标签数据的情况下，由于各种原因，在某些领域获取数据样本仍然存在挑战。为了解决这些问题，我们提出了SynCSE，一种对比学习框架，用于使用合成数据训练句子嵌入。具体而言，我们探索利用大型语言模型合成对比学习所需的数据样本，包括（1）产生给定无标签句子的正负标注（SynCSE-partial），以及（2）从零开始生成句子及其相应的标注（SynCSE-scratch）。对句子相似性和重排序任务的实验结果表明，SynCSE-partial和SynCSE-scratch两者都可以取得良好的效果。",
    "tldr": "该论文提出了一种从零开始的对比学习框架，利用合成数据训练句子嵌入。实验结果表明，该方法在句子相似性和重排序任务上取得了良好的效果。",
    "en_tdlr": "The paper presents a contrastive learning framework that trains sentence embeddings from scratch using synthesized data, achieving good results on sentence similarity and reranking tasks."
}