{
    "title": "Modeling the Q-Diversity in a Min-max Play Game for Robust Optimization. (arXiv:2305.12123v1 [cs.CL])",
    "abstract": "Models trained with empirical risk minimization (ERM) are revealed to easily rely on spurious correlations, resulting in poor generalization. Group distributionally robust optimization (group DRO) can alleviate this problem by minimizing the worst-case loss over pre-defined groups. While promising, in practice factors like expensive annotations and privacy preclude the availability of group labels. More crucially, when taking a closer look at the failure modes of out-of-distribution generalization, the typical procedure of reweighting in group DRO loses efficiency. Hinged on the limitations, in this work, we reformulate the group DRO framework by proposing Q-Diversity. Characterized by an interactive training mode, Q-Diversity relaxes the group identification from annotation into direct parameterization. Furthermore, a novel mixing strategy across groups is presented to diversify the under-represented groups. In a series of experiments on both synthetic and real-world text classificati",
    "link": "http://arxiv.org/abs/2305.12123",
    "context": "Title: Modeling the Q-Diversity in a Min-max Play Game for Robust Optimization. (arXiv:2305.12123v1 [cs.CL])\nAbstract: Models trained with empirical risk minimization (ERM) are revealed to easily rely on spurious correlations, resulting in poor generalization. Group distributionally robust optimization (group DRO) can alleviate this problem by minimizing the worst-case loss over pre-defined groups. While promising, in practice factors like expensive annotations and privacy preclude the availability of group labels. More crucially, when taking a closer look at the failure modes of out-of-distribution generalization, the typical procedure of reweighting in group DRO loses efficiency. Hinged on the limitations, in this work, we reformulate the group DRO framework by proposing Q-Diversity. Characterized by an interactive training mode, Q-Diversity relaxes the group identification from annotation into direct parameterization. Furthermore, a novel mixing strategy across groups is presented to diversify the under-represented groups. In a series of experiments on both synthetic and real-world text classificati",
    "path": "papers/23/05/2305.12123.json",
    "total_tokens": 879,
    "translated_title": "Robust Optimization中最小最大博弈中Q-Diversity的建模",
    "translated_abstract": "经验风险最小化(ERM)训练的模型往往过于依赖表面相关性，导致泛化性差。分组分布式鲁棒优化（group DRO）可以通过在预定义组上最小化最坏情况下的损失来缓解这个问题。但在实践中，昂贵的注释和隐私等因素都会阻碍组标签的可用性。更重要的是，当更深入地了解超出分布之外的泛化的失败模式时，“group DRO”中的重新加权典型过程会失去效率。基于这些限制，本文通过提出Q-Diversity重新构建了“group DRO”框架。Q-Diversity采用交互式训练模式，并将组的识别从注释中放宽到直接参数化。此外，还提出了一种新的混合策略，以分散未被充分代表的组。作者在一系列对于合成和真实世界文本分类的实验中测试了该框架的有效性。",
    "tldr": "本文提出了Q-Diversity，与group DRO较好地配合，通过交互式训练模式直接参数化组的识别，从而提高了模型的泛化能力和鲁棒性。",
    "en_tdlr": "This paper proposes Q-Diversity, which works well with group DRO and improves the model's generalization and robustness by directly parameterizing group identification through an interactive training mode."
}