{
    "title": "Is Information Extraction Solved by ChatGPT? An Analysis of Performance, Evaluation Criteria, Robustness and Errors. (arXiv:2305.14450v1 [cs.CL])",
    "abstract": "ChatGPT has stimulated the research boom in the field of large language models. In this paper, we assess the capabilities of ChatGPT from four perspectives including Performance, Evaluation Criteria, Robustness and Error Types. Specifically, we first evaluate ChatGPT's performance on 17 datasets with 14 IE sub-tasks under the zero-shot, few-shot and chain-of-thought scenarios, and find a huge performance gap between ChatGPT and SOTA results. Next, we rethink this gap and propose a soft-matching strategy for evaluation to more accurately reflect ChatGPT's performance. Then, we analyze the robustness of ChatGPT on 14 IE sub-tasks, and find that: 1) ChatGPT rarely outputs invalid responses; 2) Irrelevant context and long-tail target types greatly affect ChatGPT's performance; 3) ChatGPT cannot understand well the subject-object relationships in RE task. Finally, we analyze the errors of ChatGPT, and find that \"unannotated spans\" is the most dominant error type. This raises concerns about ",
    "link": "http://arxiv.org/abs/2305.14450",
    "context": "Title: Is Information Extraction Solved by ChatGPT? An Analysis of Performance, Evaluation Criteria, Robustness and Errors. (arXiv:2305.14450v1 [cs.CL])\nAbstract: ChatGPT has stimulated the research boom in the field of large language models. In this paper, we assess the capabilities of ChatGPT from four perspectives including Performance, Evaluation Criteria, Robustness and Error Types. Specifically, we first evaluate ChatGPT's performance on 17 datasets with 14 IE sub-tasks under the zero-shot, few-shot and chain-of-thought scenarios, and find a huge performance gap between ChatGPT and SOTA results. Next, we rethink this gap and propose a soft-matching strategy for evaluation to more accurately reflect ChatGPT's performance. Then, we analyze the robustness of ChatGPT on 14 IE sub-tasks, and find that: 1) ChatGPT rarely outputs invalid responses; 2) Irrelevant context and long-tail target types greatly affect ChatGPT's performance; 3) ChatGPT cannot understand well the subject-object relationships in RE task. Finally, we analyze the errors of ChatGPT, and find that \"unannotated spans\" is the most dominant error type. This raises concerns about ",
    "path": "papers/23/05/2305.14450.json",
    "total_tokens": 1035,
    "translated_title": "ChatGPT是否解决了信息抽取问题？性能、度量标准、鲁棒性和错误分析",
    "translated_abstract": "ChatGPT激发了大型语言模型领域的研究热潮。本文从性能、度量标准、鲁棒性和错误类型四个方面评估了ChatGPT的能力。具体而言，我们在零样本、小样本和思考串联等场景下对17个数据集的14个IE子任务评估了ChatGPT的性能，并发现ChatGPT与SOTA结果之间存在巨大的性能差距。接下来，我们重新思考这种差距，并提出一种软匹配策略以更准确地反映ChatGPT的性能。然后，我们分析了ChatGPT在14个IE子任务上的鲁棒性，并发现：1）ChatGPT很少输出无效的响应；2）不相关的上下文和长尾目标类型极大地影响了ChatGPT的性能；3）ChatGPT无法很好地理解RE任务中的主客体关系。最后，我们分析了ChatGPT的错误，并发现“未注释的跨度”是最主要的错误类型。这引起了有关现实场景下信息提取性能的担忧。",
    "tldr": "本研究从性能、度量标准、鲁棒性和错误类型四个方面评估ChatGPT的信息抽取能力，发现了ChatGPT与SOTA结果之间存在巨大的性能差距，同时提出了一种软匹配策略以更准确地反映ChatGPT的性能。",
    "en_tdlr": "This paper evaluates ChatGPT's information extraction ability from four perspectives including performance, evaluation criteria, robustness, and error types. The results show a huge performance gap between ChatGPT and SOTA results and propose a soft-matching strategy to more accurately reflect ChatGPT's performance. There are concerns about information extraction performance in real-world scenarios."
}