{
    "title": "Collective Knowledge Graph Completion with Mutual Knowledge Distillation. (arXiv:2305.15895v1 [cs.CL])",
    "abstract": "Knowledge graph completion (KGC), the task of predicting missing information based on the existing relational data inside a knowledge graph (KG), has drawn significant attention in recent years. However, the predictive power of KGC methods is often limited by the completeness of the existing knowledge graphs from different sources and languages. In monolingual and multilingual settings, KGs are potentially complementary to each other. In this paper, we study the problem of multi-KG completion, where we focus on maximizing the collective knowledge from different KGs to alleviate the incompleteness of individual KGs. Specifically, we propose a novel method called CKGC-CKD that uses relation-aware graph convolutional network encoder models on both individual KGs and a large fused KG in which seed alignments between KGs are regarded as edges for message propagation. An additional mutual knowledge distillation mechanism is also employed to maximize the knowledge transfer between the models ",
    "link": "http://arxiv.org/abs/2305.15895",
    "context": "Title: Collective Knowledge Graph Completion with Mutual Knowledge Distillation. (arXiv:2305.15895v1 [cs.CL])\nAbstract: Knowledge graph completion (KGC), the task of predicting missing information based on the existing relational data inside a knowledge graph (KG), has drawn significant attention in recent years. However, the predictive power of KGC methods is often limited by the completeness of the existing knowledge graphs from different sources and languages. In monolingual and multilingual settings, KGs are potentially complementary to each other. In this paper, we study the problem of multi-KG completion, where we focus on maximizing the collective knowledge from different KGs to alleviate the incompleteness of individual KGs. Specifically, we propose a novel method called CKGC-CKD that uses relation-aware graph convolutional network encoder models on both individual KGs and a large fused KG in which seed alignments between KGs are regarded as edges for message propagation. An additional mutual knowledge distillation mechanism is also employed to maximize the knowledge transfer between the models ",
    "path": "papers/23/05/2305.15895.json",
    "total_tokens": 934,
    "translated_title": "带互相知识蒸馏的集体知识图谱补全",
    "translated_abstract": "知识图谱补全是根据知识图谱内的现有关系数据预测丢失信息的任务，近年来受到了重视。然而，不同来源和语言的知识图谱之间的完整性常常限制了KGC方法的预测能力。在单语和多语环境中，知识图谱潜在地互补。本文研究了多知识图谱补全问题，重点是为了增强个体知识图谱的不完整性而最大化来自不同知识图谱的集体知识。具体而言，我们提出了一种新方法，称为CKGC-CKD，在个体知识图谱和一个大型聚合知识图谱上使用关系感知图卷积神经网络编码器模型，其中KG之间的种子对齐被视为消息传递的边缘。我们还采用了一种额外的互相知识蒸馏机制，以最大化模型之间的知识传递。",
    "tldr": "本文提出一种集体知识图谱补全方法，通过在一个大型聚合知识图谱上使用关系感知图卷积神经网络编码器模型来最大化不同知识图谱的集体知识，并采用互相知识蒸馏机制来增强该方法的效果。",
    "en_tdlr": "The paper proposes a Collective Knowledge Graph Completion method that maximizes collective knowledge from different knowledge graphs and employs mutual knowledge distillation mechanism to enhance the performance. The method uses relation-aware graph convolutional network encoder models on individual knowledge graphs and a large fused knowledge graph, treating seed alignments between the graphs as edges for message propagation."
}