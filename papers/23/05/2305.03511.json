{
    "title": "Shared Latent Space by Both Languages in Non-Autoregressive Neural Machine Translation. (arXiv:2305.03511v1 [cs.CL])",
    "abstract": "Latent variable modeling in non-autoregressive neural machine translation (NAT) is a promising approach to mitigate the multimodality problem. In the previous works, they added an auxiliary model to estimate the posterior distribution of the latent variable conditioned on the source and target sentences. However, it causes several disadvantages, such as redundant information extraction in the latent variable, increasing parameters, and a tendency to ignore a part of the information from the inputs. In this paper, we propose a new latent variable modeling that is based on a dual reconstruction perspective and an advanced hierarchical latent modeling approach. Our proposed method, {\\em LadderNMT}, shares a latent space across both languages so that it hypothetically alleviates or solves the above disadvantages. Experimental results quantitatively and qualitatively demonstrate that our proposed latent variable modeling learns an advantageous latent space and significantly improves transla",
    "link": "http://arxiv.org/abs/2305.03511",
    "context": "Title: Shared Latent Space by Both Languages in Non-Autoregressive Neural Machine Translation. (arXiv:2305.03511v1 [cs.CL])\nAbstract: Latent variable modeling in non-autoregressive neural machine translation (NAT) is a promising approach to mitigate the multimodality problem. In the previous works, they added an auxiliary model to estimate the posterior distribution of the latent variable conditioned on the source and target sentences. However, it causes several disadvantages, such as redundant information extraction in the latent variable, increasing parameters, and a tendency to ignore a part of the information from the inputs. In this paper, we propose a new latent variable modeling that is based on a dual reconstruction perspective and an advanced hierarchical latent modeling approach. Our proposed method, {\\em LadderNMT}, shares a latent space across both languages so that it hypothetically alleviates or solves the above disadvantages. Experimental results quantitatively and qualitatively demonstrate that our proposed latent variable modeling learns an advantageous latent space and significantly improves transla",
    "path": "papers/23/05/2305.03511.json",
    "total_tokens": 933,
    "translated_abstract": "非自回归神经机器翻译中潜变量建模是缓解多模态问题的一个有前途的方法。以前的研究中，他们添加了一个辅助模型来估计潜变量在源语言和目标语言句子条件下的后验分布。然而，这导致了几个缺点，如潜变量中提取冗余信息，增加参数，以及往往忽视输入的一部分信息。本文提出了一种基于双重重建视角和先进的分层潜变量建模方法的新潜变量建模方法{\\em LadderNMT}。我们的方法跨越了两种语言之间的潜在空间，从而假设它缓解或解决了上述缺点。实验结果定量和定性地证明我们提出的潜变量建模学习了有利的潜在空间，并显著提高了翻译效果。",
    "tldr": "本文提出了一种新的非自回归神经机器翻译的潜变量建模方法，名为{\\em LadderNMT}，通过跨越两种语言之间的潜在空间来缓解或解决潜变量建模中冗余信息提取、参数增加和忽视输入信息的问题。",
    "en_tdlr": "This paper proposes a new latent variable modeling method for non-autoregressive neural machine translation, named {\\em LadderNMT}, which shares a latent space across both languages to alleviate or solve the problems of redundant information extraction, parameter increase, and ignoring part of the input information in previous approaches. Experimentally, it is demonstrated that our proposed method learns an advantageous latent space and significantly improves translation performance."
}