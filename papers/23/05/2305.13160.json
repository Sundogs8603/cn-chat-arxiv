{
    "title": "Can ChatGPT Defend its Belief in Truth? Evaluating LLM Reasoning via Debate. (arXiv:2305.13160v2 [cs.CL] UPDATED)",
    "abstract": "Large language models (LLMs) such as ChatGPT and GPT-4 have shown impressive performance in complex reasoning tasks. However, it is difficult to know whether the models are reasoning based on deep understandings of truth and logic, or leveraging their memorized patterns in a relatively superficial way. In this work, we explore testing LLMs' reasoning by engaging with them in a debate-like conversation, where given a question, the LLM and the user need to discuss to make the correct decision starting from opposing arguments. Upon mitigating the Clever Hans effect, our task requires the LLM to not only achieve the correct answer on its own, but also be able to hold and defend its belief instead of blindly believing or getting misled by the user's (invalid) arguments and critiques, thus testing in greater depth whether the LLM grasps the essence of the reasoning required to solve the problem. Across a range of complex reasoning benchmarks spanning math, commonsense, logic and BIG-Bench ta",
    "link": "http://arxiv.org/abs/2305.13160",
    "context": "Title: Can ChatGPT Defend its Belief in Truth? Evaluating LLM Reasoning via Debate. (arXiv:2305.13160v2 [cs.CL] UPDATED)\nAbstract: Large language models (LLMs) such as ChatGPT and GPT-4 have shown impressive performance in complex reasoning tasks. However, it is difficult to know whether the models are reasoning based on deep understandings of truth and logic, or leveraging their memorized patterns in a relatively superficial way. In this work, we explore testing LLMs' reasoning by engaging with them in a debate-like conversation, where given a question, the LLM and the user need to discuss to make the correct decision starting from opposing arguments. Upon mitigating the Clever Hans effect, our task requires the LLM to not only achieve the correct answer on its own, but also be able to hold and defend its belief instead of blindly believing or getting misled by the user's (invalid) arguments and critiques, thus testing in greater depth whether the LLM grasps the essence of the reasoning required to solve the problem. Across a range of complex reasoning benchmarks spanning math, commonsense, logic and BIG-Bench ta",
    "path": "papers/23/05/2305.13160.json",
    "total_tokens": 937,
    "translated_title": "能ChatGPT保持对真相的信念吗？通过辩论评估LLM的推理能力。",
    "translated_abstract": "ChatGPT和GPT-4等大型语言模型在复杂推理任务中表现出色。然而，我们很难确定这些模型是基于对真相和逻辑的深入理解进行推理，还是仅仅利用其记忆的模式进行相对肤浅的推理。在本研究中，我们通过与LLM进行类似辩论的对话来测试其推理能力。在给定一个问题的情况下，LLM和用户需要讨论并从相对立的观点出发做出正确的决策。通过消除Clever Hans效应，我们的任务要求LLM不仅能够独立得出正确答案，还要能够坚持并捍卫自己的信念，而不是盲目地相信用户的（无效的）论证和批评，从而更深入地测试LLM是否掌握了解决问题所需的推理要点。我们在涵盖了数学、常识、逻辑和BIG-Bench等多个复杂推理基准测试中进行了实验。",
    "tldr": "本研究通过辩论式对话测试了LLM的推理能力，要求LLM在讨论过程中不仅能够得出正确答案，还能够坚持并捍卫自己的信念，从而更深入地评估其对问题的推理理解能力。",
    "en_tdlr": "This study evaluated the reasoning ability of LLMs through debate-like conversations, assessing whether the models can not only provide the correct answers but also hold and defend their beliefs, thus testing their deeper understanding of reasoning."
}