{
    "title": "Red Teaming Language Model Detectors with Language Models. (arXiv:2305.19713v1 [cs.CL])",
    "abstract": "The prevalence and high capacity of large language models (LLMs) present significant safety and ethical risks when malicious users exploit them for automated content generation. To prevent the potentially deceptive usage of LLMs, recent works have proposed several algorithms to detect machine-generated text. In this paper, we systematically test the reliability of the existing detectors, by designing two types of attack strategies to fool the detectors: 1) replacing words with their synonyms based on the context; 2) altering the writing style of generated text. These strategies are implemented by instructing LLMs to generate synonymous word substitutions or writing directives that modify the style without human involvement, and the LLMs leveraged in the attack can also be protected by detectors. Our research reveals that our attacks effectively compromise the performance of all tested detectors, thereby underscoring the urgent need for the development of more robust machine-generated t",
    "link": "http://arxiv.org/abs/2305.19713",
    "context": "Title: Red Teaming Language Model Detectors with Language Models. (arXiv:2305.19713v1 [cs.CL])\nAbstract: The prevalence and high capacity of large language models (LLMs) present significant safety and ethical risks when malicious users exploit them for automated content generation. To prevent the potentially deceptive usage of LLMs, recent works have proposed several algorithms to detect machine-generated text. In this paper, we systematically test the reliability of the existing detectors, by designing two types of attack strategies to fool the detectors: 1) replacing words with their synonyms based on the context; 2) altering the writing style of generated text. These strategies are implemented by instructing LLMs to generate synonymous word substitutions or writing directives that modify the style without human involvement, and the LLMs leveraged in the attack can also be protected by detectors. Our research reveals that our attacks effectively compromise the performance of all tested detectors, thereby underscoring the urgent need for the development of more robust machine-generated t",
    "path": "papers/23/05/2305.19713.json",
    "total_tokens": 877,
    "translated_abstract": "大型语言模型（LLMs）的普及和高容量给恶意用户利用其进行自动化内容生成带来了显着的安全和道德风险。为了防止潜在的欺骗性使用LLMs，近期的研究提出了几种检测机器生成文本的算法。本文通过设计两种类型的攻击策略来测试现有检测器的可靠性：1）基于上下文将单词替换为其同义词；2）改变生成文本的写作风格。这些策略通过指示LLMs生成同义词替换或修改风格的写作指令来实现，而攻击中使用的LLMs也可以通过检测器进行保护。我们的研究揭示了我们的攻击有效地破坏了所有测试检测器的性能，因此强调了开发更健壮的机器生成文本检测器的迫切性。",
    "tldr": "本文测试了几种检测器的可靠性并发现攻击者可以使用LLMs生成具有同义词或不同写作风格的文本来欺骗现有的检测器。这突显了开发更强大的机器生成文本检测器的迫切性。",
    "en_tdlr": "This paper tests the reliability of several detectors designed to detect machine-generated text and finds that attackers can use LLMs to generate text with synonyms or different writing styles to deceive existing detectors. This underscores the urgent need for the development of more robust machine-generated text detectors."
}