{
    "title": "Efficient Document Embeddings via Self-Contrastive Bregman Divergence Learning",
    "abstract": "arXiv:2305.16031v2 Announce Type: replace  Abstract: Learning quality document embeddings is a fundamental problem in natural language processing (NLP), information retrieval (IR), recommendation systems, and search engines. Despite recent advances in the development of transformer-based models that produce sentence embeddings with self-contrastive learning, the encoding of long documents (Ks of words) is still challenging with respect to both efficiency and quality considerations. Therefore, we train Longfomer-based document encoders using a state-of-the-art unsupervised contrastive learning method (SimCSE). Further on, we complement the baseline method -- siamese neural network -- with additional convex neural networks based on functional Bregman divergence aiming to enhance the quality of the output document representations. We show that overall the combination of a self-contrastive siamese network and our proposed neural Bregman network outperforms the baselines in two linear class",
    "link": "https://arxiv.org/abs/2305.16031",
    "context": "Title: Efficient Document Embeddings via Self-Contrastive Bregman Divergence Learning\nAbstract: arXiv:2305.16031v2 Announce Type: replace  Abstract: Learning quality document embeddings is a fundamental problem in natural language processing (NLP), information retrieval (IR), recommendation systems, and search engines. Despite recent advances in the development of transformer-based models that produce sentence embeddings with self-contrastive learning, the encoding of long documents (Ks of words) is still challenging with respect to both efficiency and quality considerations. Therefore, we train Longfomer-based document encoders using a state-of-the-art unsupervised contrastive learning method (SimCSE). Further on, we complement the baseline method -- siamese neural network -- with additional convex neural networks based on functional Bregman divergence aiming to enhance the quality of the output document representations. We show that overall the combination of a self-contrastive siamese network and our proposed neural Bregman network outperforms the baselines in two linear class",
    "path": "papers/23/05/2305.16031.json",
    "total_tokens": 767,
    "translated_title": "通过自对比布雷格曼散度学习实现高效的文档嵌入",
    "translated_abstract": "学习高质量的文档嵌入是自然语言处理（NLP）、信息检索（IR）、推荐系统和搜索引擎中的一个基础问题。尽管最近基于Transformer模型的句子嵌入具有自对比学习，但是对长文档进行编码在效率和质量方面仍具有挑战性。因此，我们使用SimCSE方法训练基于Longformer的文档编码器。此外，我们在基线方法--连体神经网络--上增加了基于泛凸布雷格曼散度的额外凸神经网络，旨在提高输出文档表示的质量。我们展示了自对比连体网络和我们提出的神经布雷格曼网络的组合总体上优于两个线性分类基准模型。",
    "tldr": "通过引入自对比连体网络和神经布雷格曼网络，提高了文档嵌入的质量和效率。",
    "en_tdlr": "Improved the quality and efficiency of document embeddings by introducing self-contrastive siamese network and neural Bregman network."
}