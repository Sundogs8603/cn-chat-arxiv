{
    "title": "What does the Failure to Reason with \"Respectively\" in Zero/Few-Shot Settings Tell Us about Language Models?. (arXiv:2305.19597v1 [cs.CL])",
    "abstract": "Humans can effortlessly understand the coordinate structure of sentences such as \"Niels Bohr and Kurt Cobain were born in Copenhagen and Seattle, respectively\". In the context of natural language inference (NLI), we examine how language models (LMs) reason with respective readings (Gawron and Kehler, 2004) from two perspectives: syntactic-semantic and commonsense-world knowledge. We propose a controlled synthetic dataset WikiResNLI and a naturally occurring dataset NatResNLI to encompass various explicit and implicit realizations of \"respectively\". We show that fine-tuned NLI models struggle with understanding such readings without explicit supervision. While few-shot learning is easy in the presence of explicit cues, longer training is required when the reading is evoked implicitly, leaving models to rely on common sense inferences. Furthermore, our fine-grained analysis indicates models fail to generalize across different constructions. To conclude, we demonstrate that LMs still lag ",
    "link": "http://arxiv.org/abs/2305.19597",
    "context": "Title: What does the Failure to Reason with \"Respectively\" in Zero/Few-Shot Settings Tell Us about Language Models?. (arXiv:2305.19597v1 [cs.CL])\nAbstract: Humans can effortlessly understand the coordinate structure of sentences such as \"Niels Bohr and Kurt Cobain were born in Copenhagen and Seattle, respectively\". In the context of natural language inference (NLI), we examine how language models (LMs) reason with respective readings (Gawron and Kehler, 2004) from two perspectives: syntactic-semantic and commonsense-world knowledge. We propose a controlled synthetic dataset WikiResNLI and a naturally occurring dataset NatResNLI to encompass various explicit and implicit realizations of \"respectively\". We show that fine-tuned NLI models struggle with understanding such readings without explicit supervision. While few-shot learning is easy in the presence of explicit cues, longer training is required when the reading is evoked implicitly, leaving models to rely on common sense inferences. Furthermore, our fine-grained analysis indicates models fail to generalize across different constructions. To conclude, we demonstrate that LMs still lag ",
    "path": "papers/23/05/2305.19597.json",
    "total_tokens": 959,
    "translated_title": "语言模型在零/少样本环境下无法正确理解“respectively”的原因研究",
    "translated_abstract": "人类可以轻松理解“Niels Bohr and Kurt Cobain were born in Copenhagen and Seattle, respectively”这种句子的协作结构。在自然语言推断（NLI）的背景下，本文从句法-语义和常识-世界知识的两个角度研究了语言模型（LMs）如何理解两个读数（Gawron和Kehler，2004） 。我们提出了一个受控合成数据集WikiResNLI 和一个自然数据集 NatResNLI，以包含各种显式和隐式实现的“respectively”。我们发现，微调后的NLI模型在没有显式监督的情况下难以理解这样的读数。当存在显式提示时，零/少样本学习很容易，而当该读数隐含时，则需要更长的训练时间，以依赖常识推理。此外，我们的细致分析表明，模型无法在不同结构之间进行泛化。总之，我们证明了在零/少样本环境下，语言模型在理解“respectively”的各种读法方面仍落后于人类。",
    "tldr": "本文研究发现，语言模型在零/少样本环境下难以理解“respectively”的各种读法，需要更长时间的训练和依赖常识推理，仍落后于人类。",
    "en_tdlr": "This paper finds that language models struggle to understand various readings of \"respectively\" in zero/few-shot settings and require longer training and reliance on common sense reasoning, indicating they are still lagging behind humans."
}