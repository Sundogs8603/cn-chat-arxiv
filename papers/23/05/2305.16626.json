{
    "title": "Evaluation of Question Generation Needs More References. (arXiv:2305.16626v1 [cs.CL])",
    "abstract": "Question generation (QG) is the task of generating a valid and fluent question based on a given context and the target answer. According to various purposes, even given the same context, instructors can ask questions about different concepts, and even the same concept can be written in different ways. However, the evaluation for QG usually depends on single reference-based similarity metrics, such as n-gram-based metric or learned metric, which is not sufficient to fully evaluate the potential of QG methods. To this end, we propose to paraphrase the reference question for a more robust QG evaluation. Using large language models such as GPT-3, we created semantically and syntactically diverse questions, then adopt the simple aggregation of the popular evaluation metrics as the final scores. Through our experiments, we found that using multiple (pseudo) references is more effective for QG evaluation while showing a higher correlation with human evaluations than evaluation with a single r",
    "link": "http://arxiv.org/abs/2305.16626",
    "context": "Title: Evaluation of Question Generation Needs More References. (arXiv:2305.16626v1 [cs.CL])\nAbstract: Question generation (QG) is the task of generating a valid and fluent question based on a given context and the target answer. According to various purposes, even given the same context, instructors can ask questions about different concepts, and even the same concept can be written in different ways. However, the evaluation for QG usually depends on single reference-based similarity metrics, such as n-gram-based metric or learned metric, which is not sufficient to fully evaluate the potential of QG methods. To this end, we propose to paraphrase the reference question for a more robust QG evaluation. Using large language models such as GPT-3, we created semantically and syntactically diverse questions, then adopt the simple aggregation of the popular evaluation metrics as the final scores. Through our experiments, we found that using multiple (pseudo) references is more effective for QG evaluation while showing a higher correlation with human evaluations than evaluation with a single r",
    "path": "papers/23/05/2305.16626.json",
    "total_tokens": 880,
    "translated_title": "评估问答生成需要更多的参考文献",
    "translated_abstract": "问答生成(QG)是一项任务，基于给定的上下文和目标答案生成一个有效和流畅的问题。根据不同的目的，即使给定相同的上下文，教师也可以提出关于不同概念的问题，甚至相同的概念也可以用不同的方式书写。然而，对于QG的评估通常依赖于单个基于参考的相似性度量，例如n-gram度量或学习度量，这不足以充分评估QG方法的潜力。为此，我们建议重新表述参考问题，以进行更强健的QG评估。使用大型语言模型，如GPT-3，我们创建了语义和句法多样的问题，然后采用流行的评估指标的简单聚合作为最终得分。通过我们的实验，我们发现使用多个（伪）参考文献对于QG评估更有效，同时与人类评估的相关性更高，而单个参考的评估则相对较低。",
    "tldr": "评估QG方法需要更多的参考文献来提高其有效性，单个参考不足以全面评估其潜力。使用多个参考文献的评估方法可以更好地与人类评估相关联。",
    "en_tdlr": "Evaluating QG methods requires more references to enhance its effectiveness, as a single reference is not sufficient to fully evaluate its potential. The evaluation method using multiple references shows a higher correlation with human evaluation."
}