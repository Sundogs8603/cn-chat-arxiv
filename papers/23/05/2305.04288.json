{
    "title": "Towards Achieving Near-optimal Utility for Privacy-Preserving Federated Learning via Data Generation and Parameter Distortion. (arXiv:2305.04288v2 [cs.LG] UPDATED)",
    "abstract": "Federated learning (FL) enables participating parties to collaboratively build a global model with boosted utility without disclosing private data information. Appropriate protection mechanisms have to be adopted to fulfill the requirements in preserving \\textit{privacy} and maintaining high model \\textit{utility}. The nature of the widely-adopted protection mechanisms including \\textit{Randomization Mechanism} and \\textit{Compression Mechanism} is to protect privacy via distorting model parameter. We measure the utility via the gap between the original model parameter and the distorted model parameter. We want to identify under what general conditions privacy-preserving federated learning can achieve near-optimal utility via data generation and parameter distortion. To provide an avenue for achieving near-optimal utility, we present an upper bound for utility loss, which is measured using two main terms called variance-reduction and model parameter discrepancy separately. Our analysis",
    "link": "http://arxiv.org/abs/2305.04288",
    "context": "Title: Towards Achieving Near-optimal Utility for Privacy-Preserving Federated Learning via Data Generation and Parameter Distortion. (arXiv:2305.04288v2 [cs.LG] UPDATED)\nAbstract: Federated learning (FL) enables participating parties to collaboratively build a global model with boosted utility without disclosing private data information. Appropriate protection mechanisms have to be adopted to fulfill the requirements in preserving \\textit{privacy} and maintaining high model \\textit{utility}. The nature of the widely-adopted protection mechanisms including \\textit{Randomization Mechanism} and \\textit{Compression Mechanism} is to protect privacy via distorting model parameter. We measure the utility via the gap between the original model parameter and the distorted model parameter. We want to identify under what general conditions privacy-preserving federated learning can achieve near-optimal utility via data generation and parameter distortion. To provide an avenue for achieving near-optimal utility, we present an upper bound for utility loss, which is measured using two main terms called variance-reduction and model parameter discrepancy separately. Our analysis",
    "path": "papers/23/05/2305.04288.json",
    "total_tokens": 862,
    "translated_title": "通过数据生成和参数畸变实现隐私保护联邦学习的接近最优效用",
    "translated_abstract": "联邦学习（FL）使参与方能够协作构建具有提高效用的全局模型，而不泄露私有数据信息。必须采用适当的保护机制来满足保护隐私和维护高模型效用的要求。目前采用的保护机制的本质，包括“随机化机制”和“压缩机制”，是通过畸变模型参数来保护隐私。我们通过原始模型参数和畸变模型参数之间的差距来衡量效用。我们想要确定在什么普遍条件下，通过数据生成和参数畸变，隐私保护的联邦学习可以实现接近最优的效用。为了提供接近最优效用的途径，我们提出了一个效用损失的上限，用两个主要项称为降低方差和模型参数差异来衡量。",
    "tldr": "本论文提出了一种用数据生成和参数畸变实现隐私保护联邦学习接近最优效用的上限方法，其中通过降低方差和模型参数差异来衡量效用损失。",
    "en_tdlr": "This paper proposes an upper bound method for achieving near-optimal utility in privacy-preserving federated learning via data generation and parameter distortion and measures utility loss through variance-reduction and model parameter discrepancy."
}