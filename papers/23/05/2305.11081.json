{
    "title": "Contrastive State Augmentations for Reinforcement Learning-Based Recommender Systems. (arXiv:2305.11081v1 [cs.IR])",
    "abstract": "Learning reinforcement learning (RL)-based recommenders from historical user-item interaction sequences is vital to generate high-reward recommendations and improve long-term cumulative benefits. However, existing RL recommendation methods encounter difficulties (i) to estimate the value functions for states which are not contained in the offline training data, and (ii) to learn effective state representations from user implicit feedback due to the lack of contrastive signals. In this work, we propose contrastive state augmentations (CSA) for the training of RL-based recommender systems. To tackle the first issue, we propose four state augmentation strategies to enlarge the state space of the offline data. The proposed method improves the generalization capability of the recommender by making the RL agent visit the local state regions and ensuring the learned value functions are similar between the original and augmented states. For the second issue, we propose introducing contrastive ",
    "link": "http://arxiv.org/abs/2305.11081",
    "context": "Title: Contrastive State Augmentations for Reinforcement Learning-Based Recommender Systems. (arXiv:2305.11081v1 [cs.IR])\nAbstract: Learning reinforcement learning (RL)-based recommenders from historical user-item interaction sequences is vital to generate high-reward recommendations and improve long-term cumulative benefits. However, existing RL recommendation methods encounter difficulties (i) to estimate the value functions for states which are not contained in the offline training data, and (ii) to learn effective state representations from user implicit feedback due to the lack of contrastive signals. In this work, we propose contrastive state augmentations (CSA) for the training of RL-based recommender systems. To tackle the first issue, we propose four state augmentation strategies to enlarge the state space of the offline data. The proposed method improves the generalization capability of the recommender by making the RL agent visit the local state regions and ensuring the learned value functions are similar between the original and augmented states. For the second issue, we propose introducing contrastive ",
    "path": "papers/23/05/2305.11081.json",
    "total_tokens": 1030,
    "translated_title": "基于强化学习的推荐系统的对比状态增强",
    "translated_abstract": "从历史用户-项目交互序列中学习基于强化学习（RL）的推荐器对于生成高回报建议和改善长期累积效益至关重要。然而，现有的RL推荐方法遇到以下困难：（i）为不包含在离线训练数据中的状态估计价值函数；以及（ii）由于缺乏对比信号，从用户隐式反馈中学习有效的状态表示。在这项工作中，我们提出了对比状态增强（CSA）来训练基于RL的推荐系统。为了解决第一个问题，我们提出了四种状态增强策略来扩大离线数据的状态空间。所提出的方法通过使RL代理访问本地状态区域并确保原始和增强状态之间学习的价值函数相似，提高了推荐器的泛化能力。为了解决第二个问题，我们提出了引入对比状态表示学习，通过最大化正样本相似性和最小化负样本相似性，使代理人学习到信息丰富的状态表示。实验结果表明，我们提出的方法在两个基准数据集上优于几种最先进的RL推荐器。",
    "tldr": "本论文提出了对比状态增强方法来训练基于强化学习的推荐系统，其中包括四种状态增强策略和对比状态表示学习，可以解决现有RL推荐方法的问题。实验结果表明该方法胜过其他最先进的RL推荐器。",
    "en_tdlr": "This paper proposes contrastive state augmentations for training RL-based recommender systems, which include four state augmentation strategies and contrastive state representation learning to address the issues faced by existing RL recommendation methods. Experimental results show that this method outperforms several state-of-the-art RL recommenders on two benchmark datasets."
}