{
    "title": "EmotionGesture: Audio-Driven Diverse Emotional Co-Speech 3D Gesture Generation. (arXiv:2305.18891v2 [cs.CV] UPDATED)",
    "abstract": "Generating vivid and diverse 3D co-speech gestures is crucial for various applications in animating virtual avatars. While most existing methods can generate gestures from audio directly, they usually overlook that emotion is one of the key factors of authentic co-speech gesture generation. In this work, we propose EmotionGesture, a novel framework for synthesizing vivid and diverse emotional co-speech 3D gestures from audio. Considering emotion is often entangled with the rhythmic beat in speech audio, we first develop an Emotion-Beat Mining module (EBM) to extract the emotion and audio beat features as well as model their correlation via a transcript-based visual-rhythm alignment. Then, we propose an initial pose based Spatial-Temporal Prompter (STP) to generate future gestures from the given initial poses. STP effectively models the spatial-temporal correlations between the initial poses and the future gestures, thus producing the spatial-temporal coherent pose prompt. Once we obtai",
    "link": "http://arxiv.org/abs/2305.18891",
    "context": "Title: EmotionGesture: Audio-Driven Diverse Emotional Co-Speech 3D Gesture Generation. (arXiv:2305.18891v2 [cs.CV] UPDATED)\nAbstract: Generating vivid and diverse 3D co-speech gestures is crucial for various applications in animating virtual avatars. While most existing methods can generate gestures from audio directly, they usually overlook that emotion is one of the key factors of authentic co-speech gesture generation. In this work, we propose EmotionGesture, a novel framework for synthesizing vivid and diverse emotional co-speech 3D gestures from audio. Considering emotion is often entangled with the rhythmic beat in speech audio, we first develop an Emotion-Beat Mining module (EBM) to extract the emotion and audio beat features as well as model their correlation via a transcript-based visual-rhythm alignment. Then, we propose an initial pose based Spatial-Temporal Prompter (STP) to generate future gestures from the given initial poses. STP effectively models the spatial-temporal correlations between the initial poses and the future gestures, thus producing the spatial-temporal coherent pose prompt. Once we obtai",
    "path": "papers/23/05/2305.18891.json",
    "total_tokens": 972,
    "translated_title": "EmotionGesture：音频驱动的多样化情感共语3D手势生成",
    "translated_abstract": "生成生动多样的3D共语手势对于给虚拟角色注入生气至关重要。尽管大多数现有方法可以直接从音频中生成手势，但它们通常忽视情感是真实共语手势生成的关键因素之一。在这项工作中，我们提出了EmotionGesture，一种从音频中合成生动多样的情感共语3D手势的新框架。考虑到情感常常与语音节奏交织在一起，我们首先开发了一个情感-节奏挖掘模块（EBM），通过基于转录的视觉-节奏对齐提取情感和音频节奏特征，并建模它们之间的关联。然后，我们提出了一个基于初始姿势的空间-时间提示器（STP），从给定的初始姿势生成未来的手势。STP有效地建模了初始姿势和未来手势之间的空间-时间关系，从而产生空间-时间一致的姿势提示。",
    "tldr": "本文提出了EmotionGesture框架，可以从音频中生成生动多样的情感共语3D手势。通过情感-节奏挖掘模块提取情感和音频节奏特征，并建模它们之间的关联；然后使用空间-时间提示器从给定的初始姿势生成未来的手势，实现空间-时间一致的姿势提示。",
    "en_tdlr": "This paper proposes the EmotionGesture framework for synthesizing vivid and diverse emotional co-speech 3D gestures from audio. It extracts emotion and audio beat features and models their correlation, and uses a spatial-temporal prompter to generate future gestures from given initial poses, achieving spatial-temporal coherence in gesture prompts."
}