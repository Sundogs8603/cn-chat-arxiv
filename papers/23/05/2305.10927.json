{
    "title": "Causal Document-Grounded Dialogue Pre-training. (arXiv:2305.10927v1 [cs.CL])",
    "abstract": "The goal of document-grounded dialogue (DocGD) is to generate a response by grounding the evidence in a supporting document in accordance with the dialogue context. This process involves four variables that are causally connected. Recently, task-specific pre-training has greatly boosted performances on many downstream tasks. Existing DocGD methods, however, continue to rely on general pre-trained language models without a specifically tailored pre-training approach that explicitly captures the causal relationships. To tackle this issue, we are the first to present a causally-complete dataset construction strategy for building million-level DocGD pre-training corpora. To better capture causality, we further propose a causally-perturbed pre-training strategy, which introduces causal perturbations on the variables and optimizes the overall causal effect. Experiments on three benchmark datasets demonstrate that our causal pre-training achieves considerable and consistent improvements under",
    "link": "http://arxiv.org/abs/2305.10927",
    "context": "Title: Causal Document-Grounded Dialogue Pre-training. (arXiv:2305.10927v1 [cs.CL])\nAbstract: The goal of document-grounded dialogue (DocGD) is to generate a response by grounding the evidence in a supporting document in accordance with the dialogue context. This process involves four variables that are causally connected. Recently, task-specific pre-training has greatly boosted performances on many downstream tasks. Existing DocGD methods, however, continue to rely on general pre-trained language models without a specifically tailored pre-training approach that explicitly captures the causal relationships. To tackle this issue, we are the first to present a causally-complete dataset construction strategy for building million-level DocGD pre-training corpora. To better capture causality, we further propose a causally-perturbed pre-training strategy, which introduces causal perturbations on the variables and optimizes the overall causal effect. Experiments on three benchmark datasets demonstrate that our causal pre-training achieves considerable and consistent improvements under",
    "path": "papers/23/05/2305.10927.json",
    "total_tokens": 861,
    "tldr": "该论文提出了一种因果关系的文件驱动对话预训练方法，通过构建因果完整的数据集以及因果扰动的预训练策略，取得了三个基准数据集上的显著且一致的性能提升。",
    "en_tdlr": "This paper proposes a causal document-grounded dialogue pre-training method that achieves considerable and consistent performance improvements on three benchmark datasets by constructing a causally-complete dataset and introducing a causally-perturbed pre-training strategy."
}