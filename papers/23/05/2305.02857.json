{
    "title": "Maximum Causal Entropy Inverse Constrained Reinforcement Learning. (arXiv:2305.02857v1 [cs.LG])",
    "abstract": "When deploying artificial agents in real-world environments where they interact with humans, it is crucial that their behavior is aligned with the values, social norms or other requirements of that environment. However, many environments have implicit constraints that are difficult to specify and transfer to a learning agent. To address this challenge, we propose a novel method that utilizes the principle of maximum causal entropy to learn constraints and an optimal policy that adheres to these constraints, using demonstrations of agents that abide by the constraints. We prove convergence in a tabular setting and provide an approximation which scales to complex environments. We evaluate the effectiveness of the learned policy by assessing the reward received and the number of constraint violations, and we evaluate the learned cost function based on its transferability to other agents. Our method has been shown to outperform state-of-the-art approaches across a variety of tasks and envi",
    "link": "http://arxiv.org/abs/2305.02857",
    "context": "Title: Maximum Causal Entropy Inverse Constrained Reinforcement Learning. (arXiv:2305.02857v1 [cs.LG])\nAbstract: When deploying artificial agents in real-world environments where they interact with humans, it is crucial that their behavior is aligned with the values, social norms or other requirements of that environment. However, many environments have implicit constraints that are difficult to specify and transfer to a learning agent. To address this challenge, we propose a novel method that utilizes the principle of maximum causal entropy to learn constraints and an optimal policy that adheres to these constraints, using demonstrations of agents that abide by the constraints. We prove convergence in a tabular setting and provide an approximation which scales to complex environments. We evaluate the effectiveness of the learned policy by assessing the reward received and the number of constraint violations, and we evaluate the learned cost function based on its transferability to other agents. Our method has been shown to outperform state-of-the-art approaches across a variety of tasks and envi",
    "path": "papers/23/05/2305.02857.json",
    "total_tokens": 927,
    "translated_title": "最大因果熵逆约束强化学习",
    "translated_abstract": "在实际环境中，当人们与人工智能代理进行交互时，代理的行为与该环境的价值观、社会规范或其他要求相一致至关重要。然而，许多环境都有难以规定和转移给学习代理的隐含限制。为了应对这一挑战，我们提出了一种新的方法，利用最大因果熵原理来学习代理遵守限制的约束条件和最优策略，并使用遵守这些限制的代理的实例进行学习。我们在表格设置中证明了收敛性，并提供了一种适用于复杂环境的近似方法。我们通过评估所获得的奖励和违反约束的次数来评估学习到的策略的有效性，并根据其对其他代理的可转移性来评估学习到的成本函数。我们的方法已被证明在各种任务和环境中优于最先进的方法。",
    "tldr": "该论文提出了一种新颖的最大因果熵逆约束强化学习方法，通过利用最大因果熵原理来学习代理遵守限制的约束条件和最优策略，使用遵守这些限制的代理的实例进行学习。此方法已被证明在各种任务和环境中优于最先进的方法。",
    "en_tdlr": "The paper proposes a new maximum causal entropy inverse constrained reinforcement learning method to align artificial agents' behavior to the values, social norms, or other requirements of the environment, using the principle of maximum causal entropy to learn constraints and an optimal policy. The method has been shown to outperform state-of-the-art approaches across a variety of tasks and environments."
}