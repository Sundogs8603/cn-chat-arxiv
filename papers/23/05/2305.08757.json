{
    "title": "Physics Informed Token Transformer. (arXiv:2305.08757v2 [cs.LG] UPDATED)",
    "abstract": "Solving Partial Differential Equations (PDEs) is the core of many fields of science and engineering. While classical approaches are often prohibitively slow, machine learning models often fail to incorporate complete system information. Over the past few years, transformers have had a significant impact on the field of Artificial Intelligence and have seen increased usage in PDE applications. However, despite their success, transformers currently lack integration with physics and reasoning. This study aims to address this issue by introducing PITT: Physics Informed Token Transformer. The purpose of PITT is to incorporate the knowledge of physics by embedding partial differential equations (PDEs) into the learning process. PITT uses an equation tokenization method to learn an analytically-driven numerical update operator. By tokenizing PDEs and embedding partial derivatives, the transformer models become aware of the underlying knowledge behind physical processes. To demonstrate this, P",
    "link": "http://arxiv.org/abs/2305.08757",
    "context": "Title: Physics Informed Token Transformer. (arXiv:2305.08757v2 [cs.LG] UPDATED)\nAbstract: Solving Partial Differential Equations (PDEs) is the core of many fields of science and engineering. While classical approaches are often prohibitively slow, machine learning models often fail to incorporate complete system information. Over the past few years, transformers have had a significant impact on the field of Artificial Intelligence and have seen increased usage in PDE applications. However, despite their success, transformers currently lack integration with physics and reasoning. This study aims to address this issue by introducing PITT: Physics Informed Token Transformer. The purpose of PITT is to incorporate the knowledge of physics by embedding partial differential equations (PDEs) into the learning process. PITT uses an equation tokenization method to learn an analytically-driven numerical update operator. By tokenizing PDEs and embedding partial derivatives, the transformer models become aware of the underlying knowledge behind physical processes. To demonstrate this, P",
    "path": "papers/23/05/2305.08757.json",
    "total_tokens": 880,
    "translated_title": "物理信息化的Token Transformer",
    "translated_abstract": "解决偏微分方程（PDEs）是许多科学和工程领域的核心。虽然传统方法往往速度慢，但机器学习模型却往往无法完整地融入系统信息。在过去几年中，Transformer对人工智能领域产生了重大影响，并在PDE应用中得到了广泛使用。然而，尽管它们取得了成功，但目前Transformer缺乏与物理和推理的整合。本研究旨在通过引入PITT：物理信息化的Token Transformer来解决这个问题。PITT的目的是通过将偏微分方程（PDEs）嵌入学习过程中来融入物理知识。PITT使用方程标记化方法来学习分析驱动的数值更新运算符。通过标记化PDEs和嵌入偏导数，Transformer模型可以意识到物理过程的基本知识。为了证明这一点，研究通过实验证明了PITT在多个PDE应用中的性能和优势。",
    "tldr": "本研究提出了一种名为PITT的物理信息化的Token Transformer模型，通过将偏微分方程嵌入学习过程中，使得模型能够融入物理知识，并在多个PDE应用中展现出性能和优势。",
    "en_tdlr": "This study introduces a physics informed Token Transformer model, PITT, which incorporates the knowledge of physics into the learning process by embedding partial differential equations. The model shows performance and advantages in multiple PDE applications."
}