{
    "title": "Fusion for Visual-Infrared Person ReID in Real-World Surveillance Using Corrupted Multimodal Data. (arXiv:2305.00320v1 [cs.CV])",
    "abstract": "Visible-infrared person re-identification (V-I ReID) seeks to match images of individuals captured over a distributed network of RGB and IR cameras. The task is challenging due to the significant differences between V and I modalities, especially under real-world conditions, where images are corrupted by, e.g, blur, noise, and weather. Indeed, state-of-art V-I ReID models cannot leverage corrupted modality information to sustain a high level of accuracy. In this paper, we propose an efficient model for multimodal V-I ReID -- named Multimodal Middle Stream Fusion (MMSF) -- that preserves modality-specific knowledge for improved robustness to corrupted multimodal images. In addition, three state-of-art attention-based multimodal fusion models are adapted to address corrupted multimodal data in V-I ReID, allowing to dynamically balance each modality importance. Recently, evaluation protocols have been proposed to assess the robustness of ReID models under challenging real-world scenarios.",
    "link": "http://arxiv.org/abs/2305.00320",
    "context": "Title: Fusion for Visual-Infrared Person ReID in Real-World Surveillance Using Corrupted Multimodal Data. (arXiv:2305.00320v1 [cs.CV])\nAbstract: Visible-infrared person re-identification (V-I ReID) seeks to match images of individuals captured over a distributed network of RGB and IR cameras. The task is challenging due to the significant differences between V and I modalities, especially under real-world conditions, where images are corrupted by, e.g, blur, noise, and weather. Indeed, state-of-art V-I ReID models cannot leverage corrupted modality information to sustain a high level of accuracy. In this paper, we propose an efficient model for multimodal V-I ReID -- named Multimodal Middle Stream Fusion (MMSF) -- that preserves modality-specific knowledge for improved robustness to corrupted multimodal images. In addition, three state-of-art attention-based multimodal fusion models are adapted to address corrupted multimodal data in V-I ReID, allowing to dynamically balance each modality importance. Recently, evaluation protocols have been proposed to assess the robustness of ReID models under challenging real-world scenarios.",
    "path": "papers/23/05/2305.00320.json",
    "total_tokens": 1038,
    "translated_title": "在受污染的多模态数据下，用于真实世界监控的视觉-红外人员再识别的融合",
    "translated_abstract": "可见光-红外人员再识别(V-I ReID)旨在匹配由分布式RGB和IR摄像机捕获的个体图像。由于V和I模态之间的显著差异，特别是在真实世界的条件下，图像受到模糊、噪声和天气等因素的干扰，该任务变得具有挑战性。事实上，最先进的V-I ReID模型不能利用受污染的模态信息来维持高水平的准确性。在本文中，我们提出了一种高效的多模态V-I ReID模型——名为多模态中间流融合(MMSF)，用于提高受污染多模态图像的鲁棒性，并针对在V-I ReID中出现的受污染多模态数据，适应了三种最先进的基于注意力的多模态融合模型，可动态平衡每种模态的重要性。近期已提出评估协议以评估ReID模型在具有挑战性的真实世界场景下的鲁棒性。",
    "tldr": "本篇论文提出了一种名为MMSF的模型，能够保留模态特定的知识，提高受污染多模态图像的鲁棒性。同时，还采用了三种最先进的基于注意力的多模态融合模型，在V-I ReID中适应受污染多模态数据，动态平衡每种模态的重要性。",
    "en_tdlr": "This paper proposes an efficient model named MMSF, which preserves modality-specific knowledge for improved robustness to corrupted multimodal images. Additionally, three state-of-the-art attention-based multimodal fusion models are adapted to address corrupted multimodal data in V-I ReID while dynamically balancing the importance of each modality. Evaluation protocols have been recently proposed to assess robustness of ReID models under challenging real-world scenarios."
}