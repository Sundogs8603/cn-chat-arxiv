{
    "title": "On the Efficacy and Noise-Robustness of Jointly Learned Speech Emotion and Automatic Speech Recognition. (arXiv:2305.12540v2 [eess.AS] UPDATED)",
    "abstract": "New-age conversational agent systems perform both speech emotion recognition (SER) and automatic speech recognition (ASR) using two separate and often independent approaches for real-world application in noisy environments. In this paper, we investigate a joint ASR-SER multitask learning approach in a low-resource setting and show that improvements are observed not only in SER, but also in ASR. We also investigate the robustness of such jointly trained models to the presence of background noise, babble, and music. Experimental results on the IEMOCAP dataset show that joint learning can improve ASR word error rate (WER) and SER classification accuracy by 10.7% and 2.3% respectively in clean scenarios. In noisy scenarios, results on data augmented with MUSAN show that the joint approach outperforms the independent ASR and SER approaches across many noisy conditions. Overall, the joint ASR-SER approach yielded more noise-resistant models than the independent ASR and SER approaches.",
    "link": "http://arxiv.org/abs/2305.12540",
    "context": "Title: On the Efficacy and Noise-Robustness of Jointly Learned Speech Emotion and Automatic Speech Recognition. (arXiv:2305.12540v2 [eess.AS] UPDATED)\nAbstract: New-age conversational agent systems perform both speech emotion recognition (SER) and automatic speech recognition (ASR) using two separate and often independent approaches for real-world application in noisy environments. In this paper, we investigate a joint ASR-SER multitask learning approach in a low-resource setting and show that improvements are observed not only in SER, but also in ASR. We also investigate the robustness of such jointly trained models to the presence of background noise, babble, and music. Experimental results on the IEMOCAP dataset show that joint learning can improve ASR word error rate (WER) and SER classification accuracy by 10.7% and 2.3% respectively in clean scenarios. In noisy scenarios, results on data augmented with MUSAN show that the joint approach outperforms the independent ASR and SER approaches across many noisy conditions. Overall, the joint ASR-SER approach yielded more noise-resistant models than the independent ASR and SER approaches.",
    "path": "papers/23/05/2305.12540.json",
    "total_tokens": 985,
    "translated_title": "关于联合学习语音情感和自动语音识别的有效性和噪声鲁棒性",
    "translated_abstract": "新时代的对话代理系统在嘈杂的环境中使用两种独立的方法执行语音情感识别（SER）和自动语音识别（ASR）。本文研究了一种低资源环境下的联合ASR-SER多任务学习方法，发现不仅在SER中取得了改进，而且在ASR中也有所提升。我们还研究了这种联合训练模型对背景噪声、胡言乱语和音乐的鲁棒性。在IEMOCAP数据集上的实验结果表明，在干净的环境中，联合学习可以将ASR单词错误率（WER）和SER分类准确率分别提高10.7％和2.3％。在加入了MUSAN的嘈杂场景中，联合方法在许多嘈杂条件下优于独立的ASR和SER方法。总之，联合ASR-SER方法比独立的ASR和SER方法产生了更具鲁棒性的模型。",
    "tldr": "本论文研究了联合学习ASR和SER的多任务学习方法，并证明在低资源环境下不仅在SER中取得了显著提升，同时也提高了ASR的性能，并且在嘈杂环境中，联合训练得到的模型比独立训练的模型更具噪声鲁棒性",
    "en_tdlr": "This paper investigates a joint ASR-SER multitask learning approach and shows that improvements are observed not only in SER, but also in ASR in low-resource setting. The joint approach outperforms the independent ASR and SER approaches across many noisy conditions, indicating its greater noise-robustness."
}