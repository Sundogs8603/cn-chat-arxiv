{
    "title": "Missing Information, Unresponsive Authors, Experimental Flaws: The Impossibility of Assessing the Reproducibility of Previous Human Evaluations in NLP. (arXiv:2305.01633v1 [cs.CL])",
    "abstract": "We report our efforts in identifying a set of previous human evaluations in NLP that would be suitable for a coordinated study examining what makes human evaluations in NLP more/less reproducible. We present our results and findings, which include that just 13\\% of papers had (i) sufficiently low barriers to reproduction, and (ii) enough obtainable information, to be considered for reproduction, and that all but one of the experiments we selected for reproduction was discovered to have flaws that made the meaningfulness of conducting a reproduction questionable. As a result, we had to change our coordinated study design from a reproduce approach to a standardise-then-reproduce-twice approach. Our overall (negative) finding that the great majority of human evaluations in NLP is not repeatable and/or not reproducible and/or too flawed to justify reproduction, paints a dire picture, but presents an opportunity for a rethink about how to design and report human evaluations in NLP.",
    "link": "http://arxiv.org/abs/2305.01633",
    "context": "Title: Missing Information, Unresponsive Authors, Experimental Flaws: The Impossibility of Assessing the Reproducibility of Previous Human Evaluations in NLP. (arXiv:2305.01633v1 [cs.CL])\nAbstract: We report our efforts in identifying a set of previous human evaluations in NLP that would be suitable for a coordinated study examining what makes human evaluations in NLP more/less reproducible. We present our results and findings, which include that just 13\\% of papers had (i) sufficiently low barriers to reproduction, and (ii) enough obtainable information, to be considered for reproduction, and that all but one of the experiments we selected for reproduction was discovered to have flaws that made the meaningfulness of conducting a reproduction questionable. As a result, we had to change our coordinated study design from a reproduce approach to a standardise-then-reproduce-twice approach. Our overall (negative) finding that the great majority of human evaluations in NLP is not repeatable and/or not reproducible and/or too flawed to justify reproduction, paints a dire picture, but presents an opportunity for a rethink about how to design and report human evaluations in NLP.",
    "path": "papers/23/05/2305.01633.json",
    "total_tokens": 1007,
    "translated_title": "缺失信息、无回应作者、实验缺陷：NLP中不可能评估以前的人类评估的再现性。",
    "translated_abstract": "我们报告了我们在识别一组先前适合进行协调研究的NLP领域人类评估的努力，以考察是什么使得NLP领域的人类评估更/ less能再现。我们提供了我们的结果和发现，其中包括仅有13％的论文具有（i）足够低的再现障碍，以及（ii）足够的可获取信息，才可以被考虑进行再现，并且我们选择进行再现的所有实验都被发现存在缺陷，这使得进行再现的有意义性值得怀疑。因此，我们不得不将我们的协调研究设计从再现方法更改为标准化-然后再现两次的方法。我们总体而言（是负面的）发现，NLP领域中的绝大多数人类评估都不能重复和/或不能再现和/或具有太多缺陷以证明其可再现性。这描绘了一个可怕的画面，但也为重新考虑如何设计和报告NLP领域的人类评估提供了机会。",
    "tldr": "该论文研究了NLP领域过去的人类评估的再现性问题，结果发现大部分人类评估都无法重复或再现，可能是由于缺失信息、无回应作者和实验缺陷等原因导致。这个结果提示我们需要重新考虑如何设计和报告人类评估实验。",
    "en_tdlr": "This paper investigates the reproducibility of previous human evaluations in NLP and finds that the great majority of them are not repeatable and/or not reproducible and/or too flawed to justify reproduction, which paints a dire picture but presents an opportunity for a rethink about how to design and report human evaluations in NLP."
}