{
    "title": "A quantitative study of NLP approaches to question difficulty estimation. (arXiv:2305.10236v1 [cs.CL])",
    "abstract": "Recent years witnessed an increase in the amount of research on the task of Question Difficulty Estimation from Text QDET with Natural Language Processing (NLP) techniques, with the goal of targeting the limitations of traditional approaches to question calibration. However, almost the entirety of previous research focused on single silos, without performing quantitative comparisons between different models or across datasets from different educational domains. In this work, we aim at filling this gap, by quantitatively analyzing several approaches proposed in previous research, and comparing their performance on three publicly available real world datasets containing questions of different types from different educational domains. Specifically, we consider reading comprehension Multiple Choice Questions (MCQs), science MCQs, and math questions. We find that Transformer based models are the best performing across different educational domains, with DistilBERT performing almost as well ",
    "link": "http://arxiv.org/abs/2305.10236",
    "context": "Title: A quantitative study of NLP approaches to question difficulty estimation. (arXiv:2305.10236v1 [cs.CL])\nAbstract: Recent years witnessed an increase in the amount of research on the task of Question Difficulty Estimation from Text QDET with Natural Language Processing (NLP) techniques, with the goal of targeting the limitations of traditional approaches to question calibration. However, almost the entirety of previous research focused on single silos, without performing quantitative comparisons between different models or across datasets from different educational domains. In this work, we aim at filling this gap, by quantitatively analyzing several approaches proposed in previous research, and comparing their performance on three publicly available real world datasets containing questions of different types from different educational domains. Specifically, we consider reading comprehension Multiple Choice Questions (MCQs), science MCQs, and math questions. We find that Transformer based models are the best performing across different educational domains, with DistilBERT performing almost as well ",
    "path": "papers/23/05/2305.10236.json",
    "total_tokens": 816,
    "tldr": "本文研究了NLP方法对问题难度评估的定量分析，比较了不同领域的数据集和模型表现，发现基于变压器的模型表现最佳。"
}