{
    "title": "Information Bottleneck Analysis of Deep Neural Networks via Lossy Compression. (arXiv:2305.08013v1 [cs.LG])",
    "abstract": "The Information Bottleneck (IB) principle offers an information-theoretic framework for analyzing the training process of deep neural networks (DNNs). Its essence lies in tracking the dynamics of two mutual information (MI) values: one between the hidden layer and the class label, and the other between the hidden layer and the DNN input. According to the hypothesis put forth by Shwartz-Ziv and Tishby (2017), the training process consists of two distinct phases: fitting and compression. The latter phase is believed to account for the good generalization performance exhibited by DNNs. Due to the challenging nature of estimating MI between high-dimensional random vectors, this hypothesis has only been verified for toy NNs or specific types of NNs, such as quantized NNs and dropout NNs. In this paper, we introduce a comprehensive framework for conducting IB analysis of general NNs. Our approach leverages the stochastic NN method proposed by Goldfeld et al. (2019) and incorporates a compres",
    "link": "http://arxiv.org/abs/2305.08013",
    "context": "Title: Information Bottleneck Analysis of Deep Neural Networks via Lossy Compression. (arXiv:2305.08013v1 [cs.LG])\nAbstract: The Information Bottleneck (IB) principle offers an information-theoretic framework for analyzing the training process of deep neural networks (DNNs). Its essence lies in tracking the dynamics of two mutual information (MI) values: one between the hidden layer and the class label, and the other between the hidden layer and the DNN input. According to the hypothesis put forth by Shwartz-Ziv and Tishby (2017), the training process consists of two distinct phases: fitting and compression. The latter phase is believed to account for the good generalization performance exhibited by DNNs. Due to the challenging nature of estimating MI between high-dimensional random vectors, this hypothesis has only been verified for toy NNs or specific types of NNs, such as quantized NNs and dropout NNs. In this paper, we introduce a comprehensive framework for conducting IB analysis of general NNs. Our approach leverages the stochastic NN method proposed by Goldfeld et al. (2019) and incorporates a compres",
    "path": "papers/23/05/2305.08013.json",
    "total_tokens": 873,
    "translated_title": "通过损失压缩分析深度神经网络的信息瓶颈(arXiv:2305.08013v1 [cs.LG])",
    "translated_abstract": "信息瓶颈原理提供了一个信息论框架，用于分析深度神经网络(DNNs)的训练过程。其核心在于跟踪隐藏层与类标签之间的互信息值和隐藏层与DNN输入之间的互信息值的动态变化。据Shwartz-Ziv和Tishby(2017)提出的假说，训练过程由两个不同的阶段组成:拟合和压缩。后者被认为是DNNs表现良好的泛化能力的原因。本文引入了一个综合框架，用于进行对一般NNs的IB分析。我们的方法利用了Goldfeld等人(2019)提出的随机神经网络方法，并结合了一种压缩方法。",
    "tldr": "本文提出了一种综合框架，用于对一般神经网络进行信息瓶颈分析，以研究训练过程中的拟合和压缩阶段。通过该分析，可以更好地理解深度神经网络的泛化能力。",
    "en_tdlr": "This paper presents a comprehensive framework for conducting Information Bottleneck (IB) analysis of general neural networks (NNs), to study the fitting and compression phases during the training process. This analysis can provide a better understanding of the generalization capability of deep neural networks."
}