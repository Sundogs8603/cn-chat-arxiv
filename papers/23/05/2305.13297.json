{
    "title": "Investigating the Role of Feed-Forward Networks in Transformers Using Parallel Attention and Feed-Forward Net Design. (arXiv:2305.13297v2 [cs.CL] UPDATED)",
    "abstract": "This paper investigates the key role of Feed-Forward Networks (FFNs) in transformer models by utilizing the Parallel Attention and Feed-Forward Net Design (PAF) architecture, and comparing it to their Series Attention and Feed-Forward Net Design (SAF) counterparts. Central to the effectiveness of PAF are two main assumptions regarding the FFN block and the attention block within a layer: 1) the primary function of the FFN block is to maintain isotropy among token embeddings and prevent their degeneration, and 2) the residual norm computed in the attention block is substantially smaller than the input token embedding norm. To empirically validate these assumptions, we train PAF variants of two large language models (RoBERTa-large and bert-large-uncased). Our results demonstrate that both assumptions hold true in the PAF design. This study contributes to a deeper understanding of the roles and interactions between FFNs and self-attention mechanisms in transformer architectures.",
    "link": "http://arxiv.org/abs/2305.13297",
    "context": "Title: Investigating the Role of Feed-Forward Networks in Transformers Using Parallel Attention and Feed-Forward Net Design. (arXiv:2305.13297v2 [cs.CL] UPDATED)\nAbstract: This paper investigates the key role of Feed-Forward Networks (FFNs) in transformer models by utilizing the Parallel Attention and Feed-Forward Net Design (PAF) architecture, and comparing it to their Series Attention and Feed-Forward Net Design (SAF) counterparts. Central to the effectiveness of PAF are two main assumptions regarding the FFN block and the attention block within a layer: 1) the primary function of the FFN block is to maintain isotropy among token embeddings and prevent their degeneration, and 2) the residual norm computed in the attention block is substantially smaller than the input token embedding norm. To empirically validate these assumptions, we train PAF variants of two large language models (RoBERTa-large and bert-large-uncased). Our results demonstrate that both assumptions hold true in the PAF design. This study contributes to a deeper understanding of the roles and interactions between FFNs and self-attention mechanisms in transformer architectures.",
    "path": "papers/23/05/2305.13297.json",
    "total_tokens": 933,
    "translated_title": "利用并行注意力和前馈网络设计研究Transformer中前馈网络的作用",
    "translated_abstract": "本文通过利用并行注意力和前馈网络设计（PAF）架构，并将其与系列注意力和前馈网络设计（SAF）进行比较，研究前馈网络在变压器模型中的关键作用。 PAF的有效性关键在于两个主要假设，即FFN块和层内注意块的主要功能为保持令牌嵌入的各向同性并防止其退化，以及注意力块中计算的残差范数远小于输入令牌嵌入范数。为了实证这些假设，我们训练了RoBERTa-large和bert-large-uncased两个大型语言模型的PAF变体。我们的结果表明，在PAF设计中，这两个假设都成立。本研究有助于深入了解Transformer架构中FFN和自注意机制之间的作用和相互作用。",
    "tldr": "本文研究了利用并行注意力和前馈网络设计（PAF）架构验证了前馈网络在变压器模型中的关键作用，并表明FFN块的主要功能是保持各向同性并防止退化，注意块中计算的残差范数远小于输入令牌嵌入范数。",
    "en_tdlr": "This paper investigates the key role of feed-forward networks (FFNs) in transformer models using Parallel Attention and Feed-Forward Net Design (PAF) architecture, which demonstrates that the primary function of the FFN block is to maintain isotropy among token embeddings and prevent their degeneration, and the residual norm computed in the attention block is substantially smaller than the input token embedding norm."
}