{
    "title": "Local SGD Accelerates Convergence by Exploiting Second Order Information of the Loss Function. (arXiv:2305.15013v2 [cs.LG] UPDATED)",
    "abstract": "With multiple iterations of updates, local statistical gradient descent (L-SGD) has been proven to be very effective in distributed machine learning schemes such as federated learning. In fact, many innovative works have shown that L-SGD with independent and identically distributed (IID) data can even outperform SGD. As a result, extensive efforts have been made to unveil the power of L-SGD. However, existing analysis failed to explain why the multiple local updates with small mini-batches of data (L-SGD) can not be replaced by the update with one big batch of data and a larger learning rate (SGD). In this paper, we offer a new perspective to understand the strength of L-SGD. We theoretically prove that, with IID data, L-SGD can effectively explore the second order information of the loss function. In particular, compared with SGD, the updates of L-SGD have much larger projection on the eigenvectors of the Hessian matrix with small eigenvalues, which leads to faster convergence. Under ",
    "link": "http://arxiv.org/abs/2305.15013",
    "context": "Title: Local SGD Accelerates Convergence by Exploiting Second Order Information of the Loss Function. (arXiv:2305.15013v2 [cs.LG] UPDATED)\nAbstract: With multiple iterations of updates, local statistical gradient descent (L-SGD) has been proven to be very effective in distributed machine learning schemes such as federated learning. In fact, many innovative works have shown that L-SGD with independent and identically distributed (IID) data can even outperform SGD. As a result, extensive efforts have been made to unveil the power of L-SGD. However, existing analysis failed to explain why the multiple local updates with small mini-batches of data (L-SGD) can not be replaced by the update with one big batch of data and a larger learning rate (SGD). In this paper, we offer a new perspective to understand the strength of L-SGD. We theoretically prove that, with IID data, L-SGD can effectively explore the second order information of the loss function. In particular, compared with SGD, the updates of L-SGD have much larger projection on the eigenvectors of the Hessian matrix with small eigenvalues, which leads to faster convergence. Under ",
    "path": "papers/23/05/2305.15013.json",
    "total_tokens": 753,
    "translated_title": "利用损失函数的二阶信息，本地 SGD 加速收敛。",
    "translated_abstract": "本文研究了利用多次本地统计梯度下降（L-SGD）更新的有效性，发现L-SGD能够探索损失函数的二阶信息，在独立同分布（IID）数据方案中超越了随机梯度下降（SGD）。我们证明了相比SGD，L-SGD的更新沿着具有小特征值的黑塞矩阵的特征向量有更大的投影，从而能更快地收敛。",
    "tldr": "本文研究了使用本地 SGD 更新的有效性，证明了在 IID 数据方案中，L-SGD 能够探索并利用损失函数的二阶信息以实现更快速的收敛。"
}