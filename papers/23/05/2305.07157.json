{
    "title": "Exploring Zero and Few-shot Techniques for Intent Classification. (arXiv:2305.07157v1 [cs.CL])",
    "abstract": "Conversational NLU providers often need to scale to thousands of intent-classification models where new customers often face the cold-start problem. Scaling to so many customers puts a constraint on storage space as well. In this paper, we explore four different zero and few-shot intent classification approaches with this low-resource constraint: 1) domain adaptation, 2) data augmentation, 3) zero-shot intent classification using descriptions large language models (LLMs), and 4) parameter-efficient fine-tuning of instruction-finetuned language models. Our results show that all these approaches are effective to different degrees in low-resource settings. Parameter-efficient fine-tuning using T-few recipe (Liu et al., 2022) on Flan-T5 (Chang et al., 2022) yields the best performance even with just one sample per intent. We also show that the zero-shot method of prompting LLMs using intent descriptions",
    "link": "http://arxiv.org/abs/2305.07157",
    "context": "Title: Exploring Zero and Few-shot Techniques for Intent Classification. (arXiv:2305.07157v1 [cs.CL])\nAbstract: Conversational NLU providers often need to scale to thousands of intent-classification models where new customers often face the cold-start problem. Scaling to so many customers puts a constraint on storage space as well. In this paper, we explore four different zero and few-shot intent classification approaches with this low-resource constraint: 1) domain adaptation, 2) data augmentation, 3) zero-shot intent classification using descriptions large language models (LLMs), and 4) parameter-efficient fine-tuning of instruction-finetuned language models. Our results show that all these approaches are effective to different degrees in low-resource settings. Parameter-efficient fine-tuning using T-few recipe (Liu et al., 2022) on Flan-T5 (Chang et al., 2022) yields the best performance even with just one sample per intent. We also show that the zero-shot method of prompting LLMs using intent descriptions",
    "path": "papers/23/05/2305.07157.json",
    "total_tokens": 937,
    "translated_title": "探索零样本和小样本技术用于意图分类",
    "translated_abstract": "会话中自然语言理解的提供者通常需要扩展到数千个意图分类模型，其中新客户经常面临冷启动问题。在拥有这么多客户的情况下扩展，会对存储空间施加限制。本文探讨了四种不同的零样本和小样本意图分类方法，这些方法受到低资源限制的制约：1）领域适应，2）数据增强，3）使用大型语言模型（LLM）的零样本意图分类，以及4）指令微调语言模型的参数有效微调。我们的结果表明，所有这些方法在低资源环境下都是有效的，但程度不同。使用Flan-T5（Chang et al，2022）在T-few配方（Liu et al，2022）上进行参数有效微调，即使每个意图只有一个样本，性能也最佳。我们还展示了使用意图描述提示LLM的零样本方法。",
    "tldr": "探讨了四种零样本和小样本意图分类方法，包括领域适应、数据增强、使用大型语言模型的零样本意图分类以及指令微调语言模型的参数有效微调，结果表明这些方法在低资源环境下都是有效的。指令微调语言模型的参数有效微调性能最佳。",
    "en_tdlr": "Explored four zero and few-shot intent classification methods, including domain adaptation, data augmentation, zero-shot intent classification using large language models (LLMs), and parameter-efficient fine-tuning of instruction-finetuned language models, and found that all of them are effective in low-resource settings. The parameter-efficient fine-tuning using T-few recipe on Flan-T5 yields the best performance even with just one sample per intent."
}