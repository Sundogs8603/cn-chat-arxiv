{
    "title": "Reactive Perturbation Defocusing for Textual Adversarial Defense. (arXiv:2305.04067v1 [cs.CL])",
    "abstract": "Recent studies have shown that large pre-trained language models are vulnerable to adversarial attacks. Existing methods attempt to reconstruct the adversarial examples. However, these methods usually have limited performance in defense against adversarial examples, while also negatively impacting the performance on natural examples. To overcome this problem, we propose a method called Reactive Perturbation Defocusing (RPD). RPD uses an adversarial detector to identify adversarial examples and reduce false defenses on natural examples. Instead of reconstructing the adversaries, RPD injects safe perturbations into adversarial examples to distract the objective models from the malicious perturbations. Our experiments on three datasets, two objective models, and various adversarial attacks show that our proposed framework successfully repairs up to approximately 97% of correctly identified adversarial examples with only about a 2% performance decrease on natural examples. We also provide ",
    "link": "http://arxiv.org/abs/2305.04067",
    "context": "Title: Reactive Perturbation Defocusing for Textual Adversarial Defense. (arXiv:2305.04067v1 [cs.CL])\nAbstract: Recent studies have shown that large pre-trained language models are vulnerable to adversarial attacks. Existing methods attempt to reconstruct the adversarial examples. However, these methods usually have limited performance in defense against adversarial examples, while also negatively impacting the performance on natural examples. To overcome this problem, we propose a method called Reactive Perturbation Defocusing (RPD). RPD uses an adversarial detector to identify adversarial examples and reduce false defenses on natural examples. Instead of reconstructing the adversaries, RPD injects safe perturbations into adversarial examples to distract the objective models from the malicious perturbations. Our experiments on three datasets, two objective models, and various adversarial attacks show that our proposed framework successfully repairs up to approximately 97% of correctly identified adversarial examples with only about a 2% performance decrease on natural examples. We also provide ",
    "path": "papers/23/05/2305.04067.json",
    "total_tokens": 893,
    "translated_title": "反应性摄入扰动：对抗文本防御的一种方法",
    "translated_abstract": "最近的研究表明，大型预训练语言模型容易受到对抗性攻击。现有的方法试图重构对抗性示例，但这些方法通常在防御对抗性示例方面性能有限，同时也会对自然示例的性能产生负面影响。为了解决这个问题，我们提出了一种称为反应性摄入扰动 (RPD) 的方法。RPD 使用对抗检测器识别对抗性示例，并减少在自然示例上的误防御。RPD 不是重构对手，而是在对抗性示例中注入安全扰动，以分散目标模型对恶意扰动的注意力。我们在三个数据集、两个目标模型和各种对抗性攻击上的实验表明，我们提出的框架成功地修复了大约 97% 的正确识别的对抗性示例，并且自然示例的性能仅降低了约 2%。",
    "tldr": "RPD对大型预训练语言模型的漏洞进行了防御，通过识别对抗性示例并注入安全扰动来减少误防御，成功地修复了高达97%的对抗性示例，在自然示例的性能仅降低了约2%。",
    "en_tdlr": "RPD defends against vulnerabilities in large pre-trained language models by identifying adversarial examples and injecting safe perturbations to reduce false defenses, successfully repairing up to 97% of identified adversarial examples with only about a 2% decrease in natural example performance."
}