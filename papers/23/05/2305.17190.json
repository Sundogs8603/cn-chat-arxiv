{
    "title": "Hardware-Efficient Transformer Training via Piecewise Affine Operations. (arXiv:2305.17190v1 [cs.LG])",
    "abstract": "Multiplications are responsible for most of the computational cost involved in neural network training and inference. Recent research has thus looked for ways to reduce the cost associated with them. Inspired by Mogami (2020), we replace multiplication with a cheap piecewise affine approximation that is achieved by adding the bit representation of the floating point numbers together as integers. We show that transformers can be trained with the resulting modified matrix multiplications on both vision and language tasks with little to no performance impact, and without changes to the training hyperparameters. We further replace all non-linearities in the networks making them fully and jointly piecewise affine in both inputs and weights. Finally, we show that we can eliminate all multiplications in the entire training process, including operations in the forward pass, backward pass and optimizer update, demonstrating the first successful training of modern neural network architectures in",
    "link": "http://arxiv.org/abs/2305.17190",
    "context": "Title: Hardware-Efficient Transformer Training via Piecewise Affine Operations. (arXiv:2305.17190v1 [cs.LG])\nAbstract: Multiplications are responsible for most of the computational cost involved in neural network training and inference. Recent research has thus looked for ways to reduce the cost associated with them. Inspired by Mogami (2020), we replace multiplication with a cheap piecewise affine approximation that is achieved by adding the bit representation of the floating point numbers together as integers. We show that transformers can be trained with the resulting modified matrix multiplications on both vision and language tasks with little to no performance impact, and without changes to the training hyperparameters. We further replace all non-linearities in the networks making them fully and jointly piecewise affine in both inputs and weights. Finally, we show that we can eliminate all multiplications in the entire training process, including operations in the forward pass, backward pass and optimizer update, demonstrating the first successful training of modern neural network architectures in",
    "path": "papers/23/05/2305.17190.json",
    "total_tokens": 876,
    "translated_title": "利用分段仿射操作实现高效Transformer训练",
    "translated_abstract": "在神经网络训练和推理中，大多数计算成本都是由乘法所贡献的。近期的研究致力于减少由此带来的成本。本文受Mogami（2020）启发，用一种廉价的分段仿射逼近替换乘法，它通过将浮点数的位表示作为整数相加来实现。我们证明了在视觉和语言任务中，可以使用所得到的修改后的矩阵乘法训练Transformer，几乎没有性能影响，并且不需要更改训练超参数。我们进一步用分段仿射替换了网络中的所有非线性，使它们在输入和权重方面都成为完全联合的分段仿射函数。最后，我们展示了如何消除整个训练过程中的所有乘法操作，包括前向传播、反向传播和优化器更新的操作，展示了现代神经网络架构的首次成功训练。",
    "tldr": "本文提出一种使用分段仿射操作代替传统乘法的高效Transformer训练方法，不需要更改训练超参数即可在视觉和语言任务中成功实现训练，同时消除了整个训练过程中的所有乘法操作。",
    "en_tdlr": "This paper proposes a hardware-efficient training method for transformers using piecewise affine operations instead of traditional multiplications, achieving successful training in both vision and language tasks without changing training hyperparameters and eliminating all multiplication operations in the entire training process."
}