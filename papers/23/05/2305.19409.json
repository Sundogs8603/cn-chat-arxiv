{
    "title": "Examining risks of racial biases in NLP tools for child protective services. (arXiv:2305.19409v1 [cs.CL])",
    "abstract": "Although much literature has established the presence of demographic bias in natural language processing (NLP) models, most work relies on curated bias metrics that may not be reflective of real-world applications. At the same time, practitioners are increasingly using algorithmic tools in high-stakes settings, with particular recent interest in NLP. In this work, we focus on one such setting: child protective services (CPS). CPS workers often write copious free-form text notes about families they are working with, and CPS agencies are actively seeking to deploy NLP models to leverage these data. Given well-established racial bias in this setting, we investigate possible ways deployed NLP is liable to increase racial disparities. We specifically examine word statistics within notes and algorithmic fairness in risk prediction, coreference resolution, and named entity recognition (NER). We document consistent algorithmic unfairness in NER models, possible algorithmic unfairness in corefe",
    "link": "http://arxiv.org/abs/2305.19409",
    "context": "Title: Examining risks of racial biases in NLP tools for child protective services. (arXiv:2305.19409v1 [cs.CL])\nAbstract: Although much literature has established the presence of demographic bias in natural language processing (NLP) models, most work relies on curated bias metrics that may not be reflective of real-world applications. At the same time, practitioners are increasingly using algorithmic tools in high-stakes settings, with particular recent interest in NLP. In this work, we focus on one such setting: child protective services (CPS). CPS workers often write copious free-form text notes about families they are working with, and CPS agencies are actively seeking to deploy NLP models to leverage these data. Given well-established racial bias in this setting, we investigate possible ways deployed NLP is liable to increase racial disparities. We specifically examine word statistics within notes and algorithmic fairness in risk prediction, coreference resolution, and named entity recognition (NER). We document consistent algorithmic unfairness in NER models, possible algorithmic unfairness in corefe",
    "path": "papers/23/05/2305.19409.json",
    "total_tokens": 964,
    "translated_title": "分析自然语言处理在儿童保护服务中的种族偏见风险",
    "translated_abstract": "虽然很多文献已经确认了自然语言处理模型中存在人口统计学偏见的问题，但是大部分研究都依赖于策划的偏见度量指标，这可能不能反映现实世界的应用。同时，从业人员越来越多地在高风险环境中使用算法工具，特别是在自然语言处理领域。本研究聚焦在儿童保护服务中的一种情况。儿童保护服务工作者经常写有关他们所服务的家庭的详细文本笔记，而儿童保护机构正在积极寻求部署自然语言处理模型来利用这些数据。鉴于在这个环境中已经存在的种族偏见，我们调查了部署的自然语言处理模型可能增加种族差异的可能途径。我们特别审查了笔记中的词汇统计和风险预测、指代消解和命名实体识别（NER）中的算法公平性。我们记录了NER模型的一致算法不公平性，并可能出现指代消解和风险预测中算法不公平的情况。",
    "tldr": "本研究调查了儿童保护服务中自然语言处理模型存在的种族偏见问题，并发现了NER模型一致存在算法不公平的情况，同时可能出现指代消解和风险预测中算法不公平的情况。",
    "en_tdlr": "This study examines the risks of racial biases in NLP models in child protective services, and finds consistent algorithmic unfairness in NER models, as well as possible unfairness in coreference resolution and risk prediction."
}