{
    "title": "Double Pessimism is Provably Efficient for Distributionally Robust Offline Reinforcement Learning: Generic Algorithm and Robust Partial Coverage. (arXiv:2305.09659v1 [cs.LG])",
    "abstract": "We study distributionally robust offline reinforcement learning (robust offline RL), which seeks to find an optimal robust policy purely from an offline dataset that can perform well in perturbed environments. We propose a generic algorithm framework \\underline{D}oubly \\underline{P}essimistic \\underline{M}odel-based \\underline{P}olicy \\underline{O}ptimization ($\\texttt{P}^2\\texttt{MPO}$) for robust offline RL, which features a novel combination of a flexible model estimation subroutine and a doubly pessimistic policy optimization step. The \\emph{double pessimism} principle is crucial to overcome the distributional shift incurred by i) the mismatch between behavior policy and the family of target policies; and ii) the perturbation of the nominal model. Under certain accuracy assumptions on the model estimation subroutine, we show that $\\texttt{P}^2\\texttt{MPO}$ is provably efficient with \\emph{robust partial coverage data}, which means that the offline dataset has good coverage of the d",
    "link": "http://arxiv.org/abs/2305.09659",
    "context": "Title: Double Pessimism is Provably Efficient for Distributionally Robust Offline Reinforcement Learning: Generic Algorithm and Robust Partial Coverage. (arXiv:2305.09659v1 [cs.LG])\nAbstract: We study distributionally robust offline reinforcement learning (robust offline RL), which seeks to find an optimal robust policy purely from an offline dataset that can perform well in perturbed environments. We propose a generic algorithm framework \\underline{D}oubly \\underline{P}essimistic \\underline{M}odel-based \\underline{P}olicy \\underline{O}ptimization ($\\texttt{P}^2\\texttt{MPO}$) for robust offline RL, which features a novel combination of a flexible model estimation subroutine and a doubly pessimistic policy optimization step. The \\emph{double pessimism} principle is crucial to overcome the distributional shift incurred by i) the mismatch between behavior policy and the family of target policies; and ii) the perturbation of the nominal model. Under certain accuracy assumptions on the model estimation subroutine, we show that $\\texttt{P}^2\\texttt{MPO}$ is provably efficient with \\emph{robust partial coverage data}, which means that the offline dataset has good coverage of the d",
    "path": "papers/23/05/2305.09659.json",
    "total_tokens": 1050,
    "translated_title": "分布式鲁棒的离线强化学习：基于双重悲观性的通用算法和强健部分覆盖",
    "translated_abstract": "本文研究了分布式鲁棒的离线强化学习（鲁棒离线RL），其旨在从离线数据集中纯粹地找到一个能够在扰动环境中表现良好的最优强鲁棒策略。我们提出了一个名为P2MPO的算法框架，其中包含了灵活的模型估计子例程和双重悲观的策略优化步骤。双重悲观性原则对于克服由行为策略和目标策略家族之间的不匹配以及名义模型的扰动所引起的分布偏移至关重要。在对模型估计子例程进行一定准确性假设的情况下，我们证明了P2MPO算法在拥有良好的鲁棒部分覆盖数据的情况下是可证明有效的。",
    "tldr": "本论文提出了一个名为P2MPO的算法框架，用于解决基于鲁棒离线RL的问题。该框架结合了灵活的模型估计子例程和双重悲观的策略优化步骤，采用双重悲观性原则以克服模型偏移等问题。研究表明，在模型准确性的假设下，该框架在拥有良好的鲁棒部分覆盖数据的情况下是具备高效性的。",
    "en_tdlr": "This paper proposes a P2MPO algorithm framework for distributionally robust offline reinforcement learning, which combines flexible model estimation and doubly pessimistic policy optimization steps to overcome distributional shift caused by behavior policy and nominal model perturbations. The framework is proven to be efficient with robust partial coverage data under certain accuracy assumptions on the model estimation subroutine."
}