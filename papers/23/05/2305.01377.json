{
    "title": "Random Function Descent. (arXiv:2305.01377v1 [math.OC])",
    "abstract": "While gradient based methods are ubiquitous in machine learning, selecting the right step size often requires \"hyperparameter tuning\". This is because backtracking procedures like Armijo's rule depend on quality evaluations in every step, which are not available in a stochastic context. Since optimization schemes can be motivated using Taylor approximations, we replace the Taylor approximation with the conditional expectation (the best $L^2$ estimator) and propose \"Random Function Descent\" (RFD). Under light assumptions common in Bayesian optimization, we prove that RFD is identical to gradient descent, but with calculable step sizes, even in a stochastic context. We beat untuned Adam in synthetic benchmarks. To close the performance gap to tuned Adam, we propose a heuristic extension competitive with tuned Adam.",
    "link": "http://arxiv.org/abs/2305.01377",
    "context": "Title: Random Function Descent. (arXiv:2305.01377v1 [math.OC])\nAbstract: While gradient based methods are ubiquitous in machine learning, selecting the right step size often requires \"hyperparameter tuning\". This is because backtracking procedures like Armijo's rule depend on quality evaluations in every step, which are not available in a stochastic context. Since optimization schemes can be motivated using Taylor approximations, we replace the Taylor approximation with the conditional expectation (the best $L^2$ estimator) and propose \"Random Function Descent\" (RFD). Under light assumptions common in Bayesian optimization, we prove that RFD is identical to gradient descent, but with calculable step sizes, even in a stochastic context. We beat untuned Adam in synthetic benchmarks. To close the performance gap to tuned Adam, we propose a heuristic extension competitive with tuned Adam.",
    "path": "papers/23/05/2305.01377.json",
    "total_tokens": 872,
    "translated_title": "随机函数下降法",
    "translated_abstract": "虽然梯度下降方法在机器学习中十分常见，但是选择正确的步长经常需要进行“超参数调整”。这是因为回溯程序如Armijo's准则依赖于每个步骤中的质量评估，而这些评估在随机情况下不可用。由于优化方案可以用Taylor逼近来解释，我们将Taylor逼近替换为条件期望（最佳的$L^2$估计），提出了“随机函数下降”（RFD）。 在Bayesian优化中常见的一些轻微假设的情况下，我们证明了RFD与梯度下降算法是相同的，但是在随机情况下具有可计算的步长。我们在合成基准测试中比未调整的Adam方法表现更好。为了缩小与调整后的Adam算法之间的性能差距，我们提出了一种启发式扩展，可与调整后的Adam方法相媲美。",
    "tldr": "本文提出了随机函数下降(RFD)算法，可以在随机环境中计算出步长并且与贝叶斯优化中的梯度下降算法相同。在合成基准测试中，RFD算法比未调整的Adam方法表现更好，提出的heuristic扩展可与调整后的Adam方法相媲美。",
    "en_tdlr": "The paper proposes a Random Function Descent (RFD) algorithm, which can calculate step size in a stochastic environment and is identical to gradient descent in Bayesian optimization. RFD algorithm outperforms untuned Adam method in synthetic benchmarks and a heuristic extension is proposed to close the performance gap with tuned Adam method."
}