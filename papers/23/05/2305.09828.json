{
    "title": "Mimetic Initialization of Self-Attention Layers. (arXiv:2305.09828v1 [cs.CV])",
    "abstract": "It is notoriously difficult to train Transformers on small datasets; typically, large pre-trained models are instead used as the starting point. We explore the weights of such pre-trained Transformers (particularly for vision) to attempt to find reasons for this discrepancy. Surprisingly, we find that simply initializing the weights of self-attention layers so that they \"look\" more like their pre-trained counterparts allows us to train vanilla Transformers faster and to higher final accuracies, particularly on vision tasks such as CIFAR-10 and ImageNet classification, where we see gains in accuracy of over 5% and 4%, respectively. Our initialization scheme is closed form, learning-free, and very simple: we set the product of the query and key weights to be approximately the identity, and the product of the value and projection weights to approximately the negative identity. As this mimics the patterns we saw in pre-trained Transformers, we call the technique \"mimetic initialization\".",
    "link": "http://arxiv.org/abs/2305.09828",
    "context": "Title: Mimetic Initialization of Self-Attention Layers. (arXiv:2305.09828v1 [cs.CV])\nAbstract: It is notoriously difficult to train Transformers on small datasets; typically, large pre-trained models are instead used as the starting point. We explore the weights of such pre-trained Transformers (particularly for vision) to attempt to find reasons for this discrepancy. Surprisingly, we find that simply initializing the weights of self-attention layers so that they \"look\" more like their pre-trained counterparts allows us to train vanilla Transformers faster and to higher final accuracies, particularly on vision tasks such as CIFAR-10 and ImageNet classification, where we see gains in accuracy of over 5% and 4%, respectively. Our initialization scheme is closed form, learning-free, and very simple: we set the product of the query and key weights to be approximately the identity, and the product of the value and projection weights to approximately the negative identity. As this mimics the patterns we saw in pre-trained Transformers, we call the technique \"mimetic initialization\".",
    "path": "papers/23/05/2305.09828.json",
    "total_tokens": 834,
    "translated_title": "自注意力层的拟态初始化",
    "translated_abstract": "在小规模数据集上训练Transformer十分困难。通常需要以大规模预训练模型作为起点。我们探索了这些预训练Transformer的权重（尤其是用于视觉任务），试图找到造成这种差异的原因。惊讶的是，我们发现仅仅通过初始化自注意力层的权重，使其“看起来”更像预训练模型，就能够更快且更高精度地训练普通Transformer，尤其是在像CIFAR-10和ImageNet分类这样的视觉任务上，我们的精度提高超过5％和4％。我们的初始化方案是闭式的、无需学习的、非常简单：我们将查询和键权重的乘积设置为近似于标识，将值和投影权重的乘积近似于负标识。由于这类似于我们在预训练Transformer中看到的模式，所以我们称为“拟态初始化”技术。",
    "tldr": "本文介绍一种名为拟态初始化的方法，通过仅仅调整自注意力层的权重初始化，即可在视觉任务中大大提高Transformer的精度。",
    "en_tdlr": "This paper introduces a method called mimetic initialization, which can significantly improve the accuracy of Transformers in vision tasks by simply adjusting the weight initialization of self-attention layers."
}