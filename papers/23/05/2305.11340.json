{
    "title": "Bayesian Reparameterization of Reward-Conditioned Reinforcement Learning with Energy-based Models. (arXiv:2305.11340v1 [cs.LG])",
    "abstract": "Recently, reward-conditioned reinforcement learning (RCRL) has gained popularity due to its simplicity, flexibility, and off-policy nature. However, we will show that current RCRL approaches are fundamentally limited and fail to address two critical challenges of RCRL -- improving generalization on high reward-to-go (RTG) inputs, and avoiding out-of-distribution (OOD) RTG queries during testing time. To address these challenges when training vanilla RCRL architectures, we propose Bayesian Reparameterized RCRL (BR-RCRL), a novel set of inductive biases for RCRL inspired by Bayes' theorem. BR-RCRL removes a core obstacle preventing vanilla RCRL from generalizing on high RTG inputs -- a tendency that the model treats different RTG inputs as independent values, which we term ``RTG Independence\". BR-RCRL also allows us to design an accompanying adaptive inference method, which maximizes total returns while avoiding OOD queries that yield unpredictable behaviors in vanilla RCRL methods. We s",
    "link": "http://arxiv.org/abs/2305.11340",
    "context": "Title: Bayesian Reparameterization of Reward-Conditioned Reinforcement Learning with Energy-based Models. (arXiv:2305.11340v1 [cs.LG])\nAbstract: Recently, reward-conditioned reinforcement learning (RCRL) has gained popularity due to its simplicity, flexibility, and off-policy nature. However, we will show that current RCRL approaches are fundamentally limited and fail to address two critical challenges of RCRL -- improving generalization on high reward-to-go (RTG) inputs, and avoiding out-of-distribution (OOD) RTG queries during testing time. To address these challenges when training vanilla RCRL architectures, we propose Bayesian Reparameterized RCRL (BR-RCRL), a novel set of inductive biases for RCRL inspired by Bayes' theorem. BR-RCRL removes a core obstacle preventing vanilla RCRL from generalizing on high RTG inputs -- a tendency that the model treats different RTG inputs as independent values, which we term ``RTG Independence\". BR-RCRL also allows us to design an accompanying adaptive inference method, which maximizes total returns while avoiding OOD queries that yield unpredictable behaviors in vanilla RCRL methods. We s",
    "path": "papers/23/05/2305.11340.json",
    "total_tokens": 786,
    "translated_title": "能量模型下的贝叶斯重参数化奖励条件强化学习",
    "translated_abstract": "最近，由于其简单灵活和离线策略特性，奖励条件强化学习（RCRL）变得越来越受欢迎。然而，我们将展示当前的RCRL方法存在根本性局限性，并未解决两个关键的RCRL挑战 - 如何改善高奖励输出的泛化能力以及如何避免测试期间的样本外奖励查询。为了解决这些问题，我们提出了贝叶斯重参数化RCRL（BR-RCRL），这是一种新颖的RCRL归纳偏置设计，灵感来自于贝叶斯定理。",
    "tldr": "该论文提出了BR-RCRL，它是一种贝叶斯重参数化算法，能够解决奖励条件强化学习中的泛化能力和样本外查询问题。",
    "en_tdlr": "This paper proposes BR-RCRL, a Bayesian reparameterization algorithm that is able to address the challenges of generalization and out-of-distribution queries in reward-conditioned reinforcement learning."
}