{
    "title": "Unlocking the Power of Open Set : A New Perspective for Open-set Noisy Label Learning. (arXiv:2305.04203v1 [cs.LG])",
    "abstract": "Learning from noisy data has attracted much attention, where most methods focus on closed-set label noise. However, a more common scenario in the real world is the presence of both open-set and closed-set noise. Existing methods typically identify and handle these two types of label noise separately by designing a specific strategy for each type. However, in many real-world scenarios, it would be challenging to identify open-set examples, especially when the dataset has been severely corrupted. Unlike the previous works, we explore how models behave when faced open-set examples, and find that a part of open-set examples gradually get integrated into certain known classes, which is beneficial for the seperation among known classes. Motivated by the phenomenon, in this paper, we propose a novel two-step contrastive learning method called CECL, which aims to deal with both types of label noise by exploiting the useful information of open-set examples. Specifically, we incorporate some ope",
    "link": "http://arxiv.org/abs/2305.04203",
    "context": "Title: Unlocking the Power of Open Set : A New Perspective for Open-set Noisy Label Learning. (arXiv:2305.04203v1 [cs.LG])\nAbstract: Learning from noisy data has attracted much attention, where most methods focus on closed-set label noise. However, a more common scenario in the real world is the presence of both open-set and closed-set noise. Existing methods typically identify and handle these two types of label noise separately by designing a specific strategy for each type. However, in many real-world scenarios, it would be challenging to identify open-set examples, especially when the dataset has been severely corrupted. Unlike the previous works, we explore how models behave when faced open-set examples, and find that a part of open-set examples gradually get integrated into certain known classes, which is beneficial for the seperation among known classes. Motivated by the phenomenon, in this paper, we propose a novel two-step contrastive learning method called CECL, which aims to deal with both types of label noise by exploiting the useful information of open-set examples. Specifically, we incorporate some ope",
    "path": "papers/23/05/2305.04203.json",
    "total_tokens": 1020,
    "translated_title": "开放集合学习的新视角：解锁开放集合的力量",
    "translated_abstract": "学习嘈杂的数据一直受到人们的关注，其中大多数方法集中在封闭集的标签噪声上。然而，在现实世界中更常见的情况是同时存在开放集合和封闭集合的噪声。现有方法通常通过为每种类型设计特定的策略来区分和处理这两种类型的标签噪声。然而，在许多现实世界的情况下，识别开放集合示例可能是具有挑战性的，特别是当数据集已经严重损坏时。本文对模型面对开放集合示例时的行为进行了探索，并发现部分开放集合示例逐渐融入某些已知类别，这有利于已知类别的分离。在这种现象的推动下，我们提出了一种新的两步对比学习方法CECL，通过利用开放集合示例的有用信息来处理两种类型的标签噪声。具体而言，我们在对比学习框架中将一些开放集合示例作为负例，可以有效地增强模型对开放集合噪声的鲁棒性。在几个基准数据集上进行的实验证明了我们方法处理开放集合和封闭集合标签噪声的有效性。",
    "tldr": "本文提出了一种新的两步对比学习方法CECL，通过利用开放集合示例的有用信息来处理两种类型的标签噪声。该方法在几个基准数据集上得到了验证。",
    "en_tdlr": "The paper proposes a novel two-step contrastive learning method called CECL to deal with both open-set and closed-set label noise by exploiting the useful information of open-set examples. The effectiveness of the method is demonstrated on several benchmark datasets."
}