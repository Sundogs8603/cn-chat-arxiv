{
    "title": "A Comprehensive Analysis of Adapter Efficiency. (arXiv:2305.07491v1 [cs.CL])",
    "abstract": "Adapters have been positioned as a parameter-efficient fine-tuning (PEFT) approach, whereby a minimal number of parameters are added to the model and fine-tuned. However, adapters have not been sufficiently analyzed to understand if PEFT translates to benefits in training/deployment efficiency and maintainability/extensibility. Through extensive experiments on many adapters, tasks, and languages in supervised and cross-lingual zero-shot settings, we clearly show that for Natural Language Understanding (NLU) tasks, the parameter efficiency in adapters does not translate to efficiency gains compared to full fine-tuning of models. More precisely, adapters are relatively expensive to train and have slightly higher deployment latency. Furthermore, the maintainability/extensibility benefits of adapters can be achieved with simpler approaches like multi-task training via full fine-tuning, which also provide relatively faster training times. We, therefore, recommend that for moderately sized m",
    "link": "http://arxiv.org/abs/2305.07491",
    "context": "Title: A Comprehensive Analysis of Adapter Efficiency. (arXiv:2305.07491v1 [cs.CL])\nAbstract: Adapters have been positioned as a parameter-efficient fine-tuning (PEFT) approach, whereby a minimal number of parameters are added to the model and fine-tuned. However, adapters have not been sufficiently analyzed to understand if PEFT translates to benefits in training/deployment efficiency and maintainability/extensibility. Through extensive experiments on many adapters, tasks, and languages in supervised and cross-lingual zero-shot settings, we clearly show that for Natural Language Understanding (NLU) tasks, the parameter efficiency in adapters does not translate to efficiency gains compared to full fine-tuning of models. More precisely, adapters are relatively expensive to train and have slightly higher deployment latency. Furthermore, the maintainability/extensibility benefits of adapters can be achieved with simpler approaches like multi-task training via full fine-tuning, which also provide relatively faster training times. We, therefore, recommend that for moderately sized m",
    "path": "papers/23/05/2305.07491.json",
    "total_tokens": 906,
    "translated_title": "适配器效率的全面分析",
    "translated_abstract": "适配器被定位为一种参数有效的微调方法，只需向模型添加最少量的参数即可进行微调。然而，对于PEFT在训练/部署效率和可维护性/可扩展性方面的优势，适配器并未得到足够的分析。通过对多个适配器、任务和语言的广泛实验，包括监督和跨语言零-shot设置，我们清楚地表明，在自然语言理解（NLU）任务中，适配器的参数效率不等同于与完全微调模型相比的效率提高。更明确地说，适配器的训练成本相对较高，并且具有稍高的部署延迟。此外，可以通过全面微调的多任务训练等简单方法实现适配器的可维护性/可扩展性优势，这也提供了相对较快的培训时间。因此，我们建议对于中等大小的神经网络模型，使用传统的全微调方法进行微调，避免使用适配器。",
    "tldr": "适配器虽然是一种参数有效的微调方法，但是在训练/部署效率和可维护性/可扩展性方面并没有实现PEFT的优势，相对的，可以使用更简单的方法，如多任务训练来实现可维护性/可扩展性的优势。",
    "en_tdlr": "Although adapters are a parameter-efficient fine-tuning method, they do not provide the expected efficiency benefits in training/deployment and maintainability/extensibility. Instead, simpler approaches like multi-task training via full fine-tuning can achieve these benefits."
}