{
    "title": "AnalogNAS: A Neural Network Design Framework for Accurate Inference with Analog In-Memory Computing. (arXiv:2305.10459v1 [cs.AR])",
    "abstract": "The advancement of Deep Learning (DL) is driven by efficient Deep Neural Network (DNN) design and new hardware accelerators. Current DNN design is primarily tailored for general-purpose use and deployment on commercially viable platforms. Inference at the edge requires low latency, compact and power-efficient models, and must be cost-effective. Digital processors based on typical von Neumann architectures are not conducive to edge AI given the large amounts of required data movement in and out of memory. Conversely, analog/mixed signal in-memory computing hardware accelerators can easily transcend the memory wall of von Neuman architectures when accelerating inference workloads. They offer increased area and power efficiency, which are paramount in edge resource-constrained environments. In this paper, we propose AnalogNAS, a framework for automated DNN design targeting deployment on analog In-Memory Computing (IMC) inference accelerators. We conduct extensive hardware simulations to d",
    "link": "http://arxiv.org/abs/2305.10459",
    "context": "Title: AnalogNAS: A Neural Network Design Framework for Accurate Inference with Analog In-Memory Computing. (arXiv:2305.10459v1 [cs.AR])\nAbstract: The advancement of Deep Learning (DL) is driven by efficient Deep Neural Network (DNN) design and new hardware accelerators. Current DNN design is primarily tailored for general-purpose use and deployment on commercially viable platforms. Inference at the edge requires low latency, compact and power-efficient models, and must be cost-effective. Digital processors based on typical von Neumann architectures are not conducive to edge AI given the large amounts of required data movement in and out of memory. Conversely, analog/mixed signal in-memory computing hardware accelerators can easily transcend the memory wall of von Neuman architectures when accelerating inference workloads. They offer increased area and power efficiency, which are paramount in edge resource-constrained environments. In this paper, we propose AnalogNAS, a framework for automated DNN design targeting deployment on analog In-Memory Computing (IMC) inference accelerators. We conduct extensive hardware simulations to d",
    "path": "papers/23/05/2305.10459.json",
    "total_tokens": 927,
    "translated_title": "AnalogNAS: 一种用于采用模拟内存计算实现准确推理的神经网络设计框架",
    "translated_abstract": "深度学习技术的发展离不开高效的深度神经网络设计和新的硬件加速器。目前的深度神经网络设计主要针对常规平台的通用部署。而在边缘推理中，需要低延迟、紧凑且功耗效率高的模型，并且在成本方面必须划算。基于 typica von Neumann 结构的数字处理器在边缘人工智能中运用较为困难，因为需要大量的数据进出内存进行处理。相反，采用模拟/混合信号内存计算硬件加速器可以轻易超越 von Neumann 结构中的内存墙，加速推理工作负载。该硬件加速器可以提供更高的区域和功耗效率，这在边缘资源受限的环境中是至关重要的。本文提出了一种名为 AnalogNAS 的框架，用于自动化设计深度神经网络，并针对模拟内存计算推理加速器的部署进行优化。我们进行了大量的硬件仿真以评估框架的性能。",
    "tldr": "AnalogNAS是一种用于自动化设计深度神经网络的框架，可以针对模拟内存计算推理加速器进行优化，提高在边缘环境下的区域和功耗效率。",
    "en_tdlr": "AnalogNAS is a framework for automated DNN design targeting deployment on analog In-Memory Computing (IMC) inference accelerators, which can optimize the area and power efficiency for edge environments."
}