{
    "title": "ZET-Speech: Zero-shot adaptive Emotion-controllable Text-to-Speech Synthesis with Diffusion and Style-based Models. (arXiv:2305.13831v1 [cs.SD])",
    "abstract": "Emotional Text-To-Speech (TTS) is an important task in the development of systems (e.g., human-like dialogue agents) that require natural and emotional speech. Existing approaches, however, only aim to produce emotional TTS for seen speakers during training, without consideration of the generalization to unseen speakers. In this paper, we propose ZET-Speech, a zero-shot adaptive emotion-controllable TTS model that allows users to synthesize any speaker's emotional speech using only a short, neutral speech segment and the target emotion label. Specifically, to enable a zero-shot adaptive TTS model to synthesize emotional speech, we propose domain adversarial learning and guidance methods on the diffusion model. Experimental results demonstrate that ZET-Speech successfully synthesizes natural and emotional speech with the desired emotion for both seen and unseen speakers. Samples are at https://ZET-Speech.github.io/ZET-Speech-Demo/.",
    "link": "http://arxiv.org/abs/2305.13831",
    "context": "Title: ZET-Speech: Zero-shot adaptive Emotion-controllable Text-to-Speech Synthesis with Diffusion and Style-based Models. (arXiv:2305.13831v1 [cs.SD])\nAbstract: Emotional Text-To-Speech (TTS) is an important task in the development of systems (e.g., human-like dialogue agents) that require natural and emotional speech. Existing approaches, however, only aim to produce emotional TTS for seen speakers during training, without consideration of the generalization to unseen speakers. In this paper, we propose ZET-Speech, a zero-shot adaptive emotion-controllable TTS model that allows users to synthesize any speaker's emotional speech using only a short, neutral speech segment and the target emotion label. Specifically, to enable a zero-shot adaptive TTS model to synthesize emotional speech, we propose domain adversarial learning and guidance methods on the diffusion model. Experimental results demonstrate that ZET-Speech successfully synthesizes natural and emotional speech with the desired emotion for both seen and unseen speakers. Samples are at https://ZET-Speech.github.io/ZET-Speech-Demo/.",
    "path": "papers/23/05/2305.13831.json",
    "total_tokens": 939,
    "translated_title": "ZET-Speech: 基于扩散和基于风格的模型的零样本自适应情感可控文本转语音合成",
    "translated_abstract": "情感语音合成是开发需要自然和情绪语音的系统（例如类人对话代理）中的重要任务。然而，现有的方法只针对训练期间见过的发言人生成情感语音，没有考虑到对未见过的发言人的泛化。本文提出了一种零样本自适应情感可控的语音合成模型ZET-Speech，只需要通过一段短的中性语音和目标情感标签即可合成任何说话者的情感语音。具体而言，我们提出了在扩散模型上进行域对抗学习和指导方法，以实现零样本自适应情感语音合成。实验结果表明，ZET-Speech成功地合成了具有所需情感的自然和情感语音，适用于见过和未见过的发言人。样本在https://ZET-Speech.github.io/ZET-Speech-Demo/上提供。",
    "tldr": "本文提出了一种零样本自适应情感可控的语音合成模型ZET-Speech, 它可以通过一段短的中性语音和目标情感标签合成任何说话者的情感语音，并且成功合成了具有所需情感的自然和情感语音，适用于见过和未见过的发言人。",
    "en_tdlr": "This paper proposes a zero-shot adaptive emotion-controllable text-to-speech synthesis model, ZET-Speech, which can synthesize emotional speech for any speaker using only a short, neutral speech segment and the target emotion label. The proposed model successfully synthesizes natural and emotional speech with the desired emotion for both seen and unseen speakers."
}