{
    "title": "On the Correspondence Between Monotonic Max-Sum GNNs and Datalog. (arXiv:2305.18015v2 [cs.AI] UPDATED)",
    "abstract": "Although there has been significant interest in applying machine learning techniques to structured data, the expressivity (i.e., a description of what can be learned) of such techniques is still poorly understood. In this paper, we study data transformations based on graph neural networks (GNNs). First, we note that the choice of how a dataset is encoded into a numeric form processable by a GNN can obscure the characterisation of a model's expressivity, and we argue that a canonical encoding provides an appropriate basis. Second, we study the expressivity of monotonic max-sum GNNs, which cover a subclass of GNNs with max and sum aggregation functions. We show that, for each such GNN, one can compute a Datalog program such that applying the GNN to any dataset produces the same facts as a single round of application of the program's rules to the dataset. Monotonic max-sum GNNs can sum an unbounded number of feature vectors which can result in arbitrarily large feature values, whereas rul",
    "link": "http://arxiv.org/abs/2305.18015",
    "context": "Title: On the Correspondence Between Monotonic Max-Sum GNNs and Datalog. (arXiv:2305.18015v2 [cs.AI] UPDATED)\nAbstract: Although there has been significant interest in applying machine learning techniques to structured data, the expressivity (i.e., a description of what can be learned) of such techniques is still poorly understood. In this paper, we study data transformations based on graph neural networks (GNNs). First, we note that the choice of how a dataset is encoded into a numeric form processable by a GNN can obscure the characterisation of a model's expressivity, and we argue that a canonical encoding provides an appropriate basis. Second, we study the expressivity of monotonic max-sum GNNs, which cover a subclass of GNNs with max and sum aggregation functions. We show that, for each such GNN, one can compute a Datalog program such that applying the GNN to any dataset produces the same facts as a single round of application of the program's rules to the dataset. Monotonic max-sum GNNs can sum an unbounded number of feature vectors which can result in arbitrarily large feature values, whereas rul",
    "path": "papers/23/05/2305.18015.json",
    "total_tokens": 1081,
    "translated_title": "基于单调最大和的图神经网络与Datalog的对应关系研究",
    "translated_abstract": "虽然应用机器学习技术到结构化数据上引起了广泛的关注，但这些技术的表达能力（即可以学习到什么）仍然不太清楚。本文研究了基于图神经网络的数据变换。首先，我们注意到如何将数据集编码成GNN可处理的数字形式可以模糊模型表达能力的描述，我们认为一种规范编码提供了适当的基础。其次，我们研究了单调最大和的GNN的表现能力，它们涵盖了具有max和sum聚合函数的GNN的一个子类。我们证明了对于每个这样的GNN，可以计算出一个Datalog程序，使得将GNN应用于任何数据集都会产生与将程序规则应用于数据集的单个循环相同的事实。单调最大和的GNN能够对无限数量的特征向量求和，这可能会导致特征值任意增大，而基于规则的系统（例如Datalog）具有保证系统始终会趋于稳定状态的单调性质。我们证明了单调最大和的GNN与Datalog之间的这种关系是紧密的，即任何单调最大和的GNN都可以表示为一个Datalog程序，反之亦然。",
    "tldr": "本文研究了数据变换基于图神经网络的表现能力，证明了单调最大和的GNN与Datalog之间的关系紧密，即任何单调最大和的GNN都可以表示为一个Datalog程序，反之亦然。",
    "en_tdlr": "This paper studies the expressivity of data transformations based on graph neural networks (GNNs), and proves the tight relationship between monotonic max-sum GNNs and Datalog by showing that any monotonic max-sum GNN can be expressed as a Datalog program, and vice versa."
}