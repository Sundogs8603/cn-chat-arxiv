{
    "title": "From Random Search to Bandit Learning in Metric Measure Spaces. (arXiv:2305.11509v1 [cs.LG])",
    "abstract": "Random Search is one of the most widely-used method for Hyperparameter Optimization, and is critical to the success of deep learning models. Despite its astonishing performance, little non-heuristic theory has been developed to describe the underlying working mechanism. This paper gives a theoretical accounting of Random Search. We introduce the concept of \\emph{scattering dimension} that describes the landscape of the underlying function, and quantifies the performance of random search. We show that, when the environment is noise-free, the output of random search converges to the optimal value in probability at rate $ \\widetilde{\\mathcal{O}} \\left( \\left( \\frac{1}{T} \\right)^{ \\frac{1}{d_s} } \\right) $, where $ d_s \\ge 0 $ is the scattering dimension of the underlying function. When the observed function values are corrupted by bounded $iid$ noise, the output of random search converges to the optimal value in probability at rate $ \\widetilde{\\mathcal{O}} \\left( \\left( \\frac{1}{T} \\rig",
    "link": "http://arxiv.org/abs/2305.11509",
    "context": "Title: From Random Search to Bandit Learning in Metric Measure Spaces. (arXiv:2305.11509v1 [cs.LG])\nAbstract: Random Search is one of the most widely-used method for Hyperparameter Optimization, and is critical to the success of deep learning models. Despite its astonishing performance, little non-heuristic theory has been developed to describe the underlying working mechanism. This paper gives a theoretical accounting of Random Search. We introduce the concept of \\emph{scattering dimension} that describes the landscape of the underlying function, and quantifies the performance of random search. We show that, when the environment is noise-free, the output of random search converges to the optimal value in probability at rate $ \\widetilde{\\mathcal{O}} \\left( \\left( \\frac{1}{T} \\right)^{ \\frac{1}{d_s} } \\right) $, where $ d_s \\ge 0 $ is the scattering dimension of the underlying function. When the observed function values are corrupted by bounded $iid$ noise, the output of random search converges to the optimal value in probability at rate $ \\widetilde{\\mathcal{O}} \\left( \\left( \\frac{1}{T} \\rig",
    "path": "papers/23/05/2305.11509.json",
    "total_tokens": 993,
    "translated_title": "从随机搜索到度量测度空间中的赌博学习",
    "translated_abstract": "随机搜索是超参数优化中最常用的方法之一，对于深度学习模型的成功至关重要。尽管其性能令人惊叹，但很少有非启发式的理论用于描述其工作机制。本文给出了关于随机搜索的理论解释。我们引入了“散射维度”的概念，描述了底层函数的状态，并量化了随机搜索的性能。我们表明，当环境没有噪声时，随机搜索的输出以概率收敛到最优值，其速率为$ \\widetilde{\\mathcal{O}} \\left( \\left( \\frac{1}{T} \\right)^{ \\frac{1}{d_s} } \\right) $，其中$ d_s \\ge 0 $是底层函数的散射维度。当观察到的函数值受到有界的独立同分布噪声影响时，随机搜索的输出以概率收敛到最优值，速率为$ \\widetilde{\\mathcal{O}} \\left( \\left( \\frac{1}{T} \\right)^{ \\frac{2}{2+d_s} } \\right) $。",
    "tldr": "本文介绍了随机搜索及其性能，引入了“散射维度”的概念，描述了底层函数的状态，量化了随机搜索的性能，并证明了在无噪声和有界噪声情况下的输出分别以一定概率收敛到最优值。",
    "en_tdlr": "This paper introduces random search and its performance, introduces the concept of \"scattering dimension\" to describe the state of the underlying function and quantify the performance of random search, and proves that the output of random search converges to the optimal value with certain probabilities under noise-free and bounded noise environments,respectively."
}