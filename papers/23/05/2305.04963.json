{
    "title": "From Relational Pooling to Subgraph GNNs: A Universal Framework for More Expressive Graph Neural Networks. (arXiv:2305.04963v1 [cs.LG])",
    "abstract": "Relational pooling is a framework for building more expressive and permutation-invariant graph neural networks. However, there is limited understanding of the exact enhancement in the expressivity of RP and its connection with the Weisfeiler Lehman hierarchy. Starting from RP, we propose to explicitly assign labels to nodes as additional features to improve expressive power of message passing neural networks. The method is then extended to higher dimensional WL, leading to a novel $k,l$-WL algorithm, a more general framework than $k$-WL. Theoretically, we analyze the expressivity of $k,l$-WL with respect to $k$ and $l$ and unifies it with a great number of subgraph GNNs. Complexity reduction methods are also systematically discussed to build powerful and practical $k,l$-GNN instances. We theoretically and experimentally prove that our method is universally compatible and capable of improving the expressivity of any base GNN model. Our $k,l$-GNNs achieve superior performance on many syn",
    "link": "http://arxiv.org/abs/2305.04963",
    "context": "Title: From Relational Pooling to Subgraph GNNs: A Universal Framework for More Expressive Graph Neural Networks. (arXiv:2305.04963v1 [cs.LG])\nAbstract: Relational pooling is a framework for building more expressive and permutation-invariant graph neural networks. However, there is limited understanding of the exact enhancement in the expressivity of RP and its connection with the Weisfeiler Lehman hierarchy. Starting from RP, we propose to explicitly assign labels to nodes as additional features to improve expressive power of message passing neural networks. The method is then extended to higher dimensional WL, leading to a novel $k,l$-WL algorithm, a more general framework than $k$-WL. Theoretically, we analyze the expressivity of $k,l$-WL with respect to $k$ and $l$ and unifies it with a great number of subgraph GNNs. Complexity reduction methods are also systematically discussed to build powerful and practical $k,l$-GNN instances. We theoretically and experimentally prove that our method is universally compatible and capable of improving the expressivity of any base GNN model. Our $k,l$-GNNs achieve superior performance on many syn",
    "path": "papers/23/05/2305.04963.json",
    "total_tokens": 990,
    "translated_title": "从关系池化到子图图神经网络：更具表现力的图神经网络的通用框架",
    "translated_abstract": "关系池化是用于构建更具表现力和置换不变的图神经网络的框架。然而，对于关系池化在表现力方面的确切增强及其与Weisfeiler-Lehman分层的联系的理解是有限的。从关系池化出发，我们提出了将节点明确标记为附加特征以提高消息传递神经网络的表现力的方法。然后将该方法扩展到高维WL，得到一种新的$k,l$-WL算法，比$k$-WL更通用的框架。我们在理论上分析了$k,l$-WL相对于$k$和$l$的表达能力，并将其与大量的子图GNN统一起来。我们还系统地讨论了复杂度降低方法，以构建强大而实用的$k,l$-GNN实例。我们在理论和实验中证明了我们的方法是通用兼容的，并能够提高任何基本GNN模型的表现力。我们的$k,l$-GNN在许多syn上实现了超越性能。",
    "tldr": "本研究提出了一种从关系池化到子图GNN的通用框架，可提高任何基本GNN模型的表现力，通过明确标记节点作为附加特征来实现此目的， 并可在许多syn上实现超越性能。",
    "en_tdlr": "This study proposes a universal framework from relational pooling to subgraph GNNs that can enhance the expressivity of any base GNN model by explicitly labeling nodes as additional features. The method achieves superior performance on many syn and includes a novel k,l-WL algorithm that unifies the expressivity of k,l-WL with a great number of subgraph GNNs. Complexity reduction methods are also discussed systematically."
}