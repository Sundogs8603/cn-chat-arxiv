{
    "title": "Integrating Holistic and Local Information to Estimate Emotional Reaction Intensity. (arXiv:2305.05534v1 [cs.CV])",
    "abstract": "Video-based Emotional Reaction Intensity (ERI) estimation measures the intensity of subjects' reactions to stimuli along several emotional dimensions from videos of the subject as they view the stimuli. We propose a multi-modal architecture for video-based ERI combining video and audio information. Video input is encoded spatially first, frame-by-frame, combining features encoding holistic aspects of the subjects' facial expressions and features encoding spatially localized aspects of their expressions. Input is then combined across time: from frame-to-frame using gated recurrent units (GRUs), then globally by a transformer. We handle variable video length with a regression token that accumulates information from all frames into a fixed-dimensional vector independent of video length. Audio information is handled similarly: spectral information extracted within each frame is integrated across time by a cascade of GRUs and a transformer with regression token. The video and audio regressi",
    "link": "http://arxiv.org/abs/2305.05534",
    "context": "Title: Integrating Holistic and Local Information to Estimate Emotional Reaction Intensity. (arXiv:2305.05534v1 [cs.CV])\nAbstract: Video-based Emotional Reaction Intensity (ERI) estimation measures the intensity of subjects' reactions to stimuli along several emotional dimensions from videos of the subject as they view the stimuli. We propose a multi-modal architecture for video-based ERI combining video and audio information. Video input is encoded spatially first, frame-by-frame, combining features encoding holistic aspects of the subjects' facial expressions and features encoding spatially localized aspects of their expressions. Input is then combined across time: from frame-to-frame using gated recurrent units (GRUs), then globally by a transformer. We handle variable video length with a regression token that accumulates information from all frames into a fixed-dimensional vector independent of video length. Audio information is handled similarly: spectral information extracted within each frame is integrated across time by a cascade of GRUs and a transformer with regression token. The video and audio regressi",
    "path": "papers/23/05/2305.05534.json",
    "total_tokens": 858,
    "translated_title": "合并整体与局部信息估计情感反应强度",
    "translated_abstract": "基于视频的情感反应强度（ERI）估计可以从被试观看刺激的视频中测量出他们对几种情感维度的反应强度。我们提出了一种多模态的视频ERI架构，结合了视频和音频信息。视频输入首先按帧进行空间编码，结合编码主体面部表情的整体特征和编码其表情局部特征的特征。然后通过门控循环单元（GRUs）从帧到帧进行时间上的组合，再通过变压器在全局范围内进行组合。我们通过一个回归标记解决可变视频长度的问题，该标记从所有帧中累积信息以产生独立于视频长度的固定维度向量。音频信息的处理类似：在每个帧内提取的频谱信息通过一系列GRUs和具有回归令牌的变压器的时间整合。视频和音频回归分别与情感反应强度相关联。",
    "tldr": "该论文提出了一种多模态架构，结合视频和音频信息来估计情感反应强度，使用回归标记解决了可变视频长度的问题，实现了对情感反应强度的准确估计。",
    "en_tdlr": "The paper proposes a multi-modal architecture combining video and audio information to estimate emotional reaction intensity (ERI), which handles variable video length with a regression token and achieves accurate estimation of ERI."
}