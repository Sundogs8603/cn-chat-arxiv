{
    "title": "NLNDE at SemEval-2023 Task 12: Adaptive Pretraining and Source Language Selection for Low-Resource Multilingual Sentiment Analysis. (arXiv:2305.00090v1 [cs.CL])",
    "abstract": "This paper describes our system developed for the SemEval-2023 Task 12 \"Sentiment Analysis for Low-resource African Languages using Twitter Dataset\". Sentiment analysis is one of the most widely studied applications in natural language processing. However, most prior work still focuses on a small number of high-resource languages. Building reliable sentiment analysis systems for low-resource languages remains challenging, due to the limited training data in this task. In this work, we propose to leverage language-adaptive and task-adaptive pretraining on African texts and study transfer learning with source language selection on top of an African language-centric pretrained language model. Our key findings are: (1) Adapting the pretrained model to the target language and task using a small yet relevant corpus improves performance remarkably by more than 10 F1 score points. (2) Selecting source languages with positive transfer gains during training can avoid harmful interference from di",
    "link": "http://arxiv.org/abs/2305.00090",
    "context": "Title: NLNDE at SemEval-2023 Task 12: Adaptive Pretraining and Source Language Selection for Low-Resource Multilingual Sentiment Analysis. (arXiv:2305.00090v1 [cs.CL])\nAbstract: This paper describes our system developed for the SemEval-2023 Task 12 \"Sentiment Analysis for Low-resource African Languages using Twitter Dataset\". Sentiment analysis is one of the most widely studied applications in natural language processing. However, most prior work still focuses on a small number of high-resource languages. Building reliable sentiment analysis systems for low-resource languages remains challenging, due to the limited training data in this task. In this work, we propose to leverage language-adaptive and task-adaptive pretraining on African texts and study transfer learning with source language selection on top of an African language-centric pretrained language model. Our key findings are: (1) Adapting the pretrained model to the target language and task using a small yet relevant corpus improves performance remarkably by more than 10 F1 score points. (2) Selecting source languages with positive transfer gains during training can avoid harmful interference from di",
    "path": "papers/23/05/2305.00090.json",
    "total_tokens": 1044,
    "translated_title": "NLNDE在SemEval-2023第12任务中：适应性预训练和来源语言选择用于低资源多语言情感分析",
    "translated_abstract": "本文描述了我们为SemEval-2023第12项任务“使用Twitter数据集进行低资源非洲语言情感分析”开发的系统。情感分析是自然语言处理中研究广泛的应用之一。然而，大多数先前的研究仍然集中在少数高资源语言上。由于此任务中训练数据有限，为低资源语言建立可靠的情感分析系统仍然具有挑战性。在这项工作中，我们提出利用语言自适应和任务自适应预训练非洲文本，并在非洲语言为中心的预训练语言模型的基础上研究源语言选择的迁移学习。我们的关键发现是：（1）使用较小但相关的语料库将预训练模型适应目标语言和任务可以使F1分数提高10个百分点以上。（2）在训练期间选择具有正迁移增益的源语言可以避免来自不同源语言的有害干扰并实现更好的性能。（3）所提出的方法在Afrikaans和Zulu语言的SemEval-2023 Task 12中均取得了最先进的表现。",
    "tldr": "本文研究了低资源多语言情感分析的问题，提出了一种利用适应性预训练和源语言选择的方法，能够显著提高性能，并且在SemEval-2023 Task 12中获得了最先进的表现。",
    "en_tdlr": "This paper proposes a language-adaptive and task-adaptive pretraining method with source language selection to solve the challenging task of building reliable sentiment analysis systems for low-resource languages. The proposed approach achieves remarkable performance improvements and state-of-the-art results on SemEval-2023 Task 12 for both Afrikaans and Zulu languages."
}