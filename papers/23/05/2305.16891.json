{
    "title": "Generalization Guarantees of Gradient Descent for Multi-Layer Neural Networks. (arXiv:2305.16891v1 [cs.LG])",
    "abstract": "Recently, significant progress has been made in understanding the generalization of neural networks (NNs) trained by gradient descent (GD) using the algorithmic stability approach. However, most of the existing research has focused on one-hidden-layer NNs and has not addressed the impact of different network scaling parameters. In this paper, we greatly extend the previous work \\cite{lei2022stability,richards2021stability} by conducting a comprehensive stability and generalization analysis of GD for multi-layer NNs. For two-layer NNs, our results are established under general network scaling parameters, relaxing previous conditions. In the case of three-layer NNs, our technical contribution lies in demonstrating its nearly co-coercive property by utilizing a novel induction strategy that thoroughly explores the effects of over-parameterization. As a direct application of our general findings, we derive the excess risk rate of $O(1/\\sqrt{n})$ for GD algorithms in both two-layer and thre",
    "link": "http://arxiv.org/abs/2305.16891",
    "context": "Title: Generalization Guarantees of Gradient Descent for Multi-Layer Neural Networks. (arXiv:2305.16891v1 [cs.LG])\nAbstract: Recently, significant progress has been made in understanding the generalization of neural networks (NNs) trained by gradient descent (GD) using the algorithmic stability approach. However, most of the existing research has focused on one-hidden-layer NNs and has not addressed the impact of different network scaling parameters. In this paper, we greatly extend the previous work \\cite{lei2022stability,richards2021stability} by conducting a comprehensive stability and generalization analysis of GD for multi-layer NNs. For two-layer NNs, our results are established under general network scaling parameters, relaxing previous conditions. In the case of three-layer NNs, our technical contribution lies in demonstrating its nearly co-coercive property by utilizing a novel induction strategy that thoroughly explores the effects of over-parameterization. As a direct application of our general findings, we derive the excess risk rate of $O(1/\\sqrt{n})$ for GD algorithms in both two-layer and thre",
    "path": "papers/23/05/2305.16891.json",
    "total_tokens": 917,
    "translated_title": "梯度下降在多层神经网络中的泛化保证",
    "translated_abstract": "近年来，通过算法稳定性方法，对梯度下降训练的神经网络（NN）的泛化进行了重大进展。然而，大部分现有研究集中在单隐藏层NN上，并没有解决不同网络缩放参数的影响。本文通过对GD在多层NN上进行全面的稳定性和泛化分析，极大地扩展了以往的工作。对于双层NN，我们的结果是在一般的网络缩放参数下建立的，放宽了以前的条件。对于三层NN，我们的技术贡献在于利用一种新的归纳策略，深入探讨了过度参数化的影响，证明了它的几乎协同约束性。通过我们的一般性发现的直接应用，我们得出了GD算法在双层和三层NN中的过量风险速率为$O(1/\\sqrt{n})$。",
    "tldr": "本论文通过全面的稳定性和泛化分析，在多层神经网络上证明了GD算法的一般性保证，为双层和三层NN推导出了过量风险率，扩展了以往研究。",
    "en_tdlr": "This paper provides comprehensive stability and generalization analysis of the gradient descent algorithm for multi-layer neural networks, and deviates from previous research which focuses on one-hidden-layer NNs. By exploring the role of different network scaling parameters, the study derives the excess risk rate of O(1/sqrt(n)) in both two-layer and three-layer NNs, extending previous research."
}