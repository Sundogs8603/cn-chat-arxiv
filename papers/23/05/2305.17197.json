{
    "title": "Entailment as Robust Self-Learner. (arXiv:2305.17197v1 [cs.CL])",
    "abstract": "Entailment has been recognized as an important metric for evaluating natural language understanding (NLU) models, and recent studies have found that entailment pretraining benefits weakly supervised fine-tuning. In this work, we design a prompting strategy that formulates a number of different NLU tasks as contextual entailment. This approach improves the zero-shot adaptation of pretrained entailment models. Secondly, we notice that self-training entailment-based models with unlabeled data can significantly improve the adaptation performance on downstream tasks. To achieve more stable improvement, we propose the Simple Pseudo-Label Editing (SimPLE) algorithm for better pseudo-labeling quality in self-training. We also found that both pretrained entailment-based models and the self-trained models are robust against adversarial evaluation data. Experiments on binary and multi-class classification tasks show that SimPLE leads to more robust self-training results, indicating that the self-",
    "link": "http://arxiv.org/abs/2305.17197",
    "context": "Title: Entailment as Robust Self-Learner. (arXiv:2305.17197v1 [cs.CL])\nAbstract: Entailment has been recognized as an important metric for evaluating natural language understanding (NLU) models, and recent studies have found that entailment pretraining benefits weakly supervised fine-tuning. In this work, we design a prompting strategy that formulates a number of different NLU tasks as contextual entailment. This approach improves the zero-shot adaptation of pretrained entailment models. Secondly, we notice that self-training entailment-based models with unlabeled data can significantly improve the adaptation performance on downstream tasks. To achieve more stable improvement, we propose the Simple Pseudo-Label Editing (SimPLE) algorithm for better pseudo-labeling quality in self-training. We also found that both pretrained entailment-based models and the self-trained models are robust against adversarial evaluation data. Experiments on binary and multi-class classification tasks show that SimPLE leads to more robust self-training results, indicating that the self-",
    "path": "papers/23/05/2305.17197.json",
    "total_tokens": 870,
    "translated_title": "认知作为强健的自学者",
    "translated_abstract": "认知已被认为是评估自然语言理解（NLU）模型重要的指标，近期研究发现，认知预训练有利于弱监督微调。本文设计了一种提示策略，将许多不同的NLU任务制定为情境认知。该方法提高了预训练认知模型的零-shot适应性。此外，我们发现使用无标签数据的自我训练认知模型可以显着提高下游任务的适应性性能。为了实现更稳定的改进，我们提出了简单的伪标签编辑（SimPLE）算法，以提高自我训练中的伪标签质量。我们还发现，预训练的认知模型和自我训练的模型都对对抗性评估数据具有强健性。",
    "tldr": "本文提出了一种将许多不同的NLU任务制定为情境认知的提示策略，并通过自我训练来提高模型的适应性性能。简单的伪标签编辑（SimPLE）算法有利于自我训练的稳定改进。",
    "en_tdlr": "The paper proposes a prompting strategy that formulates various NLU tasks as contextual entailment, and improves the adaptation performance of pretrained entailment models by self-training with unlabeled data. The Simple Pseudo-Label Editing (SimPLE) algorithm is proposed for better pseudo-labeling quality in self-training. Both pretrained entailment-based models and self-trained models are found to be robust against adversarial evaluation data."
}