{
    "title": "Self-supervised learning for infant cry analysis. (arXiv:2305.01578v1 [cs.SD])",
    "abstract": "In this paper, we explore self-supervised learning (SSL) for analyzing a first-of-its-kind database of cry recordings containing clinical indications of more than a thousand newborns. Specifically, we target cry-based detection of neurological injury as well as identification of cry triggers such as pain, hunger, and discomfort. Annotating a large database in the medical setting is expensive and time-consuming, typically requiring the collaboration of several experts over years. Leveraging large amounts of unlabeled audio data to learn useful representations can lower the cost of building robust models and, ultimately, clinical solutions. In this work, we experiment with self-supervised pre-training of a convolutional neural network on large audio datasets. We show that pre-training with SSL contrastive loss (SimCLR) performs significantly better than supervised pre-training for both neuro injury and cry triggers. In addition, we demonstrate further performance gains through SSL-based ",
    "link": "http://arxiv.org/abs/2305.01578",
    "context": "Title: Self-supervised learning for infant cry analysis. (arXiv:2305.01578v1 [cs.SD])\nAbstract: In this paper, we explore self-supervised learning (SSL) for analyzing a first-of-its-kind database of cry recordings containing clinical indications of more than a thousand newborns. Specifically, we target cry-based detection of neurological injury as well as identification of cry triggers such as pain, hunger, and discomfort. Annotating a large database in the medical setting is expensive and time-consuming, typically requiring the collaboration of several experts over years. Leveraging large amounts of unlabeled audio data to learn useful representations can lower the cost of building robust models and, ultimately, clinical solutions. In this work, we experiment with self-supervised pre-training of a convolutional neural network on large audio datasets. We show that pre-training with SSL contrastive loss (SimCLR) performs significantly better than supervised pre-training for both neuro injury and cry triggers. In addition, we demonstrate further performance gains through SSL-based ",
    "path": "papers/23/05/2305.01578.json",
    "total_tokens": 929,
    "translated_abstract": "本文探讨了在一个首例包含超过一千名新生儿临床指征的啼哭记录数据库中使用自监督学习（SSL）进行分析。具体而言，我们针对基于啼哭的神经损伤检测以及鉴别疼痛、饥饿和不适等啼哭的触发器。在医疗环境中，注释大型数据库是昂贵且耗时的，通常需要多个专家数年的合作。利用大量未标记的音频数据学习有用的表示可以降低构建强健模型和最终临床解决方案的成本。本文尝试在大型音频数据集上进行自监督卷积神经网络的预训练实验。我们表明，使用自监督对比度损失（SimCLR）进行预训练在神经系统损伤和啼哭触发器的检测上都比有监督的预训练表现更好。另外，我们通过基于SSL的数据增强方法展示了更进一步的性能提升。",
    "tldr": "本文使用自监督学习方法对大量未标记的婴儿啼哭数据进行分析，取得了较好的神经损伤及触发器识别效果，这一方法可以降低注释成本，提高医疗领域的实用性。",
    "en_tdlr": "This paper utilizes self-supervised learning to analyze a large amount of unannotated cry data of newborns, achieving significant success in detecting neurological injury and cry triggers. This method reduces the cost of annotation and improves the practicality of medical solutions."
}