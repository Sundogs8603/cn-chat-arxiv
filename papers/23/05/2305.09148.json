{
    "title": "Dual-Alignment Pre-training for Cross-lingual Sentence Embedding. (arXiv:2305.09148v1 [cs.CL])",
    "abstract": "Recent studies have shown that dual encoder models trained with the sentence-level translation ranking task are effective methods for cross-lingual sentence embedding. However, our research indicates that token-level alignment is also crucial in multilingual scenarios, which has not been fully explored previously. Based on our findings, we propose a dual-alignment pre-training (DAP) framework for cross-lingual sentence embedding that incorporates both sentence-level and token-level alignment. To achieve this, we introduce a novel representation translation learning (RTL) task, where the model learns to use one-side contextualized token representation to reconstruct its translation counterpart. This reconstruction objective encourages the model to embed translation information into the token representation. Compared to other token-level alignment methods such as translation language modeling, RTL is more suitable for dual encoder architectures and is computationally efficient. Extensive",
    "link": "http://arxiv.org/abs/2305.09148",
    "context": "Title: Dual-Alignment Pre-training for Cross-lingual Sentence Embedding. (arXiv:2305.09148v1 [cs.CL])\nAbstract: Recent studies have shown that dual encoder models trained with the sentence-level translation ranking task are effective methods for cross-lingual sentence embedding. However, our research indicates that token-level alignment is also crucial in multilingual scenarios, which has not been fully explored previously. Based on our findings, we propose a dual-alignment pre-training (DAP) framework for cross-lingual sentence embedding that incorporates both sentence-level and token-level alignment. To achieve this, we introduce a novel representation translation learning (RTL) task, where the model learns to use one-side contextualized token representation to reconstruct its translation counterpart. This reconstruction objective encourages the model to embed translation information into the token representation. Compared to other token-level alignment methods such as translation language modeling, RTL is more suitable for dual encoder architectures and is computationally efficient. Extensive",
    "path": "papers/23/05/2305.09148.json",
    "total_tokens": 903,
    "translated_title": "双重对齐预训练用于跨语言句子嵌入",
    "translated_abstract": "最近的研究表明，使用句子级别翻译排名任务训练的双编码器模型是跨语言句子嵌入的有效方法。然而，我们的研究表明，在多语言场景中，标记级别的对齐也是至关重要的，但此前尚未完全探索这一问题。基于我们的发现，我们提出了一个双重对齐预训练（DAP）框架，用于跨语言句子嵌入，它结合了句子级别和标记级别的对齐。为此，我们引入了一种新颖的表示翻译学习（RTL）任务，其中模型学习使用单侧上下文化的标记表示来重建其翻译对应物。这种重建目标鼓励模型将翻译信息嵌入标记表示中。与其他标记级别对齐方法（如翻译语言模型）相比，RTL更适合双编码器体系结构，而且计算效率更高。",
    "tldr": "该论文提出了一个双重对齐预训练框架，用于跨语言句子嵌入，它结合了句子级别和标记级别的对齐。引入了一种表示翻译学习任务，从而将翻译信息嵌入标记表示中。",
    "en_tdlr": "This paper proposes a dual-alignment pre-training framework for cross-lingual sentence embedding that incorporates both sentence-level and token-level alignment. It introduces a novel representation translation learning task to embed translation information into the token representation."
}