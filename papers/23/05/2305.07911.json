{
    "title": "Delay-Adapted Policy Optimization and Improved Regret for Adversarial MDP with Delayed Bandit Feedback. (arXiv:2305.07911v1 [cs.LG])",
    "abstract": "Policy Optimization (PO) is one of the most popular methods in Reinforcement Learning (RL). Thus, theoretical guarantees for PO algorithms have become especially important to the RL community. In this paper, we study PO in adversarial MDPs with a challenge that arises in almost every real-world application -- \\textit{delayed bandit feedback}. We give the first near-optimal regret bounds for PO in tabular MDPs, and may even surpass state-of-the-art (which uses less efficient methods). Our novel Delay-Adapted PO (DAPO) is easy to implement and to generalize, allowing us to extend our algorithm to: (i) infinite state space under the assumption of linear $Q$-function, proving the first regret bounds for delayed feedback with function approximation. (ii) deep RL, demonstrating its effectiveness in experiments on MuJoCo domains.",
    "link": "http://arxiv.org/abs/2305.07911",
    "context": "Title: Delay-Adapted Policy Optimization and Improved Regret for Adversarial MDP with Delayed Bandit Feedback. (arXiv:2305.07911v1 [cs.LG])\nAbstract: Policy Optimization (PO) is one of the most popular methods in Reinforcement Learning (RL). Thus, theoretical guarantees for PO algorithms have become especially important to the RL community. In this paper, we study PO in adversarial MDPs with a challenge that arises in almost every real-world application -- \\textit{delayed bandit feedback}. We give the first near-optimal regret bounds for PO in tabular MDPs, and may even surpass state-of-the-art (which uses less efficient methods). Our novel Delay-Adapted PO (DAPO) is easy to implement and to generalize, allowing us to extend our algorithm to: (i) infinite state space under the assumption of linear $Q$-function, proving the first regret bounds for delayed feedback with function approximation. (ii) deep RL, demonstrating its effectiveness in experiments on MuJoCo domains.",
    "path": "papers/23/05/2305.07911.json",
    "total_tokens": 931,
    "translated_title": "具有延迟赌博反馈的对抗MDP的延迟自适应策略优化和改进后悔",
    "translated_abstract": "策略优化（PO）是强化学习（RL）中最流行的方法之一。因此，对于PO算法的理论保证已经成为RL界尤为重要的问题。本文研究了具有几乎所有实际应用中都存在的挑战的对抗MDP中的PO - \\textit{延迟赌博反馈} 。我们为tabular MDP中的PO提供了第一个近乎最优的后悔上界，甚至可能超过了状态-of-the-art，而其使用的是不太有效的方法。我们的新颖的Delay-Adapted PO（DAPO）易于实现和推广，使我们能够将我们的算法扩展到：（i）在线性$Q$函数的假设下具有无限状态空间，证明了具有函数逼近的延迟反馈的第一个后悔界；（ii）深度RL，在MuJoCo领域的实验中证明了其有效性。",
    "tldr": "本文研究了具有延迟赌博反馈的对抗MDP中的PO，提出了一个新颖的延迟自适应策略优化算法DAPO，并给出了在tabular MDP和具有函数逼近的无限状态空间中的近乎最优后悔上界。",
    "en_tdlr": "This paper studies PO in adversarial MDPs with delayed bandit feedback, proposes a novel Delay-Adapted PO algorithm (DAPO), and gives near-optimal regret bounds for tabular MDPs and infinite state space with function approximation. DAPO is easy to implement and effective in experiments on MuJoCo domains."
}