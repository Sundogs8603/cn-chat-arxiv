{
    "title": "AD-KD: Attribution-Driven Knowledge Distillation for Language Model Compression. (arXiv:2305.10010v1 [cs.CL])",
    "abstract": "Knowledge distillation has attracted a great deal of interest recently to compress pre-trained language models. However, existing knowledge distillation methods suffer from two limitations. First, the student model simply imitates the teacher's behavior while ignoring the underlying reasoning. Second, these methods usually focus on the transfer of sophisticated model-specific knowledge but overlook data-specific knowledge. In this paper, we present a novel attribution-driven knowledge distillation approach, which explores the token-level rationale behind the teacher model based on Integrated Gradients (IG) and transfers attribution knowledge to the student model. To enhance the knowledge transfer of model reasoning and generalization, we further explore multi-view attribution distillation on all potential decisions of the teacher. Comprehensive experiments are conducted with BERT on the GLUE benchmark. The experimental results demonstrate the superior performance of our approach to sev",
    "link": "http://arxiv.org/abs/2305.10010",
    "context": "Title: AD-KD: Attribution-Driven Knowledge Distillation for Language Model Compression. (arXiv:2305.10010v1 [cs.CL])\nAbstract: Knowledge distillation has attracted a great deal of interest recently to compress pre-trained language models. However, existing knowledge distillation methods suffer from two limitations. First, the student model simply imitates the teacher's behavior while ignoring the underlying reasoning. Second, these methods usually focus on the transfer of sophisticated model-specific knowledge but overlook data-specific knowledge. In this paper, we present a novel attribution-driven knowledge distillation approach, which explores the token-level rationale behind the teacher model based on Integrated Gradients (IG) and transfers attribution knowledge to the student model. To enhance the knowledge transfer of model reasoning and generalization, we further explore multi-view attribution distillation on all potential decisions of the teacher. Comprehensive experiments are conducted with BERT on the GLUE benchmark. The experimental results demonstrate the superior performance of our approach to sev",
    "path": "papers/23/05/2305.10010.json",
    "total_tokens": 889,
    "translated_title": "基于归因的知识蒸馏：语言模型压缩的方法",
    "translated_abstract": "知识蒸馏近来吸引了很多关注，因为它可以压缩预训练的语言模型。然而，现有的知识蒸馏方法存在两个限制：一是学生模型仅模仿教师的行为而忽略其潜在的推理过程；二是这些方法通常关注模型特定的复杂知识转移，但忽视数据特定的知识。本文提出了一种新的基于归因的知识蒸馏方法，通过 Integrated Gradients(IG) 探索教师模型背后的令牌级别的原理，并将归因知识转移到学生模型。为了增强模型推理和泛化的知识转移，我们进一步研究了教师的所有潜在决策的多视角归因蒸馏。我们还在GLUE基准测试中使用BERT进行了全面实验。实验结果表明，我们的方法比现有的方法具有更好的性能。",
    "tldr": "本文提出了一种基于归因的知识蒸馏方法，通过 Integrated Gradients 探索教师模型的原理，将归因知识转移到学生模型，并在 GLUE 基准测试中使用BERT进行了全面实验，结果表明我们的方法具有更好的性能。",
    "en_tdlr": "This paper proposes an attribution-driven knowledge distillation approach that explores the token-level rationale of the teacher model and transfers attribution knowledge to the student model. Comprehensive experiments with BERT on the GLUE benchmark demonstrate superior performance compared to existing methods."
}