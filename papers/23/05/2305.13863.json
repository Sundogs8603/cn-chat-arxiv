{
    "title": "Probing Brain Context-Sensitivity with Masked-Attention Generation. (arXiv:2305.13863v1 [cs.CL])",
    "abstract": "Two fundamental questions in neurolinguistics concerns the brain regions that integrate information beyond the lexical level, and the size of their window of integration. To address these questions we introduce a new approach named masked-attention generation. It uses GPT-2 transformers to generate word embeddings that capture a fixed amount of contextual information. We then tested whether these embeddings could predict fMRI brain activity in humans listening to naturalistic text. The results showed that most of the cortex within the language network is sensitive to contextual information, and that the right hemisphere is more sensitive to longer contexts than the left. Masked-attention generation supports previous analyses of context-sensitivity in the brain, and complements them by quantifying the window size of context integration per voxel.",
    "link": "http://arxiv.org/abs/2305.13863",
    "context": "Title: Probing Brain Context-Sensitivity with Masked-Attention Generation. (arXiv:2305.13863v1 [cs.CL])\nAbstract: Two fundamental questions in neurolinguistics concerns the brain regions that integrate information beyond the lexical level, and the size of their window of integration. To address these questions we introduce a new approach named masked-attention generation. It uses GPT-2 transformers to generate word embeddings that capture a fixed amount of contextual information. We then tested whether these embeddings could predict fMRI brain activity in humans listening to naturalistic text. The results showed that most of the cortex within the language network is sensitive to contextual information, and that the right hemisphere is more sensitive to longer contexts than the left. Masked-attention generation supports previous analyses of context-sensitivity in the brain, and complements them by quantifying the window size of context integration per voxel.",
    "path": "papers/23/05/2305.13863.json",
    "total_tokens": 849,
    "translated_title": "探究脑区上下文敏感性：基于Masked-Attention生成的研究",
    "translated_abstract": "神经语言学中的两个基本问题是超越词汇层次整合信息的脑区以及它们的整合窗口大小。为了解决这些问题，我们引入了一种名为Masked-Attention生成的新方法。它使用GPT-2变形器生成捕获固定量上下文信息的词嵌入。然后我们测试这些嵌入能否预测人类听自然文本时的fMRI脑活动。结果显示，语言网络中的大多数皮层对上下文信息敏感，右半球比左半球更加敏感于更长的上下文。Masked-Attention生成支持之前在大脑上下文敏感性分析方面的研究，并通过量化每个体素的上下文整合窗口大小来补充它们。",
    "tldr": "本文介绍了一种新方法Masked-Attention生成，使用GPT-2变形器生成的固定量上下文信息的词嵌入可以预测人类听自然语言时的fMRI脑活动，结果表明语言网络的大多数皮层对上下文信息敏感，右半球对更长的上下文更敏感。",
    "en_tdlr": "The study introduces a new approach called masked-attention generation and shows that most of the cortex within the language network is sensitive to contextual information, and that the right hemisphere is more sensitive to longer contexts than the left through predicting fMRI brain activity using GPT-2 transformers to generate word embeddings that capture a fixed amount of contextual information."
}