{
    "title": "The Case Against Explainability. (arXiv:2305.12167v1 [cs.AI])",
    "abstract": "As artificial intelligence (AI) becomes more prevalent there is a growing demand from regulators to accompany decisions made by such systems with explanations. However, a persistent gap exists between the need to execute a meaningful right to explanation vs. the ability of Machine Learning systems to deliver on such a legal requirement. The regulatory appeal towards \"a right to explanation\" of AI systems can be attributed to the significant role of explanations, part of the notion called reason-giving, in law. Therefore, in this work we examine reason-giving's purposes in law to analyze whether reasons provided by end-user Explainability can adequately fulfill them.  We find that reason-giving's legal purposes include: (a) making a better and more just decision, (b) facilitating due-process, (c) authenticating human agency, and (d) enhancing the decision makers' authority. Using this methodology, we demonstrate end-user Explainabilty's inadequacy to fulfil reason-giving's role in law, ",
    "link": "http://arxiv.org/abs/2305.12167",
    "context": "Title: The Case Against Explainability. (arXiv:2305.12167v1 [cs.AI])\nAbstract: As artificial intelligence (AI) becomes more prevalent there is a growing demand from regulators to accompany decisions made by such systems with explanations. However, a persistent gap exists between the need to execute a meaningful right to explanation vs. the ability of Machine Learning systems to deliver on such a legal requirement. The regulatory appeal towards \"a right to explanation\" of AI systems can be attributed to the significant role of explanations, part of the notion called reason-giving, in law. Therefore, in this work we examine reason-giving's purposes in law to analyze whether reasons provided by end-user Explainability can adequately fulfill them.  We find that reason-giving's legal purposes include: (a) making a better and more just decision, (b) facilitating due-process, (c) authenticating human agency, and (d) enhancing the decision makers' authority. Using this methodology, we demonstrate end-user Explainabilty's inadequacy to fulfil reason-giving's role in law, ",
    "path": "papers/23/05/2305.12167.json",
    "total_tokens": 846,
    "translated_title": "反对解释性论文",
    "translated_abstract": "随着人工智能(AI)的普及，监管机构对于这些系统所做决策的解释要求日益增加。然而，在可解释性机器学习系统实现有意义的解释权利的需求和能力之间仍存在巨大的差距。AI系统“解释权利”的监管呼声可归因于法律中解释的重要作用。因此，在本文中，我们研究了法律中解释的目的，以分析终端用户可解释性所提供的原因是否足以满足这些目的。我们发现法律中解释的目的包括：(a)做出更好的、更公正的决定，(b)促进正当程序，(c)鉴定人类机构，(d)增强决策者的权威。通过这种方法，我们证明了终端用户可解释性的不足之处。",
    "tldr": "研究发现人工智能系统的解释权利难以充分解释法律目的，涉及到公正判决、正当程序、人类机构认证和决策者权威提升。",
    "en_tdlr": "The paper argues that the \"right to explanation\" of AI systems is difficult to fulfill the legal purposes of reason-giving, including making fair and just decisions, facilitating due-process, authenticating human agency, and enhancing the decision makers' authority."
}