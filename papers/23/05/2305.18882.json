{
    "title": "What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?. (arXiv:2305.18882v2 [cs.LG] UPDATED)",
    "abstract": "Offline goal-conditioned RL (GCRL) offers a way to train general-purpose agents from fully offline datasets. In addition to being conservative within the dataset, the generalization ability to achieve unseen goals is another fundamental challenge for offline GCRL. However, to the best of our knowledge, this problem has not been well studied yet. In this paper, we study out-of-distribution (OOD) generalization of offline GCRL both theoretically and empirically to identify factors that are important. In a number of experiments, we observe that weighted imitation learning enjoys better generalization than pessimism-based offline RL method. Based on this insight, we derive a theory for OOD generalization, which characterizes several important design choices. We then propose a new offline GCRL method, Generalizable Offline goAl-condiTioned RL (GOAT), by combining the findings from our theoretical and empirical studies. On a new benchmark containing 9 independent identically distributed (IID",
    "link": "http://arxiv.org/abs/2305.18882",
    "context": "Title: What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?. (arXiv:2305.18882v2 [cs.LG] UPDATED)\nAbstract: Offline goal-conditioned RL (GCRL) offers a way to train general-purpose agents from fully offline datasets. In addition to being conservative within the dataset, the generalization ability to achieve unseen goals is another fundamental challenge for offline GCRL. However, to the best of our knowledge, this problem has not been well studied yet. In this paper, we study out-of-distribution (OOD) generalization of offline GCRL both theoretically and empirically to identify factors that are important. In a number of experiments, we observe that weighted imitation learning enjoys better generalization than pessimism-based offline RL method. Based on this insight, we derive a theory for OOD generalization, which characterizes several important design choices. We then propose a new offline GCRL method, Generalizable Offline goAl-condiTioned RL (GOAT), by combining the findings from our theoretical and empirical studies. On a new benchmark containing 9 independent identically distributed (IID",
    "path": "papers/23/05/2305.18882.json",
    "total_tokens": 961,
    "translated_title": "离线目标条件强化学习的目标泛化所必需的因素是什么？",
    "translated_abstract": "离线目标条件强化学习（GCRL）提供了一种从完全离线数据集中训练通用代理的方法。除了在数据集内保守之外，实现未见过的目标的泛化能力是离线GCRL的另一个基本挑战。然而，据我们所知，这个问题尚未得到很好的研究。在本文中，我们从理论和实证两方面研究了离线GCRL的分布外泛化，以确定重要的因素。在一些实验中，我们观察到加权模仿学习比基于悲观离线RL方法的泛化性能更好。基于这个见解，我们推导了一个关于分布外泛化的理论，阐明了几个重要的设计选择。然后，我们提出了一个新的离线GCRL方法，即通用离线目标条件RL（GOAT），通过结合我们理论和实证研究的发现。在一个包含 9 个独立同分布（IID）数据分布的新基准测试中，我们展示了GOAT的有效性和通用性。",
    "tldr": "本文研究了离线目标条件强化学习的目标泛化，提出了一种新的离线GCRL方法GOAT，结合理论和实验结果，加权模仿学习比基于悲观离线RL方法的泛化性能更好。",
    "en_tdlr": "This paper studies the out-of-distribution generalization of offline goal-conditioned reinforcement learning (GCRL) and proposes a new method, Generalizable Offline goAl-condiTioned RL (GOAT), which combines theoretical and empirical insights to achieve better generalization performance than pessimism-based offline RL method through weighted imitation learning."
}