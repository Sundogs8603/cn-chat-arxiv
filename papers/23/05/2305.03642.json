{
    "title": "Jointly Extracting Interventions, Outcomes, and Findings from RCT Reports with LLMs. (arXiv:2305.03642v1 [cs.CL])",
    "abstract": "Results from Randomized Controlled Trials (RCTs) establish the comparative effectiveness of interventions, and are in turn critical inputs for evidence-based care. However, results from RCTs are presented in (often unstructured) natural language articles describing the design, execution, and outcomes of trials; clinicians must manually extract findings pertaining to interventions and outcomes of interest from such articles. This onerous manual process has motivated work on (semi-)automating extraction of structured evidence from trial reports. In this work we propose and evaluate a text-to-text model built on instruction-tuned Large Language Models (LLMs) to jointly extract Interventions, Outcomes, and Comparators (ICO elements) from clinical abstracts, and infer the associated results reported. Manual (expert) and automated evaluations indicate that framing evidence extraction as a conditional generation task and fine-tuning LLMs for this purpose realizes considerable ($\\sim$20 point ",
    "link": "http://arxiv.org/abs/2305.03642",
    "context": "Title: Jointly Extracting Interventions, Outcomes, and Findings from RCT Reports with LLMs. (arXiv:2305.03642v1 [cs.CL])\nAbstract: Results from Randomized Controlled Trials (RCTs) establish the comparative effectiveness of interventions, and are in turn critical inputs for evidence-based care. However, results from RCTs are presented in (often unstructured) natural language articles describing the design, execution, and outcomes of trials; clinicians must manually extract findings pertaining to interventions and outcomes of interest from such articles. This onerous manual process has motivated work on (semi-)automating extraction of structured evidence from trial reports. In this work we propose and evaluate a text-to-text model built on instruction-tuned Large Language Models (LLMs) to jointly extract Interventions, Outcomes, and Comparators (ICO elements) from clinical abstracts, and infer the associated results reported. Manual (expert) and automated evaluations indicate that framing evidence extraction as a conditional generation task and fine-tuning LLMs for this purpose realizes considerable ($\\sim$20 point ",
    "path": "papers/23/05/2305.03642.json",
    "total_tokens": 1221,
    "translated_title": "LLM模型共同提取RCT报告中干预、结果和发现信息",
    "translated_abstract": "随机对照试验（RCT）的结果确定干预措施的相对有效性，进而成为基于证据的医疗保健的关键输入。然而，RCT结果以（通常是非结构化的）自然语言文章的形式呈现，描述试验的设计、执行和结果；临床医生必须从这些文章中手动提取有关所关注的干预措施和结果的发现。这种繁琐的手动过程促使人们利用（半）自动化的方式从试验报告中提取结构化证据。在这项工作中，我们提出并评估了一个基于调整的大型语言模型（LLMs）的文本到文本模型，用于从临床摘要中共同提取干预措施、结果和比较因素（ICO元素），并推断相关的结果。人工（专家）和自动评估表明，将证据提取框架作为条件生成任务，为此目的微调LLMs可以实现相当大的（约20个点）性能提升。",
    "tldr": "本文提出了一种基于LLM调整的文本到文本模型，共同提取RCT报告中的干预、结果和发现信息，实现相当大的性能提升。",
    "en_tdlr": "This paper proposes and evaluates a text-to-text model based on instruction-tuned Large Language Models (LLMs) to jointly extract Interventions, Outcomes, and Comparators (ICO elements) from clinical abstracts, and achieves significant performance improvements."
}