{
    "title": "Two Sides of One Coin: the Limits of Untuned SGD and the Power of Adaptive Methods. (arXiv:2305.12475v1 [math.OC])",
    "abstract": "The classical analysis of Stochastic Gradient Descent (SGD) with polynomially decaying stepsize $\\eta_t = \\eta/\\sqrt{t}$ relies on well-tuned $\\eta$ depending on problem parameters such as Lipschitz smoothness constant, which is often unknown in practice. In this work, we prove that SGD with arbitrary $\\eta > 0$, referred to as untuned SGD, still attains an order-optimal convergence rate $\\widetilde{O}(T^{-1/4})$ in terms of gradient norm for minimizing smooth objectives. Unfortunately, it comes at the expense of a catastrophic exponential dependence on the smoothness constant, which we show is unavoidable for this scheme even in the noiseless setting. We then examine three families of adaptive methods $\\unicode{x2013}$ Normalized SGD (NSGD), AMSGrad, and AdaGrad $\\unicode{x2013}$ unveiling their power in preventing such exponential dependency in the absence of information about the smoothness parameter and boundedness of stochastic gradients. Our results provide theoretical justificat",
    "link": "http://arxiv.org/abs/2305.12475",
    "context": "Title: Two Sides of One Coin: the Limits of Untuned SGD and the Power of Adaptive Methods. (arXiv:2305.12475v1 [math.OC])\nAbstract: The classical analysis of Stochastic Gradient Descent (SGD) with polynomially decaying stepsize $\\eta_t = \\eta/\\sqrt{t}$ relies on well-tuned $\\eta$ depending on problem parameters such as Lipschitz smoothness constant, which is often unknown in practice. In this work, we prove that SGD with arbitrary $\\eta > 0$, referred to as untuned SGD, still attains an order-optimal convergence rate $\\widetilde{O}(T^{-1/4})$ in terms of gradient norm for minimizing smooth objectives. Unfortunately, it comes at the expense of a catastrophic exponential dependence on the smoothness constant, which we show is unavoidable for this scheme even in the noiseless setting. We then examine three families of adaptive methods $\\unicode{x2013}$ Normalized SGD (NSGD), AMSGrad, and AdaGrad $\\unicode{x2013}$ unveiling their power in preventing such exponential dependency in the absence of information about the smoothness parameter and boundedness of stochastic gradients. Our results provide theoretical justificat",
    "path": "papers/23/05/2305.12475.json",
    "total_tokens": 1004,
    "translated_title": "单调学习率 SGD 的限制和自适应方法的威力",
    "translated_abstract": "随机梯度下降（SGD）算法的经典分析依赖于经过细调的多项式衰减步长，而这需要依赖于问题参数，例如李普希茨平滑常数，而在实践中这通常是未知的。在本文中，我们证明了对于任意的 $\\eta > 0$ 的单调学习率 SGD（untuned SGD），其依然能够达到渐近最优收敛速度 $ \\tilde{O}(T^{-1/4}) $。然而，这是以光滑度常数出现灾难性指数级依赖为代价的，我们证明了即使在无噪声情况下，这种方案也是不可避免的。接着我们研究了三种自适应方法——归一化的 SGD（NSGD），AMSGrad 和 AdaGrad——揭示了它们在防止如此指数依赖存在时的威力。我们的结果提供了理论上的证明，支持了实践中使用这些方法进行无需更多信息的自适应学习的必要性。",
    "tldr": "本文证明了单调学习率 SGD 的算法依然可以达到渐近最优收敛速度，但是这是以光滑度常数出现灾难性指数级依赖为代价的，因此研究了三种自适应方法来防止这种指数依赖，实现了了准确性和自适应性的平衡。"
}