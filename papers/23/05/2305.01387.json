{
    "title": "Efficient Federated Learning with Enhanced Privacy via Lottery Ticket Pruning in Edge Computing. (arXiv:2305.01387v1 [cs.DC])",
    "abstract": "Federated learning (FL) is a collaborative learning paradigm for decentralized private data from mobile terminals (MTs). However, it suffers from issues in terms of communication, resource of MTs, and privacy. Existing privacy-preserving FL methods usually adopt the instance-level differential privacy (DP), which provides a rigorous privacy guarantee but with several bottlenecks: severe performance degradation, transmission overhead, and resource constraints of edge devices such as MTs. To overcome these drawbacks, we propose Fed-LTP, an efficient and privacy-enhanced FL framework with \\underline{\\textbf{L}}ottery \\underline{\\textbf{T}}icket \\underline{\\textbf{H}}ypothesis (LTH) and zero-concentrated D\\underline{\\textbf{P}} (zCDP). It generates a pruned global model on the server side and conducts sparse-to-sparse training from scratch with zCDP on the client side. On the server side, two pruning schemes are proposed: (i) the weight-based pruning (LTH) determines the pruned global mode",
    "link": "http://arxiv.org/abs/2305.01387",
    "context": "Title: Efficient Federated Learning with Enhanced Privacy via Lottery Ticket Pruning in Edge Computing. (arXiv:2305.01387v1 [cs.DC])\nAbstract: Federated learning (FL) is a collaborative learning paradigm for decentralized private data from mobile terminals (MTs). However, it suffers from issues in terms of communication, resource of MTs, and privacy. Existing privacy-preserving FL methods usually adopt the instance-level differential privacy (DP), which provides a rigorous privacy guarantee but with several bottlenecks: severe performance degradation, transmission overhead, and resource constraints of edge devices such as MTs. To overcome these drawbacks, we propose Fed-LTP, an efficient and privacy-enhanced FL framework with \\underline{\\textbf{L}}ottery \\underline{\\textbf{T}}icket \\underline{\\textbf{H}}ypothesis (LTH) and zero-concentrated D\\underline{\\textbf{P}} (zCDP). It generates a pruned global model on the server side and conducts sparse-to-sparse training from scratch with zCDP on the client side. On the server side, two pruning schemes are proposed: (i) the weight-based pruning (LTH) determines the pruned global mode",
    "path": "papers/23/05/2305.01387.json",
    "total_tokens": 973,
    "translated_title": "边缘计算领域实现增强隐私性的高效联邦学习方法",
    "translated_abstract": "联邦学习（FL）是一种协作学习范式，用于分散的移动终端（MT）中的私有数据。然而，它在通信、MT资源和隐私方面存在问题。现有的隐私保护FL方法通常采用实例级差分隐私（DP），它提供了严格的隐私保证，但存在性能下降、传输开销和边缘设备（如MT）的资源约束等几个瓶颈。为了克服这些缺点，我们提出了Fed-LTP，这是一种高效且增强隐私的FL框架，采用彩票卷积稀疏网络和零集中DP（zCDP）算法。它在服务器上生成修剪的全局模型，并在客户端上使用zCDP进行网络训练。服务器端提出了两种修剪方案：（i）基于权重的修剪（LTH）决定修剪全局模型所需的非零模型元素数量和位置；（ii）基于模式的修剪（PTH）修剪全局模型,使其与MT的模式相关。",
    "tldr": "本文提出了一种新的联邦学习方法，名为Fed-LTP，该方法采用彩票卷积稀疏网络和零集中差分隐私算法，能够高效实现隐私保护以及资源约束的移动终端协作学习。",
    "en_tdlr": "This paper proposes a new federated learning method called Fed-LTP, which adopts the lottery ticket pruning method and zero-concentrated differential privacy algorithm to achieve efficient and privacy-enhanced learning for mobile terminals with resource constraints."
}