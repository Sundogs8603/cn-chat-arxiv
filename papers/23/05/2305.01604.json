{
    "title": "The Training Process of Many Deep Networks Explores the Same Low-Dimensional Manifold. (arXiv:2305.01604v1 [cs.LG])",
    "abstract": "We develop information-geometric techniques to analyze the trajectories of the predictions of deep networks during training. By examining the underlying high-dimensional probabilistic models, we reveal that the training process explores an effectively low-dimensional manifold. Networks with a wide range of architectures, sizes, trained using different optimization methods, regularization techniques, data augmentation techniques, and weight initializations lie on the same manifold in the prediction space. We study the details of this manifold to find that networks with different architectures follow distinguishable trajectories but other factors have a minimal influence; larger networks train along a similar manifold as that of smaller networks, just faster; and networks initialized at very different parts of the prediction space converge to the solution along a similar manifold.",
    "link": "http://arxiv.org/abs/2305.01604",
    "context": "Title: The Training Process of Many Deep Networks Explores the Same Low-Dimensional Manifold. (arXiv:2305.01604v1 [cs.LG])\nAbstract: We develop information-geometric techniques to analyze the trajectories of the predictions of deep networks during training. By examining the underlying high-dimensional probabilistic models, we reveal that the training process explores an effectively low-dimensional manifold. Networks with a wide range of architectures, sizes, trained using different optimization methods, regularization techniques, data augmentation techniques, and weight initializations lie on the same manifold in the prediction space. We study the details of this manifold to find that networks with different architectures follow distinguishable trajectories but other factors have a minimal influence; larger networks train along a similar manifold as that of smaller networks, just faster; and networks initialized at very different parts of the prediction space converge to the solution along a similar manifold.",
    "path": "papers/23/05/2305.01604.json",
    "total_tokens": 830,
    "translated_title": "多个深度网络的训练过程探索相同的低维流形",
    "translated_abstract": "我们开发了信息几何技术来分析深度网络训练过程中预测轨迹。通过检查底层高维概率模型，我们揭示了训练过程探索的有效低维流形。具有各种体系结构、大小、使用不同优化方法、正则化技术、数据增强技术和权重初始化的网络在预测空间内位于同一流形上。我们研究了这种流形的细节，发现具有不同体系结构的网络遵循可区分的轨迹，但其他因素影响极小; 更大的网络沿着与较小的网络相似的流形训练，只是更快; 不同部分的初始化网络在相似的流形上向解决方案收敛。",
    "tldr": "本文展示了多个深度网络的训练过程探索相同的低维流形，这些网络包括不同体系结构、大小、使用不同优化方法、正则化技术、数据增强技术和权重初始化，并揭示了网络初始化位置、体系结构和大小对流形的影响。",
    "en_tdlr": "This paper shows that the training process of many deep networks explores the same low-dimensional manifold, regardless of their architecture, size, optimization methods, regularization techniques, data augmentation techniques, and weight initializations. It also reveals the minimal influence of factors such as network initialization location, architecture, and size on the manifold."
}