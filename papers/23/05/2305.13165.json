{
    "title": "Deep Neural Collapse Is Provably Optimal for the Deep Unconstrained Features Model. (arXiv:2305.13165v1 [cs.LG])",
    "abstract": "Neural collapse (NC) refers to the surprising structure of the last layer of deep neural networks in the terminal phase of gradient descent training. Recently, an increasing amount of experimental evidence has pointed to the propagation of NC to earlier layers of neural networks. However, while the NC in the last layer is well studied theoretically, much less is known about its multi-layered counterpart - deep neural collapse (DNC). In particular, existing work focuses either on linear layers or only on the last two layers at the price of an extra assumption. Our paper fills this gap by generalizing the established analytical framework for NC - the unconstrained features model - to multiple non-linear layers. Our key technical contribution is to show that, in a deep unconstrained features model, the unique global optimum for binary classification exhibits all the properties typical of DNC. This explains the existing experimental evidence of DNC. We also empirically show that (i) by opt",
    "link": "http://arxiv.org/abs/2305.13165",
    "context": "Title: Deep Neural Collapse Is Provably Optimal for the Deep Unconstrained Features Model. (arXiv:2305.13165v1 [cs.LG])\nAbstract: Neural collapse (NC) refers to the surprising structure of the last layer of deep neural networks in the terminal phase of gradient descent training. Recently, an increasing amount of experimental evidence has pointed to the propagation of NC to earlier layers of neural networks. However, while the NC in the last layer is well studied theoretically, much less is known about its multi-layered counterpart - deep neural collapse (DNC). In particular, existing work focuses either on linear layers or only on the last two layers at the price of an extra assumption. Our paper fills this gap by generalizing the established analytical framework for NC - the unconstrained features model - to multiple non-linear layers. Our key technical contribution is to show that, in a deep unconstrained features model, the unique global optimum for binary classification exhibits all the properties typical of DNC. This explains the existing experimental evidence of DNC. We also empirically show that (i) by opt",
    "path": "papers/23/05/2305.13165.json",
    "total_tokens": 1016,
    "translated_title": "深度神经网络的全局最优解可证为深度神经坍塌模型",
    "translated_abstract": "神经坍塌(NC)指的是深度神经网络在梯度下降训练的末期最后一层的惊奇结构。最近有越来越多的实验证据表明NC向神经网络的较早层传播。然而，尽管最后一层中的NC在理论上已经研究得很好，但对于其多层级的对应物-深度神经坍塌(DNC)却知之甚少。特别地，现有的工作基于线性层或仅涉及最后两层，但代价却是一个额外的假设。本文通过将基于无约束特征模型的已建立分析框架推广到多个非线性层级，弥补了这一差距。我们的关键技术贡献在于证明，在深度无约束特征模型中，用于二元分类的唯一全局最优解表现出了DNC的所有典型特征。这解释了现有关于DNC的实验证据。我们的实验也证明，通过选择合适的超参数，仅对模型的最后一层进行训练可以在不损失精度的情况下加快收敛。",
    "tldr": "本文研究了深度神经坍塌现象（Deep Neural Collapse），证明了在深度无约束特征模型中，唯一的全局最优解表现出了DNC的特征，并且通过选择合适的超参数，仅对模型的最后一层进行训练可以在不损失精度的情况下加快收敛。",
    "en_tdlr": "This paper investigates the Deep Neural Collapse phenomenon and proves that the unique global optimal for binary classification in a deep unconstrained features model exhibits all the typical features of DNC. Additionally, the paper shows that by training only the last layer of the model with suitable hyperparameters, convergence can be accelerated without sacrificing accuracy."
}