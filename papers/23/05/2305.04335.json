{
    "title": "Classification Tree Pruning Under Covariate Shift. (arXiv:2305.04335v1 [stat.ML])",
    "abstract": "We consider the problem of \\emph{pruning} a classification tree, that is, selecting a suitable subtree that balances bias and variance, in common situations with inhomogeneous training data. Namely, assuming access to mostly data from a distribution $P_{X, Y}$, but little data from a desired distribution $Q_{X, Y}$ with different $X$-marginals, we present the first efficient procedure for optimal pruning in such situations, when cross-validation and other penalized variants are grossly inadequate. Optimality is derived with respect to a notion of \\emph{average discrepancy} $P_{X} \\to Q_{X}$ (averaged over $X$ space) which significantly relaxes a recent notion -- termed \\emph{transfer-exponent} -- shown to tightly capture the limits of classification under such a distribution shift. Our relaxed notion can be viewed as a measure of \\emph{relative dimension} between distributions, as it relates to existing notions of information such as the Minkowski and Renyi dimensions.",
    "link": "http://arxiv.org/abs/2305.04335",
    "context": "Title: Classification Tree Pruning Under Covariate Shift. (arXiv:2305.04335v1 [stat.ML])\nAbstract: We consider the problem of \\emph{pruning} a classification tree, that is, selecting a suitable subtree that balances bias and variance, in common situations with inhomogeneous training data. Namely, assuming access to mostly data from a distribution $P_{X, Y}$, but little data from a desired distribution $Q_{X, Y}$ with different $X$-marginals, we present the first efficient procedure for optimal pruning in such situations, when cross-validation and other penalized variants are grossly inadequate. Optimality is derived with respect to a notion of \\emph{average discrepancy} $P_{X} \\to Q_{X}$ (averaged over $X$ space) which significantly relaxes a recent notion -- termed \\emph{transfer-exponent} -- shown to tightly capture the limits of classification under such a distribution shift. Our relaxed notion can be viewed as a measure of \\emph{relative dimension} between distributions, as it relates to existing notions of information such as the Minkowski and Renyi dimensions.",
    "path": "papers/23/05/2305.04335.json",
    "total_tokens": 1133,
    "translated_title": "基于协变量转移的分类树剪枝",
    "translated_abstract": "本文考虑在训练数据不均匀的情况下，选择适当的子树以平衡偏差和方差的分类树剪枝问题。我们提出了一种针对这种情况的最优剪枝的高效程序，该程序可以访问大部分来自分布 $P_{X，Y}$ 的数据，但是只能获得来自拥有不同 $X$-边缘的目标分布 $Q_{X，Y}$ 的少量数据。在基本交叉验证和其他进行惩罚的变体，如基于信息度量的剪枝方法非常不理想的情况下，我们提供了一种最优剪枝的方法。使用的优化标准是一个关于分布 $P_{X} \\to Q_{X}$ 的 \\emph{平均差异}（在 $X$ 空间上平均），该标准可以显著放宽最近提出的 \\emph{转移指数} 这一统计学概念，该概念被证明能够紧密地捕捉这种分布转移情况下分类的极限限制。我们放宽的标准可以被看作是分布之间的\\emph{相对维度}度量，因为它涉及到信息的现有度量概念，例如闵可夫斯基和Rényi维度。",
    "tldr": "本文提出了一种基于协变量转移的分类树剪枝方法，可以访问大部分来自分布 $P_{X，Y}$ 的数据，但是只能获得来自拥有不同 $X$-边缘的目标分布 $Q_{X，Y}$ 的少量数据。使用的优化标准是一个关于分布 $P_{X} \\to Q_{X}$ 的 \\emph{平均差异}，该标准可以显著放宽最近提出的 \\emph{转移指数}，最终可以得到最优的剪枝结果。",
    "en_tdlr": "This paper proposes an efficient procedure for optimal pruning of classification trees under circumstances when training data is inhomogeneous, using an optimization standard of average discrepancy between the distributions P_X and Q_X, which significantly relaxes a recent notion of transfer-exponent. The proposed approach can access most of the data from distribution P_X,Y and little data from the desired distribution Q_X,Y with different X-marginals."
}