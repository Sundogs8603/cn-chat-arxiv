{
    "title": "Pre-training Contextualized World Models with In-the-wild Videos for Reinforcement Learning. (arXiv:2305.18499v1 [cs.CV])",
    "abstract": "Unsupervised pre-training methods utilizing large and diverse datasets have achieved tremendous success across a range of domains. Recent work has investigated such unsupervised pre-training methods for model-based reinforcement learning (MBRL) but is limited to domain-specific or simulated data. In this paper, we study the problem of pre-training world models with abundant in-the-wild videos for efficient learning of downstream visual control tasks. However, in-the-wild videos are complicated with various contextual factors, such as intricate backgrounds and textured appearance, which precludes a world model from extracting shared world knowledge to generalize better. To tackle this issue, we introduce Contextualized World Models (ContextWM) that explicitly model both the context and dynamics to overcome the complexity and diversity of in-the-wild videos and facilitate knowledge transfer between distinct scenes. Specifically, a contextualized extension of the latent dynamics model is ",
    "link": "http://arxiv.org/abs/2305.18499",
    "context": "Title: Pre-training Contextualized World Models with In-the-wild Videos for Reinforcement Learning. (arXiv:2305.18499v1 [cs.CV])\nAbstract: Unsupervised pre-training methods utilizing large and diverse datasets have achieved tremendous success across a range of domains. Recent work has investigated such unsupervised pre-training methods for model-based reinforcement learning (MBRL) but is limited to domain-specific or simulated data. In this paper, we study the problem of pre-training world models with abundant in-the-wild videos for efficient learning of downstream visual control tasks. However, in-the-wild videos are complicated with various contextual factors, such as intricate backgrounds and textured appearance, which precludes a world model from extracting shared world knowledge to generalize better. To tackle this issue, we introduce Contextualized World Models (ContextWM) that explicitly model both the context and dynamics to overcome the complexity and diversity of in-the-wild videos and facilitate knowledge transfer between distinct scenes. Specifically, a contextualized extension of the latent dynamics model is ",
    "path": "papers/23/05/2305.18499.json",
    "total_tokens": 1132,
    "translated_title": "基于野外视频预训练的上下文化世界模型用于强化学习",
    "translated_abstract": "利用大规模多样化的数据集进行的无监督预训练已在各种领域取得了巨大成功。最近的工作调查了这种无监督预训练方法在基于模型的强化学习（MBRL）中的应用，但仅限于特定领域或模拟数据。本文研究了使用大量野外视频进行预训练世界模型，以高效学习下游视觉控制任务的问题。然而，野外视频存在各种上下文因素，如错综复杂的背景和纹理外观，这使得世界模型无法提取共享的世界知识以更好地泛化。为了解决这个问题，我们引入了上下文化世界模型（ContextWM），显式地对上下文和动态进行建模，以克服野外视频的复杂性和多样性，并促进不同场景之间的知识转移。具体而言，使用上下文化扩展的潜在动态模型来捕捉高级状态和低级观察之间的上下文依赖关系。通过使用野外视频对ContextWM进行预训练，我们展示了与从头开始训练相比，下游视觉控制任务的样本效率和控制性能均得到了显着提高。",
    "tldr": "本文研究了基于野外视频预训练的上下文化世界模型（ContextWM）用于强化学习。ContextWM采用上下文化扩展的潜在动态模型建模，从而可以更好地泛化不同场景之间的知识转移，提高下游视觉控制任务的样本效率和控制性能。"
}