{
    "title": "Probing in Context: Toward Building Robust Classifiers via Probing Large Language Models. (arXiv:2305.14171v2 [cs.CL] UPDATED)",
    "abstract": "Large language models are able to learn new tasks in context, where they are provided with instructions and a few annotated examples. However, the effectiveness of in-context learning is dependent on the provided context, and the performance on a downstream task can vary considerably, depending on the instruction. Importantly, such dependency on the context can surface in unpredictable ways, e.g., a seemingly more informative instruction might lead to a worse performance. In this paper, we propose an alternative approach, which we term in-context probing. Similar to in-context learning, we contextualize the representation of the input with an instruction, but instead of decoding the output prediction, we probe the contextualized representation to predict the label. Through a series of experiments on a diverse set of classification tasks, we show that in-context probing is significantly more robust to changes in instructions. We further show that probing performs competitive or superior",
    "link": "http://arxiv.org/abs/2305.14171",
    "context": "Title: Probing in Context: Toward Building Robust Classifiers via Probing Large Language Models. (arXiv:2305.14171v2 [cs.CL] UPDATED)\nAbstract: Large language models are able to learn new tasks in context, where they are provided with instructions and a few annotated examples. However, the effectiveness of in-context learning is dependent on the provided context, and the performance on a downstream task can vary considerably, depending on the instruction. Importantly, such dependency on the context can surface in unpredictable ways, e.g., a seemingly more informative instruction might lead to a worse performance. In this paper, we propose an alternative approach, which we term in-context probing. Similar to in-context learning, we contextualize the representation of the input with an instruction, but instead of decoding the output prediction, we probe the contextualized representation to predict the label. Through a series of experiments on a diverse set of classification tasks, we show that in-context probing is significantly more robust to changes in instructions. We further show that probing performs competitive or superior",
    "path": "papers/23/05/2305.14171.json",
    "total_tokens": 912,
    "translated_title": "在上下文中的探测：通过对大型语言模型的探测构建鲁棒的分类器",
    "translated_abstract": "大型语言模型能够在上下文中学习新任务，在提供指令和少量注释示例的情况下。然而，上下文学习的有效性取决于提供的上下文，并且下游任务的性能可能会有很大变化，取决于指令。重要的是，这种对上下文的依赖可能以不可预测的方式表现，例如，一个看似更有信息量的指令可能导致性能更差。在本文中，我们提出了一种替代方法，称之为上下文中的探测。类似于上下文学习，我们用指令对输入的表示进行上下文化，但是我们不是解码输出预测，而是探测上下文化的表示来预测标签。通过一系列在多样化的分类任务上的实验，我们展示了上下文中的探测对指令变化更加鲁棒。我们进一步展示了探测的性能与其他方法相竞争或更胜一筹。",
    "tldr": "本文提出了一种在上下文中的探测方法，用于构建鲁棒的分类器。通过探测上下文化的表示来预测标签，这种方法对指令变化更加鲁棒，并且在多样化的分类任务上表现出竞争力或更好的性能。",
    "en_tdlr": "This paper proposes an in-context probing approach for building robust classifiers. By probing the contextualized representation to predict labels, this method is more robust to changes in instructions and shows competitive or superior performance on diverse classification tasks."
}