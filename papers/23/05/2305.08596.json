{
    "title": "DarkBERT: A Language Model for the Dark Side of the Internet. (arXiv:2305.08596v2 [cs.CL] UPDATED)",
    "abstract": "Recent research has suggested that there are clear differences in the language used in the Dark Web compared to that of the Surface Web. As studies on the Dark Web commonly require textual analysis of the domain, language models specific to the Dark Web may provide valuable insights to researchers. In this work, we introduce DarkBERT, a language model pretrained on Dark Web data. We describe the steps taken to filter and compile the text data used to train DarkBERT to combat the extreme lexical and structural diversity of the Dark Web that may be detrimental to building a proper representation of the domain. We evaluate DarkBERT and its vanilla counterpart along with other widely used language models to validate the benefits that a Dark Web domain specific model offers in various use cases. Our evaluations show that DarkBERT outperforms current language models and may serve as a valuable resource for future research on the Dark Web.",
    "link": "http://arxiv.org/abs/2305.08596",
    "context": "Title: DarkBERT: A Language Model for the Dark Side of the Internet. (arXiv:2305.08596v2 [cs.CL] UPDATED)\nAbstract: Recent research has suggested that there are clear differences in the language used in the Dark Web compared to that of the Surface Web. As studies on the Dark Web commonly require textual analysis of the domain, language models specific to the Dark Web may provide valuable insights to researchers. In this work, we introduce DarkBERT, a language model pretrained on Dark Web data. We describe the steps taken to filter and compile the text data used to train DarkBERT to combat the extreme lexical and structural diversity of the Dark Web that may be detrimental to building a proper representation of the domain. We evaluate DarkBERT and its vanilla counterpart along with other widely used language models to validate the benefits that a Dark Web domain specific model offers in various use cases. Our evaluations show that DarkBERT outperforms current language models and may serve as a valuable resource for future research on the Dark Web.",
    "path": "papers/23/05/2305.08596.json",
    "total_tokens": 829,
    "translated_title": "DarkBERT：针对互联网黑暗面的语言模型",
    "translated_abstract": "最近的研究表明，与表层网络相比，暗网中使用的语言存在明显差异。由于研究暗网通常需要对域进行文本分析，因此针对暗网的语言模型可能为研究人员提供有价值的洞见。在本文中，我们介绍了DarkBERT，这是一个预先在暗网数据上训练的语言模型。我们描述了筛选和编译用于训练DarkBERT的文本数据的步骤，以应对暗网极为不同的词汇和结构多样性，这可能有害于构建该域的适当表示。我们评估了DarkBERT及其基础模型以及其他广泛使用的语言模型，以验证针对暗​​网的特定模型在各种用例中提供的好处。我们的评估显示，DarkBERT胜过当前的语言模型，可能成为未来暗网研究的有价值资源。",
    "tldr": "本研究介绍了一个在暗网数据上预先训练的语言模型——DarkBERT，对于暗网的研究具有重要的价值。",
    "en_tdlr": "This paper introduces DarkBERT, a language model pre-trained on Dark Web data, which outperforms current language models and serves as a valuable resource for future research on the Dark Web."
}