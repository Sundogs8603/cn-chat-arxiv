{
    "title": "Pre-trained Language Model with Prompts for Temporal Knowledge Graph Completion. (arXiv:2305.07912v1 [cs.CL])",
    "abstract": "Temporal Knowledge graph completion (TKGC) is a crucial task that involves reasoning at known timestamps to complete the missing part of facts and has attracted more and more attention in recent years. Most existing methods focus on learning representations based on graph neural networks while inaccurately extracting information from timestamps and insufficiently utilizing the implied information in relations. To address these problems, we propose a novel TKGC model, namely Pre-trained Language Model with Prompts for TKGC (PPT). We convert a series of sampled quadruples into pre-trained language model inputs and convert intervals between timestamps into different prompts to make coherent sentences with implicit semantic information. We train our model with a masking strategy to convert TKGC task into a masked token prediction task, which can leverage the semantic information in pre-trained language models. Experiments on three benchmark datasets and extensive analysis demonstrate that ",
    "link": "http://arxiv.org/abs/2305.07912",
    "context": "Title: Pre-trained Language Model with Prompts for Temporal Knowledge Graph Completion. (arXiv:2305.07912v1 [cs.CL])\nAbstract: Temporal Knowledge graph completion (TKGC) is a crucial task that involves reasoning at known timestamps to complete the missing part of facts and has attracted more and more attention in recent years. Most existing methods focus on learning representations based on graph neural networks while inaccurately extracting information from timestamps and insufficiently utilizing the implied information in relations. To address these problems, we propose a novel TKGC model, namely Pre-trained Language Model with Prompts for TKGC (PPT). We convert a series of sampled quadruples into pre-trained language model inputs and convert intervals between timestamps into different prompts to make coherent sentences with implicit semantic information. We train our model with a masking strategy to convert TKGC task into a masked token prediction task, which can leverage the semantic information in pre-trained language models. Experiments on three benchmark datasets and extensive analysis demonstrate that ",
    "path": "papers/23/05/2305.07912.json",
    "total_tokens": 895,
    "translated_title": "基于提示的预训练语言模型用于时间知识图谱补全",
    "translated_abstract": "时间知识图谱补全（TKGC）是一项重要的任务，它涉及在已知的时间戳上进行推理，以完成缺失部分的事实，并在近年来越来越受到关注。大多数现有方法都集中于基于图神经网络的学习表示，同时粗略地提取时间戳中的信息，并不充分利用关系中隐含的信息。为了解决这些问题，我们提出了一种新的TKGC模型，即基于提示的预训练语言模型（PPT）。我们将一系列采样的四元组转换为预训练语言模型的输入，并将时间戳之间的间隔转换为不同的提示，以形成带有隐含语义信息的连贯句子。我们使用遮盖策略训练我们的模型，将TKGC任务转换为遮盖词预测任务，从而可以利用预训练语言模型中的语义信息。实验结果和广泛的分析表明，",
    "tldr": "这篇论文提出了一种基于提示的预训练语言模型（PPT），用于时间知识图谱补全。通过遮盖策略，将TKGC任务转换为遮盖词预测任务，可以利用预训练语言模型中的语义信息。",
    "en_tdlr": "This paper proposes a pre-trained language model with prompts (PPT) for temporal knowledge graph completion (TKGC). By converting TKGC task into a masked token prediction task, PPT can leverage semantic information in pre-trained language models to better extract information from timestamps and relations."
}