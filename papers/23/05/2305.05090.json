{
    "title": "Performative Federated Learning: A Solution to Model-Dependent and Heterogeneous Distribution Shifts. (arXiv:2305.05090v1 [cs.LG])",
    "abstract": "We consider a federated learning (FL) system consisting of multiple clients and a server, where the clients aim to collaboratively learn a common decision model from their distributed data. Unlike the conventional FL framework that assumes the client's data is static, we consider scenarios where the clients' data distributions may be reshaped by the deployed decision model. In this work, we leverage the idea of distribution shift mappings in performative prediction to formalize this model-dependent data distribution shift and propose a performative federated learning framework. We first introduce necessary and sufficient conditions for the existence of a unique performative stable solution and characterize its distance to the performative optimal solution. Then we propose the performative FedAvg algorithm and show that it converges to the performative stable solution at a rate of O(1/T) under both full and partial participation schemes. In particular, we use novel proof techniques and ",
    "link": "http://arxiv.org/abs/2305.05090",
    "context": "Title: Performative Federated Learning: A Solution to Model-Dependent and Heterogeneous Distribution Shifts. (arXiv:2305.05090v1 [cs.LG])\nAbstract: We consider a federated learning (FL) system consisting of multiple clients and a server, where the clients aim to collaboratively learn a common decision model from their distributed data. Unlike the conventional FL framework that assumes the client's data is static, we consider scenarios where the clients' data distributions may be reshaped by the deployed decision model. In this work, we leverage the idea of distribution shift mappings in performative prediction to formalize this model-dependent data distribution shift and propose a performative federated learning framework. We first introduce necessary and sufficient conditions for the existence of a unique performative stable solution and characterize its distance to the performative optimal solution. Then we propose the performative FedAvg algorithm and show that it converges to the performative stable solution at a rate of O(1/T) under both full and partial participation schemes. In particular, we use novel proof techniques and ",
    "path": "papers/23/05/2305.05090.json",
    "total_tokens": 793,
    "translated_title": "表现性联邦学习：应对模型依赖和异构分布偏移的解决方案",
    "translated_abstract": "本文考虑了一个由多个客户端和一个服务器组成的联邦学习系统，其中客户端旨在从分布式数据中协作学习一个共同的决策模型。与传统的假设客户端数据静态的联邦学习框架不同，我们考虑了客户端的数据分布可能会被部署的决策模型重塑的情况。我们借鉴了表现性预测中的分布偏移映射的想法，将其形式化为模型依赖的数据分布偏移，并提出了一个表现性联邦学习框架。",
    "tldr": "本文提出了一种表现性联邦学习框架，通过引入分布映射来应对数据分布偏移，解决了模型依赖和客户端数据的异构分布。",
    "en_tdlr": "This paper proposes a performative federated learning framework to address distribution shifts caused by model dependencies and heterogeneous data distributions among clients. By leveraging distribution shift mappings, it provides a solution to the conventional FL framework that assumes static client data."
}