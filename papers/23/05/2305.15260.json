{
    "title": "Collaborative World Models: An Online-Offline Transfer RL Approach. (arXiv:2305.15260v2 [cs.LG] UPDATED)",
    "abstract": "Training visual reinforcement learning (RL) models in offline datasets is challenging due to overfitting issues in representation learning and overestimation problems in value function. In this paper, we propose a transfer learning method called Collaborative World Models (CoWorld) to improve the performance of visual RL under offline conditions. The core idea is to use an easy-to-interact, off-the-shelf simulator to train an auxiliary RL model as the online \"test bed\" for the offline policy learned in the target domain, which provides a flexible constraint for the value function -- Intuitively, we want to mitigate the overestimation problem of value functions outside the offline data distribution without impeding the exploration of actions with potential advantages. Specifically, CoWorld performs domain-collaborative representation learning to bridge the gap between online and offline hidden state distributions. Furthermore, it performs domain-collaborative behavior learning that enab",
    "link": "http://arxiv.org/abs/2305.15260",
    "context": "Title: Collaborative World Models: An Online-Offline Transfer RL Approach. (arXiv:2305.15260v2 [cs.LG] UPDATED)\nAbstract: Training visual reinforcement learning (RL) models in offline datasets is challenging due to overfitting issues in representation learning and overestimation problems in value function. In this paper, we propose a transfer learning method called Collaborative World Models (CoWorld) to improve the performance of visual RL under offline conditions. The core idea is to use an easy-to-interact, off-the-shelf simulator to train an auxiliary RL model as the online \"test bed\" for the offline policy learned in the target domain, which provides a flexible constraint for the value function -- Intuitively, we want to mitigate the overestimation problem of value functions outside the offline data distribution without impeding the exploration of actions with potential advantages. Specifically, CoWorld performs domain-collaborative representation learning to bridge the gap between online and offline hidden state distributions. Furthermore, it performs domain-collaborative behavior learning that enab",
    "path": "papers/23/05/2305.15260.json",
    "total_tokens": 1023,
    "translated_title": "协作世界模型: 一种在线离线转移RL方法。",
    "translated_abstract": "在离线数据集中训练视觉强化学习（RL）模型由于表征学习中的过度拟合问题和价值函数中的过度估计问题而具有挑战性。在本文中，我们提出了一种称为协作世界模型（CoWorld）的转移学习方法，以改善离线条件下视觉RL的性能。其核心想法是使用易于交互、现成的模拟器来训练辅助RL模型作为离线策略在目标域中学习的在线“测试床”，这为价值函数提供了灵活的约束——直观地说，我们想在不妨碍具有潜在优势的动作探索的情况下缓解离线数据分布之外的价值函数过度估计问题。具体而言，CoWorld执行域协作表示学习以弥合在线和离线隐藏状态分布之间的差距。此外，它执行域协作行为学习，使在离线数据集外的智能体能够学习在线行为策略。",
    "tldr": "本论文提出了一种名为协作世界模型（CoWorld）的转移学习方法，以改善离线条件下视觉RL的性能。其核心想法是使用易于交互的模拟器来训练辅助RL模型作为离线策略的在线“测试床”，并执行域协作表示学习和域协作行为学习，缓解离线数据分布之外的价值函数过度估计问题。"
}