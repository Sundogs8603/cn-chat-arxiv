{
    "title": "Abstractive Text Summarization Using the BRIO Training Paradigm. (arXiv:2305.13696v1 [cs.CL])",
    "abstract": "Summary sentences produced by abstractive summarization models may be coherent and comprehensive, but they lack control and rely heavily on reference summaries. The BRIO training paradigm assumes a non-deterministic distribution to reduce the model's dependence on reference summaries, and improve model performance during inference. This paper presents a straightforward but effective technique to improve abstractive summaries by fine-tuning pre-trained language models, and training them with the BRIO paradigm. We build a text summarization dataset for Vietnamese, called VieSum. We perform experiments with abstractive summarization models trained with the BRIO paradigm on the CNNDM and the VieSum datasets. The results show that the models, trained on basic hardware, outperform all existing abstractive summarization models, especially for Vietnamese.",
    "link": "http://arxiv.org/abs/2305.13696",
    "context": "Title: Abstractive Text Summarization Using the BRIO Training Paradigm. (arXiv:2305.13696v1 [cs.CL])\nAbstract: Summary sentences produced by abstractive summarization models may be coherent and comprehensive, but they lack control and rely heavily on reference summaries. The BRIO training paradigm assumes a non-deterministic distribution to reduce the model's dependence on reference summaries, and improve model performance during inference. This paper presents a straightforward but effective technique to improve abstractive summaries by fine-tuning pre-trained language models, and training them with the BRIO paradigm. We build a text summarization dataset for Vietnamese, called VieSum. We perform experiments with abstractive summarization models trained with the BRIO paradigm on the CNNDM and the VieSum datasets. The results show that the models, trained on basic hardware, outperform all existing abstractive summarization models, especially for Vietnamese.",
    "path": "papers/23/05/2305.13696.json",
    "total_tokens": 865,
    "translated_title": "使用BRIO训练范式的抽象文本摘要",
    "translated_abstract": "抽象摘要模型生成的摘要句子可能连贯全面，但缺乏控制并且严重依赖于参考摘要。BRIO训练范例假定一个非确定性分布，以减少模型对参考摘要的依赖，并提高模型在推理期间的性能。本文提出了一种简单而有效的技术，通过微调预训练语言模型，并使用BRIO训练范式进行训练，以改善抽象摘要。我们建立了一个越南文本摘要数据集，称为VieSum。我们在CNNDM和VieSum数据集上使用经过BRIO模型训练的抽象摘要模型进行实验。结果表明，经过基本硬件训练的模型优于所有现有的抽象摘要模型，特别是对于越南语而言。",
    "tldr": "本文提出了一种新的BRIO训练范式，以减少摘要模型对参考摘要的依赖，并提高其推理性能。在VieSum数据集上的实验证明，BRIO训练范式可以在基本硬件上优化抽象摘要模型，并在越南文本摘要上取得更好的表现。",
    "en_tdlr": "This paper proposes a new BRIO training paradigm to reduce the dependence of summarization models on reference summaries and improve their inference performance. Experimental results on the VieSum dataset demonstrate that the BRIO training paradigm can optimize abstractive summarization models on basic hardware and achieve better performance on Vietnamese text summarization."
}