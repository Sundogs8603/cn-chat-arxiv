{
    "title": "Improving End-to-End SLU performance with Prosodic Attention and Distillation. (arXiv:2305.08067v1 [cs.CL])",
    "abstract": "Most End-to-End SLU methods depend on the pretrained ASR or language model features for intent prediction. However, other essential information in speech, such as prosody, is often ignored. Recent research has shown improved results in classifying dialogue acts by incorporating prosodic information. The margins of improvement in these methods are minimal as the neural models ignore prosodic features. In this work, we propose prosody-attention, which uses the prosodic features differently to generate attention maps across time frames of the utterance. Then we propose prosody-distillation to explicitly learn the prosodic information in the acoustic encoder rather than concatenating the implicit prosodic features. Both the proposed methods improve the baseline results, and the prosody-distillation method gives an intent classification accuracy improvement of 8\\% and 2\\% on SLURP and STOP datasets over the prosody baseline.",
    "link": "http://arxiv.org/abs/2305.08067",
    "context": "Title: Improving End-to-End SLU performance with Prosodic Attention and Distillation. (arXiv:2305.08067v1 [cs.CL])\nAbstract: Most End-to-End SLU methods depend on the pretrained ASR or language model features for intent prediction. However, other essential information in speech, such as prosody, is often ignored. Recent research has shown improved results in classifying dialogue acts by incorporating prosodic information. The margins of improvement in these methods are minimal as the neural models ignore prosodic features. In this work, we propose prosody-attention, which uses the prosodic features differently to generate attention maps across time frames of the utterance. Then we propose prosody-distillation to explicitly learn the prosodic information in the acoustic encoder rather than concatenating the implicit prosodic features. Both the proposed methods improve the baseline results, and the prosody-distillation method gives an intent classification accuracy improvement of 8\\% and 2\\% on SLURP and STOP datasets over the prosody baseline.",
    "path": "papers/23/05/2305.08067.json",
    "total_tokens": 903,
    "translated_title": "利用韵律关注和蒸馏来提高端到端语义理解性能",
    "translated_abstract": "大多数端到端语义理解（SLU）方法依赖于预训练的自动语音识别或语言模型功能来进行意图预测。然而，语音中的其他重要信息，如韵律，经常被忽视。最近的研究表明，将韵律信息合并到对话行为分类中可以获得改进的结果。本文提出了韵律关注，它使用不同的方式利用韵律特征来生成跨时间帧的注意力图。然后，我们提出了韵律蒸馏，来明确地学习声学编码器中的韵律信息，而不是连接隐含的韵律特征。两种方法都提高了基线结果，其中韵律蒸馏方法在SLURP和STOP数据集上相对于韵律基线提高了8％和2％的意图分类准确性。",
    "tldr": "本文提出了韵律关注和韵律蒸馏方法来利用韵律特征提高端到端语义理解（SLU）的性能，其中韵律蒸馏方法相对于基线方法在SLURP和STOP数据集上提高了8％和2％的意图分类准确性。",
    "en_tdlr": "This paper proposes prosodic attention and distillation methods to improve the performance of End-to-End Semantic Understanding (SLU) by utilizing prosodic features. The prosody-distillation method improves intent classification accuracy by 8% and 2% on SLURP and STOP datasets over the prosody baseline."
}