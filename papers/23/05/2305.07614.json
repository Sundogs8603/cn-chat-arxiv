{
    "title": "NevIR: Negation in Neural Information Retrieval. (arXiv:2305.07614v1 [cs.IR])",
    "abstract": "Negation is a common everyday phenomena and has been a consistent area of weakness for language models (LMs). Although the Information Retrieval (IR) community has adopted LMs as the backbone of modern IR architectures, there has been little to no research in understanding how negation impacts neural IR. We therefore construct a straightforward benchmark on this theme: asking IR models to rank two documents that differ only by negation. We show that the results vary widely according to the type of IR architecture: cross-encoders perform best, followed by late-interaction models, and in last place are bi-encoder and sparse neural architectures. We find that most current information retrieval models do not consider negation, performing similarly or worse than randomly ranking. We show that although the obvious approach of continued fine-tuning on a dataset of contrastive documents containing negations increases performance (as does model size), there is still a large gap between machine ",
    "link": "http://arxiv.org/abs/2305.07614",
    "context": "Title: NevIR: Negation in Neural Information Retrieval. (arXiv:2305.07614v1 [cs.IR])\nAbstract: Negation is a common everyday phenomena and has been a consistent area of weakness for language models (LMs). Although the Information Retrieval (IR) community has adopted LMs as the backbone of modern IR architectures, there has been little to no research in understanding how negation impacts neural IR. We therefore construct a straightforward benchmark on this theme: asking IR models to rank two documents that differ only by negation. We show that the results vary widely according to the type of IR architecture: cross-encoders perform best, followed by late-interaction models, and in last place are bi-encoder and sparse neural architectures. We find that most current information retrieval models do not consider negation, performing similarly or worse than randomly ranking. We show that although the obvious approach of continued fine-tuning on a dataset of contrastive documents containing negations increases performance (as does model size), there is still a large gap between machine ",
    "path": "papers/23/05/2305.07614.json",
    "total_tokens": 913,
    "translated_title": "NevIR: 神经信息检索中的否定",
    "translated_abstract": "否定是一种常见而日常化的现象，也一直是语言模型的一个弱点。虽然信息检索领域采用了语言模型作为现代化架构的主干，但几乎没有研究深入了解否定对神经信息检索的影响。因此，我们构建了一个简单的基准来研究这个主题：要求信息检索模型对仅仅因为是否定而不同的两个文档进行排名。我们发现，结果根据不同的信息检索架构而有很大差异：交叉编码器表现最好，后期交互模型次之，而双编码器和稀疏神经架构排名最后。我们发现，大多数当前的信息检索模型都没有考虑否定，表现与随机排名相似或更差。我们证明，尽管在一个包含否定对照文档的数据集上继续微调明显的方法可以提高性能（模型大小也是如此），但是机器和人之间仍存在很大的差距。",
    "tldr": "本研究探讨了否定在神经信息检索中的影响，构建了基准模型，结果表明当前信息检索模型大多数都没有考虑否定，而交叉编码器是目前表现最好的架构。",
    "en_tdlr": "This study investigates the impact of negation in neural information retrieval and constructs a benchmark model, showing that most current information retrieval models do not consider negation and cross-encoders are the best-performing architecture."
}