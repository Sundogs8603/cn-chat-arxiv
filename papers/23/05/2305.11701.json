{
    "title": "S-JEA: Stacked Joint Embedding Architectures for Self-Supervised Visual Representation Learning. (arXiv:2305.11701v1 [cs.CV])",
    "abstract": "The recent emergence of Self-Supervised Learning (SSL) as a fundamental paradigm for learning image representations has, and continues to, demonstrate high empirical success in a variety of tasks. However, most SSL approaches fail to learn embeddings that capture hierarchical semantic concepts that are separable and interpretable. In this work, we aim to learn highly separable semantic hierarchical representations by stacking Joint Embedding Architectures (JEA) where higher-level JEAs are input with representations of lower-level JEA. This results in a representation space that exhibits distinct sub-categories of semantic concepts (e.g., model and colour of vehicles) in higher-level JEAs. We empirically show that representations from stacked JEA perform on a similar level as traditional JEA with comparative parameter counts and visualise the representation spaces to validate the semantic hierarchies.",
    "link": "http://arxiv.org/abs/2305.11701",
    "context": "Title: S-JEA: Stacked Joint Embedding Architectures for Self-Supervised Visual Representation Learning. (arXiv:2305.11701v1 [cs.CV])\nAbstract: The recent emergence of Self-Supervised Learning (SSL) as a fundamental paradigm for learning image representations has, and continues to, demonstrate high empirical success in a variety of tasks. However, most SSL approaches fail to learn embeddings that capture hierarchical semantic concepts that are separable and interpretable. In this work, we aim to learn highly separable semantic hierarchical representations by stacking Joint Embedding Architectures (JEA) where higher-level JEAs are input with representations of lower-level JEA. This results in a representation space that exhibits distinct sub-categories of semantic concepts (e.g., model and colour of vehicles) in higher-level JEAs. We empirically show that representations from stacked JEA perform on a similar level as traditional JEA with comparative parameter counts and visualise the representation spaces to validate the semantic hierarchies.",
    "path": "papers/23/05/2305.11701.json",
    "total_tokens": 817,
    "translated_title": "S-JEA: 堆叠联合嵌入结构用于自监督视觉表示学习",
    "translated_abstract": "自监督学习作为学习图像表示的基本范式，近年来已经在各种任务中展示了高度的实证成功。然而，大多数自监督学习方法未能学习到捕获分层语义概念的嵌入，这些概念是可分离和可解释的。本文旨在通过堆叠联合嵌入结构（JEA）来学习高度可分离的分层语义表示，其中较高级别的JEA使用较低级别JEA的表示结果作为输入。这导致表示空间表现出更明显的语义概念子类（如车辆的型号和颜色）在较高级别的JEA中。我们实验性地展示了堆叠JEA的表示与传统JEA相似，并展示了表示空间以验证语义分层结构。",
    "tldr": "本文提出了使用堆叠联合嵌入结构学习高度可分离的分层语义表示，显示出更明显的语义概念子类，并且与传统方法相似。"
}