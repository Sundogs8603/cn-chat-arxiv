{
    "title": "PROM: A Phrase-level Copying Mechanism with Pre-training for Abstractive Summarization. (arXiv:2305.06647v1 [cs.CL])",
    "abstract": "Based on the remarkable achievements of pre-trained language models in abstractive summarization, the copying mechanism has proved helpful by improving the factuality, stability, and overall performance. This work proposes PROM, a new PhRase-level cOpying Mechanism that enhances attention on n-grams, which can be applied to zero-shot summarization with pre-training. PROM adds an indicator layer to explicitly pick up tokens in n-gram that can be copied from the source, and calculates an auxiliary loss for the copying prediction. Empirical studies show that PROM makes significant improvements in fine-tuning on benchmarks. In zero-shot setting, PROM is utilized in the self-supervised pre-training on raw corpora and provides new general baselines on a wide range of summarization datasets. Further analysis shows that PROM performs more reasonable copying and contributes to faithfulness.",
    "link": "http://arxiv.org/abs/2305.06647",
    "context": "Title: PROM: A Phrase-level Copying Mechanism with Pre-training for Abstractive Summarization. (arXiv:2305.06647v1 [cs.CL])\nAbstract: Based on the remarkable achievements of pre-trained language models in abstractive summarization, the copying mechanism has proved helpful by improving the factuality, stability, and overall performance. This work proposes PROM, a new PhRase-level cOpying Mechanism that enhances attention on n-grams, which can be applied to zero-shot summarization with pre-training. PROM adds an indicator layer to explicitly pick up tokens in n-gram that can be copied from the source, and calculates an auxiliary loss for the copying prediction. Empirical studies show that PROM makes significant improvements in fine-tuning on benchmarks. In zero-shot setting, PROM is utilized in the self-supervised pre-training on raw corpora and provides new general baselines on a wide range of summarization datasets. Further analysis shows that PROM performs more reasonable copying and contributes to faithfulness.",
    "path": "papers/23/05/2305.06647.json",
    "total_tokens": 879,
    "translated_title": "PROM：基于预训练的短语级复制机制，用于抽象摘要生成",
    "translated_abstract": "基于预训练语言模型在抽象摘要生成方面的显著成就，复制机制通过提高事实性、稳定性和整体性能方面的改进证明其有助于这一进程。本文提出一种新的短语级复制机制-PROM，它增强了对n-gram的注意力，可以应用于具有预训练的零样本摘要生成中。PROM添加了一个指示器层，以明确从源中可以复制的n-gram中的令牌，并计算复制预测的辅助损失。实证研究表明，在基准微调中，PROM取得了显著改进。在零样本设置中，PROM用于对原始语料库进行自监督预训练，并在广泛的摘要数据集上提供了新的通用基线。进一步的分析表明，PROM执行更合理的复制并有助于保持忠实度。",
    "tldr": "提出了一种新的短语级复制机制-PROM，可以增强对n-gram的注意力，用于具有预训练的零样本摘要生成，大大提高了摘要的质量和可信度。",
    "en_tdlr": "A new PhRase-level cOpying Mechanism (PROM) is proposed in this paper to enhance attention on n-grams for zero-shot summarization with pre-training. PROM significantly improves the quality and faithfulness of the generated summaries in both fine-tuning and self-supervised pre-training."
}