{
    "title": "Utilizing Lexical Similarity to Enable Zero-Shot Machine Translation for Extremely Low-resource Languages. (arXiv:2305.05214v1 [cs.CL])",
    "abstract": "We address the task of machine translation from an extremely low-resource language (LRL) to English using cross-lingual transfer from a closely related high-resource language (HRL). For many of these languages, no parallel corpora are available, even monolingual corpora are limited and representations in pre-trained sequence-to-sequence models are absent. These factors limit the benefits of cross-lingual transfer from shared embedding spaces in multilingual models. However, many extremely LRLs have a high level of lexical similarity with related HRLs. We utilize this property by injecting character and character-span noise into the training data of the HRL prior to learning the vocabulary. This serves as a regularizer which makes the model more robust to lexical divergences between the HRL and LRL and better facilitates cross-lingual transfer. On closely related HRL and LRL pairs from multiple language families, we observe that our method significantly outperforms the baseline MT as we",
    "link": "http://arxiv.org/abs/2305.05214",
    "context": "Title: Utilizing Lexical Similarity to Enable Zero-Shot Machine Translation for Extremely Low-resource Languages. (arXiv:2305.05214v1 [cs.CL])\nAbstract: We address the task of machine translation from an extremely low-resource language (LRL) to English using cross-lingual transfer from a closely related high-resource language (HRL). For many of these languages, no parallel corpora are available, even monolingual corpora are limited and representations in pre-trained sequence-to-sequence models are absent. These factors limit the benefits of cross-lingual transfer from shared embedding spaces in multilingual models. However, many extremely LRLs have a high level of lexical similarity with related HRLs. We utilize this property by injecting character and character-span noise into the training data of the HRL prior to learning the vocabulary. This serves as a regularizer which makes the model more robust to lexical divergences between the HRL and LRL and better facilitates cross-lingual transfer. On closely related HRL and LRL pairs from multiple language families, we observe that our method significantly outperforms the baseline MT as we",
    "path": "papers/23/05/2305.05214.json",
    "total_tokens": 955,
    "translated_title": "利用词汇相似性实现极低资源语言的零样本机器翻译",
    "translated_abstract": "我们解决了从极低资源语言（LRL）到英语的机器翻译任务，采用从密切相关的高资源语言（HRL）进行跨语言转移。对于许多这些语言，没有平行语料库可用，即使是单语料库也很有限，并且在预训练的序列到序列模型中表示也缺失。这些因素限制了从多语言模型中共享嵌入空间的跨语言转移的好处。然而，许多极低资源语言与相关的高资源语言具有很高的词汇相似性。我们利用这个属性，将字符和字符跨度的噪声注入到HRL的训练数据中，然后再学习词汇表。这作为一个正则化器，使模型更能抵御HRL和LRL之间的词汇差异，并更好地促进跨语言转移。在来自多个语言家族的密切相关的HRL和LRL对上，我们观察到我们的方法显著优于基线机器翻译模型。",
    "tldr": "本文解决了极低资源语言到英语的机器翻译任务，利用密切相关的高资源语言的词汇相似性，注入噪声作为正则化器，使模型更能抵御词汇差异，从而更好地促进跨语言转移。",
    "en_tdlr": "This paper addresses the task of machine translation from extremely low-resource languages to English by utilizing lexical similarity with closely related high-resource languages and injecting noise as a regularizer, which significantly improves cross-lingual transfer performance."
}