{
    "title": "Label Smoothing is Robustification against Model Misspecification. (arXiv:2305.08501v1 [stat.ML])",
    "abstract": "Label smoothing (LS) adopts smoothed targets in classification tasks. For example, in binary classification, instead of the one-hot target $(1,0)^\\top$ used in conventional logistic regression (LR), LR with LS (LSLR) uses the smoothed target $(1-\\frac{\\alpha}{2},\\frac{\\alpha}{2})^\\top$ with a smoothing level $\\alpha\\in(0,1)$, which causes squeezing of values of the logit. Apart from the common regularization-based interpretation of LS that leads to an inconsistent probability estimator, we regard LSLR as modifying the loss function and consistent estimator for probability estimation. In order to study the significance of each of these two modifications by LSLR, we introduce a modified LSLR (MLSLR) that uses the same loss function as LSLR and the same consistent estimator as LR, while not squeezing the logits. For the loss function modification, we theoretically show that MLSLR with a larger smoothing level has lower efficiency with correctly-specified models, while it exhibits higher r",
    "link": "http://arxiv.org/abs/2305.08501",
    "context": "Title: Label Smoothing is Robustification against Model Misspecification. (arXiv:2305.08501v1 [stat.ML])\nAbstract: Label smoothing (LS) adopts smoothed targets in classification tasks. For example, in binary classification, instead of the one-hot target $(1,0)^\\top$ used in conventional logistic regression (LR), LR with LS (LSLR) uses the smoothed target $(1-\\frac{\\alpha}{2},\\frac{\\alpha}{2})^\\top$ with a smoothing level $\\alpha\\in(0,1)$, which causes squeezing of values of the logit. Apart from the common regularization-based interpretation of LS that leads to an inconsistent probability estimator, we regard LSLR as modifying the loss function and consistent estimator for probability estimation. In order to study the significance of each of these two modifications by LSLR, we introduce a modified LSLR (MLSLR) that uses the same loss function as LSLR and the same consistent estimator as LR, while not squeezing the logits. For the loss function modification, we theoretically show that MLSLR with a larger smoothing level has lower efficiency with correctly-specified models, while it exhibits higher r",
    "path": "papers/23/05/2305.08501.json",
    "total_tokens": 852,
    "translated_title": "标签平滑是对模型规范化的强化防御",
    "translated_abstract": "标签平滑（LS）在分类任务中采用平滑目标。例如，在二元分类中，传统的逻辑回归（LR）使用独热目标$(1,0)^\\top$，而使用标签平滑的LR（LSLR）使用平滑后的目标$(1-\\frac{\\alpha}{2},\\frac{\\alpha}{2})^\\top$，其中$\\alpha\\in(0,1)$是平滑等级，它会导致logit值挤压。除了标签平滑的常见规范化解释导致不一致的概率估计器之外，我们将LSLR视为修改损失函数和一致的概率估计器。为了研究LSLR对这两种修改的重要性，我们引入了一个修改版的LSLR，即MLSLR，它使用与LSLR相同的损失函数和与LR相同的一致估计器，但不会挤压logit值。",
    "tldr": "标签平滑是用于修改损失函数和一致的估计器的方法，它可以提高正确规定模型的效率，减小模型规范化不正确的影响。",
    "en_tdlr": "Label smoothing is a method for modifying the loss function and consistent estimator in classification tasks, which can improve the efficiency of correctly specified models and reduce the impact of misspecified regularization."
}