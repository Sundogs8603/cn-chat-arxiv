{
    "title": "Many or Few Samples? Comparing Transfer, Contrastive and Meta-Learning in Encrypted Traffic Classification. (arXiv:2305.12432v2 [cs.LG] UPDATED)",
    "abstract": "The popularity of Deep Learning (DL), coupled with network traffic visibility reduction due to the increased adoption of HTTPS, QUIC and DNS-SEC, re-ignited interest towards Traffic Classification (TC). However, to tame the dependency from task-specific large labeled datasets we need to find better ways to learn representations that are valid across tasks. In this work we investigate this problem comparing transfer learning, meta-learning and contrastive learning against reference Machine Learning (ML) tree-based and monolithic DL models (16 methods total). Using two publicly available datasets, namely MIRAGE19 (40 classes) and AppClassNet (500 classes), we show that (i) using large datasets we can obtain more general representations, (ii) contrastive learning is the best methodology and (iii) meta-learning the worst one, and (iv) while ML tree-based cannot handle large tasks but fits well small tasks, by means of reusing learned representations, DL methods are reaching tree-based mode",
    "link": "http://arxiv.org/abs/2305.12432",
    "context": "Title: Many or Few Samples? Comparing Transfer, Contrastive and Meta-Learning in Encrypted Traffic Classification. (arXiv:2305.12432v2 [cs.LG] UPDATED)\nAbstract: The popularity of Deep Learning (DL), coupled with network traffic visibility reduction due to the increased adoption of HTTPS, QUIC and DNS-SEC, re-ignited interest towards Traffic Classification (TC). However, to tame the dependency from task-specific large labeled datasets we need to find better ways to learn representations that are valid across tasks. In this work we investigate this problem comparing transfer learning, meta-learning and contrastive learning against reference Machine Learning (ML) tree-based and monolithic DL models (16 methods total). Using two publicly available datasets, namely MIRAGE19 (40 classes) and AppClassNet (500 classes), we show that (i) using large datasets we can obtain more general representations, (ii) contrastive learning is the best methodology and (iii) meta-learning the worst one, and (iv) while ML tree-based cannot handle large tasks but fits well small tasks, by means of reusing learned representations, DL methods are reaching tree-based mode",
    "path": "papers/23/05/2305.12432.json",
    "total_tokens": 925,
    "translated_title": "多还是少样本？在加密流量分类中比较迁移学习、对比学习和元学习。",
    "translated_abstract": "深度学习（DL）的流行，加上由于HTTPS、QUIC和DNS-SEC的广泛采用导致网络流量可见性降低，重新引起了对流量分类（TC）的关注。然而，为了控制依赖于特定任务的大型标记数据集的情况，我们需要寻找更好的方法来学习跨任务有效的表示。在这项工作中，我们将迁移学习、元学习和对比学习与参考机器学习（ML）基于树和单片式DL模型（总共16种方法）进行了比较。使用两个公开可用的数据集，即MIRAGE19（40类）和AppClassNet（500类），我们表明（i）使用大型数据集，我们可以得到更通用的表示，（ii）对比学习是最好的方法，（iii）元学习是最差的方法，（iv）虽然ML基于树的方法不能处理大型任务，但通过重复使用学习到的表示，DL方法正在接近基于树的模式。",
    "tldr": "该论文比较了迁移学习、元学习和对比学习等方法在加密流量分类中的效果，发现对比学习是最优的方法，并证明通过重复利用学习到的表示，DL方法可以接近基于树的模式。",
    "en_tdlr": "This paper compares the effectiveness of transfer learning, meta-learning, and contrastive learning in encrypted traffic classification, and finds that contrastive learning is the best method. It also demonstrates that by reusing learned representations, DL methods can approach tree-based models."
}