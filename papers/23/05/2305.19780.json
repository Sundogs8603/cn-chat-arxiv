{
    "title": "A technique to jointly estimate depth and depth uncertainty for unmanned aerial vehicles. (arXiv:2305.19780v1 [cs.CV])",
    "abstract": "When used by autonomous vehicles for trajectory planning or obstacle avoidance, depth estimation methods need to be reliable. Therefore, estimating the quality of the depth outputs is critical. In this paper, we show how M4Depth, a state-of-the-art depth estimation method designed for unmanned aerial vehicle (UAV) applications, can be enhanced to perform joint depth and uncertainty estimation. For that, we present a solution to convert the uncertainty estimates related to parallax generated by M4Depth into uncertainty estimates related to depth, and show that it outperforms the standard probabilistic approach. Our experiments on various public datasets demonstrate that our method performs consistently, even in zero-shot transfer. Besides, our method offers a compelling value when compared to existing multi-view depth estimation methods as it performs similarly on a multi-view depth estimation benchmark despite being 2.5 times faster and causal, as opposed to other methods. The code of ",
    "link": "http://arxiv.org/abs/2305.19780",
    "context": "Title: A technique to jointly estimate depth and depth uncertainty for unmanned aerial vehicles. (arXiv:2305.19780v1 [cs.CV])\nAbstract: When used by autonomous vehicles for trajectory planning or obstacle avoidance, depth estimation methods need to be reliable. Therefore, estimating the quality of the depth outputs is critical. In this paper, we show how M4Depth, a state-of-the-art depth estimation method designed for unmanned aerial vehicle (UAV) applications, can be enhanced to perform joint depth and uncertainty estimation. For that, we present a solution to convert the uncertainty estimates related to parallax generated by M4Depth into uncertainty estimates related to depth, and show that it outperforms the standard probabilistic approach. Our experiments on various public datasets demonstrate that our method performs consistently, even in zero-shot transfer. Besides, our method offers a compelling value when compared to existing multi-view depth estimation methods as it performs similarly on a multi-view depth estimation benchmark despite being 2.5 times faster and causal, as opposed to other methods. The code of ",
    "path": "papers/23/05/2305.19780.json",
    "total_tokens": 934,
    "translated_title": "一种联合估计无人机深度和深度不确定性的技术",
    "translated_abstract": "自主车辆在轨迹规划或避障时需要可靠的深度估计方法，并且估计深度输出的质量至关重要。本文介绍了如何增强针对无人机应用设计的最先进深度估计方法M4Depth，以执行联合深度和不确定性估计。我们提出了一种解决方案，将M4Depth产生的视差不确定性估计转换为深度相关的不确定性估计，并证明其优于标准的概率方法。我们在各种公共数据集上的实验表明，我们的方法表现始终如一，甚至在零样本转移时也能保持优异性能。此外，我们的方法与现有的多视深度估计方法相比，在多视深度估计基准测试方面表现类似，但速度更快，可造成，是其他方法的2.5倍。代码可供使用。",
    "tldr": "本文介绍了一种针对无人机深度估计的技术，可以联合估计深度和不确定性，针对M4Depth产生的视差不确定性进行转换，实现了优于标准概率方法的效果。实验表明，在各种公共数据集上始终保持着优异的性能，且速度更快，值得推广。",
    "en_tdlr": "This paper presents a technique for joint depth and uncertainty estimation for unmanned aerial vehicle (UAV) applications, which outperforms the standard probabilistic approach by converting the uncertainty estimates related to parallax generated by M4Depth into depth-related ones. The method offers consistently excellent performance on various public datasets and is 2.5 times faster and causal compared to existing multi-view depth estimation methods."
}