{
    "title": "Infinite Class Mixup. (arXiv:2305.10293v1 [cs.CV])",
    "abstract": "Mixup is a widely adopted strategy for training deep networks, where additional samples are augmented by interpolating inputs and labels of training pairs. Mixup has shown to improve classification performance, network calibration, and out-of-distribution generalisation. While effective, a cornerstone of Mixup, namely that networks learn linear behaviour patterns between classes, is only indirectly enforced since the output interpolation is performed at the probability level. This paper seeks to address this limitation by mixing the classifiers directly instead of mixing the labels for each mixed pair. We propose to define the target of each augmented sample as a uniquely new classifier, whose parameters are a linear interpolation of the classifier vectors of the input pair. The space of all possible classifiers is continuous and spans all interpolations between classifier pairs. To make optimisation tractable, we propose a dual-contrastive Infinite Class Mixup loss, where we contrast ",
    "link": "http://arxiv.org/abs/2305.10293",
    "context": "Title: Infinite Class Mixup. (arXiv:2305.10293v1 [cs.CV])\nAbstract: Mixup is a widely adopted strategy for training deep networks, where additional samples are augmented by interpolating inputs and labels of training pairs. Mixup has shown to improve classification performance, network calibration, and out-of-distribution generalisation. While effective, a cornerstone of Mixup, namely that networks learn linear behaviour patterns between classes, is only indirectly enforced since the output interpolation is performed at the probability level. This paper seeks to address this limitation by mixing the classifiers directly instead of mixing the labels for each mixed pair. We propose to define the target of each augmented sample as a uniquely new classifier, whose parameters are a linear interpolation of the classifier vectors of the input pair. The space of all possible classifiers is continuous and spans all interpolations between classifier pairs. To make optimisation tractable, we propose a dual-contrastive Infinite Class Mixup loss, where we contrast ",
    "path": "papers/23/05/2305.10293.json",
    "total_tokens": 852,
    "translated_title": "无限类别混合策略",
    "translated_abstract": "Mixup 是一种广泛采用的深度网络训练策略，通过插值输入和标签的训练对来增加额外的样本。 Mixup 已经证明可以提高分类性能、网络校准和超出分布概括。虽然有效，但Mixup的一个基石是网络在类别之间学习线性行为模式，但它只是间接地通过概率级别进行输出插值而强制执行。这篇论文旨在通过直接混合分类器而不是混合每个混合对的标签来解决这个限制。我们建议将每个增强的样本的目标定义为一个唯一的新分类器，其参数是输入对的分类器向量的线性插值。所有可能分类器的空间是连续的，涵盖了分类器对之间的所有插值。为了使优化可行，我们提出了一个双对比无限类混合损失，其中我们对每个样本对向所有其他样本对进行对比。",
    "tldr": "本文提出了一种直接通过混合分类器而不是标签来增强样本的策略。新的分类器是输入对分类器向量的线性插值，使分类器之间的关系更加准确，从而提高深度网络的分类性能。",
    "en_tdlr": "This paper proposes a strategy to augment samples by directly mixing classifiers instead of labels, where the new classifier is a linear interpolation of the classifier vectors of the input pair, improving classification performance by accurately representing relationships between classifiers."
}