{
    "title": "Revisiting Structured Variational Autoencoders. (arXiv:2305.16543v1 [stat.ML])",
    "abstract": "Structured variational autoencoders (SVAEs) combine probabilistic graphical model priors on latent variables, deep neural networks to link latent variables to observed data, and structure-exploiting algorithms for approximate posterior inference. These models are particularly appealing for sequential data, where the prior can capture temporal dependencies. However, despite their conceptual elegance, SVAEs have proven difficult to implement, and more general approaches have been favored in practice. Here, we revisit SVAEs using modern machine learning tools and demonstrate their advantages over more general alternatives in terms of both accuracy and efficiency. First, we develop a modern implementation for hardware acceleration, parallelization, and automatic differentiation of the message passing algorithms at the core of the SVAE. Second, we show that by exploiting structure in the prior, the SVAE learns more accurate models and posterior distributions, which translate into improved p",
    "link": "http://arxiv.org/abs/2305.16543",
    "context": "Title: Revisiting Structured Variational Autoencoders. (arXiv:2305.16543v1 [stat.ML])\nAbstract: Structured variational autoencoders (SVAEs) combine probabilistic graphical model priors on latent variables, deep neural networks to link latent variables to observed data, and structure-exploiting algorithms for approximate posterior inference. These models are particularly appealing for sequential data, where the prior can capture temporal dependencies. However, despite their conceptual elegance, SVAEs have proven difficult to implement, and more general approaches have been favored in practice. Here, we revisit SVAEs using modern machine learning tools and demonstrate their advantages over more general alternatives in terms of both accuracy and efficiency. First, we develop a modern implementation for hardware acceleration, parallelization, and automatic differentiation of the message passing algorithms at the core of the SVAE. Second, we show that by exploiting structure in the prior, the SVAE learns more accurate models and posterior distributions, which translate into improved p",
    "path": "papers/23/05/2305.16543.json",
    "total_tokens": 831,
    "translated_title": "重温结构化变分自编码器",
    "translated_abstract": "结构化变分自编码器（SVAEs）将概率图模型的先验应用于潜变量，利用深度神经网络将潜变量与观测数据联系起来，并使用结构化算法进行近似后验推断。这些模型对于序列数据特别有吸引力，因为先验可以捕捉时间依赖关系。然而，尽管其理念优美，但实现难度较大，实际应用中更通用的方法更受青睐。本文采用现代机器学习工具重新审视SVAEs，并证明它们在精度和效率方面优于更一般的替代方法。首先，我们开发了现代实现方法，对SVAE核心的消息传递算法进行了硬件加速、并行化和自动求导。其次，我们展示了通过利用先验中的结构，SVAE可以学习更精确的模型和后验分布，这转化为了性能的提升。",
    "tldr": "本文重温结构化变分自编码器，开发了现代实现方法并证明其在精度和效率方面优于更一般的替代方法。",
    "en_tdlr": "This paper revisits structured variational autoencoders (SVAEs), develops a modern implementation method, and demonstrates their advantages over more general alternatives in terms of both accuracy and efficiency by exploiting structure in the prior."
}