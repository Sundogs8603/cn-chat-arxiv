{
    "title": "Constructing Word-Context-Coupled Space Aligned with Associative Knowledge Relations for Interpretable Language Modeling. (arXiv:2305.11543v1 [cs.CL])",
    "abstract": "As the foundation of current natural language processing methods, pre-trained language model has achieved excellent performance. However, the black-box structure of the deep neural network in pre-trained language models seriously limits the interpretability of the language modeling process. After revisiting the coupled requirement of deep neural representation and semantics logic of language modeling, a Word-Context-Coupled Space (W2CSpace) is proposed by introducing the alignment processing between uninterpretable neural representation and interpretable statistical logic. Moreover, a clustering process is also designed to connect the word- and context-level semantics. Specifically, an associative knowledge network (AKN), considered interpretable statistical logic, is introduced in the alignment process for word-level semantics. Furthermore, the context-relative distance is employed as the semantic feature for the downstream classifier, which is greatly different from the current unint",
    "link": "http://arxiv.org/abs/2305.11543",
    "context": "Title: Constructing Word-Context-Coupled Space Aligned with Associative Knowledge Relations for Interpretable Language Modeling. (arXiv:2305.11543v1 [cs.CL])\nAbstract: As the foundation of current natural language processing methods, pre-trained language model has achieved excellent performance. However, the black-box structure of the deep neural network in pre-trained language models seriously limits the interpretability of the language modeling process. After revisiting the coupled requirement of deep neural representation and semantics logic of language modeling, a Word-Context-Coupled Space (W2CSpace) is proposed by introducing the alignment processing between uninterpretable neural representation and interpretable statistical logic. Moreover, a clustering process is also designed to connect the word- and context-level semantics. Specifically, an associative knowledge network (AKN), considered interpretable statistical logic, is introduced in the alignment process for word-level semantics. Furthermore, the context-relative distance is employed as the semantic feature for the downstream classifier, which is greatly different from the current unint",
    "path": "papers/23/05/2305.11543.json",
    "total_tokens": 867,
    "translated_title": "构建基于联想知识关系的词-上下文耦合空间用于可解释化语言建模",
    "translated_abstract": "作为当前自然语言处理方法的基础，预训练语言模型已经取得了出色的表现。然而，预训练语言模型中深度神经网络的黑箱结构严重限制了语言建模过程的可解释性。本文通过重新审视深度神经表示和语言建模的语义逻辑之间的耦合要求，引入了词-上下文耦合空间(W2CSpace)，通过介绍可解释的统计逻辑和不可解释的神经表示之间的对齐处理来实现。此外，还设计了一种聚类过程来连接词级和上下文级语义。具体来说，在对齐词级语义的过程中引入了可解释的统计逻辑的联想知识网络(AKN)。此外，上下文相对距离被用作下游分类器的语义特征，这与当前的非解释性方法非常不同。",
    "tldr": "本文提出了一种可解释的语言建模方法，通过构建词-上下文耦合空间，并引入联想知识网络和上下文相对距离作为语义特征，实现了语言建模可解释性的提高。"
}