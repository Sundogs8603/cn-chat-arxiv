{
    "title": "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models. (arXiv:2305.08283v2 [cs.CL] UPDATED)",
    "abstract": "Language models (LMs) are pretrained on diverse data sources, including news, discussion forums, books, and online encyclopedias. A significant portion of this data includes opinions and perspectives which, on one hand, celebrate democracy and diversity of ideas, and on the other hand are inherently socially biased. Our work develops new methods to (1) measure political biases in LMs trained on such corpora, along social and economic axes, and (2) measure the fairness of downstream NLP models trained on top of politically biased LMs. We focus on hate speech and misinformation detection, aiming to empirically quantify the effects of political (social, economic) biases in pretraining data on the fairness of high-stakes social-oriented tasks. Our findings reveal that pretrained LMs do have political leanings that reinforce the polarization present in pretraining corpora, propagating social biases into hate speech predictions and misinformation detectors. We discuss the implications of our",
    "link": "http://arxiv.org/abs/2305.08283",
    "context": "Title: From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models. (arXiv:2305.08283v2 [cs.CL] UPDATED)\nAbstract: Language models (LMs) are pretrained on diverse data sources, including news, discussion forums, books, and online encyclopedias. A significant portion of this data includes opinions and perspectives which, on one hand, celebrate democracy and diversity of ideas, and on the other hand are inherently socially biased. Our work develops new methods to (1) measure political biases in LMs trained on such corpora, along social and economic axes, and (2) measure the fairness of downstream NLP models trained on top of politically biased LMs. We focus on hate speech and misinformation detection, aiming to empirically quantify the effects of political (social, economic) biases in pretraining data on the fairness of high-stakes social-oriented tasks. Our findings reveal that pretrained LMs do have political leanings that reinforce the polarization present in pretraining corpora, propagating social biases into hate speech predictions and misinformation detectors. We discuss the implications of our",
    "path": "papers/23/05/2305.08283.json",
    "total_tokens": 1024,
    "translated_title": "从预训练数据到语言模型再到下游任务：追踪导致不公平NLP模型的政治偏见的轨迹。",
    "translated_abstract": "语言模型(LMs)是预训练在不同数据源上的，包括新闻、讨论论坛、书籍和在线百科全书等。这些数据中的相当一部分包括观点和角度，一方面赞扬民主和思想多样性，另一方面具有固有的社会偏见。我们的工作开发了新的方法来(1)测量基于此类语料库训练的LMs中的政治偏见，沿社会和经济轴，以及(2)衡量基于政治偏见的LMs训练的下游NLP模型的公平性。我们关注仇恨言论和虚假信息检测，旨在实证量化预训练数据中政治(社会、经济)偏见对高风险社会导向任务公正性的影响。我们的研究结果表明，预训练LMs确实存在政治倾向，加强了预训练语料库中存在的极化，将社会偏见传播到仇恨言论预测和虚假信息检测器中。我们讨论了研究的意义，并提出了开发更公正、更无偏NLP模型的建议。",
    "tldr": "本文研究测量了政治偏见在预训练语言模型和下游任务中的影响，发现预训练模型存在政治倾向，并将社会偏见传递到下游任务中，从而导致NLP模型的不公平性。",
    "en_tdlr": "This article aims to quantify the impact of political biases in pretraining data on downstream NLP models, specifically hate speech and misinformation detection. The study found that pretrained language models do have political leanings and propagate social biases, leading to unfair NLP models."
}