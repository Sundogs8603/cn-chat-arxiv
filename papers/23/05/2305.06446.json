{
    "title": "Multi-agent Reinforcement Learning: Asynchronous Communication and Linear Function Approximation. (arXiv:2305.06446v1 [cs.LG])",
    "abstract": "We study multi-agent reinforcement learning in the setting of episodic Markov decision processes, where multiple agents cooperate via communication through a central server. We propose a provably efficient algorithm based on value iteration that enable asynchronous communication while ensuring the advantage of cooperation with low communication overhead. With linear function approximation, we prove that our algorithm enjoys an $\\tilde{\\mathcal{O}}(d^{3/2}H^2\\sqrt{K})$ regret with $\\tilde{\\mathcal{O}}(dHM^2)$ communication complexity, where $d$ is the feature dimension, $H$ is the horizon length, $M$ is the total number of agents, and $K$ is the total number of episodes. We also provide a lower bound showing that a minimal $\\Omega(dM)$ communication complexity is required to improve the performance through collaboration.",
    "link": "http://arxiv.org/abs/2305.06446",
    "context": "Title: Multi-agent Reinforcement Learning: Asynchronous Communication and Linear Function Approximation. (arXiv:2305.06446v1 [cs.LG])\nAbstract: We study multi-agent reinforcement learning in the setting of episodic Markov decision processes, where multiple agents cooperate via communication through a central server. We propose a provably efficient algorithm based on value iteration that enable asynchronous communication while ensuring the advantage of cooperation with low communication overhead. With linear function approximation, we prove that our algorithm enjoys an $\\tilde{\\mathcal{O}}(d^{3/2}H^2\\sqrt{K})$ regret with $\\tilde{\\mathcal{O}}(dHM^2)$ communication complexity, where $d$ is the feature dimension, $H$ is the horizon length, $M$ is the total number of agents, and $K$ is the total number of episodes. We also provide a lower bound showing that a minimal $\\Omega(dM)$ communication complexity is required to improve the performance through collaboration.",
    "path": "papers/23/05/2305.06446.json",
    "total_tokens": 913,
    "translated_title": "多智能体强化学习: 异步通信和线性函数逼近",
    "translated_abstract": "我们研究了多智能体强化学习在情节式马尔科夫决策过程中的设置，多个智能体通过中央服务器进行通信以合作。我们提出了一种基于值迭代的可证明有效的算法，可以实现异步通信，同时确保合作优势且通信开销低。我们证明了在使用线性函数逼近的情况下，我们的算法具有 $\\tilde{\\mathcal{O}}(d^{3/2}H^2\\sqrt{K})$ 的遗憾值和 $\\tilde{\\mathcal{O}}(dHM^2)$ 的通信复杂度，其中 $d$ 是特征维数，$H$ 是时间跨度，$M$ 是智能体总数，$K$ 是总情节数。我们还提供了一个下限证明，表明通过协作至少需要 $\\Omega(dM)$ 的通信复杂度才能改善性能。",
    "tldr": "该论文探讨了多智能体强化学习在情节式马尔可夫决策过程中的协作问题，提出了一种基于值迭代的算法，可以实现异步通信，在保证合作优势的同时降低通信开销。通过提供和证明的算法和复杂度界限，为多智能体强化学习在实际应用中提供理论依据。",
    "en_tdlr": "This paper addresses the collaboration problem in multi-agent reinforcement learning in the setting of episodic Markov decision processes, proposes an algorithm based on value iteration that enables asynchronous communication with low communication overhead, and provides algorithm and complexity bounds for practical applications of multi-agent reinforcement learning."
}