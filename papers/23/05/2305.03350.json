{
    "title": "Reconstructing Training Data from Multiclass Neural Networks. (arXiv:2305.03350v1 [cs.LG])",
    "abstract": "Reconstructing samples from the training set of trained neural networks is a major privacy concern. Haim et al. (2022) recently showed that it is possible to reconstruct training samples from neural network binary classifiers, based on theoretical results about the implicit bias of gradient methods. In this work, we present several improvements and new insights over this previous work. As our main improvement, we show that training-data reconstruction is possible in the multi-class setting and that the reconstruction quality is even higher than in the case of binary classification. Moreover, we show that using weight-decay during training increases the vulnerability to sample reconstruction. Finally, while in the previous work the training set was of size at most $1000$ from $10$ classes, we show preliminary evidence of the ability to reconstruct from a model trained on $5000$ samples from $100$ classes.",
    "link": "http://arxiv.org/abs/2305.03350",
    "context": "Title: Reconstructing Training Data from Multiclass Neural Networks. (arXiv:2305.03350v1 [cs.LG])\nAbstract: Reconstructing samples from the training set of trained neural networks is a major privacy concern. Haim et al. (2022) recently showed that it is possible to reconstruct training samples from neural network binary classifiers, based on theoretical results about the implicit bias of gradient methods. In this work, we present several improvements and new insights over this previous work. As our main improvement, we show that training-data reconstruction is possible in the multi-class setting and that the reconstruction quality is even higher than in the case of binary classification. Moreover, we show that using weight-decay during training increases the vulnerability to sample reconstruction. Finally, while in the previous work the training set was of size at most $1000$ from $10$ classes, we show preliminary evidence of the ability to reconstruct from a model trained on $5000$ samples from $100$ classes.",
    "path": "papers/23/05/2305.03350.json",
    "total_tokens": 847,
    "translated_title": "从多类神经网络中重建训练数据",
    "translated_abstract": "从经过训练的神经网络的训练集中重建样本是一项重要的隐私问题。Haim等人最近表明，基于梯度方法的隐含偏差的理论结果，可以从神经网络二元分类器中重建训练样本。在本文中，我们对这项先前工作进行了多项改进和新的见解。作为我们的主要改进，我们表明在多类别设置下训练数据的重建是可能的，而且重建质量甚至比二元分类情况更高。此外，我们展示了在训练过程中使用权重衰减会增加样本重建的易受性。最后，虽然先前的工作中，训练集的大小最多只有来自10个类别的1000个样本，但我们展示了初步证据表明能够从从100个类别的5000个样本训练的模型中进行重建。",
    "tldr": "本文研究了从多类神经网络中重建训练数据的问题，比以往的二元分类情况更容易实现，并且发现在训练过程中使用权重衰减会增加样本重建的易受性。",
    "en_tdlr": "This paper investigates the problem of reconstructing training data from multiclass neural networks, which is even easier to achieve than in the case of binary classification. The authors also show that using weight-decay during training increases the vulnerability to sample reconstruction."
}