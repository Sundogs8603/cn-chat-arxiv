{
    "title": "MGR: Multi-generator based Rationalization. (arXiv:2305.04492v2 [cs.LG] UPDATED)",
    "abstract": "Rationalization is to employ a generator and a predictor to construct a self-explaining NLP model in which the generator selects a subset of human-intelligible pieces of the input text to the following predictor. However, rationalization suffers from two key challenges, i.e., spurious correlation and degeneration, where the predictor overfits the spurious or meaningless pieces solely selected by the not-yet well-trained generator and in turn deteriorates the generator. Although many studies have been proposed to address the two challenges, they are usually designed separately and do not take both of them into account. In this paper, we propose a simple yet effective method named MGR to simultaneously solve the two problems. The key idea of MGR is to employ multiple generators such that the occurrence stability of real pieces is improved and more meaningful pieces are delivered to the predictor. Empirically, we show that MGR improves the F1 score by up to 20.9% as compared to state-of-t",
    "link": "http://arxiv.org/abs/2305.04492",
    "context": "Title: MGR: Multi-generator based Rationalization. (arXiv:2305.04492v2 [cs.LG] UPDATED)\nAbstract: Rationalization is to employ a generator and a predictor to construct a self-explaining NLP model in which the generator selects a subset of human-intelligible pieces of the input text to the following predictor. However, rationalization suffers from two key challenges, i.e., spurious correlation and degeneration, where the predictor overfits the spurious or meaningless pieces solely selected by the not-yet well-trained generator and in turn deteriorates the generator. Although many studies have been proposed to address the two challenges, they are usually designed separately and do not take both of them into account. In this paper, we propose a simple yet effective method named MGR to simultaneously solve the two problems. The key idea of MGR is to employ multiple generators such that the occurrence stability of real pieces is improved and more meaningful pieces are delivered to the predictor. Empirically, we show that MGR improves the F1 score by up to 20.9% as compared to state-of-t",
    "path": "papers/23/05/2305.04492.json",
    "total_tokens": 864,
    "translated_abstract": "可解释性是利用生成器和预测器构建自解释的NLP模型的过程，其中生成器选择一部分人类可理解的输入文本，传递给后续的预测器。然而，可解释性在应用中面临两个主要挑战，即虚假相关性和恶化问题，其中预测器会过度拟合不成熟的生成器仅选择的虚假或无意义的碎片，从而导致生成器不稳定。尽管有许多研究来解决这两个问题，但它们通常是分别设计的，没有同时考虑两个问题。本文提出了一种名为MGR的简单而有效的方法，以同时解决这两个问题。MGR的关键思想是使用多个生成器，从而提高真实部分的出现稳定性，并向预测器提供更有意义的部分。实验证明，MGR将F1得分提高了高达20.9%。",
    "tldr": "本文提出了一种名为MGR的基于多生成器的NLP模型可解释性技术，用以解决可解释性中的虚假相关性和恶化问题，提高预测性能。",
    "en_tdlr": "This paper proposes a multi-generator based NLP model explainability technique named MGR to solve the challenges of spurious correlation and degeneration in rationalization, which leads to improved predictive performance."
}