{
    "title": "End-to-end Training of Deep Boltzmann Machines by Unbiased Contrastive Divergence with Local Mode Initialization. (arXiv:2305.19684v1 [cs.LG])",
    "abstract": "We address the problem of biased gradient estimation in deep Boltzmann machines (DBMs). The existing method to obtain an unbiased estimator uses a maximal coupling based on a Gibbs sampler, but when the state is high-dimensional, it takes a long time to converge. In this study, we propose to use a coupling based on the Metropolis-Hastings (MH) and to initialize the state around a local mode of the target distribution. Because of the propensity of MH to reject proposals, the coupling tends to converge in only one step with a high probability, leading to high efficiency. We find that our method allows DBMs to be trained in an end-to-end fashion without greedy pretraining. We also propose some practical techniques to further improve the performance of DBMs. We empirically demonstrate that our training algorithm enables DBMs to show comparable generative performance to other deep generative models, achieving the FID score of 10.33 for MNIST.",
    "link": "http://arxiv.org/abs/2305.19684",
    "context": "Title: End-to-end Training of Deep Boltzmann Machines by Unbiased Contrastive Divergence with Local Mode Initialization. (arXiv:2305.19684v1 [cs.LG])\nAbstract: We address the problem of biased gradient estimation in deep Boltzmann machines (DBMs). The existing method to obtain an unbiased estimator uses a maximal coupling based on a Gibbs sampler, but when the state is high-dimensional, it takes a long time to converge. In this study, we propose to use a coupling based on the Metropolis-Hastings (MH) and to initialize the state around a local mode of the target distribution. Because of the propensity of MH to reject proposals, the coupling tends to converge in only one step with a high probability, leading to high efficiency. We find that our method allows DBMs to be trained in an end-to-end fashion without greedy pretraining. We also propose some practical techniques to further improve the performance of DBMs. We empirically demonstrate that our training algorithm enables DBMs to show comparable generative performance to other deep generative models, achieving the FID score of 10.33 for MNIST.",
    "path": "papers/23/05/2305.19684.json",
    "total_tokens": 942,
    "translated_title": "通过局部模态初始化和无偏差对比散度实现深度玻尔兹曼机的端到端训练",
    "translated_abstract": "本研究解决了深度玻尔兹曼机（DBMs）中的偏差梯度估计问题。现有的获取无偏估计量的方法使用基于Gibbs采样的最大耦合，但当状态是高维时，其收敛需要很长时间。因此，我们提出了基于Metropolis-Hastings的耦合，并围绕目标分布的局部模态初始化状态。由于MH倾向于拒绝提案，这种耦合有很高的概率在一步内收敛，因此具有高效性。我们发现，我们的方法可以在不进行贪心预训练的情况下端到端地训练DBMs。我们还提出了一些实用技术，以进一步提高DBMs的性能。我们通过实验证明，我们的训练算法使DBMs能够展现出与其他深度生成模型相当的生成性能，在MNIST上达到了10.33的FID分数。",
    "tldr": "本研究提出一种基于Metropolis-Hastings耦合和局部模态初始化的方法，解决了深度玻尔兹曼机中的偏差梯度估计问题，使得DBMs可以端到端地训练，实验结果表明与其他深度生成模型相当的生成性能。",
    "en_tdlr": "This study proposes a method based on Metropolis-Hastings coupling and local mode initialization, which solves the biased gradient estimation problem in deep Boltzmann machines (DBMs) and enables DBMs to be trained in an end-to-end fashion. The experimental results show comparable generative performance to other deep generative models."
}