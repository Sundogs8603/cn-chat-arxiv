{
    "title": "Learn to Compose Syntactic and Semantic Representations Appropriately for Compositional Generalization. (arXiv:2305.12169v1 [cs.CL])",
    "abstract": "Recent studies have shown that sequence-to-sequence (Seq2Seq) models are limited in solving the compositional generalization (CG) tasks, failing to systematically generalize to unseen compositions of seen components. There is mounting evidence that one of the reasons hindering CG is the representation of the encoder uppermost layer is entangled. In other words, the syntactic and semantic representations of sequences are twisted inappropriately. However, most previous studies mainly concentrate on enhancing semantic information at token-level, rather than composing the syntactic and semantic representations of sequences appropriately as humans do. In addition, we consider the representation entanglement problem they found is not comprehensive, and further hypothesize that source keys and values representations passing into different decoder layers are also entangled. Staring from this intuition and inspired by humans' strategies for CG, we propose COMPSITION (Compose Syntactic and Seman",
    "link": "http://arxiv.org/abs/2305.12169",
    "context": "Title: Learn to Compose Syntactic and Semantic Representations Appropriately for Compositional Generalization. (arXiv:2305.12169v1 [cs.CL])\nAbstract: Recent studies have shown that sequence-to-sequence (Seq2Seq) models are limited in solving the compositional generalization (CG) tasks, failing to systematically generalize to unseen compositions of seen components. There is mounting evidence that one of the reasons hindering CG is the representation of the encoder uppermost layer is entangled. In other words, the syntactic and semantic representations of sequences are twisted inappropriately. However, most previous studies mainly concentrate on enhancing semantic information at token-level, rather than composing the syntactic and semantic representations of sequences appropriately as humans do. In addition, we consider the representation entanglement problem they found is not comprehensive, and further hypothesize that source keys and values representations passing into different decoder layers are also entangled. Staring from this intuition and inspired by humans' strategies for CG, we propose COMPSITION (Compose Syntactic and Seman",
    "path": "papers/23/05/2305.12169.json",
    "total_tokens": 1061,
    "translated_title": "学习适当地组合句法和语义表示以进行组合泛化",
    "translated_abstract": "最近的研究表明，序列到序列（Seq2Seq）模型在解决组合泛化（CG）任务时存在局限性，无法系统性地推广到看不见的已知组件组合。越来越多的证据表明，阻碍CG的原因之一是编码器最上层的表示是纠缠的，也就是说，序列的句法和语义表示被不适当地扭曲了。然而，大多数以前的研究主要集中于在标记级别上增强语义信息，而不是适当地组合序列的句法和语义表示，就像人类所做的那样。此外，我们认为他们发现的表示纠缠问题不全面，并进一步假设传递到不同解码器层的源键值表示也是纠缠在一起的。基于这个直觉和受人类CG策略的启发，我们提出了COMPSITION（适当地组合句法和语义表示以进行组合泛化），这是一个解决CG任务的新框架。COMPSITION通过分别建模句法和语义表示，并通过几何表示模块将它们组合起来，显式地组合编码器的最上层。实验结果表明，COMPSITION在合成和自然语言CG任务上均实现了最先进的性能。",
    "tldr": "该研究提出了一个名为COMPSITION的新框架，可以通过适当地组合句法和语义表示来解决组合泛化问题。实验证明该方法在合成和自然语言CG任务上实现了最先进的性能。",
    "en_tdlr": "This study proposes a novel framework called COMPSITION, which can solve the compositional generalization problem by appropriately composing syntactic and semantic representations. Experimental results show that it achieves state-of-the-art performance on both synthetic and natural language CG tasks."
}