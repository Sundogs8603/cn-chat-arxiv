{
    "title": "Activation Functions Not To Active: A Plausible Theory on Interpreting Neural Networks. (arXiv:2305.00663v1 [cs.LG])",
    "abstract": "Researchers commonly believe that neural networks model a high-dimensional space but cannot give a clear definition of this space. What is this space? What is its dimension? And does it has finite dimensions? In this paper, we develop a plausible theory on interpreting neural networks in terms of the role of activation functions in neural networks and define a high-dimensional (more precisely, an infinite-dimensional) space. We conjunction that the activation function acts as a magnifying function that maps the low-dimensional linear space into an infinite-dimensional space. Given a dataset with each example of $d$ features $f_1$, $f_2$, $\\cdots$, $f_d$, we believe that NNs model a special space with infinite dimensions, each of which is a monomial $$\\prod_{i_1, i_2, \\cdots, i_d} f_1^{i_1} f_2^{i_2} \\cdots f_d^{i_d}$$ for some non-negative integers ${i_1, i_2, \\cdots, i_d} \\in \\mathbb{Z}_{0}^{+}=\\{0,1,2,3,\\ldots\\} $. We term such an infinite-dimensional space $\\textit{ Super Space (SS)",
    "link": "http://arxiv.org/abs/2305.00663",
    "context": "Title: Activation Functions Not To Active: A Plausible Theory on Interpreting Neural Networks. (arXiv:2305.00663v1 [cs.LG])\nAbstract: Researchers commonly believe that neural networks model a high-dimensional space but cannot give a clear definition of this space. What is this space? What is its dimension? And does it has finite dimensions? In this paper, we develop a plausible theory on interpreting neural networks in terms of the role of activation functions in neural networks and define a high-dimensional (more precisely, an infinite-dimensional) space. We conjunction that the activation function acts as a magnifying function that maps the low-dimensional linear space into an infinite-dimensional space. Given a dataset with each example of $d$ features $f_1$, $f_2$, $\\cdots$, $f_d$, we believe that NNs model a special space with infinite dimensions, each of which is a monomial $$\\prod_{i_1, i_2, \\cdots, i_d} f_1^{i_1} f_2^{i_2} \\cdots f_d^{i_d}$$ for some non-negative integers ${i_1, i_2, \\cdots, i_d} \\in \\mathbb{Z}_{0}^{+}=\\{0,1,2,3,\\ldots\\} $. We term such an infinite-dimensional space $\\textit{ Super Space (SS)",
    "path": "papers/23/05/2305.00663.json",
    "total_tokens": 827,
    "translated_title": "激活函数不再激活：神经网络解释的合理理论",
    "translated_abstract": "研究人员普遍认为神经网络模拟一个高维空间，但却不能清晰地定义这个空间。那么这个空间是什么？它的维数是多少？是否具有有限的维数？本文开发了一个关于激活函数在神经网络中作用的可行理论，定义了一个高维（更精确地，是一个无限维）的空间。我们认为激活函数充当了一个放大函数的作用，将低维线性空间映射成了一个无限维的空间。",
    "tldr": "本文提供了一个合理的理论来解释神经网络的高维空间，并且将激活函数的角色描述为放大函数，将低维线性空间映射为无限维超级空间。",
    "en_tdlr": "This paper provides a plausible theory to interpret the high-dimensional space of neural networks and describes the role of activation functions as a magnifying function that maps the low-dimensional linear space into an infinite-dimensional Super Space."
}