{
    "title": "How Predictable Are Large Language Model Capabilities? A Case Study on BIG-bench. (arXiv:2305.14947v2 [cs.CL] UPDATED)",
    "abstract": "We investigate the predictability of large language model (LLM) capabilities: given records of past experiments using different model families, numbers of parameters, tasks, and numbers of in-context examples, can we accurately predict LLM performance on new experiment configurations? Answering this question has practical implications for LLM users (e.g., deciding which models to try), developers (e.g., prioritizing evaluation on representative tasks), and the research community (e.g., identifying hard-to-predict capabilities that warrant further investigation).  We study the performance prediction problem on experiment records from BIG-bench. On a random train-test split, an MLP-based predictor achieves an $R^2$ score greater than 95%, indicating the presence of learnable patterns within the experiment records. We then formulate the problem of searching for \"small-bench,\" an informative subset of BIG-bench tasks from which the performance on the full set can be maximally recovered. We",
    "link": "http://arxiv.org/abs/2305.14947",
    "context": "Title: How Predictable Are Large Language Model Capabilities? A Case Study on BIG-bench. (arXiv:2305.14947v2 [cs.CL] UPDATED)\nAbstract: We investigate the predictability of large language model (LLM) capabilities: given records of past experiments using different model families, numbers of parameters, tasks, and numbers of in-context examples, can we accurately predict LLM performance on new experiment configurations? Answering this question has practical implications for LLM users (e.g., deciding which models to try), developers (e.g., prioritizing evaluation on representative tasks), and the research community (e.g., identifying hard-to-predict capabilities that warrant further investigation).  We study the performance prediction problem on experiment records from BIG-bench. On a random train-test split, an MLP-based predictor achieves an $R^2$ score greater than 95%, indicating the presence of learnable patterns within the experiment records. We then formulate the problem of searching for \"small-bench,\" an informative subset of BIG-bench tasks from which the performance on the full set can be maximally recovered. We",
    "path": "papers/23/05/2305.14947.json",
    "total_tokens": 916,
    "translated_title": "大型语言模型能力的可预测性如何？对BIG-bench的案例研究",
    "translated_abstract": "我们研究了大型语言模型（LLM）能力的可预测性：在使用不同模型家族、参数数量、任务数量和上下文示例数量的过去实验记录的基础上，我们能否准确预测LLM在新实验配置上的性能？回答这个问题对LLM用户（例如，决定尝试哪些模型）、开发者（例如，优先评估代表性任务）和研究社区（例如，识别需要进一步调查的难以预测的能力）具有实际意义。我们在BIG-bench的实验记录上研究了性能预测问题。在随机的训练-测试分离中，基于多层感知器（MLP）的预测器的R^2得分超过95%，表明实验记录中存在可学习的模式。然后，我们提出了寻找“small-bench”的问题，即从BIG-bench任务中寻找信息丰富的子集，可以从中最大程度地恢复整个集合的性能。",
    "tldr": "本研究通过对BIG-bench实验记录的研究，发现大型语言模型（LLM）的能力具有可预测性，并提出了寻找信息丰富的子集以最大程度恢复整个集合性能的问题。",
    "en_tdlr": "This research investigates the predictability of large language model (LLM) capabilities based on experiment records from BIG-bench, showing that LLM performance can be accurately predicted. It also proposes the problem of finding an informative subset to maximize the recovery of performance on the full set."
}