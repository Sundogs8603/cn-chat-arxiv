{
    "title": "Maintaining Stability and Plasticity for Predictive Churn Reduction. (arXiv:2305.04135v1 [cs.LG])",
    "abstract": "Deployed machine learning models should be updated to take advantage of a larger sample size to improve performance, as more data is gathered over time. Unfortunately, even when model updates improve aggregate metrics such as accuracy, they can lead to errors on samples that were correctly predicted by the previous model causing per-sample regression in performance known as predictive churn. Such prediction flips erode user trust thereby reducing the effectiveness of the human-AI team as a whole. We propose a solution called Accumulated Model Combination (AMC) based keeping the previous and current model version, and generating a meta-output using the prediction of the two models. AMC is a general technique and we propose several instances of it, each having their own advantages depending on the model and data properties. AMC requires minimal additional computation and changes to training procedures. We motivate the need for AMC by showing the difficulty of making a single model consis",
    "link": "http://arxiv.org/abs/2305.04135",
    "context": "Title: Maintaining Stability and Plasticity for Predictive Churn Reduction. (arXiv:2305.04135v1 [cs.LG])\nAbstract: Deployed machine learning models should be updated to take advantage of a larger sample size to improve performance, as more data is gathered over time. Unfortunately, even when model updates improve aggregate metrics such as accuracy, they can lead to errors on samples that were correctly predicted by the previous model causing per-sample regression in performance known as predictive churn. Such prediction flips erode user trust thereby reducing the effectiveness of the human-AI team as a whole. We propose a solution called Accumulated Model Combination (AMC) based keeping the previous and current model version, and generating a meta-output using the prediction of the two models. AMC is a general technique and we propose several instances of it, each having their own advantages depending on the model and data properties. AMC requires minimal additional computation and changes to training procedures. We motivate the need for AMC by showing the difficulty of making a single model consis",
    "path": "papers/23/05/2305.04135.json",
    "total_tokens": 955,
    "translated_title": "维护稳定性和可塑性以预测流失率的降低",
    "translated_abstract": "部署的机器学习模型应该更新以利用更多的样本提高性能，随着时间的推移不断收集更多的数据。然而，即使模型更新提高了诸如准确性之类的聚合指标，它们也可能导致在先前模型正确预测的样本中出现错误，从而在性能上逐个降低，这称为预测性流失。这种预测上的差错会削弱用户的信任，从而降低人工智能团队的整体效力。我们提出一种解决方案，称为积累模型组合(AMC)，基于保留之前和当前的模型版本，并使用两个模型的预测生成一个元输出。AMC是一种通用技术，我们提出了几个实例，每个实例都有它自己的优点，具体取决于模型和数据属性。AMC需要最少的额外计算和训练过程的改变。我们通过展示在三个数据集上使单个模型始终保持一致的困难来说明AMC的必要性。在真实世界和合成数据集上的实验表明，AMC在保持或提高总体准确性的同时，显著降低了预测性流失，甚至比最先进的方法都好。",
    "tldr": "AMC是一种基于保留之前和当前的模型版本，生成元输出的通用技术，可以显著降低预测性流失，同时即使与最先进的方法相比，仍然保持或提高总体准确性。"
}