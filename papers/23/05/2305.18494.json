{
    "title": "Adapting Learned Sparse Retrieval for Long Documents. (arXiv:2305.18494v1 [cs.IR])",
    "abstract": "Learned sparse retrieval (LSR) is a family of neural retrieval methods that transform queries and documents into sparse weight vectors aligned with a vocabulary. While LSR approaches like Splade work well for short passages, it is unclear how well they handle longer documents. We investigate existing aggregation approaches for adapting LSR to longer documents and find that proximal scoring is crucial for LSR to handle long documents. To leverage this property, we proposed two adaptations of the Sequential Dependence Model (SDM) to LSR: ExactSDM and SoftSDM. ExactSDM assumes only exact query term dependence, while SoftSDM uses potential functions that model the dependence of query terms and their expansion terms (i.e., terms identified using a transformer's masked language modeling head).  Experiments on the MSMARCO Document and TREC Robust04 datasets demonstrate that both ExactSDM and SoftSDM outperform existing LSR aggregation approaches for different document length constraints. Surp",
    "link": "http://arxiv.org/abs/2305.18494",
    "context": "Title: Adapting Learned Sparse Retrieval for Long Documents. (arXiv:2305.18494v1 [cs.IR])\nAbstract: Learned sparse retrieval (LSR) is a family of neural retrieval methods that transform queries and documents into sparse weight vectors aligned with a vocabulary. While LSR approaches like Splade work well for short passages, it is unclear how well they handle longer documents. We investigate existing aggregation approaches for adapting LSR to longer documents and find that proximal scoring is crucial for LSR to handle long documents. To leverage this property, we proposed two adaptations of the Sequential Dependence Model (SDM) to LSR: ExactSDM and SoftSDM. ExactSDM assumes only exact query term dependence, while SoftSDM uses potential functions that model the dependence of query terms and their expansion terms (i.e., terms identified using a transformer's masked language modeling head).  Experiments on the MSMARCO Document and TREC Robust04 datasets demonstrate that both ExactSDM and SoftSDM outperform existing LSR aggregation approaches for different document length constraints. Surp",
    "path": "papers/23/05/2305.18494.json",
    "total_tokens": 989,
    "translated_title": "学习稀疏检索在长文档中的应用",
    "translated_abstract": "学习稀疏检索 (LSR) 是一种将查询和文档转换成与词汇表对齐的稀疏权重向量的神经检索方法。虽然像 Splade 这样的 LSR 方法在短文段上表现良好，但它们如何处理更长的文档还不清楚。本文研究了用于适应 LSR 到长文档的现有聚合方法，并发现接近打分对于 LSR 处理长文档是至关重要的。为了利用这个特性，我们提出了两种将顺序依赖模型 (SDM) 适应到 LSR 的方法：ExactSDM 和 SoftSDM。ExactSDM 假定只有精确的查询项依赖性，而 SoftSDM 使用潜在函数对查询项及其扩展项 (即使用转换器的掩码语言建模头识别的项) 的依赖关系进行建模。在 MSMARCO 文档和 TREC Robust04 数据集上的实验证明，ExactSDM 和 SoftSDM 都优于现有的 LSR 聚合方法，针对不同的文档长度约束表现出最佳的性能。",
    "tldr": "本文提出了两种方法，即 ExactSDM 和 SoftSDM，将顺序依赖模型 (SDM) 适应到学习稀疏检索 (LSR) 中，以解决长篇文档检索的问题。在 MSMARCO 文档和 TREC Robust04 数据集上的实验证明，ExactSDM 和 SoftSDM 都优于现有的 LSR 聚合方法。",
    "en_tdlr": "This paper proposes two adaptations, ExactSDM and SoftSDM, of the Sequential Dependence Model (SDM) to address the challenge of long document retrieval using learned sparse retrieval (LSR), achieving superior performance on the MSMARCO Document and TREC Robust04 datasets compared to existing LSR aggregation methods."
}