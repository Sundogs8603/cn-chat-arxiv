{
    "title": "Shap-E: Generating Conditional 3D Implicit Functions. (arXiv:2305.02463v1 [cs.CV])",
    "abstract": "We present Shap-E, a conditional generative model for 3D assets. Unlike recent work on 3D generative models which produce a single output representation, Shap-E directly generates the parameters of implicit functions that can be rendered as both textured meshes and neural radiance fields. We train Shap-E in two stages: first, we train an encoder that deterministically maps 3D assets into the parameters of an implicit function; second, we train a conditional diffusion model on outputs of the encoder. When trained on a large dataset of paired 3D and text data, our resulting models are capable of generating complex and diverse 3D assets in a matter of seconds. When compared to Point-E, an explicit generative model over point clouds, Shap-E converges faster and reaches comparable or better sample quality despite modeling a higher-dimensional, multi-representation output space. We release model weights, inference code, and samples at https://github.com/openai/shap-e.",
    "link": "http://arxiv.org/abs/2305.02463",
    "context": "Title: Shap-E: Generating Conditional 3D Implicit Functions. (arXiv:2305.02463v1 [cs.CV])\nAbstract: We present Shap-E, a conditional generative model for 3D assets. Unlike recent work on 3D generative models which produce a single output representation, Shap-E directly generates the parameters of implicit functions that can be rendered as both textured meshes and neural radiance fields. We train Shap-E in two stages: first, we train an encoder that deterministically maps 3D assets into the parameters of an implicit function; second, we train a conditional diffusion model on outputs of the encoder. When trained on a large dataset of paired 3D and text data, our resulting models are capable of generating complex and diverse 3D assets in a matter of seconds. When compared to Point-E, an explicit generative model over point clouds, Shap-E converges faster and reaches comparable or better sample quality despite modeling a higher-dimensional, multi-representation output space. We release model weights, inference code, and samples at https://github.com/openai/shap-e.",
    "path": "papers/23/05/2305.02463.json",
    "total_tokens": 897,
    "translated_title": "Shap-E: 生成有条件的3D隐式函数",
    "translated_abstract": "本文介绍了Shap-E，一种用于生成3D模型的有条件生成模型。与最近的3D生成模型不同的是，Shap-E直接生成可以呈现为纹理网格和神经辐射场的隐式函数的参数，而不是单一的输出表示。我们分两个阶段训练Shap-E：首先，我们训练一个编码器，将3D模型确定性地映射到隐式函数的参数中；其次，我们训练一个条件扩散模型，用于输出编码器的结果。在大规模的3D文本数据匹配数据集上训练时，我们得到的模型能够在几秒钟内生成复杂和多样化的3D模型。与点云的显式生成模型Point-E相比，Shap-E收敛更快，并且在对高维度、多表示输出空间进行建模的情况下，达到了相当或更好的样本质量。我们在https://github.com/openai/shap-e上公开了模型权重、推理代码和样本。",
    "tldr": "Shap-E能够生成有条件的3D隐式函数，可以呈现为纹理网格和神经辐射场，收敛更快且生成的3D模型质量相当或更好。",
    "en_tdlr": "Shap-E generates conditional 3D implicit functions that can be rendered as both textured meshes and neural radiance fields. It converges faster and produces comparable or better sample quality than Point-E, an explicit generative model over point clouds, despite modeling a higher-dimensional, multi-representation output space."
}