{
    "title": "Stability and Generalization of $\\ell_p$-Regularized Stochastic Learning for GCN. (arXiv:2305.12085v1 [cs.LG])",
    "abstract": "Graph convolutional networks (GCN) are viewed as one of the most popular representations among the variants of graph neural networks over graph data and have shown powerful performance in empirical experiments. That $\\ell_2$-based graph smoothing enforces the global smoothness of GCN, while (soft) $\\ell_1$-based sparse graph learning tends to promote signal sparsity to trade for discontinuity. This paper aims to quantify the trade-off of GCN between smoothness and sparsity, with the help of a general $\\ell_p$-regularized $(1<p\\leq 2)$ stochastic learning proposed within. While stability-based generalization analyses have been given in prior work for a second derivative objectiveness function, our $\\ell_p$-regularized learning scheme does not satisfy such a smooth condition. To tackle this issue, we propose a novel SGD proximal algorithm for GCNs with an inexact operator. For a single-layer GCN, we establish an explicit theoretical understanding of GCN with the $\\ell_p$-regularized stoc",
    "link": "http://arxiv.org/abs/2305.12085",
    "context": "Title: Stability and Generalization of $\\ell_p$-Regularized Stochastic Learning for GCN. (arXiv:2305.12085v1 [cs.LG])\nAbstract: Graph convolutional networks (GCN) are viewed as one of the most popular representations among the variants of graph neural networks over graph data and have shown powerful performance in empirical experiments. That $\\ell_2$-based graph smoothing enforces the global smoothness of GCN, while (soft) $\\ell_1$-based sparse graph learning tends to promote signal sparsity to trade for discontinuity. This paper aims to quantify the trade-off of GCN between smoothness and sparsity, with the help of a general $\\ell_p$-regularized $(1<p\\leq 2)$ stochastic learning proposed within. While stability-based generalization analyses have been given in prior work for a second derivative objectiveness function, our $\\ell_p$-regularized learning scheme does not satisfy such a smooth condition. To tackle this issue, we propose a novel SGD proximal algorithm for GCNs with an inexact operator. For a single-layer GCN, we establish an explicit theoretical understanding of GCN with the $\\ell_p$-regularized stoc",
    "path": "papers/23/05/2305.12085.json",
    "total_tokens": 1023,
    "tldr": "本文提出了一种通用的$\\ell_p$-正则化随机学习方法，旨在评估GCN在平滑性和稀疏性之间的权衡。为了解决不满足平滑条件的问题，提出了一种SGD近端算法，并建立了一个关于$\\ell_p$-正则化随机学习的GCN显式理论理解。"
}