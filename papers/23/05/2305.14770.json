{
    "title": "Using Natural Language Explanations to Rescale Human Judgments. (arXiv:2305.14770v1 [cs.CL])",
    "abstract": "The rise of large language models (LLMs) has brought a critical need for high-quality human-labeled data, particularly for processes like human feedback and evaluation. A common practice is to label data via consensus annotation over the judgments of multiple crowdworkers. However, different annotators may have different interpretations of labeling schemes unless given extensive training, and for subjective NLP tasks, even trained expert annotators can diverge heavily. We show that these nuances can be captured by high quality natural language explanations, and propose a method to rescale ordinal annotation in the presence of disagreement using LLMs. Specifically, we feed Likert ratings and corresponding natural language explanations into an LLM and prompt it to produce a numeric score. This score should reflect the underlying assessment of the example by the annotator. The presence of explanations allows the LLM to homogenize ratings across annotators in spite of scale usage differenc",
    "link": "http://arxiv.org/abs/2305.14770",
    "context": "Title: Using Natural Language Explanations to Rescale Human Judgments. (arXiv:2305.14770v1 [cs.CL])\nAbstract: The rise of large language models (LLMs) has brought a critical need for high-quality human-labeled data, particularly for processes like human feedback and evaluation. A common practice is to label data via consensus annotation over the judgments of multiple crowdworkers. However, different annotators may have different interpretations of labeling schemes unless given extensive training, and for subjective NLP tasks, even trained expert annotators can diverge heavily. We show that these nuances can be captured by high quality natural language explanations, and propose a method to rescale ordinal annotation in the presence of disagreement using LLMs. Specifically, we feed Likert ratings and corresponding natural language explanations into an LLM and prompt it to produce a numeric score. This score should reflect the underlying assessment of the example by the annotator. The presence of explanations allows the LLM to homogenize ratings across annotators in spite of scale usage differenc",
    "path": "papers/23/05/2305.14770.json",
    "total_tokens": 855,
    "translated_title": "利用自然语言解释重新调整人类评价",
    "translated_abstract": "大型语言模型（LLM）的出现带来了需要高质量人标记数据的紧迫需求，特别是对于人的反馈和评估等过程。一种常见的做法是通过多个众包工作者的共识来标注数据。然而，不同的标注者可能对标注方案有不同的解释，除非接受了广泛的培训，否则对于主观的NLP任务，甚至受过训练的专家标注者也可能会出现巨大的分歧。我们展示了这些细微差别可以通过高质量的自然语言解释进行捕捉，提出了一种使用LLM在存在分歧时重新调整大小排序注释的方法。具体而言，我们将Likert评分和相应的自然语言解释输入LLM，并提示它产生一个数字得分。这个得分应该反映注释者对示例的基本评估。解释的存在使LLM能够在尺度使用差异存在的情况下使评级在标注者之间同质化。",
    "tldr": "本文提出使用自然语言解释来调整标注者之间存在的尺度不一致，解决了主观NLP任务中标注者之间分歧的问题。",
    "en_tdlr": "This paper proposes to use natural language explanations to adjust the inconsistency in rating scales among annotators, addressing the issue of disagreement among annotators in subjective NLP tasks."
}