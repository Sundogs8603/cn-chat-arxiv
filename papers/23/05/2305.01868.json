{
    "title": "Pre-train and Search: Efficient Embedding Table Sharding with Pre-trained Neural Cost Models. (arXiv:2305.01868v1 [cs.LG])",
    "abstract": "Sharding a large machine learning model across multiple devices to balance the costs is important in distributed training. This is challenging because partitioning is NP-hard, and estimating the costs accurately and efficiently is difficult. In this work, we explore a \"pre-train, and search\" paradigm for efficient sharding. The idea is to pre-train a universal and once-for-all neural network to predict the costs of all the possible shards, which serves as an efficient sharding simulator. Built upon this pre-trained cost model, we then perform an online search to identify the best sharding plans given any specific sharding task. We instantiate this idea in deep learning recommendation models (DLRMs) and propose NeuroShard for embedding table sharding. NeuroShard pre-trains neural cost models on augmented tables to cover various sharding scenarios. Then it identifies the best column-wise and table-wise sharding plans with beam search and greedy grid search, respectively. Experiments show",
    "link": "http://arxiv.org/abs/2305.01868",
    "context": "Title: Pre-train and Search: Efficient Embedding Table Sharding with Pre-trained Neural Cost Models. (arXiv:2305.01868v1 [cs.LG])\nAbstract: Sharding a large machine learning model across multiple devices to balance the costs is important in distributed training. This is challenging because partitioning is NP-hard, and estimating the costs accurately and efficiently is difficult. In this work, we explore a \"pre-train, and search\" paradigm for efficient sharding. The idea is to pre-train a universal and once-for-all neural network to predict the costs of all the possible shards, which serves as an efficient sharding simulator. Built upon this pre-trained cost model, we then perform an online search to identify the best sharding plans given any specific sharding task. We instantiate this idea in deep learning recommendation models (DLRMs) and propose NeuroShard for embedding table sharding. NeuroShard pre-trains neural cost models on augmented tables to cover various sharding scenarios. Then it identifies the best column-wise and table-wise sharding plans with beam search and greedy grid search, respectively. Experiments show",
    "path": "papers/23/05/2305.01868.json",
    "total_tokens": 946,
    "translated_title": "预训练和搜索：基于预训练神经成本模型的高效嵌入表分片方法",
    "translated_abstract": "在分布式训练中，将大型机器学习模型分片到多个设备上以平衡成本非常重要。由于分区是NP难问题且准确和高效地估算成本很困难，因此这是具有挑战性的。本文探索了一种“预训练和搜索”范式，用于实现高效的分片。该方法是预先训练一个通用的、永久存在的神经网络，来预测所有可能的分片的成本，这个网络就是一个高效的分片模拟器。在此预训练成本模型的基础上，我们进行在线搜索，以确定给定任何特定分片任务的最佳分片计划。在深度学习推荐模型（DLRMs）中，我们将此思想实例化，并提议了NeuroShard用于嵌入表分片。NeuroShard在扩展表上预先训练神经成本模型，以涵盖各种分片场景。然后，使用波束搜索和贪心网格搜索，分别确定最佳的列和表分片计划。实验结果表明",
    "tldr": "本文探索了一种基于预训练成本模型的高效分片方法，通过神经网络预测成本，并使用在线搜索确定最佳分片计划，实验结果表明其在嵌入表分片任务中表现很好。",
    "en_tdlr": "This paper proposes an efficient sharding method based on pre-trained cost models, which predicts costs using neural networks and identifies the best sharding plan with online search. Experiments on embedding table sharding show its excellent performance."
}