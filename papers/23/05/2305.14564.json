{
    "title": "PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents. (arXiv:2305.14564v1 [cs.CL])",
    "abstract": "Strategies such as chain-of-thought prompting improve the performance of large language models (LLMs) on complex reasoning tasks by decomposing input examples into intermediate steps. However, it remains unclear how to apply such methods to reason over long input documents, in which both the decomposition and the output of each intermediate step are non-trivial to obtain. In this work, we propose PEARL, a prompting framework to improve reasoning over long documents, which consists of three stages: action mining, plan formulation, and plan execution. More specifically, given a question about a long document, PEARL decomposes the question into a sequence of actions (e.g., SUMMARIZE, FIND_EVENT, FIND_RELATION) and then executes them over the document to obtain the answer. Each stage of PEARL is implemented via zero-shot or few-shot prompting of LLMs (in our work, GPT-4) with minimal human input. We evaluate PEARL on a challenging subset of the QuALITY dataset, which contains questions tha",
    "link": "http://arxiv.org/abs/2305.14564",
    "context": "Title: PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents. (arXiv:2305.14564v1 [cs.CL])\nAbstract: Strategies such as chain-of-thought prompting improve the performance of large language models (LLMs) on complex reasoning tasks by decomposing input examples into intermediate steps. However, it remains unclear how to apply such methods to reason over long input documents, in which both the decomposition and the output of each intermediate step are non-trivial to obtain. In this work, we propose PEARL, a prompting framework to improve reasoning over long documents, which consists of three stages: action mining, plan formulation, and plan execution. More specifically, given a question about a long document, PEARL decomposes the question into a sequence of actions (e.g., SUMMARIZE, FIND_EVENT, FIND_RELATION) and then executes them over the document to obtain the answer. Each stage of PEARL is implemented via zero-shot or few-shot prompting of LLMs (in our work, GPT-4) with minimal human input. We evaluate PEARL on a challenging subset of the QuALITY dataset, which contains questions tha",
    "path": "papers/23/05/2305.14564.json",
    "total_tokens": 839,
    "translated_title": "PEARL：通过对长文档的规划和执行行动来指导大型语言模型进行推理",
    "translated_abstract": "在本文中，我们提出了一个叫做PEARL的提示框架，以改善长文档推理，它由三个阶段组成：行动挖掘、计划制定和计划执行。具体来说，给定一个有关长文档的问题，PEARL将问题分解成一系列动作（例如，SUMMARIZE、FIND_EVENT、FIND_RELATION），然后在文档上执行它们以获得答案。PEARL的每个阶段都是通过最少人工输入的零射或少射提示LLM（在我们的工作中是GPT-4）来实现的。",
    "tldr": "PEARL是一种提示框架，用于改善大型语言模型对长文档的推理。 它包括三个阶段：行动挖掘，计划制定和计划执行。 PEARL将问题分解成一系列动作并在文档上执行以获得答案。",
    "en_tdlr": "PEARL is a prompting framework for improving large language models' reasoning over long documents, consisting of three stages: action mining, plan formulation, and plan execution. It decomposes a question into a sequence of actions and executes them on the document to obtain an answer. PEARL is implemented via zero-shot or few-shot prompting of LLMs."
}