{
    "title": "Latent Interactive A2C for Improved RL in Open Many-Agent Systems. (arXiv:2305.05159v1 [cs.LG])",
    "abstract": "There is a prevalence of multiagent reinforcement learning (MARL) methods that engage in centralized training. But, these methods involve obtaining various types of information from the other agents, which may not be feasible in competitive or adversarial settings. A recent method, the interactive advantage actor critic (IA2C), engages in decentralized training coupled with decentralized execution, aiming to predict the other agents' actions from possibly noisy observations. In this paper, we present the latent IA2C that utilizes an encoder-decoder architecture to learn a latent representation of the hidden state and other agents' actions. Our experiments in two domains -each populated by many agents -- reveal that the latent IA2C significantly improves sample efficiency by reducing variance and converging faster. Additionally, we introduce open versions of these domains where the agent population may change over time, and evaluate on these instances as well.",
    "link": "http://arxiv.org/abs/2305.05159",
    "context": "Title: Latent Interactive A2C for Improved RL in Open Many-Agent Systems. (arXiv:2305.05159v1 [cs.LG])\nAbstract: There is a prevalence of multiagent reinforcement learning (MARL) methods that engage in centralized training. But, these methods involve obtaining various types of information from the other agents, which may not be feasible in competitive or adversarial settings. A recent method, the interactive advantage actor critic (IA2C), engages in decentralized training coupled with decentralized execution, aiming to predict the other agents' actions from possibly noisy observations. In this paper, we present the latent IA2C that utilizes an encoder-decoder architecture to learn a latent representation of the hidden state and other agents' actions. Our experiments in two domains -each populated by many agents -- reveal that the latent IA2C significantly improves sample efficiency by reducing variance and converging faster. Additionally, we introduce open versions of these domains where the agent population may change over time, and evaluate on these instances as well.",
    "path": "papers/23/05/2305.05159.json",
    "total_tokens": 948,
    "translated_title": "潜在交互式A2C：在开放式多智能体系统中实现强化学习的改进",
    "translated_abstract": "目前广泛应用于多智能体强化学习(MARL)的方法是集中式训练，但该方法需要从其他智能体中获得各种信息，这在竞争或对抗环境中可能不可行。最近，交互式优势演员-评论家(IA2C)方法采用了分散的训练和执行，旨在从可能存在噪声的观察中预测其他智能体的行动。本文提出了一种利用编码-解码架构学习隐藏状态和其他智能体行动的潜在IA2C，我们在两个由众多智能体组成的领域中的实验结果表明，潜在IA2C通过降低方差和更快收敛显著提高了样本效率。此外，我们还介绍了这些领域的开放版本，智能体种群可能随时间变化，并对这些实例进行了评估。",
    "tldr": "本论文介绍了一种基于编码-解码架构的潜在交互式A2C方法，以在多智能体系统中实现强化学习的改进。该方法显著提高了样本效率，通过学习隐藏状态和其他智能体的行为，解决了在竞争或对抗环境中从其他智能体中获得各种信息可能不可行的问题。",
    "en_tdlr": "This paper presents a latent interactive A2C method based on an encoder-decoder architecture, which aims to improve reinforcement learning in multi-agent systems. The method significantly improves sample efficiency and solves the problem of obtaining various types of information from other agents in competitive or adversarial environments by learning latent representation of hidden states and actions of other agents."
}