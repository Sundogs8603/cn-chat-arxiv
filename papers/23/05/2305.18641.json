{
    "title": "Enhanced Chart Understanding in Vision and Language Task via Cross-modal Pre-training on Plot Table Pairs. (arXiv:2305.18641v1 [cs.CL])",
    "abstract": "Building cross-model intelligence that can understand charts and communicate the salient information hidden behind them is an appealing challenge in the vision and language(V+L) community. The capability to uncover the underlined table data of chart figures is a critical key to automatic chart understanding. We introduce ChartT5, a V+L model that learns how to interpret table information from chart images via cross-modal pre-training on plot table pairs. Specifically, we propose two novel pre-training objectives: Masked Header Prediction (MHP) and Masked Value Prediction (MVP) to facilitate the model with different skills to interpret the table information. We have conducted extensive experiments on chart question answering and chart summarization to verify the effectiveness of the proposed pre-training strategies. In particular, on the ChartQA benchmark, our ChartT5 outperforms the state-of-the-art non-pretraining methods by over 8% performance gains.",
    "link": "http://arxiv.org/abs/2305.18641",
    "context": "Title: Enhanced Chart Understanding in Vision and Language Task via Cross-modal Pre-training on Plot Table Pairs. (arXiv:2305.18641v1 [cs.CL])\nAbstract: Building cross-model intelligence that can understand charts and communicate the salient information hidden behind them is an appealing challenge in the vision and language(V+L) community. The capability to uncover the underlined table data of chart figures is a critical key to automatic chart understanding. We introduce ChartT5, a V+L model that learns how to interpret table information from chart images via cross-modal pre-training on plot table pairs. Specifically, we propose two novel pre-training objectives: Masked Header Prediction (MHP) and Masked Value Prediction (MVP) to facilitate the model with different skills to interpret the table information. We have conducted extensive experiments on chart question answering and chart summarization to verify the effectiveness of the proposed pre-training strategies. In particular, on the ChartQA benchmark, our ChartT5 outperforms the state-of-the-art non-pretraining methods by over 8% performance gains.",
    "path": "papers/23/05/2305.18641.json",
    "total_tokens": 887,
    "translated_title": "通过绘图表对的跨模态预训练来增强视觉语言任务中的图表理解",
    "translated_abstract": "在视觉语言领域，建立可以理解图表并传达其中隐藏的信息的跨模态智能是一个有吸引力的挑战。揭示图表数据的关键是自动图表理解的关键。我们介绍了ChartT5，这是一个V + L模型，通过对绘图表对进行跨模态预训练来学习如何解释来自图表图像的表格信息。具体而言，我们提出了两种新颖的预训练目标：遮罩标题预测（MHP）和遮罩值预测（MVP），以使模型具备不同的技能来解释表格信息。我们对图表问答和图表摘要进行了广泛的实验，以验证所提出的预训练策略的有效性。特别是在ChartQA基准测试中，我们的ChartT5的性能比最先进的非预训练方法提高了8%以上。",
    "tldr": "该论文介绍了一个名为ChartT5的V+L模型，它通过对绘图表对进行跨模态预训练，学习如何解释来自图表图像的表格信息，并在图表问答任务中表现出8%以上的性能提升。",
    "en_tdlr": "This paper presents a V+L model called ChartT5, which learns to interpret table information from chart images via cross-modal pre-training on plot table pairs. It outperforms the state-of-the-art non-pretraining methods by over 8% performance gains on the ChartQA benchmark."
}