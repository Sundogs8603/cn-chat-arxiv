{
    "title": "Reinterpreting causal discovery as the task of predicting unobserved joint statistics. (arXiv:2305.06894v1 [stat.ML])",
    "abstract": "If $X,Y,Z$ denote sets of random variables, two different data sources may contain samples from $P_{X,Y}$ and $P_{Y,Z}$, respectively. We argue that causal discovery can help inferring properties of the `unobserved joint distributions' $P_{X,Y,Z}$ or $P_{X,Z}$. The properties may be conditional independences (as in `integrative causal inference') or also quantitative statements about dependences.  More generally, we define a learning scenario where the input is a subset of variables and the label is some statistical property of that subset. Sets of jointly observed variables define the training points, while unobserved sets are possible test points. To solve this learning task, we infer, as an intermediate step, a causal model from the observations that then entails properties of unobserved sets. Accordingly, we can define the VC dimension of a class of causal models and derive generalization bounds for the predictions.  Here, causal discovery becomes more modest and better accessible ",
    "link": "http://arxiv.org/abs/2305.06894",
    "context": "Title: Reinterpreting causal discovery as the task of predicting unobserved joint statistics. (arXiv:2305.06894v1 [stat.ML])\nAbstract: If $X,Y,Z$ denote sets of random variables, two different data sources may contain samples from $P_{X,Y}$ and $P_{Y,Z}$, respectively. We argue that causal discovery can help inferring properties of the `unobserved joint distributions' $P_{X,Y,Z}$ or $P_{X,Z}$. The properties may be conditional independences (as in `integrative causal inference') or also quantitative statements about dependences.  More generally, we define a learning scenario where the input is a subset of variables and the label is some statistical property of that subset. Sets of jointly observed variables define the training points, while unobserved sets are possible test points. To solve this learning task, we infer, as an intermediate step, a causal model from the observations that then entails properties of unobserved sets. Accordingly, we can define the VC dimension of a class of causal models and derive generalization bounds for the predictions.  Here, causal discovery becomes more modest and better accessible ",
    "path": "papers/23/05/2305.06894.json",
    "total_tokens": 883,
    "translated_title": "将因果发现重新解释为预测未观察到的联合统计量的任务",
    "translated_abstract": "如果$X,Y,Z$表示随机变量集，不同的数据源可以包含$P_{X,Y}$和$P_{Y,Z}$的样本。我们认为因果发现可以帮助推断“未观察到的联合分布”$P_{X,Y,Z}$或$P_{X,Z}$的性质。这些性质可以是条件独立性（如“整合因果推理”中那样），也可以是关于依赖性的定量说明。更一般地，我们定义了一个学习场景，其中输入是变量的子集，标签是该子集的某些统计属性。共同观测变量集定义了训练点，而未观察到的集合是可能的测试点。为了解决这个学习任务，我们从观察结果中推断出一个因果模型，这个因果模型可以得到未观察到集合的属性。因此，我们可以定义一个因果模型类的VC维，并为预测推导出泛化界限。因此，因果发现变得更加谦逊和易于访问。",
    "tldr": "研究者提出将因果发现视为预测未观察到联合统计量的任务，这样可以更好地推断未观察到集合的属性。",
    "en_tdlr": "The authors propose reinterpreting causal discovery as the task of predicting unobserved joint statistics, which can better infer properties of unobserved sets."
}