{
    "title": "Make Prompt-based Black-Box Tuning Colorful: Boosting Model Generalization from Three Orthogonal Perspectives. (arXiv:2305.08088v1 [cs.CL])",
    "abstract": "Large language models (LLMs) have shown increasing power on various natural language processing (NLP) tasks. However, tuning these models for downstream tasks usually needs exorbitant costs or is unavailable due to commercial considerations. Recently, black-box tuning has been proposed to address this problem by optimizing task-specific prompts without accessing the gradients and hidden representations. However, most existing works have yet fully exploited the potential of gradient-free optimization under the scenario of few-shot learning. In this paper, we describe BBT-RGB, a suite of straightforward and complementary techniques for enhancing the efficiency and performance of black-box optimization. Specifically, our method includes three plug-and-play components: (1) Two-stage derivative-free optimization strategy that facilitates fast convergence and mitigates overfitting; (2) Automatic verbalizer construction with its novel usage under few-shot settings; (3) Better prompt initializ",
    "link": "http://arxiv.org/abs/2305.08088",
    "context": "Title: Make Prompt-based Black-Box Tuning Colorful: Boosting Model Generalization from Three Orthogonal Perspectives. (arXiv:2305.08088v1 [cs.CL])\nAbstract: Large language models (LLMs) have shown increasing power on various natural language processing (NLP) tasks. However, tuning these models for downstream tasks usually needs exorbitant costs or is unavailable due to commercial considerations. Recently, black-box tuning has been proposed to address this problem by optimizing task-specific prompts without accessing the gradients and hidden representations. However, most existing works have yet fully exploited the potential of gradient-free optimization under the scenario of few-shot learning. In this paper, we describe BBT-RGB, a suite of straightforward and complementary techniques for enhancing the efficiency and performance of black-box optimization. Specifically, our method includes three plug-and-play components: (1) Two-stage derivative-free optimization strategy that facilitates fast convergence and mitigates overfitting; (2) Automatic verbalizer construction with its novel usage under few-shot settings; (3) Better prompt initializ",
    "path": "papers/23/05/2305.08088.json",
    "total_tokens": 964,
    "translated_title": "使基于提示的黑盒调优更加丰富多彩：从三个正交角度提高模型泛化能力",
    "translated_abstract": "大型语言模型在各种自然语言处理任务中已经展现出越来越强大的能力。然而，调整这些模型以用于下游任务通常需要巨额的代价或由于商业考虑而不可用。最近，提出了黑盒调优来解决这个问题，通过优化任务特定的提示而不访问梯度和隐藏表示。然而，大多数现有的作品还没有充分利用少样本学习场景下无梯度优化的潜力。在本文中，我们描述了BBT-RGB，这是一个用于增强黑盒优化效率和性能的直接且互补技术套件。具体来说，我们的方法包括三个即插即用的组件：（1）两阶段无导数优化策略，有助于快速收敛并缓解过拟合；（2）自动语言转化器构建及其在少样本设置中的新用法；（3）更好的提示初始化，基于未标记数据的语言学动机句法模式。",
    "tldr": "本文提出了BBT-RGB，一套用于增强黑盒优化效率和性能的直接且互补技术套件，包括两阶段无导数优化策略、自动语言转化器构建及其在少样本设置中的新用法以及更好的提示初始化。",
    "en_tdlr": "This paper proposes BBT-RGB, a suite of straightforward and complementary techniques for enhancing the efficiency and performance of black-box optimization, including a two-stage derivative-free optimization strategy, automatic verbalizer construction and its novel usage under few-shot settings, and better prompt initialization based on linguistically motivated syntax patterns from the unlabeled data."
}