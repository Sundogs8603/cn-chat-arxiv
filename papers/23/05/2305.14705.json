{
    "title": "Flan-MoE: Scaling Instruction-Finetuned Language Models with Sparse Mixture of Experts. (arXiv:2305.14705v1 [cs.CL])",
    "abstract": "The explosive growth of language models and their applications have led to an increased demand for efficient and scalable methods. In this paper, we introduce Flan-MoE, a set of Instruction-Finetuned Sparse Mixture-of-Expert (MoE) models. We show that naively finetuning MoE models on a task-specific dataset (in other words, no instruction-finetuning) often yield worse performance compared to dense models of the same computational complexity. However, our Flan-MoE outperforms dense models under multiple experiment settings: instruction-finetuning only and instruction-finetuning followed by task-specific finetuning. This shows that instruction-finetuning is an essential stage for MoE models. Specifically, our largest model, Flan-MoE-32B, surpasses the performance of Flan-PaLM-62B on four benchmarks, while utilizing only one-third of the FLOPs. The success of Flan-MoE encourages rethinking the design of large-scale, high-performance language models, under the setting of task-agnostic lear",
    "link": "http://arxiv.org/abs/2305.14705",
    "context": "Title: Flan-MoE: Scaling Instruction-Finetuned Language Models with Sparse Mixture of Experts. (arXiv:2305.14705v1 [cs.CL])\nAbstract: The explosive growth of language models and their applications have led to an increased demand for efficient and scalable methods. In this paper, we introduce Flan-MoE, a set of Instruction-Finetuned Sparse Mixture-of-Expert (MoE) models. We show that naively finetuning MoE models on a task-specific dataset (in other words, no instruction-finetuning) often yield worse performance compared to dense models of the same computational complexity. However, our Flan-MoE outperforms dense models under multiple experiment settings: instruction-finetuning only and instruction-finetuning followed by task-specific finetuning. This shows that instruction-finetuning is an essential stage for MoE models. Specifically, our largest model, Flan-MoE-32B, surpasses the performance of Flan-PaLM-62B on four benchmarks, while utilizing only one-third of the FLOPs. The success of Flan-MoE encourages rethinking the design of large-scale, high-performance language models, under the setting of task-agnostic lear",
    "path": "papers/23/05/2305.14705.json",
    "total_tokens": 1025,
    "translated_title": "Flan-MoE: 通过稀疏Mixture of Experts扩展指令调优的语言模型",
    "translated_abstract": "语言模型的爆炸性增长和应用需求导致有效和可扩展方法的需求增加。本文介绍了一套Instruction-Finetuned Sparse Mixture-of-Expert (MoE)语言模型，即Flan-MoE。我们发现，仅针对任务特定数据集进行MoE模型的微调会导致性能不如相同计算复杂度的密集模型。然而，我们的Flan-MoE在多个实验设置下都优于密集模型：仅指令微调和指令微调后进行任务特定微调。这表明，指令微调是MoE模型的必要阶段。具体来说，我们的最大模型Flan-MoE-32B在四个基准测试中超越了Flan-PaLM-62B的性能，同时只利用了1/3的FLOPs。Flan-MoE的成功鼓舞我们重新思考大规模、高性能语言模型的设计，尤其是在任务无关学习的情况下。",
    "tldr": "Flan-MoE是一种指令调优的稀疏Mixture of Experts（MoE）语言模型，相对于密集模型，在指令微调和任务特定微调后均表现更好。最大模型Flan-MoE-32B的性能在四个基准测试中超越Flan-PaLM-62B，同时只利用了1/3的FLOPs。",
    "en_tdlr": "Flan-MoE is a sparse Mixture of Experts (MoE) language model that is instruction-finetuned. It outperforms dense models under both instruction-finetuning and task-specific finetuning settings. The largest model, Flan-MoE-32B, surpasses the performance of Flan-PaLM-62B on four benchmarks with only one-third of the FLOPs. The success of Flan-MoE encourages rethinking the design of large-scale, high-performance language models, especially in the setting of task-agnostic learning."
}