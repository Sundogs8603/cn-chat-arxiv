{
    "title": "Semi-Infinitely Constrained Markov Decision Processes and Efficient Reinforcement Learning. (arXiv:2305.00254v1 [cs.LG])",
    "abstract": "We propose a novel generalization of constrained Markov decision processes (CMDPs) that we call the \\emph{semi-infinitely constrained Markov decision process} (SICMDP). Particularly, we consider a continuum of constraints instead of a finite number of constraints as in the case of ordinary CMDPs. We also devise two reinforcement learning algorithms for SICMDPs that we call SI-CRL and SI-CPO. SI-CRL is a model-based reinforcement learning algorithm. Given an estimate of the transition model, we first transform the reinforcement learning problem into a linear semi-infinitely programming (LSIP) problem and then use the dual exchange method in the LSIP literature to solve it. SI-CPO is a policy optimization algorithm. Borrowing the ideas from the cooperative stochastic approximation approach, we make alternative updates to the policy parameters to maximize the reward or minimize the cost. To the best of our knowledge, we are the first to apply tools from semi-infinitely programming (SIP) t",
    "link": "http://arxiv.org/abs/2305.00254",
    "context": "Title: Semi-Infinitely Constrained Markov Decision Processes and Efficient Reinforcement Learning. (arXiv:2305.00254v1 [cs.LG])\nAbstract: We propose a novel generalization of constrained Markov decision processes (CMDPs) that we call the \\emph{semi-infinitely constrained Markov decision process} (SICMDP). Particularly, we consider a continuum of constraints instead of a finite number of constraints as in the case of ordinary CMDPs. We also devise two reinforcement learning algorithms for SICMDPs that we call SI-CRL and SI-CPO. SI-CRL is a model-based reinforcement learning algorithm. Given an estimate of the transition model, we first transform the reinforcement learning problem into a linear semi-infinitely programming (LSIP) problem and then use the dual exchange method in the LSIP literature to solve it. SI-CPO is a policy optimization algorithm. Borrowing the ideas from the cooperative stochastic approximation approach, we make alternative updates to the policy parameters to maximize the reward or minimize the cost. To the best of our knowledge, we are the first to apply tools from semi-infinitely programming (SIP) t",
    "path": "papers/23/05/2305.00254.json",
    "total_tokens": 997,
    "translated_abstract": "我们提出了一种新的约束马尔科夫决策过程(CMDP)的概括，称为半无限约束马尔科夫决策过程(SICMDP)。特别地，我们考虑了一个连续的约束条件，而不是普通CMDP中的有限约束条件。我们还设计了两个针对SICMDP的强化学习算法，称为SI-CRL和SI-CPO。SI-CRL是一个基于模型的强化学习算法。给定转移模型的估计，我们首先将强化学习问题转化为线性半无限编程(LSIP)问题，然后使用LSIP文献中的对偶交换方法来解决它。SI-CPO是一个策略优化算法。借鉴合作随机逼近方法的思想，我们对策略参数进行交替更新以最大化奖励或最小化代价。据我们所知，我们是第一个将半无限编程(SIP)工具应用于CMDP强化学习的团队。",
    "tldr": "该论文提出了一种新的概括约束马尔科夫决策过程(CMDP)的半无限约束马尔科夫决策过程(SICMDP)，并利用SI-CRL和SI-CPO两种算法进行模型学习和策略优化，是首个将半无限编程(SIP)工具应用于CMDP强化学习的研究。",
    "en_tdlr": "This paper proposes a new generalization of the constrained Markov decision process (CMDP), named the semi-infinitely constrained Markov decision process (SICMDP), and devises two algorithms, SI-CRL and SI-CPO, for model learning and policy optimization. It is the first study that applies tools from semi-infinitely programming (SIP) to CMDP reinforcement learning."
}