{
    "title": "Parameter-Efficient Language Model Tuning with Active Learning in Low-Resource Settings. (arXiv:2305.14576v1 [cs.CL])",
    "abstract": "Pre-trained language models (PLMs) have ignited a surge in demand for effective fine-tuning techniques, particularly in low-resource domains and languages. Active learning (AL), a set of algorithms designed to decrease labeling costs by minimizing label complexity, has shown promise in confronting the labeling bottleneck. Concurrently, adapter modules, designed for parameter-efficient fine-tuning (PEFT), have showcased notable potential in low-resource settings. However, the interplay between AL and adapter-based PEFT remains unexplored. In our study, we empirically investigate PEFT behavior with AL in low-resource settings for text classification tasks. Our findings affirm the superiority of PEFT over full-fine tuning (FFT) in low-resource settings and demonstrate that this advantage persists in AL setups. Finally, we delve into the properties of PEFT and FFT through the lens of forgetting dynamics and instance-level representations, linking them to AL instance selection behavior and ",
    "link": "http://arxiv.org/abs/2305.14576",
    "context": "Title: Parameter-Efficient Language Model Tuning with Active Learning in Low-Resource Settings. (arXiv:2305.14576v1 [cs.CL])\nAbstract: Pre-trained language models (PLMs) have ignited a surge in demand for effective fine-tuning techniques, particularly in low-resource domains and languages. Active learning (AL), a set of algorithms designed to decrease labeling costs by minimizing label complexity, has shown promise in confronting the labeling bottleneck. Concurrently, adapter modules, designed for parameter-efficient fine-tuning (PEFT), have showcased notable potential in low-resource settings. However, the interplay between AL and adapter-based PEFT remains unexplored. In our study, we empirically investigate PEFT behavior with AL in low-resource settings for text classification tasks. Our findings affirm the superiority of PEFT over full-fine tuning (FFT) in low-resource settings and demonstrate that this advantage persists in AL setups. Finally, we delve into the properties of PEFT and FFT through the lens of forgetting dynamics and instance-level representations, linking them to AL instance selection behavior and ",
    "path": "papers/23/05/2305.14576.json",
    "total_tokens": 1026,
    "tldr": "本篇论文研究了在低资源条件下，利用主动学习和适配器模块进行参数高效的语言模型微调的效果。实验结果表明，适配器模块的效果在低资源条件下比全调参要好，而主动学习对适配器模块的效果并没有影响。文中还探讨了适配器模块和全调参在遗忘动态和实例级表示方面不同的性质，并将其与主动学习实例选择行为和结果联系起来。"
}