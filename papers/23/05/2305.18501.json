{
    "title": "DoMo-AC: Doubly Multi-step Off-policy Actor-Critic Algorithm. (arXiv:2305.18501v1 [cs.LG])",
    "abstract": "Multi-step learning applies lookahead over multiple time steps and has proved valuable in policy evaluation settings. However, in the optimal control case, the impact of multi-step learning has been relatively limited despite a number of prior efforts. Fundamentally, this might be because multi-step policy improvements require operations that cannot be approximated by stochastic samples, hence hindering the widespread adoption of such methods in practice. To address such limitations, we introduce doubly multi-step off-policy VI (DoMo-VI), a novel oracle algorithm that combines multi-step policy improvements and policy evaluations. DoMo-VI enjoys guaranteed convergence speed-up to the optimal policy and is applicable in general off-policy learning settings. We then propose doubly multi-step off-policy actor-critic (DoMo-AC), a practical instantiation of the DoMo-VI algorithm. DoMo-AC introduces a bias-variance trade-off that ensures improved policy gradient estimates. When combined with",
    "link": "http://arxiv.org/abs/2305.18501",
    "context": "Title: DoMo-AC: Doubly Multi-step Off-policy Actor-Critic Algorithm. (arXiv:2305.18501v1 [cs.LG])\nAbstract: Multi-step learning applies lookahead over multiple time steps and has proved valuable in policy evaluation settings. However, in the optimal control case, the impact of multi-step learning has been relatively limited despite a number of prior efforts. Fundamentally, this might be because multi-step policy improvements require operations that cannot be approximated by stochastic samples, hence hindering the widespread adoption of such methods in practice. To address such limitations, we introduce doubly multi-step off-policy VI (DoMo-VI), a novel oracle algorithm that combines multi-step policy improvements and policy evaluations. DoMo-VI enjoys guaranteed convergence speed-up to the optimal policy and is applicable in general off-policy learning settings. We then propose doubly multi-step off-policy actor-critic (DoMo-AC), a practical instantiation of the DoMo-VI algorithm. DoMo-AC introduces a bias-variance trade-off that ensures improved policy gradient estimates. When combined with",
    "path": "papers/23/05/2305.18501.json",
    "total_tokens": 906,
    "translated_title": "DoMo-AC: 双重多步骤离策略Actor-Critic算法",
    "translated_abstract": "多步骤学习在策略评估中具有前瞻性，但在最优控制情况下，多步骤学习的影响相对有限，原因是多步骤策略改进需要的操作无法用随机样本来近似，阻碍了这种方法在实践中的广泛应用。为了解决这些限制，我们引入了双重多步骤离策略VI(DoMo-VI)，这是一种新颖的Oracle算法，它结合了多步骤策略改进和策略评估。DoMo-VI保证收敛速度加速到最优策略，并适用于一般的离策略学习设置。然后我们提出了双重多步离策略actor-critic(DoMo-AC)，这是一个实际的DoMo-VI算法实例。DoMo-AC引入了偏差-方差权衡，以确保改进的策略梯度估计。当与...",
    "tldr": "双重多步骤离策略Actor-Critic算法能够通过结合多步骤策略改进和策略评估来促进最优控制，提高策略梯度估计，并在一般的离策略学习设置中有效。",
    "en_tdlr": "The DoMo-AC algorithm is a doubly multi-step off-policy actor-critic approach that combines multi-step policy improvements and evaluations to promote optimal control, improve policy gradient estimates, and is effective in general off-policy learning settings."
}