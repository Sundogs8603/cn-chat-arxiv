{
    "title": "Less Can Be More: Unsupervised Graph Pruning for Large-scale Dynamic Graphs. (arXiv:2305.10673v1 [cs.LG])",
    "abstract": "The prevalence of large-scale graphs poses great challenges in time and storage for training and deploying graph neural networks (GNNs). Several recent works have explored solutions for pruning the large original graph into a small and highly-informative one, such that training and inference on the pruned and large graphs have comparable performance. Although empirically effective, current researches focus on static or non-temporal graphs, which are not directly applicable to dynamic scenarios. In addition, they require labels as ground truth to learn the informative structure, limiting their applicability to new problem domains where labels are hard to obtain. To solve the dilemma, we propose and study the problem of unsupervised graph pruning on dynamic graphs. We approach the problem by our proposed STEP, a self-supervised temporal pruning framework that learns to remove potentially redundant edges from input dynamic graphs. From a technical and industrial viewpoint, our method over",
    "link": "http://arxiv.org/abs/2305.10673",
    "context": "Title: Less Can Be More: Unsupervised Graph Pruning for Large-scale Dynamic Graphs. (arXiv:2305.10673v1 [cs.LG])\nAbstract: The prevalence of large-scale graphs poses great challenges in time and storage for training and deploying graph neural networks (GNNs). Several recent works have explored solutions for pruning the large original graph into a small and highly-informative one, such that training and inference on the pruned and large graphs have comparable performance. Although empirically effective, current researches focus on static or non-temporal graphs, which are not directly applicable to dynamic scenarios. In addition, they require labels as ground truth to learn the informative structure, limiting their applicability to new problem domains where labels are hard to obtain. To solve the dilemma, we propose and study the problem of unsupervised graph pruning on dynamic graphs. We approach the problem by our proposed STEP, a self-supervised temporal pruning framework that learns to remove potentially redundant edges from input dynamic graphs. From a technical and industrial viewpoint, our method over",
    "path": "papers/23/05/2305.10673.json",
    "total_tokens": 1018,
    "translated_title": "少即是多：面向大规模动态图的无监督图剪枝",
    "translated_abstract": "大规模图的普及在训练和部署图神经网络（GNN）方面带来了极大的时间和存储挑战。最近的一些研究探索了将大原始图剪枝成一个小而高度信息化的图的解决方案，使得对修剪后的图和大图的训练和推断具有可比性的性能。虽然经验有效，但当前的研究重点是静态或非时间图，这些图对动态场景的直接应用受到限制。此外，它们需要标签作为基本事实来学习信息结构，这限制了它们对标签难以获得的新问题域的适用性。为了解决这个难题，我们提出并研究了针对动态图的无监督图剪枝问题。我们通过我们提出的STEP方法来解决这个问题，这是一个自我监督的时间剪枝框架，它学习从输入的动态图中去除潜在冗余的边缘。从技术和工业的角度来看，我们的方法克服了现有动态图剪枝技术的限制，不需要任何标记数据，并以剪枝效率和有效性方面实现了最先进的性能。",
    "tldr": "本研究提出了一个无监督图剪枝框架，名为STEP，用于解决大规模动态图的训练和部署问题，该框架不需要标签数据，并且具有最先进的剪枝效率和有效性。",
    "en_tdlr": "This paper proposes an unsupervised graph pruning framework, called STEP, for addressing the challenges in training and deploying large-scale dynamic graphs for graph neural networks. The framework overcomes limitations of existing dynamic graph pruning techniques by not requiring labeled data and achieving state-of-the-art pruning efficiency and effectiveness."
}