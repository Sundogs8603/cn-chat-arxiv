{
    "title": "SNEkhorn: Dimension Reduction with Symmetric Entropic Affinities. (arXiv:2305.13797v1 [cs.LG])",
    "abstract": "Many approaches in machine learning rely on a weighted graph to encode the similarities between samples in a dataset. Entropic affinities (EAs), which are notably used in the popular Dimensionality Reduction (DR) algorithm t-SNE, are particular instances of such graphs. To ensure robustness to heterogeneous sampling densities, EAs assign a kernel bandwidth parameter to every sample in such a way that the entropy of each row in the affinity matrix is kept constant at a specific value, whose exponential is known as perplexity. EAs are inherently asymmetric and row-wise stochastic, but they are used in DR approaches after undergoing heuristic symmetrization methods that violate both the row-wise constant entropy and stochasticity properties. In this work, we uncover a novel characterization of EA as an optimal transport problem, allowing a natural symmetrization that can be computed efficiently using dual ascent. The corresponding novel affinity matrix derives advantages from symmetric do",
    "link": "http://arxiv.org/abs/2305.13797",
    "context": "Title: SNEkhorn: Dimension Reduction with Symmetric Entropic Affinities. (arXiv:2305.13797v1 [cs.LG])\nAbstract: Many approaches in machine learning rely on a weighted graph to encode the similarities between samples in a dataset. Entropic affinities (EAs), which are notably used in the popular Dimensionality Reduction (DR) algorithm t-SNE, are particular instances of such graphs. To ensure robustness to heterogeneous sampling densities, EAs assign a kernel bandwidth parameter to every sample in such a way that the entropy of each row in the affinity matrix is kept constant at a specific value, whose exponential is known as perplexity. EAs are inherently asymmetric and row-wise stochastic, but they are used in DR approaches after undergoing heuristic symmetrization methods that violate both the row-wise constant entropy and stochasticity properties. In this work, we uncover a novel characterization of EA as an optimal transport problem, allowing a natural symmetrization that can be computed efficiently using dual ascent. The corresponding novel affinity matrix derives advantages from symmetric do",
    "path": "papers/23/05/2305.13797.json",
    "total_tokens": 843,
    "translated_title": "SNEkhorn: 对称熵亲和力下的降维方法",
    "translated_abstract": "许多机器学习方法都依赖于加权图来编码数据集中样本间的相似性。熵亲和力（EAs）是这类图的特例，它通常用于流形学习算法 t-SNE 中。为了保证对不同采样密度的数据具有鲁棒性，EAs 按一定方式对每个样本分配一个核带宽参数，以使得亲和力矩阵中每一行的熵都保持在一个特定的指数参数下。EAs具有不对称性和按行随机性，但是在经过启发式对称化方法处理后，又被用于降维。本文发现了EAs的一种新颖的优化形式，视其作为最优传输问题，实现了自然的对称化，并且可用双重上升法高效计算。由此得到的亲和力矩阵有效地避免了对称化所带来的熵和随机性问题。",
    "tldr": "提出了一种新的对称化方法用于熵亲和力下的降维算法，能够有效解决对称化过程中的熵和随机性问题。",
    "en_tdlr": "Introducing a novel symmetrization method for dimension reduction algorithm based on entropic affinities, which can effectively avoid the entropy and stochasticity issues brought by symmetrization process."
}