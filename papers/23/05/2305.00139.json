{
    "title": "Leveraging Label Non-Uniformity for Node Classification in Graph Neural Networks. (arXiv:2305.00139v1 [cs.LG])",
    "abstract": "In node classification using graph neural networks (GNNs), a typical model generates logits for different class labels at each node. A softmax layer often outputs a label prediction based on the largest logit. We demonstrate that it is possible to infer hidden graph structural information from the dataset using these logits. We introduce the key notion of label non-uniformity, which is derived from the Wasserstein distance between the softmax distribution of the logits and the uniform distribution. We demonstrate that nodes with small label non-uniformity are harder to classify correctly. We theoretically analyze how the label non-uniformity varies across the graph, which provides insights into boosting the model performance: increasing training samples with high non-uniformity or dropping edges to reduce the maximal cut size of the node set of small non-uniformity. These mechanisms can be easily added to a base GNN model. Experimental results demonstrate that our approach improves the",
    "link": "http://arxiv.org/abs/2305.00139",
    "context": "Title: Leveraging Label Non-Uniformity for Node Classification in Graph Neural Networks. (arXiv:2305.00139v1 [cs.LG])\nAbstract: In node classification using graph neural networks (GNNs), a typical model generates logits for different class labels at each node. A softmax layer often outputs a label prediction based on the largest logit. We demonstrate that it is possible to infer hidden graph structural information from the dataset using these logits. We introduce the key notion of label non-uniformity, which is derived from the Wasserstein distance between the softmax distribution of the logits and the uniform distribution. We demonstrate that nodes with small label non-uniformity are harder to classify correctly. We theoretically analyze how the label non-uniformity varies across the graph, which provides insights into boosting the model performance: increasing training samples with high non-uniformity or dropping edges to reduce the maximal cut size of the node set of small non-uniformity. These mechanisms can be easily added to a base GNN model. Experimental results demonstrate that our approach improves the",
    "path": "papers/23/05/2305.00139.json",
    "total_tokens": 881,
    "translated_title": "利用标签不均匀性进行图神经网络中的节点分类",
    "translated_abstract": "在使用图神经网络（GNN）进行节点分类时，典型的模型会为每个节点生成不同类标签的对数。通过这些用softmax层输出最大对数的标签预测，我们演示了从数据集中可以推断出隐藏的图形结构信息。我们引入了标签不均匀性的关键概念，它是从对数的softmax分布和均匀分布之间的Wasserstein距离中导出的。我们证明了具有小标签不均匀性的节点更难分类正确。我们从理论上分析了标签不均匀性在整个图中的变化，这提供了提高模型性能的见解：增加高标签不均匀性的训练样本或删除边以减少具有小标签不均匀性的节点集的最大割大小。这些机制可以轻松地添加到基本的GNN模型中。实验结果表明，我们的方法提高了模型的性能。",
    "tldr": "本文针对标签不均匀的节点分类问题，提出了一种利用标签不均匀性进行图神经网络中的节点分类方法，在理论上分析了标签不均匀性在整个图中的变化，并提供了两种提高模型性能的方式，经实验证明可行。",
    "en_tdlr": "This paper proposes a method for node classification in graph neural networks using label non-uniformity, which theoretically analyzes the variation of label non-uniformity throughout the graph and provides two mechanisms to increase model performance, verified by experiments."
}