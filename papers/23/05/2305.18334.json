{
    "title": "Are We There Yet? Product Quantization and its Hardware Acceleration. (arXiv:2305.18334v1 [cs.AR])",
    "abstract": "Conventional multiply-accumulate (MAC) operations have long dominated computation time for deep neural networks (DNNs). Recently, product quantization (PQ) has been successfully applied to these workloads, replacing MACs with memory lookups to pre-computed dot products. While this property makes PQ an attractive solution for model acceleration, little is understood about the associated trade-offs in terms of compute and memory footprint, and the impact on accuracy. Our empirical study investigates the impact of different PQ settings and training methods on layerwise reconstruction error and end-to-end model accuracy. When studying the efficiency of deploying PQ DNNs, we find that metrics such as FLOPs, number of parameters, and even CPU/GPU performance, can be misleading. To address this issue, and to more fairly assess PQ in terms of hardware efficiency, we design the first custom hardware accelerator to evaluate the speed and efficiency of running PQ models. We identify PQ configurat",
    "link": "http://arxiv.org/abs/2305.18334",
    "context": "Title: Are We There Yet? Product Quantization and its Hardware Acceleration. (arXiv:2305.18334v1 [cs.AR])\nAbstract: Conventional multiply-accumulate (MAC) operations have long dominated computation time for deep neural networks (DNNs). Recently, product quantization (PQ) has been successfully applied to these workloads, replacing MACs with memory lookups to pre-computed dot products. While this property makes PQ an attractive solution for model acceleration, little is understood about the associated trade-offs in terms of compute and memory footprint, and the impact on accuracy. Our empirical study investigates the impact of different PQ settings and training methods on layerwise reconstruction error and end-to-end model accuracy. When studying the efficiency of deploying PQ DNNs, we find that metrics such as FLOPs, number of parameters, and even CPU/GPU performance, can be misleading. To address this issue, and to more fairly assess PQ in terms of hardware efficiency, we design the first custom hardware accelerator to evaluate the speed and efficiency of running PQ models. We identify PQ configurat",
    "path": "papers/23/05/2305.18334.json",
    "total_tokens": 954,
    "translated_title": "这里是翻译过的论文标题：我们到了吗？产品量化及其硬件加速。",
    "translated_abstract": "传统的乘加（MAC）运算长期以来一直主导深度神经网络（DNN）的计算时间。最近，产品量化（PQ）已成功地应用于这些工作负载，用预先计算的点积的内存查找替换了MAC。虽然这个属性使PQ成为模型加速的一个有吸引力的解决方案，但人们很少了解与计算和存储器占用相关的权衡，以及对准确性的影响。我们的实证研究调查了不同PQ设置和训练方法对逐层重建误差和端到端模型准确性的影响。在研究部署PQ DNN的效率时，我们发现FLOP、参数数量甚至CPU/GPU性能等指标可能具有误导性。为了解决这个问题，更公平地评估PQ的硬件效率，我们设计了第一个定制的硬件加速器，用于评估运行PQ模型的速度和效率。我们确定了PQ配置的硬件性能和存储要求之间的权衡。",
    "tldr": "本文研究了产品量化（PQ）在深度神经网络中替代传统乘加（MAC）运算的效果。作者发现FLOP和参数数量等指标可能具有误导性，并设计了第一个PQ定制硬件加速器评估其性能和效率。",
    "en_tdlr": "This paper investigates the effectiveness of product quantization (PQ) as a replacement for traditional multiply-accumulate (MAC) operations in deep neural networks. By designing the first custom hardware accelerator for PQ models, the authors reveal that metrics such as FLOPs and parameter count can be misleading when evaluating PQ efficiency."
}