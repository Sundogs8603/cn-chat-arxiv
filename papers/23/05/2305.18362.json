{
    "title": "Statistically Significant Concept-based Explanation of Image Classifiers via Model Knockoffs. (arXiv:2305.18362v1 [cs.CV])",
    "abstract": "A concept-based classifier can explain the decision process of a deep learning model by human-understandable concepts in image classification problems. However, sometimes concept-based explanations may cause false positives, which misregards unrelated concepts as important for the prediction task. Our goal is to find the statistically significant concept for classification to prevent misinterpretation. In this study, we propose a method using a deep learning model to learn the image concept and then using the Knockoff samples to select the important concepts for prediction by controlling the False Discovery Rate (FDR) under a certain value. We evaluate the proposed method in our synthetic and real data experiments. Also, it shows that our method can control the FDR properly while selecting highly interpretable concepts to improve the trustworthiness of the model.",
    "link": "http://arxiv.org/abs/2305.18362",
    "context": "Title: Statistically Significant Concept-based Explanation of Image Classifiers via Model Knockoffs. (arXiv:2305.18362v1 [cs.CV])\nAbstract: A concept-based classifier can explain the decision process of a deep learning model by human-understandable concepts in image classification problems. However, sometimes concept-based explanations may cause false positives, which misregards unrelated concepts as important for the prediction task. Our goal is to find the statistically significant concept for classification to prevent misinterpretation. In this study, we propose a method using a deep learning model to learn the image concept and then using the Knockoff samples to select the important concepts for prediction by controlling the False Discovery Rate (FDR) under a certain value. We evaluate the proposed method in our synthetic and real data experiments. Also, it shows that our method can control the FDR properly while selecting highly interpretable concepts to improve the trustworthiness of the model.",
    "path": "papers/23/05/2305.18362.json",
    "total_tokens": 854,
    "translated_title": "基于概念的解释模型敲门技术检验图像分类器的显著统计结论",
    "translated_abstract": "基于概念的分类器可以通过人类可理解的概念来解释深度学习模型在图像分类问题上的决策过程。然而，有时概念解释可能会导致误报，将不相关的概念误认为是预测任务的重要因素。我们的目标是找到用于分类的显著概念以防止误解。在本研究中，我们提出了一种方法，使用深度学习模型学习图像概念，然后使用Knockoff样本选择重要概念进行预测，并控制误发现率（FDR）在某个值下。我们在合成和真实数据实验中评估了所提出的方法。结果表明，该方法可以适当地控制FDR，同时选择高度可解释的概念，提高模型的可信度。",
    "tldr": "该研究提出了一种基于概念的解释模型敲门技术，可以在图像分类任务中找到显著的概念以避免误解，并控制误发现率（FDR）在某个值下，在合成和真实数据实验中得到验证。"
}