{
    "title": "ReGen: Zero-Shot Text Classification via Training Data Generation with Progressive Dense Retrieval. (arXiv:2305.10703v1 [cs.CL])",
    "abstract": "With the development of large language models (LLMs), zero-shot learning has attracted much attention for various NLP tasks. Different from prior works that generate training data with billion-scale natural language generation (NLG) models, we propose a retrieval-enhanced framework to create training data from a general-domain unlabeled corpus. To realize this, we first conduct contrastive pretraining to learn an unsupervised dense retriever for extracting the most relevant documents using class-descriptive verbalizers. We then further propose two simple strategies, namely Verbalizer Augmentation with Demonstrations and Self-consistency Guided Filtering to improve the topic coverage of the dataset while removing noisy examples. Experiments on nine datasets demonstrate that REGEN achieves 4.3% gain over the strongest baselines and saves around 70% of the time compared to baselines using large NLG models. Besides, REGEN can be naturally integrated with recently proposed large language mo",
    "link": "http://arxiv.org/abs/2305.10703",
    "context": "Title: ReGen: Zero-Shot Text Classification via Training Data Generation with Progressive Dense Retrieval. (arXiv:2305.10703v1 [cs.CL])\nAbstract: With the development of large language models (LLMs), zero-shot learning has attracted much attention for various NLP tasks. Different from prior works that generate training data with billion-scale natural language generation (NLG) models, we propose a retrieval-enhanced framework to create training data from a general-domain unlabeled corpus. To realize this, we first conduct contrastive pretraining to learn an unsupervised dense retriever for extracting the most relevant documents using class-descriptive verbalizers. We then further propose two simple strategies, namely Verbalizer Augmentation with Demonstrations and Self-consistency Guided Filtering to improve the topic coverage of the dataset while removing noisy examples. Experiments on nine datasets demonstrate that REGEN achieves 4.3% gain over the strongest baselines and saves around 70% of the time compared to baselines using large NLG models. Besides, REGEN can be naturally integrated with recently proposed large language mo",
    "path": "papers/23/05/2305.10703.json",
    "total_tokens": 963,
    "translated_title": "ReGen: 通过渐进式密集检索生成训练数据的零样本文本分类方法",
    "translated_abstract": "随着大型语言模型（LLM）的发展，零样本学习在各种NLP任务中受到了许多关注。与以往使用数十亿级自然语言生成模型生成训练数据的方法不同，我们提出了一种检索增强的框架，从通用领域的无标签语料库中创建训练数据。为实现这一目标，我们首先进行对比预训练，使用类别描述性话语学习了一个无监督的密集检索器以提取最相关的文档。我们进一步提出了两种简单的策略，即展示增强的话语生成和自一致性引导过滤，以提高数据集的主题覆盖率，同时删除噪声样本。对九个数据集的实验表明，REGEN相较于最强的基线模型提高了4.3%的性能，并且与使用大型NLG模型的基线相比节省了约70％的时间。此外，REGEN可以自然地与最近提出的大型语言模型相结合。",
    "tldr": "本文提出了一种基于检索增强的框架，通过渐进式密集检索从通用领域的无标签语料库中创建训练数据，实现了零样本文本分类，相较于最强的基线模型提高了4.3%的性能，与使用大型NLG模型的基线相比节省了约70％的时间。",
    "en_tdlr": "This paper proposes a retrieval-enhanced framework to generate training data from a general-domain unlabeled corpus via progressive dense retrieval for zero-shot text classification, achieving 4.3% gain over the strongest baselines and saving around 70% of the time compared to baselines using large NLG models."
}