{
    "title": "Latent Exploration for Reinforcement Learning. (arXiv:2305.20065v2 [cs.RO] UPDATED)",
    "abstract": "In Reinforcement Learning, agents learn policies by exploring and interacting with the environment. Due to the curse of dimensionality, learning policies that map high-dimensional sensory input to motor output is particularly challenging. During training, state of the art methods (SAC, PPO, etc.) explore the environment by perturbing the actuation with independent Gaussian noise. While this unstructured exploration has proven successful in numerous tasks, it can be suboptimal for overactuated systems. When multiple actuators, such as motors or muscles, drive behavior, uncorrelated perturbations risk diminishing each other's effect, or modifying the behavior in a task-irrelevant way. While solutions to introduce time correlation across action perturbations exist, introducing correlation across actuators has been largely ignored. Here, we propose LATent TIme-Correlated Exploration (Lattice), a method to inject temporally-correlated noise into the latent state of the policy network, which",
    "link": "http://arxiv.org/abs/2305.20065",
    "context": "Title: Latent Exploration for Reinforcement Learning. (arXiv:2305.20065v2 [cs.RO] UPDATED)\nAbstract: In Reinforcement Learning, agents learn policies by exploring and interacting with the environment. Due to the curse of dimensionality, learning policies that map high-dimensional sensory input to motor output is particularly challenging. During training, state of the art methods (SAC, PPO, etc.) explore the environment by perturbing the actuation with independent Gaussian noise. While this unstructured exploration has proven successful in numerous tasks, it can be suboptimal for overactuated systems. When multiple actuators, such as motors or muscles, drive behavior, uncorrelated perturbations risk diminishing each other's effect, or modifying the behavior in a task-irrelevant way. While solutions to introduce time correlation across action perturbations exist, introducing correlation across actuators has been largely ignored. Here, we propose LATent TIme-Correlated Exploration (Lattice), a method to inject temporally-correlated noise into the latent state of the policy network, which",
    "path": "papers/23/05/2305.20065.json",
    "total_tokens": 885,
    "translated_title": "强化学习中的潜在探索",
    "translated_abstract": "在强化学习中，智能体通过探索和与环境互动来学习策略。由于维度灾难，学习将高维感知输入映射到运动输出的策略尤其具有挑战性。在训练过程中，最先进的方法（如SAC，PPO等）通过对作用力施加独立的高斯噪声来探索环境。尽管这种非结构化的探索方法在许多任务中证明成功，但对于过动作系统来说可能不够优化。当多个作用器（如马达或肌肉）驱动行为时，不相关的扰动可能会减少彼此的影响，或以与任务无关的方式修改行为。虽然已经存在通过引入动作扰动之间的时间相关性来解决这个问题的方法，但忽视了跨作用器之间的相关性。在本文中，我们提出了潜在时间相关探索（Lattice），一种将时间相关噪声注入到策略网络的潜在状态中的方法。",
    "tldr": "本文提出了一种名为Lattice的方法，通过向策略网络的潜在状态中注入时间相关性噪声，来解决强化学习中多作用器系统存在的探索问题。"
}