{
    "title": "Faithful Low-Resource Data-to-Text Generation through Cycle Training. (arXiv:2305.14793v1 [cs.CL])",
    "abstract": "Methods to generate text from structured data have advanced significantly in recent years, primarily due to fine-tuning of pre-trained language models on large datasets. However, such models can fail to produce output faithful to the input data, particularly on out-of-domain data. Sufficient annotated data is often not available for specific domains, leading us to seek an unsupervised approach to improve the faithfulness of output text. Since the problem is fundamentally one of consistency between the representations of the structured data and text, we evaluate the effectiveness of cycle training in this work. Cycle training uses two models which are inverses of each other: one that generates text from structured data, and one which generates the structured data from natural language text. We show that cycle training, when initialized with a small amount of supervised data (100 samples in our case), achieves nearly the same performance as fully supervised approaches for the data-to-tex",
    "link": "http://arxiv.org/abs/2305.14793",
    "context": "Title: Faithful Low-Resource Data-to-Text Generation through Cycle Training. (arXiv:2305.14793v1 [cs.CL])\nAbstract: Methods to generate text from structured data have advanced significantly in recent years, primarily due to fine-tuning of pre-trained language models on large datasets. However, such models can fail to produce output faithful to the input data, particularly on out-of-domain data. Sufficient annotated data is often not available for specific domains, leading us to seek an unsupervised approach to improve the faithfulness of output text. Since the problem is fundamentally one of consistency between the representations of the structured data and text, we evaluate the effectiveness of cycle training in this work. Cycle training uses two models which are inverses of each other: one that generates text from structured data, and one which generates the structured data from natural language text. We show that cycle training, when initialized with a small amount of supervised data (100 samples in our case), achieves nearly the same performance as fully supervised approaches for the data-to-tex",
    "path": "papers/23/05/2305.14793.json",
    "total_tokens": 924,
    "translated_title": "基于循环训练的低资源数据生成文本方法来自于准确性",
    "translated_abstract": "近年来，从结构化数据生成文本的方法取得了显著进展，主要是通过在大型数据集上微调预训练的语言模型。然而，这些模型在特定领域的数据上可能无法产生与输入数据相符的输出文本，尤其是在域外数据上。由于缺少特定领域的足够注释数据，因此我们寻求一种无监督的方法来改善输出文本的准确性。我们在本文中通过循环训练来解决这个问题，因为这个问题本质上是结构化数据和文本之间表示的一致性问题。循环训练使用两个互为反函数的模型：一个从结构化数据生成文本，另一个从自然语言文本生成结构化数据。我们表明，在少量监督数据（我们的情况下100个样本）的情况下初始化的循环训练方法，可以实现数据生成文本任务与全监督方法相近的性能，同时极大地提高了非域数据生成的文本的准确性。",
    "tldr": "本文通过基于循环训练的方法，在少量监督数据的情况下，实现了生成文本任务与全监督方法相近的性能，同时极大地提高了非域数据生成的文本的准确性。",
    "en_tdlr": "This article proposes a cycle training method to improve the faithfulness of generated text from structured data, especially on out-of-domain data. By using two inverse models, the proposed method achieves nearly the same performance as fully supervised methods with only a small amount of annotated data."
}