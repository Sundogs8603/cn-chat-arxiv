{
    "title": "Mitigating Group Bias in Federated Learning: Beyond Local Fairness. (arXiv:2305.09931v1 [cs.LG])",
    "abstract": "The issue of group fairness in machine learning models, where certain sub-populations or groups are favored over others, has been recognized for some time. While many mitigation strategies have been proposed in centralized learning, many of these methods are not directly applicable in federated learning, where data is privately stored on multiple clients. To address this, many proposals try to mitigate bias at the level of clients before aggregation, which we call locally fair training. However, the effectiveness of these approaches is not well understood. In this work, we investigate the theoretical foundation of locally fair training by studying the relationship between global model fairness and local model fairness. Additionally, we prove that for a broad class of fairness metrics, the global model's fairness can be obtained using only summary statistics from local clients. Based on that, we propose a globally fair training algorithm that directly minimizes the penalized empirical l",
    "link": "http://arxiv.org/abs/2305.09931",
    "context": "Title: Mitigating Group Bias in Federated Learning: Beyond Local Fairness. (arXiv:2305.09931v1 [cs.LG])\nAbstract: The issue of group fairness in machine learning models, where certain sub-populations or groups are favored over others, has been recognized for some time. While many mitigation strategies have been proposed in centralized learning, many of these methods are not directly applicable in federated learning, where data is privately stored on multiple clients. To address this, many proposals try to mitigate bias at the level of clients before aggregation, which we call locally fair training. However, the effectiveness of these approaches is not well understood. In this work, we investigate the theoretical foundation of locally fair training by studying the relationship between global model fairness and local model fairness. Additionally, we prove that for a broad class of fairness metrics, the global model's fairness can be obtained using only summary statistics from local clients. Based on that, we propose a globally fair training algorithm that directly minimizes the penalized empirical l",
    "path": "papers/23/05/2305.09931.json",
    "total_tokens": 1050,
    "translated_title": "缓解联邦学习中的群体偏见:超越本地公平",
    "translated_abstract": "机器学习模型中群体公平的问题已经被认识到一段时间了，其中某些子人群或群体被优先考虑。虽然在集中式学习中提出了许多缓解策略，但这些方法在联邦学习中不直接适用，因为数据存在于多个客户端上并被私下存储。为了解决这个问题，许多提议试图在聚合之前在客户端水平上缓解偏差，我们称之为本地公平训练。然而，这些方法的有效性尚未得到很好的理解。在这项工作中，我们通过研究全局模型公平性和本地模型公平性之间的关系，研究了本地公平训练的理论基础。此外，我们证明了对于广泛的公平指标类别，可以仅使用本地客户端的汇总统计信息来获得全局模型的公平性。基于此，我们提出了一种具有全局公平性的训练算法，该算法直接最小化全局模型和本地模型之间的经验惩罚L2距离，同时使用拉格朗日乘数考虑群体公平。在合成数据集和真实数据集上的实验结果显示，我们提出的方法在公平性和准确性方面优于现有方法。",
    "tldr": "本文研究了联邦学习中群体偏见问题，并提出了一种全局公平、利用拉格朗日乘数考虑群体公平的训练算法，实验证明该方法在公平性和准确性方面优于现有方法。",
    "en_tdlr": "This paper addresses the issue of group fairness in federated learning and proposes a globally fair training algorithm that takes into account group fairness using a Lagrange multiplier. Experiments show that the proposed method outperforms existing approaches in terms of fairness and accuracy."
}