{
    "title": "Large Language Models are not Fair Evaluators. (arXiv:2305.17926v2 [cs.CL] UPDATED)",
    "abstract": "In this paper, we uncover a systematic bias in the evaluation paradigm of adopting large language models~(LLMs), e.g., GPT-4, as a referee to score and compare the quality of responses generated by candidate models. We find that the quality ranking of candidate responses can be easily hacked by simply altering their order of appearance in the context. This manipulation allows us to skew the evaluation result, making one model appear considerably superior to the other, e.g., Vicuna-13B could beat ChatGPT on 66 over 80 tested queries with ChatGPT as an evaluator. To address this issue, we propose a calibration framework with three simple yet effective strategies: 1) Multiple Evidence Calibration, which requires the evaluator model to generate multiple evaluation evidence before assigning ratings; 2) Balanced Position Calibration, which aggregates results across various orders to determine the final score; 3) Human-in-the-Loop Calibration, which introduces a balanced position diversity en",
    "link": "http://arxiv.org/abs/2305.17926",
    "context": "Title: Large Language Models are not Fair Evaluators. (arXiv:2305.17926v2 [cs.CL] UPDATED)\nAbstract: In this paper, we uncover a systematic bias in the evaluation paradigm of adopting large language models~(LLMs), e.g., GPT-4, as a referee to score and compare the quality of responses generated by candidate models. We find that the quality ranking of candidate responses can be easily hacked by simply altering their order of appearance in the context. This manipulation allows us to skew the evaluation result, making one model appear considerably superior to the other, e.g., Vicuna-13B could beat ChatGPT on 66 over 80 tested queries with ChatGPT as an evaluator. To address this issue, we propose a calibration framework with three simple yet effective strategies: 1) Multiple Evidence Calibration, which requires the evaluator model to generate multiple evaluation evidence before assigning ratings; 2) Balanced Position Calibration, which aggregates results across various orders to determine the final score; 3) Human-in-the-Loop Calibration, which introduces a balanced position diversity en",
    "path": "papers/23/05/2305.17926.json",
    "total_tokens": 880,
    "translated_title": "大语言模型不是公平的评估器。",
    "translated_abstract": "在这篇论文中，我们揭示了采用大语言模型（LLMs）（例如GPT-4）作为裁判来评分和比较候选模型生成的响应质量的评估范式中存在的系统偏差。我们发现，通过简单地改变候选响应在上下文中出现的顺序，可以轻松地操纵候选响应的质量排名。这种操纵使得一个模型看起来比另一个模型要优越得多，例如，使用ChatGPT作为评估器，在80个测试查询中，Vicuna-13B可以击败ChatGPT的66个。为了解决这个问题，我们提出了一个校准框架，其中包含三个简单而有效的策略：1）多证据校准，要求评估模型在分配评分之前生成多个评估证据；2）均衡位置校准，在各种顺序中聚合结果以确定最终分数；3）人机协同校准，引入平衡的位置多样性。",
    "tldr": "本文揭示了使用大语言模型作为评估器时存在的系统偏差，可以通过改变候选响应的顺序来操纵评估结果。为了解决这个问题，提出了一个校准框架，包括多证据校准、均衡位置校准和人机协同校准。"
}