{
    "title": "RadAdapt: Radiology Report Summarization via Lightweight Domain Adaptation of Large Language Models. (arXiv:2305.01146v1 [cs.CL])",
    "abstract": "We systematically investigate lightweight strategies to adapt large language models (LLMs) for the task of radiology report summarization (RRS). Specifically, we focus on domain adaptation via pretraining (on natural language, biomedical text, and clinical text) and via prompting (zero-shot, in-context learning) or parameter-efficient fine-tuning (prefix tuning, LoRA). Our results on the MIMIC-III dataset consistently demonstrate best performance by maximally adapting to the task via pretraining on clinical text and parameter-efficient fine-tuning on RRS examples. Importantly, this method fine-tunes a mere 0.32% of parameters throughout the model, in contrast to end-to-end fine-tuning (100% of parameters). Additionally, we study the effect of in-context examples and out-of-distribution (OOD) training before concluding with a radiologist reader study and qualitative analysis. Our findings highlight the importance of domain adaptation in RRS and provide valuable insights toward developin",
    "link": "http://arxiv.org/abs/2305.01146",
    "context": "Title: RadAdapt: Radiology Report Summarization via Lightweight Domain Adaptation of Large Language Models. (arXiv:2305.01146v1 [cs.CL])\nAbstract: We systematically investigate lightweight strategies to adapt large language models (LLMs) for the task of radiology report summarization (RRS). Specifically, we focus on domain adaptation via pretraining (on natural language, biomedical text, and clinical text) and via prompting (zero-shot, in-context learning) or parameter-efficient fine-tuning (prefix tuning, LoRA). Our results on the MIMIC-III dataset consistently demonstrate best performance by maximally adapting to the task via pretraining on clinical text and parameter-efficient fine-tuning on RRS examples. Importantly, this method fine-tunes a mere 0.32% of parameters throughout the model, in contrast to end-to-end fine-tuning (100% of parameters). Additionally, we study the effect of in-context examples and out-of-distribution (OOD) training before concluding with a radiologist reader study and qualitative analysis. Our findings highlight the importance of domain adaptation in RRS and provide valuable insights toward developin",
    "path": "papers/23/05/2305.01146.json",
    "total_tokens": 1035,
    "translated_title": "RadAdapt：通过大型语言模型的轻量化领域自适应实现放射学报告摘要",
    "translated_abstract": "本文系统地研究了轻量级策略，通过预训练（自然语言，生物医学文本，临床文本）和提示（零-shot、上下文学习）或参数高效微调（前缀微调，LoRA），来适应大型语言模型（LLMs）进行放射性报告摘要（RRS）任务。结果表明，最大程度地适应任务的方法是，通过在临床文本上预先训练，然后在RRS示例上进行参数高效微调。值得注意的是，这种方法仅微调模型的0.32％的参数，与端对端微调（100％的参数）形成对比。此外，在研究上下文示例和分布外（OOD）训练的影响后，我们进行了放射科医师读者研究和定性分析。我们的研究结果强调了领域适应在RRS中的重要性，并为开发更好的放射性报告摘要模型提供了有价值的见解。",
    "tldr": "本研究重点研究了轻量化策略，通过在临床文本上进行预训练和在RRS示例上进行参数高效微调，实现适应大型语言模型进行放射性报告摘要（RRS）任务。并且该方法仅微调模型的0.32％的参数，提高了表现。研究结果强调了领域适应在RRS中的重要性，并为开发更好的放射性报告摘要模型提供了有价值的见解。",
    "en_tdlr": "This study focuses on adapting large language models for radiology report summarization via lightweight strategies, including pretraining on clinical text and parameter-efficient fine-tuning on RRS examples, which fine-tunes only 0.32% of model parameters. The importance of domain adaptation in RRS is highlighted and valuable insights are provided towards developing better models."
}