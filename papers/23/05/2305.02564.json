{
    "title": "RetroMAE-2: Duplex Masked Auto-Encoder For Pre-Training Retrieval-Oriented Language Models. (arXiv:2305.02564v1 [cs.CL])",
    "abstract": "To better support information retrieval tasks such as web search and open-domain question answering, growing effort is made to develop retrieval-oriented language models, e.g., RetroMAE and many others. Most of the existing works focus on improving the semantic representation capability for the contextualized embedding of the [CLS] token. However, recent study shows that the ordinary tokens besides [CLS] may provide extra information, which help to produce a better representation effect. As such, it's necessary to extend the current methods where all contextualized embeddings can be jointly pre-trained for the retrieval tasks. In this work, we propose a novel pre-training method called Duplex Masked Auto-Encoder, a.k.a. DupMAE. It is designed to improve the quality of semantic representation where all contextualized embeddings of the pre-trained model can be leveraged. It takes advantage of two complementary auto-encoding tasks: one reconstructs the input sentence on top of the [CLS] e",
    "link": "http://arxiv.org/abs/2305.02564",
    "context": "Title: RetroMAE-2: Duplex Masked Auto-Encoder For Pre-Training Retrieval-Oriented Language Models. (arXiv:2305.02564v1 [cs.CL])\nAbstract: To better support information retrieval tasks such as web search and open-domain question answering, growing effort is made to develop retrieval-oriented language models, e.g., RetroMAE and many others. Most of the existing works focus on improving the semantic representation capability for the contextualized embedding of the [CLS] token. However, recent study shows that the ordinary tokens besides [CLS] may provide extra information, which help to produce a better representation effect. As such, it's necessary to extend the current methods where all contextualized embeddings can be jointly pre-trained for the retrieval tasks. In this work, we propose a novel pre-training method called Duplex Masked Auto-Encoder, a.k.a. DupMAE. It is designed to improve the quality of semantic representation where all contextualized embeddings of the pre-trained model can be leveraged. It takes advantage of two complementary auto-encoding tasks: one reconstructs the input sentence on top of the [CLS] e",
    "path": "papers/23/05/2305.02564.json",
    "total_tokens": 773,
    "translated_title": "RetroMAE-2：用于预训练检索导向语言模型的双工掩码自动编码器",
    "translated_abstract": "为了更好地支持信息检索任务，例如网络搜索和开放领域问答，人们正在努力开发检索导向语言模型，例如RetroMAE和许多其他模型。本文提出了一种新的预训练方法，称为DupMAE，旨在提高预训练模型的所有上下文嵌入的语义表示质量。它利用了两个互补的自动编码任务：一个基于[CLS]嵌入重建输入句子。",
    "tldr": "本文提出了一种名为DupMAE的预训练方法，利用两个自动编码任务来提高语义表示质量，扩展了当前方法，使所有上下文嵌入都可以用于联合预训练检索任务。",
    "en_tdlr": "This paper proposes a pre-training method called Duplex Masked Auto-Encoder (DupMAE) to improve the semantic representation quality of all contextualized embeddings for retrieval tasks. It uses two complementary auto-encoding tasks and extends current methods to allow all contextualized embeddings to be jointly pre-trained."
}