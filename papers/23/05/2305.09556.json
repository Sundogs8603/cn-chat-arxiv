{
    "title": "Adapting Sentence Transformers for the Aviation Domain. (arXiv:2305.09556v1 [cs.CL])",
    "abstract": "Learning effective sentence representations is crucial for many Natural Language Processing (NLP) tasks, including semantic search, semantic textual similarity (STS), and clustering. While multiple transformer models have been developed for sentence embedding learning, these models may not perform optimally when dealing with specialized domains like aviation, which has unique characteristics such as technical jargon, abbreviations, and unconventional grammar. Furthermore, the absence of labeled datasets makes it difficult to train models specifically for the aviation domain. To address these challenges, we propose a novel approach for adapting sentence transformers for the aviation domain. Our method is a two-stage process consisting of pre-training followed by fine-tuning. During pre-training, we use Transformers and Sequential Denoising AutoEncoder (TSDAE) with aviation text data as input to improve the initial model performance. Subsequently, we fine-tune our models using a Natural ",
    "link": "http://arxiv.org/abs/2305.09556",
    "context": "Title: Adapting Sentence Transformers for the Aviation Domain. (arXiv:2305.09556v1 [cs.CL])\nAbstract: Learning effective sentence representations is crucial for many Natural Language Processing (NLP) tasks, including semantic search, semantic textual similarity (STS), and clustering. While multiple transformer models have been developed for sentence embedding learning, these models may not perform optimally when dealing with specialized domains like aviation, which has unique characteristics such as technical jargon, abbreviations, and unconventional grammar. Furthermore, the absence of labeled datasets makes it difficult to train models specifically for the aviation domain. To address these challenges, we propose a novel approach for adapting sentence transformers for the aviation domain. Our method is a two-stage process consisting of pre-training followed by fine-tuning. During pre-training, we use Transformers and Sequential Denoising AutoEncoder (TSDAE) with aviation text data as input to improve the initial model performance. Subsequently, we fine-tune our models using a Natural ",
    "path": "papers/23/05/2305.09556.json",
    "total_tokens": 1070,
    "translated_title": "针对航空领域进行句子变换器的适应性研究",
    "translated_abstract": "学习有效的句子表示对于许多自然语言处理任务至关重要，包括语义搜索、语义文本相似度（STS）和聚类。虽然已经开发了多个用于句子嵌入学习的变形器模型，但是这些模型在处理具有唯一特征的专业领域时，如航空领域，可能无法发挥最佳性能，因为航空领域包含特殊术语、缩写词和非传统语法等领域特有特点。此外，缺乏标记的数据集使得难以专门训练航空领域的模型。为了解决这些挑战，我们提出了一种针对航空领域调整句子变换器的新方法。我们的方法是一个两阶段的过程，包括预训练和微调。在预训练阶段，我们使用含航空文本数据的变形器和序列去噪自编码器(TSDAE)作为输入来提高初始模型性能。随后，我们使用少量注释的航空数据集进行自然语言推理（NLI）任务来微调我们的模型。在几个与航空相关的自然语言处理任务上的实验结果表明，我们的方法明显优于基准变换模型，并在某些情况下取得了最新的结果。",
    "tldr": "本研究提出了一种针对航空领域的句子变换器调整方法，在预训练阶段使用TSDAE模型进行改进，然后在少量注释的数据集上进行微调，实验结果表明在航空相关的自然语言处理任务中取得了最好的表现。",
    "en_tdlr": "This paper proposes a method for adapting sentence transformers for the aviation domain, which uses TSDAE for pre-training and fine-tuning with a small annotated aviation dataset. The experimental results show that this method outperforms baseline transformer models and achieves state-of-the-art results in some cases on several aviation-related NLP tasks."
}