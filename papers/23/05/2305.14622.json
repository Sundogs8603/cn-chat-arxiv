{
    "title": "EXnet: Efficient In-context Learning for Data-less Text classification. (arXiv:2305.14622v1 [cs.CL])",
    "abstract": "Large pre-trained language models (PLMs) have made significant progress in encoding world knowledge and spawned a new set of learning paradigms including zero-shot, few-shot, and in-context learning. Many language tasks can be modeled as a set of prompts (for example, is this text about geography?) and language models can provide binary answers, i.e., Yes or No. There is evidence to suggest that the next-word prediction used by many PLMs does not align well with zero-shot paradigms. Therefore, PLMs are fine-tuned as a question-answering system. In-context learning extends zero-shot learning by incorporating prompts and examples, resulting in increased task accuracy. Our paper presents EXnet, a model specifically designed to perform in-context learning without any limitations on the number of examples. We argue that in-context learning is an effective method to increase task accuracy, and providing examples facilitates cross-task generalization, especially when it comes to text classifi",
    "link": "http://arxiv.org/abs/2305.14622",
    "context": "Title: EXnet: Efficient In-context Learning for Data-less Text classification. (arXiv:2305.14622v1 [cs.CL])\nAbstract: Large pre-trained language models (PLMs) have made significant progress in encoding world knowledge and spawned a new set of learning paradigms including zero-shot, few-shot, and in-context learning. Many language tasks can be modeled as a set of prompts (for example, is this text about geography?) and language models can provide binary answers, i.e., Yes or No. There is evidence to suggest that the next-word prediction used by many PLMs does not align well with zero-shot paradigms. Therefore, PLMs are fine-tuned as a question-answering system. In-context learning extends zero-shot learning by incorporating prompts and examples, resulting in increased task accuracy. Our paper presents EXnet, a model specifically designed to perform in-context learning without any limitations on the number of examples. We argue that in-context learning is an effective method to increase task accuracy, and providing examples facilitates cross-task generalization, especially when it comes to text classifi",
    "path": "papers/23/05/2305.14622.json",
    "total_tokens": 898,
    "translated_title": "EXnet: 无数据文本分类的高效上下文学习",
    "translated_abstract": "大型预训练语言模型（PLMs）在编码世界知识方面取得了显着进展，并产生了一系列新的学习范例，包括零-shot、少-shot和上下文学习。许多语言任务可以被建模为一组提示（例如，这段文本是否与地理有关？）和语言模型可以提供二进制答案，即“是”或“否”。有证据表明，许多PLM使用的下一个单词预测与零-shot范例不相符合。因此，PLMs会被微调为问答系统。上下文学习通过将提示和示例结合起来，扩展了零-shot学习，从而提高了任务的准确性。我们的论文介绍了EXnet，这是一个专门设计用于在上下文中进行学习的模型，没有任何例外的限制。我们认为，在上下文学习是提高任务准确性的有效方法，并提供示例有助于跨任务的普适性，特别是在文本分类方面。",
    "tldr": "EXnet是一个模型，旨在进行上下文学习，可以在没有示例数量限制的情况下进行。上下文学习是提高任务准确性和跨任务普适性的有效方法，特别是在文本分类方面。",
    "en_tdlr": "EXnet is a model designed for efficient in-context learning for data-less text classification, providing no limitations on the number of examples. In-context learning is an effective method for increasing task accuracy and generalization across tasks, particularly in the field of text classification."
}