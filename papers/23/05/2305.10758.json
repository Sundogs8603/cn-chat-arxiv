{
    "title": "Extracting Low-/High- Frequency Knowledge from Graph Neural Networks and Injecting it into MLPs: An Effective GNN-to-MLP Distillation Framework. (arXiv:2305.10758v1 [cs.LG])",
    "abstract": "Recent years have witnessed the great success of Graph Neural Networks (GNNs) in handling graph-related tasks. However, MLPs remain the primary workhorse for practical industrial applications due to their desirable inference efficiency and scalability. To reduce their gaps, one can directly distill knowledge from a well-designed teacher GNN to a student MLP, which is termed as GNN-to-MLP distillation. However, the process of distillation usually entails a loss of information, and ``which knowledge patterns of GNNs are more likely to be left and distilled into MLPs?\" becomes an important question. In this paper, we first factorize the knowledge learned by GNNs into low- and high-frequency components in the spectral domain and then derive their correspondence in the spatial domain. Furthermore, we identified a potential information drowning problem for existing GNN-to-MLP distillation, i.e., the high-frequency knowledge of the pre-trained GNNs may be overwhelmed by the low-frequency know",
    "link": "http://arxiv.org/abs/2305.10758",
    "context": "Title: Extracting Low-/High- Frequency Knowledge from Graph Neural Networks and Injecting it into MLPs: An Effective GNN-to-MLP Distillation Framework. (arXiv:2305.10758v1 [cs.LG])\nAbstract: Recent years have witnessed the great success of Graph Neural Networks (GNNs) in handling graph-related tasks. However, MLPs remain the primary workhorse for practical industrial applications due to their desirable inference efficiency and scalability. To reduce their gaps, one can directly distill knowledge from a well-designed teacher GNN to a student MLP, which is termed as GNN-to-MLP distillation. However, the process of distillation usually entails a loss of information, and ``which knowledge patterns of GNNs are more likely to be left and distilled into MLPs?\" becomes an important question. In this paper, we first factorize the knowledge learned by GNNs into low- and high-frequency components in the spectral domain and then derive their correspondence in the spatial domain. Furthermore, we identified a potential information drowning problem for existing GNN-to-MLP distillation, i.e., the high-frequency knowledge of the pre-trained GNNs may be overwhelmed by the low-frequency know",
    "path": "papers/23/05/2305.10758.json",
    "total_tokens": 1039,
    "translated_title": "从图神经网络中提取低/高频知识注入MLP：一种有效的GNN-to-MLP蒸馏框架",
    "translated_abstract": "最近几年，图神经网络（GNNs）在处理与图相关的任务方面取得了巨大成功。但是，由于可实现的推断效率和可扩展性，MLPs仍然是实际工业应用的主力军。为了缩小它们之间的差距，可以直接从精心设计的教师GNN中提取知识到学生MLP中，这被称为GNN-to-MLP蒸馏。但是，蒸馏的过程通常会导致信息损失，“哪些GNN的知识模式更可能会被保留并蒸馏到MLP中？”成为一个重要问题。在本文中，我们首先在频谱域中将GNNs学习到的知识分解为低/高频成分，然后推导它们在空间域中的对应关系。此外，我们还确定了现有GNN-to-MLP蒸馏存在潜在信息淹没问题，即预训练的GNNs的高频知识可能被低频知识所覆盖。",
    "tldr": "本文提出了一种有效的GNN-to-MLP蒸馏框架，将GNNs中的低/高频知识注入MLP。通过将GNNs学习到的知识分解为低/高频成分，在空间域中推导它们的对应关系。此外，还解决了现有GNN-to-MLP蒸馏中的信息淹没问题。",
    "en_tdlr": "This paper proposes an effective GNN-to-MLP distillation framework to inject low-/high-frequency knowledge from GNNs into MLPs. By factorizing knowledge learned by GNNs into low- and high-frequency components in the spectral domain and deriving their correspondence in the spatial domain, the paper addresses the issue of information loss during distillation and tackles the problem of high-frequency knowledge of pre-trained GNNs being overwhelmed by the low-frequency knowledge."
}