{
    "title": "Wasserstein Gradient Flows for Optimizing Gaussian Mixture Policies. (arXiv:2305.10411v1 [cs.LG])",
    "abstract": "Robots often rely on a repertoire of previously-learned motion policies for performing tasks of diverse complexities. When facing unseen task conditions or when new task requirements arise, robots must adapt their motion policies accordingly. In this context, policy optimization is the \\emph{de facto} paradigm to adapt robot policies as a function of task-specific objectives. Most commonly-used motion policies carry particular structures that are often overlooked in policy optimization algorithms. We instead propose to leverage the structure of probabilistic policies by casting the policy optimization as an optimal transport problem. Specifically, we focus on robot motion policies that build on Gaussian mixture models (GMMs) and formulate the policy optimization as a Wassertein gradient flow over the GMMs space. This naturally allows us to constrain the policy updates via the $L^2$-Wasserstein distance between GMMs to enhance the stability of the policy optimization process. Furthermor",
    "link": "http://arxiv.org/abs/2305.10411",
    "context": "Title: Wasserstein Gradient Flows for Optimizing Gaussian Mixture Policies. (arXiv:2305.10411v1 [cs.LG])\nAbstract: Robots often rely on a repertoire of previously-learned motion policies for performing tasks of diverse complexities. When facing unseen task conditions or when new task requirements arise, robots must adapt their motion policies accordingly. In this context, policy optimization is the \\emph{de facto} paradigm to adapt robot policies as a function of task-specific objectives. Most commonly-used motion policies carry particular structures that are often overlooked in policy optimization algorithms. We instead propose to leverage the structure of probabilistic policies by casting the policy optimization as an optimal transport problem. Specifically, we focus on robot motion policies that build on Gaussian mixture models (GMMs) and formulate the policy optimization as a Wassertein gradient flow over the GMMs space. This naturally allows us to constrain the policy updates via the $L^2$-Wasserstein distance between GMMs to enhance the stability of the policy optimization process. Furthermor",
    "path": "papers/23/05/2305.10411.json",
    "total_tokens": 1125,
    "translated_title": "Wasserstein梯度流用于优化高斯混合策略",
    "translated_abstract": "机器人在执行各种复杂任务时通常依赖于以前学习到的运动策略库。当面临未知任务条件或出现新任务要求时，机器人必须相应地调整它们的运动策略。在这种情况下，策略优化是将机器人策略作为任务特定目标的函数适应的“事实上”的范例。大多数常用的运动策略具有特定的结构，这些结构经常被忽略在策略优化算法中。我们提出利用概率策略的结构，将策略优化作为最优输运问题进行投影。具体而言，我们专注于机器人运动策略，该策略基于高斯混合模型(GMMs)，并将策略优化构成GMMs空间上的Wassertein梯度流。这自然地允许我们通过GMMs之间的$L^2$-Wasserstein距离约束策略更新，以增强策略优化过程的稳定性。此外，我们展示了如何推导梯度更新的闭式表达式，并推导出一种近端点算法，允许我们将我们的方法扩展到大量的混合成分。我们在模拟和真实机器人任务上的实验表明，与现有的策略优化算法相比，我们的方法导致了改进的样本效率和减少了可变性。",
    "tldr": "本文提出了使用Wasserstein梯度流来优化基于高斯混合模型的机器人运动策略的方法，通过$L^2$-Wasserstein距离来约束策略更新，提高了策略优化的稳定性和样本效率。",
    "en_tdlr": "This paper proposes a method for optimizing robot motion policies based on Gaussian mixture models (GMMs) using Wasserstein gradient flows, constraining policy updates with the $L^2$-Wasserstein distance between GMMs. This improves policy optimization stability and sample efficiency, and closed-form expressions for gradient updates and a proximal point algorithm are derived to scale up the approach to large numbers of mixture components."
}