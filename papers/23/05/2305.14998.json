{
    "title": "An Examination of the Robustness of Reference-Free Image Captioning Evaluation Metrics",
    "abstract": "Recently, reference-free metrics such as CLIPScore (Hessel et al., 2021), UMIC (Lee et al., 2021), and PAC-S (Sarto et al., 2023) have been proposed for automatic reference-free evaluation of image captions. Our focus lies in evaluating the robustness of these metrics in scenarios that require distinguishing between two captions with high lexical overlap but very different meanings. Our findings reveal that despite their high correlation with human judgments, CLIPScore, UMIC, and PAC-S struggle to identify fine-grained errors. While all metrics exhibit strong sensitivity to visual grounding errors, their sensitivity to caption implausibility errors is limited. Furthermore, we found that all metrics are sensitive to variations in the size of image-relevant objects mentioned in the caption, while CLIPScore and PAC-S are also sensitive to the number of mentions of image-relevant objects in the caption. Regarding linguistic aspects of a caption, all metrics show weak comprehension of negat",
    "link": "https://arxiv.org/abs/2305.14998",
    "context": "Title: An Examination of the Robustness of Reference-Free Image Captioning Evaluation Metrics\nAbstract: Recently, reference-free metrics such as CLIPScore (Hessel et al., 2021), UMIC (Lee et al., 2021), and PAC-S (Sarto et al., 2023) have been proposed for automatic reference-free evaluation of image captions. Our focus lies in evaluating the robustness of these metrics in scenarios that require distinguishing between two captions with high lexical overlap but very different meanings. Our findings reveal that despite their high correlation with human judgments, CLIPScore, UMIC, and PAC-S struggle to identify fine-grained errors. While all metrics exhibit strong sensitivity to visual grounding errors, their sensitivity to caption implausibility errors is limited. Furthermore, we found that all metrics are sensitive to variations in the size of image-relevant objects mentioned in the caption, while CLIPScore and PAC-S are also sensitive to the number of mentions of image-relevant objects in the caption. Regarding linguistic aspects of a caption, all metrics show weak comprehension of negat",
    "path": "papers/23/05/2305.14998.json",
    "total_tokens": 955,
    "translated_title": "无参考图像标题评估指标的鲁棒性研究",
    "translated_abstract": "最近，提出了一些无参考指标，如CLIPScore（Hessel等，2021），UMIC（Lee等，2021）和PAC-S（Sarto等，2023），用于自动无参考评估图像标题。我们的研究重点在于评估这些指标在需要区分具有高词汇重叠但含义差异很大的两个标题的情况下的鲁棒性。我们的研究结果显示，尽管这些指标与人类判断具有很高的相关性，但CLIPScore、UMIC和PAC-S很难识别细粒度错误。虽然所有指标对视觉错误敏感，但对标题不合理性错误的敏感性有限。此外，我们还发现所有指标对标题中提及的与图像相关的对象的大小变化敏感，而CLIPScore和PAC-S对标题中提及的与图像相关的对象的数量也敏感。关于标题的语言方面，所有指标对否定意义的理解能力较弱。",
    "tldr": "研究评估了无参考图像标题评估指标在高词汇重叠但含义差异很大的情况下的鲁棒性，结果发现尽管这些指标与人类判断相关性较高，但对细粒度错误识别困难，并且在标题不合理性错误、图像相关对象大小变化以及标题对否定意义的理解方面存在敏感性差异。",
    "en_tdlr": "This study examined the robustness of reference-free image captioning evaluation metrics in scenarios with high lexical overlap but different meanings. The results show that despite their high correlation with human judgments, these metrics struggle to identify fine-grained errors and exhibit sensitivity differences in caption implausibility, object size variations, and comprehension of negation in captions."
}