{
    "title": "BCQQ: Batch-Constraint Quantum Q-Learning with Cyclic Data Re-uploading",
    "abstract": "arXiv:2305.00905v2 Announce Type: replace-cross  Abstract: Deep reinforcement learning (DRL) often requires a large number of data and environment interactions, making the training process time-consuming. This challenge is further exacerbated in the case of batch RL, where the agent is trained solely on a pre-collected dataset without environment interactions. Recent advancements in quantum computing suggest that quantum models might require less data for training compared to classical methods. In this paper, we investigate this potential advantage by proposing a batch RL algorithm that utilizes VQC as function approximators within the discrete batch-constraint deep Q-learning (BCQ) algorithm. Additionally, we introduce a novel data re-uploading scheme by cyclically shifting the order of input variables in the data encoding layers. We evaluate the efficiency of our algorithm on the OpenAI CartPole environment and compare its performance to the classical neural network-based discrete BC",
    "link": "https://arxiv.org/abs/2305.00905",
    "context": "Title: BCQQ: Batch-Constraint Quantum Q-Learning with Cyclic Data Re-uploading\nAbstract: arXiv:2305.00905v2 Announce Type: replace-cross  Abstract: Deep reinforcement learning (DRL) often requires a large number of data and environment interactions, making the training process time-consuming. This challenge is further exacerbated in the case of batch RL, where the agent is trained solely on a pre-collected dataset without environment interactions. Recent advancements in quantum computing suggest that quantum models might require less data for training compared to classical methods. In this paper, we investigate this potential advantage by proposing a batch RL algorithm that utilizes VQC as function approximators within the discrete batch-constraint deep Q-learning (BCQ) algorithm. Additionally, we introduce a novel data re-uploading scheme by cyclically shifting the order of input variables in the data encoding layers. We evaluate the efficiency of our algorithm on the OpenAI CartPole environment and compare its performance to the classical neural network-based discrete BC",
    "path": "papers/23/05/2305.00905.json",
    "total_tokens": 854,
    "translated_title": "BCQQ: 循环数据重新上传的批量约束量子 Q 学习",
    "translated_abstract": "深度强化学习(DRL)通常需要大量的数据和环境交互，使得训练过程耗时。这一挑战在批量RL的情况下进一步恶化，其中代理只在一个预先收集的数据集上进行训练，而不涉及环境交互。量子计算的最新进展表明，与经典方法相比，量子模型可能需要更少的数据进行训练。本文通过提出一个利用VQC作为函数逼近器的批量RL算法，在离散批量约束深度 Q 学习(BCQ)算法中探讨了这一潜在优势。此外，我们通过循环移位数据编码层中输入变量的顺序，引入了一种新颖的数据重新上传方案。我们在OpenAI CartPole环境中评估了我们算法的效率，并将其性能与基于经典神经网络的离散BC进行了比较。",
    "tldr": "本文提出了一种 BCQ 算法，将 VQC 作为函数逼近器，并通过循环移位数据编码层中输入变量的顺序来进行全新的数据重新上传方案，以提高批量 RL 的效率。",
    "en_tdlr": "This paper presents a BCQ algorithm that uses VQC as function approximators and introduces a novel data re-uploading scheme by cyclically shifting the order of input variables in the data encoding layers, aiming to improve the efficiency of batch RL."
}