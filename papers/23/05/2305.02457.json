{
    "title": "Quantifying the Dissimilarity of Texts. (arXiv:2305.02457v1 [cs.CL])",
    "abstract": "Quantifying the dissimilarity of two texts is an important aspect of a number of natural language processing tasks, including semantic information retrieval, topic classification, and document clustering. In this paper, we compared the properties and performance of different dissimilarity measures $D$ using three different representations of texts -- vocabularies, word frequency distributions, and vector embeddings -- and three simple tasks -- clustering texts by author, subject, and time period. Using the Project Gutenberg database, we found that the generalised Jensen--Shannon divergence applied to word frequencies performed strongly across all tasks, that $D$'s based on vector embedding representations led to stronger performance for smaller texts, and that the optimal choice of approach was ultimately task-dependent. We also investigated, both analytically and numerically, the behaviour of the different $D$'s when the two texts varied in length by a factor $h$. We demonstrated that",
    "link": "http://arxiv.org/abs/2305.02457",
    "context": "Title: Quantifying the Dissimilarity of Texts. (arXiv:2305.02457v1 [cs.CL])\nAbstract: Quantifying the dissimilarity of two texts is an important aspect of a number of natural language processing tasks, including semantic information retrieval, topic classification, and document clustering. In this paper, we compared the properties and performance of different dissimilarity measures $D$ using three different representations of texts -- vocabularies, word frequency distributions, and vector embeddings -- and three simple tasks -- clustering texts by author, subject, and time period. Using the Project Gutenberg database, we found that the generalised Jensen--Shannon divergence applied to word frequencies performed strongly across all tasks, that $D$'s based on vector embedding representations led to stronger performance for smaller texts, and that the optimal choice of approach was ultimately task-dependent. We also investigated, both analytically and numerically, the behaviour of the different $D$'s when the two texts varied in length by a factor $h$. We demonstrated that",
    "path": "papers/23/05/2305.02457.json",
    "total_tokens": 816,
    "translated_title": "量化文本差异",
    "translated_abstract": "量化两个文本的差异是自然语言处理任务的重要方面，包括语义信息检索、主题分类和文档聚类等。本文比较了使用三种不同的文本表示方式（词汇、词频分布和向量嵌入）和三个简单任务（按作者、主题和时间段对文本进行聚类）的不同差异度量的属性和性能。通过使用Project Gutenberg数据库, 我们发现基于单词频率的广义Jensen-Shannon分歧在所有任务中表现良好，基于向量嵌入表示的D导致小文本性能更强，而最佳方法的选择最终取决于任务。我们还在理论上和数值上研究了两个文本长度因子h变化时，不同D的行为。我们证明了...",
    "tldr": "本文比较了使用三种不同的文本表示方式和三个不同的聚类任务的性能，并发现基于单词频率的广义Jensen-Shannon分歧在所有任务中表现良好。最佳方法的选择最终取决于任务。",
    "en_tdlr": "This paper compares the properties and performance of different dissimilarity measures using three different representations of texts and three simple tasks. It found that the generalised Jensen-Shannon divergence applied to word frequencies performed strongly across all tasks and the optimal choice of approach ultimately depends on the task."
}