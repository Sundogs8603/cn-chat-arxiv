{
    "title": "Eliciting the Translation Ability of Large Language Models via Multilingual Finetuning with Translation Instructions. (arXiv:2305.15083v2 [cs.CL] UPDATED)",
    "abstract": "Large-scale Pretrained Language Models (LLMs), such as ChatGPT and GPT4, have shown strong abilities in multilingual translations, without being explicitly trained on parallel corpora. It is interesting how the LLMs obtain their ability to carry out translation instructions for different languages. In this paper, we present a detailed analysis by finetuning a multilingual pretrained language model, XGLM-7B, to perform multilingual translation following given instructions. Firstly, we show that multilingual LLMs have stronger translation abilities than previously demonstrated. For a certain language, the performance depends on its similarity to English and the amount of data used in the pretraining phase. Secondly, we find that LLMs' ability to carry out translation instructions relies on the understanding of translation instructions and the alignment among different languages. With multilingual finetuning, LLMs could learn to perform the translation task well even for those language pa",
    "link": "http://arxiv.org/abs/2305.15083",
    "context": "Title: Eliciting the Translation Ability of Large Language Models via Multilingual Finetuning with Translation Instructions. (arXiv:2305.15083v2 [cs.CL] UPDATED)\nAbstract: Large-scale Pretrained Language Models (LLMs), such as ChatGPT and GPT4, have shown strong abilities in multilingual translations, without being explicitly trained on parallel corpora. It is interesting how the LLMs obtain their ability to carry out translation instructions for different languages. In this paper, we present a detailed analysis by finetuning a multilingual pretrained language model, XGLM-7B, to perform multilingual translation following given instructions. Firstly, we show that multilingual LLMs have stronger translation abilities than previously demonstrated. For a certain language, the performance depends on its similarity to English and the amount of data used in the pretraining phase. Secondly, we find that LLMs' ability to carry out translation instructions relies on the understanding of translation instructions and the alignment among different languages. With multilingual finetuning, LLMs could learn to perform the translation task well even for those language pa",
    "path": "papers/23/05/2305.15083.json",
    "total_tokens": 1030,
    "translated_title": "通过多语言微调和翻译指令诱发大规模语言模型的翻译能力",
    "translated_abstract": "大规模预训练语言模型（LLMs），例如ChatGPT和GPT4，展现出在多语言翻译方面的强大能力，而无需明确训练并行语料库。本文研究了LLMs如何获得其对不同语言进行翻译指令的能力。我们通过对多语言预训练语言模型XGLM-7B进行微调来执行多语言翻译任务，并进行了详细分析。首先，我们展示了多语言LLMs具有比先前展示的更强的翻译能力。对于某种语言，其表现取决于其与英语的相似性和预训练阶段使用的数据量。其次，我们发现LLMs执行翻译指令的能力依赖于对翻译指令的理解以及不同语言之间的对齐。通过多语言微调，LLMs能够学习并在翻译任务中表现出良好的能力，即使对于那些语言间平行语料较少的情况。",
    "tldr": "本文通过对多语言预训练语言模型进行微调，研究了它们如何通过翻译指令执行多语言翻译任务。研究发现多语言LLMs具有较强的翻译能力，这取决于语言与英语的相似性和预训练阶段使用的数据量。此外，执行翻译指令的能力依赖于对指令的理解和不同语言之间的对齐。",
    "en_tdlr": "This paper investigates the translation ability of large language models (LLMs) through multilingual finetuning and translation instructions. The study reveals that multilingual LLMs have strong translation abilities, dependent on language similarity to English and the data used during pretraining. Furthermore, the ability to carry out translation instructions relies on understanding the instructions and alignment among different languages."
}