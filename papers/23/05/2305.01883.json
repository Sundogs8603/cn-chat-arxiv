{
    "title": "A Lightweight CNN-Transformer Model for Learning Traveling Salesman Problems. (arXiv:2305.01883v1 [cs.LG])",
    "abstract": "Transformer-based models show state-of-the-art performance even for large-scale Traveling Salesman Problems (TSPs). However, they are based on fully-connected attention models and suffer from large computational complexity and GPU memory usage. We propose a lightweight CNN-Transformer model based on a CNN embedding layer and partial self-attention. Our CNN-Transformer model is able to better learn spatial features from input data using a CNN embedding layer compared with the standard Transformer models. It also removes considerable redundancy in fully connected attention models using the proposed partial self-attention. Experiments show that the proposed model outperforms other state-of-the-art Transformer-based models in terms of TSP solution quality, GPU memory usage, and inference time. Our model consumes approximately 20% less GPU memory usage and has 45% faster inference time compared with other state-of-the-art Transformer-based models. Our code is publicly available at https://g",
    "link": "http://arxiv.org/abs/2305.01883",
    "context": "Title: A Lightweight CNN-Transformer Model for Learning Traveling Salesman Problems. (arXiv:2305.01883v1 [cs.LG])\nAbstract: Transformer-based models show state-of-the-art performance even for large-scale Traveling Salesman Problems (TSPs). However, they are based on fully-connected attention models and suffer from large computational complexity and GPU memory usage. We propose a lightweight CNN-Transformer model based on a CNN embedding layer and partial self-attention. Our CNN-Transformer model is able to better learn spatial features from input data using a CNN embedding layer compared with the standard Transformer models. It also removes considerable redundancy in fully connected attention models using the proposed partial self-attention. Experiments show that the proposed model outperforms other state-of-the-art Transformer-based models in terms of TSP solution quality, GPU memory usage, and inference time. Our model consumes approximately 20% less GPU memory usage and has 45% faster inference time compared with other state-of-the-art Transformer-based models. Our code is publicly available at https://g",
    "path": "papers/23/05/2305.01883.json",
    "total_tokens": 902,
    "translated_title": "一种轻量级CNN-Transformer模型用于学习旅行商问题",
    "translated_abstract": "基于Transformer的模型即使在大规模旅行商问题（TSPs）中也表现出最先进的性能。然而，它们基于全连接的注意模型，存在大量的计算复杂性和GPU内存使用。我们提出了一种基于CNN嵌入层和部分自注意力的轻量级CNN-Transformer模型。与标准Transformer模型相比，我们的CNN-Transformer模型能够更好地学习输入数据中的空间特征。它还使用提出的部分自注意力消除了全连接注意模型中的相当数量的冗余。实验表明，与其他最先进的基于Transformer的模型相比，所提出的模型在TSP解决方案质量、GPU内存使用和推理时间方面都表现出更好的性能。我们的模型使用GPU内存约少20％，推理时间比其他最先进的基于Transformer的模型快45％。我们的代码公开可用，网址为https://g",
    "tldr": "本文提出了一种轻量级CNN-Transformer模型，使用CNN嵌入层和部分自注意力，能更好学习输入数据中的空间特征并消除冗余。实验表明该模型在解决旅行商问题方面表现出更好的性能，例如TSP解决方案质量和GPU内存使用方面。",
    "en_tdlr": "This paper proposes a lightweight CNN-Transformer model with a CNN embedding layer and partial self-attention to better learn spatial features and eliminate redundancy. The model outperforms state-of-the-art Transformer-based models in solving traveling salesman problems in terms of solution quality, GPU memory usage, and inference time."
}