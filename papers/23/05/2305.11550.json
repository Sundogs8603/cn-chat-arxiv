{
    "title": "Viewing Knowledge Transfer in Multilingual Machine Translation Through a Representational Lens. (arXiv:2305.11550v1 [cs.CL])",
    "abstract": "We argue that translation quality alone is not a sufficient metric for measuring knowledge transfer in multilingual neural machine translation. To support this claim, we introduce Representational Transfer Potential (RTP), which measures representational similarities between languages. We show that RTP can measure both positive and negative transfer (interference), and find that RTP is strongly correlated with changes in translation quality, indicating that transfer does occur. Furthermore, we investigate data and language characteristics that are relevant for transfer, and find that multi-parallel overlap is an important yet under-explored feature. Based on this, we develop a novel training scheme, which uses an auxiliary similarity loss that encourages representations to be more invariant across languages by taking advantage of multi-parallel data. We show that our method yields increased translation quality for low- and mid-resource languages across multiple data and model setups.",
    "link": "http://arxiv.org/abs/2305.11550",
    "context": "Title: Viewing Knowledge Transfer in Multilingual Machine Translation Through a Representational Lens. (arXiv:2305.11550v1 [cs.CL])\nAbstract: We argue that translation quality alone is not a sufficient metric for measuring knowledge transfer in multilingual neural machine translation. To support this claim, we introduce Representational Transfer Potential (RTP), which measures representational similarities between languages. We show that RTP can measure both positive and negative transfer (interference), and find that RTP is strongly correlated with changes in translation quality, indicating that transfer does occur. Furthermore, we investigate data and language characteristics that are relevant for transfer, and find that multi-parallel overlap is an important yet under-explored feature. Based on this, we develop a novel training scheme, which uses an auxiliary similarity loss that encourages representations to be more invariant across languages by taking advantage of multi-parallel data. We show that our method yields increased translation quality for low- and mid-resource languages across multiple data and model setups.",
    "path": "papers/23/05/2305.11550.json",
    "total_tokens": 945,
    "translated_title": "透过表征镜头看待多语言机器翻译中的知识转移",
    "translated_abstract": "我们认为，仅仅依靠翻译质量度量多语言神经机器翻译中的知识转移是不够的。为了支持这一观点，我们引入了表征转移潜力（RTP），它测量语言之间的表征相似度。我们展示了RTP可以测量正向和负向转移（干扰），并发现RTP与翻译质量变化强相关，表明确实存在转移。此外，我们研究了与转移相关的数据和语言特征，并发现多路并行重叠是一个重要但鲜有探索的特征。基于此，我们开发了一种新的训练方案，该方案使用辅助相似性损失，通过利用多路并行数据，鼓励表征在语言之间更具不变性。我们表明，我们的方法在多个数据和模型设置中提高了低资源和中资源语言的翻译质量。",
    "tldr": "该论文引入了表征转移潜力（RTP）来衡量多语言神经机器翻译中的知识转移，发现多路并行重叠是关键特征，提出了一种新的训练方案，鼓励表征在语言之间更具不变性，并在多个数据和模型设置中提高了低资源和中资源语言的翻译质量。",
    "en_tdlr": "The paper introduces Representational Transfer Potential (RTP) to measure knowledge transfer in multilingual neural machine translation, identifying multi-parallel overlap as a key feature, and proposes a new training scheme that encourages representation invariance across languages and improves translation quality for low- and mid-resource languages across multiple data and model setups."
}