{
    "title": "Surgical-VQLA: Transformer with Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery. (arXiv:2305.11692v1 [cs.CV])",
    "abstract": "Despite the availability of computer-aided simulators and recorded videos of surgical procedures, junior residents still heavily rely on experts to answer their queries. However, expert surgeons are often overloaded with clinical and academic workloads and limit their time in answering. For this purpose, we develop a surgical question-answering system to facilitate robot-assisted surgical scene and activity understanding from recorded videos. Most of the existing VQA methods require an object detector and regions based feature extractor to extract visual features and fuse them with the embedded text of the question for answer generation. However, (1) surgical object detection model is scarce due to smaller datasets and lack of bounding box annotation; (2) current fusion strategy of heterogeneous modalities like text and image is naive; (3) the localized answering is missing, which is crucial in complex surgical scenarios. In this paper, we propose Visual Question Localized-Answering in",
    "link": "http://arxiv.org/abs/2305.11692",
    "context": "Title: Surgical-VQLA: Transformer with Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery. (arXiv:2305.11692v1 [cs.CV])\nAbstract: Despite the availability of computer-aided simulators and recorded videos of surgical procedures, junior residents still heavily rely on experts to answer their queries. However, expert surgeons are often overloaded with clinical and academic workloads and limit their time in answering. For this purpose, we develop a surgical question-answering system to facilitate robot-assisted surgical scene and activity understanding from recorded videos. Most of the existing VQA methods require an object detector and regions based feature extractor to extract visual features and fuse them with the embedded text of the question for answer generation. However, (1) surgical object detection model is scarce due to smaller datasets and lack of bounding box annotation; (2) current fusion strategy of heterogeneous modalities like text and image is naive; (3) the localized answering is missing, which is crucial in complex surgical scenarios. In this paper, we propose Visual Question Localized-Answering in",
    "path": "papers/23/05/2305.11692.json",
    "total_tokens": 950,
    "translated_title": "带有门控视觉-语言嵌入的Transformer用于机器人手术中的视觉问答",
    "translated_abstract": "尽管存在着计算机辅助模拟器和手术过程的录制视频，但初级住院医师仍然严重依赖专家来回答他们的问题。然而，专家外科医生通常承担着临床和学术工作，限制了他们回答问题的时间。为此，我们开发了一种手术问答系统，以便从录制的视频中促进机器人辅助手术场景和活动理解。本文提出了一种将Transformer模型与门控视觉-语言嵌入相结合的机器人手术视觉问答方法，解决了手术对象检测模型稀缺、异构模态融合策略不足、缺失定位答案等问题，并在基准手术VQA数据集上实现了最先进的性能。",
    "tldr": "本文提出了Surgical-VQLA方法，结合Transformer模型和门控视觉-语言嵌入，解决了手术VQA中对象检测稀缺、异构模态融合策略不足、定位答案缺失等问题，并在测试中实现了最好的表现。"
}