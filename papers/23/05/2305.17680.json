{
    "title": "Evaluating GPT-3 Generated Explanations for Hateful Content Moderation. (arXiv:2305.17680v2 [cs.CL] UPDATED)",
    "abstract": "Recent research has focused on using large language models (LLMs) to generate explanations for hate speech through fine-tuning or prompting. Despite the growing interest in this area, these generated explanations' effectiveness and potential limitations remain poorly understood. A key concern is that these explanations, generated by LLMs, may lead to erroneous judgments about the nature of flagged content by both users and content moderators. For instance, an LLM-generated explanation might inaccurately convince a content moderator that a benign piece of content is hateful. In light of this, we propose an analytical framework for examining hate speech explanations and conducted an extensive survey on evaluating such explanations. Specifically, we prompted GPT-3 to generate explanations for both hateful and non-hateful content, and a survey was conducted with 2,400 unique respondents to evaluate the generated explanations. Our findings reveal that (1) human evaluators rated the GPT-gene",
    "link": "http://arxiv.org/abs/2305.17680",
    "context": "Title: Evaluating GPT-3 Generated Explanations for Hateful Content Moderation. (arXiv:2305.17680v2 [cs.CL] UPDATED)\nAbstract: Recent research has focused on using large language models (LLMs) to generate explanations for hate speech through fine-tuning or prompting. Despite the growing interest in this area, these generated explanations' effectiveness and potential limitations remain poorly understood. A key concern is that these explanations, generated by LLMs, may lead to erroneous judgments about the nature of flagged content by both users and content moderators. For instance, an LLM-generated explanation might inaccurately convince a content moderator that a benign piece of content is hateful. In light of this, we propose an analytical framework for examining hate speech explanations and conducted an extensive survey on evaluating such explanations. Specifically, we prompted GPT-3 to generate explanations for both hateful and non-hateful content, and a survey was conducted with 2,400 unique respondents to evaluate the generated explanations. Our findings reveal that (1) human evaluators rated the GPT-gene",
    "path": "papers/23/05/2305.17680.json",
    "total_tokens": 937,
    "translated_title": "评估GPT-3生成的仇恨内容审核解释",
    "translated_abstract": "最近的研究聚焦于使用基于大型语言模型（LLMs）的Fine-tune或提示生成仇恨言论的解释。尽管这个领域越来越受关注，但这些生成解释的有效性和潜在限制仍然不为人们所了解。一个关键问题是，由LLMs生成的这些解释可能会导致用户和内容审核员对标记内容本质做出错误判断。我们提出一个分析框架来检查仇恨言论解释，并进行了一个广泛的调查来评估这些解释。我们在GPT-3上输入仇恨和非仇恨内容，发现受调查者在人工审核GPT生成的解释时，将仇恨言论解释评价为不够准确和有用。",
    "tldr": "本文通过调查和分析，评估了使用GPT-3生成的针对仇恨内容的解释是否准确和有用。结果显示，GPT-3生成的解释普遍存在过于模糊、聚焦不当等缺点，同时也存在不同类型仇恨言论生成的解释质量差异大的问题。",
    "en_tdlr": "This paper evaluates the accuracy and usefulness of explanations for hate speech generated by GPT-3 through a survey, finding that the explanations are often vague or focus on irrelevant aspects, and there are notable differences in the quality of explanations generated for specific categories of hate speech."
}