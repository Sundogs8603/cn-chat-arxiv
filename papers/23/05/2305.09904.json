{
    "title": "On the ISS Property of the Gradient Flow for Single Hidden-Layer Neural Networks with Linear Activations. (arXiv:2305.09904v1 [cs.LG])",
    "abstract": "Recent research in neural networks and machine learning suggests that using many more parameters than strictly required by the initial complexity of a regression problem can result in more accurate or faster-converging models -contrary to classical statistical belief. This phenomenon, sometimes known as ``benign overfitting'', raises questions regarding in what other ways might overparameterization affect the properties of a learning problem. In this work, we investigate the effects of overfitting on the robustness of gradient-descent training when subject to uncertainty on the gradient estimation. This uncertainty arises naturally if the gradient is estimated from noisy data or directly measured. Our object of study is a linear neural network with a single, arbitrarily wide, hidden layer and an arbitrary number of inputs and outputs. In this paper we solve the problem for the case where the input and output of our neural-network are one-dimensional, deriving sufficient conditions fo",
    "link": "http://arxiv.org/abs/2305.09904",
    "context": "Title: On the ISS Property of the Gradient Flow for Single Hidden-Layer Neural Networks with Linear Activations. (arXiv:2305.09904v1 [cs.LG])\nAbstract: Recent research in neural networks and machine learning suggests that using many more parameters than strictly required by the initial complexity of a regression problem can result in more accurate or faster-converging models -contrary to classical statistical belief. This phenomenon, sometimes known as ``benign overfitting'', raises questions regarding in what other ways might overparameterization affect the properties of a learning problem. In this work, we investigate the effects of overfitting on the robustness of gradient-descent training when subject to uncertainty on the gradient estimation. This uncertainty arises naturally if the gradient is estimated from noisy data or directly measured. Our object of study is a linear neural network with a single, arbitrarily wide, hidden layer and an arbitrary number of inputs and outputs. In this paper we solve the problem for the case where the input and output of our neural-network are one-dimensional, deriving sufficient conditions fo",
    "path": "papers/23/05/2305.09904.json",
    "total_tokens": 883,
    "translated_title": "单隐层神经网络梯度流的ISS属性研究",
    "translated_abstract": "近期神经网络和机器学习的研究表明，使用比初始回归问题所需参数更多的参数可以导致更准确或更快收敛的模型-与经典统计学的信念相反。这种现象有时被称为“良性过拟合”，它引发了对过度参数化可能如何影响学习问题属性的问题。本文研究了过拟合对梯度下降训练稳健性的影响，当梯度估计不确定时，会自然产生这种不确定性，这种不确定性会由于从噪声数据或直接测量的梯度估计而产生。我们研究了一个具有单个任意宽度的隐藏层和任意数量的输入和输出的线性神经网络。本文解决了输入和输出为一维的情况，导出了足够的条件。",
    "tldr": "过拟合如何影响梯度下降训练在梯度估计不确定时的稳健性，该研究研究了具有单个任意宽度的隐藏层和任意输入输出数量的线性神经网络。",
    "en_tdlr": "The study investigates how overfitting affects the robustness of gradient descent training when subject to uncertainty on the gradient estimation, focusing on a linear neural network with a single, arbitrarily wide, hidden layer and an arbitrary number of inputs and outputs, solving the problem for the case where the input and output of the neural-network are one-dimensional."
}