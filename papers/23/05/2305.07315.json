{
    "title": "$\\partial\\mathbb{B}$ nets: learning discrete functions by gradient descent. (arXiv:2305.07315v1 [cs.LG])",
    "abstract": "$\\partial\\mathbb{B}$ nets are differentiable neural networks that learn discrete boolean-valued functions by gradient descent. $\\partial\\mathbb{B}$ nets have two semantically equivalent aspects: a differentiable soft-net, with real weights, and a non-differentiable hard-net, with boolean weights. We train the soft-net by backpropagation and then `harden' the learned weights to yield boolean weights that bind with the hard-net. The result is a learned discrete function. `Hardening' involves no loss of accuracy, unlike existing approaches to neural network binarization. Preliminary experiments demonstrate that $\\partial\\mathbb{B}$ nets achieve comparable performance on standard machine learning problems yet are compact (due to 1-bit weights) and interpretable (due to the logical nature of the learnt functions).",
    "link": "http://arxiv.org/abs/2305.07315",
    "context": "Title: $\\partial\\mathbb{B}$ nets: learning discrete functions by gradient descent. (arXiv:2305.07315v1 [cs.LG])\nAbstract: $\\partial\\mathbb{B}$ nets are differentiable neural networks that learn discrete boolean-valued functions by gradient descent. $\\partial\\mathbb{B}$ nets have two semantically equivalent aspects: a differentiable soft-net, with real weights, and a non-differentiable hard-net, with boolean weights. We train the soft-net by backpropagation and then `harden' the learned weights to yield boolean weights that bind with the hard-net. The result is a learned discrete function. `Hardening' involves no loss of accuracy, unlike existing approaches to neural network binarization. Preliminary experiments demonstrate that $\\partial\\mathbb{B}$ nets achieve comparable performance on standard machine learning problems yet are compact (due to 1-bit weights) and interpretable (due to the logical nature of the learnt functions).",
    "path": "papers/23/05/2305.07315.json",
    "total_tokens": 886,
    "translated_title": "$\\partial\\mathbb{B}$ 神经网络：通过梯度下降学习离散函数",
    "translated_abstract": "$\\partial\\mathbb{B}$ 神经网络是一种可以通过梯度下降学习离散布尔型函数的可微分神经网络。$\\partial\\mathbb{B}$ 网络有两个语义上等效的方面：可微分的软网络和布尔权重的不可微硬网络。我们通过反向传播训练软网络，然后“硬化”学习到的权重，以获得与硬网络相结合的布尔权重，从而得到一个离散函数。与现有的神经网络二值化方法不同，硬化不会导致精度损失。初步实验表明，$\\partial\\mathbb{B}$ 网络在标准机器学习问题上实现了可比较的性能，同时具有紧凑性（由于 1 位权重）和可解释性（由于学习到函数的逻辑性质）。",
    "tldr": "研究人员提出了一种通过梯度下降学习离散函数的可微分神经网络 $\\partial\\mathbb{B}$ 网络，它通过软网络和硬网络相结合的方式实现学习到的离散函数具有可解释性和较高的精度。",
    "en_tdlr": "The researchers propose a differentiable neural network, called $\\partial\\mathbb{B}$ nets, that can learn discrete boolean-valued functions by gradient descent. By combining a differentiable soft-net and a non-differentiable hard-net with boolean weights, $\\partial\\mathbb{B}$ nets achieve comparable performance on standard machine learning problems and are compact and interpretable."
}