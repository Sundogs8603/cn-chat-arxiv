{
    "title": "CompoundPiece: Evaluating and Improving Decompounding Performance of Language Models. (arXiv:2305.14214v2 [cs.CL] UPDATED)",
    "abstract": "While many languages possess processes of joining two or more words to create compound words, previous studies have been typically limited only to languages with excessively productive compound formation (e.g., German, Dutch) and there is no public dataset containing compound and non-compound words across a large number of languages. In this work, we systematically study decompounding, the task of splitting compound words into their constituents, at a wide scale. We first address the data gap by introducing a dataset of 255k compound and non-compound words across 56 diverse languages obtained from Wiktionary. We then use this dataset to evaluate an array of Large Language Models (LLMs) on the decompounding task. We find that LLMs perform poorly, especially on words which are tokenized unfavorably by subword tokenization. We thus introduce a novel methodology to train dedicated models for decompounding. The proposed two-stage procedure relies on a fully self-supervised objective in the ",
    "link": "http://arxiv.org/abs/2305.14214",
    "context": "Title: CompoundPiece: Evaluating and Improving Decompounding Performance of Language Models. (arXiv:2305.14214v2 [cs.CL] UPDATED)\nAbstract: While many languages possess processes of joining two or more words to create compound words, previous studies have been typically limited only to languages with excessively productive compound formation (e.g., German, Dutch) and there is no public dataset containing compound and non-compound words across a large number of languages. In this work, we systematically study decompounding, the task of splitting compound words into their constituents, at a wide scale. We first address the data gap by introducing a dataset of 255k compound and non-compound words across 56 diverse languages obtained from Wiktionary. We then use this dataset to evaluate an array of Large Language Models (LLMs) on the decompounding task. We find that LLMs perform poorly, especially on words which are tokenized unfavorably by subword tokenization. We thus introduce a novel methodology to train dedicated models for decompounding. The proposed two-stage procedure relies on a fully self-supervised objective in the ",
    "path": "papers/23/05/2305.14214.json",
    "total_tokens": 999,
    "translated_title": "CompoundPiece：评估和改进语言模型的复合分词性能",
    "translated_abstract": "尽管许多语言都具有将两个或多个单词结合成复合词的过程，但以往的研究通常仅限于具有过度复合形成能力的语言（如德语、荷兰语），并且没有包含大量语言中复合和非复合词的公共数据集。在本研究中，我们系统地研究了广泛的复合分词任务，即将复合词拆分为其组成部分。我们首先通过介绍一个包含来自Wiktionary的255k个复合和非复合词的跨56种不同语言的数据集来解决数据缺口。然后，我们使用该数据集评估了一系列大型语言模型（LLMs）在复合分词任务上的表现。我们发现LLMs的性能较差，尤其是对于通过子词分词方式进行标记的单词。因此，我们引入了一种新的方法来训练专门用于复合分词的模型。所提出的两阶段方法依赖于完全自我监督的目标。",
    "tldr": "本研究介绍了一个跨56种不同语言的大规模数据集，用于评估语言模型在复合分词任务上的表现。研究发现，现有的语言模型在复合分词上表现不佳，特别是对于通过子词分词方式进行标记的单词。因此，研究还提出了一种新的方法，通过训练专门的模型来改进复合分词性能。",
    "en_tdlr": "This study introduces a large-scale dataset across 56 diverse languages to evaluate the performance of language models on the task of decompounding. The study finds that existing models perform poorly on decompounding, particularly on words tokenized unfavorably. Therefore, a novel approach is proposed to train dedicated models for improving decompounding performance."
}