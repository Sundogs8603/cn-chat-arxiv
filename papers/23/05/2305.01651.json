{
    "title": "Can LMs Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledge. (arXiv:2305.01651v1 [cs.CL])",
    "abstract": "Pre-trained language models (LMs) are used for knowledge intensive tasks like question answering, but their knowledge gets continuously outdated as the world changes. Prior work has studied targeted updates to LMs, injecting individual facts and evaluating whether the model learns these facts while not changing predictions on other contexts. We take a step forward and study LMs' abilities to make inferences based on injected facts (or propagate those facts): for example, after learning that something is a TV show, does an LM predict that you can watch it? We study this with two cloze-style tasks: an existing dataset of real-world sentences about novel entities (ECBD) as well as a new controlled benchmark with manually designed templates requiring varying levels of inference about injected knowledge. Surprisingly, we find that existing methods for updating knowledge (gradient-based fine-tuning and modifications of this approach) show little propagation of injected knowledge. These metho",
    "link": "http://arxiv.org/abs/2305.01651",
    "context": "Title: Can LMs Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledge. (arXiv:2305.01651v1 [cs.CL])\nAbstract: Pre-trained language models (LMs) are used for knowledge intensive tasks like question answering, but their knowledge gets continuously outdated as the world changes. Prior work has studied targeted updates to LMs, injecting individual facts and evaluating whether the model learns these facts while not changing predictions on other contexts. We take a step forward and study LMs' abilities to make inferences based on injected facts (or propagate those facts): for example, after learning that something is a TV show, does an LM predict that you can watch it? We study this with two cloze-style tasks: an existing dataset of real-world sentences about novel entities (ECBD) as well as a new controlled benchmark with manually designed templates requiring varying levels of inference about injected knowledge. Surprisingly, we find that existing methods for updating knowledge (gradient-based fine-tuning and modifications of this approach) show little propagation of injected knowledge. These metho",
    "path": "papers/23/05/2305.01651.json",
    "total_tokens": 1173,
    "translated_title": "语言模型能够从描述中学习新实体吗？注入知识传播中的挑战",
    "translated_abstract": "预先训练的语言模型已经被用于像问答这样的知识密集型任务，但是随着世界的变化，它们的知识不断过时。先前的工作研究了对语言模型进行有针对性的更新，注入个别的事实并评估模型是否能够学习这些事实，同时不改变其他上下文的预测。本文进一步研究了语言模型基于注入的事实进行推理的能力（或传播这些事实的能力）：例如，在学习了某个东西是电视节目之后，语言模型是否会预测你可以通过它来观看? 我们通过两种填空式任务来研究这一问题：一个是关于新实体的现实世界句子的现有数据集（ECBD），另一个是一个新的受控基准测试，其中手动设计的模板要求注入的知识具有不同程度的推理。令人惊讶的是，我们发现现有的更新知识的方法（基于梯度微调和此方法的修改）在注入知识的传播方面表现不佳。这些方法在学习个别事实方面非常成功，但在从注入知识中进行推理方面却存在困难。我们的研究突显了注入知识传播中的挑战，并建议需要新的技术来使语言模型能够学习和使用关于实体的新信息。",
    "tldr": "语言模型已被用于知识密集型任务，但其知识随着世界变化不断过时，先前的工作注入单个事实成功，但在基于注入的事实进行推理（或传播这些事实）方面表现不佳。这一研究突显了注入知识传播中的挑战，并提出需要新技术使语言模型能够学习和使用实体的新信息。",
    "en_tdlr": "Pre-trained language models have been used for knowledge intensive tasks, but their knowledge becomes outdated as the world changes. Prior work injected individual facts in LMs and evaluated whether the model learned those facts. This study takes a step forward by investigating LMs' abilities to make inferences based on injected facts and highlights the challenges in propagating injected knowledge."
}