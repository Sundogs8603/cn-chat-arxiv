{
    "title": "Learning Safety Constraints from Demonstrations with Unknown Rewards. (arXiv:2305.16147v1 [cs.LG])",
    "abstract": "We propose Convex Constraint Learning for Reinforcement Learning (CoCoRL), a novel approach for inferring shared constraints in a Constrained Markov Decision Process (CMDP) from a set of safe demonstrations with possibly different reward functions. While previous work is limited to demonstrations with known rewards or fully known environment dynamics, CoCoRL can learn constraints from demonstrations with different unknown rewards without knowledge of the environment dynamics. CoCoRL constructs a convex safe set based on demonstrations, which provably guarantees safety even for potentially sub-optimal (but safe) demonstrations. For near-optimal demonstrations, CoCoRL converges to the true safe set with no policy regret. We evaluate CoCoRL in tabular environments and a continuous driving simulation with multiple constraints. CoCoRL learns constraints that lead to safe driving behavior and that can be transferred to different tasks and environments. In contrast, alternative methods based ",
    "link": "http://arxiv.org/abs/2305.16147",
    "context": "Title: Learning Safety Constraints from Demonstrations with Unknown Rewards. (arXiv:2305.16147v1 [cs.LG])\nAbstract: We propose Convex Constraint Learning for Reinforcement Learning (CoCoRL), a novel approach for inferring shared constraints in a Constrained Markov Decision Process (CMDP) from a set of safe demonstrations with possibly different reward functions. While previous work is limited to demonstrations with known rewards or fully known environment dynamics, CoCoRL can learn constraints from demonstrations with different unknown rewards without knowledge of the environment dynamics. CoCoRL constructs a convex safe set based on demonstrations, which provably guarantees safety even for potentially sub-optimal (but safe) demonstrations. For near-optimal demonstrations, CoCoRL converges to the true safe set with no policy regret. We evaluate CoCoRL in tabular environments and a continuous driving simulation with multiple constraints. CoCoRL learns constraints that lead to safe driving behavior and that can be transferred to different tasks and environments. In contrast, alternative methods based ",
    "path": "papers/23/05/2305.16147.json",
    "total_tokens": 946,
    "translated_title": "从未知奖励的演示中学习安全约束",
    "translated_abstract": "本文提出了一种新方法，Convex Constraint Learning for Reinforcement Learning (CoCoRL)，用于从一组已知安全演示中推断Constrained Markov Decision Process (CMDP)的共享约束。与以前的方法只限于已知奖励或完全已知环境动态的演示相比，CoCoRL可以从具有不同未知奖励的演示中学习约束，而无需了解环境动态。CoCoRL基于演示构建了一个凸安全集，即使是潜在的次优演示也能保证安全。对于几乎最优演示，CoCoRL能够无误差收敛于真实的安全集。我们在表格环境和一个包含多个约束的连续驾驶仿真中评估CoCoRL。CoCoRL学习到的限制导致了安全的驾驶行为，并可以转移到不同的任务和环境。与之相反，基于学习已知回报的替代方法无法推广到具有不同回报的新环境，突显了CoCoRL在不知道回报函数的情况下学习约束的重要性。",
    "tldr": "CoCoRL是一种从不知道奖励的已知安全演示中推断约束的方法，可以用于Constrained Markov Decision Process（CMDP），并且对于几乎最优演示能够无误差收敛于真实的安全集。",
    "en_tdlr": "CoCoRL is a method for inferring constraints from known safe demonstrations with unknown rewards, applicable to Constrained Markov Decision Process (CMDP), that can converge to the true safe set with no policy regret for near-optimal demonstrations."
}