{
    "title": "Enriching language models with graph-based context information to better understand textual data. (arXiv:2305.11070v1 [cs.CL])",
    "abstract": "A considerable number of texts encountered daily are somehow connected with each other. For example, Wikipedia articles refer to other articles via hyperlinks, scientific papers relate to others via citations or (co)authors, while tweets relate via users that follow each other or reshare content. Hence, a graph-like structure can represent existing connections and be seen as capturing the \"context\" of the texts. The question thus arises if extracting and integrating such context information into a language model might help facilitate a better automated understanding of the text. In this study, we experimentally demonstrate that incorporating graph-based contextualization into BERT model enhances its performance on an example of a classification task. Specifically, on Pubmed dataset, we observed a reduction in error from 8.51% to 7.96%, while increasing the number of parameters just by 1.6%.  Our source code: https://github.com/tryptofanik/gc-bert",
    "link": "http://arxiv.org/abs/2305.11070",
    "context": "Title: Enriching language models with graph-based context information to better understand textual data. (arXiv:2305.11070v1 [cs.CL])\nAbstract: A considerable number of texts encountered daily are somehow connected with each other. For example, Wikipedia articles refer to other articles via hyperlinks, scientific papers relate to others via citations or (co)authors, while tweets relate via users that follow each other or reshare content. Hence, a graph-like structure can represent existing connections and be seen as capturing the \"context\" of the texts. The question thus arises if extracting and integrating such context information into a language model might help facilitate a better automated understanding of the text. In this study, we experimentally demonstrate that incorporating graph-based contextualization into BERT model enhances its performance on an example of a classification task. Specifically, on Pubmed dataset, we observed a reduction in error from 8.51% to 7.96%, while increasing the number of parameters just by 1.6%.  Our source code: https://github.com/tryptofanik/gc-bert",
    "path": "papers/23/05/2305.11070.json",
    "total_tokens": 845,
    "translated_title": "基于图形上下文信息的语言模型增强以更好地理解文本数据",
    "translated_abstract": "每天遇到的文本具有相互联系的情况相当多。例如，Wikipedia文章通过超链接引用其他文章，科学论文通过引用或（共同）作者与其他论文相关联，而推文则通过关注彼此或转发内容来关联。因此，类似于图形的结构可以表示现有的联系，并被视为捕捉文本的“上下文”。因此，提取和整合这种上下文信息到语言模型中是否有助于更好地自动理解文本？在本研究中，我们实验性地证明，将基于图形的上下文化纳入BERT模型会增强其在分类任务示例上的表现。具体而言，在Pubmed数据集上，我们观察到误差从8.51％降至7.96％，同时仅增加了1.6％的参数量。",
    "tldr": "基于图形上下文信息的语言模型增强可更好的理解文本，实验表明该方法提高了BERT模型在Pubmed上的分类任务表现。",
    "en_tdlr": "Incorporating graph-based contextualization into BERT model enhances its performance on classification task. This method improves the understanding of textual data by enriching language models with context information. Specifically, on Pubmed dataset, the method reduces error from 8.51% to 7.96%, while increasing the number of parameters by only 1.6%."
}