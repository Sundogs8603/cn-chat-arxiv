{
    "title": "Self-Distillation with Meta Learning for Knowledge Graph Completion. (arXiv:2305.12209v1 [cs.CL])",
    "abstract": "In this paper, we propose a selfdistillation framework with meta learning(MetaSD) for knowledge graph completion with dynamic pruning, which aims to learn compressed graph embeddings and tackle the longtail samples. Specifically, we first propose a dynamic pruning technique to obtain a small pruned model from a large source model, where the pruning mask of the pruned model could be updated adaptively per epoch after the model weights are updated. The pruned model is supposed to be more sensitive to difficult to memorize samples(e.g., longtail samples) than the source model. Then, we propose a onestep meta selfdistillation method for distilling comprehensive knowledge from the source model to the pruned model, where the two models coevolve in a dynamic manner during training. In particular, we exploit the performance of the pruned model, which is trained alongside the source model in one iteration, to improve the source models knowledge transfer ability for the next iteration via meta l",
    "link": "http://arxiv.org/abs/2305.12209",
    "context": "Title: Self-Distillation with Meta Learning for Knowledge Graph Completion. (arXiv:2305.12209v1 [cs.CL])\nAbstract: In this paper, we propose a selfdistillation framework with meta learning(MetaSD) for knowledge graph completion with dynamic pruning, which aims to learn compressed graph embeddings and tackle the longtail samples. Specifically, we first propose a dynamic pruning technique to obtain a small pruned model from a large source model, where the pruning mask of the pruned model could be updated adaptively per epoch after the model weights are updated. The pruned model is supposed to be more sensitive to difficult to memorize samples(e.g., longtail samples) than the source model. Then, we propose a onestep meta selfdistillation method for distilling comprehensive knowledge from the source model to the pruned model, where the two models coevolve in a dynamic manner during training. In particular, we exploit the performance of the pruned model, which is trained alongside the source model in one iteration, to improve the source models knowledge transfer ability for the next iteration via meta l",
    "path": "papers/23/05/2305.12209.json",
    "total_tokens": 956,
    "translated_title": "具有元学习的自蒸馏网络用于知识图谱补全",
    "translated_abstract": "本文提出了一个基于动态剪枝的元学习自蒸馏框架(MetaSD)，旨在学习压缩的图嵌入并解决长尾样本问题。具体而言，我们首先提出了一种动态剪枝技术，从一个大源模型中获取一个较小的剪枝模型，其中剪枝掩模可以在模型权重更新后每个时期自适应地更新，剪枝模型应比源模型更敏感于难于记忆的样本（例如，长尾样本）。然后，我们提出了一种单步元自蒸馏方法，以将来自源模型的全面知识蒸馏到剪枝模型中，其中两个模型在训练期间以动态方式共同进化。特别地，我们利用了与源模型同时在一个迭代中训练的剪枝模型的性能来通过元学习提高源模型的知识迁移能力，以进行下一次迭代。实验结果表明，我们提出的框架在多个基准数据集上优于现有的最先进方法。",
    "tldr": "本文提出了一个基于动态剪枝和元学习自蒸馏的框架，用于解决知识图谱补全中长尾样本问题，并在实验中取得了优于现有最先进方法的效果。",
    "en_tdlr": "This paper proposes a framework for knowledge graph completion with dynamic pruning and meta learning self-distillation, which aims to tackle the longtail sample problem. The framework outperforms state-of-the-art methods in experimental results."
}