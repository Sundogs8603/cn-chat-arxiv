{
    "title": "Exploring Lottery Prompts for Pre-trained Language Models. (arXiv:2305.19500v1 [cs.CL])",
    "abstract": "Consistently scaling pre-trained language models (PLMs) imposes substantial burdens on model adaptation, necessitating more efficient alternatives to conventional fine-tuning. Given the advantage of prompting in the zero-shot setting and the observed performance fluctuation among different prompts, we explore the instance-level prompt and their generalizability. By searching through the prompt space, we first validate the assumption that for every instance, there is almost always a lottery prompt that induces the correct prediction from the PLM, and such prompt can be obtained at a low cost thanks to the inherent ability of PLMs. Meanwhile, we find that some strong lottery prompts have high performance over the whole training set, and they are equipped with distinguishable linguistic features. Lastly, we attempt to generalize the searched strong lottery prompts to unseen data with prompt ensembling method without any parameter tuning. Experiments are conducted on various types of NLP c",
    "link": "http://arxiv.org/abs/2305.19500",
    "context": "Title: Exploring Lottery Prompts for Pre-trained Language Models. (arXiv:2305.19500v1 [cs.CL])\nAbstract: Consistently scaling pre-trained language models (PLMs) imposes substantial burdens on model adaptation, necessitating more efficient alternatives to conventional fine-tuning. Given the advantage of prompting in the zero-shot setting and the observed performance fluctuation among different prompts, we explore the instance-level prompt and their generalizability. By searching through the prompt space, we first validate the assumption that for every instance, there is almost always a lottery prompt that induces the correct prediction from the PLM, and such prompt can be obtained at a low cost thanks to the inherent ability of PLMs. Meanwhile, we find that some strong lottery prompts have high performance over the whole training set, and they are equipped with distinguishable linguistic features. Lastly, we attempt to generalize the searched strong lottery prompts to unseen data with prompt ensembling method without any parameter tuning. Experiments are conducted on various types of NLP c",
    "path": "papers/23/05/2305.19500.json",
    "total_tokens": 864,
    "translated_title": "探索用于预训练语言模型的抽奖提示",
    "translated_abstract": "一直以来，对于预训练语言模型（PLMs）的可持续扩展，在模型适应性方面存在重大负担，需要更有效的替代传统的微调方法。 鉴于在零-shot环境中提示的优势和观察到不同提示之间的性能波动，本文探讨了实例级提示及其普适性。 通过在提示空间中进行搜索，我们首先验证了这样一种假设：对于每个实例，几乎总是存在一种抽奖提示，可以以低成本获得PLM的正确预测能力。 同时，我们发现一些强大的抽奖提示在整个训练集上具有高性能，并具备可区分的语言特征。 最后，我们试图使用提示集成方法将搜索到的强大抽奖提示推广到未见数据而无需任何参数调整。 在各种类型的NLP c上进行实验。",
    "tldr": "本文通过对实例级提示及其普适性的探讨，发现了一些强大且具备可区分语言特征的抽奖提示，通过提示集成方法可将其推广到未见数据集上，为替代传统微调方法提供了更有效的选择。"
}