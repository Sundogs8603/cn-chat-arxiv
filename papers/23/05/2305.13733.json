{
    "title": "Self-Critique Prompting with Large Language Models for Inductive Instructions. (arXiv:2305.13733v1 [cs.CL])",
    "abstract": "Numerous works are proposed to improve or evaluate the capabilities of Large language models (LLMs) to fulfill user instructions. However, they neglect the possibility that user inputs may inherently contain incorrect information due to users' false beliefs or malicious intents. In this way, blindly adhering to users' false content will cause deception and harm. To address this problem, we propose a challenging benchmark consisting of Inductive Instructions (INDust) to evaluate whether LLMs could resist these instructions. The INDust includes 15K instructions across three categories: Fact-Checking Instructions, Questions based on False Premises, and Creative Instructions based on False Premises. Our experiments on several strong LLMs reveal that current LLMs can be easily deceived by INDust into generating misleading and malicious statements. Hence we employ Self-Critique prompting to encourage LLMs to not only critique themselves like in previous works but also the users, which show r",
    "link": "http://arxiv.org/abs/2305.13733",
    "context": "Title: Self-Critique Prompting with Large Language Models for Inductive Instructions. (arXiv:2305.13733v1 [cs.CL])\nAbstract: Numerous works are proposed to improve or evaluate the capabilities of Large language models (LLMs) to fulfill user instructions. However, they neglect the possibility that user inputs may inherently contain incorrect information due to users' false beliefs or malicious intents. In this way, blindly adhering to users' false content will cause deception and harm. To address this problem, we propose a challenging benchmark consisting of Inductive Instructions (INDust) to evaluate whether LLMs could resist these instructions. The INDust includes 15K instructions across three categories: Fact-Checking Instructions, Questions based on False Premises, and Creative Instructions based on False Premises. Our experiments on several strong LLMs reveal that current LLMs can be easily deceived by INDust into generating misleading and malicious statements. Hence we employ Self-Critique prompting to encourage LLMs to not only critique themselves like in previous works but also the users, which show r",
    "path": "papers/23/05/2305.13733.json",
    "total_tokens": 988,
    "translated_title": "巨型语言模型的自我批判提示用于归纳教学指导",
    "translated_abstract": "大量的工作都被提出来提高或评估大型语言模型（LLM）实现用户指令的能力。 然而，它们忽略了用户输入可能因用户的错误信念或恶意意图而固有地包含不正确的信息的可能性。 盲目地遵循用户的错误内容将导致欺骗和伤害。 为解决这个问题，我们提出了一个具有挑战性的基准，由归纳指令（INDust）组成，以评估LLMs是否能够抵抗这些指令。 INDust包括三个类别的15K指令：事实核查指令，基于错误前提的问题和基于错误前提的创意指令。 我们对几个强大的LLMs进行的实验表明，当前的LLMs可以轻易地被INDust欺骗，生成误导性和恶意的陈述。 因此，我们采用自我批判提示，以激励LLMs不仅像以前的工作那样对自己进行批评，而且对用户进行批评，它展示出r。",
    "tldr": "本研究提出了一个基准，名为INDust，用于评估大型语言模型（LLMs）对于包含错误信息的指令的抵抗能力。研究发现，当前的LLMs很容易被欺骗，因此采用自我批判提示的方法来激励LLMs不仅对自己进行批评，而且对用户进行批评。",
    "en_tdlr": "The paper proposes a benchmark called INDust to evaluate the resistance of large language models (LLMs) to instructions containing incorrect information. It is found that current LLMs are easily deceived and hence self-critique prompting is used to encourage LLMs to critique not only themselves but also the users."
}