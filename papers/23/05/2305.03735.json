{
    "title": "Stackelberg Games for Learning Emergent Behaviors During Competitive Autocurricula. (arXiv:2305.03735v1 [cs.AI])",
    "abstract": "Autocurricular training is an important sub-area of multi-agent reinforcement learning~(MARL) that allows multiple agents to learn emergent skills in an unsupervised co-evolving scheme. The robotics community has experimented autocurricular training with physically grounded problems, such as robust control and interactive manipulation tasks. However, the asymmetric nature of these tasks makes the generation of sophisticated policies challenging. Indeed, the asymmetry in the environment may implicitly or explicitly provide an advantage to a subset of agents which could, in turn, lead to a low-quality equilibrium. This paper proposes a novel game-theoretic algorithm, Stackelberg Multi-Agent Deep Deterministic Policy Gradient (ST-MADDPG), which formulates a two-player MARL problem as a Stackelberg game with one player as the `leader' and the other as the `follower' in a hierarchical interaction structure wherein the leader has an advantage. We first demonstrate that the leader's advantage",
    "link": "http://arxiv.org/abs/2305.03735",
    "context": "Title: Stackelberg Games for Learning Emergent Behaviors During Competitive Autocurricula. (arXiv:2305.03735v1 [cs.AI])\nAbstract: Autocurricular training is an important sub-area of multi-agent reinforcement learning~(MARL) that allows multiple agents to learn emergent skills in an unsupervised co-evolving scheme. The robotics community has experimented autocurricular training with physically grounded problems, such as robust control and interactive manipulation tasks. However, the asymmetric nature of these tasks makes the generation of sophisticated policies challenging. Indeed, the asymmetry in the environment may implicitly or explicitly provide an advantage to a subset of agents which could, in turn, lead to a low-quality equilibrium. This paper proposes a novel game-theoretic algorithm, Stackelberg Multi-Agent Deep Deterministic Policy Gradient (ST-MADDPG), which formulates a two-player MARL problem as a Stackelberg game with one player as the `leader' and the other as the `follower' in a hierarchical interaction structure wherein the leader has an advantage. We first demonstrate that the leader's advantage",
    "path": "papers/23/05/2305.03735.json",
    "total_tokens": 929,
    "translated_title": "Stackelberg Games用于在竞争学习自适应课程中学习新兴行为",
    "translated_abstract": "自适应课程训练是多智能体强化学习（MARL）的一个重要子领域，它允许多个智能体在无监督的共同演化方案中学习新兴的技能。机器人社区已经在物理立足的问题上尝试了自适应课程训练，例如强健控制和交互式操纵任务。然而，这些任务的不对称性使得生成复杂策略具有挑战性。事实上，环境中的不对称性可能会隐含地或明确地为某些智能体提供优势，并可能导致质量低劣的均衡状态。本文提出了一种新颖的博弈论算法 - Stackelberg深决策策略梯度（ST-MADDPG），将一个双人MARL问题制定为具有层次交互结构的Stackelberg博弈，其中一个玩家是“领导者”，另一个是“跟随者”，领导者具有优势。我们首先展示了领导者的优势",
    "tldr": "本论文提出了一种新颖的博弈论算法，Stackelberg Multi-Agent Deep Deterministic Policy Gradient(ST-MADDPG), 用于在竞争学习自适应课程中学习新兴行为。"
}