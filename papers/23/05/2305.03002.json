{
    "title": "Evaluating Post-hoc Interpretability with Intrinsic Interpretability. (arXiv:2305.03002v1 [cs.CV])",
    "abstract": "Despite Convolutional Neural Networks having reached human-level performance in some medical tasks, their clinical use has been hindered by their lack of interpretability. Two major interpretability strategies have been proposed to tackle this problem: post-hoc methods and intrinsic methods. Although there are several post-hoc methods to interpret DL models, there is significant variation between the explanations provided by each method, and it a difficult to validate them due to the lack of ground-truth. To address this challenge, we adapted the intrinsical interpretable ProtoPNet for the context of histopathology imaging and compared the attribution maps produced by it and the saliency maps made by post-hoc methods. To evaluate the similarity between saliency map methods and attribution maps we adapted 10 saliency metrics from the saliency model literature, and used the breast cancer metastases detection dataset PatchCamelyon with 327,680 patches of histopathological images of sentin",
    "link": "http://arxiv.org/abs/2305.03002",
    "context": "Title: Evaluating Post-hoc Interpretability with Intrinsic Interpretability. (arXiv:2305.03002v1 [cs.CV])\nAbstract: Despite Convolutional Neural Networks having reached human-level performance in some medical tasks, their clinical use has been hindered by their lack of interpretability. Two major interpretability strategies have been proposed to tackle this problem: post-hoc methods and intrinsic methods. Although there are several post-hoc methods to interpret DL models, there is significant variation between the explanations provided by each method, and it a difficult to validate them due to the lack of ground-truth. To address this challenge, we adapted the intrinsical interpretable ProtoPNet for the context of histopathology imaging and compared the attribution maps produced by it and the saliency maps made by post-hoc methods. To evaluate the similarity between saliency map methods and attribution maps we adapted 10 saliency metrics from the saliency model literature, and used the breast cancer metastases detection dataset PatchCamelyon with 327,680 patches of histopathological images of sentin",
    "path": "papers/23/05/2305.03002.json",
    "total_tokens": 906,
    "translated_title": "通过内在的可解释性评估后续解释性",
    "translated_abstract": "虽然卷积神经网络在某些医学任务中达到了人类水平的性能，但它们的临床应用受到了可解释性的限制。为解决这个问题，提出了两种主要的解释性策略：后续方法和内在方法。本文将内部可解释性 ProtoPNet 进行改进，适用于组织病理学图像的背景，并将其生成的指向性图与后续方法生成的显著性图进行比较。同时，为了评估显著性图方法和指向性图之间的相似性，本文从显著性模型文献中选取了10个显著性度量指标，并使用包含327,680个组织病理学图像补丁的乳腺癌转移检测数据集 PatchCamelyon 进行评估。",
    "tldr": "本文通过改进内部可解释性 ProtoPNet 方法，并将其生成的指向性图与后续方法生成的显著性图进行比较，以解决现有解释性方法间存在的差异，为卷积神经网络在组织病理学图像领域的临床应用提供了可行方案。",
    "en_tdlr": "This paper proposes a feasible solution for the clinical application of convolutional neural networks in histopathology imaging by adapting the internal interpretable ProtoPNet method, comparing the attribution maps it produces with saliency maps made by post-hoc methods, and evaluating the similarity between them using 10 saliency metrics, thus addressing the existing differences between interpretability methods."
}