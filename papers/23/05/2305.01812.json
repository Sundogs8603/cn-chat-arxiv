{
    "title": "Post-Abstention: Towards Reliably Re-Attempting the Abstained Instances in QA. (arXiv:2305.01812v1 [cs.CL])",
    "abstract": "Despite remarkable progress made in natural language processing, even the state-of-the-art models often make incorrect predictions. Such predictions hamper the reliability of systems and limit their widespread adoption in real-world applications. 'Selective prediction' partly addresses the above concern by enabling models to abstain from answering when their predictions are likely to be incorrect. While selective prediction is advantageous, it leaves us with a pertinent question 'what to do after abstention'. To this end, we present an explorative study on 'Post-Abstention', a task that allows re-attempting the abstained instances with the aim of increasing 'coverage' of the system without significantly sacrificing its 'accuracy'. We first provide mathematical formulation of this task and then explore several methods to solve it. Comprehensive experiments on 11 QA datasets show that these methods lead to considerable risk improvements -- performance metric of the Post-Abstention task -",
    "link": "http://arxiv.org/abs/2305.01812",
    "context": "Title: Post-Abstention: Towards Reliably Re-Attempting the Abstained Instances in QA. (arXiv:2305.01812v1 [cs.CL])\nAbstract: Despite remarkable progress made in natural language processing, even the state-of-the-art models often make incorrect predictions. Such predictions hamper the reliability of systems and limit their widespread adoption in real-world applications. 'Selective prediction' partly addresses the above concern by enabling models to abstain from answering when their predictions are likely to be incorrect. While selective prediction is advantageous, it leaves us with a pertinent question 'what to do after abstention'. To this end, we present an explorative study on 'Post-Abstention', a task that allows re-attempting the abstained instances with the aim of increasing 'coverage' of the system without significantly sacrificing its 'accuracy'. We first provide mathematical formulation of this task and then explore several methods to solve it. Comprehensive experiments on 11 QA datasets show that these methods lead to considerable risk improvements -- performance metric of the Post-Abstention task -",
    "path": "papers/23/05/2305.01812.json",
    "total_tokens": 887,
    "translated_title": "后放弃：关于在问答中可靠地重新尝试放弃实例的研究",
    "translated_abstract": "尽管自然语言处理取得了显著进展，但即使是最先进的模型也经常做出错误的预测。这些预测会影响系统的可靠性，并限制它们在实际应用中的广泛采用。选择性预测通过使模型能够在其预测可能不正确时放弃回答来部分地解决了上述问题。虽然选择性预测具有优势，但它留下了一个关键问题：“放弃后该怎么办”？为此，我们提出了一项关于“后放弃”的研究，该任务允许重新尝试放弃实例，以增加系统的“覆盖率”而不显着牺牲其“准确性”。我们首先提供了该任务的数学公式，然后探讨了几种解决方法。对11个问答数据集进行的全面实验表明，这些方法导致了相当大的风险改进——后放弃任务的性能指标。",
    "tldr": "本文研究再次尝试放弃实例的后期处理方法，以提高系统的覆盖范围而不显着牺牲准确性。",
    "en_tdlr": "This paper explores post-abstention task that allows re-attempting the abstained instances in QA, aiming to increase system coverage without significantly sacrificing accuracy. Comprehensive experiments on 11 QA datasets show that this method leads to considerable risk improvements for post-abstention task performance metric."
}