{
    "title": "Properties of the ENCE and other MAD-based calibration metrics. (arXiv:2305.11905v1 [cs.LG])",
    "abstract": "The Expected Normalized Calibration Error (ENCE) is a popular calibration statistic used in Machine Learning to assess the quality of prediction uncertainties for regression problems. Estimation of the ENCE is based on the binning of calibration data. In this short note, I illustrate an annoying property of the ENCE, i.e. its proportionality to the square root of the number of bins for well calibrated or nearly calibrated datasets. A similar behavior affects the calibration error based on the variance of z-scores (ZVE), and in both cases this property is a consequence of the use of a Mean Absolute Deviation (MAD) statistic to estimate calibration errors. Hence, the question arises of which number of bins to choose for a reliable estimation of calibration error statistics. A solution is proposed to infer ENCE and ZVE values that do not depend on the number of bins for datasets assumed to be calibrated, providing simultaneously a statistical calibration test. It is also shown that the ZV",
    "link": "http://arxiv.org/abs/2305.11905",
    "context": "Title: Properties of the ENCE and other MAD-based calibration metrics. (arXiv:2305.11905v1 [cs.LG])\nAbstract: The Expected Normalized Calibration Error (ENCE) is a popular calibration statistic used in Machine Learning to assess the quality of prediction uncertainties for regression problems. Estimation of the ENCE is based on the binning of calibration data. In this short note, I illustrate an annoying property of the ENCE, i.e. its proportionality to the square root of the number of bins for well calibrated or nearly calibrated datasets. A similar behavior affects the calibration error based on the variance of z-scores (ZVE), and in both cases this property is a consequence of the use of a Mean Absolute Deviation (MAD) statistic to estimate calibration errors. Hence, the question arises of which number of bins to choose for a reliable estimation of calibration error statistics. A solution is proposed to infer ENCE and ZVE values that do not depend on the number of bins for datasets assumed to be calibrated, providing simultaneously a statistical calibration test. It is also shown that the ZV",
    "path": "papers/23/05/2305.11905.json",
    "total_tokens": 1000,
    "translated_title": "关于ENCE和其他基于MAD的校准度量的特性",
    "translated_abstract": "「期望归一化校准误差（ENCE）」是机器学习中用于评估回归问题预测不确定性质量的常见校准统计量，其估计基于校准数据的分组。本文展示了ENCE的一个令人恼火的特性，即在校准良好或几乎校准的数据集上，它与分组数量的平方根成比例关系。类似的行为还影响了基于z分数（ZVE）方差的校准误差，并且在这两种情况下，此特性是使用平均绝对偏差（MAD）统计量估计校准误差的结果。因此，如何选择分组数以可靠地估计校准误差统计量成为一个问题。提出了一种解决方案，用于推断ENCE和ZVE的值，假设数据集已经校准，并同时提供统计校准测试。同时还表明，对于不断增加的分组密度，ZVE在渐近意义下等价于ENCE。",
    "tldr": "本文讨论ENEC和基于z分数（ZVE）方差的校准误差；指出在校准良好或几乎校准的数据集上，误差与分组数量的平方根成比例关系，提出一种解决方案以推断ENCE和ZVE的值，并提供统计校准测试。",
    "en_tdlr": "This paper discusses the ENCE and calibration error based on the variance of z-scores (ZVE), highlighting their proportional relationship with the square root of the number of bins for well-calibrated data. A solution is proposed to estimate ENCE and ZVE values independent of the number of bins for calibrated datasets, while providing a statistical calibration test. The paper also shows that ZVE is asymptotically equivalent to ENCE for increasing binning density."
}