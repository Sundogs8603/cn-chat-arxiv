{
    "title": "Controllable Visual-Tactile Synthesis. (arXiv:2305.03051v1 [cs.CV])",
    "abstract": "Deep generative models have various content creation applications such as graphic design, e-commerce, and virtual Try-on. However, current works mainly focus on synthesizing realistic visual outputs, often ignoring other sensory modalities, such as touch, which limits physical interaction with users. In this work, we leverage deep generative models to create a multi-sensory experience where users can touch and see the synthesized object when sliding their fingers on a haptic surface. The main challenges lie in the significant scale discrepancy between vision and touch sensing and the lack of explicit mapping from touch sensing data to a haptic rendering device. To bridge this gap, we collect high-resolution tactile data with a GelSight sensor and create a new visuotactile clothing dataset. We then develop a conditional generative model that synthesizes both visual and tactile outputs from a single sketch. We evaluate our method regarding image quality and tactile rendering accuracy. Fi",
    "link": "http://arxiv.org/abs/2305.03051",
    "context": "Title: Controllable Visual-Tactile Synthesis. (arXiv:2305.03051v1 [cs.CV])\nAbstract: Deep generative models have various content creation applications such as graphic design, e-commerce, and virtual Try-on. However, current works mainly focus on synthesizing realistic visual outputs, often ignoring other sensory modalities, such as touch, which limits physical interaction with users. In this work, we leverage deep generative models to create a multi-sensory experience where users can touch and see the synthesized object when sliding their fingers on a haptic surface. The main challenges lie in the significant scale discrepancy between vision and touch sensing and the lack of explicit mapping from touch sensing data to a haptic rendering device. To bridge this gap, we collect high-resolution tactile data with a GelSight sensor and create a new visuotactile clothing dataset. We then develop a conditional generative model that synthesizes both visual and tactile outputs from a single sketch. We evaluate our method regarding image quality and tactile rendering accuracy. Fi",
    "path": "papers/23/05/2305.03051.json",
    "total_tokens": 912,
    "translated_title": "可控的视觉-触觉合成",
    "translated_abstract": "深度生成模型在图形设计、电子商务和虚拟试穿等内容创作应用方面具有各种可能性，然而目前的研究主要集中在合成逼真的视觉输出上，往往忽略其他感官模态，如触觉，这限制了与用户之间的物理交互。在这项工作中，我们利用深度生成模型创建了一种多感官体验，用户可以在触觉表面上滑动手指时触摸和看到合成对象。主要的挑战在于视觉和触觉感知之间的巨大尺度差异以及触觉传感数据到触觉渲染设备的显式映射的缺乏。为了弥合这一差距，我们使用GelSight传感器收集高分辨率的触感数据，并创建了一个新的视觉触觉服装数据集。然后，我们开发了一种有条件的生成模型，从单个草图中合成视觉和触觉输出。我们评估了我们的方法，包括图像质量和触觉渲染精度。",
    "tldr": "本文利用深度生成模型实现了视觉-触觉的交互合成，用户可以通过触感表面触摸和看到合成物体，包括创建一个新的触感数据集和开发一个条件生成模型。",
    "en_tdlr": "This paper uses deep generative models to achieve visual-tactile interaction synthesis, allowing users to touch and see the synthesized object through a haptic surface, including creating a new tactile dataset and developing a conditional generative model."
}