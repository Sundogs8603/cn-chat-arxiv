{
    "title": "GersteinLab at MEDIQA-Chat 2023: Clinical Note Summarization from Doctor-Patient Conversations through Fine-tuning and In-context Learning. (arXiv:2305.05001v1 [cs.CL])",
    "abstract": "This paper presents our contribution to the MEDIQA-2023 Dialogue2Note shared task, encompassing both subtask A and subtask B. We approach the task as a dialogue summarization problem and implement two distinct pipelines: (a) a fine-tuning of a pre-trained dialogue summarization model and GPT-3, and (b) few-shot in-context learning (ICL) using a large language model, GPT-4. Both methods achieve excellent results in terms of ROUGE-1 F1, BERTScore F1 (deberta-xlarge-mnli), and BLEURT, with scores of 0.4011, 0.7058, and 0.5421, respectively. Additionally, we predict the associated section headers using RoBERTa and SciBERT based classification models. Our team ranked fourth among all teams, while each team is allowed to submit three runs as part of their submission. We also utilize expert annotations to demonstrate that the notes generated through the ICL GPT-4 are better than all other baselines. The code for our submission is available.",
    "link": "http://arxiv.org/abs/2305.05001",
    "context": "Title: GersteinLab at MEDIQA-Chat 2023: Clinical Note Summarization from Doctor-Patient Conversations through Fine-tuning and In-context Learning. (arXiv:2305.05001v1 [cs.CL])\nAbstract: This paper presents our contribution to the MEDIQA-2023 Dialogue2Note shared task, encompassing both subtask A and subtask B. We approach the task as a dialogue summarization problem and implement two distinct pipelines: (a) a fine-tuning of a pre-trained dialogue summarization model and GPT-3, and (b) few-shot in-context learning (ICL) using a large language model, GPT-4. Both methods achieve excellent results in terms of ROUGE-1 F1, BERTScore F1 (deberta-xlarge-mnli), and BLEURT, with scores of 0.4011, 0.7058, and 0.5421, respectively. Additionally, we predict the associated section headers using RoBERTa and SciBERT based classification models. Our team ranked fourth among all teams, while each team is allowed to submit three runs as part of their submission. We also utilize expert annotations to demonstrate that the notes generated through the ICL GPT-4 are better than all other baselines. The code for our submission is available.",
    "path": "papers/23/05/2305.05001.json",
    "total_tokens": 996,
    "translated_title": "GersteinLab在MEDIQA-Chat 2023中的贡献：通过精调和上下文学习整合医患交流中的临床记录摘要(arXiv:2305.05001v1 [cs.CL])",
    "translated_abstract": "本文介绍了我们在MEDIQA-2023 Dialogue2Note共享任务中的贡献，包括子任务A和子任务B。我们将任务视为一种对话摘要问题，并实现了两个不同的流程：（a）对预训练的对话摘要模型和GPT-3进行微调，（b）使用大型语言模型GPT-4进行少量上下文学习（ICL）。两种方法在ROUGE-1 F1，BERTScore F1（deberta-xlarge-mnli）和BLEURT方面均取得了极好的结果，分别为0.4011、0.7058和0.5421。此外，我们使用RoBERTa和SciBERT基于分类模型来预测相关的章节标题。我们的团队在所有团队中排名第四，每个团队允许提交三次运行作为其提交的一部分。我们还利用专家注释证明了通过ICL GPT-4生成的摘要优于所有其他基线。我们的代码已公开。",
    "tldr": "本研究团队参与MEDIQA-Chat 2023共享任务，实现了医患交流中临床记录摘要的自动化生成。通过对预训练模型的微调和少量上下文学习，本方法在关键指标上取得了极好的结果。",
    "en_tdlr": "GersteinLab participated in the MEDIQA-Chat 2023 shared task and presented a method for automatically generating clinical note summaries from doctor-patient conversations. The method involves fine-tuning a pre-trained model and implementing in-context learning, resulting in excellent results on key metrics."
}