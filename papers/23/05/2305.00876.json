{
    "title": "Exactly Tight Information-Theoretic Generalization Error Bound for the Quadratic Gaussian Problem. (arXiv:2305.00876v1 [cs.IT])",
    "abstract": "We provide a new information-theoretic generalization error bound that is exactly tight (i.e., matching even the constant) for the canonical quadratic Gaussian mean estimation problem. Despite considerable existing efforts in deriving information-theoretic generalization error bounds, applying them to this simple setting where sample average is used as the estimate of the mean value of Gaussian data has not yielded satisfying results. In fact, most existing bounds are order-wise loose in this setting, which has raised concerns about the fundamental capability of information-theoretic bounds in reasoning the generalization behavior for machine learning. The proposed new bound adopts the individual-sample-based approach proposed by Bu et al., but also has several key new ingredients. Firstly, instead of applying the change of measure inequality on the loss function, we apply it to the generalization error function itself; secondly, the bound is derived in a conditional manner; lastly, a ",
    "link": "http://arxiv.org/abs/2305.00876",
    "context": "Title: Exactly Tight Information-Theoretic Generalization Error Bound for the Quadratic Gaussian Problem. (arXiv:2305.00876v1 [cs.IT])\nAbstract: We provide a new information-theoretic generalization error bound that is exactly tight (i.e., matching even the constant) for the canonical quadratic Gaussian mean estimation problem. Despite considerable existing efforts in deriving information-theoretic generalization error bounds, applying them to this simple setting where sample average is used as the estimate of the mean value of Gaussian data has not yielded satisfying results. In fact, most existing bounds are order-wise loose in this setting, which has raised concerns about the fundamental capability of information-theoretic bounds in reasoning the generalization behavior for machine learning. The proposed new bound adopts the individual-sample-based approach proposed by Bu et al., but also has several key new ingredients. Firstly, instead of applying the change of measure inequality on the loss function, we apply it to the generalization error function itself; secondly, the bound is derived in a conditional manner; lastly, a ",
    "path": "papers/23/05/2305.00876.json",
    "total_tokens": 927,
    "translated_title": "二次高斯问题的信息理论泛化误差的完全紧确界",
    "translated_abstract": "我们提供了一种新的信息理论泛化误差紧确界，对于典型的二次高斯均值估计问题，它是完全紧确的（即匹配常数）。尽管在推导信息论泛化误差界方面进行了相当多的努力，但将其应用于使用样本平均作为高斯数据均值估计的简单设置并没有产生令人满意的结果。事实上，在这种情况下，大多数现有的界都是松散的，这引起了人们对于信息理论界在推理机器学习的泛化行为方面的基本能力的关注。提出的新的界采用了Bu等人提出的基于单个样本的方法，但也有几个关键的新组成部分。 首先，我们不是将测量变换不等式应用于损失函数，而是应用于泛化误差函数本身；其次，界是有条件地导出的； 最后，",
    "tldr": "该论文提出了一种新的信息理论泛化误差紧确界，对于典型的二次高斯均值估计问题，它是完全紧确的。与现有的界不同，这个新界利用了个体样本的方法，并对泛化误差函数进行了测量变换不等式和条件导出。",
    "en_tdlr": "This paper proposes a new information-theoretic generalization error bound that is exactly tight, matching even the constant, for the canonical quadratic Gaussian mean estimation problem. The proposed bound adopts the individual-sample-based approach and applies the change of measure inequality on the generalization error function itself, deriving the bound in a conditional manner and introducing new key ingredients."
}