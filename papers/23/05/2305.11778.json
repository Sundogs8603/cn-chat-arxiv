{
    "title": "Cross-Lingual Supervision improves Large Language Models Pre-training. (arXiv:2305.11778v1 [cs.CL])",
    "abstract": "The recent rapid progress in pre-training Large Language Models has relied on using self-supervised language modeling objectives like next token prediction or span corruption. On the other hand, Machine Translation Systems are mostly trained using cross-lingual supervision that requires aligned data between source and target languages. We demonstrate that pre-training Large Language Models on a mixture of a self-supervised Language Modeling objective and the supervised Machine Translation objective, therefore including cross-lingual parallel data during pre-training, yields models with better in-context learning abilities. As pre-training is a very resource-intensive process and a grid search on the best mixing ratio between the two objectives is prohibitively expensive, we propose a simple yet effective strategy to learn it during pre-training.",
    "link": "http://arxiv.org/abs/2305.11778",
    "context": "Title: Cross-Lingual Supervision improves Large Language Models Pre-training. (arXiv:2305.11778v1 [cs.CL])\nAbstract: The recent rapid progress in pre-training Large Language Models has relied on using self-supervised language modeling objectives like next token prediction or span corruption. On the other hand, Machine Translation Systems are mostly trained using cross-lingual supervision that requires aligned data between source and target languages. We demonstrate that pre-training Large Language Models on a mixture of a self-supervised Language Modeling objective and the supervised Machine Translation objective, therefore including cross-lingual parallel data during pre-training, yields models with better in-context learning abilities. As pre-training is a very resource-intensive process and a grid search on the best mixing ratio between the two objectives is prohibitively expensive, we propose a simple yet effective strategy to learn it during pre-training.",
    "path": "papers/23/05/2305.11778.json",
    "total_tokens": 818,
    "translated_title": "《跨语种监督提高大型语言模型预训练质量》",
    "translated_abstract": "近期大型语言模型的迅速进展主要依赖于使用自监督语言建模目标（如下一个标记预测或跨度损坏）。与此相反，机器翻译系统主要使用需要源语言和目标语言之间对齐数据的跨语种监督进行训练。我们证明，在预训练过程中，将大型语言模型使用自监督语言建模目标和受监督的机器翻译目标混合，因此在预训练过程中包含跨语种并行数据，可产生更好的上下文学习能力。由于预训练是非常资源密集的过程，而在两个目标之间进行最佳混合比例的网格搜索是非常昂贵的，我们提出了一种简单而有效的策略来在预训练中学习这些知识。",
    "tldr": "本文表明，将大型语言模型的预训练中使用跨语种并行数据能提高其上下文学习能力；同时，提出了一种简单且有效的策略来学习两个目标之间的最佳混合比例。",
    "en_tdlr": "This paper shows that including cross-lingual parallel data in pre-training large language models can improve their in-context learning abilities, and proposes a simple yet effective strategy to learn the best mixing ratio between two objectives during pre-training."
}