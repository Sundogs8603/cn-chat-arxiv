{
    "title": "A Frustratingly Simple Decoding Method for Neural Text Generation",
    "abstract": "arXiv:2305.12675v2 Announce Type: replace  Abstract: We introduce a frustratingly simple, super efficient and surprisingly effective decoding method, which we call Frustratingly Simple Decoding (FSD), for neural text generation. The idea behind FSD is straightforward: we build an anti-LM based on previously generated text and use this anti-LM to penalize future generation of what has been generated. The anti-LM can be implemented as simple as an n-gram language model or a vectorized variant. In this way, FSD introduces no extra model parameters and negligible computational overhead (FSD can be as fast as greedy search). Despite the simplicity, FSD is surprisingly effective; Experiments show that FSD can outperform the canonical methods to date (i.e., nucleus sampling) as well as several strong baselines that were proposed recently.",
    "link": "https://arxiv.org/abs/2305.12675",
    "context": "Title: A Frustratingly Simple Decoding Method for Neural Text Generation\nAbstract: arXiv:2305.12675v2 Announce Type: replace  Abstract: We introduce a frustratingly simple, super efficient and surprisingly effective decoding method, which we call Frustratingly Simple Decoding (FSD), for neural text generation. The idea behind FSD is straightforward: we build an anti-LM based on previously generated text and use this anti-LM to penalize future generation of what has been generated. The anti-LM can be implemented as simple as an n-gram language model or a vectorized variant. In this way, FSD introduces no extra model parameters and negligible computational overhead (FSD can be as fast as greedy search). Despite the simplicity, FSD is surprisingly effective; Experiments show that FSD can outperform the canonical methods to date (i.e., nucleus sampling) as well as several strong baselines that were proposed recently.",
    "path": "papers/23/05/2305.12675.json",
    "total_tokens": 847,
    "translated_title": "一种令人沮丧地简单的神经文本生成解码方法",
    "translated_abstract": "我们介绍了一种令人沮丧地简单、超级高效且出奇有效的解码方法，我们称之为Frustratingly Simple Decoding（FSD），用于神经文本生成。FSD的思想很简单：我们构建一个基于先前生成文本的反语言模型，并使用这个反语言模型来惩罚未来生成已生成的内容。这个反语言模型可以实现得非常简单，可以是一个n-gram语言模型或者一个向量化变体。因此，FSD不引入额外的模型参数，计算开销微乎其微（FSD速度可以像贪婪搜索一样快）。尽管如此简单，FSD却出奇地有效；实验证明，FSD可以胜过迄今为止的经典方法（即核采样方法）以及最近提出的一些强基线方法。",
    "tldr": "提出了一种称为Frustratingly Simple Decoding (FSD)的神经文本生成解码方法，通过构建反语言模型来惩罚未来生成已生成的内容，实验证明其通过引入几乎无额外计算开销就能有效超越传统方法（如核采样）。",
    "en_tdlr": "Introduced a decoding method called Frustratingly Simple Decoding (FSD) for neural text generation, which penalizes future generation of previously generated text by using an anti-language model, and experimentally demonstrated its effectiveness in outperforming traditional methods (such as nucleus sampling) with almost negligible computational overhead."
}