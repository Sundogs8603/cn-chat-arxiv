{
    "title": "Exploring Large Language Models for Classical Philology. (arXiv:2305.13698v1 [cs.CL])",
    "abstract": "Recent advances in NLP have led to the creation of powerful language models for many languages including Ancient Greek and Latin. While prior work on Classical languages unanimously uses BERT, in this work we create four language models for Ancient Greek that vary along two dimensions to study their versatility for tasks of interest for Classical languages: we explore (i) encoder-only and encoder-decoder architectures using RoBERTa and T5 as strong model types, and create for each of them (ii) a monolingual Ancient Greek and a multilingual instance that includes Latin and English. We evaluate all models on morphological and syntactic tasks, including lemmatization, which demonstrates the added value of T5's decoding abilities. We further define two probing tasks to investigate the knowledge acquired by models pre-trained on Classical texts. Our experiments provide the first benchmarking analysis of existing models of Ancient Greek. Results show that our models provide significant impro",
    "link": "http://arxiv.org/abs/2305.13698",
    "context": "Title: Exploring Large Language Models for Classical Philology. (arXiv:2305.13698v1 [cs.CL])\nAbstract: Recent advances in NLP have led to the creation of powerful language models for many languages including Ancient Greek and Latin. While prior work on Classical languages unanimously uses BERT, in this work we create four language models for Ancient Greek that vary along two dimensions to study their versatility for tasks of interest for Classical languages: we explore (i) encoder-only and encoder-decoder architectures using RoBERTa and T5 as strong model types, and create for each of them (ii) a monolingual Ancient Greek and a multilingual instance that includes Latin and English. We evaluate all models on morphological and syntactic tasks, including lemmatization, which demonstrates the added value of T5's decoding abilities. We further define two probing tasks to investigate the knowledge acquired by models pre-trained on Classical texts. Our experiments provide the first benchmarking analysis of existing models of Ancient Greek. Results show that our models provide significant impro",
    "path": "papers/23/05/2305.13698.json",
    "total_tokens": 1064,
    "translated_title": "探索大型语言模型在古典语言学中的应用",
    "translated_abstract": "自然语言处理领域的最新进展已经为包括古希腊语和拉丁语在内的许多语言创建了强大的语言模型。然而，在以往的古典语言研究中，大多数研究都使用BERT。本研究中，我们创建了四个古希腊语言模型，通过两个维度的变化来研究它们在古典语言学中的多功能性。我们探索使用RoBERTa和T5作为强大的语言模型类型的编码器和编码器-解码器架构，并为每个模型创建了包含拉丁语和英语的单语古希腊语和多语言实例。我们评估了所有模型在形态和句法任务上的表现，包括词形还原，展示了T5的解码能力的附加价值。我们进一步定义了两个探测任务来研究预训练在古典文本上的模型所获得的知识。我们的实验提供了对现有古希腊语言模型的第一批基准分析。结果表明，我们的模型相对于当前的最新技术水平提供了显著的改进，并且多语言模型优于单语模型。",
    "tldr": "本论文探索使用RoBERTa和T5作为强大的语言模型类型的编码器和编码器-解码器架构，创建了古希腊语和拉丁语等多语言实例的四个古希腊语言模型。测试结果表明，这些模型在形态和句法任务上的表现都得到了显著的提升，并且多语言模型表现优于单语模型。",
    "en_tdlr": "This paper explores the use of RoBERTa and T5 as strong language model types to create four language models for Ancient Greek, including multilingual instances with Latin and English. Results show significant improvement in morphological and syntactic tasks compared to current state-of-the-art, with multilingual models outperforming monolingual models."
}