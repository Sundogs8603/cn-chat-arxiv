{
    "title": "Empirical Analysis of the Inductive Bias of Recurrent Neural Networks by Discrete Fourier Transform of Output Sequences. (arXiv:2305.09178v1 [cs.LG])",
    "abstract": "A unique feature of Recurrent Neural Networks (RNNs) is that it incrementally processes input sequences. In this research, we aim to uncover the inherent generalization properties, i.e., inductive bias, of RNNs with respect to how frequently RNNs switch the outputs through time steps in the sequence classification task, which we call output sequence frequency. Previous work analyzed inductive bias by training models with a few synthetic data and comparing the model's generalization with candidate generalization patterns. However, when examining the output sequence frequency, previous methods cannot be directly applied since enumerating candidate patterns is computationally difficult for longer sequences. To this end, we propose to directly calculate the output sequence frequency for each model by regarding the outputs of the model as discrete-time signals and applying frequency domain analysis. Experimental results showed that Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU",
    "link": "http://arxiv.org/abs/2305.09178",
    "context": "Title: Empirical Analysis of the Inductive Bias of Recurrent Neural Networks by Discrete Fourier Transform of Output Sequences. (arXiv:2305.09178v1 [cs.LG])\nAbstract: A unique feature of Recurrent Neural Networks (RNNs) is that it incrementally processes input sequences. In this research, we aim to uncover the inherent generalization properties, i.e., inductive bias, of RNNs with respect to how frequently RNNs switch the outputs through time steps in the sequence classification task, which we call output sequence frequency. Previous work analyzed inductive bias by training models with a few synthetic data and comparing the model's generalization with candidate generalization patterns. However, when examining the output sequence frequency, previous methods cannot be directly applied since enumerating candidate patterns is computationally difficult for longer sequences. To this end, we propose to directly calculate the output sequence frequency for each model by regarding the outputs of the model as discrete-time signals and applying frequency domain analysis. Experimental results showed that Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU",
    "path": "papers/23/05/2305.09178.json",
    "total_tokens": 1069,
    "translated_title": "通过离散傅里叶变换分析递归神经网络的归纳偏置",
    "translated_abstract": "递归神经网络(RNNs)的一个独特特性是它逐步处理输入序列。本研究旨在揭示RNNs内在的归纳偏置，即在序列分类任务中，RNNs在多久时间步骤中通过输出进行切换。我们称之为输出序列频率。以前的工作通过训练一些合成数据的模型，并将模型的泛化性与候选泛化模式进行比较，分析了归纳偏置。然而，当检查输出序列频率时，由于枚举候选模式在更长的序列上需要大量计算，以前的方法不能直接应用。为此，我们提出了通过将模型的输出视为离散时间信号，并应用频率域分析来直接计算每个模型的输出序列频率的方法。实验结果表明，长短时记忆（LSTM）和门控循环单元（GRU）在输出序列频率方面具有不同的归纳偏置。具体来说，LSTM倾向于在时间步骤之间更少地切换输出，这表明LSTM更喜欢各个时间步之间的依赖关系，更适合需要长期记忆的任务。",
    "tldr": "通过离散傅里叶变换，直接计算每个模型的输出序列频率，研究发现长短时记忆（LSTM）和门控循环单元（GRU）在输出序列频率方面具有不同的归纳偏置，LSTM更适合需要长期记忆任务。",
    "en_tdlr": "By applying Discrete Fourier Transform to calculate the output sequence frequency of each model, this research discovers the different inductive biases of Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU), with LSTM having a stronger preference for dependencies across time steps and being more suitable for tasks that require long-term memory."
}