{
    "title": "Emergent Agentic Transformer from Chain of Hindsight Experience. (arXiv:2305.16554v1 [cs.LG])",
    "abstract": "Large transformer models powered by diverse data and model scale have dominated natural language modeling and computer vision and pushed the frontier of multiple AI areas. In reinforcement learning (RL), despite many efforts into transformer-based policies, a key limitation, however, is that current transformer-based policies cannot learn by directly combining information from multiple sub-optimal trials. In this work, we address this issue using recently proposed chain of hindsight to relabel experience, where we train a transformer on a sequence of trajectory experience ascending sorted according to their total rewards. Our method consists of relabelling target return of each trajectory to the maximum total reward among in sequence of trajectories and training an autoregressive model to predict actions conditioning on past states, actions, rewards, target returns, and task completion tokens, the resulting model, Agentic Transformer (AT), can learn to improve upon itself both at train",
    "link": "http://arxiv.org/abs/2305.16554",
    "context": "Title: Emergent Agentic Transformer from Chain of Hindsight Experience. (arXiv:2305.16554v1 [cs.LG])\nAbstract: Large transformer models powered by diverse data and model scale have dominated natural language modeling and computer vision and pushed the frontier of multiple AI areas. In reinforcement learning (RL), despite many efforts into transformer-based policies, a key limitation, however, is that current transformer-based policies cannot learn by directly combining information from multiple sub-optimal trials. In this work, we address this issue using recently proposed chain of hindsight to relabel experience, where we train a transformer on a sequence of trajectory experience ascending sorted according to their total rewards. Our method consists of relabelling target return of each trajectory to the maximum total reward among in sequence of trajectories and training an autoregressive model to predict actions conditioning on past states, actions, rewards, target returns, and task completion tokens, the resulting model, Agentic Transformer (AT), can learn to improve upon itself both at train",
    "path": "papers/23/05/2305.16554.json",
    "total_tokens": 844,
    "translated_abstract": "大型的Transformer模型借助多样的数据和模型规模，在自然语言建模和计算机视觉方面占据主导地位，推动了多个AI领域的前沿。在强化学习中，尽管许多Efforts致力于基于Transformer的策略，但关键限制在于当前基于Transformer的策略不能直接从多个次优试验中汇总信息进行学习。在这项工作中，我们使用最近提出的后见之明链来重新标记经验，其中我们训练一个Transformer模型，根据其总奖励的升序序列学习轨迹经验。我们的方法包括将每个轨迹的目标回报重新标记为序列中的最大总回报，基于过去的状态、动作、奖励、目标回报和任务完成标记训练自回归模型来预测动作，结果模型，即主动Transformer（AT）能够在训练和测试过程中不断改进自己。",
    "tldr": "本研究提出了一种基于后见之明链的新兴主动Transformer模型，能够从多个次优试验中直接学习信息，具有在强化学习中的实用价值。",
    "en_tdlr": "This paper proposes an emerging agentic Transformer model based on the chain of hindsight, which can directly learn information from multiple sub-optimal trials and has practical value in reinforcement learning."
}