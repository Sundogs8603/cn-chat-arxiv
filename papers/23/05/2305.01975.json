{
    "title": "A Survey on Dataset Distillation: Approaches, Applications and Future Directions. (arXiv:2305.01975v1 [cs.LG])",
    "abstract": "Dataset distillation is attracting more attention in machine learning as training sets continue to grow and the cost of training state-of-the-art models becomes increasingly high. By synthesizing datasets with high information density, dataset distillation offers a range of potential applications, including support for continual learning, neural architecture search, and privacy protection. Despite recent advances, we lack a holistic understanding of the approaches and applications. Our survey aims to bridge this gap by first proposing a taxonomy of dataset distillation, characterizing existing approaches, and then systematically reviewing the data modalities, and related applications. In addition, we summarize the challenges and discuss future directions for this field of research.",
    "link": "http://arxiv.org/abs/2305.01975",
    "context": "Title: A Survey on Dataset Distillation: Approaches, Applications and Future Directions. (arXiv:2305.01975v1 [cs.LG])\nAbstract: Dataset distillation is attracting more attention in machine learning as training sets continue to grow and the cost of training state-of-the-art models becomes increasingly high. By synthesizing datasets with high information density, dataset distillation offers a range of potential applications, including support for continual learning, neural architecture search, and privacy protection. Despite recent advances, we lack a holistic understanding of the approaches and applications. Our survey aims to bridge this gap by first proposing a taxonomy of dataset distillation, characterizing existing approaches, and then systematically reviewing the data modalities, and related applications. In addition, we summarize the challenges and discuss future directions for this field of research.",
    "path": "papers/23/05/2305.01975.json",
    "total_tokens": 852,
    "translated_title": "数据集蒸馏研究综述: 方法、应用和未来方向",
    "translated_abstract": "随着训练集的不断增长和训练最先进的模型成本越来越高，数据集蒸馏在机器学习中越来越受到关注。通过合成高信息密度的数据集，数据集蒸馏提供了一系列潜在应用，包括支持持续学习、神经架构搜索和隐私保护。尽管近年来有了一些进展，但我们缺乏对方法和应用的全面理解。我们的调查旨在填补这一空白，首先提出一种数据集蒸馏的分类方法，对现有方法进行特征化，然后系统地回顾数据模态和相关应用。此外，我们总结了挑战并讨论了这一研究领域的未来方向。",
    "tldr": "数据集蒸馏在机器学习中越来越重要。该方法可以通过合成高信息密度的数据集来支持持续学习、神经架构搜索和隐私保护。这篇综述性调查论文提出了一种分类方法，对现有方法进行了特征化，并系统回顾了数据模态和相关应用，同时总结了挑战并讨论了未来方向。",
    "en_tdlr": "Dataset distillation is becoming increasingly important in machine learning, offering potential applications such as support for continual learning, neural architecture search, and privacy protection by synthesizing datasets with high information density. This survey proposed a taxonomy of dataset distillation, characterized existing approaches, reviewed data modalities and related applications while summarizing challenges and discussing future directions of this research field."
}