{
    "title": "Graph Reinforcement Learning for Network Control via Bi-Level Optimization. (arXiv:2305.09129v1 [cs.LG])",
    "abstract": "Optimization problems over dynamic networks have been extensively studied and widely used in the past decades to formulate numerous real-world problems. However, (1) traditional optimization-based approaches do not scale to large networks, and (2) the design of good heuristics or approximation algorithms often requires significant manual trial-and-error. In this work, we argue that data-driven strategies can automate this process and learn efficient algorithms without compromising optimality. To do so, we present network control problems through the lens of reinforcement learning and propose a graph network-based framework to handle a broad class of problems. Instead of naively computing actions over high-dimensional graph elements, e.g., edges, we propose a bi-level formulation where we (1) specify a desired next state via RL, and (2) solve a convex program to best achieve it, leading to drastically improved scalability and performance. We further highlight a collection of desirable f",
    "link": "http://arxiv.org/abs/2305.09129",
    "context": "Title: Graph Reinforcement Learning for Network Control via Bi-Level Optimization. (arXiv:2305.09129v1 [cs.LG])\nAbstract: Optimization problems over dynamic networks have been extensively studied and widely used in the past decades to formulate numerous real-world problems. However, (1) traditional optimization-based approaches do not scale to large networks, and (2) the design of good heuristics or approximation algorithms often requires significant manual trial-and-error. In this work, we argue that data-driven strategies can automate this process and learn efficient algorithms without compromising optimality. To do so, we present network control problems through the lens of reinforcement learning and propose a graph network-based framework to handle a broad class of problems. Instead of naively computing actions over high-dimensional graph elements, e.g., edges, we propose a bi-level formulation where we (1) specify a desired next state via RL, and (2) solve a convex program to best achieve it, leading to drastically improved scalability and performance. We further highlight a collection of desirable f",
    "path": "papers/23/05/2305.09129.json",
    "total_tokens": 992,
    "translated_title": "基于图形强化学习的网络控制双层优化方案",
    "translated_abstract": "在过去的几十年中，对动态网络的优化问题进行了广泛的研究，并被广泛用于规划无数的真实世界问题。然而，(1)传统的基于优化的方法不能扩展到大型网络，(2)设计好的启发式或近似算法往往需要大量的手动试验。在本研究中，我们认为数据驱动的策略可以自动化这个过程，并学习有效的算法而不牺牲优化性能。为此，我们通过强化学习的视角提出了网络控制问题，并提出了一个基于图形网络的框架来处理广泛的问题类。我们提出了一个双层优化方案，而不是天真地对高维图形元素（例如边缘）进行操作，(1)我们通过RL指定了下一个状态，(2)并解决一个凸问题来最好地实现它，从而大大提高了可扩展性和性能。我们进一步强调了我们的框架可以实现的一系列期望的特性，包括鲁棒性、灵活性和可解释性，并在网络控制的一些基本问题，包括图形染色和设施选址问题上验证了我们的方法的有效性。",
    "tldr": "本文提出了一种基于图形强化学习的网络控制方法，通过双层优化实现较好的扩展性和性能，并具备鲁棒性、灵活性和可解释性。",
    "en_tdlr": "This paper proposes a graph reinforcement learning-based framework for network control problems, which achieves better scalability and performance through bi-level optimization, and features robustness, flexibility and interpretability. The approach is validated on canonical network control problems including graph coloring and facility location problems."
}