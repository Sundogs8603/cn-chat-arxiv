{
    "title": "Exploring the Space of Key-Value-Query Models with Intention. (arXiv:2305.10203v1 [cs.LG])",
    "abstract": "Attention-based models have been a key element of many recent breakthroughs in deep learning. Two key components of Attention are the structure of its input (which consists of keys, values and queries) and the computations by which these three are combined. In this paper we explore the space of models that share said input structure but are not restricted to the computations of Attention. We refer to this space as Keys-Values-Queries (KVQ) Space. Our goal is to determine whether there are any other stackable models in KVQ Space that Attention cannot efficiently approximate, which we can implement with our current deep learning toolbox and that solve problems that are interesting to the community. Maybe surprisingly, the solution to the standard least squares problem satisfies these properties. A neural network module that is able to compute this solution not only enriches the set of computations that a neural network can represent but is also provably a strict generalisation of Linear ",
    "link": "http://arxiv.org/abs/2305.10203",
    "context": "Title: Exploring the Space of Key-Value-Query Models with Intention. (arXiv:2305.10203v1 [cs.LG])\nAbstract: Attention-based models have been a key element of many recent breakthroughs in deep learning. Two key components of Attention are the structure of its input (which consists of keys, values and queries) and the computations by which these three are combined. In this paper we explore the space of models that share said input structure but are not restricted to the computations of Attention. We refer to this space as Keys-Values-Queries (KVQ) Space. Our goal is to determine whether there are any other stackable models in KVQ Space that Attention cannot efficiently approximate, which we can implement with our current deep learning toolbox and that solve problems that are interesting to the community. Maybe surprisingly, the solution to the standard least squares problem satisfies these properties. A neural network module that is able to compute this solution not only enriches the set of computations that a neural network can represent but is also provably a strict generalisation of Linear ",
    "path": "papers/23/05/2305.10203.json",
    "total_tokens": 890,
    "translated_title": "带有意图的键值查询模型的空间探索",
    "translated_abstract": "基于注意力机制的模型已成为深度学习最新突破的关键要素。注意力的两个关键组成部分是其输入的结构（包含键，值和查询）以及这三个部分如何进行组合的计算。在本文中，我们探索了共享上述输入结构但不仅限于注意力计算的模型空间。我们称这个空间为键-值-查询（KVQ）空间。我们的目标是确定是否存在Attention无法高效逼近、我们可以使用当前的深度学习工具箱实现，并解决社区感兴趣的问题的其他可堆叠模型。也许令人惊讶的是，标准的最小二乘问题的解决方案满足这些属性。能够计算这个解的神经网络模块不仅丰富了神经网络表示的计算集合，而且还被证明是线性回归的一个严格的推广。",
    "tldr": "该论文探索了共享 Attention 输入结构但不限于 Attention 计算的模型空间，发现可以使用神经网络计算最小二乘问题的解，这是 Attention 无法高效逼近的，对于神经网络而言也是一种严格的扩展。",
    "en_tdlr": "This paper explores the space of models that share the input structure of Attention but are not restricted to its computations. Surprisingly, they found that the solution to the standard least squares problem can be computed effectively with a neural network module, which is a strict generalisation of Linear Regression that Attention cannot efficiently approximate."
}