{
    "title": "Unjustified Sample Sizes and Generalizations in Explainable AI Research: Principles for More Inclusive User Studies. (arXiv:2305.09477v1 [cs.AI])",
    "abstract": "Many ethical frameworks require artificial intelligence (AI) systems to be explainable. Explainable AI (XAI) models are frequently tested for their adequacy in user studies. Since different people may have different explanatory needs, it is important that participant samples in user studies are large enough to represent the target population to enable generalizations. However, it is unclear to what extent XAI researchers reflect on and justify their sample sizes or avoid broad generalizations across people. We analyzed XAI user studies (N = 220) published between 2012 and 2022. Most studies did not offer rationales for their sample sizes. Moreover, most papers generalized their conclusions beyond their target population, and there was no evidence that broader conclusions in quantitative studies were correlated with larger samples. These methodological problems can impede evaluations of whether XAI systems implement the explainability called for in ethical frameworks. We outline princip",
    "link": "http://arxiv.org/abs/2305.09477",
    "context": "Title: Unjustified Sample Sizes and Generalizations in Explainable AI Research: Principles for More Inclusive User Studies. (arXiv:2305.09477v1 [cs.AI])\nAbstract: Many ethical frameworks require artificial intelligence (AI) systems to be explainable. Explainable AI (XAI) models are frequently tested for their adequacy in user studies. Since different people may have different explanatory needs, it is important that participant samples in user studies are large enough to represent the target population to enable generalizations. However, it is unclear to what extent XAI researchers reflect on and justify their sample sizes or avoid broad generalizations across people. We analyzed XAI user studies (N = 220) published between 2012 and 2022. Most studies did not offer rationales for their sample sizes. Moreover, most papers generalized their conclusions beyond their target population, and there was no evidence that broader conclusions in quantitative studies were correlated with larger samples. These methodological problems can impede evaluations of whether XAI systems implement the explainability called for in ethical frameworks. We outline princip",
    "path": "papers/23/05/2305.09477.json",
    "total_tokens": 929,
    "translated_title": "解释型AI研究中未经证明的样本量和推广：更具包容性用户研究的原则",
    "translated_abstract": "许多伦理框架要求人工智能系统可解释。解释型AI（XAI）模型经常在用户研究中测试其充分性。由于不同的人可能有不同的解释需求，因此参与者样本在用户研究中应足够大，以代表目标人群，以实现推广。然而，尚不清楚XAI研究人员在多大程度上反思和证明其样本量或避免跨人群广泛推广。我们分析了2012年至2022年间发表的220篇XAI用户研究。大多数研究没有提供样本量的理由。此外，大多数论文将其结论推广到目标人群之外，并且没有证据表明定量研究中更广泛的结论与更大的样本有关。这些方法论问题可能会妨碍评估XAI系统是否实现了伦理框架中提出的可解释性。我们概述了更具包容性的XAI研究用户研究原则，以解决这些问题。",
    "tldr": "解释型AI研究中的参与者样本量和推广问题限制了该领域对XAI系统可解释性的评估，需要更具包容性的用户研究原则来解决这些问题。"
}