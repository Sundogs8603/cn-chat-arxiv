{
    "title": "PaD: Program-aided Distillation Specializes Large Models in Reasoning. (arXiv:2305.13888v1 [cs.CL])",
    "abstract": "While Large Language Models (LLMs) excel in several natural language processing tasks, their size and inaccessibility present challenges for extensive practical application. Previous studies acquire specialized skills through distillation on LLMs, which result in trading generic abilities, called model specialization. As for reasoning ability, chain-of-thought was synthesized to subsequent distillation. However, due to hallucination, synthetic chain-of-thought from LLMs contains faulty reasoning. These incorrect reasoning steps damage the reasoning capability. To tackle above issues, we propose Program-aided Distillation (PaD), which distills LLMs to obtain specialized small models in reasoning tasks. In PaD, we strengthen specialized models with program-aided reasoning, and help them overcome faulty reasoning steps with automated error checking. Experimental results demonstrate that, on the GSM8K benchmark, a 0.06B model using PaD can not only outperform certain LLMs (e.g., LLaMA), bu",
    "link": "http://arxiv.org/abs/2305.13888",
    "context": "Title: PaD: Program-aided Distillation Specializes Large Models in Reasoning. (arXiv:2305.13888v1 [cs.CL])\nAbstract: While Large Language Models (LLMs) excel in several natural language processing tasks, their size and inaccessibility present challenges for extensive practical application. Previous studies acquire specialized skills through distillation on LLMs, which result in trading generic abilities, called model specialization. As for reasoning ability, chain-of-thought was synthesized to subsequent distillation. However, due to hallucination, synthetic chain-of-thought from LLMs contains faulty reasoning. These incorrect reasoning steps damage the reasoning capability. To tackle above issues, we propose Program-aided Distillation (PaD), which distills LLMs to obtain specialized small models in reasoning tasks. In PaD, we strengthen specialized models with program-aided reasoning, and help them overcome faulty reasoning steps with automated error checking. Experimental results demonstrate that, on the GSM8K benchmark, a 0.06B model using PaD can not only outperform certain LLMs (e.g., LLaMA), bu",
    "path": "papers/23/05/2305.13888.json",
    "total_tokens": 1003,
    "translated_title": "PaD: 程序辅助蒸馏专注于推理的大型模型",
    "translated_abstract": "尽管大型语言模型（LLMs）在几个自然语言处理任务中表现优异，但它们的大小和不可访问性对于广泛的实际应用仍然存在挑战。先前的研究通过对LLMs进行精炼以获取专业技能，在商业场景中实现了通用能力的交换，称为模型专业化。对于推理能力，公司已合成用于后续提炼的思维链。但是，由于幻觉，LLMs的合成思维链包含错误推理，这些不正确的推理步骤损害了推理能力。为了解决上述问题，我们提出了程序辅助蒸馏（PaD），它可以蒸馏LLMs以在推理任务中获得专业化的小模型。在PaD中，我们使用程序辅助推理加强专业化模型，并通过自动化错误检查来帮助它们克服错误的推理步骤。实验结果表明，在GSM8K基准测试中，使用PaD的0.06B模型不仅可以胜过某些LLMs（例如LLaMA），而且还可以取得比其他模型更好的性能。",
    "tldr": "本文提出了一种程序辅助蒸馏（PaD）技术，它可以蒸馏大型语言模型（LLMs）以在推理任务中获得专业化的小模型。PaD使用程序辅助推理加强专业化模型，并通过自动化错误检查来帮助它们克服错误的推理步骤。",
    "en_tdlr": "This paper proposes a Program-aided Distillation (PaD) technique, which distills Large Language Models (LLMs) to obtain specialized small models in reasoning tasks. PaD strengthens specialized models with program-aided reasoning and helps them overcome faulty reasoning steps with automated error checking."
}