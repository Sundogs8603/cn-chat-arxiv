{
    "title": "SFP: Spurious Feature-targeted Pruning for Out-of-Distribution Generalization. (arXiv:2305.11615v1 [cs.LG])",
    "abstract": "Model substructure learning aims to find an invariant network substructure that can have better out-of-distribution (OOD) generalization than the original full structure. Existing works usually search the invariant substructure using modular risk minimization (MRM) with fully exposed out-domain data, which may bring about two drawbacks: 1) Unfairness, due to the dependence of the full exposure of out-domain data; and 2) Sub-optimal OOD generalization, due to the equally feature-untargeted pruning on the whole data distribution. Based on the idea that in-distribution (ID) data with spurious features may have a lower experience risk, in this paper, we propose a novel Spurious Feature-targeted model Pruning framework, dubbed SFP, to automatically explore invariant substructures without referring to the above drawbacks. Specifically, SFP identifies spurious features within ID instances during training using our theoretically verified task loss, upon which, SFP attenuates the corresponding ",
    "link": "http://arxiv.org/abs/2305.11615",
    "context": "Title: SFP: Spurious Feature-targeted Pruning for Out-of-Distribution Generalization. (arXiv:2305.11615v1 [cs.LG])\nAbstract: Model substructure learning aims to find an invariant network substructure that can have better out-of-distribution (OOD) generalization than the original full structure. Existing works usually search the invariant substructure using modular risk minimization (MRM) with fully exposed out-domain data, which may bring about two drawbacks: 1) Unfairness, due to the dependence of the full exposure of out-domain data; and 2) Sub-optimal OOD generalization, due to the equally feature-untargeted pruning on the whole data distribution. Based on the idea that in-distribution (ID) data with spurious features may have a lower experience risk, in this paper, we propose a novel Spurious Feature-targeted model Pruning framework, dubbed SFP, to automatically explore invariant substructures without referring to the above drawbacks. Specifically, SFP identifies spurious features within ID instances during training using our theoretically verified task loss, upon which, SFP attenuates the corresponding ",
    "path": "papers/23/05/2305.11615.json",
    "total_tokens": 1009,
    "translated_title": "SFP: 针对伪特征的修剪方法，用于识别无分布概括问题",
    "translated_abstract": "模型子结构学习旨在找到一个不变的网络子结构，可以比原始的完整结构更好地进行超出分布范围（OOD）概括。现有的工作通常使用完全暴露的域外数据来搜索不变的子结构，从而可能带来两个缺点：1）不公平，因为完全暴露出域外数据的依赖性；和2）次优的OOD概括，由于对整个数据分布进行了同样的特征未命中修剪。基于ID数据中的伪特征可能具有更低的体验风险的想法，在本文中，我们提出了一种新的伪特征定向的模型修剪框架，称为SFP，以自动探索不变的子结构，而不考虑上述缺点。具体而言，SFP在培训过程中使用我们在理论上验证的任务丢失识别ID实例中的伪特征，基于此，SFP减弱了相应的特征作用，以提高OOD泛化能力。",
    "tldr": "SFP提出了一种针对模型子结构的修剪框架，SFP可以自动探索不变的子结构，而不考虑对完全暴露于域外数据的依赖性以及对整个数据分布进行同样特征未命中修剪带来的缺点。这种方法的核心在于，利用ID数据中的伪特征来降低风险。",
    "en_tdlr": "SFP proposes a pruning framework for model substructures that can automatically explore invariant substructures without relying on the dependence on fully exposed out-domain data or equally feature-untargeted pruning on the whole data distribution. The method is based on the idea that ID data with spurious features may have a lower experience risk, and uses this information to attenuate the corresponding features and improve OOD generalization."
}