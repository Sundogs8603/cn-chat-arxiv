{
    "title": "Improving Language Model Negotiation with Self-Play and In-Context Learning from AI Feedback. (arXiv:2305.10142v1 [cs.CL])",
    "abstract": "We study whether multiple large language models (LLMs) can autonomously improve each other in a negotiation game by playing, reflecting, and criticizing. We are interested in this question because if LLMs were able to improve each other, it would imply the possibility of creating strong AI agents with minimal human intervention. We ask two LLMs to negotiate with each other, playing the roles of a buyer and a seller, respectively. They aim to reach a deal with the buyer targeting a lower price and the seller a higher one. A third language model, playing the critic, provides feedback to a player to improve the player's negotiation strategies. We let the two agents play multiple rounds, using previous negotiation history and AI feedback as in-context demonstrations to improve the model's negotiation strategy iteratively. We use different LLMs (GPT and Claude) for different roles and use the deal price as the evaluation metric. Our experiments reveal multiple intriguing findings: (1) Only ",
    "link": "http://arxiv.org/abs/2305.10142",
    "context": "Title: Improving Language Model Negotiation with Self-Play and In-Context Learning from AI Feedback. (arXiv:2305.10142v1 [cs.CL])\nAbstract: We study whether multiple large language models (LLMs) can autonomously improve each other in a negotiation game by playing, reflecting, and criticizing. We are interested in this question because if LLMs were able to improve each other, it would imply the possibility of creating strong AI agents with minimal human intervention. We ask two LLMs to negotiate with each other, playing the roles of a buyer and a seller, respectively. They aim to reach a deal with the buyer targeting a lower price and the seller a higher one. A third language model, playing the critic, provides feedback to a player to improve the player's negotiation strategies. We let the two agents play multiple rounds, using previous negotiation history and AI feedback as in-context demonstrations to improve the model's negotiation strategy iteratively. We use different LLMs (GPT and Claude) for different roles and use the deal price as the evaluation metric. Our experiments reveal multiple intriguing findings: (1) Only ",
    "path": "papers/23/05/2305.10142.json",
    "total_tokens": 886,
    "translated_title": "自我博弈与人工智能反馈中的上下文学习改进语言模型的谈判策略",
    "translated_abstract": "本文研究了多个大型语言模型（LLM）能否通过玩耍、反思和批评在谈判游戏中彼此自主改进。如果LLM能够相互提高，则意味着可以在最小人工干预的情况下创建强大的人工智能代理。我们让两个LLM扮演买方和卖方角色进行协商，第三个LLM扮演批评家，为一方提供反馈以改进其谈判策略。我们使用历史交易记录和人工智能反馈作为上下文演示来迭代地改进模型的谈判策略。我们使用不同的LLM（GPT和Claude）来扮演不同的角色，并使用交易价格作为评估指标。实验揭示出多个有趣的发现。",
    "tldr": "本文研究了多个大型语言模型能否通过自我博弈和反馈互相提高，在谈判游戏中进行谈判，达成交易。使用历史记录和人工智能反馈迭代改进模型的谈判策略。"
}