{
    "title": "Probabilistic Modeling: Proving the Lottery Ticket Hypothesis in Spiking Neural Network. (arXiv:2305.12148v1 [cs.LG])",
    "abstract": "The Lottery Ticket Hypothesis (LTH) states that a randomly-initialized large neural network contains a small sub-network (i.e., winning tickets) which, when trained in isolation, can achieve comparable performance to the large network. LTH opens up a new path for network pruning. Existing proofs of LTH in Artificial Neural Networks (ANNs) are based on continuous activation functions, such as ReLU, which satisfying the Lipschitz condition. However, these theoretical methods are not applicable in Spiking Neural Networks (SNNs) due to the discontinuous of spiking function. We argue that it is possible to extend the scope of LTH by eliminating Lipschitz condition. Specifically, we propose a novel probabilistic modeling approach for spiking neurons with complicated spatio-temporal dynamics. Then we theoretically and experimentally prove that LTH holds in SNNs. According to our theorem, we conclude that pruning directly in accordance with the weight size in existing SNNs is clearly not optim",
    "link": "http://arxiv.org/abs/2305.12148",
    "context": "Title: Probabilistic Modeling: Proving the Lottery Ticket Hypothesis in Spiking Neural Network. (arXiv:2305.12148v1 [cs.LG])\nAbstract: The Lottery Ticket Hypothesis (LTH) states that a randomly-initialized large neural network contains a small sub-network (i.e., winning tickets) which, when trained in isolation, can achieve comparable performance to the large network. LTH opens up a new path for network pruning. Existing proofs of LTH in Artificial Neural Networks (ANNs) are based on continuous activation functions, such as ReLU, which satisfying the Lipschitz condition. However, these theoretical methods are not applicable in Spiking Neural Networks (SNNs) due to the discontinuous of spiking function. We argue that it is possible to extend the scope of LTH by eliminating Lipschitz condition. Specifically, we propose a novel probabilistic modeling approach for spiking neurons with complicated spatio-temporal dynamics. Then we theoretically and experimentally prove that LTH holds in SNNs. According to our theorem, we conclude that pruning directly in accordance with the weight size in existing SNNs is clearly not optim",
    "path": "papers/23/05/2305.12148.json",
    "total_tokens": 923,
    "translated_title": "概率建模：在脉冲神经网络中证明中彩票假设",
    "translated_abstract": "中彩票假设（LTH）指出，随机初始化的大型神经网络包含一个小的子网络（即中奖券） ，当该子网络单独训练时，可以达到与大型网络相当的性能。 LTH为网络修剪开辟了一条新路。现有的人工神经网络（ANN）中的LTH证明是基于连续激活函数（如ReLU），这些函数满足Lipschitz条件。 然而，由于脉冲函数的不连续性，这些理论方法在脉冲神经网络（SNN）中不适用。我们认为可以通过消除Lipschitz条件来扩展LTH的范围。具体而言，我们提出了一种新的概率建模方法，用于处理具有复杂时空动态的脉冲神经元。然后，我们从理论和实验上证明了LTH在SNN中成立。根据我们的定理，我们得出结论，根据现有SNN中的权重大小进行修剪显然不是最优的方法。",
    "tldr": "本研究提出了一种新的概率建模方法，证明了中彩票假设在脉冲神经网络中的适用性，并指出根据现有SNN中的权重大小进行修剪不是最优的方法。"
}