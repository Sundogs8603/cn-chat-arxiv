{
    "title": "GRILL: Grounded Vision-language Pre-training via Aligning Text and Image Regions. (arXiv:2305.14676v1 [cs.CL])",
    "abstract": "Generalization to unseen tasks is an important ability for few-shot learners to achieve better zero-/few-shot performance on diverse tasks. However, such generalization to vision-language tasks including grounding and generation tasks has been under-explored; existing few-shot VL models struggle to handle tasks that involve object grounding and multiple images such as visual commonsense reasoning or NLVR2. In this paper, we introduce GRILL, GRounded vIsion Language aLigning, a novel VL model that can be generalized to diverse tasks including visual question answering, captioning, and grounding tasks with no or very few training instances. Specifically, GRILL learns object grounding and localization by exploiting object-text alignments, which enables it to transfer to grounding tasks in a zero-/few-shot fashion. We evaluate our model on various zero-/few-shot VL tasks and show that it consistently surpasses the state-of-the-art few-shot methods.",
    "link": "http://arxiv.org/abs/2305.14676",
    "context": "Title: GRILL: Grounded Vision-language Pre-training via Aligning Text and Image Regions. (arXiv:2305.14676v1 [cs.CL])\nAbstract: Generalization to unseen tasks is an important ability for few-shot learners to achieve better zero-/few-shot performance on diverse tasks. However, such generalization to vision-language tasks including grounding and generation tasks has been under-explored; existing few-shot VL models struggle to handle tasks that involve object grounding and multiple images such as visual commonsense reasoning or NLVR2. In this paper, we introduce GRILL, GRounded vIsion Language aLigning, a novel VL model that can be generalized to diverse tasks including visual question answering, captioning, and grounding tasks with no or very few training instances. Specifically, GRILL learns object grounding and localization by exploiting object-text alignments, which enables it to transfer to grounding tasks in a zero-/few-shot fashion. We evaluate our model on various zero-/few-shot VL tasks and show that it consistently surpasses the state-of-the-art few-shot methods.",
    "path": "papers/23/05/2305.14676.json",
    "total_tokens": 866,
    "translated_title": "GRILL：基于文本和图像区域对齐的基于场景的视觉语言预训练模型",
    "translated_abstract": "少样本学习器的泛化能力是实现更好的零/少样本性能的重要能力，但这种泛化适用于包括基础和生产任务在内的视觉语言任务一直未得到充分探索。本文介绍了GRILL，一种新型视觉语言模型，可以推广到包括视觉问答、标题和基础任务在内的各种任务，其训练样本很少或没有样本。具体而言，GRILL通过利用对象-文本对齐来学习对象的定位，这使其能够以零/少样本的方式转移到基础任务。我们在各种零/少样本VL任务上评估了我们的模型，并展示了它始终优于现有的少样本方法。",
    "tldr": "GRILL是一种基于文本和图像区域对齐的基于场景的视觉语言预训练模型，它可以在训练样本很少或没有样本的情况下用于各种视觉语言任务，并且在各种零/少样本VL任务上表现更好。",
    "en_tdlr": "GRILL is a novel grounded vision language model that can be generalized to diverse tasks including visual question answering, captioning, and grounding tasks with no or very few training instances by exploiting object-text alignments. It consistently surpasses the state-of-the-art few-shot methods in various zero-/few-shot VL tasks."
}