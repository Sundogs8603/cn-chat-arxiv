{
    "title": "Improving Textless Spoken Language Understanding with Discrete Units as Intermediate Target. (arXiv:2305.18096v2 [cs.CL] UPDATED)",
    "abstract": "Spoken Language Understanding (SLU) is a task that aims to extract semantic information from spoken utterances. Previous research has made progress in end-to-end SLU by using paired speech-text data, such as pre-trained Automatic Speech Recognition (ASR) models or paired text as intermediate targets. However, acquiring paired transcripts is expensive and impractical for unwritten languages. On the other hand, Textless SLU extracts semantic information from speech without utilizing paired transcripts. However, the absence of intermediate targets and training guidance for textless SLU often results in suboptimal performance. In this work, inspired by the content-disentangled discrete units from self-supervised speech models, we proposed to use discrete units as intermediate guidance to improve textless SLU performance. Our method surpasses the baseline method on five SLU benchmark corpora. Additionally, we find that unit guidance facilitates few-shot learning and enhances the model's abi",
    "link": "http://arxiv.org/abs/2305.18096",
    "context": "Title: Improving Textless Spoken Language Understanding with Discrete Units as Intermediate Target. (arXiv:2305.18096v2 [cs.CL] UPDATED)\nAbstract: Spoken Language Understanding (SLU) is a task that aims to extract semantic information from spoken utterances. Previous research has made progress in end-to-end SLU by using paired speech-text data, such as pre-trained Automatic Speech Recognition (ASR) models or paired text as intermediate targets. However, acquiring paired transcripts is expensive and impractical for unwritten languages. On the other hand, Textless SLU extracts semantic information from speech without utilizing paired transcripts. However, the absence of intermediate targets and training guidance for textless SLU often results in suboptimal performance. In this work, inspired by the content-disentangled discrete units from self-supervised speech models, we proposed to use discrete units as intermediate guidance to improve textless SLU performance. Our method surpasses the baseline method on five SLU benchmark corpora. Additionally, we find that unit guidance facilitates few-shot learning and enhances the model's abi",
    "path": "papers/23/05/2305.18096.json",
    "total_tokens": 916,
    "translated_title": "用离散单元作为中间目标提高无文本语音理解技术的能力",
    "translated_abstract": "语音理解（SLU）是一项旨在从口语中提取语义信息的任务。先前的研究通过使用配对的语音转文本数据，如预训练的自动语音识别（ASR）模型或配对文本作为中间目标，在端到端SLU上取得了进展。然而，获取配对的文本转录对于无书面语言来说是昂贵且不实际的。另一方面，无文本SLU从语音中提取语义信息而不使用配对的文本。然而，无文本SLU缺乏中间目标和训练指导常常导致表现不佳。在本研究中，受到自监督语音模型中内容分解离散单元的启发，我们提出使用离散单元作为中间指导，以提高无文本SLU的性能。我们的方法在五个SLU基准数据集上超过了基准方法。此外，我们发现单元指导有助于小样本学习，并增强了模型的能力。",
    "tldr": "本研究提出使用离散单元作为中间指导，以提高无文本语音理解技术的性能。在五个基准数据集上，我们的方法超过了基准方法，并发现单元指导有助于小样本学习和提升模型能力。",
    "en_tdlr": "This research proposes the use of discrete units as intermediate guidance to improve the performance of textless spoken language understanding. Our method surpasses the baseline on five benchmark datasets and we find that unit guidance facilitates few-shot learning and enhances model capability."
}