{
    "title": "Cheaply Evaluating Inference Efficiency Metrics for Autoregressive Transformer APIs. (arXiv:2305.02440v1 [cs.LG])",
    "abstract": "Large language models (LLMs) power many state-of-the-art systems in natural language processing. However, these models are extremely computationally expensive, even at inference time, raising the natural question: when is the extra cost of deploying a larger model worth the anticipated boost in capabilities? Better understanding this tradeoff fundamentally could benefit from an inference efficiency metric that is both (i) easily comparable across models from different providers, and (ii) representative of the true cost of running queries in an isolated performance environment. Unfortunately, access to LLMs today is largely restricted to black-box text generation APIs and raw runtimes measured through this interface do not satisfy these desiderata: model providers can apply various software and hardware optimizations orthogonal to the model, and models served on shared infrastructure are susceptible to performance contention. To circumvent these problems, we propose a new metric for com",
    "link": "http://arxiv.org/abs/2305.02440",
    "context": "Title: Cheaply Evaluating Inference Efficiency Metrics for Autoregressive Transformer APIs. (arXiv:2305.02440v1 [cs.LG])\nAbstract: Large language models (LLMs) power many state-of-the-art systems in natural language processing. However, these models are extremely computationally expensive, even at inference time, raising the natural question: when is the extra cost of deploying a larger model worth the anticipated boost in capabilities? Better understanding this tradeoff fundamentally could benefit from an inference efficiency metric that is both (i) easily comparable across models from different providers, and (ii) representative of the true cost of running queries in an isolated performance environment. Unfortunately, access to LLMs today is largely restricted to black-box text generation APIs and raw runtimes measured through this interface do not satisfy these desiderata: model providers can apply various software and hardware optimizations orthogonal to the model, and models served on shared infrastructure are susceptible to performance contention. To circumvent these problems, we propose a new metric for com",
    "path": "papers/23/05/2305.02440.json",
    "total_tokens": 887,
    "translated_title": "廉价评估自回归Transformer API推断效率度量",
    "translated_abstract": "大型语言模型(LMM)在自然语言处理的很多最先进系统中发挥着作用。然而，即使在推理时，这些模型也非常计算密集，这引发了一个自然的问题: 部署更大的模型的额外成本是否值得预期的能力提升?更好地理解这种权衡需要一个推理效率度量，它既可以在来自不同供应商的模型之间轻松比较，又可以代表在隔离性能环境中运行查询的真实成本。但是，今天访问LLMs在很大程度上仅限于黑匣子文本生成API，通过此接口测量的原始运行时间不能满足这些愿望:模型提供者可以应用与模型不相关的各种软件和硬件优化，而在共享基础设施上提供的模型容易受到性能争用的影响。为了解决这些问题，我们提出了一种新的度量方法，用于比较在用于自回归Transformer API中服务的模型上运行查询的相关成本。",
    "tldr": "本文提出了一个新的度量方法，用于比较Transformer API上模型推理效率的相关成本，以解决现有度量方法在软件和硬件优化和共享基础设施方面的缺陷。",
    "en_tdlr": "This paper proposes a new metric for comparing the related cost of inference efficiency in Transformer APIs, in order to overcome the limitations of current metrics in terms of software and hardware optimizations and shared infrastructures."
}