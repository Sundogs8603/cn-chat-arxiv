{
    "title": "Off-Policy RL Algorithms Can be Sample-Efficient for Continuous Control via Sample Multiple Reuse. (arXiv:2305.18443v1 [cs.LG])",
    "abstract": "Sample efficiency is one of the most critical issues for online reinforcement learning (RL). Existing methods achieve higher sample efficiency by adopting model-based methods, Q-ensemble, or better exploration mechanisms. We, instead, propose to train an off-policy RL agent via updating on a fixed sampled batch multiple times, thus reusing these samples and better exploiting them within a single optimization loop. We name our method sample multiple reuse (SMR). We theoretically show the properties of Q-learning with SMR, e.g., convergence. Furthermore, we incorporate SMR with off-the-shelf off-policy RL algorithms and conduct experiments on a variety of continuous control benchmarks. Empirical results show that SMR significantly boosts the sample efficiency of the base methods across most of the evaluated tasks without any hyperparameter tuning or additional tricks.",
    "link": "http://arxiv.org/abs/2305.18443",
    "context": "Title: Off-Policy RL Algorithms Can be Sample-Efficient for Continuous Control via Sample Multiple Reuse. (arXiv:2305.18443v1 [cs.LG])\nAbstract: Sample efficiency is one of the most critical issues for online reinforcement learning (RL). Existing methods achieve higher sample efficiency by adopting model-based methods, Q-ensemble, or better exploration mechanisms. We, instead, propose to train an off-policy RL agent via updating on a fixed sampled batch multiple times, thus reusing these samples and better exploiting them within a single optimization loop. We name our method sample multiple reuse (SMR). We theoretically show the properties of Q-learning with SMR, e.g., convergence. Furthermore, we incorporate SMR with off-the-shelf off-policy RL algorithms and conduct experiments on a variety of continuous control benchmarks. Empirical results show that SMR significantly boosts the sample efficiency of the base methods across most of the evaluated tasks without any hyperparameter tuning or additional tricks.",
    "path": "papers/23/05/2305.18443.json",
    "total_tokens": 866,
    "translated_title": "基于多次采样重复利用的离线强化学习算法可在连续控制中实现高样本利用效率",
    "translated_abstract": "样本效率是在线强化学习中最关键的问题之一。现有方法通过采用基于模型、Q-ensemble或更好的探索机制来实现更高的样本利用效率。相反，我们提出通过多次更新单个优化循环中的固定采样批次，从而重复使用这些样本并更好地利用它们来训练离线强化学习智能体。我们将我们的方法命名为“sample multiple reuse”（SMR）。我们从理论上展示了Q-learning与SMR的属性，例如收敛性。此外，我们将SMR与现有的离线强化学习算法结合起来，并在各种连续控制基准测试上进行实验。实证结果表明，SMR显着提高了大多数评估任务的基础方法的样本利用效率，并且没有超参数调整或其他技巧。",
    "tldr": "本文提出了一种基于多次采样重复利用的离线强化学习算法，称为SMR。理论和实验证明SMR可以提高离线强化学习算法在连续控制问题中的样本利用效率。",
    "en_tdlr": "The paper proposes an off-policy RL algorithm called SMR that can improve the sample efficiency of existing methods by reusing fixed sampled batches. Theoretical and empirical results show that SMR can significantly boost the sample efficiency of off-policy RL algorithms in continuous control tasks without hyperparameter tuning or additional tricks."
}