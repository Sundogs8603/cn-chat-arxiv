{
    "title": "When Gradient Descent Meets Derivative-Free Optimization: A Match Made in Black-Box Scenario. (arXiv:2305.10013v1 [cs.CL])",
    "abstract": "Large pre-trained language models (PLMs) have garnered significant attention for their versatility and potential for solving a wide spectrum of natural language processing (NLP) tasks. However, the cost of running these PLMs may be prohibitive. Furthermore, PLMs may not be open-sourced due to commercial considerations and potential risks of misuse, such as GPT-3. The parameters and gradients of PLMs are unavailable in this scenario. To solve the issue, black-box tuning has been proposed, which utilizes derivative-free optimization (DFO), instead of gradient descent, for training task-specific continuous prompts. However, these gradient-free methods still exhibit a significant gap compared to gradient-based methods. In this paper, we introduce gradient descent into black-box tuning scenario through knowledge distillation. Furthermore, we propose a novel method GDFO, which integrates gradient descent and derivative-free optimization to optimize task-specific continuous prompts in a harmo",
    "link": "http://arxiv.org/abs/2305.10013",
    "context": "Title: When Gradient Descent Meets Derivative-Free Optimization: A Match Made in Black-Box Scenario. (arXiv:2305.10013v1 [cs.CL])\nAbstract: Large pre-trained language models (PLMs) have garnered significant attention for their versatility and potential for solving a wide spectrum of natural language processing (NLP) tasks. However, the cost of running these PLMs may be prohibitive. Furthermore, PLMs may not be open-sourced due to commercial considerations and potential risks of misuse, such as GPT-3. The parameters and gradients of PLMs are unavailable in this scenario. To solve the issue, black-box tuning has been proposed, which utilizes derivative-free optimization (DFO), instead of gradient descent, for training task-specific continuous prompts. However, these gradient-free methods still exhibit a significant gap compared to gradient-based methods. In this paper, we introduce gradient descent into black-box tuning scenario through knowledge distillation. Furthermore, we propose a novel method GDFO, which integrates gradient descent and derivative-free optimization to optimize task-specific continuous prompts in a harmo",
    "path": "papers/23/05/2305.10013.json",
    "total_tokens": 968,
    "translated_title": "当梯度下降遇到无导数优化：黑盒场景下的完美组合",
    "translated_abstract": "大型预训练语言模型（PLMs）因其多功能性和解决广泛自然语言处理（NLP）任务的潜力而备受关注。然而，运行这些PLMs的成本可能是禁止的。此外，由于商业考虑和潜在的误用风险（例如GPT-3），PLMs可能未开放源代码。在这种情况下，无导数优化（DFO）提出了黑盒调整的解决方案，用于训练任务特定的连续提示，而不是使用梯度下降。然而，与基于梯度的方法相比，这些无梯度方法仍然存在显着差距。本文通过知识蒸馏将梯度下降引入黑盒调整场景，并提出了一种新的方法GDFO，将梯度下降和无导数优化融合到一起，以协调的方式优化任务特定的连续提示。我们在各种NLP任务上进行了广泛的实验，并展示了我们提出的方法优于现有的无梯度和基于梯度的方法。",
    "tldr": "本文介绍了一种新方法GDFO，将梯度下降和无导数优化结合在一起，协调地优化任务特定的连续提示。实验证明，该方法优于现有的无梯度和基于梯度的方法。",
    "en_tdlr": "The paper introduces a novel method, GDFO, that integrates gradient descent and derivative-free optimization to optimize task-specific continuous prompts in a harmonious way, which outperforms existing gradient-free and gradient-based methods based on extensive experiments on a range of NLP tasks."
}