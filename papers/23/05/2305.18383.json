{
    "title": "A Three-regime Model of Network Pruning. (arXiv:2305.18383v1 [stat.ML])",
    "abstract": "Recent work has highlighted the complex influence training hyperparameters, e.g., the number of training epochs, can have on the prunability of machine learning models. Perhaps surprisingly, a systematic approach to predict precisely how adjusting a specific hyperparameter will affect prunability remains elusive. To address this gap, we introduce a phenomenological model grounded in the statistical mechanics of learning. Our approach uses temperature-like and load-like parameters to model the impact of neural network (NN) training hyperparameters on pruning performance. A key empirical result we identify is a sharp transition phenomenon: depending on the value of a load-like parameter in the pruned model, increasing the value of a temperature-like parameter in the pre-pruned model may either enhance or impair subsequent pruning performance. Based on this transition, we build a three-regime model by taxonomizing the global structure of the pruned NN loss landscape. Our model reveals tha",
    "link": "http://arxiv.org/abs/2305.18383",
    "context": "Title: A Three-regime Model of Network Pruning. (arXiv:2305.18383v1 [stat.ML])\nAbstract: Recent work has highlighted the complex influence training hyperparameters, e.g., the number of training epochs, can have on the prunability of machine learning models. Perhaps surprisingly, a systematic approach to predict precisely how adjusting a specific hyperparameter will affect prunability remains elusive. To address this gap, we introduce a phenomenological model grounded in the statistical mechanics of learning. Our approach uses temperature-like and load-like parameters to model the impact of neural network (NN) training hyperparameters on pruning performance. A key empirical result we identify is a sharp transition phenomenon: depending on the value of a load-like parameter in the pruned model, increasing the value of a temperature-like parameter in the pre-pruned model may either enhance or impair subsequent pruning performance. Based on this transition, we build a three-regime model by taxonomizing the global structure of the pruned NN loss landscape. Our model reveals tha",
    "path": "papers/23/05/2305.18383.json",
    "total_tokens": 948,
    "translated_title": "神经网络修剪的三重模型",
    "translated_abstract": "最近的研究强调了训练超参数（例如训练轮数）对机器学习模型修剪的影响，然而如何精确预测调整某一特定超参数对修剪的影响仍具有挑战性。为了解决这个问题，本文提出了一种基于学习统计力学的现象学模型，使用类似温度和负载的模型参数来建模神经网络训练超参数对修剪性能的影响。我们发现了一个关键的实证结果：根据修剪后的模型中的一种负载类参数的值，当增加修剪前模型中一种类似温度的参数的值时，修剪性能可能会得到优化或损害。基于这种转变，我们通过分类修剪后神经网络损失景观的全局结构构建了一个三重模型。该模型揭示了修剪的优化过程以及与修剪相关的神经网络损失景观的变化规律。",
    "tldr": "该论文提出了一种基于学习统计力学的现象学模型，使用类似温度和负载的模型参数来建模神经网络训练超参数对修剪性能的影响，通过分类修剪后神经网络损失景观的全局结构构建了一个三重模型，揭示了修剪的优化过程以及对神经网络损失景观的变化规律。",
    "en_tdlr": "This paper proposes a phenomenological model based on statistical mechanics of learning to predict the impact of neural network training hyperparameters on pruning performance. The three-regime model is built by classifying the global structure of pruned neural network loss landscape, revealing the optimization process and the change patterns in loss landscape during pruning."
}