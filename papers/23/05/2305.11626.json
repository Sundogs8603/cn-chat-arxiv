{
    "title": "CCT-Code: Cross-Consistency Training for Multilingual Clone Detection and Code Search. (arXiv:2305.11626v1 [cs.CL])",
    "abstract": "We consider the clone detection and information retrieval problems for source code, well-known tasks important for any programming language. Although it is also an important and interesting problem to find code snippets that operate identically but are written in different programming languages, to the best of our knowledge multilingual clone detection has not been studied in literature. In this work, we formulate the multilingual clone detection problem and present XCD, a new benchmark dataset produced from the CodeForces submissions dataset. Moreover, we present a novel training procedure, called cross-consistency training (CCT), that we apply to train language models on source code in different programming languages. The resulting CCT-LM model, initialized with GraphCodeBERT and fine-tuned with CCT, achieves new state of the art, outperforming existing approaches on the POJ-104 clone detection benchmark with 95.67\\% MAP and AdvTest code search benchmark with 47.18\\% MRR; it also sho",
    "link": "http://arxiv.org/abs/2305.11626",
    "context": "Title: CCT-Code: Cross-Consistency Training for Multilingual Clone Detection and Code Search. (arXiv:2305.11626v1 [cs.CL])\nAbstract: We consider the clone detection and information retrieval problems for source code, well-known tasks important for any programming language. Although it is also an important and interesting problem to find code snippets that operate identically but are written in different programming languages, to the best of our knowledge multilingual clone detection has not been studied in literature. In this work, we formulate the multilingual clone detection problem and present XCD, a new benchmark dataset produced from the CodeForces submissions dataset. Moreover, we present a novel training procedure, called cross-consistency training (CCT), that we apply to train language models on source code in different programming languages. The resulting CCT-LM model, initialized with GraphCodeBERT and fine-tuned with CCT, achieves new state of the art, outperforming existing approaches on the POJ-104 clone detection benchmark with 95.67\\% MAP and AdvTest code search benchmark with 47.18\\% MRR; it also sho",
    "path": "papers/23/05/2305.11626.json",
    "total_tokens": 867,
    "translated_title": "CCT-Code：面向多语言克隆检测和代码搜索的跨语言一致性训练",
    "translated_abstract": "本文考虑源代码的克隆检测和信息检索问题，这两个问题对于任何编程语言都非常重要。我们提出了多语言克隆检测问题，并从CodeForces提交数据集产生了一个新的基准数据集XCD。此外，我们提出了一种新型的训练方法，称为跨语言一致性训练（CCT），用于在不同的编程语言中训练语言模型，进而得到基于CCT-LM 模型。该模型继承了GraphCodeBERT并用CCT微调，达到了95.67\\% MAP和47.18\\% MRR的性能，成功创造了新的最优结果。",
    "tldr": "本研究提出了多语言克隆检测问题，并从CodeForces数据集开发了一个新的基准数据集XCD。我们使用跨语言一致性训练（CCT）方法训练了语言模型，得到了具有新颖性能的CCT-LM模型，超过了现有的方法。",
    "en_tdlr": "This study introduces multilingual clone detection problem and produces a new benchmark dataset XCD from CodeForces. The authors propose a novel training method, cross-consistency training (CCT), to train language models and develop a new CCT-LM model, which outperforms existing approaches with new state-of-the-art performance on the POJ-104 clone detection benchmark and AdvTest code search benchmark."
}