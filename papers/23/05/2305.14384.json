{
    "title": "Adversarial Nibbler: A Data-Centric Challenge for Improving the Safety of Text-to-Image Models. (arXiv:2305.14384v1 [cs.LG])",
    "abstract": "The generative AI revolution in recent years has been spurred by an expansion in compute power and data quantity, which together enable extensive pre-training of powerful text-to-image (T2I) models. With their greater capabilities to generate realistic and creative content, these T2I models like DALL-E, MidJourney, Imagen or Stable Diffusion are reaching ever wider audiences. Any unsafe behaviors inherited from pretraining on uncurated internet-scraped datasets thus have the potential to cause wide-reaching harm, for example, through generated images which are violent, sexually explicit, or contain biased and derogatory stereotypes. Despite this risk of harm, we lack systematic and structured evaluation datasets to scrutinize model behavior, especially adversarial attacks that bypass existing safety filters. A typical bottleneck in safety evaluation is achieving a wide coverage of different types of challenging examples in the evaluation set, i.e., identifying 'unknown unknowns' or lon",
    "link": "http://arxiv.org/abs/2305.14384",
    "context": "Title: Adversarial Nibbler: A Data-Centric Challenge for Improving the Safety of Text-to-Image Models. (arXiv:2305.14384v1 [cs.LG])\nAbstract: The generative AI revolution in recent years has been spurred by an expansion in compute power and data quantity, which together enable extensive pre-training of powerful text-to-image (T2I) models. With their greater capabilities to generate realistic and creative content, these T2I models like DALL-E, MidJourney, Imagen or Stable Diffusion are reaching ever wider audiences. Any unsafe behaviors inherited from pretraining on uncurated internet-scraped datasets thus have the potential to cause wide-reaching harm, for example, through generated images which are violent, sexually explicit, or contain biased and derogatory stereotypes. Despite this risk of harm, we lack systematic and structured evaluation datasets to scrutinize model behavior, especially adversarial attacks that bypass existing safety filters. A typical bottleneck in safety evaluation is achieving a wide coverage of different types of challenging examples in the evaluation set, i.e., identifying 'unknown unknowns' or lon",
    "path": "papers/23/05/2305.14384.json",
    "total_tokens": 965,
    "translated_title": "Adversarial Nibbler: 一种提高文本生成图像模型安全性的数据挑战",
    "translated_abstract": "最近几年，由于计算能力和数据量的扩大，生成式AI革命推动了文本生成图像（T2I）模型的广泛预训练。具备产生逼真而富有创造力的内容，例如DALL-E，MidJourney，Imagen或Stable Diffusion的这些T2I模型已经吸引了越来越多的受众。然而，因为预训练所使用的互联网爬取的未经筛选的数据集中可能存在的不安全行为，这些模型具有潜在的造成广泛伤害的能力，例如生成具有暴力、性暴力、偏见和贬损性的刻板印象的图像。尽管存在此类风险，但我们缺乏系统性和结构化的评估数据集来检查模型行为，特别是对攻击现有安全过滤器的对抗攻击。评估安全性的一个典型瓶颈在于实现评估集合中各种类型具有挑战性实例的广泛覆盖率，即确定“未知的未知”或单个攻击实例不足以展示模型的漏洞。",
    "tldr": "本论文提出了一种数据挑战，旨在提高文本生成图像模型的安全性，以解决目前缺乏系统性和结构化的评估数据集的问题。该数据挑战旨在检查模型的不良行为，特别是对抗攻击。"
}