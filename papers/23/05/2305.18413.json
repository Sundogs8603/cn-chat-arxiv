{
    "title": "Learning to Learn from APIs: Black-Box Data-Free Meta-Learning. (arXiv:2305.18413v1 [cs.LG])",
    "abstract": "Data-free meta-learning (DFML) aims to enable efficient learning of new tasks by meta-learning from a collection of pre-trained models without access to the training data. Existing DFML work can only meta-learn from (i) white-box and (ii) small-scale pre-trained models (iii) with the same architecture, neglecting the more practical setting where the users only have inference access to the APIs with arbitrary model architectures and model scale inside. To solve this issue, we propose a Bi-level Data-free Meta Knowledge Distillation (BiDf-MKD) framework to transfer more general meta knowledge from a collection of black-box APIs to one single meta model. Specifically, by just querying APIs, we inverse each API to recover its training data via a zero-order gradient estimator and then perform meta-learning via a novel bi-level meta knowledge distillation structure, in which we design a boundary query set recovery technique to recover a more informative query set near the decision boundary. ",
    "link": "http://arxiv.org/abs/2305.18413",
    "context": "Title: Learning to Learn from APIs: Black-Box Data-Free Meta-Learning. (arXiv:2305.18413v1 [cs.LG])\nAbstract: Data-free meta-learning (DFML) aims to enable efficient learning of new tasks by meta-learning from a collection of pre-trained models without access to the training data. Existing DFML work can only meta-learn from (i) white-box and (ii) small-scale pre-trained models (iii) with the same architecture, neglecting the more practical setting where the users only have inference access to the APIs with arbitrary model architectures and model scale inside. To solve this issue, we propose a Bi-level Data-free Meta Knowledge Distillation (BiDf-MKD) framework to transfer more general meta knowledge from a collection of black-box APIs to one single meta model. Specifically, by just querying APIs, we inverse each API to recover its training data via a zero-order gradient estimator and then perform meta-learning via a novel bi-level meta knowledge distillation structure, in which we design a boundary query set recovery technique to recover a more informative query set near the decision boundary. ",
    "path": "papers/23/05/2305.18413.json",
    "total_tokens": 812,
    "translated_title": "从API学习学习：黑盒数据无关元学习",
    "translated_abstract": "无数据元学习（DFML）旨在通过从一组预训练模型进行元学习而无需访问训练数据，从而实现高效学习新任务。现有的DFML工作仅能从（i）白盒和（ii）小规模预训练模型（iii）相同的架构中元学习，忽略了更实际的设置，即用户仅能通过任意模型架构和规模的API进行推断。为解决这个问题，我们提出了一个双层数据无关元知识蒸馏（BiDf-MKD）框架，将更通用的元知识从一组黑盒API转移到一个单一的元模型中。",
    "tldr": "该论文提出了一个BiDf-MKD框架，可以从一组API库中无需访问训练数据，直接进行元学习；能够在更广泛的黑盒API上进行元学习，提高了元模型的泛化性能和应用范围。",
    "en_tdlr": "The paper proposes a BiDf-MKD framework that enables meta-learning directly from a collection of APIs without accessing training data, improving the meta-model's generalization performance and applicability on a wider range of black-box APIs."
}