{
    "title": "Synaptic Weight Distributions Depend on the Geometry of Plasticity. (arXiv:2305.19394v1 [q-bio.NC])",
    "abstract": "Most learning algorithms in machine learning rely on gradient descent to adjust model parameters, and a growing literature in computational neuroscience leverages these ideas to study synaptic plasticity in the brain. However, the vast majority of this work ignores a critical underlying assumption: the choice of distance for synaptic changes (i.e. the geometry of synaptic plasticity). Gradient descent assumes that the distance is Euclidean, but many other distances are possible, and there is no reason that biology necessarily uses Euclidean geometry. Here, using the theoretical tools provided by mirror descent, we show that, regardless of the loss being minimized, the distribution of synaptic weights will depend on the geometry of synaptic plasticity. We use these results to show that experimentally-observed log-normal weight distributions found in several brain areas are not consistent with standard gradient descent (i.e. a Euclidean geometry), but rather with non-Euclidean distances.",
    "link": "http://arxiv.org/abs/2305.19394",
    "context": "Title: Synaptic Weight Distributions Depend on the Geometry of Plasticity. (arXiv:2305.19394v1 [q-bio.NC])\nAbstract: Most learning algorithms in machine learning rely on gradient descent to adjust model parameters, and a growing literature in computational neuroscience leverages these ideas to study synaptic plasticity in the brain. However, the vast majority of this work ignores a critical underlying assumption: the choice of distance for synaptic changes (i.e. the geometry of synaptic plasticity). Gradient descent assumes that the distance is Euclidean, but many other distances are possible, and there is no reason that biology necessarily uses Euclidean geometry. Here, using the theoretical tools provided by mirror descent, we show that, regardless of the loss being minimized, the distribution of synaptic weights will depend on the geometry of synaptic plasticity. We use these results to show that experimentally-observed log-normal weight distributions found in several brain areas are not consistent with standard gradient descent (i.e. a Euclidean geometry), but rather with non-Euclidean distances.",
    "path": "papers/23/05/2305.19394.json",
    "total_tokens": 974,
    "translated_title": "突触权重分布取决于可塑性的几何形态",
    "translated_abstract": "机器学习中的大多数学习算法都依赖于梯度下降调整模型参数，计算神经科学中日益增长的文献利用这些思想研究突触可塑性。然而，绝大部分此类研究忽略了一个关键的基本假设：突触变化的距离选择（即突触可塑性的几何形态）。梯度下降假定距离为欧几里得距离，但许多其他距离也是可能的，并且生物学不一定使用欧几里得几何形态。在这里，我们使用镜像下降提供的理论工具表明，无论最小化的损失为何，突触权重的分布都取决于突触可塑性的几何形态。我们利用这些结果表明，在几个大脑区域中发现的实验观测到的对数正态权重分布与标准的梯度下降（即欧几里得几何形态）不一致，而是与非欧几里得距离一致。",
    "tldr": "计算神经科学的研究表明，突触权重分布取决于突触可塑性的几何形态，进而表明实验观测到的对数正态权重分布与标准的梯度下降模型不一致，可能说明大脑中使用的是非欧几里得距离。",
    "en_tdlr": "This computational neuroscience study shows that the distribution of synaptic weights depends on the geometry of synaptic plasticity, indicating that experimentally-observed log-normal weight distributions may not be consistent with standard gradient descent and could suggest the use of non-Euclidian distances in the brain."
}