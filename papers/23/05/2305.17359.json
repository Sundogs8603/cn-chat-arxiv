{
    "title": "DNA-GPT: Divergent N-Gram Analysis for Training-Free Detection of GPT-Generated Text. (arXiv:2305.17359v2 [cs.CL] UPDATED)",
    "abstract": "Large language models (LLMs) have notably enhanced the fluency and diversity of machine-generated text. However, this progress also presents a significant challenge in detecting the origin of a given text, and current research on detection methods lags behind the rapid evolution of LLMs. Conventional training-based methods have limitations in flexibility, particularly when adapting to new domains, and they often lack explanatory power. To address this gap, we propose a novel training-free detection strategy called Divergent N-Gram Analysis (DNA-GPT). Given a text, we first truncate it in the middle and then use only the preceding portion as input to the LLMs to regenerate the new remaining parts. By analyzing the differences between the original and new remaining parts through N-gram analysis in black-box or probability divergence in white-box, we unveil significant discrepancies between the distribution of machine-generated text and the distribution of human-written text. We conducted",
    "link": "http://arxiv.org/abs/2305.17359",
    "context": "Title: DNA-GPT: Divergent N-Gram Analysis for Training-Free Detection of GPT-Generated Text. (arXiv:2305.17359v2 [cs.CL] UPDATED)\nAbstract: Large language models (LLMs) have notably enhanced the fluency and diversity of machine-generated text. However, this progress also presents a significant challenge in detecting the origin of a given text, and current research on detection methods lags behind the rapid evolution of LLMs. Conventional training-based methods have limitations in flexibility, particularly when adapting to new domains, and they often lack explanatory power. To address this gap, we propose a novel training-free detection strategy called Divergent N-Gram Analysis (DNA-GPT). Given a text, we first truncate it in the middle and then use only the preceding portion as input to the LLMs to regenerate the new remaining parts. By analyzing the differences between the original and new remaining parts through N-gram analysis in black-box or probability divergence in white-box, we unveil significant discrepancies between the distribution of machine-generated text and the distribution of human-written text. We conducted",
    "path": "papers/23/05/2305.17359.json",
    "total_tokens": 889,
    "translated_title": "DNA-GPT: 无需训练的Divergent N-Gram分析用于检测GPT生成的文本",
    "translated_abstract": "大型语言模型（LLMs）显着提高了机器生成文本的流畅性和多样性。然而，这一进展也给检测给定文本的来源带来了重大挑战，并且目前的检测方法研究滞后于LLMs的快速发展。传统的基于训练的方法在灵活性方面存在局限性，特别是在适应新领域时，它们往往缺乏解释能力。为了弥补这一差距，我们提出了一种新颖的无需训练的检测策略，称为Divergent N-Gram分析（DNA-GPT）。给定一段文本，我们首先截断其中间部分，然后仅使用前面的部分作为输入来重新生成剩下的部分。通过在黑盒中进行N-gram分析或在白盒中进行概率分布差异分析，我们揭示了机器生成文本的分布与人类写作文本的分布之间的显著差异。我们进行了实证研究...",
    "tldr": "DNA-GPT是一种无需训练的检测策略，通过Divergent N-Gram分析来发现机器生成文本与人类写作文本之间的显著差异。",
    "en_tdlr": "DNA-GPT is a training-free detection strategy that uses Divergent N-Gram analysis to uncover significant differences between machine-generated texts and human-written texts."
}