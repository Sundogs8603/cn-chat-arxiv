{
    "title": "Towards the Flatter Landscape and Better Generalization in Federated Learning under Client-level Differential Privacy. (arXiv:2305.00873v2 [cs.LG] UPDATED)",
    "abstract": "To defend the inference attacks and mitigate the sensitive information leakages in Federated Learning (FL), client-level Differentially Private FL (DPFL) is the de-facto standard for privacy protection by clipping local updates and adding random noise. However, existing DPFL methods tend to make a sharp loss landscape and have poor weight perturbation robustness, resulting in severe performance degradation. To alleviate these issues, we propose a novel DPFL algorithm named DP-FedSAM, which leverages gradient perturbation to mitigate the negative impact of DP. Specifically, DP-FedSAM integrates Sharpness Aware Minimization (SAM) optimizer to generate local flatness models with improved stability and weight perturbation robustness, which results in the small norm of local updates and robustness to DP noise, thereby improving the performance. To further reduce the magnitude of random noise while achieving better performance, we propose DP-FedSAM-$top_k$ by adopting the local update sparsi",
    "link": "http://arxiv.org/abs/2305.00873",
    "context": "Title: Towards the Flatter Landscape and Better Generalization in Federated Learning under Client-level Differential Privacy. (arXiv:2305.00873v2 [cs.LG] UPDATED)\nAbstract: To defend the inference attacks and mitigate the sensitive information leakages in Federated Learning (FL), client-level Differentially Private FL (DPFL) is the de-facto standard for privacy protection by clipping local updates and adding random noise. However, existing DPFL methods tend to make a sharp loss landscape and have poor weight perturbation robustness, resulting in severe performance degradation. To alleviate these issues, we propose a novel DPFL algorithm named DP-FedSAM, which leverages gradient perturbation to mitigate the negative impact of DP. Specifically, DP-FedSAM integrates Sharpness Aware Minimization (SAM) optimizer to generate local flatness models with improved stability and weight perturbation robustness, which results in the small norm of local updates and robustness to DP noise, thereby improving the performance. To further reduce the magnitude of random noise while achieving better performance, we propose DP-FedSAM-$top_k$ by adopting the local update sparsi",
    "path": "papers/23/05/2305.00873.json",
    "total_tokens": 981,
    "translated_title": "支持客户端差分隐私的联邦学习中更平的梯度图和更好的泛化能力",
    "translated_abstract": "为了保护隐私和减少信息泄露，采用客户端级别差分隐私的联邦学习方法已经成为保护用户隐私的标准。然而，目前的方案往往导致尖锐的损失图像和权重扰动的鲁棒性较差，从而导致性能下降。为了缓解这些问题，我们提出了一种新的客户端级别差分隐私联邦学习算法DP-FedSAM，该算法利用梯度扰动来减轻差分隐私的负面影响。DP-FedSAM将锐度感知优化器（SAM）整合到算法中，生成局部平坦模型，从而提高了稳定性和权重扰动鲁棒性，结果产生了局部更新的小范数和对DP噪声的鲁棒性，从而提高了性能。为了进一步减少随机噪声的幅度同时实现更好的性能，我们提出了DP-FedSAM-top_k，采用局部更新稀疏性的方法。",
    "tldr": "提出了一种名为DP-FedSAM的算法，它利用梯度扰动来减轻差分隐私的负面影响，并将锐度感知优化器整合到算法中，生成更平缓的损失曲面和更好的权重扰动鲁棒性，从而提高联邦学习的性能。",
    "en_tdlr": "Proposed a DP-FedSAM algorithm that uses gradient perturbation to mitigate the negative impact of differentially private FL, and integrates a Sharpness Aware Minimization optimizer to generate flatter loss landscapes and weight perturbation robustness, improving federated learning performance."
}