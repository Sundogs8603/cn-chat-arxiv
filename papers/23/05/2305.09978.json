{
    "title": "Stochastic Ratios Tracking Algorithm for Large Scale Machine Learning Problems. (arXiv:2305.09978v1 [cs.LG])",
    "abstract": "Many machine learning applications and tasks rely on the stochastic gradient descent (SGD) algorithm and its variants. Effective step length selection is crucial for the success of these algorithms, which has motivated the development of algorithms such as ADAM or AdaGrad. In this paper, we propose a novel algorithm for adaptive step length selection in the classical SGD framework, which can be readily adapted to other stochastic algorithms. Our proposed algorithm is inspired by traditional nonlinear optimization techniques and is supported by analytical findings. We show that under reasonable conditions, the algorithm produces step lengths in line with well-established theoretical requirements, and generates iterates that converge to a stationary neighborhood of a solution in expectation. We test the proposed algorithm on logistic regressions and deep neural networks and demonstrate that the algorithm can generate step lengths comparable to the best step length obtained from manual tu",
    "link": "http://arxiv.org/abs/2305.09978",
    "context": "Title: Stochastic Ratios Tracking Algorithm for Large Scale Machine Learning Problems. (arXiv:2305.09978v1 [cs.LG])\nAbstract: Many machine learning applications and tasks rely on the stochastic gradient descent (SGD) algorithm and its variants. Effective step length selection is crucial for the success of these algorithms, which has motivated the development of algorithms such as ADAM or AdaGrad. In this paper, we propose a novel algorithm for adaptive step length selection in the classical SGD framework, which can be readily adapted to other stochastic algorithms. Our proposed algorithm is inspired by traditional nonlinear optimization techniques and is supported by analytical findings. We show that under reasonable conditions, the algorithm produces step lengths in line with well-established theoretical requirements, and generates iterates that converge to a stationary neighborhood of a solution in expectation. We test the proposed algorithm on logistic regressions and deep neural networks and demonstrate that the algorithm can generate step lengths comparable to the best step length obtained from manual tu",
    "path": "papers/23/05/2305.09978.json",
    "total_tokens": 850,
    "translated_title": "大规模机器学习问题的随机比率跟踪算法",
    "translated_abstract": "许多机器学习应用和任务都依赖于随机梯度下降（SGD）算法及其变体。有效的步长选择对算法的成功至关重要，这促进了诸如ADAM或AdaGrad之类的算法的发展。在本文中，我们提出了一种新颖的算法，在经典的SGD框架下实现自适应步长选择，它可以轻松适应其他随机算法。我们的算法灵感来自传统的非线性优化技术，并受到分析发现的支持。我们展示了在合理条件下，该算法产生符合良好理论要求的步长，并在期望下生成收敛于解的静止邻域的迭代。我们在逻辑回归和深度神经网络上测试了所提出的算法，并证明了该算法可以生成与手动调整得到的最佳步长相当的步长。",
    "tldr": "本文提出了一种新的算法，在经典的SGD框架下实现自适应步长选择，在逻辑回归和深度神经网络上测试了所提出的算法，并证明了该算法可以生成与手动调整得到的最佳步长相当的步长。"
}