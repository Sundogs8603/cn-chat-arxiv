{
    "title": "Subword Segmental Machine Translation: Unifying Segmentation and Target Sentence Generation. (arXiv:2305.07005v1 [cs.CL])",
    "abstract": "Subword segmenters like BPE operate as a preprocessing step in neural machine translation and other (conditional) language models. They are applied to datasets before training, so translation or text generation quality relies on the quality of segmentations. We propose a departure from this paradigm, called subword segmental machine translation (SSMT). SSMT unifies subword segmentation and MT in a single trainable model. It learns to segment target sentence words while jointly learning to generate target sentences. To use SSMT during inference we propose dynamic decoding, a text generation algorithm that adapts segmentations as it generates translations. Experiments across 6 translation directions show that SSMT improves chrF scores for morphologically rich agglutinative languages. Gains are strongest in the very low-resource scenario. SSMT also learns subwords that are closer to morphemes compared to baselines and proves more robust on a test set constructed for evaluating morphologic",
    "link": "http://arxiv.org/abs/2305.07005",
    "context": "Title: Subword Segmental Machine Translation: Unifying Segmentation and Target Sentence Generation. (arXiv:2305.07005v1 [cs.CL])\nAbstract: Subword segmenters like BPE operate as a preprocessing step in neural machine translation and other (conditional) language models. They are applied to datasets before training, so translation or text generation quality relies on the quality of segmentations. We propose a departure from this paradigm, called subword segmental machine translation (SSMT). SSMT unifies subword segmentation and MT in a single trainable model. It learns to segment target sentence words while jointly learning to generate target sentences. To use SSMT during inference we propose dynamic decoding, a text generation algorithm that adapts segmentations as it generates translations. Experiments across 6 translation directions show that SSMT improves chrF scores for morphologically rich agglutinative languages. Gains are strongest in the very low-resource scenario. SSMT also learns subwords that are closer to morphemes compared to baselines and proves more robust on a test set constructed for evaluating morphologic",
    "path": "papers/23/05/2305.07005.json",
    "total_tokens": 1034,
    "translated_title": "子词级分段机器翻译：统一分词和目标句子生成",
    "translated_abstract": "子词分词器（如BPE）在神经机器翻译和其他（条件）语言模型中作为预处理步骤。在训练之前应用它们于数据集上，因此翻译或文本生成的质量取决于分词的质量。我们提出了一种称为子词级分段机器翻译（SSMT）的偏离于此范例的方法。SSMT将子词分割和MT在一个可训练模型中统一起来。它学习分割目标句子词，同时联合学习生成目标句子。为了在推理过程中使用SSMT，我们提出了动态解码，这是一种随着生成翻译适应分割的文本生成算法。在6个翻译方向的实验表明，SSMT提高了词素丰富的聚集性语言的chrF分数。增益在非常低资源的情况下最强。与基准相比，SSMT还学习到了更接近语素的子词，并且在用于评估形态的测试集上证明了更大的鲁棒性。",
    "tldr": "子词级分段机器翻译（SSMT）是一种将子词分割和MT在一个可训练模型中统一起来的方法，它学习分割目标句子词，同时联合学习生成目标句子。在6个翻译方向的实验中表明，SSMT提高了词素丰富的聚集性语言的chrF分数，且具有更高的鲁棒性。",
    "en_tdlr": "Subword segmental machine translation (SSMT) unifies subword segmentation and MT in a single trainable model, which learns to segment target sentence words while jointly learning to generate target sentences. Experiments across 6 translation directions show that SSMT improves chrF scores for morphologically rich agglutinative languages, and it has greater robustness."
}