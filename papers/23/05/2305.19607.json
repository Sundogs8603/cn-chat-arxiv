{
    "title": "Adversarial Clean Label Backdoor Attacks and Defenses on Text Classification Systems. (arXiv:2305.19607v1 [cs.CL])",
    "abstract": "Clean-label (CL) attack is a form of data poisoning attack where an adversary modifies only the textual input of the training data, without requiring access to the labeling function. CL attacks are relatively unexplored in NLP, as compared to label flipping (LF) attacks, where the latter additionally requires access to the labeling function as well. While CL attacks are more resilient to data sanitization and manual relabeling methods than LF attacks, they often demand as high as ten times the poisoning budget than LF attacks. In this work, we first introduce an Adversarial Clean Label attack which can adversarially perturb in-class training examples for poisoning the training set. We then show that an adversary can significantly bring down the data requirements for a CL attack, using the aforementioned approach, to as low as 20% of the data otherwise required. We then systematically benchmark and analyze a number of defense methods, for both LF and CL attacks, some previously employed",
    "link": "http://arxiv.org/abs/2305.19607",
    "context": "Title: Adversarial Clean Label Backdoor Attacks and Defenses on Text Classification Systems. (arXiv:2305.19607v1 [cs.CL])\nAbstract: Clean-label (CL) attack is a form of data poisoning attack where an adversary modifies only the textual input of the training data, without requiring access to the labeling function. CL attacks are relatively unexplored in NLP, as compared to label flipping (LF) attacks, where the latter additionally requires access to the labeling function as well. While CL attacks are more resilient to data sanitization and manual relabeling methods than LF attacks, they often demand as high as ten times the poisoning budget than LF attacks. In this work, we first introduce an Adversarial Clean Label attack which can adversarially perturb in-class training examples for poisoning the training set. We then show that an adversary can significantly bring down the data requirements for a CL attack, using the aforementioned approach, to as low as 20% of the data otherwise required. We then systematically benchmark and analyze a number of defense methods, for both LF and CL attacks, some previously employed",
    "path": "papers/23/05/2305.19607.json",
    "total_tokens": 828,
    "translated_title": "文本分类系统中的对抗性干净标签后门攻击及其防御",
    "translated_abstract": "干净标签攻击是一种数据污染攻击，攻击者仅修改训练数据的文本输入，而无需访问标注函数。本文首先引入了一种对抗性干净标签攻击，可以故意扰动训练集中的同类样本来污染训练集。然后，我们展示了攻击者可以显著降低干净标签攻击的数据要求，使用上述方法可以将要求的数据量降低到原来的20%。最后，我们系统地评估和分析了一些既可防御干净标签攻击又可防御标签翻转攻击的防御方法。",
    "tldr": "本文针对文本分类系统提出了一种对抗性干净标签攻击，并提高了攻击成功率，比起标签翻转攻击更难以防御。同时，研究了多种防御策略对两种攻击的有效性。",
    "en_tdlr": "This paper proposes an adversarial clean label attack and improves its success rate on text classification systems, which is more difficult to defend against than label flipping attacks. It also analyzes the effectiveness of various defense methods against both types of attacks."
}