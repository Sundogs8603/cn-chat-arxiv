{
    "title": "When your Cousin has the Right Connections: Unsupervised Bilingual Lexicon Induction for Related Data-Imbalanced Languages",
    "abstract": "arXiv:2305.14012v2 Announce Type: replace  Abstract: Most existing approaches for unsupervised bilingual lexicon induction (BLI) depend on good quality static or contextual embeddings requiring large monolingual corpora for both languages. However, unsupervised BLI is most likely to be useful for low-resource languages (LRLs), where large datasets are not available. Often we are interested in building bilingual resources for LRLs against related high-resource languages (HRLs), resulting in severely imbalanced data settings for BLI. We first show that state-of-the-art BLI methods in the literature exhibit near-zero performance for severely data-imbalanced language pairs, indicating that these settings require more robust techniques. We then present a new method for unsupervised BLI between a related LRL and HRL that only requires inference on a masked language model of the HRL, and demonstrate its effectiveness on truly low-resource languages Bhojpuri and Magahi (with <5M monolingual to",
    "link": "https://arxiv.org/abs/2305.14012",
    "context": "Title: When your Cousin has the Right Connections: Unsupervised Bilingual Lexicon Induction for Related Data-Imbalanced Languages\nAbstract: arXiv:2305.14012v2 Announce Type: replace  Abstract: Most existing approaches for unsupervised bilingual lexicon induction (BLI) depend on good quality static or contextual embeddings requiring large monolingual corpora for both languages. However, unsupervised BLI is most likely to be useful for low-resource languages (LRLs), where large datasets are not available. Often we are interested in building bilingual resources for LRLs against related high-resource languages (HRLs), resulting in severely imbalanced data settings for BLI. We first show that state-of-the-art BLI methods in the literature exhibit near-zero performance for severely data-imbalanced language pairs, indicating that these settings require more robust techniques. We then present a new method for unsupervised BLI between a related LRL and HRL that only requires inference on a masked language model of the HRL, and demonstrate its effectiveness on truly low-resource languages Bhojpuri and Magahi (with <5M monolingual to",
    "path": "papers/23/05/2305.14012.json",
    "total_tokens": 903,
    "translated_title": "当你的表亲有正确的连接：非监督双语词表诱导用于相关数据不平衡的语言",
    "translated_abstract": "大多数现有的非监督双语词表诱导（BLI）方法依赖于需要大型单语语料库的良好质量的静态或上下文嵌入。然而，非监督BLI最有可能对低资源语言（LRLs）有用，对于这些语言，大型数据集是不可用的。我们经常对建立LRLs与相关高资源语言（HRLs）之间的双语资源感兴趣，结果造成BLI的数据设置出现严重不平衡。我们首先展示了文献中现有的最先进的BLI方法对严重数据不平衡的语言对表现接近零的性能，这表明这些设置需要更强大的技术。然后我们提出了一种新的方法，用于在相关的LRL和HRL之间进行无监督的BLI，该方法只需要对HRL的掩码语言模型进行推断，并展示了对Bhojpuri和Magahi这两种真正低资源语言的有效性（单语语料小于5M）。",
    "tldr": "提出了一种新的非监督双语词表诱导方法，可以在相关低资源语言和高资源语言之间进行词表诱导，只需要对高资源语言的掩码语言模型进行推断。",
    "en_tdlr": "A new method for unsupervised bilingual lexicon induction between related low-resource languages and high-resource languages is proposed, which only requires inference on a masked language model of the high-resource language."
}