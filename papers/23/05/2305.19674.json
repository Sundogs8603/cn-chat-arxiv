{
    "title": "Online-to-PAC Conversions: Generalization Bounds via Regret Analysis. (arXiv:2305.19674v1 [stat.ML])",
    "abstract": "We present a new framework for deriving bounds on the generalization bound of statistical learning algorithms from the perspective of online learning. Specifically, we construct an online learning game called the \"generalization game\", where an online learner is trying to compete with a fixed statistical learning algorithm in predicting the sequence of generalization gaps on a training set of i.i.d. data points. We establish a connection between the online and statistical learning setting by showing that the existence of an online learning algorithm with bounded regret in this game implies a bound on the generalization error of the statistical learning algorithm, up to a martingale concentration term that is independent of the complexity of the statistical learning method. This technique allows us to recover several standard generalization bounds including a range of PAC-Bayesian and information-theoretic guarantees, as well as generalizations thereof.",
    "link": "http://arxiv.org/abs/2305.19674",
    "context": "Title: Online-to-PAC Conversions: Generalization Bounds via Regret Analysis. (arXiv:2305.19674v1 [stat.ML])\nAbstract: We present a new framework for deriving bounds on the generalization bound of statistical learning algorithms from the perspective of online learning. Specifically, we construct an online learning game called the \"generalization game\", where an online learner is trying to compete with a fixed statistical learning algorithm in predicting the sequence of generalization gaps on a training set of i.i.d. data points. We establish a connection between the online and statistical learning setting by showing that the existence of an online learning algorithm with bounded regret in this game implies a bound on the generalization error of the statistical learning algorithm, up to a martingale concentration term that is independent of the complexity of the statistical learning method. This technique allows us to recover several standard generalization bounds including a range of PAC-Bayesian and information-theoretic guarantees, as well as generalizations thereof.",
    "path": "papers/23/05/2305.19674.json",
    "total_tokens": 838,
    "translated_title": "在线到PAC的转换: 通过遗憾分析得出泛化界限",
    "translated_abstract": "我们提出了一个新的框架，通过在线学习的视角推导出统计学习算法的泛化界限。具体而言，我们构建了一个在线学习游戏称为“泛化游戏”，其中在线学习器试图与固定的统计学习算法竞争，预测独立同分布数据点训练集上的泛化间隙序列。我们通过展示在这个游戏中存在有界遗憾的在线学习算法与统计学习设置之间的联系来建立这种关联，这意味着统计学习算法的泛化错误存在一个界限，直到与统计学习方法的复杂性无关的鞅浓度项。这种技术允许我们恢复几个标准的泛化限制，包括一系列的PAC-Bayesian保证和信息理论保证，以及它们的推广。",
    "tldr": "本文提出了在线学习游戏“泛化游戏”的框架，将在线学习算法的表现和统计学习算法的泛化界限联系了起来，并得出了一些标准的泛化限制。",
    "en_tdlr": "This paper proposes a framework of online learning game called the \"generalization game\" to connect the performance of online learning algorithms and the generalization bounds of statistical learning algorithms, and derives several standard generalization bounds."
}