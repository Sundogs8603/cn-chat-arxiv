{
    "title": "Self Information Update for Large Language Models through Mitigating Exposure Bias. (arXiv:2305.18582v1 [cs.CL])",
    "abstract": "Current LLMs have demonstrated remarkable capabilities in addressing users' requests for various types of information. However, these models are limited by the most recent data available in their pretraining corpora, rendering them incapable of providing up-to-date information. Retraining LLMs from scratch is cost-prohibitive, and the effectiveness of continual fine-tuning on new corpora has not been thoroughly examined. Additionally, current update procedures typically demand significant human input to prepare the information into more structured format, such as knowledge triples, conversational data or responses with human feedback. In this study, we conduct a comprehensive examination of a novel self information update task in LLMs, which only requires the provision of informative text corpora. For instance, we can use the latest news articles to update the LLMs' existing knowledge. We define the self information update task and assess the continual fine-tuning approach for this pur",
    "link": "http://arxiv.org/abs/2305.18582",
    "context": "Title: Self Information Update for Large Language Models through Mitigating Exposure Bias. (arXiv:2305.18582v1 [cs.CL])\nAbstract: Current LLMs have demonstrated remarkable capabilities in addressing users' requests for various types of information. However, these models are limited by the most recent data available in their pretraining corpora, rendering them incapable of providing up-to-date information. Retraining LLMs from scratch is cost-prohibitive, and the effectiveness of continual fine-tuning on new corpora has not been thoroughly examined. Additionally, current update procedures typically demand significant human input to prepare the information into more structured format, such as knowledge triples, conversational data or responses with human feedback. In this study, we conduct a comprehensive examination of a novel self information update task in LLMs, which only requires the provision of informative text corpora. For instance, we can use the latest news articles to update the LLMs' existing knowledge. We define the self information update task and assess the continual fine-tuning approach for this pur",
    "path": "papers/23/05/2305.18582.json",
    "total_tokens": 835,
    "translated_title": "通过减少暴露偏差实现大型语言模型的自我信息更新",
    "translated_abstract": "当今的大型语言模型在各种信息请求方面展示了卓越的能力。然而，这些模型受其预训练语料库中最新数据的限制，使其无法提供最新的信息。从头开始对大型语言模型进行重新训练代价较高，并且对新语料库进行连续微调的效果尚未得到全面检查。此外，目前的更新程序通常需要大量人力投入，将信息准备成更结构化的形式，如知识三元组、对话数据或带有人类反馈的响应。在本研究中，我们对大型语言模型中的自我信息更新任务进行了全面的研究，这只需要提供信息丰富的文本语料库。例如，我们可以使用最新的新闻文章来更新LLM的现有知识。我们定义了自我信息更新任务，并评估了连续微调方法在此任务中的效果。",
    "tldr": "本文提出了一种使用信息丰富的文本语料库来帮助现有的大型语言模型进行自我信息更新的方法，有效减轻了暴露偏差的影响。"
}