{
    "title": "Investigating the Effect of Hard Negative Sample Distribution on Contrastive Knowledge Graph Embedding. (arXiv:2305.10563v1 [cs.AI])",
    "abstract": "The success of the knowledge graph completion task heavily depends on the quality of the knowledge graph embeddings (KGEs), which relies on self-supervised learning and augmenting the dataset with negative triples. There is a gap in literature between the theoretical analysis of negative samples on contrastive loss and heuristic generation of quality (i.e., hard) negative triples. In this paper, we modify the InfoNCE loss to explicitly account for the negative sample distribution. We show minimizing InfoNCE loss with hard negatives maximizes the KL-divergence between the given and negative triple embedding. However, we also show that hard negatives can lead to false negatives (i.e., accidentally factual triples) and reduce downstream task performance. To address this issue, we propose a novel negative sample distribution that uses the graph structure of the knowledge graph to remove the false negative triples. We call our algorithm Hardness and Structure-aware (\\textbf{HaSa}) contrasti",
    "link": "http://arxiv.org/abs/2305.10563",
    "context": "Title: Investigating the Effect of Hard Negative Sample Distribution on Contrastive Knowledge Graph Embedding. (arXiv:2305.10563v1 [cs.AI])\nAbstract: The success of the knowledge graph completion task heavily depends on the quality of the knowledge graph embeddings (KGEs), which relies on self-supervised learning and augmenting the dataset with negative triples. There is a gap in literature between the theoretical analysis of negative samples on contrastive loss and heuristic generation of quality (i.e., hard) negative triples. In this paper, we modify the InfoNCE loss to explicitly account for the negative sample distribution. We show minimizing InfoNCE loss with hard negatives maximizes the KL-divergence between the given and negative triple embedding. However, we also show that hard negatives can lead to false negatives (i.e., accidentally factual triples) and reduce downstream task performance. To address this issue, we propose a novel negative sample distribution that uses the graph structure of the knowledge graph to remove the false negative triples. We call our algorithm Hardness and Structure-aware (\\textbf{HaSa}) contrasti",
    "path": "papers/23/05/2305.10563.json",
    "total_tokens": 908,
    "translated_title": "探究硬负采样分布对对比知识图谱嵌入的影响",
    "translated_abstract": "知识图谱补全任务的成功很大程度上依赖于知识图谱嵌入（KGEs）的质量，它依赖于自监督学习和用负三元组增强数据集。在负采样的对比损失的理论分析和高质量（即硬）负采样的启发式生成之间存在差距。在本文中，我们修改了InfoNCE损失，显式考虑了负采样分布。我们展示了用硬负样本最小化InfoNCE损失可以最大化给定三元组和负三元组之间的KL散度。然而，我们也证明硬负样本会导致假负样本（即错误的事实三元组）并降低下游任务性能。为了解决这个问题，我们提出了一种通过知识图谱的图结构去除假负三元组的新型负采样分布。我们将我们的算法称为考虑硬度和结构的对比（HaSa）算法。",
    "tldr": "本文探究了负采样分布对对比知识图谱嵌入的影响，提出考虑硬度和结构的对比（HaSa）算法，用于去除假负样本，提高知识图谱补全任务的性能。",
    "en_tdlr": "This paper investigates the effect of negative sample distribution on contrastive knowledge graph embedding and proposes the HaSa algorithm to remove false negative samples and improve the performance of knowledge graph completion task."
}