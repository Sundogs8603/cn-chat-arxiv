{
    "title": "The Power of Learned Locally Linear Models for Nonlinear Policy Optimization. (arXiv:2305.09619v1 [cs.LG])",
    "abstract": "A common pipeline in learning-based control is to iteratively estimate a model of system dynamics, and apply a trajectory optimization algorithm e.g.~$\\mathtt{iLQR}$ - on the learned model to minimize a target cost. This paper conducts a rigorous analysis of a simplified variant of this strategy for general nonlinear systems. We analyze an algorithm which iterates between estimating local linear models of nonlinear system dynamics and performing $\\mathtt{iLQR}$-like policy updates. We demonstrate that this algorithm attains sample complexity polynomial in relevant problem parameters, and, by synthesizing locally stabilizing gains, overcomes exponential dependence in problem horizon. Experimental results validate the performance of our algorithm, and compare to natural deep-learning baselines.",
    "link": "http://arxiv.org/abs/2305.09619",
    "context": "Title: The Power of Learned Locally Linear Models for Nonlinear Policy Optimization. (arXiv:2305.09619v1 [cs.LG])\nAbstract: A common pipeline in learning-based control is to iteratively estimate a model of system dynamics, and apply a trajectory optimization algorithm e.g.~$\\mathtt{iLQR}$ - on the learned model to minimize a target cost. This paper conducts a rigorous analysis of a simplified variant of this strategy for general nonlinear systems. We analyze an algorithm which iterates between estimating local linear models of nonlinear system dynamics and performing $\\mathtt{iLQR}$-like policy updates. We demonstrate that this algorithm attains sample complexity polynomial in relevant problem parameters, and, by synthesizing locally stabilizing gains, overcomes exponential dependence in problem horizon. Experimental results validate the performance of our algorithm, and compare to natural deep-learning baselines.",
    "path": "papers/23/05/2305.09619.json",
    "total_tokens": 833,
    "translated_title": "学习的局部线性模型在非线性策略优化中的威力",
    "translated_abstract": "在基于学习的控制中，常见的流程是逐步估计系统动力学模型，并应用轨迹优化算法（例如$\\mathtt{iLQR}$）在学习的模型上进行优化，以最小化目标成本。本文对一种简化版的此策略应用于一般非线性系统的情况进行了严格分析。我们分析了一种算法，该算法在估计非线性系统动态的局部线性模型和执行类似于$\\mathtt{iLQR}$的策略更新之间进行迭代。我们证明该算法在相关问题参数中达到了多项式的样本复杂度，并通过合成局部稳定增益，克服了在问题区间上的指数依赖性。实验结果验证了我们算法的性能，并与自然的深度学习基线进行了比较。",
    "tldr": "本文介绍了一种学习非线性系统动态的策略优化算法，该算法通过估计局部线性模型和执行类似于$\\mathtt{iLQR}$的策略更新之间的迭代来实现，具有多项式的样本复杂度并克服了指数区间上的依赖性。",
    "en_tdlr": "This paper presents a policy optimization algorithm that learns the dynamics of nonlinear systems, which iterates between estimating locally linear models and performing policy updates similar to $\\mathtt{iLQR}$, achieving polynomial sample complexity and overcoming exponential dependence on the horizon."
}