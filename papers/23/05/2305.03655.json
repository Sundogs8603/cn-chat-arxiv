{
    "title": "White-Box Multi-Objective Adversarial Attack on Dialogue Generation. (arXiv:2305.03655v1 [cs.CL])",
    "abstract": "Pre-trained transformers are popular in state-of-the-art dialogue generation (DG) systems. Such language models are, however, vulnerable to various adversarial samples as studied in traditional tasks such as text classification, which inspires our curiosity about their robustness in DG systems. One main challenge of attacking DG models is that perturbations on the current sentence can hardly degrade the response accuracy because the unchanged chat histories are also considered for decision-making. Instead of merely pursuing pitfalls of performance metrics such as BLEU, ROUGE, we observe that crafting adversarial samples to force longer generation outputs benefits attack effectiveness -- the generated responses are typically irrelevant, lengthy, and repetitive. To this end, we propose a white-box multi-objective attack method called DGSlow. Specifically, DGSlow balances two objectives -- generation accuracy and length, via a gradient-based multi-objective optimizer and applies an adapti",
    "link": "http://arxiv.org/abs/2305.03655",
    "context": "Title: White-Box Multi-Objective Adversarial Attack on Dialogue Generation. (arXiv:2305.03655v1 [cs.CL])\nAbstract: Pre-trained transformers are popular in state-of-the-art dialogue generation (DG) systems. Such language models are, however, vulnerable to various adversarial samples as studied in traditional tasks such as text classification, which inspires our curiosity about their robustness in DG systems. One main challenge of attacking DG models is that perturbations on the current sentence can hardly degrade the response accuracy because the unchanged chat histories are also considered for decision-making. Instead of merely pursuing pitfalls of performance metrics such as BLEU, ROUGE, we observe that crafting adversarial samples to force longer generation outputs benefits attack effectiveness -- the generated responses are typically irrelevant, lengthy, and repetitive. To this end, we propose a white-box multi-objective attack method called DGSlow. Specifically, DGSlow balances two objectives -- generation accuracy and length, via a gradient-based multi-objective optimizer and applies an adapti",
    "path": "papers/23/05/2305.03655.json",
    "total_tokens": 881,
    "translated_title": "白盒多目标对话生成对抗攻击",
    "translated_abstract": "预训练的转换器在最先进的对话生成系统中很受欢迎。 然而，这种语言模型容易受到各种对抗样本的攻击，这在传统任务（如文本分类）中已经得到了研究，这激发了我们对它们在DG系统中的鲁棒性的好奇心。 其中一个主要的挑战是攻击DG模型的时候，对当前句子的扰动几乎不会降低响应的准确性，因为未改变的聊天记录也会被考虑进行决策。我们观察到，通过精心制作对抗样本来迫使生成更长的输出，有利于攻击的有效性 - 生成的响应通常是不相关、冗长和重复的。因此，我们提出了一种名为DGSlow的白盒多目标攻击方法。具体来说，DGSlow通过基于梯度的多目标优化器平衡两个目标 - 生成准确度和长度，并使用自适应方法实施攻击。",
    "tldr": "该论文提出了一种新的白盒多目标对话生成对抗攻击方法，DGSlow。通过平衡生成准确性和长度两个目标，DGSlow利用生成更长的输出来提高攻击效果。",
    "en_tdlr": "This paper proposes a new white-box multi-objective adversarial attack method, DGSlow, on dialogue generation systems by balancing the objectives of generation accuracy and length, which utilizes longer generation outputs for better attack effectiveness."
}