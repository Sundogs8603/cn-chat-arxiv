{
    "title": "Recasting Self-Attention with Holographic Reduced Representations. (arXiv:2305.19534v1 [cs.LG])",
    "abstract": "In recent years, self-attention has become the dominant paradigm for sequence modeling in a variety of domains. However, in domains with very long sequence lengths the $\\mathcal{O}(T^2)$ memory and $\\mathcal{O}(T^2 H)$ compute costs can make using transformers infeasible. Motivated by problems in malware detection, where sequence lengths of $T \\geq 100,000$ are a roadblock to deep learning, we re-cast self-attention using the neuro-symbolic approach of Holographic Reduced Representations (HRR). In doing so we perform the same high-level strategy of the standard self-attention: a set of queries matching against a set of keys, and returning a weighted response of the values for each key. Implemented as a ``Hrrformer'' we obtain several benefits including $\\mathcal{O}(T H \\log H)$ time complexity, $\\mathcal{O}(T H)$ space complexity, and convergence in $10\\times$ fewer epochs. Nevertheless, the Hrrformer achieves near state-of-the-art accuracy on LRA benchmarks and we are able to learn wi",
    "link": "http://arxiv.org/abs/2305.19534",
    "context": "Title: Recasting Self-Attention with Holographic Reduced Representations. (arXiv:2305.19534v1 [cs.LG])\nAbstract: In recent years, self-attention has become the dominant paradigm for sequence modeling in a variety of domains. However, in domains with very long sequence lengths the $\\mathcal{O}(T^2)$ memory and $\\mathcal{O}(T^2 H)$ compute costs can make using transformers infeasible. Motivated by problems in malware detection, where sequence lengths of $T \\geq 100,000$ are a roadblock to deep learning, we re-cast self-attention using the neuro-symbolic approach of Holographic Reduced Representations (HRR). In doing so we perform the same high-level strategy of the standard self-attention: a set of queries matching against a set of keys, and returning a weighted response of the values for each key. Implemented as a ``Hrrformer'' we obtain several benefits including $\\mathcal{O}(T H \\log H)$ time complexity, $\\mathcal{O}(T H)$ space complexity, and convergence in $10\\times$ fewer epochs. Nevertheless, the Hrrformer achieves near state-of-the-art accuracy on LRA benchmarks and we are able to learn wi",
    "path": "papers/23/05/2305.19534.json",
    "total_tokens": 943,
    "translated_title": "用全息约化表示重新建模自注意力",
    "translated_abstract": "近年来，自注意力已经成为各个领域序列建模的主要范例。然而，在序列长度非常长的领域中，复杂度为$\\mathcal{O}(T^2)$的内存和$\\mathcal{O}(T^2 \\cdot H)$的计算成本可能会使得使用变形金刚网络不可行。受惊物检测中$T \\geq 100,000$的序列长度成为深度学习的拦路虎的问题的启发，我们使用全息约化表示（HRR）的神经符号化方法重新构建自注意力。这样我们执行相同的高级策略，即标准自 注意力的查询匹配钥匙，返回每个键的值的加权响应。通过实现“Hrrformer”，我们获得了一些好处，包括$\\mathcal{O}(T H \\log H)$的时间复杂度、$\\mathcal{O}(T H)$的空间复杂度和收敛于$10\\times$更少的迭代次数。然而，Hrrformer在LRA基准测试中实现了接近于最先进的准确度，我们能够学习到深度模型。",
    "tldr": "本文提出了一种使用HRR的神经符号方法重新构建自注意力的方法，可以实现较低的时间和空间复杂度，并在LRA基准测试中获得了接近于最先进的准确度。"
}