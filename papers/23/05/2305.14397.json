{
    "title": "Reviewing Evolution of Learning Functions and Semantic Information Measures for Understanding Deep Learning. (arXiv:2305.14397v1 [cs.IT])",
    "abstract": "A new trend in deep learning, represented by Mutual Information Neural Estimation (MINE) and Information Noise Contrast Estimation (InfoNCE), is emerging. In this trend, similarity functions and Estimated Mutual Information (EMI) are used as learning and objective functions. Coincidentally, EMI is essentially the same as Semantic Mutual Information (SeMI) proposed by the author 30 years ago. This paper first reviews the evolutionary histories of semantic information measures and learning functions. Then, it briefly introduces the author's semantic information G theory with the rate-fidelity function R(G) (G denotes SeMI, and R(G) extends R(D)) and its applications to multi-label learning, the maximum Mutual Information (MI) classification, and mixture models. Then it discusses how we should understand the relationship between SeMI and Shan-non's MI, two generalized entropies (fuzzy entropy and coverage entropy), Autoencoders, Gibbs distributions, and partition functions from the perspe",
    "link": "http://arxiv.org/abs/2305.14397",
    "context": "Title: Reviewing Evolution of Learning Functions and Semantic Information Measures for Understanding Deep Learning. (arXiv:2305.14397v1 [cs.IT])\nAbstract: A new trend in deep learning, represented by Mutual Information Neural Estimation (MINE) and Information Noise Contrast Estimation (InfoNCE), is emerging. In this trend, similarity functions and Estimated Mutual Information (EMI) are used as learning and objective functions. Coincidentally, EMI is essentially the same as Semantic Mutual Information (SeMI) proposed by the author 30 years ago. This paper first reviews the evolutionary histories of semantic information measures and learning functions. Then, it briefly introduces the author's semantic information G theory with the rate-fidelity function R(G) (G denotes SeMI, and R(G) extends R(D)) and its applications to multi-label learning, the maximum Mutual Information (MI) classification, and mixture models. Then it discusses how we should understand the relationship between SeMI and Shan-non's MI, two generalized entropies (fuzzy entropy and coverage entropy), Autoencoders, Gibbs distributions, and partition functions from the perspe",
    "path": "papers/23/05/2305.14397.json",
    "total_tokens": 911,
    "translated_title": "深度学习中学习函数和语义信息度量的演变综述",
    "translated_abstract": "深度学习中的一种新趋势是互信息神经估计（MINE）和信息噪声对比估计（InfoNCE）。在这种趋势中，相似性函数和估算的互信息（EMI）被用作学习和目标函数。巧合的是，EMI本质上与作者30年前提出的语义互信息（SeMI）相同。本文首先回顾了语义信息度量和学习函数的演化历史。然后，它简要介绍了作者的语义信息G理论及其在多标签学习、最大互信息（MI）分类和混合模型中的应用。接下来，本文讨论了如何从不同的角度理解SeMI和香农的MI之间的关系，以及模糊熵和覆盖熵这两种广义熵、自编码器、吉布斯分布和分区函数。",
    "tldr": "这篇论文回顾了语义信息度量和学习函数的演化历史，并介绍了作者的语义信息G理论及其在多标签学习、最大互信息（MI）分类和混合模型中的应用。此外，它还讨论了如何理解SeMI和Shannon的MI之间的关系以及其他相关概念。",
    "en_tdlr": "This paper reviews the evolutionary history of semantic information measures and learning functions in deep learning, and introduces the author's semantic information G theory and its applications in various tasks. It also discusses the relationship between SeMI and Shannon's MI, as well as other related concepts."
}