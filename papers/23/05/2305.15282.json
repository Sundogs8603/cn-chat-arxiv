{
    "title": "A Simple and Effective Framework for Strict Zero-Shot Hierarchical Classification. (arXiv:2305.15282v2 [cs.CL] UPDATED)",
    "abstract": "In recent years, large language models (LLMs) have achieved strong performance on benchmark tasks, especially in zero or few-shot settings. However, these benchmarks often do not adequately address the challenges posed in the real-world, such as that of hierarchical classification. In order to address this challenge, we propose refactoring conventional tasks on hierarchical datasets into a more indicative long-tail prediction task. We observe LLMs are more prone to failure in these cases. To address these limitations, we propose the use of entailment-contradiction prediction in conjunction with LLMs, which allows for strong performance in a strict zero-shot setting. Importantly, our method does not require any parameter updates, a resource-intensive process and achieves strong performance across multiple datasets.",
    "link": "http://arxiv.org/abs/2305.15282",
    "context": "Title: A Simple and Effective Framework for Strict Zero-Shot Hierarchical Classification. (arXiv:2305.15282v2 [cs.CL] UPDATED)\nAbstract: In recent years, large language models (LLMs) have achieved strong performance on benchmark tasks, especially in zero or few-shot settings. However, these benchmarks often do not adequately address the challenges posed in the real-world, such as that of hierarchical classification. In order to address this challenge, we propose refactoring conventional tasks on hierarchical datasets into a more indicative long-tail prediction task. We observe LLMs are more prone to failure in these cases. To address these limitations, we propose the use of entailment-contradiction prediction in conjunction with LLMs, which allows for strong performance in a strict zero-shot setting. Importantly, our method does not require any parameter updates, a resource-intensive process and achieves strong performance across multiple datasets.",
    "path": "papers/23/05/2305.15282.json",
    "total_tokens": 817,
    "translated_title": "一种严格零样例分层分类的简单有效框架",
    "translated_abstract": "最近几年，大型语言模型（LLMs）在基准任务中表现出良好的性能，特别是在零样例或少样例情况下。然而，这些基准测试通常未能充分解决现实世界中所面临的挑战，如分层分类的挑战。为了解决这个挑战，我们将传统的分层数据集上的任务重新定义为更具指示性的长尾预测任务。我们观察到LLMs在这些情况下更容易失败。为了解决这些限制，我们提出在LLMs的基础上使用蕴含-矛盾预测方法，在严格零样例情况下获得强大的性能。重要的是，我们的方法不需要任何参数更新，这是一种资源密集型的过程，并且在多个数据集上获得了强大的性能。",
    "tldr": "本研究提出了一种蕴含-矛盾预测方法，与大型语言模型结合，用于解决分层数据集中的零样例分类问题，成功实现了严格零样例分层分类。",
    "en_tdlr": "This study proposes an entailment-contradiction prediction method, combined with large language models, to address the zero-shot hierarchical classification problem in hierarchical datasets, achieving strict zero-shot hierarchical classification performance without the need for parameter updates."
}