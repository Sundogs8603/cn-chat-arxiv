{
    "title": "Estimating Large Language Model Capabilities without Labeled Test Data. (arXiv:2305.14802v1 [cs.CL])",
    "abstract": "Large Language Models (LLMs) have exhibited an impressive ability to perform in-context learning (ICL) from only a few examples, but the success of ICL varies widely from task to task. Thus, it is important to quickly determine whether ICL is applicable to a new task, but directly evaluating ICL accuracy can be expensive in situations where test data is expensive to annotate -- the exact situations where ICL is most appealing. In this paper, we propose the task of ICL accuracy estimation, in which we predict the accuracy of an LLM when doing in-context learning on a new task given only unlabeled data for that task. To perform ICL accuracy estimation, we propose a method that trains a meta-model using LLM confidence scores as features. We compare our method to several strong accuracy estimation baselines on a new benchmark that covers 4 LLMs and 3 task collections. On average, the meta-model improves over all baselines and achieves the same estimation performance as directly evaluating ",
    "link": "http://arxiv.org/abs/2305.14802",
    "context": "Title: Estimating Large Language Model Capabilities without Labeled Test Data. (arXiv:2305.14802v1 [cs.CL])\nAbstract: Large Language Models (LLMs) have exhibited an impressive ability to perform in-context learning (ICL) from only a few examples, but the success of ICL varies widely from task to task. Thus, it is important to quickly determine whether ICL is applicable to a new task, but directly evaluating ICL accuracy can be expensive in situations where test data is expensive to annotate -- the exact situations where ICL is most appealing. In this paper, we propose the task of ICL accuracy estimation, in which we predict the accuracy of an LLM when doing in-context learning on a new task given only unlabeled data for that task. To perform ICL accuracy estimation, we propose a method that trains a meta-model using LLM confidence scores as features. We compare our method to several strong accuracy estimation baselines on a new benchmark that covers 4 LLMs and 3 task collections. On average, the meta-model improves over all baselines and achieves the same estimation performance as directly evaluating ",
    "path": "papers/23/05/2305.14802.json",
    "total_tokens": 949,
    "translated_title": "无标签测试数据下大型语言模型能力的估计",
    "translated_abstract": "大型语言模型（LLMs）展示了惊人的在上下文学习（ICL）中只需要几个例子即可执行的能力，但ICL的成功在不同任务中变化很大。因此，快速确定ICL是否适用于新任务非常重要，但直接评估ICL的准确性可能在测试数据昂贵的情况下变得昂贵，而ICL最有吸引力的正是这些情况。在本文中，我们提出ICL准确性估计任务，即在仅给定该任务的未标记数据的情况下，预测LLM在新任务上进行上下文学习时的准确性。为了执行ICL准确性估计，我们提出了一种使用LLM置信度分数作为特征来训练元模型的方法。我们在覆盖4个LLM和3个任务集合的新基准测试上将我们的方法与几种强的准确性估计基线进行比较。平均而言，元模型优于所有基线，并实现与直接评估相同的估计性能。",
    "tldr": "本文提出了一种在无标签测试数据下估计大型语言模型能力的方法，通过使用LLM置信度分数训练元模型来执行上下文学习准确性估计任务，并在相关基准测试中取得了优于其他基线的成果。",
    "en_tdlr": "This paper proposes a method to estimate the capabilities of large language models when performing in-context learning on a new task without labeled test data. By training a meta-model using LLM confidence scores as features, the proposed method achieves better estimation performance than several strong baselines on a new benchmark covering 4 LLMs and 3 task collections."
}