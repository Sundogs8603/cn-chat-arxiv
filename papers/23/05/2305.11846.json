{
    "title": "Any-to-Any Generation via Composable Diffusion. (arXiv:2305.11846v1 [cs.CV])",
    "abstract": "We present Composable Diffusion (CoDi), a novel generative model capable of generating any combination of output modalities, such as language, image, video, or audio, from any combination of input modalities. Unlike existing generative AI systems, CoDi can generate multiple modalities in parallel and its input is not limited to a subset of modalities like text or image. Despite the absence of training datasets for many combinations of modalities, we propose to align modalities in both the input and output space. This allows CoDi to freely condition on any input combination and generate any group of modalities, even if they are not present in the training data. CoDi employs a novel composable generation strategy which involves building a shared multimodal space by bridging alignment in the diffusion process, enabling the synchronized generation of intertwined modalities, such as temporally aligned video and audio. Highly customizable and flexible, CoDi achieves strong joint-modality gen",
    "link": "http://arxiv.org/abs/2305.11846",
    "context": "Title: Any-to-Any Generation via Composable Diffusion. (arXiv:2305.11846v1 [cs.CV])\nAbstract: We present Composable Diffusion (CoDi), a novel generative model capable of generating any combination of output modalities, such as language, image, video, or audio, from any combination of input modalities. Unlike existing generative AI systems, CoDi can generate multiple modalities in parallel and its input is not limited to a subset of modalities like text or image. Despite the absence of training datasets for many combinations of modalities, we propose to align modalities in both the input and output space. This allows CoDi to freely condition on any input combination and generate any group of modalities, even if they are not present in the training data. CoDi employs a novel composable generation strategy which involves building a shared multimodal space by bridging alignment in the diffusion process, enabling the synchronized generation of intertwined modalities, such as temporally aligned video and audio. Highly customizable and flexible, CoDi achieves strong joint-modality gen",
    "path": "papers/23/05/2305.11846.json",
    "total_tokens": 931,
    "translated_title": "通过可组合扩散实现任意输入与输出之间的生成",
    "translated_abstract": "本文提出了一种新的生成模型，称之为可组合扩散（CoDi）。这个模型能够从任意输入模态和任意组合中生成各种输出模态，如语言、图像、视频或音频。与现有的生成 AI 系统不同，CoDi 可以同时生成多个模态，并且其输入不局限于文本或图像的子集。尽管很多模态的组合缺乏训练数据集，但我们提出了在输入空间和输出空间中进行模态对齐的方法，从而使 CoDi 可以自由地对任何输入组合进行条件生成，并生成任何模态组合，即使这些组合不在训练数据中。CoDi采用了一种新的可组合生成策略，通过在扩散过程中构建共享的多模态空间来实现对齐，从而实现了交织模态的同步生成，例如时间上对齐的视频和音频。高度可定制和灵活，CoDi 实现了强大的联合模态生成。",
    "tldr": "CoDi是一种生成模型，通过在输入空间和输出空间中进行模态对齐，实现了可组合生成策略，从而可以生成任意组合的输出模态。 CoDi 非常灵活，能够生成多个模态，如图像、视频、语言和音频，甚至在训练数据中不存在的模态组合。",
    "en_tdlr": "CoDi is a generative model that can produce any combination of output modalities from any combination of input modalities. It aligns modalities in the input and output space, allowing for flexible and customizable generation. CoDi can generate multiple modalities in parallel, including those not present in the training data."
}