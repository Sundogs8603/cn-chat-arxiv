{
    "title": "Network-GIANT: Fully distributed Newton-type optimization via harmonic Hessian consensus. (arXiv:2305.07898v1 [math.OC])",
    "abstract": "This paper considers the problem of distributed multi-agent learning, where the global aim is to minimize a sum of local objective (empirical loss) functions through local optimization and information exchange between neighbouring nodes. We introduce a Newton-type fully distributed optimization algorithm, Network-GIANT, which is based on GIANT, a Federated learning algorithm that relies on a centralized parameter server. The Network-GIANT algorithm is designed via a combination of gradient-tracking and a Newton-type iterative algorithm at each node with consensus based averaging of local gradient and Newton updates. We prove that our algorithm guarantees semi-global and exponential convergence to the exact solution over the network assuming strongly convex and smooth loss functions. We provide empirical evidence of the superior convergence performance of Network-GIANT over other state-of-art distributed learning algorithms such as Network-DANE and Newton-Raphson Consensus.",
    "link": "http://arxiv.org/abs/2305.07898",
    "context": "Title: Network-GIANT: Fully distributed Newton-type optimization via harmonic Hessian consensus. (arXiv:2305.07898v1 [math.OC])\nAbstract: This paper considers the problem of distributed multi-agent learning, where the global aim is to minimize a sum of local objective (empirical loss) functions through local optimization and information exchange between neighbouring nodes. We introduce a Newton-type fully distributed optimization algorithm, Network-GIANT, which is based on GIANT, a Federated learning algorithm that relies on a centralized parameter server. The Network-GIANT algorithm is designed via a combination of gradient-tracking and a Newton-type iterative algorithm at each node with consensus based averaging of local gradient and Newton updates. We prove that our algorithm guarantees semi-global and exponential convergence to the exact solution over the network assuming strongly convex and smooth loss functions. We provide empirical evidence of the superior convergence performance of Network-GIANT over other state-of-art distributed learning algorithms such as Network-DANE and Newton-Raphson Consensus.",
    "path": "papers/23/05/2305.07898.json",
    "total_tokens": 986,
    "translated_title": "Network-GIANT: 基于哈蒙莫特 Hessian 一致性的全分布式牛顿型优化",
    "translated_abstract": "本文考虑了分布式多代理学习的问题，其中全局目标是通过本地优化和节点之间的信息交换来最小化本地目标（经验损失）函数的总和。 我们介绍了一种新的牛顿型完全分布式优化算法，Network-GIANT，它基于 GIANT，这是一种依赖于集中式参数服务器的联邦学习算法。 Network-GIANT 算法是通过在每个节点上使用梯度跟踪和牛顿型迭代算法的组合以及本地梯度和牛顿更新的共识平均来设计的。我们证明了我们的算法保证了对网络上的严格凸和光滑损失函数的半全局和指数收敛到精确解。我们提供了 Network-GIANT 优于其他最先进的分布式学习算法（如 Network-DANE 和 Newton-Raphson Consensus）的收敛性能的实证证据。",
    "tldr": "本文提出了一种基于哈蒙莫特 Hessian 一致性的全分布式牛顿型优化算法 Network-GIANT，将梯度跟踪和牛顿型迭代算法相结合，经证明对严格凸和光滑损失函数有半全局和指数收敛到精确解的保证，实验证明 Network-GIANT 优于其他分布式学习算法（如 Network-DANE 和 Newton-Raphson Consensus）的收敛性能。",
    "en_tdlr": "This paper proposes a fully distributed Newton-type optimization algorithm, named Network-GIANT, which combines gradient-tracking and a Newton-type iterative algorithm with consensus based averaging of local gradient and Newton updates. It guarantees semi-global and exponential convergence to the exact solution for strongly convex and smooth loss functions. The empirical results show that Network-GIANT performs better than other state-of-art distributed learning algorithms such as Network-DANE and Newton-Raphson Consensus."
}