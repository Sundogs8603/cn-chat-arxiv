{
    "title": "Multimodal Graph Transformer for Multimodal Question Answering. (arXiv:2305.00581v1 [cs.CV])",
    "abstract": "Despite the success of Transformer models in vision and language tasks, they often learn knowledge from enormous data implicitly and cannot utilize structured input data directly. On the other hand, structured learning approaches such as graph neural networks (GNNs) that integrate prior information can barely compete with Transformer models. In this work, we aim to benefit from both worlds and propose a novel Multimodal Graph Transformer for question answering tasks that requires performing reasoning across multiple modalities. We introduce a graph-involved plug-and-play quasi-attention mechanism to incorporate multimodal graph information, acquired from text and visual data, to the vanilla self-attention as effective prior. In particular, we construct the text graph, dense region graph, and semantic graph to generate adjacency matrices, and then compose them with input vision and language features to perform downstream reasoning. Such a way of regularizing self-attention with graph in",
    "link": "http://arxiv.org/abs/2305.00581",
    "context": "Title: Multimodal Graph Transformer for Multimodal Question Answering. (arXiv:2305.00581v1 [cs.CV])\nAbstract: Despite the success of Transformer models in vision and language tasks, they often learn knowledge from enormous data implicitly and cannot utilize structured input data directly. On the other hand, structured learning approaches such as graph neural networks (GNNs) that integrate prior information can barely compete with Transformer models. In this work, we aim to benefit from both worlds and propose a novel Multimodal Graph Transformer for question answering tasks that requires performing reasoning across multiple modalities. We introduce a graph-involved plug-and-play quasi-attention mechanism to incorporate multimodal graph information, acquired from text and visual data, to the vanilla self-attention as effective prior. In particular, we construct the text graph, dense region graph, and semantic graph to generate adjacency matrices, and then compose them with input vision and language features to perform downstream reasoning. Such a way of regularizing self-attention with graph in",
    "path": "papers/23/05/2305.00581.json",
    "total_tokens": 997,
    "translated_title": "多模态图转换器用于多模态问答",
    "translated_abstract": "尽管Transformer模型在视觉和语言任务中取得了成功，但它们通常只是暗示性地学习庞大的数据，并且不能直接利用结构化输入数据。另一方面，结构化学习方法如图神经网络（GNN）可以整合先前的信息，但与Transformer模型几乎无法竞争。在这项工作中，我们旨在从两个世界中受益，并提出了一种新的多模态图转换器，用于需要在多个模态之间进行推理的问答任务。我们引入了一个涉及图形的即插即用准注意力机制，以将从文本和视觉数据中获取的多模态图形信息并入 vanilla self-attention 中作为有效先验知识。特别地，我们构建文本图、密集区域图和语义图生成邻接矩阵，然后将它们与输入的视觉和语言特征组合以执行下游推理。使用图形信息来规范self-attention机制，增强了模型理解结构化输入数据的能力，并且在两种具有挑战性的多模态问答基准(VQA和GQA)上取得了明显的性能提升。",
    "tldr": "本文提出了一种新的多模态图转换器，它使用图神经网络将文本和视觉数据的多模态图形信息与vanilla self-attention机制相结合，以增强模型理解结构化输入数据的能力，并在两种具有挑战性的多模态问答基准上取得了性能提升。",
    "en_tdlr": "This paper proposes a novel Multimodal Graph Transformer by incorporating graph neural networks to integrate multimodal graph information into the vanilla self-attention mechanism, which enhances the model's ability to reason over structured input data and leads to significant performance improvements on two challenging multimodal question answering benchmarks, VQA and GQA."
}