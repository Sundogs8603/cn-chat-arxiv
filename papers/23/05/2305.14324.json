{
    "title": "Ties Matter: Meta-Evaluating Modern Metrics with Pairwise Accuracy and Tie Calibration. (arXiv:2305.14324v2 [cs.CL] UPDATED)",
    "abstract": "Kendall's tau is frequently used to meta-evaluate how well machine translation (MT) evaluation metrics score individual translations. Its focus on pairwise score comparisons is intuitive but raises the question of how ties should be handled, a gray area that has motivated different variants in the literature. We demonstrate that, in settings like modern MT meta-evaluation, existing variants have weaknesses arising from their handling of ties, and in some situations can even be gamed. We propose instead to meta-evaluate metrics with a version of pairwise accuracy that gives metrics credit for correctly predicting ties, in combination with a tie calibration procedure that automatically introduces ties into metric scores, enabling fair comparison between metrics that do and do not predict ties. We argue and provide experimental evidence that these modifications lead to fairer ranking-based assessments of metric performance.",
    "link": "http://arxiv.org/abs/2305.14324",
    "context": "Title: Ties Matter: Meta-Evaluating Modern Metrics with Pairwise Accuracy and Tie Calibration. (arXiv:2305.14324v2 [cs.CL] UPDATED)\nAbstract: Kendall's tau is frequently used to meta-evaluate how well machine translation (MT) evaluation metrics score individual translations. Its focus on pairwise score comparisons is intuitive but raises the question of how ties should be handled, a gray area that has motivated different variants in the literature. We demonstrate that, in settings like modern MT meta-evaluation, existing variants have weaknesses arising from their handling of ties, and in some situations can even be gamed. We propose instead to meta-evaluate metrics with a version of pairwise accuracy that gives metrics credit for correctly predicting ties, in combination with a tie calibration procedure that automatically introduces ties into metric scores, enabling fair comparison between metrics that do and do not predict ties. We argue and provide experimental evidence that these modifications lead to fairer ranking-based assessments of metric performance.",
    "path": "papers/23/05/2305.14324.json",
    "total_tokens": 822,
    "translated_title": "Ties Matter: 用成对准确性和关联性校准元评估现代指标",
    "translated_abstract": "Kendall's tau经常被用来元评估机器翻译评估指标对个别翻译的评分。它对成对分数比较的重点很直观，但也引发了如何处理关联性的问题，这是一个模糊的领域，激发了文献中不同变种的研究。我们证明，在现代机器翻译元评估等环境中，现有的变种由于其对关联性的处理而存在缺陷，并且在某些情况下甚至可以被操纵。相反，我们建议使用一种成对准确性的版本对指标进行元评估，该版本给予指标正确预测关联性的信用，并结合自动引入关联性到指标评分的关联性校准过程，使得可以公平比较那些预测和不预测关联性的指标。我们通过论证并提供实验证据，表明这些改动可以更公平地评估指标的排名性能。",
    "tldr": "提出使用成对准确性和关联性校准的方法对现代指标进行元评估，以更公平地评估指标的排名性能。",
    "en_tdlr": "Proposed a method of meta-evaluating modern metrics using pairwise accuracy and tie calibration to assess metric ranking performance more fairly."
}