{
    "title": "Use of Speech Impairment Severity for Dysarthric Speech Recognition. (arXiv:2305.10659v1 [eess.AS])",
    "abstract": "A key challenge in dysarthric speech recognition is the speaker-level diversity attributed to both speaker-identity associated factors such as gender, and speech impairment severity. Most prior researches on addressing this issue focused on using speaker-identity only. To this end, this paper proposes a novel set of techniques to use both severity and speaker-identity in dysarthric speech recognition: a) multitask training incorporating severity prediction error; b) speaker-severity aware auxiliary feature adaptation; and c) structured LHUC transforms separately conditioned on speaker-identity and severity. Experiments conducted on UASpeech suggest incorporating additional speech impairment severity into state-of-the-art hybrid DNN, E2E Conformer and pre-trained Wav2vec 2.0 ASR systems produced statistically significant WER reductions up to 4.78% (14.03% relative). Using the best system the lowest published WER of 17.82% (51.25% on very low intelligibility) was obtained on UASpeech.",
    "link": "http://arxiv.org/abs/2305.10659",
    "context": "Title: Use of Speech Impairment Severity for Dysarthric Speech Recognition. (arXiv:2305.10659v1 [eess.AS])\nAbstract: A key challenge in dysarthric speech recognition is the speaker-level diversity attributed to both speaker-identity associated factors such as gender, and speech impairment severity. Most prior researches on addressing this issue focused on using speaker-identity only. To this end, this paper proposes a novel set of techniques to use both severity and speaker-identity in dysarthric speech recognition: a) multitask training incorporating severity prediction error; b) speaker-severity aware auxiliary feature adaptation; and c) structured LHUC transforms separately conditioned on speaker-identity and severity. Experiments conducted on UASpeech suggest incorporating additional speech impairment severity into state-of-the-art hybrid DNN, E2E Conformer and pre-trained Wav2vec 2.0 ASR systems produced statistically significant WER reductions up to 4.78% (14.03% relative). Using the best system the lowest published WER of 17.82% (51.25% on very low intelligibility) was obtained on UASpeech.",
    "path": "papers/23/05/2305.10659.json",
    "total_tokens": 916,
    "translated_title": "使用语音障碍程度进行口吃症语音识别",
    "translated_abstract": "口吃症语音识别中的关键挑战在于发音障碍严重程度因素与说话人身份等因素所导致的说话人层面的多样性。之前的研究主要集中于使用说话人身份来解决这个问题，而本文提出了一系列新的技术，即：a）多任务训练，包括发音障碍严重预测误差；b）以说话人-障碍程度为重点的辅助特征调整；c）仅针对说话人身份和障碍程度进行的结构化LHUC变换。在UASpeech上进行的实验表明，将额外的语音障碍程度纳入最先进的混合DNN、E2E Conformer和预训练的Wav2vec 2.0 ASR系统中，可以实现显著的字错率（WER）降低，最高可达4.78%（相对于14.03%的WER降低）。使用最佳系统，在UASpeech上可以获得已发布的最低WER为17.82%（对于非常低的可理解性，为51.25%）。",
    "tldr": "本文提出了一种使用发音障碍严重程度和说话人身份的口吃症语音识别技术，实现了显著的字错率降低。",
    "en_tdlr": "This paper proposes a technique for dysarthric speech recognition using both severity and speaker-identity, which achieved significant WER reductions."
}