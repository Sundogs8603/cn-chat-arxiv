{
    "title": "Replicability in Reinforcement Learning. (arXiv:2305.19562v1 [cs.LG])",
    "abstract": "We initiate the mathematical study of replicability as an algorithmic property in the context of reinforcement learning (RL). We focus on the fundamental setting of discounted tabular MDPs with access to a generative model. Inspired by Impagliazzo et al. [2022], we say that an RL algorithm is replicable if, with high probability, it outputs the exact same policy after two executions on i.i.d. samples drawn from the generator when its internal randomness is the same. We first provide an efficient $\\rho$-replicable algorithm for $(\\varepsilon, \\delta)$-optimal policy estimation with sample and time complexity $\\widetilde O\\left(\\frac{N^3\\cdot\\log(1/\\delta)}{(1-\\gamma)^5\\cdot\\varepsilon^2\\cdot\\rho^2}\\right)$, where $N$ is the number of state-action pairs. Next, for the subclass of deterministic algorithms, we provide a lower bound of order $\\Omega\\left(\\frac{N^3}{(1-\\gamma)^3\\cdot\\varepsilon^2\\cdot\\rho^2}\\right)$. Then, we study a relaxed version of replicability proposed by Kalavasis et ",
    "link": "http://arxiv.org/abs/2305.19562",
    "context": "Title: Replicability in Reinforcement Learning. (arXiv:2305.19562v1 [cs.LG])\nAbstract: We initiate the mathematical study of replicability as an algorithmic property in the context of reinforcement learning (RL). We focus on the fundamental setting of discounted tabular MDPs with access to a generative model. Inspired by Impagliazzo et al. [2022], we say that an RL algorithm is replicable if, with high probability, it outputs the exact same policy after two executions on i.i.d. samples drawn from the generator when its internal randomness is the same. We first provide an efficient $\\rho$-replicable algorithm for $(\\varepsilon, \\delta)$-optimal policy estimation with sample and time complexity $\\widetilde O\\left(\\frac{N^3\\cdot\\log(1/\\delta)}{(1-\\gamma)^5\\cdot\\varepsilon^2\\cdot\\rho^2}\\right)$, where $N$ is the number of state-action pairs. Next, for the subclass of deterministic algorithms, we provide a lower bound of order $\\Omega\\left(\\frac{N^3}{(1-\\gamma)^3\\cdot\\varepsilon^2\\cdot\\rho^2}\\right)$. Then, we study a relaxed version of replicability proposed by Kalavasis et ",
    "path": "papers/23/05/2305.19562.json",
    "total_tokens": 1305,
    "translated_title": "强化学习中的可复现性研究",
    "translated_abstract": "我们在强化学习 (RL) 的背景下，将可复现性作为算法属性进行了数学研究。我们关注的是具有生成模型访问权的带折扣表格MDP的基本设置。受Impagliazzo等人 [2022]的启发，如果在内部随机性相同时，RL算法在从生成器抽取的两个独立和同分布的样本上执行两次并输出完全相同的策略，则表示该RL算法是可复制的。我们首先提供一个有效的$\\rho$-可复制算法，用于$(\\varepsilon,\\delta)$-最优策略估计，其样本和时间复杂度为 $\\widetilde O\\left(\\frac{N^3\\cdot\\log(1/\\delta)}{(1-\\gamma)^5\\cdot\\varepsilon^2\\cdot\\rho^2}\\right)$，其中$N$是状态-动作对的数量。然后，对于确定性算法的子类，我们提供了 $ \\Omega\\left(\\frac {N^3}{(1-\\gamma)^3\\cdot\\varepsilon^2\\cdot\\rho^2}\\right) $ 阶的下限。接下来，我们研究了Kalavasis等人[2019]提出的可复制性的松弛版本，其中仅要求算法的输出接近复制算法的输出，而不是相同。我们提供了一种有效算法，其时间和样本复杂度为 $\\widetilde O\\left(\\frac{N^5\\cdot\\log(1/\\delta)}{(1-\\gamma)^9\\cdot\\varepsilon^4\\cdot\\rho^2}\\right)$，用于$(\\varepsilon,\\delta)$意义下的可复制性，这比先前与相关问题的界限更好。最后，我们讨论了我们的结果对RL算法设计和可重复性研究的未来方向的影响。",
    "tldr": "这篇论文研究了在强化学习中的可复制性，提出了可复制算法和松弛可复制算法，并给出了相应的时间和样本复杂度，这对于RL算法设计以及未来的可复制性研究具有影响。"
}