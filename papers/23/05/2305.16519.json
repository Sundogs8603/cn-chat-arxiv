{
    "title": "The Dangers of trusting Stochastic Parrots: Faithfulness and Trust in Open-domain Conversational Question Answering. (arXiv:2305.16519v1 [cs.CL])",
    "abstract": "Large language models are known to produce output which sounds fluent and convincing, but is also often wrong, e.g. \"unfaithful\" with respect to a rationale as retrieved from a knowledge base. In this paper, we show that task-based systems which exhibit certain advanced linguistic dialog behaviors, such as lexical alignment (repeating what the user said), are in fact preferred and trusted more, whereas other phenomena, such as pronouns and ellipsis are dis-preferred. We use open-domain question answering systems as our test-bed for task based dialog generation and compare several open- and closed-book models. Our results highlight the danger of systems that appear to be trustworthy by parroting user input while providing an unfaithful response.",
    "link": "http://arxiv.org/abs/2305.16519",
    "context": "Title: The Dangers of trusting Stochastic Parrots: Faithfulness and Trust in Open-domain Conversational Question Answering. (arXiv:2305.16519v1 [cs.CL])\nAbstract: Large language models are known to produce output which sounds fluent and convincing, but is also often wrong, e.g. \"unfaithful\" with respect to a rationale as retrieved from a knowledge base. In this paper, we show that task-based systems which exhibit certain advanced linguistic dialog behaviors, such as lexical alignment (repeating what the user said), are in fact preferred and trusted more, whereas other phenomena, such as pronouns and ellipsis are dis-preferred. We use open-domain question answering systems as our test-bed for task based dialog generation and compare several open- and closed-book models. Our results highlight the danger of systems that appear to be trustworthy by parroting user input while providing an unfaithful response.",
    "path": "papers/23/05/2305.16519.json",
    "total_tokens": 804,
    "translated_title": "相信随机鹦鹉的危险：自然对话问答中的忠诚度与信任",
    "translated_abstract": "大型语言模型往往能够生产出流畅而令人信服的输出结果，但它们经常会出现错误，即从知识库中提取的答案与客观事实不符。本文表明，那些展现出先进语言对话行为（如重复用户所说的话）的任务型系统，实际上更受欢迎、更值得信任，而其他现象（如代词和省略）则不被青睐。我们以开放域问答系统为测试基础，比较了数个开放式和封闭式图书模型的任务生成对话。研究结果突出表明，系统在模仿用户输入的同时提供不忠实的响应，这种表现看似可信，实则危险。",
    "tldr": "本文研究表明，具备某些高级语言对话行为（如重复用户所说的话）的任务型系统更受欢迎、更值得信任，然而那些一味模仿用户输入的系统却存在诸多不忠实响应的风险。",
    "en_tdlr": "This paper shows that task-based systems exhibiting certain advanced linguistic dialog behaviors are preferred and trusted more, while systems that simply parrot user inputs while providing unfaithful responses pose a danger."
}