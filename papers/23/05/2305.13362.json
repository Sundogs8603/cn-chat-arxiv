{
    "title": "On quantum backpropagation, information reuse, and cheating measurement collapse. (arXiv:2305.13362v1 [quant-ph])",
    "abstract": "The success of modern deep learning hinges on the ability to train neural networks at scale. Through clever reuse of intermediate information, backpropagation facilitates training through gradient computation at a total cost roughly proportional to running the function, rather than incurring an additional factor proportional to the number of parameters - which can now be in the trillions. Naively, one expects that quantum measurement collapse entirely rules out the reuse of quantum information as in backpropagation. But recent developments in shadow tomography, which assumes access to multiple copies of a quantum state, have challenged that notion. Here, we investigate whether parameterized quantum models can train as efficiently as classical neural networks. We show that achieving backpropagation scaling is impossible without access to multiple copies of a state. With this added ability, we introduce an algorithm with foundations in shadow tomography that matches backpropagation scali",
    "link": "http://arxiv.org/abs/2305.13362",
    "context": "Title: On quantum backpropagation, information reuse, and cheating measurement collapse. (arXiv:2305.13362v1 [quant-ph])\nAbstract: The success of modern deep learning hinges on the ability to train neural networks at scale. Through clever reuse of intermediate information, backpropagation facilitates training through gradient computation at a total cost roughly proportional to running the function, rather than incurring an additional factor proportional to the number of parameters - which can now be in the trillions. Naively, one expects that quantum measurement collapse entirely rules out the reuse of quantum information as in backpropagation. But recent developments in shadow tomography, which assumes access to multiple copies of a quantum state, have challenged that notion. Here, we investigate whether parameterized quantum models can train as efficiently as classical neural networks. We show that achieving backpropagation scaling is impossible without access to multiple copies of a state. With this added ability, we introduce an algorithm with foundations in shadow tomography that matches backpropagation scali",
    "path": "papers/23/05/2305.13362.json",
    "total_tokens": 1020,
    "translated_title": "关于量子反向传播、信息重用和欺骗测量坍塌的研究",
    "translated_abstract": "现代深度学习的成功建立在能够大规模训练神经网络的能力上。通过巧妙重用中间信息，反向传播通过计算函数的梯度来实现训练，总成本大致与运行函数成比例，而不会产生与参数数量成比例的额外因素，这个数量现在可能高达数万亿。很自然地，人们认为量子测量坍塌完全排除了如反向传播中的量子信息重用。但最近阴影测量的发展挑战了这一概念，阴影测量假设可以访问量子态的多个副本。在这里，我们调查参数化量子模型是否能像经典神经网络那样高效地训练。我们表明，要实现反向传播的扩展需要访问一个状态的多个副本，缺少这种能力是不可能的。在此基础上，我们介绍了一个源于阴影测量的算法，它与反向传播的性能匹配。",
    "tldr": "这篇论文研究了量子模型是否能够像经典神经网络一样高效地进行训练，发现要实现反向传播的扩展需要访问一个状态的多个副本，缺少这种能力是不可能的，基于阴影测量的算法可以与反向传播的性能匹配。",
    "en_tdlr": "This paper investigates whether parameterized quantum models can be trained as efficiently as classical neural networks using backpropagation, and shows that achieving backpropagation scaling is impossible without access to multiple copies of a state. Based on shadow tomography, the proposed algorithm matches the performance of backpropagation."
}