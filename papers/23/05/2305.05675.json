{
    "title": "UAdam: Unified Adam-Type Algorithmic Framework for Non-Convex Stochastic Optimization. (arXiv:2305.05675v1 [cs.LG])",
    "abstract": "Adam-type algorithms have become a preferred choice for optimisation in the deep learning setting, however, despite success, their convergence is still not well understood. To this end, we introduce a unified framework for Adam-type algorithms (called UAdam). This is equipped with a general form of the second-order moment, which makes it possible to include Adam and its variants as special cases, such as NAdam, AMSGrad, AdaBound, AdaFom, and Adan. This is supported by a rigorous convergence analysis of UAdam in the non-convex stochastic setting, showing that UAdam converges to the neighborhood of stationary points with the rate of $\\mathcal{O}(1/T)$. Furthermore, the size of neighborhood decreases as $\\beta$ increases. Importantly, our analysis only requires the first-order momentum factor to be close enough to 1, without any restrictions on the second-order momentum factor. Theoretical results also show that vanilla Adam can converge by selecting appropriate hyperparameters, which pro",
    "link": "http://arxiv.org/abs/2305.05675",
    "context": "Title: UAdam: Unified Adam-Type Algorithmic Framework for Non-Convex Stochastic Optimization. (arXiv:2305.05675v1 [cs.LG])\nAbstract: Adam-type algorithms have become a preferred choice for optimisation in the deep learning setting, however, despite success, their convergence is still not well understood. To this end, we introduce a unified framework for Adam-type algorithms (called UAdam). This is equipped with a general form of the second-order moment, which makes it possible to include Adam and its variants as special cases, such as NAdam, AMSGrad, AdaBound, AdaFom, and Adan. This is supported by a rigorous convergence analysis of UAdam in the non-convex stochastic setting, showing that UAdam converges to the neighborhood of stationary points with the rate of $\\mathcal{O}(1/T)$. Furthermore, the size of neighborhood decreases as $\\beta$ increases. Importantly, our analysis only requires the first-order momentum factor to be close enough to 1, without any restrictions on the second-order momentum factor. Theoretical results also show that vanilla Adam can converge by selecting appropriate hyperparameters, which pro",
    "path": "papers/23/05/2305.05675.json",
    "total_tokens": 978,
    "translated_title": "UAdam：非凸随机优化的统一Adam型算法框架",
    "translated_abstract": "Adam型算法在深度学习中已成为优化的首选，然而，尽管成功，其收敛性仍不够理解。为此，我们引入了一个统一Adam型算法框架（称为UAdam）。它配备了第二阶矩的一般形式，可以包括Adam及其变体作为特例，如NAdam、AMSGrad、AdaBound、AdaFom和Adan。在非凸随机设置下，我们对UAdam进行了严格的收敛性分析，表明UAdam以$\\mathcal{O}(1/T)$的速率收敛到静止点的邻域。此外，随着$\\beta$的增加，邻域的大小减小。重要的是，我们的分析仅要求第一阶动量因子足够接近1，对于第二阶动量因子没有任何限制。理论结果还表明，通过选择适当的超参数，香草Adam可以收敛，这为实践者选择Adam族算法的超参数提供了指导。",
    "tldr": "本论文提出了统一的Adam型算法框架UAdam，该框架是包括Adam及其变体在内的特例，并且在非凸随机设置下具有收敛性，可以以$\\mathcal{O}(1/T)$的速率收敛到静止点的邻域。通过选择适当的超参数，香草Adam也可以收敛。",
    "en_tdlr": "This paper proposes a unified framework for Adam-type algorithms, called UAdam, which includes Adam and its variants as special cases. The framework has convergence properties in the non-convex stochastic setting and can converge to the neighborhood of stationary points with a rate of $\\mathcal{O}(1/T)$. Theoretical results also show that vanilla Adam can converge by selecting appropriate hyperparameters."
}