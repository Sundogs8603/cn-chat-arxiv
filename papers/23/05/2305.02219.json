{
    "title": "LESS-VFL: Communication-Efficient Feature Selection for Vertical Federated Learning. (arXiv:2305.02219v1 [cs.LG])",
    "abstract": "We propose LESS-VFL, a communication-efficient feature selection method for distributed systems with vertically partitioned data. We consider a system of a server and several parties with local datasets that share a sample ID space but have different feature sets. The parties wish to collaboratively train a model for a prediction task. As part of the training, the parties wish to remove unimportant features in the system to improve generalization, efficiency, and explainability. In LESS-VFL, after a short pre-training period, the server optimizes its part of the global model to determine the relevant outputs from party models. This information is shared with the parties to then allow local feature selection without communication. We analytically prove that LESS-VFL removes spurious features from model training. We provide extensive empirical evidence that LESS-VFL can achieve high accuracy and remove spurious features at a fraction of the communication cost of other feature selection a",
    "link": "http://arxiv.org/abs/2305.02219",
    "context": "Title: LESS-VFL: Communication-Efficient Feature Selection for Vertical Federated Learning. (arXiv:2305.02219v1 [cs.LG])\nAbstract: We propose LESS-VFL, a communication-efficient feature selection method for distributed systems with vertically partitioned data. We consider a system of a server and several parties with local datasets that share a sample ID space but have different feature sets. The parties wish to collaboratively train a model for a prediction task. As part of the training, the parties wish to remove unimportant features in the system to improve generalization, efficiency, and explainability. In LESS-VFL, after a short pre-training period, the server optimizes its part of the global model to determine the relevant outputs from party models. This information is shared with the parties to then allow local feature selection without communication. We analytically prove that LESS-VFL removes spurious features from model training. We provide extensive empirical evidence that LESS-VFL can achieve high accuracy and remove spurious features at a fraction of the communication cost of other feature selection a",
    "path": "papers/23/05/2305.02219.json",
    "total_tokens": 948,
    "translated_title": "LESS-VFL：用于竖直联邦学习的通信高效特征选择",
    "translated_abstract": "我们提出了LESS-VFL，这是一种用于具有竖直分割数据的分布式系统的通信高效特征选择方法。我们考虑一个由服务器和多个带有本地数据集的参与者组成的系统，这些数据集共享样本ID空间，但具有不同的特征集。参与者希望协作地训练一个用于预测任务的模型。作为训练的一部分，参与者希望在系统中删除不重要的特征，以提高泛化性能，效率和可解释性。在LESS-VFL中，经过短暂的预训练后，服务器优化其全局模型的一部分，以确定来自参与者模型的相关输出。这些信息与参与者共享，然后允许本地特征选择而无需通信。我们从理论上证明了LESS-VFL可以从模型训练中删除虚假特征。我们提供了广泛的经验证据表明LESS-VFL可以以其他特征选择算法的通信成本的一小部分获得高精度并删除虚假特征。",
    "tldr": "我们提出了LESS-VFL方法，用于竖直联邦学习中通信高效的特征选择。该方法通过短暂的预训练和本地特征选择，在减少通信成本的同时，可以从模型训练中删除虚假特征。"
}