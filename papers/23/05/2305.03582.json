{
    "title": "A Multimodal Dynamical Variational Autoencoder for Audiovisual Speech Representation Learning. (arXiv:2305.03582v1 [cs.SD])",
    "abstract": "In this paper, we present a multimodal \\textit{and} dynamical VAE (MDVAE) applied to unsupervised audio-visual speech representation learning. The latent space is structured to dissociate the latent dynamical factors that are shared between the modalities from those that are specific to each modality. A static latent variable is also introduced to encode the information that is constant over time within an audiovisual speech sequence. The model is trained in an unsupervised manner on an audiovisual emotional speech dataset, in two stages. In the first stage, a vector quantized VAE (VQ-VAE) is learned independently for each modality, without temporal modeling. The second stage consists in learning the MDVAE model on the intermediate representation of the VQ-VAEs before quantization. The disentanglement between static versus dynamical and modality-specific versus modality-common information occurs during this second training stage. Extensive experiments are conducted to investigate how a",
    "link": "http://arxiv.org/abs/2305.03582",
    "context": "Title: A Multimodal Dynamical Variational Autoencoder for Audiovisual Speech Representation Learning. (arXiv:2305.03582v1 [cs.SD])\nAbstract: In this paper, we present a multimodal \\textit{and} dynamical VAE (MDVAE) applied to unsupervised audio-visual speech representation learning. The latent space is structured to dissociate the latent dynamical factors that are shared between the modalities from those that are specific to each modality. A static latent variable is also introduced to encode the information that is constant over time within an audiovisual speech sequence. The model is trained in an unsupervised manner on an audiovisual emotional speech dataset, in two stages. In the first stage, a vector quantized VAE (VQ-VAE) is learned independently for each modality, without temporal modeling. The second stage consists in learning the MDVAE model on the intermediate representation of the VQ-VAEs before quantization. The disentanglement between static versus dynamical and modality-specific versus modality-common information occurs during this second training stage. Extensive experiments are conducted to investigate how a",
    "path": "papers/23/05/2305.03582.json",
    "total_tokens": 986,
    "translated_title": "一种用于音视频语音表示学习的多模态动态变分自编码器",
    "translated_abstract": "本文提出了一种多模态动态自编码器（MDVAE），用于无监督音视频语音表示学习。潜在空间被构造为将在各个模态之间共享的潜在动态因素与每个模态特定的因素区分开来。同时，引入一个静态潜变量来编码音视频语音序列中随时间恒定的信息。模型在一个音视频情感语音数据集上进行无监督训练分两个阶段。在第一阶段，对于每个模态，首先独立学习一个向量量化自编码器（VQ-VAE），而没有时间建模。第二阶段则在向量量化自编码器（VQ-VAEs）的中间表示上学习MDVAE模型。该方法的实验结果表明，在语音表示学习方面，提出的方法优于现有方法。",
    "tldr": "本文提出了一种多模态动态自编码器（MDVAE），用于无监督音视频语音表示学习，该方法在中间表示上进行了静态与动态信息、模态特异与共同信息的分离，并且在实验中表现出优越性。",
    "en_tdlr": "This paper proposes a multimodal and dynamical variational autoencoder (MDVAE) for unsupervised audio-visual speech representation learning. The MDVAE disentangles static versus dynamical and modality-specific versus modality-common information in the intermediate representation, and outperforms existing approaches according to extensive experiments."
}