{
    "title": "Out-of-Distributed Semantic Pruning for Robust Semi-Supervised Learning. (arXiv:2305.18158v2 [cs.CV] UPDATED)",
    "abstract": "Recent advances in robust semi-supervised learning (SSL) typically filter out-of-distribution (OOD) information at the sample level. We argue that an overlooked problem of robust SSL is its corrupted information on semantic level, practically limiting the development of the field. In this paper, we take an initial step to explore and propose a unified framework termed OOD Semantic Pruning (OSP), which aims at pruning OOD semantics out from in-distribution (ID) features. Specifically, (i) we propose an aliasing OOD matching module to pair each ID sample with an OOD sample with semantic overlap. (ii) We design a soft orthogonality regularization, which first transforms each ID feature by suppressing its semantic component that is collinear with paired OOD sample. It then forces the predictions before and after soft orthogonality decomposition to be consistent. Being practically simple, our method shows a strong performance in OOD detection and ID classification on challenging benchmarks.",
    "link": "http://arxiv.org/abs/2305.18158",
    "context": "Title: Out-of-Distributed Semantic Pruning for Robust Semi-Supervised Learning. (arXiv:2305.18158v2 [cs.CV] UPDATED)\nAbstract: Recent advances in robust semi-supervised learning (SSL) typically filter out-of-distribution (OOD) information at the sample level. We argue that an overlooked problem of robust SSL is its corrupted information on semantic level, practically limiting the development of the field. In this paper, we take an initial step to explore and propose a unified framework termed OOD Semantic Pruning (OSP), which aims at pruning OOD semantics out from in-distribution (ID) features. Specifically, (i) we propose an aliasing OOD matching module to pair each ID sample with an OOD sample with semantic overlap. (ii) We design a soft orthogonality regularization, which first transforms each ID feature by suppressing its semantic component that is collinear with paired OOD sample. It then forces the predictions before and after soft orthogonality decomposition to be consistent. Being practically simple, our method shows a strong performance in OOD detection and ID classification on challenging benchmarks.",
    "path": "papers/23/05/2305.18158.json",
    "total_tokens": 969,
    "translated_title": "鲁棒半监督学习的分布外语义修剪",
    "translated_abstract": "最近鲁棒半监督学习（SSL）中的进展通常在样本级别过滤分布外（OOD）信息。我们认为鲁棒SSL的一个被忽视的问题是其在语义级别上的损坏信息，实际上限制了该领域的发展。在本文中，我们采取了初步措施，探索并提出了一种统一的框架，称为OOD语义修剪（OSP），旨在从分布内（ID）特征中修剪OOD语义。具体而言，(i)我们提出了一个别名OOD匹配模块，将每个ID样本与一个OOD样本进行语义重叠匹配。(ii)我们设计了一个软正交正则化，它首先通过抑制与配对OOD样本共线的语义成分来转换每个ID特征。然后，它强制在软正交分解之前和之后的预测结果一致。我们的方法在具有挑战性的基准测试中表现出强大的性能，能够进行OOD检测和ID分类。",
    "tldr": "本文提出了一种命名为OOD语义修剪（OSP）的框架，该框架旨在从分布内（ID）特征中修剪掉分布外（OOD）的语义信息，这有助于提高鲁棒半监督学习的性能。",
    "en_tdlr": "The paper proposes a framework called OOD Semantic Pruning (OSP) to prune OOD semantics from in-distribution (ID) features which can improve the performance of robust semi-supervised learning. The OSP uses an aliasing OOD matching module to pair each ID sample with an OOD sample with semantic overlap and a soft orthogonality regularization to transform each ID feature by suppressing its semantic component that is collinear with paired OOD sample."
}