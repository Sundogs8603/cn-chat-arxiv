{
    "title": "Text-To-Concept (and Back) via Cross-Model Alignment. (arXiv:2305.06386v1 [cs.CV])",
    "abstract": "We observe that the mapping between an image's representation in one model to its representation in another can be learned surprisingly well with just a linear layer, even across diverse models. Building on this observation, we propose $\\textit{text-to-concept}$, where features from a fixed pretrained model are aligned linearly to the CLIP space, so that text embeddings from CLIP's text encoder become directly comparable to the aligned features. With text-to-concept, we convert fixed off-the-shelf vision encoders to surprisingly strong zero-shot classifiers for free, with accuracy at times even surpassing that of CLIP, despite being much smaller models and trained on a small fraction of the data compared to CLIP. We show other immediate use-cases of text-to-concept, like building concept bottleneck models with no concept supervision, diagnosing distribution shifts in terms of human concepts, and retrieving images satisfying a set of text-based constraints. Lastly, we demonstrate the fe",
    "link": "http://arxiv.org/abs/2305.06386",
    "context": "Title: Text-To-Concept (and Back) via Cross-Model Alignment. (arXiv:2305.06386v1 [cs.CV])\nAbstract: We observe that the mapping between an image's representation in one model to its representation in another can be learned surprisingly well with just a linear layer, even across diverse models. Building on this observation, we propose $\\textit{text-to-concept}$, where features from a fixed pretrained model are aligned linearly to the CLIP space, so that text embeddings from CLIP's text encoder become directly comparable to the aligned features. With text-to-concept, we convert fixed off-the-shelf vision encoders to surprisingly strong zero-shot classifiers for free, with accuracy at times even surpassing that of CLIP, despite being much smaller models and trained on a small fraction of the data compared to CLIP. We show other immediate use-cases of text-to-concept, like building concept bottleneck models with no concept supervision, diagnosing distribution shifts in terms of human concepts, and retrieving images satisfying a set of text-based constraints. Lastly, we demonstrate the fe",
    "path": "papers/23/05/2305.06386.json",
    "total_tokens": 945,
    "translated_title": "跨模型对齐实现文本到概念的转换",
    "translated_abstract": "本文观察到即使在不同模型之间，图像表示的映射也可以通过仅使用线性层进行学习。在此基础上，本文提出了\"Text-To-Concept\"，其中来自预训练模型的特征与CLIP空间进行线性对齐，使得来自CLIP文本编码器的文本嵌入可直接与对齐特征进行比较。通过Text-To-Concept转换，可免费将固定的现成视觉编码器转换为强大的零样本分类器，有时甚至可以超过CLIP的精度，即使这些编码器比CLIP小得多，并且相对于CLIP，训练数据仅占很小一部分。本文还展示了Text-To-Concept的其他直接应用：如构建不需要概念监督的概念瓶颈模型，通过人类概念诊断分布移位，并检索满足一组基于文本的约束条件的图像。最后，本文证明了特征对准确实现界限。",
    "tldr": "本文介绍了\"Text-To-Concept\"的方法，通过特征对齐，将来自预训练模型的特征转换为可与文本编码器比较的标准化形式，并免费将视觉编码器转换为零样本分类器。"
}