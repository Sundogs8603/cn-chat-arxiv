{
    "title": "Whitening-based Contrastive Learning of Sentence Embeddings. (arXiv:2305.17746v2 [cs.CL] UPDATED)",
    "abstract": "This paper presents a whitening-based contrastive learning method for sentence embedding learning (WhitenedCSE), which combines contrastive learning with a novel shuffled group whitening. Generally, contrastive learning pulls distortions of a single sample (i.e., positive samples) close and push negative samples far away, correspondingly facilitating the alignment and uniformity in the feature space. A popular alternative to the \"pushing'' operation is whitening the feature space, which scatters all the samples for uniformity. Since the whitening and the contrastive learning have large redundancy w.r.t. the uniformity, they are usually used separately and do not easily work together. For the first time, this paper integrates whitening into the contrastive learning scheme and facilitates two benefits. 1) Better uniformity. We find that these two approaches are not totally redundant but actually have some complementarity due to different uniformity mechanism. 2) Better alignment. We rand",
    "link": "http://arxiv.org/abs/2305.17746",
    "context": "Title: Whitening-based Contrastive Learning of Sentence Embeddings. (arXiv:2305.17746v2 [cs.CL] UPDATED)\nAbstract: This paper presents a whitening-based contrastive learning method for sentence embedding learning (WhitenedCSE), which combines contrastive learning with a novel shuffled group whitening. Generally, contrastive learning pulls distortions of a single sample (i.e., positive samples) close and push negative samples far away, correspondingly facilitating the alignment and uniformity in the feature space. A popular alternative to the \"pushing'' operation is whitening the feature space, which scatters all the samples for uniformity. Since the whitening and the contrastive learning have large redundancy w.r.t. the uniformity, they are usually used separately and do not easily work together. For the first time, this paper integrates whitening into the contrastive learning scheme and facilitates two benefits. 1) Better uniformity. We find that these two approaches are not totally redundant but actually have some complementarity due to different uniformity mechanism. 2) Better alignment. We rand",
    "path": "papers/23/05/2305.17746.json",
    "total_tokens": 882,
    "translated_title": "基于白化的对比学习句子嵌入",
    "translated_abstract": "本文提出了一种基于白化的对比学习方法用于学习句子嵌入（WhitenedCSE），它将对比学习与一种新颖的洗牌组白化结合起来。通常，对比学习拉近一个样本的扭曲（即正样本）并将负样本远离，从而促进特征空间的对齐和一致性。\"推\"操作的另一种流行替代方案是白化特征空间，它散布所有样本以实现一致性。由于白化和对比学习相对于一致性具有大量冗余，它们通常单独使用且不容易共同工作。本文首次将白化集成到对比学习方案中，并促进了两个好处。1) 更好的一致性。我们发现这两种方法不完全冗余，实际上由于不同的一致性机制，它们有一些互补性。2) 更好的对齐。",
    "tldr": "本文提出一种基于白化的对比学习方法用于学习句子嵌入，它将对比学习与白化方法结合起来，同时具备更好的一致性和更好的对齐效果。"
}