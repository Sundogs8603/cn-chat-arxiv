{
    "title": "Towards Higher Pareto Frontier in Multilingual Machine Translation. (arXiv:2305.15718v1 [cs.CL])",
    "abstract": "Multilingual neural machine translation has witnessed remarkable progress in recent years. However, the long-tailed distribution of multilingual corpora poses a challenge of Pareto optimization, i.e., optimizing for some languages may come at the cost of degrading the performance of others. Existing balancing training strategies are equivalent to a series of Pareto optimal solutions, which trade off on a Pareto frontier. In this work, we propose a new training framework, Pareto Mutual Distillation (Pareto-MD), towards pushing the Pareto frontier outwards rather than making trade-offs. Specifically, Pareto-MD collaboratively trains two Pareto optimal solutions that favor different languages and allows them to learn from the strengths of each other via knowledge distillation. Furthermore, we introduce a novel strategy to enable stronger communication between Pareto optimal solutions and broaden the applicability of our approach. Experimental results on the widely-used WMT and TED dataset",
    "link": "http://arxiv.org/abs/2305.15718",
    "context": "Title: Towards Higher Pareto Frontier in Multilingual Machine Translation. (arXiv:2305.15718v1 [cs.CL])\nAbstract: Multilingual neural machine translation has witnessed remarkable progress in recent years. However, the long-tailed distribution of multilingual corpora poses a challenge of Pareto optimization, i.e., optimizing for some languages may come at the cost of degrading the performance of others. Existing balancing training strategies are equivalent to a series of Pareto optimal solutions, which trade off on a Pareto frontier. In this work, we propose a new training framework, Pareto Mutual Distillation (Pareto-MD), towards pushing the Pareto frontier outwards rather than making trade-offs. Specifically, Pareto-MD collaboratively trains two Pareto optimal solutions that favor different languages and allows them to learn from the strengths of each other via knowledge distillation. Furthermore, we introduce a novel strategy to enable stronger communication between Pareto optimal solutions and broaden the applicability of our approach. Experimental results on the widely-used WMT and TED dataset",
    "path": "papers/23/05/2305.15718.json",
    "total_tokens": 944,
    "translated_title": "走向更高的多语言机器翻译帕累托前沿",
    "translated_abstract": "近年来，多语言神经机器翻译取得了显著进展。然而，多语料库的长尾分布形成了帕累托最优化的挑战，即为了优化某些语言的翻译，可能损害其他语言的性能。现有的平衡训练策略等同于一系列帕累托最优解，它们在帕累托前沿上进行权衡。在本文中，我们提出了一种新的训练框架——帕累托互攻（Pareto-MD），旨在将帕累托前沿向外推进，而不是进行权衡。具体来说，Pareto-MD共同训练两个偏向不同语言的帕累托最优解，并通过知识蒸馏让它们相互学习优点。此外，我们还引入了一种新的策略，以实现更强大的帕累托最优解之间的通信，拓宽了我们方法的适用范围。实验结果表明，在广泛使用的WMT和TED数据集上，我们的方法都获得了显著的改进。",
    "tldr": "本文提出了一种新的训练框架——帕累托互攻（Pareto-MD），旨在将帕累托前沿向外推进，而不是进行权衡，以提高多语言机器翻译性能。",
    "en_tdlr": "This paper proposes a new training framework, Pareto Mutual Distillation (Pareto-MD), to push the Pareto frontier outwards rather than compromising among languages for better multilingual machine translation performance."
}