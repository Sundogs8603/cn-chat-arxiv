{
    "title": "Conditional mean embeddings and optimal feature selection via positive definite kernels. (arXiv:2305.08100v1 [cs.LG])",
    "abstract": "Motivated by applications, we consider here new operator theoretic approaches to Conditional mean embeddings (CME). Our present results combine a spectral analysis-based optimization scheme with the use of kernels, stochastic processes, and constructive learning algorithms. For initially given non-linear data, we consider optimization-based feature selections. This entails the use of convex sets of positive definite (p.d.) kernels in a construction of optimal feature selection via regression algorithms from learning models. Thus, with initial inputs of training data (for a suitable learning algorithm,) each choice of p.d. kernel $K$ in turn yields a variety of Hilbert spaces and realizations of features. A novel idea here is that we shall allow an optimization over selected sets of kernels $K$ from a convex set $C$ of positive definite kernels $K$. Hence our \\textquotedblleft optimal\\textquotedblright{} choices of feature representations will depend on a secondary optimization over p.d",
    "link": "http://arxiv.org/abs/2305.08100",
    "context": "Title: Conditional mean embeddings and optimal feature selection via positive definite kernels. (arXiv:2305.08100v1 [cs.LG])\nAbstract: Motivated by applications, we consider here new operator theoretic approaches to Conditional mean embeddings (CME). Our present results combine a spectral analysis-based optimization scheme with the use of kernels, stochastic processes, and constructive learning algorithms. For initially given non-linear data, we consider optimization-based feature selections. This entails the use of convex sets of positive definite (p.d.) kernels in a construction of optimal feature selection via regression algorithms from learning models. Thus, with initial inputs of training data (for a suitable learning algorithm,) each choice of p.d. kernel $K$ in turn yields a variety of Hilbert spaces and realizations of features. A novel idea here is that we shall allow an optimization over selected sets of kernels $K$ from a convex set $C$ of positive definite kernels $K$. Hence our \\textquotedblleft optimal\\textquotedblright{} choices of feature representations will depend on a secondary optimization over p.d",
    "path": "papers/23/05/2305.08100.json",
    "total_tokens": 784,
    "translated_title": "条件平均嵌入和通过正定核的最优特征选择",
    "translated_abstract": "本文提出一种新的算子理论方法来解决条件平均嵌入问题。基于谱分析优化算法和核、随机过程以及建设性学习算法的组合结果，我们构造了对于非线性数据的基于优化的特征选择。通过使用正定核的凸集，我们可以选择最优的特征表示，从而得到多种希尔伯特空间和特征的实现方式。我们的一个新想法是，我们允许从正定核的凸集中选择一组核，并在此基础上进行二次优化，以获得“最佳”特征表示。",
    "tldr": "本文提出一种新的算子理论方法来解决条件平均嵌入问题，构造了非线性数据的基于优化的特征选择。通过使用正定核的凸集，得到多种希尔伯特空间和特征的实现方式。",
    "en_tdlr": "This paper proposes a new operator theoretic approach to solve Conditional mean embeddings (CME) and constructs optimization-based feature selection for non-linear data. By using convex sets of positive definite kernels, it obtains multiple implementations of features from a variety of Hilbert spaces."
}