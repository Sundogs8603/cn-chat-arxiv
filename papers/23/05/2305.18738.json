{
    "title": "Generating Behaviorally Diverse Policies with Latent Diffusion Models. (arXiv:2305.18738v2 [cs.LG] UPDATED)",
    "abstract": "Recent progress in Quality Diversity Reinforcement Learning (QD-RL) has enabled learning a collection of behaviorally diverse, high performing policies. However, these methods typically involve storing thousands of policies, which results in high space-complexity and poor scaling to additional behaviors. Condensing the archive into a single model while retaining the performance and coverage of the original collection of policies has proved challenging. In this work, we propose using diffusion models to distill the archive into a single generative model over policy parameters. We show that our method achieves a compression ratio of 13x while recovering 98% of the original rewards and 89% of the original coverage. Further, the conditioning mechanism of diffusion models allows for flexibly selecting and sequencing behaviors, including using language. Project website: https://sites.google.com/view/policydiffusion/home",
    "link": "http://arxiv.org/abs/2305.18738",
    "context": "Title: Generating Behaviorally Diverse Policies with Latent Diffusion Models. (arXiv:2305.18738v2 [cs.LG] UPDATED)\nAbstract: Recent progress in Quality Diversity Reinforcement Learning (QD-RL) has enabled learning a collection of behaviorally diverse, high performing policies. However, these methods typically involve storing thousands of policies, which results in high space-complexity and poor scaling to additional behaviors. Condensing the archive into a single model while retaining the performance and coverage of the original collection of policies has proved challenging. In this work, we propose using diffusion models to distill the archive into a single generative model over policy parameters. We show that our method achieves a compression ratio of 13x while recovering 98% of the original rewards and 89% of the original coverage. Further, the conditioning mechanism of diffusion models allows for flexibly selecting and sequencing behaviors, including using language. Project website: https://sites.google.com/view/policydiffusion/home",
    "path": "papers/23/05/2305.18738.json",
    "total_tokens": 875,
    "translated_title": "使用潜在扩散模型生成行为多样化的策略",
    "translated_abstract": "最近在品质多样化强化学习（QD-RL）领域取得了进展，使得学习行为多样化、高性能的策略成为可能。然而，这些方法通常涉及存储数千个策略，导致空间复杂度高且难以适应更多行为的扩展。将档案压缩为单个模型，同时保留原始策略集的性能和覆盖率，已被证明是具有挑战性的。在本文中，我们提出使用扩散模型将归档压缩为单个对于策略参数的生成模型。我们展示了我们的方法实现了13倍的压缩比率，同时恢复了98%的原始回报和89%的原始覆盖率。扩散模型的调节机制还允许灵活选择和排序行为，包括使用语言。",
    "tldr": "本文使用扩散模型将档案压缩为单个对于策略参数的生成模型，实现了13倍的压缩比率，同时保留了98%的原始回报和89%的原始覆盖率，并允许灵活选择和排序行为。",
    "en_tdlr": "This paper proposes using diffusion models to compress an archive of policies into a single generative model over policy parameters, achieving a compression ratio of 13x while retaining 98% of the original rewards and 89% of the original coverage. The conditioning mechanism of diffusion models allows for flexible selection and sequencing of behaviors, including using language."
}