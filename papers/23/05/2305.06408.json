{
    "title": "Accelerating Batch Active Learning Using Continual Learning Techniques. (arXiv:2305.06408v1 [cs.LG])",
    "abstract": "A major problem with Active Learning (AL) is high training costs since models are typically retrained from scratch after every query round. We start by demonstrating that standard AL on neural networks with warm starting fails, both to accelerate training and to avoid catastrophic forgetting when using fine-tuning over AL query rounds. We then develop a new class of techniques, circumventing this problem, by biasing further training towards previously labeled sets. We accomplish this by employing existing, and developing novel, replay-based Continual Learning (CL) algorithms that are effective at quickly learning the new without forgetting the old, especially when data comes from an evolving distribution. We call this paradigm Continual Active Learning (CAL). We show CAL achieves significant speedups using a plethora of replay schemes that use model distillation and that select diverse, uncertain points from the history. We conduct experiments across many data domains, including natura",
    "link": "http://arxiv.org/abs/2305.06408",
    "context": "Title: Accelerating Batch Active Learning Using Continual Learning Techniques. (arXiv:2305.06408v1 [cs.LG])\nAbstract: A major problem with Active Learning (AL) is high training costs since models are typically retrained from scratch after every query round. We start by demonstrating that standard AL on neural networks with warm starting fails, both to accelerate training and to avoid catastrophic forgetting when using fine-tuning over AL query rounds. We then develop a new class of techniques, circumventing this problem, by biasing further training towards previously labeled sets. We accomplish this by employing existing, and developing novel, replay-based Continual Learning (CL) algorithms that are effective at quickly learning the new without forgetting the old, especially when data comes from an evolving distribution. We call this paradigm Continual Active Learning (CAL). We show CAL achieves significant speedups using a plethora of replay schemes that use model distillation and that select diverse, uncertain points from the history. We conduct experiments across many data domains, including natura",
    "path": "papers/23/05/2305.06408.json",
    "total_tokens": 958,
    "translated_title": "使用永续学习技术加速批次主动学习",
    "translated_abstract": "主动学习（AL）的一个主要问题是训练成本很高，因为模型通常在每个查询轮之后都要从头开始重新训练。本文首先演示了使用预热启动的标准神经网络进行AL的失败，既不能加速训练，又不能避免在AL查询轮上使用微调时发生灾难性遗忘。然后，我们开发了一类新的技术，通过偏向先前标记集来加速训练。我们通过使用现有的和发展新的基于回放的永续学习（CL）算法实现这一点，这些算法在快速学习新知识而不遗忘老知识的情况下，在数据来自不断变化的分布时特别有效。我们将这一范例称为“永续性主动学习”（CAL）。我们展示了CAL通过使用大量的回放方案来实现显著的加速效果，这些方案使用模型蒸馏并从历史中选择多样化的和不确定的点。我们在许多数据领域进行了实验，包括自然",
    "tldr": "本文介绍了一种新的技术，即永续性主动学习（CAL），通过偏向先前标记集来加速训练，通过使用一系列回放方案，包括模型蒸馏和从历史中选择多样化的和不确定的点。实验结果表明，CAL可以大幅提升训练速度。"
}