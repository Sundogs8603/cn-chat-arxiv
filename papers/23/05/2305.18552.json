{
    "title": "Learning Linear Groups in Neural Networks. (arXiv:2305.18552v1 [cs.LG])",
    "abstract": "Employing equivariance in neural networks leads to greater parameter efficiency and improved generalization performance through the encoding of domain knowledge in the architecture; however, the majority of existing approaches require an a priori specification of the desired symmetries. We present a neural network architecture, Linear Group Networks (LGNs), for learning linear groups acting on the weight space of neural networks. Linear groups are desirable due to their inherent interpretability, as they can be represented as finite matrices. LGNs learn groups without any supervision or knowledge of the hidden symmetries in the data and the groups can be mapped to well known operations in machine learning. We use LGNs to learn groups on multiple datasets while considering different downstream tasks; we demonstrate that the linear group structure depends on both the data distribution and the considered task.",
    "link": "http://arxiv.org/abs/2305.18552",
    "context": "Title: Learning Linear Groups in Neural Networks. (arXiv:2305.18552v1 [cs.LG])\nAbstract: Employing equivariance in neural networks leads to greater parameter efficiency and improved generalization performance through the encoding of domain knowledge in the architecture; however, the majority of existing approaches require an a priori specification of the desired symmetries. We present a neural network architecture, Linear Group Networks (LGNs), for learning linear groups acting on the weight space of neural networks. Linear groups are desirable due to their inherent interpretability, as they can be represented as finite matrices. LGNs learn groups without any supervision or knowledge of the hidden symmetries in the data and the groups can be mapped to well known operations in machine learning. We use LGNs to learn groups on multiple datasets while considering different downstream tasks; we demonstrate that the linear group structure depends on both the data distribution and the considered task.",
    "path": "papers/23/05/2305.18552.json",
    "total_tokens": 856,
    "translated_title": "学习神经网络中的线性群",
    "translated_abstract": "在神经网络中利用等变性在架构中编码领域知识，可导致更大的参数效率和改进的泛化性能。然而，大部分现有方法需要事先指定所需的对称性。本文提出了一种名为线性群网络（LGNs）的神经网络架构，用于学习作用于神经网络权重空间上的线性群。线性群具有固有的可解释性，因为它们可以表示为有限矩阵。LGNs学习群体，而不需要任何监督或隐藏对称性的知识，并且这些群体可以映射到机器学习中的众所周知的操作。我们使用LGNs来在多个数据集上学习群体，同时考虑不同的下游任务; 我们证明，线性群结构取决于数据分布和考虑的任务。",
    "tldr": "本研究提出了一种神经网络架构——线性群网络(LGNs)，它可以学习作用于神经网络权重空间上的线性群，而无需预先指定所需的对称性。该结构具有良好的可解释性，并且可以适用于不同数据集及下游任务。",
    "en_tdlr": "This paper proposes a neural network architecture, Linear Group Networks (LGNs), for learning linear groups acting on the weight space of neural networks. The LGNs can learn groups without any supervision or knowledge of the hidden symmetries in the data, and can be applied to different datasets and downstream tasks with good interpretability."
}