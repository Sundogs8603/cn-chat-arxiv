{
    "title": "No-Regret Online Reinforcement Learning with Adversarial Losses and Transitions. (arXiv:2305.17380v1 [cs.LG])",
    "abstract": "Existing online learning algorithms for adversarial Markov Decision Processes achieve ${O}(\\sqrt{T})$ regret after $T$ rounds of interactions even if the loss functions are chosen arbitrarily by an adversary, with the caveat that the transition function has to be fixed. This is because it has been shown that adversarial transition functions make no-regret learning impossible. Despite such impossibility results, in this work, we develop algorithms that can handle both adversarial losses and adversarial transitions, with regret increasing smoothly in the degree of maliciousness of the adversary. More concretely, we first propose an algorithm that enjoys $\\widetilde{{O}}(\\sqrt{T} + C^{\\textsf{P}})$ regret where $C^{\\textsf{P}}$ measures how adversarial the transition functions are and can be at most ${O}(T)$. While this algorithm itself requires knowledge of $C^{\\textsf{P}}$, we further develop a black-box reduction approach that removes this requirement. Moreover, we also show that furth",
    "link": "http://arxiv.org/abs/2305.17380",
    "context": "Title: No-Regret Online Reinforcement Learning with Adversarial Losses and Transitions. (arXiv:2305.17380v1 [cs.LG])\nAbstract: Existing online learning algorithms for adversarial Markov Decision Processes achieve ${O}(\\sqrt{T})$ regret after $T$ rounds of interactions even if the loss functions are chosen arbitrarily by an adversary, with the caveat that the transition function has to be fixed. This is because it has been shown that adversarial transition functions make no-regret learning impossible. Despite such impossibility results, in this work, we develop algorithms that can handle both adversarial losses and adversarial transitions, with regret increasing smoothly in the degree of maliciousness of the adversary. More concretely, we first propose an algorithm that enjoys $\\widetilde{{O}}(\\sqrt{T} + C^{\\textsf{P}})$ regret where $C^{\\textsf{P}}$ measures how adversarial the transition functions are and can be at most ${O}(T)$. While this algorithm itself requires knowledge of $C^{\\textsf{P}}$, we further develop a black-box reduction approach that removes this requirement. Moreover, we also show that furth",
    "path": "papers/23/05/2305.17380.json",
    "total_tokens": 932,
    "translated_title": "具有对抗性损失和转换的无遗憾在线强化学习",
    "translated_abstract": "现有的对抗性马尔可夫决策过程的在线学习算法可以在与对手的$ T $轮交互之后实现${ O}(\\sqrt{T})$的后悔，即使损失函数是由对手任意选择的，但前提是转移函数必须固定。这是因为已经有研究表明，对抗性转移函数使无悔学习变得不可能。尽管存在这种不可能性结果，我们开发了可以处理对抗性损失和对抗性转换的算法，后悔逐渐增加与对手的恶意程度成比例。更具体地说，我们首先提出了一种算法，它的后悔为$\\widetilde{{O}}(\\sqrt{T} + C^{\\textsf{P}})$，其中$C^{\\textsf{P}}$表示转换函数的对抗性，最多可以为${O}(T)$。虽然此算法本身需要$C^{\\textsf{P}}$的知识，但我们还开发了一种黑盒缩减方法来消除此要求。此外，我们还展示了一种进一步的方法，使得算法能够处理任意长度的锚定期。",
    "tldr": "本文提出了一种算法，可以处理对抗性损失和对抗性转换，且后悔逐渐增加与对手的恶意程度成比例。"
}