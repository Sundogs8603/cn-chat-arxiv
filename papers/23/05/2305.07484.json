{
    "title": "Online Learning Under A Separable Stochastic Approximation Framework. (arXiv:2305.07484v1 [cs.LG])",
    "abstract": "We propose an online learning algorithm for a class of machine learning models under a separable stochastic approximation framework. The essence of our idea lies in the observation that certain parameters in the models are easier to optimize than others. In this paper, we focus on models where some parameters have a linear nature, which is common in machine learning. In one routine of the proposed algorithm, the linear parameters are updated by the recursive least squares (RLS) algorithm, which is equivalent to a stochastic Newton method; then, based on the updated linear parameters, the nonlinear parameters are updated by the stochastic gradient method (SGD). The proposed algorithm can be understood as a stochastic approximation version of block coordinate gradient descent approach in which one part of the parameters is updated by a second-order SGD method while the other part is updated by a first-order SGD. Global convergence of the proposed online algorithm for non-convex cases is ",
    "link": "http://arxiv.org/abs/2305.07484",
    "context": "Title: Online Learning Under A Separable Stochastic Approximation Framework. (arXiv:2305.07484v1 [cs.LG])\nAbstract: We propose an online learning algorithm for a class of machine learning models under a separable stochastic approximation framework. The essence of our idea lies in the observation that certain parameters in the models are easier to optimize than others. In this paper, we focus on models where some parameters have a linear nature, which is common in machine learning. In one routine of the proposed algorithm, the linear parameters are updated by the recursive least squares (RLS) algorithm, which is equivalent to a stochastic Newton method; then, based on the updated linear parameters, the nonlinear parameters are updated by the stochastic gradient method (SGD). The proposed algorithm can be understood as a stochastic approximation version of block coordinate gradient descent approach in which one part of the parameters is updated by a second-order SGD method while the other part is updated by a first-order SGD. Global convergence of the proposed online algorithm for non-convex cases is ",
    "path": "papers/23/05/2305.07484.json",
    "total_tokens": 904,
    "translated_title": "分离随机逼近框架下的在线学习算法",
    "translated_abstract": "本文提出了一个基于分离随机逼近框架的在线学习算法，适用于一类机器学习模型。我们的想法的重点在于观察模型中某些参数比其他参数更容易优化。本文重点关注一类线性参数较多的机器学习模型。我们的算法使用递归最小二乘（RLS）算法来更新线性参数，然后基于更新后的线性参数，使用随机梯度下降（SGD）算法来更新非线性参数。这个算法可以理解为块坐标梯度下降方法的随机逼近版本，在这个版本中，其中一部分参数使用二阶随机梯度下降方法更新，而另一部分参数使用一阶随机梯度下降更新。虽然该算法对于非凸问题的全局收敛性没有讨论，但在几个数据集上的实证结果证明了其高效和有效性。",
    "tldr": "本篇论文提出了一种新的在线学习算法，通过分离随机逼近框架，使用递归最小二乘算法和随机梯度下降算法分别更新模型的线性和非线性参数。此算法在多个数据集上表现出高效和有效性。",
    "en_tdlr": "This paper proposes a new online learning algorithm using a separable stochastic approximation framework, which updates linear and nonlinear parameters separately with recursive least squares and stochastic gradient descent algorithms, respectively. The algorithm shows effectiveness and efficiency on several datasets."
}