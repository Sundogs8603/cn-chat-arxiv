{
    "title": "On the Minimax Regret for Online Learning with Feedback Graphs. (arXiv:2305.15383v2 [cs.LG] UPDATED)",
    "abstract": "In this work, we improve on the upper and lower bounds for the regret of online learning with strongly observable undirected feedback graphs. The best known upper bound for this problem is $\\mathcal{O}\\bigl(\\sqrt{\\alpha T\\ln K}\\bigr)$, where $K$ is the number of actions, $\\alpha$ is the independence number of the graph, and $T$ is the time horizon. The $\\sqrt{\\ln K}$ factor is known to be necessary when $\\alpha = 1$ (the experts case). On the other hand, when $\\alpha = K$ (the bandits case), the minimax rate is known to be $\\Theta\\bigl(\\sqrt{KT}\\bigr)$, and a lower bound $\\Omega\\bigl(\\sqrt{\\alpha T}\\bigr)$ is known to hold for any $\\alpha$. Our improved upper bound $\\mathcal{O}\\bigl(\\sqrt{\\alpha T(1+\\ln(K/\\alpha))}\\bigr)$ holds for any $\\alpha$ and matches the lower bounds for bandits and experts, while interpolating intermediate cases. To prove this result, we use FTRL with $q$-Tsallis entropy for a carefully chosen value of $q \\in [1/2, 1)$ that varies with $\\alpha$. The analysis of ",
    "link": "http://arxiv.org/abs/2305.15383",
    "context": "Title: On the Minimax Regret for Online Learning with Feedback Graphs. (arXiv:2305.15383v2 [cs.LG] UPDATED)\nAbstract: In this work, we improve on the upper and lower bounds for the regret of online learning with strongly observable undirected feedback graphs. The best known upper bound for this problem is $\\mathcal{O}\\bigl(\\sqrt{\\alpha T\\ln K}\\bigr)$, where $K$ is the number of actions, $\\alpha$ is the independence number of the graph, and $T$ is the time horizon. The $\\sqrt{\\ln K}$ factor is known to be necessary when $\\alpha = 1$ (the experts case). On the other hand, when $\\alpha = K$ (the bandits case), the minimax rate is known to be $\\Theta\\bigl(\\sqrt{KT}\\bigr)$, and a lower bound $\\Omega\\bigl(\\sqrt{\\alpha T}\\bigr)$ is known to hold for any $\\alpha$. Our improved upper bound $\\mathcal{O}\\bigl(\\sqrt{\\alpha T(1+\\ln(K/\\alpha))}\\bigr)$ holds for any $\\alpha$ and matches the lower bounds for bandits and experts, while interpolating intermediate cases. To prove this result, we use FTRL with $q$-Tsallis entropy for a carefully chosen value of $q \\in [1/2, 1)$ that varies with $\\alpha$. The analysis of ",
    "path": "papers/23/05/2305.15383.json",
    "total_tokens": 1079,
    "translated_title": "在具有反馈图的在线学习中的Minimax遗憾",
    "translated_abstract": "在这项工作中，我们改进了具有强可观察无向反馈图的在线学习遗憾的上下界。该问题的已知最优上界为$\\mathcal {O}\\bigl(\\sqrt{\\alpha T\\ln K}\\bigr)$，其中$K$是行动数量，$\\alpha$是图的独立数，$T$是时间范围。 $\\sqrt{\\ln K}$因子在$\\alpha=1$（专家案例）时被认为是必要的。另一方面，当$\\alpha=K$（强盗案例）时，Minimax率为$\\Theta\\bigl(\\sqrt{KT}\\bigr)$，并且已知对于任何$\\alpha$，下界为$\\Omega\\bigl(\\sqrt{\\alpha T}\\bigr)$。我们的改进上界$\\mathcal {O}\\bigl(\\sqrt{\\alpha T(1+\\ln(K/\\alpha))}\\bigr)$适用于任何$\\alpha$，与强盗和专家的下界相匹配，并插值中间案例。为了证明这个结果，我们使用了具有特定值$q \\in [1/2, 1)$随$\\alpha$变化的FTRL和$q$-Tsallis熵的方法。",
    "tldr": "本论文改进了具有强可观察无向反馈图的在线学习遗憾的上下界，并提出了一个适用于任意$\\alpha$情况下的改进上界，该上界与强盗案例和专家案例下界相匹配，并中间插值，证明过程使用了特定值$q \\in [1/2, 1)$随$\\alpha$变化的FTRL和$q$-Tsallis熵的方法。",
    "en_tdlr": "This paper improves the upper and lower bounds for the regret of online learning with strongly observable undirected feedback graphs. It introduces an improved upper bound that matches the lower bounds for the bandits and experts cases, while interpolating intermediate cases. The analysis of the result utilizes FTRL with q-Tsallis entropy for a carefully chosen value of q that varies with alpha."
}