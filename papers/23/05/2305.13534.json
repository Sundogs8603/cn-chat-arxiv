{
    "title": "How Language Model Hallucinations Can Snowball. (arXiv:2305.13534v1 [cs.CL])",
    "abstract": "A major risk of using language models in practical applications is their tendency to hallucinate incorrect statements. Hallucinations are often attributed to knowledge gaps in LMs, but we hypothesize that in some cases, when justifying previously generated hallucinations, LMs output false claims that they can separately recognize as incorrect. We construct three question-answering datasets where ChatGPT and GPT-4 often state an incorrect answer and offer an explanation with at least one incorrect claim. Crucially, we find that ChatGPT and GPT-4 can identify 67% and 87% of their own mistakes, respectively. We refer to this phenomenon as hallucination snowballing: an LM over-commits to early mistakes, leading to more mistakes that it otherwise would not make.",
    "link": "http://arxiv.org/abs/2305.13534",
    "context": "Title: How Language Model Hallucinations Can Snowball. (arXiv:2305.13534v1 [cs.CL])\nAbstract: A major risk of using language models in practical applications is their tendency to hallucinate incorrect statements. Hallucinations are often attributed to knowledge gaps in LMs, but we hypothesize that in some cases, when justifying previously generated hallucinations, LMs output false claims that they can separately recognize as incorrect. We construct three question-answering datasets where ChatGPT and GPT-4 often state an incorrect answer and offer an explanation with at least one incorrect claim. Crucially, we find that ChatGPT and GPT-4 can identify 67% and 87% of their own mistakes, respectively. We refer to this phenomenon as hallucination snowballing: an LM over-commits to early mistakes, leading to more mistakes that it otherwise would not make.",
    "path": "papers/23/05/2305.13534.json",
    "total_tokens": 805,
    "translated_title": "语言模型的幻觉如何会越来越严重",
    "translated_abstract": "在实际应用中使用语言模型的一个主要风险是它们倾向于产生错误的语句。这些幻觉通常归因于语言模型中的知识缺口，但我们假设在某些情况下，当证明之前产生的幻觉时，语言模型会输出错误的声明，它们可以单独地识别为不正确的。我们构建了三个问答数据集，其中ChatGPT和GPT-4经常陈述错误的答案，并提供至少一个不正确的声明的解释。重要的是，我们发现ChatGPT和GPT-4可以分别识别其自己错误的67％和87％。我们将这种现象称为幻觉滚雪球：语言模型过度致力于早期的错误，导致更多的错误，否则它不会犯这些错误。",
    "tldr": "语言模型在生产中容易产生幻觉错误，这些幻觉会导致模型产生更多的错误，并且模型可以自行识别其中的一些错误。",
    "en_tdlr": "Language models exhibit hallucinations in practical applications leading to incorrect statements, which could be due to knowledge gaps, but also due to models overcommitting to early mistakes, leading to further mistakes. Models are capable of identifying some of these mistakes. This phenomenon is referred to as hallucination snowballing."
}