{
    "title": "Assumption Generation for the Verification of Learning-Enabled Autonomous Systems. (arXiv:2305.18372v1 [cs.AI])",
    "abstract": "Providing safety guarantees for autonomous systems is difficult as these systems operate in complex environments that require the use of learning-enabled components, such as deep neural networks (DNNs) for visual perception. DNNs are hard to analyze due to their size (they can have thousands or millions of parameters), lack of formal specifications (DNNs are typically learnt from labeled data, in the absence of any formal requirements), and sensitivity to small changes in the environment. We present an assume-guarantee style compositional approach for the formal verification of system-level safety properties of such autonomous systems. Our insight is that we can analyze the system in the absence of the DNN perception components by automatically synthesizing assumptions on the DNN behaviour that guarantee the satisfaction of the required safety properties. The synthesized assumptions are the weakest in the sense that they characterize the output sequences of all the possible DNNs that, ",
    "link": "http://arxiv.org/abs/2305.18372",
    "context": "Title: Assumption Generation for the Verification of Learning-Enabled Autonomous Systems. (arXiv:2305.18372v1 [cs.AI])\nAbstract: Providing safety guarantees for autonomous systems is difficult as these systems operate in complex environments that require the use of learning-enabled components, such as deep neural networks (DNNs) for visual perception. DNNs are hard to analyze due to their size (they can have thousands or millions of parameters), lack of formal specifications (DNNs are typically learnt from labeled data, in the absence of any formal requirements), and sensitivity to small changes in the environment. We present an assume-guarantee style compositional approach for the formal verification of system-level safety properties of such autonomous systems. Our insight is that we can analyze the system in the absence of the DNN perception components by automatically synthesizing assumptions on the DNN behaviour that guarantee the satisfaction of the required safety properties. The synthesized assumptions are the weakest in the sense that they characterize the output sequences of all the possible DNNs that, ",
    "path": "papers/23/05/2305.18372.json",
    "total_tokens": 850,
    "translated_title": "学习增强自主系统验证中的假设生成",
    "translated_abstract": "自主系统的安全保证具有挑战性，因为这些系统在需要使用深度神经网络（DNN）等学习增强组件的复杂环境中运行，用于视觉感知。 DNN由于规模大（可能具有成千上万个参数）、缺乏正式规范（DNN通常是从有标签的数据中学习，缺乏任何正式要求）以及对环境中微小变化的敏感性而难以分析。 我们提出了一种假设保证式组合方法，用于验证这种自主系统的系统级安全属性。 我们的洞察力在于，我们可以通过自动合成有保证的DNN行为的假设，在没有DNN感知组件的情况下分析系统，以保证满足所需的安全属性。 合成的假设是最弱的，因为它们表征了所有可能的DNN的输出序列。",
    "tldr": "本文提出了一种为安全保证提供假设的做法，以用于验证具有复杂环境和学习增强组件的自主系统，通过自动生成适当的DNN行为假设，来满足要求的安全属性。",
    "en_tdlr": "This paper proposes a method to provide assumptions for safety guarantees in the verification of autonomous systems with complex environments and learning-enabled components. By automatically generating appropriate assumptions on the behavior of DNNs, system-level safety properties can be verified."
}