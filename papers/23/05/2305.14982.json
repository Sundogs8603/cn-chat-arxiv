{
    "title": "LAraBench: Benchmarking Arabic AI with Large Language Models",
    "abstract": "Recent advancements in Large Language Models (LLMs) have significantly influenced the landscape of language and speech research. Despite this progress, these models lack specific benchmarking against state-of-the-art (SOTA) models tailored to particular languages and tasks. LAraBench addresses this gap for Arabic Natural Language Processing (NLP) and Speech Processing tasks, including sequence tagging and content classification across different domains. We utilized models such as GPT-3.5-turbo, GPT-4, BLOOMZ, Jais-13b-chat, Whisper, and USM, employing zero and few-shot learning techniques to tackle 33 distinct tasks across 61 publicly available datasets. This involved 98 experimental setups, encompassing ~296K data points, ~46 hours of speech, and 30 sentences for Text-to-Speech (TTS). This effort resulted in 330+ sets of experiments. Our analysis focused on measuring the performance gap between SOTA models and LLMs. The overarching trend observed was that SOTA models generally outperf",
    "link": "https://arxiv.org/abs/2305.14982",
    "context": "Title: LAraBench: Benchmarking Arabic AI with Large Language Models\nAbstract: Recent advancements in Large Language Models (LLMs) have significantly influenced the landscape of language and speech research. Despite this progress, these models lack specific benchmarking against state-of-the-art (SOTA) models tailored to particular languages and tasks. LAraBench addresses this gap for Arabic Natural Language Processing (NLP) and Speech Processing tasks, including sequence tagging and content classification across different domains. We utilized models such as GPT-3.5-turbo, GPT-4, BLOOMZ, Jais-13b-chat, Whisper, and USM, employing zero and few-shot learning techniques to tackle 33 distinct tasks across 61 publicly available datasets. This involved 98 experimental setups, encompassing ~296K data points, ~46 hours of speech, and 30 sentences for Text-to-Speech (TTS). This effort resulted in 330+ sets of experiments. Our analysis focused on measuring the performance gap between SOTA models and LLMs. The overarching trend observed was that SOTA models generally outperf",
    "path": "papers/23/05/2305.14982.json",
    "total_tokens": 925,
    "translated_title": "LAraBench：基于大语言模型进行阿拉伯语AI的基准测试",
    "translated_abstract": "近期大语言模型（LLMs）的进展显著影响了语言和语音研究领域。尽管取得了进步，但这些模型尚缺乏特定语言和任务的最新模型进行对比的基准测试。LAraBench针对阿拉伯自然语言处理（NLP）和语音处理任务提供了这方面的解决方案，包括序列标注和跨不同领域的内容分类。我们采用了GPT-3.5-turbo、GPT-4、BLOOMZ、Jais-13b-chat、Whisper和USM等模型，运用零样本学习和少样本学习技术，应对了33个独立任务和61个公开可用的数据集。这涉及98个实验设置，包括约296K个数据点、约46小时的语音和30个用于文本到语音（TTS）的句子。这一努力产生了330+组实验。我们的分析重点是衡量最新模型和LLMs之间的性能差距。总体趋势表明，最新模型一般表现更优。",
    "tldr": "LAraBench是一个针对阿拉伯语自然语言处理和语音处理任务的基准测试平台，通过多种实验设置和性能衡量指标，证明最新模型通常表现优于大语言模型（LLMs）。",
    "en_tdlr": "LAraBench is a benchmarking platform for Arabic Natural Language Processing and Speech Processing tasks, which demonstrates that state-of-the-art models generally outperform Large Language Models (LLMs) based on various experimental setups and performance metrics."
}