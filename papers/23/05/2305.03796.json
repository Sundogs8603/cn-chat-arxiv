{
    "title": "Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation. (arXiv:2305.03796v1 [cs.CL])",
    "abstract": "Unlike recurrent models, conventional wisdom has it that Transformers cannot perfectly model regular languages. Inspired by the notion of working memory, we propose a new Transformer variant named RegularGPT. With its novel combination of Weight-Sharing, Adaptive-Depth, and Sliding-Dilated-Attention, RegularGPT constructs working memory along the depth dimension, thereby enabling efficient and successful modeling of regular languages such as PARITY. We further test RegularGPT on the task of natural language length extrapolation and surprisingly find that it rediscovers the local windowed attention effect deemed necessary in prior work for length extrapolation.",
    "link": "http://arxiv.org/abs/2305.03796",
    "context": "Title: Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation. (arXiv:2305.03796v1 [cs.CL])\nAbstract: Unlike recurrent models, conventional wisdom has it that Transformers cannot perfectly model regular languages. Inspired by the notion of working memory, we propose a new Transformer variant named RegularGPT. With its novel combination of Weight-Sharing, Adaptive-Depth, and Sliding-Dilated-Attention, RegularGPT constructs working memory along the depth dimension, thereby enabling efficient and successful modeling of regular languages such as PARITY. We further test RegularGPT on the task of natural language length extrapolation and surprisingly find that it rediscovers the local windowed attention effect deemed necessary in prior work for length extrapolation.",
    "path": "papers/23/05/2305.03796.json",
    "total_tokens": 736,
    "translated_title": "Transformer工作记忆使得正则语言推理和自然语言长度外推成为可能",
    "translated_abstract": "不同于循环模型，普遍认为Transformer不能完美地建模正则语言。受到工作记忆的启发，我们提出了一种名为RegularGPT的新型Transformer变种。通过Weigh-Sharing、Adaptive-Depth和Sliding-Dilated-Attention的新颖组合，RegularGPT沿深度维度构建工作记忆，从而能够有效地成功地建模正则语言，如PARITY。我们进一步在自然语言长度外推任务上测试了RegularGPT，令人惊讶的是，它重新发现了先前工作中被认为对于长度外推至关重要的局部窗口注意力效应。",
    "tldr": "本文提出了一个名为RegularGPT的新型Transformer变体，其通过Weigh-Sharing、Adaptive-Depth和Sliding-Dilated-Attention的新颖组合，沿深度维度构建工作记忆，从而成功地建模了正则语言，如PARITY，并在自然语言长度外推任务中表现出良好效果。",
    "en_tdlr": "The paper proposes a new variant of Transformer model called RegularGPT that can successfully model regular languages by constructing working memory along the depth dimension. It also discovers the local windowed attention effect necessary for natural language length extrapolation."
}