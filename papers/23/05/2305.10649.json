{
    "title": "ZeroPrompt: Streaming Acoustic Encoders are Zero-Shot Masked LMs. (arXiv:2305.10649v1 [cs.SD])",
    "abstract": "In this paper, we present ZeroPrompt (Figure 1-(a)) and the corresponding Prompt-and-Refine strategy (Figure 3), two simple but effective \\textbf{training-free} methods to decrease the Token Display Time (TDT) of streaming ASR models \\textbf{without any accuracy loss}. The core idea of ZeroPrompt is to append zeroed content to each chunk during inference, which acts like a prompt to encourage the model to predict future tokens even before they were spoken. We argue that streaming acoustic encoders naturally have the modeling ability of Masked Language Models and our experiments demonstrate that ZeroPrompt is engineering cheap and can be applied to streaming acoustic encoders on any dataset without any accuracy loss. Specifically, compared with our baseline models, we achieve 350 $\\sim$ 700ms reduction on First Token Display Time (TDT-F) and 100 $\\sim$ 400ms reduction on Last Token Display Time (TDT-L), with theoretically and experimentally equal WER on both Aishell-1 and Librispeech da",
    "link": "http://arxiv.org/abs/2305.10649",
    "context": "Title: ZeroPrompt: Streaming Acoustic Encoders are Zero-Shot Masked LMs. (arXiv:2305.10649v1 [cs.SD])\nAbstract: In this paper, we present ZeroPrompt (Figure 1-(a)) and the corresponding Prompt-and-Refine strategy (Figure 3), two simple but effective \\textbf{training-free} methods to decrease the Token Display Time (TDT) of streaming ASR models \\textbf{without any accuracy loss}. The core idea of ZeroPrompt is to append zeroed content to each chunk during inference, which acts like a prompt to encourage the model to predict future tokens even before they were spoken. We argue that streaming acoustic encoders naturally have the modeling ability of Masked Language Models and our experiments demonstrate that ZeroPrompt is engineering cheap and can be applied to streaming acoustic encoders on any dataset without any accuracy loss. Specifically, compared with our baseline models, we achieve 350 $\\sim$ 700ms reduction on First Token Display Time (TDT-F) and 100 $\\sim$ 400ms reduction on Last Token Display Time (TDT-L), with theoretically and experimentally equal WER on both Aishell-1 and Librispeech da",
    "path": "papers/23/05/2305.10649.json",
    "total_tokens": 980,
    "translated_title": "ZeroPrompt: 流式语音编码器是零-shot遮蔽语言模型",
    "translated_abstract": "本文提出了ZeroPrompt和对应的Prompt-and-Refine策略，这是两种简单而有效的无需训练的方法，能够降低流式语音识别模型的Token Display Time（TDT），而且不会造成任何精度损失。ZeroPrompt的核心思想是在推理过程中向每个语音块附加零内容，这类似于提示，鼓励模型在未来标记之前预测它们。我们认为，流式语音编码器自然具有Masked Language Model的建模能力，我们的实验表明，ZeroPrompt具有廉价的工程成本，可以应用于任何数据集上的流式语音编码器，而不会损失精度。具体来说，与我们的基准模型相比，在Aishell-1和Librispeech数据集上，我们实现了首个标记显示时间（TDT-F）的350~700ms的降低以及最后一个标记显示时间（TDT-L）的100~400ms的降低，理论上和实验上的WER精度相等。",
    "tldr": "本文提出了ZeroPrompt和对应的Prompt-and-Refine策略，可以降低流式语音识别模型的TDT，而且不会损失精度，具有工程便捷性，能够在任何数据集上应用。",
    "en_tdlr": "This paper presents ZeroPrompt and the corresponding Prompt-and-Refine strategy, which can reduce the Token Display Time of streaming ASR models without any accuracy loss and can be applied to any dataset with engineering convenience. The core idea is to append zeroed content to each chunk during inference, which acts like a prompt to encourage the model to predict future tokens even before they were spoken. The experiments demonstrate that streaming acoustic encoders naturally have the modeling ability of Masked Language Models."
}