{
    "title": "Automated Metrics for Medical Multi-Document Summarization Disagree with Human Evaluations. (arXiv:2305.13693v1 [cs.CL])",
    "abstract": "Evaluating multi-document summarization (MDS) quality is difficult. This is especially true in the case of MDS for biomedical literature reviews, where models must synthesize contradicting evidence reported across different documents. Prior work has shown that rather than performing the task, models may exploit shortcuts that are difficult to detect using standard n-gram similarity metrics such as ROUGE. Better automated evaluation metrics are needed, but few resources exist to assess metrics when they are proposed. Therefore, we introduce a dataset of human-assessed summary quality facets and pairwise preferences to encourage and support the development of better automated evaluation methods for literature review MDS. We take advantage of community submissions to the Multi-document Summarization for Literature Review (MSLR) shared task to compile a diverse and representative sample of generated summaries. We analyze how automated summarization evaluation metrics correlate with lexical",
    "link": "http://arxiv.org/abs/2305.13693",
    "context": "Title: Automated Metrics for Medical Multi-Document Summarization Disagree with Human Evaluations. (arXiv:2305.13693v1 [cs.CL])\nAbstract: Evaluating multi-document summarization (MDS) quality is difficult. This is especially true in the case of MDS for biomedical literature reviews, where models must synthesize contradicting evidence reported across different documents. Prior work has shown that rather than performing the task, models may exploit shortcuts that are difficult to detect using standard n-gram similarity metrics such as ROUGE. Better automated evaluation metrics are needed, but few resources exist to assess metrics when they are proposed. Therefore, we introduce a dataset of human-assessed summary quality facets and pairwise preferences to encourage and support the development of better automated evaluation methods for literature review MDS. We take advantage of community submissions to the Multi-document Summarization for Literature Review (MSLR) shared task to compile a diverse and representative sample of generated summaries. We analyze how automated summarization evaluation metrics correlate with lexical",
    "path": "papers/23/05/2305.13693.json",
    "total_tokens": 827,
    "translated_title": "医学多文献摘要的自动评估指标与人工评估不一致",
    "translated_abstract": "评估多文献摘要（MDS）质量是困难的，特别是在生物医学文献综述的情况下。先前的工作表明，模型可能会利用难以用标准n-gram相似度指标（如ROUGE）检测的快捷方式，而不是执行任务。需要更好的自动化评估指标，但提出评估指标时很少有资源可以评估。因此，我们介绍了一个人工评估的摘要质量方面和成对偏好的数据集，以鼓励和支持更好的文献综述MDS自动化评估方法的发展。我们利用社区提交的多文档综述（MSLR）共享任务，编编译了多样化且代表性的摘要样本，并分析了自动摘要评估指标与词汇相的关系。",
    "tldr": "在医学文献综述的多文献摘要中，现有的自动评估指标与人工评估存在不一致性，需要更好的评估指标和数据集支持。",
    "en_tdlr": "Existing automated metrics for evaluating multi-document summarization (MDS) in biomedical literature reviews are inconsistent with human evaluations, indicating the need for better metrics and datasets to support the development of improved evaluation methods."
}