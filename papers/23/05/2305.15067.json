{
    "title": "Not All Metrics Are Guilty: Improving NLG Evaluation by Diversifying References",
    "abstract": "arXiv:2305.15067v2 Announce Type: replace  Abstract: Most research about natural language generation (NLG) relies on evaluation benchmarks with limited references for a sample, which may result in poor correlations with human judgements. The underlying reason is that one semantic meaning can actually be expressed in different forms, and the evaluation with a single or few references may not accurately reflect the quality of the model's hypotheses. To address this issue, this paper presents a simple and effective method, named Div-Ref, to enhance existing evaluation benchmarks by enriching the number of references. We leverage large language models (LLMs) to diversify the expression of a single reference into multiple high-quality ones to cover the semantic space of the reference sentence as much as possible. We conduct comprehensive experiments to empirically demonstrate that diversifying the expression of reference can significantly enhance the correlation between automatic evaluation",
    "link": "https://arxiv.org/abs/2305.15067",
    "context": "Title: Not All Metrics Are Guilty: Improving NLG Evaluation by Diversifying References\nAbstract: arXiv:2305.15067v2 Announce Type: replace  Abstract: Most research about natural language generation (NLG) relies on evaluation benchmarks with limited references for a sample, which may result in poor correlations with human judgements. The underlying reason is that one semantic meaning can actually be expressed in different forms, and the evaluation with a single or few references may not accurately reflect the quality of the model's hypotheses. To address this issue, this paper presents a simple and effective method, named Div-Ref, to enhance existing evaluation benchmarks by enriching the number of references. We leverage large language models (LLMs) to diversify the expression of a single reference into multiple high-quality ones to cover the semantic space of the reference sentence as much as possible. We conduct comprehensive experiments to empirically demonstrate that diversifying the expression of reference can significantly enhance the correlation between automatic evaluation",
    "path": "papers/23/05/2305.15067.json",
    "total_tokens": 744,
    "translated_title": "并非所有评估指标都应受到指责：通过多样化参考文献改进自然语言生成评估",
    "translated_abstract": "大多数关于自然语言生成（NLG）的研究依赖于具有有限参考文献的评估基准，这可能导致与人类判断的相关性较差。本文提出了一种简单有效的方法，称为Div-Ref，通过丰富参考文献的数量来增强现有的评估基准。我们利用大型语言模型（LLMs）将单个参考文献的表达多样化为多个高质量的表达，以尽可能覆盖参考句的语义空间。我们进行了全面的实验，从经验上证明多样化参考文献的表达可以显著增强自动评估之间的相关性。",
    "tldr": "通过增加参考文献的多样性，本文提出的方法Div-Ref 显著提高了自然语言生成评估的相关性"
}