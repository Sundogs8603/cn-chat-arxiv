{
    "title": "Representation Equivalent Neural Operators: a Framework for Alias-free Operator Learning. (arXiv:2305.19913v2 [cs.LG] UPDATED)",
    "abstract": "Recently, operator learning, or learning mappings between infinite-dimensional function spaces, has garnered significant attention, notably in relation to learning partial differential equations from data. Conceptually clear when outlined on paper, neural operators necessitate discretization in the transition to computer implementations. This step can compromise their integrity, often causing them to deviate from the underlying operators. This research offers a fresh take on neural operators with a framework Representation equivalent Neural Operators (ReNO) designed to address these issues. At its core is the concept of operator aliasing, which measures inconsistency between neural operators and their discrete representations. We explore this for widely-used operator learning techniques. Our findings detail how aliasing introduces errors when handling different discretizations and grids and loss of crucial continuous structures. More generally, this framework not only sheds light on ex",
    "link": "http://arxiv.org/abs/2305.19913",
    "context": "Title: Representation Equivalent Neural Operators: a Framework for Alias-free Operator Learning. (arXiv:2305.19913v2 [cs.LG] UPDATED)\nAbstract: Recently, operator learning, or learning mappings between infinite-dimensional function spaces, has garnered significant attention, notably in relation to learning partial differential equations from data. Conceptually clear when outlined on paper, neural operators necessitate discretization in the transition to computer implementations. This step can compromise their integrity, often causing them to deviate from the underlying operators. This research offers a fresh take on neural operators with a framework Representation equivalent Neural Operators (ReNO) designed to address these issues. At its core is the concept of operator aliasing, which measures inconsistency between neural operators and their discrete representations. We explore this for widely-used operator learning techniques. Our findings detail how aliasing introduces errors when handling different discretizations and grids and loss of crucial continuous structures. More generally, this framework not only sheds light on ex",
    "path": "papers/23/05/2305.19913.json",
    "total_tokens": 913,
    "translated_title": "Representation Equivalent Neural Operators: 一种无别名的操作符学习框架",
    "translated_abstract": "最近，操作符学习，或者学习无限维函数空间之间的映射，引起了相当大的关注，特别是与从数据中学习偏微分方程相关的领域。在纸上概念上很清晰的神经操作符在转换成计算机实现时需要离散化。这一步骤可能会损害它们的完整性，导致它们与底层操作符偏离。这项研究提供了一个对神经操作符的新见解，采用了一种名为表示等效神经操作符（ReNO）的框架来解决这些问题。其核心是操作符别名的概念，用于衡量神经操作符与其离散表示之间的不一致性。我们探索了这一问题在常用的操作符学习技术中的应用。我们的研究结果详细说明了当处理不同的离散化和网格以及关键的连续结构时，别名会引入误差。更一般的说，这一框架不仅能够揭示离散化引入的问题，还能够为神经操作符的学习提供新的视角。",
    "tldr": "这项研究提出了一种名为ReNO的框架，解决了神经操作符在离散实现时出现的完整性损失和误差问题，为神经操作符的学习提供了新的视角。",
    "en_tdlr": "This research introduces a framework called ReNO, which addresses the issues of integrity loss and errors that arise when implementing neural operators in a discrete manner, providing a new perspective on learning neural operators."
}