{
    "title": "NeuroComparatives: Neuro-Symbolic Distillation of Comparative Knowledge. (arXiv:2305.04978v1 [cs.CL])",
    "abstract": "Comparative knowledge (e.g., steel is stronger and heavier than styrofoam) is an essential component of our world knowledge, yet understudied in prior literature. In this paper, we study the task of comparative knowledge acquisition, motivated by the dramatic improvements in the capabilities of extreme-scale language models like GPT-3, which have fueled efforts towards harvesting their knowledge into knowledge bases. However, access to inference API for such models is limited, thereby restricting the scope and the diversity of the knowledge acquisition. We thus ask a seemingly implausible question: whether more accessible, yet considerably smaller and weaker models such as GPT-2, can be utilized to acquire comparative knowledge, such that the resulting quality is on par with their large-scale counterparts?  We introduce NeuroComparatives, a novel framework for comparative knowledge distillation using lexically-constrained decoding, followed by stringent filtering of generated knowledge",
    "link": "http://arxiv.org/abs/2305.04978",
    "context": "Title: NeuroComparatives: Neuro-Symbolic Distillation of Comparative Knowledge. (arXiv:2305.04978v1 [cs.CL])\nAbstract: Comparative knowledge (e.g., steel is stronger and heavier than styrofoam) is an essential component of our world knowledge, yet understudied in prior literature. In this paper, we study the task of comparative knowledge acquisition, motivated by the dramatic improvements in the capabilities of extreme-scale language models like GPT-3, which have fueled efforts towards harvesting their knowledge into knowledge bases. However, access to inference API for such models is limited, thereby restricting the scope and the diversity of the knowledge acquisition. We thus ask a seemingly implausible question: whether more accessible, yet considerably smaller and weaker models such as GPT-2, can be utilized to acquire comparative knowledge, such that the resulting quality is on par with their large-scale counterparts?  We introduce NeuroComparatives, a novel framework for comparative knowledge distillation using lexically-constrained decoding, followed by stringent filtering of generated knowledge",
    "path": "papers/23/05/2305.04978.json",
    "total_tokens": 851,
    "translated_title": "NeuroComparatives：比较知识的神经符号提炼",
    "translated_abstract": "比较知识是我们世界知识的重要组成部分，但在以前的文献中研究不足。本文研究比较知识获取任务，受到像GPT-3这样极端规模语言模型能力的显着提高的推动，推动了将他们的知识收集到知识库中的努力。但是，这些模型的推理API访问受到限制，从而限制了知识获取的范围和多样性。因此，我们提出了一个看似不可行的问题：更易于访问、规模更小、性能更弱的模型（如GPT-2）是否可以用于获取比较知识，从而达到与大规模模型相当的质量？我们引入了NeuroComparatives，一种使用词汇约束解码的比较知识提炼新框架，其后紧密过滤生成的知识。",
    "tldr": "本文提出了一种新的用于比较知识提炼的神经符号框架，名称为NeuroComparatives。该框架利用词汇约束解码进行比较知识提炼，以及对生成的知识进行严格过滤。",
    "en_tdlr": "The paper proposes a novel framework named NeuroComparatives for distilling comparative knowledge using lexically-constrained decoding and stringent filtering of generated knowledge."
}