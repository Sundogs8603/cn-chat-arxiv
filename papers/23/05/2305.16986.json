{
    "title": "NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models. (arXiv:2305.16986v2 [cs.CV] UPDATED)",
    "abstract": "Trained with an unprecedented scale of data, large language models (LLMs) like ChatGPT and GPT-4 exhibit the emergence of significant reasoning abilities from model scaling. Such a trend underscored the potential of training LLMs with unlimited language data, advancing the development of a universal embodied agent. In this work, we introduce the NavGPT, a purely LLM-based instruction-following navigation agent, to reveal the reasoning capability of GPT models in complex embodied scenes by performing zero-shot sequential action prediction for vision-and-language navigation (VLN). At each step, NavGPT takes the textual descriptions of visual observations, navigation history, and future explorable directions as inputs to reason the agent's current status, and makes the decision to approach the target. Through comprehensive experiments, we demonstrate NavGPT can explicitly perform high-level planning for navigation, including decomposing instruction into sub-goal, integrating commonsense k",
    "link": "http://arxiv.org/abs/2305.16986",
    "context": "Title: NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models. (arXiv:2305.16986v2 [cs.CV] UPDATED)\nAbstract: Trained with an unprecedented scale of data, large language models (LLMs) like ChatGPT and GPT-4 exhibit the emergence of significant reasoning abilities from model scaling. Such a trend underscored the potential of training LLMs with unlimited language data, advancing the development of a universal embodied agent. In this work, we introduce the NavGPT, a purely LLM-based instruction-following navigation agent, to reveal the reasoning capability of GPT models in complex embodied scenes by performing zero-shot sequential action prediction for vision-and-language navigation (VLN). At each step, NavGPT takes the textual descriptions of visual observations, navigation history, and future explorable directions as inputs to reason the agent's current status, and makes the decision to approach the target. Through comprehensive experiments, we demonstrate NavGPT can explicitly perform high-level planning for navigation, including decomposing instruction into sub-goal, integrating commonsense k",
    "path": "papers/23/05/2305.16986.json",
    "total_tokens": 1149,
    "translated_title": "NavGPT: 带有大型语言模型的视觉语言导航中的显式推理",
    "translated_abstract": "大型语言模型（LLM）例如ChatGPT和GPT-4以前所未有的规模进行训练，从模型的扩展中展现出显著的推理能力。这种趋势强调了使用无限语言数据训练LLM的潜力，推动了通用体现智能体的发展。本文介绍了NavGPT，这是一个纯粹基于LLM的指令跟随导航智能体，通过为视觉语言导航（VLN）执行零-shot的连续动作预测，揭示了对于在复杂的现实场景下GPT模型的推理能力。在每一步中，NavGPT将视觉观察、导航历史和未来可探索方向的文本描述作为输入，推理出智能体的当前状态，并决定如何接近目标。通过全面的实验，我们证明了NavGPT可以明确地执行导航的高级规划，包括将指令分解成子目标、整合常识知识以进行障碍物避免，并参考先前的步骤进行澄清。我们的结果表明，LLM可能成为复杂顺序决策任务中的传统流程的强有力替代品，展示了通用体现智能体发展的美好前景。",
    "tldr": "NavGPT是基于LLM的导航智能体，可以在视觉语言导航（VLN）中，通过对文本描述进行推理，执行零-shot连续动作预测。该模型具有高级规划能力，可以将指令分解成子目标、整合常识知识以进行障碍物避免，并参考先前的步骤进行澄清。NavGPT展示了通用体现智能体发展的美好前景。"
}