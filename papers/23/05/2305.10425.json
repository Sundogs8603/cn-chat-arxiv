{
    "title": "SLiC-HF: Sequence Likelihood Calibration with Human Feedback. (arXiv:2305.10425v1 [cs.CL])",
    "abstract": "Learning from human feedback has been shown to be effective at aligning language models with human preferences. Past work has often relied on Reinforcement Learning from Human Feedback (RLHF), which optimizes the language model using reward scores assigned from a reward model trained on human preference data. In this work we show how the recently introduced Sequence Likelihood Calibration (SLiC), can also be used to effectively learn from human preferences (SLiC-HF). Furthermore, we demonstrate this can be done with human feedback data collected for a different model, similar to off-policy, offline RL data. Automatic and human evaluation experiments on the TL;DR summarization task show that SLiC-HF significantly improves supervised fine-tuning baselines. Furthermore, SLiC-HF presents a competitive alternative to the PPO RLHF implementation used in past work while being much simpler to implement, easier to tune and more computationally efficient in practice.",
    "link": "http://arxiv.org/abs/2305.10425",
    "context": "Title: SLiC-HF: Sequence Likelihood Calibration with Human Feedback. (arXiv:2305.10425v1 [cs.CL])\nAbstract: Learning from human feedback has been shown to be effective at aligning language models with human preferences. Past work has often relied on Reinforcement Learning from Human Feedback (RLHF), which optimizes the language model using reward scores assigned from a reward model trained on human preference data. In this work we show how the recently introduced Sequence Likelihood Calibration (SLiC), can also be used to effectively learn from human preferences (SLiC-HF). Furthermore, we demonstrate this can be done with human feedback data collected for a different model, similar to off-policy, offline RL data. Automatic and human evaluation experiments on the TL;DR summarization task show that SLiC-HF significantly improves supervised fine-tuning baselines. Furthermore, SLiC-HF presents a competitive alternative to the PPO RLHF implementation used in past work while being much simpler to implement, easier to tune and more computationally efficient in practice.",
    "path": "papers/23/05/2305.10425.json",
    "total_tokens": 903,
    "translated_title": "SLiC-HF：人类反馈的序列似然校准",
    "translated_abstract": "已经证明，从人类反馈中学习可以有效地将语言模型与人类偏好对齐。过去的工作通常依赖于从人类偏好数据训练的奖励模型分配的奖励分数，利用人类反馈进行强化学习（RLHF）来优化语言模型。在本文中，我们展示了最近引入的序列似然校准（SLiC）如何有效地应用于从人类偏好中学习（SLiC-HF）。此外，我们证明这可以使用为不同模型收集的人类反馈数据来完成，类似于离线RL数据的离线学习。自动化和人类评估实验表明，SLiC-HF显著改进了监督微调基线。此外，SLiC-HF是过去工作中使用的PPO RLHF实现的竞争性替代，而且在实践中更简单、更易于调整，并具有更高的计算效率。",
    "tldr": "本文提出了一种新方法，SLiC-HF，可以利用序列似然校准从人类偏好中学习，相较于过去的方法更加简单高效，并在TL;DR自动摘要任务中显著提高了监督微调基线。",
    "en_tdlr": "This paper proposes a new method, SLiC-HF, that can effectively learn from human preferences using sequence likelihood calibration. It is simpler, easier to tune and more computationally efficient than previous methods, and significantly improves supervised fine-tuning baselines in the TL;DR summarization task."
}