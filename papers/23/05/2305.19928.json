{
    "title": "A Global Context Mechanism for Sequence Labeling. (arXiv:2305.19928v2 [cs.CL] UPDATED)",
    "abstract": "Sequential labeling tasks necessitate the computation of sentence representations for each word within a given sentence. With the advent of advanced pretrained language models; one common approach involves incorporating a BiLSTM layer to bolster the sequence structure information at the output level. Nevertheless, it has been empirically demonstrated (P.-H. Li et al., 2020) that the potential of BiLSTM for generating sentence representations for sequence labeling tasks is constrained, primarily due to the amalgamation of fragments form past and future sentence representations to form a complete sentence representation. In this study, we discovered that strategically integrating the whole sentence representation, which existing in the first cell and last cell of BiLSTM, into sentence representation of ecah cell, could markedly enhance the F1 score and accuracy. Using BERT embedded within BiLSTM as illustration, we conducted exhaustive experiments on nine datasets for sequence labeling t",
    "link": "http://arxiv.org/abs/2305.19928",
    "context": "Title: A Global Context Mechanism for Sequence Labeling. (arXiv:2305.19928v2 [cs.CL] UPDATED)\nAbstract: Sequential labeling tasks necessitate the computation of sentence representations for each word within a given sentence. With the advent of advanced pretrained language models; one common approach involves incorporating a BiLSTM layer to bolster the sequence structure information at the output level. Nevertheless, it has been empirically demonstrated (P.-H. Li et al., 2020) that the potential of BiLSTM for generating sentence representations for sequence labeling tasks is constrained, primarily due to the amalgamation of fragments form past and future sentence representations to form a complete sentence representation. In this study, we discovered that strategically integrating the whole sentence representation, which existing in the first cell and last cell of BiLSTM, into sentence representation of ecah cell, could markedly enhance the F1 score and accuracy. Using BERT embedded within BiLSTM as illustration, we conducted exhaustive experiments on nine datasets for sequence labeling t",
    "path": "papers/23/05/2305.19928.json",
    "total_tokens": 808,
    "tldr": "本研究发现将存在于BiLSTM的第一个单元和最后一个单元中的整个句子表示整合到每个单元的句子表示中可以显著提高序列标注的性能。"
}