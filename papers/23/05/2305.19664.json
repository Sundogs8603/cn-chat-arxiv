{
    "title": "Unveiling Cross Modality Bias in Visual Question Answering: A Causal View with Possible Worlds VQA. (arXiv:2305.19664v1 [cs.CV])",
    "abstract": "To increase the generalization capability of VQA systems, many recent studies have tried to de-bias spurious language or vision associations that shortcut the question or image to the answer. Despite these efforts, the literature fails to address the confounding effect of vision and language simultaneously. As a result, when they reduce bias learned from one modality, they usually increase bias from the other. In this paper, we first model a confounding effect that causes language and vision bias simultaneously, then propose a counterfactual inference to remove the influence of this effect. The model trained in this strategy can concurrently and efficiently reduce vision and language bias. To the best of our knowledge, this is the first work to reduce biases resulting from confounding effects of vision and language in VQA, leveraging causal explain-away relations. We accompany our method with an explain-away strategy, pushing the accuracy of the questions with numerical answers results",
    "link": "http://arxiv.org/abs/2305.19664",
    "context": "Title: Unveiling Cross Modality Bias in Visual Question Answering: A Causal View with Possible Worlds VQA. (arXiv:2305.19664v1 [cs.CV])\nAbstract: To increase the generalization capability of VQA systems, many recent studies have tried to de-bias spurious language or vision associations that shortcut the question or image to the answer. Despite these efforts, the literature fails to address the confounding effect of vision and language simultaneously. As a result, when they reduce bias learned from one modality, they usually increase bias from the other. In this paper, we first model a confounding effect that causes language and vision bias simultaneously, then propose a counterfactual inference to remove the influence of this effect. The model trained in this strategy can concurrently and efficiently reduce vision and language bias. To the best of our knowledge, this is the first work to reduce biases resulting from confounding effects of vision and language in VQA, leveraging causal explain-away relations. We accompany our method with an explain-away strategy, pushing the accuracy of the questions with numerical answers results",
    "path": "papers/23/05/2305.19664.json",
    "total_tokens": 906,
    "translated_title": "揭示视觉问答中的跨模态偏见：可能世界VQA的因果视角",
    "translated_abstract": "为了增强VQA系统的泛化能力，许多最近的研究尝试消除捷径式的语言或视觉关联来回答问题。尽管这些研究致力于此，但文献还没有同时解决视觉和语言的混淆效应。结果，他们从一个模态减少学习偏见时，通常会增加另一个模态的偏见。本文首先建模导致语言和视觉偏见的混淆效应，然后提出一个反事实的推理方法来消除这种影响。在这种策略下训练的模型可以同时且高效地减少视觉和语言偏见。据我们所知，这是第一篇利用因果关系解释消除视觉问答中视觉和语言混淆效应的地址偏见的作品。我们将我们的方法与一个解释消除策略相结合，提高数字答案问题的准确性结果。",
    "tldr": "为了解决VQA系统因视觉和语言的混淆效应而导致的双重偏差问题，本文提出了一个反事实的推理方法。该方法可以同时并高效地减少视觉和语言偏见。",
    "en_tdlr": "This paper proposes a counterfactual inference to eliminate the confounding effect of vision and language bias in VQA systems, leading to a reduction in biases from both modalities. The approach leverages causal explain-away relations and is the first to address the issue of confounding effects in VQA systems."
}