{
    "title": "Class Conditional Gaussians for Continual Learning. (arXiv:2305.19076v1 [cs.LG])",
    "abstract": "Dealing with representation shift is one of the main problems in online continual learning. Current methods mainly solve this by reducing representation shift, but leave the classifier on top of the representation to slowly adapt, in many update steps, to the remaining representation shift, increasing forgetting. We propose DeepCCG, an empirical Bayesian approach to solve this problem. DeepCCG works by updating the posterior of a class conditional Gaussian classifier such that the classifier adapts instantly to representation shift. The use of a class conditional Gaussian classifier also enables DeepCCG to use a log conditional marginal likelihood loss to update the representation, which can be seen as a new type of replay. To perform the update to the classifier and representation, DeepCCG maintains a fixed number of examples in memory and so a key part of DeepCCG is selecting what examples to store, choosing the subset that minimises the KL divergence between the true posterior and t",
    "link": "http://arxiv.org/abs/2305.19076",
    "context": "Title: Class Conditional Gaussians for Continual Learning. (arXiv:2305.19076v1 [cs.LG])\nAbstract: Dealing with representation shift is one of the main problems in online continual learning. Current methods mainly solve this by reducing representation shift, but leave the classifier on top of the representation to slowly adapt, in many update steps, to the remaining representation shift, increasing forgetting. We propose DeepCCG, an empirical Bayesian approach to solve this problem. DeepCCG works by updating the posterior of a class conditional Gaussian classifier such that the classifier adapts instantly to representation shift. The use of a class conditional Gaussian classifier also enables DeepCCG to use a log conditional marginal likelihood loss to update the representation, which can be seen as a new type of replay. To perform the update to the classifier and representation, DeepCCG maintains a fixed number of examples in memory and so a key part of DeepCCG is selecting what examples to store, choosing the subset that minimises the KL divergence between the true posterior and t",
    "path": "papers/23/05/2305.19076.json",
    "total_tokens": 869,
    "translated_abstract": "处理在线持续学习中的表征漂移问题是当前的主要问题之一。目前的方法主要通过减少表征漂移来解决这个问题，但是残留的表征漂移仍然会导致分类器随着多次更新而逐渐适应，进而导致遗忘。我们提出了DeepCCG，这是一种经验贝叶斯方法，通过更新类条件高斯分类器的后验概率，使分类器能够即时适应表征漂移。类条件高斯分类器的使用还允许DeepCCG使用对数条件边际似然损失来更新表征，这可以看作是一种新类型的重播。为了对分类器和表征进行更新，DeepCCG在内存中维护一定数量的示例，因此DeepCCG的一个关键部分是选择需要存储的示例，选择最小化真实后验分布与近似后验分布KL散度的子集。",
    "tldr": "该论文提出了DeepCCG方法，采用类条件高斯分类器和对数条件边际似然损失来解决在线持续学习中的表征漂移问题，可以有效避免分类器逐渐适应而导致的遗忘。",
    "en_tdlr": "This paper proposes the DeepCCG method, which uses class conditional Gaussian classifier and log conditional marginal likelihood loss to solve the representation shift problem in online continual learning, which can effectively avoid forgetting caused by the classifier gradually adapting."
}