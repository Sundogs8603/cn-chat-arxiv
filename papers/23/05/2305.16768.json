{
    "title": "Towards a Common Understanding of Contributing Factors for Cross-Lingual Transfer in Multilingual Language Models: A Review. (arXiv:2305.16768v1 [cs.CL])",
    "abstract": "In recent years, pre-trained Multilingual Language Models (MLLMs) have shown a strong ability to transfer knowledge across different languages. However, given that the aspiration for such an ability has not been explicitly incorporated in the design of the majority of MLLMs, it is challenging to obtain a unique and straightforward explanation for its emergence. In this review paper, we survey literature that investigates different factors contributing to the capacity of MLLMs to perform zero-shot cross-lingual transfer and subsequently outline and discuss these factors in detail. To enhance the structure of this review and to facilitate consolidation with future studies, we identify five categories of such factors. In addition to providing a summary of empirical evidence from past studies, we identify consensuses among studies with consistent findings and resolve conflicts among contradictory ones. Our work contextualizes and unifies existing research streams which aim at explaining th",
    "link": "http://arxiv.org/abs/2305.16768",
    "context": "Title: Towards a Common Understanding of Contributing Factors for Cross-Lingual Transfer in Multilingual Language Models: A Review. (arXiv:2305.16768v1 [cs.CL])\nAbstract: In recent years, pre-trained Multilingual Language Models (MLLMs) have shown a strong ability to transfer knowledge across different languages. However, given that the aspiration for such an ability has not been explicitly incorporated in the design of the majority of MLLMs, it is challenging to obtain a unique and straightforward explanation for its emergence. In this review paper, we survey literature that investigates different factors contributing to the capacity of MLLMs to perform zero-shot cross-lingual transfer and subsequently outline and discuss these factors in detail. To enhance the structure of this review and to facilitate consolidation with future studies, we identify five categories of such factors. In addition to providing a summary of empirical evidence from past studies, we identify consensuses among studies with consistent findings and resolve conflicts among contradictory ones. Our work contextualizes and unifies existing research streams which aim at explaining th",
    "path": "papers/23/05/2305.16768.json",
    "total_tokens": 955,
    "translated_title": "多语言语言模型的跨语言转移因素的共同理解：一篇综述",
    "translated_abstract": "近年来，预训练的多语言语言模型（MLLMs）表现出了在不同语言之间传递知识的强大能力。然而，鉴于这种能力的追求并未明确地融入大多数MLLM设计中，很难对其出现进行独特而直接的解释。在本综述论文中，我们调查了研究MLLM的跨语言转移能力的不同因素的文献，并详细概述和讨论了这些因素。为了增强这个综述的结构并便于与未来的研究整合，我们确定了五类这样的因素。除提供过去研究的经验证据概述外，我们还确定了在具有一致发现的研究中的共识，并解决了在矛盾的研究中的冲突。我们的工作将旨在解释MLLM跨语言转移能力的现有研究流进行了全面的背景和统一。",
    "tldr": "本文对多语言语言模型 (MLLMs)的跨语言转移能力的不同因素进行了调查和综述，将这些因素分成五类并提供了过去研究的经验证据。本文的工作旨在全面背景和统一MLLMs跨语言转移的现有研究流。",
    "en_tdlr": "This review paper surveys literature that investigates different factors contributing to the capacity of Multilingual Language Models (MLLMs) to perform zero-shot cross-lingual transfer, identifies five categories of such factors and provides empirical evidence from past studies. The work aims at contextualizing and unifying the existing research streams which aim at explaining MLLMs' cross-lingual transfer ability."
}