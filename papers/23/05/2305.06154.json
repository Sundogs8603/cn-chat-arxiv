{
    "title": "Alleviating Over-smoothing for Unsupervised Sentence Representation. (arXiv:2305.06154v1 [cs.CL])",
    "abstract": "Currently, learning better unsupervised sentence representations is the pursuit of many natural language processing communities. Lots of approaches based on pre-trained language models (PLMs) and contrastive learning have achieved promising results on this task. Experimentally, we observe that the over-smoothing problem reduces the capacity of these powerful PLMs, leading to sub-optimal sentence representations. In this paper, we present a Simple method named Self-Contrastive Learning (SSCL) to alleviate this issue, which samples negatives from PLMs intermediate layers, improving the quality of the sentence representation. Our proposed method is quite simple and can be easily extended to various state-of-the-art models for performance boosting, which can be seen as a plug-and-play contrastive framework for learning unsupervised sentence representation. Extensive results prove that SSCL brings the superior performance improvements of different strong baselines (e.g., BERT and SimCSE) on",
    "link": "http://arxiv.org/abs/2305.06154",
    "context": "Title: Alleviating Over-smoothing for Unsupervised Sentence Representation. (arXiv:2305.06154v1 [cs.CL])\nAbstract: Currently, learning better unsupervised sentence representations is the pursuit of many natural language processing communities. Lots of approaches based on pre-trained language models (PLMs) and contrastive learning have achieved promising results on this task. Experimentally, we observe that the over-smoothing problem reduces the capacity of these powerful PLMs, leading to sub-optimal sentence representations. In this paper, we present a Simple method named Self-Contrastive Learning (SSCL) to alleviate this issue, which samples negatives from PLMs intermediate layers, improving the quality of the sentence representation. Our proposed method is quite simple and can be easily extended to various state-of-the-art models for performance boosting, which can be seen as a plug-and-play contrastive framework for learning unsupervised sentence representation. Extensive results prove that SSCL brings the superior performance improvements of different strong baselines (e.g., BERT and SimCSE) on",
    "path": "papers/23/05/2305.06154.json",
    "total_tokens": 867,
    "translated_title": "缓解无监督句子表示中的平滑问题",
    "translated_abstract": "当前，学习更好的无监督句子表示是许多自然语言处理社区的追求。许多基于预训练语言模型（PLMs）和对比学习的方法在此任务上取得了有希望的结果。实验表明，过度平滑的问题降低了这些强大PLMs的能力，导致子优句子表示。在本文中，我们提出了一种简单的方法，称为自对抗学习（SSCL），以缓解这个问题，该方法从PLMs中间层中采样负面样本，提高句子表示的质量。我们提出的方法非常简单，可以轻松地扩展到各种最先进的模型进行性能提升，可以看作是一种插件式的对比框架，用于学习无监督句子表示。广泛的结果证明，SSCL带来了不同强基线的卓越性能改进（例如BERT和SimCSE）。",
    "tldr": "本论文提出了自对抗学习（SSCL）方法，通过从PLMs较低层中采样负样本，缓解了无监督句子表示平滑问题，提升了句子表示的质量和性能。"
}