{
    "title": "On Evaluating Adversarial Robustness of Large Vision-Language Models. (arXiv:2305.16934v2 [cs.CV] UPDATED)",
    "abstract": "Large vision-language models (VLMs) such as GPT-4 have achieved unprecedented performance in response generation, especially with visual inputs, enabling more creative and adaptable interaction than large language models such as ChatGPT. Nonetheless, multimodal generation exacerbates safety concerns, since adversaries may successfully evade the entire system by subtly manipulating the most vulnerable modality (e.g., vision). To this end, we propose evaluating the robustness of open-source large VLMs in the most realistic and high-risk setting, where adversaries have only black-box system access and seek to deceive the model into returning the targeted responses. In particular, we first craft targeted adversarial examples against pretrained models such as CLIP and BLIP, and then transfer these adversarial examples to other VLMs such as MiniGPT-4, LLaVA, UniDiffuser, BLIP-2, and Img2Prompt. In addition, we observe that black-box queries on these VLMs can further improve the effectiveness",
    "link": "http://arxiv.org/abs/2305.16934",
    "context": "Title: On Evaluating Adversarial Robustness of Large Vision-Language Models. (arXiv:2305.16934v2 [cs.CV] UPDATED)\nAbstract: Large vision-language models (VLMs) such as GPT-4 have achieved unprecedented performance in response generation, especially with visual inputs, enabling more creative and adaptable interaction than large language models such as ChatGPT. Nonetheless, multimodal generation exacerbates safety concerns, since adversaries may successfully evade the entire system by subtly manipulating the most vulnerable modality (e.g., vision). To this end, we propose evaluating the robustness of open-source large VLMs in the most realistic and high-risk setting, where adversaries have only black-box system access and seek to deceive the model into returning the targeted responses. In particular, we first craft targeted adversarial examples against pretrained models such as CLIP and BLIP, and then transfer these adversarial examples to other VLMs such as MiniGPT-4, LLaVA, UniDiffuser, BLIP-2, and Img2Prompt. In addition, we observe that black-box queries on these VLMs can further improve the effectiveness",
    "path": "papers/23/05/2305.16934.json",
    "total_tokens": 948,
    "translated_title": "评估大型视觉语言模型的对抗鲁棒性",
    "translated_abstract": "大型视觉语言模型（VLMs）如GPT-4在生成响应方面取得了前所未有的性能，尤其是在视觉输入方面，使得交互更有创造力和适应性，而不仅仅是大型语言模型如ChatGPT。然而，多模态生成加剧了安全性问题，因为对手可以通过微妙地操纵最易受攻击的模态（例如视觉）成功避开整个系统。为此，我们提出在最真实和高风险的情境下评估开源大型VLMs的鲁棒性，其中对手只能黑盒访问系统，并试图欺骗模型返回目标响应。具体而言，我们首先针对预训练模型（如CLIP和BLIP）构建有针对性的对抗样本，然后将这些对抗样本转移到其他VLMs（如MiniGPT-4、LLaVA、UniDiffuser、BLIP-2和Img2Prompt）。此外，我们观察到，在这些VLMs上进行黑盒查询可以进一步提高效果。",
    "tldr": "该论文提出在最真实和高风险的情境中评估大型视觉语言模型的对抗鲁棒性。作者首先构建有针对性的对抗样本，然后将其转移到其他模型中进行评估，并观察到黑盒查询可以改进效果。",
    "en_tdlr": "This paper proposes evaluating the adversarial robustness of large vision-language models in a realistic and high-risk setting. The authors first craft targeted adversarial examples and then transfer them to other models for evaluation, observing that black-box queries can improve the effectiveness."
}