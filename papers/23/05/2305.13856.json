{
    "title": "On the Optimal Batch Size for Byzantine-Robust Distributed Learning. (arXiv:2305.13856v1 [cs.LG])",
    "abstract": "Byzantine-robust distributed learning (BRDL), in which computing devices are likely to behave abnormally due to accidental failures or malicious attacks, has recently become a hot research topic. However, even in the independent and identically distributed (i.i.d.) case, existing BRDL methods will suffer from a significant drop on model accuracy due to the large variance of stochastic gradients. Increasing batch sizes is a simple yet effective way to reduce the variance. However, when the total number of gradient computation is fixed, a too-large batch size will lead to a too-small iteration number (update number), which may also degrade the model accuracy. In view of this challenge, we mainly study the optimal batch size when the total number of gradient computation is fixed in this work. In particular, we theoretically and empirically show that when the total number of gradient computation is fixed, the optimal batch size in BRDL increases with the fraction of Byzantine workers. Ther",
    "link": "http://arxiv.org/abs/2305.13856",
    "context": "Title: On the Optimal Batch Size for Byzantine-Robust Distributed Learning. (arXiv:2305.13856v1 [cs.LG])\nAbstract: Byzantine-robust distributed learning (BRDL), in which computing devices are likely to behave abnormally due to accidental failures or malicious attacks, has recently become a hot research topic. However, even in the independent and identically distributed (i.i.d.) case, existing BRDL methods will suffer from a significant drop on model accuracy due to the large variance of stochastic gradients. Increasing batch sizes is a simple yet effective way to reduce the variance. However, when the total number of gradient computation is fixed, a too-large batch size will lead to a too-small iteration number (update number), which may also degrade the model accuracy. In view of this challenge, we mainly study the optimal batch size when the total number of gradient computation is fixed in this work. In particular, we theoretically and empirically show that when the total number of gradient computation is fixed, the optimal batch size in BRDL increases with the fraction of Byzantine workers. Ther",
    "path": "papers/23/05/2305.13856.json",
    "total_tokens": 901,
    "translated_title": "论拜占庭容错分布式学习的最佳批处理大小",
    "translated_abstract": "近来，由于意外失误或恶意攻击导致计算设备异常行为的拜占庭容错分布式学习（BRDL）已成为热门研究课题。然而，在独立同分布（i.i.d.）的情况下，由于随机梯度的大方差，现有的BRDL方法仍会导致模型准确率显著下降。增加批处理大小是减少方差的简单而有效的方法。然而，当梯度计算总数固定时，过大的批处理大小会导致迭代次数过少（更新次数），可能也会降低模型准确率。针对这一挑战，本文主要研究在固定梯度计算总数的情况下最佳的批处理大小。具体而言，我们在理论和经验上表明，当梯度计算总数固定时，BRDL中最佳的批处理大小随拜占庭工人的比例增加而增加。",
    "tldr": "本文研究的问题是在拜占庭容错分布式学习中，当梯度计算总数固定时，最佳的批处理大小随拜占庭工人的比例增加而增加。",
    "en_tdlr": "This paper studies the optimal batch size in Byzantine-robust distributed learning when the total number of gradient computation is fixed, and shows that the optimal batch size increases with the fraction of Byzantine workers both theoretically and empirically."
}