{
    "title": "Tractable Probabilistic Graph Representation Learning with Graph-Induced Sum-Product Networks. (arXiv:2305.10544v1 [cs.LG])",
    "abstract": "We introduce Graph-Induced Sum-Product Networks (GSPNs), a new probabilistic framework for graph representation learning that can tractably answer probabilistic queries. Inspired by the computational trees induced by vertices in the context of message-passing neural networks, we build hierarchies of sum-product networks (SPNs) where the parameters of a parent SPN are learnable transformations of the a-posterior mixing probabilities of its children's sum units. Due to weight sharing and the tree-shaped computation graphs of GSPNs, we obtain the efficiency and efficacy of deep graph networks with the additional advantages of a purely probabilistic model. We show the model's competitiveness on scarce supervision scenarios, handling missing data, and graph classification in comparison to popular neural models. We complement the experiments with qualitative analyses on hyper-parameters and the model's ability to answer probabilistic queries.",
    "link": "http://arxiv.org/abs/2305.10544",
    "context": "Title: Tractable Probabilistic Graph Representation Learning with Graph-Induced Sum-Product Networks. (arXiv:2305.10544v1 [cs.LG])\nAbstract: We introduce Graph-Induced Sum-Product Networks (GSPNs), a new probabilistic framework for graph representation learning that can tractably answer probabilistic queries. Inspired by the computational trees induced by vertices in the context of message-passing neural networks, we build hierarchies of sum-product networks (SPNs) where the parameters of a parent SPN are learnable transformations of the a-posterior mixing probabilities of its children's sum units. Due to weight sharing and the tree-shaped computation graphs of GSPNs, we obtain the efficiency and efficacy of deep graph networks with the additional advantages of a purely probabilistic model. We show the model's competitiveness on scarce supervision scenarios, handling missing data, and graph classification in comparison to popular neural models. We complement the experiments with qualitative analyses on hyper-parameters and the model's ability to answer probabilistic queries.",
    "path": "papers/23/05/2305.10544.json",
    "total_tokens": 852,
    "translated_title": "可计算的基于图诱导的和积网络进行概率图表示学习",
    "translated_abstract": "我们介绍了基于图诱导的和积网络 (GSPN)，它是一种新的概率框架，用于图表示学习，可以可计算地回答概率查询。受消息传递神经网络中由顶点引起的计算树的启发，我们建立了一组和积网络（SPN）的层次结构，其中父SPN的参数是其子级的后验混合概率的可学习变换。由于权重共享和GSPN的树状计算图，我们获得了纯概率模型的效率和深度图网络的效果。我们在缺乏监督的情况下，处理缺失数据和图分类问题，证明了该模型相对于流行的神经模型的竞争力。我们通过超参数和模型回答概率查询的能力进行定性分析。",
    "tldr": "GSPNs是一种新的概率框架，用于图表示学习，可以可计算地回答概率查询，并通过权重共享和树状计算图的优势获得了纯概率模型的效率和深度图网络的效果。",
    "en_tdlr": "GSPNs is a new probabilistic framework for graph representation learning that can tractably answer probabilistic queries. It gains the efficiency and efficacy of deep graph networks by weight sharing and the tree-shaped computation graphs of GSPNs, with the additional advantages of a purely probabilistic model."
}