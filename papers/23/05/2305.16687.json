{
    "title": "Balanced Supervised Contrastive Learning for Few-Shot Class-Incremental Learning. (arXiv:2305.16687v1 [cs.CV])",
    "abstract": "Few-shot class-incremental learning (FSCIL) presents the primary challenge of balancing underfitting to a new session's task and forgetting the tasks from previous sessions. To address this challenge, we develop a simple yet powerful learning scheme that integrates effective methods for each core component of the FSCIL network, including the feature extractor, base session classifiers, and incremental session classifiers. In feature extractor training, our goal is to obtain balanced generic representations that benefit both current viewable and unseen or past classes. To achieve this, we propose a balanced supervised contrastive loss that effectively balances these two objectives. In terms of classifiers, we analyze and emphasize the importance of unifying initialization methods for both the base and incremental session classifiers. Our method demonstrates outstanding ability for new task learning and preventing forgetting on CUB200, CIFAR100, and miniImagenet datasets, with significan",
    "link": "http://arxiv.org/abs/2305.16687",
    "context": "Title: Balanced Supervised Contrastive Learning for Few-Shot Class-Incremental Learning. (arXiv:2305.16687v1 [cs.CV])\nAbstract: Few-shot class-incremental learning (FSCIL) presents the primary challenge of balancing underfitting to a new session's task and forgetting the tasks from previous sessions. To address this challenge, we develop a simple yet powerful learning scheme that integrates effective methods for each core component of the FSCIL network, including the feature extractor, base session classifiers, and incremental session classifiers. In feature extractor training, our goal is to obtain balanced generic representations that benefit both current viewable and unseen or past classes. To achieve this, we propose a balanced supervised contrastive loss that effectively balances these two objectives. In terms of classifiers, we analyze and emphasize the importance of unifying initialization methods for both the base and incremental session classifiers. Our method demonstrates outstanding ability for new task learning and preventing forgetting on CUB200, CIFAR100, and miniImagenet datasets, with significan",
    "path": "papers/23/05/2305.16687.json",
    "total_tokens": 852,
    "translated_title": "面向少样本类增量学习的平衡监督对比学习方法",
    "translated_abstract": "少样本类增量学习(FSCIL)主要面临的挑战是如何平衡新任务的欠拟合和遗忘之前任务的问题。为了解决这个问题，我们提出了一种简单而强大的学习方法，集成了FSCIL网络的核心组件的有效方法，包括特征提取器、基础会话分类器和增量会话分类器。在特征提取器的训练中，我们的目标是获得平衡的通用表示，既有利于当前可见类，又有利于未来或过去的类。为了实现这一点，我们提出了一种平衡的监督对比损失，可以有效平衡这两个目标。在分类器方面，我们分析并强调了对于基础和增量会话分类器的统一初始化方法的重要性。我们的方法在CUB200、CIFAR100和miniImagenet数据集上表现出了显著的新任务学习能力和防止遗忘的能力。",
    "tldr": "该论文提出了一种平衡监督对比学习方法来解决少样本类增量学习中的平衡性问题，并且在三个数据集上的表现均非常优秀。"
}