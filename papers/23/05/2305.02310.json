{
    "title": "Real-Time Radiance Fields for Single-Image Portrait View Synthesis. (arXiv:2305.02310v1 [cs.CV])",
    "abstract": "We present a one-shot method to infer and render a photorealistic 3D representation from a single unposed image (e.g., face portrait) in real-time. Given a single RGB input, our image encoder directly predicts a canonical triplane representation of a neural radiance field for 3D-aware novel view synthesis via volume rendering. Our method is fast (24 fps) on consumer hardware, and produces higher quality results than strong GAN-inversion baselines that require test-time optimization. To train our triplane encoder pipeline, we use only synthetic data, showing how to distill the knowledge from a pretrained 3D GAN into a feedforward encoder. Technical contributions include a Vision Transformer-based triplane encoder, a camera data augmentation strategy, and a well-designed loss function for synthetic data training. We benchmark against the state-of-the-art methods, demonstrating significant improvements in robustness and image quality in challenging real-world settings. We showcase our res",
    "link": "http://arxiv.org/abs/2305.02310",
    "context": "Title: Real-Time Radiance Fields for Single-Image Portrait View Synthesis. (arXiv:2305.02310v1 [cs.CV])\nAbstract: We present a one-shot method to infer and render a photorealistic 3D representation from a single unposed image (e.g., face portrait) in real-time. Given a single RGB input, our image encoder directly predicts a canonical triplane representation of a neural radiance field for 3D-aware novel view synthesis via volume rendering. Our method is fast (24 fps) on consumer hardware, and produces higher quality results than strong GAN-inversion baselines that require test-time optimization. To train our triplane encoder pipeline, we use only synthetic data, showing how to distill the knowledge from a pretrained 3D GAN into a feedforward encoder. Technical contributions include a Vision Transformer-based triplane encoder, a camera data augmentation strategy, and a well-designed loss function for synthetic data training. We benchmark against the state-of-the-art methods, demonstrating significant improvements in robustness and image quality in challenging real-world settings. We showcase our res",
    "path": "papers/23/05/2305.02310.json",
    "total_tokens": 969,
    "translated_title": "适用于单张图像人像的实时辐射场合成",
    "translated_abstract": "我们提出了一个单拍摄方法，可以从单张未经过姿势调整的图像（例如面部肖像）中推断和渲染出逼真的3D表示，并实时合成。 给定单个RGB输入，我们的图像编码器直接预测由神经辐射场的规范三面图表示，通过体渲染进行三维感知的新视图合成。我们的方法在消费级硬件上快速（24fps），且产生的质量高于需要测试时间优化的强GAN反演基线。为了训练三面图编码器管道，我们只使用合成数据，展示了如何从预训练的3D GAN中提取知识，并将其蒸馏成前馈编码器。技术贡献包括基于Vision Transformer的三面图编码器、相机数据增强策略以及针对合成数据训练的良好设计的损失函数。我们在最先进的方法上进行基准测试，在具有挑战性的现实世界场景中展示了显着的鲁棒性和图像质量改进。我们展示了我们的结果，表明它能够生成具有高质量的3D感知人像合成结果。",
    "tldr": "该论文提出了一种适用于单张图片的实时辐射场合成方法，能够从单张未经过姿势调整的图像中推断和渲染出逼真的3D表示，并产生高质量的3D感知人像合成结果。"
}