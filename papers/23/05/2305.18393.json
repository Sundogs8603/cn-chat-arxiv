{
    "title": "Training Private Models That Know What They Don't Know. (arXiv:2305.18393v1 [cs.LG])",
    "abstract": "Training reliable deep learning models which avoid making overconfident but incorrect predictions is a longstanding challenge. This challenge is further exacerbated when learning has to be differentially private: protection provided to sensitive data comes at the price of injecting additional randomness into the learning process. In this work, we conduct a thorough empirical investigation of selective classifiers -- that can abstain when they are unsure -- under a differential privacy constraint. We find that several popular selective prediction approaches are ineffective in a differentially private setting as they increase the risk of privacy leakage. At the same time, we identify that a recent approach that only uses checkpoints produced by an off-the-shelf private learning algorithm stands out as particularly suitable under DP. Further, we show that differential privacy does not just harm utility but also degrades selective classification performance. To analyze this effect across p",
    "link": "http://arxiv.org/abs/2305.18393",
    "context": "Title: Training Private Models That Know What They Don't Know. (arXiv:2305.18393v1 [cs.LG])\nAbstract: Training reliable deep learning models which avoid making overconfident but incorrect predictions is a longstanding challenge. This challenge is further exacerbated when learning has to be differentially private: protection provided to sensitive data comes at the price of injecting additional randomness into the learning process. In this work, we conduct a thorough empirical investigation of selective classifiers -- that can abstain when they are unsure -- under a differential privacy constraint. We find that several popular selective prediction approaches are ineffective in a differentially private setting as they increase the risk of privacy leakage. At the same time, we identify that a recent approach that only uses checkpoints produced by an off-the-shelf private learning algorithm stands out as particularly suitable under DP. Further, we show that differential privacy does not just harm utility but also degrades selective classification performance. To analyze this effect across p",
    "path": "papers/23/05/2305.18393.json",
    "total_tokens": 933,
    "translated_title": "训练“知道自己不知道什么”的私有模型",
    "translated_abstract": "训练可靠的深度学习模型以避免过度自信但不正确的预测一直是一个长期的挑战。当学习必须是差分隐私时，这一挑战进一步恶化：对敏感数据提供的保护会导致在学习过程中注入额外的随机性。在这项研究中，我们对在差分隐私约束下可以放弃的选择分类器进行了彻底的实证调查。我们发现几种流行的选择性预测方法在差分隐私环境下是无效的，因为它们增加了隐私泄露的风险。同时，我们发现最近一种方法，只使用由现成的私有学习算法产生的检查点，特别适用于DP（差分隐私）。此外，我们还展示了差分隐私不仅会损害效用，还会降低选择性分类的性能。为了分析这种效应，我们将实证方法在各种数据集上进行了测试，并展示了其通用性和有用性。",
    "tldr": "本研究通过实证调查发现，在差分隐私约束下，几种流行的选择性预测方法会增加隐私泄露的风险。该研究发现一种只使用现成私有学习算法检查点的方法最适用于差分隐私。",
    "en_tdlr": "This study investigates the effectiveness of selective classifiers that can abstain when unsure under differential privacy constraints. It identifies popular selective prediction approaches as ineffective due to increasing privacy leakage risk, while highlighting a recent approach using checkpoints from off-the-shelf private learning algorithms as particularly suitable under differential privacy. The study also shows that differential privacy not only harms utility but also degrades selective classification performance."
}