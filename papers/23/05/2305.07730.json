{
    "title": "Learning in Inverse Optimization: Incenter Cost, Augmented Suboptimality Loss, and Algorithms. (arXiv:2305.07730v1 [math.OC] CROSS LISTED)",
    "abstract": "In Inverse Optimization (IO), an expert agent solves an optimization problem parametric in an exogenous signal. From a learning perspective, the goal is to learn the expert's cost function given a dataset of signals and corresponding optimal actions. Motivated by the geometry of the IO set of consistent cost vectors, we introduce the \"incenter\" concept, a new notion akin to circumcenter recently proposed by Besbes et al. [2022]. Discussing the geometric and robustness interpretation of the incenter cost vector, we develop corresponding tractable convex reformulations, which are in contrast with the circumcenter, which we show is equivalent to an intractable optimization program. We further propose a novel loss function called Augmented Suboptimality Loss (ASL), as a relaxation of the incenter concept, for problems with inconsistent data. Exploiting the structure of the ASL, we propose a novel first-order algorithm, which we name Stochastic Approximate Mirror Descent. This algorithm com",
    "link": "http://arxiv.org/abs/2305.07730",
    "context": "Title: Learning in Inverse Optimization: Incenter Cost, Augmented Suboptimality Loss, and Algorithms. (arXiv:2305.07730v1 [math.OC] CROSS LISTED)\nAbstract: In Inverse Optimization (IO), an expert agent solves an optimization problem parametric in an exogenous signal. From a learning perspective, the goal is to learn the expert's cost function given a dataset of signals and corresponding optimal actions. Motivated by the geometry of the IO set of consistent cost vectors, we introduce the \"incenter\" concept, a new notion akin to circumcenter recently proposed by Besbes et al. [2022]. Discussing the geometric and robustness interpretation of the incenter cost vector, we develop corresponding tractable convex reformulations, which are in contrast with the circumcenter, which we show is equivalent to an intractable optimization program. We further propose a novel loss function called Augmented Suboptimality Loss (ASL), as a relaxation of the incenter concept, for problems with inconsistent data. Exploiting the structure of the ASL, we propose a novel first-order algorithm, which we name Stochastic Approximate Mirror Descent. This algorithm com",
    "path": "papers/23/05/2305.07730.json",
    "total_tokens": 966,
    "translated_title": "逆优化学习：内心成本、增强次优损失和算法",
    "translated_abstract": "在逆优化学习中，专家代理人解决参数化于外部信号的优化问题。从学习的角度，目标是在给定一个信号和相应最优行动的数据集的情况下，学习专家的成本函数。受到与逆优化集一致的成本向量的几何形状的启发，我们引入了类似于Besbes等人最近提出的外心概念的 \"内心\"概念。我们讨论了内心成本向量的几何和鲁棒性解释，并开发了相应的可行凸形式，与外心相反，我们展示了外接圆等效于一个难以处理的优化程序。我们进一步提出了一种新型的损失函数，称为增强次优损失（ASL），作为内心概念的一种松弛形式，用于处理不一致数据的问题。利用ASL的结构，我们提出了一种新颖的一阶算法，命名为随机逼近镜像下降。这种算法带来了比现有算法更好的性能。",
    "tldr": "本论文提出了逆优化学习的新概念——内心概念，以及相应的可行凸形式，并开发了新型损失函数ASL以及一阶算法Stochastic Approximate Mirror Descent（SAM）来学习专家的成本函数。",
    "en_tdlr": "This paper proposes a new concept, the \"incenter\" concept, and its corresponding tractable convex reformulations for inverse optimization learning. It also introduces a novel loss function called Augmented Suboptimality Loss (ASL) and a first-order algorithm named Stochastic Approximate Mirror Descent (SAM) to learn the expert's cost function."
}