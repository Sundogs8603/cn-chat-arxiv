{
    "title": "AMTSS: An Adaptive Multi-Teacher Single-Student Knowledge Distillation Framework For Multilingual Language Inference. (arXiv:2305.07928v1 [cs.CL])",
    "abstract": "Knowledge distillation is of key importance to launching multilingual pre-trained language models for real applications. To support cost-effective language inference in multilingual settings, we propose AMTSS, an adaptive multi-teacher single-student distillation framework, which allows distilling knowledge from multiple teachers to a single student. We first introduce an adaptive learning strategy and teacher importance weight, which enables a student to effectively learn from max-margin teachers and easily adapt to new languages. Moreover, we present a shared student encoder with different projection layers in support of multiple languages, which contributes to largely reducing development and machine cost. Experimental results show that AMTSS gains competitive results on the public XNLI dataset and the realistic industrial dataset AliExpress (AE) in the E-commerce scenario.",
    "link": "http://arxiv.org/abs/2305.07928",
    "context": "Title: AMTSS: An Adaptive Multi-Teacher Single-Student Knowledge Distillation Framework For Multilingual Language Inference. (arXiv:2305.07928v1 [cs.CL])\nAbstract: Knowledge distillation is of key importance to launching multilingual pre-trained language models for real applications. To support cost-effective language inference in multilingual settings, we propose AMTSS, an adaptive multi-teacher single-student distillation framework, which allows distilling knowledge from multiple teachers to a single student. We first introduce an adaptive learning strategy and teacher importance weight, which enables a student to effectively learn from max-margin teachers and easily adapt to new languages. Moreover, we present a shared student encoder with different projection layers in support of multiple languages, which contributes to largely reducing development and machine cost. Experimental results show that AMTSS gains competitive results on the public XNLI dataset and the realistic industrial dataset AliExpress (AE) in the E-commerce scenario.",
    "path": "papers/23/05/2305.07928.json",
    "total_tokens": 852,
    "translated_title": "AMTSS：一种适用于多语言语言推理的自适应多教师单学生知识蒸馏框架",
    "translated_abstract": "知识蒸馏对于推出多语言预训练语言模型的实际应用至关重要。为支持多语言设置下的成本效益的语言推理，我们提出了AMTSS，一种自适应多教师单学生蒸馏框架，它允许从多个老师中提炼知识到一个学生中。我们首先引入一种自适应学习策略和教师重要性权重，使学生能够有效地从最大边缘老师中学习，并轻松适应新语言。此外，我们提出了一个共享的学生编码器，以不同的投影层支持多种语言，这有助于大大降低开发和机器成本。实验结果显示，AMTSS在公共XNLI数据集和真实的电子商务行业数据集AliExpress（AE）中获得了竞争性的结果。",
    "tldr": "AMTSS是一种自适应多教师单学生知识蒸馏框架，可以在多语言设置下支持成本效益的语言推理，并取得了在XNLI数据集和AliExpress中竞争性的结果。",
    "en_tdlr": "AMTSS is an adaptive multi-teacher single-student distillation framework that supports cost-effective language inference in multilingual settings and achieves competitive results in the XNLI dataset and AliExpress."
}