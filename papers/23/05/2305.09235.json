{
    "title": "Synthetic data, real errors: how (not) to publish and use synthetic data. (arXiv:2305.09235v1 [cs.LG])",
    "abstract": "Generating synthetic data through generative models is gaining interest in the ML community and beyond, promising a future where datasets can be tailored to individual needs. Unfortunately, synthetic data is usually not perfect, resulting in potential errors in downstream tasks. In this work we explore how the generative process affects the downstream ML task. We show that the naive synthetic data approach -- using synthetic data as if it is real -- leads to downstream models and analyses that do not generalize well to real data. As a first step towards better ML in the synthetic data regime, we introduce Deep Generative Ensemble (DGE) -- a framework inspired by Deep Ensembles that aims to implicitly approximate the posterior distribution over the generative process model parameters. DGE improves downstream model training, evaluation, and uncertainty quantification, vastly outperforming the naive approach on average. The largest improvements are achieved for minority classes and low-de",
    "link": "http://arxiv.org/abs/2305.09235",
    "context": "Title: Synthetic data, real errors: how (not) to publish and use synthetic data. (arXiv:2305.09235v1 [cs.LG])\nAbstract: Generating synthetic data through generative models is gaining interest in the ML community and beyond, promising a future where datasets can be tailored to individual needs. Unfortunately, synthetic data is usually not perfect, resulting in potential errors in downstream tasks. In this work we explore how the generative process affects the downstream ML task. We show that the naive synthetic data approach -- using synthetic data as if it is real -- leads to downstream models and analyses that do not generalize well to real data. As a first step towards better ML in the synthetic data regime, we introduce Deep Generative Ensemble (DGE) -- a framework inspired by Deep Ensembles that aims to implicitly approximate the posterior distribution over the generative process model parameters. DGE improves downstream model training, evaluation, and uncertainty quantification, vastly outperforming the naive approach on average. The largest improvements are achieved for minority classes and low-de",
    "path": "papers/23/05/2305.09235.json",
    "total_tokens": 968,
    "translated_title": "合成数据，真实误差：如何（不）发布和使用合成数据",
    "translated_abstract": "通过生成模型生成合成数据在机器学习社区和其他领域越来越受到关注，这种方法承诺将来可以根据个体需求定制数据集。不幸的是，合成数据通常并不完美，可能导致下游任务中的潜在错误。在本文中，我们探讨了生成过程对下游机器学习任务的影响。我们展示了单纯的合成数据方法——将合成数据视为真实数据使用——会导致下游模型和分析无法很好地推广到真实数据。作为合成数据环境下更好的机器学习的第一步，我们引入了深度生成集成（DGE）——受到深度集成启发的框架，旨在隐式地近似生成过程模型参数的后验分布。DGE改善了下游模型的训练、评估和不确定性量化，平均而言远远优于单纯的方法。对于少数类和低密度区域，最大的改进效果得到了实现。",
    "tldr": "合成数据在机器学习领域受到了越来越多的关注，但是不完美的合成数据可能会导致下游机器学习任务中的潜在错误。为了改进这种情况，研究人员引入了深度生成集成（DGE）框架来近似生成过程模型参数的后验分布，以提高下游模型的训练和评估效果。",
    "en_tdlr": "Synthetic data has gained increasing attention in the machine learning community, but the imperfection of synthetic data may result in potential errors in downstream tasks. To address this issue, researchers introduce a Deep Generative Ensemble (DGE) framework to approximate the posterior distribution over the generative process model parameters, which improves downstream model training and evaluation."
}