{
    "title": "HiPool: Modeling Long Documents Using Graph Neural Networks. (arXiv:2305.03319v1 [cs.CL])",
    "abstract": "Encoding long sequences in Natural Language Processing (NLP) is a challenging problem. Though recent pretraining language models achieve satisfying performances in many NLP tasks, they are still restricted by a pre-defined maximum length, making them challenging to be extended to longer sequences. So some recent works utilize hierarchies to model long sequences. However, most of them apply sequential models for upper hierarchies, suffering from long dependency issues. In this paper, we alleviate these issues through a graph-based method. We first chunk the sequence with a fixed length to model the sentence-level information. We then leverage graphs to model intra- and cross-sentence correlations with a new attention mechanism. Additionally, due to limited standard benchmarks for long document classification (LDC), we propose a new challenging benchmark, totaling six datasets with up to 53k samples and 4034 average tokens' length. Evaluation shows our model surpasses competitive baselin",
    "link": "http://arxiv.org/abs/2305.03319",
    "context": "Title: HiPool: Modeling Long Documents Using Graph Neural Networks. (arXiv:2305.03319v1 [cs.CL])\nAbstract: Encoding long sequences in Natural Language Processing (NLP) is a challenging problem. Though recent pretraining language models achieve satisfying performances in many NLP tasks, they are still restricted by a pre-defined maximum length, making them challenging to be extended to longer sequences. So some recent works utilize hierarchies to model long sequences. However, most of them apply sequential models for upper hierarchies, suffering from long dependency issues. In this paper, we alleviate these issues through a graph-based method. We first chunk the sequence with a fixed length to model the sentence-level information. We then leverage graphs to model intra- and cross-sentence correlations with a new attention mechanism. Additionally, due to limited standard benchmarks for long document classification (LDC), we propose a new challenging benchmark, totaling six datasets with up to 53k samples and 4034 average tokens' length. Evaluation shows our model surpasses competitive baselin",
    "path": "papers/23/05/2305.03319.json",
    "total_tokens": 926,
    "translated_title": "HiPool：利用图神经网络对长文档进行建模",
    "translated_abstract": "在自然语言处理中，编码长序列是一个具有挑战性的问题。虽然最近的预训练语言模型在许多NLP任务中达到了令人满意的表现，但它们仍受到预定义的最大长度的限制，使得它们难以扩展到更长的序列。因此，一些最近的工作利用层次结构来建模长序列。然而，它们大多数是对上层使用顺序模型，面临着长期依赖问题。在本文中，我们通过一种基于图的方法来缓解这些问题。我们首先使用固定长度对序列进行分块，以模拟句子级别的信息。然后，我们利用图来模拟句内和跨句的关联性，并使用一种新的注意力机制。此外，由于长文档分类的基准测试数据较少，我们提出了一个新的有挑战性的基准测试，共计六个数据集，样本总数达53000个，平均标记长度为4034个。评估结果显示，我们的模型在所有基准测试中均优于竞争基线方法，实现了最先进的性能。",
    "tldr": "本论文提出了一种基于图神经网络的方法来模拟长文档，解决了顺序模型中的长期依赖问题，在新提出的基准测试中达到了最先进的性能。",
    "en_tdlr": "This paper proposes a graph-based method using graph neural networks to model long documents, alleviating long dependency issues in sequential models. The proposed method achieves state-of-the-art performance on a new challenging benchmark."
}