{
    "title": "Attacks on Online Learners: a Teacher-Student Analysis. (arXiv:2305.11132v1 [stat.ML])",
    "abstract": "Machine learning models are famously vulnerable to adversarial attacks: small ad-hoc perturbations of the data that can catastrophically alter the model predictions. While a large literature has studied the case of test-time attacks on pre-trained models, the important case of attacks in an online learning setting has received little attention so far. In this work, we use a control-theoretical perspective to study the scenario where an attacker may perturb data labels to manipulate the learning dynamics of an online learner. We perform a theoretical analysis of the problem in a teacher-student setup, considering different attack strategies, and obtaining analytical results for the steady state of simple linear learners. These results enable us to prove that a discontinuous transition in the learner's accuracy occurs when the attack strength exceeds a critical threshold. We then study empirically attacks on learners with complex architectures using real data, confirming the insights of ",
    "link": "http://arxiv.org/abs/2305.11132",
    "context": "Title: Attacks on Online Learners: a Teacher-Student Analysis. (arXiv:2305.11132v1 [stat.ML])\nAbstract: Machine learning models are famously vulnerable to adversarial attacks: small ad-hoc perturbations of the data that can catastrophically alter the model predictions. While a large literature has studied the case of test-time attacks on pre-trained models, the important case of attacks in an online learning setting has received little attention so far. In this work, we use a control-theoretical perspective to study the scenario where an attacker may perturb data labels to manipulate the learning dynamics of an online learner. We perform a theoretical analysis of the problem in a teacher-student setup, considering different attack strategies, and obtaining analytical results for the steady state of simple linear learners. These results enable us to prove that a discontinuous transition in the learner's accuracy occurs when the attack strength exceeds a critical threshold. We then study empirically attacks on learners with complex architectures using real data, confirming the insights of ",
    "path": "papers/23/05/2305.11132.json",
    "total_tokens": 941,
    "translated_title": "在线学习者的攻击：一项教师-学生分析",
    "translated_abstract": "机器学习模型通常容易受到对抗性攻击：数据的微小扰动可能会使模型的预测结果产生灾难性的影响。虽然大量的文献研究了对已经预先训练的模型进行测试时的攻击情况，但在线学习环境下的攻击情况却鲜有研究。本文使用控制理论的视角研究了在线学习者可能存在的标签扰动攻击情况，考虑了不同的攻击策略，并针对简单线性学习器的稳态获得了分析结果。这些结果可以证明，当攻击强度超过临界阈值时，学习器的准确率会出现不连续的转变。然后我们使用真实数据对复杂结构的学习器进行了实证分析，验证了理论分析的洞见并揭示了遭受攻击的学习器的新行为。",
    "tldr": "本文利用控制理论的视角研究了在线学习环境下可能遭受到的标签扰动攻击情况，得出攻击强度超过临界阈值时学习准确率将出现不连续转变的结论，并验证了理论在复杂结构学习器上的适用性。",
    "en_tdlr": "This paper adopts a control-theoretical perspective to study the scenario where an attacker may perturb data labels to manipulate the learning dynamics of an online learner, showing that a discontinuous transition in the learner's accuracy occurs when the attack strength exceeds a critical threshold, and confirming the insights of theoretical analysis on learners with complex architectures using real data."
}