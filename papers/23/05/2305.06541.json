{
    "title": "Spectral Clustering on Large Datasets: When Does it Work? Theory from Continuous Clustering and Density Cheeger-Buser. (arXiv:2305.06541v1 [cs.LG])",
    "abstract": "Spectral clustering is one of the most popular clustering algorithms that has stood the test of time. It is simple to describe, can be implemented using standard linear algebra, and often finds better clusters than traditional clustering algorithms like $k$-means and $k$-centers. The foundational algorithm for two-way spectral clustering, by Shi and Malik, creates a geometric graph from data and finds a spectral cut of the graph.  In modern machine learning, many data sets are modeled as a large number of points drawn from a probability density function. Little is known about when spectral clustering works in this setting -- and when it doesn't. Past researchers justified spectral clustering by appealing to the graph Cheeger inequality (which states that the spectral cut of a graph approximates the ``Normalized Cut''), but this justification is known to break down on large data sets.  We provide theoretically-informed intuition about spectral clustering on large data sets drawn from pr",
    "link": "http://arxiv.org/abs/2305.06541",
    "context": "Title: Spectral Clustering on Large Datasets: When Does it Work? Theory from Continuous Clustering and Density Cheeger-Buser. (arXiv:2305.06541v1 [cs.LG])\nAbstract: Spectral clustering is one of the most popular clustering algorithms that has stood the test of time. It is simple to describe, can be implemented using standard linear algebra, and often finds better clusters than traditional clustering algorithms like $k$-means and $k$-centers. The foundational algorithm for two-way spectral clustering, by Shi and Malik, creates a geometric graph from data and finds a spectral cut of the graph.  In modern machine learning, many data sets are modeled as a large number of points drawn from a probability density function. Little is known about when spectral clustering works in this setting -- and when it doesn't. Past researchers justified spectral clustering by appealing to the graph Cheeger inequality (which states that the spectral cut of a graph approximates the ``Normalized Cut''), but this justification is known to break down on large data sets.  We provide theoretically-informed intuition about spectral clustering on large data sets drawn from pr",
    "path": "papers/23/05/2305.06541.json",
    "total_tokens": 918,
    "translated_title": "大数据集上的谱聚类：在何时有效？来自连续聚类和密度Cheeger-Buser理论的证明。",
    "translated_abstract": "谱聚类是最受欢迎的聚类算法之一，已经经受了时间的考验。它易于描述，可以使用标准的线性代数实现，并且通常比传统聚类算法如K-means和K-centers找到更好的聚类。由Shi和Malik开发的两向谱聚类基础算法从数据中创建几何图形，并找到图形的谱切割。在现代机器学习中，许多数据集被建模为从概率密度函数中提取的大量点。对于这种方式如何进行谱聚类我们知之甚少，过去的研究者通过引用图Cheeger不等式证明了谱聚类的正确性（即表示图谱切割逼近“归一化切割”），但这种证明在大数据集上崩溃了。我们提供了关于在大数据集上进行谱聚类的理论直觉，这些数据集是从概率密度函数中提取的。",
    "tldr": "本文研究了大数据集上的谱聚类问题，提供了关于在大数据集上进行谱聚类的理论直觉，从概率密度函数中提取大量点时的谱聚类问题得到了一定的解决。",
    "en_tdlr": "This paper investigates the problem of spectral clustering on large datasets and provides theoretically informed intuition about spectral clustering on such datasets extracted from probability density functions. It offers a partial solution to the problem of spectral clustering when a large number of points are drawn from such functions."
}