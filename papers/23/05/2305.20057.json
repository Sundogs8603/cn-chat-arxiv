{
    "title": "Three-Way Trade-Off in Multi-Objective Learning: Optimization, Generalization and Conflict-Avoidance. (arXiv:2305.20057v2 [cs.LG] UPDATED)",
    "abstract": "Multi-objective learning (MOL) problems often arise in emerging machine learning problems when there are multiple learning criteria or multiple learning tasks. Recent works have developed various dynamic weighting algorithms for MOL such as MGDA and its variants, where the central idea is to find an update direction that avoids conflicts among objectives. Albeit its appealing intuition, empirical studies show that dynamic weighting methods may not always outperform static ones. To understand this theory-practical gap, we focus on a new stochastic variant of MGDA - the Multi-objective gradient with Double sampling (MoDo) algorithm, and study the generalization performance of the dynamic weighting-based MoDo and its interplay with optimization through the lens of algorithm stability. Perhaps surprisingly, we find that the key rationale behind MGDA -- updating along conflict-avoidant direction - may hinder dynamic weighting algorithms from achieving the optimal ${\\cal O}(1/\\sqrt{n})$ popu",
    "link": "http://arxiv.org/abs/2305.20057",
    "context": "Title: Three-Way Trade-Off in Multi-Objective Learning: Optimization, Generalization and Conflict-Avoidance. (arXiv:2305.20057v2 [cs.LG] UPDATED)\nAbstract: Multi-objective learning (MOL) problems often arise in emerging machine learning problems when there are multiple learning criteria or multiple learning tasks. Recent works have developed various dynamic weighting algorithms for MOL such as MGDA and its variants, where the central idea is to find an update direction that avoids conflicts among objectives. Albeit its appealing intuition, empirical studies show that dynamic weighting methods may not always outperform static ones. To understand this theory-practical gap, we focus on a new stochastic variant of MGDA - the Multi-objective gradient with Double sampling (MoDo) algorithm, and study the generalization performance of the dynamic weighting-based MoDo and its interplay with optimization through the lens of algorithm stability. Perhaps surprisingly, we find that the key rationale behind MGDA -- updating along conflict-avoidant direction - may hinder dynamic weighting algorithms from achieving the optimal ${\\cal O}(1/\\sqrt{n})$ popu",
    "path": "papers/23/05/2305.20057.json",
    "total_tokens": 918,
    "translated_title": "多目标学习中的三重权衡：优化、泛化和冲突避免",
    "translated_abstract": "在新兴的机器学习问题中，当存在多个学习准则或多个学习任务时，多目标学习（MOL）问题经常出现。最近的研究提出了各种动态加权算法用于MOL，如MGDA及其变种，其核心思想是找到一个能够避免目标冲突的更新方向。尽管其直观吸引人，实证研究表明动态加权方法并不总是优于静态方法。为了理解这一理论与实践的差距，我们重点研究了MGDA的新随机变体-多目标梯度双采样（MoDo）算法，并通过算法稳定性的视角研究了基于动态加权的MoDo算法的泛化性能以及其与优化的相互作用。出乎意料的是，我们发现MGDA背后的关键原理-沿着避免冲突的方向进行更新-可能会阻碍动态加权算法实现${\\cal O}(1/\\sqrt{n})$的最优泛化性能。",
    "tldr": "本文研究了多目标学习中的三重权衡问题，通过研究动态加权算法在MoDo算法中的泛化性能和与优化的相互作用，发现了动态加权方法的局限性。",
    "en_tdlr": "This paper explores the trade-off in multi-objective learning, studying the generalization performance and interplay with optimization of dynamic weighting algorithms in the MoDo algorithm, revealing their limitations."
}