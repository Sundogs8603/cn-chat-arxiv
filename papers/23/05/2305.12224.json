{
    "title": "On the Trade-off of Intra-/Inter-class Diversity for Supervised Pre-training. (arXiv:2305.12224v1 [cs.LG])",
    "abstract": "Pre-training datasets are critical for building state-of-the-art machine learning models, motivating rigorous study on their impact on downstream tasks. In this work, we study the impact of the trade-off between the intra-class diversity (the number of samples per class) and the inter-class diversity (the number of classes) of a supervised pre-training dataset. Empirically, we found that with the size of the pre-training dataset fixed, the best downstream performance comes with a balance on the intra-/inter-class diversity. To understand the underlying mechanism, we show theoretically that the downstream performance depends monotonically on both types of diversity. Notably, our theory reveals that the optimal class-to-sample ratio (#classes / #samples per class) is invariant to the size of the pre-training dataset, which motivates an application of predicting the optimal number of pre-training classes. We demonstrate the effectiveness of this application by an improvement of around 2 p",
    "link": "http://arxiv.org/abs/2305.12224",
    "context": "Title: On the Trade-off of Intra-/Inter-class Diversity for Supervised Pre-training. (arXiv:2305.12224v1 [cs.LG])\nAbstract: Pre-training datasets are critical for building state-of-the-art machine learning models, motivating rigorous study on their impact on downstream tasks. In this work, we study the impact of the trade-off between the intra-class diversity (the number of samples per class) and the inter-class diversity (the number of classes) of a supervised pre-training dataset. Empirically, we found that with the size of the pre-training dataset fixed, the best downstream performance comes with a balance on the intra-/inter-class diversity. To understand the underlying mechanism, we show theoretically that the downstream performance depends monotonically on both types of diversity. Notably, our theory reveals that the optimal class-to-sample ratio (#classes / #samples per class) is invariant to the size of the pre-training dataset, which motivates an application of predicting the optimal number of pre-training classes. We demonstrate the effectiveness of this application by an improvement of around 2 p",
    "path": "papers/23/05/2305.12224.json",
    "total_tokens": 989,
    "translated_title": "监督预训练中类内/类间多样性的权衡",
    "translated_abstract": "预训练数据集对于构建最先进的机器学习模型至关重要，因此需要对它们对下游任务的影响进行严格研究。在本文中，我们研究了监督预训练数据集中类内多样性（每个类别的样本数）和类间多样性（类别数）之间的权衡对下游表现的影响。实证表明，当预训练数据集大小固定时，最佳的下游表现取决于类内/类间多样性的平衡。为了了解其基本机制，我们理论上证明了下游表现单调地取决于两种多样性。值得注意的是，我们的理论揭示了最佳的类别样本比（#类别 / #每类样本数）不受预训练数据集大小的影响，这启发我们应用预测最佳的预训练类别数。我们通过实验证明了这种应用的有效性，性能提升约为2个百分点。",
    "tldr": "本文研究了监督预训练数据集中类内多样性和类间多样性之间的权衡对下游任务表现的影响，并理论上证明了下游性能单调地取决于这两种多样性。最佳的类别样本比（#类别 / #每类样本数）与预训练数据集大小无关，可以应用于预测最佳的预训练类别数。",
    "en_tdlr": "This paper studies the trade-off between intra-class and inter-class diversity in supervised pre-training data sets and theoretically proves that downstream performance is monotonically dependent on both types of diversity. The optimal class-to-sample ratio (#classes / #samples per class) is shown to be invariant to the size of the pre-training dataset, which can be used to predict the optimal number of pre-training classes. This application shows an improvement of around 2 percentage points."
}