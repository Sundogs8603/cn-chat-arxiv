{
    "title": "MoCA: Memory-Centric, Adaptive Execution for Multi-Tenant Deep Neural Networks. (arXiv:2305.05843v1 [cs.DC])",
    "abstract": "Driven by the wide adoption of deep neural networks (DNNs) across different application domains, multi-tenancy execution, where multiple DNNs are deployed simultaneously on the same hardware, has been proposed to satisfy the latency requirements of different applications while improving the overall system utilization. However, multi-tenancy execution could lead to undesired system-level resource contention, causing quality-of-service (QoS) degradation for latency-critical applications. To address this challenge, we propose MoCA, an adaptive multi-tenancy system for DNN accelerators. Unlike existing solutions that focus on compute resource partition, MoCA dynamically manages shared memory resources of co-located applications to meet their QoS targets. Specifically, MoCA leverages the regularities in both DNN operators and accelerators to dynamically modulate memory access rates based on their latency targets and user-defined priorities so that co-located applications get the resources t",
    "link": "http://arxiv.org/abs/2305.05843",
    "context": "Title: MoCA: Memory-Centric, Adaptive Execution for Multi-Tenant Deep Neural Networks. (arXiv:2305.05843v1 [cs.DC])\nAbstract: Driven by the wide adoption of deep neural networks (DNNs) across different application domains, multi-tenancy execution, where multiple DNNs are deployed simultaneously on the same hardware, has been proposed to satisfy the latency requirements of different applications while improving the overall system utilization. However, multi-tenancy execution could lead to undesired system-level resource contention, causing quality-of-service (QoS) degradation for latency-critical applications. To address this challenge, we propose MoCA, an adaptive multi-tenancy system for DNN accelerators. Unlike existing solutions that focus on compute resource partition, MoCA dynamically manages shared memory resources of co-located applications to meet their QoS targets. Specifically, MoCA leverages the regularities in both DNN operators and accelerators to dynamically modulate memory access rates based on their latency targets and user-defined priorities so that co-located applications get the resources t",
    "path": "papers/23/05/2305.05843.json",
    "total_tokens": 869,
    "translated_abstract": "受深度神经网络在不同应用领域的广泛应用推动，多租户执行已被提出，在同一硬件上同时部署多个DNN，以满足不同应用程序的延迟要求，同时提高整个系统的利用率。然而，多租户执行可能导致不希望的系统级资源争用，从而导致时延关键应用的服务质量（QoS）降低。为了解决这一挑战，我们提出MoCA，这是一种自适应的DNN加速器多租户系统。与现有解决方案重点放在计算资源分区不同，MoCA动态管理共享内存资源，以满足其QoS目标的相互放置应用程序。具体而言，MoCA利用DNN操作符和加速器的规律，根据它们的延迟目标和用户定义的优先级动态调节内存访问率，以便共存应用获取资源",
    "tldr": "MoCA是一个自适应的DNN加速器多租户系统，它通过动态调节内存访问率来管理共享内存资源，以满足共存应用的QoS目标。",
    "en_tdlr": "MoCA is an adaptive multi-tenancy system for DNN accelerators that manages shared memory resources by dynamically modulating memory access rates based on their latency targets and user-defined priorities to meet co-located applications' QoS targets."
}