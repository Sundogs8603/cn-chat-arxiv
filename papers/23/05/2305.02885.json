{
    "title": "Input Layer Binarization with Bit-Plane Encoding. (arXiv:2305.02885v1 [cs.LG])",
    "abstract": "Binary Neural Networks (BNNs) use 1-bit weights and activations to efficiently execute deep convolutional neural networks on edge devices. Nevertheless, the binarization of the first layer is conventionally excluded, as it leads to a large accuracy loss. The few works addressing the first layer binarization, typically increase the number of input channels to enhance data representation; such data expansion raises the amount of operations needed and it is feasible only on systems with enough computational resources. In this work, we present a new method to binarize the first layer using directly the 8-bit representation of input data; we exploit the standard bit-planes encoding to extract features bit-wise (using depth-wise convolutions); after a re-weighting stage, features are fused again. The resulting model is fully binarized and our first layer binarization approach is model independent. The concept is evaluated on three classification datasets (CIFAR10, SVHN and CIFAR100) for diff",
    "link": "http://arxiv.org/abs/2305.02885",
    "context": "Title: Input Layer Binarization with Bit-Plane Encoding. (arXiv:2305.02885v1 [cs.LG])\nAbstract: Binary Neural Networks (BNNs) use 1-bit weights and activations to efficiently execute deep convolutional neural networks on edge devices. Nevertheless, the binarization of the first layer is conventionally excluded, as it leads to a large accuracy loss. The few works addressing the first layer binarization, typically increase the number of input channels to enhance data representation; such data expansion raises the amount of operations needed and it is feasible only on systems with enough computational resources. In this work, we present a new method to binarize the first layer using directly the 8-bit representation of input data; we exploit the standard bit-planes encoding to extract features bit-wise (using depth-wise convolutions); after a re-weighting stage, features are fused again. The resulting model is fully binarized and our first layer binarization approach is model independent. The concept is evaluated on three classification datasets (CIFAR10, SVHN and CIFAR100) for diff",
    "path": "papers/23/05/2305.02885.json",
    "total_tokens": 830,
    "translated_title": "比特平面编码实现的输入层二值化",
    "translated_abstract": "二值神经网络（BNN）使用1位权重和激活来在边缘设备上高效执行深度卷积神经网络。然而，传统上将第一层二值化的效果不佳，导致精度损失很大。本文提出了一种新的方法，直接使用8位表示输入数据来进行第一层二值化；我们利用标准的比特平面编码按比特的方式提取特征(使用深度卷积); 经过重新加权和特征融合，最终的模型是完全二值化的，我们的第一层二值化方法是与模型无关的。该概念在三个分类数据集（CIFAR10、SVHN和CIFAR100）上进行了评估。",
    "tldr": "该论文提出了一种使用比特平面编码实现的输入层二值化的方法，避免了传统方法中由于数据扩展导致的计算量增加问题，从而实现了完全二值化的模型，并在多个数据集上进行了评估。",
    "en_tdlr": "This paper presents a method for binarizing the first layer of neural networks using bit-plane encoding, which avoids the computation increase caused by data expansion in traditional methods and achieves fully binarized models. The proposed approach is evaluated on multiple datasets and found to be effective."
}