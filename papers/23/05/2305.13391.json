{
    "title": "EnSiam: Self-Supervised Learning With Ensemble Representations. (arXiv:2305.13391v1 [cs.CV])",
    "abstract": "Recently, contrastive self-supervised learning, where the proximity of representations is determined based on the identities of samples, has made remarkable progress in unsupervised representation learning. SimSiam is a well-known example in this area, known for its simplicity yet powerful performance. However, it is known to be sensitive to changes in training configurations, such as hyperparameters and augmentation settings, due to its structural characteristics. To address this issue, we focus on the similarity between contrastive learning and the teacher-student framework in knowledge distillation. Inspired by the ensemble-based knowledge distillation approach, the proposed method, EnSiam, aims to improve the contrastive learning procedure using ensemble representations. This can provide stable pseudo labels, providing better performance. Experiments demonstrate that EnSiam outperforms previous state-of-the-art methods in most cases, including the experiments on ImageNet, which sho",
    "link": "http://arxiv.org/abs/2305.13391",
    "context": "Title: EnSiam: Self-Supervised Learning With Ensemble Representations. (arXiv:2305.13391v1 [cs.CV])\nAbstract: Recently, contrastive self-supervised learning, where the proximity of representations is determined based on the identities of samples, has made remarkable progress in unsupervised representation learning. SimSiam is a well-known example in this area, known for its simplicity yet powerful performance. However, it is known to be sensitive to changes in training configurations, such as hyperparameters and augmentation settings, due to its structural characteristics. To address this issue, we focus on the similarity between contrastive learning and the teacher-student framework in knowledge distillation. Inspired by the ensemble-based knowledge distillation approach, the proposed method, EnSiam, aims to improve the contrastive learning procedure using ensemble representations. This can provide stable pseudo labels, providing better performance. Experiments demonstrate that EnSiam outperforms previous state-of-the-art methods in most cases, including the experiments on ImageNet, which sho",
    "path": "papers/23/05/2305.13391.json",
    "total_tokens": 865,
    "translated_title": "EnSiam: 带有集成表示的自监督学习",
    "translated_abstract": "最近，基于对样本的身份判断来确定表示相似性的对比自监督学习在无监督表示学习中取得了显著进展。SimSiam是该领域的一个著名示例，以其简单但强大的性能而闻名。 然而，由于其结构特征，它对于训练配置（如超参数和增广设置）的变化非常敏感。 为了解决这个问题，我们关注对比学习与知识蒸馏中的师生框架之间的相似性。受集成式知识蒸馏方法启发，提出了一种名为EnSiam的方法，旨在使用集成表示来改进对比学习过程。 这可以提供稳定的伪标签，从而提供更好的性能。 实验证明，EnSiam在大多数情况下都优于以前的最先进方法，包括在ImageNet上的实验。",
    "tldr": "EnSiam提出了一种带有集成表示的自监督学习方法，旨在解决在训练配置改变时SimSiam的性能下降的问题。在大多数实验中，EnSiam的表现优于之前的最先进方法。",
    "en_tdlr": "EnSiam proposes a self-supervised learning method with ensemble representations to address the issue of decreased performance of SimSiam in the face of changes in training configurations. In most experiments, EnSiam outperforms previous state-of-the-art methods."
}