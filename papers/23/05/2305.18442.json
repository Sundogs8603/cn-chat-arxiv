{
    "title": "Improved Projection-free Online Continuous Submodular Maximization. (arXiv:2305.18442v1 [cs.LG])",
    "abstract": "We investigate the problem of online learning with monotone and continuous DR-submodular reward functions, which has received great attention recently. To efficiently handle this problem, especially in the case with complicated decision sets, previous studies have proposed an efficient projection-free algorithm called Mono-Frank-Wolfe (Mono-FW) using $O(T)$ gradient evaluations and linear optimization steps in total. However, it only attains a $(1-1/e)$-regret bound of $O(T^{4/5})$. In this paper, we propose an improved projection-free algorithm, namely POBGA, which reduces the regret bound to $O(T^{3/4})$ while keeping the same computational complexity as Mono-FW. Instead of modifying Mono-FW, our key idea is to make a novel combination of a projection-based algorithm called online boosting gradient ascent, an infeasible projection technique, and a blocking technique. Furthermore, we consider the decentralized setting and develop a variant of POBGA, which not only reduces the current ",
    "link": "http://arxiv.org/abs/2305.18442",
    "context": "Title: Improved Projection-free Online Continuous Submodular Maximization. (arXiv:2305.18442v1 [cs.LG])\nAbstract: We investigate the problem of online learning with monotone and continuous DR-submodular reward functions, which has received great attention recently. To efficiently handle this problem, especially in the case with complicated decision sets, previous studies have proposed an efficient projection-free algorithm called Mono-Frank-Wolfe (Mono-FW) using $O(T)$ gradient evaluations and linear optimization steps in total. However, it only attains a $(1-1/e)$-regret bound of $O(T^{4/5})$. In this paper, we propose an improved projection-free algorithm, namely POBGA, which reduces the regret bound to $O(T^{3/4})$ while keeping the same computational complexity as Mono-FW. Instead of modifying Mono-FW, our key idea is to make a novel combination of a projection-based algorithm called online boosting gradient ascent, an infeasible projection technique, and a blocking technique. Furthermore, we consider the decentralized setting and develop a variant of POBGA, which not only reduces the current ",
    "path": "papers/23/05/2305.18442.json",
    "total_tokens": 1062,
    "translated_title": "改进的无投影在线连续次模最大化算法",
    "translated_abstract": "本文研究了具有单调和连续DR-子模奖励函数的在线学习问题，该问题近年来受到了广泛关注。为了有效地处理这个问题，尤其是在具有复杂决策集的情况下，先前的研究使用了一种名为Mono-Frank-Wolfe（Mono-FW）的高效无投影算法，总共需要 $O(T)$ 梯度评估和线性优化步骤。然而，它只能实现 $O(T^{4/5})$ 的$(1-1/e)$-后悔度上限。本文提出一种改进的无投影算法，即POBGA，该算法可以将后悔度上限降低到 $O(T^{3/4})$，同时保持与Mono-FW相同的计算复杂性。我们的关键想法不是修改Mono-FW，而是将一种称为在线boosting梯度上升的投影算法、一种不可行的投影技术和一种阻塞技术巧妙结合。此外，我们考虑去中心化设置并开发了POBGA的一个变体，它不仅减少了当前代码的计算复杂性，而且在具有低局部性的设置中表现出色。",
    "tldr": "本文提出了一种名为POBGA的改进的无投影算法，该算法可用于在线连续子模最大化问题，具有更低的后悔度上限，同时保持与先前算法相同的计算复杂性。这是通过将投影算法、阻塞技术和在线boosting梯度上升技术相结合实现的。该算法具有分散式实现的变体，适用于低局部性的问题。",
    "en_tdlr": "This paper proposes an improved projection-free algorithm called POBGA for online continuous submodular maximization, which achieves a lower regret bound while maintaining the same computational complexity as previous algorithms. The key idea involves combining a projection-based algorithm, a blocking technique, and online boosting gradient ascent. The algorithm also has a decentralized variant for low locality problems."
}