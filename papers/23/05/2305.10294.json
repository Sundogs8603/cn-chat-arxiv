{
    "title": "DualFL: A Duality-based Federated Learning Algorithm with Communication Acceleration in the General Convex Regime. (arXiv:2305.10294v1 [cs.LG])",
    "abstract": "We propose a novel training algorithm called DualFL (Dualized Federated Learning), for solving a distributed optimization problem in federated learning. Our approach is based on a specific dual formulation of the federated learning problem. DualFL achieves communication acceleration under various settings on smoothness and strong convexity of the problem. Moreover, it theoretically guarantees the use of inexact local solvers, preserving its optimal communication complexity even with inexact local solutions. DualFL is the first federated learning algorithm that achieves communication acceleration, even when the cost function is either nonsmooth or non-strongly convex. Numerical results demonstrate that the practical performance of DualFL is comparable to those of state-of-the-art federated learning algorithms, and it is robust with respect to hyperparameter tuning.",
    "link": "http://arxiv.org/abs/2305.10294",
    "context": "Title: DualFL: A Duality-based Federated Learning Algorithm with Communication Acceleration in the General Convex Regime. (arXiv:2305.10294v1 [cs.LG])\nAbstract: We propose a novel training algorithm called DualFL (Dualized Federated Learning), for solving a distributed optimization problem in federated learning. Our approach is based on a specific dual formulation of the federated learning problem. DualFL achieves communication acceleration under various settings on smoothness and strong convexity of the problem. Moreover, it theoretically guarantees the use of inexact local solvers, preserving its optimal communication complexity even with inexact local solutions. DualFL is the first federated learning algorithm that achieves communication acceleration, even when the cost function is either nonsmooth or non-strongly convex. Numerical results demonstrate that the practical performance of DualFL is comparable to those of state-of-the-art federated learning algorithms, and it is robust with respect to hyperparameter tuning.",
    "path": "papers/23/05/2305.10294.json",
    "total_tokens": 834,
    "translated_title": "DualFL：一种基于对偶的Federated Learning算法及在一般凸情形下加速通讯",
    "translated_abstract": "我们提出了一种名为DualFL（Dualized Federated Learning）的新型训练算法，用于解决联邦学习的分布式优化问题。我们的方法基于联邦学习问题的特定对偶形式。DualFL在不同的光滑性和强凸性设置下实现通讯加速。此外，它在理论上保证使用不精确的本地求解器，即使是使用不精确的本地解决方案，也可以保持其最佳通信复杂度。DualFL是第一个实现通讯加速的联邦学习算法，即使成本函数既非光滑也非强凸，也可以使用。数值结果表明，DualFL的实际性能与最先进的联邦学习算法相当，并且对超参数调整是稳健的。",
    "tldr": "DualFL是一种基于对偶的联邦学习算法，通过具体对偶形式解决分布式优化问题，并保证了即使使用不精确的本地解决方案也可以实现最佳通信复杂度。",
    "en_tdlr": "DualFL is a duality-based federated learning algorithm that provides communication acceleration and optimal communication complexity even with inexact local solutions. It achieves these benefits while still being able to handle non-smooth and non-strongly convex cost functions, making it a robust and comparable option to state-of-the-art federated learning algorithms."
}