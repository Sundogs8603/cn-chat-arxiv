{
    "title": "Constructing Holistic Measures for Social Biases in Masked Language Models. (arXiv:2305.07795v1 [cs.CL])",
    "abstract": "Masked Language Models (MLMs) have been successful in many natural language processing tasks. However, real-world stereotype biases are likely to be reflected in MLMs due to their learning from large text corpora. Most of the evaluation metrics proposed in the past adopt different masking strategies, designed with the log-likelihood of MLMs. They lack holistic considerations such as variance for stereotype bias and anti-stereotype bias samples. In this paper, the log-likelihoods of stereotype bias and anti-stereotype bias samples output by MLMs are considered Gaussian distributions. Two evaluation metrics, Kullback Leibler Divergence Score (KLDivS) and Jensen Shannon Divergence Score (JSDivS) are proposed to evaluate social biases in MLMs The experimental results on the public datasets StereoSet and CrowS-Pairs demonstrate that KLDivS and JSDivS are more stable and interpretable compared to the metrics proposed in the past.",
    "link": "http://arxiv.org/abs/2305.07795",
    "context": "Title: Constructing Holistic Measures for Social Biases in Masked Language Models. (arXiv:2305.07795v1 [cs.CL])\nAbstract: Masked Language Models (MLMs) have been successful in many natural language processing tasks. However, real-world stereotype biases are likely to be reflected in MLMs due to their learning from large text corpora. Most of the evaluation metrics proposed in the past adopt different masking strategies, designed with the log-likelihood of MLMs. They lack holistic considerations such as variance for stereotype bias and anti-stereotype bias samples. In this paper, the log-likelihoods of stereotype bias and anti-stereotype bias samples output by MLMs are considered Gaussian distributions. Two evaluation metrics, Kullback Leibler Divergence Score (KLDivS) and Jensen Shannon Divergence Score (JSDivS) are proposed to evaluate social biases in MLMs The experimental results on the public datasets StereoSet and CrowS-Pairs demonstrate that KLDivS and JSDivS are more stable and interpretable compared to the metrics proposed in the past.",
    "path": "papers/23/05/2305.07795.json",
    "total_tokens": 935,
    "translated_title": "构建掩码语言模型中社会偏见的整体评估指标",
    "translated_abstract": "掩码语言模型（MLMs）在许多自然语言处理任务中取得了成功。然而，由于从大型文本语料库中学习，MLMs 很可能反映现实中的刻板印象偏见。过去提出的大多数评估指标采用不同的掩码策略，设计了MLMs 的对数似然函数。这些指标缺乏对刻板印象和反刻板印象样本变化的考虑。本文将MLMs输出的刻板印象和反刻板印象样本的对数似然函数视为高斯分布，提出了两个评估指标——Kullback Leibler 散度得分（KLDivS）和Jensen Shannon 距离得分（JSDivS），以评估MLMs中的社会偏见。StereoSet 和CrowS-Pairs的公共数据集上的实验结果表明，与过去提出的指标相比，KLDivS和JSDivS更加稳定和可解释。",
    "tldr": "本文提出了KLDivS和JSDivS这两个评估指标，将掩码语言模型输出的刻板印象和反刻板印象样本的对数似然函数视为高斯分布，可以更稳定、可解释地评估MLMs中的社会偏见。",
    "en_tdlr": "This paper proposes two evaluation metrics, KLDivS and JSDivS, which consider the log-likelihoods of stereotype bias and anti-stereotype bias samples output by MLMs as Gaussian distributions, to evaluate social biases in MLMs more stably and interpretably compared to the metrics proposed in the past, as shown in the experimental results on public datasets StereoSet and CrowS-Pairs."
}