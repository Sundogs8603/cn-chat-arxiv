{
    "title": "Optimal Weighted Random Forests. (arXiv:2305.10042v1 [stat.ML])",
    "abstract": "The random forest (RF) algorithm has become a very popular prediction method for its great flexibility and promising accuracy. In RF, it is conventional to put equal weights on all the base learners (trees) to aggregate their predictions. However, the predictive performances of different trees within the forest can be very different due to the randomization of the embedded bootstrap sampling and feature selection. In this paper, we focus on RF for regression and propose two optimal weighting algorithms, namely the 1 Step Optimal Weighted RF (1step-WRF$_\\mathrm{opt}$) and 2 Steps Optimal Weighted RF (2steps-WRF$_\\mathrm{opt}$), that combine the base learners through the weights determined by weight choice criteria. Under some regularity conditions, we show that these algorithms are asymptotically optimal in the sense that the resulting squared loss and risk are asymptotically identical to those of the infeasible but best possible model averaging estimator. Numerical studies conducted on",
    "link": "http://arxiv.org/abs/2305.10042",
    "context": "Title: Optimal Weighted Random Forests. (arXiv:2305.10042v1 [stat.ML])\nAbstract: The random forest (RF) algorithm has become a very popular prediction method for its great flexibility and promising accuracy. In RF, it is conventional to put equal weights on all the base learners (trees) to aggregate their predictions. However, the predictive performances of different trees within the forest can be very different due to the randomization of the embedded bootstrap sampling and feature selection. In this paper, we focus on RF for regression and propose two optimal weighting algorithms, namely the 1 Step Optimal Weighted RF (1step-WRF$_\\mathrm{opt}$) and 2 Steps Optimal Weighted RF (2steps-WRF$_\\mathrm{opt}$), that combine the base learners through the weights determined by weight choice criteria. Under some regularity conditions, we show that these algorithms are asymptotically optimal in the sense that the resulting squared loss and risk are asymptotically identical to those of the infeasible but best possible model averaging estimator. Numerical studies conducted on",
    "path": "papers/23/05/2305.10042.json",
    "total_tokens": 1110,
    "translated_title": "最优加权随机森林",
    "translated_abstract": "该论文介绍了一种针对随机森林 (RF) 算法的优化加权方法，针对回归问题，提出了一步最优加权随机森林 (1step-WRF$_\\mathrm{opt}$) 和两步最优加权随机森林 (2steps-WRF$_\\mathrm{opt}$)，通过加权选择准则来组合基本学习器的预测结果。作者证明了这些算法是渐近最优的，即得到的平方损失和风险与不可行但最佳模型平均估计量的相对差异渐近等同。最后，作者使用数据研究了算法的性能表现。",
    "tldr": "本论文提出了针对RF算法的1步和2步最优加权随机森林算法来处理预测性能差异问题，证明渐近最优，并进行了数据研究。",
    "en_tdlr": "This paper proposes optimal weighted random forests (WRF) algorithms, including 1-step and 2-step WRF to address the issue of varying predictive performance among trees in RF regression. The algorithms are proved asymptotically optimal and evaluated via numerical studies."
}