{
    "title": "Meta-Reinforcement Learning Based on Self-Supervised Task Representation Learning. (arXiv:2305.00286v1 [cs.LG])",
    "abstract": "Meta-reinforcement learning enables artificial agents to learn from related training tasks and adapt to new tasks efficiently with minimal interaction data. However, most existing research is still limited to narrow task distributions that are parametric and stationary, and does not consider out-of-distribution tasks during the evaluation, thus, restricting its application. In this paper, we propose MoSS, a context-based Meta-reinforcement learning algorithm based on Self-Supervised task representation learning to address this challenge. We extend meta-RL to broad non-parametric task distributions which have never been explored before, and also achieve state-of-the-art results in non-stationary and out-of-distribution tasks. Specifically, MoSS consists of a task inference module and a policy module. We utilize the Gaussian mixture model for task representation to imitate the parametric and non-parametric task variations. Additionally, our online adaptation strategy enables the agent to",
    "link": "http://arxiv.org/abs/2305.00286",
    "context": "Title: Meta-Reinforcement Learning Based on Self-Supervised Task Representation Learning. (arXiv:2305.00286v1 [cs.LG])\nAbstract: Meta-reinforcement learning enables artificial agents to learn from related training tasks and adapt to new tasks efficiently with minimal interaction data. However, most existing research is still limited to narrow task distributions that are parametric and stationary, and does not consider out-of-distribution tasks during the evaluation, thus, restricting its application. In this paper, we propose MoSS, a context-based Meta-reinforcement learning algorithm based on Self-Supervised task representation learning to address this challenge. We extend meta-RL to broad non-parametric task distributions which have never been explored before, and also achieve state-of-the-art results in non-stationary and out-of-distribution tasks. Specifically, MoSS consists of a task inference module and a policy module. We utilize the Gaussian mixture model for task representation to imitate the parametric and non-parametric task variations. Additionally, our online adaptation strategy enables the agent to",
    "path": "papers/23/05/2305.00286.json",
    "total_tokens": 1055,
    "translated_title": "基于自监督任务表示学习的元强化学习",
    "translated_abstract": "元强化学习通过学习相关训练任务并最小化交互数据，使人工智能代理能够高效地适应新任务。然而，目前大部分相关研究仍局限于参数化和固定分布的狭窄任务集，并且在评估过程中不考虑任务分布的偏移，这限制了其应用。本文提出了一种基于自监督任务表示学习的上下文元强化学习算法MoSS，以应对这一挑战。该算法扩展了元强化学习到了先前未探索过的广泛非参数化任务分布，同时在非固定和偏移任务方面取得了最先进的结果。",
    "tldr": "该论文提出了一种基于自监督任务表示学习的元强化学习算法MoSS，可使人工智能代理能够适应未探索的任务分布及快速适应新任务。",
    "en_tdlr": "This paper proposes a context-based meta-reinforcement learning algorithm, MoSS, based on self-supervised task representation learning, which can extend meta-RL to broad non-parametric task distributions and achieve state-of-the-art results in non-stationary and out-of-distribution tasks."
}