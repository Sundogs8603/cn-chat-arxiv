{
    "title": "Teaching Probabilistic Logical Reasoning to Transformers",
    "abstract": "In this paper, we evaluate the capability of transformer-based language models in making inferences over uncertain text that includes uncertain rules of reasoning. We cover both Pre-trained Language Models (PLMs) and generative Large Language Models (LLMs). Our evaluation results show that both generations of language models struggle with reasoning over uncertain text. We propose a novel end-to-end fine-tuning approach, Probabilistic Constraint Training (PCT), that utilizes probabilistic logical rules as constraints in the fine-tuning phase without relying on these rules in the inference stage. To assess the effectiveness of PCT, we utilize the related corpora and, additionally, create a new and more challenging benchmark that, unlike the previous ones, uses instance-specific rules. Our study demonstrates that PCT improves the transformer-based language model's intrinsic reasoning and makes their probabilistic logical reasoning process more explicit and explainable. Furthermore, PCT eq",
    "link": "https://arxiv.org/abs/2305.13179",
    "context": "Title: Teaching Probabilistic Logical Reasoning to Transformers\nAbstract: In this paper, we evaluate the capability of transformer-based language models in making inferences over uncertain text that includes uncertain rules of reasoning. We cover both Pre-trained Language Models (PLMs) and generative Large Language Models (LLMs). Our evaluation results show that both generations of language models struggle with reasoning over uncertain text. We propose a novel end-to-end fine-tuning approach, Probabilistic Constraint Training (PCT), that utilizes probabilistic logical rules as constraints in the fine-tuning phase without relying on these rules in the inference stage. To assess the effectiveness of PCT, we utilize the related corpora and, additionally, create a new and more challenging benchmark that, unlike the previous ones, uses instance-specific rules. Our study demonstrates that PCT improves the transformer-based language model's intrinsic reasoning and makes their probabilistic logical reasoning process more explicit and explainable. Furthermore, PCT eq",
    "path": "papers/23/05/2305.13179.json",
    "total_tokens": 863,
    "translated_title": "将概率逻辑推理教给变压器",
    "translated_abstract": "在本文中，我们评估了基于变压器的语言模型在推理不确定的文本上的能力，其中包括不确定的推理规则。我们涵盖了预训练语言模型（PLMs）和生成大型语言模型（LLMs）。我们的评估结果表明，这两代语言模型在推理不确定文本方面都存在困难。我们提出了一种新颖的端到端微调方法，概率约束训练（PCT），它在微调阶段利用概率逻辑规则作为约束，而不依赖这些规则在推理阶段。为了评估PCT的有效性，我们利用相关语料库，并额外创建了一个更具挑战性的基准测试，与之前的测试不同，它使用了实例特定的规则。我们的研究表明，PCT提高了基于变压器的语言模型的内在推理能力，使其概率逻辑推理过程更明确和可解释。此外，PCT与传统方法相比，在不确定的文本上取得了更好的性能。",
    "tldr": "本文评估了基于变压器的语言模型在推理不确定文本上的能力，并提出了一种概率约束训练（PCT）的方法来提高模型的概率逻辑推理能力。"
}