{
    "title": "Contrastive Language-Image Pretrained Models are Zero-Shot Human Scanpath Predictors. (arXiv:2305.12380v2 [cs.CV] UPDATED)",
    "abstract": "Understanding the mechanisms underlying human attention is a fundamental challenge for both vision science and artificial intelligence. While numerous computational models of free-viewing have been proposed, less is known about the mechanisms underlying task-driven image exploration. To address this gap, we present CapMIT1003, a database of captions and click-contingent image explorations collected during captioning tasks. CapMIT1003 is based on the same stimuli from the well-known MIT1003 benchmark, for which eye-tracking data under free-viewing conditions is available, which offers a promising opportunity to concurrently study human attention under both tasks. We make this dataset publicly available to facilitate future research in this field. In addition, we introduce NevaClip, a novel zero-shot method for predicting visual scanpaths that combines contrastive language-image pretrained (CLIP) models with biologically-inspired neural visual attention (NeVA) algorithms. NevaClip simula",
    "link": "http://arxiv.org/abs/2305.12380",
    "context": "Title: Contrastive Language-Image Pretrained Models are Zero-Shot Human Scanpath Predictors. (arXiv:2305.12380v2 [cs.CV] UPDATED)\nAbstract: Understanding the mechanisms underlying human attention is a fundamental challenge for both vision science and artificial intelligence. While numerous computational models of free-viewing have been proposed, less is known about the mechanisms underlying task-driven image exploration. To address this gap, we present CapMIT1003, a database of captions and click-contingent image explorations collected during captioning tasks. CapMIT1003 is based on the same stimuli from the well-known MIT1003 benchmark, for which eye-tracking data under free-viewing conditions is available, which offers a promising opportunity to concurrently study human attention under both tasks. We make this dataset publicly available to facilitate future research in this field. In addition, we introduce NevaClip, a novel zero-shot method for predicting visual scanpaths that combines contrastive language-image pretrained (CLIP) models with biologically-inspired neural visual attention (NeVA) algorithms. NevaClip simula",
    "path": "papers/23/05/2305.12380.json",
    "total_tokens": 783,
    "translated_abstract": "理解人类注意机制是视觉科学和人工智能的基本挑战。针对任务驱动的图像探索机制研究较少。本文提出了CapMIT1003数据库，包含了字幕和点击依赖的图像探索数据集。为了让研究更加便捷，我们公开了该数据集。此外，我们提出了一种新的零样本方法NevaClip，将对比学习语言-图像预训练模型与生物启发式神经视觉注意力算法（NeVA）相结合，用于预测视觉注视路径。",
    "tldr": "本文提出了CapMIT1003数据库，包含了任务驱动的图像探索数据集并公开了该数据集。同时，我们还提出了零样本方法NevaClip，用于预测视觉注视路径。",
    "en_tdlr": "This paper proposes CapMIT1003, a database of caption and click-contingent image explorations collected during captioning tasks, and introduces NevaClip, a novel zero-shot method for predicting visual scanpaths. The author also makes the database publicly available for further research."
}