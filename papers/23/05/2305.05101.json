{
    "title": "Towards unraveling calibration biases in medical image analysis. (arXiv:2305.05101v1 [eess.IV])",
    "abstract": "In recent years the development of artificial intelligence (AI) systems for automated medical image analysis has gained enormous momentum. At the same time, a large body of work has shown that AI systems can systematically and unfairly discriminate against certain populations in various application scenarios. These two facts have motivated the emergence of algorithmic fairness studies in this field. Most research on healthcare algorithmic fairness to date has focused on the assessment of biases in terms of classical discrimination metrics such as AUC and accuracy. Potential biases in terms of model calibration, however, have only recently begun to be evaluated. This is especially important when working with clinical decision support systems, as predictive uncertainty is key for health professionals to optimally evaluate and combine multiple sources of information. In this work we study discrimination and calibration biases in models trained for automatic detection of malignant dermatol",
    "link": "http://arxiv.org/abs/2305.05101",
    "context": "Title: Towards unraveling calibration biases in medical image analysis. (arXiv:2305.05101v1 [eess.IV])\nAbstract: In recent years the development of artificial intelligence (AI) systems for automated medical image analysis has gained enormous momentum. At the same time, a large body of work has shown that AI systems can systematically and unfairly discriminate against certain populations in various application scenarios. These two facts have motivated the emergence of algorithmic fairness studies in this field. Most research on healthcare algorithmic fairness to date has focused on the assessment of biases in terms of classical discrimination metrics such as AUC and accuracy. Potential biases in terms of model calibration, however, have only recently begun to be evaluated. This is especially important when working with clinical decision support systems, as predictive uncertainty is key for health professionals to optimally evaluate and combine multiple sources of information. In this work we study discrimination and calibration biases in models trained for automatic detection of malignant dermatol",
    "path": "papers/23/05/2305.05101.json",
    "total_tokens": 916,
    "translated_title": "旨在揭示医学图像分析中的校准偏差",
    "translated_abstract": "近年来，人工智能系统在医学图像分析领域的自动化应用取得了巨大发展。与此同时，大量研究表明，在各种应用场景中，人工智能系统可能存在系统性和不公平的对某些人群的歧视。这两个事实促使算法公平性研究在这个领域的出现。迄今为止，关于医疗算法公平性的大部分研究侧重于以经典的歧视指标例如AUC和准确度来评估偏差。然而，模型校准方面的潜在偏差只是最近开始得到评估。这在使用临床决策支持系统时尤为重要，因为预测不确定性是医疗专业人员优化评估和结合多个信息来源的关键。本工作研究了自动检测恶性皮肤病变的模型中的歧视和校准偏差。",
    "tldr": "本研究旨在解决医学图像分析中AI系统的校准偏差问题，即模型预测与实际数据不符合的问题。目前，大部分关于医疗算法公平性的研究都侧重于歧视偏差的评估，而校准偏差的评估仍然不足。",
    "en_tdlr": "This study aims to address the calibration biases in AI systems for medical image analysis, which refers to the discrepancy between predicted outcomes and actual outcomes. Most current research on healthcare algorithmic fairness focuses on assessing discrimination biases, while evaluation of calibration biases is still insufficient."
}