{
    "title": "Make a Choice! Knowledge Base Question Answering with In-Context Learning. (arXiv:2305.13972v1 [cs.CL])",
    "abstract": "Question answering over knowledge bases (KBQA) aims to answer factoid questions with a given knowledge base (KB). Due to the large scale of KB, annotated data is impossible to cover all fact schemas in KB, which poses a challenge to the generalization ability of methods that require a sufficient amount of annotated data. Recently, LLMs have shown strong few-shot performance in many NLP tasks. We expect LLM can help existing methods improve their generalization ability, especially in low-resource situations. In this paper, we present McL-KBQA, a framework that incorporates the few-shot ability of LLM into the KBQA method via ICL-based multiple choice and then improves the effectiveness of the QA tasks. Experimental results on two KBQA datasets demonstrate the competitive performance of McL-KBQA with strong improvements in generalization. We expect to explore a new way to QA tasks from KBQA in conjunction with LLM, how to generate answers normatively and correctly with strong generalizat",
    "link": "http://arxiv.org/abs/2305.13972",
    "context": "Title: Make a Choice! Knowledge Base Question Answering with In-Context Learning. (arXiv:2305.13972v1 [cs.CL])\nAbstract: Question answering over knowledge bases (KBQA) aims to answer factoid questions with a given knowledge base (KB). Due to the large scale of KB, annotated data is impossible to cover all fact schemas in KB, which poses a challenge to the generalization ability of methods that require a sufficient amount of annotated data. Recently, LLMs have shown strong few-shot performance in many NLP tasks. We expect LLM can help existing methods improve their generalization ability, especially in low-resource situations. In this paper, we present McL-KBQA, a framework that incorporates the few-shot ability of LLM into the KBQA method via ICL-based multiple choice and then improves the effectiveness of the QA tasks. Experimental results on two KBQA datasets demonstrate the competitive performance of McL-KBQA with strong improvements in generalization. We expect to explore a new way to QA tasks from KBQA in conjunction with LLM, how to generate answers normatively and correctly with strong generalizat",
    "path": "papers/23/05/2305.13972.json",
    "total_tokens": 910,
    "translated_title": "做出选择！基于上下文学习的知识库问答",
    "translated_abstract": "知识库问答（KBQA）旨在利用给定的知识库（KB）回答事实类问题。由于KB的大规模，注释数据无法涵盖KB中的所有事实模式，这也给需要足够注释数据的方法的概括能力带来了挑战。最近，LLMs在许多NLP任务中表现出了强大的少样本性能。我们期望LLMs可以帮助现有方法提高它们的概括能力，特别是在资源匮乏的情况下。在本文中，我们提出了McL-KBQA，这是一个通过基于上下文学习的多项选择将LLMs的一些样本能力纳入KBQA方法并提高QA任务效果的框架。在两个KBQA数据集上的实验结果表明，McL-KBQA表现竞争力强，概括能力得到了显著提高。我们希望探索一种结合LLMs的方法来解决KBQA中的QA任务，如何使回答规范正确且概括能力强。",
    "tldr": "McL-KBQA是一个新的框架，它通过基于上下文学习的多项选择将LLMs的一些样本能力纳入KBQA方法，从而显著提高了概括能力，有效性和效果。",
    "en_tdlr": "McL-KBQA is a new framework that incorporates the few-shot abilities of LLMs into the KBQA method via in-context learning-based multiple choice, significantly improving generalization, effectiveness, and overall performance."
}