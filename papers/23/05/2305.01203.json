{
    "title": "Optimizing Guided Traversal for Fast Learned Sparse Retrieval. (arXiv:2305.01203v1 [cs.IR])",
    "abstract": "Recent studies show that BM25-driven dynamic index skipping can greatly accelerate MaxScore-based document retrieval based on the learned sparse representation derived by DeepImpact. This paper investigates the effectiveness of such a traversal guidance strategy during top k retrieval when using other models such as SPLADE and uniCOIL, and finds that unconstrained BM25-driven skipping could have a visible relevance degradation when the BM25 model is not well aligned with a learned weight model or when retrieval depth k is small. This paper generalizes the previous work and optimizes the BM25 guided index traversal with a two-level pruning control scheme and model alignment for fast retrieval using a sparse representation. Although there can be a cost of increased latency, the proposed scheme is much faster than the original MaxScore method without BM25 guidance while retaining the relevance effectiveness. This paper analyzes the competitiveness of this two-level pruning scheme, and eva",
    "link": "http://arxiv.org/abs/2305.01203",
    "context": "Title: Optimizing Guided Traversal for Fast Learned Sparse Retrieval. (arXiv:2305.01203v1 [cs.IR])\nAbstract: Recent studies show that BM25-driven dynamic index skipping can greatly accelerate MaxScore-based document retrieval based on the learned sparse representation derived by DeepImpact. This paper investigates the effectiveness of such a traversal guidance strategy during top k retrieval when using other models such as SPLADE and uniCOIL, and finds that unconstrained BM25-driven skipping could have a visible relevance degradation when the BM25 model is not well aligned with a learned weight model or when retrieval depth k is small. This paper generalizes the previous work and optimizes the BM25 guided index traversal with a two-level pruning control scheme and model alignment for fast retrieval using a sparse representation. Although there can be a cost of increased latency, the proposed scheme is much faster than the original MaxScore method without BM25 guidance while retaining the relevance effectiveness. This paper analyzes the competitiveness of this two-level pruning scheme, and eva",
    "path": "papers/23/05/2305.01203.json",
    "total_tokens": 880,
    "translated_title": "优化引导遍历以实现快速学习稀疏检索",
    "translated_abstract": "最近的研究表明，基于由DeepImpact生成的学习稀疏表示的MaxScore文档检索可以通过BM25驱动的动态索引跳过大大加快。本文研究了在使用其他模型（如SPLADE和uniCOIL）进行top k检索时使用这种遍历引导策略的有效性，并发现当BM25模型与学习权重模型不完全对齐或检索深度k很小时，无约束的BM25 driven跳过可能会导致显著的相关性降低。本文通过二级剪枝控制方案和模型对齐优化了BM25引导的索引遍历，以使用稀疏表示快速检索。虽然可能会增加延迟成本，但拟议的方案比没有BM25引导的原始MaxScore方法快得多，同时保留相关性的有效性。本文分析了这个两级剪枝方案的竞争力，并评估了其在几个现实数据集上的有效性。",
    "tldr": "本文通过优化BM25引导的索引遍历，提出了一个使用稀疏表示进行快速学习稀疏检索的解决方案。这种方案比原始MaxScore方法快得多，同时保留相关性的有效性。",
    "en_tdlr": "This paper proposes a solution for fast learned sparse retrieval using sparse representation by optimizing BM25-guided index traversal. The proposed scheme is much faster than the original MaxScore method without BM25 guidance while maintaining relevance effectiveness."
}