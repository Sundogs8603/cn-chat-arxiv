{
    "title": "Escaping mediocrity: how two-layer networks learn hard single-index models with SGD. (arXiv:2305.18502v1 [stat.ML])",
    "abstract": "This study explores the sample complexity for two-layer neural networks to learn a single-index target function under Stochastic Gradient Descent (SGD), focusing on the challenging regime where many flat directions are present at initialization. It is well-established that in this scenario $n=O(d\\log{d})$ samples are typically needed. However, we provide precise results concerning the pre-factors in high-dimensional contexts and for varying widths. Notably, our findings suggest that overparameterization can only enhance convergence by a constant factor within this problem class. These insights are grounded in the reduction of SGD dynamics to a stochastic process in lower dimensions, where escaping mediocrity equates to calculating an exit time. Yet, we demonstrate that a deterministic approximation of this process adequately represents the escape time, implying that the role of stochasticity may be minimal in this scenario.",
    "link": "http://arxiv.org/abs/2305.18502",
    "context": "Title: Escaping mediocrity: how two-layer networks learn hard single-index models with SGD. (arXiv:2305.18502v1 [stat.ML])\nAbstract: This study explores the sample complexity for two-layer neural networks to learn a single-index target function under Stochastic Gradient Descent (SGD), focusing on the challenging regime where many flat directions are present at initialization. It is well-established that in this scenario $n=O(d\\log{d})$ samples are typically needed. However, we provide precise results concerning the pre-factors in high-dimensional contexts and for varying widths. Notably, our findings suggest that overparameterization can only enhance convergence by a constant factor within this problem class. These insights are grounded in the reduction of SGD dynamics to a stochastic process in lower dimensions, where escaping mediocrity equates to calculating an exit time. Yet, we demonstrate that a deterministic approximation of this process adequately represents the escape time, implying that the role of stochasticity may be minimal in this scenario.",
    "path": "papers/23/05/2305.18502.json",
    "total_tokens": 923,
    "translated_title": "逃离平庸：双层神经网络如何在 SGD 下学习困难的单指标模型",
    "translated_abstract": "本研究探讨了在随机梯度下降（SGD）下双层神经网络学习单指数目标函数的样本复杂度问题，重点关注在初始化时存在许多平坦方向的挑战性情况。已经有研究表明，这种情况下通常需要 $n=O(d\\log{d})$ 个样本。但是，我们提供了在高维度和不同宽度情况下的前置因子的精确结果。值得注意的是，我们的发现表明，在这个问题类中，过参数化只会增加一定因子的收敛性。这些见解基于 SGD 动态的低维度随机过程模型，其中逃离平庸等同于计算出站出时间。然而，我们证明这个过程的确定性近似足以代表逃逸时间，这意味着在这种情况下随机性的作用可能很小。",
    "tldr": "研究探讨在 SGD 下双层神经网络学习单指数目标函数的样本复杂度问题，发现过参数化只会增加一定因子的收敛性，不同维度和宽度的前置因子精确结果揭示。",
    "en_tdlr": "This study explores the sample complexity for two-layer neural networks to learn a single-index target function under SGD, and found that overparameterization can only enhance convergence by a constant factor within this problem class. Precise results concerning the pre-factors in high-dimensional contexts and for varying widths are provided. Our insights are grounded in the reduction of SGD dynamics to a stochastic process in lower dimensions, where escaping mediocrity equates to calculating an exit time."
}