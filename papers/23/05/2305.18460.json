{
    "title": "Minimum Width of Leaky-ReLU Neural Networks for Uniform Universal Approximation. (arXiv:2305.18460v1 [cs.LG])",
    "abstract": "The study of universal approximation properties (UAP) for neural networks (NN) has a long history. When the network width is unlimited, only a single hidden layer is sufficient for UAP. In contrast, when the depth is unlimited, the width for UAP needs to be not less than the critical width $w^*_{\\min}=\\max(d_x,d_y)$, where $d_x$ and $d_y$ are the dimensions of the input and output, respectively. Recently, \\cite{cai2022achieve} shows that a leaky-ReLU NN with this critical width can achieve UAP for $L^p$ functions on a compact domain $K$, \\emph{i.e.,} the UAP for $L^p(K,\\mathbb{R}^{d_y})$. This paper examines a uniform UAP for the function class $C(K,\\mathbb{R}^{d_y})$ and gives the exact minimum width of the leaky-ReLU NN as $w_{\\min}=\\max(d_x+1,d_y)+1_{d_y=d_x+1}$, which involves the effects of the output dimensions. To obtain this result, we propose a novel lift-flow-discretization approach that shows that the uniform UAP has a deep connection with topological theory.",
    "link": "http://arxiv.org/abs/2305.18460",
    "context": "Title: Minimum Width of Leaky-ReLU Neural Networks for Uniform Universal Approximation. (arXiv:2305.18460v1 [cs.LG])\nAbstract: The study of universal approximation properties (UAP) for neural networks (NN) has a long history. When the network width is unlimited, only a single hidden layer is sufficient for UAP. In contrast, when the depth is unlimited, the width for UAP needs to be not less than the critical width $w^*_{\\min}=\\max(d_x,d_y)$, where $d_x$ and $d_y$ are the dimensions of the input and output, respectively. Recently, \\cite{cai2022achieve} shows that a leaky-ReLU NN with this critical width can achieve UAP for $L^p$ functions on a compact domain $K$, \\emph{i.e.,} the UAP for $L^p(K,\\mathbb{R}^{d_y})$. This paper examines a uniform UAP for the function class $C(K,\\mathbb{R}^{d_y})$ and gives the exact minimum width of the leaky-ReLU NN as $w_{\\min}=\\max(d_x+1,d_y)+1_{d_y=d_x+1}$, which involves the effects of the output dimensions. To obtain this result, we propose a novel lift-flow-discretization approach that shows that the uniform UAP has a deep connection with topological theory.",
    "path": "papers/23/05/2305.18460.json",
    "total_tokens": 1134,
    "translated_title": "Leaky-ReLU神经网络在均匀通用逼近中的最小宽度研究",
    "translated_abstract": "对神经网络的通用逼近性质（UAP）的研究历史悠久。当网络宽度不受限制时，只需要一个隐藏层即可进行UAP。相反，当深度不受限制时，UAP的宽度需要不小于临界宽度$w^*_{\\min}=\\max(d_x,d_y)$, 其中$d_x$和$d_y$分别是输入和输出的维度。最近，\\cite{cai2022achieve}表明，具有这种临界宽度的Leaky-ReLU神经网络可以在紧致域$K$上实现$L^p$函数的UAP，即$L^p(K,\\mathbb{R}^{d_y})$的UAP。本文研究了函数类$C(K,\\mathbb{R}^{d_y})$的均匀UAP，并给出了Leaky-ReLU NN的确切最小宽度，为$w_{\\min}=\\max(d_x+1,d_y)+1_{d_y=d_x+1}$，其中涉及输出维度的影响。为了得到这个结果，我们提出了一种新的lift-flow-discretization方法，证明了均匀UAP与拓扑理论有着深刻的联系。",
    "tldr": "研究表明具有临界宽度的Leaky-ReLU神经网络可以在紧致域K上实现$L^p(K,\\mathbb{R}^{d_y})$的UAP，而本文给出的最小宽度$w_{\\min}=\\max(d_x+1,d_y)+1_{d_y=d_x+1}$则适用于函数类$C(K,\\mathbb{R}^{d_y})$，考虑到输出维度的影响。",
    "en_tdlr": "The study shows that a Leaky-ReLU neural network with critical width can achieve UAP for $L^p(K,\\mathbb{R}^{d_y})$ functions on a compact domain K, while the exact minimum width $w_{\\min}=\\max(d_x+1,d_y)+1_{d_y=d_x+1}$ given in this paper is suitable for the function class $C(K,\\mathbb{R}^{d_y})$, considering the influence of the output dimensions."
}