{
    "title": "Token-Level Fitting Issues of Seq2seq Models. (arXiv:2305.04493v2 [cs.CL] UPDATED)",
    "abstract": "Sequence-to-sequence (seq2seq) models have been widely used for natural language processing, computer vision, and other deep learning tasks. We find that seq2seq models trained with early-stopping suffer from issues at the token level. In particular, while some tokens in the vocabulary demonstrate overfitting, others underfit when training is stopped. Experiments show that the phenomena are pervasive in different models, even in fine-tuned large pretrained-models. We identify three major factors that influence token-level fitting, which include token frequency, parts-of-speech, and prediction discrepancy. Further, we find that external factors such as language, model size, domain, data scale, and pretraining can also influence the fitting of tokens.",
    "link": "http://arxiv.org/abs/2305.04493",
    "context": "Title: Token-Level Fitting Issues of Seq2seq Models. (arXiv:2305.04493v2 [cs.CL] UPDATED)\nAbstract: Sequence-to-sequence (seq2seq) models have been widely used for natural language processing, computer vision, and other deep learning tasks. We find that seq2seq models trained with early-stopping suffer from issues at the token level. In particular, while some tokens in the vocabulary demonstrate overfitting, others underfit when training is stopped. Experiments show that the phenomena are pervasive in different models, even in fine-tuned large pretrained-models. We identify three major factors that influence token-level fitting, which include token frequency, parts-of-speech, and prediction discrepancy. Further, we find that external factors such as language, model size, domain, data scale, and pretraining can also influence the fitting of tokens.",
    "path": "papers/23/05/2305.04493.json",
    "total_tokens": 790,
    "translated_title": "Seq2seq模型的Token级拟合问题",
    "translated_abstract": "序列到序列（seq2seq）模型已广泛用于自然语言处理、计算机视觉和其他深度学习任务。我们发现，使用early-stopping训练的seq2seq模型在token级别存在问题。特别是，虽然词汇表中的某些token表现出过拟合，但当训练停止时，其他token则表现出欠拟合。实验表明，即使在 fine-tune 的大型预训练模型中，这种现象也很普遍。我们确定了影响Token级别拟合的三个主要因素，包括Token频率、词性和预测差异。此外，我们发现，语言、模型大小、领域、数据规模和预训练等外部因素也可以影响Token的拟合情况。",
    "tldr": "研究发现使用early-stopping训练的seq2seq模型在token级别存在拟合问题，影响因素包括Token频率、词性和预测差异以及外部因素如语言、模型大小、领域、数据规模和预训练等。",
    "en_tdlr": "This paper identifies token-level fitting issues of seq2seq models trained with early-stopping, which can be influenced by factors such as token frequency, parts-of-speech, prediction discrepancy, language, model size, domain, data scale, and pretraining."
}