{
    "title": "RNNS: Representation Nearest Neighbor Search Black-Box Attack on Code Models. (arXiv:2305.05896v1 [cs.CR])",
    "abstract": "Pre-trained code models are mainly evaluated using the in-distribution test data. The robustness of models, i.e., the ability to handle hard unseen data, still lacks evaluation. In this paper, we propose a novel search-based black-box adversarial attack guided by model behaviours for pre-trained programming language models, named Representation Nearest Neighbor Search(RNNS), to evaluate the robustness of Pre-trained PL models. Unlike other black-box adversarial attacks, RNNS uses the model-change signal to guide the search in the space of the variable names collected from real-world projects. Specifically, RNNS contains two main steps, 1) indicate which variable (attack position location) we should attack based on model uncertainty, and 2) search which adversarial tokens we should use for variable renaming according to the model behaviour observations. We evaluate RNNS on 6 code tasks (e.g., clone detection), 3 programming languages (Java, Python, and C), and 3 pre-trained code models:",
    "link": "http://arxiv.org/abs/2305.05896",
    "context": "Title: RNNS: Representation Nearest Neighbor Search Black-Box Attack on Code Models. (arXiv:2305.05896v1 [cs.CR])\nAbstract: Pre-trained code models are mainly evaluated using the in-distribution test data. The robustness of models, i.e., the ability to handle hard unseen data, still lacks evaluation. In this paper, we propose a novel search-based black-box adversarial attack guided by model behaviours for pre-trained programming language models, named Representation Nearest Neighbor Search(RNNS), to evaluate the robustness of Pre-trained PL models. Unlike other black-box adversarial attacks, RNNS uses the model-change signal to guide the search in the space of the variable names collected from real-world projects. Specifically, RNNS contains two main steps, 1) indicate which variable (attack position location) we should attack based on model uncertainty, and 2) search which adversarial tokens we should use for variable renaming according to the model behaviour observations. We evaluate RNNS on 6 code tasks (e.g., clone detection), 3 programming languages (Java, Python, and C), and 3 pre-trained code models:",
    "path": "papers/23/05/2305.05896.json",
    "total_tokens": 1073,
    "tldr": "提出了一种新颖的黑盒对抗攻击方法RNNS，用于评估预训练编程语言模型的鲁棒性。RNNS使用模型改变信号引导变量名空间中的搜索，具有两个主要步骤：1）根据模型不确定性指示应攻击哪个变量，2）根据模型行为观察，搜索应使用哪些对抗性标记进行变量重命名。在多个代码任务、编程语言和预训练代码模型上测试表明RNNS攻击效果显著。"
}