{
    "title": "Decision-Aware Actor-Critic with Function Approximation and Theoretical Guarantees. (arXiv:2305.15249v2 [cs.LG] UPDATED)",
    "abstract": "Actor-critic (AC) methods are widely used in reinforcement learning (RL) and benefit from the flexibility of using any policy gradient method as the actor and value-based method as the critic. The critic is usually trained by minimizing the TD error, an objective that is potentially decorrelated with the true goal of achieving a high reward with the actor. We address this mismatch by designing a joint objective for training the actor and critic in a decision-aware fashion. We use the proposed objective to design a generic, AC algorithm that can easily handle any function approximation. We explicitly characterize the conditions under which the resulting algorithm guarantees monotonic policy improvement, regardless of the choice of the policy and critic parameterization. Instantiating the generic algorithm results in an actor that involves maximizing a sequence of surrogate functions (similar to TRPO, PPO) and a critic that involves minimizing a closely connected objective. Using simple ",
    "link": "http://arxiv.org/abs/2305.15249",
    "context": "Title: Decision-Aware Actor-Critic with Function Approximation and Theoretical Guarantees. (arXiv:2305.15249v2 [cs.LG] UPDATED)\nAbstract: Actor-critic (AC) methods are widely used in reinforcement learning (RL) and benefit from the flexibility of using any policy gradient method as the actor and value-based method as the critic. The critic is usually trained by minimizing the TD error, an objective that is potentially decorrelated with the true goal of achieving a high reward with the actor. We address this mismatch by designing a joint objective for training the actor and critic in a decision-aware fashion. We use the proposed objective to design a generic, AC algorithm that can easily handle any function approximation. We explicitly characterize the conditions under which the resulting algorithm guarantees monotonic policy improvement, regardless of the choice of the policy and critic parameterization. Instantiating the generic algorithm results in an actor that involves maximizing a sequence of surrogate functions (similar to TRPO, PPO) and a critic that involves minimizing a closely connected objective. Using simple ",
    "path": "papers/23/05/2305.15249.json",
    "total_tokens": 910,
    "translated_title": "具有函数逼近和理论保证的决策感知演员-评论家算法",
    "translated_abstract": "演员评论家 (AC) 方法广泛应用于强化学习 (RL) 中，并从使用任何策略梯度方法作为演员和基于值方法作为评论家的灵活性中受益。评论家通常通过最小化 TD 误差来训练，这是一个与实现高奖励的真实目标可能脱钩的客观标准。我们通过设计一个决策感知的联合目标来解决这种不匹配。我们使用提出的目标来设计一个通用的 AC 算法，可以轻松处理任何函数逼近。我们明确表征了在选择策略和评论家参数化的情况下，所得算法保证单调策略改进的条件。实例化通用算法将导致涉及最大化一系列替代函数 (类似于 TRPO、PPO) 的演员和涉及最小化一个密切相关目标的评论家。使用简单的方法加速了证明的过程，同时还引入了新的研究方向。",
    "tldr": "提出了一种具有函数逼近和理论保证的决策感知演员-评论家算法，通过设计联合目标来解决演员和评论家之间的不匹配，并且无论策略和评论家参数化的选择如何，该算法都保证单调策略改进。",
    "en_tdlr": "A decision-aware actor-critic algorithm with function approximation and theoretical guarantees is proposed, addressing the mismatch between the actor and critic by designing a joint objective. The resulting algorithm guarantees monotonic policy improvement regardless of the choice of policy and critic parameterization."
}