{
    "title": "Estimation Beyond Data Reweighting: Kernel Method of Moments. (arXiv:2305.10898v1 [cs.LG])",
    "abstract": "Moment restrictions and their conditional counterparts emerge in many areas of machine learning and statistics ranging from causal inference to reinforcement learning. Estimators for these tasks, generally called methods of moments, include the prominent generalized method of moments (GMM) which has recently gained attention in causal inference. GMM is a special case of the broader family of empirical likelihood estimators which are based on approximating a population distribution by means of minimizing a $\\varphi$-divergence to an empirical distribution. However, the use of $\\varphi$-divergences effectively limits the candidate distributions to reweightings of the data samples. We lift this long-standing limitation and provide a method of moments that goes beyond data reweighting. This is achieved by defining an empirical likelihood estimator based on maximum mean discrepancy which we term the kernel method of moments (KMM). We provide a variant of our estimator for conditional moment",
    "link": "http://arxiv.org/abs/2305.10898",
    "context": "Title: Estimation Beyond Data Reweighting: Kernel Method of Moments. (arXiv:2305.10898v1 [cs.LG])\nAbstract: Moment restrictions and their conditional counterparts emerge in many areas of machine learning and statistics ranging from causal inference to reinforcement learning. Estimators for these tasks, generally called methods of moments, include the prominent generalized method of moments (GMM) which has recently gained attention in causal inference. GMM is a special case of the broader family of empirical likelihood estimators which are based on approximating a population distribution by means of minimizing a $\\varphi$-divergence to an empirical distribution. However, the use of $\\varphi$-divergences effectively limits the candidate distributions to reweightings of the data samples. We lift this long-standing limitation and provide a method of moments that goes beyond data reweighting. This is achieved by defining an empirical likelihood estimator based on maximum mean discrepancy which we term the kernel method of moments (KMM). We provide a variant of our estimator for conditional moment",
    "path": "papers/23/05/2305.10898.json",
    "total_tokens": 1041,
    "translated_title": "超越数据重新加权：核矩法估计",
    "translated_abstract": "在机器学习与统计学等多个领域中都会出现矩约束和条件对应，其中，广义矩法（GMM）作为一个估计模型已经引起了人们的关注。然而，往往由于使用 $\\varphi$-散度的相关限制将候选分布限制为数据样本的重新加权。而本论文提出了一种新的矩估计方法——基于最大均值偏差的经验似然估计器，即核矩法(KMM)，其实现超越了对数据的重新加权。",
    "tldr": "本论文提出了一种新的核矩法估计器，称为KMM，其用于超越数据重新加权的矩方法模型，解除了关于使用 $\\varphi$-散度相关的限制。",
    "en_tdlr": "This paper proposes a new kernel method of moments (KMM) estimator that goes beyond data reweighting, lifting the long-standing limitation of $\\varphi$-divergences, and providing a model for moment estimation."
}