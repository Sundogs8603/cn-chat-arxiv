{
    "title": "Text encoders bottleneck compositionality in contrastive vision-language models. (arXiv:2305.14897v2 [cs.CL] UPDATED)",
    "abstract": "Performant vision-language (VL) models like CLIP represent captions using a single vector. How much information about language is lost in this bottleneck? We first curate CompPrompts, a set of increasingly compositional image captions that VL models should be able to capture (e.g., single object, to object+property, to multiple interacting objects). Then, we train text-only recovery probes that aim to reconstruct captions from single-vector text representations produced by several VL models. This approach does not require images, allowing us to test on a broader range of scenes compared to prior work. We find that: 1) CLIP's text encoder falls short on more compositional inputs, including object relationships, attribute-object association, counting, and negations; 2) some text encoders work significantly better than others; and 3) text-only recovery performance predicts multi-modal matching performance on ControlledImCaps: a new evaluation benchmark we collect and release consisting of",
    "link": "http://arxiv.org/abs/2305.14897",
    "context": "Title: Text encoders bottleneck compositionality in contrastive vision-language models. (arXiv:2305.14897v2 [cs.CL] UPDATED)\nAbstract: Performant vision-language (VL) models like CLIP represent captions using a single vector. How much information about language is lost in this bottleneck? We first curate CompPrompts, a set of increasingly compositional image captions that VL models should be able to capture (e.g., single object, to object+property, to multiple interacting objects). Then, we train text-only recovery probes that aim to reconstruct captions from single-vector text representations produced by several VL models. This approach does not require images, allowing us to test on a broader range of scenes compared to prior work. We find that: 1) CLIP's text encoder falls short on more compositional inputs, including object relationships, attribute-object association, counting, and negations; 2) some text encoders work significantly better than others; and 3) text-only recovery performance predicts multi-modal matching performance on ControlledImCaps: a new evaluation benchmark we collect and release consisting of",
    "path": "papers/23/05/2305.14897.json",
    "total_tokens": 937,
    "translated_title": "文本编码器限制了对比视觉-语言模型的组合性能",
    "translated_abstract": "高性能的视觉-语言模型（VL）如CLIP使用单一向量表示标题。在这个瓶颈中失去了多少关于语言的信息？我们首先策划了CompPrompts，这是一组越来越复杂的图像标题，VL模型应该能够捕捉到（例如，单个对象，到对象+属性，到多个互动对象）。然后，我们训练了仅基于文本的恢复探针，旨在从几个VL模型生成的单一向量文本表示中重建标题。这种方法不需要图像，相对于之前的工作，使我们能够在更广泛的场景上进行测试。我们发现：1）CLIP的文本编码器在更复杂的输入上表现不佳，包括对象关系、属性-对象关联、计数和否定；2）一些文本编码器比其他编码器要好得多；3）仅基于文本的恢复性能预测了ControlledImCaps上的多模态匹配性能：这是我们收集和发布的一个新的评估基准。",
    "tldr": "本研究发现，在对比视觉-语言模型中，使用单个向量表示标语的文本编码器在处理更加复杂的输入时表现不佳，但某些文本编码器的性能明显优于其他编码器。仅基于文本的恢复性能能够预测多模态匹配性能。"
}