{
    "title": "Pre-training Language Model as a Multi-perspective Course Learner. (arXiv:2305.03981v1 [cs.CL])",
    "abstract": "ELECTRA, the generator-discriminator pre-training framework, has achieved impressive semantic construction capability among various downstream tasks. Despite the convincing performance, ELECTRA still faces the challenges of monotonous training and deficient interaction. Generator with only masked language modeling (MLM) leads to biased learning and label imbalance for discriminator, decreasing learning efficiency; no explicit feedback loop from discriminator to generator results in the chasm between these two components, underutilizing the course learning. In this study, a multi-perspective course learning (MCL) method is proposed to fetch a many degrees and visual angles for sample-efficient pre-training, and to fully leverage the relationship between generator and discriminator. Concretely, three self-supervision courses are designed to alleviate inherent flaws of MLM and balance the label in a multi-perspective way. Besides, two self-correction courses are proposed to bridge the cha",
    "link": "http://arxiv.org/abs/2305.03981",
    "context": "Title: Pre-training Language Model as a Multi-perspective Course Learner. (arXiv:2305.03981v1 [cs.CL])\nAbstract: ELECTRA, the generator-discriminator pre-training framework, has achieved impressive semantic construction capability among various downstream tasks. Despite the convincing performance, ELECTRA still faces the challenges of monotonous training and deficient interaction. Generator with only masked language modeling (MLM) leads to biased learning and label imbalance for discriminator, decreasing learning efficiency; no explicit feedback loop from discriminator to generator results in the chasm between these two components, underutilizing the course learning. In this study, a multi-perspective course learning (MCL) method is proposed to fetch a many degrees and visual angles for sample-efficient pre-training, and to fully leverage the relationship between generator and discriminator. Concretely, three self-supervision courses are designed to alleviate inherent flaws of MLM and balance the label in a multi-perspective way. Besides, two self-correction courses are proposed to bridge the cha",
    "path": "papers/23/05/2305.03981.json",
    "total_tokens": 925,
    "translated_title": "作为多角度课程学习机器的预训练语言模型",
    "translated_abstract": "ELECTRA是一个生成器-鉴别器的预训练框架，已经在各种下游任务中取得了令人印象深刻的语义构建能力。尽管性能令人信服，但ELECTRA仍面临单调的训练和不足的交互挑战。只有掩码语言建模（MLM）的生成器会导致对于辨别器的学习效率下降的偏向性学习和标签不平衡;同时没有明确的辨别器到生成器的反馈环路导致这两个组件之间的差距，未充分利用课程学习。因此，本研究提出了一种多角度课程学习（MCL）方法，以多个角度获取样本高效预训练，并充分利用生成器和辨别器之间的关系。具体而言，设计了三个自监督课程来减轻MLM的固有缺陷，并以多角度方式平衡标签。此外，还提出了两种自我修正课程来弥合辨别器和生成器之间的差距，从而充分利用课程学习。",
    "tldr": "本文提出了一种多角度课程学习的方法，使用三个自监督课程和两个自我修正课程，以平衡标签和充分利用生成器和辨别器之间的关系进行高效的预训练。"
}