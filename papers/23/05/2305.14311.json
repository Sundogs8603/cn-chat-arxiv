{
    "title": "Statistical Indistinguishability of Learning Algorithms. (arXiv:2305.14311v1 [cs.LG])",
    "abstract": "When two different parties use the same learning rule on their own data, how can we test whether the distributions of the two outcomes are similar? In this paper, we study the similarity of outcomes of learning rules through the lens of the Total Variation (TV) distance of distributions. We say that a learning rule is TV indistinguishable if the expected TV distance between the posterior distributions of its outputs, executed on two training data sets drawn independently from the same distribution, is small. We first investigate the learnability of hypothesis classes using TV indistinguishable learners. Our main results are information-theoretic equivalences between TV indistinguishability and existing algorithmic stability notions such as replicability and approximate differential privacy. Then, we provide statistical amplification and boosting algorithms for TV indistinguishable learners.",
    "link": "http://arxiv.org/abs/2305.14311",
    "context": "Title: Statistical Indistinguishability of Learning Algorithms. (arXiv:2305.14311v1 [cs.LG])\nAbstract: When two different parties use the same learning rule on their own data, how can we test whether the distributions of the two outcomes are similar? In this paper, we study the similarity of outcomes of learning rules through the lens of the Total Variation (TV) distance of distributions. We say that a learning rule is TV indistinguishable if the expected TV distance between the posterior distributions of its outputs, executed on two training data sets drawn independently from the same distribution, is small. We first investigate the learnability of hypothesis classes using TV indistinguishable learners. Our main results are information-theoretic equivalences between TV indistinguishability and existing algorithmic stability notions such as replicability and approximate differential privacy. Then, we provide statistical amplification and boosting algorithms for TV indistinguishable learners.",
    "path": "papers/23/05/2305.14311.json",
    "total_tokens": 809,
    "translated_title": "学习算法的统计不可区分性",
    "translated_abstract": "当两个不同的用户在他们自己的数据上使用相同的学习规则时，我们如何测试两个结果的分布是否相似？本文通过分布的总变差距离来研究学习规则结果的相似性。我们称学习规则在期望总变差距离上是小的，若其输出的后验分布在两个独立于相同分布的训练数据集上执行时。我们首先研究使用 TV 不可区分学习器的假设类可学习性。我们的主要结果是 TV 不可区分性和现有算法稳定性概念之间的信息论等价性，如可复制性和近似差分隐私。然后，我们提供了 TV 不可区分学习器的统计扩大和提升算法。",
    "tldr": "本文研究了学习算法的统计不可区分性。他们通过学习规则的结果相似性来检验两个输出分布是否相似。本文发现 TV 不可区分性与现有算法稳定性概念等价，并提供了相关算法。",
    "en_tdlr": "This paper studies the statistical indistinguishability of learning algorithms by examining the similarity of outcomes of learning rules. The authors found information-theoretic equivalences between TV indistinguishability and existing algorithmic stability notions, and provided statistical amplification and boosting algorithms for TV indistinguishable learners."
}