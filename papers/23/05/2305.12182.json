{
    "title": "Glot500: Scaling Multilingual Corpora and Language Models to 500 Languages. (arXiv:2305.12182v1 [cs.CL])",
    "abstract": "The NLP community has mainly focused on scaling Large Language Models (LLMs) vertically, i.e., making them better for about 100 languages. We instead scale LLMs horizontally: we create, through continued pretraining, Glot500-m, an LLM that covers 511 languages, almost all of them low-resource. An important part of this effort is to collect and clean Glot500-c, a corpus that covers these 511 languages and allows us to train Glot500-m. We evaluate Glot500-m on five diverse tasks across these languages. We observe large improvements for both high-resource and lowresource languages compared to an XLM-R baseline. Our analysis shows that no single factor explains the quality of multilingual LLM representations. Rather, a combination of factors determines quality including corpus size, script, \"help\" from related languages and the total capacity of the model. Our work addresses an important goal of NLP research: we should not limit NLP to a small fraction of the world's languages and instead ",
    "link": "http://arxiv.org/abs/2305.12182",
    "context": "Title: Glot500: Scaling Multilingual Corpora and Language Models to 500 Languages. (arXiv:2305.12182v1 [cs.CL])\nAbstract: The NLP community has mainly focused on scaling Large Language Models (LLMs) vertically, i.e., making them better for about 100 languages. We instead scale LLMs horizontally: we create, through continued pretraining, Glot500-m, an LLM that covers 511 languages, almost all of them low-resource. An important part of this effort is to collect and clean Glot500-c, a corpus that covers these 511 languages and allows us to train Glot500-m. We evaluate Glot500-m on five diverse tasks across these languages. We observe large improvements for both high-resource and lowresource languages compared to an XLM-R baseline. Our analysis shows that no single factor explains the quality of multilingual LLM representations. Rather, a combination of factors determines quality including corpus size, script, \"help\" from related languages and the total capacity of the model. Our work addresses an important goal of NLP research: we should not limit NLP to a small fraction of the world's languages and instead ",
    "path": "papers/23/05/2305.12182.json",
    "total_tokens": 1038,
    "translated_title": "Glot500：扩展500种语言的多语料库和语言模型",
    "translated_abstract": "自然语言处理领域一直专注于使大型语言模型在大约100种语言中更加出色。我们通过不断的预训练，水平扩展大型语言模型。我们创建了Glot500-m，这是一个覆盖了511种语言的语言模型，其中几乎所有语言都是低资源语言。这项工作的重要部分是收集和清理Glot500-c，这是一个包括这511种语言的语料库，可以让我们对Glot500-m进行训练。我们在这些语言上评估了Glot500-m在五个不同的任务中的表现。我们观察到，与XLM-R基线相比，Glot500-m在高资源和低资源语言的表现都有了很大的提高。我们的分析表明，没有单一因素可以解释多语言大型语言模型表示的质量。相反，多个因素决定了质量，包括语料库大小、脚本、相关语言的“帮助”以及模型的总容量。我们的工作解决了自然语言处理研究的一个重要目标：我们不应该将自然语言处理局限于世界语言的一小部分，而是应该让它涵盖更广泛的语言范围。",
    "tldr": "Glot500是一个水平扩展的语言模型，覆盖了511种低资源语言。相比于XLM-R基线，Glot500展现出了更好的高资源和低资源语言表现。该模型质量的决定因素包括语料库大小、脚本、相关语言的“帮助”和模型的总容量。",
    "en_tdlr": "Glot500 is a horizontally scaled language model covering 511 low-resource languages. Compared to the XLM-R baseline, Glot500 shows better performance for both high-resource and low-resource languages. The quality of the model is determined by factors such as corpus size, script, \"help\" from related languages, and the total capacity of the model."
}