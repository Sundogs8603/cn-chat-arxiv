{
    "title": "PandaGPT: One Model To Instruction-Follow Them All. (arXiv:2305.16355v1 [cs.CL])",
    "abstract": "We present PandaGPT, an approach to emPower large lANguage moDels with visual and Auditory instruction-following capabilities. Our pilot experiments show that PandaGPT can perform complex tasks such as detailed image description generation, writing stories inspired by videos, and answering questions about audios. More interestingly, PandaGPT can take multimodal inputs simultaneously and compose their semantics naturally. For example, PandaGPT can connect how objects look in an image/video and how they sound in an audio. To do so, PandaGPT combines the multimodal encoders from ImageBind and the large language models from Vicuna. Notably, only aligned image-text pairs are required for the training of PandaGPT. Thanks to the strong capability of ImageBind in embedding data from different modalities into the same space, PandaGPT displays emergent, i.e. zero-shot, cross-modal behaviors for data other than image and text (e.g., video, audio, depth, thermal, and IMU). We hope that PandaGPT se",
    "link": "http://arxiv.org/abs/2305.16355",
    "context": "Title: PandaGPT: One Model To Instruction-Follow Them All. (arXiv:2305.16355v1 [cs.CL])\nAbstract: We present PandaGPT, an approach to emPower large lANguage moDels with visual and Auditory instruction-following capabilities. Our pilot experiments show that PandaGPT can perform complex tasks such as detailed image description generation, writing stories inspired by videos, and answering questions about audios. More interestingly, PandaGPT can take multimodal inputs simultaneously and compose their semantics naturally. For example, PandaGPT can connect how objects look in an image/video and how they sound in an audio. To do so, PandaGPT combines the multimodal encoders from ImageBind and the large language models from Vicuna. Notably, only aligned image-text pairs are required for the training of PandaGPT. Thanks to the strong capability of ImageBind in embedding data from different modalities into the same space, PandaGPT displays emergent, i.e. zero-shot, cross-modal behaviors for data other than image and text (e.g., video, audio, depth, thermal, and IMU). We hope that PandaGPT se",
    "path": "papers/23/05/2305.16355.json",
    "total_tokens": 1022,
    "translated_title": "PandaGPT: 一种能够执行图像、音频指令的语言模型",
    "translated_abstract": "我们提出了一种名为PandaGPT的方法，通过对齐图像-文本对训练语言模型，将ImageBind的多模态编码器和Vicuna的大型语言模型结合起来，使其具有视觉和听觉指令跟踪能力。实验结果表明，除了可以生成详细的图像描述、写故事、还能够自然地组合多个数据的语义，以回答各种问题。",
    "tldr": "PandaGPT是一种能够同时接受多模态输入，并用于生成复杂任务输出的可学习语言模型。",
    "en_tdlr": "PandaGPT is a learnable language model that can accept multimodal inputs and generate complex outputs for various tasks."
}