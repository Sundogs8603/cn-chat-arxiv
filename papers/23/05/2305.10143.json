{
    "title": "An Empirical Study on the Language Modal in Visual Question Answering. (arXiv:2305.10143v1 [cs.AI])",
    "abstract": "Generalization beyond in-domain experience to out-of-distribution data is of paramount significance in the AI domain. Of late, state-of-the-art Visual Question Answering (VQA) models have shown impressive performance on in-domain data, partially due to the language priors bias which, however, hinders the generalization ability in practice. This paper attempts to provide new insights into the influence of language modality on VQA performance from an empirical study perspective. To achieve this, we conducted a series of experiments on six models. The results of these experiments revealed that, 1) apart from prior bias caused by question types, there is a notable influence of postfix-related bias in inducing biases, and 2) training VQA models with word-sequence-related variant questions demonstrated improved performance on the out-of-distribution benchmark, and the LXMERT even achieved a 10-point gain without adopting any debiasing methods. We delved into the underlying reasons behind the",
    "link": "http://arxiv.org/abs/2305.10143",
    "context": "Title: An Empirical Study on the Language Modal in Visual Question Answering. (arXiv:2305.10143v1 [cs.AI])\nAbstract: Generalization beyond in-domain experience to out-of-distribution data is of paramount significance in the AI domain. Of late, state-of-the-art Visual Question Answering (VQA) models have shown impressive performance on in-domain data, partially due to the language priors bias which, however, hinders the generalization ability in practice. This paper attempts to provide new insights into the influence of language modality on VQA performance from an empirical study perspective. To achieve this, we conducted a series of experiments on six models. The results of these experiments revealed that, 1) apart from prior bias caused by question types, there is a notable influence of postfix-related bias in inducing biases, and 2) training VQA models with word-sequence-related variant questions demonstrated improved performance on the out-of-distribution benchmark, and the LXMERT even achieved a 10-point gain without adopting any debiasing methods. We delved into the underlying reasons behind the",
    "path": "papers/23/05/2305.10143.json",
    "total_tokens": 1023,
    "tldr": "本文通过实验研究了语言模态对VQA的影响，发现除了由不同类型问题引起的偏置外，后缀相关偏置也会导致模型的偏置，并且使用与单词序列相关的变体问题来训练模型可以提高模型在超出分布数据上的性能。",
    "en_tdlr": "This paper empirically studies the influence of language modality on VQA performance, revealing that postfix-related bias can induce bias in VQA models, in addition to prior bias caused by different question types. Training VQA models with word-sequence-related variant questions can improve their performance on out-of-distribution data, and even achieve a 10-point gain for the LXMERT model without debiasing methods."
}