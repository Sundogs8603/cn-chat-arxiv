{
    "title": "Divide-and-Conquer the NAS puzzle in Resource Constrained Federated Learning Systems. (arXiv:2305.07135v1 [cs.LG])",
    "abstract": "Federated Learning (FL) is a privacy-preserving distributed machine learning approach geared towards applications in edge devices. However, the problem of designing custom neural architectures in federated environments is not tackled from the perspective of overall system efficiency. In this paper, we propose DC-NAS -- a divide-and-conquer approach that performs supernet-based Neural Architecture Search (NAS) in a federated system by systematically sampling the search space. We propose a novel diversified sampling strategy that balances exploration and exploitation of the search space by initially maximizing the distance between the samples and progressively shrinking this distance as the training progresses. We then perform channel pruning to reduce the training complexity at the devices further. We show that our approach outperforms several sampling strategies including Hadamard sampling, where the samples are maximally separated. We evaluate our method on the CIFAR10, CIFAR100, EMNI",
    "link": "http://arxiv.org/abs/2305.07135",
    "context": "Title: Divide-and-Conquer the NAS puzzle in Resource Constrained Federated Learning Systems. (arXiv:2305.07135v1 [cs.LG])\nAbstract: Federated Learning (FL) is a privacy-preserving distributed machine learning approach geared towards applications in edge devices. However, the problem of designing custom neural architectures in federated environments is not tackled from the perspective of overall system efficiency. In this paper, we propose DC-NAS -- a divide-and-conquer approach that performs supernet-based Neural Architecture Search (NAS) in a federated system by systematically sampling the search space. We propose a novel diversified sampling strategy that balances exploration and exploitation of the search space by initially maximizing the distance between the samples and progressively shrinking this distance as the training progresses. We then perform channel pruning to reduce the training complexity at the devices further. We show that our approach outperforms several sampling strategies including Hadamard sampling, where the samples are maximally separated. We evaluate our method on the CIFAR10, CIFAR100, EMNI",
    "path": "papers/23/05/2305.07135.json",
    "total_tokens": 961,
    "tldr": "本文提出了一种名为DC-NAS的分治策略，在联邦系统中采用超网络进行神经网络架构搜索。该方法采用新颖的多样化采样策略进行搜索空间的探索和开发，并进一步通过通道剪枝来进一步减少设备上的训练复杂度，从而有效提高了整体系统效率。"
}