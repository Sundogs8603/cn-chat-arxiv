{
    "title": "Sharpness-Aware Minimization Leads to Low-Rank Features. (arXiv:2305.16292v2 [cs.LG] UPDATED)",
    "abstract": "Sharpness-aware minimization (SAM) is a recently proposed method that minimizes the sharpness of the training loss of a neural network. While its generalization improvement is well-known and is the primary motivation, we uncover an additional intriguing effect of SAM: reduction of the feature rank which happens at different layers of a neural network. We show that this low-rank effect occurs very broadly: for different architectures such as fully-connected networks, convolutional networks, vision transformers and for different objectives such as regression, classification, language-image contrastive training. To better understand this phenomenon, we provide a mechanistic understanding of how low-rank features arise in a simple two-layer network. We observe that a significant number of activations gets entirely pruned by SAM which directly contributes to the rank reduction. We confirm this effect theoretically and check that it can also occur in deep networks, although the overall rank ",
    "link": "http://arxiv.org/abs/2305.16292",
    "context": "Title: Sharpness-Aware Minimization Leads to Low-Rank Features. (arXiv:2305.16292v2 [cs.LG] UPDATED)\nAbstract: Sharpness-aware minimization (SAM) is a recently proposed method that minimizes the sharpness of the training loss of a neural network. While its generalization improvement is well-known and is the primary motivation, we uncover an additional intriguing effect of SAM: reduction of the feature rank which happens at different layers of a neural network. We show that this low-rank effect occurs very broadly: for different architectures such as fully-connected networks, convolutional networks, vision transformers and for different objectives such as regression, classification, language-image contrastive training. To better understand this phenomenon, we provide a mechanistic understanding of how low-rank features arise in a simple two-layer network. We observe that a significant number of activations gets entirely pruned by SAM which directly contributes to the rank reduction. We confirm this effect theoretically and check that it can also occur in deep networks, although the overall rank ",
    "path": "papers/23/05/2305.16292.json",
    "total_tokens": 846,
    "translated_title": "锐度感知最小化导致低秩特征",
    "translated_abstract": "锐度感知最小化（SAM）是一种最小化神经网络训练损失锐度的方法。尽管其泛化性能提升已被广泛认可并成为主要动机，我们发现了SAM的另一个有趣效果：在神经网络的不同层级上发生特征秩降低。我们展示了这种低秩效果的广泛性：适用于全连接网络、卷积网络、视觉变换器等不同体系结构，适用于回归、分类、语言图像对比训练等不同目标。为了更好地理解这一现象，我们在一个简单的两层网络中提供了低秩特征产生的机制性理解。我们观察到SAM将大量的激活完全修剪掉，直接导致秩的降低。我们在理论上验证了这一效果，并检查了它在深度网络中也可能发生，尽管整体秩会有略微的增加。",
    "tldr": "锐度感知最小化方法SAM在神经网络训练中不仅能提升泛化性能，还可以降低特征的秩，适用于不同网络架构和任务类型。",
    "en_tdlr": "Sharpness-aware minimization method SAM improves the generalization performance of neural networks and reduces the rank of features, applicable to various network architectures and task types."
}