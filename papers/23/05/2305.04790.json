{
    "title": "MultiModal-GPT: A Vision and Language Model for Dialogue with Humans. (arXiv:2305.04790v2 [cs.CV] UPDATED)",
    "abstract": "We present a vision and language model named MultiModal-GPT to conduct multi-round dialogue with humans. MultiModal-GPT can follow various instructions from humans, such as generating a detailed caption, counting the number of interested objects, and answering general questions from users. MultiModal-GPT is parameter-efficiently fine-tuned from OpenFlamingo, with Low-rank Adapter (LoRA) added both in the cross-attention part and the self-attention part of the language model. We first construct instruction templates with vision and language data for multi-modality instruction tuning to make the model understand and follow human instructions. We find the quality of training data is vital for the dialogue performance, where few data containing short answers can lead the model to respond shortly to any instructions. To further enhance the ability to chat with humans of the MultiModal-GPT, we utilize language-only instruction-following data to train the MultiModal-GPT jointly. The joint tra",
    "link": "http://arxiv.org/abs/2305.04790",
    "context": "Title: MultiModal-GPT: A Vision and Language Model for Dialogue with Humans. (arXiv:2305.04790v2 [cs.CV] UPDATED)\nAbstract: We present a vision and language model named MultiModal-GPT to conduct multi-round dialogue with humans. MultiModal-GPT can follow various instructions from humans, such as generating a detailed caption, counting the number of interested objects, and answering general questions from users. MultiModal-GPT is parameter-efficiently fine-tuned from OpenFlamingo, with Low-rank Adapter (LoRA) added both in the cross-attention part and the self-attention part of the language model. We first construct instruction templates with vision and language data for multi-modality instruction tuning to make the model understand and follow human instructions. We find the quality of training data is vital for the dialogue performance, where few data containing short answers can lead the model to respond shortly to any instructions. To further enhance the ability to chat with humans of the MultiModal-GPT, we utilize language-only instruction-following data to train the MultiModal-GPT jointly. The joint tra",
    "path": "papers/23/05/2305.04790.json",
    "total_tokens": 948,
    "translated_title": "多模态-GPT: 用于与人类对话的视觉与语言模型",
    "translated_abstract": "我们提出了一个名为MultiModal-GPT的视觉与语言模型，用于与人类进行多轮对话。 MultiModal-GPT可以遵循人类的各种指令，例如生成详细的字幕，计算感兴趣对象的数量以及回答用户的常见问题。 我们通过OpenFlamingo进行参数有效地微调MultiModal-GPT，并在语言模型的交叉关注部分和自我关注部分中添加了低秩适配器（LoRA）。 我们首先使用视觉和语言数据构建指令模板，用于多模态指令调整，让模型理解和遵循人类指令。 我们发现对话表现的训练数据质量至关重要，其中很少包含简短回答的数据会使模型对任何指令都作出简短回答。为了进一步增强MultiModal-GPT与人类聊天的能力，我们利用仅语言的指令跟随数据联合训练MultiModal-GPT。联合训练显著提高了MultiModal-GPT在对话任务中的表现。",
    "tldr": "MultiModal-GPT是一个用于与人类进行多轮对话的视觉与语言模型，可以遵循人类的各种指令，并且通过联合训练表现得更好。",
    "en_tdlr": "MultiModal-GPT is a vision and language model for conducting multi-round dialogue with humans. It can follow various instructions and is fine-tuned from OpenFlamingo with Low-rank Adapter added in the attention parts. To enhance its ability to chat with humans, language-only instruction-following data is utilized to train it jointly, significantly improving its performance."
}