{
    "title": "UniTRec: A Unified Text-to-Text Transformer and Joint Contrastive Learning Framework for Text-based Recommendation. (arXiv:2305.15756v1 [cs.CL])",
    "abstract": "Prior study has shown that pretrained language models (PLM) can boost the performance of text-based recommendation. In contrast to previous works that either use PLM to encode user history as a whole input text, or impose an additional aggregation network to fuse multi-turn history representations, we propose a unified local- and global-attention Transformer encoder to better model two-level contexts of user history. Moreover, conditioned on user history encoded by Transformer encoders, our framework leverages Transformer decoders to estimate the language perplexity of candidate text items, which can serve as a straightforward yet significant contrastive signal for user-item text matching. Based on this, our framework, UniTRec, unifies the contrastive objectives of discriminative matching scores and candidate text perplexity to jointly enhance text-based recommendation. Extensive evaluation shows that UniTRec delivers SOTA performance on three text-based recommendation tasks. Code is a",
    "link": "http://arxiv.org/abs/2305.15756",
    "context": "Title: UniTRec: A Unified Text-to-Text Transformer and Joint Contrastive Learning Framework for Text-based Recommendation. (arXiv:2305.15756v1 [cs.CL])\nAbstract: Prior study has shown that pretrained language models (PLM) can boost the performance of text-based recommendation. In contrast to previous works that either use PLM to encode user history as a whole input text, or impose an additional aggregation network to fuse multi-turn history representations, we propose a unified local- and global-attention Transformer encoder to better model two-level contexts of user history. Moreover, conditioned on user history encoded by Transformer encoders, our framework leverages Transformer decoders to estimate the language perplexity of candidate text items, which can serve as a straightforward yet significant contrastive signal for user-item text matching. Based on this, our framework, UniTRec, unifies the contrastive objectives of discriminative matching scores and candidate text perplexity to jointly enhance text-based recommendation. Extensive evaluation shows that UniTRec delivers SOTA performance on three text-based recommendation tasks. Code is a",
    "path": "papers/23/05/2305.15756.json",
    "total_tokens": 896,
    "translated_title": "UniTRec: 一个统一的文本到文本变换器和联合对比学习框架用于基于文本的推荐",
    "translated_abstract": "先前的研究表明，预训练语言模型（PLM）可以提高基于文本的推荐性能。与以往工作不同，我们提出了一个统一的局部-全局注意力Transformer编码器，以更好地建模用户历史的两个层次的上下文。此外，在基于Transformer编码器编码的用户历史的条件下，我们的框架利用Transformer解码器估计候选文本项的语言困惑度，这可以作为用户-物品文本匹配的简单而重要的对比信号。基于此，我们的框架UniTRec将区分性匹配得分和候选文本困惑度的对比目标统一起来，以共同增强基于文本的推荐。广泛的评估表明，UniTRec在三个基于文本的推荐任务上提供了SOTA性能。代码可在https://anonymous.com上找到。",
    "tldr": "UniTRec是一个文本到文本的推荐框架，采用了统一的局部-全局注意力Transformer编码器来处理用户历史的上下文，并且使用Transformer解码器的语言困惑度来构建对比信号，可以显著提高性能。",
    "en_tdlr": "UniTRec is a unified text-to-text recommendation framework that utilizes a local- and global-attention Transformer encoder to model context of user history and a Transformer decoder to estimate candidate text perplexity, delivering SOTA performance on three text-based recommendation tasks."
}