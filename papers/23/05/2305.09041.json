{
    "title": "What Matters in Reinforcement Learning for Tractography. (arXiv:2305.09041v1 [cs.LG])",
    "abstract": "Recently, deep reinforcement learning (RL) has been proposed to learn the tractography procedure and train agents to reconstruct the structure of the white matter without manually curated reference streamlines. While the performances reported were competitive, the proposed framework is complex, and little is still known about the role and impact of its multiple parts. In this work, we thoroughly explore the different components of the proposed framework, such as the choice of the RL algorithm, seeding strategy, the input signal and reward function, and shed light on their impact. Approximately 7,400 models were trained for this work, totalling nearly 41,000 hours of GPU time. Our goal is to guide researchers eager to explore the possibilities of deep RL for tractography by exposing what works and what does not work with the category of approach. As such, we ultimately propose a series of recommendations concerning the choice of RL algorithm, the input to the agents, the reward function",
    "link": "http://arxiv.org/abs/2305.09041",
    "context": "Title: What Matters in Reinforcement Learning for Tractography. (arXiv:2305.09041v1 [cs.LG])\nAbstract: Recently, deep reinforcement learning (RL) has been proposed to learn the tractography procedure and train agents to reconstruct the structure of the white matter without manually curated reference streamlines. While the performances reported were competitive, the proposed framework is complex, and little is still known about the role and impact of its multiple parts. In this work, we thoroughly explore the different components of the proposed framework, such as the choice of the RL algorithm, seeding strategy, the input signal and reward function, and shed light on their impact. Approximately 7,400 models were trained for this work, totalling nearly 41,000 hours of GPU time. Our goal is to guide researchers eager to explore the possibilities of deep RL for tractography by exposing what works and what does not work with the category of approach. As such, we ultimately propose a series of recommendations concerning the choice of RL algorithm, the input to the agents, the reward function",
    "path": "papers/23/05/2305.09041.json",
    "total_tokens": 882,
    "translated_title": "强化学习在Tractography中的作用",
    "translated_abstract": "最近，提出了利用深度强化学习（RL）来学习Tractography过程并训练代理人在没有手动筛选的参考流线的情况下重建白质结构。虽然报告的表现颇具竞争力，但所提出的框架复杂，并且对于其多个部分的作用和影响还知之甚少。在这项工作中，我们深入探讨了所提出框架的不同组成部分，例如RL算法的选择，播种策略，输入信号和奖励函数，并阐明了它们的影响。本次研究共训练了约7400个模型，共计近41000小时的GPU时间。我们的目标是指导热衷于探索深度RL在Tractography中可能性的研究人员，展示这种方法的优势和不足。因此，我们最终提出了关于RL算法的选择、代理人输入、奖励函数和播种策略的一系列建议。",
    "tldr": "本论文深入探讨了强化学习在Tractography中不同的组成部分，提出了关于RL算法选择、代理人输入、奖励函数和播种策略的一系列建议。",
    "en_tdlr": "This paper thoroughly explores the different components of deep reinforcement learning (RL) for tractography, and proposes a series of recommendations concerning the choice of RL algorithm, the input to the agents, the reward function, and seeding strategies."
}