{
    "title": "Efficient Diffusion Policies for Offline Reinforcement Learning. (arXiv:2305.20081v2 [cs.LG] UPDATED)",
    "abstract": "Offline reinforcement learning (RL) aims to learn optimal policies from offline datasets, where the parameterization of policies is crucial but often overlooked. Recently, Diffsuion-QL significantly boosts the performance of offline RL by representing a policy with a diffusion model, whose success relies on a parametrized Markov Chain with hundreds of steps for sampling. However, Diffusion-QL suffers from two critical limitations. 1) It is computationally inefficient to forward and backward through the whole Markov chain during training. 2) It is incompatible with maximum likelihood-based RL algorithms (e.g., policy gradient methods) as the likelihood of diffusion models is intractable. Therefore, we propose efficient diffusion policy (EDP) to overcome these two challenges. EDP approximately constructs actions from corrupted ones at training to avoid running the sampling chain. We conduct extensive experiments on the D4RL benchmark. The results show that EDP can reduce the diffusion po",
    "link": "http://arxiv.org/abs/2305.20081",
    "context": "Title: Efficient Diffusion Policies for Offline Reinforcement Learning. (arXiv:2305.20081v2 [cs.LG] UPDATED)\nAbstract: Offline reinforcement learning (RL) aims to learn optimal policies from offline datasets, where the parameterization of policies is crucial but often overlooked. Recently, Diffsuion-QL significantly boosts the performance of offline RL by representing a policy with a diffusion model, whose success relies on a parametrized Markov Chain with hundreds of steps for sampling. However, Diffusion-QL suffers from two critical limitations. 1) It is computationally inefficient to forward and backward through the whole Markov chain during training. 2) It is incompatible with maximum likelihood-based RL algorithms (e.g., policy gradient methods) as the likelihood of diffusion models is intractable. Therefore, we propose efficient diffusion policy (EDP) to overcome these two challenges. EDP approximately constructs actions from corrupted ones at training to avoid running the sampling chain. We conduct extensive experiments on the D4RL benchmark. The results show that EDP can reduce the diffusion po",
    "path": "papers/23/05/2305.20081.json",
    "total_tokens": 971,
    "translated_title": "离线强化学习的高效扩散策略",
    "translated_abstract": "离线强化学习旨在从离线数据集中学习最优策略，其中策略的参数化是关键但常常被忽视。最近，Diffusion-QL通过使用扩散模型表示策略，显著提高了离线强化学习的性能，该模型的成功依赖于参数化的马尔可夫链进行采样，但是Diffusion-QL存在两个关键限制。1）在训练过程中，通过整个马尔可夫链进行前向和反向传播计算效率低下。2）由于扩散模型的似然函数难以计算，与基于最大似然的强化学习算法（如策略梯度方法）不兼容。因此，我们提出了高效扩散策略（EDP）来克服这两个挑战。EDP在训练过程中通过从损坏的动作中近似构建动作，避免了运行采样链。我们在D4RL基准测试中进行了大量实验。结果表明，EDP能够减少扩散的潜在计算复杂性。",
    "tldr": "本论文提出了高效扩散策略（EDP）用于解决离线强化学习中的两个关键挑战，即计算效率低和难以与最大似然的强化学习算法兼容。EDP通过近似构建动作来避免运行采样链，并在实验中得到验证。",
    "en_tdlr": "This paper proposes an efficient diffusion policy (EDP) to address two critical challenges in offline reinforcement learning: low computational efficiency and incompatibility with maximum likelihood-based RL algorithms. EDP avoids running the sampling chain by approximately constructing actions and is validated through extensive experiments."
}