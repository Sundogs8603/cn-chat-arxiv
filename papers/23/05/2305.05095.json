{
    "title": "Less is More: Removing Text-regions Improves CLIP Training Efficiency and Robustness. (arXiv:2305.05095v1 [cs.CV])",
    "abstract": "The CLIP (Contrastive Language-Image Pre-training) model and its variants are becoming the de facto backbone in many applications. However, training a CLIP model from hundreds of millions of image-text pairs can be prohibitively expensive. Furthermore, the conventional CLIP model doesn't differentiate between the visual semantics and meaning of text regions embedded in images. This can lead to non-robustness when the text in the embedded region doesn't match the image's visual appearance. In this paper, we discuss two effective approaches to improve the efficiency and robustness of CLIP training: (1) augmenting the training dataset while maintaining the same number of optimization steps, and (2) filtering out samples that contain text regions in the image. By doing so, we significantly improve the classification and retrieval accuracy on public benchmarks like ImageNet and CoCo. Filtering out images with text regions also protects the model from typographic attacks. To verify this, we ",
    "link": "http://arxiv.org/abs/2305.05095",
    "context": "Title: Less is More: Removing Text-regions Improves CLIP Training Efficiency and Robustness. (arXiv:2305.05095v1 [cs.CV])\nAbstract: The CLIP (Contrastive Language-Image Pre-training) model and its variants are becoming the de facto backbone in many applications. However, training a CLIP model from hundreds of millions of image-text pairs can be prohibitively expensive. Furthermore, the conventional CLIP model doesn't differentiate between the visual semantics and meaning of text regions embedded in images. This can lead to non-robustness when the text in the embedded region doesn't match the image's visual appearance. In this paper, we discuss two effective approaches to improve the efficiency and robustness of CLIP training: (1) augmenting the training dataset while maintaining the same number of optimization steps, and (2) filtering out samples that contain text regions in the image. By doing so, we significantly improve the classification and retrieval accuracy on public benchmarks like ImageNet and CoCo. Filtering out images with text regions also protects the model from typographic attacks. To verify this, we ",
    "path": "papers/23/05/2305.05095.json",
    "total_tokens": 1002,
    "translated_title": "少即是多：移除文本区域提高CLIP训练效率和鲁棒性",
    "translated_abstract": "CLIP（对比语言-图像预训练）模型及其变体正在成为许多应用的事实标准。然而，从数亿个图像-文本对中训练CLIP模型可能会因成本高昂而不切实际。此外，传统的CLIP模型不区分嵌入图像中的文本区域的视觉语义和含义。当嵌入区域的文本与图像的视觉外观不匹配时，这可能导致非鲁棒性。在本文中，我们讨论了两种有效的方法来提高CLIP培训的效率和鲁棒性：（1）增加训练数据集，同时保持相同数量的优化步骤，以及（2）过滤图像中包含文本区域的样本。通过这样做，我们显着提高了公共基准测试（如ImageNet和CoCo）的分类和检索准确性。同时，过滤带有文本区域的图像还可以保护模型免受印刷攻击。为了验证这一点，我们......",
    "tldr": "本文提出两种方法来提高CLIP培训效率和鲁棒性：（1）通过增加训练数据集来提高效率，（2）过滤掉图像中带有文本区域的样本以增强鲁棒性和防御印刷攻击，从而在ImageNet和Coco等公共基准测试上显着提高了分类和检索准确性。",
    "en_tdlr": "This paper proposes two methods to improve the efficiency and robustness of CLIP training: (1) improving efficiency by increasing the training dataset, and (2) filtering out samples with text regions in the image for enhanced robustness and defense against typographic attacks, resulting in a significant improvement in classification and retrieval accuracy on public benchmarks such as ImageNet and CoCo."
}