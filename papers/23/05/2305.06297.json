{
    "title": "Why Don't You Do Something About It? Outlining Connections between AI Explanations and User Actions. (arXiv:2305.06297v1 [cs.HC])",
    "abstract": "A core assumption of explainable AI systems is that explanations change what users know, thereby enabling them to act within their complex socio-technical environments. Despite the centrality of action, explanations are often organized and evaluated based on technical aspects. Prior work varies widely in the connections it traces between information provided in explanations and resulting user actions. An important first step in centering action in evaluations is understanding what the XAI community collectively recognizes as the range of information that explanations can present and what actions are associated with them. In this paper, we present our framework, which maps prior work on information presented in explanations and user action, and we discuss the gaps we uncovered about the information presented to users.",
    "link": "http://arxiv.org/abs/2305.06297",
    "context": "Title: Why Don't You Do Something About It? Outlining Connections between AI Explanations and User Actions. (arXiv:2305.06297v1 [cs.HC])\nAbstract: A core assumption of explainable AI systems is that explanations change what users know, thereby enabling them to act within their complex socio-technical environments. Despite the centrality of action, explanations are often organized and evaluated based on technical aspects. Prior work varies widely in the connections it traces between information provided in explanations and resulting user actions. An important first step in centering action in evaluations is understanding what the XAI community collectively recognizes as the range of information that explanations can present and what actions are associated with them. In this paper, we present our framework, which maps prior work on information presented in explanations and user action, and we discuss the gaps we uncovered about the information presented to users.",
    "path": "papers/23/05/2305.06297.json",
    "total_tokens": 854,
    "translated_title": "为什么不行动起来？揭示可解释人工智能和用户行动之间的联系。",
    "translated_abstract": "可解释人工智能系统的核心假设是解释可以改变用户的知识，从而使他们能够在复杂的社会技术环境中行动。尽管行动至关重要，但解释通常是基于技术方面组织和评估的。先前的工作在提供的信息和用户行为之间的联系方面差异很大。在评估中将行动置于中心位置的一个重要的第一步是理解可解释人工智能社区集体认可的解释可以提供的信息范围以及与之相关联的行动。在本文中，我们提出了一个框架，将关于可解释人工智能解释中呈现的信息和用户行动的先前工作进行了映射，并讨论了我们发现的呈现给用户的信息方面的差距。",
    "tldr": "可解释人工智能（XAI）系统的核心假设是解释可以改变用户的知识并促进他们在复杂的技术环境中采取行动。本文提出了一个框架来映射解释中呈现的信息和用户采取的行动之间的联系，并探讨了现有工作中的信息缺口。",
    "en_tdlr": "The core assumption of explainable AI (XAI) systems is that explanations can change users' knowledge and facilitate their actions in complex technical environments. This paper proposes a framework to map the connections between information presented in explanations and resulting user actions, and discusses the gaps in information presented to users found in prior work."
}