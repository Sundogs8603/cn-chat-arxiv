{
    "title": "Ditto: A Simple and Efficient Approach to Improve Sentence Embeddings. (arXiv:2305.10786v1 [cs.CL])",
    "abstract": "Prior studies diagnose the anisotropy problem in sentence representations from pre-trained language models, e.g., BERT, without fine-tuning. Our analysis reveals that the sentence embeddings from BERT suffer from a bias towards uninformative words, limiting the performance in semantic textual similarity (STS) tasks. To address this bias, we propose a simple and efficient unsupervised approach, Diagonal Attention Pooling (Ditto), which weights words with model-based importance estimations and computes the weighted average of word representations from pre-trained models as sentence embeddings. Ditto can be easily applied to any pre-trained language model as a postprocessing operation. Compared to prior sentence embedding approaches, Ditto does not add parameters nor requires any learning. Empirical evaluations demonstrate that our proposed Ditto can alleviate the anisotropy problem and improve various pre-trained models on STS tasks.",
    "link": "http://arxiv.org/abs/2305.10786",
    "context": "Title: Ditto: A Simple and Efficient Approach to Improve Sentence Embeddings. (arXiv:2305.10786v1 [cs.CL])\nAbstract: Prior studies diagnose the anisotropy problem in sentence representations from pre-trained language models, e.g., BERT, without fine-tuning. Our analysis reveals that the sentence embeddings from BERT suffer from a bias towards uninformative words, limiting the performance in semantic textual similarity (STS) tasks. To address this bias, we propose a simple and efficient unsupervised approach, Diagonal Attention Pooling (Ditto), which weights words with model-based importance estimations and computes the weighted average of word representations from pre-trained models as sentence embeddings. Ditto can be easily applied to any pre-trained language model as a postprocessing operation. Compared to prior sentence embedding approaches, Ditto does not add parameters nor requires any learning. Empirical evaluations demonstrate that our proposed Ditto can alleviate the anisotropy problem and improve various pre-trained models on STS tasks.",
    "path": "papers/23/05/2305.10786.json",
    "total_tokens": 855,
    "translated_title": "Ditto: 一种改进句子嵌入的简洁有效方法",
    "translated_abstract": "先前的研究诊断了由预训练语言模型（如BERT）产生的句子表示中存在的各向异性问题，没有进行微调。我们的分析揭示了BERT产生的句子嵌入存在偏向于非信息性单词的偏见，限制了在语义文本相似性（STS）任务中的性能。为了解决这种偏见问题，我们提出了一个简单有效的无监督方法——对角线注意力池化（Ditto），该方法使用基于模型的重要性估计权重单词，并计算预训练模型的单词表示的加权平均值作为句子嵌入。Ditto可以轻松地应用于任何预训练语言模型作为后处理操作。与先前的句子嵌入方法相比，Ditto不添加参数也不需要任何学习。实证评估表明，我们提出的Ditto可以缓解各向异性问题并提高在STS任务中各种预训练模型的性能。",
    "tldr": "提出了一种名为Ditto的简单有效的方法，其可以解决预训练语言模型中存在的各向异性问题，并在语义文本相似性任务中提高模型的性能。",
    "en_tdlr": "Ditto proposes a simple and efficient approach, called Diagonal Attention Pooling, that solves the anisotropy problem in pre-trained language models and improves their performance on semantic textual similarity tasks."
}