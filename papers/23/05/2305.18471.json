{
    "title": "Convergence of AdaGrad for Non-convex Objectives: Simple Proofs and Relaxed Assumptions. (arXiv:2305.18471v1 [cs.LG])",
    "abstract": "We provide a simple convergence proof for AdaGrad optimizing non-convex objectives under only affine noise variance and bounded smoothness assumptions. The proof is essentially based on a novel auxiliary function $\\xi$ that helps eliminate the complexity of handling the correlation between the numerator and denominator of AdaGrad's update. Leveraging simple proofs, we are able to obtain tighter results than existing results \\citep{faw2022power} and extend the analysis to several new and important cases. Specifically, for the over-parameterized regime, we show that AdaGrad needs only $\\mathcal{O}(\\frac{1}{\\varepsilon^2})$ iterations to ensure the gradient norm smaller than $\\varepsilon$, which matches the rate of SGD and significantly tighter than existing rates $\\mathcal{O}(\\frac{1}{\\varepsilon^4})$ for AdaGrad. We then discard the bounded smoothness assumption and consider a realistic assumption on smoothness called $(L_0,L_1)$-smooth condition, which allows local smoothness to grow w",
    "link": "http://arxiv.org/abs/2305.18471",
    "context": "Title: Convergence of AdaGrad for Non-convex Objectives: Simple Proofs and Relaxed Assumptions. (arXiv:2305.18471v1 [cs.LG])\nAbstract: We provide a simple convergence proof for AdaGrad optimizing non-convex objectives under only affine noise variance and bounded smoothness assumptions. The proof is essentially based on a novel auxiliary function $\\xi$ that helps eliminate the complexity of handling the correlation between the numerator and denominator of AdaGrad's update. Leveraging simple proofs, we are able to obtain tighter results than existing results \\citep{faw2022power} and extend the analysis to several new and important cases. Specifically, for the over-parameterized regime, we show that AdaGrad needs only $\\mathcal{O}(\\frac{1}{\\varepsilon^2})$ iterations to ensure the gradient norm smaller than $\\varepsilon$, which matches the rate of SGD and significantly tighter than existing rates $\\mathcal{O}(\\frac{1}{\\varepsilon^4})$ for AdaGrad. We then discard the bounded smoothness assumption and consider a realistic assumption on smoothness called $(L_0,L_1)$-smooth condition, which allows local smoothness to grow w",
    "path": "papers/23/05/2305.18471.json",
    "total_tokens": 1139,
    "translated_title": "AdaGrad算法在非凸目标优化中的收敛性: 简明证明和宽松假设",
    "translated_abstract": "我们提供了针对仅有互易噪声方差和有界平滑性假设的非凸目标的AdaGrad算法的简单收敛性证明。该证明基本上基于一个新颖的辅助函数$\\xi$，有助于消除处理AdaGrad更新的分子和分母之间相关性的复杂性。通过简单的证明，我们能够获得比现有结果\\citep{faw2022power}更紧密的结果，并将分析扩展到几种新的重要情况。具体而言，在超参数化的情况下，我们表明AdaGrad只需要$\\mathcal{O}(\\frac{1}{\\varepsilon^2})$次迭代，就可以确保梯度范数小于$\\varepsilon$，这与SGD的速率相匹配，并且比AdaGrad的现有速率$\\mathcal{O}(\\frac{1}{\\varepsilon^4})$明显更紧密。然后，我们放弃有界平滑假设，并考虑一种称为$(L_0,L_1)$-平滑条件的实际平滑假设，该假设允许本地平滑性增长",
    "tldr": "本文提出了仅有互易噪声方差和有界平滑性假设的非凸目标的AdaGrad算法的简单收敛性证明，证明中基于辅助函数$\\xi$，比现有结果更紧密，在超参数化的情况下，能够确保梯度范数小于$\\varepsilon$的迭代次数为$\\mathcal{O}(\\frac{1}{\\varepsilon^2})$，并考虑了一种实际平滑假设$(L_0,L_1)$-平滑条件。",
    "en_tdlr": "This paper presents a simple convergence proof for AdaGrad optimizing non-convex objectives under only affine noise variance and bounded smoothness assumptions, using a novel auxiliary function $\\xi$. The proof can obtain tighter results than existing results and extends the analysis to several new cases. The paper shows that for the over-parameterized regime, AdaGrad needs only $\\mathcal{O}(\\frac{1}{\\varepsilon^2})$ iterations to ensure the gradient norm smaller than $\\varepsilon$, matching the rate of SGD. It also considers a realistic smoothness assumption called $(L_0,L_1)$-smooth condition."
}