{
    "title": "On First-Order Meta-Reinforcement Learning with Moreau Envelopes. (arXiv:2305.12216v1 [cs.LG])",
    "abstract": "Meta-Reinforcement Learning (MRL) is a promising framework for training agents that can quickly adapt to new environments and tasks. In this work, we study the MRL problem under the policy gradient formulation, where we propose a novel algorithm that uses Moreau envelope surrogate regularizers to jointly learn a meta-policy that is adjustable to the environment of each individual task. Our algorithm, called Moreau Envelope Meta-Reinforcement Learning (MEMRL), learns a meta-policy that can adapt to a distribution of tasks by efficiently updating the policy parameters using a combination of gradient-based optimization and Moreau Envelope regularization. Moreau Envelopes provide a smooth approximation of the policy optimization problem, which enables us to apply standard optimization techniques and converge to an appropriate stationary point. We provide a detailed analysis of the MEMRL algorithm, where we show a sublinear convergence rate to a first-order stationary point for non-convex p",
    "link": "http://arxiv.org/abs/2305.12216",
    "context": "Title: On First-Order Meta-Reinforcement Learning with Moreau Envelopes. (arXiv:2305.12216v1 [cs.LG])\nAbstract: Meta-Reinforcement Learning (MRL) is a promising framework for training agents that can quickly adapt to new environments and tasks. In this work, we study the MRL problem under the policy gradient formulation, where we propose a novel algorithm that uses Moreau envelope surrogate regularizers to jointly learn a meta-policy that is adjustable to the environment of each individual task. Our algorithm, called Moreau Envelope Meta-Reinforcement Learning (MEMRL), learns a meta-policy that can adapt to a distribution of tasks by efficiently updating the policy parameters using a combination of gradient-based optimization and Moreau Envelope regularization. Moreau Envelopes provide a smooth approximation of the policy optimization problem, which enables us to apply standard optimization techniques and converge to an appropriate stationary point. We provide a detailed analysis of the MEMRL algorithm, where we show a sublinear convergence rate to a first-order stationary point for non-convex p",
    "path": "papers/23/05/2305.12216.json",
    "total_tokens": 868,
    "translated_title": "关于Moreau包络的一阶元强化学习",
    "translated_abstract": "元强化学习(MRL)是一种训练智能体在新环境和任务中快速适应的有前途的框架。在本文中，我们研究了基于策略梯度的MRL问题，提出了一种利用Moreau包络代理正则化器来共同学习可以适应每个任务环境的元策略的新算法。我们的算法称为Moreau包络元强化学习(MEMRL)，它通过梯度优化和Moreau包络正则化的组合有效地更新策略参数，学习可以适应任务分布的元策略。Moreau包络提供了策略优化问题的平滑近似，使我们能够应用标准优化技术并收敛到适当的稳定点。我们对MEMRL算法进行了详细的分析，展示了对于非凸问题，它可以以亚线性的收敛速度达到一阶稳定点。",
    "tldr": "本文提出了一种Moreau包络元强化学习算法（MEMRL），通过利用Moreau包络代理正则化器，可以学习一个可以适应任务分布的元策略。",
    "en_tdlr": "The proposed Moreau Envelope Meta-Reinforcement Learning (MEMRL) algorithm utilizes Moreau envelope surrogate regularizers to learn a meta-policy that can adapt to the environment of each individual task, achieving sublinear convergence rate to a first-order stationary point for non-convex problems."
}