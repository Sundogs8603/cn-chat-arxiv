{
    "title": "Bactrian-X: Multilingual Replicable Instruction-Following Models with Low-Rank Adaptation. (arXiv:2305.15011v2 [cs.CL] UPDATED)",
    "abstract": "Instruction tuning has shown great promise in improving the performance of large language models. However, research on multilingual instruction tuning has been limited due to the scarcity of high-quality instruction-response datasets across different languages. To bridge this gap, we present Bactrian-X, a comprehensive multilingual parallel dataset of 3.4 million instruction-response pairs across 52 languages. Leveraging this dataset, we train a set of adapters using low-rank adaptation (LoRA), which are lightweight components that seamlessly integrate with large language models. These adapters have a substantially lower parameter count than the base model, making them easily replaceable and usable as plug-ins for different languages or language groups. Extensive experiments in various multilingual evaluation settings demonstrate that models derived from LoRA-based training over Bactrian-X outperform both the vanilla models and existing instruction-tuned models. The code and models are",
    "link": "http://arxiv.org/abs/2305.15011",
    "context": "Title: Bactrian-X: Multilingual Replicable Instruction-Following Models with Low-Rank Adaptation. (arXiv:2305.15011v2 [cs.CL] UPDATED)\nAbstract: Instruction tuning has shown great promise in improving the performance of large language models. However, research on multilingual instruction tuning has been limited due to the scarcity of high-quality instruction-response datasets across different languages. To bridge this gap, we present Bactrian-X, a comprehensive multilingual parallel dataset of 3.4 million instruction-response pairs across 52 languages. Leveraging this dataset, we train a set of adapters using low-rank adaptation (LoRA), which are lightweight components that seamlessly integrate with large language models. These adapters have a substantially lower parameter count than the base model, making them easily replaceable and usable as plug-ins for different languages or language groups. Extensive experiments in various multilingual evaluation settings demonstrate that models derived from LoRA-based training over Bactrian-X outperform both the vanilla models and existing instruction-tuned models. The code and models are",
    "path": "papers/23/05/2305.15011.json",
    "total_tokens": 954,
    "translated_title": "Bactrian-X: 具有低秩适应性的多语言可复制指令跟随模型",
    "translated_abstract": "指令调优已经显示出提升大型语言模型性能的巨大潜力。然而，由于不同语言之间高质量指令-响应数据集的稀缺性，对多语言指令调优的研究一直受到限制。为了填补这一空白，我们提出了Bactrian-X，这是一个涵盖52种语言的综合多语言并行数据集，包含340万个指令-响应对。利用这个数据集，我们使用低秩适应性（LoRA）训练了一组适配器，它们是轻量级组件，能够无缝集成到大型语言模型中。这些适配器的参数数目显著低于基础模型，使它们可以轻松替换，并用作不同语言或语言组的插件。在各种多语言评估设置下进行的广泛实验证明，基于Bactrian-X上的LoRA训练获得的模型优于纯模型和现有的指令调优模型。代码和模型已经发布。",
    "tldr": "Bactrian-X是一个多语言可复制指令跟随模型，利用低秩适应性（LoRA）训练，具有低参数数量、易于替换的特点。在综合多语言评估设置中，Bactrian-X模型在性能上优于纯模型和现有的指令调优模型。",
    "en_tdlr": "Bactrian-X is a multilingual replicable instruction-following model trained using low-rank adaptation (LoRA), with low parameter count and easy replaceability. In extensive multilingual evaluation settings, Bactrian-X models outperform vanilla models and existing instruction-tuned models."
}