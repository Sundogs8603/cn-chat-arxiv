{
    "title": "Generating Data for Symbolic Language with Large Language Models. (arXiv:2305.13917v1 [cs.CL])",
    "abstract": "While large language models (LLMs) bring not only performance but also complexity, recent work has started to turn LLMs into data generators rather than task inferencers, where another affordable task model is trained for efficient deployment and inference. However, such an approach has primarily been applied to natural language tasks and has not yet been explored for symbolic language tasks with complex structured outputs (e.g., semantic parsing and code generation). In this paper, we propose SymGen which utilizes LLMs for generating various annotation-expensive symbolic language data. SymGen consists of an informative prompt to steer generation and an agreement-based verifier to improve data correctness. We conduct extensive experiments on six symbolic language tasks across various settings. Compared with the LLMs, we demonstrate the 1\\%-sized task model can achieve comparable or better performance, largely cutting inference and deployment costs. We also show that generated data with",
    "link": "http://arxiv.org/abs/2305.13917",
    "context": "Title: Generating Data for Symbolic Language with Large Language Models. (arXiv:2305.13917v1 [cs.CL])\nAbstract: While large language models (LLMs) bring not only performance but also complexity, recent work has started to turn LLMs into data generators rather than task inferencers, where another affordable task model is trained for efficient deployment and inference. However, such an approach has primarily been applied to natural language tasks and has not yet been explored for symbolic language tasks with complex structured outputs (e.g., semantic parsing and code generation). In this paper, we propose SymGen which utilizes LLMs for generating various annotation-expensive symbolic language data. SymGen consists of an informative prompt to steer generation and an agreement-based verifier to improve data correctness. We conduct extensive experiments on six symbolic language tasks across various settings. Compared with the LLMs, we demonstrate the 1\\%-sized task model can achieve comparable or better performance, largely cutting inference and deployment costs. We also show that generated data with",
    "path": "papers/23/05/2305.13917.json",
    "total_tokens": 946,
    "translated_title": "利用大型语言模型生成符号语言数据",
    "translated_abstract": "尽管大型语言模型（LLMs）带来了性能提升，但也增加了复杂性。最近的研究开始将LLMs转换为数据生成器而不是任务推理器，通过训练另一个可负担的任务模型以实现高效部署和推理。然而，这种方法主要被应用于自然语言任务，并且尚未探索用于具有复杂结构输出（例如语义解析和代码生成）的符号语言任务。本文提出了SymGen，利用LLMs生成各种注释昂贵的符号语言数据。SymGen由信息提示和基于协议的验证器组成，以提高数据的正确性。我们在各种设置下对六个符号语言任务进行了大量实验。与LLMs相比，我们证明1\\%大小的任务模型可以实现相当或更好的性能，大大降低了推理和部署成本。我们还展示了使用SymGen生成数据可以提高符号语言任务的性能和通用性。",
    "tldr": "符号语言任务中，利用大型语言模型（LLMs）生成数据的方法被提出。SymGen由信息提示和基于协议的验证器组成，可以生成各种注释昂贵的符号语言数据。相对于LLMs，使用1%大小的任务模型性能相当或更好，大幅削减了推理和部署成本。使用SymGen生成数据可以提高符号语言任务的性能和通用性。",
    "en_tdlr": "This paper proposes SymGen, a method for utilizing large language models (LLMs) to generate annotation-expensive symbolic language data for tasks such as semantic parsing and code generation. The generated data can achieve comparable or better performance with a 1% sized task model, greatly reducing inference and deployment costs. SymGen consists of an informative prompt and an agreement-based verifier to improve data correctness."
}