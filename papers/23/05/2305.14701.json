{
    "title": "Modeling rapid language learning by distilling Bayesian priors into artificial neural networks. (arXiv:2305.14701v1 [cs.CL])",
    "abstract": "Humans can learn languages from remarkably little experience. Developing computational models that explain this ability has been a major challenge in cognitive science. Bayesian models that build in strong inductive biases factors that guide generalization - have been successful at explaining how humans might generalize from few examples in controlled settings but are usually too restrictive to be tractably applied to more naturalistic data. By contrast, neural networks have flexible representations that allow them to learn well from naturalistic data but require many more examples than humans receive. We show that learning from limited naturalistic data is possible with an approach that combines the strong inductive biases of a Bayesian model with the flexible representations of a neural network. This approach works by distilling a Bayesian model's biases into a neural network. Like a Bayesian model, the resulting system can learn formal linguistic patterns from a small number of ex",
    "link": "http://arxiv.org/abs/2305.14701",
    "context": "Title: Modeling rapid language learning by distilling Bayesian priors into artificial neural networks. (arXiv:2305.14701v1 [cs.CL])\nAbstract: Humans can learn languages from remarkably little experience. Developing computational models that explain this ability has been a major challenge in cognitive science. Bayesian models that build in strong inductive biases factors that guide generalization - have been successful at explaining how humans might generalize from few examples in controlled settings but are usually too restrictive to be tractably applied to more naturalistic data. By contrast, neural networks have flexible representations that allow them to learn well from naturalistic data but require many more examples than humans receive. We show that learning from limited naturalistic data is possible with an approach that combines the strong inductive biases of a Bayesian model with the flexible representations of a neural network. This approach works by distilling a Bayesian model's biases into a neural network. Like a Bayesian model, the resulting system can learn formal linguistic patterns from a small number of ex",
    "path": "papers/23/05/2305.14701.json",
    "total_tokens": 869,
    "translated_title": "论文标题：将贝叶斯先验注入神经网络中进行快速语言学习建模",
    "translated_abstract": "人类可以从极少的经验中学习语言。开发能够解释这种能力的计算模型一直是认知科学的一项重大挑战。贝叶斯模型将指导概括的强烈归纳偏见因素结合起来，成功地解释了人类如何从少数控制环境中的例子中进行归纳，但通常过于严格而无法应用于更自然的数据。相比之下，神经网络具有灵活的表示形式，使它们能够很好地从自然数据中进行学习，但需要比人类接收到的更多的示例。我们展示了一种方法，通过将贝叶斯模型的偏见注入神经网络中，从有限的自然数据中进行学习是可能的。与贝叶斯模型一样，结果系统可以从少量的示例中学习形式语言模式，同时仍保持灵活性，以推广到新的示例。",
    "tldr": "本文介绍了一种将贝叶斯模型的归纳偏见注入神经网络中的方法，可以从有限的自然数据中进行学习，同时具有灵活性以推广到新的示例。",
    "en_tdlr": "This paper presents a method of distilling the inductive biases of a Bayesian model into a neural network, allowing for learning from limited naturalistic data while maintaining flexibility to generalize to new examples."
}