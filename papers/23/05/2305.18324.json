{
    "title": "Regex-augmented Domain Transfer Topic Classification based on a Pre-trained Language Model: An application in Financial Domain. (arXiv:2305.18324v1 [cs.CL])",
    "abstract": "A common way to use large pre-trained language models for downstream tasks is to fine tune them using additional layers. This may not work well if downstream domain is a specialized domain whereas the large language model has been pre-trained on a generic corpus. In this paper, we discuss the use of regular expression patterns employed as features for domain knowledge during the process of fine tuning, in addition to domain specific text. Our experiments on real scenario production data show that this method of fine tuning improves the downstream text classification tasks as compared to fine tuning only on domain specific text. We also show that the use of attention network for fine tuning improves results compared to simple linear layers.",
    "link": "http://arxiv.org/abs/2305.18324",
    "context": "Title: Regex-augmented Domain Transfer Topic Classification based on a Pre-trained Language Model: An application in Financial Domain. (arXiv:2305.18324v1 [cs.CL])\nAbstract: A common way to use large pre-trained language models for downstream tasks is to fine tune them using additional layers. This may not work well if downstream domain is a specialized domain whereas the large language model has been pre-trained on a generic corpus. In this paper, we discuss the use of regular expression patterns employed as features for domain knowledge during the process of fine tuning, in addition to domain specific text. Our experiments on real scenario production data show that this method of fine tuning improves the downstream text classification tasks as compared to fine tuning only on domain specific text. We also show that the use of attention network for fine tuning improves results compared to simple linear layers.",
    "path": "papers/23/05/2305.18324.json",
    "total_tokens": 794,
    "translated_title": "基于预训练语言模型的正则表达式增强的领域迁移主题分类：金融领域中的应用",
    "translated_abstract": "使用大型预训练语言模型进行下游任务的常见方法是使用额外的层进行微调。但当下游领域是一个专业领域而大型语言模型已经在通用语料库上进行了预训练时，这种方法可能效果不佳。本文讨论了在微调过程中使用正则表达式模式作为领域知识的特征，以及领域特定文本。我们在实际生产数据上的实验表明，与仅在领域特定文本上进行微调相比，这种微调方法改善了下游文本分类任务的表现。我们还展示了使用注意力网络进行微调比简单的线性层获得更好的结果。",
    "tldr": "本文介绍了将正则表达式模式作为领域知识特征与领域特定文本一起用于微调预训练语言模型的方法，通过在金融领域中实验，证明这种方法可以改善下游文本分类任务的表现。",
    "en_tdlr": "This paper discusses the approach of using regular expression patterns as features of domain knowledge during the process of fine-tuning pre-trained language models, along with domain-specific text, and shows through experiments in the financial domain that this method improves downstream text classification tasks."
}