{
    "title": "Comprehensive Solution Program Centric Pretraining for Table-and-Text Hybrid Numerical Reasoning. (arXiv:2305.07475v1 [cs.CL])",
    "abstract": "Numerical reasoning over table-and-text hybrid passages, such as financial reports, poses significant challenges and has numerous potential applications. Noise and irrelevant variables in the model input have been a hindrance to its performance. Additionally, coarse-grained supervision of the whole solution program has impeded the model's ability to learn the underlying numerical reasoning process. In this paper, we propose three pretraining tasks that operate at both the whole program and sub-program level: Variable Integrity Ranking, which guides the model to focus on useful variables; Variable Operator Prediction, which decomposes the supervision into fine-grained single operator prediction; and Variable Keyphrase Masking, which encourages the model to identify key evidence that sub-programs are derived from. Experimental results demonstrate the effectiveness of our proposed methods, surpassing transformer-based model baselines.",
    "link": "http://arxiv.org/abs/2305.07475",
    "context": "Title: Comprehensive Solution Program Centric Pretraining for Table-and-Text Hybrid Numerical Reasoning. (arXiv:2305.07475v1 [cs.CL])\nAbstract: Numerical reasoning over table-and-text hybrid passages, such as financial reports, poses significant challenges and has numerous potential applications. Noise and irrelevant variables in the model input have been a hindrance to its performance. Additionally, coarse-grained supervision of the whole solution program has impeded the model's ability to learn the underlying numerical reasoning process. In this paper, we propose three pretraining tasks that operate at both the whole program and sub-program level: Variable Integrity Ranking, which guides the model to focus on useful variables; Variable Operator Prediction, which decomposes the supervision into fine-grained single operator prediction; and Variable Keyphrase Masking, which encourages the model to identify key evidence that sub-programs are derived from. Experimental results demonstrate the effectiveness of our proposed methods, surpassing transformer-based model baselines.",
    "path": "papers/23/05/2305.07475.json",
    "total_tokens": 866,
    "translated_title": "面向表格与文本混合数值推理的解决方案全面预训练",
    "translated_abstract": "对于金融报告等表格与文本混合的语境中的数值推理，存在噪声和无关变量仍然是现实的挑战和潜在应用的难点。而粗糙的整个解决方案程序的监督阻碍了模型学习潜在的数值推理过程。在本文中，我们提出了三个预训练任务，既涉及到整个程序也涉及到子程序级别的变量完整性排序、变量运算预测和变量关键词屏蔽。这些任务鼓励模型关注有用的变量、将监督分解为细粒度的单个运算符预测和确定子程序来源的关键证据。实验结果表明了我们提出的方法的有效性，超过了基于Transformer的模型基线。",
    "tldr": "本文提出了面向表格与文本混合数值推理的全面预训练解决方案，通过变量完整性排序、变量运算预测和变量关键词屏蔽等任务鼓励模型关注有用的变量和确定子程序来源的关键证据，实验结果超过了基于Transformer的模型基线。",
    "en_tdlr": "This paper proposes a comprehensive pretraining solution for table-and-text hybrid numerical reasoning by providing tasks such as variable integrity ranking, variable operator prediction, and variable keyphrase masking that encourage the model to focus on useful variables and identify key evidence for sub-programs, resulting in better performance than transformer-based model baselines."
}