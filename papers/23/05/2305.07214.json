{
    "title": "MMG-Ego4D: Multi-Modal Generalization in Egocentric Action Recognition. (arXiv:2305.07214v1 [cs.CV])",
    "abstract": "In this paper, we study a novel problem in egocentric action recognition, which we term as \"Multimodal Generalization\" (MMG). MMG aims to study how systems can generalize when data from certain modalities is limited or even completely missing. We thoroughly investigate MMG in the context of standard supervised action recognition and the more challenging few-shot setting for learning new action categories. MMG consists of two novel scenarios, designed to support security, and efficiency considerations in real-world applications: (1) missing modality generalization where some modalities that were present during the train time are missing during the inference time, and (2) cross-modal zero-shot generalization, where the modalities present during the inference time and the training time are disjoint. To enable this investigation, we construct a new dataset MMG-Ego4D containing data points with video, audio, and inertial motion sensor (IMU) modalities. Our dataset is derived from Ego4D data",
    "link": "http://arxiv.org/abs/2305.07214",
    "context": "Title: MMG-Ego4D: Multi-Modal Generalization in Egocentric Action Recognition. (arXiv:2305.07214v1 [cs.CV])\nAbstract: In this paper, we study a novel problem in egocentric action recognition, which we term as \"Multimodal Generalization\" (MMG). MMG aims to study how systems can generalize when data from certain modalities is limited or even completely missing. We thoroughly investigate MMG in the context of standard supervised action recognition and the more challenging few-shot setting for learning new action categories. MMG consists of two novel scenarios, designed to support security, and efficiency considerations in real-world applications: (1) missing modality generalization where some modalities that were present during the train time are missing during the inference time, and (2) cross-modal zero-shot generalization, where the modalities present during the inference time and the training time are disjoint. To enable this investigation, we construct a new dataset MMG-Ego4D containing data points with video, audio, and inertial motion sensor (IMU) modalities. Our dataset is derived from Ego4D data",
    "path": "papers/23/05/2305.07214.json",
    "total_tokens": 993,
    "translated_title": "MMG-Ego4D: 基于多模态泛化的自我中心动作识别",
    "translated_abstract": "本文研究自我中心动作识别中的一个新问题，命名为“多模态泛化”(MMG)。MMG旨在研究当某些模态的数据受到限制甚至完全缺失时，系统如何进行泛化。我们在标准监督动作识别和学习新的动作分类的更具挑战性的少样本场景下彻底研究了MMG。MMG包括两个新的场景，旨在支持实际应用中的安全和效率考虑：(1)缺失模态泛化，在推断时一些在训练时存在的模态缺失了，(2)跨模态零样本泛化，在推断时和训练时的模态是不相交的。为了进行这项调查，我们构建了一个新的数据集MMG-Ego4D，其中包含视频、音频和惯性运动传感器(IMU)模态的数据点。我们的数据集源于Ego4D数据集。",
    "tldr": "本文研究了自我中心动作识别中的多模态泛化问题，构建了新的数据集MMG-Ego4D，其中包含视频、音频和惯性运动传感器(IMU)模态。我们在标准监督动作识别和学习新的动作分类的少样本场景下对MMG进行了彻底的研究。",
    "en_tdlr": "This paper studies the problem of multimodal generalization in egocentric action recognition, and constructs a new dataset MMG-Ego4D containing video, audio, and inertial motion sensor (IMU) modalities. Thorough investigation is conducted in both standard supervised action recognition and the more challenging few-shot setting for learning new action categories. Two novel scenarios, missing modality generalization and cross-modal zero-shot generalization, are proposed to support security and efficiency considerations in real-world applications."
}