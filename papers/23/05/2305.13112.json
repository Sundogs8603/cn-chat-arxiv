{
    "title": "Rethinking the Evaluation for Conversational Recommendation in the Era of Large Language Models. (arXiv:2305.13112v2 [cs.CL] UPDATED)",
    "abstract": "The recent success of large language models (LLMs) has shown great potential to develop more powerful conversational recommender systems (CRSs), which rely on natural language conversations to satisfy user needs. In this paper, we embark on an investigation into the utilization of ChatGPT for conversational recommendation, revealing the inadequacy of the existing evaluation protocol. It might over-emphasize the matching with the ground-truth items or utterances generated by human annotators, while neglecting the interactive nature of being a capable CRS. To overcome the limitation, we further propose an interactive Evaluation approach based on LLMs named iEvaLM that harnesses LLM-based user simulators. Our evaluation approach can simulate various interaction scenarios between users and systems. Through the experiments on two publicly available CRS datasets, we demonstrate notable improvements compared to the prevailing evaluation protocol. Furthermore, we emphasize the evaluation of ex",
    "link": "http://arxiv.org/abs/2305.13112",
    "context": "Title: Rethinking the Evaluation for Conversational Recommendation in the Era of Large Language Models. (arXiv:2305.13112v2 [cs.CL] UPDATED)\nAbstract: The recent success of large language models (LLMs) has shown great potential to develop more powerful conversational recommender systems (CRSs), which rely on natural language conversations to satisfy user needs. In this paper, we embark on an investigation into the utilization of ChatGPT for conversational recommendation, revealing the inadequacy of the existing evaluation protocol. It might over-emphasize the matching with the ground-truth items or utterances generated by human annotators, while neglecting the interactive nature of being a capable CRS. To overcome the limitation, we further propose an interactive Evaluation approach based on LLMs named iEvaLM that harnesses LLM-based user simulators. Our evaluation approach can simulate various interaction scenarios between users and systems. Through the experiments on two publicly available CRS datasets, we demonstrate notable improvements compared to the prevailing evaluation protocol. Furthermore, we emphasize the evaluation of ex",
    "path": "papers/23/05/2305.13112.json",
    "total_tokens": 946,
    "translated_title": "在大语言模型时代重新思考对话型推荐系统的评估",
    "translated_abstract": "最近大语言模型（LLMs）的成功表明其在发展更强大的对话型推荐系统（CRSs）方面具有巨大潜力，这些系统依赖于自然语言对话来满足用户需求。本文探讨了利用ChatGPT进行对话型推荐的可行性，并揭示了现有评估协议的不足之处。现有评估协议可能过分强调与由人类标注者生成的地面真实物品或话语的匹配，而忽视了作为一种有能力的CRS的交互性质。为了克服这种局限性，我们进一步提出了一种基于LLMs的交互式评估方法，名为iEvaLM，该方法利用了基于LLMs的用户模拟器。我们的评估方法可以模拟用户和系统之间的各种交互场景。通过对两个公开可用的CRS数据集进行实验，我们证明了与流行的评估协议相比的显著改进。此外，我们强调了外部知识的评估。",
    "tldr": "本文重新思考了大语言模型时代下对话型推荐系统的评估问题，提出了一种基于大语言模型的交互式评估方法iEvaLM，通过实验证明了该方法相较于现有评估协议具有显著的改进，并强调了对外部知识的评估。",
    "en_tdlr": "This paper rethinks the evaluation of conversational recommendation systems in the era of large language models (LLMs). It proposes an interactive evaluation method called iEvaLM based on LLMs, showing notable improvements compared to existing evaluation protocols. Emphasis is also placed on the evaluation of external knowledge."
}