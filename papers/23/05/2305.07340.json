{
    "title": "MedGPTEval: A Dataset and Benchmark to Evaluate Responses of Large Language Models in Medicine. (arXiv:2305.07340v1 [cs.CL])",
    "abstract": "METHODS: First, a set of evaluation criteria is designed based on a comprehensive literature review. Second, existing candidate criteria are optimized for using a Delphi method by five experts in medicine and engineering. Third, three clinical experts design a set of medical datasets to interact with LLMs. Finally, benchmarking experiments are conducted on the datasets. The responses generated by chatbots based on LLMs are recorded for blind evaluations by five licensed medical experts. RESULTS: The obtained evaluation criteria cover medical professional capabilities, social comprehensive capabilities, contextual capabilities, and computational robustness, with sixteen detailed indicators. The medical datasets include twenty-seven medical dialogues and seven case reports in Chinese. Three chatbots are evaluated, ChatGPT by OpenAI, ERNIE Bot by Baidu Inc., and Doctor PuJiang (Dr. PJ) by Shanghai Artificial Intelligence Laboratory. Experimental results show that Dr. PJ outperforms ChatGP",
    "link": "http://arxiv.org/abs/2305.07340",
    "context": "Title: MedGPTEval: A Dataset and Benchmark to Evaluate Responses of Large Language Models in Medicine. (arXiv:2305.07340v1 [cs.CL])\nAbstract: METHODS: First, a set of evaluation criteria is designed based on a comprehensive literature review. Second, existing candidate criteria are optimized for using a Delphi method by five experts in medicine and engineering. Third, three clinical experts design a set of medical datasets to interact with LLMs. Finally, benchmarking experiments are conducted on the datasets. The responses generated by chatbots based on LLMs are recorded for blind evaluations by five licensed medical experts. RESULTS: The obtained evaluation criteria cover medical professional capabilities, social comprehensive capabilities, contextual capabilities, and computational robustness, with sixteen detailed indicators. The medical datasets include twenty-seven medical dialogues and seven case reports in Chinese. Three chatbots are evaluated, ChatGPT by OpenAI, ERNIE Bot by Baidu Inc., and Doctor PuJiang (Dr. PJ) by Shanghai Artificial Intelligence Laboratory. Experimental results show that Dr. PJ outperforms ChatGP",
    "path": "papers/23/05/2305.07340.json",
    "total_tokens": 824,
    "tldr": "MedGPTEval提出了一组医学专业能力、社交综合能力、上下文能力和计算稳健性的评估标准，并设计了一组中文医学数据集，用于评估三个聊天机器人的性能，包括ChatGPT、ERNIE Bot和Doctor PuJiang（Dr. PJ）。",
    "en_tdlr": "MedGPTEval proposes a set of evaluation criteria covering medical professional capabilities, social comprehensive capabilities, contextual capabilities, and computational robustness, and designs a set of medical datasets in Chinese to evaluate the performance of three chatbots, including ChatGPT, ERNIE Bot, and Doctor PuJiang (Dr. PJ)."
}