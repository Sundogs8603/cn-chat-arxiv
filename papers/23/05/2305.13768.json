{
    "title": "One-step differentiation of iterative algorithms. (arXiv:2305.13768v1 [math.OC])",
    "abstract": "In appropriate frameworks, automatic differentiation is transparent to the user at the cost of being a significant computational burden when the number of operations is large. For iterative algorithms, implicit differentiation alleviates this issue but requires custom implementation of Jacobian evaluation. In this paper, we study one-step differentiation, also known as Jacobian-free backpropagation, a method as easy as automatic differentiation and as performant as implicit differentiation for fast algorithms (e.g., superlinear optimization methods). We provide a complete theoretical approximation analysis with specific examples (Newton's method, gradient descent) along with its consequences in bilevel optimization. Several numerical examples illustrate the well-foundness of the one-step estimator.",
    "link": "http://arxiv.org/abs/2305.13768",
    "context": "Title: One-step differentiation of iterative algorithms. (arXiv:2305.13768v1 [math.OC])\nAbstract: In appropriate frameworks, automatic differentiation is transparent to the user at the cost of being a significant computational burden when the number of operations is large. For iterative algorithms, implicit differentiation alleviates this issue but requires custom implementation of Jacobian evaluation. In this paper, we study one-step differentiation, also known as Jacobian-free backpropagation, a method as easy as automatic differentiation and as performant as implicit differentiation for fast algorithms (e.g., superlinear optimization methods). We provide a complete theoretical approximation analysis with specific examples (Newton's method, gradient descent) along with its consequences in bilevel optimization. Several numerical examples illustrate the well-foundness of the one-step estimator.",
    "path": "papers/23/05/2305.13768.json",
    "total_tokens": 741,
    "translated_title": "迭代算法的一步微分",
    "translated_abstract": "在适当的框架中，自动微分对用户透明，但在操作数量大时代价昂贵。对于迭代算法，隐式微分可以缓解此问题，但需要自定义实现雅各比矩阵评估。在本文中，我们研究了一步微分，也称为无雅各比反向传播，这是一种像自动微分一样简单且像隐式微分一样高效的方法，适用于快速算法（例如，超线性优化方法）。我们提供了完整的理论近似分析和具体示例（牛顿法，梯度下降），以及在双层优化中的应用。几个数字示例说明了一步估计器的合理性。",
    "tldr": "本文研究了一种简单易实现的方法--一步微分用于快速算法中，能够像自动微分一样简单，像隐式微分一样高效，减少计算量，对于双层优化有许多应用。",
    "en_tdlr": "This paper studies a simple and easy-to-implement method called one-step differentiation for fast algorithms, which can reduce computational burden like automatic differentiation and be performant like implicit differentiation. It has many applications in bilevel optimization."
}