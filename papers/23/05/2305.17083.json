{
    "title": "A Policy Gradient Method for Confounded POMDPs. (arXiv:2305.17083v1 [stat.ML])",
    "abstract": "In this paper, we propose a policy gradient method for confounded partially observable Markov decision processes (POMDPs) with continuous state and observation spaces in the offline setting. We first establish a novel identification result to non-parametrically estimate any history-dependent policy gradient under POMDPs using the offline data. The identification enables us to solve a sequence of conditional moment restrictions and adopt the min-max learning procedure with general function approximation for estimating the policy gradient. We then provide a finite-sample non-asymptotic bound for estimating the gradient uniformly over a pre-specified policy class in terms of the sample size, length of horizon, concentratability coefficient and the measure of ill-posedness in solving the conditional moment restrictions. Lastly, by deploying the proposed gradient estimation in the gradient ascent algorithm, we show the global convergence of the proposed algorithm in finding the history-depe",
    "link": "http://arxiv.org/abs/2305.17083",
    "context": "Title: A Policy Gradient Method for Confounded POMDPs. (arXiv:2305.17083v1 [stat.ML])\nAbstract: In this paper, we propose a policy gradient method for confounded partially observable Markov decision processes (POMDPs) with continuous state and observation spaces in the offline setting. We first establish a novel identification result to non-parametrically estimate any history-dependent policy gradient under POMDPs using the offline data. The identification enables us to solve a sequence of conditional moment restrictions and adopt the min-max learning procedure with general function approximation for estimating the policy gradient. We then provide a finite-sample non-asymptotic bound for estimating the gradient uniformly over a pre-specified policy class in terms of the sample size, length of horizon, concentratability coefficient and the measure of ill-posedness in solving the conditional moment restrictions. Lastly, by deploying the proposed gradient estimation in the gradient ascent algorithm, we show the global convergence of the proposed algorithm in finding the history-depe",
    "path": "papers/23/05/2305.17083.json",
    "total_tokens": 928,
    "translated_title": "一种针对混淆部分可观测马尔可夫决策过程的策略梯度方法",
    "translated_abstract": "本文提出了一种针对具有连续状态和观察空间的混淆部分可观测马尔可夫决策过程（POMDP）的策略梯度方法，在离线设置下使用。我们首先建立了一个新颖的识别结果，以在离线数据下非参数地估计POMDP中的任何历史依赖策略梯度。识别结果使我们能够解决一系列条件矩限制，并采用具有一般函数逼近的最小最大学习过程来估计策略梯度。然后，我们针对预先指定的策略类提供了一个有限样本的非渐近估计界限，以了解样本大小、时间长度、集中度系数和求解条件矩限制的伪正则度量对于均匀估计梯度的影响。最后，通过在梯度上升算法中使用所提出的梯度估计，我们展示了所提出的算法在找到历史依赖性策略梯度方面的全局收敛性。",
    "tldr": "本文提出了一种针对混淆部分可观测马尔可夫决策过程的新型策略梯度方法，该方法在离线设置下可同时处理连续状态和观察空间，具有高效性和准确性。",
    "en_tdlr": "This paper proposes a novel policy gradient method for confounded partially observable Markov decision processes, which can handle both continuous state and observation spaces in the offline setting with high efficiency and accuracy."
}