{
    "title": "A Bootstrap Algorithm for Fast Supervised Learning. (arXiv:2305.03099v1 [cs.LG])",
    "abstract": "Training a neural network (NN) typically relies on some type of curve-following method, such as gradient descent (GD) (and stochastic gradient descent (SGD)), ADADELTA, ADAM or limited memory algorithms. Convergence for these algorithms usually relies on having access to a large quantity of observations in order to achieve a high level of accuracy and, with certain classes of functions, these algorithms could take multiple epochs of data points to catch on. Herein, a different technique with the potential of achieving dramatically better speeds of convergence, especially for shallow networks, is explored: it does not curve-follow but rather relies on 'decoupling' hidden layers and on updating their weighted connections through bootstrapping, resampling and linear regression. By utilizing resampled observations, the convergence of this process is empirically shown to be remarkably fast and to require a lower amount of data points: in particular, our experiments show that one needs a fra",
    "link": "http://arxiv.org/abs/2305.03099",
    "context": "Title: A Bootstrap Algorithm for Fast Supervised Learning. (arXiv:2305.03099v1 [cs.LG])\nAbstract: Training a neural network (NN) typically relies on some type of curve-following method, such as gradient descent (GD) (and stochastic gradient descent (SGD)), ADADELTA, ADAM or limited memory algorithms. Convergence for these algorithms usually relies on having access to a large quantity of observations in order to achieve a high level of accuracy and, with certain classes of functions, these algorithms could take multiple epochs of data points to catch on. Herein, a different technique with the potential of achieving dramatically better speeds of convergence, especially for shallow networks, is explored: it does not curve-follow but rather relies on 'decoupling' hidden layers and on updating their weighted connections through bootstrapping, resampling and linear regression. By utilizing resampled observations, the convergence of this process is empirically shown to be remarkably fast and to require a lower amount of data points: in particular, our experiments show that one needs a fra",
    "path": "papers/23/05/2305.03099.json",
    "total_tokens": 858,
    "translated_title": "一种用于快速有监督学习的Bootstrap算法",
    "translated_abstract": "训练神经网络（NN）通常依赖某种类型的曲线跟随方法，例如梯度下降（GD）（和随机梯度下降（SGD）），ADADELTA，ADAM或有限内存算法。这些算法的收敛通常依赖于访问大量的观测值以实现高精度，并且对于某些函数类，这些算法可能需要多个epoch的数据点才能进行。本文探讨了一种不同的技术，可以实现更快的收敛速度，尤其是对于浅层的网络而言。它不是曲线跟随，而是依赖于“分离”隐藏层并通过自助法、重抽样和线性回归来更新它们的加权连接。通过利用重抽样的观测值，本方法的收敛被实证地显示出快速和需要更少的数据点：特别是，我们的实验表明，我们只需要少量的数据点即可。",
    "tldr": "本文探讨了一种Bootstrap算法，该算法可以通过自助法、重抽样和线性回归来更新隐藏层的加权连接，从而达到更快的收敛速度。",
    "en_tdlr": "This paper explores a Bootstrap algorithm for fast supervised learning, which updates the weighted connections of hidden layers through bootstrapping, resampling, and linear regression, achieving remarkably fast convergence with fewer data points."
}