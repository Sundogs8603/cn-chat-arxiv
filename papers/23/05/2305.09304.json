{
    "title": "OmniSafe: An Infrastructure for Accelerating Safe Reinforcement Learning Research. (arXiv:2305.09304v1 [cs.LG])",
    "abstract": "AI systems empowered by reinforcement learning (RL) algorithms harbor the immense potential to catalyze societal advancement, yet their deployment is often impeded by significant safety concerns. Particularly in safety-critical applications, researchers have raised concerns about unintended harms or unsafe behaviors of unaligned RL agents. The philosophy of safe reinforcement learning (SafeRL) is to align RL agents with harmless intentions and safe behavioral patterns. In SafeRL, agents learn to develop optimal policies by receiving feedback from the environment, while also fulfilling the requirement of minimizing the risk of unintended harm or unsafe behavior. However, due to the intricate nature of SafeRL algorithm implementation, combining methodologies across various domains presents a formidable challenge. This had led to an absence of a cohesive and efficacious learning framework within the contemporary SafeRL research milieu. In this work, we introduce a foundational framework d",
    "link": "http://arxiv.org/abs/2305.09304",
    "context": "Title: OmniSafe: An Infrastructure for Accelerating Safe Reinforcement Learning Research. (arXiv:2305.09304v1 [cs.LG])\nAbstract: AI systems empowered by reinforcement learning (RL) algorithms harbor the immense potential to catalyze societal advancement, yet their deployment is often impeded by significant safety concerns. Particularly in safety-critical applications, researchers have raised concerns about unintended harms or unsafe behaviors of unaligned RL agents. The philosophy of safe reinforcement learning (SafeRL) is to align RL agents with harmless intentions and safe behavioral patterns. In SafeRL, agents learn to develop optimal policies by receiving feedback from the environment, while also fulfilling the requirement of minimizing the risk of unintended harm or unsafe behavior. However, due to the intricate nature of SafeRL algorithm implementation, combining methodologies across various domains presents a formidable challenge. This had led to an absence of a cohesive and efficacious learning framework within the contemporary SafeRL research milieu. In this work, we introduce a foundational framework d",
    "path": "papers/23/05/2305.09304.json",
    "total_tokens": 909,
    "translated_title": "OmniSafe：一种加速安全强化学习研究的基础设施",
    "translated_abstract": "强化学习（RL）算法赋能的AI系统有着促进社会进步的巨大潜力，但它们的部署常常受到重大安全隐患的阻碍。尤其是在安全关键的应用中，研究人员已经引起了对不受约束的RL代理的意外伤害或不安全行为的担忧。安全强化学习（SafeRL）的理念是将RL代理与无害意图和安全行为模式相一致。在SafeRL中，代理通过从环境中接收反馈来学习开发优化策略，同时也满足了将意外伤害或不安全行为的风险最小化的要求。然而，由于SafeRL算法实现的复杂性，跨越各个领域的方法的结合提出了巨大的挑战。这导致了当代SafeRL研究环境中缺乏一个协调和有效的学习框架。在这项工作中，我们介绍了一个基础框架：OmniSafe，用于加速安全强化学习研究。",
    "tldr": "OmniSafe是一种基础设施，用于加速安全强化学习研究，帮助解决当代SafeRL研究环境中缺乏协调和有效的学习框架的问题。",
    "en_tdlr": "OmniSafe is an infrastructure for accelerating safe reinforcement learning research, helping to address the problem of the lack of a coordinated and effective learning framework in contemporary SafeRL research environments."
}