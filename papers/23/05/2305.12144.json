{
    "title": "DiffCap: Exploring Continuous Diffusion on Image Captioning. (arXiv:2305.12144v1 [cs.CV])",
    "abstract": "Current image captioning works usually focus on generating descriptions in an autoregressive manner. However, there are limited works that focus on generating descriptions non-autoregressively, which brings more decoding diversity. Inspired by the success of diffusion models on generating natural-looking images, we propose a novel method DiffCap to apply continuous diffusions on image captioning. Unlike image generation where the output is fixed-size and continuous, image description length varies with discrete tokens. Our method transforms discrete tokens in a natural way and applies continuous diffusion on them to successfully fuse extracted image features for diffusion caption generation. Our experiments on COCO dataset demonstrate that our method uses a much simpler structure to achieve comparable results to the previous non-autoregressive works. Apart from quality, an intriguing property of DiffCap is its high diversity during generation, which is missing from many autoregressive ",
    "link": "http://arxiv.org/abs/2305.12144",
    "context": "Title: DiffCap: Exploring Continuous Diffusion on Image Captioning. (arXiv:2305.12144v1 [cs.CV])\nAbstract: Current image captioning works usually focus on generating descriptions in an autoregressive manner. However, there are limited works that focus on generating descriptions non-autoregressively, which brings more decoding diversity. Inspired by the success of diffusion models on generating natural-looking images, we propose a novel method DiffCap to apply continuous diffusions on image captioning. Unlike image generation where the output is fixed-size and continuous, image description length varies with discrete tokens. Our method transforms discrete tokens in a natural way and applies continuous diffusion on them to successfully fuse extracted image features for diffusion caption generation. Our experiments on COCO dataset demonstrate that our method uses a much simpler structure to achieve comparable results to the previous non-autoregressive works. Apart from quality, an intriguing property of DiffCap is its high diversity during generation, which is missing from many autoregressive ",
    "path": "papers/23/05/2305.12144.json",
    "total_tokens": 858,
    "translated_title": "DiffCap：探索图像字幕生成中的连续扩散",
    "translated_abstract": "当前图像字幕生成主要集中在以自回归方式生成描述上。然而，针对非自回归方式生成描述的研究仍然有限，这种方式可以带来更多的解码多样性。受扩散模型在生成自然图像方面的成功启发，我们提出了一种新方法DiffCap，在图像字幕生成中应用连续扩散。与输出大小连续的图像生成不同，图像描述长度随着离散标记而变化。我们的方法以自然的方式转换离散标记，并对它们应用连续扩散，以成功融合提取的图像特征进行扩散字幕生成。我们在COCO数据集上的实验表明，我们的方法使用更简单的结构，即可达到与以往非自回归作品相当的结果。除了质量，DiffCap的一个有趣特性是其在生成过程中高多样性，这在许多自回归作品中都缺少。",
    "tldr": "DiffCap通过应用连续扩散，以自然的方式转换离散标记，并成功融合提取的图像特征进行扩散字幕生成，其具有更多的解码多样性。"
}