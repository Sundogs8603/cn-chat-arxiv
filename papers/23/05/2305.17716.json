{
    "title": "InDL: A New Datasets and Benchmark for In-Diagram Logic Interpreting based on Visual Illusion. (arXiv:2305.17716v2 [cs.CV] UPDATED)",
    "abstract": "This paper introduces a novel approach to evaluating deep learning models' capacity for in-diagram logic interpretation. Leveraging the intriguing realm of visual illusions, we establish a unique dataset, InDL, designed to rigorously test and benchmark these models. Deep learning has witnessed remarkable progress in domains such as computer vision and natural language processing. However, models often stumble in tasks requiring logical reasoning due to their inherent 'black box' characteristics, which obscure the decision-making process. Our work presents a new lens to understand these models better by focusing on their handling of visual illusions -- a complex interplay of perception and logic. We utilize six classic geometric optical illusions to create a comparative framework between human and machine visual perception. This methodology offers a quantifiable measure to rank models, elucidating potential weaknesses and providing actionable insights for model improvements. Our experim",
    "link": "http://arxiv.org/abs/2305.17716",
    "context": "Title: InDL: A New Datasets and Benchmark for In-Diagram Logic Interpreting based on Visual Illusion. (arXiv:2305.17716v2 [cs.CV] UPDATED)\nAbstract: This paper introduces a novel approach to evaluating deep learning models' capacity for in-diagram logic interpretation. Leveraging the intriguing realm of visual illusions, we establish a unique dataset, InDL, designed to rigorously test and benchmark these models. Deep learning has witnessed remarkable progress in domains such as computer vision and natural language processing. However, models often stumble in tasks requiring logical reasoning due to their inherent 'black box' characteristics, which obscure the decision-making process. Our work presents a new lens to understand these models better by focusing on their handling of visual illusions -- a complex interplay of perception and logic. We utilize six classic geometric optical illusions to create a comparative framework between human and machine visual perception. This methodology offers a quantifiable measure to rank models, elucidating potential weaknesses and providing actionable insights for model improvements. Our experim",
    "path": "papers/23/05/2305.17716.json",
    "total_tokens": 822,
    "translated_title": "InDL: 基于视错觉的图中逻辑解释新数据集和基准的研究",
    "translated_abstract": "本文提出了一种新的方法来评估深度学习模型在图中逻辑解释方面的能力。通过利用有趣领域的视错觉，我们建立了一个独特的数据集InDL，旨在严格测试和基准这些模型。我们利用六个经典的几何视觉错觉，创建了一个人类和机器视觉感知的可比性框架。这种方法提供了一个可量化的衡量模型的方法，阐明了可能存在的缺陷，并提供了改进模型的行动洞察力。",
    "tldr": "本文提出了一个基于视错觉的独特数据集InDL，用于测试和评估深度学习模型的图中逻辑解释能力。利用几何光学视错觉，建立可比性框架用于阐明模型可能存在的缺陷和提供改进模型的洞察力。",
    "en_tdlr": "This paper proposes a unique dataset, InDL, based on visual illusions to test and evaluate the capacity of deep learning models for in-diagram logic interpretation. By utilizing six classic geometric optical illusions, a comparative framework is established to rank models and provide actionable insights for improvement."
}