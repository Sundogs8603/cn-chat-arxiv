{
    "title": "BigVideo: A Large-scale Video Subtitle Translation Dataset for Multimodal Machine Translation. (arXiv:2305.18326v1 [cs.CV])",
    "abstract": "We present a large-scale video subtitle translation dataset, BigVideo, to facilitate the study of multi-modality machine translation. Compared with the widely used How2 and VaTeX datasets, BigVideo is more than 10 times larger, consisting of 4.5 million sentence pairs and 9,981 hours of videos. We also introduce two deliberately designed test sets to verify the necessity of visual information: Ambiguous with the presence of ambiguous words, and Unambiguous in which the text context is self-contained for translation. To better model the common semantics shared across texts and videos, we introduce a contrastive learning method in the cross-modal encoder. Extensive experiments on the BigVideo show that: a) Visual information consistently improves the NMT model in terms of BLEU, BLEURT, and COMET on both Ambiguous and Unambiguous test sets. b) Visual information helps disambiguation, compared to the strong text baseline on terminology-targeted scores and human evaluation. Dataset and our ",
    "link": "http://arxiv.org/abs/2305.18326",
    "context": "Title: BigVideo: A Large-scale Video Subtitle Translation Dataset for Multimodal Machine Translation. (arXiv:2305.18326v1 [cs.CV])\nAbstract: We present a large-scale video subtitle translation dataset, BigVideo, to facilitate the study of multi-modality machine translation. Compared with the widely used How2 and VaTeX datasets, BigVideo is more than 10 times larger, consisting of 4.5 million sentence pairs and 9,981 hours of videos. We also introduce two deliberately designed test sets to verify the necessity of visual information: Ambiguous with the presence of ambiguous words, and Unambiguous in which the text context is self-contained for translation. To better model the common semantics shared across texts and videos, we introduce a contrastive learning method in the cross-modal encoder. Extensive experiments on the BigVideo show that: a) Visual information consistently improves the NMT model in terms of BLEU, BLEURT, and COMET on both Ambiguous and Unambiguous test sets. b) Visual information helps disambiguation, compared to the strong text baseline on terminology-targeted scores and human evaluation. Dataset and our ",
    "path": "papers/23/05/2305.18326.json",
    "total_tokens": 1065,
    "translated_title": "BigVideo:一个用于多模式机器翻译的大规模视频字幕翻译数据集",
    "translated_abstract": "我们提出了一个大规模的视频字幕翻译数据集BigVideo，以促进多模式机器翻译的研究。与广泛使用的How2和VaTeX数据集相比，BigVideo超过10倍，包括450万个句子对和9981小时的视频。我们还引入了两个有意设计的测试集来验证视觉信息的必要性：一个是一个有歧义词的不确定集合，另一个是在其中文本上下文对于翻译是自包含的明确集合。为了更好地建模文本和视频共享的共同语义，我们在跨模态编码器中引入了一种对比学习方法。在BigVideo上进行的广泛实验表明：a）视觉信息在歧义和明确的测试集上始终提高NMT模型的BLEU、BLEURT和COMET得分。b）视觉信息对于术语定位得分和人工评估而言，有助于消除歧义，与强文本基线相比。数据集和我们的翻译模型都是公开可用的。",
    "tldr": "提出了一个大规模的视频字幕翻译数据集BigVideo， 集成了4.5 million句子对和9981小时视频，设计了有歧义和明确的测试集，引入了一种对比学习方法，实验结果表明，视觉信息可以提高NMT模型的BLEU、BLEURT和COMET得分，有助于消除歧义，数据集和翻译模型公开可用。",
    "en_tdlr": "BigVideo is a large-scale video subtitle translation dataset for multi-modality machine translation, consisting of 4.5 million sentence pairs and 9,981 hours of videos. The dataset includes two deliberately designed test sets to verify the necessity of visual information. The study introduces a contrastive learning method in the cross-modal encoder to model the common semantics shared across texts and videos. The results show that visual information consistently improves the NMT model in terms of BLEU, BLEURT, and COMET, and helps disambiguation, compared to the strong text baseline on terminology-targeted scores and human evaluation. The dataset and translation model are publicly available."
}