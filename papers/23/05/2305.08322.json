{
    "title": "C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models. (arXiv:2305.08322v2 [cs.CL] UPDATED)",
    "abstract": "New NLP benchmarks are urgently needed to align with the rapid development of large language models (LLMs). We present C-Eval, the first comprehensive Chinese evaluation suite designed to assess advanced knowledge and reasoning abilities of foundation models in a Chinese context. C-Eval comprises multiple-choice questions across four difficulty levels: middle school, high school, college, and professional. The questions span 52 diverse disciplines, ranging from humanities to science and engineering. C-Eval is accompanied by C-Eval Hard, a subset of very challenging subjects in C-Eval that requires advanced reasoning abilities to solve. We conduct a comprehensive evaluation of the most advanced LLMs on C-Eval, including both English- and Chinese-oriented models. Results indicate that only GPT-4 could achieve an average accuracy of over 60%, suggesting that there is still significant room for improvement for current LLMs. We anticipate C-Eval will help analyze important strengths and sho",
    "link": "http://arxiv.org/abs/2305.08322",
    "context": "Title: C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models. (arXiv:2305.08322v2 [cs.CL] UPDATED)\nAbstract: New NLP benchmarks are urgently needed to align with the rapid development of large language models (LLMs). We present C-Eval, the first comprehensive Chinese evaluation suite designed to assess advanced knowledge and reasoning abilities of foundation models in a Chinese context. C-Eval comprises multiple-choice questions across four difficulty levels: middle school, high school, college, and professional. The questions span 52 diverse disciplines, ranging from humanities to science and engineering. C-Eval is accompanied by C-Eval Hard, a subset of very challenging subjects in C-Eval that requires advanced reasoning abilities to solve. We conduct a comprehensive evaluation of the most advanced LLMs on C-Eval, including both English- and Chinese-oriented models. Results indicate that only GPT-4 could achieve an average accuracy of over 60%, suggesting that there is still significant room for improvement for current LLMs. We anticipate C-Eval will help analyze important strengths and sho",
    "path": "papers/23/05/2305.08322.json",
    "total_tokens": 959,
    "translated_title": "C-Eval: 用于基础模型的多级多学科中文评估套件",
    "translated_abstract": "随着大型语言模型（LLM）的快速发展，迫切需要新的自然语言处理基准来评估这些模型的高级知识和推理能力。我们介绍了C-Eval，这是第一个专为中文语境下基础模型评估而设计的全面评估套件。C-Eval包含四个难度级别的选择题：初中、高中、大学和专业水平。这些题目涵盖52个不同的学科，包括人文、科学和工程学科。C-Eval还配备了C-Eval Hard，这是C-Eval中一些极具挑战性的科目，需要高级推理能力才能解决。我们对包括英文和中文模型在内的最先进的LLM在C-Eval上进行了全面评估。结果表明，只有GPT-4可以实现超过60％的平均准确率，这表明当前的LLM仍有很大的改进空间。我们期望C-Eval将有助于分析重要的优势和短板。",
    "tldr": "C-Eval是第一个专为中文基础模型评估而设计的全面套件，涵盖52个不同学科的多级别选择题和挑战性科目。评估结果表明，只有GPT-4能够达到超过60％的平均准确率，还有改进空间。",
    "en_tdlr": "C-Eval is the first comprehensive suite designed for evaluating foundation models in a Chinese context, featuring multi-level multiple-choice questions and challenging subjects across 52 different disciplines. Our evaluation of the most advanced language models, including both English and Chinese models, suggests that there is still significant room for improvement, with only GPT-4 achieving an average accuracy of over 60%."
}