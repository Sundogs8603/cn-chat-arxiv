{
    "title": "Leveraging Synthetic Targets for Machine Translation. (arXiv:2305.06155v1 [cs.CL])",
    "abstract": "In this work, we provide a recipe for training machine translation models in a limited resource setting by leveraging synthetic target data generated using a large pre-trained model. We show that consistently across different benchmarks in bilingual, multilingual, and speech translation setups, training models on synthetic targets outperforms training on the actual ground-truth data. This performance gap grows bigger with increasing limits on the amount of available resources in the form of the size of the dataset and the number of parameters in the model. We also provide preliminary analysis into whether this boost in performance is linked to ease of optimization or more deterministic nature of the predictions, and whether this paradigm leads to better out-of-distribution performance across different testing domains.",
    "link": "http://arxiv.org/abs/2305.06155",
    "context": "Title: Leveraging Synthetic Targets for Machine Translation. (arXiv:2305.06155v1 [cs.CL])\nAbstract: In this work, we provide a recipe for training machine translation models in a limited resource setting by leveraging synthetic target data generated using a large pre-trained model. We show that consistently across different benchmarks in bilingual, multilingual, and speech translation setups, training models on synthetic targets outperforms training on the actual ground-truth data. This performance gap grows bigger with increasing limits on the amount of available resources in the form of the size of the dataset and the number of parameters in the model. We also provide preliminary analysis into whether this boost in performance is linked to ease of optimization or more deterministic nature of the predictions, and whether this paradigm leads to better out-of-distribution performance across different testing domains.",
    "path": "papers/23/05/2305.06155.json",
    "total_tokens": 770,
    "translated_title": "利用合成目标进行机器翻译",
    "translated_abstract": "在本文中，我们提供了一种在有限资源情况下训练机器翻译模型的方法，该方法通过利用通过大型预训练模型生成的合成目标数据。我们表明，在双语、多语言和语音翻译设置的不同基准测试中，使用合成目标训练模型的性能优于使用实际的正确数据训练模型。这种性能差距随着可用资源的限制（数据集大小和模型参数数量）的增加而变得更大。我们还对性能提升是否与优化的便利性或预测的更确定性相关进行了初步分析，以及这种范例是否能够提高在不同测试领域的超出分布性能。",
    "tldr": "本文提供了一种利用预训练模型生成合成目标从而提高机器翻译性能的方法，并发现其在不同测试基准下的表现优于使用真实数据训练，这一方法在有限资源的情况下尤其有用。",
    "en_tdlr": "This paper proposes a method to improve machine translation performance by using synthetic target data generated by pre-trained models and shows that it outperforms using real data, especially in limited resource situations. Analysis is also conducted on the nature of this boost in performance and its potential for improved out-of-distribution performance."
}