{
    "title": "How Informative is the Approximation Error from Tensor Decomposition for Neural Network Compression?. (arXiv:2305.05318v1 [cs.LG])",
    "abstract": "Tensor decompositions have been successfully applied to compress neural networks. The compression algorithms using tensor decompositions commonly minimize the approximation error on the weights. Recent work assumes the approximation error on the weights is a proxy for the performance of the model to compress multiple layers and fine-tune the compressed model. Surprisingly, little research has systematically evaluated which approximation errors can be used to make choices regarding the layer, tensor decomposition method, and level of compression. To close this gap, we perform an experimental study to test if this assumption holds across different layers and types of decompositions, and what the effect of fine-tuning is. We include the approximation error on the features resulting from a compressed layer in our analysis to test if this provides a better proxy, as it explicitly takes the data into account. We find the approximation error on the weights has a positive correlation with the ",
    "link": "http://arxiv.org/abs/2305.05318",
    "context": "Title: How Informative is the Approximation Error from Tensor Decomposition for Neural Network Compression?. (arXiv:2305.05318v1 [cs.LG])\nAbstract: Tensor decompositions have been successfully applied to compress neural networks. The compression algorithms using tensor decompositions commonly minimize the approximation error on the weights. Recent work assumes the approximation error on the weights is a proxy for the performance of the model to compress multiple layers and fine-tune the compressed model. Surprisingly, little research has systematically evaluated which approximation errors can be used to make choices regarding the layer, tensor decomposition method, and level of compression. To close this gap, we perform an experimental study to test if this assumption holds across different layers and types of decompositions, and what the effect of fine-tuning is. We include the approximation error on the features resulting from a compressed layer in our analysis to test if this provides a better proxy, as it explicitly takes the data into account. We find the approximation error on the weights has a positive correlation with the ",
    "path": "papers/23/05/2305.05318.json",
    "total_tokens": 886,
    "tldr": "本文通过实验研究探讨了使用张量分解压缩神经网络时权重近似误差对压缩效果的指示作用，并发现其与模型的压缩率、分解方法以及微调次数都有正相关关系。"
}