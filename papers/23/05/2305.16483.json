{
    "title": "Sample Efficient Reinforcement Learning in Mixed Systems through Augmented Samples and Its Applications to Queueing Networks. (arXiv:2305.16483v1 [cs.LG])",
    "abstract": "This paper considers a class of reinforcement learning problems, which involve systems with two types of states: stochastic and pseudo-stochastic. In such systems, stochastic states follow a stochastic transition kernel while the transitions of pseudo-stochastic states are deterministic given the stochastic states/transitions. We refer to such systems as mixed systems, which are widely used in various applications, including manufacturing systems, communication networks, and queueing networks. We propose a sample efficient RL method that accelerates learning by generating augmented data samples. The proposed algorithm is data-driven and learns the policy from data samples from both real and augmented samples. This method significantly improves learning by reducing the sample complexity such that the dataset only needs to have sufficient coverage of the stochastic states. We analyze the sample complexity of the proposed method under Fitted Q Iteration (FQI) and demonstrate that the opti",
    "link": "http://arxiv.org/abs/2305.16483",
    "context": "Title: Sample Efficient Reinforcement Learning in Mixed Systems through Augmented Samples and Its Applications to Queueing Networks. (arXiv:2305.16483v1 [cs.LG])\nAbstract: This paper considers a class of reinforcement learning problems, which involve systems with two types of states: stochastic and pseudo-stochastic. In such systems, stochastic states follow a stochastic transition kernel while the transitions of pseudo-stochastic states are deterministic given the stochastic states/transitions. We refer to such systems as mixed systems, which are widely used in various applications, including manufacturing systems, communication networks, and queueing networks. We propose a sample efficient RL method that accelerates learning by generating augmented data samples. The proposed algorithm is data-driven and learns the policy from data samples from both real and augmented samples. This method significantly improves learning by reducing the sample complexity such that the dataset only needs to have sufficient coverage of the stochastic states. We analyze the sample complexity of the proposed method under Fitted Q Iteration (FQI) and demonstrate that the opti",
    "path": "papers/23/05/2305.16483.json",
    "total_tokens": 935,
    "translated_title": "增强样本下混合系统中有效强化学习及其在排队网络中的应用。",
    "translated_abstract": "本文研究了一类强化学习问题，涉及具有两种状态的系统：随机状态和伪随机状态。在这种系统中，随机状态遵循随机转移核而伪随机状态的转移是在给定随机状态/转移的情况下是确定的。我们称这样的系统为混合系统。这种系统被广泛应用于各种应用，包括制造系统、通信网络和排队网络。我们提出了一种能够通过产生增强的数据样本来加速学习的样本高效的强化学习方法。该算法是数据驱动的，并从实际和增强样本的数据样本中学习策略，从而显著改善了学习的效果。我们分析了在Fitted Q Iteration（FQI）下提出的方法的样本复杂度，并证明了学习策略的最优性差随着真实数据样本数量的增加而减少。我们将我们的方法应用于解决一类排队网络问题，其中混合状态空间自然而然地出现。",
    "tldr": "本文提出了一种在混合系统中通过增强数据样本进行学习的方法，应用于排队网络问题，能够显著提高学习效率和降低样本复杂度。",
    "en_tdlr": "This paper proposes a sample-efficient reinforcement learning method for mixed systems that accelerates learning by generating augmented data samples, and applies it to solve queueing network problems, significantly improving learning efficiency and reducing sample complexity."
}