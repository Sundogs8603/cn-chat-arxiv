{
    "title": "What Do Self-Supervised Vision Transformers Learn?. (arXiv:2305.00729v1 [cs.CV])",
    "abstract": "We present a comparative study on how and why contrastive learning (CL) and masked image modeling (MIM) differ in their representations and in their performance of downstream tasks. In particular, we demonstrate that self-supervised Vision Transformers (ViTs) have the following properties: (1) CL trains self-attentions to capture longer-range global patterns than MIM, such as the shape of an object, especially in the later layers of the ViT architecture. This CL property helps ViTs linearly separate images in their representation spaces. However, it also makes the self-attentions collapse into homogeneity for all query tokens and heads. Such homogeneity of self-attention reduces the diversity of representations, worsening scalability and dense prediction performance. (2) CL utilizes the low-frequency signals of the representations, but MIM utilizes high-frequencies. Since low- and high-frequency information respectively represent shapes and textures, CL is more shape-oriented and MIM m",
    "link": "http://arxiv.org/abs/2305.00729",
    "context": "Title: What Do Self-Supervised Vision Transformers Learn?. (arXiv:2305.00729v1 [cs.CV])\nAbstract: We present a comparative study on how and why contrastive learning (CL) and masked image modeling (MIM) differ in their representations and in their performance of downstream tasks. In particular, we demonstrate that self-supervised Vision Transformers (ViTs) have the following properties: (1) CL trains self-attentions to capture longer-range global patterns than MIM, such as the shape of an object, especially in the later layers of the ViT architecture. This CL property helps ViTs linearly separate images in their representation spaces. However, it also makes the self-attentions collapse into homogeneity for all query tokens and heads. Such homogeneity of self-attention reduces the diversity of representations, worsening scalability and dense prediction performance. (2) CL utilizes the low-frequency signals of the representations, but MIM utilizes high-frequencies. Since low- and high-frequency information respectively represent shapes and textures, CL is more shape-oriented and MIM m",
    "path": "papers/23/05/2305.00729.json",
    "total_tokens": 949,
    "translated_title": "自监督视觉变压器学习什么？",
    "translated_abstract": "本文对比了对比学习（CL）和遮蔽图像建模（MIM）在其表示和下游任务表现方面的差异，并阐述了自监督视觉变压器（ViTs）的性质。通过实验证明，CL训练自我关注力以捕捉比MIM更长程的全局模式，例如物体的形状，尤其是在ViT架构的后几层中。这使得ViTs能够在其表示空间中线性分离图像。但是，它也使得自我关注力对于所有查询标记和头部的同质性崩溃。这种自我关注力的同质性降低了表征的多样性，恶化了可扩展性和密集预测性能　。CL利用表示的低频信号，而MIM利用高频信号。由于低频和高频信息分别代表形状和质地，因此CL更加注重形状，而MIM则更加注重质地。",
    "tldr": "本文比较了对比学习和遮蔽图像建模在表示和 下游任务表现方面的差异。实验证明，自监督视觉变压器利用对比学习时能够捕捉更长程的全局模式并线性分离图像，但在自我关注力的同质性、可扩展性和密集预测性能方面存在一些问题。"
}