{
    "title": "Exploration of Unranked Items in Safe Online Learning to Re-Rank. (arXiv:2305.01202v1 [cs.IR])",
    "abstract": "Bandit algorithms for online learning to rank (OLTR) problems often aim to maximize long-term revenue by utilizing user feedback. From a practical point of view, however, such algorithms have a high risk of hurting user experience due to their aggressive exploration. Thus, there has been a rising demand for safe exploration in recent years. One approach to safe exploration is to gradually enhance the quality of an original ranking that is already guaranteed acceptable quality. In this paper, we propose a safe OLTR algorithm that efficiently exchanges one of the items in the current ranking with an item outside the ranking (i.e., an unranked item) to perform exploration. We select an unranked item optimistically to explore based on Kullback-Leibler upper confidence bounds (KL-UCB) and safely re-rank the items including the selected one. Through experiments, we demonstrate that the proposed algorithm improves long-term regret from baselines without any safety violation.",
    "link": "http://arxiv.org/abs/2305.01202",
    "context": "Title: Exploration of Unranked Items in Safe Online Learning to Re-Rank. (arXiv:2305.01202v1 [cs.IR])\nAbstract: Bandit algorithms for online learning to rank (OLTR) problems often aim to maximize long-term revenue by utilizing user feedback. From a practical point of view, however, such algorithms have a high risk of hurting user experience due to their aggressive exploration. Thus, there has been a rising demand for safe exploration in recent years. One approach to safe exploration is to gradually enhance the quality of an original ranking that is already guaranteed acceptable quality. In this paper, we propose a safe OLTR algorithm that efficiently exchanges one of the items in the current ranking with an item outside the ranking (i.e., an unranked item) to perform exploration. We select an unranked item optimistically to explore based on Kullback-Leibler upper confidence bounds (KL-UCB) and safely re-rank the items including the selected one. Through experiments, we demonstrate that the proposed algorithm improves long-term regret from baselines without any safety violation.",
    "path": "papers/23/05/2305.01202.json",
    "total_tokens": 895,
    "translated_title": "安全在线学习中未排序项目的探索与重新排序",
    "translated_abstract": "在线学习排序问题的贝叶斯算法经常试图利用用户反馈最大化长期收益。然而，从实际角度考虑，这种算法由于过于激进的探索而具有损害用户体验的高风险。因此，近年来，对安全探索的需求不断增加。本文提出了一种安全的在线学习排序算法，它通过将当前排名中的一个项目与排名外的项目（即未排序项目）高效地交换来执行探索并逐步提高原始排名的质量。我们基于Kullback-Leibler上置信度界限（KL-UCB）乐观地选择一个未排名项目进行探索，并安全地对包括所选项目在内的项目进行重新排序。通过实验，我们证明了所提出的算法可以在没有任何安全违规的情况下提高长期悔恨。",
    "tldr": "本文提出了一种安全的在线学习排序算法，通过将当前排名中的一个项目与排名外的项目高效地交换来执行探索，并基于Kullback-Leibler上置信度界限（KL-UCB）对未排序项目进行乐观选择和安全的排序，以提高长期收益。",
    "en_tdlr": "This paper proposes a safe online learning to rank algorithm which efficiently exchanges one item in the current ranking with an unranked item to perform exploration and re-ranks the items safely based on optimistic selection of unranked items using Kullback-Leibler upper confidence bounds (KL-UCB), improving long-term regret without any safety violation."
}