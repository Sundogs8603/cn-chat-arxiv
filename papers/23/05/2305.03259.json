{
    "title": "Clothes Grasping and Unfolding Based on RGB-D Semantic Segmentation. (arXiv:2305.03259v1 [cs.CV])",
    "abstract": "Clothes grasping and unfolding is a core step in robotic-assisted dressing. Most existing works leverage depth images of clothes to train a deep learning-based model to recognize suitable grasping points. These methods often utilize physics engines to synthesize depth images to reduce the cost of real labeled data collection. However, the natural domain gap between synthetic and real images often leads to poor performance of these methods on real data. Furthermore, these approaches often struggle in scenarios where grasping points are occluded by the clothing item itself. To address the above challenges, we propose a novel Bi-directional Fractal Cross Fusion Network (BiFCNet) for semantic segmentation, enabling recognition of graspable regions in order to provide more possibilities for grasping. Instead of using depth images only, we also utilize RGB images with rich color features as input to our network in which the Fractal Cross Fusion (FCF) module fuses RGB and depth data by consid",
    "link": "http://arxiv.org/abs/2305.03259",
    "context": "Title: Clothes Grasping and Unfolding Based on RGB-D Semantic Segmentation. (arXiv:2305.03259v1 [cs.CV])\nAbstract: Clothes grasping and unfolding is a core step in robotic-assisted dressing. Most existing works leverage depth images of clothes to train a deep learning-based model to recognize suitable grasping points. These methods often utilize physics engines to synthesize depth images to reduce the cost of real labeled data collection. However, the natural domain gap between synthetic and real images often leads to poor performance of these methods on real data. Furthermore, these approaches often struggle in scenarios where grasping points are occluded by the clothing item itself. To address the above challenges, we propose a novel Bi-directional Fractal Cross Fusion Network (BiFCNet) for semantic segmentation, enabling recognition of graspable regions in order to provide more possibilities for grasping. Instead of using depth images only, we also utilize RGB images with rich color features as input to our network in which the Fractal Cross Fusion (FCF) module fuses RGB and depth data by consid",
    "path": "papers/23/05/2305.03259.json",
    "total_tokens": 976,
    "translated_title": "基于RGB-D语义分割的服装抓取和展开",
    "translated_abstract": "服装的抓取和展开是机器人辅助穿衣的核心步骤。大多数现有作品利用服装的深度图像训练深度学习模型以识别合适的抓取点。然而，这些方法通常利用物理引擎来合成深度图像以减少真实标记数据的收集成本。然而，合成与真实图像之间的自然领域差距常常导致这些方法在实际数据上表现不佳。此外，这些方法通常在抓取点被服装物品本身遮挡的场景中难以处理。为了解决上述挑战，我们提出了一种新颖的双向分形交叉融合网络（BiFCNet）进行语义分割，实现对可抓取区域的识别以提供更多的抓取可能性。我们不仅使用深度图像，还利用具有丰富色彩特征的RGB图像作为网络输入，其中分形交叉融合（FCF）模块通过将RGB和深度数据融合来考虑图像信息。",
    "tldr": "本论文针对机器人辅助穿衣过程中的服装抓取和展开问题，提出了一种基于RGB-D语义分割的双向分形交叉融合网络，可以识别可抓取区域提供更多抓取可能性，并且使用RGB和深度数据融合来考虑图像信息。"
}