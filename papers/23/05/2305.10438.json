{
    "title": "IMAGINATOR: Pre-Trained Image+Text Joint Embeddings using Word-Level Grounding of Images. (arXiv:2305.10438v1 [cs.CL])",
    "abstract": "Word embeddings, i.e., semantically meaningful vector representation of words, are largely influenced by the distributional hypothesis \"You shall know a word by the company it keeps\" (Harris, 1954), whereas modern prediction-based neural network embeddings rely on design choices and hyperparameter optimization. Word embeddings like Word2Vec, GloVe etc. well capture the contextuality and real-world analogies but contemporary convolution-based image embeddings such as VGGNet, AlexNet, etc. do not capture contextual knowledge. The popular king-queen analogy does not hold true for most commonly used vision embeddings.  In this paper, we introduce a pre-trained joint embedding (JE), named IMAGINATOR, trained on 21K distinct image objects level from 1M image+text pairs. JE is a way to encode multimodal data into a vector space where the text modality serves as the ground-ing key, which the complementary modality (in this case, the image) is anchored with. IMAGINATOR encapsulates three indivi",
    "link": "http://arxiv.org/abs/2305.10438",
    "context": "Title: IMAGINATOR: Pre-Trained Image+Text Joint Embeddings using Word-Level Grounding of Images. (arXiv:2305.10438v1 [cs.CL])\nAbstract: Word embeddings, i.e., semantically meaningful vector representation of words, are largely influenced by the distributional hypothesis \"You shall know a word by the company it keeps\" (Harris, 1954), whereas modern prediction-based neural network embeddings rely on design choices and hyperparameter optimization. Word embeddings like Word2Vec, GloVe etc. well capture the contextuality and real-world analogies but contemporary convolution-based image embeddings such as VGGNet, AlexNet, etc. do not capture contextual knowledge. The popular king-queen analogy does not hold true for most commonly used vision embeddings.  In this paper, we introduce a pre-trained joint embedding (JE), named IMAGINATOR, trained on 21K distinct image objects level from 1M image+text pairs. JE is a way to encode multimodal data into a vector space where the text modality serves as the ground-ing key, which the complementary modality (in this case, the image) is anchored with. IMAGINATOR encapsulates three indivi",
    "path": "papers/23/05/2305.10438.json",
    "total_tokens": 853,
    "translated_title": "IMAGINATOR：使用基于单词级别图像本体的预训练图像+文本联合嵌入",
    "translated_abstract": "单词嵌入是一种语义有意义的单词向量表示，主要受到分布假设“你应该通过它的伴侣来认识一个单词”（Harris，1954）的影响，而现代基于预测的神经网络嵌入则依赖于设计选择和超参数优化。这篇论文介绍了一种名为IMAGINATOR的预训练联合嵌入（JE），它是在1M个图像+文本对中从21K个不同的图像对象级别进行训练的。JE是一种将多模态数据编码为矢量空间的方法，其中文本模态作为基础关键词，而补充模态（在这种情况下为图像）则与之相连。",
    "tldr": "IMAGINATOR是一个使用基于单词级别图像本体的预训练图像+文本联合嵌入，能将多模态数据编码为矢量空间。",
    "en_tdlr": "IMAGINATOR is a pre-trained joint embedding (JE) that uses word-level grounding of images, able to encode multimodal data into a vector space, in which the text modality serves as the grounding key and the complementary modality (in this case, the image) is anchored with it."
}