{
    "title": "BCEdge: SLO-Aware DNN Inference Services with Adaptive Batching on Edge Platforms. (arXiv:2305.01519v1 [cs.LG])",
    "abstract": "As deep neural networks (DNNs) are being applied to a wide range of edge intelligent applications, it is critical for edge inference platforms to have both high-throughput and low-latency at the same time. Such edge platforms with multiple DNN models pose new challenges for scheduler designs. First, each request may have different service level objectives (SLOs) to improve quality of service (QoS). Second, the edge platforms should be able to efficiently schedule multiple heterogeneous DNN models so that system utilization can be improved. To meet these two goals, this paper proposes BCEdge, a novel learning-based scheduling framework that takes adaptive batching and concurrent execution of DNN inference services on edge platforms. We define a utility function to evaluate the trade-off between throughput and latency. The scheduler in BCEdge leverages maximum entropy-based deep reinforcement learning (DRL) to maximize utility by 1) co-optimizing batch size and 2) the number of concurren",
    "link": "http://arxiv.org/abs/2305.01519",
    "context": "Title: BCEdge: SLO-Aware DNN Inference Services with Adaptive Batching on Edge Platforms. (arXiv:2305.01519v1 [cs.LG])\nAbstract: As deep neural networks (DNNs) are being applied to a wide range of edge intelligent applications, it is critical for edge inference platforms to have both high-throughput and low-latency at the same time. Such edge platforms with multiple DNN models pose new challenges for scheduler designs. First, each request may have different service level objectives (SLOs) to improve quality of service (QoS). Second, the edge platforms should be able to efficiently schedule multiple heterogeneous DNN models so that system utilization can be improved. To meet these two goals, this paper proposes BCEdge, a novel learning-based scheduling framework that takes adaptive batching and concurrent execution of DNN inference services on edge platforms. We define a utility function to evaluate the trade-off between throughput and latency. The scheduler in BCEdge leverages maximum entropy-based deep reinforcement learning (DRL) to maximize utility by 1) co-optimizing batch size and 2) the number of concurren",
    "path": "papers/23/05/2305.01519.json",
    "total_tokens": 1010,
    "translated_title": "BCEdge：适应边缘平台上的自适应批处理和SLO感知的DNN推断服务",
    "translated_abstract": "随着深度神经网络(DNN)应用于各种边缘智能应用，边缘推断平台具有高吞吐量和低延迟对于提高服务质量非常关键。对于具有多个DNN模型的边缘平台，这种情况给调度程序设计带来了新的挑战。首先，每个请求的服务水平目标(SLOs)都不同，以提高服务质量(QoS)。其次，边缘平台应该能够有效地调度多个异构DNN模型，以提高系统利用率。为了达到这两个目标，本文提出了BCEdge，一种新颖的学习-based调度框架，采用自适应批处理和DNN推断服务在边缘平台上进行并发执行。我们定义了一个实用函数来评估吞吐量和延迟之间的权衡。BCEdge中的调度程序利用了基于最大熵的深度强化学习(DRL)来通过1)共同优化批大小和2)基于SLOs的同时执行数量以最大化效用。实验结果表明，BCEdge在吞吐量和SLO达成率方面优于现有的最先进方法。",
    "tldr": "BCEdge是一种新颖的学习-based调度框架，使用自适应批处理和DNN推断服务进行并发执行，通过最大化效用来同时实现高吞吐量和低延迟，且优于现有的最先进方法。",
    "en_tdlr": "BCEdge is a novel learning-based scheduling framework for concurrent execution of DNN inference services using adaptive batching, which maximizes utility for both high throughput and low latency, and outperforms existing state-of-the-art approaches."
}