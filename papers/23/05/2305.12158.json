{
    "title": "Model-based adaptation for sample efficient transfer in reinforcement learning control of parameter-varying systems. (arXiv:2305.12158v1 [eess.SY])",
    "abstract": "In this paper, we leverage ideas from model-based control to address the sample efficiency problem of reinforcement learning (RL) algorithms. Accelerating learning is an active field of RL highly relevant in the context of time-varying systems. Traditional transfer learning methods propose to use prior knowledge of the system behavior to devise a gradual or immediate data-driven transformation of the control policy obtained through RL. Such transformation is usually computed by estimating the performance of previous control policies based on measurements recently collected from the system. However, such retrospective measures have debatable utility with no guarantees of positive transfer in most cases. Instead, we propose a model-based transformation, such that when actions from a control policy are applied to the target system, a positive transfer is achieved. The transformation can be used as an initialization for the reinforcement learning process to converge to a new optimum. We va",
    "link": "http://arxiv.org/abs/2305.12158",
    "context": "Title: Model-based adaptation for sample efficient transfer in reinforcement learning control of parameter-varying systems. (arXiv:2305.12158v1 [eess.SY])\nAbstract: In this paper, we leverage ideas from model-based control to address the sample efficiency problem of reinforcement learning (RL) algorithms. Accelerating learning is an active field of RL highly relevant in the context of time-varying systems. Traditional transfer learning methods propose to use prior knowledge of the system behavior to devise a gradual or immediate data-driven transformation of the control policy obtained through RL. Such transformation is usually computed by estimating the performance of previous control policies based on measurements recently collected from the system. However, such retrospective measures have debatable utility with no guarantees of positive transfer in most cases. Instead, we propose a model-based transformation, such that when actions from a control policy are applied to the target system, a positive transfer is achieved. The transformation can be used as an initialization for the reinforcement learning process to converge to a new optimum. We va",
    "path": "papers/23/05/2305.12158.json",
    "total_tokens": 676,
    "translated_title": "基于模型的适应性强化学习控制在参数变化系统的样本有效迁移中的应用",
    "translated_abstract": "本文提出基于模型的控制方法来解决强化学习算法的样本有效性问题。我们提出一种模型转换方法，通过将控制策略应用于目标系统，实现正向迁移，从而为强化学习过程提供初始化，以便其收敛到新的最优值。",
    "tldr": "本文提出一种基于模型的方法来解决强化学习中样本有效性问题，通过模型转换实现正向迁移，并用作强化学习过程的初始化以达到新的最优值。",
    "en_tdlr": "This paper proposes a model-based approach to address the sample efficiency problem of reinforcement learning, which achieves positive transfer through model transformation and serves as initialization for the reinforcement learning process to converge to a new optimum."
}