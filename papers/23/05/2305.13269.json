{
    "title": "Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources. (arXiv:2305.13269v2 [cs.CL] UPDATED)",
    "abstract": "We present chain-of-knowledge (CoK), a novel framework that augments large language models (LLMs) by dynamically incorporating grounding information from heterogeneous sources. It results in more factual rationales and reduced hallucination in generation. Specifically, CoK consists of three stages: reasoning preparation, dynamic knowledge adapting, and answer consolidation. Given a knowledge-intensive question, CoK first prepares several preliminary rationales and answers while identifying the relevant knowledge domains. If there is no majority consensus among the answers from samples, CoK corrects the rationales step by step by adapting knowledge from the identified domains. These corrected rationales can plausibly serve as a better foundation for the final answer consolidation. Unlike prior studies that primarily use unstructured data, CoK also leverages structured knowledge sources such as Wikidata and tables that provide more reliable factual information. To access both unstructure",
    "link": "http://arxiv.org/abs/2305.13269",
    "context": "Title: Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources. (arXiv:2305.13269v2 [cs.CL] UPDATED)\nAbstract: We present chain-of-knowledge (CoK), a novel framework that augments large language models (LLMs) by dynamically incorporating grounding information from heterogeneous sources. It results in more factual rationales and reduced hallucination in generation. Specifically, CoK consists of three stages: reasoning preparation, dynamic knowledge adapting, and answer consolidation. Given a knowledge-intensive question, CoK first prepares several preliminary rationales and answers while identifying the relevant knowledge domains. If there is no majority consensus among the answers from samples, CoK corrects the rationales step by step by adapting knowledge from the identified domains. These corrected rationales can plausibly serve as a better foundation for the final answer consolidation. Unlike prior studies that primarily use unstructured data, CoK also leverages structured knowledge sources such as Wikidata and tables that provide more reliable factual information. To access both unstructure",
    "path": "papers/23/05/2305.13269.json",
    "total_tokens": 854,
    "translated_title": "Chain-of-Knowledge:通过多源动态知识适应为大型语言模型提供准确的基础信息",
    "translated_abstract": "我们提出了一种新颖的框架，链式知识（CoK），通过动态地整合来自不同来源的基础信息来增强大型语言模型(LLMs)。它可以产生更多的事实依据，减少生成的幻觉。具体而言，CoK包括三个阶段：推理准备、动态知识适应和答案整合。给定一个知识密集型问题，CoK首先准备若干个初步的依据和答案，同时识别出相关的知识领域。如果样本中的答案没有多数共识，CoK通过从识别出的领域中逐步适应知识来纠正依据。这些纠正后的依据可以更好地作为最终答案整合的基础。不同于之前主要使用非结构化数据的研究，CoK还利用结构化的知识源，如Wikidata和表格，提供更可靠的事实信息。",
    "tldr": "Chain-of-Knowledge通过整合多源动态知识为大型语言模型提供准确的基础信息，减少生成的幻觉，可以产生更可靠的答案。",
    "en_tdlr": "Chain-of-Knowledge enhances large language models by incorporating dynamically adapted knowledge from heterogeneous sources, resulting in more reliable answers and reduced generation hallucination."
}