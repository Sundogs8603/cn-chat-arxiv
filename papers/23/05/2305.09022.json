{
    "title": "It Takes Two to Tango: Navigating Conceptualizations of NLP Tasks and Measurements of Performance. (arXiv:2305.09022v1 [cs.CL])",
    "abstract": "Progress in NLP is increasingly measured through benchmarks; hence, contextualizing progress requires understanding when and why practitioners may disagree about the validity of benchmarks. We develop a taxonomy of disagreement, drawing on tools from measurement modeling, and distinguish between two types of disagreement: 1) how tasks are conceptualized and 2) how measurements of model performance are operationalized. To provide evidence for our taxonomy, we conduct a meta-analysis of relevant literature to understand how NLP tasks are conceptualized, as well as a survey of practitioners about their impressions of different factors that affect benchmark validity. Our meta-analysis and survey across eight tasks, ranging from coreference resolution to question answering, uncover that tasks are generally not clearly and consistently conceptualized and benchmarks suffer from operationalization disagreements. These findings support our proposed taxonomy of disagreement. Finally, based on ou",
    "link": "http://arxiv.org/abs/2305.09022",
    "context": "Title: It Takes Two to Tango: Navigating Conceptualizations of NLP Tasks and Measurements of Performance. (arXiv:2305.09022v1 [cs.CL])\nAbstract: Progress in NLP is increasingly measured through benchmarks; hence, contextualizing progress requires understanding when and why practitioners may disagree about the validity of benchmarks. We develop a taxonomy of disagreement, drawing on tools from measurement modeling, and distinguish between two types of disagreement: 1) how tasks are conceptualized and 2) how measurements of model performance are operationalized. To provide evidence for our taxonomy, we conduct a meta-analysis of relevant literature to understand how NLP tasks are conceptualized, as well as a survey of practitioners about their impressions of different factors that affect benchmark validity. Our meta-analysis and survey across eight tasks, ranging from coreference resolution to question answering, uncover that tasks are generally not clearly and consistently conceptualized and benchmarks suffer from operationalization disagreements. These findings support our proposed taxonomy of disagreement. Finally, based on ou",
    "path": "papers/23/05/2305.09022.json",
    "total_tokens": 881,
    "translated_title": "双方共舞：导航NLP任务的概念化和性能测量",
    "translated_abstract": "自然语言处理（NLP）的进展越来越多地通过基准进行衡量；因此，了解从业者可能在基准的有效性上存在分歧的原因和时间需要对进展进行具体化。我们利用测量建模工具，开发了一种分歧分类方法，区分了两种类型的分歧：1）如何概念化任务；2）如何操作化模型性能的测量。为了提供支持我们分类的证据，我们进行了相关文献的元分析，以了解NLP任务的概念化，以及对影响基准有效性的不同因素印象的从业者的调查。我们对从核心参考解决到问答等八项任务的元分析和调查发现，任务通常没有明确和一致的概念，基准存在操作化分歧。这些发现支持了我们提出的分歧分类方法。最后，基于结果，我们提出了解决基准争议的建议。",
    "tldr": "本文针对NLP的任务和性能测量提出了一种分歧分类方法，经过相关研究和调查，发现现有任务没有明确和一致的概念，基准存在操作化分歧，提出了解决基准争议的建议。",
    "en_tdlr": "This paper proposes a taxonomy of disagreement for NLP tasks and measurements of performance, and through analysis and survey, finds that the conceptualization of tasks is not clear and consistent, and benchmarks suffer from operationalization disagreements. The paper also provides suggestions for resolving benchmark disputes."
}