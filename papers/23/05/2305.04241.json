{
    "title": "Vcc: Scaling Transformers to 128K Tokens or More by Prioritizing Important Tokens. (arXiv:2305.04241v2 [cs.CL] UPDATED)",
    "abstract": "Transformers are central in modern natural language processing and computer vision applications. Despite recent works devoted to reducing the quadratic cost of such models (as a function of the sequence length), dealing with ultra long sequences (e.g., with more than 16K tokens) remains challenging. Applications such as answering questions based on a book or summarizing a scientific article are inefficient or infeasible. Here, we propose to significantly improve the efficiency of Transformers for ultra long sequences, by compressing the sequence into a much smaller representation at each layer. Specifically, by exploiting the fact that in many tasks, only a small subset of special tokens (we call VIP-tokens) are most relevant to the final prediction, we propose a VIP-token centric compression (VCC) scheme which selectively compresses the sequence based on their impact on approximating the representation of the VIP-tokens. Compared with competitive baselines, our algorithm is not only e",
    "link": "http://arxiv.org/abs/2305.04241",
    "context": "Title: Vcc: Scaling Transformers to 128K Tokens or More by Prioritizing Important Tokens. (arXiv:2305.04241v2 [cs.CL] UPDATED)\nAbstract: Transformers are central in modern natural language processing and computer vision applications. Despite recent works devoted to reducing the quadratic cost of such models (as a function of the sequence length), dealing with ultra long sequences (e.g., with more than 16K tokens) remains challenging. Applications such as answering questions based on a book or summarizing a scientific article are inefficient or infeasible. Here, we propose to significantly improve the efficiency of Transformers for ultra long sequences, by compressing the sequence into a much smaller representation at each layer. Specifically, by exploiting the fact that in many tasks, only a small subset of special tokens (we call VIP-tokens) are most relevant to the final prediction, we propose a VIP-token centric compression (VCC) scheme which selectively compresses the sequence based on their impact on approximating the representation of the VIP-tokens. Compared with competitive baselines, our algorithm is not only e",
    "path": "papers/23/05/2305.04241.json",
    "total_tokens": 819,
    "translated_title": "Vcc: 通过优先处理重要标记将Transformer扩展到128K令牌或更多",
    "translated_abstract": "Transformer模型在现代自然语言处理和计算机视觉应用中扮演关键角色。尽管近年来有人致力于降低这些模型的二次成本（作为序列长度的函数），但处理超长序列（如超过16K标记）仍然具有挑战性。本文提出了一种将Ultra long sequences的Transformer模型的效率显着提高的方法，即在每层将序列压缩为更小的表示，通过利用许多任务中仅有的少数的特殊标记（我们称其为VIP标记）与最终预测结果最相关的这个事实，我们提出了基于VIP标记的压缩方法，即VIP标记中心压缩（VCC）方案，该方案根据其对近似VIP标记表示的影响有选择地压缩序列。",
    "tldr": "这篇论文提出了一种新的方法 VCC，通过优先处理最重要的VIP标记，一定程度上压缩序列，从而使Transformer模型可处理长度更长的序列。",
    "en_tdlr": "This paper proposes a new method, VCC, which compresses the sequence to a certain extent by prioritizing the most important VIP tokens, thus enabling the Transformer model to handle longer sequences."
}