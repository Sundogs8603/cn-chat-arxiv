{
    "title": "CGCE: A Chinese Generative Chat Evaluation Benchmark for General and Financial Domains. (arXiv:2305.14471v1 [cs.CL])",
    "abstract": "Generative chat models, such as ChatGPT and GPT-4, have revolutionized natural language generation (NLG) by incorporating instructions and human feedback to achieve significant performance improvements. However, the lack of standardized evaluation benchmarks for chat models, particularly for Chinese and domain-specific models, hinders their assessment and progress. To address this gap, we introduce the Chinese Generative Chat Evaluation (CGCE) benchmark, focusing on general and financial domains. The CGCE benchmark encompasses diverse tasks, including 200 questions in the general domain and 150 specific professional questions in the financial domain. Manual scoring evaluates factors such as accuracy, coherence, expression clarity, and completeness. The CGCE benchmark provides researchers with a standardized framework to assess and compare Chinese generative chat models, fostering advancements in NLG research.",
    "link": "http://arxiv.org/abs/2305.14471",
    "context": "Title: CGCE: A Chinese Generative Chat Evaluation Benchmark for General and Financial Domains. (arXiv:2305.14471v1 [cs.CL])\nAbstract: Generative chat models, such as ChatGPT and GPT-4, have revolutionized natural language generation (NLG) by incorporating instructions and human feedback to achieve significant performance improvements. However, the lack of standardized evaluation benchmarks for chat models, particularly for Chinese and domain-specific models, hinders their assessment and progress. To address this gap, we introduce the Chinese Generative Chat Evaluation (CGCE) benchmark, focusing on general and financial domains. The CGCE benchmark encompasses diverse tasks, including 200 questions in the general domain and 150 specific professional questions in the financial domain. Manual scoring evaluates factors such as accuracy, coherence, expression clarity, and completeness. The CGCE benchmark provides researchers with a standardized framework to assess and compare Chinese generative chat models, fostering advancements in NLG research.",
    "path": "papers/23/05/2305.14471.json",
    "total_tokens": 898,
    "translated_title": "CGCE：一个适用于通用和金融领域的中文生成式聊天评测基准",
    "translated_abstract": "生成式聊天模型，如ChatGPT和GPT-4，通过整合指令和人类反馈来实现显著的性能改进，革新了自然语言生成技术。然而，缺乏标准化的聊天模型评测基准，特别是适用于中文和特定领域模型的评测基准，这阻碍了它们的评估和进步。为了解决这个问题，我们介绍了中国生成式聊天（CGCE）基准，专注于通用和金融领域。CGCE基准涵盖了多个任务，包括200个通用领域问题和150个金融领域的特定职业问题。手动评分考虑了准确性、连贯性、表达清晰度和完整度等因素。CGCE基准提供了一个标准化的框架，以评估和比较中文生成式聊天模型，促进了自然语言生成研究的进展。",
    "tldr": "中国的CGCE基准为通用和金融领域建立了标准化的评测框架，包含200个通用领域问题和150个金融领域特定的职业问题，旨在提升自然语言生成研究的发展。",
    "en_tdlr": "The CGCE benchmark, designed for general and financial domains, provides a standardized evaluation framework for Chinese generative chat models. It includes 200 general domain questions and 150 specific professional questions in finance, and aims to foster advances in natural language generation research."
}