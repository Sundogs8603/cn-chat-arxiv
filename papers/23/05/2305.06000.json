{
    "title": "Global Convergence of Deep Galerkin and PINNs Methods for Solving Partial Differential Equations. (arXiv:2305.06000v1 [math.NA])",
    "abstract": "Numerically solving high-dimensional partial differential equations (PDEs) is a major challenge. Conventional methods, such as finite difference methods, are unable to solve high-dimensional PDEs due to the curse-of-dimensionality. A variety of deep learning methods have been recently developed to try and solve high-dimensional PDEs by approximating the solution using a neural network. In this paper, we prove global convergence for one of the commonly-used deep learning algorithms for solving PDEs, the Deep Galerkin Method (DGM). DGM trains a neural network approximator to solve the PDE using stochastic gradient descent. We prove that, as the number of hidden units in the single-layer network goes to infinity (i.e., in the ``wide network limit\"), the trained neural network converges to the solution of an infinite-dimensional linear ordinary differential equation (ODE). The PDE residual of the limiting approximator converges to zero as the training time $\\rightarrow \\infty$. Under mild ",
    "link": "http://arxiv.org/abs/2305.06000",
    "context": "Title: Global Convergence of Deep Galerkin and PINNs Methods for Solving Partial Differential Equations. (arXiv:2305.06000v1 [math.NA])\nAbstract: Numerically solving high-dimensional partial differential equations (PDEs) is a major challenge. Conventional methods, such as finite difference methods, are unable to solve high-dimensional PDEs due to the curse-of-dimensionality. A variety of deep learning methods have been recently developed to try and solve high-dimensional PDEs by approximating the solution using a neural network. In this paper, we prove global convergence for one of the commonly-used deep learning algorithms for solving PDEs, the Deep Galerkin Method (DGM). DGM trains a neural network approximator to solve the PDE using stochastic gradient descent. We prove that, as the number of hidden units in the single-layer network goes to infinity (i.e., in the ``wide network limit\"), the trained neural network converges to the solution of an infinite-dimensional linear ordinary differential equation (ODE). The PDE residual of the limiting approximator converges to zero as the training time $\\rightarrow \\infty$. Under mild ",
    "path": "papers/23/05/2305.06000.json",
    "total_tokens": 1003,
    "translated_title": "求解偏微分方程的 Deep Galerkin 与 PINNs 方法的全局收敛性。",
    "translated_abstract": "在高维偏微分方程的数值求解中，由于维度诅咒，传统方法如有限差分无法求解。近期，一系列基于深度学习的方式被开发出来，通过神经网络来逼近求解结果。在这篇论文中，我们证明了目前解决偏微分方程的深度学习算法 Deep Galerkin Method (DGM) 的全局收敛性。DGM 通过随机梯度下降训练一个神经网络逼近求解结果。当单层网络中隐层单元的数量趋向于无穷大时（即\"宽度网络极限\"），训练后的神经网络逐渐收敛于一个无限维的线性常微分方程的解。极限逼近器的 PDE 残差在训练时间趋近于无穷大时趋近于零。在合理的假设下，我们将 Deep Galerkin Method 的结果与另一种常用方法 PINNs 的结果进行比较， PINNs 在一些情况下可能会更稳定。",
    "tldr": "该论文证明了 Deep Galerkin Method（DGM）算法在全局收敛时，训练得到的神经网络会收敛于一个无限维的线性常微分方程的解，从而逼近求解高维偏微分方程。在一些情况下，PINNs 方法会更稳定。"
}