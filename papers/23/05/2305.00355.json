{
    "title": "MH-DETR: Video Moment and Highlight Detection with Cross-modal Transformer. (arXiv:2305.00355v1 [cs.CV])",
    "abstract": "With the increasing demand for video understanding, video moment and highlight detection (MHD) has emerged as a critical research topic. MHD aims to localize all moments and predict clip-wise saliency scores simultaneously. Despite progress made by existing DETR-based methods, we observe that these methods coarsely fuse features from different modalities, which weakens the temporal intra-modal context and results in insufficient cross-modal interaction. To address this issue, we propose MH-DETR (Moment and Highlight Detection Transformer) tailored for MHD. Specifically, we introduce a simple yet efficient pooling operator within the uni-modal encoder to capture global intra-modal context. Moreover, to obtain temporally aligned cross-modal features, we design a plug-and-play cross-modal interaction module between the encoder and decoder, seamlessly integrating visual and textual features. Comprehensive experiments on QVHighlights, Charades-STA, Activity-Net, and TVSum datasets show that",
    "link": "http://arxiv.org/abs/2305.00355",
    "context": "Title: MH-DETR: Video Moment and Highlight Detection with Cross-modal Transformer. (arXiv:2305.00355v1 [cs.CV])\nAbstract: With the increasing demand for video understanding, video moment and highlight detection (MHD) has emerged as a critical research topic. MHD aims to localize all moments and predict clip-wise saliency scores simultaneously. Despite progress made by existing DETR-based methods, we observe that these methods coarsely fuse features from different modalities, which weakens the temporal intra-modal context and results in insufficient cross-modal interaction. To address this issue, we propose MH-DETR (Moment and Highlight Detection Transformer) tailored for MHD. Specifically, we introduce a simple yet efficient pooling operator within the uni-modal encoder to capture global intra-modal context. Moreover, to obtain temporally aligned cross-modal features, we design a plug-and-play cross-modal interaction module between the encoder and decoder, seamlessly integrating visual and textual features. Comprehensive experiments on QVHighlights, Charades-STA, Activity-Net, and TVSum datasets show that",
    "path": "papers/23/05/2305.00355.json",
    "total_tokens": 936,
    "translated_title": "MH-DETR: 基于跨模态 Transformer 的视频片段和精华部分检测",
    "translated_abstract": "随着对视频理解的需求不断增长，视频片段和精华部分检测(MHD)已成为一个关键的研究主题。MHD旨在同时本地化所有时刻并预测剪辑级显著性分数。尽管现有的DETR-based方法取得了进展，我们观察到这些方法粗略地融合了来自不同模态的特征，这削弱了时间内部上下文，并导致跨模态交互不足。为了解决这个问题，我们提出了专门为MHD定制的MH-DETR (Moment and Highlight Detection Transformer)。具体地，我们在单模编码器内引入了一个简单而高效的汇集算子，以捕获全局内模上下文。此外，为了获得时间上对齐的跨模态特征，我们设计了一种插拔式的跨模态交互模块，将视觉和文本特征无缝集成在编码器和解码器之间。在QVHighlights，Charades-STA，Activity-Net和TVSum数据集上的全面实验表明，我们的模型优于现有方法。",
    "tldr": "该论文提出了面向视频片段和精华部分检测的MH-DETR模型，使用跨模态Transformer来获取时间上对齐的跨模态特征，解决了现有方法中时间内部上下文不足的问题。",
    "en_tdlr": "The paper proposes the MH-DETR model for Video Moment and Highlight Detection, which uses cross-modal Transformer to obtain temporally-aligned cross-modal features, addressing the issue of insufficient temporal intra-modal context in existing methods. Comprehensive experiments show that the proposed model performs better than existing methods."
}