{
    "title": "Shattering the Agent-Environment Interface for Fine-Tuning Inclusive Language Models. (arXiv:2305.11455v1 [cs.CL])",
    "abstract": "A centerpiece of the ever-popular reinforcement learning from human feedback (RLHF) approach to fine-tuning autoregressive language models is the explicit training of a reward model to emulate human feedback, distinct from the language model itself. This reward model is then coupled with policy-gradient methods to dramatically improve the alignment between language model outputs and desired responses. In this work, we adopt a novel perspective wherein a pre-trained language model is itself simultaneously a policy, reward function, and transition function. An immediate consequence of this is that reward learning and language model fine-tuning can be performed jointly and directly, without requiring any further downstream policy optimization. While this perspective does indeed break the traditional agent-environment interface, we nevertheless maintain that there can be enormous statistical benefits afforded by bringing to bear traditional algorithmic concepts from reinforcement learning.",
    "link": "http://arxiv.org/abs/2305.11455",
    "context": "Title: Shattering the Agent-Environment Interface for Fine-Tuning Inclusive Language Models. (arXiv:2305.11455v1 [cs.CL])\nAbstract: A centerpiece of the ever-popular reinforcement learning from human feedback (RLHF) approach to fine-tuning autoregressive language models is the explicit training of a reward model to emulate human feedback, distinct from the language model itself. This reward model is then coupled with policy-gradient methods to dramatically improve the alignment between language model outputs and desired responses. In this work, we adopt a novel perspective wherein a pre-trained language model is itself simultaneously a policy, reward function, and transition function. An immediate consequence of this is that reward learning and language model fine-tuning can be performed jointly and directly, without requiring any further downstream policy optimization. While this perspective does indeed break the traditional agent-environment interface, we nevertheless maintain that there can be enormous statistical benefits afforded by bringing to bear traditional algorithmic concepts from reinforcement learning.",
    "path": "papers/23/05/2305.11455.json",
    "total_tokens": 889,
    "translated_title": "突破智能体-环境界面，优化具有包容性的语言模型的微调",
    "translated_abstract": "在微调自回归语言模型中，从人类反馈的增强学习方法（RLHF）的重要组成部分是显式训练一个奖励模型来模拟人类反馈，而不是语言模型本身。然后，将这个奖励模型与策略梯度方法耦合起来，从而显著提高语言模型输出与期望响应之间的一致性。在这项工作中，我们采取了一种新颖的观点，即预训练的语言模型本身同时是策略、奖励函数和转移函数。这个观点的一个直接结果是，可以同时直接进行奖励学习和语言模型微调，无需进一步的下游策略优化。虽然这个观点确实打破了传统智能体-环境界面，但我们仍然认为，将强化学习的传统算法概念运用于这种方法中可以带来巨大的统计收益。",
    "tldr": "该论文提出了一种新颖的思路，将预训练的语言模型本身同时作为策略、奖励函数和转移函数，可以直接进行奖励学习和语言模型微调，可以带来巨大的统计收益。",
    "en_tdlr": "This paper proposes a novel perspective where a pre-trained language model serves as a policy, reward function, and transition function, enabling joint and direct reward learning and language model fine-tuning without downstream policy optimization, offering enormous statistical benefits."
}