{
    "title": "Explaining Language Models' Predictions with High-Impact Concepts. (arXiv:2305.02160v1 [cs.CL])",
    "abstract": "The emergence of large-scale pretrained language models has posed unprecedented challenges in deriving explanations of why the model has made some predictions. Stemmed from the compositional nature of languages, spurious correlations have further undermined the trustworthiness of NLP systems, leading to unreliable model explanations that are merely correlated with the output predictions. To encourage fairness and transparency, there exists an urgent demand for reliable explanations that allow users to consistently understand the model's behavior. In this work, we propose a complete framework for extending concept-based interpretability methods to NLP. Specifically, we propose a post-hoc interpretability method for extracting predictive high-level features (concepts) from the pretrained model's hidden layer activations. We optimize for features whose existence causes the output predictions to change substantially, \\ie generates a high impact. Moreover, we devise several evaluation metri",
    "link": "http://arxiv.org/abs/2305.02160",
    "context": "Title: Explaining Language Models' Predictions with High-Impact Concepts. (arXiv:2305.02160v1 [cs.CL])\nAbstract: The emergence of large-scale pretrained language models has posed unprecedented challenges in deriving explanations of why the model has made some predictions. Stemmed from the compositional nature of languages, spurious correlations have further undermined the trustworthiness of NLP systems, leading to unreliable model explanations that are merely correlated with the output predictions. To encourage fairness and transparency, there exists an urgent demand for reliable explanations that allow users to consistently understand the model's behavior. In this work, we propose a complete framework for extending concept-based interpretability methods to NLP. Specifically, we propose a post-hoc interpretability method for extracting predictive high-level features (concepts) from the pretrained model's hidden layer activations. We optimize for features whose existence causes the output predictions to change substantially, \\ie generates a high impact. Moreover, we devise several evaluation metri",
    "path": "papers/23/05/2305.02160.json",
    "total_tokens": 933,
    "translated_title": "利用高影响概念解释语言模型的预测",
    "translated_abstract": "大规模预训练语言模型的出现对推断模型的解释提出了前所未有的挑战。由于语言的组合性质，虚假相关继续削弱着 NLP 系统的可信度，导致模型的解释仅仅与输出预测相关，无法靠谱地解释模型的行为。为了促进公平和透明度，在 NLP 领域有着迫切需求可靠的解释，使用户能够一致地理解模型的行为。在这项工作中，我们提出了一个完整的框架，将基于概念的可解释性方法推广到 NLP 领域。具体而言，我们提出了一种用于从预训练模型的隐藏层激活中提取预测高级特征（概念）的后期可解释性方法。我们优化具有高影响力的特征存在，这些特征会导致输出预测发生显著变化。",
    "tldr": "本论文提供了一个完整的框架，将基于概念的可解释性方法推广到 NLP 领域。通过后期可解释性方法，从预训练模型的隐藏层激活中提取预测高级特征（概念），并优化具有高影响力的特征存在，使其能够准确地解释模型的行为。",
    "en_tdlr": "This paper presents a complete framework for extending concept-based interpretability methods to NLP, and proposes a post-hoc interpretability method for extracting predictive high-level concepts from the hidden layer activations of pretrained language models. The method optimizes for high-impact features that cause substantial changes in output predictions, enabling accurate explanations of the model's behavior."
}