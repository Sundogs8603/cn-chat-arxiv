{
    "title": "CLIP-S$^4$: Language-Guided Self-Supervised Semantic Segmentation. (arXiv:2305.01040v1 [cs.CV])",
    "abstract": "Existing semantic segmentation approaches are often limited by costly pixel-wise annotations and predefined classes. In this work, we present CLIP-S$^4$ that leverages self-supervised pixel representation learning and vision-language models to enable various semantic segmentation tasks (e.g., unsupervised, transfer learning, language-driven segmentation) without any human annotations and unknown class information. We first learn pixel embeddings with pixel-segment contrastive learning from different augmented views of images. To further improve the pixel embeddings and enable language-driven semantic segmentation, we design two types of consistency guided by vision-language models: 1) embedding consistency, aligning our pixel embeddings to the joint feature space of a pre-trained vision-language model, CLIP; and 2) semantic consistency, forcing our model to make the same predictions as CLIP over a set of carefully designed target classes with both known and unknown prototypes. Thus, CL",
    "link": "http://arxiv.org/abs/2305.01040",
    "context": "Title: CLIP-S$^4$: Language-Guided Self-Supervised Semantic Segmentation. (arXiv:2305.01040v1 [cs.CV])\nAbstract: Existing semantic segmentation approaches are often limited by costly pixel-wise annotations and predefined classes. In this work, we present CLIP-S$^4$ that leverages self-supervised pixel representation learning and vision-language models to enable various semantic segmentation tasks (e.g., unsupervised, transfer learning, language-driven segmentation) without any human annotations and unknown class information. We first learn pixel embeddings with pixel-segment contrastive learning from different augmented views of images. To further improve the pixel embeddings and enable language-driven semantic segmentation, we design two types of consistency guided by vision-language models: 1) embedding consistency, aligning our pixel embeddings to the joint feature space of a pre-trained vision-language model, CLIP; and 2) semantic consistency, forcing our model to make the same predictions as CLIP over a set of carefully designed target classes with both known and unknown prototypes. Thus, CL",
    "path": "papers/23/05/2305.01040.json",
    "total_tokens": 1072,
    "translated_title": "CLIP-S$^4$: 语言引导的自监督语义分割",
    "translated_abstract": "传统的语义分割方法常常受到昂贵的像素级标注和预定义类别的限制。本文提出了一种名为CLIP-S$^4$的语言引导自监督语义分割方法，旨在利用自监督像素表示学习和视觉-语言模型，支持各种语义分割任务，无需人工注释和未知类别信息即可完成。",
    "tldr": "CLIP-S$^4$是一种无需像素级标注和预定义类别的语言引导自监督语义分割方法，利用自监督像素表示学习和视觉-语言模型，支持各种语义分割任务。",
    "en_tdlr": "CLIP-S$^4$ is a language-guided self-supervised semantic segmentation method that eliminates the need for costly pixel-wise annotations and predefined class limitations, and supports various semantic segmentation tasks by leveraging self-supervised pixel representation learning and vision-language models."
}