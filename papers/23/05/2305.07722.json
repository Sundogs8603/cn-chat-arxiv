{
    "title": "In Search of Verifiability: Explanations Rarely Enable Complementary Performance in AI-Advised Decision Making. (arXiv:2305.07722v1 [cs.AI])",
    "abstract": "The current literature on AI-advised decision making -- involving explainable AI systems advising human decision makers -- presents a series of inconclusive and confounding results. To synthesize these findings, we propose a simple theory that elucidates the frequent failure of AI explanations to engender appropriate reliance and complementary decision making performance. We argue explanations are only useful to the extent that they allow a human decision maker to verify the correctness of an AI's prediction, in contrast to other desiderata, e.g., interpretability or spelling out the AI's reasoning process. Prior studies find in many decision making contexts AI explanations do not facilitate such verification. Moreover, most contexts fundamentally do not allow verification, regardless of explanation method. We conclude with a discussion of potential approaches for more effective explainable AI-advised decision making and human-AI collaboration.",
    "link": "http://arxiv.org/abs/2305.07722",
    "context": "Title: In Search of Verifiability: Explanations Rarely Enable Complementary Performance in AI-Advised Decision Making. (arXiv:2305.07722v1 [cs.AI])\nAbstract: The current literature on AI-advised decision making -- involving explainable AI systems advising human decision makers -- presents a series of inconclusive and confounding results. To synthesize these findings, we propose a simple theory that elucidates the frequent failure of AI explanations to engender appropriate reliance and complementary decision making performance. We argue explanations are only useful to the extent that they allow a human decision maker to verify the correctness of an AI's prediction, in contrast to other desiderata, e.g., interpretability or spelling out the AI's reasoning process. Prior studies find in many decision making contexts AI explanations do not facilitate such verification. Moreover, most contexts fundamentally do not allow verification, regardless of explanation method. We conclude with a discussion of potential approaches for more effective explainable AI-advised decision making and human-AI collaboration.",
    "path": "papers/23/05/2305.07722.json",
    "total_tokens": 843,
    "translated_title": "寻求可验证性: 解释很少能够在AI辅助决策中提高决策性能",
    "translated_abstract": "目前关于AI辅助决策的文献，涉及可解释的AI系统为人类决策者提供建议，并呈现出一系列不确定和令人困惑的结果。为了综合这些发现，我们提出了一个简单的理论，阐明了AI解释经常无法促使适当的依赖和互补决策表现的失败。我们认为解释只有在允许人类决策者验证AI预测的正确性时才有用，而不是其他期望，例如可解释性或清晰阐述AI的推理过程。先前的研究发现，在许多决策环境中，AI解释并未促进这种验证。此外，无论解释方法如何，大多数环境基本上都无法进行验证。我们最后讨论了更有效的可解释AI辅助决策和人工智能协作的潜在方法。",
    "tldr": "AI解释只有在允许人类决策者验证AI预测的正确性时才有用，而大多数决策环境无法进行验证。",
    "en_tdlr": "Explanations only enable complementary decision-making performance in AI-advised environments if they allow human decision-makers to verify the correctness of AI predictions, but in most decision contexts such verification is not possible regardless of the explanation method used."
}