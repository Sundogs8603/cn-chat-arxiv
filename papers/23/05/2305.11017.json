{
    "title": "Deep Metric Tensor Regularized Policy Gradient. (arXiv:2305.11017v1 [cs.LG])",
    "abstract": "Policy gradient algorithms are an important family of deep reinforcement learning techniques. Many past research endeavors focused on using the first-order policy gradient information to train policy networks. Different from these works, we conduct research in this paper driven by the believe that properly utilizing and controlling Hessian information associated with the policy gradient can noticeably improve the performance of policy gradient algorithms. One key Hessian information that attracted our attention is the Hessian trace, which gives the divergence of the policy gradient vector field in the Euclidean policy parametric space. We set the goal to generalize this Euclidean policy parametric space into a general Riemmanian manifold by introducing a metric tensor field $g_ab$ in the parametric space. This is achieved through newly developed mathematical tools, deep learning algorithms, and metric tensor deep neural networks (DNNs). Armed with these technical developments, we propo",
    "link": "http://arxiv.org/abs/2305.11017",
    "context": "Title: Deep Metric Tensor Regularized Policy Gradient. (arXiv:2305.11017v1 [cs.LG])\nAbstract: Policy gradient algorithms are an important family of deep reinforcement learning techniques. Many past research endeavors focused on using the first-order policy gradient information to train policy networks. Different from these works, we conduct research in this paper driven by the believe that properly utilizing and controlling Hessian information associated with the policy gradient can noticeably improve the performance of policy gradient algorithms. One key Hessian information that attracted our attention is the Hessian trace, which gives the divergence of the policy gradient vector field in the Euclidean policy parametric space. We set the goal to generalize this Euclidean policy parametric space into a general Riemmanian manifold by introducing a metric tensor field $g_ab$ in the parametric space. This is achieved through newly developed mathematical tools, deep learning algorithms, and metric tensor deep neural networks (DNNs). Armed with these technical developments, we propo",
    "path": "papers/23/05/2305.11017.json",
    "total_tokens": 893,
    "translated_title": "深度度量张量正则化策略梯度",
    "translated_abstract": "策略梯度算法是深度强化学习技术中重要的一类。过去的许多研究都着眼于使用一阶策略梯度信息来训练策略网络。与这些工作不同，本文的研究基于这样的信念：合理利用和控制与策略梯度相关的海森信息可以显著提高策略梯度算法的性能。我们关注的一个关键海森信息是海森跟踪值，它给出了欧几里得策略参数空间中策略梯度向量场的发散。我们的目标是通过在参数空间引入度量张量场$g_ab$来将欧几里得策略参数空间推广到一般的黎曼流形上。这通过新开发的数学工具、深度学习算法和度量张量深度神经网络(DNN)来实现。拥有这些技术发展，我们提出了一种新的深度度量张量正则化策略梯度算法。",
    "tldr": "本文研究通过引入度量张量场$g_ab$将欧几里得策略参数空间推广到一般的黎曼流形上，并提出了一种新的深度度量张量正则化策略梯度算法。",
    "en_tdlr": "This paper proposes a new deep metric tensor regularized policy gradient algorithm by introducing a metric tensor field to generalize the Euclidean policy parametric space into a general Riemannian manifold."
}