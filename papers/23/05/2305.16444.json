{
    "title": "Don't Retrain, Just Rewrite: Countering Adversarial Perturbations by Rewriting Text. (arXiv:2305.16444v1 [cs.CL])",
    "abstract": "Can language models transform inputs to protect text classifiers against adversarial attacks? In this work, we present ATINTER, a model that intercepts and learns to rewrite adversarial inputs to make them non-adversarial for a downstream text classifier. Our experiments on four datasets and five attack mechanisms reveal that ATINTER is effective at providing better adversarial robustness than existing defense approaches, without compromising task accuracy. For example, on sentiment classification using the SST-2 dataset, our method improves the adversarial accuracy over the best existing defense approach by more than 4% with a smaller decrease in task accuracy (0.5% vs 2.5%). Moreover, we show that ATINTER generalizes across multiple downstream tasks and classifiers without having to explicitly retrain it for those settings. Specifically, we find that when ATINTER is trained to remove adversarial perturbations for the sentiment classification task on the SST-2 dataset, it even transfe",
    "link": "http://arxiv.org/abs/2305.16444",
    "context": "Title: Don't Retrain, Just Rewrite: Countering Adversarial Perturbations by Rewriting Text. (arXiv:2305.16444v1 [cs.CL])\nAbstract: Can language models transform inputs to protect text classifiers against adversarial attacks? In this work, we present ATINTER, a model that intercepts and learns to rewrite adversarial inputs to make them non-adversarial for a downstream text classifier. Our experiments on four datasets and five attack mechanisms reveal that ATINTER is effective at providing better adversarial robustness than existing defense approaches, without compromising task accuracy. For example, on sentiment classification using the SST-2 dataset, our method improves the adversarial accuracy over the best existing defense approach by more than 4% with a smaller decrease in task accuracy (0.5% vs 2.5%). Moreover, we show that ATINTER generalizes across multiple downstream tasks and classifiers without having to explicitly retrain it for those settings. Specifically, we find that when ATINTER is trained to remove adversarial perturbations for the sentiment classification task on the SST-2 dataset, it even transfe",
    "path": "papers/23/05/2305.16444.json",
    "total_tokens": 936,
    "translated_title": "不需重新训练，只需重写：通过重写文本来对抗对抗性扰动攻击",
    "translated_abstract": "语言模型能否转化输入以保护文本分类器免受对抗性攻击？本文提出了ATINTER模型，该模型截获并学习重写对抗性输入以使其对于下游文本分类器来说不具有对抗性。我们在四个数据集和五种攻击机制上的实验证明，ATINTER比现有的防御方法更有效，并且不会牺牲任务准确性。例如，使用SST-2数据集进行情感分类，我们的方法提高了比我们最好的现有防御方法更多的4%的对抗准确性，而任务准确性的下降更小(0.5%比2.5%)。此外，我们证明了ATINTER在不需要明确为这些设置重新训练的情况下横向推广到多个下游任务和分类器。具体而言，我们发现当ATINTER被训练以删除SST-2数据集情感分类任务的对抗性扰动时，它甚至可以传输。",
    "tldr": "本文提出了ATINTER模型，该模型可以重写对抗性输入以使其对于下游文本分类器来说不具有对抗性，在多个数据集和攻击机制的实验中证明了其比现有防御方法更有效，且不会牺牲任务准确性",
    "en_tdlr": "This paper proposes the ATINTER model, which can rewrite adversarial inputs to make them non-adversarial for downstream text classifiers. Experimental results on multiple datasets and attack mechanisms show that the ATINTER model is more effective than existing defense methods and does not compromise task accuracy. The model can even transfer to multiple downstream tasks and classifiers without explicit retraining."
}