{
    "title": "Visually grounded few-shot word acquisition with fewer shots. (arXiv:2305.15937v1 [cs.CL])",
    "abstract": "We propose a visually grounded speech model that acquires new words and their visual depictions from just a few word-image example pairs. Given a set of test images and a spoken query, we ask the model which image depicts the query word. Previous work has simplified this problem by either using an artificial setting with digit word-image pairs or by using a large number of examples per class. We propose an approach that can work on natural word-image pairs but with less examples, i.e. fewer shots. Our approach involves using the given word-image example pairs to mine new unsupervised word-image training pairs from large collections of unlabelled speech and images. Additionally, we use a word-to-image attention mechanism to determine word-image similarity. With this new model, we achieve better performance with fewer shots than any existing approach.",
    "link": "http://arxiv.org/abs/2305.15937",
    "context": "Title: Visually grounded few-shot word acquisition with fewer shots. (arXiv:2305.15937v1 [cs.CL])\nAbstract: We propose a visually grounded speech model that acquires new words and their visual depictions from just a few word-image example pairs. Given a set of test images and a spoken query, we ask the model which image depicts the query word. Previous work has simplified this problem by either using an artificial setting with digit word-image pairs or by using a large number of examples per class. We propose an approach that can work on natural word-image pairs but with less examples, i.e. fewer shots. Our approach involves using the given word-image example pairs to mine new unsupervised word-image training pairs from large collections of unlabelled speech and images. Additionally, we use a word-to-image attention mechanism to determine word-image similarity. With this new model, we achieve better performance with fewer shots than any existing approach.",
    "path": "papers/23/05/2305.15937.json",
    "total_tokens": 887,
    "translated_title": "用更少的数据进行视觉上有依据的少样本词汇习得",
    "translated_abstract": "我们提出了一种基于视觉的语音模型，它可以从仅有少量的词-图像示例对中习得新的词汇及其视觉表示。给定一组测试图像和一个口头查询，我们要求模型指出哪个图像展示了查询词。先前的工作要么使用数字词-图像对的人造环境来简化该问题，要么使用每个类别大量的示例。我们提出了一种方法，可以在自然的词-图像对上进行，但只需更少的数据，即更少的样本。我们的方法包括使用给定的词-图像示例对从大量未标记的语音和图像中挖掘新的无监督词-图像训练对。另外，我们使用了一种单词到图像的注意力机制来确定词-图像的相似度。通过这种新模型，我们实现了比任何现有方法都更好的性能，而且只需更少的数据量。",
    "tldr": "该论文提出了一种基于视觉的语音模型，可以从仅有少量的词-图像示例对中习得新的词汇及其视觉表示，并且与现有方法相比，该模型在使用更少的样本时取得了更好的性能。",
    "en_tdlr": "This paper proposes a visually grounded speech model that can acquire new words and their visual representations from just a few word-image example pairs. The model achieves better performance with fewer shots than any existing approach by using a word-to-image attention mechanism to determine word-image similarity and mining new unsupervised word-image training pairs from large collections of unlabelled speech and images."
}