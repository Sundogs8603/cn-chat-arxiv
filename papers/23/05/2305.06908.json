{
    "title": "CoMoSpeech: One-Step Speech and Singing Voice Synthesis via Consistency Model. (arXiv:2305.06908v1 [cs.SD])",
    "abstract": "Denoising diffusion probabilistic models (DDPMs) have shown promising performance for speech synthesis. However, a large number of iterative steps are required to achieve high sample quality, which restricts the inference speed. Maintaining sample quality while increasing sampling speed has become a challenging task. In this paper, we propose a \"Co\"nsistency \"Mo\"del-based \"Speech\" synthesis method, CoMoSpeech, which achieve speech synthesis through a single diffusion sampling step while achieving high audio quality. The consistency constraint is applied to distill a consistency model from a well-designed diffusion-based teacher model, which ultimately yields superior performances in the distilled CoMoSpeech. Our experiments show that by generating audio recordings by a single sampling step, the CoMoSpeech achieves an inference speed more than 150 times faster than real-time on a single NVIDIA A100 GPU, which is comparable to FastSpeech2, making diffusion-sampling based speech synthesis",
    "link": "http://arxiv.org/abs/2305.06908",
    "context": "Title: CoMoSpeech: One-Step Speech and Singing Voice Synthesis via Consistency Model. (arXiv:2305.06908v1 [cs.SD])\nAbstract: Denoising diffusion probabilistic models (DDPMs) have shown promising performance for speech synthesis. However, a large number of iterative steps are required to achieve high sample quality, which restricts the inference speed. Maintaining sample quality while increasing sampling speed has become a challenging task. In this paper, we propose a \"Co\"nsistency \"Mo\"del-based \"Speech\" synthesis method, CoMoSpeech, which achieve speech synthesis through a single diffusion sampling step while achieving high audio quality. The consistency constraint is applied to distill a consistency model from a well-designed diffusion-based teacher model, which ultimately yields superior performances in the distilled CoMoSpeech. Our experiments show that by generating audio recordings by a single sampling step, the CoMoSpeech achieves an inference speed more than 150 times faster than real-time on a single NVIDIA A100 GPU, which is comparable to FastSpeech2, making diffusion-sampling based speech synthesis",
    "path": "papers/23/05/2305.06908.json",
    "total_tokens": 883,
    "tldr": "CoMoSpeech是一种基于一致性模型的语音合成方法，可以通过单次扩散采样步骤实现语音和歌声的合成，速度非常快。",
    "en_tdlr": "CoMoSpeech is a speech and singing voice synthesis method based on consistency model, which achieves synthesis through a single diffusion sampling step while maintaining high audio quality and fast inference speed, making it a promising alternative to existing methods."
}