{
    "title": "Large Language Models are Zero-Shot Rankers for Recommender Systems. (arXiv:2305.08845v2 [cs.IR] UPDATED)",
    "abstract": "Recently, large language models (LLMs) (e.g., GPT-4) have demonstrated impressive general-purpose task-solving abilities, including the potential to approach recommendation tasks. Along this line of research, this work aims to investigate the capacity of LLMs that act as the ranking model for recommender systems. We first formalize the recommendation problem as a conditional ranking task, considering sequential interaction histories as conditions and the items retrieved by other candidate generation models as candidates. To solve the ranking task by LLMs, we carefully design the prompting template and conduct extensive experiments on two widely-used datasets. We show that LLMs have promising zero-shot ranking abilities but (1) struggle to perceive the order of historical interactions, and (2) can be biased by popularity or item positions in the prompts. We demonstrate that these issues can be alleviated using specially designed prompting and bootstrapping strategies. Equipped with thes",
    "link": "http://arxiv.org/abs/2305.08845",
    "context": "Title: Large Language Models are Zero-Shot Rankers for Recommender Systems. (arXiv:2305.08845v2 [cs.IR] UPDATED)\nAbstract: Recently, large language models (LLMs) (e.g., GPT-4) have demonstrated impressive general-purpose task-solving abilities, including the potential to approach recommendation tasks. Along this line of research, this work aims to investigate the capacity of LLMs that act as the ranking model for recommender systems. We first formalize the recommendation problem as a conditional ranking task, considering sequential interaction histories as conditions and the items retrieved by other candidate generation models as candidates. To solve the ranking task by LLMs, we carefully design the prompting template and conduct extensive experiments on two widely-used datasets. We show that LLMs have promising zero-shot ranking abilities but (1) struggle to perceive the order of historical interactions, and (2) can be biased by popularity or item positions in the prompts. We demonstrate that these issues can be alleviated using specially designed prompting and bootstrapping strategies. Equipped with thes",
    "path": "papers/23/05/2305.08845.json",
    "total_tokens": 863,
    "translated_title": "大型语言模型是零-shot推荐系统排名者",
    "translated_abstract": "最近，大型语言模型（例如GPT-4）展示出了令人印象深刻的通用任务解决能力，包括潜力接近推荐任务。在这一研究方向上，本文旨在研究作为推荐系统排名模型的LLMs的能力。",
    "tldr": "大型语言模型表现出有希望的零-shot排名能力，但在感知历史互动顺序和受到偏见影响方面存在问题。本研究通过特殊设计的提示和引导策略来缓解这些问题。",
    "en_tdlr": "Large language models show promising zero-shot ranking abilities but have issues in perceiving the order of historical interactions and being biased. This study addresses these issues through specially designed prompting and bootstrapping strategies."
}