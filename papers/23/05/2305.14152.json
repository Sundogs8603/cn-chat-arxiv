{
    "title": "Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization. (arXiv:2305.14152v2 [cs.LG] UPDATED)",
    "abstract": "Large language models (LLMs) face the challenges in fine-tuning and deployment due to their high memory demands and computational costs. While parameter-efficient fine-tuning (PEFT) methods aim to reduce the memory usage of the optimizer state during fine-tuning, the inherent size of pre-trained LLM weights continues to be a pressing concern. Even though quantization techniques are widely proposed to ease memory demands and accelerate LLM inference, most of these techniques are geared towards the deployment phase. To bridge this gap, this paper presents Parameter-Efficient and Quantization-aware Adaptation (PEQA) - a simple yet effective method that combines the advantages of PEFT with quantized LLMs. By updating solely the quantization scales, PEQA can be directly applied to quantized LLMs, ensuring seamless task transitions. Parallel to existing PEFT methods, PEQA significantly reduces the memory overhead associated with the optimizer state. Furthermore, it leverages the advantages o",
    "link": "http://arxiv.org/abs/2305.14152",
    "context": "Title: Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization. (arXiv:2305.14152v2 [cs.LG] UPDATED)\nAbstract: Large language models (LLMs) face the challenges in fine-tuning and deployment due to their high memory demands and computational costs. While parameter-efficient fine-tuning (PEFT) methods aim to reduce the memory usage of the optimizer state during fine-tuning, the inherent size of pre-trained LLM weights continues to be a pressing concern. Even though quantization techniques are widely proposed to ease memory demands and accelerate LLM inference, most of these techniques are geared towards the deployment phase. To bridge this gap, this paper presents Parameter-Efficient and Quantization-aware Adaptation (PEQA) - a simple yet effective method that combines the advantages of PEFT with quantized LLMs. By updating solely the quantization scales, PEQA can be directly applied to quantized LLMs, ensuring seamless task transitions. Parallel to existing PEFT methods, PEQA significantly reduces the memory overhead associated with the optimizer state. Furthermore, it leverages the advantages o",
    "path": "papers/23/05/2305.14152.json",
    "total_tokens": 907,
    "translated_title": "通过子4位整数量化实现压缩大型语言模型的内存高效微调",
    "translated_abstract": "大型语言模型（LLM）面临着在微调和部署过程中由于其高内存需求和计算成本而带来的挑战。虽然参数高效微调（PEFT）方法旨在减少微调过程中优化器状态的内存使用量，但预训练LLM权重本身的大小仍然是一个紧迫的问题。尽管量化技术被广泛提出来缓解内存需求和加快LLM推理速度，但大多数这些技术都是针对部署阶段。为了弥合这一差距，本文提出了参数高效和量化感知适应（PEQA）-一种简单而有效的方法，将PEFT的优点与量化LLM结合起来。通过仅更新量化尺度，PEQA可以直接应用于量化LLM，确保平稳的任务转换。与现有的PEFT方法并行，PEQA显着减少了与优化器状态相关的内存开销。此外，它还充分利用了",
    "tldr": "本文提出了一种称为PEQA的方法，结合了参数高效微调和量化LLM的优点。通过仅更新量化尺度，PEQA可以高效地微调压缩大型语言模型，并减少内存开销。",
    "en_tdlr": "This paper presents PEQA, a method that combines parameter-efficient fine-tuning with quantized large language models. By updating only the quantization scales, PEQA efficiently fine-tunes compressed language models and reduces memory overhead."
}