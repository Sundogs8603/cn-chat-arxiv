{
    "title": "Massively Parallel Reweighted Wake-Sleep. (arXiv:2305.11022v1 [cs.LG])",
    "abstract": "Reweighted wake-sleep (RWS) is a machine learning method for performing Bayesian inference in a very general class of models. RWS draws $K$ samples from an underlying approximate posterior, then uses importance weighting to provide a better estimate of the true posterior. RWS then updates its approximate posterior towards the importance-weighted estimate of the true posterior. However, recent work [Chattergee and Diaconis, 2018] indicates that the number of samples required for effective importance weighting is exponential in the number of latent variables. Attaining such a large number of importance samples is intractable in all but the smallest models. Here, we develop massively parallel RWS, which circumvents this issue by drawing $K$ samples of all $n$ latent variables, and individually reasoning about all $K^n$ possible combinations of samples. While reasoning about $K^n$ combinations might seem intractable, the required computations can be performed in polynomial time by exploiti",
    "link": "http://arxiv.org/abs/2305.11022",
    "context": "Title: Massively Parallel Reweighted Wake-Sleep. (arXiv:2305.11022v1 [cs.LG])\nAbstract: Reweighted wake-sleep (RWS) is a machine learning method for performing Bayesian inference in a very general class of models. RWS draws $K$ samples from an underlying approximate posterior, then uses importance weighting to provide a better estimate of the true posterior. RWS then updates its approximate posterior towards the importance-weighted estimate of the true posterior. However, recent work [Chattergee and Diaconis, 2018] indicates that the number of samples required for effective importance weighting is exponential in the number of latent variables. Attaining such a large number of importance samples is intractable in all but the smallest models. Here, we develop massively parallel RWS, which circumvents this issue by drawing $K$ samples of all $n$ latent variables, and individually reasoning about all $K^n$ possible combinations of samples. While reasoning about $K^n$ combinations might seem intractable, the required computations can be performed in polynomial time by exploiti",
    "path": "papers/23/05/2305.11022.json",
    "total_tokens": 933,
    "translated_title": "大规模并行重新加权唤醒-睡眠",
    "translated_abstract": "重新加权唤醒-睡眠算法（RWS）是一种适用于非常通用的模型执行贝叶斯推断的机器学习方法。它从潜在近似后验概率中抽取$K$个样本，然后使用重要性加权来提供更好的真实后验概率估计。RWS然后更新其近似后验概率，向真实后验概率的重要性加权估计移动。然而，近期的研究表明，对于有效的重要性加权，所需样本数与潜在变量的数量呈指数关系。在所有但最小的模型中实现如此大数量的重要性样本是不可行的。 在这里，我们开发了大规模并行的RWS，通过抽取所有$n$个潜在变量的$K$个样本，并单独考虑所有$K^n$个可能的样本组合，避免了这个问题。虽然考虑$K^n$个组合似乎是不可行的，但所需的计算可以通过利用计算结构简化在多项式时间内完成。",
    "tldr": "大规模并行重新加权唤醒-睡眠算法通过抽取$K^n$个可能的样本组合，避免了原方法中大量潜在变量数目导致有效性下降问题。"
}