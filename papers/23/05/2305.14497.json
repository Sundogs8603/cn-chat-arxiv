{
    "title": "Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement. (arXiv:2305.14497v1 [cs.CL])",
    "abstract": "Prompting methods such as Chain-of-Thought (CoT) have shed new light on enhancing the reasoning capabilities of large language models, and researchers have extensively explored the generation process of rationales and answers. However, they have overlooked the potential challenges posed by the poor quality of reasoning problems, which may influence the reasoning performance significantly. In this work, we propose Self-Polish (SP), a novel method that facilitates the model's problem-solving process by prompting them to progressively refine the given problems to be more comprehensible and solvable. Specifically, the method teaches models to eliminate irrelevant information, rearrange the logic structure and organize local conditions into new ones parallelly. SP is orthogonal to all other prompting methods, making it convenient to integrate with state-of-the-art techniques for further improvement. We conduct thorough experiments on five benchmarks to illustrate the effectiveness of the pr",
    "link": "http://arxiv.org/abs/2305.14497",
    "context": "Title: Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement. (arXiv:2305.14497v1 [cs.CL])\nAbstract: Prompting methods such as Chain-of-Thought (CoT) have shed new light on enhancing the reasoning capabilities of large language models, and researchers have extensively explored the generation process of rationales and answers. However, they have overlooked the potential challenges posed by the poor quality of reasoning problems, which may influence the reasoning performance significantly. In this work, we propose Self-Polish (SP), a novel method that facilitates the model's problem-solving process by prompting them to progressively refine the given problems to be more comprehensible and solvable. Specifically, the method teaches models to eliminate irrelevant information, rearrange the logic structure and organize local conditions into new ones parallelly. SP is orthogonal to all other prompting methods, making it convenient to integrate with state-of-the-art techniques for further improvement. We conduct thorough experiments on five benchmarks to illustrate the effectiveness of the pr",
    "path": "papers/23/05/2305.14497.json",
    "total_tokens": 808,
    "translated_title": "自我精磨：通过问题精化增强大型语言模型的推理能力",
    "translated_abstract": "利用 Chain-of-Thought（CoT）等提示方法可以增强大型语言模型的推理能力，研究人员广泛探索了合理化和答案生成过程。然而，他们忽略了低质量推理问题可能会显着影响推理性能的潜在挑战。本文提出了Self-Polish（SP）方法，通过促使模型逐步完善给定的问题以使其更易理解和可解决，以促进模型的问题解决过程。具体而言，该方法教授模型消除无关信息，重新排列逻辑结构，并将局部条件并行组织成新的条件。 SP与所有其他提示方法正交，方便将其与最先进的技术集成以进一步提高性能。我们在五个基准测试中进行了彻底的实验以证明其有效性。",
    "tldr": "Self-Polish是一种方法，通过促使模型逐步完善给定的问题以使其更易理解和可解决，以增强大型语言模型的推理能力。",
    "en_tdlr": "Self-Polish is a method that enhances the reasoning capabilities of large language models by prompting them to progressively refine the given problems to be more comprehensible and solvable."
}