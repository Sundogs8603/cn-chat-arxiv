{
    "title": "Solving Cosine Similarity Underestimation between High Frequency Words by L2 Norm Discounting. (arXiv:2305.10610v1 [cs.CL])",
    "abstract": "Cosine similarity between two words, computed using their contextualised token embeddings obtained from masked language models (MLMs) such as BERT has shown to underestimate the actual similarity between those words (Zhou et al., 2022). This similarity underestimation problem is particularly severe for highly frequent words. Although this problem has been noted in prior work, no solution has been proposed thus far. We observe that the L2 norm of contextualised embeddings of a word correlates with its log-frequency in the pretraining corpus. Consequently, the larger L2 norms associated with the highly frequent words reduce the cosine similarity values measured between them, thus underestimating the similarity scores. To solve this issue, we propose a method to discount the L2 norm of a contextualised word embedding by the frequency of that word in a corpus when measuring the cosine similarities between words. We show that the so called stop words behave differently from the rest of the ",
    "link": "http://arxiv.org/abs/2305.10610",
    "context": "Title: Solving Cosine Similarity Underestimation between High Frequency Words by L2 Norm Discounting. (arXiv:2305.10610v1 [cs.CL])\nAbstract: Cosine similarity between two words, computed using their contextualised token embeddings obtained from masked language models (MLMs) such as BERT has shown to underestimate the actual similarity between those words (Zhou et al., 2022). This similarity underestimation problem is particularly severe for highly frequent words. Although this problem has been noted in prior work, no solution has been proposed thus far. We observe that the L2 norm of contextualised embeddings of a word correlates with its log-frequency in the pretraining corpus. Consequently, the larger L2 norms associated with the highly frequent words reduce the cosine similarity values measured between them, thus underestimating the similarity scores. To solve this issue, we propose a method to discount the L2 norm of a contextualised word embedding by the frequency of that word in a corpus when measuring the cosine similarities between words. We show that the so called stop words behave differently from the rest of the ",
    "path": "papers/23/05/2305.10610.json",
    "total_tokens": 895,
    "translated_title": "利用L2范数折扣解决高频词余弦相似度低估问题",
    "translated_abstract": "通过使用来自掩码语言模型（MLM）如BERT的上下文化标记嵌入来计算两个单词之间的余弦相似性，已经证明会低估这些单词之间的实际相似性。高频词的相似度低估问题尤其严重。虽然这个问题已经在先前的工作中被注意到，但目前尚未提出解决方案。我们观察到一个单词的上下文化嵌入的L2范数与其在预训练语料库中的对数频率相关。因此，与高频词相关的更大的L2范数降低了它们之间的余弦相似度值，因此低估了它们之间的相似度分数。为解决这个问题，我们提出了一种方法，在计算词之间的余弦相似度时，通过该词在语料库中的频率折扣来计算上下文化单词嵌入的L2范数。我们展示了所谓的停用词表现不同于其他单词。",
    "tldr": "通过L2范数折扣来解决高频词的余弦相似度低估问题",
    "en_tdlr": "Discounting L2 norm is proposed to solve the cosine similarity underestimation between high frequency words, where the larger L2 norms of the highly frequent words reduce the cosine similarity values measured between them, and stop words behave differently from the rest of the words."
}