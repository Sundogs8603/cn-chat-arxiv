{
    "title": "Differentially Private Adapters for Parameter Efficient Acoustic Modeling. (arXiv:2305.11360v1 [cs.SD])",
    "abstract": "In this work, we devise a parameter-efficient solution to bring differential privacy (DP) guarantees into adaptation of a cross-lingual speech classifier. We investigate a new frozen pre-trained adaptation framework for DP-preserving speech modeling without full model fine-tuning. First, we introduce a noisy teacher-student ensemble into a conventional adaptation scheme leveraging a frozen pre-trained acoustic model and attain superior performance than DP-based stochastic gradient descent (DPSGD). Next, we insert residual adapters (RA) between layers of the frozen pre-trained acoustic model. The RAs reduce training cost and time significantly with a negligible performance drop. Evaluated on the open-access Multilingual Spoken Words (MLSW) dataset, our solution reduces the number of trainable parameters by 97.5% using the RAs with only a 4% performance drop with respect to fine-tuning the cross-lingual speech classifier while preserving DP guarantees.",
    "link": "http://arxiv.org/abs/2305.11360",
    "context": "Title: Differentially Private Adapters for Parameter Efficient Acoustic Modeling. (arXiv:2305.11360v1 [cs.SD])\nAbstract: In this work, we devise a parameter-efficient solution to bring differential privacy (DP) guarantees into adaptation of a cross-lingual speech classifier. We investigate a new frozen pre-trained adaptation framework for DP-preserving speech modeling without full model fine-tuning. First, we introduce a noisy teacher-student ensemble into a conventional adaptation scheme leveraging a frozen pre-trained acoustic model and attain superior performance than DP-based stochastic gradient descent (DPSGD). Next, we insert residual adapters (RA) between layers of the frozen pre-trained acoustic model. The RAs reduce training cost and time significantly with a negligible performance drop. Evaluated on the open-access Multilingual Spoken Words (MLSW) dataset, our solution reduces the number of trainable parameters by 97.5% using the RAs with only a 4% performance drop with respect to fine-tuning the cross-lingual speech classifier while preserving DP guarantees.",
    "path": "papers/23/05/2305.11360.json",
    "total_tokens": 982,
    "translated_title": "差分隐私适配器与参数高效声学建模",
    "translated_abstract": "本文提出了一种高效的方案，将差分隐私（DP）保证引入跨语言语音分类器的适配中。我们探索了一种新的冻结预训练适配框架，用于保持DP保护语音建模，无需全模型微调。首先，我们将一个有噪声的教师-学生集成引入传统的适配方案中，利用冻结的预训练声学模型，并取得比基于随机梯度下降的DP（DPSGD）更好的性能。接着，我们在冻结的预训练声学模型的层之间插入残差适配器（RA）。RA显著降低了训练成本和时间，同时性能下降可忽略不计。在公开的多语言口语词语（MLSW）数据集上进行评估，我们的方案使用RA降低了97.5%的可训练参数，而与微调跨语言语音分类器相比仅有4%的性能下降，同时保持DP保证。",
    "tldr": "本研究提出一种高效的方案，将差分隐私保证引入跨语言语音分类器的适配中，使用教师-学生集成和残差适配器可以显著降低训练成本和时间，同时保持DP保证，使得可训练参数减少97.5%且性能基本不受影响。",
    "en_tdlr": "This study proposes an efficient solution for incorporating differential privacy (DP) guarantees into cross-lingual speech classifier adaptation, achieving superior performance compared to DP-based stochastic gradient descent (DPSGD) through the use of a noisy teacher-student ensemble and residual adapters (RA). The solution reduces trainable parameters by 97.5% while maintaining DP guarantees and only a 4% performance drop compared to fine-tuning the classifier."
}