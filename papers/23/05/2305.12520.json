{
    "title": "SLaDe: A Portable Small Language Model Decompiler for Optimized Assembly. (arXiv:2305.12520v2 [cs.PL] UPDATED)",
    "abstract": "Decompilation is a well-studied area with numerous high-quality tools available. These are frequently used for security tasks and to port legacy code. However, they regularly generate difficult-to-read programs and require a large amount of engineering effort to support new programming languages and ISAs. Recent interest in neural approaches has produced portable tools that generate readable code. However, to-date such techniques are usually restricted to synthetic programs without optimization, and no models have evaluated their portability. Furthermore, while the code generated may be more readable, it is usually incorrect. This paper presents SLaDe, a Small Language model Decompiler based on a sequence-to-sequence transformer trained over real-world code. We develop a novel tokenizer and exploit no-dropout training to produce high-quality code. We utilize type-inference to generate programs that are more readable and accurate than standard analytic and recent neural approaches. Unli",
    "link": "http://arxiv.org/abs/2305.12520",
    "context": "Title: SLaDe: A Portable Small Language Model Decompiler for Optimized Assembly. (arXiv:2305.12520v2 [cs.PL] UPDATED)\nAbstract: Decompilation is a well-studied area with numerous high-quality tools available. These are frequently used for security tasks and to port legacy code. However, they regularly generate difficult-to-read programs and require a large amount of engineering effort to support new programming languages and ISAs. Recent interest in neural approaches has produced portable tools that generate readable code. However, to-date such techniques are usually restricted to synthetic programs without optimization, and no models have evaluated their portability. Furthermore, while the code generated may be more readable, it is usually incorrect. This paper presents SLaDe, a Small Language model Decompiler based on a sequence-to-sequence transformer trained over real-world code. We develop a novel tokenizer and exploit no-dropout training to produce high-quality code. We utilize type-inference to generate programs that are more readable and accurate than standard analytic and recent neural approaches. Unli",
    "path": "papers/23/05/2305.12520.json",
    "total_tokens": 930,
    "translated_title": "SLaDe: 一种用于优化汇编代码的可移植小型语言模型反编译器",
    "translated_abstract": "反编译是一个研究较为广泛的领域，有许多高质量的工具可供使用。这些工具通常用于安全任务和移植遗留代码。然而，它们经常生成难以阅读的程序，并且需要大量的工程工作来支持新的编程语言和指令集架构。最近关注神经方法产生了能生成可读代码的可移植工具。然而，迄今为止，这些技术通常只适用于没有优化的合成程序，并且没有模型评估它们的可移植性。此外，虽然生成的代码可能更易读，但通常是不正确的。本文介绍了SLaDe，一种基于序列到序列变换器训练的小型语言模型反编译器，该变换器是使用实际代码进行训练的。我们开发了一种新颖的分词器，并利用无丢弃训练来产生高质量的代码。我们利用类型推断生成比标准分析方法和最近的神经方法更易读和更准确的程序。",
    "tldr": "SLaDe是一种基于小型语言模型的反编译器，通过训练真实代码的序列到序列变换器，使用了新颖的分词器、无丢弃训练和类型推断等方法，能够生成更易读和更准确的程序。",
    "en_tdlr": "SLaDe is a decompiler based on a small language model that generates more readable and accurate programs by training a sequence-to-sequence transformer on real-world code, utilizing a novel tokenizer, no-dropout training, and type inference."
}