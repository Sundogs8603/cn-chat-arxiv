{
    "title": "Meta-review Generation with Checklist-guided Iterative Introspection. (arXiv:2305.14647v1 [cs.CL])",
    "abstract": "Opinions in the scientific domain can be divergent, leading to controversy or consensus among reviewers. However, current opinion summarization datasets mostly focus on product review domains, which do not account for this variability under the assumption that the input opinions are non-controversial. To address this gap, we propose the task of scientific opinion summarization, where research paper reviews are synthesized into meta-reviews. To facilitate this task, we introduce a new ORSUM dataset covering 10,989 paper meta-reviews and 40,903 paper reviews from 39 conferences. Furthermore, we propose the Checklist-guided Iterative Introspection (CGI$^2$) approach, which breaks down the task into several stages and iteratively refines the summary under the guidance of questions from a checklist. We conclude that (1) human-written summaries are not always reliable since many do not follow the guideline, and (2) the combination of task decomposition and iterative self-refinement shows pro",
    "link": "http://arxiv.org/abs/2305.14647",
    "context": "Title: Meta-review Generation with Checklist-guided Iterative Introspection. (arXiv:2305.14647v1 [cs.CL])\nAbstract: Opinions in the scientific domain can be divergent, leading to controversy or consensus among reviewers. However, current opinion summarization datasets mostly focus on product review domains, which do not account for this variability under the assumption that the input opinions are non-controversial. To address this gap, we propose the task of scientific opinion summarization, where research paper reviews are synthesized into meta-reviews. To facilitate this task, we introduce a new ORSUM dataset covering 10,989 paper meta-reviews and 40,903 paper reviews from 39 conferences. Furthermore, we propose the Checklist-guided Iterative Introspection (CGI$^2$) approach, which breaks down the task into several stages and iteratively refines the summary under the guidance of questions from a checklist. We conclude that (1) human-written summaries are not always reliable since many do not follow the guideline, and (2) the combination of task decomposition and iterative self-refinement shows pro",
    "path": "papers/23/05/2305.14647.json",
    "total_tokens": 899,
    "translated_title": "基于检查表引导迭代自查的元评审生成",
    "translated_abstract": "在科学领域中，不同的观点可能会导致对评审意见的争议或共识。然而，当前的观点总结数据集主要集中在产品评论领域，没有考虑到这种可变性，假设输入的意见是没有争议的。为了填补这一空白，我们提出了科学观点总结的任务，将研究论文评审合成为元评审。为了促进这个任务，我们引入了一个新的ORSUM数据集，涵盖了来自39个会议的10,989篇论文元审查和40,903篇论文审查。此外，我们提出了基于检查表引导迭代自查的方法，将任务分解为几个阶段，并在检查表的指导下迭代地完善摘要。我们得出结论：（1）人工撰写的摘要并不总是可靠的，因为许多人并没有遵循指南，（2）任务分解和迭代自我完善的组合表现出了良好的效果。",
    "tldr": "本文提出了科学观点总结的任务，以合成研究论文评审的元评审，引入了一个新的ORSUM数据集，并提出了基于检查表引导迭代自查的方法，该方法表现出良好的效果。"
}