{
    "title": "Cross-modality Data Augmentation for End-to-End Sign Language Translation. (arXiv:2305.11096v2 [cs.CL] UPDATED)",
    "abstract": "End-to-end sign language translation (SLT) aims to convert sign language videos into spoken language texts directly without intermediate representations. It has been a challenging task due to the modality gap between sign videos and texts and the data scarcity of labeled data. To tackle these challenges, we propose a novel Cross-modality Data Augmentation (XmDA) framework to transfer the powerful gloss-to-text translation capabilities to end-to-end sign language translation (i.e. video-to-text) by exploiting pseudo gloss-text pairs from the sign gloss translation model. Specifically, XmDA consists of two key components, namely, cross-modality mix-up and cross-modality knowledge distillation. The former explicitly encourages the alignment between sign video features and gloss embeddings to bridge the modality gap. The latter utilizes the generation knowledge from gloss-to-text teacher models to guide the spoken language text generation. Experimental results on two widely used SLT datase",
    "link": "http://arxiv.org/abs/2305.11096",
    "context": "Title: Cross-modality Data Augmentation for End-to-End Sign Language Translation. (arXiv:2305.11096v2 [cs.CL] UPDATED)\nAbstract: End-to-end sign language translation (SLT) aims to convert sign language videos into spoken language texts directly without intermediate representations. It has been a challenging task due to the modality gap between sign videos and texts and the data scarcity of labeled data. To tackle these challenges, we propose a novel Cross-modality Data Augmentation (XmDA) framework to transfer the powerful gloss-to-text translation capabilities to end-to-end sign language translation (i.e. video-to-text) by exploiting pseudo gloss-text pairs from the sign gloss translation model. Specifically, XmDA consists of two key components, namely, cross-modality mix-up and cross-modality knowledge distillation. The former explicitly encourages the alignment between sign video features and gloss embeddings to bridge the modality gap. The latter utilizes the generation knowledge from gloss-to-text teacher models to guide the spoken language text generation. Experimental results on two widely used SLT datase",
    "path": "papers/23/05/2305.11096.json",
    "total_tokens": 1064,
    "translated_title": "跨模态数据增强用于端到端手语翻译",
    "translated_abstract": "端到端手语翻译旨在直接将手语视频转换为口语文本，无需中间表示。受手语视频和文本之间的模态差距和标记数据的稀缺性的挑战，这一任务一直很具有挑战性。为了应对这些挑战，我们提出了一种新颖的“跨模态数据增强（XmDA）”框架，通过利用来自手语单词翻译模型的伪手语单词-文本对，将强大的手语单词到文本的翻译能力转移到了端到端手语翻译（即视频到文本）。具体来说，XmDA包括两个关键组成部分，即跨模态混合和跨模态知识蒸馏。前者明确地促进手语视频特征和手语单词嵌入之间的对齐，以弥合模态差距。后者利用来自手语单词到文本的教师模型的生成知识来指导口语文本生成。在两个广泛使用的手语翻译数据集LIBRISIGN和WLASL上的实验结果表明，XmDA在自动评估指标和人类评估方面均明显优于现有的最先进方法。",
    "tldr": "本文提出了一种Cross-modality Data Augmentation（XmDA）框架，通过利用来自手语单词翻译模型的伪手语单词-文本对，将强大的手语单词到文本的翻译能力转移到了端到端手语翻译，实验结果表明XmDA在该领域中明显优于现有的最先进方法。",
    "en_tdlr": "This paper proposes a Cross-modality Data Augmentation (XmDA) framework that transfers powerful gloss-to-text translation capabilities to end-to-end sign language translation by exploiting pseudo gloss-text pairs from the sign gloss translation model, resulting in significant outperformance of state-of-the-art methods in both automatic evaluation metrics and human evaluations."
}