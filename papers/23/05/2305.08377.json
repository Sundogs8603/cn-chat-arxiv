{
    "title": "Text Classification via Large Language Models. (arXiv:2305.08377v2 [cs.CL] UPDATED)",
    "abstract": "Despite the remarkable success of large-scale Language Models (LLMs) such as GPT-3, their performances still significantly underperform fine-tuned models in the task of text classification. This is due to (1) the lack of reasoning ability in addressing complex linguistic phenomena (e.g., intensification, contrast, irony etc); (2) limited number of tokens allowed in in-context learning.  In this paper, we introduce Clue And Reasoning Prompting (CARP). CARP adopts a progressive reasoning strategy tailored to addressing the complex linguistic phenomena involved in text classification: CARP first prompts LLMs to find superficial clues (e.g., keywords, tones, semantic relations, references, etc), based on which a diagnostic reasoning process is induced for final decisions. To further address the limited-token issue, CARP uses a fine-tuned model on the supervised dataset for $k$NN demonstration search in the in-context learning, allowing the model to take the advantage of both LLM's generali",
    "link": "http://arxiv.org/abs/2305.08377",
    "context": "Title: Text Classification via Large Language Models. (arXiv:2305.08377v2 [cs.CL] UPDATED)\nAbstract: Despite the remarkable success of large-scale Language Models (LLMs) such as GPT-3, their performances still significantly underperform fine-tuned models in the task of text classification. This is due to (1) the lack of reasoning ability in addressing complex linguistic phenomena (e.g., intensification, contrast, irony etc); (2) limited number of tokens allowed in in-context learning.  In this paper, we introduce Clue And Reasoning Prompting (CARP). CARP adopts a progressive reasoning strategy tailored to addressing the complex linguistic phenomena involved in text classification: CARP first prompts LLMs to find superficial clues (e.g., keywords, tones, semantic relations, references, etc), based on which a diagnostic reasoning process is induced for final decisions. To further address the limited-token issue, CARP uses a fine-tuned model on the supervised dataset for $k$NN demonstration search in the in-context learning, allowing the model to take the advantage of both LLM's generali",
    "path": "papers/23/05/2305.08377.json",
    "total_tokens": 943,
    "translated_title": "基于大型语言模型的文本分类",
    "translated_abstract": "尽管像GPT-3这样的大规模语言模型（LLM）取得了显著的成功，但它们在文本分类任务中的表现仍然显著不及微调模型。这是由于(1)缺乏处理复杂语言现象（例如强调、对比、反讽等）的推理能力； (2)在上下文学习中只允许有限数量的标记。在本文中，我们介绍了Clue And Reasoning Prompting (CARP)，CARP采用一种逐步推理策略，旨在应对涉及文本分类的复杂语言现象：CARP首先提示LLMs找到表面线索（例如关键词、语气、语义关系、参考等），然后诱导诊断性推理过程作出最终决策。为了进一步解决有限标记的问题，CARP在监督数据集上使用微调模型进行$k$NN演示搜索，在上下文学习中充分利用了LLM的优势。",
    "tldr": "本文介绍了Clue And Reasoning Prompting (CARP)算法，采用逐步推理策略优化了大型语言模型在文本分类中处理复杂语言现象的能力；并通过在监督数据集上使用微调模型进行$k$NN演示搜索，解决了上下文学习中有限标记的问题。",
    "en_tdlr": "This paper introduces the Clue And Reasoning Prompting (CARP) algorithm, which improves the ability of large language models (LLMs) to handle complex linguistic phenomena in text classification through a progressive reasoning strategy; and solves the problem of limited tokens in in-context learning by using a fine-tuned model for $k$NN demonstration search on a supervised dataset."
}