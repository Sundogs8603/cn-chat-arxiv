{
    "title": "HighLight: Efficient and Flexible DNN Acceleration with Hierarchical Structured Sparsity. (arXiv:2305.12718v2 [cs.AR] UPDATED)",
    "abstract": "Due to complex interactions among various deep neural network (DNN) optimization techniques, modern DNNs can have weights and activations that are dense or sparse with diverse sparsity degrees. To offer a good trade-off between accuracy and hardware performance, an ideal DNN accelerator should have high flexibility to efficiently translate DNN sparsity into reductions in energy and/or latency without incurring significant complexity overhead.  This paper introduces hierarchical structured sparsity (HSS), with the key insight that we can systematically represent diverse sparsity degrees by having them hierarchically composed from multiple simple sparsity patterns. As a result, HSS simplifies the underlying hardware since it only needs to support simple sparsity patterns; this significantly reduces the sparsity acceleration overhead, which improves efficiency. Motivated by such opportunities, we propose a simultaneously efficient and flexible accelerator, named HighLight, to accelerate D",
    "link": "http://arxiv.org/abs/2305.12718",
    "context": "Title: HighLight: Efficient and Flexible DNN Acceleration with Hierarchical Structured Sparsity. (arXiv:2305.12718v2 [cs.AR] UPDATED)\nAbstract: Due to complex interactions among various deep neural network (DNN) optimization techniques, modern DNNs can have weights and activations that are dense or sparse with diverse sparsity degrees. To offer a good trade-off between accuracy and hardware performance, an ideal DNN accelerator should have high flexibility to efficiently translate DNN sparsity into reductions in energy and/or latency without incurring significant complexity overhead.  This paper introduces hierarchical structured sparsity (HSS), with the key insight that we can systematically represent diverse sparsity degrees by having them hierarchically composed from multiple simple sparsity patterns. As a result, HSS simplifies the underlying hardware since it only needs to support simple sparsity patterns; this significantly reduces the sparsity acceleration overhead, which improves efficiency. Motivated by such opportunities, we propose a simultaneously efficient and flexible accelerator, named HighLight, to accelerate D",
    "path": "papers/23/05/2305.12718.json",
    "total_tokens": 890,
    "translated_title": "HighLight: 高效灵活的深度神经网络加速器与分层结构稀疏性",
    "translated_abstract": "由于各种深度神经网络（DNN）优化技术之间的复杂相互作用，现代DNN的权重和激活可以是稠密的或稀疏的，其稀疏程度各不相同。为了在准确性和硬件性能之间找到一个良好的权衡，理想的DNN加速器应具有高度灵活性，能够有效地将DNN的稀疏性转化为能量和/或延迟的降低而不会带来显著的复杂性开销。本文介绍了分层结构稀疏性（HSS）, 通过将多个简单稀疏模式层级组合来系统地表示不同的稀疏程度。因此，HSS简化了底层硬件，因为它只需要支持简单稀疏模式，这显著减少了稀疏加速的开销，提高了效率。在这种机会的推动下，我们提出了一种同时高效灵活的加速器，命名为HighLight，用于加速DNN。",
    "tldr": "HighLight是一种高效灵活的DNN加速器，通过分层结构稀疏性实现了对DNN的有效加速，优化了能耗和延迟的权衡。"
}