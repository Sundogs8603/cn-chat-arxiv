{
    "title": "DEnsity: Open-domain Dialogue Evaluation Metric using Density Estimation. (arXiv:2305.04720v2 [cs.CL] UPDATED)",
    "abstract": "Despite the recent advances in open-domain dialogue systems, building a reliable evaluation metric is still a challenging problem. Recent studies proposed learnable metrics based on classification models trained to distinguish the correct response. However, neural classifiers are known to make overly confident predictions for examples from unseen distributions. We propose DEnsity, which evaluates a response by utilizing density estimation on the feature space derived from a neural classifier. Our metric measures how likely a response would appear in the distribution of human conversations. Moreover, to improve the performance of DEnsity, we utilize contrastive learning to further compress the feature space. Experiments on multiple response evaluation datasets show that DEnsity correlates better with human evaluations than the existing metrics. Our code is available at https://github.com/ddehun/DEnsity.",
    "link": "http://arxiv.org/abs/2305.04720",
    "context": "Title: DEnsity: Open-domain Dialogue Evaluation Metric using Density Estimation. (arXiv:2305.04720v2 [cs.CL] UPDATED)\nAbstract: Despite the recent advances in open-domain dialogue systems, building a reliable evaluation metric is still a challenging problem. Recent studies proposed learnable metrics based on classification models trained to distinguish the correct response. However, neural classifiers are known to make overly confident predictions for examples from unseen distributions. We propose DEnsity, which evaluates a response by utilizing density estimation on the feature space derived from a neural classifier. Our metric measures how likely a response would appear in the distribution of human conversations. Moreover, to improve the performance of DEnsity, we utilize contrastive learning to further compress the feature space. Experiments on multiple response evaluation datasets show that DEnsity correlates better with human evaluations than the existing metrics. Our code is available at https://github.com/ddehun/DEnsity.",
    "path": "papers/23/05/2305.04720.json",
    "total_tokens": 830,
    "translated_title": "DEnsity: 利用密度估计的开放域对话评估度量",
    "translated_abstract": "尽管开放域对话系统在最近取得了一些进展，但构建一个可靠的评估度量仍然是一个具有挑战性的问题。最近的研究提出了基于分类模型的可学习度量，它们被训练用于区分正确的响应。然而，神经分类器对于来自未见分布的样本会做出过于自信的预测。我们提出了 DEsity，利用神经分类器从特征空间派生特征，并利用密度估计来评估响应。我们的度量器测量响应在人类对话分布中出现的可能性。此外，为了提高 DEnsity 的性能，我们利用对比学习进一步压缩了特征空间。多个响应评估数据集上的实验表明，DEnsity 与现有度量器相比更好地与人类评估相关。我们的代码可在 https://github.com/ddehun/DEnsity 获取。",
    "tldr": "DEnsity 提出了一种利用密度估计的开放域对话评估新方法，在特征空间中评估响应可能性来更好地与人类评估相关。",
    "en_tdlr": "DEnsity proposes a novel approach to open-domain dialogue evaluation using density estimation, which measures the probability of a response appearing in the distribution of human conversations in the feature space to better correlate with human evaluations."
}