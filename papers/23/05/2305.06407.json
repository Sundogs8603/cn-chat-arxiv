{
    "title": "Combo of Thinking and Observing for Outside-Knowledge VQA. (arXiv:2305.06407v1 [cs.CV])",
    "abstract": "Outside-knowledge visual question answering is a challenging task that requires both the acquisition and the use of open-ended real-world knowledge. Some existing solutions draw external knowledge into the cross-modality space which overlooks the much vaster textual knowledge in natural-language space, while others transform the image into a text that further fuses with the textual knowledge into the natural-language space and completely abandons the use of visual features. In this paper, we are inspired to constrain the cross-modality space into the same space of natural-language space which makes the visual features preserved directly, and the model still benefits from the vast knowledge in natural-language space. To this end, we propose a novel framework consisting of a multimodal encoder, a textual encoder and an answer decoder. Such structure allows us to introduce more types of knowledge including explicit and implicit multimodal and textual knowledge. Extensive experiments valid",
    "link": "http://arxiv.org/abs/2305.06407",
    "context": "Title: Combo of Thinking and Observing for Outside-Knowledge VQA. (arXiv:2305.06407v1 [cs.CV])\nAbstract: Outside-knowledge visual question answering is a challenging task that requires both the acquisition and the use of open-ended real-world knowledge. Some existing solutions draw external knowledge into the cross-modality space which overlooks the much vaster textual knowledge in natural-language space, while others transform the image into a text that further fuses with the textual knowledge into the natural-language space and completely abandons the use of visual features. In this paper, we are inspired to constrain the cross-modality space into the same space of natural-language space which makes the visual features preserved directly, and the model still benefits from the vast knowledge in natural-language space. To this end, we propose a novel framework consisting of a multimodal encoder, a textual encoder and an answer decoder. Such structure allows us to introduce more types of knowledge including explicit and implicit multimodal and textual knowledge. Extensive experiments valid",
    "path": "papers/23/05/2305.06407.json",
    "total_tokens": 890,
    "translated_title": "结合思考和观察的外部知识视觉问答",
    "translated_abstract": "外部知识的视觉问答是一个具有挑战性的任务，需要获取和使用开放式真实世界的知识。一些现有的解决方案将外部知识引入跨模态空间中，忽视了自然语言空间中更广泛的文本知识，而另一些解决方案将图像转化为文本，进一步融合自然语言空间中的文本知识，并完全放弃了对视觉特征的使用。在本文中，我们受到的启发是把跨模态空间限制在自然语言空间的同一空间中，使得视觉特征直接地保留下来，并且模型仍然能够从自然语言空间中丰富的知识中获益。为此，我们提出了一个新的框架，包括多模态编码器、文本编码器和答案解码器。这样的结构允许我们引入更多类型的知识，包括显式和隐式多模态和文本知识。",
    "tldr": "本文提出了一种新的视觉问答框架，将跨模态空间限制在自然语言空间的同一空间中，既保留了视觉特征，也能够从丰富的自然语言知识中获益。",
    "en_tdlr": "This paper proposes a new visual question answering framework that constrains the cross-modality space into the same space of natural-language space, preserving visual features and benefiting from the vast knowledge in natural-language space."
}