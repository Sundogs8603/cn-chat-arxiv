{
    "title": "Select without Fear: Almost All Mini-Batch Schedules Generalize Optimally. (arXiv:2305.02247v1 [cs.LG])",
    "abstract": "We establish matching upper and lower generalization error bounds for mini-batch Gradient Descent (GD) training with either deterministic or stochastic, data-independent, but otherwise arbitrary batch selection rules. We consider smooth Lipschitz-convex/nonconvex/strongly-convex loss functions, and show that classical upper bounds for Stochastic GD (SGD) also hold verbatim for such arbitrary nonadaptive batch schedules, including all deterministic ones. Further, for convex and strongly-convex losses we prove matching lower bounds directly on the generalization error uniform over the aforementioned class of batch schedules, showing that all such batch schedules generalize optimally. Lastly, for smooth (non-Lipschitz) nonconvex losses, we show that full-batch (deterministic) GD is essentially optimal, among all possible batch schedules within the considered class, including all stochastic ones.",
    "link": "http://arxiv.org/abs/2305.02247",
    "context": "Title: Select without Fear: Almost All Mini-Batch Schedules Generalize Optimally. (arXiv:2305.02247v1 [cs.LG])\nAbstract: We establish matching upper and lower generalization error bounds for mini-batch Gradient Descent (GD) training with either deterministic or stochastic, data-independent, but otherwise arbitrary batch selection rules. We consider smooth Lipschitz-convex/nonconvex/strongly-convex loss functions, and show that classical upper bounds for Stochastic GD (SGD) also hold verbatim for such arbitrary nonadaptive batch schedules, including all deterministic ones. Further, for convex and strongly-convex losses we prove matching lower bounds directly on the generalization error uniform over the aforementioned class of batch schedules, showing that all such batch schedules generalize optimally. Lastly, for smooth (non-Lipschitz) nonconvex losses, we show that full-batch (deterministic) GD is essentially optimal, among all possible batch schedules within the considered class, including all stochastic ones.",
    "path": "papers/23/05/2305.02247.json",
    "total_tokens": 919,
    "translated_title": "毫不畏惧地选择：几乎所有的小批量训练方案都能够优化。",
    "translated_abstract": "我们证明了带有确定性或随机性、数据独立的小批量梯度下降训练的匹配上下一般化误差界限，但批量选择规则是任意的。我们考虑光滑的Lipschitz-凸性/非凸性/强凸性损失函数，并证明了随机梯度下降的经典上限界限也适用于这样任意的非自适应批量调度，包括所有确定性的调度方案。进一步地，对于凸和强凸的损失函数，我们直接证明了在上述批量调度类上一致的一般化误差下的匹配下限界限，表明所有这样的批量调度都能达到最优的一般化。最后，对于光滑的（非Lipschitz）非凸性损失函数，我们证明了在所考虑的类别内，包括所有随机批处理方案，全批量（确定性）梯度下降是最优的。",
    "tldr": "本文证明了对于数据独立的批处理方案，几乎所有小批量梯度下降训练都能够优化，其中包括所有的确定性方案和随机方案。此外，所有这样的批量调度都能达到最优的一般化误差下限。"
}