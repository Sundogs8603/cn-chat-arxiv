{
    "title": "In-Context Analogical Reasoning with Pre-Trained Language Models. (arXiv:2305.17626v2 [cs.AI] UPDATED)",
    "abstract": "Analogical reasoning is a fundamental capacity of human cognition that allows us to reason abstractly about novel situations by relating them to past experiences. While it is thought to be essential for robust reasoning in AI systems, conventional approaches require significant training and/or hard-coding of domain knowledge to be applied to benchmark tasks. Inspired by cognitive science research that has found connections between human language and analogy-making, we explore the use of intuitive language-based abstractions to support analogy in AI systems. Specifically, we apply large pre-trained language models (PLMs) to visual Raven's Progressive Matrices (RPM), a common relational reasoning test. By simply encoding the perceptual features of the problem into language form, we find that PLMs exhibit a striking capacity for zero-shot relational reasoning, exceeding human performance and nearing supervised vision-based methods. We explore different encodings that vary the level of abs",
    "link": "http://arxiv.org/abs/2305.17626",
    "context": "Title: In-Context Analogical Reasoning with Pre-Trained Language Models. (arXiv:2305.17626v2 [cs.AI] UPDATED)\nAbstract: Analogical reasoning is a fundamental capacity of human cognition that allows us to reason abstractly about novel situations by relating them to past experiences. While it is thought to be essential for robust reasoning in AI systems, conventional approaches require significant training and/or hard-coding of domain knowledge to be applied to benchmark tasks. Inspired by cognitive science research that has found connections between human language and analogy-making, we explore the use of intuitive language-based abstractions to support analogy in AI systems. Specifically, we apply large pre-trained language models (PLMs) to visual Raven's Progressive Matrices (RPM), a common relational reasoning test. By simply encoding the perceptual features of the problem into language form, we find that PLMs exhibit a striking capacity for zero-shot relational reasoning, exceeding human performance and nearing supervised vision-based methods. We explore different encodings that vary the level of abs",
    "path": "papers/23/05/2305.17626.json",
    "total_tokens": 889,
    "translated_title": "基于预先训练语言模型的情境类比推理研究",
    "translated_abstract": "类比推理是人类认知的基本能力之一，可以通过将新的情况与过去的经验关联来进行抽象推理。虽然它被认为对于AI系统的强大推理至关重要，但传统方法需要进行大量的训练和/或固化特定的领域知识才能应用于基准任务中。受到认知科学研究发现人类语言与类比制作之间的联系的启发，我们探索使用直观的基于语言的抽象来支持人工智能系统中的类比。具体而言，我们使用大型预先训练的语言模型（PLMs）对视觉Raven的渐进矩阵（RPM）进行类比推理。通过将问题的感知特征简单地编码成语言形式，我们发现PLMs表现出了惊人的零-shot关系推理能力，超过了人类表现并接近于受监督的基于视觉的方法。我们探索了不同的编码方法，以变化抽象的水平。",
    "tldr": "本研究提出了一种基于语言模型的情境类比推理方法，通过将问题的感知特征编码成语言形式，能够实现高效的零-shot关系推理，超越传统方法和人类水平。",
    "en_tdlr": "This study proposes a language-based approach for in-context analogical reasoning, which encodes the perceptual features of the problem into language form and achieves efficient zero-shot relational reasoning, exceeding traditional methods and human performance."
}