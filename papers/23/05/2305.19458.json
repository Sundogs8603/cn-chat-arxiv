{
    "title": "A Unified Audio-Visual Learning Framework for Localization, Separation, and Recognition. (arXiv:2305.19458v1 [cs.SD])",
    "abstract": "The ability to accurately recognize, localize and separate sound sources is fundamental to any audio-visual perception task. Historically, these abilities were tackled separately, with several methods developed independently for each task. However, given the interconnected nature of source localization, separation, and recognition, independent models are likely to yield suboptimal performance as they fail to capture the interdependence between these tasks. To address this problem, we propose a unified audio-visual learning framework (dubbed OneAVM) that integrates audio and visual cues for joint localization, separation, and recognition. OneAVM comprises a shared audio-visual encoder and task-specific decoders trained with three objectives. The first objective aligns audio and visual representations through a localized audio-visual correspondence loss. The second tackles visual source separation using a traditional mix-and-separate framework. Finally, the third objective reinforces vis",
    "link": "http://arxiv.org/abs/2305.19458",
    "context": "Title: A Unified Audio-Visual Learning Framework for Localization, Separation, and Recognition. (arXiv:2305.19458v1 [cs.SD])\nAbstract: The ability to accurately recognize, localize and separate sound sources is fundamental to any audio-visual perception task. Historically, these abilities were tackled separately, with several methods developed independently for each task. However, given the interconnected nature of source localization, separation, and recognition, independent models are likely to yield suboptimal performance as they fail to capture the interdependence between these tasks. To address this problem, we propose a unified audio-visual learning framework (dubbed OneAVM) that integrates audio and visual cues for joint localization, separation, and recognition. OneAVM comprises a shared audio-visual encoder and task-specific decoders trained with three objectives. The first objective aligns audio and visual representations through a localized audio-visual correspondence loss. The second tackles visual source separation using a traditional mix-and-separate framework. Finally, the third objective reinforces vis",
    "path": "papers/23/05/2305.19458.json",
    "total_tokens": 985,
    "translated_title": "一种统一的音视频学习框架，用于定位、分离和识别",
    "translated_abstract": "准确识别、定位和分离声源对于任何音视频感知任务来说都至关重要。历史上，这些能力分别被解决，为每个任务单独开发了多种方法。然而，由于源定位、分离和识别具有相互关联的性质，独立模型可能会产生次优性能，因为它们未能捕捉这些任务之间的相互依赖关系。为解决这个问题，我们提出了一个统一的音视频学习框架(称为OneAVM)，它集成了音频和视觉线索，用于联合定位、分离和识别。OneAVM包括一个共享的音视频编码器和针对不同任务的解码器，使用三个目标进行训练。第一个目标通过局部音视频对应损失对齐音频和视觉表示。第二个目标利用传统的混合分离框架来解决视觉源分离问题。最后，第三个目标强化视觉识别能力，使其能够协同进行定位、分离和识别。",
    "tldr": "该论文提出了一种统一的音视频学习框架，用于联合定位、分离和识别，包括共享的音视频编码器和针对不同任务的解码器，通过局部音视频对应损失、混合分离框架和强化视觉识别能力来优化不同任务之间的相互依赖性能。",
    "en_tdlr": "This paper proposes a unified audio-visual learning framework (OneAVM) for joint localization, separation, and recognition, which includes a shared audio-visual encoder and task-specific decoders trained with three objectives: localized audio-visual correspondence loss, traditional mix-and-separate framework, and reinforced visual recognition, in order to optimize the interdependent performance of different tasks."
}