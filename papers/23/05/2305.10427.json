{
    "title": "Accelerating Transformer Inference for Translation via Parallel Decoding. (arXiv:2305.10427v1 [cs.CL])",
    "abstract": "Autoregressive decoding limits the efficiency of transformers for Machine Translation (MT). The community proposed specific network architectures and learning-based methods to solve this issue, which are expensive and require changes to the MT model, trading inference speed at the cost of the translation quality. In this paper, we propose to address the problem from the point of view of decoding algorithms, as a less explored but rather compelling direction. We propose to reframe the standard greedy autoregressive decoding of MT with a parallel formulation leveraging Jacobi and Gauss-Seidel fixed-point iteration methods for fast inference. This formulation allows to speed up existing models without training or modifications while retaining translation quality. We present three parallel decoding algorithms and test them on different languages and models showing how the parallelization introduces a speedup up to 38% w.r.t. the standard autoregressive decoding and nearly 2x when scaling t",
    "link": "http://arxiv.org/abs/2305.10427",
    "context": "Title: Accelerating Transformer Inference for Translation via Parallel Decoding. (arXiv:2305.10427v1 [cs.CL])\nAbstract: Autoregressive decoding limits the efficiency of transformers for Machine Translation (MT). The community proposed specific network architectures and learning-based methods to solve this issue, which are expensive and require changes to the MT model, trading inference speed at the cost of the translation quality. In this paper, we propose to address the problem from the point of view of decoding algorithms, as a less explored but rather compelling direction. We propose to reframe the standard greedy autoregressive decoding of MT with a parallel formulation leveraging Jacobi and Gauss-Seidel fixed-point iteration methods for fast inference. This formulation allows to speed up existing models without training or modifications while retaining translation quality. We present three parallel decoding algorithms and test them on different languages and models showing how the parallelization introduces a speedup up to 38% w.r.t. the standard autoregressive decoding and nearly 2x when scaling t",
    "path": "papers/23/05/2305.10427.json",
    "total_tokens": 831,
    "translated_title": "并行解码加速Transformer在翻译中的应用",
    "translated_abstract": "自回归解码限制了Transformer在机器翻译中的效率。社区提出了特定的网络架构和基于学习的方法来解决这个问题，但它们都很昂贵并且需要改变机器翻译模型，以推导出解码速度和翻译质量之间的平衡。本文从解码算法的角度提出了一个解决方案，将标准的贪心自回归解码转化为并行解码，并利用雅克比和高斯-塞德尔迭代方法实现快速推断。该算法不需要修改现有模型，并在保持翻译质量的同时加速了现有模型。我们提出了三种并行解码算法，并在不同语言和模型上进行了测试，证明并行化解码相对于标准自回归解码可提高达38％的速度，当扩展模型时，速度几乎提高了2倍。",
    "tldr": "通过并行解码，本文提出了一种快速推断Transformer在翻译中的应用的方法，不需要修改现有模型并在保持翻译质量的同时加速了现有模型。",
    "en_tdlr": "This paper proposes a method for fast inference of Transformers in translation through parallel decoding, which does not require modifying existing models and speeds up existing models while maintaining translation quality."
}