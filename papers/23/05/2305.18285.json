{
    "title": "Partially Personalized Federated Learning: Breaking the Curse of Data Heterogeneity. (arXiv:2305.18285v1 [cs.LG])",
    "abstract": "We present a partially personalized formulation of Federated Learning (FL) that strikes a balance between the flexibility of personalization and cooperativeness of global training. In our framework, we split the variables into global parameters, which are shared across all clients, and individual local parameters, which are kept private. We prove that under the right split of parameters, it is possible to find global parameters that allow each client to fit their data perfectly, and refer to the obtained problem as overpersonalized. For instance, the shared global parameters can be used to learn good data representations, whereas the personalized layers are fine-tuned for a specific client. Moreover, we present a simple algorithm for the partially personalized formulation that offers significant benefits to all clients. In particular, it breaks the curse of data heterogeneity in several settings, such as training with local steps, asynchronous training, and Byzantine-robust training.",
    "link": "http://arxiv.org/abs/2305.18285",
    "context": "Title: Partially Personalized Federated Learning: Breaking the Curse of Data Heterogeneity. (arXiv:2305.18285v1 [cs.LG])\nAbstract: We present a partially personalized formulation of Federated Learning (FL) that strikes a balance between the flexibility of personalization and cooperativeness of global training. In our framework, we split the variables into global parameters, which are shared across all clients, and individual local parameters, which are kept private. We prove that under the right split of parameters, it is possible to find global parameters that allow each client to fit their data perfectly, and refer to the obtained problem as overpersonalized. For instance, the shared global parameters can be used to learn good data representations, whereas the personalized layers are fine-tuned for a specific client. Moreover, we present a simple algorithm for the partially personalized formulation that offers significant benefits to all clients. In particular, it breaks the curse of data heterogeneity in several settings, such as training with local steps, asynchronous training, and Byzantine-robust training.",
    "path": "papers/23/05/2305.18285.json",
    "total_tokens": 949,
    "translated_title": "部分个性化联邦学习：打破数据异构之咒",
    "translated_abstract": "我们提出了部分个性化联邦学习（FL）的模型，旨在平衡个性化与全局训练的合作性之间的关系。在我们的框架中，我们将变量分为全局参数和个体本地参数。证明了在正确的参数拆分下，可以找到允许每个客户端完美拟合其数据的全局参数，并将所得到的问题称为过度个性化问题。共享的全局参数可以用于学习优秀的数据表示，而个性化层则为特定客户端进行微调。此外，我们提出了一种简单的算法来解决部分个性化学习问题，为所有客户端带来了显著的益处。特别地，在许多情况下，如使用本地步骤，异步训练和拜占庭-鲁棒训练中，这种算法打破了数据异构的咒语。",
    "tldr": "本文提出了部分个性化联邦学习模型，将变量分为全局参数和个体本地参数，解决了数据异构问题，为每个客户端提供完美数据拟合的全局参数。此方法的共享的全局参数可用于学习优秀的数据表示，而个性化层则可用于特定客户端的微调。",
    "en_tdlr": "This paper proposes a partially personalized Federated Learning model, which splits variables into global and individual local parameters and allows each client to fit their data perfectly with the obtained overpersonalized problem. The shared global parameters can be used for learning good data representations, and personalized layers can be fine-tuned for specific clients. This method breaks the curse of data heterogeneity and offers significant benefits for all clients in various settings such as training with local steps, asynchronous training, and Byzantine-robust training."
}