{
    "title": "Slow Kill for Big Data Learning. (arXiv:2305.01726v1 [stat.ML])",
    "abstract": "Big-data applications often involve a vast number of observations and features, creating new challenges for variable selection and parameter estimation. This paper presents a novel technique called ``slow kill,'' which utilizes nonconvex constrained optimization, adaptive $\\ell_2$-shrinkage, and increasing learning rates. The fact that the problem size can decrease during the slow kill iterations makes it particularly effective for large-scale variable screening. The interaction between statistics and optimization provides valuable insights into controlling quantiles, stepsize, and shrinkage parameters in order to relax the regularity conditions required to achieve the desired level of statistical accuracy. Experimental results on real and synthetic data show that slow kill outperforms state-of-the-art algorithms in various situations while being computationally efficient for large-scale data.",
    "link": "http://arxiv.org/abs/2305.01726",
    "context": "Title: Slow Kill for Big Data Learning. (arXiv:2305.01726v1 [stat.ML])\nAbstract: Big-data applications often involve a vast number of observations and features, creating new challenges for variable selection and parameter estimation. This paper presents a novel technique called ``slow kill,'' which utilizes nonconvex constrained optimization, adaptive $\\ell_2$-shrinkage, and increasing learning rates. The fact that the problem size can decrease during the slow kill iterations makes it particularly effective for large-scale variable screening. The interaction between statistics and optimization provides valuable insights into controlling quantiles, stepsize, and shrinkage parameters in order to relax the regularity conditions required to achieve the desired level of statistical accuracy. Experimental results on real and synthetic data show that slow kill outperforms state-of-the-art algorithms in various situations while being computationally efficient for large-scale data.",
    "path": "papers/23/05/2305.01726.json",
    "total_tokens": 807,
    "translated_title": "大数据学习的慢杀技巧",
    "translated_abstract": "大数据应用通常涉及大量的观察和特征，这为变量选择和参数估计带来了新的挑战。本文介绍了一种称为“慢杀”的新技术，它利用非凸约束优化、自适应$\\ell_2$收缩和逐步增加的学习率。在慢杀迭代过程中，问题规模可以减小，这使其特别适用于大规模变量筛选。统计和优化之间的相互作用提供了有关控制分位数、步长和收缩参数以放松所需的正则性条件以实现所需统计精度的有价值的见解。实验结果表明慢杀在各种情况下优于现有算法，并且在大规模数据上具有高效的计算能力。",
    "tldr": "本文提出了一种称为“慢杀”的技术，它利用非凸约束优化、自适应$\\ell_2$收缩和逐步增加的学习率，可以在大规模数据上实现高效变量筛选和统计精度。",
    "en_tdlr": "This paper proposes a technique called \"slow kill,\" which utilizes nonconvex constrained optimization, adaptive $\\ell_2$-shrinkage, and increasing learning rates for efficient variable selection and parameter estimation in big-data applications. Its effectiveness is demonstrated through experimental results on real and synthetic data."
}