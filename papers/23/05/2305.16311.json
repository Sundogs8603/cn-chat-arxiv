{
    "title": "Break-A-Scene: Extracting Multiple Concepts from a Single Image. (arXiv:2305.16311v2 [cs.CV] UPDATED)",
    "abstract": "Text-to-image model personalization aims to introduce a user-provided concept to the model, allowing its synthesis in diverse contexts. However, current methods primarily focus on the case of learning a single concept from multiple images with variations in backgrounds and poses, and struggle when adapted to a different scenario. In this work, we introduce the task of textual scene decomposition: given a single image of a scene that may contain several concepts, we aim to extract a distinct text token for each concept, enabling fine-grained control over the generated scenes. To this end, we propose augmenting the input image with masks that indicate the presence of target concepts. These masks can be provided by the user or generated automatically by a pre-trained segmentation model. We then present a novel two-phase customization process that optimizes a set of dedicated textual embeddings (handles), as well as the model weights, striking a delicate balance between accurately capturin",
    "link": "http://arxiv.org/abs/2305.16311",
    "context": "Title: Break-A-Scene: Extracting Multiple Concepts from a Single Image. (arXiv:2305.16311v2 [cs.CV] UPDATED)\nAbstract: Text-to-image model personalization aims to introduce a user-provided concept to the model, allowing its synthesis in diverse contexts. However, current methods primarily focus on the case of learning a single concept from multiple images with variations in backgrounds and poses, and struggle when adapted to a different scenario. In this work, we introduce the task of textual scene decomposition: given a single image of a scene that may contain several concepts, we aim to extract a distinct text token for each concept, enabling fine-grained control over the generated scenes. To this end, we propose augmenting the input image with masks that indicate the presence of target concepts. These masks can be provided by the user or generated automatically by a pre-trained segmentation model. We then present a novel two-phase customization process that optimizes a set of dedicated textual embeddings (handles), as well as the model weights, striking a delicate balance between accurately capturin",
    "path": "papers/23/05/2305.16311.json",
    "total_tokens": 908,
    "translated_title": "从单个图像中提取多个概念的场景分解：破解现场",
    "translated_abstract": "文本到图像模型个性化的目标是引入用户提供的概念到模型中，以便在不同的情境中合成。然而，当前的方法主要集中在从具有不同背景和姿势变化的多个图像中学习单个概念的情况，当应用到不同的场景时会遇到困难。在这项工作中，我们引入了文本场景分解的任务：给定一个可能包含多个概念的场景的单个图像，我们旨在提取每个概念的独特文本标记，从而对生成的场景进行精细控制。为此，我们提出了一种增强输入图像的方法，用来指示目标概念的存在的掩码。这些掩码可以由用户提供，也可以由预训练的分割模型自动生成。然后，我们提出了一种新颖的两阶段定制流程，优化一组专用的文本嵌入（句柄）以及模型权重，以在准确捕捉概念的同时保持平衡。",
    "tldr": "该论文提出了一种从单个图像中提取多个概念的文本场景分解方法，通过增加目标概念的掩码和优化文本嵌入和模型权重的方式，实现对生成场景的精细控制。"
}