{
    "title": "Generative Table Pre-training Empowers Models for Tabular Prediction. (arXiv:2305.09696v1 [cs.LG])",
    "abstract": "Recently, the topic of table pre-training has attracted considerable research interest. However, how to employ table pre-training to boost the performance of tabular prediction remains an open challenge. In this paper, we propose TapTap, the first attempt that leverages table pre-training to empower models for tabular prediction. After pre-training on a large corpus of real-world tabular data, TapTap can generate high-quality synthetic tables to support various applications on tabular data, including privacy protection, low resource regime, missing value imputation, and imbalanced classification. Extensive experiments on 12 datasets demonstrate that TapTap outperforms a total of 16 baselines in different scenarios. Meanwhile, it can be easily combined with various backbone models, including LightGBM, Multilayer Perceptron (MLP) and Transformer. Moreover, with the aid of table pre-training, models trained using synthetic data generated by TapTap can even compete with models using the or",
    "link": "http://arxiv.org/abs/2305.09696",
    "context": "Title: Generative Table Pre-training Empowers Models for Tabular Prediction. (arXiv:2305.09696v1 [cs.LG])\nAbstract: Recently, the topic of table pre-training has attracted considerable research interest. However, how to employ table pre-training to boost the performance of tabular prediction remains an open challenge. In this paper, we propose TapTap, the first attempt that leverages table pre-training to empower models for tabular prediction. After pre-training on a large corpus of real-world tabular data, TapTap can generate high-quality synthetic tables to support various applications on tabular data, including privacy protection, low resource regime, missing value imputation, and imbalanced classification. Extensive experiments on 12 datasets demonstrate that TapTap outperforms a total of 16 baselines in different scenarios. Meanwhile, it can be easily combined with various backbone models, including LightGBM, Multilayer Perceptron (MLP) and Transformer. Moreover, with the aid of table pre-training, models trained using synthetic data generated by TapTap can even compete with models using the or",
    "path": "papers/23/05/2305.09696.json",
    "total_tokens": 955,
    "translated_title": "生成式表格预训练增强了表格预测模型",
    "translated_abstract": "近年来，表格预训练已经成为研究的热点，但如何利用表格预训练来提高表格预测的性能仍然是一个开放性挑战。本文提出了TapTap，这是第一个利用表格预训练来增强表格预测模型的尝试。在对大量实际世界的表格数据进行预训练后，TapTap能够生成高质量的合成表格，以支持各种表格数据应用，包括隐私保护、低资源环境、缺失值插补和失衡分类。在12个数据集上的广泛实验表明，TapTap在不同场景下优于16个基线。同时，它可以轻松地与各种骨干模型结合使用，包括LightGBM、多层感知机（MLP）和Transformer。此外，在表格预训练的帮助下，使用TapTap生成的合成数据进行训练的模型甚至可以与使用原始真实数据的模型竞争，实现了相当甚至更好的性能。",
    "tldr": "本文提出了TapTap，一种通过表格预训练生成高质量合成表格来提高表格预测性能的方法。在12个数据集实验中，TapTap在不同场景下优于16个基线，并可以与多个骨干模型结合使用。",
    "en_tdlr": "This paper proposes TapTap, a method that leverages table pre-training to generate high-quality synthetic tables to boost the performance of tabular prediction. Extensive experiments on 12 datasets demonstrate that TapTap outperforms a total of 16 baselines in different scenarios, and can be easily combined with various backbone models."
}