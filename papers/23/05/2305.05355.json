{
    "title": "Turning Privacy-preserving Mechanisms against Federated Learning. (arXiv:2305.05355v1 [cs.LG])",
    "abstract": "Recently, researchers have successfully employed Graph Neural Networks (GNNs) to build enhanced recommender systems due to their capability to learn patterns from the interaction between involved entities. In addition, previous studies have investigated federated learning as the main solution to enable a native privacy-preserving mechanism for the construction of global GNN models without collecting sensitive data into a single computation unit. Still, privacy issues may arise as the analysis of local model updates produced by the federated clients can return information related to sensitive local data. For this reason, experts proposed solutions that combine federated learning with Differential Privacy strategies and community-driven approaches, which involve combining data from neighbor clients to make the individual local updates less dependent on local sensitive data. In this paper, we identify a crucial security flaw in such a configuration, and we design an attack capable of dece",
    "link": "http://arxiv.org/abs/2305.05355",
    "context": "Title: Turning Privacy-preserving Mechanisms against Federated Learning. (arXiv:2305.05355v1 [cs.LG])\nAbstract: Recently, researchers have successfully employed Graph Neural Networks (GNNs) to build enhanced recommender systems due to their capability to learn patterns from the interaction between involved entities. In addition, previous studies have investigated federated learning as the main solution to enable a native privacy-preserving mechanism for the construction of global GNN models without collecting sensitive data into a single computation unit. Still, privacy issues may arise as the analysis of local model updates produced by the federated clients can return information related to sensitive local data. For this reason, experts proposed solutions that combine federated learning with Differential Privacy strategies and community-driven approaches, which involve combining data from neighbor clients to make the individual local updates less dependent on local sensitive data. In this paper, we identify a crucial security flaw in such a configuration, and we design an attack capable of dece",
    "path": "papers/23/05/2305.05355.json",
    "total_tokens": 848,
    "translated_title": "将隐私保护机制用于联邦学习的探讨",
    "translated_abstract": "最近，研究人员成功地利用图形神经网络（GNN）构建了增强型推荐系统，因为它们能够从相关实体之间的交互中学习模式。此外，先前的研究已经调查了联合学习作为建立全局GNN模型的本地隐私保护机制的主要解决方案，而无需将敏感数据收集到单个计算单元中。然而，隐私问题可能会出现，因为分布式客户端生成的局部模型更新的分析可能会返回与敏感本地数据相关的信息。出于这个原因，专家们提出了将联合学习与差分隐私策略和社区驱动方法相结合的解决方案，这涉及将来自邻居客户端的数据组合起来，使个体局部更新不那么依赖于局部敏感数据。在本文中，我们确定了这种配置中一个关键的安全漏洞，并设计了一种攻击，能够欺骗。",
    "tldr": "本文探讨了如何将隐私保护机制应用于联邦学习，并指出了该配置中的一个关键安全漏洞，并设计了对此漏洞的攻击方式。",
    "en_tdlr": "This paper discusses how to apply privacy-preserving mechanisms to federated learning, identifies a critical security flaw in such a configuration, and designs an attack against it."
}