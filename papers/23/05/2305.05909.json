{
    "title": "Robust multi-agent coordination via evolutionary generation of auxiliary adversarial attackers. (arXiv:2305.05909v1 [cs.MA])",
    "abstract": "Cooperative multi-agent reinforcement learning (CMARL) has shown to be promising for many real-world applications. Previous works mainly focus on improving coordination ability via solving MARL-specific challenges (e.g., non-stationarity, credit assignment, scalability), but ignore the policy perturbation issue when testing in a different environment. This issue hasn't been considered in problem formulation or efficient algorithm design. To address this issue, we firstly model the problem as a limited policy adversary Dec-POMDP (LPA-Dec-POMDP), where some coordinators from a team might accidentally and unpredictably encounter a limited number of malicious action attacks, but the regular coordinators still strive for the intended goal. Then, we propose Robust Multi-Agent Coordination via Evolutionary Generation of Auxiliary Adversarial Attackers (ROMANCE), which enables the trained policy to encounter diversified and strong auxiliary adversarial attacks during training, thus achieving h",
    "link": "http://arxiv.org/abs/2305.05909",
    "context": "Title: Robust multi-agent coordination via evolutionary generation of auxiliary adversarial attackers. (arXiv:2305.05909v1 [cs.MA])\nAbstract: Cooperative multi-agent reinforcement learning (CMARL) has shown to be promising for many real-world applications. Previous works mainly focus on improving coordination ability via solving MARL-specific challenges (e.g., non-stationarity, credit assignment, scalability), but ignore the policy perturbation issue when testing in a different environment. This issue hasn't been considered in problem formulation or efficient algorithm design. To address this issue, we firstly model the problem as a limited policy adversary Dec-POMDP (LPA-Dec-POMDP), where some coordinators from a team might accidentally and unpredictably encounter a limited number of malicious action attacks, but the regular coordinators still strive for the intended goal. Then, we propose Robust Multi-Agent Coordination via Evolutionary Generation of Auxiliary Adversarial Attackers (ROMANCE), which enables the trained policy to encounter diversified and strong auxiliary adversarial attacks during training, thus achieving h",
    "path": "papers/23/05/2305.05909.json",
    "total_tokens": 1038,
    "translated_title": "基于演化辅助敌对攻击的鲁棒多智能体协调",
    "translated_abstract": "合作多智能体强化学习(CMARL)在许多实际应用中表现出良好的前景。以往的工作主要集中在通过解决针对MARL的挑战（如非稳态、信用分配、可扩展性）来提高协调能力，但在不同的环境中测试时忽略了策略扰动问题。这个问题在问题定义或算法设计方面还没有得到考虑。为了解决这个问题，我们首先将问题建模为一个有限策略对抗Dec-POMDP问题(LPA-Dec-POMDP)，其中一些来自团队的协调员可能会意外而不可预测地遭遇到有限数量的恶意行为攻击，但常规的协调员仍然会为既定目标而努力。然后，我们提出了通过演化生成辅助敌对攻击来实现鲁棒多智能体协调(ROMANCE)的方法，这样训练的策略可以在训练过程中遭遇多样化和强大的辅助敌对攻击，从而在测试场景中实现高鲁棒性。我们在两个具有挑战性的基准测试中的实验表明，ROMANCE优于最先进的CMARL方法。",
    "tldr": "该论文提出了一种基于演化生成辅助敌对攻击来提高训练策略鲁棒性的方法，用于解决合作多智能体强化学习中在不同环境中测试时策略扰动的问题，并在实验中表现优于最先进的CMARL方法。",
    "en_tdlr": "The paper proposes a method called ROMANCE that generates auxiliary adversarial attacks to train policies, thus improving robustness in testing scenarios for cooperative multi-agent reinforcement learning. It outperforms state-of-the-art CMARL methods in experiments on challenging benchmarks."
}