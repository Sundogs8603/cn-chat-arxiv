{
    "title": "Test like you Train in Implicit Deep Learning. (arXiv:2305.15042v1 [cs.LG])",
    "abstract": "Implicit deep learning has recently gained popularity with applications ranging from meta-learning to Deep Equilibrium Networks (DEQs). In its general formulation, it relies on expressing some components of deep learning pipelines implicitly, typically via a root equation called the inner problem. In practice, the solution of the inner problem is approximated during training with an iterative procedure, usually with a fixed number of inner iterations. During inference, the inner problem needs to be solved with new data. A popular belief is that increasing the number of inner iterations compared to the one used during training yields better performance. In this paper, we question such an assumption and provide a detailed theoretical analysis in a simple setting. We demonstrate that overparametrization plays a key role: increasing the number of iterations at test time cannot improve performance for overparametrized networks. We validate our theory on an array of implicit deep-learning pr",
    "link": "http://arxiv.org/abs/2305.15042",
    "context": "Title: Test like you Train in Implicit Deep Learning. (arXiv:2305.15042v1 [cs.LG])\nAbstract: Implicit deep learning has recently gained popularity with applications ranging from meta-learning to Deep Equilibrium Networks (DEQs). In its general formulation, it relies on expressing some components of deep learning pipelines implicitly, typically via a root equation called the inner problem. In practice, the solution of the inner problem is approximated during training with an iterative procedure, usually with a fixed number of inner iterations. During inference, the inner problem needs to be solved with new data. A popular belief is that increasing the number of inner iterations compared to the one used during training yields better performance. In this paper, we question such an assumption and provide a detailed theoretical analysis in a simple setting. We demonstrate that overparametrization plays a key role: increasing the number of iterations at test time cannot improve performance for overparametrized networks. We validate our theory on an array of implicit deep-learning pr",
    "path": "papers/23/05/2305.15042.json",
    "total_tokens": 864,
    "translated_title": "隐式深度学习中的像训练一样测试",
    "translated_abstract": "隐式深度学习最近在元学习和Deep Equilibrium Networks（DEQs）等应用中变得流行。在其一般形式中，它依赖于通过一个称为内部问题的根方程隐含地表达深度学习流程的一些组件。在实践中，内部问题的解决方案通常通过迭代过程在训练期间进行近似计算，通常使用固定数量的内部迭代。在推断期间，内部问题需要使用新数据进行求解。一种普遍的信念是，与训练期间使用的内部迭代次数相比，增加测试时间的内部迭代次数可以提高性能。在本文中，我们质疑这种假设，并在简单的设置中提供了详细的理论分析。我们证明了过度参数化起着关键作用：对于过度参数化的网络，增加测试时间的迭代次数不能改善性能。我们在一系列隐式深度学习问题上验证了我们的理论。",
    "tldr": "本文研究了隐式深度学习中测试时间内部迭代次数的影响，提出过度参数化时增加迭代次数无法提高性能的理论，并在实验中进行验证。"
}