{
    "title": "FedMR: Federated Learning via Model Recombination. (arXiv:2305.10730v1 [cs.LG])",
    "abstract": "Although Federated Learning (FL) enables global model training across clients without compromising their raw data, existing Federated Averaging (FedAvg)-based methods suffer from the problem of low inference performance, especially for unevenly distributed data among clients. This is mainly because i) FedAvg initializes client models with the same global models, which makes the local training hard to escape from the local search for optimal solutions; and ii) by averaging model parameters in a coarse manner, FedAvg eclipses the individual characteristics of local models. To address such issues that strongly limit the inference capability of FL, we propose a novel and effective FL paradigm named FedMR (Federated Model Recombination). Unlike conventional FedAvg-based methods, the cloud server of FedMR shuffles each layer of collected local models and recombines them to achieve new models for local training on clients. Due to the diversified initialization models for clients coupled with ",
    "link": "http://arxiv.org/abs/2305.10730",
    "context": "Title: FedMR: Federated Learning via Model Recombination. (arXiv:2305.10730v1 [cs.LG])\nAbstract: Although Federated Learning (FL) enables global model training across clients without compromising their raw data, existing Federated Averaging (FedAvg)-based methods suffer from the problem of low inference performance, especially for unevenly distributed data among clients. This is mainly because i) FedAvg initializes client models with the same global models, which makes the local training hard to escape from the local search for optimal solutions; and ii) by averaging model parameters in a coarse manner, FedAvg eclipses the individual characteristics of local models. To address such issues that strongly limit the inference capability of FL, we propose a novel and effective FL paradigm named FedMR (Federated Model Recombination). Unlike conventional FedAvg-based methods, the cloud server of FedMR shuffles each layer of collected local models and recombines them to achieve new models for local training on clients. Due to the diversified initialization models for clients coupled with ",
    "path": "papers/23/05/2305.10730.json",
    "total_tokens": 826,
    "translated_title": "FedMR：基于模型重组的联邦学习",
    "translated_abstract": "联邦学习（FL）使得客户端在不泄露原始数据的情况下进行全局模型训练，但现有的基于联邦平均（FedAvg）的方法在推理性能方面存在问题，特别是在客户端数据分布不均匀时。本文提出了一种名为FedMR（联邦模型重组）的新型联邦学习范式，通过对收集到的本地模型的每个层进行混洗和重组，在客户端进行局部训练时可以获得具有多样的初始模型的新模型，从而缓解了子优或有偏局部模型的问题。",
    "tldr": "FedMR是一种基于模型重组的联邦学习范式，可以通过混洗和重组每个层的本地模型来缓解子优或有偏局部模型的问题，从而在推理性能方面表现出色。",
    "en_tdlr": "FedMR is a novel Federated Learning paradigm that uses model recombination to mitigate the issues of sub-optimal or biased local models, achieving significantly higher inference performance with less communication overhead than existing FedAvg-based methods, as demonstrated by benchmark experiments."
}