{
    "title": "More for Less: Safe Policy Improvement With Stronger Performance Guarantees. (arXiv:2305.07958v1 [cs.LG])",
    "abstract": "In an offline reinforcement learning setting, the safe policy improvement (SPI) problem aims to improve the performance of a behavior policy according to which sample data has been generated. State-of-the-art approaches to SPI require a high number of samples to provide practical probabilistic guarantees on the improved policy's performance. We present a novel approach to the SPI problem that provides the means to require less data for such guarantees. Specifically, to prove the correctness of these guarantees, we devise implicit transformations on the data set and the underlying environment model that serve as theoretical foundations to derive tighter improvement bounds for SPI. Our empirical evaluation, using the well-established SPI with baseline bootstrapping (SPIBB) algorithm, on standard benchmarks shows that our method indeed significantly reduces the sample complexity of the SPIBB algorithm.",
    "link": "http://arxiv.org/abs/2305.07958",
    "context": "Title: More for Less: Safe Policy Improvement With Stronger Performance Guarantees. (arXiv:2305.07958v1 [cs.LG])\nAbstract: In an offline reinforcement learning setting, the safe policy improvement (SPI) problem aims to improve the performance of a behavior policy according to which sample data has been generated. State-of-the-art approaches to SPI require a high number of samples to provide practical probabilistic guarantees on the improved policy's performance. We present a novel approach to the SPI problem that provides the means to require less data for such guarantees. Specifically, to prove the correctness of these guarantees, we devise implicit transformations on the data set and the underlying environment model that serve as theoretical foundations to derive tighter improvement bounds for SPI. Our empirical evaluation, using the well-established SPI with baseline bootstrapping (SPIBB) algorithm, on standard benchmarks shows that our method indeed significantly reduces the sample complexity of the SPIBB algorithm.",
    "path": "papers/23/05/2305.07958.json",
    "total_tokens": 779,
    "translated_title": "更少的数据更强的安全策略优化性能保证",
    "translated_abstract": "在离线强化学习环境中，安全策略优化(SPI)问题旨在根据生成样本数据的行为策略，提高其性能。现有的解决SPI问题的方法需要较高数量的样本，以提供对改进策略性能的实际概率保证。我们提出了一种新的方法来解决SPI问题，用较少的数据即可获得这样的保证。具体来说，为了证明这些保证的正确性，我们设计了数据集和基础环境模型上的隐式转换，这些转换作为推导更紧密的SPI改进界限的理论基础。我们使用专业的SPI与基线引导(SPIBB)算法在标准基准测试中进行实证评估，结果表明我们的方法确实显著降低了SPIBB算法的样本复杂度。",
    "tldr": "本论文提出了一种新的方法，可以用较少的数据保证安全策略优化(SPI)的性能，并降低了SPIBB算法的样本复杂度。",
    "en_tdlr": "This paper proposes a novel approach that can ensure the performance of safe policy improvement (SPI) with less data and reduce the sample complexity of the SPIBB algorithm."
}