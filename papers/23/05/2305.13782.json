{
    "title": "Images in Language Space: Exploring the Suitability of Large Language Models for Vision & Language Tasks. (arXiv:2305.13782v1 [cs.CL])",
    "abstract": "Large language models have demonstrated robust performance on various language tasks using zero-shot or few-shot learning paradigms. While being actively researched, multimodal models that can additionally handle images as input have yet to catch up in size and generality with language-only models. In this work, we ask whether language-only models can be utilised for tasks that require visual input -- but also, as we argue, often require a strong reasoning component. Similar to some recent related work, we make visual information accessible to the language model using separate verbalisation models. Specifically, we investigate the performance of open-source, open-access language models against GPT-3 on five vision-language tasks when given textually-encoded visual information. Our results suggest that language models are effective for solving vision-language tasks even with limited samples. This approach also enhances the interpretability of a model's output by providing a means of tra",
    "link": "http://arxiv.org/abs/2305.13782",
    "context": "Title: Images in Language Space: Exploring the Suitability of Large Language Models for Vision & Language Tasks. (arXiv:2305.13782v1 [cs.CL])\nAbstract: Large language models have demonstrated robust performance on various language tasks using zero-shot or few-shot learning paradigms. While being actively researched, multimodal models that can additionally handle images as input have yet to catch up in size and generality with language-only models. In this work, we ask whether language-only models can be utilised for tasks that require visual input -- but also, as we argue, often require a strong reasoning component. Similar to some recent related work, we make visual information accessible to the language model using separate verbalisation models. Specifically, we investigate the performance of open-source, open-access language models against GPT-3 on five vision-language tasks when given textually-encoded visual information. Our results suggest that language models are effective for solving vision-language tasks even with limited samples. This approach also enhances the interpretability of a model's output by providing a means of tra",
    "path": "papers/23/05/2305.13782.json",
    "total_tokens": 940,
    "translated_title": "语言空间中的图像：探索大型语言模型在视觉语言任务中的适用性。",
    "translated_abstract": "大型语言模型已经在各种语言任务中展现了强大的性能，使用了零样本或少样本学习范式。虽然多模型模型正在积极研究中，可以额外处理图像作为输入，但在大小和普适性上还没有达到仅语言模型。在这项工作中，我们询问只使用语言模型是否可以用于需要视觉输入的任务，并且正如我们所争论的，这些任务通常需要强大的推理能力。类似于一些最近的相关工作，我们使用单独的语言模型来使语言模型可以访问视觉信息。具体而言，我们研究了开源、开放访问的语言模型相对于GPT-3在给定以文本编码的视觉信息时在五个视觉语言任务上的表现。我们的结果表明，即使样本有限，语言模型也可以有效地解决视觉语言任务。这种方法还通过提供手段来增强模型输出的可解释性。",
    "tldr": "该论文研究了只使用语言模型是否可以用于需要视觉输入和强大推理能力的任务，并使用单独的语言模型使其可以访问视觉信息的方法。结果表明，即使样本有限，语言模型也可以成功解决视觉语言任务，还提高了模型输出的可解释性。",
    "en_tdlr": "This paper investigates whether language-only models can be used for tasks that require visual input and strong reasoning abilities by making visual information accessible to the language model using separate verbalisation models. The results suggest that language models can effectively solve vision-language tasks even with limited samples and enhance the interpretability of model outputs."
}