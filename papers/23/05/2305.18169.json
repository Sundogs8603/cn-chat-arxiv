{
    "title": "LM-CPPF: Paraphrasing-Guided Data Augmentation for Contrastive Prompt-Based Few-Shot Fine-Tuning. (arXiv:2305.18169v2 [cs.CL] UPDATED)",
    "abstract": "In recent years, there has been significant progress in developing pre-trained language models for NLP. However, these models often struggle when fine-tuned on small datasets. To address this issue, researchers have proposed various adaptation approaches. Prompt-based tuning is arguably the most common way, especially for larger models. Previous research shows that adding contrastive learning to prompt-based fine-tuning is effective as it helps the model generate embeddings that are more distinguishable between classes, and it can also be more sample-efficient as the model learns from positive and negative examples simultaneously. One of the most important components of contrastive learning is data augmentation, but unlike computer vision, effective data augmentation for NLP is still challenging. This paper proposes LM-CPPF, Contrastive Paraphrasing-guided Prompt-based Fine-tuning of Language Models, which leverages prompt-based few-shot paraphrasing using generative language models, e",
    "link": "http://arxiv.org/abs/2305.18169",
    "context": "Title: LM-CPPF: Paraphrasing-Guided Data Augmentation for Contrastive Prompt-Based Few-Shot Fine-Tuning. (arXiv:2305.18169v2 [cs.CL] UPDATED)\nAbstract: In recent years, there has been significant progress in developing pre-trained language models for NLP. However, these models often struggle when fine-tuned on small datasets. To address this issue, researchers have proposed various adaptation approaches. Prompt-based tuning is arguably the most common way, especially for larger models. Previous research shows that adding contrastive learning to prompt-based fine-tuning is effective as it helps the model generate embeddings that are more distinguishable between classes, and it can also be more sample-efficient as the model learns from positive and negative examples simultaneously. One of the most important components of contrastive learning is data augmentation, but unlike computer vision, effective data augmentation for NLP is still challenging. This paper proposes LM-CPPF, Contrastive Paraphrasing-guided Prompt-based Fine-tuning of Language Models, which leverages prompt-based few-shot paraphrasing using generative language models, e",
    "path": "papers/23/05/2305.18169.json",
    "total_tokens": 927,
    "translated_title": "LM-CPPF: 基于释义引导的数据增强用于对比型Prompt的少样本微调",
    "translated_abstract": "近年来，预训练语言模型在自然语言处理领域取得了显著进展，但这些模型在少量数据集上的微调仍然存在缺陷。为了解决这个问题，研究人员提出了各种适应性方法。对基础Prompt进行微调是一种较为普遍的方式，尤其适用于大型模型。之前的研究显示，将对比学习添加到对Prompt的微调中是有效的，因为它帮助模型生成更能够区分不同分类之间的嵌入，而且它同时还能从正负示例中学习，更加节省样本。对比学习最重要的组成部分之一是数据增强，在计算机视觉领域得到广泛应用，但对于NLP来说，有效的数据增强仍然具有挑战性。本文提出了LM-CPPF，通过生成式语言模型引导的少样本释义型对比Prompt微调，它利用基础Prompt的少样本释义生成语言模型来完成数据增强。",
    "tldr": "本文提出了一种基于释义引导的数据增强用于对比型Prompt的少样本微调方法。该方法利用基础Prompt的少样本释义生成语言模型完成数据增强。",
    "en_tdlr": "This paper proposes a paraphrasing-guided data augmentation method for contrastive prompt-based few-shot fine-tuning. The method leverages generative language models to perform data augmentation on prompt-based few-shot paraphrasing, in order to enhance the model's ability to distinguish between classes and improve sample efficiency."
}