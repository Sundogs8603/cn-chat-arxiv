{
    "title": "Horizon-free Reinforcement Learning in Adversarial Linear Mixture MDPs. (arXiv:2305.08359v1 [cs.LG])",
    "abstract": "Recent studies have shown that episodic reinforcement learning (RL) is no harder than bandits when the total reward is bounded by $1$, and proved regret bounds that have a polylogarithmic dependence on the planning horizon $H$. However, it remains an open question that if such results can be carried over to adversarial RL, where the reward is adversarially chosen at each episode. In this paper, we answer this question affirmatively by proposing the first horizon-free policy search algorithm. To tackle the challenges caused by exploration and adversarially chosen reward, our algorithm employs (1) a variance-uncertainty-aware weighted least square estimator for the transition kernel; and (2) an occupancy measure-based technique for the online search of a \\emph{stochastic} policy. We show that our algorithm achieves an $\\tilde{O}\\big((d+\\log (|\\mathcal{S}|^2 |\\mathcal{A}|))\\sqrt{K}\\big)$ regret with full-information feedback, where $d$ is the dimension of a known feature mapping linearly ",
    "link": "http://arxiv.org/abs/2305.08359",
    "context": "Title: Horizon-free Reinforcement Learning in Adversarial Linear Mixture MDPs. (arXiv:2305.08359v1 [cs.LG])\nAbstract: Recent studies have shown that episodic reinforcement learning (RL) is no harder than bandits when the total reward is bounded by $1$, and proved regret bounds that have a polylogarithmic dependence on the planning horizon $H$. However, it remains an open question that if such results can be carried over to adversarial RL, where the reward is adversarially chosen at each episode. In this paper, we answer this question affirmatively by proposing the first horizon-free policy search algorithm. To tackle the challenges caused by exploration and adversarially chosen reward, our algorithm employs (1) a variance-uncertainty-aware weighted least square estimator for the transition kernel; and (2) an occupancy measure-based technique for the online search of a \\emph{stochastic} policy. We show that our algorithm achieves an $\\tilde{O}\\big((d+\\log (|\\mathcal{S}|^2 |\\mathcal{A}|))\\sqrt{K}\\big)$ regret with full-information feedback, where $d$ is the dimension of a known feature mapping linearly ",
    "path": "papers/23/05/2305.08359.json",
    "total_tokens": 1085,
    "translated_title": "对抗性线性混合MDP中的无地平线强化学习",
    "translated_abstract": "最近的研究表明，当总奖励受到1的限制时，分集式强化学习(RL)并不比单臂匪徒问题更难，并证明了这种情况下在规划地平线H上的遗憾界具有对数多项式依赖性。然而，是否可以将这种结果推广到对抗性RL中的奖励被每集合控制的情况仍然是一个开放性问题。本文通过提出第一种无地平线策略搜索算法，肯定回答了这个问题。为了解决探索和对抗性奖励带来的挑战，我们的算法采用了(1)方差-不确定性感知加权最小二乘估计器用于转移核心的估计；和(2)一种基于占用测量的技术，用于在线搜索一个随机策略。我们证明了，在完全信息反馈下，我们的算法达到了一个$\\tilde{O}\\big((d+\\log (|\\mathcal{S}|^2 |\\mathcal{A}|))\\sqrt{K}\\big)$的遗憾界。其中$d$是已知特征映射的维度。",
    "tldr": "本文提出了第一种在对抗性RL中实现无地平线策略搜索的算法，并通过采用方差-不确定性感知加权最小二乘估计器和基于占用测量的技术解决了探索和对抗性奖励的挑战。算法达到了一个 $\\tilde{O}\\big((d+\\log (|\\mathcal{S}|^2 |\\mathcal{A}|))\\sqrt{K}\\big)$ 的遗憾界。",
    "en_tdlr": "This paper proposes the first horizon-free policy search algorithm in adversarial RL, tackling the challenges caused by exploration and adversarially chosen reward using a variance-uncertainty-aware weighted least square estimator and an occupancy measure-based technique. The algorithm achieves an $\\tilde{O}\\big((d+\\log (|\\mathcal{S}|^2 |\\mathcal{A}|))\\sqrt{K}\\big)$ regret with full-information feedback."
}