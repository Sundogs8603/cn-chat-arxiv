{
    "title": "Federated Neural Compression Under Heterogeneous Data. (arXiv:2305.16416v1 [cs.LG])",
    "abstract": "We discuss a federated learned compression problem, where the goal is to learn a compressor from real-world data which is scattered across clients and may be statistically heterogeneous, yet share a common underlying representation. We propose a distributed source model that encompasses both characteristics, and naturally suggests a compressor architecture that uses analysis and synthesis transforms shared by clients. Inspired by personalized federated learning methods, we employ an entropy model that is personalized to each client. This allows for a global latent space to be learned across clients, and personalized entropy models that adapt to the clients' latent distributions. We show empirically that this strategy outperforms solely local methods, which indicates that learned compression also benefits from a shared global representation in statistically heterogeneous federated settings.",
    "link": "http://arxiv.org/abs/2305.16416",
    "context": "Title: Federated Neural Compression Under Heterogeneous Data. (arXiv:2305.16416v1 [cs.LG])\nAbstract: We discuss a federated learned compression problem, where the goal is to learn a compressor from real-world data which is scattered across clients and may be statistically heterogeneous, yet share a common underlying representation. We propose a distributed source model that encompasses both characteristics, and naturally suggests a compressor architecture that uses analysis and synthesis transforms shared by clients. Inspired by personalized federated learning methods, we employ an entropy model that is personalized to each client. This allows for a global latent space to be learned across clients, and personalized entropy models that adapt to the clients' latent distributions. We show empirically that this strategy outperforms solely local methods, which indicates that learned compression also benefits from a shared global representation in statistically heterogeneous federated settings.",
    "path": "papers/23/05/2305.16416.json",
    "total_tokens": 807,
    "translated_title": "异构数据下的联邦神经压缩",
    "translated_abstract": "本文探讨了一种联邦学习压缩问题，其目标是从散布在客户端上且可能是统计异构的真实世界数据中学习一个压缩器，但保有共同的潜在表示。我们提出了一个分布式源模型，既包括这两种特征，也自然地提出了一种压缩器结构，该结构使用由客户端共享的分析和合成变换。受个性化联邦学习方法的启发，我们采用了一个对每个客户端个性化的熵模型。这允许在客户端之间学习全局潜在空间，并个性化地调整以适应客户端的潜在分布。经验证明，该策略优于仅使用本地方法，这表明在统计异构的联邦设置中，学习压缩也受益于共享的全局表示。",
    "tldr": "本文介绍了在异构数据下的联邦神经压缩问题，提出了一个分布式源模型和个性化熵模型的解决方案，在联邦设置中，使用全局共享表示优于本地方法。",
    "en_tdlr": "This paper presents a solution to the federated neural compression problem under heterogeneous data by proposing a distributed source model and personalized entropy model, and shows that the use of shared global representation performs better than local methods in a federated setting with statistically heterogeneous data."
}