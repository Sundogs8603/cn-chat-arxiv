{
    "title": "KEPLET: Knowledge-Enhanced Pretrained Language Model with Topic Entity Awareness. (arXiv:2305.01810v1 [cs.CL])",
    "abstract": "In recent years, Pre-trained Language Models (PLMs) have shown their superiority by pre-training on unstructured text corpus and then fine-tuning on downstream tasks. On entity-rich textual resources like Wikipedia, Knowledge-Enhanced PLMs (KEPLMs) incorporate the interactions between tokens and mentioned entities in pre-training, and are thus more effective on entity-centric tasks such as entity linking and relation classification. Although exploiting Wikipedia's rich structures to some extent, conventional KEPLMs still neglect a unique layout of the corpus where each Wikipedia page is around a topic entity (identified by the page URL and shown in the page title). In this paper, we demonstrate that KEPLMs without incorporating the topic entities will lead to insufficient entity interaction and biased (relation) word semantics. We thus propose KEPLET, a novel Knowledge-Enhanced Pre-trained LanguagE model with Topic entity awareness. In an end-to-end manner, KEPLET identifies where to a",
    "link": "http://arxiv.org/abs/2305.01810",
    "context": "Title: KEPLET: Knowledge-Enhanced Pretrained Language Model with Topic Entity Awareness. (arXiv:2305.01810v1 [cs.CL])\nAbstract: In recent years, Pre-trained Language Models (PLMs) have shown their superiority by pre-training on unstructured text corpus and then fine-tuning on downstream tasks. On entity-rich textual resources like Wikipedia, Knowledge-Enhanced PLMs (KEPLMs) incorporate the interactions between tokens and mentioned entities in pre-training, and are thus more effective on entity-centric tasks such as entity linking and relation classification. Although exploiting Wikipedia's rich structures to some extent, conventional KEPLMs still neglect a unique layout of the corpus where each Wikipedia page is around a topic entity (identified by the page URL and shown in the page title). In this paper, we demonstrate that KEPLMs without incorporating the topic entities will lead to insufficient entity interaction and biased (relation) word semantics. We thus propose KEPLET, a novel Knowledge-Enhanced Pre-trained LanguagE model with Topic entity awareness. In an end-to-end manner, KEPLET identifies where to a",
    "path": "papers/23/05/2305.01810.json",
    "total_tokens": 948,
    "translated_title": "KEPLET: 一种带有主题实体感知的知识增强预训练语言模型",
    "translated_abstract": "近年来，预训练语言模型（PLM）通过在未结构化文本语料上进行预训练，然后在下游任务上进行微调以显示其优越性。在实体丰富的文本资源（如维基百科）中，知识增强的PLM（KEPLMs）在预训练过程中将标记与所提及的实体之间的交互结合起来，因此在实体中心任务（如实体链接和关系分类）上更有效。虽然传统KEPLM在某种程度上利用了维基百科的丰富结构，但它们仍然忽略了每个维基百科页面围绕一个主题实体的独特布局（由页面URL标识并在页面标题中显示）。本文证明，如果不加入主题实体，KEPLM将导致实体交互不足和偏差（关系）词语语义。因此，我们提出了KEPLET，一种新颖的带有主题实体感知的知识增强预训练语言模型。通过一种端到端的方式，KEPLET确定预训练语料中的主题实体，并改进了KEPLM的实体交互和词语语义表示。",
    "tldr": "本文提出了一种知识增强预训练语言模型，即KEPLET，在预训练语料中加入主题实体感知，从而改善了实体交互和词语语义表示。"
}