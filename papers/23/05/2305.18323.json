{
    "title": "ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models. (arXiv:2305.18323v1 [cs.CL])",
    "abstract": "Augmented Language Models (ALMs) blend the reasoning capabilities of Large Language Models (LLMs) with tools that allow for knowledge retrieval and action execution. Existing ALM systems trigger LLM thought processes while pulling observations from these tools in an interleaved fashion. Specifically, an LLM reasons to call an external tool, gets halted to fetch the tool's response, and then decides the next action based on all preceding response tokens. Such a paradigm, though straightforward and easy to implement, often leads to huge computation complexity from redundant prompts and repeated execution. This study addresses such challenges for the first time, proposing a modular paradigm ReWOO (Reasoning WithOut Observation) that detaches the reasoning process from external observations, thus significantly reducing token consumption. Comprehensive evaluations across six public NLP benchmarks and a curated dataset reveal consistent performance enhancements with our proposed methodology.",
    "link": "http://arxiv.org/abs/2305.18323",
    "context": "Title: ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models. (arXiv:2305.18323v1 [cs.CL])\nAbstract: Augmented Language Models (ALMs) blend the reasoning capabilities of Large Language Models (LLMs) with tools that allow for knowledge retrieval and action execution. Existing ALM systems trigger LLM thought processes while pulling observations from these tools in an interleaved fashion. Specifically, an LLM reasons to call an external tool, gets halted to fetch the tool's response, and then decides the next action based on all preceding response tokens. Such a paradigm, though straightforward and easy to implement, often leads to huge computation complexity from redundant prompts and repeated execution. This study addresses such challenges for the first time, proposing a modular paradigm ReWOO (Reasoning WithOut Observation) that detaches the reasoning process from external observations, thus significantly reducing token consumption. Comprehensive evaluations across six public NLP benchmarks and a curated dataset reveal consistent performance enhancements with our proposed methodology.",
    "path": "papers/23/05/2305.18323.json",
    "total_tokens": 737,
    "translated_title": "ReWOO：将推理与观察分离，实现高效增强语言模型",
    "translated_abstract": "增强语言模型（ALMs）将大型语言模型（LLMs）的推理能力与允许知识检索和行动执行的工具相结合。现有的ALM系统以交错方式触发LLM思考过程，同时从这些工具中提取观察。本研究首次提出了一种模块化范式ReWOO（Without Observation Reasoning），将推理过程与外部观察分离，从而显著减少标记消耗。在六个公共NLP基准测试和一个策划数据集中进行全面评估，结果显示采用我们提出的方法可以显著提高性能。",
    "tldr": "ReWOO是一种将推理过程与外部观察分离的模块化范式，从而可以减少标记消耗并提高性能。",
    "en_tdlr": "ReWOO proposes a modular paradigm to detach the reasoning process from external observations in augmented language models, which significantly reduces token consumption and improves performance."
}