{
    "title": "Self-Supervised Sentence Compression for Meeting Summarization. (arXiv:2305.07988v1 [cs.CL])",
    "abstract": "The conventional summarization model often fails to capture critical information in meeting transcripts, as meeting corpus usually involves multiple parties with lengthy conversations and is stuffed with redundant and trivial content. To tackle this problem, we present SVB, an effective and efficient framework for meeting summarization that `compress' the redundancy while preserving important content via three processes: sliding-window dialogue restoration and \\textbf{S}coring, channel-wise importance score \\textbf{V}oting, and relative positional \\textbf{B}ucketing. Specifically, under the self-supervised paradigm, the sliding-window scoring aims to rate the importance of each token from multiple views. Then these ratings are aggregated by channel-wise voting. Tokens with high ratings will be regarded as salient information and labeled as \\textit{anchors}. Finally, to tailor the lengthy input to an acceptable length for the language model, the relative positional bucketing algorithm i",
    "link": "http://arxiv.org/abs/2305.07988",
    "context": "Title: Self-Supervised Sentence Compression for Meeting Summarization. (arXiv:2305.07988v1 [cs.CL])\nAbstract: The conventional summarization model often fails to capture critical information in meeting transcripts, as meeting corpus usually involves multiple parties with lengthy conversations and is stuffed with redundant and trivial content. To tackle this problem, we present SVB, an effective and efficient framework for meeting summarization that `compress' the redundancy while preserving important content via three processes: sliding-window dialogue restoration and \\textbf{S}coring, channel-wise importance score \\textbf{V}oting, and relative positional \\textbf{B}ucketing. Specifically, under the self-supervised paradigm, the sliding-window scoring aims to rate the importance of each token from multiple views. Then these ratings are aggregated by channel-wise voting. Tokens with high ratings will be regarded as salient information and labeled as \\textit{anchors}. Finally, to tailor the lengthy input to an acceptable length for the language model, the relative positional bucketing algorithm i",
    "path": "papers/23/05/2305.07988.json",
    "total_tokens": 900,
    "translated_title": "自监督的句子压缩用于会议摘要",
    "translated_abstract": "传统总结模型往往无法捕捉到会议记录中的关键信息，因为会议语料库通常涉及多个参与者的冗长对话并充斥着冗余和琐碎的内容。为了解决这个问题，我们提出了SVB，一个有效且高效的会议摘要框架，通过三个过程“压缩”冗余而保留重要内容：滑动窗口对话恢复与评分、通道重要性得分投票和相对位置分箱。具体来说，在自监督范式下，滑动窗口评分旨在从多个视角评估每个标记的重要性。然后通过通道投票聚合这些评分。具有高评分的标记将被视为显着信息并标记为“anchers”。最后，为了使输入长度适合语言模型的长度限制，采用了相对位置分箱算法。",
    "tldr": "本论文提出了一种自监督学习的会议摘要框架SVB，通过三个过程压缩冗余内容并保留关键信息。其中，滑动窗口对话恢复与评分、通道重要性得分投票和相对位置分箱等算法用于实现这个框架。",
    "en_tdlr": "This paper proposes a self-supervised framework for meeting summarization called SVB, which compresses redundancy and preserves important content through three processes: sliding-window dialogue restoration and scoring, channel-wise importance score voting, and relative positional bucketing. The framework employs algorithms such as sliding-window scoring, channel-wise voting, and relative positional bucketing to implement the framework."
}