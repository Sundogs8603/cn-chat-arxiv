{
    "title": "On the Generalization of Diffusion Model. (arXiv:2305.14712v1 [cs.LG])",
    "abstract": "The diffusion probabilistic generative models are widely used to generate high-quality data. Though they can synthetic data that does not exist in the training set, the rationale behind such generalization is still unexplored. In this paper, we formally define the generalization of the generative model, which is measured by the mutual information between the generated data and the training set. The definition originates from the intuition that the model which generates data with less correlation to the training set exhibits better generalization ability. Meanwhile, we show that for the empirical optimal diffusion model, the data generated by a deterministic sampler are all highly related to the training set, thus poor generalization. This result contradicts the observation of the trained diffusion model's (approximating empirical optima) extrapolation ability (generating unseen data). To understand this contradiction, we empirically verify the difference between the sufficiently traine",
    "link": "http://arxiv.org/abs/2305.14712",
    "context": "Title: On the Generalization of Diffusion Model. (arXiv:2305.14712v1 [cs.LG])\nAbstract: The diffusion probabilistic generative models are widely used to generate high-quality data. Though they can synthetic data that does not exist in the training set, the rationale behind such generalization is still unexplored. In this paper, we formally define the generalization of the generative model, which is measured by the mutual information between the generated data and the training set. The definition originates from the intuition that the model which generates data with less correlation to the training set exhibits better generalization ability. Meanwhile, we show that for the empirical optimal diffusion model, the data generated by a deterministic sampler are all highly related to the training set, thus poor generalization. This result contradicts the observation of the trained diffusion model's (approximating empirical optima) extrapolation ability (generating unseen data). To understand this contradiction, we empirically verify the difference between the sufficiently traine",
    "path": "papers/23/05/2305.14712.json",
    "total_tokens": 871,
    "translated_abstract": "扩散概率生成模型被广泛用于生成高质量数据。尽管它们可以合成训练集中不存在的数据，但是这种泛化背后的原理仍未被探索。在本文中，我们正式定义了生成模型的泛化，它由生成数据和训练集之间的互信息来衡量。定义起源于这样的直觉，即生成的数据与训练集之间的相关性越小，模型的泛化能力越好。同时，我们表明针对实验最佳扩散模型，通过确定性采样得到的数据都与训练集高度相关，因此泛化能力较差。这一结果与训练过的扩散模型（逼近实验最优解）的外推能力（生成未见过的数据）相矛盾。为了理解这种矛盾，我们经验证明了足够训练的和近似实验最优扩散模型之间的差异。",
    "tldr": "本文正式定义了生成模型的泛化程度，通过生成数据和训练集之间的互信息来衡量。同时，发现通过确定性采样得到的数据都与训练集高度相关，泛化能力较差。",
    "en_tdlr": "This paper formally defines the generalization of generative models, which is measured by the mutual information between generated data and the training set. The study reveals that the data generated by a deterministic sampler are highly related to the training set, thus showing poor generalization ability."
}