{
    "title": "AMELI: Enhancing Multimodal Entity Linking with Fine-Grained Attributes. (arXiv:2305.14725v1 [cs.CL])",
    "abstract": "We propose attribute-aware multimodal entity linking, where the input is a mention described with a text and image, and the goal is to predict the corresponding target entity from a multimodal knowledge base (KB) where each entity is also described with a text description, a visual image and a set of attributes and values. To support this research, we construct AMELI, a large-scale dataset consisting of 18,472 reviews and 35,598 products. To establish baseline performance on AMELI, we experiment with the current state-of-the-art multimodal entity linking approaches and our enhanced attribute-aware model and demonstrate the importance of incorporating the attribute information into the entity linking process. To be best of our knowledge, we are the first to build benchmark dataset and solutions for the attribute-aware multimodal entity linking task. Datasets and codes will be made publicly available.",
    "link": "http://arxiv.org/abs/2305.14725",
    "context": "Title: AMELI: Enhancing Multimodal Entity Linking with Fine-Grained Attributes. (arXiv:2305.14725v1 [cs.CL])\nAbstract: We propose attribute-aware multimodal entity linking, where the input is a mention described with a text and image, and the goal is to predict the corresponding target entity from a multimodal knowledge base (KB) where each entity is also described with a text description, a visual image and a set of attributes and values. To support this research, we construct AMELI, a large-scale dataset consisting of 18,472 reviews and 35,598 products. To establish baseline performance on AMELI, we experiment with the current state-of-the-art multimodal entity linking approaches and our enhanced attribute-aware model and demonstrate the importance of incorporating the attribute information into the entity linking process. To be best of our knowledge, we are the first to build benchmark dataset and solutions for the attribute-aware multimodal entity linking task. Datasets and codes will be made publicly available.",
    "path": "papers/23/05/2305.14725.json",
    "total_tokens": 793,
    "translated_title": "AMELI:细粒度属性增强多模态实体链接",
    "translated_abstract": "我们提出了一种属性感知的多模态实体链接方法，其中输入是一个由文本和图像描述的提及，目标是从一个多模态知识库中预测相应的目标实体，其中每个实体都是用文本描述、视觉图像和一组属性值描述的。为了支持这项研究，我们构建了一个大型数据集AMELI，其中包含18,472个评论和35,598个产品。我们在AMELI上进行了实验，使用当前最先进的多模态实体链接方法和我们增强的属性感知模型来建立基准性能，并展示了将属性信息纳入实体链接过程中的重要性。据我们所知，我们是第一个为属性感知多模态实体链接任务建立基准数据集和解决方案的团队。数据集和代码将公开提供。",
    "tldr": "提出了一种属性感知的多模态实体链接方法，并构建了一个大型数据集AMELI，实验证明了将属性信息纳入实体链接过程的重要性。",
    "en_tdlr": "Proposed an attribute-aware multimodal entity linking method, and constructed a large-scale dataset AMELI. It has been demonstrated that the incorporation of attribute information is essential in the entity linking process through experiments with state-of-the-art approaches on AMELI."
}