{
    "title": "APPLS: Evaluating Evaluation Metrics for Plain Language Summarization",
    "abstract": "While there has been significant development of models for Plain Language Summarization (PLS), evaluation remains a challenge. PLS lacks a dedicated assessment metric, and the suitability of text generation evaluation metrics is unclear due to the unique transformations involved (e.g., adding background explanations, removing specialized terminology). To address these concerns, our study presents a granular meta-evaluation testbed, APPLS, designed to evaluate metrics for PLS. We define a set of perturbations along four criteria inspired by previous work that a PLS metric should capture: informativeness, simplification, coherence, and faithfulness. An analysis of metrics using our testbed reveals that current metrics fail to capture simplification consistently. In response, we introduce POMME, a new metric designed to assess text simplification in PLS; the metric is calculated as the normalized perplexity difference between an in-domain and out-of-domain language model. We demonstrate P",
    "link": "https://arxiv.org/abs/2305.14341",
    "context": "Title: APPLS: Evaluating Evaluation Metrics for Plain Language Summarization\nAbstract: While there has been significant development of models for Plain Language Summarization (PLS), evaluation remains a challenge. PLS lacks a dedicated assessment metric, and the suitability of text generation evaluation metrics is unclear due to the unique transformations involved (e.g., adding background explanations, removing specialized terminology). To address these concerns, our study presents a granular meta-evaluation testbed, APPLS, designed to evaluate metrics for PLS. We define a set of perturbations along four criteria inspired by previous work that a PLS metric should capture: informativeness, simplification, coherence, and faithfulness. An analysis of metrics using our testbed reveals that current metrics fail to capture simplification consistently. In response, we introduce POMME, a new metric designed to assess text simplification in PLS; the metric is calculated as the normalized perplexity difference between an in-domain and out-of-domain language model. We demonstrate P",
    "path": "papers/23/05/2305.14341.json",
    "total_tokens": 936,
    "translated_title": "APPLS: 评估纯语言摘要的评价指标",
    "translated_abstract": "尽管对于纯语言摘要（PLS）的模型有了很大的发展，但评估仍然是一个挑战。PLS缺乏专门的评估指标，由于涉及到独特的转换（例如，添加背景解释，删除专业术语），因此对于文本生成评估指标的适用性尚不清楚。为了解决这些问题，我们的研究提出了一个细致的元评估测试平台APPLS，旨在评估PLS的指标。我们根据先前工作的启发，定义了四个标准上的一组扰动，PLS指标应该捕捉到：信息性、简化度、连贯性和忠实度。使用我们的测试平台对指标进行分析发现，当前的指标未能始终捕捉到简化度。作为回应，我们引入了一种新的指标POMME，旨在评估PLS中文本简化；该指标是根据域内和域外语言模型之间的标准化困惑度差计算得到的。我们演示了POMME的效果，并与其他指标进行了比较。",
    "tldr": "本文提出了一个用于评估纯语言摘要的指标测试平台APPLS，并引入了一种新的指标POMME来评估PLS中的文本简化。通过对指标的分析发现，当前的指标未能始终捕捉到简化度。",
    "en_tdlr": "This paper introduces a testbed, APPLS, for evaluating metrics for Plain Language Summarization (PLS) and proposes a new metric, POMME, for assessing text simplification in PLS. An analysis of existing metrics reveals their lack of consistency in capturing simplification."
}