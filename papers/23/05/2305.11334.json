{
    "title": "Writing your own book: A method for going from closed to open book QA to improve robustness and performance of smaller LLMs. (arXiv:2305.11334v1 [cs.CL])",
    "abstract": "We introduce two novel methods, Tree-Search and Self-contextualizing QA, designed to enhance the performance of large language models (LLMs) in question-answering tasks. Tree-Search is a sampling technique specifically created to extract diverse information from an LLM for a given prompt. Self-contextualizing QA leverages Tree-Search to enable the model to create its own context using a wide range of information relevant to the prompt, evaluate it explicitly and return a open book answer to the initial prompt . We demonstrate that the quality of generated answers improves according to various metrics, including accuracy, informativeness, coherence, and consistency, as evaluated by GPT3.5(text-davinci-003). Furthermore, we show that our methods result in increased robustness and that performance is positively correlated with tree size, benefiting both answer quality and robustness. Finally, we discuss other promising applications of Tree-Search, highlighting its potential to enhance a b",
    "link": "http://arxiv.org/abs/2305.11334",
    "context": "Title: Writing your own book: A method for going from closed to open book QA to improve robustness and performance of smaller LLMs. (arXiv:2305.11334v1 [cs.CL])\nAbstract: We introduce two novel methods, Tree-Search and Self-contextualizing QA, designed to enhance the performance of large language models (LLMs) in question-answering tasks. Tree-Search is a sampling technique specifically created to extract diverse information from an LLM for a given prompt. Self-contextualizing QA leverages Tree-Search to enable the model to create its own context using a wide range of information relevant to the prompt, evaluate it explicitly and return a open book answer to the initial prompt . We demonstrate that the quality of generated answers improves according to various metrics, including accuracy, informativeness, coherence, and consistency, as evaluated by GPT3.5(text-davinci-003). Furthermore, we show that our methods result in increased robustness and that performance is positively correlated with tree size, benefiting both answer quality and robustness. Finally, we discuss other promising applications of Tree-Search, highlighting its potential to enhance a b",
    "path": "papers/23/05/2305.11334.json",
    "total_tokens": 1015,
    "translated_title": "编写自己的书：一种从闭合到开放式书本QA的方法，改善较小LLM的健壮性和性能。",
    "translated_abstract": "我们介绍了两种新颖的方法，Tree-Search和自我上下文QA，旨在提高大型语言模型（LLMs）在问答任务中的性能。 Tree-Search是一种采样技术，专门用于从给定提示的LLM中提取多样化的信息。自我上下文QA利用Tree-Search，使模型能够使用与提示相关的各种信息创建自己的上下文，明确评估并返回初始提示的开放式答案。我们证明了按照各种指标（包括GPT3.5（text-davinci-003）评估的准确性、信息量、连贯性和一致性）评估的生成答案质量得到了改善。此外，我们表明，我们的方法导致了增加的健壮性，并且性能与树大小呈正相关，从而有益于答案质量和健壮性。最后，我们讨论了Tree-Search的其他有 promising 应用，突出了其提高较小LLM健壮性和性能的潜力。",
    "tldr": "本文介绍了两种新颖的方法，Tree-Search和自我上下文QA，可提高大型语言模型在问答任务中的性能。Tree-Search采样技术有助于从提示中提取多样化信息，而自我上下文QA可使模型创建自己的上下文，生成更好的开放式答案。此外，这些方法可提高健壮性和性能。",
    "en_tdlr": "This paper presents two novel methods, Tree-Search and self-contextualizing QA, that improve the performance of large language models in question-answering tasks. Tree-Search is a sampling technique that extracts diverse information from a given prompt, while self-contextualizing QA enables the model to create its own context and generate better open book answers. These methods also lead to increased robustness and performance."
}