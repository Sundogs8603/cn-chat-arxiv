{
    "title": "RAND: Robustness Aware Norm Decay For Quantized Seq2seq Models. (arXiv:2305.15536v1 [eess.AS])",
    "abstract": "With the rapid increase in the size of neural networks, model compression has become an important area of research. Quantization is an effective technique at decreasing the model size, memory access, and compute load of large models. Despite recent advances in quantization aware training (QAT) technique, most papers present evaluations that are focused on computer vision tasks, which have different training dynamics compared to sequence tasks. In this paper, we first benchmark the impact of popular techniques such as straight through estimator, pseudo-quantization noise, learnable scale parameter, clipping, etc. on 4-bit seq2seq models across a suite of speech recognition datasets ranging from 1,000 hours to 1 million hours, as well as one machine translation dataset to illustrate its applicability outside of speech.  Through the experiments, we report that noise based QAT suffers when there is insufficient regularization signal flowing back to the quantization scale. We propose low co",
    "link": "http://arxiv.org/abs/2305.15536",
    "context": "Title: RAND: Robustness Aware Norm Decay For Quantized Seq2seq Models. (arXiv:2305.15536v1 [eess.AS])\nAbstract: With the rapid increase in the size of neural networks, model compression has become an important area of research. Quantization is an effective technique at decreasing the model size, memory access, and compute load of large models. Despite recent advances in quantization aware training (QAT) technique, most papers present evaluations that are focused on computer vision tasks, which have different training dynamics compared to sequence tasks. In this paper, we first benchmark the impact of popular techniques such as straight through estimator, pseudo-quantization noise, learnable scale parameter, clipping, etc. on 4-bit seq2seq models across a suite of speech recognition datasets ranging from 1,000 hours to 1 million hours, as well as one machine translation dataset to illustrate its applicability outside of speech.  Through the experiments, we report that noise based QAT suffers when there is insufficient regularization signal flowing back to the quantization scale. We propose low co",
    "path": "papers/23/05/2305.15536.json",
    "total_tokens": 813,
    "translated_title": "RAND:用于量化Seq2seq模型的鲁棒性感知范数衰减技术",
    "translated_abstract": "随着神经网络规模的迅速增长，模型压缩已成为重要的研究领域。量化是一种有效的技术，可以降低大型模型的大小、内存访问和计算负载。本文研究了一系列流行技术在4位Seq2seq模型中的影响，通过多个语音识别数据集和一个机器翻译数据集的实验，发现基于噪声的量化技术QAT在量化范围回传时缺乏正则化信号时会失效。",
    "tldr": "本文提出了用于量化Seq2seq模型的鲁棒性感知范数衰减技术，在语音识别和机器翻译任务中表现良好，特别是在处理量化范围回传时缺乏正则化信号的情况下更是如此。"
}