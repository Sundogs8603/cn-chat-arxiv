{
    "title": "SignSVRG: fixing SignSGD via variance reduction. (arXiv:2305.13187v1 [math.OC])",
    "abstract": "We consider the problem of unconstrained minimization of finite sums of functions. We propose a simple, yet, practical way to incorporate variance reduction techniques into SignSGD, guaranteeing convergence that is similar to the full sign gradient descent. The core idea is first instantiated on the problem of minimizing sums of convex and Lipschitz functions and is then extended to the smooth case via variance reduction. Our analysis is elementary and much simpler than the typical proof for variance reduction methods. We show that for smooth functions our method gives $\\mathcal{O}(1 / \\sqrt{T})$ rate for expected norm of the gradient and $\\mathcal{O}(1/T)$ rate in the case of smooth convex functions, recovering convergence results of deterministic methods, while preserving computational advantages of SignSGD.",
    "link": "http://arxiv.org/abs/2305.13187",
    "context": "Title: SignSVRG: fixing SignSGD via variance reduction. (arXiv:2305.13187v1 [math.OC])\nAbstract: We consider the problem of unconstrained minimization of finite sums of functions. We propose a simple, yet, practical way to incorporate variance reduction techniques into SignSGD, guaranteeing convergence that is similar to the full sign gradient descent. The core idea is first instantiated on the problem of minimizing sums of convex and Lipschitz functions and is then extended to the smooth case via variance reduction. Our analysis is elementary and much simpler than the typical proof for variance reduction methods. We show that for smooth functions our method gives $\\mathcal{O}(1 / \\sqrt{T})$ rate for expected norm of the gradient and $\\mathcal{O}(1/T)$ rate in the case of smooth convex functions, recovering convergence results of deterministic methods, while preserving computational advantages of SignSGD.",
    "path": "papers/23/05/2305.13187.json",
    "total_tokens": 841,
    "translated_title": "SignSVRG: 通过方差缩减修正SignSGD",
    "translated_abstract": "本文考虑了无约束函数的有限和最小化问题。我们提出了一种简单但实用的方法，将方差缩减技术纳入到SignSGD中，并保证了类似完整符号梯度下降的收敛性。该核心思想首先应用于凸和Lipschitz函数和的最小化问题，然后通过方差缩减扩展到平滑情况。我们的分析很基础，比典型的方差缩减方法的证明简单得多。我们表明，对于平滑函数，我们的方法给出了期望梯度范数的$ \\mathcal {O}（1 / \\ sqrt {T}）$收敛速度，并且对于平滑凸函数的情况，收敛速度为$ \\mathcal {O}（1 / T）$，恢复了确定性方法的收敛结果，同时保留了SignSGD的计算优势。",
    "tldr": "本文介绍了一种简单但实用的方法，将方差缩减技术纳入到SignSGD中，并保证了类似完整符号梯度下降的收敛性。"
}