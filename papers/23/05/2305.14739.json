{
    "title": "Trusting Your Evidence: Hallucinate Less with Context-aware Decoding. (arXiv:2305.14739v1 [cs.CL])",
    "abstract": "Language models (LMs) often struggle to pay enough attention to the input context, and generate texts that are unfaithful or contain hallucinations. To mitigate this issue, we present context-aware decoding (CAD), which follows a contrastive output distribution that amplifies the difference between the output probabilities when a model is used with and without context. Our experiments show that CAD, without additional training, significantly improves the faithfulness of different LM families, including OPT, GPT, LLaMA and FLAN-T5 for summarization tasks (e.g., 14.3% gain for LLaMA in factuality metrics). Furthermore, CAD is particularly effective in overriding a model's prior knowledge when it contradicts the provided context, leading to substantial improvements in tasks where resolving the knowledge conflict is essential.",
    "link": "http://arxiv.org/abs/2305.14739",
    "context": "Title: Trusting Your Evidence: Hallucinate Less with Context-aware Decoding. (arXiv:2305.14739v1 [cs.CL])\nAbstract: Language models (LMs) often struggle to pay enough attention to the input context, and generate texts that are unfaithful or contain hallucinations. To mitigate this issue, we present context-aware decoding (CAD), which follows a contrastive output distribution that amplifies the difference between the output probabilities when a model is used with and without context. Our experiments show that CAD, without additional training, significantly improves the faithfulness of different LM families, including OPT, GPT, LLaMA and FLAN-T5 for summarization tasks (e.g., 14.3% gain for LLaMA in factuality metrics). Furthermore, CAD is particularly effective in overriding a model's prior knowledge when it contradicts the provided context, leading to substantial improvements in tasks where resolving the knowledge conflict is essential.",
    "path": "papers/23/05/2305.14739.json",
    "total_tokens": 819,
    "translated_title": "在上下文感知解码的帮助下信任您的证据：更少的幻觉",
    "translated_abstract": "语言模型经常难以充分关注输入上下文，并生成不忠实或包含幻觉的文本。为了缓解这个问题，我们提出了上下文感知解码 (CAD)，它遵循对比输出分布，可以放大使用模型时有无上下文时输出概率之间的差异。我们的实验表明，CAD在不需要额外训练的情况下，显著提高了不同LM系列的忠实度，包括OPT、GPT、LLaMA和FLAN-T5用于总结任务 (例如，LLaMA在事实度度量方面获得了14.3%的增益)。此外，当模型的先验知识与所提供的上下文冲突时，CAD特别有效，从而在解决知识冲突至关重要的任务中取得了显着的改进。",
    "tldr": "提出了上下文感知解码 (CAD)，在不需要额外训练的情况下，显著提高了不同LM系列的忠实度，特别是在解决知识冲突至关重要的任务中取得了显着的改进。",
    "en_tdlr": "The paper proposes context-aware decoding (CAD) to improve the faithfulness of language models for summarization tasks without additional training, and is particularly effective in tasks where resolving knowledge conflicts is essential."
}