{
    "title": "Fantastic DNN Classifiers and How to Identify them without Data. (arXiv:2305.15563v1 [cs.LG])",
    "abstract": "Current algorithms and architecture can create excellent DNN classifier models from example data. In general, larger training datasets result in better model estimations, which improve test performance. Existing methods for predicting generalization performance are based on hold-out test examples. To the best of our knowledge, at present no method exists that can estimate the quality of a trained DNN classifier without test data. In this paper, we show that the quality of a trained DNN classifier can be assessed without any example data. We consider DNNs to be composed of a feature extractor and a feature classifier; the feature extractor's output is fed to the classifier. The proposed method iteratively creates class prototypes in the input space for each class by minimizing a cross-entropy loss function at the output of the network. We use these prototypes and their feature relationships to reveal the quality of the classifier. We have developed two metrics: one using the features of",
    "link": "http://arxiv.org/abs/2305.15563",
    "context": "Title: Fantastic DNN Classifiers and How to Identify them without Data. (arXiv:2305.15563v1 [cs.LG])\nAbstract: Current algorithms and architecture can create excellent DNN classifier models from example data. In general, larger training datasets result in better model estimations, which improve test performance. Existing methods for predicting generalization performance are based on hold-out test examples. To the best of our knowledge, at present no method exists that can estimate the quality of a trained DNN classifier without test data. In this paper, we show that the quality of a trained DNN classifier can be assessed without any example data. We consider DNNs to be composed of a feature extractor and a feature classifier; the feature extractor's output is fed to the classifier. The proposed method iteratively creates class prototypes in the input space for each class by minimizing a cross-entropy loss function at the output of the network. We use these prototypes and their feature relationships to reveal the quality of the classifier. We have developed two metrics: one using the features of",
    "path": "papers/23/05/2305.15563.json",
    "total_tokens": 902,
    "translated_title": "神奇的DNN分类器及其无需数据识别方法",
    "translated_abstract": "当前算法和结构可以从示例数据创建优秀的DNN分类器模型。一般来说，更大的训练数据集会导致更好的模型估计，从而提高测试性能。目前存在的预测广义性能的方法基于保留测试示例。据我们所知，目前不存在可以在没有测试数据的情况下估计训练过的DNN分类器质量的方法。本文提出了一种方法，可以评估已经训练的DNN分类器的质量，而不需要任何示例数据。我们认为DNN由特征提取器和特征分类器组成；将特征提取器输出馈送到分类器。所提出的方法通过在网络输出处最小化交叉熵损失函数，迭代地为每个类在输入空间中创建类原型。我们使用这些原型及其特征关系来揭示分类器的质量。我们开发了两个度量标准:一个使用特征的",
    "tldr": "本文提出一种方法可以在没有测试数据的情况下评估已经训练的DNN分类器的质量，通过在网络输出处最小化交叉熵损失函数，迭代地为每个类在输入空间中创建类原型，并使用这些原型及其特征关系来揭示分类器的质量。",
    "en_tdlr": "This paper proposes a method to assess the quality of a trained DNN classifier without any test data by iteratively creating class prototypes in the input space for each class through minimizing a cross-entropy loss function at the output of the network, and using these prototypes and their feature relationships to reveal the quality of the classifier."
}