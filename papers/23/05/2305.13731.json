{
    "title": "Text Is All You Need: Learning Language Representations for Sequential Recommendation. (arXiv:2305.13731v1 [cs.IR])",
    "abstract": "Sequential recommendation aims to model dynamic user behavior from historical interactions. Existing methods rely on either explicit item IDs or general textual features for sequence modeling to understand user preferences. While promising, these approaches still struggle to model cold-start items or transfer knowledge to new datasets. In this paper, we propose to model user preferences and item features as language representations that can be generalized to new items and datasets. To this end, we present a novel framework, named Recformer, which effectively learns language representations for sequential recommendation. Specifically, we propose to formulate an item as a \"sentence\" (word sequence) by flattening item key-value attributes described by text so that an item sequence for a user becomes a sequence of sentences. For recommendation, Recformer is trained to understand the \"sentence\" sequence and retrieve the next \"sentence\". To encode item sequences, we design a bi-directional T",
    "link": "http://arxiv.org/abs/2305.13731",
    "context": "Title: Text Is All You Need: Learning Language Representations for Sequential Recommendation. (arXiv:2305.13731v1 [cs.IR])\nAbstract: Sequential recommendation aims to model dynamic user behavior from historical interactions. Existing methods rely on either explicit item IDs or general textual features for sequence modeling to understand user preferences. While promising, these approaches still struggle to model cold-start items or transfer knowledge to new datasets. In this paper, we propose to model user preferences and item features as language representations that can be generalized to new items and datasets. To this end, we present a novel framework, named Recformer, which effectively learns language representations for sequential recommendation. Specifically, we propose to formulate an item as a \"sentence\" (word sequence) by flattening item key-value attributes described by text so that an item sequence for a user becomes a sequence of sentences. For recommendation, Recformer is trained to understand the \"sentence\" sequence and retrieve the next \"sentence\". To encode item sequences, we design a bi-directional T",
    "path": "papers/23/05/2305.13731.json",
    "total_tokens": 999,
    "translated_title": "文本是唯一需要的：用于序列推荐的语言表示学习",
    "translated_abstract": "序列推荐旨在从历史交互中建模动态用户行为。现有方法依靠明确的项目ID或一般文本特征进行序列建模，以理解用户喜好。尽管很有前途，但这些方法仍然难以建模冷启动项目或将知识转移至新数据集。在本文中，我们建议将用户喜好和项目特征建模为可以推广到新项目和数据集的语言表示。为此，我们提出了一个名为Recformer的新框架，它有效地学习序列推荐的语言表示。具体而言，我们建议通过展平由文本描述的项目键值属性，将项目作为“句子”（单词序列）来编写，以便用户的项目序列成为句子序列。为推荐，Recformer被训练以理解“句子”序列并检索下一个“句子”。为了编码项目序列，我们设计了一个双向Transformer，利用自我注意机制来捕捉长期依赖关系。我们在三个具有不同特征的公共数据集上进行了广泛的实验，并展示了Recformer始终优于现有最先进的方法。",
    "tldr": "本研究提出了一种名为Recformer的框架，它将用户喜好和项目特征建模为可以推广到新项目和数据集的语言表示，并利用双向Transformer来捕捉长期依赖关系，用于序列推荐，比目前最先进的方法表现更好。",
    "en_tdlr": "This paper proposes a framework called Recformer, which models user preferences and item features as language representations that can be generalized to new items and datasets. It utilizes a bi-directional Transformer to capture long-term dependencies for sequential recommendation, outperforming state-of-the-art methods consistently in experiments on three different datasets."
}