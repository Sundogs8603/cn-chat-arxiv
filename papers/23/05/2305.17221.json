{
    "title": "Federated Learning for Semantic Parsing: Task Formulation, Evaluation Setup, New Algorithms. (arXiv:2305.17221v1 [cs.CL])",
    "abstract": "This paper studies a new task of federated learning (FL) for semantic parsing, where multiple clients collaboratively train one global model without sharing their semantic parsing data. By leveraging data from multiple clients, the FL paradigm can be especially beneficial for clients that have little training data to develop a data-hungry neural semantic parser on their own. We propose an evaluation setup to study this task, where we re-purpose widely-used single-domain text-to-SQL datasets as clients to form a realistic heterogeneous FL setting and collaboratively train a global model. As standard FL algorithms suffer from the high client heterogeneity in our realistic setup, we further propose a novel LOss Reduction Adjusted Re-weighting (Lorar) mechanism to mitigate the performance degradation, which adjusts each client's contribution to the global model update based on its training loss reduction during each round. Our intuition is that the larger the loss reduction, the further aw",
    "link": "http://arxiv.org/abs/2305.17221",
    "context": "Title: Federated Learning for Semantic Parsing: Task Formulation, Evaluation Setup, New Algorithms. (arXiv:2305.17221v1 [cs.CL])\nAbstract: This paper studies a new task of federated learning (FL) for semantic parsing, where multiple clients collaboratively train one global model without sharing their semantic parsing data. By leveraging data from multiple clients, the FL paradigm can be especially beneficial for clients that have little training data to develop a data-hungry neural semantic parser on their own. We propose an evaluation setup to study this task, where we re-purpose widely-used single-domain text-to-SQL datasets as clients to form a realistic heterogeneous FL setting and collaboratively train a global model. As standard FL algorithms suffer from the high client heterogeneity in our realistic setup, we further propose a novel LOss Reduction Adjusted Re-weighting (Lorar) mechanism to mitigate the performance degradation, which adjusts each client's contribution to the global model update based on its training loss reduction during each round. Our intuition is that the larger the loss reduction, the further aw",
    "path": "papers/23/05/2305.17221.json",
    "total_tokens": 1121,
    "translated_title": "基于联邦学习的语义解析任务：任务形式，评估设置及新算法",
    "translated_abstract": "本文研究了一种新的联邦学习任务，即针对语义解析的联邦学习，多个客户端共同训练一个全局模型，而无需共享其语义分析数据。通过利用多个客户端的数据，联邦学习模式对于那些没有足够训练数据来开发一个数据饥饿的神经语义分析器的客户端尤其有益。我们提出了一种评估设置来研究这个任务，将广泛使用的单域文本到SQL数据集作为客户端来形成一个现实的异构联邦学习设置，并协同训练一个全局模型。由于我们的现实设置中客户群的异质性很高，标准的联邦学习算法会受到影响，所以我们进一步提出了一种新的机制LOss Reduction Adjusted Re-weighting (Lorar)来缓解性能下降，该机制基于客户端每轮训练损失的减少情况来调节每个客户端对于全局模型更新的贡献。我们的直觉是，损失减少的越多，客户端离全局最优解就越远，其对模型更新的贡献就应该越高。同时，我们还提出了一个针对异构文本到SQL FL设置的新的FL算法FedSQL。我们的实验表明，FedSQL和Lorar显著优于现有的FL算法和我们提出的FL设置中的强基线。",
    "tldr": "本文研究了基于联邦学习的语义解析任务，提出了评估设置和新算法。实验表明，新算法FedSQL和Lorar优于现有的FL算法和我们提出的设置的强基线。",
    "en_tdlr": "This paper proposes a new task of federated learning for semantic parsing and evaluates it using a realistic heterogeneous setting. The proposed algorithms, Lorar and FedSQL, outperform state-of-the-art FL algorithms and strong baselines in the proposed setup."
}