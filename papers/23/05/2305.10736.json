{
    "title": "Counterfactual Debiasing for Generating Factually Consistent Text Summaries. (arXiv:2305.10736v1 [cs.CL])",
    "abstract": "Despite substantial progress in abstractive text summarization to generate fluent and informative texts, the factual inconsistency in the generated summaries remains an important yet challenging problem to be solved. In this paper, we construct causal graphs for abstractive text summarization and identify the intrinsic causes of the factual inconsistency, i.e., the language bias and irrelevancy bias, and further propose a debiasing framework, named CoFactSum, to alleviate the causal effects of these biases by counterfactual estimation. Specifically, the proposed CoFactSum provides two counterfactual estimation strategies, i.e., Explicit Counterfactual Masking with an explicit dynamic masking strategy, and Implicit Counterfactual Training with an implicit discriminative cross-attention mechanism. Meanwhile, we design a Debiasing Degree Adjustment mechanism to dynamically adapt the debiasing degree at each decoding step. Extensive experiments on two widely-used summarization datasets dem",
    "link": "http://arxiv.org/abs/2305.10736",
    "context": "Title: Counterfactual Debiasing for Generating Factually Consistent Text Summaries. (arXiv:2305.10736v1 [cs.CL])\nAbstract: Despite substantial progress in abstractive text summarization to generate fluent and informative texts, the factual inconsistency in the generated summaries remains an important yet challenging problem to be solved. In this paper, we construct causal graphs for abstractive text summarization and identify the intrinsic causes of the factual inconsistency, i.e., the language bias and irrelevancy bias, and further propose a debiasing framework, named CoFactSum, to alleviate the causal effects of these biases by counterfactual estimation. Specifically, the proposed CoFactSum provides two counterfactual estimation strategies, i.e., Explicit Counterfactual Masking with an explicit dynamic masking strategy, and Implicit Counterfactual Training with an implicit discriminative cross-attention mechanism. Meanwhile, we design a Debiasing Degree Adjustment mechanism to dynamically adapt the debiasing degree at each decoding step. Extensive experiments on two widely-used summarization datasets dem",
    "path": "papers/23/05/2305.10736.json",
    "total_tokens": 961,
    "translated_title": "对生成事实一致性文本摘要进行反事实偏差校正",
    "translated_abstract": "尽管在生成流畅且信息丰富的文本摘要方面已经取得了实质性的进展，但所生成摘要的事实不一致性仍然是一个重要而具有挑战性的问题。本文针对抽象文本摘要构建因果图，并确定了造成事实不一致性的内在原因，即语言偏见和无关性偏见，并进一步提出了一个名为CoFactSum的去偏框架，通过反事实估计减轻这些偏差的因果影响。具体而言，所提出的CoFactSum提供了两种反事实估计策略，即明确反事实遮蔽具有明确的动态遮蔽策略，并采用隐式鉴别交叉关注机制的隐式反事实训练。同时，我们设计了一个去偏度调整机制，在每个解码步骤动态适应去偏程度。对两个广泛使用的摘要数据集的广泛实验证明了CoFactSum有效地减少了事实不一致性，并优于现有的最新的摘要模型。",
    "tldr": "本文提出了一个名为CoFactSum的去偏框架，通过反事实估计减轻原因，解决了生成文本摘要的事实不一致性问题，并在两个广泛使用的数据集上取得了优于现有模型的效果。",
    "en_tdlr": "The paper proposes a debiasing framework called CoFactSum to alleviate the causal effects of language and irrelevancy biases in abstractive text summarization, achieving reduced factual inconsistency and outperforming state-of-the-art models on two widely-used summarization datasets."
}