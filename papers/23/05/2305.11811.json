{
    "title": "Monte-Carlo Search for an Equilibrium in Dec-POMDPs. (arXiv:2305.11811v1 [cs.AI])",
    "abstract": "Decentralized partially observable Markov decision processes (Dec-POMDPs) formalize the problem of designing individual controllers for a group of collaborative agents under stochastic dynamics and partial observability. Seeking a global optimum is difficult (NEXP complete), but seeking a Nash equilibrium -- each agent policy being a best response to the other agents -is more accessible, and allowed addressing infinite-horizon problems with solutions in the form of finite state controllers. In this paper, we show that this approach can be adapted to cases where only a generative model (a simulator) of the Dec-POMDP is available. This requires relying on a simulation-based POMDP solver to construct an agent's FSC node by node. A related process is used to heuristically derive initial FSCs. Experiment with benchmarks shows that MC-JESP is competitive with exisiting Dec-POMDP solvers, even better than many offline methods using explicit models.",
    "link": "http://arxiv.org/abs/2305.11811",
    "context": "Title: Monte-Carlo Search for an Equilibrium in Dec-POMDPs. (arXiv:2305.11811v1 [cs.AI])\nAbstract: Decentralized partially observable Markov decision processes (Dec-POMDPs) formalize the problem of designing individual controllers for a group of collaborative agents under stochastic dynamics and partial observability. Seeking a global optimum is difficult (NEXP complete), but seeking a Nash equilibrium -- each agent policy being a best response to the other agents -is more accessible, and allowed addressing infinite-horizon problems with solutions in the form of finite state controllers. In this paper, we show that this approach can be adapted to cases where only a generative model (a simulator) of the Dec-POMDP is available. This requires relying on a simulation-based POMDP solver to construct an agent's FSC node by node. A related process is used to heuristically derive initial FSCs. Experiment with benchmarks shows that MC-JESP is competitive with exisiting Dec-POMDP solvers, even better than many offline methods using explicit models.",
    "path": "papers/23/05/2305.11811.json",
    "total_tokens": 884,
    "translated_title": "Dec-POMDP中的Monte Carlo搜索算法寻找均衡",
    "translated_abstract": "去中心化部分可观察马尔可夫决策过程（Dec-POMDPs）正式表述了在随机动态和部分可观测性下为一组协作代理设计各自控制器的问题。寻求全局最优解是困难的（NEXP完全），但寻求纳什均衡 - 每个代理策略都是对其他代理的最佳响应问题 - 更容易，同时允许以有限状态控制器的形式解决无限地平线问题。在这篇论文中，我们展示了这种方法可以适用于仅可用Dec-POMDP的生成模型（模拟器）的情况。这需要依靠基于模拟的POMDP求解器逐个节点地构建代理的FSC节点。相关过程用于启发式地导出初始的FSC。基准实验表明，MC-JESP与现有的Dec-POMDP求解器竞争力强，甚至比使用显式模型的许多离线方法更好。",
    "tldr": "本文在Dec-POMDP领域使用了Monte Carlo搜索算法来寻找纳什均衡，最终的基准实验表明MC-JESP可以有效解决控制器问题并达到预期效果。",
    "en_tdlr": "This paper uses Monte Carlo search algorithm to find Nash equilibrium in the field of Dec-POMDP, and the benchmark experiments show that MC-JESP can effectively solve the problem of controllers and achieve the expected results."
}