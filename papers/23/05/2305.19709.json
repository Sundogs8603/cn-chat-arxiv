{
    "title": "XPhoneBERT: A Pre-trained Multilingual Model for Phoneme Representations for Text-to-Speech. (arXiv:2305.19709v1 [cs.CL])",
    "abstract": "We present XPhoneBERT, the first multilingual model pre-trained to learn phoneme representations for the downstream text-to-speech (TTS) task. Our XPhoneBERT has the same model architecture as BERT-base, trained using the RoBERTa pre-training approach on 330M phoneme-level sentences from nearly 100 languages and locales. Experimental results show that employing XPhoneBERT as an input phoneme encoder significantly boosts the performance of a strong neural TTS model in terms of naturalness and prosody and also helps produce fairly high-quality speech with limited training data. We publicly release our pre-trained XPhoneBERT with the hope that it would facilitate future research and downstream TTS applications for multiple languages. Our XPhoneBERT model is available at https://github.com/VinAIResearch/XPhoneBERT",
    "link": "http://arxiv.org/abs/2305.19709",
    "context": "Title: XPhoneBERT: A Pre-trained Multilingual Model for Phoneme Representations for Text-to-Speech. (arXiv:2305.19709v1 [cs.CL])\nAbstract: We present XPhoneBERT, the first multilingual model pre-trained to learn phoneme representations for the downstream text-to-speech (TTS) task. Our XPhoneBERT has the same model architecture as BERT-base, trained using the RoBERTa pre-training approach on 330M phoneme-level sentences from nearly 100 languages and locales. Experimental results show that employing XPhoneBERT as an input phoneme encoder significantly boosts the performance of a strong neural TTS model in terms of naturalness and prosody and also helps produce fairly high-quality speech with limited training data. We publicly release our pre-trained XPhoneBERT with the hope that it would facilitate future research and downstream TTS applications for multiple languages. Our XPhoneBERT model is available at https://github.com/VinAIResearch/XPhoneBERT",
    "path": "papers/23/05/2305.19709.json",
    "total_tokens": 814,
    "translated_title": "XPhoneBERT: 一种用于文本转语音的预训练多语言语音单位表示模型",
    "translated_abstract": "我们提出了XPhoneBERT，这是第一个预训练的多语言模型，用于学习下游文本转语音（TTS）任务的语音单位表示。我们的XPhoneBERT具有与BERT-base相同的模型架构，使用RoBERTa预训练方法在将近100种语言和语境中的330M个语音单位级句子上进行训练。实验结果表明，将XPhoneBERT作为输入语音编码器显著提升了强神经网络TTS模型的自然度和语调，并且在有限的训练数据下也有较高的语音质量。我们公开发布了我们的预训练XPhoneBERT，希望它能促进未来的多种语言TTS应用的研究。我们的XPhoneBERT模型可在https://github.com/VinAIResearch/XPhoneBERT获得。",
    "tldr": "XPhoneBERT是第一个用于文本转语音的预训练多语言语音单位表示模型，通过其作为输入语音编码器，可以显著提高TTS模型的性能和语音质量。",
    "en_tdlr": "XPhoneBERT is the first pre-trained multilingual model focused on learning phoneme representations for text-to-speech task, which significantly improves the performance and naturalness of TTS model by using it as input phoneme encoder."
}