{
    "title": "EnCore: Fine-Grained Entity Typing by Pre-Training Entity Encoders on Coreference Chains. (arXiv:2305.12924v2 [cs.CL] UPDATED)",
    "abstract": "Entity typing is the task of assigning semantic types to the entities that are mentioned in a text. In the case of fine-grained entity typing (FET), a large set of candidate type labels is considered. Since obtaining sufficient amounts of manual annotations is then prohibitively expensive, FET models are typically trained using distant supervision. In this paper, we propose to improve on this process by pre-training an entity encoder such that embeddings of coreferring entities are more similar to each other than to the embeddings of other entities. The main problem with this strategy, which helps to explain why it has not previously been considered, is that predicted coreference links are often too noisy. We show that this problem can be addressed by using a simple trick: we only consider coreference links that are predicted by two different off-the-shelf systems. With this prudent use of coreference links, our pre-training strategy allows us to improve the state-of-the-art in benchma",
    "link": "http://arxiv.org/abs/2305.12924",
    "context": "Title: EnCore: Fine-Grained Entity Typing by Pre-Training Entity Encoders on Coreference Chains. (arXiv:2305.12924v2 [cs.CL] UPDATED)\nAbstract: Entity typing is the task of assigning semantic types to the entities that are mentioned in a text. In the case of fine-grained entity typing (FET), a large set of candidate type labels is considered. Since obtaining sufficient amounts of manual annotations is then prohibitively expensive, FET models are typically trained using distant supervision. In this paper, we propose to improve on this process by pre-training an entity encoder such that embeddings of coreferring entities are more similar to each other than to the embeddings of other entities. The main problem with this strategy, which helps to explain why it has not previously been considered, is that predicted coreference links are often too noisy. We show that this problem can be addressed by using a simple trick: we only consider coreference links that are predicted by two different off-the-shelf systems. With this prudent use of coreference links, our pre-training strategy allows us to improve the state-of-the-art in benchma",
    "path": "papers/23/05/2305.12924.json",
    "total_tokens": 845,
    "translated_title": "EnCore:通过在指代链上预训练实体编码器来进行精细粒度实体类型标注",
    "translated_abstract": "实体类型标注是将语义类型分配给文本中提到的实体的任务。在细粒度实体类型标注（FET）的情况下，考虑了一个大型的候选类型标签集合。由于获取足够的手动注释往往成本高昂，因此FET模型通常使用远程监督进行训练。在本文中，我们提出通过预训练实体编码器，使得共指实体的嵌入更相似于彼此，而不是其他实体，来改进这个过程。这种策略的主要问题是，预测的共指链接通常存在噪声。我们证明通过使用一个简单的技巧可以解决该问题：我们只考虑由两个不同的现成系统预测的共指链接。通过谨慎使用共指链接，我们的预训练策略使我们能够改进基准测试结果",
    "tldr": "本文提出了EnCore模型，通过预训练实体编码器并使用谨慎筛选的共指链接，提高了细粒度实体类型标注的性能。",
    "en_tdlr": "This paper proposes the EnCore model, which improves the performance of fine-grained entity typing by pre-training entity encoders and using carefully selected coreference links."
}