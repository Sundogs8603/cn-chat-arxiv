{
    "title": "Interpretable multimodal sentiment analysis based on textual modality descriptions by using large-scale language models. (arXiv:2305.06162v1 [cs.CL])",
    "abstract": "Multimodal sentiment analysis is an important area for understanding the user's internal states. Deep learning methods were effective, but the problem of poor interpretability has gradually gained attention. Previous works have attempted to use attention weights or vector distributions to provide interpretability. However, their explanations were not intuitive and can be influenced by different trained models. This study proposed a novel approach to provide interpretability by converting nonverbal modalities into text descriptions and by using large-scale language models for sentiment predictions. This provides an intuitive approach to directly interpret what models depend on with respect to making decisions from input texts, thus significantly improving interpretability. Specifically, we convert descriptions based on two feature patterns for the audio modality and discrete action units for the facial modality. Experimental results on two sentiment analysis tasks demonstrated that the ",
    "link": "http://arxiv.org/abs/2305.06162",
    "context": "Title: Interpretable multimodal sentiment analysis based on textual modality descriptions by using large-scale language models. (arXiv:2305.06162v1 [cs.CL])\nAbstract: Multimodal sentiment analysis is an important area for understanding the user's internal states. Deep learning methods were effective, but the problem of poor interpretability has gradually gained attention. Previous works have attempted to use attention weights or vector distributions to provide interpretability. However, their explanations were not intuitive and can be influenced by different trained models. This study proposed a novel approach to provide interpretability by converting nonverbal modalities into text descriptions and by using large-scale language models for sentiment predictions. This provides an intuitive approach to directly interpret what models depend on with respect to making decisions from input texts, thus significantly improving interpretability. Specifically, we convert descriptions based on two feature patterns for the audio modality and discrete action units for the facial modality. Experimental results on two sentiment analysis tasks demonstrated that the ",
    "path": "papers/23/05/2305.06162.json",
    "total_tokens": 844,
    "translated_abstract": "多模态情感分析是理解用户内在状态的重要领域。深度学习方法有效，但可解释性差的问题逐渐受到关注。先前的研究尝试使用注意力权重或向量分布提供可解释性。但是，这些解释不够直观且易受不同训练模型的影响。本研究提出了一种新方法，通过将非言语模态转换为文本描述并使用大规模语言模型进行情感预测来提供可解释性。这提供了一种直观的方法来直接解释模型在决策输入文本时依赖于什么，从而显着提高了可解释性。具体地，我们基于音频模态的两种特征模式和基于面部模态的离散动作单元进行转换描述。两个情感分析任务的实验结果证明了提出方法的有效性。",
    "tldr": "本论文提出了一种新方法，通过将非言语模态转换为文本描述并使用大规模语言模型进行情感预测来提高多模态情感分析的可解释性。实验结果证明了该方法的有效性。",
    "en_tdlr": "This paper proposes a new approach to improve the interpretability of multimodal sentiment analysis by converting nonverbal modalities into text descriptions and using large-scale language models for sentiment predictions. Experimental results demonstrate the effectiveness of this approach."
}