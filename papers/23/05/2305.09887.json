{
    "title": "Simplifying Distributed Neural Network Training on Massive Graphs: Randomized Partitions Improve Model Aggregation. (arXiv:2305.09887v1 [cs.LG])",
    "abstract": "Distributed training of GNNs enables learning on massive graphs (e.g., social and e-commerce networks) that exceed the storage and computational capacity of a single machine. To reach performance comparable to centralized training, distributed frameworks focus on maximally recovering cross-instance node dependencies with either communication across instances or periodic fallback to centralized training, which create overhead and limit the framework scalability. In this work, we present a simplified framework for distributed GNN training that does not rely on the aforementioned costly operations, and has improved scalability, convergence speed and performance over the state-of-the-art approaches. Specifically, our framework (1) assembles independent trainers, each of which asynchronously learns a local model on locally-available parts of the training graph, and (2) only conducts periodic (time-based) model aggregation to synchronize the local models. Backed by our theoretical analysis, ",
    "link": "http://arxiv.org/abs/2305.09887",
    "context": "Title: Simplifying Distributed Neural Network Training on Massive Graphs: Randomized Partitions Improve Model Aggregation. (arXiv:2305.09887v1 [cs.LG])\nAbstract: Distributed training of GNNs enables learning on massive graphs (e.g., social and e-commerce networks) that exceed the storage and computational capacity of a single machine. To reach performance comparable to centralized training, distributed frameworks focus on maximally recovering cross-instance node dependencies with either communication across instances or periodic fallback to centralized training, which create overhead and limit the framework scalability. In this work, we present a simplified framework for distributed GNN training that does not rely on the aforementioned costly operations, and has improved scalability, convergence speed and performance over the state-of-the-art approaches. Specifically, our framework (1) assembles independent trainers, each of which asynchronously learns a local model on locally-available parts of the training graph, and (2) only conducts periodic (time-based) model aggregation to synchronize the local models. Backed by our theoretical analysis, ",
    "path": "papers/23/05/2305.09887.json",
    "total_tokens": 1086,
    "translated_title": "简化大规模图上分布式神经网络训练：随机分割改善模型聚合",
    "translated_abstract": "分布式 GNN 的训练可以使我们学习超出单个机器存储和计算能力的大规模图（如社交和电子商务网络）。为了达到与集中式训练相当的性能，分布式框架专注于通过实例间通信或定期回退到集中式训练来最大限度地恢复跨实例节点的依赖关系，这需要额外的开销并限制了框架的可扩展性。本文提出了一个简化的分布式 GNN 训练框架，不需要以上昂贵的操作，具有比现有方法更好的可扩展性、收敛速度和性能。具体而言，我们的框架（1）组装独立的训练器，每个训练器异步地在训练图的本地部分上学习本地模型，（2）只进行定期的（基于时间的）模型聚合，以同步各个本地模型。经过理论分析支持，我们表明，随机图分割使得独立的训练器能够隐式地交换信息，而无需显式通信，因此定期聚合就足以收敛到与集中式训练相同的性能。我们的方法简单、高效，并且适用于各种 GNN 模型（例如 GCN、GAT 等），使其非常适用于实际的分布式 GNN 应用。",
    "tldr": "本论文提出了一种简化的分布式 GNN 训练框架，只进行定期的模型聚合，并使用随机分割来隐式交换信息，实现了比现有方法更好的可扩展性、收敛速度和性能。",
    "en_tdlr": "This paper proposes a simplified framework for distributed GNN training that only conducts periodic model aggregation and implicitly exchanges information using random graph partitions, achieving better scalability, convergence speed, and performance than existing methods."
}