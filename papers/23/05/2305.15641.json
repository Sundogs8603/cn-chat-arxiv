{
    "title": "A Robust Classifier Under Missing-Not-At-Random Sample Selection Bias. (arXiv:2305.15641v1 [cs.LG])",
    "abstract": "The shift between the training and testing distributions is commonly due to sample selection bias, a type of bias caused by non-random sampling of examples to be included in the training set. Although there are many approaches proposed to learn a classifier under sample selection bias, few address the case where a subset of labels in the training set are missing-not-at-random (MNAR) as a result of the selection process. In statistics, Greene's method formulates this type of sample selection with logistic regression as the prediction model. However, we find that simply integrating this method into a robust classification framework is not effective for this bias setting. In this paper, we propose BiasCorr, an algorithm that improves on Greene's method by modifying the original training set in order for a classifier to learn under MNAR sample selection bias. We provide theoretical guarantee for the improvement of BiasCorr over Greene's method by analyzing its bias. Experimental results on",
    "link": "http://arxiv.org/abs/2305.15641",
    "context": "Title: A Robust Classifier Under Missing-Not-At-Random Sample Selection Bias. (arXiv:2305.15641v1 [cs.LG])\nAbstract: The shift between the training and testing distributions is commonly due to sample selection bias, a type of bias caused by non-random sampling of examples to be included in the training set. Although there are many approaches proposed to learn a classifier under sample selection bias, few address the case where a subset of labels in the training set are missing-not-at-random (MNAR) as a result of the selection process. In statistics, Greene's method formulates this type of sample selection with logistic regression as the prediction model. However, we find that simply integrating this method into a robust classification framework is not effective for this bias setting. In this paper, we propose BiasCorr, an algorithm that improves on Greene's method by modifying the original training set in order for a classifier to learn under MNAR sample selection bias. We provide theoretical guarantee for the improvement of BiasCorr over Greene's method by analyzing its bias. Experimental results on",
    "path": "papers/23/05/2305.15641.json",
    "total_tokens": 951,
    "translated_title": "一种在缺失非随机样本选择偏差下的鲁棒分类器",
    "translated_abstract": "训练和测试分布之间的偏移通常是由于样本选择偏差造成的，这是一种由于样本非随机抽样而导致的偏差，包括在训练集中的示例。尽管有许多方法用于在样本选择偏差下学习分类器，但很少涉及在训练集中的子集标签缺失是由于选择过程的缺失非随机性。在统计学中，格林方法利用逻辑回归作为预测模型来表示这种类型的样本选择。然而，我们发现将这种方法简单地集成到鲁棒分类框架中对于这种偏差设置并不有效。在本文中，我们提出了BiasCorr算法，通过修改原始训练集来使分类器在缺失非随机样本选择偏差下进行学习。我们通过分析偏差提供了BiasCorr相对于Greene方法的改进的理论保证。实验结果表明，BiasCorr在MNAR样本选择偏差下表现更好。",
    "tldr": "该论文介绍了一种名为BiasCorr的算法，它可以在训练集中的子集标签缺失是由于选择过程的缺失非随机性的情况下，通过修改原始训练集使分类器在缺失非随机样本选择偏差下进行学习。",
    "en_tdlr": "The paper proposes a new algorithm, named BiasCorr, that modifies the original training set to enable a classifier to learn under missing-not-at-random (MNAR) sample selection bias, which is caused by non-random sampling of examples to be included in the training set. The algorithm provides a theoretical guarantee for improvement over previous methods and performs better in experimental results."
}