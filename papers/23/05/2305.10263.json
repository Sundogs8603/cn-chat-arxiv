{
    "title": "M3KE: A Massive Multi-Level Multi-Subject Knowledge Evaluation Benchmark for Chinese Large Language Models. (arXiv:2305.10263v1 [cs.CL])",
    "abstract": "Large language models have recently made tremendous progress in a variety of aspects, e.g., cross-task generalization, instruction following. Comprehensively evaluating the capability of large language models in multiple tasks is of great importance. In this paper, we propose M3KE, a Massive Multi-Level Multi-Subject Knowledge Evaluation benchmark, which is developed to measure knowledge acquired by Chinese large language models by testing their multitask accuracy in zero- and few-shot settings. We have collected 20,477 questions from 71 tasks. Our selection covers all major levels of Chinese education system, ranging from the primary school to college, as well as a wide variety of subjects, including humanities, history, politics, law, education, psychology, science, technology, art and religion. All questions are multiple-choice questions with four options, hence guaranteeing a standardized and unified assessment process. We've assessed a number of state-of-the-art open-source Chines",
    "link": "http://arxiv.org/abs/2305.10263",
    "context": "Title: M3KE: A Massive Multi-Level Multi-Subject Knowledge Evaluation Benchmark for Chinese Large Language Models. (arXiv:2305.10263v1 [cs.CL])\nAbstract: Large language models have recently made tremendous progress in a variety of aspects, e.g., cross-task generalization, instruction following. Comprehensively evaluating the capability of large language models in multiple tasks is of great importance. In this paper, we propose M3KE, a Massive Multi-Level Multi-Subject Knowledge Evaluation benchmark, which is developed to measure knowledge acquired by Chinese large language models by testing their multitask accuracy in zero- and few-shot settings. We have collected 20,477 questions from 71 tasks. Our selection covers all major levels of Chinese education system, ranging from the primary school to college, as well as a wide variety of subjects, including humanities, history, politics, law, education, psychology, science, technology, art and religion. All questions are multiple-choice questions with four options, hence guaranteeing a standardized and unified assessment process. We've assessed a number of state-of-the-art open-source Chines",
    "path": "papers/23/05/2305.10263.json",
    "total_tokens": 1135,
    "translated_title": "M3KE:一种面向中国大语言模型的大规模多级多主题知识评估基准",
    "translated_abstract": "大型语言模型最近在各个方面取得了巨大的进展，例如跨任务通用性，指令遵循等。全面评估大语言模型在多个任务中的能力非常重要。在本文中，我们提出了M3KE，一种大规模多级多主题知识评估基准，旨在通过测试零和几个示例设置下的多任务准确性来衡量中文大语言模型所获得的知识。我们收集了71个任务的20,477个问题。选择涵盖了中国教育体系的所有主要层次，从小学到大学，以及广泛的学科，包括人文，历史，政治，法律，教育，心理，科学，技术，艺术和宗教。所有问题都是四个选项的多选题，因此保证了标准化和统一的评估流程。我们使用我们的基准测试了一些最先进的开源中文大语言模型，包括GPT-2，RoBERTa，ERNIE和ELECTRA，并提供了详细的结果和分析。我们展示了M3KE可以有效地评估大型语言模型，并全面了解它们整合和利用多个知识来源的能力。",
    "tldr": "本文提出了一种面向中国大语言模型的大规模多级多主题知识评估基准M3KE，收集了20,477个问题以覆盖中国教育体系的所有主要层次和广泛的学科，使用多任务准确性测试法有效地评估了四个大语言模型GPT-2，RoBERTa，ERNIE和ELECTRA对多源知识的整合和利用能力。",
    "en_tdlr": "This paper proposes a Massive Multi-Level Multi-Subject Knowledge Evaluation benchmark, M3KE, for Chinese large language models, which comprehensively evaluates their ability to incorporate and utilize multiple sources of knowledge. The benchmark includes 20,477 questions covering all major levels of Chinese education system and a wide variety of subjects. Four state-of-the-art open-source Chinese language models have been assessed, and the benchmark proves effective in evaluating their multi-task accuracy in zero- and few-shot settings."
}