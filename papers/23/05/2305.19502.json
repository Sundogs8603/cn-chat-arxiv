{
    "title": "Graph Entropy Minimization for Semi-supervised Node Classification. (arXiv:2305.19502v1 [cs.LG])",
    "abstract": "Node classifiers are required to comprehensively reduce prediction errors, training resources, and inference latency in the industry. However, most graph neural networks (GNN) concentrate only on one or two of them. The compromised aspects thus are the shortest boards on the bucket, hindering their practical deployments for industrial-level tasks. This work proposes a novel semi-supervised learning method termed Graph Entropy Minimization (GEM) to resolve the three issues simultaneously. GEM benefits its one-hop aggregation from massive uncategorized nodes, making its prediction accuracy comparable to GNNs with two or more hops message passing. It can be decomposed to support stochastic training with mini-batches of independent edge samples, achieving extremely fast sampling and space-saving training. While its one-hop aggregation is faster in inference than deep GNNs, GEM can be further accelerated to an extreme by deriving a non-hop classifier via online knowledge distillation. Thus,",
    "link": "http://arxiv.org/abs/2305.19502",
    "context": "Title: Graph Entropy Minimization for Semi-supervised Node Classification. (arXiv:2305.19502v1 [cs.LG])\nAbstract: Node classifiers are required to comprehensively reduce prediction errors, training resources, and inference latency in the industry. However, most graph neural networks (GNN) concentrate only on one or two of them. The compromised aspects thus are the shortest boards on the bucket, hindering their practical deployments for industrial-level tasks. This work proposes a novel semi-supervised learning method termed Graph Entropy Minimization (GEM) to resolve the three issues simultaneously. GEM benefits its one-hop aggregation from massive uncategorized nodes, making its prediction accuracy comparable to GNNs with two or more hops message passing. It can be decomposed to support stochastic training with mini-batches of independent edge samples, achieving extremely fast sampling and space-saving training. While its one-hop aggregation is faster in inference than deep GNNs, GEM can be further accelerated to an extreme by deriving a non-hop classifier via online knowledge distillation. Thus,",
    "path": "papers/23/05/2305.19502.json",
    "total_tokens": 1006,
    "translated_title": "图熵最小化用于半监督节点分类",
    "translated_abstract": "工业领域需要综合降低预测误差、训练资源和推理延迟，而节点分类器实现这些方面往往只关注其中一两个方面。因此，妥协的方面成为最短的短板，阻碍了它们在工业级任务中的实际部署。本研究提出了一种新的半监督学习方法，称为图熵最小化（GEM），可同时解决这三个问题。GEM从大量未分类节点中受益于其一跳聚合，使其预测精度与具有两个或更多跳消息传递的GNN相当。它可以被分解为支持最小边缘独立样本的随机训练，实现极快的采样和节省空间的训练。虽然它的一跳聚合在推理中比深度GNN快，但通过在线知识蒸馏可以进一步加速到极致，从而得到非跳节点分类器。",
    "tldr": "本研究提出一种名为图熵最小化的半监督学习方法，可同时解决预测误差、训练资源和推理延迟等三个问题。其采用从大量未分类节点中的一跳聚合进行预测，使得其预测精度与具有两个或更多跳消息传递的GNN相当。而且它还支持随机训练和在线知识蒸馏等加速技术，提高了性能和效率。",
    "en_tdlr": "This study proposes a novel semi-supervised learning method called Graph Entropy Minimization (GEM), which can simultaneously solve the three problems of prediction error, training resources, and inference latency. Its one-hop aggregation for prediction from massive uncategorized nodes achieves comparable accuracy as GNNs with more hops. Furthermore, it supports acceleration techniques such as stochastic training and online knowledge distillation, improving both performance and efficiency."
}