{
    "title": "A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks. (arXiv:2305.11073v1 [cs.CL])",
    "abstract": "Conformer, a convolution-augmented Transformer variant, has become the de facto encoder architecture for speech processing due to its superior performance in various tasks, including automatic speech recognition (ASR), speech translation (ST) and spoken language understanding (SLU). Recently, a new encoder called E-Branchformer has outperformed Conformer in the LibriSpeech ASR benchmark, making it promising for more general speech applications. This work compares E-Branchformer and Conformer through extensive experiments using different types of end-to-end sequence-to-sequence models. Results demonstrate that E-Branchformer achieves comparable or better performance than Conformer in almost all evaluation sets across 15 ASR, 2 ST, and 3 SLU benchmarks, while being more stable during training. We will release our training configurations and pre-trained models for reproducibility, which can benefit the speech community.",
    "link": "http://arxiv.org/abs/2305.11073",
    "context": "Title: A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks. (arXiv:2305.11073v1 [cs.CL])\nAbstract: Conformer, a convolution-augmented Transformer variant, has become the de facto encoder architecture for speech processing due to its superior performance in various tasks, including automatic speech recognition (ASR), speech translation (ST) and spoken language understanding (SLU). Recently, a new encoder called E-Branchformer has outperformed Conformer in the LibriSpeech ASR benchmark, making it promising for more general speech applications. This work compares E-Branchformer and Conformer through extensive experiments using different types of end-to-end sequence-to-sequence models. Results demonstrate that E-Branchformer achieves comparable or better performance than Conformer in almost all evaluation sets across 15 ASR, 2 ST, and 3 SLU benchmarks, while being more stable during training. We will release our training configurations and pre-trained models for reproducibility, which can benefit the speech community.",
    "path": "papers/23/05/2305.11073.json",
    "total_tokens": 884,
    "translated_title": "电子分支变形器与变形器在语音识别、翻译和理解任务中的比较研究",
    "translated_abstract": "Conformer是一种卷积变换器，由于在自动语音识别（ASR）、语音翻译（ST）和口语理解（SLU）等各种任务中表现出优异性能，已经成为语音处理的事实标准编码器架构。最近，一种名为E-Branchformer的新编码器在LibriSpeech ASR基准测试中超越了Conformer，使其在更普遍的语音应用中变得有前途。本研究通过使用不同类型的端到端序列到序列模型进行广泛实验，比较了E-Branchformer和Conformer。结果表明，在15个ASR、2个ST和3个SLU基准测试的几乎所有评估集中，E-Branchformer的表现与Conformer相当甚至更好，同时在训练过程中更为稳定。我们将发布我们的训练配置和预训练模型以实现可重复性，从中受益的将是语音社区。",
    "tldr": "本研究比较了E-Branchformer和Conformer在语音识别、翻译和理解任务中的性能，结果表明E-Branchformer在多个基准测试中表现相当甚至更好，并且更加稳定。",
    "en_tdlr": "This study compares E-Branchformer and Conformer in speech recognition, translation, and understanding tasks, showing that E-Branchformer performs comparably or better than Conformer in multiple benchmarks and is more stable during training."
}