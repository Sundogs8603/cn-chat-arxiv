{
    "title": "A Survey on Offline Model-Based Reinforcement Learning. (arXiv:2305.03360v1 [cs.LG])",
    "abstract": "Model-based approaches are becoming increasingly popular in the field of offline reinforcement learning, with high potential in real-world applications due to the model's capability of thoroughly utilizing the large historical datasets available with supervised learning techniques. This paper presents a literature review of recent work in offline model-based reinforcement learning, a field that utilizes model-based approaches in offline reinforcement learning. The survey provides a brief overview of the concepts and recent developments in both offline reinforcement learning and model-based reinforcement learning, and discuss the intersection of the two fields. We then presents key relevant papers in the field of offline model-based reinforcement learning and discuss their methods, particularly their approaches in solving the issue of distributional shift, the main problem faced by all current offline model-based reinforcement learning methods. We further discuss key challenges faced by",
    "link": "http://arxiv.org/abs/2305.03360",
    "context": "Title: A Survey on Offline Model-Based Reinforcement Learning. (arXiv:2305.03360v1 [cs.LG])\nAbstract: Model-based approaches are becoming increasingly popular in the field of offline reinforcement learning, with high potential in real-world applications due to the model's capability of thoroughly utilizing the large historical datasets available with supervised learning techniques. This paper presents a literature review of recent work in offline model-based reinforcement learning, a field that utilizes model-based approaches in offline reinforcement learning. The survey provides a brief overview of the concepts and recent developments in both offline reinforcement learning and model-based reinforcement learning, and discuss the intersection of the two fields. We then presents key relevant papers in the field of offline model-based reinforcement learning and discuss their methods, particularly their approaches in solving the issue of distributional shift, the main problem faced by all current offline model-based reinforcement learning methods. We further discuss key challenges faced by",
    "path": "papers/23/05/2305.03360.json",
    "total_tokens": 956,
    "translated_title": "离线基于模型的强化学习综述",
    "translated_abstract": "基于模型的方法在离线强化学习领域变得越来越流行，由于模型能够充分利用监督学习技术中大规模历史数据集的优势，在实际应用中具有非常高的潜力。本文对目前离线基于模型的强化学习研究进行了文献综述。综述简要介绍了离线强化学习和基于模型的强化学习的概念和最新发展，并探讨了两个领域的交叉点。接着，本文介绍了离线基于模型的强化学习领域的关键文献，并讨论了它们的方法，特别是它们在解决分布变化问题方面的方法，这是当前所有离线基于模型的强化学习方法面临的主要问题。本文还进一步讨论了该领域面临的关键挑战。",
    "tldr": "本文是一篇关于离线基于模型的强化学习最新进展的综述性文章，讨论了该领域的最新概念、方法和挑战。文献综述介绍了当前离线基于模型的强化学习领域的关键文献，并重点讨论了如何解决分布变化问题。该领域面临的关键挑战也被讨论和总结。",
    "en_tdlr": "This paper is a survey on the latest progress of offline model-based reinforcement learning. It presents an overview of the concepts, methods, and challenges in both offline reinforcement learning and model-based reinforcement learning, and discusses the intersection of the two fields. The survey introduces key papers in the field of offline model-based reinforcement learning and focuses on discussing methods to solve the distributional shift problem, which is the main challenge for all current offline model-based reinforcement learning methods."
}