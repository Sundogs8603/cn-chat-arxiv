{
    "title": "DualCross: Cross-Modality Cross-Domain Adaptation for Monocular BEV Perception. (arXiv:2305.03724v1 [cs.CV])",
    "abstract": "Closing the domain gap between training and deployment and incorporating multiple sensor modalities are two challenging yet critical topics for self-driving. Existing work only focuses on single one of the above topics, overlooking the simultaneous domain and modality shift which pervasively exists in real-world scenarios. A model trained with multi-sensor data collected in Europe may need to run in Asia with a subset of input sensors available. In this work, we propose DualCross, a cross-modality cross-domain adaptation framework to facilitate the learning of a more robust monocular bird's-eye-view (BEV) perception model, which transfers the point cloud knowledge from a LiDAR sensor in one domain during the training phase to the camera-only testing scenario in a different domain. This work results in the first open analysis of cross-domain cross-sensor perception and adaptation for monocular 3D tasks in the wild. We benchmark our approach on large-scale datasets under a wide range of ",
    "link": "http://arxiv.org/abs/2305.03724",
    "context": "Title: DualCross: Cross-Modality Cross-Domain Adaptation for Monocular BEV Perception. (arXiv:2305.03724v1 [cs.CV])\nAbstract: Closing the domain gap between training and deployment and incorporating multiple sensor modalities are two challenging yet critical topics for self-driving. Existing work only focuses on single one of the above topics, overlooking the simultaneous domain and modality shift which pervasively exists in real-world scenarios. A model trained with multi-sensor data collected in Europe may need to run in Asia with a subset of input sensors available. In this work, we propose DualCross, a cross-modality cross-domain adaptation framework to facilitate the learning of a more robust monocular bird's-eye-view (BEV) perception model, which transfers the point cloud knowledge from a LiDAR sensor in one domain during the training phase to the camera-only testing scenario in a different domain. This work results in the first open analysis of cross-domain cross-sensor perception and adaptation for monocular 3D tasks in the wild. We benchmark our approach on large-scale datasets under a wide range of ",
    "path": "papers/23/05/2305.03724.json",
    "total_tokens": 888,
    "translated_title": "DualCross: 单目BEV感知的跨模态与跨领域自适应方法",
    "translated_abstract": "在自动驾驶中，缩小训练和部署之间的领域差距并结合多种传感器模态是两个具有挑战性但至关重要的问题。现有的工作只关注上述两个问题中的一个，忽视了实际场景中普遍存在的领域和模态的同时变化。在本文中，我们提出了DualCross，一个跨模态和跨领域的自适应框架，以促进学习更为稳健的单目鸟瞰感知模型，它可以在训练阶段从一个领域的LiDAR传感器中转移点云知识，在不同领域的仅摄像头的测试场景中。这项工作是第一次对单目3D任务的跨领域跨传感器感知和适应进行了开放性分析。我们在大规模数据集上对我们的方法进行了基准测试，覆盖了广泛的场景。",
    "tldr": "DualCross是一个跨模态和跨领域的自适应框架，旨在使单目BEV感知更加鲁棒，并实现了跨领域跨传感器感知和适应。",
    "en_tdlr": "DualCross is a cross-modality and cross-domain adaptation framework aimed at making monocular bird's-eye-view (BEV) perception more robust, and achieves cross-domain cross-sensor perception and adaptation for monocular 3D tasks in the wild."
}