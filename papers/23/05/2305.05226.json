{
    "title": "Multi-Teacher Knowledge Distillation For Text Image Machine Translation. (arXiv:2305.05226v1 [cs.CL])",
    "abstract": "Text image machine translation (TIMT) has been widely used in various real-world applications, which translates source language texts in images into another target language sentence. Existing methods on TIMT are mainly divided into two categories: the recognition-then-translation pipeline model and the end-to-end model. However, how to transfer knowledge from the pipeline model into the end-to-end model remains an unsolved problem. In this paper, we propose a novel Multi-Teacher Knowledge Distillation (MTKD) method to effectively distillate knowledge into the end-to-end TIMT model from the pipeline model. Specifically, three teachers are utilized to improve the performance of the end-to-end TIMT model. The image encoder in the end-to-end TIMT model is optimized with the knowledge distillation guidance from the recognition teacher encoder, while the sequential encoder and decoder are improved by transferring knowledge from the translation sequential and decoder teacher models. Furthermo",
    "link": "http://arxiv.org/abs/2305.05226",
    "context": "Title: Multi-Teacher Knowledge Distillation For Text Image Machine Translation. (arXiv:2305.05226v1 [cs.CL])\nAbstract: Text image machine translation (TIMT) has been widely used in various real-world applications, which translates source language texts in images into another target language sentence. Existing methods on TIMT are mainly divided into two categories: the recognition-then-translation pipeline model and the end-to-end model. However, how to transfer knowledge from the pipeline model into the end-to-end model remains an unsolved problem. In this paper, we propose a novel Multi-Teacher Knowledge Distillation (MTKD) method to effectively distillate knowledge into the end-to-end TIMT model from the pipeline model. Specifically, three teachers are utilized to improve the performance of the end-to-end TIMT model. The image encoder in the end-to-end TIMT model is optimized with the knowledge distillation guidance from the recognition teacher encoder, while the sequential encoder and decoder are improved by transferring knowledge from the translation sequential and decoder teacher models. Furthermo",
    "path": "papers/23/05/2305.05226.json",
    "total_tokens": 888,
    "translated_title": "文本图像机器翻译多教师知识蒸馏",
    "translated_abstract": "文本图像机器翻译（TIMT）已被广泛应用于各种实际应用程序中，它将图像中的源语言文本翻译成另一种目标语言句子。TIMT的现有方法主要分为两种类别：识别-然后-翻译流程模型和端到端模型。然而，如何从管道模型向端到端模型传递知识仍然是一个未解决的问题。在本文中，我们提出了一种新的多教师知识蒸馏（MTKD）方法，可以有效地将知识蒸馏到管道模型中并传递给端到端TIMT模型。具体而言，利用三个教师来提高端到端TIMT模型的性能。端到端TIMT模型中的图像编码器使用识别教师编码器的知识蒸馏指导进行优化，而顺序编码器和解码器则通过从翻译顺序和解码器教师模型传递知识来改善性能。",
    "tldr": "本文提出了一种多教师知识蒸馏方法，可以将知识有效地蒸馏到管道模型中并传递给端到端TIMT模型，从而提高性能。",
    "en_tdlr": "This paper proposes a novel Multi-Teacher Knowledge Distillation (MTKD) method for text image machine translation (TIMT), which effectively transfers knowledge from pipeline models to end-to-end models and improves the performance of the latter."
}