{
    "title": "Universal Approximation and the Topological Neural Network. (arXiv:2305.16639v1 [cs.LG])",
    "abstract": "A topological neural network (TNN), which takes data from a Tychonoff topological space instead of the usual finite dimensional space, is introduced. As a consequence, a distributional neural network (DNN) that takes Borel measures as data is also introduced. Combined these new neural networks facilitate things like recognizing long range dependence, heavy tails and other properties in stochastic process paths or like acting on belief states produced by particle filtering or hidden Markov model algorithms. The veracity of the TNN and DNN are then established herein by a strong universal approximation theorem for Tychonoff spaces and its corollary for spaces of measures. These theorems show that neural networks can arbitrarily approximate uniformly continuous functions (with respect to the sup metric) associated with a unique uniformity. We also provide some discussion showing that neural networks on positive-finite measures are a generalization of the recent deep learning notion of dee",
    "link": "http://arxiv.org/abs/2305.16639",
    "context": "Title: Universal Approximation and the Topological Neural Network. (arXiv:2305.16639v1 [cs.LG])\nAbstract: A topological neural network (TNN), which takes data from a Tychonoff topological space instead of the usual finite dimensional space, is introduced. As a consequence, a distributional neural network (DNN) that takes Borel measures as data is also introduced. Combined these new neural networks facilitate things like recognizing long range dependence, heavy tails and other properties in stochastic process paths or like acting on belief states produced by particle filtering or hidden Markov model algorithms. The veracity of the TNN and DNN are then established herein by a strong universal approximation theorem for Tychonoff spaces and its corollary for spaces of measures. These theorems show that neural networks can arbitrarily approximate uniformly continuous functions (with respect to the sup metric) associated with a unique uniformity. We also provide some discussion showing that neural networks on positive-finite measures are a generalization of the recent deep learning notion of dee",
    "path": "papers/23/05/2305.16639.json",
    "total_tokens": 971,
    "translated_title": "通用逼近和拓扑神经网络",
    "translated_abstract": "本文介绍了一种拓扑神经网络（TNN），它使用来自Tychonoff拓扑空间而不是通常的有限维空间的数据。由此引入了一个接受Borel度量为数据的分布神经网络（DNN）。这些新的神经网络结合起来有助于识别随机过程路径中的长期依赖、重尾分布和其他特性，也有助于对粒子滤波或隐马尔可夫模型算法产生的信念状态进行操作。本文还通过Tychonoff空间的强通用逼近定理及其度量空间的推论来证明了TNN和DNN的正确性。这些定理表明，神经网络可以任意逼近与唯一一致性度量相关的一致连续函数（关于上确界度量）。此外，我们还提供了一些讨论，表明正定度量上的神经网络是最近深度学习“深感知机”概念的一种推广。",
    "tldr": "本文介绍了一种拓扑神经网络，操作于Tychonoff拓扑空间而非有限维空间的数据，结合分布神经网络可识别随机过程路径中的长期依赖、重尾分布等特性，且证明了其可以任意逼近任何一致连续函数。",
    "en_tdlr": "The paper introduces a topological neural network that operates on Tychonoff spaces instead of finite dimensional spaces, and a distributional neural network that takes Borel measures as data, which can recognize long range dependence and heavy tails. The TNN and DNN are proven to be valid through a strong universal approximation theorem, which states that neural networks can arbitrarily approximate uniformly continuous functions associated with a unique uniformity. Additionally, the paper discusses the generalization of neural networks on positive-finite measures to the recent concept of deep learning known as deep belief networks."
}