{
    "title": "Make-An-Animation: Large-Scale Text-conditional 3D Human Motion Generation. (arXiv:2305.09662v1 [cs.CV])",
    "abstract": "Text-guided human motion generation has drawn significant interest because of its impactful applications spanning animation and robotics. Recently, application of diffusion models for motion generation has enabled improvements in the quality of generated motions. However, existing approaches are limited by their reliance on relatively small-scale motion capture data, leading to poor performance on more diverse, in-the-wild prompts. In this paper, we introduce Make-An-Animation, a text-conditioned human motion generation model which learns more diverse poses and prompts from large-scale image-text datasets, enabling significant improvement in performance over prior works. Make-An-Animation is trained in two stages. First, we train on a curated large-scale dataset of (text, static pseudo-pose) pairs extracted from image-text datasets. Second, we fine-tune on motion capture data, adding additional layers to model the temporal dimension. Unlike prior diffusion models for motion generation,",
    "link": "http://arxiv.org/abs/2305.09662",
    "context": "Title: Make-An-Animation: Large-Scale Text-conditional 3D Human Motion Generation. (arXiv:2305.09662v1 [cs.CV])\nAbstract: Text-guided human motion generation has drawn significant interest because of its impactful applications spanning animation and robotics. Recently, application of diffusion models for motion generation has enabled improvements in the quality of generated motions. However, existing approaches are limited by their reliance on relatively small-scale motion capture data, leading to poor performance on more diverse, in-the-wild prompts. In this paper, we introduce Make-An-Animation, a text-conditioned human motion generation model which learns more diverse poses and prompts from large-scale image-text datasets, enabling significant improvement in performance over prior works. Make-An-Animation is trained in two stages. First, we train on a curated large-scale dataset of (text, static pseudo-pose) pairs extracted from image-text datasets. Second, we fine-tune on motion capture data, adding additional layers to model the temporal dimension. Unlike prior diffusion models for motion generation,",
    "path": "papers/23/05/2305.09662.json",
    "total_tokens": 973,
    "translated_title": "Make-An-Animation: 大规模文本条件下的三维人体动作生成",
    "translated_abstract": "文本指导下的人体动作生成因其在动画和机器人领域的应用而备受关注。最近，扩散模型在动作生成方面的应用使生成动作的质量得到了改进。然而，现有方法局限于相对较小规模的运动捕捉数据，导致对更加多样化的“野外”提示的性能较差。本文介绍了Make-An-Animation，这是一个文本条件的人体运动生成模型，它从大规模的图像文本数据集中学习更多样化的姿势和提示，从而在前期工作的基础上显着提高了性能。Make-An-Animation经过两个阶段的训练。首先，我们在一个策划的大规模数据集上进行训练，该数据集包括从图像文本数据集中提取的（文本，静态假姿态）对。其次，我们在运动捕捉数据上进行微调，添加额外的层以模拟时间维度。与以往的扩散模型不同，Make-An-Animation采用两阶段训练过程，基于大规模的图像文本数据集生成更加多样化和高质量的人体运动。",
    "tldr": "Make-An-Animation 是一个用于文本条件下三维人体运动生成的模型，通过大规模数据集的训练得以弥补现有方法的局限，提高了运动生成的质量和多样性。"
}