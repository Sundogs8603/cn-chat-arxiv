{
    "title": "Forward-Forward Training of an Optical Neural Network. (arXiv:2305.19170v2 [cs.LG] UPDATED)",
    "abstract": "Neural networks (NN) have demonstrated remarkable capabilities in various tasks, but their computation-intensive nature demands faster and more energy-efficient hardware implementations. Optics-based platforms, using technologies such as silicon photonics and spatial light modulators, offer promising avenues for achieving this goal. However, training multiple trainable layers in tandem with these physical systems poses challenges, as they are difficult to fully characterize and describe with differentiable functions, hindering the use of error backpropagation algorithm. The recently introduced Forward-Forward Algorithm (FFA) eliminates the need for perfect characterization of the learning system and shows promise for efficient training with large numbers of programmable parameters. The FFA does not require backpropagating an error signal to update the weights, rather the weights are updated by only sending information in one direction. The local loss function for each set of trainable ",
    "link": "http://arxiv.org/abs/2305.19170",
    "context": "Title: Forward-Forward Training of an Optical Neural Network. (arXiv:2305.19170v2 [cs.LG] UPDATED)\nAbstract: Neural networks (NN) have demonstrated remarkable capabilities in various tasks, but their computation-intensive nature demands faster and more energy-efficient hardware implementations. Optics-based platforms, using technologies such as silicon photonics and spatial light modulators, offer promising avenues for achieving this goal. However, training multiple trainable layers in tandem with these physical systems poses challenges, as they are difficult to fully characterize and describe with differentiable functions, hindering the use of error backpropagation algorithm. The recently introduced Forward-Forward Algorithm (FFA) eliminates the need for perfect characterization of the learning system and shows promise for efficient training with large numbers of programmable parameters. The FFA does not require backpropagating an error signal to update the weights, rather the weights are updated by only sending information in one direction. The local loss function for each set of trainable ",
    "path": "papers/23/05/2305.19170.json",
    "total_tokens": 845,
    "translated_title": "光学神经网络的前向训练",
    "translated_abstract": "神经网络在各种任务中展示出了卓越的能力，但它们密集的计算性质要求更快速和更节能的硬件实现。基于光学的平台，利用硅光子学和空间光调制器等技术，为实现这一目标提供了有希望的途径。然而，与这些物理系统同时训练多个可训练的层面临挑战，因为它们很难完全表征并用可微函数进行描述，从而阻碍了误差反向传播算法的使用。最近引入的前向-前向算法（FFA）消除了对学习系统完美表征的需求，并显示出在具有大量可编程参数的高效训练方面的潜力。FFA不需要通过反向传播误差信号来更新权重，而是仅通过单向传递信息来更新权重。对于每组可训练的参数，本地损失函数用于优化权重。",
    "tldr": "这篇论文介绍了一种光学神经网络的前向训练方法，使用前向-前向算法进行权重更新，解决了光学平台训练多个可训练层面临的挑战。",
    "en_tdlr": "This paper presents a forward-forward training method for optical neural networks, using the forward-forward algorithm for weight updates, addressing the challenges of training multiple trainable layers in optics-based platforms."
}