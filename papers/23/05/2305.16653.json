{
    "title": "AdaPlanner: Adaptive Planning from Feedback with Language Models. (arXiv:2305.16653v1 [cs.CL])",
    "abstract": "Large language models (LLMs) have recently demonstrated the potential in acting as autonomous agents for sequential decision-making tasks. However, most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase. We propose a closed-loop approach, AdaPlanner, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback. In AdaPlanner, the LLM agent adaptively refines its plan from feedback with both in-plan and out-of-plan refinement strategies. To mitigate hallucination, we develop a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities. Furthermore, we propose a skill discovery mechanism that leverages successful plans as few-shot exemplars, enabling the",
    "link": "http://arxiv.org/abs/2305.16653",
    "context": "Title: AdaPlanner: Adaptive Planning from Feedback with Language Models. (arXiv:2305.16653v1 [cs.CL])\nAbstract: Large language models (LLMs) have recently demonstrated the potential in acting as autonomous agents for sequential decision-making tasks. However, most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase. We propose a closed-loop approach, AdaPlanner, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback. In AdaPlanner, the LLM agent adaptively refines its plan from feedback with both in-plan and out-of-plan refinement strategies. To mitigate hallucination, we develop a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities. Furthermore, we propose a skill discovery mechanism that leverages successful plans as few-shot exemplars, enabling the",
    "path": "papers/23/05/2305.16653.json",
    "total_tokens": 924,
    "translated_title": "AdaPlanner:自适应规划与语言模型的反馈。 （arXiv：2305.16653v1 [cs.CL]）",
    "translated_abstract": "最近的大型语言模型（LLM）展示了在序列决策任务中作为自主代理的潜力。然而，大多数现有方法要么贪婪地采取行动而没有计划，要么依赖于不可适应环境反馈的静态计划。因此，随着问题复杂性和计划水平的增加，LLM代理的顺序决策性能会退化。我们提出了一种闭环方法AdaPlanner，它允许LLM代理根据环境反馈自适应地改进其自动生成的计划。在AdaPlanner中，LLM代理通过计划内和计划外的改进策略自适应地改进其计划。为了减轻幻觉，我们开发了一种代码风格的LLM提示结构，促进了跨各种任务，环境和代理能力的计划生成。此外，我们提出了一种技能发现机制，利用成功的计划作为少量示例，使计划更具普适性。",
    "tldr": "LLM代理可以通过Adaplanner自适应改进自己的计划以应对环境反馈，为此提出计划内外的改进策略以及代码风格的LLM提示结构和技能发现机制。",
    "en_tdlr": "AdaPlanner proposes a closed-loop approach to enable LLM agents to refine their plans adaptively from feedback by developing in-plan and out-of-plan refinement strategies, a code-style LLM prompt structure and a skill discovery mechanism with successful plans as few-shot exemplars."
}