{
    "title": "Prompt Learning to Mitigate Catastrophic Forgetting in Cross-lingual Transfer for Open-domain Dialogue Generation. (arXiv:2305.07393v1 [cs.CL])",
    "abstract": "Dialogue systems for non-English languages have long been under-explored. In this paper, we take the first step to investigate few-shot cross-lingual transfer learning (FS-XLT) and multitask learning (MTL) in the context of open-domain dialogue generation for non-English languages with limited data. We observed catastrophic forgetting in both FS-XLT and MTL for all 6 languages in our preliminary experiments. To mitigate the issue, we propose a simple yet effective prompt learning approach that can preserve the multilinguality of multilingual pre-trained language model (mPLM) in FS-XLT and MTL by bridging the gap between pre-training and fine-tuning with Fixed-prompt LM Tuning and our hand-crafted prompts. Experimental results on all 6 languages in terms of both automatic and human evaluations demonstrate the effectiveness of our approach. Our code is available at https://github.com/JeremyLeiLiu/XLinguDial.",
    "link": "http://arxiv.org/abs/2305.07393",
    "context": "Title: Prompt Learning to Mitigate Catastrophic Forgetting in Cross-lingual Transfer for Open-domain Dialogue Generation. (arXiv:2305.07393v1 [cs.CL])\nAbstract: Dialogue systems for non-English languages have long been under-explored. In this paper, we take the first step to investigate few-shot cross-lingual transfer learning (FS-XLT) and multitask learning (MTL) in the context of open-domain dialogue generation for non-English languages with limited data. We observed catastrophic forgetting in both FS-XLT and MTL for all 6 languages in our preliminary experiments. To mitigate the issue, we propose a simple yet effective prompt learning approach that can preserve the multilinguality of multilingual pre-trained language model (mPLM) in FS-XLT and MTL by bridging the gap between pre-training and fine-tuning with Fixed-prompt LM Tuning and our hand-crafted prompts. Experimental results on all 6 languages in terms of both automatic and human evaluations demonstrate the effectiveness of our approach. Our code is available at https://github.com/JeremyLeiLiu/XLinguDial.",
    "path": "papers/23/05/2305.07393.json",
    "total_tokens": 918,
    "translated_title": "针对开放域对话生成的跨语言迁移中降低灾难性遗忘的提示学习",
    "translated_abstract": "长期以来，非英语语言的对话系统一直未得到充分探索。本文第一次在非英语语言有限数据的开放域对话生成中研究了少样本跨语言迁移学习（FS-XLT）和多任务学习（MTL）。在初步实验中，我们发现在FS-XLT和MTL中所有的6种语言中都存在灾难性遗忘。为了减轻这一问题，我们提出了一种简单而有效的提示学习方法，通过固定提示语言模型调参和我们手工制作的提示语来弥合预训练和微调之间的差距，从而保持多语言预训练语言模型（mPLM）在FS-XLT和MTL中的多语言性。在所有6种语言上的自动和人工评估结果都证明了我们方法的有效性。我们的代码可在 https://github.com/JeremyLeiLiu/XLinguDial 上获取。",
    "tldr": "本文提出了一种提示学习方法，以解决开放域非英语语言对话系统中少量数据下的跨语言迁移学习和多任务学习中的灾难性遗忘问题，并在六种语言上的实验中证明了其有效性。",
    "en_tdlr": "This paper presents a prompt learning approach to alleviate catastrophic forgetting in few-shot cross-lingual transfer learning and multitask learning for open-domain dialogue generation in non-English languages with limited data, and the effectiveness of the proposed method is demonstrated in experiments conducted on six languages."
}