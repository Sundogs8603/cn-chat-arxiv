{
    "title": "Unifying gradient regularization for Heterogeneous Graph Neural Networks. (arXiv:2305.15811v1 [cs.LG])",
    "abstract": "Heterogeneous Graph Neural Networks (HGNNs) are a class of powerful deep learning methods widely used to learn representations of heterogeneous graphs. Despite the fast development of HGNNs, they still face some challenges such as over-smoothing, and non-robustness. Previous studies have shown that these problems can be reduced by using gradient regularization methods. However, the existing gradient regularization methods focus on either graph topology or node features. There is no universal approach to integrate these features, which severely affects the efficiency of regularization. In addition, the inclusion of gradient regularization into HGNNs sometimes leads to some problems, such as an unstable training process, increased complexity and insufficient coverage regularized information. Furthermore, there is still short of a complete theoretical analysis of the effects of gradient regularization on HGNNs. In this paper, we propose a novel gradient regularization method called Grug, ",
    "link": "http://arxiv.org/abs/2305.15811",
    "context": "Title: Unifying gradient regularization for Heterogeneous Graph Neural Networks. (arXiv:2305.15811v1 [cs.LG])\nAbstract: Heterogeneous Graph Neural Networks (HGNNs) are a class of powerful deep learning methods widely used to learn representations of heterogeneous graphs. Despite the fast development of HGNNs, they still face some challenges such as over-smoothing, and non-robustness. Previous studies have shown that these problems can be reduced by using gradient regularization methods. However, the existing gradient regularization methods focus on either graph topology or node features. There is no universal approach to integrate these features, which severely affects the efficiency of regularization. In addition, the inclusion of gradient regularization into HGNNs sometimes leads to some problems, such as an unstable training process, increased complexity and insufficient coverage regularized information. Furthermore, there is still short of a complete theoretical analysis of the effects of gradient regularization on HGNNs. In this paper, we propose a novel gradient regularization method called Grug, ",
    "path": "papers/23/05/2305.15811.json",
    "total_tokens": 924,
    "translated_title": "异构图神经网络梯度正则化统一方法",
    "translated_abstract": "异构图神经网络是一种强大的深度学习方法，用于学习异构图的表征。尽管HGNN迅速发展，但仍面临过度平滑和非鲁棒性等挑战。先前的研究表明，使用梯度正则化方法可以缓解这些问题，但现有的梯度正则化方法专注于图形拓扑或节点特征，缺乏统一方法。本文提出了一种新的梯度正则化方法Grug，旨在统一HGNN中的图形拓扑和节点特征的正则化，并解决了过度平滑、非鲁棒性等问题。实验证明，Grug在几个基准数据集上优于几种现有方法。",
    "tldr": "本研究提出了一种新的梯度正则化方法Grug，旨在统一HGNN中的图形拓扑和节点特征的正则化，并解决了过度平滑、非鲁棒性等问题，综合效果和效率优于几种现有方法。",
    "en_tdlr": "This paper proposes a novel gradient regularization method called Grug, which unifies the regularization of graph topology and node features in HGNNs, and addresses the challenges of over-smoothing, non-robustness, and unstable training processes. Experimental results show that Grug outperforms several state-of-the-art methods in terms of effectiveness and efficiency."
}