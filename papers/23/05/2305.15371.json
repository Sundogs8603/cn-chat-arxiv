{
    "title": "Stochastic Unrolled Federated Learning",
    "abstract": "Algorithm unrolling has emerged as a learning-based optimization paradigm that unfolds truncated iterative algorithms in trainable neural-network optimizers. We introduce Stochastic UnRolled Federated learning (SURF), a method that expands algorithm unrolling to federated learning in order to expedite its convergence. Our proposed method tackles two challenges of this expansion, namely the need to feed whole datasets to the unrolled optimizers to find a descent direction and the decentralized nature of federated learning. We circumvent the former challenge by feeding stochastic mini-batches to each unrolled layer and imposing descent constraints to guarantee its convergence. We address the latter challenge by unfolding the distributed gradient descent (DGD) algorithm in a graph neural network (GNN)-based unrolled architecture, which preserves the decentralized nature of training in federated learning. We theoretically prove that our proposed unrolled optimizer converges to a near-optim",
    "link": "https://arxiv.org/abs/2305.15371",
    "context": "Title: Stochastic Unrolled Federated Learning\nAbstract: Algorithm unrolling has emerged as a learning-based optimization paradigm that unfolds truncated iterative algorithms in trainable neural-network optimizers. We introduce Stochastic UnRolled Federated learning (SURF), a method that expands algorithm unrolling to federated learning in order to expedite its convergence. Our proposed method tackles two challenges of this expansion, namely the need to feed whole datasets to the unrolled optimizers to find a descent direction and the decentralized nature of federated learning. We circumvent the former challenge by feeding stochastic mini-batches to each unrolled layer and imposing descent constraints to guarantee its convergence. We address the latter challenge by unfolding the distributed gradient descent (DGD) algorithm in a graph neural network (GNN)-based unrolled architecture, which preserves the decentralized nature of training in federated learning. We theoretically prove that our proposed unrolled optimizer converges to a near-optim",
    "path": "papers/23/05/2305.15371.json",
    "total_tokens": 868,
    "translated_title": "随机展开的联邦学习",
    "translated_abstract": "算法展开已经成为一种基于学习的优化范式，它将截断的迭代算法展开为可训练的神经网络优化器。我们引入了随机展开的联邦学习（SURF）方法，这种方法将算法展开应用于联邦学习，以加快其收敛速度。我们的方法解决了这种扩展面临的两个挑战，即需要将整个数据集提供给展开的优化器以找到合适的方向，以及联邦学习的分布式特性。我们通过给每个展开层提供随机小批量数据并施加下降约束来解决前一个挑战，以保证其收敛。我们通过在基于图神经网络（GNN）的展开架构中展开分布式梯度下降（DGD）算法来解决后一个挑战，从而保持联邦学习中的训练分布式特性。我们在理论上证明了我们提出的展开优化器收敛于近优解。",
    "tldr": "随机展开的联邦学习（SURF）是一种扩展了算法展开到联邦学习的优化方法，在解决需要整个数据集的挑战和保持联邦学习分布式特性的同时，加快了收敛速度。"
}