{
    "title": "Probing the Role of Positional Information in Vision-Language Models. (arXiv:2305.10046v1 [cs.CL])",
    "abstract": "In most Vision-Language models (VL), the understanding of the image structure is enabled by injecting the position information (PI) about objects in the image. In our case study of LXMERT, a state-of-the-art VL model, we probe the use of the PI in the representation and study its effect on Visual Question Answering. We show that the model is not capable of leveraging the PI for the image-text matching task on a challenge set where only position differs. Yet, our experiments with probing confirm that the PI is indeed present in the representation. We introduce two strategies to tackle this: (i) Positional Information Pre-training and (ii) Contrastive Learning on PI using Cross-Modality Matching. Doing so, the model can correctly classify if images with detailed PI statements match. Additionally to the 2D information from bounding boxes, we introduce the object's depth as new feature for a better object localization in the space. Even though we were able to improve the model properties a",
    "link": "http://arxiv.org/abs/2305.10046",
    "context": "Title: Probing the Role of Positional Information in Vision-Language Models. (arXiv:2305.10046v1 [cs.CL])\nAbstract: In most Vision-Language models (VL), the understanding of the image structure is enabled by injecting the position information (PI) about objects in the image. In our case study of LXMERT, a state-of-the-art VL model, we probe the use of the PI in the representation and study its effect on Visual Question Answering. We show that the model is not capable of leveraging the PI for the image-text matching task on a challenge set where only position differs. Yet, our experiments with probing confirm that the PI is indeed present in the representation. We introduce two strategies to tackle this: (i) Positional Information Pre-training and (ii) Contrastive Learning on PI using Cross-Modality Matching. Doing so, the model can correctly classify if images with detailed PI statements match. Additionally to the 2D information from bounding boxes, we introduce the object's depth as new feature for a better object localization in the space. Even though we were able to improve the model properties a",
    "path": "papers/23/05/2305.10046.json",
    "total_tokens": 954,
    "translated_title": "探索位置信息在视觉语言模型中的作用",
    "translated_abstract": "在大多数视觉语言模型（VL）中，理解图像结构需要注入有关图像中物体位置信息（PI）。在我们对LXMERT进行的案例研究中，这是一种最先进的VL模型，我们探讨了PI在表示中的使用及其对视觉问答的影响。我们表明，该模型不能利用PI处理仅位置不同的挑战集上的图像 - 文本匹配任务。但是，我们通过探查实验证实了表示中确实存在PI。我们引入了两种策略来解决这个问题：（i）位置信息预训练和（ii）使用跨模态匹配的PI对比度学习。通过这样做，模型可以正确分类具有详细PI陈述的图像是否匹配。除了来自边界框的2D信息外，我们引入物体深度作为新特征，以在空间中更好地定位对象。尽管我们成功地改善了模型性能，但在我们的故事中仍有进一步的挑战，值得进一步研究。",
    "tldr": "本研究调查了在视觉语言模型中位置信息的使用，表明模型存在位置信息，但不能很好地利用它进行图像 - 文本匹配。通过引入位置信息预训练和跨模态匹配的PI对比度学习，在挑战集上成功地改善了模型性能。",
    "en_tdlr": "This study investigates the use of positional information in Vision-Language models, showing that it exists in the model's representation but is not effectively utilized for image-text matching. Strategies of Positional Information Pre-training and Contrastive Learning on PI using Cross-Modality Matching are proposed to improve the model performance on a challenge set where only position differs."
}