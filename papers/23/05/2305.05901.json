{
    "title": "Text-guided High-definition Consistency Texture Model. (arXiv:2305.05901v1 [cs.CV])",
    "abstract": "With the advent of depth-to-image diffusion models, text-guided generation, editing, and transfer of realistic textures are no longer difficult. However, due to the limitations of pre-trained diffusion models, they can only create low-resolution, inconsistent textures. To address this issue, we present the High-definition Consistency Texture Model (HCTM), a novel method that can generate high-definition and consistent textures for 3D meshes according to the text prompts. We achieve this by leveraging a pre-trained depth-to-image diffusion model to generate single viewpoint results based on the text prompt and a depth map. We fine-tune the diffusion model with Parameter-Efficient Fine-Tuning to quickly learn the style of the generated result, and apply the multi-diffusion strategy to produce high-resolution and consistent results from different viewpoints. Furthermore, we propose a strategy that prevents the appearance of noise on the textures caused by backpropagation. Our proposed app",
    "link": "http://arxiv.org/abs/2305.05901",
    "context": "Title: Text-guided High-definition Consistency Texture Model. (arXiv:2305.05901v1 [cs.CV])\nAbstract: With the advent of depth-to-image diffusion models, text-guided generation, editing, and transfer of realistic textures are no longer difficult. However, due to the limitations of pre-trained diffusion models, they can only create low-resolution, inconsistent textures. To address this issue, we present the High-definition Consistency Texture Model (HCTM), a novel method that can generate high-definition and consistent textures for 3D meshes according to the text prompts. We achieve this by leveraging a pre-trained depth-to-image diffusion model to generate single viewpoint results based on the text prompt and a depth map. We fine-tune the diffusion model with Parameter-Efficient Fine-Tuning to quickly learn the style of the generated result, and apply the multi-diffusion strategy to produce high-resolution and consistent results from different viewpoints. Furthermore, we propose a strategy that prevents the appearance of noise on the textures caused by backpropagation. Our proposed app",
    "path": "papers/23/05/2305.05901.json",
    "total_tokens": 969,
    "translated_title": "文本引导下的高清一致纹理模型",
    "translated_abstract": "随着深度到图像扩散模型的出现，逼真纹理的文本引导生成、编辑和转移不再困难。然而，由于预训练扩散模型的限制，它们只能创建低分辨率、不一致的纹理。为了解决这个问题，我们提出了一种新的方法：高清一致纹理模型（HCTM），它可以根据文本提示为 3D 网格生成高清晰度和一致的纹理。我们利用预训练的深度到图像扩散模型根据文本提示和深度图生成单视点结果，利用参数高效微调方法对扩散模型进行微调，以快速学习所生成结果的风格，并采用多扩散策略从不同视点产生高分辨率和一致的结果。此外，我们提出了一种策略，防止由反向传播引起的纹理上出现噪声。",
    "tldr": "该论文介绍了一种名为 HCTM 的方法，它可以根据文本提示为 3D 网格生成高清晰度和一致的纹理。该方法结合预训练深度到图像扩散模型和参数高效微调方法，利用多扩散策略生成高分辨率和一致的结果，并提出了一种防止噪声出现的策略。",
    "en_tdlr": "This paper presents a method called HCTM, which can generate high-definition and consistent textures for 3D meshes according to text prompts. The method combines pre-trained depth-to-image diffusion models with parameter-efficient fine-tuning methods, uses multi-diffusion strategies to produce high-resolution and consistent results, and proposes a strategy to prevent the appearance of noise on textures caused by backpropagation."
}