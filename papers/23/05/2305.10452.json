{
    "title": "Comparison of classifiers in challenge scheme. (arXiv:2305.10452v1 [cs.LG])",
    "abstract": "In recent decades, challenges have become very popular in scientific research as these are crowdsourcing schemes. In particular, challenges are essential for developing machine learning algorithms. For the challenges settings, it is vital to establish the scientific question, the dataset (with adequate quality, quantity, diversity, and complexity), performance metrics, as well as a way to authenticate the participants' results (Gold Standard). This paper addresses the problem of evaluating the performance of different competitors (algorithms) under the restrictions imposed by the challenge scheme, such as the comparison of multiple competitors with a unique dataset (with fixed size), a minimal number of submissions and, a set of metrics chosen to assess performance. The algorithms are sorted according to the performance metric. Still, it is common to observe performance differences among competitors as small as hundredths or even thousandths, so the question is whether the differences ",
    "link": "http://arxiv.org/abs/2305.10452",
    "context": "Title: Comparison of classifiers in challenge scheme. (arXiv:2305.10452v1 [cs.LG])\nAbstract: In recent decades, challenges have become very popular in scientific research as these are crowdsourcing schemes. In particular, challenges are essential for developing machine learning algorithms. For the challenges settings, it is vital to establish the scientific question, the dataset (with adequate quality, quantity, diversity, and complexity), performance metrics, as well as a way to authenticate the participants' results (Gold Standard). This paper addresses the problem of evaluating the performance of different competitors (algorithms) under the restrictions imposed by the challenge scheme, such as the comparison of multiple competitors with a unique dataset (with fixed size), a minimal number of submissions and, a set of metrics chosen to assess performance. The algorithms are sorted according to the performance metric. Still, it is common to observe performance differences among competitors as small as hundredths or even thousandths, so the question is whether the differences ",
    "path": "papers/23/05/2305.10452.json",
    "total_tokens": 814,
    "translated_title": "对比分类器在挑战方案中的表现",
    "translated_abstract": "近几十年来，挑战方案作为一种众包机制在科学研究中越来越受欢迎。特别是对于开发机器学习算法来说，挑战方案至关重要。本文讨论了在挑战方案的限制下评估不同竞争者（算法）表现的问题，包括使用唯一的数据集（大小固定）、规定最小提交次数和一组性能指标等措施。分类器按性能指标排序，但常常发现竞争对手间的性能差异微乎其微，甚至只有千分之一左右。因此关键问题是，这些差异能否对最终结果产生重大影响。",
    "tldr": "本文介绍了在挑战方案的限制下评估不同竞争者(算法)表现的问题，包括使用唯一的数据集、规定最小提交次数和一组性能指标等措施。关键问题是，这些差异能否对最终结果产生重大影响。",
    "en_tdlr": "This paper discusses how to evaluate the performance of different algorithms under the restrictions imposed by the challenge scheme, including using a unique dataset, setting a minimum number of submissions, and a set of performance metrics. The key question is whether the performance differences can have a significant impact on the final results."
}