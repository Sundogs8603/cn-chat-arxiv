{
    "title": "Dropout Drops Double Descent. (arXiv:2305.16179v2 [cs.LG] UPDATED)",
    "abstract": "In this paper, we find and analyze that we can easily drop the double descent by only adding one dropout layer before the fully-connected linear layer. The surprising double-descent phenomenon has drawn public attention in recent years, making the prediction error rise and drop as we increase either sample or model size. The current paper shows that it is possible to alleviate these phenomena by using optimal dropout in the linear regression model and the nonlinear random feature regression, both theoretically and empirically. % ${y}=X{\\beta}^0+{\\epsilon}$ with $X\\in\\mathbb{R}^{n\\times p}$. We obtain the optimal dropout hyperparameter by estimating the ground truth ${\\beta}^0$ with generalized ridge typed estimator $\\hat{{\\beta}}=(X^TX+\\alpha\\cdot\\mathrm{diag}(X^TX))^{-1}X^T{y}$. Moreover, we empirically show that optimal dropout can achieve a monotonic test error curve in nonlinear neural networks using Fashion-MNIST and CIFAR-10. Our results suggest considering dropout for risk curve",
    "link": "http://arxiv.org/abs/2305.16179",
    "context": "Title: Dropout Drops Double Descent. (arXiv:2305.16179v2 [cs.LG] UPDATED)\nAbstract: In this paper, we find and analyze that we can easily drop the double descent by only adding one dropout layer before the fully-connected linear layer. The surprising double-descent phenomenon has drawn public attention in recent years, making the prediction error rise and drop as we increase either sample or model size. The current paper shows that it is possible to alleviate these phenomena by using optimal dropout in the linear regression model and the nonlinear random feature regression, both theoretically and empirically. % ${y}=X{\\beta}^0+{\\epsilon}$ with $X\\in\\mathbb{R}^{n\\times p}$. We obtain the optimal dropout hyperparameter by estimating the ground truth ${\\beta}^0$ with generalized ridge typed estimator $\\hat{{\\beta}}=(X^TX+\\alpha\\cdot\\mathrm{diag}(X^TX))^{-1}X^T{y}$. Moreover, we empirically show that optimal dropout can achieve a monotonic test error curve in nonlinear neural networks using Fashion-MNIST and CIFAR-10. Our results suggest considering dropout for risk curve",
    "path": "papers/23/05/2305.16179.json",
    "total_tokens": 731,
    "translated_title": "Dropout可以缓解双下降现象的研究",
    "translated_abstract": "本研究发现并分析，通过在全连接线性层之前添加一个dropout层，可以轻松地缓解双下降现象。双下降现象在近年来引起了公众的关注，即随着样本或模型规模的增加，预测误差会先上升再下降。本文从理论和实证两个方面证明，通过在线性回归模型和非线性随机特征回归中使用最佳的dropout，可以缓解这些现象。",
    "tldr": "本研究发现通过在全连接线性层之前添加一个dropout层，可以缓解双下降现象，从而提高模型的预测准确性。",
    "en_tdlr": "This paper proposes that adding a dropout layer before the fully-connected linear layer can alleviate the double descent phenomenon and improve the prediction accuracy of the model."
}