{
    "title": "AQ-GT: a Temporally Aligned and Quantized GRU-Transformer for Co-Speech Gesture Synthesis. (arXiv:2305.01241v1 [cs.HC])",
    "abstract": "The generation of realistic and contextually relevant co-speech gestures is a challenging yet increasingly important task in the creation of multimodal artificial agents. Prior methods focused on learning a direct correspondence between co-speech gesture representations and produced motions, which created seemingly natural but often unconvincing gestures during human assessment. We present an approach to pre-train partial gesture sequences using a generative adversarial network with a quantization pipeline. The resulting codebook vectors serve as both input and output in our framework, forming the basis for the generation and reconstruction of gestures. By learning the mapping of a latent space representation as opposed to directly mapping it to a vector representation, this framework facilitates the generation of highly realistic and expressive gestures that closely replicate human movement and behavior, while simultaneously avoiding artifacts in the generation process. We evaluate ou",
    "link": "http://arxiv.org/abs/2305.01241",
    "context": "Title: AQ-GT: a Temporally Aligned and Quantized GRU-Transformer for Co-Speech Gesture Synthesis. (arXiv:2305.01241v1 [cs.HC])\nAbstract: The generation of realistic and contextually relevant co-speech gestures is a challenging yet increasingly important task in the creation of multimodal artificial agents. Prior methods focused on learning a direct correspondence between co-speech gesture representations and produced motions, which created seemingly natural but often unconvincing gestures during human assessment. We present an approach to pre-train partial gesture sequences using a generative adversarial network with a quantization pipeline. The resulting codebook vectors serve as both input and output in our framework, forming the basis for the generation and reconstruction of gestures. By learning the mapping of a latent space representation as opposed to directly mapping it to a vector representation, this framework facilitates the generation of highly realistic and expressive gestures that closely replicate human movement and behavior, while simultaneously avoiding artifacts in the generation process. We evaluate ou",
    "path": "papers/23/05/2305.01241.json",
    "total_tokens": 928,
    "translated_title": "AQ-GT:一种时间对齐并量化的GRU-Transformer，用于共语手势合成",
    "translated_abstract": "在创建多模式人工智能代理时，生成逼真且与上下文相关的共语手势是一项具有挑战性但越来越重要的任务。以往的方法主要集中于学习共语手势表示和实际动作之间的直接对应关系，这种方法通常会在人类评估中产生似乎自然但常常不令人信服的手势。我们提出了一种使用生成对抗网络和量化流水线对部分手势序列进行预训练的方法。所得到的码本向量既是我们框架中的输入，也是输出，构成手势的生成和重构的基础。通过学习潜在空间表示的映射，而不是将其直接映射到矢量表示，该框架促进了高度逼真且富有表现力的手势生成，这些手势与人类的运动和行为非常相似，同时避免了生成过程中的伪像。我们评估了我们的模型在两个数据集上的效果，并发现它比以前的方法产生了更逼真和丰富的手势。",
    "tldr": "本文提出了一种使用生成对抗网络和量化流水线对部分手势序列进行预训练并基于潜在空间的生成方法，从而实现高度逼真、富有表现力、并避免了伪像的手势生成。",
    "en_tdlr": "This paper proposes a pre-training and latent space-based generation method for cospeech gesture synthesis using a generative adversarial network and a quantization pipeline, which achieves highly realistic, expressive, and artifact-free gesture generation."
}