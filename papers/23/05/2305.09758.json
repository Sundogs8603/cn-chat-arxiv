{
    "title": "A Video Is Worth 4096 Tokens: Verbalize Story Videos To Understand Them In Zero Shot. (arXiv:2305.09758v1 [cs.CV])",
    "abstract": "Multimedia content, such as advertisements and story videos, exhibit a rich blend of creativity and multiple modalities. They incorporate elements like text, visuals, audio, and storytelling techniques, employing devices like emotions, symbolism, and slogans to convey meaning. While previous research in multimedia understanding has focused mainly on videos with specific actions like cooking, there is a dearth of large annotated training datasets, hindering the development of supervised learning models with satisfactory performance for real-world applications. However, the rise of large language models (LLMs) has witnessed remarkable zero-shot performance in various natural language processing (NLP) tasks, such as emotion classification, question-answering, and topic classification. To bridge this performance gap in multimedia understanding, we propose verbalizing story videos to generate their descriptions in natural language and then performing video-understanding tasks on the generat",
    "link": "http://arxiv.org/abs/2305.09758",
    "context": "Title: A Video Is Worth 4096 Tokens: Verbalize Story Videos To Understand Them In Zero Shot. (arXiv:2305.09758v1 [cs.CV])\nAbstract: Multimedia content, such as advertisements and story videos, exhibit a rich blend of creativity and multiple modalities. They incorporate elements like text, visuals, audio, and storytelling techniques, employing devices like emotions, symbolism, and slogans to convey meaning. While previous research in multimedia understanding has focused mainly on videos with specific actions like cooking, there is a dearth of large annotated training datasets, hindering the development of supervised learning models with satisfactory performance for real-world applications. However, the rise of large language models (LLMs) has witnessed remarkable zero-shot performance in various natural language processing (NLP) tasks, such as emotion classification, question-answering, and topic classification. To bridge this performance gap in multimedia understanding, we propose verbalizing story videos to generate their descriptions in natural language and then performing video-understanding tasks on the generat",
    "path": "papers/23/05/2305.09758.json",
    "total_tokens": 1270,
    "tldr": "本文提出了一种方法，将视频转化为自然语言生成描述，在生成的描述上执行视频理解任务。我们创建了一个新的数据集，使用LLMs训练视频描述生成器，并在下游视频理解任务上取得了显著的零-shot性能。此方法可以使我们仅仅使用视频的口述描述就能够像人类一样理解视频。",
    "en_tdlr": "The paper proposes a method to understand videos by generating natural language descriptions of them and performing video-understanding tasks on these descriptions. They create a new dataset called the Verbalized-Story Video (VS-V) dataset and train a video description generator using LLMs achieving remarkable zero-shot performance on downstream video-understanding tasks. This method enables human-like understanding of videos using only their verbal descriptions."
}