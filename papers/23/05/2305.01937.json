{
    "title": "Can Large Language Models Be an Alternative to Human Evaluations?. (arXiv:2305.01937v1 [cs.CL])",
    "abstract": "Human evaluation is indispensable and inevitable for assessing the quality of texts generated by machine learning models or written by humans. However, human evaluation is very difficult to reproduce and its quality is notoriously unstable, hindering fair comparisons among different natural language processing (NLP) models and algorithms. Recently, large language models (LLMs) have demonstrated exceptional performance on unseen tasks when only the task instructions are provided. In this paper, we explore if such an ability of the LLMs can be used as an alternative to human evaluation. We present the LLMs with the exact same instructions, samples to be evaluated, and questions used to conduct human evaluation, and then ask the LLMs to generate responses to those questions; we dub this LLM evaluation. We use human evaluation and LLM evaluation to evaluate the texts in two NLP tasks: open-ended story generation and adversarial attacks. We show that the result of LLM evaluation is consiste",
    "link": "http://arxiv.org/abs/2305.01937",
    "context": "Title: Can Large Language Models Be an Alternative to Human Evaluations?. (arXiv:2305.01937v1 [cs.CL])\nAbstract: Human evaluation is indispensable and inevitable for assessing the quality of texts generated by machine learning models or written by humans. However, human evaluation is very difficult to reproduce and its quality is notoriously unstable, hindering fair comparisons among different natural language processing (NLP) models and algorithms. Recently, large language models (LLMs) have demonstrated exceptional performance on unseen tasks when only the task instructions are provided. In this paper, we explore if such an ability of the LLMs can be used as an alternative to human evaluation. We present the LLMs with the exact same instructions, samples to be evaluated, and questions used to conduct human evaluation, and then ask the LLMs to generate responses to those questions; we dub this LLM evaluation. We use human evaluation and LLM evaluation to evaluate the texts in two NLP tasks: open-ended story generation and adversarial attacks. We show that the result of LLM evaluation is consiste",
    "path": "papers/23/05/2305.01937.json",
    "total_tokens": 857,
    "translated_title": "大型语言模型能否替代人类评估？",
    "translated_abstract": "人类评估对于评估由机器学习模型生成或由人类编写的文本质量是必不可少和不可避免的。然而，人类评估非常难以重现，其质量也是非常不稳定的，这阻碍了不同自然语言处理（NLP）模型和算法之间的公平比较。最近，当只提供任务说明时，大型语言模型（LLMs）在未见过的任务上表现出了异常的性能。在本文中，我们探讨了LLMs的这种能力能否用作人类评估的替代品。我们向LLMs提供与进行人类评估相同的说明、待评估样本和问题，并要求LLMs对这些问题生成响应；我们将其称为LLM的评估。我们使用人类评估和LLM评估来评估两个NLP任务中的文本：开放式故事生成和对抗性攻击。我们表明LLM评估的结果是一致的。",
    "tldr": "本文研究了大型语言模型是否能够替代人类评估。实验结果表明在两个NLP任务上，使用LLM评估和人类评估得到的结果是一致的。",
    "en_tdlr": "This paper investigates if large language models can be used as an alternative to human evaluation. The results show that in two NLP tasks, using LLM evaluation and human evaluation yields consistent results."
}