{
    "title": "Multi-agent Continual Coordination via Progressive Task Contextualization. (arXiv:2305.13937v1 [cs.MA])",
    "abstract": "Cooperative Multi-agent Reinforcement Learning (MARL) has attracted significant attention and played the potential for many real-world applications. Previous arts mainly focus on facilitating the coordination ability from different aspects (e.g., non-stationarity, credit assignment) in single-task or multi-task scenarios, ignoring the stream of tasks that appear in a continual manner. This ignorance makes the continual coordination an unexplored territory, neither in problem formulation nor efficient algorithms designed. Towards tackling the mentioned issue, this paper proposes an approach Multi-Agent Continual Coordination via Progressive Task Contextualization, dubbed MACPro. The key point lies in obtaining a factorized policy, using shared feature extraction layers but separated independent task heads, each specializing in a specific class of tasks. The task heads can be progressively expanded based on the learned task contextualization. Moreover, to cater to the popular CTDE paradi",
    "link": "http://arxiv.org/abs/2305.13937",
    "context": "Title: Multi-agent Continual Coordination via Progressive Task Contextualization. (arXiv:2305.13937v1 [cs.MA])\nAbstract: Cooperative Multi-agent Reinforcement Learning (MARL) has attracted significant attention and played the potential for many real-world applications. Previous arts mainly focus on facilitating the coordination ability from different aspects (e.g., non-stationarity, credit assignment) in single-task or multi-task scenarios, ignoring the stream of tasks that appear in a continual manner. This ignorance makes the continual coordination an unexplored territory, neither in problem formulation nor efficient algorithms designed. Towards tackling the mentioned issue, this paper proposes an approach Multi-Agent Continual Coordination via Progressive Task Contextualization, dubbed MACPro. The key point lies in obtaining a factorized policy, using shared feature extraction layers but separated independent task heads, each specializing in a specific class of tasks. The task heads can be progressively expanded based on the learned task contextualization. Moreover, to cater to the popular CTDE paradi",
    "path": "papers/23/05/2305.13937.json",
    "total_tokens": 892,
    "translated_title": "多智能体持续协作：逐步任务情境化的方法",
    "translated_abstract": "合作多智能体强化学习在许多实际应用中表现出了巨大的潜力。本文提出了一种名为MACPro的方法，它采用分解策略，使用共享特征提取层但是分离的独立任务头，每个任务头分别专注于特定类别的任务，并可以基于学习的任务情境逐步扩展。此外，该方法还结合了先进的辅助任务和CTDE概念，以进一步稳定和提高训练效果。在各种环境下验证了此方法的有效性和效率，包括经典的棋盘游戏和复杂的Atari游戏。",
    "tldr": "本文提出了一种名为MACPro的方法，它采用分解策略来实现多智能体持续协作，该方法可以基于各自学习到的任务情境逐步扩展，并且还结合了辅助任务和CTDE概念，验证了其在经典棋盘游戏和Atari游戏中的有效性。",
    "en_tdlr": "This paper proposes a method called MACPro for multi-agent continual coordination, which employs a factorized policy for specialized task heads that can be expanded based on learned task contextualization, and integrates with auxiliary tasks and CTDE concept for better training stability and effectiveness. The method is verified on classic board games and sophisticated Atari games."
}