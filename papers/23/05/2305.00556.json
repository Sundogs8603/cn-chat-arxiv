{
    "title": "Reconstructing seen images from human brain activity via guided stochastic search. (arXiv:2305.00556v1 [q-bio.NC])",
    "abstract": "Visual reconstruction algorithms are an interpretive tool that map brain activity to pixels. Past reconstruction algorithms employed brute-force search through a massive library to select candidate images that, when passed through an encoding model, accurately predict brain activity. Here, we use conditional generative diffusion models to extend and improve this search-based strategy. We decode a semantic descriptor from human brain activity (7T fMRI) in voxels across most of visual cortex, then use a diffusion model to sample a small library of images conditioned on this descriptor. We pass each sample through an encoding model, select the images that best predict brain activity, and then use these images to seed another library. We show that this process converges on high-quality reconstructions by refining low-level image details while preserving semantic content across iterations. Interestingly, the time-to-convergence differs systematically across visual cortex, suggesting a succi",
    "link": "http://arxiv.org/abs/2305.00556",
    "context": "Title: Reconstructing seen images from human brain activity via guided stochastic search. (arXiv:2305.00556v1 [q-bio.NC])\nAbstract: Visual reconstruction algorithms are an interpretive tool that map brain activity to pixels. Past reconstruction algorithms employed brute-force search through a massive library to select candidate images that, when passed through an encoding model, accurately predict brain activity. Here, we use conditional generative diffusion models to extend and improve this search-based strategy. We decode a semantic descriptor from human brain activity (7T fMRI) in voxels across most of visual cortex, then use a diffusion model to sample a small library of images conditioned on this descriptor. We pass each sample through an encoding model, select the images that best predict brain activity, and then use these images to seed another library. We show that this process converges on high-quality reconstructions by refining low-level image details while preserving semantic content across iterations. Interestingly, the time-to-convergence differs systematically across visual cortex, suggesting a succi",
    "path": "papers/23/05/2305.00556.json",
    "total_tokens": 941,
    "translated_title": "通过引导随机搜索从人脑活动中重建视觉图像",
    "translated_abstract": "视觉重建算法是一种将脑活动映射到像素的解释工具。过去的重建算法采用大规模库的暴力搜索来选择候选图像，这些图像通过编码模型可以准确地预测脑活动。本研究使用条件生成扩散模型来扩展和改进这种基于搜索的策略。我们在大部分视觉皮层的体素中从人脑活动（7T fMRI）解码出语义描述符，然后使用扩散模型在此描述符的条件下对一小组图像进行采样。我们将每个样本通过编码模型，选择最能准确预测脑活动的图像，然后使用这些图像来种子另一个库。我们展示了这个过程通过在迭代中细化低级图像细节，同时保留语义内容而收敛到高质量的重建结果。有趣的是，收敛所需的时间在视觉皮层中有系统差异，表明了脑区的高层抽象概念需要更长的时间来集成和反映。",
    "tldr": "本研究使用条件生成扩散模型，改进过去的视觉重建算法，通过对一小组图像的采样和编码模型的选择，实现了从人脑活动中高质量、保留语义内容的重建结果，并发现了视觉皮层不同区域的重建时间差异。",
    "en_tdlr": "This study improved visual reconstruction algorithms by using conditional generative diffusion models to sample and select candidate images based on a semantic descriptor decoded from human brain activity, resulting in high-quality reconstructions with preserved semantic content and revealed systematic differences of convergence time across visual cortex."
}