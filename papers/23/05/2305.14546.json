{
    "title": "On the Transferability of Whisper-based Representations for \"In-the-Wild\" Cross-Task Downstream Speech Applications. (arXiv:2305.14546v1 [eess.AS])",
    "abstract": "Large self-supervised pre-trained speech models have achieved remarkable success across various speech-processing tasks. The self-supervised training of these models leads to universal speech representations that can be used for different downstream tasks, ranging from automatic speech recognition (ASR) to speaker identification. Recently, Whisper, a transformer-based model was proposed and trained on large amount of weakly supervised data for ASR; it outperformed several state-of-the-art self-supervised models. Given the superiority of Whisper for ASR, in this paper we explore the transferability of the representation for four other speech tasks in SUPERB benchmark. Moreover, we explore the robustness of Whisper representation for ``in the wild'' tasks where speech is corrupted by environment noise and room reverberation. Experimental results show Whisper achieves promising results across tasks and environmental conditions, thus showing potential for cross-task real-world deployment.",
    "link": "http://arxiv.org/abs/2305.14546",
    "context": "Title: On the Transferability of Whisper-based Representations for \"In-the-Wild\" Cross-Task Downstream Speech Applications. (arXiv:2305.14546v1 [eess.AS])\nAbstract: Large self-supervised pre-trained speech models have achieved remarkable success across various speech-processing tasks. The self-supervised training of these models leads to universal speech representations that can be used for different downstream tasks, ranging from automatic speech recognition (ASR) to speaker identification. Recently, Whisper, a transformer-based model was proposed and trained on large amount of weakly supervised data for ASR; it outperformed several state-of-the-art self-supervised models. Given the superiority of Whisper for ASR, in this paper we explore the transferability of the representation for four other speech tasks in SUPERB benchmark. Moreover, we explore the robustness of Whisper representation for ``in the wild'' tasks where speech is corrupted by environment noise and room reverberation. Experimental results show Whisper achieves promising results across tasks and environmental conditions, thus showing potential for cross-task real-world deployment.",
    "path": "papers/23/05/2305.14546.json",
    "total_tokens": 948,
    "translated_title": "基于Whisper的表示在各种交叉任务下的可迁移性研究",
    "translated_abstract": "大型自监督预训练的语音模型在各种语音处理任务中取得了显着的成功。这些模型的自监督训练导致了可用于不同下游任务的通用语音表示，从自动语音识别(ASR)到说话人识别等任务。最近，提出了一个基于变压器的模型Whisper，通过大量的弱监督数据进行了ASR的训练； 它优于几种现有的最先进的自监督模型。鉴于Whisper在ASR方面的优越性，在这篇论文中，我们探讨了该表示在SUPERB基准测试中其他四个语音任务的可迁移性。此外，我们还探讨了Whisper表示在被环境噪声和房间混响破坏的“野外”任务中的鲁棒性。实验结果表明，Whisper在各种任务和环境条件下都取得了有希望的结果，因此展示了在跨任务的真实环境部署方面的潜力。",
    "tldr": "Whisper模型是一个基于变压器的模型，可用于各种语音任务。本篇文章探讨了Whisper表示在其他四种语音任务和“野外”任务中的可迁移性和鲁棒性。实验结果表明，Whisper具有跨任务的真实环境部署潜力。",
    "en_tdlr": "The Whisper model, based on transformer architecture, has shown strong performance on various speech tasks. This paper investigates the transferability and robustness of Whisper representations for four other speech tasks and \"in the wild\" scenarios with environmental noise and room reverberation. Experimental results demonstrate the potential of Whisper for cross-task real-world deployment."
}