{
    "title": "Extending Memory for Language Modelling. (arXiv:2305.11462v1 [cs.CL])",
    "abstract": "Breakthroughs in deep learning and memory networks have made major advances in natural language understanding. Language is sequential and information carried through the sequence can be captured through memory networks. Learning the sequence is one of the key aspects in learning the language. However, memory networks are not capable of holding infinitely long sequences in their memories and are limited by various constraints such as the vanishing or exploding gradient problem. Therefore, natural language understanding models are affected when presented with long sequential text. We introduce Long Term Memory network (LTM) to learn from infinitely long sequences. LTM gives priority to the current inputs to allow it to have a high impact. Language modeling is an important factor in natural language understanding. LTM was tested in language modeling, which requires long term memory. LTM is tested on Penn Tree bank dataset, Google Billion Word dataset and WikiText-2 dataset. We compare LTM",
    "link": "http://arxiv.org/abs/2305.11462",
    "context": "Title: Extending Memory for Language Modelling. (arXiv:2305.11462v1 [cs.CL])\nAbstract: Breakthroughs in deep learning and memory networks have made major advances in natural language understanding. Language is sequential and information carried through the sequence can be captured through memory networks. Learning the sequence is one of the key aspects in learning the language. However, memory networks are not capable of holding infinitely long sequences in their memories and are limited by various constraints such as the vanishing or exploding gradient problem. Therefore, natural language understanding models are affected when presented with long sequential text. We introduce Long Term Memory network (LTM) to learn from infinitely long sequences. LTM gives priority to the current inputs to allow it to have a high impact. Language modeling is an important factor in natural language understanding. LTM was tested in language modeling, which requires long term memory. LTM is tested on Penn Tree bank dataset, Google Billion Word dataset and WikiText-2 dataset. We compare LTM",
    "path": "papers/23/05/2305.11462.json",
    "total_tokens": 960,
    "translated_title": "扩展语言模型的记忆容量",
    "translated_abstract": "深度学习和记忆网络的突破使自然语言理解取得了重大进展。语言是连续的，通过序列传递的信息可以通过记忆网络捕获。学习序列是学习语言的关键因素之一。然而，记忆网络不能在其内存中持有无限长的序列，并受到各种约束的限制，例如消失或爆炸梯度问题。因此，当面对长序列文本时，自然语言理解模型会受到影响。我们引入了长期记忆网络（LTM），以从无限长的序列中学习。 LTM 重视当前输入，以使其具有高影响力。语言建模是自然语言理解的重要因素。 LTM 在需要长期记忆的语言建模中进行了测试。 LTM 在 Penn Tree Bank 数据集，Google Billion字数据集和WikiText-2数据集上进行了测试。我们将 LTM 与其他最先进的模型进行比较，并表明 LTM 在所有三个数据集上的困惑度都比它们低，且参数更少。此外，我们还展示了 LTM 可用于文本生成。",
    "tldr": "本论文介绍了一种名为“长期记忆网络”的方法，可用于从无限长序列中学习，以扩展归纳语言建模的记忆容量，并在三个数据集上的测试中表现优异。该方法还可用于文本生成。",
    "en_tdlr": "This paper introduces a method called \"Long Term Memory network\" (LTM) that learns from infinitely long sequences to extend memory capacity for inductive language modeling. LTM outperforms other state-of-the-art models in perplexity on three datasets and can also be used for text generation."
}