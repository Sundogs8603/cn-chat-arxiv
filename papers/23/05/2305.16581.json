{
    "title": "An Investigation of Noise in Morphological Inflection. (arXiv:2305.16581v1 [cs.CL])",
    "abstract": "With a growing focus on morphological inflection systems for languages where high-quality data is scarce, training data noise is a serious but so far largely ignored concern. We aim at closing this gap by investigating the types of noise encountered within a pipeline for truly unsupervised morphological paradigm completion and its impact on morphological inflection systems: First, we propose an error taxonomy and annotation pipeline for inflection training data. Then, we compare the effect of different types of noise on multiple state-of-the-art inflection models. Finally, we propose a novel character-level masked language modeling (CMLM) pretraining objective and explore its impact on the models' resistance to noise. Our experiments show that various architectures are impacted differently by separate types of noise, but encoder-decoders tend to be more robust to noise than models trained with a copy bias. CMLM pretraining helps transformers, but has lower impact on LSTMs.",
    "link": "http://arxiv.org/abs/2305.16581",
    "context": "Title: An Investigation of Noise in Morphological Inflection. (arXiv:2305.16581v1 [cs.CL])\nAbstract: With a growing focus on morphological inflection systems for languages where high-quality data is scarce, training data noise is a serious but so far largely ignored concern. We aim at closing this gap by investigating the types of noise encountered within a pipeline for truly unsupervised morphological paradigm completion and its impact on morphological inflection systems: First, we propose an error taxonomy and annotation pipeline for inflection training data. Then, we compare the effect of different types of noise on multiple state-of-the-art inflection models. Finally, we propose a novel character-level masked language modeling (CMLM) pretraining objective and explore its impact on the models' resistance to noise. Our experiments show that various architectures are impacted differently by separate types of noise, but encoder-decoders tend to be more robust to noise than models trained with a copy bias. CMLM pretraining helps transformers, but has lower impact on LSTMs.",
    "path": "papers/23/05/2305.16581.json",
    "total_tokens": 980,
    "translated_title": "语态变化中噪声的调查",
    "translated_abstract": "随着对语态细微变化系统的关注越来越多，而这些语言缺乏高质量的数据，训练中的噪声是一个严重但迄今为止很少被关注的问题。我们旨在通过调查真正无监督的形态范式完成管道中遇到的噪声类型并探索其对语态变化系统的影响来弥合这一差距：首先，我们提出了一个错误分类法和注释管道的活动训练数据。然后，我们比较了不同类型噪音对多种最先进的语态变化模型的影响。最后，我们提出了一种新的字符级掩码语言建模（CMLM）预训练目标，并探索了它对模型抗噪性的影响。我们的实验证明，各种体系结构受到不同类型的噪声的影响不同，但编码器-解码器比带有复制偏差的模型更为稳健。CMLM预训练有助于transformers，但对LSTMs的影响较低。",
    "tldr": "该研究调查了语态变化中噪声的不同类型，并探索了它们对非监督形态学范式完成和语态变化系统的影响。实验发现编码器解码器比复制偏差的模型更为稳健，CMLM预训练可提高transformer模型的稳定性。",
    "en_tdlr": "This research investigates the impact of noise on unsupervised morphological paradigm completion and morphological inflection systems in languages with scarce data. The study proposes an error taxonomy and annotation pipeline for inflection training data while exploring the effect of different noise types on state-of-the-art inflection models. The results indicate that encoder-decoders are more robust to noise than models with a copy bias, and CMLM pretraining improves the stability of transformers."
}