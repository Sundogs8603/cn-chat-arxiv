{
    "title": "The Web Can Be Your Oyster for Improving Large Language Models. (arXiv:2305.10998v1 [cs.CL])",
    "abstract": "Large language models (LLMs) encode a large amount of world knowledge. However, as such knowledge is frozen at the time of model training, the models become static and limited by the training data at that time. In order to further improve the capacity of LLMs for knowledge-intensive tasks, we consider augmenting LLMs with the large-scale web using search engine. Unlike previous augmentation sources (e.g., Wikipedia data dump), the web provides broader, more comprehensive and constantly updated information. In this paper, we present a web-augmented LLM UNIWEB, which is trained over 16 knowledge-intensive tasks in a unified text-to-text format. Instead of simply using the retrieved contents from web, our approach has made two major improvements. Firstly, we propose an adaptive search engine assisted learning method that can self-evaluate the confidence level of LLM's predictions, and adaptively determine when to refer to the web for more data, which can avoid useless or noisy augmentatio",
    "link": "http://arxiv.org/abs/2305.10998",
    "context": "Title: The Web Can Be Your Oyster for Improving Large Language Models. (arXiv:2305.10998v1 [cs.CL])\nAbstract: Large language models (LLMs) encode a large amount of world knowledge. However, as such knowledge is frozen at the time of model training, the models become static and limited by the training data at that time. In order to further improve the capacity of LLMs for knowledge-intensive tasks, we consider augmenting LLMs with the large-scale web using search engine. Unlike previous augmentation sources (e.g., Wikipedia data dump), the web provides broader, more comprehensive and constantly updated information. In this paper, we present a web-augmented LLM UNIWEB, which is trained over 16 knowledge-intensive tasks in a unified text-to-text format. Instead of simply using the retrieved contents from web, our approach has made two major improvements. Firstly, we propose an adaptive search engine assisted learning method that can self-evaluate the confidence level of LLM's predictions, and adaptively determine when to refer to the web for more data, which can avoid useless or noisy augmentatio",
    "path": "papers/23/05/2305.10998.json",
    "total_tokens": 926,
    "translated_title": "网络可以为改进大语言模型提供更多的可选资源",
    "translated_abstract": "大型语言模型(LLMs)可以编码大量的世界知识。然而，由于这些知识在模型训练时已经固化，这些模型变得静态，并且受到了当时训练数据的限制。为了进一步提高LLMs在知识密集型任务中的能力，我们考虑使用搜索引擎将LLMs与海量网络进行融合增强。与以往的增强来源（如维基百科数据转储）不同，网络提供了更广泛、更全面且不断更新的信息。在本文中，我们提出了一个名为UNIWEB的网络增强LLM，它以统一的文本到文本格式在16个知识密集型任务上进行训练。我们的方法并不是简单地使用从网络检索到的内容，而是提出了一种自适应的搜索引擎辅助学习方法，该方法可以自我评估LLM的预测置信度，并自适应地确定何时参考网络获取更多数据，从而避免无用或嘈杂的增强。",
    "tldr": "本文提出了一种新的大型语言模型增强方法，即使用自适应搜索引擎辅助学习方法，将大规模网络数据与LLMs融合，从而避免无用或嘈杂的增强。",
    "en_tdlr": "This paper proposes a new method for enhancing large language models by integrating them with web data using an adaptive search engine assisted learning method, which can evaluate the confidence level of predictions and determine when to refer to the web for more data, thus avoiding useless or noisy augmentation."
}