{
    "title": "Boosting Distributed Machine Learning Training Through Loss-tolerant Transmission Protocol. (arXiv:2305.04279v1 [cs.DC] CROSS LISTED)",
    "abstract": "Distributed Machine Learning (DML) systems are utilized to enhance the speed of model training in data centers (DCs) and edge nodes. The Parameter Server (PS) communication architecture is commonly employed, but it faces severe long-tail latency caused by many-to-one \"incast\" traffic patterns, negatively impacting training throughput. To address this challenge, we design the \\textbf{L}oss-tolerant \\textbf{T}ransmission \\textbf{P}rotocol (LTP), which permits partial loss of gradients during synchronization to avoid unneeded retransmission and contributes to faster synchronization per iteration. LTP implements loss-tolerant transmission through \\textit{out-of-order transmission} and \\textit{out-of-order Acknowledges (ACKs)}. LTP employs \\textit{Early Close} to adjust the loss-tolerant threshold based on network conditions and \\textit{Bubble Filling} for data correction to maintain training accuracy. LTP is implemented by C++ and integrated into PyTorch. Evaluations on a testbed of 8 work",
    "link": "http://arxiv.org/abs/2305.04279",
    "context": "Title: Boosting Distributed Machine Learning Training Through Loss-tolerant Transmission Protocol. (arXiv:2305.04279v1 [cs.DC] CROSS LISTED)\nAbstract: Distributed Machine Learning (DML) systems are utilized to enhance the speed of model training in data centers (DCs) and edge nodes. The Parameter Server (PS) communication architecture is commonly employed, but it faces severe long-tail latency caused by many-to-one \"incast\" traffic patterns, negatively impacting training throughput. To address this challenge, we design the \\textbf{L}oss-tolerant \\textbf{T}ransmission \\textbf{P}rotocol (LTP), which permits partial loss of gradients during synchronization to avoid unneeded retransmission and contributes to faster synchronization per iteration. LTP implements loss-tolerant transmission through \\textit{out-of-order transmission} and \\textit{out-of-order Acknowledges (ACKs)}. LTP employs \\textit{Early Close} to adjust the loss-tolerant threshold based on network conditions and \\textit{Bubble Filling} for data correction to maintain training accuracy. LTP is implemented by C++ and integrated into PyTorch. Evaluations on a testbed of 8 work",
    "path": "papers/23/05/2305.04279.json",
    "total_tokens": 974,
    "translated_title": "通过容忍丢失的传输协议提升分布式机器学习训练速度",
    "translated_abstract": "分布式机器学习（DML）系统被用于提高数据中心和边缘节点中模型训练的速度。参数服务器（PS）通信架构常被采用，但由于多对一的“incast”流量模式导致了严重的长尾延迟，对训练吞吐量产生了负面影响。为了解决这个问题，我们设计了“容忍丢失传输协议”（LTP），它允许在同步过程中部分丢失梯度，以避免不必要的重传，并提高每次迭代的同步速度。LTP通过“乱序传输”（out-of-order transmission）和“乱序确认”（out-of-order ACKs）来实现容忍丢失的传输。LTP利用“提前关闭”（Early Close）根据网络条件调整容忍丢失的阈值，并使用“填充气泡”（Bubble Filling）进行数据校正以保持训练精度。LTP由C++实现并集成到PyTorch中。我们在一个由8个工作节点组成的实验平台上进行了评估。",
    "tldr": "通过设计容忍丢失传输协议（LTP），提高了分布式机器学习训练的速度和吞吐量，该协议允许部分梯度丢失，并通过乱序传输和乱序确认进行实现。",
    "en_tdlr": "By designing the Loss-tolerant Transmission Protocol (LTP), this paper improves the speed and throughput of distributed machine learning training. LTP allows for partial gradient loss and implements loss-tolerant transmission through out-of-order transmission and out-of-order ACKs."
}