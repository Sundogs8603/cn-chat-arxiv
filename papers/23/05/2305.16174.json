{
    "title": "From Latent Graph to Latent Topology Inference: Differentiable Cell Complex Module. (arXiv:2305.16174v2 [cs.LG] UPDATED)",
    "abstract": "Latent Graph Inference (LGI) relaxed the reliance of Graph Neural Networks (GNNs) on a given graph topology by dynamically learning it. However, most of LGI methods assume to have a (noisy, incomplete, improvable, ...) input graph to rewire and can solely learn regular graph topologies. In the wake of the success of Topological Deep Learning (TDL), we study Latent Topology Inference (LTI) for learning higher-order cell complexes (with sparse and not regular topology) describing multi-way interactions between data points. To this aim, we introduce the Differentiable Cell Complex Module (DCM), a novel learnable function that computes cell probabilities in the complex to improve the downstream task. We show how to integrate DCM with cell complex message passing networks layers and train it in a end-to-end fashion, thanks to a two-step inference procedure that avoids an exhaustive search across all possible cells in the input, thus maintaining scalability. Our model is tested on several ho",
    "link": "http://arxiv.org/abs/2305.16174",
    "context": "Title: From Latent Graph to Latent Topology Inference: Differentiable Cell Complex Module. (arXiv:2305.16174v2 [cs.LG] UPDATED)\nAbstract: Latent Graph Inference (LGI) relaxed the reliance of Graph Neural Networks (GNNs) on a given graph topology by dynamically learning it. However, most of LGI methods assume to have a (noisy, incomplete, improvable, ...) input graph to rewire and can solely learn regular graph topologies. In the wake of the success of Topological Deep Learning (TDL), we study Latent Topology Inference (LTI) for learning higher-order cell complexes (with sparse and not regular topology) describing multi-way interactions between data points. To this aim, we introduce the Differentiable Cell Complex Module (DCM), a novel learnable function that computes cell probabilities in the complex to improve the downstream task. We show how to integrate DCM with cell complex message passing networks layers and train it in a end-to-end fashion, thanks to a two-step inference procedure that avoids an exhaustive search across all possible cells in the input, thus maintaining scalability. Our model is tested on several ho",
    "path": "papers/23/05/2305.16174.json",
    "total_tokens": 1009,
    "translated_title": "从潜在图到潜在拓扑推断：可微分的单复形模块",
    "translated_abstract": "潜在图推断（LGI）通过动态学习来减少图神经网络（GNNs）对给定图拓扑的依赖。然而，大多数LGI方法假设存在（噪声、不完整、可改进的...）输入图来重新连接，并且只能学习常规的图拓扑。在拓扑深度学习（TDL）取得成功之后，我们研究了用于学习描述数据点之间多向交互的高阶单复形（具有稀疏且不规则的拓扑结构）的潜在拓扑推断（LTI）。为此，我们引入了可微分的单复形模块（DCM），一种计算复杂中单元概率的新型可学习函数，用于改进下游任务。我们展示了如何将DCM与单复形消息传递网络层集成，以及如何通过两步推断过程在端到端的方式进行训练，避免对输入中所有可能的单元进行详尽搜索，从而保持可伸缩性。我们的模型在多个高阶拓扑推断任务上进行了测试。",
    "tldr": "该论文研究了一种从潜在图到潜在拓扑推断的方法，通过引入可微分的单复形模块（DCM），学习描述数据点之间多向交互的高阶单复形的稀疏且不规则的拓扑结构，并展示了如何将其与单复形消息传递网络层集成以提高下游任务的效果。",
    "en_tdlr": "This paper introduces a method for latent topology inference by learning sparse and irregular topological structures of higher-order cell complexes describing multi-way interactions between data points, through the use of a differentiable cell complex module (DCM). Integration of DCM with cell complex message passing networks layers improves downstream tasks."
}