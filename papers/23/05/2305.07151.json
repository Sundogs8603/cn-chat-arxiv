{
    "title": "Overinformative Question Answering by Humans and Machines. (arXiv:2305.07151v1 [cs.CL])",
    "abstract": "When faced with a polar question, speakers often provide overinformative answers going beyond a simple \"yes\" or \"no\". But what principles guide the selection of additional information? In this paper, we provide experimental evidence from two studies suggesting that overinformativeness in human answering is driven by considerations of relevance to the questioner's goals which they flexibly adjust given the functional context in which the question is uttered. We take these human results as a strong benchmark for investigating question-answering performance in state-of-the-art neural language models, conducting an extensive evaluation on items from human experiments. We find that most models fail to adjust their answering behavior in a human-like way and tend to include irrelevant information. We show that GPT-3 is highly sensitive to the form of the prompt and only achieves human-like answer patterns when guided by an example and cognitively-motivated explanation.",
    "link": "http://arxiv.org/abs/2305.07151",
    "context": "Title: Overinformative Question Answering by Humans and Machines. (arXiv:2305.07151v1 [cs.CL])\nAbstract: When faced with a polar question, speakers often provide overinformative answers going beyond a simple \"yes\" or \"no\". But what principles guide the selection of additional information? In this paper, we provide experimental evidence from two studies suggesting that overinformativeness in human answering is driven by considerations of relevance to the questioner's goals which they flexibly adjust given the functional context in which the question is uttered. We take these human results as a strong benchmark for investigating question-answering performance in state-of-the-art neural language models, conducting an extensive evaluation on items from human experiments. We find that most models fail to adjust their answering behavior in a human-like way and tend to include irrelevant information. We show that GPT-3 is highly sensitive to the form of the prompt and only achieves human-like answer patterns when guided by an example and cognitively-motivated explanation.",
    "path": "papers/23/05/2305.07151.json",
    "total_tokens": 864,
    "translated_title": "人类和机器的过度信息提供问答",
    "translated_abstract": "当面临一个极性问题时，说话者经常提供超出简单的“是”或“否”的过度信息性答案。但是什么原则指导了选择额外的信息？在本文中，我们提供了两项研究的实验证据，表明人类回答的过度信息性是由于考虑到对问者目标的相关性，他们在话语的功能环境中灵活地调整。我们将这些人类结果作为评估最先进的神经语言模型问答表现的强有力基准，对人类实验项目进行了广泛的评估。我们发现，大多数模型未能以类似人类的方式调整其回答行为，而倾向于包含无关的信息。我们表明，GPT-3对提示形式非常敏感，只有在有例子和以认知为基础的解释指导时，才能实现类似人类的答案模式。",
    "tldr": "人类在回答问题时会考虑提供额外信息的相关性，而大多数神经语言模型则未能以类似人类的方式进行回答，GPT-3仅在有示例和认知基础解释指导时才能实现人类一样的答案模式。",
    "en_tdlr": "Humans provide overinformative answers driven by relevance to the questioner's goals, while most neural language models fail to adjust their behavior similarly. GPT-3 only achieves human-like answer patterns when guided by an example and cognitively-motivated explanation."
}