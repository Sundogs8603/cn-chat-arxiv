{
    "title": "Rethinking Population-assisted Off-policy Reinforcement Learning. (arXiv:2305.02949v1 [cs.LG])",
    "abstract": "While off-policy reinforcement learning (RL) algorithms are sample efficient due to gradient-based updates and data reuse in the replay buffer, they struggle with convergence to local optima due to limited exploration. On the other hand, population-based algorithms offer a natural exploration strategy, but their heuristic black-box operators are inefficient. Recent algorithms have integrated these two methods, connecting them through a shared replay buffer. However, the effect of using diverse data from population optimization iterations on off-policy RL algorithms has not been thoroughly investigated. In this paper, we first analyze the use of off-policy RL algorithms in combination with population-based algorithms, showing that the use of population data could introduce an overlooked error and harm performance. To test this, we propose a uniform and scalable training design and conduct experiments on our tailored framework in robot locomotion tasks from the OpenAI gym. Our results su",
    "link": "http://arxiv.org/abs/2305.02949",
    "context": "Title: Rethinking Population-assisted Off-policy Reinforcement Learning. (arXiv:2305.02949v1 [cs.LG])\nAbstract: While off-policy reinforcement learning (RL) algorithms are sample efficient due to gradient-based updates and data reuse in the replay buffer, they struggle with convergence to local optima due to limited exploration. On the other hand, population-based algorithms offer a natural exploration strategy, but their heuristic black-box operators are inefficient. Recent algorithms have integrated these two methods, connecting them through a shared replay buffer. However, the effect of using diverse data from population optimization iterations on off-policy RL algorithms has not been thoroughly investigated. In this paper, we first analyze the use of off-policy RL algorithms in combination with population-based algorithms, showing that the use of population data could introduce an overlooked error and harm performance. To test this, we propose a uniform and scalable training design and conduct experiments on our tailored framework in robot locomotion tasks from the OpenAI gym. Our results su",
    "path": "papers/23/05/2305.02949.json",
    "total_tokens": 1117,
    "translated_title": "重新思考基于种群协助的离策略强化学习",
    "translated_abstract": "离策略强化学习算法由于梯度更新和在回放缓冲区中的数据重用而具有高效的样本利用率，但由于有限的探索能力往往难以收敛到局部最优解。另一方面，基于种群的算法提供了一种自然的探索策略，但其启发式黑盒操作效率低下。近期的算法将这两种方法相结合，通过共享回放缓冲区连接它们。然而，种群优化迭代中使用多样化数据对离策略强化学习算法的影响尚未得到彻底的研究。本文首先分析了基于种群优化的离策略强化学习算法的使用，揭示了使用种群数据可能会引入被忽视的误差并影响性能。为了检验这一点，我们提出了一种统一和可扩展的训练设计，并在开放AI gym中的机器人运动任务中进行了实验。结果表明，在训练过程中添加种群数据确实会损害性能，而使用种群优化的最新数据作为离策略更新的校正项是一个简单而有效的解决方案。这个校正项可以在样本利用率和最终性能方面都显著改进。",
    "tldr": "本文旨在重新审视基于种群协助的离策略强化学习，通过实验证明了将来自种群优化迭代的多样化数据添加到离策略更新中会降低性能，并提出了一种简单而有效的解决方案，即仅使用种群优化的最新数据作为离策略更新的校正项，从而显著改善了样本利用率和最终性能。",
    "en_tdlr": "This paper aims to rethink population-assisted off-policy reinforcement learning, and demonstrates experimentally that adding diverse data from population optimization iterations to off-policy updates can harm performance, proposing a simple and effective solution of using only the most recent data as a correction term. This correction term substantially improves both sample efficiency and final performance."
}