{
    "title": "Enhancing Accuracy and Robustness through Adversarial Training in Class Incremental Continual Learning. (arXiv:2305.13678v1 [cs.LG])",
    "abstract": "In real life, adversarial attack to deep learning models is a fatal security issue. However, the issue has been rarely discussed in a widely used class-incremental continual learning (CICL). In this paper, we address problems of applying adversarial training to CICL, which is well-known defense method against adversarial attack. A well-known problem of CICL is class-imbalance that biases a model to the current task by a few samples of previous tasks. Meeting with the adversarial training, the imbalance causes another imbalance of attack trials over tasks. Lacking clean data of a minority class by the class-imbalance and increasing of attack trials from a majority class by the secondary imbalance, adversarial training distorts optimal decision boundaries. The distortion eventually decreases both accuracy and robustness than adversarial training. To exclude the effects, we propose a straightforward but significantly effective method, External Adversarial Training (EAT) which can be appli",
    "link": "http://arxiv.org/abs/2305.13678",
    "context": "Title: Enhancing Accuracy and Robustness through Adversarial Training in Class Incremental Continual Learning. (arXiv:2305.13678v1 [cs.LG])\nAbstract: In real life, adversarial attack to deep learning models is a fatal security issue. However, the issue has been rarely discussed in a widely used class-incremental continual learning (CICL). In this paper, we address problems of applying adversarial training to CICL, which is well-known defense method against adversarial attack. A well-known problem of CICL is class-imbalance that biases a model to the current task by a few samples of previous tasks. Meeting with the adversarial training, the imbalance causes another imbalance of attack trials over tasks. Lacking clean data of a minority class by the class-imbalance and increasing of attack trials from a majority class by the secondary imbalance, adversarial training distorts optimal decision boundaries. The distortion eventually decreases both accuracy and robustness than adversarial training. To exclude the effects, we propose a straightforward but significantly effective method, External Adversarial Training (EAT) which can be appli",
    "path": "papers/23/05/2305.13678.json",
    "total_tokens": 1053,
    "translated_title": "在类增量连续学习中通过对抗性训练提高准确性和鲁棒性。",
    "translated_abstract": "在现实生活中，针对深度学习模型的对抗攻击是一个致命的安全问题。然而，这个问题在广泛使用的类增量连续学习（Class Incremental Continual Learning，CICL）中很少被讨论。在本文中，我们解决了将对抗性训练应用于CICL的问题，这是一种强效的对抗攻击防御方法。CICL已知的一个问题是类别不平衡，它通过前几次任务的少量样本将模型偏向于当前任务。和对抗性训练相遇时，不平衡会导致任务的攻击次数出现另一种不平衡。由于类别不平衡缺乏少数类的干净数据，并且由于第二种不平衡增加了来自多数类的攻击次数，对抗性训练会扭曲最优决策边界。这种扭曲最终会降低准确性和鲁棒性，而不是提高。为了消除这些影响，我们提出了一种直接而显著有效的方法——外部对抗性训练（External Adversarial Training，EAT），它可以应用到CICL中。",
    "tldr": "本文针对在类增量连续学习中应用对抗性训练时出现的问题，提出了一种外部对抗性训练方法（EAT），可以避免类别不平衡和攻击样本的不平衡所导致的最优决策边界扭曲问题，从而提高深度学习模型的准确性和鲁棒性。",
    "en_tdlr": "This paper proposes External Adversarial Training (EAT) to address the problem of adversarial training in class-incremental continual learning (CICL). EAT can effectively avoid the distortion of optimal decision boundaries caused by class imbalance and the imbalance of attack trials over tasks, thus improving the accuracy and robustness of deep learning models."
}