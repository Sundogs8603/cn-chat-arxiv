{
    "title": "Self-Supervised Gaussian Regularization of Deep Classifiers for Mahalanobis-Distance-Based Uncertainty Estimation. (arXiv:2305.13849v1 [cs.CV])",
    "abstract": "Recent works show that the data distribution in a network's latent space is useful for estimating classification uncertainty and detecting Out-of-distribution (OOD) samples. To obtain a well-regularized latent space that is conducive for uncertainty estimation, existing methods bring in significant changes to model architectures and training procedures. In this paper, we present a lightweight, fast, and high-performance regularization method for Mahalanobis distance-based uncertainty prediction, and that requires minimal changes to the network's architecture. To derive Gaussian latent representation favourable for Mahalanobis Distance calculation, we introduce a self-supervised representation learning method that separates in-class representations into multiple Gaussians. Classes with non-Gaussian representations are automatically identified and dynamically clustered into multiple new classes that are approximately Gaussian. Evaluation on standard OOD benchmarks shows that our method a",
    "link": "http://arxiv.org/abs/2305.13849",
    "context": "Title: Self-Supervised Gaussian Regularization of Deep Classifiers for Mahalanobis-Distance-Based Uncertainty Estimation. (arXiv:2305.13849v1 [cs.CV])\nAbstract: Recent works show that the data distribution in a network's latent space is useful for estimating classification uncertainty and detecting Out-of-distribution (OOD) samples. To obtain a well-regularized latent space that is conducive for uncertainty estimation, existing methods bring in significant changes to model architectures and training procedures. In this paper, we present a lightweight, fast, and high-performance regularization method for Mahalanobis distance-based uncertainty prediction, and that requires minimal changes to the network's architecture. To derive Gaussian latent representation favourable for Mahalanobis Distance calculation, we introduce a self-supervised representation learning method that separates in-class representations into multiple Gaussians. Classes with non-Gaussian representations are automatically identified and dynamically clustered into multiple new classes that are approximately Gaussian. Evaluation on standard OOD benchmarks shows that our method a",
    "path": "papers/23/05/2305.13849.json",
    "total_tokens": 936,
    "translated_title": "基于自监督高斯正则化的深度分类器马氏距离不确定性评估",
    "translated_abstract": "近期的工作表明，网络的潜在空间中的数据分布对于估计分类不确定性和检测超出分布范围（OOD）的样本非常有用。为了获得适用于不确定性估计的良好正则化潜在空间，现有方法对模型架构和训练程序进行了重大改变。在本文中，我们提出了一种用于马氏距离基础不确定性预测的轻量级、快速、高性能正则化方法，并且对网络架构的改动要求最小。为了得到适用于马氏距离计算的高斯潜在表示，我们引入了一种自监督表示学习方法，将类内表示分为多个高斯。具有非高斯表示的类别被自动识别并动态聚类为多个大概率是高斯分布的类别。在标准OOD基准测试上的评估显示出，我们的方法实现了最先进的性能，同时保持轻量级和高效的模型架构。",
    "tldr": "本文提出了一种自监督高斯正则化的深度分类器，可用于马氏距离不确定性评估，相比现有方法，该方法不需要对模型架构和训练程序做出大的改变，并在标准OOD基准测试上取得了最先进的性能。",
    "en_tdlr": "This paper proposes a self-supervised Gaussian regularization method for deep classifiers to estimate uncertainty based on Mahalanobis distance. The method requires minimal changes to the model's architecture while achieving state-of-the-art performance on standard OOD benchmarks."
}