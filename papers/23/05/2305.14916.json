{
    "title": "CoinEM: Tuning-Free Particle-Based Variational Inference for Latent Variable Models. (arXiv:2305.14916v1 [stat.ML])",
    "abstract": "We introduce two new particle-based algorithms for learning latent variable models via marginal maximum likelihood estimation, including one which is entirely tuning-free. Our methods are based on the perspective of marginal maximum likelihood estimation as an optimization problem: namely, as the minimization of a free energy functional. One way to solve this problem is to consider the discretization of a gradient flow associated with the free energy. We study one such approach, which resembles an extension of the popular Stein variational gradient descent algorithm. In particular, we establish a descent lemma for this algorithm, which guarantees that the free energy decreases at each iteration. This method, and any other obtained as the discretization of the gradient flow, will necessarily depend on a learning rate which must be carefully tuned by the practitioner in order to ensure convergence at a suitable rate. With this in mind, we also propose another algorithm for optimizing the",
    "link": "http://arxiv.org/abs/2305.14916",
    "context": "Title: CoinEM: Tuning-Free Particle-Based Variational Inference for Latent Variable Models. (arXiv:2305.14916v1 [stat.ML])\nAbstract: We introduce two new particle-based algorithms for learning latent variable models via marginal maximum likelihood estimation, including one which is entirely tuning-free. Our methods are based on the perspective of marginal maximum likelihood estimation as an optimization problem: namely, as the minimization of a free energy functional. One way to solve this problem is to consider the discretization of a gradient flow associated with the free energy. We study one such approach, which resembles an extension of the popular Stein variational gradient descent algorithm. In particular, we establish a descent lemma for this algorithm, which guarantees that the free energy decreases at each iteration. This method, and any other obtained as the discretization of the gradient flow, will necessarily depend on a learning rate which must be carefully tuned by the practitioner in order to ensure convergence at a suitable rate. With this in mind, we also propose another algorithm for optimizing the",
    "path": "papers/23/05/2305.14916.json",
    "total_tokens": 902,
    "translated_title": "CoinEM：无需调参的基于粒子的潜变量模型变分推断方法",
    "translated_abstract": "本文提出两种基于粒子的新型算法，用于通过边际最大似然估计学习潜变量模型，其中一种完全无需调参。我们的方法基于将边际最大似然估计视为优化问题的角度：即将其视为自由能泛函的最小化。解决这个问题的一种方法是考虑自由能关联的梯度流的离散化。我们研究了一种类似于流行的 Stein 变分梯度下降算法的方法。特别地，我们为此算法建立了下降引理，保证了自由能在每次迭代中下降。但此方法和其他由梯度流的离散化得到的方法都必须依赖于学习率，该学习率必须由从业者仔细调整，以确保以合适的速率收敛。为此，我们还提出了另一种算法用于优化这个问题，该算法是完全无需调参的。",
    "tldr": "本文提出了两种无需调参的基于粒子的变分推断算法，其中一种是通过考虑边缘最大似然估计为自由能泛函最小化得到的，另一种是用于优化该问题的算法，完全无需调参。",
    "en_tdlr": "This paper proposes two tuning-free particle-based variational inference algorithms for learning latent variable models, with one obtained by considering marginal maximum likelihood estimation as the minimization of a free energy functional, and the other being a fully tuning-free algorithm for optimizing this problem."
}