{
    "title": "Benchmarking Diverse-Modal Entity Linking with Generative Models. (arXiv:2305.17337v1 [cs.CL])",
    "abstract": "Entities can be expressed in diverse formats, such as texts, images, or column names and cell values in tables. While existing entity linking (EL) models work well on per modality configuration, such as text-only EL, visual grounding, or schema linking, it is more challenging to design a unified model for diverse modality configurations. To bring various modality configurations together, we constructed a benchmark for diverse-modal EL (DMEL) from existing EL datasets, covering all three modalities including text, image, and table. To approach the DMEL task, we proposed a generative diverse-modal model (GDMM) following a multimodal-encoder-decoder paradigm. Pre-training \\Model with rich corpora builds a solid foundation for DMEL without storing the entire KB for inference. Fine-tuning GDMM builds a stronger DMEL baseline, outperforming state-of-the-art task-specific EL models by 8.51 F1 score on average. Additionally, extensive error analyses are conducted to highlight the challenges of",
    "link": "http://arxiv.org/abs/2305.17337",
    "context": "Title: Benchmarking Diverse-Modal Entity Linking with Generative Models. (arXiv:2305.17337v1 [cs.CL])\nAbstract: Entities can be expressed in diverse formats, such as texts, images, or column names and cell values in tables. While existing entity linking (EL) models work well on per modality configuration, such as text-only EL, visual grounding, or schema linking, it is more challenging to design a unified model for diverse modality configurations. To bring various modality configurations together, we constructed a benchmark for diverse-modal EL (DMEL) from existing EL datasets, covering all three modalities including text, image, and table. To approach the DMEL task, we proposed a generative diverse-modal model (GDMM) following a multimodal-encoder-decoder paradigm. Pre-training \\Model with rich corpora builds a solid foundation for DMEL without storing the entire KB for inference. Fine-tuning GDMM builds a stronger DMEL baseline, outperforming state-of-the-art task-specific EL models by 8.51 F1 score on average. Additionally, extensive error analyses are conducted to highlight the challenges of",
    "path": "papers/23/05/2305.17337.json",
    "total_tokens": 944,
    "translated_title": "用生成模型进行多模态实体链接的基准测试",
    "translated_abstract": "实体可以用不同的格式来表达，如文本、图像或表格中的列名和单元格值。虽然现有的实体链接（EL）模型在每种模式配置上都表现出色，例如仅文本EL、视觉定位或模式链接，但为多种模式配置设计统一模型更具挑战性。为了将各种模态配置结合起来，我们从现有EL数据集构建了一个包含文本、图像和表格三种模态的多模态EL（DMEL）基准测试。为了解决DMEL任务，我们提出了一个生成多模态模型（GDMM），遵循多模态编码器-解码器范例。将\\Model用丰富的语料库预训练可以在不保存整个KB进行推理的情况下为DMEL构建坚实的基础。微调GDMM可以构建更强大的DMEL基线，在平均F1分数上优于最先进的特定任务EL模型8.51分。此外，进行了广泛的错误分析，突出了将多模态实体翻译和所提出方法的优势所在。",
    "tldr": "新方法提出了一个多模态实体链接的基准测试，并使用预训练的生成多模态模型，在平均F1分数上优于最先进的特定任务EL模型8.51分。",
    "en_tdlr": "A new benchmark for diverse-modal entity linking (DMEL) was proposed and a generative diverse-modal model (GDMM) was used to outperform state-of-the-art task-specific EL models by 8.51 F1 score on average, with pre-training and fine-tuning approaches. The proposed method highlights the challenges of translating diverse-modal entities and its advantages."
}