{
    "title": "A Reminder of its Brittleness: Language Reward Shaping May Hinder Learning for Instruction Following Agents. (arXiv:2305.16621v1 [cs.AI])",
    "abstract": "Teaching agents to follow complex written instructions has been an important yet elusive goal. One technique for improving learning efficiency is language reward shaping (LRS), which is used in reinforcement learning (RL) to reward actions that represent progress towards a sparse reward. We argue that the apparent success of LRS is brittle, and prior positive findings can be attributed to weak RL baselines. Specifically, we identified suboptimal LRS designs that reward partially matched trajectories, and we characterised a novel type of reward perturbation that addresses this issue based on the concept of loosening task constraints. We provided theoretical and empirical evidence that agents trained using LRS rewards converge more slowly compared to pure RL agents.",
    "link": "http://arxiv.org/abs/2305.16621",
    "context": "Title: A Reminder of its Brittleness: Language Reward Shaping May Hinder Learning for Instruction Following Agents. (arXiv:2305.16621v1 [cs.AI])\nAbstract: Teaching agents to follow complex written instructions has been an important yet elusive goal. One technique for improving learning efficiency is language reward shaping (LRS), which is used in reinforcement learning (RL) to reward actions that represent progress towards a sparse reward. We argue that the apparent success of LRS is brittle, and prior positive findings can be attributed to weak RL baselines. Specifically, we identified suboptimal LRS designs that reward partially matched trajectories, and we characterised a novel type of reward perturbation that addresses this issue based on the concept of loosening task constraints. We provided theoretical and empirical evidence that agents trained using LRS rewards converge more slowly compared to pure RL agents.",
    "path": "papers/23/05/2305.16621.json",
    "total_tokens": 829,
    "translated_title": "语言奖励塑造可能影响指令遵循代理的学习：提醒其脆弱性",
    "translated_abstract": "教导代理程序遵守复杂的书面指令一直是一个重要而又难以实现的目标。提高学习效率的一种技术是语言奖励塑造（LRS），它在强化学习中用于奖励代表朝着稀疏奖励的进展方向的行动。我们认为，LRS的表面成功是脆弱的，并且之前的积极结果可能归因于弱的强化学习基线。具体而言，我们确定了奖励部分匹配轨迹的次优LRS设计，并基于放宽任务约束的概念，对一种新型的奖励扰动进行了表征以解决这个问题。我们提供了理论和实证证据表明，使用LRS奖励训练的代理程序较纯强化学习代理程序收敛速度较慢。",
    "tldr": "该论文研究表明，对于复杂指令遵循代理的学习，语言奖励塑造技术可能会影响代理程序的学习，其中表面上的成功可能是脆弱的。",
    "en_tdlr": "This paper suggests that the language reward shaping (LRS) technique may hinder the learning process of instruction following agents by providing brittle success, as they tend to have slower convergence rates compared to pure reinforcement learning agents."
}