{
    "title": "FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation. (arXiv:2305.14251v2 [cs.CL] UPDATED)",
    "abstract": "Evaluating the factuality of long-form text generated by large language models (LMs) is non-trivial because (1) generations often contain a mixture of supported and unsupported pieces of information, making binary judgments of quality inadequate, and (2) human evaluation is time-consuming and costly. In this paper, we introduce FACTSCORE, a new evaluation that breaks a generation into a series of atomic facts and computes the percentage of atomic facts supported by a reliable knowledge source. We conduct an extensive human evaluation to obtain FACTSCOREs of people biographies generated by several state-of-the-art commercial LMs -- InstructGPT, ChatGPT, and the retrieval-augmented PerplexityAI -- and report new analysis demonstrating the need for such a fine-grained score (e.g., ChatGPT only achieves 58%). Since human evaluation is costly, we also introduce an automated model that estimates FACTSCORE using retrieval and a strong language model, with less than a 2% error rate. Finally, w",
    "link": "http://arxiv.org/abs/2305.14251",
    "context": "Title: FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation. (arXiv:2305.14251v2 [cs.CL] UPDATED)\nAbstract: Evaluating the factuality of long-form text generated by large language models (LMs) is non-trivial because (1) generations often contain a mixture of supported and unsupported pieces of information, making binary judgments of quality inadequate, and (2) human evaluation is time-consuming and costly. In this paper, we introduce FACTSCORE, a new evaluation that breaks a generation into a series of atomic facts and computes the percentage of atomic facts supported by a reliable knowledge source. We conduct an extensive human evaluation to obtain FACTSCOREs of people biographies generated by several state-of-the-art commercial LMs -- InstructGPT, ChatGPT, and the retrieval-augmented PerplexityAI -- and report new analysis demonstrating the need for such a fine-grained score (e.g., ChatGPT only achieves 58%). Since human evaluation is costly, we also introduce an automated model that estimates FACTSCORE using retrieval and a strong language model, with less than a 2% error rate. Finally, w",
    "path": "papers/23/05/2305.14251.json",
    "total_tokens": 1027,
    "translated_title": "FActScore: 对长文本生成中事实准确性的细粒度原子评估",
    "translated_abstract": "评估大型语言模型生成的长文本的事实性是一项棘手的任务，因为（1）生成的内容通常包含支持和不支持的信息，使得二元判断质量不足，（2）人工评估耗时且成本高。本文介绍了FACTSCORE，一种新的评估方法，它将生成内容分解为一系列原子事实，并计算被可靠知识源支持的原子事实的百分比。我们进行了广泛的人工评估，得出了几个最先进商业语言模型（InstructGPT、ChatGPT和增强提取PerplexityAI）生成的人物传记的FACTSCORE，并报道了新的分析结果，证明了对于这样的细粒度评分的需求（例如，ChatGPT只达到58%）。由于人工评估费时费力，我们还引入了一种使用检索和强语言模型估计FACTSCORE的自动化模型，误差率不超过2%。",
    "tldr": "本文介绍了一种称为FACTSCORE的评估方法，它通过将生成的内容分解为原子事实，并计算被可靠知识源支持的比例来评估大型语言模型生成的长文本的事实准确性。通过广泛的人工评估，我们发现商业语言模型中仅有58%的ChatGPT传记达到了高水平的事实准确性。此外，我们还引入了一种自动化模型，利用检索和强语言模型估计FACTSCORE，误差率低于2%。",
    "en_tdlr": "This paper introduces FACTSCORE, an evaluation method for assessing the factuality of long-form text generated by large language models. It breaks the generation down into atomic facts and calculates the percentage of supported atomic facts. The study shows that only 58% of biographies generated by ChatGPT achieve high factual precision. A automated model is also proposed with less than a 2% error rate to estimate FACTSCORE."
}