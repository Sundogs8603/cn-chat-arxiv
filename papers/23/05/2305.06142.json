{
    "title": "Feature Expansion for Graph Neural Networks. (arXiv:2305.06142v1 [cs.LG])",
    "abstract": "Graph neural networks aim to learn representations for graph-structured data and show impressive performance, particularly in node classification. Recently, many methods have studied the representations of GNNs from the perspective of optimization goals and spectral graph theory. However, the feature space that dominates representation learning has not been systematically studied in graph neural networks. In this paper, we propose to fill this gap by analyzing the feature space of both spatial and spectral models. We decompose graph neural networks into determined feature spaces and trainable weights, providing the convenience of studying the feature space explicitly using matrix space analysis. In particular, we theoretically find that the feature space tends to be linearly correlated due to repeated aggregations. Motivated by these findings, we propose 1) feature subspaces flattening and 2) structural principal components to expand the feature space. Extensive experiments verify the ",
    "link": "http://arxiv.org/abs/2305.06142",
    "context": "Title: Feature Expansion for Graph Neural Networks. (arXiv:2305.06142v1 [cs.LG])\nAbstract: Graph neural networks aim to learn representations for graph-structured data and show impressive performance, particularly in node classification. Recently, many methods have studied the representations of GNNs from the perspective of optimization goals and spectral graph theory. However, the feature space that dominates representation learning has not been systematically studied in graph neural networks. In this paper, we propose to fill this gap by analyzing the feature space of both spatial and spectral models. We decompose graph neural networks into determined feature spaces and trainable weights, providing the convenience of studying the feature space explicitly using matrix space analysis. In particular, we theoretically find that the feature space tends to be linearly correlated due to repeated aggregations. Motivated by these findings, we propose 1) feature subspaces flattening and 2) structural principal components to expand the feature space. Extensive experiments verify the ",
    "path": "papers/23/05/2305.06142.json",
    "total_tokens": 796,
    "translated_title": "图神经网络的特征扩展",
    "translated_abstract": "图神经网络旨在学习图结构数据的表示，并展现出令人瞩目的性能，尤其在节点分类方面。然而，支配表示学习的特征空间在图神经网络中尚未被系统地研究。本文提出通过分析空间模型和谱模型的特征空间来填补这一空白。我们将图神经网络分解为确定的特征空间和可训练的权重，从而通过矩阵空间分析明确地研究特征空间。特别地，我们在理论上发现，由于重复聚合，特征空间倾向于线性相关。基于这些发现，我们提出1）特征子空间展开和2）结构主成分来扩展特征空间。广泛的实验验证了我们的方法，取得了更好的结果。",
    "tldr": "本文通过分析图神经网络中的特征空间，提出了特征子空间展开和结构主成分两种方法来扩展特征空间，从而获得更好的结果。",
    "en_tdlr": "This paper proposes feature subspaces flattening and structural principal components to expand the feature space in graph neural networks, based on the analysis of the feature space of both spatial and spectral models, achieving better results in extensive experiments."
}