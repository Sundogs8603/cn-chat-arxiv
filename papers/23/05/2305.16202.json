{
    "title": "DP-SGD Without Clipping: The Lipschitz Neural Network Way",
    "abstract": "arXiv:2305.16202v2 Announce Type: replace  Abstract: State-of-the-art approaches for training Differentially Private (DP) Deep Neural Networks (DNN) face difficulties to estimate tight bounds on the sensitivity of the network's layers, and instead rely on a process of per-sample gradient clipping. This clipping process not only biases the direction of gradients but also proves costly both in memory consumption and in computation. To provide sensitivity bounds and bypass the drawbacks of the clipping process, we propose to rely on Lipschitz constrained networks. Our theoretical analysis reveals an unexplored link between the Lipschitz constant with respect to their input and the one with respect to their parameters. By bounding the Lipschitz constant of each layer with respect to its parameters, we prove that we can train these networks with privacy guarantees. Our analysis not only allows the computation of the aforementioned sensitivities at scale, but also provides guidance on how to",
    "link": "https://arxiv.org/abs/2305.16202",
    "context": "Title: DP-SGD Without Clipping: The Lipschitz Neural Network Way\nAbstract: arXiv:2305.16202v2 Announce Type: replace  Abstract: State-of-the-art approaches for training Differentially Private (DP) Deep Neural Networks (DNN) face difficulties to estimate tight bounds on the sensitivity of the network's layers, and instead rely on a process of per-sample gradient clipping. This clipping process not only biases the direction of gradients but also proves costly both in memory consumption and in computation. To provide sensitivity bounds and bypass the drawbacks of the clipping process, we propose to rely on Lipschitz constrained networks. Our theoretical analysis reveals an unexplored link between the Lipschitz constant with respect to their input and the one with respect to their parameters. By bounding the Lipschitz constant of each layer with respect to its parameters, we prove that we can train these networks with privacy guarantees. Our analysis not only allows the computation of the aforementioned sensitivities at scale, but also provides guidance on how to",
    "path": "papers/23/05/2305.16202.json",
    "total_tokens": 865,
    "translated_title": "无剪切的DP-SGD：利普希茨神经网络方式",
    "translated_abstract": "最先进的训练差分隐私（DP）深度神经网络（DNN）方法在估计网络层的灵敏度上遇到困难，而是依赖于每个样本的梯度剪切过程。本文提出依赖于利普希茨约束网络来提供灵敏度界限并规避剪切过程的缺点。我们的理论分析揭示了与其参数相关的利普希茨常数与其输入相关的利普希茨常数之间的未开发的联系。通过限制每一层相对于其参数的利普希茨常数，我们证明可以训练这些具有隐私保证的网络。我们的分析不仅使规模化地计算上述灵敏度成为可能，还提供了如何进行的指导。",
    "tldr": "提出一种无需剪切的DP-SGD训练方法，依赖于利普希茨约束网络提供灵敏度界限，规避了剪切过程的缺点，并证明了可以通过限制每一层相对于其参数的利普希茨常数来训练这些具有隐私保证的网络。",
    "en_tdlr": "Propose a DP-SGD training method without clipping, relying on Lipschitz constrained networks to provide sensitivity bounds, avoid the drawbacks of clipping process, and prove the training of these networks with privacy guarantees by bounding the Lipschitz constant of each layer with respect to its parameters."
}