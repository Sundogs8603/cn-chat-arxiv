{
    "title": "How does the task complexity of masked pretraining objectives affect downstream performance?. (arXiv:2305.10992v1 [cs.CL])",
    "abstract": "Masked language modeling (MLM) is a widely used self-supervised pretraining objective, where a model needs to predict an original token that is replaced with a mask given contexts. Although simpler and computationally efficient pretraining objectives, e.g., predicting the first character of a masked token, have recently shown comparable results to MLM, no objectives with a masking scheme actually outperform it in downstream tasks. Motivated by the assumption that their lack of complexity plays a vital role in the degradation, we validate whether more complex masked objectives can achieve better results and investigate how much complexity they should have to perform comparably to MLM. Our results using GLUE, SQuAD, and Universal Dependencies benchmarks demonstrate that more complicated objectives tend to show better downstream results with at least half of the MLM complexity needed to perform comparably to MLM. Finally, we discuss how we should pretrain a model using a masked objective ",
    "link": "http://arxiv.org/abs/2305.10992",
    "context": "Title: How does the task complexity of masked pretraining objectives affect downstream performance?. (arXiv:2305.10992v1 [cs.CL])\nAbstract: Masked language modeling (MLM) is a widely used self-supervised pretraining objective, where a model needs to predict an original token that is replaced with a mask given contexts. Although simpler and computationally efficient pretraining objectives, e.g., predicting the first character of a masked token, have recently shown comparable results to MLM, no objectives with a masking scheme actually outperform it in downstream tasks. Motivated by the assumption that their lack of complexity plays a vital role in the degradation, we validate whether more complex masked objectives can achieve better results and investigate how much complexity they should have to perform comparably to MLM. Our results using GLUE, SQuAD, and Universal Dependencies benchmarks demonstrate that more complicated objectives tend to show better downstream results with at least half of the MLM complexity needed to perform comparably to MLM. Finally, we discuss how we should pretrain a model using a masked objective ",
    "path": "papers/23/05/2305.10992.json",
    "total_tokens": 896,
    "translated_title": "掩码预训练任务的复杂度如何影响下游任务表现？",
    "translated_abstract": "掩码语言建模是一种广泛使用的自监督预训练任务，其中模型需要预测替换上下文中的原始token的掩码。尽管最近使用更简单且计算较少的预训练任务，例如预测掩码标记的第一个字符，已经表现出与掩码语言建模相当的结果，但使用掩码方案的任务目前还没有超越掩码语言建模。本文假设缺乏复杂性是造成其性能下降的关键，我们验证了更复杂的掩码任务是否能够实现更好的结果，并探究它们的复杂度需要达到多少才能与掩码语言建模表现相当。我们使用GLUE、SQuAD和Universal Dependencies基准测试结果表明，更复杂的任务倾向于展现更好的下游任务表现，至少需要掩码语言建模复杂度的一半才能与其表现相当。最后，我们讨论了如何使用掩码任务预训练模型。",
    "tldr": "本文研究了掩码预训练任务的复杂度对下游任务表现的影响，并发现更复杂的任务可以实现更好的结果。这一发现表明，掩码预训练任务可以通过增加其复杂度来提高其性能。"
}