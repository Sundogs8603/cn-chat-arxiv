{
    "title": "Response Length Perception and Sequence Scheduling: An LLM-Empowered LLM Inference Pipeline. (arXiv:2305.13144v2 [cs.CL] UPDATED)",
    "abstract": "Large language models (LLMs) have revolutionized the field of AI, demonstrating unprecedented capacity across various tasks. However, the inference process for LLMs comes with significant computational costs. In this paper, we propose an efficient LLM inference pipeline that harnesses the power of LLMs. Our approach begins by tapping into the potential of LLMs to accurately perceive and predict the response length with minimal overhead. By leveraging this information, we introduce an efficient sequence scheduling technique that groups queries with similar response lengths into micro-batches. We evaluate our approach on real-world instruction datasets using the LLaMA-based model, and our results demonstrate an impressive 86% improvement in inference throughput without compromising effectiveness. Notably, our method is orthogonal to other inference acceleration techniques, making it a valuable addition to many existing toolkits (e.g., FlashAttention, Quantization) for LLM inference.",
    "link": "http://arxiv.org/abs/2305.13144",
    "context": "Title: Response Length Perception and Sequence Scheduling: An LLM-Empowered LLM Inference Pipeline. (arXiv:2305.13144v2 [cs.CL] UPDATED)\nAbstract: Large language models (LLMs) have revolutionized the field of AI, demonstrating unprecedented capacity across various tasks. However, the inference process for LLMs comes with significant computational costs. In this paper, we propose an efficient LLM inference pipeline that harnesses the power of LLMs. Our approach begins by tapping into the potential of LLMs to accurately perceive and predict the response length with minimal overhead. By leveraging this information, we introduce an efficient sequence scheduling technique that groups queries with similar response lengths into micro-batches. We evaluate our approach on real-world instruction datasets using the LLaMA-based model, and our results demonstrate an impressive 86% improvement in inference throughput without compromising effectiveness. Notably, our method is orthogonal to other inference acceleration techniques, making it a valuable addition to many existing toolkits (e.g., FlashAttention, Quantization) for LLM inference.",
    "path": "papers/23/05/2305.13144.json",
    "total_tokens": 926,
    "translated_title": "回复长度感知与序列调度：一种利用LLM的高效LLM推理流水线。",
    "translated_abstract": "大型语言模型(LLMs)已经在各种任务上显示出前所未有的能力，革命了人工智能领域。然而，LLMs的推理过程具有重要的计算成本。本文提出了一种利用LLMs的高效LLM推理流水线。我们的方法通过利用LLMs准确感知和预测响应长度的潜力，并引入一种高效的序列调度技术，将具有类似响应长度的查询分组成微批。我们使用基于LLaMA模型的真实世界指令数据集来评估我们的方法，结果显示出86%的推理吞吐量的提高，同时不影响有效性。值得注意的是，我们的方法与其他推理加速技术无关，是LLMs推理许多现有工具包(如FlashAttention、量化)的有价值补充。",
    "tldr": "本文提出了一种利用LLMs的高效LLM推理流水线，通过利用LLMs准确感知和预测响应长度的潜力，并引入一种高效的序列调度技术，将具有类似响应长度的查询分组成微批。实验结果表明，该方法在实现高效的推理吞吐量的同时也不影响有效性。",
    "en_tdlr": "This paper proposes an efficient LLM inference pipeline that leverages the potential of LLMs to accurately perceive and predict the response length and introduces a sequence scheduling technique that groups queries with similar response lengths into micro-batches. Experimental results demonstrate an impressive 86% improvement in inference throughput without compromising effectiveness, and the method is orthogonal to other inference acceleration techniques, making it a valuable addition to many existing toolkits for LLM inference."
}