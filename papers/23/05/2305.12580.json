{
    "title": "A Framework for Bidirectional Decoding: Case Study in Morphological Inflection. (arXiv:2305.12580v2 [cs.CL] UPDATED)",
    "abstract": "Transformer-based encoder-decoder models that generate outputs in a left-to-right fashion have become standard for sequence-to-sequence tasks. In this paper, we propose a framework for decoding that produces sequences from the \"outside-in\": at each step, the model chooses to generate a token on the left, on the right, or join the left and right sequences. We argue that this is more principled than prior bidirectional decoders. Our proposal supports a variety of model architectures and includes several training methods, such as a dynamic programming algorithm that marginalizes out the latent ordering variable. Our model sets state-of-the-art (SOTA) on the 2022 and 2023 shared tasks, beating the next best systems by over 4.7 and 2.7 points in average accuracy respectively. The model performs particularly well on long sequences, can implicitly learn the split point of words composed of stem and affix, and performs better relative to the baseline on datasets that have fewer unique lemmas (",
    "link": "http://arxiv.org/abs/2305.12580",
    "context": "Title: A Framework for Bidirectional Decoding: Case Study in Morphological Inflection. (arXiv:2305.12580v2 [cs.CL] UPDATED)\nAbstract: Transformer-based encoder-decoder models that generate outputs in a left-to-right fashion have become standard for sequence-to-sequence tasks. In this paper, we propose a framework for decoding that produces sequences from the \"outside-in\": at each step, the model chooses to generate a token on the left, on the right, or join the left and right sequences. We argue that this is more principled than prior bidirectional decoders. Our proposal supports a variety of model architectures and includes several training methods, such as a dynamic programming algorithm that marginalizes out the latent ordering variable. Our model sets state-of-the-art (SOTA) on the 2022 and 2023 shared tasks, beating the next best systems by over 4.7 and 2.7 points in average accuracy respectively. The model performs particularly well on long sequences, can implicitly learn the split point of words composed of stem and affix, and performs better relative to the baseline on datasets that have fewer unique lemmas (",
    "path": "papers/23/05/2305.12580.json",
    "total_tokens": 861,
    "translated_title": "一种双向解码的框架：形态变化为案例研究",
    "translated_abstract": "基于Transformer的编码器-解码器模型在生成序列任务中以从左到右的方式已成为标准。本文提出了一种从“外向内”生成序列的解码框架：在每个步骤中，模型选择在左边、右边生成一个标记，或者将左右序列连接起来。我们认为这比之前的双向解码更有原则性。我们的提议支持各种模型架构，并包括几种训练方法，例如将潜在的排序变量边缘化的动态规划算法。我们的模型在2022年和2023年共享任务中创造了最新成果(SOTA)，在平均准确性方面分别比其他最佳系统高出4.7个和2.7个点。该模型在长序列上表现出色，可以隐式学习由词干和词缀组成的单词的分割点，并在具有较少唯一词素的数据集上相对于基线表现更好。",
    "tldr": "本文提出了一种双向解码框架，可以从“外向内”生成序列，并且在多个模型架构和训练方法上进行了改进。该模型在长序列上表现优异，在2022年和2023年共享任务上取得了最佳结果。"
}