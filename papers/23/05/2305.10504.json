{
    "title": "Model-Free Robust Average-Reward Reinforcement Learning. (arXiv:2305.10504v1 [cs.LG])",
    "abstract": "Robust Markov decision processes (MDPs) address the challenge of model uncertainty by optimizing the worst-case performance over an uncertainty set of MDPs. In this paper, we focus on the robust average-reward MDPs under the model-free setting. We first theoretically characterize the structure of solutions to the robust average-reward Bellman equation, which is essential for our later convergence analysis. We then design two model-free algorithms, robust relative value iteration (RVI) TD and robust RVI Q-learning, and theoretically prove their convergence to the optimal solution. We provide several widely used uncertainty sets as examples, including those defined by the contamination model, total variation, Chi-squared divergence, Kullback-Leibler (KL) divergence and Wasserstein distance.",
    "link": "http://arxiv.org/abs/2305.10504",
    "context": "Title: Model-Free Robust Average-Reward Reinforcement Learning. (arXiv:2305.10504v1 [cs.LG])\nAbstract: Robust Markov decision processes (MDPs) address the challenge of model uncertainty by optimizing the worst-case performance over an uncertainty set of MDPs. In this paper, we focus on the robust average-reward MDPs under the model-free setting. We first theoretically characterize the structure of solutions to the robust average-reward Bellman equation, which is essential for our later convergence analysis. We then design two model-free algorithms, robust relative value iteration (RVI) TD and robust RVI Q-learning, and theoretically prove their convergence to the optimal solution. We provide several widely used uncertainty sets as examples, including those defined by the contamination model, total variation, Chi-squared divergence, Kullback-Leibler (KL) divergence and Wasserstein distance.",
    "path": "papers/23/05/2305.10504.json",
    "total_tokens": 824,
    "translated_title": "无模型鲁棒平均奖励强化学习",
    "translated_abstract": "鲁棒马尔可夫决策过程通过在一个马尔可夫决策过程不确定性集合中优化最坏情况的性能来解决模型不确定性的挑战。本文着眼于无模型情况下的鲁棒平均奖励马尔可夫决策过程。我们首先理论上描述了鲁棒平均奖励Bellman方程的解结构，这对我们后面的收敛分析至关重要。接着，我们设计了两个无模型算法，鲁棒相对价值迭代(TD)和鲁棒RVI Q-learning，并理论上证明了它们收敛到最优解。我们提供了几个广泛使用的不确定性集合的示例，包括污染模型、总变差、卡方散度、KL散度和Wasserstein距离。",
    "tldr": "本文研究了无模型鲁棒平均奖励马尔可夫决策过程，提出两种算法并证明了它们收敛到最优解。我们给出了几个广泛使用的不确定性集合作为示例。",
    "en_tdlr": "This paper focuses on the problem of robust average-reward Markov decision processes under model-free setting and proposes two model-free algorithms that can converge to the optimal solution. Various widely used uncertainty sets are provided as examples."
}