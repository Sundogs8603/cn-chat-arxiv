{
    "title": "Complexity of Feed-Forward Neural Networks from the Perspective of Functional Equivalence. (arXiv:2305.11417v1 [cs.LG])",
    "abstract": "In this paper, we investigate the complexity of feed-forward neural networks by examining the concept of functional equivalence, which suggests that different network parameterizations can lead to the same function. We utilize the permutation invariance property to derive a novel covering number bound for the class of feedforward neural networks, which reveals that the complexity of a neural network can be reduced by exploiting this property. Furthermore, based on the symmetric structure of parameter space, we demonstrate that an appropriate strategy of random parameter initialization can increase the probability of convergence for optimization. We found that overparameterized networks tend to be easier to train in the sense that increasing the width of neural networks leads to a vanishing volume of the effective parameter space. Our findings offer new insights into overparameterization and have significant implications for understanding generalization and optimization in deep learning",
    "link": "http://arxiv.org/abs/2305.11417",
    "context": "Title: Complexity of Feed-Forward Neural Networks from the Perspective of Functional Equivalence. (arXiv:2305.11417v1 [cs.LG])\nAbstract: In this paper, we investigate the complexity of feed-forward neural networks by examining the concept of functional equivalence, which suggests that different network parameterizations can lead to the same function. We utilize the permutation invariance property to derive a novel covering number bound for the class of feedforward neural networks, which reveals that the complexity of a neural network can be reduced by exploiting this property. Furthermore, based on the symmetric structure of parameter space, we demonstrate that an appropriate strategy of random parameter initialization can increase the probability of convergence for optimization. We found that overparameterized networks tend to be easier to train in the sense that increasing the width of neural networks leads to a vanishing volume of the effective parameter space. Our findings offer new insights into overparameterization and have significant implications for understanding generalization and optimization in deep learning",
    "path": "papers/23/05/2305.11417.json",
    "total_tokens": 860,
    "translated_title": "从功能等价的角度看前馈神经网络的复杂性。",
    "translated_abstract": "本文通过考察功能等价的概念来研究前馈神经网络的复杂性，该概念表明不同的网络参数化可以导致相同的函数。我们利用置换不变性的特性为前馈神经网络类导出了一个新的覆盖数上界，发现利用该性质可以降低神经网络的复杂度。此外，基于参数空间的对称结构，我们证明适当的随机参数初始化策略可以增加优化收敛的概率。我们发现，过参数化的网络往往更容易训练，即增加神经网络的宽度会导致有效参数空间的体积趋近于零。本研究结果揭示了过参数化的新见解，并对深度学习中的泛化和优化理解具有重要意义。",
    "tldr": "本文从功能等价的角度出发研究前馈神经网络的复杂性，发现利用置换不变性的特性可以降低网络的复杂度，通过过参数化可以增加训练网络的容易程度，并对深度学习中的优化和泛化理解具有重要意义。",
    "en_tdlr": "This paper investigates the complexity of feed-forward neural networks from the perspective of functional equivalence. It reveals that exploiting the permutation invariance property can reduce the complexity of a neural network. The overparameterized networks tend to be easier to train, and these findings have significant implications for understanding generalization and optimization in deep learning."
}