{
    "title": "Parameter-Efficient Cross-lingual Transfer of Vision and Language Models via Translation-based Alignment. (arXiv:2305.03510v1 [cs.CL])",
    "abstract": "Pre-trained vision and language models such as CLIP have witnessed remarkable success in connecting images and texts with a primary focus on English texts. Despite recent efforts to extend CLIP to support other languages, disparities in performance among different languages have been observed due to uneven resource availability. Additionally, current cross-lingual transfer methods of those pre-trained models would consume excessive resources for a large number of languages. Therefore, we propose a new parameter-efficient cross-lingual transfer learning framework that utilizes a translation-based alignment method to mitigate multilingual disparities and explores parameter-efficient fine-tuning methods for parameter-efficient cross-lingual transfer. Extensive experiments on XTD and Multi30K datasets, covering 11 languages under zero-shot, few-shot, and full-dataset learning scenarios, show that our framework significantly reduces the multilingual disparities among languages and improves ",
    "link": "http://arxiv.org/abs/2305.03510",
    "context": "Title: Parameter-Efficient Cross-lingual Transfer of Vision and Language Models via Translation-based Alignment. (arXiv:2305.03510v1 [cs.CL])\nAbstract: Pre-trained vision and language models such as CLIP have witnessed remarkable success in connecting images and texts with a primary focus on English texts. Despite recent efforts to extend CLIP to support other languages, disparities in performance among different languages have been observed due to uneven resource availability. Additionally, current cross-lingual transfer methods of those pre-trained models would consume excessive resources for a large number of languages. Therefore, we propose a new parameter-efficient cross-lingual transfer learning framework that utilizes a translation-based alignment method to mitigate multilingual disparities and explores parameter-efficient fine-tuning methods for parameter-efficient cross-lingual transfer. Extensive experiments on XTD and Multi30K datasets, covering 11 languages under zero-shot, few-shot, and full-dataset learning scenarios, show that our framework significantly reduces the multilingual disparities among languages and improves ",
    "path": "papers/23/05/2305.03510.json",
    "total_tokens": 869,
    "translated_title": "基于翻译对齐的视觉语言模型跨语言迁移的参数高效方法",
    "translated_abstract": "预训练的视觉语言模型（如CLIP）在连接图像和英语文本方面取得了显著的成功。尽管最近试图扩展CLIP以支持其他语言，但由于资源不平衡，观察到了不同语言之间的性能差异。此外，当前的预训练模型的跨语言迁移方法会消耗大量资源。因此，我们提出了一种新的参数高效的跨语言迁移学习框架，利用基于翻译的对齐方法来减轻多语言差异，并探索参数高效的微调方法来实现参数高效的跨语言迁移。在XTD和Multi30K数据集上进行了广泛的实验，涵盖了零-shot、few-shot和全数据集学习场景下的11种语言，结果显示我们的框架显著减少了语言之间的多语言差异，并提高了性能。",
    "tldr": "本文提出了一个通过翻译对齐的方式实现参数高效、跨语言的迁移学习框架，实验结果显示该框架能够显著减少多语言之间的性能差异。"
}