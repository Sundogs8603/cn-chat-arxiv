{
    "title": "Complexity of Neural Network Training and ETR: Extensions with Effectively Continuous Functions. (arXiv:2305.11833v1 [cs.LO])",
    "abstract": "We study the complexity of the problem of training neural networks defined via various activation functions. The training problem is known to be existsR-complete with respect to linear activation functions and the ReLU activation function. We consider the complexity of the problem with respect to the sigmoid activation function and other effectively continuous functions. We show that these training problems are polynomial-time many-one bireducible to the existential theory of the reals extended with the corresponding activation functions. In particular, we establish that the sigmoid activation function leads to the existential theory of the reals with the exponential function. It is thus open, and equivalent with the decidability of the existential theory of the reals with the exponential function, whether training neural networks using the sigmoid activation function is algorithmically solvable. In contrast, we obtain that the training problem is undecidable if sinusoidal activation f",
    "link": "http://arxiv.org/abs/2305.11833",
    "context": "Title: Complexity of Neural Network Training and ETR: Extensions with Effectively Continuous Functions. (arXiv:2305.11833v1 [cs.LO])\nAbstract: We study the complexity of the problem of training neural networks defined via various activation functions. The training problem is known to be existsR-complete with respect to linear activation functions and the ReLU activation function. We consider the complexity of the problem with respect to the sigmoid activation function and other effectively continuous functions. We show that these training problems are polynomial-time many-one bireducible to the existential theory of the reals extended with the corresponding activation functions. In particular, we establish that the sigmoid activation function leads to the existential theory of the reals with the exponential function. It is thus open, and equivalent with the decidability of the existential theory of the reals with the exponential function, whether training neural networks using the sigmoid activation function is algorithmically solvable. In contrast, we obtain that the training problem is undecidable if sinusoidal activation f",
    "path": "papers/23/05/2305.11833.json",
    "total_tokens": 876,
    "translated_title": "神经网络训练的复杂性及ETR: 有效连续函数拓展",
    "translated_abstract": "本文研究了通过不同激活函数定义的神经网络训练问题的复杂性。对于线性和ReLU激活函数而言，已知训练问题是存在R-完备的。我们考虑了sigmoid激活函数和其他有效连续函数的问题复杂性。我们证明了这些训练问题可在多项式时间的 many-one 可还原到关于相应激活函数拓展的实数存在性理论上。特别地，我们证明了sigmoid激活函数导致了带有指数函数的实数存在性理论。因此，使用sigmoid激活函数训练神经网络是否具有算法可解性以及实数存在性理论和指数函数的可判定性等问题都是开放的。相反，在使用正弦激活函数时，我们发现训练问题是不可判定的。",
    "tldr": "本文研究了神经网络训练问题的复杂性，证明了sigmoid激活函数与实数存在性理论和指数函数相关，这使得神经网络在使用sigmoid激活函数时算法可解性存在疑问。同时，使用正弦激活函数时训练问题是不可判定的。",
    "en_tdlr": "This paper studies the complexity of training neural networks with various activation functions, proving that the sigmoid activation function is related to the existential theory of the reals and exponential function, raising questions about solvability when used in training. Furthermore, using the sinusoidal activation function makes the training problem undecidable."
}