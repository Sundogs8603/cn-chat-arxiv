{
    "title": "On Contrastive Learning of Semantic Similarity forCode to Code Search. (arXiv:2305.03843v1 [cs.SE])",
    "abstract": "This paper introduces a novel code-to-code search technique that enhances the performance of Large Language Models (LLMs) by including both static and dynamic features as well as utilizing both similar and dissimilar examples during training. We present the first-ever code search method that encodes dynamic runtime information during training without the need to execute either the corpus under search or the search query at inference time and the first code search technique that trains on both positive and negative reference samples. To validate the efficacy of our approach, we perform a set of studies demonstrating the capability of enhanced LLMs to perform cross-language code-to-code search.  Our evaluation demonstrates that the effectiveness of our approach is consistent across various model architectures and programming languages. We outperform the state-of-the-art cross-language search tool by up to 44.7\\%. Moreover, our ablation studies reveal that even a single positive and negat",
    "link": "http://arxiv.org/abs/2305.03843",
    "context": "Title: On Contrastive Learning of Semantic Similarity forCode to Code Search. (arXiv:2305.03843v1 [cs.SE])\nAbstract: This paper introduces a novel code-to-code search technique that enhances the performance of Large Language Models (LLMs) by including both static and dynamic features as well as utilizing both similar and dissimilar examples during training. We present the first-ever code search method that encodes dynamic runtime information during training without the need to execute either the corpus under search or the search query at inference time and the first code search technique that trains on both positive and negative reference samples. To validate the efficacy of our approach, we perform a set of studies demonstrating the capability of enhanced LLMs to perform cross-language code-to-code search.  Our evaluation demonstrates that the effectiveness of our approach is consistent across various model architectures and programming languages. We outperform the state-of-the-art cross-language search tool by up to 44.7\\%. Moreover, our ablation studies reveal that even a single positive and negat",
    "path": "papers/23/05/2305.03843.json",
    "total_tokens": 979,
    "translated_title": "语义相似性对比学习在代码搜索中的应用",
    "translated_abstract": "本文提出了一种新颖的代码搜索技术，通过在训练中包括静态和动态特征，利用相似和不相似的样本，增强大型语言模型（LLMs）的性能。我们提出了第一种在训练期间编码动态运行时信息的代码搜索方法，而无需在推断时执行要搜索的语料库或搜索查询，并且第一种在正负参考样本上进行训练的代码搜索技术。为验证我们方法的有效性，我们执行了一系列研究，证明增强型LLMs能够进行跨语言代码搜索，并且我们的方法在各种模型架构和编程语言中的性能是一致的。我们的评估结果表明，我们的方法比最先进的跨语言搜索工具提高了高达44.7％的性能。此外，我们的消融研究揭示，即使只有一种正负样本进行训练，也能显著优于传统技术只使用正样本的技术。",
    "tldr": "本文提出了一种结合静态和动态特征以及利用相似和不相似样本进行训练的代码搜索技术，在训练期间编码动态运行时信息。实验证明该方法在各种编程语言和模型架构中均具有一致的性能，比最先进的跨语言搜索工具提高了高达44.7%的性能。",
    "en_tdlr": "This paper presents a code-to-code search technique that combines static and dynamic features and utilizes both similar and dissimilar examples during training, encoding dynamic runtime information during training. The approach consistently outperforms state-of-the-art cross-language search tools, achieving up to 44.7% better performance, and is effective across various model architectures and programming languages even with just a single positive and negative sample during training."
}