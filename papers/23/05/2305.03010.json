{
    "title": "Sentence Embedding Leaks More Information than You Expect: Generative Embedding Inversion Attack to Recover the Whole Sentence. (arXiv:2305.03010v1 [cs.CL])",
    "abstract": "Sentence-level representations are beneficial for various natural language processing tasks. It is commonly believed that vector representations can capture rich linguistic properties. Currently, large language models (LMs) achieve state-of-the-art performance on sentence embedding. However, some recent works suggest that vector representations from LMs can cause information leakage. In this work, we further investigate the information leakage issue and propose a generative embedding inversion attack (GEIA) that aims to reconstruct input sequences based only on their sentence embeddings. Given the black-box access to a language model, we treat sentence embeddings as initial tokens' representations and train or fine-tune a powerful decoder model to decode the whole sequences directly. We conduct extensive experiments to demonstrate that our generative inversion attack outperforms previous embedding inversion attacks in classification metrics and generates coherent and contextually simil",
    "link": "http://arxiv.org/abs/2305.03010",
    "context": "Title: Sentence Embedding Leaks More Information than You Expect: Generative Embedding Inversion Attack to Recover the Whole Sentence. (arXiv:2305.03010v1 [cs.CL])\nAbstract: Sentence-level representations are beneficial for various natural language processing tasks. It is commonly believed that vector representations can capture rich linguistic properties. Currently, large language models (LMs) achieve state-of-the-art performance on sentence embedding. However, some recent works suggest that vector representations from LMs can cause information leakage. In this work, we further investigate the information leakage issue and propose a generative embedding inversion attack (GEIA) that aims to reconstruct input sequences based only on their sentence embeddings. Given the black-box access to a language model, we treat sentence embeddings as initial tokens' representations and train or fine-tune a powerful decoder model to decode the whole sequences directly. We conduct extensive experiments to demonstrate that our generative inversion attack outperforms previous embedding inversion attacks in classification metrics and generates coherent and contextually simil",
    "path": "papers/23/05/2305.03010.json",
    "total_tokens": 1129,
    "translated_title": "句子嵌入泄露的信息比您想象的要多：生成式嵌入逆向攻击用于恢复整个句子",
    "translated_abstract": "句子级别的表示对于各种自然语言处理任务都有益处。人们普遍认为向量表示可以捕捉到丰富的语言属性。目前，大型语言模型(LMs)在句子嵌入方面实现了最先进的性能。然而，一些最新的研究表明，从LMs中获得的向量表示可能会导致信息泄露。在这项工作中，我们进一步研究了信息泄露问题，并提出了一种生成式嵌入逆向攻击(GEIA)，旨在仅基于其句子嵌入来重构输入序列。鉴于对语言模型的黑盒访问，我们将句子嵌入视为初始标记的表示，并训练或微调一个强大的解码器模型直接解码整个序列。我们进行了广泛的实验，证明我们的生成逆向攻击在分类指标上优于先前的嵌入逆向攻击，并生成连贯且上下文相关的句子。此外，我们进行了一系列削减研究，以表明所提出的攻击是有效且稳健的。我们的工作突显了自然语言处理中隐私和安全的重要性，并呼吁进一步研究开发针对此类攻击的防御机制。",
    "tldr": "本论文提出了一种生成式嵌入逆向攻击方法，使用该方法可以只基于句子嵌入来重构输入序列，从而实现信息泄露的攻击。本方法在分类指标上优于先前的嵌入逆向攻击，生成的句子连贯且上下文相关。同时，该研究对自然语言处理中的隐私和安全问题提出了警醒，并呼吁进一步研究开发针对此类攻击的防御机制。",
    "en_tdlr": "This paper proposes a generative embedding inversion attack that can reconstruct input sequences only based on their sentence embeddings, which highlights the issue of information leakage and calls for further research in developing defense mechanisms against such attacks."
}