{
    "title": "Annotating and Detecting Fine-grained Factual Errors for Dialogue Summarization. (arXiv:2305.16548v1 [cs.CL])",
    "abstract": "A series of datasets and models have been proposed for summaries generated for well-formatted documents such as news articles. Dialogue summaries, however, have been under explored. In this paper, we present the first dataset with fine-grained factual error annotations named DIASUMFACT. We define fine-grained factual error detection as a sentence-level multi-label classification problem, and we evaluate two state-of-the-art (SOTA) models on our dataset. Both models yield sub-optimal results, with a macro-averaged F1 score of around 0.25 over 6 error classes. We further propose an unsupervised model ENDERANKER via candidate ranking using pretrained encoder-decoder models. Our model performs on par with the SOTA models while requiring fewer resources. These observations confirm the challenges in detecting factual errors from dialogue summaries, which call for further studies, for which our dataset and results offer a solid foundation.",
    "link": "http://arxiv.org/abs/2305.16548",
    "context": "Title: Annotating and Detecting Fine-grained Factual Errors for Dialogue Summarization. (arXiv:2305.16548v1 [cs.CL])\nAbstract: A series of datasets and models have been proposed for summaries generated for well-formatted documents such as news articles. Dialogue summaries, however, have been under explored. In this paper, we present the first dataset with fine-grained factual error annotations named DIASUMFACT. We define fine-grained factual error detection as a sentence-level multi-label classification problem, and we evaluate two state-of-the-art (SOTA) models on our dataset. Both models yield sub-optimal results, with a macro-averaged F1 score of around 0.25 over 6 error classes. We further propose an unsupervised model ENDERANKER via candidate ranking using pretrained encoder-decoder models. Our model performs on par with the SOTA models while requiring fewer resources. These observations confirm the challenges in detecting factual errors from dialogue summaries, which call for further studies, for which our dataset and results offer a solid foundation.",
    "path": "papers/23/05/2305.16548.json",
    "total_tokens": 914,
    "translated_title": "对话摘要中细粒度事实错误的注释和检测",
    "translated_abstract": "针对格式良好的文档（如新闻文章）生成的摘要，已经提出了一系列数据集和模型。但是，对话摘要一直未被探索。本文介绍了第一个带有细粒度事实错误注释的数据集 DIASUMFACT。我们将细粒度事实错误检测定义为一个句子级多标签分类问题，并评估了两个最先进的模型在我们的数据集上的表现。两个模型都产生了次优结果，六个错误类别的宏平均 F1 分数约为 0.25。我们进一步提出了一种使用预训练编码器-解码器模型进行候选排名的无监督模型 ENDERANKER。我们的模型表现与 SOTA 模型不相上下，同时需要较少的资源。这些观察结果证实了从对话摘要中检测事实错误的挑战，这需要进一步的研究，而我们的数据集和实验结果为此提供了坚实的基础。",
    "tldr": "本论文提出第一个注释有细粒度事实错误的对话摘要数据集，探索了细粒度事实错误检测作为一个句子级多标签分类问题的挑战，并提出了一种无监督模型，取得了和 SOTA 模型相近的效果。",
    "en_tdlr": "This paper presents the first annotated dataset with fine-grained factual errors for dialogue summarization, explores the challenge of fine-grained factual error detection as a sentence-level multi-label classification problem, and proposes an unsupervised model that achieves comparable results with state-of-the-art models."
}