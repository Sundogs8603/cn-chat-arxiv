{
    "title": "Exploring Better Text Image Translation with Multimodal Codebook. (arXiv:2305.17415v2 [cs.CL] UPDATED)",
    "abstract": "Text image translation (TIT) aims to translate the source texts embedded in the image to target translations, which has a wide range of applications and thus has important research value. However, current studies on TIT are confronted with two main bottlenecks: 1) this task lacks a publicly available TIT dataset, 2) dominant models are constructed in a cascaded manner, which tends to suffer from the error propagation of optical character recognition (OCR). In this work, we first annotate a Chinese-English TIT dataset named OCRMT30K, providing convenience for subsequent studies. Then, we propose a TIT model with a multimodal codebook, which is able to associate the image with relevant texts, providing useful supplementary information for translation. Moreover, we present a multi-stage training framework involving text machine translation, image-text alignment, and TIT tasks, which fully exploits additional bilingual texts, OCR dataset and our OCRMT30K dataset to train our model. Extensi",
    "link": "http://arxiv.org/abs/2305.17415",
    "context": "Title: Exploring Better Text Image Translation with Multimodal Codebook. (arXiv:2305.17415v2 [cs.CL] UPDATED)\nAbstract: Text image translation (TIT) aims to translate the source texts embedded in the image to target translations, which has a wide range of applications and thus has important research value. However, current studies on TIT are confronted with two main bottlenecks: 1) this task lacks a publicly available TIT dataset, 2) dominant models are constructed in a cascaded manner, which tends to suffer from the error propagation of optical character recognition (OCR). In this work, we first annotate a Chinese-English TIT dataset named OCRMT30K, providing convenience for subsequent studies. Then, we propose a TIT model with a multimodal codebook, which is able to associate the image with relevant texts, providing useful supplementary information for translation. Moreover, we present a multi-stage training framework involving text machine translation, image-text alignment, and TIT tasks, which fully exploits additional bilingual texts, OCR dataset and our OCRMT30K dataset to train our model. Extensi",
    "path": "papers/23/05/2305.17415.json",
    "total_tokens": 1048,
    "translated_title": "基于多模态码本的文本图片翻译探索",
    "translated_abstract": "文本图片翻译是将图像中嵌入的原始文本翻译成目标语言的任务。该任务有着广泛的应用，并具有重要的研究价值。然而，目前的文本图片翻译研究面临两个主要瓶颈：1）缺少公开的文本图片翻译数据集；2）主流模型采用级联模式构建，容易受到光字符识别错误传播的影响。本文首先注释了一个名为OCRMT30K的中英文文本图片翻译数据集，为后续研究提供了便利。然后，我们提出了一种多模态码本的文本图片翻译模型，能够将图像与相关文本关联起来，提供有用的补充信息进行翻译。此外，我们提出了一个多阶段训练框架，包括文本机器翻译、图像文本对齐和文本图片翻译任务，充分利用了额外的双语文本、光字符识别数据集和OCRMT30K数据集来训练我们的模型。扩展实验表明，我们的模型优于最先进的方法，并达到了竞争性的表现。",
    "tldr": "该研究提出了一个基于多模态码本的文本图片翻译模型。通过构建一个多阶段训练框架，利用了额外的双语文本和光学字符识别数据集，该模型能够将图像与相关文本关联起来，提供有用的补充信息，取得了比目前最先进的方法更好的效果。",
    "en_tdlr": "This paper proposes a text image translation model with a multimodal codebook, which associates the image with relevant texts, providing useful supplementary information for translation. Through a multi-stage training framework utilizing additional bilingual texts and optical character recognition datasets, the model outperforms state-of-the-art methods."
}