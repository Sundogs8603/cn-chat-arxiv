{
    "title": "Visual Chain of Thought: Bridging Logical Gaps with Multimodal Infillings. (arXiv:2305.02317v1 [cs.CL])",
    "abstract": "Recent advances in large language models elicit reasoning in a chain of thought that allows models to decompose problems in a human-like fashion. Though this paradigm improves multi-step reasoning ability in language models, it is limited by being unimodal and applied mainly to question-answering tasks. We claim that incorporating visual augmentation into reasoning is essential, especially for complex, imaginative tasks. Consequently, we introduce VCoT, a novel method that leverages chain of thought prompting with vision-language grounding to recursively bridge the logical gaps within sequential data. Our method uses visual guidance to generate synthetic multimodal infillings that add consistent and novel information to reduce the logical gaps for downstream tasks that can benefit from temporal reasoning, as well as provide interpretability into models' multi-step reasoning. We apply VCoT to the Visual Storytelling and WikiHow summarization datasets and demonstrate through human evalua",
    "link": "http://arxiv.org/abs/2305.02317",
    "context": "Title: Visual Chain of Thought: Bridging Logical Gaps with Multimodal Infillings. (arXiv:2305.02317v1 [cs.CL])\nAbstract: Recent advances in large language models elicit reasoning in a chain of thought that allows models to decompose problems in a human-like fashion. Though this paradigm improves multi-step reasoning ability in language models, it is limited by being unimodal and applied mainly to question-answering tasks. We claim that incorporating visual augmentation into reasoning is essential, especially for complex, imaginative tasks. Consequently, we introduce VCoT, a novel method that leverages chain of thought prompting with vision-language grounding to recursively bridge the logical gaps within sequential data. Our method uses visual guidance to generate synthetic multimodal infillings that add consistent and novel information to reduce the logical gaps for downstream tasks that can benefit from temporal reasoning, as well as provide interpretability into models' multi-step reasoning. We apply VCoT to the Visual Storytelling and WikiHow summarization datasets and demonstrate through human evalua",
    "path": "papers/23/05/2305.02317.json",
    "total_tokens": 920,
    "translated_title": "视觉思维链：多模态填充技术弥合逻辑差距",
    "translated_abstract": "大型自然语言模型的出现提高了模型的多步推理能力，能以人类方式分解问题。然而，该范例由于其单模态性质并且主要应用于问答任务而受到限制。我们认为将视觉增强内容纳入推理是必要的，尤其是针对复杂想象任务。因此，我们介绍了VCoT，一种新颖的方法，它利用思维链激励和视觉语言组合来递归地弥合时序数据中的逻辑差距。我们的方法使用视觉引导生成合成的多模态填充，以添加一致且新颖的信息，并减少下游任务中需要时序推理的逻辑差距，同时提供模型的多步推理的解释性。我们将VCoT应用于视觉叙事和WikiHow摘要数据集，并通过人工评估展示了其性能的提升。",
    "tldr": "VCoT是一种使用思维链激励和视觉语言组合递归地弥合时序数据中逻辑差距的新颖方法，其使用视觉引导生成合成的多模态填充以添加一致且新颖的信息，并减少需要时序推理的逻辑差距。",
    "en_tdlr": "VCoT is a novel method that uses chain of thought prompting with vision-language grounding to recursively bridge the logical gaps within sequential data, by utilizing visual guidance to generate synthetic multimodal infillings that add consistent and novel information to reduce the logical gaps for downstream tasks that can benefit from temporal reasoning, as well as provide interpretability into models’ multi-step reasoning."
}