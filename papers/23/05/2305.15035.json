{
    "title": "Self-ICL: Zero-Shot In-Context Learning with Self-Generated Demonstrations. (arXiv:2305.15035v2 [cs.CL] UPDATED)",
    "abstract": "Large language models (LLMs) have exhibited striking in-context learning (ICL) ability to adapt to target tasks with a few input-output demonstrations. For better ICL, different methods are proposed to select representative demonstrations from existing training corpora. However, such settings are not aligned with real-world practices, as end-users usually query LMs without access to demonstration pools. In this work, we introduce Self-ICL -- a simple framework which bootstraps LMs' intrinsic capabilities to perform zero-shot ICL. Given a test input, Self-ICL first prompts the model to generate pseudo-inputs. Next, the model predicts pseudo-labels for the pseudo-inputs via zero-shot prompting. Finally, we perform ICL for the test input with the pseudo-input-label pairs as demonstrations. Evaluation on 23 BIG-Bench Hard tasks shows Self-ICL outperforms zero-shot baselines on both average accuracy and head-to-head comparison. Moreover, with zero-shot chain-of-thought, Self-ICL achieves re",
    "link": "http://arxiv.org/abs/2305.15035",
    "context": "Title: Self-ICL: Zero-Shot In-Context Learning with Self-Generated Demonstrations. (arXiv:2305.15035v2 [cs.CL] UPDATED)\nAbstract: Large language models (LLMs) have exhibited striking in-context learning (ICL) ability to adapt to target tasks with a few input-output demonstrations. For better ICL, different methods are proposed to select representative demonstrations from existing training corpora. However, such settings are not aligned with real-world practices, as end-users usually query LMs without access to demonstration pools. In this work, we introduce Self-ICL -- a simple framework which bootstraps LMs' intrinsic capabilities to perform zero-shot ICL. Given a test input, Self-ICL first prompts the model to generate pseudo-inputs. Next, the model predicts pseudo-labels for the pseudo-inputs via zero-shot prompting. Finally, we perform ICL for the test input with the pseudo-input-label pairs as demonstrations. Evaluation on 23 BIG-Bench Hard tasks shows Self-ICL outperforms zero-shot baselines on both average accuracy and head-to-head comparison. Moreover, with zero-shot chain-of-thought, Self-ICL achieves re",
    "path": "papers/23/05/2305.15035.json",
    "total_tokens": 1048,
    "translated_title": "自我生成示范的零样本上下文学习(Self-ICL)",
    "translated_abstract": "大型语言模型(LLMs)展示了在少量输入输出示范下适应目标任务的惊人的上下文学习(ICL)能力。为了更好地进行ICL，提出了不同的方法从现有的训练语料库中选择代表性的示范。然而，这种设置与现实世界的实践不一致，因为最终用户通常在没有访问示范池的情况下查询LLMs。在这项工作中，我们引入了自我生成示范的零样本上下文学习(Self-ICL) - 一个简单的框架，通过引导LLMs的内在能力来执行零样本ICL。给定一个测试输入，Self-ICL首先提示模型生成伪输入。然后，模型通过零样本提示为伪输入预测伪标签。最后，我们使用伪输入-标签对作为示范来进行测试输入的ICL。在23个BIG-Bench难任务的评估中，Self-ICL在平均准确率和头对头比较方面优于零样本基线。此外，通过零样本思维链路，Self-ICL实现了连续ICL。",
    "tldr": "Self-ICL是一个简单的框架，通过自我生成示范的方式实现零样本上下文学习。它利用大型语言模型内在的能力来生成伪输入并预测伪标签，然后将这些伪输入和标签作为示范用于目标任务的学习。通过在23个难任务上的评估，Self-ICL在平均准确率和头对头比较方面表现优于其他零样本方法。",
    "en_tdlr": "Self-ICL is a simple framework for zero-shot in-context learning, which utilizes the intrinsic capabilities of large language models to generate pseudo-inputs and predict pseudo-labels. These pseudo-input-label pairs are then used as demonstrations for learning the target task. Evaluation on 23 challenging tasks shows that Self-ICL outperforms other zero-shot baselines in terms of average accuracy and head-to-head comparison."
}