{
    "title": "Data Representations' Study of Latent Image Manifolds. (arXiv:2305.19730v1 [cs.LG])",
    "abstract": "Deep neural networks have been demonstrated to achieve phenomenal success in many domains, and yet their inner mechanisms are not well understood. In this paper, we investigate the curvature of image manifolds, i.e., the manifold deviation from being flat in its principal directions. We find that state-of-the-art trained convolutional neural networks for image classification have a characteristic curvature profile along layers: an initial steep increase, followed by a long phase of a plateau, and followed by another increase. In contrast, this behavior does not appear in untrained networks in which the curvature flattens. We also show that the curvature gap between the last two layers has a strong correlation with the generalization capability of the network. Moreover, we find that the intrinsic dimension of latent codes is not necessarily indicative of curvature. Finally, we observe that common regularization methods such as mixup yield flatter representations when compared to other m",
    "link": "http://arxiv.org/abs/2305.19730",
    "context": "Title: Data Representations' Study of Latent Image Manifolds. (arXiv:2305.19730v1 [cs.LG])\nAbstract: Deep neural networks have been demonstrated to achieve phenomenal success in many domains, and yet their inner mechanisms are not well understood. In this paper, we investigate the curvature of image manifolds, i.e., the manifold deviation from being flat in its principal directions. We find that state-of-the-art trained convolutional neural networks for image classification have a characteristic curvature profile along layers: an initial steep increase, followed by a long phase of a plateau, and followed by another increase. In contrast, this behavior does not appear in untrained networks in which the curvature flattens. We also show that the curvature gap between the last two layers has a strong correlation with the generalization capability of the network. Moreover, we find that the intrinsic dimension of latent codes is not necessarily indicative of curvature. Finally, we observe that common regularization methods such as mixup yield flatter representations when compared to other m",
    "path": "papers/23/05/2305.19730.json",
    "total_tokens": 899,
    "translated_title": "潜在图像流形的数据表示研究",
    "translated_abstract": "深度神经网络在许多领域取得了惊人的成功，但其内在机制尚未得到很好的理解。本文研究了图像流形的曲率，即在其主方向上的流形偏离平坦的程度。我们发现，用于图像分类的最先进的卷积神经网络在层间具有特征曲率剖面：一个初始急剧增加，接着是长时间的平台期，然后是另一个增加。相反，在未经训练的网络中不出现这种行为，其中曲率变平。我们还表明，最后两层之间的曲率差异与网络的泛化能力有强烈的相关性。此外，我们发现，潜在编码的内在维度并非必然表征曲率。最后，我们观察到，mixup等常见的规范化方法在与其他方法相比时产生更平的表示。",
    "tldr": "本文研究了图像流形的曲率，其中最先进的卷积神经网络在层间具有特征曲率剖面，曲率差异与网络的泛化能力有强烈的相关性，且mixup等常见的规范化方法产生更平的表示。"
}