{
    "title": "Video Prediction Models as Rewards for Reinforcement Learning. (arXiv:2305.14343v2 [cs.LG] UPDATED)",
    "abstract": "Specifying reward signals that allow agents to learn complex behaviors is a long-standing challenge in reinforcement learning. A promising approach is to extract preferences for behaviors from unlabeled videos, which are widely available on the internet. We present Video Prediction Rewards (VIPER), an algorithm that leverages pretrained video prediction models as action-free reward signals for reinforcement learning. Specifically, we first train an autoregressive transformer on expert videos and then use the video prediction likelihoods as reward signals for a reinforcement learning agent. VIPER enables expert-level control without programmatic task rewards across a wide range of DMC, Atari, and RLBench tasks. Moreover, generalization of the video prediction model allows us to derive rewards for an out-of-distribution environment where no expert data is available, enabling cross-embodiment generalization for tabletop manipulation. We see our work as starting point for scalable reward s",
    "link": "http://arxiv.org/abs/2305.14343",
    "context": "Title: Video Prediction Models as Rewards for Reinforcement Learning. (arXiv:2305.14343v2 [cs.LG] UPDATED)\nAbstract: Specifying reward signals that allow agents to learn complex behaviors is a long-standing challenge in reinforcement learning. A promising approach is to extract preferences for behaviors from unlabeled videos, which are widely available on the internet. We present Video Prediction Rewards (VIPER), an algorithm that leverages pretrained video prediction models as action-free reward signals for reinforcement learning. Specifically, we first train an autoregressive transformer on expert videos and then use the video prediction likelihoods as reward signals for a reinforcement learning agent. VIPER enables expert-level control without programmatic task rewards across a wide range of DMC, Atari, and RLBench tasks. Moreover, generalization of the video prediction model allows us to derive rewards for an out-of-distribution environment where no expert data is available, enabling cross-embodiment generalization for tabletop manipulation. We see our work as starting point for scalable reward s",
    "path": "papers/23/05/2305.14343.json",
    "total_tokens": 912,
    "translated_title": "作为奖励的视频预测模型用于强化学习",
    "translated_abstract": "在强化学习中，制定让代理学习复杂行为的奖励信号一直是一个长期的挑战。一种有前途的方法是从广泛可用于互联网上的无标注视频中提取行为偏好。我们提出了Video Prediction Rewards (VIPER)，这种算法利用预训练的视频预测模型作为不需要行为干预的强化学习奖励信号。具体而言，我们首先在专家视频上训练一个自回归Transformer，然后将视频预测可能性用作强化学习代理的奖励信号。VIPER使得在DMC、Atari和RLBench任务等广泛的任务范围内，在没有编程任务奖励的情况下实现专家级的控制。此外，视频预测模型的泛化使得我们能够为没有专家数据可用的分布外环境导出奖励信号，从而实现桌面操纵的跨体现能力。我们认为我们的工作是具有伸缩性的奖励制定的起点。",
    "tldr": "本文提出了VIPER算法，利用预训练的视频预测模型作为强化学习的奖励信号来学习复杂行为，从而实现在广泛任务范围内的专家级控制，同时具有泛化性。",
    "en_tdlr": "This paper proposes the VIPER algorithm, which leverages pretrained video prediction models as reinforcement learning reward signals to learn complex behaviors, achieving expert-level control in a wide range of tasks and cross-embodiment generalization, providing a scalable starting point for reward specification."
}