{
    "title": "Multi-task Hierarchical Adversarial Inverse Reinforcement Learning. (arXiv:2305.12633v2 [cs.LG] UPDATED)",
    "abstract": "Multi-task Imitation Learning (MIL) aims to train a policy capable of performing a distribution of tasks based on multi-task expert demonstrations, which is essential for general-purpose robots. Existing MIL algorithms suffer from low data efficiency and poor performance on complex long-horizontal tasks. We develop Multi-task Hierarchical Adversarial Inverse Reinforcement Learning (MH-AIRL) to learn hierarchically-structured multi-task policies, which is more beneficial for compositional tasks with long horizons and has higher expert data efficiency through identifying and transferring reusable basic skills across tasks. To realize this, MH-AIRL effectively synthesizes context-based multi-task learning, AIRL (an IL approach), and hierarchical policy learning. Further, MH-AIRL can be adopted to demonstrations without the task or skill annotations (i.e., state-action pairs only) which are more accessible in practice. Theoretical justifications are provided for each module of MH-AIRL, and",
    "link": "http://arxiv.org/abs/2305.12633",
    "context": "Title: Multi-task Hierarchical Adversarial Inverse Reinforcement Learning. (arXiv:2305.12633v2 [cs.LG] UPDATED)\nAbstract: Multi-task Imitation Learning (MIL) aims to train a policy capable of performing a distribution of tasks based on multi-task expert demonstrations, which is essential for general-purpose robots. Existing MIL algorithms suffer from low data efficiency and poor performance on complex long-horizontal tasks. We develop Multi-task Hierarchical Adversarial Inverse Reinforcement Learning (MH-AIRL) to learn hierarchically-structured multi-task policies, which is more beneficial for compositional tasks with long horizons and has higher expert data efficiency through identifying and transferring reusable basic skills across tasks. To realize this, MH-AIRL effectively synthesizes context-based multi-task learning, AIRL (an IL approach), and hierarchical policy learning. Further, MH-AIRL can be adopted to demonstrations without the task or skill annotations (i.e., state-action pairs only) which are more accessible in practice. Theoretical justifications are provided for each module of MH-AIRL, and",
    "path": "papers/23/05/2305.12633.json",
    "total_tokens": 892,
    "translated_title": "多任务分层对抗逆强化学习",
    "translated_abstract": "多任务模仿学习旨在通过多任务专家演示训练出能够执行一系列任务的策略，这对于通用目的的机器人至关重要。现有的多任务模仿学习算法在数据效率和复杂长时任务的性能上存在问题。我们开发了多任务分层对抗逆强化学习（MH-AIRL），用于学习分层结构的多任务策略，这对于具有长时间范围的组合任务更加有益，并且通过识别和传递可重复使用的基本技能来提高专家数据的效率。为实现这一目标，MH-AIRL有效地合成了基于上下文的多任务学习、AIRL（一种模仿学习方法）和分层策略学习。此外，MH-AIRL还可以适应没有任务或技能注释的演示（即仅包含状态-动作对），这在实践中更易于获取。对于MH-AIRL的每个模块都提供了理论上的证明。",
    "tldr": "多任务分层对抗逆强化学习（MH-AIRL）通过分层结构和基本技能的可重复使用来提高多任务模仿学习的效率，适用于没有任务或技能注释的演示。",
    "en_tdlr": "Multi-task Hierarchical Adversarial Inverse Reinforcement Learning (MH-AIRL) improves the efficiency of multi-task imitation learning by utilizing hierarchical structures and reusable basic skills, and it is applicable to demonstrations without task or skill annotations."
}