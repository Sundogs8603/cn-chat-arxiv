{
    "title": "Successor-Predecessor Intrinsic Exploration. (arXiv:2305.15277v2 [cs.LG] UPDATED)",
    "abstract": "Exploration is essential in reinforcement learning, particularly in environments where external rewards are sparse. Here we focus on exploration with intrinsic rewards, where the agent transiently augments the external rewards with self-generated intrinsic rewards. Although the study of intrinsic rewards has a long history, existing methods focus on composing the intrinsic reward based on measures of future prospects of states, ignoring the information contained in the retrospective structure of transition sequences. Here we argue that the agent can utilise retrospective information to generate explorative behaviour with structure-awareness, facilitating efficient exploration based on global instead of local information. We propose Successor-Predecessor Intrinsic Exploration (SPIE), an exploration algorithm based on a novel intrinsic reward combining prospective and retrospective information. We show that SPIE yields more efficient and ethologically plausible exploratory behaviour in e",
    "link": "http://arxiv.org/abs/2305.15277",
    "context": "Title: Successor-Predecessor Intrinsic Exploration. (arXiv:2305.15277v2 [cs.LG] UPDATED)\nAbstract: Exploration is essential in reinforcement learning, particularly in environments where external rewards are sparse. Here we focus on exploration with intrinsic rewards, where the agent transiently augments the external rewards with self-generated intrinsic rewards. Although the study of intrinsic rewards has a long history, existing methods focus on composing the intrinsic reward based on measures of future prospects of states, ignoring the information contained in the retrospective structure of transition sequences. Here we argue that the agent can utilise retrospective information to generate explorative behaviour with structure-awareness, facilitating efficient exploration based on global instead of local information. We propose Successor-Predecessor Intrinsic Exploration (SPIE), an exploration algorithm based on a novel intrinsic reward combining prospective and retrospective information. We show that SPIE yields more efficient and ethologically plausible exploratory behaviour in e",
    "path": "papers/23/05/2305.15277.json",
    "total_tokens": 834,
    "translated_title": "后续前导内在探索",
    "translated_abstract": "在强化学习中，探索对于那些外部奖励稀缺的环境非常重要。本文侧重于利用内在奖励进行探索，即代理器使用自我生成的内在奖励临时增加外部奖励。尽管内在奖励的研究有着悠久的历史，但现有的方法都集中于根据状态的未来前景度量来构成内在奖励，忽视了转移序列的回顾结构中所蕴含的信息。在本文中，我们认为代理器可以利用回顾信息来生成具有结构感知能力的探索行为，以基于整体而非局部信息的方式实现高效的探索。我们提出了一种基于新颖内在奖励的模型——后续前导内在探索 (SPIE) 算法。实验结果表明，SPIE 能够产生更加高效和生态学合理的探索行为。",
    "tldr": "后续前导内在探索是一种基于新颖内在奖励的探索算法，它利用回顾信息并结合前瞻信息，以在强化学习中实现高效且生态学合理的探索行为。",
    "en_tdlr": "Successor-Predecessor Intrinsic Exploration is an exploration algorithm based on a novel intrinsic reward that combines retrospective and prospective information, enabling efficient and ecologically plausible exploration in reinforcement learning."
}