{
    "title": "Theoretical Analysis of Inductive Biases in Deep Convolutional Networks. (arXiv:2305.08404v1 [cs.LG])",
    "abstract": "In this paper, we study the inductive biases in convolutional neural networks (CNNs), which are believed to be vital drivers behind CNNs' exceptional performance on vision-like tasks. We first analyze the universality of CNNs, i.e., the ability to approximate continuous functions. We prove that a depth of $\\mathcal{O}(\\log d)$ is sufficient for achieving universality, where $d$ is the input dimension. This is a significant improvement over existing results that required a depth of $\\Omega(d)$. We also prove that learning sparse functions with CNNs needs only $\\tilde{\\mathcal{O}}(\\log^2d)$ samples, indicating that deep CNNs can efficiently capture long-range sparse correlations. Note that all these are achieved through a novel combination of increased network depth and the utilization of multichanneling and downsampling.  Lastly, we study the inductive biases of weight sharing and locality through the lens of symmetry. To separate two biases, we introduce locally-connected networks (LCN",
    "link": "http://arxiv.org/abs/2305.08404",
    "context": "Title: Theoretical Analysis of Inductive Biases in Deep Convolutional Networks. (arXiv:2305.08404v1 [cs.LG])\nAbstract: In this paper, we study the inductive biases in convolutional neural networks (CNNs), which are believed to be vital drivers behind CNNs' exceptional performance on vision-like tasks. We first analyze the universality of CNNs, i.e., the ability to approximate continuous functions. We prove that a depth of $\\mathcal{O}(\\log d)$ is sufficient for achieving universality, where $d$ is the input dimension. This is a significant improvement over existing results that required a depth of $\\Omega(d)$. We also prove that learning sparse functions with CNNs needs only $\\tilde{\\mathcal{O}}(\\log^2d)$ samples, indicating that deep CNNs can efficiently capture long-range sparse correlations. Note that all these are achieved through a novel combination of increased network depth and the utilization of multichanneling and downsampling.  Lastly, we study the inductive biases of weight sharing and locality through the lens of symmetry. To separate two biases, we introduce locally-connected networks (LCN",
    "path": "papers/23/05/2305.08404.json",
    "total_tokens": 1132,
    "translated_title": "深卷积神经网络中归纳偏置的理论分析",
    "translated_abstract": "本文研究卷积神经网络（CNN）中的归纳偏置，这被认为是CNN在视觉任务上表现异常出色的重要驱动因素。我们首先分析了CNN的普适性，即逼近连续函数的能力。我们证明了$\\mathcal{O}(\\log d)$的深度就足以实现普适性，其中$d$是输入维度。这相比于现有结果需要$\\Omega(d)$的深度是一项重大改进。我们还证明了用CNN学习稀疏函数只需要$\\tilde{\\mathcal{O}}(\\log^2d)$个样本，表明深度CNN可以有效地捕捉长程稀疏相关性。我们的研究还分析了共享权重和局部性的归纳偏置，通过对称性得出结论。为了区分这两种偏见，我们引入了局部连接网络（LCN）并证明了它们在表示需要有限平移等变和高方向选择性的函数方面的优越性。我们的结果为深CNN的成功提供了理论洞察力，同时更好地理解了它们的局限性。",
    "tldr": "本文研究深卷积神经网络（CNN）中的归纳偏置，证明了$\\mathcal{O}(\\log d)$的深度就足以实现普适性，用CNN学习稀疏函数只需要$\\tilde{\\mathcal{O}}(\\log^2d)$个样本。同时，通过局部连接网络（LCN）分析了权重共享和局部性的归纳偏置的区别，得出了它们在表示需要有限平移等变和高方向选择性的函数方面的优越性。",
    "en_tdlr": "This paper studies the inductive biases in deep convolutional neural networks (CNNs) and proves that a depth of $\\mathcal{O}(\\log d)$ is sufficient for achieving universality and that learning sparse functions with CNNs needs only $\\tilde{\\mathcal{O}}(\\log^2d)$ samples. The paper also introduces locally-connected networks (LCN) to analyze the biases of weight sharing and locality and demonstrates their superiority in representing functions that require limited translation equivariance but high orientation selectivity."
}