{
    "title": "High-Dimensional Smoothed Entropy Estimation via Dimensionality Reduction. (arXiv:2305.04712v2 [cs.IT] UPDATED)",
    "abstract": "We study the problem of overcoming exponential sample complexity in differential entropy estimation under Gaussian convolutions. Specifically, we consider the estimation of the differential entropy $h(X+Z)$ via $n$ independently and identically distributed samples of $X$, where $X$ and $Z$ are independent $D$-dimensional random variables with $X$ sub-Gaussian with bounded second moment and $Z\\sim\\mathcal{N}(0,\\sigma^2I_D)$. Under the absolute-error loss, the above problem has a parametric estimation rate of $\\frac{c^D}{\\sqrt{n}}$, which is exponential in data dimension $D$ and often problematic for applications. We overcome this exponential sample complexity by projecting $X$ to a low-dimensional space via principal component analysis (PCA) before the entropy estimation, and show that the asymptotic error overhead vanishes as the unexplained variance of the PCA vanishes. This implies near-optimal performance for inherently low-dimensional structures embedded in high-dimensional spaces,",
    "link": "http://arxiv.org/abs/2305.04712",
    "context": "Title: High-Dimensional Smoothed Entropy Estimation via Dimensionality Reduction. (arXiv:2305.04712v2 [cs.IT] UPDATED)\nAbstract: We study the problem of overcoming exponential sample complexity in differential entropy estimation under Gaussian convolutions. Specifically, we consider the estimation of the differential entropy $h(X+Z)$ via $n$ independently and identically distributed samples of $X$, where $X$ and $Z$ are independent $D$-dimensional random variables with $X$ sub-Gaussian with bounded second moment and $Z\\sim\\mathcal{N}(0,\\sigma^2I_D)$. Under the absolute-error loss, the above problem has a parametric estimation rate of $\\frac{c^D}{\\sqrt{n}}$, which is exponential in data dimension $D$ and often problematic for applications. We overcome this exponential sample complexity by projecting $X$ to a low-dimensional space via principal component analysis (PCA) before the entropy estimation, and show that the asymptotic error overhead vanishes as the unexplained variance of the PCA vanishes. This implies near-optimal performance for inherently low-dimensional structures embedded in high-dimensional spaces,",
    "path": "papers/23/05/2305.04712.json",
    "total_tokens": 942,
    "translated_title": "通过降维实现高维平滑熵估计",
    "translated_abstract": "本文研究在高斯卷积下克服微分熵估计的指数样本复杂性问题。具体而言，我们考虑通过$X$的$n$个独立同分布样本来估计微分熵$h(X+Z)$，其中$X$和$Z$是独立的$D$维随机变量，其中$X$是次高斯的，二阶矩有界，而$Z\\sim\\mathcal{N}(0,\\sigma^2I_D)$。在绝对误差损失下，上述问题具有$\\frac{c^D}{\\sqrt{n}}$的参数估计速率，其在数据维度$D$中呈指数级增长，常常在应用中引发问题。我们通过在熵估计之前通过主成分分析（PCA）将$X$投影到低维空间来克服这种指数样本复杂性，证明了当PCA的未解释方差消失时渐近误差的开销也将消失。这意味着在高维空间中嵌入本质上低维结构的近乎最优性能。",
    "tldr": "本文研究如何通过PCA将数据投影到低维空间来降低微分熵估计的指数样本复杂性问题，并表明该方法对于嵌入高维空间中的低维结构具有近乎最优性能。",
    "en_tdlr": "This paper studies how to overcome the problem of exponential sample complexity in differential entropy estimation under Gaussian convolutions by projecting data onto a low-dimensional space via PCA, and demonstrates the near-optimal performance of this approach for inherently low-dimensional structures embedded in high-dimensional spaces."
}