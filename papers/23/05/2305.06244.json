{
    "title": "Explainable Knowledge Distillation for On-device Chest X-Ray Classification. (arXiv:2305.06244v1 [cs.CV])",
    "abstract": "Automated multi-label chest X-rays (CXR) image classification has achieved substantial progress in clinical diagnosis via utilizing sophisticated deep learning approaches. However, most deep models have high computational demands, which makes them less feasible for compact devices with low computational requirements. To overcome this problem, we propose a knowledge distillation (KD) strategy to create the compact deep learning model for the real-time multi-label CXR image classification. We study different alternatives of CNNs and Transforms as the teacher to distill the knowledge to a smaller student. Then, we employed explainable artificial intelligence (XAI) to provide the visual explanation for the model decision improved by the KD. Our results on three benchmark CXR datasets show that our KD strategy provides the improved performance on the compact student model, thus being the feasible choice for many limited hardware platforms. For instance, when using DenseNet161 as the teacher",
    "link": "http://arxiv.org/abs/2305.06244",
    "context": "Title: Explainable Knowledge Distillation for On-device Chest X-Ray Classification. (arXiv:2305.06244v1 [cs.CV])\nAbstract: Automated multi-label chest X-rays (CXR) image classification has achieved substantial progress in clinical diagnosis via utilizing sophisticated deep learning approaches. However, most deep models have high computational demands, which makes them less feasible for compact devices with low computational requirements. To overcome this problem, we propose a knowledge distillation (KD) strategy to create the compact deep learning model for the real-time multi-label CXR image classification. We study different alternatives of CNNs and Transforms as the teacher to distill the knowledge to a smaller student. Then, we employed explainable artificial intelligence (XAI) to provide the visual explanation for the model decision improved by the KD. Our results on three benchmark CXR datasets show that our KD strategy provides the improved performance on the compact student model, thus being the feasible choice for many limited hardware platforms. For instance, when using DenseNet161 as the teacher",
    "path": "papers/23/05/2305.06244.json",
    "total_tokens": 970,
    "translated_title": "可解释的知识蒸馏用于设备内胸部X光分类",
    "translated_abstract": "利用复杂的深度学习方法进行自动多标签胸部X射线（CXR）图像分类已经在临床诊断中取得了显著进展。然而，大多数深度模型具有高计算需求，这使得它们不太适合低计算需求的小型设备。为了解决这个问题，我们提出了一种知识蒸馏（KD）策略，用于创建用于实时多标签CXR图像分类的紧凑深度学习模型。我们研究了不同的CNN和Transforms作为teacher来向较小的student蒸馏知识。然后，我们采用可解释的人工智能（XAI）来提供模型决策的可视化解释，从而改善KD。我们在三个基准CXR数据集上的结果表明，我们的KD策略提供了紧凑学生模型的改进性能，因此是许多有限硬件平台的可行选择。例如，当以DenseNet161作为teacher模型时，我们的方法将模型大小从115 MB减小到5 MB，评估时间加速了4倍。",
    "tldr": "该论文提出了一种利用可解释的人工智能（XAI）技术和知识蒸馏（KD）策略，创建用于设备内多标签CXr图像分类的紧凑深度学习模型，在三个基准CXr数据集上表现出更好的性能。",
    "en_tdlr": "This paper proposes a method for creating compact deep learning models for on-device multi-label CXR image classification using explainable artificial intelligence (XAI) technology and a knowledge distillation (KD) strategy, resulting in improved performance on three benchmark CXR datasets."
}