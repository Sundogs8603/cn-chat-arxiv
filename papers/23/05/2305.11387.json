{
    "title": "Justices for Information Bottleneck Theory. (arXiv:2305.11387v1 [cs.LG])",
    "abstract": "This study comes as a timely response to mounting criticism of the information bottleneck (IB) theory, injecting fresh perspectives to rectify misconceptions and reaffirm its validity. Firstly, we introduce an auxiliary function to reinterpret the maximal coding rate reduction method as a special yet local optimal case of IB theory. Through this auxiliary function, we clarify the paradox of decreasing mutual information during the application of ReLU activation in deep learning (DL) networks. Secondly, we challenge the doubts about IB theory's applicability by demonstrating its capacity to explain the absence of a compression phase with linear activation functions in hidden layers, when viewed through the lens of the auxiliary function. Lastly, by taking a novel theoretical stance, we provide a new way to interpret the inner organizations of DL networks by using IB theory, aligning them with recent experimental evidence. Thus, this paper serves as an act of justice for IB theory, poten",
    "link": "http://arxiv.org/abs/2305.11387",
    "context": "Title: Justices for Information Bottleneck Theory. (arXiv:2305.11387v1 [cs.LG])\nAbstract: This study comes as a timely response to mounting criticism of the information bottleneck (IB) theory, injecting fresh perspectives to rectify misconceptions and reaffirm its validity. Firstly, we introduce an auxiliary function to reinterpret the maximal coding rate reduction method as a special yet local optimal case of IB theory. Through this auxiliary function, we clarify the paradox of decreasing mutual information during the application of ReLU activation in deep learning (DL) networks. Secondly, we challenge the doubts about IB theory's applicability by demonstrating its capacity to explain the absence of a compression phase with linear activation functions in hidden layers, when viewed through the lens of the auxiliary function. Lastly, by taking a novel theoretical stance, we provide a new way to interpret the inner organizations of DL networks by using IB theory, aligning them with recent experimental evidence. Thus, this paper serves as an act of justice for IB theory, poten",
    "path": "papers/23/05/2305.11387.json",
    "total_tokens": 905,
    "translated_title": "信息瓶颈理论的公正之声",
    "translated_abstract": "本研究对信息瓶颈（IB）理论不断增加的质疑做出及时回应，注入新的观点以纠正误解并重申其有效性。首先，我们引入一种辅助函数，将最大编码率降低法解释为信息瓶颈理论的特殊但局部最优情况。通过这种辅助函数，我们澄清了在深度学习（DL）网络中应用ReLU激活时互信息下降的悖论。其次，我们通过辅助函数的视角，证明了IB理论解释线性激活函数在隐藏层中没有压缩阶段的能力，挑战了对IB理论适用性的质疑。最后，通过提出一种新的理论观点，我们使用IB理论提供了一种解释DL网络内部组织的新方法，并与最近的实验证据相一致。因此，本文是对IB理论的公正之声，也为未来该理论的发展提供了前瞻性的思考。",
    "tldr": "本论文从引入一个辅助函数，证明深度学习中ReLU激活下互信息下降的悖论，挑战了对信息瓶颈理论适用性的质疑，提供了使用该理论解释DL网络内部组织的新方法。",
    "en_tdlr": "This paper introduces an auxiliary function to clarify the paradox of decreasing mutual information during the application of ReLU activation in deep learning networks, challenges the doubts about the applicability of information bottleneck theory, and provides a new way to interpret the inner organizations of deep learning networks using this theory."
}