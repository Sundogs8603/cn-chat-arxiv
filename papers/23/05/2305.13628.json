{
    "title": "Improving Self-training for Cross-lingual Named Entity Recognition with Contrastive and Prototype Learning. (arXiv:2305.13628v1 [cs.CL])",
    "abstract": "In cross-lingual named entity recognition (NER), self-training is commonly used to bridge the linguistic gap by training on pseudo-labeled target-language data. However, due to sub-optimal performance on target languages, the pseudo labels are often noisy and limit the overall performance. In this work, we aim to improve self-training for cross-lingual NER by combining representation learning and pseudo label refinement in one coherent framework. Our proposed method, namely ContProto mainly comprises two components: (1) contrastive self-training and (2) prototype-based pseudo-labeling. Our contrastive self-training facilitates span classification by separating clusters of different classes, and enhances cross-lingual transferability by producing closely-aligned representations between the source and target language. Meanwhile, prototype-based pseudo-labeling effectively improves the accuracy of pseudo labels during training. We evaluate ContProto on multiple transfer pairs, and experim",
    "link": "http://arxiv.org/abs/2305.13628",
    "context": "Title: Improving Self-training for Cross-lingual Named Entity Recognition with Contrastive and Prototype Learning. (arXiv:2305.13628v1 [cs.CL])\nAbstract: In cross-lingual named entity recognition (NER), self-training is commonly used to bridge the linguistic gap by training on pseudo-labeled target-language data. However, due to sub-optimal performance on target languages, the pseudo labels are often noisy and limit the overall performance. In this work, we aim to improve self-training for cross-lingual NER by combining representation learning and pseudo label refinement in one coherent framework. Our proposed method, namely ContProto mainly comprises two components: (1) contrastive self-training and (2) prototype-based pseudo-labeling. Our contrastive self-training facilitates span classification by separating clusters of different classes, and enhances cross-lingual transferability by producing closely-aligned representations between the source and target language. Meanwhile, prototype-based pseudo-labeling effectively improves the accuracy of pseudo labels during training. We evaluate ContProto on multiple transfer pairs, and experim",
    "path": "papers/23/05/2305.13628.json",
    "total_tokens": 1041,
    "translated_title": "利用对比自我训练和原型学习改进跨语言命名实体识别的自训练方法",
    "translated_abstract": "在跨语言命名实体识别中，自训练通常用于通过在伪标记目标语言数据上进行训练来弥合语言差距。然而，由于目标语言性能不佳，伪标签通常存在噪声，限制了整体性能。本文旨在通过在一个一致的框架中结合表示学习和伪标签精化来改进跨语言命名实体识别的自训练方法。我们提出的方法主要包括两个组成部分：（1）对比自我训练和（2）基于原型的伪标记。我们的对比自我训练通过分离不同类别的聚类来促进跨语言转移，并通过产生源语言和目标语言之间紧密对齐的表示来增强跨语言可转移性。同时，基于原型的伪标记在训练过程中有效提高了伪标签的准确性。我们在多个转移对上评估ContProto，实验结果表明，我们的方法优于最先进的基准，并在各种基准上取得了显着的改进。",
    "tldr": "本文提出了一种名为ContProto的方法，通过对比自我训练和基于原型的伪标记，结合表示学习和伪标签精化，在一个一致的框架中提高了跨语言命名实体识别的自训练方法；实验结果表明，ContProto 方法在多个转移对上优于最先进的基准，并在各种基准上取得了显着的改进。"
}