{
    "title": "Emergent inabilities? Inverse scaling over the course of pretraining. (arXiv:2305.14681v1 [cs.CL])",
    "abstract": "Does inverse scaling only occur as a function of model parameter size, or can it also occur over the course of training? We carry out an exploratory study investigating whether, over the course of training on the language modeling task, the performance of language models at specific tasks can decrease while general performance remains high. We find that for two tasks from the Inverse Scaling Challenge - quote-repetition and redefine-math - this is indeed the case. Specifically, we find that for Pythia (Biderman et al., 2023) models with a higher number of parameters, performance decreases over the course of training at these two tasks, despite these models showing standard (positive) scaling overall. This highlights the importance of testing model performance at all relevant benchmarks any time they are trained on additional data, even if their overall performance improves.",
    "link": "http://arxiv.org/abs/2305.14681",
    "context": "Title: Emergent inabilities? Inverse scaling over the course of pretraining. (arXiv:2305.14681v1 [cs.CL])\nAbstract: Does inverse scaling only occur as a function of model parameter size, or can it also occur over the course of training? We carry out an exploratory study investigating whether, over the course of training on the language modeling task, the performance of language models at specific tasks can decrease while general performance remains high. We find that for two tasks from the Inverse Scaling Challenge - quote-repetition and redefine-math - this is indeed the case. Specifically, we find that for Pythia (Biderman et al., 2023) models with a higher number of parameters, performance decreases over the course of training at these two tasks, despite these models showing standard (positive) scaling overall. This highlights the importance of testing model performance at all relevant benchmarks any time they are trained on additional data, even if their overall performance improves.",
    "path": "papers/23/05/2305.14681.json",
    "total_tokens": 848,
    "translated_title": "训练过程中的逆比例缩放现象：累赘还是贡献？",
    "translated_abstract": "模型参数大小是否会导致逆比例缩放现象呢？这个研究探讨了语言模型在训练中针对特定任务的表现是否会下降，尽管整体表现仍然不错。研究发现在逆比例缩放挑战中的两个任务 - 引述重复和重新定义数学，Pythia模型运用更多的参数时，这两个任务在训练过程中的性能确实会下降，尽管这些模型在整体上表现良好。这突显了在任何时候如果模型被训练了额外的数据，那么需要测试模型在相应基准上的表现，即使它们的整体表现有所提高。",
    "tldr": "该研究探讨了模型在训练中针对特定任务的表现是否会下降，发现Pythia模型在更多的参数下，尽管整体表现良好，但在引述重复和重新定义数学两个任务的训练中性能下降。需要在任何时候测试模型在相应基准上的表现。",
    "en_tdlr": "This study explores whether performance of language models at specific tasks can decrease over the course of training, finding that Pythia models with more parameters show decreasing performance at two tasks from the Inverse Scaling Challenge, despite overall good performance, highlighting the importance of testing model performance at relevant benchmarks at all times."
}