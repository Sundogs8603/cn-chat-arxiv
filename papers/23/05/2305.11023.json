{
    "title": "Generalized Multiple Intent Conditioned Slot Filling. (arXiv:2305.11023v1 [cs.CL])",
    "abstract": "Natural language understanding includes the tasks of intent detection (identifying a user's objectives) and slot filling (extracting the entities relevant to those objectives). Prior slot filling methods assume that each intent type cannot occur more than once within a message, however this is often not a valid assumption for real-world settings. In this work, we generalize slot filling by removing the constraint of unique intents in a message. We cast this as a JSON generation task and approach it using a language model. We create a pre-training dataset by combining DBpedia and existing slot filling datasets that we convert for JSON generation. We also generate an in-domain dataset using GPT-3. We train T5 models for this task (with and without exemplars in the prompt) and find that both training datasets improve performance, and that the model is able to generalize to intent types not seen during training.",
    "link": "http://arxiv.org/abs/2305.11023",
    "context": "Title: Generalized Multiple Intent Conditioned Slot Filling. (arXiv:2305.11023v1 [cs.CL])\nAbstract: Natural language understanding includes the tasks of intent detection (identifying a user's objectives) and slot filling (extracting the entities relevant to those objectives). Prior slot filling methods assume that each intent type cannot occur more than once within a message, however this is often not a valid assumption for real-world settings. In this work, we generalize slot filling by removing the constraint of unique intents in a message. We cast this as a JSON generation task and approach it using a language model. We create a pre-training dataset by combining DBpedia and existing slot filling datasets that we convert for JSON generation. We also generate an in-domain dataset using GPT-3. We train T5 models for this task (with and without exemplars in the prompt) and find that both training datasets improve performance, and that the model is able to generalize to intent types not seen during training.",
    "path": "papers/23/05/2305.11023.json",
    "total_tokens": 856,
    "translated_title": "通用多意图条件下的词槽填充",
    "translated_abstract": "自然语言理解包括意图检测（识别用户的目标）和词槽填充（提取与这些目标相关的实体）任务。以往的词槽填充方法假设每个意图类型在一条消息中只出现一次，但在真实场景下这通常不是一个有效的假设。本文通过去除消息中唯一意图的约束，对词槽填充进行泛化。我们将其视为一个 JSON 生成任务，并使用语言模型对其进行处理。我们通过结合 DBpedia 和现有的词槽填充数据集并将其转换为 JSON 生成任务来创建一个预训练数据集。我们还使用 GPT-3 生成了一个领域内数据集。我们训练了 T5 模型（带或不带提示示例）并发现两个训练数据集均能提高性能，而模型能够泛化到训练过程中未见过的意图类型。",
    "tldr": "本文针对现实场景下意图重复的问题，对词槽填充进行了泛化。通过结合数据集进行预训练，使用语言模型进行 JSON 生成任务处理，T5 模型能够泛化到未见过的意图类型。",
    "en_tdlr": "This paper generalizes the slot filling task by removing the assumption of unique intents in a message. They approach it as a JSON generation task using a language model and utilize pre-training datasets. The T5 model is able to generalize to intent types not seen during training."
}