{
    "title": "Sentiment Perception Adversarial Attacks on Neural Machine Translation Systems. (arXiv:2305.01437v1 [cs.CL])",
    "abstract": "With the advent of deep learning methods, Neural Machine Translation (NMT) systems have become increasingly powerful. However, deep learning based systems are susceptible to adversarial attacks, where imperceptible changes to the input can cause undesirable changes at the output of the system. To date there has been little work investigating adversarial attacks on sequence-to-sequence systems, such as NMT models. Previous work in NMT has examined attacks with the aim of introducing target phrases in the output sequence. In this work, adversarial attacks for NMT systems are explored from an output perception perspective. Thus the aim of an attack is to change the perception of the output sequence, without altering the perception of the input sequence. For example, an adversary may distort the sentiment of translated reviews to have an exaggerated positive sentiment. In practice it is challenging to run extensive human perception experiments, so a proxy deep-learning classifier applied t",
    "link": "http://arxiv.org/abs/2305.01437",
    "context": "Title: Sentiment Perception Adversarial Attacks on Neural Machine Translation Systems. (arXiv:2305.01437v1 [cs.CL])\nAbstract: With the advent of deep learning methods, Neural Machine Translation (NMT) systems have become increasingly powerful. However, deep learning based systems are susceptible to adversarial attacks, where imperceptible changes to the input can cause undesirable changes at the output of the system. To date there has been little work investigating adversarial attacks on sequence-to-sequence systems, such as NMT models. Previous work in NMT has examined attacks with the aim of introducing target phrases in the output sequence. In this work, adversarial attacks for NMT systems are explored from an output perception perspective. Thus the aim of an attack is to change the perception of the output sequence, without altering the perception of the input sequence. For example, an adversary may distort the sentiment of translated reviews to have an exaggerated positive sentiment. In practice it is challenging to run extensive human perception experiments, so a proxy deep-learning classifier applied t",
    "path": "papers/23/05/2305.01437.json",
    "total_tokens": 868,
    "translated_abstract": "随着深度学习方法的出现，神经机器翻译(NMT)系统变得越来越强大。然而，基于深度学习的系统容易受到对抗攻击，在输入中不可察觉的改变可能会导致系统输出不良结果。迄今为止，很少有研究探讨序列到序列系统，如NMT模型的对抗攻击。之前在NMT方面的工作，主要处理的是为了在输出序列中引入目标短语的攻击。在这项工作中，研究了从输出感知角度探索NMT系统的对抗攻击。因此，攻击的目的是改变输出序列的感知，而不会改变输入序列的感知。例如，攻击者可能会扭曲翻译评论的情感，使其具有夸大的正面情绪。在实践中，进行广泛的人类感知实验是具有挑战性的，因此使用代理深度学习分类器进行了实验。",
    "tldr": "本文研究通过改变NMT系统的输出方式进行对抗攻击，而不影响输入序列的感知，提出了一种新的攻击策略。",
    "en_tdlr": "This paper explores adversarial attacks on NMT systems from an output perception perspective, aiming to change the perception of the output sequence without altering the perception of the input sequence, which proposes a new attack strategy."
}