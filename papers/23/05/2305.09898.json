{
    "title": "Balancing Lexical and Semantic Quality in Abstractive Summarization. (arXiv:2305.09898v1 [cs.CL])",
    "abstract": "An important problem of the sequence-to-sequence neural models widely used in abstractive summarization is exposure bias. To alleviate this problem, re-ranking systems have been applied in recent years. Despite some performance improvements, this approach remains underexplored. Previous works have mostly specified the rank through the ROUGE score and aligned candidate summaries, but there can be quite a large gap between the lexical overlap metric and semantic similarity. In this paper, we propose a novel training method in which a re-ranker balances the lexical and semantic quality. We further newly define false positives in ranking and present a strategy to reduce their influence. Experiments on the CNN/DailyMail and XSum datasets show that our method can estimate the meaning of summaries without seriously degrading the lexical aspect. More specifically, it achieves an 89.67 BERTScore on the CNN/DailyMail dataset, reaching new state-of-the-art performance. Our code is publicly availa",
    "link": "http://arxiv.org/abs/2305.09898",
    "context": "Title: Balancing Lexical and Semantic Quality in Abstractive Summarization. (arXiv:2305.09898v1 [cs.CL])\nAbstract: An important problem of the sequence-to-sequence neural models widely used in abstractive summarization is exposure bias. To alleviate this problem, re-ranking systems have been applied in recent years. Despite some performance improvements, this approach remains underexplored. Previous works have mostly specified the rank through the ROUGE score and aligned candidate summaries, but there can be quite a large gap between the lexical overlap metric and semantic similarity. In this paper, we propose a novel training method in which a re-ranker balances the lexical and semantic quality. We further newly define false positives in ranking and present a strategy to reduce their influence. Experiments on the CNN/DailyMail and XSum datasets show that our method can estimate the meaning of summaries without seriously degrading the lexical aspect. More specifically, it achieves an 89.67 BERTScore on the CNN/DailyMail dataset, reaching new state-of-the-art performance. Our code is publicly availa",
    "path": "papers/23/05/2305.09898.json",
    "total_tokens": 935,
    "translated_title": "抽象化摘要中平衡词汇和语义质量的方法",
    "translated_abstract": "序列到序列的神经模型在抽象化摘要中被广泛应用，但是曝光偏差是一个重要的问题。为了缓解这个问题，最近几年一直使用重新排序系统。尽管有些性能改进，但这个方法仍然不太成熟。以前的工作大多是通过ROUGE分数和对齐候选摘要来指定排名，但词汇重叠指标和语义相似度之间可能存在很大差距。在本文中，我们提出了一种新的训练方法，在其中重新排列程序平衡词汇和语义质量。我们进一步重新定义了排名中的假阳性，并提出了一种减少它们影响的策略。在CNN / DailyMail和XSum数据集上的实验表明，我们的方法可以估计摘要的含义，而不会严重降低词汇方面的质量。具体来说，我们在CNN / DailyMail数据集上实现了89.67的BERTScore，达到了新的最先进性能。我们的代码是公开的。",
    "tldr": "本文提出了一种新的训练方法，其中重新排列硬件平衡了词汇和语义质量，以缓解抽象化摘要中的暴露偏差问题。",
    "en_tdlr": "This paper proposes a novel training method in abstractive summarization that balances the lexical and semantic quality, in order to alleviate the exposure bias problem. The re-ranker not only specifies the rank through the ROUGE score and aligned candidate summaries, but also reduces the influence of false positives in ranking. The proposed method achieved state-of-the-art performance on the CNN/DailyMail dataset."
}