{
    "title": "Exploring Chain-of-Thought Style Prompting for Text-to-SQL. (arXiv:2305.14215v2 [cs.CL] UPDATED)",
    "abstract": "In-context learning with large language models (LLMs) has recently caught increasing attention due to its superior few-shot performance on various tasks. However, its performance on text-to-SQL parsing still has much room for improvement. In this paper, we hypothesize that a crucial aspect of LLMs to improve for text-to-SQL parsing is their multi-step reasoning ability. Thus, we systematically study how to enhance LLMs' reasoning ability through chain of thought (CoT) style prompting, including the original chain-of-thought prompting (Wei et al., 2022b) and least-to-most prompting (Zhou et al., 2023). Our experiments demonstrate that iterative prompting as in Zhou et al. (2023) may be unnecessary for text-to-SQL parsing, and using detailed reasoning steps tends to have more error propagation issues. Based on these findings, we propose a new CoT-style prompting method for text-to-SQL parsing. It brings 5.2 and 6.5 point absolute gains on the Spider development set and the Spider Realist",
    "link": "http://arxiv.org/abs/2305.14215",
    "context": "Title: Exploring Chain-of-Thought Style Prompting for Text-to-SQL. (arXiv:2305.14215v2 [cs.CL] UPDATED)\nAbstract: In-context learning with large language models (LLMs) has recently caught increasing attention due to its superior few-shot performance on various tasks. However, its performance on text-to-SQL parsing still has much room for improvement. In this paper, we hypothesize that a crucial aspect of LLMs to improve for text-to-SQL parsing is their multi-step reasoning ability. Thus, we systematically study how to enhance LLMs' reasoning ability through chain of thought (CoT) style prompting, including the original chain-of-thought prompting (Wei et al., 2022b) and least-to-most prompting (Zhou et al., 2023). Our experiments demonstrate that iterative prompting as in Zhou et al. (2023) may be unnecessary for text-to-SQL parsing, and using detailed reasoning steps tends to have more error propagation issues. Based on these findings, we propose a new CoT-style prompting method for text-to-SQL parsing. It brings 5.2 and 6.5 point absolute gains on the Spider development set and the Spider Realist",
    "path": "papers/23/05/2305.14215.json",
    "total_tokens": 999,
    "translated_title": "探索面向文本到SQL转换的思维链式提示",
    "translated_abstract": "最近，基于大型语言模型（LLM）的上下文学习因其在各种任务上杰出的少样本性能而引起了越来越多的关注。然而，在文本到SQL解析方面，其性能仍有很大提升空间。本文假设改进文本到SQL解析的LLM的一个关键方面是其多步推理能力。因此，我们系统地研究了如何通过思维链式提示来提升LLM的推理能力，包括原始的思维链式提示（Wei等，2022b）和由小到大的提示（Zhou等，2023）。我们的实验表明，如Zhou等人（2023）所示的迭代提示在文本到SQL解析中可能是不必要的，并且使用详细的推理步骤往往会产生更多的错误传播问题。基于这些发现，我们提出了一种新的面向文本到SQL转换的CoT风格提示方法。它在Spider开发集和Spider Realist数据集上分别提高了5.2和6.5个绝对分数。",
    "tldr": "本文研究了通过思维链式提示来提升大型语言模型在文本到SQL解析中的推理能力。实验表明，迭代提示法可能是不必要的，并且详细的推理步骤容易出现错误传播问题。在此基础上，提出了一种新的CoT风格提示方法，显著提高了文本到SQL解析的性能。",
    "en_tdlr": "This paper explores the improvement of large language models' reasoning ability in text-to-SQL parsing through chain-of-thought style prompting. The experiments show that iterative prompting may be unnecessary and detailed reasoning steps can cause error propagation issues. Based on these findings, a new CoT-style prompting method is proposed, which significantly improves the performance of text-to-SQL parsing."
}