{
    "title": "DynamicStereo: Consistent Dynamic Depth from Stereo Videos. (arXiv:2305.02296v1 [cs.CV])",
    "abstract": "We consider the problem of reconstructing a dynamic scene observed from a stereo camera. Most existing methods for depth from stereo treat different stereo frames independently, leading to temporally inconsistent depth predictions. Temporal consistency is especially important for immersive AR or VR scenarios, where flickering greatly diminishes the user experience. We propose DynamicStereo, a novel transformer-based architecture to estimate disparity for stereo videos. The network learns to pool information from neighboring frames to improve the temporal consistency of its predictions. Our architecture is designed to process stereo videos efficiently through divided attention layers. We also introduce Dynamic Replica, a new benchmark dataset containing synthetic videos of people and animals in scanned environments, which provides complementary training and evaluation data for dynamic stereo closer to real applications than existing datasets. Training with this dataset further improves ",
    "link": "http://arxiv.org/abs/2305.02296",
    "context": "Title: DynamicStereo: Consistent Dynamic Depth from Stereo Videos. (arXiv:2305.02296v1 [cs.CV])\nAbstract: We consider the problem of reconstructing a dynamic scene observed from a stereo camera. Most existing methods for depth from stereo treat different stereo frames independently, leading to temporally inconsistent depth predictions. Temporal consistency is especially important for immersive AR or VR scenarios, where flickering greatly diminishes the user experience. We propose DynamicStereo, a novel transformer-based architecture to estimate disparity for stereo videos. The network learns to pool information from neighboring frames to improve the temporal consistency of its predictions. Our architecture is designed to process stereo videos efficiently through divided attention layers. We also introduce Dynamic Replica, a new benchmark dataset containing synthetic videos of people and animals in scanned environments, which provides complementary training and evaluation data for dynamic stereo closer to real applications than existing datasets. Training with this dataset further improves ",
    "path": "papers/23/05/2305.02296.json",
    "total_tokens": 900,
    "translated_title": "DynamicStereo：来自立体视频的一致动态深度",
    "translated_abstract": "本文考虑从立体摄像头观察的动态场景中重建深度的问题。大多数现有的立体深度方法独立地对待不同的立体帧，导致时间上不一致的深度预测。对于沉浸式 AR 或 VR 场景，时间一致性尤其重要，因为闪烁会大大降低用户体验。我们提出了 DynamicStereo，一种新颖的基于 transformer 的体系结构，用于估计立体视频的视差。网络学习从相邻帧中汇集信息，以改善其预测的时间一致性。我们的体系结构设计通过划分注意力层来高效处理立体视频。我们还介绍了 Dynamic Replica，这是一个新的基准数据集，其中包含扫描环境中的人和动物的合成视频，为动态立体更接近真实应用提供了补充的训练和评估数据。使用这个数据集的训练进一步提高了模型预测的性能。",
    "tldr": "本文提出了一种新颖的 DynamicStereo 架构，用于从立体视频中估计视差，并从相邻帧中汇集信息，以改善其预测的时间一致性。同时，提出了一个新的 Dynamic Replica 数据集作为基准数据集，更接近真实应用场景，用于训练和评估动态立体的性能。",
    "en_tdlr": "This paper proposes a novel DynamicStereo architecture for estimating disparity from stereo videos and pooling information from neighboring frames to improve its temporal consistency. Additionally, a new benchmark dataset called Dynamic Replica is introduced for training and evaluating dynamic stereo, which better approximates real-world scenarios."
}