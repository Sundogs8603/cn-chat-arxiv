{
    "title": "Learning Easily Updated General Purpose Text Representations with Adaptable Task-Specific Prefixes. (arXiv:2305.13499v1 [cs.CL])",
    "abstract": "Many real-world applications require making multiple predictions from the same text. Fine-tuning a large pre-trained language model for each downstream task causes computational burdens in the inference time due to several times of forward passes. To amortize the computational cost, freezing the language model and building lightweight models for downstream tasks based on fixed text representations are common solutions. Accordingly, how to learn fixed but general text representations that can generalize well to unseen downstream tasks becomes a challenge. Previous works have shown that the generalizability of representations can be improved by fine-tuning the pre-trained language model with some source tasks in a multi-tasking way. In this work, we propose a prefix-based method to learn the fixed text representations with source tasks. We learn a task-specific prefix for each source task independently and combine them to get the final representations. Our experimental results show that ",
    "link": "http://arxiv.org/abs/2305.13499",
    "context": "Title: Learning Easily Updated General Purpose Text Representations with Adaptable Task-Specific Prefixes. (arXiv:2305.13499v1 [cs.CL])\nAbstract: Many real-world applications require making multiple predictions from the same text. Fine-tuning a large pre-trained language model for each downstream task causes computational burdens in the inference time due to several times of forward passes. To amortize the computational cost, freezing the language model and building lightweight models for downstream tasks based on fixed text representations are common solutions. Accordingly, how to learn fixed but general text representations that can generalize well to unseen downstream tasks becomes a challenge. Previous works have shown that the generalizability of representations can be improved by fine-tuning the pre-trained language model with some source tasks in a multi-tasking way. In this work, we propose a prefix-based method to learn the fixed text representations with source tasks. We learn a task-specific prefix for each source task independently and combine them to get the final representations. Our experimental results show that ",
    "path": "papers/23/05/2305.13499.json",
    "total_tokens": 896,
    "translated_title": "通过可适应任务特定前缀学习易于更新的通用文本表示",
    "translated_abstract": "许多实际应用需要从相同的文本中进行多次预测。针对每个下游任务微调大型预训练的语言模型会在推断时带来计算负担，因为需要多次正向传递。为了摊销计算成本，冻结语言模型并基于固定文本表示为下游任务建立轻量级模型是常见的解决方案。因此，如何学习一种固定但通用的文本表示，以便在未知的下游任务中表现良好，成为一项挑战。过去的研究表明，通过以多任务的方式对预训练的语言模型进行微调，可以提高表示的通用性。在本文中，我们提出了一种基于前缀的方法，用于学习带有源任务的固定文本表示。我们独立地学习每个源任务的任务特定前缀，并将它们组合成最终表示。我们的实验结果表明，...",
    "tldr": "本文提出了一种基于前缀的方法，用于学习带有源任务的固定文本表示。独立地学习每个源任务的任务特定前缀，并将它们组合成最终表示，以解决如何学习易于更新、适用广泛的通用文本表示的挑战。",
    "en_tdlr": "The paper proposes a prefix-based method to learn fixed text representations with source tasks. By independently learning task-specific prefixes for each source task and combining them to get the final representation, it addresses the challenge of learning easily updated and widely applicable general purpose text representations."
}