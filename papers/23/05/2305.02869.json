{
    "title": "2x Faster Language Model Pre-training via Masked Structural Growth. (arXiv:2305.02869v1 [cs.CL])",
    "abstract": "Acceleration of large language model pre-training is a critical issue in present NLP research. In this paper, we focus on speeding up pre-training by progressively growing from a small Transformer structure to a large one. There are two main research problems related to progressive growth: growth schedule and growth operator. For growth schedule, existing work has explored multi-stage expansion of depth and feedforward layers. However, the impact of each dimension on the schedule's efficiency is still an open question. For growth operator, existing work relies on the initialization of new weights to inherit knowledge, and achieve only non-strict function preservation, limiting further optimization of training dynamics. To address these issues, we propose Masked Structural Growth (MSG), including growth schedules involving all possible dimensions and strictly function-preserving growth operators that is independent of the initialization of new weights. Experiments show that MSG is signi",
    "link": "http://arxiv.org/abs/2305.02869",
    "context": "Title: 2x Faster Language Model Pre-training via Masked Structural Growth. (arXiv:2305.02869v1 [cs.CL])\nAbstract: Acceleration of large language model pre-training is a critical issue in present NLP research. In this paper, we focus on speeding up pre-training by progressively growing from a small Transformer structure to a large one. There are two main research problems related to progressive growth: growth schedule and growth operator. For growth schedule, existing work has explored multi-stage expansion of depth and feedforward layers. However, the impact of each dimension on the schedule's efficiency is still an open question. For growth operator, existing work relies on the initialization of new weights to inherit knowledge, and achieve only non-strict function preservation, limiting further optimization of training dynamics. To address these issues, we propose Masked Structural Growth (MSG), including growth schedules involving all possible dimensions and strictly function-preserving growth operators that is independent of the initialization of new weights. Experiments show that MSG is signi",
    "path": "papers/23/05/2305.02869.json",
    "total_tokens": 890,
    "translated_title": "通过掩码结构成长实现2倍语言模型预训练加速",
    "translated_abstract": "在当今自然语言处理研究中，加速大型语言模型预训练是一个关键问题。本文旨在通过从小型Transformer结构逐步扩展到大型结构，加快预训练进程。这种渐进式成长的主要研究问题有两个，即成长进程和成长操作。对于成长进程，现有研究已经探索了深度和前馈层的多阶段扩展，但每个维度对进程效率的影响仍然是一个未解决的问题。而对于成长操作，现有研究依赖于新权重的初始化来继承原有的知识，只实现了非严格的函数保留，从而限制了进一步的训练动态优化。为解决这些问题，本文提出了掩码结构成长（MSG），其中包括涉及所有可能维度的成长进程和独立于新权重初始化的函数严格保留成长操作。实验证明，MSG可显著加速语言模型预训练。",
    "tldr": "本文提出了掩码结构成长（MSG），可以加速语言模型的预训练，其中包括全维度成长进程和独立于新权重初始化的函数严格保留成长操作。",
    "en_tdlr": "This paper proposes Masked Structural Growth (MSG) to speed up pre-training of large language models, which includes growth schedules involving all possible dimensions and strictly function-preserving growth operators that are independent of the initialization of new weights."
}