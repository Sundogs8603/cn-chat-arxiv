{
    "title": "Adapter-TST: A Parameter Efficient Method for Multiple-Attribute Text Style Transfer. (arXiv:2305.05945v1 [cs.CL])",
    "abstract": "Adapting a large language model for multiple-attribute text style transfer via fine-tuning can be challenging due to the significant amount of computational resources and labeled data required for the specific task. In this paper, we address this challenge by introducing AdapterTST, a framework that freezes the pre-trained model's original parameters and enables the development of a multiple-attribute text style transfer model. Using BART as the backbone model, Adapter-TST utilizes different neural adapters to capture different attribute information, like a plug-in connected to BART. Our method allows control over multiple attributes, like sentiment, tense, voice, etc., and configures the adapters' architecture to generate multiple outputs respected to attributes or compositional editing on the same sentence. We evaluate the proposed model on both traditional sentiment transfer and multiple-attribute transfer tasks. The experiment results demonstrate that Adapter-TST outperforms all th",
    "link": "http://arxiv.org/abs/2305.05945",
    "context": "Title: Adapter-TST: A Parameter Efficient Method for Multiple-Attribute Text Style Transfer. (arXiv:2305.05945v1 [cs.CL])\nAbstract: Adapting a large language model for multiple-attribute text style transfer via fine-tuning can be challenging due to the significant amount of computational resources and labeled data required for the specific task. In this paper, we address this challenge by introducing AdapterTST, a framework that freezes the pre-trained model's original parameters and enables the development of a multiple-attribute text style transfer model. Using BART as the backbone model, Adapter-TST utilizes different neural adapters to capture different attribute information, like a plug-in connected to BART. Our method allows control over multiple attributes, like sentiment, tense, voice, etc., and configures the adapters' architecture to generate multiple outputs respected to attributes or compositional editing on the same sentence. We evaluate the proposed model on both traditional sentiment transfer and multiple-attribute transfer tasks. The experiment results demonstrate that Adapter-TST outperforms all th",
    "path": "papers/23/05/2305.05945.json",
    "total_tokens": 889,
    "tldr": "Adapter-TST是一种高效参数的多属性文本风格转换方法，使用BART作为骨干模型并利用不同的神经适配器来捕获不同的属性信息，并通过配置适配器的架构来生成多个与属性相关的输出或在同一句子上进行组合编辑。"
}