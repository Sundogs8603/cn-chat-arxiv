{
    "title": "SlimFit: Memory-Efficient Fine-Tuning of Transformer-based Models Using Training Dynamics. (arXiv:2305.18513v1 [cs.CL])",
    "abstract": "Transformer-based models, such as BERT and ViT, have achieved state-of-the-art results across different natural language processing (NLP) and computer vision (CV) tasks. However, these models are extremely memory intensive during their fine-tuning process, making them difficult to deploy on GPUs with limited memory resources. To address this issue, we introduce a new tool called SlimFit that reduces the memory requirements of these models by dynamically analyzing their training dynamics and freezing less-contributory layers during fine-tuning. The layers to freeze are chosen using a runtime inter-layer scheduling algorithm. SlimFit adopts quantization and pruning for particular layers to balance the load of dynamic activations and to minimize the memory footprint of static activations, where static activations refer to those that cannot be discarded regardless of freezing. This allows SlimFit to freeze up to 95% of layers and reduce the overall on-device GPU memory usage of transformer",
    "link": "http://arxiv.org/abs/2305.18513",
    "context": "Title: SlimFit: Memory-Efficient Fine-Tuning of Transformer-based Models Using Training Dynamics. (arXiv:2305.18513v1 [cs.CL])\nAbstract: Transformer-based models, such as BERT and ViT, have achieved state-of-the-art results across different natural language processing (NLP) and computer vision (CV) tasks. However, these models are extremely memory intensive during their fine-tuning process, making them difficult to deploy on GPUs with limited memory resources. To address this issue, we introduce a new tool called SlimFit that reduces the memory requirements of these models by dynamically analyzing their training dynamics and freezing less-contributory layers during fine-tuning. The layers to freeze are chosen using a runtime inter-layer scheduling algorithm. SlimFit adopts quantization and pruning for particular layers to balance the load of dynamic activations and to minimize the memory footprint of static activations, where static activations refer to those that cannot be discarded regardless of freezing. This allows SlimFit to freeze up to 95% of layers and reduce the overall on-device GPU memory usage of transformer",
    "path": "papers/23/05/2305.18513.json",
    "total_tokens": 919,
    "translated_title": "SlimFit：使用训练动态实现基于Transformer模型的内存高效微调",
    "translated_abstract": "基于Transformer的模型（如BERT和ViT）在不同的自然语言处理（NLP）和计算机视觉（CV）任务中取得了最先进的结果。然而，在微调过程中，这些模型的内存占用极高，使它们难以在内存资源受限的GPU上部署。为了应对这个问题，我们引入了一个称为SlimFit的新工具，通过动态分析训练过程中的训练动态并在微调过程中冻结贡献较少的层来减少这些模型的内存要求。运行时层间调度算法选择要冻结的层。SlimFit对特定层采用量化和剪枝，以平衡动态激活的负载，并最小化不能丢弃的静态激活的内存占用，其中静态激活指无论冻结与否都不能丢弃的激活。这使得SlimFit可以冻结多达95％的层，并将变压器模型的整体设备GPU内存使用率降低高达5倍，而不影响性能。",
    "tldr": "SlimFit是一个新工具，通过动态分析模型的训练动态并在微调过程中冻结不影响性能的层，将基于Transformer模型的内存占用降低了5倍，而不影响性能。",
    "en_tdlr": "SlimFit is a new tool that reduces the memory usage of transformer-based models by up to 5x without sacrificing performance by dynamically analyzing training dynamics and freezing less-contributory layers during fine-tuning."
}