{
    "title": "MetaModulation: Learning Variational Feature Hierarchies for Few-Shot Learning with Fewer Tasks. (arXiv:2305.10309v1 [cs.LG])",
    "abstract": "Meta-learning algorithms are able to learn a new task using previously learned knowledge, but they often require a large number of meta-training tasks which may not be readily available. To address this issue, we propose a method for few-shot learning with fewer tasks, which we call MetaModulation. The key idea is to use a neural network to increase the density of the meta-training tasks by modulating batch normalization parameters during meta-training. Additionally, we modify parameters at various network levels, rather than just a single layer, to increase task diversity. To account for the uncertainty caused by the limited training tasks, we propose a variational MetaModulation where the modulation parameters are treated as latent variables. We also introduce learning variational feature hierarchies by the variational MetaModulation, which modulates features at all layers and can consider task uncertainty and generate more diverse tasks. The ablation studies illustrate the advantage",
    "link": "http://arxiv.org/abs/2305.10309",
    "context": "Title: MetaModulation: Learning Variational Feature Hierarchies for Few-Shot Learning with Fewer Tasks. (arXiv:2305.10309v1 [cs.LG])\nAbstract: Meta-learning algorithms are able to learn a new task using previously learned knowledge, but they often require a large number of meta-training tasks which may not be readily available. To address this issue, we propose a method for few-shot learning with fewer tasks, which we call MetaModulation. The key idea is to use a neural network to increase the density of the meta-training tasks by modulating batch normalization parameters during meta-training. Additionally, we modify parameters at various network levels, rather than just a single layer, to increase task diversity. To account for the uncertainty caused by the limited training tasks, we propose a variational MetaModulation where the modulation parameters are treated as latent variables. We also introduce learning variational feature hierarchies by the variational MetaModulation, which modulates features at all layers and can consider task uncertainty and generate more diverse tasks. The ablation studies illustrate the advantage",
    "path": "papers/23/05/2305.10309.json",
    "total_tokens": 912,
    "translated_title": "MetaModulation：在少任务情况下学习变分特征层次的Few-Shot Learning方法",
    "translated_abstract": "元学习算法能够利用先前学习的知识来学习新任务，但通常需要大量元训练任务，这些任务可能不容易得到。为解决这个问题，我们提出了一种名为MetaModulation的少任务Few-Shot Learning方法。关键思想是使用神经网络在元训练期间调制批量归一化参数以增加元训练任务的密度。此外，我们在各个网络层次修改参数，而不仅仅是单个层次，以增加任务多样性。为了考虑有限的训练任务所引起的不确定性，我们提出了一种变分MetaModulation，其中调制参数被视为潜在变量。我们还通过变分MetaModulation介绍了学习变分特征层次，该方法调制所有层次的特征，可以考虑任务不确定性并生成更多样的任务。消融研究证明了本方法的优越性。",
    "tldr": "提出了一种名为MetaModulation的少任务Few-Shot Learning方法，使用神经网络在元训练期间调制批量归一化参数以增加元训练任务的密度。本方法通过变分MetaModulation介绍了学习变分特征层次，可以考虑任务不确定性并生成更多样的任务。",
    "en_tdlr": "Proposed MetaModulation, a Few-Shot Learning method with fewer tasks, that increases the density of meta-training tasks by modulating batch normalization parameters using neural networks. Introducing the variational MetaModulation learning variational feature hierarchies that can consider task uncertainty and generate more diverse tasks."
}