{
    "title": "Scalable Performance Analysis for Vision-Language Models. (arXiv:2305.18786v2 [cs.CV] UPDATED)",
    "abstract": "Joint vision-language models have shown great performance over a diverse set of tasks. However, little is known about their limitations, as the high dimensional space learned by these models makes it difficult to identify semantic errors. Recent work has addressed this problem by designing highly controlled probing task benchmarks. Our paper introduces a more scalable solution that relies on already annotated benchmarks. Our method consists of extracting a large set of diverse features from a vision-language benchmark and measuring their correlation with the output of the target model. We confirm previous findings that CLIP behaves like a bag of words model and performs better with nouns and verbs; we also uncover novel insights such as CLIP getting confused by concrete words. Our framework is available at https://github.com/MichiganNLP/Scalable-VLM-Probing and can be used with other multimodal models and benchmarks.",
    "link": "http://arxiv.org/abs/2305.18786",
    "context": "Title: Scalable Performance Analysis for Vision-Language Models. (arXiv:2305.18786v2 [cs.CV] UPDATED)\nAbstract: Joint vision-language models have shown great performance over a diverse set of tasks. However, little is known about their limitations, as the high dimensional space learned by these models makes it difficult to identify semantic errors. Recent work has addressed this problem by designing highly controlled probing task benchmarks. Our paper introduces a more scalable solution that relies on already annotated benchmarks. Our method consists of extracting a large set of diverse features from a vision-language benchmark and measuring their correlation with the output of the target model. We confirm previous findings that CLIP behaves like a bag of words model and performs better with nouns and verbs; we also uncover novel insights such as CLIP getting confused by concrete words. Our framework is available at https://github.com/MichiganNLP/Scalable-VLM-Probing and can be used with other multimodal models and benchmarks.",
    "path": "papers/23/05/2305.18786.json",
    "total_tokens": 934,
    "translated_title": "视觉语言模型的可扩展性能分析",
    "translated_abstract": "联合视觉语言模型已经在各种任务中表现出良好的性能。 然而，由于这些模型学习的高维空间使得识别语义错误变得困难，因此我们对它们的限制知之甚少。 最近的工作通过设计高度可控的探测任务基准来解决这个问题。 本文提出了一种更可扩展的解决方案，该方案依赖于已注释的基准。 我们的方法包括从视觉语言基准中提取大量不同的特征，并测量它们与目标模型的输出之间的相关性。我们确认先前的研究结果，即CLIP类似于词袋模型，并且在名词和动词方面表现更好; 我们还发现了一些新的见解，例如CLIP被具体的词汇困扰。我们的框架可在https://github.com/MichiganNLP/Scalable-VLM-Probing上获得，在其他多模式模型和基准测试中也可以使用。",
    "tldr": "本文提出了一种可扩展的视觉语言基准测试方案，它可以利用已有的注释基准测试，并通过提取多种不同的特征来测量模型输出的相关性。实验结果发现，CLIP模型类似于词袋模型，在名词和动词方面表现良好，但容易被具体词汇困扰。",
    "en_tdlr": "This paper proposes a scalable solution for analyzing the performance of vision-language models by extracting diverse features from already annotated benchmarks and measuring their correlation with the model output. The experimental results reveal that CLIP behaves like a bag of words model and performs better with nouns and verbs, but is easily confused by concrete words."
}