{
    "title": "When are ensembles really effective?. (arXiv:2305.12313v1 [stat.ML])",
    "abstract": "Ensembling has a long history in statistical data analysis, with many impactful applications. However, in many modern machine learning settings, the benefits of ensembling are less ubiquitous and less obvious. We study, both theoretically and empirically, the fundamental question of when ensembling yields significant performance improvements in classification tasks. Theoretically, we prove new results relating the \\emph{ensemble improvement rate} (a measure of how much ensembling decreases the error rate versus a single model, on a relative scale) to the \\emph{disagreement-error ratio}. We show that ensembling improves performance significantly whenever the disagreement rate is large relative to the average error rate; and that, conversely, one classifier is often enough whenever the disagreement rate is low relative to the average error rate. On the way to proving these results, we derive, under a mild condition called \\emph{competence}, improved upper and lower bounds on the average ",
    "link": "http://arxiv.org/abs/2305.12313",
    "context": "Title: When are ensembles really effective?. (arXiv:2305.12313v1 [stat.ML])\nAbstract: Ensembling has a long history in statistical data analysis, with many impactful applications. However, in many modern machine learning settings, the benefits of ensembling are less ubiquitous and less obvious. We study, both theoretically and empirically, the fundamental question of when ensembling yields significant performance improvements in classification tasks. Theoretically, we prove new results relating the \\emph{ensemble improvement rate} (a measure of how much ensembling decreases the error rate versus a single model, on a relative scale) to the \\emph{disagreement-error ratio}. We show that ensembling improves performance significantly whenever the disagreement rate is large relative to the average error rate; and that, conversely, one classifier is often enough whenever the disagreement rate is low relative to the average error rate. On the way to proving these results, we derive, under a mild condition called \\emph{competence}, improved upper and lower bounds on the average ",
    "path": "papers/23/05/2305.12313.json",
    "total_tokens": 694,
    "translated_title": "集成学习何时有效？",
    "translated_abstract": "集成学习在统计数据分析中具有悠久的历史，并有许多具有影响力的应用。然而，在许多现代机器学习设置中，集成学习的好处不是普遍且不明显。本文从理论和实证两方面研究了集成学习何时在分类任务中能够显著提高性能的基本问题。",
    "tldr": "本文研究证明了在分类任务中，当集成模型的错误率较单个模型更低，且模型间不同意的错误率超过平均错误率，则集成学习能够显著提高性能。",
    "en_tdlr": "The paper studies the fundamental question of when ensembling yields significant performance improvements in classification tasks. Theoretically, they prove that ensembling improves performance significantly whenever the disagreement rate is large relative to the average error rate."
}