{
    "title": "Free Lunch for Efficient Textual Commonsense Integration in Language Models. (arXiv:2305.15516v1 [cs.CL])",
    "abstract": "Recent years have witnessed the emergence of textual commonsense knowledge bases, aimed at providing more nuanced and context-rich knowledge. The integration of external commonsense into language models has been shown to be a key enabler in advancing the state-of-the-art for a wide range of NLP tasks. However, incorporating textual commonsense descriptions is computationally expensive, as compared to encoding conventional symbolic knowledge. In this paper, we propose a method to improve its efficiency without modifying the model. We group training samples with similar commonsense descriptions into a single batch, thus reusing the encoded description across multiple samples. One key observation is that the upper bound of batch partitioning can be reduced to the classic {\\it graph k-cut problem}. Consequently, we propose a spectral clustering-based algorithm to solve this problem. Extensive experiments illustrate that the proposed batch partitioning approach effectively reduces the compu",
    "link": "http://arxiv.org/abs/2305.15516",
    "context": "Title: Free Lunch for Efficient Textual Commonsense Integration in Language Models. (arXiv:2305.15516v1 [cs.CL])\nAbstract: Recent years have witnessed the emergence of textual commonsense knowledge bases, aimed at providing more nuanced and context-rich knowledge. The integration of external commonsense into language models has been shown to be a key enabler in advancing the state-of-the-art for a wide range of NLP tasks. However, incorporating textual commonsense descriptions is computationally expensive, as compared to encoding conventional symbolic knowledge. In this paper, we propose a method to improve its efficiency without modifying the model. We group training samples with similar commonsense descriptions into a single batch, thus reusing the encoded description across multiple samples. One key observation is that the upper bound of batch partitioning can be reduced to the classic {\\it graph k-cut problem}. Consequently, we propose a spectral clustering-based algorithm to solve this problem. Extensive experiments illustrate that the proposed batch partitioning approach effectively reduces the compu",
    "path": "papers/23/05/2305.15516.json",
    "total_tokens": 831,
    "translated_title": "语言模型中高效文本常识融合的免费午餐",
    "translated_abstract": "最近几年，文本常识知识库的出现旨在提供更加细致和丰富的上下文知识。将外部常识融合进语言模型已被证明是推进各种NLP任务的关键。然而，与编码传统符号知识相比，将文本常识描述合并是计算上昂贵的。在本文中，我们提出了一种不修改模型的方法来提高其效率。我们将具有相似常识描述的训练样本分成一个批次，从而在多个样本中重复使用编码描述。其中一个关键观察是，批次划分的上限可以缩小到经典的图k分割问题。因此，我们提出了一种基于谱聚类的算法来解决这个问题。大量实验证明了所提出的批次划分方法有效地减少了计算复杂度。",
    "tldr": "本文提出了一种计算上更加高效的方法来将文本常识描述集成到语言模型中，将具有相似常识描述的样本分批次进行编码以提高效率。",
    "en_tdlr": "This paper proposes a computationally efficient method to integrate textual commonsense descriptions into language models by grouping training samples with similar descriptions into batches to reuse encoded descriptions and using a spectral clustering-based algorithm to reduce batch partitioning complexity."
}