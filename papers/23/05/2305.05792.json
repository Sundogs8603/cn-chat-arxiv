{
    "title": "Testing for Overfitting. (arXiv:2305.05792v1 [stat.ML])",
    "abstract": "High complexity models are notorious in machine learning for overfitting, a phenomenon in which models well represent data but fail to generalize an underlying data generating process. A typical procedure for circumventing overfitting computes empirical risk on a holdout set and halts once (or flags that/when) it begins to increase. Such practice often helps in outputting a well-generalizing model, but justification for why it works is primarily heuristic.  We discuss the overfitting problem and explain why standard asymptotic and concentration results do not hold for evaluation with training data. We then proceed to introduce and argue for a hypothesis test by means of which both model performance may be evaluated using training data, and overfitting quantitatively defined and detected. We rely on said concentration bounds which guarantee that empirical means should, with high probability, approximate their true mean to conclude that they should approximate each other. We stipulate co",
    "link": "http://arxiv.org/abs/2305.05792",
    "context": "Title: Testing for Overfitting. (arXiv:2305.05792v1 [stat.ML])\nAbstract: High complexity models are notorious in machine learning for overfitting, a phenomenon in which models well represent data but fail to generalize an underlying data generating process. A typical procedure for circumventing overfitting computes empirical risk on a holdout set and halts once (or flags that/when) it begins to increase. Such practice often helps in outputting a well-generalizing model, but justification for why it works is primarily heuristic.  We discuss the overfitting problem and explain why standard asymptotic and concentration results do not hold for evaluation with training data. We then proceed to introduce and argue for a hypothesis test by means of which both model performance may be evaluated using training data, and overfitting quantitatively defined and detected. We rely on said concentration bounds which guarantee that empirical means should, with high probability, approximate their true mean to conclude that they should approximate each other. We stipulate co",
    "path": "papers/23/05/2305.05792.json",
    "total_tokens": 803,
    "translated_title": "过拟合的测试",
    "translated_abstract": "在机器学习中，高复杂度的模型常见过拟合现象，即模型能够很好地代表数据，但无法推广到基础数据生成过程。解决过拟合的典型方法是在留置集上计算经验风险，一旦风险开始增加，就停止（或标记何时停止）。虽然这种方法输出了良好泛化的模型，但其实现原理主要是启发式的。本文讨论过拟合问题，解释了为什么使用训练数据进行评估时，标准渐近和浓度结果不成立。我们随后提出并阐述了一个假设检验，通过该检验可以对使用训练数据评估模型性能，并量化地定义和检测过拟合。我们依靠确保经验均值应该高概率地近似其真实均值的浓度界限，以得出他们应该相互接近的结论。",
    "tldr": "本文提出了一种能够使用训练数据进行评估模型性能的假设检验方法，可以准确地定义和检测过拟合。",
    "en_tdlr": "This paper proposes a hypothesis test for evaluating model performance using training data, and quantitatively defining and detecting overfitting."
}