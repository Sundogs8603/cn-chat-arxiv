{
    "title": "Discovering Individual Rewards in Collective Behavior through Inverse Multi-Agent Reinforcement Learning. (arXiv:2305.10548v1 [cs.LG])",
    "abstract": "The discovery of individual objectives in collective behavior of complex dynamical systems such as fish schools and bacteria colonies is a long-standing challenge. Inverse reinforcement learning is a potent approach for addressing this challenge but its applicability to dynamical systems, involving continuous state-action spaces and multiple interacting agents, has been limited. In this study, we tackle this challenge by introducing an off-policy inverse multi-agent reinforcement learning algorithm (IMARL). Our approach combines the ReF-ER techniques with guided cost learning. By leveraging demonstrations, our algorithm automatically uncovers the reward function and learns an effective policy for the agents. Through extensive experimentation, we demonstrate that the proposed policy captures the behavior observed in the provided data, and achieves promising results across problem domains including single agent models in the OpenAI gym and multi-agent models of schooling behavior. The pr",
    "link": "http://arxiv.org/abs/2305.10548",
    "context": "Title: Discovering Individual Rewards in Collective Behavior through Inverse Multi-Agent Reinforcement Learning. (arXiv:2305.10548v1 [cs.LG])\nAbstract: The discovery of individual objectives in collective behavior of complex dynamical systems such as fish schools and bacteria colonies is a long-standing challenge. Inverse reinforcement learning is a potent approach for addressing this challenge but its applicability to dynamical systems, involving continuous state-action spaces and multiple interacting agents, has been limited. In this study, we tackle this challenge by introducing an off-policy inverse multi-agent reinforcement learning algorithm (IMARL). Our approach combines the ReF-ER techniques with guided cost learning. By leveraging demonstrations, our algorithm automatically uncovers the reward function and learns an effective policy for the agents. Through extensive experimentation, we demonstrate that the proposed policy captures the behavior observed in the provided data, and achieves promising results across problem domains including single agent models in the OpenAI gym and multi-agent models of schooling behavior. The pr",
    "path": "papers/23/05/2305.10548.json",
    "total_tokens": 890,
    "translated_title": "通过反向多智能体强化学习发现集体行为中的个体奖励",
    "translated_abstract": "发现复杂动态系统（例如鱼群和细菌群落）中的集体行为中的个体目标是一个长期的挑战。逆强化学习是解决这一挑战的有效方法，但其在涉及连续状态-动作空间和多个交互代理的动态系统中的适用性受到限制。本研究通过引入一种离线逆向多智能体强化学习算法（IMARL）来解决这一挑战。我们的方法结合了ReF-ER技术和导向成本学习。通过利用演示，我们的算法自动发现奖励函数并学习代理的有效策略。通过广泛的实验，我们证明了所提出的策略捕捉到了提供数据中观察到的行为，并在包括OpenAI gym中的单代理模型和涉及多代理的模型中的问题域中取得了有希望的结果。",
    "tldr": "本论文介绍了一种离线逆向多智能体强化学习算法，通过利用演示，自动发现奖励函数并学习代理的有效策略，用于在复杂动态系统中寻找集体行为中的个体目标。",
    "en_tdlr": "This paper presents an off-policy inverse multi-agent reinforcement learning algorithm that automatically uncovers the reward function and learns an effective policy for agents by leveraging demonstrations. The proposed approach addresses the challenge of discovering individual objectives in collective behavior of complex dynamical systems and achieves promising results in various problem domains."
}