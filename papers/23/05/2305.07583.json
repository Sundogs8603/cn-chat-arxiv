{
    "title": "MoMo: Momentum Models for Adaptive Learning Rates. (arXiv:2305.07583v1 [cs.LG])",
    "abstract": "We present new adaptive learning rates that can be used with any momentum method. To showcase our new learning rates we develop MoMo and MoMo-Adam, which are SGD with momentum (SGDM) and Adam together with our new adaptive learning rates. Our MoMo methods are motivated through model-based stochastic optimization, wherein we use momentum estimates of the batch losses and gradients sampled at each iteration to build a model of the loss function. Our model also makes use of any known lower bound of the loss function by using truncation. Indeed most losses are bounded below by zero. We then approximately minimize this model at each iteration to compute the next step. For losses with unknown lower bounds, we develop new on-the-fly estimates of the lower bound that we use in our model. Numerical experiments show that our MoMo methods improve over SGDM and Adam in terms of accuracy and robustness to hyperparameter tuning for training image classifiers on MNIST, CIFAR10, CIFAR100, Imagenet32, ",
    "link": "http://arxiv.org/abs/2305.07583",
    "context": "Title: MoMo: Momentum Models for Adaptive Learning Rates. (arXiv:2305.07583v1 [cs.LG])\nAbstract: We present new adaptive learning rates that can be used with any momentum method. To showcase our new learning rates we develop MoMo and MoMo-Adam, which are SGD with momentum (SGDM) and Adam together with our new adaptive learning rates. Our MoMo methods are motivated through model-based stochastic optimization, wherein we use momentum estimates of the batch losses and gradients sampled at each iteration to build a model of the loss function. Our model also makes use of any known lower bound of the loss function by using truncation. Indeed most losses are bounded below by zero. We then approximately minimize this model at each iteration to compute the next step. For losses with unknown lower bounds, we develop new on-the-fly estimates of the lower bound that we use in our model. Numerical experiments show that our MoMo methods improve over SGDM and Adam in terms of accuracy and robustness to hyperparameter tuning for training image classifiers on MNIST, CIFAR10, CIFAR100, Imagenet32, ",
    "path": "papers/23/05/2305.07583.json",
    "total_tokens": 1005,
    "translated_title": "MoMo: 动量模型的自适应学习率",
    "translated_abstract": "本文提出了新的自适应学习率，可与任何动量方法一起使用。为了展示我们的新学习率，我们开发了MoMo和MoMo-Adam，它们是具有动量（SGDM）的SGD和Adam方法与我们的新自适应学习率一起使用。我们的MoMo方法是通过基于模型的随机优化来激发的，其中我们使用每次迭代采样的批量损失和梯度的动量估计来构建损失函数模型。我们的模型还利用了已知损失函数下限的截断方法。实际上，大多数损失都被下限为零。然后，在每次迭代中，我们近似最小化此模型以计算下一步。对于具有未知下限的损失，我们开发了新的即时下限估计，这些估计用于我们的模型中。数值实验表明，我们的MoMo方法在MNIST、CIFAR10、CIFAR100和Imagenet32等数据集的图像分类训练中，相较于SGDM和Adam，在精度和超参数调优的鲁棒性方面都有所提高。",
    "tldr": "本文提出了新的自适应学习率，可与任何动量方法一起使用，通过构建损失函数模型并使用下限截断，以及即时估计未知下限，来近似最小化该模型以计算下一步，实验表明，相较于SGDM和Adam，该方法在精度和超参数调优的鲁棒性方面有所提高。",
    "en_tdlr": "This paper proposes new adaptive learning rates that can be used with any momentum method, through building a model of the loss function and using truncation for known lower bounds, as well as developing on-the-fly estimates of lower bounds for unknown losses, the proposed method, MoMo, shows improvement over SGDM and Adam in terms of accuracy and robustness to hyperparameter tuning for image classification on various datasets."
}