{
    "title": "AMII: Adaptive Multimodal Inter-personal and Intra-personal Model for Adapted Behavior Synthesis. (arXiv:2305.11310v1 [cs.HC])",
    "abstract": "Socially Interactive Agents (SIAs) are physical or virtual embodied agents that display similar behavior as human multimodal behavior. Modeling SIAs' non-verbal behavior, such as speech and facial gestures, has always been a challenging task, given that a SIA can take the role of a speaker or a listener. A SIA must emit appropriate behavior adapted to its own speech, its previous behaviors (intra-personal), and the User's behaviors (inter-personal) for both roles. We propose AMII, a novel approach to synthesize adaptive facial gestures for SIAs while interacting with Users and acting interchangeably as a speaker or as a listener. AMII is characterized by modality memory encoding schema - where modality corresponds to either speech or facial gestures - and makes use of attention mechanisms to capture the intra-personal and inter-personal relationships. We validate our approach by conducting objective evaluations and comparing it with the state-of-the-art approaches.",
    "link": "http://arxiv.org/abs/2305.11310",
    "context": "Title: AMII: Adaptive Multimodal Inter-personal and Intra-personal Model for Adapted Behavior Synthesis. (arXiv:2305.11310v1 [cs.HC])\nAbstract: Socially Interactive Agents (SIAs) are physical or virtual embodied agents that display similar behavior as human multimodal behavior. Modeling SIAs' non-verbal behavior, such as speech and facial gestures, has always been a challenging task, given that a SIA can take the role of a speaker or a listener. A SIA must emit appropriate behavior adapted to its own speech, its previous behaviors (intra-personal), and the User's behaviors (inter-personal) for both roles. We propose AMII, a novel approach to synthesize adaptive facial gestures for SIAs while interacting with Users and acting interchangeably as a speaker or as a listener. AMII is characterized by modality memory encoding schema - where modality corresponds to either speech or facial gestures - and makes use of attention mechanisms to capture the intra-personal and inter-personal relationships. We validate our approach by conducting objective evaluations and comparing it with the state-of-the-art approaches.",
    "path": "papers/23/05/2305.11310.json",
    "total_tokens": 892,
    "translated_title": "AMII：自适应多模态人际和自我模型用于行为合成",
    "translated_abstract": "社交交互代理（SIAs）是显示与人类多模态行为类似的实体或虚拟化身的代理。对于建模SIAs的非语言行为（如语音和面部手势），始终是一项具有挑战性的任务，因为SIA可以扮演发言人或听众的角色。SIA在两种角色下都必须发出适当的行为，以适应其自己的语音、其以前的行为（自我）以及用户的行为（人际）。我们提出了AMII，一种新颖的方法，用于在SIAs与用户交互并交替扮演发言人或听众时合成自适应面部手势。AMII的特点是模态记忆编码模式，其中模态对应于语音或面部手势，并使用注意机制捕捉自我和人际关系。我们通过进行客观评估并与最先进的方法进行比较来验证我们的方法。",
    "tldr": "AMII是一种面部手势合成方法，通过模态记忆编码模式和注意机制，实现了对自我和人际关系的捕捉，从而适应性地显示行为。",
    "en_tdlr": "AMII is a novel approach to synthesize adaptive facial gestures for socially interactive agents (SIAs) using modality memory encoding schema and attention mechanisms to capture intra-personal and inter-personal relationships, which shows appropriate behavior adapted to its own speech and previous behaviors as well as the user's behaviors in both speaker and listener roles."
}