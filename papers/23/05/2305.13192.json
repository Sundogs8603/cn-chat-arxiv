{
    "title": "SimCSE++: Improving Contrastive Learning for Sentence Embeddings from Two Perspectives. (arXiv:2305.13192v2 [cs.CL] UPDATED)",
    "abstract": "This paper improves contrastive learning for sentence embeddings from two perspectives: handling dropout noise and addressing feature corruption. Specifically, for the first perspective, we identify that the dropout noise from negative pairs affects the model's performance. Therefore, we propose a simple yet effective method to deal with such type of noise. Secondly, we pinpoint the rank bottleneck of current solutions to feature corruption and propose a dimension-wise contrastive learning objective to address this issue. Both proposed methods are generic and can be applied to any contrastive learning based models for sentence embeddings. Experimental results on standard benchmarks demonstrate that combining both proposed methods leads to a gain of 1.8 points compared to the strong baseline SimCSE configured with BERT base. Furthermore, applying the proposed method to DiffCSE, another strong contrastive learning based baseline, results in a gain of 1.4 points.",
    "link": "http://arxiv.org/abs/2305.13192",
    "context": "Title: SimCSE++: Improving Contrastive Learning for Sentence Embeddings from Two Perspectives. (arXiv:2305.13192v2 [cs.CL] UPDATED)\nAbstract: This paper improves contrastive learning for sentence embeddings from two perspectives: handling dropout noise and addressing feature corruption. Specifically, for the first perspective, we identify that the dropout noise from negative pairs affects the model's performance. Therefore, we propose a simple yet effective method to deal with such type of noise. Secondly, we pinpoint the rank bottleneck of current solutions to feature corruption and propose a dimension-wise contrastive learning objective to address this issue. Both proposed methods are generic and can be applied to any contrastive learning based models for sentence embeddings. Experimental results on standard benchmarks demonstrate that combining both proposed methods leads to a gain of 1.8 points compared to the strong baseline SimCSE configured with BERT base. Furthermore, applying the proposed method to DiffCSE, another strong contrastive learning based baseline, results in a gain of 1.4 points.",
    "path": "papers/23/05/2305.13192.json",
    "total_tokens": 867,
    "translated_title": "SimCSE++：从两个角度改进对比学习用于句子嵌入",
    "translated_abstract": "本文从两个角度改进对比学习用于句子嵌入：处理dropout噪声和解决特征破坏问题。具体来说，对于第一个角度，我们发现来自负样本的dropout噪声影响了模型的性能。因此，我们提出了一种简单而有效的方法来处理这种类型的噪声。其次，我们找到了当前解决方法中特征破坏的排名瓶颈，并提出了一种维度-wise对比学习目标来解决这个问题。两种提出的方法是通用的，可以应用于任何基于对比学习的句子嵌入模型。标准基准实验结果表明，将两种提出的方法结合起来，相对于以BERT base配置的强基线SimCSE，可以获得1.8个点的增益。此外，将提出的方法应用于另一个强对比学习基线DiffCSE，可以获得1.4个点的增益。",
    "tldr": "本文从处理dropout噪声和解决特征破坏问题两个角度改进对比学习用于句子嵌入，提出的方法通用且有效，实验结果表明可以获得显著的性能提升。"
}