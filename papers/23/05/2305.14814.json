{
    "title": "What functions can Graph Neural Networks compute on random graphs? The role of Positional Encoding. (arXiv:2305.14814v1 [cs.LG])",
    "abstract": "We aim to deepen the theoretical understanding of Graph Neural Networks (GNNs) on large graphs, with a focus on their expressive power. Existing analyses relate this notion to the graph isomorphism problem, which is mostly relevant for graphs of small sizes, or studied graph classification or regression tasks, while prediction tasks on nodes are far more relevant on large graphs. Recently, several works showed that, on very general random graphs models, GNNs converge to certains functions as the number of nodes grows. In this paper, we provide a more complete and intuitive description of the function space generated by equivariant GNNs for node-tasks, through general notions of convergence that encompass several previous examples. We emphasize the role of input node features, and study the impact of node Positional Encodings (PEs), a recent line of work that has been shown to yield state-of-the-art results in practice. Through the study of several examples of PEs on large random graphs",
    "link": "http://arxiv.org/abs/2305.14814",
    "context": "Title: What functions can Graph Neural Networks compute on random graphs? The role of Positional Encoding. (arXiv:2305.14814v1 [cs.LG])\nAbstract: We aim to deepen the theoretical understanding of Graph Neural Networks (GNNs) on large graphs, with a focus on their expressive power. Existing analyses relate this notion to the graph isomorphism problem, which is mostly relevant for graphs of small sizes, or studied graph classification or regression tasks, while prediction tasks on nodes are far more relevant on large graphs. Recently, several works showed that, on very general random graphs models, GNNs converge to certains functions as the number of nodes grows. In this paper, we provide a more complete and intuitive description of the function space generated by equivariant GNNs for node-tasks, through general notions of convergence that encompass several previous examples. We emphasize the role of input node features, and study the impact of node Positional Encodings (PEs), a recent line of work that has been shown to yield state-of-the-art results in practice. Through the study of several examples of PEs on large random graphs",
    "path": "papers/23/05/2305.14814.json",
    "total_tokens": 874,
    "translated_title": "随机图上图神经网络能计算哪些函数？位置编码的作用。",
    "translated_abstract": "本文旨在深入理解大规模图中图神经网络（GNN）的表达能力。现有的分析将这个概念与图同构问题联系起来，这对于小规模的图最为相关，或者是研究了图分类或回归任务，而节点上的预测任务则更加相关。最近，几个作品表明，在非常通用的随机图模型上，随着节点数量的增加，GNN会收敛于某些函数。在本文中，我们通过更加完整和直观的收敛理论描述了用于节点任务的等变GNN生成的函数空间，该理论包括了以前的几个例子。我们强调了输入节点特征的作用，并研究了节点位置编码（PEs）的影响，这是一种最近的研究方向，已经在实践中证明了其在表现上的最佳结果。通过对大型随机图上多个PE示例的研究，",
    "tldr": "本文研究了大型随机图上等变GNN计算节点任务所生成的函数空间，强调了输入节点特征的作用和节点位置编码（PE）对表现的影响。",
    "en_tdlr": "This paper studies the function space generated by equivariant GNNs for node-tasks on large random graphs, emphasizing the role of input node features and studying the impact of node Positional Encodings (PEs)."
}