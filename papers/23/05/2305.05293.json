{
    "title": "On the Limitations of Model Stealing with Uncertainty Quantification Models. (arXiv:2305.05293v1 [cs.LG])",
    "abstract": "Model stealing aims at inferring a victim model's functionality at a fraction of the original training cost. While the goal is clear, in practice the model's architecture, weight dimension, and original training data can not be determined exactly, leading to mutual uncertainty during stealing. In this work, we explicitly tackle this uncertainty by generating multiple possible networks and combining their predictions to improve the quality of the stolen model. For this, we compare five popular uncertainty quantification models in a model stealing task. Surprisingly, our results indicate that the considered models only lead to marginal improvements in terms of label agreement (i.e., fidelity) to the stolen model. To find the cause of this, we inspect the diversity of the model's prediction by looking at the prediction variance as a function of training iterations. We realize that during training, the models tend to have similar predictions, indicating that the network diversity we wanted",
    "link": "http://arxiv.org/abs/2305.05293",
    "context": "Title: On the Limitations of Model Stealing with Uncertainty Quantification Models. (arXiv:2305.05293v1 [cs.LG])\nAbstract: Model stealing aims at inferring a victim model's functionality at a fraction of the original training cost. While the goal is clear, in practice the model's architecture, weight dimension, and original training data can not be determined exactly, leading to mutual uncertainty during stealing. In this work, we explicitly tackle this uncertainty by generating multiple possible networks and combining their predictions to improve the quality of the stolen model. For this, we compare five popular uncertainty quantification models in a model stealing task. Surprisingly, our results indicate that the considered models only lead to marginal improvements in terms of label agreement (i.e., fidelity) to the stolen model. To find the cause of this, we inspect the diversity of the model's prediction by looking at the prediction variance as a function of training iterations. We realize that during training, the models tend to have similar predictions, indicating that the network diversity we wanted",
    "path": "papers/23/05/2305.05293.json",
    "total_tokens": 974,
    "translated_title": "关于使用不确定性量化模型的模型盗窃限制",
    "translated_abstract": "模型盗窃旨在以原始训练成本的一小部分推断受害者模型的功能。然而，在实践中，模型的架构、权重尺寸和原始训练数据无法准确确定，导致在盗窃过程中相互不确定。在这项工作中，我们通过生成多个可能的网络，并将它们的预测组合起来来显式地处理这种不确定性，从而提高盗窃模型的质量。为此，我们比较了五个流行的不确定性量化模型在模型盗窃任务中的表现。令人惊讶的是，我们的结果表明，这些考虑的模型在标签一致性（即保真度）方面只能带来微小的改进。为了找到原因，我们通过查看预测方差作为训练迭代函数的方式来检查模型预测的多样性。我们意识到，在训练过程中，模型往往具有相似的预测，这表明我们想要利用的网络多样性不存在。",
    "tldr": "本文研究了使用不确定性量化模型进行模型盗窃的局限性，发现在实际盗窃过程中相互不确定是不可避免的。作者尝试使用多个可能的网络并将它们的预测组合以提高质量，但结果表明只有微弱的改善。作者发现网络多样性不足是导致这一结果的原因之一。",
    "en_tdlr": "This paper investigates the limitations of model stealing with uncertainty quantification models, and finds that mutual uncertainty is inevitable in practical stealing. The authors try to improve the quality by generating multiple possible networks and combining their predictions, but the results show only marginal improvements. It is revealed that the lack of network diversity is one of the reasons for this outcome."
}