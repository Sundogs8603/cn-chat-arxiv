{
    "title": "Optimality of Message-Passing Architectures for Sparse Graphs. (arXiv:2305.10391v1 [cs.LG])",
    "abstract": "We study the node classification problem on feature-decorated graphs in the sparse setting, i.e., when the expected degree of a node is $O(1)$ in the number of nodes. Such graphs are typically known to be locally tree-like. We introduce a notion of Bayes optimality for node classification tasks, called asymptotic local Bayes optimality, and compute the optimal classifier according to this criterion for a fairly general statistical data model with arbitrary distributions of the node features and edge connectivity. The optimal classifier is implementable using a message-passing graph neural network architecture. We then compute the generalization error of this classifier and compare its performance against existing learning methods theoretically on a well-studied statistical model with naturally identifiable signal-to-noise ratios (SNRs) in the data. We find that the optimal message-passing architecture interpolates between a standard MLP in the regime of low graph signal and a typical c",
    "link": "http://arxiv.org/abs/2305.10391",
    "context": "Title: Optimality of Message-Passing Architectures for Sparse Graphs. (arXiv:2305.10391v1 [cs.LG])\nAbstract: We study the node classification problem on feature-decorated graphs in the sparse setting, i.e., when the expected degree of a node is $O(1)$ in the number of nodes. Such graphs are typically known to be locally tree-like. We introduce a notion of Bayes optimality for node classification tasks, called asymptotic local Bayes optimality, and compute the optimal classifier according to this criterion for a fairly general statistical data model with arbitrary distributions of the node features and edge connectivity. The optimal classifier is implementable using a message-passing graph neural network architecture. We then compute the generalization error of this classifier and compare its performance against existing learning methods theoretically on a well-studied statistical model with naturally identifiable signal-to-noise ratios (SNRs) in the data. We find that the optimal message-passing architecture interpolates between a standard MLP in the regime of low graph signal and a typical c",
    "path": "papers/23/05/2305.10391.json",
    "total_tokens": 875,
    "translated_title": "稀疏图的消息传递架构的最优性",
    "translated_abstract": "我们研究了特征装饰图上的节点分类问题，在稀疏设置下，即节点的预期度数为节点数的O(1)时。这样的图通常被称为本地树状图。我们引入了一种叫做渐近本地贝叶斯最优性的节点分类任务的贝叶斯最优性概念，并根据这个标准计算了具有任意节点特征和边连接分布的相当一般的统计数据模型的最优分类器。该最优分类器可以使用消息传递图神经网络架构实现。然后我们计算了该分类器的泛化误差，并在一个已经研究充分的统计模型上从理论上与现有的学习方法进行比较。我们发现，在低图信号的情况下，最佳消息传递架构插值于标准MLP和一种典型的c架构之间。",
    "tldr": "本研究证明了将消息传递神经网络应用于稀疏图的节点分类任务是渐近本地贝叶斯最优的，提出了一种实现最优分类器的算法，并将最优分类器的性能理论上与现有学习方法进行了比较。",
    "en_tdlr": "This study proves that applying message-passing neural networks to node classification tasks on sparse graphs is asymptotically locally Bayes optimal. It proposes an algorithm to implement the optimal classifier and theoretically compares its performance with existing learning methods."
}