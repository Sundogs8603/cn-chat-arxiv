{
    "title": "Diagnosing Transformers: Illuminating Feature Spaces for Clinical Decision-Making. (arXiv:2305.17588v2 [cs.CL] UPDATED)",
    "abstract": "Pre-trained transformers are often fine-tuned to aid clinical decision-making using limited clinical notes. Model interpretability is crucial, especially in high-stakes domains like medicine, to establish trust and ensure safety, which requires human engagement. We introduce SUFO, a systematic framework that enhances interpretability of fine-tuned transformer feature spaces. SUFO utilizes a range of analytic and visualization techniques, including Supervised probing, Unsupervised similarity analysis, Feature dynamics, and Outlier analysis to address key questions about model trust and interpretability. We conduct a case study investigating the impact of pre-training data where we focus on real-world pathology classification tasks, and validate our findings on MedNLI. We evaluate five 110M-sized pre-trained transformer models, categorized into general-domain (BERT, TNLR), mixed-domain (BioBERT, Clinical BioBERT), and domain-specific (PubMedBERT) groups. Our SUFO analyses reveal that: (1",
    "link": "http://arxiv.org/abs/2305.17588",
    "context": "Title: Diagnosing Transformers: Illuminating Feature Spaces for Clinical Decision-Making. (arXiv:2305.17588v2 [cs.CL] UPDATED)\nAbstract: Pre-trained transformers are often fine-tuned to aid clinical decision-making using limited clinical notes. Model interpretability is crucial, especially in high-stakes domains like medicine, to establish trust and ensure safety, which requires human engagement. We introduce SUFO, a systematic framework that enhances interpretability of fine-tuned transformer feature spaces. SUFO utilizes a range of analytic and visualization techniques, including Supervised probing, Unsupervised similarity analysis, Feature dynamics, and Outlier analysis to address key questions about model trust and interpretability. We conduct a case study investigating the impact of pre-training data where we focus on real-world pathology classification tasks, and validate our findings on MedNLI. We evaluate five 110M-sized pre-trained transformer models, categorized into general-domain (BERT, TNLR), mixed-domain (BioBERT, Clinical BioBERT), and domain-specific (PubMedBERT) groups. Our SUFO analyses reveal that: (1",
    "path": "papers/23/05/2305.17588.json",
    "total_tokens": 957,
    "translated_title": "诊断变压器：揭示临床决策中的特征空间。 (arXiv:2305.17588v2 [cs.CL] UPDATED)",
    "translated_abstract": "在医学等高风险领域，为了建立信任和确保安全，模型的可解释性至关重要，而使用有限的临床记录对预训练的变压器进行微调以辅助临床决策。我们引入了一种名为SUFO的系统框架，该框架增强了微调的变压器特征空间的可解释性。SUFO利用一系列分析和可视化技术，包括监督探索、无监督相似性分析、特征动态和异常值分析，来解决关于模型信任和可解释性的关键问题。我们进行了一个案例研究，研究了预训练数据对真实世界病理分类任务的影响，并在MedNLI上验证了我们的发现。我们评估了五个110M规模的预训练变压器模型，分为通用领域（BERT, TNLR）、混合领域（BioBERT, Clinical BioBERT）和领域特定（PubMedBERT）组。我们的SUFO分析揭示了：(1)",
    "tldr": "该论文介绍了一种名为SUFO的框架，用于增强微调的变压器模型的可解释性。该框架利用多种分析和可视化技术，解决了模型信任和可解释性的关键问题，并通过案例研究验证了其有效性。"
}