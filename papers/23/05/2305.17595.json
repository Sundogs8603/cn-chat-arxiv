{
    "title": "Python Wrapper for Simulating Multi-Fidelity Optimization on HPO Benchmarks without Any Wait. (arXiv:2305.17595v2 [cs.LG] UPDATED)",
    "abstract": "Hyperparameter (HP) optimization of deep learning (DL) is essential for high performance. As DL often requires several hours to days for its training, HP optimization (HPO) of DL is often prohibitively expensive. This boosted the emergence of tabular or surrogate benchmarks, which enable querying the (predictive) performance of DL with a specific HP configuration in a fraction. However, since the actual runtime of a DL training is significantly different from its query response time, simulators of an asynchronous HPO, e.g. multi-fidelity optimization, must wait for the actual runtime at each iteration in a na\\\"ive implementation; otherwise, the evaluation order during simulation does not match with the real experiment. To ease this issue, we developed a Python wrapper and describe its usage. This wrapper forces each worker to wait so that we yield exactly the same evaluation order as in the real experiment with only $10^{-2}$ seconds of waiting instead of waiting several hours. Our imp",
    "link": "http://arxiv.org/abs/2305.17595",
    "context": "Title: Python Wrapper for Simulating Multi-Fidelity Optimization on HPO Benchmarks without Any Wait. (arXiv:2305.17595v2 [cs.LG] UPDATED)\nAbstract: Hyperparameter (HP) optimization of deep learning (DL) is essential for high performance. As DL often requires several hours to days for its training, HP optimization (HPO) of DL is often prohibitively expensive. This boosted the emergence of tabular or surrogate benchmarks, which enable querying the (predictive) performance of DL with a specific HP configuration in a fraction. However, since the actual runtime of a DL training is significantly different from its query response time, simulators of an asynchronous HPO, e.g. multi-fidelity optimization, must wait for the actual runtime at each iteration in a na\\\"ive implementation; otherwise, the evaluation order during simulation does not match with the real experiment. To ease this issue, we developed a Python wrapper and describe its usage. This wrapper forces each worker to wait so that we yield exactly the same evaluation order as in the real experiment with only $10^{-2}$ seconds of waiting instead of waiting several hours. Our imp",
    "path": "papers/23/05/2305.17595.json",
    "total_tokens": 934,
    "translated_title": "Python封装器用于在HPO基准测试上模拟多保真度优化，无需等待",
    "translated_abstract": "深度学习的超参数（HP）优化对于高性能至关重要。由于深度学习往往需要几小时到几天的训练时间，因此深度学习的HP优化通常是难以承受的昂贵的。这促使出现了表格或替代基准测试，可以在一小部分时间内查询特定HP配置的DL的（预测）性能。然而，由于DL训练的实际运行时间与查询响应时间明显不同，异步HPO（例如多保真度优化）的模拟器必须在每次迭代中等待实际运行时间，否则模拟中的评估顺序不符合实际实验。为了解决这个问题，我们开发了一个Python封装器并描述了它的用法。这个封装器强制每个工作进程等待，以便我们只需等待$10^{-2}$秒，就可以获得与实际实验完全相同的评估顺序，而不是等待几个小时。",
    "tldr": "本研究开发了一个Python封装器，用于在HPO基准测试上模拟多保真度优化，通过强制每个工作进程等待，可以减少多小时的等待时间，使得模拟结果与实际实验的评估顺序完全一致。",
    "en_tdlr": "This paper presents a Python wrapper for simulating multi-fidelity optimization on HPO benchmarks. By forcing each worker to wait, the simulated evaluation order matches exactly with the real experiment, reducing the waiting time from several hours to a fraction of a second."
}