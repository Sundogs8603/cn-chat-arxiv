{
    "title": "Diff-Instruct: A Universal Approach for Transferring Knowledge From Pre-trained Diffusion Models. (arXiv:2305.18455v1 [cs.LG])",
    "abstract": "Due to the ease of training, ability to scale, and high sample quality, diffusion models (DMs) have become the preferred option for generative modeling, with numerous pre-trained models available for a wide variety of datasets. Containing intricate information about data distributions, pre-trained DMs are valuable assets for downstream applications. In this work, we consider learning from pre-trained DMs and transferring their knowledge to other generative models in a data-free fashion. Specifically, we propose a general framework called Diff-Instruct to instruct the training of arbitrary generative models as long as the generated samples are differentiable with respect to the model parameters. Our proposed Diff-Instruct is built on a rigorous mathematical foundation where the instruction process directly corresponds to minimizing a novel divergence we call Integral Kullback-Leibler (IKL) divergence. IKL is tailored for DMs by calculating the integral of the KL divergence along a diffu",
    "link": "http://arxiv.org/abs/2305.18455",
    "context": "Title: Diff-Instruct: A Universal Approach for Transferring Knowledge From Pre-trained Diffusion Models. (arXiv:2305.18455v1 [cs.LG])\nAbstract: Due to the ease of training, ability to scale, and high sample quality, diffusion models (DMs) have become the preferred option for generative modeling, with numerous pre-trained models available for a wide variety of datasets. Containing intricate information about data distributions, pre-trained DMs are valuable assets for downstream applications. In this work, we consider learning from pre-trained DMs and transferring their knowledge to other generative models in a data-free fashion. Specifically, we propose a general framework called Diff-Instruct to instruct the training of arbitrary generative models as long as the generated samples are differentiable with respect to the model parameters. Our proposed Diff-Instruct is built on a rigorous mathematical foundation where the instruction process directly corresponds to minimizing a novel divergence we call Integral Kullback-Leibler (IKL) divergence. IKL is tailored for DMs by calculating the integral of the KL divergence along a diffu",
    "path": "papers/23/05/2305.18455.json",
    "total_tokens": 1148,
    "translated_title": "Diff-Instruct: 一种从预训练扩散模型中传递知识的通用方法",
    "translated_abstract": "由于训练容易、可扩展性和样本质量高，扩散模型 (DMs) 已成为生成建模的首选方法，并有大量已预训练的模型适用于各种数据集。预训练 DMs 包含有关数据分布的复杂信息，对下游应用非常有价值。本文考虑从预训练 DMs 中学习并以无需数据方式将其知识传递给其他生成模型。具体而言，我们提出了一个通用框架 Diff-Instruct，该框架能指导任何生成模型的训练，只要生成的样本在模型参数方面是可微的。我们的 Diff-Instruct 建立在一个严谨的数学基础上，其中指导过程直接对应于最小化称为积分Kullback-Leibler (IKL) 散度的新型散度。IKL 是针对 DMs 定制的，通过计算沿扩散轨迹的 KL 散度的积分来捕获扩散过程信息，因此只需要一个预训练 DM 和一个数据集。我们通过在三个不同的应用程序：半监督学习、图像合成和视频预测中展示其优越性来证明了我们方法的有效性。",
    "tldr": "本文提出了一种通用框架 Diff-Instruct，能够以无需数据方式将预训练扩散模型中的知识传递给其他生成模型，仅需预训练 DM 和一个数据集。该框架是建立在严谨的数学基础上的，指导过程直接对应于最小化一种新型散度——Integral Kullback-Leibler (IKL) 散度。我们的方法在半监督学习、图像合成和视频预测中展示了其优越性。",
    "en_tdlr": "This paper proposes a universal framework called Diff-Instruct to transfer knowledge from pre-trained diffusion models to other generative models in a data-free fashion. The proposed framework is built on a rigorous mathematical foundation where the instruction process directly corresponds to the minimization of a novel divergence called Integral Kullback-Leibler (IKL) divergence. The effectiveness of the method is demonstrated in three different applications: semi-supervised learning, image synthesis, and video prediction."
}