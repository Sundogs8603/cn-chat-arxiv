{
    "title": "Causal-Based Supervision of Attention in Graph Neural Network: A Better and Simpler Choice towards Powerful Attention. (arXiv:2305.13115v2 [cs.LG] UPDATED)",
    "abstract": "Recent years have witnessed the great potential of attention mechanism in graph representation learning. However, while variants of attention-based GNNs are setting new benchmarks for numerous real-world datasets, recent works have pointed out that their induced attentions are less robust and generalizable against noisy graphs due to lack of direct supervision. In this paper, we present a new framework which utilizes the tool of causality to provide a powerful supervision signal for the learning process of attention functions. Specifically, we estimate the direct causal effect of attention to the final prediction, and then maximize such effect to guide attention attending to more meaningful neighbors. Our method can serve as a plug-and-play module for any canonical attention-based GNNs in an end-to-end fashion. Extensive experiments on a wide range of benchmark datasets illustrated that, by directly supervising attention functions, the model is able to converge faster with a clearer de",
    "link": "http://arxiv.org/abs/2305.13115",
    "context": "Title: Causal-Based Supervision of Attention in Graph Neural Network: A Better and Simpler Choice towards Powerful Attention. (arXiv:2305.13115v2 [cs.LG] UPDATED)\nAbstract: Recent years have witnessed the great potential of attention mechanism in graph representation learning. However, while variants of attention-based GNNs are setting new benchmarks for numerous real-world datasets, recent works have pointed out that their induced attentions are less robust and generalizable against noisy graphs due to lack of direct supervision. In this paper, we present a new framework which utilizes the tool of causality to provide a powerful supervision signal for the learning process of attention functions. Specifically, we estimate the direct causal effect of attention to the final prediction, and then maximize such effect to guide attention attending to more meaningful neighbors. Our method can serve as a plug-and-play module for any canonical attention-based GNNs in an end-to-end fashion. Extensive experiments on a wide range of benchmark datasets illustrated that, by directly supervising attention functions, the model is able to converge faster with a clearer de",
    "path": "papers/23/05/2305.13115.json",
    "total_tokens": 923,
    "translated_title": "基于因果推理的图神经网络的监督注意力：更好和更简单的选择，实现强大的关注力。",
    "translated_abstract": "最近几年，注意力机制在图表示学习中展现了巨大的潜力。然而，虽然基于注意力的图神经网络的变体正在为许多现实世界的数据集设定新的基准，但最近的研究指出，由于缺乏直接监督，它们所产生的关注力对于嘈杂的图表达不够稳健和具有一般性。在本文中，我们提出了一个新的框架，利用因果性工具为注意力函数的学习过程提供强大的监督信号。具体而言，我们估计了注意力对于最终预测的直接因果效应，然后最大化该效应，引导注意力关注更有意义的邻居。我们的方法可以作为任何经典的基于注意力的图神经网络的即插即用模块，在端到端的方式下使用。广泛的实验在各种基准数据集上表明，通过直接监督注意力函数，模型能够更快地收敛并产生更清晰的结果。",
    "tldr": "这篇论文提出了一种基于因果推理的框架，通过直接监督注意力函数，提供了强大的监督信号，使得基于注意力的图神经网络在嘈杂的图表达中更加稳健和具有一般性。",
    "en_tdlr": "This paper proposes a framework based on causal-based reasoning to provide strong supervision for attention functions in graph neural networks, making them more robust and generalizable against noisy graphs."
}