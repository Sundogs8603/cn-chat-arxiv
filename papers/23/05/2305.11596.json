{
    "title": "Mitigating Backdoor Poisoning Attacks through the Lens of Spurious Correlation. (arXiv:2305.11596v1 [cs.CL])",
    "abstract": "Modern NLP models are often trained over large untrusted datasets, raising the potential for a malicious adversary to compromise model behaviour. For instance, backdoors can be implanted through crafting training instances with a specific textual trigger and a target label. This paper posits that backdoor poisoning attacks exhibit spurious correlation between simple text features and classification labels, and accordingly, proposes methods for mitigating spurious correlation as means of defence. Our empirical study reveals that the malicious triggers are highly correlated to their target labels; therefore such correlations are extremely distinguishable compared to those scores of benign features, and can be used to filter out potentially problematic instances. Compared with several existing defences, our defence method significantly reduces attack success rates across backdoor attacks, and in the case of insertion based attacks, our method provides a near-perfect defence.",
    "link": "http://arxiv.org/abs/2305.11596",
    "context": "Title: Mitigating Backdoor Poisoning Attacks through the Lens of Spurious Correlation. (arXiv:2305.11596v1 [cs.CL])\nAbstract: Modern NLP models are often trained over large untrusted datasets, raising the potential for a malicious adversary to compromise model behaviour. For instance, backdoors can be implanted through crafting training instances with a specific textual trigger and a target label. This paper posits that backdoor poisoning attacks exhibit spurious correlation between simple text features and classification labels, and accordingly, proposes methods for mitigating spurious correlation as means of defence. Our empirical study reveals that the malicious triggers are highly correlated to their target labels; therefore such correlations are extremely distinguishable compared to those scores of benign features, and can be used to filter out potentially problematic instances. Compared with several existing defences, our defence method significantly reduces attack success rates across backdoor attacks, and in the case of insertion based attacks, our method provides a near-perfect defence.",
    "path": "papers/23/05/2305.11596.json",
    "total_tokens": 866,
    "translated_title": "透过虚假相关性的视角缓解后门污染攻击",
    "translated_abstract": "现代NLP模型通常在大型不可信数据集上进行训练，这提高了恶意对手破坏模型行为的可能性。本文认为，后门污染攻击展示了简单文本特征和分类标签之间的虚假相关性，因此提出了减轻虚假相关性的方法作为防御手段。我们的实证研究表明，恶意触发器与其目标标签高度相关，因此这些相关性与良性特征得分相比极易区分，并可用于过滤可能有问题的实例。与几种现有的防御措施相比，我们的防御方法显著降低了所有后门攻击的成功率，并且在插入式攻击的情况下，我们的方法提供了几乎完美的防御。",
    "tldr": "本文提出了一种缓解后门污染攻击的方法，通过减轻文本特征和分类标签之间的虚假相关性来防御攻击，可以显著降低所有后门攻击的成功率，并在插入式攻击的情况下提供了几乎完美的防御。",
    "en_tdlr": "This paper proposes a method for mitigating backdoor poisoning attacks by reducing spurious correlation between simple text features and classification labels as a means of defence, which significantly reduces attack success rates for all backdoor attacks and provides near-perfect defence in insertion-based attacks."
}