{
    "title": "Bulk-Switching Memristor-based Compute-In-Memory Module for Deep Neural Network Training. (arXiv:2305.14547v1 [cs.AR])",
    "abstract": "The need for deep neural network (DNN) models with higher performance and better functionality leads to the proliferation of very large models. Model training, however, requires intensive computation time and energy. Memristor-based compute-in-memory (CIM) modules can perform vector-matrix multiplication (VMM) in situ and in parallel, and have shown great promises in DNN inference applications. However, CIM-based model training faces challenges due to non-linear weight updates, device variations, and low-precision in analog computing circuits. In this work, we experimentally implement a mixed-precision training scheme to mitigate these effects using a bulk-switching memristor CIM module. Lowprecision CIM modules are used to accelerate the expensive VMM operations, with high precision weight updates accumulated in digital units. Memristor devices are only changed when the accumulated weight update value exceeds a pre-defined threshold. The proposed scheme is implemented with a system-on",
    "link": "http://arxiv.org/abs/2305.14547",
    "context": "Title: Bulk-Switching Memristor-based Compute-In-Memory Module for Deep Neural Network Training. (arXiv:2305.14547v1 [cs.AR])\nAbstract: The need for deep neural network (DNN) models with higher performance and better functionality leads to the proliferation of very large models. Model training, however, requires intensive computation time and energy. Memristor-based compute-in-memory (CIM) modules can perform vector-matrix multiplication (VMM) in situ and in parallel, and have shown great promises in DNN inference applications. However, CIM-based model training faces challenges due to non-linear weight updates, device variations, and low-precision in analog computing circuits. In this work, we experimentally implement a mixed-precision training scheme to mitigate these effects using a bulk-switching memristor CIM module. Lowprecision CIM modules are used to accelerate the expensive VMM operations, with high precision weight updates accumulated in digital units. Memristor devices are only changed when the accumulated weight update value exceeds a pre-defined threshold. The proposed scheme is implemented with a system-on",
    "path": "papers/23/05/2305.14547.json",
    "total_tokens": 1283,
    "translated_title": "基于体效应开关忆阻器的内存计算模块用于深度神经网络训练",
    "translated_abstract": "越来越大的深度学习模型需要更高性能和更好的功能，但模型训练需要高强度的计算时间和能量。基于忆阻器的内存计算（CIM）模块可以在原地和并行执行向量-矩阵乘法（VMM），在DNN推理应用中表现出极大的应用前景。然而，基于CIM的模型训练面临着非线性权重更新、器件变化和模拟计算电路低精度等挑战。在本文中，我们利用基于体效应开关忆阻器的CIM模块实现了一种混合精度训练方案以缓解这些影响。低精度CIM模块用于加速昂贵的VMM操作，而高精度权重更新在数字单元中累积。当累积的权重更新值超过预定义的阈值时才更改忆阻器设备。所提出的方案采用SoC设计实现，并使用两个基准数据集进行评估。实验结果表明，与传统数字实现相比，所提出的CIM模块实现了高达3倍的加速和2.5倍的能量效率，并且与全精度数字训练相比，最高可达94%的权重更新精度。",
    "tldr": "本文提出了一种基于体效应开关忆阻器的内存计算模块用于深度神经网络（DNN）模型的训练。实现了一种混合精度训练方案，使用低精度内存计算（CIM）模块加速昂贵的向量-矩阵乘法（VMM）操作，并在数字单元中积累高精度的权重更新，通化累计的权重更新值超过阈值时，才更新忆阻器设备。实验结果表明，所提出的CIM模块相对于传统数字实现的加速和效率可达到3倍和2.5倍，并且与全精度数字训练相比，权重更新精度最高可达94%。"
}