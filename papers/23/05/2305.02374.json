{
    "title": "A Novel Plagiarism Detection Approach Combining BERT-based Word Embedding, Attention-based LSTMs and an Improved Differential Evolution Algorithm. (arXiv:2305.02374v1 [cs.CL])",
    "abstract": "Detecting plagiarism involves finding similar items in two different sources. In this article, we propose a novel method for detecting plagiarism that is based on attention mechanism-based long short-term memory (LSTM) and bidirectional encoder representations from transformers (BERT) word embedding, enhanced with optimized differential evolution (DE) method for pre-training and a focal loss function for training. BERT could be included in a downstream task and fine-tuned as a task-specific BERT can be included in a downstream task and fine-tuned as a task-specific structure, while the trained BERT model is capable of detecting various linguistic characteristics. Unbalanced classification is one of the primary issues with plagiarism detection. We suggest a focal loss-based training technique that carefully learns minority class instances to solve this. Another issue that we tackle is the training phase itself, which typically employs gradient-based methods like back-propagation for the",
    "link": "http://arxiv.org/abs/2305.02374",
    "context": "Title: A Novel Plagiarism Detection Approach Combining BERT-based Word Embedding, Attention-based LSTMs and an Improved Differential Evolution Algorithm. (arXiv:2305.02374v1 [cs.CL])\nAbstract: Detecting plagiarism involves finding similar items in two different sources. In this article, we propose a novel method for detecting plagiarism that is based on attention mechanism-based long short-term memory (LSTM) and bidirectional encoder representations from transformers (BERT) word embedding, enhanced with optimized differential evolution (DE) method for pre-training and a focal loss function for training. BERT could be included in a downstream task and fine-tuned as a task-specific BERT can be included in a downstream task and fine-tuned as a task-specific structure, while the trained BERT model is capable of detecting various linguistic characteristics. Unbalanced classification is one of the primary issues with plagiarism detection. We suggest a focal loss-based training technique that carefully learns minority class instances to solve this. Another issue that we tackle is the training phase itself, which typically employs gradient-based methods like back-propagation for the",
    "path": "papers/23/05/2305.02374.json",
    "total_tokens": 726,
    "translated_abstract": "抄袭检测涉及在两个不同的来源中找到相似的内容。本文提出了一种基于注意力机制LSTM和BERT词嵌入的新型抄袭检测方法，利用优化的微分进化算法进行预训练，使用focal loss函数进行训练，解决了非平衡分类和训练阶段的问题。",
    "tldr": "本文提出了一种结合BERT词嵌入、注意力机制LSTM和改进微分进化算法的新型抄袭检测方法，使用focal loss函数进行训练解决了非平衡分类和训练阶段的问题。"
}