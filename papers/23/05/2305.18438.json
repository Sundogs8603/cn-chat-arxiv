{
    "title": "Reinforcement Learning with Human Feedback: Learning Dynamic Choices via Pessimism. (arXiv:2305.18438v1 [cs.LG])",
    "abstract": "In this paper, we study offline Reinforcement Learning with Human Feedback (RLHF) where we aim to learn the human's underlying reward and the MDP's optimal policy from a set of trajectories induced by human choices. RLHF is challenging for multiple reasons: large state space but limited human feedback, the bounded rationality of human decisions, and the off-policy distribution shift. In this paper, we focus on the Dynamic Discrete Choice (DDC) model for modeling and understanding human choices. DCC, rooted in econometrics and decision theory, is widely used to model a human decision-making process with forward-looking and bounded rationality. We propose a \\underline{D}ynamic-\\underline{C}hoice-\\underline{P}essimistic-\\underline{P}olicy-\\underline{O}ptimization (DCPPO) method. \\ The method involves a three-stage process: The first step is to estimate the human behavior policy and the state-action value function via maximum likelihood estimation (MLE); the second step recovers the human ",
    "link": "http://arxiv.org/abs/2305.18438",
    "context": "Title: Reinforcement Learning with Human Feedback: Learning Dynamic Choices via Pessimism. (arXiv:2305.18438v1 [cs.LG])\nAbstract: In this paper, we study offline Reinforcement Learning with Human Feedback (RLHF) where we aim to learn the human's underlying reward and the MDP's optimal policy from a set of trajectories induced by human choices. RLHF is challenging for multiple reasons: large state space but limited human feedback, the bounded rationality of human decisions, and the off-policy distribution shift. In this paper, we focus on the Dynamic Discrete Choice (DDC) model for modeling and understanding human choices. DCC, rooted in econometrics and decision theory, is widely used to model a human decision-making process with forward-looking and bounded rationality. We propose a \\underline{D}ynamic-\\underline{C}hoice-\\underline{P}essimistic-\\underline{P}olicy-\\underline{O}ptimization (DCPPO) method. \\ The method involves a three-stage process: The first step is to estimate the human behavior policy and the state-action value function via maximum likelihood estimation (MLE); the second step recovers the human ",
    "path": "papers/23/05/2305.18438.json",
    "total_tokens": 920,
    "tldr": "本文提出了一种基于动态选择悲观主义策略优化的RLHF方法，用于从人类选择轨迹中学习人类的意愿和理性，并改进当前政策以保持相对政策价值不变。"
}