{
    "title": "Adversarial Adaptive Sampling: Unify PINN and Optimal Transport for the Approximation of PDEs. (arXiv:2305.18702v1 [stat.ML])",
    "abstract": "Solving partial differential equations (PDEs) is a central task in scientific computing. Recently, neural network approximation of PDEs has received increasing attention due to its flexible meshless discretization and its potential for high-dimensional problems. One fundamental numerical difficulty is that random samples in the training set introduce statistical errors into the discretization of loss functional which may become the dominant error in the final approximation, and therefore overshadow the modeling capability of the neural network. In this work, we propose a new minmax formulation to optimize simultaneously the approximate solution, given by a neural network model, and the random samples in the training set, provided by a deep generative model. The key idea is to use a deep generative model to adjust random samples in the training set such that the residual induced by the approximate PDE solution can maintain a smooth profile when it is being minimized. Such an idea is ach",
    "link": "http://arxiv.org/abs/2305.18702",
    "context": "Title: Adversarial Adaptive Sampling: Unify PINN and Optimal Transport for the Approximation of PDEs. (arXiv:2305.18702v1 [stat.ML])\nAbstract: Solving partial differential equations (PDEs) is a central task in scientific computing. Recently, neural network approximation of PDEs has received increasing attention due to its flexible meshless discretization and its potential for high-dimensional problems. One fundamental numerical difficulty is that random samples in the training set introduce statistical errors into the discretization of loss functional which may become the dominant error in the final approximation, and therefore overshadow the modeling capability of the neural network. In this work, we propose a new minmax formulation to optimize simultaneously the approximate solution, given by a neural network model, and the random samples in the training set, provided by a deep generative model. The key idea is to use a deep generative model to adjust random samples in the training set such that the residual induced by the approximate PDE solution can maintain a smooth profile when it is being minimized. Such an idea is ach",
    "path": "papers/23/05/2305.18702.json",
    "total_tokens": 1201,
    "translated_title": "对抗式自适应采样：将PINN和最优传输统一用于PDE近似",
    "translated_abstract": "求解偏微分方程（PDE）是科学计算的一个核心任务。近年来，使用神经网络逼近PDE引起了越来越多的关注，具有无网格离散的灵活性和解决高维问题的潜力。一个基本的计算困难是训练集中的随机样本引入了统计错误，可能成为最终逼近中占主导的误差，从而掩盖了神经网络的建模能力。本文提出了一种新的minmax公式，同时优化近似的解和由深度生成模型提供的训练集中的随机样本。关键思想是使用深度生成模型调整训练集中的随机样本，使近似PDE解引起的残余在最小化时能保持平滑的轮廓。这种想法是通过在PINN优化过程中引入对抗性损失项来实现的，该损失项鼓励神经网络学习稳定的解，即使训练集中的样本有限或输入含噪声。我们进一步展示，所提出的方法可以自然地扩展到纳入最优传输约束，从而形成将PINN和最优传输的优点结合起来的统一框架，用于PDE近似。",
    "tldr": "本研究提出了一种新的深度生成模型来调整训练集中的随机样本，以使PDE解的残余在最小化时能保持平滑的轮廓，并通过引入对抗性损失项优化PINN模型，从而使神经网络学习稳定的解。同时本文还展示了该方法可以扩展到纳入最优传输约束，从而形成了将PINN和最优传输的优点结合起来的统一框架，用于PDE近似。",
    "en_tdlr": "This study proposes a new minmax formulation that optimizes the approximate solution and the random samples in the training set simultaneously by using a deep generative model to adjust the random samples in the training set and introducing an adversarial loss term into the PINN optimization process, leading to a stable neural network solution even when the training set has limited samples or noisy inputs. Furthermore, the proposed method can be extended to incorporate optimal transport constraints, combining the strengths of PINNs and optimal transport for PDE approximation."
}