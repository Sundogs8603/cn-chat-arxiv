{
    "title": "Assessing the Impact of Context Inference Error and Partial Observability on RL Methods for Just-In-Time Adaptive Interventions. (arXiv:2305.09913v1 [cs.LG])",
    "abstract": "Just-in-Time Adaptive Interventions (JITAIs) are a class of personalized health interventions developed within the behavioral science community. JITAIs aim to provide the right type and amount of support by iteratively selecting a sequence of intervention options from a pre-defined set of components in response to each individual's time varying state. In this work, we explore the application of reinforcement learning methods to the problem of learning intervention option selection policies. We study the effect of context inference error and partial observability on the ability to learn effective policies. Our results show that the propagation of uncertainty from context inferences is critical to improving intervention efficacy as context uncertainty increases, while policy gradient algorithms can provide remarkable robustness to partially observed behavioral state information.",
    "link": "http://arxiv.org/abs/2305.09913",
    "context": "Title: Assessing the Impact of Context Inference Error and Partial Observability on RL Methods for Just-In-Time Adaptive Interventions. (arXiv:2305.09913v1 [cs.LG])\nAbstract: Just-in-Time Adaptive Interventions (JITAIs) are a class of personalized health interventions developed within the behavioral science community. JITAIs aim to provide the right type and amount of support by iteratively selecting a sequence of intervention options from a pre-defined set of components in response to each individual's time varying state. In this work, we explore the application of reinforcement learning methods to the problem of learning intervention option selection policies. We study the effect of context inference error and partial observability on the ability to learn effective policies. Our results show that the propagation of uncertainty from context inferences is critical to improving intervention efficacy as context uncertainty increases, while policy gradient algorithms can provide remarkable robustness to partially observed behavioral state information.",
    "path": "papers/23/05/2305.09913.json",
    "total_tokens": 942,
    "translated_title": "评估上下文推断误差和部分可观察性对及时自适应干预RL方法的影响",
    "translated_abstract": "及时自适应干预(JITAIs)是行为科学界开发的一类个性化健康干预。 JITAIs旨在通过从预定义的组件集中迭代选择干预选项序列来响应每个个体的时间变化状态，以提供正确类型和数量的支持。在这项工作中，我们探讨了强化学习方法应用于学习干预选项选择策略的问题。我们研究了上下文推断误差和部分可观察性对学习有效策略的能力的影响。我们的结果表明，当上下文不确定性增加时，从上下文推断中传播的不确定性对提高干预效果至关重要，而策略梯度算法可以提供对部分观察到的行为状态信息的非凡鲁棒性。",
    "tldr": "本文探讨了在及时自适应干预中，强化学习方法如何学习干预选项选择策略，结果表明上下文推断误差和部分可观察性对学习有效策略的能力产生影响，通过在上下文不确定性增加时从上下文推断中传播的不确定性可以提高干预效果，而策略梯度算法可以提供对部分观察到的行为状态信息的非凡鲁棒性。",
    "en_tdlr": "This paper explores how reinforcement learning methods can be used to learn intervention option selection policies for Just-in-Time Adaptive Interventions(JITAIs). The study shows that context inference error and partial observability can affect the ability to learn effective policies, and that policy gradient algorithms can provide remarkable robustness to partially observed behavioral state information while propagating uncertainty from context inferences is critical to improving intervention efficacy as context uncertainty increases."
}