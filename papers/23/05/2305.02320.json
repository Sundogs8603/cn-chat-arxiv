{
    "title": "Generating Synthetic Documents for Cross-Encoder Re-Rankers: A Comparative Study of ChatGPT and Human Experts. (arXiv:2305.02320v1 [cs.IR])",
    "abstract": "We investigate the usefulness of generative Large Language Models (LLMs) in generating training data for cross-encoder re-rankers in a novel direction: generating synthetic documents instead of synthetic queries. We introduce a new dataset, ChatGPT-RetrievalQA, and compare the effectiveness of models fine-tuned on LLM-generated and human-generated data. Data generated with generative LLMs can be used to augment training data, especially in domains with smaller amounts of labeled data. We build ChatGPT-RetrievalQA based on an existing dataset, human ChatGPT Comparison Corpus (HC3), consisting of public question collections with human responses and answers from ChatGPT. We fine-tune a range of cross-encoder re-rankers on either human-generated or ChatGPT-generated data. Our evaluation on MS MARCO DEV, TREC DL'19, and TREC DL'20 demonstrates that cross-encoder re-ranking models trained on ChatGPT responses are statistically significantly more effective zero-shot re-rankers than those trai",
    "link": "http://arxiv.org/abs/2305.02320",
    "context": "Title: Generating Synthetic Documents for Cross-Encoder Re-Rankers: A Comparative Study of ChatGPT and Human Experts. (arXiv:2305.02320v1 [cs.IR])\nAbstract: We investigate the usefulness of generative Large Language Models (LLMs) in generating training data for cross-encoder re-rankers in a novel direction: generating synthetic documents instead of synthetic queries. We introduce a new dataset, ChatGPT-RetrievalQA, and compare the effectiveness of models fine-tuned on LLM-generated and human-generated data. Data generated with generative LLMs can be used to augment training data, especially in domains with smaller amounts of labeled data. We build ChatGPT-RetrievalQA based on an existing dataset, human ChatGPT Comparison Corpus (HC3), consisting of public question collections with human responses and answers from ChatGPT. We fine-tune a range of cross-encoder re-rankers on either human-generated or ChatGPT-generated data. Our evaluation on MS MARCO DEV, TREC DL'19, and TREC DL'20 demonstrates that cross-encoder re-ranking models trained on ChatGPT responses are statistically significantly more effective zero-shot re-rankers than those trai",
    "path": "papers/23/05/2305.02320.json",
    "total_tokens": 1010,
    "translated_title": "产生用于交叉编码器的合成文档: ChatGPT和人类专家的比较研究。",
    "translated_abstract": "我们探讨了生成大型语言模型（LLMs）在产生训练数据方面的有用性，以供交叉编码器的重新排序进行对比研究，从而产生了合成文档而不是合成查询的新方向。我们引入了一个新的数据集，ChatGPT-RetrievalQA，并比较了在LLM-generated和human-generated数据上进行微调的模型的有效性。使用生成的LLMs数据可以用于增加训练数据，特别是在标记数据较少的领域中。我们基于现有数据集，即由公共问题集和ChatGPT的人类回答和答案组成的人类ChatGPT比较语料库（HC3）构建了ChatGPT-RetrievalQA。我们在MS MARCO DEV，TREC DL'19和TREC DL'20上进行评估，结果表明在ChatGPT响应方面受过训练的交叉编码器重新排序模型是零-shot重新排序器比那些接受了人类生成数据的有效性显著更高的。",
    "tldr": "本研究探讨了使用大型语言模型生成合成文档，作为交叉编码器重新排序的训练数据的有效性。并引入了一个新的数据集ChatGPT-RetrievalQA，最终发现ChatGPT反应训练的交叉编码器重排模型比使用人类生成数据的模型更有效。",
    "en_tdlr": "This study explores the usefulness of generating synthetic documents using large language models as training data for cross-encoder re-rankers. A new dataset, ChatGPT-RetrievalQA, is introduced, and models fine-tuned on LLM-generated data are compared to those trained on human-generated data. Results demonstrate that cross-encoder re-ranking models trained on ChatGPT responses are more effective zero-shot re-rankers than those trained on human-generated data."
}