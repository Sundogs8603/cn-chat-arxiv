{
    "title": "Convergence and scaling of Boolean-weight optimization for hardware reservoirs. (arXiv:2305.07908v1 [stat.ML])",
    "abstract": "Hardware implementation of neural network are an essential step to implement next generation efficient and powerful artificial intelligence solutions.  Besides the realization of a parallel, efficient and scalable hardware architecture, the optimization of the system's extremely large parameter space with sampling-efficient approaches is essential.  Here, we analytically derive the scaling laws for highly efficient Coordinate Descent applied to optimizing the readout layer of a random recurrently connection neural network, a reservoir.  We demonstrate that the convergence is exponential and scales linear with the network's number of neurons.  Our results perfectly reproduce the convergence and scaling of a large-scale photonic reservoir implemented in a proof-of-concept experiment.  Our work therefore provides a solid foundation for such optimization in hardware networks, and identifies future directions that are promising for optimizing convergence speed during learning leveraging mea",
    "link": "http://arxiv.org/abs/2305.07908",
    "context": "Title: Convergence and scaling of Boolean-weight optimization for hardware reservoirs. (arXiv:2305.07908v1 [stat.ML])\nAbstract: Hardware implementation of neural network are an essential step to implement next generation efficient and powerful artificial intelligence solutions.  Besides the realization of a parallel, efficient and scalable hardware architecture, the optimization of the system's extremely large parameter space with sampling-efficient approaches is essential.  Here, we analytically derive the scaling laws for highly efficient Coordinate Descent applied to optimizing the readout layer of a random recurrently connection neural network, a reservoir.  We demonstrate that the convergence is exponential and scales linear with the network's number of neurons.  Our results perfectly reproduce the convergence and scaling of a large-scale photonic reservoir implemented in a proof-of-concept experiment.  Our work therefore provides a solid foundation for such optimization in hardware networks, and identifies future directions that are promising for optimizing convergence speed during learning leveraging mea",
    "path": "papers/23/05/2305.07908.json",
    "total_tokens": 804,
    "translated_title": "布尔权重优化的收敛性和尺度在硬件水库中的应用",
    "translated_abstract": "实现神经网络的硬件化是实现下一代高效和强大人工智能解决方案的重要一步。除了实现并行、高效和可扩展的硬件架构外，用抽样有效的方法优化系统极大的参数空间也至关重要。本研究分析地导出了高效坐标下降法在优化随机复杂神经网络--水库的读出层所需的尺度定律。我们证明了收敛是指数级的，并且随着网络神经元数量的线性缩放。我们的研究完全复现了一个大规模的光子水库实验中的收敛性和尺度。因此，我们的工作为硬件网络优化提供了坚实的基础，并确定了有前途的优化收束速度的未来方向。",
    "tldr": "给出了在随机水库模型上使用坐标下降法进行优化的收敛性分析和尺度定律，为硬件网络优化提供了坚实的基础。"
}