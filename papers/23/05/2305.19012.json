{
    "title": "StyleAvatar3D: Leveraging Image-Text Diffusion Models for High-Fidelity 3D Avatar Generation. (arXiv:2305.19012v2 [cs.CV] UPDATED)",
    "abstract": "The recent advancements in image-text diffusion models have stimulated research interest in large-scale 3D generative models. Nevertheless, the limited availability of diverse 3D resources presents significant challenges to learning. In this paper, we present a novel method for generating high-quality, stylized 3D avatars that utilizes pre-trained image-text diffusion models for data generation and a Generative Adversarial Network (GAN)-based 3D generation network for training. Our method leverages the comprehensive priors of appearance and geometry offered by image-text diffusion models to generate multi-view images of avatars in various styles. During data generation, we employ poses extracted from existing 3D models to guide the generation of multi-view images. To address the misalignment between poses and images in data, we investigate view-specific prompts and develop a coarse-to-fine discriminator for GAN training. We also delve into attribute-related prompts to increase the dive",
    "link": "http://arxiv.org/abs/2305.19012",
    "context": "Title: StyleAvatar3D: Leveraging Image-Text Diffusion Models for High-Fidelity 3D Avatar Generation. (arXiv:2305.19012v2 [cs.CV] UPDATED)\nAbstract: The recent advancements in image-text diffusion models have stimulated research interest in large-scale 3D generative models. Nevertheless, the limited availability of diverse 3D resources presents significant challenges to learning. In this paper, we present a novel method for generating high-quality, stylized 3D avatars that utilizes pre-trained image-text diffusion models for data generation and a Generative Adversarial Network (GAN)-based 3D generation network for training. Our method leverages the comprehensive priors of appearance and geometry offered by image-text diffusion models to generate multi-view images of avatars in various styles. During data generation, we employ poses extracted from existing 3D models to guide the generation of multi-view images. To address the misalignment between poses and images in data, we investigate view-specific prompts and develop a coarse-to-fine discriminator for GAN training. We also delve into attribute-related prompts to increase the dive",
    "path": "papers/23/05/2305.19012.json",
    "total_tokens": 1000,
    "translated_title": "StyleAvatar3D：利用图像-文本扩散模型生成高保真3D头像的新方法",
    "translated_abstract": "最近图像-文本扩散模型的进展刺激了大规模3D生成模型的研究兴趣。然而，有限的多样化3D资源的可用性对学习提出了重大挑战。在本文中，我们提出了一种新方法，利用预训练的图像-文本扩散模型进行数据生成和基于生成对抗网络（GAN）的3D生成网络进行训练，以生成高质量、风格化的3D头像。我们的方法利用图像-文本扩散模型提供的完整外观和几何先验生成各种风格的多视角头像图像。在数据生成过程中，我们利用现有3D模型中提取的姿势来引导多视角图像的生成。为了解决数据中姿势和图像的不对齐，我们研究了视点特定提示并开发了一个粗到细的GAN鉴别器进行训练。我们还深入研究了属性相关提示以增加多样性。",
    "tldr": "本文提出了一种新方法，利用图像-文本扩散模型进行数据生成和基于生成对抗网络（GAN）的3D生成网络进行训练，以生成高质量、风格化的3D头像，同时在生成过程中增加了现有3D模型中提取的姿势来引导多视角的图像生成，并提出了视点特定提示、粗到细的GAN鉴别器以及属性相关提示等方法以增加多样性。",
    "en_tdlr": "This paper proposes a method that generates high-quality, stylized 3D avatars using pre-trained image-text diffusion models and a GAN-based 3D generation network, and increases diversity by incorporating poses extracted from existing 3D models and attribute-related prompts, as well as using view-specific prompts and a coarse-to-fine discriminator for GAN training."
}