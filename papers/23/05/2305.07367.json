{
    "title": "S-REINFORCE: A Neuro-Symbolic Policy Gradient Approach for Interpretable Reinforcement Learning. (arXiv:2305.07367v1 [cs.LG])",
    "abstract": "This paper presents a novel RL algorithm, S-REINFORCE, which is designed to generate interpretable policies for dynamic decision-making tasks. The proposed algorithm leverages two types of function approximators, namely Neural Network (NN) and Symbolic Regressor (SR), to produce numerical and symbolic policies, respectively. The NN component learns to generate a numerical probability distribution over the possible actions using a policy gradient, while the SR component captures the functional form that relates the associated states with the action probabilities. The SR-generated policy expressions are then utilized through importance sampling to improve the rewards received during the learning process. We have tested the proposed S-REINFORCE algorithm on various dynamic decision-making problems with low and high dimensional action spaces, and the results demonstrate its effectiveness and impact in achieving interpretable solutions. By leveraging the strengths of both NN and SR, S-REINF",
    "link": "http://arxiv.org/abs/2305.07367",
    "context": "Title: S-REINFORCE: A Neuro-Symbolic Policy Gradient Approach for Interpretable Reinforcement Learning. (arXiv:2305.07367v1 [cs.LG])\nAbstract: This paper presents a novel RL algorithm, S-REINFORCE, which is designed to generate interpretable policies for dynamic decision-making tasks. The proposed algorithm leverages two types of function approximators, namely Neural Network (NN) and Symbolic Regressor (SR), to produce numerical and symbolic policies, respectively. The NN component learns to generate a numerical probability distribution over the possible actions using a policy gradient, while the SR component captures the functional form that relates the associated states with the action probabilities. The SR-generated policy expressions are then utilized through importance sampling to improve the rewards received during the learning process. We have tested the proposed S-REINFORCE algorithm on various dynamic decision-making problems with low and high dimensional action spaces, and the results demonstrate its effectiveness and impact in achieving interpretable solutions. By leveraging the strengths of both NN and SR, S-REINF",
    "path": "papers/23/05/2305.07367.json",
    "total_tokens": 885,
    "translated_title": "S-REINFORCE：一种神经符号策略梯度方法以实现可解释强化学习",
    "translated_abstract": "本文提出了一种新颖的强化学习算法S-REINFORCE，旨在为动态决策任务生成可解释策略。该算法利用两种类型的函数逼近器，即神经网络（NN）和符号回归器（SR），分别生成数字和符号策略。NN组件通过策略梯度学习生成可能操作的数字概率分布，而SR组件则捕获与操作概率相关的状态间关系的函数形式。然后通过重要性抽样利用SR生成的策略表达式改进学习过程中接收的奖励。我们在具有低和高维行动空间的各种动态决策问题上测试了提出的S-REINFORCE算法，结果展示了其实现可解释解决方案的有效性和影响力。通过利用NN和SR的优势，S-REINFORCE提供了一种新方法来生成强化学习任务的可解释策略。",
    "tldr": "S-REINFORCE是一种神经符号策略梯度方法，利用神经网络和符号回归器生成数字和符号策略，从而为强化学习任务提供可解释解决方案。",
    "en_tdlr": "S-REINFORCE is a neuro-symbolic policy gradient approach which generates numerical and symbolic policies using neural network and symbolic regressor respectively, providing interpretable solutions for reinforcement learning tasks."
}