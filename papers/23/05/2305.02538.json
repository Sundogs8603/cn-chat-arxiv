{
    "title": "Cuttlefish: Low-rank Model Training without All The Tuning. (arXiv:2305.02538v1 [cs.LG])",
    "abstract": "Recent research has shown that training low-rank neural networks can effectively reduce the total number of trainable parameters without sacrificing predictive accuracy, resulting in end-to-end speedups. However, low-rank model training necessitates adjusting several additional factorization hyperparameters, such as the rank of the factorization at each layer. In this paper, we tackle this challenge by introducing Cuttlefish, an automated low-rank training approach that eliminates the need for tuning factorization hyperparameters. Cuttlefish leverages the observation that after a few epochs of full-rank training, the stable rank (i.e., an approximation of the true rank) of each layer stabilizes at a constant value. Cuttlefish switches from full-rank to low-rank training once the stable ranks of all layers have converged, setting the dimension of each factorization to its corresponding stable rank. Our results show that Cuttlefish generates models up to 5.6 times smaller than full-rank ",
    "link": "http://arxiv.org/abs/2305.02538",
    "context": "Title: Cuttlefish: Low-rank Model Training without All The Tuning. (arXiv:2305.02538v1 [cs.LG])\nAbstract: Recent research has shown that training low-rank neural networks can effectively reduce the total number of trainable parameters without sacrificing predictive accuracy, resulting in end-to-end speedups. However, low-rank model training necessitates adjusting several additional factorization hyperparameters, such as the rank of the factorization at each layer. In this paper, we tackle this challenge by introducing Cuttlefish, an automated low-rank training approach that eliminates the need for tuning factorization hyperparameters. Cuttlefish leverages the observation that after a few epochs of full-rank training, the stable rank (i.e., an approximation of the true rank) of each layer stabilizes at a constant value. Cuttlefish switches from full-rank to low-rank training once the stable ranks of all layers have converged, setting the dimension of each factorization to its corresponding stable rank. Our results show that Cuttlefish generates models up to 5.6 times smaller than full-rank ",
    "path": "papers/23/05/2305.02538.json",
    "total_tokens": 1194,
    "translated_title": "Cuttlefish: 无需调整超参数的低秩模型训练",
    "translated_abstract": "最近的研究表明，训练低秩神经网络可以有效减少可训练参数的总数，而不会影响预测准确率，从而实现端到端的加速。但是，低秩模型的训练需要调整多个因式分解超参数。在本文中，我们通过引入 Cuttlefish，一种自动化的低秩训练方法，消除了调整因式分解超参数的需要。Cuttlefish 利用了一种观察结果，即在几个 epoch 的完全秩训练后，每个层的稳定秩稳定在一个常数值。一旦所有层的稳定秩都收敛，Cuttlefish 就从完全秩训练切换到低秩训练，将每个因式分解的维数设置为其相应的稳定秩。我们的实验结果表明，使用 Cuttlefish 生成的模型比完全秩训练小多达 5.6 倍。",
    "tldr": "Cuttlefish 是一种新的自动化低秩训练方法，可以有效地减少可训练参数的数量，而无需调整因式分解超参数即可实现加速，可生成比完全秩训练小多达 5.6 倍的模型。"
}