{
    "title": "UNIMO-3: Multi-granularity Interaction for Vision-Language Representation Learning. (arXiv:2305.13697v1 [cs.CL])",
    "abstract": "Vision-and-language (VL) pre-training, which aims to learn a general representation of image-text pairs that can be transferred to various vision-and-language tasks. Compared with modeling uni-modal data, the main challenge of the VL model is: how to learn the cross-modal interaction from multimodal data, especially the fine-grained interaction. Existing works have shown that fully transformer-based models that adopt attention mechanisms to learn in-layer cross-model interaction can demonstrate impressive performance on various cross-modal downstream tasks. However, they ignored that the semantic information of the different modals at the same layer was not uniform, which leads to the cross-modal interaction collapsing into a limited multi-modal semantic information interaction. In this work, we propose the UNIMO-3 model, which has the capacity to simultaneously learn the multimodal in-layer interaction and cross-layer interaction. UNIMO-3 model can establish effective connections betw",
    "link": "http://arxiv.org/abs/2305.13697",
    "context": "Title: UNIMO-3: Multi-granularity Interaction for Vision-Language Representation Learning. (arXiv:2305.13697v1 [cs.CL])\nAbstract: Vision-and-language (VL) pre-training, which aims to learn a general representation of image-text pairs that can be transferred to various vision-and-language tasks. Compared with modeling uni-modal data, the main challenge of the VL model is: how to learn the cross-modal interaction from multimodal data, especially the fine-grained interaction. Existing works have shown that fully transformer-based models that adopt attention mechanisms to learn in-layer cross-model interaction can demonstrate impressive performance on various cross-modal downstream tasks. However, they ignored that the semantic information of the different modals at the same layer was not uniform, which leads to the cross-modal interaction collapsing into a limited multi-modal semantic information interaction. In this work, we propose the UNIMO-3 model, which has the capacity to simultaneously learn the multimodal in-layer interaction and cross-layer interaction. UNIMO-3 model can establish effective connections betw",
    "path": "papers/23/05/2305.13697.json",
    "total_tokens": 721,
    "translated_title": "UNIMO-3: 多层次交互的视觉语言表示学习模型",
    "translated_abstract": "视觉语言预训练旨在学习通用的图文配对表示，以便在各种视觉语言任务中进行转移。与建模单模态数据相比，VL 模型面临的主要挑战是如何从多模态数据中学习跨模态交互，特别是细粒度交互。本文提出了 UNIMO-3 模型，具有同时学习多模态内层交互和跨层交互的能力。 UNIMO-3 模型能够建立有效的连接，从而更好地学习多模态语义信息的交互。",
    "tldr": "本研究提出了 UNIMO-3 模型，具有多层次交互能力，能够更好地学习多模态语义信息的交互。",
    "en_tdlr": "This study proposes the UNIMO-3 model with multi-granularity interaction, which can better learn the interaction of multimodal semantic information."
}