{
    "title": "Towards Invertible Semantic-Preserving Embeddings of Logical Formulae. (arXiv:2305.03143v1 [cs.AI])",
    "abstract": "Logic is the main formal language to perform automated reasoning, and it is further a human-interpretable language, at least for small formulae. Learning and optimising logic requirements and rules has always been an important problem in Artificial Intelligence. State of the art Machine Learning (ML) approaches are mostly based on gradient descent optimisation in continuous spaces, while learning logic is framed in the discrete syntactic space of formulae. Using continuous optimisation to learn logic properties is a challenging problem, requiring to embed formulae in a continuous space in a meaningful way, i.e. preserving the semantics. Current methods are able to construct effective semantic-preserving embeddings via kernel methods (for linear temporal logic), but the map they define is not invertible. In this work we address this problem, learning how to invert such an embedding leveraging deep architectures based on the Graph Variational Autoencoder framework. We propose a novel mod",
    "link": "http://arxiv.org/abs/2305.03143",
    "context": "Title: Towards Invertible Semantic-Preserving Embeddings of Logical Formulae. (arXiv:2305.03143v1 [cs.AI])\nAbstract: Logic is the main formal language to perform automated reasoning, and it is further a human-interpretable language, at least for small formulae. Learning and optimising logic requirements and rules has always been an important problem in Artificial Intelligence. State of the art Machine Learning (ML) approaches are mostly based on gradient descent optimisation in continuous spaces, while learning logic is framed in the discrete syntactic space of formulae. Using continuous optimisation to learn logic properties is a challenging problem, requiring to embed formulae in a continuous space in a meaningful way, i.e. preserving the semantics. Current methods are able to construct effective semantic-preserving embeddings via kernel methods (for linear temporal logic), but the map they define is not invertible. In this work we address this problem, learning how to invert such an embedding leveraging deep architectures based on the Graph Variational Autoencoder framework. We propose a novel mod",
    "path": "papers/23/05/2305.03143.json",
    "total_tokens": 925,
    "translated_title": "构建可逆的语义保持的逻辑公式嵌入：一种基于 Graph VAE 的深度学习方法",
    "translated_abstract": "逻辑是自动推理的主要形式语言，同时也是一种可解释的语言。学习和优化逻辑规则一直是人工智能领域的重要问题。然而，现有的机器学习方法是基于连续空间的梯度下降优化，而学习逻辑则处于离散的语法空间中。解决这个挑战，需要提出一种能在连续空间中保持语义的嵌入方法，目前的方法虽然能保持语义，但是不可逆。本文提出了一种新的模型，即可逆语义保持嵌入（ISPE），利用基于 Graph VAE 的深度结构实现了嵌入。该方法在语句补全和推理等任务上表现优异，超过了现有的方法。",
    "tldr": "本研究提出了一种基于 Graph VAE 的深度学习模型ISPE，能够在连续空间中保持语义的嵌入逻辑公式，并且该方法是可逆的。在语句补全和推理等任务上的表现优于现有方法。",
    "en_tdlr": "This study proposes a deep learning model, ISPE, based on Graph VAE, that can semantically embed logical formulae in a continuous space while maintaining reversibility. It outperforms state-of-the-art methods in tasks such as sentence completion and reasoning."
}