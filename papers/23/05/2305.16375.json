{
    "title": "Data Topology-Dependent Upper Bounds of Neural Network Widths. (arXiv:2305.16375v1 [cs.LG])",
    "abstract": "This paper investigates the relationship between the universal approximation property of deep neural networks and topological characteristics of datasets. Our primary contribution is to introduce data topology-dependent upper bounds on the network width. Specifically, we first show that a three-layer neural network, applying a ReLU activation function and max pooling, can be designed to approximate an indicator function over a compact set, one that is encompassed by a tight convex polytope. This is then extended to a simplicial complex, deriving width upper bounds based on its topological structure. Further, we calculate upper bounds in relation to the Betti numbers of select topological spaces. Finally, we prove the universal approximation property of three-layer ReLU networks using our topological approach. We also verify that gradient descent converges to the network structure proposed in our study.",
    "link": "http://arxiv.org/abs/2305.16375",
    "context": "Title: Data Topology-Dependent Upper Bounds of Neural Network Widths. (arXiv:2305.16375v1 [cs.LG])\nAbstract: This paper investigates the relationship between the universal approximation property of deep neural networks and topological characteristics of datasets. Our primary contribution is to introduce data topology-dependent upper bounds on the network width. Specifically, we first show that a three-layer neural network, applying a ReLU activation function and max pooling, can be designed to approximate an indicator function over a compact set, one that is encompassed by a tight convex polytope. This is then extended to a simplicial complex, deriving width upper bounds based on its topological structure. Further, we calculate upper bounds in relation to the Betti numbers of select topological spaces. Finally, we prove the universal approximation property of three-layer ReLU networks using our topological approach. We also verify that gradient descent converges to the network structure proposed in our study.",
    "path": "papers/23/05/2305.16375.json",
    "total_tokens": 776,
    "translated_title": "神经网络宽度与数据拓扑特征相关的上界探究",
    "translated_abstract": "本文研究了深度神经网络普适逼近性质与数据拓扑特征之间的关系。我们的主要贡献是引入了数据拓扑相关的网络宽度上界。具体而言，我们首先证明了一个三层神经网络，应用ReLU激活函数和最大池化，可以设计来逼近一个在紧凑凸多面体内封装的指示函数。然后我们将其扩展到一个单纯复合体，基于其拓扑结构推导宽度上界。此外，我们还通过选择拓扑空间的Betti数计算上界。最后，我们通过拓扑方法证明了三层ReLU网络的普适逼近性质。我们还验证了梯度下降算法收敛于本研究提出的网络结构。",
    "tldr": "本文引入了数据拓扑相关的神经网络宽度上界，并通过拓扑方法证明了三层ReLU网络的普适逼近性质。",
    "en_tdlr": "This paper introduces data topology-dependent upper bounds on neural network width and proves the universal approximation property of three-layer ReLU networks using topological approach."
}