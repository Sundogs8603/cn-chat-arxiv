{
    "title": "Continual Task Allocation in Meta-Policy Network via Sparse Prompting. (arXiv:2305.18444v1 [cs.LG])",
    "abstract": "How to train a generalizable meta-policy by continually learning a sequence of tasks? It is a natural human skill yet challenging to achieve by current reinforcement learning: the agent is expected to quickly adapt to new tasks (plasticity) meanwhile retaining the common knowledge from previous tasks (stability). We address it by \"Continual Task Allocation via Sparse Prompting (CoTASP)\", which learns over-complete dictionaries to produce sparse masks as prompts extracting a sub-network for each task from a meta-policy network. By optimizing the sub-network and prompts alternatively, CoTASP updates the meta-policy via training a task-specific policy. The dictionary is then updated to align the optimized prompts with tasks' embedding, thereby capturing their semantic correlations. Hence, relevant tasks share more neurons in the meta-policy network via similar prompts while cross-task interference causing forgetting is effectively restrained. Given a trained meta-policy with updated dicti",
    "link": "http://arxiv.org/abs/2305.18444",
    "context": "Title: Continual Task Allocation in Meta-Policy Network via Sparse Prompting. (arXiv:2305.18444v1 [cs.LG])\nAbstract: How to train a generalizable meta-policy by continually learning a sequence of tasks? It is a natural human skill yet challenging to achieve by current reinforcement learning: the agent is expected to quickly adapt to new tasks (plasticity) meanwhile retaining the common knowledge from previous tasks (stability). We address it by \"Continual Task Allocation via Sparse Prompting (CoTASP)\", which learns over-complete dictionaries to produce sparse masks as prompts extracting a sub-network for each task from a meta-policy network. By optimizing the sub-network and prompts alternatively, CoTASP updates the meta-policy via training a task-specific policy. The dictionary is then updated to align the optimized prompts with tasks' embedding, thereby capturing their semantic correlations. Hence, relevant tasks share more neurons in the meta-policy network via similar prompts while cross-task interference causing forgetting is effectively restrained. Given a trained meta-policy with updated dicti",
    "path": "papers/23/05/2305.18444.json",
    "total_tokens": 1023,
    "translated_title": "基于稀疏提示的元策略网络中的持续任务分配",
    "translated_abstract": "如何通过不断学习一系列任务来训练一个具有一般化能力的元策略，是当前强化学习面临的挑战。本文提出了一种名为“连续任务分配的稀疏提示（CoTASP）”的解决方案，通过学习过完备字典来生成稀疏掩码作为提示，从元策略网络中提取与每个任务相关的子网络。通过交替优化子网络和提示，CoTASP更新了元策略，通过训练特定于任务的策略来实现。然后更新字典，以使优化后的提示与任务嵌入相匹配，从而捕捉其语义相关性。因此，相关任务通过相似的提示在元策略网络中共享更多的神经元，而跨任务干扰导致遗忘被有效地约束。给定经过训练的元策略和更新后的字典，我们可以通过推导相应的提示来迅速适应新任务，从而从元策略中提取相关的子网络。我们在一组导航任务上评估了CoTASP，并展示了它在任务完成度、样本效率和泛化能力方面优于现有的基线方法。",
    "tldr": "本文提出的CoTASP可以通过学习过完备字典来生成稀疏掩码作为提示，从而从元策略网络中提取与每个任务相关的子网络，实现了快速适应新任务，同时保留了之前任务的共同知识。",
    "en_tdlr": "CoTASP proposed in this paper can extract task-specific sub-network from a meta-policy network by learning over-complete dictionaries to generate sparse masks as prompts, achieving fast adaptation to new tasks while retaining common knowledge from previous tasks."
}