{
    "title": "Vocabulary for Universal Approximation: A Linguistic Perspective of Mapping Compositions. (arXiv:2305.12205v1 [cs.LG])",
    "abstract": "In recent years, deep learning-based sequence modelings, such as language models, have received much attention and success, which pushes researchers to explore the possibility of transforming non-sequential problems into a sequential form. Following this thought, deep neural networks can be represented as composite functions of a sequence of mappings, linear or nonlinear, where each composition can be viewed as a \\emph{word}. However, the weights of linear mappings are undetermined and hence require an infinite number of words. In this article, we investigate the finite case and constructively prove the existence of a finite \\emph{vocabulary} $V=\\{\\phi_i: \\mathbb{R}^d \\to \\mathbb{R}^d | i=1,...,n\\}$ with $n=O(d^2)$ for the universal approximation. That is, for any continuous mapping $f: \\mathbb{R}^d \\to \\mathbb{R}^d$, compact domain $\\Omega$ and $\\varepsilon>0$, there is a sequence of mappings $\\phi_{i_1}, ..., \\phi_{i_m} \\in V, m \\in \\mathbb{Z}_+$, such that the composition $\\phi_{i_m",
    "link": "http://arxiv.org/abs/2305.12205",
    "context": "Title: Vocabulary for Universal Approximation: A Linguistic Perspective of Mapping Compositions. (arXiv:2305.12205v1 [cs.LG])\nAbstract: In recent years, deep learning-based sequence modelings, such as language models, have received much attention and success, which pushes researchers to explore the possibility of transforming non-sequential problems into a sequential form. Following this thought, deep neural networks can be represented as composite functions of a sequence of mappings, linear or nonlinear, where each composition can be viewed as a \\emph{word}. However, the weights of linear mappings are undetermined and hence require an infinite number of words. In this article, we investigate the finite case and constructively prove the existence of a finite \\emph{vocabulary} $V=\\{\\phi_i: \\mathbb{R}^d \\to \\mathbb{R}^d | i=1,...,n\\}$ with $n=O(d^2)$ for the universal approximation. That is, for any continuous mapping $f: \\mathbb{R}^d \\to \\mathbb{R}^d$, compact domain $\\Omega$ and $\\varepsilon>0$, there is a sequence of mappings $\\phi_{i_1}, ..., \\phi_{i_m} \\in V, m \\in \\mathbb{Z}_+$, such that the composition $\\phi_{i_m",
    "path": "papers/23/05/2305.12205.json",
    "total_tokens": 1029,
    "translated_title": "通用逼近的词汇：一种将映射组合看作语言的视角",
    "translated_abstract": "近年来，基于深度学习的序列建模，如语言模型，受到了广泛关注和成功的应用，这促使研究人员探索将非连续问题转化为连续形式的可能性。本文沿着这个思路，将深度神经网络表示为一系列映射函数的组合，其中每个组合可视为一个“单词”。然而，线性映射的权重是未确定的，因此需要无限数量的单词。本文研究有限情况，构建性地证明了通用逼近的有限“词汇”$V=\\{\\phi_i: \\mathbb{R}^d \\to \\mathbb{R}^d | i=1,...,n\\}$存在，其中$n = O(d^2)$。也就是说，对于任何连续映射$f: \\mathbb{R}^d \\to \\mathbb{R}^d$、紧致域$\\Omega$和$\\varepsilon>0$，存在映射序列$\\phi_{i_1}, ..., \\phi_{i_m} \\in V, m \\in \\mathbb{Z}_+$，使得组合$\\phi_{i_m}$能够逼近$f$和$\\Omega$中的每个点，且误差小于$\\varepsilon$。",
    "tldr": "本文探讨了通用逼近的词汇，证明了有限“词汇”存在并可用于逼近任何连续映射$f$和紧致域$\\Omega$中的每个点，误差小于$\\varepsilon$。",
    "en_tdlr": "This article investigates the vocabulary for universal approximation and constructively proves the existence of a finite vocabulary for approximating any continuous mapping $f$ and each point in a compact domain with an error less than $\\varepsilon$."
}