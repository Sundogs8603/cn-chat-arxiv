{
    "title": "What In-Context Learning \"Learns\" In-Context: Disentangling Task Recognition and Task Learning. (arXiv:2305.09731v1 [cs.CL])",
    "abstract": "Large language models (LLMs) exploit in-context learning (ICL) to solve tasks with only a few demonstrations, but its mechanisms are not yet well-understood. Some works suggest that LLMs only recall already learned concepts from pre-training, while others hint that ICL performs implicit learning over demonstrations. We characterize two ways through which ICL leverages demonstrations. Task recognition (TR) captures the extent to which LLMs can recognize a task through demonstrations -- even without ground-truth labels -and apply their pre-trained priors, whereas task learning (TL) is the ability to capture new input-label mappings unseen in pre-training. Using a wide range of classification datasets and three LLM families (GPT-3, LLaMA and OPT), we design controlled experiments to disentangle the roles of TR and TL in ICL. We show that (1) models can achieve non-trivial performance with only TR, and TR does not further improve with larger models or more demonstrations; (2) LLMs acquir",
    "link": "http://arxiv.org/abs/2305.09731",
    "context": "Title: What In-Context Learning \"Learns\" In-Context: Disentangling Task Recognition and Task Learning. (arXiv:2305.09731v1 [cs.CL])\nAbstract: Large language models (LLMs) exploit in-context learning (ICL) to solve tasks with only a few demonstrations, but its mechanisms are not yet well-understood. Some works suggest that LLMs only recall already learned concepts from pre-training, while others hint that ICL performs implicit learning over demonstrations. We characterize two ways through which ICL leverages demonstrations. Task recognition (TR) captures the extent to which LLMs can recognize a task through demonstrations -- even without ground-truth labels -and apply their pre-trained priors, whereas task learning (TL) is the ability to capture new input-label mappings unseen in pre-training. Using a wide range of classification datasets and three LLM families (GPT-3, LLaMA and OPT), we design controlled experiments to disentangle the roles of TR and TL in ICL. We show that (1) models can achieve non-trivial performance with only TR, and TR does not further improve with larger models or more demonstrations; (2) LLMs acquir",
    "path": "papers/23/05/2305.09731.json",
    "total_tokens": 937,
    "translated_title": "在语境中学习：“学习”语境中的任务识别和任务学习的区分。",
    "translated_abstract": "大型语言模型通过利用语境中的学习来解决只有少数演示的任务，但其机制尚未得到很好的理解。一些研究表明LLMs仅回忆来自预训练的已学概念，而其他研究则暗示ICL执行演示的隐含学习。本文通过任务识别(TR)和任务学习(TL)两种方式表征了ICL利用演示的方式。我们使用各种分类数据集和三个LLM系列（GPT-3、LLaMA和OPT）进行控制实验，在ICL中区分TR和TL的角色。我们发现：（1）模型只使用TR就能取得非平凡的性能，TR不会随着更大的模型或更多的演示而进一步改善；（2）LLMs能够通过TL学习新的输入-标签映射，而TR则主要利用预先训练的先验知识。",
    "tldr": "本研究通过任务识别和任务学习两种方式表征了ICL利用演示的方式，发现LLMs利用不同机制进行任务的解决，TR主要利用先验知识，而TL则具备学习新的输入-标签映射的能力。",
    "en_tdlr": "This paper characterizes how in-context learning (ICL) leverages demonstrations through task recognition (TR) and task learning (TL), showing that Large language models (LLMs) use different mechanisms to solve tasks, with TR mainly using prior knowledge and TL having the ability to learn new input-label mappings."
}