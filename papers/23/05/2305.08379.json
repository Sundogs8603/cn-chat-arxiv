{
    "title": "TESS: Text-to-Text Self-Conditioned Simplex Diffusion",
    "abstract": "arXiv:2305.08379v2 Announce Type: replace  Abstract: Diffusion models have emerged as a powerful paradigm for generation, obtaining strong performance in various continuous domains. However, applying continuous diffusion models to natural language remains challenging due to its discrete nature and the need for a large number of diffusion steps to generate text, making diffusion-based generation expensive. In this work, we propose Text-to-text Self-conditioned Simplex Diffusion (TESS), a text diffusion model that is fully non-autoregressive, employs a new form of self-conditioning, and applies the diffusion process on the logit simplex space rather than the learned embedding space. Through extensive experiments on natural language understanding and generation tasks including summarization, text simplification, paraphrase generation, and question generation, we demonstrate that TESS outperforms state-of-the-art non-autoregressive models, requires fewer diffusion steps with minimal drop i",
    "link": "https://arxiv.org/abs/2305.08379",
    "context": "Title: TESS: Text-to-Text Self-Conditioned Simplex Diffusion\nAbstract: arXiv:2305.08379v2 Announce Type: replace  Abstract: Diffusion models have emerged as a powerful paradigm for generation, obtaining strong performance in various continuous domains. However, applying continuous diffusion models to natural language remains challenging due to its discrete nature and the need for a large number of diffusion steps to generate text, making diffusion-based generation expensive. In this work, we propose Text-to-text Self-conditioned Simplex Diffusion (TESS), a text diffusion model that is fully non-autoregressive, employs a new form of self-conditioning, and applies the diffusion process on the logit simplex space rather than the learned embedding space. Through extensive experiments on natural language understanding and generation tasks including summarization, text simplification, paraphrase generation, and question generation, we demonstrate that TESS outperforms state-of-the-art non-autoregressive models, requires fewer diffusion steps with minimal drop i",
    "path": "papers/23/05/2305.08379.json",
    "total_tokens": 906,
    "translated_title": "TESS：文本到文本自条件单纯形扩散",
    "translated_abstract": "扩散模型已经成为一种在各种连续领域中表现出色的生成方法范式。然而，将连续扩散模型应用于自然语言仍然具有挑战性，因为自然语言是离散的，并且需要大量的扩散步骤来生成文本，这使得基于扩散的生成变得昂贵。在这项工作中，我们提出了文本到文本自条件单纯形扩散（TESS），这是一个全非自回归的文本扩散模型，采用一种新形式的自条件，将扩散过程应用于逻辑空间而不是学习嵌入空间。通过对包括总结、文本简化、释义生成和问题生成在内的自然语言理解和生成任务的广泛实验，我们证明了TESS优于最先进的非自回归模型，在需要更少的扩散步骤的情况下表现出最小的性能下降。",
    "tldr": "TESS是一个全非自回归的文本扩散模型，通过在逻辑空间而不是学习嵌入空间应用扩散过程，进行了自条件单纯形扩散，实验证明在自然语言理解和生成任务中表现优于最先进的非自回归模型，并且所需的扩散步骤更少。",
    "en_tdlr": "TESS is a fully non-autoregressive text diffusion model that applies diffusion process on the logit simplex space instead of the learned embedding space, demonstrating superior performance over state-of-the-art non-autoregressive models in natural language understanding and generation tasks with fewer diffusion steps required."
}