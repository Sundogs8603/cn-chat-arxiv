{
    "title": "Content-Adaptive Downsampling in Convolutional Neural Networks. (arXiv:2305.09504v1 [cs.CV])",
    "abstract": "Many convolutional neural networks (CNNs) rely on progressive downsampling of their feature maps to increase the network's receptive field and decrease computational cost. However, this comes at the price of losing granularity in the feature maps, limiting the ability to correctly understand images or recover fine detail in dense prediction tasks. To address this, common practice is to replace the last few downsampling operations in a CNN with dilated convolutions, allowing to retain the feature map resolution without reducing the receptive field, albeit increasing the computational cost. This allows to trade off predictive performance against cost, depending on the output feature resolution. By either regularly downsampling or not downsampling the entire feature map, existing work implicitly treats all regions of the input image and subsequent feature maps as equally important, which generally does not hold. We propose an adaptive downsampling scheme that generalizes the above idea by",
    "link": "http://arxiv.org/abs/2305.09504",
    "context": "Title: Content-Adaptive Downsampling in Convolutional Neural Networks. (arXiv:2305.09504v1 [cs.CV])\nAbstract: Many convolutional neural networks (CNNs) rely on progressive downsampling of their feature maps to increase the network's receptive field and decrease computational cost. However, this comes at the price of losing granularity in the feature maps, limiting the ability to correctly understand images or recover fine detail in dense prediction tasks. To address this, common practice is to replace the last few downsampling operations in a CNN with dilated convolutions, allowing to retain the feature map resolution without reducing the receptive field, albeit increasing the computational cost. This allows to trade off predictive performance against cost, depending on the output feature resolution. By either regularly downsampling or not downsampling the entire feature map, existing work implicitly treats all regions of the input image and subsequent feature maps as equally important, which generally does not hold. We propose an adaptive downsampling scheme that generalizes the above idea by",
    "path": "papers/23/05/2305.09504.json",
    "total_tokens": 1110,
    "translated_title": "卷积神经网络中的内容自适应下采样",
    "translated_abstract": "许多卷积神经网络（CNN）依赖于逐步降采样其特征映射来增加网络的感受野并降低计算成本。然而，这是以在特征映射中失去粒度的代价为代价的，这限制了正确理解图像或在密集预测任务中恢复细节。为了解决这个问题，通常的做法是用膨胀卷积替换CNN中的最后几个下采样操作，从而在不减少接受域的情况下保留特征地图的分辨率，尽管增加了计算成本。这允许根据输出特征分辨率在预测性能和成本之间进行权衡。现有的工作通过定期下采样或不下采样整个特征映射，隐含地将输入图像和随后的特征映射的所有区域视为同等重要，这在一般情况下并不成立。我们提出了一种自适应下采样方案，通过允许基于其感知重要性，以不同的分辨率处理输入图像的不同区域来概括上述思想。为此，我们引入了一个可学习的门控机制，自适应地确定每个区域是否下采样以及下采样的程度。我们证明了我们的方法在密集预测任务中可以提高准确性，且计算成本和网络大小开销最小。",
    "tldr": "本研究提出了一种内容自适应下采样的方法，通过允许基于感知重要性以不同的分辨率处理不同输入图像的区域，提高了神经网络在密集预测任务中的准确性，且计算成本和网络大小开销最小。",
    "en_tdlr": "This paper proposes a content-adaptive downsampling method that improves the accuracy of neural networks in dense prediction tasks by allowing different regions of the input image to be processed at different resolutions based on their perceived importance, while minimizing computational cost and network size overheads."
}