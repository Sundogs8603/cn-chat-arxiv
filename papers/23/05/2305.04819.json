{
    "title": "Local Optimization Achieves Global Optimality in Multi-Agent Reinforcement Learning. (arXiv:2305.04819v1 [cs.LG])",
    "abstract": "Policy optimization methods with function approximation are widely used in multi-agent reinforcement learning. However, it remains elusive how to design such algorithms with statistical guarantees. Leveraging a multi-agent performance difference lemma that characterizes the landscape of multi-agent policy optimization, we find that the localized action value function serves as an ideal descent direction for each local policy. Motivated by the observation, we present a multi-agent PPO algorithm in which the local policy of each agent is updated similarly to vanilla PPO. We prove that with standard regularity conditions on the Markov game and problem-dependent quantities, our algorithm converges to the globally optimal policy at a sublinear rate. We extend our algorithm to the off-policy setting and introduce pessimism to policy evaluation, which aligns with experiments. To our knowledge, this is the first provably convergent multi-agent PPO algorithm in cooperative Markov games.",
    "link": "http://arxiv.org/abs/2305.04819",
    "context": "Title: Local Optimization Achieves Global Optimality in Multi-Agent Reinforcement Learning. (arXiv:2305.04819v1 [cs.LG])\nAbstract: Policy optimization methods with function approximation are widely used in multi-agent reinforcement learning. However, it remains elusive how to design such algorithms with statistical guarantees. Leveraging a multi-agent performance difference lemma that characterizes the landscape of multi-agent policy optimization, we find that the localized action value function serves as an ideal descent direction for each local policy. Motivated by the observation, we present a multi-agent PPO algorithm in which the local policy of each agent is updated similarly to vanilla PPO. We prove that with standard regularity conditions on the Markov game and problem-dependent quantities, our algorithm converges to the globally optimal policy at a sublinear rate. We extend our algorithm to the off-policy setting and introduce pessimism to policy evaluation, which aligns with experiments. To our knowledge, this is the first provably convergent multi-agent PPO algorithm in cooperative Markov games.",
    "path": "papers/23/05/2305.04819.json",
    "total_tokens": 892,
    "translated_title": "多智能体强化学习中的局部优化达到全局最优",
    "translated_abstract": "带函数逼近的策略优化方法在多智能体强化学习中被广泛使用，但如何设计具有统计保证的算法仍然难以捉摸。利用多智能体表现差异引理，该引理表征了多智能体策略优化的潜在空间，我们发现本地化的动作价值函数对于每个局部策略都可以作为理想的下降方向。根据这一观察结果，我们提出了一个多智能体PPO算法，其中每个智能体的本地策略更新类似于vanilla PPO。我们证明，对于马尔可夫博弈和问题相关量的标准正则性条件，我们的算法以亚线性速度收敛于全局最优策略。我们将算法扩展到离线策略设置，并引入悲观主义来评估策略，这与实验结果相符。据我们所知，这是首个能在协作马尔可夫博弈中证明收敛的多智能体PPO算法。",
    "tldr": "本文提出了一种多智能体PPO算法，利用局部优化达到全局最优，具有统计保证，这是首个能在协作马尔可夫博弈中证明收敛的算法。",
    "en_tdlr": "This paper proposes a multi-agent PPO algorithm that achieves global optimality through local optimization, with statistical guarantees, and is the first proven convergent algorithm in cooperative Markov games."
}