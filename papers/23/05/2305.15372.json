{
    "title": "Learning high-level visual representations from a child's perspective without strong inductive biases. (arXiv:2305.15372v2 [cs.CV] UPDATED)",
    "abstract": "Young children develop sophisticated internal models of the world based on their visual experience. Can such models be learned from a child's visual experience without strong inductive biases? To investigate this, we train state-of-the-art neural networks on a realistic proxy of a child's visual experience without any explicit supervision or domain-specific inductive biases. Specifically, we train both embedding models and generative models on 200 hours of headcam video from a single child collected over two years and comprehensively evaluate their performance in downstream tasks using various reference models as yardsticks. On average, the best embedding models perform at a respectable 70% of a high-performance ImageNet-trained model, despite substantial differences in training data. They also learn broad semantic categories and object localization capabilities without explicit supervision, but they are less object-centric than models trained on all of ImageNet. Generative models trai",
    "link": "http://arxiv.org/abs/2305.15372",
    "context": "Title: Learning high-level visual representations from a child's perspective without strong inductive biases. (arXiv:2305.15372v2 [cs.CV] UPDATED)\nAbstract: Young children develop sophisticated internal models of the world based on their visual experience. Can such models be learned from a child's visual experience without strong inductive biases? To investigate this, we train state-of-the-art neural networks on a realistic proxy of a child's visual experience without any explicit supervision or domain-specific inductive biases. Specifically, we train both embedding models and generative models on 200 hours of headcam video from a single child collected over two years and comprehensively evaluate their performance in downstream tasks using various reference models as yardsticks. On average, the best embedding models perform at a respectable 70% of a high-performance ImageNet-trained model, despite substantial differences in training data. They also learn broad semantic categories and object localization capabilities without explicit supervision, but they are less object-centric than models trained on all of ImageNet. Generative models trai",
    "path": "papers/23/05/2305.15372.json",
    "total_tokens": 972,
    "translated_title": "从儿童视角学习高级视觉表示而不引入强归纳偏差",
    "translated_abstract": "年幼的儿童通过他们的视觉经验发展出复杂的世界内部模型。可以从儿童的视觉经验中学习这样的模型而不引入强归纳偏差吗？为了调查这个问题，我们在一个儿童的真实视觉经验的代理上训练了最先进的神经网络，没有任何明确的监督或领域特定的归纳偏差。具体地，我们使用来自单个儿童的200小时头戴摄像机视频训练了嵌入模型和生成模型，并使用各种参考模型作为衡量标准全面评估了它们在下游任务中的性能。平均而言，最佳的嵌入模型在表现上达到了一种高性能的ImageNet训练模型的70%，尽管训练数据存在相当大的差异。它们还学习了广泛的语义类别和对象定位能力，而不需要明确的监督，但它们比在全部ImageNet上训练的模型更少关注对象。生成模型训练效果较差…",
    "tldr": "通过儿童的视觉经验，我们在没有引入强归纳偏差的情况下训练了最先进的神经网络模型，并成功学习了广泛的语义类别和对象定位能力。嵌入模型表现为ImageNet模型的70%水平，尽管训练数据存在差异。"
}