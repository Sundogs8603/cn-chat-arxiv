{
    "title": "Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback. (arXiv:2305.14975v2 [cs.CL] UPDATED)",
    "abstract": "A trustworthy real-world prediction system should produce well-calibrated confidence scores; that is, its confidence in an answer should be indicative of the likelihood that the answer is correct, enabling deferral to an expert in cases of low-confidence predictions. Recent studies have shown that unsupervised pre-training produces large language models (LMs) whose conditional probabilities are remarkably well-calibrated. However, the most widely-used LMs are fine-tuned with reinforcement learning from human feedback (RLHF-LMs), and some studies have suggested that RLHF-LMs produce conditional probabilities that are very poorly calibrated. In light of this perceived weakness, we conduct a broad evaluation of methods for extracting confidence scores from RLHF-LMs. For RLHF-LMs such as ChatGPT, GPT-4, and Claude, we find that verbalized confidences emitted as output tokens are typically better-calibrated than the model's conditional probabilities on the TriviaQA, SciQ, and TruthfulQA ben",
    "link": "http://arxiv.org/abs/2305.14975",
    "context": "Title: Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback. (arXiv:2305.14975v2 [cs.CL] UPDATED)\nAbstract: A trustworthy real-world prediction system should produce well-calibrated confidence scores; that is, its confidence in an answer should be indicative of the likelihood that the answer is correct, enabling deferral to an expert in cases of low-confidence predictions. Recent studies have shown that unsupervised pre-training produces large language models (LMs) whose conditional probabilities are remarkably well-calibrated. However, the most widely-used LMs are fine-tuned with reinforcement learning from human feedback (RLHF-LMs), and some studies have suggested that RLHF-LMs produce conditional probabilities that are very poorly calibrated. In light of this perceived weakness, we conduct a broad evaluation of methods for extracting confidence scores from RLHF-LMs. For RLHF-LMs such as ChatGPT, GPT-4, and Claude, we find that verbalized confidences emitted as output tokens are typically better-calibrated than the model's conditional probabilities on the TriviaQA, SciQ, and TruthfulQA ben",
    "path": "papers/23/05/2305.14975.json",
    "total_tokens": 973,
    "translated_title": "只需提问即可进行标定: 从人类反馈的语言模型中获取标定的置信度得分的策略",
    "translated_abstract": "一个可信赖的实际预测系统应该产生良好标定的置信度得分；也就是说，其对答案的置信度应该能够表明答案正确的可能性，从而在置信度较低的情况下可以寻求专家意见。最近的研究表明，无监督预训练产生的大型语言模型（LMs）的条件概率非常好地进行了标定。然而，最广泛使用的LMs是通过从人类反馈进行强化学习进行精调（RLHF-LMs），一些研究指出RLHF-LMs产生的条件概率非常差地进行标定。鉴于这种知觉上的弱点，我们对从RLHF-LMs中提取置信度得分的方法进行了广泛评估。对于像ChatGPT、GPT-4和Claude这样的RLHF-LMs，我们发现输出标记中发出的语言化的置信度通常比模型在TriviaQA、SciQ和TruthfulQA ben上的条件概率更好地进行了标定。",
    "tldr": "本论文对从人类反馈的语言模型中提取置信度得分的方法进行了广泛评估。研究发现，对于RLHF-LMs，像ChatGPT、GPT-4和Claude这样的模型输出的语言化置信度通常比条件概率更好地进行了标定。",
    "en_tdlr": "This paper conducts a comprehensive evaluation of methods for extracting confidence scores from language models fine-tuned with human feedback (RLHF-LMs). The study finds that for RLHF-LMs such as ChatGPT, GPT-4, and Claude, the verbalized confidences emitted as output tokens are typically better calibrated than the model's conditional probabilities."
}