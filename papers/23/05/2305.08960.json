{
    "title": "Training Neural Networks without Backpropagation: A Deeper Dive into the Likelihood Ratio Method. (arXiv:2305.08960v1 [cs.LG])",
    "abstract": "Backpropagation (BP) is the most important gradient estimation method for training neural networks in deep learning. However, the literature shows that neural networks trained by BP are vulnerable to adversarial attacks. We develop the likelihood ratio (LR) method, a new gradient estimation method, for training a broad range of neural network architectures, including convolutional neural networks, recurrent neural networks, graph neural networks, and spiking neural networks, without recursive gradient computation. We propose three methods to efficiently reduce the variance of the gradient estimation in the neural network training process. Our experiments yield numerical results for training different neural networks on several datasets. All results demonstrate that the LR method is effective for training various neural networks and significantly improves the robustness of the neural networks under adversarial attacks relative to the BP method.",
    "link": "http://arxiv.org/abs/2305.08960",
    "context": "Title: Training Neural Networks without Backpropagation: A Deeper Dive into the Likelihood Ratio Method. (arXiv:2305.08960v1 [cs.LG])\nAbstract: Backpropagation (BP) is the most important gradient estimation method for training neural networks in deep learning. However, the literature shows that neural networks trained by BP are vulnerable to adversarial attacks. We develop the likelihood ratio (LR) method, a new gradient estimation method, for training a broad range of neural network architectures, including convolutional neural networks, recurrent neural networks, graph neural networks, and spiking neural networks, without recursive gradient computation. We propose three methods to efficiently reduce the variance of the gradient estimation in the neural network training process. Our experiments yield numerical results for training different neural networks on several datasets. All results demonstrate that the LR method is effective for training various neural networks and significantly improves the robustness of the neural networks under adversarial attacks relative to the BP method.",
    "path": "papers/23/05/2305.08960.json",
    "total_tokens": 846,
    "translated_title": "不使用反向传播训练神经网络：深入探究似然比方法",
    "translated_abstract": "反向传播是深度学习中训练神经网络的最重要的梯度估计方法。然而，文献表明，通过反向传播训练的神经网络容易受到对抗性攻击。我们开发了似然比方法，这是一种新的梯度估计方法，可以训练广泛的神经网络架构，包括卷积神经网络、循环神经网络、图神经网络和脉冲神经网络，而无需递归梯度计算。我们提出了三种方法来有效地减少神经网络训练过程中梯度估计的方差。我们的实验在多个数据集上训练不同的神经网络，并得到了数值结果。所有结果都表明，相对于反向传播方法，似然比方法对抗性攻击下有效地训练了各种神经网络，并显着提高了神经网络的鲁棒性。",
    "tldr": "提出一种新的似然比方法来训练神经网络，无需使用递归梯度计算，并在多种神经网络架构上有效地减少了对抗性攻击对模型造成的影响。",
    "en_tdlr": "The likelihood ratio method is proposed to train neural networks without recursive gradient computation and significantly reduces the vulnerability of neural networks to adversarial attacks for various network architectures."
}