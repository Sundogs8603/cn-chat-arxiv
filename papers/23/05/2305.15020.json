{
    "title": "An Efficient Multilingual Language Model Compression through Vocabulary Trimming. (arXiv:2305.15020v2 [cs.CL] UPDATED)",
    "abstract": "Multilingual language model (LM) have become a powerful tool in NLP especially for non-English languages. Nevertheless, model parameters of multilingual LMs remain large due to the larger embedding matrix of the vocabulary covering tokens in different languages. On the contrary, monolingual LMs can be trained in a target language with the language-specific vocabulary only, but this requires a large budget and availability of reliable corpora to achieve a high-quality LM from scratch. In this paper, we propose vocabulary-trimming (VT), a method to reduce a multilingual LM vocabulary to a target language by deleting irrelevant tokens from its vocabulary. In theory, VT can compress any existing multilingual LM to build monolingual LMs in any language covered by the multilingual LM. In our experiments, we show that VT can retain the original performance of the multilingual LM, while being smaller in size (in general around 50% of the original vocabulary size is enough) than the original mu",
    "link": "http://arxiv.org/abs/2305.15020",
    "context": "Title: An Efficient Multilingual Language Model Compression through Vocabulary Trimming. (arXiv:2305.15020v2 [cs.CL] UPDATED)\nAbstract: Multilingual language model (LM) have become a powerful tool in NLP especially for non-English languages. Nevertheless, model parameters of multilingual LMs remain large due to the larger embedding matrix of the vocabulary covering tokens in different languages. On the contrary, monolingual LMs can be trained in a target language with the language-specific vocabulary only, but this requires a large budget and availability of reliable corpora to achieve a high-quality LM from scratch. In this paper, we propose vocabulary-trimming (VT), a method to reduce a multilingual LM vocabulary to a target language by deleting irrelevant tokens from its vocabulary. In theory, VT can compress any existing multilingual LM to build monolingual LMs in any language covered by the multilingual LM. In our experiments, we show that VT can retain the original performance of the multilingual LM, while being smaller in size (in general around 50% of the original vocabulary size is enough) than the original mu",
    "path": "papers/23/05/2305.15020.json",
    "total_tokens": 918,
    "translated_title": "通过词汇修剪实现高效的多语言语言模型压缩",
    "translated_abstract": "多语言语言模型（LM）已经成为自然语言处理中非英语语言的强大工具。然而，由于涵盖不同语言标记的词汇嵌入矩阵较大，多语言LM的模型参数仍然很大。相反，单一语言模型可以使用特定于语言的词汇在目标语言中训练，但这需要大量预算和可靠语料库才能从头开始实现高质量的语言模型。在本文中，我们提出了词汇修剪（VT）的方法，通过从词汇中删除不相关的标记，将多语言LM的词汇减少到目标语言。理论上，VT可以压缩任何现有的多语言LM，以在多语言LM涵盖的任何语言中构建单一语言模型。在我们的实验中，我们展示了VT可以保留多语言LM的原始性能，同时尺寸更小（通常只需原始词汇大小的约50％）。",
    "tldr": "该论文提出了一种名为词汇修剪（VT）的方法，通过删除多语言语言模型中的不相关标记，将其压缩为目标语言模型。实验证明，词汇修剪可以在保持多语言模型性能的同时，降低了模型的大小。"
}