{
    "title": "Maybe Only 0.5% Data is Needed: A Preliminary Exploration of Low Training Data Instruction Tuning. (arXiv:2305.09246v1 [cs.AI])",
    "abstract": "Instruction tuning for large language models (LLMs) has gained attention from researchers due to its ability to unlock the potential of LLMs in following instructions. While instruction tuning offers advantages for facilitating the adaptation of large language models (LLMs) to downstream tasks as a fine-tuning approach, training models with tens of millions or even billions of parameters on large amounts of data results in unaffordable computational costs. To address this, we focus on reducing the data used in LLM instruction tuning to decrease training costs and improve data efficiency, dubbed as Low Training Data Instruction Tuning (LTD Instruction Tuning). Specifically, this paper conducts a preliminary exploration into reducing the data used in LLM training and identifies several observations regarding task specialization for LLM training, such as the optimization of performance for a specific task, the number of instruction types required for instruction tuning, and the amount of ",
    "link": "http://arxiv.org/abs/2305.09246",
    "context": "Title: Maybe Only 0.5% Data is Needed: A Preliminary Exploration of Low Training Data Instruction Tuning. (arXiv:2305.09246v1 [cs.AI])\nAbstract: Instruction tuning for large language models (LLMs) has gained attention from researchers due to its ability to unlock the potential of LLMs in following instructions. While instruction tuning offers advantages for facilitating the adaptation of large language models (LLMs) to downstream tasks as a fine-tuning approach, training models with tens of millions or even billions of parameters on large amounts of data results in unaffordable computational costs. To address this, we focus on reducing the data used in LLM instruction tuning to decrease training costs and improve data efficiency, dubbed as Low Training Data Instruction Tuning (LTD Instruction Tuning). Specifically, this paper conducts a preliminary exploration into reducing the data used in LLM training and identifies several observations regarding task specialization for LLM training, such as the optimization of performance for a specific task, the number of instruction types required for instruction tuning, and the amount of ",
    "path": "papers/23/05/2305.09246.json",
    "total_tokens": 737,
    "translated_title": "或许只需要0.5％的数据：低数据量训练指令调整初步探索",
    "translated_abstract": "针对训练大型语言模型（LLMs）的问题，本文进行了初步探索，以降低LLM指令调整所需的数据量，从而减少培训成本，提高数据效率。 研究发现，LLM指令调整的数据可以降至0.5％而不影响性能，将这种方法称为Low Training Data Instruction Tuning（LTD Instruction Tuning）。",
    "tldr": "本研究发现只需使用0.5%数据便可以进行训练大型语言模型（LLMs）指令调整，并且不影响性能表现。这种方法可以提高数据效率和节约培训成本。",
    "en_tdlr": "This paper proposes a Low Training Data Instruction Tuning (LTD Instruction Tuning) method to reduce the data used in large language model (LLM) instruction tuning. The research finds that only 0.5% of data is needed and performance is not impacted. This approach improves data efficiency and saves training costs."
}