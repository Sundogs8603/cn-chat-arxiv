{
    "title": "Language Models with Rationality. (arXiv:2305.14250v2 [cs.CL] UPDATED)",
    "abstract": "While large language models (LLMs) are proficient at question-answering (QA), it is not always clear how (or even if) an answer follows from their latent \"beliefs\". This lack of interpretability is a growing impediment to widespread use of LLMs. To address this, our goals are to make model beliefs and their inferential relationships explicit, and to resolve inconsistencies that may exist, so that answers are supported by interpretable chains of reasoning drawn from a consistent network of beliefs. Our approach, which we call REFLEX, is to add a rational, self-reflecting layer on top of the LLM. First, given a question, we construct a belief graph using a backward-chaining process to materialize relevant model beliefs (including beliefs about answer candidates) and their inferential relationships. Second, we identify and minimize contradictions in that graph using a formal constraint reasoner. We find that REFLEX significantly improves consistency (by 8%-11% absolute) without harming ov",
    "link": "http://arxiv.org/abs/2305.14250",
    "context": "Title: Language Models with Rationality. (arXiv:2305.14250v2 [cs.CL] UPDATED)\nAbstract: While large language models (LLMs) are proficient at question-answering (QA), it is not always clear how (or even if) an answer follows from their latent \"beliefs\". This lack of interpretability is a growing impediment to widespread use of LLMs. To address this, our goals are to make model beliefs and their inferential relationships explicit, and to resolve inconsistencies that may exist, so that answers are supported by interpretable chains of reasoning drawn from a consistent network of beliefs. Our approach, which we call REFLEX, is to add a rational, self-reflecting layer on top of the LLM. First, given a question, we construct a belief graph using a backward-chaining process to materialize relevant model beliefs (including beliefs about answer candidates) and their inferential relationships. Second, we identify and minimize contradictions in that graph using a formal constraint reasoner. We find that REFLEX significantly improves consistency (by 8%-11% absolute) without harming ov",
    "path": "papers/23/05/2305.14250.json",
    "total_tokens": 899,
    "translated_title": "具有合理性的语言模型",
    "translated_abstract": "虽然大型语言模型(Large Language Models, LLMs)在问答中非常擅长，但它们的答案与其内在的“信念”之间的关系往往不明确。这种缺乏解释性阻碍了LLMs的广泛使用。为了解决这个问题，我们的目标是使模型的信念以及它们的推理关系变得明确，并消除可能存在的矛盾，以便答案能够通过从一致的信念网络中得出的可解释的推理链来支持。我们的方法名为REFLEX，在LLM之上添加了一个具有合理性和自反性的层。首先，给定一个问题，我们使用反向链接过程构建一个信念图，以实现相关模型信念(包括对答案候选者的信念)及其推理关系。其次，我们使用形式约束推理器识别和最小化该图中的矛盾。我们发现，REFLEX显著提高了一致性(绝对值提升了8%-11%)，而不损害已有的性能。",
    "tldr": "本研究提出了一种名为REFLEX的方法，在大型语言模型上添加了合理性和自反性层，以使模型的答案得以解释并消除潜在的矛盾。",
    "en_tdlr": "This paper introduces a method called REFLEX, which adds a rational and self-reflecting layer on top of large language models (LLMs) to achieve interpretability and resolve potential contradictions in the model's answers."
}