{
    "title": "Sharpened Lazy Incremental Quasi-Newton Method. (arXiv:2305.17283v1 [math.OC])",
    "abstract": "We consider the finite sum minimization of $n$ strongly convex and smooth functions with Lipschitz continuous Hessians in $d$ dimensions. In many applications where such problems arise, including maximum likelihood estimation, empirical risk minimization, and unsupervised learning, the number of observations $n$ is large, and it becomes necessary to use incremental or stochastic algorithms whose per-iteration complexity is independent of $n$. Of these, the incremental/stochastic variants of the Newton method exhibit superlinear convergence, but incur a per-iteration complexity of $O(d^3)$, which may be prohibitive in large-scale settings. On the other hand, the incremental Quasi-Newton method incurs a per-iteration complexity of $O(d^2)$ but its superlinear convergence rate has only been characterized asymptotically. This work puts forth the Sharpened Lazy Incremental Quasi-Newton (SLIQN) method that achieves the best of both worlds: an explicit superlinear convergence rate with a per-",
    "link": "http://arxiv.org/abs/2305.17283",
    "context": "Title: Sharpened Lazy Incremental Quasi-Newton Method. (arXiv:2305.17283v1 [math.OC])\nAbstract: We consider the finite sum minimization of $n$ strongly convex and smooth functions with Lipschitz continuous Hessians in $d$ dimensions. In many applications where such problems arise, including maximum likelihood estimation, empirical risk minimization, and unsupervised learning, the number of observations $n$ is large, and it becomes necessary to use incremental or stochastic algorithms whose per-iteration complexity is independent of $n$. Of these, the incremental/stochastic variants of the Newton method exhibit superlinear convergence, but incur a per-iteration complexity of $O(d^3)$, which may be prohibitive in large-scale settings. On the other hand, the incremental Quasi-Newton method incurs a per-iteration complexity of $O(d^2)$ but its superlinear convergence rate has only been characterized asymptotically. This work puts forth the Sharpened Lazy Incremental Quasi-Newton (SLIQN) method that achieves the best of both worlds: an explicit superlinear convergence rate with a per-",
    "path": "papers/23/05/2305.17283.json",
    "total_tokens": 748,
    "translated_title": "优化退火算法的误差界和局部搜索策略",
    "translated_abstract": "本文考虑了具有$Lipschitz$连续Hessian矩阵在$d$维空间中，$n$个强凸光滑函数的有限和最小化问题。在许多应用中，$n$的数量很大，因此必须使用每迭代一次与$n$无关的增量式或随机算法。本文提出了一种新算法—— Sharpened Lazy Incremental Quasi-Newton (SLIQN) 方法，其具有显式的超线性收敛速率和$O(d^2)$的迭代复杂度。",
    "tldr": "本文提出了一种新算法—— Sharpened Lazy Incremental Quasi-Newton (SLIQN) 方法，其具有显式的超线性收敛速率和$O(d^2)$的迭代复杂度。"
}