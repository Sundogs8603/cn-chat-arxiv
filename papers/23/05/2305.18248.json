{
    "title": "Do Language Models Know When They're Hallucinating References?. (arXiv:2305.18248v2 [cs.CL] UPDATED)",
    "abstract": "State-of-the-art language models (LMs) are famous for \"hallucinating\" references. These fabricated article and book titles lead to harms, obstacles to their use, and public backlash. While other types of LM hallucinations are also important, we propose hallucinated references as the \"drosophila\" of research on hallucination in large language models (LLMs), as they are particularly easy to study. We show that simple search engine queries reliably identify such hallucinations, which facilitates evaluation. To begin to dissect the nature of hallucinated LM references, we attempt to classify them using black-box queries to the same LM, without consulting any external resources. Consistency checks done with \"direct\" queries about whether the generated reference title is real (inspired by Kadavath et al. 2022, Lin et al. 2022, Manakul et al. 2023) are compared to consistency checks with \"indirect\" queries which ask for ancillary details such as the authors of the work. These consistency chec",
    "link": "http://arxiv.org/abs/2305.18248",
    "context": "Title: Do Language Models Know When They're Hallucinating References?. (arXiv:2305.18248v2 [cs.CL] UPDATED)\nAbstract: State-of-the-art language models (LMs) are famous for \"hallucinating\" references. These fabricated article and book titles lead to harms, obstacles to their use, and public backlash. While other types of LM hallucinations are also important, we propose hallucinated references as the \"drosophila\" of research on hallucination in large language models (LLMs), as they are particularly easy to study. We show that simple search engine queries reliably identify such hallucinations, which facilitates evaluation. To begin to dissect the nature of hallucinated LM references, we attempt to classify them using black-box queries to the same LM, without consulting any external resources. Consistency checks done with \"direct\" queries about whether the generated reference title is real (inspired by Kadavath et al. 2022, Lin et al. 2022, Manakul et al. 2023) are compared to consistency checks with \"indirect\" queries which ask for ancillary details such as the authors of the work. These consistency chec",
    "path": "papers/23/05/2305.18248.json",
    "total_tokens": 949,
    "translated_title": "语言模型知道自己在产生“幻觉”参考文献吗？",
    "translated_abstract": "目前最先进的语言模型以其“幻觉”参考文献而闻名。这些虚构的文章和书名引起了危害，对它们的使用造成了障碍，并引起了公众的反弹。尽管其他类型的语言模型幻觉也很重要，但我们将幻觉参考文献提出作为大型语言模型(LLMs)中幻觉研究的“果蝇”，因为它们特别容易研究。我们展示了简单的搜索引擎查询可可靠地识别此类幻觉，从而便于评估。为了开始剖析幻觉语言模型参考文献的性质，我们尝试使用对同一语言模型的黑盒查询来对其进行分类，而不借助任何外部资源。我们将“直接”查询的一致性检查与“间接”查询的一致性检查进行了比较，后者询问了附加的细节，如作品的作者。",
    "tldr": "本研究针对大型语言模型中的“幻觉”参考文献进行了研究，通过简单的搜索引擎查询可可靠地识别这些幻觉。并且通过对同一语言模型进行黑盒查询来进行分类，揭示了幻觉参考文献的性质。"
}