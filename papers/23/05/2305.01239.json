{
    "title": "DRPT: Disentangled and Recurrent Prompt Tuning for Compositional Zero-Shot Learning. (arXiv:2305.01239v1 [cs.CV])",
    "abstract": "Compositional Zero-shot Learning (CZSL) aims to recognize novel concepts composed of known knowledge without training samples. Standard CZSL either identifies visual primitives or enhances unseen composed entities, and as a result, entanglement between state and object primitives cannot be fully utilized. Admittedly, vision- language models (VLMs) could naturally cope with CZSL through tuning prompts, while uneven entanglement leads prompts to be dragged into local optimum. In this paper, we take a further step to introduce a novel Disentangled and Recurrent Prompt Tuning framework termed DRPT to better tap the potential of VLMs in CZSL. Specifically, the state and object primitives are deemed as learnable tokens of vocabulary embedded in prompts and tuned on seen compositions. Instead of jointly tuning state and object, we devise a disentangled and recurrent tuning strategy to suppress the traction force caused by entanglement and gradually optimize the token parameters, leading to a ",
    "link": "http://arxiv.org/abs/2305.01239",
    "context": "Title: DRPT: Disentangled and Recurrent Prompt Tuning for Compositional Zero-Shot Learning. (arXiv:2305.01239v1 [cs.CV])\nAbstract: Compositional Zero-shot Learning (CZSL) aims to recognize novel concepts composed of known knowledge without training samples. Standard CZSL either identifies visual primitives or enhances unseen composed entities, and as a result, entanglement between state and object primitives cannot be fully utilized. Admittedly, vision- language models (VLMs) could naturally cope with CZSL through tuning prompts, while uneven entanglement leads prompts to be dragged into local optimum. In this paper, we take a further step to introduce a novel Disentangled and Recurrent Prompt Tuning framework termed DRPT to better tap the potential of VLMs in CZSL. Specifically, the state and object primitives are deemed as learnable tokens of vocabulary embedded in prompts and tuned on seen compositions. Instead of jointly tuning state and object, we devise a disentangled and recurrent tuning strategy to suppress the traction force caused by entanglement and gradually optimize the token parameters, leading to a ",
    "path": "papers/23/05/2305.01239.json",
    "total_tokens": 989,
    "translated_title": "DRPT: 分离与循环提示调整的组合零样本学习",
    "translated_abstract": "组合零样本学习旨在识别由已知知识组成的新概念，没有训练样本。标准的组合零样本学习要么识别视觉原语，要么增强看不见的组合实体，导致状态和对象原语之间的纠缠无法完全利用。本文提出了一种新的分离和循环提示调整框架DRPT，通过学习提示中的词汇嵌入状态和对象原语，并在看到的组合中调整它们，优化记号参数。相比于联合调整状态和对象，我们采用了分离和循环的调整策略，抑制了纠缠所引起的牵引力，并逐步优化了记号参数，从而实现了更有效和高效的组合零样本学习模型。在三个基准数据集上的实验证明，DRPT实现了最先进的CZSL性能，超过现有方法的显著差距。",
    "tldr": "该论文提出了一种名为DRPT的算法，通过分离与循环提示调整的方式来优化记号参数，使得在组合零样本学习中所采用的视觉-语言模型能够更有效地进行识别。实验表明，DRPT在三个基准测试数据集上取得了最先进的性能表现。",
    "en_tdlr": "This paper proposes a novel Disentangled and Recurrent Prompt Tuning framework (DRPT) for Compositional Zero-shot Learning (CZSL), which optimizes the token parameters in vision-language models (VLMs) through a disentangled and recurrent tuning strategy. DRPT surpassed existing CZSL methods with significant margin on three benchmark datasets."
}