{
    "title": "AxomiyaBERTa: A Phonologically-aware Transformer Model for Assamese. (arXiv:2305.13641v1 [cs.CL])",
    "abstract": "Despite their successes in NLP, Transformer-based language models still require extensive computing resources and suffer in low-resource or low-compute settings. In this paper, we present AxomiyaBERTa, a novel BERT model for Assamese, a morphologically-rich low-resource language (LRL) of Eastern India. AxomiyaBERTa is trained only on the masked language modeling (MLM) task, without the typical additional next sentence prediction (NSP) objective, and our results show that in resource-scarce settings for very low-resource languages like Assamese, MLM alone can be successfully leveraged for a range of tasks. AxomiyaBERTa achieves SOTA on token-level tasks like Named Entity Recognition and also performs well on \"longer-context\" tasks like Cloze-style QA and Wiki Title Prediction, with the assistance of a novel embedding disperser and phonological signals respectively. Moreover, we show that AxomiyaBERTa can leverage phonological signals for even more challenging tasks, such as a novel cros",
    "link": "http://arxiv.org/abs/2305.13641",
    "context": "Title: AxomiyaBERTa: A Phonologically-aware Transformer Model for Assamese. (arXiv:2305.13641v1 [cs.CL])\nAbstract: Despite their successes in NLP, Transformer-based language models still require extensive computing resources and suffer in low-resource or low-compute settings. In this paper, we present AxomiyaBERTa, a novel BERT model for Assamese, a morphologically-rich low-resource language (LRL) of Eastern India. AxomiyaBERTa is trained only on the masked language modeling (MLM) task, without the typical additional next sentence prediction (NSP) objective, and our results show that in resource-scarce settings for very low-resource languages like Assamese, MLM alone can be successfully leveraged for a range of tasks. AxomiyaBERTa achieves SOTA on token-level tasks like Named Entity Recognition and also performs well on \"longer-context\" tasks like Cloze-style QA and Wiki Title Prediction, with the assistance of a novel embedding disperser and phonological signals respectively. Moreover, we show that AxomiyaBERTa can leverage phonological signals for even more challenging tasks, such as a novel cros",
    "path": "papers/23/05/2305.13641.json",
    "total_tokens": 1009,
    "translated_title": "AxomiyaBERTa：一款面向阿萨姆语的音韵感知变压器模型",
    "translated_abstract": "尽管变压器模型在自然语言处理中取得了成功，但它们仍需要大量的计算资源，并且在低资源或低计算环境中表现差。本文提出了AxomiyaBERTa，一种用于东印度阿萨姆语的新型BERT模型。AxomiyaBERTa仅在遮蔽语言模型（MLM）任务中进行训练，而不是典型的附加下一句预测（NSP）目标。我们的结果表明，在像阿萨姆语这样的低资源语言中，仅使用MLM就可以成功地利用一系列任务。AxomiyaBERTa在命名实体识别等令牌级任务上取得了SOTA的成果，并且在Cloze-style QA 和 Wiki Title Prediction 等“长篇内容”任务中表现良好，得益于新颖的嵌入离散化器和音韵信号。此外，我们还展示了AxomiyaBERTa可以利用音韵信号来处理更具挑战性的任务，例如一种新的交叉编码任务。",
    "tldr": "AxomiyaBERTa是面向低资源语言阿萨姆语的一种创新的BERT模型，仅使用遮蔽语言模型（MLM）任务进行训练，能够在命名实体识别等任务中取得SOTA表现，并通过新颖的嵌入离散化器和音韵信号处理长篇内容任务。",
    "en_tdlr": "AxomiyaBERTa is an innovative BERT model for the low-resource language of Assamese, trained only on the masked language modeling (MLM) task, achieving SOTA in tasks like named entity recognition, and handling longer-text tasks with the help of a novel embedding disperser and phonological signals. It can even handle more challenging tasks such as a novel cross-lingual coding task by leveraging phonological signals."
}