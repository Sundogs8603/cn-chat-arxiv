{
    "title": "New Equivalences Between Interpolation and SVMs: Kernels and Structured Features. (arXiv:2305.02304v1 [stat.ML])",
    "abstract": "The support vector machine (SVM) is a supervised learning algorithm that finds a maximum-margin linear classifier, often after mapping the data to a high-dimensional feature space via the kernel trick. Recent work has demonstrated that in certain sufficiently overparameterized settings, the SVM decision function coincides exactly with the minimum-norm label interpolant. This phenomenon of support vector proliferation (SVP) is especially interesting because it allows us to understand SVM performance by leveraging recent analyses of harmless interpolation in linear and kernel models. However, previous work on SVP has made restrictive assumptions on the data/feature distribution and spectrum. In this paper, we present a new and flexible analysis framework for proving SVP in an arbitrary reproducing kernel Hilbert space with a flexible class of generative models for the labels. We present conditions for SVP for features in the families of general bounded orthonormal systems (e.g. Fourier f",
    "link": "http://arxiv.org/abs/2305.02304",
    "context": "Title: New Equivalences Between Interpolation and SVMs: Kernels and Structured Features. (arXiv:2305.02304v1 [stat.ML])\nAbstract: The support vector machine (SVM) is a supervised learning algorithm that finds a maximum-margin linear classifier, often after mapping the data to a high-dimensional feature space via the kernel trick. Recent work has demonstrated that in certain sufficiently overparameterized settings, the SVM decision function coincides exactly with the minimum-norm label interpolant. This phenomenon of support vector proliferation (SVP) is especially interesting because it allows us to understand SVM performance by leveraging recent analyses of harmless interpolation in linear and kernel models. However, previous work on SVP has made restrictive assumptions on the data/feature distribution and spectrum. In this paper, we present a new and flexible analysis framework for proving SVP in an arbitrary reproducing kernel Hilbert space with a flexible class of generative models for the labels. We present conditions for SVP for features in the families of general bounded orthonormal systems (e.g. Fourier f",
    "path": "papers/23/05/2305.02304.json",
    "total_tokens": 877,
    "translated_title": "揭示插值和支持向量机之间的新等价性：核函数和结构化特征",
    "translated_abstract": "支持向量机（SVM）是一种监督学习算法，它通过核技巧将数据映射到高维特征空间，找到最大间隔线性分类器。最近的研究表明，在某些过度参数化的情况下，SVM的决策函数与最小范数标签插值完全重合。这种支持向量增殖（SVP）现象特别有趣，因为它使我们能够通过利用线性和核模型中无害插值的最近分析来理解SVM的性能。然而，先前关于SVP的工作对数据/特征分布和频谱做出了限制性假设。在本文中，我们提出了一种新的、灵活的分析框架，用于在任意再生核希尔伯特空间中，对标签的生成模型的一类灵活性特征进行SVP证明。我们提出了局限于一般有界正交系统族（例如Fourier函数族）特征的SVP条件",
    "tldr": "本文提出了一种新的、灵活的分析框架，用于证明在任意再生核希尔伯特空间中特征的SVP条件"
}