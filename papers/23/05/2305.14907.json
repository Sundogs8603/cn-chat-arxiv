{
    "title": "Coverage-based Example Selection for In-Context Learning. (arXiv:2305.14907v2 [cs.CL] UPDATED)",
    "abstract": "In-context learning (ICL), the ability of large language models to perform novel tasks by conditioning on a prompt with a few task examples, requires these examples to be informative about the test instance. The standard approach of independently ranking and selecting the most similar examples selects redundant examples while omitting important information. In this work, we show that BERTScore-Recall (BSR) selects better examples that demonstrate more of the salient aspects, e.g. reasoning patterns, of the test input. We further extend BSR and many standard metrics to easily optimizable set-level metrics, giving still better coverage of those salient aspects. On 15 datasets spanning 6 tasks and with 7 diverse LLMs, we show that (1) BSR is the superior metric for in-context example selection across the board, and (2) for compositional tasks, set selection using Set-BSR outperforms independent ranking by up to 17 points on average and, despite being training-free, surpasses methods that ",
    "link": "http://arxiv.org/abs/2305.14907",
    "context": "Title: Coverage-based Example Selection for In-Context Learning. (arXiv:2305.14907v2 [cs.CL] UPDATED)\nAbstract: In-context learning (ICL), the ability of large language models to perform novel tasks by conditioning on a prompt with a few task examples, requires these examples to be informative about the test instance. The standard approach of independently ranking and selecting the most similar examples selects redundant examples while omitting important information. In this work, we show that BERTScore-Recall (BSR) selects better examples that demonstrate more of the salient aspects, e.g. reasoning patterns, of the test input. We further extend BSR and many standard metrics to easily optimizable set-level metrics, giving still better coverage of those salient aspects. On 15 datasets spanning 6 tasks and with 7 diverse LLMs, we show that (1) BSR is the superior metric for in-context example selection across the board, and (2) for compositional tasks, set selection using Set-BSR outperforms independent ranking by up to 17 points on average and, despite being training-free, surpasses methods that ",
    "path": "papers/23/05/2305.14907.json",
    "total_tokens": 1017,
    "translated_title": "基于覆盖率的上下文学习中示例选择方法",
    "translated_abstract": "上下文学习（ICL）是大型语言模型通过在一些任务示例上进行条件约束从而实现新任务的能力，这要求这些示例对测试实例具有信息量。标准的方法是独立地对最相似的示例进行排名和选择，这样选择出的示例会重复且遗漏重要信息。本研究表明，BERTScore-Recall（BSR）选择了更好的示例，这些示例展示了测试输入的关键方面，如推理模式。我们进一步扩展了BSR和许多标准度量方法，将其转化为易于优化的集合级别度量方法，从而更好地覆盖这些关键方面。在涵盖6个任务的15个数据集和7个不同的LLM上，我们展示了（1）BSR在上下文示例选择方面是优越的度量方法，（2）对于组合任务，使用Set-BSR进行集合选择的性能优于独立排名，平均提高17个百分点，并且尽管无需训练但超过了现有方法。",
    "tldr": "本研究提出了一种基于覆盖率的上下文学习中示例选择方法，通过使用BERTScore-Recall度量方法选择更好的示例来展示测试输入的关键方面，同时还通过扩展成集合级别度量方法进一步提高了覆盖率表现。实验证明BSR是上下文示例选择中优越的度量方法，并且对于组合任务，使用Set-BSR进行集合选择可以显著提高性能。",
    "en_tdlr": "This work proposes a coverage-based example selection method for in-context learning, which selects better examples using the BERTScore-Recall metric to demonstrate the salient aspects of the test input, and further improves coverage by extending it to set-level metrics. Experimental results show that BSR is superior in in-context example selection, and using Set-BSR for set selection significantly improves performance for compositional tasks."
}