{
    "title": "How to Choose How to Choose Your Chatbot: A Massively Multi-System MultiReference Data Set for Dialog Metric Evaluation. (arXiv:2305.14533v1 [cs.CL])",
    "abstract": "We release MMSMR, a Massively Multi-System MultiReference dataset to enable future work on metrics and evaluation for dialog. Automatic metrics for dialogue evaluation should be robust proxies for human judgments; however, the verification of robustness is currently far from satisfactory. To quantify the robustness correlation and understand what is necessary in a test set, we create and release an 8-reference dialog dataset by extending single-reference evaluation sets and introduce this new language learning conversation dataset. We then train 1750 systems and evaluate them on our novel test set and the DailyDialog dataset. We release the novel test set, and model hyper parameters, inference outputs, and metric scores for each system on a variety of datasets.",
    "link": "http://arxiv.org/abs/2305.14533",
    "context": "Title: How to Choose How to Choose Your Chatbot: A Massively Multi-System MultiReference Data Set for Dialog Metric Evaluation. (arXiv:2305.14533v1 [cs.CL])\nAbstract: We release MMSMR, a Massively Multi-System MultiReference dataset to enable future work on metrics and evaluation for dialog. Automatic metrics for dialogue evaluation should be robust proxies for human judgments; however, the verification of robustness is currently far from satisfactory. To quantify the robustness correlation and understand what is necessary in a test set, we create and release an 8-reference dialog dataset by extending single-reference evaluation sets and introduce this new language learning conversation dataset. We then train 1750 systems and evaluate them on our novel test set and the DailyDialog dataset. We release the novel test set, and model hyper parameters, inference outputs, and metric scores for each system on a variety of datasets.",
    "path": "papers/23/05/2305.14533.json",
    "total_tokens": 824,
    "translated_title": "如何选择您的聊天机器人：用于对话指标评估的大规模多系统多参考数据集",
    "translated_abstract": "我们发布了MMSMR，这是一个大规模多系统多参考数据集，旨在促进对话的度量和评估的未来工作。用于对话评估的自动指标应该是人类判断的可靠代理；然而，目前对其稳健性的验证还远远不够令人满意。为了量化稳健性相关性并了解测试集中所需的内容，我们扩展了单参考评估集，推出了一个包含8个参考对话的数据集，并介绍了这个新的语言学习对话数据集。然后我们训练了1750个系统，并在我们的新测试集和DailyDialog数据集上对它们进行了评估。我们发布了这个新的测试集，以及每个系统在各种数据集上的模型超参数、推理输出和指标分数。",
    "tldr": "该研究发布了MMSMR数据集，该数据集包含8个参考对话，旨在促进对话度量和评估的未来工作。该研究使用1750个系统对其进行了评估，以了解稳健相关性并了解测试集中所需的内容。"
}