{
    "title": "Learning Missing Modal Electronic Health Records with Unified Multi-modal Data Embedding and Modality-Aware Attention. (arXiv:2305.02504v1 [cs.LG])",
    "abstract": "Electronic Health Record (EHR) provides abundant information through various modalities. However, learning multi-modal EHR is currently facing two major challenges, namely, 1) data embedding and 2) cases with missing modality. A lack of shared embedding function across modalities can discard the temporal relationship between different EHR modalities. On the other hand, most EHR studies are limited to relying only on EHR Times-series, and therefore, missing modality in EHR has not been well-explored. Therefore, in this study, we introduce a Unified Multi-modal Set Embedding (UMSE) and Modality-Aware Attention (MAA) with Skip Bottleneck (SB). UMSE treats all EHR modalities without a separate imputation module or error-prone carry-forward, whereas MAA with SB learns missing modal EHR with effective modality-aware attention. Our model outperforms other baseline models in mortality, vasopressor need, and intubation need prediction with the MIMIC-IV dataset.",
    "link": "http://arxiv.org/abs/2305.02504",
    "context": "Title: Learning Missing Modal Electronic Health Records with Unified Multi-modal Data Embedding and Modality-Aware Attention. (arXiv:2305.02504v1 [cs.LG])\nAbstract: Electronic Health Record (EHR) provides abundant information through various modalities. However, learning multi-modal EHR is currently facing two major challenges, namely, 1) data embedding and 2) cases with missing modality. A lack of shared embedding function across modalities can discard the temporal relationship between different EHR modalities. On the other hand, most EHR studies are limited to relying only on EHR Times-series, and therefore, missing modality in EHR has not been well-explored. Therefore, in this study, we introduce a Unified Multi-modal Set Embedding (UMSE) and Modality-Aware Attention (MAA) with Skip Bottleneck (SB). UMSE treats all EHR modalities without a separate imputation module or error-prone carry-forward, whereas MAA with SB learns missing modal EHR with effective modality-aware attention. Our model outperforms other baseline models in mortality, vasopressor need, and intubation need prediction with the MIMIC-IV dataset.",
    "path": "papers/23/05/2305.02504.json",
    "total_tokens": 995,
    "translated_title": "统一多模态数据嵌入和模态感知注意力学习缺失模态电子病历",
    "translated_abstract": "电子病历(EHR)通过多种模态提供了丰富的信息。然而，学习多模态EHR目前面临两个主要挑战，即数据嵌入和缺失模态的情况。缺乏跨模态的共享嵌入函数会丢弃不同EHR模态之间的时间关系。另一方面，大多数EHR研究仅依赖EHR时序，并且因此，EHR中的缺失模态尚未得到很好的探索。因此，在本研究中，我们引入了统一多模态集嵌入(UMSE)和模态感知注意力(MAA)与跳过瓶颈(SB)。UMSE对待所有EHR模态而无需单独的插补模块或容易出错的向前传递，而MAA与SB学习缺失的模态EHR，具有有效的模态感知注意力。我们的模型在MIMIC-IV数据集中的死亡率、血管加压素需要和插管需要预测中优于其他基线模型。",
    "tldr": "该论文提出了一种能够统一处理多模态电子病历的学习方法，并通过引入跳过瓶颈和模态感知注意力解决了缺失模态的情况，取得了在死亡率、血管加压素需要和插管需要预测等方面的表现优于其他基线模型。",
    "en_tdlr": "The paper proposes a learning method that can handle multi-modal electronic health records in a unified manner, and solves the problem of missing modality by introducing skip bottleneck and modality-aware attention. The proposed method outperforms other baseline models in mortality, vasopressor need, and intubation need prediction."
}