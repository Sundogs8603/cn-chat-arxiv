{
    "title": "DeCoR: Defy Knowledge Forgetting by Predicting Earlier Audio Codes. (arXiv:2305.18441v1 [eess.AS])",
    "abstract": "Lifelong audio feature extraction involves learning new sound classes incrementally, which is essential for adapting to new data distributions over time. However, optimizing the model only on new data can lead to catastrophic forgetting of previously learned tasks, which undermines the model's ability to perform well over the long term. This paper introduces a new approach to continual audio representation learning called DeCoR. Unlike other methods that store previous data, features, or models, DeCoR indirectly distills knowledge from an earlier model to the latest by predicting quantization indices from a delayed codebook. We demonstrate that DeCoR improves acoustic scene classification accuracy and integrates well with continual self-supervised representation learning. Our approach introduces minimal storage and computation overhead, making it a lightweight and efficient solution for continual learning.",
    "link": "http://arxiv.org/abs/2305.18441",
    "context": "Title: DeCoR: Defy Knowledge Forgetting by Predicting Earlier Audio Codes. (arXiv:2305.18441v1 [eess.AS])\nAbstract: Lifelong audio feature extraction involves learning new sound classes incrementally, which is essential for adapting to new data distributions over time. However, optimizing the model only on new data can lead to catastrophic forgetting of previously learned tasks, which undermines the model's ability to perform well over the long term. This paper introduces a new approach to continual audio representation learning called DeCoR. Unlike other methods that store previous data, features, or models, DeCoR indirectly distills knowledge from an earlier model to the latest by predicting quantization indices from a delayed codebook. We demonstrate that DeCoR improves acoustic scene classification accuracy and integrates well with continual self-supervised representation learning. Our approach introduces minimal storage and computation overhead, making it a lightweight and efficient solution for continual learning.",
    "path": "papers/23/05/2305.18441.json",
    "total_tokens": 927,
    "translated_title": "DeCoR: 通过预测早期音频编码来避免知识遗忘",
    "translated_abstract": "终身音频特征提取需要逐步学习新的声音类别，以适应随时间变化的新数据分布。然而，仅在新数据上优化模型可能会导致先前学习的任务灾难性遗忘，这会破坏模型在长期内的良好表现能力。本文介绍了一种名为DeCoR的新的持续音频表示学习方法。与存储先前数据、特征或模型的其他方法不同，DeCoR通过从延迟码本预测量化索引间接从早期模型向最新模型提炼知识。我们证明了DeCoR提高了声学场景分类准确性，并与持续的自监督表示学习相结合效果良好。我们的方法提供了极小的存储和计算开销，是持续学习的一种轻量级和高效的解决方案。",
    "tldr": "DeCoR是一种持续音频表示学习方法，通过预测早期音频编码中的量化索引，间接从早期模型向最新模型提取知识，避免在学习新数据时遗忘以前学习的任务。这种方法提高了声学场景分类准确性，与持续自监督表示学习相结合，存储和计算开销小，是一种高效的持续学习解决方案。",
    "en_tdlr": "DeCoR is a continual audio representation learning approach that avoids knowledge forgetting of previously learned tasks when optimizing the model on new data by indirectly distilling knowledge from an earlier model to the latest through predicting quantization indices from a delayed codebook. It improves acoustic scene classification accuracy and integrates well with continual self-supervised representation learning, while providing minimal storage and computation overhead."
}