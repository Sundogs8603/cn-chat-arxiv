{
    "title": "DietCNN: Multiplication-free Inference for Quantized CNNs. (arXiv:2305.05274v1 [cs.CV])",
    "abstract": "The rising demand for networked embedded systems with machine intelligence has been a catalyst for sustained attempts by the research community to implement Convolutional Neural Networks (CNN) based inferencing on embedded resource-limited devices. Redesigning a CNN by removing costly multiplication operations has already shown promising results in terms of reducing inference energy usage. This paper proposes a new method for replacing multiplications in a CNN by table look-ups. Unlike existing methods that completely modify the CNN operations, the proposed methodology preserves the semantics of the major CNN operations. Conforming to the existing mechanism of the CNN layer operations ensures that the reliability of a standard CNN is preserved. It is shown that the proposed multiplication-free CNN, based on a single activation codebook, can achieve 4.7x, 5.6x, and 3.5x reduction in energy per inference in an FPGA implementation of MNIST-LeNet-5, CIFAR10-VGG-11, and Tiny ImageNet-ResNet",
    "link": "http://arxiv.org/abs/2305.05274",
    "context": "Title: DietCNN: Multiplication-free Inference for Quantized CNNs. (arXiv:2305.05274v1 [cs.CV])\nAbstract: The rising demand for networked embedded systems with machine intelligence has been a catalyst for sustained attempts by the research community to implement Convolutional Neural Networks (CNN) based inferencing on embedded resource-limited devices. Redesigning a CNN by removing costly multiplication operations has already shown promising results in terms of reducing inference energy usage. This paper proposes a new method for replacing multiplications in a CNN by table look-ups. Unlike existing methods that completely modify the CNN operations, the proposed methodology preserves the semantics of the major CNN operations. Conforming to the existing mechanism of the CNN layer operations ensures that the reliability of a standard CNN is preserved. It is shown that the proposed multiplication-free CNN, based on a single activation codebook, can achieve 4.7x, 5.6x, and 3.5x reduction in energy per inference in an FPGA implementation of MNIST-LeNet-5, CIFAR10-VGG-11, and Tiny ImageNet-ResNet",
    "path": "papers/23/05/2305.05274.json",
    "total_tokens": 863,
    "translated_title": "DietCNN:用于量化CNN的无乘积推理",
    "translated_abstract": "网络嵌入式系统与机器智能的不断增长的需求已经成为促进研究界在嵌入式资源有限的设备上实现基于卷积神经网络（CNN）的推断的催化剂。删除昂贵的乘法操作来重新设计CNN已经显示出在减少推断能量使用方面具有有前途的效果。本文提出了一种用查表法代替CNN中乘法的新方法。与完全修改CNN操作的现有方法不同，所提出的方法保留了主要CNN操作的语义。符合CNN层操作的现有机制确保了标准CNN的可靠性。实验证明，基于单个激活码本的无乘积CNN在MNIST-LeNet-5、CIFAR10-VGG-11和Tiny ImageNet-ResNet的FPGA实现中，每次推理能够实现4.7倍、5.6倍和3.5倍的能量降低。",
    "tldr": "本文提出了一种用查表法代替CNN中乘法的新方法，其保留了主要CNN操作的语义，且可在FPGA实现中实现显著的能量降低，具有重要的实用价值。"
}