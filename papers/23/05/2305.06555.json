{
    "title": "Domain Incremental Lifelong Learning in an Open World. (arXiv:2305.06555v1 [cs.CL])",
    "abstract": "Lifelong learning (LL) is an important ability for NLP models to learn new tasks continuously. Architecture-based approaches are reported to be effective implementations for LL models. However, it is non-trivial to extend previous approaches to domain incremental LL scenarios since they either require access to task identities in the testing phase or cannot handle samples from unseen tasks. In this paper, we propose \\textbf{Diana}: a \\underline{d}ynam\\underline{i}c \\underline{a}rchitecture-based lifelo\\underline{n}g le\\underline{a}rning model that tries to learn a sequence of tasks with a prompt-enhanced language model. Four types of hierarchically organized prompts are used in Diana to capture knowledge from different granularities. Specifically, we dedicate task-level prompts to capture task-specific knowledge to retain high LL performances and maintain instance-level prompts to learn knowledge shared across input samples to improve the model's generalization performance. Moreover, w",
    "link": "http://arxiv.org/abs/2305.06555",
    "context": "Title: Domain Incremental Lifelong Learning in an Open World. (arXiv:2305.06555v1 [cs.CL])\nAbstract: Lifelong learning (LL) is an important ability for NLP models to learn new tasks continuously. Architecture-based approaches are reported to be effective implementations for LL models. However, it is non-trivial to extend previous approaches to domain incremental LL scenarios since they either require access to task identities in the testing phase or cannot handle samples from unseen tasks. In this paper, we propose \\textbf{Diana}: a \\underline{d}ynam\\underline{i}c \\underline{a}rchitecture-based lifelo\\underline{n}g le\\underline{a}rning model that tries to learn a sequence of tasks with a prompt-enhanced language model. Four types of hierarchically organized prompts are used in Diana to capture knowledge from different granularities. Specifically, we dedicate task-level prompts to capture task-specific knowledge to retain high LL performances and maintain instance-level prompts to learn knowledge shared across input samples to improve the model's generalization performance. Moreover, w",
    "path": "papers/23/05/2305.06555.json",
    "total_tokens": 828,
    "translated_title": "开放世界下的领域增量生机学习",
    "translated_abstract": "终身学习是NLP模型不断学习新任务的重要能力。基于架构的方法被报道为LL模型的有效实现。然而，将先前的方法扩展到领域增量LL场景并非易事，因为它们要么需要在测试阶段访问任务身份，要么无法处理来自未见任务的样本。在本文中，我们提出Diana：一种基于动态架构的生命周期学习模型，试图通过增强语言模型来学习一系列任务。 Diana使用四种层次化组织的提示来捕获不同粒度的知识。",
    "tldr": "本文提出了Diana模型，一种基于动态架构的生命周期学习模型，它使用四种层次化组织的提示来学习一系列任务。其中，任务级提示用于捕获任务特定的知识，实例级提示用于学习跨输入样本共享的知识，从而提高模型的泛化性能。",
    "en_tdlr": "This paper proposes the Diana model, a dynamic architecture-based lifelong learning model that utilizes four hierarchically organized prompts to learn a sequence of tasks. Task-level prompts are used to capture task-specific knowledge for high LL performance, while instance-level prompts are used to learn knowledge shared across input samples to improve the model's generalization performance."
}