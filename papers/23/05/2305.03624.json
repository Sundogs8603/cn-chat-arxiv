{
    "title": "Retraining A Graph-based Recommender with Interests Disentanglement. (arXiv:2305.03624v1 [cs.IR])",
    "abstract": "In a practical recommender system, new interactions are continuously observed. Some interactions are expected, because they largely follow users' long-term preferences. Some other interactions are indications of recent trends in user preference changes or marketing positions of new items. Accordingly, the recommender needs to be periodically retrained or updated to capture the new trends, and yet not to forget the long-term preferences. In this paper, we propose a novel and generic retraining framework called Disentangled Incremental Learning (DIL) for graph-based recommenders. We assume that long-term preferences are well captured in the existing model, in the form of model parameters learned from past interactions. New preferences can be learned from the user-item bipartite graph constructed using the newly observed interactions. In DIL, we design an Information Extraction Module to extract historical preferences from the existing model. Then we blend the historical and new preferenc",
    "link": "http://arxiv.org/abs/2305.03624",
    "context": "Title: Retraining A Graph-based Recommender with Interests Disentanglement. (arXiv:2305.03624v1 [cs.IR])\nAbstract: In a practical recommender system, new interactions are continuously observed. Some interactions are expected, because they largely follow users' long-term preferences. Some other interactions are indications of recent trends in user preference changes or marketing positions of new items. Accordingly, the recommender needs to be periodically retrained or updated to capture the new trends, and yet not to forget the long-term preferences. In this paper, we propose a novel and generic retraining framework called Disentangled Incremental Learning (DIL) for graph-based recommenders. We assume that long-term preferences are well captured in the existing model, in the form of model parameters learned from past interactions. New preferences can be learned from the user-item bipartite graph constructed using the newly observed interactions. In DIL, we design an Information Extraction Module to extract historical preferences from the existing model. Then we blend the historical and new preferenc",
    "path": "papers/23/05/2305.03624.json",
    "total_tokens": 1036,
    "translated_title": "通过兴趣解耦重新训练基于图的推荐系统",
    "translated_abstract": "在实际的推荐系统中，会不断观察到新的交互。一些交互是预期的，因为它们大部分遵循用户的长期偏好。其他一些交互则表明用户偏好的最新趋势或新物品的营销立场。因此，推荐算法需要周期性地重新训练或更新，以捕捉新的趋势，同时不要忘记长期偏好。本文提出了一种称为Disentangled Incremental Learning（DIL）的新型通用重新训练框架，用于基于图的推荐系统。假设长期偏好已经以学习自过去交互的模型参数的形式在现有模型中得到良好捕捉。新偏好可以通过使用新观察到的交互构建的用户-物品二分图来学习。在Disentangled Incremental Learning（DIL）中，设计了信息提取模块来从现有模型中提取历史偏好。然后，我们通过基于新设计的正则化项的Disentangled Information Embedding（解耦信息嵌入）来混合历史和新偏好。解耦的嵌入可以直接用于推荐或下游任务。我们在三个基准数据集上进行了广泛的实验。实验结果表明，DIL在推荐准确性和效率方面均优于多个最新的方法。",
    "tldr": "本文提出了Disentangled Incremental Learning (DIL)框架，可以通过兴趣解耦的方式重新训练基于图的推荐系统，具有较高的准确性和效率。",
    "en_tdlr": "This paper proposes a Disentangled Incremental Learning (DIL) framework for retraining graph-based recommender systems via interest disentanglement, achieving higher accuracy and efficiency."
}