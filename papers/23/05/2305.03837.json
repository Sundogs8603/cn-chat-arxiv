{
    "title": "Mask The Bias: Improving Domain-Adaptive Generalization of CTC-based ASR with Internal Language Model Estimation. (arXiv:2305.03837v1 [eess.AS])",
    "abstract": "End-to-end ASR models trained on large amount of data tend to be implicitly biased towards language semantics of the training data. Internal language model estimation (ILME) has been proposed to mitigate this bias for autoregressive models such as attention-based encoder-decoder and RNN-T. Typically, ILME is performed by modularizing the acoustic and language components of the model architecture, and eliminating the acoustic input to perform log-linear interpolation with the text-only posterior. However, for CTC-based ASR, it is not as straightforward to decouple the model into such acoustic and language components, as CTC log-posteriors are computed in a non-autoregressive manner. In this work, we propose a novel ILME technique for CTC-based ASR models. Our method iteratively masks the audio timesteps to estimate a pseudo log-likelihood of the internal LM by accumulating log-posteriors for only the masked timesteps. Extensive evaluation across multiple out-of-domain datasets reveals t",
    "link": "http://arxiv.org/abs/2305.03837",
    "context": "Title: Mask The Bias: Improving Domain-Adaptive Generalization of CTC-based ASR with Internal Language Model Estimation. (arXiv:2305.03837v1 [eess.AS])\nAbstract: End-to-end ASR models trained on large amount of data tend to be implicitly biased towards language semantics of the training data. Internal language model estimation (ILME) has been proposed to mitigate this bias for autoregressive models such as attention-based encoder-decoder and RNN-T. Typically, ILME is performed by modularizing the acoustic and language components of the model architecture, and eliminating the acoustic input to perform log-linear interpolation with the text-only posterior. However, for CTC-based ASR, it is not as straightforward to decouple the model into such acoustic and language components, as CTC log-posteriors are computed in a non-autoregressive manner. In this work, we propose a novel ILME technique for CTC-based ASR models. Our method iteratively masks the audio timesteps to estimate a pseudo log-likelihood of the internal LM by accumulating log-posteriors for only the masked timesteps. Extensive evaluation across multiple out-of-domain datasets reveals t",
    "path": "papers/23/05/2305.03837.json",
    "total_tokens": 923,
    "translated_abstract": "在大量数据上训练的端到端ASR模型往往会隐含地偏向于训练数据的语言语义。已经提出了内部语言模型估计（ILME）来减轻自回归模型（如注意力编码器-解码器和RNN-T）的这种偏见。通常，ILME通过将模型架构的声学和语言组件模块化，并消除声学输入来执行仅基于文本的后验概率的对数线性插值来执行。然而，对于基于CTC的ASR来说，将模型解耦为这样的声学和语言组件并不是那么直接，因为CTC对数后验概率是以非自回归的方式计算的。在这项工作中，我们提出了一种新的用于CTC-based ASR模型的ILME技术。我们的方法通过迭代遮盖音频时间步来估计内部LM的伪对数似然，仅累加仅对被遮盖时间步的对数后验概率。在多个领域外数据集上进行广泛评估。",
    "tldr": "本论文介绍了一种新的用于CTC-based ASR模型的内部语言模型估计技术，可以避免模型偏向于训练数据语言语义的问题。",
    "en_tdlr": "This paper proposes a novel internal language model estimation technique for CTC-based ASR models that mitigates the bias towards the language semantics of the training data."
}