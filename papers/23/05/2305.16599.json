{
    "title": "Bridging the Domain Gaps in Context Representations for k-Nearest Neighbor Neural Machine Translation. (arXiv:2305.16599v1 [cs.CL])",
    "abstract": "$k$-Nearest neighbor machine translation ($k$NN-MT) has attracted increasing attention due to its ability to non-parametrically adapt to new translation domains. By using an upstream NMT model to traverse the downstream training corpus, it is equipped with a datastore containing vectorized key-value pairs, which are retrieved during inference to benefit translation. However, there often exists a significant gap between upstream and downstream domains, which hurts the retrieval accuracy and the final translation quality. To deal with this issue, we propose a novel approach to boost the datastore retrieval of $k$NN-MT by reconstructing the original datastore. Concretely, we design a reviser to revise the key representations, making them better fit for the downstream domain. The reviser is trained using the collected semantically-related key-queries pairs, and optimized by two proposed losses: one is the key-queries semantic distance ensuring each revised key representation is semanticall",
    "link": "http://arxiv.org/abs/2305.16599",
    "context": "Title: Bridging the Domain Gaps in Context Representations for k-Nearest Neighbor Neural Machine Translation. (arXiv:2305.16599v1 [cs.CL])\nAbstract: $k$-Nearest neighbor machine translation ($k$NN-MT) has attracted increasing attention due to its ability to non-parametrically adapt to new translation domains. By using an upstream NMT model to traverse the downstream training corpus, it is equipped with a datastore containing vectorized key-value pairs, which are retrieved during inference to benefit translation. However, there often exists a significant gap between upstream and downstream domains, which hurts the retrieval accuracy and the final translation quality. To deal with this issue, we propose a novel approach to boost the datastore retrieval of $k$NN-MT by reconstructing the original datastore. Concretely, we design a reviser to revise the key representations, making them better fit for the downstream domain. The reviser is trained using the collected semantically-related key-queries pairs, and optimized by two proposed losses: one is the key-queries semantic distance ensuring each revised key representation is semanticall",
    "path": "papers/23/05/2305.16599.json",
    "total_tokens": 895,
    "translated_title": "连接上下文表示中的领域差距，用于k最近邻神经机器翻译",
    "translated_abstract": "k最近邻机器翻译(kNN-MT)因其能够非参数地适应新的翻译领域而受到越来越多的关注。通过使用上游NMT模型遍历下游训练语料库，它配备了一个包含向量化键值对的数据存储库，在推理过程中检索这些信息来提高翻译质量。然而，上下游领域之间经常存在显着差距，这会损害检索准确性和最终翻译质量。为了解决这个问题，我们提出了一种新的方法，通过重建原始数据存储库来提高kNN-MT的数据检索。具体而言，我们设计了一个修订器来修订关键表示，使其更适合下游领域。修订器使用收集的语义相关键-查询对进行训练，并通过两个提出的损失进行优化：一个是键-查询语义距离，确保每个修订的键表示在语义上相似。",
    "tldr": "本文提出了一种新的方法，可以通过重建原始数据存储库来提高kNN-MT的向下数据检索，解决了上游和下游领域之间的显着差距问题。"
}