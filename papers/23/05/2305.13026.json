{
    "title": "DUMB: A Benchmark for Smart Evaluation of Dutch Models. (arXiv:2305.13026v2 [cs.CL] UPDATED)",
    "abstract": "We introduce the Dutch Model Benchmark: DUMB. The benchmark includes a diverse set of datasets for low-, medium- and high-resource tasks. The total set of nine tasks includes four tasks that were previously not available in Dutch. Instead of relying on a mean score across tasks, we propose Relative Error Reduction (RER), which compares the DUMB performance of language models to a strong baseline which can be referred to in the future even when assessing different sets of language models. Through a comparison of 14 pre-trained language models (mono- and multi-lingual, of varying sizes), we assess the internal consistency of the benchmark tasks, as well as the factors that likely enable high performance. Our results indicate that current Dutch monolingual models under-perform and suggest training larger Dutch models with other architectures and pre-training objectives. At present, the highest performance is achieved by DeBERTaV3 (large), XLM-R (large) and mDeBERTaV3 (base). In addition t",
    "link": "http://arxiv.org/abs/2305.13026",
    "context": "Title: DUMB: A Benchmark for Smart Evaluation of Dutch Models. (arXiv:2305.13026v2 [cs.CL] UPDATED)\nAbstract: We introduce the Dutch Model Benchmark: DUMB. The benchmark includes a diverse set of datasets for low-, medium- and high-resource tasks. The total set of nine tasks includes four tasks that were previously not available in Dutch. Instead of relying on a mean score across tasks, we propose Relative Error Reduction (RER), which compares the DUMB performance of language models to a strong baseline which can be referred to in the future even when assessing different sets of language models. Through a comparison of 14 pre-trained language models (mono- and multi-lingual, of varying sizes), we assess the internal consistency of the benchmark tasks, as well as the factors that likely enable high performance. Our results indicate that current Dutch monolingual models under-perform and suggest training larger Dutch models with other architectures and pre-training objectives. At present, the highest performance is achieved by DeBERTaV3 (large), XLM-R (large) and mDeBERTaV3 (base). In addition t",
    "path": "papers/23/05/2305.13026.json",
    "total_tokens": 1018,
    "translated_title": "DUMB: 用于智能评估荷兰语模型的基准测试",
    "translated_abstract": "我们引入了荷兰模型基准测试：DUMB。该基准测试包括一组用于低、中和高资源任务的多样化数据集。总共有九个任务，其中四个任务以前在荷兰语中还没有。我们提出了相对误差减少 (RER) 的概念，而不是依赖于任务的均值分数，RER 对比了语言模型在DUMB基准测试中与强基准线的表现，这可以在今后评估不同语言模型集时作为参考。通过比较14个预训练语言模型（单语和多语、不同大小的模型），我们评估了基准测试任务的内部一致性以及可能导致高性能的因素。我们的结果表明目前的荷兰单语模型表现不佳，并建议使用其他架构和预训练目标来训练更大的荷兰模型。目前，DeBERTaV3 (large)、XLM-R (large)和mDeBERTaV3 (base) 实现了最高性能。",
    "tldr": "我们引入了DUMB基准测试，用于智能评估荷兰语模型。通过比较不同大小和类型的预训练语言模型，我们发现目前的荷兰单语模型表现不佳，建议使用其他架构和预训练目标来训练更大的荷兰模型。在该基准测试中，DeBERTaV3 (large)、XLM-R (large)和mDeBERTaV3 (base)取得了最高性能。",
    "en_tdlr": "We introduce DUMB, a benchmark for smart evaluation of Dutch models. By comparing different sizes and types of pre-trained language models, we find that current Dutch monolingual models under-perform and suggest training larger Dutch models with other architectures and pre-training objectives. The highest performance is achieved by DeBERTaV3 (large), XLM-R (large), and mDeBERTaV3 (base) in this benchmark."
}