{
    "title": "Multi-Domain Learning From Insufficient Annotations. (arXiv:2305.02757v1 [cs.LG])",
    "abstract": "Multi-domain learning (MDL) refers to simultaneously constructing a model or a set of models on datasets collected from different domains. Conventional approaches emphasize domain-shared information extraction and domain-private information preservation, following the shared-private framework (SP models), which offers significant advantages over single-domain learning. However, the limited availability of annotated data in each domain considerably hinders the effectiveness of conventional supervised MDL approaches in real-world applications. In this paper, we introduce a novel method called multi-domain contrastive learning (MDCL) to alleviate the impact of insufficient annotations by capturing both semantic and structural information from both labeled and unlabeled data.Specifically, MDCL comprises two modules: inter-domain semantic alignment and intra-domain contrast. The former aims to align annotated instances of the same semantic category from distinct domains within a shared hidd",
    "link": "http://arxiv.org/abs/2305.02757",
    "context": "Title: Multi-Domain Learning From Insufficient Annotations. (arXiv:2305.02757v1 [cs.LG])\nAbstract: Multi-domain learning (MDL) refers to simultaneously constructing a model or a set of models on datasets collected from different domains. Conventional approaches emphasize domain-shared information extraction and domain-private information preservation, following the shared-private framework (SP models), which offers significant advantages over single-domain learning. However, the limited availability of annotated data in each domain considerably hinders the effectiveness of conventional supervised MDL approaches in real-world applications. In this paper, we introduce a novel method called multi-domain contrastive learning (MDCL) to alleviate the impact of insufficient annotations by capturing both semantic and structural information from both labeled and unlabeled data.Specifically, MDCL comprises two modules: inter-domain semantic alignment and intra-domain contrast. The former aims to align annotated instances of the same semantic category from distinct domains within a shared hidd",
    "path": "papers/23/05/2305.02757.json",
    "total_tokens": 1064,
    "translated_title": "不充分标注下的多领域学习",
    "translated_abstract": "多领域学习(MDL)指同时在来自不同领域的数据集上构建一个模型或一组模型。传统方法强调域共享信息的提取和域私有信息的保留，遵循共享-私有架构(SP模型)，这比单领域学习具有明显优势。然而，在每个领域中有限的已注释数据的可用性，严重阻碍了传统监督MDL方法在实际应用中的有效性。本文介绍了一种称为多领域对比学习(MDCL)的新方法，通过捕获来自标记和未标记数据的语义和结构信息，缓解了不充分注释的影响。具体而言，MDCL包括两个模块：域间语义对齐和域内对比。前者旨在将不同领域中相同语义类别的已标注实例在共享的隐空间中对齐，而后者旨在在每个领域内最大化分离来自不同类别的实例。我们在三个多领域学习任务上进行实验，包括图像分类、情感分析和假新闻检测，结果表明我们提出的MDCL方法在各种注释方案下优于现有的最先进MDL方法。",
    "tldr": "提出了一种名为多领域对比学习（MDCL）的新方法，在原有方法的基础上，利用来自标记和未标记数据的语义和结构信息，解决了不充分注释的问题，并在实验中取得了优异的成果。",
    "en_tdlr": "A novel multi-domain learning method called Multi-Domain Contrastive Learning (MDCL) is proposed to alleviate the impact of insufficient annotations by capturing both semantic and structural information from both labeled and unlabeled data. By aligning annotated instances of the same semantic category from distinct domains within a shared hidden space and maximizing the separation of instances from different categories within each domain, MDCL outperforms existing state-of-the-art MDL methods in various annotation scenarios on three multi-domain learning tasks."
}