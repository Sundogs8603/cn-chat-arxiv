{
    "title": "Sources of Hallucination by Large Language Models on Inference Tasks. (arXiv:2305.14552v1 [cs.CL])",
    "abstract": "Large Language Models (LLMs) are claimed to be capable of Natural Language Inference (NLI), necessary for applied tasks like question answering and summarization, yet this capability is under-explored. We present a series of behavioral studies on several LLM families (LLaMA, GPT-3.5, and PaLM) which probe their behavior using controlled experiments. We establish two factors which predict much of their performance, and propose that these are major sources of hallucination in generative LLM. First, the most influential factor is memorization of the training data. We show that models falsely label NLI test samples as entailing when the hypothesis is attested in the training text, regardless of the premise. We further show that named entity IDs are used as \"indices\" to access the memorized data. Second, we show that LLMs exploit a further corpus-based heuristic using the relative frequencies of words. We show that LLMs score significantly worse on NLI test samples which do not conform to t",
    "link": "http://arxiv.org/abs/2305.14552",
    "context": "Title: Sources of Hallucination by Large Language Models on Inference Tasks. (arXiv:2305.14552v1 [cs.CL])\nAbstract: Large Language Models (LLMs) are claimed to be capable of Natural Language Inference (NLI), necessary for applied tasks like question answering and summarization, yet this capability is under-explored. We present a series of behavioral studies on several LLM families (LLaMA, GPT-3.5, and PaLM) which probe their behavior using controlled experiments. We establish two factors which predict much of their performance, and propose that these are major sources of hallucination in generative LLM. First, the most influential factor is memorization of the training data. We show that models falsely label NLI test samples as entailing when the hypothesis is attested in the training text, regardless of the premise. We further show that named entity IDs are used as \"indices\" to access the memorized data. Second, we show that LLMs exploit a further corpus-based heuristic using the relative frequencies of words. We show that LLMs score significantly worse on NLI test samples which do not conform to t",
    "path": "papers/23/05/2305.14552.json",
    "total_tokens": 925,
    "translated_abstract": "大型语言模型（LLMs）被认为能够进行自然语言推理（NLI），是问答和摘要等应用任务所必需的，但这种能力鲜有探究。我们对几个LLM家族（LLaMA、GPT-3.5和PaLM）进行了一系列行为研究，通过对其行为进行控制实验，探讨它们的行为。我们确定了两个预测它们大部分性能的因素，并提出这些是生成LLM中幻觉的主要来源。首先，最具有影响力的因素是对训练数据的记忆。我们表明，当假设在训练文本中被证实时，模型会错误地将NLI测试样本标记为属于该假设，而不考虑前提。我们进一步表明，命名实体ID被用作“索引”来访问记忆数据。其次，我们显示LLMs利用一个基于语料库的启发式方法，使用词的相对频率。我们表明，LLMs在不符合此启发式方法的NLI测试样本上得分明显较差。",
    "tldr": "本文研究了大型语言模型在自然语言推理任务上的表现，确定了记忆训练数据和基于语料库的启发式方法是模型产生幻觉的主要原因。",
    "en_tdlr": "This paper investigates the performance of large language models on natural language inference tasks and identifies memorization of training data and a corpus-based heuristic using word frequencies as major sources of hallucination in generative LLMs."
}