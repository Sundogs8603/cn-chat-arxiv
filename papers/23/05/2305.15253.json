{
    "title": "Rethinking the Evaluation Protocol of Domain Generalization",
    "abstract": "arXiv:2305.15253v2 Announce Type: replace-cross  Abstract: Domain generalization aims to solve the challenge of Out-of-Distribution (OOD) generalization by leveraging common knowledge learned from multiple training domains to generalize to unseen test domains. To accurately evaluate the OOD generalization ability, it is required that test data information is unavailable. However, the current domain generalization protocol may still have potential test data information leakage. This paper examines the risks of test data information leakage from two aspects of the current evaluation protocol: supervised pretraining on ImageNet and oracle model selection. We propose modifications to the current protocol that we should employ self-supervised pretraining or train from scratch instead of employing the current supervised pretraining, and we should use multiple test domains. These would result in a more precise evaluation of OOD generalization ability. We also rerun the algorithms with the mod",
    "link": "https://arxiv.org/abs/2305.15253",
    "context": "Title: Rethinking the Evaluation Protocol of Domain Generalization\nAbstract: arXiv:2305.15253v2 Announce Type: replace-cross  Abstract: Domain generalization aims to solve the challenge of Out-of-Distribution (OOD) generalization by leveraging common knowledge learned from multiple training domains to generalize to unseen test domains. To accurately evaluate the OOD generalization ability, it is required that test data information is unavailable. However, the current domain generalization protocol may still have potential test data information leakage. This paper examines the risks of test data information leakage from two aspects of the current evaluation protocol: supervised pretraining on ImageNet and oracle model selection. We propose modifications to the current protocol that we should employ self-supervised pretraining or train from scratch instead of employing the current supervised pretraining, and we should use multiple test domains. These would result in a more precise evaluation of OOD generalization ability. We also rerun the algorithms with the mod",
    "path": "papers/23/05/2305.15253.json",
    "total_tokens": 827,
    "translated_title": "重新思考领域泛化的评估协议",
    "translated_abstract": "领域泛化目的是通过利用从多个训练领域学到的共同知识，解决面向未见测试领域的超出分布（OOD）泛化挑战。为了准确评估OOD泛化能力，需要测试数据信息不可用。然而，当前的领域泛化协议仍可能存在潜在的测试数据信息泄漏。本文从当前评估协议的两个方面：在ImageNet上进行监督预训练和oracle模型选择，探讨测试数据信息泄漏的风险。我们提出修改当前协议的建议，即应该采用自监督预训练或从头开始训练，而不是采用当前的监督预训练，并且应该使用多个测试领域。这将导致对OOD泛化能力更精确的评估。我们还重新运行了带有修改后协议的算法。",
    "tldr": "重新评估领域泛化的评估协议，提出采用自监督预训练或从头开始训练，使用多个测试领域，以更准确评估OOD泛化能力。",
    "en_tdlr": "Rethink the evaluation protocol of domain generalization by suggesting self-supervised pretraining or training from scratch, using multiple test domains for a more precise evaluation of Out-of-Distribution (OOD) generalization ability."
}