{
    "title": "Momentum Provably Improves Error Feedback!. (arXiv:2305.15155v2 [cs.LG] UPDATED)",
    "abstract": "Due to the high communication overhead when training machine learning models in a distributed environment, modern algorithms invariably rely on lossy communication compression. However, when untreated, the errors caused by compression propagate, and can lead to severely unstable behavior, including exponential divergence. Almost a decade ago, Seide et al [2014] proposed an error feedback (EF) mechanism, which we refer to as EF14, as an immensely effective heuristic for mitigating this issue. However, despite steady algorithmic and theoretical advances in the EF field in the last decade, our understanding is far from complete. In this work we address one of the most pressing issues. In particular, in the canonical nonconvex setting, all known variants of EF rely on very large batch sizes to converge, which can be prohibitive in practice. We propose a surprisingly simple fix which removes this issue both theoretically, and in practice: the application of Polyak's momentum to the latest i",
    "link": "http://arxiv.org/abs/2305.15155",
    "context": "Title: Momentum Provably Improves Error Feedback!. (arXiv:2305.15155v2 [cs.LG] UPDATED)\nAbstract: Due to the high communication overhead when training machine learning models in a distributed environment, modern algorithms invariably rely on lossy communication compression. However, when untreated, the errors caused by compression propagate, and can lead to severely unstable behavior, including exponential divergence. Almost a decade ago, Seide et al [2014] proposed an error feedback (EF) mechanism, which we refer to as EF14, as an immensely effective heuristic for mitigating this issue. However, despite steady algorithmic and theoretical advances in the EF field in the last decade, our understanding is far from complete. In this work we address one of the most pressing issues. In particular, in the canonical nonconvex setting, all known variants of EF rely on very large batch sizes to converge, which can be prohibitive in practice. We propose a surprisingly simple fix which removes this issue both theoretically, and in practice: the application of Polyak's momentum to the latest i",
    "path": "papers/23/05/2305.15155.json",
    "total_tokens": 880,
    "translated_title": "动量被证明可以改善误差反馈！",
    "translated_abstract": "由于在分布式环境中训练机器学习模型时存在较高的通信开销，现代算法总是依赖于有损压缩通信。然而，如果不加处理，压缩引起的误差会传播，并且可能导致严重的不稳定行为，包括指数级发散。大约十年前，Seide等人[2014]提出了一种称为EF14的错误反馈（EF）机制，作为缓解这个问题的极其有效的启发式方法。然而，尽管在过去十年中，EF领域在算法和理论方面有了稳定的进展，但我们对问题的理解还远未完善。在本文中，我们解决了其中一个最紧迫的问题。特别地，在经典的非凸设置中，所有已知的EF变种都依赖于非常大的批次大小才能收敛，这在实践中可能是禁止的。我们提出了一个令人惊讶地简单的解决方案，从理论和实践上消除了这个问题：将Polyak的动量应用到最新的i",
    "tldr": "这项研究证明动量可以改善机器学习中的误差反馈问题，并提出了一个简单的解决方案，解决了批次大小过大的问题。",
    "en_tdlr": "This study demonstrates that momentum can improve error feedback in machine learning and proposes a simple solution to the problem of large batch sizes."
}