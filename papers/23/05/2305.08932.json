{
    "title": "MIMEx: Intrinsic Rewards from Masked Input Modeling. (arXiv:2305.08932v1 [cs.LG])",
    "abstract": "Exploring in environments with high-dimensional observations is hard. One promising approach for exploration is to use intrinsic rewards, which often boils down to estimating \"novelty\" of states, transitions, or trajectories with deep networks. Prior works have shown that conditional prediction objectives such as masked autoencoding can be seen as stochastic estimation of pseudo-likelihood. We show how this perspective naturally leads to a unified view on existing intrinsic reward approaches: they are special cases of conditional prediction, where the estimation of novelty can be seen as pseudo-likelihood estimation with different mask distributions. From this view, we propose a general framework for deriving intrinsic rewards -- Masked Input Modeling for Exploration (MIMEx) -- where the mask distribution can be flexibly tuned to control the difficulty of the underlying conditional prediction task. We demonstrate that MIMEx can achieve superior results when compared against competitive",
    "link": "http://arxiv.org/abs/2305.08932",
    "context": "Title: MIMEx: Intrinsic Rewards from Masked Input Modeling. (arXiv:2305.08932v1 [cs.LG])\nAbstract: Exploring in environments with high-dimensional observations is hard. One promising approach for exploration is to use intrinsic rewards, which often boils down to estimating \"novelty\" of states, transitions, or trajectories with deep networks. Prior works have shown that conditional prediction objectives such as masked autoencoding can be seen as stochastic estimation of pseudo-likelihood. We show how this perspective naturally leads to a unified view on existing intrinsic reward approaches: they are special cases of conditional prediction, where the estimation of novelty can be seen as pseudo-likelihood estimation with different mask distributions. From this view, we propose a general framework for deriving intrinsic rewards -- Masked Input Modeling for Exploration (MIMEx) -- where the mask distribution can be flexibly tuned to control the difficulty of the underlying conditional prediction task. We demonstrate that MIMEx can achieve superior results when compared against competitive",
    "path": "papers/23/05/2305.08932.json",
    "total_tokens": 846,
    "translated_title": "MIMEx: 遮盖输入建模的内在奖励",
    "translated_abstract": "在高维观测环境中进行探索很困难。使用内在奖励的一种有前途的方法是使用深度网络估计状态、转换或轨迹的“新颖性”。之前的研究表明，条件预测目标，如遮盖自动编码可看作伪似然的随机估计。我们展示了这一观点如何自然地导致对现有内在奖励方法的统一看法:它们是条件预测的特例，在这种情况下，新颖性的估计可以看作是使用不同的遮盖分布进行伪似然估计。从这个角度，我们提出了一个通用的框架——遮盖输入建模探索内在奖励(MIMEx)，其中遮盖分布可以灵活调整以控制底层条件预测任务的难度。我们演示了当与竞争方法相比时，MIMEx可以取得优越的结果。",
    "tldr": "MIMEx是一个通用的框架，它使用遮盖输入建模来提取内在奖励，通过控制遮盖分布来控制难度，可以在高维环境中取得优越的探索结果。",
    "en_tdlr": "MIMEx is a general framework for deriving intrinsic rewards, which uses masked input modeling to control difficulty, and can achieve superior exploration results in high-dimensional environments."
}