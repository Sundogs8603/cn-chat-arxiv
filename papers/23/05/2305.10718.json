{
    "title": "Discounted Thompson Sampling for Non-Stationary Bandit Problems. (arXiv:2305.10718v1 [cs.LG])",
    "abstract": "Non-stationary multi-armed bandit (NS-MAB) problems have recently received significant attention. NS-MAB are typically modelled in two scenarios: abruptly changing, where reward distributions remain constant for a certain period and change at unknown time steps, and smoothly changing, where reward distributions evolve smoothly based on unknown dynamics. In this paper, we propose Discounted Thompson Sampling (DS-TS) with Gaussian priors to address both non-stationary settings. Our algorithm passively adapts to changes by incorporating a discounted factor into Thompson Sampling. DS-TS method has been experimentally validated, but analysis of the regret upper bound is currently lacking. Under mild assumptions, we show that DS-TS with Gaussian priors can achieve nearly optimal regret bound on the order of $\\tilde{O}(\\sqrt{TB_T})$ for abruptly changing and $\\tilde{O}(T^{\\beta})$ for smoothly changing, where $T$ is the number of time steps, $B_T$ is the number of breakpoints, $\\beta$ is asso",
    "link": "http://arxiv.org/abs/2305.10718",
    "context": "Title: Discounted Thompson Sampling for Non-Stationary Bandit Problems. (arXiv:2305.10718v1 [cs.LG])\nAbstract: Non-stationary multi-armed bandit (NS-MAB) problems have recently received significant attention. NS-MAB are typically modelled in two scenarios: abruptly changing, where reward distributions remain constant for a certain period and change at unknown time steps, and smoothly changing, where reward distributions evolve smoothly based on unknown dynamics. In this paper, we propose Discounted Thompson Sampling (DS-TS) with Gaussian priors to address both non-stationary settings. Our algorithm passively adapts to changes by incorporating a discounted factor into Thompson Sampling. DS-TS method has been experimentally validated, but analysis of the regret upper bound is currently lacking. Under mild assumptions, we show that DS-TS with Gaussian priors can achieve nearly optimal regret bound on the order of $\\tilde{O}(\\sqrt{TB_T})$ for abruptly changing and $\\tilde{O}(T^{\\beta})$ for smoothly changing, where $T$ is the number of time steps, $B_T$ is the number of breakpoints, $\\beta$ is asso",
    "path": "papers/23/05/2305.10718.json",
    "total_tokens": 1105,
    "translated_title": "针对非稳态赌博机问题的折扣汤普森抽样算法",
    "translated_abstract": "近年来，非稳态多臂赌博机问题受到了显著关注。NS-MAB通常在两种情况下进行建模：突然性变化和平滑性变化。在本文中，我们提出了带有高斯先验的折扣汤普森采样算法（DS-TS）以解决这两个非稳态设置。我们的算法通过将折扣因子纳入汤普森采样来被动适应变化。DS-TS方法经过实验验证，但缺乏对遗憾上限的分析。在温和的假设下，我们证明了带有高斯先验的DS-TS可以在突然性变化的情况下实现近乎最优的遗憾上限（$\\tilde{O} (\\sqrt {TB_T})$），在平滑性变化的情况下实现 $\\tilde{O}(T^{\\beta})$ 的近乎最优遗憾上限，其中 $T$ 是时间步数，$B_T$ 是断点数，$\\beta$ 与收益分布的平滑性有关，$\\tilde{O}$ 是对数遗憾上限。",
    "tldr": "该论文提出了一种针对非稳态多臂赌博机问题的折扣汤普森抽样算法（DS-TS），可以解决突然性变化和平滑性变化的问题，并且在两种情况下具有近乎最优的遗憾上限。",
    "en_tdlr": "This paper proposes a Discounted Thompson Sampling (DS-TS) algorithm with Gaussian priors for non-stationary multi-armed bandit (NS-MAB) problems that addresses both abruptly changing and smoothly changing scenarios. The algorithm passively adapts to changes and achieves nearly optimal regret bounds under mild assumptions, with $\\tilde{O}(\\sqrt{TB_T})$ for abruptly changing and $\\tilde{O}(T^{\\beta})$ for smoothly changing, where $T$ is the number of time steps, $B_T$ is the number of breakpoints, $\\beta$ is associated with the smoothness of the reward distribution, and $\\tilde{O}$ is the logarithmic regret upper bound."
}