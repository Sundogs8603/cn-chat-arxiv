{
    "title": "Curriculum Learning in Job Shop Scheduling using Reinforcement Learning. (arXiv:2305.10192v1 [cs.AI])",
    "abstract": "Solving job shop scheduling problems (JSSPs) with a fixed strategy, such as a priority dispatching rule, may yield satisfactory results for several problem instances but, nevertheless, insufficient results for others. From this single-strategy perspective finding a near optimal solution to a specific JSSP varies in difficulty even if the machine setup remains the same. A recent intensively researched and promising method to deal with difficulty variability is Deep Reinforcement Learning (DRL), which dynamically adjusts an agent's planning strategy in response to difficult instances not only during training, but also when applied to new situations. In this paper, we further improve DLR as an underlying method by actively incorporating the variability of difficulty within the same problem size into the design of the learning process. We base our approach on a state-of-the-art methodology that solves JSSP by means of DRL and graph neural network embeddings. Our work supplements the traini",
    "link": "http://arxiv.org/abs/2305.10192",
    "context": "Title: Curriculum Learning in Job Shop Scheduling using Reinforcement Learning. (arXiv:2305.10192v1 [cs.AI])\nAbstract: Solving job shop scheduling problems (JSSPs) with a fixed strategy, such as a priority dispatching rule, may yield satisfactory results for several problem instances but, nevertheless, insufficient results for others. From this single-strategy perspective finding a near optimal solution to a specific JSSP varies in difficulty even if the machine setup remains the same. A recent intensively researched and promising method to deal with difficulty variability is Deep Reinforcement Learning (DRL), which dynamically adjusts an agent's planning strategy in response to difficult instances not only during training, but also when applied to new situations. In this paper, we further improve DLR as an underlying method by actively incorporating the variability of difficulty within the same problem size into the design of the learning process. We base our approach on a state-of-the-art methodology that solves JSSP by means of DRL and graph neural network embeddings. Our work supplements the traini",
    "path": "papers/23/05/2305.10192.json",
    "total_tokens": 860,
    "translated_abstract": "使用固定策略，如优先级调度规则来解决工作车间调度问题（JSSPs）可能会为几个问题实例产生令人满意的结果，但对其他问题实例来说结果不足。最近，一种解决难度可变性问题的有前途的方法是深度强化学习（DRL），它可以动态调整代理的计划策略来应对困难情况，不仅在训练期间，而且在应用到新情况时也是如此。在这篇论文中，我们进一步改进了DRL作为基础方法，通过在学习过程的设计中积极地将困难度变化性纳入相同问题的规模中。我们的方法基于一种通过DRL和图神经网络嵌入来解决JSSP的最新方法。我们的工作补充了培训。",
    "tldr": "本文研究了一种使用强化学习的课程学习方法来解决工作车间调度问题，通过积极将困难度变化性纳入学习过程的设计中，以增强该方法的效果。",
    "en_tdlr": "This paper explores a curriculum learning approach using reinforcement learning to solve job shop scheduling problems. By actively incorporating the variability of difficulty within the same problem size into the learning process, the proposed method improves upon previous techniques based on deep reinforcement learning and graph neural network embeddings."
}