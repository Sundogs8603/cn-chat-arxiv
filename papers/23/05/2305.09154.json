{
    "title": "Progressive Translation: Improving Domain Robustness of Neural Machine Translation with Intermediate Sequences. (arXiv:2305.09154v1 [cs.CL])",
    "abstract": "Previous studies show that intermediate supervision signals benefit various Natural Language Processing tasks. However, it is not clear whether there exist intermediate signals that benefit Neural Machine Translation (NMT). Borrowing techniques from Statistical Machine Translation, we propose intermediate signals which are intermediate sequences from the \"source-like\" structure to the \"target-like\" structure. Such intermediate sequences introduce an inductive bias that reflects a domain-agnostic principle of translation, which reduces spurious correlations that are harmful to out-of-domain generalisation. Furthermore, we introduce a full-permutation multi-task learning to alleviate the spurious causal relations from intermediate sequences to the target, which results from exposure bias. The Minimum Bayes Risk decoding algorithm is used to pick the best candidate translation from all permutations to further improve the performance. Experiments show that the introduced intermediate signa",
    "link": "http://arxiv.org/abs/2305.09154",
    "context": "Title: Progressive Translation: Improving Domain Robustness of Neural Machine Translation with Intermediate Sequences. (arXiv:2305.09154v1 [cs.CL])\nAbstract: Previous studies show that intermediate supervision signals benefit various Natural Language Processing tasks. However, it is not clear whether there exist intermediate signals that benefit Neural Machine Translation (NMT). Borrowing techniques from Statistical Machine Translation, we propose intermediate signals which are intermediate sequences from the \"source-like\" structure to the \"target-like\" structure. Such intermediate sequences introduce an inductive bias that reflects a domain-agnostic principle of translation, which reduces spurious correlations that are harmful to out-of-domain generalisation. Furthermore, we introduce a full-permutation multi-task learning to alleviate the spurious causal relations from intermediate sequences to the target, which results from exposure bias. The Minimum Bayes Risk decoding algorithm is used to pick the best candidate translation from all permutations to further improve the performance. Experiments show that the introduced intermediate signa",
    "path": "papers/23/05/2305.09154.json",
    "total_tokens": 971,
    "translated_title": "迭代翻译：使用中间序列来提高神经机器翻译的领域鲁棒性",
    "translated_abstract": "先前的研究表明，中间监督信号有助于各种自然语言处理任务。然而，目前尚不清楚是否存在有助于神经机器翻译（NMT）的中间信号。借鉴统计机器翻译技术，我们提出了中间信号，即从“源语言”结构到“目标语言”结构的中间序列。这种中间序列引入了一个归纳偏置，反映了一种领域无关的翻译原则，可以减少有害于跨领域泛化的虚假相关性。此外，我们引入了全排列多任务学习，以减轻由暴露偏差引起的中间序列对目标的虚假因果关系。使用最小贝叶斯风险译码算法从所有排列中选择最佳候选翻译进一步提高性能。实验表明，引入的中间信号显著提高了NMT模型跨不同领域的鲁棒性，并减小了内部和外部翻译之间的性能差距。",
    "tldr": "该论文提出了使用中间序列来提高神经机器翻译的领域鲁棒性的方法，并通过全排列多任务学习和最小贝叶斯风险译码算法进一步提高了模型性能。",
    "en_tdlr": "This paper proposes a method to improve the domain robustness of Neural Machine Translation by using intermediate sequences and introduces full-permutation multi-task learning and Minimum Bayes Risk decoding algorithm to further improve the performance. The introduced approach significantly reduces the performance gap between in-domain and out-of-domain translations."
}