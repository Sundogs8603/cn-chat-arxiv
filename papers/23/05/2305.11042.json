{
    "title": "A unified framework for information-theoretic generalization bounds. (arXiv:2305.11042v1 [cs.LG])",
    "abstract": "This paper presents a general methodology for deriving information-theoretic generalization bounds for learning algorithms. The main technical tool is a probabilistic decorrelation lemma based on a change of measure and a relaxation of Young's inequality in $L_{\\psi_p}$ Orlicz spaces. Using the decorrelation lemma in combination with other techniques, such as symmetrization, couplings, and chaining in the space of probability measures, we obtain new upper bounds on the generalization error, both in expectation and in high probability, and recover as special cases many of the existing generalization bounds, including the ones based on mutual information, conditional mutual information, stochastic chaining, and PAC-Bayes inequalities. In addition, the Fernique-Talagrand upper bound on the expected supremum of a subgaussian process emerges as a special case.",
    "link": "http://arxiv.org/abs/2305.11042",
    "context": "Title: A unified framework for information-theoretic generalization bounds. (arXiv:2305.11042v1 [cs.LG])\nAbstract: This paper presents a general methodology for deriving information-theoretic generalization bounds for learning algorithms. The main technical tool is a probabilistic decorrelation lemma based on a change of measure and a relaxation of Young's inequality in $L_{\\psi_p}$ Orlicz spaces. Using the decorrelation lemma in combination with other techniques, such as symmetrization, couplings, and chaining in the space of probability measures, we obtain new upper bounds on the generalization error, both in expectation and in high probability, and recover as special cases many of the existing generalization bounds, including the ones based on mutual information, conditional mutual information, stochastic chaining, and PAC-Bayes inequalities. In addition, the Fernique-Talagrand upper bound on the expected supremum of a subgaussian process emerges as a special case.",
    "path": "papers/23/05/2305.11042.json",
    "total_tokens": 873,
    "translated_title": "一种信息论通用泛化界统一框架",
    "translated_abstract": "本文提出了一种通用的方法来导出学习算法的信息论泛化界。主要的技术工具是基于改变测度和松弛Young不等式在$L_{\\psi_p}$Orlicz空间中的概率去相关性引理。采用去相关性引理与其他技术，如对称化、耦合和概率测度空间中的chaining，我们得到了新的泛化误差上限，包括期望和高概率，同时，我们也恢复了许多现有的泛化界，包括基于互信息、条件互信息、随机chaining和PAC-Bayes不等式的界。此外，Fernique-Talagrand上界也作为一种特殊情况呈现出来。",
    "tldr": "该论文提出了一种基于概率去相关引理和概率测度空间中一些其他技术的通用方法，可以得到新的学习算法的信息论泛化上限，并且能够恢复许多现有的泛化界，如基于互信息、条件互信息、随机chaining和PAC-Bayes不等式的界。",
    "en_tdlr": "This paper proposes a unified methodology for deriving information-theoretic generalization bounds for learning algorithms by introducing a probabilistic decorrelation lemma and combining it with other techniques such as symmetrization, couplings, and chaining in the space of probability measures. New upper bounds on the generalization error are obtained, including the ones based on mutual information, conditional mutual information, stochastic chaining, and PAC-Bayes inequalities. The Fernique-Talagrand upper bound is recovered as a special case."
}