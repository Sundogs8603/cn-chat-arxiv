{
    "title": "XuanYuan 2.0: A Large Chinese Financial Chat Model with Hundreds of Billions Parameters. (arXiv:2305.12002v1 [cs.CL])",
    "abstract": "In recent years, pre-trained language models have undergone rapid development with the emergence of large-scale models. However, there is a lack of open-sourced chat models specifically designed for the Chinese language, especially in the field of Chinese finance, at the scale of hundreds of billions. To address this gap, we introduce XuanYuan 2.0, the largest Chinese chat model to date, built upon the BLOOM-176B architecture. Additionally, we propose a novel training method called hybrid-tuning to mitigate catastrophic forgetting. By combining general-domain with domain-specific knowledge and integrating the stages of pre-training and fine-tuning, XuanYuan 2.0 is capable of providing accurate and contextually appropriate responses in the Chinese financial domain.",
    "link": "http://arxiv.org/abs/2305.12002",
    "context": "Title: XuanYuan 2.0: A Large Chinese Financial Chat Model with Hundreds of Billions Parameters. (arXiv:2305.12002v1 [cs.CL])\nAbstract: In recent years, pre-trained language models have undergone rapid development with the emergence of large-scale models. However, there is a lack of open-sourced chat models specifically designed for the Chinese language, especially in the field of Chinese finance, at the scale of hundreds of billions. To address this gap, we introduce XuanYuan 2.0, the largest Chinese chat model to date, built upon the BLOOM-176B architecture. Additionally, we propose a novel training method called hybrid-tuning to mitigate catastrophic forgetting. By combining general-domain with domain-specific knowledge and integrating the stages of pre-training and fine-tuning, XuanYuan 2.0 is capable of providing accurate and contextually appropriate responses in the Chinese financial domain.",
    "path": "papers/23/05/2305.12002.json",
    "total_tokens": 840,
    "translated_title": "XuanYuan 2.0：一个具有数百亿参数的大型中文金融聊天模型",
    "translated_abstract": "近年来，随着大规模模型的出现，预训练语言模型经历了快速发展。然而，专门针对中文语言的开源聊天模型在规模上仍存在缺乏，尤其是在中文金融领域。为了填补这一空白，我们介绍了 XuanYuan 2.0，它是目前最大的中文聊天模型，采用了 BLOOM-176B 架构。此外，我们提出了一种新的训练方法，称为混合微调，以减轻灾难性遗忘。通过将通用领域与特定领域的知识相结合，并集成预训练与微调阶段，XuanYuan 2.0 能够在中文金融领域提供准确和上下文适当的回答。",
    "tldr": "XuanYuan 2.0是目前最大的中文聊天模型，采用了BLOOM-176B架构，并提出了混合微调训练方法，能够在中文金融领域提供准确和上下文适当的回答。",
    "en_tdlr": "XuanYuan 2.0 is the largest Chinese chat model to date, built upon the BLOOM-176B architecture and using a novel hybrid-tuning method to provide accurate and contextually appropriate responses in the Chinese financial domain."
}