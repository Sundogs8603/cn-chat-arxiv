{
    "title": "Adaptive Federated Pruning in Hierarchical Wireless Networks. (arXiv:2305.09042v1 [cs.LG])",
    "abstract": "Federated Learning (FL) is a promising privacy-preserving distributed learning framework where a server aggregates models updated by multiple devices without accessing their private datasets. Hierarchical FL (HFL), as a device-edge-cloud aggregation hierarchy, can enjoy both the cloud server's access to more datasets and the edge servers' efficient communications with devices. However, the learning latency increases with the HFL network scale due to the increasing number of edge servers and devices with limited local computation capability and communication bandwidth. To address this issue, in this paper, we introduce model pruning for HFL in wireless networks to reduce the neural network scale. We present the convergence analysis of an upper on the l2 norm of gradients for HFL with model pruning, analyze the computation and communication latency of the proposed model pruning scheme, and formulate an optimization problem to maximize the convergence rate under a given latency threshold ",
    "link": "http://arxiv.org/abs/2305.09042",
    "context": "Title: Adaptive Federated Pruning in Hierarchical Wireless Networks. (arXiv:2305.09042v1 [cs.LG])\nAbstract: Federated Learning (FL) is a promising privacy-preserving distributed learning framework where a server aggregates models updated by multiple devices without accessing their private datasets. Hierarchical FL (HFL), as a device-edge-cloud aggregation hierarchy, can enjoy both the cloud server's access to more datasets and the edge servers' efficient communications with devices. However, the learning latency increases with the HFL network scale due to the increasing number of edge servers and devices with limited local computation capability and communication bandwidth. To address this issue, in this paper, we introduce model pruning for HFL in wireless networks to reduce the neural network scale. We present the convergence analysis of an upper on the l2 norm of gradients for HFL with model pruning, analyze the computation and communication latency of the proposed model pruning scheme, and formulate an optimization problem to maximize the convergence rate under a given latency threshold ",
    "path": "papers/23/05/2305.09042.json",
    "total_tokens": 873,
    "translated_title": "分层无线网络中的自适应联邦剪枝",
    "translated_abstract": "联邦学习是一种有前途的隐私保护分布式学习框架，其中服务器汇总了由多个设备更新的模型，而不会访问它们的私有数据集。分层联邦学习作为设备-边缘-云聚合层次结构，既可以享受云服务器对更多数据集的访问权，又可以享受边缘服务器与设备之间的有效通信。然而，由于边缘服务器和设备数量增加，本地计算能力和通信带宽有限，HFL网络规模的增加会导致学习延迟增加。为了解决这个问题，本文介绍了在无线网络中使用模型剪枝来降低神经网络规模。我们介绍了基于模型剪枝的HFL梯度l2范数上限的收敛分析，分析了所提出的模型剪枝方案的计算和通信延迟，并制定了一个优化问题，以在给定的延迟阈值下最大化收敛速率。",
    "tldr": "本文提出了一种针对分层联邦学习的自适应模型剪枝方法，来降低神经网络规模并解决由于设备增多导致的学习延迟问题。",
    "en_tdlr": "This paper proposes an adaptive model pruning method for hierarchical federated learning to reduce neural network size and solve the learning latency issue caused by an increasing number of devices."
}