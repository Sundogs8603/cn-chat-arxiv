{
    "title": "A comprehensive theoretical framework for the optimization of neural networks classification performance with respect to weighted metrics. (arXiv:2305.13472v1 [cs.LG])",
    "abstract": "In many contexts, customized and weighted classification scores are designed in order to evaluate the goodness of the predictions carried out by neural networks. However, there exists a discrepancy between the maximization of such scores and the minimization of the loss function in the training phase. In this paper, we provide a complete theoretical setting that formalizes weighted classification metrics and then allows the construction of losses that drive the model to optimize these metrics of interest. After a detailed theoretical analysis, we show that our framework includes as particular instances well-established approaches such as classical cost-sensitive learning, weighted cross entropy loss functions and value-weighted skill scores.",
    "link": "http://arxiv.org/abs/2305.13472",
    "context": "Title: A comprehensive theoretical framework for the optimization of neural networks classification performance with respect to weighted metrics. (arXiv:2305.13472v1 [cs.LG])\nAbstract: In many contexts, customized and weighted classification scores are designed in order to evaluate the goodness of the predictions carried out by neural networks. However, there exists a discrepancy between the maximization of such scores and the minimization of the loss function in the training phase. In this paper, we provide a complete theoretical setting that formalizes weighted classification metrics and then allows the construction of losses that drive the model to optimize these metrics of interest. After a detailed theoretical analysis, we show that our framework includes as particular instances well-established approaches such as classical cost-sensitive learning, weighted cross entropy loss functions and value-weighted skill scores.",
    "path": "papers/23/05/2305.13472.json",
    "total_tokens": 721,
    "translated_title": "关于神经网络分类性能优化基于加权度量的综合理论框架",
    "translated_abstract": "在许多情况下，为了评估神经网络所做出的预测的准确程度，需要设计定制化和加权分类评分方法。然而，在训练阶段中最大化这些评分与最小化损失函数之间存在差异。本文提出了一个完整的理论框架，形式化了加权分类度量，并允许构建损失函数以驱使模型优化这些有趣的指标。经过详细的理论分析，我们发现我们的框架包括经典的成本敏感学习、加权交叉熵损失函数和值加权技能得分等已确立的方法。",
    "tldr": "本论文提出了一个理论框架，可以驱使模型优化加权分类度量标准，包括成本敏感学习、加权交叉熵损失函数和值加权技能得分等已确立的方法。",
    "en_tdlr": "This paper presents a theoretical framework that drives the model to optimize weighted classification metrics, including established methods such as cost-sensitive learning, weighted cross entropy loss functions, and value-weighted skill scores."
}