{
    "title": "Enhancing Black-Box Few-Shot Text Classification with Prompt-Based Data Augmentation. (arXiv:2305.13785v1 [cs.CL])",
    "abstract": "Training or finetuning large-scale language models (LLMs) such as GPT-3 requires substantial computation resources, motivating recent efforts to explore parameter-efficient adaptation to downstream tasks. One practical area of research is to treat these models as black boxes and interact with them through their inference APIs. In this paper, we investigate how to optimize few-shot text classification without accessing the gradients of the LLMs. To achieve this, we treat the black-box model as a feature extractor and train a classifier with the augmented text data. Data augmentation is performed using prompt-based finetuning on an auxiliary language model with a much smaller parameter size than the black-box model. Through extensive experiments on eight text classification datasets, we show that our approach, dubbed BT-Classifier, significantly outperforms state-of-the-art black-box few-shot learners and performs on par with methods that rely on full-model tuning.",
    "link": "http://arxiv.org/abs/2305.13785",
    "context": "Title: Enhancing Black-Box Few-Shot Text Classification with Prompt-Based Data Augmentation. (arXiv:2305.13785v1 [cs.CL])\nAbstract: Training or finetuning large-scale language models (LLMs) such as GPT-3 requires substantial computation resources, motivating recent efforts to explore parameter-efficient adaptation to downstream tasks. One practical area of research is to treat these models as black boxes and interact with them through their inference APIs. In this paper, we investigate how to optimize few-shot text classification without accessing the gradients of the LLMs. To achieve this, we treat the black-box model as a feature extractor and train a classifier with the augmented text data. Data augmentation is performed using prompt-based finetuning on an auxiliary language model with a much smaller parameter size than the black-box model. Through extensive experiments on eight text classification datasets, we show that our approach, dubbed BT-Classifier, significantly outperforms state-of-the-art black-box few-shot learners and performs on par with methods that rely on full-model tuning.",
    "path": "papers/23/05/2305.13785.json",
    "total_tokens": 805,
    "translated_title": "基于提示的数据增强提升黑盒少样本文本分类",
    "translated_abstract": "训练或微调大规模语言模型如 GPT-3 需要大量计算资源，这推动了最近探索参数高效适应下游任务的努力。这篇论文研究了如何优化少样本文本分类，而无需访问 LLM 的梯度。为了实现这一点，我们将黑盒模型视为特征提取器，并使用增强的文本数据训练分类器。数据增强是通过在一个比黑盒模型参数规模小得多的辅助语言模型上进行基于提示的微调来完成的。通过对八个文本分类数据集的广泛实验，我们展示了我们的方法（称为 BT-Classifier）显著优于最先进的黑盒少样本学习器，并与依赖于全模型调整的方法表现相当。",
    "tldr": "本论文提出一种基于提示的数据增强方法，通过在辅助语言模型上进行微调来实现，从而提高了黑盒少样本文本分类的性能。",
    "en_tdlr": "This paper proposes a prompt-based data augmentation method for enhancing black-box few-shot text classification, which significantly outperforms state-of-the-art black-box few-shot learners and performs on par with methods that rely on full-model tuning."
}