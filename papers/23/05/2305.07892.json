{
    "title": "DAC-MR: Data Augmentation Consistency Based Meta-Regularization for Meta-Learning. (arXiv:2305.07892v1 [cs.LG])",
    "abstract": "Meta learning recently has been heavily researched and helped advance the contemporary machine learning. However, achieving well-performing meta-learning model requires a large amount of training tasks with high-quality meta-data representing the underlying task generalization goal, which is sometimes difficult and expensive to obtain for real applications. Current meta-data-driven meta-learning approaches, however, are fairly hard to train satisfactory meta-models with imperfect training tasks. To address this issue, we suggest a meta-knowledge informed meta-learning (MKIML) framework to improve meta-learning by additionally integrating compensated meta-knowledge into meta-learning process. We preliminarily integrate meta-knowledge into meta-objective via using an appropriate meta-regularization (MR) objective to regularize capacity complexity of the meta-model function class to facilitate better generalization on unseen tasks. As a practical implementation, we introduce data augmenta",
    "link": "http://arxiv.org/abs/2305.07892",
    "context": "Title: DAC-MR: Data Augmentation Consistency Based Meta-Regularization for Meta-Learning. (arXiv:2305.07892v1 [cs.LG])\nAbstract: Meta learning recently has been heavily researched and helped advance the contemporary machine learning. However, achieving well-performing meta-learning model requires a large amount of training tasks with high-quality meta-data representing the underlying task generalization goal, which is sometimes difficult and expensive to obtain for real applications. Current meta-data-driven meta-learning approaches, however, are fairly hard to train satisfactory meta-models with imperfect training tasks. To address this issue, we suggest a meta-knowledge informed meta-learning (MKIML) framework to improve meta-learning by additionally integrating compensated meta-knowledge into meta-learning process. We preliminarily integrate meta-knowledge into meta-objective via using an appropriate meta-regularization (MR) objective to regularize capacity complexity of the meta-model function class to facilitate better generalization on unseen tasks. As a practical implementation, we introduce data augmenta",
    "path": "papers/23/05/2305.07892.json",
    "total_tokens": 929,
    "translated_title": "DAC-MR: 基于数据增强一致性的元学习元正则化",
    "translated_abstract": "近年来，元学习在机器学习领域内备受关注并推动了现代机器学习的发展。然而，要实现表现良好的元学习模型需要大量具有高质量元数据的训练任务，以表示底层任务泛化目标，有时对于真实应用而言难以获得。当前基于元数据的元学习方法，难以使用不完美的训练任务训练令人满意的元模型。为了解决这个问题，我们提出了一个元知识信息增强的元学习框架（MKIML），通过将补偿的元知识集成到元学习过程中，全面提高元学习的性能。我们通过使用适当的元正则化（MR）目标将元知识初步集成到元目标中，来规范元模型函数类的容量复杂度，有助于在未知任务上实现更好的泛化性能。作为一种实用化实现，我们提出数据增强。",
    "tldr": "为了提高元学习的性能，我们提出了一个基于元知识信息增强的元学习框架。我们通过使用适当的MR目标将元知识初步集成到元目标中，来规范元模型函数类的容量复杂度，有助于在未知任务上实现更好的泛化性能。",
    "en_tdlr": "We propose a meta-knowledge informed meta-learning framework to enhance performance in meta-learning by integrating compensated meta-knowledge into the process. By using an appropriate MR objective to regulate the capacity complexity of the meta-model function class, we aim to improve generalization on unseen tasks. Data augmentation is introduced as a practical implementation."
}