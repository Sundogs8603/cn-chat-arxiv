{
    "title": "Unsupervised ASR via Cross-Lingual Pseudo-Labeling. (arXiv:2305.13330v1 [eess.AS])",
    "abstract": "Recent work has shown that it is possible to train an $\\textit{unsupervised}$ automatic speech recognition (ASR) system using only unpaired audio and text. Existing unsupervised ASR methods assume that no labeled data can be used for training. We argue that even if one does not have any labeled audio for a given language, there is $\\textit{always}$ labeled data available for other languages. We show that it is possible to use character-level acoustic models (AMs) from other languages to bootstrap an $\\textit{unsupervised}$ AM in a new language. Here, \"unsupervised\" means no labeled audio is available for the $\\textit{target}$ language. Our approach is based on two key ingredients: (i) generating pseudo-labels (PLs) of the $\\textit{target}$ language using some $\\textit{other}$ language AM and (ii) constraining these PLs with a $\\textit{target language model}$. Our approach is effective on Common Voice: e.g. transfer of English AM to Swahili achieves 18% WER. It also outperforms characte",
    "link": "http://arxiv.org/abs/2305.13330",
    "context": "Title: Unsupervised ASR via Cross-Lingual Pseudo-Labeling. (arXiv:2305.13330v1 [eess.AS])\nAbstract: Recent work has shown that it is possible to train an $\\textit{unsupervised}$ automatic speech recognition (ASR) system using only unpaired audio and text. Existing unsupervised ASR methods assume that no labeled data can be used for training. We argue that even if one does not have any labeled audio for a given language, there is $\\textit{always}$ labeled data available for other languages. We show that it is possible to use character-level acoustic models (AMs) from other languages to bootstrap an $\\textit{unsupervised}$ AM in a new language. Here, \"unsupervised\" means no labeled audio is available for the $\\textit{target}$ language. Our approach is based on two key ingredients: (i) generating pseudo-labels (PLs) of the $\\textit{target}$ language using some $\\textit{other}$ language AM and (ii) constraining these PLs with a $\\textit{target language model}$. Our approach is effective on Common Voice: e.g. transfer of English AM to Swahili achieves 18% WER. It also outperforms characte",
    "path": "papers/23/05/2305.13330.json",
    "total_tokens": 997,
    "translated_title": "基于跨语言伪标注的无监督自动语音识别",
    "translated_abstract": "最近的研究表明，可以仅使用非配对的音频和文本来训练无监督自动语音识别（ASR）系统。现有的无监督ASR方法假定不能使用任何标注数据进行训练。本文认为，即使没有给定语言的任何标注音频，也始终可以使用其他语言中的标注数据。本文展示了如何使用其他语言的字符级声学模型（AM），来引导新语言的无监督AM。 这里，“无监督”意味着没有可用于目标语言的标注音频。本文的方法基于两个关键因素：（i）使用其他语言AM生成“目标”语言的伪标签（PLs）；（ii）使用“目标语言模型”限制这些PLs。我们的方法在Common Voice上非常有效：例如，将英语AM传递到斯瓦希里语可以实现18％的WER。 它还在不同语言的多个数据集上优于基于字符的基线模型。",
    "tldr": "本研究提出了一种基于跨语言伪标注的无监督ASR方法，能够使用其他语言中的标注数据来引导新语言的无监督AM。在Common Voice上取得了良好的效果，可以实现18% WER。而且在不同语言的数据集上都优于基线模型。",
    "en_tdlr": "This paper proposes a method for unsupervised ASR via cross-lingual pseudo-labeling, using labeled data from other languages to bootstrap an unsupervised AM for a new language. Achieving good results on Common Voice with 18% WER, this approach outperforms character-based baseline models on multiple datasets in different languages."
}