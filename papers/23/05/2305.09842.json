{
    "title": "A Note on Dimensionality Reduction in Deep Neural Networks using Empirical Interpolation Method. (arXiv:2305.09842v1 [cs.LG])",
    "abstract": "Empirical interpolation method (EIM) is a well-known technique to efficiently approximate parameterized functions. This paper proposes to use EIM algorithm to efficiently reduce the dimension of the training data within supervised machine learning. This is termed as DNN-EIM. Applications in data science (e.g., MNIST) and parameterized (and time-dependent) partial differential equations (PDEs) are considered. The proposed DNNs in case of classification are trained in parallel for each class. This approach is sequential, i.e., new classes can be added without having to retrain the network. In case of PDEs, a DNN is designed corresponding to each EIM point. Again, these networks can be trained in parallel, for each EIM point. In all cases, the parallel networks require fewer than ten times the number of training weights. Significant gains are observed in terms of training times, without sacrificing accuracy.",
    "link": "http://arxiv.org/abs/2305.09842",
    "context": "Title: A Note on Dimensionality Reduction in Deep Neural Networks using Empirical Interpolation Method. (arXiv:2305.09842v1 [cs.LG])\nAbstract: Empirical interpolation method (EIM) is a well-known technique to efficiently approximate parameterized functions. This paper proposes to use EIM algorithm to efficiently reduce the dimension of the training data within supervised machine learning. This is termed as DNN-EIM. Applications in data science (e.g., MNIST) and parameterized (and time-dependent) partial differential equations (PDEs) are considered. The proposed DNNs in case of classification are trained in parallel for each class. This approach is sequential, i.e., new classes can be added without having to retrain the network. In case of PDEs, a DNN is designed corresponding to each EIM point. Again, these networks can be trained in parallel, for each EIM point. In all cases, the parallel networks require fewer than ten times the number of training weights. Significant gains are observed in terms of training times, without sacrificing accuracy.",
    "path": "papers/23/05/2305.09842.json",
    "total_tokens": 903,
    "translated_title": "使用经验插值方法在深度神经网络中进行降维的注记",
    "translated_abstract": "经验插值方法（EIM）是一种有效估计参数化函数的技术。本文提出了一种名为DNN-EIM的算法，使用EIM算法在监督机器学习中有效地减少训练数据的维数。考虑了在数据科学（例如MNIST）和参数（以及时变）偏微分方程（PDE）方面的应用。对于分类，所提出的DNN是为每个类别并行训练。这种方法是顺序的，即可以添加新的类别，而不必重新训练网络。在PDE的情况下，为每个EIM点设计了一个DNN。同样，可以为每个EIM点并行训练这些网络。在所有情况下，与训练权重相比，并行网络所需的权重少于10倍。通过本文所提出的方法，在不牺牲准确性的情况下，显著缩短了训练时间。",
    "tldr": "本文提出了一种名为DNN-EIM的算法来在监督机器学习中使用EIM算法有效地减少训练数据的维数。同时考虑了在分类和PDEs方面的应用，该算法可以为每个类别或EIM点设计并行的DNN，所需权重比传统方法少得多。",
    "en_tdlr": "This paper proposes a DNN-EIM algorithm to efficiently reduce the dimension of training data and achieve significant training time gains without sacrificing accuracy in both classification and PDE applications. Parallel DNNs are designed for each class or EIM point, requiring significantly fewer weights than traditional methods."
}