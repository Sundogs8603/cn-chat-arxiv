{
    "title": "Lifting the Curse of Capacity Gap in Distilling Language Models. (arXiv:2305.12129v1 [cs.CL])",
    "abstract": "Pretrained language models (LMs) have shown compelling performance on various downstream tasks, but unfortunately they require a tremendous amount of inference compute. Knowledge distillation finds a path to compress LMs to small ones with a teacher-student paradigm. However, when the capacity gap between the teacher and the student is large, a curse of capacity gap appears, invoking a deficiency in distilling LMs. While a few studies have been carried out to fill the gap, the curse is not yet well tackled. In this paper, we aim at lifting the curse of capacity gap via enlarging the capacity of the student without notably increasing the inference compute. Largely motivated by sparse activation regime of mixture of experts (MoE), we propose a mixture of minimal experts (MiniMoE), which imposes extra parameters to the student but introduces almost no additional inference compute. Experimental results on GLUE and CoNLL demonstrate the curse of capacity gap is lifted by the magic of MiniMo",
    "link": "http://arxiv.org/abs/2305.12129",
    "context": "Title: Lifting the Curse of Capacity Gap in Distilling Language Models. (arXiv:2305.12129v1 [cs.CL])\nAbstract: Pretrained language models (LMs) have shown compelling performance on various downstream tasks, but unfortunately they require a tremendous amount of inference compute. Knowledge distillation finds a path to compress LMs to small ones with a teacher-student paradigm. However, when the capacity gap between the teacher and the student is large, a curse of capacity gap appears, invoking a deficiency in distilling LMs. While a few studies have been carried out to fill the gap, the curse is not yet well tackled. In this paper, we aim at lifting the curse of capacity gap via enlarging the capacity of the student without notably increasing the inference compute. Largely motivated by sparse activation regime of mixture of experts (MoE), we propose a mixture of minimal experts (MiniMoE), which imposes extra parameters to the student but introduces almost no additional inference compute. Experimental results on GLUE and CoNLL demonstrate the curse of capacity gap is lifted by the magic of MiniMo",
    "path": "papers/23/05/2305.12129.json",
    "total_tokens": 926,
    "translated_title": "解除预训练语言模型中的容量差异诅咒",
    "translated_abstract": "预训练语言模型在各种下游任务中表现出色，但不幸的是，它们需要大量的推理计算。知识蒸馏通过师生范式为小型模型压缩预训练语言模型，但当师生之间的容量差距很大时，容量差距诅咒会出现，导致蒸馏语言模型不足。虽然已有几项研究填补了这一差距，但诅咒仍未得到很好的解决。在本文中，我们旨在通过增加学生的容量而不明显增加推理计算来解除容量差异诅咒。受混合专家(Sparse Activation Regime of Mixture of Experts (MoE))的启发，我们提出了最小专家(MiniMoE)的混合物，这为学生引入了额外的参数，但几乎没有引入任何额外的推理计算。在GLUE和CoNLL上的实验结果表明，MiniMoE的魔力消除了容量差距诅咒。",
    "tldr": "本文提出了一种新的知识蒸馏方法（MiniMoE），通过增加学生的容量而不明显增加推理计算解除容量差异诅咒，并在GLUE和CoNLL上进行了实验验证。"
}