{
    "title": "Logit-Based Ensemble Distribution Distillation for Robust Autoregressive Sequence Uncertainties. (arXiv:2305.10384v1 [cs.LG])",
    "abstract": "Efficiently and reliably estimating uncertainty is an important objective in deep learning. It is especially pertinent to autoregressive sequence tasks, where training and inference costs are typically very high. However, existing research has predominantly focused on tasks with static data such as image classification. In this work, we investigate Ensemble Distribution Distillation (EDD) applied to large-scale natural language sequence-to-sequence data. EDD aims to compress the superior uncertainty performance of an expensive (teacher) ensemble into a cheaper (student) single model. Importantly, the ability to separate knowledge (epistemic) and data (aleatoric) uncertainty is retained. Existing probability-space approaches to EDD, however, are difficult to scale to large vocabularies. We show, for modern transformer architectures on large-scale translation tasks, that modelling the ensemble logits, instead of softmax probabilities, leads to significantly better students. Moreover, the",
    "link": "http://arxiv.org/abs/2305.10384",
    "context": "Title: Logit-Based Ensemble Distribution Distillation for Robust Autoregressive Sequence Uncertainties. (arXiv:2305.10384v1 [cs.LG])\nAbstract: Efficiently and reliably estimating uncertainty is an important objective in deep learning. It is especially pertinent to autoregressive sequence tasks, where training and inference costs are typically very high. However, existing research has predominantly focused on tasks with static data such as image classification. In this work, we investigate Ensemble Distribution Distillation (EDD) applied to large-scale natural language sequence-to-sequence data. EDD aims to compress the superior uncertainty performance of an expensive (teacher) ensemble into a cheaper (student) single model. Importantly, the ability to separate knowledge (epistemic) and data (aleatoric) uncertainty is retained. Existing probability-space approaches to EDD, however, are difficult to scale to large vocabularies. We show, for modern transformer architectures on large-scale translation tasks, that modelling the ensemble logits, instead of softmax probabilities, leads to significantly better students. Moreover, the",
    "path": "papers/23/05/2305.10384.json",
    "total_tokens": 875,
    "translated_title": "基于Logit的集成分布蒸馏在自回归序列的不确定性中的应用",
    "translated_abstract": "高效可靠地估计不确定性是深度学习的一个重要目标，特别是在训练和推理成本通常非常高的自回归序列任务中。本文研究了应用于大规模自然语言序列到序列数据的集成分布蒸馏（Ensemble Distribution Distillation，EDD）方法。EDD旨在将昂贵的（teacher）集成模型的优越不确定性性能压缩到更便宜的（student）单一模型中。重要的是，它保留了将知识（认知）和数据（随机）不确定性分开的能力。现有的概率空间方法对于大词汇量的任务来说不易扩展。我们表明，在大规模翻译任务的现代Transformers模型中，对集成模型的logits进行建模比对softmax概率进行建模，能够显著提高student模型的表现。",
    "tldr": "本论文介绍了一种基于Logit的集成模型蒸馏方法，能够有效地将知识（epistemic）和数据（aleatoric）不确定性分开，对于大规模自然语言序列到序列的任务能够提高student模型的表现。"
}