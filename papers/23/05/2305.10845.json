{
    "title": "TAPIR: Learning Adaptive Revision for Incremental Natural Language Understanding with a Two-Pass Model. (arXiv:2305.10845v1 [cs.CL])",
    "abstract": "Language is by its very nature incremental in how it is produced and processed. This property can be exploited by NLP systems to produce fast responses, which has been shown to be beneficial for real-time interactive applications. Recent neural network-based approaches for incremental processing mainly use RNNs or Transformers. RNNs are fast but monotonic (cannot correct earlier output, which can be necessary in incremental processing). Transformers, on the other hand, consume whole sequences, and hence are by nature non-incremental. A restart-incremental interface that repeatedly passes longer input prefixes can be used to obtain partial outputs, while providing the ability to revise. However, this method becomes costly as the sentence grows longer. In this work, we propose the Two-pass model for AdaPtIve Revision (TAPIR) and introduce a method to obtain an incremental supervision signal for learning an adaptive revision policy. Experimental results on sequence labelling show that our",
    "link": "http://arxiv.org/abs/2305.10845",
    "context": "Title: TAPIR: Learning Adaptive Revision for Incremental Natural Language Understanding with a Two-Pass Model. (arXiv:2305.10845v1 [cs.CL])\nAbstract: Language is by its very nature incremental in how it is produced and processed. This property can be exploited by NLP systems to produce fast responses, which has been shown to be beneficial for real-time interactive applications. Recent neural network-based approaches for incremental processing mainly use RNNs or Transformers. RNNs are fast but monotonic (cannot correct earlier output, which can be necessary in incremental processing). Transformers, on the other hand, consume whole sequences, and hence are by nature non-incremental. A restart-incremental interface that repeatedly passes longer input prefixes can be used to obtain partial outputs, while providing the ability to revise. However, this method becomes costly as the sentence grows longer. In this work, we propose the Two-pass model for AdaPtIve Revision (TAPIR) and introduce a method to obtain an incremental supervision signal for learning an adaptive revision policy. Experimental results on sequence labelling show that our",
    "path": "papers/23/05/2305.10845.json",
    "total_tokens": 875,
    "translated_title": "TAPIR：使用双通道模型学习自适应修订增量自然语言理解",
    "translated_abstract": "语言本质上是增量式的，这对自然语言处理系统来说是个优势，可以为实时交互应用提供快速响应。最近的基于神经网络的增量处理方法主要使用RNN或Transformer。RNN速度快但单调（不能纠正早期的输出，这在增量处理中很必要）。另一方面，Transformer使用整个序列，因此本质上不是增量的。为了获得部分输出并提供修订能力，可以使用重启增量界面重复传递更长的输入前缀。然而，随着句子变得越来越长，这种方法变得代价高昂。在这项工作中，我们提出了AdaPtIve修订的双通道模型TAPIR，并介绍了一种获得自适应修订策略的增量监督信号的方法。序列标记的实验结果表明，我们的模型优于现有的增量模型，并在ATIS数据集上实现了最先进的性能。",
    "tldr": "TAPIR使用双通道模型实现自适应修订增量自然语言理解，并取得了在ATIS数据集上最先进的性能。",
    "en_tdlr": "TAPIR uses a two-pass model to achieve adaptive revision for incremental natural language understanding, and achieves state-of-the-art performance on the ATIS dataset."
}