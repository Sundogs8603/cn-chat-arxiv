{
    "title": "Do Not Blindly Imitate the Teacher: Using Perturbed Loss for Knowledge Distillation. (arXiv:2305.05010v1 [cs.LG])",
    "abstract": "Knowledge distillation is a popular technique to transfer knowledge from large teacher models to a small student model. Typically, the student learns to imitate the teacher by minimizing the KL divergence of its output distribution with the teacher's output distribution. In this work, we argue that such a learning objective is sub-optimal because there exists a discrepancy between the teacher's output distribution and the ground truth label distribution. Therefore, forcing the student to blindly imitate the unreliable teacher output distribution leads to inferior performance. To this end, we propose a novel knowledge distillation objective PTLoss by first representing the vanilla KL-based distillation loss function via a Maclaurin series and then perturbing the leading-order terms in this series. This perturbed loss implicitly transforms the original teacher into a proxy teacher with a distribution closer to the ground truth distribution. We establish the theoretical connection between",
    "link": "http://arxiv.org/abs/2305.05010",
    "context": "Title: Do Not Blindly Imitate the Teacher: Using Perturbed Loss for Knowledge Distillation. (arXiv:2305.05010v1 [cs.LG])\nAbstract: Knowledge distillation is a popular technique to transfer knowledge from large teacher models to a small student model. Typically, the student learns to imitate the teacher by minimizing the KL divergence of its output distribution with the teacher's output distribution. In this work, we argue that such a learning objective is sub-optimal because there exists a discrepancy between the teacher's output distribution and the ground truth label distribution. Therefore, forcing the student to blindly imitate the unreliable teacher output distribution leads to inferior performance. To this end, we propose a novel knowledge distillation objective PTLoss by first representing the vanilla KL-based distillation loss function via a Maclaurin series and then perturbing the leading-order terms in this series. This perturbed loss implicitly transforms the original teacher into a proxy teacher with a distribution closer to the ground truth distribution. We establish the theoretical connection between",
    "path": "papers/23/05/2305.05010.json",
    "total_tokens": 864,
    "translated_title": "不要盲目模仿老师：使用扰动损失进行知识蒸馏",
    "translated_abstract": "知识蒸馏是一种常用的技术，用于将大型教师模型的知识传输到小型学生模型中。通常，学生通过最小化其输出分布和教师的输出分布之间的KL散度来模仿教师。本文认为这种学习目标是次优的，因为教师的输出分布与地面真实标签分布存在差异。因此，强制学生盲目模仿不可靠的教师输出分布会导致性能下降。为此，我们提出了一种新的知识蒸馏目标函数PTLoss，首先通过Maclaurin级数表示香草KL蒸馏损失函数，然后扰动该级数中的主导项。这种扰动损失隐式地将原始老师转换为具有更接近地面真实分布的代理老师。我们建立了香草KL蒸馏和扰动KL蒸馏之间的理论联系。",
    "tldr": "本文提出了一种新的知识蒸馏目标函数PTLoss，通过扰动老师的输出分布，使其更接近真实标签分布，从而提高学生的性能。",
    "en_tdlr": "This paper proposes a novel knowledge distillation objective PTLoss to improve the performance of small student models by perturbing the output distribution of the teacher model, making it closer to the ground truth label distribution."
}