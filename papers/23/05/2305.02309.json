{
    "title": "CodeGen2: Lessons for Training LLMs on Programming and Natural Languages. (arXiv:2305.02309v1 [cs.LG])",
    "abstract": "Large language models (LLMs) have demonstrated remarkable abilities in representation learning for program synthesis and understanding tasks. The quality of the learned representations appears to be dictated by the neural scaling laws as a function of the number of model parameters and observations, while imposing upper bounds on the model performance by the amount of available data and compute, which is costly.  In this study, we attempt to render the training of LLMs for program synthesis more efficient by unifying four key components: (1) model architectures, (2) learning methods, (3) infill sampling, and, (4) data distributions. Specifically, for the model architecture, we attempt to unify encoder and decoder-based models into a single prefix-LM. For learning methods, (i) causal language modeling, (ii) span corruption, (iii) infilling are unified into a simple learning algorithm. For infill sampling, we explore the claim of a \"free lunch\" hypothesis. For data distributions, the eff",
    "link": "http://arxiv.org/abs/2305.02309",
    "context": "Title: CodeGen2: Lessons for Training LLMs on Programming and Natural Languages. (arXiv:2305.02309v1 [cs.LG])\nAbstract: Large language models (LLMs) have demonstrated remarkable abilities in representation learning for program synthesis and understanding tasks. The quality of the learned representations appears to be dictated by the neural scaling laws as a function of the number of model parameters and observations, while imposing upper bounds on the model performance by the amount of available data and compute, which is costly.  In this study, we attempt to render the training of LLMs for program synthesis more efficient by unifying four key components: (1) model architectures, (2) learning methods, (3) infill sampling, and, (4) data distributions. Specifically, for the model architecture, we attempt to unify encoder and decoder-based models into a single prefix-LM. For learning methods, (i) causal language modeling, (ii) span corruption, (iii) infilling are unified into a simple learning algorithm. For infill sampling, we explore the claim of a \"free lunch\" hypothesis. For data distributions, the eff",
    "path": "papers/23/05/2305.02309.json",
    "total_tokens": 778,
    "translated_title": "CodeGen2：编程和自然语言LLM训练的经验",
    "translated_abstract": "大型语言模型在编程合成和理解任务的表示学习中展示了卓越的能力。学习到的表示质量似乎由神经比例定律作为模型参数和观察值的函数决定，同时通过可用数据和计算量的数量限制模型的性能。本研究尝试通过统一四个关键组件使LLMs的程序合成训练更加高效：（1）模型架构，（2）学习方法，（3）填充采样和（4）数据分布。",
    "tldr": "本研究旨在提高LLMs编程合成训练的效率，通过统一模型架构、学习方法、填充采样和数据分布，来提高训练模型的泛化能力。",
    "en_tdlr": "This study aims to improve the efficiency of LLMs training for program synthesis by unifying model architectures, learning methods, infill sampling, and data distributions, and  enhancing the generalization ability of trained models."
}