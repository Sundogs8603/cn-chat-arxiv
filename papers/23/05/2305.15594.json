{
    "title": "Flocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models. (arXiv:2305.15594v1 [cs.LG])",
    "abstract": "Large language models (LLMs) are excellent in-context learners. However, the sensitivity of data contained in prompts raises privacy concerns. Our work first shows that these concerns are valid: we instantiate a simple but highly effective membership inference attack against the data used to prompt LLMs. To address this vulnerability, one could forego prompting and resort to fine-tuning LLMs with known algorithms for private gradient descent. However, this comes at the expense of the practicality and efficiency offered by prompting. Therefore, we propose to privately learn to prompt. We first show that soft prompts can be obtained privately through gradient descent on downstream data. However, this is not the case for discrete prompts. Thus, we orchestrate a noisy vote among an ensemble of LLMs presented with different prompts, i.e., a flock of stochastic parrots. The vote privately transfers the flock's knowledge into a single public prompt. We show that LLMs prompted with our private",
    "link": "http://arxiv.org/abs/2305.15594",
    "context": "Title: Flocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models. (arXiv:2305.15594v1 [cs.LG])\nAbstract: Large language models (LLMs) are excellent in-context learners. However, the sensitivity of data contained in prompts raises privacy concerns. Our work first shows that these concerns are valid: we instantiate a simple but highly effective membership inference attack against the data used to prompt LLMs. To address this vulnerability, one could forego prompting and resort to fine-tuning LLMs with known algorithms for private gradient descent. However, this comes at the expense of the practicality and efficiency offered by prompting. Therefore, we propose to privately learn to prompt. We first show that soft prompts can be obtained privately through gradient descent on downstream data. However, this is not the case for discrete prompts. Thus, we orchestrate a noisy vote among an ensemble of LLMs presented with different prompts, i.e., a flock of stochastic parrots. The vote privately transfers the flock's knowledge into a single public prompt. We show that LLMs prompted with our private",
    "path": "papers/23/05/2305.15594.json",
    "total_tokens": 810,
    "translated_title": "随机鹦鹉群体：用差分隐私促进大型语言模型的学习",
    "translated_abstract": "大型语言模型(LLMs)在上下文学习中表现出色。 然而，提示中包含的数据的敏感性引起了隐私问题。文章首先证明了这些问题是合理的：我们对用于提示LLMs的数据进行了简单但非常有效的成员推断攻击。为了解决这个问题，作者提出了一种私有的提示学习方法，并展示了私有的软提示可以通过下游数据的梯度下降实现。而离散提示则需要用多个LLMs进行嘈杂的表决，即随机鹦鹉群体，来将其知识传递到一个公共提示中。",
    "tldr": "本文提出了一种差分隐私的提示学习方法，可用于大型语言模型，包括软提示和通过随机鹦鹉群体进行的离散提示，以解决由于提示数据敏感性引起的隐私问题。",
    "en_tdlr": "This paper proposes a differentially private prompt learning method for large language models, including soft prompts learned by gradient descent and discrete prompts achieved through noisy voting among an ensemble of LLMs, to address privacy concerns caused by sensitive prompt data."
}