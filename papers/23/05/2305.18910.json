{
    "title": "Precision-Recall Divergence Optimization for Generative Modeling with GANs and Normalizing Flows. (arXiv:2305.18910v2 [cs.LG] UPDATED)",
    "abstract": "Achieving a balance between image quality (precision) and diversity (recall) is a significant challenge in the domain of generative models. Current state-of-the-art models primarily rely on optimizing heuristics, such as the Fr\\'echet Inception Distance. While recent developments have introduced principled methods for evaluating precision and recall, they have yet to be successfully integrated into the training of generative models. Our main contribution is a novel training method for generative models, such as Generative Adversarial Networks and Normalizing Flows, which explicitly optimizes a user-defined trade-off between precision and recall. More precisely, we show that achieving a specified precision-recall trade-off corresponds to minimizing a unique $f$-divergence from a family we call the \\textit{PR-divergences}. Conversely, any $f$-divergence can be written as a linear combination of PR-divergences and corresponds to a weighted precision-recall trade-off. Through comprehensive",
    "link": "http://arxiv.org/abs/2305.18910",
    "context": "Title: Precision-Recall Divergence Optimization for Generative Modeling with GANs and Normalizing Flows. (arXiv:2305.18910v2 [cs.LG] UPDATED)\nAbstract: Achieving a balance between image quality (precision) and diversity (recall) is a significant challenge in the domain of generative models. Current state-of-the-art models primarily rely on optimizing heuristics, such as the Fr\\'echet Inception Distance. While recent developments have introduced principled methods for evaluating precision and recall, they have yet to be successfully integrated into the training of generative models. Our main contribution is a novel training method for generative models, such as Generative Adversarial Networks and Normalizing Flows, which explicitly optimizes a user-defined trade-off between precision and recall. More precisely, we show that achieving a specified precision-recall trade-off corresponds to minimizing a unique $f$-divergence from a family we call the \\textit{PR-divergences}. Conversely, any $f$-divergence can be written as a linear combination of PR-divergences and corresponds to a weighted precision-recall trade-off. Through comprehensive",
    "path": "papers/23/05/2305.18910.json",
    "total_tokens": 907,
    "translated_title": "通过GANs和归一化流模型的Precision-Recall分歧优化进行生成建模",
    "translated_abstract": "在生成模型领域，平衡图像质量（精确性）和多样性（召回率）是一个重要的挑战。目前的最先进模型主要依赖于优化启发式方法，如Fr\\'echet Inception Distance。虽然最近的发展引入了一些原则性的方法来评估精确性和召回率，但它们尚未成功地整合到生成模型的训练中。我们的主要贡献是一种新颖的生成模型训练方法，如生成对抗网络和归一化流模型，它明确地优化了用户定义的精确性和召回率的权衡。更具体地说，我们证明了实现指定的精确性-召回率权衡等价于最小化一组我们称之为“PR-divergences”的独特的$f$-divergence。反之，任何$f$-divergence都可以写成PR-divergences的线性组合，并对应一个加权精确性-召回率权衡。通过全面的试验，我们展示了我们的方法明显超过了先前的算法。",
    "tldr": "本文提出了一种通过优化Precision-Recall分歧来平衡生成模型图像质量和多样性的新训练方法，实现了用户定义的精确性和召回率权衡。"
}