{
    "title": "Gradient Descent Monotonically Decreases the Sharpness of Gradient Flow Solutions in Scalar Networks and Beyond. (arXiv:2305.13064v1 [cs.LG])",
    "abstract": "Recent research shows that when Gradient Descent (GD) is applied to neural networks, the loss almost never decreases monotonically. Instead, the loss oscillates as gradient descent converges to its ''Edge of Stability'' (EoS). Here, we find a quantity that does decrease monotonically throughout GD training: the sharpness attained by the gradient flow solution (GFS)-the solution that would be obtained if, from now until convergence, we train with an infinitesimal step size. Theoretically, we analyze scalar neural networks with the squared loss, perhaps the simplest setting where the EoS phenomena still occur. In this model, we prove that the GFS sharpness decreases monotonically. Using this result, we characterize settings where GD provably converges to the EoS in scalar networks. Empirically, we show that GD monotonically decreases the GFS sharpness in a squared regression model as well as practical neural network architectures.",
    "link": "http://arxiv.org/abs/2305.13064",
    "context": "Title: Gradient Descent Monotonically Decreases the Sharpness of Gradient Flow Solutions in Scalar Networks and Beyond. (arXiv:2305.13064v1 [cs.LG])\nAbstract: Recent research shows that when Gradient Descent (GD) is applied to neural networks, the loss almost never decreases monotonically. Instead, the loss oscillates as gradient descent converges to its ''Edge of Stability'' (EoS). Here, we find a quantity that does decrease monotonically throughout GD training: the sharpness attained by the gradient flow solution (GFS)-the solution that would be obtained if, from now until convergence, we train with an infinitesimal step size. Theoretically, we analyze scalar neural networks with the squared loss, perhaps the simplest setting where the EoS phenomena still occur. In this model, we prove that the GFS sharpness decreases monotonically. Using this result, we characterize settings where GD provably converges to the EoS in scalar networks. Empirically, we show that GD monotonically decreases the GFS sharpness in a squared regression model as well as practical neural network architectures.",
    "path": "papers/23/05/2305.13064.json",
    "total_tokens": 925,
    "translated_title": "梯度下降单调递减了标量网络和其他模型中的梯度流解的尖锐度。",
    "translated_abstract": "最近的研究表明，当将梯度下降（GD）应用于神经网络时，损失几乎不会单调递减。相反，损失会在梯度下降收敛到其“稳定边缘”（EoS）时振荡。在这里，我们发现一种在GD训练过程中单调递减的量：梯度流解（GFS）所达到的尖锐度 - 如果从现在开始到收敛，我们使用无穷小的步长进行训练所获得的解决方案。在理论上，我们使用平方损失分析标量神经网络，这可能是EoS现象仍然发生的最简单的情况。在这个模型中，我们证明了GFS尖锐度单调递减。使用这个结果，我们描述了GD在标量网络中可以被证明收敛到EoS的设置。在实验上，我们展示了GD在平方回归模型以及实际神经网络架构中单调递减了GFS的尖锐度。",
    "tldr": "对于标量网络和其他模型，研究表明梯度下降会单调递减梯度流解的尖锐度。",
    "en_tdlr": "Gradually decreasing the sharpness of gradient flow solutions (GFS), gradient descent (GD) tends to converge to its Edge of Stability (EoS) while applying neural networks. It is shown that the sharpness of GFS monotonically decreases in scalar networks where the EoS phenomena occur in a squared loss model and practical neural network architectures."
}