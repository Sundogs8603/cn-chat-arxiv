{
    "title": "Learning Likelihood Ratios with Neural Network Classifiers. (arXiv:2305.10500v1 [hep-ph])",
    "abstract": "The likelihood ratio is a crucial quantity for statistical inference in science that enables hypothesis testing, construction of confidence intervals, reweighting of distributions, and more. Many modern scientific applications, however, make use of data- or simulation-driven models for which computing the likelihood ratio can be very difficult or even impossible. By applying the so-called ``likelihood ratio trick,'' approximations of the likelihood ratio may be computed using clever parametrizations of neural network-based classifiers. A number of different neural network setups can be defined to satisfy this procedure, each with varying performance in approximating the likelihood ratio when using finite training data. We present a series of empirical studies detailing the performance of several common loss functionals and parametrizations of the classifier output in approximating the likelihood ratio of two univariate and multivariate Gaussian distributions as well as simulated high-e",
    "link": "http://arxiv.org/abs/2305.10500",
    "context": "Title: Learning Likelihood Ratios with Neural Network Classifiers. (arXiv:2305.10500v1 [hep-ph])\nAbstract: The likelihood ratio is a crucial quantity for statistical inference in science that enables hypothesis testing, construction of confidence intervals, reweighting of distributions, and more. Many modern scientific applications, however, make use of data- or simulation-driven models for which computing the likelihood ratio can be very difficult or even impossible. By applying the so-called ``likelihood ratio trick,'' approximations of the likelihood ratio may be computed using clever parametrizations of neural network-based classifiers. A number of different neural network setups can be defined to satisfy this procedure, each with varying performance in approximating the likelihood ratio when using finite training data. We present a series of empirical studies detailing the performance of several common loss functionals and parametrizations of the classifier output in approximating the likelihood ratio of two univariate and multivariate Gaussian distributions as well as simulated high-e",
    "path": "papers/23/05/2305.10500.json",
    "total_tokens": 817,
    "translated_title": "使用神经网络分类器学习似然比",
    "translated_abstract": "在科学中，似然比是统计推断的关键性量，它使得假设检验、置信区间构建、分布加权等成为可能。然而，许多现代科学应用使用基于数据或基于模拟的模型，而计算似然比可能非常困难甚至不可能。通过应用所谓的“似然比技巧”，可以使用聪明的神经网络分类器参数化来计算似然比的近似值。可以定义许多不同的神经网络设置来满足此过程，每个设置在使用有限训练数据时近似似然比的性能各异。我们提出了一系列经验研究，详细介绍了几种常见损失函数和分类器输出参数化在近似两个一元和多元高斯分布的似然比方面的表现以及模拟高能物理的信号和背景事件。",
    "tldr": "该研究介绍了一种使用神经网络分类器参数化计算似然比的技巧，并详细比较了不同设置的性能。这对于许多数据或基于模拟的科学应用非常有用。",
    "en_tdlr": "This study presents a technique for computing likelihood ratios using clever parametrizations of neural network-based classifiers and compares the performance of different setups. It is useful for many data or simulation-driven scientific applications."
}