{
    "title": "RepCL: Exploring Effective Representation for Continual Text Classification. (arXiv:2305.07289v1 [cs.CL])",
    "abstract": "Continual learning (CL) aims to constantly learn new knowledge over time while avoiding catastrophic forgetting on old tasks. In this work, we focus on continual text classification under the class-incremental setting. Recent CL studies find that the representations learned in one task may not be effective for other tasks, namely representation bias problem. For the first time we formally analyze representation bias from an information bottleneck perspective and suggest that exploiting representations with more class-relevant information could alleviate the bias. To this end, we propose a novel replay-based continual text classification method, RepCL. Our approach utilizes contrastive and generative representation learning objectives to capture more class-relevant features. In addition, RepCL introduces an adversarial replay strategy to alleviate the overfitting problem of replay. Experiments demonstrate that RepCL effectively alleviates forgetting and achieves state-of-the-art perform",
    "link": "http://arxiv.org/abs/2305.07289",
    "context": "Title: RepCL: Exploring Effective Representation for Continual Text Classification. (arXiv:2305.07289v1 [cs.CL])\nAbstract: Continual learning (CL) aims to constantly learn new knowledge over time while avoiding catastrophic forgetting on old tasks. In this work, we focus on continual text classification under the class-incremental setting. Recent CL studies find that the representations learned in one task may not be effective for other tasks, namely representation bias problem. For the first time we formally analyze representation bias from an information bottleneck perspective and suggest that exploiting representations with more class-relevant information could alleviate the bias. To this end, we propose a novel replay-based continual text classification method, RepCL. Our approach utilizes contrastive and generative representation learning objectives to capture more class-relevant features. In addition, RepCL introduces an adversarial replay strategy to alleviate the overfitting problem of replay. Experiments demonstrate that RepCL effectively alleviates forgetting and achieves state-of-the-art perform",
    "path": "papers/23/05/2305.07289.json",
    "total_tokens": 859,
    "translated_title": "RepCL: 探索有效的表示方法以进行持续文本分类",
    "translated_abstract": "持续学习旨在不断学习新知识，同时避免忘记旧任务造成的灾难性后果。本文研究的是类增量设置下的持续文本分类。最近的持续学习研究发现，为一项任务学习到的表示方法可能对其他任务不起作用，即表示偏见问题。我们首次从信息瓶颈的角度正式分析了表示偏差，并建议利用更多类相关信息来消除偏见。为此，我们提出了一种基于回放的持续文本分类方法RepCL。我们的方法利用对比和生成表示学习目标来捕获更多的类相关特征。此外，RepCL引入了对抗式回放策略以消除回放的过拟合问题。实验表明，RepCL有效地缓解了遗忘问题，并实现了最先进的性能。",
    "tldr": "本文探讨了类增量设置下的持续文本分类问题中的表示偏见，从信息瓶颈的角度提出了利用更多类相关信息消除偏见的方法，即RepCL方法，并证明了其能有效地解决遗忘问题并实现最先进的性能。",
    "en_tdlr": "This paper explores the problem of representation bias in continual text classification under the class-incremental setting. The authors propose a novel replay-based method, RepCL, to alleviate the bias by utilizing class-relevant information. Experiments demonstrate that RepCL effectively addresses the forgetting problem and achieves state-of-the-art performance."
}