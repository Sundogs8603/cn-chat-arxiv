{
    "title": "Single Node Injection Label Specificity Attack on Graph Neural Networks via Reinforcement Learning. (arXiv:2305.02901v1 [cs.LG])",
    "abstract": "Graph neural networks (GNNs) have achieved remarkable success in various real-world applications. However, recent studies highlight the vulnerability of GNNs to malicious perturbations. Previous adversaries primarily focus on graph modifications or node injections to existing graphs, yielding promising results but with notable limitations. Graph modification attack~(GMA) requires manipulation of the original graph, which is often impractical, while graph injection attack~(GIA) necessitates training a surrogate model in the black-box setting, leading to significant performance degradation due to divergence between the surrogate architecture and the actual victim model. Furthermore, most methods concentrate on a single attack goal and lack a generalizable adversary to develop distinct attack strategies for diverse goals, thus limiting precise control over victim model behavior in real-world scenarios. To address these issues, we present a gradient-free generalizable adversary that inject",
    "link": "http://arxiv.org/abs/2305.02901",
    "context": "Title: Single Node Injection Label Specificity Attack on Graph Neural Networks via Reinforcement Learning. (arXiv:2305.02901v1 [cs.LG])\nAbstract: Graph neural networks (GNNs) have achieved remarkable success in various real-world applications. However, recent studies highlight the vulnerability of GNNs to malicious perturbations. Previous adversaries primarily focus on graph modifications or node injections to existing graphs, yielding promising results but with notable limitations. Graph modification attack~(GMA) requires manipulation of the original graph, which is often impractical, while graph injection attack~(GIA) necessitates training a surrogate model in the black-box setting, leading to significant performance degradation due to divergence between the surrogate architecture and the actual victim model. Furthermore, most methods concentrate on a single attack goal and lack a generalizable adversary to develop distinct attack strategies for diverse goals, thus limiting precise control over victim model behavior in real-world scenarios. To address these issues, we present a gradient-free generalizable adversary that inject",
    "path": "papers/23/05/2305.02901.json",
    "total_tokens": 989,
    "tldr": "本文提出了一种对图神经网络的攻击方法，通过强化学习优化多节点注入攻击，来欺骗目标GNN的标签预测，该方法可以在白盒和黑盒设置下成功欺骗GNN的预测性能。",
    "en_tdlr": "This paper proposes a method to attack graph neural networks by utilizing reinforcement learning to optimize multi-node injection attack and deceive the target GNN's label prediction. The proposed method shows effectiveness in successfully deceiving GNN's prediction performance in both white-box and black-box settings."
}