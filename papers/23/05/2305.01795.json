{
    "title": "Multimodal Procedural Planning via Dual Text-Image Prompting. (arXiv:2305.01795v1 [cs.CL])",
    "abstract": "Embodied agents have achieved prominent performance in following human instructions to complete tasks. However, the potential of providing instructions informed by texts and images to assist humans in completing tasks remains underexplored. To uncover this capability, we present the multimodal procedural planning (MPP) task, in which models are given a high-level goal and generate plans of paired text-image steps, providing more complementary and informative guidance than unimodal plans. The key challenges of MPP are to ensure the informativeness, temporal coherence,and accuracy of plans across modalities. To tackle this, we propose Text-Image Prompting (TIP), a dual-modality prompting method that jointly leverages zero-shot reasoning ability in large language models (LLMs) and compelling text-to-image generation ability from diffusion-based models. TIP improves the interaction in the dual modalities using Text-to-Image Bridge and Image-to-Text Bridge, allowing LLMs to guide the textua",
    "link": "http://arxiv.org/abs/2305.01795",
    "context": "Title: Multimodal Procedural Planning via Dual Text-Image Prompting. (arXiv:2305.01795v1 [cs.CL])\nAbstract: Embodied agents have achieved prominent performance in following human instructions to complete tasks. However, the potential of providing instructions informed by texts and images to assist humans in completing tasks remains underexplored. To uncover this capability, we present the multimodal procedural planning (MPP) task, in which models are given a high-level goal and generate plans of paired text-image steps, providing more complementary and informative guidance than unimodal plans. The key challenges of MPP are to ensure the informativeness, temporal coherence,and accuracy of plans across modalities. To tackle this, we propose Text-Image Prompting (TIP), a dual-modality prompting method that jointly leverages zero-shot reasoning ability in large language models (LLMs) and compelling text-to-image generation ability from diffusion-based models. TIP improves the interaction in the dual modalities using Text-to-Image Bridge and Image-to-Text Bridge, allowing LLMs to guide the textua",
    "path": "papers/23/05/2305.01795.json",
    "total_tokens": 952,
    "translated_title": "通过双文本-图像提示的多模态过程规划",
    "translated_abstract": "实体代理已经在遵循人类指令完成任务方面取得了杰出表现。然而，利用文本和图像提供指导以帮助人类完成任务的潜力仍未得到充分探索。为了揭示这种能力，我们提出了多模态过程规划（MPP）任务，其中模型给出了高级目标并生成配对的文本-图像步骤的计划，提供比单模态计划更丰富和信息丰富的指导。MPP的关键挑战是确保跨模态的计划在信息性，时间连贯性和准确性方面的可靠性。为了解决这个问题，我们提出了文本-图像提示（TIP），一种双模态提示方法，它同时利用大型语言模型（LLM）中的零-shot推理能力和基于扩散的模型中令人信服的文本到图像生成能力。TIP使用文本-图像桥和图像-文本桥改善了双模态交互，使LLMs引导文本和图像之间的转换。",
    "tldr": "该论文提出了一个名为MPP的任务，用于使用文本和图像生成计划，帮助人类完成任务。为了解决跨模态的计划的可靠性问题，该论文提出了一种双模态提示方法TIP，它利用了大型语言模型和文本到图像生成模型的能力。",
    "en_tdlr": "This paper proposes a task called MPP, which generates plans using text and images to assist humans in completing tasks. To ensure the reliability of cross-modal plans, the paper proposes a dual-modality prompting method called TIP that leverages the capabilities of large language models and text-to-image generation models."
}