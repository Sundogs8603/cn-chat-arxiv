{
    "title": "What Can We Learn from Unlearnable Datasets?. (arXiv:2305.19254v2 [cs.LG] UPDATED)",
    "abstract": "In an era of widespread web scraping, unlearnable dataset methods have the potential to protect data privacy by preventing deep neural networks from generalizing. But in addition to a number of practical limitations that make their use unlikely, we make a number of findings that call into question their ability to safeguard data. First, it is widely believed that neural networks trained on unlearnable datasets only learn shortcuts, simpler rules that are not useful for generalization. In contrast, we find that networks actually can learn useful features that can be reweighed for high test performance, suggesting that image protection is not assured. Unlearnable datasets are also believed to induce learning shortcuts through linear separability of added perturbations. We provide a counterexample, demonstrating that linear separability of perturbations is not a necessary condition. To emphasize why linearly separable perturbations should not be relied upon, we propose an orthogonal proje",
    "link": "http://arxiv.org/abs/2305.19254",
    "context": "Title: What Can We Learn from Unlearnable Datasets?. (arXiv:2305.19254v2 [cs.LG] UPDATED)\nAbstract: In an era of widespread web scraping, unlearnable dataset methods have the potential to protect data privacy by preventing deep neural networks from generalizing. But in addition to a number of practical limitations that make their use unlikely, we make a number of findings that call into question their ability to safeguard data. First, it is widely believed that neural networks trained on unlearnable datasets only learn shortcuts, simpler rules that are not useful for generalization. In contrast, we find that networks actually can learn useful features that can be reweighed for high test performance, suggesting that image protection is not assured. Unlearnable datasets are also believed to induce learning shortcuts through linear separability of added perturbations. We provide a counterexample, demonstrating that linear separability of perturbations is not a necessary condition. To emphasize why linearly separable perturbations should not be relied upon, we propose an orthogonal proje",
    "path": "papers/23/05/2305.19254.json",
    "total_tokens": 964,
    "translated_title": "无法学习的数据集可以给我们带来哪些启示？",
    "translated_abstract": "在普遍进行网络爬虫的时代，无法学习的数据集方法具有保护数据隐私、防止深度神经网络泛化的潜力。但除了一些实际限制使得它们的使用不太可能外，我们发现一些结果对其保护数据能力提出了质疑。首先，人们普遍认为在无法学习的数据集上训练的神经网络只会学习到捷径，即并不适用于泛化的简单规则。然而，我们发现网络实际上可以学习到有用的特征，并且这些特征可以重新加权以获得高测试性能，这表明图像的保护并不能得到保证。无法学习的数据集据信通过添加扰动的线性可分性来诱导学习捷径。我们提供了一个反例，证明了扰动的线性可分性并不是必要条件。为了强调为什么不能依赖线性可分的扰动，我们提出了一个正交投影的方法。",
    "tldr": "无法学习的数据集方法具有保护数据隐私的潜力，但实际使用受到限制。我们发现神经网络在无法学习的数据集上可以学习到有用的特征，而不仅仅是简单规则，这对图像保护的效果不确定。此外，线性可分的扰动并不是诱导学习捷径的必要条件，因此不能依赖它们。",
    "en_tdlr": "Unlearnable dataset methods have the potential to protect data privacy, but their practical use is limited. Neural networks trained on unlearnable datasets can learn useful features, challenging the belief that they only learn shortcuts. Linear separability of perturbations, thought to induce learning shortcuts, is not always necessary."
}