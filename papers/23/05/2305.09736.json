{
    "title": "ADDSL: Hand Gesture Detection and Sign Language Recognition on Annotated Danish Sign Language. (arXiv:2305.09736v1 [cs.CV])",
    "abstract": "For a long time, detecting hand gestures and recognizing them as letters or numbers has been a challenging task. This creates communication barriers for individuals with disabilities. This paper introduces a new dataset, the Annotated Dataset for Danish Sign Language (ADDSL). Annota-tions for the dataset were made using the open-source tool LabelImg in the YOLO format. Using this dataset, a one-stage ob-ject detector model (YOLOv5) was trained with the CSP-DarkNet53 backbone and YOLOv3 head to recognize letters (A-Z) and numbers (0-9) using only seven unique images per class (without augmen-tation). Five models were trained with 350 epochs, resulting in an average inference time of 9.02ms per image and a best accu-racy of 92% when compared to previous research. Our results show that modified model is efficient and more accurate than existing work in the same field. The code repository for our model is available at the GitHub repository https://github.com/s4nyam/pvt-addsl.",
    "link": "http://arxiv.org/abs/2305.09736",
    "context": "Title: ADDSL: Hand Gesture Detection and Sign Language Recognition on Annotated Danish Sign Language. (arXiv:2305.09736v1 [cs.CV])\nAbstract: For a long time, detecting hand gestures and recognizing them as letters or numbers has been a challenging task. This creates communication barriers for individuals with disabilities. This paper introduces a new dataset, the Annotated Dataset for Danish Sign Language (ADDSL). Annota-tions for the dataset were made using the open-source tool LabelImg in the YOLO format. Using this dataset, a one-stage ob-ject detector model (YOLOv5) was trained with the CSP-DarkNet53 backbone and YOLOv3 head to recognize letters (A-Z) and numbers (0-9) using only seven unique images per class (without augmen-tation). Five models were trained with 350 epochs, resulting in an average inference time of 9.02ms per image and a best accu-racy of 92% when compared to previous research. Our results show that modified model is efficient and more accurate than existing work in the same field. The code repository for our model is available at the GitHub repository https://github.com/s4nyam/pvt-addsl.",
    "path": "papers/23/05/2305.09736.json",
    "total_tokens": 994,
    "translated_title": "ADDSL: 基于标注的丹麦手语的手势检测与识别",
    "translated_abstract": "长期以来，将手势检测并将其识别为字母或数字一直是一项具有挑战性的任务。这给残障人士带来了沟通障碍。本文介绍了一个新的数据集，即丹麦手语注释数据集（ADDSL）。使用开源工具LabelImg在YOLO格式中制作了数据集的注释。利用此数据集，使用CSP-DarkNet53骨干和YOLOv3头的单阶段目标检测器模型（YOLOv5）通过每类仅使用七个独特的图像（不进行数据增强）来训练，以识别字母（A-Z）和数字（0-9）。训练五个模型，共350个周期，得到每张图像的平均推断时间为9.02ms，与之前的研究相比，最佳准确率为92%。我们的结果表明，修改后的模型比同领域的现有工作更高效和准确。我们模型的代码库可在GitHub存储库https://github.com/s4nyam/pvt-addsl 上获得。",
    "tldr": "本研究引入了一个新的丹麦手语注释数据集（ADDSL）并使用该数据集训练了一个基于YOLOv5的手势检测和字母数字识别模型，准确率最高可达92%。与同领域现有工作相比，该模型更高效和准确。",
    "en_tdlr": "This study introduces a new annotated dataset for Danish Sign Language (ADDSL) and trains a gesture detection and letter/number recognition model based on YOLOv5 using the dataset, achieving a maximum accuracy of 92%. The modified model is shown to be more efficient and accurate than existing work in the same field."
}