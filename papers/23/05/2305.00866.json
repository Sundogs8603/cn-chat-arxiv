{
    "title": "Attack-SAM: Towards Attacking Segment Anything Model With Adversarial Examples. (arXiv:2305.00866v2 [cs.CV] UPDATED)",
    "abstract": "Segment Anything Model (SAM) has attracted significant attention recently, due to its impressive performance on various downstream tasks in a zero-short manner. Computer vision (CV) area might follow the natural language processing (NLP) area to embark on a path from task-specific vision models toward foundation models. However, deep vision models are widely recognized as vulnerable to adversarial examples, which fool the model to make wrong predictions with imperceptible perturbation. Such vulnerability to adversarial attacks causes serious concerns when applying deep models to security-sensitive applications. Therefore, it is critical to know whether the vision foundation model SAM can also be fooled by adversarial attacks. To the best of our knowledge, our work is the first of its kind to conduct a comprehensive investigation on how to attack SAM with adversarial examples. With the basic attack goal set to mask removal, we investigate the adversarial robustness of SAM in the full wh",
    "link": "http://arxiv.org/abs/2305.00866",
    "context": "Title: Attack-SAM: Towards Attacking Segment Anything Model With Adversarial Examples. (arXiv:2305.00866v2 [cs.CV] UPDATED)\nAbstract: Segment Anything Model (SAM) has attracted significant attention recently, due to its impressive performance on various downstream tasks in a zero-short manner. Computer vision (CV) area might follow the natural language processing (NLP) area to embark on a path from task-specific vision models toward foundation models. However, deep vision models are widely recognized as vulnerable to adversarial examples, which fool the model to make wrong predictions with imperceptible perturbation. Such vulnerability to adversarial attacks causes serious concerns when applying deep models to security-sensitive applications. Therefore, it is critical to know whether the vision foundation model SAM can also be fooled by adversarial attacks. To the best of our knowledge, our work is the first of its kind to conduct a comprehensive investigation on how to attack SAM with adversarial examples. With the basic attack goal set to mask removal, we investigate the adversarial robustness of SAM in the full wh",
    "path": "papers/23/05/2305.00866.json",
    "total_tokens": 881,
    "translated_title": "Attack-SAM: 面向用对抗样本攻击的分割万物模型",
    "translated_abstract": "最近，由于其在零短模式下在各种下游任务中的出色表现，分割万物模型（SAM）受到了广泛关注。然而，深度视觉模型被公认为容易受到对抗性攻击的攻击，这种攻击会在不可察觉的扰动下欺骗模型进行错误预测。为了使深度模型适用于安全敏感的应用程序，了解视觉基础模型SAM是否也容易受到对抗性攻击是至关重要的。据我们所知，我们的工作是第一个全面研究如何使用对抗样本攻击SAM的工作。我们以掩码移除为基本攻击目标，探讨了SAM的对抗性鲁棒性，这是理解和提高基础视觉模型安全性的重要步骤。",
    "tldr": "本文是第一个全面研究如何使用对抗样本攻击分割万物模型SAM的工作，该工作重要的贡献为探讨SAM的对抗性鲁棒性，有助于理解和提高基础视觉模型安全性。",
    "en_tdlr": "This paper is the first comprehensive investigation on how to attack the segment anything model (SAM) with adversarial examples, exploring the adversarial robustness of SAM which is important for improving the security of foundation vision models."
}