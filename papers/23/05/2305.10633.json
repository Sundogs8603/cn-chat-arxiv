{
    "title": "Smoothing the Landscape Boosts the Signal for SGD: Optimal Sample Complexity for Learning Single Index Models. (arXiv:2305.10633v1 [cs.LG])",
    "abstract": "We focus on the task of learning a single index model $\\sigma(w^\\star \\cdot x)$ with respect to the isotropic Gaussian distribution in $d$ dimensions. Prior work has shown that the sample complexity of learning $w^\\star$ is governed by the information exponent $k^\\star$ of the link function $\\sigma$, which is defined as the index of the first nonzero Hermite coefficient of $\\sigma$. Ben Arous et al. (2021) showed that $n \\gtrsim d^{k^\\star-1}$ samples suffice for learning $w^\\star$ and that this is tight for online SGD. However, the CSQ lower bound for gradient based methods only shows that $n \\gtrsim d^{k^\\star/2}$ samples are necessary. In this work, we close the gap between the upper and lower bounds by showing that online SGD on a smoothed loss learns $w^\\star$ with $n \\gtrsim d^{k^\\star/2}$ samples. We also draw connections to statistical analyses of tensor PCA and to the implicit regularization effects of minibatch SGD on empirical losses.",
    "link": "http://arxiv.org/abs/2305.10633",
    "context": "Title: Smoothing the Landscape Boosts the Signal for SGD: Optimal Sample Complexity for Learning Single Index Models. (arXiv:2305.10633v1 [cs.LG])\nAbstract: We focus on the task of learning a single index model $\\sigma(w^\\star \\cdot x)$ with respect to the isotropic Gaussian distribution in $d$ dimensions. Prior work has shown that the sample complexity of learning $w^\\star$ is governed by the information exponent $k^\\star$ of the link function $\\sigma$, which is defined as the index of the first nonzero Hermite coefficient of $\\sigma$. Ben Arous et al. (2021) showed that $n \\gtrsim d^{k^\\star-1}$ samples suffice for learning $w^\\star$ and that this is tight for online SGD. However, the CSQ lower bound for gradient based methods only shows that $n \\gtrsim d^{k^\\star/2}$ samples are necessary. In this work, we close the gap between the upper and lower bounds by showing that online SGD on a smoothed loss learns $w^\\star$ with $n \\gtrsim d^{k^\\star/2}$ samples. We also draw connections to statistical analyses of tensor PCA and to the implicit regularization effects of minibatch SGD on empirical losses.",
    "path": "papers/23/05/2305.10633.json",
    "total_tokens": 906,
    "translated_title": "平滑化风景可提升SGD信号：学习单指数模型的最优样本复杂度研究",
    "translated_abstract": "本文研究了在$d$维度上使用各向同性高斯分布来学习单指数模型$\\sigma(w^\\star \\cdot x)$的任务。先前的研究表明，学习$w^\\star$的样本复杂度是由链接函数$\\sigma$的信息指数$k^\\star$所决定的，它被定义为$\\sigma$的第一个非零Hermite系数的指数。本文通过展示基于平滑损失的在线SGD使用$n \\gtrsim d^{k^\\star/2}$个样本可以学习$w^\\star$，弥补了上下界之间的差距。作者还将其与张量PCA的统计分析和小批量SGD在经验损失上的隐式正则化效应联系起来。",
    "tldr": "本文提出了使用平滑化的损失来优化在线SGD的信号，可以使用$n \\gtrsim d^{k^\\star/2}$个样本学习单指数模型$w^\\star$，并与张量PCA和小批量SGD的正则化效应有关。",
    "en_tdlr": "This paper proposes to boost the signal for online SGD by smoothing the loss and shows that $n \\gtrsim d^{k^\\star/2}$ samples are sufficient for learning the single index model $w^\\star$. It also draws connections to statistical analyses of tensor PCA and the implicit regularization effects of minibatch SGD on empirical losses."
}