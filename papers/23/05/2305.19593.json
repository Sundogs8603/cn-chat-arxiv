{
    "title": "Exploring the Vulnerabilities of Machine Learning and Quantum Machine Learning to Adversarial Attacks using a Malware Dataset: A Comparative Analysis. (arXiv:2305.19593v1 [cs.LG])",
    "abstract": "The burgeoning fields of machine learning (ML) and quantum machine learning (QML) have shown remarkable potential in tackling complex problems across various domains. However, their susceptibility to adversarial attacks raises concerns when deploying these systems in security sensitive applications. In this study, we present a comparative analysis of the vulnerability of ML and QML models, specifically conventional neural networks (NN) and quantum neural networks (QNN), to adversarial attacks using a malware dataset. We utilize a software supply chain attack dataset known as ClaMP and develop two distinct models for QNN and NN, employing Pennylane for quantum implementations and TensorFlow and Keras for traditional implementations. Our methodology involves crafting adversarial samples by introducing random noise to a small portion of the dataset and evaluating the impact on the models performance using accuracy, precision, recall, and F1 score metrics. Based on our observations, both M",
    "link": "http://arxiv.org/abs/2305.19593",
    "context": "Title: Exploring the Vulnerabilities of Machine Learning and Quantum Machine Learning to Adversarial Attacks using a Malware Dataset: A Comparative Analysis. (arXiv:2305.19593v1 [cs.LG])\nAbstract: The burgeoning fields of machine learning (ML) and quantum machine learning (QML) have shown remarkable potential in tackling complex problems across various domains. However, their susceptibility to adversarial attacks raises concerns when deploying these systems in security sensitive applications. In this study, we present a comparative analysis of the vulnerability of ML and QML models, specifically conventional neural networks (NN) and quantum neural networks (QNN), to adversarial attacks using a malware dataset. We utilize a software supply chain attack dataset known as ClaMP and develop two distinct models for QNN and NN, employing Pennylane for quantum implementations and TensorFlow and Keras for traditional implementations. Our methodology involves crafting adversarial samples by introducing random noise to a small portion of the dataset and evaluating the impact on the models performance using accuracy, precision, recall, and F1 score metrics. Based on our observations, both M",
    "path": "papers/23/05/2305.19593.json",
    "total_tokens": 987,
    "translated_title": "使用恶意软件数据集探究机器学习和量子机器学习的易受攻击漏洞: 一项比较分析",
    "translated_abstract": "机器学习和量子机器学习已经展现了在各领域中处理复杂问题的巨大潜力。然而，当在安全敏感的应用中部署这些系统时，其易受攻击的特性引起了人们的担忧。本研究使用恶意软件数据集进行比较分析，探究传统神经网络和量子神经网络对于攻击的易感性。我们使用名为ClaMP的软件供应链攻击数据集，分别为QNN和NN开发两个不同的模型，并使用Pennylane进行量子实现，使用TensorFlow和Keras进行传统实现。我们的方法是通过向数据集的一小部分引入随机噪声来创建对抗性样本，并使用准确度、精确度、召回率和F1分数指标评估其对模型性能的影响。基于我们的观察结果，我们得出结论，当训练使用恶意软件数据集时，机器学习和量子机器学习模型都容易受到攻击。在我们的比较分析中发现，与传统NN相比，QNN表现出更高的易感性。",
    "tldr": "本研究比较了传统神经网络和量子神经网络在恶意软件数据集上对于攻击的易感性，并发现QNN表现出更高的易感性。",
    "en_tdlr": "This study explores the vulnerability of conventional neural networks (NNs) and quantum neural networks (QNNs) to adversarial attacks using a malware dataset, and finds that QNNs are more susceptible to attacks than NNs."
}