{
    "title": "Chain-of-Skills: A Configurable Model for Open-domain Question Answering. (arXiv:2305.03130v1 [cs.CL])",
    "abstract": "The retrieval model is an indispensable component for real-world knowledge-intensive tasks, e.g., open-domain question answering (ODQA). As separate retrieval skills are annotated for different datasets, recent work focuses on customized methods, limiting the model transferability and scalability. In this work, we propose a modular retriever where individual modules correspond to key skills that can be reused across datasets. Our approach supports flexible skill configurations based on the target domain to boost performance. To mitigate task interference, we design a novel modularization parameterization inspired by sparse Transformer. We demonstrate that our model can benefit from self-supervised pretraining on Wikipedia and fine-tuning using multiple ODQA datasets, both in a multi-task fashion. Our approach outperforms recent self-supervised retrievers in zero-shot evaluations and achieves state-of-the-art fine-tuned retrieval performance on NQ, HotpotQA and OTT-QA.",
    "link": "http://arxiv.org/abs/2305.03130",
    "context": "Title: Chain-of-Skills: A Configurable Model for Open-domain Question Answering. (arXiv:2305.03130v1 [cs.CL])\nAbstract: The retrieval model is an indispensable component for real-world knowledge-intensive tasks, e.g., open-domain question answering (ODQA). As separate retrieval skills are annotated for different datasets, recent work focuses on customized methods, limiting the model transferability and scalability. In this work, we propose a modular retriever where individual modules correspond to key skills that can be reused across datasets. Our approach supports flexible skill configurations based on the target domain to boost performance. To mitigate task interference, we design a novel modularization parameterization inspired by sparse Transformer. We demonstrate that our model can benefit from self-supervised pretraining on Wikipedia and fine-tuning using multiple ODQA datasets, both in a multi-task fashion. Our approach outperforms recent self-supervised retrievers in zero-shot evaluations and achieves state-of-the-art fine-tuned retrieval performance on NQ, HotpotQA and OTT-QA.",
    "path": "papers/23/05/2305.03130.json",
    "total_tokens": 945,
    "translated_title": "Chain-of-Skills: 一个可配置的用于开放领域问答的模型",
    "translated_abstract": "在实际的知识密集任务中，如开放领域问答（ODQA），检索模型是不可或缺的组件。由于不同数据集的注释有着不同的检索技能，近期的工作侧重于定制方法，限制了模型的可转移性和可扩展性。在这项工作中，我们提出了一种模块化检索器，其中各个模块对应于可以在数据集之间重复使用的关键技能。我们的方法支持基于目标领域的灵活技能配置，以提高性能。为了减轻任务干扰，我们设计了一种受稀疏 Transformer 启发的新型模块化参数化方法。我们证明了我们的模型可以在维基百科上进行自我监督预训练，并在多个 ODQA 数据集上进行微调，具有多任务的特点。我们的方法在零样例评估中优于最近的自我监督检索器，并在 NQ、HotpotQA 和 OTT-QA 上获得了最先进的微调检索性能。",
    "tldr": "本论文提出了一种模块化检索器可以在数据集间重复使用，支持针对目标领域的灵活技能配置，通过自我监督预训练和微调多个 ODQA 数据集，实现了最新颖的微调检索性能。",
    "en_tdlr": "This paper proposes a modular retriever that can be reused across datasets, and supports flexible skill configurations based on the target domain to boost performance. By self-supervised pretraining on Wikipedia and fine-tuning using multiple ODQA datasets in a multi-task fashion, the approach achieves state-of-the-art fine-tuned retrieval performance on NQ, HotpotQA and OTT-QA, and outperforms recent self-supervised retrievers in zero-shot evaluations."
}