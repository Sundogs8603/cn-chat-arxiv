{
    "title": "Accelerating Neural Self-Improvement via Bootstrapping. (arXiv:2305.01547v1 [cs.LG])",
    "abstract": "Few-shot learning with sequence-processing neural networks (NNs) has recently attracted a new wave of attention in the context of large language models. In the standard N-way K-shot learning setting, an NN is explicitly optimised to learn to classify unlabelled inputs by observing a sequence of NK labelled examples. This pressures the NN to learn a learning algorithm that achieves optimal performance, given the limited number of training examples. Here we study an auxiliary loss that encourages further acceleration of few-shot learning, by applying recently proposed bootstrapped meta-learning to NN few-shot learners: we optimise the K-shot learner to match its own performance achievable by observing more than NK examples, using only NK examples. Promising results are obtained on the standard Mini-ImageNet dataset. Our code is public.",
    "link": "http://arxiv.org/abs/2305.01547",
    "context": "Title: Accelerating Neural Self-Improvement via Bootstrapping. (arXiv:2305.01547v1 [cs.LG])\nAbstract: Few-shot learning with sequence-processing neural networks (NNs) has recently attracted a new wave of attention in the context of large language models. In the standard N-way K-shot learning setting, an NN is explicitly optimised to learn to classify unlabelled inputs by observing a sequence of NK labelled examples. This pressures the NN to learn a learning algorithm that achieves optimal performance, given the limited number of training examples. Here we study an auxiliary loss that encourages further acceleration of few-shot learning, by applying recently proposed bootstrapped meta-learning to NN few-shot learners: we optimise the K-shot learner to match its own performance achievable by observing more than NK examples, using only NK examples. Promising results are obtained on the standard Mini-ImageNet dataset. Our code is public.",
    "path": "papers/23/05/2305.01547.json",
    "total_tokens": 788,
    "translated_title": "通过自助模型引导加速神经网络的自我提高",
    "translated_abstract": "在大型语言模型的背景下，序列处理神经网络（NN）的少样本学习最近引起了新的关注。在标准的N类K样本学习环境中，NN被明确优化为学习通过观察序列NK标记示例来对未标记的输入进行分类，从而迫使NN学习实现最佳性能的学习算法，考虑到有限的训练示例。在这里，我们研究了一种辅助损失函数，通过将最近提出的自助元学习应用于NN少样本学习器，鼓励进一步加速少样本学习：我们通过仅仅利用NK个样本，将K样本学习器的性能优化到其可以观察到多于NK个样本时的性能，从而获得了有希望的结果，在标准的Mini-ImageNet数据集上测试，我们的代码是公开的。",
    "tldr": "本研究通过引入自助元学习加速神经网络的自我提高，提高了神经网络在少样本学习中的表现。",
    "en_tdlr": "This study introduces bootstrapped meta-learning to accelerate neural self-improvement, boosting the performance of neural networks in few-shot learning."
}