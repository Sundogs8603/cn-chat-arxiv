{
    "title": "VideoOFA: Two-Stage Pre-Training for Video-to-Text Generation. (arXiv:2305.03204v1 [cs.CV])",
    "abstract": "We propose a new two-stage pre-training framework for video-to-text generation tasks such as video captioning and video question answering: A generative encoder-decoder model is first jointly pre-trained on massive image-text data to learn fundamental vision-language concepts, and then adapted to video data in an intermediate video-text pre-training stage to learn video-specific skills such as spatio-temporal reasoning. As a result, our VideoOFA model achieves new state-of-the-art performance on four Video Captioning benchmarks, beating prior art by an average of 9.7 points in CIDEr score. It also outperforms existing models on two open-ended Video Question Answering datasets, showcasing its generalization capability as a universal video-to-text model.",
    "link": "http://arxiv.org/abs/2305.03204",
    "context": "Title: VideoOFA: Two-Stage Pre-Training for Video-to-Text Generation. (arXiv:2305.03204v1 [cs.CV])\nAbstract: We propose a new two-stage pre-training framework for video-to-text generation tasks such as video captioning and video question answering: A generative encoder-decoder model is first jointly pre-trained on massive image-text data to learn fundamental vision-language concepts, and then adapted to video data in an intermediate video-text pre-training stage to learn video-specific skills such as spatio-temporal reasoning. As a result, our VideoOFA model achieves new state-of-the-art performance on four Video Captioning benchmarks, beating prior art by an average of 9.7 points in CIDEr score. It also outperforms existing models on two open-ended Video Question Answering datasets, showcasing its generalization capability as a universal video-to-text model.",
    "path": "papers/23/05/2305.03204.json",
    "total_tokens": 805,
    "translated_title": "VideoOFA：用于视频到文本生成的两阶段预训练模型",
    "translated_abstract": "我们提出了一个新的两阶段预训练框架，用于视频字幕和视频问答等任务的视频到文本生成：首先，一个生成式编码器-解码器模型在大规模的图像-文本数据上进行联合预训练，以学习基本的视觉语言概念，然后在一个中间的视频-文本预训练阶段对视频数据进行调整，以学习视频特定的技能，如时空推理。因此，我们的VideoOFA模型在四个视频字幕基准测试中实现了新的最先进的性能，在CIDEr分数上平均击败了先前的工艺水平9.7个收益点。它还在两个开放式视频问答数据集上表现优异，展示了它作为通用视频到文本模型的泛化能力。",
    "tldr": "本文提出了一个用于视频到文本生成的两阶段预训练模型，能够学习视频特定的技能，取得了四项视频字幕基准测试的最先进性能，同时在两个开放式视频问答数据集上表现优异。",
    "en_tdlr": "This paper proposes a two-stage pre-training framework for video-to-text generation tasks, which can learn video-specific skills and achieves state-of-the-art performance on four video captioning benchmarks and two open-ended video question answering datasets."
}