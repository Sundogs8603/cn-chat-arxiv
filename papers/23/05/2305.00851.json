{
    "title": "Revisiting Robustness in Graph Machine Learning. (arXiv:2305.00851v2 [cs.LG] UPDATED)",
    "abstract": "Many works show that node-level predictions of Graph Neural Networks (GNNs) are unrobust to small, often termed adversarial, changes to the graph structure. However, because manual inspection of a graph is difficult, it is unclear if the studied perturbations always preserve a core assumption of adversarial examples: that of unchanged semantic content. To address this problem, we introduce a more principled notion of an adversarial graph, which is aware of semantic content change. Using Contextual Stochastic Block Models (CSBMs) and real-world graphs, our results uncover: $i)$ for a majority of nodes the prevalent perturbation models include a large fraction of perturbed graphs violating the unchanged semantics assumption; $ii)$ surprisingly, all assessed GNNs show over-robustness - that is robustness beyond the point of semantic change. We find this to be a complementary phenomenon to adversarial examples and show that including the label-structure of the training graph into the infer",
    "link": "http://arxiv.org/abs/2305.00851",
    "context": "Title: Revisiting Robustness in Graph Machine Learning. (arXiv:2305.00851v2 [cs.LG] UPDATED)\nAbstract: Many works show that node-level predictions of Graph Neural Networks (GNNs) are unrobust to small, often termed adversarial, changes to the graph structure. However, because manual inspection of a graph is difficult, it is unclear if the studied perturbations always preserve a core assumption of adversarial examples: that of unchanged semantic content. To address this problem, we introduce a more principled notion of an adversarial graph, which is aware of semantic content change. Using Contextual Stochastic Block Models (CSBMs) and real-world graphs, our results uncover: $i)$ for a majority of nodes the prevalent perturbation models include a large fraction of perturbed graphs violating the unchanged semantics assumption; $ii)$ surprisingly, all assessed GNNs show over-robustness - that is robustness beyond the point of semantic change. We find this to be a complementary phenomenon to adversarial examples and show that including the label-structure of the training graph into the infer",
    "path": "papers/23/05/2305.00851.json",
    "total_tokens": 1120,
    "translated_title": "重访图机器学习的鲁棒性",
    "translated_abstract": "许多研究表明，图神经网络（GNN）的节点层面预测对图结构的微小更改——通常称为对抗性更改——不稳健。然而，由于手动检查图形困难，不清楚研究的扰动是否总是保留对抗性示例的核心假设:即不改变语义内容。为解决这个问题，我们引入了一种更基本的对抗性图形概念，它知道语义内容的改变。通过使用语境随机块模型（CSBMs）和真实世界的图表，我们的结果发现：i）对于大多数节点，主要的扰动模型包括大量扰动图违反了未改变的语义假设；ii）令人惊讶的是，所有评估的GNN都表现出过度鲁棒性——即超出语义变化点的鲁棒性。我们发现这是对抗性示例的补充现象，并显示将训练图的标签-结构包括在推理的图中可以避免这种过度鲁棒性。最后，提出了一种基于所提出的对抗性概念的鲁棒性测试，并表明它与最终的主动学习性能相关。",
    "tldr": "GNNs在节点层面预测中可能会因图结构小的更改而导致不稳健。本文提出了对抗性图的概念，并发现所有评估的GNN都表现出过度鲁棒性，包括超出语义变化点。在推理的图中包括训练图的标签-结构可以避免过度鲁棒性。基于对抗性概念的鲁棒性测试与最终主动学习表现相关。",
    "en_tdlr": "The article revisits the issue of unrobustness in Graph Neural Networks (GNNs) at the node-level prediction that results from small and supposedly adversarial changes to graph structure. The introduced concept of an adversarial graph is shown to avoid over-robustness in GNNs by safeguarding against semantic change while including the label-structure of the training graph in the inferred graph."
}