{
    "title": "Phase transitions in the mini-batch size for sparse and dense neural networks. (arXiv:2305.06435v1 [cond-mat.dis-nn])",
    "abstract": "The use of mini-batches of data in training artificial neural networks is nowadays very common. Despite its broad usage, theories explaining quantitatively how large or small the optimal mini-batch size should be are missing. This work presents a systematic attempt at understanding the role of the mini-batch size in training two-layer neural networks. Working in the teacher-student scenario, with a sparse teacher, and focusing on tasks of different complexity, we quantify the effects of changing the mini-batch size $m$. We find that often the generalization performances of the student strongly depend on $m$ and may undergo sharp phase transitions at a critical value $m_c$, such that for $m<m_c$ the training process fails, while for $m>m_c$ the student learns perfectly or generalizes very well the teacher. Phase transitions are induced by collective phenomena firstly discovered in statistical mechanics and later observed in many fields of science. Finding a phase transition varying the ",
    "link": "http://arxiv.org/abs/2305.06435",
    "context": "Title: Phase transitions in the mini-batch size for sparse and dense neural networks. (arXiv:2305.06435v1 [cond-mat.dis-nn])\nAbstract: The use of mini-batches of data in training artificial neural networks is nowadays very common. Despite its broad usage, theories explaining quantitatively how large or small the optimal mini-batch size should be are missing. This work presents a systematic attempt at understanding the role of the mini-batch size in training two-layer neural networks. Working in the teacher-student scenario, with a sparse teacher, and focusing on tasks of different complexity, we quantify the effects of changing the mini-batch size $m$. We find that often the generalization performances of the student strongly depend on $m$ and may undergo sharp phase transitions at a critical value $m_c$, such that for $m<m_c$ the training process fails, while for $m>m_c$ the student learns perfectly or generalizes very well the teacher. Phase transitions are induced by collective phenomena firstly discovered in statistical mechanics and later observed in many fields of science. Finding a phase transition varying the ",
    "path": "papers/23/05/2305.06435.json",
    "total_tokens": 937,
    "translated_title": "稀疏和密集神经网络中的小批量大小的相变",
    "translated_abstract": "在训练人工神经网络时，使用小批量数据现在非常普遍。尽管已经广泛使用，但缺少定量解释最佳小批量大小应该是多大的理论。本文尝试系统地理解小批量大小在训练两层神经网络中的作用。在教师-学生情境下，使用稀疏教师，并聚焦于不同复杂度的任务，我们量化了改变小批量大小m的影响。我们发现，通常情况下，学生的泛化性能强烈依赖于m，并且可能在临界值mc处经历尖锐的相变，这样当m< mc时，训练过程失败，而当m> mc时，学生可以完美地学习或很好地泛化教师。相变是由统计力学首次发现的集体现象，并在许多科学领域观察到。找到在深度学习中改变小批量大小的相变，可以阐明神经网络优化的基本机制。",
    "tldr": "本文系统地研究了小批量大小对稀疏和密集神经网络训练的影响，发现在临界值处会出现尖锐的相变，阐明了神经网络优化的基本机制。",
    "en_tdlr": "This paper systematically studies the impact of mini-batch size on the training of sparse and dense neural networks, finding sharp phase transitions at the critical value, shedding light on the fundamental mechanisms underlying the optimization of neural networks."
}