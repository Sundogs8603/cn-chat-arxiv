{
    "title": "Investigating Forgetting in Pre-Trained Representations Through Continual Learning. (arXiv:2305.05968v1 [cs.CL])",
    "abstract": "Representation forgetting refers to the drift of contextualized representations during continual training. Intuitively, the representation forgetting can influence the general knowledge stored in pre-trained language models (LMs), but the concrete effect is still unclear. In this paper, we study the effect of representation forgetting on the generality of pre-trained language models, i.e. the potential capability for tackling future downstream tasks. Specifically, we design three metrics, including overall generality destruction (GD), syntactic knowledge forgetting (SynF), and semantic knowledge forgetting (SemF), to measure the evolution of general knowledge in continual learning. With extensive experiments, we find that the generality is destructed in various pre-trained LMs, and syntactic and semantic knowledge is forgotten through continual learning. Based on our experiments and analysis, we further get two insights into alleviating general knowledge forgetting: 1) training on gene",
    "link": "http://arxiv.org/abs/2305.05968",
    "context": "Title: Investigating Forgetting in Pre-Trained Representations Through Continual Learning. (arXiv:2305.05968v1 [cs.CL])\nAbstract: Representation forgetting refers to the drift of contextualized representations during continual training. Intuitively, the representation forgetting can influence the general knowledge stored in pre-trained language models (LMs), but the concrete effect is still unclear. In this paper, we study the effect of representation forgetting on the generality of pre-trained language models, i.e. the potential capability for tackling future downstream tasks. Specifically, we design three metrics, including overall generality destruction (GD), syntactic knowledge forgetting (SynF), and semantic knowledge forgetting (SemF), to measure the evolution of general knowledge in continual learning. With extensive experiments, we find that the generality is destructed in various pre-trained LMs, and syntactic and semantic knowledge is forgotten through continual learning. Based on our experiments and analysis, we further get two insights into alleviating general knowledge forgetting: 1) training on gene",
    "path": "papers/23/05/2305.05968.json",
    "total_tokens": 888,
    "translated_title": "通过持续学习研究预训练表示中的遗忘现象",
    "translated_abstract": "表示遗忘是指在不断的训练过程中，上下文表示的漂移。直观地说，表示遗忘会影响预训练语言模型中储存的通用知识，但具体影响仍不清楚。本文研究表示遗忘对预训练语言模型的通用性的影响，即处理未来下游任务的潜在能力。我们设计了三个度量标准，包括总体通用性破坏（GD）、句法知识遗忘（SynF）和语义知识遗忘（SemF），以衡量通用知识在持续学习中的演变。通过大量实验，我们发现多种预训练语言模型的通用性遭到破坏，而且语法和语义知识在持续学习中也会遗忘。基于我们的实验和分析，我们进一步得出两个减轻通用知识遗忘的见解：1）在通用和特定任务之间平衡， 2）在序列到序列模型中加入辅助任务。",
    "tldr": "本文研究了在持续学习中的表示遗忘对预训练语言模型通用性的影响，并提出了两个减轻该遗忘的见解。",
    "en_tdlr": "This paper investigates the impact of representation forgetting in continual learning on the generality of pre-trained language models, and proposes two insights into alleviating this forgetting."
}