{
    "title": "BiasX: \"Thinking Slow\" in Toxic Content Moderation with Explanations of Implied Social Biases. (arXiv:2305.13589v1 [cs.CL])",
    "abstract": "Toxicity annotators and content moderators often default to mental shortcuts when making decisions. This can lead to subtle toxicity being missed, and seemingly toxic but harmless content being over-detected. We introduce BiasX, a framework that enhances content moderation setups with free-text explanations of statements' implied social biases, and explore its effectiveness through a large-scale crowdsourced user study. We show that indeed, participants substantially benefit from explanations for correctly identifying subtly (non-)toxic content. The quality of explanations is critical: imperfect machine-generated explanations (+2.4% on hard toxic examples) help less compared to expert-written human explanations (+7.2%). Our results showcase the promise of using free-text explanations to encourage more thoughtful toxicity moderation.",
    "link": "http://arxiv.org/abs/2305.13589",
    "context": "Title: BiasX: \"Thinking Slow\" in Toxic Content Moderation with Explanations of Implied Social Biases. (arXiv:2305.13589v1 [cs.CL])\nAbstract: Toxicity annotators and content moderators often default to mental shortcuts when making decisions. This can lead to subtle toxicity being missed, and seemingly toxic but harmless content being over-detected. We introduce BiasX, a framework that enhances content moderation setups with free-text explanations of statements' implied social biases, and explore its effectiveness through a large-scale crowdsourced user study. We show that indeed, participants substantially benefit from explanations for correctly identifying subtly (non-)toxic content. The quality of explanations is critical: imperfect machine-generated explanations (+2.4% on hard toxic examples) help less compared to expert-written human explanations (+7.2%). Our results showcase the promise of using free-text explanations to encourage more thoughtful toxicity moderation.",
    "path": "papers/23/05/2305.13589.json",
    "total_tokens": 937,
    "translated_title": "BiasX：使用隐含社会偏见解释在有害内容审查中\"缓慢思考\"",
    "translated_abstract": "在有害内容的注释和审查中，注释员和审查员经常采用心理快捷方式做出决策。这可能会导致错过微妙的有害性，而看似有害但无害的内容被过度检测。我们介绍了BiasX，这是一个框架，通过陈述的隐含社会偏见的自由文本解释来增强内容审查设置，并通过大规模的众包用户研究来探索其有效性。我们展示了参与者通过解释正确识别微妙的（非）有害内容的实际获益。解释的质量至关重要:不完美的机器生成的解释（+2.4%在难以处理的有害样例上）相比专家撰写的人工解释（+7.2%）帮助较少。我们的结果展示了使用自由文本解释鼓励更加深思熟虑的有毒性审查的希望。",
    "tldr": "BiasX是一个框架，通过输入自由文本解释的隐含社会偏见来提高内容审核的质量。经过大规模的用户研究，我们展示了解释对于准确识别建议的有害内容的微妙程度有很大的帮助。机器生成的解释仅能提高2.4％的有效性，而人工撰写的解释能够提高7.2％的有效性。",
    "en_tdlr": "BiasX is a framework that enhances content moderation by inputting free-text explanations of implied social biases. Through a large-scale user study, the effectiveness of explanations in correctly identifying subtly (non-)toxic content was demonstrated. Imperfect machine-generated explanations only improved effectiveness by 2.4%, whereas expert-written human explanations improved it by 7.2%."
}