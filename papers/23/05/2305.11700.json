{
    "title": "Exploring the Upper Limits of Text-Based Collaborative Filtering Using Large Language Models: Discoveries and Insights. (arXiv:2305.11700v1 [cs.IR])",
    "abstract": "Text-based collaborative filtering (TCF) has become the mainstream approach for text and news recommendation, utilizing text encoders, also known as language models (LMs), to represent items. However, existing TCF models primarily focus on using small or medium-sized LMs. It remains uncertain what impact replacing the item encoder with one of the largest and most powerful LMs, such as the 175-billion parameter GPT-3 model, would have on recommendation performance. Can we expect unprecedented results? To this end, we conduct an extensive series of experiments aimed at exploring the performance limits of the TCF paradigm. Specifically, we increase the size of item encoders from one hundred million to one hundred billion to reveal the scaling limits of the TCF paradigm. We then examine whether these extremely large LMs could enable a universal item representation for the recommendation task. Furthermore, we compare the performance of the TCF paradigm utilizing the most powerful LMs to the",
    "link": "http://arxiv.org/abs/2305.11700",
    "context": "Title: Exploring the Upper Limits of Text-Based Collaborative Filtering Using Large Language Models: Discoveries and Insights. (arXiv:2305.11700v1 [cs.IR])\nAbstract: Text-based collaborative filtering (TCF) has become the mainstream approach for text and news recommendation, utilizing text encoders, also known as language models (LMs), to represent items. However, existing TCF models primarily focus on using small or medium-sized LMs. It remains uncertain what impact replacing the item encoder with one of the largest and most powerful LMs, such as the 175-billion parameter GPT-3 model, would have on recommendation performance. Can we expect unprecedented results? To this end, we conduct an extensive series of experiments aimed at exploring the performance limits of the TCF paradigm. Specifically, we increase the size of item encoders from one hundred million to one hundred billion to reveal the scaling limits of the TCF paradigm. We then examine whether these extremely large LMs could enable a universal item representation for the recommendation task. Furthermore, we compare the performance of the TCF paradigm utilizing the most powerful LMs to the",
    "path": "papers/23/05/2305.11700.json",
    "total_tokens": 1102,
    "translated_title": "探究利用大型语言模型探索基于文本的协同过滤的极限：发现和认识",
    "translated_abstract": "基于文本的协同过滤成为现今文本和新闻推荐的主流方法，利用文本编码器或语言模型(LMs)表示物品。然而，现有的文本协同过滤模型主要集中在使用中小型的LMs上，如果将物品编码器替换为最大最强大的1750亿参数的GPT-3模型，将会对推荐性能产生什么影响尚不确定。作者开展了一系列实验，探索TCF程序的性能极限。具体来说，作者将物品编码器规模从一亿扩大到一百亿以揭示TCF程序的扩展极限，同时还探究了使用超大LMs是否能实现推荐任务的通用物品表示方法。此外，作者比较了使用最强大的LMs和中等LMs实现的基于文本协同过滤的性能差异。",
    "tldr": "本论文探究了在基于文本的协同过滤中使用大型语言模型所能带来的性能提升，并揭示了TCF程序扩展的极限。研究人员比较了使用不同大小的语言模型在基于文本的协同过滤算法中的性能表现。",
    "en_tdlr": "This paper explores the performance improvement that can be achieved in text-based collaborative filtering using large language models, and reveals the scaling limits of TCF. The researchers compare the performance of TCF using different sizes of language models in the recommendation task."
}