{
    "title": "SANE: The phases of gradient descent through Sharpness Adjusted Number of Effective parameters. (arXiv:2305.18490v1 [cs.LG])",
    "abstract": "Modern neural networks are undeniably successful. Numerous studies have investigated how the curvature of loss landscapes can affect the quality of solutions. In this work we consider the Hessian matrix during network training. We reiterate the connection between the number of \"well-determined\" or \"effective\" parameters and the generalisation performance of neural nets, and we demonstrate its use as a tool for model comparison. By considering the local curvature, we propose Sharpness Adjusted Number of Effective parameters (SANE), a measure of effective dimensionality for the quality of solutions. We show that SANE is robust to large learning rates, which represent learning regimes that are attractive but (in)famously unstable. We provide evidence and characterise the Hessian shifts across \"loss basins\" at large learning rates. Finally, extending our analysis to deeper neural networks, we provide an approximation to the full-network Hessian, exploiting the natural ordering of neural we",
    "link": "http://arxiv.org/abs/2305.18490",
    "context": "Title: SANE: The phases of gradient descent through Sharpness Adjusted Number of Effective parameters. (arXiv:2305.18490v1 [cs.LG])\nAbstract: Modern neural networks are undeniably successful. Numerous studies have investigated how the curvature of loss landscapes can affect the quality of solutions. In this work we consider the Hessian matrix during network training. We reiterate the connection between the number of \"well-determined\" or \"effective\" parameters and the generalisation performance of neural nets, and we demonstrate its use as a tool for model comparison. By considering the local curvature, we propose Sharpness Adjusted Number of Effective parameters (SANE), a measure of effective dimensionality for the quality of solutions. We show that SANE is robust to large learning rates, which represent learning regimes that are attractive but (in)famously unstable. We provide evidence and characterise the Hessian shifts across \"loss basins\" at large learning rates. Finally, extending our analysis to deeper neural networks, we provide an approximation to the full-network Hessian, exploiting the natural ordering of neural we",
    "path": "papers/23/05/2305.18490.json",
    "total_tokens": 891,
    "translated_title": "基于锐度调整的有效参数数量梯度下降SANE算法",
    "translated_abstract": "现代神经网络非常成功。许多研究已经调查了损失面曲率如何影响解的质量。本文考虑神经网络训练期间的Hessian矩阵。我们重申了“确定良好”或“有效”参数的数量与神经网络泛化性能之间的联系，并将其演示为模型比较工具。通过考虑局部曲率，我们提出了Sharpness Adjusted Number of Effective parameters (SANE)算法，这是一种针对解质量的有效维数度量。我们表明，SANE对大学习率具有鲁棒性，这代表了有吸引力但声名狼藉的不稳定学习区域。我们提供证据并表征了大学习率下“损失盆地”的Hessian矩阵变化。最后，扩展我们的分析到更深的神经网络中，我们提供了对全网络Hessian矩阵的近似，利用神经元自然排序。",
    "tldr": "本文提出了一种基于锐度调整的的有效参数数量梯度下降SANE算法，用于解质量的有效维数度量，并且对大学习率也有较好的鲁棒性。"
}