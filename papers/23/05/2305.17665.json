{
    "title": "Acceleration of stochastic gradient descent with momentum by averaging: finite-sample rates and asymptotic normality. (arXiv:2305.17665v1 [cs.LG])",
    "abstract": "Stochastic gradient descent with momentum (SGDM) has been widely used in many machine learning and statistical applications. Despite the observed empirical benefits of SGDM over traditional SGD, the theoretical understanding of the role of momentum for different learning rates in the optimization process remains widely open. We analyze the finite-sample convergence rate of SGDM under the strongly convex settings and show that, with a large batch size, the mini-batch SGDM converges faster than mini-batch SGD to a neighborhood of the optimal value. Furthermore, we analyze the Polyak-averaging version of the SGDM estimator, establish its asymptotic normality, and justify its asymptotic equivalence to the averaged SGD.",
    "link": "http://arxiv.org/abs/2305.17665",
    "context": "Title: Acceleration of stochastic gradient descent with momentum by averaging: finite-sample rates and asymptotic normality. (arXiv:2305.17665v1 [cs.LG])\nAbstract: Stochastic gradient descent with momentum (SGDM) has been widely used in many machine learning and statistical applications. Despite the observed empirical benefits of SGDM over traditional SGD, the theoretical understanding of the role of momentum for different learning rates in the optimization process remains widely open. We analyze the finite-sample convergence rate of SGDM under the strongly convex settings and show that, with a large batch size, the mini-batch SGDM converges faster than mini-batch SGD to a neighborhood of the optimal value. Furthermore, we analyze the Polyak-averaging version of the SGDM estimator, establish its asymptotic normality, and justify its asymptotic equivalence to the averaged SGD.",
    "path": "papers/23/05/2305.17665.json",
    "total_tokens": 797,
    "translated_title": "通过平均加速动量随机梯度下降：有限样本速率和渐近正态性",
    "translated_abstract": "动量随机梯度下降（SGDM）被广泛应用于许多机器学习和统计应用中。尽管SGDM相对于传统的随机梯度下降具有观察到的经验优势，但在优化过程中动量对不同学习率的作用的理论理解仍然是开放的。我们在强凸设置下分析了SGDM的有限样本收敛速率，并表明在较大的批量大小下，小批量SGDM比小批量SGD更快地收敛到最优值的邻域。此外，我们分析了SGDM估计量的Polyak平均版本，建立了它的渐近正态性，并证明了它与平均SGD的渐近等价性。",
    "tldr": "研究了动量随机梯度下降（SGDM）和其Polyak-averaging版本的特性，表明在较大的批量大小下，小批量SGDM比小批量SGD更快地收敛到最优值的邻域。",
    "en_tdlr": "Explored the properties of stochastic gradient descent with momentum (SGDM) and its Polyak-averaging version, which indicates that mini-batch SGDM converges faster than mini-batch SGD to a neighborhood of the optimal value with a large batch size."
}