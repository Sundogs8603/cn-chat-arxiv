{
    "title": "Detecting and Mitigating Indirect Stereotypes in Word Embeddings. (arXiv:2305.14574v1 [cs.CL])",
    "abstract": "Societal biases in the usage of words, including harmful stereotypes, are frequently learned by common word embedding methods. These biases manifest not only between a word and an explicit marker of its stereotype, but also between words that share related stereotypes. This latter phenomenon, sometimes called \"indirect bias,'' has resisted prior attempts at debiasing. In this paper, we propose a novel method called Biased Indirect Relationship Modification (BIRM) to mitigate indirect bias in distributional word embeddings by modifying biased relationships between words before embeddings are learned. This is done by considering how the co-occurrence probability of a given pair of words changes in the presence of words marking an attribute of bias, and using this to average out the effect of a bias attribute. To evaluate this method, we perform a series of common tests and demonstrate that measures of bias in the word embeddings are reduced in exchange for minor reduction in the semantic",
    "link": "http://arxiv.org/abs/2305.14574",
    "context": "Title: Detecting and Mitigating Indirect Stereotypes in Word Embeddings. (arXiv:2305.14574v1 [cs.CL])\nAbstract: Societal biases in the usage of words, including harmful stereotypes, are frequently learned by common word embedding methods. These biases manifest not only between a word and an explicit marker of its stereotype, but also between words that share related stereotypes. This latter phenomenon, sometimes called \"indirect bias,'' has resisted prior attempts at debiasing. In this paper, we propose a novel method called Biased Indirect Relationship Modification (BIRM) to mitigate indirect bias in distributional word embeddings by modifying biased relationships between words before embeddings are learned. This is done by considering how the co-occurrence probability of a given pair of words changes in the presence of words marking an attribute of bias, and using this to average out the effect of a bias attribute. To evaluate this method, we perform a series of common tests and demonstrate that measures of bias in the word embeddings are reduced in exchange for minor reduction in the semantic",
    "path": "papers/23/05/2305.14574.json",
    "total_tokens": 963,
    "translated_title": "探测和减少词嵌入中的间接刻板印象",
    "translated_abstract": "常见的词嵌入方法会学习到在使用单词时存在的社会偏见和有害刻板印象。这些偏见不仅存在于词本身和其明确的刻板印象标记之间，而且还存在于共享相关刻板印象的词之间。这种称为“间接偏见”的现象已经阻碍了之前的试图消除这些偏见的尝试。本文提出了一种称为“有偏间接关系修改（BIRM）”的新方法，在嵌入词之前通过修改单词之间的有偏关系来减轻分布式词嵌入中的间接偏见。方法是通过考虑在标记偏见属性的单词存在的情况下给定一对单词的共现概率如何变化，并利用这一点平均偏见属性的影响。为了评估这种方法，我们进行了一系列常见的测试，并证明了在稍微减少语义方面的同时，减少了词嵌入中的偏见测量。",
    "tldr": "本文提出了一种新方法，称为有偏间接关系修改（BIRM），以减轻分布式词嵌入中的间接偏见，通过考虑标记偏见属性的单词在存在情况下给定一对单词的共现概率如何变化，并利用这一点平均偏见属性的影响。",
    "en_tdlr": "This paper proposes a novel method called Biased Indirect Relationship Modification (BIRM) to mitigate indirect bias in distributional word embeddings by modifying biased relationships between words before embeddings are learned, which considers how the co-occurrence probability of a given pair of words changes in the presence of words marking an attribute of bias, and using this to average out the effect of a bias attribute."
}