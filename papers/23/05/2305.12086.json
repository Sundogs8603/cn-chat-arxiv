{
    "title": "Prefix Propagation: Parameter-Efficient Tuning for Long Sequences. (arXiv:2305.12086v1 [cs.CL])",
    "abstract": "Parameter-efficient tuning aims to mitigate the large memory requirements of adapting pretrained language models for downstream tasks. For example, one popular method, prefix-tuning, prepends trainable tokens to sequences while freezing the rest of the model's parameters. Although such models attain comparable performance with fine-tuning when applied to sequences with short to moderate lengths, we show their inferior performance when modelling long sequences. To bridge this gap, we propose prefix-propagation, a simple but effective approach that conditions prefixes on previous hidden states. We empirically demonstrate that prefix-propagation outperforms prefix-tuning across long-document tasks, while using 50% fewer parameters. To further investigate the proposed architecture, we also show its advantage in calibration, and perform additional study on its relationship with kernel attention. To the best of our knowledge, this work is the first to focus on parameter-efficient learning fo",
    "link": "http://arxiv.org/abs/2305.12086",
    "context": "Title: Prefix Propagation: Parameter-Efficient Tuning for Long Sequences. (arXiv:2305.12086v1 [cs.CL])\nAbstract: Parameter-efficient tuning aims to mitigate the large memory requirements of adapting pretrained language models for downstream tasks. For example, one popular method, prefix-tuning, prepends trainable tokens to sequences while freezing the rest of the model's parameters. Although such models attain comparable performance with fine-tuning when applied to sequences with short to moderate lengths, we show their inferior performance when modelling long sequences. To bridge this gap, we propose prefix-propagation, a simple but effective approach that conditions prefixes on previous hidden states. We empirically demonstrate that prefix-propagation outperforms prefix-tuning across long-document tasks, while using 50% fewer parameters. To further investigate the proposed architecture, we also show its advantage in calibration, and perform additional study on its relationship with kernel attention. To the best of our knowledge, this work is the first to focus on parameter-efficient learning fo",
    "path": "papers/23/05/2305.12086.json",
    "total_tokens": 667,
    "translated_title": "前缀传播: 针对长序列参数高效调整的方法",
    "translated_abstract": "参数高效调整旨在减轻针对下游任务调整预训练语言模型的大内存需求。本文提出前缀传播这一简单有效的方法来弥补目前前缀调整存在的问题，并展示前缀传播与前缀调整相比在处理长文档任务时具有更好的性能，所需参数也只有前者的一半。",
    "tldr": "前缀传播是一种针对长序列参数高效调整的方法，可实现50%减少参数且在处理长文档任务时具有更优性能。",
    "en_tdlr": "Prefix propagation is a parameter-efficient tuning method for long sequences that outperforms prefix-tuning with 50% fewer parameters, especially for long-document tasks. In addition, it has advantages in calibration and is related to kernel attention."
}