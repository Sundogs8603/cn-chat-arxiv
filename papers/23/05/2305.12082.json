{
    "title": "SneakyPrompt: Evaluating Robustness of Text-to-image Generative Models' Safety Filters. (arXiv:2305.12082v1 [cs.LG])",
    "abstract": "Text-to-image generative models such as Stable Diffusion and DALL$\\cdot$E 2 have attracted much attention since their publication due to their wide application in the real world. One challenging problem of text-to-image generative models is the generation of Not-Safe-for-Work (NSFW) content, e.g., those related to violence and adult. Therefore, a common practice is to deploy a so-called safety filter, which blocks NSFW content based on either text or image features. Prior works have studied the possible bypass of such safety filters. However, existing works are largely manual and specific to Stable Diffusion's official safety filter. Moreover, the bypass ratio of Stable Diffusion's safety filter is as low as 23.51% based on our evaluation.  In this paper, we propose the first automated attack framework, called SneakyPrompt, to evaluate the robustness of real-world safety filters in state-of-the-art text-to-image generative models. Our key insight is to search for alternative tokens in ",
    "link": "http://arxiv.org/abs/2305.12082",
    "context": "Title: SneakyPrompt: Evaluating Robustness of Text-to-image Generative Models' Safety Filters. (arXiv:2305.12082v1 [cs.LG])\nAbstract: Text-to-image generative models such as Stable Diffusion and DALL$\\cdot$E 2 have attracted much attention since their publication due to their wide application in the real world. One challenging problem of text-to-image generative models is the generation of Not-Safe-for-Work (NSFW) content, e.g., those related to violence and adult. Therefore, a common practice is to deploy a so-called safety filter, which blocks NSFW content based on either text or image features. Prior works have studied the possible bypass of such safety filters. However, existing works are largely manual and specific to Stable Diffusion's official safety filter. Moreover, the bypass ratio of Stable Diffusion's safety filter is as low as 23.51% based on our evaluation.  In this paper, we propose the first automated attack framework, called SneakyPrompt, to evaluate the robustness of real-world safety filters in state-of-the-art text-to-image generative models. Our key insight is to search for alternative tokens in ",
    "path": "papers/23/05/2305.12082.json",
    "total_tokens": 956,
    "translated_title": "SneakyPrompt：评估文本生成图像模型安全过滤器的鲁棒性",
    "translated_abstract": "文本生成图像模型，如Stable Diffusion和DALL$\\cdot$E 2等，由于它们在现实世界中的广泛应用而受到广泛关注。文本生成图像模型面临的一个挑战性问题是生成不安全内容，例如与暴力和成人相关的内容。因此，常见做法是部署所谓的安全过滤器，基于文本或图像特征阻止不安全内容。先前的工作研究了此类安全过滤器的可能绕过方式。然而，现有的工作在很大程度上是手动完成并专门针对Stable Diffusion官方的安全过滤器。此外，基于我们的评估，Stable Diffusion的安全过滤器的绕过比率仅为23.51％。在本文中，我们提出了第一个自动攻击框架SneakyPrompt，以评估最先进的文本生成图像模型中现实世界安全过滤器的鲁棒性。我们的关键洞见是搜索备选令牌来绕过安全过滤器。",
    "tldr": "本文提出了第一个自动攻击框架SneakyPrompt，以评估最先进的文本生成图像模型中的安全过滤器的鲁棒性，该框架的关键洞见是搜索备选令牌来绕过安全过滤器。",
    "en_tdlr": "SneakyPrompt is the first automated attack framework proposed in this paper to evaluate the robustness of safety filters in state-of-the-art text-to-image generative models, with the key insight being to search for alternative tokens to bypass the safety filters."
}