{
    "title": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning. (arXiv:2305.06500v1 [cs.CV])",
    "abstract": "General-purpose language models that can solve various language-domain tasks have emerged driven by the pre-training and instruction-tuning pipeline. However, building general-purpose vision-language models is challenging due to the increased task discrepancy introduced by the additional visual input. Although vision-language pre-training has been widely studied, vision-language instruction tuning remains relatively less explored. In this paper, we conduct a systematic and comprehensive study on vision-language instruction tuning based on the pre-trained BLIP-2 models. We gather a wide variety of 26 publicly available datasets, transform them into instruction tuning format and categorize them into two clusters for held-in instruction tuning and held-out zero-shot evaluation. Additionally, we introduce instruction-aware visual feature extraction, a crucial method that enables the model to extract informative features tailored to the given instruction. The resulting InstructBLIP models a",
    "link": "http://arxiv.org/abs/2305.06500",
    "context": "Title: InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning. (arXiv:2305.06500v1 [cs.CV])\nAbstract: General-purpose language models that can solve various language-domain tasks have emerged driven by the pre-training and instruction-tuning pipeline. However, building general-purpose vision-language models is challenging due to the increased task discrepancy introduced by the additional visual input. Although vision-language pre-training has been widely studied, vision-language instruction tuning remains relatively less explored. In this paper, we conduct a systematic and comprehensive study on vision-language instruction tuning based on the pre-trained BLIP-2 models. We gather a wide variety of 26 publicly available datasets, transform them into instruction tuning format and categorize them into two clusters for held-in instruction tuning and held-out zero-shot evaluation. Additionally, we introduce instruction-aware visual feature extraction, a crucial method that enables the model to extract informative features tailored to the given instruction. The resulting InstructBLIP models a",
    "path": "papers/23/05/2305.06500.json",
    "total_tokens": 870,
    "translated_title": "InstructBLIP: 通过指令调整实现通用视觉语言模型",
    "translated_abstract": "驱动了预训练和指令调整流程的通用语言模型已经出现，可以解决各种语言领域的任务。然而，由于增加了额外的视觉输入，建立通用视觉语言模型仍然具有挑战性。尽管视觉语言预训练已经广泛研究，但视觉语言指令调整仍然相对较少探讨。在本文中，我们基于预训练的BLIP-2模型对视觉语言指令调整进行了系统全面的研究。我们收集了26个公开可用的数据集，并将它们转换为指令调整格式并分类为两个集群，用于保持指令调整和保持零-shot评估。此外，我们引入了指令感知的视觉特征提取，这是一种关键的方法，使模型能够提取适合于给定指令的信息特征。结果，InstructBLIP模型实现",
    "tldr": "本文对视觉语言指令调整进行了系统全面的研究，引入了指令感知的视觉特征提取这种关键的方法，使模型能够提取适合于给定指令的信息特征。"
}