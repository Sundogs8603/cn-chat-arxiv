{
    "title": "Representation Of Lexical Stylistic Features In Language Models' Embedding Space. (arXiv:2305.18657v2 [cs.CL] UPDATED)",
    "abstract": "The representation space of pretrained Language Models (LMs) encodes rich information about words and their relationships (e.g., similarity, hypernymy, polysemy) as well as abstract semantic notions (e.g., intensity). In this paper, we demonstrate that lexical stylistic notions such as complexity, formality, and figurativeness, can also be identified in this space. We show that it is possible to derive a vector representation for each of these stylistic notions from only a small number of seed pairs. Using these vectors, we can characterize new texts in terms of these dimensions by performing simple calculations in the corresponding embedding space. We conduct experiments on five datasets and find that static embeddings encode these features more accurately at the level of words and phrases, whereas contextualized LMs perform better on sentences. The lower performance of contextualized representations at the word level is partially attributable to the anisotropy of their vector space, ",
    "link": "http://arxiv.org/abs/2305.18657",
    "context": "Title: Representation Of Lexical Stylistic Features In Language Models' Embedding Space. (arXiv:2305.18657v2 [cs.CL] UPDATED)\nAbstract: The representation space of pretrained Language Models (LMs) encodes rich information about words and their relationships (e.g., similarity, hypernymy, polysemy) as well as abstract semantic notions (e.g., intensity). In this paper, we demonstrate that lexical stylistic notions such as complexity, formality, and figurativeness, can also be identified in this space. We show that it is possible to derive a vector representation for each of these stylistic notions from only a small number of seed pairs. Using these vectors, we can characterize new texts in terms of these dimensions by performing simple calculations in the corresponding embedding space. We conduct experiments on five datasets and find that static embeddings encode these features more accurately at the level of words and phrases, whereas contextualized LMs perform better on sentences. The lower performance of contextualized representations at the word level is partially attributable to the anisotropy of their vector space, ",
    "path": "papers/23/05/2305.18657.json",
    "total_tokens": 968,
    "translated_title": "语言模型嵌入空间中的词汇文体特征表征",
    "translated_abstract": "预训练语言模型嵌入空间编码了关于单词及其关系（如相似性、上义词、一词多义）以及抽象的语义概念（如强度）丰富的信息。本文展示了此空间中的词汇文体特征，如复杂性、正式程度和比喻性。我们证明，仅使用少量种子对可以得出每个文体概念的向量表示。利用这些向量，在相应的嵌入空间中进行简单计算，即可通过这些维度表征新文本。我们在五个数据集上进行实验，发现静态嵌入空间更准确地编码了单词和短语的特征，而上下文化的语言模型则在句子级别上表现更好。上下文化表示在单词级别上的较低性能部分归因于其向量空间的各向异性。",
    "tldr": "本文展示了预训练语言模型嵌入空间中的词汇文体特征，并提出了仅使用少量种子对每个文体概念得出向量表示的方法。研究发现静态嵌入空间更准确地编码了单词和短语的特征，而上下文化的语言模型则在句子级别上表现更好。",
    "en_tdlr": "The paper presents the identification of lexical stylistic notions, such as complexity, formality, and figurativeness, in the representation space of pretrained Language Models. The study shows the possibility of deriving vector representation for each of these stylistic notions from only a small number of seed pairs. The experiments conducted on five datasets found that static embeddings encode these features more accurately at the level of words and phrases, whereas contextualized LMs perform better on sentences."
}