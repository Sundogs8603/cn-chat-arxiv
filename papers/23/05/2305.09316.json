{
    "title": "Enhancing Keyphrase Extraction from Long Scientific Documents using Graph Embeddings. (arXiv:2305.09316v1 [cs.CL])",
    "abstract": "In this study, we investigate using graph neural network (GNN) representations to enhance contextualized representations of pre-trained language models (PLMs) for keyphrase extraction from lengthy documents. We show that augmenting a PLM with graph embeddings provides a more comprehensive semantic understanding of words in a document, particularly for long documents. We construct a co-occurrence graph of the text and embed it using a graph convolutional network (GCN) trained on the task of edge prediction. We propose a graph-enhanced sequence tagging architecture that augments contextualized PLM embeddings with graph representations. Evaluating on benchmark datasets, we demonstrate that enhancing PLMs with graph embeddings outperforms state-of-the-art models on long documents, showing significant improvements in F1 scores across all the datasets. Our study highlights the potential of GNN representations as a complementary approach to improve PLM performance for keyphrase extraction fro",
    "link": "http://arxiv.org/abs/2305.09316",
    "context": "Title: Enhancing Keyphrase Extraction from Long Scientific Documents using Graph Embeddings. (arXiv:2305.09316v1 [cs.CL])\nAbstract: In this study, we investigate using graph neural network (GNN) representations to enhance contextualized representations of pre-trained language models (PLMs) for keyphrase extraction from lengthy documents. We show that augmenting a PLM with graph embeddings provides a more comprehensive semantic understanding of words in a document, particularly for long documents. We construct a co-occurrence graph of the text and embed it using a graph convolutional network (GCN) trained on the task of edge prediction. We propose a graph-enhanced sequence tagging architecture that augments contextualized PLM embeddings with graph representations. Evaluating on benchmark datasets, we demonstrate that enhancing PLMs with graph embeddings outperforms state-of-the-art models on long documents, showing significant improvements in F1 scores across all the datasets. Our study highlights the potential of GNN representations as a complementary approach to improve PLM performance for keyphrase extraction fro",
    "path": "papers/23/05/2305.09316.json",
    "total_tokens": 934,
    "translated_title": "利用图嵌入增强从长篇科技论文中提取关键词的方法",
    "translated_abstract": "本研究探讨了使用图神经网络（GNN）表示强化预训练语言模型（PLMs）对长篇文档中的关键词提取的上下文表示的方法。我们展示了利用图嵌入增强PLM提供更全面的语义理解文档中的单词，特别是对于长篇文档。我们构建了文本的共现图，并使用在边预测任务上训练的图卷积网络（GCN）来嵌入它。我们提出了一种图增强的序列标记架构，它将上下文化的PLM嵌入与图表示相结合。在基准测试数据集上的评估表明，增强PLM与图嵌入比现有技术的模型在长文档上表现更出色，在所有数据集中都显着提高F1得分。我们的研究突出了GNN表示作为改进PLM性能的一种补充方法的潜力。",
    "tldr": "本文研究了使用图神经网络表示加强预训练语言模型对长篇科技论文的关键词提取。通过构建文本共现图并结合图表示和上下文化的PLM嵌入，我们展示了对于长篇文档，增强PLMs性能比现有技术的模型表现更加出色。",
    "en_tdlr": "This study proposes a method for enhancing keyphrase extraction from long scientific documents using graph neural network representations. By augmenting pre-trained language models with graph embeddings, a more comprehensive semantic understanding of words is obtained. Experimental results demonstrate that the proposed method outperforms state-of-the-art models on long documents."
}