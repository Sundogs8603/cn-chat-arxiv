{
    "title": "Ahead-of-Time P-Tuning. (arXiv:2305.10835v1 [cs.LG])",
    "abstract": "In this paper, we propose Ahead-of-Time (AoT) P-Tuning, a novel parameter-efficient fine-tuning method for pre-trained Language Models (LMs) that adds input-dependent bias before each Transformer layer. We evaluate AoT P-Tuning on GLUE and SuperGLUE benchmarking datasets using RoBERTa and DeBERTa models, showing that it outperforms BitFit and is comparable or better than other baseline methods for efficient fine-tuning. Additionally, we assess the inference overhead of AoT P-Tuning and demonstrate that it introduces negligible overhead compared to established baseline methods. Our method enables multi-task inference with a single backbone LM, making it a practical solution for real-world applications.",
    "link": "http://arxiv.org/abs/2305.10835",
    "context": "Title: Ahead-of-Time P-Tuning. (arXiv:2305.10835v1 [cs.LG])\nAbstract: In this paper, we propose Ahead-of-Time (AoT) P-Tuning, a novel parameter-efficient fine-tuning method for pre-trained Language Models (LMs) that adds input-dependent bias before each Transformer layer. We evaluate AoT P-Tuning on GLUE and SuperGLUE benchmarking datasets using RoBERTa and DeBERTa models, showing that it outperforms BitFit and is comparable or better than other baseline methods for efficient fine-tuning. Additionally, we assess the inference overhead of AoT P-Tuning and demonstrate that it introduces negligible overhead compared to established baseline methods. Our method enables multi-task inference with a single backbone LM, making it a practical solution for real-world applications.",
    "path": "papers/23/05/2305.10835.json",
    "total_tokens": 854,
    "translated_title": "Ahead-of-Time P-Tuning：一种应用于预训练语言模型的参数节约的微调方法",
    "translated_abstract": "本文提出了 Ahead-of-Time （AoT）P-Tuning，一种新颖的微调方法，可以在每个Transformer层之前添加输入相关的偏置，以应用于预训练的语言模型（LMs）。我们使用RoBERTa和DeBERTa模型在GLUE和SuperGLUE基准数据集上评估AoT P-Tuning，结果表明它优于BitFit，并且与其他基准方法相比，效率更高。此外，我们评估了AoT P-Tuning的推理开销，并证明它与已建立的基准方法相比，引入的开销可以忽略不计。我们的方法可以使用单个骨干LM进行多任务推理，从而成为实际应用的解决方案。",
    "tldr": "本文提出了 Ahead-of-Time （AoT）P-Tuning，一种新颖的微调方法，它通过在每个Transformer层之前添加输入相关的偏置，实现了应用于预训练语言模型的参数节约。该方法在GLUE和SuperGLUE基准数据集上优于BitFit，并可用于多任务推理，而推理开销却很小。",
    "en_tdlr": "The paper proposes Ahead-of-Time (AoT) P-Tuning, a novel parameter-efficient fine-tuning method for pre-trained Language Models (LMs) that adds input-dependent bias before each Transformer layer. The method outperforms BitFit and is comparable or better than other baseline methods for efficient fine-tuning on GLUE and SuperGLUE benchmarking datasets using RoBERTa and DeBERTa models. Additionally, the inference overhead of AoT P-Tuning is negligible, enabling multi-task inference with a single backbone LM, making it a practical solution for real-world applications."
}