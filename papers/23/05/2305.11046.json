{
    "title": "Difference of Submodular Minimization via DC Programming. (arXiv:2305.11046v1 [cs.LG])",
    "abstract": "Minimizing the difference of two submodular (DS) functions is a problem that naturally occurs in various machine learning problems. Although it is well known that a DS problem can be equivalently formulated as the minimization of the difference of two convex (DC) functions, existing algorithms do not fully exploit this connection. A classical algorithm for DC problems is called the DC algorithm (DCA). We introduce variants of DCA and its complete form (CDCA) that we apply to the DC program corresponding to DS minimization. We extend existing convergence properties of DCA, and connect them to convergence properties on the DS problem. Our results on DCA match the theoretical guarantees satisfied by existing DS algorithms, while providing a more complete characterization of convergence properties. In the case of CDCA, we obtain a stronger local minimality guarantee. Our numerical results show that our proposed algorithms outperform existing baselines on two applications: speech corpus sel",
    "link": "http://arxiv.org/abs/2305.11046",
    "context": "Title: Difference of Submodular Minimization via DC Programming. (arXiv:2305.11046v1 [cs.LG])\nAbstract: Minimizing the difference of two submodular (DS) functions is a problem that naturally occurs in various machine learning problems. Although it is well known that a DS problem can be equivalently formulated as the minimization of the difference of two convex (DC) functions, existing algorithms do not fully exploit this connection. A classical algorithm for DC problems is called the DC algorithm (DCA). We introduce variants of DCA and its complete form (CDCA) that we apply to the DC program corresponding to DS minimization. We extend existing convergence properties of DCA, and connect them to convergence properties on the DS problem. Our results on DCA match the theoretical guarantees satisfied by existing DS algorithms, while providing a more complete characterization of convergence properties. In the case of CDCA, we obtain a stronger local minimality guarantee. Our numerical results show that our proposed algorithms outperform existing baselines on two applications: speech corpus sel",
    "path": "papers/23/05/2305.11046.json",
    "total_tokens": 905,
    "translated_title": "DC规划算法在子模最小化问题上的应用",
    "translated_abstract": "在各种机器学习问题中，最小化两个子模（DS）函数的差异是一个自然产生的问题。虽然已经有人知道DS问题可以等价地转化为两个凸（DC）函数的差异最小化问题，但现有算法并没有充分利用这种联系。对于DC问题，一个经典的算法叫做DC算法（DCA）。我们介绍了DCA及其完整形式（CDCA）的变体，并将其应用于对应于DS最小化的DC程序中。我们扩展了DCA的现有收敛性质，并将它们与DS问题的收敛性质联系起来。我们的DCA结果与现有的DS算法满足相同的理论保证，同时提供了更完整的收敛性质描述。对于CDCA的情况，我们获得了更强的局部最小保证。我们的数字实验结果表明，我们提出的算法在两个应用——语音语料库选择特征优化和文档摘要中均优于现有的基线算法。",
    "tldr": "本文介绍了一种新的算法，利用DC规划算法来解决子模最小化问题，并证明收敛性质比现有算法更全面，同时在语音特征选择和文档摘要等应用中取得更好的性能。",
    "en_tdlr": "This paper introduces a new algorithm that uses DC programming to solve the problem of minimizing the difference of two submodular functions, and proves that the convergence properties are more comprehensive than existing algorithms, while achieving better performance in applications such as speech feature optimization and document summarization."
}