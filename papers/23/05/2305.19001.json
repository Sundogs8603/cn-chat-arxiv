{
    "title": "Sharp high-probability sample complexities for policy evaluation with linear function approximation. (arXiv:2305.19001v1 [stat.ML])",
    "abstract": "This paper is concerned with the problem of policy evaluation with linear function approximation in discounted infinite horizon Markov decision processes. We investigate the sample complexities required to guarantee a predefined estimation error of the best linear coefficients for two widely-used policy evaluation algorithms: the temporal difference (TD) learning algorithm and the two-timescale linear TD with gradient correction (TDC) algorithm. In both the on-policy setting, where observations are generated from the target policy, and the off-policy setting, where samples are drawn from a behavior policy potentially different from the target policy, we establish the first sample complexity bound with high-probability convergence guarantee that attains the optimal dependence on the tolerance level. We also exhihit an explicit dependence on problem-related quantities, and show in the on-policy setting that our upper bound matches the minimax lower bound on crucial problem parameters, in",
    "link": "http://arxiv.org/abs/2305.19001",
    "context": "Title: Sharp high-probability sample complexities for policy evaluation with linear function approximation. (arXiv:2305.19001v1 [stat.ML])\nAbstract: This paper is concerned with the problem of policy evaluation with linear function approximation in discounted infinite horizon Markov decision processes. We investigate the sample complexities required to guarantee a predefined estimation error of the best linear coefficients for two widely-used policy evaluation algorithms: the temporal difference (TD) learning algorithm and the two-timescale linear TD with gradient correction (TDC) algorithm. In both the on-policy setting, where observations are generated from the target policy, and the off-policy setting, where samples are drawn from a behavior policy potentially different from the target policy, we establish the first sample complexity bound with high-probability convergence guarantee that attains the optimal dependence on the tolerance level. We also exhihit an explicit dependence on problem-related quantities, and show in the on-policy setting that our upper bound matches the minimax lower bound on crucial problem parameters, in",
    "path": "papers/23/05/2305.19001.json",
    "total_tokens": 828,
    "translated_title": "线性函数逼近下的策略评估的高概率样本复杂度",
    "translated_abstract": "本文涉及使用线性函数逼近在无限时间马尔可夫决策过程中进行策略评估的问题。我们研究了两种广泛使用的策略评估算法（时间差分学习算法和带有梯度校正的两个时间尺度线性时间差分算法）所需的样本复杂度，以保证最佳线性系数的预定义估计误差。在策略设置和离线设置中，我们建立了第一个具有高概率收敛保证的样本复杂度界限，达到了与容差水平的最佳关联性。我们还展示了与问题相关量明确的关系，并在策略设置中展示了我们的上限界限与关键问题参数上的极小极大下限界限相匹配。",
    "tldr": "本文研究线性函数逼近下的策略评估问题，提出了两个广泛使用的算法所需的样本复杂度，具有高概率收敛保证且与容差水平的关联性最佳。",
    "en_tdlr": "This paper proposes sample complexities for linear function approximation in policy evaluation problem, provides algorithms with high-probability convergence guarantee and optimal dependence on tolerance level, achieves the minimax lower bound on crucial problem parameters and exhibits explicit dependence on problem-related quantities."
}