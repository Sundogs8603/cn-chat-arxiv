{
    "title": "Universal approximation with complex-valued deep narrow neural networks. (arXiv:2305.16910v1 [math.FA])",
    "abstract": "We study the universality of complex-valued neural networks with bounded widths and arbitrary depths. Under mild assumptions, we give a full description of those activation functions $\\varrho:\\mathbb{CC}\\to \\mathbb{C}$ that have the property that their associated networks are universal, i.e., are capable of approximating continuous functions to arbitrary accuracy on compact domains. Precisely, we show that deep narrow complex-valued networks are universal if and only if their activation function is neither holomorphic, nor antiholomorphic, nor $\\mathbb{R}$-affine. This is a much larger class of functions than in the dual setting of arbitrary width and fixed depth. Unlike in the real case, the sufficient width differs significantly depending on the considered activation function. We show that a width of $2n+2m+5$ is always sufficient and that in general a width of $\\max\\{2n,2m\\}$ is necessary. We prove, however, that a width of $n+m+4$ suffices for a rich subclass of the admissible acti",
    "link": "http://arxiv.org/abs/2305.16910",
    "context": "Title: Universal approximation with complex-valued deep narrow neural networks. (arXiv:2305.16910v1 [math.FA])\nAbstract: We study the universality of complex-valued neural networks with bounded widths and arbitrary depths. Under mild assumptions, we give a full description of those activation functions $\\varrho:\\mathbb{CC}\\to \\mathbb{C}$ that have the property that their associated networks are universal, i.e., are capable of approximating continuous functions to arbitrary accuracy on compact domains. Precisely, we show that deep narrow complex-valued networks are universal if and only if their activation function is neither holomorphic, nor antiholomorphic, nor $\\mathbb{R}$-affine. This is a much larger class of functions than in the dual setting of arbitrary width and fixed depth. Unlike in the real case, the sufficient width differs significantly depending on the considered activation function. We show that a width of $2n+2m+5$ is always sufficient and that in general a width of $\\max\\{2n,2m\\}$ is necessary. We prove, however, that a width of $n+m+4$ suffices for a rich subclass of the admissible acti",
    "path": "papers/23/05/2305.16910.json",
    "total_tokens": 1082,
    "translated_title": "带有复值的深窄神经网络的普适逼近",
    "translated_abstract": "我们研究了具有有界宽度和任意深度的复值神经网络的普适性。在温和的假设下，我们给出了那些激活函数 $\\varrho:\\mathbb{CC}\\to \\mathbb{C}$ 的完整描述，这些函数具有这样一个属性：它们关联的网络是普适的，即能够在紧致域上逼近连续函数至任意精度。准确地说，我们表明了当且仅当它们的激活函数既不是全纯的，也不是反全纯的，也不是 $\\mathbb{R}$-仿射的，深窄的复值网络是普适的。这是一个比宽度任意、深度固定的对偶设置中更大的函数类。与实值情况不同的是，足够的宽度依赖于考虑的激活函数。我们表明，宽度为 $2n+2m+5$ 总是足够的，并且通常 $\\max\\{2n,2m\\}$ 是必要的。然而，我们证明了对于一类可允许的激活函数，宽度为 $n+m+4$ 是足够的。",
    "tldr": "本文研究了具有有界宽度和任意深度的复值神经网络的普适性，发现当且仅当激活函数既不是全纯的，也不是反全纯的，也不是 $\\mathbb{R}$-仿射的时，深窄的复值网络具有普适逼近能力。我们还发现足够的宽度依赖于考虑的激活函数，对于一类可允许的激活函数，宽度为 $n+m+4$ 是足够的。",
    "en_tdlr": "This article examines the universality of complex-valued neural networks with bounded widths and arbitrary depths, and finds that deep and narrow complex-valued networks have universal approximation capabilities only when the activation function is neither holomorphic, antiholomorphic, nor $\\mathbb{R}$-affine."
}