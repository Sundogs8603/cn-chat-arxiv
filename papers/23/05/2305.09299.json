{
    "title": "UniS-MMC: Multimodal Classification via Unimodality-supervised Multimodal Contrastive Learning. (arXiv:2305.09299v1 [cs.CV])",
    "abstract": "Multimodal learning aims to imitate human beings to acquire complementary information from multiple modalities for various downstream tasks. However, traditional aggregation-based multimodal fusion methods ignore the inter-modality relationship, treat each modality equally, suffer sensor noise, and thus reduce multimodal learning performance. In this work, we propose a novel multimodal contrastive method to explore more reliable multimodal representations under the weak supervision of unimodal predicting. Specifically, we first capture task-related unimodal representations and the unimodal predictions from the introduced unimodal predicting task. Then the unimodal representations are aligned with the more effective one by the designed multimodal contrastive method under the supervision of the unimodal predictions. Experimental results with fused features on two image-text classification benchmarks UPMC-Food-101 and N24News show that our proposed Unimodality-Supervised MultiModal Contra",
    "link": "http://arxiv.org/abs/2305.09299",
    "context": "Title: UniS-MMC: Multimodal Classification via Unimodality-supervised Multimodal Contrastive Learning. (arXiv:2305.09299v1 [cs.CV])\nAbstract: Multimodal learning aims to imitate human beings to acquire complementary information from multiple modalities for various downstream tasks. However, traditional aggregation-based multimodal fusion methods ignore the inter-modality relationship, treat each modality equally, suffer sensor noise, and thus reduce multimodal learning performance. In this work, we propose a novel multimodal contrastive method to explore more reliable multimodal representations under the weak supervision of unimodal predicting. Specifically, we first capture task-related unimodal representations and the unimodal predictions from the introduced unimodal predicting task. Then the unimodal representations are aligned with the more effective one by the designed multimodal contrastive method under the supervision of the unimodal predictions. Experimental results with fused features on two image-text classification benchmarks UPMC-Food-101 and N24News show that our proposed Unimodality-Supervised MultiModal Contra",
    "path": "papers/23/05/2305.09299.json",
    "total_tokens": 864,
    "tldr": "本论文提出了UniS-MMC方法，通过单模态监督的多模态对比学习实现多模态分类。实验结果表明该方法具有更好的性能。",
    "en_tdlr": "The paper proposes a novel UniS-MMC method to explore more reliable multimodal representations under the weak supervision of unimodal predicting. The experimental results show that the proposed method achieves better performance in image-text classification benchmarks."
}