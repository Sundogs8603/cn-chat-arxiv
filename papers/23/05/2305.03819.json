{
    "title": "Adapting Transformer Language Models for Predictive Typing in Brain-Computer Interfaces. (arXiv:2305.03819v1 [cs.CL])",
    "abstract": "Brain-computer interfaces (BCI) are an important mode of alternative and augmentative communication for many people. Unlike keyboards, many BCI systems do not display even the 26 letters of English at one time, let alone all the symbols in more complex systems. Using language models to make character-level predictions, therefore, can greatly speed up BCI typing (Ghosh and Kristensson, 2017). While most existing BCI systems employ character n-gram models or no LM at all, this paper adapts several wordpiece-level Transformer LMs to make character predictions and evaluates them on typing tasks. GPT-2 fares best on clean text, but different LMs react differently to noisy histories. We further analyze the effect of character positions in a word and context lengths.",
    "link": "http://arxiv.org/abs/2305.03819",
    "context": "Title: Adapting Transformer Language Models for Predictive Typing in Brain-Computer Interfaces. (arXiv:2305.03819v1 [cs.CL])\nAbstract: Brain-computer interfaces (BCI) are an important mode of alternative and augmentative communication for many people. Unlike keyboards, many BCI systems do not display even the 26 letters of English at one time, let alone all the symbols in more complex systems. Using language models to make character-level predictions, therefore, can greatly speed up BCI typing (Ghosh and Kristensson, 2017). While most existing BCI systems employ character n-gram models or no LM at all, this paper adapts several wordpiece-level Transformer LMs to make character predictions and evaluates them on typing tasks. GPT-2 fares best on clean text, but different LMs react differently to noisy histories. We further analyze the effect of character positions in a word and context lengths.",
    "path": "papers/23/05/2305.03819.json",
    "total_tokens": 827,
    "translated_title": "适应变压器语言模型用于脑机界面预测式打字",
    "translated_abstract": "脑机界面是许多人重要的替代和辅助交流方式。与键盘不同，许多脑机界面系统不会一次显示甚至英语中的26个字母，更不要说所有符号了。因此，使用语言模型进行字符级预测可以极大地加速BCI打字。本文将几个单词级变压器语言模型适应为字符预测，并在打字任务中对它们进行评估。其中，GPT-2在干净文本上表现最佳，但不同的语言模型对嘈杂的历史反应不同。我们进一步分析了单词中的字符位置和上下文长度的影响。",
    "tldr": "本文研究将变压器语言模型应用于脑机界面预测式打字任务中。在评估几个单词级变压器语言模型后，GPT-2在干净印刷体上表现最佳，但是不同的变压器模型在嘈杂的历史轨迹上有不同的反应。",
    "en_tdlr": "This paper adapts several wordpiece-level Transformer language models to make character predictions in Brain-Computer Interface (BCI) typing tasks, with GPT-2 performing best on clean text but different models reacting differently to noisy histories. The effect of character positions in a word and context lengths are further analyzed."
}