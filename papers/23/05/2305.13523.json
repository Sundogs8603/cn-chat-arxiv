{
    "title": "A Study of Generative Large Language Model for Medical Research and Healthcare. (arXiv:2305.13523v1 [cs.CL])",
    "abstract": "There is enormous enthusiasm and concerns in using large language models (LLMs) in healthcare, yet current assumptions are all based on general-purpose LLMs such as ChatGPT. This study develops a clinical generative LLM, GatorTronGPT, using 277 billion words of mixed clinical and English text with a GPT-3 architecture of 20 billion parameters. GatorTronGPT improves biomedical natural language processing for medical research. Synthetic NLP models trained using GatorTronGPT generated text outperform NLP models trained using real-world clinical text. Physicians Turing test using 1 (worst) to 9 (best) scale shows that there is no significant difference in linguistic readability (p = 0.22; 6.57 of GatorTronGPT compared with 6.93 of human) and clinical relevance (p = 0.91; 7.0 of GatorTronGPT compared with 6.97 of human) and that physicians cannot differentiate them (p < 0.001). This study provides insights on the opportunities and challenges of LLMs for medical research and healthcare.",
    "link": "http://arxiv.org/abs/2305.13523",
    "context": "Title: A Study of Generative Large Language Model for Medical Research and Healthcare. (arXiv:2305.13523v1 [cs.CL])\nAbstract: There is enormous enthusiasm and concerns in using large language models (LLMs) in healthcare, yet current assumptions are all based on general-purpose LLMs such as ChatGPT. This study develops a clinical generative LLM, GatorTronGPT, using 277 billion words of mixed clinical and English text with a GPT-3 architecture of 20 billion parameters. GatorTronGPT improves biomedical natural language processing for medical research. Synthetic NLP models trained using GatorTronGPT generated text outperform NLP models trained using real-world clinical text. Physicians Turing test using 1 (worst) to 9 (best) scale shows that there is no significant difference in linguistic readability (p = 0.22; 6.57 of GatorTronGPT compared with 6.93 of human) and clinical relevance (p = 0.91; 7.0 of GatorTronGPT compared with 6.97 of human) and that physicians cannot differentiate them (p < 0.001). This study provides insights on the opportunities and challenges of LLMs for medical research and healthcare.",
    "path": "papers/23/05/2305.13523.json",
    "total_tokens": 1063,
    "translated_title": "生成式大型语言模型在医疗研究与健康保健中的应用研究",
    "translated_abstract": "应用大型语言模型（LLMs）在医疗保健领域备受瞩目，但当前的假设都是基于通用型的LLMs，如ChatGPT。本研究开发了一种临床生成式LLM，GatorTronGPT，使用2770亿个混合临床与英语文本和一个200亿参数的GPT-3架构。GatorTronGPT改进了医学研究的生物医学自然语言处理。使用GatorTronGPT训练的合成NLP模型生成的文本性能优于使用真实临床文本训练的NLP模型。使用1（最差）到9（最好）的刻度进行的医生图灵测试表明，语言可读性（p = 0.22; GatorTronGPT为6.57，人类为6.93）和临床相关性（p = 0.91; GatorTronGPT为7.0，人类为6.97）没有显着差异，并且医生无法区分它们（p <0.001）。此研究提供了关于LLMs在医学研究和保健中的机遇和挑战的见解。",
    "tldr": "本研究开发了一种临床生成式语言模型——GatorTronGPT，它改善了生物医学自然语言处理，使用它训练的合成NLP模型性能优于使用真实临床文本训练的NLP模型，医生也无法区分它和真实临床文本的差异。",
    "en_tdlr": "This study develops a clinical generative LLM, GatorTronGPT, which improves biomedical natural language processing for medical research, and synthetic NLP models trained using GatorTronGPT outperform models trained using real-world clinical text. Physicians cannot differentiate the performance of GatorTronGPT and real-world clinical text."
}