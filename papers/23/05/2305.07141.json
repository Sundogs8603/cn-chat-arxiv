{
    "title": "The ConceptARC Benchmark: Evaluating Understanding and Generalization in the ARC Domain. (arXiv:2305.07141v1 [cs.LG])",
    "abstract": "The abilities to form and abstract concepts is key to human intelligence, but such abilities remain lacking in state-of-the-art AI systems. There has been substantial research on conceptual abstraction in AI, particularly using idealized domains such as Raven's Progressive Matrices and Bongard problems, but even when AI systems succeed on such problems, the systems are rarely evaluated in depth to see if they have actually grasped the concepts they are meant to capture.  In this paper we describe an in-depth evaluation benchmark for the Abstraction and Reasoning Corpus (ARC), a collection of few-shot abstraction and analogy problems developed by Chollet [2019]. In particular, we describe ConceptARC, a new, publicly available benchmark in the ARC domain that systematically assesses abstraction and generalization abilities on a number of basic spatial and semantic concepts. ConceptARC differs from the original ARC dataset in that it is specifically organized around \"concept groups\" -- se",
    "link": "http://arxiv.org/abs/2305.07141",
    "context": "Title: The ConceptARC Benchmark: Evaluating Understanding and Generalization in the ARC Domain. (arXiv:2305.07141v1 [cs.LG])\nAbstract: The abilities to form and abstract concepts is key to human intelligence, but such abilities remain lacking in state-of-the-art AI systems. There has been substantial research on conceptual abstraction in AI, particularly using idealized domains such as Raven's Progressive Matrices and Bongard problems, but even when AI systems succeed on such problems, the systems are rarely evaluated in depth to see if they have actually grasped the concepts they are meant to capture.  In this paper we describe an in-depth evaluation benchmark for the Abstraction and Reasoning Corpus (ARC), a collection of few-shot abstraction and analogy problems developed by Chollet [2019]. In particular, we describe ConceptARC, a new, publicly available benchmark in the ARC domain that systematically assesses abstraction and generalization abilities on a number of basic spatial and semantic concepts. ConceptARC differs from the original ARC dataset in that it is specifically organized around \"concept groups\" -- se",
    "path": "papers/23/05/2305.07141.json",
    "total_tokens": 871,
    "translated_title": "ConceptARC基准：评估ARC领域的理解和泛化能力",
    "translated_abstract": "形成和抽象概念的能力是人类智能的关键，但现有的人工智能系统在这方面仍然欠缺。在人工智能中进行了大量关于概念抽象的研究，特别是使用理想化的领域，如Raven的渐进矩阵和Bongard问题，但即使在人工智能系统成功解决这些问题时，这些系统的实际理解情况也很少被评估。本文描述了一种针对抽象和推理数据集（ARC）的深入评估基准，ARC是Chollet [2019]开发的一组少量抽象和类比问题集。具体而言，我们描述了一个名为ConceptARC的新的、公开可用的ARC基准，它在许多基本空间和语义概念上系统地评估了抽象和泛化能力。与原始的ARC数据集不同，ConceptARC特别围绕“概念组”进行组织。",
    "tldr": "本研究提出了一个新的基准数据集ConceptARC，针对ARC领域的抽象和推理问题进行了深入评估，以提高人工智能系统的抽象和泛化能力。",
    "en_tdlr": "This study proposes a new benchmark dataset, ConceptARC, which evaluates the abstraction and generalization abilities of AI systems in the ARC domain, aiming to improve their performance in these areas."
}