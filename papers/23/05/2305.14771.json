{
    "title": "SSD-2: Scaling and Inference-time Fusion of Diffusion Language Models. (arXiv:2305.14771v1 [cs.CL])",
    "abstract": "Diffusion-based language models (LMs) have been shown to be competent generative models that are easy to control at inference and are a promising alternative to autoregressive LMs. While autoregressive LMs have benefited immensely from scaling and instruction-based learning, existing studies on diffusion LMs have been conducted on a relatively smaller scale. Starting with a recently proposed diffusion model SSD-LM, in this work we explore methods to scale it from 0.4B to 13B parameters, proposing several techniques to improve its training and inference efficiency. We call the new model SSD-2. We further show that this model can be easily finetuned to follow instructions. Finally, leveraging diffusion models' capability at inference-time control, we show that SSD-2 facilitates novel ensembles with 100x smaller models that can be customized and deployed by individual users. We find that compared to autoregressive models, the collaboration between diffusion models is more effective, leadi",
    "link": "http://arxiv.org/abs/2305.14771",
    "context": "Title: SSD-2: Scaling and Inference-time Fusion of Diffusion Language Models. (arXiv:2305.14771v1 [cs.CL])\nAbstract: Diffusion-based language models (LMs) have been shown to be competent generative models that are easy to control at inference and are a promising alternative to autoregressive LMs. While autoregressive LMs have benefited immensely from scaling and instruction-based learning, existing studies on diffusion LMs have been conducted on a relatively smaller scale. Starting with a recently proposed diffusion model SSD-LM, in this work we explore methods to scale it from 0.4B to 13B parameters, proposing several techniques to improve its training and inference efficiency. We call the new model SSD-2. We further show that this model can be easily finetuned to follow instructions. Finally, leveraging diffusion models' capability at inference-time control, we show that SSD-2 facilitates novel ensembles with 100x smaller models that can be customized and deployed by individual users. We find that compared to autoregressive models, the collaboration between diffusion models is more effective, leadi",
    "path": "papers/23/05/2305.14771.json",
    "total_tokens": 939,
    "translated_title": "SSD-2：扩展和推理时间融合的扩散语言模型",
    "translated_abstract": "基于扩散的语言模型被证明是具有竞争力的生成模型，易于在推理时进行控制，并且是自回归语言模型的有希望的替代方案。然而，现有的扩散模型仅在相对较小的规模下进行了研究。本文通过对最近提出的扩散模型SSD-LM的研究，探索了将其从0.4B扩展到13B参数的方法，并提出了几种改进其训练和推理效率的技术。我们命名这个新模型为SSD-2。我们还展示了这个模型可以很容易地通过微调来遵循指令。最后，利用扩散模型在推理时的控制能力，我们展示了SSD-2可以与100倍更小的模型合作形成新的集成模型，这些模型可以由个人用户进行定制和部署。与自回归模型相比，我们发现扩散模型之间的协作更加有效，从而产生更好的结果。",
    "tldr": "本文介绍了一种扩散语言模型SSD-2，它可以从0.4B扩展到13B参数，并经过微调来遵循指令。与自回归模型相比，使用SSD-2可以形成更有效的模型合作，产生更好的结果。",
    "en_tdlr": "This paper introduces a diffusion language model SSD-2, which can be scaled from 0.4B to 13B parameters and can follow instructions through fine-tuning. Compared to autoregressive models, using SSD-2 can form more effective model collaborations and produce better results."
}