{
    "title": "Serial Contrastive Knowledge Distillation for Continual Few-shot Relation Extraction. (arXiv:2305.06616v1 [cs.CL])",
    "abstract": "Continual few-shot relation extraction (RE) aims to continuously train a model for new relations with few labeled training data, of which the major challenges are the catastrophic forgetting of old relations and the overfitting caused by data sparsity. In this paper, we propose a new model, namely SCKD, to accomplish the continual few-shot RE task. Specifically, we design serial knowledge distillation to preserve the prior knowledge from previous models and conduct contrastive learning with pseudo samples to keep the representations of samples in different relations sufficiently distinguishable. Our experiments on two benchmark datasets validate the effectiveness of SCKD for continual few-shot RE and its superiority in knowledge transfer and memory utilization over state-of-the-art models.",
    "link": "http://arxiv.org/abs/2305.06616",
    "context": "Title: Serial Contrastive Knowledge Distillation for Continual Few-shot Relation Extraction. (arXiv:2305.06616v1 [cs.CL])\nAbstract: Continual few-shot relation extraction (RE) aims to continuously train a model for new relations with few labeled training data, of which the major challenges are the catastrophic forgetting of old relations and the overfitting caused by data sparsity. In this paper, we propose a new model, namely SCKD, to accomplish the continual few-shot RE task. Specifically, we design serial knowledge distillation to preserve the prior knowledge from previous models and conduct contrastive learning with pseudo samples to keep the representations of samples in different relations sufficiently distinguishable. Our experiments on two benchmark datasets validate the effectiveness of SCKD for continual few-shot RE and its superiority in knowledge transfer and memory utilization over state-of-the-art models.",
    "path": "papers/23/05/2305.06616.json",
    "total_tokens": 791,
    "translated_title": "序列对比知识蒸馏，用于连续的少样本关系抽取",
    "translated_abstract": "连续的少样本关系抽取旨在通过少量标注的训练样本不断地为新关系训练模型，其中最大的挑战是旧关系的灾难性遗忘和由数据稀疏性造成的过度拟合。本文提出了一种新模型SCKD，来完成连续的少样本关系抽取任务。具体来说，我们设计了序列知识蒸馏来保存以前模型的先前知识，并使用伪样本进行对比学习，以保持不同关系样本的表示足够可区分。我们在两个基准数据集上的实验验证了SCKD在连续的少样本关系抽取中的有效性，以及其在知识转移和内存利用方面优于现有技术模型。",
    "tldr": "本文提出了 SCKD 模型，使用序列知识蒸馏与对比学习，实现连续的少样本关系抽取任务。该方法可以有效解决旧关系遗忘和过度拟合的问题。"
}