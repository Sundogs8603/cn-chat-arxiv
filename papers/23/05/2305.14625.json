{
    "title": "KNN-LM Does Not Improve Open-ended Text Generation. (arXiv:2305.14625v1 [cs.CL])",
    "abstract": "In this paper, we study the generation quality of interpolation-based retrieval-augmented language models (LMs). These methods, best exemplified by the KNN-LM, interpolate the LM's predicted distribution of the next word with a distribution formed from the most relevant retrievals for a given prefix. While the KNN-LM and related methods yield impressive decreases in perplexity, we discover that they do not exhibit corresponding improvements in open-ended generation quality, as measured by both automatic evaluation metrics (e.g., MAUVE) and human evaluations. Digging deeper, we find that interpolating with a retrieval distribution actually increases perplexity compared to a baseline Transformer LM for the majority of tokens in the WikiText-103 test set, even though the overall perplexity is lower due to a smaller number of tokens for which perplexity dramatically decreases after interpolation. However, when decoding a long sequence at inference time, significant improvements on this sma",
    "link": "http://arxiv.org/abs/2305.14625",
    "context": "Title: KNN-LM Does Not Improve Open-ended Text Generation. (arXiv:2305.14625v1 [cs.CL])\nAbstract: In this paper, we study the generation quality of interpolation-based retrieval-augmented language models (LMs). These methods, best exemplified by the KNN-LM, interpolate the LM's predicted distribution of the next word with a distribution formed from the most relevant retrievals for a given prefix. While the KNN-LM and related methods yield impressive decreases in perplexity, we discover that they do not exhibit corresponding improvements in open-ended generation quality, as measured by both automatic evaluation metrics (e.g., MAUVE) and human evaluations. Digging deeper, we find that interpolating with a retrieval distribution actually increases perplexity compared to a baseline Transformer LM for the majority of tokens in the WikiText-103 test set, even though the overall perplexity is lower due to a smaller number of tokens for which perplexity dramatically decreases after interpolation. However, when decoding a long sequence at inference time, significant improvements on this sma",
    "path": "papers/23/05/2305.14625.json",
    "total_tokens": 918,
    "translated_title": "KNN-LM并不能提高开放式文本生成的质量",
    "translated_abstract": "本文研究了基于插值的检索增强语言模型（LM）的生成质量。这些方法以KNN-LM为代表，将LM预测的下一个单词分布与给定前缀最相关的检索形成的分布插值。虽然KNN-LM和相关方法在困惑度方面表现出色，但我们发现它们在开放式生成质量方面没有相应的改进，这是通过自动评估指标（例如MAUVE）和人工评估来衡量的。进一步探究，我们发现与检索分布插值相比，对于WikiText-103测试集中的大多数标记，插值会增加困惑度，尽管由于困惑度显著降低的标记数量更少，因此总体上困惑度更低。然而，当在推理时解码长序列时，对这个小标记集的显著改进并不能转化为整个序列的持续性改进，这表明KNN-LM和相关方法的改进是局部现象，而不是LM的普遍改进。",
    "tldr": "本文研究了基于插值的检索增强语言模型（LM）的生成质量，发现KNN-LM并不能提高开放式文本生成的整体质量。",
    "en_tdlr": "This paper studies the generation quality of interpolation-based retrieval-augmented language models (LMs), and finds that KNN-LM does not improve open-ended text generation overall."
}