{
    "title": "Sequence-Level Knowledge Distillation for Class-Incremental End-to-End Spoken Language Understanding. (arXiv:2305.13899v1 [eess.AS])",
    "abstract": "The ability to learn new concepts sequentially is a major weakness for modern neural networks, which hinders their use in non-stationary environments. Their propensity to fit the current data distribution to the detriment of the past acquired knowledge leads to the catastrophic forgetting issue. In this work we tackle the problem of Spoken Language Understanding applied to a continual learning setting. We first define a class-incremental scenario for the SLURP dataset. Then, we propose three knowledge distillation (KD) approaches to mitigate forgetting for a sequence-to-sequence transformer model: the first KD method is applied to the encoder output (audio-KD), and the other two work on the decoder output, either directly on the token-level (tok-KD) or on the sequence-level (seq-KD) distributions. We show that the seq-KD substantially improves all the performance metrics, and its combination with the audio-KD further decreases the average WER and enhances the entity prediction metric.",
    "link": "http://arxiv.org/abs/2305.13899",
    "context": "Title: Sequence-Level Knowledge Distillation for Class-Incremental End-to-End Spoken Language Understanding. (arXiv:2305.13899v1 [eess.AS])\nAbstract: The ability to learn new concepts sequentially is a major weakness for modern neural networks, which hinders their use in non-stationary environments. Their propensity to fit the current data distribution to the detriment of the past acquired knowledge leads to the catastrophic forgetting issue. In this work we tackle the problem of Spoken Language Understanding applied to a continual learning setting. We first define a class-incremental scenario for the SLURP dataset. Then, we propose three knowledge distillation (KD) approaches to mitigate forgetting for a sequence-to-sequence transformer model: the first KD method is applied to the encoder output (audio-KD), and the other two work on the decoder output, either directly on the token-level (tok-KD) or on the sequence-level (seq-KD) distributions. We show that the seq-KD substantially improves all the performance metrics, and its combination with the audio-KD further decreases the average WER and enhances the entity prediction metric.",
    "path": "papers/23/05/2305.13899.json",
    "total_tokens": 916,
    "translated_title": "针对增量学习的端到端语音理解序列级知识蒸馏",
    "translated_abstract": "现代神经网络在逐步学习新概念方面的能力是一个重要的弱点，这妨碍了它们在非平稳环境中的使用。它们倾向于将当前数据分布拟合得越来越好，而忽略了过去所获取的知识，导致了灾难性的遗忘问题。本文解决了应用于连续学习情境的口语语言理解问题。我们首先为SLURP数据集定义了一个增量类别场景，并针对序列到序列的Transformer模型提出了三种知识蒸馏（KD）方法以减轻遗忘：第一种KD方法应用于编码器输出（audio-KD），其余两种方法则分别在解码器输出的标记级（tok-KD）或序列级（seq-KD）分布上进行。我们展示了seq-KD显著地改善了所有绩效指标，将它与audio-KD相结合进一步降低了平均词错误率（WER）并提高了实体预测指标。",
    "tldr": "本文针对连续学习场景下的口语语言理解问题，提出了增量类别场景和三种知识蒸馏方法，并显示序列级知识蒸馏可以显著改善绩效。",
    "en_tdlr": "This paper addresses the problem of spoken language understanding in a continual learning setting, proposes an incremental class scenario and three knowledge distillation methods, and shows that the sequence-level knowledge distillation substantially improves performance metrics."
}