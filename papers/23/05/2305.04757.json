{
    "title": "Augmented Large Language Models with Parametric Knowledge Guiding. (arXiv:2305.04757v2 [cs.CL] UPDATED)",
    "abstract": "Large Language Models (LLMs) have significantly advanced natural language processing (NLP) with their impressive language understanding and generation capabilities. However, their performance may be suboptimal for domain-specific tasks that require specialized knowledge due to limited exposure to the related data. Additionally, the lack of transparency of most state-of-the-art (SOTA) LLMs, which can only be accessed via APIs, impedes further fine-tuning with domain custom data. Moreover, providing private data to the LLMs' owner leads to data privacy problems. To address these challenges, we propose the novel Parametric Knowledge Guiding (PKG) framework, which equips LLMs with a knowledge-guiding module to access relevant knowledge without altering the LLMs' parameters. Our PKG is based on open-source \"white-box\" language models, allowing offline memory of any knowledge that LLMs require. We demonstrate that our PKG framework can enhance the performance of \"black-box\" LLMs on a range o",
    "link": "http://arxiv.org/abs/2305.04757",
    "context": "Title: Augmented Large Language Models with Parametric Knowledge Guiding. (arXiv:2305.04757v2 [cs.CL] UPDATED)\nAbstract: Large Language Models (LLMs) have significantly advanced natural language processing (NLP) with their impressive language understanding and generation capabilities. However, their performance may be suboptimal for domain-specific tasks that require specialized knowledge due to limited exposure to the related data. Additionally, the lack of transparency of most state-of-the-art (SOTA) LLMs, which can only be accessed via APIs, impedes further fine-tuning with domain custom data. Moreover, providing private data to the LLMs' owner leads to data privacy problems. To address these challenges, we propose the novel Parametric Knowledge Guiding (PKG) framework, which equips LLMs with a knowledge-guiding module to access relevant knowledge without altering the LLMs' parameters. Our PKG is based on open-source \"white-box\" language models, allowing offline memory of any knowledge that LLMs require. We demonstrate that our PKG framework can enhance the performance of \"black-box\" LLMs on a range o",
    "path": "papers/23/05/2305.04757.json",
    "total_tokens": 941,
    "translated_title": "带参数知识引导的增强型大语言模型",
    "translated_abstract": "大型语言模型 (LLM) 以其出色的语言理解和生成能力，极大地推进了自然语言处理（NLP）。但是，由于对相关数据的有限接触，它们在需要专业知识的领域特定任务上的表现可能不够优化。此外，大多数最先进的 LLM 缺乏透明度，只能通过 API 访问, 这阻止了进一步用领域定制数据进行微调。此外，向 LLM 所有者提供私有数据会导致数据隐私问题。为解决这些挑战，我们提出了新型的带参数知识引导 (PKG) 框架，该框架为 LLM 配备了知识引导模块，以访问相关知识，而无需改变 LLM 的参数。我们的 PKG 基于开源的“白盒”语言模型，允许离线存储 LLM 需要的任何知识。我们证明了我们的 PKG 框架可以提高“黑盒”LLM在各种NLP任务上的性能。",
    "tldr": "这篇论文提出了一种带参数知识引导的增强型大语言模型框架，通过为LLMs装备信息引导模块来访问相关知识，同时保持LLMs的参数不变。这个框架可以提高黑盒LLMs在各种NLP任务上的性能。",
    "en_tdlr": "This paper proposes an augmented large language model framework with parametric knowledge guiding (PKG) to access relevant knowledge without altering the LLMs' parameters. This framework can enhance the performance of black-box LLMs on various NLP tasks."
}