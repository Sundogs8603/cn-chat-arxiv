{
    "title": "MT-SLVR: Multi-Task Self-Supervised Learning for Transformation In(Variant) Representations. (arXiv:2305.17191v1 [cs.LG])",
    "abstract": "Contrastive self-supervised learning has gained attention for its ability to create high-quality representations from large unlabelled data sets. A key reason that these powerful features enable data-efficient learning of downstream tasks is that they provide augmentation invariance, which is often a useful inductive bias. However, the amount and type of invariances preferred is not known apriori, and varies across different downstream tasks. We therefore propose a multi-task self-supervised framework (MT-SLVR) that learns both variant and invariant features in a parameter-efficient manner. Our multi-task representation provides a strong and flexible feature that benefits diverse downstream tasks. We evaluate our approach on few-shot classification tasks drawn from a variety of audio domains and demonstrate improved classification performance on all of them",
    "link": "http://arxiv.org/abs/2305.17191",
    "context": "Title: MT-SLVR: Multi-Task Self-Supervised Learning for Transformation In(Variant) Representations. (arXiv:2305.17191v1 [cs.LG])\nAbstract: Contrastive self-supervised learning has gained attention for its ability to create high-quality representations from large unlabelled data sets. A key reason that these powerful features enable data-efficient learning of downstream tasks is that they provide augmentation invariance, which is often a useful inductive bias. However, the amount and type of invariances preferred is not known apriori, and varies across different downstream tasks. We therefore propose a multi-task self-supervised framework (MT-SLVR) that learns both variant and invariant features in a parameter-efficient manner. Our multi-task representation provides a strong and flexible feature that benefits diverse downstream tasks. We evaluate our approach on few-shot classification tasks drawn from a variety of audio domains and demonstrate improved classification performance on all of them",
    "path": "papers/23/05/2305.17191.json",
    "total_tokens": 782,
    "translated_title": "MT-SLVR: 多任务自监督学习用于变换表示中的特征学习",
    "translated_abstract": "对比自监督学习因其能从大型未标记数据集中创建高质量表示而受到关注。这些强大的特征为下游任务的数据高效学习提供了扩充不变性，这通常是一种有用的归纳偏差。然而，从先验上不知道所需的不变性数量和类型，并且在不同的下游任务中变化。因此，我们提出了一种多任务自监督框架(MT-SLVR)，以一种参数高效的方式学习变体和不变特征。我们的多任务表示提供了强大和灵活的特征，可使多样的下游任务受益。我们在来自各种音频领域的少样本分类任务上评估了我们的方法，并证明了在所有任务上均有改善的分类性能。",
    "tldr": "该论文提出了一种名为 MT-SLVR 的框架，用于解决自监督学习中的不变性问题，以改善不同的下游任务的分类性能。",
    "en_tdlr": "This paper proposes a framework named MT-SLVR to address the invariance problem in self-supervised learning, aiming to improve the classification performance for different downstream tasks."
}