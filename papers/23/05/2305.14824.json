{
    "title": "Mitigating Temporal Misalignment by Discarding Outdated Facts",
    "abstract": "arXiv:2305.14824v3 Announce Type: replace  Abstract: While large language models are able to retain vast amounts of world knowledge seen during pretraining, such knowledge is prone to going out of date and is nontrivial to update. Furthermore, these models are often used under temporal misalignment, tasked with answering questions about the present, despite having only been trained on data collected in the past. To mitigate the effects of temporal misalignment, we propose fact duration prediction: the task of predicting how long a given fact will remain true. In our experiments, we demonstrate that identifying which facts are prone to rapid change can help models avoid reciting outdated information and determine which predictions require seeking out up-to-date knowledge sources. We also show how modeling fact duration improves calibration for knowledge-intensive tasks, such as open-retrieval question answering, under temporal misalignment, by discarding volatile facts. Our data and cod",
    "link": "https://arxiv.org/abs/2305.14824",
    "context": "Title: Mitigating Temporal Misalignment by Discarding Outdated Facts\nAbstract: arXiv:2305.14824v3 Announce Type: replace  Abstract: While large language models are able to retain vast amounts of world knowledge seen during pretraining, such knowledge is prone to going out of date and is nontrivial to update. Furthermore, these models are often used under temporal misalignment, tasked with answering questions about the present, despite having only been trained on data collected in the past. To mitigate the effects of temporal misalignment, we propose fact duration prediction: the task of predicting how long a given fact will remain true. In our experiments, we demonstrate that identifying which facts are prone to rapid change can help models avoid reciting outdated information and determine which predictions require seeking out up-to-date knowledge sources. We also show how modeling fact duration improves calibration for knowledge-intensive tasks, such as open-retrieval question answering, under temporal misalignment, by discarding volatile facts. Our data and cod",
    "path": "papers/23/05/2305.14824.json",
    "total_tokens": 855,
    "translated_title": "通过丢弃过时事实来减轻时间不一致性问题",
    "translated_abstract": "虽然大型语言模型能够保留在预训练过程中见过的大量世界知识，但这些知识容易过时，并且更新起来并不是一件简单的事情。此外，这些模型经常在时间不一致性下使用，被要求回答关于现在的问题，尽管它们只是在过去收集的数据上训练过。为了减轻时间不一致性的影响，我们提出了事实持续时间预测：预测一个给定事实会保持真实的时间长度的任务。在我们的实验中，我们展示了识别哪些事实容易快速变化可以帮助模型避免重复过时信息，并确定哪些预测需要寻找最新知识源。我们还展示了如何建模事实持续时间可以提高对知识密集任务的校准，例如在时间不一致性下的开放性检索问答，通过丢弃易变事实。我们的数据和实现代码",
    "tldr": "提出了一种通过预测事实的持续时间来减轻时间不一致性问题的方法，从而避免语言模型重复提供过时信息，并帮助模型提高知识密集任务的校准性。",
    "en_tdlr": "Proposed a method to mitigate temporal misalignment by predicting the duration of facts, thereby avoiding redundant outdated information in language models and enhancing calibration for knowledge-intensive tasks."
}