{
    "title": "Learning Activation Functions for Sparse Neural Networks. (arXiv:2305.10964v1 [cs.LG])",
    "abstract": "Sparse Neural Networks (SNNs) can potentially demonstrate similar performance to their dense counterparts while saving significant energy and memory at inference. However, the accuracy drop incurred by SNNs, especially at high pruning ratios, can be an issue in critical deployment conditions. While recent works mitigate this issue through sophisticated pruning techniques, we shift our focus to an overlooked factor: hyperparameters and activation functions. Our analyses have shown that the accuracy drop can additionally be attributed to (i) Using ReLU as the default choice for activation functions unanimously, and (ii) Fine-tuning SNNs with the same hyperparameters as dense counterparts. Thus, we focus on learning a novel way to tune activation functions for sparse networks and combining these with a separate hyperparameter optimization (HPO) regime for sparse networks. By conducting experiments on popular DNN models (LeNet-5, VGG-16, ResNet-18, and EfficientNet-B0) trained on MNIST, CI",
    "link": "http://arxiv.org/abs/2305.10964",
    "context": "Title: Learning Activation Functions for Sparse Neural Networks. (arXiv:2305.10964v1 [cs.LG])\nAbstract: Sparse Neural Networks (SNNs) can potentially demonstrate similar performance to their dense counterparts while saving significant energy and memory at inference. However, the accuracy drop incurred by SNNs, especially at high pruning ratios, can be an issue in critical deployment conditions. While recent works mitigate this issue through sophisticated pruning techniques, we shift our focus to an overlooked factor: hyperparameters and activation functions. Our analyses have shown that the accuracy drop can additionally be attributed to (i) Using ReLU as the default choice for activation functions unanimously, and (ii) Fine-tuning SNNs with the same hyperparameters as dense counterparts. Thus, we focus on learning a novel way to tune activation functions for sparse networks and combining these with a separate hyperparameter optimization (HPO) regime for sparse networks. By conducting experiments on popular DNN models (LeNet-5, VGG-16, ResNet-18, and EfficientNet-B0) trained on MNIST, CI",
    "path": "papers/23/05/2305.10964.json",
    "total_tokens": 914,
    "translated_title": "学习为稀疏神经网络激活函数设置",
    "translated_abstract": "稀疏神经网络（SNN）在推断时可以节省大量能量和内存，同时可以表现出类似于密集神经网络的性能。 然而，在高修剪比率下SNN的准确度降低可能在关键部署条件下成为问题。 在最近的研究中，通过复杂的修剪技术来缓解这个问题，但我们关注被忽略的因素：超参数和激活函数。 我们的分析表明，准确度下降可以额外归因于（i）普遍使用ReLU作为激活函数的默认选择，以及（ii）使用与密集网络相同的超参数来微调SNN。 因此，我们专注于学习为稀疏网络调整激活函数，并将其与稀疏网络的分开超参数优化方案相结合。 通过对在MNIST上训练的流行DNN模型（LeNet-5，VGG-16，ResNet-18和EfficientNet-B0）进行实验",
    "tldr": "本论文针对稀疏神经网络的准确性下降问题，发现激活函数和超参数是导致问题的主要原因，提出学习为稀疏网络调整激活函数并分开超参数优化方案的解决方法。",
    "en_tdlr": "This paper addresses the accuracy drop problem of Sparse Neural Networks (SNNs), which can be attributed to activation functions and hyperparameters. The proposed solution is to learn to adjust activation functions for SNNs and separate hyperparameter optimization for SNNs. The approach is validated on popular DNN models trained on MNIST dataset."
}