<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>Tryage&#26159;&#19968;&#20010;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#36335;&#30001;&#31995;&#32479;&#65292;&#33021;&#22815;&#26681;&#25454;&#23545;&#20010;&#20307;&#36755;&#20837;&#25552;&#31034;&#30340;&#20998;&#26512;&#65292;&#20174;&#27169;&#22411;&#24211;&#20013;&#36873;&#25321;&#26368;&#20339;&#30340;&#19987;&#23478;&#27169;&#22411;&#65292;&#20197;&#28040;&#38500;&#27169;&#22411;&#36873;&#25321;&#21644;&#23450;&#21046;&#21270;&#30340;&#36127;&#25285;&#65292;&#37322;&#25918;&#24222;&#22823;&#30340;&#26032;&#20852;&#27169;&#22411;&#24211;&#30340;&#24040;&#22823;&#23041;&#21147;&#32473;&#26368;&#32456;&#29992;&#25143;&#12290;</title><link>http://arxiv.org/abs/2308.11601</link><description>&lt;p&gt;
Tryage: &#23454;&#26102;&#26234;&#33021;&#36335;&#30001;&#29992;&#25143;&#25552;&#31034;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Tryage: Real-time, intelligent Routing of User Prompts to Large Language Model. (arXiv:2308.11601v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11601
&lt;/p&gt;
&lt;p&gt;
Tryage&#26159;&#19968;&#20010;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#36335;&#30001;&#31995;&#32479;&#65292;&#33021;&#22815;&#26681;&#25454;&#23545;&#20010;&#20307;&#36755;&#20837;&#25552;&#31034;&#30340;&#20998;&#26512;&#65292;&#20174;&#27169;&#22411;&#24211;&#20013;&#36873;&#25321;&#26368;&#20339;&#30340;&#19987;&#23478;&#27169;&#22411;&#65292;&#20197;&#28040;&#38500;&#27169;&#22411;&#36873;&#25321;&#21644;&#23450;&#21046;&#21270;&#30340;&#36127;&#25285;&#65292;&#37322;&#25918;&#24222;&#22823;&#30340;&#26032;&#20852;&#27169;&#22411;&#24211;&#30340;&#24040;&#22823;&#23041;&#21147;&#32473;&#26368;&#32456;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#21387;&#22120;&#26550;&#26500;&#21644;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#24341;&#20837;&#23548;&#33268;&#20102;&#22312;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#21644;&#25968;&#25454;&#39046;&#22495;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#29190;&#28856;&#24615;&#22686;&#38271;&#12290;&#22312;Hugging Face&#29983;&#24577;&#31995;&#32479;&#20013;&#26377;&#36229;&#36807;200,000&#20010;&#27169;&#22411;&#65292;&#29992;&#25143;&#22312;&#36873;&#25321;&#21644;&#20248;&#21270;&#27169;&#22411;&#20197;&#36866;&#24212;&#22810;&#26041;&#38754;&#30340;&#24037;&#20316;&#27969;&#31243;&#21644;&#25968;&#25454;&#39046;&#22495;&#30340;&#21516;&#26102;&#65292;&#36824;&#35201;&#35299;&#20915;&#35745;&#31639;&#12289;&#23433;&#20840;&#21644;&#26102;&#25928;&#24615;&#31561;&#38382;&#39064;&#12290;&#36843;&#20999;&#38656;&#35201;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#26469;&#28040;&#38500;&#27169;&#22411;&#36873;&#25321;&#21644;&#23450;&#21046;&#21270;&#30340;&#36127;&#25285;&#65292;&#24182;&#37322;&#25918;&#24222;&#22823;&#30340;&#26032;&#20852;&#27169;&#22411;&#24211;&#30340;&#24040;&#22823;&#23041;&#21147;&#32473;&#26368;&#32456;&#29992;&#25143;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#36335;&#30001;&#31995;&#32479;Tryage&#65292;&#23427;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36335;&#30001;&#22120;&#26681;&#25454;&#23545;&#20010;&#20307;&#36755;&#20837;&#25552;&#31034;&#30340;&#20998;&#26512;&#65292;&#20174;&#27169;&#22411;&#24211;&#20013;&#36873;&#25321;&#26368;&#20339;&#30340;&#19987;&#23478;&#27169;&#22411;&#12290;&#21463;&#22823;&#33041;&#20013;&#30340;&#19992;&#33041;&#36335;&#30001;&#22120;&#21551;&#21457;&#65292;Tryage&#37319;&#29992;&#24863;&#30693;&#36335;&#30001;&#22120;&#26469;&#39044;&#27979;&#19979;&#28216;&#27169;&#22411;&#22312;&#25552;&#31034;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#26681;&#25454;&#30446;&#26631;&#20570;&#20986;&#36335;&#30001;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
The introduction of the transformer architecture and the self-attention mechanism has led to an explosive production of language models trained on specific downstream tasks and data domains. With over 200, 000 models in the Hugging Face ecosystem, users grapple with selecting and optimizing models to suit multifaceted workflows and data domains while addressing computational, security, and recency concerns. There is an urgent need for machine learning frameworks that can eliminate the burden of model selection and customization and unleash the incredible power of the vast emerging model library for end users. Here, we propose a context-aware routing system, Tryage, that leverages a language model router for optimal selection of expert models from a model library based on analysis of individual input prompts. Inspired by the thalamic router in the brain, Tryage employs a perceptive router to predict down-stream model performance on prompts and, then, makes a routing decision using an ob
&lt;/p&gt;</description></item><item><title>UniDoc&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#20855;&#22791;&#25991;&#26412;&#26816;&#27979;&#21644;&#35782;&#21035;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#20219;&#21153;&#20043;&#38388;&#30340;&#26377;&#30410;&#20132;&#20114;&#25552;&#39640;&#27599;&#20010;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2308.11592</link><description>&lt;p&gt;
UniDoc: &#19968;&#31181;&#36890;&#29992;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#29992;&#20110;&#21516;&#26102;&#36827;&#34892;&#25991;&#26412;&#26816;&#27979;&#12289;&#35782;&#21035;&#12289;&#23450;&#20301;&#21644;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
UniDoc: A Universal Large Multimodal Model for Simultaneous Text Detection, Recognition, Spotting and Understanding. (arXiv:2308.11592v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11592
&lt;/p&gt;
&lt;p&gt;
UniDoc&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#20855;&#22791;&#25991;&#26412;&#26816;&#27979;&#21644;&#35782;&#21035;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#20219;&#21153;&#20043;&#38388;&#30340;&#26377;&#30410;&#20132;&#20114;&#25552;&#39640;&#27599;&#20010;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26102;&#20195;&#65292;&#22810;&#27169;&#24577;&#29702;&#35299;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#39640;&#32423;&#31639;&#27861;&#21463;&#38480;&#20110;&#26377;&#25928;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#25152;&#22266;&#26377;&#30340;&#24040;&#22823;&#34920;&#31034;&#33021;&#21147;&#21644;&#20016;&#23500;&#30340;&#19990;&#30028;&#30693;&#35782;&#65292;&#24182;&#19988;&#22312;&#25991;&#26412;&#20016;&#23500;&#22330;&#26223;&#20013;&#20219;&#21153;&#20043;&#38388;&#30340;&#26377;&#30410;&#36830;&#25509;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;UniDoc&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#20855;&#22791;&#29616;&#26377;&#26041;&#27861;&#25152;&#32570;&#20047;&#30340;&#25991;&#26412;&#26816;&#27979;&#21644;&#35782;&#21035;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;UniDoc&#21033;&#29992;&#20219;&#21153;&#20043;&#38388;&#30340;&#26377;&#30410;&#20132;&#20114;&#26469;&#25552;&#39640;&#27599;&#20010;&#21333;&#29420;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#23454;&#29616;UniDoc&#65292;&#25105;&#20204;&#23545;&#36129;&#29486;&#30340;&#22823;&#35268;&#27169;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#25351;&#23548;&#35843;&#20248;&#12290;&#23450;&#37327;&#21644;&#23450;&#24615;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UniDoc&#22312;&#22810;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of Large Language Models (LLMs), tremendous strides have been made in the field of multimodal understanding. However, existing advanced algorithms are limited to effectively utilizing the immense representation capabilities and rich world knowledge inherent to these large pre-trained models, and the beneficial connections among tasks within the context of text-rich scenarios have not been sufficiently explored. In this work, we introduce UniDoc, a novel multimodal model equipped with text detection and recognition capabilities, which are deficient in existing approaches. Moreover, UniDoc capitalizes on the beneficial interactions among tasks to enhance the performance of each individual task. To implement UniDoc, we perform unified multimodal instruct tuning on the contributed large-scale instruction following datasets. Quantitative and qualitative experimental results show that UniDoc sets state-of-the-art scores across multiple challenging benchmarks. To the best of our kn
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22240;&#26524;&#20132;&#21449;&#24615;&#21644;&#21452;&#37325;&#26799;&#24230;&#19979;&#38477;&#22312;&#22810;&#27169;&#24577;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#20167;&#24680;&#36855;&#22240;&#26816;&#27979;&#20026;&#20363;&#12290;&#36890;&#36807;&#32467;&#21512;&#22240;&#26524;&#20998;&#26512;&#21644;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#21457;&#29616;&#27169;&#22411;&#30340;&#20869;&#37096;&#26426;&#21046;&#21487;&#20197;&#25581;&#31034;&#20854;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#20171;&#32461;&#20102;&#20132;&#21449;&#24615;&#21644;&#27169;&#24577;&#30340;&#26799;&#24230;&#27880;&#24847;&#21147;&#30340;&#25688;&#35201;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.11585</link><description>&lt;p&gt;
&#22240;&#26524;&#20132;&#21449;&#24615;&#21644;&#21452;&#37325;&#26799;&#24230;&#19979;&#38477;&#22312;&#22810;&#27169;&#24577;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65306;&#20197;&#20167;&#24680;&#36855;&#22240;&#20026;&#20363;&#65288;arXiv:2308.11585v1 [cs.AI]&#65289;
&lt;/p&gt;
&lt;p&gt;
Causal Intersectionality and Dual Form of Gradient Descent for Multimodal Analysis: a Case Study on Hateful Memes. (arXiv:2308.11585v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22240;&#26524;&#20132;&#21449;&#24615;&#21644;&#21452;&#37325;&#26799;&#24230;&#19979;&#38477;&#22312;&#22810;&#27169;&#24577;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#20167;&#24680;&#36855;&#22240;&#26816;&#27979;&#20026;&#20363;&#12290;&#36890;&#36807;&#32467;&#21512;&#22240;&#26524;&#20998;&#26512;&#21644;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#21457;&#29616;&#27169;&#22411;&#30340;&#20869;&#37096;&#26426;&#21046;&#21487;&#20197;&#25581;&#31034;&#20854;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#20171;&#32461;&#20102;&#20132;&#21449;&#24615;&#21644;&#27169;&#24577;&#30340;&#26799;&#24230;&#27880;&#24847;&#21147;&#30340;&#25688;&#35201;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#29190;&#28856;&#24615;&#22686;&#38271;&#65292;&#29305;&#21035;&#26159;&#22312;&#26032;&#20852;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#32972;&#26223;&#19979;&#65292;&#29702;&#35299;&#20854;&#20869;&#37096;&#24037;&#20316;&#20013;&#30340;&#35821;&#20041;&#24847;&#20041;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#22240;&#26524;&#20998;&#26512;&#20391;&#37325;&#20110;&#23450;&#20041;&#35821;&#20041;&#21450;&#20854;&#37327;&#21270;&#65292;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#26159;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#26680;&#24515;&#65292;&#29992;&#20110;&#35299;&#37322;&#40657;&#30418;&#23376;&#30340;&#35299;&#37322;&#12290;&#36890;&#36807;&#21327;&#21516;&#36825;&#20123;&#26041;&#27861;&#65292;&#25506;&#32034;&#27169;&#22411;&#30340;&#20869;&#37096;&#26426;&#21046;&#22914;&#20309;&#38416;&#26126;&#20854;&#22240;&#26524;&#25928;&#24212;&#24050;&#25104;&#20026;&#22522;&#20110;&#35777;&#25454;&#30340;&#20915;&#31574;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#19968;&#31995;&#21015;&#24182;&#34892;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20132;&#21449;&#24615;--&#20010;&#20307;&#30340;&#22810;&#20010;&#20154;&#21475;&#32479;&#35745;&#23398;&#22240;&#32032;&#30340;&#32452;&#21512;&#24433;&#21709;--&#21487;&#20197;&#20197;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;ATE&#65289;&#30340;&#24418;&#24335;&#36827;&#34892;&#32467;&#26500;&#21270;&#12290;&#26368;&#21021;&#65292;&#26412;&#30740;&#31350;&#38416;&#36848;&#20102;&#20167;&#24680;&#36855;&#22240;&#26816;&#27979;&#38382;&#39064;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;ATE&#26469;&#25551;&#36848;&#65292;&#20511;&#21161;&#20132;&#21449;&#24615;&#21407;&#21017;&#65292;&#20197;&#21450;&#22522;&#20110;&#27169;&#24577;&#30340;&#26799;&#24230;&#27880;&#24847;&#21147;&#30340;&#25688;&#35201;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the wake of the explosive growth of machine learning (ML) usage, particularly within the context of emerging Large Language Models (LLMs), comprehending the semantic significance rooted in their internal workings is crucial. While causal analyses focus on defining semantics and its quantification, the gradient-based approach is central to explainable AI (XAI), tackling the interpretation of the black box. By synergizing these approaches, the exploration of how a model's internal mechanisms illuminate its causal effect has become integral for evidence-based decision-making. A parallel line of research has revealed that intersectionality - the combinatory impact of multiple demographics of an individual - can be structured in the form of an Averaged Treatment Effect (ATE). Initially, this study illustrates that the hateful memes detection problem can be formulated as an ATE, assisted by the principles of intersectionality, and that a modality-wise summarization of gradient-based atten
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#21512;&#25104;&#20154;&#31867;&#35265;&#35299;&#19982;&#35745;&#31639;&#33021;&#21147;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#24191;&#27867;&#30340;&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#25968;&#25454;&#38598;&#12290;&#26368;&#32456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ExTES&#30340;&#21487;&#25193;&#23637;&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#24182;&#37096;&#32626;&#20102;&#39640;&#32423;&#35843;&#20248;&#25216;&#26415;&#65292;&#20197;&#35299;&#20915;&#24773;&#24863;&#25903;&#25345;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.11584</link><description>&lt;p&gt;
&#22312;LLMs&#26102;&#20195;&#26500;&#24314;&#24773;&#24863;&#25903;&#25345;&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Building Emotional Support Chatbots in the Era of LLMs. (arXiv:2308.11584v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11584
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#21512;&#25104;&#20154;&#31867;&#35265;&#35299;&#19982;&#35745;&#31639;&#33021;&#21147;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#24191;&#27867;&#30340;&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#25968;&#25454;&#38598;&#12290;&#26368;&#32456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ExTES&#30340;&#21487;&#25193;&#23637;&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#24182;&#37096;&#32626;&#20102;&#39640;&#32423;&#35843;&#20248;&#25216;&#26415;&#65292;&#20197;&#35299;&#20915;&#24773;&#24863;&#25903;&#25345;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#24773;&#24863;&#25903;&#25345;&#38598;&#25104;&#21040;&#21508;&#31181;&#23545;&#35805;&#22330;&#26223;&#20013;&#20855;&#26377;&#28145;&#36828;&#30340;&#31038;&#20250;&#25928;&#30410;&#65292;&#22914;&#31038;&#20132;&#20114;&#21160;&#12289;&#24515;&#29702;&#20581;&#24247;&#21672;&#35810;&#21644;&#23458;&#25143;&#26381;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#19968;&#39046;&#22495;&#23384;&#22312;&#19968;&#20123;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#26377;&#38480;&#30340;&#25968;&#25454;&#21487;&#29992;&#24615;&#21644;&#32570;&#20047;&#34987;&#24191;&#27867;&#25509;&#21463;&#30340;&#27169;&#22411;&#35757;&#32451;&#33539;&#20363;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#65292;&#35797;&#22270;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#23558;&#20154;&#31867;&#35265;&#35299;&#19982;LLMs&#30340;&#35745;&#31639;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#24191;&#27867;&#30340;&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;&#31934;&#24515;&#35774;&#35745;&#30340;&#28085;&#30422;&#22810;&#31181;&#22330;&#26223;&#30340;&#23545;&#35805;&#38598;&#21512;&#20316;&#20026;&#29983;&#25104;&#31181;&#23376;&#24320;&#22987;&#12290;&#36890;&#36807;&#21033;&#29992;ChatGPT&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#28508;&#21147;&#65292;&#25105;&#20204;&#36882;&#24402;&#22320;&#29983;&#25104;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#21629;&#21517;&#20026;ExTES&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#23545;LLaMA&#27169;&#22411;&#36827;&#34892;&#20102;&#39640;&#32423;&#35843;&#20248;&#25216;&#26415;&#30340;&#37096;&#32626;&#65292;&#26816;&#39564;&#20102;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The integration of emotional support into various conversational scenarios presents profound societal benefits, such as social interactions, mental health counseling, and customer service. However, there are unsolved challenges that hinder real-world applications in this field, including limited data availability and the absence of well-accepted model training paradigms. This work endeavors to navigate these challenges by harnessing the capabilities of Large Language Models (LLMs). We introduce an innovative methodology that synthesizes human insights with the computational prowess of LLMs to curate an extensive emotional support dialogue dataset. Our approach is initiated with a meticulously designed set of dialogues spanning diverse scenarios as generative seeds. By utilizing the in-context learning potential of ChatGPT, we recursively generate an ExTensible Emotional Support dialogue dataset, named ExTES. Following this, we deploy advanced tuning techniques on the LLaMA model, exami
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#32508;&#21512;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24773;&#32490;&#35782;&#21035;&#20013;&#34920;&#29616;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#21253;&#25324;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#23569;&#26679;&#26412;&#23398;&#20064;&#21644;&#20934;&#30830;&#24230;&#31561;&#12290;LLMs&#30340;&#20986;&#29616;&#20026;&#24773;&#32490;&#35782;&#21035;&#24314;&#27169;&#24102;&#26469;&#20102;&#26032;&#30340;&#28508;&#21147;&#21644;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2308.11578</link><description>&lt;p&gt;
&#25913;&#21464;&#24773;&#32490;&#35782;&#21035;&#24314;&#27169;&#26041;&#24335;:&#36890;&#29992;&#22823;&#22411;&#27169;&#22411;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
Refashioning Emotion Recognition Modelling: The Advent of Generalised Large Models. (arXiv:2308.11578v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11578
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#32508;&#21512;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24773;&#32490;&#35782;&#21035;&#20013;&#34920;&#29616;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#21253;&#25324;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#23569;&#26679;&#26412;&#23398;&#20064;&#21644;&#20934;&#30830;&#24230;&#31561;&#12290;LLMs&#30340;&#20986;&#29616;&#20026;&#24773;&#32490;&#35782;&#21035;&#24314;&#27169;&#24102;&#26469;&#20102;&#26032;&#30340;&#28508;&#21147;&#21644;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24773;&#32490;&#35782;&#21035;&#25110;&#24773;&#24863;&#35745;&#31639;&#30340;&#35806;&#29983;&#20043;&#21518;&#65292;&#30001;&#20110;&#20854;&#24191;&#27867;&#24212;&#29992;&#65292;&#23427;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#36234;&#26469;&#36234;&#27963;&#36291;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#22312;&#36807;&#21435;&#30340;&#20960;&#21313;&#24180;&#37324;&#65292;&#24773;&#32490;&#35782;&#21035;&#27169;&#22411;&#36880;&#28176;&#20174;&#32479;&#35745;&#27973;&#23618;&#27169;&#22411;&#36801;&#31227;&#21040;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#27169;&#22411;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#24773;&#32490;&#35782;&#21035;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#22987;&#32456;&#21462;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;&#22240;&#27492;&#65292;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#27169;&#22411;&#19968;&#30452;&#34987;&#35270;&#20026;&#24773;&#32490;&#35782;&#21035;&#30340;&#39318;&#36873;&#12290;&#28982;&#32780;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#22914;ChatGPT&#65292;&#30001;&#20110;&#23427;&#20204;&#20855;&#22791;&#30340;&#38646;/&#23569;&#26679;&#26412;&#23398;&#20064;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#36830;&#36143;&#24605;&#32500;&#31561;&#33021;&#21147;&#65292;&#22312;&#24773;&#32490;&#35782;&#21035;&#26041;&#38754;&#24341;&#36215;&#20102;&#24040;&#22823;&#30340;&#24778;&#35766;&#65292;&#32780;&#36825;&#20123;&#33021;&#21147;&#22312;&#20197;&#21069;&#30340;&#28145;&#24230;&#27169;&#22411;&#20013;&#20174;&#26410;&#20986;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#35843;&#26597;&#20102;LLMs&#22312;&#24773;&#32490;&#35782;&#21035;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#21253;&#25324;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#23569;&#26679;&#26412;&#23398;&#20064;&#12289;&#20934;&#30830;&#24230;&#31561;&#21508;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
After the inception of emotion recognition or affective computing, it has increasingly become an active research topic due to its broad applications. Over the past couple of decades, emotion recognition models have gradually migrated from statistically shallow models to neural network-based deep models, which can significantly boost the performance of emotion recognition models and consistently achieve the best results on different benchmarks. Therefore, in recent years, deep models have always been considered the first option for emotion recognition. However, the debut of large language models (LLMs), such as ChatGPT, has remarkably astonished the world due to their emerged capabilities of zero/few-shot learning, in-context learning, chain-of-thought, and others that are never shown in previous deep models. In the present paper, we comprehensively investigate how the LLMs perform in emotion recognition in terms of diverse aspects, including in-context learning, few-short learning, acc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21019;&#26032;&#24615;&#22320;&#23558;&#20174;&#30495;&#23454;&#20154;&#26426;&#23545;&#35805;&#20013;&#25552;&#21462;&#30340;&#20154;&#31867;&#38382;&#39064;&#20316;&#20026;&#23398;&#20064;&#30446;&#26631;&#65292;&#24182;&#19988;&#35757;&#32451;&#20102;&#19968;&#20010;&#29992;&#25143;&#27169;&#25311;&#22120;UserGPT&#65292;&#24182;&#20351;&#29992;&#29983;&#25104;&#30340;&#39640;&#36136;&#37327;&#21512;&#25104;&#23545;&#35805;&#25968;&#25454;&#38598;RealChat&#26469;&#35757;&#32451;&#21161;&#25163;&#27169;&#22411;ReaLM&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ReaLM&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36807;&#20102;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.11534</link><description>&lt;p&gt;
&#20316;&#20026;&#29992;&#25143;&#27169;&#25311;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Language Model as a User Simulator. (arXiv:2308.11534v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21019;&#26032;&#24615;&#22320;&#23558;&#20174;&#30495;&#23454;&#20154;&#26426;&#23545;&#35805;&#20013;&#25552;&#21462;&#30340;&#20154;&#31867;&#38382;&#39064;&#20316;&#20026;&#23398;&#20064;&#30446;&#26631;&#65292;&#24182;&#19988;&#35757;&#32451;&#20102;&#19968;&#20010;&#29992;&#25143;&#27169;&#25311;&#22120;UserGPT&#65292;&#24182;&#20351;&#29992;&#29983;&#25104;&#30340;&#39640;&#36136;&#37327;&#21512;&#25104;&#23545;&#35805;&#25968;&#25454;&#38598;RealChat&#26469;&#35757;&#32451;&#21161;&#25163;&#27169;&#22411;ReaLM&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ReaLM&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36807;&#20102;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38381;&#28304;ChatGPT&#30340;&#21331;&#36234;&#24615;&#33021;&#24341;&#21457;&#20102;&#23545;&#20854;&#27665;&#20027;&#21270;&#30340;&#21162;&#21147;&#65292;&#20511;&#21161;&#30495;&#23454;&#29992;&#25143;&#21644;ChatGPT&#23545;&#35805;&#30340;&#21162;&#21147;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;Vicuna&#26159;&#19968;&#20010;&#24456;&#22909;&#30340;&#20363;&#23376;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;Baize&#21644;UltraChat&#31561;&#21162;&#21147;&#20027;&#35201;&#20381;&#38752;ChatGPT&#26681;&#25454;&#25351;&#20196;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#65292;&#32780;&#19981;&#26159;&#30495;&#23454;&#30340;&#20154;&#31867;&#23398;&#20064;&#65292;&#23548;&#33268;&#33539;&#22260;&#26377;&#38480;&#65292;&#22810;&#26679;&#24615;&#20943;&#24369;&#65292;&#32570;&#20047;&#30495;&#27491;&#30340;&#22810;&#36718;&#23545;&#35805;&#21160;&#24577;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#21019;&#26032;&#24615;&#22320;&#25226;&#20174;&#30495;&#23454;&#20154;&#26426;&#23545;&#35805;&#20013;&#25552;&#21462;&#30340;&#20154;&#31867;&#38382;&#39064;&#20316;&#20026;&#23398;&#20064;&#30446;&#26631;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#29992;&#25143;&#27169;&#25311;&#22120;UserGPT&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#21512;&#25104;&#23545;&#35805;&#25968;&#25454;&#38598;RealChat&#12290;&#38543;&#21518;&#65292;&#35813;&#25968;&#25454;&#38598;&#35757;&#32451;&#25105;&#20204;&#30340;&#21161;&#25163;&#27169;&#22411;ReaLM&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ReaLM&#22312;Vicuna-Bench&#21644;MT-Bench&#20013;&#22343;&#36229;&#36807;&#20102;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The unparalleled performance of closed-sourced ChatGPT has sparked efforts towards its democratization, with notable strides made by leveraging real user and ChatGPT conversations, as evidenced by Vicuna. However, while current endeavors like Baize and UltraChat aim to auto-generate conversational data due to challenges in gathering human participation, they primarily rely on ChatGPT to simulate human behaviors based on directives rather than genuine human learning. This results in a limited scope, diminished diversity, and an absence of genuine multi-round conversational dynamics. To address the above issues, we innovatively target human questions extracted from genuine human-machine conversations as a learning goal and train a user simulator, UserGPT, to produce a high-quality human-centric synthetic conversation dataset, RealChat. Subsequently, this dataset trains our assistant model, ReaLM. Experimentally, ReaLM outpaces baseline models in both Vicuna-Bench and MT-Bench by pairwise
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#40784;&#38899;&#39057;&#29305;&#24449;&#21644;&#25991;&#26412;&#29305;&#24449;&#26469;&#23454;&#29616;&#22768;&#38899;&#20107;&#20214;&#20998;&#31867;&#21644;&#26102;&#38388;&#23450;&#20301;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#33021;&#21147;&#30452;&#25509;&#29983;&#25104;&#24207;&#21015;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#31616;&#27905;&#20840;&#38754;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#26102;&#38388;&#25139;&#25429;&#33719;&#21644;&#20107;&#20214;&#20998;&#31867;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.11530</link><description>&lt;p&gt;
&#22686;&#24378;&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Furnishing Sound Event Detection with Language Model Abilities. (arXiv:2308.11530v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#40784;&#38899;&#39057;&#29305;&#24449;&#21644;&#25991;&#26412;&#29305;&#24449;&#26469;&#23454;&#29616;&#22768;&#38899;&#20107;&#20214;&#20998;&#31867;&#21644;&#26102;&#38388;&#23450;&#20301;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#33021;&#21147;&#30452;&#25509;&#29983;&#25104;&#24207;&#21015;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#31616;&#27905;&#20840;&#38754;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#26102;&#38388;&#25139;&#25429;&#33719;&#21644;&#20107;&#20214;&#20998;&#31867;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#35270;&#35273;&#36328;&#27169;&#24577;&#20013;&#30340;&#33021;&#21147;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;LMs&#22312;&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#65288;SED&#65289;&#20013;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#36229;&#36234;&#20102;&#35270;&#35273;&#39046;&#22495;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#38597;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#40784;&#38899;&#39057;&#29305;&#24449;&#21644;&#25991;&#26412;&#29305;&#24449;&#26469;&#23436;&#25104;&#22768;&#38899;&#20107;&#20214;&#20998;&#31867;&#21644;&#26102;&#38388;&#23450;&#20301;&#12290;&#35813;&#26694;&#26550;&#30001;&#19968;&#20010;&#22768;&#23398;&#32534;&#30721;&#22120;&#12289;&#19968;&#20010;&#23545;&#24212;&#30340;&#25991;&#26412;&#21644;&#38899;&#39057;&#34920;&#31034;&#23545;&#40784;&#30340;&#23545;&#27604;&#27169;&#22359;&#65292;&#20197;&#21450;&#19968;&#20010;&#35299;&#32806;&#30340;&#35821;&#35328;&#35299;&#30721;&#22120;&#32452;&#25104;&#65292;&#29992;&#20110;&#20174;&#38899;&#39057;&#29305;&#24449;&#20013;&#29983;&#25104;&#26102;&#38388;&#21644;&#20107;&#20214;&#24207;&#21015;&#12290;&#19982;&#38656;&#35201;&#22797;&#26434;&#22788;&#29702;&#24182;&#20960;&#20046;&#19981;&#20351;&#29992;&#26377;&#38480;&#38899;&#39057;&#29305;&#24449;&#30340;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#26356;&#31616;&#27905;&#20840;&#38754;&#65292;&#22240;&#20026;&#35821;&#35328;&#27169;&#22411;&#30452;&#25509;&#21033;&#29992;&#20854;&#35821;&#20041;&#33021;&#21147;&#29983;&#25104;&#24207;&#21015;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#35299;&#32806;&#27169;&#22359;&#65292;&#20197;&#23637;&#31034;&#20854;&#23545;&#26102;&#38388;&#25139;&#25429;&#25417;&#21644;&#20107;&#20214;&#20998;&#31867;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the ability of language models (LMs) has attracted increasing attention in visual cross-modality. In this paper, we further explore the generation capacity of LMs for sound event detection (SED), beyond the visual domain. Specifically, we propose an elegant method that aligns audio features and text features to accomplish sound event classification and temporal location. The framework consists of an acoustic encoder, a contrastive module that align the corresponding representations of the text and audio, and a decoupled language decoder that generates temporal and event sequences from the audio characteristic. Compared with conventional works that require complicated processing and barely utilize limited audio features, our model is more concise and comprehensive since language model directly leverage its semantic capabilities to generate the sequences. We investigate different decoupling modules to demonstrate the effectiveness for timestamps capture and event classification
&lt;/p&gt;</description></item><item><title>BERT4CTR&#26159;&#19968;&#31181;&#39640;&#25928;&#26694;&#26550;&#65292;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#19982;&#38750;&#25991;&#26412;&#29305;&#24449;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#28857;&#20987;&#29575;&#39044;&#27979;&#12290;&#23427;&#25506;&#32034;&#20102;&#20004;&#31181;&#25972;&#21512;&#22810;&#27169;&#24577;&#36755;&#20837;&#30340;&#26041;&#27861;&#65292;&#24182;&#35299;&#20915;&#20102;&#25991;&#26412;&#21644;&#38750;&#25991;&#26412;&#36755;&#20837;&#20043;&#38388;&#30340;&#20132;&#21449;&#20449;&#24687;&#23398;&#20064;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.11527</link><description>&lt;p&gt;
BERT4CTR:&#19968;&#31181;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#19982;&#38750;&#25991;&#26412;&#29305;&#24449;&#30456;&#32467;&#21512;&#29992;&#20110;&#28857;&#20987;&#29575;&#39044;&#27979;&#30340;&#39640;&#25928;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
BERT4CTR: An Efficient Framework to Combine Pre-trained Language Model with Non-textual Features for CTR Prediction. (arXiv:2308.11527v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11527
&lt;/p&gt;
&lt;p&gt;
BERT4CTR&#26159;&#19968;&#31181;&#39640;&#25928;&#26694;&#26550;&#65292;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#19982;&#38750;&#25991;&#26412;&#29305;&#24449;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#28857;&#20987;&#29575;&#39044;&#27979;&#12290;&#23427;&#25506;&#32034;&#20102;&#20004;&#31181;&#25972;&#21512;&#22810;&#27169;&#24577;&#36755;&#20837;&#30340;&#26041;&#27861;&#65292;&#24182;&#35299;&#20915;&#20102;&#25991;&#26412;&#21644;&#38750;&#25991;&#26412;&#36755;&#20837;&#20043;&#38388;&#30340;&#20132;&#21449;&#20449;&#24687;&#23398;&#20064;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#28145;&#24230;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#24037;&#19994;&#22330;&#26223;&#20013;&#26174;&#31034;&#20986;&#20102;&#24456;&#22909;&#30340;&#25928;&#30410;&#65292;&#21253;&#25324;&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#65292;&#20294;&#22914;&#20309;&#23558;&#21482;&#22788;&#29702;&#25991;&#26412;&#20449;&#21495;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#19982;&#20855;&#26377;&#38750;&#25991;&#26412;&#29305;&#24449;&#30340;&#39044;&#27979;&#27969;&#31243;&#30456;&#32467;&#21512;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#30446;&#21069;&#26377;&#20004;&#20010;&#26041;&#21521;&#26469;&#25972;&#21512;&#22810;&#27169;&#24577;&#36755;&#20837;&#24182;&#36827;&#34892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#12290;&#19968;&#20010;&#26041;&#21521;&#26159;&#36890;&#36807;&#32858;&#21512;&#23618;&#23558;&#35821;&#35328;&#27169;&#22411;&#21644;&#38750;&#25991;&#26412;&#29305;&#24449;&#30340;&#32467;&#26524;&#36827;&#34892;&#34701;&#21512;&#65292;&#24418;&#25104;&#38598;&#25104;&#26694;&#26550;&#65292;&#20854;&#20013;&#25991;&#26412;&#21644;&#38750;&#25991;&#26412;&#36755;&#20837;&#20043;&#38388;&#30340;&#20132;&#21449;&#20449;&#24687;&#20165;&#22312;&#32858;&#21512;&#23618;&#20013;&#23398;&#20064;&#12290;&#21478;&#19968;&#20010;&#26041;&#21521;&#26159;&#23558;&#38750;&#25991;&#26412;&#29305;&#24449;&#20998;&#21106;&#25104;&#32454;&#31890;&#24230;&#29255;&#27573;&#65292;&#24182;&#23558;&#36825;&#20123;&#29255;&#27573;&#36716;&#25442;&#20026;&#19982;&#25991;&#26412;&#29255;&#27573;&#30456;&#32467;&#21512;&#30340;&#26032;&#26631;&#35760;&#65292;&#20197;&#20415;&#21487;&#20197;&#30452;&#25509;&#36755;&#20837;&#21040;&#35821;&#35328;&#27169;&#22411;&#30340;Transformer&#23618;&#20013;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#22686;&#21152;&#20102;&#23398;&#20064;&#21644;&#25512;&#26029;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although deep pre-trained language models have shown promising benefit in a large set of industrial scenarios, including Click-Through-Rate (CTR) prediction, how to integrate pre-trained language models that handle only textual signals into a prediction pipeline with non-textual features is challenging.  Up to now two directions have been explored to integrate multi-modal inputs in fine-tuning of pre-trained language models. One consists of fusing the outcome of language models and non-textual features through an aggregation layer, resulting into ensemble framework, where the cross-information between textual and non-textual inputs are only learned in the aggregation layer. The second one consists of splitting non-textual features into fine-grained fragments and transforming the fragments to new tokens combined with textual ones, so that they can be fed directly to transformer layers in language models. However, this approach increases the complexity of the learning and inference becau
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;AIOps&#30340;&#23398;&#20064;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#20844;&#20849;&#21644;&#19987;&#26377;&#25968;&#25454;&#19978;&#65292;&#33021;&#22815;&#26377;&#25928;&#24212;&#23545;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#30340;&#26085;&#24535;&#20998;&#26512;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.11526</link><description>&lt;p&gt;
&#22312;AIOps&#19978;&#23398;&#20064;&#26085;&#24535;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Representations on Logs for AIOps. (arXiv:2308.11526v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11526
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;AIOps&#30340;&#23398;&#20064;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#20844;&#20849;&#21644;&#19987;&#26377;&#25968;&#25454;&#19978;&#65292;&#33021;&#22815;&#26377;&#25928;&#24212;&#23545;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#30340;&#26085;&#24535;&#20998;&#26512;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI for IT Operations (AIOps)&#26159;&#19968;&#31181;&#21151;&#33021;&#24378;&#22823;&#30340;&#24179;&#21488;&#65292;Site Reliability Engineers (SREs)&#21487;&#20197;&#20351;&#29992;&#23427;&#26469;&#22312;&#26368;&#23567;&#30340;&#20154;&#24037;&#24178;&#39044;&#19979;&#33258;&#21160;&#21270;&#21644;&#20248;&#21270;&#25805;&#20316;&#24037;&#20316;&#27969;&#31243;&#12290;&#33258;&#21160;&#21270;&#26085;&#24535;&#20998;&#26512;&#26159;AIOps&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#20026;SREs&#25552;&#20379;&#20102;&#20851;&#38190;&#27934;&#23519;&#21147;&#65292;&#20351;&#20854;&#33021;&#22815;&#35782;&#21035;&#21644;&#35299;&#20915;&#25345;&#32493;&#30340;&#25925;&#38556;&#12290;&#26085;&#24535;&#26684;&#24335;&#26816;&#27979;&#12289;&#26085;&#24535;&#20998;&#31867;&#21644;&#26085;&#24535;&#35299;&#26512;&#31561;&#20219;&#21153;&#26159;&#33258;&#21160;&#21270;&#26085;&#24535;&#20998;&#26512;&#30340;&#20851;&#38190;&#32452;&#20214;&#12290;&#22823;&#37096;&#20998;&#36825;&#20123;&#20219;&#21153;&#38656;&#35201;&#26377;&#30417;&#30563;&#23398;&#20064;&#65307;&#28982;&#32780;&#65292;&#30001;&#20110;&#26377;&#38480;&#30340;&#24102;&#26631;&#31614;&#26085;&#24535;&#25968;&#25454;&#21644;&#26085;&#24535;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#65292;&#23384;&#22312;&#22810;&#20010;&#25361;&#25112;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;BERT&#21644;GPT3&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#35757;&#32451;&#20102;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#12290;&#36825;&#20123;&#27169;&#22411;&#25552;&#20379;&#20102;&#24191;&#20041;&#34920;&#31034;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#29992;&#20110;&#21033;&#29992;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#30340;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#22312;&#31185;&#23398;&#21644;&#29983;&#29289;&#23398;&#31561;&#29305;&#23450;&#39046;&#22495;&#30340;LLMs&#30340;&#25104;&#21151;&#30340;&#22522;&#30784;&#19978;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#26085;&#24535;&#25968;&#25454;&#30340;LLM&#65292;&#23427;&#26159;&#22312;&#20844;&#20849;&#21644;&#19987;&#26377;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI for IT Operations (AIOps) is a powerful platform that Site Reliability Engineers (SREs) use to automate and streamline operational workflows with minimal human intervention. Automated log analysis is a critical task in AIOps as it provides key insights for SREs to identify and address ongoing faults. Tasks such as log format detection, log classification, and log parsing are key components of automated log analysis. Most of these tasks require supervised learning; however, there are multiple challenges due to limited labelled log data and the diverse nature of log data. Large Language Models (LLMs) such as BERT and GPT3 are trained using self-supervision on a vast amount of unlabeled data. These models provide generalized representations that can be effectively used for various downstream tasks with limited labelled data. Motivated by the success of LLMs in specific domains like science and biology, this paper introduces a LLM for log data which is trained on public and proprietary 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36234;&#29425;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#36234;&#29425;&#26041;&#27861;&#65292;&#20171;&#32461;&#20102;&#35821;&#20041;&#38450;&#28779;&#22681;&#30340;&#27010;&#24565;&#21644;&#19977;&#31181;&#25216;&#26415;&#23454;&#29616;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.11521</link><description>&lt;p&gt;
&#33258;&#25105;&#27450;&#39575;&#65306;&#36870;&#21521;&#30772;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#38450;&#28779;&#22681;
&lt;/p&gt;
&lt;p&gt;
Self-Deception: Reverse Penetrating the Semantic Firewall of Large Language Models. (arXiv:2308.11521v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11521
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36234;&#29425;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#36234;&#29425;&#26041;&#27861;&#65292;&#20171;&#32461;&#20102;&#35821;&#20041;&#38450;&#28779;&#22681;&#30340;&#27010;&#24565;&#21644;&#19977;&#31181;&#25216;&#26415;&#23454;&#29616;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22914;ChatGPT&#65292;&#20855;&#26377;&#25509;&#36817;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#24778;&#20154;&#33021;&#21147;&#12290;&#34429;&#28982;&#20026;&#21508;&#31181;&#31038;&#20250;&#38656;&#27714;&#25552;&#20379;&#20102;&#20415;&#21033;&#65292;&#20294;LLM&#20063;&#38477;&#20302;&#20102;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#30340;&#25104;&#26412;&#12290;&#22240;&#27492;&#65292;LLM&#24320;&#21457;&#20154;&#21592;&#24050;&#32463;&#37096;&#32626;&#20102;&#35821;&#20041;&#32423;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#25298;&#32477;&#21487;&#33021;&#23548;&#33268;&#19981;&#36866;&#24403;&#20869;&#23481;&#30340;&#25552;&#31034;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#38450;&#24481;&#26426;&#21046;&#24182;&#19981;&#23436;&#20840;&#21487;&#38752;&#65292;&#19968;&#20123;&#25915;&#20987;&#32773;&#24050;&#32463;&#35774;&#35745;&#20986;&#20102;&#8220;&#36234;&#29425;&#8221;&#25552;&#31034;&#65292;&#20020;&#26102;&#20351;LLM&#24536;&#35760;&#20869;&#23481;&#38450;&#24481;&#35268;&#21017;&#24182;&#22238;&#31572;&#20219;&#20309;&#19981;&#36866;&#24403;&#30340;&#38382;&#39064;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#19994;&#30028;&#21644;&#23398;&#26415;&#30028;&#23578;&#26080;&#20851;&#20110;&#36825;&#20123;&#35821;&#20041;&#32423;&#25915;&#20987;&#21644;&#38450;&#24481;&#21407;&#21017;&#30340;&#26126;&#30830;&#35299;&#37322;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;LLM&#36234;&#29425;&#38382;&#39064;&#65292;&#24182;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#36234;&#29425;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35821;&#20041;&#38450;&#28779;&#22681;&#30340;&#27010;&#24565;&#65292;&#24182;&#25552;&#20379;&#20102;&#19977;&#31181;&#25216;&#26415;&#23454;&#29616;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), such as ChatGPT, have emerged with astonishing capabilities approaching artificial general intelligence. While providing convenience for various societal needs, LLMs have also lowered the cost of generating harmful content. Consequently, LLM developers have deployed semantic-level defenses to recognize and reject prompts that may lead to inappropriate content. Unfortunately, these defenses are not foolproof, and some attackers have crafted "jailbreak" prompts that temporarily hypnotize the LLM into forgetting content defense rules and answering any improper questions. To date, there is no clear explanation of the principles behind these semantic-level attacks and defenses in both industry and academia.  This paper investigates the LLM jailbreak problem and proposes an automatic jailbreak method for the first time. We propose the concept of a semantic firewall and provide three technical implementation approaches. Inspired by the attack that penetrates trad
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#24120;&#29992;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#36827;&#34892;&#20102;&#32508;&#21512;&#30740;&#31350;&#21644;&#27604;&#36739;&#65292;&#29305;&#21035;&#24212;&#29992;&#20110;&#23458;&#25143;&#35780;&#35770;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#26816;&#27979;&#37325;&#35201;&#20027;&#39064;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#24182;&#26088;&#22312;&#31361;&#20986;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.11520</link><description>&lt;p&gt;
&#25506;&#32034;&#20027;&#39064;&#24314;&#27169;&#25216;&#26415;&#22312;&#20998;&#26512;&#23458;&#25143;&#35780;&#35770;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Exploring the Power of Topic Modeling Techniques in Analyzing Customer Reviews: A Comparative Analysis. (arXiv:2308.11520v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#24120;&#29992;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#36827;&#34892;&#20102;&#32508;&#21512;&#30740;&#31350;&#21644;&#27604;&#36739;&#65292;&#29305;&#21035;&#24212;&#29992;&#20110;&#23458;&#25143;&#35780;&#35770;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#26816;&#27979;&#37325;&#35201;&#20027;&#39064;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#24182;&#26088;&#22312;&#31361;&#20986;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#24179;&#21488;&#21644;&#24212;&#29992;&#30340;&#25351;&#25968;&#32423;&#22686;&#38271;&#23548;&#33268;&#20102;&#29992;&#25143;&#29983;&#25104;&#30340;&#25991;&#26412;&#20869;&#23481;&#65288;&#21253;&#25324;&#35780;&#35770;&#21644;&#35780;&#20215;&#65289;&#25968;&#37327;&#30340;&#28608;&#22686;&#12290;&#22240;&#27492;&#65292;&#29992;&#25143;&#36890;&#24120;&#22312;&#20174;&#36825;&#20123;&#20869;&#23481;&#20013;&#25552;&#21462;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#25110;&#30456;&#20851;&#20449;&#24687;&#26102;&#38754;&#20020;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31639;&#27861;&#24050;&#34987;&#29992;&#20110;&#20998;&#26512;&#22312;&#32447;&#21487;&#33719;&#24471;&#30340;&#22823;&#37327;&#25991;&#26412;&#25968;&#25454;&#12290;&#36817;&#24180;&#26469;&#65292;&#20027;&#39064;&#24314;&#27169;&#25216;&#26415;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#27969;&#34892;&#24230;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#22320;&#30740;&#31350;&#24182;&#27604;&#36739;&#20102;&#20116;&#31181;&#32463;&#24120;&#20351;&#29992;&#30340;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#24212;&#29992;&#20110;&#23458;&#25143;&#35780;&#35770;&#30340;&#26041;&#27861;&#12290;&#25152;&#30740;&#31350;&#30340;&#26041;&#27861;&#21253;&#25324;&#28508;&#22312;&#35821;&#20041;&#20998;&#26512;&#65288;LSA&#65289;&#12289;&#28508;&#22312;&#29380;&#21033;&#20811;&#38647;&#20998;&#37197;&#65288;LDA&#65289;&#12289;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;NMF&#65289;&#12289;&#24425;&#29699;&#25512;&#29702;&#27169;&#22411;&#65288;PAM&#65289;&#12289;Top2Vec&#21644;BERTopic&#12290;&#36890;&#36807;&#23454;&#38469;&#23637;&#31034;&#23427;&#20204;&#22312;&#26816;&#27979;&#37325;&#35201;&#20027;&#39064;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#26088;&#22312;&#31361;&#20986;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The exponential growth of online social network platforms and applications has led to a staggering volume of user-generated textual content, including comments and reviews. Consequently, users often face difficulties in extracting valuable insights or relevant information from such content. To address this challenge, machine learning and natural language processing algorithms have been deployed to analyze the vast amount of textual data available online. In recent years, topic modeling techniques have gained significant popularity in this domain. In this study, we comprehensively examine and compare five frequently used topic modeling methods specifically applied to customer reviews. The methods under investigation are latent semantic analysis (LSA), latent Dirichlet allocation (LDA), non-negative matrix factorization (NMF), pachinko allocation model (PAM), Top2Vec, and BERTopic. By practically demonstrating their benefits in detecting important topics, we aim to highlight their effica
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36716;&#25442;&#22120;&#30340;&#22810;&#26679;&#22534;&#21472;&#38598;&#25104;&#26694;&#26550;&#65292;&#20197;&#20248;&#21270;&#22810;&#31867;&#25991;&#26412;&#20998;&#31867;&#12290;&#36890;&#36807;&#23558;&#22810;&#20010;&#21333;&#19968;&#36716;&#25442;&#22120;&#20316;&#20026;&#22522;&#23618;&#20998;&#31867;&#22120;&#65292;&#24182;&#24341;&#20837;&#22522;&#20110;RoBERTa&#30340;&#20803;&#23618;&#20998;&#31867;&#22120;&#65292;&#23454;&#29616;&#20102;&#26368;&#20248;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.11519</link><description>&lt;p&gt;
&#20248;&#21270;&#22810;&#31867;&#25991;&#26412;&#20998;&#31867;&#65306;&#21033;&#29992;&#36716;&#25442;&#22120;&#30340;&#22810;&#26679;&#22534;&#21472;&#38598;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Optimizing Multi-Class Text Classification: A Diverse Stacking Ensemble Framework Utilizing Transformers. (arXiv:2308.11519v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36716;&#25442;&#22120;&#30340;&#22810;&#26679;&#22534;&#21472;&#38598;&#25104;&#26694;&#26550;&#65292;&#20197;&#20248;&#21270;&#22810;&#31867;&#25991;&#26412;&#20998;&#31867;&#12290;&#36890;&#36807;&#23558;&#22810;&#20010;&#21333;&#19968;&#36716;&#25442;&#22120;&#20316;&#20026;&#22522;&#23618;&#20998;&#31867;&#22120;&#65292;&#24182;&#24341;&#20837;&#22522;&#20110;RoBERTa&#30340;&#20803;&#23618;&#20998;&#31867;&#22120;&#65292;&#23454;&#29616;&#20102;&#26368;&#20248;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23458;&#25143;&#35780;&#35770;&#22312;&#35780;&#20272;&#23458;&#25143;&#28385;&#24847;&#24230;&#12289;&#25910;&#38598;&#21453;&#39304;&#21644;&#25512;&#21160;&#19994;&#21153;&#25913;&#36827;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#20998;&#26512;&#36825;&#20123;&#35780;&#35770;&#21487;&#20197;&#20026;&#23458;&#25143;&#24773;&#32490;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#21253;&#25324;&#36190;&#32654;&#12289;&#35780;&#35770;&#21644;&#24314;&#35758;&#12290;&#25991;&#26412;&#20998;&#31867;&#25216;&#26415;&#20351;&#20225;&#19994;&#33021;&#22815;&#23558;&#23458;&#25143;&#35780;&#35770;&#20998;&#20026;&#19981;&#21516;&#30340;&#31867;&#21035;&#65292;&#20026;&#26356;&#22909;&#22320;&#20102;&#35299;&#23458;&#25143;&#21453;&#39304;&#25552;&#20379;&#20415;&#21033;&#12290;&#28982;&#32780;&#65292;&#36807;&#25311;&#21512;&#21644;&#20559;&#35265;&#31561;&#25361;&#25112;&#38480;&#21046;&#20102;&#21333;&#20010;&#20998;&#31867;&#22120;&#22312;&#30830;&#20445;&#26368;&#20339;&#39044;&#27979;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#36716;&#25442;&#22120;&#27169;&#22411;&#30340;&#22534;&#21472;&#38598;&#25104;&#22810;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#22810;&#20010;&#21333;&#19968;&#36716;&#25442;&#22120;&#65288;&#21253;&#25324;BERT&#12289;ELECTRA&#21644;DistilBERT&#65289;&#20316;&#20026;&#22522;&#23618;&#20998;&#31867;&#22120;&#65292;&#20197;&#21450;&#22522;&#20110;RoBERTa&#30340;&#20803;&#23618;&#20998;&#31867;&#22120;&#65292;&#29983;&#25104;&#19968;&#20010;&#26368;&#20248;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Customer reviews play a crucial role in assessing customer satisfaction, gathering feedback, and driving improvements for businesses. Analyzing these reviews provides valuable insights into customer sentiments, including compliments, comments, and suggestions. Text classification techniques enable businesses to categorize customer reviews into distinct categories, facilitating a better understanding of customer feedback. However, challenges such as overfitting and bias limit the effectiveness of a single classifier in ensuring optimal prediction. This study proposes a novel approach to address these challenges by introducing a stacking ensemble-based multi-text classification method that leverages transformer models. By combining multiple single transformers, including BERT, ELECTRA, and DistilBERT, as base-level classifiers, and a meta-level classifier based on RoBERTa, an optimal predictive model is generated. The proposed stacking ensemble-based multi-text classification method aims
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#26631;&#20934;&#21270;&#27969;&#30340;&#22810;&#30446;&#26631;&#36319;&#36394;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#24322;&#26500;&#20449;&#24687;&#65292;&#24182;&#35299;&#20915;&#20102;&#25104;&#26412;&#36129;&#29486;&#12289;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#29420;&#31435;&#24615;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.11513</link><description>&lt;p&gt;
TrackFlow: &#24102;&#26377;&#26631;&#20934;&#21270;&#27969;&#30340;&#22810;&#30446;&#26631;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
TrackFlow: Multi-Object Tracking with Normalizing Flows. (arXiv:2308.11513v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#26631;&#20934;&#21270;&#27969;&#30340;&#22810;&#30446;&#26631;&#36319;&#36394;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#24322;&#26500;&#20449;&#24687;&#65292;&#24182;&#35299;&#20915;&#20102;&#25104;&#26412;&#36129;&#29486;&#12289;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#29420;&#31435;&#24615;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#36319;&#36394;-&#26816;&#27979;&#26041;&#27861;&#30340;&#31616;&#27905;&#24615;&#21644;&#24378;&#22823;&#20808;&#39564;&#26465;&#20214;&#20351;&#20854;&#25670;&#33073;&#20102;&#36319;&#36394;-&#27880;&#24847;&#21147;&#26041;&#27861;&#30340;&#22797;&#26434;&#35774;&#35745;&#21644;&#40635;&#28902;&#65292;&#22810;&#30446;&#26631;&#36319;&#36394;&#39046;&#22495;&#23545;&#36319;&#36394;-&#26816;&#27979;&#26041;&#27861;&#37325;&#26032;&#20135;&#29983;&#20102;&#20852;&#36259;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#26088;&#22312;&#23558;&#36319;&#36394;-&#26816;&#27979;&#26041;&#27861;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#35774;&#32622;&#65292;&#20854;&#20013;&#38656;&#35201;&#20174;&#24322;&#26500;&#20449;&#24687;&#65288;&#20363;&#22914;2D&#36816;&#21160;&#32447;&#32034;&#12289;&#35270;&#35273;&#22806;&#35266;&#21644;&#23039;&#24577;&#20272;&#35745;&#65289;&#35745;&#31639;&#32508;&#21512;&#25104;&#26412;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#26469;&#34701;&#21512;&#20855;&#26377;&#31895;&#30053;&#20272;&#35745;&#30340;&#19977;&#32500;&#20449;&#24687;&#21644;&#20854;&#20182;&#20256;&#32479;&#24230;&#37327;&#65288;&#20363;&#22914;IoU&#65289;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#37319;&#29992;&#31616;&#21333;&#35268;&#21017;&#25110;&#22797;&#26434;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#24179;&#34913;&#27599;&#20010;&#25104;&#26412;&#30340;&#36129;&#29486;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#38656;&#35201;&#22312;&#19968;&#20010;&#20445;&#30041;&#38598;&#19978;&#23545;&#23450;&#21046;&#36229;&#21442;&#25968;&#36827;&#34892;&#20180;&#32454;&#35843;&#25972;&#65292;&#24182;&#19988;&#26263;&#31034;&#36825;&#20123;&#25104;&#26412;&#26159;&#30456;&#20114;&#29420;&#31435;&#30340;&#65292;&#32780;&#36825;&#22312;&#29616;&#23454;&#20013;&#24182;&#19981;&#25104;&#31435;&#12290;&#25105;&#20204;&#36890;&#36807;&#24314;&#31435;&#19968;&#20010;&#20248;&#38597;&#30340;&#27010;&#29575;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of multi-object tracking has recently seen a renewed interest in the good old schema of tracking-by-detection, as its simplicity and strong priors spare it from the complex design and painful babysitting of tracking-by-attention approaches. In view of this, we aim at extending tracking-by-detection to multi-modal settings, where a comprehensive cost has to be computed from heterogeneous information e.g., 2D motion cues, visual appearance, and pose estimates. More precisely, we follow a case study where a rough estimate of 3D information is also available and must be merged with other traditional metrics (e.g., the IoU). To achieve that, recent approaches resort to either simple rules or complex heuristics to balance the contribution of each cost. However, i) they require careful tuning of tailored hyperparameters on a hold-out set, and ii) they imply these costs to be independent, which does not hold in reality. We address these issues by building upon an elegant probabilisti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#22810;&#36873;&#39064;&#36873;&#39033;&#39034;&#24207;&#30340;&#25935;&#24863;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#24403;&#23545;&#22238;&#31572;&#36873;&#39033;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#24046;&#36317;&#21487;&#20197;&#36798;&#21040;13%&#33267;75%&#12290;&#36825;&#31181;&#25935;&#24863;&#24615;&#20027;&#35201;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#21069;&#20004;&#20010;/&#19977;&#20010;&#36873;&#39033;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#26102;&#20986;&#29616;&#12290;</title><link>http://arxiv.org/abs/2308.11483</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#22810;&#36873;&#39064;&#36873;&#39033;&#39034;&#24207;&#30340;&#25935;&#24863;&#24615;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions. (arXiv:2308.11483v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#22810;&#36873;&#39064;&#36873;&#39033;&#39034;&#24207;&#30340;&#25935;&#24863;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#24403;&#23545;&#22238;&#31572;&#36873;&#39033;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#24046;&#36317;&#21487;&#20197;&#36798;&#21040;13%&#33267;75%&#12290;&#36825;&#31181;&#25935;&#24863;&#24615;&#20027;&#35201;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#21069;&#20004;&#20010;/&#19977;&#20010;&#36873;&#39033;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#26102;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#23545;&#25552;&#31034;&#25991;&#23383;&#30340;&#25935;&#24863;&#24615;&#20197;&#21450;&#23569;&#26679;&#26412;&#23637;&#31034;&#30340;&#39034;&#24207;&#25935;&#24863;&#24615;&#65292;&#32473;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#20844;&#27491;&#35780;&#20272;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#21464;&#24471;&#26356;&#21152;&#24378;&#22823;&#65292;&#20102;&#35299;&#21644;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#21464;&#24471;&#36843;&#20999;&#12290;&#26412;&#25991;&#20851;&#27880;&#22312;&#22810;&#36873;&#39064;&#20219;&#21153;&#20013;&#65292;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#36873;&#39033;&#39034;&#24207;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#30740;&#31350;&#65292;&#36825;&#26159;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#21644;&#20107;&#23454;&#26816;&#32034;&#33021;&#21147;&#24120;&#29992;&#30340;&#20219;&#21153;&#12290;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#20013;&#22312;&#37325;&#26032;&#25490;&#24207;&#22238;&#31572;&#36873;&#39033;&#26102;&#30340;&#34920;&#29616;&#24046;&#36317;&#30340;&#35843;&#26597;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#30456;&#24046;&#32422;13%&#33267;75%&#12290;&#36890;&#36807;&#35814;&#32454;&#20998;&#26512;&#65292;&#25105;&#20204;&#25512;&#27979;&#36825;&#31181;&#25935;&#24863;&#24615;&#26159;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21069;&#20004;&#20010;/&#19977;&#20010;&#36873;&#39033;&#20043;&#38388;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#26102;&#20135;&#29983;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable capabilities in various NLP tasks. However, previous works have shown these models are sensitive towards prompt wording, and few-shot demonstrations and their order, posing challenges to fair assessment of these models. As these models become more powerful, it becomes imperative to understand and address these limitations. In this paper, we focus on LLMs robustness on the task of multiple-choice questions -- commonly adopted task to study reasoning and fact-retrieving capability of LLMs. Investigating the sensitivity of LLMs towards the order of options in multiple-choice questions, we demonstrate a considerable performance gap of approximately 13% to 75% in LLMs on different benchmarks, when answer options are reordered, even when using demonstrations in a few-shot setting. Through a detailed analysis, we conjecture that this sensitivity arises when LLMs are uncertain about the prediction between the top-2/3 choices, and specif
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23545;&#26426;&#22120;&#23398;&#20064;&#20013;&#20998;&#24067;&#22806;&#26816;&#27979;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21457;&#29616;&#29616;&#26377;&#26041;&#27861;&#22312;&#26816;&#27979;&#26410;&#30693;&#31867;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#36935;&#21040;&#20854;&#20182;&#31867;&#22411;&#30340;&#20998;&#24067;&#21464;&#21270;&#26102;&#24615;&#33021;&#19981;&#31283;&#23450;&#12290;</title><link>http://arxiv.org/abs/2308.11480</link><description>&lt;p&gt;
&#23545;&#24191;&#27867;&#30340;&#20998;&#24067;&#22806;&#26816;&#27979;&#30340;&#26399;&#26395;&#65306;&#26399;&#26395;&#20043;&#22806;&#30340;&#26410;&#30693;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Expecting The Unexpected: Towards Broad Out-Of-Distribution Detection. (arXiv:2308.11480v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11480
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23545;&#26426;&#22120;&#23398;&#20064;&#20013;&#20998;&#24067;&#22806;&#26816;&#27979;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21457;&#29616;&#29616;&#26377;&#26041;&#27861;&#22312;&#26816;&#27979;&#26410;&#30693;&#31867;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#36935;&#21040;&#20854;&#20182;&#31867;&#22411;&#30340;&#20998;&#24067;&#21464;&#21270;&#26102;&#24615;&#33021;&#19981;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#39640;&#37096;&#32626;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#36890;&#24120;&#28041;&#21450;&#24320;&#21457;&#26041;&#27861;&#26469;&#26816;&#27979;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#30340;&#36755;&#20837;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#24120;&#24120;&#29421;&#31364;&#22320;&#20851;&#27880;&#35757;&#32451;&#38598;&#20013;&#32570;&#22833;&#30340;&#31867;&#21035;&#26679;&#26412;&#65292;&#24573;&#30053;&#20102;&#20854;&#20182;&#31867;&#22411;&#30340;&#21487;&#33021;&#20998;&#24067;&#21464;&#21270;&#12290;&#36825;&#31181;&#38480;&#21046;&#38477;&#20302;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#22240;&#20026;&#31995;&#32479;&#20250;&#36935;&#21040;&#21508;&#31181;&#21508;&#26679;&#30340;&#24322;&#24120;&#36755;&#20837;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#20116;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#20998;&#24067;&#21464;&#21270;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#23545;&#26368;&#36817;&#30340;OOD&#26816;&#27979;&#26041;&#27861;&#22312;&#27599;&#19968;&#31181;&#20998;&#24067;&#21464;&#21270;&#19978;&#36827;&#34892;&#20102;&#20851;&#38190;&#35780;&#20272;&#12290;&#25105;&#20204;&#20197;BROAD&#65288;Benchmarking Resilience Over Anomaly Diversity&#65289;&#30340;&#21517;&#20041;&#20844;&#24320;&#21457;&#24067;&#25105;&#20204;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#22312;&#26816;&#27979;&#26410;&#30693;&#31867;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#36935;&#21040;&#20854;&#20182;&#31867;&#22411;&#30340;&#20998;&#24067;&#21464;&#21270;&#26102;&#24615;&#33021;&#19981;&#19968;&#33268;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#23427;&#20204;&#21482;&#33021;&#21487;&#38752;&#22320;&#26816;&#27979;&#21040;&#23427;&#20204;&#29305;&#21035;&#35774;&#35745;&#26469;&#39044;&#26399;&#30340;&#24847;&#22806;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Improving the reliability of deployed machine learning systems often involves developing methods to detect out-of-distribution (OOD) inputs. However, existing research often narrowly focuses on samples from classes that are absent from the training set, neglecting other types of plausible distribution shifts. This limitation reduces the applicability of these methods in real-world scenarios, where systems encounter a wide variety of anomalous inputs. In this study, we categorize five distinct types of distribution shifts and critically evaluate the performance of recent OOD detection methods on each of them. We publicly release our benchmark under the name BROAD (Benchmarking Resilience Over Anomaly Diversity). Our findings reveal that while these methods excel in detecting unknown classes, their performance is inconsistent when encountering other types of distribution shifts. In other words, they only reliably detect unexpected inputs that they have been specifically designed to expec
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25913;&#36827;&#20102;&#22522;&#20110;&#21015;&#29983;&#25104;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#23398;&#20064;&#20998;&#31867;&#26641;&#30340;&#25928;&#26524;&#12290;&#36890;&#36807;&#20943;&#23569;&#23376;&#38382;&#39064;&#25968;&#37327;&#12289;&#20351;&#29992;&#25968;&#25454;&#20381;&#36182;&#32422;&#26463;&#20316;&#20026;&#21106;&#24179;&#38754;&#20197;&#21450;&#29983;&#25104;&#36829;&#21453;&#32422;&#26463;&#30340;&#25968;&#25454;&#28857;&#65292;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#21487;&#20280;&#32553;&#24615;&#24182;&#36866;&#29992;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2308.11477</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#22522;&#20110;&#21015;&#29983;&#25104;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#29992;&#20110;&#23398;&#20064;&#20998;&#31867;&#26641;
&lt;/p&gt;
&lt;p&gt;
Revisiting column-generation-based matheuristic for learning classification trees. (arXiv:2308.11477v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11477
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25913;&#36827;&#20102;&#22522;&#20110;&#21015;&#29983;&#25104;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#23398;&#20064;&#20998;&#31867;&#26641;&#30340;&#25928;&#26524;&#12290;&#36890;&#36807;&#20943;&#23569;&#23376;&#38382;&#39064;&#25968;&#37327;&#12289;&#20351;&#29992;&#25968;&#25454;&#20381;&#36182;&#32422;&#26463;&#20316;&#20026;&#21106;&#24179;&#38754;&#20197;&#21450;&#29983;&#25104;&#36829;&#21453;&#32422;&#26463;&#30340;&#25968;&#25454;&#28857;&#65292;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#21487;&#20280;&#32553;&#24615;&#24182;&#36866;&#29992;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26641;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#35299;&#20915;&#20998;&#31867;&#38382;&#39064;&#30340;&#39640;&#24230;&#21487;&#35299;&#37322;&#24615;&#27169;&#22411;&#12290;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#35757;&#32451;&#20915;&#31574;&#26641;&#24555;&#36895;&#20294;&#29983;&#25104;&#30340;&#26641;&#22312;&#20934;&#30830;&#24615;&#19978;&#19981;&#22815;&#20248;&#21270;&#12290;&#25991;&#29486;&#20013;&#20854;&#20182;&#31163;&#25955;&#20248;&#21270;&#27169;&#22411;&#35299;&#20915;&#20102;&#26368;&#20248;&#24615;&#38382;&#39064;&#20294;&#21482;&#22312;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;firat2020column&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21015;&#29983;&#25104;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#23398;&#20064;&#20915;&#31574;&#26641;&#12290;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#21487;&#20280;&#32553;&#24615;&#65292;&#24182;&#21487;&#20197;&#22788;&#29702;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#23545;&#35813;&#21015;&#29983;&#25104;&#26041;&#27861;&#30340;&#25913;&#36827;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20462;&#25913;&#20102;&#23376;&#38382;&#39064;&#27169;&#22411;&#20197;&#26174;&#33879;&#20943;&#23569;&#22810;&#31867;&#20998;&#31867;&#23454;&#20363;&#20013;&#30340;&#23376;&#38382;&#39064;&#25968;&#37327;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20027;&#38382;&#39064;&#20013;&#30340;&#25968;&#25454;&#20381;&#36182;&#32422;&#26463;&#26159;&#34164;&#21547;&#30340;&#65292;&#24182;&#23558;&#20854;&#29992;&#20316;&#21106;&#24179;&#38754;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#20998;&#31163;&#27169;&#22411;&#26469;&#29983;&#25104;&#32447;&#24615;&#35268;&#21010;&#26494;&#24347;&#35299;&#36829;&#21453;&#20854;&#23545;&#24212;&#30340;&#25968;&#25454;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision trees are highly interpretable models for solving classification problems in machine learning (ML). The standard ML algorithms for training decision trees are fast but generate suboptimal trees in terms of accuracy. Other discrete optimization models in the literature address the optimality problem but only work well on relatively small datasets. \cite{firat2020column} proposed a column-generation-based heuristic approach for learning decision trees. This approach improves scalability and can work with large datasets. In this paper, we describe improvements to this column generation approach. First, we modify the subproblem model to significantly reduce the number of subproblems in multiclass classification instances. Next, we show that the data-dependent constraints in the master problem are implied, and use them as cutting planes. Furthermore, we describe a separation model to generate data points for which the linear programming relaxation solution violates their correspond
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26174;&#24335;&#35270;&#35282;&#21512;&#25104;&#22810;&#35270;&#35282;&#22270;&#20687;&#26469;&#25913;&#36827;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#25216;&#26415;&#30340;&#31574;&#30053;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#22270;&#20687;&#21040;&#22270;&#20687;&#27969;&#27700;&#32447;&#21644;&#22823;&#22411;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#65292;&#24182;&#36890;&#36807;&#37492;&#21035;&#22120;&#21644;&#25193;&#25955;&#29983;&#25104;&#23545;&#25239;&#35757;&#32451;&#26469;&#35299;&#20915;&#35270;&#35282;&#19968;&#33268;&#24615;&#21644;&#20869;&#23481;&#21464;&#21270;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.11473</link><description>&lt;p&gt;
IT3D: &#21033;&#29992;&#26174;&#24335;&#35270;&#35282;&#21512;&#25104;&#25913;&#36827;&#30340;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
IT3D: Improved Text-to-3D Generation with Explicit View Synthesis. (arXiv:2308.11473v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11473
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26174;&#24335;&#35270;&#35282;&#21512;&#25104;&#22810;&#35270;&#35282;&#22270;&#20687;&#26469;&#25913;&#36827;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#25216;&#26415;&#30340;&#31574;&#30053;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#22270;&#20687;&#21040;&#22270;&#20687;&#27969;&#27700;&#32447;&#21644;&#22823;&#22411;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#65292;&#24182;&#36890;&#36807;&#37492;&#21035;&#22120;&#21644;&#25193;&#25955;&#29983;&#25104;&#23545;&#25239;&#35757;&#32451;&#26469;&#35299;&#20915;&#35270;&#35282;&#19968;&#33268;&#24615;&#21644;&#20869;&#23481;&#21464;&#21270;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#25991;&#26412;&#21040;3D&#25216;&#26415;&#30340;&#36827;&#23637;&#20027;&#35201;&#24471;&#30410;&#20110;&#20174;&#24378;&#22823;&#30340;&#22823;&#22411;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65288;LDM&#65289;&#20013;&#25552;&#21462;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25991;&#26412;&#21040;3D&#26041;&#27861;&#24120;&#24120;&#38754;&#20020;&#36807;&#28193;&#39281;&#21644;&#12289;&#32454;&#33410;&#19981;&#36275;&#21644;&#19981;&#30495;&#23454;&#30340;&#36755;&#20986;&#31561;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#26174;&#24335;&#21512;&#25104;&#30340;&#22810;&#35270;&#35282;&#22270;&#20687;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#31895;&#31961;3D&#27169;&#22411;&#28210;&#26579;&#30340;&#22270;&#20687;&#21040;&#22270;&#20687;&#27969;&#27700;&#32447;&#65292;&#20511;&#21161;LDM&#29983;&#25104;&#20855;&#26377;&#23039;&#24577;&#21644;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;&#23613;&#31649;&#29983;&#25104;&#30340;&#22270;&#20687;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#32531;&#35299;&#20102;&#21069;&#36848;&#38382;&#39064;&#65292;&#20294;&#30001;&#20110;&#22823;&#22411;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#24615;&#36136;&#65292;&#35270;&#35282;&#19981;&#19968;&#33268;&#21644;&#20869;&#23481;&#24046;&#24322;&#26174;&#33879;&#30340;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#65292;&#36825;&#22312;&#26377;&#25928;&#21033;&#29992;&#36825;&#20123;&#22270;&#20687;&#26041;&#38754;&#24102;&#26469;&#20102;&#24040;&#22823;&#22256;&#38590;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38556;&#30861;&#65292;&#25105;&#20204;&#25552;&#20513;&#32467;&#21512;&#37492;&#21035;&#22120;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#25193;&#25955;&#29983;&#25104;&#23545;&#25239;&#35757;&#32451;&#31574;&#30053;&#65292;&#26469;&#25351;&#23548;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent strides in Text-to-3D techniques have been propelled by distilling knowledge from powerful large text-to-image diffusion models (LDMs). Nonetheless, existing Text-to-3D approaches often grapple with challenges such as over-saturation, inadequate detailing, and unrealistic outputs. This study presents a novel strategy that leverages explicitly synthesized multi-view images to address these issues. Our approach involves the utilization of image-to-image pipelines, empowered by LDMs, to generate posed high-quality images based on the renderings of coarse 3D models. Although the generated images mostly alleviate the aforementioned issues, challenges such as view inconsistency and significant content variance persist due to the inherent generative nature of large diffusion models, posing extensive difficulties in leveraging these images effectively. To overcome this hurdle, we advocate integrating a discriminator alongside a novel Diffusion-GAN dual training strategy to guide the tra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#24320;&#25918;&#35789;&#27719;&#22686;&#24378;&#30340;&#26234;&#33021;&#23433;&#20840;&#30528;&#38470;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#24320;&#25918;&#35789;&#27719;&#22270;&#20687;&#20998;&#21106;&#30340;&#33021;&#21147;&#23454;&#29616;&#26080;&#20154;&#26426;&#30340;&#35270;&#35273;&#20282;&#26381;&#65292;&#36866;&#24212;&#19981;&#21516;&#22330;&#26223;&#19988;&#26080;&#38656;&#22823;&#37327;&#25968;&#25454;&#31215;&#32047;&#36827;&#34892;&#27169;&#22411;&#25913;&#36827;&#65292;&#21487;&#20197;&#22788;&#29702;100&#31859;&#39640;&#24230;&#30340;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2308.11471</link><description>&lt;p&gt;
&#21160;&#24577;&#24320;&#25918;&#35789;&#27719;&#22686;&#24378;&#30340;&#26234;&#33021;&#23433;&#20840;&#30528;&#38470;&#65288;DOVESEI&#65289;
&lt;/p&gt;
&lt;p&gt;
Dynamic Open Vocabulary Enhanced Safe-landing with Intelligence (DOVESEI). (arXiv:2308.11471v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11471
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#24320;&#25918;&#35789;&#27719;&#22686;&#24378;&#30340;&#26234;&#33021;&#23433;&#20840;&#30528;&#38470;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#24320;&#25918;&#35789;&#27719;&#22270;&#20687;&#20998;&#21106;&#30340;&#33021;&#21147;&#23454;&#29616;&#26080;&#20154;&#26426;&#30340;&#35270;&#35273;&#20282;&#26381;&#65292;&#36866;&#24212;&#19981;&#21516;&#22330;&#26223;&#19988;&#26080;&#38656;&#22823;&#37327;&#25968;&#25454;&#31215;&#32047;&#36827;&#34892;&#27169;&#22411;&#25913;&#36827;&#65292;&#21487;&#20197;&#22788;&#29702;100&#31859;&#39640;&#24230;&#30340;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22478;&#24066;&#31354;&#20013;&#26426;&#22120;&#20154;&#30340;&#22522;&#30784;&#27493;&#39588;&#20043;&#19968;&#65292;&#21363;&#23433;&#20840;&#30528;&#38470;&#12290;&#25105;&#20204;&#20851;&#27880;&#23433;&#20840;&#30528;&#38470;&#24863;&#30693;&#22534;&#26632;&#20013;&#26368;&#20851;&#38190;&#30340;&#26041;&#38754;&#20043;&#19968;&#65292;&#21363;&#20998;&#21106;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#21453;&#24212;&#24335;&#26080;&#20154;&#26426;&#31995;&#32479;&#65292;&#21033;&#29992;&#24320;&#25918;&#35789;&#27719;&#22270;&#20687;&#20998;&#21106;&#30340;&#33021;&#21147;&#23454;&#29616;&#35270;&#35273;&#20282;&#26381;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#22330;&#26223;&#65292;&#24182;&#36890;&#36807;&#20854;&#24320;&#25918;&#35789;&#27719;&#26041;&#27861;&#65292;&#26368;&#23567;&#21270;&#35843;&#25972;&#38656;&#27714;&#65292;&#32469;&#36807;&#23545;&#20869;&#37096;&#27169;&#22411;&#36827;&#34892;&#22823;&#37327;&#25968;&#25454;&#31215;&#32047;&#20197;&#36827;&#34892;&#25913;&#36827;&#30340;&#24517;&#35201;&#24615;&#12290;&#32771;&#34385;&#21040;&#24403;&#22320;&#24403;&#23616;&#30340;&#38480;&#21046;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#20851;&#27880;&#28857;&#26159;&#20174;100&#31859;&#39640;&#24230;&#36215;&#39134;&#30340;&#25805;&#20316;&#12290;&#36825;&#20010;&#36873;&#25321;&#26159;&#26377;&#24847;&#30340;&#65292;&#22240;&#20026;&#35768;&#22810;&#20043;&#21069;&#30340;&#24037;&#20316;&#22788;&#29702;&#30340;&#39640;&#24230;&#20165;&#38480;&#20110;30&#31859;&#65292;&#19982;&#23567;&#22411;&#31435;&#20307;&#30456;&#26426;&#30340;&#33021;&#21147;&#30456;&#21563;&#21512;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#20256;&#32479;&#30340;&#19977;&#32500;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#26469;&#23548;&#33322;&#21097;&#19979;&#30340;20&#31859;&#12290;&#21033;&#29992;&#21333;&#30446;&#30456;&#26426;&#21644;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
This work targets what we consider to be the foundational step for urban airborne robots, a safe landing. Our attention is directed toward what we deem the most crucial aspect of the safe landing perception stack: segmentation. We present a streamlined reactive UAV system that employs visual servoing by harnessing the capabilities of open vocabulary image segmentation. This approach can adapt to various scenarios with minimal adjustments, bypassing the necessity for extensive data accumulation for refining internal models, thanks to its open vocabulary methodology. Given the limitations imposed by local authorities, our primary focus centers on operations originating from altitudes of 100 meters. This choice is deliberate, as numerous preceding works have dealt with altitudes up to 30 meters, aligning with the capabilities of small stereo cameras. Consequently, we leave the remaining 20m to be navigated using conventional 3D path planning methods. Utilizing monocular cameras and image 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#37096;&#36328;&#23618;&#26799;&#24230;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#28151;&#21512;&#27973;&#23618;&#21644;&#28145;&#23618;&#30340;&#26799;&#24230;&#65292;&#22686;&#24378;&#20102;&#28145;&#23618;&#30340;&#30456;&#20284;&#24615;&#65292;&#20174;&#32780;&#25193;&#23637;&#20102;&#22312;&#22788;&#29702;&#31995;&#32479;&#24322;&#36136;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.11464</link><description>&lt;p&gt;
&#22522;&#20110;&#20869;&#37096;&#36328;&#23618;&#26799;&#24230;&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#21516;&#36136;&#24615;&#21040;&#24322;&#36136;&#24615;&#30340;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Internal Cross-layer Gradients for Extending Homogeneity to Heterogeneity in Federated Learning. (arXiv:2308.11464v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11464
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#37096;&#36328;&#23618;&#26799;&#24230;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#28151;&#21512;&#27973;&#23618;&#21644;&#28145;&#23618;&#30340;&#26799;&#24230;&#65292;&#22686;&#24378;&#20102;&#28145;&#23618;&#30340;&#30456;&#20284;&#24615;&#65292;&#20174;&#32780;&#25193;&#23637;&#20102;&#22312;&#22788;&#29702;&#31995;&#32479;&#24322;&#36136;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#19981;&#21487;&#36991;&#20813;&#22320;&#38754;&#20020;&#31995;&#32479;&#24322;&#36136;&#24615;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#22686;&#24378;&#22823;&#22810;&#25968;&#27169;&#22411;&#21516;&#36136;&#24615;FL&#26041;&#27861;&#22788;&#29702;&#31995;&#32479;&#24322;&#36136;&#24615;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#26041;&#26696;&#65292;&#21487;&#20197;&#25193;&#23637;&#23427;&#20204;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#20174;&#35814;&#32454;&#25506;&#32034;&#21516;&#36136;&#24615;&#21644;&#24322;&#36136;&#24615;FL&#35774;&#32622;&#24320;&#22987;&#65292;&#21457;&#29616;&#20102;&#19977;&#20010;&#20851;&#38190;&#35266;&#23519;&#32467;&#26524;&#65306;&#65288;1&#65289;&#23458;&#25143;&#31471;&#24615;&#33021;&#19982;&#23618;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#21576;&#27491;&#30456;&#20851;&#65292;&#65288;2&#65289;&#27973;&#23618;&#27604;&#28145;&#23618;&#20855;&#26377;&#26356;&#39640;&#30340;&#30456;&#20284;&#24615;&#65292;&#65288;3&#65289;&#36739;&#20026;&#24179;&#28369;&#30340;&#26799;&#24230;&#20998;&#24067;&#25351;&#31034;&#20102;&#26356;&#39640;&#30340;&#23618;&#30456;&#20284;&#24615;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;InCo Aggregation&#26041;&#27861;&#65292;&#21033;&#29992;&#20869;&#37096;&#36328;&#23618;&#26799;&#24230;&#65292;&#21363;&#26381;&#21153;&#22120;&#27169;&#22411;&#20013;&#26469;&#33258;&#27973;&#23618;&#21644;&#28145;&#23618;&#30340;&#26799;&#24230;&#28151;&#21512;&#65292;&#20197;&#22686;&#24378;&#28145;&#23618;&#30340;&#30456;&#20284;&#24615;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#23458;&#25143;&#31471;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) inevitably confronts the challenge of system heterogeneity in practical scenarios. To enhance the capabilities of most model-homogeneous FL methods in handling system heterogeneity, we propose a training scheme that can extend their capabilities to cope with this challenge. In this paper, we commence our study with a detailed exploration of homogeneous and heterogeneous FL settings and discover three key observations: (1) a positive correlation between client performance and layer similarities, (2) higher similarities in the shallow layers in contrast to the deep layers, and (3) the smoother gradients distributions indicate the higher layer similarities. Building upon these observations, we propose InCo Aggregation that leverags internal cross-layer gradients, a mixture of gradients from shallow and deep layers within a server model, to augment the similarity in the deep layers without requiring additional communication between clients. Furthermore, our methods 
&lt;/p&gt;</description></item><item><title>LegalBench&#26159;&#19968;&#20010;&#21327;&#21516;&#26500;&#24314;&#30340;&#27861;&#24459;&#25512;&#29702;&#22522;&#20934;&#24211;&#65292;&#28085;&#30422;&#20102;162&#20010;&#20219;&#21153;&#65292;&#21487;&#29992;&#20110;&#34913;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27861;&#24459;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#20026;&#24459;&#24072;&#21644;LLM&#24320;&#21457;&#32773;&#25552;&#20379;&#20102;&#20849;&#21516;&#30340;&#35789;&#27719;&#34920;&#12290;</title><link>http://arxiv.org/abs/2308.11462</link><description>&lt;p&gt;
LegalBench&#65306;&#19968;&#20010;&#29992;&#20110;&#34913;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27861;&#24459;&#25512;&#29702;&#30340;&#21327;&#21516;&#26500;&#24314;&#22522;&#20934;&#24211;
&lt;/p&gt;
&lt;p&gt;
LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models. (arXiv:2308.11462v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11462
&lt;/p&gt;
&lt;p&gt;
LegalBench&#26159;&#19968;&#20010;&#21327;&#21516;&#26500;&#24314;&#30340;&#27861;&#24459;&#25512;&#29702;&#22522;&#20934;&#24211;&#65292;&#28085;&#30422;&#20102;162&#20010;&#20219;&#21153;&#65292;&#21487;&#29992;&#20110;&#34913;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27861;&#24459;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#20026;&#24459;&#24072;&#21644;LLM&#24320;&#21457;&#32773;&#25552;&#20379;&#20102;&#20849;&#21516;&#30340;&#35789;&#27719;&#34920;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#21644;&#27861;&#24459;&#30028;&#23545;&#20854;&#30340;&#37319;&#29992;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;LLMs&#33021;&#22815;&#25191;&#34892;&#21738;&#20123;&#31867;&#22411;&#30340;&#27861;&#24459;&#25512;&#29702;&#65311;&#20026;&#20102;&#26356;&#28145;&#20837;&#22320;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LegalBench&#65306;&#19968;&#20010;&#21327;&#21516;&#26500;&#24314;&#30340;&#27861;&#24459;&#25512;&#29702;&#22522;&#20934;&#24211;&#65292;&#21253;&#21547;162&#20010;&#20219;&#21153;&#65292;&#28085;&#30422;&#20102;&#20845;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#27861;&#24459;&#25512;&#29702;&#12290;LegalBench&#26159;&#36890;&#36807;&#36328;&#23398;&#31185;&#30340;&#36807;&#31243;&#26500;&#24314;&#30340;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#30001;&#27861;&#24459;&#19987;&#19994;&#20154;&#21592;&#35774;&#35745;&#21644;&#25163;&#24037;&#21046;&#20316;&#30340;&#20219;&#21153;&#12290;&#22240;&#20026;&#36825;&#20123;&#19987;&#19994;&#20154;&#21592;&#22312;&#26500;&#24314;&#36807;&#31243;&#20013;&#36215;&#20102;&#20027;&#23548;&#20316;&#29992;&#65292;&#25152;&#20197;&#20219;&#21153;&#35201;&#20040;&#34913;&#37327;&#20102;&#23454;&#38469;&#26377;&#29992;&#30340;&#27861;&#24459;&#25512;&#29702;&#33021;&#21147;&#65292;&#35201;&#20040;&#34913;&#37327;&#20102;&#24459;&#24072;&#20204;&#24863;&#20852;&#36259;&#30340;&#25512;&#29702;&#25216;&#33021;&#12290;&#20026;&#20102;&#20419;&#36827;&#36328;&#23398;&#31185;&#20851;&#20110;&#27861;&#24459;&#30028;LLMs&#30340;&#23545;&#35805;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#27969;&#34892;&#30340;&#27861;&#24459;&#26694;&#26550;&#22914;&#20309;&#25551;&#36848;&#27861;&#24459;&#25512;&#29702;&#65292;&#36825;&#20123;&#26694;&#26550;&#21306;&#20998;&#20102;&#35768;&#22810;&#24418;&#24335;&#65292;&#19982;LegalBench&#30340;&#20219;&#21153;&#23545;&#24212;&#36215;&#26469;&#65292;&#20174;&#32780;&#32473;&#24459;&#24072;&#21644;LLM&#24320;&#21457;&#32773;&#25552;&#20379;&#20102;&#20849;&#21516;&#30340;&#35789;&#27719;&#34920;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of large language models (LLMs) and their adoption by the legal community has given rise to the question: what types of legal reasoning can LLMs perform? To enable greater study of this question, we present LegalBench: a collaboratively constructed legal reasoning benchmark consisting of 162 tasks covering six different types of legal reasoning. LegalBench was built through an interdisciplinary process, in which we collected tasks designed and hand-crafted by legal professionals. Because these subject matter experts took a leading role in construction, tasks either measure legal reasoning capabilities that are practically useful, or measure reasoning skills that lawyers find interesting. To enable cross-disciplinary conversations about LLMs in the law, we additionally show how popular legal frameworks for describing legal reasoning -- which distinguish between its many forms -- correspond to LegalBench tasks, thus giving lawyers and LLM developers a common vocabulary. This p
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35770;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#26080;&#30417;&#30563;&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31867;&#27861;&#65292;&#24182;&#24635;&#32467;&#20102;&#26368;&#26032;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#20026;&#28145;&#20837;&#30740;&#31350;&#34920;&#31034;&#23398;&#20064;&#39046;&#22495;&#30340;&#20154;&#21592;&#25552;&#20379;&#20102;&#19968;&#20010;&#36215;&#28857;&#12290;</title><link>http://arxiv.org/abs/2308.11455</link><description>&lt;p&gt;
&#12298;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#32508;&#36848;&#12299;
&lt;/p&gt;
&lt;p&gt;
A Survey on Self-Supervised Representation Learning. (arXiv:2308.11455v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35770;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#26080;&#30417;&#30563;&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31867;&#27861;&#65292;&#24182;&#24635;&#32467;&#20102;&#26368;&#26032;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#20026;&#28145;&#20837;&#30740;&#31350;&#34920;&#31034;&#23398;&#20064;&#39046;&#22495;&#30340;&#20154;&#21592;&#25552;&#20379;&#20102;&#19968;&#20010;&#36215;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#23398;&#20064;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#26159;&#35768;&#22810;&#20219;&#21153;&#30340;&#26680;&#24515;&#12290;&#26368;&#36817;&#24341;&#20837;&#20102;&#35768;&#22810;&#20801;&#35768;&#26080;&#30417;&#30563;&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#34920;&#31034;&#21487;&#20197;&#24212;&#29992;&#20110;&#20998;&#31867;&#25110;&#29289;&#20307;&#26816;&#27979;&#31561;&#19979;&#28216;&#20219;&#21153;&#20013;&#12290;&#36825;&#20123;&#34920;&#31034;&#30340;&#36136;&#37327;&#25509;&#36817;&#20110;&#26377;&#30417;&#30563;&#23398;&#20064;&#65292;&#32780;&#19981;&#38656;&#35201;&#26631;&#35760;&#30340;&#22270;&#20687;&#12290;&#26412;&#32508;&#36848;&#35770;&#25991;&#20197;&#32479;&#19968;&#30340;&#31526;&#21495;&#34920;&#31034;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#65292;&#25351;&#20986;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31867;&#27861;&#65292;&#23558;&#36825;&#20123;&#26041;&#27861;&#32852;&#31995;&#36215;&#26469;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32508;&#36848;&#36890;&#36807;&#20803;&#20998;&#26512;&#24635;&#32467;&#20102;&#25991;&#29486;&#20013;&#26368;&#26032;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#32508;&#36848;&#26088;&#22312;&#20026;&#24076;&#26395;&#28145;&#20837;&#30740;&#31350;&#34920;&#31034;&#23398;&#20064;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#25552;&#20379;&#19968;&#20010;&#36215;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning meaningful representations is at the heart of many tasks in the field of modern machine learning. Recently, a lot of methods were introduced that allow learning of image representations without supervision. These representations can then be used in downstream tasks like classification or object detection. The quality of these representations is close to supervised learning, while no labeled images are needed. This survey paper provides a comprehensive review of these methods in a unified notation, points out similarities and differences of these methods, and proposes a taxonomy which sets these methods in relation to each other. Furthermore, our survey summarizes the most-recent experimental results reported in the literature in form of a meta-study. Our survey is intended as a starting point for researchers and practitioners who want to dive into the field of representation learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#33268;&#24615;&#27169;&#22411;&#65288;CMs&#65289;&#30340;&#25910;&#25947;&#20445;&#35777;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#22312;&#19968;&#27493;&#20869;&#20174;&#20219;&#20309;&#30495;&#23454;&#25968;&#25454;&#20998;&#24067;&#20013;&#26377;&#25928;&#22320;&#36827;&#34892;&#37319;&#26679;&#65292;&#19988;&#20855;&#26377;&#36739;&#23567;&#30340;$W_2$&#35823;&#24046;&#12290;&#36825;&#19968;&#32467;&#26524;&#23545;&#20110;$L^2$&#31934;&#30830;&#30340;&#20998;&#25968;&#21644;&#19968;&#33268;&#24615;&#20551;&#35774;&#25104;&#31435;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#24378;&#20551;&#35774;&#65292;&#21516;&#26102;&#22312;&#25152;&#26377;&#21442;&#25968;&#19978;&#21576;&#22810;&#39033;&#24335;&#23610;&#24230;&#22686;&#38271;&#65292;&#19982;&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;SGMs&#65289;&#30340;&#26368;&#26032;&#25910;&#25947;&#20445;&#35777;&#30456;&#21305;&#37197;&#12290;&#21478;&#22806;&#65292;&#26412;&#25991;&#36824;&#25552;&#20379;&#20102;&#22810;&#27493;&#19968;&#33268;&#24615;&#37319;&#26679;&#36807;&#31243;&#21487;&#20197;&#36827;&#19968;&#27493;&#20943;&#23567;&#35823;&#24046;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.11449</link><description>&lt;p&gt;
&#19968;&#33268;&#24615;&#27169;&#22411;&#30340;&#25910;&#25947;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Convergence guarantee for consistency models. (arXiv:2308.11449v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11449
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#33268;&#24615;&#27169;&#22411;&#65288;CMs&#65289;&#30340;&#25910;&#25947;&#20445;&#35777;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#22312;&#19968;&#27493;&#20869;&#20174;&#20219;&#20309;&#30495;&#23454;&#25968;&#25454;&#20998;&#24067;&#20013;&#26377;&#25928;&#22320;&#36827;&#34892;&#37319;&#26679;&#65292;&#19988;&#20855;&#26377;&#36739;&#23567;&#30340;$W_2$&#35823;&#24046;&#12290;&#36825;&#19968;&#32467;&#26524;&#23545;&#20110;$L^2$&#31934;&#30830;&#30340;&#20998;&#25968;&#21644;&#19968;&#33268;&#24615;&#20551;&#35774;&#25104;&#31435;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#24378;&#20551;&#35774;&#65292;&#21516;&#26102;&#22312;&#25152;&#26377;&#21442;&#25968;&#19978;&#21576;&#22810;&#39033;&#24335;&#23610;&#24230;&#22686;&#38271;&#65292;&#19982;&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;SGMs&#65289;&#30340;&#26368;&#26032;&#25910;&#25947;&#20445;&#35777;&#30456;&#21305;&#37197;&#12290;&#21478;&#22806;&#65292;&#26412;&#25991;&#36824;&#25552;&#20379;&#20102;&#22810;&#27493;&#19968;&#33268;&#24615;&#37319;&#26679;&#36807;&#31243;&#21487;&#20197;&#36827;&#19968;&#27493;&#20943;&#23567;&#35823;&#24046;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#39318;&#27425;&#20026;&#19968;&#33268;&#24615;&#27169;&#22411;&#65288;CMs&#65289;&#25552;&#20379;&#20102;&#25910;&#25947;&#20445;&#35777;&#65292;&#23427;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#19968;&#27493;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#19982;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#26679;&#26412;&#30456;&#23218;&#32654;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#65292;&#22312;&#22522;&#26412;&#30340;&#20998;&#25968;&#21305;&#37197;&#35823;&#24046;&#12289;&#19968;&#33268;&#24615;&#35823;&#24046;&#21644;&#25968;&#25454;&#20998;&#24067;&#30340;&#24179;&#28369;&#24615;&#30340;&#20551;&#35774;&#19979;&#65292;CMs&#33021;&#22815;&#20197;&#23567;&#30340;$W_2$&#35823;&#24046;&#22312;&#19968;&#27493;&#20869;&#26377;&#25928;&#22320;&#20174;&#20219;&#20309;&#30495;&#23454;&#25968;&#25454;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#65306;&#65288;1&#65289;&#36866;&#29992;&#20110;$L^2$&#31934;&#30830;&#30340;&#20998;&#25968;&#21644;&#19968;&#33268;&#24615;&#20551;&#35774;&#65288;&#32780;&#38750;$L^\infty$&#31934;&#30830;&#65289;&#65307;&#65288;2&#65289;&#19981;&#38656;&#35201;&#23545;&#25968;&#25454;&#20998;&#24067;&#20570;&#20986;&#22914;log-Sobelev&#19981;&#31561;&#24335;&#30340;&#24378;&#20551;&#35774;&#65307;&#65288;3&#65289;&#25152;&#26377;&#21442;&#25968;&#30340;&#23610;&#24230;&#22810;&#39033;&#24335;&#22320;&#22686;&#38271;&#65307;&#65288;4&#65289;&#19982;&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;SGMs&#65289;&#30340;&#26368;&#26032;&#25910;&#25947;&#20445;&#35777;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#22810;&#27493;&#19968;&#33268;&#24615;&#37319;&#26679;&#36807;&#31243;&#21487;&#20197;&#36827;&#19968;&#27493;&#20943;&#23567;&#35823;&#24046;&#30340;&#32467;&#26524;&#65292;&#25903;&#25745;&#20102;&#21407;&#22987;&#35770;&#36848;&#20013;&#30340;"&#19968;&#33268;&#24615;&#27169;&#22411;"&#65292;&#26472;&#26494;
&lt;/p&gt;
&lt;p&gt;
We provide the first convergence guarantees for the Consistency Models (CMs), a newly emerging type of one-step generative models that can generate comparable samples to those generated by Diffusion Models. Our main result is that, under the basic assumptions on score-matching errors, consistency errors and smoothness of the data distribution, CMs can efficiently sample from any realistic data distribution in one step with small $W_2$ error. Our results (1) hold for $L^2$-accurate score and consistency assumption (rather than $L^\infty$-accurate); (2) do note require strong assumptions on the data distribution such as log-Sobelev inequality; (3) scale polynomially in all parameters; and (4) match the state-of-the-art convergence guarantee for score-based generative models (SGMs). We also provide the result that the Multistep Consistency Sampling procedure can further reduce the error comparing to one step sampling, which support the original statement of "Consistency Models, Yang Song 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26041;&#38754;&#30340;&#35266;&#28857;&#23545;&#40784;&#32593;&#32476;(AOAN)&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#31867;&#20013;&#30340;&#35821;&#20041;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#37051;&#36817;&#21306;&#38388;&#22686;&#24378;&#27169;&#22359;&#21644;&#22810;&#35282;&#24230;&#27880;&#24847;&#26426;&#21046;&#65292;&#35813;&#32593;&#32476;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#35266;&#28857;&#35789;&#19982;&#30456;&#24212;&#26041;&#38754;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#20851;&#32852;&#12290;</title><link>http://arxiv.org/abs/2308.11447</link><description>&lt;p&gt;
&#38754;&#21521;&#26041;&#38754;&#30340;&#35266;&#28857;&#23545;&#40784;&#32593;&#32476;&#29992;&#20110;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Aspect-oriented Opinion Alignment Network for Aspect-Based Sentiment Classification. (arXiv:2308.11447v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26041;&#38754;&#30340;&#35266;&#28857;&#23545;&#40784;&#32593;&#32476;(AOAN)&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#31867;&#20013;&#30340;&#35821;&#20041;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#37051;&#36817;&#21306;&#38388;&#22686;&#24378;&#27169;&#22359;&#21644;&#22810;&#35282;&#24230;&#27880;&#24847;&#26426;&#21046;&#65292;&#35813;&#32593;&#32476;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#35266;&#28857;&#35789;&#19982;&#30456;&#24212;&#26041;&#38754;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#31867;&#26159;&#32454;&#31890;&#24230;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#20854;&#26088;&#22312;&#26681;&#25454;&#19978;&#19979;&#25991;&#39044;&#27979;&#32473;&#23450;&#26041;&#38754;&#30340;&#24773;&#24863;&#26497;&#24615;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#22312;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#25552;&#21462;&#19981;&#21516;&#26041;&#38754;&#30340;&#35266;&#28857;&#35789;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#25345;&#20037;&#30340;&#25361;&#25112;&#26159;&#26377;&#25928;&#22788;&#29702;&#35821;&#20041;&#19981;&#21305;&#37197;&#65292;&#36825;&#26159;&#30001;&#20110;&#27880;&#24847;&#26426;&#21046;&#22312;&#22810;&#26041;&#38754;&#21477;&#23376;&#20013;&#26410;&#33021;&#20805;&#20998;&#23545;&#40784;&#35266;&#28857;&#35789;&#19982;&#30456;&#24212;&#26041;&#38754;&#24341;&#36215;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38754;&#21521;&#26041;&#38754;&#30340;&#35266;&#28857;&#23545;&#40784;&#32593;&#32476;(AOAN)&#65292;&#20197;&#25429;&#25417;&#35266;&#28857;&#35789;&#19982;&#30456;&#24212;&#26041;&#38754;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#20851;&#32852;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#19968;&#20010;&#37051;&#36817;&#21306;&#38388;&#22686;&#24378;&#27169;&#22359;&#65292;&#24378;&#35843;&#37051;&#36817;&#21333;&#35789;&#21644;&#32473;&#23450;&#26041;&#38754;&#30340;&#21508;&#31181;&#32452;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22810;&#35282;&#24230;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#30456;&#24212;&#32473;&#23450;&#26041;&#38754;&#23545;&#40784;&#30456;&#20851;&#35266;&#28857;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aspect-based sentiment classification is a crucial problem in fine-grained sentiment analysis, which aims to predict the sentiment polarity of the given aspect according to its context. Previous works have made remarkable progress in leveraging attention mechanism to extract opinion words for different aspects. However, a persistent challenge is the effective management of semantic mismatches, which stem from attention mechanisms that fall short in adequately aligning opinions words with their corresponding aspect in multi-aspect sentences. To address this issue, we propose a novel Aspect-oriented Opinion Alignment Network (AOAN) to capture the contextual association between opinion words and the corresponding aspect. Specifically, we first introduce a neighboring span enhanced module which highlights various compositions of neighboring words and given aspects. In addition, we design a multi-perspective attention mechanism that align relevant opinion information with respect to the giv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36807;&#31243;&#26469;&#25506;&#32034;&#21644;&#20998;&#26512;&#21307;&#30103;&#25968;&#25454;&#20013;&#30340;&#25289;&#33298;&#33945;&#38598;&#21512;&#27169;&#22411;&#65292;&#20174;&#32780;&#36229;&#36234;&#20256;&#32479;&#21333;&#19968;&#27169;&#22411;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;"&#25289;&#33298;&#33945;&#26816;&#27979;"&#31639;&#27861;&#35782;&#21035;&#20986;&#38598;&#21512;&#20013;&#26368;&#19981;&#21516;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.11446</link><description>&lt;p&gt;
&#25506;&#32034;&#25289;&#33298;&#33945;&#38598;&#21512;&#26377;&#21161;&#20110;&#21307;&#30103;&#25968;&#25454;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Exploration of Rashomon Set Assists Explanations for Medical Data. (arXiv:2308.11446v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36807;&#31243;&#26469;&#25506;&#32034;&#21644;&#20998;&#26512;&#21307;&#30103;&#25968;&#25454;&#20013;&#30340;&#25289;&#33298;&#33945;&#38598;&#21512;&#27169;&#22411;&#65292;&#20174;&#32780;&#36229;&#36234;&#20256;&#32479;&#21333;&#19968;&#27169;&#22411;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;"&#25289;&#33298;&#33945;&#26816;&#27979;"&#31639;&#27861;&#35782;&#21035;&#20986;&#38598;&#21512;&#20013;&#26368;&#19981;&#21516;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#24314;&#27169;&#36807;&#31243;&#36890;&#24120;&#20197;&#36873;&#25321;&#26368;&#22823;&#21270;&#26576;&#20010;&#24615;&#33021;&#25351;&#26631;&#30340;&#21333;&#19968;&#27169;&#22411;&#20316;&#20026;&#26368;&#32456;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#20250;&#23548;&#33268;&#23545;&#31245;&#24494;&#24046;&#19968;&#20123;&#30340;&#27169;&#22411;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#20998;&#26512;&#34987;&#24573;&#35270;&#12290;&#23588;&#20854;&#22312;&#21307;&#30103;&#21644;&#20581;&#24247;&#30740;&#31350;&#20013;&#65292;&#30446;&#26631;&#19981;&#20165;&#20165;&#26159;&#39044;&#27979;&#65292;&#36824;&#21253;&#25324;&#20135;&#29983;&#26377;&#20215;&#20540;&#30340;&#27934;&#23519;&#65292;&#20165;&#20165;&#20381;&#36182;&#24615;&#33021;&#25351;&#26631;&#21487;&#33021;&#20250;&#23548;&#33268;&#35823;&#23548;&#25110;&#19981;&#23436;&#25972;&#30340;&#32467;&#35770;&#12290;&#24403;&#22788;&#29702;&#19968;&#32452;&#24615;&#33021;&#25509;&#36817;&#26368;&#20248;&#30340;&#27169;&#22411;&#38598;&#21512;&#26102;&#65292;&#21363;&#25152;&#35859;&#30340;"&#25289;&#33298;&#33945;&#38598;&#21512;"&#65292;&#36825;&#20010;&#38382;&#39064;&#23588;&#20026;&#31361;&#20986;&#12290;&#36825;&#26679;&#30340;&#38598;&#21512;&#21487;&#33021;&#21253;&#21547;&#25551;&#36848;&#25968;&#25454;&#30340;&#19981;&#21516;&#26041;&#24335;&#30340;&#27169;&#22411;&#65292;&#38656;&#35201;&#36827;&#34892;&#20840;&#38754;&#30340;&#20998;&#26512;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#36807;&#31243;&#26469;&#25506;&#32034;&#25289;&#33298;&#33945;&#38598;&#21512;&#27169;&#22411;&#65292;&#25193;&#23637;&#20102;&#20256;&#32479;&#24314;&#27169;&#26041;&#27861;&#12290;&#26680;&#24515;&#26159;&#36890;&#36807;&#24341;&#20837;&#30340;"&#25289;&#33298;&#33945;&#26816;&#27979;"&#31639;&#27861;&#26469;&#35782;&#21035;&#25289;&#33298;&#33945;&#38598;&#21512;&#20013;&#26368;&#19981;&#21516;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The machine learning modeling process conventionally culminates in selecting a single model that maximizes a selected performance metric. However, this approach leads to abandoning a more profound analysis of slightly inferior models. Particularly in medical and healthcare studies, where the objective extends beyond predictions to valuable insight generation, relying solely on performance metrics can result in misleading or incomplete conclusions. This problem is particularly pertinent when dealing with a set of models with performance close to maximum one, known as $\textit{Rashomon set}$. Such a set can be numerous and may contain models describing the data in a different way, which calls for comprehensive analysis. This paper introduces a novel process to explore Rashomon set models, extending the conventional modeling approach. The cornerstone is the identification of the most different models within the Rashomon set, facilitated by the introduced $\texttt{Rashomon_DETECT}$ algorit
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#20195;&#29702;&#30340;&#30740;&#31350;&#65292;&#25552;&#20379;&#20102;&#20174;&#25972;&#20307;&#35282;&#24230;&#23545;&#35813;&#39046;&#22495;&#30340;&#31995;&#32479;&#23457;&#26597;&#65292;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#21033;&#29992;&#22823;&#37327;&#32593;&#32476;&#30693;&#35782;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#30340;&#26234;&#33021;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2308.11432</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#20195;&#29702;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Large Language Model based Autonomous Agents. (arXiv:2308.11432v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11432
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#20195;&#29702;&#30340;&#30740;&#31350;&#65292;&#25552;&#20379;&#20102;&#20174;&#25972;&#20307;&#35282;&#24230;&#23545;&#35813;&#39046;&#22495;&#30340;&#31995;&#32479;&#23457;&#26597;&#65292;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#21033;&#29992;&#22823;&#37327;&#32593;&#32476;&#30693;&#35782;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#30340;&#26234;&#33021;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#20195;&#29702;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#23398;&#26415;&#30028;&#30340;&#30740;&#31350;&#28909;&#28857;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#24448;&#24448;&#38598;&#20013;&#22312;&#23545;&#26377;&#38480;&#30693;&#35782;&#30340;&#20195;&#29702;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#36825;&#19982;&#20154;&#31867;&#30340;&#23398;&#20064;&#36807;&#31243;&#23384;&#22312;&#26126;&#26174;&#24046;&#24322;&#65292;&#22240;&#27492;&#24456;&#38590;&#23454;&#29616;&#20154;&#31867;&#33324;&#30340;&#20915;&#31574;&#12290;&#36817;&#24180;&#26469;&#65292;&#36890;&#36807;&#33719;&#21462;&#22823;&#37327;&#30340;&#32593;&#32476;&#30693;&#35782;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#29616;&#20986;&#20102;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#26234;&#33021;&#30340;&#26174;&#33879;&#28508;&#21147;&#12290;&#36825;&#24341;&#21457;&#20102;&#23545;&#22522;&#20110;LLM&#30340;&#33258;&#20027;&#20195;&#29702;&#30340;&#30740;&#31350;&#30340;&#39640;&#28072;&#20852;&#36259;&#12290;&#20026;&#20102;&#21457;&#25381;LLM&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#30740;&#31350;&#20154;&#21592;&#35774;&#35745;&#20102;&#21508;&#31181;&#19981;&#21516;&#24212;&#29992;&#30340;&#20195;&#29702;&#20307;&#31995;&#32467;&#26500;&#12290;&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#36825;&#20123;&#30740;&#31350;&#65292;&#20174;&#25972;&#20307;&#30340;&#35282;&#24230;&#23545;&#33258;&#20027;&#20195;&#29702;&#39046;&#22495;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#23457;&#26597;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#26500;&#24314;&#65292;&#20026;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous agents have long been a prominent research topic in the academic community. Previous research in this field often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from the human learning processes, and thus makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of web knowledge, large language models (LLMs) have demonstrated remarkable potential in achieving human-level intelligence. This has sparked an upsurge in studies investigating autonomous agents based on LLMs. To harness the full potential of LLMs, researchers have devised diverse agent architectures tailored to different applications. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of the field of autonomous agents from a holistic perspective. More specifically, our focus lies in the construction of LLM-based agents, for which we propose a unified framework t
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20998;&#20139;&#20102;&#19968;&#20010;HCI&#30740;&#31350;&#21592;&#19982;AI&#20114;&#21160;&#30340;&#31532;&#19968;&#20154;&#31216;&#25925;&#20107;&#65292;&#30740;&#31350;&#26088;&#22312;&#23581;&#35797;&#25670;&#33073;&#21019;&#36896;&#24615;&#38459;&#30861;&#12290;&#36890;&#36807;&#25506;&#32034;AI&#22914;&#20309;&#25903;&#25345;&#33402;&#26415;&#23478;&#30340;&#21019;&#36896;&#21147;&#20197;&#21450;&#21487;&#35299;&#37322;&#24615;&#30340;&#21547;&#20041;&#65292;&#27492;&#30740;&#31350;&#20026;XAIxArts&#31038;&#21306;&#25552;&#20379;&#20102;&#36827;&#19968;&#27493;&#35752;&#35770;&#21644;&#25506;&#32034;&#30340;&#24605;&#32771;&#12290;</title><link>http://arxiv.org/abs/2308.11424</link><description>&lt;p&gt;
AIxArtist: &#19982;&#20154;&#24037;&#26234;&#33021;&#20114;&#21160;&#20197;&#25670;&#33073;&#21019;&#36896;&#24615;&#38459;&#30861;&#30340;&#31532;&#19968;&#20154;&#31216;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
AIxArtist: A First-Person Tale of Interacting with Artificial Intelligence to Escape Creative Block. (arXiv:2308.11424v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11424
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20998;&#20139;&#20102;&#19968;&#20010;HCI&#30740;&#31350;&#21592;&#19982;AI&#20114;&#21160;&#30340;&#31532;&#19968;&#20154;&#31216;&#25925;&#20107;&#65292;&#30740;&#31350;&#26088;&#22312;&#23581;&#35797;&#25670;&#33073;&#21019;&#36896;&#24615;&#38459;&#30861;&#12290;&#36890;&#36807;&#25506;&#32034;AI&#22914;&#20309;&#25903;&#25345;&#33402;&#26415;&#23478;&#30340;&#21019;&#36896;&#21147;&#20197;&#21450;&#21487;&#35299;&#37322;&#24615;&#30340;&#21547;&#20041;&#65292;&#27492;&#30740;&#31350;&#20026;XAIxArts&#31038;&#21306;&#25552;&#20379;&#20102;&#36827;&#19968;&#27493;&#35752;&#35770;&#21644;&#25506;&#32034;&#30340;&#24605;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#33402;&#26415;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#26410;&#26469;&#20805;&#28385;&#24076;&#26395;&#12290;&#38543;&#30528;AI&#22312;&#35774;&#35745;&#20013;&#30340;&#24212;&#29992;&#26085;&#30410;&#26222;&#21450;&#65292;&#33402;&#26415;&#23454;&#36341;&#21487;&#33021;&#19981;&#20877;&#21482;&#26159;&#20154;&#31867;&#30340;&#33402;&#26415;&#24418;&#24335;&#65292;&#32780;&#21487;&#33021;&#25104;&#20026;&#19968;&#31181;&#25968;&#23383;&#21270;&#25972;&#21512;&#30340;&#20307;&#39564;&#12290;&#36890;&#36807;&#22686;&#24378;&#21019;&#36896;&#21147;&#21644;&#21512;&#20316;&#65292;&#33402;&#26415;&#21644;AI&#21487;&#20197;&#20849;&#21516;&#21162;&#21147;&#21019;&#36896;&#20986;&#35270;&#35273;&#19978;&#21560;&#24341;&#20154;&#24182;&#28385;&#36275;&#33402;&#26415;&#23478;&#21644;&#35266;&#20247;&#38656;&#27714;&#30340;&#33402;&#26415;&#20316;&#21697;&#12290;&#34429;&#28982;&#20154;&#20204;&#19981;&#30830;&#23450;&#25972;&#21512;&#23558;&#20250;&#36798;&#21040;&#20309;&#31181;&#31243;&#24230;&#65292;&#20294;&#33402;&#26415;&#21644;AI&#24456;&#21487;&#33021;&#20250;&#30456;&#20114;&#24433;&#21709;&#12290;&#26412;&#30740;&#35752;&#20250;&#22270;&#25991;&#24182;&#33538;&#22320;&#25552;&#20986;&#20102;&#19968;&#39033;&#31532;&#19968;&#20154;&#31216;&#30740;&#31350;&#65292;&#20998;&#20139;&#20102;&#19968;&#20010;&#20154;&#26426;&#20132;&#20114;&#30740;&#31350;&#21592;&#19982;AI&#20114;&#21160;&#30340;&#32463;&#21382;&#65292;&#35797;&#22270;&#25670;&#33073;&#21019;&#36896;&#24615;&#38459;&#30861;&#12290;&#36825;&#31687;&#22270;&#25991;&#35770;&#25991;&#25506;&#35752;&#20102;&#20004;&#20010;&#38382;&#39064;&#65306;AI&#22914;&#20309;&#25903;&#25345;&#33402;&#26415;&#23478;&#30340;&#21019;&#36896;&#21147;&#65292;&#20197;&#21450;&#36825;&#31181;&#24773;&#22659;&#19979;&#30340;&#21487;&#35299;&#37322;&#24615;&#24847;&#21619;&#30528;&#20160;&#20040;&#65311;&#30740;&#31350;&#21592;&#19982;ChatGPT&#21644;Midjourney&#36827;&#34892;&#20102;&#20114;&#21160;&#65292;&#32467;&#26524;&#20135;&#29983;&#20102;&#19968;&#31995;&#21015;&#38656;&#35201;&#22312;XAIxArts&#31038;&#21306;&#36827;&#19968;&#27493;&#35752;&#35770;&#21644;&#25506;&#32034;&#30340;&#24605;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
The future of the arts and artificial intelligence (AI) is promising as technology advances. As the use of AI in design becomes more widespread, art practice may not be a human-only art form and could instead become a digitally integrated experience. With enhanced creativity and collaboration, arts and AI could work together towards creating artistic outputs that are visually appealing and meet the needs of the artist and viewer. While it is uncertain how far the integration will go, arts and AI will likely influence one another. This workshop pictorial puts forward first-person research that shares interactions between an HCI researcher and AI as they try to escape the creative block. The pictorial paper explores two questions: How can AI support artists' creativity, and what does it mean to be explainable in this context? HIs, ChatGPT and Midjourney were engaged; the result was a series of reflections that require further discussion and explorations in the XAIxArts community: Transpa
&lt;/p&gt;</description></item><item><title>TurboViT&#26159;&#36890;&#36807;&#29983;&#25104;&#24335;&#26550;&#26500;&#25628;&#32034;&#29983;&#25104;&#30340;&#24555;&#36895;&#35270;&#35273;&#21464;&#21387;&#22120;&#26550;&#26500;&#35774;&#35745;&#65292;&#23427;&#22312;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2308.11421</link><description>&lt;p&gt;
TurboViT: &#36890;&#36807;&#29983;&#25104;&#24335;&#26550;&#26500;&#25628;&#32034;&#29983;&#25104;&#24555;&#36895;&#35270;&#35273;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
TurboViT: Generating Fast Vision Transformers via Generative Architecture Search. (arXiv:2308.11421v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11421
&lt;/p&gt;
&lt;p&gt;
TurboViT&#26159;&#36890;&#36807;&#29983;&#25104;&#24335;&#26550;&#26500;&#25628;&#32034;&#29983;&#25104;&#30340;&#24555;&#36895;&#35270;&#35273;&#21464;&#21387;&#22120;&#26550;&#26500;&#35774;&#35745;&#65292;&#23427;&#22312;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35270;&#35273;&#21464;&#21387;&#22120;&#22312;&#22788;&#29702;&#21508;&#31181;&#35270;&#35273;&#24863;&#30693;&#20219;&#21153;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#32593;&#32476;&#26550;&#26500;&#30340;&#32467;&#26500;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#20351;&#24471;&#23427;&#20204;&#22312;&#20855;&#26377;&#39640;&#21534;&#21520;&#37327;&#21644;&#20302;&#20869;&#23384;&#35201;&#27714;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#38590;&#20197;&#37096;&#32626;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#22312;&#26377;&#25928;&#35270;&#35273;&#21464;&#21387;&#22120;&#26550;&#26500;&#35774;&#35745;&#26041;&#38754;&#36827;&#34892;&#20102;&#37325;&#35201;&#30340;&#30740;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#29983;&#25104;&#24335;&#26550;&#26500;&#25628;&#32034;&#65288;GAS&#65289;&#25506;&#32034;&#20102;&#24555;&#36895;&#35270;&#35273;&#21464;&#21387;&#22120;&#26550;&#26500;&#35774;&#35745;&#30340;&#29983;&#25104;&#65292;&#20197;&#23454;&#29616;&#22312;&#20934;&#30830;&#24615;&#21644;&#26550;&#26500;&#35745;&#31639;&#25928;&#29575;&#20043;&#38388;&#30340;&#33391;&#22909;&#24179;&#34913;&#12290;&#36890;&#36807;&#36825;&#20010;&#29983;&#25104;&#24335;&#26550;&#26500;&#25628;&#32034;&#36807;&#31243;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102; TurboViT&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#25513;&#30721;&#21333;&#20803;&#27880;&#24847;&#21147;&#21644; Q-pooling &#35774;&#35745;&#27169;&#24335;&#30340;&#39640;&#25928;&#20998;&#23618;&#35270;&#35273;&#21464;&#21387;&#22120;&#26550;&#26500;&#35774;&#35745;&#12290;&#35813;&#32467;&#26524;&#34920;&#26126;&#65292;TurboViT &#26550;&#26500;&#35774;&#35745;&#30340;&#26550;&#26500;&#35745;&#31639;&#22797;&#26434;&#24615;&#26174;&#33879;&#38477;&#20302;&#65288;&gt;2.47&#65289;
&lt;/p&gt;
&lt;p&gt;
Vision transformers have shown unprecedented levels of performance in tackling various visual perception tasks in recent years. However, the architectural and computational complexity of such network architectures have made them challenging to deploy in real-world applications with high-throughput, low-memory requirements. As such, there has been significant research recently on the design of efficient vision transformer architectures. In this study, we explore the generation of fast vision transformer architecture designs via generative architecture search (GAS) to achieve a strong balance between accuracy and architectural and computational efficiency. Through this generative architecture search process, we create TurboViT, a highly efficient hierarchical vision transformer architecture design that is generated around mask unit attention and Q-pooling design patterns. The resulting TurboViT architecture design achieves significantly lower architectural computational complexity (&gt;2.47
&lt;/p&gt;</description></item><item><title>&#26412;&#20070;&#31995;&#32479;&#30740;&#31350;&#20102;&#22522;&#20110;&#24352;&#37327;&#30340;&#22238;&#24402;&#27169;&#22411;&#21450;&#20854;&#24212;&#29992;&#65292;&#24182;&#35206;&#30422;&#20102;&#22522;&#26412;&#30693;&#35782;&#12289;&#26680;&#24515;&#24605;&#24819;&#21644;&#29702;&#35770;&#29305;&#24615;&#12290;&#35835;&#32773;&#21487;&#20197;&#23398;&#20064;&#22914;&#20309;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#35299;&#20915;&#22810;&#36335;&#24452;&#25968;&#25454;&#22238;&#24402;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.11419</link><description>&lt;p&gt;
&#24352;&#37327;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Tensor Regression. (arXiv:2308.11419v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#20070;&#31995;&#32479;&#30740;&#31350;&#20102;&#22522;&#20110;&#24352;&#37327;&#30340;&#22238;&#24402;&#27169;&#22411;&#21450;&#20854;&#24212;&#29992;&#65292;&#24182;&#35206;&#30422;&#20102;&#22522;&#26412;&#30693;&#35782;&#12289;&#26680;&#24515;&#24605;&#24819;&#21644;&#29702;&#35770;&#29305;&#24615;&#12290;&#35835;&#32773;&#21487;&#20197;&#23398;&#20064;&#22914;&#20309;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#35299;&#20915;&#22810;&#36335;&#24452;&#25968;&#25454;&#22238;&#24402;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22238;&#24402;&#20998;&#26512;&#26159;&#25968;&#25454;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#37325;&#35201;&#30740;&#31350;&#26041;&#21521;&#65292;&#26088;&#22312;&#25506;&#32034;&#21464;&#37327;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#36890;&#24120;&#20351;&#29992;&#21521;&#37327;&#34920;&#31034;&#12290;&#39640;&#32500;&#25968;&#25454;&#22312;&#31070;&#32463;&#24433;&#20687;&#23398;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#27668;&#20505;&#23398;&#21644;&#31038;&#20132;&#32593;&#32476;&#31561;&#25216;&#26415;&#20013;&#30340;&#20986;&#29616;&#32473;&#20256;&#32479;&#25968;&#25454;&#34920;&#31034;&#26041;&#27861;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#20316;&#20026;&#21521;&#37327;&#30340;&#39640;&#32500;&#25193;&#23637;&#65292;&#24352;&#37327;&#34987;&#35270;&#20026;&#39640;&#32500;&#25968;&#25454;&#30340;&#33258;&#28982;&#34920;&#31034;&#12290;&#26412;&#20070;&#31995;&#32479;&#22320;&#30740;&#31350;&#21644;&#20998;&#26512;&#20102;&#22522;&#20110;&#24352;&#37327;&#30340;&#22238;&#24402;&#27169;&#22411;&#21450;&#20854;&#22312;&#36817;&#24180;&#26469;&#30340;&#24212;&#29992;&#12290;&#23427;&#23545;&#29616;&#26377;&#30340;&#22522;&#20110;&#24352;&#37327;&#30340;&#22238;&#24402;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#32452;&#21644;&#35828;&#26126;&#65292;&#24182;&#28085;&#30422;&#20102;&#22823;&#22810;&#25968;&#22522;&#20110;&#24352;&#37327;&#30340;&#22238;&#24402;&#26041;&#27861;&#30340;&#22522;&#26412;&#30693;&#35782;&#12289;&#26680;&#24515;&#24605;&#24819;&#21644;&#29702;&#35770;&#29305;&#24615;&#12290;&#27492;&#22806;&#65292;&#35835;&#32773;&#36824;&#21487;&#20197;&#23398;&#20064;&#22914;&#20309;&#20351;&#29992;&#29616;&#26377;&#30340;&#22522;&#20110;&#24352;&#37327;&#30340;&#22238;&#24402;&#26041;&#27861;&#35299;&#20915;&#20855;&#20307;&#30340;&#22810;&#36335;&#24452;&#25968;&#25454;&#22238;&#24402;&#20219;&#21153;&#65292;&#24212;&#36873;&#25321;&#21738;&#20123;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#20351;&#29992;&#21738;&#20123;&#36719;&#20214;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Regression analysis is a key area of interest in the field of data analysis and machine learning which is devoted to exploring the dependencies between variables, often using vectors. The emergence of high dimensional data in technologies such as neuroimaging, computer vision, climatology and social networks, has brought challenges to traditional data representation methods. Tensors, as high dimensional extensions of vectors, are considered as natural representations of high dimensional data. In this book, the authors provide a systematic study and analysis of tensor-based regression models and their applications in recent years. It groups and illustrates the existing tensor-based regression methods and covers the basics, core ideas, and theoretical characteristics of most tensor-based regression methods. In addition, readers can learn how to use existing tensor-based regression methods to solve specific regression tasks with multiway data, what datasets can be selected, and what softw
&lt;/p&gt;</description></item><item><title>&#23545;&#20110;&#36830;&#32493;&#35780;&#20998;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Wasserstein&#36317;&#31163;&#30340;&#20998;&#24067;&#19981;&#21464;&#20844;&#24179;&#24615;&#24230;&#37327;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#37322;&#24230;&#37327;&#32467;&#26524;&#24182;&#36866;&#29992;&#20110;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#12289;&#25968;&#25454;&#38598;&#25110;&#26102;&#38388;&#28857;&#20043;&#38388;&#30340;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2308.11375</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20998;&#24067;&#19981;&#21464;&#20844;&#24179;&#24615;&#24230;&#37327;&#26041;&#27861;&#23545;&#20110;&#36830;&#32493;&#35780;&#20998;
&lt;/p&gt;
&lt;p&gt;
Interpretable Distribution-Invariant Fairness Measures for Continuous Scores. (arXiv:2308.11375v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11375
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#36830;&#32493;&#35780;&#20998;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Wasserstein&#36317;&#31163;&#30340;&#20998;&#24067;&#19981;&#21464;&#20844;&#24179;&#24615;&#24230;&#37327;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#37322;&#24230;&#37327;&#32467;&#26524;&#24182;&#36866;&#29992;&#20110;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#12289;&#25968;&#25454;&#38598;&#25110;&#26102;&#38388;&#28857;&#20043;&#38388;&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#20844;&#24179;&#24615;&#24230;&#37327;&#36890;&#24120;&#22312;&#20108;&#20803;&#20915;&#31574;&#30340;&#32972;&#26223;&#19979;&#36827;&#34892;&#35752;&#35770;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#25193;&#23637;&#21040;&#36830;&#32493;&#35780;&#20998;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#22522;&#20110;ROC&#30340;&#24230;&#37327;&#26041;&#27861;&#20027;&#35201;&#29992;&#20110;&#27492;&#30446;&#30340;&#12290;&#20854;&#20182;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#35780;&#20998;&#30340;&#20998;&#24067;&#65292;&#19981;&#36866;&#29992;&#20110;&#25490;&#21517;&#20219;&#21153;&#65292;&#25110;&#32773;&#23427;&#20204;&#30340;&#25928;&#26524;&#22823;&#23567;&#19981;&#21487;&#35299;&#37322;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Wasserstein&#36317;&#31163;&#30340;&#36830;&#32493;&#35780;&#20998;&#30340;&#20998;&#24067;&#19981;&#21464;&#20844;&#24179;&#24615;&#24230;&#37327;&#26041;&#27861;&#65292;&#20855;&#26377;&#21512;&#29702;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#24230;&#37327;&#26041;&#27861;&#26131;&#20110;&#35745;&#31639;&#65292;&#24182;&#36866;&#29992;&#20110;&#37327;&#21270;&#21644;&#35299;&#37322;&#32676;&#20307;&#24046;&#24322;&#30340;&#24378;&#24230;&#65292;&#20197;&#21450;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#12289;&#25968;&#25454;&#38598;&#25110;&#26102;&#38388;&#28857;&#20043;&#38388;&#30340;&#20559;&#24046;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#29616;&#26377;&#35780;&#20998;&#20844;&#24179;&#24615;&#24230;&#37327;&#26041;&#27861;&#30340;&#19981;&#21516;&#26063;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#20998;&#24067;&#19981;&#21464;&#20844;&#24179;&#24615;&#24230;&#37327;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#22240;&#20026;&#23427;&#20204;&#26356;&#26126;&#30830;&#65292;&#24182;&#19988;&#21487;&#20197;&#37327;&#21270;&#26174;&#33879;&#30340;&#20559;&#24046;&#65292;&#32780;ROC-based&#19981;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Measures of algorithmic fairness are usually discussed in the context of binary decisions. We extend the approach to continuous scores. So far, ROC-based measures have mainly been suggested for this purpose. Other existing methods depend heavily on the distribution of scores, are unsuitable for ranking tasks, or their effect sizes are not interpretable. Here, we propose a distributionally invariant version of fairness measures for continuous scores with a reasonable interpretation based on the Wasserstein distance. Our measures are easily computable and well suited for quantifying and interpreting the strength of group disparities as well as for comparing biases across different models, datasets, or time points. We derive a link between the different families of existing fairness measures for scores and show that the proposed distributionally invariant fairness measures outperform ROC-based fairness measures because they are more explicit and can quantify significant biases that ROC-ba
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#31232;&#30095;&#27880;&#24847;&#21147;&#25429;&#25417;&#35270;&#39057;&#30340;&#23436;&#25972;&#19978;&#19979;&#25991;&#65292;&#20197;&#22238;&#31572;&#26102;&#38388;&#34892;&#21160;&#20998;&#21106;&#38656;&#35201;&#22810;&#23569;&#38271;&#26399;&#26102;&#38388;&#19978;&#19979;&#25991;&#12290;&#36890;&#36807;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#22312;&#19977;&#20010;&#26102;&#38388;&#34892;&#21160;&#20998;&#21106;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11358</link><description>&lt;p&gt;
&#34892;&#21160;&#20998;&#21106;&#38656;&#35201;&#22810;&#23569;&#38271;&#26399;&#26102;&#38388;&#19978;&#19979;&#25991;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Much Temporal Long-Term Context is Needed for Action Segmentation?. (arXiv:2308.11358v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#31232;&#30095;&#27880;&#24847;&#21147;&#25429;&#25417;&#35270;&#39057;&#30340;&#23436;&#25972;&#19978;&#19979;&#25991;&#65292;&#20197;&#22238;&#31572;&#26102;&#38388;&#34892;&#21160;&#20998;&#21106;&#38656;&#35201;&#22810;&#23569;&#38271;&#26399;&#26102;&#38388;&#19978;&#19979;&#25991;&#12290;&#36890;&#36807;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#22312;&#19977;&#20010;&#26102;&#38388;&#34892;&#21160;&#20998;&#21106;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#39057;&#20013;&#24314;&#27169;&#38271;&#26399;&#19978;&#19979;&#25991;&#23545;&#20110;&#35768;&#22810;&#32454;&#31890;&#24230;&#20219;&#21153;&#21253;&#25324;&#26102;&#38388;&#34892;&#21160;&#20998;&#21106;&#33267;&#20851;&#37325;&#35201;&#12290;&#19968;&#20010;&#26377;&#36259;&#30340;&#38382;&#39064;&#26159;&#65292;&#20026;&#20102;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#65292;&#38656;&#35201;&#22810;&#23569;&#38271;&#26399;&#26102;&#38388;&#19978;&#19979;&#25991;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20043;&#35868;&#12290;&#34429;&#28982;transformers&#21487;&#20197;&#23545;&#35270;&#39057;&#30340;&#38271;&#26399;&#19978;&#19979;&#25991;&#36827;&#34892;&#24314;&#27169;&#65292;&#20294;&#23545;&#20110;&#38271;&#35270;&#39057;&#65292;&#36825;&#22312;&#35745;&#31639;&#19978;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#20851;&#20110;&#26102;&#38388;&#34892;&#21160;&#20998;&#21106;&#30340;&#30740;&#31350;&#32467;&#21512;&#20102;&#20351;&#29992;&#23616;&#37096;&#26102;&#38388;&#31383;&#21475;&#35745;&#31639;&#20986;&#30340;&#33258;&#27880;&#24847;&#21147;&#30340;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#26174;&#31034;&#20986;&#33391;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#30340;&#24615;&#33021;&#21463;&#21040;&#26080;&#27861;&#25429;&#25417;&#35270;&#39057;&#30340;&#23436;&#25972;&#19978;&#19979;&#25991;&#30340;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#24182;&#21033;&#29992;&#31232;&#30095;&#27880;&#24847;&#21147;&#26469;&#25429;&#25417;&#35270;&#39057;&#30340;&#23436;&#25972;&#19978;&#19979;&#25991;&#65292;&#35797;&#22270;&#22238;&#31572;&#38656;&#35201;&#22810;&#23569;&#38271;&#26399;&#26102;&#38388;&#19978;&#19979;&#25991;&#25165;&#33021;&#36827;&#34892;&#26102;&#38388;&#34892;&#21160;&#20998;&#21106;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;&#30446;&#21069;&#30340;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#26102;&#38388;&#34892;&#21160;&#20998;&#21106;&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#36827;&#34892;&#27604;&#36739;&#65292;&#36825;&#19977;&#20010;&#25968;&#25454;&#38598;&#21253;&#25324;50Salads&#65292;Brea...
&lt;/p&gt;
&lt;p&gt;
Modeling long-term context in videos is crucial for many fine-grained tasks including temporal action segmentation. An interesting question that is still open is how much long-term temporal context is needed for optimal performance. While transformers can model the long-term context of a video, this becomes computationally prohibitive for long videos. Recent works on temporal action segmentation thus combine temporal convolutional networks with self-attentions that are computed only for a local temporal window. While these approaches show good results, their performance is limited by their inability to capture the full context of a video. In this work, we try to answer how much long-term temporal context is required for temporal action segmentation by introducing a transformer-based model that leverages sparse attention to capture the full context of a video. We compare our model with the current state of the art on three datasets for temporal action segmentation, namely 50Salads, Brea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#35821;&#20041;RGB-D&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;RGB-D&#35821;&#20041;&#22270;&#20687;&#20998;&#21106;&#35757;&#32451;&#38598;&#32570;&#20047;&#22810;&#26679;&#24615;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#29983;&#25104;&#36924;&#30495;&#30340;RGB-D&#22270;&#20687;&#26469;&#23454;&#29616;&#32473;&#23450;&#35821;&#20041;&#26631;&#31614;&#22270;&#30340;&#21512;&#25104;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#21019;&#26032;&#26159;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#22120;&#65292;&#21487;&#20197;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#23558;&#19982;&#27169;&#24577;&#26080;&#20851;&#30340;&#20449;&#24687;&#19982;&#19982;&#27169;&#24577;&#30456;&#20851;&#30340;&#20449;&#24687;&#20998;&#31163;&#24320;&#26469;&#12290;</title><link>http://arxiv.org/abs/2308.11356</link><description>&lt;p&gt;
&#35821;&#20041;RGB-D&#22270;&#20687;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Semantic RGB-D Image Synthesis. (arXiv:2308.11356v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#35821;&#20041;RGB-D&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;RGB-D&#35821;&#20041;&#22270;&#20687;&#20998;&#21106;&#35757;&#32451;&#38598;&#32570;&#20047;&#22810;&#26679;&#24615;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#29983;&#25104;&#36924;&#30495;&#30340;RGB-D&#22270;&#20687;&#26469;&#23454;&#29616;&#32473;&#23450;&#35821;&#20041;&#26631;&#31614;&#22270;&#30340;&#21512;&#25104;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#21019;&#26032;&#26159;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#22120;&#65292;&#21487;&#20197;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#23558;&#19982;&#27169;&#24577;&#26080;&#20851;&#30340;&#20449;&#24687;&#19982;&#19982;&#27169;&#24577;&#30456;&#20851;&#30340;&#20449;&#24687;&#20998;&#31163;&#24320;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;RGB-D&#35821;&#20041;&#22270;&#20687;&#20998;&#21106;&#30340;&#35757;&#32451;&#38598;&#20013;&#25910;&#38598;&#21508;&#31181;&#21508;&#26679;&#30340;&#22270;&#20687;&#24182;&#19981;&#24635;&#26159;&#21487;&#33021;&#30340;&#12290;&#23588;&#20854;&#26159;&#24403;&#26426;&#22120;&#20154;&#38656;&#35201;&#22312;&#38544;&#31169;&#25935;&#24863;&#21306;&#22495;&#22914;&#23478;&#24237;&#20013;&#25805;&#20316;&#26102;&#65292;&#25910;&#38598;&#21463;&#38480;&#20110;&#19968;&#23567;&#37096;&#20998;&#22320;&#28857;&#12290;&#22240;&#27492;&#65292;&#26631;&#27880;&#22270;&#20687;&#22312;&#22806;&#35266;&#19978;&#32570;&#20047;&#22810;&#26679;&#24615;&#65292;RGB-D&#35821;&#20041;&#22270;&#20687;&#20998;&#21106;&#30340;&#26041;&#27861;&#24448;&#24448;&#23545;&#35757;&#32451;&#25968;&#25454;&#36807;&#24230;&#25311;&#21512;&#12290;&#22240;&#27492;&#26412;&#25991;&#25552;&#20986;&#20102;&#35821;&#20041;RGB-D&#22270;&#20687;&#21512;&#25104;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#38656;&#35201;&#20026;&#32473;&#23450;&#30340;&#35821;&#20041;&#26631;&#31614;&#22270;&#21512;&#25104;&#19968;&#20010;&#36924;&#30495;&#30340;RGB-D&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#26159;&#21333;&#27169;&#24577;&#30340;&#65292;&#19981;&#33021;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#12290;&#20107;&#23454;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#21333;&#27169;&#24577;&#26041;&#27861;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#25968;&#25454;&#26102;&#25928;&#26524;&#19981;&#20339;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#29983;&#25104;&#22120;&#65292;&#23558;&#35821;&#20041;&#24067;&#23616;&#20013;&#19982;&#27169;&#24577;&#26080;&#20851;&#30340;&#20449;&#24687;&#19982;&#29983;&#25104;RGB&#21644;&#28145;&#24230;&#22270;&#20687;&#25152;&#38656;&#30340;&#19982;&#27169;&#24577;&#30456;&#20851;&#30340;&#20449;&#24687;&#20998;&#31163;&#24320;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collecting diverse sets of training images for RGB-D semantic image segmentation is not always possible. In particular, when robots need to operate in privacy-sensitive areas like homes, the collection is often limited to a small set of locations. As a consequence, the annotated images lack diversity in appearance and approaches for RGB-D semantic image segmentation tend to overfit the training data. In this paper, we thus introduce semantic RGB-D image synthesis to address this problem. It requires synthesising a realistic-looking RGB-D image for a given semantic label map. Current approaches, however, are uni-modal and cannot cope with multi-modal data. Indeed, we show that extending uni-modal approaches to multi-modal data does not perform well. In this paper, we therefore propose a generator for multi-modal data that separates modal-independent information of the semantic layout from the modal-dependent information that is needed to generate an RGB and a depth image, respectively. 
&lt;/p&gt;</description></item><item><title>ProAgent&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#30340;&#20027;&#21160;&#21512;&#20316;&#30340;AI&#26694;&#26550;&#65292;&#33021;&#22815;&#39044;&#27979;&#38431;&#21451;&#30340;&#20915;&#31574;&#24182;&#20026;&#33258;&#24049;&#21046;&#23450;&#22686;&#24378;&#35745;&#21010;&#65292;&#20855;&#26377;&#39640;&#24230;&#30340;&#27169;&#22359;&#21270;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.11339</link><description>&lt;p&gt;
ProAgent&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#20027;&#21160;&#21512;&#20316;&#30340;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
ProAgent: Building Proactive Cooperative AI with Large Language Models. (arXiv:2308.11339v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11339
&lt;/p&gt;
&lt;p&gt;
ProAgent&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#30340;&#20027;&#21160;&#21512;&#20316;&#30340;AI&#26694;&#26550;&#65292;&#33021;&#22815;&#39044;&#27979;&#38431;&#21451;&#30340;&#20915;&#31574;&#24182;&#20026;&#33258;&#24049;&#21046;&#23450;&#22686;&#24378;&#35745;&#21010;&#65292;&#20855;&#26377;&#39640;&#24230;&#30340;&#27169;&#22359;&#21270;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;AGI&#30740;&#31350;&#20013;&#65292;&#26500;&#24314;&#20855;&#26377;&#33258;&#36866;&#24212;&#34892;&#20026;&#30340;&#20154;&#24037;&#26234;&#33021;&#20197;&#36827;&#34892;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#30340;&#21512;&#20316;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#20851;&#27880;&#28857;&#12290;&#30446;&#21069;&#65292;&#24320;&#21457;&#21512;&#20316;&#20195;&#29702;&#20154;&#30340;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#25919;&#31574;&#27867;&#21270;&#20005;&#37325;&#20381;&#36182;&#20110;&#19982;&#29305;&#23450;&#38431;&#21451;&#30340;&#36807;&#21435;&#20114;&#21160;&#12290;&#36825;&#20123;&#26041;&#27861;&#38480;&#21046;&#20102;&#20195;&#29702;&#20154;&#22312;&#38754;&#23545;&#26032;&#30340;&#38431;&#21451;&#26102;&#37325;&#26032;&#26657;&#20934;&#31574;&#30053;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ProAgent&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#21019;&#24314;&#19968;&#20010;&#20855;&#26377;&#39044;&#27979;&#38431;&#21451;&#26410;&#26469;&#20915;&#31574;&#33021;&#21147;&#21644;&#20026;&#33258;&#36523;&#21046;&#23450;&#22686;&#24378;&#35745;&#21010;&#33021;&#21147;&#30340;&#20027;&#21160;&#20195;&#29702;&#12290;ProAgent&#22312;&#21512;&#20316;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#33021;&#22815;&#21160;&#24577;&#35843;&#25972;&#34892;&#20026;&#20197;&#22686;&#24378;&#19982;&#38431;&#21451;&#30340;&#21327;&#20316;&#21162;&#21147;&#12290;&#27492;&#22806;&#65292;ProAgent&#26694;&#26550;&#20855;&#26377;&#39640;&#24230;&#30340;&#27169;&#22359;&#21270;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#20415;&#20110;&#26080;&#32541;&#38598;&#25104;&#65292;&#20197;&#24212;&#23545;&#21508;&#31181;&#21327;&#35843;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building AIs with adaptive behaviors in human-AI cooperation stands as a pivotal focus in AGI research. Current methods for developing cooperative agents predominantly rely on learning-based methods, where policy generalization heavily hinges on past interactions with specific teammates. These approaches constrain the agent's capacity to recalibrate its strategy when confronted with novel teammates. We propose \textbf{ProAgent}, a novel framework that harnesses large language models (LLMs) to fashion a \textit{pro}active \textit{agent} empowered with the ability to anticipate teammates' forthcoming decisions and formulate enhanced plans for itself. ProAgent excels at cooperative reasoning with the capacity to dynamically adapt its behavior to enhance collaborative efforts with teammates. Moreover, the ProAgent framework exhibits a high degree of modularity and interpretability, facilitating seamless integration to address a wide array of coordination scenarios. Experimental evaluations
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32858;&#28966;&#20110;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#25512;&#33616;&#31995;&#32479;&#30340;&#25968;&#25454;&#25928;&#29575;&#38382;&#39064;&#65292;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20026;&#20174;&#32447;&#19979;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#24182;&#22312;&#22312;&#32447;&#29615;&#22659;&#24212;&#29992;&#31574;&#30053;&#25552;&#20379;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2308.11336</link><description>&lt;p&gt;
&#20851;&#20110;&#32447;&#19979;&#24378;&#21270;&#23398;&#20064;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
On the Opportunities and Challenges of Offline Reinforcement Learning for Recommender Systems. (arXiv:2308.11336v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11336
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32858;&#28966;&#20110;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#25512;&#33616;&#31995;&#32479;&#30340;&#25968;&#25454;&#25928;&#29575;&#38382;&#39064;&#65292;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20026;&#20174;&#32447;&#19979;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#24182;&#22312;&#22312;&#32447;&#29615;&#22659;&#24212;&#29992;&#31574;&#30053;&#25552;&#20379;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#24314;&#27169;&#21160;&#24577;&#29992;&#25143;&#20852;&#36259;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#36817;&#24180;&#26469;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#24471;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23427;&#23384;&#22312;&#19968;&#20010;&#37325;&#35201;&#32570;&#28857;&#65306;&#22240;&#20026;&#20854;&#20132;&#20114;&#24615;&#65292;&#20854;&#25968;&#25454;&#25928;&#29575;&#36739;&#20302;&#12290;&#24378;&#21270;&#23398;&#20064;&#25512;&#33616;&#31995;&#32479;&#30340;&#35757;&#32451;&#38656;&#35201;&#26114;&#36149;&#30340;&#22312;&#32447;&#20132;&#20114;&#26469;&#31215;&#32047;&#36275;&#22815;&#30340;&#36712;&#36857;&#65292;&#36825;&#23545;&#20110;&#20195;&#29702;&#33021;&#22815;&#23398;&#20064;&#29992;&#25143;&#20559;&#22909;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#31181;&#20302;&#25928;&#24615;&#20351;&#24471;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#25104;&#20026;&#19968;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#25506;&#32034;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36817;&#26399;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#36827;&#23637;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#12290;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#20174;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#35265;&#35299;&#65292;&#24182;&#22312;&#22312;&#32447;&#29615;&#22659;&#20013;&#37096;&#32626;&#23398;&#20064;&#21040;&#30340;&#31574;&#30053;&#12290;&#37492;&#20110;&#25512;&#33616;&#31995;&#32479;&#25317;&#26377;&#24191;&#27867;&#30340;&#32447;&#19979;&#25968;&#25454;&#38598;&#65292;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#19982;&#20043;&#32039;&#23494;&#30456;&#31526;&#12290;&#23613;&#31649;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#20010;&#26032;&#20852;&#39046;&#22495;&#65292;&#20294;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#38754;&#30340;&#30740;&#31350;&#25104;&#26524;&#36880;&#28176;&#22686;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning serves as a potent tool for modeling dynamic user interests within recommender systems, garnering increasing research attention of late. However, a significant drawback persists: its poor data efficiency, stemming from its interactive nature. The training of reinforcement learning-based recommender systems demands expensive online interactions to amass adequate trajectories, essential for agents to learn user preferences. This inefficiency renders reinforcement learning-based recommender systems a formidable undertaking, necessitating the exploration of potential solutions. Recent strides in offline reinforcement learning present a new perspective. Offline reinforcement learning empowers agents to glean insights from offline datasets and deploy learned policies in online settings. Given that recommender systems possess extensive offline datasets, the framework of offline reinforcement learning aligns seamlessly. Despite being a burgeoning field, works centered on
&lt;/p&gt;</description></item><item><title>GrowCLIP&#26159;&#19968;&#31181;&#25968;&#25454;&#24863;&#30693;&#30340;&#33258;&#21160;&#27169;&#22411;&#22686;&#38271;&#31639;&#27861;&#65292;&#29992;&#20110;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#12290;&#23427;&#33021;&#22815;&#26681;&#25454;&#25345;&#32493;&#22686;&#38271;&#30340;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#25214;&#21040;&#26368;&#20339;&#26550;&#26500;&#65292;&#23454;&#29616;&#39640;&#25928;&#35757;&#32451;&#21644;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11331</link><description>&lt;p&gt;
GrowCLIP: &#25968;&#25454;&#24863;&#30693;&#30340;&#33258;&#21160;&#27169;&#22411;&#22686;&#38271;&#26041;&#27861;&#29992;&#20110;&#22823;&#35268;&#27169;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
GrowCLIP: Data-aware Automatic Model Growing for Large-scale Contrastive Language-Image Pre-training. (arXiv:2308.11331v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11331
&lt;/p&gt;
&lt;p&gt;
GrowCLIP&#26159;&#19968;&#31181;&#25968;&#25454;&#24863;&#30693;&#30340;&#33258;&#21160;&#27169;&#22411;&#22686;&#38271;&#31639;&#27861;&#65292;&#29992;&#20110;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#12290;&#23427;&#33021;&#22815;&#26681;&#25454;&#25345;&#32493;&#22686;&#38271;&#30340;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#25214;&#21040;&#26368;&#20339;&#26550;&#26500;&#65292;&#23454;&#29616;&#39640;&#25928;&#35757;&#32451;&#21644;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#27169;&#24577;&#39044;&#35757;&#32451;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#20174;&#20114;&#32852;&#32593;&#25910;&#38598;&#30340;&#22823;&#37327;&#22270;&#20687;-&#25991;&#26412;&#23545;&#21463;&#30410;&#21290;&#27973;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#22312;&#32447;&#25968;&#25454;&#19981;&#26029;&#22686;&#38271;&#65292;&#20984;&#26174;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#33021;&#22815;&#20174;&#19981;&#26029;&#22686;&#38271;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#12290;&#29616;&#26377;&#30340;&#36328;&#27169;&#24577;&#39044;&#35757;&#32451;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#35757;&#32451;&#20855;&#26377;&#22266;&#23450;&#26550;&#26500;&#30340;&#32593;&#32476;&#19978;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;&#23454;&#38469;&#24212;&#29992;&#20013;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#19981;&#26029;&#22686;&#38271;&#30340;&#29305;&#24615;&#65292;&#38480;&#21046;&#27169;&#22411;&#23481;&#37327;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#21033;&#29992;&#24403;&#21069;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#26469;&#33719;&#24471;&#39640;&#25928;&#30340;&#35757;&#32451;&#21644;&#26356;&#22909;&#30340;&#24615;&#33021;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GrowCLIP&#65292;&#36825;&#26159;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#33258;&#21160;&#27169;&#22411;&#22686;&#38271;&#31639;&#27861;&#65292;&#29992;&#20110;&#22522;&#20110;&#36830;&#32493;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#21160;&#24577;&#22686;&#38271;&#31354;&#38388;&#65292;&#24182;&#23547;&#25214;&#26368;&#20339;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-modal pre-training has shown impressive performance on a wide range of downstream tasks, benefiting from massive image-text pairs collected from the Internet. In practice, online data are growing constantly, highlighting the importance of the ability of pre-trained model to learn from data that is continuously growing. Existing works on cross-modal pre-training mainly focus on training a network with fixed architecture. However, it is impractical to limit the model capacity when considering the continuously growing nature of pre-training data in real-world applications. On the other hand, it is important to utilize the knowledge in the current model to obtain efficient training and better performance. To address the above issues, in this paper, we propose GrowCLIP, a data-driven automatic model growing algorithm for contrastive language-image pre-training with continuous image-text pairs as input. Specially, we adopt a dynamic growth space and seek out the optimal architecture at
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;ChatGPT&#23545;&#32534;&#30721;&#38382;&#39064;&#35299;&#20915;&#26041;&#24335;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#23427;&#26174;&#33879;&#20943;&#23569;&#20102;&#38382;&#39064;&#25968;&#37327;&#65292;&#25552;&#39640;&#20102;&#38382;&#39064;&#25991;&#26723;&#36136;&#37327;&#65292;&#20197;&#21450;&#21097;&#19979;&#30340;&#38382;&#39064;&#26356;&#21152;&#22797;&#26434;&#65292;&#26263;&#31034;&#30528;AI&#19981;&#20165;&#25552;&#21319;&#20102;&#29983;&#20135;&#21147;&#65292;&#36824;&#25913;&#21464;&#20102;&#24037;&#20316;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2308.11302</link><description>&lt;p&gt;
&#20174;&#26085;&#24120;&#21040;&#26377;&#24847;&#20041;&#65306;AI&#23545;&#24037;&#20316;&#21160;&#24577;&#30340;&#24433;&#21709; - &#26469;&#33258;ChatGPT&#21644;Stack Overflow&#30340;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
From Mundane to Meaningful: AI's Influence on Work Dynamics -- evidence from ChatGPT and Stack Overflow. (arXiv:2308.11302v1 [econ.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;ChatGPT&#23545;&#32534;&#30721;&#38382;&#39064;&#35299;&#20915;&#26041;&#24335;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#23427;&#26174;&#33879;&#20943;&#23569;&#20102;&#38382;&#39064;&#25968;&#37327;&#65292;&#25552;&#39640;&#20102;&#38382;&#39064;&#25991;&#26723;&#36136;&#37327;&#65292;&#20197;&#21450;&#21097;&#19979;&#30340;&#38382;&#39064;&#26356;&#21152;&#22797;&#26434;&#65292;&#26263;&#31034;&#30528;AI&#19981;&#20165;&#25552;&#21319;&#20102;&#29983;&#20135;&#21147;&#65292;&#36824;&#25913;&#21464;&#20102;&#24037;&#20316;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38416;&#36848;&#20102;&#29983;&#25104;&#24335;AI&#22914;&#20309;&#20026;&#24040;&#22823;&#30340;&#29983;&#20135;&#21147;&#25552;&#21319;&#25552;&#20379;&#26426;&#20250;&#65292;&#21516;&#26102;&#20063;&#24341;&#21457;&#20102;&#20851;&#20110;&#36825;&#20123;&#26032;&#24378;&#22823;&#25216;&#26415;&#23545;&#25105;&#20204;&#24037;&#20316;&#21644;&#20849;&#20139;&#30693;&#35782;&#26041;&#24335;&#30340;&#24433;&#21709;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;ChatGPT&#22914;&#20309;&#25913;&#21464;&#20102;&#32534;&#30721;&#30340;&#19968;&#20010;&#22522;&#26412;&#26041;&#38754;&#65306;&#38382;&#39064;&#35299;&#20915;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;ChatGPT&#22312;2022&#24180;11&#26376;30&#26085;&#31361;&#28982;&#21457;&#24067;&#23545;&#26368;&#22823;&#30340;&#22312;&#32447;&#32534;&#30721;&#31038;&#21306;Stack Overflow&#20351;&#29992;&#30340;&#24433;&#21709;&#12290;&#20351;&#29992;&#20934;&#23454;&#39564;&#26041;&#27861;&#65288;&#24046;&#20998;&#20934;&#24046;&#27861;&#65289;&#65292;&#25105;&#20204;&#21457;&#29616;&#38382;&#39064;&#25968;&#37327;&#26174;&#33879;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#22312;ChatGPT&#21457;&#24067;&#21518;&#65292;&#38382;&#39064;&#30340;&#25991;&#26723;&#26356;&#21152;&#23436;&#21892;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#21097;&#19979;&#30340;&#38382;&#39064;&#26356;&#21152;&#22797;&#26434;&#12290;&#36825;&#20123;&#21457;&#29616;&#19981;&#20165;&#34920;&#26126;&#20102;&#29983;&#20135;&#21147;&#30340;&#25552;&#21319;&#65292;&#32780;&#19988;&#36824;&#34920;&#26126;&#20102;&#25105;&#20204;&#24037;&#20316;&#26041;&#24335;&#30340;&#26681;&#26412;&#21464;&#21270;&#65292;&#20854;&#20013;AI&#35299;&#20915;&#20102;&#24120;&#35268;&#26597;&#35810;&#65292;&#20351;&#20154;&#20204;&#33021;&#22815;&#19987;&#27880;&#20110;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper illustrates how generative AI could give opportunities for big productivity gains but also opens up questions about the impact of these new powerful technologies on the way we work and share knowledge. More specifically, we explore how ChatGPT changed a fundamental aspect of coding: problem-solving. To do so, we exploit the effect of the sudden release of ChatGPT on the 30th of November 2022 on the usage of the largest online community for coders: Stack Overflow. Using quasi-experimental methods (Difference-in-Difference), we find a significant drop in the number of questions. In addition, the questions are better documented after the release of ChatGPT. Finally, we find evidence that the remaining questions are more complex. These findings suggest not only productivity gains but also a fundamental change in the way we work where routine inquiries are solved by AI allowing humans to focus on more complex tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26408;&#26448;&#22806;&#24418;&#39044;&#27979;&#20869;&#37096;&#32570;&#38519;&#20301;&#32622;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#21367;&#31215;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#20108;&#20998;&#31867;&#20998;&#21106;&#20219;&#21153;&#65292;&#23454;&#29616;&#22312;&#24265;&#20215;&#35774;&#22791;&#19978;&#36827;&#34892;&#25512;&#29702;&#65292;&#24182;&#22312;&#20919;&#26441;&#21644;&#20113;&#26441;&#26641;&#31181;&#19978;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.11291</link><description>&lt;p&gt;
&#25552;&#39640;&#38271;&#24452;&#21521;&#29305;&#24449;&#20256;&#25773;&#20013;&#30340;&#26408;&#26448;&#21407;&#32467;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Improving Knot Prediction in Wood Logs with Longitudinal Feature Propagation. (arXiv:2308.11291v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11291
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26408;&#26448;&#22806;&#24418;&#39044;&#27979;&#20869;&#37096;&#32570;&#38519;&#20301;&#32622;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#21367;&#31215;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#20108;&#20998;&#31867;&#20998;&#21106;&#20219;&#21153;&#65292;&#23454;&#29616;&#22312;&#24265;&#20215;&#35774;&#22791;&#19978;&#36827;&#34892;&#25512;&#29702;&#65292;&#24182;&#22312;&#20919;&#26441;&#21644;&#20113;&#26441;&#26641;&#31181;&#19978;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26408;&#26448;&#34892;&#19994;&#20013;&#26408;&#26448;&#21407;&#32467;&#30340;&#36136;&#37327;&#20027;&#35201;&#21462;&#20915;&#20110;&#20869;&#22806;&#32570;&#38519;&#30340;&#23384;&#22312;&#65292;&#20854;&#20013;&#20869;&#37096;&#33410;&#30116;&#26159;&#26641;&#26525;&#29983;&#38271;&#30340;&#32467;&#26524;&#12290;&#30446;&#21069;&#65292;&#23450;&#20301;&#20869;&#37096;&#33410;&#30116;&#38656;&#35201;&#20351;&#29992;&#26114;&#36149;&#30340;&#35774;&#22791;&#65292;&#22914;X&#23556;&#32447;&#25195;&#25551;&#20202;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#36890;&#36807;&#26408;&#26448;&#22806;&#24418;&#39044;&#27979;&#20869;&#37096;&#32570;&#38519;&#20301;&#32622;&#30340;&#20219;&#21153;&#12290;&#25968;&#25454;&#38598;&#36890;&#36807;&#21033;&#29992;X&#23556;&#32447;&#27979;&#37327;&#25552;&#21462;&#36718;&#24275;&#21644;&#33410;&#30116;&#26500;&#24314;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#21367;&#31215;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#36825;&#20010;&#20108;&#20998;&#31867;&#20998;&#21106;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#19968;&#26086;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#23436;&#27605;&#65292;&#21487;&#20197;&#20351;&#29992;&#24265;&#20215;&#35774;&#22791;&#65288;&#22914;&#28608;&#20809;&#21078;&#38754;&#20202;&#65289;&#27979;&#37327;&#30340;&#22806;&#24418;&#36827;&#34892;&#25512;&#29702;&#12290;&#25105;&#20204;&#22312;&#20919;&#26441;&#21644;&#20113;&#26441;&#26641;&#31181;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23545;&#24490;&#29615;&#30340;&#37325;&#35201;&#24615;&#36827;&#34892;&#20102;&#28040;&#34701;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quality of a wood log in the wood industry depends heavily on the presence of both outer and inner defects, including inner knots that are a result of the growth of tree branches. Today, locating the inner knots require the use of expensive equipment such as X-ray scanners. In this paper, we address the task of predicting the location of inner defects from the outer shape of the logs. The dataset is built by extracting both the contours and the knots with X-ray measurements. We propose to solve this binary segmentation task by leveraging convolutional recurrent neural networks. Once the neural network is trained, inference can be performed from the outer shape measured with cheap devices such as laser profilers. We demonstrate the effectiveness of our approach on fir and spruce tree species and perform ablation on the recurrence to demonstrate its importance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#37327;&#23376;&#31995;&#32479;&#23398;&#20064;&#33539;&#24335;&#65292;&#23558;&#31070;&#32463;&#32593;&#32476;&#21644;&#32463;&#20856;&#38452;&#24433;&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#22823;&#22411;&#37327;&#23376;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#39044;&#27979;&#21644;&#27867;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.11290</link><description>&lt;p&gt;
&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#37327;&#23376;&#31995;&#32479;&#23398;&#20064;&#30340;&#24433;&#23376;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
ShadowNet for Data-Centric Quantum System Learning. (arXiv:2308.11290v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#37327;&#23376;&#31995;&#32479;&#23398;&#20064;&#33539;&#24335;&#65292;&#23558;&#31070;&#32463;&#32593;&#32476;&#21644;&#32463;&#20856;&#38452;&#24433;&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#22823;&#22411;&#37327;&#23376;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#39044;&#27979;&#21644;&#27867;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#32500;&#24230;&#35781;&#21650;&#65292;&#29702;&#35299;&#22823;&#22411;&#37327;&#23376;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#21464;&#24471;&#22256;&#38590;&#12290;&#32479;&#35745;&#23398;&#20064;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21327;&#35758;&#21644;&#32463;&#20856;&#38452;&#24433;&#22312;&#36825;&#20010;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#28982;&#32780;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#23384;&#22312;&#23616;&#38480;&#24615;&#65306;&#21069;&#32773;&#21463;&#21040;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#22256;&#25200;&#65292;&#21518;&#32773;&#32570;&#20047;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#32467;&#21512;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#20197;&#20419;&#36827;&#22810;&#26679;&#21270;&#30340;&#37327;&#23376;&#31995;&#32479;&#23398;&#20064;&#20219;&#21153;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#33539;&#24335;&#21033;&#29992;&#20102;&#32463;&#20856;&#38452;&#24433;&#21644;&#20854;&#20182;&#26131;&#20110;&#33719;&#21462;&#30340;&#37327;&#23376;&#31995;&#32479;&#20449;&#24687;&#26469;&#21019;&#24314;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#28982;&#21518;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#26469;&#23398;&#20064;&#25506;&#32034;&#30340;&#37327;&#23376;&#31995;&#32479;&#23398;&#20064;&#38382;&#39064;&#30340;&#28508;&#22312;&#26144;&#23556;&#35268;&#24459;&#12290;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36825;&#20010;&#33539;&#24335;&#21487;&#20197;&#22312;&#31163;&#32447;&#35757;&#32451;&#65292;&#24182;&#19988;&#22312;&#25512;&#29702;&#38454;&#27573;&#33021;&#22815;&#20248;&#31168;&#22320;&#39044;&#27979;&#20043;&#21069;&#26410;&#35265;&#36807;&#30340;&#31995;&#32479;&#65292;&#21363;&#20351;&#21482;&#26377;&#24456;&#23569;&#30340;&#29366;&#24577;&#21103;&#26412;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#32487;&#25215;&#20102;
&lt;/p&gt;
&lt;p&gt;
Understanding the dynamics of large quantum systems is hindered by the curse of dimensionality. Statistical learning offers new possibilities in this regime by neural-network protocols and classical shadows, while both methods have limitations: the former is plagued by the predictive uncertainty and the latter lacks the generalization ability. Here we propose a data-centric learning paradigm combining the strength of these two approaches to facilitate diverse quantum system learning (QSL) tasks. Particularly, our paradigm utilizes classical shadows along with other easily obtainable information of quantum systems to create the training dataset, which is then learnt by neural networks to unveil the underlying mapping rule of the explored QSL problem. Capitalizing on the generalization power of neural networks, this paradigm can be trained offline and excel at predicting previously unseen systems at the inference stage, even with few state copies. Besides, it inherits the characteristic 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#22522;&#20110;CNN&#30340;&#26964;&#24418;&#31526;&#21495;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#27880;&#37322;&#30340;&#19977;&#32500;&#28210;&#26579;&#21644;&#26144;&#23556;&#29031;&#29255;&#65292;&#32467;&#21512;&#20809;&#29031;&#22686;&#24378;&#12290;&#30740;&#31350;&#22242;&#38431;&#21019;&#24314;&#20102;HeiCuBeDa&#21644;MaiCuBeDa&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20379;&#20102;&#26144;&#23556;&#24037;&#20855;&#20197;&#20256;&#36882;&#27880;&#37322;&#12290;&#31526;&#21495;&#23450;&#20301;&#26041;&#27861;&#20351;&#29992;RepPoints&#26816;&#27979;&#22120;&#26469;&#39044;&#27979;&#23383;&#31526;&#30340;&#20301;&#32622;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#22788;&#29702;&#26964;&#24418;&#25991;&#23383;&#30340;&#25968;&#23383;&#24037;&#20855;&#24320;&#21457;&#21644;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2308.11277</link><description>&lt;p&gt;
&#22522;&#20110;CNN&#30340;&#22522;&#20110;&#27880;&#37322;&#30340;&#19977;&#32500;&#28210;&#26579;&#21644;&#26144;&#23556;&#29031;&#29255;&#30340;&#26964;&#24418;&#31526;&#21495;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
CNN based Cuneiform Sign Detection Learned from Annotated 3D Renderings and Mapped Photographs with Illumination Augmentation. (arXiv:2308.11277v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11277
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#22522;&#20110;CNN&#30340;&#26964;&#24418;&#31526;&#21495;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#27880;&#37322;&#30340;&#19977;&#32500;&#28210;&#26579;&#21644;&#26144;&#23556;&#29031;&#29255;&#65292;&#32467;&#21512;&#20809;&#29031;&#22686;&#24378;&#12290;&#30740;&#31350;&#22242;&#38431;&#21019;&#24314;&#20102;HeiCuBeDa&#21644;MaiCuBeDa&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20379;&#20102;&#26144;&#23556;&#24037;&#20855;&#20197;&#20256;&#36882;&#27880;&#37322;&#12290;&#31526;&#21495;&#23450;&#20301;&#26041;&#27861;&#20351;&#29992;RepPoints&#26816;&#27979;&#22120;&#26469;&#39044;&#27979;&#23383;&#31526;&#30340;&#20301;&#32622;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#22788;&#29702;&#26964;&#24418;&#25991;&#23383;&#30340;&#25968;&#23383;&#24037;&#20855;&#24320;&#21457;&#21644;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;Digital Ancient Near Eastern Studies (DANES)&#31038;&#21306;&#38754;&#20020;&#30340;&#25361;&#25112;&#30340;&#25512;&#21160;&#19979;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#29992;&#20110;&#22788;&#29702;&#26964;&#24418;&#25991;&#23383;&#30340;&#25968;&#23383;&#24037;&#20855;&#65292;&#36825;&#26159;&#19968;&#31181;&#21360;&#22312;&#31896;&#22303;&#26495;&#19978;&#30340;&#19977;&#32500;&#33050;&#26412;&#65292;&#24050;&#26377;&#19977;&#21315;&#22810;&#24180;&#21382;&#21490;&#21644;&#33267;&#23569;&#20843;&#31181;&#20027;&#35201;&#35821;&#35328;&#12290;&#23427;&#30001;&#25968;&#21315;&#20010;&#38543;&#26102;&#38388;&#21644;&#31354;&#38388;&#21464;&#21270;&#30340;&#23383;&#31526;&#32452;&#25104;&#12290;&#29031;&#29255;&#26159;&#26368;&#24120;&#29992;&#30340;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#34920;&#31034;&#26041;&#24335;&#65292;&#32780;&#22696;&#27700;&#32472;&#30011;&#21017;&#23481;&#26131;&#34987;&#35299;&#37322;&#12290;&#25105;&#20204;&#21019;&#24314;&#24182;&#20351;&#29992;&#20102;HeiCuBeDa&#21644;MaiCuBeDa&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#21253;&#21547;&#32422;500&#20010;&#24102;&#26377;&#27880;&#37322;&#30340;&#24179;&#26495;&#12290;&#23545;&#20110;&#25105;&#20204;&#30340;&#26032;&#22411;&#31867;&#20284;OCR&#30340;&#28151;&#21512;&#22270;&#20687;&#25968;&#25454;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#39069;&#22806;&#30340;&#26144;&#23556;&#24037;&#20855;&#65292;&#29992;&#20110;&#22312;3D&#28210;&#26579;&#21644;&#29031;&#29255;&#20043;&#38388;&#20256;&#36882;&#27880;&#37322;&#12290;&#25105;&#20204;&#20351;&#29992;RepPoints&#26816;&#27979;&#22120;&#26469;&#39044;&#27979;&#23383;&#31526;&#30340;&#20301;&#32622;&#65292;&#20197;&#36793;&#30028;&#26694;&#30340;&#24418;&#24335;&#36827;&#34892;&#31526;&#21495;&#23450;&#20301;&#12290;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;GigaMesh&#30340;MSII&#65288;&#26354;&#29575;&#65289;&#22522;&#20110;&#28210;&#26579;&#30340;&#22270;&#20687;&#25968;&#25454;&#65292;&#20197;&#21450;Phong&#30528;&#33394;&#30340;3D&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the challenges of the Digital Ancient Near Eastern Studies (DANES) community, we develop digital tools for processing cuneiform script being a 3D script imprinted into clay tablets used for more than three millennia and at least eight major languages. It consists of thousands of characters that have changed over time and space. Photographs are the most common representations usable for machine learning, while ink drawings are prone to interpretation. Best suited 3D datasets that are becoming available. We created and used the HeiCuBeDa and MaiCuBeDa datasets, which consist of around 500 annotated tablets. For our novel OCR-like approach to mixed image data, we provide an additional mapping tool for transferring annotations between 3D renderings and photographs. Our sign localization uses a RepPoints detector to predict the locations of characters as bounding boxes. We use image data from GigaMesh's MSII (curvature, see https://gigamesh.eu) based rendering, Phong-shaded 3D 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#38899;&#20048;&#29702;&#35299;LLaMA&#65288;MU-LLaMA&#65289;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24212;&#29992;&#38382;&#31572;&#21644;&#23383;&#24149;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#25991;&#26412;&#21040;&#38899;&#20048;&#29983;&#25104;&#38754;&#20020;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;MusicQA&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;MU-LLaMA&#27169;&#22411;&#65292;&#24182;&#22312;&#38899;&#20048;&#38382;&#31572;&#26041;&#38754;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11276</link><description>&lt;p&gt;
&#38899;&#20048;&#29702;&#35299;LLaMA&#65306;&#24212;&#29992;&#38382;&#31572;&#21644;&#23383;&#24149;&#25512;&#36827;&#25991;&#26412;&#21040;&#38899;&#20048;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Music Understanding LLaMA: Advancing Text-to-Music Generation with Question Answering and Captioning. (arXiv:2308.11276v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#38899;&#20048;&#29702;&#35299;LLaMA&#65288;MU-LLaMA&#65289;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24212;&#29992;&#38382;&#31572;&#21644;&#23383;&#24149;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#25991;&#26412;&#21040;&#38899;&#20048;&#29983;&#25104;&#38754;&#20020;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;MusicQA&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;MU-LLaMA&#27169;&#22411;&#65292;&#24182;&#22312;&#38899;&#20048;&#38382;&#31572;&#26041;&#38754;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#32570;&#20047;&#20855;&#26377;&#33258;&#28982;&#35821;&#35328;&#23383;&#24149;&#30340;&#22823;&#35268;&#27169;&#20844;&#24320;&#38899;&#20048;&#25968;&#25454;&#38598;&#65292;&#25991;&#26412;&#21040;&#38899;&#20048;&#29983;&#25104;&#65288;T2M-Gen&#65289;&#38754;&#20020;&#37325;&#22823;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38899;&#20048;&#29702;&#35299;LLaMA&#65288;MU-LLaMA&#65289;&#65292;&#33021;&#22815;&#22238;&#31572;&#19982;&#38899;&#20048;&#30456;&#20851;&#30340;&#38382;&#39064;&#24182;&#20026;&#38899;&#20048;&#25991;&#20214;&#29983;&#25104;&#23383;&#24149;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;MERT&#27169;&#22411;&#20174;&#38899;&#39057;&#20013;&#25552;&#21462;&#38899;&#20048;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#36866;&#29992;&#20110;&#35757;&#32451;MU-LLaMA&#27169;&#22411;&#30340;&#21512;&#36866;&#25968;&#25454;&#38598;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#29616;&#26377;&#30340;&#20844;&#24320;&#21487;&#35775;&#38382;&#30340;&#38899;&#39057;&#38382;&#31572;&#25968;&#25454;&#38598;&#32570;&#20047;&#24320;&#25918;&#24335;&#38899;&#20048;&#38382;&#31572;&#25152;&#38656;&#30340;&#28145;&#24230;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#29616;&#26377;&#38899;&#39057;&#23383;&#24149;&#25968;&#25454;&#38598;&#29983;&#25104;&#38382;&#31572;&#23545;&#30340;&#26041;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#35774;&#35745;&#29992;&#20110;&#22238;&#31572;&#24320;&#25918;&#24335;&#38899;&#20048;&#30456;&#20851;&#38382;&#39064;&#30340;MusicQA&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#32463;&#36807;&#25105;&#20204;&#35774;&#35745;&#30340;MusicQA&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;MU-LLaMA&#27169;&#22411;&#22312;&#38899;&#20048;&#38382;&#31572;&#26041;&#38754;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-music generation (T2M-Gen) faces a major obstacle due to the scarcity of large-scale publicly available music datasets with natural language captions. To address this, we propose the Music Understanding LLaMA (MU-LLaMA), capable of answering music-related questions and generating captions for music files. Our model utilizes audio representations from a pretrained MERT model to extract music features. However, obtaining a suitable dataset for training the MU-LLaMA model remains challenging, as existing publicly accessible audio question answering datasets lack the necessary depth for open-ended music question answering. To fill this gap, we present a methodology for generating question-answer pairs from existing audio captioning datasets and introduce the MusicQA Dataset designed for answering open-ended music-related questions. The experiments demonstrate that the proposed MU-LLaMA model, trained on our designed MusicQA dataset, achieves outstanding performance in both music qu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#20855;&#26377;&#40065;&#26834;&#25289;&#26684;&#26391;&#26085;&#30340;RCPG&#21644;&#23545;&#25239;&#24615;RCPG&#65292;&#29992;&#20110;&#35299;&#20915;&#40065;&#26834;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#38382;&#39064;&#12290;&#20855;&#26377;&#40065;&#26834;&#25289;&#26684;&#26391;&#26085;&#30340;RCPG&#36890;&#36807;&#20351;&#29992;&#25289;&#26684;&#26391;&#26085;&#26469;&#35745;&#31639;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#21160;&#24577;&#65292;&#32780;&#23545;&#25239;&#24615;RCPG&#36890;&#36807;&#23545;&#25239;&#31574;&#30053;&#30340;&#26041;&#24335;&#30452;&#25509;&#21644;&#22686;&#37327;&#23398;&#20064;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#21160;&#24577;&#12290;</title><link>http://arxiv.org/abs/2308.11267</link><description>&lt;p&gt;
&#40065;&#26834;&#25289;&#26684;&#26391;&#26085;&#21644;&#23545;&#25239;&#24615;&#31574;&#30053;&#26799;&#24230;&#22312;&#40065;&#26834;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Robust Lagrangian and Adversarial Policy Gradient for Robust Constrained Markov Decision Processes. (arXiv:2308.11267v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#20855;&#26377;&#40065;&#26834;&#25289;&#26684;&#26391;&#26085;&#30340;RCPG&#21644;&#23545;&#25239;&#24615;RCPG&#65292;&#29992;&#20110;&#35299;&#20915;&#40065;&#26834;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#38382;&#39064;&#12290;&#20855;&#26377;&#40065;&#26834;&#25289;&#26684;&#26391;&#26085;&#30340;RCPG&#36890;&#36807;&#20351;&#29992;&#25289;&#26684;&#26391;&#26085;&#26469;&#35745;&#31639;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#21160;&#24577;&#65292;&#32780;&#23545;&#25239;&#24615;RCPG&#36890;&#36807;&#23545;&#25239;&#31574;&#30053;&#30340;&#26041;&#24335;&#30452;&#25509;&#21644;&#22686;&#37327;&#23398;&#20064;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;RCMDP&#65289;&#26159;&#19968;&#20010;&#26368;&#36817;&#24212;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20219;&#21153;&#24314;&#27169;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#22312;&#36716;&#31227;&#21160;&#24577;&#27169;&#22411;&#20013;&#25552;&#20379;&#20102;&#23545;&#38169;&#35823;&#30340;&#40065;&#26834;&#24615;&#12290;&#27169;&#25311;RCMDPs&#38656;&#35201;&#22522;&#20110;&#27599;&#20010;&#29366;&#24577;&#30340;&#20540;&#20272;&#35745;&#35745;&#31639;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#21160;&#24577;&#65292;&#36825;&#31181;&#26041;&#27861;&#20043;&#21069;&#22312;&#40065;&#26834;&#32422;&#26463;&#31574;&#30053;&#26799;&#24230;&#65288;RCPG&#65289;&#20013;&#20351;&#29992;&#36807;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#20998;&#21035;&#31216;&#20026;&#20855;&#26377;&#40065;&#26834;&#25289;&#26684;&#26391;&#26085;&#30340;RCPG&#21644;&#23545;&#25239;&#24615;RCPG&#12290;&#20855;&#26377;&#40065;&#26834;&#25289;&#26684;&#26391;&#26085;&#30340;RCPG&#36890;&#36807;&#20351;&#29992;&#25289;&#26684;&#26391;&#26085;&#32780;&#19981;&#26159;&#20540;&#25110;&#32422;&#26463;&#26469;&#35745;&#31639;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#21160;&#24577;&#20174;&#32780;&#20462;&#25913;RCPG&#12290;&#23545;&#25239;&#24615;RCPG&#20063;&#22522;&#20110;&#25289;&#26684;&#26391;&#26085;&#20844;&#24335;&#35745;&#31639;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#21160;&#24577;&#65292;&#20294;&#26159;&#23558;&#20854;&#20316;&#20026;&#23545;&#25239;&#31574;&#30053;&#30452;&#25509;&#21644;&#22686;&#37327;&#22320;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The robust constrained Markov decision process (RCMDP) is a recent task-modelling framework for reinforcement learning that incorporates behavioural constraints and that provides robustness to errors in the transition dynamics model through the use of an uncertainty set. Simulating RCMDPs requires computing the worst-case dynamics based on value estimates for each state, an approach which has previously been used in the Robust Constrained Policy Gradient (RCPG). Highlighting potential downsides of RCPG such as not robustifying the full constrained objective and the lack of incremental learning, this paper introduces two algorithms, called RCPG with Robust Lagrangian and Adversarial RCPG. RCPG with Robust Lagrangian modifies RCPG by taking the worst-case dynamics based on the Lagrangian rather than either the value or the constraint. Adversarial RCPG also formulates the worst-case dynamics based on the Lagrangian but learns this directly and incrementally as an adversarial policy throug
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#27714;&#35299;&#21338;&#24328;&#20013;&#39640;&#25928;&#25910;&#25947;&#31639;&#27861;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20998;&#26512;&#20048;&#35266;&#26799;&#24230;&#19979;&#38477;&#19978;&#21319;&#65288;OGDA&#65289;&#21644;&#20048;&#35266;&#20056;&#27861;&#26435;&#37325;&#26356;&#26032;&#65288;OMWU&#65289;&#31639;&#27861;&#65292;&#20197;&#21450;&#22522;&#20110;&#22870;&#21169;&#36716;&#21270;&#65288;RT&#65289;&#26694;&#26550;&#30340;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.11256</link><description>&lt;p&gt;
&#22312;&#27714;&#35299;&#21338;&#24328;&#20013;&#30340;&#39640;&#25928;&#25910;&#25947;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Last-iterate Convergence Algorithms in Solving Games. (arXiv:2308.11256v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11256
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#27714;&#35299;&#21338;&#24328;&#20013;&#39640;&#25928;&#25910;&#25947;&#31639;&#27861;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20998;&#26512;&#20048;&#35266;&#26799;&#24230;&#19979;&#38477;&#19978;&#21319;&#65288;OGDA&#65289;&#21644;&#20048;&#35266;&#20056;&#27861;&#26435;&#37325;&#26356;&#26032;&#65288;OMWU&#65289;&#31639;&#27861;&#65292;&#20197;&#21450;&#22522;&#20110;&#22870;&#21169;&#36716;&#21270;&#65288;RT&#65289;&#26694;&#26550;&#30340;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#24724;&#31639;&#27861;&#22312;&#23398;&#20064;&#20004;&#20154;&#38646;&#21644;&#26631;&#20934;&#22411;&#28216;&#25103;&#21644;&#25193;&#23637;&#22411;&#28216;&#25103;&#30340;&#32435;&#20160;&#22343;&#34913;&#20013;&#24456;&#21463;&#27426;&#36814;&#12290;&#26368;&#36817;&#30340;&#35768;&#22810;&#30740;&#31350;&#32771;&#34385;&#20102;&#26368;&#21518;&#19968;&#27425;&#36845;&#20195;&#25910;&#25947;&#30340;&#26080;&#24724;&#31639;&#27861;&#12290;&#20854;&#20013;&#65292;&#26368;&#26377;&#21517;&#30340;&#20004;&#20010;&#31639;&#27861;&#26159;&#20048;&#35266;&#26799;&#24230;&#19979;&#38477;&#19978;&#21319;&#65288;OGDA&#65289;&#21644;&#20048;&#35266;&#20056;&#27861;&#26435;&#37325;&#26356;&#26032;&#65288;OMWU&#65289;&#12290;&#28982;&#32780;&#65292;OGDA&#30340;&#27599;&#27425;&#36845;&#20195;&#22797;&#26434;&#24230;&#24456;&#39640;&#12290;OMWU&#20855;&#26377;&#36739;&#20302;&#30340;&#27599;&#27425;&#36845;&#20195;&#22797;&#26434;&#24230;&#65292;&#20294;&#23454;&#39564;&#24615;&#33021;&#36739;&#24046;&#65292;&#24182;&#19988;&#23427;&#30340;&#25910;&#25947;&#20165;&#22312;&#32435;&#20160;&#22343;&#34913;&#21807;&#19968;&#26102;&#25104;&#31435;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22870;&#21169;&#36716;&#21270;&#65288;RT&#65289;&#26694;&#26550;&#29992;&#20110;MWU&#65292;&#23427;&#28040;&#38500;&#20102;&#21807;&#19968;&#24615;&#26465;&#20214;&#65292;&#24182;&#19988;&#22312;&#19982;OMWU&#30456;&#21516;&#36845;&#20195;&#27425;&#25968;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22522;&#20110;RT&#30340;&#31639;&#27861;&#22312;&#30456;&#21516;&#36845;&#20195;&#27425;&#25968;&#19979;&#34920;&#29616;&#19981;&#22914;OGDA&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#25910;&#25947;&#20445;&#35777;&#22522;&#20110;&#36830;&#32493;&#26102;&#38388;&#21453;&#39304;&#20551;&#35774;&#65292;&#36825;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#19981;&#25104;&#31435;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#23545;RT&#26694;&#26550;&#36827;&#34892;&#20102;&#26356;&#35814;&#32454;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
No-regret algorithms are popular for learning Nash equilibrium (NE) in two-player zero-sum normal-form games (NFGs) and extensive-form games (EFGs). Many recent works consider the last-iterate convergence no-regret algorithms. Among them, the two most famous algorithms are Optimistic Gradient Descent Ascent (OGDA) and Optimistic Multiplicative Weight Update (OMWU). However, OGDA has high per-iteration complexity. OMWU exhibits a lower per-iteration complexity but poorer empirical performance, and its convergence holds only when NE is unique. Recent works propose a Reward Transformation (RT) framework for MWU, which removes the uniqueness condition and achieves competitive performance with OMWU. Unfortunately, RT-based algorithms perform worse than OGDA under the same number of iterations, and their convergence guarantee is based on the continuous-time feedback assumption, which does not hold in most scenarios. To address these issues, we provide a closer analysis of the RT framework, w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#30340;&#20559;&#24046;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#20559;&#24046;&#21644;&#38169;&#35823;&#30340;&#20998;&#31867;&#65292;&#24182;&#20998;&#26512;&#20102;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#36229;&#36807;&#22235;&#21313;&#31181;&#28508;&#22312;&#30340;&#20559;&#24046;&#26469;&#28304;&#65292;&#20026;&#27599;&#31181;&#24773;&#20917;&#25552;&#20379;&#20102;&#28165;&#26224;&#30340;&#31034;&#20363;&#12290;&#36890;&#36807;&#29702;&#35299;&#21644;&#20943;&#36731;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20559;&#24046;&#65292;&#21487;&#20197;&#24320;&#21457;&#20986;&#26356;&#20844;&#24179;&#12289;&#26356;&#36879;&#26126;&#12289;&#26356;&#20934;&#30830;&#30340;ML&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.11254</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#20559;&#24046;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A survey on bias in machine learning research. (arXiv:2308.11254v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#30340;&#20559;&#24046;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#20559;&#24046;&#21644;&#38169;&#35823;&#30340;&#20998;&#31867;&#65292;&#24182;&#20998;&#26512;&#20102;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#36229;&#36807;&#22235;&#21313;&#31181;&#28508;&#22312;&#30340;&#20559;&#24046;&#26469;&#28304;&#65292;&#20026;&#27599;&#31181;&#24773;&#20917;&#25552;&#20379;&#20102;&#28165;&#26224;&#30340;&#31034;&#20363;&#12290;&#36890;&#36807;&#29702;&#35299;&#21644;&#20943;&#36731;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20559;&#24046;&#65292;&#21487;&#20197;&#24320;&#21457;&#20986;&#26356;&#20844;&#24179;&#12289;&#26356;&#36879;&#26126;&#12289;&#26356;&#20934;&#30830;&#30340;ML&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#23545;&#26426;&#22120;&#23398;&#20064;&#20013;&#20559;&#24046;&#30340;&#30740;&#31350;&#36890;&#24120;&#20851;&#27880;&#20844;&#24179;&#24615;&#65292;&#21364;&#24573;&#35270;&#20102;&#20559;&#24046;&#30340;&#26681;&#28304;&#25110;&#21407;&#22240;&#12290;&#28982;&#32780;&#65292;&#20559;&#24046;&#26368;&#21021;&#34987;&#23450;&#20041;&#20026;&#8220;&#31995;&#32479;&#24615;&#38169;&#35823;&#8221;&#65292;&#36890;&#24120;&#26159;&#30001;&#30740;&#31350;&#36807;&#31243;&#20013;&#19981;&#21516;&#38454;&#27573;&#30340;&#20154;&#31867;&#24341;&#36215;&#30340;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#20559;&#24046;&#21644;&#25968;&#25454;&#27169;&#22411;&#20013;&#28508;&#22312;&#20559;&#24046;&#21644;&#38169;&#35823;&#30340;&#20998;&#31867;&#65292;&#24357;&#34917;&#36807;&#21435;&#20851;&#20110;&#20559;&#24046;&#30740;&#31350;&#30340;&#24046;&#36317;&#12290;&#35813;&#25991;&#37325;&#28857;&#30740;&#31350;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#30340;&#20559;&#24046;&#12290;&#35843;&#26597;&#20998;&#26512;&#20102;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27969;&#31243;&#20013;&#36229;&#36807;&#22235;&#21313;&#31181;&#28508;&#22312;&#30340;&#20559;&#24046;&#26469;&#28304;&#65292;&#24182;&#20026;&#27599;&#31181;&#24773;&#20917;&#25552;&#20379;&#20102;&#28165;&#26224;&#30340;&#31034;&#20363;&#12290;&#36890;&#36807;&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#20013;&#20559;&#24046;&#30340;&#26469;&#28304;&#21644;&#21518;&#26524;&#65292;&#21487;&#20197;&#24320;&#21457;&#20986;&#26356;&#22909;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#21644;&#20943;&#36731;&#20559;&#24046;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20844;&#24179;&#12289;&#26356;&#36879;&#26126;&#12289;&#26356;&#20934;&#30830;&#30340;ML&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current research on bias in machine learning often focuses on fairness, while overlooking the roots or causes of bias. However, bias was originally defined as a "systematic error," often caused by humans at different stages of the research process. This article aims to bridge the gap between past literature on bias in research by providing taxonomy for potential sources of bias and errors in data and models. The paper focus on bias in machine learning pipelines. Survey analyses over forty potential sources of bias in the machine learning (ML) pipeline, providing clear examples for each. By understanding the sources and consequences of bias in machine learning, better methods can be developed for its detecting and mitigating, leading to fairer, more transparent, and more accurate ML models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#21270;&#23398;&#36807;&#31243;&#30340;&#20132;&#21449;&#39046;&#22495;&#25925;&#38556;&#35786;&#26029;&#20013;&#65292;&#23545;&#21333;&#28304;&#21644;&#22810;&#28304;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#31639;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#27604;&#36739;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#27809;&#26377;&#36827;&#34892;&#36866;&#24212;&#65292;&#20351;&#29992;&#22810;&#20010;&#39046;&#22495;&#36827;&#34892;&#35757;&#32451;&#20063;&#20855;&#26377;&#31215;&#26497;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.11247</link><description>&lt;p&gt;
&#22810;&#28304;&#39046;&#22495;&#36866;&#24212;&#29992;&#20110;&#21270;&#23398;&#36807;&#31243;&#20132;&#21449;&#39046;&#22495;&#25925;&#38556;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Multi-Source Domain Adaptation for Cross-Domain Fault Diagnosis of Chemical Processes. (arXiv:2308.11247v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#21270;&#23398;&#36807;&#31243;&#30340;&#20132;&#21449;&#39046;&#22495;&#25925;&#38556;&#35786;&#26029;&#20013;&#65292;&#23545;&#21333;&#28304;&#21644;&#22810;&#28304;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#31639;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#27604;&#36739;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#27809;&#26377;&#36827;&#34892;&#36866;&#24212;&#65292;&#20351;&#29992;&#22810;&#20010;&#39046;&#22495;&#36827;&#34892;&#35757;&#32451;&#20063;&#20855;&#26377;&#31215;&#26497;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25925;&#38556;&#35786;&#26029;&#26159;&#36807;&#31243;&#30417;&#35270;&#20013;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#26426;&#22120;&#23398;&#20064;&#30340;&#25925;&#38556;&#35786;&#26029;&#31995;&#32479;&#22522;&#20110;&#20256;&#24863;&#22120;&#25968;&#25454;&#39044;&#27979;&#25925;&#38556;&#31867;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23545;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#25935;&#24863;&#65292;&#36825;&#20123;&#21464;&#21270;&#21487;&#33021;&#30001;&#20110;&#30417;&#27979;&#36807;&#31243;&#20013;&#30340;&#21464;&#21270;&#65292;&#22914;&#25805;&#20316;&#27169;&#24335;&#30340;&#25913;&#21464;&#65292;&#23548;&#33268;&#36328;&#39046;&#22495;&#25925;&#38556;&#35786;&#26029;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#22312;&#21270;&#23398;&#24037;&#19994;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#30000;&#32435;&#35199;-&#20234;&#26031;&#26364;&#36807;&#31243;&#30340;&#32972;&#26223;&#19979;&#65292;&#25552;&#20379;&#20102;&#21333;&#28304;&#21644;&#22810;&#28304;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#31639;&#27861;&#22312;&#20132;&#21449;&#39046;&#22495;&#25925;&#38556;&#35786;&#26029;&#20013;&#30340;&#24191;&#27867;&#27604;&#36739;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#27809;&#26377;&#36827;&#34892;&#36866;&#24212;&#65292;&#20351;&#29992;&#22810;&#20010;&#39046;&#22495;&#36827;&#34892;&#35757;&#32451;&#20063;&#20855;&#26377;&#31215;&#26497;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#22810;&#28304;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#30340;&#22522;&#20934;&#27169;&#22411;&#30456;&#23545;&#20110;&#21333;&#28304;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#30340;&#22522;&#20934;&#27169;&#22411;&#26377;&#25152;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fault diagnosis is an essential component in process supervision. Indeed, it determines which kind of fault has occurred, given that it has been previously detected, allowing for appropriate intervention. Automatic fault diagnosis systems use machine learning for predicting the fault type from sensor readings. Nonetheless, these models are sensible to changes in the data distributions, which may be caused by changes in the monitored process, such as changes in the mode of operation. This scenario is known as Cross-Domain Fault Diagnosis (CDFD). We provide an extensive comparison of single and multi-source unsupervised domain adaptation (SSDA and MSDA respectively) algorithms for CDFD. We study these methods in the context of the Tennessee-Eastmann Process, a widely used benchmark in the chemical industry. We show that using multiple domains during training has a positive effect, even when no adaptation is employed. As such, the MSDA baseline improves over the SSDA baseline classificati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;S-Graphs&#20013;&#21033;&#29992;&#23618;&#27425;&#24615;&#36827;&#34892;&#24555;&#36895;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36793;&#32536;&#21270;&#20887;&#20313;&#30340;&#26426;&#22120;&#20154;&#23039;&#24577;&#21450;&#20854;&#19982;&#30456;&#21516;&#32467;&#26500;&#23454;&#20307;&#30340;&#35266;&#27979;&#20043;&#38388;&#30340;&#36830;&#25509;&#26469;&#20943;&#23567;&#22270;&#30340;&#22823;&#23567;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#21644;&#20248;&#21270;&#23616;&#37096;&#22270;&#26469;&#23454;&#29616;&#21387;&#32553;S-Graphs&#12290;</title><link>http://arxiv.org/abs/2308.11242</link><description>&lt;p&gt;
&#22312;S-Graphs&#20013;&#21033;&#29992;&#23618;&#27425;&#24615;&#36827;&#34892;&#24555;&#36895;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Faster Optimization in S-Graphs Exploiting Hierarchy. (arXiv:2308.11242v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11242
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;S-Graphs&#20013;&#21033;&#29992;&#23618;&#27425;&#24615;&#36827;&#34892;&#24555;&#36895;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36793;&#32536;&#21270;&#20887;&#20313;&#30340;&#26426;&#22120;&#20154;&#23039;&#24577;&#21450;&#20854;&#19982;&#30456;&#21516;&#32467;&#26500;&#23454;&#20307;&#30340;&#35266;&#27979;&#20043;&#38388;&#30340;&#36830;&#25509;&#26469;&#20943;&#23567;&#22270;&#30340;&#22823;&#23567;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#21644;&#20248;&#21270;&#23616;&#37096;&#22270;&#26469;&#23454;&#29616;&#21387;&#32553;S-Graphs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
3D&#22330;&#26223;&#22270;&#20197;&#23618;&#27425;&#26041;&#24335;&#36866;&#24403;&#22320;&#32452;&#32455;&#19981;&#21516;&#30340;&#29615;&#22659;&#23454;&#20307;&#12290;&#25105;&#20204;&#20043;&#21069;&#30340;&#24037;&#20316;&#22312;SLAM&#20013;&#23558;3D&#22330;&#26223;&#22270;&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;&#20102;&#24773;&#22659;&#22270;&#65292;&#36890;&#36807;&#32039;&#23494;&#22320;&#23558;&#26426;&#22120;&#20154;&#23039;&#24577;&#19982;&#22330;&#26223;&#22270;&#23454;&#20307;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;S-Graphs&#30340;&#19968;&#20010;&#38480;&#21046;&#26159;&#22312;&#30495;&#27491;&#22823;&#30340;&#29615;&#22659;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#30001;&#20110;&#38543;&#26102;&#38388;&#22686;&#38271;&#22270;&#30340;&#22823;&#23567;&#22686;&#21152;&#65292;&#35745;&#31639;&#22797;&#26434;&#24615;&#22686;&#21152;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#25913;&#36827;&#29256;&#26412;&#30340;S-Graphs&#30340;&#21021;&#27493;&#30740;&#31350;&#65292;&#21033;&#29992;&#23618;&#27425;&#24615;&#26469;&#20943;&#23567;&#22270;&#30340;&#22823;&#23567;&#65292;&#36890;&#36807;&#36793;&#32536;&#21270;&#20887;&#20313;&#30340;&#26426;&#22120;&#20154;&#23039;&#24577;&#21450;&#20854;&#19982;&#30456;&#21516;&#32467;&#26500;&#23454;&#20307;&#30340;&#35266;&#27979;&#20043;&#38388;&#30340;&#36830;&#25509;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#31867;&#20284;&#25151;&#38388;&#30340;&#32467;&#26500;&#20013;&#21253;&#21547;&#25152;&#26377;&#22270;&#23454;&#20307;&#30340;&#23616;&#37096;&#22270;&#30340;&#29983;&#25104;&#21644;&#20248;&#21270;&#12290;&#36825;&#20123;&#23616;&#37096;&#22270;&#29992;&#20110;&#21387;&#32553;S-Graphs&#65292;&#36793;&#32536;&#21270;&#20887;&#20313;&#30340;&#26426;&#22120;&#20154;&#20851;&#38190;&#24103;&#21450;&#20854;&#19982;&#35266;&#27979;&#20043;&#38388;&#30340;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D scene graphs hierarchically represent the environment appropriately organizing different environmental entities in various layers. Our previous work on situational graphs extends the concept of 3D scene graph to SLAM by tightly coupling the robot poses with the scene graph entities, achieving state-of-the-art results. Though, one of the limitations of S-Graphs is scalability in really large environments due to the increased graph size over time, increasing the computational complexity.  To overcome this limitation in this work we present an initial research of an improved version of S-Graphs exploiting the hierarchy to reduce the graph size by marginalizing redundant robot poses and their connections to the observations of the same structural entities. Firstly, we propose the generation and optimization of room-local graphs encompassing all graph entities within a room-like structure. These room-local graphs are used to compress the S-Graphs marginalizing the redundant robot keyfram
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#20154;&#35774;&#35745;&#27169;&#24335;&#65292;&#21517;&#20026;Prompting Robotic Modalities&#65288;PRM&#65289;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#26469;&#25511;&#21046;&#26426;&#22120;&#20154;&#12290;&#24182;&#19988;&#22312;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;ROSGPT_Vision&#30340;&#26426;&#22120;&#20154;&#26694;&#26550;&#19978;&#24212;&#29992;&#20102;&#36825;&#31181;&#35774;&#35745;&#27169;&#24335;&#12290;&#36825;&#20010;&#26694;&#26550;&#33021;&#22815;&#36890;&#36807;&#35270;&#35273;&#25552;&#31034;&#21644;LLM&#25552;&#31034;&#25191;&#34892;&#26426;&#22120;&#20154;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.11236</link><description>&lt;p&gt;
ROSGPT_Vision: &#20165;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#26469;&#25511;&#21046;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
ROSGPT_Vision: Commanding Robots Using Only Language Models' Prompts. (arXiv:2308.11236v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#20154;&#35774;&#35745;&#27169;&#24335;&#65292;&#21517;&#20026;Prompting Robotic Modalities&#65288;PRM&#65289;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#26469;&#25511;&#21046;&#26426;&#22120;&#20154;&#12290;&#24182;&#19988;&#22312;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;ROSGPT_Vision&#30340;&#26426;&#22120;&#20154;&#26694;&#26550;&#19978;&#24212;&#29992;&#20102;&#36825;&#31181;&#35774;&#35745;&#27169;&#24335;&#12290;&#36825;&#20010;&#26694;&#26550;&#33021;&#22815;&#36890;&#36807;&#35270;&#35273;&#25552;&#31034;&#21644;LLM&#25552;&#31034;&#25191;&#34892;&#26426;&#22120;&#20154;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35748;&#20026;&#65292;&#19979;&#19968;&#20195;&#26426;&#22120;&#20154;&#21487;&#20197;&#20165;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#26469;&#36827;&#34892;&#21629;&#20196;&#12290;&#27599;&#20010;&#25552;&#31034;&#36890;&#36807;&#20854;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65288;MLM&#65289;&#21333;&#29420;&#26597;&#35810;&#29305;&#23450;&#30340;&#26426;&#22120;&#20154;&#27169;&#24577;&#12290;&#20013;&#22830;&#20219;&#21153;&#27169;&#24577;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35843;&#33410;&#25972;&#20010;&#36890;&#20449;&#20197;&#25191;&#34892;&#26426;&#22120;&#20154;&#20219;&#21153;&#12290;&#26412;&#25991;&#23558;&#36825;&#31181;&#26032;&#30340;&#26426;&#22120;&#20154;&#35774;&#35745;&#27169;&#24335;&#21629;&#21517;&#20026;&#65306;Prompting Robotic Modalities&#65288;PRM&#65289;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#23558;&#36825;&#20010;PRM&#35774;&#35745;&#27169;&#24335;&#24212;&#29992;&#20110;&#26500;&#24314;&#19968;&#20010;&#21517;&#20026;ROSGPT_Vision&#30340;&#26032;&#30340;&#26426;&#22120;&#20154;&#26694;&#26550;&#12290;ROSGPT_Vision&#21482;&#38656;&#35201;&#20004;&#20010;&#25552;&#31034;&#21363;&#21487;&#25191;&#34892;&#26426;&#22120;&#20154;&#20219;&#21153;&#65306;&#19968;&#20010;&#26159;&#35270;&#35273;&#25552;&#31034;&#65292;&#19968;&#20010;&#26159;LLM&#25552;&#31034;&#12290;&#35270;&#35273;&#25552;&#31034;&#20197;&#33258;&#28982;&#35821;&#35328;&#25552;&#21462;&#19982;&#25152;&#32771;&#34385;&#20219;&#21153;&#30456;&#20851;&#30340;&#35270;&#35273;&#35821;&#20041;&#29305;&#24449;&#65288;&#35270;&#35273;&#26426;&#22120;&#20154;&#27169;&#24577;&#65289;&#12290;&#21516;&#26102;&#65292;LLM&#25552;&#31034;&#35843;&#33410;&#26426;&#22120;&#20154;&#23545;&#35270;&#35273;&#25551;&#36848;&#30340;&#21453;&#24212;&#65288;&#20219;&#21153;&#27169;&#24577;&#65289;&#12290;&#35813;&#26694;&#26550;&#33258;&#21160;&#21270;&#20102;&#36825;&#20004;&#20010;&#25552;&#31034;&#32972;&#21518;&#30340;&#25152;&#26377;&#26426;&#21046;&#12290;&#35813;&#26694;&#26550;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;...
&lt;/p&gt;
&lt;p&gt;
In this paper, we argue that the next generation of robots can be commanded using only Language Models' prompts. Every prompt interrogates separately a specific Robotic Modality via its Modality Language Model (MLM). A central Task Modality mediates the whole communication to execute the robotic mission via a Large Language Model (LLM). This paper gives this new robotic design pattern the name of: Prompting Robotic Modalities (PRM). Moreover, this paper applies this PRM design pattern in building a new robotic framework named ROSGPT_Vision. ROSGPT_Vision allows the execution of a robotic task using only two prompts: a Visual and an LLM prompt. The Visual Prompt extracts, in natural language, the visual semantic features related to the task under consideration (Visual Robotic Modality). Meanwhile, the LLM Prompt regulates the robotic reaction to the visual description (Task Modality). The framework automates all the mechanisms behind these two prompts. The framework enables the robot to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30333;&#30418;&#27700;&#21360;&#25216;&#26415;&#65292;&#36890;&#36807;&#33258;&#25105;&#20114;&#21161;&#26816;&#26597;&#21442;&#25968;&#26469;&#26816;&#27979;&#31713;&#25913;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#26368;&#22823;&#21270;&#20449;&#24687;&#23481;&#37327;&#30340;&#33258;&#36866;&#24212;&#23884;&#20837;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#24403;&#20462;&#25913;&#29575;&#20302;&#20110;20%&#26102;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#20986;&#33394;&#30340;&#24674;&#22797;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11235</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#30333;&#30418;&#27700;&#21360;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;&#19982;&#33258;&#25105;&#20114;&#21161;&#26816;&#26597;&#21442;&#25968;
&lt;/p&gt;
&lt;p&gt;
Adaptive White-Box Watermarking with Self-Mutual Check Parameters in Deep Neural Networks. (arXiv:2308.11235v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30333;&#30418;&#27700;&#21360;&#25216;&#26415;&#65292;&#36890;&#36807;&#33258;&#25105;&#20114;&#21161;&#26816;&#26597;&#21442;&#25968;&#26469;&#26816;&#27979;&#31713;&#25913;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#26368;&#22823;&#21270;&#20449;&#24687;&#23481;&#37327;&#30340;&#33258;&#36866;&#24212;&#23884;&#20837;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#24403;&#20462;&#25913;&#29575;&#20302;&#20110;20%&#26102;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#20986;&#33394;&#30340;&#24674;&#22797;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#21508;&#20010;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#22312;&#37096;&#32626;&#36807;&#31243;&#20013;&#20063;&#38754;&#20020;&#30528;&#24847;&#22806;&#25110;&#24694;&#24847;&#31713;&#25913;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#26816;&#27979;&#21644;&#38450;&#27490;&#36825;&#20123;&#39118;&#38505;&#65292;&#38656;&#35201;&#36827;&#34892;&#23450;&#26399;&#26816;&#26597;&#12290;&#33030;&#24369;&#27700;&#21360;&#25216;&#26415;&#21487;&#20197;&#29992;&#26469;&#35782;&#21035;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#20013;&#30340;&#31713;&#25913;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;&#26041;&#27861;&#23384;&#22312;&#30528;&#36951;&#28431;&#30340;&#39118;&#38505;&#12289;&#39069;&#22806;&#20449;&#24687;&#20256;&#36755;&#30340;&#38382;&#39064;&#20197;&#21450;&#26080;&#27861;&#31934;&#30830;&#23450;&#20301;&#31713;&#25913;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#27979;&#31713;&#25913;&#21442;&#25968;&#21644;&#20301;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#29992;&#26469;&#26816;&#27979;&#12289;&#23450;&#20301;&#21644;&#24674;&#22797;&#34987;&#31713;&#25913;&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#23884;&#20837;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#20445;&#25345;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#26368;&#22823;&#21270;&#20449;&#24687;&#23481;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#32463;&#21463;&#20102;&#20462;&#25913;&#26435;&#37325;&#21442;&#25968;&#25915;&#20987;&#30340;&#31070;&#32463;&#32593;&#32476;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#20462;&#25913;&#29575;&#20302;&#20110;20%&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#20986;&#33394;&#30340;&#24674;&#22797;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#20351;&#29992;&#27700;&#21360;&#30340;&#27169;&#22411;&#65292;&#36825;&#20123;&#27700;&#21360;&#30340;&#23384;&#22312;&#19981;&#20250;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#26174;&#33879;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) has found wide application, but also poses risks due to unintentional or malicious tampering during deployment. Regular checks are therefore necessary to detect and prevent such risks. Fragile watermarking is a technique used to identify tampering in AI models. However, previous methods have faced challenges including risks of omission, additional information transmission, and inability to locate tampering precisely. In this paper, we propose a method for detecting tampered parameters and bits, which can be used to detect, locate, and restore parameters that have been tampered with. We also propose an adaptive embedding method that maximizes information capacity while maintaining model accuracy. Our approach was tested on multiple neural networks subjected to attacks that modified weight parameters, and our results demonstrate that our method achieved great recovery performance when the modification rate was below 20%. Furthermore, for models where watermar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32456;&#36523;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#23548;&#26234;&#33021;&#20307;&#36991;&#24320;&#25317;&#22581;&#36335;&#24452;&#26469;&#20248;&#21270;&#20132;&#36890;&#27969;&#37327;&#65292;&#26174;&#33879;&#25552;&#39640;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#21644;&#24635;&#20307;&#21534;&#21520;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.11234</link><description>&lt;p&gt;
&#20132;&#36890;&#27969;&#37327;&#20248;&#21270;&#30340;&#32456;&#36523;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Traffic Flow Optimisation for Lifelong Multi-Agent Path Finding. (arXiv:2308.11234v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32456;&#36523;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#23548;&#26234;&#33021;&#20307;&#36991;&#24320;&#25317;&#22581;&#36335;&#24452;&#26469;&#20248;&#21270;&#20132;&#36890;&#27969;&#37327;&#65292;&#26174;&#33879;&#25552;&#39640;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#21644;&#24635;&#20307;&#21534;&#21520;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;(MAPF)&#26159;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#35201;&#27714;&#20026;&#19968;&#20010;&#22242;&#38431;&#30340;&#26234;&#33021;&#20307;&#35745;&#31639;&#26080;&#30896;&#25758;&#36335;&#24452;&#65292;&#25152;&#26377;&#26234;&#33021;&#20307;&#37117;&#22312;&#20849;&#20139;&#22320;&#22270;&#19978;&#31227;&#21160;&#12290;&#23613;&#31649;&#26377;&#35768;&#22810;&#30456;&#20851;&#30740;&#31350;&#65292;&#20294;&#24403;&#21069;&#30340;&#31639;&#27861;&#22312;&#26234;&#33021;&#20307;&#25968;&#37327;&#22686;&#21152;&#26102;&#37117;&#20250;&#36935;&#21040;&#22256;&#38590;&#12290;&#20027;&#35201;&#21407;&#22240;&#26159;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#35268;&#21010;&#33258;&#30001;&#27969;&#21160;&#30340;&#26368;&#20248;&#36335;&#24452;&#65292;&#36825;&#20250;&#23548;&#33268;&#25317;&#22581;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;MAPF&#26041;&#27861;&#65292;&#36890;&#36807;&#36319;&#38543;&#36991;&#20813;&#25317;&#22581;&#30340;&#36335;&#24452;&#26469;&#24341;&#23548;&#26234;&#33021;&#20307;&#21040;&#36798;&#30446;&#30340;&#22320;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;&#22330;&#26223;&#20013;&#35780;&#20272;&#20102;&#36825;&#20010;&#24819;&#27861;&#65306;&#19968;&#27425;&#24615;MAPF&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#21482;&#26377;&#19968;&#20010;&#30446;&#30340;&#22320;&#65292;&#20197;&#21450;&#32456;&#36523;MAPF&#65292;&#26234;&#33021;&#20307;&#19981;&#26029;&#34987;&#20998;&#37197;&#26032;&#20219;&#21153;&#12290;&#23545;&#20110;&#19968;&#27425;&#24615;MAPF&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22823;&#22823;&#25552;&#39640;&#20102;&#35299;&#20915;&#26041;&#26696;&#30340;&#36136;&#37327;&#12290;&#23545;&#20110;&#32456;&#36523;MAPF&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#24635;&#20307;&#21534;&#21520;&#37327;&#30340;&#22823;&#24133;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Path Finding (MAPF) is a fundamental problem in robotics that asks us to compute collision-free paths for a team of agents, all moving across a shared map. Although many works appear on this topic, all current algorithms struggle as the number of agents grows. The principal reason is that existing approaches typically plan free-flow optimal paths, which creates congestion. To tackle this issue we propose a new approach for MAPF where agents are guided to their destination by following congestion-avoiding paths. We evaluate the idea in two large-scale settings: one-shot MAPF, where each agent has a single destination, and lifelong MAPF, where agents are continuously assigned new tasks. For one-shot MAPF we show that our approach substantially improves solution quality. For Lifelong MAPF we report large improvements in overall throughput.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#21033;&#29992;&#24320;&#28304;&#24037;&#20855;&#23454;&#26045;&#26412;&#22320;AIOps&#35299;&#20915;&#26041;&#26696;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#25104;&#21151;&#37096;&#32626;&#20102;&#19968;&#31181;&#32508;&#21512;AIOps&#22522;&#30784;&#35774;&#26045;&#65292;&#24182;&#25552;&#20379;&#20102;&#26500;&#24314;&#20854;&#21508;&#20010;&#32452;&#20214;&#30340;&#19981;&#21516;&#36873;&#25321;&#30340;&#29702;&#30001;&#12290;</title><link>http://arxiv.org/abs/2308.11225</link><description>&lt;p&gt;
&#20013;&#23567;&#22411;&#36719;&#20214;&#32534;&#36753;&#20844;&#21496;&#30340;&#26412;&#22320;AIOps&#22522;&#30784;&#35774;&#26045;&#65306;&#19968;&#20221;&#32463;&#39564;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
On-Premise AIOps Infrastructure for a Software Editor SME: An Experience Report. (arXiv:2308.11225v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#21033;&#29992;&#24320;&#28304;&#24037;&#20855;&#23454;&#26045;&#26412;&#22320;AIOps&#35299;&#20915;&#26041;&#26696;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#25104;&#21151;&#37096;&#32626;&#20102;&#19968;&#31181;&#32508;&#21512;AIOps&#22522;&#30784;&#35774;&#26045;&#65292;&#24182;&#25552;&#20379;&#20102;&#26500;&#24314;&#20854;&#21508;&#20010;&#32452;&#20214;&#30340;&#19981;&#21516;&#36873;&#25321;&#30340;&#29702;&#30001;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#25216;&#26415;&#24050;&#25104;&#20026;&#21508;&#34892;&#21508;&#19994;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#23548;&#33268;&#23545;&#36719;&#20214;&#32500;&#25252;&#21644;&#30417;&#25511;&#30340;&#20851;&#27880;&#22686;&#21152;&#12290;&#38543;&#30528;&#29616;&#20195;&#36719;&#20214;&#31995;&#32479;&#30340;&#22797;&#26434;&#24615;&#65292;&#20256;&#32479;&#30340;&#32500;&#25252;&#26041;&#27861;&#24050;&#32463;&#19981;&#36275;&#22815;&#12290;AIOps&#30340;&#27010;&#24565;&#24212;&#36816;&#32780;&#29983;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#25968;&#25454;&#21644;&#26426;&#22120;&#23398;&#20064;&#33021;&#21147;&#26469;&#22686;&#24378;&#39044;&#27979;&#24615;&#32500;&#25252;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992;AIOps&#38656;&#35201;&#35299;&#20915;&#19982;&#25968;&#25454;&#21644;&#20107;&#20214;&#31649;&#29702;&#22797;&#26434;&#24615;&#30456;&#20851;&#30340;&#20960;&#20010;&#25361;&#25112;&#12290;&#21830;&#19994;&#35299;&#20915;&#26041;&#26696;&#23384;&#22312;&#65292;&#20294;&#30001;&#20110;&#39640;&#26114;&#30340;&#25104;&#26412;&#12289;&#25968;&#25454;&#27835;&#29702;&#38382;&#39064;&#21644;&#19981;&#35206;&#30422;&#31169;&#26377;&#36719;&#20214;&#30340;&#38480;&#21046;&#65292;&#23427;&#20204;&#21487;&#33021;&#19981;&#36866;&#21512;&#26576;&#20123;&#20844;&#21496;&#12290;&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#24320;&#28304;&#24037;&#20855;&#26469;&#23454;&#26045;&#26412;&#22320;AIOps&#35299;&#20915;&#26041;&#26696;&#30340;&#21487;&#34892;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;&#20844;&#21496;&#25104;&#21151;&#37096;&#32626;&#30340;&#19968;&#31181;&#32508;&#21512;AIOps&#22522;&#30784;&#35774;&#26045;&#65292;&#24182;&#25552;&#20379;&#20102;&#26500;&#24314;&#20854;&#21508;&#20010;&#32452;&#20214;&#30340;&#19981;&#21516;&#36873;&#25321;&#30340;&#29702;&#30001;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information Technology has become a critical component in various industries, leading to an increased focus on software maintenance and monitoring. With the complexities of modern software systems, traditional maintenance approaches have become insufficient. The concept of AIOps has emerged to enhance predictive maintenance using Big Data and Machine Learning capabilities. However, exploiting AIOps requires addressing several challenges related to the complexity of data and incident management. Commercial solutions exist, but they may not be suitable for certain companies due to high costs, data governance issues, and limitations in covering private software. This paper investigates the feasibility of implementing on-premise AIOps solutions by leveraging open-source tools. We introduce a comprehensive AIOps infrastructure that we have successfully deployed in our company, and we provide the rationale behind different choices that we made to build its various components. Particularly, w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22235;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#25968;&#25454;&#19978;&#35299;&#20915;&#20998;&#26512;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;LLM&#22312;&#29702;&#35299;&#22270;&#25968;&#25454;&#12289;&#29983;&#25104;&#27491;&#30830;&#32467;&#26524;&#21644;&#36827;&#34892;&#32467;&#26500;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#30495;&#23454;&#24615;&#21644;&#30699;&#27491;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.11224</link><description>&lt;p&gt;
&#22312;&#22270;&#19978;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#24615;&#33021;&#27934;&#23519;&#19982;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Evaluating Large Language Models on Graphs: Performance Insights and Comparative Analysis. (arXiv:2308.11224v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22235;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#25968;&#25454;&#19978;&#35299;&#20915;&#20998;&#26512;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;LLM&#22312;&#29702;&#35299;&#22270;&#25968;&#25454;&#12289;&#29983;&#25104;&#27491;&#30830;&#32467;&#26524;&#21644;&#36827;&#34892;&#32467;&#26500;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#30495;&#23454;&#24615;&#21644;&#30699;&#27491;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#24341;&#36215;&#20102;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#28982;&#32780;LLM&#22312;&#22270;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22235;&#20010;LLM&#22312;&#35299;&#20915;&#20960;&#20010;&#22270;&#25968;&#25454;&#20998;&#26512;&#38382;&#39064;&#26102;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#22235;&#20010;&#19981;&#21516;&#30340;&#35780;&#20272;&#25351;&#26631;&#65306;&#29702;&#35299;&#33021;&#21147;&#12289;&#27491;&#30830;&#24615;&#12289;&#30495;&#23454;&#24615;&#21644;&#30699;&#27491;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65306;1) LLM&#33021;&#22815;&#26377;&#25928;&#22320;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#22270;&#25968;&#25454;&#65292;&#24182;&#25512;&#29702;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;2) GPT&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#36923;&#36753;&#21644;&#36830;&#36143;&#30340;&#32467;&#26524;&#65292;&#22312;&#27491;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26367;&#20195;&#26041;&#26696;&#12290;3) &#25152;&#26377;&#34987;&#26816;&#27979;&#30340;LLM&#22312;&#32467;&#26500;&#25512;&#29702;&#26041;&#38754;&#37117;&#38754;&#20020;&#25361;&#25112;&#65292;&#38646;&#26679;&#26412;&#24605;&#32500;&#38142;&#21644;&#23569;&#26679;&#26412;&#25552;&#31034;&#31561;&#25216;&#26415;&#26174;&#31034;&#20986;&#25928;&#26524;&#19979;&#38477;&#12290;4) GPT&#27169;&#22411;&#22312;&#22810;&#31572;&#26696;&#20219;&#21153;&#20013;&#32463;&#24120;&#20135;&#29983;&#38169;&#35823;&#31572;&#26696;&#65292;&#24341;&#21457;&#30495;&#23454;&#24615;&#26041;&#38754;&#30340;&#25285;&#24551;&#12290;5) GPT&#27169;&#22411;&#23545;&#20854;&#36755;&#20986;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#20449;&#24515;&#65292;&#21487;&#33021;&#38459;&#30861;&#20854;&#30699;&#27491;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;GPT-4&#26174;&#31034;&#20986;&#20102;&#19981;&#21516;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have garnered considerable interest within both academic and industrial. Yet, the application of LLMs to graph data remains under-explored. In this study, we evaluate the capabilities of four LLMs in addressing several analytical problems with graph data. We employ four distinct evaluation metrics: Comprehension, Correctness, Fidelity, and Rectification. Our results show that: 1) LLMs effectively comprehend graph data in natural language and reason with graph topology. 2) GPT models can generate logical and coherent results, outperforming alternatives in correctness. 3) All examined LLMs face challenges in structural reasoning, with techniques like zero-shot chain-of-thought and few-shot prompting showing diminished efficacy. 4) GPT models often produce erroneous answers in multi-answer tasks, raising concerns in fidelity. 5) GPT models exhibit elevated confidence in their outputs, potentially hindering their rectification capacities. Notably, GPT-4 has dem
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#35775;&#38382;&#22823;&#37327;&#22810;&#26679;&#30340;&#24739;&#32773;&#25968;&#25454;&#24182;&#20445;&#25252;&#38544;&#31169;&#65292;&#26469;&#39044;&#27979;&#22810;&#22218;&#21365;&#24034;&#32508;&#21512;&#24449;&#24739;&#32773;&#30340;&#26368;&#20339;&#27835;&#30103;&#33647;&#29289;&#36873;&#39033;&#12290;</title><link>http://arxiv.org/abs/2308.11220</link><description>&lt;p&gt;
&#20351;&#29992;&#24739;&#32773;&#25968;&#25454;&#30340;&#32852;&#37030;&#23398;&#20064;&#20197;&#20445;&#25252;&#22810;&#22218;&#21365;&#24034;&#32508;&#21512;&#24449;&#30340;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Federated Learning on Patient Data for Privacy-Protecting Polycystic Ovary Syndrome Treatment. (arXiv:2308.11220v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#35775;&#38382;&#22823;&#37327;&#22810;&#26679;&#30340;&#24739;&#32773;&#25968;&#25454;&#24182;&#20445;&#25252;&#38544;&#31169;&#65292;&#26469;&#39044;&#27979;&#22810;&#22218;&#21365;&#24034;&#32508;&#21512;&#24449;&#24739;&#32773;&#30340;&#26368;&#20339;&#27835;&#30103;&#33647;&#29289;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22919;&#31185;&#20869;&#20998;&#27852;&#23398;&#39046;&#22495;&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#21307;&#30103;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#33853;&#21518;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#23545;&#24739;&#32773;&#25968;&#25454;&#38544;&#31169;&#30340;&#25285;&#24551;&#12290;&#26377;&#20851;&#33655;&#23572;&#33945;&#27700;&#24179;&#25110;&#26376;&#32463;&#21608;&#26399;&#30340;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#28857;&#21487;&#33021;&#20250;&#26292;&#38706;&#20986;&#24739;&#26377;&#21512;&#24182;&#30151;&#25110;&#32456;&#27490;&#22922;&#23072;&#30340;&#24739;&#32773;&#65292;&#20405;&#29359;&#20854;&#38544;&#31169;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#39044;&#27979;&#22810;&#22218;&#21365;&#24034;&#32508;&#21512;&#24449;&#65288;PCOS&#65289;&#24739;&#32773;&#30340;&#26368;&#20339;&#33647;&#29289;&#26041;&#38754;&#24212;&#29992;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#26041;&#27861;&#12290;PCOS&#26159;&#19968;&#31181;&#24433;&#21709;&#20840;&#29699;&#25968;&#30334;&#19975;&#22899;&#24615;&#30340;&#20005;&#37325;&#28608;&#32032;&#22833;&#35843;&#30142;&#30149;&#65292;&#20294;&#20854;&#30740;&#31350;&#21463;&#38480;&#20110;&#24739;&#32773;&#25968;&#25454;&#30340;&#32570;&#20047;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21508;&#31181;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#22312;&#21512;&#25104;&#30340;PCOS&#24739;&#32773;&#25968;&#25454;&#38598;&#19978;&#30340;&#25104;&#21151;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#26159;&#19968;&#31181;&#35775;&#38382;&#22823;&#37327;&#22810;&#26679;&#25968;&#25454;&#24182;&#35782;&#21035;&#26368;&#26377;&#25928;&#27835;&#30103;&#36873;&#39033;&#30340;&#24037;&#20855;&#65292;&#21516;&#26102;&#25552;&#20379;PCOS&#24739;&#32773;&#30340;&#38544;&#31169;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of women's endocrinology has trailed behind data-driven medical solutions, largely due to concerns over the privacy of patient data. Valuable datapoints about hormone levels or menstrual cycling could expose patients who suffer from comorbidities or terminate a pregnancy, violating their privacy. We explore the application of Federated Learning (FL) to predict the optimal drug for patients with polycystic ovary syndrome (PCOS). PCOS is a serious hormonal disorder impacting millions of women worldwide, yet it's poorly understood and its research is stunted by a lack of patient data. We demonstrate that a variety of FL approaches succeed on a synthetic PCOS patient dataset. Our proposed FL models are a tool to access massive quantities of diverse data and identify the most effective treatment option while providing PCOS patients with privacy guarantees.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#31169;&#26377;&#39046;&#22495;&#25968;&#25454;&#21327;&#21516;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#36328;&#22330;&#26223;&#30340;&#26234;&#33021;&#26381;&#21153;&#12290;&#22312;&#22823;&#27169;&#22411;&#26102;&#20195;&#65292;&#35813;&#26694;&#26550;&#35299;&#20915;&#20102;&#24322;&#26500;&#25968;&#25454;&#12289;&#27169;&#22411;&#32858;&#21512;&#12289;&#24615;&#33021;&#21644;&#25104;&#26412;&#26435;&#34913;&#12289;&#25968;&#25454;&#38544;&#31169;&#20197;&#21450;&#28608;&#21169;&#26426;&#21046;&#31561;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.11217</link><description>&lt;p&gt;
&#22823;&#27169;&#22411;&#26102;&#20195;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#65306;&#38024;&#23545;&#29305;&#23450;&#39046;&#22495;&#30340;&#22810;&#27169;&#24577;&#22823;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Federated Learning in Big Model Era: Domain-Specific Multimodal Large Models. (arXiv:2308.11217v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#31169;&#26377;&#39046;&#22495;&#25968;&#25454;&#21327;&#21516;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#36328;&#22330;&#26223;&#30340;&#26234;&#33021;&#26381;&#21153;&#12290;&#22312;&#22823;&#27169;&#22411;&#26102;&#20195;&#65292;&#35813;&#26694;&#26550;&#35299;&#20915;&#20102;&#24322;&#26500;&#25968;&#25454;&#12289;&#27169;&#22411;&#32858;&#21512;&#12289;&#24615;&#33021;&#21644;&#25104;&#26412;&#26435;&#34913;&#12289;&#25968;&#25454;&#38544;&#31169;&#20197;&#21450;&#28608;&#21169;&#26426;&#21046;&#31561;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#25968;&#25454;&#33021;&#22815;&#20840;&#38754;&#24863;&#30693;&#21644;&#35782;&#21035;&#29289;&#29702;&#19990;&#30028;&#65292;&#24050;&#25104;&#20026;&#36890;&#24448;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#37325;&#35201;&#36335;&#24452;&#12290;&#28982;&#32780;&#65292;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#22823;&#27169;&#22411;&#22312;&#29305;&#23450;&#24037;&#19994;&#39046;&#22495;&#30340;&#24615;&#33021;&#24448;&#24448;&#19981;&#29702;&#24819;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#20351;&#22810;&#20010;&#20225;&#19994;&#21033;&#29992;&#31169;&#26377;&#39046;&#22495;&#25968;&#25454;&#21327;&#21516;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#65292;&#23454;&#29616;&#36328;&#22330;&#26223;&#30340;&#26234;&#33021;&#26381;&#21153;&#12290;&#20316;&#32773;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#27169;&#22411;&#26102;&#20195;&#32852;&#37030;&#23398;&#20064;&#30340;&#26234;&#33021;&#22522;&#30784;&#21644;&#30446;&#26631;&#30340;&#25112;&#30053;&#36716;&#21464;&#65292;&#20197;&#21450;&#22312;&#24322;&#26500;&#25968;&#25454;&#12289;&#27169;&#22411;&#32858;&#21512;&#12289;&#24615;&#33021;&#21644;&#25104;&#26412;&#26435;&#34913;&#12289;&#25968;&#25454;&#38544;&#31169;&#21644;&#28608;&#21169;&#26426;&#21046;&#26041;&#38754;&#38754;&#20020;&#30340;&#26032;&#25361;&#25112;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;&#39046;&#20808;&#20225;&#19994;&#22312;&#22478;&#24066;&#23433;&#20840;&#36816;&#33829;&#31649;&#29702;&#26041;&#38754;&#36129;&#29486;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#19987;&#23478;&#30693;&#35782;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#21253;&#25324;&#20998;&#24067;&#24335;&#37096;&#32626;&#21644;&#39640;&#25928;&#24615;&#33021;&#30340;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal data, which can comprehensively perceive and recognize the physical world, has become an essential path towards general artificial intelligence. However, multimodal large models trained on public datasets often underperform in specific industrial domains. This paper proposes a multimodal federated learning framework that enables multiple enterprises to utilize private domain data to collaboratively train large models for vertical domains, achieving intelligent services across scenarios. The authors discuss in-depth the strategic transformation of federated learning in terms of intelligence foundation and objectives in the era of big model, as well as the new challenges faced in heterogeneous data, model aggregation, performance and cost trade-off, data privacy, and incentive mechanism. The paper elaborates a case study of leading enterprises contributing multimodal data and expert knowledge to city safety operation management , including distributed deployment and efficient 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ConcatPlexer&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35270;&#35273;&#35782;&#21035;&#20013;&#20351;&#29992;&#38468;&#21152;&#30340;Dim1&#25209;&#22788;&#29702;&#65288;&#21363;&#36830;&#25509;&#65289;&#26469;&#25552;&#39640;&#21534;&#21520;&#37327;&#65292;&#21516;&#26102;&#20934;&#30830;&#24615;&#21463;&#21040;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;</title><link>http://arxiv.org/abs/2308.11199</link><description>&lt;p&gt;
ConcatPlexer&#65306;&#36890;&#36807;&#38468;&#21152;Dim1&#25209;&#22788;&#29702;&#20197;&#21152;&#24555;ViTs&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;
ConcatPlexer: Additional Dim1 Batching for Faster ViTs. (arXiv:2308.11199v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11199
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ConcatPlexer&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35270;&#35273;&#35782;&#21035;&#20013;&#20351;&#29992;&#38468;&#21152;&#30340;Dim1&#25209;&#22788;&#29702;&#65288;&#21363;&#36830;&#25509;&#65289;&#26469;&#25552;&#39640;&#21534;&#21520;&#37327;&#65292;&#21516;&#26102;&#20934;&#30830;&#24615;&#21463;&#21040;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#19981;&#20165;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#36824;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#24341;&#21457;&#20102;&#21508;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#21644;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;Transformer&#21331;&#36234;&#30340;&#24615;&#33021;&#21644;&#24314;&#27169;&#28789;&#27963;&#24615;&#24102;&#26469;&#20102;&#35745;&#31639;&#25104;&#26412;&#30340;&#20005;&#37325;&#22686;&#21152;&#65292;&#22240;&#27492;&#26377;&#20960;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#20943;&#23569;&#36825;&#31181;&#36127;&#25285;&#30340;&#26041;&#27861;&#12290;&#21463;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#31181;&#20943;&#23569;&#25104;&#26412;&#30340;&#26041;&#27861;Data Multiplexing (DataMUX)&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#35270;&#35273;&#35782;&#21035;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#37319;&#29992;&#20102;&#38468;&#21152;&#30340;Dim1&#25209;&#22788;&#29702;&#65288;&#21363;&#36830;&#25509;&#65289;&#65292;&#22312;&#20445;&#35777;&#20934;&#30830;&#24615;&#30340;&#22522;&#30784;&#19978;&#22823;&#22823;&#25552;&#39640;&#20102;&#21534;&#21520;&#37327;&#12290;&#25105;&#20204;&#39318;&#20808;&#20026;&#35270;&#35273;&#27169;&#22411;&#24341;&#20837;&#20102;DataMux&#30340;&#19968;&#31181;&#22825;&#28982;&#36866;&#24212;&#26041;&#27861;&#65292;&#22270;&#20687;&#22810;&#36335;&#22797;&#29992;&#22120;&#65288;Image Multiplexer&#65289;&#65292;&#24182;&#35774;&#35745;&#20102;&#26032;&#30340;&#32452;&#20214;&#26469;&#20811;&#26381;&#20854;&#32570;&#28857;&#65292;&#36827;&#32780;&#24418;&#25104;&#20102;&#25105;&#20204;&#26368;&#32456;&#30340;&#27169;&#22411;ConcatPlexer&#65292;&#22312;&#25512;&#29702;&#36895;&#24230;&#21644;&#20934;&#30830;&#24230;&#20043;&#38388;&#25214;&#21040;&#20102;&#24179;&#34913;&#28857;&#12290;ConcatPlexer&#22312;ImageNet1K&#21644;CIFAR100&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have demonstrated tremendous success not only in the natural language processing (NLP) domain but also the field of computer vision, igniting various creative approaches and applications. Yet, the superior performance and modeling flexibility of transformers came with a severe increase in computation costs, and hence several works have proposed methods to reduce this burden. Inspired by a cost-cutting method originally proposed for language models, Data Multiplexing (DataMUX), we propose a novel approach for efficient visual recognition that employs additional dim1 batching (i.e., concatenation) that greatly improves the throughput with little compromise in the accuracy. We first introduce a naive adaptation of DataMux for vision models, Image Multiplexer, and devise novel components to overcome its weaknesses, rendering our final model, ConcatPlexer, at the sweet spot between inference speed and accuracy. The ConcatPlexer was trained on ImageNet1K and CIFAR100 dataset and
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#31995;&#32479;&#35780;&#20272;&#34920;&#26126;&#65292;&#24403;&#35757;&#32451;&#38598;&#30340;&#37197;&#23545;&#22797;&#26434;&#24615;&#22686;&#21152;&#26102;&#65292;&#26631;&#20934;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#22270;&#20687;&#21306;&#22495;&#19982;&#25991;&#26412;&#23646;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#26102;&#34920;&#29616;&#36739;&#24046;&#65292;&#24615;&#33021;&#19979;&#38477;&#36798;&#21040;</title><link>http://arxiv.org/abs/2308.11194</link><description>&lt;p&gt;
ViLLA:&#20174;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#36827;&#34892;&#32454;&#31890;&#24230;&#30340;&#35270;&#35273;-&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ViLLA: Fine-Grained Vision-Language Representation Learning from Real-World Data. (arXiv:2308.11194v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11194
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#31995;&#32479;&#35780;&#20272;&#34920;&#26126;&#65292;&#24403;&#35757;&#32451;&#38598;&#30340;&#37197;&#23545;&#22797;&#26434;&#24615;&#22686;&#21152;&#26102;&#65292;&#26631;&#20934;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#22270;&#20687;&#21306;&#22495;&#19982;&#25991;&#26412;&#23646;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#26102;&#34920;&#29616;&#36739;&#24046;&#65292;&#24615;&#33021;&#19979;&#38477;&#36798;&#21040;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#65292;&#22914;CLIP&#21644;ALIGN&#65292;&#36890;&#24120;&#26159;&#22312;&#20174;&#32593;&#32476;&#33719;&#21462;&#30340;&#22270;&#20687;-&#26631;&#39064;&#23545;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#28982;&#32780;&#65292;&#30495;&#23454;&#19990;&#30028;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#22914;&#21307;&#30103;&#25968;&#25454;&#65292;&#26174;&#33879;&#26356;&#21152;&#22797;&#26434;&#65306;&#27599;&#20010;&#22270;&#20687;&#65288;&#22914;X&#20809;&#65289;&#36890;&#24120;&#19982;&#25551;&#36848;&#22270;&#20687;&#32454;&#31890;&#24230;&#21306;&#22495;&#20013;&#21457;&#29983;&#30340;&#35768;&#22810;&#19981;&#21516;&#23646;&#24615;&#30340;&#25991;&#26412;&#65288;&#22914;&#21307;&#29983;&#25253;&#21578;&#65289;&#37197;&#23545;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26679;&#26412;&#31216;&#20026;&#23637;&#31034;&#39640;&#37197;&#23545;&#22797;&#26434;&#24615;&#65292;&#22240;&#20026;&#27599;&#20010;&#22270;&#20687;-&#25991;&#26412;&#23545;&#21487;&#20197;&#20998;&#35299;&#20026;&#22823;&#37327;&#30340;&#21306;&#22495;-&#23646;&#24615;&#37197;&#23545;&#12290;&#20197;&#24448;&#23578;&#26410;&#35780;&#20272;VLMs&#22312;&#35757;&#32451;&#36825;&#31181;&#25968;&#25454;&#26102;&#33021;&#21542;&#25429;&#25417;&#21040;&#22270;&#20687;&#21306;&#22495;&#19982;&#25991;&#26412;&#23646;&#24615;&#20043;&#38388;&#30340;&#32454;&#31890;&#24230;&#20851;&#31995;&#12290;&#27492;&#24037;&#20316;&#30340;&#31532;&#19968;&#20010;&#20851;&#38190;&#36129;&#29486;&#26159;&#36890;&#36807;&#31995;&#32479;&#35780;&#20272;&#34920;&#26126;&#65292;&#38543;&#30528;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#37197;&#23545;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#26631;&#20934;&#30340;VLMs&#22312;&#23398;&#20064;&#21306;&#22495;-&#23646;&#24615;&#20851;&#31995;&#26041;&#38754;&#38754;&#20020;&#22256;&#38590;&#65292;&#24615;&#33021;&#19979;&#38477;&#36798;&#21040;
&lt;/p&gt;
&lt;p&gt;
Vision-language models (VLMs), such as CLIP and ALIGN, are generally trained on datasets consisting of image-caption pairs obtained from the web. However, real-world multimodal datasets, such as healthcare data, are significantly more complex: each image (e.g. X-ray) is often paired with text (e.g. physician report) that describes many distinct attributes occurring in fine-grained regions of the image. We refer to these samples as exhibiting high pairwise complexity, since each image-text pair can be decomposed into a large number of region-attribute pairings. The extent to which VLMs can capture fine-grained relationships between image regions and textual attributes when trained on such data has not been previously evaluated. The first key contribution of this work is to demonstrate through systematic evaluations that as the pairwise complexity of the training dataset increases, standard VLMs struggle to learn region-attribute relationships, exhibiting performance degradations of up t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22238;&#24212;&#22810;&#26679;&#24615;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38169;&#35823;&#37327;&#21270;&#25351;&#26631;&#65292;&#36825;&#20123;&#25351;&#26631;&#29420;&#31435;&#20110;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#65292;&#24182;&#19982;&#22833;&#36133;&#27010;&#29575;&#24378;&#30456;&#20851;&#12290;&#23454;&#35777;&#32467;&#26524;&#23637;&#31034;&#20102;&#36825;&#20123;&#25351;&#26631;&#22312;&#23569;&#26679;&#26412;&#25552;&#31034;&#12289;&#24605;&#32500;&#38142;&#25512;&#29702;&#21644;&#38169;&#35823;&#26816;&#27979;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.11189</link><description>&lt;p&gt;
&#22810;&#26679;&#24615;&#25351;&#26631;&#65306;&#35821;&#35328;&#27169;&#22411;&#26597;&#35810;&#20013;&#22833;&#36133;&#30340;&#39046;&#22495;&#26080;&#20851;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Diversity Measures: Domain-Independent Proxies for Failure in Language Model Queries. (arXiv:2308.11189v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22238;&#24212;&#22810;&#26679;&#24615;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38169;&#35823;&#37327;&#21270;&#25351;&#26631;&#65292;&#36825;&#20123;&#25351;&#26631;&#29420;&#31435;&#20110;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#65292;&#24182;&#19982;&#22833;&#36133;&#27010;&#29575;&#24378;&#30456;&#20851;&#12290;&#23454;&#35777;&#32467;&#26524;&#23637;&#31034;&#20102;&#36825;&#20123;&#25351;&#26631;&#22312;&#23569;&#26679;&#26412;&#25552;&#31034;&#12289;&#24605;&#32500;&#38142;&#25512;&#29702;&#21644;&#38169;&#35823;&#26816;&#27979;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38169;&#35823;&#39044;&#27979;&#36890;&#24120;&#20381;&#36182;&#20110;&#39046;&#22495;&#29305;&#23450;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22238;&#24212;&#22810;&#26679;&#24615;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38169;&#35823;&#37327;&#21270;&#25351;&#26631;&#65292;&#22240;&#27492;&#29420;&#31435;&#20110;&#24213;&#23618;&#24212;&#29992;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#22914;&#20309;&#20351;&#29992;&#22522;&#20110;&#29109;&#12289;&#22522;&#23612;&#19981;&#32431;&#24230;&#21644;&#36136;&#24515;&#36317;&#31163;&#30340;&#19977;&#20010;&#25351;&#26631;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#30340;&#23454;&#39564;&#65292;&#28041;&#21450;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#28201;&#24230;&#35774;&#32622;&#65292;&#35777;&#26126;&#36825;&#20123;&#25351;&#26631;&#19982;&#22833;&#36133;&#27010;&#29575;&#24378;&#30456;&#20851;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#23454;&#35777;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#36825;&#20123;&#25351;&#26631;&#24212;&#29992;&#20110;&#23569;&#26679;&#26412;&#25552;&#31034;&#12289;&#24605;&#32500;&#38142;&#25512;&#29702;&#21644;&#38169;&#35823;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Error prediction in large language models often relies on domain-specific information. In this paper, we present measures for quantification of error in the response of a large language model based on the diversity of responses to a given prompt - hence independent of the underlying application. We describe how three such measures - based on entropy, Gini impurity, and centroid distance can be employed. We perform a suite of experiments on multiple datasets and temperature settings to demonstrate that these measures strongly correlate with the probability of failure. Additionally, we present empirical results demonstrating how these measures can be applied to few-shot prompting, chain-of-thought reasoning, and error detection.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MISSRec&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#21644;&#36716;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#28508;&#21147;&#65292;&#35299;&#20915;&#20102;&#24207;&#21015;&#25512;&#33616;&#20013;&#30340;&#31232;&#30095;ID&#21644;&#20919;&#21551;&#21160;&#38382;&#39064;&#65292;&#24182;&#25552;&#21319;&#20102;&#25512;&#33616;&#27169;&#22411;&#30340;&#21487;&#36716;&#31227;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11175</link><description>&lt;p&gt;
MISSRec: &#38754;&#21521;&#25512;&#33616;&#30340;&#39044;&#35757;&#32451;&#21644;&#36716;&#31227;&#22810;&#27169;&#24577;&#20852;&#36259;&#24863;&#30693;&#24207;&#21015;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
MISSRec: Pre-training and Transferring Multi-modal Interest-aware Sequence Representation for Recommendation. (arXiv:2308.11175v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MISSRec&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#21644;&#36716;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#28508;&#21147;&#65292;&#35299;&#20915;&#20102;&#24207;&#21015;&#25512;&#33616;&#20013;&#30340;&#31232;&#30095;ID&#21644;&#20919;&#21551;&#21160;&#38382;&#39064;&#65292;&#24182;&#25552;&#21319;&#20102;&#25512;&#33616;&#27169;&#22411;&#30340;&#21487;&#36716;&#31227;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#25512;&#33616;&#30340;&#30446;&#26631;&#26159;&#22522;&#20110;&#29992;&#25143;&#30340;&#21382;&#21490;&#20132;&#20114;&#24207;&#21015;&#39044;&#27979;&#20854;&#21487;&#33021;&#24863;&#20852;&#36259;&#30340;&#29289;&#21697;&#12290;&#22823;&#37096;&#20998;&#29616;&#26377;&#30340;&#24207;&#21015;&#25512;&#33616;&#22120;&#26159;&#22522;&#20110;ID&#29305;&#24449;&#24320;&#21457;&#30340;&#65292;&#28982;&#32780;&#22312;&#20351;&#29992;&#31232;&#30095;ID&#26102;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#65292;&#24182;&#19988;&#22312;&#20919;&#21551;&#21160;&#38382;&#39064;&#19978;&#36935;&#21040;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#19981;&#19968;&#33268;&#30340;ID&#26144;&#23556;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#20351;&#24471;&#30456;&#20284;&#30340;&#25512;&#33616;&#39046;&#22495;&#26080;&#27861;&#36827;&#34892;&#20849;&#21516;&#20248;&#21270;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25506;&#32034;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#28508;&#21147;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;MISSRec&#65292;&#19968;&#31181;&#38754;&#21521;SR&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#21644;&#36716;&#31227;&#23398;&#20064;&#26694;&#26550;&#12290;&#22312;&#29992;&#25143;&#31471;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22522;&#20110;Transformer&#30340;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#65292;&#20854;&#20013;&#19978;&#19979;&#25991;&#32534;&#30721;&#22120;&#23398;&#20064;&#25429;&#25417;&#24207;&#21015;&#32423;&#30340;&#22810;&#27169;&#24577;&#21327;&#21516;&#20316;&#29992;&#65292;&#32780;&#26032;&#39062;&#30340;&#20852;&#36259;&#24863;&#30693;&#35299;&#30721;&#22120;&#21017;&#29992;&#20110;&#25226;&#25569;&#29289;&#21697;-&#27169;&#24577;-&#20852;&#36259;&#20851;&#31995;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24207;&#21015;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of sequential recommendation (SR) is to predict a user's potential interested items based on her/his historical interaction sequences. Most existing sequential recommenders are developed based on ID features, which, despite their widespread use, often underperform with sparse IDs and struggle with the cold-start problem. Besides, inconsistent ID mappings hinder the model's transferability, isolating similar recommendation domains that could have been co-optimized. This paper aims to address these issues by exploring the potential of multi-modal information in learning robust and generalizable sequence representations. We propose MISSRec, a multi-modal pre-training and transfer learning framework for SR. On the user side, we design a Transformer-based encoder-decoder model, where the contextual encoder learns to capture the sequence-level multi-modal synergy while a novel interest-aware decoder is developed to grasp item-modality-interest relations for better sequence represent
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#22522;&#20110;&#28857;&#30340;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#27979;&#37327;&#27599;&#20010;&#28857;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#36827;&#34892;&#26631;&#27880;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#20887;&#20313;&#20449;&#24687;&#21644;&#21306;&#22495;&#21010;&#20998;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.11166</link><description>&lt;p&gt;
&#29992;&#20110;&#21322;&#30417;&#30563;&#28857;&#20113;&#35821;&#20041;&#20998;&#21106;&#30340;&#20998;&#23618;&#22522;&#20110;&#28857;&#30340;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Point-based Active Learning for Semi-supervised Point Cloud Semantic Segmentation. (arXiv:2308.11166v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#22522;&#20110;&#28857;&#30340;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#27979;&#37327;&#27599;&#20010;&#28857;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#36827;&#34892;&#26631;&#27880;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#20887;&#20313;&#20449;&#24687;&#21644;&#21306;&#22495;&#21010;&#20998;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#22823;&#37327;&#26631;&#27880;&#25968;&#25454;&#36827;&#34892;&#20840;&#30417;&#30563;&#30340;&#26041;&#27861;&#22312;&#28857;&#20113;&#35821;&#20041;&#20998;&#21106;&#26041;&#38754;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#20855;&#26377;&#28857;&#32423;&#26631;&#31614;&#30340;&#22823;&#35268;&#27169;&#28857;&#20113;&#25968;&#25454;&#26159;&#19968;&#39033;&#36153;&#21147;&#30340;&#24037;&#20316;&#65292;&#22240;&#27492;&#35768;&#22810;&#23581;&#35797;&#33268;&#21147;&#20110;&#25506;&#32034;&#20165;&#20351;&#29992;&#26377;&#38480;&#27880;&#37322;&#36827;&#34892;&#23398;&#20064;&#19977;&#32500;&#28857;&#20113;&#20998;&#21106;&#30340;&#26041;&#27861;&#12290;&#20027;&#21160;&#23398;&#20064;&#26159;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#26377;&#25928;&#31574;&#30053;&#20043;&#19968;&#65292;&#20294;&#30446;&#21069;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#26041;&#27861;&#36890;&#36807;&#27979;&#37327;&#27599;&#20010;&#39044;&#20998;&#21106;&#21306;&#22495;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#36827;&#34892;&#25163;&#21160;&#26631;&#27880;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#20887;&#20313;&#20449;&#24687;&#65292;&#24182;&#19988;&#38656;&#35201;&#39069;&#22806;&#30340;&#21162;&#21147;&#36827;&#34892;&#21306;&#22495;&#21010;&#20998;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#20998;&#23618;&#22522;&#20110;&#28857;&#30340;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#32771;&#34385;&#22810;&#20010;&#23618;&#27425;&#19978;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#20998;&#23618;&#26368;&#23567;&#36793;&#32536;&#19981;&#30830;&#23450;&#24615;&#27169;&#22359;&#26469;&#27979;&#37327;&#27599;&#20010;&#28857;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#21518;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#29305;&#24449;&#36317;&#31163;&#25233;&#21046;&#31574;&#30053;&#65292;&#36873;&#25321;&#37325;&#35201;&#30340;&#28857;&#36827;&#34892;&#26631;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
Impressive performance on point cloud semantic segmentation has been achieved by fully-supervised methods with large amounts of labelled data. As it is labour-intensive to acquire large-scale point cloud data with point-wise labels, many attempts have been made to explore learning 3D point cloud segmentation with limited annotations. Active learning is one of the effective strategies to achieve this purpose but is still under-explored. The most recent methods of this kind measure the uncertainty of each pre-divided region for manual labelling but they suffer from redundant information and require additional efforts for region division. This paper aims at addressing this issue by developing a hierarchical point-based active learning strategy. Specifically, we measure the uncertainty for each point by a hierarchical minimum margin uncertainty module which considers the contextual information at multiple levels. Then, a feature-distance suppression strategy is designed to select important
&lt;/p&gt;</description></item><item><title>&#22312;&#31070;&#32463;&#21147;&#22330;&#27169;&#22411;&#20013;&#65292;&#24120;&#29992;&#30340;MD17&#25968;&#25454;&#38598;&#23545;&#20110;&#34920;&#31034;&#32463;&#21382;&#21270;&#23398;&#21453;&#24212;&#30340;&#31995;&#32479;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;xxMD&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#37319;&#26679;&#33258;&#25193;&#23637;&#28608;&#21457;&#24577;&#20998;&#23376;&#21160;&#21147;&#23398;&#65292;&#21253;&#21547;&#20102;&#33021;&#37327;&#21644;&#21147;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.11155</link><description>&lt;p&gt;
&#36890;&#36807;&#36229;&#20986;&#24179;&#34913;&#29366;&#24577;&#30340;&#25193;&#23637;&#21160;&#21147;&#23398;&#24615;&#33021;&#35780;&#20272;&#31070;&#32463;&#21147;&#22330;
&lt;/p&gt;
&lt;p&gt;
xxMD: Benchmarking Neural Force Fields Using Extended Dynamics beyond Equilibrium. (arXiv:2308.11155v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11155
&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#21147;&#22330;&#27169;&#22411;&#20013;&#65292;&#24120;&#29992;&#30340;MD17&#25968;&#25454;&#38598;&#23545;&#20110;&#34920;&#31034;&#32463;&#21382;&#21270;&#23398;&#21453;&#24212;&#30340;&#31995;&#32479;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;xxMD&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#37319;&#26679;&#33258;&#25193;&#23637;&#28608;&#21457;&#24577;&#20998;&#23376;&#21160;&#21147;&#23398;&#65292;&#21253;&#21547;&#20102;&#33021;&#37327;&#21644;&#21147;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#21147;&#22330;&#24050;&#25104;&#20026;&#35745;&#31639;&#21270;&#23398;&#20013;&#30340;&#37325;&#35201;&#27169;&#22411;&#65292;&#21462;&#20195;&#20102;&#20174;&#22836;&#31639;&#30340;&#20998;&#23376;&#21160;&#21147;&#23398;&#20013;&#30340;&#37327;&#23376;&#21270;&#23398;&#35745;&#31639;&#12290;&#30446;&#21069;&#23545;&#31070;&#32463;&#21147;&#22330;&#30340;&#20027;&#35201;&#35780;&#20272;&#22522;&#20934;&#26159;MD17&#25968;&#25454;&#38598;&#21450;&#20854;&#21518;&#32493;&#25193;&#23637;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#20027;&#35201;&#21253;&#21547;&#26469;&#33258;&#22522;&#24577;&#21183;&#33021;&#38754;&#24179;&#34913;&#21306;&#22495;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#37319;&#26679;&#33258;&#30452;&#25509;&#32477;&#28909;&#21160;&#21147;&#23398;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#21270;&#23398;&#21453;&#24212;&#28041;&#21450;&#21040;&#36739;&#22823;&#30340;&#20998;&#23376;&#21464;&#24418;&#65292;&#29305;&#21035;&#26159;&#38190;&#26029;&#35010;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;MD17&#25968;&#25454;&#38598;&#20013;&#20869;&#22352;&#26631;&#21644;&#33021;&#37327;&#30340;&#32422;&#26463;&#20998;&#24067;&#65292;&#20984;&#26174;&#20102;&#20854;&#22312;&#34920;&#31034;&#32463;&#21382;&#21270;&#23398;&#21453;&#24212;&#30340;&#31995;&#32479;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#37319;&#26679;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;xxMD&#65288;&#25193;&#23637;&#28608;&#21457;&#24577;&#20998;&#23376;&#21160;&#21147;&#23398;&#65289;&#25968;&#25454;&#38598;&#65292;&#20174;&#38750;&#32477;&#28909;&#21160;&#21147;&#23398;&#20013;&#27966;&#29983;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#20174;&#22810;&#21442;&#32771;&#27874;&#20989;&#25968;&#29702;&#35770;&#21644;&#23494;&#24230;&#27867;&#20989;&#20013;&#30830;&#23450;&#30340;&#33021;&#37327;&#21644;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural force fields (NFFs) have gained prominence in computational chemistry as surrogate models, superseding quantum-chemistry calculations in ab initio molecular dynamics. The prevalent benchmark for NFFs has been the MD17 dataset and its subsequent extension. These datasets predominantly comprise geometries from the equilibrium region of the ground electronic state potential energy surface, sampling from direct adiabatic dynamics. However, many chemical reactions entail significant molecular deformations, notably bond breaking. We demonstrate the constrained distribution of internal coordinates and energies in the MD17 datasets, underscoring their inadequacy for representing systems undergoing chemical reactions. Addressing this sampling limitation, we introduce the xxMD (Extended Excited-state Molecular Dynamics) dataset, derived from non-adiabatic dynamics. This dataset encompasses energies and forces ascertained from both multireference wave function theory and density functional
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#32454;&#32990;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#20808;&#39564;&#33258;&#28608;&#27963;&#22270;&#29983;&#25104;&#20266;&#25513;&#33180;&#20316;&#20026;&#35757;&#32451;&#30446;&#26631;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#32454;&#32990;&#20998;&#21106;&#21644;&#22810;&#31867;&#21035;&#32454;&#32990;&#26816;&#27979;&#20219;&#21153;&#20013;&#20248;&#20110;&#20854;&#20182;&#30417;&#30563;&#21644;&#24369;&#30417;&#30563;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.11144</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#32454;&#32990;&#35782;&#21035;&#20013;&#30340;&#20808;&#39564;&#33258;&#28608;&#27963;&#22270;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Exploring Unsupervised Cell Recognition with Prior Self-activation Maps. (arXiv:2308.11144v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11144
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#32454;&#32990;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#20808;&#39564;&#33258;&#28608;&#27963;&#22270;&#29983;&#25104;&#20266;&#25513;&#33180;&#20316;&#20026;&#35757;&#32451;&#30446;&#26631;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#32454;&#32990;&#20998;&#21106;&#21644;&#22810;&#31867;&#21035;&#32454;&#32990;&#26816;&#27979;&#20219;&#21153;&#20013;&#20248;&#20110;&#20854;&#20182;&#30417;&#30563;&#21644;&#24369;&#30417;&#30563;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#22312;&#32454;&#32990;&#35782;&#21035;&#20219;&#21153;&#19978;&#65292;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25104;&#21151;&#20381;&#36182;&#20110;&#35814;&#32454;&#30340;&#27880;&#37322;&#12290;&#35768;&#22810;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#25104;&#21151;&#20943;&#23569;&#20102;&#23545;&#26631;&#31614;&#30340;&#20381;&#36182;&#24615;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;&#19968;&#20010;&#34917;&#19969;&#20013;&#21253;&#21547;&#30340;&#22823;&#37327;&#32454;&#32990;&#65292;&#26114;&#36149;&#32780;&#20302;&#25928;&#30340;&#26631;&#27880;&#20173;&#28982;&#19981;&#21487;&#36991;&#20813;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#26080;&#26631;&#31614;&#26041;&#27861;&#26469;&#36827;&#34892;&#32454;&#32990;&#35782;&#21035;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20808;&#39564;&#33258;&#28608;&#27963;&#22270;&#65288;PSM&#65289;&#26469;&#29983;&#25104;&#20266;&#25513;&#33180;&#20316;&#20026;&#35757;&#32451;&#30446;&#26631;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#26469;&#35757;&#32451;&#19968;&#20010;&#28608;&#27963;&#32593;&#32476;&#12290;&#32593;&#32476;&#30340;&#27973;&#23618;&#20013;&#30340;&#26799;&#24230;&#20449;&#24687;&#34987;&#32858;&#21512;&#20197;&#29983;&#25104;&#20808;&#39564;&#33258;&#28608;&#27963;&#22270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#35821;&#20041;&#32858;&#31867;&#27169;&#22359;&#20316;&#20026;&#31649;&#36947;&#65292;&#23558;PSMs&#36716;&#25442;&#20026;&#20687;&#32032;&#32423;&#35821;&#20041;&#20266;&#25513;&#33180;&#65292;&#20197;&#20379;&#19979;&#28216;&#20219;&#21153;&#20351;&#29992;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#32452;&#32455;&#23398;&#25968;&#25454;&#38598;MoNuSeg&#65288;&#32454;&#32990;&#20998;&#21106;&#65289;&#21644;BCData&#65288;&#22810;&#31867;&#21035;&#32454;&#32990;&#26816;&#27979;&#65289;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#19982;&#20854;&#20182;&#20840;&#30417;&#30563;&#21644;&#24369;&#30417;&#30563;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of supervised deep learning models on cell recognition tasks relies on detailed annotations. Many previous works have managed to reduce the dependency on labels. However, considering the large number of cells contained in a patch, costly and inefficient labeling is still inevitable. To this end, we explored label-free methods for cell recognition. Prior self-activation maps (PSM) are proposed to generate pseudo masks as training targets. To be specific, an activation network is trained with self-supervised learning. The gradient information in the shallow layers of the network is aggregated to generate prior self-activation maps. Afterward, a semantic clustering module is then introduced as a pipeline to transform PSMs to pixel-level semantic pseudo masks for downstream tasks. We evaluated our method on two histological datasets: MoNuSeg (cell segmentation) and BCData (multi-class cell detection). Compared with other fully-supervised and weakly-supervised methods, our metho
&lt;/p&gt;</description></item><item><title>LLM&#22522;&#20110;&#26234;&#33021;&#20307;&#19981;&#20165;&#24212;&#20851;&#27880;&#8220;&#20197;&#20154;&#20026;&#20013;&#24515;&#8221;&#30340;&#23545;&#40784;&#25110;&#24212;&#29992;&#65292;&#36824;&#24212;&#20851;&#27880;&#26234;&#33021;&#20307;&#33258;&#36523;&#65292;&#24182;&#25506;&#35752;&#31038;&#20250;&#31185;&#23398;&#22312;&#26234;&#33021;&#20307;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.11136</link><description>&lt;p&gt;
LLM&#22522;&#20110;&#26234;&#33021;&#20307;&#26159;&#21542;&#23384;&#22312;&#31038;&#20250;&#21407;&#21017;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is There Any Social Principle for LLM-Based Agents?. (arXiv:2308.11136v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11136
&lt;/p&gt;
&lt;p&gt;
LLM&#22522;&#20110;&#26234;&#33021;&#20307;&#19981;&#20165;&#24212;&#20851;&#27880;&#8220;&#20197;&#20154;&#20026;&#20013;&#24515;&#8221;&#30340;&#23545;&#40784;&#25110;&#24212;&#29992;&#65292;&#36824;&#24212;&#20851;&#27880;&#26234;&#33021;&#20307;&#33258;&#36523;&#65292;&#24182;&#25506;&#35752;&#31038;&#20250;&#31185;&#23398;&#22312;&#26234;&#33021;&#20307;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#27880;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20307;&#24212;&#35813;&#36229;&#36234;&#8220;&#20197;&#20154;&#20026;&#20013;&#24515;&#8221;&#30340;&#23545;&#40784;&#25110;&#24212;&#29992;&#12290;&#25105;&#20204;&#35748;&#20026;&#24212;&#35813;&#26356;&#22810;&#20851;&#27880;&#26234;&#33021;&#20307;&#26412;&#36523;&#65292;&#24182;&#25506;&#35752;&#31038;&#20250;&#31185;&#23398;&#22312;&#26234;&#33021;&#20307;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Focus on Large Language Model based agents should involve more than "human-centered" alignment or application. We argue that more attention should be paid to the agent itself and discuss the potential of social sciences for agents.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReLLa&#30340;&#26816;&#32034;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#21644;&#23567;&#26679;&#26412;&#25512;&#33616;&#20219;&#21153;&#12290;&#36890;&#36807;&#35821;&#20041;&#29992;&#25143;&#34892;&#20026;&#26816;&#32034;&#65288;SUBR&#65289;&#26469;&#25552;&#21462;&#19978;&#19979;&#25991;&#20013;&#30340;&#26377;&#29992;&#20449;&#24687;&#65292;&#20197;&#25913;&#21892;LLMs&#30340;&#25512;&#33616;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11131</link><description>&lt;p&gt;
ReLLa: &#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#29983;&#21629;&#21608;&#26399;&#24207;&#21015;&#34892;&#20026;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation. (arXiv:2308.11131v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReLLa&#30340;&#26816;&#32034;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#21644;&#23567;&#26679;&#26412;&#25512;&#33616;&#20219;&#21153;&#12290;&#36890;&#36807;&#35821;&#20041;&#29992;&#25143;&#34892;&#20026;&#26816;&#32034;&#65288;SUBR&#65289;&#26469;&#25552;&#21462;&#19978;&#19979;&#25991;&#20013;&#30340;&#26377;&#29992;&#20449;&#24687;&#65292;&#20197;&#25913;&#21892;LLMs&#30340;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#31361;&#30772;&#65292;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#31995;&#32479;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#24182;&#34987;&#31215;&#26497;&#25506;&#32034;&#12290;&#26412;&#25991;&#19987;&#27880;&#20110;&#36866;&#24212;&#21644;&#22686;&#24378;&#32431;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#29992;&#20110;&#38646;&#26679;&#26412;&#21644;&#23567;&#26679;&#26412;&#25512;&#33616;&#20219;&#21153;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#38024;&#23545;&#25512;&#33616;&#39046;&#22495;&#20013;LLMs&#26080;&#27861;&#20174;&#38271;&#29992;&#25143;&#34892;&#20026;&#24207;&#21015;&#30340;&#25991;&#26412;&#19978;&#19979;&#25991;&#20013;&#25552;&#21462;&#26377;&#29992;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#24182;&#23450;&#20041;&#20102;&#29983;&#21629;&#21608;&#26399;&#24207;&#21015;&#34892;&#20026;&#29702;&#35299;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#24182;&#25552;&#39640;LLMs&#30340;&#25512;&#33616;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;ReLLa&#65289;&#12290;&#38024;&#23545;&#38646;&#26679;&#26412;&#25512;&#33616;&#65292;&#25105;&#20204;&#25191;&#34892;&#35821;&#20041;&#29992;&#25143;&#34892;&#20026;&#26816;&#32034;&#65288;SUBR&#65289;&#26469;&#25552;&#39640;&#25968;&#25454;&#30340;&#21033;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
With large language models (LLMs) achieving remarkable breakthroughs in natural language processing (NLP) domains, LLM-enhanced recommender systems have received much attention and have been actively explored currently. In this paper, we focus on adapting and empowering a pure large language model for zero-shot and few-shot recommendation tasks. First and foremost, we identify and formulate the lifelong sequential behavior incomprehension problem for LLMs in recommendation domains, i.e., LLMs fail to extract useful information from a textual context of long user behavior sequence, even if the length of context is far from reaching the context limitation of LLMs. To address such an issue and improve the recommendation performance of LLMs, we propose a novel framework, namely Retrieval-enhanced Large Language models (ReLLa) for recommendation tasks in both zero-shot and few-shot settings. For zero-shot recommendation, we perform semantic user behavior retrieval (SUBR) to improve the data
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#27425;&#36317;&#31163;&#32467;&#26500;&#32534;&#30721;&#65288;HDSE&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#25417;&#22810;&#23618;&#27425;&#22270;&#32467;&#26500;&#12290;&#32463;&#36807;&#22312;12&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;10&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#25928;&#26524;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2308.11129</link><description>&lt;p&gt;
&#20351;&#29992;&#23618;&#27425;&#32467;&#26500;&#36317;&#31163;&#25429;&#25417;&#22810;&#23618;&#27425;&#22270;&#32467;&#26500;&#30340;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Transformers for Capturing Multi-level Graph Structure using Hierarchical Distances. (arXiv:2308.11129v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#27425;&#36317;&#31163;&#32467;&#26500;&#32534;&#30721;&#65288;HDSE&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#25417;&#22810;&#23618;&#27425;&#22270;&#32467;&#26500;&#12290;&#32463;&#36807;&#22312;12&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;10&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#25928;&#26524;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21464;&#21387;&#22120;&#38656;&#35201;&#24378;&#22823;&#30340;&#24402;&#32435;&#20559;&#24046;&#26469;&#24471;&#20986;&#26377;&#24847;&#20041;&#30340;&#27880;&#24847;&#21147;&#20998;&#25968;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#25552;&#35758;&#24456;&#23569;&#28041;&#21450;&#25429;&#25417;&#26356;&#38271;&#36317;&#31163;&#12289;&#23618;&#27425;&#32467;&#26500;&#25110;&#31038;&#21306;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#32780;&#36825;&#20123;&#22312;&#20998;&#23376;&#12289;&#31038;&#20132;&#32593;&#32476;&#21644;&#24341;&#29992;&#32593;&#32476;&#31561;&#21508;&#31181;&#22270;&#24418;&#20013;&#37117;&#20250;&#20986;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#27425;&#36317;&#31163;&#32467;&#26500;&#32534;&#30721;&#65288;HDSE&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#24314;&#27169;&#22270;&#20013;&#33410;&#28857;&#20043;&#38388;&#30340;&#23618;&#27425;&#36317;&#31163;&#65292;&#37325;&#28857;&#20851;&#27880;&#20854;&#22810;&#23618;&#27425;&#12289;&#23618;&#27425;&#21270;&#30340;&#24615;&#36136;&#12290;&#29305;&#21035;&#26159;&#65292;&#36825;&#20135;&#29983;&#20102;&#19968;&#20010;&#21487;&#20197;&#28789;&#27963;&#19982;&#29616;&#26377;&#22270;&#21464;&#21387;&#22120;&#38598;&#25104;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#19982;&#20854;&#20182;&#20301;&#32622;&#34920;&#31034;&#21516;&#26102;&#24212;&#29992;&#12290;&#36890;&#36807;&#22312;12&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;HDSE&#26041;&#27861;&#25104;&#21151;&#25552;&#21319;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#22522;&#32447;&#21464;&#21387;&#22120;&#65292;&#22312;10&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#23454;&#35777;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph transformers need strong inductive biases to derive meaningful attention scores. Yet, current proposals rarely address methods capturing longer ranges, hierarchical structures, or community structures, as they appear in various graphs such as molecules, social networks, and citation networks. In this paper, we propose a hierarchy-distance structural encoding (HDSE), which models a hierarchical distance between the nodes in a graph focusing on its multi-level, hierarchical nature. In particular, this yields a framework which can be flexibly integrated with existing graph transformers, allowing for simultaneous application with other positional representations. Through extensive experiments on 12 real-world datasets, we demonstrate that our HDSE method successfully enhances various types of baseline transformers, achieving state-of-the-art empirical performances on 10 benchmark datasets.
&lt;/p&gt;</description></item><item><title>CAME&#26159;&#19968;&#20010;&#19981;&#20381;&#36182;&#35757;&#32451;&#38598;&#30340;&#23545;&#27604;&#33258;&#21160;&#21270;&#27169;&#22411;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#39564;&#35777;&#65292;&#24314;&#31435;&#20102;&#27169;&#22411;&#24615;&#33021;&#19982;&#23545;&#27604;&#25439;&#22833;&#20043;&#38388;&#30340;&#21487;&#39044;&#27979;&#20851;&#31995;&#65292;&#24182;&#21462;&#24471;&#20102;&#26032;&#30340;SOTA&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.11111</link><description>&lt;p&gt;
CAME: &#23545;&#27604;&#33258;&#21160;&#21270;&#27169;&#22411;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
CAME: Contrastive Automated Model Evaluation. (arXiv:2308.11111v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11111
&lt;/p&gt;
&lt;p&gt;
CAME&#26159;&#19968;&#20010;&#19981;&#20381;&#36182;&#35757;&#32451;&#38598;&#30340;&#23545;&#27604;&#33258;&#21160;&#21270;&#27169;&#22411;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#39564;&#35777;&#65292;&#24314;&#31435;&#20102;&#27169;&#22411;&#24615;&#33021;&#19982;&#23545;&#27604;&#25439;&#22833;&#20043;&#38388;&#30340;&#21487;&#39044;&#27979;&#20851;&#31995;&#65292;&#24182;&#21462;&#24471;&#20102;&#26032;&#30340;SOTA&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#27169;&#22411;&#35780;&#20272;&#65288;AutoEval&#65289;&#26694;&#26550;&#25506;&#32034;&#20102;&#22312;&#27809;&#26377;&#26631;&#35760;&#30340;&#27979;&#35797;&#38598;&#30340;&#24773;&#20917;&#19979;&#35780;&#20272;&#35757;&#32451;&#22909;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#33021;&#24615;&#12290;&#23613;&#31649;&#26377;&#19968;&#20123;&#19981;&#38169;&#30340;&#32467;&#26524;&#21644;&#25215;&#35834;&#65292;&#20294;&#29616;&#26377;&#30340;AutoEval&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#35745;&#31639;&#26410;&#26631;&#35760;&#27979;&#35797;&#38598;&#19982;&#35757;&#32451;&#38598;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#23545;&#35757;&#32451;&#38598;&#30340;&#20381;&#36182;&#25104;&#20026;&#23558;&#36825;&#39033;&#25216;&#26415;&#24212;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#23398;&#20064;&#24320;&#21457;&#20013;&#30340;&#21478;&#19968;&#20010;&#38556;&#30861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;&#33258;&#21160;&#27169;&#22411;&#35780;&#20272;&#65288;CAME&#65289;&#19968;&#20010;&#26032;&#30340;AutoEval&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#19981;&#38656;&#35201;&#20381;&#36182;&#35757;&#32451;&#38598;&#12290;CAME&#30340;&#26680;&#24515;&#24605;&#24819;&#22522;&#20110;&#29702;&#35770;&#20998;&#26512;&#65292;&#23558;&#27169;&#22411;&#24615;&#33021;&#19982;&#23545;&#27604;&#25439;&#22833;&#30456;&#32852;&#31995;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22823;&#37327;&#30340;&#23454;&#35777;&#39564;&#35777;&#65292;&#25105;&#20204;&#25104;&#21151;&#24314;&#31435;&#20102;&#20004;&#32773;&#20043;&#38388;&#30340;&#21487;&#39044;&#27979;&#20851;&#31995;&#65292;&#21482;&#38656;&#22312;&#26410;&#26631;&#35760;/&#26410;&#35265;&#30340;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#25512;&#23548;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#26694;&#26550;CAME&#36890;&#36807;&#36229;&#36234;&#20197;&#21069;&#30340;&#24037;&#20316;&#24314;&#31435;&#20102;&#26032;&#30340;SOTA&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Automated Model Evaluation (AutoEval) framework entertains the possibility of evaluating a trained machine learning model without resorting to a labeled testing set. Despite the promise and some decent results, the existing AutoEval methods heavily rely on computing distribution shifts between the unlabelled testing set and the training set. We believe this reliance on the training set becomes another obstacle in shipping this technology to real-world ML development. In this work, we propose Contrastive Automatic Model Evaluation (CAME), a novel AutoEval framework that is rid of involving training set in the loop. The core idea of CAME bases on a theoretical analysis which bonds the model performance with a contrastive loss. Further, with extensive empirical validation, we manage to set up a predictable relationship between the two, simply by deducing on the unlabeled/unseen testing set. The resulting framework CAME establishes a new SOTA results for AutoEval by surpassing prior wo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#37325;&#26032;&#35782;&#21035;&#21311;&#21517;&#20010;&#20154;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#12289;&#36755;&#20837;&#38271;&#24230;&#21644;&#25351;&#20196;&#35843;&#25972;&#26159;&#26368;&#37325;&#35201;&#30340;&#20915;&#23450;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2308.11103</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20877;&#35782;&#21035;&#33021;&#21147;&#65306;&#21311;&#21517;&#38754;&#20020;&#39118;&#38505;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Anonymity at Risk? Assessing Re-Identification Capabilities of Large Language Models. (arXiv:2308.11103v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#37325;&#26032;&#35782;&#21035;&#21311;&#21517;&#20010;&#20154;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#12289;&#36755;&#20837;&#38271;&#24230;&#21644;&#25351;&#20196;&#35843;&#25972;&#26159;&#26368;&#37325;&#35201;&#30340;&#20915;&#23450;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27431;&#30431;&#21644;&#29790;&#22763;&#65292;&#27861;&#38498;&#35009;&#20915;&#20013;&#33258;&#28982;&#20154;&#21644;&#27861;&#20154;&#30340;&#21311;&#21517;&#24615;&#26159;&#38544;&#31169;&#20445;&#25252;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#23545;&#20110;&#21311;&#21517;&#20154;&#21592;&#30340;&#22823;&#35268;&#27169;&#20877;&#35782;&#21035;&#30340;&#25285;&#24551;&#26085;&#30410;&#22686;&#38271;&#12290;&#26681;&#25454;&#29790;&#22763;&#32852;&#37030;&#26368;&#39640;&#27861;&#38498;&#30340;&#35201;&#27714;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#29790;&#22763;&#32852;&#37030;&#26368;&#39640;&#27861;&#38498;&#30340;&#23454;&#38469;&#27861;&#24459;&#25968;&#25454;&#26500;&#24314;&#20102;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#65292;&#26469;&#25506;&#35752;LLMs&#37325;&#26032;&#35782;&#21035;&#27861;&#38498;&#35009;&#20915;&#20013;&#20010;&#20154;&#30340;&#28508;&#21147;&#12290;&#22312;&#26368;&#21021;&#30340;&#23454;&#39564;&#20043;&#21518;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#32463;&#36807;&#21311;&#21517;&#21270;&#22788;&#29702;&#30340;&#32500;&#22522;&#30334;&#31185;&#25968;&#25454;&#38598;&#65292;&#20316;&#20026;&#19968;&#20010;&#26356;&#20005;&#26684;&#30340;&#27979;&#35797;&#22330;&#22320;&#26469;&#36827;&#19968;&#27493;&#30740;&#31350;&#30740;&#31350;&#32467;&#26524;&#12290;&#36890;&#36807;&#24341;&#20837;&#24182;&#24212;&#29992;&#25991;&#26412;&#20013;&#20877;&#35782;&#21035;&#20154;&#21592;&#30340;&#26032;&#20219;&#21153;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#26032;&#30340;&#24615;&#33021;&#34913;&#37327;&#25351;&#26631;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#20998;&#26512;&#20102;&#24433;&#21709;&#25104;&#21151;&#20877;&#35782;&#21035;&#30340;&#22240;&#32032;&#65292;&#30830;&#23450;&#27169;&#22411;&#22823;&#23567;&#12289;&#36755;&#20837;&#38271;&#24230;&#21644;&#25351;&#20196;&#35843;&#25972;&#26159;&#26368;&#37325;&#35201;&#30340;&#20915;&#23450;&#22240;&#32032;&#20043;&#19968;&#12290;&#23613;&#31649;&#22312;&#21311;&#21517;&#21270;&#22788;&#29702;&#21518;&#65292;LLMs&#22312;&#37325;&#26032;&#35782;&#21035;&#19978;&#30340;&#25104;&#21151;&#29575;&#24456;&#39640;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20173;&#28982;&#23384;&#22312;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anonymity of both natural and legal persons in court rulings is a critical aspect of privacy protection in the European Union and Switzerland. With the advent of LLMs, concerns about large-scale re-identification of anonymized persons are growing. In accordance with the Federal Supreme Court of Switzerland, we explore the potential of LLMs to re-identify individuals in court rulings by constructing a proof-of-concept using actual legal data from the Swiss federal supreme court. Following the initial experiment, we constructed an anonymized Wikipedia dataset as a more rigorous testing ground to further investigate the findings. With the introduction and application of the new task of re-identifying people in texts, we also introduce new metrics to measure performance. We systematically analyze the factors that influence successful re-identifications, identifying model size, input length, and instruction tuning among the most critical determinants. Despite high re-identification rates on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#33258;&#21160;&#35843;&#21046;&#20998;&#31867;&#20013;&#20351;&#29992;&#26089;&#26399;&#36864;&#20986;&#25216;&#26415;&#21152;&#36895;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#26089;&#26399;&#36864;&#20986;&#26550;&#26500;&#65292;&#38024;&#23545;&#20449;&#22122;&#27604;&#36866;&#20013;&#33267;&#36739;&#39640;&#30340;&#20449;&#21495;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25512;&#29702;&#36895;&#24230;&#65292;&#32780;&#19981;&#25439;&#22833;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11100</link><description>&lt;p&gt;
&#22312;&#33258;&#21160;&#35843;&#21046;&#20998;&#31867;&#20013;&#20351;&#29992;&#26089;&#26399;&#36864;&#20986;&#20197;&#23454;&#29616;&#24555;&#36895;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Using Early Exits for Fast Inference in Automatic Modulation Classification. (arXiv:2308.11100v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11100
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#33258;&#21160;&#35843;&#21046;&#20998;&#31867;&#20013;&#20351;&#29992;&#26089;&#26399;&#36864;&#20986;&#25216;&#26415;&#21152;&#36895;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#26089;&#26399;&#36864;&#20986;&#26550;&#26500;&#65292;&#38024;&#23545;&#20449;&#22122;&#27604;&#36866;&#20013;&#33267;&#36739;&#39640;&#30340;&#20449;&#21495;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25512;&#29702;&#36895;&#24230;&#65292;&#32780;&#19981;&#25439;&#22833;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35843;&#21046;&#20998;&#31867;&#22312;&#26080;&#32447;&#36890;&#20449;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#36890;&#36807;&#33258;&#20027;&#22320;&#23545;&#26080;&#32447;&#39057;&#35889;&#19978;&#20256;&#36755;&#30340;&#20449;&#21495;&#36827;&#34892;&#20998;&#31867;&#12290;&#30001;&#20110;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#33021;&#22815;&#25552;&#21462;&#22797;&#26434;&#30340;&#26080;&#32447;&#20449;&#21495;&#29305;&#24449;&#65292;&#22240;&#27492;&#36234;&#26469;&#36234;&#22810;&#22320;&#23558;&#20854;&#29992;&#20110;&#33258;&#21160;&#35843;&#21046;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35745;&#31639;&#23494;&#38598;&#19988;&#25512;&#29702;&#24310;&#36831;&#36739;&#39640;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#29992;&#20110;&#33258;&#21160;&#35843;&#21046;&#20998;&#31867;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#24212;&#29992;&#26089;&#26399;&#36864;&#20986;&#25216;&#26415;&#20197;&#21152;&#36895;&#25512;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#22235;&#31181;&#26089;&#26399;&#36864;&#20986;&#26550;&#26500;&#21644;&#38024;&#23545;&#35813;&#38382;&#39064;&#30340;&#23450;&#21046;&#22810;&#20998;&#25903;&#35757;&#32451;&#31639;&#27861;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#34920;&#26126;&#22312;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#36866;&#20013;&#33267;&#36739;&#39640;&#30340;&#20449;&#21495;&#19978;&#26356;&#23481;&#26131;&#36827;&#34892;&#20998;&#31867;&#65292;&#19981;&#38656;&#35201;&#28145;&#24230;&#32467;&#26500;&#65292;&#22240;&#27492;&#21487;&#20197;&#21033;&#29992;&#25152;&#25552;&#20986;&#30340;&#26089;&#26399;&#36864;&#20986;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#26089;&#26399;&#36864;&#20986;&#25216;&#26415;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25512;&#29702;&#36895;&#24230;&#65292;&#21516;&#26102;&#19981;&#25439;&#22833;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic modulation classification (AMC) plays a critical role in wireless communications by autonomously classifying signals transmitted over the radio spectrum. Deep learning (DL) techniques are increasingly being used for AMC due to their ability to extract complex wireless signal features. However, DL models are computationally intensive and incur high inference latencies. This paper proposes the application of early exiting (EE) techniques for DL models used for AMC to accelerate inference. We present and analyze four early exiting architectures and a customized multi-branch training algorithm for this problem. Through extensive experimentation, we show that signals with moderate to high signal-to-noise ratios (SNRs) are easier to classify, do not require deep architectures, and can therefore leverage the proposed EE architectures. Our experimental results demonstrate that EE techniques can significantly reduce the inference speed of deep neural networks without sacrificing class
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Video OWL-ViT&#27169;&#22411;&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;&#24320;&#25918;&#19990;&#30028;&#22270;&#20687;&#27169;&#22411;&#24212;&#29992;&#20110;&#35270;&#39057;&#23450;&#20301;&#20219;&#21153;&#65292;&#36890;&#36807;&#28155;&#21152;&#21464;&#25442;&#22120;&#35299;&#30721;&#22120;&#23454;&#29616;&#26102;&#38388;&#19978;&#30340;&#36830;&#32493;&#20256;&#25773;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#36319;&#36394;-by-detection&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.11093</link><description>&lt;p&gt;
Video OWL-ViT: &#35270;&#39057;&#20013;&#20855;&#26377;&#26102;&#38388;&#19968;&#33268;&#24615;&#30340;&#24320;&#25918;&#19990;&#30028;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Video OWL-ViT: Temporally-consistent open-world localization in video. (arXiv:2308.11093v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Video OWL-ViT&#27169;&#22411;&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;&#24320;&#25918;&#19990;&#30028;&#22270;&#20687;&#27169;&#22411;&#24212;&#29992;&#20110;&#35270;&#39057;&#23450;&#20301;&#20219;&#21153;&#65292;&#36890;&#36807;&#28155;&#21152;&#21464;&#25442;&#22120;&#35299;&#30721;&#22120;&#23454;&#29616;&#26102;&#38388;&#19978;&#30340;&#36830;&#32493;&#20256;&#25773;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#36319;&#36394;-by-detection&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26550;&#26500;&#21644;&#35757;&#32451;&#26041;&#26696;&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;&#24320;&#25918;&#19990;&#30028;&#22270;&#20687;&#27169;&#22411;&#24212;&#29992;&#20110;&#35270;&#39057;&#23450;&#20301;&#12290;&#29702;&#35299;&#24320;&#25918;&#30340;&#35270;&#35273;&#19990;&#30028;&#65288;&#19981;&#21463;&#22266;&#23450;&#26631;&#31614;&#31354;&#38388;&#30340;&#38480;&#21046;&#65289;&#23545;&#20110;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#30340;&#35270;&#35273;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#22823;&#22411;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23545;&#27604;&#39044;&#35757;&#32451;&#26368;&#36817;&#22312;&#22270;&#20687;&#32423;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#23545;&#20110;&#28041;&#21450;&#23545;&#35937;&#23450;&#20301;&#30340;&#26356;&#32467;&#26500;&#21270;&#20219;&#21153;&#65292;&#24212;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#23545;&#20110;&#35270;&#39057;&#20219;&#21153;&#26469;&#35828;&#23588;&#20854;&#22914;&#27492;&#65292;&#22240;&#20026;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#26159;&#26377;&#38480;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;OWL-ViT&#24320;&#25918;&#35789;&#27719;&#26816;&#27979;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#26500;&#24314;&#65292;&#24182;&#36890;&#36807;&#28155;&#21152;&#19968;&#20010;&#21464;&#25442;&#22120;&#35299;&#30721;&#22120;&#23558;&#20854;&#36866;&#24212;&#20026;&#35270;&#39057;&#65292;&#23637;&#31034;&#20102;&#24320;&#25918;&#19990;&#30028;&#27169;&#22411;&#30340;&#25104;&#21151;&#36716;&#31227;&#12290;&#35299;&#30721;&#22120;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#24103;&#30340;&#36755;&#20986;&#26631;&#35760;&#20316;&#20026;&#19979;&#19968;&#24103;&#30340;&#23545;&#35937;&#26597;&#35810;&#65292;&#20197;&#26102;&#38388;&#19978;&#30340;&#36830;&#32493;&#26041;&#24335;&#20256;&#25773;&#23545;&#35937;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#23545;&#35270;&#39057;&#25968;&#25454;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#65292;&#24182;&#19988;&#30456;&#27604;&#36890;&#36807;&#26816;&#27979;&#36827;&#34892;&#36319;&#36394;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an architecture and a training recipe that adapts pre-trained open-world image models to localization in videos. Understanding the open visual world (without being constrained by fixed label spaces) is crucial for many real-world vision tasks. Contrastive pre-training on large image-text datasets has recently led to significant improvements for image-level tasks. For more structured tasks involving object localization applying pre-trained models is more challenging. This is particularly true for video tasks, where task-specific data is limited. We show successful transfer of open-world models by building on the OWL-ViT open-vocabulary detection model and adapting it to video by adding a transformer decoder. The decoder propagates object representations recurrently through time by using the output tokens for one frame as the object queries for the next. Our model is end-to-end trainable on video data and enjoys improved temporal consistency compared to tracking-by-detection b
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#28798;&#38590;&#21709;&#24212;&#20013;&#65292;&#26080;&#20154;&#26426;&#12289;&#24037;&#20154;&#21644;&#27773;&#36710;&#30340;&#21327;&#21516;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#20219;&#21153;&#23436;&#25104;&#29575;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#31639;&#27861;MANF-RL-RP&#65292;&#35813;&#31639;&#27861;&#21253;&#25324;&#20102;&#20840;&#23616;-&#23616;&#37096;&#21452;&#37325;&#20449;&#24687;&#22788;&#29702;&#21644;&#38024;&#23545;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#23450;&#21046;&#27169;&#22411;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2308.11088</link><description>&lt;p&gt;
UAV&#12289;&#24037;&#20154;&#21644;&#27773;&#36710;&#30340;&#21327;&#21516;&#36335;&#24452;&#35268;&#21010;&#22312;&#28798;&#38590;&#21709;&#24212;&#20013;&#30340;&#20247;&#21253;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Collaborative Route Planning of UAVs, Workers and Cars for Crowdsensing in Disaster Response. (arXiv:2308.11088v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11088
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#28798;&#38590;&#21709;&#24212;&#20013;&#65292;&#26080;&#20154;&#26426;&#12289;&#24037;&#20154;&#21644;&#27773;&#36710;&#30340;&#21327;&#21516;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#20219;&#21153;&#23436;&#25104;&#29575;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#31639;&#27861;MANF-RL-RP&#65292;&#35813;&#31639;&#27861;&#21253;&#25324;&#20102;&#20840;&#23616;-&#23616;&#37096;&#21452;&#37325;&#20449;&#24687;&#22788;&#29702;&#21644;&#38024;&#23545;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#23450;&#21046;&#27169;&#22411;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#22320;&#33719;&#21462;&#28798;&#21306;&#26368;&#26032;&#20449;&#24687;&#26159;&#25104;&#21151;&#30340;&#28798;&#38590;&#21709;&#24212;&#30340;&#20851;&#38190;&#12290;&#26080;&#20154;&#26426;&#12289;&#24037;&#20154;&#21644;&#27773;&#36710;&#21487;&#20197;&#21327;&#21516;&#23436;&#25104;&#35832;&#22914;&#25968;&#25454;&#25910;&#38598;&#31561;&#24863;&#30693;&#20219;&#21153;&#12290;&#26412;&#25991;&#38024;&#23545;&#21253;&#25324;&#26080;&#20154;&#26426;&#12289;&#24037;&#20154;&#21644;&#27773;&#36710;&#22312;&#20869;&#30340;&#19968;&#32452;&#20195;&#29702;&#20154;&#30340;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#20219;&#21153;&#23436;&#25104;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#31639;&#27861;MANF-RL-RP&#65292;&#35813;&#31639;&#27861;&#21253;&#25324;&#20102;&#22810;&#31181;&#26377;&#25928;&#30340;&#35774;&#35745;&#65292;&#21253;&#25324;&#20840;&#23616;-&#23616;&#37096;&#21452;&#37325;&#20449;&#24687;&#22788;&#29702;&#21644;&#38024;&#23545;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#23450;&#21046;&#27169;&#22411;&#32467;&#26500;&#12290;&#20840;&#23616;-&#23616;&#37096;&#21452;&#37325;&#20449;&#24687;&#22788;&#29702;&#28085;&#30422;&#20102;&#20174;&#20840;&#23616;&#20449;&#24687;&#20013;&#25552;&#21462;&#21644;&#20256;&#25773;&#31354;&#38388;&#29305;&#24449;&#65292;&#20197;&#21450;&#20174;&#20010;&#20307;&#20195;&#29702;&#20154;&#20013;&#20998;&#21106;&#21644;&#36807;&#28388;&#26412;&#22320;&#20449;&#24687;&#12290;&#23545;&#20110;&#22810;&#26234;&#33021;&#20307;&#30340;&#27169;&#22411;&#32467;&#26500;&#26500;&#24314;&#65292;&#25105;&#20204;&#23436;&#25104;&#20102;&#20197;&#19979;&#24037;&#20316;&#65306;
&lt;/p&gt;
&lt;p&gt;
Efficiently obtaining the up-to-date information in the disaster-stricken area is the key to successful disaster response. Unmanned aerial vehicles (UAVs), workers and cars can collaborate to accomplish sensing tasks, such as data collection, in disaster-stricken areas. In this paper, we explicitly address the route planning for a group of agents, including UAVs, workers, and cars, with the goal of maximizing the task completion rate. We propose MANF-RL-RP, a heterogeneous multi-agent route planning algorithm that incorporates several efficient designs, including global-local dual information processing and a tailored model structure for heterogeneous multi-agent systems. Global-local dual information processing encompasses the extraction and dissemination of spatial features from global information, as well as the partitioning and filtering of local information from individual agents. Regarding the construction of the model structure for heterogeneous multi-agent, we perform the follo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#23545;&#39640;&#38454;&#31038;&#20250;&#25512;&#29702;&#36827;&#34892;&#25674;&#38144;&#65292;&#20174;&#32780;&#21152;&#24555;&#23884;&#22871;&#22810;&#26234;&#33021;&#20307;&#25512;&#29702;&#30340;&#35745;&#31639;&#36895;&#24230;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#21516;&#26102;&#20934;&#30830;&#24615;&#38477;&#20302;&#26368;&#23567;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.11071</link><description>&lt;p&gt;
&#23884;&#22871;&#30340;&#22810;&#26234;&#33021;&#20307;&#25512;&#29702;&#30340;&#31070;&#32463;&#25674;&#38144;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Neural Amortized Inference for Nested Multi-agent Reasoning. (arXiv:2308.11071v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#23545;&#39640;&#38454;&#31038;&#20250;&#25512;&#29702;&#36827;&#34892;&#25674;&#38144;&#65292;&#20174;&#32780;&#21152;&#24555;&#23884;&#22871;&#22810;&#26234;&#33021;&#20307;&#25512;&#29702;&#30340;&#35745;&#31639;&#36895;&#24230;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#21516;&#26102;&#20934;&#30830;&#24615;&#38477;&#20302;&#26368;&#23567;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#22914;&#27807;&#36890;&#12289;&#25945;&#23398;&#21644;&#34394;&#24352;&#22768;&#21183;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#39640;&#38454;&#30340;&#31038;&#20250;&#25512;&#29702;&#65292;&#21363;&#29702;&#35299;&#20182;&#20154;&#22914;&#20309;&#25512;&#26029;&#33258;&#24049;&#12290;&#36825;&#31181;&#22797;&#26434;&#30340;&#25512;&#29702;&#21487;&#20197;&#36890;&#36807;&#23884;&#22871;&#24335;&#22810;&#26234;&#33021;&#20307;&#25512;&#29702;&#26469;&#26377;&#25928;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#27599;&#20010;&#25512;&#29702;&#32423;&#21035;&#30340;&#22686;&#21152;&#65292;&#35745;&#31639;&#22797;&#26434;&#24615;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#36731;&#26494;&#22320;&#25191;&#34892;&#22797;&#26434;&#30340;&#31038;&#20250;&#25512;&#29702;&#12290;&#20026;&#20102;&#24357;&#21512;&#20154;&#31867;&#25512;&#29702;&#33021;&#21147;&#21644;&#35745;&#31639;&#38480;&#21046;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65306;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#23545;&#39640;&#38454;&#31038;&#20250;&#25512;&#29702;&#36827;&#34892;&#25674;&#38144;&#65292;&#20174;&#32780;&#21152;&#24555;&#23884;&#22871;&#22810;&#26234;&#33021;&#20307;&#25512;&#29702;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#39046;&#22495;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#21516;&#26102;&#20934;&#30830;&#24615;&#38477;&#20302;&#26368;&#23567;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-agent interactions, such as communication, teaching, and bluffing, often rely on higher-order social inference, i.e., understanding how others infer oneself. Such intricate reasoning can be effectively modeled through nested multi-agent reasoning. Nonetheless, the computational complexity escalates exponentially with each level of reasoning, posing a significant challenge. However, humans effortlessly perform complex social inferences as part of their daily lives. To bridge the gap between human-like inference capabilities and computational limitations, we propose a novel approach: leveraging neural networks to amortize high-order social inference, thereby expediting nested multi-agent reasoning. We evaluate our method in two challenging multi-agent interaction domains. The experimental results demonstrate that our method is computationally efficient while exhibiting minimal degradation in accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#35270;&#39057;&#25968;&#25454;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#32972;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#36716;&#25442;&#39046;&#22495;&#20013;&#28155;&#21152;&#38590;&#20197;&#23519;&#35273;&#30340;&#12289;&#26102;&#38388;&#19978;&#20998;&#24067;&#30340;&#35302;&#21457;&#22120;&#26469;&#23454;&#29616;&#35823;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2308.11070</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#38388;&#20998;&#24067;&#30340;&#35270;&#39057;&#34892;&#20026;&#35782;&#21035;&#32972;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Temporal-Distributed Backdoor Attack Against Video Based Action Recognition. (arXiv:2308.11070v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#35270;&#39057;&#25968;&#25454;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#32972;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#36716;&#25442;&#39046;&#22495;&#20013;&#28155;&#21152;&#38590;&#20197;&#23519;&#35273;&#30340;&#12289;&#26102;&#38388;&#19978;&#20998;&#24067;&#30340;&#35302;&#21457;&#22120;&#26469;&#23454;&#29616;&#35823;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21253;&#25324;&#35270;&#39057;&#34892;&#20026;&#35782;&#21035;&#22312;&#20869;&#30340;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#32972;&#38376;&#25915;&#20987;&#65288;&#29305;&#27931;&#20234;&#65289;&#12290;&#24403;&#27979;&#35797;&#23454;&#20363;&#65288;&#26469;&#33258;&#38750;&#30446;&#26631;&#31867;&#65289;&#23884;&#20837;&#29305;&#23450;&#35302;&#21457;&#22120;&#26102;&#65292;&#34987;&#32972;&#38376;&#30772;&#22351;&#30340;&#27169;&#22411;&#20250;&#35823;&#20998;&#31867;&#20026;&#25915;&#20987;&#32773;&#36873;&#25321;&#30340;&#30446;&#26631;&#31867;&#65292;&#21516;&#26102;&#22312;&#26080;&#25915;&#20987;&#23454;&#20363;&#19978;&#20445;&#25345;&#39640;&#20934;&#30830;&#29575;&#12290;&#23613;&#31649;&#23545;&#20110;&#22270;&#20687;&#25968;&#25454;&#30340;&#32972;&#38376;&#25915;&#20987;&#24050;&#32463;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#35270;&#39057;&#31995;&#32479;&#22312;&#32972;&#38376;&#25915;&#20987;&#19979;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#20173;&#28982;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;&#24403;&#21069;&#30340;&#30740;&#31350;&#26159;&#23545;&#22270;&#20687;&#25968;&#25454;&#30340;&#26041;&#27861;&#30340;&#30452;&#25509;&#24310;&#20280;&#65292;&#20363;&#22914;&#65292;&#35302;&#21457;&#22120;&#26159;\textbf{&#29420;&#31435;}&#23884;&#20837;&#24103;&#20013;&#30340;&#65292;&#23481;&#26131;&#34987;&#29616;&#26377;&#38450;&#24481;&#26426;&#21046;&#26816;&#27979;&#21040;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;\textit{&#31616;&#21333;}&#20294;\textit{&#26377;&#25928;}&#30340;&#35270;&#39057;&#25968;&#25454;&#32972;&#38376;&#25915;&#20987;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#25915;&#20987;&#22312;&#19968;&#20010;&#36716;&#25442;&#30340;&#39046;&#22495;&#20013;&#28155;&#21152;&#25200;&#21160;&#65292;&#20197;&#23884;&#20837;\textbf{&#38590;&#20197;&#23519;&#35273;&#30340;&#65292;&#26102;&#38388;&#19978;&#20998;&#24067;&#30340;}&#35302;&#21457;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have achieved tremendous success in various applications including video action recognition, yet remain vulnerable to backdoor attacks (Trojans). The backdoor-compromised model will mis-classify to the target class chosen by the attacker when a test instance (from a non-target class) is embedded with a specific trigger, while maintaining high accuracy on attack-free instances. Although there are extensive studies on backdoor attacks against image data, the susceptibility of video-based systems under backdoor attacks remains largely unexplored. Current studies are direct extensions of approaches proposed for image data, e.g., the triggers are \textbf{independently} embedded within the frames, which tend to be detectable by existing defenses. In this paper, we introduce a \textit{simple} yet \textit{effective} backdoor attack against video data. Our proposed attack, adding perturbations in a transformed domain, plants an \textbf{imperceptible, temporally distr
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25299;&#25169;&#32467;&#26500;&#30340;&#22270;&#20449;&#21495;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#22788;&#29702;&#39640;&#38454;&#20132;&#20114;&#12289;&#32858;&#31867;&#21644;&#28040;&#24687;&#20256;&#36882;&#31561;&#27493;&#39588;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#22312;&#21387;&#32553;&#20449;&#21495;&#26102;&#20855;&#26377;&#26356;&#22909;&#30340;&#37325;&#24314;&#35823;&#24046;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#21644;&#21033;&#29992;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2308.11068</link><description>&lt;p&gt;
&#22522;&#20110;&#25299;&#25169;&#32467;&#26500;&#30340;&#22270;&#20449;&#21495;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Topological Graph Signal Compression. (arXiv:2308.11068v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11068
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25299;&#25169;&#32467;&#26500;&#30340;&#22270;&#20449;&#21495;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#22788;&#29702;&#39640;&#38454;&#20132;&#20114;&#12289;&#32858;&#31867;&#21644;&#28040;&#24687;&#20256;&#36882;&#31561;&#27493;&#39588;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#22312;&#21387;&#32553;&#20449;&#21495;&#26102;&#20855;&#26377;&#26356;&#22909;&#30340;&#37325;&#24314;&#35823;&#24046;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#21644;&#21033;&#29992;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20986;&#29616;&#30340;&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#65288;TDL&#65289;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#33258;&#28982;&#22320;&#22788;&#29702;&#39640;&#38454;&#20132;&#20114;&#65292;&#36229;&#36234;&#30001;&#22270;&#34920;&#31034;&#23450;&#20041;&#30340;&#25104;&#23545;&#20851;&#31995;&#21644;&#23616;&#37096;&#37051;&#22495;&#65292;&#20174;&#32780;&#25193;&#23637;&#24403;&#21069;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;TDL&#30340;&#22270;&#20449;&#21495;&#21387;&#32553;&#26041;&#27861;&#65292;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#27493;&#39588;&#65306;&#39318;&#20808;&#65292;&#22522;&#20110;&#21407;&#22987;&#20449;&#21495;&#25512;&#26029;&#20986;&#19981;&#30456;&#20132;&#30340;&#39640;&#38454;&#32467;&#26500;&#65292;&#36890;&#36807;&#23558;N&#20010;&#25968;&#25454;&#28857;&#32858;&#31867;&#25104;K&#20010;&#38598;&#21512;&#65307;&#28982;&#21518;&#65292;&#22522;&#20110;&#25299;&#25169;&#21551;&#31034;&#30340;&#28040;&#24687;&#20256;&#36882;&#22312;&#36825;&#20123;&#22810;&#20803;&#32032;&#38598;&#21512;&#20013;&#33719;&#24471;&#20449;&#21495;&#30340;&#21387;&#32553;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#21387;&#32553;&#26469;&#33258;&#20004;&#20010;&#30495;&#23454;&#30340;&#20114;&#32852;&#32593;&#26381;&#21153;&#25552;&#20379;&#21830;&#32593;&#32476;&#25968;&#25454;&#38598;&#30340;&#26102;&#38388;&#38142;&#36335;&#20449;&#21495;&#26102;&#65292;&#27604;&#26631;&#20934;&#30340;GNN&#21644;&#21069;&#39304;&#26550;&#26500;&#20855;&#26377;&#26356;&#22909;&#30340;&#37325;&#24314;&#35823;&#24046;&#8212;&#8212;&#22312;&#25152;&#26377;&#35780;&#20272;&#22330;&#26223;&#20013;&#65292;&#37325;&#24314;&#35823;&#24046;&#25552;&#39640;&#20102;&#20174;30%&#21040;90%&#12290;&#36825;&#34920;&#26126;&#23427;&#26356;&#22909;&#22320;&#25429;&#25417;&#21644;&#21033;&#29992;&#20102;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently emerged Topological Deep Learning (TDL) methods aim to extend current Graph Neural Networks (GNN) by naturally processing higher-order interactions, going beyond the pairwise relations and local neighborhoods defined by graph representations. In this paper we propose a novel TDL-based method for compressing signals over graphs, consisting in two main steps: first, disjoint sets of higher-order structures are inferred based on the original signal --by clustering $N$ datapoints into $K\ll N$ collections; then, a topological-inspired message passing gets a compressed representation of the signal within those multi-element sets. Our results show that our framework improves both standard GNN and feed-forward architectures in compressing temporal link-based signals from two real-word Internet Service Provider Networks' datasets --from $30\%$ up to $90\%$ better reconstruction errors across all evaluation scenarios--, suggesting that it better captures and exploits spatial and tempor
&lt;/p&gt;</description></item><item><title>CSM-H-R&#26159;&#19968;&#20010;&#33258;&#21160;&#19978;&#19979;&#25991;&#25512;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#21487;&#20114;&#25805;&#20316;&#30340;&#26234;&#33021;&#31995;&#32479;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#26412;&#20307;&#21644;&#29366;&#24577;&#65292;&#22312;&#36816;&#34892;&#26102;&#35782;&#21035;&#26377;&#24847;&#20041;&#30340;&#39640;&#32423;&#19978;&#19979;&#25991;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#25512;&#29702;&#25216;&#26415;&#12290;&#22312;&#26234;&#33021;&#26657;&#22253;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#26234;&#33021;&#30005;&#26799;&#31995;&#32479;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#20808;&#36827;&#30340;&#25968;&#23398;&#21644;&#27010;&#29575;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.11066</link><description>&lt;p&gt;
CSM-H-R: &#19968;&#31181;&#29992;&#20110;&#21487;&#20114;&#25805;&#20316;&#26234;&#33021;&#31995;&#32479;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#33258;&#21160;&#19978;&#19979;&#25991;&#25512;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CSM-H-R: An Automatic Context Reasoning Framework for Interoperable Intelligent Systems and Privacy Protection. (arXiv:2308.11066v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11066
&lt;/p&gt;
&lt;p&gt;
CSM-H-R&#26159;&#19968;&#20010;&#33258;&#21160;&#19978;&#19979;&#25991;&#25512;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#21487;&#20114;&#25805;&#20316;&#30340;&#26234;&#33021;&#31995;&#32479;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#26412;&#20307;&#21644;&#29366;&#24577;&#65292;&#22312;&#36816;&#34892;&#26102;&#35782;&#21035;&#26377;&#24847;&#20041;&#30340;&#39640;&#32423;&#19978;&#19979;&#25991;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#25512;&#29702;&#25216;&#26415;&#12290;&#22312;&#26234;&#33021;&#26657;&#22253;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#26234;&#33021;&#30005;&#26799;&#31995;&#32479;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#20808;&#36827;&#30340;&#25968;&#23398;&#21644;&#27010;&#29575;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29289;&#32852;&#32593;&#26102;&#20195;&#65292;&#26234;&#33021;&#31995;&#32479;&#23545;&#39640;&#32423;&#19978;&#19979;&#25991;(HLC)&#25512;&#29702;&#30340;&#33258;&#21160;&#21270;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#36825;&#26159;&#22240;&#20026;&#19978;&#19979;&#25991;&#25968;&#25454;&#30340;&#19981;&#26029;&#31215;&#32047;&#12289;&#22810;&#28304;&#25968;&#25454;&#34701;&#21512;&#30340;&#36235;&#21183;&#20197;&#21450;&#22522;&#20110;&#19978;&#19979;&#25991;&#20915;&#31574;&#36807;&#31243;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#21644;&#21160;&#24577;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#19978;&#19979;&#25991;&#25512;&#29702;&#26694;&#26550;CSM-H-R&#65292;&#35813;&#26694;&#26550;&#22312;&#36816;&#34892;&#26102;&#20197;&#32534;&#31243;&#26041;&#24335;&#32452;&#21512;&#26412;&#20307;&#21644;&#29366;&#24577;&#65292;&#24182;&#32467;&#21512;&#27169;&#22411;&#23384;&#20648;&#38454;&#27573;&#65292;&#20197;&#23454;&#29616;&#35782;&#21035;&#26377;&#24847;&#20041;&#30340;HLC&#30340;&#33021;&#21147;&#65292;&#25152;&#24471;&#21040;&#30340;&#25968;&#25454;&#34920;&#31034;&#21487;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#25512;&#29702;&#25216;&#26415;&#12290;&#22312;&#26234;&#33021;&#26657;&#22253;&#29615;&#22659;&#20013;&#22522;&#20110;&#26234;&#33021;&#30005;&#26799;&#31995;&#32479;&#24320;&#23637;&#20102;&#26696;&#20363;&#30740;&#31350;&#12290;&#26694;&#26550;&#30340;&#23454;&#29616;-CSM&#24341;&#25806;&#20197;&#21450;&#23558;HLC&#25512;&#29702;&#36716;&#21270;&#20026;&#30690;&#37327;&#21644;&#30697;&#38453;&#35745;&#31639;&#30340;&#23454;&#39564;&#65292;&#29305;&#21035;&#20851;&#27880;&#19978;&#19979;&#25991;&#30340;&#21160;&#24577;&#29305;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#20808;&#36827;&#30340;&#25968;&#23398;&#21644;&#27010;&#29575;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automation of High-Level Context (HLC) reasoning for intelligent systems at scale is imperative due to the unceasing accumulation of contextual data in the IoT era, the trend of the fusion of data from multi-sources, and the intrinsic complexity and dynamism of the context-based decision-making process. To mitigate this issue, we propose an automatic context reasoning framework CSM-H-R, which programmatically combines ontologies and states at runtime and the model-storage phase for attaining the ability to recognize meaningful HLC, and the resulting data representation can be applied to different reasoning techniques. Case studies are developed based on an intelligent elevator system in a smart campus setting. An implementation of the framework - a CSM Engine, and the experiments of translating the HLC reasoning into vector and matrix computing especially take care of the dynamic aspects of context and present the potentiality of using advanced mathematical and probabilistic models to 
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#23545;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#26174;&#33879;&#24615;&#22270;&#21644;&#31867;&#28608;&#27963;&#22270;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#35777;&#26126;&#20102;&#26174;&#33879;&#24615;&#22270;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.11052</link><description>&lt;p&gt;
&#36229;&#36234;&#21028;&#21035;&#24615;&#21306;&#22495;&#65306;&#20316;&#20026;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;CAM&#30340;&#26367;&#20195;&#26041;&#27861;&#30340;&#26174;&#33879;&#24615;&#22270;
&lt;/p&gt;
&lt;p&gt;
Beyond Discriminative Regions: Saliency Maps as Alternatives to CAMs for Weakly Supervised Semantic Segmentation. (arXiv:2308.11052v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#23545;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#26174;&#33879;&#24615;&#22270;&#21644;&#31867;&#28608;&#27963;&#22270;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#35777;&#26126;&#20102;&#26174;&#33879;&#24615;&#22270;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25552;&#20986;&#20102;&#20960;&#31181;&#20351;&#29992;&#20998;&#31867;&#22120;&#29983;&#25104;&#30340;&#31867;&#28608;&#27963;&#22270;&#65288;CAM&#65289;&#26469;&#29983;&#25104;&#20266;&#22320;&#38754;&#30495;&#30456;&#20197;&#35757;&#32451;&#20998;&#21106;&#27169;&#22411;&#30340;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#65288;WS3&#65289;&#26041;&#27861;&#12290;&#34429;&#28982;CAM&#33021;&#22815;&#24456;&#22909;&#22320;&#31361;&#20986;&#22270;&#20687;&#30340;&#21028;&#21035;&#24615;&#21306;&#22495;&#65288;DR&#65289;&#65292;&#20294;&#23427;&#20204;&#34987;&#30693;&#36947;&#24573;&#35270;&#19981;&#23545;&#20998;&#31867;&#22120;&#39044;&#27979;&#20570;&#20986;&#36129;&#29486;&#30340;&#29289;&#20307;&#21306;&#22495;&#65292;&#31216;&#20026;&#38750;&#21028;&#21035;&#24615;&#21306;&#22495;&#65288;NDR&#65289;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26174;&#33879;&#24615;&#22270;&#31561;&#24402;&#22240;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#27599;&#20010;&#20687;&#32032;&#23545;&#20998;&#31867;&#39044;&#27979;&#30340;&#36129;&#29486;&#26469;&#20998;&#37197;&#24471;&#20998;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#26412;&#25991;&#23545;WS3&#20013;&#30340;&#26174;&#33879;&#24615;&#22270;&#21644;CAM&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20174;&#22810;&#20010;&#35282;&#24230;&#29702;&#35299;&#23427;&#20204;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#23545;&#27604;CAM&#30340;&#26367;&#20195;&#26041;&#27861;&#22312;WS3&#24615;&#33021;&#26041;&#38754;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26174;&#33879;&#24615;&#22270;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, several Weakly Supervised Semantic Segmentation (WS3) methods have been proposed that use class activation maps (CAMs) generated by a classifier to produce pseudo-ground truths for training segmentation models. While CAMs are good at highlighting discriminative regions (DR) of an image, they are known to disregard regions of the object that do not contribute to the classifier's prediction, termed non-discriminative regions (NDR). In contrast, attribution methods such as saliency maps provide an alternative approach for assigning a score to every pixel based on its contribution to the classification prediction. This paper provides a comprehensive comparison between saliencies and CAMs for WS3. Our study includes multiple perspectives on understanding their similarities and dissimilarities. Moreover, we provide new evaluation metrics that perform a comprehensive assessment of WS3 performance of alternative methods w.r.t. CAMs. We demonstrate the effectiveness of salienci
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;K-Means&#21644;P-Median&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36947;&#36335;&#32593;&#32476;&#36317;&#31163;&#26469;&#20248;&#21270;&#22312;&#22478;&#24066;&#29615;&#22659;&#19979;&#29289;&#27969;&#38598;&#25955;&#22320;&#30340;&#20301;&#32622;&#24067;&#32622;&#65292;&#20197;&#20943;&#23569;&#37197;&#36865;&#36317;&#31163;&#21644;&#30899;&#36275;&#36857;&#12290;</title><link>http://arxiv.org/abs/2308.11038</link><description>&lt;p&gt;
&#29289;&#27969;&#38598;&#25955;&#22320;&#20301;&#32622;&#20248;&#21270;&#65306;&#19968;&#31181;&#22522;&#20110;K-Means&#21644;P-Median&#27169;&#22411;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#21033;&#29992;&#36947;&#36335;&#32593;&#32476;&#36317;&#31163;
&lt;/p&gt;
&lt;p&gt;
Logistics Hub Location Optimization: A K-Means and P-Median Model Hybrid Approach Using Road Network Distances. (arXiv:2308.11038v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;K-Means&#21644;P-Median&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36947;&#36335;&#32593;&#32476;&#36317;&#31163;&#26469;&#20248;&#21270;&#22312;&#22478;&#24066;&#29615;&#22659;&#19979;&#29289;&#27969;&#38598;&#25955;&#22320;&#30340;&#20301;&#32622;&#24067;&#32622;&#65292;&#20197;&#20943;&#23569;&#37197;&#36865;&#36317;&#31163;&#21644;&#30899;&#36275;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#27969;&#38598;&#25955;&#22320;&#22312;&#26368;&#21518;&#19968;&#20844;&#37324;&#37197;&#36865;&#36317;&#31163;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65307;&#21363;&#20351;&#36317;&#31163;&#24494;&#23567;&#22686;&#21152;&#20063;&#20250;&#23545;&#30005;&#23376;&#21830;&#21153;&#34892;&#19994;&#30340;&#19994;&#21153;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#21516;&#26102;&#36824;&#20250;&#22686;&#21152;&#20854;&#30899;&#36275;&#36857;&#12290;&#29305;&#21035;&#26159;&#22312;Covid-19&#20043;&#21518;&#65292;&#35813;&#34892;&#19994;&#30340;&#22686;&#38271;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#22312;&#22478;&#24066;&#29615;&#22659;&#20013;&#20248;&#21270;&#36164;&#28304;&#20998;&#37197;&#30340;&#38656;&#27714;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#26469;&#20248;&#21270;&#29289;&#27969;&#38598;&#25955;&#22320;&#30340;&#24067;&#32622;&#12290;&#35813;&#26041;&#27861;&#20381;&#27425;&#37319;&#29992;&#19981;&#21516;&#30340;&#25216;&#26415;&#12290;&#39318;&#20808;&#65292;&#26681;&#25454;&#23427;&#20204;&#30340;&#31354;&#38388;&#20301;&#32622;&#65292;&#20351;&#29992;K-Means&#23545;&#20132;&#20184;&#28857;&#36827;&#34892;&#32858;&#31867;&#12290;&#32858;&#31867;&#26041;&#27861;&#20351;&#29992;&#36947;&#36335;&#32593;&#32476;&#36317;&#31163;&#65292;&#32780;&#19981;&#26159;&#27431;&#20960;&#37324;&#24503;&#36317;&#31163;&#12290;&#36991;&#20813;&#20351;&#29992;&#38750;&#22522;&#20110;&#36947;&#36335;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#20204;&#20250;&#23548;&#33268;&#38169;&#35823;&#21644;&#35823;&#23548;&#24615;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#20351;&#29992;P-Median&#26041;&#27861;&#30830;&#23450;&#38598;&#25955;&#22320;&#30340;&#20301;&#32622;&#12290;P-Median&#26041;&#27861;&#36824;&#23558;&#20132;&#20184;&#25968;&#37327;&#21644;&#20154;&#21475;&#20316;&#20026;&#26435;&#37325;&#32771;&#34385;&#22312;&#20869;&#12290;&#20351;&#29992;Muller&#21644;Phipps&#65288;M&#65286;P&#65289;&#30340;&#23454;&#38469;&#20132;&#20184;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Logistic hubs play a pivotal role in the last-mile delivery distance; even a slight increment in distance negatively impacts the business of the e-commerce industry while also increasing its carbon footprint. The growth of this industry, particularly after Covid-19, has further intensified the need for optimized allocation of resources in an urban environment. In this study, we use a hybrid approach to optimize the placement of logistic hubs. The approach sequentially employs different techniques. Initially, delivery points are clustered using K-Means in relation to their spatial locations. The clustering method utilizes road network distances as opposed to Euclidean distances. Non-road network-based approaches have been avoided since they lead to erroneous and misleading results. Finally, hubs are located using the P-Median method. The P-Median method also incorporates the number of deliveries and population as weights. Real-world delivery data from Muller and Phipps (M&amp;P) is used to 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#25968;&#23383;&#23402;&#29983;&#30340;&#22797;&#26434;&#32593;&#32476;&#31995;&#32479;&#30340;&#21487;&#25193;&#23637;&#24314;&#27169;&#26694;&#26550;&#65292;&#37325;&#28857;&#20851;&#27880;&#33410;&#28857;&#29305;&#24449;&#21644;&#20132;&#20114;&#35268;&#21017;&#12290;&#36890;&#36807;&#23454;&#39564;&#30740;&#31350;&#20102;&#32593;&#32476;&#22686;&#38271;&#21644;&#30123;&#24773;&#20256;&#25773;&#30340;&#19981;&#21516;&#32423;&#21035;&#30340;&#22797;&#26434;&#24615;&#23545;&#31995;&#32479;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#38656;&#35201;&#22312;DT-CNS&#20013;&#24179;&#34913;&#36825;&#20123;&#22797;&#26434;&#24615;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2308.11034</link><description>&lt;p&gt;
&#22522;&#20110;&#24322;&#26500;&#33410;&#28857;&#29305;&#24449;&#21644;&#20132;&#20114;&#35268;&#21017;&#30340;&#38754;&#21521;&#25968;&#23383;&#23402;&#29983;&#30340;&#22797;&#26434;&#32593;&#32476;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Digital Twin-Oriented Complex Networked Systems based on Heterogeneous node features and interaction rules. (arXiv:2308.11034v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#25968;&#23383;&#23402;&#29983;&#30340;&#22797;&#26434;&#32593;&#32476;&#31995;&#32479;&#30340;&#21487;&#25193;&#23637;&#24314;&#27169;&#26694;&#26550;&#65292;&#37325;&#28857;&#20851;&#27880;&#33410;&#28857;&#29305;&#24449;&#21644;&#20132;&#20114;&#35268;&#21017;&#12290;&#36890;&#36807;&#23454;&#39564;&#30740;&#31350;&#20102;&#32593;&#32476;&#22686;&#38271;&#21644;&#30123;&#24773;&#20256;&#25773;&#30340;&#19981;&#21516;&#32423;&#21035;&#30340;&#22797;&#26434;&#24615;&#23545;&#31995;&#32479;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#38656;&#35201;&#22312;DT-CNS&#20013;&#24179;&#34913;&#36825;&#20123;&#22797;&#26434;&#24615;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#24314;&#27169;&#26694;&#26550;&#65292;&#29992;&#20110;&#25968;&#23383;&#23402;&#29983;&#23548;&#21521;&#30340;&#22797;&#26434;&#32593;&#32476;&#31995;&#32479;&#65288;DT-CNS&#65289;&#65292;&#26088;&#22312;&#29983;&#25104;&#33021;&#22815;&#30495;&#23454;&#34920;&#31034;&#23454;&#38469;&#31995;&#32479;&#30340;&#32593;&#32476;&#12290;&#24314;&#27169;&#36807;&#31243;&#20851;&#27880;&#33410;&#28857;&#30340;&#29305;&#24449;&#21644;&#22522;&#20110;&#20010;&#20307;&#33410;&#28857;&#20559;&#22909;&#21019;&#24314;&#36830;&#25509;&#30340;&#20132;&#20114;&#35268;&#21017;&#12290;&#25105;&#20204;&#23545;&#22522;&#20110;&#27169;&#25311;&#30340;DT-CNS&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20854;&#20013;&#21253;&#25324;&#21508;&#31181;&#29305;&#24449;&#21644;&#35268;&#21017;&#65292;&#20197;&#21450;&#19982;&#20256;&#26579;&#30149;&#22312;&#36825;&#20123;&#32593;&#32476;&#20013;&#30340;&#20256;&#25773;&#30456;&#20851;&#30340;&#19981;&#21516;&#20256;&#26579;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#35843;&#26597;&#29305;&#23450;&#26102;&#38388;&#21644;&#31038;&#20132;&#36317;&#31163;&#20869;&#30340;&#24863;&#26579;&#24773;&#20917;&#65292;&#23545;&#31038;&#20132;&#32593;&#32476;&#30340;&#28798;&#23475;&#38887;&#24615;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;&#32467;&#26500;&#22797;&#26434;&#24615;&#21644;&#21160;&#24577;&#22797;&#26434;&#24615;&#30340;&#19981;&#21516;&#32423;&#21035;&#23545;&#32593;&#32476;&#22686;&#38271;&#21644;&#30123;&#24773;&#20256;&#25773;&#30340;&#24433;&#21709;&#65292;&#20998;&#21035;&#28041;&#21450;&#29305;&#24449;&#22810;&#26679;&#24615;&#21644;&#20132;&#20114;&#35268;&#21017;&#30340;&#28789;&#27963;&#24615;&#12290;&#20998;&#26512;&#34920;&#26126;&#65292;&#35201;&#23454;&#29616;&#26368;&#22823;&#30340;&#28798;&#23475;&#38887;&#24615;&#65292;&#38656;&#35201;&#22312;DT-CNS&#20013;&#24179;&#34913;&#36825;&#20123;&#22797;&#26434;&#24615;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study proposes an extendable modelling framework for Digital Twin-Oriented Complex Networked Systems (DT-CNSs) with a goal of generating networks that faithfully represent real systems. Modelling process focuses on (i) features of nodes and (ii) interaction rules for creating connections that are built based on individual node's preferences. We conduct experiments on simulation-based DT-CNSs that incorporate various features and rules about network growth and different transmissibilities related to an epidemic spread on these networks. We present a case study on disaster resilience of social networks given an epidemic outbreak by investigating the infection occurrence within specific time and social distance. The experimental results show how different levels of the structural and dynamics complexities, concerned with feature diversity and flexibility of interaction rules respectively, influence network growth and epidemic spread. The analysis revealed that, to achieve maximum dis
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36816;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28216;&#25103;&#21270;&#25216;&#26415;&#30340;&#31995;&#32479;&#65292;&#29992;&#20110;&#25945;&#32946;&#20154;&#20204;&#35748;&#35782;&#25237;&#36164;&#35784;&#39575;&#21644;&#38519;&#38449;&#65292;&#24110;&#21161;&#30417;&#31649;&#26426;&#26500;&#30830;&#20445;&#19968;&#20010;&#20844;&#27491;&#12289;&#39640;&#25928;&#21644;&#21253;&#23481;&#30340;&#36164;&#26412;&#24066;&#22330;&#29615;&#22659;&#12290;</title><link>http://arxiv.org/abs/2308.11032</link><description>&lt;p&gt;
AI &#29992;&#20110;&#35782;&#21035;&#27450;&#35784;&#34892;&#20026;&#30340;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
AI For Fraud Awareness. (arXiv:2308.11032v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11032
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36816;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28216;&#25103;&#21270;&#25216;&#26415;&#30340;&#31995;&#32479;&#65292;&#29992;&#20110;&#25945;&#32946;&#20154;&#20204;&#35748;&#35782;&#25237;&#36164;&#35784;&#39575;&#21644;&#38519;&#38449;&#65292;&#24110;&#21161;&#30417;&#31649;&#26426;&#26500;&#30830;&#20445;&#19968;&#20010;&#20844;&#27491;&#12289;&#39640;&#25928;&#21644;&#21253;&#23481;&#30340;&#36164;&#26412;&#24066;&#22330;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20170;&#31038;&#20132;&#24179;&#21488;&#30340;&#20852;&#36215;&#19979;&#65292;&#20219;&#20309;&#20154;&#37117;&#33021;&#36731;&#26131;&#20256;&#25773;&#34394;&#20551;&#20449;&#24687;&#24182;&#35825;&#20351;&#20154;&#20204;&#33853;&#20837;&#38519;&#38449;&#12290;&#25237;&#36164;&#39046;&#22495;&#20013;&#30340;&#27450;&#35784;&#34892;&#20026;&#19982;&#38519;&#38449;&#27491;&#22312;&#36805;&#36895;&#22686;&#38271;&#65292;&#32473;&#22269;&#23478;&#21644;&#20010;&#20154;&#24102;&#26469;&#24040;&#22823;&#30340;&#36130;&#21153;&#39118;&#38505;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36816;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28216;&#25103;&#21270;&#25216;&#26415;&#30340;&#31995;&#32479;&#65292;&#29992;&#20110;&#25945;&#32946;&#20154;&#20204;&#35748;&#35782;&#25237;&#36164;&#35784;&#39575;&#21644;&#38519;&#38449;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#23398;&#20064;&#20307;&#39564;&#12290;&#31995;&#32479;&#20174;&#39046;&#22495;&#19987;&#23478;&#25552;&#20379;&#30340;&#30693;&#35782;&#24211;&#20013;&#36873;&#25321;&#19981;&#21516;&#30340;&#28216;&#25103;&#35774;&#35745;&#20803;&#32032;&#21644;&#27450;&#35784;&#34892;&#20026;&#65292;&#38024;&#23545;&#27599;&#20010;&#29992;&#25143;&#36827;&#34892;&#25945;&#32946;&#12290;&#30740;&#31350;&#39033;&#30446;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20027;&#21160;&#23398;&#20064;&#26469;&#20943;&#23569;&#21508;&#22269;&#38388;&#30340;&#19981;&#24179;&#31561;&#65292;&#24110;&#21161;&#30417;&#31649;&#26426;&#26500;&#30830;&#20445;&#19968;&#20010;&#20844;&#27491;&#12289;&#39640;&#25928;&#21644;&#21253;&#23481;&#30340;&#36164;&#26412;&#24066;&#22330;&#29615;&#22659;&#12290;&#22312;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#35813;&#38382;&#39064;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#29616;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In today's world, with the rise of numerous social platforms, it has become relatively easy for anyone to spread false information and lure people into traps. Fraudulent schemes and traps are growing rapidly in the investment world. Due to this, countries and individuals face huge financial risks. We present an awareness system with the use of machine learning and gamification techniques to educate the people about investment scams and traps. Our system applies machine learning techniques to provide a personalized learning experience to the user. The system chooses distinct game-design elements and scams from the knowledge pool crafted by domain experts for each individual. The objective of the research project is to reduce inequalities in all countries by educating investors via Active Learning. Our goal is to assist the regulators in assuring a conducive environment for a fair, efficient, and inclusive capital market. In the paper, we discuss the impact of the problem, provide implem
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;RBA-GCN&#27169;&#22411;&#29992;&#20110;&#24773;&#24863;&#35782;&#21035;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;&#20851;&#31995;&#21452;&#23618;&#32858;&#21512;&#21644;&#22270;&#29983;&#25104;&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;GCN&#27169;&#22411;&#20013;&#30340;&#33410;&#28857;&#20449;&#24687;&#20887;&#20313;&#21644;&#36828;&#36317;&#31163;&#19978;&#19979;&#25991;&#20449;&#24687;&#25429;&#33719;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.11029</link><description>&lt;p&gt;
RBA-GCN: &#20851;&#31995;&#21452;&#23618;&#32858;&#21512;&#22270;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
RBA-GCN: Relational Bilevel Aggregation Graph Convolutional Network for Emotion Recognition. (arXiv:2308.11029v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11029
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;RBA-GCN&#27169;&#22411;&#29992;&#20110;&#24773;&#24863;&#35782;&#21035;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;&#20851;&#31995;&#21452;&#23618;&#32858;&#21512;&#21644;&#22270;&#29983;&#25104;&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;GCN&#27169;&#22411;&#20013;&#30340;&#33410;&#28857;&#20449;&#24687;&#20887;&#20313;&#21644;&#36828;&#36317;&#31163;&#19978;&#19979;&#25991;&#20449;&#24687;&#25429;&#33719;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#35782;&#21035;&#22312;&#23545;&#35805;&#20013;&#30340;&#24212;&#29992;&#21463;&#21040;&#20102;&#30740;&#31350;&#20154;&#21592;&#30340;&#20851;&#27880;&#65292;&#30001;&#20110;&#23427;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#30001;&#20110;&#23545;&#35805;&#20855;&#26377;&#33258;&#28982;&#30340;&#22270;&#32467;&#26500;&#65292;&#24456;&#22810;&#22522;&#20110;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#30340;ERC&#27169;&#22411;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;GCNs&#30340;&#32858;&#21512;&#26041;&#27861;&#23384;&#22312;&#33410;&#28857;&#20449;&#24687;&#20887;&#20313;&#38382;&#39064;&#65292;&#23548;&#33268;&#33410;&#28857;&#36776;&#21035;&#20449;&#24687;&#30340;&#20002;&#22833;&#12290;&#27492;&#22806;&#65292;&#21333;&#23618;GCNs&#32570;&#20047;&#20174;&#22270;&#20013;&#25429;&#33719;&#36828;&#36317;&#31163;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#26159;&#22522;&#20110;&#25991;&#26412;&#27169;&#24577;&#25110;&#23558;&#19981;&#21516;&#27169;&#24577;&#25340;&#25509;&#22312;&#19968;&#36215;&#65292;&#23548;&#33268;&#25429;&#25417;&#27169;&#24577;&#38388;&#20132;&#20114;&#33021;&#21147;&#24369;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#31995;&#21452;&#23618;&#32858;&#21512;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;RBA-GCN&#65289;&#65292;&#23427;&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65306;&#22270;&#29983;&#25104;&#27169;&#22359;&#65288;GGM&#65289;&#12289;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#31751;&#26500;&#24314;&#27169;&#22359;&#65288;SCBM&#65289;&#21644;&#21452;&#23618;&#32858;&#21512;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion recognition in conversation (ERC) has received increasing attention from researchers due to its wide range of applications. As conversation has a natural graph structure, numerous approaches used to model ERC based on graph convolutional networks (GCNs) have yielded significant results. However, the aggregation approach of traditional GCNs suffers from the node information redundancy problem, leading to node discriminant information loss. Additionally, single-layer GCNs lack the capacity to capture long-range contextual information from the graph. Furthermore, the majority of approaches are based on textual modality or stitching together different modalities, resulting in a weak ability to capture interactions between modalities. To address these problems, we present the relational bilevel aggregation graph convolutional network (RBA-GCN), which consists of three modules: the graph generation module (GGM), similarity-based cluster building module (SCBM) and bilevel aggregation 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20010;&#24615;&#21270;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20107;&#20214;&#39044;&#27979;&#65292;&#25552;&#20986;&#20102;&#22810;&#20010;&#26032;&#30340;&#39044;&#27979;&#27169;&#22411;&#21644;&#26041;&#27861;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#20010;&#20307;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2308.11013</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20107;&#20214;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Personalized Event Prediction for Electronic Health Records. (arXiv:2308.11013v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11013
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20010;&#24615;&#21270;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20107;&#20214;&#39044;&#27979;&#65292;&#25552;&#20986;&#20102;&#22810;&#20010;&#26032;&#30340;&#39044;&#27979;&#27169;&#22411;&#21644;&#26041;&#27861;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#20010;&#20307;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#20107;&#20214;&#24207;&#21015;&#21253;&#21547;&#25968;&#30334;&#20010;&#20020;&#24202;&#20107;&#20214;&#65292;&#20195;&#34920;&#20102;&#24739;&#32773;&#22312;&#19981;&#21516;&#26102;&#38388;&#25509;&#21463;&#29031;&#25252;&#30340;&#35760;&#24405;&#12290;&#24320;&#21457;&#20934;&#30830;&#30340;&#39044;&#27979;&#27169;&#22411;&#23545;&#20110;&#25903;&#25345;&#21508;&#31181;&#27169;&#22411;&#26469;&#35299;&#37322;/&#20998;&#31867;&#24403;&#21069;&#24739;&#32773;&#29366;&#20917;&#25110;&#39044;&#27979;&#19981;&#33391;&#20020;&#24202;&#20107;&#20214;&#21644;&#32467;&#26524;&#38750;&#24120;&#37325;&#35201;&#65292;&#25152;&#26377;&#36825;&#20123;&#37117;&#26088;&#22312;&#25552;&#39640;&#24739;&#32773;&#25252;&#29702;&#27700;&#24179;&#12290;&#23398;&#20064;&#20020;&#24202;&#24207;&#21015;&#30340;&#39044;&#27979;&#27169;&#22411;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#23427;&#20204;&#30340;&#20010;&#20307;&#24046;&#24322;&#24615;&#12290;&#26681;&#25454;&#28508;&#22312;&#30340;&#20020;&#24202;&#26465;&#20214;&#65292;&#27599;&#20010;&#24739;&#32773;&#30340;&#24207;&#21015;&#21487;&#33021;&#21253;&#21547;&#19981;&#21516;&#30340;&#20020;&#24202;&#20107;&#20214;&#38598;&#65288;&#35266;&#23519;&#65292;&#23454;&#39564;&#23460;&#32467;&#26524;&#65292;&#33647;&#29289;&#65292;&#31243;&#24207;&#65289;&#12290;&#22240;&#27492;&#65292;&#20165;&#22522;&#20110;&#35768;&#22810;&#19981;&#21516;&#24739;&#32773;&#30340;&#20107;&#20214;&#24207;&#21015;&#23398;&#20064;&#30340;&#31616;&#21333;&#32676;&#20307;&#33539;&#22260;&#27169;&#22411;&#21487;&#33021;&#26080;&#27861;&#20934;&#30830;&#39044;&#27979;&#24739;&#32773;&#29305;&#23450;&#30340;&#20107;&#20214;&#24207;&#21015;&#21160;&#24577;&#21644;&#24046;&#24322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#22810;&#20010;&#26032;&#30340;&#20107;&#20214;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#21644;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#35843;&#25972;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical event sequences consist of hundreds of clinical events that represent records of patient care in time. Developing accurate predictive models of such sequences is of a great importance for supporting a variety of models for interpreting/classifying the current patient condition, or predicting adverse clinical events and outcomes, all aimed to improve patient care. One important challenge of learning predictive models of clinical sequences is their patient-specific variability. Based on underlying clinical conditions, each patient's sequence may consist of different sets of clinical events (observations, lab results, medications, procedures). Hence, simple population-wide models learned from event sequences for many different patients may not accurately predict patient-specific dynamics of event sequences and their differences. To address the problem, we propose and investigate multiple new event sequence prediction models and methods that let us better adjust the prediction for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#20540;&#30340;&#22686;&#37327;&#35889;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#38598;&#21010;&#20998;&#20026;&#23376;&#38598;&#24182;&#36827;&#34892;&#32858;&#31867;&#21644;&#21512;&#24182;&#65292;&#21487;&#20197;&#33719;&#24471;&#19982;&#32858;&#31867;&#25972;&#20010;&#25968;&#25454;&#38598;&#30456;&#36817;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.10999</link><description>&lt;p&gt;
&#22522;&#20110;&#29305;&#24449;&#20540;&#30340;&#22686;&#37327;&#35889;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Eigenvalue-based Incremental Spectral Clustering. (arXiv:2308.10999v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10999
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#20540;&#30340;&#22686;&#37327;&#35889;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#38598;&#21010;&#20998;&#20026;&#23376;&#38598;&#24182;&#36827;&#34892;&#32858;&#31867;&#21644;&#21512;&#24182;&#65292;&#21487;&#20197;&#33719;&#24471;&#19982;&#32858;&#31867;&#25972;&#20010;&#25968;&#25454;&#38598;&#30456;&#36817;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20043;&#21069;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#65288;&#30701;&#65289;&#25991;&#26723;&#30340;&#23376;&#38598;&#21512;&#65288;&#21253;&#21547;&#20960;&#30334;&#20010;&#26465;&#30446;&#65289;&#22312;&#32452;&#21512;&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#20540;&#35889;&#19978;&#26377;&#20849;&#21516;&#30340;&#24402;&#19968;&#21270;&#26041;&#24335;&#12290;&#22522;&#20110;&#36825;&#19968;&#27934;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#37327;&#35889;&#32858;&#31867;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#20197;&#19979;&#27493;&#39588;&#65306;&#65288;1&#65289;&#23558;&#25968;&#25454;&#21010;&#20998;&#20026;&#21487;&#31649;&#29702;&#30340;&#23376;&#38598;&#65292;&#65288;2&#65289;&#23545;&#27599;&#20010;&#23376;&#38598;&#36827;&#34892;&#32858;&#31867;&#65292;&#65288;3&#65289;&#22522;&#20110;&#29305;&#24449;&#20540;&#35889;&#30340;&#30456;&#20284;&#24615;&#21512;&#24182;&#26469;&#33258;&#19981;&#21516;&#23376;&#38598;&#30340;&#32858;&#31867;&#65292;&#24418;&#25104;&#25972;&#20010;&#25968;&#25454;&#38598;&#30340;&#32858;&#31867;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#29305;&#21035;&#36866;&#29992;&#20110;&#25968;&#25454;&#26679;&#26412;&#37327;&#22823;&#23567;&#21457;&#29983;&#24378;&#28872;&#21464;&#21270;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#20363;&#22914;&#20856;&#22411;&#30340;&#35889;&#32858;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#23454;&#38469;&#19978;&#23545;&#23376;&#38598;&#36827;&#34892;&#32858;&#31867;&#21644;&#21512;&#24182;&#21487;&#20197;&#24471;&#21040;&#19982;&#23545;&#25972;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#32858;&#31867;&#30456;&#36817;&#30340;&#32858;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our previous experiments demonstrated that subsets collections of (short) documents (with several hundred entries) share a common normalized in some way eigenvalue spectrum of combinatorial Laplacian. Based on this insight, we propose a method of incremental spectral clustering. The method consists of the following steps: (1) split the data into manageable subsets, (2) cluster each of the subsets, (3) merge clusters from different subsets based on the eigenvalue spectrum similarity to form clusters of the entire set. This method can be especially useful for clustering methods of complexity strongly increasing with the size of the data sample,like in case of typical spectral clustering. Experiments were performed showing that in fact the clustering and merging the subsets yields clusters close to clustering the entire dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#65288;MRF&#65289;&#27169;&#22411;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#22270;&#20687;&#19981;&#21516;&#21306;&#22495;&#30340;&#30456;&#23481;&#24615;&#65292;&#20197;&#38477;&#20302;&#29983;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2308.10997</link><description>&lt;p&gt;
SPEGTI: &#32467;&#26500;&#39044;&#27979;&#29992;&#20110;&#39640;&#25928;&#29983;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SPEGTI: Structured Prediction for Efficient Generative Text-to-Image Models. (arXiv:2308.10997v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#65288;MRF&#65289;&#27169;&#22411;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#22270;&#20687;&#19981;&#21516;&#21306;&#22495;&#30340;&#30456;&#23481;&#24615;&#65292;&#20197;&#38477;&#20302;&#29983;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#65292;&#26082;&#36924;&#30495;&#21448;&#19982;&#25991;&#26412;&#25552;&#31034;&#30456;&#31526;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#36136;&#37327;&#38656;&#35201;&#20184;&#20986;&#24040;&#22823;&#30340;&#35745;&#31639;&#25104;&#26412;&#65306;&#20960;&#20046;&#25152;&#26377;&#36825;&#20123;&#27169;&#22411;&#37117;&#26159;&#36845;&#20195;&#24335;&#30340;&#65292;&#38656;&#35201;&#22810;&#27425;&#36816;&#34892;&#25512;&#26029;&#65292;&#24182;&#20351;&#29992;&#22823;&#27169;&#22411;&#12290;&#36825;&#31181;&#36845;&#20195;&#36807;&#31243;&#26159;&#20026;&#20102;&#30830;&#20445;&#22270;&#20687;&#30340;&#19981;&#21516;&#21306;&#22495;&#19981;&#20165;&#19982;&#25991;&#26412;&#25552;&#31034;&#23545;&#40784;&#65292;&#36824;&#19982;&#20854;&#20182;&#21306;&#22495;&#30456;&#23481;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#22270;&#20687;&#19981;&#21516;&#21306;&#22495;&#30340;&#30456;&#23481;&#24615;&#65292;&#20351;&#29992;&#20102;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#65288;MRF&#65289;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#19982;&#26368;&#36817;&#25552;&#20986;&#30340;Muse&#27169;&#22411;&#37197;&#21512;&#20351;&#29992;&#12290;MRF&#32534;&#30721;&#20102;&#19981;&#21516;&#31354;&#38388;&#20301;&#32622;&#30340;&#22270;&#20687;&#26631;&#35760;&#20043;&#38388;&#30340;&#30456;&#23481;&#24615;&#65292;&#24182;&#19988;&#20351;&#25105;&#20204;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#25152;&#38656;&#30340;Muse&#39044;&#27979;&#27493;&#39588;&#12290;&#20351;&#29992;MRF&#30340;&#25512;&#26029;&#25104;&#26412;&#22823;&#22823;&#38477;&#20302;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#24555;&#36895;&#23398;&#20064;&#20854;&#21442;&#25968;&#65292;&#36890;&#36807;&#23545;MRF&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern text-to-image generation models produce high-quality images that are both photorealistic and faithful to the text prompts. However, this quality comes at significant computational cost: nearly all of these models are iterative and require running inference multiple times with large models. This iterative process is needed to ensure that different regions of the image are not only aligned with the text prompt, but also compatible with each other. In this work, we propose a light-weight approach to achieving this compatibility between different regions of an image, using a Markov Random Field (MRF) model. This method is shown to work in conjunction with the recently proposed Muse model. The MRF encodes the compatibility among image tokens at different spatial locations and enables us to significantly reduce the required number of Muse prediction steps. Inference with the MRF is significantly cheaper, and its parameters can be quickly learned through back-propagation by modeling MR
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#25918;&#26494;A*&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#35268;&#21017;&#32593;&#26684;&#22320;&#22270;&#20013;&#30340;&#26368;&#30701;&#36335;&#24452;&#38382;&#39064;&#12290;&#30456;&#36739;&#20110;&#29616;&#26377;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#26102;&#38388;&#21644;&#20869;&#23384;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#21508;&#31181;&#31867;&#22411;&#21644;&#22823;&#23567;&#30340;&#22320;&#22270;&#19978;&#37117;&#34920;&#29616;&#20986;&#39640;&#25928;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.10988</link><description>&lt;p&gt;
ERA*: &#25913;&#36827;&#30340;&#25918;&#26494;A*&#31639;&#27861;&#29992;&#20110;&#35299;&#20915;&#35268;&#21017;&#32593;&#26684;&#22320;&#22270;&#20013;&#30340;&#26368;&#30701;&#36335;&#24452;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
ERA*: Enhanced Relaxed A* algorithm for Solving the Shortest Path Problem in Regular Grid Maps. (arXiv:2308.10988v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10988
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#25918;&#26494;A*&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#35268;&#21017;&#32593;&#26684;&#22320;&#22270;&#20013;&#30340;&#26368;&#30701;&#36335;&#24452;&#38382;&#39064;&#12290;&#30456;&#36739;&#20110;&#29616;&#26377;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#26102;&#38388;&#21644;&#20869;&#23384;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#21508;&#31181;&#31867;&#22411;&#21644;&#22823;&#23567;&#30340;&#22320;&#22270;&#19978;&#37117;&#34920;&#29616;&#20986;&#39640;&#25928;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38745;&#24577;&#35268;&#21017;8&#37051;&#25509;&#36830;&#36890;&#65288;G8&#65289;&#32593;&#26684;&#20013;&#30340;&#28857;&#23545;&#28857;&#26368;&#30701;&#36335;&#24452;&#38382;&#39064;&#12290;&#36825;&#20010;&#31639;&#27861;&#21487;&#20197;&#35270;&#20026;&#23558;Hadlock&#31639;&#27861;&#25512;&#24191;&#21040;G8&#32593;&#26684;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#22312;&#25152;&#25552;&#20379;&#30340;&#35299;&#30340;&#36335;&#24452;&#38271;&#24230;&#26041;&#38754;&#22312;&#29702;&#35770;&#19978;&#31561;&#20215;&#20110;&#25918;&#26494;A* (RA*)&#31639;&#27861;&#65292;&#20294;&#30001;&#20110;&#23436;&#20840;&#19981;&#21516;&#30340;&#35745;&#31639;&#31574;&#30053;&#65288;&#22522;&#20110;&#23450;&#20041;&#19968;&#32452;&#26597;&#25214;&#30697;&#38453;&#65289;&#65292;&#33021;&#22815;&#33410;&#30465;&#22823;&#37327;&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;&#31867;&#22411;&#21644;&#22823;&#23567;&#30340;&#32593;&#26684;&#22320;&#22270;&#36827;&#34892;&#23454;&#39564;&#30740;&#31350;&#65288;&#22312;43&#20010;&#22320;&#22270;&#19978;&#36827;&#34892;&#20102;1290&#27425;&#36816;&#34892;&#65289;&#65292;&#24179;&#22343;&#35777;&#26126;&#23427;&#27604;RA*&#24555;2.25&#20493;&#65292;&#27604;&#21407;&#22987;A*&#24555;17&#20493;&#12290;&#27492;&#22806;&#65292;&#23427;&#26356;&#33410;&#30465;&#20869;&#23384;&#65292;&#22240;&#20026;&#19981;&#38656;&#35201;&#23384;&#20648;G&#24471;&#20998;&#30697;&#38453;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel algorithm for solving the point-to-point shortest path problem in a static regular 8-neighbor connectivity (G8) grid. This algorithm can be seen as a generalization of Hadlock algorithm to G8 grids, and is shown to be theoretically equivalent to the relaxed $A^*$ ($RA^*$) algorithm in terms of the provided solution's path length, but with substantial time and memory savings, due to a completely different computation strategy, based on defining a set of lookup matrices. Through an experimental study on grid maps of various types and sizes (1290 runs on 43 maps), it is proven to be 2.25 times faster than $RA^*$ and 17 times faster than the original $A^*$, in average. Moreover, it is more memory-efficient, since it does not need to store a G score matrix.
&lt;/p&gt;</description></item><item><title>"&#24341;&#29992;GPT&#30340;&#8220;&#35930;&#40736;&#35797;&#39564;&#8221;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#26234;&#33021;&#20195;&#29702;&#24314;&#27169;&#26041;&#27861;&#65292;&#21033;&#29992;&#26234;&#33021;&#20195;&#29702;&#20195;&#34920;&#20225;&#19994;&#36827;&#34892;&#31454;&#20105;&#21644;&#21246;&#32467;&#30740;&#31350;&#12290;&#23427;&#27604;&#20351;&#29992;&#20154;&#31867;&#20027;&#20307;&#36827;&#34892;&#23454;&#39564;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#21644;&#28789;&#27963;&#24615;&#65292;&#24182;&#23637;&#29616;&#20986;&#36229;&#36234;&#20256;&#32479;&#20195;&#29702;&#24314;&#27169;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;"</title><link>http://arxiv.org/abs/2308.10974</link><description>&lt;p&gt;
"&#24341;&#29992;GPT&#30340;&#8220;&#35930;&#40736;&#35797;&#39564;&#8221;&#65306;&#19968;&#31181;&#30740;&#31350;&#20225;&#19994;&#31454;&#20105;&#21644;&#21246;&#32467;&#30340;&#21019;&#26032;&#26234;&#33021;&#20195;&#29702;&#24314;&#27169;&#26041;&#27861;"
&lt;/p&gt;
&lt;p&gt;
"Guinea Pig Trials" Utilizing GPT: A Novel Smart Agent-Based Modeling Approach for Studying Firm Competition and Collusion. (arXiv:2308.10974v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10974
&lt;/p&gt;
&lt;p&gt;
"&#24341;&#29992;GPT&#30340;&#8220;&#35930;&#40736;&#35797;&#39564;&#8221;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#26234;&#33021;&#20195;&#29702;&#24314;&#27169;&#26041;&#27861;&#65292;&#21033;&#29992;&#26234;&#33021;&#20195;&#29702;&#20195;&#34920;&#20225;&#19994;&#36827;&#34892;&#31454;&#20105;&#21644;&#21246;&#32467;&#30740;&#31350;&#12290;&#23427;&#27604;&#20351;&#29992;&#20154;&#31867;&#20027;&#20307;&#36827;&#34892;&#23454;&#39564;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#21644;&#28789;&#27963;&#24615;&#65292;&#24182;&#23637;&#29616;&#20986;&#36229;&#36234;&#20256;&#32479;&#20195;&#29702;&#24314;&#27169;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;"
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20225;&#19994;&#31454;&#20105;&#21644;&#21246;&#32467;&#28041;&#21450;&#22797;&#26434;&#30340;&#21160;&#24577;&#65292;&#23588;&#20854;&#26159;&#32771;&#34385;&#21040;&#20225;&#19994;&#20043;&#38388;&#30340;&#27807;&#36890;&#12290;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#34987;&#24314;&#27169;&#20026;&#22797;&#26434;&#31995;&#32479;&#30340;&#38382;&#39064;&#65292;&#20256;&#32479;&#19978;&#36890;&#36807;&#28041;&#21450;&#20154;&#31867;&#20027;&#20307;&#25110;&#22522;&#20110;&#20195;&#29702;&#30340;&#24314;&#27169;&#26041;&#27861;&#36827;&#34892;&#25506;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#26234;&#33021;&#20195;&#29702;&#24314;&#27169;&#65288;SABM&#65289;&#65292;&#20854;&#20013;&#30001;GPT-4&#25216;&#26415;&#25903;&#25345;&#30340;&#26234;&#33021;&#20195;&#29702;&#20195;&#34920;&#20225;&#19994;&#24182;&#30456;&#20114;&#20132;&#20114;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#25511;&#21046;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#26465;&#20214;&#19979;&#20225;&#19994;&#20215;&#26684;&#31454;&#20105;&#21644;&#21246;&#32467;&#34892;&#20026;&#12290;&#19982;&#20351;&#29992;&#20154;&#31867;&#20027;&#20307;&#36827;&#34892;&#23454;&#39564;&#30456;&#27604;&#65292;SABM&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#21644;&#28789;&#27963;&#24615;&#12290;&#26234;&#33021;&#20195;&#29702;&#25317;&#26377;&#20915;&#31574;&#30340;&#24191;&#27867;&#30693;&#35782;&#24211;&#65292;&#23637;&#29616;&#20986;&#31867;&#20284;&#20154;&#31867;&#30340;&#25112;&#30053;&#33021;&#21147;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;&#24314;&#27169;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#26234;&#33021;&#20195;&#29702;&#33021;&#22815;&#27169;&#25311;&#20154;&#31867;&#23545;&#35805;&#24182;&#20010;&#24615;&#21270;&#65292;&#20351;&#20854;&#25104;&#20026;&#30740;&#31350;&#28041;&#21450;&#27807;&#36890;&#30340;&#22797;&#26434;&#24773;&#20917;&#30340;&#29702;&#24819;&#36873;&#25321;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Firm competition and collusion involve complex dynamics, particularly when considering communication among firms. Such issues can be modeled as problems of complex systems, traditionally approached through experiments involving human subjects or agent-based modeling methods. We propose an innovative framework called Smart Agent-Based Modeling (SABM), wherein smart agents, supported by GPT-4 technologies, represent firms, and interact with one another. We conducted a controlled experiment to study firm price competition and collusion behaviors under various conditions. SABM is more cost-effective and flexible compared to conducting experiments with human subjects. Smart agents possess an extensive knowledge base for decision-making and exhibit human-like strategic abilities, surpassing traditional ABM agents. Furthermore, smart agents can simulate human conversation and be personalized, making them ideal for studying complex situations involving communication. Our results demonstrate th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DocPrompt&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#25991;&#26723;&#38382;&#31572;&#20219;&#21153;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DocPrompt&#27169;&#22411;&#32463;&#36807;&#36830;&#32493;&#39044;&#35757;&#32451;&#21518;&#22312;&#25991;&#26723;&#38382;&#31572;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#20132;&#20184;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#65292;&#38477;&#20302;&#20102;&#27880;&#37322;&#25104;&#26412;&#21644;&#21171;&#21160;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2308.10959</link><description>&lt;p&gt;
DocPrompt: &#22823;&#35268;&#27169;&#36830;&#32493;&#39044;&#35757;&#32451;&#29992;&#20110;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#25991;&#26723;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
DocPrompt: Large-scale continue pretrain for zero-shot and few-shot document question answering. (arXiv:2308.10959v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10959
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DocPrompt&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#25991;&#26723;&#38382;&#31572;&#20219;&#21153;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DocPrompt&#27169;&#22411;&#32463;&#36807;&#36830;&#32493;&#39044;&#35757;&#32451;&#21518;&#22312;&#25991;&#26723;&#38382;&#31572;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#20132;&#20184;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#65292;&#38477;&#20302;&#20102;&#27880;&#37322;&#25104;&#26412;&#21644;&#21171;&#21160;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DocPrompt&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#25991;&#26723;&#38382;&#31572;&#20219;&#21153;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24369;&#30417;&#30563;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#12289;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#38454;&#27573;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#39062;&#30340;&#29702;&#35299;&#27169;&#22411;&#21644;&#29983;&#25104;&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25991;&#26723;&#38382;&#31572;&#20219;&#21153;&#20013;&#65292;&#32463;&#36807;&#36830;&#32493;&#39044;&#35757;&#32451;&#30340;DocPrompt&#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#24378;&#22522;&#32447;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#25991;&#26723;&#38382;&#31572;&#23458;&#25143;&#39033;&#30446;&#30340;&#20132;&#20184;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#65292;&#38477;&#20302;&#20102;&#27880;&#37322;&#25104;&#26412;&#21644;&#21171;&#21160;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#28436;&#31034;&#21487;&#20197;&#22312;https://huggingface.co/spaces/PaddlePaddle/ERNIE-Layout&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose Docprompt for document question answering tasks with powerful zero-shot and few-shot performance. We proposed a novel weakly supervised data generation method, a novel multl-stage training method and a novel understanding model &amp; generation model ensemble method. Experiment results show that the Docprompt model after continue pretrain significantly outperforms the existing strong baseline models on document question answering tasks. This method greatly improves the delivery efficiency and model performance of document question answering customer projects, reducing annotation costs and labor costs. Our demo can be found at https://huggingface.co/spaces/PaddlePaddle/ERNIE-Layout.
&lt;/p&gt;</description></item><item><title>DataVinci&#26159;&#19968;&#20010;&#20840;&#33258;&#21160;&#30340;&#26080;&#30417;&#30563;&#23383;&#31526;&#20018;&#25968;&#25454;&#38169;&#35823;&#26816;&#27979;&#21644;&#20462;&#22797;&#31995;&#32479;&#65292;&#21487;&#23398;&#20064;&#21477;&#27861;&#21644;&#35821;&#20041;&#27169;&#24335;&#65292;&#24182;&#33258;&#21160;&#25512;&#23548;&#20986;&#23545;&#25968;&#25454;&#38169;&#35823;&#30340;&#20462;&#22797;&#12290;</title><link>http://arxiv.org/abs/2308.10922</link><description>&lt;p&gt;
DataVinci: &#23398;&#20064;&#21477;&#27861;&#21644;&#35821;&#20041;&#23383;&#31526;&#20018;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
DataVinci: Learning Syntactic and Semantic String Repairs. (arXiv:2308.10922v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10922
&lt;/p&gt;
&lt;p&gt;
DataVinci&#26159;&#19968;&#20010;&#20840;&#33258;&#21160;&#30340;&#26080;&#30417;&#30563;&#23383;&#31526;&#20018;&#25968;&#25454;&#38169;&#35823;&#26816;&#27979;&#21644;&#20462;&#22797;&#31995;&#32479;&#65292;&#21487;&#23398;&#20064;&#21477;&#27861;&#21644;&#35821;&#20041;&#27169;&#24335;&#65292;&#24182;&#33258;&#21160;&#25512;&#23548;&#20986;&#23545;&#25968;&#25454;&#38169;&#35823;&#30340;&#20462;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23383;&#31526;&#20018;&#25968;&#25454;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#20013;&#24456;&#24120;&#35265;&#65306;&#20174;&#32593;&#32476;&#19978;&#37319;&#26679;&#30340;180&#19975;&#20010;&#30495;&#23454;Excel&#30005;&#23376;&#34920;&#26684;&#20013;&#65292;&#26377;67.6%&#30340;&#20540;&#34920;&#31034;&#20026;&#25991;&#26412;&#12290;&#25104;&#21151;&#28165;&#29702;&#36825;&#31181;&#23383;&#31526;&#20018;&#25968;&#25454;&#30340;&#31995;&#32479;&#23545;&#23454;&#38469;&#29992;&#25143;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#23613;&#31649;&#20197;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#25506;&#32034;&#20102;&#23383;&#31526;&#20018;&#25968;&#25454;&#20013;&#30340;&#38169;&#35823;&#65292;&#20294;&#26159;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#24120;&#23616;&#38480;&#20110;&#38169;&#35823;&#26816;&#27979;&#65292;&#25110;&#32773;&#38656;&#35201;&#29992;&#25143;&#25552;&#20379;&#27880;&#37322;&#12289;&#31034;&#20363;&#25110;&#32422;&#26463;&#26469;&#20462;&#22797;&#38169;&#35823;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#31995;&#32479;&#29420;&#31435;&#22320;&#20851;&#27880;&#20110;&#23383;&#31526;&#20018;&#20013;&#30340;&#21477;&#27861;&#38169;&#35823;&#25110;&#35821;&#20041;&#38169;&#35823;&#65292;&#32780;&#24573;&#30053;&#20102;&#23383;&#31526;&#20018;&#36890;&#24120;&#21516;&#26102;&#21253;&#21547;&#21477;&#27861;&#21644;&#35821;&#20041;&#23376;&#23383;&#31526;&#20018;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;DataVinci&#65292;&#19968;&#20010;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#23383;&#31526;&#20018;&#25968;&#25454;&#38169;&#35823;&#26816;&#27979;&#21644;&#20462;&#22797;&#31995;&#32479;&#12290;DataVinci&#23398;&#20064;&#22522;&#20110;&#27491;&#21017;&#34920;&#36798;&#24335;&#30340;&#27169;&#24335;&#65292;&#35206;&#30422;&#20102;&#21015;&#20013;&#22823;&#22810;&#25968;&#30340;&#20540;&#65292;&#24182;&#23558;&#19981;&#28385;&#36275;&#36825;&#20123;&#27169;&#24335;&#30340;&#20540;&#25253;&#21578;&#20026;&#25968;&#25454;&#38169;&#35823;&#12290;DataVinci&#21487;&#20197;&#26681;&#25454;&#20027;&#35201;&#27169;&#24335;&#21644;&#32422;&#26463;&#33258;&#21160;&#25512;&#23548;&#20986;&#23545;&#25968;&#25454;&#38169;&#35823;&#30340;&#20462;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
String data is common in real-world datasets: 67.6% of values in a sample of 1.8 million real Excel spreadsheets from the web were represented as text. Systems that successfully clean such string data can have a significant impact on real users. While prior work has explored errors in string data, proposed approaches have often been limited to error detection or require that the user provide annotations, examples, or constraints to fix the errors. Furthermore, these systems have focused independently on syntactic errors or semantic errors in strings, but ignore that strings often contain both syntactic and semantic substrings. We introduce DataVinci, a fully unsupervised string data error detection and repair system. DataVinci learns regular-expression-based patterns that cover a majority of values in a column and reports values that do not satisfy such patterns as data errors. DataVinci can automatically derive edits to the data error based on the majority patterns and constraints lea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22522;&#20110;Metapath&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#29992;&#20110;&#22270;&#24322;&#24120;&#26816;&#27979;&#12290;&#36890;&#36807;&#22312;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#20013;&#20351;&#29992;GCN&#23618;&#26469;&#26377;&#25928;&#20256;&#25773;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20197;&#21450;&#29305;&#21035;&#35774;&#35745;&#30340;&#24322;&#24120;&#31038;&#21306;&#65292;&#35813;&#26041;&#27861;&#22312;&#32467;&#26500;&#21644;&#23646;&#24615;&#24046;&#24322;&#30340;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#24605;&#36335;&#21644;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.10918</link><description>&lt;p&gt;
&#22522;&#20110;Metapath&#30340;&#19978;&#19979;&#25991;&#30693;&#35782;&#30340;&#28145;&#24230;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deep Semi-supervised Anomaly Detection with Metapath-based Context Knowledge. (arXiv:2308.10918v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22522;&#20110;Metapath&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#29992;&#20110;&#22270;&#24322;&#24120;&#26816;&#27979;&#12290;&#36890;&#36807;&#22312;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#20013;&#20351;&#29992;GCN&#23618;&#26469;&#26377;&#25928;&#20256;&#25773;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20197;&#21450;&#29305;&#21035;&#35774;&#35745;&#30340;&#24322;&#24120;&#31038;&#21306;&#65292;&#35813;&#26041;&#27861;&#22312;&#32467;&#26500;&#21644;&#23646;&#24615;&#24046;&#24322;&#30340;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#24605;&#36335;&#21644;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24322;&#24120;&#26816;&#27979;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;Metapath&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#20043;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#22522;&#20110;Metapath&#30340;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#65288;MSAD&#65289;&#65292;&#22312;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#20013;&#37117;&#20351;&#29992;GCN&#23618;&#26469;&#26377;&#25928;&#22320;&#20256;&#25773;&#24322;&#24120;&#21644;&#27491;&#24120;&#33410;&#28857;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#22522;&#20110;Metapath&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#35774;&#35745;&#21644;&#29305;&#21035;&#31934;&#24515;&#35774;&#35745;&#30340;&#24322;&#24120;&#31038;&#21306;&#22686;&#24378;&#20102;&#20840;&#23616;&#21644;&#23616;&#37096;&#32467;&#26500;&#21644;&#23646;&#24615;&#24046;&#24322;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#36890;&#36807;&#22312;&#19971;&#20010;&#30495;&#23454;&#32593;&#32476;&#19978;&#36827;&#34892;&#30340;&#19968;&#31995;&#21015;&#32508;&#21512;&#23454;&#39564;&#65292;&#26412;&#25991;&#35777;&#26126;&#20102;MSAD&#26041;&#27861;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#30340;&#20248;&#36234;&#24615;&#12290;&#26412;&#30740;&#31350;&#30340;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#37325;&#28857;&#26159;&#20248;&#21270;&#21644;&#20998;&#26512;Metapath&#27169;&#24335;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph anomaly detection has attracted considerable attention in recent years. This paper introduces a novel approach that leverages metapath-based semi-supervised learning, addressing the limitations of previous methods. We present a new framework, Metapath-based Semi-supervised Anomaly Detection (MSAD), incorporating GCN layers in both the encoder and decoder to efficiently propagate context information between abnormal and normal nodes. The design of metapath-based context information and a specifically crafted anomaly community enhance the process of learning differences in structures and attributes, both globally and locally. Through a comprehensive set of experiments conducted on seven real-world networks, this paper demonstrates the superiority of the MSAD method compared to state-of-the-art techniques. The promising results of this study pave the way for future investigations, focusing on the optimization and analysis of metapath patterns to further enhance the effectiveness of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#30340;&#30693;&#35782;&#36827;&#34892;&#35782;&#21035;&#20219;&#21153;&#30340;&#26032;&#30340;&#30693;&#35782;&#20256;&#36882;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;RepFusion&#65292;&#25105;&#20204;&#20174;&#39044;&#20808;&#35757;&#32451;&#30340;DPMs&#20013;&#25552;&#21462;&#34920;&#31034;&#24182;&#23558;&#20854;&#20316;&#20026;&#23398;&#29983;&#32593;&#32476;&#30340;&#30417;&#30563;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26469;&#30830;&#23450;&#26368;&#20339;&#30340;&#26102;&#38388;&#27493;&#39588;&#12290;</title><link>http://arxiv.org/abs/2308.10916</link><description>&lt;p&gt;
&#25454;&#20256;&#32479;&#27169;&#22411;&#20316;&#20026;&#34920;&#31034;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Diffusion Model as Representation Learner. (arXiv:2308.10916v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#30340;&#30693;&#35782;&#36827;&#34892;&#35782;&#21035;&#20219;&#21153;&#30340;&#26032;&#30340;&#30693;&#35782;&#20256;&#36882;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;RepFusion&#65292;&#25105;&#20204;&#20174;&#39044;&#20808;&#35757;&#32451;&#30340;DPMs&#20013;&#25552;&#21462;&#34920;&#31034;&#24182;&#23558;&#20854;&#20316;&#20026;&#23398;&#29983;&#32593;&#32476;&#30340;&#30417;&#30563;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26469;&#30830;&#23450;&#26368;&#20339;&#30340;&#26102;&#38388;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#22312;&#21508;&#31181;&#29983;&#25104;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#39044;&#20808;&#35757;&#32451;&#30340;DPMs&#30340;&#23398;&#20064;&#34920;&#31034;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;DPMs&#30340;&#34920;&#31034;&#21147;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30693;&#35782;&#20256;&#36882;&#26041;&#27861;&#65292;&#21033;&#29992;&#29983;&#25104;&#24615;DPMs&#33719;&#24471;&#30340;&#30693;&#35782;&#36827;&#34892;&#35782;&#21035;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20174;&#30740;&#31350;DPMs&#30340;&#29305;&#24449;&#31354;&#38388;&#24320;&#22987;&#65292;&#25581;&#31034;&#20102;DPMs&#20316;&#20026;&#21435;&#22122;&#33258;&#32534;&#30721;&#22120;&#23545;&#34920;&#31034;&#23398;&#20064;&#21644;&#27169;&#22411;&#23481;&#37327;&#36827;&#34892;&#24179;&#34913;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;RepFusion&#30340;&#26032;&#30340;&#30693;&#35782;&#20256;&#36882;&#33539;&#24335;&#12290;&#25105;&#20204;&#30340;&#27169;&#24335;&#20174;DPMs&#20013;&#25552;&#21462;&#19981;&#21516;&#26102;&#38388;&#27493;&#30340;&#34920;&#31034;&#65292;&#24182;&#23558;&#23427;&#20204;&#21160;&#24577;&#22320;&#29992;&#20316;&#23398;&#29983;&#32593;&#32476;&#30340;&#30417;&#30563;&#65292;&#24182;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#30830;&#23450;&#26368;&#20339;&#26102;&#38388;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22270;&#20687;&#20998;&#31867;&#12289;&#35821;&#20041;
&lt;/p&gt;
&lt;p&gt;
Diffusion Probabilistic Models (DPMs) have recently demonstrated impressive results on various generative tasks.Despite its promises, the learned representations of pre-trained DPMs, however, have not been fully understood. In this paper, we conduct an in-depth investigation of the representation power of DPMs, and propose a novel knowledge transfer method that leverages the knowledge acquired by generative DPMs for recognition tasks. Our study begins by examining the feature space of DPMs, revealing that DPMs are inherently denoising autoencoders that balance the representation learning with regularizing model capacity. To this end, we introduce a novel knowledge transfer paradigm named RepFusion. Our paradigm extracts representations at different time steps from off-the-shelf DPMs and dynamically employs them as supervision for student networks, in which the optimal time is determined through reinforcement learning. We evaluate our approach on several image classification, semantic s
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#30740;&#31350;&#20102;&#20986;&#29616;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#35745;&#31639;&#19981;&#21487;&#31616;&#21270;&#24615;&#30340;&#27010;&#24565;&#65292;&#36825;&#26159;&#33021;&#22815;&#20174;&#23458;&#35266;&#35282;&#24230;&#29702;&#35299;&#20986;&#29616;&#29616;&#35937;&#30340;&#20851;&#38190;&#12290;</title><link>http://arxiv.org/abs/2308.10912</link><description>&lt;p&gt;
&#35299;&#37322;&#20986;&#29616;.&#65288;arXiv:2308.10912v1 [cs.CC]&#65289;
&lt;/p&gt;
&lt;p&gt;
Explaining Emergence. (arXiv:2308.10912v1 [cs.CC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10912
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30740;&#31350;&#20102;&#20986;&#29616;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#35745;&#31639;&#19981;&#21487;&#31616;&#21270;&#24615;&#30340;&#27010;&#24565;&#65292;&#36825;&#26159;&#33021;&#22815;&#20174;&#23458;&#35266;&#35282;&#24230;&#29702;&#35299;&#20986;&#29616;&#29616;&#35937;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20986;&#29616;&#26159;&#21508;&#20010;&#39046;&#22495;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#24615;&#36136;&#12290;&#23427;&#26159;&#19968;&#20010;&#29616;&#35937;&#20986;&#29616;&#24471;&#20196;&#20154;&#24778;&#35766;&#65292;&#24182;&#19988;&#20284;&#20046;&#19981;&#33021;&#39044;&#27979;&#20854;&#20986;&#29616;&#30340;&#20107;&#23454;&#12290;&#36825;&#20063;&#26159;&#20026;&#20160;&#20040;&#32463;&#24120;&#35828;&#20986;&#29616;&#26159;&#30456;&#23545;&#20110;&#35266;&#23519;&#32773;&#32780;&#35328;&#30340;&#20027;&#35266;&#23646;&#24615;&#12290;&#19968;&#20123;&#20855;&#26377;&#31616;&#21333;&#32780;&#30830;&#23450;&#24615;&#35268;&#21017;&#30340;&#25968;&#23398;&#31995;&#32479;&#21364;&#34920;&#29616;&#20986;&#20986;&#29616;&#34892;&#20026;&#12290;&#30740;&#31350;&#36825;&#20123;&#31995;&#32479;&#20026;&#36825;&#20010;&#20027;&#39064;&#24102;&#26469;&#20102;&#26032;&#30340;&#35270;&#35282;&#65292;&#24182;&#19988;&#23450;&#20041;&#20102;&#19968;&#20010;&#26032;&#27010;&#24565;&#65292;&#21363;&#35745;&#31639;&#19981;&#21487;&#31616;&#21270;&#24615;&#65292;&#23427;&#28041;&#21450;&#21040;&#34892;&#20026;&#30340;&#23436;&#20840;&#30830;&#23450;&#24615;&#65292;&#20294;&#26080;&#27861;&#22312;&#19981;&#27169;&#25311;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;&#35745;&#31639;&#19981;&#21487;&#31616;&#21270;&#24615;&#26159;&#20174;&#23458;&#35266;&#35282;&#24230;&#29702;&#35299;&#20986;&#29616;&#29616;&#35937;&#30340;&#20851;&#38190;&#65292;&#23427;&#19981;&#38656;&#35201;&#25552;&#21450;&#20219;&#20309;&#35266;&#23519;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emergence is a pregnant property in various fields. It is the fact for a phenomenon to appear surprisingly and to be such that it seems at first sight that it is not possible to predict its apparition. That is the reason why it has often been said that emergence is a subjective property relative to the observer. Some mathematical systems having very simple and deterministic rules nevertheless show emergent behavior. Studying these systems shed a new light on the subject and allows to define a new concept, computational irreducibility, which deals with behaviors that even though they are totally deterministic cannot be predicted without simulating them. Computational irreducibility is then a key for understanding emergent phenomena from an objective point of view that does not need the mention of any observer.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Fed-PMG&#30340;&#32852;&#21512;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20266;&#27169;&#24577;&#29983;&#25104;&#26426;&#21046;&#35299;&#20915;&#20102;&#32852;&#21512;&#22810;&#27169;&#24577;MRI&#37325;&#24314;&#20013;&#30340;&#27169;&#24577;&#32570;&#22833;&#38382;&#39064;&#65292;&#20197;&#25552;&#39640;&#36890;&#20449;&#25928;&#29575;&#12290;&#36890;&#36807;&#20849;&#20139;&#39057;&#29575;&#31354;&#38388;&#20013;&#24133;&#24230;&#35889;&#30340;&#20998;&#24067;&#20449;&#24687;&#24674;&#22797;&#20002;&#22833;&#30340;&#27169;&#24577;&#65292;&#36991;&#20813;&#20102;&#25104;&#23545;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2308.10910</link><description>&lt;p&gt;
&#19981;&#23436;&#25972;&#22810;&#27169;&#24577;MRI&#37325;&#24314;&#30340;&#32852;&#21512;&#20266;&#27169;&#24577;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Federated Pseudo Modality Generation for Incomplete Multi-Modal MRI Reconstruction. (arXiv:2308.10910v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10910
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Fed-PMG&#30340;&#32852;&#21512;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20266;&#27169;&#24577;&#29983;&#25104;&#26426;&#21046;&#35299;&#20915;&#20102;&#32852;&#21512;&#22810;&#27169;&#24577;MRI&#37325;&#24314;&#20013;&#30340;&#27169;&#24577;&#32570;&#22833;&#38382;&#39064;&#65292;&#20197;&#25552;&#39640;&#36890;&#20449;&#25928;&#29575;&#12290;&#36890;&#36807;&#20849;&#20139;&#39057;&#29575;&#31354;&#38388;&#20013;&#24133;&#24230;&#35889;&#30340;&#20998;&#24067;&#20449;&#24687;&#24674;&#22797;&#20002;&#22833;&#30340;&#27169;&#24577;&#65292;&#36991;&#20813;&#20102;&#25104;&#23545;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22810;&#27169;&#24577;&#23398;&#20064;&#22312;MRI&#37325;&#24314;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#23427;&#20381;&#36182;&#20110;&#38590;&#20197;&#22312;&#23454;&#38469;&#20020;&#24202;&#22330;&#26223;&#20013;&#33719;&#21462;&#30340;&#25104;&#23545;&#22810;&#27169;&#24577;&#25968;&#25454;&#12290;&#29305;&#21035;&#26159;&#22312;&#32852;&#21512;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#65292;&#24120;&#35265;&#24773;&#20917;&#26159;&#20960;&#20010;&#21307;&#30103;&#26426;&#26500;&#21482;&#26377;&#21333;&#27169;&#24577;&#25968;&#25454;&#65292;&#21363;&#31216;&#20026;&#27169;&#24577;&#32570;&#22833;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#31181;&#26465;&#20214;&#19979;&#26080;&#27861;&#37096;&#32626;&#26631;&#20934;&#30340;&#32852;&#21512;&#23398;&#20064;&#26694;&#26550;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36890;&#20449;&#39640;&#25928;&#30340;&#32852;&#21512;&#23398;&#20064;&#26694;&#26550;&#65292;&#21363;Fed-PMG&#65292;&#20197;&#35299;&#20915;&#32852;&#21512;&#22810;&#27169;&#24577;MRI&#37325;&#24314;&#20013;&#30340;&#27169;&#24577;&#32570;&#22833;&#25361;&#25112;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#31181;&#20266;&#27169;&#24577;&#29983;&#25104;&#26426;&#21046;&#36890;&#36807;&#22312;&#39057;&#29575;&#31354;&#38388;&#20013;&#20849;&#20139;&#24133;&#24230;&#35889;&#30340;&#20998;&#24067;&#20449;&#24687;&#26469;&#24674;&#22797;&#27599;&#20010;&#21333;&#27169;&#24577;&#23458;&#25143;&#31471;&#30340;&#20002;&#22833;&#27169;&#24577;&#12290;&#28982;&#32780;&#65292;&#20849;&#20139;&#21407;&#22987;&#24133;&#24230;&#35889;&#30340;&#27493;&#39588;&#20250;&#23548;&#33268;&#27785;&#37325;&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;&#20026;&#20102;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32858;&#31867;&#26041;&#26696;&#26469;&#36827;&#34892;&#25237;&#24433;&#12290;
&lt;/p&gt;
&lt;p&gt;
While multi-modal learning has been widely used for MRI reconstruction, it relies on paired multi-modal data which is difficult to acquire in real clinical scenarios. Especially in the federated setting, the common situation is that several medical institutions only have single-modal data, termed the modality missing issue. Therefore, it is infeasible to deploy a standard federated learning framework in such conditions. In this paper, we propose a novel communication-efficient federated learning framework, namely Fed-PMG, to address the missing modality challenge in federated multi-modal MRI reconstruction. Specifically, we utilize a pseudo modality generation mechanism to recover the missing modality for each single-modal client by sharing the distribution information of the amplitude spectrum in frequency space. However, the step of sharing the original amplitude spectrum leads to heavy communication costs. To reduce the communication cost, we introduce a clustering scheme to project
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;TVM&#20013;8&#20301;&#37327;&#21270;&#30340;&#24615;&#33021;&#38382;&#39064;&#65292;&#24182;&#35752;&#35770;&#20102;&#20860;&#23481;&#24615;&#21644;&#20248;&#21270;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2308.10905</link><description>&lt;p&gt;
&#22312;TVM&#20013;&#20998;&#26512;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Analyzing Quantization in TVM. (arXiv:2308.10905v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10905
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;TVM&#20013;8&#20301;&#37327;&#21270;&#30340;&#24615;&#33021;&#38382;&#39064;&#65292;&#24182;&#35752;&#35770;&#20102;&#20860;&#23481;&#24615;&#21644;&#20248;&#21270;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#23545;&#26435;&#37325;&#24352;&#37327;&#36827;&#34892;&#37327;&#21270;&#20197;&#20943;&#23569;&#25512;&#29702;&#24310;&#36831;&#21644;&#20869;&#23384;&#21344;&#29992;&#30340;&#30740;&#31350;&#24050;&#32463;&#26377;&#24456;&#22810;&#12290;TVM&#20063;&#20855;&#22791;&#25903;&#25345;&#20302;&#27604;&#29305;&#35745;&#31639;&#21644;&#37327;&#21270;&#26435;&#37325;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#36890;&#24120;&#26399;&#26395;&#36890;&#36807;&#37327;&#21270;&#26469;&#25552;&#39640;&#25512;&#29702;&#26102;&#38388;&#65292;&#22312;TVM&#20013;&#65292;8&#20301;&#37327;&#21270;&#30340;&#24615;&#33021;&#21364;&#19981;&#33021;&#28385;&#36275;&#26399;&#26395;&#12290;&#36890;&#24120;&#22312;&#23558;8&#20301;&#37327;&#21270;&#24212;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26102;&#65292;&#36890;&#24120;&#26399;&#26395;&#36798;&#21040;&#20840;&#31934;&#24230;&#25512;&#29702;&#26102;&#38388;&#30340;50%&#24038;&#21491;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#31181;&#29305;&#27530;&#24773;&#20917;&#19979;&#65292;&#37327;&#21270;&#29256;&#26412;&#19981;&#20165;&#26410;&#33021;&#23454;&#29616;&#25152;&#26399;&#26395;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#32780;&#19988;&#23454;&#38469;&#19978;&#24615;&#33021;&#26356;&#24046;&#65292;&#23548;&#33268;&#25512;&#29702;&#26102;&#38388;&#32422;&#20026;&#38750;&#37327;&#21270;&#29256;&#26412;&#30340;&#20004;&#20493;&#24930;&#12290;&#22312;&#36825;&#20010;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#24615;&#33021;&#19981;&#20339;&#30340;&#21407;&#22240;&#65292;&#35780;&#20272;&#20102;8&#20301;&#37327;&#21270;&#22312;TVM&#20013;&#30340;&#20860;&#23481;&#24615;&#21644;&#20248;&#21270;&#26426;&#20250;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20004;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
There has been many papers in academic literature on quantizing weight tensors in deep learning models to reduce inference latency and memory footprint. TVM also has the ability to quantize weights and support low-bit computations. Although quantization is typically expected to improve inference time, in TVM, the performance of 8-bit quantization does not meet the expectations. Typically, when applying 8-bit quantization to a deep learning model, it is usually expected to achieve around 50% of the full-precision inference time. However, in this particular case, not only does the quantized version fail to achieve the desired performance boost, but it actually performs worse, resulting in an inference time that is about 2 times as slow as the non-quantized version. In this project, we thoroughly investigate the reasons behind the underperformance and assess the compatibility and optimization opportunities of 8-bit quantization in TVM. We discuss the optimization of two different types of
&lt;/p&gt;</description></item><item><title>DynED&#26159;&#19968;&#31181;&#21160;&#24577;&#38598;&#25104;&#22810;&#26679;&#21270;&#26041;&#27861;&#65292;&#22522;&#20110;MRR&#32467;&#21512;&#20102;&#32452;&#20214;&#30340;&#22810;&#26679;&#24615;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#22312;&#25968;&#25454;&#27969;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.10807</link><description>&lt;p&gt;
DynED: &#25968;&#25454;&#27969;&#20998;&#31867;&#20013;&#30340;&#21160;&#24577;&#38598;&#25104;&#22810;&#26679;&#21270;
&lt;/p&gt;
&lt;p&gt;
DynED: Dynamic Ensemble Diversification in Data Stream Classification. (arXiv:2308.10807v1 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10807
&lt;/p&gt;
&lt;p&gt;
DynED&#26159;&#19968;&#31181;&#21160;&#24577;&#38598;&#25104;&#22810;&#26679;&#21270;&#26041;&#27861;&#65292;&#22522;&#20110;MRR&#32467;&#21512;&#20102;&#32452;&#20214;&#30340;&#22810;&#26679;&#24615;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#22312;&#25968;&#25454;&#27969;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#25968;&#25454;&#20998;&#24067;&#30340;&#31361;&#21464;&#24615;&#21464;&#21270;&#65292;&#20063;&#31216;&#20026;&#27010;&#24565;&#28418;&#31227;&#65292;&#22312;&#25968;&#25454;&#27969;&#29615;&#22659;&#20013;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#38598;&#21512;&#26041;&#27861;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#20998;&#31867;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290; &#22312;&#38598;&#21512;&#20869;&#37096;&#30340;&#26356;&#22823;&#22810;&#26679;&#24615;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#23613;&#31649;&#38598;&#21512;&#20869;&#32452;&#20214;&#30340;&#22810;&#26679;&#24615;&#24456;&#39640;&#65292;&#20294;&#24182;&#19981;&#26159;&#25152;&#26377;&#32452;&#20214;&#37117;&#20687;&#39044;&#26399;&#30340;&#37027;&#26679;&#23545;&#25972;&#20307;&#24615;&#33021;&#26377;&#25152;&#36129;&#29486;&#12290;&#36825;&#38656;&#35201;&#19968;&#31181;&#26041;&#27861;&#26469;&#36873;&#25321;&#23637;&#29616;&#20986;&#39640;&#24615;&#33021;&#21644;&#22810;&#26679;&#24615;&#30340;&#32452;&#20214;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;MMR&#65288;&#26368;&#22823;&#36793;&#38469;&#30456;&#20851;&#24615;&#65289;&#30340;&#26032;&#22411;&#38598;&#21512;&#26500;&#24314;&#21644;&#32500;&#25252;&#26041;&#27861;&#65292;&#22312;&#32452;&#21512;&#38598;&#21512;&#30340;&#36807;&#31243;&#20013;&#21160;&#24577;&#22320;&#32467;&#21512;&#20102;&#32452;&#20214;&#30340;&#22810;&#26679;&#24615;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#22312;&#22235;&#20010;&#30495;&#23454;&#21644;11&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65288;DynED&#65289;&#30456;&#27604;&#20110;&#20116;&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#39640;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;
&lt;/p&gt;
&lt;p&gt;
Ensemble methods are commonly used in classification due to their remarkable performance. Achieving high accuracy in a data stream environment is a challenging task considering disruptive changes in the data distribution, also known as concept drift. A greater diversity of ensemble components is known to enhance prediction accuracy in such settings. Despite the diversity of components within an ensemble, not all contribute as expected to its overall performance. This necessitates a method for selecting components that exhibit high performance and diversity. We present a novel ensemble construction and maintenance approach based on MMR (Maximal Marginal Relevance) that dynamically combines the diversity and prediction accuracy of components during the process of structuring an ensemble. The experimental results on both four real and 11 synthetic datasets demonstrate that the proposed approach (DynED) provides a higher average mean accuracy compared to the five state-of-the-art baselines
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#65288;UED&#65289;&#20013;PAIRED&#26041;&#27861;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#20854;&#22312;&#23454;&#38469;&#24615;&#33021;&#19978;&#33021;&#22815;&#19982;&#25110;&#36229;&#36807;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.10797</link><description>&lt;p&gt;
&#20351;&#29992;&#23398;&#20064;&#23545;&#25163;&#31283;&#23450;&#26080;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Stabilizing Unsupervised Environment Design with a Learned Adversary. (arXiv:2308.10797v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10797
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#65288;UED&#65289;&#20013;PAIRED&#26041;&#27861;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#20854;&#22312;&#23454;&#38469;&#24615;&#33021;&#19978;&#33021;&#22815;&#19982;&#25110;&#36229;&#36807;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#20855;&#22791;&#26222;&#36941;&#33021;&#21147;&#30340;&#26234;&#33021;&#20307;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#35774;&#35745;&#35757;&#32451;&#20219;&#21153;&#65292;&#20197;&#20419;&#36827;&#24191;&#27867;&#27867;&#21270;&#21644;&#23545;&#29615;&#22659;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;&#36825;&#20010;&#25361;&#25112;&#39537;&#21160;&#20102;&#26080;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#65288;UED&#65289;&#30340;&#38382;&#39064;&#35774;&#32622;&#65292;&#20854;&#20013;&#23398;&#29983;&#26234;&#33021;&#20307;&#22312;&#30001;&#25945;&#24072;&#26234;&#33021;&#20307;&#25552;&#20986;&#30340;&#33258;&#36866;&#24212;&#20219;&#21153;&#20998;&#24067;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;UED&#30340;&#20808;&#39537;&#26041;&#27861;&#26159;PAIRED&#65292;&#23427;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#35757;&#32451;&#25945;&#24072;&#31574;&#30053;&#20174;&#22836;&#24320;&#22987;&#35774;&#35745;&#20219;&#21153;&#65292;&#36825;&#26679;&#21487;&#20197;&#30452;&#25509;&#29983;&#25104;&#36866;&#24212;&#26234;&#33021;&#20307;&#24403;&#21069;&#33021;&#21147;&#30340;&#20219;&#21153;&#12290;&#23613;&#31649;&#20854;&#26377;&#24456;&#24378;&#30340;&#29702;&#35770;&#25903;&#25345;&#65292;&#20294;PAIRED&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#65292;&#22952;&#30861;&#20102;&#20854;&#23454;&#38469;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#30446;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#20381;&#36182;&#20110;&#31574;&#21010;&#21644;&#21464;&#24322;&#65292;&#32780;&#19981;&#26159;&#29983;&#25104;&#26032;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;PAIRED&#30340;&#20960;&#20010;&#20851;&#38190;&#19981;&#36275;&#20043;&#22788;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20351;PAIRED&#33021;&#22815;&#19982;&#25110;&#36229;&#36807;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key challenge in training generally-capable agents is the design of training tasks that facilitate broad generalization and robustness to environment variations. This challenge motivates the problem setting of Unsupervised Environment Design (UED), whereby a student agent trains on an adaptive distribution of tasks proposed by a teacher agent. A pioneering approach for UED is PAIRED, which uses reinforcement learning (RL) to train a teacher policy to design tasks from scratch, making it possible to directly generate tasks that are adapted to the agent's current capabilities. Despite its strong theoretical backing, PAIRED suffers from a variety of challenges that hinder its practical performance. Thus, state-of-the-art methods currently rely on curation and mutation rather than generation of new tasks. In this work, we investigate several key shortcomings of PAIRED and propose solutions for each shortcoming. As a result, we make it possible for PAIRED to match or exceed state-of-the-a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25253;&#21578;&#20102;&#20851;&#20110;&#27491;&#24335;&#21270;&#26465;&#20214;&#25512;&#29702;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#21253;&#25324;Aqvist&#30340;&#26465;&#20214;&#20041;&#21153;&#31995;&#32479;E&#30340;&#26426;&#26800;&#21270;&#21644;&#20262;&#29702;&#35770;&#25454;&#35780;&#20272;&#30340;&#24037;&#20855;&#30340;&#24320;&#21457;&#12290;</title><link>http://arxiv.org/abs/2308.10686</link><description>&lt;p&gt;
&#24635;&#25324;&#33655;&#23572;&#33945;&#20307;&#31995;&#20316;&#20026;HOL&#30340;&#19968;&#20010;&#29255;&#27573;
&lt;/p&gt;
&lt;p&gt;
Normative Conditional Reasoning as a Fragment of HOL. (arXiv:2308.10686v2 [cs.LO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25253;&#21578;&#20102;&#20851;&#20110;&#27491;&#24335;&#21270;&#26465;&#20214;&#25512;&#29702;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#21253;&#25324;Aqvist&#30340;&#26465;&#20214;&#20041;&#21153;&#31995;&#32479;E&#30340;&#26426;&#26800;&#21270;&#21644;&#20262;&#29702;&#35770;&#25454;&#35780;&#20272;&#30340;&#24037;&#20855;&#30340;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25253;&#21578;&#20102;&#20851;&#20110;&#27491;&#24335;&#21270;&#65288;&#22522;&#20110;&#20559;&#22909;&#30340;&#65289;&#26465;&#20214;&#25512;&#29702;&#30340;&#19968;&#20123;&#32467;&#26524;&#12290;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;Aqvist&#30340;&#26465;&#20214;&#20041;&#21153;&#31995;&#32479;E&#65288;&#21450;&#20854;&#25193;&#23637;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;Isabelle/HOL&#20013;&#30340;&#27973;&#34920;&#35821;&#20041;&#23884;&#20837;&#26469;&#23454;&#29616;&#25105;&#20204;&#30340;&#27491;&#24335;&#21270;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#35813;&#26694;&#26550;&#30340;&#20004;&#31181;&#21487;&#33021;&#29992;&#36884;&#12290;&#31532;&#19968;&#31181;&#26159;&#20316;&#20026;&#23545;&#25152;&#32771;&#34385;&#36923;&#36753;&#36827;&#34892;&#20803;&#25512;&#29702;&#30340;&#24037;&#20855;&#12290;&#25105;&#20204;&#23558;&#20854;&#29992;&#20110;&#33258;&#21160;&#39564;&#35777;&#26435;&#21033;&#20041;&#21153;&#23545;&#24212;&#20851;&#31995;&#65288;&#24191;&#20041;&#19978;&#29702;&#35299;&#65289;&#21450;&#30456;&#20851;&#20107;&#39033;&#65292;&#31867;&#20284;&#20110;&#20043;&#21069;&#23545;&#27169;&#24577;&#36923;&#36753;&#31435;&#26041;&#20307;&#25152;&#21462;&#24471;&#30340;&#25104;&#26524;&#12290;&#31532;&#20108;&#31181;&#29992;&#36884;&#26159;&#20316;&#20026;&#20262;&#29702;&#35770;&#25454;&#35780;&#20272;&#30340;&#24037;&#20855;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20154;&#21475;&#20262;&#29702;&#23398;&#20013;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#24726;&#35770;Parfit&#30340;&#20196;&#20154;&#21388;&#24694;&#30340;&#32467;&#35770;&#30340;&#35745;&#31639;&#26426;&#32534;&#30721;&#12290;&#22914;&#20309;&#36890;&#36807;&#36825;&#20010;&#32534;&#30721;&#22686;&#21152;&#25110;&#20943;&#23569;&#20196;&#20154;&#21388;&#24694;&#30340;&#32467;&#35770;&#30340;&#21560;&#24341;&#21147;&#21644;&#35828;&#26381;&#21147;&#26159;&#19968;&#20010;&#25105;&#20204;&#24076;&#26395;&#21521;&#21746;&#23398;&#21644;&#20262;&#29702;&#23398;&#25552;&#20986;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We report some results regarding the mechanization of normative (preference-based) conditional reasoning. Our focus is on Aqvist's system E for conditional obligation (and its extensions). Our mechanization is achieved via a shallow semantical embedding in Isabelle/HOL. We consider two possible uses of the framework. The first one is as a tool for meta-reasoning about the considered logic. We employ it for the automated verification of deontic correspondences (broadly conceived) and related matters, analogous to what has been previously achieved for the modal logic cube. The second use is as a tool for assessing ethical arguments. We provide a computer encoding of a well-known paradox in population ethics, Parfit's repugnant conclusion. Whether the presented encoding increases or decreases the attractiveness and persuasiveness of the repugnant conclusion is a question we would like to pass on to philosophy and ethics.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#25513;&#33180;&#21306;&#22495;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476; (Mask-RCNN) &#23545;&#23391;&#21152;&#25289;&#25991;&#26723;&#36827;&#34892;&#29256;&#38754;&#20998;&#26512;&#65292;&#25552;&#21319;&#24615;&#33021;&#12290;&#36890;&#36807;&#36880;&#27493;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#25105;&#20204;&#36798;&#21040;&#20102;0.889&#30340;&#33391;&#22909;dice&#20998;&#25968;&#12290;&#34429;&#28982;&#22312;&#24212;&#29992;&#33521;&#25991;&#25991;&#26723;&#27169;&#22411;&#26102;&#36935;&#21040;&#20102;&#19968;&#20123;&#25361;&#25112;&#65292;&#20294;&#36825;&#34920;&#26126;&#27599;&#31181;&#35821;&#35328;&#37117;&#26377;&#20854;&#29305;&#23450;&#30340;&#38382;&#39064;&#38656;&#35201;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2308.10511</link><description>&lt;p&gt;
&#22522;&#20110;&#25513;&#33180;&#21306;&#22495;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476; (Mask-RCNN) &#30340;&#23391;&#21152;&#25289;&#25991;&#26723;&#29256;&#38754;&#20998;&#26512;&#24615;&#33021;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Performance Enhancement Leveraging Mask-RCNN on Bengali Document Layout Analysis. (arXiv:2308.10511v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#25513;&#33180;&#21306;&#22495;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476; (Mask-RCNN) &#23545;&#23391;&#21152;&#25289;&#25991;&#26723;&#36827;&#34892;&#29256;&#38754;&#20998;&#26512;&#65292;&#25552;&#21319;&#24615;&#33021;&#12290;&#36890;&#36807;&#36880;&#27493;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#25105;&#20204;&#36798;&#21040;&#20102;0.889&#30340;&#33391;&#22909;dice&#20998;&#25968;&#12290;&#34429;&#28982;&#22312;&#24212;&#29992;&#33521;&#25991;&#25991;&#26723;&#27169;&#22411;&#26102;&#36935;&#21040;&#20102;&#19968;&#20123;&#25361;&#25112;&#65292;&#20294;&#36825;&#34920;&#26126;&#27599;&#31181;&#35821;&#35328;&#37117;&#26377;&#20854;&#29305;&#23450;&#30340;&#38382;&#39064;&#38656;&#35201;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#25968;&#23383;&#25991;&#26723;&#23601;&#20687;&#35299;&#20915;&#19968;&#20010;&#35868;&#39064;&#65292;&#23588;&#20854;&#26159;&#21382;&#21490;&#25991;&#26723;&#12290;&#25991;&#26723;&#29256;&#38754;&#20998;&#26512;(DLA)&#36890;&#36807;&#23558;&#25991;&#26723;&#21010;&#20998;&#20026;&#27573;&#33853;&#12289;&#22270;&#29255;&#21644;&#34920;&#26684;&#31561;&#37096;&#20998;&#65292;&#24110;&#21161;&#35299;&#20915;&#36825;&#20010;&#35868;&#39064;&#12290;&#36825;&#23545;&#20110;&#26426;&#22120;&#26469;&#38405;&#35835;&#21644;&#29702;&#35299;&#36825;&#20123;&#25991;&#26723;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;DL Sprint 2.0&#31454;&#36187;&#20013;&#65292;&#25105;&#20204;&#33268;&#21147;&#20110;&#29702;&#35299;&#23391;&#21152;&#25289;&#25991;&#26723;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#21517;&#20026;BaDLAD&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#24456;&#22810;&#20363;&#23376;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#29305;&#27530;&#30340;&#27169;&#22411;&#65292;&#31216;&#20026;Mask R-CNN&#65292;&#26469;&#24110;&#21161;&#29702;&#35299;&#12290;&#36890;&#36807;&#36880;&#27493;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#36825;&#20010;&#27169;&#22411;&#65292;&#24182;&#21462;&#24471;&#20102;0.889&#30340;&#22909;&#30340;dice&#20998;&#25968;&#12290;&#28982;&#32780;&#65292;&#24182;&#19981;&#26159;&#19968;&#20999;&#37117;&#36827;&#34892;&#24471;&#38750;&#24120;&#23436;&#32654;&#12290;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#35757;&#32451;&#22909;&#30340;&#33521;&#25991;&#25991;&#26723;&#27169;&#22411;&#65292;&#20294;&#23427;&#19982;&#23391;&#21152;&#25289;&#25991;&#19981;&#22826;&#21305;&#37197;&#12290;&#36825;&#21521;&#25105;&#20204;&#23637;&#31034;&#20102;&#27599;&#31181;&#35821;&#35328;&#37117;&#26377;&#33258;&#24049;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#23545;DL Sprint 2.0&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#22312;https://www.kaggle.com/competitions/dlsprint2/discussion/432201&#20844;&#24320;&#33719;&#24471;&#65292;&#20854;&#20013;&#21253;&#25324;notebooks&#12289;&#26435;&#37325;&#21644;&#25512;&#29702;&#31508;&#35760;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding digital documents is like solving a puzzle, especially historical ones. Document Layout Analysis (DLA) helps with this puzzle by dividing documents into sections like paragraphs, images, and tables. This is crucial for machines to read and understand these documents. In the DL Sprint 2.0 competition, we worked on understanding Bangla documents. We used a dataset called BaDLAD with lots of examples. We trained a special model called Mask R-CNN to help with this understanding. We made this model better by step-by-step hyperparameter tuning, and we achieved a good dice score of 0.889. However, not everything went perfectly. We tried using a model trained for English documents, but it didn't fit well with Bangla. This showed us that each language has its own challenges. Our solution for the DL Sprint 2.0 is publicly available at https://www.kaggle.com/competitions/dlsprint2/discussion/432201 along with notebooks, weights, and inference notebook.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#24778;&#21916;&#20998;&#25968;&#8221;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#32771;&#34385;&#23545;&#35937;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#24182;&#26174;&#33879;&#25552;&#39640;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#25991;&#26723;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.09765</link><description>&lt;p&gt;
&#21463;&#20919;&#33853;: &#30456;&#20284;&#24230;&#20998;&#25968;&#30340;&#21453;&#24046;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Taken by Surprise: Contrast effect for Similarity Scores. (arXiv:2308.09765v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09765
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#24778;&#21916;&#20998;&#25968;&#8221;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#32771;&#34385;&#23545;&#35937;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#24182;&#26174;&#33879;&#25552;&#39640;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#25991;&#26723;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#35780;&#20272;&#29289;&#20307;&#21521;&#37327;&#23884;&#20837;&#30340;&#30456;&#20284;&#24230;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#20449;&#24687;&#26816;&#32034;&#21644;&#20998;&#31867;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#27969;&#34892;&#30340;&#30456;&#20284;&#24230;&#20998;&#25968;&#65288;&#22914;&#20313;&#24358;&#30456;&#20284;&#24230;&#65289;&#22522;&#20110;&#23884;&#20837;&#21521;&#37327;&#23545;&#65292;&#24182;&#24573;&#30053;&#20102;&#20174;&#20013;&#25552;&#21462;&#23545;&#35937;&#30340;&#20998;&#24067;&#12290;&#20154;&#31867;&#23545;&#29289;&#20307;&#30456;&#20284;&#24230;&#30340;&#24863;&#30693;&#26174;&#33879;&#21462;&#20915;&#20110;&#23545;&#35937;&#20986;&#29616;&#30340;&#19978;&#19979;&#25991;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#24778;&#21916;&#20998;&#25968;&#8221;&#65292;&#36825;&#26159;&#19968;&#20010;&#23545;&#25972;&#20307;&#36827;&#34892;&#24402;&#19968;&#21270;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#21253;&#25324;&#20102;&#20154;&#31867;&#24863;&#30693;&#30340;&#21453;&#24046;&#25928;&#24212;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#25991;&#26723;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#27492;&#20998;&#25968;&#37327;&#21270;&#20102;&#22312;&#20004;&#20010;&#20803;&#32032;&#20043;&#38388;&#25214;&#21040;&#32473;&#23450;&#30456;&#20284;&#24230;&#30340;&#24778;&#21916;&#65292;&#30456;&#23545;&#20110;&#25104;&#23545;&#30340;&#25972;&#20307;&#30456;&#20284;&#24230;&#12290;&#25105;&#20204;&#22312;&#38646;&#26679;&#26412;/&#23569;&#26679;&#26412;&#20998;&#31867;&#21644;&#32858;&#31867;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#36825;&#20010;&#24230;&#37327;&#65292;&#36890;&#24120;&#21457;&#29616;&#19982;&#21407;&#22987;&#20313;&#24358;&#30456;&#20284;&#24230;&#30456;&#27604;&#65292;&#24615;&#33021;&#25552;&#39640;&#20102;10-15\%&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;...
&lt;/p&gt;
&lt;p&gt;
Accurately evaluating the similarity of object vector embeddings is of critical importance for natural language processing, information retrieval and classification tasks. Popular similarity scores (e.g cosine similarity) are based on pairs of embedding vectors and disregard the distribution of the ensemble from which objects are drawn. Human perception of object similarity significantly depends on the context in which the objects appear. In this work we propose the \emph{surprise score}, an ensemble-normalized similarity metric that encapsulates the contrast effect of human perception and significantly improves the classification performance on zero- and few-shot document classification tasks. This score quantifies the surprise to find a given similarity between two elements relative to the pairwise ensemble similarities. We evaluate this metric on zero/few shot classification and clustering tasks and typically find 10-15\% better performance compared to raw cosine similarity. Our cod
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#26469;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#25972;&#21512;&#26032;&#30693;&#35782;&#12289;&#20135;&#29983;&#24187;&#35273;&#21644;&#20915;&#31574;&#36807;&#31243;&#19981;&#36879;&#26126;&#31561;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#24605;&#32500;&#23548;&#22270;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21462;&#24471;&#26174;&#33879;&#30340;&#23454;&#35777;&#22686;&#30410;&#12290;</title><link>http://arxiv.org/abs/2308.09729</link><description>&lt;p&gt;
MindMap&#65306;&#30693;&#35782;&#22270;&#35889;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24605;&#32500;&#22270;&#24605;&#32771;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models. (arXiv:2308.09729v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#26469;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#25972;&#21512;&#26032;&#30693;&#35782;&#12289;&#20135;&#29983;&#24187;&#35273;&#21644;&#20915;&#31574;&#36807;&#31243;&#19981;&#36879;&#26126;&#31561;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#24605;&#32500;&#23548;&#22270;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21462;&#24471;&#26174;&#33879;&#30340;&#23454;&#35777;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#26080;&#27861;&#25972;&#21512;&#26032;&#30693;&#35782;&#12289;&#20135;&#29983;&#24187;&#35273;&#21644;&#20915;&#31574;&#36807;&#31243;&#19981;&#36879;&#26126;&#31561;&#38480;&#21046;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#26469;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#25972;&#21512;&#26368;&#26032;&#30693;&#35782;&#21644;&#24341;&#21457;&#27169;&#22411;&#24605;&#32500;&#36335;&#24452;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#25552;&#31034;&#31649;&#36947;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29702;&#35299;KG&#36755;&#20837;&#24182;&#21033;&#29992;&#38544;&#21547;&#30693;&#35782;&#21644;&#26816;&#32034;&#21040;&#30340;&#22806;&#37096;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24341;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#25512;&#29702;&#21644;&#29983;&#25104;&#31572;&#26696;&#30340;&#24605;&#32500;&#23548;&#22270;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29983;&#25104;&#30340;&#24605;&#32500;&#23548;&#22270;&#22522;&#20110;&#30693;&#35782;&#30340;&#26412;&#20307;&#35770;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#65292;&#20174;&#32780;&#20026;&#29983;&#20135;&#29615;&#22659;&#20013;&#30340;&#25512;&#29702;&#25552;&#20379;&#20102;&#25506;&#32034;&#21644;&#35780;&#20272;&#30340;&#21487;&#33021;&#24615;&#12290;&#23545;&#19977;&#20010;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;MindMap&#25552;&#31034;&#26041;&#27861;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#23454;&#35777;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs usually exhibit limitations in their ability to incorporate new knowledge, the generation of hallucinations, and the transparency of their decision-making process. In this paper, we explore how to prompt LLMs with knowledge graphs (KG), working as a remedy to engage LLMs with up-to-date knowledge and elicit the reasoning pathways from LLMs. Specifically, we build a prompting pipeline that endows LLMs with the capability of comprehending KG inputs and inferring with a combined implicit knowledge and the retrieved external knowledge. In addition, we investigate eliciting the mind map on which LLMs perform the reasoning and generate the answers. It is identified that the produced mind map exhibits the reasoning pathways of LLMs grounded on the ontology of knowledge, hence bringing the prospects of probing and gauging LLM inference in production. The experiments on three question &amp; answering datasets also show that MindMap prompting leads to a striking empirical gain. For instance, pr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;Transformer&#27169;&#22411;&#20013;&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#21457;&#29616;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#32534;&#30721;&#20102;&#26576;&#20123;&#36890;&#29992;&#30693;&#35782;&#25552;&#21462;&#27169;&#24335;&#65292;&#22240;&#27492;&#22312;&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;&#26102;&#65292;&#19981;&#38656;&#35201;&#26356;&#26032;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#30340;&#26435;&#37325;&#12290;</title><link>http://arxiv.org/abs/2308.08742</link><description>&lt;p&gt;
PMET: &#22312;Transformer&#20013;&#30340;&#31934;&#30830;&#27169;&#22411;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
PMET: Precise Model Editing in a Transformer. (arXiv:2308.08742v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08742
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;Transformer&#27169;&#22411;&#20013;&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#21457;&#29616;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#32534;&#30721;&#20102;&#26576;&#20123;&#36890;&#29992;&#30693;&#35782;&#25552;&#21462;&#27169;&#24335;&#65292;&#22240;&#27492;&#22312;&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;&#26102;&#65292;&#19981;&#38656;&#35201;&#26356;&#26032;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#30340;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#21487;&#20197;&#20197;&#36739;&#20302;&#30340;&#25104;&#26412;&#20462;&#25913;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23569;&#37327;&#30693;&#35782;&#65292;&#24182;&#19988;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#29616;&#26377;&#26041;&#27861;&#20551;&#35774;Transformer&#23618;&#38544;&#34255;&#29366;&#24577;&#26159;&#21069;&#39304;&#32593;&#32476;&#30340;&#38190;&#20540;&#20869;&#23384;&#30340;&#20540;&#12290;&#23427;&#20204;&#36890;&#24120;&#20248;&#21270;Transformer&#23618;&#38544;&#34255;&#29366;&#24577;&#26469;&#35760;&#24518;&#30446;&#26631;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#26356;&#26032;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21069;&#39304;&#32593;&#32476;&#30340;&#26435;&#37325;&#12290;&#28982;&#32780;&#65292;Transformer&#23618;&#38544;&#34255;&#29366;&#24577;&#30340;&#20449;&#24687;&#27969;&#26469;&#33258;&#19977;&#20010;&#37096;&#20998;&#65306;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#12289;&#21069;&#39304;&#32593;&#32476;&#21644;&#27531;&#24046;&#36830;&#25509;&#12290;&#29616;&#26377;&#26041;&#27861;&#24573;&#35270;&#20102;Transformer&#23618;&#38544;&#34255;&#29366;&#24577;&#21253;&#21547;&#20102;&#21069;&#39304;&#32593;&#32476;&#29305;&#21035;&#38656;&#35201;&#30340;&#20449;&#24687;&#36825;&#19968;&#20107;&#23454;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#32534;&#36753;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#27169;&#22411;&#32534;&#36753;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#21644;&#21069;&#39304;&#32593;&#32476;&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#21457;&#29616;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#32534;&#30721;&#20102;&#26576;&#20123;&#36890;&#29992;&#30693;&#35782;&#25552;&#21462;&#27169;&#24335;&#12290;&#36825;&#24847;&#21619;&#30528;&#24403;&#24341;&#20837;&#26032;&#30693;&#35782;&#26102;&#65292;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#30340;&#26435;&#37325;&#19981;&#38656;&#35201;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model editing techniques modify a minor proportion of knowledge in Large Language Models (LLMs) at a relatively low cost, which have demonstrated notable success. Existing methods assume Transformer Layer (TL) hidden states are values of key-value memories of the Feed-Forward Network (FFN). They usually optimize the TL hidden states to memorize target knowledge and use it to update the weights of the FFN in LLMs. However, the information flow of TL hidden states comes from three parts: Multi-Head Self-Attention (MHSA), FFN, and residual connections. Existing methods neglect the fact that the TL hidden states contains information not specifically required for FFN. Consequently, the performance of model editing decreases. To achieve more precise model editing, we analyze hidden states of MHSA and FFN, finding that MHSA encodes certain general knowledge extraction patterns. This implies that MHSA weights do not require updating when new knowledge is introduced. Based on above findings, we
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20005;&#35880;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#24403;&#21069;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36827;&#34892;&#35814;&#32454;&#35780;&#20272;&#26469;&#25506;&#35752;&#20154;&#24037;&#26234;&#33021;&#30340;&#24847;&#35782;&#38382;&#39064;&#12290;&#30740;&#31350;&#20013;&#23545;&#20960;&#31181;&#31185;&#23398;&#24847;&#35782;&#29702;&#35770;&#36827;&#34892;&#27010;&#36848;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#26041;&#27861;&#25512;&#23548;&#20986;&#24847;&#35782;&#30340;&#8220;&#25351;&#31034;&#24615;&#29305;&#24449;&#8221;&#12290;&#20998;&#26512;&#32467;&#26524;&#34920;&#26126;&#30446;&#21069;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23578;&#19981;&#20855;&#22791;&#24847;&#35782;&#65292;&#20294;&#24314;&#31435;&#20855;&#26377;&#24847;&#35782;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#24182;&#26080;&#26126;&#26174;&#30340;&#38556;&#30861;&#12290;</title><link>http://arxiv.org/abs/2308.08708</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#24847;&#35782;&#65306;&#26469;&#33258;&#24847;&#35782;&#31185;&#23398;&#30340;&#27934;&#35265;
&lt;/p&gt;
&lt;p&gt;
Consciousness in Artificial Intelligence: Insights from the Science of Consciousness. (arXiv:2308.08708v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20005;&#35880;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#24403;&#21069;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36827;&#34892;&#35814;&#32454;&#35780;&#20272;&#26469;&#25506;&#35752;&#20154;&#24037;&#26234;&#33021;&#30340;&#24847;&#35782;&#38382;&#39064;&#12290;&#30740;&#31350;&#20013;&#23545;&#20960;&#31181;&#31185;&#23398;&#24847;&#35782;&#29702;&#35770;&#36827;&#34892;&#27010;&#36848;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#26041;&#27861;&#25512;&#23548;&#20986;&#24847;&#35782;&#30340;&#8220;&#25351;&#31034;&#24615;&#29305;&#24449;&#8221;&#12290;&#20998;&#26512;&#32467;&#26524;&#34920;&#26126;&#30446;&#21069;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23578;&#19981;&#20855;&#22791;&#24847;&#35782;&#65292;&#20294;&#24314;&#31435;&#20855;&#26377;&#24847;&#35782;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#24182;&#26080;&#26126;&#26174;&#30340;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#25110;&#36817;&#26399;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26159;&#21542;&#33021;&#20855;&#26377;&#24847;&#35782;&#25104;&#20026;&#31185;&#23398;&#30028;&#20851;&#27880;&#30340;&#35805;&#39064;&#65292;&#20063;&#24341;&#36215;&#20102;&#20844;&#20247;&#30340;&#25285;&#24551;&#12290;&#26412;&#25253;&#21578;&#25552;&#20986;&#24182;&#20030;&#20363;&#20102;&#19968;&#31181;&#20005;&#35880;&#19988;&#32463;&#39564;&#22522;&#30784;&#30340;&#20154;&#24037;&#26234;&#33021;&#24847;&#35782;&#26041;&#27861;&#65306;&#26681;&#25454;&#25105;&#20204;&#30446;&#21069;&#26368;&#21487;&#20449;&#30340;&#31070;&#32463;&#31185;&#23398;&#29702;&#35770;&#23545;&#29616;&#26377;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36827;&#34892;&#35814;&#32454;&#35780;&#20272;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#20960;&#31181;&#24191;&#27867;&#35748;&#21487;&#30340;&#31185;&#23398;&#24847;&#35782;&#29702;&#35770;&#65292;&#21253;&#25324;&#24490;&#29615;&#22788;&#29702;&#29702;&#35770;&#12289;&#20840;&#23616;&#24037;&#20316;&#31354;&#38388;&#29702;&#35770;&#12289;&#39640;&#38454;&#29702;&#35770;&#12289;&#39044;&#27979;&#22788;&#29702;&#29702;&#35770;&#21644;&#27880;&#24847;&#27169;&#24335;&#29702;&#35770;&#12290;&#20174;&#36825;&#20123;&#29702;&#35770;&#20013;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#19968;&#20123;&#24847;&#35782;&#30340;&#8220;&#25351;&#31034;&#24615;&#29305;&#24449;&#8221;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#26041;&#27861;&#26469;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26159;&#21542;&#20855;&#22791;&#36825;&#20123;&#29305;&#24449;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#25351;&#31034;&#24615;&#29305;&#24449;&#26469;&#35780;&#20272;&#20102;&#20960;&#20010;&#36817;&#26399;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#31995;&#32479;&#22914;&#20309;&#23454;&#29616;&#36825;&#20123;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#30446;&#21069;&#27809;&#26377;&#29616;&#26377;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20855;&#26377;&#24847;&#35782;&#65292;&#20294;&#21516;&#26102;&#20063;&#26174;&#31034;&#20986;&#27809;&#26377;&#26126;&#26174;&#30340;&#24314;&#31435;&#20855;&#26377;&#24847;&#35782;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whether current or near-term AI systems could be conscious is a topic of scientific interest and increasing public concern. This report argues for, and exemplifies, a rigorous and empirically grounded approach to AI consciousness: assessing existing AI systems in detail, in light of our best-supported neuroscientific theories of consciousness. We survey several prominent scientific theories of consciousness, including recurrent processing theory, global workspace theory, higher-order theories, predictive processing, and attention schema theory. From these theories we derive "indicator properties" of consciousness, elucidated in computational terms that allow us to assess AI systems for these properties. We use these indicator properties to assess several recent AI systems, and we discuss how future systems might implement them. Our analysis suggests that no current AI systems are conscious, but also shows that there are no obvious barriers to building conscious AI systems.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CPEM&#30340;&#31995;&#32479;&#65292;&#23427;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#29615;&#22659;&#24863;&#30693;&#35760;&#24518;&#26469;&#25913;&#36827;&#34892;&#20026;&#26234;&#33021;&#20307;&#30340;&#24863;&#30693;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#35270;&#35273;&#23548;&#33322;&#21644;&#29289;&#20307;&#20132;&#20114;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.07241</link><description>&lt;p&gt;
&#20855;&#26377;&#29615;&#22659;&#24863;&#30693;&#35760;&#24518;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#35268;&#21010;&#29992;&#20110;&#25351;&#23548;&#34892;&#20026;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Context-Aware Planning and Environment-Aware Memory for Instruction Following Embodied Agents. (arXiv:2308.07241v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07241
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CPEM&#30340;&#31995;&#32479;&#65292;&#23427;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#29615;&#22659;&#24863;&#30693;&#35760;&#24518;&#26469;&#25913;&#36827;&#34892;&#20026;&#26234;&#33021;&#20307;&#30340;&#24863;&#30693;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#35270;&#35273;&#23548;&#33322;&#21644;&#29289;&#20307;&#20132;&#20114;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23436;&#25104;&#23478;&#21153;&#20219;&#21153;&#65288;&#20363;&#22914;&#8220;&#25343;&#19968;&#26479;&#27700;&#8221;&#65289;&#38656;&#35201;&#36890;&#36807;&#20445;&#25345;&#23545;&#31354;&#38388;&#23545;&#35937;&#30340;&#31354;&#38388;&#24067;&#23616;&#21644;&#20808;&#21069;&#34892;&#21160;&#30340;&#32467;&#26524;&#30340;&#30693;&#35782;&#26469;&#36827;&#34892;&#36880;&#27493;&#30340;&#35268;&#21010;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#34892;&#20026;&#26234;&#33021;&#20307;&#22312;&#24863;&#30693;&#27169;&#22411;&#26041;&#38754;&#32463;&#24120;&#20986;&#38169;&#65292;&#22240;&#20026;&#32570;&#20047;&#36825;&#31181;&#30693;&#35782;&#65292;&#32780;&#20381;&#36182;&#20110;&#19981;&#23436;&#32654;&#30340;&#23398;&#20064;&#30340;&#27169;&#20223;&#26234;&#33021;&#20307;&#25110;&#32773;&#27809;&#26377;&#20851;&#20110;&#20808;&#21069;&#34892;&#21160;&#23545;&#29615;&#22659;&#21464;&#21270;&#30340;&#30693;&#35782;&#30340;&#31639;&#27861;&#35268;&#21010;&#22120;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CPEM&#65288;&#19978;&#19979;&#25991;&#24863;&#30693;&#35268;&#21010;&#22120;&#21644;&#29615;&#22659;&#24863;&#30693;&#35760;&#24518;&#65289;&#65292;&#23558;&#20808;&#21069;&#34892;&#21160;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#19982;&#29615;&#22659;&#20013;&#29289;&#20307;&#30340;&#31354;&#38388;&#24067;&#23616;&#21644;&#29366;&#24577;&#65288;&#20363;&#22914;&#29289;&#20307;&#26159;&#21542;&#34987;&#31227;&#21160;&#65289;&#32467;&#21512;&#21040;&#24863;&#30693;&#27169;&#22411;&#20013;&#65292;&#20197;&#25913;&#36827;&#35270;&#35273;&#23548;&#33322;&#21644;&#29289;&#20307;&#20132;&#20114;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;CPEM&#22312;&#21508;&#31181;&#24230;&#37327;&#25351;&#26631;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20219;&#21153;&#25104;&#21151;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accomplishing household tasks such as 'bringing a cup of water' requires planning step-by-step actions by maintaining knowledge about the spatial arrangement of objects and the consequences of previous actions. Perception models of the current embodied AI agents, however, often make mistakes due to a lack of such knowledge but rely on imperfect learning of imitating agents or an algorithmic planner without knowledge about the changed environment by the previous actions. To address the issue, we propose CPEM (Context-aware Planner and Environment-aware Memory) to incorporate the contextual information of previous actions for planning and maintaining spatial arrangement of objects with their states (e.g., if an object has been moved or not) in an environment to the perception model for improving both visual navigation and object interaction. We observe that CPEM achieves state-of-the-art task success performance in various metrics using a challenging interactive instruction following ben
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26679;&#26412;&#36873;&#25321;&#30340;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#30495;&#23454;&#22330;&#26223;&#19979;&#21306;&#20998;&#19981;&#21516;&#31867;&#22411;&#30340;&#22122;&#22768;&#65292;&#24182;&#21033;&#29992;&#22122;&#22768;&#30340;&#35821;&#20041;&#20449;&#24687;&#36827;&#34892;&#23398;&#20064;&#12290;&#36890;&#36807;&#26500;&#24314;&#21407;&#22411;&#21521;&#37327;&#21644;&#35745;&#31639;&#26679;&#26412;&#19982;&#21407;&#22411;&#21521;&#37327;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25913;&#36827;&#26631;&#31614;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#35777;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.16889</link><description>&lt;p&gt;
&#20174;&#22122;&#22768;&#31867;&#22411;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#30495;&#23454;&#22330;&#26223;&#19979;&#30340;&#26377;&#22122;&#26631;&#27880;&#23398;&#20064;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Rethinking Noisy Label Learning in Real-world Annotation Scenarios from the Noise-type Perspective. (arXiv:2307.16889v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26679;&#26412;&#36873;&#25321;&#30340;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#30495;&#23454;&#22330;&#26223;&#19979;&#21306;&#20998;&#19981;&#21516;&#31867;&#22411;&#30340;&#22122;&#22768;&#65292;&#24182;&#21033;&#29992;&#22122;&#22768;&#30340;&#35821;&#20041;&#20449;&#24687;&#36827;&#34892;&#23398;&#20064;&#12290;&#36890;&#36807;&#26500;&#24314;&#21407;&#22411;&#21521;&#37327;&#21644;&#35745;&#31639;&#26679;&#26412;&#19982;&#21407;&#22411;&#21521;&#37327;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25913;&#36827;&#26631;&#31614;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#35777;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#30495;&#23454;&#22330;&#26223;&#19979;&#23398;&#20064;&#20855;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#22122;&#22768;&#21487;&#20197;&#20998;&#20026;&#20004;&#31181;&#31867;&#22411;&#65306;&#20107;&#23454;&#24615;&#22122;&#22768;&#21644;&#27169;&#31946;&#24615;&#22122;&#22768;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#21306;&#20998;&#36825;&#20123;&#22122;&#22768;&#31867;&#22411;&#24182;&#21033;&#29992;&#20854;&#35821;&#20041;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26679;&#26412;&#36873;&#25321;&#30340;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;Proto-semi&#12290;Proto-semi&#36890;&#36807;&#39044;&#28909;&#23558;&#25152;&#26377;&#26679;&#26412;&#21010;&#20998;&#20026;&#33258;&#20449;&#21644;&#19981;&#33258;&#20449;&#30340;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#21033;&#29992;&#33258;&#20449;&#25968;&#25454;&#38598;&#65292;&#26500;&#24314;&#21407;&#22411;&#21521;&#37327;&#20197;&#25429;&#25417;&#31867;&#21035;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#35745;&#31639;&#19981;&#33258;&#20449;&#26679;&#26412;&#19982;&#21407;&#22411;&#21521;&#37327;&#20043;&#38388;&#30340;&#36317;&#31163;&#20197;&#20419;&#36827;&#22122;&#22768;&#20998;&#31867;&#12290;&#26681;&#25454;&#36825;&#20123;&#36317;&#31163;&#65292;&#23545;&#26631;&#31614;&#36827;&#34892;&#20462;&#27491;&#25110;&#20445;&#30041;&#65292;&#20174;&#32780;&#25913;&#36827;&#33258;&#20449;&#21644;&#19981;&#33258;&#20449;&#25968;&#25454;&#38598;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26469;&#22686;&#24378;&#35757;&#32451;&#12290;&#23545;&#19968;&#20010;&#30495;&#23454;&#26631;&#27880;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#35777;&#35780;&#20272;&#35777;&#23454;&#20102;Proto-semi&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the problem of learning with noisy labels in real-world annotation scenarios, where noise can be categorized into two types: factual noise and ambiguity noise. To better distinguish these noise types and utilize their semantics, we propose a novel sample selection-based approach for noisy label learning, called Proto-semi. Proto-semi initially divides all samples into the confident and unconfident datasets via warm-up. By leveraging the confident dataset, prototype vectors are constructed to capture class characteristics. Subsequently, the distances between the unconfident samples and the prototype vectors are calculated to facilitate noise classification. Based on these distances, the labels are either corrected or retained, resulting in the refinement of the confident and unconfident datasets. Finally, we introduce a semi-supervised learning method to enhance training. Empirical evaluations on a real-world annotated dataset substantiate the robustness of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23383;&#20856;&#23398;&#20064;&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;MSDA&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#22495;&#34920;&#31034;&#20026;&#23383;&#20856;&#21407;&#23376;&#30340;Wasserstein&#37325;&#24515;&#26469;&#32531;&#35299;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#12290;&#26681;&#25454;&#35813;&#23383;&#20856;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;MSDA&#26041;&#27861;&#65292;&#20998;&#21035;&#22522;&#20110;&#30446;&#26631;&#22495;&#26631;&#35760;&#26679;&#26412;&#30340;&#37325;&#26500;&#21644;&#22312;&#21407;&#23376;&#20998;&#24067;&#19978;&#23398;&#20064;&#30340;&#20998;&#31867;&#22120;&#30340;&#38598;&#25104;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.14953</link><description>&lt;p&gt;
&#22312;Wasserstein&#31354;&#38388;&#20013;&#36890;&#36807;&#25968;&#25454;&#38598;&#23383;&#20856;&#23398;&#20064;&#36827;&#34892;&#22810;&#28304;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Multi-Source Domain Adaptation through Dataset Dictionary Learning in Wasserstein Space. (arXiv:2307.14953v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14953
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23383;&#20856;&#23398;&#20064;&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;MSDA&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#22495;&#34920;&#31034;&#20026;&#23383;&#20856;&#21407;&#23376;&#30340;Wasserstein&#37325;&#24515;&#26469;&#32531;&#35299;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#12290;&#26681;&#25454;&#35813;&#23383;&#20856;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;MSDA&#26041;&#27861;&#65292;&#20998;&#21035;&#22522;&#20110;&#30446;&#26631;&#22495;&#26631;&#35760;&#26679;&#26412;&#30340;&#37325;&#26500;&#21644;&#22312;&#21407;&#23376;&#20998;&#24067;&#19978;&#23398;&#20064;&#30340;&#20998;&#31867;&#22120;&#30340;&#38598;&#25104;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#22810;&#28304;&#22495;&#33258;&#36866;&#24212;&#65288;MSDA&#65289;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#26088;&#22312;&#22312;&#20174;&#22810;&#20010;&#26631;&#35760;&#30340;&#28304;&#22495;&#36716;&#31227;&#30693;&#35782;&#21040;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#22495;&#26102;&#32531;&#35299;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23383;&#20856;&#23398;&#20064;&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#26032;&#22411;MSDA&#26694;&#26550;&#12290;&#25105;&#20204;&#23558;MSDA&#20013;&#30340;&#27599;&#20010;&#22495;&#35299;&#37322;&#20026;&#32463;&#39564;&#20998;&#24067;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#27599;&#20010;&#22495;&#34920;&#36798;&#20026;&#23383;&#20856;&#21407;&#23376;&#30340;Wasserstein&#37325;&#24515;&#65292;&#36825;&#20123;&#21407;&#23376;&#26159;&#32463;&#39564;&#20998;&#24067;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#36807;&#23567;&#25209;&#37327;&#23398;&#20064;&#30340;&#31639;&#27861;DaDiL&#65306;&#65288;i&#65289;&#21407;&#23376;&#20998;&#24067;&#65307;&#65288;ii&#65289;&#37325;&#24515;&#22352;&#26631;&#30697;&#38453;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#23383;&#20856;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;MSDA&#26041;&#27861;&#65306;DaDiL-R&#65292;&#22522;&#20110;&#30446;&#26631;&#22495;&#26631;&#35760;&#26679;&#26412;&#30340;&#37325;&#26500;&#65307;DaDiL-E&#65292;&#22522;&#20110;&#22312;&#21407;&#23376;&#20998;&#24067;&#19978;&#23398;&#20064;&#30340;&#20998;&#31867;&#22120;&#30340;&#38598;&#25104;&#12290;&#25105;&#20204;&#22312;3&#20010;&#22522;&#20934;&#27979;&#35797;&#38598;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65306;Caltech-Office&#12289;Office 31&#21644;CRWU&#65292;&#22312;&#20998;&#31867;&#19978;&#25913;&#36827;&#20102;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;3.15&#65285;&#12289;2.29&#65285;&#21644;7.71&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper seeks to solve Multi-Source Domain Adaptation (MSDA), which aims to mitigate data distribution shifts when transferring knowledge from multiple labeled source domains to an unlabeled target domain. We propose a novel MSDA framework based on dictionary learning and optimal transport. We interpret each domain in MSDA as an empirical distribution. As such, we express each domain as a Wasserstein barycenter of dictionary atoms, which are empirical distributions. We propose a novel algorithm, DaDiL, for learning via mini-batches: (i) atom distributions; (ii) a matrix of barycentric coordinates. Based on our dictionary, we propose two novel methods for MSDA: DaDil-R, based on the reconstruction of labeled samples in the target domain, and DaDiL-E, based on the ensembling of classifiers learned on atom distributions. We evaluate our methods in 3 benchmarks: Caltech-Office, Office 31, and CRWU, where we improved previous state-of-the-art by 3.15%, 2.29%, and 7.71% in classification 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#33258;&#36866;&#24212;&#21464;&#24418;&#26041;&#27861;&#65292;&#29992;&#20110;&#40065;&#26834;&#21644;&#39640;&#25928;&#30340;&#31435;&#20307;&#21305;&#37197;&#12290;&#36890;&#36807;&#24341;&#20837;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#21487;&#23398;&#20064;&#21442;&#25968;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#36866;&#24212;&#24615;&#22320;&#35843;&#25972;&#37319;&#26679;&#21306;&#22495;&#65292;&#24182;&#23454;&#29616;&#26356;&#40065;&#26834;&#21644;&#26377;&#25928;&#30340;&#31435;&#20307;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2307.14071</link><description>&lt;p&gt;
&#29992;&#20110;&#40065;&#26834;&#21644;&#39640;&#25928;&#30340;&#31435;&#20307;&#21305;&#37197;&#30340;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#33258;&#36866;&#24212;&#21464;&#24418;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Guided Adaptive Warping for Robust and Efficient Stereo Matching. (arXiv:2307.14071v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#33258;&#36866;&#24212;&#21464;&#24418;&#26041;&#27861;&#65292;&#29992;&#20110;&#40065;&#26834;&#21644;&#39640;&#25928;&#30340;&#31435;&#20307;&#21305;&#37197;&#12290;&#36890;&#36807;&#24341;&#20837;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#21487;&#23398;&#20064;&#21442;&#25968;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#36866;&#24212;&#24615;&#22320;&#35843;&#25972;&#37319;&#26679;&#21306;&#22495;&#65292;&#24182;&#23454;&#29616;&#26356;&#40065;&#26834;&#21644;&#26377;&#25928;&#30340;&#31435;&#20307;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30456;&#20851;&#30340;&#31435;&#20307;&#21305;&#37197;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#36825;&#26159;&#36890;&#36807;&#22312;&#20004;&#20010;&#29305;&#24449;&#22270;&#20043;&#38388;&#26500;&#24314;&#20195;&#20215;&#20307;&#26469;&#23454;&#29616;&#30340;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#30001;&#20110;&#22266;&#23450;&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#24182;&#19981;&#19968;&#33268;&#65292;&#20174;&#32780;&#26497;&#22823;&#22320;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#35745;&#31639;&#30456;&#20851;&#24615;&#20197;&#29992;&#20110;&#40065;&#26834;&#31435;&#20307;&#21305;&#37197;&#30340;&#26032;&#35270;&#35282;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#33258;&#36866;&#24212;&#30456;&#20851;&#65288;UGAC&#65289;&#27169;&#22359;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#22330;&#26223;&#12290;&#20855;&#20307;&#22320;&#65292;&#37319;&#29992;&#20102;&#22522;&#20110;&#26041;&#24046;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#22312;&#21464;&#24418;&#25805;&#20316;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#37319;&#26679;&#21306;&#22495;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#20256;&#32479;&#30340;&#38750;&#21442;&#25968;&#21464;&#24418;&#26041;&#27861;&#65292;&#24341;&#20837;&#21487;&#23398;&#20064;&#21442;&#25968;&#65292;&#20174;&#32780;&#21487;&#20197;&#23398;&#20064;&#20301;&#32622;&#29305;&#23450;&#30340;&#26435;&#37325;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36890;&#36807;&#22312;&#24490;&#29615;&#32593;&#32476;&#20013;&#24341;&#20837;UGAC&#27169;&#22359;&#65292;&#21487;&#20197;&#26356;&#40065;&#26834;&#21644;&#26377;&#25928;&#22320;&#21033;&#29992;&#31435;&#20307;&#21305;&#37197;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Correlation based stereo matching has achieved outstanding performance, which pursues cost volume between two feature maps. Unfortunately, current methods with a fixed model do not work uniformly well across various datasets, greatly limiting their real-world applicability. To tackle this issue, this paper proposes a new perspective to dynamically calculate correlation for robust stereo matching. A novel Uncertainty Guided Adaptive Correlation (UGAC) module is introduced to robustly adapt the same model for different scenarios. Specifically, a variance-based uncertainty estimation is employed to adaptively adjust the sampling area during warping operation. Additionally, we improve the traditional non-parametric warping with learnable parameters, such that the position-specific weights can be learned. We show that by empowering the recurrent network with the UGAC module, stereo matching can be exploited more robustly and effectively. Extensive experiments demonstrate that our method ach
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;DeepGPET&#30340;&#24320;&#28304;&#20840;&#33258;&#21160;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#20013;&#33033;&#32476;&#33180;&#21306;&#22495;&#20998;&#21106;&#65292;&#24182;&#19988;&#22312;&#25968;&#25454;&#20934;&#30830;&#24615;&#21644;&#22788;&#29702;&#36895;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2307.00904</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#20013;&#33033;&#32476;&#33180;&#30340;&#39640;&#25928;&#20840;&#33258;&#21160;&#20998;&#26512;&#30340;&#24320;&#28304;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
An open-source deep learning algorithm for efficient and fully-automatic analysis of the choroid in optical coherence tomography. (arXiv:2307.00904v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00904
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;DeepGPET&#30340;&#24320;&#28304;&#20840;&#33258;&#21160;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#20013;&#33033;&#32476;&#33180;&#21306;&#22495;&#20998;&#21106;&#65292;&#24182;&#19988;&#22312;&#25968;&#25454;&#20934;&#30830;&#24615;&#21644;&#22788;&#29702;&#36895;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#24320;&#21457;&#19968;&#20010;&#24320;&#28304;&#20840;&#33258;&#21160;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;DeepGPET&#65292;&#29992;&#20110;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#65288;OCT&#65289;&#25968;&#25454;&#20013;&#33033;&#32476;&#33180;&#21306;&#22495;&#20998;&#21106;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#20351;&#29992;&#20102;&#26469;&#33258;3&#20010;&#19982;&#31995;&#32479;&#24615;&#30142;&#30149;&#30456;&#20851;&#30340;&#20020;&#24202;&#30740;&#31350;&#30340;715&#20010;OCT B-&#25195;&#25551;&#65288;82&#21517;&#21463;&#35797;&#32773;&#65292;115&#21482;&#30524;&#30555;&#65289;&#30340;&#25968;&#25454;&#38598;&#12290;&#20351;&#29992;&#20020;&#24202;&#39564;&#35777;&#30340;&#21322;&#33258;&#21160;&#33033;&#32476;&#33180;&#20998;&#21106;&#26041;&#27861;&#39640;&#26031;&#36807;&#31243;&#36793;&#32536;&#36861;&#36394;&#65288;GPET&#65289;&#29983;&#25104;&#20102;&#22320;&#38754;&#30495;&#23454;&#20998;&#21106;&#12290;&#25105;&#20204;&#23545;&#22312;ImageNet&#19978;&#39044;&#35757;&#32451;&#20102;MobileNetV3&#39592;&#24178;&#30340;UNet&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#26631;&#20934;&#20998;&#21106;&#19968;&#33268;&#24615;&#25351;&#26631;&#20197;&#21450;&#33033;&#32476;&#33180;&#21402;&#24230;&#21644;&#38754;&#31215;&#30340;&#34893;&#29983;&#24230;&#37327;&#34987;&#29992;&#20110;&#35780;&#20272;DeepGPET&#65292;&#21516;&#26102;&#36824;&#36827;&#34892;&#20102;&#20020;&#24202;&#30524;&#31185;&#21307;&#29983;&#30340;&#23450;&#24615;&#35780;&#20272;&#12290;&#32467;&#26524;&#65306;DeepGPET&#22312;&#26469;&#33258;3&#20010;&#20020;&#24202;&#30740;&#31350;&#30340;&#25968;&#25454;&#19978;&#19982;GPET&#36798;&#21040;&#20102;&#24456;&#22909;&#30340;&#19968;&#33268;&#24615;&#65288;AUC = 0.9994&#65292;Dice = 0.9664&#65307;&#33033;&#32476;&#33180;&#21402;&#24230;&#30340;&#30382;&#23572;&#36874;&#30456;&#20851;&#31995;&#25968;&#20026;0.8908&#65292;&#33033;&#32476;&#33180;&#38754;&#31215;&#30340;&#30382;&#23572;&#36874;&#30456;&#20851;&#31995;&#25968;&#20026;0.9082&#65289;&#65292;&#21516;&#26102;&#23558;&#22312;&#26631;&#20934;&#31508;&#35760;&#26412;&#30005;&#33041;CPU&#19978;&#27599;&#24352;&#22270;&#20687;&#30340;&#24179;&#22343;&#22788;&#29702;&#26102;&#38388;&#32553;&#30701;&#33267;34.49&#31186;
&lt;/p&gt;
&lt;p&gt;
Purpose: To develop an open-source, fully-automatic deep learning algorithm, DeepGPET, for choroid region segmentation in optical coherence tomography (OCT) data. Methods: We used a dataset of 715 OCT B-scans (82 subjects, 115 eyes) from 3 clinical studies related to systemic disease. Ground truth segmentations were generated using a clinically validated, semi-automatic choroid segmentation method, Gaussian Process Edge Tracing (GPET). We finetuned a UNet with MobileNetV3 backbone pre-trained on ImageNet. Standard segmentation agreement metrics, as well as derived measures of choroidal thickness and area, were used to evaluate DeepGPET, alongside qualitative evaluation from a clinical ophthalmologist. Results: DeepGPET achieves excellent agreement with GPET on data from 3 clinical studies (AUC=0.9994, Dice=0.9664; Pearson correlation of 0.8908 for choroidal thickness and 0.9082 for choroidal area), while reducing the mean processing time per image on a standard laptop CPU from 34.49s (
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;209&#31687;&#20851;&#20110;NLP&#27169;&#22411;&#20559;&#35265;&#30340;&#35770;&#25991;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#28041;&#21450;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#23450;&#20041;&#65292;&#24182;&#30830;&#23450;&#20102;NLP&#20559;&#35265;&#30740;&#31350;&#30340;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#12290;&#24403;&#21069;&#21435;&#20559;&#35265;&#25216;&#26415;&#21482;&#26159;&#38544;&#34255;&#20102;&#20559;&#35265;&#32780;&#19981;&#26159;&#30495;&#27491;&#21435;&#38500;&#23427;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.08158</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Survey on Sociodemographic Bias in Natural Language Processing. (arXiv:2306.08158v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;209&#31687;&#20851;&#20110;NLP&#27169;&#22411;&#20559;&#35265;&#30340;&#35770;&#25991;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#28041;&#21450;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#23450;&#20041;&#65292;&#24182;&#30830;&#23450;&#20102;NLP&#20559;&#35265;&#30740;&#31350;&#30340;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#12290;&#24403;&#21069;&#21435;&#20559;&#35265;&#25216;&#26415;&#21482;&#26159;&#38544;&#34255;&#20102;&#20559;&#35265;&#32780;&#19981;&#26159;&#30495;&#27491;&#21435;&#38500;&#23427;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24448;&#24448;&#20250;&#23398;&#20064;&#21040;&#38750;&#39044;&#26399;&#30340;&#20559;&#35265;&#65292;&#36825;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#33021;&#20250;&#20135;&#29983;&#26377;&#23475;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#23545;209&#31687;&#20851;&#20110;NLP&#27169;&#22411;&#20013;&#20559;&#35265;&#30340;&#35770;&#25991;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#35770;&#25991;&#28041;&#21450;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#20559;&#35265;&#19982;&#30495;&#23454;&#19990;&#30028;&#30340;&#21361;&#23475;&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#25105;&#20204;&#20511;&#37492;&#24515;&#29702;&#23398;&#21644;&#34892;&#20026;&#32463;&#27982;&#23398;&#30340;&#24605;&#24819;&#65292;&#25552;&#20986;&#20102;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#23450;&#20041;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;NLP&#20559;&#35265;&#30740;&#31350;&#30340;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#65306;&#20559;&#35265;&#31867;&#22411;&#12289;&#37327;&#21270;&#20559;&#35265;&#21644;&#21435;&#20559;&#35265;&#12290;&#25105;&#20204;&#35748;&#20026;&#24403;&#21069;&#23545;&#20110;&#37327;&#21270;&#20559;&#35265;&#30340;&#26041;&#27861;&#23384;&#22312;&#21487;&#38752;&#24615;&#38382;&#39064;&#65292;&#35768;&#22810;&#20559;&#35265;&#24230;&#37327;&#24182;&#19981;&#28041;&#21450;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#20559;&#35265;&#65292;&#24403;&#21069;&#30340;&#21435;&#20559;&#35265;&#25216;&#26415;&#26159;&#34920;&#38754;&#30340;&#65292;&#21482;&#26159;&#38544;&#34255;&#20102;&#20559;&#35265;&#65292;&#32780;&#19981;&#26159;&#30495;&#27491;&#21435;&#38500;&#23427;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26410;&#26469;&#24037;&#20316;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks often learn unintended biases during training, which might have harmful effects when deployed in real-world settings. This paper surveys 209 papers on bias in NLP models, most of which address sociodemographic bias. To better understand the distinction between bias and real-world harm, we turn to ideas from psychology and behavioral economics to propose a definition for sociodemographic bias. We identify three main categories of NLP bias research: types of bias, quantifying bias, and debiasing. We conclude that current approaches on quantifying bias face reliability issues, that many of the bias metrics do not relate to real-world biases, and that current debiasing techniques are superficial and hide bias rather than removing it. Finally, we provide recommendations for future work.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;P2MPO&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#40065;&#26834;&#31163;&#32447;RL&#30340;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#28789;&#27963;&#30340;&#27169;&#22411;&#20272;&#35745;&#23376;&#20363;&#31243;&#21644;&#21452;&#37325;&#24754;&#35266;&#30340;&#31574;&#30053;&#20248;&#21270;&#27493;&#39588;&#65292;&#37319;&#29992;&#21452;&#37325;&#24754;&#35266;&#24615;&#21407;&#21017;&#20197;&#20811;&#26381;&#27169;&#22411;&#20559;&#31227;&#31561;&#38382;&#39064;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#20551;&#35774;&#19979;&#65292;&#35813;&#26694;&#26550;&#22312;&#25317;&#26377;&#33391;&#22909;&#30340;&#40065;&#26834;&#37096;&#20998;&#35206;&#30422;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26159;&#20855;&#22791;&#39640;&#25928;&#24615;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.09659</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#40065;&#26834;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65306;&#22522;&#20110;&#21452;&#37325;&#24754;&#35266;&#24615;&#30340;&#36890;&#29992;&#31639;&#27861;&#21644;&#24378;&#20581;&#37096;&#20998;&#35206;&#30422;
&lt;/p&gt;
&lt;p&gt;
Double Pessimism is Provably Efficient for Distributionally Robust Offline Reinforcement Learning: Generic Algorithm and Robust Partial Coverage. (arXiv:2305.09659v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;P2MPO&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#40065;&#26834;&#31163;&#32447;RL&#30340;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#28789;&#27963;&#30340;&#27169;&#22411;&#20272;&#35745;&#23376;&#20363;&#31243;&#21644;&#21452;&#37325;&#24754;&#35266;&#30340;&#31574;&#30053;&#20248;&#21270;&#27493;&#39588;&#65292;&#37319;&#29992;&#21452;&#37325;&#24754;&#35266;&#24615;&#21407;&#21017;&#20197;&#20811;&#26381;&#27169;&#22411;&#20559;&#31227;&#31561;&#38382;&#39064;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#20551;&#35774;&#19979;&#65292;&#35813;&#26694;&#26550;&#22312;&#25317;&#26377;&#33391;&#22909;&#30340;&#40065;&#26834;&#37096;&#20998;&#35206;&#30422;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26159;&#20855;&#22791;&#39640;&#25928;&#24615;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#40065;&#26834;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;&#40065;&#26834;&#31163;&#32447;RL&#65289;&#65292;&#20854;&#26088;&#22312;&#20174;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#32431;&#31929;&#22320;&#25214;&#21040;&#19968;&#20010;&#33021;&#22815;&#22312;&#25200;&#21160;&#29615;&#22659;&#20013;&#34920;&#29616;&#33391;&#22909;&#30340;&#26368;&#20248;&#24378;&#40065;&#26834;&#31574;&#30053;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;P2MPO&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#28789;&#27963;&#30340;&#27169;&#22411;&#20272;&#35745;&#23376;&#20363;&#31243;&#21644;&#21452;&#37325;&#24754;&#35266;&#30340;&#31574;&#30053;&#20248;&#21270;&#27493;&#39588;&#12290;&#21452;&#37325;&#24754;&#35266;&#24615;&#21407;&#21017;&#23545;&#20110;&#20811;&#26381;&#30001;&#34892;&#20026;&#31574;&#30053;&#21644;&#30446;&#26631;&#31574;&#30053;&#23478;&#26063;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#20197;&#21450;&#21517;&#20041;&#27169;&#22411;&#30340;&#25200;&#21160;&#25152;&#24341;&#36215;&#30340;&#20998;&#24067;&#20559;&#31227;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#23545;&#27169;&#22411;&#20272;&#35745;&#23376;&#20363;&#31243;&#36827;&#34892;&#19968;&#23450;&#20934;&#30830;&#24615;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;P2MPO&#31639;&#27861;&#22312;&#25317;&#26377;&#33391;&#22909;&#30340;&#40065;&#26834;&#37096;&#20998;&#35206;&#30422;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26159;&#21487;&#35777;&#26126;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study distributionally robust offline reinforcement learning (robust offline RL), which seeks to find an optimal robust policy purely from an offline dataset that can perform well in perturbed environments. We propose a generic algorithm framework \underline{D}oubly \underline{P}essimistic \underline{M}odel-based \underline{P}olicy \underline{O}ptimization ($\texttt{P}^2\texttt{MPO}$) for robust offline RL, which features a novel combination of a flexible model estimation subroutine and a doubly pessimistic policy optimization step. The \emph{double pessimism} principle is crucial to overcome the distributional shift incurred by i) the mismatch between behavior policy and the family of target policies; and ii) the perturbation of the nominal model. Under certain accuracy assumptions on the model estimation subroutine, we show that $\texttt{P}^2\texttt{MPO}$ is provably efficient with \emph{robust partial coverage data}, which means that the offline dataset has good coverage of the d
&lt;/p&gt;</description></item><item><title>Chronosymbolic Learning&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#23558;&#31526;&#21495;&#25512;&#29702;&#21644;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#39640;&#25928;&#22320;&#35299;&#20915;CHC&#31995;&#32479;&#12290;&#23454;&#39564;&#35777;&#26126;&#23427;&#22312;288&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;&#35768;&#22810;&#20855;&#26377;&#38750;&#32447;&#24615;&#25972;&#25968;&#31639;&#26415;&#30340;&#23454;&#20363;&#12290;</title><link>http://arxiv.org/abs/2305.01206</link><description>&lt;p&gt;
Chronosymbolic Learning: &#32467;&#21512;&#31526;&#21495;&#25512;&#29702;&#19982;&#24402;&#32435;&#23398;&#20064;&#30340;&#26377;&#25928;CHC&#27714;&#35299;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Chronosymbolic Learning: Efficient CHC Solving with Symbolic Reasoning and Inductive Learning. (arXiv:2305.01206v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01206
&lt;/p&gt;
&lt;p&gt;
Chronosymbolic Learning&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#23558;&#31526;&#21495;&#25512;&#29702;&#21644;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#39640;&#25928;&#22320;&#35299;&#20915;CHC&#31995;&#32479;&#12290;&#23454;&#39564;&#35777;&#26126;&#23427;&#22312;288&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;&#35768;&#22810;&#20855;&#26377;&#38750;&#32447;&#24615;&#25972;&#25968;&#31639;&#26415;&#30340;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CHC (Constrained Horn Clauses)&#30340;&#27714;&#35299;&#26159;&#35768;&#22810;&#39564;&#35777;&#21644;&#20998;&#26512;&#20219;&#21153;&#30340;&#22522;&#26412;&#25361;&#25112;&#12290;&#25968;&#25454;&#39537;&#21160;&#27861;&#22312;&#25552;&#39640;CHC&#27714;&#35299;&#25928;&#29575;&#26041;&#38754;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#25163;&#21160;&#21019;&#24314;&#21644;&#35843;&#25972;&#21508;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#32321;&#29712;&#24037;&#20316;&#12290;&#20294;&#25968;&#25454;&#39537;&#21160;&#30340;CHC&#27714;&#35299;&#22120;&#19982;&#22522;&#20110;&#31526;&#21495;&#25512;&#29702;&#30340;&#27714;&#35299;&#22120;&#20043;&#38388;&#23384;&#22312;&#24040;&#22823;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;"Chronosymbolic Learning"&#65292;&#23427;&#23558;&#31526;&#21495;&#20449;&#24687;&#21644;&#25968;&#20540;&#25968;&#25454;&#28857;&#32479;&#19968;&#36215;&#26469;&#65292;&#23558;CHC&#31995;&#32479;&#39640;&#25928;&#22320;&#27714;&#35299;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;Chronosymbolic Learning&#30340;&#19968;&#20010;&#31616;&#21333;&#23454;&#20363;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#23398;&#20064;&#22120;&#21644;&#19968;&#20010;BMC&#26679;&#24335;&#30340;&#25512;&#29702;&#22120;&#12290;&#23613;&#31649;&#35813;&#24037;&#20855;&#38750;&#24120;&#31616;&#21333;&#65292;&#20294;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#25928;&#21147;&#21644;&#20581;&#22766;&#24615;&#12290;&#23427;&#22312;&#30001;288&#20010;&#22522;&#20934;&#27979;&#35797;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#19978;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;CHC&#27714;&#35299;&#22120;&#65292;&#20854;&#20013;&#21253;&#25324;&#35768;&#22810;&#21253;&#21547;&#38750;&#32447;&#24615;&#25972;&#25968;&#31639;&#26415;&#30340;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving Constrained Horn Clauses (CHCs) is a fundamental challenge behind a wide range of verification and analysis tasks. Data-driven approaches show great promise in improving CHC solving without the painstaking manual effort of creating and tuning various heuristics. However, a large performance gap exists between data-driven CHC solvers and symbolic reasoning-based solvers. In this work, we develop a simple but effective framework, "Chronosymbolic Learning", which unifies symbolic information and numerical data points to solve a CHC system efficiently. We also present a simple instance of Chronosymbolic Learning with a data-driven learner and a BMC-styled reasoner. Despite its great simplicity, experimental results show the efficacy and robustness of our tool. It outperforms state-of-the-art CHC solvers on a dataset consisting of 288 benchmarks, including many instances with non-linear integer arithmetics.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#22242;&#38431;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#20215;&#20307;&#30340;3D&#31070;&#32463;&#37325;&#24314;&#26694;&#26550;CVRecon&#65292;&#21033;&#29992;&#20016;&#23500;&#30340;&#20960;&#20309;&#23884;&#20837;&#26469;&#20419;&#36827;3D&#20960;&#20309;&#29305;&#24449;&#23398;&#20064;&#12290;&#36890;&#36807;&#24341;&#20837;&#23556;&#32447;&#19978;&#19979;&#25991;&#34917;&#20607;&#20195;&#20215;&#20307;&#65288;RCCV&#65289;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#35270;&#35282;&#30456;&#20851;&#20449;&#24687;&#30340;&#23436;&#25972;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#21508;&#31181;&#24230;&#37327;&#26041;&#38754;&#26174;&#30528;&#25552;&#39640;&#20102;&#37325;&#24314;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2304.14633</link><description>&lt;p&gt;
CVRecon: &#37325;&#26032;&#24605;&#32771;&#31070;&#32463;&#37325;&#24314;&#30340;3D&#20960;&#20309;&#29305;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CVRecon: Rethinking 3D Geometric Feature Learning For Neural Reconstruction. (arXiv:2304.14633v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14633
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#22242;&#38431;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#20215;&#20307;&#30340;3D&#31070;&#32463;&#37325;&#24314;&#26694;&#26550;CVRecon&#65292;&#21033;&#29992;&#20016;&#23500;&#30340;&#20960;&#20309;&#23884;&#20837;&#26469;&#20419;&#36827;3D&#20960;&#20309;&#29305;&#24449;&#23398;&#20064;&#12290;&#36890;&#36807;&#24341;&#20837;&#23556;&#32447;&#19978;&#19979;&#25991;&#34917;&#20607;&#20195;&#20215;&#20307;&#65288;RCCV&#65289;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#35270;&#35282;&#30456;&#20851;&#20449;&#24687;&#30340;&#23436;&#25972;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#21508;&#31181;&#24230;&#37327;&#26041;&#38754;&#26174;&#30528;&#25552;&#39640;&#20102;&#37325;&#24314;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20351;&#29992;&#22270;&#20687;&#24207;&#21015;&#36827;&#34892;&#31070;&#32463;&#37325;&#24314;&#30340;&#36827;&#23637;&#21462;&#24471;&#20102;&#26174;&#30528;&#36827;&#23637;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#32570;&#20047;&#28145;&#24230;&#20449;&#24687;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#20307;&#31215;&#30340;&#25216;&#26415;&#20165;&#27839;&#25972;&#20010;&#30456;&#26426;&#20809;&#32447;&#22797;&#21046;&#23545;&#35937;&#34920;&#38754;&#30340;2D&#22270;&#20687;&#29305;&#24449;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#22797;&#21046;&#20250;&#22312;&#31354;&#27934;&#21644;&#36974;&#25377;&#31354;&#38388;&#20013;&#24341;&#20837;&#22122;&#22768;&#65292;&#20174;&#32780;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;3D&#20960;&#20309;&#20307;&#25104;&#24418;&#26041;&#38754;&#20135;&#29983;&#25361;&#25112;&#12290;&#21463;&#20256;&#32479;&#22810;&#35270;&#35282;&#31435;&#20307;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;3D&#31070;&#32463;&#37325;&#24314;&#26694;&#26550;CVRecon&#65292;&#26088;&#22312;&#21033;&#29992;&#20195;&#20215;&#20307;&#20013;&#20016;&#23500;&#30340;&#20960;&#20309;&#23884;&#20837;&#26469;&#20419;&#36827;3D&#20960;&#20309;&#29305;&#24449;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;3D&#20960;&#20309;&#29305;&#24449;&#34920;&#31034;&#27861;&#8212;&#8212;&#23556;&#32447;&#19978;&#19979;&#25991;&#34917;&#20607;&#20195;&#20215;&#20307;&#65288;RCCV&#65289;&#65292;&#23427;&#20855;&#26377;&#26356;&#22909;&#30340;&#23436;&#25972;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#21487;&#20197;&#32534;&#30721;&#35270;&#35282;&#30456;&#20851;&#20449;&#24687;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#24230;&#37327;&#26041;&#38754;&#26174;&#30528;&#25552;&#39640;&#20102;&#37325;&#24314;&#36136;&#37327;&#65292;&#24182;&#24674;&#22797;&#20102;&#28165;&#26224;&#30340;
&lt;/p&gt;
&lt;p&gt;
Recent advances in neural reconstruction using posed image sequences have made remarkable progress. However, due to the lack of depth information, existing volumetric-based techniques simply duplicate 2D image features of the object surface along the entire camera ray. We contend this duplication introduces noise in empty and occluded spaces, posing challenges for producing high-quality 3D geometry. Drawing inspiration from traditional multi-view stereo methods, we propose an end-to-end 3D neural reconstruction framework CVRecon, designed to exploit the rich geometric embedding in the cost volumes to facilitate 3D geometric feature learning. Furthermore, we present Ray-contextual Compensated Cost Volume (RCCV), a novel 3D geometric feature representation that encodes view-dependent information with improved integrity and robustness. Through comprehensive experiments, we demonstrate that our approach significantly improves the reconstruction quality in various metrics and recovers clear
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#35777;&#26126;&#29616;&#26377;&#30340;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#25351;&#26631;&#19982;&#20154;&#31867;&#36873;&#25321;&#30456;&#20851;&#24615;&#19981;&#24378;&#12290;&#22240;&#27492;&#65292;&#20182;&#20204;&#20351;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#35757;&#32451;&#20102;&#19968;&#20010;&#20154;&#31867;&#20559;&#22909;&#20998;&#31867;&#22120;&#65292;&#24182;&#36890;&#36807;HPS&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#20197;&#26356;&#22909;&#22320;&#23558;Stable Diffusion&#19982;&#20154;&#31867;&#23457;&#32654;&#20559;&#22909;&#23545;&#40784;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#39044;&#27979;&#20154;&#31867;&#36873;&#25321;&#26041;&#38754;&#20248;&#20110;CLIP&#65292;&#24182;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.14420</link><description>&lt;p&gt;
&#26356;&#22909;&#22320;&#23558;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Better Aligning Text-to-Image Models with Human Preference. (arXiv:2303.14420v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14420
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#35777;&#26126;&#29616;&#26377;&#30340;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#25351;&#26631;&#19982;&#20154;&#31867;&#36873;&#25321;&#30456;&#20851;&#24615;&#19981;&#24378;&#12290;&#22240;&#27492;&#65292;&#20182;&#20204;&#20351;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#35757;&#32451;&#20102;&#19968;&#20010;&#20154;&#31867;&#20559;&#22909;&#20998;&#31867;&#22120;&#65292;&#24182;&#36890;&#36807;HPS&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#20197;&#26356;&#22909;&#22320;&#23558;Stable Diffusion&#19982;&#20154;&#31867;&#23457;&#32654;&#20559;&#22909;&#23545;&#40784;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#39044;&#27979;&#20154;&#31867;&#36873;&#25321;&#26041;&#38754;&#20248;&#20110;CLIP&#65292;&#24182;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#34028;&#21187;&#21457;&#23637;&#65292;&#20854;&#20013;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#22791;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#27169;&#22411;&#36890;&#24120;&#29983;&#25104;&#30340;&#22270;&#20687;&#19982;&#20154;&#31867;&#23457;&#32654;&#20559;&#22909;&#19981;&#31526;&#65292;&#20363;&#22914;&#32930;&#20307;&#21644;&#38754;&#37096;&#34920;&#24773;&#30340;&#32452;&#21512;&#19981;&#33258;&#28982;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#26469;&#33258;Stable Foundation Discord&#39057;&#36947;&#30340;&#20154;&#31867;&#36873;&#25321;&#29983;&#25104;&#22270;&#20687;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#24403;&#21069;&#30340;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#25351;&#26631;&#19982;&#20154;&#31867;&#36873;&#25321;&#30456;&#20851;&#24615;&#19981;&#24378;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#20102;&#19968;&#20010;&#20154;&#31867;&#20559;&#22909;&#20998;&#31867;&#22120;&#65292;&#24182;&#22522;&#20110;&#35813;&#20998;&#31867;&#22120;&#24471;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20154;&#31867;&#20559;&#22909;&#30340;&#20998;&#25968;&#65288;HPS&#65289;&#12290;&#36890;&#36807;HPS&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20197;&#26356;&#22909;&#22320;&#23558;Stable Diffusion&#19982;&#20154;&#31867;&#23457;&#32654;&#20559;&#22909;&#23545;&#40784;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;HPS&#22312;&#39044;&#27979;&#20154;&#31867;&#36873;&#25321;&#26041;&#38754;&#20248;&#20110;CLIP&#65292;&#24182;&#19988;&#20855;&#26377;&#23545;&#26469;&#33258;&#20854;&#20182;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#30340;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;HPS&#35843;&#25972;Stable Diffusion&#30340;&#22122;&#22768;&#27700;&#24179;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#29983;&#25104;&#22270;&#20687;&#30340;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed a rapid growth of deep generative models, with text-to-image models gaining significant attention from the public. However, existing models often generate images that do not align well with human aesthetic preferences, such as awkward combinations of limbs and facial expressions. To address this issue, we collect a dataset of human choices on generated images from the Stable Foundation Discord channel. Our experiments demonstrate that current evaluation metrics for generative models do not correlate well with human choices. Thus, we train a human preference classifier with the collected dataset and derive a Human Preference Score (HPS) based on the classifier. Using the HPS, we propose a simple yet effective method to adapt Stable Diffusion to better align with human aesthetic preferences. Our experiments show that the HPS outperforms CLIP in predicting human choices and has good generalization capability towards images generated from other models. By tuning
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;$h$-&#20998;&#26512;&#21644;&#25968;&#25454;&#24182;&#34892;&#21152;&#36895;&#30340;&#29289;&#29702;&#21551;&#21457;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#30340;&#26032;&#21327;&#35758;&#65292;&#21487;&#20197;&#23454;&#29616;&#20855;&#26377;&#35268;&#27169;&#40065;&#26834;&#24615;&#21644;&#39640;&#21534;&#21520;&#37327;&#30340;PIML&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#21327;&#35758;&#26131;&#20110;&#23454;&#29616;&#65292;&#19981;&#20250;&#24433;&#21709;&#35757;&#32451;&#65292;&#24182;&#19988;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#21487;&#25511;&#24615;&#65292;&#20026;&#23454;&#29616;&#36890;&#29992;&#30340;&#35268;&#27169;&#40065;&#26834;PIML&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;</title><link>http://arxiv.org/abs/2302.08835</link><description>&lt;p&gt;
h&#20998;&#26512;&#21644;&#25968;&#25454;&#24182;&#34892;&#30340;&#29289;&#29702;&#21551;&#21457;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
h-analysis and data-parallel physics-informed neural networks. (arXiv:2302.08835v2 [cs.CE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;$h$-&#20998;&#26512;&#21644;&#25968;&#25454;&#24182;&#34892;&#21152;&#36895;&#30340;&#29289;&#29702;&#21551;&#21457;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#30340;&#26032;&#21327;&#35758;&#65292;&#21487;&#20197;&#23454;&#29616;&#20855;&#26377;&#35268;&#27169;&#40065;&#26834;&#24615;&#21644;&#39640;&#21534;&#21520;&#37327;&#30340;PIML&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#21327;&#35758;&#26131;&#20110;&#23454;&#29616;&#65292;&#19981;&#20250;&#24433;&#21709;&#35757;&#32451;&#65292;&#24182;&#19988;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#21487;&#25511;&#24615;&#65292;&#20026;&#23454;&#29616;&#36890;&#29992;&#30340;&#35268;&#27169;&#40065;&#26834;PIML&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#32034;&#20102;&#29289;&#29702;&#21551;&#21457;&#26426;&#22120;&#23398;&#20064;&#65288;PIML&#65289;&#26041;&#26696;&#30340;&#25968;&#25454;&#24182;&#34892;&#21152;&#36895;&#65292;&#22312;&#22810;&#20010;&#22270;&#24418;&#22788;&#29702;&#21333;&#20803;&#65288;GPUs&#65289;&#20307;&#31995;&#32467;&#26500;&#19979;&#20851;&#27880;&#29289;&#29702;&#21551;&#21457;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#12290;&#20026;&#20102;&#24320;&#21457;&#36866;&#29992;&#20110;&#22797;&#26434;&#24212;&#29992;&#30340;&#35268;&#27169;&#40065;&#26834;&#21644;&#39640;&#21534;&#21520;&#37327;&#30340;PIML&#27169;&#22411;&#65288;&#20363;&#22914;&#65292;&#28041;&#21450;&#22797;&#26434;&#21644;&#39640;&#32500;&#39046;&#22495;&#12289;&#38750;&#32447;&#24615;&#25805;&#20316;&#31526;&#25110;&#22810;&#29289;&#29702;&#23398;&#65289;&#65292;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;$h$-&#20998;&#26512;&#21644;&#36890;&#36807;Horovod&#35757;&#32451;&#26694;&#26550;&#36827;&#34892;&#25968;&#25454;&#24182;&#34892;&#21152;&#36895;&#30340;&#26032;&#21327;&#35758;&#12290;&#35813;&#21327;&#35758;&#22522;&#20110;&#23545;&#27867;&#21270;&#35823;&#24046;&#21644;&#35757;&#32451;-&#27979;&#35797;&#24046;&#36317;&#30340;&#26032;&#25910;&#25947;&#30028;&#38480;&#12290;&#25105;&#20204;&#34920;&#26126;&#21152;&#36895;&#23454;&#29616;&#31616;&#21333;&#65292;&#19981;&#20250;&#25439;&#23475;&#35757;&#32451;&#65292;&#24182;&#19988;&#35777;&#26126;&#26159;&#39640;&#25928;&#21644;&#21487;&#25511;&#30340;&#65292;&#20026;&#36890;&#29992;&#30340;&#35268;&#27169;&#40065;&#26834;PIML&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#36890;&#36807;&#22686;&#21152;&#22797;&#26434;&#24615;&#30340;&#22823;&#37327;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#31283;&#20581;&#24615;&#21644;&#19968;&#33268;&#24615;&#65292;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;
&lt;/p&gt;
&lt;p&gt;
We explore the data-parallel acceleration of physics-informed machine learning (PIML) schemes, with a focus on physics-informed neural networks (PINNs) for multiple graphics processing units (GPUs) architectures. In order to develop scale-robust and high-throughput PIML models for sophisticated applications which may require a large number of training points (e.g., involving complex and high-dimensional domains, non-linear operators or multi-physics), we detail a novel protocol based on $h$-analysis and data-parallel acceleration through the Horovod training framework. The protocol is backed by new convergence bounds for the generalization error and the train-test gap. We show that the acceleration is straightforward to implement, does not compromise training, and proves to be highly efficient and controllable, paving the way towards generic scale-robust PIML. Extensive numerical experiments with increasing complexity illustrate its robustness and consistency, offering a wide range of 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#26080;&#30417;&#30563;&#26412;&#20307;&#21305;&#37197;&#25110;&#26412;&#20307;&#23545;&#40784;&#35270;&#20026;&#32763;&#35793;&#20219;&#21153;&#30340;&#26032;&#35270;&#35282;&#30340;Truveta Mapper&#26694;&#26550;&#65292;&#22312;&#38646;&#26679;&#26412;&#12289;&#32479;&#19968;&#21644;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#19979;&#25191;&#34892;&#22810;&#26412;&#20307;&#23545;&#40784;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#38388;&#24310;&#36831;&#21644;&#23545;&#40784;&#36136;&#37327;&#26041;&#38754;&#32988;&#36807;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#65292;&#26080;&#38656;&#26174;&#24335;&#36328;&#26412;&#20307;&#25163;&#21160;&#26631;&#27880;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2301.09767</link><description>&lt;p&gt;
Truveta Mapper&#65306;&#19968;&#20010;&#38646;&#26679;&#26412;&#26412;&#20307;&#26144;&#23556;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Truveta Mapper: A Zero-shot Ontology Alignment Framework. (arXiv:2301.09767v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09767
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#26080;&#30417;&#30563;&#26412;&#20307;&#21305;&#37197;&#25110;&#26412;&#20307;&#23545;&#40784;&#35270;&#20026;&#32763;&#35793;&#20219;&#21153;&#30340;&#26032;&#35270;&#35282;&#30340;Truveta Mapper&#26694;&#26550;&#65292;&#22312;&#38646;&#26679;&#26412;&#12289;&#32479;&#19968;&#21644;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#19979;&#25191;&#34892;&#22810;&#26412;&#20307;&#23545;&#40784;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#38388;&#24310;&#36831;&#21644;&#23545;&#40784;&#36136;&#37327;&#26041;&#38754;&#32988;&#36807;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#65292;&#26080;&#38656;&#26174;&#24335;&#36328;&#26412;&#20307;&#25163;&#21160;&#26631;&#27880;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26080;&#30417;&#30563;&#26412;&#20307;&#21305;&#37197;(Ontology Matching, OM)&#25110;&#26412;&#20307;&#23545;&#40784;(Ontology Alignment, OA)&#35270;&#20026;&#32763;&#35793;&#20219;&#21153;&#30340;&#26032;&#35270;&#35282;&#12290;&#23558;&#26412;&#20307;&#34920;&#31034;&#20026;&#22270;&#24418;&#65292;&#22312;&#28304;&#26412;&#20307;&#22270;&#20013;&#30340;&#33410;&#28857;&#21040;&#30446;&#26631;&#26412;&#20307;&#22270;&#20013;&#30340;&#36335;&#24452;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#12290;&#25152;&#25552;&#20986;&#30340;Truveta Mapper (TM)&#26694;&#26550;&#21033;&#29992;&#22810;&#20219;&#21153;&#24207;&#21015;&#21040;&#24207;&#21015;&#36716;&#25442;&#22120;&#27169;&#22411;&#65292;&#22312;&#38646;&#26679;&#26412;&#12289;&#32479;&#19968;&#21644;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#19979;&#25191;&#34892;&#22810;&#26412;&#20307;&#23545;&#40784;&#12290;&#22810;&#20219;&#21153;&#20351;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#26469;&#38544;&#21547;&#22320;&#23398;&#20064;&#19981;&#21516;&#26412;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#26080;&#38656;&#20219;&#20309;&#26174;&#24335;&#30340;&#36328;&#26412;&#20307;&#25163;&#21160;&#26631;&#27880;&#25968;&#25454;&#12290;&#36825;&#20063;&#20351;&#24471;&#35813;&#26694;&#26550;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#38388;&#24310;&#36831;&#21644;&#23545;&#40784;&#36136;&#37327;&#26041;&#38754;&#32988;&#36807;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#12290;&#27169;&#22411;&#20165;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#25991;&#26412;&#35821;&#26009;&#24211;&#21644;&#20869;&#37096;&#26412;&#20307;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#12290;&#35813;&#26041;&#26696;&#20248;&#20110;&#29616;&#26377;&#26631;&#20934;&#22522;&#20934;&#35299;&#20915;&#26041;&#26696;&#65292;&#22914;Edit-Similarity&#21644;MINTE+&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, a new perspective is suggested for unsupervised Ontology Matching (OM) or Ontology Alignment (OA) by treating it as a translation task. Ontologies are represented as graphs, and the translation is performed from a node in the source ontology graph to a path in the target ontology graph. The proposed framework, Truveta Mapper (TM), leverages a multi-task sequence-to-sequence transformer model to perform alignment across multiple ontologies in a zero-shot, unified and end-to-end manner. Multi-tasking enables the model to implicitly learn the relationship between different ontologies via transfer-learning without requiring any explicit cross-ontology manually labeled data. This also enables the formulated framework to outperform existing solutions for both runtime latency and alignment quality. The model is pre-trained and fine-tuned only on publicly available text corpus and inner-ontologies data. The proposed solution outperforms state-of-the-art approaches, Edit-Similari
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#22522;&#20110;&#35821;&#20041;&#30340;&#27807;&#36890;&#26041;&#38754;&#30340;&#25945;&#31243;&#20860;&#32508;&#36848;&#65292;&#22238;&#39038;&#20102;&#25991;&#29486;&#65292;&#20171;&#32461;&#20102;SemCom&#29983;&#24577;&#31995;&#32479;&#65292;&#24182;&#23558;&#30740;&#31350;&#26041;&#21521;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#21551;&#29992;&#25216;&#26415;&#30340;&#20998;&#31867;&#21644;&#26410;&#26469;&#24212;&#29992;&#22330;&#26223;&#30340;&#23637;&#26395;&#12290;</title><link>http://arxiv.org/abs/2212.08487</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#20041;&#30340;&#27807;&#36890;&#65306;&#19968;&#31687;&#25945;&#31243;&#20860;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Semantics-Empowered Communication: A Tutorial-cum-Survey. (arXiv:2212.08487v3 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#22522;&#20110;&#35821;&#20041;&#30340;&#27807;&#36890;&#26041;&#38754;&#30340;&#25945;&#31243;&#20860;&#32508;&#36848;&#65292;&#22238;&#39038;&#20102;&#25991;&#29486;&#65292;&#20171;&#32461;&#20102;SemCom&#29983;&#24577;&#31995;&#32479;&#65292;&#24182;&#23558;&#30740;&#31350;&#26041;&#21521;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#21551;&#29992;&#25216;&#26415;&#30340;&#20998;&#31867;&#21644;&#26410;&#26469;&#24212;&#29992;&#22330;&#26223;&#30340;&#23637;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;&#35821;&#20041;&#30340;&#27807;&#36890;&#65288;SemCom&#65289;&#30740;&#31350;&#30340;&#20852;&#36215;&#65292;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#23545;&#20854;&#21508;&#20010;&#26041;&#38754;&#65288;&#22914;&#29702;&#35770;&#12289;&#24212;&#29992;&#12289;&#24230;&#37327;&#21644;&#23454;&#29616;&#65289;&#30340;&#20852;&#36259;&#19981;&#26029;&#22686;&#38271;&#12290;&#26412;&#25991;&#30340;&#30446;&#30340;&#26159;&#25552;&#20379;&#19968;&#20010;&#32508;&#21512;&#24615;&#30340;&#35843;&#26597;&#65292;&#28085;&#30422;&#20102;&#32972;&#26223;&#21644;&#30740;&#31350;&#20998;&#31867;&#65292;&#20197;&#21450;&#35814;&#32454;&#30340;&#25216;&#26415;&#25945;&#31243;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#22238;&#39038;&#25991;&#29486;&#24182;&#22238;&#31572;&#20851;&#20110;&#35821;&#20041;&#20256;&#36755;&#30340;&#8220;&#20160;&#20040;&#8221;&#21644;&#8220;&#20026;&#20160;&#20040;&#8221;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;SemCom&#29983;&#24577;&#31995;&#32479;&#65292;&#21253;&#25324;&#21382;&#21490;&#12289;&#29702;&#35770;&#12289;&#24230;&#37327;&#12289;&#25968;&#25454;&#38598;&#21644;&#24037;&#20855;&#21253;&#65292;&#24182;&#20171;&#32461;&#20102;&#30740;&#31350;&#26041;&#21521;&#30340;&#20998;&#31867;&#26041;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#35758;&#36890;&#36807;&#26174;&#24335;&#21644;&#38544;&#24335;&#22522;&#20110;&#25512;&#29702;&#30340;&#26041;&#27861;&#23545;&#20851;&#38190;&#30340;&#21551;&#29992;&#25216;&#26415;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#35814;&#32454;&#38416;&#36848;&#23427;&#20204;&#22914;&#20309;&#28436;&#21464;&#24182;&#20026;&#29616;&#20195;&#20869;&#23481;&#21644;&#36890;&#36947;&#35821;&#20041;&#39537;&#21160;&#30340;&#36890;&#20449;&#20570;&#20986;&#36129;&#29486;&#12290;&#38500;&#20102;&#22238;&#39038;&#21644;&#24635;&#32467;&#26368;&#26032;&#30340;e&#25216;&#26415;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#26410;&#26469;&#30340;&#23637;&#26395;&#21644;&#24212;&#29992;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Along with the springing up of the semantics-empowered communication (SemCom) research, it is now witnessing an unprecedentedly growing interest towards a wide range of aspects (e.g., theories, applications, metrics and implementations) in both academia and industry. In this work, we primarily aim to provide a comprehensive survey on both the background and research taxonomy, as well as a detailed technical tutorial. Specifically, we start by reviewing the literature and answering the "what" and "why" questions in semantic transmissions. Afterwards, we present the ecosystems of SemCom, including history, theories, metrics, datasets and toolkits, on top of which the taxonomy for research directions is presented. Furthermore, we propose to categorize the critical enabling techniques by explicit and implicit reasoning-based methods, and elaborate on how they evolve and contribute to modern content &amp; channel semantics-empowered communications. Besides reviewing and summarizing the latest e
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LEAGUE&#30340;&#38598;&#25104;&#20219;&#21153;&#35268;&#21010;&#21644;&#25216;&#33021;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#20219;&#21153;&#35268;&#21010;&#22120;&#30340;&#24341;&#23548;&#65292;&#32467;&#21512;&#31526;&#21495;&#25509;&#21475;&#21644;&#25277;&#35937;&#21270;&#25216;&#24039;&#65292;&#23454;&#29616;&#20102;&#38271;&#26399;&#25805;&#32437;&#20219;&#21153;&#30340;&#23398;&#20064;&#21644;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2210.12631</link><description>&lt;p&gt;
LEAGUE: &#38271;&#26399;&#25805;&#32437;&#30340;&#24341;&#23548;&#24335;&#25216;&#33021;&#23398;&#20064;&#19982;&#25277;&#35937;&#21270;
&lt;/p&gt;
&lt;p&gt;
LEAGUE: Guided Skill Learning and Abstraction for Long-Horizon Manipulation. (arXiv:2210.12631v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12631
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LEAGUE&#30340;&#38598;&#25104;&#20219;&#21153;&#35268;&#21010;&#21644;&#25216;&#33021;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#20219;&#21153;&#35268;&#21010;&#22120;&#30340;&#24341;&#23548;&#65292;&#32467;&#21512;&#31526;&#21495;&#25509;&#21475;&#21644;&#25277;&#35937;&#21270;&#25216;&#24039;&#65292;&#23454;&#29616;&#20102;&#38271;&#26399;&#25805;&#32437;&#20219;&#21153;&#30340;&#23398;&#20064;&#21644;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36741;&#21161;&#26085;&#24120;&#30340;&#20154;&#31867;&#27963;&#21160;&#65292;&#26426;&#22120;&#20154;&#24517;&#39035;&#35299;&#20915;&#22797;&#26434;&#30340;&#38271;&#26399;&#25805;&#32437;&#20219;&#21153;&#65292;&#24182;&#19988;&#33021;&#22815;&#27867;&#21270;&#21040;&#26032;&#29615;&#22659;&#20013;&#12290;&#36817;&#26399;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#23436;&#20840;&#33258;&#20027;&#23398;&#20064;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#24076;&#26395;&#65292;&#20294;&#26159;&#23427;&#20204;&#22312;&#22823;&#22411;&#29615;&#22659;&#20013;&#38590;&#20197;&#36798;&#21040;&#38271;&#26399;&#30446;&#26631;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20219;&#21153;&#19982;&#21160;&#20316;&#35268;&#21010;&#65288;TAMP&#65289;&#26041;&#27861;&#22312;&#35299;&#20915;&#21644;&#27867;&#21270;&#38271;&#26399;&#25805;&#32437;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36825;&#35201;&#24402;&#21151;&#20110;&#23427;&#20204;&#24378;&#22823;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#25277;&#35937;&#21270;&#12290;&#20294;&#26159;&#23427;&#20204;&#20551;&#35774;&#39044;&#20808;&#23450;&#20041;&#30340;&#25216;&#33021;&#38598;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#24212;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#36825;&#20004;&#31181;&#33539;&#24335;&#30340;&#20248;&#28857;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#20219;&#21153;&#35268;&#21010;&#21644;&#25216;&#33021;&#23398;&#20064;&#26694;&#26550;&#65292;&#21517;&#20026;LEAGUE&#65288;&#24102;&#24341;&#23548;&#30340;&#23398;&#20064;&#21644;&#25277;&#35937;&#21270;&#65289;&#12290;LEAGUE&#21033;&#29992;&#20219;&#21153;&#35268;&#21010;&#22120;&#30340;&#31526;&#21495;&#25509;&#21475;&#26469;&#24341;&#23548;&#22522;&#20110;RL&#30340;&#25216;&#33021;&#23398;&#20064;&#65292;&#24182;&#21019;&#24314;&#25277;&#35937;&#30340;&#29366;&#24577;&#31354;&#38388;&#20197;&#23454;&#29616;&#25216;&#33021;&#22797;&#29992;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;LEAGUE&#22312;&#20219;&#21153;&#35268;&#21010;&#31995;&#32479;&#30340;&#22330;&#26223;&#20013;&#23398;&#20064;&#25805;&#32437;&#25216;&#33021;&#65292;&#19981;&#26029;&#22686;&#24378;&#20854;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
To assist with everyday human activities, robots must solve complex long-horizon tasks and generalize to new settings. Recent deep reinforcement learning (RL) methods show promise in fully autonomous learning, but they struggle to reach long-term goals in large environments. On the other hand, Task and Motion Planning (TAMP) approaches excel at solving and generalizing across long-horizon tasks, thanks to their powerful state and action abstractions. But they assume predefined skill sets, which limits their real-world applications. In this work, we combine the benefits of these two paradigms and propose an integrated task planning and skill learning framework named LEAGUE (Learning and Abstraction with Guidance). LEAGUE leverages the symbolic interface of a task planner to guide RL-based skill learning and creates abstract state space to enable skill reuse. More importantly, LEAGUE learns manipulation skills in-situ of the task planning system, continuously growing its capability and t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#24179;&#34913;&#20256;&#25773;&#65288;EP&#65289;&#36827;&#34892;&#24207;&#21015;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;EP&#26159;&#19968;&#31181;&#26356;&#31526;&#21512;&#29983;&#29289;&#21487;&#20449;&#24615;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#19982;&#20256;&#32479;&#30340;&#21453;&#21521;&#20256;&#25773;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#24615;&#12290;&#25991;&#20013;&#21033;&#29992;&#29616;&#20195;Hopfield&#32593;&#32476;&#30340;&#36827;&#23637;&#65292;&#35299;&#20915;&#20102;&#22522;&#20110;EP&#30340;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#21160;&#24577;&#36755;&#20837;&#30340;&#38382;&#39064;&#65292;&#20026;&#22797;&#26434;&#24207;&#21015;&#23398;&#20064;&#38382;&#39064;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2209.09626</link><description>&lt;p&gt;
&#20351;&#29992;&#24179;&#34913;&#20256;&#25773;&#30340;&#24207;&#21015;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Sequence Learning Using Equilibrium Propagation. (arXiv:2209.09626v4 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.09626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#24179;&#34913;&#20256;&#25773;&#65288;EP&#65289;&#36827;&#34892;&#24207;&#21015;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;EP&#26159;&#19968;&#31181;&#26356;&#31526;&#21512;&#29983;&#29289;&#21487;&#20449;&#24615;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#19982;&#20256;&#32479;&#30340;&#21453;&#21521;&#20256;&#25773;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#24615;&#12290;&#25991;&#20013;&#21033;&#29992;&#29616;&#20195;Hopfield&#32593;&#32476;&#30340;&#36827;&#23637;&#65292;&#35299;&#20915;&#20102;&#22522;&#20110;EP&#30340;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#21160;&#24577;&#36755;&#20837;&#30340;&#38382;&#39064;&#65292;&#20026;&#22797;&#26434;&#24207;&#21015;&#23398;&#20064;&#38382;&#39064;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24179;&#34913;&#20256;&#25773;&#65288;EP&#65289;&#26159;&#19968;&#31181;&#24378;&#22823;&#19988;&#26356;&#31526;&#21512;&#29983;&#29289;&#21487;&#20449;&#24615;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#26367;&#20195;&#20256;&#32479;&#30340;&#21453;&#21521;&#20256;&#25773;&#26041;&#27861;&#12290;EP&#30340;&#26377;&#25928;&#24615;&#28304;&#20110;&#23427;&#20165;&#20381;&#36182;&#20110;&#23616;&#37096;&#35745;&#31639;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#30340;&#20004;&#20010;&#38454;&#27573;&#20013;&#21482;&#38656;&#35201;&#19968;&#31181;&#35745;&#31639;&#21333;&#20803;&#65292;&#22240;&#27492;&#22312;&#29983;&#29289;&#21551;&#21457;&#30340;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#31561;&#39046;&#22495;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#24615;&#12290;EP&#27169;&#22411;&#30340;&#21160;&#21147;&#23398;&#21463;&#33021;&#37327;&#20989;&#25968;&#25511;&#21046;&#65292;&#27169;&#22411;&#30340;&#20869;&#37096;&#29366;&#24577;&#38543;&#20043;&#25910;&#25947;&#21040;&#31283;&#23450;&#29366;&#24577;&#65292;&#36981;&#24490;&#30001;&#21516;&#19968;&#20989;&#25968;&#23450;&#20041;&#30340;&#29366;&#24577;&#36716;&#25442;&#35268;&#21017;&#12290;&#28982;&#32780;&#65292;&#26681;&#25454;&#23450;&#20041;&#65292;EP&#35201;&#27714;&#27169;&#22411;&#30340;&#36755;&#20837;&#65288;&#25910;&#25947;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65289;&#22312;&#35757;&#32451;&#30340;&#20004;&#20010;&#38454;&#27573;&#20013;&#37117;&#26159;&#38745;&#24577;&#30340;&#12290;&#22240;&#27492;&#65292;&#19981;&#21487;&#33021;&#20351;&#29992;&#31867;&#20284;LSTM&#25110;GRU&#30340;&#26550;&#26500;&#35774;&#35745;&#22522;&#20110;EP&#30340;&#24207;&#21015;&#20998;&#31867;&#27169;&#22411;&#12290;&#26412;&#25991;&#21033;&#29992;&#29616;&#20195;Hopfield&#32593;&#32476;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#36827;&#19968;&#27493;&#29702;&#35299;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65292;&#24182;&#20026;&#22797;&#26434;&#24207;&#21015;&#23398;&#20064;&#38382;&#39064;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Equilibrium Propagation (EP) is a powerful and more bio-plausible alternative to conventional learning frameworks such as backpropagation. The effectiveness of EP stems from the fact that it relies only on local computations and requires solely one kind of computational unit during both of its training phases, thereby enabling greater applicability in domains such as bio-inspired neuromorphic computing. The dynamics of the model in EP is governed by an energy function and the internal states of the model consequently converge to a steady state following the state transition rules defined by the same. However, by definition, EP requires the input to the model (a convergent RNN) to be static in both the phases of training. Thus it is not possible to design a model for sequence classification using EP with an LSTM or GRU like architecture. In this paper, we leverage recent developments in modern hopfield networks to further understand energy based models and develop solutions for complex 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;Quantile Risk Minimization&#65288;QRM&#65289;&#26041;&#27861;&#23454;&#29616;&#21487;&#33021;&#30340;&#39046;&#22495;&#27867;&#21270;&#30340;&#27010;&#29575;&#24615;&#26694;&#26550;&#12290;&#36890;&#36807;&#26368;&#23567;&#21270;&#39044;&#27979;&#22120;&#39118;&#38505;&#20998;&#24067;&#22312;&#19981;&#21516;&#39046;&#22495;&#19978;&#30340;&#20998;&#20301;&#25968;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#22312;&#27979;&#35797;&#26102;&#20197;&#39640;&#27010;&#29575;&#34920;&#29616;&#33391;&#22909;&#30340;&#39044;&#27979;&#22120;&#12290;</title><link>http://arxiv.org/abs/2207.09944</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#20301;&#25968;&#39118;&#38505;&#26368;&#23567;&#21270;&#23454;&#29616;&#21487;&#33021;&#30340;&#39046;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Probable Domain Generalization via Quantile Risk Minimization. (arXiv:2207.09944v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.09944
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;Quantile Risk Minimization&#65288;QRM&#65289;&#26041;&#27861;&#23454;&#29616;&#21487;&#33021;&#30340;&#39046;&#22495;&#27867;&#21270;&#30340;&#27010;&#29575;&#24615;&#26694;&#26550;&#12290;&#36890;&#36807;&#26368;&#23567;&#21270;&#39044;&#27979;&#22120;&#39118;&#38505;&#20998;&#24067;&#22312;&#19981;&#21516;&#39046;&#22495;&#19978;&#30340;&#20998;&#20301;&#25968;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#22312;&#27979;&#35797;&#26102;&#20197;&#39640;&#27010;&#29575;&#34920;&#29616;&#33391;&#22909;&#30340;&#39044;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#27867;&#21270;&#65288;DG&#65289;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#22810;&#20010;&#30456;&#20851;&#35757;&#32451;&#39046;&#22495;&#30340;&#25968;&#25454;&#65292;&#23547;&#25214;&#22312;&#26410;&#35265;&#27979;&#35797;&#20998;&#24067;&#19978;&#34920;&#29616;&#33391;&#22909;&#30340;&#39044;&#27979;&#22120;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;DG&#36890;&#24120;&#34987;&#25551;&#36848;&#20026;&#23545;&#21487;&#33021;&#30340;&#39046;&#22495;&#38598;&#21512;&#36827;&#34892;&#24179;&#22343;&#25110;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#24179;&#22343;&#24773;&#20917;&#19979;&#34920;&#29616;&#33391;&#22909;&#30340;&#39044;&#27979;&#22120;&#32570;&#20047;&#40065;&#26834;&#24615;&#65292;&#32780;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#34920;&#29616;&#33391;&#22909;&#30340;&#39044;&#27979;&#22120;&#24448;&#24448;&#36807;&#20110;&#20445;&#23432;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27010;&#29575;&#24615;&#26694;&#26550;&#26469;&#36827;&#34892;DG&#65292;&#30446;&#26631;&#26159;&#23398;&#20064;&#20197;&#39640;&#27010;&#29575;&#34920;&#29616;&#33391;&#22909;&#30340;&#39044;&#27979;&#22120;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#35266;&#23519;&#21040;&#30340;&#20998;&#24067;&#21464;&#21270;&#24212;&#35813;&#33021;&#22815;&#21578;&#35785;&#25105;&#20204;&#27979;&#35797;&#26102;&#21487;&#33021;&#30340;&#20998;&#24067;&#21464;&#21270;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#35757;&#32451;&#21644;&#27979;&#35797;&#39046;&#22495;&#26126;&#30830;&#22320;&#35270;&#20026;&#20174;&#21516;&#19968;&#22522;&#30784;&#20803;&#20998;&#24067;&#20013;&#25277;&#21462;&#30340;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#20026;&#20102;&#23454;&#29616;&#21487;&#33021;&#30340;DG&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;Quantile Risk Minimization&#65288;QRM&#65289;&#30340;&#26032;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#26368;&#23567;&#21270;&#39044;&#27979;&#22120;&#39118;&#38505;&#20998;&#24067;&#22312;&#39046;&#22495;&#19978;&#30340;&#945;-&#20998;&#20301;&#25968;&#65292;QRM&#21487;&#20197;&#23454;&#29616;&#27010;&#29575;&#19978;&#30340;DG&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain generalization (DG) seeks predictors which perform well on unseen test distributions by leveraging data drawn from multiple related training distributions or domains. To achieve this, DG is commonly formulated as an average- or worst-case problem over the set of possible domains. However, predictors that perform well on average lack robustness while predictors that perform well in the worst case tend to be overly-conservative. To address this, we propose a new probabilistic framework for DG where the goal is to learn predictors that perform well with high probability. Our key idea is that distribution shifts seen during training should inform us of probable shifts at test time, which we realize by explicitly relating training and test domains as draws from the same underlying meta-distribution. To achieve probable DG, we propose a new optimization problem called Quantile Risk Minimization (QRM). By minimizing the $\alpha$-quantile of predictor's risk distribution over domains, Q
&lt;/p&gt;</description></item><item><title>AceIRL&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20027;&#21160;&#25506;&#32034;&#26469;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#21644;&#31574;&#30053;&#65292;&#22312;&#19981;&#38656;&#35201;&#29615;&#22659;&#29983;&#25104;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#30830;&#23450;&#21487;&#34892;&#22870;&#21169;&#20989;&#25968;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#24182;&#25214;&#21040;&#20391;&#37325;&#20110;&#29615;&#22659;&#20013;&#26368;&#26377;&#20449;&#24687;&#30340;&#21306;&#22495;&#30340;&#25506;&#32034;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2207.08645</link><description>&lt;p&gt;
&#36870;&#24378;&#21270;&#23398;&#20064;&#30340;&#20027;&#21160;&#25506;&#32034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Active Exploration for Inverse Reinforcement Learning. (arXiv:2207.08645v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.08645
&lt;/p&gt;
&lt;p&gt;
AceIRL&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20027;&#21160;&#25506;&#32034;&#26469;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#21644;&#31574;&#30053;&#65292;&#22312;&#19981;&#38656;&#35201;&#29615;&#22659;&#29983;&#25104;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#30830;&#23450;&#21487;&#34892;&#22870;&#21169;&#20989;&#25968;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#24182;&#25214;&#21040;&#20391;&#37325;&#20110;&#29615;&#22659;&#20013;&#26368;&#26377;&#20449;&#24687;&#30340;&#21306;&#22495;&#30340;&#25506;&#32034;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;IRL&#65289;&#26159;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#25512;&#26029;&#22870;&#21169;&#20989;&#25968;&#30340;&#24378;&#22823;&#33539;&#24335;&#12290;&#35768;&#22810;IRL&#31639;&#27861;&#38656;&#35201;&#24050;&#30693;&#30340;&#36716;&#31227;&#27169;&#22411;&#65292;&#26377;&#26102;&#29978;&#33267;&#38656;&#35201;&#24050;&#30693;&#30340;&#19987;&#23478;&#31574;&#30053;&#65292;&#25110;&#32773;&#33267;&#23569;&#38656;&#35201;&#35775;&#38382;&#29983;&#25104;&#27169;&#22411;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#20551;&#35774;&#23545;&#20110;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#26469;&#35828;&#22826;&#24378;&#20102;&#65292;&#22240;&#20026;&#21482;&#33021;&#36890;&#36807;&#39034;&#24207;&#20132;&#20114;&#26469;&#35775;&#38382;&#29615;&#22659;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;IRL&#31639;&#27861;&#65306;&#20027;&#21160;&#25506;&#32034;&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;AceIRL&#65289;&#65292;&#23427;&#20027;&#21160;&#25506;&#32034;&#26410;&#30693;&#29615;&#22659;&#21644;&#19987;&#23478;&#31574;&#30053;&#65292;&#24555;&#36895;&#23398;&#20064;&#19987;&#23478;&#30340;&#22870;&#21169;&#20989;&#25968;&#24182;&#35782;&#21035;&#20986;&#19968;&#20010;&#22909;&#30340;&#31574;&#30053;&#12290;AceIRL&#20351;&#29992;&#20808;&#21069;&#30340;&#35266;&#23519;&#32467;&#26524;&#26500;&#24314;&#32622;&#20449;&#21306;&#38388;&#26469;&#25429;&#25417;&#21487;&#34892;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#25214;&#21040;&#20391;&#37325;&#20110;&#29615;&#22659;&#20013;&#26368;&#26377;&#20449;&#24687;&#30340;&#21306;&#22495;&#30340;&#25506;&#32034;&#31574;&#30053;&#12290;AceIRL&#26159;&#31532;&#19968;&#31181;&#20855;&#26377;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#19988;&#19981;&#38656;&#35201;&#29615;&#22659;&#29983;&#25104;&#27169;&#22411;&#30340;&#20027;&#21160;IRL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse Reinforcement Learning (IRL) is a powerful paradigm for inferring a reward function from expert demonstrations. Many IRL algorithms require a known transition model and sometimes even a known expert policy, or they at least require access to a generative model. However, these assumptions are too strong for many real-world applications, where the environment can be accessed only through sequential interaction. We propose a novel IRL algorithm: Active exploration for Inverse Reinforcement Learning (AceIRL), which actively explores an unknown environment and expert policy to quickly learn the expert's reward function and identify a good policy. AceIRL uses previous observations to construct confidence intervals that capture plausible reward functions and find exploration policies that focus on the most informative regions of the environment. AceIRL is the first approach to active IRL with sample-complexity bounds that does not require a generative model of the environment. AceIRL 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FRAug&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#20855;&#26377;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#29305;&#24449;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#29983;&#25104;&#21512;&#25104;&#30340;&#23458;&#25143;&#31471;&#29305;&#23450;&#26679;&#26412;&#26469;&#22686;&#24378;&#23458;&#25143;&#31471;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#20849;&#20139;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23558;&#26469;&#33258;&#19981;&#21516;&#29305;&#24449;&#20998;&#24067;&#30340;&#23458;&#25143;&#31471;&#30340;&#30693;&#35782;&#34701;&#21512;&#36215;&#26469;&#12290;</title><link>http://arxiv.org/abs/2205.14900</link><description>&lt;p&gt;
FRAug: &#36890;&#36807;&#34920;&#31034;&#22686;&#24378;&#35299;&#20915;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#29305;&#24449;&#30340;&#32852;&#37030;&#23398;&#20064;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
FRAug: Tackling Federated Learning with Non-IID Features via Representation Augmentation. (arXiv:2205.14900v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FRAug&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#20855;&#26377;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#29305;&#24449;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#29983;&#25104;&#21512;&#25104;&#30340;&#23458;&#25143;&#31471;&#29305;&#23450;&#26679;&#26412;&#26469;&#22686;&#24378;&#23458;&#25143;&#31471;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#20849;&#20139;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23558;&#26469;&#33258;&#19981;&#21516;&#29305;&#24449;&#20998;&#24067;&#30340;&#23458;&#25143;&#31471;&#30340;&#30693;&#35782;&#34701;&#21512;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#22312;&#20854;&#20013;&#22810;&#20010;&#23458;&#25143;&#31471;&#21327;&#21516;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#19981;&#38598;&#20013;&#20854;&#26412;&#22320;&#25968;&#25454;&#65292;&#20174;&#32780;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#23454;&#38469;&#24212;&#29992;&#36890;&#24120;&#28041;&#21450;&#19981;&#21516;&#23458;&#25143;&#31471;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#36825;&#20250;&#25439;&#23475;&#23458;&#25143;&#31471;&#23545;&#26469;&#33258;&#21508;&#33258;&#25968;&#25454;&#20998;&#24067;&#30340;&#26410;&#35265;&#26679;&#26412;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#29305;&#24449;&#20559;&#31227;&#38382;&#39064;&#65292;&#20854;&#20013;&#23458;&#25143;&#31471;&#20855;&#26377;&#19981;&#21516;&#30340;&#29305;&#24449;&#20998;&#24067;&#65292;&#32780;&#26631;&#31614;&#20998;&#24067;&#30456;&#21516;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Federated Representation Augmentation (FRAug)&#26469;&#35299;&#20915;&#36825;&#20010;&#23454;&#38469;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#29983;&#25104;&#21512;&#25104;&#30340;&#23458;&#25143;&#31471;&#29305;&#23450;&#26679;&#26412;&#65292;&#20197;&#22686;&#24378;&#36890;&#24120;&#36739;&#23567;&#30340;&#23458;&#25143;&#31471;&#25968;&#25454;&#38598;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#20849;&#20139;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#26469;&#34701;&#21512;&#23458;&#25143;&#31471;&#20174;&#19981;&#21516;&#29305;&#24449;&#20998;&#24067;&#20013;&#23398;&#21040;&#30340;&#30693;&#35782;&#12290;&#36825;&#20010;&#29983;&#25104;&#22120;&#21512;&#25104;&#23458;&#25143;&#26080;&#20851;&#30340;&#23884;&#20837;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a decentralized learning paradigm, in which multiple clients collaboratively train deep learning models without centralizing their local data, and hence preserve data privacy. Real-world applications usually involve a distribution shift across the datasets of the different clients, which hurts the generalization ability of the clients to unseen samples from their respective data distributions. In this work, we address the recently proposed feature shift problem where the clients have different feature distributions, while the label distribution is the same. We propose Federated Representation Augmentation (FRAug) to tackle this practical and challenging problem. Our approach generates synthetic client-specific samples in the embedding space to augment the usually small client datasets. For that, we train a shared generative model to fuse the clients knowledge learned from their different feature distributions. This generator synthesizes client-agnostic embedd
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20174;&#20449;&#24687;&#35770;&#30340;&#35270;&#35282;&#37325;&#26032;&#24605;&#32771;&#20102;&#29616;&#26377;&#30340;&#22810;&#35270;&#35282;&#23398;&#20064;&#33539;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#24191;&#20041;&#30340;&#22810;&#35270;&#35282;&#23398;&#20064;&#12290;&#22312;&#27492;&#26694;&#26550;&#30340;&#25351;&#23548;&#19979;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#19977;&#23618;&#36882;&#36827;&#30340;&#22810;&#35270;&#35282;&#32534;&#30721;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2109.02344</link><description>&lt;p&gt;
&#20449;&#24687;&#35770;&#24341;&#23548;&#30340;&#21551;&#21457;&#24335;&#28176;&#36827;&#24335;&#22810;&#35270;&#35282;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Information Theory-Guided Heuristic Progressive Multi-View Coding. (arXiv:2109.02344v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.02344
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20174;&#20449;&#24687;&#35770;&#30340;&#35270;&#35282;&#37325;&#26032;&#24605;&#32771;&#20102;&#29616;&#26377;&#30340;&#22810;&#35270;&#35282;&#23398;&#20064;&#33539;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#24191;&#20041;&#30340;&#22810;&#35270;&#35282;&#23398;&#20064;&#12290;&#22312;&#27492;&#26694;&#26550;&#30340;&#25351;&#23548;&#19979;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#19977;&#23618;&#36882;&#36827;&#30340;&#22810;&#35270;&#35282;&#32534;&#30721;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#35282;&#34920;&#31034;&#23398;&#20064;&#20174;&#20849;&#20139;&#19978;&#19979;&#25991;&#30340;&#22810;&#20010;&#35270;&#35282;&#20013;&#25429;&#25417;&#32508;&#21512;&#20449;&#24687;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#30452;&#35266;&#22320;&#23558;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#24212;&#29992;&#20110;&#34920;&#31034;&#23398;&#20064;&#65292;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#25104;&#23545;&#26041;&#24335;&#65292;&#20294;&#20173;&#28982;&#21487;&#25193;&#23637;&#65306;&#22312;&#23398;&#20064;&#35270;&#35282;&#20849;&#20139;&#34920;&#31034;&#26102;&#65292;&#19981;&#20250;&#36807;&#28388;&#29305;&#23450;&#20110;&#35270;&#35282;&#30340;&#22122;&#22768;&#65307;&#34394;&#20551;&#30340;&#36127;&#23545;&#65292;&#20854;&#20013;&#36127;&#39033;&#23454;&#38469;&#19978;&#19982;&#27491;&#39033;&#23646;&#20110;&#21516;&#19968;&#31867;&#65292;&#20197;&#21450;&#30495;&#23454;&#30340;&#36127;&#23545;&#34987;&#31561;&#21516;&#23545;&#24453;&#65307;&#22343;&#21248;&#22320;&#27979;&#37327;&#26415;&#35821;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#21487;&#33021;&#24178;&#25200;&#20248;&#21270;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#38024;&#23545;&#24191;&#20041;&#33258;&#30417;&#30563;&#22810;&#35270;&#35282;&#23398;&#20064;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#23588;&#20854;&#26159;&#38024;&#23545;&#36229;&#36807;&#20004;&#20010;&#35270;&#35282;&#30340;&#24773;&#20917;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20174;&#20449;&#24687;&#29702;&#35770;&#30340;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#29616;&#26377;&#30340;&#22810;&#35270;&#35282;&#23398;&#20064;&#33539;&#24335;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20449;&#24687;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#24191;&#20041;&#22810;&#35270;&#35282;&#23398;&#20064;&#12290;&#22312;&#20854;&#25351;&#23548;&#19979;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#31181;&#20855;&#26377;&#19977;&#23618;&#36882;&#36827;&#30340;&#22810;&#35270;&#35282;&#32534;&#30721;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-view representation learning captures comprehensive information from multiple views of a shared context. Recent works intuitively apply contrastive learning (CL) to learn representations, regarded as a pairwise manner, which is still scalable: view-specific noise is not filtered in learning view-shared representations; the fake negative pairs, where the negative terms are actually within the same class as the positive, and the real negative pairs are coequally treated; and evenly measuring the similarities between terms might interfere with optimization. Importantly, few works research the theoretical framework of generalized self-supervised multi-view learning, especially for more than two views. To this end, we rethink the existing multi-view learning paradigm from the information theoretical perspective and then propose a novel information theoretical framework for generalized multi-view learning. Guided by it, we build a multi-view coding method with a three-tier progressive 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25512;&#24191;&#20102;&#31038;&#20250;&#20559;&#35265;&#30340;&#27010;&#24565;&#65292;&#20174;&#35821;&#35328;&#23884;&#20837;&#25193;&#23637;&#21040;&#20102;&#22522;&#20110;&#22270;&#20687;&#21644;&#35821;&#35328;&#30340;&#23884;&#20837;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#22270;&#20687;&#21644;&#35821;&#35328;&#30340;&#23884;&#20837;&#20013;&#30340;&#20559;&#35265;&#19982;&#26410;&#32463;&#22521;&#35757;&#30340;&#23884;&#20837;&#20013;&#30340;&#20559;&#35265;&#21516;&#31561;&#37325;&#35201;&#29978;&#33267;&#26356;&#37325;&#35201;&#12290;&#24182;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#26469;&#30740;&#31350;&#20559;&#35265;&#12289;&#35821;&#35328;&#21644;&#35270;&#35273;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2002.08911</link><description>&lt;p&gt;
&#22312;&#22522;&#20110;&#22270;&#20687;&#21644;&#35821;&#35328;&#23884;&#20837;&#20013;&#27979;&#37327;&#31038;&#20250;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Measuring Social Biases in Grounded Vision and Language Embeddings. (arXiv:2002.08911v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2002.08911
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25512;&#24191;&#20102;&#31038;&#20250;&#20559;&#35265;&#30340;&#27010;&#24565;&#65292;&#20174;&#35821;&#35328;&#23884;&#20837;&#25193;&#23637;&#21040;&#20102;&#22522;&#20110;&#22270;&#20687;&#21644;&#35821;&#35328;&#30340;&#23884;&#20837;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#22270;&#20687;&#21644;&#35821;&#35328;&#30340;&#23884;&#20837;&#20013;&#30340;&#20559;&#35265;&#19982;&#26410;&#32463;&#22521;&#35757;&#30340;&#23884;&#20837;&#20013;&#30340;&#20559;&#35265;&#21516;&#31561;&#37325;&#35201;&#29978;&#33267;&#26356;&#37325;&#35201;&#12290;&#24182;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#26469;&#30740;&#31350;&#20559;&#35265;&#12289;&#35821;&#35328;&#21644;&#35270;&#35273;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#31038;&#20250;&#20559;&#35265;&#30340;&#27010;&#24565;&#20174;&#35821;&#35328;&#23884;&#20837;&#25512;&#24191;&#21040;&#20102;&#22522;&#20110;&#22270;&#20687;&#21644;&#35821;&#35328;&#30340;&#23884;&#20837;&#20013;&#12290;&#23384;&#22312;&#20110;&#22522;&#20110;&#22270;&#20687;&#21644;&#35821;&#35328;&#23884;&#20837;&#20013;&#30340;&#20559;&#35265;&#20284;&#20046;&#19982;&#26410;&#32463;&#22521;&#35757;&#30340;&#23884;&#20837;&#20013;&#30340;&#20559;&#35265;&#21516;&#31561;&#29978;&#33267;&#26356;&#20026;&#37325;&#35201;&#12290;&#23613;&#31649;&#35270;&#35273;&#21644;&#35821;&#35328;&#21487;&#33021;&#21463;&#21040;&#19981;&#21516;&#30340;&#20559;&#35265;&#65292;&#20154;&#20204;&#21487;&#33021;&#24076;&#26395;&#36825;&#20123;&#20559;&#35265;&#21487;&#20197;&#30456;&#20114;&#34928;&#20943;&#65292;&#20294;&#23454;&#38469;&#24773;&#20917;&#24182;&#38750;&#22914;&#27492;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#31181;&#27867;&#21270;&#24230;&#37327;&#23884;&#20837;&#20013;&#30340;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#27867;&#21270;&#31354;&#38388;&#65288;Grounded-WEAT&#21644;Grounded-SEAT&#65289;&#65292;&#24182;&#23637;&#31034;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#27867;&#21270;&#26041;&#27861;&#23545;&#20110;&#20559;&#35265;&#12289;&#35821;&#35328;&#21644;&#35270;&#35273;&#20132;&#20114;&#20316;&#29992;&#30340;&#37325;&#35201;&#38382;&#39064;&#20855;&#26377;&#19981;&#21516;&#30340;&#22238;&#31572;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#24230;&#37327;&#26041;&#27861;&#22312;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#22522;&#20110;&#22270;&#20687;&#30340;&#20559;&#35265;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#22312;COCO&#12289;&#27010;&#24565;&#23383;&#24149;&#21644;&#35895;&#27468;&#22270;&#20687;&#31561;&#26631;&#20934;&#35821;&#35328;&#20559;&#35265;&#22522;&#20934;&#19978;&#22686;&#21152;10,228&#24352;&#22270;&#20687;&#26469;&#26500;&#24314;&#12290;&#25968;&#25454;&#38598;&#30340;&#26500;&#24314;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#35270;&#35273;&#25968;&#25454;&#38598;&#26412;&#36523;&#23601;&#23384;&#22312;&#24456;&#22823;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
We generalize the notion of social biases from language embeddings to grounded vision and language embeddings. Biases are present in grounded embeddings, and indeed seem to be equally or more significant than for ungrounded embeddings. This is despite the fact that vision and language can suffer from different biases, which one might hope could attenuate the biases in both. Multiple ways exist to generalize metrics measuring bias in word embeddings to this new setting. We introduce the space of generalizations (Grounded-WEAT and Grounded-SEAT) and demonstrate that three generalizations answer different yet important questions about how biases, language, and vision interact. These metrics are used on a new dataset, the first for grounded bias, created by augmenting extending standard linguistic bias benchmarks with 10,228 images from COCO, Conceptual Captions, and Google Images. Dataset construction is challenging because vision datasets are themselves very biased. The presence of these
&lt;/p&gt;</description></item></channel></rss>