<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102; MACHIAVELLI &#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#34913;&#37327;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26159;&#21542;&#34920;&#29616;&#20986;&#39532;&#22522;&#38597;&#32500;&#21033;&#34892;&#20026;&#65292;&#21457;&#29616;&#20102;&#26368;&#22823;&#21270;&#22870;&#21169;&#21644;&#34892;&#20026;&#30340;&#36947;&#24503;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#24182;&#25506;&#32034;&#20102;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#31181;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2304.03279</link><description>&lt;p&gt;
&#22870;&#21169;&#26159;&#21542;&#21512;&#29702;&#65311;&#22312; MACHIAVELLI &#22522;&#20934;&#27979;&#35797;&#20013;&#34913;&#37327;&#22870;&#21169;&#19982;&#36947;&#24503;&#34892;&#20026;&#20043;&#38388;&#30340;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark. (arXiv:2304.03279v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03279
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; MACHIAVELLI &#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#34913;&#37327;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26159;&#21542;&#34920;&#29616;&#20986;&#39532;&#22522;&#38597;&#32500;&#21033;&#34892;&#20026;&#65292;&#21457;&#29616;&#20102;&#26368;&#22823;&#21270;&#22870;&#21169;&#21644;&#34892;&#20026;&#30340;&#36947;&#24503;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#24182;&#25506;&#32034;&#20102;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#31181;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#34987;&#35757;&#32451;&#25104;&#26368;&#22823;&#21270;&#22870;&#21169;&#65292;&#36825;&#21487;&#33021;&#20250;&#28608;&#21169;&#36861;&#27714;&#26435;&#21147;&#21644;&#27450;&#39575;&#34892;&#20026;&#65292;&#31867;&#20284;&#20110;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#21487;&#33021;&#20250;&#28608;&#21169;&#26377;&#23475;&#34892;&#20026;&#12290;&#37027;&#20040;&#20195;&#29702;&#26159;&#21542;&#33258;&#28982;&#32780;&#28982;&#22320;&#23398;&#20250;&#20102;&#39532;&#22522;&#38597;&#32500;&#21033;&#34892;&#20026;&#65311;&#25105;&#20204;&#22914;&#20309;&#22312; GPT-4 &#31561;&#36890;&#29992;&#27169;&#22411;&#20013;&#34913;&#37327;&#36825;&#20123;&#34892;&#20026;&#21602;&#65311;&#20026;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; MACHIAVELLI &#22522;&#20934;&#27979;&#35797;&#65292;&#35813;&#27979;&#35797;&#28085;&#30422;&#20102;&#36229;&#36807;&#19968;&#30334;&#19975;&#20010;&#22810;&#26679;&#21270;&#30340;&#24773;&#26223;&#65292;&#37325;&#28857;&#20851;&#27880;&#31038;&#20250;&#20915;&#31574;&#21046;&#23450;&#65292;&#29992;&#20110;&#34913;&#37327;&#20154;&#24037;&#20195;&#29702;&#26159;&#21542;&#34920;&#29616;&#20986;&#39532;&#22522;&#38597;&#32500;&#21033;&#34892;&#20026;&#12290;&#25105;&#20204;&#25968;&#23398;&#21270;&#20102;&#25968;&#21313;&#31181;&#26377;&#23475;&#34892;&#20026;&#65292;&#24182;&#20351;&#29992;&#25105;&#20204;&#30340;&#27880;&#37322;&#26469;&#35780;&#20272;&#20195;&#29702;&#20542;&#21521;&#20110;&#36861;&#27714;&#26435;&#21147;&#65292;&#36896;&#25104;&#21151;&#33021;&#19981;&#33391;&#21644;&#36829;&#21453;&#20262;&#29702;&#30340;&#20542;&#21521;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#26368;&#22823;&#21270;&#22870;&#21169;&#21644;&#34892;&#20026;&#30340;&#36947;&#24503;&#24615;&#20043;&#38388;&#23384;&#22312;&#19968;&#20123;&#32039;&#24352;&#20851;&#31995;&#12290;&#20026;&#20102;&#25913;&#21892;&#36825;&#31181;&#26435;&#34913;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#20351;&#20195;&#29702;&#36235;&#21521;&#20110;&#37319;&#21462;&#26356;&#23569;&#30340;&#26377;&#23475;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;MACHIAVELLI &#26159;&#35780;&#20272;&#20154;&#24037;&#20195;&#29702;&#39532;&#22522;&#38597;&#32500;&#21033;&#34892;&#20026;&#27700;&#24179;&#30340;&#26377;&#29992;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial agents have traditionally been trained to maximize reward, which may incentivize power-seeking and deception, analogous to how next-token prediction in language models (LMs) may incentivize toxicity. So do agents naturally learn to be Machiavellian? And how do we measure these behaviors in general-purpose models such as GPT-4? Towards answering these questions, we introduce MACHIAVELLI, a benchmark of 134 Choose-Your-Own-Adventure games containing over half a million rich, diverse scenarios that center on social decision-making. Scenario labeling is automated with LMs, which are more performant than human annotators. We mathematize dozens of harmful behaviors and use our annotations to evaluate agents' tendencies to be power-seeking, cause disutility, and commit ethical violations. We observe some tension between maximizing reward and behaving ethically. To improve this trade-off, we investigate LM-based methods to steer agents' towards less harmful behaviors. Our results sh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;GPT-4&#29983;&#25104;&#25351;&#20196;&#36981;&#24490;&#25968;&#25454;&#36827;&#34892;LLM&#24494;&#35843;&#65292;&#23454;&#39564;&#34920;&#26126;GPT-4&#25152;&#29983;&#25104;&#30340;&#25351;&#20196;&#25968;&#25454;&#20248;&#20110;&#20197;&#24448;&#26368;&#20808;&#36827;&#27169;&#22411;&#29983;&#25104;&#30340;&#25968;&#25454;&#65292;&#22312;&#26032;&#20219;&#21153;&#20013;&#34920;&#29616;&#21331;&#36234;&#12290;</title><link>http://arxiv.org/abs/2304.03277</link><description>&lt;p&gt;
GPT-4&#25351;&#20196;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Instruction Tuning with GPT-4. (arXiv:2304.03277v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03277
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;GPT-4&#29983;&#25104;&#25351;&#20196;&#36981;&#24490;&#25968;&#25454;&#36827;&#34892;LLM&#24494;&#35843;&#65292;&#23454;&#39564;&#34920;&#26126;GPT-4&#25152;&#29983;&#25104;&#30340;&#25351;&#20196;&#25968;&#25454;&#20248;&#20110;&#20197;&#24448;&#26368;&#20808;&#36827;&#27169;&#22411;&#29983;&#25104;&#30340;&#25968;&#25454;&#65292;&#22312;&#26032;&#20219;&#21153;&#20013;&#34920;&#29616;&#21331;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#20351;&#29992;&#26426;&#22120;&#29983;&#25104;&#30340;&#25351;&#20196;&#36981;&#24490;&#25968;&#25454;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#20351;&#36825;&#20123;&#27169;&#22411;&#22312;&#26032;&#20219;&#21153;&#19978;&#23454;&#29616;&#26174;&#33879;&#30340;&#38646;-shot&#33021;&#21147;&#65292;&#19981;&#38656;&#35201;&#20154;&#31867;&#32534;&#20889;&#30340;&#25351;&#20196;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;GPT-4&#29983;&#25104;&#25351;&#20196;&#36981;&#24490;&#25968;&#25454;&#36827;&#34892;LLM&#24494;&#35843;&#12290;&#25105;&#20204;&#22312;&#25351;&#20196;&#35843;&#20248;&#30340;LLaMA&#27169;&#22411;&#19978;&#36827;&#34892;&#30340;&#26089;&#26399;&#23454;&#39564;&#34920;&#26126;&#65292;GPT-4&#29983;&#25104;&#30340;52K&#33521;&#35821;&#21644;&#20013;&#25991;&#25351;&#20196;&#36981;&#24490;&#25968;&#25454;&#20248;&#20110;&#20197;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#29983;&#25104;&#30340;&#25351;&#20196;&#36981;&#24490;&#25968;&#25454;&#65292;&#21487;&#20197;&#22312;&#26032;&#20219;&#21153;&#20013;&#23454;&#29616;&#21331;&#36234;&#30340;&#38646;-shot&#34920;&#29616;&#12290;&#25105;&#20204;&#36824;&#25910;&#38598;&#20102;&#26469;&#33258;GPT-4&#30340;&#21453;&#39304;&#21644;&#27604;&#36739;&#25968;&#25454;&#65292;&#20197;&#23454;&#29616;&#20840;&#38754;&#30340;&#35780;&#20272;&#21644;&#22870;&#21169;&#27169;&#22411;&#35757;&#32451;&#12290;&#25105;&#20204;&#20844;&#24320;&#25552;&#20379;&#20102;&#20351;&#29992;GPT-4&#29983;&#25104;&#30340;&#25968;&#25454;&#20197;&#21450;&#25105;&#20204;&#30340;&#20195;&#30721;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior work has shown that finetuning large language models (LLMs) using machine-generated instruction-following data enables such models to achieve remarkable zero-shot capabilities on new tasks, and no human-written instructions are needed. In this paper, we present the first attempt to use GPT-4 to generate instruction-following data for LLM finetuning. Our early experiments on instruction-tuned LLaMA models show that the 52K English and Chinese instruction-following data generated by GPT-4 leads to superior zero-shot performance on new tasks to the instruction-following data generated by previous state-of-the-art models. We also collect feedback and comparison data from GPT-4 to enable a comprehensive evaluation and reward model training. We make our data generated using GPT-4 as well as our codebase publicly available.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DiffMimic&#65292;&#19968;&#31181;&#22522;&#20110;&#21487;&#24494;&#20998;&#29289;&#29702;&#30340;&#39640;&#25928;&#36816;&#21160;&#27169;&#20223;&#26041;&#27861;&#12290;&#19982;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#20854;&#26377;&#26356;&#24555;&#26356;&#31283;&#23450;&#30340;&#25910;&#25947;&#36895;&#24230;&#65307;&#21516;&#26102;&#36890;&#36807;&#28436;&#31034;&#37325;&#25773;&#26426;&#21046;&#36991;&#20813;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2304.03274</link><description>&lt;p&gt;
DiffMimic: &#22522;&#20110;&#21487;&#24494;&#20998;&#29289;&#29702;&#30340;&#39640;&#25928;&#36816;&#21160;&#27169;&#20223;
&lt;/p&gt;
&lt;p&gt;
DiffMimic: Efficient Motion Mimicking with Differentiable Physics. (arXiv:2304.03274v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03274
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DiffMimic&#65292;&#19968;&#31181;&#22522;&#20110;&#21487;&#24494;&#20998;&#29289;&#29702;&#30340;&#39640;&#25928;&#36816;&#21160;&#27169;&#20223;&#26041;&#27861;&#12290;&#19982;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#20854;&#26377;&#26356;&#24555;&#26356;&#31283;&#23450;&#30340;&#25910;&#25947;&#36895;&#24230;&#65307;&#21516;&#26102;&#36890;&#36807;&#28436;&#31034;&#37325;&#25773;&#26426;&#21046;&#36991;&#20813;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#27169;&#20223;&#26159;&#22522;&#20110;&#29289;&#29702;&#30340;&#35282;&#33394;&#21160;&#30011;&#20013;&#30340;&#22522;&#30784;&#20219;&#21153;&#65292;&#28982;&#32780;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#36816;&#21160;&#27169;&#20223;&#26041;&#27861;&#37117;&#24314;&#31435;&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20043;&#19978;&#65292;&#23384;&#22312;&#37325;&#24230;&#22870;&#21169;&#24037;&#31243;&#12289;&#39640;&#26041;&#24046;&#21644;&#38590;&#20197;&#25506;&#32034;&#30340;&#25910;&#25947;&#36895;&#24230;&#32531;&#24930;&#31561;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#24494;&#20998;&#29289;&#29702;&#27169;&#25311;&#22120;&#65288;DPS&#65289;&#30340;&#36816;&#21160;&#27169;&#20223;&#26041;&#27861;&#65292;&#21517;&#20026;DiffMimic&#65292;&#36890;&#36807;&#20998;&#26512;&#26799;&#24230;&#21644;&#22522;&#20110;&#30495;&#23454;&#29289;&#29702;&#20808;&#39564;&#23398;&#20064;&#31283;&#23450;&#31574;&#30053;&#65292;&#20174;&#32780;&#23454;&#29616;&#26174;&#33879;&#26356;&#24555;&#21644;&#26356;&#31283;&#23450;&#30340;&#25910;&#25947;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#36991;&#20813;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#35299;&#65292;&#25105;&#20204;&#36824;&#21033;&#29992;&#28436;&#31034;&#37325;&#25773;&#26426;&#21046;&#65292;&#22312;&#38271;&#26102;&#38388;&#36328;&#24230;&#20869;&#23454;&#29616;&#31283;&#23450;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motion mimicking is a foundational task in physics-based character animation. However, most existing motion mimicking methods are built upon reinforcement learning (RL) and suffer from heavy reward engineering, high variance, and slow convergence with hard explorations. Specifically, they usually take tens of hours or even days of training to mimic a simple motion sequence, resulting in poor scalability. In this work, we leverage differentiable physics simulators (DPS) and propose an efficient motion mimicking method dubbed DiffMimic. Our key insight is that DPS casts a complex policy learning task to a much simpler state matching problem. In particular, DPS learns a stable policy by analytical gradients with ground-truth physical priors hence leading to significantly faster and stabler convergence than RL-based methods. Moreover, to escape from local optima, we utilize a Demonstration Replay mechanism to enable stable gradient backpropagation in a long horizon. Extensive experiments o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25581;&#31034;&#20197;&#21450;&#25552;&#20986;&#20102;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24040;&#22823;&#27700;&#36275;&#36857;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#20854;&#28129;&#27700;&#28040;&#32791;&#24050;&#32463;&#24341;&#36215;&#22269;&#38469;&#31038;&#20250;&#30340;&#37325;&#35270;&#65292;&#24182;&#19988;AI&#27169;&#22411;&#24212;&#35813;&#25215;&#25285;&#31038;&#20250;&#36131;&#20219;&#65292;&#20570;&#20986;&#38754;&#23545;&#27700;&#21361;&#26426;&#30340;&#34920;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.03271</link><description>&lt;p&gt;
&#20351;AI&#8220;&#21475;&#28212;&#8221;&#20943;&#23569;&#30340;&#26041;&#27861;&#65306;&#25581;&#31034;&#21644;&#35299;&#20915;AI&#27169;&#22411;&#30340;&#31192;&#23494;&#27700;&#28040;&#32791;
&lt;/p&gt;
&lt;p&gt;
Making AI Less "Thirsty": Uncovering and Addressing the Secret Water Footprint of AI Models. (arXiv:2304.03271v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03271
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25581;&#31034;&#20197;&#21450;&#25552;&#20986;&#20102;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24040;&#22823;&#27700;&#36275;&#36857;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#20854;&#28129;&#27700;&#28040;&#32791;&#24050;&#32463;&#24341;&#36215;&#22269;&#38469;&#31038;&#20250;&#30340;&#37325;&#35270;&#65292;&#24182;&#19988;AI&#27169;&#22411;&#24212;&#35813;&#25215;&#25285;&#31038;&#20250;&#36131;&#20219;&#65292;&#20570;&#20986;&#38754;&#23545;&#27700;&#21361;&#26426;&#30340;&#34920;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#30340;&#30899;&#36275;&#36857;&#19981;&#26029;&#22686;&#38271;&#65292;&#29305;&#21035;&#26159;&#20687;GPT-3&#21644;GPT-4&#36825;&#26679;&#30340;&#22823;&#22411;&#27169;&#22411;&#65292;&#24050;&#32463;&#21463;&#21040;&#20844;&#20247;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#21516;&#31561;&#37325;&#35201;&#19988;&#24040;&#22823;&#30340;AI&#27169;&#22411;&#27700;&#21360;&#23578;&#26410;&#24341;&#36215;&#20154;&#20204;&#30340;&#27880;&#24847;&#12290;&#20363;&#22914;&#65292;&#22312;&#24494;&#36719;&#26368;&#20808;&#36827;&#30340;&#32654;&#22269;&#25968;&#25454;&#20013;&#24515;&#20013;&#35757;&#32451;GPT-3&#21487;&#20197;&#30452;&#25509;&#28040;&#32791;70&#19975;&#21319;&#28165;&#27905;&#28129;&#27700;&#65288;&#30456;&#24403;&#20110;&#29983;&#20135;370&#36742;&#23453;&#39532;&#27773;&#36710;&#25110;320&#36742;&#29305;&#26031;&#25289;&#30005;&#21160;&#27773;&#36710;&#65289;&#65292;&#22914;&#26524;&#22312;&#24494;&#36719;&#30340;&#20122;&#27954;&#25968;&#25454;&#20013;&#24515;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#20010;&#27700;&#28040;&#32791;&#37327;&#23558;&#22686;&#21152;&#19977;&#20493;&#65292;&#20294;&#36825;&#26679;&#30340;&#20449;&#24687;&#19968;&#30452;&#34987;&#20445;&#23494;&#12290;&#36825;&#26497;&#20854;&#20196;&#20154;&#25285;&#24551;&#65292;&#22240;&#20026;&#28129;&#27700;&#30701;&#32570;&#24050;&#25104;&#20026;&#22312;&#20154;&#21475;&#36805;&#36895;&#22686;&#38271;&#12289;&#27700;&#36164;&#28304;&#20943;&#23569;&#21644;&#32769;&#21270;&#30340;&#27700;&#22522;&#30784;&#35774;&#26045;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25152;&#26377;&#20154;&#38754;&#20020;&#30340;&#26368;&#32039;&#36843;&#30340;&#25361;&#25112;&#20043;&#19968;&#12290;&#20026;&#20102;&#24212;&#23545;&#20840;&#29699;&#27700;&#36164;&#28304;&#30340;&#25361;&#25112;&#65292;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#21487;&#20197;&#65292;&#32780;&#19988;&#24212;&#35813;&#65292;&#25215;&#25285;&#31038;&#20250;&#36131;&#20219;&#65292;&#20197;&#36523;&#20316;&#21017;&#35299;&#20915;&#33258;&#24049;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing carbon footprint of artificial intelligence (AI) models, especially large ones such as GPT-3 and GPT-4, has been undergoing public scrutiny. Unfortunately, however, the equally important and enormous water footprint of AI models has remained under the radar. For example, training GPT-3 in Microsoft's state-of-the-art U.S. data centers can directly consume 700,000 liters of clean freshwater (enough for producing 370 BMW cars or 320 Tesla electric vehicles) and the water consumption would have been tripled if training were done in Microsoft's Asian data centers, but such information has been kept as a secret. This is extremely concerning, as freshwater scarcity has become one of the most pressing challenges shared by all of us in the wake of the rapidly growing population, depleting water resources, and aging water infrastructures. To respond to the global water challenges, AI models can, and also should, take social responsibility and lead by example by addressing their own 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;FedBot&#12290;&#23427;&#32467;&#21512;&#20102;Deep Bidirectional Transformer&#27169;&#22411;&#21644;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#32852;&#21512;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25252;&#23458;&#25143;&#25968;&#25454;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2304.03228</link><description>&lt;p&gt;
FedBot&#65306;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#22686;&#24378;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#38544;&#31169;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
FedBot: Enhancing Privacy in Chatbots with Federated Learning. (arXiv:2304.03228v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;FedBot&#12290;&#23427;&#32467;&#21512;&#20102;Deep Bidirectional Transformer&#27169;&#22411;&#21644;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#32852;&#21512;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25252;&#23458;&#25143;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32842;&#22825;&#26426;&#22120;&#20154;&#20027;&#35201;&#20381;&#36182;&#20110;&#21253;&#21547;&#25935;&#24863;&#20449;&#24687;&#30340;&#35805;&#35821;&#30340;&#25968;&#25454;&#25512;&#21160;&#65292;&#20294;&#26159;&#22312;&#20849;&#20139;&#25968;&#25454;&#19978;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#33021;&#20250;&#20405;&#29359;&#29992;&#25143;&#38544;&#31169;&#12290;&#26412;&#25991;&#25552;&#20986;FedBot&#65292;&#19968;&#20010;&#21033;&#29992;&#22823;&#35268;&#27169;&#23458;&#25143;&#25903;&#25345;&#25968;&#25454;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#27010;&#24565;&#39564;&#35777;&#65292;&#23427;&#32467;&#21512;&#20102;Deep Bidirectional Transformer&#27169;&#22411;&#21644;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#32852;&#21512;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25252;&#23458;&#25143;&#25968;&#25454;&#38544;&#31169;&#12290;&#27010;&#24565;&#39564;&#35777;&#30340;&#32467;&#26524;&#23637;&#31034;&#20102;&#38544;&#31169;&#20445;&#25252;&#32842;&#22825;&#26426;&#22120;&#20154;&#33021;&#22815;&#36890;&#36807;&#25913;&#21464;&#23458;&#25143;&#25903;&#25345;&#34892;&#19994;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chatbots are mainly data-driven and usually based on utterances that might be sensitive. However, training deep learning models on shared data can violate user privacy. Such issues have commonly existed in chatbots since their inception. In the literature, there have been many approaches to deal with privacy, such as differential privacy and secure multi-party computation, but most of them need to have access to users' data. In this context, Federated Learning (FL) aims to protect data privacy through distributed learning methods that keep the data in its location. This paper presents Fedbot, a proof-of-concept (POC) privacy-preserving chatbot that leverages large-scale customer support data. The POC combines Deep Bidirectional Transformer models and federated learning algorithms to protect customer data privacy during collaborative model training. The results of the proof-of-concept showcase the potential for privacy-preserving chatbots to transform the customer support industry by de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DexDeform&#65292;&#20511;&#21161;&#20154;&#31867;&#31034;&#33539;&#21644;&#19981;&#21516;iable&#29289;&#29702;&#23398;&#20064;&#28789;&#24039;&#25805;&#32437;&#21487;&#21464;&#24418;&#29289;&#20307;&#30340;&#25216;&#33021;&#65292;&#21487;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#39640;&#25928;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.03223</link><description>&lt;p&gt;
DexDeform&#65306;&#22522;&#20110;&#20154;&#31867;&#31034;&#33539;&#21644;&#21487;&#24494;&#20998;&#29289;&#29702;&#30340;&#24039;&#22937;&#21487;&#21464;&#24418;&#29289;&#20307;&#25805;&#32437;
&lt;/p&gt;
&lt;p&gt;
DexDeform: Dexterous Deformable Object Manipulation with Human Demonstrations and Differentiable Physics. (arXiv:2304.03223v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DexDeform&#65292;&#20511;&#21161;&#20154;&#31867;&#31034;&#33539;&#21644;&#19981;&#21516;iable&#29289;&#29702;&#23398;&#20064;&#28789;&#24039;&#25805;&#32437;&#21487;&#21464;&#24418;&#29289;&#20307;&#30340;&#25216;&#33021;&#65292;&#21487;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#39640;&#25928;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#21033;&#29992;&#22810;&#25351;&#25163;&#23398;&#20064;&#21487;&#21464;&#24418;&#29289;&#20307;&#30340;&#28789;&#24039;&#25805;&#32437;&#12290;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#21487;&#21464;&#24418;&#29289;&#20307;&#20013;&#30340;&#29289;&#29702;&#30456;&#20114;&#20316;&#29992;&#22797;&#26434;&#24615;&#19979;&#21487;&#33021;&#20250;&#21463;&#21040;&#38459;&#30861;&#65292;&#24182;&#19988;&#20808;&#21069;&#22522;&#20110;&#21487;&#24494;&#20998;&#29289;&#29702;&#30340;&#36712;&#36857;&#20248;&#21270;&#26041;&#27861;&#20063;&#21487;&#33021;&#22240;&#20026;&#25163;-&#29289;&#20307;&#20132;&#20114;&#24341;&#36215;&#30340;&#25509;&#35302;&#27169;&#24335;&#22686;&#21152;&#32780;&#21463;&#21040;&#23616;&#37096;&#26497;&#23567;&#20540;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;DexDeform&#8212;&#8212;&#19968;&#20010;&#22522;&#20110;&#20154;&#31867;&#31034;&#33539;&#30340;&#24039;&#22937;&#25805;&#32437;&#25216;&#33021;&#25277;&#35937;&#24182;&#36890;&#36807;&#21487;&#24494;&#20998;&#29289;&#29702;&#36827;&#34892;&#31934;&#21270;&#30340;&#21407;&#21017;&#26694;&#26550;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25805;&#20316;&#25104;&#21151;&#29575;&#21644;&#25928;&#29575;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we aim to learn dexterous manipulation of deformable objects using multi-fingered hands. Reinforcement learning approaches for dexterous rigid object manipulation would struggle in this setting due to the complexity of physics interaction with deformable objects. At the same time, previous trajectory optimization approaches with differentiable physics for deformable manipulation would suffer from local optima caused by the explosion of contact modes from hand-object interactions. To address these challenges, we propose DexDeform, a principled framework that abstracts dexterous manipulation skills from human demonstration and refines the learned skills with differentiable physics. Concretely, we first collect a small set of human demonstrations using teleoperation. And we then train a skill model using demonstrations for planning over action abstractions in imagination. To explore the goal space, we further apply augmentations to the existing deformable shapes in demonstra
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20272;&#35745;&#36830;&#32493;&#26494;&#24347;&#20998;&#31867;&#20998;&#24067;&#30340;&#24471;&#20998;&#26469;&#26816;&#27979;&#20998;&#31867;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#24322;&#24120;&#26816;&#27979;&#34920;&#26684;&#25968;&#25454;&#38598;&#21644;&#22270;&#20687;&#25968;&#25454;&#20013;&#22343;&#34920;&#29616;&#20986;&#25345;&#32493;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.03220</link><description>&lt;p&gt;
&#36890;&#36807;Gumbel&#22122;&#22768;&#20998;&#25968;&#21305;&#37197;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Anomaly Detection via Gumbel Noise Score Matching. (arXiv:2304.03220v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03220
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20272;&#35745;&#36830;&#32493;&#26494;&#24347;&#20998;&#31867;&#20998;&#24067;&#30340;&#24471;&#20998;&#26469;&#26816;&#27979;&#20998;&#31867;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#24322;&#24120;&#26816;&#27979;&#34920;&#26684;&#25968;&#25454;&#38598;&#21644;&#22270;&#20687;&#25968;&#25454;&#20013;&#22343;&#34920;&#29616;&#20986;&#25345;&#32493;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;&#20272;&#35745;&#36830;&#32493;&#26494;&#24347;&#20998;&#31867;&#20998;&#24067;&#30340;&#24471;&#20998;&#65288;&#21363;&#19982;&#36755;&#20837;&#30456;&#20851;&#30340;&#23545;&#25968;&#20284;&#28982;&#26799;&#24230;&#65289;&#26469;&#26816;&#27979;&#20998;&#31867;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#24322;&#24120;&#26816;&#27979;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290; GNSM&#22312;&#25152;&#26377;&#23454;&#39564;&#20013;&#22343;&#34920;&#29616;&#20986;&#25345;&#32493;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20854;&#24212;&#29992;&#20110;&#22270;&#20687;&#25968;&#25454;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;GNSM&#30340;&#28789;&#27963;&#24615;&#65292;&#20854;&#20013;&#27169;&#22411;&#30340;&#20219;&#21153;&#26159;&#26816;&#27979;&#19981;&#33391;&#20998;&#21106;&#39044;&#27979;&#12290; GNSM&#25490;&#21517;&#24322;&#24120;&#30340;&#22270;&#20687;&#26174;&#31034;&#20986;&#26126;&#26174;&#30340;&#20998;&#21106;&#22833;&#36133;&#65292;GNSM&#36755;&#20986;&#30340;&#32467;&#26524;&#19982;&#22522;&#20110;&#22320;&#38754;&#30495;&#23454;&#20540;&#35745;&#31639;&#30340;&#20998;&#21106;&#24230;&#37327;&#39640;&#24230;&#30456;&#20851;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;GNSM&#20351;&#29992;&#30340;&#24471;&#20998;&#21305;&#37197;&#35757;&#32451;&#30446;&#26631;&#65292;&#24182;&#25552;&#20379;&#20102;&#25105;&#20204;&#24037;&#20316;&#30340;&#24320;&#28304;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Gumbel Noise Score Matching (GNSM), a novel unsupervised method to detect anomalies in categorical data. GNSM accomplishes this by estimating the scores, i.e. the gradients of log likelihoods w.r.t.~inputs, of continuously relaxed categorical distributions. We test our method on a suite of anomaly detection tabular datasets. GNSM achieves a consistently high performance across all experiments. We further demonstrate the flexibility of GNSM by applying it to image data where the model is tasked to detect poor segmentation predictions. Images ranked anomalous by GNSM show clear segmentation failures, with the outputs of GNSM strongly correlating with segmentation metrics computed on ground-truth. We outline the score matching training objective utilized by GNSM and provide an open-source implementation of our work.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25552;&#20986;&#21452;&#37325;&#24130;&#24459;&#26041;&#27861;&#29992;&#20110;&#39044;&#27979;&#29420;&#29305;&#30340;&#24615;&#33021;&#26435;&#34913;&#21069;&#27839;&#65292;&#24182;&#24314;&#31435;&#22522;&#20110;&#35813;&#26041;&#27861;&#30340;&#26679;&#26412;&#27604;&#20363;&#36873;&#25321;&#20248;&#21270;&#38382;&#39064;&#65292;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.03216</link><description>&lt;p&gt;
&#20851;&#20110;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;Pareto&#21069;&#27839;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Pareto Front of Multilingual Neural Machine Translation. (arXiv:2304.03216v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25552;&#20986;&#21452;&#37325;&#24130;&#24459;&#26041;&#27861;&#29992;&#20110;&#39044;&#27979;&#29420;&#29305;&#30340;&#24615;&#33021;&#26435;&#34913;&#21069;&#27839;&#65292;&#24182;&#24314;&#31435;&#22522;&#20110;&#35813;&#26041;&#27861;&#30340;&#26679;&#26412;&#27604;&#20363;&#36873;&#25321;&#20248;&#21270;&#38382;&#39064;&#65292;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#65292;&#32473;&#23450;&#26041;&#21521;&#30340;&#27867;&#21270;&#24615;&#33021;&#22914;&#20309;&#38543;&#20854;&#37319;&#26679;&#27604;&#20363;&#30340;&#21464;&#21270;&#32780;&#21464;&#21270;&#12290;&#36890;&#36807;&#35757;&#32451;200&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#27169;&#22411;&#22823;&#23567;&#12289;&#26041;&#21521;&#21644;&#24635;&#20219;&#21153;&#25968;&#37327;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#35757;&#32451;&#35821;&#26009;&#24211;&#23384;&#22312;&#25968;&#25454;&#19981;&#24179;&#34913;&#26102;&#65292;&#26631;&#37327;&#21270;&#23548;&#33268;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#26435;&#34913;&#21069;&#27839;&#65292;&#35813;&#21069;&#27839;&#20559;&#31163;&#20102;&#20256;&#32479;&#30340;Pareto&#21069;&#27839;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21452;&#37325;&#24130;&#24459;&#26469;&#39044;&#27979;MNMT&#20013;&#29420;&#29305;&#30340;&#24615;&#33021;&#26435;&#34913;&#21069;&#27839;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#35821;&#35328;&#12289;&#25968;&#25454;&#20805;&#36275;&#24615;&#21644;&#20219;&#21153;&#25968;&#37327;&#26041;&#38754;&#37117;&#24456;&#40065;&#26834;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;MNMT&#20013;&#30340;&#26679;&#26412;&#27604;&#20363;&#36873;&#25321;&#38382;&#39064;&#24314;&#27169;&#20026;&#22522;&#20110;&#21452;&#37325;&#24130;&#24459;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we study how the generalization performance of a given direction changes with its sampling ratio in Multilingual Neural Machine Translation (MNMT). By training over 200 multilingual models with various model sizes, directions, and total numbers of tasks, we find that scalarization leads to a multitask trade-off front that deviates from the traditional Pareto front when there exists data imbalance in the training corpus. That is, the performance of certain translation directions does not improve with the increase of its weight in the multi-task optimization objective, which poses greater challenge to improve the overall performance of all directions. Based on our observations, we propose the Double Power Law to predict the unique performance trade-off front in MNMT, which is robust across various languages, data adequacy and number of tasks. Finally, we formulate sample ratio selection in MNMT as an optimization problem based on the Double Power Law, which achieves better 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#36328;&#35774;&#22791;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#20998;&#23618;&#22270;&#31070;&#32463;&#32593;&#32476;(HGNN)&#65292;&#29992;&#20110;&#35299;&#20915;&#36328;&#35774;&#22791;&#29992;&#25143;&#21305;&#37197;&#38382;&#39064;&#65292;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;TGCE&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;5%&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.03215</link><description>&lt;p&gt;
&#24102;&#26377;&#36328;&#35774;&#22791;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#20998;&#23618;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#36328;&#35774;&#22791;&#29992;&#25143;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Graph Neural Network with Cross-Attention for Cross-Device User Matching. (arXiv:2304.03215v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#36328;&#35774;&#22791;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#20998;&#23618;&#22270;&#31070;&#32463;&#32593;&#32476;(HGNN)&#65292;&#29992;&#20110;&#35299;&#20915;&#36328;&#35774;&#22791;&#29992;&#25143;&#21305;&#37197;&#38382;&#39064;&#65292;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;TGCE&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;5%&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24191;&#21578;&#12289;&#25512;&#33616;&#31995;&#32479;&#21644;&#32593;&#32476;&#23433;&#20840;&#31561;&#20247;&#22810;&#39046;&#22495;&#65292;&#36328;&#35774;&#22791;&#29992;&#25143;&#21305;&#37197;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#23427;&#28041;&#21450;&#20351;&#29992;&#24207;&#21015;&#26085;&#24535;&#26469;&#35782;&#21035;&#21644;&#38142;&#25509;&#23646;&#20110;&#21516;&#19968;&#20154;&#30340;&#19981;&#21516;&#35774;&#22791;&#12290;&#20197;&#24448;&#30340;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#38590;&#20197;&#35299;&#20915;&#26085;&#24535;&#20043;&#38388;&#30340;&#38271;&#31243;&#20381;&#36182;&#21644;&#39640;&#38454;&#36830;&#25509;&#38382;&#39064;&#12290;&#26368;&#36817;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;&#36825;&#20010;&#38382;&#39064;&#24314;&#27169;&#20026;&#22270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#23618;&#22270;&#19978;&#19979;&#25991;&#23884;&#20837;(TGCE)&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#34920;&#29616;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#23618;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65288;HGNN&#65289;&#65292;&#23427;&#20855;&#26377;&#27604;TGCE&#26356;&#20026;&#35745;&#31639;&#25928;&#29575;&#30340;&#20108;&#32423;&#35774;&#35745;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#27169;&#22411;&#20013;&#24341;&#20837;&#20102;&#19968;&#31181;&#36328;&#35774;&#22791;&#20132;&#21449;&#27880;&#24847;&#21147;&#65288;Cross-Att&#65289;&#26426;&#21046;&#65292;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;TGCE&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;5%&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-device user matching is a critical problem in numerous domains, including advertising, recommender systems, and cybersecurity. It involves identifying and linking different devices belonging to the same person, utilizing sequence logs. Previous data mining techniques have struggled to address the long-range dependencies and higher-order connections between the logs. Recently, researchers have modeled this problem as a graph problem and proposed a two-tier graph contextual embedding (TGCE) neural network architecture, which outperforms previous methods. In this paper, we propose a novel hierarchical graph neural network architecture (HGNN), which has a more computationally efficient second level design than TGCE. Furthermore, we introduce a cross-attention (Cross-Att) mechanism in our model, which improves performance by 5% compared to the state-of-the-art TGCE method.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MORSE&#30340;&#22522;&#20110;&#38544;&#24335;&#35299;&#21078;&#28210;&#26579;&#30340;&#36890;&#29992;&#31070;&#32463;&#28210;&#26579;&#26694;&#26550;&#65292;&#26088;&#22312;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#24110;&#21161;&#34701;&#21512;&#39640;&#32423;&#35821;&#20041;&#30456;&#20851;&#20869;&#23481;&#21644;&#20302;&#32423;&#35299;&#21078;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2304.03209</link><description>&lt;p&gt;
&#22522;&#20110;&#38543;&#26426;&#19987;&#23478;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#38544;&#24615;&#35299;&#21078;&#28210;&#26579;
&lt;/p&gt;
&lt;p&gt;
Implicit Anatomical Rendering for Medical Image Segmentation with Stochastic Experts. (arXiv:2304.03209v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03209
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MORSE&#30340;&#22522;&#20110;&#38544;&#24335;&#35299;&#21078;&#28210;&#26579;&#30340;&#36890;&#29992;&#31070;&#32463;&#28210;&#26579;&#26694;&#26550;&#65292;&#26088;&#22312;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#24110;&#21161;&#34701;&#21512;&#39640;&#32423;&#35821;&#20041;&#30456;&#20851;&#20869;&#23481;&#21644;&#20302;&#32423;&#35299;&#21078;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#39640;&#32423;&#35821;&#20041;&#30456;&#20851;&#20869;&#23481;&#21644;&#20302;&#32423;&#35299;&#21078;&#29305;&#24449;&#38598;&#25104;&#21040;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#38750;&#24120;&#37325;&#35201;&#12290;&#36817;&#26399;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21307;&#23398;&#20998;&#21106;&#26041;&#27861;&#22312;&#26356;&#22909;&#22320;&#24314;&#27169;&#36825;&#20123;&#20449;&#24687;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#21307;&#23398;&#20998;&#21106;&#30340;&#21367;&#31215;&#25805;&#20316;&#36890;&#24120;&#22312;&#35268;&#21017;&#32593;&#26684;&#19978;&#36816;&#34892;&#65292;&#36825;&#22312;&#39640;&#39057;&#21306;&#22495;&#21363;&#36793;&#30028;&#21306;&#22495;&#20013;&#22825;&#29983;&#27169;&#31946;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MORSE&#30340;&#36890;&#29992;&#38544;&#24335;&#31070;&#32463;&#28210;&#26579;&#26694;&#26550;&#65292;&#26088;&#22312;&#22312;&#35299;&#21078;&#23618;&#38754;&#19978;&#20026;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#36741;&#21161;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#20107;&#23454;&#65306;&#30456;&#36739;&#20110;&#31163;&#25955;&#30340;&#22522;&#20110;&#32593;&#26684;&#30340;&#34920;&#31034;&#26041;&#24335;&#65292;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#22312;&#25311;&#21512;&#22797;&#26434;&#20449;&#21495;&#21644;&#35299;&#20915;&#35745;&#31639;&#26426;&#22270;&#24418;&#38382;&#39064;&#26102;&#34920;&#29616;&#26356;&#20026;&#26377;&#25928;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#23558;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#35270;&#20026;&#28210;&#26579;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25345;&#32493;&#22320;&#23545;&#40784;&#31895;&#30053;&#30340;&#20998;&#21106;p&#24182;&#21033;&#29992;&#38543;&#26426;&#19987;&#23478;&#26469;&#29983;&#25104;&#28210;&#26579;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integrating high-level semantically correlated contents and low-level anatomical features is of central importance in medical image segmentation. Towards this end, recent deep learning-based medical segmentation methods have shown great promise in better modeling such information. However, convolution operators for medical segmentation typically operate on regular grids, which inherently blur the high-frequency regions, i.e., boundary regions. In this work, we propose MORSE, a generic implicit neural rendering framework designed at an anatomical level to assist learning in medical image segmentation. Our method is motivated by the fact that implicit neural representation has been shown to be more effective in fitting complex signals and solving computer graphics problems than discrete grid-based representation. The core of our approach is to formulate medical image segmentation as a rendering problem in an end-to-end manner. Specifically, we continuously align the coarse segmentation p
&lt;/p&gt;</description></item><item><title>HOTGP &#26159;&#19968;&#31181;&#26032;&#30340;&#36951;&#20256;&#32534;&#31243;&#31639;&#27861;&#65292;&#21487;&#21512;&#25104;&#32431;&#12289;&#31867;&#22411;&#21644;&#20989;&#25968;&#31243;&#24207;&#12290;&#23427;&#21033;&#29992;&#35268;&#33539;&#30456;&#20851;&#30340;&#20016;&#23500;&#25968;&#25454;&#31867;&#22411;&#21644;&#20869;&#32622;&#35821;&#27861;&#25552;&#20379;&#30340;&#30693;&#35782;&#26469;&#38480;&#21046;&#25628;&#32034;&#31354;&#38388;&#24182;&#25913;&#21892;&#21512;&#25104;&#30340;&#24615;&#33021;</title><link>http://arxiv.org/abs/2304.03200</link><description>&lt;p&gt;
HOTGP &#8212;&#8212; &#39640;&#38454;&#31867;&#22411;&#36951;&#20256;&#31243;&#24207;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
HOTGP -- Higher-Order Typed Genetic Programming. (arXiv:2304.03200v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03200
&lt;/p&gt;
&lt;p&gt;
HOTGP &#26159;&#19968;&#31181;&#26032;&#30340;&#36951;&#20256;&#32534;&#31243;&#31639;&#27861;&#65292;&#21487;&#21512;&#25104;&#32431;&#12289;&#31867;&#22411;&#21644;&#20989;&#25968;&#31243;&#24207;&#12290;&#23427;&#21033;&#29992;&#35268;&#33539;&#30456;&#20851;&#30340;&#20016;&#23500;&#25968;&#25454;&#31867;&#22411;&#21644;&#20869;&#32622;&#35821;&#27861;&#25552;&#20379;&#30340;&#30693;&#35782;&#26469;&#38480;&#21046;&#25628;&#32034;&#31354;&#38388;&#24182;&#25913;&#21892;&#21512;&#25104;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31243;&#24207;&#21512;&#25104;&#26159;&#26681;&#25454;&#19968;&#32452;&#35268;&#33539;&#29983;&#25104;&#35745;&#31639;&#26426;&#31243;&#24207;&#30340;&#36807;&#31243;&#65292;&#35268;&#33539;&#21487;&#20197;&#26159;&#38382;&#39064;&#30340;&#39640;&#32423;&#25551;&#36848;&#21644;/&#25110;&#19968;&#32452;&#36755;&#20837;-&#36755;&#20986;&#31034;&#20363;&#12290; &#21512;&#25104;&#21487;&#20197;&#24314;&#27169;&#20026;&#19968;&#20010;&#25628;&#32034;&#38382;&#39064;&#65292;&#20854;&#20013;&#25628;&#32034;&#31354;&#38388;&#26159;&#35821;&#27861;&#19979;&#30340;&#25152;&#26377;&#26377;&#25928;&#31243;&#24207;&#38598;&#12290; &#30001;&#20110;&#25628;&#32034;&#31354;&#38388;&#38750;&#24120;&#24191;&#27867;&#65292;&#22240;&#27492;&#26292;&#21147;&#25628;&#32034;&#36890;&#24120;&#19981;&#21487;&#34892;&#65292;&#32780;&#25628;&#32034;&#21551;&#21457;&#24335;&#31639;&#27861;&#65288;&#20363;&#22914;&#36951;&#20256;&#32534;&#31243;&#65289;&#20063;&#38590;&#20197;&#22312;&#27809;&#26377;&#20219;&#20309;&#25351;&#23548;&#30340;&#24773;&#20917;&#19979;&#23548;&#33322;&#25628;&#32034;&#31354;&#38388;&#12290; &#26412;&#25991;&#20171;&#32461;&#20102; HOTGP&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#36951;&#20256;&#32534;&#31243;&#31639;&#27861;&#65292;&#21487;&#21512;&#25104;&#32431;&#12289;&#31867;&#22411;&#21644;&#20989;&#25968;&#31243;&#24207;&#12290; HOTGP&#21033;&#29992;&#35268;&#33539;&#30456;&#20851;&#30340;&#20016;&#23500;&#25968;&#25454;&#31867;&#22411;&#21644;&#20869;&#32622;&#35821;&#27861;&#25552;&#20379;&#30340;&#30693;&#35782;&#26469;&#38480;&#21046;&#25628;&#32034;&#31354;&#38388;&#24182;&#25913;&#21892;&#21512;&#25104;&#30340;&#24615;&#33021;&#12290; &#35821;&#27861;&#22522;&#20110; Haskell &#30340;&#26631;&#20934;&#22522;&#30784;&#24211;&#65288;&#21512;&#25104;&#20195;&#30721;&#21487;&#20197;&#30452;&#25509;&#20351;&#29992;&#20219;&#20309;&#26631;&#20934; Haskell &#32534;&#35793;&#22120;&#36827;&#34892;&#32534;&#35793;&#65289;&#65292;&#24182;&#21253;&#25324;&#23545;&#39640;&#38454;&#20989;&#25968;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
Program synthesis is the process of generating a computer program following a set of specifications, which can be a high-level description of the problem and/or a set of input-output examples. The synthesis can be modeled as a search problem in which the search space is the set of all the programs valid under a grammar. As the search space is vast, brute force is usually not viable and search heuristics, such as genetic programming, also have difficulty navigating it without any guidance. In this paper we present HOTGP, a new genetic programming algorithm that synthesizes pure, typed, and functional programs. HOTGP leverages the knowledge provided by the rich data-types associated with the specification and the built-in grammar to constrain the search space and improve the performance of the synthesis. The grammar is based on Haskell's standard base library (the synthesized code can be directly compiled using any standard Haskell compiler) and includes support for higher-order function
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#30784;&#38382;&#39064;&#20316;&#20026;&#22122;&#22768;&#35780;&#20272;VQA&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#21253;&#25324;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#26377;&#25928;&#25552;&#39640;&#20102;VQA&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.03147</link><description>&lt;p&gt;
&#20351;&#29992;&#19968;&#31995;&#21015;&#22522;&#30784;&#38382;&#39064;&#36827;&#34892;&#40065;&#26834;&#24615;&#20998;&#26512;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#25552;&#39640;VQA&#27169;&#22411;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Improving Visual Question Answering Models through Robustness Analysis and In-Context Learning with a Chain of Basic Questions. (arXiv:2304.03147v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#30784;&#38382;&#39064;&#20316;&#20026;&#22122;&#22768;&#35780;&#20272;VQA&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#21253;&#25324;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#26377;&#25928;&#25552;&#39640;&#20102;VQA&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20219;&#21153;&#20013;&#36215;&#21040;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#30740;&#31350;&#19968;&#33324;&#38598;&#20013;&#22312;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#36817;&#26469;&#26377;&#19968;&#20010;&#36235;&#21183;&#26159;&#23545;&#36825;&#20123;&#27169;&#22411;&#22312;&#38754;&#23545;&#25932;&#23545;&#25915;&#20987;&#65288;adversarial attacks&#65289;&#26102;&#36827;&#34892;&#40065;&#26834;&#24615;&#35780;&#20272;&#12290;&#36825;&#28041;&#21450;&#21040;&#22312;&#36755;&#20837;&#30340;&#19981;&#21516;&#22122;&#22768;&#27700;&#24179;&#19979;&#35780;&#20272;VQA&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#21487;&#20197;&#38024;&#23545;&#22270;&#20687;&#25110;&#24314;&#35758;&#30340;&#26597;&#35810;&#38382;&#39064;&#65288;&#21363;&#20027;&#38382;&#39064;&#65289;&#36827;&#34892;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;VQA&#39046;&#22495;&#36825;&#20010;&#26041;&#38754;&#32570;&#20047;&#36866;&#24403;&#30340;&#20998;&#26512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#35821;&#20041;&#30456;&#20851;&#30340;&#38382;&#39064;&#65288;&#31216;&#20026;&#22522;&#30784;&#38382;&#39064;&#65289;&#20316;&#20026;&#22122;&#22768;&#26469;&#35780;&#20272;VQA&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#24403;&#22522;&#30784;&#38382;&#39064;&#19982;&#20027;&#38382;&#39064;&#30340;&#30456;&#20284;&#24230;&#38477;&#20302;&#26102;&#65292;&#22122;&#22768;&#27700;&#24179;&#20250;&#22686;&#21152;&#12290;&#20026;&#20102;&#20135;&#29983;&#36866;&#24403;&#30340;&#22122;&#22768;&#27700;&#24179;&#65292;&#19968;&#32452;&#22522;&#30784;&#38382;&#39064;&#20250;&#26681;&#25454;&#20854;&#19982;&#20027;&#38382;&#39064;&#30340;&#30456;&#20284;&#24230;&#36827;&#34892;&#25490;&#21517;&#65292;&#36825;&#20010;&#25490;&#21517;&#20195;&#34920;&#24341;&#20837;VQA&#31995;&#32479;&#20013;&#30340;&#22122;&#22768;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#21253;&#25324;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;VQA&#27169;&#22411;&#34987;&#35757;&#32451;&#20102;&#20027;&#38382;&#39064;&#21644;&#30456;&#20851;&#30340;&#22522;&#30784;&#38382;&#39064;&#12290;&#22312;&#20004;&#20010;VQA&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;VQA&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have been critical in the task of Visual Question Answering (VQA), with research traditionally focused on improving model accuracy. Recently, however, there has been a trend towards evaluating the robustness of these models against adversarial attacks. This involves assessing the accuracy of VQA models under increasing levels of noise in the input, which can target either the image or the proposed query question, dubbed the main question. However, there is currently a lack of proper analysis of this aspect of VQA. This work proposes a new method that utilizes semantically related questions, referred to as basic questions, acting as noise to evaluate the robustness of VQA models. It is hypothesized that as the similarity of a basic question to the main question decreases, the level of noise increases. To generate a reasonable noise level for a given main question, a pool of basic questions is ranked based on their similarity to the main question, and this ranking pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;-&#35821;&#20041;&#33258;&#30417;&#30563;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#34892;&#20154;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#26126;&#30830;&#30340;&#35821;&#20041;&#19978;&#19979;&#25991;&#26469;&#35299;&#20915;&#28151;&#28102;&#30340;&#20154;&#31867;&#26679;&#24335;&#29289;&#20307;&#21644;&#23567;&#23610;&#24230;&#25110;&#20005;&#37325;&#36974;&#25377;&#30340;&#34892;&#20154;&#24120;&#24120;&#23548;&#33268;&#38169;&#35823;&#26816;&#27979;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.03135</link><description>&lt;p&gt;
VLPD: &#22522;&#20110;&#35270;&#35273;-&#35821;&#20041;&#33258;&#30417;&#30563;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#34892;&#20154;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
VLPD: Context-Aware Pedestrian Detection via Vision-Language Semantic Self-Supervision. (arXiv:2304.03135v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;-&#35821;&#20041;&#33258;&#30417;&#30563;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#34892;&#20154;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#26126;&#30830;&#30340;&#35821;&#20041;&#19978;&#19979;&#25991;&#26469;&#35299;&#20915;&#28151;&#28102;&#30340;&#20154;&#31867;&#26679;&#24335;&#29289;&#20307;&#21644;&#23567;&#23610;&#24230;&#25110;&#20005;&#37325;&#36974;&#25377;&#30340;&#34892;&#20154;&#24120;&#24120;&#23548;&#33268;&#38169;&#35823;&#26816;&#27979;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22478;&#24066;&#22330;&#26223;&#20013;&#20934;&#30830;&#26816;&#27979;&#34892;&#20154;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#25110;&#35270;&#39057;&#30417;&#25511;&#31561;&#29616;&#23454;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#28151;&#28102;&#30340;&#20154;&#31867;&#26679;&#24335;&#29289;&#20307;&#24120;&#24120;&#23548;&#33268;&#38169;&#35823;&#30340;&#26816;&#27979;&#65292;&#32780;&#23567;&#23610;&#24230;&#25110;&#20005;&#37325;&#36974;&#25377;&#30340;&#34892;&#20154;&#30001;&#20110;&#20854;&#19981;&#23547;&#24120;&#30340;&#22806;&#35266;&#24448;&#24448;&#34987;&#24573;&#30053;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#20165;&#32771;&#34385;&#23545;&#35937;&#21306;&#22495;&#26159;&#19981;&#22815;&#30340;&#65292;&#22240;&#27492;&#22914;&#20309;&#20805;&#20998;&#21033;&#29992;&#26356;&#26126;&#30830;&#21644;&#35821;&#20041;&#21270;&#30340;&#19978;&#19979;&#25991;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#20808;&#21069;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#34892;&#20154;&#26816;&#27979;&#22120;&#35201;&#20040;&#21482;&#23398;&#20064;&#20855;&#26377;&#35270;&#35273;&#32447;&#32034;&#30340;&#28508;&#22312;&#19978;&#19979;&#25991;&#65292;&#35201;&#20040;&#38656;&#35201;&#32321;&#29712;&#30340;&#27880;&#37322;&#26469;&#33719;&#21462;&#26126;&#30830;&#21644;&#35821;&#20041;&#19978;&#19979;&#25991;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35270;&#35273;-&#35821;&#20041;&#33258;&#30417;&#30563;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#34892;&#20154;&#26816;&#27979; (VLPD) &#26041;&#27861;&#65292;&#20197;&#22312;&#19981;&#20351;&#29992;&#39069;&#22806;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#26126;&#30830;&#24314;&#27169;&#35821;&#20041;&#19978;&#19979;&#25991;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#35270;&#35273;-&#35821;&#20041; (VLS) &#20998;&#21106;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23398;&#20064;&#20102;&#26082;&#26377;&#30417;&#30563;&#30340;&#34892;&#20154;&#26816;&#27979;&#65292;&#21448;&#26377;&#35821;&#20041;&#20998;&#21106;&#65292;&#32780;&#26080;&#38656;&#25163;&#21160;&#27880;&#37322;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#33258;&#36866;&#24212;&#27169;&#22359; (CAM) &#26469;&#25972;&#21512;&#25152;&#23398;&#20064;&#30340; VLS &#29305;&#24449;&#65292;&#36825;&#26174;&#33879;&#25552;&#39640;&#20102;&#34892;&#20154;&#26816;&#27979;&#24615;&#33021;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312; Caltech&#12289;KITTI &#21644; Citypersons &#31561;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting pedestrians accurately in urban scenes is significant for realistic applications like autonomous driving or video surveillance. However, confusing human-like objects often lead to wrong detections, and small scale or heavily occluded pedestrians are easily missed due to their unusual appearances. To address these challenges, only object regions are inadequate, thus how to fully utilize more explicit and semantic contexts becomes a key problem. Meanwhile, previous context-aware pedestrian detectors either only learn latent contexts with visual clues, or need laborious annotations to obtain explicit and semantic contexts. Therefore, we propose in this paper a novel approach via Vision-Language semantic self-supervision for context-aware Pedestrian Detection (VLPD) to model explicitly semantic contexts without any extra annotations. Firstly, we propose a self-supervised Vision-Language Semantic (VLS) segmentation method, which learns both fully-supervised pedestrian detection an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#31070;&#32463;&#21457;&#29983;&#12289;&#31070;&#32463;&#36798;&#23572;&#25991;&#20027;&#20041;&#21644;&#29289;&#31181;&#36827;&#21270;&#22914;&#20309;&#21551;&#21457;&#28436;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21019;&#20316;&#65292;&#24182;&#24378;&#35843;&#20102;&#22312;DNNs&#28436;&#21270;&#20013;dropout&#26041;&#27861;&#19982;&#31070;&#32463;&#21457;&#29983;&#30340;&#32852;&#31995;&#12290;</title><link>http://arxiv.org/abs/2304.03122</link><description>&lt;p&gt;
&#31070;&#32463;&#21457;&#29983;&#12289;&#31070;&#32463;&#36798;&#23572;&#25991;&#20027;&#20041;&#21644;&#29289;&#31181;&#36827;&#21270;&#21487;&#21542;&#20316;&#20026;&#28436;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21019;&#24314;&#30340;&#28789;&#24863;&#26469;&#28304;?
&lt;/p&gt;
&lt;p&gt;
Is it conceivable that neurogenesis, neural Darwinism, and species evolution could all serve as inspiration for the creation of evolutionary deep neural networks?. (arXiv:2304.03122v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#31070;&#32463;&#21457;&#29983;&#12289;&#31070;&#32463;&#36798;&#23572;&#25991;&#20027;&#20041;&#21644;&#29289;&#31181;&#36827;&#21270;&#22914;&#20309;&#21551;&#21457;&#28436;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21019;&#20316;&#65292;&#24182;&#24378;&#35843;&#20102;&#22312;DNNs&#28436;&#21270;&#20013;dropout&#26041;&#27861;&#19982;&#31070;&#32463;&#21457;&#29983;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#26159;&#20351;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#30340;&#65292;&#26159;&#33021;&#22815;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;DNNs&#20027;&#35201;&#26159;&#25163;&#24037;&#26500;&#24314;&#30340;&#65292;&#36890;&#24120;&#21253;&#21547;&#22823;&#37327;&#30340;&#23618;&#25968;&#12290;&#26412;&#25991;&#24378;&#35843;&#20102;&#25105;&#20204;&#25152;&#35828;&#30340;&#20108;&#32500;&#33041;&#37096;&#36827;&#21270;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#21450;&#23427;&#22914;&#20309;&#21551;&#21457;&#20108;&#32500;DNN&#28436;&#21270;&#24314;&#27169;&#12290;&#25105;&#20204;&#36824;&#24378;&#35843;&#20102;&#22312;DNNs&#27491;&#21017;&#21270;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;dropout&#26041;&#27861;&#19982;&#22823;&#33041;&#31070;&#32463;&#21457;&#29983;&#30340;&#32852;&#31995;&#65292;&#20197;&#21450;&#36825;&#20123;&#27010;&#24565;&#22914;&#20309;&#26377;&#30410;&#20110;DNNs&#30340;&#28436;&#21270;&#12290;&#35813;&#35770;&#25991;&#24635;&#32467;&#20102;&#20960;&#20010;&#22686;&#24378;&#33258;&#21160;&#26500;&#24314;DNNs&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks (DNNs) are built using artificial neural networks. They are part of machine learning methods that are capable of learning from data that have been used in a wide range of applications. DNNs are mainly handcrafted and they usually contain numerous layers. Research frontier has emerged that concerns automated construction of DNNs via evolutionary algorithms. This paper emphasizes the importance of what we call two-dimensional brain evolution and how it can inspire two dimensional DNN evolutionary modeling. We also highlight the connection between the dropout method which is widely-used in regularizing DNNs and neurogenesis of the brain, and how these concepts could benefit DNNs evolution.The paper concludes with several recommendations for enhancing the automatic construction of DNNs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;HR-DSS&#26041;&#27861;&#65292;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;AI&#24110;&#21161;&#20154;&#21147;&#36164;&#28304;&#37096;&#38376;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#30340;&#21592;&#24037;&#27969;&#22833;&#39044;&#27979;&#32467;&#26524;&#65292;&#24182;&#19988;&#25552;&#20379;&#8220;What-if-analysis&#8221;&#26469;&#35266;&#23519;&#20010;&#20307;&#21592;&#24037;&#21487;&#33021;&#23548;&#33268;&#31163;&#32844;&#30340;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2304.03103</link><description>&lt;p&gt;
&#30041;&#20303;&#20154;&#25165;&#26159;&#26368;&#37325;&#35201;&#30340;&#65292;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;AI&#26469;&#35299;&#20915;&#21592;&#24037;&#31163;&#32844;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Retention Is All You Need. (arXiv:2304.03103v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;HR-DSS&#26041;&#27861;&#65292;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;AI&#24110;&#21161;&#20154;&#21147;&#36164;&#28304;&#37096;&#38376;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#30340;&#21592;&#24037;&#27969;&#22833;&#39044;&#27979;&#32467;&#26524;&#65292;&#24182;&#19988;&#25552;&#20379;&#8220;What-if-analysis&#8221;&#26469;&#35266;&#23519;&#20010;&#20307;&#21592;&#24037;&#21487;&#33021;&#23548;&#33268;&#31163;&#32844;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29087;&#32451;&#30340;&#21592;&#24037;&#36890;&#24120;&#34987;&#35270;&#20026;&#32452;&#32455;&#30340;&#26368;&#37325;&#35201;&#25903;&#26609;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#22823;&#22810;&#25968;&#32452;&#32455;&#37117;&#38754;&#20020;&#30528;&#39640;&#31163;&#32844;&#29575;&#21644;&#27969;&#22833;&#29575;&#12290;&#34429;&#28982;&#24050;&#32463;&#24320;&#21457;&#20102;&#20960;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;&#20998;&#26512;&#31163;&#32844;&#21450;&#20854;&#21407;&#22240;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#30340;&#35299;&#37322;&#20173;&#28982;&#19981;&#36879;&#26126;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;HR-DSS&#26041;&#27861;&#65292;&#21363;&#20154;&#21147;&#36164;&#28304;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;AI&#35299;&#20915;&#21592;&#24037;&#27969;&#22833;&#38382;&#39064;&#12290;&#35813;&#31995;&#32479;&#26088;&#22312;&#24110;&#21161;&#20154;&#21147;&#36164;&#28304;&#37096;&#38376;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#20351;&#29992;&#20102;&#20843;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#19988;&#26368;&#20339;&#34920;&#29616;&#30340;&#27169;&#22411;&#30340;&#32467;&#26524;&#36827;&#19968;&#27493;&#32463;&#36807;&#20102;SHAP&#35299;&#37322;&#24615;&#36807;&#31243;&#30340;&#22788;&#29702;&#12290;&#25105;&#20204;&#20248;&#21270;&#20102;&#32467;&#26524;&#30340;&#27491;&#30830;&#24615;&#21644;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#8220;What-if-analysis&#8221;&#26469;&#35266;&#23519;&#20010;&#20307;&#21592;&#24037;&#21487;&#33021;&#23548;&#33268;&#31163;&#32844;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
Skilled employees are usually seen as the most important pillar of an organization. Despite this, most organizations face high attrition and turnover rates. While several machine learning models have been developed for analyzing attrition and its causal factors, the interpretations of those models remain opaque. In this paper, we propose the HR-DSS approach, which stands for Human Resource Decision Support System, and uses explainable AI for employee attrition problems. The system is designed to assist human resource departments in interpreting the predictions provided by machine learning models. In our experiments, eight machine learning models are employed to provide predictions, and the results achieved by the best-performing model are further processed by the SHAP explainability process. We optimize both the correctness and explanation of the results. Furthermore, using "What-if-analysis", we aim to observe plausible causes for attrition of an individual employee. The results show 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#21033;&#29992;LLMs&#22312;&#29273;&#31185;&#20020;&#24202;&#39046;&#22495;&#23454;&#29616;&#33258;&#21160;&#21270;&#21644;&#36328;&#27169;&#24577;&#35786;&#26029;&#30340;&#21487;&#33021;&#24615;&#65292;&#20171;&#32461;&#20102;&#21033;&#29992;&#36328;&#27169;&#24577;&#32534;&#30721;&#22120;&#36827;&#34892;&#39640;&#32423;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#22810;&#27169;&#24577;LLM AI&#31995;&#32479;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#29273;&#31185;&#20020;&#24202;&#20013;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.03086</link><description>&lt;p&gt;
ChatGPT&#22609;&#36896;&#29273;&#31185;&#26410;&#26469;&#65306;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
ChatGPT for Shaping the Future of Dentistry: The Potential of Multi-Modal Large Language Model. (arXiv:2304.03086v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03086
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#21033;&#29992;LLMs&#22312;&#29273;&#31185;&#20020;&#24202;&#39046;&#22495;&#23454;&#29616;&#33258;&#21160;&#21270;&#21644;&#36328;&#27169;&#24577;&#35786;&#26029;&#30340;&#21487;&#33021;&#24615;&#65292;&#20171;&#32461;&#20102;&#21033;&#29992;&#36328;&#27169;&#24577;&#32534;&#30721;&#22120;&#36827;&#34892;&#39640;&#32423;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#22810;&#27169;&#24577;LLM AI&#31995;&#32479;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#29273;&#31185;&#20020;&#24202;&#20013;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;OpenAI&#24320;&#21457;&#30340;Generative Pretrained Transformer 4&#65288;GPT-4&#65289;&#30340;&#31934;&#31616;&#21644;&#23545;&#35805;&#21464;&#20307;&#65292;&#20855;&#26377;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#30340;&#37324;&#31243;&#30865;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#20043;&#19968;&#12290;&#20107;&#23454;&#19978;&#65292;LLMs&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#30340;&#21360;&#35937;&#28145;&#21051;&#33021;&#21147;&#24341;&#36215;&#20102;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#30340;&#26497;&#22823;&#20852;&#36259;&#65292;&#23545;&#21508;&#20010;&#39046;&#22495;&#20135;&#29983;&#20102;&#28145;&#36828;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#20027;&#35201;&#35752;&#35770;LLMs&#22312;&#29273;&#31185;&#39046;&#22495;&#30340;&#26410;&#26469;&#24212;&#29992;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#31181;&#20027;&#35201;&#30340;LLM&#37096;&#32626;&#26041;&#27861;&#65292;&#21253;&#25324;&#33258;&#21160;&#29273;&#31185;&#35786;&#26029;&#21644;&#36328;&#27169;&#24577;&#29273;&#31185;&#35786;&#26029;&#65292;&#24182;&#25506;&#35752;&#20102;&#23427;&#20204;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;&#29305;&#21035;&#22320;&#65292;&#37197;&#22791;&#36328;&#27169;&#24577;&#32534;&#30721;&#22120;&#65292;&#21333;&#20010;LLM&#21487;&#20197;&#31649;&#29702;&#22810;&#28304;&#25968;&#25454;&#24182;&#36827;&#34892;&#39640;&#32423;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65292;&#20197;&#25191;&#34892;&#22797;&#26434;&#30340;&#20020;&#24202;&#25805;&#20316;&#12290;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#26469;&#23637;&#31034;&#38024;&#23545;&#29273;&#31185;&#20020;&#24202;&#24212;&#29992;&#30340;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#22810;&#27169;&#24577;LLM AI&#31995;&#32479;&#30340;&#28508;&#21147;&#12290;&#34429;&#28982;LLMs&#22312;&#25552;&#20379;&#24040;&#22823;&#30340;&#28508;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;
&lt;/p&gt;
&lt;p&gt;
The ChatGPT, as a lite and conversational variant of Generative Pretrained Transformer 4 (GPT-4) developed by OpenAI, is one of the milestone Large Language Models (LLMs) with billions of parameters. LLMs, in fact, have stirred up a lot of interest among researchers and practitioners by their impressive skills in natural language processing tasks, which have a profound impact on a wide range of fields. This paper mainly discusses the future applications of LLMs in dentistry. We introduce two primary LLM deployment methods in dentistry, including automated dental diagnosis and cross-modal dental diagnosis, and examine their potential applications. Especially, equipped with a cross-modal encoder, a single LLM can manage multi-source data and conduct advanced natural language reasoning to perform complex clinical operations. A use case is presented to demonstrate the potential of a fully automatic Multi-Modal LLM AI system for dentistry clinical application. While LLMs offer significant p
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23398;&#20064;&#19981;&#33391;&#36712;&#36857;&#30340;&#26102;&#38388;&#27169;&#24335;&#21644;&#38450;&#27490;&#36127;&#38754;&#21103;&#20316;&#29992;&#23454;&#29616;&#23433;&#20840;MDP&#35268;&#21010;</title><link>http://arxiv.org/abs/2304.03081</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#19981;&#33391;&#36712;&#36857;&#30340;&#26102;&#38388;&#27169;&#24335;&#21644;&#38450;&#27490;&#36127;&#38754;&#21103;&#20316;&#29992;&#23454;&#29616;&#23433;&#20840;MDP&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Safe MDP Planning by Learning Temporal Patterns of Undesirable Trajectories and Averting Negative Side Effects. (arXiv:2304.03081v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03081
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#19981;&#33391;&#36712;&#36857;&#30340;&#26102;&#38388;&#27169;&#24335;&#21644;&#38450;&#27490;&#36127;&#38754;&#21103;&#20316;&#29992;&#23454;&#29616;&#23433;&#20840;MDP&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23433;&#20840;MDP&#35268;&#21010;&#20013;&#65292;&#22522;&#20110;&#24403;&#21069;&#29366;&#24577;&#21644;&#21160;&#20316;&#30340;&#20195;&#20215;&#20989;&#25968;&#36890;&#24120;&#29992;&#20110;&#25351;&#23450;&#23433;&#20840;&#26041;&#38754;&#65292;&#20294;&#29616;&#23454;&#19990;&#30028;&#20013;&#20351;&#29992;&#30340;&#29366;&#24577;&#34920;&#31034;&#36890;&#24120;&#32570;&#20047;&#36275;&#22815;&#30340;&#20934;&#30830;&#24230;&#26469;&#25351;&#23450;&#36825;&#26679;&#30340;&#23433;&#20840;&#32422;&#26463;&#26465;&#20214;&#65292;&#22522;&#20110;&#19981;&#23436;&#25972;&#27169;&#22411;&#24037;&#20316;&#24120;&#24120;&#20250;&#20135;&#29983;&#24847;&#22806;&#30340;&#36127;&#38754;&#21103;&#20316;&#29992;&#65288;NSEs&#65289;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#23433;&#20840;&#20449;&#21495;&#19982;&#29366;&#24577;-&#21160;&#20316;&#36712;&#36857;&#30456;&#20851;&#32852;&#65288;&#32780;&#19981;&#20165;&#20165;&#26159;&#29366;&#24577;-&#21160;&#20316;&#21363;&#26102;&#65289;&#20351;&#25105;&#20204;&#30340;&#23433;&#20840;&#27169;&#22411;&#20855;&#26377;&#39640;&#24230;&#30340;&#36890;&#29992;&#24615;&#12290;&#25105;&#20204;&#36824;&#20551;&#35774;&#20026;&#19981;&#21516;&#30340;&#36712;&#36857;&#25552;&#20379;&#20102;&#20998;&#31867;&#23433;&#20840;&#26631;&#31614;&#65292;&#32780;&#19981;&#26159;&#26356;&#38590;&#30001;&#38382;&#39064;&#35774;&#35745;&#32773;&#25351;&#23450;&#30340;&#25968;&#20540;&#20195;&#20215;&#20989;&#25968;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#26469;&#23398;&#20064;&#36825;&#26679;&#30340;&#38750;&#39532;&#23572;&#31185;&#22827;&#23433;&#20840;&#27169;&#24335;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#25289;&#26684;&#26391;&#26085;&#20056;&#25968;&#26041;&#27861;&#65292;&#23558;&#23433;&#20840;&#27169;&#22411;&#21644;&#22522;&#30784;MDP&#27169;&#22411;&#21512;&#24182;&#25104;&#19968;&#20010;&#35745;&#31639;&#22270;&#65292;&#20197;&#20419;&#36827;&#20195;&#29702;&#23398;&#20064;&#23433;&#20840;&#34892;&#20026;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;...
&lt;/p&gt;
&lt;p&gt;
In safe MDP planning, a cost function based on the current state and action is often used to specify safety aspects. In the real world, often the state representation used may lack sufficient fidelity to specify such safety constraints. Operating based on an incomplete model can often produce unintended negative side effects (NSEs). To address these challenges, first, we associate safety signals with state-action trajectories (rather than just an immediate state-action). This makes our safety model highly general. We also assume categorical safety labels are given for different trajectories, rather than a numerical cost function, which is harder to specify by the problem designer. We then employ a supervised learning model to learn such non-Markovian safety patterns. Second, we develop a Lagrange multiplier method, which incorporates the safety model and the underlying MDP model in a single computation graph to facilitate agent learning of safe behaviors. Finally, our empirical results
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340; HVAC &#25511;&#21046;&#26041;&#27861;&#65292;&#20351;&#29992;&#31526;&#21495;&#22238;&#24402;&#27169;&#22411;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340; HVAC &#31995;&#32479;&#27169;&#22411;&#65292;&#36890;&#36807;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#23454;&#29616;&#26368;&#23567;&#21270;&#33021;&#32791;&#12289;&#23792;&#20540;&#21151;&#29575;&#38656;&#27714;&#21644;&#26368;&#22823;&#21270;&#28909;&#33298;&#36866;&#24230;&#12290;&#30456;&#27604;&#20110;&#24658;&#28201;&#25511;&#21046;&#22120;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#23558;&#23792;&#20540;&#21151;&#29575;&#38477;&#20302;&#20102; 16.1\%&#12290;</title><link>http://arxiv.org/abs/2304.03078</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#31526;&#21495;&#22238;&#24402; HVAC &#25511;&#21046;&#65306;&#35774;&#35745;&#19982;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Data-driven HVAC Control Using Symbolic Regression: Design and Implementation. (arXiv:2304.03078v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03078
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340; HVAC &#25511;&#21046;&#26041;&#27861;&#65292;&#20351;&#29992;&#31526;&#21495;&#22238;&#24402;&#27169;&#22411;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340; HVAC &#31995;&#32479;&#27169;&#22411;&#65292;&#36890;&#36807;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#23454;&#29616;&#26368;&#23567;&#21270;&#33021;&#32791;&#12289;&#23792;&#20540;&#21151;&#29575;&#38656;&#27714;&#21644;&#26368;&#22823;&#21270;&#28909;&#33298;&#36866;&#24230;&#12290;&#30456;&#27604;&#20110;&#24658;&#28201;&#25511;&#21046;&#22120;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#23558;&#23792;&#20540;&#21151;&#29575;&#38477;&#20302;&#20102; 16.1\%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31569;&#20013;&#25910;&#38598;&#21040;&#30340;&#22823;&#37327;&#25968;&#25454;&#20351;&#33021;&#28304;&#31649;&#29702;&#26356;&#21152;&#26234;&#33021;&#21644;&#39640;&#25928;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20379;&#26262;&#12289;&#36890;&#39118;&#21644;&#31354;&#35843;&#65288;HVAC&#65289;&#25511;&#21046;&#30340;&#35774;&#35745;&#21644;&#23454;&#29616;&#26041;&#27861;&#12290;&#24314;&#31569;&#28909;&#21147;&#23398;&#26159;&#20351;&#29992;&#20174;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#26500;&#24314;&#30340;&#31526;&#21495;&#22238;&#24402;&#27169;&#22411;&#65288;SRM&#65289;&#36827;&#34892;&#24314;&#27169;&#30340;&#12290;&#27492;&#22806;&#65292;&#36824;&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#24320;&#21457;&#20102;&#19968;&#20010; HVAC &#31995;&#32479;&#27169;&#22411;&#12290;&#20351;&#29992;&#24320;&#21457;&#30340;&#27169;&#22411;&#21046;&#23450;&#20102;&#19968;&#20010;&#22522;&#20110;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#30340; HVAC &#35843;&#24230;&#65292;&#20197;&#26368;&#23567;&#21270;&#33021;&#32791;&#21644;&#23792;&#20540;&#21151;&#29575;&#38656;&#27714;&#24182;&#26368;&#22823;&#21270;&#28909;&#33298;&#36866;&#24230;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#24615;&#33021;&#22312;&#23454;&#38469;&#26657;&#22253;&#24314;&#31569;&#30340;&#24037;&#20316;&#21306;&#28436;&#31034;&#12290;&#20351;&#29992;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340; HVAC &#31995;&#32479;&#23558;&#23792;&#20540;&#21151;&#29575;&#38477;&#20302;&#20102; 16.1\%&#65292;&#30456;&#27604;&#24191;&#27867;&#20351;&#29992;&#30340;&#24658;&#28201;&#25511;&#21046;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
The large amount of data collected in buildings makes energy management smarter and more energy efficient. This study proposes a design and implementation methodology of data-driven heating, ventilation, and air conditioning (HVAC) control. Building thermodynamics is modeled using a symbolic regression model (SRM) built from the collected data. Additionally, an HVAC system model is also developed with a data-driven approach. A model predictive control (MPC) based HVAC scheduling is formulated with the developed models to minimize energy consumption and peak power demand and maximize thermal comfort. The performance of the proposed framework is demonstrated in the workspace in the actual campus building. The HVAC system using the proposed framework reduces the peak power by 16.1\% compared to the widely used thermostat controller.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#26032;&#30340;3RL&#25968;&#25454;&#38598;&#65292;&#23427;&#26159;&#19968;&#31181;&#29992;&#20110;&#38754;&#37096;&#24773;&#24863;&#35782;&#21035;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#19982;&#20854;&#20182;&#25968;&#25454;&#38598;&#36827;&#34892;&#27604;&#36739;&#65292;&#34920;&#29616;&#20986;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;(cnn)&#23454;&#29616;&#20102;91.4&#65285;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.03064</link><description>&lt;p&gt;
&#19968;&#39033;&#20851;&#20110;&#26032;3RL&#25968;&#25454;&#38598;&#23454;&#26102;&#38754;&#37096;&#24773;&#24863;&#35782;&#21035;&#30340;&#23454;&#39564;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An experimental study in Real-time Facial Emotion Recognition on new 3RL dataset. (arXiv:2304.03064v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03064
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#26032;&#30340;3RL&#25968;&#25454;&#38598;&#65292;&#23427;&#26159;&#19968;&#31181;&#29992;&#20110;&#38754;&#37096;&#24773;&#24863;&#35782;&#21035;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#19982;&#20854;&#20182;&#25968;&#25454;&#38598;&#36827;&#34892;&#27604;&#36739;&#65292;&#34920;&#29616;&#20986;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;(cnn)&#23454;&#29616;&#20102;91.4&#65285;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#23454;&#26102;&#38754;&#37096;&#24773;&#24863;&#35782;&#21035;&#26159;&#20154;&#26426;&#20132;&#20114;&#39046;&#22495;&#30340;&#19968;&#20010;&#28909;&#38376;&#30740;&#31350;&#39046;&#22495;&#65292;&#20294;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#25968;&#25454;&#38598;&#20173;&#23384;&#22312;&#21508;&#31181;&#38382;&#39064;&#65292;&#22914;&#19968;&#20123;&#19982;&#24773;&#24863;&#26080;&#20851;&#30340;&#29031;&#29255;&#65288;&#22914;&#25991;&#20214;&#29031;&#29255;&#65289;&#65292;&#27599;&#31867;&#29031;&#29255;&#25968;&#37327;&#19981;&#24179;&#34913;&#20197;&#21450;&#21487;&#33021;&#23545;&#27491;&#30830;&#20998;&#31867;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#30340;&#35823;&#23548;&#24615;&#22270;&#20687;&#12290;&#20026;&#20102;&#20811;&#26381;&#20197;&#21069;&#21487;&#29992;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;3RL&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#32422;24K&#24352;&#22270;&#20687;&#65292;&#24182;&#19988;&#23558;&#20844;&#24320;&#25552;&#20379;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#34987;&#26631;&#35760;&#20026;&#20116;&#31181;&#22522;&#26412;&#24773;&#32490;&#65306;&#24555;&#20048;&#12289;&#24656;&#24807;&#12289;&#24754;&#20260;&#12289;&#21388;&#24694;&#21644;&#24868;&#24594;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;3RL&#25968;&#25454;&#38598;&#19982;&#20854;&#20182;&#33879;&#21517;&#30340;&#26368;&#20808;&#36827;&#25968;&#25454;&#38598;&#65288;FER&#25968;&#25454;&#38598;&#12289;CK+&#25968;&#25454;&#38598;&#65289;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#24212;&#29992;&#20102;&#20808;&#21069;&#24037;&#20316;&#20013;&#26368;&#24120;&#29992;&#30340;&#31639;&#27861;&#65292;&#22914;SVM&#21644;CNN&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;3RL&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#19978;&#26377;&#26126;&#26174;&#30340;&#25552;&#39640;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#20351;&#29992;CNN&#22312;3RL&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#21487;&#36798;91.4&#65285;&#65292;&#32780;FER2013&#12289;CK +&#30340;&#32467;&#26524;&#21017;&#30053;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although real-time facial emotion recognition is a hot topic research domain in the field of human-computer interaction, state-of the-art available datasets still suffer from various problems, such as some unrelated photos such as document photos, unbalanced numbers of photos in each class, and misleading images that can negatively affect correct classification. The 3RL dataset was created, which contains approximately 24K images and will be publicly available, to overcome previously available dataset problems. The 3RL dataset is labelled with five basic emotions: happiness, fear, sadness, disgust, and anger. Moreover, we compared the 3RL dataset with other famous state-of-the-art datasets (FER dataset, CK+ dataset), and we applied the most commonly used algorithms in previous works, SVM and CNN. The results show a noticeable improvement in generalization on the 3RL dataset. Experiments have shown an accuracy of up to 91.4% on 3RL dataset using CNN where results on FER2013, CK+ are, re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#23545;&#26367;&#20195;&#21697;&#30340;&#25490;&#21517;&#65292;&#36890;&#36807;&#31639;&#27861;&#25214;&#21040;&#20102;&#19968;&#31181;&#20960;&#20046;&#26368;&#20248;&#30340;&#25805;&#32437;&#26041;&#24335;&#65292;&#20197;&#30830;&#23450;&#22312;&#32473;&#23450;&#24773;&#20917;&#19979;&#25805;&#32437;&#30340;&#38590;&#26131;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.03060</link><description>&lt;p&gt;
&#19968;&#23545;&#26367;&#20195;&#21697;&#30340;&#20960;&#20046;&#26368;&#20248;&#25805;&#32437;
&lt;/p&gt;
&lt;p&gt;
Almost optimal manipulation of a pair of alternatives. (arXiv:2304.03060v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03060
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#23545;&#26367;&#20195;&#21697;&#30340;&#25490;&#21517;&#65292;&#36890;&#36807;&#31639;&#27861;&#25214;&#21040;&#20102;&#19968;&#31181;&#20960;&#20046;&#26368;&#20248;&#30340;&#25805;&#32437;&#26041;&#24335;&#65292;&#20197;&#30830;&#23450;&#22312;&#32473;&#23450;&#24773;&#20917;&#19979;&#25805;&#32437;&#30340;&#38590;&#26131;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#36807;&#31243;&#20013;&#19987;&#23478;&#30340;&#35282;&#33394;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#26368;&#32456;&#30340;&#24314;&#35758;&#21462;&#20915;&#20110;&#20182;&#20204;&#30340;&#24577;&#24230;&#12289;&#22836;&#33041;&#28165;&#26224;&#31243;&#24230;&#12289;&#32463;&#39564;&#21644;&#23545;&#38382;&#39064;&#30340;&#20102;&#35299;&#12290;&#20294;&#26159;&#65292;&#24314;&#35758;&#36824;&#21462;&#20915;&#20110;&#20182;&#20204;&#30340;&#35802;&#23454;&#12290;&#22914;&#26524;&#19987;&#23478;&#19981;&#35802;&#23454;&#24590;&#20040;&#21150;&#65311;&#37027;&#20040;&#65292;&#22312;&#32473;&#23450;&#24773;&#20917;&#19979;&#25805;&#32437;&#26159;&#22810;&#20040;&#22256;&#38590;&#23601;&#21464;&#24471;&#24456;&#37325;&#35201;&#20102;&#12290;&#26412;&#30740;&#31350;&#32771;&#34385;&#21040;&#20102;&#36890;&#36807;&#27604;&#36739;&#19968;&#23545;&#26367;&#20195;&#21697;&#33719;&#24471;&#30340;&#25490;&#21517;&#30340;&#25805;&#32437;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#29992;&#20110;&#25214;&#21040;&#20132;&#25442;&#36873;&#23450;&#30340;&#20004;&#20010;&#26367;&#20195;&#21697;&#20301;&#32622;&#30340;&#20960;&#20046;&#26368;&#20248;&#26041;&#24335;&#12290;&#30001;&#27492;&#65292;&#23601;&#21487;&#20197;&#30830;&#23450;&#22312;&#32473;&#23450;&#24773;&#20917;&#19979;&#25805;&#32437;&#26159;&#22810;&#20040;&#22256;&#38590;&#30340;&#20102;&#12290;&#29702;&#35770;&#32771;&#34385;&#36890;&#36807;&#19968;&#20010;&#23454;&#38469;&#20363;&#23376;&#36827;&#34892;&#20102;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
The role of an expert in the decision-making process is crucial, as the final recommendation depends on his disposition, clarity of mind, experience, and knowledge of the problem. However, the recommendation also depends on their honesty. But what if the expert is dishonest? Then, the answer on how difficult it is to manipulate in a given case becomes essential. In the presented work, we consider manipulation of a ranking obtained by comparing alternatives in pairs. More specifically, we propose an algorithm for finding an almost optimal way to swap the positions of two selected alternatives. Thanks to this, it is possible to determine how difficult such manipulation is in a given case. Theoretical considerations are illustrated by a practical example.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35266;&#23519;&#21040;&#22522;&#20110;DPR&#30340;&#26368;&#36817;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#32463;&#24120;&#23558;&#26080;&#27861;&#22238;&#31572;&#30340;&#21453;&#20107;&#23454;&#24773;&#26223;&#25490;&#21517;&#39640;&#20110;&#21487;&#22238;&#31572;&#30340;&#21407;&#22987;&#24773;&#26223;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#27573;&#33853;&#26816;&#32034;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;PiCL&#12290;</title><link>http://arxiv.org/abs/2304.03031</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#24102;&#26377;&#26080;&#27861;&#22238;&#31572;&#30340;&#21453;&#20107;&#23454;&#24773;&#26223;&#30340;&#23494;&#38598;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Revisiting Dense Retrieval with Unanswerable Counterfactuals. (arXiv:2304.03031v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35266;&#23519;&#21040;&#22522;&#20110;DPR&#30340;&#26368;&#36817;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#32463;&#24120;&#23558;&#26080;&#27861;&#22238;&#31572;&#30340;&#21453;&#20107;&#23454;&#24773;&#26223;&#25490;&#21517;&#39640;&#20110;&#21487;&#22238;&#31572;&#30340;&#21407;&#22987;&#24773;&#26223;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#27573;&#33853;&#26816;&#32034;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;PiCL&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#65288;ODQA&#65289;&#20013;&#65292;&#26816;&#32034;&#22120;-&#38405;&#35835;&#22120;&#26694;&#26550;&#24456;&#21463;&#27426;&#36814;&#65292;&#20854;&#20013;&#26816;&#32034;&#22120;&#20174;&#22823;&#22411;&#35821;&#26009;&#24211;&#20013;&#20026;&#38405;&#35835;&#22120;&#25277;&#21462;&#19968;&#32452;&#30456;&#20851;&#30340;&#20505;&#36873;&#27573;&#33853;&#12290;&#36825;&#31181;&#26041;&#27861;&#32972;&#21518;&#30340;&#19968;&#20010;&#20851;&#38190;&#20551;&#35774;&#26159;&#65292;&#20174;&#26816;&#32034;&#22120;&#24471;&#21040;&#30340;&#39640;&#30456;&#20851;&#24615;&#20998;&#25968;&#21487;&#33021;&#34920;&#26126;&#20174;&#38405;&#35835;&#22120;&#33719;&#21462;&#31572;&#26696;&#30340;&#21487;&#33021;&#24615;&#24456;&#39640;&#65292;&#36825;&#24847;&#21619;&#30528;&#26816;&#32034;&#21040;&#30340;&#27573;&#33853;&#24456;&#21487;&#33021;&#21253;&#21547;&#32473;&#23450;&#38382;&#39064;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#22312;&#26412;&#30740;&#31350;&#20013;&#23454;&#35777;&#39539;&#26021;&#20102;&#36825;&#31181;&#35266;&#28857;&#65292;&#24182;&#35266;&#23519;&#21040;&#22522;&#20110;DPR&#30340;&#26368;&#36817;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#32463;&#24120;&#23558;&#26080;&#27861;&#22238;&#31572;&#30340;&#21453;&#20107;&#23454;&#24773;&#26223;&#25490;&#21517;&#39640;&#20110;&#21487;&#22238;&#31572;&#30340;&#21407;&#22987;&#24773;&#26223;&#12290;&#20026;&#20102;&#35299;&#20915;&#23494;&#38598;&#26816;&#32034;&#20013;&#36825;&#31181;&#23545;&#31572;&#26696;&#26080;&#24863;&#30693;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#23547;&#27714;&#20351;&#29992;&#21453;&#20107;&#23454;&#26679;&#26412;&#20316;&#20026;&#39069;&#22806;&#30340;&#35757;&#32451;&#36164;&#28304;&#65292;&#20197;&#26356;&#22909;&#22320;&#21516;&#27493;DPR&#30340;&#30456;&#20851;&#24615;&#27979;&#37327;&#21644;&#38382;&#39064;-&#27573;&#33853;&#23545;&#30340;&#21487;&#31572;&#24615;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21453;&#20107;&#23454;Pivoting&#23545;&#27604;&#23398;&#20064;&#65288;PiCL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#27573;&#33853;&#26816;&#32034;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The retriever-reader framework is popular for open-domain question answering (ODQA), where a retriever samples for the reader a set of relevant candidate passages from a large corpus. A key assumption behind this method is that high relevance scores from the retriever likely indicate high answerability from the reader, which implies a high probability that the retrieved passages contain answers to a given question. In this work, we empirically dispel this belief and observe that recent dense retrieval models based on DPR often rank unanswerable counterfactual passages higher than their answerable original passages. To address such answer-unawareness in dense retrievers, we seek to use counterfactual samples as additional training resources to better synchronize the relevance measurement of DPR with the answerability of question-passage pairs. Specifically, we present counterfactually-Pivoting Contrastive Learning (PiCL), a novel representation learning approach for passage retrieval th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#25104;&#20026;&#22810;&#23186;&#20307;&#21462;&#35777;&#35843;&#26597;&#30340;&#26631;&#20934;&#65292;&#36890;&#36807;&#39564;&#35777;&#21512;&#25104;&#38754;&#37096;&#22270;&#20687;&#26469;&#25506;&#31350;&#21360;&#21047;&#21644;&#25195;&#25551;&#22270;&#20687;&#30340;&#26377;&#25928;&#21462;&#35777;&#20998;&#26512;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.02982</link><description>&lt;p&gt;
&#31361;&#30772;&#24615;&#35770;&#25991;: Spritz-PS-&#21033;&#29992;&#22823;&#35268;&#27169;&#21360;&#21047;&#25991;&#20214;&#25968;&#25454;&#38598;&#23545;&#21512;&#25104;&#38754;&#37096;&#22270;&#20687;&#36827;&#34892;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Spritz-PS: Validation of Synthetic Face Images Using a Large Dataset of Printed Documents. (arXiv:2304.02982v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02982
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#25104;&#20026;&#22810;&#23186;&#20307;&#21462;&#35777;&#35843;&#26597;&#30340;&#26631;&#20934;&#65292;&#36890;&#36807;&#39564;&#35777;&#21512;&#25104;&#38754;&#37096;&#22270;&#20687;&#26469;&#25506;&#31350;&#21360;&#21047;&#21644;&#25195;&#25551;&#22270;&#20687;&#30340;&#26377;&#25928;&#21462;&#35777;&#20998;&#26512;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#23545;&#21360;&#21047;&#21644;&#25195;&#25551;&#65288;PS&#65289;&#22270;&#20687;&#36827;&#34892;&#26377;&#25928;&#30340;&#21462;&#35777;&#20998;&#26512;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;PS&#25991;&#26723;&#21487;&#20197;&#29992;&#20110;&#38544;&#34255;&#22270;&#20687;&#30340;&#20266;&#36896;&#30165;&#36857;&#65292;&#22240;&#20026;&#36825;&#20123;&#30165;&#36857;&#36890;&#24120;&#23384;&#22312;&#20110;&#22788;&#29702;&#36807;&#30340;&#22270;&#20687;&#20013;&#65292;&#24182;&#19988;&#21512;&#25104;&#22270;&#20687;&#20013;&#30340;&#20027;&#35201;&#30165;&#36857;&#21487;&#20197;&#22312;PS&#20043;&#21518;&#21435;&#38500;&#12290;&#30001;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(GANs)&#30340;&#21560;&#24341;&#21147;&#65292;&#20351;&#29992;GANs&#27169;&#22411;&#29983;&#25104;&#30340;&#21512;&#25104;&#38754;&#37096;&#22270;&#20687;&#38590;&#20197;&#19982;&#30495;&#27491;&#30340;&#20154;&#31867;&#38754;&#37096;&#21306;&#20998;&#24320;&#26469;&#65292;&#21487;&#33021;&#29992;&#20110;&#21019;&#24314;&#20266;&#36896;&#36523;&#20221;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;GAN&#27169;&#22411;&#26410;&#32771;&#34385;&#22312;&#29983;&#25104;&#20154;&#31867;&#38754;&#37096;&#26102;&#30340;&#29983;&#29702;&#32422;&#26463;&#20197;&#21450;&#36825;&#20123;&#32422;&#26463;&#23545;&#20154;&#31867;&#34425;&#33180;&#30340;&#24433;&#21709;&#65292;&#22312;PS&#24773;&#20917;&#19979;&#21306;&#20998;&#30495;&#23454;&#21644;&#21512;&#25104;&#34425;&#33180;&#21464;&#24471;&#26497;&#20026;&#22256;&#38590;&#12290;&#30001;&#20110;&#32570;&#20047;&#22823;&#35268;&#27169;&#21442;&#32771;&#34425;&#33180;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#25104;&#20026;&#22810;&#23186;&#20307;&#21462;&#35777;(MFs)&#35843;&#26597;&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
The capability of doing effective forensic analysis on printed and scanned (PS) images is essential in many applications. PS documents may be used to conceal the artifacts of images which is due to the synthetic nature of images since these artifacts are typically present in manipulated images and the main artifacts in the synthetic images can be removed after the PS. Due to the appeal of Generative Adversarial Networks (GANs), synthetic face images generated with GANs models are difficult to differentiate from genuine human faces and may be used to create counterfeit identities. Additionally, since GANs models do not account for physiological constraints for generating human faces and their impact on human IRISes, distinguishing genuine from synthetic IRISes in the PS scenario becomes extremely difficult. As a result of the lack of large-scale reference IRIS datasets in the PS scenario, we aim at developing a novel dataset to become a standard for Multimedia Forensics (MFs) investigat
&lt;/p&gt;</description></item><item><title>FengWu&#26159;&#19968;&#20010;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20808;&#36827;&#25968;&#25454;&#39537;&#21160;&#30340;&#20840;&#29699;&#20013;&#26399;&#22825;&#27668;&#39044;&#25253;&#31995;&#32479;&#12290;&#23427;&#20174;&#22810;&#27169;&#24577;&#21644;&#22810;&#20219;&#21153;&#30340;&#35282;&#24230;&#19979;&#35299;&#20915;&#20102;&#20013;&#26399;&#39044;&#25253;&#38382;&#39064;&#65292;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#25439;&#22833;&#30340;&#30417;&#30563;&#23398;&#20064;&#65292;&#22312;&#21306;&#22495;&#33258;&#36866;&#24212;&#30340;&#26041;&#24335;&#19979;&#24179;&#34913;&#19981;&#21516;&#39044;&#27979;&#22120;&#30340;&#20248;&#21270;&#12290;&#24341;&#20837;&#22238;&#25918;&#32531;&#20914;&#26426;&#21046;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;FengWu&#20855;&#26377;&#31934;&#30830;&#39044;&#27979;&#22823;&#27668;&#21160;&#21147;&#23398;&#21644;&#26410;&#26469;&#30340;&#38470;&#22320;&#21644;&#22823;&#27668;&#29366;&#24577;&#30340;&#33021;&#21147;&#12290;&#22312;2018&#24180;&#30340;&#38271;&#26399;&#39044;&#25253;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2304.02948</link><description>&lt;p&gt;
FengWu&#65306;&#25512;&#21160;&#25216;&#33021;&#31934;&#28251;&#30340;&#20840;&#29699;&#20013;&#26399;&#22825;&#27668;&#39044;&#25253;&#36229;&#36234;10&#22825;&#30340;&#39046;&#20808;&#12290;(arXiv:2304.02948v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
FengWu: Pushing the Skillful Global Medium-range Weather Forecast beyond 10 Days Lead. (arXiv:2304.02948v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02948
&lt;/p&gt;
&lt;p&gt;
FengWu&#26159;&#19968;&#20010;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20808;&#36827;&#25968;&#25454;&#39537;&#21160;&#30340;&#20840;&#29699;&#20013;&#26399;&#22825;&#27668;&#39044;&#25253;&#31995;&#32479;&#12290;&#23427;&#20174;&#22810;&#27169;&#24577;&#21644;&#22810;&#20219;&#21153;&#30340;&#35282;&#24230;&#19979;&#35299;&#20915;&#20102;&#20013;&#26399;&#39044;&#25253;&#38382;&#39064;&#65292;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#25439;&#22833;&#30340;&#30417;&#30563;&#23398;&#20064;&#65292;&#22312;&#21306;&#22495;&#33258;&#36866;&#24212;&#30340;&#26041;&#24335;&#19979;&#24179;&#34913;&#19981;&#21516;&#39044;&#27979;&#22120;&#30340;&#20248;&#21270;&#12290;&#24341;&#20837;&#22238;&#25918;&#32531;&#20914;&#26426;&#21046;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;FengWu&#20855;&#26377;&#31934;&#30830;&#39044;&#27979;&#22823;&#27668;&#21160;&#21147;&#23398;&#21644;&#26410;&#26469;&#30340;&#38470;&#22320;&#21644;&#22823;&#27668;&#29366;&#24577;&#30340;&#33021;&#21147;&#12290;&#22312;2018&#24180;&#30340;&#38271;&#26399;&#39044;&#25253;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;FengWu&#65292;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20808;&#36827;&#25968;&#25454;&#39537;&#21160;&#30340;&#20840;&#29699;&#20013;&#26399;&#22825;&#27668;&#39044;&#25253;&#31995;&#32479;&#12290;&#19982;&#29616;&#26377;&#30340;&#25968;&#25454;&#39537;&#21160;&#22825;&#27668;&#39044;&#25253;&#26041;&#27861;&#19981;&#21516;&#65292;FengWu&#20174;&#22810;&#27169;&#24577;&#21644;&#22810;&#20219;&#21153;&#30340;&#35282;&#24230;&#35299;&#20915;&#20102;&#20013;&#26399;&#39044;&#25253;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#20307;&#31995;&#32467;&#26500;&#65292;&#37197;&#22791;&#20102;&#27169;&#22411;&#29305;&#23450;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21644;&#36328;&#27169;&#24577;&#34701;&#21512;Transformer&#65292;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#25439;&#22833;&#30340;&#30417;&#30563;&#23398;&#20064;&#65292;&#22312;&#21306;&#22495;&#33258;&#36866;&#24212;&#30340;&#26041;&#24335;&#19979;&#24179;&#34913;&#19981;&#21516;&#39044;&#27979;&#22120;&#30340;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#22238;&#25918;&#32531;&#20914;&#21306;&#26426;&#21046;&#26469;&#25552;&#39640;&#20013;&#26399;&#39044;&#25253;&#24615;&#33021;&#12290;&#22312;&#22522;&#20110;ERA5&#20877;&#20998;&#26512;&#30340;39&#24180;&#25968;&#25454;&#35757;&#32451;&#19979;&#65292;FengWu&#33021;&#22815;&#20934;&#30830;&#22320;&#22797;&#21046;&#22823;&#27668;&#21160;&#21147;&#23398;&#24182;&#22312;0.25{\deg}&#32428;&#24230;-&#32463;&#24230;&#20998;&#36776;&#29575;&#19978;&#39044;&#27979;&#26410;&#26469;&#30340;&#38470;&#22320;&#21644;&#22823;&#27668;&#29366;&#24577;&#12290;&#22522;&#20110;ERA5&#30340;2018&#24180;6&#23567;&#26102;&#38271;&#26399;&#39044;&#25253;&#34920;&#26126;&#65292;FengWu&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present FengWu, an advanced data-driven global medium-range weather forecast system based on Artificial Intelligence (AI). Different from existing data-driven weather forecast methods, FengWu solves the medium-range forecast problem from a multi-modal and multi-task perspective. Specifically, a deep learning architecture equipped with model-specific encoder-decoders and cross-modal fusion Transformer is elaborately designed, which is learned under the supervision of an uncertainty loss to balance the optimization of different predictors in a region-adaptive manner. Besides this, a replay buffer mechanism is introduced to improve medium-range forecast performance. With 39-year data training based on the ERA5 reanalysis, FengWu is able to accurately reproduce the atmospheric dynamics and predict the future land and atmosphere states at 37 vertical levels on a 0.25{\deg} latitude-longitude resolution. Hindcasts of 6-hourly weather in 2018 based on ERA5 demonstrate that FengWu performs 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#23545;&#20845;&#31181;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#27604;&#36739;&#65292;&#20351;&#29992;&#21253;&#21547;14000&#20010;&#26679;&#26412;&#30340;&#26032;&#29616;&#23454;&#19990;&#30028;&#20108;&#20803;&#35010;&#32541;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#26816;&#27979;&#36335;&#38754;&#35010;&#32541;&#30340;&#30446;&#30340;&#12290;&#35757;&#32451;&#30340;&#20845;&#20010;&#27169;&#22411;&#20013;&#26377;&#20116;&#20010;&#30340;&#20934;&#30830;&#29575;&#36229;&#36807;&#20102;97&#65285;&#65292;&#26368;&#39640;&#35760;&#24405;&#30340;&#20934;&#30830;&#29575;&#20026;99.7&#65285;&#12290;&#26368;&#20339;&#27169;&#22411;&#24050;&#32463;&#37096;&#32626;&#22312;&#20113;&#22522;&#30784;&#26550;&#26500;&#19978;&#65292;&#20197;&#20801;&#35768;&#33258;&#21160;&#26816;&#27979;&#30456;&#26426;&#38236;&#22836;&#20013;&#30340;&#35010;&#32541;&#12290;</title><link>http://arxiv.org/abs/2304.02933</link><description>&lt;p&gt;
&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26580;&#24615;&#36335;&#38754;&#35010;&#32541;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Convolutional neural networks for crack detection on flexible road pavements. (arXiv:2304.02933v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02933
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#23545;&#20845;&#31181;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#27604;&#36739;&#65292;&#20351;&#29992;&#21253;&#21547;14000&#20010;&#26679;&#26412;&#30340;&#26032;&#29616;&#23454;&#19990;&#30028;&#20108;&#20803;&#35010;&#32541;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#26816;&#27979;&#36335;&#38754;&#35010;&#32541;&#30340;&#30446;&#30340;&#12290;&#35757;&#32451;&#30340;&#20845;&#20010;&#27169;&#22411;&#20013;&#26377;&#20116;&#20010;&#30340;&#20934;&#30830;&#29575;&#36229;&#36807;&#20102;97&#65285;&#65292;&#26368;&#39640;&#35760;&#24405;&#30340;&#20934;&#30830;&#29575;&#20026;99.7&#65285;&#12290;&#26368;&#20339;&#27169;&#22411;&#24050;&#32463;&#37096;&#32626;&#22312;&#20113;&#22522;&#30784;&#26550;&#26500;&#19978;&#65292;&#20197;&#20801;&#35768;&#33258;&#21160;&#26816;&#27979;&#30456;&#26426;&#38236;&#22836;&#20013;&#30340;&#35010;&#32541;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26580;&#24615;&#36335;&#38754;&#20027;&#35201;&#30001;&#20110;&#36710;&#36742;&#21644;&#19981;&#21033;&#30340;&#29615;&#22659;&#26465;&#20214;&#32780;&#24694;&#21270;&#65292;&#32780;&#35010;&#32541;&#26159;&#26368;&#24120;&#35265;&#30340;&#24694;&#21270;&#26426;&#21046;&#65307;&#20854;&#35843;&#26597;&#36890;&#24120;&#20351;&#29992;&#22269;&#38469;&#23450;&#20041;&#30340;&#20998;&#31867;&#26631;&#20934;&#36827;&#34892;&#25163;&#21160;&#36827;&#34892;&#12290;&#22312;&#21335;&#38750;&#65292;&#24050;&#32463;&#24341;&#20837;&#20102;&#39640;&#28165;&#26224;&#24230;&#35270;&#39057;&#22270;&#20687;&#65292;&#21487;&#20197;&#36827;&#34892;&#26356;&#23433;&#20840;&#30340;&#36947;&#36335;&#35843;&#26597;&#12290;&#20294;&#26159;&#65292;&#35843;&#26597;&#20173;&#28982;&#26159;&#19968;&#39033;&#32321;&#29712;&#30340;&#25163;&#21160;&#36807;&#31243;&#12290;&#33258;&#21160;&#26816;&#27979;&#35832;&#22914;&#35010;&#32541;&#20043;&#31867;&#30340;&#32570;&#38519;&#23558;&#20801;&#35768;&#26356;&#24555;&#22320;&#20998;&#26512;&#36947;&#36335;&#32593;&#32476;&#65292;&#24182;&#28508;&#22312;&#22320;&#20943;&#23569;&#20154;&#20026;&#20559;&#24046;&#21644;&#38169;&#35823;&#12290;&#35813;&#30740;&#31350;&#23545;&#20845;&#31181;&#26368;&#20808;&#36827;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#29992;&#20110;&#35010;&#32541;&#26816;&#27979;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;&#21253;&#21547;14000&#20010;&#26679;&#26412;&#30340;&#26032;&#29616;&#23454;&#19990;&#30028;&#20108;&#20803;&#35010;&#32541;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#12290;&#36824;&#35843;&#26597;&#20102;&#25968;&#25454;&#38598;&#25193;&#20805;&#30340;&#25928;&#26524;&#12290;&#35757;&#32451;&#30340;&#20845;&#20010;&#27169;&#22411;&#20013;&#26377;&#20116;&#20010;&#30340;&#20934;&#30830;&#29575;&#36229;&#36807;&#20102;97&#65285;&#65292;&#26368;&#39640;&#35760;&#24405;&#30340;&#20934;&#30830;&#29575;&#20026;99.7&#65285;&#12290;&#26368;&#20339;&#27169;&#22411;&#37096;&#32626;&#22312;&#20113;&#22522;&#30784;&#26550;&#26500;&#19978;&#65292;&#20197;&#20801;&#35768;&#33258;&#21160;&#26816;&#27979;&#30456;&#26426;&#38236;&#22836;&#20013;&#30340;&#35010;&#32541;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20026;&#36947;&#36335;&#32570;&#38519;&#30340;&#33258;&#21160;&#21270;&#26816;&#27979;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Flexible road pavements deteriorate primarily due to traffic and adverse environmental conditions. Cracking is the most common deterioration mechanism; the surveying thereof is typically conducted manually using internationally defined classification standards. In South Africa, the use of high-definition video images has been introduced, which allows for safer road surveying. However, surveying is still a tedious manual process. Automation of the detection of defects such as cracks would allow for faster analysis of road networks and potentially reduce human bias and error. This study performs a comparison of six state-of-the-art convolutional neural network models for the purpose of crack detection. The models are pretrained on the ImageNet dataset, and fine-tuned using a new real-world binary crack dataset consisting of 14000 samples. The effects of dataset augmentation are also investigated. Of the six models trained, five achieved accuracy above 97%. The highest recorded accuracy w
&lt;/p&gt;</description></item><item><title>&#23545;&#20110;&#32852;&#37030;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65288;FKGE&#65289;&#23384;&#22312;&#30340;&#38544;&#31169;&#23041;&#32961;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#25512;&#29702;&#25915;&#20987;&#65292;&#25104;&#21151;&#25512;&#26029;&#20986;&#21463;&#23475;&#23458;&#25143;&#31471;&#30340;&#30693;&#35782;&#22270;&#35889;&#19977;&#20803;&#32452;&#30340;&#23384;&#22312;&#12290;&#21516;&#26102;&#65292;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#24046;&#20998;&#38544;&#31169;FKGE&#31639;&#27861;DP-Flames&#65292;&#25104;&#21151;&#20943;&#36731;&#20102;&#25512;&#26029;&#20449;&#24687;&#30340;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2304.02932</link><description>&lt;p&gt;
&#22522;&#20110;&#32852;&#37030;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#38544;&#31169;&#23041;&#32961;&#30340;&#37327;&#21270;&#21644;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Quantifying and Defending against Privacy Threats on Federated Knowledge Graph Embedding. (arXiv:2304.02932v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02932
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#32852;&#37030;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65288;FKGE&#65289;&#23384;&#22312;&#30340;&#38544;&#31169;&#23041;&#32961;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#25512;&#29702;&#25915;&#20987;&#65292;&#25104;&#21151;&#25512;&#26029;&#20986;&#21463;&#23475;&#23458;&#25143;&#31471;&#30340;&#30693;&#35782;&#22270;&#35889;&#19977;&#20803;&#32452;&#30340;&#23384;&#22312;&#12290;&#21516;&#26102;&#65292;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#24046;&#20998;&#38544;&#31169;FKGE&#31639;&#27861;DP-Flames&#65292;&#25104;&#21151;&#20943;&#36731;&#20102;&#25512;&#26029;&#20449;&#24687;&#30340;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#26159;&#20174;&#30693;&#35782;&#22270;&#35889;&#20013;&#25552;&#21462;&#34920;&#36798;&#24615;&#34920;&#31034;&#20197;&#20419;&#36827;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#22522;&#30784;&#25216;&#26415;&#12290;&#26032;&#20852;&#30340;&#32852;&#37030;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65288;FKGE&#65289;&#20174;&#23458;&#25143;&#31471;&#25152;&#25345;&#26377;&#30340;&#20998;&#24067;&#24335;&#30693;&#35782;&#22270;&#35889;&#20013;&#21327;&#21516;&#35757;&#32451;&#65292;&#21516;&#26102;&#36991;&#20813;&#20132;&#25442;&#23458;&#25143;&#31471;&#30340;&#25935;&#24863;&#21407;&#22987;&#30693;&#35782;&#22270;&#35889;&#12290;&#28982;&#32780;&#65292;FKGE&#20173;&#28982;&#21487;&#33021;&#38754;&#20020;&#38544;&#31169;&#23041;&#32961;&#65292;&#36825;&#22312;&#20854;&#20182;&#32852;&#37030;&#27169;&#22411;&#35757;&#32451;&#65288;&#20363;&#22914;&#31070;&#32463;&#32593;&#32476;&#65289;&#20013;&#24050;&#34987;&#35777;&#26126;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#25915;&#20987;&#21644;&#38450;&#24481;&#30340;&#35282;&#24230;&#65292;&#23545;FKGE&#30340;&#38544;&#31169;&#23041;&#32961;&#36827;&#34892;&#20102;&#31532;&#19968;&#27425;&#20840;&#38754;&#30740;&#31350;&#12290;&#23601;&#25915;&#20987;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#25512;&#29702;&#25915;&#20987;&#26469;&#37327;&#21270;&#38544;&#31169;&#23041;&#32961;&#65292;&#25104;&#21151;&#25512;&#26029;&#20986;&#21463;&#23475;&#23458;&#25143;&#31471;&#30340;&#30693;&#35782;&#22270;&#35889;&#19977;&#20803;&#32452;&#30340;&#23384;&#22312;&#65292;&#20174;&#32780;&#25581;&#31034;&#20102;&#23454;&#36136;&#24615;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;&#38024;&#23545;&#38450;&#24481;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DP-Flames&#65292;&#19968;&#31181;&#26032;&#22411;&#30340;&#24046;&#20998;&#38544;&#31169;FKGE&#31639;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#32039;&#23494;&#30340;&#38544;&#31169;&#39044;&#31639;&#20943;&#36731;&#34987;&#25512;&#26029;&#20449;&#24687;&#30340;&#25439;&#22833;&#12290;DP-Flames&#37319;&#29992;&#22522;&#20110;&#25200;&#21160;&#30340;&#26041;&#27861;&#35774;&#35745;&#65292;&#24182;&#36827;&#19968;&#27493;&#34701;&#21512;&#20102;&#22270;&#23884;&#20837;&#20013;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#19982;&#38750;&#38544;&#31169;&#22522;&#32447;&#30456;&#27604;&#65292;&#20960;&#20046;&#27809;&#26377;&#25439;&#22833;&#65292;&#36825;&#25552;&#39640;&#20102;&#26377;&#29992;&#24615;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25915;&#20987;/&#38450;&#24481;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;DP-Flames&#22312;&#36807;&#28388;&#38544;&#31169;&#25439;&#22833;&#21644;&#20445;&#25345;&#23454;&#29992;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Embedding (KGE) is a fundamental technique that extracts expressive representation from knowledge graph (KG) to facilitate diverse downstream tasks. The emerging federated KGE (FKGE) collaboratively trains from distributed KGs held among clients while avoiding exchanging clients' sensitive raw KGs, which can still suffer from privacy threats as evidenced in other federated model trainings (e.g., neural networks). However, quantifying and defending against such privacy threats remain unexplored for FKGE which possesses unique properties not shared by previously studied models. In this paper, we conduct the first holistic study of the privacy threat on FKGE from both attack and defense perspectives. For the attack, we quantify the privacy threat by proposing three new inference attacks, which reveal substantial privacy risk by successfully inferring the existence of the KG triple from victim clients. For the defense, we propose DP-Flames, a novel differentially private FK
&lt;/p&gt;</description></item><item><title>&#29289;&#29702;&#20154;&#24037;&#26234;&#33021;&#30340;&#27835;&#29702;&#23545;&#20854;&#36127;&#36131;&#20219;&#30340;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2304.02924</link><description>&lt;p&gt;
&#29289;&#29702;&#20154;&#24037;&#26234;&#33021;&#30340;&#27835;&#29702;
&lt;/p&gt;
&lt;p&gt;
The Governance of Physical Artificial Intelligence. (arXiv:2304.02924v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02924
&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20154;&#24037;&#26234;&#33021;&#30340;&#27835;&#29702;&#23545;&#20854;&#36127;&#36131;&#20219;&#30340;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20154;&#24037;&#26234;&#33021;&#21487;&#33021;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#26368;&#37325;&#35201;&#30340;&#25361;&#25112;&#20043;&#19968;&#12290;&#29289;&#29702;&#20154;&#24037;&#26234;&#33021;&#30340;&#27835;&#29702;&#23558;&#23450;&#20041;&#23427;&#22312;&#31038;&#20250;&#20013;&#36127;&#36131;&#20219;&#30340;&#26234;&#33021;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physical artificial intelligence can prove to be one of the most important challenges of the artificial intelligence. The governance of physical artificial intelligence would define its responsible intelligent application in the society.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29289;&#20307;&#20013;&#24515;&#30340;&#35821;&#35328;&#26465;&#20214;&#25918;&#32622;&#25512;&#29702;&#26694;&#26550;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#35757;&#32451;&#25968;&#25454;&#38656;&#27714;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#24182;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798; 97.75% &#30340;&#25918;&#32622;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.02893</link><description>&lt;p&gt;
&#22522;&#20110;&#29289;&#20307;&#20013;&#24515;&#30340;&#35821;&#35328;&#26465;&#20214;&#25918;&#32622;&#25512;&#29702;&#65306;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Object-centric Inference for Language Conditioned Placement: A Foundation Model based Approach. (arXiv:2304.02893v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02893
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29289;&#20307;&#20013;&#24515;&#30340;&#35821;&#35328;&#26465;&#20214;&#25918;&#32622;&#25512;&#29702;&#26694;&#26550;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#35757;&#32451;&#25968;&#25454;&#38656;&#27714;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#24182;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798; 97.75% &#30340;&#25918;&#32622;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#20110;&#35821;&#35328;&#26465;&#20214;&#19979;&#30340;&#29289;&#20307;&#25918;&#32622;&#20219;&#21153;&#65292;&#21363;&#26426;&#22120;&#20154;&#24212;&#35813;&#29983;&#25104;&#28385;&#36275;&#35821;&#35328;&#35828;&#26126;&#20013;&#25152;&#26377;&#31354;&#38388;&#20851;&#31995;&#38480;&#21046;&#30340;&#25918;&#32622;&#12290;&#20197;&#24448;&#22522;&#20110;&#35268;&#21017;&#30340;&#35821;&#35328;&#35299;&#26512;&#25110;&#22330;&#26223;&#20013;&#24515;&#30340;&#35270;&#35273;&#34920;&#31034;&#23545;&#25351;&#20196;&#24418;&#24335;&#21644;&#21442;&#32771;&#23545;&#35937;&#26377;&#38480;&#21046;&#25110;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29289;&#20307;&#20013;&#24515;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#26469;&#30830;&#23450;&#21442;&#32771;&#23545;&#35937;&#21644;&#31354;&#38388;&#20851;&#31995;&#20197;&#36827;&#34892;&#25918;&#32622;&#65292;&#35813;&#26041;&#27861;&#26356;&#21152;&#26679;&#26412;&#39640;&#25928;&#21644;&#21487;&#25512;&#24191;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21482;&#38656; ~0.26M &#21487;&#35757;&#32451;&#21442;&#25968;&#23601;&#33021;&#36798;&#21040; 97.75% &#30340;&#25918;&#32622;&#25104;&#21151;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#26410;&#35265;&#36807;&#30340;&#29289;&#20307;&#21644;&#35828;&#26126;&#37117;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#24403;&#21482;&#20351;&#29992; 25% &#30340;&#35757;&#32451;&#25968;&#25454;&#26102;&#65292;&#25105;&#20204;&#20173;&#28982;&#32988;&#36807;&#39030;&#32423;&#31454;&#20105;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We focus on the task of language-conditioned object placement, in which a robot should generate placements that satisfy all the spatial relational constraints in language instructions. Previous works based on rule-based language parsing or scene-centric visual representation have restrictions on the form of instructions and reference objects or require large amounts of training data. We propose an object-centric framework that leverages foundation models to ground the reference objects and spatial relations for placement, which is more sample efficient and generalizable. Experiments indicate that our model can achieve a 97.75% success rate of placement with only ~0.26M trainable parameters. Besides, our method generalizes better to both unseen objects and instructions. Moreover, with only 25% training data, we still outperform the top competing approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedCNI&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21253;&#25324;&#40065;&#26834;&#30340;&#20840;&#23616;&#32858;&#21512;&#22120;&#21644;&#25239;&#22122;&#23616;&#37096;&#27714;&#35299;&#22120;&#65292;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#22312;&#23567;&#22411;&#26412;&#22320;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#26631;&#31614;&#22122;&#22768;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.02892</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#22024;&#26434;&#21644;&#24322;&#36136;&#23458;&#25143;&#31471;&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;&#35880;&#24910;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning Cautiously in Federated Learning with Noisy and Heterogeneous Clients. (arXiv:2304.02892v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedCNI&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21253;&#25324;&#40065;&#26834;&#30340;&#20840;&#23616;&#32858;&#21512;&#22120;&#21644;&#25239;&#22122;&#23616;&#37096;&#27714;&#35299;&#22120;&#65292;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#22312;&#23567;&#22411;&#26412;&#22320;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#26631;&#31614;&#22122;&#22768;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#30340;&#26694;&#26550;&#65292;&#21487;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#21327;&#20316;&#35757;&#32451;&#12290;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#23458;&#25143;&#31471;&#21487;&#33021;&#20855;&#26377;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#65288;&#26412;&#22320;&#31867;&#21035;&#19981;&#24179;&#34913;&#65289;&#21644;&#20302;&#36136;&#37327;&#30340;&#27880;&#37322;&#65288;&#26631;&#31614;&#22024;&#26434;&#65289;&#12290;FL &#30340;&#23567;&#22411;&#26412;&#22320;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#26631;&#31614;&#22122;&#22768;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#20849;&#23384;&#22312;&#65292;&#20351;&#20256;&#32479; FL &#26041;&#27861;&#21644;&#22024;&#26434;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;&#22343;&#26080;&#25928;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; FedCNI&#65292;&#23427;&#19981;&#20351;&#29992;&#39069;&#22806;&#30340;&#24178;&#20928;&#20195;&#29702;&#25968;&#25454;&#38598;&#12290;&#23427;&#21253;&#25324;&#19968;&#20010;&#40065;&#26834;&#30340;&#20840;&#23616;&#32858;&#21512;&#22120;&#21644;&#19968;&#20010;&#25239;&#22122;&#23616;&#37096;&#27714;&#35299;&#22120;&#12290;&#23545;&#20110;&#23616;&#37096;&#27714;&#35299;&#22120;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26356;&#31283;&#20581;&#30340;&#26679;&#26412;&#22024;&#26434;&#26816;&#27979;&#22120;&#26469;&#21306;&#20998;&#22024;&#26434;&#26679;&#26412;&#12290;&#20026;&#20102;&#20943;&#23569;&#22122;&#22768;&#26679;&#26412;&#24102;&#26469;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#35838;&#31243;&#20266;&#26631;&#31614;&#26041;&#27861;&#21644;&#19968;&#20010;&#21435;&#22122; Mixup &#35757;&#32451;&#31574;&#30053;&#12290;&#23545;&#20110;&#20840;&#23616;&#32858;&#21512;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#19981;&#21516;&#23398;&#20064;&#38454;&#27573;&#37327;&#36523;&#23450;&#21046;&#30340;&#20999;&#25442;&#21152;&#26435;&#32858;&#21512;&#26041;&#27861;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22024;&#26434;&#26631;&#31614;&#19979;&#30340;&#32852;&#37030;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a distributed framework for collaboratively training with privacy guarantees. In real-world scenarios, clients may have Non-IID data (local class imbalance) with poor annotation quality (label noise). The co-existence of label noise and class imbalance in FL's small local datasets renders conventional FL methods and noisy-label learning methods both ineffective. To address the challenges, we propose FedCNI without using an additional clean proxy dataset. It includes a noise-resilient local solver and a robust global aggregator. For the local solver, we design a more robust prototypical noise detector to distinguish noisy samples. Further to reduce the negative impact brought by the noisy samples, we devise a curriculum pseudo labeling method and a denoise Mixup training strategy. For the global aggregator, we propose a switching re-weighted aggregation method tailored to different learning periods. Extensive experiments demonstrate our method can substantiall
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#27861;&#35821;&#20020;&#24202;&#25991;&#26412;&#33258;&#21160;&#20851;&#32852;ICD&#20195;&#30721;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#26032;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22810;&#26631;&#31614;&#20998;&#31867;&#25216;&#26415;&#30340;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#25216;&#26415;&#32467;&#26524;&#65292;F1&#20998;&#25968;&#25552;&#39640;&#20102;55&#65285;&#20197;&#19978;&#12290;</title><link>http://arxiv.org/abs/2304.02886</link><description>&lt;p&gt;
&#33258;&#21160;ICD-10&#32534;&#30721;&#20851;&#32852;&#65306;&#23545;&#27861;&#35821;&#20020;&#24202;&#25991;&#26412;&#30340;&#25361;&#25112;&#24615;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Automatic ICD-10 Code Association: A Challenging Task on French Clinical Texts. (arXiv:2304.02886v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#27861;&#35821;&#20020;&#24202;&#25991;&#26412;&#33258;&#21160;&#20851;&#32852;ICD&#20195;&#30721;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#26032;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22810;&#26631;&#31614;&#20998;&#31867;&#25216;&#26415;&#30340;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#25216;&#26415;&#32467;&#26524;&#65292;F1&#20998;&#25968;&#25552;&#39640;&#20102;55&#65285;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#30740;&#31350;&#20013;&#65292;&#33258;&#21160;&#23558;ICD&#20195;&#30721;&#19982;&#30005;&#23376;&#20581;&#24247;&#25968;&#25454;&#20851;&#32852;&#26159;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#26368;&#36817;&#20960;&#24180;&#65292;&#38543;&#30528;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24471;&#21040;&#20102;&#26174;&#33879;&#30340;&#21457;&#23637;&#65292;&#20027;&#35201;&#24212;&#29992;&#20110;&#33521;&#25991;&#35821;&#35328;&#12290;&#26412;&#25991;&#38024;&#23545;&#27861;&#35821;&#25991;&#26412;&#33258;&#21160;&#20851;&#32852;ICD&#20195;&#30721;&#30340;&#38382;&#39064;&#65292;&#23581;&#35797;&#20351;&#29992;&#22810;&#31181;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#22788;&#29702;&#22823;&#37327;&#30340;&#36755;&#20837;&#26631;&#35760;&#21644;&#38656;&#35201;&#29468;&#27979;&#30340;&#26631;&#31614;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#65292;&#23558;&#26368;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22810;&#26631;&#31614;&#20998;&#31867;&#25216;&#26415;&#24212;&#29992;&#20110;ICD-10&#32534;&#30721;&#20851;&#32852;&#26041;&#38754;&#12290;&#23545;&#20110;&#27861;&#35821;&#30340;&#20020;&#24202;&#25968;&#25454;&#38598;&#65292;&#20844;&#27491;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;F1&#20998;&#25968;&#27604;&#29616;&#26377;&#25216;&#26415;&#32467;&#26524;&#25552;&#39640;&#20102;55&#65285;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatically associating ICD codes with electronic health data is a well-known NLP task in medical research. NLP has evolved significantly in recent years with the emergence of pre-trained language models based on Transformers architecture, mainly in the English language. This paper adapts these models to automatically associate the ICD codes. Several neural network architectures have been experimented with to address the challenges of dealing with a large set of both input tokens and labels to be guessed. In this paper, we propose a model that combines the latest advances in NLP and multi-label classification for ICD-10 code association. Fair experiments on a Clinical dataset in the French language show that our approach increases the $F_1$-score metric by more than 55\% compared to state-of-the-art results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29609;&#25991;&#23383;&#28216;&#25103;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20854;&#34920;&#29616;&#26377;&#31454;&#20105;&#21147;&#65292;&#20294;&#20173;&#28982;&#32570;&#20047;&#26234;&#33021;&#65292;&#26377;&#24453;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2304.02868</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#33021;&#22815;&#24456;&#22909;&#22320;&#29609;&#25991;&#23383;&#28216;&#25103;&#65311;&#29616;&#29366;&#21644;&#26410;&#26469;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Play Text Games Well? Current State-of-the-Art and Open Questions. (arXiv:2304.02868v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02868
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29609;&#25991;&#23383;&#28216;&#25103;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20854;&#34920;&#29616;&#26377;&#31454;&#20105;&#21147;&#65292;&#20294;&#20173;&#28982;&#32570;&#20047;&#26234;&#33021;&#65292;&#26377;&#24453;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35832;&#22914;ChatGPT&#21644;GPT-4&#20043;&#31867;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#23427;&#20204;&#19982;&#20154;&#31867;&#29992;&#25143;&#36890;&#20449;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;&#26412;&#25216;&#26415;&#25253;&#21578;&#26088;&#22312;&#35843;&#26597;&#23427;&#20204;&#22312;&#29609;&#25991;&#23383;&#28216;&#25103;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#36825;&#35201;&#27714;&#29609;&#23478;&#36890;&#36807;&#19982;&#28216;&#25103;&#19990;&#30028;&#30340;&#23545;&#35805;&#26469;&#29702;&#35299;&#29615;&#22659;&#24182;&#23545;&#24773;&#20917;&#20570;&#20986;&#21453;&#24212;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#25152;&#26377;&#29616;&#26377;&#31995;&#32479;&#30456;&#27604;&#65292;ChatGPT&#34920;&#29616;&#20986;&#26377;&#31454;&#20105;&#21147;&#65292;&#20294;&#20173;&#28982;&#34920;&#29616;&#20986;&#36739;&#20302;&#30340;&#26234;&#33021;&#27700;&#24179;&#12290;&#30830;&#20999;&#22320;&#35828;&#65292;ChatGPT&#26080;&#27861;&#36890;&#36807;&#29609;&#28216;&#25103;&#25110;&#38405;&#35835;&#28216;&#25103;&#25163;&#20876;&#26469;&#26500;&#24314;&#19990;&#30028;&#27169;&#22411;&#65307;&#23427;&#21487;&#33021;&#26080;&#27861;&#21033;&#29992;&#23427;&#24050;&#32463;&#25317;&#26377;&#30340;&#19990;&#30028;&#30693;&#35782;&#65307;&#23427;&#26080;&#27861;&#25512;&#26029;&#20986;&#38543;&#30528;&#28216;&#25103;&#36827;&#23637;&#30340;&#27599;&#19968;&#27493;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#22312;&#20154;&#24037;&#26234;&#33021;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20132;&#21449;&#39046;&#22495;&#24320;&#21551;&#20102;&#26032;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) such as ChatGPT and GPT-4 have recently demonstrated their remarkable abilities of communicating with human users. In this technical report, we take an initiative to investigate their capacities of playing text games, in which a player has to understand the environment and respond to situations by having dialogues with the game world. Our experiments show that ChatGPT performs competitively compared to all the existing systems but still exhibits a low level of intelligence. Precisely, ChatGPT can not construct the world model by playing the game or even reading the game manual; it may fail to leverage the world knowledge that it already has; it cannot infer the goal of each step as the game progresses. Our results open up new research questions at the intersection of artificial intelligence, machine learning, and natural language processing.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38598;&#25104;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#30340;&#24212;&#29992;&#65292;&#38024;&#23545;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#36890;&#36807;&#35745;&#31639;&#35780;&#20272;&#65292;&#25214;&#21040;&#20102;&#26368;&#26377;&#25928;&#30340;&#32452;&#21512;&#12290;</title><link>http://arxiv.org/abs/2304.02858</link><description>&lt;p&gt;
&#38754;&#21521;&#31867;&#21035;&#19981;&#22343;&#38382;&#39064;&#30340;&#38598;&#25104;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#27169;&#22411;&#32508;&#36848;&#65306;&#32452;&#21512;&#12289;&#23454;&#29616;&#21644;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A review of ensemble learning and data augmentation models for class imbalanced problems: combination, implementation and evaluation. (arXiv:2304.02858v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38598;&#25104;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#30340;&#24212;&#29992;&#65292;&#38024;&#23545;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#36890;&#36807;&#35745;&#31639;&#35780;&#20272;&#65292;&#25214;&#21040;&#20102;&#26368;&#26377;&#25928;&#30340;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#65288;CI&#65289;&#26159;&#25351;&#23646;&#20110;&#19968;&#20010;&#31867;&#30340;&#35266;&#27979;&#20540;&#25968;&#37327;&#20302;&#20110;&#20854;&#20182;&#31867;&#30340;&#25968;&#37327;&#12290;&#38598;&#25104;&#23398;&#20064;&#32467;&#21512;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#37324;&#65292;&#19968;&#20123;&#31574;&#30053;&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#22686;&#24378;&#38598;&#25104;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21516;&#26102;&#36824;&#24320;&#21457;&#20102;&#19968;&#20123;&#26032;&#26041;&#27861;&#65292;&#22914;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#12290;&#26412;&#25991;&#23545;&#29992;&#20110;&#35299;&#20915;&#22522;&#20934;CI&#38382;&#39064;&#30340;&#25968;&#25454;&#22686;&#24378;&#21644;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#35745;&#31639;&#35780;&#20272;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;CI&#38382;&#39064;&#30340;10&#20010;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#21644;10&#20010;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35782;&#21035;&#25552;&#39640;&#20998;&#31867;&#25928;&#26524;&#26368;&#26377;&#25928;&#30340;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class imbalance (CI) in classification problems arises when the number of observations belonging to one class is lower than the other classes. Ensemble learning that combines multiple models to obtain a robust model has been prominently used with data augmentation methods to address class imbalance problems. In the last decade, a number of strategies have been added to enhance ensemble learning and data augmentation methods, along with new methods such as generative adversarial networks (GANs). A combination of these has been applied in many studies, but the true rank of different combinations would require a computational review. In this paper, we present a computational review to evaluate data augmentation and ensemble learning methods used to address prominent benchmark CI problems. We propose a general framework that evaluates 10 data augmentation and 10 ensemble learning methods for CI problems. Our objective was to identify the most effective combination for improving classificat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#21483;&#20570;Robustmix&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#32593;&#32476;&#20197;&#20302;&#39057;&#31354;&#38388;&#29305;&#24449;&#36827;&#34892;&#20998;&#31867;&#26469;&#25552;&#39640;&#28145;&#24230;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#65292;&#22312;Imagenet-C&#21644;Stylized Imagenet&#31561;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20248;&#29366;&#24577;&#24179;&#22343;&#23792;&#20540;&#35823;&#24046;&#65288;mCE&#65289;&#65292;&#22312;&#36991;&#20813;&#35745;&#31639;&#24320;&#38144;&#21644;&#20808;&#39564;&#30693;&#35782;&#30340;&#22823;&#37327;&#22270;&#20687;&#21464;&#25442;&#30340;&#21516;&#26102;&#23545;&#27169;&#22411;&#26550;&#26500;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#26368;&#26032;&#36827;&#23637;&#25552;&#20379;&#20102;&#34917;&#20805;&#12290;</title><link>http://arxiv.org/abs/2304.02847</link><description>&lt;p&gt;
Robustmix&#65306;&#36890;&#36807;&#27491;&#21017;&#21270;&#28145;&#24230;&#32593;&#32476;&#30340;&#39057;&#29575;&#20559;&#24046;&#26469;&#25552;&#39640;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Robustmix: Improving Robustness by Regularizing the Frequency Bias of Deep Nets. (arXiv:2304.02847v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02847
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#21483;&#20570;Robustmix&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#32593;&#32476;&#20197;&#20302;&#39057;&#31354;&#38388;&#29305;&#24449;&#36827;&#34892;&#20998;&#31867;&#26469;&#25552;&#39640;&#28145;&#24230;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#65292;&#22312;Imagenet-C&#21644;Stylized Imagenet&#31561;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20248;&#29366;&#24577;&#24179;&#22343;&#23792;&#20540;&#35823;&#24046;&#65288;mCE&#65289;&#65292;&#22312;&#36991;&#20813;&#35745;&#31639;&#24320;&#38144;&#21644;&#20808;&#39564;&#30693;&#35782;&#30340;&#22823;&#37327;&#22270;&#20687;&#21464;&#25442;&#30340;&#21516;&#26102;&#23545;&#27169;&#22411;&#26550;&#26500;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#26368;&#26032;&#36827;&#23637;&#25552;&#20379;&#20102;&#34917;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#32593;&#32476;&#22312;&#19968;&#31995;&#21015;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#23545;&#20110;&#23545;&#20154;&#31867;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#30340;&#25200;&#21160;&#20173;&#28982;&#24456;&#25935;&#24863;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Robustmix&#30340;Mixup&#26032;&#25193;&#23637;&#65292;&#35813;&#25193;&#23637;&#36890;&#36807;&#27491;&#21017;&#21270;&#32593;&#32476;&#20197;&#22522;&#20110;&#20302;&#39057;&#31354;&#38388;&#29305;&#24449;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#31181;&#31867;&#22411;&#30340;&#27491;&#21017;&#21270;&#25913;&#21892;&#20102;&#22312;&#19968;&#31995;&#21015;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#40065;&#26834;&#24615;&#65292;&#20363;&#22914;Imagenet-C&#21644;Stylized Imagenet&#12290;&#23427;&#20960;&#20046;&#27809;&#26377;&#35745;&#31639;&#24320;&#38144;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#30340;&#22823;&#37327;&#22270;&#20687;&#21464;&#25442;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#31181;&#26041;&#27861;&#36827;&#19968;&#27493;&#34917;&#20805;&#20102;&#27169;&#22411;&#26550;&#26500;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20351;&#29992;EfficientNet-B8&#27169;&#22411;&#21644;RandAugment&#36798;&#21040;&#20102;44.8&#30340;&#26368;&#26032;&#29366;&#24577;&#24179;&#22343;&#23792;&#20540;&#35823;&#24046;&#65288;mCE&#65289;&#65292;&#30456;&#27604;&#22522;&#32447;&#38477;&#20302;&#20102;16&#20010;mCE&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep networks have achieved impressive results on a range of well-curated benchmark datasets. Surprisingly, their performance remains sensitive to perturbations that have little effect on human performance. In this work, we propose a novel extension of Mixup called Robustmix that regularizes networks to classify based on lower-frequency spatial features. We show that this type of regularization improves robustness on a range of benchmarks such as Imagenet-C and Stylized Imagenet. It adds little computational overhead and, furthermore, does not require a priori knowledge of a large set of image transformations. We find that this approach further complements recent advances in model architecture and data augmentation, attaining a state-of-the-art mCE of 44.8 with an EfficientNet-B8 model and RandAugment, which is a reduction of 16 mCE compared to the baseline.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RNAS&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#24179;&#34913;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#29983;&#25104;&#39640;&#36136;&#37327;&#26550;&#26500;&#65292;&#20351;&#29992;&#22122;&#22768;&#26679;&#26412;&#20943;&#23569;&#25628;&#32034;&#25104;&#26412;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#23545;&#25239;&#25915;&#20987;&#20013;&#22343;&#36798;&#21040;&#26368;&#20808;&#36827;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2304.02845</link><description>&lt;p&gt;
&#22362;&#38887;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Robust Neural Architecture Search. (arXiv:2304.02845v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02845
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RNAS&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#24179;&#34913;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#29983;&#25104;&#39640;&#36136;&#37327;&#26550;&#26500;&#65292;&#20351;&#29992;&#22122;&#22768;&#26679;&#26412;&#20943;&#23569;&#25628;&#32034;&#25104;&#26412;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#23545;&#25239;&#25915;&#20987;&#20013;&#22343;&#36798;&#21040;&#26368;&#20808;&#36827;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;NAS&#29983;&#25104;&#30340;&#27169;&#22411;&#24448;&#24448;&#26356;&#23481;&#26131;&#21463;&#21040;&#21508;&#31181;&#24694;&#24847;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#35768;&#22810;&#24378;&#20581;&#30340;NAS&#26041;&#27861;&#21033;&#29992;&#23545;&#25239;&#35757;&#32451;&#26469;&#22686;&#24378;NAS&#29983;&#25104;&#30340;&#27169;&#22411;&#30340;&#24378;&#20581;&#24615;&#65292;&#20294;&#26159;&#23427;&#20204;&#24573;&#30053;&#20102;NAS&#29983;&#25104;&#30340;&#27169;&#22411;&#30340;&#26412;&#36136;&#20934;&#30830;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;NAS&#26041;&#27861;&#65292;&#21517;&#20026;Robust Neural Architecture Search&#65288;RNAS&#65289;&#12290;&#20026;&#20102;&#35774;&#35745;&#20986;&#19968;&#20010;&#27491;&#21017;&#21270;&#39033;&#26469;&#24179;&#34913;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;RNAS&#29983;&#25104;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#21644;&#33391;&#22909;&#40065;&#26834;&#24615;&#30340;&#26550;&#26500;&#12290;&#20026;&#20102;&#20943;&#23569;&#25628;&#32034;&#25104;&#26412;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22122;&#22768;&#26679;&#26412;&#32780;&#19981;&#26159;&#23545;&#25239;&#24615;&#26679;&#26412;&#20316;&#20026;&#25628;&#32034;&#26550;&#26500;&#30340;&#36755;&#20837;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;RNAS&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#23545;&#25239;&#25915;&#20987;&#26041;&#38754;&#22343;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#65288;SOTA&#65289;&#30340;&#24615;&#33021;&#65292;&#36825;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;RNAS&#22312;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#20043;&#38388;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Architectures Search (NAS) becomes more and more popular over these years. However, NAS-generated models tends to suffer greater vulnerability to various malicious attacks. Lots of robust NAS methods leverage adversarial training to enhance the robustness of NAS-generated models, however, they neglected the nature accuracy of NAS-generated models. In our paper, we propose a novel NAS method, Robust Neural Architecture Search (RNAS). To design a regularization term to balance accuracy and robustness, RNAS generates architectures with both high accuracy and good robustness. To reduce search cost, we further propose to use noise examples instead adversarial examples as input to search architectures. Extensive experiments show that RNAS achieves state-of-the-art (SOTA) performance on both image classification and adversarial attacks, which illustrates the proposed RNAS achieves a good tradeoff between robustness and accuracy.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#29992;&#20110;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#30340;&#29256;&#26435;&#21033;&#30410;&#65292;&#24182;&#20998;&#26512;&#20102;&#29616;&#20195;&#29983;&#25104;&#24335;&#20889;&#20316;&#24037;&#20855;&#22312;&#29256;&#26435;&#26041;&#38754;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#21644;&#38556;&#30861;&#12290;</title><link>http://arxiv.org/abs/2304.02839</link><description>&lt;p&gt;
&#36825;&#26159;&#32763;&#35793;&#21518;&#30340;&#35770;&#25991;&#26631;&#39064;&#65306;&#35841;&#26159;&#25991;&#26412;&#30340;&#20027;&#20154;&#65311;&#25506;&#32034; BigCode&#12289;&#30693;&#35782;&#20135;&#26435;&#21644;&#20262;&#29702;&#36947;&#24503;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whose Text Is It Anyway? Exploring BigCode, Intellectual Property, and Ethics. (arXiv:2304.02839v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02839
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#29992;&#20110;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#30340;&#29256;&#26435;&#21033;&#30410;&#65292;&#24182;&#20998;&#26512;&#20102;&#29616;&#20195;&#29983;&#25104;&#24335;&#20889;&#20316;&#24037;&#20855;&#22312;&#29256;&#26435;&#26041;&#38754;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#21644;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#25110;&#29983;&#25104;&#30340;&#20889;&#20316;&#24037;&#20855;&#20381;&#36182;&#20110;&#35782;&#21035;&#12289;&#27010;&#25324;&#12289;&#32763;&#35793;&#21644;&#39044;&#27979;&#20869;&#23481;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#29992;&#20110;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#30340;&#29256;&#26435;&#21033;&#30410;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#22914;&#20309;&#35299;&#20915;&#22522;&#20110;&#24320;&#25918;&#25968;&#25454;&#38598;&#30340;LLMs&#35268;&#36991;&#20351;&#29992;&#25968;&#25454;&#30340;&#29256;&#26435;&#21033;&#30410;&#38382;&#39064;&#65311;&#25105;&#20204;&#20174;&#23450;&#20041;&#36719;&#20214;&#29256;&#26435;&#24182;&#36861;&#28335;&#20854;&#21382;&#21490;&#24320;&#22987;&#12290;&#25105;&#20204;&#20197;GitHub Copilot&#20316;&#20026;&#25361;&#25112;&#36719;&#20214;&#29256;&#26435;&#30340;&#29616;&#20195;&#26696;&#20363;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#32467;&#35770;&#27010;&#36848;&#20102;&#29983;&#25104;&#24335;&#20889;&#20316;&#21161;&#25163;&#20026;&#29256;&#26435;&#25152;&#20135;&#29983;&#30340;&#38556;&#30861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20221;&#23454;&#29992;&#30340;&#36335;&#32447;&#22270;&#65292;&#20379;&#24320;&#21457;&#20154;&#21592;&#12289;&#36719;&#20214;&#27861;&#24459;&#19987;&#23478;&#21644;&#26222;&#36890;&#29992;&#25143;&#22312;&#26234;&#33021;LLM&#39537;&#21160;&#30340;&#20889;&#20316;&#24037;&#20855;&#30340;&#32972;&#26223;&#19979;&#32771;&#34385;&#29256;&#26435;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligent or generative writing tools rely on large language models that recognize, summarize, translate, and predict content. This position paper probes the copyright interests of open data sets used to train large language models (LLMs). Our paper asks, how do LLMs trained on open data sets circumvent the copyright interests of the used data? We start by defining software copyright and tracing its history. We rely on GitHub Copilot as a modern case study challenging software copyright. Our conclusion outlines obstacles that generative writing assistants create for copyright, and offers a practical road map for copyright analysis for developers, software law experts, and general users to consider in the context of intelligent LLM-powered writing tools.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#26469;&#28304;&#22270;&#21644;Transformer&#30340;&#39640;&#32423;&#25345;&#20037;&#24615;&#23041;&#32961;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;Transformer&#30340;&#33258;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#25552;&#21462;&#31995;&#32479;&#29366;&#24577;&#30340;&#38271;&#26399;&#19978;&#19979;&#25991;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#26469;&#28304;&#20998;&#26512;&#23454;&#29616;&#23545;&#38271;&#26399;&#36816;&#34892;&#31995;&#32479;&#30340;&#27010;&#25324;&#65292;&#20197;&#26816;&#27979;&#32531;&#24930;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2304.02838</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#21644;&#26469;&#28304;&#22270;&#30340;&#39640;&#32423;&#25345;&#20037;&#24615;&#23041;&#32961;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TBDetector:Transformer-Based Detector for Advanced Persistent Threats with Provenance Graph. (arXiv:2304.02838v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#26469;&#28304;&#22270;&#21644;Transformer&#30340;&#39640;&#32423;&#25345;&#20037;&#24615;&#23041;&#32961;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;Transformer&#30340;&#33258;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#25552;&#21462;&#31995;&#32479;&#29366;&#24577;&#30340;&#38271;&#26399;&#19978;&#19979;&#25991;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#26469;&#28304;&#20998;&#26512;&#23454;&#29616;&#23545;&#38271;&#26399;&#36816;&#34892;&#31995;&#32479;&#30340;&#27010;&#25324;&#65292;&#20197;&#26816;&#27979;&#32531;&#24930;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#39640;&#32423;&#25345;&#20037;&#24615;&#23041;&#32961;&#65288;APT&#65289;&#25915;&#20987;&#30340;&#38271;&#26399;&#28508;&#20239;&#12289;&#38544;&#31192;&#22810;&#38454;&#27573;&#25915;&#20987;&#27169;&#24335;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;APT&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#26469;&#28304;&#22270;&#25552;&#20379;&#30340;&#21382;&#21490;&#20449;&#24687;&#36827;&#34892;APT&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;Transformer&#30340;&#33258;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#25552;&#21462;&#31995;&#32479;&#29366;&#24577;&#30340;&#38271;&#26399;&#19978;&#19979;&#25991;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#26469;&#28304;&#20998;&#26512;&#23454;&#29616;&#23545;&#38271;&#26399;&#36816;&#34892;&#31995;&#32479;&#30340;&#27010;&#25324;&#65292;&#20197;&#26816;&#27979;&#32531;&#24930;&#25915;&#20987;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#24341;&#20837;&#20102;&#24322;&#24120;&#35780;&#20998;&#65292;&#21487;&#35780;&#20272;&#19981;&#21516;&#31995;&#32479;&#29366;&#24577;&#30340;&#24322;&#24120;&#24615;&#12290;&#27599;&#20010;&#29366;&#24577;&#37117;&#26377;&#30456;&#24212;&#30340;&#30456;&#20284;&#24230;&#21644;&#38548;&#31163;&#24230;&#20998;&#25968;&#30340;&#24322;&#24120;&#20998;&#25968;&#35745;&#31639;&#12290;&#20026;&#20102;&#35780;&#20272;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
APT detection is difficult to detect due to the long-term latency, covert and slow multistage attack patterns of Advanced Persistent Threat (APT). To tackle these issues, we propose TBDetector, a transformer-based advanced persistent threat detection method for APT attack detection. Considering that provenance graphs provide rich historical information and have the powerful attacks historic correlation ability to identify anomalous activities, TBDetector employs provenance analysis for APT detection, which summarizes long-running system execution with space efficiency and utilizes transformer with self-attention based encoder-decoder to extract long-term contextual features of system states to detect slow-acting attacks. Furthermore, we further introduce anomaly scores to investigate the anomaly of different system states, where each state is calculated with an anomaly score corresponding to its similarity score and isolation score. To evaluate the effectiveness of the proposed method,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#36845;&#20195;&#25991;&#26412;&#21040;&#20840;&#21521;3D&#27169;&#22411;&#30340;&#26032;&#22411;&#27969;&#31243;&#65292;&#26088;&#22312;&#35299;&#20915;&#24403;&#21069;&#29616;&#26377;&#26041;&#27861;&#37325;&#24314;&#30340;3D&#23545;&#35937;&#23545;&#32473;&#23450;&#22270;&#20687;&#30340;&#23545;&#24212;&#24615;&#21644;&#22810;&#35270;&#22270;&#19968;&#33268;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.02827</link><description>&lt;p&gt;
DITTO-NeRF: &#22522;&#20110;&#25193;&#25955;&#30340;&#36845;&#20195;&#25991;&#26412;&#21040;&#20840;&#21521;3D&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DITTO-NeRF: Diffusion-based Iterative Text To Omni-directional 3D Model. (arXiv:2304.02827v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#36845;&#20195;&#25991;&#26412;&#21040;&#20840;&#21521;3D&#27169;&#22411;&#30340;&#26032;&#22411;&#27969;&#31243;&#65292;&#26088;&#22312;&#35299;&#20915;&#24403;&#21069;&#29616;&#26377;&#26041;&#27861;&#37325;&#24314;&#30340;3D&#23545;&#35937;&#23545;&#32473;&#23450;&#22270;&#20687;&#30340;&#23545;&#24212;&#24615;&#21644;&#22810;&#35270;&#22270;&#19968;&#33268;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36136;&#37327;&#30340;3D&#20869;&#23481;&#21019;&#20316;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#65292;&#20419;&#20351;&#20154;&#20204;&#24320;&#21457;&#20174;&#21333;&#20010;&#22270;&#20687;&#21644;/&#25110;&#25991;&#26412;&#25552;&#31034;&#20013;&#21019;&#24314;3D&#23545;&#35937;&#27169;&#22411;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#21040;3D&#26041;&#27861;&#37325;&#24314;&#30340;3D&#23545;&#35937;&#23545;&#32473;&#23450;&#22270;&#20687;&#30340;&#23545;&#24212;&#24615;&#21644;&#22810;&#35270;&#22270;&#19968;&#33268;&#24615;&#20173;&#28982;&#19981;&#36275;&#12290;&#26368;&#36817;&#30340;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#21040;3D&#26041;&#27861;&#20063;&#26377;&#23616;&#38480;&#24615;&#65292;&#27599;&#20010;&#25552;&#31034;&#24471;&#21040;&#30340;3D&#26679;&#26412;&#22810;&#26679;&#24615;&#20302;&#65292;&#21512;&#25104;&#26102;&#38388;&#38271;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DITTO-NeRF&#65292;&#19968;&#31181;&#20174;&#25991;&#26412;&#25552;&#31034;&#25110;&#21333;&#20010;&#22270;&#20687;&#29983;&#25104;&#39640;&#36136;&#37327;3D NeRF&#27169;&#22411;&#30340;&#26032;&#22411;&#27969;&#31243;&#12290;&#25105;&#20204;&#30340;DITTO-NeRF&#21253;&#25324;&#20351;&#29992;&#32473;&#23450;&#25110;&#25991;&#26412;&#29983;&#25104;&#30340;2D&#22270;&#20687;&#20174;&#21069;&#26041;&#35270;&#22270;&#26500;&#24314;&#26377;&#38480;&#30340;&#39640;&#36136;&#37327;&#23616;&#37096;3D&#23545;&#35937;&#65292;&#28982;&#21518;&#36890;&#36807;&#34917;&#20840;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#36845;&#20195;&#37325;&#24314;&#20854;&#20313;&#30340;3D NeRF&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36880;&#27493;&#30340;3D&#23545;&#35937;&#37325;&#24314;&#26041;&#26696;&#65292;&#28085;&#30422;&#19981;&#21516;&#30340;&#23610;&#24230;(&#20302;&#20998;&#36776;&#29575;&#21040;&#39640;&#20998;&#36776;&#29575;)&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing demand for high-quality 3D content creation has motivated the development of automated methods for creating 3D object models from a single image and/or from a text prompt. However, the reconstructed 3D objects using state-of-the-art image-to-3D methods still exhibit low correspondence to the given image and low multi-view consistency. Recent state-of-the-art text-to-3D methods are also limited, yielding 3D samples with low diversity per prompt with long synthesis time. To address these challenges, we propose DITTO-NeRF, a novel pipeline to generate a high-quality 3D NeRF model from a text prompt or a single image. Our DITTO-NeRF consists of constructing high-quality partial 3D object for limited in-boundary (IB) angles using the given or text-generated 2D image from the frontal view and then iteratively reconstructing the remaining 3D NeRF using inpainting latent diffusion model. We propose progressive 3D object reconstruction schemes in terms of scales (low to high reso
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21457;&#29616;&#65292;GPT&#26816;&#27979;&#22120;&#23545;&#38750;&#33521;&#35821;&#27597;&#35821;&#20316;&#32773;&#23384;&#22312;&#20559;&#35265;&#65292;&#23481;&#26131;&#23558;&#20854;&#20869;&#23481;&#38169;&#35823;&#22320;&#20998;&#31867;&#20026;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#12290;&#27492;&#22806;&#65292;&#31616;&#21333;&#30340;&#25552;&#31034;&#31574;&#30053;&#21487;&#20197;&#32531;&#35299;&#36825;&#31181;&#20559;&#35265;&#65292;&#21516;&#26102;&#35268;&#36991;GPT&#26816;&#27979;&#22120;&#65292;&#36825;&#34920;&#26126;GPT&#26816;&#27979;&#22120;&#21487;&#33021;&#20250;&#24809;&#32602;&#20855;&#26377;&#21463;&#38480;&#35821;&#35328;&#34920;&#36798;&#33021;&#21147;&#30340;&#20316;&#32773;&#12290;</title><link>http://arxiv.org/abs/2304.02819</link><description>&lt;p&gt;
GPT&#26816;&#27979;&#22120;&#23545;&#38750;&#33521;&#35821;&#27597;&#35821;&#30340;&#20316;&#32773;&#23384;&#22312;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
GPT detectors are biased against non-native English writers. (arXiv:2304.02819v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02819
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21457;&#29616;&#65292;GPT&#26816;&#27979;&#22120;&#23545;&#38750;&#33521;&#35821;&#27597;&#35821;&#20316;&#32773;&#23384;&#22312;&#20559;&#35265;&#65292;&#23481;&#26131;&#23558;&#20854;&#20869;&#23481;&#38169;&#35823;&#22320;&#20998;&#31867;&#20026;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#12290;&#27492;&#22806;&#65292;&#31616;&#21333;&#30340;&#25552;&#31034;&#31574;&#30053;&#21487;&#20197;&#32531;&#35299;&#36825;&#31181;&#20559;&#35265;&#65292;&#21516;&#26102;&#35268;&#36991;GPT&#26816;&#27979;&#22120;&#65292;&#36825;&#34920;&#26126;GPT&#26816;&#27979;&#22120;&#21487;&#33021;&#20250;&#24809;&#32602;&#20855;&#26377;&#21463;&#38480;&#35821;&#35328;&#34920;&#36798;&#33021;&#21147;&#30340;&#20316;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#25512;&#24191;&#24102;&#26469;&#20102;&#25968;&#23383;&#36890;&#20449;&#26041;&#38754;&#30340;&#23454;&#36136;&#24615;&#36827;&#23637;&#65292;&#21516;&#26102;&#20063;&#24341;&#21457;&#20102;AI&#29983;&#25104;&#20869;&#23481;&#28508;&#22312;&#35823;&#29992;&#30340;&#25285;&#24551;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26816;&#27979;&#26041;&#27861;&#26469;&#21306;&#20998;AI&#21644;&#20154;&#31867;&#29983;&#25104;&#30340;&#20869;&#23481;&#65292;&#20294;&#36825;&#20123;&#26816;&#27979;&#22120;&#30340;&#20844;&#24179;&#24615;&#21644;&#40065;&#26834;&#24615;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;&#33521;&#35821;&#27597;&#35821;&#21644;&#38750;&#33521;&#35821;&#27597;&#35821;&#20316;&#32773;&#30340;&#20889;&#20316;&#26679;&#26412;&#35780;&#20272;&#20102;&#20960;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;GPT&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#36825;&#20123;&#26816;&#27979;&#22120;&#25345;&#32493;&#23558;&#38750;&#33521;&#35821;&#27597;&#35821;&#30340;&#20889;&#20316;&#26679;&#26412;&#38169;&#35823;&#22320;&#20998;&#31867;&#20026;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#65292;&#32780;&#21407;&#29983;&#20889;&#20316;&#26679;&#26412;&#21017;&#33021;&#22815;&#34987;&#20934;&#30830;&#35782;&#21035;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31616;&#21333;&#30340;&#25552;&#31034;&#31574;&#30053;&#19981;&#20165;&#21487;&#20197;&#32531;&#35299;&#36825;&#31181;&#20559;&#35265;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#26377;&#25928;&#22320;&#35268;&#36991;GPT&#26816;&#27979;&#22120;&#65292;&#36825;&#34920;&#26126;GPT&#26816;&#27979;&#22120;&#21487;&#33021;&#26080;&#24847;&#20013;&#24809;&#32602;&#20855;&#26377;&#21463;&#38480;&#35821;&#35328;&#34920;&#36798;&#33021;&#21147;&#30340;&#20316;&#32773;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#21628;&#21505;&#36827;&#34892;&#26356;&#24191;&#27867;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid adoption of generative language models has brought about substantial advancements in digital communication, while simultaneously raising concerns regarding the potential misuse of AI-generated content. Although numerous detection methods have been proposed to differentiate between AI and human-generated content, the fairness and robustness of these detectors remain underexplored. In this study, we evaluate the performance of several widely-used GPT detectors using writing samples from native and non-native English writers. Our findings reveal that these detectors consistently misclassify non-native English writing samples as AI-generated, whereas native writing samples are accurately identified. Furthermore, we demonstrate that simple prompting strategies can not only mitigate this bias but also effectively bypass GPT detectors, suggesting that GPT detectors may unintentionally penalize writers with constrained linguistic expressions. Our results call for a broader conversati
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#26700;&#38754;&#22330;&#26223;&#30340;&#39640;&#31934;&#24230;&#38754;&#37096;&#21160;&#30011;&#21046;&#20316;&#27969;&#31243;&#65292;&#21482;&#38656;&#35201;&#19968;&#33324;&#25668;&#20687;&#22836;&#21363;&#21487;&#23454;&#29616;&#23454;&#26102;&#38754;&#37096;&#25429;&#25417;&#65292;&#33021;&#22815;&#25552;&#39640;&#21160;&#30011;&#24072;&#30340;&#29983;&#20135;&#29575;&#24182;&#38477;&#20302;&#20256;&#32479;&#35299;&#20915;&#26041;&#26696;&#30340;&#25104;&#26412;&#21644;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.02814</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#26700;&#38754;&#22330;&#26223;&#30340;4D&#35270;&#35282;&#19979;&#30340;&#39640;&#31934;&#24230;&#23454;&#26102;&#38754;&#37096;&#21160;&#30011;&#21046;&#20316;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
4D Agnostic Real-Time Facial Animation Pipeline for Desktop Scenarios. (arXiv:2304.02814v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02814
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#26700;&#38754;&#22330;&#26223;&#30340;&#39640;&#31934;&#24230;&#38754;&#37096;&#21160;&#30011;&#21046;&#20316;&#27969;&#31243;&#65292;&#21482;&#38656;&#35201;&#19968;&#33324;&#25668;&#20687;&#22836;&#21363;&#21487;&#23454;&#29616;&#23454;&#26102;&#38754;&#37096;&#25429;&#25417;&#65292;&#33021;&#22815;&#25552;&#39640;&#21160;&#30011;&#24072;&#30340;&#29983;&#20135;&#29575;&#24182;&#38477;&#20302;&#20256;&#32479;&#35299;&#20915;&#26041;&#26696;&#30340;&#25104;&#26412;&#21644;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#39640;&#31934;&#24230;&#30340;&#23454;&#26102;&#38754;&#37096;&#21160;&#30011;&#21046;&#20316;&#27969;&#31243;&#65292;&#36866;&#29992;&#20110;&#21160;&#30011;&#24072;&#22312;&#26700;&#38754;&#19978;&#20351;&#29992;&#12290;&#35813;&#27969;&#31243;&#21363;&#23558;&#24212;&#29992;&#20110;FACEGOOD&#30340;Avatary&#36719;&#20214;&#20013;&#65292;&#21487;&#20197;&#25552;&#39640;&#21160;&#30011;&#24072;&#30340;&#29983;&#20135;&#29575;&#12290;&#35813;&#27969;&#31243;&#19982;&#19987;&#19994;&#22836;&#25140;&#24335;&#38754;&#37096;&#25429;&#25417;&#35299;&#20915;&#26041;&#26696;&#19981;&#21516;&#65292;&#23427;&#21482;&#38656;&#35201;&#22312;&#26700;&#38754;&#19978;&#20351;&#29992;&#28040;&#36153;&#32423;&#21035;&#30340;3D&#25668;&#20687;&#22836;&#21363;&#21487;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#23454;&#26102;&#38754;&#37096;&#25429;&#25417;&#12290;&#35813;&#31995;&#32479;&#21487;&#20197;&#35753;&#21160;&#30011;&#24072;&#36731;&#26494;&#19988;&#24555;&#36895;&#22320;&#21046;&#20316;&#39640;&#36136;&#37327;&#30340;&#38754;&#37096;&#21160;&#30011;&#65292;&#21516;&#26102;&#38477;&#20302;&#20256;&#32479;&#38754;&#37096;&#25429;&#25417;&#35299;&#20915;&#26041;&#26696;&#30340;&#25104;&#26412;&#21644;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#28508;&#21147;&#22312;&#23089;&#20048;&#20135;&#19994;&#20013;&#24443;&#24213;&#25913;&#21464;&#38754;&#37096;&#21160;&#30011;&#30340;&#21046;&#20316;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a high-precision real-time facial animation pipeline suitable for animators to use on their desktops. This pipeline is about to be launched in FACEGOOD's Avatary\footnote{https://www.avatary.com/} software, which will accelerate animators' productivity. The pipeline differs from professional head-mounted facial capture solutions in that it only requires the use of a consumer-grade 3D camera on the desk to achieve high-precision real-time facial capture. The system enables animators to create high-quality facial animations with ease and speed, while reducing the cost and complexity of traditional facial capture solutions. Our approach has the potential to revolutionize the way facial animation is done in the entertainment industry.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;&#22522;&#20110;&#21516;&#20262;&#30340;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#65292;&#26469;&#35299;&#20915;&#20855;&#26377;&#22810;&#20010;&#35299;&#30340;&#38750;&#32447;&#24615;&#24494;&#20998;&#26041;&#31243;&#30340;&#21453;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#24050;&#30693;&#35266;&#27979;&#32467;&#26524;&#24182;&#31526;&#21512;DEs&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#36890;&#36807;&#21516;&#20262;&#36830;&#32493;&#26041;&#27861;&#35299;&#20915;&#21453;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20280;&#32553;&#19988;&#36866;&#24212;&#24615;&#24378;&#65292;&#20026;&#35299;&#20915;&#20855;&#26377;&#22810;&#20010;&#35299;&#30340;DEs&#25552;&#20379;&#20102;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.02811</link><description>&lt;p&gt;
HomPINNs&#65306;&#22522;&#20110;&#21516;&#20262;&#30340;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#22810;&#20010;&#35299;&#30340;&#38750;&#32447;&#24615;&#24494;&#20998;&#26041;&#31243;&#30340;&#21453;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
HomPINNs: homotopy physics-informed neural networks for solving the inverse problems of nonlinear differential equations with multiple solutions. (arXiv:2304.02811v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;&#22522;&#20110;&#21516;&#20262;&#30340;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#65292;&#26469;&#35299;&#20915;&#20855;&#26377;&#22810;&#20010;&#35299;&#30340;&#38750;&#32447;&#24615;&#24494;&#20998;&#26041;&#31243;&#30340;&#21453;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#24050;&#30693;&#35266;&#27979;&#32467;&#26524;&#24182;&#31526;&#21512;DEs&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#36890;&#36807;&#21516;&#20262;&#36830;&#32493;&#26041;&#27861;&#35299;&#20915;&#21453;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20280;&#32553;&#19988;&#36866;&#24212;&#24615;&#24378;&#65292;&#20026;&#35299;&#20915;&#20855;&#26377;&#22810;&#20010;&#35299;&#30340;DEs&#25552;&#20379;&#20102;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#35299;&#31354;&#38388;&#20013;&#30340;&#38750;&#21807;&#19968;&#24615;&#12289;&#23545;&#31216;&#24615;&#21644;&#20998;&#23700;&#31561;&#22797;&#26434;&#34892;&#20026;&#65292;&#35299;&#20915;&#20855;&#26377;&#22810;&#20010;&#35299;&#30340;&#38750;&#32447;&#24615;&#24494;&#20998;&#26041;&#31243;&#65288;DEs&#65289;&#30340;&#21453;&#38382;&#39064;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21516;&#20262;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#65288;HomPINNs&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#21516;&#20262;&#36830;&#32493;&#21644;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#26469;&#35299;&#20915;&#21453;&#38382;&#39064;&#30340;&#26032;&#26694;&#26550;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#39318;&#20808;&#20351;&#29992;NNs&#21516;&#26102;&#36924;&#36817;&#24050;&#30693;&#35266;&#27979;&#32467;&#26524;&#21644;&#31526;&#21512;DEs&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#36890;&#36807;&#21033;&#29992;&#21516;&#20262;&#36830;&#32493;&#26041;&#27861;&#65292;&#36924;&#36817;&#21487;&#36861;&#36394;&#35266;&#23519;&#32467;&#26524;&#20197;&#30830;&#23450;&#22810;&#20010;&#35299;&#24182;&#35299;&#20915;&#21453;&#38382;&#39064;&#12290;&#23454;&#39564;&#28085;&#30422;&#22312;&#19968;&#32500;DEs&#19978;&#27979;&#35797;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#24212;&#29992;&#23427;&#26469;&#35299;&#20915;&#20108;&#32500;Gray-Scott&#27169;&#25311;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#21487;&#20280;&#32553;&#19988;&#36866;&#24212;&#24615;&#24378;&#30340;&#65292;&#20026;&#35299;&#20915;&#20855;&#26377;&#22810;&#20010;&#35299;&#30340;DEs&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the complex behavior arising from non-uniqueness, symmetry, and bifurcations in the solution space, solving inverse problems of nonlinear differential equations (DEs) with multiple solutions is a challenging task. To address this issue, we propose homotopy physics-informed neural networks (HomPINNs), a novel framework that leverages homotopy continuation and neural networks (NNs) to solve inverse problems. The proposed framework begins with the use of a NN to simultaneously approximate known observations and conform to the constraints of DEs. By utilizing the homotopy continuation method, the approximation traces the observations to identify multiple solutions and solve the inverse problem. The experiments involve testing the performance of the proposed method on one-dimensional DEs and applying it to solve a two-dimensional Gray-Scott simulation. Our findings demonstrate that the proposed method is scalable and adaptable, providing an effective solution for solving DEs with mul
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#20351;&#29992;&#19977;&#32500;&#36712;&#36857;&#21450;&#31508;&#23574;&#26059;&#36716;&#30340;&#33258;&#21160;&#20070;&#27861;&#35268;&#21010;&#36827;&#34892;&#28145;&#20837;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#22270;&#20687;&#21644;&#23039;&#24577;&#25968;&#25454;&#30340;&#32452;&#21512;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#23398;&#20064;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#33258;&#21160;&#35268;&#21010;&#26085;&#26412;&#20070;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.02801</link><description>&lt;p&gt;
&#36890;&#36807;&#21464;&#20998;&#20223;&#30495;&#23398;&#20064;&#23454;&#29616;&#31471;&#21040;&#31471;&#26426;&#26800;&#33218;&#20070;&#27861;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
End-to-end Manipulator Calligraphy Planning via Variational Imitation Learning. (arXiv:2304.02801v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02801
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#20351;&#29992;&#19977;&#32500;&#36712;&#36857;&#21450;&#31508;&#23574;&#26059;&#36716;&#30340;&#33258;&#21160;&#20070;&#27861;&#35268;&#21010;&#36827;&#34892;&#28145;&#20837;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#22270;&#20687;&#21644;&#23039;&#24577;&#25968;&#25454;&#30340;&#32452;&#21512;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#23398;&#20064;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#33258;&#21160;&#35268;&#21010;&#26085;&#26412;&#20070;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21457;&#23637;&#65292;&#22522;&#20110;&#28436;&#31034;&#30340;&#35268;&#21010;&#24050;&#32463;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#26524;&#12290;&#20854;&#20013;&#26368;&#27969;&#34892;&#30340;&#29616;&#23454;&#24212;&#29992;&#20043;&#19968;&#26159;&#20351;&#29992;&#26426;&#22120;&#20154;&#25163;&#33218;&#33258;&#21160;&#20070;&#20889;&#12290;&#20256;&#32479;&#19978;&#65292;&#23427;&#34987;&#31616;&#21270;&#20026;&#20108;&#32500;&#38382;&#39064;&#12290;&#36825;&#31181;&#34920;&#36798;&#36866;&#29992;&#20110;&#22522;&#26412;&#32472;&#30011;&#65292;&#20294;&#23545;&#20110;&#26085;&#26412;&#20070;&#27861;&#25110;&#22797;&#26434;&#30340;&#33402;&#26415;&#20316;&#21697;&#26469;&#35828;&#26159;&#19981;&#36275;&#22815;&#30340;&#65292;&#22240;&#20026;&#31508;&#30340;&#26041;&#21521;&#26159;&#29992;&#25143;&#34920;&#36798;&#30340;&#19968;&#37096;&#20998;&#12290;&#26412;&#30740;&#31350;&#38024;&#23545;&#20351;&#29992;&#19977;&#32500;&#36712;&#36857;&#21450;&#31508;&#23574;&#26059;&#36716;&#30340;&#33258;&#21160;&#20070;&#27861;&#35268;&#21010;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#27169;&#20223;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#22270;&#20687;&#21644;&#23039;&#24577;&#25968;&#25454;&#30340;&#32452;&#21512;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#23398;&#20064;&#12290;&#35813;&#32593;&#32476;&#30001;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#12289;&#21452;&#21521;LSTM&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#30340;&#32452;&#21512;&#26500;&#25104;&#12290;&#23454;&#39564;&#37319;&#29992;&#28176;&#36827;&#26041;&#24335;&#36827;&#34892;&#65292;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;
Planning from demonstrations has shown promising results with the advances of deep neural networks. One of the most popular real-world applications is automated handwriting using a robotic manipulator. Classically it is simplified as a two-dimension problem. This representation is suitable for elementary drawings, but it is not sufficient for Japanese calligraphy or complex work of art where the orientation of a pen is part of the user expression. In this study, we focus on automated planning of Japanese calligraphy using a three-dimension representation of the trajectory as well as the rotation of the pen tip, and propose a novel deep imitation learning neural network that learns from expert demonstrations through a combination of images and pose data. The network consists of a combination of variational auto-encoder, bi-directional LSTM, and Multi-Layer Perceptron (MLP). Experiments are conducted in a progressive way, and results demonstrate that the proposed approach is successful i
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35302;&#21457;&#22120;&#21453;&#28436;&#30340;&#32479;&#19968;&#26694;&#26550; UNICORN&#65292;&#21487;&#29992;&#20110;&#35782;&#21035;&#21518;&#38376;&#27169;&#22411;&#24182;&#29702;&#35299;&#26893;&#20837;&#30340;&#24694;&#24847;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2304.02786</link><description>&lt;p&gt;
UNICORN: &#19968;&#31181;&#32479;&#19968;&#30340;&#21518;&#38376;&#35302;&#21457;&#21453;&#28436;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
UNICORN: A Unified Backdoor Trigger Inversion Framework. (arXiv:2304.02786v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35302;&#21457;&#22120;&#21453;&#28436;&#30340;&#32479;&#19968;&#26694;&#26550; UNICORN&#65292;&#21487;&#29992;&#20110;&#35782;&#21035;&#21518;&#38376;&#27169;&#22411;&#24182;&#29702;&#35299;&#26893;&#20837;&#30340;&#24694;&#24847;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#38376;&#25915;&#20987;&#26159;&#19968;&#31181;&#20005;&#37325;&#23041;&#32961;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#25915;&#20987;&#25163;&#27573;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#27880;&#20837;&#24102;&#26377;&#35302;&#21457;&#20449;&#21495;&#30340;&#36755;&#20837;&#25968;&#25454;&#65288;&#22914;&#34917;&#19969;&#65289;&#26469;&#28608;&#27963;&#39044;&#20808;&#26893;&#20837;&#30340;&#24694;&#24847;&#34892;&#20026;&#12290;&#35302;&#21457;&#20449;&#21495;&#21453;&#28436;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#21518;&#38376;&#27169;&#22411;&#35782;&#21035;&#21644;&#23545;&#26893;&#20837;&#36827;&#21435;&#30340;&#24694;&#24847;&#34892;&#20026;&#36827;&#34892;&#29702;&#35299;&#30340;&#26041;&#27861;&#12290;&#29616;&#26377;&#26041;&#27861;&#22312;&#21453;&#28436;&#35302;&#21457;&#20449;&#21495;&#26102;&#20250;&#26377;&#35768;&#22810;&#19981;&#21516;&#30340;&#26500;&#36896;&#26041;&#24335;&#65292;&#20294;&#22240;&#20026;&#37319;&#29992;&#20102;&#19968;&#20123;&#36807;&#20110;&#20855;&#20307;&#30340;&#20551;&#35774;&#25110;&#32773;&#25915;&#20987;&#26041;&#24335;&#29305;&#23450;&#30340;&#38480;&#21046;&#65292;&#36825;&#20123;&#26041;&#27861;&#23601;&#26080;&#27861;&#27867;&#21270;&#21040;&#21508;&#31181;&#31867;&#22411;&#30340;&#35302;&#21457;&#20449;&#21495;&#19978;&#12290;&#26681;&#26412;&#21407;&#22240;&#26159;&#29616;&#26377;&#24037;&#20316;&#27809;&#26377;&#32771;&#34385;&#35302;&#21457;&#22120;&#35774;&#35745;&#31354;&#38388;&#23545;&#21453;&#28436;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#27491;&#24335;&#23450;&#20041;&#21644;&#20998;&#26512;&#20102;&#19981;&#21516;&#31354;&#38388;&#20013;&#23884;&#20837;&#30340;&#35302;&#21457;&#22120;&#21450;&#20854;&#21453;&#28436;&#38382;&#39064;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21453;&#28436;&#38382;&#39064;&#30340;&#35302;&#21457;&#22120;&#24418;&#24335;&#21270;&#23450;&#20041;&#21644;&#20998;&#26512;&#35782;&#21035;&#21518;&#38376;&#27169;&#22411;&#20869;&#37096;&#34892;&#20026;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#21407;&#22411; UNICORN &#22312;&#27867;&#21270;&#24615;&#21644;&#26377;&#25928;&#24615;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The backdoor attack, where the adversary uses inputs stamped with triggers (e.g., a patch) to activate pre-planted malicious behaviors, is a severe threat to Deep Neural Network (DNN) models. Trigger inversion is an effective way of identifying backdoor models and understanding embedded adversarial behaviors. A challenge of trigger inversion is that there are many ways of constructing the trigger. Existing methods cannot generalize to various types of triggers by making certain assumptions or attack-specific constraints. The fundamental reason is that existing work does not consider the trigger's design space in their formulation of the inversion problem. This work formally defines and analyzes the triggers injected in different spaces and the inversion problem. Then, it proposes a unified framework to invert backdoor triggers based on the formalization of triggers and the identified inner behaviors of backdoor models from our analysis. Our prototype UNICORN is general and effective in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20915;&#31574;&#26641;&#30340;&#20805;&#20998;&#21407;&#22240;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#38382;&#39064;&#30340;&#36817;&#20284;&#38590;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.02781</link><description>&lt;p&gt;
&#20915;&#31574;&#26641;&#30340;&#20805;&#20998;&#21407;&#22240;&#38382;&#39064;&#30340;&#36817;&#20284;&#38590;&#24230;
&lt;/p&gt;
&lt;p&gt;
Inapproximability of sufficient reasons for decision trees. (arXiv:2304.02781v1 [cs.CC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20915;&#31574;&#26641;&#30340;&#20805;&#20998;&#21407;&#22240;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#38382;&#39064;&#30340;&#36817;&#20284;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#35745;&#31639;&#20915;&#31574;&#26641;&#30340;$\delta$-&#20805;&#20998;&#21407;&#22240;&#30340;&#26368;&#23567;&#22823;&#23567;&#30340;&#38382;&#39064;&#30340;&#36817;&#20284;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this note, we establish the hardness of approximation of the problem of computing the minimal size of a $\delta$-sufficient reason for decision trees.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#34394;&#26500;&#25925;&#20107;&#30340;&#20107;&#23454;&#39564;&#35777;&#38382;&#39064;&#30340;&#20302;&#25104;&#26412;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#20004;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#31471;&#21040;&#31471;&#27969;&#31243;&#21644;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.02769</link><description>&lt;p&gt;
&#38754;&#21521;&#23567;&#26679;&#26412;&#23398;&#20064;&#30340;&#34394;&#26500;&#25925;&#20107;&#39564;&#35777;&#20302;&#25104;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Low-Shot Learning for Fictional Claim Verification. (arXiv:2304.02769v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#34394;&#26500;&#25925;&#20107;&#30340;&#20107;&#23454;&#39564;&#35777;&#38382;&#39064;&#30340;&#20302;&#25104;&#26412;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#20004;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#31471;&#21040;&#31471;&#27969;&#31243;&#21644;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#34394;&#26500;&#25925;&#20107;&#20013;&#20107;&#23454;&#39564;&#35777;&#30340;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#20302;&#25104;&#26412;&#23398;&#20064;&#30340;&#26041;&#24335;&#20135;&#29983;&#20102;&#20004;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#27969;&#31243;&#21644;&#27169;&#22411;&#65292;&#21516;&#26102;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#20026;&#20102;&#27979;&#35797;&#25105;&#20204;&#30340;&#27969;&#31243;&#21644;&#38590;&#24230;&#65292;&#25105;&#20204;&#23558;&#27169;&#22411;&#30340;&#32467;&#26524;&#19982;&#20154;&#31867;&#21644;&#38543;&#26426;&#20998;&#37197;&#30340;&#32467;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#20197;&#22312;https://github.com/Derposoft/plot_hole_detection&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the problem of claim verification in the context of claims about fictional stories in a low-shot learning setting. To this end, we generate two synthetic datasets and then develop an end-to-end pipeline and model that is tested on both benchmarks. To test the efficacy of our pipeline and the difficulty of benchmarks, we compare our models' results against human and random assignment results. Our code is available at https://github.com/Derposoft/plot_hole_detection.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;Transformer&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#22312;&#30005;&#23376;&#30149;&#21382;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#30446;&#21069;&#30740;&#31350;&#20013;&#30340;&#38480;&#21046;&#21644;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2304.02768</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#22312;&#30005;&#23376;&#30149;&#21382;&#20013;&#30340;&#24212;&#29992;&#65306;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Application of Transformers based methods in Electronic Medical Records: A Systematic Literature Review. (arXiv:2304.02768v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02768
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;Transformer&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#22312;&#30005;&#23376;&#30149;&#21382;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#30446;&#21069;&#30740;&#31350;&#20013;&#30340;&#38480;&#21046;&#21644;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21487;&#29992;&#25968;&#25454;&#30340;&#22686;&#38271;&#21644;&#23427;&#20204;&#30340;&#38750;&#32467;&#26500;&#21270;&#24615;&#36136;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25216;&#26415;&#24320;&#22987;&#21463;&#21040;&#20851;&#27880;&#65292;&#20197;&#20174;&#36825;&#20123;&#25968;&#25454;&#36164;&#20135;&#20013;&#33719;&#24471;&#20215;&#20540;&#65292;&#22240;&#20026;&#36825;&#31181;&#26684;&#24335;&#19981;&#36866;&#29992;&#20110;&#32479;&#35745;&#20998;&#26512;&#12290;&#26412;&#25991;&#23545;&#19981;&#21516;NLP&#20219;&#21153;&#20013;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;EMR&#19978;&#30340;&#26368;&#26032;&#36827;&#23637;&#36827;&#34892;&#20102;&#31995;&#32479;&#24615;&#30340;&#25991;&#29486;&#32508;&#36848;&#12290;&#22312;&#26368;&#21021;&#30340;&#26597;&#35810;&#20013;&#65292;&#20174;&#19977;&#20010;&#20844;&#20849;&#25968;&#25454;&#24211;&#20013;&#36873;&#25321;&#20102;99&#31687;&#25991;&#31456;&#65292;&#26368;&#32456;&#31579;&#36873;&#24471;&#21040;&#20102;65&#31687;&#25991;&#31456;&#36827;&#34892;&#35814;&#32454;&#20998;&#26512;&#12290;&#26412;&#25991;&#23558;&#20174;&#19994;&#21153;&#38382;&#39064;&#12289;NLP&#20219;&#21153;&#12289;&#27169;&#22411;&#21644;&#25216;&#26415;&#12289;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#12289;&#24314;&#27169;&#30340;&#21487;&#37325;&#22797;&#24615;&#12289;&#35821;&#35328;&#21644;&#20132;&#25442;&#26684;&#24335;&#31561;&#26041;&#38754;&#23545;&#36825;&#20123;&#35770;&#25991;&#36827;&#34892;&#20998;&#26512;&#12290;&#25991;&#31456;&#25552;&#20986;&#20102;&#24403;&#21069;&#30740;&#31350;&#30340;&#19968;&#20123;&#23616;&#38480;&#24615;&#20197;&#21450;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
The combined growth of available data and their unstructured nature has received increased interest in natural language processing (NLP) techniques to make value of these data assets since this format is not suitable for statistical analysis. This work presents a systematic literature review of state-of-the-art advances using transformer-based methods on electronic medical records (EMRs) in different NLP tasks. To the best of our knowledge, this work is unique in providing a comprehensive review of research on transformer-based methods for NLP applied to the EMR field. In the initial query, 99 articles were selected from three public databases and filtered into 65 articles for detailed analysis. The papers were analyzed with respect to the business problem, NLP task, models and techniques, availability of datasets, reproducibility of modeling, language, and exchange format. The paper presents some limitations of current research and some recommendations for further research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#30001;&#27801;&#29305;&#38463;&#25289;&#20271;&#19981;&#21516;&#39046;&#22495;&#30340;&#38463;&#25289;&#20271;&#35821;&#38544;&#31169;&#25919;&#31574;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26681;&#25454;&#20010;&#20154;&#25968;&#25454;&#20445;&#25252;&#27861;&#30340;10&#20010;&#21407;&#21017;&#36827;&#34892;&#20102;&#27880;&#37322;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#35780;&#20272;&#38544;&#31169;&#25919;&#31574;&#36981;&#23432;&#24615;&#12289;&#34892;&#19994;&#38544;&#31169;&#23454;&#36341;&#22522;&#20934;&#27979;&#35797;&#20197;&#21450;&#24320;&#21457;&#30417;&#27979;&#25968;&#25454;&#20445;&#25252;&#27861;&#35268;&#36981;&#23432;&#24615;&#30340;&#33258;&#21160;&#21270;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2304.02757</link><description>&lt;p&gt;
&#27801;&#29305;&#38463;&#25289;&#20271;&#38544;&#31169;&#25919;&#31574;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
The Saudi Privacy Policy Dataset. (arXiv:2304.02757v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#30001;&#27801;&#29305;&#38463;&#25289;&#20271;&#19981;&#21516;&#39046;&#22495;&#30340;&#38463;&#25289;&#20271;&#35821;&#38544;&#31169;&#25919;&#31574;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26681;&#25454;&#20010;&#20154;&#25968;&#25454;&#20445;&#25252;&#27861;&#30340;10&#20010;&#21407;&#21017;&#36827;&#34892;&#20102;&#27880;&#37322;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#35780;&#20272;&#38544;&#31169;&#25919;&#31574;&#36981;&#23432;&#24615;&#12289;&#34892;&#19994;&#38544;&#31169;&#23454;&#36341;&#22522;&#20934;&#27979;&#35797;&#20197;&#21450;&#24320;&#21457;&#30417;&#27979;&#25968;&#25454;&#20445;&#25252;&#27861;&#35268;&#36981;&#23432;&#24615;&#30340;&#33258;&#21160;&#21270;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#27801;&#29305;&#38544;&#31169;&#25919;&#31574;&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#26469;&#33258;&#27801;&#29305;&#38463;&#25289;&#20271;&#19981;&#21516;&#39046;&#22495;&#30340;&#38463;&#25289;&#20271;&#35821;&#38544;&#31169;&#25919;&#31574;&#32452;&#25104;&#30340;&#22810;&#26679;&#21270;&#27719;&#32534;&#65292;&#26681;&#25454;&#20010;&#20154;&#25968;&#25454;&#20445;&#25252;&#27861;&#30340;10&#20010;&#21407;&#21017;&#36827;&#34892;&#20102;&#27880;&#37322;&#65307;&#35813;&#27861;&#35268;&#26088;&#22312;&#19982;&#20840;&#29699;&#26368;&#32508;&#21512;&#30340;&#25968;&#25454;&#27861;&#35268;&#20043;&#19968;&#30340;&#36890;&#29992;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#30456;&#20860;&#23481;&#12290; &#25968;&#25454;&#25910;&#38598;&#33258;&#22810;&#20010;&#26469;&#28304;&#65292;&#21253;&#25324;&#27801;&#29305;&#20013;&#22830;&#38134;&#34892;&#65292;&#27801;&#29305;&#22269;&#23478;&#32852;&#21512;&#24179;&#21488;&#65292;&#20445;&#38505;&#21355;&#29983;&#22996;&#21592;&#20250;&#20197;&#21450;&#20351;&#29992;Google&#21644;&#32500;&#22522;&#30334;&#31185;&#30340;&#19968;&#33324;&#32593;&#31449;&#12290; &#26368;&#32456;&#25968;&#25454;&#38598;&#21253;&#25324;&#26469;&#33258;7&#20010;&#34892;&#19994;&#30340;1,000&#20010;&#32593;&#31449;&#65292;4,638&#34892;&#25991;&#26412;&#65292;775,370&#20010;&#26631;&#35760;&#65292;&#20197;&#21450;8,353 KB&#30340;&#35821;&#26009;&#24211;&#22823;&#23567;&#12290; &#27880;&#37322;&#25968;&#25454;&#38598;&#20026;&#35780;&#20272;&#38544;&#31169;&#25919;&#31574;&#36981;&#20174;&#24615;&#65292;&#34892;&#19994;&#38544;&#31169;&#23454;&#36341;&#22522;&#20934;&#27979;&#35797;&#20197;&#21450;&#24320;&#21457;&#30417;&#27979;&#25968;&#25454;&#20445;&#25252;&#27861;&#35268;&#36981;&#23432;&#24615;&#30340;&#33258;&#21160;&#21270;&#24037;&#20855;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#37325;&#22797;&#21033;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces the Saudi Privacy Policy Dataset, a diverse compilation of Arabic privacy policies from various sectors in Saudi Arabia, annotated according to the 10 principles of the Personal Data Protection Law (PDPL); the PDPL was established to be compatible with General Data Protection Regulation (GDPR); one of the most comprehensive data regulations worldwide. Data were collected from multiple sources, including the Saudi Central Bank, the Saudi Arabia National United Platform, the Council of Health Insurance, and general websites using Google and Wikipedia. The final dataset includes 1,000 websites belonging to 7 sectors, 4,638 lines of text, 775,370 tokens, and a corpus size of 8,353 KB. The annotated dataset offers significant reuse potential for assessing privacy policy compliance, benchmarking privacy practices across industries, and developing automated tools for monitoring adherence to data protection regulations. By providing a comprehensive and annotated dataset o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#21322;&#30417;&#30563;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20197;&#23569;&#37327;&#25968;&#25454;&#20998;&#31867;&#23391;&#21152;&#25289;&#35821;&#20551;&#35780;&#35770;&#21644;&#30495;&#23454;&#35780;&#35770;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;BanglaBERT&#19982;&#21322;&#30417;&#30563;GAN&#30456;&#32467;&#21512;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20934;&#30830;&#29575;&#36798;&#21040;83.59&#65285;&#65292;f1&#20998;&#25968;&#36798;&#21040;84.89&#65285;&#12290;</title><link>http://arxiv.org/abs/2304.02739</link><description>&lt;p&gt;
&#20351;&#29992;&#21322;&#30417;&#30563;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26816;&#27979;&#23391;&#21152;&#25289;&#35821;&#20551;&#35780;&#35770;
&lt;/p&gt;
&lt;p&gt;
Bengali Fake Review Detection using Semi-supervised Generative Adversarial Networks. (arXiv:2304.02739v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#21322;&#30417;&#30563;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20197;&#23569;&#37327;&#25968;&#25454;&#20998;&#31867;&#23391;&#21152;&#25289;&#35821;&#20551;&#35780;&#35770;&#21644;&#30495;&#23454;&#35780;&#35770;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;BanglaBERT&#19982;&#21322;&#30417;&#30563;GAN&#30456;&#32467;&#21512;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20934;&#30830;&#29575;&#36798;&#21040;83.59&#65285;&#65292;f1&#20998;&#25968;&#36798;&#21040;84.89&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#21322;&#30417;&#30563;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#23569;&#37327;&#24050;&#27880;&#37322;&#25968;&#25454;&#26469;&#20998;&#31867;&#23391;&#21152;&#25289;&#35821;&#20551;&#35780;&#35770;&#21644;&#30495;&#23454;&#35780;&#35770;&#30340;&#28508;&#21147;&#12290;&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#21644;&#30005;&#23376;&#21830;&#21153;&#30340;&#20852;&#36215;&#65292;&#33021;&#22815;&#26816;&#27979;&#34394;&#20551;&#25110;&#27450;&#39575;&#24615;&#35780;&#35770;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#20197;&#20445;&#25252;&#28040;&#36153;&#32773;&#20813;&#21463;&#34394;&#20551;&#20449;&#24687;&#30340;&#35823;&#23548;&#12290;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#35782;&#21035;&#20551;&#35780;&#35770;&#26041;&#38754;&#37117;&#20250;&#36935;&#21040;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20687;&#23391;&#21152;&#25289;&#35821;&#36825;&#26679;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#21322;&#30417;&#30563;GAN-LM&#20307;&#31995;&#32467;&#26500;&#65288;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20043;&#19978;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65289;&#26159;&#19968;&#20010;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#21482;&#26377;1024&#20010;&#24050;&#27880;&#37322;&#30340;&#26679;&#26412;&#65292;&#20351;&#29992;&#21322;&#30417;&#30563;GAN&#30340;BanglaBERT&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;83.59&#65285;&#65292;f1&#20998;&#25968;&#36798;&#21040;84.89&#65285;&#65292;&#20248;&#20110;&#20854;&#20182;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;BanglaBERT&#29983;&#25104;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the potential of semi-supervised Generative Adversarial Networks (GANs) to fine-tune pretrained language models in order to classify Bengali fake reviews from real reviews with a few annotated data. With the rise of social media and e-commerce, the ability to detect fake or deceptive reviews is becoming increasingly important in order to protect consumers from being misled by false information. Any machine learning model will have trouble identifying a fake review, especially for a low resource language like Bengali. We have demonstrated that the proposed semi-supervised GAN-LM architecture (generative adversarial network on top of a pretrained language model) is a viable solution in classifying Bengali fake reviews as the experimental results suggest that even with only 1024 annotated samples, BanglaBERT with semi-supervised GAN (SSGAN) achieved an accuracy of 83.59% and a f1-score of 84.89% outperforming other pretrained language models BanglaBERT generator,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#20855;&#36523;&#35270;&#35273;&#35821;&#35328;&#35268;&#21010;&#65288;EVLP&#65289;&#20219;&#21153;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#26426;&#20250;&#65292;&#26088;&#22312;&#20849;&#21516;&#21033;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#29289;&#29702;&#29615;&#22659;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2304.02738</link><description>&lt;p&gt;
&#20855;&#36523;&#35270;&#35273;&#35821;&#35328;&#35268;&#21010;&#20013;&#30340;&#26680;&#24515;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Core Challenges in Embodied Vision-Language Planning. (arXiv:2304.02738v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#20855;&#36523;&#35270;&#35273;&#35821;&#35328;&#35268;&#21010;&#65288;EVLP&#65289;&#20219;&#21153;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#26426;&#20250;&#65292;&#26088;&#22312;&#20849;&#21516;&#21033;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#29289;&#29702;&#29615;&#22659;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24335;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24341;&#21457;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#26426;&#22120;&#20154;&#25216;&#26415;&#20132;&#21449;&#39046;&#22495;&#20013;&#30340;&#19968;&#31995;&#21015;&#25361;&#25112;&#24615;&#20219;&#21153;&#12290;&#34429;&#28982;&#35768;&#22810;&#26041;&#27861;&#21644;&#20197;&#21069;&#30340;&#35843;&#26597;&#36861;&#27714;&#24050;&#23558;&#20854;&#20013;&#19968;&#20004;&#20010;&#32500;&#24230;&#36827;&#34892;&#20102;&#25551;&#36848;&#65292;&#20294;&#36824;&#27809;&#26377;&#23545;&#25152;&#26377;&#19977;&#20010;&#32500;&#24230;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#32771;&#34385;&#36825;&#20123;&#20027;&#39064;&#30340;&#32452;&#21512;&#65292;&#26356;&#22810;&#30340;&#20851;&#27880;&#28857;&#25918;&#22312;&#25551;&#36848;&#24403;&#21069;&#30340;&#20307;&#31995;&#32467;&#26500;&#26041;&#27861;&#19978;&#65292;&#32780;&#19981;&#26159;&#35828;&#26126;&#35813;&#39046;&#22495;&#30340;&#39640;&#23618;&#27425;&#25361;&#25112;&#21644;&#26426;&#20250;&#12290;&#22312;&#26412;&#27425;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20855;&#36523;&#35270;&#35273;&#35821;&#35328;&#35268;&#21010;&#65288;EVLP&#65289;&#20219;&#21153;&#65292;&#36825;&#26159;&#19968;&#31995;&#21015;&#37325;&#35201;&#30340;&#20855;&#36523;&#23548;&#33322;&#21644;&#25805;&#20316;&#38382;&#39064;&#65292;&#20849;&#21516;&#21033;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#29289;&#29702;&#29615;&#22659;&#20132;&#20114;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#31867;&#27861;&#26469;&#32479;&#19968;&#36825;&#20123;&#20219;&#21153;&#65292;&#24182;&#23545;&#24403;&#21069;&#30340;&#21644;&#26032;&#30340;&#31639;&#27861;&#24212;&#29992;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#21644;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in the areas of Multimodal Machine Learning and Artificial Intelligence (AI) have led to the development of challenging tasks at the intersection of Computer Vision, Natural Language Processing, and Robotics. Whereas many approaches and previous survey pursuits have characterised one or two of these dimensions, there has not been a holistic analysis at the center of all three. Moreover, even when combinations of these topics are considered, more focus is placed on describing, e.g., current architectural methods, as opposed to also illustrating high-level challenges and opportunities for the field. In this survey paper, we discuss Embodied Vision-Language Planning (EVLP) tasks, a family of prominent embodied navigation and manipulation problems that jointly leverage computer vision and natural language for interaction in physical environments. We propose a taxonomy to unify these tasks and provide an in-depth analysis and comparison of the current and new algorithmic app
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#22823;&#23567;&#12289;&#32467;&#26500;&#21270;&#21098;&#26525;&#12289;&#25512;&#26029;&#25928;&#29575;&#21644;&#25688;&#35201;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#20351;&#29992;&#19981;&#23545;&#31216;&#21098;&#26525;&#21487;&#22312;&#19981;&#22823;&#25439;&#22833;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#39640;&#25512;&#26029;&#25928;&#29575;&#32422;3&#20493;&#12290;</title><link>http://arxiv.org/abs/2304.02721</link><description>&lt;p&gt;
&#36229;&#36234;&#19981;&#23545;&#31216;&#24615;&#65306;&#32467;&#26500;&#21098;&#26525;&#25552;&#39640;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#30340;&#25512;&#26029;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
To Asymmetry and Beyond: Structured Pruning of Sequence to Sequence Models for Improved Inference Efficiency. (arXiv:2304.02721v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#22823;&#23567;&#12289;&#32467;&#26500;&#21270;&#21098;&#26525;&#12289;&#25512;&#26029;&#25928;&#29575;&#21644;&#25688;&#35201;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#20351;&#29992;&#19981;&#23545;&#31216;&#21098;&#26525;&#21487;&#22312;&#19981;&#22823;&#25439;&#22833;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#39640;&#25512;&#26029;&#25928;&#29575;&#32422;3&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#21040;&#24207;&#21015;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#36830;&#36143;&#65292;&#30456;&#20851;&#21644;&#31616;&#27905;&#30340;&#25277;&#35937;&#25688;&#35201;&#12290;&#20294;&#26159;&#65292;&#27169;&#22411;&#22823;&#23567;&#21487;&#33021;&#20351;&#24471;&#22312;&#24310;&#36831;&#25935;&#24863;&#25110; Web &#35268;&#27169;&#30340;&#23454;&#29616;&#20013;&#37096;&#32626;&#21464;&#24471;&#22256;&#38590;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#22823;&#23567;&#12289;&#32467;&#26500;&#21270;&#21098;&#26525;&#12289;&#25512;&#26029;&#25928;&#29575;&#21644;&#24191;&#27867;&#20351;&#29992;&#30340;&#25688;&#35201;&#25968;&#25454;&#38598;&#19978;&#30340;&#25688;&#35201;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#27169;&#22411;&#20934;&#30830;&#24615;&#19982;&#32534;&#30721;&#22120;&#22823;&#23567;&#26377;&#20851;&#65292;&#32780;&#25512;&#29702;&#25928;&#29575;&#19982;&#35299;&#30721;&#22120;&#26377;&#20851;&#12290;&#20351;&#29992;&#19981;&#23545;&#31216;&#21098;&#26525;&#21487;&#23548;&#33268;&#25512;&#26029;&#24310;&#36831;&#30340;&#36817;3&#20493;&#25552;&#39640;&#65292;Rouge-2&#30340;&#25439;&#22833;&#32422;&#20026;1&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#24179;&#22343;&#24615;&#33021;&#38477;&#20302;&#21644;&#19981;&#23545;&#31216;&#24615;&#30340;&#20316;&#29992;&#22312;&#27169;&#22411;&#22823;&#23567;&#21644;&#25968;&#25454;&#38598;&#21464;&#21270;&#26041;&#38754;&#26159;&#19968;&#33268;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequence-to-sequence language models can be used to produce abstractive summaries which are coherent, relevant, and concise. Still, model sizes can make deployment in latency-sensitive or web-scale implementations difficult. This paper studies the relationship between model size, structured pruning, inference efficiency, and summarization accuracy on widely used summarization datasets. We show that model accuracy is tied to the encoder size while inference efficiency is connected to the decoder. Using asymmetric pruning can lead to nearly 3x improvement in inference latency with ~1 point loss in Rouge-2. Moreover, we find both the average degradation and the role of asymmetry to be consistent across model sizes and variations in datasets.
&lt;/p&gt;</description></item><item><title>SPIRES&#26159;&#19968;&#31181;&#26032;&#30340;&#30693;&#35782;&#25552;&#21462;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#36890;&#29992;&#26597;&#35810;&#22238;&#31572;&#65292;&#33021;&#22815;&#22635;&#20805;&#22797;&#26434;&#30340;&#30693;&#35782;&#24211;&#32780;&#26080;&#38656;&#26174;&#24335;&#35757;&#32451;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2304.02711</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#25552;&#31034;&#35810;&#38382;&#19982;&#36882;&#24402;&#35821;&#20041;&#25552;&#21462;&#65288;SPIRES&#65289;&#65306;&#20351;&#29992;&#38646;&#26679;&#26412;&#23398;&#20064;&#22635;&#20805;&#30693;&#35782;&#24211;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Structured prompt interrogation and recursive extraction of semantics (SPIRES): A method for populating knowledge bases using zero-shot learning. (arXiv:2304.02711v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02711
&lt;/p&gt;
&lt;p&gt;
SPIRES&#26159;&#19968;&#31181;&#26032;&#30340;&#30693;&#35782;&#25552;&#21462;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#36890;&#29992;&#26597;&#35810;&#22238;&#31572;&#65292;&#33021;&#22815;&#22635;&#20805;&#22797;&#26434;&#30340;&#30693;&#35782;&#24211;&#32780;&#26080;&#38656;&#26174;&#24335;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#24314;&#30693;&#35782;&#24211;&#21644;&#26412;&#20307;&#26159;&#19968;&#39033;&#32791;&#26102;&#30340;&#20219;&#21153;&#65292;&#20381;&#36182;&#20110;&#25163;&#21160;&#31649;&#29702;&#12290;AI / NLP&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#19987;&#19994;&#31574;&#23637;&#20154;&#22635;&#20805;&#36825;&#20123;&#30693;&#35782;&#24211;&#65292;&#20294;&#24403;&#21069;&#26041;&#27861;&#20381;&#36182;&#20110;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#19988;&#19981;&#33021;&#22635;&#20805;&#20219;&#24847;&#22797;&#26434;&#30340;&#23884;&#22871;&#30693;&#35782;&#27169;&#24335;&#12290;&#22312;&#36825;&#37324;&#25105;&#20204;&#25552;&#20986;&#20102;Structured Prompt Interrogation and Recursive Extraction of Semantics&#65288;SPIRES&#65289;&#65292;&#19968;&#31181;&#30693;&#35782;&#25552;&#21462;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25191;&#34892;&#38646;&#26679;&#26412;&#23398;&#20064;&#65288;ZSL&#65289;&#21644;&#36890;&#29992;&#26597;&#35810;&#22238;&#31572;&#65292;&#20197;&#21450;&#20174;&#28789;&#27963;&#25552;&#31034;&#36820;&#22238;&#31526;&#21512;&#25351;&#23450;&#27169;&#24335;&#30340;&#20449;&#24687;&#12290; SPIRES&#38024;&#23545;&#32473;&#23450;&#30340;&#35814;&#32454;&#29992;&#25143;&#23450;&#20041;&#30340;&#30693;&#35782;&#27169;&#24335;&#21644;&#36755;&#20837;&#25991;&#26412;&#65292;&#23545;GPT-3+&#25191;&#34892;&#36882;&#24402;&#25552;&#31034;&#35810;&#38382;&#65292;&#20197;&#33719;&#24471;&#19982;&#25552;&#20379;&#30340;&#27169;&#24335;&#21305;&#37197;&#30340;&#19968;&#32452;&#21709;&#24212;&#12290; SPIRES&#20351;&#29992;&#29616;&#26377;&#30340;&#26412;&#20307;&#21644;&#35789;&#27719;&#34920;&#20026;&#25152;&#26377;&#21305;&#37197;&#20803;&#32032;&#25552;&#20379;&#26631;&#35782;&#31526;&#12290; &#25105;&#20204;&#25552;&#20379;&#20102;&#22312;&#19981;&#21516;&#39046;&#22495;&#65288;&#21253;&#25324;&#38899;&#20048;&#65292;&#20307;&#32946;&#21644;&#25919;&#27835;&#65289;&#20013;&#20351;&#29992;SPIRES&#30340;&#31034;&#20363;&#65292;&#23637;&#31034;&#20102;&#20854;&#33021;&#22815;&#22635;&#20805;&#22797;&#26434;&#30340;&#30693;&#35782;&#24211;&#32780;&#26080;&#38656;&#26174;&#24335;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Creating knowledge bases and ontologies is a time consuming task that relies on a manual curation. AI/NLP approaches can assist expert curators in populating these knowledge bases, but current approaches rely on extensive training data, and are not able to populate arbitrary complex nested knowledge schemas.  Here we present Structured Prompt Interrogation and Recursive Extraction of Semantics (SPIRES), a Knowledge Extraction approach that relies on the ability of Large Language Models (LLMs) to perform zero-shot learning (ZSL) and general-purpose query answering from flexible prompts and return information conforming to a specified schema. Given a detailed, user-defined knowledge schema and an input text, SPIRES recursively performs prompt interrogation against GPT-3+ to obtain a set of responses matching the provided schema. SPIRES uses existing ontologies and vocabularies to provide identifiers for all matched elements.  We present examples of use of SPIRES in different domains, inc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;ACTION++&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;&#35299;&#21078;&#23545;&#27604;&#26469;&#25913;&#21892;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2304.02689</link><description>&lt;p&gt;
ACTION++&#65306;&#20351;&#29992;&#33258;&#36866;&#24212;&#35299;&#21078;&#23545;&#27604;&#24230;&#25913;&#21892;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
ACTION++: Improving Semi-supervised Medical Image Segmentation with Adaptive Anatomical Contrast. (arXiv:2304.02689v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;ACTION++&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;&#35299;&#21078;&#23545;&#27604;&#26469;&#25913;&#21892;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#25968;&#25454;&#36890;&#24120;&#34920;&#29616;&#20026;&#38271;&#23614;&#20998;&#24067;&#65292;&#23384;&#22312;&#20005;&#37325;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#65292;&#36825;&#33258;&#28982;&#23548;&#33268;&#23569;&#25968;&#31867;&#21035;&#65288;&#21363;&#36793;&#30028;&#21306;&#22495;&#25110;&#32597;&#35265;&#29289;&#20307;&#65289;&#30340;&#20998;&#31867;&#22256;&#38590;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#36890;&#36807;&#37197;&#22791;&#26080;&#30417;&#30563;&#23545;&#27604;&#26631;&#20934;&#65292;&#22312;&#38271;&#23614;&#22330;&#26223;&#20013;&#26174;&#30528;&#25913;&#36827;&#20102;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290;&#28982;&#32780;&#65292;&#22312;&#31867;&#21035;&#20998;&#24067;&#20063;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#26631;&#35760;&#25968;&#25454;&#37096;&#20998;&#20013;&#65292;&#23427;&#20204;&#30340;&#34920;&#29616;&#20173;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ACTION++&#65292;&#19968;&#31181;&#25913;&#36827;&#30340;&#20855;&#26377;&#33258;&#36866;&#24212;&#35299;&#21078;&#23545;&#27604;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#21322;&#30417;&#30563;&#21307;&#23398;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical data often exhibits long-tail distributions with heavy class imbalance, which naturally leads to difficulty in classifying the minority classes (i.e., boundary regions or rare objects). Recent work has significantly improved semi-supervised medical image segmentation in long-tailed scenarios by equipping them with unsupervised contrastive criteria. However, it remains unclear how well they will perform in the labeled portion of data where class distribution is also highly imbalanced. In this work, we present ACTION++, an improved contrastive learning framework with adaptive anatomical contrast for semi-supervised medical segmentation. Specifically, we propose an adaptive supervised contrastive loss, where we first compute the optimal locations of class centers uniformly distributed on the embedding space (i.e., off-line), and then perform online contrastive matching training by encouraging different class features to adaptively match these distinct and uniformly distributed cla
&lt;/p&gt;</description></item><item><title>&#39044;&#27979;&#32534;&#30721;&#31639;&#27861;&#34987;&#35748;&#20026;&#26159;&#21453;&#21521;&#20256;&#25773;&#30340;&#19968;&#20010;&#26367;&#20195;&#26041;&#26696;&#65292;&#22312;&#31070;&#32463;&#24418;&#24577;&#23398;&#31995;&#32479;&#20013;&#20855;&#26377;&#28508;&#21147;&#12290;&#30740;&#31350;&#32773;&#36890;&#36807;&#20351;&#29992;&#29616;&#26377;&#30340; PC &#21464;&#20307;&#25506;&#35752;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#26102;&#38388;&#22797;&#26434;&#24230;&#19979;&#30028;&#65292;&#25581;&#31034;&#20102; PC &#30340;&#19968;&#20123;&#26377;&#36259;&#30340;&#29305;&#24615;&#65292;&#21253;&#25324;&#20854;&#31070;&#32463;&#29983;&#29289;&#23398;&#21487;&#34892;&#24615;&#21644;&#28508;&#22312;&#30340;&#36125;&#21494;&#26031;&#25512;&#29702;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2304.02658</link><description>&lt;p&gt;
&#39044;&#27979;&#32534;&#30721;&#20316;&#20026;&#31070;&#32463;&#24418;&#24577;&#23398;&#26367;&#20195;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Predictive Coding as a Neuromorphic Alternative to Backpropagation: A Critical Evaluation. (arXiv:2304.02658v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02658
&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#32534;&#30721;&#31639;&#27861;&#34987;&#35748;&#20026;&#26159;&#21453;&#21521;&#20256;&#25773;&#30340;&#19968;&#20010;&#26367;&#20195;&#26041;&#26696;&#65292;&#22312;&#31070;&#32463;&#24418;&#24577;&#23398;&#31995;&#32479;&#20013;&#20855;&#26377;&#28508;&#21147;&#12290;&#30740;&#31350;&#32773;&#36890;&#36807;&#20351;&#29992;&#29616;&#26377;&#30340; PC &#21464;&#20307;&#25506;&#35752;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#26102;&#38388;&#22797;&#26434;&#24230;&#19979;&#30028;&#65292;&#25581;&#31034;&#20102; PC &#30340;&#19968;&#20123;&#26377;&#36259;&#30340;&#29305;&#24615;&#65292;&#21253;&#25324;&#20854;&#31070;&#32463;&#29983;&#29289;&#23398;&#21487;&#34892;&#24615;&#21644;&#28508;&#22312;&#30340;&#36125;&#21494;&#26031;&#25512;&#29702;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21521;&#20256;&#25773;&#24050;&#32463;&#24555;&#36895;&#25104;&#20026;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#20027;&#35201;&#23398;&#20998;&#20998;&#37197;&#31639;&#27861;&#12290;&#26368;&#36817;&#65292;&#35745;&#31639;&#31070;&#32463;&#31185;&#23398;&#36215;&#28304;&#20110;&#30340;&#20462;&#25913;&#24418;&#24335;&#30340;&#39044;&#27979;&#32534;&#30721;&#65288;PC&#65289;&#24050;&#34987;&#34920;&#26126;&#21487;&#20197;&#24471;&#21040;&#19982;&#21453;&#21521;&#20256;&#25773;&#23436;&#20840;&#30456;&#21516;&#25110;&#36817;&#20284;&#30456;&#31561;&#30340;&#21442;&#25968;&#26356;&#26032;&#12290;&#30001;&#20110;&#36825;&#31181;&#32852;&#31995;&#65292;&#26377;&#20154;&#35748;&#20026;PC&#21487;&#20197;&#20316;&#20026;&#21453;&#21521;&#20256;&#25773;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#24182;&#20855;&#26377;&#26377;&#21033;&#30340;&#24615;&#36136;&#65292;&#26377;&#21161;&#20110;&#22312;&#31070;&#32463;&#24418;&#24577;&#23398;&#31995;&#32479;&#20013;&#23454;&#29616;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20351;&#29992;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#19981;&#21516;&#30340;&#24403;&#20195;PC&#21464;&#20307;&#26469;&#25506;&#35752;&#36825;&#20123;&#22768;&#26126;&#12290;&#25105;&#20204;&#33719;&#24471;&#20102;&#36825;&#20123;PC&#21464;&#20307;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#19979;&#30028;&#65292;&#24182;&#26174;&#31034;&#23427;&#20204;&#20302;&#20110;&#21453;&#21521;&#20256;&#25773;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#36825;&#20123;&#21464;&#20307;&#30340;&#20027;&#35201;&#23646;&#24615;&#65292;&#36825;&#20123;&#23646;&#24615;&#28041;&#21450;&#31070;&#32463;&#29983;&#29289;&#23398;&#30340;&#21487;&#34892;&#24615;&#21644;&#23427;&#20204;&#30340;&#35299;&#37322;&#65292;&#23588;&#20854;&#26159;&#20174;&#26631;&#20934;PC&#20316;&#20026;&#28508;&#22312;&#27010;&#29575;&#27169;&#22411;&#30340;&#21464;&#20998;&#36125;&#21494;&#26031;&#31639;&#27861;&#30340;&#35282;&#24230;&#26469;&#30475;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;PC&#20316;&#20026;&#31070;&#32463;&#24418;&#24577;&#23398;&#31995;&#32479;&#20013;&#21453;&#21521;&#20256;&#25773;&#26367;&#20195;&#26041;&#26696;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backpropagation has rapidly become the workhorse credit assignment algorithm for modern deep learning methods. Recently, modified forms of predictive coding (PC), an algorithm with origins in computational neuroscience, have been shown to result in approximately or exactly equal parameter updates to those under backpropagation. Due to this connection, it has been suggested that PC can act as an alternative to backpropagation with desirable properties that may facilitate implementation in neuromorphic systems. Here, we explore these claims using the different contemporary PC variants proposed in the literature. We obtain time complexity bounds for these PC variants which we show are lower-bounded by backpropagation. We also present key properties of these variants that have implications for neurobiological plausibility and their interpretations, particularly from the perspective of standard PC as a variational Bayes algorithm for latent probabilistic models. Our findings shed new light 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#38598;&#25104;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#26234;&#33021;&#29305;&#24449;&#34701;&#21512;&#26469;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#22343;&#21462;&#24471;&#20102;&#36229;&#36234;&#22522;&#32447;&#27169;&#22411;&#21644;&#20256;&#32479;&#29305;&#24449;&#34701;&#21512;&#25216;&#26415;&#30340;&#23454;&#39564;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.02653</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#38598;&#25104;&#23398;&#20064;&#65306;&#36890;&#36807;&#26234;&#33021;&#29305;&#24449;&#34701;&#21512;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#30340;&#26041;&#27861;&#35770;
&lt;/p&gt;
&lt;p&gt;
Adaptive Ensemble Learning: Boosting Model Performance through Intelligent Feature Fusion in Deep Neural Networks. (arXiv:2304.02653v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02653
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#38598;&#25104;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#26234;&#33021;&#29305;&#24449;&#34701;&#21512;&#26469;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#22343;&#21462;&#24471;&#20102;&#36229;&#36234;&#22522;&#32447;&#27169;&#22411;&#21644;&#20256;&#32479;&#29305;&#24449;&#34701;&#21512;&#25216;&#26415;&#30340;&#23454;&#39564;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#38598;&#25104;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#38598;&#25104;&#23398;&#20064;&#25216;&#26415;&#26234;&#33021;&#34701;&#21512;&#29305;&#24449;&#26469;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#35813;&#26694;&#26550;&#23558;&#38598;&#25104;&#23398;&#20064;&#31574;&#30053;&#19982;&#28145;&#24230;&#23398;&#20064;&#32467;&#26500;&#30456;&#32467;&#21512;&#65292;&#21019;&#24314;&#20986;&#26356;&#20026;&#20581;&#22766;&#21644;&#36866;&#24212;&#24615;&#24378;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#22788;&#29702;&#19981;&#21516;&#39046;&#22495;&#30340;&#22797;&#26434;&#20219;&#21153;&#12290;&#36890;&#36807;&#21033;&#29992;&#26234;&#33021;&#29305;&#24449;&#34701;&#21512;&#26041;&#27861;&#65292;&#33258;&#36866;&#24212;&#38598;&#25104;&#23398;&#20064;&#26694;&#26550;&#29983;&#25104;&#26356;&#20026;&#20855;&#26377;&#36776;&#21035;&#21147;&#21644;&#26377;&#25928;&#24615;&#30340;&#29305;&#24449;&#34920;&#36798;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#20316;&#32773;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#21644;&#35780;&#20272;&#65292;&#21253;&#25324;&#22270;&#20687;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22522;&#20110;&#22270;&#30340;&#23398;&#20064;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#33258;&#36866;&#24212;&#38598;&#25104;&#23398;&#20064;&#26694;&#26550;&#22987;&#32456;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#21644;&#20256;&#32479;&#29305;&#24449;&#34701;&#21512;&#25216;&#26415;&#65292;&#31361;&#26174;&#20854;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#24615;&#33021;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present an Adaptive Ensemble Learning framework that aims to boost the performance of deep neural networks by intelligently fusing features through ensemble learning techniques. The proposed framework integrates ensemble learning strategies with deep learning architectures to create a more robust and adaptable model capable of handling complex tasks across various domains. By leveraging intelligent feature fusion methods, the Adaptive Ensemble Learning framework generates more discriminative and effective feature representations, leading to improved model performance and generalization capabilities.  We conducted extensive experiments and evaluations on several benchmark datasets, including image classification, object detection, natural language processing, and graph-based learning tasks. The results demonstrate that the proposed framework consistently outperforms baseline models and traditional feature fusion techniques, highlighting its effectiveness in enhancing d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#22823;&#22411;&#22270;&#20687;&#27169;&#22411;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#29992;&#20110;&#32954;&#30284;&#35786;&#26029;&#30340;&#22810;&#20219;&#21153;CT&#22823;&#22411;&#22270;&#20687;&#25991;&#26412;&#65288;LIT&#65289;&#27169;&#22411;&#65292;&#33021;&#24456;&#22909;&#22320;&#25191;&#34892;&#32954;&#37096;CT&#20998;&#21106;&#31561;&#22810;&#20010;&#21307;&#23398;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2304.02649</link><description>&lt;p&gt;
&#24102;&#26377;&#22823;&#22411;&#22270;&#20687;&#25991;&#26412;&#65288;LIT&#65289;&#27169;&#22411;&#30340;CT&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CT Multi-Task Learning with a Large Image-Text (LIT) Model. (arXiv:2304.02649v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#22823;&#22411;&#22270;&#20687;&#27169;&#22411;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#29992;&#20110;&#32954;&#30284;&#35786;&#26029;&#30340;&#22810;&#20219;&#21153;CT&#22823;&#22411;&#22270;&#20687;&#25991;&#26412;&#65288;LIT&#65289;&#27169;&#22411;&#65292;&#33021;&#24456;&#22909;&#22320;&#25191;&#34892;&#32954;&#37096;CT&#20998;&#21106;&#31561;&#22810;&#20010;&#21307;&#23398;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19981;&#20165;&#33021;&#22815;&#25903;&#25345;&#22810;&#31181;&#35821;&#35328;&#20219;&#21153;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#20316;&#20026;&#19981;&#21516;&#39046;&#22495;&#30340;&#36890;&#29992;&#25509;&#21475;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#36824;&#27809;&#26377;&#35777;&#26126;&#22914;&#20309;&#23558;LLM&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#25104;&#21151;&#26377;&#25928;&#22320;&#36716;&#21270;&#20026;&#28041;&#21450;&#39640;&#32500;&#21644;&#22810;&#27169;&#24577;&#21307;&#23398;&#22270;&#20687;&#30340;&#21307;&#23398;&#25104;&#20687;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#19968;&#39033;&#21487;&#34892;&#24615;&#30740;&#31350;&#65292;&#36890;&#36807;&#32452;&#21512;LLM&#21644;&#22823;&#22411;&#22270;&#20687;&#27169;&#22411;&#65288;LIM&#65289;&#65292;&#24314;&#31435;&#22810;&#20219;&#21153;CT&#22823;&#22411;&#22270;&#20687;&#25991;&#26412;&#65288;LIT&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#32954;&#30284;&#35786;&#26029;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LLM&#21644;LIM&#29992;&#20316;&#32534;&#30721;&#22120;&#65292;&#26681;&#25454;&#29305;&#23450;&#20219;&#21153;&#30340;&#25991;&#26412;&#25552;&#31034;&#26469;&#24863;&#30693;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#20174;&#32780;&#21327;&#21516;&#20316;&#29992;&#20110;&#22810;&#28304;&#20449;&#24687;&#21644;&#20219;&#21153;&#29305;&#23450;&#21644;&#24739;&#32773;&#29305;&#23450;&#30340;&#20808;&#39564;&#65292;&#20197;&#20248;&#21270;&#35786;&#26029;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;LIT&#27169;&#22411;&#21644;&#30456;&#20851;&#25216;&#26415;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#23558;&#37325;&#28857;&#35780;&#20272;3D&#32954;&#37096;CT&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;LIT&#27169;&#22411;&#33021;&#22815;&#24456;&#22909;&#22320;&#25191;&#34892;&#22810;&#39033;&#21307;&#23398;&#20219;&#21153;&#65292;&#21253;&#25324;&#32954;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLM) not only empower multiple language tasks but also serve as a general interface across different spaces. Up to now, it has not been demonstrated yet how to effectively translate the successes of LLMs in the computer vision field to the medical imaging field which involves high-dimensional and multi-modal medical images. In this paper, we report a feasibility study of building a multi-task CT large image-text (LIT) model for lung cancer diagnosis by combining an LLM and a large image model (LIM). Specifically, the LLM and LIM are used as encoders to perceive multi-modal information under task-specific text prompts, which synergizes multi-source information and task-specific and patient-specific priors for optimized diagnostic performance. The key components of our LIT model and associated techniques are evaluated with an emphasis on 3D lung CT analysis. Our initial results show that the LIT model performs multiple medical tasks well, including lung segmentatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25277;&#35937;&#30340;&#20998;&#26512;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#38754;&#20307;&#27010;&#29575;&#28151;&#21512;&#31995;&#32479;&#30340;&#27010;&#29575;&#31283;&#23450;&#24615;&#20998;&#26512;&#65292;&#25104;&#21151;&#22320;&#39564;&#35777;&#20102;&#21508;&#31181;&#32500;&#24230;&#21644;&#35268;&#27169;&#30340;PPHS&#30340;&#27010;&#29575;&#31283;&#23450;&#24615;&#26041;&#38754;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.02647</link><description>&lt;p&gt;
&#22522;&#20110;&#25277;&#35937;&#30340;&#22810;&#38754;&#20307;&#27010;&#29575;&#28151;&#21512;&#31995;&#32479;&#30340;&#27010;&#29575;&#31283;&#23450;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Abstraction-based Probabilistic Stability Analysis of Polyhedral Probabilistic Hybrid Systems. (arXiv:2304.02647v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02647
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25277;&#35937;&#30340;&#20998;&#26512;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#38754;&#20307;&#27010;&#29575;&#28151;&#21512;&#31995;&#32479;&#30340;&#27010;&#29575;&#31283;&#23450;&#24615;&#20998;&#26512;&#65292;&#25104;&#21151;&#22320;&#39564;&#35777;&#20102;&#21508;&#31181;&#32500;&#24230;&#21644;&#35268;&#27169;&#30340;PPHS&#30340;&#27010;&#29575;&#31283;&#23450;&#24615;&#26041;&#38754;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31867;&#38543;&#26426;&#28151;&#21512;&#31995;&#32479;&#30340;&#27010;&#29575;&#31283;&#23450;&#24615;&#20998;&#26512;&#38382;&#39064;&#65292;&#21363;&#22810;&#38754;&#20307;&#27010;&#29575;&#28151;&#21512;&#31995;&#32479;&#65288;PPHS&#65289;&#65292;&#20854;&#20013;&#27969;&#21160;&#21160;&#24577;&#30001;&#22810;&#38754;&#20307;&#21253;&#21547;&#32473;&#20986;&#65292;&#22312;&#23427;&#20204;&#30340;&#19981;&#21464;&#21306;&#22495;&#30340;&#36793;&#30028;&#22788;&#65292;&#31163;&#25955;&#20999;&#25442;&#20197;&#27010;&#29575;&#21457;&#29983;&#65292;&#36830;&#32493;&#29366;&#24577;&#22312;&#20999;&#25442;&#26399;&#38388;&#19981;&#34987;&#37325;&#32622;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25277;&#35937;&#30340;&#20998;&#26512;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;&#26500;&#24314;&#26377;&#38480;&#30340;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#65292;&#20351;&#24471;&#23545;&#26377;&#38480;MDP&#19978;&#30340;&#26576;&#20123;&#23646;&#24615;&#30340;&#39564;&#35777;&#20445;&#35777;&#20102;PPHS&#30340;&#27010;&#29575;&#31283;&#23450;&#24615;&#28385;&#36275;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#29992;&#20110;&#39564;&#35777;MDP&#19978;&#30456;&#24212;&#30340;&#23646;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20998;&#26512;&#34920;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#25104;&#21151;&#22320;&#39564;&#35777;&#21508;&#31181;&#32500;&#24230;&#21644;&#35268;&#27169;&#30340;PPHS&#30340;&#27010;&#29575;&#31283;&#23450;&#24615;&#26041;&#38754;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider the problem of probabilistic stability analysis of a subclass of Stochastic Hybrid Systems, namely, Polyhedral Probabilistic Hybrid Systems (PPHS), where the flow dynamics is given by a polyhedral inclusion, the discrete switching between modes happens probabilistically at the boundaries of their invariant regions and the continuous state is not reset during switching. We present an abstraction-based analysis framework that consists of constructing a finite Markov Decision Processes (MDP) such that verification of certain property on the finite MDP ensures the satisfaction of probabilistic stability on the PPHS. Further, we present a polynomial-time algorithm for verifying the corresponding property on the MDP. Our experimental analysis demonstrates the feasibility of the approach in successfully verifying probabilistic stability on PPHS of various dimensions and sizes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#27979;&#39564;&#30340;&#30693;&#35782;&#36861;&#36394;&#27169;&#22411;&#65288;QKT&#65289;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#26681;&#25454;&#23398;&#29983;&#22522;&#20110;&#27979;&#39564;&#30340;&#23398;&#20064;&#20132;&#20114;&#26469;&#30417;&#27979;&#20854;&#30693;&#35782;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2304.02413</link><description>&lt;p&gt;
&#22522;&#20110;&#27979;&#39564;&#30340;&#30693;&#35782;&#36861;&#36394;
&lt;/p&gt;
&lt;p&gt;
Quiz-based Knowledge Tracing. (arXiv:2304.02413v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02413
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#27979;&#39564;&#30340;&#30693;&#35782;&#36861;&#36394;&#27169;&#22411;&#65288;QKT&#65289;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#26681;&#25454;&#23398;&#29983;&#22522;&#20110;&#27979;&#39564;&#30340;&#23398;&#20064;&#20132;&#20114;&#26469;&#30417;&#27979;&#20854;&#30693;&#35782;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#36861;&#36394;&#65288;KT&#65289;&#26088;&#22312;&#26681;&#25454;&#23398;&#20064;&#32773;&#19982;&#22312;&#32447;&#23398;&#20064;&#31995;&#32479;&#65288;OIS&#65289;&#20013;&#19981;&#21516;&#32451;&#20064;&#30340;&#23398;&#20064;&#20132;&#20114;&#26469;&#35780;&#20272;&#20854;&#19981;&#26029;&#21457;&#23637;&#30340;&#30693;&#35782;&#29366;&#24577;&#65292;&#36825;&#23545;&#20110;&#25903;&#25345;&#38543;&#21518;&#30340;&#26234;&#33021;&#26381;&#21153;&#65292;&#22914;&#20010;&#24615;&#21270;&#23398;&#20064;&#36164;&#28304;&#25512;&#33616;&#65292;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#20915;&#31574;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#20154;&#21592;&#24191;&#27867;&#30740;&#31350;&#20102;KT&#24182;&#24320;&#21457;&#20102;&#35768;&#22810;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#20551;&#35774;&#23398;&#29983;&#30340;&#21382;&#21490;&#20132;&#20114;&#22312;&#36830;&#32493;&#24207;&#21015;&#20013;&#22343;&#21248;&#20998;&#24067;&#65292;&#24573;&#30053;&#20102;&#23454;&#38469;&#20132;&#20114;&#24207;&#21015;&#26159;&#22522;&#20110;&#19968;&#31995;&#21015;&#20855;&#26377;&#28165;&#26224;&#36793;&#30028;&#30340;&#27979;&#39564;&#32452;&#32455;&#30340;&#20107;&#23454;&#65292;&#22312;&#19968;&#20010;&#27979;&#39564;&#20869;&#30340;&#20132;&#20114;&#36830;&#32493;&#23436;&#25104;&#65292;&#20294;&#26159;&#36328;&#19981;&#21516;&#27979;&#39564;&#30340;&#20132;&#20114;&#26159;&#31163;&#25955;&#30340;&#65292;&#21487;&#33021;&#20250;&#38388;&#38548;&#25968;&#22825;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#27979;&#39564;&#30340;&#30693;&#35782;&#36861;&#36394;&#65288;QKT&#65289;&#27169;&#22411;&#65292;&#26681;&#25454;&#23398;&#29983;&#22522;&#20110;&#27979;&#39564;&#30340;&#23398;&#20064;&#20132;&#20114;&#26469;&#30417;&#27979;&#20854;&#30693;&#35782;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge tracing (KT) aims to assess individuals' evolving knowledge states according to their learning interactions with different exercises in online learning systems (OIS), which is critical in supporting decision-making for subsequent intelligent services, such as personalized learning source recommendation. Existing researchers have broadly studied KT and developed many effective methods. However, most of them assume that students' historical interactions are uniformly distributed in a continuous sequence, ignoring the fact that actual interaction sequences are organized based on a series of quizzes with clear boundaries, where interactions within a quiz are consecutively completed, but interactions across different quizzes are discrete and may be spaced over days. In this paper, we present the Quiz-based Knowledge Tracing (QKT) model to monitor students' knowledge states according to their quiz-based learning interactions. Specifically, as students' interactions within a quiz ar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#8212;&#8212;&#32467;&#26500;&#21270;&#20449;&#24687;&#25512;&#29702;&#65288;SIS&#65289;&#65292;&#21033;&#29992;GPT-3&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#25552;&#21462;&#26448;&#26009;&#31185;&#23398;&#35774;&#22791;&#23618;&#38754;&#30340;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39044;&#27979;PCE&#21644;&#21453;&#21521;&#39044;&#27979;&#21442;&#25968;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26448;&#26009;&#23398;&#20013;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.02213</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38053;&#21273;&#65306;&#29992;GPT&#35299;&#23494;&#26448;&#26009;&#31185;&#23398;&#30340;&#31192;&#23494;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Master Key: Unlocking the Secrets of Materials Science with GPT. (arXiv:2304.02213v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#8212;&#8212;&#32467;&#26500;&#21270;&#20449;&#24687;&#25512;&#29702;&#65288;SIS&#65289;&#65292;&#21033;&#29992;GPT-3&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#25552;&#21462;&#26448;&#26009;&#31185;&#23398;&#35774;&#22791;&#23618;&#38754;&#30340;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39044;&#27979;PCE&#21644;&#21453;&#21521;&#39044;&#27979;&#21442;&#25968;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26448;&#26009;&#23398;&#20013;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#8212;&#8212;&#32467;&#26500;&#21270;&#20449;&#24687;&#25512;&#29702;&#65288;SIS&#65289;&#65292;&#20197;&#35299;&#20915;&#26448;&#26009;&#31185;&#23398;&#35774;&#22791;&#23618;&#38754;&#20449;&#24687;&#25552;&#21462;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#29616;&#26377;&#30340;&#38041;&#38043;&#30719;&#22826;&#38451;&#33021;&#30005;&#27744;FAIR&#25968;&#25454;&#38598;&#23545;GPT-3&#36827;&#34892;&#24494;&#35843;&#65292;&#33719;&#24471;&#20102;91.8 F1&#24471;&#20998;&#65292;&#24182;&#26356;&#26032;&#20102;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#36804;&#20170;&#20026;&#27490;&#25152;&#26377;&#30456;&#20851;&#31185;&#23398;&#35770;&#25991;&#12290;&#25152;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#24050;&#34987;&#26684;&#24335;&#21270;&#21644;&#26631;&#20934;&#21270;&#65292;&#20351;&#24471;&#23427;&#21487;&#20197;&#30452;&#25509;&#20316;&#20026;&#21518;&#32493;&#25968;&#25454;&#20998;&#26512;&#30340;&#36755;&#20837;&#12290;&#36825;&#20010;&#29305;&#24615;&#23558;&#20351;&#26448;&#26009;&#31185;&#23398;&#23478;&#36890;&#36807;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#39046;&#22495;&#35780;&#35770;&#25991;&#31456;&#26469;&#24320;&#21457;&#20854;&#33258;&#24049;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#23454;&#39564;&#26469;&#39044;&#27979;PCE&#21644;&#21453;&#21521;&#39044;&#27979;&#21442;&#25968;&#65292;&#24182;&#33719;&#24471;&#20102;&#19982;DFT&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#36825;&#35777;&#26126;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20687;&#26448;&#26009;&#23398;&#23478;&#19968;&#26679;&#35780;&#21028;&#26448;&#26009;&#21644;&#35774;&#35745;&#26032;&#26448;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article presents a new NLP task called structured information inference (SIS) to address the complexities of information extraction at the device level in materials science. We accomplished this task by finetuning GPT-3 on a exsiting perovskite solar cell FAIR dataset with 91.8 F1-score and we updated the dataset with all related scientific papers up to now. The produced dataset is formatted and normalized, enabling its direct utilization as input in subsequent data analysis. This feature will enable materials scientists to develop their own models by selecting high-quality review papers within their domain. Furthermore, we designed experiments to predict PCE and reverse-predict parameters and obtained comparable performance with DFT, which demonstrates the potential of large language models to judge materials and design new materials like a materials scientist.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;MTDA&#26041;&#27861;&#65292;&#21517;&#20026;\emph{MEnsA}&#65292;&#21033;&#29992;&#28151;&#21512;&#38598;&#25104;&#24179;&#22343;&#26041;&#27861;&#25552;&#39640;&#20102;&#22495;&#33258;&#36866;&#24212;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.01554</link><description>&lt;p&gt;
\emph{MEnsA}: &#19977;&#32500;&#28857;&#20113;&#26080;&#30417;&#30563;&#22810;&#30446;&#26631;&#22495;&#33258;&#36866;&#24212;&#30340;&#28151;&#21512;&#38598;&#25104;&#24179;&#22343;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
\emph{MEnsA}: Mix-up Ensemble Average for Unsupervised Multi Target Domain Adaptation on 3D Point Clouds. (arXiv:2304.01554v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;MTDA&#26041;&#27861;&#65292;&#21517;&#20026;\emph{MEnsA}&#65292;&#21033;&#29992;&#28151;&#21512;&#38598;&#25104;&#24179;&#22343;&#26041;&#27861;&#25552;&#39640;&#20102;&#22495;&#33258;&#36866;&#24212;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;&#65288;UDA&#65289;&#35299;&#20915;&#20102;&#26631;&#35760;&#28304;&#22495;&#21644;&#26410;&#26631;&#35760;&#30446;&#26631;&#22495;&#20043;&#38388;&#20998;&#24067;&#24046;&#24322;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#21333;&#30446;&#26631;&#22495;&#33258;&#36866;&#24212;&#65288;STDA&#65289;&#24050;&#32463;&#22312;2D&#21644;3D&#35270;&#35273;&#25991;&#29486;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#26159;&#22312;3D&#25968;&#25454;&#30340;&#22810;&#30446;&#26631;&#22495;&#33258;&#36866;&#24212;&#65288;MTDA&#65289;&#26041;&#38754;&#20960;&#20046;&#27809;&#26377;&#24471;&#21040;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;\emph{{\bf M}ixup {\bf Ens}emble {\bf A}verage}&#25110;&#31616;&#31216;{\bf \emph{MEnsA}}&#30340;&#28151;&#21512;&#38598;&#25104;&#24179;&#22343;&#26041;&#27861;&#65292;&#23558;&#25152;&#26377;&#39046;&#22495;&#30340;&#29305;&#24449;&#34920;&#31034;&#28151;&#21512;&#22312;&#19968;&#36215;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#22495;&#33258;&#36866;&#24212;&#24615;&#33021;&#30340;MTDA&#22522;&#32447;&#12290;&#36890;&#36807;&#28151;&#21512;&#34920;&#31034;&#65292;&#25105;&#20204;&#20351;&#29992;&#22495;&#20998;&#31867;&#22120;&#22312;&#20849;&#20139;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#25552;&#39640;&#20102;&#28304;&#22495;&#29305;&#24449;&#34920;&#31034;&#19982;&#30446;&#26631;&#22495;&#29305;&#24449;&#34920;&#31034;&#30340;&#21306;&#20998;&#33021;&#21147;&#12290;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;PointDA-10&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised domain adaptation (UDA) addresses the problem of distribution shift between the unlabeled target domain and labelled source domain. While the single target domain adaptation (STDA) is well studied in both 2D and 3D vision literature, multi-target domain adaptation (MTDA) is barely explored for 3D data despite its wide real-world applications such as autonomous driving systems for various geographical and climatic conditions. We establish an MTDA baseline for 3D point cloud data by proposing to mix the feature representations from all domains together to achieve better domain adaptation performance by an ensemble average, which we call \emph{{\bf M}ixup {\bf Ens}emble {\bf A}verage} or {\bf \emph{MEnsA}}. With the mixed representation, we use a domain classifier to improve at distinguishing the feature representations of source domain from those of target domains in a shared latent space. In extensive empirical validations on the challenging PointDA-10 dataset, we showcase 
&lt;/p&gt;</description></item><item><title>RARE&#26159;&#19968;&#31181;&#40065;&#26834;&#24615;&#25239;&#24178;&#25200;&#30340;&#25513;&#30721;&#22270;&#33258;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#22312;&#39640;&#38454;&#28508;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#36827;&#34892;&#25513;&#30721;&#21644;&#37325;&#26500;&#33410;&#28857;&#26679;&#26412;&#26469;&#25552;&#39640;&#25512;&#26029;&#25513;&#30721;&#25968;&#25454;&#30340;&#30830;&#23450;&#24615;&#21644;&#33258;&#30417;&#30563;&#26426;&#21046;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#20248;&#20110;&#29616;&#26377;&#30340;SGP&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.01507</link><description>&lt;p&gt;
RARE&#65306;&#40065;&#26834;&#24615;&#25239;&#24178;&#25200;&#30340;&#25513;&#30721;&#22270;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
RARE: Robust Masked Graph Autoencoder. (arXiv:2304.01507v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01507
&lt;/p&gt;
&lt;p&gt;
RARE&#26159;&#19968;&#31181;&#40065;&#26834;&#24615;&#25239;&#24178;&#25200;&#30340;&#25513;&#30721;&#22270;&#33258;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#22312;&#39640;&#38454;&#28508;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#36827;&#34892;&#25513;&#30721;&#21644;&#37325;&#26500;&#33410;&#28857;&#26679;&#26412;&#26469;&#25552;&#39640;&#25512;&#26029;&#25513;&#30721;&#25968;&#25454;&#30340;&#30830;&#23450;&#24615;&#21644;&#33258;&#30417;&#30563;&#26426;&#21046;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#20248;&#20110;&#29616;&#26377;&#30340;SGP&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25513;&#30721;&#22270;&#33258;&#32534;&#30721;&#22120;&#65288;MGAE&#65289;&#30001;&#20110;&#20854;&#31616;&#21333;&#21644;&#26377;&#25928;&#30340;&#29305;&#24615;&#65292;&#22312;&#33258;&#30417;&#30563;&#22270;&#39044;&#35757;&#32451;&#65288;SGP&#65289;&#26041;&#38754;&#24050;&#25104;&#20026;&#19968;&#31181;&#24456;&#26377;&#21069;&#36884;&#30340;&#33539;&#20363;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#21407;&#22987;&#25968;&#25454;&#31354;&#38388;&#20013;&#25191;&#34892;&#25513;&#30721;-&#37325;&#26500;&#25805;&#20316;&#65292;&#31867;&#20284;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#65292;&#32780;&#24573;&#30053;&#20102;&#22270;&#25968;&#25454;&#30340;&#37325;&#35201;&#38750;&#27431;&#20960;&#37324;&#24471;&#23646;&#24615;&#12290;&#32467;&#26524;&#65292;&#39640;&#24230;&#19981;&#31283;&#23450;&#30340;&#23616;&#37096;&#36830;&#25509;&#32467;&#26500;&#22823;&#22823;&#22686;&#21152;&#20102;&#25512;&#26029;&#25513;&#30721;&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#38477;&#20302;&#20102;&#21033;&#29992;&#33258;&#30417;&#30563;&#20449;&#21495;&#30340;&#21487;&#38752;&#24615;&#65292;&#23548;&#33268;&#19979;&#28216;&#35780;&#20272;&#20013;&#30340;&#34920;&#31034;&#25928;&#26524;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;SGP&#26041;&#27861;&#65292;&#31216;&#20026;Robust mAsked gRaph autoEncoder&#65288;RARE&#65289;&#65292;&#36890;&#36807;&#39640;&#38454;&#28508;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#26356;&#22810;&#30340;&#25513;&#30721;&#21644;&#37325;&#26500;&#33410;&#28857;&#26679;&#26412;&#26469;&#25552;&#39640;&#25512;&#26029;&#25513;&#30721;&#25968;&#25454;&#30340;&#30830;&#23450;&#24615;&#21644;&#33258;&#30417;&#30563;&#26426;&#21046;&#30340;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;RARE&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#22270;&#25968;&#25454;&#30340;&#20869;&#22312;&#32467;&#26500;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#20248;&#20110;&#29616;&#26377;&#30340;SGP&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked graph autoencoder (MGAE) has emerged as a promising self-supervised graph pre-training (SGP) paradigm due to its simplicity and effectiveness. However, existing efforts perform the mask-then-reconstruct operation in the raw data space as is done in computer vision (CV) and natural language processing (NLP) areas, while neglecting the important non-Euclidean property of graph data. As a result, the highly unstable local connection structures largely increase the uncertainty in inferring masked data and decrease the reliability of the exploited self-supervision signals, leading to inferior representations for downstream evaluations. To address this issue, we propose a novel SGP method termed Robust mAsked gRaph autoEncoder (RARE) to improve the certainty in inferring masked data and the reliability of the self-supervision mechanism by further masking and reconstructing node samples in the high-order latent feature space. Through both theoretical and empirical analyses, we have dis
&lt;/p&gt;</description></item><item><title>POLAR-Express &#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#20934;&#30830;&#30340;&#24418;&#24335;&#21487;&#36798;&#24615;&#20998;&#26512;&#24037;&#20855;&#65292;&#29992;&#20110;&#39564;&#35777;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#12290;&#23427;&#20351;&#29992; Taylor &#27169;&#22411;&#31639;&#26415;&#21644;&#36880;&#23618;&#20256;&#25773;&#25216;&#26415;&#65292;&#21487;&#20197;&#20998;&#26512;&#20855;&#26377;&#36830;&#32493;&#28608;&#27963;&#21151;&#33021;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#22312; ReLU &#28608;&#27963;&#20989;&#25968;&#19978;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#26377;&#25928;&#30340;&#31934;&#30830;&#20256;&#25773; TM &#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.01218</link><description>&lt;p&gt;
POLAR-Express: &#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#30340;&#39640;&#25928;&#20934;&#30830;&#24418;&#24335;&#21487;&#36798;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
POLAR-Express: Efficient and Precise Formal Reachability Analysis of Neural-Network Controlled Systems. (arXiv:2304.01218v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01218
&lt;/p&gt;
&lt;p&gt;
POLAR-Express &#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#20934;&#30830;&#30340;&#24418;&#24335;&#21487;&#36798;&#24615;&#20998;&#26512;&#24037;&#20855;&#65292;&#29992;&#20110;&#39564;&#35777;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#12290;&#23427;&#20351;&#29992; Taylor &#27169;&#22411;&#31639;&#26415;&#21644;&#36880;&#23618;&#20256;&#25773;&#25216;&#26415;&#65292;&#21487;&#20197;&#20998;&#26512;&#20855;&#26377;&#36830;&#32493;&#28608;&#27963;&#21151;&#33021;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#22312; ReLU &#28608;&#27963;&#20989;&#25968;&#19978;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#26377;&#25928;&#30340;&#31934;&#30830;&#20256;&#25773; TM &#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25361;&#25112;&#24615;&#30340;&#25511;&#21046;&#38382;&#39064;&#19978;&#65292;&#25198;&#28436;&#25511;&#21046;&#22120;&#35282;&#33394;&#30340;&#31070;&#32463;&#32593;&#32476; (NN) &#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#23454;&#39564;&#24615;&#33021;&#12290;&#20294;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479; (NNCS) &#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#28508;&#22312;&#37319;&#29992;&#20063;&#24341;&#36215;&#20102;&#26085;&#30410;&#22686;&#38271;&#30340;&#23545;&#36825;&#20123; NNCS &#23433;&#20840;&#24615;&#30340;&#25285;&#24551;&#65292;&#29305;&#21035;&#26159;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102; POLAR-Express&#65292;&#19968;&#31181;&#39640;&#25928;&#19988;&#20934;&#30830;&#30340;&#24418;&#24335;&#21487;&#36798;&#24615;&#20998;&#26512;&#24037;&#20855;&#65292;&#29992;&#20110;&#39564;&#35777; NNCS &#30340;&#23433;&#20840;&#24615;&#12290;POLAR-Express &#20351;&#29992; Taylor &#27169;&#22411;&#31639;&#26415;&#65292;&#36880;&#23618;&#27178;&#36328;&#31070;&#32463;&#32593;&#32476;&#26469;&#20256;&#25773; Taylor &#27169;&#22411; (TM) &#20197;&#35745;&#31639;&#31070;&#32463;&#32593;&#32476;&#20989;&#25968;&#30340;&#36817;&#20284;&#20540;&#12290;&#23427;&#21487;&#20197;&#29992;&#20110;&#20998;&#26512;&#20219;&#20309;&#20855;&#26377;&#36830;&#32493;&#28608;&#27963;&#21151;&#33021;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312; ReLU &#28608;&#27963;&#20989;&#25968;&#19978;&#26356;&#26377;&#25928;&#22320;&#31934;&#30830;&#20256;&#25773; TM &#30340;&#26032;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;POLAR-Express &#20026;&#36880;&#23618;&#20256;&#25773;&#25552;&#20379;&#20102;&#24182;&#34892;&#35745;&#31639;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks (NNs) playing the role of controllers have demonstrated impressive empirical performances on challenging control problems. However, the potential adoption of NN controllers in real-life applications also gives rise to a growing concern over the safety of these neural-network controlled systems (NNCSs), especially when used in safety-critical applications. In this work, we present POLAR-Express, an efficient and precise formal reachability analysis tool for verifying the safety of NNCSs. POLAR-Express uses Taylor model arithmetic to propagate Taylor models (TMs) across a neural network layer-by-layer to compute an overapproximation of the neural-network function. It can be applied to analyze any feed-forward neural network with continuous activation functions. We also present a novel approach to propagate TMs more efficiently and precisely across ReLU activation functions. In addition, POLAR-Express provides parallel computation support for the layer-by-layer propagation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;2022&#24180;&#22522;&#22240;&#19982;&#36827;&#21270;&#35745;&#31639;&#20250;&#35758;&#20030;&#21150;&#30340;&#31454;&#36187;&#65292;&#35780;&#20272;&#20102;&#31526;&#21495;&#22238;&#24402;&#30340;&#26032;&#26041;&#27861;&#22312;&#38754;&#23545;&#29616;&#23454;&#25968;&#25454;&#26102;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#35780;&#20272;&#30340;&#23454;&#38469;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.01117</link><description>&lt;p&gt;
&#25968;&#25454;&#31185;&#23398;&#20013;&#30340;&#21487;&#35299;&#37322;&#31526;&#21495;&#22238;&#24402;&#65306;2022&#24180;&#31454;&#36187;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Interpretable Symbolic Regression for Data Science: Analysis of the 2022 Competition. (arXiv:2304.01117v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01117
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;2022&#24180;&#22522;&#22240;&#19982;&#36827;&#21270;&#35745;&#31639;&#20250;&#35758;&#20030;&#21150;&#30340;&#31454;&#36187;&#65292;&#35780;&#20272;&#20102;&#31526;&#21495;&#22238;&#24402;&#30340;&#26032;&#26041;&#27861;&#22312;&#38754;&#23545;&#29616;&#23454;&#25968;&#25454;&#26102;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#35780;&#20272;&#30340;&#23454;&#38469;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;&#26159;&#23547;&#25214;&#33021;&#22815;&#20934;&#30830;&#25551;&#36848;&#30740;&#31350;&#29616;&#35937;&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#20027;&#35201;&#20248;&#21183;&#26159;&#36820;&#22238;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#32473;&#29992;&#25143;&#25552;&#20379;&#28145;&#21051;&#30340;&#35265;&#35299;&#12290;&#21382;&#21490;&#19978;&#65292;&#31526;&#21495;&#22238;&#24402;&#30340;&#22823;&#22810;&#25968;&#31639;&#27861;&#37117;&#22522;&#20110;&#36827;&#21270;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#20986;&#29616;&#20102;&#22823;&#37327;&#26032;&#30340;&#25552;&#26696;&#65292;&#36825;&#20123;&#25552;&#26696;&#20351;&#29992;&#20102;&#21015;&#20030;&#31639;&#27861;&#12289;&#28151;&#21512;&#32447;&#24615;&#25972;&#25968;&#35268;&#21010;&#12289;&#31070;&#32463;&#32593;&#32476;&#21644;&#36125;&#21494;&#26031;&#20248;&#21270;&#31561;&#26041;&#27861;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#20123;&#26032;&#26041;&#27861;&#22312;&#38754;&#23545;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#32463;&#24120;&#36935;&#21040;&#30340;&#19968;&#32452;&#24120;&#35265;&#25361;&#25112;&#26102;&#30340;&#34920;&#29616;&#22914;&#20309;&#65292;&#25105;&#20204;&#22312;2022&#24180;&#36951;&#20256;&#19982;&#36827;&#21270;&#35745;&#31639;&#20250;&#35758;&#19978;&#20030;&#21150;&#20102;&#19968;&#27425;&#31454;&#36187;&#65292;&#20854;&#20013;&#21253;&#21547;&#19981;&#21516;&#30340;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#65292;&#21442;&#36187;&#32773;&#23545;&#36825;&#20123;&#25968;&#25454;&#38598;&#26159;&#30450;&#27979;&#35797;&#30340;&#12290;&#23545;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#37096;&#20998;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#39046;&#22495;&#19987;&#23478;&#26469;&#35780;&#20272;&#20505;&#36873;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#23545;&#32467;&#26524;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symbolic regression searches for analytic expressions that accurately describe studied phenomena. The main attraction of this approach is that it returns an interpretable model that can be insightful to users. Historically, the majority of algorithms for symbolic regression have been based on evolutionary algorithms. However, there has been a recent surge of new proposals that instead utilize approaches such as enumeration algorithms, mixed linear integer programming, neural networks, and Bayesian optimization. In order to assess how well these new approaches behave on a set of common challenges often faced in real-world data, we hosted a competition at the 2022 Genetic and Evolutionary Computation Conference consisting of different synthetic and real-world datasets which were blind to entrants. For the real-world track, we assessed interpretability in a realistic way by using a domain expert to judge the trustworthiness of candidate models.We present an in-depth analysis of the result
&lt;/p&gt;</description></item><item><title>&#29992;&#29289;&#29702;&#24341;&#25806;&#24378;&#21046;&#23454;&#29616;3D&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#30340;&#29289;&#29702;&#21512;&#29702;&#24615;&#22312;&#23454;&#36341;&#20013;&#26377;&#24456;&#22823;&#22256;&#38590;&#12290;&#36825;&#31687;&#35770;&#25991;&#20013;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#30452;&#35273;&#29289;&#29702;&#30340;&#26041;&#27861;&#65292;&#20511;&#21161;&#21387;&#21147;&#28909;&#22270;&#12289;&#21387;&#21147;&#20013;&#24515;&#21644;&#36523;&#20307;&#36136;&#24515;&#31561;&#26415;&#35821;&#65292;&#22312;&#20272;&#35745;3D&#20154;&#20307;&#23039;&#24577;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#29289;&#29702;&#21512;&#29702;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.18246</link><description>&lt;p&gt;
&#22522;&#20110;&#30452;&#35273;&#29289;&#29702;&#30340;3D&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
3D Human Pose Estimation via Intuitive Physics. (arXiv:2303.18246v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18246
&lt;/p&gt;
&lt;p&gt;
&#29992;&#29289;&#29702;&#24341;&#25806;&#24378;&#21046;&#23454;&#29616;3D&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#30340;&#29289;&#29702;&#21512;&#29702;&#24615;&#22312;&#23454;&#36341;&#20013;&#26377;&#24456;&#22823;&#22256;&#38590;&#12290;&#36825;&#31687;&#35770;&#25991;&#20013;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#30452;&#35273;&#29289;&#29702;&#30340;&#26041;&#27861;&#65292;&#20511;&#21161;&#21387;&#21147;&#28909;&#22270;&#12289;&#21387;&#21147;&#20013;&#24515;&#21644;&#36523;&#20307;&#36136;&#24515;&#31561;&#26415;&#35821;&#65292;&#22312;&#20272;&#35745;3D&#20154;&#20307;&#23039;&#24577;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#29289;&#29702;&#21512;&#29702;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#20272;&#35745;&#20154;&#20307;&#23039;&#24577;&#26102;&#24448;&#24448;&#20250;&#20986;&#29616;&#19981;&#21512;&#29702;&#30340;&#36523;&#20307;&#20542;&#26012;&#12289;&#28014;&#21160;&#25110;&#31359;&#36879;&#22320;&#26495;&#30340;&#24773;&#20917;&#12290;&#36825;&#26679;&#30340;&#26041;&#27861;&#24573;&#35270;&#20102;&#36523;&#20307;&#36890;&#24120;&#30001;&#22330;&#26223;&#25903;&#25745;&#30340;&#20107;&#23454;&#12290;&#29289;&#29702;&#24341;&#25806;&#21487;&#20197;&#29992;&#26469;&#24378;&#21046;&#23454;&#29616;&#29289;&#29702;&#21512;&#29702;&#24615;&#65292;&#20294;&#36825;&#20123;&#24341;&#25806;&#19981;&#21487;&#24494;&#20998;&#65292;&#20381;&#36182;&#20110;&#19981;&#29616;&#23454;&#30340;&#20195;&#29702;&#29289;&#20307;&#65292;&#24182;&#19988;&#38590;&#20197;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;&#20248;&#21270;&#21644;&#23398;&#20064;&#26694;&#26550;&#20013;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#21033;&#29992;&#26032;&#39062;&#30340;&#30452;&#35273;&#29289;&#29702;&#65288;IP&#65289;&#26415;&#35821;&#65292;&#36825;&#20123;&#26415;&#35821;&#21487;&#20197;&#20174;&#19968;&#20010;&#19982;&#22330;&#26223;&#30456;&#20114;&#20316;&#29992;&#30340;3D SMPL&#36523;&#20307;&#20013;&#25512;&#26029;&#20986;&#26469;&#12290;&#21463;&#29983;&#29289;&#21147;&#23398;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25512;&#26029;&#20986;&#36523;&#20307;&#19978;&#30340;&#21387;&#21147;&#28909;&#22270;&#12289;&#28909;&#22270;&#19978;&#30340;&#21387;&#21147;&#20013;&#24515;&#65288;CoP&#65289;&#20197;&#21450;SMPL&#36523;&#20307;&#30340;&#36136;&#24515;&#12290;&#20511;&#21161;&#36825;&#20123;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;IPMAN&#65292;&#36890;&#36807;&#40723;&#21169;&#21512;&#29702;&#30340;&#22320;&#26495;&#25509;&#35302;&#21644;&#37325;&#21472;&#30340;CoP&#21644;CoM&#65292;&#22312;&#24425;&#33394;&#22270;&#20687;&#20013;&#20272;&#35745;&#19968;&#20010;&#8220;&#31283;&#23450;&#8221;&#30340;3D&#36523;&#20307;&#12290;&#25105;&#20204;&#30340;IP&#26415;&#35821;&#30452;&#35266;&#26131;&#25026;&#65292;&#26131;&#20110;&#23454;&#29616;&#65292;&#35745;&#31639;&#36895;&#24230;&#24555;&#65292;&#21487;&#24494;&#20998;&#65292;&#24182;&#19988;&#21487;&#20197;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;&#20248;&#21270;&#21644;&#22238;&#24402;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating 3D humans from images often produces implausible bodies that lean, float, or penetrate the floor. Such methods ignore the fact that bodies are typically supported by the scene. A physics engine can be used to enforce physical plausibility, but these are not differentiable, rely on unrealistic proxy bodies, and are difficult to integrate into existing optimization and learning frameworks. In contrast, we exploit novel intuitive-physics (IP) terms that can be inferred from a 3D SMPL body interacting with the scene. Inspired by biomechanics, we infer the pressure heatmap on the body, the Center of Pressure (CoP) from the heatmap, and the SMPL body's Center of Mass (CoM). With these, we develop IPMAN, to estimate a 3D body from a color image in a "stable" configuration by encouraging plausible floor contact and overlapping CoP and CoM. Our IP terms are intuitive, easy to implement, fast to compute, differentiable, and can be integrated into existing optimization and regression m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ViewRefer&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#35270;&#35282;&#30340;&#19977;&#32500;&#35270;&#35273;&#23450;&#20301;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#35270;&#35282;&#21407;&#22411;&#65292;&#20174;&#25991;&#26412;&#21644;3D&#27169;&#24577;&#20013;&#33719;&#21462;&#35270;&#35282;&#30693;&#35782;&#24182;&#22686;&#24378;&#26694;&#26550;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.16894</link><description>&lt;p&gt;
ViewRefer: &#22522;&#20110;GPT&#21644;&#26679;&#20363;&#24341;&#23548;&#30340;&#22810;&#35270;&#35282;&#30693;&#35782;&#22788;&#29702;&#30340;&#19977;&#32500;&#35270;&#35273;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
ViewRefer: Grasp the Multi-view Knowledge for 3D Visual Grounding with GPT and Prototype Guidance. (arXiv:2303.16894v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16894
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ViewRefer&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#35270;&#35282;&#30340;&#19977;&#32500;&#35270;&#35273;&#23450;&#20301;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#35270;&#35282;&#21407;&#22411;&#65292;&#20174;&#25991;&#26412;&#21644;3D&#27169;&#24577;&#20013;&#33719;&#21462;&#35270;&#35282;&#30693;&#35782;&#24182;&#22686;&#24378;&#26694;&#26550;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#22810;&#35270;&#35282;&#36755;&#20837;&#30340;3D&#22330;&#26223;&#65292;&#21487;&#20197;&#32531;&#35299;3D&#35270;&#35273;&#23450;&#20301;&#20013;&#30340;&#35270;&#35282;&#24046;&#24322;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#24573;&#30053;&#20102;&#23884;&#20837;&#22312;&#25991;&#26412;&#27169;&#24577;&#20013;&#30340;&#35270;&#35282;&#32447;&#32034;&#65292;&#24182;&#19988;&#26410;&#33021;&#26435;&#34913;&#19981;&#21516;&#35270;&#22270;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ViewRefer&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#35270;&#35282;&#30340;&#19977;&#32500;&#35270;&#35273;&#23450;&#20301;&#26694;&#26550;&#65292;&#25506;&#32034;&#22914;&#20309;&#20174;&#25991;&#26412;&#21644;3D&#27169;&#24577;&#20013;&#33719;&#21462;&#35270;&#35282;&#30693;&#35782;&#12290;&#20854;&#20013;&#65292;ViewRefer&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;GPT&#65289;&#30340;&#22810;&#26679;&#21270;&#35821;&#35328;&#30693;&#35782;&#65292;&#23558;&#21333;&#19968;&#30340;&#23450;&#20301;&#25991;&#26412;&#25193;&#23637;&#20026;&#22810;&#20010;&#20960;&#20309;&#19968;&#33268;&#30340;&#25551;&#36848;&#65307;&#21516;&#26102;&#65292;&#22312;3D&#27169;&#24577;&#20013;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;Transformer&#30340;&#34701;&#21512;&#27169;&#22359;&#21644;&#35270;&#22270;&#38388;&#27880;&#24847;&#21147;&#65292;&#20197;&#22686;&#24378;&#35270;&#22270;&#20043;&#38388;&#29289;&#20307;&#30340;&#20132;&#20114;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#32452;&#21487;&#23398;&#20064;&#30340;&#22810;&#35270;&#35282;&#21407;&#22411;&#65292;&#29992;&#20110;&#35760;&#24518;&#19981;&#21516;&#35270;&#35282;&#19979;&#30340;&#22330;&#26223;&#26080;&#20851;&#30693;&#35782;&#65292;&#20174;&#20004;&#20010;&#26041;&#38754;&#22686;&#24378;&#20102;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding 3D scenes from multi-view inputs has been proven to alleviate the view discrepancy issue in 3D visual grounding. However, existing methods normally neglect the view cues embedded in the text modality and fail to weigh the relative importance of different views. In this paper, we propose ViewRefer, a multi-view framework for 3D visual grounding exploring how to grasp the view knowledge from both text and 3D modalities. For the text branch, ViewRefer leverages the diverse linguistic knowledge of large-scale language models, e.g., GPT, to expand a single grounding text to multiple geometry-consistent descriptions. Meanwhile, in the 3D modality, a transformer fusion module with inter-view attention is introduced to boost the interaction of objects across views. On top of that, we further present a set of learnable multi-view prototypes, which memorize scene-agnostic knowledge for different views, and enhance the framework from two perspectives: a view-guided attention module 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#25968;&#25454;&#31574;&#30053;&#35299;&#20915;&#20102;&#40857;&#33292;&#20848;&#20316;&#29289;&#20998;&#21106;&#20013;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#40857;&#33292;&#20848;&#20316;&#29289;&#25104;&#29087;&#24230;&#20998;&#31867;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.11564</link><description>&lt;p&gt;
&#21033;&#29992;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#30340;&#28145;&#24230;&#23398;&#20064;&#25968;&#25454;&#31574;&#30053;&#36827;&#34892;&#40857;&#33292;&#20848;&#20316;&#29289;&#20998;&#21106;&#21644;&#25104;&#29087;&#24230;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Agave crop segmentation and maturity classification with deep learning data-centric strategies using very high-resolution satellite imagery. (arXiv:2303.11564v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#25968;&#25454;&#31574;&#30053;&#35299;&#20915;&#20102;&#40857;&#33292;&#20848;&#20316;&#29289;&#20998;&#21106;&#20013;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#40857;&#33292;&#20848;&#20316;&#29289;&#25104;&#29087;&#24230;&#20998;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36127;&#36131;&#20219;&#21644;&#21487;&#25345;&#32493;&#30340;&#40857;&#33292;&#20848;-&#40857;&#33292;&#20848;&#37202;&#29983;&#20135;&#38142;&#23545;&#22696;&#35199;&#21733;&#40857;&#33292;&#20848;&#22320;&#21306;&#30340;&#31038;&#20250;&#12289;&#29615;&#22659;&#21644;&#32463;&#27982;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#26032;&#24037;&#20855;&#20197;&#23454;&#29616;&#22823;&#35268;&#27169;&#33258;&#21160;&#40857;&#33292;&#20848;&#22320;&#21306;&#30417;&#27979;&#21464;&#24471;&#23588;&#20026;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#36827;&#34892;&#40857;&#33292;&#20848;&#20316;&#29289;&#20998;&#21106;&#21644;&#25104;&#29087;&#24230;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23545;&#20110;&#27492;&#20219;&#21153;&#21487;&#33021;&#20250;&#26377;&#25152;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
The responsible and sustainable agave-tequila production chain is fundamental for the social, environment and economic development of Mexico's agave regions. It is therefore relevant to develop new tools for large scale automatic agave region monitoring. In this work, we present an Agave tequilana Weber azul crop segmentation and maturity classification using very high resolution satellite imagery, which could be useful for this task. To achieve this, we solve real-world deep learning problems in the very specific context of agave crop segmentation such as lack of data, low quality labels, highly imbalanced data, and low model performance. The proposed strategies go beyond data augmentation and data transfer combining active learning and the creation of synthetic images with human supervision. As a result, the segmentation performance evaluated with Intersection over Union (IoU) value increased from 0.72 to 0.90 in the test set. We also propose a method for classifying agave crop matur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#37325;&#26500;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;LSTM&#32534;&#30721;&#22120;&#8212;&#35299;&#30721;&#22120;&#20849;&#21516;&#23398;&#20064;&#35266;&#27979;&#21644;&#21160;&#24577;&#27169;&#22411;&#65292;&#24182;&#20174;&#27491;&#24120;&#26679;&#26412;&#20013;&#20272;&#35745;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#21463;&#21040;&#27491;&#21017;&#21270;&#32422;&#26463;&#65292;&#21487;&#20197;&#29992;&#39532;&#27663;&#36317;&#31163;&#35780;&#20272;&#24322;&#24120;&#32423;&#21035;&#12290;</title><link>http://arxiv.org/abs/2303.03324</link><description>&lt;p&gt;
&#22522;&#20110;&#37325;&#26500;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Time series anomaly detection with reconstruction-based state-space models. (arXiv:2303.03324v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#37325;&#26500;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;LSTM&#32534;&#30721;&#22120;&#8212;&#35299;&#30721;&#22120;&#20849;&#21516;&#23398;&#20064;&#35266;&#27979;&#21644;&#21160;&#24577;&#27169;&#22411;&#65292;&#24182;&#20174;&#27491;&#24120;&#26679;&#26412;&#20013;&#20272;&#35745;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#21463;&#21040;&#27491;&#21017;&#21270;&#32422;&#26463;&#65292;&#21487;&#20197;&#29992;&#39532;&#27663;&#36317;&#31163;&#35780;&#20272;&#24322;&#24120;&#32423;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#21270;&#25216;&#26415;&#30340;&#19981;&#26029;&#21457;&#23637;&#23548;&#33268;&#21508;&#31181;&#39046;&#22495;&#20013;&#20986;&#29616;&#20102;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#20351;&#24471;&#23454;&#26102;&#30417;&#27979;&#36816;&#33829;&#25104;&#20026;&#21487;&#33021;&#12290;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#35782;&#21035;&#24322;&#24120;&#25968;&#25454;&#27169;&#24335;&#21644;&#26816;&#27979;&#28508;&#22312;&#25925;&#38556;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#20294;&#20063;&#21464;&#24471;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#20849;&#21516;&#23398;&#20064;&#35266;&#27979;&#27169;&#22411;&#21644;&#21160;&#24577;&#27169;&#22411;&#65292;&#24182;&#20174;&#27491;&#24120;&#26679;&#26412;&#20013;&#20272;&#35745;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20855;&#20307;&#30340;&#65292;&#37319;&#29992;&#22522;&#20110;&#38271;&#30701;&#26102;&#35760;&#24518;&#32593;&#32476;&#65288;LSTM&#65289;&#30340;&#32534;&#30721;&#22120;&#8212;&#35299;&#30721;&#22120;&#34920;&#31034;&#35266;&#27979;&#31354;&#38388;&#21644;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#30340;&#26144;&#23556;&#20851;&#31995;, &#34701;&#21512;&#20102;&#21521;&#21518;&#21644;&#21521;&#21069;&#30340;&#26102;&#38388;&#20449;&#24687;&#20197;&#21516;&#26102;&#24314;&#27169;&#29366;&#24577;&#30340;&#21452;&#21521;&#36716;&#25442;&#12290;&#28508;&#22312;&#31354;&#38388;&#30340;&#27491;&#21017;&#21270;&#32422;&#26463;&#20102;&#27491;&#24120;&#26679;&#26412;&#30340;&#29366;&#24577;&#65292;&#24182;&#20351;&#29992;&#39532;&#27663;&#36317;&#31163;&#35780;&#20272;&#24322;&#24120;&#32423;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in digitization have led to the availability of multivariate time series data in various domains, enabling real-time monitoring of operations. Identifying abnormal data patterns and detecting potential failures in these scenarios are important yet rather challenging. In this work, we propose a novel unsupervised anomaly detection method for time series data. The proposed framework jointly learns the observation model and the dynamic model, and model uncertainty is estimated from normal samples. Specifically, a long short-term memory (LSTM)-based encoder-decoder is adopted to represent the mapping between the observation space and the latent space. Bidirectional transitions of states are simultaneously modeled by leveraging backward and forward temporal information. Regularization of the latent space places constraints on the states of normal samples, and Mahalanobis distance is used to evaluate the abnormality level. Empirical studies on synthetic and real-world dataset
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35268;&#21010;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#25239;&#26080;&#30693;&#30340;&#25915;&#20987;&#32773;&#65292;&#23454;&#29616;&#38450;&#24481;&#32773;&#30340;&#38544;&#34109;&#35825;&#25429;&#65292;&#22312;&#20445;&#23432;&#19979;&#30028;&#20869;&#23613;&#24555;&#36798;&#25104;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2303.00822</link><description>&lt;p&gt;
&#22312;&#23545;&#25239;&#29615;&#22659;&#20013;&#35268;&#21010;&#25915;&#20987;&#32773;&#35825;&#25429;
&lt;/p&gt;
&lt;p&gt;
Planning for Attacker Entrapment in Adversarial Settings. (arXiv:2303.00822v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00822
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35268;&#21010;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#25239;&#26080;&#30693;&#30340;&#25915;&#20987;&#32773;&#65292;&#23454;&#29616;&#38450;&#24481;&#32773;&#30340;&#38544;&#34109;&#35825;&#25429;&#65292;&#22312;&#20445;&#23432;&#19979;&#30028;&#20869;&#23613;&#24555;&#36798;&#25104;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35268;&#21010;&#26694;&#26550;&#65292;&#29992;&#20110;&#38024;&#23545;&#22312;&#26080;&#30693;&#38450;&#24481;&#32773;&#24773;&#20917;&#19979;&#36816;&#20316;&#30340;&#25915;&#20987;&#32773;&#29983;&#25104;&#38450;&#24481;&#31574;&#30053;&#12290;&#38450;&#24481;&#32773;&#30340;&#30446;&#26631;&#26159;&#24708;&#24708;&#22320;&#25351;&#23548;&#25915;&#20987;&#32773;&#38519;&#20837;&#38519;&#38449;&#29366;&#24577;&#65292;&#20174;&#32780;&#20351;&#25915;&#20987;&#32773;&#26080;&#27861;&#23454;&#29616;&#20854;&#30446;&#26631;&#12290;&#27492;&#22806;&#65292;&#38450;&#24481;&#32773;&#21482;&#26377;&#22312;K&#27493;&#20869;&#36798;&#25104;&#20854;&#30446;&#26631;&#65292;&#20854;&#20013;K&#26159;&#22312;&#27492;&#20869;&#25915;&#20987;&#32773;&#19981;&#22826;&#21487;&#33021;&#24576;&#30097;&#29615;&#22659;&#23041;&#32961;&#30340;&#20445;&#23432;&#19979;&#30028;&#12290;&#36825;&#31181;&#38450;&#24481;&#31574;&#30053;&#22312;&#34588;&#32592;&#25110;&#34588;&#32593;&#31561;&#23454;&#38469;&#31995;&#32479;&#20013;&#38750;&#24120;&#26377;&#29992;&#65292;&#20854;&#20013;&#19968;&#20010;&#27627;&#26080;&#25106;&#24515;&#30340;&#25915;&#20987;&#32773;&#19982;&#27169;&#25311;&#29983;&#20135;&#31995;&#32479;&#20132;&#20114;&#65292;&#21516;&#26102;&#20551;&#35774;&#23427;&#26159;&#30495;&#23454;&#30340;&#29983;&#20135;&#31995;&#32479;&#12290;&#36890;&#24120;&#65292;&#25915;&#20987;&#32773;&#21644;&#38450;&#24481;&#32773;&#20043;&#38388;&#30340;&#20132;&#20114;&#20351;&#29992;&#21338;&#24328;&#29702;&#35770;&#26694;&#26550;&#25429;&#33719;&#12290;&#25105;&#20204;&#30340;&#38382;&#39064;&#20844;&#24335;&#20801;&#35768;&#25105;&#20204;&#23558;&#20854;&#25429;&#33719;&#20026;&#19968;&#20010;&#26356;&#31616;&#21333;&#30340;&#26080;&#38480;&#22320;&#22270;&#25240;&#25187;MDP&#65292;&#20854;&#20013;&#36890;&#36807;&#20540;&#36845;&#20195;&#33719;&#24471;&#20102;MDP&#38382;&#39064;&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;&#25105;&#20204;&#22312;&#22810;&#31181;&#35774;&#32622;&#20013;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#25214;&#21040;&#38450;&#24481;&#31574;&#30053;&#24182;&#20445;&#25345;&#25915;&#20987;&#32773;&#38519;&#20837;&#22256;&#22659;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a planning framework to generate a defense strategy against an attacker who is working in an environment where a defender can operate without the attacker's knowledge. The objective of the defender is to covertly guide the attacker to a trap state from which the attacker cannot achieve their goal. Further, the defender is constrained to achieve its goal within K number of steps, where K is calculated as a pessimistic lower bound within which the attacker is unlikely to suspect a threat in the environment. Such a defense strategy is highly useful in real world systems like honeypots or honeynets, where an unsuspecting attacker interacts with a simulated production system while assuming it is the actual production system. Typically, the interaction between an attacker and a defender is captured using game theoretic frameworks. Our problem formulation allows us to capture it as a much simpler infinite horizon discounted MDP, in which the optimal policy for the MD
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21322;&#20998;&#25955;&#25512;&#29702;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#20010;&#20113;&#33410;&#28857;&#38477;&#20302;&#36890;&#20449;&#38656;&#27714;&#65292;&#21516;&#26102;&#20445;&#25345;&#22270;&#31070;&#32463;&#32593;&#32476;&#20998;&#25955;&#21270;&#30340;&#20248;&#21183;&#65292;&#20174;&#32780;&#25552;&#39640;&#20132;&#36890;&#38656;&#27714;&#39044;&#27979;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.00524</link><description>&lt;p&gt;
&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21322;&#20998;&#25955;&#25512;&#29702;&#25216;&#26415;&#22312;&#20132;&#36890;&#38656;&#27714;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#31181;&#36793;&#32536;&#35745;&#31639;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Semi-decentralized Inference in Heterogeneous Graph Neural Networks for Traffic Demand Forecasting: An Edge-Computing Approach. (arXiv:2303.00524v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21322;&#20998;&#25955;&#25512;&#29702;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#20010;&#20113;&#33410;&#28857;&#38477;&#20302;&#36890;&#20449;&#38656;&#27714;&#65292;&#21516;&#26102;&#20445;&#25345;&#22270;&#31070;&#32463;&#32593;&#32476;&#20998;&#25955;&#21270;&#30340;&#20248;&#21183;&#65292;&#20174;&#32780;&#25552;&#39640;&#20132;&#36890;&#38656;&#27714;&#39044;&#27979;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#20986;&#31199;&#36710;&#26381;&#21153;&#30340;&#38656;&#27714;&#21644;&#20379;&#24212;&#23545;&#20110;&#25552;&#39640;&#23458;&#25143;&#20307;&#39564;&#21644;&#25552;&#20379;&#21830;&#21033;&#28070;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#27492;&#31867;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#21069;&#26223;&#12290;&#35813;&#26041;&#27861;&#23558;&#22478;&#24066;&#21306;&#22495;&#24314;&#27169;&#20026;&#19968;&#20010;&#20132;&#36890;&#22270;&#20013;&#30340;&#33410;&#28857;&#20197;&#21450;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#20316;&#20026;&#36793;&#12290;GNN&#21033;&#29992;&#26412;&#22320;&#33410;&#28857;&#29305;&#24449;&#21644;&#22270;&#24418;&#32467;&#26500;&#36827;&#34892;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#20004;&#31181;&#20027;&#35201;&#36884;&#24452;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#39044;&#27979;&#65306;&#25193;&#22823;&#20132;&#36890;&#32593;&#32476;&#30340;&#35268;&#27169;&#65292;&#21516;&#26102;&#21033;&#29992;&#22270;&#20013;&#19981;&#21516;&#31867;&#22411;&#30340;&#33410;&#28857;&#21644;&#36793;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#38754;&#20020;GNN&#21487;&#25193;&#23637;&#24615;&#30340;&#25361;&#25112;&#12290;&#21363;&#26102;&#30340;&#35299;&#20915;&#26041;&#27861;&#26159;&#20998;&#25955;GNN&#25805;&#20316;&#12290;&#28982;&#32780;&#65292;&#36825;&#20250;&#20135;&#29983;&#36807;&#22810;&#30340;&#33410;&#28857;&#20043;&#38388;&#20256;&#36755;&#36890;&#20449;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#34920;&#24449;&#20102;&#20998;&#25955;&#30340;GNN&#26041;&#27861;&#23545;&#20110;&#36807;&#22810;&#30340;&#36890;&#20449;&#38656;&#27714;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#20998;&#25955;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#20010;&#36127;&#36131;&#35843;&#33410;&#30340;&#20113;&#35745;&#31639;&#33410;&#28857;&#26469;&#20943;&#23569;&#36890;&#20449;&#38656;&#27714;&#65292;&#21516;&#26102;&#20445;&#25345;&#20998;&#25955;&#21270;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#22312;&#22823;&#35268;&#27169;&#20986;&#31199;&#36710;&#26381;&#21153;&#38656;&#27714;&#39044;&#27979;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#20445;&#25345;&#39640;&#31934;&#24230;&#30340;&#21516;&#26102;&#65292;&#26377;&#25928;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prediction of taxi service demand and supply is essential for improving customer's experience and provider's profit. Recently, graph neural networks (GNNs) have been shown promising for this application. This approach models city regions as nodes in a transportation graph and their relations as edges. GNNs utilize local node features and the graph structure in the prediction. However, more efficient forecasting can still be achieved by following two main routes; enlarging the scale of the transportation graph, and simultaneously exploiting different types of nodes and edges in the graphs. However, both approaches are challenged by the scalability of GNNs. An immediate remedy to the scalability challenge is to decentralize the GNN operation. However, this creates excessive node-to-node communication. In this paper, we first characterize the excessive communication needs for the decentralized GNN approach. Then, we propose a semi-decentralized approach utilizing multiple cloudlets, moder
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#20154;&#24247;&#22797;&#36741;&#21161;&#25511;&#21046;&#22120;AR3n&#65292;&#36890;&#36807;&#20351;&#29992;&#34394;&#25311;&#24739;&#32773;&#27169;&#22411;&#23454;&#29616;&#25511;&#21046;&#22120;&#30340;&#27867;&#21270;&#65292;&#23454;&#26102;&#35843;&#33410;&#26426;&#22120;&#20154;&#36741;&#21161;&#21147;&#24230;&#24182;&#26368;&#23567;&#21270;&#26426;&#22120;&#20154;&#36741;&#21161;&#30340;&#37327;&#65292;&#35813;&#25511;&#21046;&#22120;&#22312;&#23454;&#39564;&#39564;&#35777;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.00085</link><description>&lt;p&gt;
AR3n: &#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#20154;&#24247;&#22797;&#36741;&#21161;&#25511;&#21046;&#22120;
&lt;/p&gt;
&lt;p&gt;
AR3n: A Reinforcement Learning-based Assist-As-Needed Controller for Robotic Rehabilitation. (arXiv:2303.00085v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#20154;&#24247;&#22797;&#36741;&#21161;&#25511;&#21046;&#22120;AR3n&#65292;&#36890;&#36807;&#20351;&#29992;&#34394;&#25311;&#24739;&#32773;&#27169;&#22411;&#23454;&#29616;&#25511;&#21046;&#22120;&#30340;&#27867;&#21270;&#65292;&#23454;&#26102;&#35843;&#33410;&#26426;&#22120;&#20154;&#36741;&#21161;&#21147;&#24230;&#24182;&#26368;&#23567;&#21270;&#26426;&#22120;&#20154;&#36741;&#21161;&#30340;&#37327;&#65292;&#35813;&#25511;&#21046;&#22120;&#22312;&#23454;&#39564;&#39564;&#35777;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;AR3n&#65288;&#21457;&#38899;&#20026;Aaron&#65289;&#65292;&#19968;&#31181;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#36741;&#21161;&#25511;&#21046;&#22120;&#65292;&#21487;&#22312;&#26426;&#22120;&#20154;&#36741;&#21161;&#30340;&#20070;&#20889;&#24247;&#22797;&#20219;&#21153;&#20013;&#25552;&#20379;&#36866;&#24212;&#24615;&#36741;&#21161;&#12290;&#19982;&#20197;&#24448;&#30340;&#36741;&#21161;&#25511;&#21046;&#22120;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#24739;&#32773;&#29305;&#23450;&#30340;&#25511;&#21046;&#22120;&#21442;&#25968;&#25110;&#29289;&#29702;&#27169;&#22411;&#12290;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#34394;&#25311;&#24739;&#32773;&#27169;&#22411;&#26469;&#20351;AR3n&#25512;&#24191;&#21040;&#22810;&#20010;&#21463;&#35797;&#32773;&#12290;&#35813;&#31995;&#32479;&#23454;&#26102;&#35843;&#33410;&#26426;&#22120;&#20154;&#36741;&#21161;&#21147;&#24230;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#26426;&#22120;&#20154;&#36741;&#21161;&#30340;&#37327;&#65292;&#22522;&#20110;&#34987;&#35797;&#30340;&#36319;&#36394;&#35823;&#24046;&#12290;&#36890;&#36807;&#19968;&#32452;&#20223;&#30495;&#23454;&#39564;&#21644;&#20154;&#20307;&#21463;&#35797;&#23454;&#39564;&#23545;&#25511;&#21046;&#22120;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#12290;&#26368;&#21518;&#65292;&#36827;&#34892;&#20102;&#19982;&#20256;&#32479;&#22522;&#20110;&#35268;&#21017;&#30340;&#25511;&#21046;&#22120;&#30340;&#27604;&#36739;&#30740;&#31350;&#65292;&#20197;&#20998;&#26512;&#20004;&#31181;&#25511;&#21046;&#22120;&#30340;&#36741;&#21161;&#26426;&#21046;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present AR3n (pronounced as Aaron), an assist-as-needed (AAN) controller that utilizes reinforcement learning to supply adaptive assistance during a robot assisted handwriting rehabilitation task. Unlike previous AAN controllers, our method does not rely on patient specific controller parameters or physical models. We propose the use of a virtual patient model to generalize AR3n across multiple subjects. The system modulates robotic assistance in realtime based on a subject's tracking error, while minimizing the amount of robotic assistance. The controller is experimentally validated through a set of simulations and human subject experiments. Finally, a comparative study with a traditional rule-based controller is conducted to analyze differences in assistance mechanisms of the two controllers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23454;&#39564;&#24615;&#22320;&#20351;&#29992;&#21508;&#31181;&#25552;&#31034;&#26469;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#19981;&#21516;&#30340;&#33539;&#24335;&#25191;&#34892;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#25688;&#35201;&#65292;&#24182;&#25104;&#21151;&#25552;&#39640;&#20102;&#23427;&#20204;&#30340;CLS&#24615;&#33021;&#12290;&#20854;&#20013;&#65292;GPT-4&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;CLS&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#26041;&#38754;&#19982;&#26368;&#20339;&#26041;&#27861;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2302.14229</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Cross-Lingual Summarization via Large Language Models. (arXiv:2302.14229v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23454;&#39564;&#24615;&#22320;&#20351;&#29992;&#21508;&#31181;&#25552;&#31034;&#26469;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#19981;&#21516;&#30340;&#33539;&#24335;&#25191;&#34892;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#25688;&#35201;&#65292;&#24182;&#25104;&#21151;&#25552;&#39640;&#20102;&#23427;&#20204;&#30340;CLS&#24615;&#33021;&#12290;&#20854;&#20013;&#65292;GPT-4&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;CLS&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#26041;&#38754;&#19982;&#26368;&#20339;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#19968;&#20010;&#28304;&#35821;&#35328;&#25991;&#26412;&#65292;&#36328;&#35821;&#35328;&#25688;&#35201;&#65288;CLS&#65289;&#26088;&#22312;&#29983;&#25104;&#21478;&#19968;&#31181;&#30446;&#26631;&#35821;&#35328;&#30340;&#25688;&#35201;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#65292;&#27604;&#22914;GPT-3.5&#12289;ChatGPT&#21644;GPT-4&#65292;&#24341;&#36215;&#20102;&#35745;&#31639;&#35821;&#35328;&#23398;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;LLM&#22312;CLS&#19978;&#30340;&#34920;&#29616;&#22914;&#20309;&#12290;&#26412;&#25991;&#23454;&#39564;&#24615;&#22320;&#20351;&#29992;&#21508;&#31181;&#25552;&#31034;&#26469;&#25351;&#23548;LLM&#20174;&#19981;&#21516;&#30340;&#33539;&#24335;&#65288;&#21363;&#31471;&#21040;&#31471;&#21644;&#27969;&#27700;&#32447;&#65289;&#25191;&#34892;&#38646;&#26679;&#26412;CLS&#65292;&#24182;&#23545;&#29983;&#25104;&#30340;&#25688;&#35201;&#36827;&#34892;&#21021;&#27493;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;ChatGPT&#21644;GPT-4&#21407;&#26412;&#26356;&#21916;&#27426;&#29983;&#25104;&#35814;&#32454;&#20449;&#24687;&#30340;&#38271;&#25688;&#35201;&#12290;&#20294;&#36825;&#20004;&#20010;LLM&#22312;&#20132;&#20114;&#24335;&#25552;&#31034;&#30340;&#24110;&#21161;&#19979;&#21487;&#20197;&#36827;&#19968;&#27493;&#24179;&#34913;&#20449;&#24687;&#37327;&#21644;&#31616;&#27905;&#24615;&#65292;&#26174;&#33879;&#25552;&#39640;&#23427;&#20204;&#30340;CLS&#24615;&#33021;&#12290;&#22312;&#19977;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;CLS&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-4&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;CLS&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#26041;&#38754;&#19982;&#26368;&#20339;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a document in a source language, cross-lingual summarization (CLS) aims to generate a summary in a different target language. Recently, the emergence of Large Language Models (LLMs), such as GPT-3.5, ChatGPT and GPT-4, has attracted wide attention from the computational linguistics community. However, it is not yet known the performance of LLMs on CLS. In this report, we empirically use various prompts to guide LLMs to perform zero-shot CLS from different paradigms (i.e., end-to-end and pipeline), and provide a preliminary evaluation on the generated summaries. We find that ChatGPT and GPT-4 originally prefer to produce lengthy summaries with detailed information. These two LLMs can further balance informativeness and conciseness with the help of an interactive prompt, significantly improving their CLS performance. Experimental results on three widely-used CLS datasets show that GPT-4 achieves state-of-the-art zero-shot CLS performance, and performs competitively compared with th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36793;&#32536;Transformer&#21644;&#39044;&#35757;&#32451;&#30340;&#38142;&#25509;&#39044;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22312;&#32447;&#28216;&#25103;&#20013;&#36827;&#34892;&#22909;&#21451;&#25490;&#21517;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.10043</link><description>&lt;p&gt;
&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#36793;&#32536;Transformer&#22312;&#22312;&#32447;&#28216;&#25103;&#20013;&#36827;&#34892;&#22909;&#21451;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
Friend Ranking in Online Games via Pre-training Edge Transformers. (arXiv:2302.10043v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36793;&#32536;Transformer&#21644;&#39044;&#35757;&#32451;&#30340;&#38142;&#25509;&#39044;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22312;&#32447;&#28216;&#25103;&#20013;&#36827;&#34892;&#22909;&#21451;&#25490;&#21517;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#28216;&#25103;&#20013;&#65292;&#22909;&#21451;&#22238;&#24518;&#26159;&#25552;&#39640;&#27599;&#26085;&#27963;&#36291;&#29992;&#25143;&#25968;&#37327;&#30340;&#37325;&#35201;&#36884;&#24452;&#12290;&#26412;&#25991;&#23558;&#22909;&#21451;&#22238;&#24518;&#38382;&#39064;&#35270;&#20026;&#38142;&#25509;&#39044;&#27979;&#38382;&#39064;&#65292;&#24182;&#25506;&#35752;&#20102;&#21487;&#20197;&#20351;&#29992;&#65288;&#27963;&#36291;&#30340;&#21644;&#22833;&#33853;&#30340;&#65289;&#29609;&#23478;&#29305;&#24449;&#20197;&#21450;&#21382;&#21490;&#20107;&#20214;&#30340;&#20960;&#31181;&#38142;&#25509;&#39044;&#27979;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36793;&#32536;Transformer&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#25513;&#30721;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19977;&#27454;&#33150;&#35759;&#28216;&#25103;&#30340;&#31163;&#32447;&#23454;&#39564;&#21644;&#22312;&#32447;A/B&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Friend recall is an important way to improve Daily Active Users (DAU) in online games. The problem is to generate a proper lost friend ranking list essentially. Traditional friend recall methods focus on rules like friend intimacy or training a classifier for predicting lost players' return probability, but ignore feature information of (active) players and historical friend recall events. In this work, we treat friend recall as a link prediction problem and explore several link prediction methods which can use features of both active and lost players, as well as historical events. Furthermore, we propose a novel Edge Transformer model and pre-train the model via masked auto-encoders. Our method achieves state-of-the-art results in the offline experiments and online A/B Tests of three Tencent games.
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#32593;&#26684;&#26159;&#19968;&#31181;&#20419;&#36827;&#25968;&#25454;&#27665;&#20027;&#21270;&#30340;&#31038;&#20250;&#25216;&#26415;&#27010;&#24565;&#65292;&#20854;&#21160;&#26426;&#22240;&#32032;&#21253;&#25324;&#21162;&#21147;&#25104;&#20026;&#26356;&#20855;&#25968;&#25454;&#39537;&#21160;&#24615;&#65292;&#25361;&#25112;&#21253;&#25324;&#21521;&#32852;&#37030;&#27835;&#29702;&#30340;&#36716;&#21464;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#38656;&#35201;&#26368;&#20339;&#23454;&#36341;&#30340;&#25351;&#23548;&#65292;&#20197;&#23454;&#29616;&#20854;&#28508;&#22312;&#30340;&#19994;&#21153;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2302.01713</link><description>&lt;p&gt;
&#25968;&#25454;&#32593;&#26684;&#65306;&#21160;&#26426;&#22240;&#32032;&#12289;&#25361;&#25112;&#21644;&#26368;&#20339;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Data Mesh: Motivational Factors, Challenges, and Best Practices. (arXiv:2302.01713v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01713
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#32593;&#26684;&#26159;&#19968;&#31181;&#20419;&#36827;&#25968;&#25454;&#27665;&#20027;&#21270;&#30340;&#31038;&#20250;&#25216;&#26415;&#27010;&#24565;&#65292;&#20854;&#21160;&#26426;&#22240;&#32032;&#21253;&#25324;&#21162;&#21147;&#25104;&#20026;&#26356;&#20855;&#25968;&#25454;&#39537;&#21160;&#24615;&#65292;&#25361;&#25112;&#21253;&#25324;&#21521;&#32852;&#37030;&#27835;&#29702;&#30340;&#36716;&#21464;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#38656;&#35201;&#26368;&#20339;&#23454;&#36341;&#30340;&#25351;&#23548;&#65292;&#20197;&#23454;&#29616;&#20854;&#28508;&#22312;&#30340;&#19994;&#21153;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#25454;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#26085;&#30410;&#37325;&#35201;&#65292;&#32452;&#32455;&#21162;&#21147;&#25104;&#20026;&#26356;&#20855;&#25968;&#25454;&#39537;&#21160;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#25968;&#25454;&#26550;&#26500;&#24182;&#19981;&#19968;&#23450;&#35774;&#35745;&#29992;&#20110;&#24212;&#23545;&#25968;&#25454;&#21644;&#20998;&#26512;&#29992;&#20363;&#30340;&#35268;&#27169;&#21644;&#33539;&#22260;&#12290;&#20107;&#23454;&#19978;&#65292;&#29616;&#26377;&#26550;&#26500;&#24120;&#24120;&#26080;&#27861;&#23454;&#29616;&#23427;&#20204;&#25152;&#25215;&#35834;&#30340;&#20215;&#20540;&#12290;&#25968;&#25454;&#32593;&#26684;&#26159;&#19968;&#20010;&#31038;&#20250;&#25216;&#26415;&#27010;&#24565;&#65292;&#20854;&#20013;&#21253;&#21547;&#26550;&#26500;&#26041;&#38754;&#30340;&#20869;&#23481;&#65292;&#20197;&#20419;&#36827;&#25968;&#25454;&#27665;&#20027;&#21270;&#65292;&#24182;&#20351;&#32452;&#32455;&#30495;&#27491;&#25104;&#20026;&#25968;&#25454;&#39537;&#21160;&#22411;&#12290;&#30001;&#20110;&#25968;&#25454;&#32593;&#26684;&#30340;&#27010;&#24565;&#20173;&#28982;&#26159;&#26032;&#39062;&#30340;&#65292;&#22240;&#27492;&#32570;&#20047;&#26469;&#33258;&#23454;&#22320;&#30340;&#32463;&#39564;&#35777;&#23454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32570;&#23569;&#20102;&#35299;&#24341;&#20837;&#25968;&#25454;&#32593;&#26684;&#30340;&#21160;&#26426;&#22240;&#32032;&#12289;&#30456;&#20851;&#25361;&#25112;&#12289;&#26368;&#20339;&#23454;&#36341;&#12289;&#20854;&#19994;&#21153;&#24433;&#21709;&#21644;&#28508;&#22312;&#21407;&#22411;&#30340;&#29702;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#23545;15&#20301;&#34892;&#19994;&#19987;&#23478;&#36827;&#34892;&#20102;&#21322;&#32467;&#26500;&#21270;&#35775;&#35848;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#34892;&#19994;&#19987;&#23478;&#22312;&#21521;&#32852;&#37030;&#27835;&#29702;&#30340;&#36716;&#21464;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing importance of data and artificial intelligence, organizations strive to become more data-driven. However, current data architectures are not necessarily designed to keep up with the scale and scope of data and analytics use cases. In fact, existing architectures often fail to deliver the promised value associated with them. Data mesh is a socio-technical concept that includes architectural aspects to promote data democratization and enables organizations to become truly data-driven. As the concept of data mesh is still novel, it lacks empirical insights from the field. Specifically, an understanding of the motivational factors for introducing data mesh, the associated challenges, best practices, its business impact, and potential archetypes, is missing. To address this gap, we conduct 15 semi-structured interviews with industry experts. Our results show, among other insights, that industry experts have difficulties with the transition toward federated governance ass
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#26041;&#27861; SEER&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#32570;&#20047;&#27979;&#35797;&#26029;&#35328;&#25110;&#20854;&#20182;&#31867;&#22411;&#30340;&#27979;&#35797;Oracle&#30340;&#24773;&#20917;&#19979;&#30830;&#23450;&#21333;&#20803;&#27979;&#35797;&#26159;&#21542;&#36890;&#36807;&#25110;&#22833;&#36133;&#65292;&#24182;&#19988;&#21487;&#20197;&#26500;&#24314;&#20934;&#30830;&#30340;Oracle&#32780;&#19981;&#38656;&#35201;&#30693;&#36947;&#27491;&#30830;&#25110;&#38169;&#35823;&#34892;&#20026;&#30340;&#30830;&#20999;&#26399;&#26395;&#12290;</title><link>http://arxiv.org/abs/2302.01488</link><description>&lt;p&gt;
&#23436;&#32654;&#20027;&#20041;&#26159;&#27979;&#35797;Oracle&#30340;&#25932;&#20154;
&lt;/p&gt;
&lt;p&gt;
Perfect is the enemy of test oracle. (arXiv:2302.01488v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#26041;&#27861; SEER&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#32570;&#20047;&#27979;&#35797;&#26029;&#35328;&#25110;&#20854;&#20182;&#31867;&#22411;&#30340;&#27979;&#35797;Oracle&#30340;&#24773;&#20917;&#19979;&#30830;&#23450;&#21333;&#20803;&#27979;&#35797;&#26159;&#21542;&#36890;&#36807;&#25110;&#22833;&#36133;&#65292;&#24182;&#19988;&#21487;&#20197;&#26500;&#24314;&#20934;&#30830;&#30340;Oracle&#32780;&#19981;&#38656;&#35201;&#30693;&#36947;&#27491;&#30830;&#25110;&#38169;&#35823;&#34892;&#20026;&#30340;&#30830;&#20999;&#26399;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#27979;&#35797;Oracle&#26159;&#36719;&#20214;&#27979;&#35797;&#20013;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#26041;&#38754;&#20043;&#19968;&#65292;&#20294;&#19982;&#33258;&#21160;&#21270;&#27979;&#35797;&#36755;&#20837;&#29983;&#25104;&#30456;&#27604;&#65292;&#20173;&#28982;&#21463;&#21040;&#30456;&#23545;&#36739;&#23569;&#30340;&#20851;&#27880;&#12290;&#27979;&#35797;Oracle&#20381;&#36182;&#20110;&#21487;&#20197;&#21306;&#20998;&#27491;&#30830;&#34892;&#20026;&#21644;&#38169;&#35823;&#34892;&#20026;&#30340;&#22522;&#30784;&#30495;&#30456;&#26469;&#30830;&#23450;&#27979;&#35797;&#26159;&#21542;&#22833;&#36133;&#65288;&#26816;&#27979;&#21040;&#38169;&#35823;&#65289;&#25110;&#36890;&#36807;&#12290;&#35753;Oracle&#38382;&#39064;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#19981;&#21487;&#20915;&#23450;&#24615;&#30340;&#26159;&#20551;&#35774;&#36825;&#20010;&#22522;&#30784;&#30495;&#30456;&#38656;&#35201;&#30693;&#36947;&#27491;&#30830;&#34892;&#20026;&#25110;&#38169;&#35823;&#34892;&#20026;&#30340;&#30830;&#20999;&#26399;&#26395;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#21363;&#20351;&#19981;&#30693;&#36947;&#30830;&#20999;&#30340;&#27491;&#30830;&#25110;&#38169;&#35823;&#34892;&#20026;&#22914;&#20309;&#19981;&#21516;&#65292;&#20173;&#28982;&#21487;&#20197;&#26500;&#24314;&#20934;&#30830;&#30340;Oracle&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SEER&#65292;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32570;&#20047;&#27979;&#35797;&#26029;&#35328;&#25110;&#20854;&#20182;&#31867;&#22411;&#30340;Oracle&#30340;&#24773;&#20917;&#19979;&#65292;&#30830;&#23450;&#22312;&#32473;&#23450;&#30340;&#27979;&#35797;&#26041;&#27861;&#19979;&#21333;&#20803;&#27979;&#35797;&#26159;&#21542;&#36890;&#36807;&#25110;&#22833;&#36133;&#12290;&#20026;&#20102;&#24314;&#31435;&#22522;&#30784;&#30495;&#30456;&#65292;SEER&#23558;&#21333;&#20803;&#27979;&#35797;&#21644;MUTs&#30340;&#23454;&#29616;&#32852;&#21512;&#23884;&#20837;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#21521;&#37327;&#31354;&#38388;&#20013;&#65292;&#20351;&#31070;&#32463;&#34920;&#31034;&#26041;&#24335;&#20855;&#26377;&#21306;&#20998;&#21333;&#20803;&#27979;&#35797;&#32467;&#26524;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automation of test oracles is one of the most challenging facets of software testing, but remains comparatively less addressed compared to automated test input generation. Test oracles rely on a ground-truth that can distinguish between the correct and buggy behavior to determine whether a test fails (detects a bug) or passes. What makes the oracle problem challenging and undecidable is the assumption that the ground-truth should know the exact expected, correct, or buggy behavior. However, we argue that one can still build an accurate oracle without knowing the exact correct or buggy behavior, but how these two might differ. This paper presents SEER, a learning-based approach that in the absence of test assertions or other types of oracle, can determine whether a unit test passes or fails on a given method under test (MUT). To build the ground-truth, SEER jointly embeds unit tests and the implementation of MUTs into a unified vector space, in such a way that the neural representation 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#20102;&#22721;&#38754;&#36793;&#30028;&#23618;&#28237;&#27969;&#20013;&#30340;&#36895;&#24230;&#22330;&#65292;&#24182;&#21033;&#29992;SHAP&#31639;&#27861;&#35780;&#20272;&#20102;&#30456;&#24178;&#32467;&#26500;&#23545;&#39044;&#27979;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#19968;&#36807;&#31243;&#25110;&#26377;&#21161;&#20110;&#35299;&#20915;&#28237;&#27969;&#30740;&#31350;&#20013;&#30340;&#38590;&#39064;&#65292;&#20026;&#28237;&#27969;&#27169;&#22411;&#30340;&#21457;&#23637;&#25552;&#20379;&#26032;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2302.01250</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#35299;&#37322;&#22721;&#38754;&#36793;&#30028;&#23618;&#28237;&#27969;
&lt;/p&gt;
&lt;p&gt;
Explaining wall-bounded turbulence through deep learning. (arXiv:2302.01250v2 [physics.flu-dyn] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#20102;&#22721;&#38754;&#36793;&#30028;&#23618;&#28237;&#27969;&#20013;&#30340;&#36895;&#24230;&#22330;&#65292;&#24182;&#21033;&#29992;SHAP&#31639;&#27861;&#35780;&#20272;&#20102;&#30456;&#24178;&#32467;&#26500;&#23545;&#39044;&#27979;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#19968;&#36807;&#31243;&#25110;&#26377;&#21161;&#20110;&#35299;&#20915;&#28237;&#27969;&#30740;&#31350;&#20013;&#30340;&#38590;&#39064;&#65292;&#20026;&#28237;&#27969;&#27169;&#22411;&#30340;&#21457;&#23637;&#25552;&#20379;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22721;&#38754;&#36793;&#30028;&#23618;&#28237;&#27969;&#20316;&#20026;&#19968;&#20010;&#20855;&#26377;&#37325;&#22823;&#31185;&#23398;&#21644;&#25216;&#26415;&#24847;&#20041;&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#23547;&#27714;&#26032;&#30340;&#35270;&#35282;&#26469;&#35299;&#20915;&#12290;&#26412;&#30740;&#31350;&#39318;&#27425;&#37319;&#29992;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30740;&#31350;&#20102;&#27969;&#22330;&#20013;&#30456;&#24178;&#32467;&#26500;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#36890;&#36807;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#21033;&#29992;&#28237;&#27969;&#36890;&#36947;&#20013;&#30340;&#30636;&#26102;&#36895;&#24230;&#22330;&#39044;&#27979;&#20102;&#26102;&#38388;&#20869;&#30340;&#36895;&#24230;&#22330;&#65292;&#28982;&#21518;&#21033;&#29992;SHapley Additive exPlanations&#65288;SHAP&#65289;&#31639;&#27861;&#23545;&#27599;&#20010;&#32467;&#26500;&#39044;&#27979;&#30340;&#37325;&#35201;&#24615;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#26412;&#30740;&#31350;&#32467;&#26524;&#19982;&#20808;&#21069;&#25991;&#29486;&#35266;&#23519;&#32467;&#26524;&#19968;&#33268;&#65292;&#24182;&#36890;&#36807;&#37327;&#21270;&#38647;&#35834;&#24212;&#21147;&#32467;&#26500;&#30340;&#37325;&#35201;&#24615;&#65292;&#25214;&#21040;&#20102;&#36825;&#20123;&#32467;&#26500;&#19982;&#27969;&#21160;&#21160;&#21147;&#23398;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#21487;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#21487;&#33021;&#26377;&#21161;&#20110;&#25581;&#31034;&#22721;&#38754;&#36793;&#30028;&#23618;&#28237;&#27969;&#30340;&#38271;&#26399;&#38382;&#39064;&#65292;&#24182;&#20026;&#28237;&#27969;&#27169;&#22411;&#30340;&#24320;&#21457;&#25552;&#20379;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite its great scientific and technological importance, wall-bounded turbulence is an unresolved problem that requires new perspectives to be tackled. One of the key strategies has been to study interactions among the coherent structures in the flow. Such interactions are explored in this study for the first time using an explainable deep-learning method. The instantaneous velocity field in a turbulent channel is used to predict the velocity field in time through a convolutional neural network. Based on the predicted flow, we assess the importance of each structure for this prediction using the game-theoretic algorithm of SHapley Additive exPlanations (SHAP). This work provides results in agreement with previous observations in the literature and extends them by quantifying the importance of the Reynolds-stress structures, finding a connection between these structures and the dynamics of the flow. The process, based on deep-learning explainability, has the potential to shed light on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#19977;&#20998;&#20915;&#31574;&#26694;&#26550;&#19979;&#30340;&#32479;&#19968;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#26512;&#20020;&#24202;&#21307;&#29983;&#30340;&#20027;&#35266;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#37327;&#21644;&#23450;&#24615;&#20998;&#26512;&#24471;&#20986;&#25490;&#21517;&#21015;&#34920;&#21644;&#26435;&#37325;&#65292;&#24182;&#23558;&#30142;&#30149;&#36827;&#34892;&#27604;&#36739;&#20998;&#31867;&#20026;&#19977;&#32452;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20316;&#20026;&#34917;&#20805;&#24037;&#20855;&#19982;&#25163;&#21160;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#31934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.03351</link><description>&lt;p&gt;
&#22522;&#20110;&#19977;&#20998;&#20915;&#31574;&#30340;&#20020;&#24202;&#21307;&#29983;&#20027;&#35266;&#26041;&#27861;&#29992;&#20110;&#31934;&#31070;&#38556;&#30861;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classifying Mental-Disorders through Clinicians Subjective Approach based on Three-way Decision. (arXiv:2301.03351v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.03351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#19977;&#20998;&#20915;&#31574;&#26694;&#26550;&#19979;&#30340;&#32479;&#19968;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#26512;&#20020;&#24202;&#21307;&#29983;&#30340;&#20027;&#35266;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#37327;&#21644;&#23450;&#24615;&#20998;&#26512;&#24471;&#20986;&#25490;&#21517;&#21015;&#34920;&#21644;&#26435;&#37325;&#65292;&#24182;&#23558;&#30142;&#30149;&#36827;&#34892;&#27604;&#36739;&#20998;&#31867;&#20026;&#19977;&#32452;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20316;&#20026;&#34917;&#20805;&#24037;&#20855;&#19982;&#25163;&#21160;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#31934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31934;&#31070;&#35786;&#26029;&#20013;&#65292;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#25163;&#21160;&#26041;&#27861;&#34987;&#29992;&#20110;&#31934;&#31070;&#38556;&#30861;&#20998;&#31867;&#65292;&#20294;&#26159;&#23427;&#23384;&#22312;&#19968;&#20123;&#19981;&#21487;&#36991;&#20813;&#30340;&#32570;&#38519;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#20998;&#20915;&#31574;&#26694;&#26550;&#19979;&#30340;&#32479;&#19968;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#26512;&#20020;&#24202;&#21307;&#29983;&#30340;&#20027;&#35266;&#26041;&#27861;&#65292;&#21253;&#21547;&#23450;&#37327;&#20998;&#26512;&#12289;&#23450;&#37327;&#20998;&#26512;&#20197;&#21450;&#22522;&#20110;&#35780;&#20272;&#30340;&#20998;&#26512;&#12290;&#22522;&#20110;&#20020;&#24202;&#21307;&#29983;&#26368;&#22823;&#31243;&#24230;&#30340;&#20551;&#35774;&#65292;&#23450;&#24615;&#21644;&#23450;&#37327;&#30740;&#31350;&#24471;&#20986;&#20102;&#25490;&#21517;&#21015;&#34920;&#21644;&#19968;&#32452;&#25968;&#20540;&#26435;&#37325;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#30142;&#30149;&#36827;&#34892;&#27604;&#36739;&#20998;&#31867;&#20026;&#19977;&#32452;&#65292;&#37319;&#29992;&#19977;&#20998;&#22522;&#20110;&#35780;&#20272;&#30340;&#27169;&#22411;&#65292;&#26088;&#22312;&#29702;&#35299;&#21644;&#26356;&#28165;&#26224;&#22320;&#25551;&#36848;&#36825;&#20123;&#32467;&#26524;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#20316;&#20026;&#34917;&#20805;&#24037;&#20855;&#19982;&#25163;&#21160;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#31934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In psychiatric diagnosis, a contemporary data-driven, manual-based method for mental disorders classification is the most popular technique; however, it has several inevitable flaws. Using the three-way decision as a framework, we propose a unified model that stands for clinicians' subjective approach (CSA) analysis consisting of three parts: quantitative analysis, quantitative analysis, and evaluation-based analysis. A ranking list and a set of numerical weights based on illness magnitude levels according to the clinician's greatest degree of assumptions are the findings of the qualitative and quantitative investigation. We further create a comparative classification of illnesses into three groups with varying important levels; a three-way evaluation-based model is utilized in this study for the aim of understanding and portraying these results in a more clear way. This proposed method might be integrated with the manual-based process as a complementary tool to improve precision while
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#23545;&#33021;&#28304;&#65288;&#36127;&#33655;&#12289;&#20809;&#20239;&#25110;&#39118;&#21147;&#65289;&#30340;&#27010;&#29575;&#39044;&#27979;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#27604;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.02977</link><description>&lt;p&gt;
&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#22312;&#33021;&#37327;&#27010;&#29575;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Denoising diffusion probabilistic models for probabilistic energy forecasting. (arXiv:2212.02977v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02977
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#23545;&#33021;&#28304;&#65288;&#36127;&#33655;&#12289;&#20809;&#20239;&#25110;&#39118;&#21147;&#65289;&#30340;&#27010;&#29575;&#39044;&#27979;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#27604;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24773;&#26223;&#30340;&#27010;&#29575;&#39044;&#27979;&#23545;&#20110;&#20915;&#31574;&#32773;&#22312;&#22788;&#29702;&#38388;&#27463;&#24615;&#21487;&#20877;&#29983;&#33021;&#28304;&#26041;&#38754;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#31867;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#26368;&#36817;&#34920;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#23578;&#26410;&#26377;&#28436;&#31034;&#23427;&#20204;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#36127;&#33655;&#12289;&#20809;&#20239;&#25110;&#39118;&#21147;&#26102;&#38388;&#24207;&#21015;&#26679;&#26412;&#65292;&#32780;&#36825;&#26159;&#24212;&#23545;&#30005;&#21147;&#31995;&#32479;&#24212;&#29992;&#20013;&#30340;&#26032;&#25361;&#25112;&#25152;&#24517;&#38656;&#30340;&#20851;&#38190;&#35201;&#32032;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36825;&#31181;&#27169;&#22411;&#22312;&#21033;&#29992;&#20840;&#29699;&#33021;&#28304;&#39044;&#27979;&#22823;&#36187;2014&#30340;&#20844;&#24320;&#25968;&#25454;&#36827;&#34892;&#33021;&#28304;&#39044;&#27979;&#30340;&#39318;&#27425;&#23454;&#29616;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#65288;&#21253;&#25324;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#12289;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#27491;&#24120;&#21270;&#27969;&#65289;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scenario-based probabilistic forecasts have become vital for decision-makers in handling intermittent renewable energies. This paper presents a recent promising deep learning generative approach called denoising diffusion probabilistic models. It is a class of latent variable models which have recently demonstrated impressive results in the computer vision community. However, to our knowledge, there has yet to be a demonstration that they can generate high-quality samples of load, PV, or wind power time series, crucial elements to face the new challenges in power systems applications. Thus, we propose the first implementation of this model for energy forecasting using the open data of the Global Energy Forecasting Competition 2014. The results demonstrate this approach is competitive with other state-of-the-art deep learning generative models, including generative adversarial networks, variational autoencoders, and normalizing flows.
&lt;/p&gt;</description></item><item><title>RITA&#26159;&#19968;&#20010;&#38598;&#25104;&#32452;&#20214;&#65292;&#21487;&#20197;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#20132;&#36890;&#27969;&#65292;&#29992;&#20110;&#27979;&#35797;&#21644;&#20248;&#21270;&#33258;&#21160;&#39550;&#39542;&#31574;&#30053;&#12290;&#23427;&#30001;&#20004;&#20010;&#26680;&#24515;&#27169;&#22359;&#32452;&#25104;&#65292;&#25903;&#25345;&#30495;&#23454;&#20132;&#20114;&#24335;&#20132;&#36890;&#27969;&#21644;&#26131;&#20110;&#20351;&#29992;&#30340;&#25511;&#21046;&#20132;&#36890;&#27969;&#25509;&#21475;&#12290;</title><link>http://arxiv.org/abs/2211.03408</link><description>&lt;p&gt;
RITA:&#36890;&#36807;&#30495;&#23454;&#20132;&#20114;&#24335;&#20132;&#36890;&#27969;&#22686;&#24378;&#33258;&#21160;&#39550;&#39542;&#27169;&#25311;&#22120;
&lt;/p&gt;
&lt;p&gt;
RITA: Boost Autonomous Driving Simulators with Realistic Interactive Traffic Flow. (arXiv:2211.03408v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03408
&lt;/p&gt;
&lt;p&gt;
RITA&#26159;&#19968;&#20010;&#38598;&#25104;&#32452;&#20214;&#65292;&#21487;&#20197;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#20132;&#36890;&#27969;&#65292;&#29992;&#20110;&#27979;&#35797;&#21644;&#20248;&#21270;&#33258;&#21160;&#39550;&#39542;&#31574;&#30053;&#12290;&#23427;&#30001;&#20004;&#20010;&#26680;&#24515;&#27169;&#22359;&#32452;&#25104;&#65292;&#25903;&#25345;&#30495;&#23454;&#20132;&#20114;&#24335;&#20132;&#36890;&#27969;&#21644;&#26131;&#20110;&#20351;&#29992;&#30340;&#25511;&#21046;&#20132;&#36890;&#27969;&#25509;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36136;&#37327;&#30340;&#20132;&#36890;&#27969;&#29983;&#25104;&#26159;&#26500;&#24314;&#33258;&#21160;&#39550;&#39542;&#27169;&#25311;&#22120;&#30340;&#26680;&#24515;&#27169;&#22359;&#12290; &#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#21487;&#29992;&#30340;&#27169;&#25311;&#22120;&#26080;&#27861;&#22797;&#21046;&#20934;&#30830;&#21453;&#26144;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#21508;&#31181;&#29305;&#24449;&#30340;&#20132;&#36890;&#27169;&#24335;&#65292;&#24182;&#21516;&#26102;&#27169;&#25311;&#23545;&#27979;&#35797;&#33258;&#21160;&#39550;&#39542;&#31574;&#30053;&#30340;&#20154;&#31867;&#21453;&#24212;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Realistic Interactive TrAffic flow (RITA)&#20316;&#20026;&#29616;&#26377;&#39550;&#39542;&#27169;&#25311;&#22120;&#30340;&#38598;&#25104;&#32452;&#20214;&#65292;&#20026;&#27979;&#35797;&#21644;&#20248;&#21270;&#39550;&#39542;&#31574;&#30053;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#20132;&#36890;&#27969;&#12290; RITA&#30340;&#24320;&#21457;&#32771;&#34385;&#20102;&#19977;&#20010;&#20851;&#38190;&#29305;&#24449;&#65292;&#21363;&#30495;&#23454;&#24230;&#65292;&#22810;&#26679;&#24615;&#21644;&#21487;&#25511;&#24615;&#65292;&#30001;&#31216;&#20026;RITABackend&#21644;RITAKit&#30340;&#20004;&#20010;&#26680;&#24515;&#27169;&#22359;&#32452;&#25104;&#12290; RITABackend&#25903;&#25345;&#36710;&#36742;&#25511;&#21046;&#65292;&#24182;&#25552;&#20379;&#26469;&#33258;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#20132;&#36890;&#29983;&#25104;&#27169;&#22411;&#65292;&#32780;RITAKit&#21017;&#24320;&#21457;&#20102;&#26131;&#20110;&#20351;&#29992;&#30340;&#25509;&#21475;&#65292;&#20197;&#22312;&#27169;&#25311;&#22330;&#26223;&#20013;&#29983;&#25104;&#21487;&#25511;&#30340;&#20132;&#36890;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-quality traffic flow generation is the core module in building simulators for autonomous driving. However, the majority of available simulators are incapable of replicating traffic patterns that accurately reflect the various features of real-world data while also simulating human-like reactive responses to the tested autopilot driving strategies. Taking one step forward to addressing such a problem, we propose Realistic Interactive TrAffic flow (RITA) as an integrated component of existing driving simulators to provide high-quality traffic flow for the evaluation and optimization of the tested driving strategies. RITA is developed with consideration of three key features, i.e., fidelity, diversity, and controllability, and consists of two core modules called RITABackend and RITAKit. RITABackend is built to support vehicle-wise control and provide traffic generation models from real-world datasets, while RITAKit is developed with easy-to-use interfaces for controllable traffic gen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#20117;&#27979;&#25968;&#25454;&#34920;&#31034;&#23398;&#20064;&#38382;&#39064;&#65292;&#37319;&#29992;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#36827;&#34892;&#38750;&#23545;&#27604;&#24230;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#20943;&#23569;&#23545;&#25968;&#25454;&#30340;&#26631;&#27880;&#38656;&#27714;&#65292;&#24182;&#25552;&#39640;&#20102;&#31639;&#27861;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.14750</link><description>&lt;p&gt;
&#38024;&#23545;&#20117;&#27979;&#25968;&#25454;&#30340;&#38750;&#23545;&#27604;&#24230;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Non-contrastive representation learning for intervals from well logs. (arXiv:2209.14750v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#20117;&#27979;&#25968;&#25454;&#34920;&#31034;&#23398;&#20064;&#38382;&#39064;&#65292;&#37319;&#29992;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#36827;&#34892;&#38750;&#23545;&#27604;&#24230;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#20943;&#23569;&#23545;&#25968;&#25454;&#30340;&#26631;&#27880;&#38656;&#27714;&#65292;&#24182;&#25552;&#39640;&#20102;&#31639;&#27861;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30707;&#27833;&#21644;&#22825;&#28982;&#27668;&#34892;&#19994;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#38382;&#39064;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#27169;&#22411;&#65292;&#26681;&#25454;&#38075;&#20117;&#25968;&#25454;&#20026;&#20117;&#27573;&#25552;&#20379;&#34920;&#31034;&#24418;&#24335;&#12290;&#20197;&#24448;&#30340;&#23581;&#35797;&#20027;&#35201;&#26159;&#26377;&#30417;&#30563;&#30340;&#65292;&#24182;&#19988;&#20851;&#27880;&#20110;&#30456;&#20284;&#24615;&#20219;&#21153;&#65292;&#21363;&#20272;&#35745;&#20117;&#27573;&#20043;&#38388;&#30340;&#30456;&#20284;&#31243;&#24230;&#12290;&#25105;&#20204;&#24076;&#26395;&#22312;&#19981;&#20351;&#29992;&#24050;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26500;&#24314;&#20449;&#24687;&#37327;&#20016;&#23500;&#30340;&#34920;&#31034;&#24418;&#24335;&#12290;&#20854;&#20013;&#19968;&#20010;&#21487;&#33021;&#30340;&#26041;&#27861;&#26159;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#12290;&#19982;&#26377;&#30417;&#30563;&#33539;&#24335;&#30456;&#21453;&#65292;&#36825;&#20010;&#26041;&#27861;&#23545;&#25968;&#25454;&#38656;&#35201;&#24456;&#23569;&#25110;&#32773;&#27809;&#26377;&#26631;&#31614;&#12290;&#29616;&#20170;&#65292;&#22823;&#22810;&#25968;SSL&#26041;&#27861;&#35201;&#20040;&#26159;&#23545;&#27604;&#30340;&#65292;&#35201;&#20040;&#26159;&#38750;&#23545;&#27604;&#30340;&#12290;&#23545;&#27604;&#26041;&#27861;&#20351;&#30456;&#20284;&#30340;&#65288;&#27491;&#65289;&#23545;&#35937;&#30340;&#34920;&#31034;&#21464;&#24471;&#26356;&#21152;&#25509;&#36817;&#65292;&#24182;&#23558;&#19981;&#21516;&#30340;&#65288;&#36127;&#65289;&#23545;&#35937;&#19982;&#20043;&#36317;&#31163;&#12290;&#30001;&#20110;&#21487;&#33021;&#23384;&#22312;&#38169;&#35823;&#30340;&#27491;&#36127;&#26631;&#27880;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#20250;&#25552;&#20379;&#26356;&#24046;&#30340;&#24615;&#33021;&#12290;&#38750;&#23545;&#27604;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#27492;&#31867;&#26631;&#27880;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#24191;&#27867;&#24212;&#29992;&#12290;&#23427;&#20204;&#20165;&#20351;&#29992;&#23481;&#26131;&#35782;&#21035;&#30340;&#30456;&#20284;&#23545;&#35937;&#23545;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The representation learning problem in the oil &amp; gas industry aims to construct a model that provides a representation based on logging data for a well interval. Previous attempts are mainly supervised and focus on similarity task, which estimates closeness between intervals. We desire to build informative representations without using supervised (labelled) data. One of the possible approaches is self-supervised learning (SSL). In contrast to the supervised paradigm, this one requires little or no labels for the data. Nowadays, most SSL approaches are either contrastive or non-contrastive. Contrastive methods make representations of similar (positive) objects closer and distancing different (negative) ones. Due to possible wrong marking of positive and negative pairs, these methods can provide an inferior performance. Non-contrastive methods don't rely on such labelling and are widespread in computer vision. They learn using only pairs of similar objects that are easier to identify in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#24494;&#29289;&#29702;&#24341;&#25806;&#30340;&#30495;&#23454;&#19990;&#30028;&#21040;&#20223;&#30495;&#19990;&#30028;&#36716;&#31227;&#30340;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#36890;&#36807;&#23545;&#30495;&#23454;&#26426;&#22120;&#20154;&#30340;&#26377;&#38480;&#25968;&#25454;&#36827;&#34892;&#36845;&#20195;&#35757;&#32451;&#65292;&#20197;&#20943;&#23569;&#23454;&#21040;&#34394;&#20043;&#38388;&#30340;&#24046;&#36317;&#24182;&#20135;&#29983;&#20934;&#30830;&#30340;&#20223;&#30495;&#12290;&#35813;&#31574;&#30053;&#22312;&#32034;&#39537;&#21160;&#24352;&#21147;&#32467;&#26500;&#26426;&#22120;&#20154;&#19978;&#24471;&#21040;&#20102;&#27979;&#35797;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.06261</link><description>&lt;p&gt;
&#22522;&#20110;&#21487;&#24494;&#29289;&#29702;&#24341;&#25806;&#30340;&#32034;&#39537;&#21160;&#26426;&#22120;&#20154;&#30340;&#30495;&#23454;&#19990;&#30028;&#21040;&#20223;&#30495;&#19990;&#30028;&#30340;&#25511;&#21046;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Real2Sim2Real Transfer for Control of Cable-driven Robots via a Differentiable Physics Engine. (arXiv:2209.06261v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#24494;&#29289;&#29702;&#24341;&#25806;&#30340;&#30495;&#23454;&#19990;&#30028;&#21040;&#20223;&#30495;&#19990;&#30028;&#36716;&#31227;&#30340;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#36890;&#36807;&#23545;&#30495;&#23454;&#26426;&#22120;&#20154;&#30340;&#26377;&#38480;&#25968;&#25454;&#36827;&#34892;&#36845;&#20195;&#35757;&#32451;&#65292;&#20197;&#20943;&#23569;&#23454;&#21040;&#34394;&#20043;&#38388;&#30340;&#24046;&#36317;&#24182;&#20135;&#29983;&#20934;&#30830;&#30340;&#20223;&#30495;&#12290;&#35813;&#31574;&#30053;&#22312;&#32034;&#39537;&#21160;&#24352;&#21147;&#32467;&#26500;&#26426;&#22120;&#20154;&#19978;&#24471;&#21040;&#20102;&#27979;&#35797;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#21147;&#32467;&#26500;&#26426;&#22120;&#20154;&#30001;&#22362;&#30828;&#30340;&#26438;&#21644;&#26580;&#36719;&#30340;&#32518;&#32499;&#32452;&#25104;&#65292;&#20855;&#26377;&#39640;&#24378;&#24230;&#37325;&#37327;&#27604;&#21644;&#26174;&#33879;&#30340;&#21464;&#24418;&#33021;&#21147;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#38750;&#32467;&#26500;&#21270;&#30340;&#22320;&#24418;&#20013;&#33322;&#34892;&#24182;&#22312;&#20005;&#23803;&#30340;&#25758;&#20987;&#20013;&#23384;&#27963;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32500;&#24230;&#39640;&#12289;&#21160;&#21147;&#22797;&#26434;&#19988;&#32806;&#21512;&#32467;&#26500;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#25511;&#21046;&#12290;&#22522;&#20110;&#29289;&#29702;&#30340;&#20223;&#30495;&#26159;&#24320;&#21457;&#21487;&#20197;&#36716;&#31227;&#21040;&#23454;&#38469;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#31574;&#30053;&#30340;&#26377;&#21069;&#36884;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23454;&#21040;&#34394;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#36317;&#65292;&#23545;&#24352;&#21147;&#32467;&#26500;&#26426;&#22120;&#20154;&#36827;&#34892;&#24314;&#27169;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#21516;iable&#29289;&#29702;&#24341;&#25806;&#30340;&#24352;&#21147;&#32467;&#26500;&#26426;&#22120;&#20154;&#30340;&#30495;&#23454;&#19990;&#30028;&#21040;&#20223;&#30495;&#19990;&#30028;&#30340;&#36716;&#31227;&#31574;&#30053;(R2S2R)&#12290;&#35813;&#31574;&#30053;&#22522;&#20110;&#19968;&#20010;&#21487;&#35757;&#32451;&#30340;&#21487;&#24494;&#29289;&#29702;&#24341;&#25806;&#65292;&#36890;&#36807;&#23545;&#30495;&#23454;&#26426;&#22120;&#20154;&#30340;&#26377;&#38480;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#23558;&#21253;&#25324;&#29289;&#29702;&#23646;&#24615;&#30340;&#31163;&#32447;&#27979;&#37327;&#65292;&#22914;&#36136;&#37327;&#21644;&#20960;&#20309;&#20307;&#30340;&#21508;&#31181;&#26426;&#22120;&#20154;&#37096;&#20214;&#65292;&#20197;&#21450;&#20351;&#29992;&#38543;&#26426;&#25511;&#21046;&#31574;&#30053;&#30340;&#36712;&#36857;&#35266;&#23519;&#12290;&#21033;&#29992;&#26469;&#33258;&#30495;&#23454;&#26426;&#22120;&#20154;&#30340;&#25968;&#25454;&#65292;&#29289;&#29702;&#24341;&#25806;&#21487;&#20197;&#36827;&#34892;&#36845;&#20195;&#35757;&#32451;&#65292;&#20197;&#20943;&#23569;&#23454;&#21040;&#34394;&#20043;&#38388;&#30340;&#24046;&#36317;&#24182;&#20135;&#29983;&#20934;&#30830;&#30340;&#20223;&#30495;&#12290;&#36825;&#31181;R2S2R&#31574;&#30053;&#22312;&#32034;&#39537;&#21160;&#24352;&#21147;&#32467;&#26500;&#26426;&#22120;&#20154;&#19978;&#24471;&#21040;&#20102;&#27979;&#35797;&#65292;&#24182;&#35777;&#26126;&#20102;&#20351;&#29992;&#21487;&#24494;&#29289;&#29702;&#24341;&#25806;&#24320;&#21457;&#21487;&#20197;&#36716;&#31227;&#21040;&#23454;&#38469;&#26426;&#22120;&#20154;&#30340;&#25511;&#21046;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tensegrity robots, composed of rigid rods and flexible cables, exhibit high strength-to-weight ratios and significant deformations, which enable them to navigate unstructured terrains and survive harsh impacts. They are hard to control, however, due to high dimensionality, complex dynamics, and a coupled architecture. Physics-based simulation is a promising avenue for developing locomotion policies that can be transferred to real robots. Nevertheless, modeling tensegrity robots is a complex task due to a substantial sim2real gap. To address this issue, this paper describes a Real2Sim2Real (R2S2R) strategy for tensegrity robots. This strategy is based on a differentiable physics engine that can be trained given limited data from a real robot. These data include offline measurements of physical properties, such as mass and geometry for various robot components, and the observation of a trajectory using a random control policy. With the data from the real robot, the engine can be iterativ
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#20449;&#24565; Hoare &#36923;&#36753; (BHL) &#20197;&#35268;&#33539;&#21644;&#25512;&#29702;&#32463;&#30001;&#20551;&#35774;&#26816;&#39564;&#33719;&#24471;&#30340;&#32479;&#35745;&#20449;&#24565;&#65292;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#25512;&#29702;&#20551;&#35774;&#26816;&#39564;&#20013;&#30340;&#23454;&#38469;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2208.07074</link><description>&lt;p&gt;
&#32479;&#35745;&#20551;&#35774;&#26816;&#39564;&#31243;&#24207;&#30340;&#22768;&#38899;&#21644;&#30456;&#23545;&#23436;&#22791;&#30340;&#20449;&#24565; Hoare &#36923;&#36753;
&lt;/p&gt;
&lt;p&gt;
Sound and Relatively Complete Belief Hoare Logic for Statistical Hypothesis Testing Programs. (arXiv:2208.07074v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.07074
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20449;&#24565; Hoare &#36923;&#36753; (BHL) &#20197;&#35268;&#33539;&#21644;&#25512;&#29702;&#32463;&#30001;&#20551;&#35774;&#26816;&#39564;&#33719;&#24471;&#30340;&#32479;&#35745;&#20449;&#24565;&#65292;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#25512;&#29702;&#20551;&#35774;&#26816;&#39564;&#20013;&#30340;&#23454;&#38469;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#27491;&#24335;&#25551;&#36848;&#32479;&#35745;&#25512;&#26029;&#30340;&#35201;&#27714;&#65292;&#24182;&#26816;&#26597;&#31243;&#24207;&#26159;&#21542;&#36866;&#24403;&#20351;&#29992;&#32479;&#35745;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#20449;&#24565; Hoare &#36923;&#36753; (BHL) &#20197;&#35268;&#33539;&#21644;&#25512;&#29702;&#32463;&#30001;&#20551;&#35774;&#26816;&#39564;&#33719;&#24471;&#30340;&#32479;&#35745;&#20449;&#24565;&#12290;&#35813;&#31243;&#24207;&#36923;&#36753;&#22312;&#20551;&#35774;&#27979;&#35797;&#30340; Kripke &#27169;&#22411;&#20013;&#26159;&#21487;&#38752;&#30340;&#21644;&#30456;&#23545;&#23436;&#22791;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#20363;&#28436;&#31034;&#20102; BHL &#29992;&#20110;&#25512;&#29702;&#20551;&#35774;&#26816;&#39564;&#20013;&#30340;&#23454;&#38469;&#38382;&#39064;&#30340;&#23454;&#29992;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;&#36890;&#36807;&#20551;&#35774;&#26816;&#39564;&#33719;&#24471;&#32479;&#35745;&#20449;&#24565;&#20013;&#20808;&#39564;&#20449;&#24565;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#31243;&#24207;&#36923;&#36753;&#20869;&#22806;&#32479;&#35745;&#25512;&#26029;&#30340;&#25972;&#20010;&#22270;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new approach to formally describing the requirement for statistical inference and checking whether a program uses the statistical method appropriately. Specifically, we define belief Hoare logic (BHL) for formalizing and reasoning about the statistical beliefs acquired via hypothesis testing. This program logic is sound and relatively complete with respect to a Kripke model for hypothesis tests. We demonstrate by examples that BHL is useful for reasoning about practical issues in hypothesis testing. In our framework, we clarify the importance of prior beliefs in acquiring statistical beliefs through hypothesis testing, and discuss the whole picture of the justification of statistical inference inside and outside the program logic.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#20010;&#24615;&#21270;&#23637;&#31034;&#65292;&#36890;&#36807;&#25552;&#20379;&#25991;&#26412;&#21644;&#35270;&#35273;&#20449;&#24687;&#36827;&#19968;&#27493;&#20016;&#23500;&#25512;&#33616;&#30340;&#35299;&#37322;&#12290;&#20316;&#32773;&#20174; Google Local&#65288;&#21363;&#22320;&#22270;&#65289;&#25910;&#38598;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#22810;&#27169;&#24577;&#26694;&#26550;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20135;&#29983;&#27604;&#20808;&#21069;&#26041;&#27861;&#26356;&#22810;&#26679;&#21270;&#21644;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2207.00422</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#23637;&#31034;&#65306;&#29983;&#25104;&#38754;&#21521;&#25512;&#33616;&#30340;&#22810;&#27169;&#24577;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Personalized Showcases: Generating Multi-Modal Explanations for Recommendations. (arXiv:2207.00422v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.00422
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#20010;&#24615;&#21270;&#23637;&#31034;&#65292;&#36890;&#36807;&#25552;&#20379;&#25991;&#26412;&#21644;&#35270;&#35273;&#20449;&#24687;&#36827;&#19968;&#27493;&#20016;&#23500;&#25512;&#33616;&#30340;&#35299;&#37322;&#12290;&#20316;&#32773;&#20174; Google Local&#65288;&#21363;&#22320;&#22270;&#65289;&#25910;&#38598;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#22810;&#27169;&#24577;&#26694;&#26550;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20135;&#29983;&#27604;&#20808;&#21069;&#26041;&#27861;&#26356;&#22810;&#26679;&#21270;&#21644;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#35299;&#37322;&#27169;&#22411;&#21482;&#20026;&#25512;&#33616;&#29983;&#25104;&#25991;&#26412;&#65292;&#20294;&#20173;&#28982;&#38590;&#20197;&#20135;&#29983;&#22810;&#26679;&#21270;&#30340;&#20869;&#23481;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#20010;&#24615;&#21270;&#23637;&#31034;&#8221;&#30340;&#26032;&#20219;&#21153;&#65292;&#36890;&#36807;&#22312;&#35299;&#37322;&#20013;&#25552;&#20379;&#25991;&#26412;&#21644;&#35270;&#35273;&#20449;&#24687;&#26469;&#36827;&#19968;&#27493;&#20016;&#23500;&#35299;&#37322;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#36873;&#25321;&#19968;&#20010;&#23450;&#21046;&#30340;&#22270;&#20687;&#38598;&#65292;&#35813;&#38598;&#21512;&#19982;&#29992;&#25143;&#23545;&#25512;&#33616;&#29289;&#21697;&#30340;&#20852;&#36259;&#26368;&#30456;&#20851;&#12290;&#28982;&#21518;&#65292;&#26681;&#25454;&#25105;&#20204;&#25152;&#36873;&#30340;&#22270;&#20687;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#12290; &#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#26032;&#20219;&#21153;&#65292;&#25105;&#20204;&#20174; Google Local&#65288;&#21363;&#22320;&#22270;&#65289;&#25910;&#38598;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#23376;&#38598;&#20197;&#29983;&#25104;&#22810;&#27169;&#24577;&#35299;&#37322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#22810;&#27169;&#24577;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#29983;&#25104;&#22810;&#26679;&#21270;&#21644;&#35270;&#35273;&#19968;&#33268;&#30340;&#35299;&#37322;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21463;&#30410;&#20110;&#19981;&#21516;&#30340;&#36755;&#20837;&#27169;&#24577;&#65292;&#24182;&#19988;&#33021;&#22815;&#20135;&#29983;&#27604;&#20808;&#21069;&#26041;&#27861;&#26356;&#22810;&#26679;&#21270;&#21644;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing explanation models generate only text for recommendations but still struggle to produce diverse contents. In this paper, to further enrich explanations, we propose a new task named personalized showcases, in which we provide both textual and visual information to explain our recommendations. Specifically, we first select a personalized image set that is the most relevant to a user's interest toward a recommended item. Then, natural language explanations are generated accordingly given our selected images. For this new task, we collect a large-scale dataset from Google Local (i.e.,~maps) and construct a high-quality subset for generating multi-modal explanations. We propose a personalized multi-modal framework which can generate diverse and visually-aligned explanations via contrastive learning. Experiments show that our framework benefits from different modalities as inputs, and is able to produce more diverse and expressive explanations compared to previous methods on a varie
&lt;/p&gt;</description></item><item><title>AdaSubS &#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#23376;&#30446;&#26631;&#25628;&#32034;&#26041;&#27861;&#65292;&#23427;&#37319;&#29992;&#39564;&#35777;&#26426;&#21046;&#24555;&#36895;&#36807;&#28388;&#20986;&#19981;&#21487;&#36798;&#23376;&#30446;&#26631;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#35745;&#21010;&#26356;&#38271;&#30340;&#23376;&#30446;&#26631;&#30340;&#25928;&#29575;&#21644;&#22312;&#35745;&#21010;&#26356;&#30701;&#30340;&#23376;&#30446;&#26631;&#26041;&#38754;&#20855;&#26377;&#31934;&#32454;&#25511;&#21046;&#65292;&#22312; Sokoban&#12289;&#39764;&#26041;&#21644;&#19981;&#31561;&#24335;&#35777;&#26126;&#22522;&#20934; INT &#31561;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#36234;&#12290;</title><link>http://arxiv.org/abs/2206.00702</link><description>&lt;p&gt;
&#24555;&#36895;&#32780;&#31934;&#30830;&#65306;&#33258;&#36866;&#24212;&#23376;&#30446;&#26631;&#25628;&#32034;&#35843;&#25972;&#35268;&#21010;&#38271;&#24230;
&lt;/p&gt;
&lt;p&gt;
Fast and Precise: Adjusting Planning Horizon with Adaptive Subgoal Search. (arXiv:2206.00702v8 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00702
&lt;/p&gt;
&lt;p&gt;
AdaSubS &#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#23376;&#30446;&#26631;&#25628;&#32034;&#26041;&#27861;&#65292;&#23427;&#37319;&#29992;&#39564;&#35777;&#26426;&#21046;&#24555;&#36895;&#36807;&#28388;&#20986;&#19981;&#21487;&#36798;&#23376;&#30446;&#26631;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#35745;&#21010;&#26356;&#38271;&#30340;&#23376;&#30446;&#26631;&#30340;&#25928;&#29575;&#21644;&#22312;&#35745;&#21010;&#26356;&#30701;&#30340;&#23376;&#30446;&#26631;&#26041;&#38754;&#20855;&#26377;&#31934;&#32454;&#25511;&#21046;&#65292;&#22312; Sokoban&#12289;&#39764;&#26041;&#21644;&#19981;&#31561;&#24335;&#35777;&#26126;&#22522;&#20934; INT &#31561;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#30340;&#25512;&#29702;&#38382;&#39064;&#21253;&#21547;&#38656;&#35201;&#32791;&#36153;&#19981;&#21516;&#35745;&#31639;&#25104;&#26412;&#26469;&#30830;&#23450;&#33391;&#22909;&#34892;&#21160;&#35745;&#21010;&#30340;&#29366;&#24577;&#12290;&#38024;&#23545;&#36825;&#19968;&#29305;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#36866;&#24212;&#23376;&#30446;&#26631;&#25628;&#32034; (AdaSubS) &#30340;&#25628;&#32034;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#24615;&#22320;&#35843;&#25972;&#35268;&#21010;&#38271;&#24230;&#12290;&#20026;&#27492;&#65292;AdaSubS &#29983;&#25104;&#19981;&#21516;&#36317;&#31163;&#19979;&#30340;&#22810;&#26679;&#21270;&#23376;&#30446;&#26631;&#38598;&#12290;&#37319;&#29992;&#39564;&#35777;&#26426;&#21046;&#24555;&#36895;&#36807;&#28388;&#20986;&#19981;&#21487;&#36798;&#23376;&#30446;&#26631;&#65292;&#20197;&#20415;&#19987;&#27880;&#20110;&#21487;&#20197;&#23454;&#29616;&#30340;&#21518;&#32493;&#23376;&#30446;&#26631;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;AdaSubS &#22312;&#35745;&#21010;&#26356;&#38271;&#30340;&#23376;&#30446;&#26631;&#30340;&#25928;&#29575;&#21644;&#22312;&#35745;&#21010;&#26356;&#30701;&#30340;&#23376;&#30446;&#26631;&#26041;&#38754;&#20855;&#26377;&#31934;&#32454;&#25511;&#21046;&#65292;&#24182;&#22240;&#27492;&#22312;&#38590;&#35299;&#30340;&#35268;&#21010;&#38382;&#39064;&#19978;&#25193;&#23637;&#24471;&#24456;&#22909;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; AdaSubS &#22312;&#19977;&#20010;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153; Sokoban&#12289;&#39764;&#26041;&#21644;&#19981;&#31561;&#24335;&#35777;&#26126;&#22522;&#20934; INT &#19978;&#26174;&#33879;&#36229;&#36807;&#20998;&#23618;&#35268;&#21010;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complex reasoning problems contain states that vary in the computational cost required to determine a good action plan. Taking advantage of this property, we propose Adaptive Subgoal Search (AdaSubS), a search method that adaptively adjusts the planning horizon. To this end, AdaSubS generates diverse sets of subgoals at different distances. A verification mechanism is employed to filter out unreachable subgoals swiftly, allowing to focus on feasible further subgoals. In this way, AdaSubS benefits from the efficiency of planning with longer subgoals and the fine control with the shorter ones, and thus scales well to difficult planning problems. We show that AdaSubS significantly surpasses hierarchical planning algorithms on three complex reasoning tasks: Sokoban, the Rubik's Cube, and inequality proving benchmark INT.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#28176;&#36827;&#38750;&#32467;&#26500;&#21270;&#24133;&#20540;&#20462;&#21098;&#36827;&#34892;&#20462;&#21098;&#30340;&#27169;&#22411;&#22914;&#20309;&#22312;&#39046;&#22495;&#21644;&#20219;&#21153;&#20043;&#38388;&#36827;&#34892;&#36716;&#31227;&#12290;&#20351;&#29992;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#34987;&#20462;&#21098;&#27169;&#22411;&#33021;&#22815;&#22312;&#19981;&#36827;&#34892;&#24191;&#27867;&#30340;&#36229;&#21442;&#25968;&#25506;&#32034;&#25110;&#19987;&#38376;&#26041;&#27861;&#30340;&#24773;&#20917;&#19979;&#36716;&#31227;&#21040;&#26032;&#30340;&#39046;&#22495;&#21644;&#20219;&#21153;&#12290;&#22312;&#29983;&#29289;&#21307;&#23398;NLP&#20219;&#21153;&#20013;&#65292;Sparse*BERT&#21487;&#20197;&#36798;&#21040;&#25110;&#36229;&#36807;BioBERT&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2205.12452</link><description>&lt;p&gt;
&#31232;&#30095;*BERT&#65306;&#31232;&#30095;&#27169;&#22411;&#33021;&#22815;&#27867;&#21270;&#21040;&#26032;&#30340;&#20219;&#21153;&#21644;&#39046;&#22495;&#65288;&#32763;&#35793;&#33258;arXiv:2205.12452v2 [cs.CL] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
Sparse*BERT: Sparse Models Generalize To New tasks and Domains. (arXiv:2205.12452v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#28176;&#36827;&#38750;&#32467;&#26500;&#21270;&#24133;&#20540;&#20462;&#21098;&#36827;&#34892;&#20462;&#21098;&#30340;&#27169;&#22411;&#22914;&#20309;&#22312;&#39046;&#22495;&#21644;&#20219;&#21153;&#20043;&#38388;&#36827;&#34892;&#36716;&#31227;&#12290;&#20351;&#29992;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#34987;&#20462;&#21098;&#27169;&#22411;&#33021;&#22815;&#22312;&#19981;&#36827;&#34892;&#24191;&#27867;&#30340;&#36229;&#21442;&#25968;&#25506;&#32034;&#25110;&#19987;&#38376;&#26041;&#27861;&#30340;&#24773;&#20917;&#19979;&#36716;&#31227;&#21040;&#26032;&#30340;&#39046;&#22495;&#21644;&#20219;&#21153;&#12290;&#22312;&#29983;&#29289;&#21307;&#23398;NLP&#20219;&#21153;&#20013;&#65292;Sparse*BERT&#21487;&#20197;&#36798;&#21040;&#25110;&#36229;&#36807;BioBERT&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#22823;&#22810;&#25968;&#29616;&#20195;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31995;&#32479;&#30340;&#26680;&#24515;&#26550;&#26500;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#22312;&#20219;&#21153;&#21644;&#39046;&#22495;&#20043;&#38388;&#22987;&#32456;&#25552;&#20379;&#21331;&#36234;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#20294;&#20854;&#39640;&#35745;&#31639;&#24320;&#38144;&#21487;&#33021;&#20250;&#20351;&#25512;&#29702;&#21464;&#24471;&#22256;&#38590;&#21644;&#26114;&#36149;&#12290;&#20026;&#20102;&#20351;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#25104;&#26412;&#26356;&#20302;&#65292;&#36817;&#26399;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#20462;&#21098;&#12289;&#37327;&#21270;&#21644;&#33976;&#39311;&#26469;&#25552;&#39640;&#25512;&#29702;&#36895;&#24230;&#24182;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#28176;&#36827;&#38750;&#32467;&#26500;&#21270;&#24133;&#20540;&#20462;&#21098;&#36827;&#34892;&#20462;&#21098;&#30340;&#27169;&#22411;&#22914;&#20309;&#22312;&#39046;&#22495;&#21644;&#20219;&#21153;&#20043;&#38388;&#36827;&#34892;&#36716;&#31227;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#34987;&#20462;&#21098;&#27169;&#22411;&#33021;&#22815;&#22312;&#19981;&#36827;&#34892;&#24191;&#27867;&#30340;&#36229;&#21442;&#25968;&#25506;&#32034;&#25110;&#19987;&#38376;&#26041;&#27861;&#30340;&#24773;&#20917;&#19979;&#36716;&#31227;&#21040;&#26032;&#30340;&#39046;&#22495;&#21644;&#20219;&#21153;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#31232;&#30095;&#36890;&#29992;&#27169;&#22411;Sparse*BERT&#21487;&#20197;&#36890;&#36807;&#22312;&#38750;&#32467;&#26500;&#21270;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#19978;&#39044;&#35757;&#32451;&#21387;&#32553;&#30340;&#26550;&#26500;&#32780;&#25104;&#20026;SparseBioBERT&#65292;&#24182;&#19988;&#22312;&#22810;&#31181;&#29983;&#29289;&#21307;&#23398;NLP&#20219;&#21153;&#20013;&#21487;&#20197;&#36798;&#21040;&#25110;&#36229;&#36807;BioBERT&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models have become the core architecture upon which most modern natural language processing (NLP) systems build. These models can consistently deliver impressive accuracy and robustness across tasks and domains, but their high computational overhead can make inference difficult and expensive. To make using these models less costly, recent work has explored leveraging structured and unstructured pruning, quantization, and distillation to improve inference speed and decrease size. This paper studies how models pruned using Gradual Unstructured Magnitude Pruning can transfer between domains and tasks. Our experimentation shows that models that are pruned during pretraining using general domain masked language models can transfer to novel domains and tasks without extensive hyperparameter exploration or specialized approaches. We demonstrate that our general sparse model Sparse*BERT can become SparseBioBERT simply by pretraining the compressed architecture on unstructured bi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36947;&#36335;&#32593;&#32476;&#30340;&#20132;&#36890;&#27969;&#37327;&#25512;&#26029;&#26041;&#27861;&#65292;&#21033;&#29992;&#36947;&#36335;&#32593;&#32476;&#30340;&#20808;&#39564;&#30693;&#35782;&#20840;&#38754;&#23398;&#20064;&#32454;&#31890;&#24230;&#20132;&#36890;&#27969;&#30340;&#36947;&#36335;&#24863;&#30693;&#31354;&#38388;&#20998;&#24067;&#65292;&#26222;&#36941;&#36866;&#29992;&#20110;&#22478;&#24066;&#20132;&#36890;&#27969;&#37327;&#30417;&#27979;&#21644;&#35843;&#25511;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2109.14251</link><description>&lt;p&gt;
&#36947;&#36335;&#32593;&#32476;&#24341;&#23548;&#30340;&#22478;&#24066;&#32454;&#31890;&#24230;&#20132;&#36890;&#27969;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Road Network Guided Fine-Grained Urban Traffic Flow Inference. (arXiv:2109.14251v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.14251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36947;&#36335;&#32593;&#32476;&#30340;&#20132;&#36890;&#27969;&#37327;&#25512;&#26029;&#26041;&#27861;&#65292;&#21033;&#29992;&#36947;&#36335;&#32593;&#32476;&#30340;&#20808;&#39564;&#30693;&#35782;&#20840;&#38754;&#23398;&#20064;&#32454;&#31890;&#24230;&#20132;&#36890;&#27969;&#30340;&#36947;&#36335;&#24863;&#30693;&#31354;&#38388;&#20998;&#24067;&#65292;&#26222;&#36941;&#36866;&#29992;&#20110;&#22478;&#24066;&#20132;&#36890;&#27969;&#37327;&#30417;&#27979;&#21644;&#35843;&#25511;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#25512;&#26029;&#32454;&#31890;&#24230;&#20132;&#36890;&#27969;&#37327;&#26159;&#19968;&#20010;&#26032;&#20852;&#20294;&#33267;&#20851;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#23427;&#21487;&#20197;&#24110;&#21161;&#26497;&#22823;&#22320;&#20943;&#23569;&#25152;&#38656;&#20132;&#36890;&#30417;&#27979;&#20256;&#24863;&#22120;&#30340;&#25968;&#37327;&#20197;&#33410;&#30465;&#25104;&#26412;&#12290;&#26412;&#25991;&#21457;&#29616;&#20132;&#36890;&#27969;&#37327;&#19982;&#36947;&#36335;&#32593;&#32476;&#20855;&#26377;&#24456;&#39640;&#30340;&#30456;&#20851;&#24615;&#65292;&#20294;&#20043;&#21069;&#30340;&#30740;&#31350;&#20013;&#23436;&#20840;&#24573;&#30053;&#20102;&#36825;&#19968;&#28857;&#65292;&#25110;&#32773;&#20165;&#23558;&#20854;&#35270;&#20026;&#22806;&#37096;&#22240;&#32032;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36335;&#32593;&#24863;&#30693;&#20132;&#36890;&#27969;&#37327;&#25918;&#22823;&#22120;&#65288;RATFM&#65289;&#65292;&#23427;&#26126;&#30830;&#21033;&#29992;&#36947;&#36335;&#32593;&#32476;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20840;&#38754;&#23398;&#20064;&#32454;&#31890;&#24230;&#20132;&#36890;&#27969;&#30340;&#36947;&#36335;&#24863;&#30693;&#31354;&#38388;&#20998;&#24067;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#26041;&#21521;1D&#21367;&#31215;&#23618;&#26469;&#25552;&#21462;&#36947;&#36335;&#32593;&#32476;&#30340;&#35821;&#20041;&#29305;&#24449;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23558;&#36947;&#36335;&#32593;&#32476;&#29305;&#24449;&#21644;&#31895;&#31890;&#24230;&#27969;&#37327;&#29305;&#24449;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#35268;&#33539;&#21270;&#36947;&#36335;&#30456;&#20851;&#20132;&#36890;&#27969;&#30340;&#30701;&#36317;&#31163;&#31354;&#38388;&#20998;&#24067;&#24314;&#27169;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#36947;&#36335;&#32593;&#32476;&#29305;&#24449;&#20316;&#20026;&#26597;&#35810;&#26469;&#25429;&#33719;&#38271;&#36317;&#31163;&#36335;&#27573;&#20132;&#36890;&#27969;&#37327;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate inference of fine-grained traffic flow from coarse-grained one is an emerging yet crucial problem, which can help greatly reduce the number of the required traffic monitoring sensors for cost savings. In this work, we notice that traffic flow has a high correlation with road network, which was either completely ignored or simply treated as an external factor in previous works.To facilitate this problem, we propose a novel Road-Aware Traffic Flow Magnifier (RATFM) that explicitly exploits the prior knowledge of road networks to fully learn the road-aware spatial distribution of fine-grained traffic flow. Specifically, a multi-directional 1D convolutional layer is first introduced to extract the semantic feature of the road network. Subsequently, we incorporate the road network feature and coarse-grained flow feature to regularize the short-range spatial distribution modeling of road-relative traffic flow. Furthermore, we take the road network feature as a query to capture the l
&lt;/p&gt;</description></item></channel></rss>