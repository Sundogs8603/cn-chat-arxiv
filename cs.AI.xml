<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>Make-An-Animation &#26159;&#19968;&#20010;&#29992;&#20110;&#25991;&#26412;&#26465;&#20214;&#19979;&#19977;&#32500;&#20154;&#20307;&#36816;&#21160;&#29983;&#25104;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#35757;&#32451;&#24471;&#20197;&#24357;&#34917;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#65292;&#25552;&#39640;&#20102;&#36816;&#21160;&#29983;&#25104;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09662</link><description>&lt;p&gt;
Make-An-Animation: &#22823;&#35268;&#27169;&#25991;&#26412;&#26465;&#20214;&#19979;&#30340;&#19977;&#32500;&#20154;&#20307;&#21160;&#20316;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Make-An-Animation: Large-Scale Text-conditional 3D Human Motion Generation. (arXiv:2305.09662v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09662
&lt;/p&gt;
&lt;p&gt;
Make-An-Animation &#26159;&#19968;&#20010;&#29992;&#20110;&#25991;&#26412;&#26465;&#20214;&#19979;&#19977;&#32500;&#20154;&#20307;&#36816;&#21160;&#29983;&#25104;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#35757;&#32451;&#24471;&#20197;&#24357;&#34917;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#65292;&#25552;&#39640;&#20102;&#36816;&#21160;&#29983;&#25104;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#25351;&#23548;&#19979;&#30340;&#20154;&#20307;&#21160;&#20316;&#29983;&#25104;&#22240;&#20854;&#22312;&#21160;&#30011;&#21644;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#24212;&#29992;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#26368;&#36817;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#21160;&#20316;&#29983;&#25104;&#26041;&#38754;&#30340;&#24212;&#29992;&#20351;&#29983;&#25104;&#21160;&#20316;&#30340;&#36136;&#37327;&#24471;&#21040;&#20102;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#23616;&#38480;&#20110;&#30456;&#23545;&#36739;&#23567;&#35268;&#27169;&#30340;&#36816;&#21160;&#25429;&#25417;&#25968;&#25454;&#65292;&#23548;&#33268;&#23545;&#26356;&#21152;&#22810;&#26679;&#21270;&#30340;&#8220;&#37326;&#22806;&#8221;&#25552;&#31034;&#30340;&#24615;&#33021;&#36739;&#24046;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Make-An-Animation&#65292;&#36825;&#26159;&#19968;&#20010;&#25991;&#26412;&#26465;&#20214;&#30340;&#20154;&#20307;&#36816;&#21160;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#20174;&#22823;&#35268;&#27169;&#30340;&#22270;&#20687;&#25991;&#26412;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#26356;&#22810;&#26679;&#21270;&#30340;&#23039;&#21183;&#21644;&#25552;&#31034;&#65292;&#20174;&#32780;&#22312;&#21069;&#26399;&#24037;&#20316;&#30340;&#22522;&#30784;&#19978;&#26174;&#30528;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;Make-An-Animation&#32463;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#35757;&#32451;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#31574;&#21010;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#20174;&#22270;&#20687;&#25991;&#26412;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#30340;&#65288;&#25991;&#26412;&#65292;&#38745;&#24577;&#20551;&#23039;&#24577;&#65289;&#23545;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#22312;&#36816;&#21160;&#25429;&#25417;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#28155;&#21152;&#39069;&#22806;&#30340;&#23618;&#20197;&#27169;&#25311;&#26102;&#38388;&#32500;&#24230;&#12290;&#19982;&#20197;&#24448;&#30340;&#25193;&#25955;&#27169;&#22411;&#19981;&#21516;&#65292;Make-An-Animation&#37319;&#29992;&#20004;&#38454;&#27573;&#35757;&#32451;&#36807;&#31243;&#65292;&#22522;&#20110;&#22823;&#35268;&#27169;&#30340;&#22270;&#20687;&#25991;&#26412;&#25968;&#25454;&#38598;&#29983;&#25104;&#26356;&#21152;&#22810;&#26679;&#21270;&#21644;&#39640;&#36136;&#37327;&#30340;&#20154;&#20307;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-guided human motion generation has drawn significant interest because of its impactful applications spanning animation and robotics. Recently, application of diffusion models for motion generation has enabled improvements in the quality of generated motions. However, existing approaches are limited by their reliance on relatively small-scale motion capture data, leading to poor performance on more diverse, in-the-wild prompts. In this paper, we introduce Make-An-Animation, a text-conditioned human motion generation model which learns more diverse poses and prompts from large-scale image-text datasets, enabling significant improvement in performance over prior works. Make-An-Animation is trained in two stages. First, we train on a curated large-scale dataset of (text, static pseudo-pose) pairs extracted from image-text datasets. Second, we fine-tune on motion capture data, adding additional layers to model the temporal dimension. Unlike prior diffusion models for motion generation,
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;P2MPO&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#40065;&#26834;&#31163;&#32447;RL&#30340;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#28789;&#27963;&#30340;&#27169;&#22411;&#20272;&#35745;&#23376;&#20363;&#31243;&#21644;&#21452;&#37325;&#24754;&#35266;&#30340;&#31574;&#30053;&#20248;&#21270;&#27493;&#39588;&#65292;&#37319;&#29992;&#21452;&#37325;&#24754;&#35266;&#24615;&#21407;&#21017;&#20197;&#20811;&#26381;&#27169;&#22411;&#20559;&#31227;&#31561;&#38382;&#39064;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#20551;&#35774;&#19979;&#65292;&#35813;&#26694;&#26550;&#22312;&#25317;&#26377;&#33391;&#22909;&#30340;&#40065;&#26834;&#37096;&#20998;&#35206;&#30422;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26159;&#20855;&#22791;&#39640;&#25928;&#24615;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.09659</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#40065;&#26834;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65306;&#22522;&#20110;&#21452;&#37325;&#24754;&#35266;&#24615;&#30340;&#36890;&#29992;&#31639;&#27861;&#21644;&#24378;&#20581;&#37096;&#20998;&#35206;&#30422;
&lt;/p&gt;
&lt;p&gt;
Double Pessimism is Provably Efficient for Distributionally Robust Offline Reinforcement Learning: Generic Algorithm and Robust Partial Coverage. (arXiv:2305.09659v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;P2MPO&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#40065;&#26834;&#31163;&#32447;RL&#30340;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#28789;&#27963;&#30340;&#27169;&#22411;&#20272;&#35745;&#23376;&#20363;&#31243;&#21644;&#21452;&#37325;&#24754;&#35266;&#30340;&#31574;&#30053;&#20248;&#21270;&#27493;&#39588;&#65292;&#37319;&#29992;&#21452;&#37325;&#24754;&#35266;&#24615;&#21407;&#21017;&#20197;&#20811;&#26381;&#27169;&#22411;&#20559;&#31227;&#31561;&#38382;&#39064;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#20551;&#35774;&#19979;&#65292;&#35813;&#26694;&#26550;&#22312;&#25317;&#26377;&#33391;&#22909;&#30340;&#40065;&#26834;&#37096;&#20998;&#35206;&#30422;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26159;&#20855;&#22791;&#39640;&#25928;&#24615;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#40065;&#26834;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;&#40065;&#26834;&#31163;&#32447;RL&#65289;&#65292;&#20854;&#26088;&#22312;&#20174;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#32431;&#31929;&#22320;&#25214;&#21040;&#19968;&#20010;&#33021;&#22815;&#22312;&#25200;&#21160;&#29615;&#22659;&#20013;&#34920;&#29616;&#33391;&#22909;&#30340;&#26368;&#20248;&#24378;&#40065;&#26834;&#31574;&#30053;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;P2MPO&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#28789;&#27963;&#30340;&#27169;&#22411;&#20272;&#35745;&#23376;&#20363;&#31243;&#21644;&#21452;&#37325;&#24754;&#35266;&#30340;&#31574;&#30053;&#20248;&#21270;&#27493;&#39588;&#12290;&#21452;&#37325;&#24754;&#35266;&#24615;&#21407;&#21017;&#23545;&#20110;&#20811;&#26381;&#30001;&#34892;&#20026;&#31574;&#30053;&#21644;&#30446;&#26631;&#31574;&#30053;&#23478;&#26063;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#20197;&#21450;&#21517;&#20041;&#27169;&#22411;&#30340;&#25200;&#21160;&#25152;&#24341;&#36215;&#30340;&#20998;&#24067;&#20559;&#31227;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#23545;&#27169;&#22411;&#20272;&#35745;&#23376;&#20363;&#31243;&#36827;&#34892;&#19968;&#23450;&#20934;&#30830;&#24615;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;P2MPO&#31639;&#27861;&#22312;&#25317;&#26377;&#33391;&#22909;&#30340;&#40065;&#26834;&#37096;&#20998;&#35206;&#30422;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26159;&#21487;&#35777;&#26126;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study distributionally robust offline reinforcement learning (robust offline RL), which seeks to find an optimal robust policy purely from an offline dataset that can perform well in perturbed environments. We propose a generic algorithm framework \underline{D}oubly \underline{P}essimistic \underline{M}odel-based \underline{P}olicy \underline{O}ptimization ($\texttt{P}^2\texttt{MPO}$) for robust offline RL, which features a novel combination of a flexible model estimation subroutine and a doubly pessimistic policy optimization step. The \emph{double pessimism} principle is crucial to overcome the distributional shift incurred by i) the mismatch between behavior policy and the family of target policies; and ii) the perturbation of the nominal model. Under certain accuracy assumptions on the model estimation subroutine, we show that $\texttt{P}^2\texttt{MPO}$ is provably efficient with \emph{robust partial coverage data}, which means that the offline dataset has good coverage of the d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#22120;&#21644;&#22768;&#26126;&#24615;&#20219;&#21153;&#35268;&#33539;&#30340;&#21487;&#28385;&#36275;&#24615;&#36741;&#21161;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.09656</link><description>&lt;p&gt;
&#22768;&#26126;&#25552;&#31034;&#19979;&#30340;&#21487;&#28385;&#36275;&#24615;&#36741;&#21161;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Satisfiability-Aided Language Models Using Declarative Prompting. (arXiv:2305.09656v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#22120;&#21644;&#22768;&#26126;&#24615;&#20219;&#21153;&#35268;&#33539;&#30340;&#21487;&#28385;&#36275;&#24615;&#36741;&#21161;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#28385;&#36275;&#24615;&#36741;&#21161;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19968;&#20010;&#22768;&#26126;&#24615;&#20219;&#21153;&#35268;&#33539;&#65292;&#24182;&#21033;&#29992;&#19968;&#20010;&#29616;&#25104;&#30340;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#22120;&#24471;&#20986;&#26368;&#32456;&#31572;&#26696;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#20004;&#20010;&#20851;&#38190;&#20248;&#28857;&#65306;&#31532;&#19968;&#65292;&#22768;&#26126;&#24615;&#35268;&#33539;&#27604;&#25512;&#29702;&#27493;&#39588;&#26356;&#25509;&#36817;&#38382;&#39064;&#25551;&#36848;&#65292;&#22240;&#27492;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#35299;&#26512;&#23427;&#65307;&#31532;&#20108;&#65292;&#36890;&#36807;&#23558;&#23454;&#38469;&#25512;&#29702;&#20219;&#21153;&#22996;&#25176;&#32473;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#22120;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20445;&#35777;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior work has combined chain-of-thought prompting in large language models (LLMs) with programmatic representations to perform effective and transparent reasoning. While such an approach works very well for tasks that only require forward reasoning (e.g., straightforward arithmetic), it is less effective for constraint solving tasks that require more sophisticated planning and search. In this paper, we propose a new satisfiability-aided language modeling approach for improving the reasoning capabilities of LLMs. We use an LLM to generate a declarative task specification rather than an imperative program and leverage an off-the-shelf automated theorem prover to derive the final answer. This approach has two key advantages. The declarative specification is closer to the problem description than the reasoning steps are, so the LLM can parse it more accurately. Furthermore, by offloading the actual reasoning task to an automated theorem prover, our approach can guarantee the correctness o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Prompt-Tuning DT&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36712;&#36857;&#27573;&#20316;&#20026;prompt&#26469;&#25351;&#23548;RL agent&#33719;&#21462;&#29615;&#22659;&#20449;&#24687;&#65292;&#20174;&#32780;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#20248;&#31168;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.09648</link><description>&lt;p&gt;
&#22522;&#20110;&#20559;&#22909;&#25490;&#24207;&#30340;Prompt-Tuning&#20915;&#31574;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Prompt-Tuning Decision Transformer with Preference Ranking. (arXiv:2305.09648v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Prompt-Tuning DT&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36712;&#36857;&#27573;&#20316;&#20026;prompt&#26469;&#25351;&#23548;RL agent&#33719;&#21462;&#29615;&#22659;&#20449;&#24687;&#65292;&#20174;&#32780;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#20248;&#31168;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt-tuning&#24050;&#25104;&#20026;&#19968;&#31181;&#24456;&#26377;&#21069;&#36884;&#30340;&#36866;&#24212;&#39044;&#35757;&#32451;&#27169;&#22411;&#21040;&#19979;&#28216;&#20219;&#21153;&#25110;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#26041;&#27861;&#12290;Prompt learning &#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#30001;&#20110;RL prompts&#20013;&#21253;&#21547;&#30340;&#22797;&#26434;&#29289;&#29702;&#21547;&#20041;&#21644;&#29615;&#22659;&#29305;&#23450;&#20449;&#24687;&#65292;&#20854;&#36866;&#29992;&#24615;&#26377;&#38480;&#12290;&#36825;&#20123;&#22240;&#32032;&#38656;&#35201;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#26469;&#27169;&#20223;&#28436;&#31034;&#65292;&#24182;&#22312;&#23398;&#20064;&#21518;&#21487;&#33021;&#23548;&#33268;&#24847;&#20041;&#30340;&#20007;&#22833;&#12290;&#27492;&#22806;&#65292;&#23558;prompt-tuning&#26041;&#27861;&#30452;&#25509;&#25193;&#23637;&#21040;RL&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;RL prompts&#26159;&#26681;&#25454;&#29615;&#22659;&#24314;&#27169;&#21644;&#20998;&#26512;&#26469;&#25351;&#23548;agent&#34892;&#20026;&#30340;&#65292;&#32780;&#19981;&#26159;&#22635;&#34917;&#32570;&#22833;&#30340;&#20449;&#24687;&#65292;&#22240;&#27492;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#35843;&#25972;prompt&#26684;&#24335;&#65292;&#22914;&#22312;NLP&#20013;&#65292;&#21487;&#33021;&#19981;&#20250;&#20135;&#29983;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Prompt-Tuning DT&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#36890;&#36807;&#23558;&#36712;&#36857;&#27573;&#29992;&#20316;prompt&#26469;&#25351;&#23548;RL agent&#33719;&#21462;&#29615;&#22659;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#40657;&#30418;&#35843;&#25972;&#26469;&#20248;&#21270;prompt&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt-tuning has emerged as a promising method for adapting pre-trained models to downstream tasks or aligning with human preferences. Prompt learning is widely used in NLP but has limited applicability to RL due to the complex physical meaning and environment-specific information contained within RL prompts. These factors require supervised learning to imitate the demonstrations and may result in a loss of meaning after learning. Additionally, directly extending prompt-tuning approaches to RL is challenging because RL prompts guide agent behavior based on environmental modeling and analysis, rather than filling in missing information, making it unlikely that adjustments to the prompt format for downstream tasks, as in NLP, can yield significant improvements. In this work, we propose the Prompt-Tuning DT algorithm to address these challenges by using trajectory segments as prompts to guide RL agents in acquiring environmental information and optimizing prompts via black-box tuning to 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#32463;&#36807;&#20840;&#22269;&#20195;&#34920;&#24615;&#35843;&#26597;&#24494;&#35843;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#22686;&#24378;&#35843;&#26597;&#30340;&#35266;&#28857;&#39044;&#27979;&#65292;&#21462;&#24471;&#20102;&#22312;&#36951;&#28431;&#25968;&#25454;&#25554;&#20540;&#21644;&#22238;&#28335;&#25512;&#29702;&#26041;&#38754;&#20248;&#31168;&#30340;&#25104;&#26524;&#65292;&#22312;&#38646;&#27425;&#39044;&#27979;&#26041;&#38754;&#20173;&#38656;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2305.09620</link><description>&lt;p&gt;
AI&#22686;&#24378;&#30340;&#35843;&#26597;&#65306;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20840;&#22269;&#20195;&#34920;&#24615;&#35843;&#26597;&#30340;&#35266;&#28857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
AI-Augmented Surveys: Leveraging Large Language Models for Opinion Prediction in Nationally Representative Surveys. (arXiv:2305.09620v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#32463;&#36807;&#20840;&#22269;&#20195;&#34920;&#24615;&#35843;&#26597;&#24494;&#35843;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#22686;&#24378;&#35843;&#26597;&#30340;&#35266;&#28857;&#39044;&#27979;&#65292;&#21462;&#24471;&#20102;&#22312;&#36951;&#28431;&#25968;&#25454;&#25554;&#20540;&#21644;&#22238;&#28335;&#25512;&#29702;&#26041;&#38754;&#20248;&#31168;&#30340;&#25104;&#26524;&#65292;&#22312;&#38646;&#27425;&#39044;&#27979;&#26041;&#38754;&#20173;&#38656;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#32463;&#36807;&#20840;&#22269;&#20195;&#34920;&#24615;&#35843;&#26597;&#24494;&#35843;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#22686;&#24378;&#35843;&#26597;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;LLMs&#22312;&#35266;&#28857;&#39044;&#27979;&#20013;&#65292;&#36951;&#28431;&#25968;&#25454;&#25554;&#20540;&#65292;&#22238;&#28335;&#25512;&#29702;&#21644;&#38646;&#27425;&#39044;&#27979;&#19977;&#20010;&#19981;&#21516;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#35770;&#26694;&#26550;&#65292;&#23558;&#35843;&#26597;&#38382;&#39064;&#12289;&#20010;&#20154;&#20449;&#24565;&#21644;&#26102;&#38388;&#32972;&#26223;&#30340;&#31070;&#32463;&#23884;&#20837;&#24341;&#20837;&#21040;&#35266;&#28857;&#39044;&#27979;&#30340;&#20010;&#24615;&#21270;LLMs&#20013;&#12290;&#22312;1972&#24180;&#21040;2021&#24180;&#30340;&#8220;&#24120;&#35268;&#31038;&#20250;&#35843;&#26597;&#8221;&#20013;&#65292;&#25105;&#20204;&#20174;68,846&#21517;&#32654;&#22269;&#20154;&#20013;&#33719;&#24471;&#20102;3,110&#20010;&#20108;&#36827;&#21046;&#35266;&#28857;&#65292;&#22312;Alpaca-7b&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#25104;&#26524;&#65292;&#22312;&#32570;&#22833;&#25968;&#25454;&#25554;&#20540;&#65288;AUC=0.87&#65292;&#20844;&#24320;&#35266;&#28857;&#39044;&#27979;&#20026;$\rho$=0.99&#65289;&#21644;&#22238;&#28335;&#25512;&#29702;&#65288;AUC=0.86&#65292;$\rho$=0.98&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#20123;&#26174;&#33879;&#30340;&#39044;&#27979;&#33021;&#21147;&#33021;&#22815;&#20197;&#39640;&#32622;&#20449;&#24230;&#22635;&#34917;&#32570;&#22833;&#30340;&#36235;&#21183;&#65292;&#24182;&#26631;&#26126;&#20844;&#20247;&#24577;&#24230;&#20309;&#26102;&#21457;&#29983;&#21464;&#21270;&#65292;&#22914;&#21516;&#24615;&#23130;&#23035;&#30340;&#33719;&#21462;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#22312;&#38646;&#27425;&#39044;&#27979;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#30340;&#34920;&#29616;&#21463;&#21040;&#38480;&#21046;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
How can we use large language models (LLMs) to augment surveys? This paper investigates three distinct applications of LLMs fine-tuned by nationally representative surveys for opinion prediction -- missing data imputation, retrodiction, and zero-shot prediction. We present a new methodological framework that incorporates neural embeddings of survey questions, individual beliefs, and temporal contexts to personalize LLMs in opinion prediction. Among 3,110 binarized opinions from 68,846 Americans in the General Social Survey from 1972 to 2021, our best models based on Alpaca-7b excels in missing data imputation (AUC = 0.87 for personal opinion prediction and $\rho$ = 0.99 for public opinion prediction) and retrodiction (AUC = 0.86, $\rho$ = 0.98). These remarkable prediction capabilities allow us to fill in missing trends with high confidence and pinpoint when public attitudes changed, such as the rising support for same-sex marriage. However, the models show limited performance in a zer
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Med-PaLM2&#65292;&#36890;&#36807;&#32467;&#21512;&#22522;&#30784;LLM&#25913;&#36827;&#12289;&#21307;&#23398;&#39046;&#22495;&#24494;&#35843;&#21644;&#25552;&#31034;&#31574;&#30053;&#65292;&#24182;&#29992;&#26032;&#39062;&#30340;&#38598;&#25104;&#31934;&#28860;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;MedQA&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;86.5%&#30340;&#21307;&#23398;&#38382;&#31572;&#20934;&#30830;&#29575;&#65292;&#36808;&#21521;&#21307;&#23398;&#19987;&#23478;&#32423;&#21035;&#30340;&#38382;&#31572;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.09617</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#38382;&#31572;&#20013;&#30340;&#24212;&#29992;&#65306;&#36808;&#21521;&#21307;&#23398;&#19987;&#23478;&#32423;&#21035;&#30340;&#38382;&#31572;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Towards Expert-Level Medical Question Answering with Large Language Models. (arXiv:2305.09617v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09617
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Med-PaLM2&#65292;&#36890;&#36807;&#32467;&#21512;&#22522;&#30784;LLM&#25913;&#36827;&#12289;&#21307;&#23398;&#39046;&#22495;&#24494;&#35843;&#21644;&#25552;&#31034;&#31574;&#30053;&#65292;&#24182;&#29992;&#26032;&#39062;&#30340;&#38598;&#25104;&#31934;&#28860;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;MedQA&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;86.5%&#30340;&#21307;&#23398;&#38382;&#31572;&#20934;&#30830;&#29575;&#65292;&#36808;&#21521;&#21307;&#23398;&#19987;&#23478;&#32423;&#21035;&#30340;&#38382;&#31572;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#35832;&#22914;&#22260;&#26827;&#21644;&#34507;&#30333;&#36136;&#25240;&#21472;&#31561;&#8220;&#23439;&#20255;&#25361;&#25112;&#8221;&#26041;&#38754;&#21462;&#24471;&#20102;&#37324;&#31243;&#30865;&#24335;&#30340;&#36827;&#23637;&#12290;&#20294;&#22238;&#31572;&#21307;&#23398;&#38382;&#39064;&#24182;&#20687;&#21307;&#29983;&#19968;&#26679;&#36827;&#34892;&#25512;&#29702;&#34987;&#35748;&#20026;&#20063;&#26159;&#19968;&#31181;&#23439;&#20255;&#25361;&#25112;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#38382;&#31572;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65307;Med-PaLM&#26159;&#31532;&#19968;&#20010;&#22312;MedQA&#25968;&#25454;&#38598;&#19978;&#20197;67.2&#65285;&#30340;&#20998;&#25968;&#36229;&#36807;&#32654;&#22269;&#21307;&#30103;&#25191;&#19994;&#32771;&#35797;&#65288;USMLE&#65289;&#26679;&#24335;&#38382;&#39064;&#30340;&#8220;&#21450;&#26684;&#8221;&#20998;&#25968;&#30340;&#27169;&#22411;&#12290; &#28982;&#32780;&#65292;&#23545;&#27604;&#27169;&#22411;&#31572;&#26696;&#21644;&#21307;&#29983;&#31572;&#26696;&#65292;&#36825;&#39033;&#21644;&#20854;&#20182;&#20808;&#21069;&#24037;&#20316;&#34920;&#26126;&#36824;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Med-PaLM2&#65292;&#36890;&#36807;&#21033;&#29992;&#22522;&#30784;LLM&#25913;&#36827;&#65288;PaLM2&#65289;&#12289;&#21307;&#23398;&#39046;&#22495;&#24494;&#35843;&#21644;&#25552;&#31034;&#31574;&#30053;&#65288;&#21253;&#25324;&#26032;&#39062;&#30340;&#38598;&#25104;&#31934;&#28860;&#26041;&#27861;&#65289;&#26469;&#24357;&#21512;&#36825;&#20123;&#24046;&#36317;&#12290;&#22312;MedQA&#25968;&#25454;&#38598;&#19978;&#65292;Med-PaLM2&#30340;&#24471;&#20998;&#21487;&#36798;86.5&#65285;&#65292;&#27604;Med-PaLM&#25552;&#39640;&#20102;&#36229;&#36807;11&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent artificial intelligence (AI) systems have reached milestones in "grand challenges" ranging from Go to protein-folding. The capability to retrieve medical knowledge, reason over it, and answer medical questions comparably to physicians has long been viewed as one such grand challenge.  Large language models (LLMs) have catalyzed significant progress in medical question answering; Med-PaLM was the first model to exceed a "passing" score in US Medical Licensing Examination (USMLE) style questions with a score of 67.2% on the MedQA dataset. However, this and other prior work suggested significant room for improvement, especially when models' answers were compared to clinicians' answers. Here we present Med-PaLM 2, which bridges these gaps by leveraging a combination of base LLM improvements (PaLM 2), medical domain finetuning, and prompting strategies including a novel ensemble refinement approach.  Med-PaLM 2 scored up to 86.5% on the MedQA dataset, improving upon Med-PaLM by over 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#24402;&#19968;&#21270;&#27969;&#26694;&#26550;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#24182;&#21457;&#30340;&#22312;&#20998;&#24067;&#20869;&#35823;&#20998;&#31867;&#65288;IDM&#65289;&#21644;&#36234;&#30028;&#26816;&#27979;&#65292;&#21487;&#20197;&#25193;&#23637;&#20808;&#21069;&#37096;&#32626;&#30340;&#20998;&#21106;&#27169;&#22411;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#65292;&#24182;&#22312;&#27979;&#35797;&#20013;&#23454;&#29616;&#24456;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.09610</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#24402;&#19968;&#21270;&#27969;&#30340;&#35821;&#20041;&#20998;&#21106;&#20013;&#24182;&#21457;&#35823;&#20998;&#31867;&#21644;&#36234;&#30028;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Concurrent Misclassification and Out-of-Distribution Detection for Semantic Segmentation via Energy-Based Normalizing Flow. (arXiv:2305.09610v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09610
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#24402;&#19968;&#21270;&#27969;&#26694;&#26550;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#24182;&#21457;&#30340;&#22312;&#20998;&#24067;&#20869;&#35823;&#20998;&#31867;&#65288;IDM&#65289;&#21644;&#36234;&#30028;&#26816;&#27979;&#65292;&#21487;&#20197;&#25193;&#23637;&#20808;&#21069;&#37096;&#32626;&#30340;&#20998;&#21106;&#27169;&#22411;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#65292;&#24182;&#22312;&#27979;&#35797;&#20013;&#23454;&#29616;&#24456;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#23545;&#20110;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#20998;&#24067;&#30456;&#20284;&#30340;&#27979;&#35797;&#26679;&#26412;&#36827;&#34892;&#20998;&#31867;&#20855;&#26377;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204;&#30340;&#21028;&#21035;&#24335;&#38381;&#21512;&#26041;&#27861;&#22312;&#23454;&#38469;&#25968;&#25454;&#35774;&#32622;&#20013;&#23545;&#20110;&#20998;&#24067;&#20559;&#31227;&#21644;&#36234;&#30028;&#65288;OOD&#65289;&#31867;&#19981;&#22815;&#31283;&#20581;&#12290;&#22240;&#27492;&#65292;&#22312;&#27979;&#35797;&#26102;&#38388;&#20351;&#29992;&#39044;&#27979;&#27010;&#29575;&#20316;&#20026;&#32622;&#20449;&#24230;&#20998;&#25968;&#26102;&#65292;&#39044;&#27979;&#30340;&#27010;&#29575;&#21487;&#33021;&#38750;&#24120;&#19981;&#31934;&#30830;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27491;&#24120;&#21270;&#27969;&#26694;&#26550;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#24182;&#21457;&#30340;&#22312;&#20998;&#24067;&#20869;&#35823;&#20998;&#31867;&#65288;IDM&#65289;&#21644;&#36234;&#30028;&#26816;&#27979;&#12290;&#25152;&#25552;&#20986;&#30340;&#22522;&#20110;&#27969;&#30340;&#24102;&#26377;&#33021;&#37327;&#36755;&#20837;&#30340;&#26816;&#27979;&#22120;&#65288;FlowEneDet&#65289;&#21487;&#20197;&#25193;&#23637;&#20808;&#21069;&#37096;&#32626;&#30340;&#20998;&#21106;&#27169;&#22411;&#32780;&#26080;&#38656;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;FlowEneDet&#22312;&#20869;&#23384;&#21344;&#29992;&#26041;&#38754;&#20855;&#26377;&#26497;&#23567;&#30340;&#22686;&#21152;&#65292;&#22240;&#27492;&#20855;&#26377;&#20302;&#22797;&#26434;&#24230;&#26550;&#26500;&#12290;&#22312;Cityscapes&#12289;Cityscapes-C&#12289;FishyScapes&#21644; SegmentMeIfYouCan&#35780;&#20272;&#20013;&#65292;FlowEneDet&#22312;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;DeepLabV3+&#21644;Translated title:&#22522;&#20110;&#33021;&#37327;&#24402;&#19968;&#21270;&#27969;&#30340;&#35821;&#20041;&#20998;&#21106;&#20013;&#24182;&#21457;&#35823;&#20998;&#31867;&#21644;&#36234;&#30028;&#26816;&#27979;&#30340;&#27979;&#35797;&#20013;&#23454;&#29616;&#24456;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent semantic segmentation models accurately classify test-time examples that are similar to a training dataset distribution. However, their discriminative closed-set approach is not robust in practical data setups with distributional shifts and out-of-distribution (OOD) classes. As a result, the predicted probabilities can be very imprecise when used as confidence scores at test time. To address this, we propose a generative model for concurrent in-distribution misclassification (IDM) and OOD detection that relies on a normalizing flow framework. The proposed flow-based detector with an energy-based inputs (FlowEneDet) can extend previously deployed segmentation models without their time-consuming retraining. Our FlowEneDet results in a low-complexity architecture with marginal increase in the memory footprint. FlowEneDet achieves promising results on Cityscapes, Cityscapes-C, FishyScapes and SegmentMeIfYouCan benchmarks in IDM/OOD detection when applied to pretrained DeepLabV3+ and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;&#32321;&#24537;&#22810;&#36710;&#36947;&#39640;&#36895;&#20844;&#36335;&#20132;&#36890;&#25317;&#22581;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23398;&#20064;&#36866;&#24212;&#24615;&#32469;&#34892;&#31574;&#30053;&#65292;&#22312;&#20943;&#23569;&#25317;&#22581;&#21644;&#25552;&#39640;&#36710;&#36895;&#30340;&#21516;&#26102;&#65292;&#20248;&#21270;&#39640;&#36895;&#20844;&#36335;&#36710;&#36947;&#21644;&#21608;&#36793;&#23616;&#37096;&#25910;&#21457;&#36335;&#32593;&#30340;&#20351;&#29992;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#21487;&#20197;&#26174;&#33879;&#20248;&#21270;&#36947;&#36335;&#25910;&#21457;&#25928;&#29575;&#24182;&#20943;&#23569;&#24635;&#36890;&#34892;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2305.09600</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26368;&#22823;&#21270;&#32321;&#24537;&#36335;&#27573;&#36890;&#34892;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning to Maximize Arterial Usage during Extreme Congestion. (arXiv:2305.09600v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09600
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;&#32321;&#24537;&#22810;&#36710;&#36947;&#39640;&#36895;&#20844;&#36335;&#20132;&#36890;&#25317;&#22581;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23398;&#20064;&#36866;&#24212;&#24615;&#32469;&#34892;&#31574;&#30053;&#65292;&#22312;&#20943;&#23569;&#25317;&#22581;&#21644;&#25552;&#39640;&#36710;&#36895;&#30340;&#21516;&#26102;&#65292;&#20248;&#21270;&#39640;&#36895;&#20844;&#36335;&#36710;&#36947;&#21644;&#21608;&#36793;&#23616;&#37096;&#25910;&#21457;&#36335;&#32593;&#30340;&#20351;&#29992;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#21487;&#20197;&#26174;&#33879;&#20248;&#21270;&#36947;&#36335;&#25910;&#21457;&#25928;&#29575;&#24182;&#20943;&#23569;&#24635;&#36890;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36947;&#36335;&#32593;&#32476;&#20013;&#65292;&#20132;&#36890;&#20107;&#25925;&#21644;&#20854;&#20182;&#20107;&#20214;&#65292;&#22914;&#26524;&#19981;&#21152;&#20197;&#32531;&#35299;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#32423;&#32852;&#25925;&#38556;&#65292;&#24433;&#21709;&#31995;&#32479;&#30340;&#22823;&#37096;&#20998;&#21151;&#33021;&#12290;&#21450;&#26102;&#22788;&#29702;&#36825;&#31181;&#26497;&#31471;&#25317;&#22581;&#24773;&#20917;&#26159;&#38477;&#20302;&#25490;&#25918;&#37327;&#12289;&#25552;&#39640;&#29983;&#20135;&#29575;&#21644;&#25913;&#21892;&#22478;&#24066;&#29983;&#27963;&#36136;&#37327;&#30340;&#24517;&#35201;&#25163;&#27573;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#26041;&#27861;&#65292;&#22312;&#32321;&#24537;&#30340;&#22810;&#36710;&#36947;&#39640;&#36895;&#20844;&#36335;&#19978;&#20943;&#23569;&#20132;&#36890;&#25317;&#22581;&#12290;&#35813;&#26234;&#33021;&#20307;&#34987;&#35757;&#32451;&#23398;&#20064;&#36866;&#24212;&#24615;&#32469;&#34892;&#31574;&#30053;&#65292;&#20197;&#20415;&#22312;&#25317;&#22581;&#30340;&#39640;&#36895;&#20844;&#36335;&#20132;&#36890;&#20013;&#26368;&#20248;&#22320;&#21033;&#29992;&#39640;&#36895;&#20844;&#36335;&#36710;&#36947;&#21644;&#21608;&#36793;&#23616;&#37096;&#25910;&#21457;&#36335;&#32593;&#65292;&#22870;&#21169;&#26159;&#20943;&#23569;&#25317;&#22581;&#21450;&#25552;&#39640;&#36710;&#36895;&#12290;&#23454;&#39564;&#35774;&#32622;&#22312;&#32654;&#22269;&#21326;&#30427;&#39039;&#24030;Shoreline&#24066;&#30340;&#19968;&#27573;&#38271;2.6&#33521;&#37324;&#12289;4&#26465;&#36710;&#36947;&#30340;&#39640;&#36895;&#20844;&#36335;&#20013;&#36827;&#34892;&#65292;&#22312;&#24494;&#35266;&#21644;&#36830;&#32493;&#30340;&#32508;&#21512;&#20132;&#36890;&#27169;&#25311;&#22120;SUMO&#65288;&#22478;&#24066;&#31227;&#21160;&#20223;&#30495;&#65289;&#20013;&#27169;&#25311;&#20102;&#20004;&#20010;&#20986;&#21475;&#21644;&#30456;&#20851;&#30340;&#25910;&#21457;&#36335;&#65292;&#21516;&#26102;&#20351;&#29992;&#20102;&#21442;&#25968;&#21270;&#30340;&#36716;&#31227;&#23398;&#20064;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#26497;&#31471;&#25317;&#22581;&#26102;&#30340;&#36947;&#36335;&#25910;&#21457;&#25928;&#29575;&#24182;&#20943;&#23569;&#24635;&#36890;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collisions, crashes, and other incidents on road networks, if left unmitigated, can potentially cause cascading failures that can affect large parts of the system. Timely handling such extreme congestion scenarios is imperative to reduce emissions, enhance productivity, and improve the quality of urban living. In this work, we propose a Deep Reinforcement Learning (DRL) approach to reduce traffic congestion on multi-lane freeways during extreme congestion. The agent is trained to learn adaptive detouring strategies for congested freeway traffic such that the freeway lanes along with the local arterial network in proximity are utilized optimally, with rewards being congestion reduction and traffic speed improvement. The experimental setup is a 2.6-mile-long 4-lane freeway stretch in Shoreline, Washington, USA with two exits and associated arterial roads simulated on a microscopic and continuous multi-modal traffic simulator SUMO (Simulation of Urban MObility) while using parameterized t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#20851;&#33410;&#29289;&#20307;&#25805;&#20316;&#20013;&#20351;&#29992;&#26412;&#20307;&#24863;&#30693;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#65292;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#31995;&#32479;&#24182;&#36827;&#34892;&#21021;&#27493;&#27979;&#35797;&#65292;&#21457;&#29616;&#20102;&#20854;&#34920;&#29616;&#33391;&#22909;&#65292;&#23545;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;&#20851;&#33410;&#29289;&#20307;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2305.09584</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#20851;&#33410;&#29289;&#20307;&#25805;&#20316;&#20013;&#30340;&#26412;&#20307;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Revisiting Proprioceptive Sensing for Articulated Object Manipulation. (arXiv:2305.09584v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09584
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#20851;&#33410;&#29289;&#20307;&#25805;&#20316;&#20013;&#20351;&#29992;&#26412;&#20307;&#24863;&#30693;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#65292;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#31995;&#32479;&#24182;&#36827;&#34892;&#21021;&#27493;&#27979;&#35797;&#65292;&#21457;&#29616;&#20102;&#20854;&#34920;&#29616;&#33391;&#22909;&#65292;&#23545;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;&#20851;&#33410;&#29289;&#20307;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36741;&#21161;&#20154;&#31867;&#30340;&#26426;&#22120;&#20154;&#38656;&#35201;&#19982;&#20851;&#33410;&#29289;&#20307;&#65288;&#20363;&#22914;&#27249;&#26588;&#25110;&#24494;&#27874;&#28809;&#65289;&#20132;&#20114;&#12290;&#26089;&#26399;&#30740;&#31350;&#20351;&#29992;&#26412;&#20307;&#24863;&#30693;&#26469;&#22312;&#25509;&#35302;&#36807;&#31243;&#20013;&#20272;&#35745;&#20851;&#33410;&#26426;&#21046;&#12290;&#20294;&#26159;&#65292;&#29616;&#22312;&#20960;&#20046;&#25152;&#26377;&#31995;&#32479;&#22312;&#25509;&#35302;&#36807;&#31243;&#20013;&#20165;&#20351;&#29992;&#35270;&#35273;&#32780;&#19981;&#20877;&#32771;&#34385;&#26412;&#20307;&#24863;&#30693;&#20449;&#24687;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#25509;&#35302;&#36807;&#31243;&#20013;&#20351;&#29992;&#26412;&#20307;&#24863;&#30693;&#20449;&#24687;&#26159;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#26469;&#28304;&#65292;&#24182;&#19988;&#22312;&#25991;&#29486;&#20013;&#27809;&#26377;&#26126;&#30830;&#30340;&#19981;&#20351;&#29992;&#23427;&#30340;&#21160;&#26426;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#20174;&#32473;&#23450;&#30340;&#25569;&#21462;&#24320;&#22987;&#65292;&#20351;&#29992;&#26412;&#20307;&#24863;&#30693;&#26469;&#25511;&#21046;&#20301;&#32622;&#21644;&#24179;&#34892;&#22841;&#25345;&#22120;&#25171;&#24320;&#27249;&#26588;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23450;&#24615;&#35780;&#20272;&#65292;&#21457;&#29616;&#22841;&#25345;&#22120;&#21644;&#25226;&#25163;&#20043;&#38388;&#30340;&#28369;&#21160;&#38480;&#21046;&#20102;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#25105;&#20204;&#21457;&#29616;&#35813;&#31995;&#32479;&#24050;&#32463;&#34920;&#29616;&#24471;&#38750;&#24120;&#22909;&#12290;&#36825;&#24341;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#25105;&#20204;&#26159;&#21542;&#24212;&#35813;&#26356;&#22810;&#22320;&#21033;&#29992;&#20851;&#33410;&#29289;&#20307;&#25805;&#20316;&#20013;&#30340;&#26412;&#20307;&#24863;&#30693;&#20449;&#24687;&#65311;
&lt;/p&gt;
&lt;p&gt;
Robots that assist humans will need to interact with articulated objects such as cabinets or microwaves. Early work on creating systems for doing so used proprioceptive sensing to estimate joint mechanisms during contact. However, nowadays, almost all systems use only vision and no longer consider proprioceptive information during contact. We believe that proprioceptive information during contact is a valuable source of information and did not find clear motivation for not using it in the literature. Therefore, in this paper, we create a system that, starting from a given grasp, uses proprioceptive sensing to open cabinets with a position-controlled robot and a parallel gripper. We perform a qualitative evaluation of this system, where we find that slip between the gripper and handle limits the performance. Nonetheless, we find that the system already performs quite well. This poses the question: should we make more use of proprioceptive information during contact in articulated object
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;UOR&#65292;&#21487;&#20197;&#33258;&#21160;&#36873;&#25321;&#35302;&#21457;&#22120;&#24182;&#23398;&#20064;&#36890;&#29992;&#36755;&#20986;&#34920;&#31034;&#65292;&#25104;&#21151;&#29575;&#39640;&#36798;99.3&#65285;&#65292;&#33021;&#22815;&#23545;&#22810;&#31181;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#19979;&#28216;&#20219;&#21153;&#23454;&#26045;&#25915;&#20987;&#65292;&#19988;&#21487;&#31361;&#30772;&#26368;&#26032;&#30340;&#38450;&#24481;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.09574</link><description>&lt;p&gt;
UOR&#65306;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
UOR: Universal Backdoor Attacks on Pre-trained Language Models. (arXiv:2305.09574v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;UOR&#65292;&#21487;&#20197;&#33258;&#21160;&#36873;&#25321;&#35302;&#21457;&#22120;&#24182;&#23398;&#20064;&#36890;&#29992;&#36755;&#20986;&#34920;&#31034;&#65292;&#25104;&#21151;&#29575;&#39640;&#36798;99.3&#65285;&#65292;&#33021;&#22815;&#23545;&#22810;&#31181;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#19979;&#28216;&#20219;&#21153;&#23454;&#26045;&#25915;&#20987;&#65292;&#19988;&#21487;&#31361;&#30772;&#26368;&#26032;&#30340;&#38450;&#24481;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#26893;&#20837;&#21518;&#38376;&#21487;&#20197;&#20256;&#36882;&#21040;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#65292;&#36825;&#23545;&#23433;&#20840;&#26500;&#25104;&#20102;&#20005;&#37325;&#23041;&#32961;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#38024;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21518;&#38376;&#25915;&#20987;&#22823;&#37117;&#26159;&#38750;&#30446;&#26631;&#21644;&#29305;&#23450;&#20219;&#21153;&#30340;&#12290;&#24456;&#23569;&#26377;&#38024;&#23545;&#30446;&#26631;&#21644;&#20219;&#21153;&#19981;&#21487;&#30693;&#24615;&#30340;&#26041;&#27861;&#20351;&#29992;&#25163;&#21160;&#39044;&#23450;&#20041;&#30340;&#35302;&#21457;&#22120;&#21644;&#36755;&#20986;&#34920;&#31034;&#65292;&#36825;&#20351;&#24471;&#25915;&#20987;&#25928;&#26524;&#19981;&#22815;&#24378;&#22823;&#21644;&#26222;&#36866;&#12290;&#26412;&#25991;&#39318;&#20808;&#24635;&#32467;&#20102;&#19968;&#20010;&#26356;&#20855;&#23041;&#32961;&#24615;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21518;&#38376;&#25915;&#20987;&#24212;&#28385;&#36275;&#30340;&#35201;&#27714;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;UOR&#65292;&#36890;&#36807;&#23558;&#25163;&#21160;&#36873;&#25321;&#21464;&#25104;&#33258;&#21160;&#20248;&#21270;&#65292;&#25171;&#30772;&#20102;&#20197;&#24448;&#26041;&#27861;&#30340;&#29942;&#39048;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#34987;&#27745;&#26579;&#30340;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65292;&#21487;&#20197;&#33258;&#21160;&#23398;&#20064;&#21508;&#31181;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#35302;&#21457;&#22120;&#30340;&#26356;&#21152;&#22343;&#21248;&#21644;&#36890;&#29992;&#36755;&#20986;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#26799;&#24230;&#25628;&#32034;&#36873;&#21462;&#36866;&#24403;&#30340;&#35302;&#21457;&#35789;&#65292;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#35789;&#27719;&#34920;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;UOR&#21487;&#20197;&#22312;&#21508;&#31181;PLMs&#21644;&#19979;&#28216;&#20219;&#21153;&#20013;&#23454;&#29616;&#39640;&#21518;&#38376;&#25104;&#21151;&#29575;&#65288;&#39640;&#36798;99.3&#65285;&#65289;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;UOR&#36824;&#21487;&#20197;&#31361;&#30772;&#23545;&#25239;&#21518;&#38376;&#25915;&#20987;&#30340;&#26368;&#26032;&#38450;&#24481;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backdoors implanted in pre-trained language models (PLMs) can be transferred to various downstream tasks, which exposes a severe security threat. However, most existing backdoor attacks against PLMs are un-targeted and task-specific. Few targeted and task-agnostic methods use manually pre-defined triggers and output representations, which prevent the attacks from being more effective and general. In this paper, we first summarize the requirements that a more threatening backdoor attack against PLMs should satisfy, and then propose a new backdoor attack method called UOR, which breaks the bottleneck of the previous approach by turning manual selection into automatic optimization. Specifically, we define poisoned supervised contrastive learning which can automatically learn the more uniform and universal output representations of triggers for various PLMs. Moreover, we use gradient search to select appropriate trigger words which can be adaptive to different PLMs and vocabularies. Experi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22522;&#20110;&#23450;&#24615;&#20998;&#26512;&#65292;&#23545;&#36127;&#36131;&#23558;AI&#20262;&#29702;&#34701;&#20837;&#20135;&#21697;&#24320;&#21457;&#30340;&#25216;&#26415;&#24037;&#20154;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20262;&#29702;&#20225;&#19994;&#23478;&#38754;&#20020;&#30340;&#19977;&#22823;&#38556;&#30861;&#26159;&#65306; &#22312;&#20844;&#21496;&#20135;&#21697;&#21457;&#24067;&#20026;&#20013;&#24515;&#30340;&#29615;&#22659;&#20013;&#65292;&#38590;&#20197;&#20351;&#20262;&#29702;&#20248;&#20808;&#32771;&#34385;&#65307;&#22312;&#20197;&#25351;&#26631;&#28608;&#21169;&#20844;&#21496;&#30446;&#26631;&#30340;&#24773;&#20917;&#19979;&#65292;&#20262;&#29702;&#38590;&#20197;&#37327;&#21270;&#65307;&#39057;&#32321;&#30340;&#22242;&#38431;&#37325;&#32452;&#20351;&#24471;&#33719;&#21462;&#30693;&#35782;&#21644;&#32500;&#25252;&#20851;&#31995;&#21464;&#24471;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2305.09573</link><description>&lt;p&gt;
AI&#20262;&#29702;&#65306;&#32452;&#32455;&#25361;&#25112;&#21644;&#20262;&#29702;&#20225;&#19994;&#23478;&#20013;&#30340;&#39118;&#38505;&#20010;&#20154;&#21270;
&lt;/p&gt;
&lt;p&gt;
Walking the Walk of AI Ethics: Organizational Challenges and the Individualization of Risk among Ethics Entrepreneurs. (arXiv:2305.09573v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09573
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22522;&#20110;&#23450;&#24615;&#20998;&#26512;&#65292;&#23545;&#36127;&#36131;&#23558;AI&#20262;&#29702;&#34701;&#20837;&#20135;&#21697;&#24320;&#21457;&#30340;&#25216;&#26415;&#24037;&#20154;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20262;&#29702;&#20225;&#19994;&#23478;&#38754;&#20020;&#30340;&#19977;&#22823;&#38556;&#30861;&#26159;&#65306; &#22312;&#20844;&#21496;&#20135;&#21697;&#21457;&#24067;&#20026;&#20013;&#24515;&#30340;&#29615;&#22659;&#20013;&#65292;&#38590;&#20197;&#20351;&#20262;&#29702;&#20248;&#20808;&#32771;&#34385;&#65307;&#22312;&#20197;&#25351;&#26631;&#28608;&#21169;&#20844;&#21496;&#30446;&#26631;&#30340;&#24773;&#20917;&#19979;&#65292;&#20262;&#29702;&#38590;&#20197;&#37327;&#21270;&#65307;&#39057;&#32321;&#30340;&#22242;&#38431;&#37325;&#32452;&#20351;&#24471;&#33719;&#21462;&#30693;&#35782;&#21644;&#32500;&#25252;&#20851;&#31995;&#21464;&#24471;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20844;&#20247;&#23545;&#25216;&#26415;&#20449;&#20219;&#19979;&#38477;&#30340;&#32972;&#26223;&#19979;&#65292;&#35745;&#31639;&#26426;&#20262;&#29702;&#24050;&#32463;&#25104;&#20026;&#28966;&#28857;&#65292;&#24182;&#19988;&#25209;&#35780;&#23478;&#24050;&#32463;&#23545;&#20225;&#19994;&#20262;&#29702;&#27927;&#28068;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#25506;&#35752;AI&#20262;&#29702;&#20215;&#20540;&#22312;&#25216;&#26415;&#20844;&#21496;&#20013;&#30340;&#23454;&#38469;&#23454;&#26045;&#24773;&#20917;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#36127;&#36131;&#23558;AI&#20262;&#29702;&#34701;&#20837;&#20135;&#21697;&#24320;&#21457;&#30340;&#25216;&#26415;&#24037;&#20154;&#36827;&#34892;&#23450;&#24615;&#20998;&#26512;&#65292;&#21457;&#29616;&#24037;&#20154;&#20204;&#38754;&#20020;&#19968;&#20010;&#25919;&#31574;&#12289;&#23454;&#36341;&#21644;&#32467;&#26524;&#20998;&#31163;&#30340;&#29615;&#22659;&#12290;&#25105;&#20204;&#23558;AI&#20262;&#29702;&#24037;&#20316;&#32773;&#35299;&#26512;&#20026;&#20262;&#29702;&#20225;&#19994;&#23478;&#65292;&#20182;&#20204;&#22312;&#32452;&#32455;&#20869;&#24037;&#20316;&#65292;&#20197;&#20351;&#26032;&#30340;&#20262;&#29702;&#30456;&#20851;&#23454;&#36341;&#21046;&#24230;&#21270;&#12290;&#25105;&#20204;&#26174;&#31034;&#20262;&#29702;&#20225;&#19994;&#23478;&#38754;&#20020;&#19977;&#20010;&#20027;&#35201;&#38556;&#30861;&#12290;&#39318;&#20808;&#65292;&#20182;&#20204;&#22312;&#22260;&#32469;&#36719;&#20214;&#20135;&#21697;&#21457;&#24067;&#20026;&#20013;&#24515;&#30340;&#29615;&#22659;&#20013;&#21162;&#21147;&#20351;&#20262;&#29702;&#20248;&#20808;&#32771;&#34385;&#12290;&#20854;&#27425;&#65292;&#22312;&#20197;&#25351;&#26631;&#28608;&#21169;&#20844;&#21496;&#30446;&#26631;&#30340;&#24773;&#20917;&#19979;&#65292;&#20262;&#29702;&#22312;&#38590;&#20197;&#37327;&#21270;&#12290;&#31532;&#19977;&#65292;&#39057;&#32321;&#30340;&#22242;&#38431;&#37325;&#32452;&#20351;&#24471;&#33719;&#21462;&#30693;&#35782;&#21644;&#32500;&#25252;&#20851;&#31995;&#21464;&#24471;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
Amidst decline in public trust in technology, computing ethics have taken center stage, and critics have raised questions about corporate ethics washing. Yet few studies examine the actual implementation of AI ethics values in technology companies. Based on a qualitative analysis of technology workers tasked with integrating AI ethics into product development, we find that workers experience an environment where policies, practices, and outcomes are decoupled. We analyze AI ethics workers as ethics entrepreneurs who work to institutionalize new ethics-related practices within organizations. We show that ethics entrepreneurs face three major barriers to their work. First, they struggle to have ethics prioritized in an environment centered around software product launches. Second, ethics are difficult to quantify in a context where company goals are incentivized by metrics. Third, the frequent reorganization of teams makes it difficult to access knowledge and maintain relationships centr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#31181;&#33258;&#28982;&#30340;&#32858;&#21512;&#26041;&#27861;&#65306;&#22522;&#20110;&#20849;&#21516;&#29305;&#24449;&#23558;&#25968;&#25454;&#28857;&#20998;&#32452;&#30340;&#31934;&#36873;&#21253;&#21644;&#23558;&#25968;&#25454;&#28857;&#38543;&#26426;&#20998;&#32452;&#30340;&#38543;&#26426;&#21253;&#65292;&#23545;&#20110;&#31934;&#36873;&#21253;&#35774;&#32622;&#21644;&#24191;&#27867;&#30340;&#25439;&#22833;&#20989;&#25968;&#33539;&#22260;&#20869;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#32780;&#19981;&#20250;&#23548;&#33268;&#25968;&#25454;&#32858;&#21512;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2305.09557</link><description>&lt;p&gt;
&#22823;&#25968;&#25454;&#23398;&#20064;&#65306;&#31934;&#36873;&#21253;&#19982;&#38543;&#26426;&#21253;&#30340;&#23545;&#27604;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Learning from Aggregated Data: Curated Bags versus Random Bags. (arXiv:2305.09557v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#31181;&#33258;&#28982;&#30340;&#32858;&#21512;&#26041;&#27861;&#65306;&#22522;&#20110;&#20849;&#21516;&#29305;&#24449;&#23558;&#25968;&#25454;&#28857;&#20998;&#32452;&#30340;&#31934;&#36873;&#21253;&#21644;&#23558;&#25968;&#25454;&#28857;&#38543;&#26426;&#20998;&#32452;&#30340;&#38543;&#26426;&#21253;&#65292;&#23545;&#20110;&#31934;&#36873;&#21253;&#35774;&#32622;&#21644;&#24191;&#27867;&#30340;&#25439;&#22833;&#20989;&#25968;&#33539;&#22260;&#20869;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#32780;&#19981;&#20250;&#23548;&#33268;&#25968;&#25454;&#32858;&#21512;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#26159;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#37096;&#32626;&#30340;&#19968;&#20010;&#20027;&#35201;&#20851;&#27880;&#28857;&#65292;&#36825;&#20123;&#31995;&#32479;&#25910;&#38598;&#26469;&#33258;&#21508;&#31181;&#32676;&#20307;&#30340;&#25968;&#25454;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#31181;&#38382;&#39064;&#65292;&#19968;&#31181;&#26041;&#27861;&#26159;&#20197;&#32858;&#21512;&#30340;&#24418;&#24335;&#25910;&#38598;&#21644;&#21457;&#24067;&#25968;&#25454;&#26631;&#31614;&#65292;&#20174;&#32780;&#21487;&#20197;&#23558;&#21333;&#20010;&#29992;&#25143;&#30340;&#20449;&#24687;&#19982;&#20854;&#20182;&#29992;&#25143;&#30340;&#20449;&#24687;&#32452;&#21512;&#36215;&#26469;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#32858;&#21512;&#25968;&#25454;&#26631;&#31614;&#32780;&#38750;&#21333;&#20010;&#26631;&#31614;&#26469;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#33021;&#24615;&#65292;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#33258;&#28982;&#30340;&#32858;&#21512;&#26041;&#27861;&#65306;&#22522;&#20110;&#20849;&#21516;&#29305;&#24449;&#23558;&#25968;&#25454;&#28857;&#20998;&#32452;&#30340;&#31934;&#36873;&#21253;&#21644;&#23558;&#25968;&#25454;&#28857;&#38543;&#26426;&#20998;&#32452;&#30340;&#38543;&#26426;&#21253;&#12290;&#23545;&#20110;&#31934;&#36873;&#21253;&#35774;&#32622;&#21644;&#24191;&#27867;&#30340;&#25439;&#22833;&#20989;&#25968;&#33539;&#22260;&#20869;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#32780;&#19981;&#20250;&#23548;&#33268;&#25968;&#25454;&#32858;&#21512;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#20197;&#19979;&#35266;&#23519;&#65306;&#25439;&#22833;&#20989;&#25968;&#30340;&#26799;&#24230;&#20043;&#21644;&#21487;&#20197;&#34920;&#31034;&#20026;&#27599;&#20010;&#21253;&#30340;&#26799;&#24230;&#30340;&#21152;&#26435;&#21644;&#65292;&#20854;&#20013;&#26435;&#37325;&#26159;&#21253;&#30340;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Protecting user privacy is a major concern for many machine learning systems that are deployed at scale and collect from a diverse set of population. One way to address this concern is by collecting and releasing data labels in an aggregated manner so that the information about a single user is potentially combined with others. In this paper, we explore the possibility of training machine learning models with aggregated data labels, rather than individual labels. Specifically, we consider two natural aggregation procedures suggested by practitioners: curated bags where the data points are grouped based on common features and random bags where the data points are grouped randomly in bag of similar sizes. For the curated bag setting and for a broad range of loss functions, we show that we can perform gradient-based learning without any degradation in performance that may result from aggregating data. Our method is based on the observation that the sum of the gradients of the loss functio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;&#27880;&#24847;&#21147;EEG&#30561;&#30496;&#20998;&#26399;&#65288;HASS&#65289;&#26694;&#26550;&#65292;&#37319;&#29992;&#26102;&#31354;&#27880;&#24847;&#21147;&#26426;&#21046;&#33258;&#36866;&#24212;&#22320;&#20998;&#37197;&#32473;&#20104;&#36890;&#36947;&#38388;&#21644;&#36890;&#36947;&#20869;&#30340;EEG&#29255;&#27573;&#26435;&#37325;&#65292;&#26174;&#33879;&#25552;&#39640;&#20856;&#22411;&#30561;&#30496;&#20998;&#26399;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.09543</link><description>&lt;p&gt;
&#22522;&#20110;&#28151;&#21512;&#27880;&#24847;&#21147;&#30340;EEG&#30561;&#30496;&#20998;&#26399;
&lt;/p&gt;
&lt;p&gt;
EEG-based Sleep Staging with Hybrid Attention. (arXiv:2305.09543v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;&#27880;&#24847;&#21147;EEG&#30561;&#30496;&#20998;&#26399;&#65288;HASS&#65289;&#26694;&#26550;&#65292;&#37319;&#29992;&#26102;&#31354;&#27880;&#24847;&#21147;&#26426;&#21046;&#33258;&#36866;&#24212;&#22320;&#20998;&#37197;&#32473;&#20104;&#36890;&#36947;&#38388;&#21644;&#36890;&#36947;&#20869;&#30340;EEG&#29255;&#27573;&#26435;&#37325;&#65292;&#26174;&#33879;&#25552;&#39640;&#20856;&#22411;&#30561;&#30496;&#20998;&#26399;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30561;&#30496;&#20998;&#26399;&#23545;&#20110;&#35780;&#20272;&#30561;&#30496;&#36136;&#37327;&#21644;&#35786;&#26029;&#30561;&#30496;&#38556;&#30861;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#19981;&#21516;&#30561;&#30496;&#38454;&#27573;&#26399;&#38388;&#65292;&#25429;&#25417;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20449;&#21495;&#20869;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20851;&#31995;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#28151;&#21512;&#27880;&#24847;&#21147;EEG&#30561;&#30496;&#20998;&#26399;&#65288;HASS&#65289;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#26102;&#31354;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#26681;&#25454;&#19981;&#21516;&#30561;&#30496;&#38454;&#27573;&#26399;&#38388;&#30340;&#22823;&#33041;&#31354;&#38388;-&#26102;&#38388;&#20851;&#31995;&#33258;&#36866;&#24212;&#22320;&#20998;&#37197;&#32473;&#20104;&#36890;&#36947;&#38388;&#21644;&#36890;&#36947;&#20869;&#30340;EEG&#29255;&#27573;&#26435;&#37325;&#12290;&#22312;MASS&#21644;ISRUC&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;HASS&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20856;&#22411;&#30561;&#30496;&#20998;&#26399;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#32531;&#35299;&#20102;&#22312;&#30561;&#30496;&#20998;&#26399;&#26399;&#38388;&#25429;&#25417;EEG&#20449;&#21495;&#26102;&#30340;&#31354;&#38388;-&#26102;&#38388;&#20851;&#31995;&#30340;&#22256;&#38590;&#65292;&#24182;&#26377;&#26395;&#25552;&#39640;&#20020;&#24202;&#21644;&#30740;&#31350;&#29615;&#22659;&#20013;&#30561;&#30496;&#35780;&#20272;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sleep staging is critical for assessing sleep quality and diagnosing sleep disorders. However, capturing both the spatial and temporal relationships within electroencephalogram (EEG) signals during different sleep stages remains challenging. In this paper, we propose a novel framework called the Hybrid Attention EEG Sleep Staging (HASS) Framework. Specifically, we propose a well-designed spatio-temporal attention mechanism to adaptively assign weights to inter-channels and intra-channel EEG segments based on the spatio-temporal relationship of the brain during different sleep stages. Experiment results on the MASS and ISRUC datasets demonstrate that HASS can significantly improve typical sleep staging networks. Our proposed framework alleviates the difficulties of capturing the spatial-temporal relationship of EEG signals during sleep staging and holds promise for improving the accuracy and reliability of sleep assessment in both clinical and research settings.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#8220;&#36830;&#25509;&#35884;&#35823;&#8221;&#20316;&#20026;&#20844;&#24179;&#24615;&#38382;&#39064;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#38382;&#39064;&#65292;&#20197;&#24110;&#21161;AI&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#36991;&#20813;&#31867;&#20284;&#24773;&#20917;&#22312;&#26410;&#26469;&#20013;&#22797;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.09535</link><description>&lt;p&gt;
&#12298;&#29747;&#36798;&#65292;&#20986;&#20102;&#20160;&#20040;&#38382;&#39064;&#65311;&#12299;&#8220;&#36830;&#25509;&#35884;&#35823;&#8221;&#20316;&#20026;&#20844;&#24179;&#24615;&#38382;&#39064;&#30340;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
What's the Problem, Linda? The Conjunction Fallacy as a Fairness Problem. (arXiv:2305.09535v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09535
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#8220;&#36830;&#25509;&#35884;&#35823;&#8221;&#20316;&#20026;&#20844;&#24179;&#24615;&#38382;&#39064;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#38382;&#39064;&#65292;&#20197;&#24110;&#21161;AI&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#36991;&#20813;&#31867;&#20284;&#24773;&#20917;&#22312;&#26410;&#26469;&#20013;&#22797;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#27491;&#22312;&#19987;&#27880;&#20110;&#21019;&#24314;&#23613;&#21487;&#33021;&#25509;&#36817;&#20154;&#31867;&#26234;&#33021;&#30340;&#33258;&#21160;&#20915;&#31574;&#31995;&#32479;&#12290;&#36825;&#19968;&#21162;&#21147;&#25512;&#21160;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#24515;&#29702;&#23398;&#31561;&#35748;&#30693;&#39046;&#22495;&#12290; Daniel Kahneman&#21644;&#24050;&#25925;&#30340;Amos Tversky&#22312;&#26377;&#20559;&#35265;&#30340;&#20154;&#31867;&#20915;&#31574;&#21046;&#23450;&#26041;&#38754;&#30340;&#24037;&#20316;&#65292;&#21253;&#25324;&#23545;&#36830;&#25509;&#35884;&#35823;&#30340;&#30740;&#31350;&#65292;&#22240;&#27492;&#36827;&#34892;&#20102;&#31532;&#20108;&#27425;&#22797;&#20852;&#12290; &#22312;&#36830;&#25509;&#35884;&#35823;&#19979;&#65292;&#20915;&#31574;&#21046;&#23450;&#32773;&#20250;&#36829;&#21453;&#22522;&#26412;&#27010;&#29575;&#27861;&#21017;&#65292;&#35748;&#20026;&#36830;&#35789;&#27604;&#20854;&#20013;&#19968;&#20010;&#37096;&#20998;&#26356;&#26377;&#21487;&#33021;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#19982;&#29747;&#36798;&#38382;&#39064;&#26368;&#20026;&#33879;&#21517;&#30340;&#23454;&#39564;&#65292;&#23427;&#24050;&#34987;&#35777;&#26126;&#26159;&#32463;&#24471;&#36215;&#26102;&#38388;&#32771;&#39564;&#30340;&#12290;&#34429;&#28982;&#36825;&#31181;&#36328;&#23398;&#31185;&#30340;&#21162;&#21147;&#21463;&#21040;&#27426;&#36814;&#65292;&#20294;&#25105;&#20204;&#25285;&#24515;&#65292;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20154;&#21592;&#24573;&#30053;&#20102;&#29747;&#36798;&#38382;&#39064;&#25152;&#25429;&#25417;&#21040;&#30340;&#39537;&#21160;&#21147;&#65306;&#29747;&#36798;&#24517;&#39035;&#34987;&#21051;&#26495;&#22320;&#25551;&#36848;&#20026;&#19968;&#20010;&#22899;&#24615;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#29747;&#36798;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;AI&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#35748;&#20026;&#36830;&#25509;&#35884;&#35823;&#26159;&#20559;&#35265;&#25968;&#25454;&#38598;&#22914;&#20309;&#23548;&#33268;&#20559;&#35265;&#32467;&#26524;&#30340;&#26126;&#26174;&#20363;&#23376;&#65292;&#20174;&#32780;&#24310;&#32493;&#21644;&#25918;&#22823;&#29616;&#26377;&#30340;&#31995;&#32479;&#24615;&#20559;&#35265;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#38382;&#39064;&#20379;AI&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#20154;&#21592;&#20351;&#29992;&#65292;&#20197;&#36991;&#20813;&#31867;&#20284;&#24773;&#20917;&#22312;&#26410;&#26469;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of Artificial Intelligence (AI) is focusing on creating automated decision-making (ADM) systems that operate as close as possible to human-like intelligence. This effort has pushed AI researchers into exploring cognitive fields like psychology. The work of Daniel Kahneman and the late Amos Tversky on biased human decision-making, including the study of the conjunction fallacy, has experienced a second revival because of this. Under the conjunction fallacy a human decision-maker will go against basic probability laws and rank as more likely a conjunction over one of its parts. It has been proven overtime through a set of experiments with the Linda Problem being the most famous one. Although this interdisciplinary effort is welcomed, we fear that AI researchers ignore the driving force behind the conjunction fallacy as captured by the Linda Problem: the fact that Linda must be stereotypically described as a woman. In this paper we revisit the Linda Problem and formulate it as a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29616;&#23454;&#26102;&#21516;&#26102;&#22810;&#29289;&#20307;&#19977;&#32500;&#24418;&#29366;&#37325;&#24314;&#12289;6DoF&#23039;&#24577;&#20272;&#35745;&#21644;&#23494;&#38598;&#25235;&#21462;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#26080;&#38656;&#39034;&#24207;&#24863;&#30693;&#21644;&#25235;&#21462;&#35268;&#21010;&#27493;&#39588;&#65292;&#20855;&#26377;&#24555;&#36895;&#25512;&#29702;&#19988;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.09510</link><description>&lt;p&gt;
&#23454;&#26102;&#21516;&#26102;&#22810;&#29289;&#20307;&#19977;&#32500;&#24418;&#29366;&#37325;&#24314;&#65292;6DoF&#23039;&#24577;&#20272;&#35745;&#21644;&#23494;&#38598;&#25235;&#21462;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time Simultaneous Multi-Object 3D Shape Reconstruction, 6DoF Pose Estimation and Dense Grasp Prediction. (arXiv:2305.09510v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09510
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29616;&#23454;&#26102;&#21516;&#26102;&#22810;&#29289;&#20307;&#19977;&#32500;&#24418;&#29366;&#37325;&#24314;&#12289;6DoF&#23039;&#24577;&#20272;&#35745;&#21644;&#23494;&#38598;&#25235;&#21462;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#26080;&#38656;&#39034;&#24207;&#24863;&#30693;&#21644;&#25235;&#21462;&#35268;&#21010;&#27493;&#39588;&#65292;&#20855;&#26377;&#24555;&#36895;&#25512;&#29702;&#19988;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#25805;&#20316;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#31995;&#32479;&#20381;&#36182;&#20110;&#24863;&#30693;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#25552;&#20379;&#26377;&#20851;&#22330;&#26223;&#20013;&#23545;&#35937;&#30340;&#20960;&#20309;&#65288;&#23039;&#24577;&#21644;&#19977;&#32500;&#24418;&#29366;&#65289;&#20197;&#21450;&#20854;&#20182;&#35821;&#20041;&#20449;&#24687;&#65288;&#20363;&#22914;&#23545;&#35937;&#26631;&#31614;&#65289;&#30340;&#20449;&#24687;&#12290;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#36873;&#25321;&#30456;&#20851;&#23545;&#35937;&#19978;&#30340;&#21487;&#34892;&#25235;&#21462;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#25552;&#20379;&#22330;&#26223;&#20013;&#25152;&#26377;&#23545;&#35937;&#30340;&#20960;&#20309;&#21644;&#35821;&#20041;&#20449;&#24687;&#20197;&#21450;&#36825;&#20123;&#23545;&#35937;&#19978;&#30340;&#21487;&#34892;&#25235;&#21462;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20027;&#35201;&#20248;&#28857;&#22312;&#20110;&#20854;&#36895;&#24230;&#65292;&#22240;&#20026;&#23427;&#36991;&#20813;&#20102;&#39034;&#24207;&#24863;&#30693;&#21644;&#25235;&#21462;&#35268;&#21010;&#27493;&#39588;&#12290;&#36890;&#36807;&#35814;&#32454;&#30340;&#23450;&#37327;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#19982;&#38754;&#21521;&#23545;&#35937;&#24418;&#29366;&#12289;&#23039;&#24577;&#21644;&#25235;&#21462;&#39044;&#27979;&#30340;&#26368;&#26032;&#19987;&#29992;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#25552;&#20379;&#27599;&#31186;30&#24103;&#30340;&#24555;&#36895;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robotic manipulation systems operating in complex environments rely on perception systems that provide information about the geometry (pose and 3D shape) of the objects in the scene along with other semantic information such as object labels. This information is then used for choosing the feasible grasps on relevant objects. In this paper, we present a novel method to provide this geometric and semantic information of all objects in the scene as well as feasible grasps on those objects simultaneously. The main advantage of our method is its speed as it avoids sequential perception and grasp planning steps. With detailed quantitative analysis, we show that our method delivers competitive performance compared to the state-of-the-art dedicated methods for object shape, pose, and grasp predictions while providing fast inference at 30 frames per second speed.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20174;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#33021;&#22815;&#23436;&#20840;&#34920;&#36798;&#27010;&#29575;&#37327;&#21270;&#25512;&#29702;&#21644;&#22240;&#26524;&#25928;&#24212;&#30340; do-calculus &#25512;&#29702;&#30340;&#24418;&#24335;&#35821;&#35328;&#30340;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;&#65292;&#24182;&#30830;&#31435;&#20102;&#36825;&#20123;&#38382;&#39064;&#30340;&#31934;&#30830;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#35813;&#30740;&#31350;&#35777;&#26126;&#20102;&#22312;&#27010;&#29575;&#21644;&#22240;&#26524;&#25512;&#26029;&#20013;&#36890;&#24120;&#20351;&#29992;&#30340;&#26631;&#20934;&#35821;&#35328;&#30340;&#21464;&#31181;&#30340;&#31639;&#27861;&#38480;&#21046;&#26356;&#24378;&#12290;</title><link>http://arxiv.org/abs/2305.09508</link><description>&lt;p&gt;
&#28041;&#21450;&#27010;&#29575;&#21644;&#22240;&#26524;&#25512;&#29702;&#30340;&#25512;&#29702;&#22256;&#38590;&#24230;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Hardness of Reasoning about Probabilities and Causality. (arXiv:2305.09508v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09508
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20174;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#33021;&#22815;&#23436;&#20840;&#34920;&#36798;&#27010;&#29575;&#37327;&#21270;&#25512;&#29702;&#21644;&#22240;&#26524;&#25928;&#24212;&#30340; do-calculus &#25512;&#29702;&#30340;&#24418;&#24335;&#35821;&#35328;&#30340;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;&#65292;&#24182;&#30830;&#31435;&#20102;&#36825;&#20123;&#38382;&#39064;&#30340;&#31934;&#30830;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#35813;&#30740;&#31350;&#35777;&#26126;&#20102;&#22312;&#27010;&#29575;&#21644;&#22240;&#26524;&#25512;&#26029;&#20013;&#36890;&#24120;&#20351;&#29992;&#30340;&#26631;&#20934;&#35821;&#35328;&#30340;&#21464;&#31181;&#30340;&#31639;&#27861;&#38480;&#21046;&#26356;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20174;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#33021;&#22815;&#23436;&#20840;&#34920;&#36798;&#27010;&#29575;&#37327;&#21270;&#25512;&#29702;&#21644;&#22240;&#26524;&#25928;&#24212;&#30340; do-calculus &#25512;&#29702;&#30340;&#24418;&#24335;&#35821;&#35328;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#23454;&#20363;&#20844;&#24335;&#20801;&#35768;&#34920;&#36798;&#27010;&#29575;&#21644;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#35768;&#22810;&#20219;&#21153;&#30340;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#30830;&#31435;&#36825;&#20123;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;&#30340;&#31934;&#30830;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#33258;&#28982;&#22797;&#26434;&#31867;&#65292;&#21629;&#21517;&#20026; succ$\exists$R&#65292;&#23427;&#21487;&#20197;&#34987;&#35270;&#20026;&#20247;&#25152;&#21608;&#30693;&#30340; $\exists$R &#31867;&#30340;&#31616;&#27905;&#21464;&#31181;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#25105;&#20204;&#32771;&#34385;&#30340;&#38382;&#39064;&#26159; succ$\exists$R &#31867;&#23436;&#20840;&#30340;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#24847;&#21619;&#30528;&#23545;&#20110;&#19968;&#20123;&#22312;&#27010;&#29575;&#21644;&#22240;&#26524;&#25512;&#26029;&#20013;&#36890;&#24120;&#20351;&#29992;&#30340;&#26631;&#20934;&#35821;&#35328;&#30340;&#21464;&#31181;&#65292;&#23427;&#20204;&#27604; Fagin&#12289;Halpern &#21644; Megiddo (1990)&#20197;&#21450; Moss\'{e}&#12289;Ibeling &#21644; Icard (2022)&#35777;&#26126;&#30340;&#31639;&#27861;&#38480;&#21046;&#26356;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study formal languages which are capable of fully expressing quantitative probabilistic reasoning and do-calculus reasoning for causal effects, from a computational complexity perspective. We focus on satisfiability problems whose instance formulas allow expressing many tasks in probabilistic and causal inference. The main contribution of this work is establishing the exact computational complexity of these satisfiability problems. We introduce a new natural complexity class, named succ$\exists$R, which can be viewed as a succinct variant of the well-studied class $\exists$R, and show that the problems we consider are complete for succ$\exists$R. Our results imply even stronger algorithmic limitations than were proven by Fagin, Halpern, and Megiddo (1990) and Moss\'{e}, Ibeling, and Icard (2022) for some variants of the standard languages used commonly in probabilistic and causal inference.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;ALC&#26412;&#20307;&#20013;&#25552;&#21462;&#36890;&#29992;&#27169;&#22359;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#36890;&#36807;&#35780;&#20272;&#34920;&#26126;&#20854;&#21487;&#20197;&#22312;&#26174;&#30528;&#36739;&#30701;&#30340;&#26102;&#38388;&#20869;&#35745;&#31639;&#20986;&#65292;&#30456;&#27604;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#21487;&#20197;&#24471;&#21040;&#26356;&#23567;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.09503</link><description>&lt;p&gt;
ALC&#26412;&#20307;&#30340;&#36890;&#29992;&#27169;&#22359;&#30340;&#39640;&#25928;&#35745;&#31639;&#26041;&#27861;&#65288;&#25193;&#23637;&#29256;&#65289;
&lt;/p&gt;
&lt;p&gt;
Efficient Computation of General Modules for ALC Ontologies (Extended Version). (arXiv:2305.09503v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09503
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;ALC&#26412;&#20307;&#20013;&#25552;&#21462;&#36890;&#29992;&#27169;&#22359;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#36890;&#36807;&#35780;&#20272;&#34920;&#26126;&#20854;&#21487;&#20197;&#22312;&#26174;&#30528;&#36739;&#30701;&#30340;&#26102;&#38388;&#20869;&#35745;&#31639;&#20986;&#65292;&#30456;&#27604;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#21487;&#20197;&#24471;&#21040;&#26356;&#23567;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#25551;&#36848;&#36923;&#36753;ALC&#20013;&#25552;&#21462;&#26412;&#20307;&#36890;&#29992;&#27169;&#22359;&#30340;&#26041;&#27861;&#12290;&#23545;&#20110;&#29992;&#25143;&#25351;&#23450;&#38598;&#21512;&#30340;&#26415;&#35821;&#65292;&#19968;&#20010;&#27169;&#22359;&#26159;&#19968;&#20010;&#29702;&#35770;&#24615;&#36136;&#19978;&#22823;&#22823;&#31616;&#21270;&#30340;&#26412;&#20307;&#65292;&#23427;&#21487;&#20197;&#20445;&#30041;&#25152;&#26377;&#34164;&#21547;&#12290;&#22240;&#27492;&#65292;&#23427;&#20855;&#26377;&#26412;&#20307;&#37325;&#29992;&#21644;&#26412;&#20307;&#20998;&#26512;&#31561;&#24212;&#29992;&#12290;&#19981;&#21516;&#20110;&#20256;&#32479;&#27169;&#22359;&#65292;&#36890;&#29992;&#27169;&#22359;&#21487;&#20197;&#20351;&#29992;&#36755;&#20837;&#26412;&#20307;&#20013;&#26410;&#26174;&#24335;&#20986;&#29616;&#30340;&#20844;&#29702;&#65292;&#36825;&#20801;&#35768;&#39069;&#22806;&#30340;&#31616;&#26126;&#24615;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#36890;&#29992;&#27169;&#22359;&#20165;&#38024;&#23545;&#36731;&#37327;&#32423;&#25551;&#36848;&#36923;&#36753;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#32771;&#34385;&#26356;&#20855;&#34920;&#36798;&#21147;&#30340;&#25551;&#36848;&#36923;&#36753;ALC&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26159;&#19968;&#31181;&#22522;&#20110;&#22343;&#21248;&#25554;&#20540;&#30340;&#26032;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#30001;&#19968;&#20123;&#26032;&#30340;&#29702;&#35770;&#32467;&#26524;&#25903;&#25345;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#36890;&#29992;&#27169;&#22359;&#36890;&#24120;&#27604;&#32463;&#20856;&#27169;&#22359;&#21644;&#26368;&#20808;&#36827;&#30340;&#22343;&#21248;&#25554;&#20540;&#35745;&#31639;&#20986;&#30340;&#22343;&#21248;&#25554;&#20540;&#23567;&#65292;&#24182;&#19988;&#19982;&#22343;&#21248;&#25554;&#20540;&#30456;&#27604;&#65292;&#21487;&#20197;&#22312;&#26174;&#30528;&#36739;&#30701;&#30340;&#26102;&#38388;&#20869;&#35745;&#31639;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a method for extracting general modules for ontologies formulated in the description logic ALC. A module for an ontology is an ideally substantially smaller ontology that preserves all entailments for a user-specified set of terms. As such, it has applications such as ontology reuse and ontology analysis. Different from classical modules, general modules may use axioms not explicitly present in the input ontology, which allows for additional conciseness. So far, general modules have only been investigated for lightweight description logics. We present the first work that considers the more expressive description logic ALC. In particular, our contribution is a new method based on uniform interpolation supported by some new theoretical results. Our evaluation indicates that our general modules are often smaller than classical modules and uniform interpolants computed by the state-of-the-art, and compared with uniform interpolants, can be computed in a significantly shorter tim
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#31163;&#25955;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30452;&#25509;&#29983;&#25104;&#22797;&#35843;&#31526;&#21495;&#38899;&#20048;&#12290;&#35813;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#26679;&#26412;&#36136;&#37327;&#65292;&#24182;&#20801;&#35768;&#22312;&#38899;&#31526;&#32423;&#21035;&#19978;&#36827;&#34892;&#28789;&#27963;&#30340;&#25554;&#20540;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#36890;&#36807;&#32479;&#35745;&#25351;&#26631;&#23545;&#38899;&#20048;&#26679;&#26412;&#36136;&#37327;&#36827;&#34892;&#23450;&#37327;&#35780;&#20272;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.09489</link><description>&lt;p&gt;
&#29992;&#20110;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#30340;&#31163;&#25955;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Discrete Diffusion Probabilistic Models for Symbolic Music Generation. (arXiv:2305.09489v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09489
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#31163;&#25955;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30452;&#25509;&#29983;&#25104;&#22797;&#35843;&#31526;&#21495;&#38899;&#20048;&#12290;&#35813;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#26679;&#26412;&#36136;&#37327;&#65292;&#24182;&#20801;&#35768;&#22312;&#38899;&#31526;&#32423;&#21035;&#19978;&#36827;&#34892;&#28789;&#27963;&#30340;&#25554;&#20540;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#36890;&#36807;&#32479;&#35745;&#25351;&#26631;&#23545;&#38899;&#20048;&#26679;&#26412;&#36136;&#37327;&#36827;&#34892;&#23450;&#37327;&#35780;&#20272;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#32493;&#21644;&#31163;&#25955;&#39046;&#22495;&#20013;&#65292;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#26679;&#26412;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#31163;&#25955;DDPM&#65288;D3PM&#65289;&#23578;&#26410;&#24212;&#29992;&#20110;&#31526;&#21495;&#38899;&#20048;&#39046;&#22495;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;D3PM&#30452;&#25509;&#29983;&#25104;&#22797;&#35843;&#31526;&#21495;&#38899;&#20048;&#12290;&#26681;&#25454;&#24403;&#21069;&#30340;&#23450;&#37327;&#35780;&#20272;&#25351;&#26631;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23637;&#31034;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#26679;&#26412;&#36136;&#37327;&#65292;&#24182;&#20801;&#35768;&#22312;&#38899;&#31526;&#32423;&#21035;&#19978;&#36827;&#34892;&#28789;&#27963;&#30340;&#25554;&#20540;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#23545;&#21518;&#26399;&#20998;&#31867;&#22120;&#25351;&#23548;&#30340;&#21487;&#35775;&#38382;&#24615;&#65292;&#25193;&#22823;&#20102;&#21487;&#33021;&#24212;&#29992;&#30340;&#33539;&#22260;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20063;&#23545;&#36890;&#36807;&#32479;&#35745;&#25351;&#26631;&#23545;&#38899;&#20048;&#26679;&#26412;&#36136;&#37327;&#36827;&#34892;&#23450;&#37327;&#35780;&#20272;&#25345;&#26377;&#25209;&#21028;&#30340;&#35266;&#28857;&#65292;&#24182;&#21576;&#29616;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#24178;&#25200;&#25105;&#20204;&#30340;&#25351;&#26631;&#65292;&#29983;&#25104;&#23436;&#20840;&#34394;&#20551;&#30340;&#38750;&#38899;&#20048;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising Diffusion Probabilistic Models (DDPMs) have made great strides in generating high-quality samples in both discrete and continuous domains. However, Discrete DDPMs (D3PMs) have yet to be applied to the domain of Symbolic Music. This work presents the direct generation of Polyphonic Symbolic Music using D3PMs. Our model exhibits state-of-the-art sample quality, according to current quantitative evaluation metrics, and allows for flexible infilling at the note level. We further show, that our models are accessible to post-hoc classifier guidance, widening the scope of possible applications. However, we also cast a critical view on quantitative evaluation of music sample quality via statistical metrics, and present a simple algorithm that can confound our metrics with completely spurious, non-musical samples.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#19968;&#27425;&#24615;&#22320;&#20849;&#21516;&#35774;&#35745;&#25239;&#20307;CDR&#30340;1D&#24207;&#21015;&#21644;3D&#32467;&#26500;&#65292;&#35299;&#20915;&#20960;&#20309;&#24314;&#27169;&#21644;&#20302;&#25928;&#25512;&#26029;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.09480</link><description>&lt;p&gt;
&#20132;&#21449;&#38376;&#25511;&#22810;&#23618;&#24863;&#30693;&#26426;&#19979;&#30340;&#34507;&#30333;&#36136;&#22797;&#21512;&#29289;&#19981;&#21464;&#23884;&#20837;&#26159;&#19968;&#31181;&#19968;&#27425;&#24615;&#25239;&#20307;&#35774;&#35745;&#22120;
&lt;/p&gt;
&lt;p&gt;
Protein Complex Invariant Embedding with Cross-Gate MLP is A One-Shot Antibody Designer. (arXiv:2305.09480v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09480
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#19968;&#27425;&#24615;&#22320;&#20849;&#21516;&#35774;&#35745;&#25239;&#20307;CDR&#30340;1D&#24207;&#21015;&#21644;3D&#32467;&#26500;&#65292;&#35299;&#20915;&#20960;&#20309;&#24314;&#27169;&#21644;&#20302;&#25928;&#25512;&#26029;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25239;&#20307;&#26159;&#30001;&#20813;&#30123;&#31995;&#32479;&#20135;&#29983;&#30340;&#38024;&#23545;&#22806;&#26469;&#29289;&#36136;&#25110;&#25239;&#21407;&#30340;&#37325;&#35201;&#34507;&#30333;&#36136;&#12290;&#25239;&#20307;&#30340;&#29305;&#24322;&#24615;&#30001;&#20854;&#20114;&#34917;&#20915;&#23450;&#21306;&#65288;CDR&#65289;&#20915;&#23450;&#65292;CDR&#20301;&#20110;&#25239;&#20307;&#38142;&#30340;&#21487;&#21464;&#21306;&#22495;&#20013;&#65292;&#24418;&#25104;&#19982;&#25239;&#21407;&#32467;&#21512;&#30340;&#20301;&#28857;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#21033;&#29992;&#22797;&#26434;&#30340;&#25216;&#26415;&#29983;&#25104;CDR&#65292;&#20294;&#23427;&#20204;&#36973;&#21463;&#20102;&#20960;&#20309;&#24314;&#27169;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#24120;&#35265;&#30340;&#36845;&#20195;&#31934;&#21270;&#31574;&#30053;&#23548;&#33268;&#20102;&#20302;&#25928;&#30340;&#25512;&#26029;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#19968;&#27425;&#24615;&#22320;&#20849;&#21516;&#35774;&#35745;CDR&#30340;1D&#24207;&#21015;&#21644;3D&#32467;&#26500;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#23558;&#25239;&#20307;CDR&#35774;&#35745;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;&#65288;i&#65289;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#20960;&#20309;&#24314;&#27169;&#21644;&#65288;ii&#65289;&#24207;&#21015;&#32467;&#26500;&#20849;&#23398;&#20064;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#34507;&#30333;&#36136;&#22797;&#21512;&#29289;&#19981;&#21464;&#23884;&#20837;&#65292;&#21487;&#25429;&#25417;&#34507;&#30333;&#36136;&#39592;&#26550;&#21407;&#23376;&#65288;&#21253;&#25324;C&#945;&#12289;N&#12289;C&#21644;O&#21407;&#23376;&#65289;&#20043;&#38388;&#30340;&#20869;&#37096;&#21644;&#22806;&#37096;&#32452;&#20998;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#23454;&#29616;&#20840;&#38754;&#30340;&#20960;&#20309;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Antibodies are crucial proteins produced by the immune system in response to foreign substances or antigens. The specificity of an antibody is determined by its complementarity-determining regions (CDRs), which are located in the variable domains of the antibody chains and form the antigen-binding site. Previous studies have utilized complex techniques to generate CDRs, but they suffer from inadequate geometric modeling. Moreover, the common iterative refinement strategies lead to an inefficient inference. In this paper, we propose a deep generative model that can co-design 1D sequences and 3D structures of CDRs in a one-shot manner. To achieve this, we decouple the antibody CDR design into two stages: (i) geometric modeling of protein structures and (ii) sequence-structure co-learning. We develop a protein complex invariant embedding that captures both intra- and inter-component interactions among the backbone atoms including C$\alpha$, N, C, and O atoms to achieve comprehensive geome
&lt;/p&gt;</description></item><item><title>&#35299;&#37322;&#22411;AI&#30740;&#31350;&#20013;&#30340;&#21442;&#19982;&#32773;&#26679;&#26412;&#37327;&#21644;&#25512;&#24191;&#38382;&#39064;&#38480;&#21046;&#20102;&#35813;&#39046;&#22495;&#23545;XAI&#31995;&#32479;&#21487;&#35299;&#37322;&#24615;&#30340;&#35780;&#20272;&#65292;&#38656;&#35201;&#26356;&#20855;&#21253;&#23481;&#24615;&#30340;&#29992;&#25143;&#30740;&#31350;&#21407;&#21017;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.09477</link><description>&lt;p&gt;
&#35299;&#37322;&#22411;AI&#30740;&#31350;&#20013;&#26410;&#32463;&#35777;&#26126;&#30340;&#26679;&#26412;&#37327;&#21644;&#25512;&#24191;&#65306;&#26356;&#20855;&#21253;&#23481;&#24615;&#29992;&#25143;&#30740;&#31350;&#30340;&#21407;&#21017;
&lt;/p&gt;
&lt;p&gt;
Unjustified Sample Sizes and Generalizations in Explainable AI Research: Principles for More Inclusive User Studies. (arXiv:2305.09477v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09477
&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#22411;AI&#30740;&#31350;&#20013;&#30340;&#21442;&#19982;&#32773;&#26679;&#26412;&#37327;&#21644;&#25512;&#24191;&#38382;&#39064;&#38480;&#21046;&#20102;&#35813;&#39046;&#22495;&#23545;XAI&#31995;&#32479;&#21487;&#35299;&#37322;&#24615;&#30340;&#35780;&#20272;&#65292;&#38656;&#35201;&#26356;&#20855;&#21253;&#23481;&#24615;&#30340;&#29992;&#25143;&#30740;&#31350;&#21407;&#21017;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20262;&#29702;&#26694;&#26550;&#35201;&#27714;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21487;&#35299;&#37322;&#12290;&#35299;&#37322;&#22411;AI&#65288;XAI&#65289;&#27169;&#22411;&#32463;&#24120;&#22312;&#29992;&#25143;&#30740;&#31350;&#20013;&#27979;&#35797;&#20854;&#20805;&#20998;&#24615;&#12290;&#30001;&#20110;&#19981;&#21516;&#30340;&#20154;&#21487;&#33021;&#26377;&#19981;&#21516;&#30340;&#35299;&#37322;&#38656;&#27714;&#65292;&#22240;&#27492;&#21442;&#19982;&#32773;&#26679;&#26412;&#22312;&#29992;&#25143;&#30740;&#31350;&#20013;&#24212;&#36275;&#22815;&#22823;&#65292;&#20197;&#20195;&#34920;&#30446;&#26631;&#20154;&#32676;&#65292;&#20197;&#23454;&#29616;&#25512;&#24191;&#12290;&#28982;&#32780;&#65292;&#23578;&#19981;&#28165;&#26970;XAI&#30740;&#31350;&#20154;&#21592;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#21453;&#24605;&#21644;&#35777;&#26126;&#20854;&#26679;&#26412;&#37327;&#25110;&#36991;&#20813;&#36328;&#20154;&#32676;&#24191;&#27867;&#25512;&#24191;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;2012&#24180;&#33267;2022&#24180;&#38388;&#21457;&#34920;&#30340;220&#31687;XAI&#29992;&#25143;&#30740;&#31350;&#12290;&#22823;&#22810;&#25968;&#30740;&#31350;&#27809;&#26377;&#25552;&#20379;&#26679;&#26412;&#37327;&#30340;&#29702;&#30001;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#35770;&#25991;&#23558;&#20854;&#32467;&#35770;&#25512;&#24191;&#21040;&#30446;&#26631;&#20154;&#32676;&#20043;&#22806;&#65292;&#24182;&#19988;&#27809;&#26377;&#35777;&#25454;&#34920;&#26126;&#23450;&#37327;&#30740;&#31350;&#20013;&#26356;&#24191;&#27867;&#30340;&#32467;&#35770;&#19982;&#26356;&#22823;&#30340;&#26679;&#26412;&#26377;&#20851;&#12290;&#36825;&#20123;&#26041;&#27861;&#35770;&#38382;&#39064;&#21487;&#33021;&#20250;&#22952;&#30861;&#35780;&#20272;XAI&#31995;&#32479;&#26159;&#21542;&#23454;&#29616;&#20102;&#20262;&#29702;&#26694;&#26550;&#20013;&#25552;&#20986;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#26356;&#20855;&#21253;&#23481;&#24615;&#30340;XAI&#30740;&#31350;&#29992;&#25143;&#30740;&#31350;&#21407;&#21017;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many ethical frameworks require artificial intelligence (AI) systems to be explainable. Explainable AI (XAI) models are frequently tested for their adequacy in user studies. Since different people may have different explanatory needs, it is important that participant samples in user studies are large enough to represent the target population to enable generalizations. However, it is unclear to what extent XAI researchers reflect on and justify their sample sizes or avoid broad generalizations across people. We analyzed XAI user studies (N = 220) published between 2012 and 2022. Most studies did not offer rationales for their sample sizes. Moreover, most papers generalized their conclusions beyond their target population, and there was no evidence that broader conclusions in quantitative studies were correlated with larger samples. These methodological problems can impede evaluations of whether XAI systems implement the explainability called for in ethical frameworks. We outline princip
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24179;&#21488;&#65292;&#23427;&#21487;&#20197;&#25193;&#23637;&#21644;&#26381;&#21153;&#20110;&#22823;&#35268;&#27169;&#24320;&#25918;&#39046;&#22495;&#30693;&#35782;&#22270;&#35889;&#65292;&#21516;&#26102;&#20171;&#32461;&#20102;&#35757;&#32451;&#30693;&#35782;&#22270;&#23884;&#20837;&#30340;&#31649;&#36947;&#20197;&#21450;&#22914;&#20309;&#21019;&#24314;&#35821;&#20041;&#27880;&#37322;&#26381;&#21153;&#21644;&#25512;&#21160;&#24320;&#25918;&#39046;&#22495;&#30693;&#35782;&#25277;&#21462;&#12290;</title><link>http://arxiv.org/abs/2305.09464</link><description>&lt;p&gt;
&#26500;&#24314;&#21644;&#26381;&#21153;&#20110;&#22823;&#35268;&#27169;&#24320;&#25918;&#39046;&#22495;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
Growing and Serving Large Open-domain Knowledge Graphs. (arXiv:2305.09464v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09464
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24179;&#21488;&#65292;&#23427;&#21487;&#20197;&#25193;&#23637;&#21644;&#26381;&#21153;&#20110;&#22823;&#35268;&#27169;&#24320;&#25918;&#39046;&#22495;&#30693;&#35782;&#22270;&#35889;&#65292;&#21516;&#26102;&#20171;&#32461;&#20102;&#35757;&#32451;&#30693;&#35782;&#22270;&#23884;&#20837;&#30340;&#31649;&#36947;&#20197;&#21450;&#22914;&#20309;&#21019;&#24314;&#35821;&#20041;&#27880;&#37322;&#26381;&#21153;&#21644;&#25512;&#21160;&#24320;&#25918;&#39046;&#22495;&#30693;&#35782;&#25277;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#24320;&#25918;&#39046;&#22495;&#30693;&#35782;&#22270;&#35889;(KG)&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#26102;&#38754;&#20020;&#35768;&#22810;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#24179;&#21488;Saga&#30340;&#25193;&#23637;&#65292;&#35813;&#24179;&#21488;&#21487;&#25345;&#32493;&#22320;&#26500;&#24314;&#21644;&#26381;&#21153;&#20110;&#30693;&#35782;&#22270;&#35889;&#35268;&#27169;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#30693;&#35782;&#22270;&#23884;&#20837;&#30340;&#31649;&#36947;&#65292;&#21487;&#29992;&#20110;&#25903;&#25345;&#20027;&#35201;&#21151;&#33021;&#65292;&#22914;&#20107;&#23454;&#25490;&#21517;&#12289;&#20107;&#23454;&#39564;&#35777;&#12289;&#30456;&#20851;&#23454;&#20307;&#26381;&#21153;&#20197;&#21450;&#25903;&#25345;&#23454;&#20307;&#38142;&#25509;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#22914;&#20309;&#21033;&#29992;&#25105;&#20204;&#30340;&#24179;&#21488;&#65288;&#21253;&#25324;&#22270;&#23884;&#20837;&#65289;&#26469;&#21019;&#24314;&#35821;&#20041;&#27880;&#37322;&#26381;&#21153;&#65292;&#23558;&#38750;&#32467;&#26500;&#21270;Web&#25991;&#26723;&#38142;&#25509;&#21040;&#25105;&#20204;KG&#20013;&#30340;&#23454;&#20307;&#12290;Web&#30340;&#35821;&#20041;&#27880;&#37322;&#26377;&#25928;&#22320;&#25193;&#23637;&#20102;&#25105;&#20204;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#25552;&#20379;&#20102;&#38142;&#25509;&#21040;&#24320;&#25918;&#39046;&#22495;Web&#20869;&#23481;&#30340;&#36793;&#32536;&#65292;&#21487;&#29992;&#20110;&#21508;&#31181;&#25628;&#32034;&#21644;&#25490;&#21517;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#27880;&#37322;&#30340;Web&#25991;&#26723;&#26469;&#25512;&#21160;&#24320;&#25918;&#39046;&#22495;&#30693;&#35782;&#25277;&#21462;&#12290;&#36825;&#20010;&#26377;&#38024;&#23545;&#24615;&#30340;&#25552;&#21462;&#26694;&#26550;&#35782;&#21035;&#20102;KG&#20013;&#30340;&#37325;&#35201;&#35206;&#30422;&#38382;&#39064;&#65292;&#28982;&#21518;&#23547;&#25214;&#20102;&#30446;&#26631;&#30340;&#30456;&#20851;&#25968;&#25454;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Applications of large open-domain knowledge graphs (KGs) to real-world problems pose many unique challenges. In this paper, we present extensions to Saga our platform for continuous construction and serving of knowledge at scale. In particular, we describe a pipeline for training knowledge graph embeddings that powers key capabilities such as fact ranking, fact verification, a related entities service, and support for entity linking. We then describe how our platform, including graph embeddings, can be leveraged to create a Semantic Annotation service that links unstructured Web documents to entities in our KG. Semantic annotation of the Web effectively expands our knowledge graph with edges to open-domain Web content which can be used in various search and ranking problems. Finally, we leverage annotated Web documents to drive Open-domain Knowledge Extraction. This targeted extraction framework identifies important coverage issues in the KG, then finds relevant data sources for target
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#25945;&#24072;-&#23398;&#29983;&#26041;&#26696;&#21644;&#22810;&#31181;&#35889;&#22270;&#30340;&#20302;&#22797;&#26434;&#24230;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#22768;&#26223;&#20998;&#31867;&#65292;&#33719;&#24471;&#20102;57.4%&#30340;&#26368;&#20339;&#20998;&#31867;&#20934;&#30830;&#29575;&#65292;&#30456;&#27604;DCASE&#22522;&#32447;&#25552;&#39640;14.5%&#12290;</title><link>http://arxiv.org/abs/2305.09463</link><description>&lt;p&gt;
&#37319;&#29992;&#25945;&#24072;-&#23398;&#29983;&#26041;&#26696;&#21644;&#22810;&#31181;&#35889;&#22270;&#30340;&#20302;&#22797;&#26434;&#24230;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#22768;&#26223;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Low-complexity deep learning frameworks for acoustic scene classification using teacher-student scheme and multiple spectrograms. (arXiv:2305.09463v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#25945;&#24072;-&#23398;&#29983;&#26041;&#26696;&#21644;&#22810;&#31181;&#35889;&#22270;&#30340;&#20302;&#22797;&#26434;&#24230;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#22768;&#26223;&#20998;&#31867;&#65292;&#33719;&#24471;&#20102;57.4%&#30340;&#26368;&#20339;&#20998;&#31867;&#20934;&#30830;&#29575;&#65292;&#30456;&#27604;DCASE&#22522;&#32447;&#25552;&#39640;14.5%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22768;&#26223;&#20998;&#31867;&#30340;&#20302;&#22797;&#26434;&#24230;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#30001;&#20004;&#20010;&#20027;&#35201;&#38454;&#27573;&#32452;&#25104;&#65306;&#65288;&#38454;&#27573;I&#65289;&#35757;&#32451;&#25945;&#24072;&#32593;&#32476;; &#21644;&#65288;&#38454;&#27573;II&#65289;&#21033;&#29992;&#25945;&#24072;&#32593;&#32476;&#30340;&#21387;&#32553;&#30693;&#35782;&#35757;&#32451;&#23398;&#29983;&#32593;&#32476;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#20013;&#65292;&#35757;&#32451;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25945;&#24072;&#27169;&#22411;&#12290;&#22312;&#35757;&#32451;&#25945;&#24072;&#20043;&#21518;&#65292;&#25552;&#21462;&#29305;&#24449;&#22270;&#65292;&#21363;&#25945;&#24072;&#32593;&#32476;&#30340;&#20498;&#25968;&#31532;&#20108;&#23618;&#30340;&#29305;&#24449;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#20013;&#65292;&#21033;&#29992;&#20174;&#25945;&#24072;&#25552;&#21462;&#30340;&#29305;&#24449;&#22270;&#26469;&#35757;&#32451;&#20302;&#22797;&#26434;&#24230;&#30340;&#23398;&#29983;&#32593;&#32476;&#12290;&#25105;&#20204;&#22312;DCASE 2023&#20219;&#21153;1&#24320;&#21457;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#65292;&#28385;&#36275;&#20302;&#22797;&#26434;&#24230;&#35201;&#27714;&#65292;&#24182;&#21462;&#24471;&#20102;57.4&#65285;&#30340;&#26368;&#20339;&#20998;&#31867;&#20934;&#30830;&#29575;&#65292;&#30456;&#27604;DCASE&#22522;&#32447;&#25552;&#39640;&#20102;14.5&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this technical report, a low-complexity deep learning system for acoustic scene classification (ASC) is presented. The proposed system comprises two main phases: (Phase I) Training a teacher network; and (Phase II) training a student network using distilled knowledge from the teacher. In the first phase, the teacher, which presents a large footprint model, is trained. After training the teacher, the embeddings, which are the feature map of the second last layer of the teacher, are extracted. In the second phase, the student network, which presents a low complexity model, is trained with the embeddings extracted from the teacher. Our experiments conducted on DCASE 2023 Task 1 Development dataset have fulfilled the requirement of low-complexity and achieved the best classification accuracy of 57.4%, improving DCASE baseline by 14.5%.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#32467;&#21512;&#24207;&#21015;&#20844;&#20132;&#32593;&#32476;&#35774;&#35745;&#21644;&#26368;&#20248;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20934;&#30830;&#20272;&#35745;&#28508;&#22312;&#20986;&#34892;&#38656;&#27714;&#65292;&#24182;&#35268;&#36991;&#35774;&#35745;&#19982;&#23454;&#38469;&#38656;&#27714;&#19981;&#19968;&#33268;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2305.09452</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#30456;&#20851;&#20449;&#24565;&#19979;&#26368;&#20248;&#23398;&#20064;&#30340;&#24207;&#21015;&#20844;&#20132;&#32593;&#32476;&#35774;&#35745;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A sequential transit network design algorithm with optimal learning under correlated beliefs. (arXiv:2305.09452v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09452
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#32467;&#21512;&#24207;&#21015;&#20844;&#20132;&#32593;&#32476;&#35774;&#35745;&#21644;&#26368;&#20248;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20934;&#30830;&#20272;&#35745;&#28508;&#22312;&#20986;&#34892;&#38656;&#27714;&#65292;&#24182;&#35268;&#36991;&#35774;&#35745;&#19982;&#23454;&#38469;&#38656;&#27714;&#19981;&#19968;&#33268;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#20272;&#35745;&#28508;&#22312;&#38656;&#27714;&#23545;&#20110;&#20844;&#20132;&#26381;&#21153;&#36335;&#32447;&#35774;&#35745;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#24207;&#21015;&#20844;&#20132;&#32593;&#32476;&#35774;&#35745;&#21644;&#26368;&#20248;&#23398;&#20064;&#30340;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#31639;&#27861;&#12290;&#36816;&#33829;&#21830;&#36880;&#27493;&#25193;&#23637;&#20854;&#36335;&#32447;&#31995;&#32479;&#65292;&#20197;&#36991;&#20813;&#35774;&#35745;&#36335;&#32447;&#19982;&#23454;&#38469;&#20986;&#34892;&#38656;&#27714;&#19981;&#19968;&#33268;&#30340;&#39118;&#38505;&#12290;&#21516;&#26102;&#65292;&#35266;&#27979;&#21040;&#30340;&#20449;&#24687;&#23558;&#34987;&#23384;&#26723;&#20197;&#26356;&#26032;&#36816;&#33829;&#21830;&#24403;&#21069;&#20351;&#29992;&#30340;&#30693;&#35782;&#12290;&#31639;&#27861;&#20013;&#27604;&#36739;&#20102;&#19977;&#31181;&#23398;&#20064;&#31574;&#30053;&#65306;&#22810;&#33218;&#32769;&#34382;&#26426;&#12289;&#30693;&#35782;&#26799;&#24230;&#21644;&#20855;&#26377;&#30456;&#20851;&#20449;&#24565;&#30340;&#30693;&#35782;&#26799;&#24230;&#12290;&#35813;&#31639;&#27861;&#30340;&#27169;&#25311;&#32467;&#26524;&#26174;&#31034;&#20854;&#22312;&#20844;&#20132;&#32593;&#32476;&#35774;&#35745;&#20013;&#30340;&#33391;&#22909;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mobility service route design requires potential demand information to well accommodate travel demand within the service region. Transit planners and operators can access various data sources including household travel survey data and mobile device location logs. However, when implementing a mobility system with emerging technologies, estimating demand level becomes harder because of more uncertainties with user behaviors. Therefore, this study proposes an artificial intelligence-driven algorithm that combines sequential transit network design with optimal learning. An operator gradually expands its route system to avoid risks from inconsistency between designed routes and actual travel demand. At the same time, observed information is archived to update the knowledge that the operator currently uses. Three learning policies are compared within the algorithm: multi-armed bandit, knowledge gradient, and knowledge gradient with correlated beliefs. For validation, a new route system is de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#19968;&#20010;&#21517;&#20026;RECENT&#30340;&#20851;&#31995;&#25277;&#21462;&#31995;&#32479;&#65292;&#20316;&#32773;&#22312;TACRED&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20043;&#21069;&#30340;&#26368;&#26032;&#25104;&#26524;74.8&#30340;F1&#20998;&#25968;&#65292;&#20294;&#22312;&#26356;&#27491;&#38169;&#35823;&#21644;&#37325;&#26032;&#35780;&#20272;&#21518;&#20854;F1&#20998;&#25968;&#20026;65.16&#12290;</title><link>http://arxiv.org/abs/2305.09410</link><description>&lt;p&gt;
&#20851;&#20110;F1&#20998;&#25968;&#30340;&#35752;&#35770;&#8212;&#8212;&#20197;&#26368;&#36817;&#30340;&#20851;&#31995;&#25277;&#21462;&#31995;&#32479;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
About Evaluation of F1 Score for RECENT Relation Extraction System. (arXiv:2305.09410v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#19968;&#20010;&#21517;&#20026;RECENT&#30340;&#20851;&#31995;&#25277;&#21462;&#31995;&#32479;&#65292;&#20316;&#32773;&#22312;TACRED&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20043;&#21069;&#30340;&#26368;&#26032;&#25104;&#26524;74.8&#30340;F1&#20998;&#25968;&#65292;&#20294;&#22312;&#26356;&#27491;&#38169;&#35823;&#21644;&#37325;&#26032;&#35780;&#20272;&#21518;&#20854;F1&#20998;&#25968;&#20026;65.16&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;Shengfei Lyu&#21644;Huanhuan Chen&#22312;Findings of the Association for Computational Linguistics&#65306;ACL-IJCNLP 2021&#19978;&#21457;&#34920;&#30340;&#35770;&#25991;&#8220;Relation Classification with Entity Type Restriction&#8221;&#20013;&#20351;&#29992;&#30340;F1&#20998;&#25968;&#35780;&#20272;&#26041;&#27861;&#12290;&#20316;&#32773;&#21019;&#24314;&#30340;&#31995;&#32479;&#21517;&#20026;RECENT&#65292;&#22768;&#31216;&#20854;&#22312;TACRED&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#65288;&#24403;&#26102;&#30340;&#65289;&#26368;&#26032;&#30340;75.2&#65288;&#20043;&#21069;&#20026;74.8&#65289;&#30340;&#26368;&#26032;&#25104;&#26524;&#65292;&#20294;&#22312;&#26356;&#27491;&#38169;&#35823;&#21644;&#37325;&#26032;&#35780;&#20272;&#21518;&#26368;&#32456;&#32467;&#26524;&#20026;65.16&#12290;
&lt;/p&gt;
&lt;p&gt;
This document contains a discussion of the F1 score evaluation used in the article 'Relation Classification with Entity Type Restriction' by Shengfei Lyu, Huanhuan Chen published on Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. The authors created a system named RECENT and claim it achieves (then) a new state-of-the-art result 75.2 (previous 74.8) on the TACRED dataset, while after correcting errors and reevaluation the final result is 65.16
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20154;&#24037;&#29983;&#21629;&#24179;&#21488;Lenia&#65292;&#36890;&#36807;&#35782;&#21035;&#22797;&#26434;&#26032;&#20852;&#34892;&#20026;&#30340;&#24230;&#37327;&#26631;&#20934;&#21644;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#20135;&#29983;&#19981;&#21516;&#34892;&#20026;&#30340;&#32467;&#26524;&#65292;&#20197;&#36827;&#21270;&#20986;&#26356;&#22909;&#30340;Lenia&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2305.09378</link><description>&lt;p&gt;
&#22312;Lenia&#20013;&#25429;&#33719;&#26032;&#20852;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
Capturing Emerging Complexity in Lenia. (arXiv:2305.09378v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09378
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#24037;&#29983;&#21629;&#24179;&#21488;Lenia&#65292;&#36890;&#36807;&#35782;&#21035;&#22797;&#26434;&#26032;&#20852;&#34892;&#20026;&#30340;&#24230;&#37327;&#26631;&#20934;&#21644;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#20135;&#29983;&#19981;&#21516;&#34892;&#20026;&#30340;&#32467;&#26524;&#65292;&#20197;&#36827;&#21270;&#20986;&#26356;&#22909;&#30340;Lenia&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39033;&#30446;&#25506;&#35752;&#20102;Lenia&#65292;&#36825;&#26159;&#19968;&#20010;&#27169;&#25311;&#25968;&#23383;&#29983;&#29289;&#31995;&#32479;&#30340;&#20154;&#24037;&#29983;&#21629;&#24179;&#21488;&#12290;Lenia&#30340;&#29983;&#24577;&#31995;&#32479;&#30001;&#31616;&#21333;&#30340;&#20154;&#24037;&#29983;&#29289;&#32452;&#25104;&#65292;&#23427;&#20204;&#21487;&#20197;&#31227;&#21160;&#12289;&#28040;&#32791;&#12289;&#29983;&#38271;&#21644;&#32321;&#27542;&#12290;&#35813;&#24179;&#21488;&#26159;&#19968;&#20010;&#30740;&#31350;&#20154;&#24037;&#29983;&#21629;&#21644;&#36827;&#21270;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#22240;&#20026;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#21644;&#28789;&#27963;&#30340;&#29615;&#22659;&#65292;&#29992;&#20110;&#21019;&#24314;&#20855;&#26377;&#19981;&#21516;&#33021;&#21147;&#21644;&#34892;&#20026;&#30340;&#22810;&#26679;&#21270;&#29983;&#29289;&#12290;&#35813;&#30740;&#31350;&#30340;&#20851;&#38190;&#26159;&#22312;Lenia&#20013;&#27979;&#37327;&#22797;&#26434;&#24615;&#65292;&#35782;&#21035;&#27979;&#37327;&#35268;&#21017;&#30340;&#38271;&#26399;&#22797;&#26434;&#24615;&#26032;&#20852;&#34892;&#20026;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#26088;&#22312;&#36827;&#21270;&#20986;&#23578;&#26410;&#21457;&#29616;&#30340;&#26356;&#22909;&#30340;Lenia&#34892;&#20026;&#12290;&#36951;&#20256;&#31639;&#27861;&#20351;&#29992;&#30456;&#37051;&#21306;&#22495;&#25110;&#26680;&#20316;&#20026;&#22522;&#22240;&#22411;&#65292;&#21516;&#26102;&#20445;&#25345;Lenia&#30340;&#20854;&#20182;&#21442;&#25968;&#65288;&#20363;&#22914;&#29983;&#38271;&#20989;&#25968;&#65289;&#19981;&#21464;&#65292;&#20197;&#20135;&#29983;&#19981;&#21516;&#20154;&#21475;&#34892;&#20026;&#30340;&#32467;&#26524;&#65292;&#28982;&#21518;&#27979;&#37327;&#36866;&#24212;&#24230;&#20540;&#20197;&#20915;&#23450;&#25152;&#24471;&#34892;&#20026;&#30340;&#22797;&#26434;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#26102;&#38388;&#21464;&#21270;&#20316;&#20026;&#36866;&#24212;&#24230;&#20989;&#25968;&#65292;
&lt;/p&gt;
&lt;p&gt;
This research project investigates Lenia, an artificial life platform that simulates ecosystems of digital creatures. Lenia's ecosystem consists of simple, artificial organisms that can move, consume, grow, and reproduce. The platform is important as a tool for studying artificial life and evolution, as it provides a scalable and flexible environment for creating a diverse range of organisms with varying abilities and behaviors. Measuring complexity in Lenia is a key aspect of the study, which identifies the metrics for measuring long-term complex emerging behavior of rules, with the aim of evolving better Lenia behaviors which are yet not discovered. The Genetic Algorithm uses neighborhoods or kernels as genotype while keeping the rest of the parameters of Lenia as fixed, for example growth function, to produce different behaviors respective to the population and then measures fitness value to decide the complexity of the resulting behavior. First, we use Variation over Time as a fitn
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#24207;&#21015;&#21040;&#24207;&#21015;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#35780;&#20272;&#22810;&#36890;&#36947;&#30005;&#38459;&#25239;&#34880;&#27969;&#21160;&#21147;&#23398;&#30417;&#27979;&#20013;&#24515;&#32954;&#23481;&#31215;&#20449;&#21495;&#65288;CVS&#65289;&#30340;&#36816;&#21160;&#35825;&#23548;&#21487;&#38752;&#24615;&#38477;&#20302;&#12290;&#36890;&#36807;&#21033;&#29992;&#38271;&#30701;&#26102;&#35760;&#24518;&#21644;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#32467;&#26500;&#65292;&#32534;&#30721;&#22120; - &#35299;&#30721;&#22120;&#27169;&#22411;&#21487;&#20197;&#25429;&#25417;&#21040;&#26242;&#24577; CVS &#24207;&#21015;&#20013;&#23384;&#22312;&#30340;&#19978;&#19979;&#25991;&#30693;&#35782;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#26412;&#26041;&#27861;&#19981;&#38656;&#35201;&#25163;&#21160;&#27880;&#37322;&#36816;&#21160;&#24433;&#21709;&#20197;&#21450;&#32570;&#20047;&#22312; CVS &#38543;&#26102;&#38388;&#20986;&#29616;&#19978;&#19979;&#25991;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#36816;&#21160;&#24341;&#36215;&#24322;&#24120;&#30340;&#26174;&#24335;&#26426;&#21046;&#65292;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.09368</link><description>&lt;p&gt;
&#22810;&#36890;&#36947;&#30005;&#38459;&#25239;&#34880;&#27969;&#21160;&#21147;&#23398;&#30417;&#27979;&#20013;&#30340;&#33258;&#21160;&#20449;&#21495;&#36136;&#37327;&#35780;&#20272;&#30340;&#26080;&#30417;&#30563;&#24207;&#21015;&#21040;&#24207;&#21015;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unsupervised sequence-to-sequence learning for automatic signal quality assessment in multi-channel electrical impedance-based hemodynamic monitoring. (arXiv:2305.09368v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#24207;&#21015;&#21040;&#24207;&#21015;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#35780;&#20272;&#22810;&#36890;&#36947;&#30005;&#38459;&#25239;&#34880;&#27969;&#21160;&#21147;&#23398;&#30417;&#27979;&#20013;&#24515;&#32954;&#23481;&#31215;&#20449;&#21495;&#65288;CVS&#65289;&#30340;&#36816;&#21160;&#35825;&#23548;&#21487;&#38752;&#24615;&#38477;&#20302;&#12290;&#36890;&#36807;&#21033;&#29992;&#38271;&#30701;&#26102;&#35760;&#24518;&#21644;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#32467;&#26500;&#65292;&#32534;&#30721;&#22120; - &#35299;&#30721;&#22120;&#27169;&#22411;&#21487;&#20197;&#25429;&#25417;&#21040;&#26242;&#24577; CVS &#24207;&#21015;&#20013;&#23384;&#22312;&#30340;&#19978;&#19979;&#25991;&#30693;&#35782;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#26412;&#26041;&#27861;&#19981;&#38656;&#35201;&#25163;&#21160;&#27880;&#37322;&#36816;&#21160;&#24433;&#21709;&#20197;&#21450;&#32570;&#20047;&#22312; CVS &#38543;&#26102;&#38388;&#20986;&#29616;&#19978;&#19979;&#25991;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#36816;&#21160;&#24341;&#36215;&#24322;&#24120;&#30340;&#26174;&#24335;&#26426;&#21046;&#65292;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#35780;&#20272;&#22810;&#36890;&#36947;&#30005;&#38459;&#25239;&#34880;&#27969;&#21160;&#21147;&#23398;&#30417;&#27979;&#20013;&#24515;&#32954;&#23481;&#31215;&#20449;&#21495;&#65288;CVS&#65289;&#30340;&#36816;&#21160;&#35825;&#23548;&#21487;&#38752;&#24615;&#38477;&#20302;&#12290;&#35813;&#26041;&#27861;&#35797;&#22270;&#35299;&#20915;&#29616;&#26377;&#23398;&#20064;&#22411;&#35780;&#20272;&#26041;&#27861;&#30340;&#32570;&#28857;&#65292;&#20363;&#22914;&#38656;&#35201;&#25163;&#21160;&#27880;&#37322;&#36816;&#21160;&#24433;&#21709;&#20197;&#21450;&#32570;&#20047;&#22312; CVS &#38543;&#26102;&#38388;&#20986;&#29616;&#19978;&#19979;&#25991;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#36816;&#21160;&#24341;&#36215;&#24322;&#24120;&#30340;&#26174;&#24335;&#26426;&#21046;&#12290;&#36890;&#36807;&#21033;&#29992;&#38271;&#30701;&#26102;&#35760;&#24518;&#21644;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#32467;&#26500;&#65292;&#32534;&#30721;&#22120; - &#35299;&#30721;&#22120;&#27169;&#22411;&#19981;&#20165;&#34987;&#35757;&#32451;&#25104;&#33258;&#25105;&#37325;&#29616; CVS &#30340;&#36755;&#20837;&#24207;&#21015;&#65292;&#32780;&#19988;&#36824;&#34987;&#35757;&#32451;&#25104;&#20197;&#24179;&#34892;&#26041;&#24335;&#22806;&#25512;&#26410;&#26469;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#27169;&#22411;&#21487;&#20197;&#25429;&#25417;&#21040;&#26242;&#24577; CVS &#24207;&#21015;&#20013;&#23384;&#22312;&#30340;&#19978;&#19979;&#25991;&#30693;&#35782;&#65292;&#21516;&#26102;&#34987;&#35268;&#33539;&#21270;&#20197;&#25506;&#32034;&#25972;&#20010;&#26102;&#38388;&#24207;&#21015;&#30340;&#19968;&#33324;&#20851;&#31995;&#12290;&#26681;&#25454;&#23454;&#38469;&#21644;&#39044;&#27979; CVS &#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#26816;&#27979;&#20986;&#20302;&#36136;&#37327;&#30340;&#21463;&#36816;&#21160;&#24433;&#21709;&#30340; CVS&#12290;&#22810;&#36890;&#36947; EIT &#22522;&#24515;&#34880;&#31649;&#30417;&#27979;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#36816;&#21160;&#24341;&#36215;&#30340;&#27979;&#37327;&#36136;&#37327;&#35780;&#20272;&#26041;&#27861;&#30340;&#31454;&#20105;&#24615;&#33021;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study proposes an unsupervised sequence-to-sequence learning approach that automatically assesses the motion-induced reliability degradation of the cardiac volume signal (CVS) in multi-channel electrical impedance-based hemodynamic monitoring. The proposed method attempts to tackle shortcomings in existing learning-based assessment approaches, such as the requirement of manual annotation for motion influence and the lack of explicit mechanisms for realizing motion-induced abnormalities under contextual variations in CVS over time. By utilizing long-short term memory and variational auto-encoder structures, an encoder--decoder model is trained not only to self-reproduce an input sequence of the CVS but also to extrapolate the future in a parallel fashion. By doing so, the model can capture contextual knowledge lying in a temporal CVS sequence while being regularized to explore a general relationship over the entire time-series. A motion-influenced CVS of low-quality is detected, ba
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#25552;&#31034;&#30340;&#22810;&#27169;&#24577;&#35270;&#35273;&#29702;&#35299;&#25216;&#26415;&#65292;&#36890;&#36807;&#20998;&#31163;&#35821;&#20041;&#20449;&#24687;&#25552;&#39640;&#23545;&#22270;&#20687;&#30340;&#29702;&#35299;&#65292;&#26088;&#22312;&#25512;&#36827;&#22270;&#20687;&#35782;&#21035;&#21644;&#29702;&#35299;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2305.09333</link><description>&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#22810;&#27169;&#24577;&#35270;&#35273;&#29702;&#35299;&#25216;&#26415;&#23545;&#22270;&#20687;&#35821;&#20041;&#20449;&#24687;&#36827;&#34892;&#20998;&#31163;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Multi-modal Visual Understanding with Prompts for Semantic Information Disentanglement of Image. (arXiv:2305.09333v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#25552;&#31034;&#30340;&#22810;&#27169;&#24577;&#35270;&#35273;&#29702;&#35299;&#25216;&#26415;&#65292;&#36890;&#36807;&#20998;&#31163;&#35821;&#20041;&#20449;&#24687;&#25552;&#39640;&#23545;&#22270;&#20687;&#30340;&#29702;&#35299;&#65292;&#26088;&#22312;&#25512;&#36827;&#22270;&#20687;&#35782;&#21035;&#21644;&#29702;&#35299;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#31034;&#26469;&#36827;&#34892;&#22810;&#27169;&#24577;&#35270;&#35273;&#29702;&#35299;&#25216;&#26415;&#21487;&#21033;&#29992;&#21508;&#31181;&#35270;&#35273;&#21450;&#25991;&#26412;&#32447;&#32034;&#65292;&#25552;&#39640;&#23545;&#22270;&#20687;&#30340;&#35821;&#20041;&#29702;&#35299;&#12290;&#36825;&#31181;&#26041;&#27861;&#32467;&#21512;&#20102;&#35270;&#35273;&#21644;&#35821;&#35328;&#22788;&#29702;&#65292;&#33021;&#22815;&#20135;&#29983;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#21644;&#35782;&#21035;&#22270;&#20687;&#12290;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#25552;&#31034;&#30340;&#25216;&#26415;&#65292;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#20851;&#27880;&#22270;&#20687;&#30340;&#26576;&#20123;&#29305;&#24449;&#65292;&#20197;&#25552;&#21462;&#19979;&#28216;&#20219;&#21153;&#25152;&#38656;&#30340;&#26377;&#29992;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#22810;&#27169;&#24577;&#29702;&#35299;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#26356;&#24378;&#22823;&#30340;&#22270;&#20687;&#34920;&#31034;&#26469;&#25913;&#21892;&#21333;&#27169;&#24577;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#35270;&#35273;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#32467;&#21512;&#26159;&#25512;&#36827;&#22270;&#20687;&#35782;&#21035;&#21644;&#29702;&#35299;&#39046;&#22495;&#30340;&#26377;&#26395;&#30740;&#31350;&#26041;&#21521;&#12290;&#26412;&#25991;&#23558;&#23581;&#35797;&#19968;&#20123;&#25552;&#31034;&#35774;&#35745;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#26356;&#22909;&#22320;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal visual understanding of images with prompts involves using various visual and textual cues to enhance the semantic understanding of images. This approach combines both vision and language processing to generate more accurate predictions and recognition of images. By utilizing prompt-based techniques, models can learn to focus on certain features of an image to extract useful information for downstream tasks. Additionally, multi-modal understanding can improve upon single modality models by providing more robust representations of images. Overall, the combination of visual and textual information is a promising area of research for advancing image recognition and understanding. In this paper we will try an amount of prompt design methods and propose a new method for better extraction of semantic information
&lt;/p&gt;</description></item><item><title>&#25512;&#33616;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#36234;&#26469;&#36234;&#24341;&#36215;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#23588;&#20854;&#26159;&#29992;&#25143;&#20844;&#24179;&#24615;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#24456;&#22810;&#35299;&#20915;&#26041;&#26696;&#26469;&#20943;&#36731;&#29992;&#25143;&#22312;&#20351;&#29992;&#25512;&#33616;&#31995;&#32479;&#36807;&#31243;&#20013;&#20307;&#39564;&#21040;&#30340;&#27495;&#35270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.09330</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#29992;&#25143;&#20844;&#24179;&#24615;: &#26041;&#27861;&#21644;&#35780;&#20272;&#30340;&#31995;&#32479;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Consumer-side Fairness in Recommender Systems: A Systematic Survey of Methods and Evaluation. (arXiv:2305.09330v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09330
&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#36234;&#26469;&#36234;&#24341;&#36215;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#23588;&#20854;&#26159;&#29992;&#25143;&#20844;&#24179;&#24615;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#24456;&#22810;&#35299;&#20915;&#26041;&#26696;&#26469;&#20943;&#36731;&#29992;&#25143;&#22312;&#20351;&#29992;&#25512;&#33616;&#31995;&#32479;&#36807;&#31243;&#20013;&#20307;&#39564;&#21040;&#30340;&#27495;&#35270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#21270;&#27700;&#24179;&#19981;&#26029;&#25552;&#39640;&#30340;&#24403;&#21069;&#31038;&#20250;&#20013;&#65292;&#38754;&#20020;&#30528;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#30340;&#24040;&#22823;&#25361;&#25112;&#12290;&#25512;&#33616;&#31995;&#32479;&#24050;&#32463;&#25104;&#20026;&#24110;&#21161;&#29992;&#25143;&#23548;&#33322;&#26085;&#30410;&#22686;&#38271;&#30340;&#25968;&#25454;&#37327;&#65292;&#20197;&#21450;&#24110;&#21161;&#20379;&#24212;&#21830;&#21521;&#24863;&#20852;&#36259;&#30340;&#29992;&#25143;&#33829;&#38144;&#20135;&#21697;&#30340;&#24517;&#19981;&#21487;&#23569;&#30340;&#24037;&#20855;&#12290;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20013;&#30340;&#27495;&#35270;&#38382;&#39064;&#26085;&#30410;&#31361;&#20986;&#65292;&#36825;&#20419;&#20351;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30740;&#31350;&#22914;&#20309;&#30830;&#20445;&#25512;&#33616;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#12290;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#36825;&#20123;&#38382;&#39064;&#22312;&#32844;&#19994;&#25512;&#33616;&#20013;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#20307;&#29616;&#65292;&#21382;&#21490;&#25968;&#25454;&#20013;&#30340;&#20559;&#35265;&#21487;&#33021;&#23548;&#33268;&#25512;&#33616;&#31995;&#32479;&#23558;&#19968;&#20010;&#24615;&#21035;&#19982;&#36739;&#20302;&#30340;&#24037;&#36164;&#25110;&#21051;&#26495;&#21360;&#35937;&#32852;&#31995;&#36215;&#26469;&#12290;&#29305;&#21035;&#22320;&#65292;&#29992;&#25143;&#20844;&#24179;&#24615;&#20851;&#27880;&#22914;&#20309;&#20943;&#36731;&#29992;&#25143;&#22312;&#20351;&#29992;&#25512;&#33616;&#31995;&#32479;&#36807;&#31243;&#20013;&#20307;&#39564;&#21040;&#30340;&#27495;&#35270;&#38382;&#39064;&#65292;&#35813;&#39046;&#22495;&#24050;&#32463;&#20986;&#29616;&#20102;&#24456;&#22810;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#19981;&#21516;&#31867;&#22411;&#30340;&#27495;&#35270;&#12290;&#25152;&#36848;&#27495;&#35270;&#30340;&#24615;&#36136;&#21462;&#20915;&#20110;&#25152;&#22788;&#30340;&#24773;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the current landscape of ever-increasing levels of digitalization, we are facing major challenges pertaining to scalability. Recommender systems have become irreplaceable both for helping users navigate the increasing amounts of data and, conversely, aiding providers in marketing products to interested users. The growing awareness of discrimination in machine learning methods has recently motivated both academia and industry to research how fairness can be ensured in recommender systems. For recommender systems, such issues are well exemplified by occupation recommendation, where biases in historical data may lead to recommender systems relating one gender to lower wages or to the propagation of stereotypes. In particular, consumer-side fairness, which focuses on mitigating discrimination experienced by users of recommender systems, has seen a vast number of diverse approaches for addressing different types of discrimination. The nature of said discrimination depends on the setting 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65292;&#21033;&#29992;&#26469;&#33258;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;BERT&#30340;&#19978;&#19979;&#25991;&#21270;&#35789;&#23884;&#20837;&#65292;&#21487;&#20197;&#22312;&#19981;&#20351;&#29992;&#20219;&#20309;BoW&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#25512;&#26029;&#20986;&#25991;&#26723;&#30340;&#20027;&#39064;&#20998;&#24067;&#65292;&#24182;&#30452;&#25509;&#20174;&#19978;&#19979;&#25991;&#21270;&#35789;&#23884;&#20837;&#20013;&#25512;&#26029;&#20986;&#25991;&#26723;&#20013;&#27599;&#20010;&#21333;&#35789;&#30340;&#20027;&#39064;&#20998;&#24067;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#20248;&#20110;&#20165;&#20381;&#36182;BoW&#34920;&#31034;&#21644;&#20854;&#20182;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#30340;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.09329</link><description>&lt;p&gt;
BERTTM: &#21033;&#29992;&#26469;&#33258;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#21270;&#35789;&#21521;&#37327;&#36827;&#34892;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
BERTTM: Leveraging Contextualized Word Embeddings from Pre-trained Language Models for Neural Topic Modeling. (arXiv:2305.09329v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09329
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65292;&#21033;&#29992;&#26469;&#33258;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;BERT&#30340;&#19978;&#19979;&#25991;&#21270;&#35789;&#23884;&#20837;&#65292;&#21487;&#20197;&#22312;&#19981;&#20351;&#29992;&#20219;&#20309;BoW&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#25512;&#26029;&#20986;&#25991;&#26723;&#30340;&#20027;&#39064;&#20998;&#24067;&#65292;&#24182;&#30452;&#25509;&#20174;&#19978;&#19979;&#25991;&#21270;&#35789;&#23884;&#20837;&#20013;&#25512;&#26029;&#20986;&#25991;&#26723;&#20013;&#27599;&#20010;&#21333;&#35789;&#30340;&#20027;&#39064;&#20998;&#24067;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#20248;&#20110;&#20165;&#20381;&#36182;BoW&#34920;&#31034;&#21644;&#20854;&#20182;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#30340;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36817;&#24180;&#26469;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#20027;&#39064;&#24314;&#27169;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20013;&#25198;&#28436;&#30528;&#26085;&#30410;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#20027;&#39064;&#27169;&#22411;&#20173;&#28982;&#20381;&#36182;&#20110;&#35789;&#34955;&#65288;BoW&#65289;&#20449;&#24687;&#65292;&#26080;&#35770;&#26159;&#20316;&#20026;&#35757;&#32451;&#36755;&#20837;&#36824;&#26159;&#35757;&#32451;&#30446;&#26631;&#12290;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#25429;&#25417;&#25991;&#26723;&#20013;&#30340;&#21333;&#35789;&#39034;&#24207;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#24182;&#23548;&#33268;&#23427;&#20204;&#22312;&#22788;&#29702;&#26032;&#25991;&#26723;&#20013;&#30340;&#26410;&#35266;&#23519;&#21040;&#30340;&#21333;&#35789;&#26102;&#36935;&#21040;&#22256;&#38590;&#12290;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19978;&#19979;&#25991;&#21270;&#35789;&#21521;&#37327;&#22312;&#35789;&#20041;&#28040;&#27495;&#30340;&#33021;&#21147;&#19978;&#34920;&#29616;&#20248;&#36234;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#22788;&#29702;OOV&#21333;&#35789;&#26102;&#26159;&#26377;&#25928;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;BERT&#30340;&#19978;&#19979;&#25991;&#21270;&#35789;&#23884;&#20837;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#22312;&#19981;&#20351;&#29992;&#20219;&#20309;BoW&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#25512;&#26029;&#20986;&#25991;&#26723;&#30340;&#20027;&#39064;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#20174;&#19978;&#19979;&#25991;&#21270;&#35789;&#23884;&#20837;&#20013;&#25512;&#26029;&#20986;&#25991;&#26723;&#20013;&#27599;&#20010;&#21333;&#35789;&#30340;&#20027;&#39064;&#20998;&#24067;&#12290;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#20165;&#20381;&#36182;BoW&#34920;&#31034;&#21644;&#20854;&#20182;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#30340;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of neural topic models in recent years, topic modelling is playing an increasingly important role in natural language understanding. However, most existing topic models still rely on bag-of-words (BoW) information, either as training input or training target. This limits their ability to capture word order information in documents and causes them to suffer from the out-of-vocabulary (OOV) issue, i.e. they cannot handle unobserved words in new documents. Contextualized word embeddings from pre-trained language models show superiority in the ability of word sense disambiguation and prove to be effective in dealing with OOV words. In this work, we developed a novel neural topic model combining contextualized word embeddings from the pre-trained language model BERT. The model can infer the topic distribution of a document without using any BoW information. In addition, the model can infer the topic distribution of each word in a document directly from the contextualize
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#21152;&#24378;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23545;&#38271;&#31687;&#31185;&#25216;&#35770;&#25991;&#30340;&#20851;&#38190;&#35789;&#25552;&#21462;&#12290;&#36890;&#36807;&#26500;&#24314;&#25991;&#26412;&#20849;&#29616;&#22270;&#24182;&#32467;&#21512;&#22270;&#34920;&#31034;&#21644;&#19978;&#19979;&#25991;&#21270;&#30340;PLM&#23884;&#20837;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#38271;&#31687;&#25991;&#26723;&#65292;&#22686;&#24378;PLMs&#24615;&#33021;&#27604;&#29616;&#26377;&#25216;&#26415;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#21152;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2305.09316</link><description>&lt;p&gt;
&#21033;&#29992;&#22270;&#23884;&#20837;&#22686;&#24378;&#20174;&#38271;&#31687;&#31185;&#25216;&#35770;&#25991;&#20013;&#25552;&#21462;&#20851;&#38190;&#35789;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Enhancing Keyphrase Extraction from Long Scientific Documents using Graph Embeddings. (arXiv:2305.09316v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#21152;&#24378;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23545;&#38271;&#31687;&#31185;&#25216;&#35770;&#25991;&#30340;&#20851;&#38190;&#35789;&#25552;&#21462;&#12290;&#36890;&#36807;&#26500;&#24314;&#25991;&#26412;&#20849;&#29616;&#22270;&#24182;&#32467;&#21512;&#22270;&#34920;&#31034;&#21644;&#19978;&#19979;&#25991;&#21270;&#30340;PLM&#23884;&#20837;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#38271;&#31687;&#25991;&#26723;&#65292;&#22686;&#24378;PLMs&#24615;&#33021;&#27604;&#29616;&#26377;&#25216;&#26415;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#21152;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#34920;&#31034;&#24378;&#21270;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#23545;&#38271;&#31687;&#25991;&#26723;&#20013;&#30340;&#20851;&#38190;&#35789;&#25552;&#21462;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21033;&#29992;&#22270;&#23884;&#20837;&#22686;&#24378;PLM&#25552;&#20379;&#26356;&#20840;&#38754;&#30340;&#35821;&#20041;&#29702;&#35299;&#25991;&#26723;&#20013;&#30340;&#21333;&#35789;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#38271;&#31687;&#25991;&#26723;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#25991;&#26412;&#30340;&#20849;&#29616;&#22270;&#65292;&#24182;&#20351;&#29992;&#22312;&#36793;&#39044;&#27979;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#26469;&#23884;&#20837;&#23427;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#22686;&#24378;&#30340;&#24207;&#21015;&#26631;&#35760;&#26550;&#26500;&#65292;&#23427;&#23558;&#19978;&#19979;&#25991;&#21270;&#30340;PLM&#23884;&#20837;&#19982;&#22270;&#34920;&#31034;&#30456;&#32467;&#21512;&#12290;&#22312;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#22686;&#24378;PLM&#19982;&#22270;&#23884;&#20837;&#27604;&#29616;&#26377;&#25216;&#26415;&#30340;&#27169;&#22411;&#22312;&#38271;&#25991;&#26723;&#19978;&#34920;&#29616;&#26356;&#20986;&#33394;&#65292;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#20013;&#37117;&#26174;&#30528;&#25552;&#39640;F1&#24471;&#20998;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#31361;&#20986;&#20102;GNN&#34920;&#31034;&#20316;&#20026;&#25913;&#36827;PLM&#24615;&#33021;&#30340;&#19968;&#31181;&#34917;&#20805;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we investigate using graph neural network (GNN) representations to enhance contextualized representations of pre-trained language models (PLMs) for keyphrase extraction from lengthy documents. We show that augmenting a PLM with graph embeddings provides a more comprehensive semantic understanding of words in a document, particularly for long documents. We construct a co-occurrence graph of the text and embed it using a graph convolutional network (GCN) trained on the task of edge prediction. We propose a graph-enhanced sequence tagging architecture that augments contextualized PLM embeddings with graph representations. Evaluating on benchmark datasets, we demonstrate that enhancing PLMs with graph embeddings outperforms state-of-the-art models on long documents, showing significant improvements in F1 scores across all the datasets. Our study highlights the potential of GNN representations as a complementary approach to improve PLM performance for keyphrase extraction fro
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HybRank&#30340;&#28151;&#21512;&#19982;&#21327;&#20316;&#30340;&#27573;&#33853;&#20877;&#25490;&#24207;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#19978;&#28216;&#26816;&#32034;&#22120;&#30340;&#30456;&#20284;&#24615;&#24230;&#37327;&#23454;&#29616;&#27573;&#33853;&#21327;&#20316;&#65292;&#20877;&#21033;&#29992;&#31232;&#30095;&#21644;&#23494;&#38598;&#26816;&#32034;&#22120;&#30340;&#35789;&#27719;&#21644;&#35821;&#20041;&#23646;&#24615;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22686;&#24378;&#21253;&#25324;&#20808;&#21069;&#34987;&#37325;&#26032;&#25490;&#24207;&#30340;&#27573;&#33853;&#21015;&#34920;&#22312;&#20869;&#30340;&#20219;&#24847;&#27573;&#33853;&#21015;&#34920;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#24615;&#33021;&#31283;&#23450;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.09313</link><description>&lt;p&gt;
&#28151;&#21512;&#19982;&#21327;&#20316;&#30340;&#27573;&#33853;&#20877;&#25490;&#24207;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Hybrid and Collaborative Passage Reranking. (arXiv:2305.09313v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09313
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HybRank&#30340;&#28151;&#21512;&#19982;&#21327;&#20316;&#30340;&#27573;&#33853;&#20877;&#25490;&#24207;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#19978;&#28216;&#26816;&#32034;&#22120;&#30340;&#30456;&#20284;&#24615;&#24230;&#37327;&#23454;&#29616;&#27573;&#33853;&#21327;&#20316;&#65292;&#20877;&#21033;&#29992;&#31232;&#30095;&#21644;&#23494;&#38598;&#26816;&#32034;&#22120;&#30340;&#35789;&#27719;&#21644;&#35821;&#20041;&#23646;&#24615;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22686;&#24378;&#21253;&#25324;&#20808;&#21069;&#34987;&#37325;&#26032;&#25490;&#24207;&#30340;&#27573;&#33853;&#21015;&#34920;&#22312;&#20869;&#30340;&#20219;&#24847;&#27573;&#33853;&#21015;&#34920;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#24615;&#33021;&#31283;&#23450;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27573;&#33853;&#26816;&#32034;&#31995;&#32479;&#20013;&#65292;&#21021;&#22987;&#26816;&#32034;&#32467;&#26524;&#21487;&#33021;&#19981;&#23613;&#22914;&#20154;&#24847;&#65292;&#38656;&#35201;&#36890;&#36807;&#37325;&#26032;&#25490;&#24207;&#26041;&#26696;&#36827;&#34892;&#25913;&#21892;&#12290;&#29616;&#26377;&#30340;&#27573;&#33853;&#37325;&#26032;&#25490;&#24207;&#26041;&#26696;&#20027;&#35201;&#38598;&#20013;&#20110;&#20016;&#23500;&#26597;&#35810;&#21644;&#27599;&#20010;&#27573;&#33853;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#24573;&#30053;&#20102;&#22312;&#21021;&#22987;&#26816;&#32034;&#21015;&#34920;&#20013;&#25490;&#21517;&#38752;&#21069;&#30340;&#22810;&#20010;&#27573;&#33853;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#19982;&#21327;&#20316;&#30340;&#27573;&#33853;&#20877;&#25490;&#24207;&#26041;&#27861;&#65288;HybRank&#65289;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#19978;&#28216;&#26816;&#32034;&#22120;&#30340;&#30456;&#20284;&#24615;&#24230;&#37327;&#36827;&#34892;&#27573;&#33853;&#21327;&#20316;&#65292;&#24182;&#32467;&#21512;&#31232;&#30095;&#21644;&#23494;&#38598;&#26816;&#32034;&#22120;&#30340;&#35789;&#27719;&#21644;&#35821;&#20041;&#23646;&#24615;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#29616;&#25104;&#30340;&#26816;&#32034;&#22120;&#29305;&#24449;&#65292;HybRank&#26159;&#19968;&#20010;&#25554;&#20214;&#20877;&#25490;&#24207;&#22120;&#65292;&#33021;&#22815;&#22686;&#24378;&#21253;&#25324;&#20808;&#21069;&#37325;&#26032;&#25490;&#24207;&#30340;&#27573;&#33853;&#21015;&#34920;&#22312;&#20869;&#30340;&#20219;&#24847;&#27573;&#33853;&#21015;&#34920;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#27604;&#26222;&#36941;&#30340;&#26816;&#32034;&#21644;&#20877;&#25490;&#24207;&#26041;&#27861;&#24615;&#33021;&#31283;&#23450;&#30340;&#25552;&#21319;&#65292;&#24182;&#39564;&#35777;&#20102;HybRank&#30340;&#26680;&#24515;&#32452;&#20214;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In passage retrieval system, the initial passage retrieval results may be unsatisfactory, which can be refined by a reranking scheme. Existing solutions to passage reranking focus on enriching the interaction between query and each passage separately, neglecting the context among the top-ranked passages in the initial retrieval list. To tackle this problem, we propose a Hybrid and Collaborative Passage Reranking (HybRank) method, which leverages the substantial similarity measurements of upstream retrievers for passage collaboration and incorporates the lexical and semantic properties of sparse and dense retrievers for reranking. Besides, built on off-the-shelf retriever features, HybRank is a plug-in reranker capable of enhancing arbitrary passage lists including previously reranked ones. Extensive experiments demonstrate the stable improvements of performance over prevalent retrieval and reranking methods, and verify the effectiveness of the core components of HybRank.
&lt;/p&gt;</description></item><item><title>OmniSafe&#26159;&#19968;&#31181;&#22522;&#30784;&#35774;&#26045;&#65292;&#29992;&#20110;&#21152;&#36895;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#65292;&#24110;&#21161;&#35299;&#20915;&#24403;&#20195;SafeRL&#30740;&#31350;&#29615;&#22659;&#20013;&#32570;&#20047;&#21327;&#35843;&#21644;&#26377;&#25928;&#30340;&#23398;&#20064;&#26694;&#26550;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.09304</link><description>&lt;p&gt;
OmniSafe&#65306;&#19968;&#31181;&#21152;&#36895;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#30340;&#22522;&#30784;&#35774;&#26045;
&lt;/p&gt;
&lt;p&gt;
OmniSafe: An Infrastructure for Accelerating Safe Reinforcement Learning Research. (arXiv:2305.09304v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09304
&lt;/p&gt;
&lt;p&gt;
OmniSafe&#26159;&#19968;&#31181;&#22522;&#30784;&#35774;&#26045;&#65292;&#29992;&#20110;&#21152;&#36895;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#65292;&#24110;&#21161;&#35299;&#20915;&#24403;&#20195;SafeRL&#30740;&#31350;&#29615;&#22659;&#20013;&#32570;&#20047;&#21327;&#35843;&#21644;&#26377;&#25928;&#30340;&#23398;&#20064;&#26694;&#26550;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#36171;&#33021;&#30340;AI&#31995;&#32479;&#26377;&#30528;&#20419;&#36827;&#31038;&#20250;&#36827;&#27493;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#30340;&#37096;&#32626;&#24120;&#24120;&#21463;&#21040;&#37325;&#22823;&#23433;&#20840;&#38544;&#24739;&#30340;&#38459;&#30861;&#12290;&#23588;&#20854;&#26159;&#22312;&#23433;&#20840;&#20851;&#38190;&#30340;&#24212;&#29992;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#24341;&#36215;&#20102;&#23545;&#19981;&#21463;&#32422;&#26463;&#30340;RL&#20195;&#29702;&#30340;&#24847;&#22806;&#20260;&#23475;&#25110;&#19981;&#23433;&#20840;&#34892;&#20026;&#30340;&#25285;&#24551;&#12290;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#65288;SafeRL&#65289;&#30340;&#29702;&#24565;&#26159;&#23558;RL&#20195;&#29702;&#19982;&#26080;&#23475;&#24847;&#22270;&#21644;&#23433;&#20840;&#34892;&#20026;&#27169;&#24335;&#30456;&#19968;&#33268;&#12290;&#22312;SafeRL&#20013;&#65292;&#20195;&#29702;&#36890;&#36807;&#20174;&#29615;&#22659;&#20013;&#25509;&#25910;&#21453;&#39304;&#26469;&#23398;&#20064;&#24320;&#21457;&#20248;&#21270;&#31574;&#30053;&#65292;&#21516;&#26102;&#20063;&#28385;&#36275;&#20102;&#23558;&#24847;&#22806;&#20260;&#23475;&#25110;&#19981;&#23433;&#20840;&#34892;&#20026;&#30340;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#35201;&#27714;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;SafeRL&#31639;&#27861;&#23454;&#29616;&#30340;&#22797;&#26434;&#24615;&#65292;&#36328;&#36234;&#21508;&#20010;&#39046;&#22495;&#30340;&#26041;&#27861;&#30340;&#32467;&#21512;&#25552;&#20986;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#36825;&#23548;&#33268;&#20102;&#24403;&#20195;SafeRL&#30740;&#31350;&#29615;&#22659;&#20013;&#32570;&#20047;&#19968;&#20010;&#21327;&#35843;&#21644;&#26377;&#25928;&#30340;&#23398;&#20064;&#26694;&#26550;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#30784;&#26694;&#26550;&#65306;OmniSafe&#65292;&#29992;&#20110;&#21152;&#36895;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI systems empowered by reinforcement learning (RL) algorithms harbor the immense potential to catalyze societal advancement, yet their deployment is often impeded by significant safety concerns. Particularly in safety-critical applications, researchers have raised concerns about unintended harms or unsafe behaviors of unaligned RL agents. The philosophy of safe reinforcement learning (SafeRL) is to align RL agents with harmless intentions and safe behavioral patterns. In SafeRL, agents learn to develop optimal policies by receiving feedback from the environment, while also fulfilling the requirement of minimizing the risk of unintended harm or unsafe behavior. However, due to the intricate nature of SafeRL algorithm implementation, combining methodologies across various domains presents a formidable challenge. This had led to an absence of a cohesive and efficacious learning framework within the contemporary SafeRL research milieu. In this work, we introduce a foundational framework d
&lt;/p&gt;</description></item><item><title>&#36825;&#26159;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#34987;&#30830;&#35748;&#20026;&#35088;&#21270;&#34746;(P. canaliculata)&#34507;&#30340;&#22270;&#20687;&#20197;&#21450;&#23545;&#24212;&#30340;&#36793;&#30028;&#26694;&#27880;&#37322;&#65292;&#26088;&#22312;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20998;&#26512;P. canaliculata&#29289;&#31181;&#30340;&#20256;&#25773;&#24773;&#20917;&#65292;&#24182;&#25903;&#25345;&#20854;&#20182;&#38656;&#35201;&#19982;&#35088;&#21270;&#34746;&#34507;&#30456;&#20851;&#30340;&#35270;&#35273;&#25968;&#25454;&#30340;&#35843;&#26597;&#27963;&#21160;&#12290;</title><link>http://arxiv.org/abs/2305.09302</link><description>&lt;p&gt;
Pink-Eggs&#25968;&#25454;&#38598;V1&#65306;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#23884;&#20837;&#24335;&#35299;&#20915;&#26041;&#26696;&#31649;&#29702;&#20837;&#20405;&#29289;&#31181;&#30340;&#19968;&#27493;
&lt;/p&gt;
&lt;p&gt;
Pink-Eggs Dataset V1: A Step Toward Invasive Species Management Using Deep Learning Embedded Solutions. (arXiv:2305.09302v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09302
&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#34987;&#30830;&#35748;&#20026;&#35088;&#21270;&#34746;(P. canaliculata)&#34507;&#30340;&#22270;&#20687;&#20197;&#21450;&#23545;&#24212;&#30340;&#36793;&#30028;&#26694;&#27880;&#37322;&#65292;&#26088;&#22312;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20998;&#26512;P. canaliculata&#29289;&#31181;&#30340;&#20256;&#25773;&#24773;&#20917;&#65292;&#24182;&#25903;&#25345;&#20854;&#20182;&#38656;&#35201;&#19982;&#35088;&#21270;&#34746;&#34507;&#30456;&#20851;&#30340;&#35270;&#35273;&#25968;&#25454;&#30340;&#35843;&#26597;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#34987;&#30830;&#35748;&#20026;&#35088;&#21270;&#34746;(P. canaliculata)&#34507;&#30340;&#22270;&#20687;&#65292;&#38468;&#24102;&#20102;&#23545;&#24212;&#30340;&#36793;&#30028;&#26694;&#27880;&#37322;&#12290;&#35813;&#25968;&#25454;&#38598;&#30340;&#30446;&#30340;&#26159;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#20998;&#26512;P. canaliculata&#29289;&#31181;&#30340;&#20256;&#25773;&#24773;&#20917;&#65292;&#24182;&#25903;&#25345;&#20854;&#20182;&#38656;&#35201;&#19982;&#35088;&#21270;&#34746;&#34507;&#30456;&#20851;&#30340;&#35270;&#35273;&#25968;&#25454;&#30340;&#35843;&#26597;&#27963;&#21160;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#27880;&#24847;&#30340;&#26159;&#65292;&#26377;&#20854;&#20182;&#29289;&#31181;&#22312;&#32654;&#27954;&#22320;&#21306;&#20135;&#29983;&#31867;&#20284;&#35088;&#21270;&#34746;&#34507;&#30340;&#35266;&#23519;&#35760;&#24405;&#12290;&#22240;&#27492;&#65292;&#20219;&#20309;&#28040;&#38500;&#36825;&#20123;&#34507;&#30340;&#20915;&#23450;&#30340;&#20851;&#38190;&#21069;&#25552;&#26159;&#26126;&#30830;&#23427;&#20204;&#26159;&#21542;&#23436;&#20840;&#24402;&#23646;&#20110;&#20837;&#20405;&#30340;P. canaliculata&#25110;&#26159;&#21542;&#28041;&#21450;&#20854;&#20182;&#29289;&#31181;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel dataset consisting of images depicting pink eggs that have been identified as Pomacea canaliculata eggs, accompanied by corresponding bounding box annotations. The purpose of this dataset is to aid researchers in the analysis of the spread of Pomacea canaliculata species by utilizing deep learning techniques, as well as supporting other investigative pursuits that require visual data pertaining to the eggs of Pomacea canaliculata. It is worth noting, however, that the identity of the eggs in question is not definitively established, as other species within the same taxonomic family have been observed to lay similar-looking eggs in regions of the Americas. Therefore, a crucial prerequisite to any decision regarding the elimination of these eggs would be to establish with certainty whether they are exclusively attributable to invasive Pomacea canaliculata or if other species are also involved. The dataset is available at https://www.kaggle.com/datasets/deeshenzhen/pi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Dune Neural Network&#8221;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#23558;&#32593;&#32476;&#30340;&#27599;&#20010;&#33258;&#30001;&#21442;&#25968;&#34920;&#31034;&#20026;&#19981;&#30830;&#23450;&#24615;&#21306;&#38388;&#65292;&#24182;&#23545;&#27599;&#20010;&#36755;&#20837;&#20803;&#32032;&#24212;&#29992;&#32447;&#24615;&#21464;&#25442;&#65292;&#23454;&#29616;&#20102;&#23545;&#20110;&#30333;&#22122;&#22768;&#25968;&#25454;&#30340;&#22122;&#22768;&#40065;&#26834;&#24615;&#33021;&#65292;&#21363;&#20351;&#20026;&#38750;&#24120;&#22024;&#26434;&#30340;&#36755;&#20837;&#22270;&#20687;&#65292;&#27492;&#26041;&#27861;&#20063;&#27604;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#30340;&#20154;&#31867;&#22312;&#27979;&#35797;&#38598;&#19978;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.09276</link><description>&lt;p&gt;
&#22122;&#22768;&#40065;&#26834;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Noise robust neural network architecture. (arXiv:2305.09276v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Dune Neural Network&#8221;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#23558;&#32593;&#32476;&#30340;&#27599;&#20010;&#33258;&#30001;&#21442;&#25968;&#34920;&#31034;&#20026;&#19981;&#30830;&#23450;&#24615;&#21306;&#38388;&#65292;&#24182;&#23545;&#27599;&#20010;&#36755;&#20837;&#20803;&#32032;&#24212;&#29992;&#32447;&#24615;&#21464;&#25442;&#65292;&#23454;&#29616;&#20102;&#23545;&#20110;&#30333;&#22122;&#22768;&#25968;&#25454;&#30340;&#22122;&#22768;&#40065;&#26834;&#24615;&#33021;&#65292;&#21363;&#20351;&#20026;&#38750;&#24120;&#22024;&#26434;&#30340;&#36755;&#20837;&#22270;&#20687;&#65292;&#27492;&#26041;&#27861;&#20063;&#27604;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#30340;&#20154;&#31867;&#22312;&#27979;&#35797;&#38598;&#19978;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Dune Neural Network&#8221;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#35782;&#21035;&#19968;&#33324;&#22122;&#22768;&#22270;&#20687;&#65292;&#32780;&#19981;&#38656;&#35201;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#28155;&#21152;&#20219;&#20309;&#20154;&#24037;&#22122;&#22768;&#12290;&#36890;&#36807;&#23558;&#32593;&#32476;&#30340;&#27599;&#20010;&#33258;&#30001;&#21442;&#25968;&#34920;&#31034;&#20026;&#19981;&#30830;&#23450;&#24615;&#21306;&#38388;&#65292;&#24182;&#23545;&#27599;&#20010;&#36755;&#20837;&#20803;&#32032;&#24212;&#29992;&#32447;&#24615;&#21464;&#25442;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#24471;&#21040;&#30340;&#26550;&#26500;&#22312;&#38754;&#23545;&#26377;&#30333;&#22122;&#22768;&#30340;&#36755;&#20837;&#25968;&#25454;&#26102;&#20855;&#26377;&#30456;&#24403;&#22909;&#30340;&#22122;&#22768;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23558;&#31616;&#21333;&#30340;Dune&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20110;MNIST&#25968;&#25454;&#38598;&#65292;&#24182;&#35777;&#26126;&#20102;&#21363;&#20351;&#22312;&#23545;&#20110;&#20154;&#31867;&#38590;&#20197;&#35782;&#21035;&#30340;&#38750;&#24120;&#22024;&#26434;&#30340;&#36755;&#20837;&#22270;&#20687;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20063;&#27604;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#30340;&#20154;&#31867;&#22312;&#27979;&#35797;&#38598;&#19978;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#20934;&#30830;&#24230;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#35768;&#22810;&#20854;&#20182;&#28155;&#21152;&#20102;&#21508;&#31181;&#32972;&#26223;&#27169;&#24335;&#30340;&#31034;&#20363;&#37117;&#24456;&#40065;&#26834;&#12290;
&lt;/p&gt;
&lt;p&gt;
In which we propose neural network architecture (dune neural network) for recognizing general noisy image without adding any artificial noise in the training data. By representing each free parameter of the network as an uncertainty interval, and applying a linear transformation to each input element, we show that the resulting architecture achieves decent noise robustness when faced with input data with white noise. We apply simple dune neural networks for MNIST dataset and demonstrate that even for very noisy input images which are hard for human to recognize, our approach achieved better test set accuracy than human without dataset augmentation. We also find that our method is robust for many other examples with various background patterns added.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#36866;&#24212;&#24615;&#30340;&#24120;&#35265;&#35780;&#20272;&#26041;&#27861;&#8212;&#8212;&#22312;&#32447;&#20934;&#30830;&#29575;&#24230;&#37327;&#12290;&#35813;&#24230;&#37327;&#26159;&#19981;&#21487;&#38752;&#30340;&#65292;&#29616;&#26377;&#31639;&#27861;&#20250;&#23398;&#20064;&#34920;&#38754;&#30340;&#26631;&#31614;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26469;&#34913;&#37327;&#36866;&#24212;&#24615;&#24182;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2305.09275</link><description>&lt;p&gt;
&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#24555;&#36895;&#36866;&#24212;&#24615;&#65306;&#25105;&#20204;&#35780;&#20272;&#24471;&#23545;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Rapid Adaptation in Online Continual Learning: Are We Evaluating It Right?. (arXiv:2305.09275v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#36866;&#24212;&#24615;&#30340;&#24120;&#35265;&#35780;&#20272;&#26041;&#27861;&#8212;&#8212;&#22312;&#32447;&#20934;&#30830;&#29575;&#24230;&#37327;&#12290;&#35813;&#24230;&#37327;&#26159;&#19981;&#21487;&#38752;&#30340;&#65292;&#29616;&#26377;&#31639;&#27861;&#20250;&#23398;&#20064;&#34920;&#38754;&#30340;&#26631;&#31614;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26469;&#34913;&#37327;&#36866;&#24212;&#24615;&#24182;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#36866;&#24212;&#24615;&#30340;&#24120;&#35265;&#35780;&#20272;&#26041;&#27861;&#8212;&#8212;&#22312;&#32447;&#20934;&#30830;&#29575;&#24230;&#37327;&#12290;&#35813;&#24230;&#37327;&#34913;&#37327;&#27169;&#22411;&#22312;&#25509;&#19979;&#26469;&#30340;&#20960;&#20010;&#26679;&#26412;&#19978;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#24230;&#37327;&#26159;&#19981;&#21487;&#38752;&#30340;&#12290;&#21363;&#20351;&#26159;&#31354;&#27867;&#30340;&#30450;&#20998;&#31867;&#22120;&#20063;&#33021;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#27969;&#20013;&#34920;&#38754;&#30340;&#26631;&#31614;&#30456;&#20851;&#24615;&#26469;&#23454;&#29616;&#38750;&#24120;&#39640;&#30340;&#22312;&#32447;&#20934;&#30830;&#29575;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#20063;&#33021;&#22815;&#23454;&#29616;&#39640;&#22312;&#32447;&#20934;&#30830;&#29575;&#65292;&#20294;&#26159;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#20449;&#24687;&#20445;&#30041;&#33021;&#21147;&#65292;&#34920;&#26126;&#23427;&#20204;&#24847;&#22806;&#22320;&#23398;&#20064;&#20102;&#34920;&#38754;&#30340;&#26631;&#31614;&#30456;&#20851;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28040;&#38500;&#34920;&#38754;&#30340;&#30456;&#20851;&#24615;&#30340;&#36817;&#26399;&#26679;&#26412;&#20934;&#30830;&#24230;&#30340;&#26032;&#24230;&#37327;&#26469;&#34913;&#37327;&#36866;&#24212;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#24230;&#37327;&#22312;&#21508;&#31181;&#35745;&#31639;&#39044;&#31639;&#19979;&#23545;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20013;&#29616;&#26377;&#30340;OCL&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#21457;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#21487;&#20197;&#36798;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We revisit the common practice of evaluating adaptation of Online Continual Learning (OCL) algorithms through the metric of online accuracy, which measures the accuracy of the model on the immediate next few samples. However, we show that this metric is unreliable, as even vacuous blind classifiers, which do not use input images for prediction, can achieve unrealistically high online accuracy by exploiting spurious label correlations in the data stream. Our study reveals that existing OCL algorithms can also achieve high online accuracy, but perform poorly in retaining useful information, suggesting that they unintentionally learn spurious label correlations. To address this issue, we propose a novel metric for measuring adaptation based on the accuracy on the near-future samples, where spurious correlations are removed. We benchmark existing OCL approaches using our proposed metric on large-scale datasets under various computational budgets and find that better generalization can be a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36951;&#20256;&#31639;&#27861;&#32534;&#30721;&#34920;&#31034;&#26469;&#35299;&#20915;&#26053;&#34892;&#21830;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#19982;&#26368;&#20808;&#36827;&#30340;&#32534;&#30721;&#34920;&#31034;&#30340;&#27604;&#36739;&#36827;&#34892;&#20102;&#24615;&#33021;&#35780;&#20215;&#12290;</title><link>http://arxiv.org/abs/2305.09257</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#30340;&#33410;&#28857;&#31227;&#20301;&#32534;&#30721;&#34920;&#31034;&#26469;&#35299;&#20915;&#26053;&#34892;&#21830;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
A new node-shift encoding representation for the travelling salesman problem. (arXiv:2305.09257v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09257
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36951;&#20256;&#31639;&#27861;&#32534;&#30721;&#34920;&#31034;&#26469;&#35299;&#20915;&#26053;&#34892;&#21830;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#19982;&#26368;&#20808;&#36827;&#30340;&#32534;&#30721;&#34920;&#31034;&#30340;&#27604;&#36739;&#36827;&#34892;&#20102;&#24615;&#33021;&#35780;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#36951;&#20256;&#31639;&#27861;&#32534;&#30721;&#34920;&#31034;&#26469;&#35299;&#20915;&#26053;&#34892;&#21830;&#38382;&#39064;&#12290;&#20026;&#20102;&#35780;&#20215;&#25152;&#25552;&#20986;&#30340;&#26579;&#33394;&#20307;&#32467;&#26500;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;&#26368;&#20808;&#36827;&#30340;&#32534;&#30721;&#34920;&#31034;&#36827;&#34892;&#27604;&#36739;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#26469;&#33258;TSPLIB&#30340;14&#20010;&#19981;&#21516;&#22823;&#23567;&#30340;&#22522;&#20934;&#12290;&#26368;&#32456;&#65292;&#22312;&#36827;&#34892;&#23454;&#39564;&#30740;&#31350;&#21518;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#25152;&#33719;&#24471;&#30340;&#32467;&#26524;&#24182;&#24471;&#20986;&#20102;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new genetic algorithm encoding representation to solve the travelling salesman problem. To assess the performance of the proposed chromosome structure, we compare it with state-of-the-art encoding representations. For that purpose, we use 14 benchmarks of different sizes taken from TSPLIB. Finally, after conducting the experimental study, we report the obtained results and draw our conclusion.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#21462;&#25972;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#27169;&#22411;&#35745;&#25968;&#30340;&#39640;&#25928;&#29575;&#21644;&#39640;&#32622;&#20449;&#24230;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2305.09247</link><description>&lt;p&gt;
&#21462;&#25972;&#36935;&#19978;&#36817;&#20284;&#27169;&#22411;&#35745;&#25968;
&lt;/p&gt;
&lt;p&gt;
Rounding Meets Approximate Model Counting. (arXiv:2305.09247v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#21462;&#25972;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#27169;&#22411;&#35745;&#25968;&#30340;&#39640;&#25928;&#29575;&#21644;&#39640;&#32622;&#20449;&#24230;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#35745;&#25968;&#38382;&#39064;&#65292;&#20063;&#31216;&#20026;&#65283; SAT&#65292;&#26159;&#35745;&#31639;&#32473;&#23450;&#24067;&#23572;&#20844;&#24335;$ F $&#30340;&#27169;&#22411;&#25110;&#28385;&#36275;&#36171;&#20540;&#30340;&#25968;&#37327;&#30340;&#38382;&#39064;&#12290;&#27169;&#22411;&#35745;&#25968;&#26159;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#26368;&#36817;&#65292;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;&#22522;&#20110;&#21704;&#24076;&#30340;&#25216;&#26415;&#36827;&#34892;&#36817;&#20284;&#27169;&#22411;&#35745;&#25968;&#65292;&#24182;&#25552;&#20379;$&#65288;\varepsilon&#65292;\delta&#65289;$&#20445;&#35777;&#65306;&#21363;&#65292;&#35745;&#25968;&#36820;&#22238;&#30340;&#32467;&#26524;&#19982;&#30830;&#20999;&#35745;&#25968;&#30340;$(1+\varepsilon)$-&#20056;&#25968;&#20855;&#26377;$1-\delta$&#30340;&#32622;&#20449;&#24230;&#12290;&#34429;&#28982;&#21704;&#24076;&#25216;&#26415;&#21487;&#22312;$ \delta $&#36275;&#22815;&#22823;&#26102;&#20855;&#26377;&#21512;&#29702;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#20294;&#22312;&#36739;&#23567;&#30340;$\delta$&#20540;&#26102;&#65292;&#20854;&#21487;&#25193;&#23637;&#24615;&#21463;&#21040;&#20005;&#37325;&#24433;&#21709;&#65292;&#20174;&#32780;&#38459;&#27490;&#20102;&#20854;&#22312;&#38656;&#35201;&#39640;&#32622;&#20449;&#24230;&#20272;&#35745;&#30340;&#24212;&#29992;&#39046;&#22495;&#30340;&#37319;&#29992;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#35299;&#20915;&#21704;&#24076;&#25216;&#26415;&#30340;&#33268;&#21629;&#24369;&#28857;&#65306;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21462;&#25972;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#36816;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of model counting, also known as #SAT, is to compute the number of models or satisfying assignments of a given Boolean formula $F$. Model counting is a fundamental problem in computer science with a wide range of applications. In recent years, there has been a growing interest in using hashing-based techniques for approximate model counting that provide $(\varepsilon, \delta)$-guarantees: i.e., the count returned is within a $(1+\varepsilon)$-factor of the exact count with confidence at least $1-\delta$. While hashing-based techniques attain reasonable scalability for large enough values of $\delta$, their scalability is severely impacted for smaller values of $\delta$, thereby preventing their adoption in application domains that require estimates with high confidence.  The primary contribution of this paper is to address the Achilles heel of hashing-based techniques: we propose a novel approach based on rounding that allows us to achieve a significant reduction in runtime
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#21482;&#38656;&#20351;&#29992;0.5%&#25968;&#25454;&#20415;&#21487;&#20197;&#36827;&#34892;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25351;&#20196;&#35843;&#25972;&#65292;&#24182;&#19988;&#19981;&#24433;&#21709;&#24615;&#33021;&#34920;&#29616;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#21644;&#33410;&#32422;&#22521;&#35757;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2305.09246</link><description>&lt;p&gt;
&#25110;&#35768;&#21482;&#38656;&#35201;0.5&#65285;&#30340;&#25968;&#25454;&#65306;&#20302;&#25968;&#25454;&#37327;&#35757;&#32451;&#25351;&#20196;&#35843;&#25972;&#21021;&#27493;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Maybe Only 0.5% Data is Needed: A Preliminary Exploration of Low Training Data Instruction Tuning. (arXiv:2305.09246v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#21482;&#38656;&#20351;&#29992;0.5%&#25968;&#25454;&#20415;&#21487;&#20197;&#36827;&#34892;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25351;&#20196;&#35843;&#25972;&#65292;&#24182;&#19988;&#19981;&#24433;&#21709;&#24615;&#33021;&#34920;&#29616;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#21644;&#33410;&#32422;&#22521;&#35757;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#38382;&#39064;&#65292;&#26412;&#25991;&#36827;&#34892;&#20102;&#21021;&#27493;&#25506;&#32034;&#65292;&#20197;&#38477;&#20302;LLM&#25351;&#20196;&#35843;&#25972;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#65292;&#20174;&#32780;&#20943;&#23569;&#22521;&#35757;&#25104;&#26412;&#65292;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#12290; &#30740;&#31350;&#21457;&#29616;&#65292;LLM&#25351;&#20196;&#35843;&#25972;&#30340;&#25968;&#25454;&#21487;&#20197;&#38477;&#33267;0.5&#65285;&#32780;&#19981;&#24433;&#21709;&#24615;&#33021;&#65292;&#23558;&#36825;&#31181;&#26041;&#27861;&#31216;&#20026;Low Training Data Instruction Tuning&#65288;LTD Instruction Tuning&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction tuning for large language models (LLMs) has gained attention from researchers due to its ability to unlock the potential of LLMs in following instructions. While instruction tuning offers advantages for facilitating the adaptation of large language models (LLMs) to downstream tasks as a fine-tuning approach, training models with tens of millions or even billions of parameters on large amounts of data results in unaffordable computational costs. To address this, we focus on reducing the data used in LLM instruction tuning to decrease training costs and improve data efficiency, dubbed as Low Training Data Instruction Tuning (LTD Instruction Tuning). Specifically, this paper conducts a preliminary exploration into reducing the data used in LLM training and identifies several observations regarding task specialization for LLM training, such as the optimization of performance for a specific task, the number of instruction types required for instruction tuning, and the amount of 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24212;&#29992;&#20110;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#29289;&#32852;&#32593;&#12289;&#36793;&#32536;&#21644;&#20113;&#30340;&#38544;&#31169;&#20445;&#25252;&#38598;&#25104;&#22686;&#24378;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#34701;&#21512;&#21644;&#38598;&#25104;&#23398;&#20064;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#21033;&#29992;&#24046;&#20998;&#38544;&#31169;&#21644;&#36801;&#31227;&#23398;&#20064;&#30830;&#20445;&#22312;&#36793;&#32536;&#26381;&#21153;&#22120;&#19978;&#26368;&#23567;&#25439;&#22833;&#21644;&#26356;&#39640;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.09224</link><description>&lt;p&gt;
&#38754;&#21521;&#36793;&#32536;&#20113;&#34701;&#21512;&#30340;&#38544;&#31169;&#20445;&#25252;&#38598;&#25104;&#22686;&#24378;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving Ensemble Infused Enhanced Deep Neural Network Framework for Edge Cloud Convergence. (arXiv:2305.09224v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09224
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24212;&#29992;&#20110;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#29289;&#32852;&#32593;&#12289;&#36793;&#32536;&#21644;&#20113;&#30340;&#38544;&#31169;&#20445;&#25252;&#38598;&#25104;&#22686;&#24378;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#34701;&#21512;&#21644;&#38598;&#25104;&#23398;&#20064;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#21033;&#29992;&#24046;&#20998;&#38544;&#31169;&#21644;&#36801;&#31227;&#23398;&#20064;&#30830;&#20445;&#22312;&#36793;&#32536;&#26381;&#21153;&#22120;&#19978;&#26368;&#23567;&#25439;&#22833;&#21644;&#26356;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#12289;&#36793;&#32536;&#21644;&#20113;&#30340;&#25910;&#25947;&#30340;&#38544;&#31169;&#20445;&#25252;&#38598;&#25104;&#22686;&#24378;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#23398;&#20064;&#26694;&#26550;&#12290;&#22312;&#36825;&#31181;&#25910;&#25947;&#20013;&#65292;&#36793;&#32536;&#26381;&#21153;&#22120;&#29992;&#20110;&#23384;&#20648; IoT &#20135;&#29983;&#30340;&#29983;&#29289;&#22270;&#20687;&#24182;&#25176;&#31649; DNN &#31639;&#27861;&#20197;&#36827;&#34892;&#26412;&#22320;&#27169;&#22411;&#35757;&#32451;&#12290;&#20113;&#29992;&#20110;&#21512;&#24182;&#26412;&#22320;&#27169;&#22411;&#12290;&#20351;&#29992;&#26412;&#22320;&#25968;&#25454;&#38598;&#30340; DNN &#22522;&#30784;&#35757;&#32451;&#36807;&#31243;&#21487;&#33021;&#21463;&#21040;&#20302;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#21069;&#36848;&#30340;&#34701;&#21512;&#21644;&#38598;&#25104;&#23398;&#20064;&#26469;&#25913;&#21892;&#12290;&#38598;&#25104;&#23398;&#20064;&#20801;&#35768;&#22810;&#20010;&#21442;&#19982;&#32773;&#22806;&#21253;&#20854;&#26412;&#22320;&#27169;&#22411;&#65292;&#20197;&#20135;&#29983;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#30340;&#24191;&#20041;&#26368;&#32456;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#38598;&#25104;&#23398;&#20064;&#25552;&#39640;&#20102;&#20174;&#26368;&#32456;&#27169;&#22411;&#20013;&#27844;&#28431;&#25935;&#24863;&#31169;&#20154;&#25968;&#25454;&#30340;&#39118;&#38505;&#12290;&#35813;&#25552;&#20986;&#30340;&#26694;&#26550;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#24046;&#20998;&#38544;&#31169;&#30340;&#38544;&#31169;&#20445;&#25252; DNN&#65292;&#24182;&#32467;&#21512;&#36801;&#31227;&#23398;&#20064;&#36827;&#34892;&#26412;&#22320;&#27169;&#22411;&#29983;&#25104;&#65292;&#20197;&#30830;&#20445;&#22312;&#36793;&#32536;&#26381;&#21153;&#22120;&#19978;&#26368;&#23567;&#25439;&#22833;&#21644;&#26356;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a privacy-preserving ensemble infused enhanced Deep Neural Network (DNN) based learning framework in this paper for Internet-of-Things (IoT), edge, and cloud convergence in the context of healthcare. In the convergence, edge server is used for both storing IoT produced bioimage and hosting DNN algorithm for local model training. The cloud is used for ensembling local models. The DNN-based training process of a model with a local dataset suffers from low accuracy, which can be improved by the aforementioned convergence and Ensemble Learning. The ensemble learning allows multiple participants to outsource their local model for producing a generalized final model with high accuracy. Nevertheless, Ensemble Learning elevates the risk of leaking sensitive private data from the final model. The proposed framework presents a Differential Privacy-based privacy-preserving DNN with Transfer Learning for a local model generation to ensure minimal loss and higher efficiency at edge serve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#23558;&#22810;&#35821;&#35328;&#25688;&#35201;&#21644;&#36328;&#35821;&#35328;&#25688;&#35201;&#32479;&#19968;&#21040;&#26356;&#36890;&#29992;&#30340;&#22810;&#23545;&#22810;&#25688;&#35201;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#39044;&#20808;&#35757;&#32451;&#30340;M2MS&#27169;&#22411;&#8220;Pisces&#8221;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#22788;&#29702;&#20219;&#20309;&#35821;&#35328;&#30340;&#25991;&#26723;&#24182;&#29983;&#25104;&#25688;&#35201;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Pisces&#22312;&#38646;-shot&#26041;&#21521;&#19978;&#34920;&#29616;&#26174;&#30528;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.09220</link><description>&lt;p&gt;
&#36208;&#21521;&#32479;&#19968;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Towards Unifying Multi-Lingual and Cross-Lingual Summarization. (arXiv:2305.09220v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#23558;&#22810;&#35821;&#35328;&#25688;&#35201;&#21644;&#36328;&#35821;&#35328;&#25688;&#35201;&#32479;&#19968;&#21040;&#26356;&#36890;&#29992;&#30340;&#22810;&#23545;&#22810;&#25688;&#35201;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#39044;&#20808;&#35757;&#32451;&#30340;M2MS&#27169;&#22411;&#8220;Pisces&#8221;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#22788;&#29702;&#20219;&#20309;&#35821;&#35328;&#30340;&#25991;&#26723;&#24182;&#29983;&#25104;&#25688;&#35201;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Pisces&#22312;&#38646;-shot&#26041;&#21521;&#19978;&#34920;&#29616;&#26174;&#30528;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36866;&#24212;&#22810;&#35821;&#35328;&#30340;&#19990;&#30028;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#22810;&#35821;&#35328;&#25688;&#35201;&#65288;MLS&#65289;&#21644;&#36328;&#35821;&#35328;&#25688;&#35201;&#65288;CLS&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#20010;&#20219;&#21153;&#22240;&#20026;&#23450;&#20041;&#30340;&#19981;&#21516;&#32780;&#34987;&#20998;&#21035;&#30740;&#31350;&#65292;&#38480;&#21046;&#20102;&#20004;&#32773;&#30340;&#20860;&#23481;&#24615;&#21644;&#31995;&#32479;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#23558;MLS&#21644;CLS&#32479;&#19968;&#21040;&#26356;&#36890;&#29992;&#30340;&#35774;&#32622;&#20013;&#65292;&#21363;&#22810;&#23545;&#22810;&#25688;&#35201;&#65288;M2MS&#65289;&#65292;&#20854;&#20013;&#21333;&#20010;&#27169;&#22411;&#21487;&#20197;&#22788;&#29702;&#20219;&#20309;&#35821;&#35328;&#30340;&#25991;&#26723;&#24182;&#29983;&#25104;&#23427;&#20204;&#30340;&#25688;&#35201;&#65292;&#20063;&#21487;&#20197;&#29992;&#20219;&#20309;&#35821;&#35328;&#12290;&#20316;&#20026;&#36890;&#21521;M2MS&#30340;&#31532;&#19968;&#27493;&#65292;&#25105;&#20204;&#36827;&#34892;&#21021;&#27493;&#30740;&#31350;&#65292;&#34920;&#26126;M2MS&#21487;&#20197;&#26356;&#22909;&#22320;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#20256;&#36882;&#20219;&#21153;&#30693;&#35782;&#65292;&#32780;&#19981;&#26159;MLS&#21644;CLS&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Pisces&#65292;&#36825;&#26159;&#19968;&#20010;&#39044;&#20808;&#35757;&#32451;&#30340;M2MS&#27169;&#22411;&#65292;&#36890;&#36807;&#19977;&#38454;&#27573;&#30340;&#39044;&#20808;&#35757;&#32451;&#23398;&#20064;&#35821;&#35328;&#24314;&#27169;&#12289;&#36328;&#35821;&#35328;&#21644;&#25688;&#35201;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;Pisces&#26174;&#30528;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#65292;&#29305;&#21035;&#26159;&#22312;&#38646;-shot&#26041;&#21521;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
To adapt text summarization to the multilingual world, previous work proposes multi-lingual summarization (MLS) and cross-lingual summarization (CLS). However, these two tasks have been studied separately due to the different definitions, which limits the compatible and systematic research on both of them. In this paper, we aim to unify MLS and CLS into a more general setting, i.e., many-to-many summarization (M2MS), where a single model could process documents in any language and generate their summaries also in any language. As the first step towards M2MS, we conduct preliminary studies to show that M2MS can better transfer task knowledge across different languages than MLS and CLS. Furthermore, we propose Pisces, a pre-trained M2MS model that learns language modeling, cross-lingual ability and summarization ability via three-stage pre-training. Experimental results indicate that our Pisces significantly outperforms the state-of-the-art baselines, especially in the zero-shot directio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23433;&#20840;&#30340;&#22810;&#26041;&#35745;&#31639;&#30340;&#38598;&#25104;&#32852;&#37030;&#23398;&#20064;&#21644;&#21306;&#22359;&#38142;&#25216;&#26415;&#65292;&#20801;&#35768;&#24322;&#26500;&#27169;&#22411;&#20174;&#21307;&#30103;&#26426;&#26500;&#30340;&#25968;&#25454;&#20013;&#21327;&#21516;&#23398;&#20064;&#65292;&#20445;&#38556;&#25968;&#25454;&#38544;&#31169;&#21644;&#23436;&#25972;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09209</link><description>&lt;p&gt;
&#21306;&#22359;&#38142;&#19979;&#30340;&#21487;&#20449;&#38544;&#31169;&#20445;&#25252;&#30340;&#21307;&#30103;4.0&#32423;&#23618;&#27425;&#38598;&#25104;&#21644;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Trustworthy Privacy-preserving Hierarchical Ensemble and Federated Learning in Healthcare 4.0 with Blockchain. (arXiv:2305.09209v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23433;&#20840;&#30340;&#22810;&#26041;&#35745;&#31639;&#30340;&#38598;&#25104;&#32852;&#37030;&#23398;&#20064;&#21644;&#21306;&#22359;&#38142;&#25216;&#26415;&#65292;&#20801;&#35768;&#24322;&#26500;&#27169;&#22411;&#20174;&#21307;&#30103;&#26426;&#26500;&#30340;&#25968;&#25454;&#20013;&#21327;&#21516;&#23398;&#20064;&#65292;&#20445;&#38556;&#25968;&#25454;&#38544;&#31169;&#21644;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#21644;&#36890;&#20449;&#25216;&#26415;&#30340;&#36827;&#27493;&#24341;&#39046;&#20102;&#24037;&#19994;4.0&#30340;&#26102;&#20195;&#12290;&#27492;&#36716;&#21464;&#38543;&#20043;&#32780;&#26469;&#30340;&#26159;&#21307;&#30103;&#34892;&#19994;&#21019;&#36896;&#20986;&#30340;&#21307;&#30103;4.0&#12290;&#22312;&#21307;&#30103;4.0&#20013;&#65292;&#20351;&#29992;&#29289;&#32852;&#32593;&#21307;&#30103;&#24433;&#20687;&#35774;&#22791;&#36827;&#34892;&#26089;&#26399;&#30142;&#30149;&#26816;&#27979;&#20351;&#21307;&#25252;&#20154;&#21592;&#33021;&#22815;&#25552;&#39640;&#21307;&#30103;&#26426;&#26500;&#30340;&#26381;&#21153;&#36136;&#37327;&#12290;&#20294;&#30001;&#20110;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#65292;&#22312;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#25968;&#25454;&#26041;&#38754;&#65292;&#21307;&#30103;4.0&#20173;&#33853;&#21518;&#20110;&#20854;&#20182;&#24037;&#19994;4.0&#12290;&#27492;&#22806;&#65292;&#26426;&#26500;&#30340;&#22810;&#26679;&#21270;&#23384;&#20648;&#21644;&#35745;&#31639;&#33021;&#21147;&#38480;&#21046;&#20102;&#26426;&#26500;&#34701;&#21512;&#30456;&#21516;&#35757;&#32451;&#27169;&#22411;&#32467;&#26500;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23433;&#20840;&#30340;&#22810;&#26041;&#35745;&#31639;&#30340;&#38598;&#25104;&#32852;&#37030;&#23398;&#20064;&#65292;&#36890;&#36807;&#21306;&#22359;&#38142;&#25216;&#26415;&#20351;&#24322;&#26500;&#27169;&#22411;&#33021;&#22815;&#20174;&#21307;&#30103;&#26426;&#26500;&#30340;&#25968;&#25454;&#20013;&#21327;&#21516;&#23398;&#20064;&#32780;&#19981;&#20250;&#20405;&#29359;&#29992;&#25143;&#30340;&#38544;&#31169;&#12290;&#21306;&#22359;&#38142;&#30340;&#23646;&#24615;&#36824;&#20801;&#35768;&#21508;&#26041;&#22312;&#27809;&#26377;&#20013;&#24515;&#21270;&#26381;&#21153;&#22120;&#30340;&#24773;&#20917;&#19979;&#20139;&#21463;&#25968;&#25454;&#23436;&#25972;&#24615;&#32780;&#19981;&#38656;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advancement of Internet and Communication Technologies (ICTs) has led to the era of Industry 4.0. This shift is followed by healthcare industries creating the term Healthcare 4.0. In Healthcare 4.0, the use of IoT-enabled medical imaging devices for early disease detection has enabled medical practitioners to increase healthcare institutions' quality of service. However, Healthcare 4.0 is still lagging in Artificial Intelligence and big data compared to other Industry 4.0 due to data privacy concerns. In addition, institutions' diverse storage and computing capabilities restrict institutions from incorporating the same training model structure. This paper presents a secure multi-party computation-based ensemble federated learning with blockchain that enables heterogeneous models to collaboratively learn from healthcare institutions' data without violating users' privacy. Blockchain properties also allow the party to enjoy data integrity without trust in a centralized server while a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26435;&#37325;&#33707;&#27604;&#20044;&#26031;&#20998;&#25968;&#20316;&#20026;&#19968;&#20010;&#21442;&#25968;&#21270;&#30340;&#24402;&#22240;&#26694;&#26550;&#65292;&#21487;&#20197;&#28085;&#30422;&#24456;&#22810;&#19981;&#21516;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#21253;&#25324;&#29305;&#24449;&#20132;&#20114;&#65292;&#35299;&#20915;&#20102;&#26041;&#27861;&#32321;&#34893;&#21644;&#19981;&#21487;&#27604;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#30740;&#31350;&#26041;&#27861;&#30340;&#21521;&#37327;&#31354;&#38388;&#65292;&#25552;&#20379;&#20102;&#19968;&#20123;&#26032;&#26041;&#27861;&#21644;&#35299;&#37322;&#65292;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#20854;&#22810;&#21151;&#33021;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09204</link><description>&lt;p&gt;
&#26435;&#37325;&#33707;&#27604;&#20044;&#26031;&#20998;&#25968;&#65306;&#19968;&#20010;&#29305;&#24449;&#24402;&#22240;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
The Weighted M\"obius Score: A Unified Framework for Feature Attribution. (arXiv:2305.09204v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26435;&#37325;&#33707;&#27604;&#20044;&#26031;&#20998;&#25968;&#20316;&#20026;&#19968;&#20010;&#21442;&#25968;&#21270;&#30340;&#24402;&#22240;&#26694;&#26550;&#65292;&#21487;&#20197;&#28085;&#30422;&#24456;&#22810;&#19981;&#21516;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#21253;&#25324;&#29305;&#24449;&#20132;&#20114;&#65292;&#35299;&#20915;&#20102;&#26041;&#27861;&#32321;&#34893;&#21644;&#19981;&#21487;&#27604;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#30740;&#31350;&#26041;&#27861;&#30340;&#21521;&#37327;&#31354;&#38388;&#65292;&#25552;&#20379;&#20102;&#19968;&#20123;&#26032;&#26041;&#27861;&#21644;&#35299;&#37322;&#65292;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#20854;&#22810;&#21151;&#33021;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#24402;&#22240;&#26088;&#22312;&#36890;&#36807;&#35782;&#21035;&#27599;&#20010;&#29305;&#24449;&#23545;&#39044;&#27979;&#30340;&#24433;&#21709;&#26469;&#35299;&#37322;&#40657;&#30418;&#27169;&#22411;&#39044;&#27979;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#23558;&#29305;&#24449;&#24402;&#22240;&#25193;&#23637;&#21040;&#22810;&#20010;&#29305;&#24449;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#32479;&#19968;&#30340;&#26694;&#26550;&#23548;&#33268;&#26041;&#27861;&#30340;&#22823;&#37327;&#32321;&#34893;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#19981;&#33021;&#30452;&#25509;&#27604;&#36739;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21442;&#25968;&#21270;&#30340;&#24402;&#22240;&#26694;&#26550;&#8212;&#8212;&#26435;&#37325;&#33707;&#27604;&#20044;&#26031;&#20998;&#25968;&#65292;&#24182;&#26174;&#31034;&#20102;&#35768;&#22810;&#19981;&#21516;&#30340;&#38024;&#23545;&#21333;&#20010;&#29305;&#24449;&#21644;&#29305;&#24449;&#20132;&#20114;&#30340;&#24402;&#22240;&#26041;&#27861;&#26159;&#29305;&#20363;&#65292;&#36824;&#39564;&#35777;&#20102;&#19968;&#20123;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#30740;&#31350;&#24402;&#22240;&#26041;&#27861;&#30340;&#21521;&#37327;&#31354;&#38388;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21033;&#29992;&#26631;&#20934;&#32447;&#24615;&#20195;&#25968;&#24037;&#20855;&#65292;&#24182;&#22312;&#21508;&#20010;&#39046;&#22495;&#25552;&#20379;&#35299;&#37322;&#65292;&#21253;&#25324;&#21512;&#20316;&#21338;&#24328;&#29702;&#35770;&#21644;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#36825;&#20123;&#24402;&#22240;&#26041;&#27861;&#24212;&#29992;&#20110;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#29305;&#24449;&#20132;&#20114;&#26469;&#23454;&#35777;&#20102;&#26694;&#26550;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature attribution aims to explain the reasoning behind a black-box model's prediction by identifying the impact of each feature on the prediction. Recent work has extended feature attribution to interactions between multiple features. However, the lack of a unified framework has led to a proliferation of methods that are often not directly comparable. This paper introduces a parameterized attribution framework -- the Weighted M\"obius Score -- and (i) shows that many different attribution methods for both individual features and feature interactions are special cases and (ii) identifies some new methods. By studying the vector space of attribution methods, our framework utilizes standard linear algebra tools and provides interpretations in various fields, including cooperative game theory and causal mediation analysis. We empirically demonstrate the framework's versatility and effectiveness by applying these attribution methods to feature interactions in sentiment analysis and chain-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#36845;&#20195;&#20449;&#24565;&#20462;&#27491;&#20013;&#30340;&#19977;&#31181;&#29366;&#24577;&#34920;&#31034;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#29992;&#23383;&#20856;&#24207;&#20462;&#35746;&#30340;&#37325;&#20889;&#21382;&#21490;&#26159;&#26368;&#26377;&#25928;&#29575;&#30340;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;Horn&#20844;&#24335;&#26159;&#21542;&#31561;&#20215;&#20110;neg&#12290;</title><link>http://arxiv.org/abs/2305.09200</link><description>&lt;p&gt;
&#25105;&#20204;&#33021;&#21542;&#24536;&#35760;&#25105;&#20204;&#30340;&#23398;&#20064;&#26041;&#24335;&#65311;&#27604;&#36739;&#36845;&#20195;&#20449;&#24565;&#20462;&#27491;&#20013;&#30340;&#29366;&#24577;&#34920;&#31034;(arXiv:2305.09200v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
Can we forget how we learned? Representing states in iterated belief revision}. (arXiv:2305.09200v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#36845;&#20195;&#20449;&#24565;&#20462;&#27491;&#20013;&#30340;&#19977;&#31181;&#29366;&#24577;&#34920;&#31034;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#29992;&#23383;&#20856;&#24207;&#20462;&#35746;&#30340;&#37325;&#20889;&#21382;&#21490;&#26159;&#26368;&#26377;&#25928;&#29575;&#30340;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;Horn&#20844;&#24335;&#26159;&#21542;&#31561;&#20215;&#20110;neg&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#36845;&#20195;&#20449;&#24565;&#20462;&#27491;&#20013;&#19977;&#31181;&#26368;&#24120;&#35265;&#30340;&#29366;&#24577;&#34920;&#31034;&#26041;&#27861;&#65306;&#26174;&#24335;&#34920;&#31034;&#65292;&#25353;&#23618;&#27425;&#34920;&#31034;&#21644;&#25353;&#21382;&#21490;&#34920;&#31034;&#12290;&#21069;&#32773;&#26159;&#27169;&#22411;&#20043;&#38388;&#30340;&#36830;&#36890;&#20559;&#24207;&#20851;&#31995;&#65292;&#31532;&#20108;&#31181;&#26159;&#34920;&#31034;&#31561;&#20215;&#31867;&#30340;&#20844;&#24335;&#21015;&#34920;&#65292;&#31532;&#19977;&#31181;&#26159;&#20808;&#21069;&#20462;&#35746;&#30340;&#24207;&#21015;&#12290;&#21518;&#32773;&#21462;&#20915;&#20110;&#20462;&#35746;&#35821;&#20041;&#21644;&#21382;&#21490;&#37325;&#20889;&#65292;&#32780;&#21069;&#32773;&#21017;&#21462;&#20915;&#20110;&#20801;&#35768;&#30340;&#37325;&#20889;&#12290;&#25152;&#26377;&#26426;&#21046;&#37117;&#34920;&#31034;&#25152;&#26377;&#21487;&#33021;&#30340;&#29366;&#24577;&#12290;&#29992;&#23383;&#20856;&#24207;&#20462;&#35746;&#30340;&#37325;&#20889;&#21382;&#21490;&#22312;&#22823;&#23567;&#26041;&#38754;&#27604;&#20854;&#20182;&#32771;&#34385;&#30340;&#34920;&#31034;&#26041;&#27861;&#26356;&#26377;&#25928;&#29575;&#12290;&#35777;&#26126;&#20102;&#36825;&#26679;&#19968;&#20010;&#21382;&#21490;&#30340;&#20887;&#20313;&#26159;&#19968;&#31181;&#36731;&#24494;&#30340;&#37325;&#20889;&#12290;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#36825;&#26159;&#19968;&#20010;coNP&#23436;&#20840;&#38382;&#39064;&#65292;&#21363;&#20351;&#22312;Horn&#20844;&#24335;&#30340;&#20004;&#27425;&#20462;&#35746;&#21382;&#21490;&#25110;&#20219;&#24847;&#38271;&#24230;&#30340;&#20462;&#35746;&#21382;&#21490;&#19978;&#65292;&#36825;&#20063;&#26159;&#22256;&#38590;&#30340;&#65292;&#20294;&#22312;&#20004;&#20010;Horn&#20844;&#24335;&#30340;&#21382;&#21490;&#19978;&#65292;&#23427;&#26159;&#22810;&#39033;&#24335;&#30340;&#12290;&#19968;&#20010;&#27425;&#35201;&#30340;&#25216;&#26415;&#32467;&#26524;&#26159;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;&#19968;&#20010;Horn&#20844;&#24335;&#26159;&#21542;&#31561;&#20215;&#20110;neg&#12290;
&lt;/p&gt;
&lt;p&gt;
The three most common representations of states in iterated belief revision are compared: explicit, by levels and by history. The first is a connected preorder between models, the second is a list of formulae representing equivalence classes, the third is the sequence of the previous revisions. The latter depends on the revision semantics and on history rewriting, and the latter depends on the allowed rewritings. All mechanisms represent all possible states. A rewritten history of lexicographic revision is more efficient than the other considered representations in terms of size with arbitrary history rewritings. Establishing the redundancy of such a history is a mild rewriting. It is coNP-complete in the general case, and is hard even on histories of two revisions or revisions of arbitrary length of Horn formulae, and is polynomial on histories of two Horn formulae. A minor technical result is a polynomial-time algorithm for establishing whether a Horn formula is equivalent to the neg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;NODE&#65289;&#22312;&#38754;&#23545;&#22122;&#22768;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#26102;&#25152;&#34920;&#29616;&#20986;&#30340;&#33258;&#28982;&#40065;&#26834;&#24615;&#65292;&#24182;&#36890;&#36807;&#25511;&#21046;ODE&#21160;&#21147;&#23398;&#30340;Lipschitz&#24120;&#25968;&#26469;&#26174;&#33879;&#25552;&#39640;&#20854;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.09179</link><description>&lt;p&gt;
Ortho-ODE&#65306;&#22686;&#24378;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Ortho-ODE: Enhancing Robustness and of Neural ODEs against Adversarial Attacks. (arXiv:2305.09179v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09179
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;NODE&#65289;&#22312;&#38754;&#23545;&#22122;&#22768;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#26102;&#25152;&#34920;&#29616;&#20986;&#30340;&#33258;&#28982;&#40065;&#26834;&#24615;&#65292;&#24182;&#36890;&#36807;&#25511;&#21046;ODE&#21160;&#21147;&#23398;&#30340;Lipschitz&#24120;&#25968;&#26469;&#26174;&#33879;&#25552;&#39640;&#20854;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;NODE&#65289;&#36890;&#36807;&#20351;&#29992;&#25968;&#20540;&#27714;&#35299;&#22120;&#26469;&#27714;&#35299;&#30001;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#34920;&#31034;&#30340;&#24494;&#20998;&#26041;&#31243;&#65292;&#20174;&#32780;&#24341;&#21457;&#20102;&#19968;&#31181;&#20855;&#26377;&#26080;&#38480;&#28145;&#24230;&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#33539;&#24335;&#12290; NODE&#26088;&#22312;&#35299;&#20915;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;NODE&#23545;&#21508;&#31181;&#22122;&#22768;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#34920;&#29616;&#20986;&#20102;&#40065;&#26834;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;NODE&#30340;&#33258;&#28982;&#40065;&#26834;&#24615;&#24182;&#32771;&#23519;&#20854;&#20013;&#20196;&#20154;&#24778;&#35766;&#34892;&#20026;&#30340;&#21407;&#22240;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36890;&#36807;&#25511;&#21046;ODE&#21160;&#21147;&#23398;&#30340;Lipschitz&#24120;&#25968;&#65292;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#20854;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#20174;Grownwall&#19981;&#31561;&#24335;&#20013;&#25512;&#23548;&#20986;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32472;&#21046;&#25910;&#32553;&#29702;&#35770;&#21644;Grownwall&#19981;&#31561;&#24335;&#20043;&#38388;&#30340;&#31867;&#27604;&#12290;&#23454;&#39564;&#19978;&#65292;&#25105;&#20204;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#65288;MNIST&#12289;CIFAR-10&#21644;CIFAR 100&#65289;&#19978;&#35777;&#23454;&#20102;&#22686;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#33258;&#36866;&#24212;&#21644;&#38750;&#33258;&#36866;&#24212;&#27714;&#35299;&#22120;&#23545;NODE&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Ordinary Differential Equations (NODEs) probed the usage of numerical solvers to solve the differential equation characterized by a Neural Network (NN), therefore initiating a new paradigm of deep learning models with infinite depth. NODEs were designed to tackle the irregular time series problem. However, NODEs have demonstrated robustness against various noises and adversarial attacks. This paper is about the natural robustness of NODEs and examines the cause behind such surprising behaviour. We show that by controlling the Lipschitz constant of the ODE dynamics the robustness can be significantly improved. We derive our approach from Grownwall's inequality. Further, we draw parallels between contractivity theory and Grownwall's inequality. Experimentally we corroborate the enhanced robustness on numerous datasets - MNIST, CIFAR-10, and CIFAR 100. We also present the impact of adaptive and non-adaptive solvers on the robustness of NODEs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#25968;&#25454;&#38598;&#32479;&#19968;&#27867;&#21270;&#65288;SUG&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#32454;&#31890;&#24230;&#23376;&#22495;&#23545;&#40784;&#21644;&#26679;&#26412;&#32423;&#22495;&#24863;&#30693;&#27880;&#24847;&#21147;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#19977;&#32500;&#28857;&#20113;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.09160</link><description>&lt;p&gt;
SUG&#65306;&#21333;&#25968;&#25454;&#38598;&#32479;&#19968;&#27867;&#21270;&#29992;&#20110; 3D &#28857;&#20113;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
SUG: Single-dataset Unified Generalization for 3D Point Cloud Classification. (arXiv:2305.09160v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#25968;&#25454;&#38598;&#32479;&#19968;&#27867;&#21270;&#65288;SUG&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#32454;&#31890;&#24230;&#23376;&#22495;&#23545;&#40784;&#21644;&#26679;&#26412;&#32423;&#22495;&#24863;&#30693;&#27880;&#24847;&#21147;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#19977;&#32500;&#28857;&#20113;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#39046;&#22495;&#27867;&#21270;&#65288;DG&#65289;&#38382;&#39064;&#22312;&#20108;&#32500;&#22270;&#20687;&#20219;&#21153;&#20013;&#22686;&#38271;&#36805;&#36895;&#65292;&#20294;&#20854;&#22312;&#19977;&#32500;&#28857;&#20113;&#25968;&#25454;&#19978;&#30340;&#25506;&#32034;&#20173;&#28982;&#19981;&#36275;&#65292;&#24182;&#19988;&#21463;&#21040;&#26356;&#22797;&#26434;&#21644;&#19981;&#30830;&#23450;&#30340;&#36328;&#22495;&#24046;&#24322;&#20197;&#21450;&#19981;&#22343;&#21248;&#30340;&#36328;&#31867;&#27169;&#24577;&#20998;&#24067;&#30340;&#25361;&#25112;&#12290;&#19982;&#20043;&#21069;&#30340;&#20108;&#32500; DG &#24037;&#20316;&#19981;&#21516;&#65292;&#26412;&#25991;&#20851;&#27880;&#19977;&#32500; DG &#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181; Single-dataset Unified Generalization&#65288;SUG&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20165;&#21033;&#29992;&#21333;&#20010;&#28304;&#25968;&#25454;&#38598;&#26469;&#32531;&#35299;&#32463;&#36807;&#20805;&#20998;&#35757;&#32451;&#30340;&#28304;&#27169;&#22411;&#38754;&#20020;&#30340;&#26410;&#30693;&#22495;&#24046;&#24322;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#31181;&#22810;&#32454;&#31890;&#24230;&#23376;&#22495;&#23545;&#40784;&#65288;MSA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#26469;&#33258;&#21333;&#20010;&#28304;&#25968;&#25454;&#38598;&#30340;&#20998;&#35010;&#23376;&#22495;&#20043;&#38388;&#25191;&#34892;&#22810;&#32454;&#31890;&#24230;&#29305;&#24449;&#23545;&#40784;&#36807;&#31243;&#26469;&#32422;&#26463;&#23398;&#20064;&#34920;&#31034;&#20026;&#22495;&#19981;&#21487;&#30693;&#21644;&#26377;&#21306;&#21035;&#24615;&#12290;&#28982;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26679;&#26412;&#32423;&#22495;&#24863;&#30693;&#27880;&#24847;&#21147;&#65288;SDA&#65289;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#21487;&#20197;&#26681;&#25454;&#27599;&#20010;&#26679;&#26412;&#25152;&#23646;&#30340;&#19981;&#21516;&#23376;&#22495;&#36873;&#25321;&#24615;&#22320;&#22686;&#24378;&#26131;&#20110;&#36866;&#24212;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although Domain Generalization (DG) problem has been fast-growing in the 2D image tasks, its exploration on 3D point cloud data is still insufficient and challenged by more complex and uncertain cross-domain variances with uneven inter-class modality distribution. In this paper, different from previous 2D DG works, we focus on the 3D DG problem and propose a Single-dataset Unified Generalization (SUG) framework that only leverages a single source dataset to alleviate the unforeseen domain differences faced by a well-trained source model. Specifically, we first design a Multi-grained Sub-domain Alignment (MSA) method, which can constrain the learned representations to be domain-agnostic and discriminative, by performing a multi-grained feature alignment process between the splitted sub-domains from the single source dataset. Then, a Sample-level Domain-aware Attention (SDA) strategy is presented, which can selectively enhance easy-to-adapt samples from different sub-domains according to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#32467;&#21512;&#21487;&#35757;&#32451;&#21160;&#24577;&#33021;&#37327;&#24863;&#30693;&#21644;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#21487;&#35745;&#31639;&#22270;&#20687;&#27169;&#22411;&#65292;&#26088;&#22312;&#25429;&#25417;&#29983;&#29289;&#35270;&#35273;&#31995;&#32479;&#20013;&#36816;&#21160;&#24863;&#30693;&#30340;&#26680;&#24515;&#32467;&#26500;&#35745;&#31639;&#36807;&#31243;&#65292;&#22635;&#34917;&#20102;&#20174;&#33258;&#28982;&#22330;&#26223;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#21487;&#35745;&#31639;&#22270;&#20687;&#27169;&#22411;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2305.09156</link><description>&lt;p&gt;
&#21487;&#35757;&#32451;&#21160;&#24577;&#33021;&#37327;&#24863;&#30693;&#21644;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#20154;&#31867;&#35270;&#35273;&#21160;&#24577;&#22788;&#29702;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Modelling Human Visual Motion Processing with Trainable Motion Energy Sensing and a Self-attention Network for Adaptive Motion Integration. (arXiv:2305.09156v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09156
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#32467;&#21512;&#21487;&#35757;&#32451;&#21160;&#24577;&#33021;&#37327;&#24863;&#30693;&#21644;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#21487;&#35745;&#31639;&#22270;&#20687;&#27169;&#22411;&#65292;&#26088;&#22312;&#25429;&#25417;&#29983;&#29289;&#35270;&#35273;&#31995;&#32479;&#20013;&#36816;&#21160;&#24863;&#30693;&#30340;&#26680;&#24515;&#32467;&#26500;&#35745;&#31639;&#36807;&#31243;&#65292;&#22635;&#34917;&#20102;&#20174;&#33258;&#28982;&#22330;&#26223;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#21487;&#35745;&#31639;&#22270;&#20687;&#27169;&#22411;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#21160;&#24577;&#22788;&#29702;&#23545;&#20110;&#29983;&#29289;&#24863;&#30693;&#21644;&#19982;&#21160;&#24577;&#29615;&#22659;&#20132;&#20114;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#22312;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#39046;&#22495;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#65292;&#20294;&#23578;&#26410;&#24314;&#31435;&#36215;&#33021;&#22815;&#20197;&#19968;&#31181;&#19982;&#20154;&#31867;&#35270;&#35273;&#22788;&#29702;&#19968;&#33268;&#30340;&#26041;&#24335;&#20174;&#33258;&#28982;&#22330;&#26223;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#21487;&#35745;&#31639;&#22270;&#20687;&#27169;&#22411;&#12290;&#21516;&#26102;&#65292;&#30001;&#28145;&#24230;&#23398;&#20064;&#25512;&#21160;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#26368;&#36817;&#36827;&#23637;&#22312;&#20809;&#27969;&#20272;&#35745;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;, &#36825;&#26159;&#19982;&#36816;&#21160;&#24863;&#30693;&#23494;&#20999;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#21512;&#20154;&#31867;&#35270;&#35273;&#27169;&#22411;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#21487;&#35745;&#31639;&#22270;&#20687;&#27169;&#22411;&#65292;&#26088;&#22312;&#25429;&#25417;&#29983;&#29289;&#35270;&#35273;&#31995;&#32479;&#20013;&#36816;&#21160;&#24863;&#30693;&#30340;&#26680;&#24515;&#32467;&#26500;V1-MT&#20013;&#30340;&#35745;&#31639;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual motion processing is essential for organisms to perceive and interact with dynamic environments. Despite extensive research in cognitive neuroscience, image-computable models that can extract informative motion flow from natural scenes in a manner consistent with human visual processing have yet to be established. Meanwhile, recent advancements in computer vision (CV), propelled by deep learning, have led to significant progress in optical flow estimation, a task closely related to motion perception. Here we propose an image-computable model of human motion perception by bridging the gap between human and CV models. Specifically, we introduce a novel two-stage approach that combines trainable motion energy sensing with a recurrent self-attention network for adaptive motion integration and segregation. This model architecture aims to capture the computations in V1-MT, the core structure for motion perception in the biological visual system. In silico neurophysiology reveals that 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#37325;&#23545;&#40784;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#36328;&#35821;&#35328;&#21477;&#23376;&#23884;&#20837;&#65292;&#23427;&#32467;&#21512;&#20102;&#21477;&#23376;&#32423;&#21035;&#21644;&#26631;&#35760;&#32423;&#21035;&#30340;&#23545;&#40784;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#34920;&#31034;&#32763;&#35793;&#23398;&#20064;&#20219;&#21153;&#65292;&#20174;&#32780;&#23558;&#32763;&#35793;&#20449;&#24687;&#23884;&#20837;&#26631;&#35760;&#34920;&#31034;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.09148</link><description>&lt;p&gt;
&#21452;&#37325;&#23545;&#40784;&#39044;&#35757;&#32451;&#29992;&#20110;&#36328;&#35821;&#35328;&#21477;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Dual-Alignment Pre-training for Cross-lingual Sentence Embedding. (arXiv:2305.09148v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09148
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#37325;&#23545;&#40784;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#36328;&#35821;&#35328;&#21477;&#23376;&#23884;&#20837;&#65292;&#23427;&#32467;&#21512;&#20102;&#21477;&#23376;&#32423;&#21035;&#21644;&#26631;&#35760;&#32423;&#21035;&#30340;&#23545;&#40784;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#34920;&#31034;&#32763;&#35793;&#23398;&#20064;&#20219;&#21153;&#65292;&#20174;&#32780;&#23558;&#32763;&#35793;&#20449;&#24687;&#23884;&#20837;&#26631;&#35760;&#34920;&#31034;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#21477;&#23376;&#32423;&#21035;&#32763;&#35793;&#25490;&#21517;&#20219;&#21153;&#35757;&#32451;&#30340;&#21452;&#32534;&#30721;&#22120;&#27169;&#22411;&#26159;&#36328;&#35821;&#35328;&#21477;&#23376;&#23884;&#20837;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#22810;&#35821;&#35328;&#22330;&#26223;&#20013;&#65292;&#26631;&#35760;&#32423;&#21035;&#30340;&#23545;&#40784;&#20063;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20294;&#27492;&#21069;&#23578;&#26410;&#23436;&#20840;&#25506;&#32034;&#36825;&#19968;&#38382;&#39064;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#37325;&#23545;&#40784;&#39044;&#35757;&#32451;&#65288;DAP&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#36328;&#35821;&#35328;&#21477;&#23376;&#23884;&#20837;&#65292;&#23427;&#32467;&#21512;&#20102;&#21477;&#23376;&#32423;&#21035;&#21644;&#26631;&#35760;&#32423;&#21035;&#30340;&#23545;&#40784;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#34920;&#31034;&#32763;&#35793;&#23398;&#20064;&#65288;RTL&#65289;&#20219;&#21153;&#65292;&#20854;&#20013;&#27169;&#22411;&#23398;&#20064;&#20351;&#29992;&#21333;&#20391;&#19978;&#19979;&#25991;&#21270;&#30340;&#26631;&#35760;&#34920;&#31034;&#26469;&#37325;&#24314;&#20854;&#32763;&#35793;&#23545;&#24212;&#29289;&#12290;&#36825;&#31181;&#37325;&#24314;&#30446;&#26631;&#40723;&#21169;&#27169;&#22411;&#23558;&#32763;&#35793;&#20449;&#24687;&#23884;&#20837;&#26631;&#35760;&#34920;&#31034;&#20013;&#12290;&#19982;&#20854;&#20182;&#26631;&#35760;&#32423;&#21035;&#23545;&#40784;&#26041;&#27861;&#65288;&#22914;&#32763;&#35793;&#35821;&#35328;&#27169;&#22411;&#65289;&#30456;&#27604;&#65292;RTL&#26356;&#36866;&#21512;&#21452;&#32534;&#30721;&#22120;&#20307;&#31995;&#32467;&#26500;&#65292;&#32780;&#19988;&#35745;&#31639;&#25928;&#29575;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have shown that dual encoder models trained with the sentence-level translation ranking task are effective methods for cross-lingual sentence embedding. However, our research indicates that token-level alignment is also crucial in multilingual scenarios, which has not been fully explored previously. Based on our findings, we propose a dual-alignment pre-training (DAP) framework for cross-lingual sentence embedding that incorporates both sentence-level and token-level alignment. To achieve this, we introduce a novel representation translation learning (RTL) task, where the model learns to use one-side contextualized token representation to reconstruct its translation counterpart. This reconstruction objective encourages the model to embed translation information into the token representation. Compared to other token-level alignment methods such as translation language modeling, RTL is more suitable for dual encoder architectures and is computationally efficient. Extensive
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35745;&#31639;&#21644;&#20998;&#26512;ReLU&#32593;&#32476;&#22810;&#38754;&#20307;&#30340;&#21333;&#32431;&#24418;&#30452;&#26041;&#22270;&#65292;&#21457;&#29616;&#22312;&#21021;&#22987;&#21270;&#21644;&#26799;&#24230;&#19979;&#38477;&#26102;&#23427;&#20204;&#32467;&#26500;&#30456;&#23545;&#31616;&#21333;&#65292;&#36825;&#35828;&#26126;&#20102;&#19968;&#31181;&#26032;&#30340;&#38544;&#24335;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2305.09145</link><description>&lt;p&gt;
&#28145;&#23618;ReLU&#32593;&#32476;&#30340;&#22810;&#38754;&#20307;&#24322;&#24120;&#31616;&#21333;
&lt;/p&gt;
&lt;p&gt;
Deep ReLU Networks Have Surprisingly Simple Polytopes. (arXiv:2305.09145v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09145
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35745;&#31639;&#21644;&#20998;&#26512;ReLU&#32593;&#32476;&#22810;&#38754;&#20307;&#30340;&#21333;&#32431;&#24418;&#30452;&#26041;&#22270;&#65292;&#21457;&#29616;&#22312;&#21021;&#22987;&#21270;&#21644;&#26799;&#24230;&#19979;&#38477;&#26102;&#23427;&#20204;&#32467;&#26500;&#30456;&#23545;&#31616;&#21333;&#65292;&#36825;&#35828;&#26126;&#20102;&#19968;&#31181;&#26032;&#30340;&#38544;&#24335;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ReLU&#32593;&#32476;&#26159;&#19968;&#31181;&#22810;&#38754;&#20307;&#19978;&#30340;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;&#12290;&#30740;&#31350;&#36825;&#31181;&#22810;&#38754;&#20307;&#30340;&#24615;&#36136;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#30446;&#21069;&#65292;&#23545;&#20110;&#22810;&#38754;&#20307;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#20165;&#20572;&#30041;&#22312;&#35745;&#31639;&#25968;&#37327;&#30340;&#27700;&#24179;&#65292;&#36825;&#36828;&#36828;&#19981;&#33021;&#23436;&#25972;&#22320;&#25551;&#36848;&#22810;&#38754;&#20307;&#12290;&#20026;&#20102;&#23558;&#29305;&#24449;&#25552;&#21319;&#21040;&#19968;&#20010;&#26032;&#30340;&#27700;&#24179;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#19977;&#35282;&#21078;&#20998;&#22810;&#38754;&#20307;&#24471;&#20986;&#22810;&#38754;&#20307;&#30340;&#24418;&#29366;&#12290;&#36890;&#36807;&#35745;&#31639;&#21644;&#20998;&#26512;&#19981;&#21516;&#22810;&#38754;&#20307;&#30340;&#21333;&#32431;&#24418;&#30452;&#26041;&#22270;&#65292;&#25105;&#20204;&#21457;&#29616;ReLU&#32593;&#32476;&#22312;&#21021;&#22987;&#21270;&#21644;&#26799;&#24230;&#19979;&#38477;&#26102;&#20855;&#26377;&#30456;&#23545;&#31616;&#21333;&#30340;&#22810;&#38754;&#20307;&#32467;&#26500;&#65292;&#23613;&#31649;&#36825;&#20123;&#22810;&#38754;&#20307;&#20174;&#29702;&#35770;&#19978;&#26469;&#35828;&#21487;&#20197;&#38750;&#24120;&#20016;&#23500;&#21644;&#22797;&#26434;&#12290;&#36825;&#19968;&#21457;&#29616;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#26032;&#30340;&#38544;&#24335;&#20559;&#35265;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#38750;&#24179;&#20961;&#30340;&#32452;&#21512;&#25512;&#23548;&#26469;&#29702;&#35770;&#19978;&#35299;&#37322;&#20026;&#20160;&#20040;&#22686;&#21152;&#28145;&#24230;&#19981;&#20250;&#21019;&#24314;&#26356;&#22797;&#26434;&#30340;&#22810;&#38754;&#20307;&#65292;&#36890;&#36807;&#38480;&#21046;&#27599;&#20010;&#32500;&#24230;&#30340;&#24179;&#22343;&#21333;&#32431;&#24418;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
A ReLU network is a piecewise linear function over polytopes. Figuring out the properties of such polytopes is of fundamental importance for the research and development of neural networks. So far, either theoretical or empirical studies on polytopes only stay at the level of counting their number, which is far from a complete characterization of polytopes. To upgrade the characterization to a new level, here we propose to study the shapes of polytopes via the number of simplices obtained by triangulating the polytope. Then, by computing and analyzing the histogram of simplices across polytopes, we find that a ReLU network has relatively simple polytopes under both initialization and gradient descent, although these polytopes theoretically can be rather diverse and complicated. This finding can be appreciated as a novel implicit bias. Next, we use nontrivial combinatorial derivation to theoretically explain why adding depth does not create a more complicated polytope by bounding the av
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#35760;&#24518;&#26426;&#21046;&#65292;&#21457;&#29616;&#39044;&#35757;&#32451;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#27169;&#22411;&#30340;&#35760;&#24518;&#33021;&#21147;&#65292;&#32780;&#30693;&#35782;&#30456;&#20851;&#24615;&#21644;&#22810;&#26679;&#24615;&#23545;&#20110;&#35760;&#24518;&#24418;&#25104;&#20063;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.09144</link><description>&lt;p&gt;
&#35760;&#24518;&#36824;&#26159;&#24536;&#21364;&#65311;&#28145;&#20837;&#25506;&#35752;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#35760;&#24518;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Retentive or Forgetful? Diving into the Knowledge Memorizing Mechanism of Language Models. (arXiv:2305.09144v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09144
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#35760;&#24518;&#26426;&#21046;&#65292;&#21457;&#29616;&#39044;&#35757;&#32451;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#27169;&#22411;&#30340;&#35760;&#24518;&#33021;&#21147;&#65292;&#32780;&#30693;&#35782;&#30456;&#20851;&#24615;&#21644;&#22810;&#26679;&#24615;&#23545;&#20110;&#35760;&#24518;&#24418;&#25104;&#20063;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35760;&#24518;&#26159;&#26368;&#22522;&#26412;&#30340;&#35748;&#30693;&#21151;&#33021;&#20043;&#19968;&#65292;&#26159;&#23384;&#20648;&#19990;&#30028;&#30693;&#35782;&#21644;&#27963;&#21160;&#32463;&#21382;&#30340;&#20648;&#34255;&#24211;&#12290;&#36817;&#24180;&#26469;&#65292;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#35760;&#24518;&#33021;&#21147;&#12290;&#30456;&#21453;&#65292;&#27809;&#26377;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#23384;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#31181;&#20445;&#25345;-&#36951;&#24536;&#30340;&#30683;&#30462;&#24182;&#20102;&#35299;&#35821;&#35328;&#27169;&#22411;&#30340;&#35760;&#24518;&#26426;&#21046;&#65292;&#25105;&#20204;&#36890;&#36807;&#25511;&#21046;&#30446;&#26631;&#30693;&#35782;&#31867;&#22411;&#12289;&#23398;&#20064;&#31574;&#30053;&#21644;&#23398;&#20064;&#26102;&#38388;&#34920;&#31561;&#65292;&#24320;&#23637;&#20102;&#28145;&#20837;&#30340;&#23454;&#39564;&#30740;&#31350;&#12290;&#32467;&#26524;&#21457;&#29616;&#65306;1&#65289;&#20256;&#32479;&#35821;&#35328;&#27169;&#22411;&#26159;&#23481;&#26131;&#36951;&#24536;&#30340;&#65307;2&#65289;&#39044;&#35757;&#32451;&#21487;&#20197;&#20351;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#35760;&#24518;&#33021;&#21147;&#65307;3&#65289;&#30693;&#35782;&#30456;&#20851;&#24615;&#21644;&#22810;&#26679;&#24615;&#26174;&#33879;&#24433;&#21709;&#35760;&#24518;&#24418;&#25104;&#12290;&#36825;&#20123;&#32467;&#35770;&#26377;&#21161;&#20110;&#29702;&#35299;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#24182;&#20026;&#35774;&#35745;&#21644;&#35780;&#20272;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#26041;&#27861;&#21644;&#25512;&#29702;&#31639;&#27861;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Memory is one of the most essential cognitive functions serving as a repository of world knowledge and episodes of activities. In recent years, large-scale pre-trained language models have shown remarkable memorizing ability. On the contrary, vanilla neural networks without pre-training have been long observed suffering from the catastrophic forgetting problem. To investigate such a retentive-forgetful contradiction and understand the memory mechanism of language models, we conduct thorough experiments by controlling the target knowledge types, the learning strategies and the learning schedules. We find that: 1) Vanilla language models are forgetful; 2) Pre-training leads to retentive language models; 3) Knowledge relevance and diversification significantly influence the memory formation. These conclusions are useful for understanding the abilities of pre-trained language models and shed light on designing and evaluating new learning and inference algorithms of language models.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26041;&#27861;&#26469;&#23547;&#25214;&#29468;&#35789;&#28216;&#25103;&#30340;&#26368;&#20248;&#31574;&#30053;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65288;&#24555;&#36895;&#25214;&#21040;&#26368;&#20248;&#35299;&#65289;&#65292;&#20197;Wordle&#29468;&#35789;&#28216;&#25103;&#20026;&#20363;&#20855;&#20307;&#35828;&#26126;&#12290;</title><link>http://arxiv.org/abs/2305.09111</link><description>&lt;p&gt;
&#20851;&#20110;Wordle&#21644;&#19968;&#33324;&#29468;&#35789;&#28216;&#25103;&#20013;&#30340;&#26368;&#20248;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
On Optimal Strategies for Wordle and General Guessing Games. (arXiv:2305.09111v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09111
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26041;&#27861;&#26469;&#23547;&#25214;&#29468;&#35789;&#28216;&#25103;&#30340;&#26368;&#20248;&#31574;&#30053;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65288;&#24555;&#36895;&#25214;&#21040;&#26368;&#20248;&#35299;&#65289;&#65292;&#20197;Wordle&#29468;&#35789;&#28216;&#25103;&#20026;&#20363;&#20855;&#20307;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;Wordle&#28216;&#25103;&#30340;&#28779;&#29190;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#29468;&#35789;&#28216;&#25103;&#30340;&#20852;&#36259;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23547;&#25214;&#29468;&#35789;&#28216;&#25103;&#26368;&#20248;&#31574;&#30053;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#26522;&#20030;&#25628;&#32034;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#20960;&#20010;&#23450;&#29702;&#65292;&#26500;&#24314;&#19968;&#20010;&#35777;&#26126;&#29468;&#35789;&#28216;&#25103;&#31574;&#30053;&#26368;&#20248;&#24615;&#30340;&#36890;&#29992;&#29702;&#35770;&#12290;&#36825;&#39033;&#24037;&#20316;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#24847;&#29468;&#35789;&#28216;&#25103;&#65292;&#25105;&#20204;&#20197;Wordle&#28216;&#25103;&#20316;&#20026;&#20363;&#23376;&#23637;&#31034;&#20855;&#20307;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent popularity of Wordle has revived interest in guessing games. We develop a general method for finding optimal strategies for guessing games while avoiding an exhaustive search. Our main contributions are several theorems that build towards a general theory to prove the optimality of a strategy for a guessing game. This work is developed to apply to any guessing game, but we use Wordle as an example to present concrete results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;Transformer-based Video Question Answering&#26041;&#27861;&#65292;&#21363;&#23558;&#35270;&#39057;&#24103;&#36830;&#25509;&#25104; $n\times n$ &#30340;&#30697;&#38453;&#65292;&#20174;&#32780;&#23558;&#22270;&#20687;&#32534;&#30721;&#22120;&#30340;&#20351;&#29992;&#37327;&#20174; $n^{2}$ &#20943;&#23569;&#21040;1&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#35757;&#32451;&#21644;&#25512;&#29702;&#36895;&#24230;&#21644;&#33410;&#30465;&#20102;&#23384;&#20648;&#31354;&#38388;&#65292;&#32780;&#20173;&#28982;&#20445;&#25345;&#20102;&#21407;&#22987;&#35270;&#39057;&#30340;&#26102;&#38388;&#32467;&#26500;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.09107</link><description>&lt;p&gt;
&#35270;&#39057;&#20540;&#24471; $n\times n$ &#24352;&#22270;&#20687;&#21527;? &#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#35270;&#39057;&#38382;&#31572;&#39640;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Is a Video worth $n\times n$ Images? A Highly Efficient Approach to Transformer-based Video Question Answering. (arXiv:2305.09107v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;Transformer-based Video Question Answering&#26041;&#27861;&#65292;&#21363;&#23558;&#35270;&#39057;&#24103;&#36830;&#25509;&#25104; $n\times n$ &#30340;&#30697;&#38453;&#65292;&#20174;&#32780;&#23558;&#22270;&#20687;&#32534;&#30721;&#22120;&#30340;&#20351;&#29992;&#37327;&#20174; $n^{2}$ &#20943;&#23569;&#21040;1&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#35757;&#32451;&#21644;&#25512;&#29702;&#36895;&#24230;&#21644;&#33410;&#30465;&#20102;&#23384;&#20648;&#31354;&#38388;&#65292;&#32780;&#20173;&#28982;&#20445;&#25345;&#20102;&#21407;&#22987;&#35270;&#39057;&#30340;&#26102;&#38388;&#32467;&#26500;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22522;&#20110;Transformer&#30340;&#35270;&#39057;&#38382;&#31572;&#65288;VideoQA&#65289;&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#19968;&#20010;&#25110;&#22810;&#20010;&#22270;&#20687;&#32534;&#30721;&#22120;&#29420;&#31435;&#32534;&#30721;&#24103;&#65292;&#24182;&#22312;&#24103;&#21644;&#38382;&#39064;&#20043;&#38388;&#36827;&#34892;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#27169;&#24335;&#20250;&#23548;&#33268;&#26174;&#33879;&#30340;&#20869;&#23384;&#20351;&#29992;&#21644;&#35757;&#32451;&#21644;&#25512;&#29702;&#36895;&#24230;&#30340;&#19981;&#21487;&#36991;&#20813;&#30340;&#20943;&#24930;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29616;&#26377;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#39640;&#25928;VideoQA&#26041;&#27861;&#65292;&#20854;&#20013;&#25105;&#20204;&#23558;&#35270;&#39057;&#24103;&#36830;&#25509;&#21040;&#19968;&#20010; $n\times n$ &#30697;&#38453;&#20013;&#65292;&#28982;&#21518;&#23558;&#20854;&#36716;&#25442;&#20026;&#19968;&#24352;&#22270;&#20687;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#23558;&#22270;&#20687;&#32534;&#30721;&#22120;&#30340;&#20351;&#29992;&#20174; $n^{2}$&#20943;&#23569;&#21040;1&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#21407;&#22987;&#35270;&#39057;&#30340;&#26102;&#38388;&#32467;&#26500;&#12290;&#22312;MSRVTT&#21644;TrafficQA&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20197;&#36817; $4\times$ &#26356;&#24555;&#30340;&#36895;&#24230;&#21644;&#21482;&#26377;30&#65285;&#30340;&#20869;&#23384;&#20351;&#29992;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#38598;&#25104;&#21040;VideoQA&#31995;&#32479;&#20013;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#21482;&#26377;&#24456;&#23567;&#20195;&#20215;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#21487;&#27604;&#29978;&#33267;&#20248;&#24322;&#30340;&#34920;&#29616;&#65292;&#21516;&#26102;&#35757;&#32451;&#21644;&#25512;&#29702;&#36895;&#24230;&#26174;&#33879;&#21152;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional Transformer-based Video Question Answering (VideoQA) approaches generally encode frames independently through one or more image encoders followed by interaction between frames and question. However, such schema would incur significant memory use and inevitably slow down the training and inference speed. In this work, we present a highly efficient approach for VideoQA based on existing vision-language pre-trained models where we concatenate video frames to a $n\times n$ matrix and then convert it to one image. By doing so, we reduce the use of the image encoder from $n^{2}$ to $1$ while maintaining the temporal structure of the original video. Experimental results on MSRVTT and TrafficQA show that our proposed approach achieves state-of-the-art performance with nearly $4\times$ faster speed and only 30% memory use. We show that by integrating our approach into VideoQA systems we can achieve comparable, even superior, performance with a significant speed up for training and 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35748;&#30693;&#36890;&#29992;&#27169;&#22411;&#30340; System-1 &#21644; System-2 &#23454;&#29616;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20854;&#21306;&#20998;&#19981;&#28165;&#30340;&#38382;&#39064;&#12290;&#36890;&#29992;&#27169;&#22411;&#25552;&#20379;&#20102;&#23545;&#28041;&#21450; System-1 &#21644; System-2 &#30340;&#35745;&#31639;&#21333;&#20803;&#12289;&#22522;&#30784;&#26426;&#21046;&#21644;&#20854;&#23545;&#23398;&#20064;&#12289;&#20803;&#35748;&#30693;&#21644;&#24773;&#24863;&#30340;&#24433;&#21709;&#30340;&#20840;&#38754;&#35270;&#37326;&#12290;</title><link>http://arxiv.org/abs/2305.09091</link><description>&lt;p&gt;
AAAI 2022 &#31179;&#23395;&#30740;&#35752;&#20250;&#65306;&#22522;&#20110;&#35748;&#30693;&#36890;&#29992;&#27169;&#22411;&#23454;&#29616;&#30340; System-1 &#21644; System-2
&lt;/p&gt;
&lt;p&gt;
AAAI 2022 Fall Symposium: System-1 and System-2 realized within the Common Model of Cognition. (arXiv:2305.09091v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09091
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35748;&#30693;&#36890;&#29992;&#27169;&#22411;&#30340; System-1 &#21644; System-2 &#23454;&#29616;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20854;&#21306;&#20998;&#19981;&#28165;&#30340;&#38382;&#39064;&#12290;&#36890;&#29992;&#27169;&#22411;&#25552;&#20379;&#20102;&#23545;&#28041;&#21450; System-1 &#21644; System-2 &#30340;&#35745;&#31639;&#21333;&#20803;&#12289;&#22522;&#30784;&#26426;&#21046;&#21644;&#20854;&#23545;&#23398;&#20064;&#12289;&#20803;&#35748;&#30693;&#21644;&#24773;&#24863;&#30340;&#24433;&#21709;&#30340;&#20840;&#38754;&#35270;&#37326;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI &#39046;&#22495;&#20013;&#23545; System-1 &#21644; System-2 &#20108;&#20803;&#27169;&#22411;&#30340;&#24341;&#20837;&#19968;&#30452;&#21463;&#21040;&#20854;&#21306;&#20998;&#19981;&#28165;&#30340;&#38459;&#30861;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558; System-1 &#21644; System-2 &#32622;&#20110;&#35748;&#30693;&#36890;&#29992;&#27169;&#22411;&#20013;&#26469;&#35299;&#20915;&#27492;&#38382;&#39064;&#21644;&#20854;&#20182;&#38382;&#39064;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#34987;&#35748;&#20026;&#26159; System-1 &#21644; 2 &#29305;&#24449;&#30340;&#19981;&#21516;&#20043;&#22788;&#23454;&#38469;&#19978;&#24418;&#25104;&#20102;&#19968;&#31181;&#35748;&#30693;&#23646;&#24615;&#30340;&#33539;&#22260;&#12290;&#36890;&#29992;&#27169;&#22411;&#25552;&#20379;&#20102;&#26377;&#20851;&#28041;&#21450; System-1 &#21644; System-2 &#30340;&#35745;&#31639;&#21333;&#20803;&#12289;&#20854;&#22522;&#30784;&#26426;&#21046;&#20197;&#21450;&#23545;&#23398;&#20064;&#12289;&#20803;&#35748;&#30693;&#21644;&#24773;&#24863;&#30340;&#24433;&#21709;&#30340;&#20840;&#38754;&#35270;&#37326;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attempts to import dual-system descriptions of System-1 and System-2 into AI have been hindered by a lack of clarity over their distinction. We address this and other issues by situating System-1 and System-2 within the Common Model of Cognition. Results show that what are thought to be distinctive characteristics of System-1 and 2 instead form a spectrum of cognitive properties. The Common Model provides a comprehensive vision of the computational units involved in System-1 and System-2, their underlying mechanisms, and the implications for learning, metacognition, and emotion.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#22522;&#20110;IRT&#30340;&#27169;&#22411;&#26469;&#20998;&#26512;&#20154;&#31867;&#23545;&#20110;AI&#30340;&#24863;&#30693;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20154;&#20204;&#26222;&#36941;&#26399;&#26395;AI&#22242;&#38431;&#20249;&#20276;&#30340;&#34920;&#29616;&#20248;&#20110;&#20154;&#31867;&#65292;&#24182;&#19988;&#20154;&#20204;&#23545;AI&#19982;&#20154;&#31867;&#25104;&#21592;&#30340;&#24515;&#29702;&#27169;&#22411;&#19981;&#21516;&#12290;</title><link>http://arxiv.org/abs/2305.09064</link><description>&lt;p&gt;
&#25429;&#25417;&#20154;&#31867;&#23545;AI&#30340;&#24515;&#29702;&#27169;&#22411;&#65306;&#22522;&#20110;IRT&#26041;&#27861;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Capturing Humans' Mental Models of AI: An Item Response Theory Approach. (arXiv:2305.09064v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09064
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#22522;&#20110;IRT&#30340;&#27169;&#22411;&#26469;&#20998;&#26512;&#20154;&#31867;&#23545;&#20110;AI&#30340;&#24863;&#30693;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20154;&#20204;&#26222;&#36941;&#26399;&#26395;AI&#22242;&#38431;&#20249;&#20276;&#30340;&#34920;&#29616;&#20248;&#20110;&#20154;&#31867;&#65292;&#24182;&#19988;&#20154;&#20204;&#23545;AI&#19982;&#20154;&#31867;&#25104;&#21592;&#30340;&#24515;&#29702;&#27169;&#22411;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#20837;&#20102;&#35299;&#20154;&#31867;&#22914;&#20309;&#24863;&#30693;&#19982;AI&#25645;&#26723;&#30340;&#20449;&#24687;&#65292;&#26159;&#25105;&#20204;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#21512;&#20316;&#30340;&#37325;&#35201;&#22522;&#30784;&#12290;&#25105;&#20204;&#20511;&#37492;&#35748;&#30693;&#31185;&#23398;&#30340;&#30456;&#20851;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;IRT&#30340;&#26694;&#26550;&#26469;&#27169;&#25311;&#20154;&#20204;&#30340;&#24863;&#30693;&#65292;&#38024;&#23545;&#20154;&#19982;&#20154;&#12289;&#20154;&#19982;AI&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#37325;&#28857;&#30740;&#31350;&#20004;&#32773;&#24863;&#30693;&#20043;&#38388;&#30340;&#24322;&#21516;&#65292;&#20197;&#21450;&#20154;&#20204;&#23545;AI&#22242;&#38431;&#20249;&#20276;&#30340;&#26399;&#26395;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#20204;&#26222;&#36941;&#26399;&#26395;&#20154;&#24037;&#26234;&#33021;&#30340;&#34920;&#29616;&#20248;&#20110;&#20154;&#31867;&#65292;&#32780;&#19988;&#23545;AI&#25104;&#21592;&#30340;&#24515;&#29702;&#27169;&#22411;&#19982;&#20154;&#31867;&#25104;&#21592;&#30340;&#19981;&#21516;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#23398;&#20064;&#12289;&#21453;&#39304;&#21644;&#20010;&#20307;&#24046;&#24322;&#31561;&#22240;&#32032;&#21516;&#26679;&#26377;&#21487;&#33021;&#24433;&#21709;&#20154;&#20204;&#30340;&#24515;&#29702;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#25429;&#25417;&#21644;&#20998;&#26512;&#20154;&#31867;&#23545;&#19981;&#21516;&#24773;&#22659;&#20013;&#30340;AI&#24863;&#30693;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#29992;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Improving our understanding of how humans perceive AI teammates is an important foundation for our general understanding of human-AI teams. Extending relevant work from cognitive science, we propose a framework based on item response theory for modeling these perceptions. We apply this framework to real-world experiments, in which each participant works alongside another person or an AI agent in a question-answering setting, repeatedly assessing their teammate's performance. Using this experimental data, we demonstrate the use of our framework for testing research questions about people's perceptions of both AI agents and other people. We contrast mental models of AI teammates with those of human teammates as we characterize the dimensionality of these mental models, their development over time, and the influence of the participants' own self-perception. Our results indicate that people expect AI agents' performance to be significantly better on average than the performance of other hu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#22522;&#20110;&#36317;&#31163;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#32771;&#34385;&#23569;&#37327;&#25968;&#25454;&#20043;&#38388;&#30340;&#31867;&#20869;&#36317;&#31163;&#21644;&#31867;&#38388;&#36317;&#31163;&#26469;&#32771;&#34385;&#23884;&#20837;&#21521;&#37327;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#22686;&#24378;&#24230;&#37327;&#23398;&#20064;&#20013;&#30340;&#31867;&#21035;&#21487;&#20998;&#24615;&#65292;&#23454;&#29616;&#20248;&#31168;&#30340;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.09062</link><description>&lt;p&gt;
SuSana Distance&#26159;&#20320;&#25152;&#38656;&#35201;&#30340;&#65306;&#36890;&#36807;&#20004;&#31181;&#26032;&#30340;&#22522;&#20110;&#36317;&#31163;&#30340;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#24230;&#37327;&#23398;&#20064;&#20013;&#30340;&#31867;&#21035;&#21487;&#20998;&#24615;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
SuSana Distancia is all you need: Enforcing class separability in metric learning via two novel distance-based loss functions for few-shot image classification. (arXiv:2305.09062v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#22522;&#20110;&#36317;&#31163;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#32771;&#34385;&#23569;&#37327;&#25968;&#25454;&#20043;&#38388;&#30340;&#31867;&#20869;&#36317;&#31163;&#21644;&#31867;&#38388;&#36317;&#31163;&#26469;&#32771;&#34385;&#23884;&#20837;&#21521;&#37327;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#22686;&#24378;&#24230;&#37327;&#23398;&#20064;&#20013;&#30340;&#31867;&#21035;&#21487;&#20998;&#24615;&#65292;&#23454;&#29616;&#20248;&#31168;&#30340;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#23398;&#20064;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#20165;&#21033;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#26679;&#26412;&#23398;&#20064;&#26032;&#30340;&#27010;&#24565;&#12290;&#22522;&#20110;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#30340;&#26368;&#36817;&#24037;&#20316;&#21033;&#29992;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#25903;&#25345;&#65288;&#35757;&#32451;&#65289;&#21644;&#26597;&#35810;&#38598;&#65288;&#27979;&#35797;&#65289;&#65292;&#26088;&#22312;&#23398;&#20064;&#36825;&#20123;&#38598;&#21512;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#27604;&#36739;&#24230;&#37327;&#12290;&#30001;&#20110;&#25968;&#25454;&#32570;&#20047;&#65292;&#23884;&#20837;&#32593;&#32476;&#30340;&#23398;&#20064;&#36807;&#31243;&#25104;&#20026;&#23569;&#26679;&#26412;&#20219;&#21153;&#30340;&#37325;&#35201;&#37096;&#20998;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#20351;&#29992;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#24213;&#23618;&#28508;&#22312;&#31354;&#38388;&#30340;&#23646;&#24615;&#21644;&#31867;&#21035;&#20043;&#38388;&#30340;&#21487;&#20998;&#24615;&#24182;&#26410;&#23436;&#20840;&#24378;&#21046;&#25191;&#34892;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#32771;&#34385;&#23569;&#37327;&#25968;&#25454;&#20043;&#38388;&#30340;&#31867;&#20869;&#36317;&#31163;&#21644;&#31867;&#38388;&#36317;&#31163;&#26469;&#32771;&#34385;&#23884;&#20837;&#21521;&#37327;&#30340;&#37325;&#35201;&#24615;&#12290;&#31532;&#19968;&#20010;&#25439;&#22833;&#20989;&#25968;&#26159;Proto-Triplet Loss&#65292;&#23427;&#22522;&#20110;&#21407;&#22987;&#19977;&#20803;&#32452;&#25439;&#22833;&#65292;&#24182;&#28155;&#21152;&#20102;&#21407;&#22411;&#21521;&#37327;&#12290;&#31532;&#20108;&#20010;&#25439;&#22833;&#20989;&#25968;&#26159;SuSana Distance Loss&#65292;&#23427;&#32771;&#34385;&#20102;&#21516;&#31867;&#26679;&#26412;&#21644;&#19981;&#21516;&#31867;&#26679;&#26412;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23454;&#29616;&#31867;&#21035;&#21487;&#20998;&#24615;&#21644;&#36798;&#21040;&#26368;&#20808;&#36827;&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot learning is a challenging area of research that aims to learn new concepts with only a few labeled samples of data. Recent works based on metric-learning approaches leverage the meta-learning approach, which is encompassed by episodic tasks that make use a support (training) and query set (test) with the objective of learning a similarity comparison metric between those sets. Due to the lack of data, the learning process of the embedding network becomes an important part of the few-shot task. Previous works have addressed this problem using metric learning approaches, but the properties of the underlying latent space and the separability of the difference classes on it was not entirely enforced. In this work, we propose two different loss functions which consider the importance of the embedding vectors by looking at the intra-class and inter-class distance between the few data. The first loss function is the Proto-Triplet Loss, which is based on the original triplet loss with 
&lt;/p&gt;</description></item><item><title>LoViT&#26159;&#19968;&#31181;&#29992;&#20110;&#25163;&#26415;&#38454;&#27573;&#35782;&#21035;&#30340;&#38271;&#35270;&#39057;Transformer&#65292;&#23427;&#36890;&#36807;&#32467;&#21512;&#26102;&#38388;&#20016;&#23500;&#30340;&#31354;&#38388;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#22810;&#23610;&#24230;&#26102;&#38388;&#32858;&#21512;&#22120;&#26469;&#23545;&#38271;&#35270;&#39057;&#36827;&#34892;&#20998;&#26512;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.08989</link><description>&lt;p&gt;
LoViT: &#29992;&#20110;&#25163;&#26415;&#38454;&#27573;&#35782;&#21035;&#30340;&#38271;&#35270;&#39057;Transformer
&lt;/p&gt;
&lt;p&gt;
LoViT: Long Video Transformer for Surgical Phase Recognition. (arXiv:2305.08989v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08989
&lt;/p&gt;
&lt;p&gt;
LoViT&#26159;&#19968;&#31181;&#29992;&#20110;&#25163;&#26415;&#38454;&#27573;&#35782;&#21035;&#30340;&#38271;&#35270;&#39057;Transformer&#65292;&#23427;&#36890;&#36807;&#32467;&#21512;&#26102;&#38388;&#20016;&#23500;&#30340;&#31354;&#38388;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#22810;&#23610;&#24230;&#26102;&#38388;&#32858;&#21512;&#22120;&#26469;&#23545;&#38271;&#35270;&#39057;&#36827;&#34892;&#20998;&#26512;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26500;&#24314;&#33021;&#22815;&#37327;&#21270;&#34920;&#29616;&#24182;&#30417;&#30563;&#25163;&#26415;&#27969;&#31243;&#25191;&#34892;&#30340;&#19978;&#19979;&#25991;&#24037;&#20855;&#26041;&#38754;&#65292;&#22312;&#32447;&#25163;&#26415;&#38454;&#27573;&#35782;&#21035;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#26377;&#38480;&#65292;&#22240;&#20026;&#23427;&#20204;&#20351;&#29992;&#22522;&#20110;&#24103;&#32423;&#30417;&#30563;&#30340;&#31354;&#38388;&#29305;&#24449;&#25552;&#21462;&#22120;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#30001;&#20110;&#30456;&#20284;&#24103;&#22312;&#19981;&#21516;&#38454;&#27573;&#20986;&#29616;&#32780;&#23548;&#33268;&#30340;&#38169;&#35823;&#39044;&#27979;&#65292;&#24182;&#19988;&#30001;&#20110;&#35745;&#31639;&#38480;&#21046;&#32780;&#26410;&#33021;&#24456;&#22909;&#22320;&#34701;&#21512;&#26412;&#22320;&#21644;&#20840;&#23616;&#29305;&#24449;&#65292;&#36825;&#21487;&#33021;&#24433;&#21709;&#25163;&#26415;&#24178;&#39044;&#20013;&#36890;&#24120;&#36935;&#21040;&#30340;&#38271;&#35270;&#39057;&#30340;&#20998;&#26512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LoViT&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#29992;&#20110;&#34701;&#21512;&#30701;&#26399;&#21644;&#38271;&#26399;&#26102;&#38388;&#20449;&#24687;&#65292;&#23427;&#32467;&#21512;&#20102;&#19968;&#20010;&#26102;&#38388;&#20016;&#23500;&#30340;&#31354;&#38388;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#19968;&#20010;&#22810;&#23610;&#24230;&#26102;&#38388;&#32858;&#21512;&#22120;&#65292;&#21518;&#32773;&#30001;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#30340;&#20004;&#20010;&#32423;&#32852;L-Trans&#27169;&#22359;&#21644;&#19968;&#20010;&#22522;&#20110;ProbSparse&#33258;&#27880;&#24847;&#21147;&#30340;G-Informer&#27169;&#22359;&#32452;&#25104;&#65292;&#29992;&#20110;&#22788;&#29702;&#20840;&#23616;&#26102;&#38388;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#22810;&#23610;&#24230;&#26102;&#38388;&#22836;&#32467;&#21512;&#26412;&#22320;&#21644;&#20840;&#23616;&#29305;&#24449;&#26469;&#35782;&#21035;&#38271;&#35270;&#39057;&#20013;&#30340;&#25163;&#26415;&#38454;&#27573;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LoViT&#22312;&#20004;&#20010;&#20844;&#20849;&#25163;&#26415;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online surgical phase recognition plays a significant role towards building contextual tools that could quantify performance and oversee the execution of surgical workflows. Current approaches are limited since they train spatial feature extractors using frame-level supervision that could lead to incorrect predictions due to similar frames appearing at different phases, and poorly fuse local and global features due to computational constraints which can affect the analysis of long videos commonly encountered in surgical interventions. In this paper, we present a two-stage method, called Long Video Transformer (LoViT) for fusing short- and long-term temporal information that combines a temporally-rich spatial feature extractor and a multi-scale temporal aggregator consisting of two cascaded L-Trans modules based on self-attention, followed by a G-Informer module based on ProbSparse self-attention for processing global temporal information. The multi-scale temporal head then combines loc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#24322;&#26500;&#25968;&#25454;&#38598;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#21327;&#35843;&#21644;&#25968;&#25454;&#22635;&#34917;&#31561;&#20851;&#38190;&#27493;&#39588;&#32435;&#20837;&#26550;&#26500;&#24895;&#26223;&#65292;&#26469;&#20419;&#36827;&#25968;&#25454;&#31649;&#29702;&#20449;&#24687;&#31995;&#32479;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#20132;&#21449;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2305.08985</link><description>&lt;p&gt;
&#24322;&#26500;&#25968;&#25454;&#38598;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Federated Learning over Harmonized Data Silos. (arXiv:2305.08985v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08985
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#24322;&#26500;&#25968;&#25454;&#38598;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#21327;&#35843;&#21644;&#25968;&#25454;&#22635;&#34917;&#31561;&#20851;&#38190;&#27493;&#39588;&#32435;&#20837;&#26550;&#26500;&#24895;&#26223;&#65292;&#26469;&#20419;&#36827;&#25968;&#25454;&#31649;&#29702;&#20449;&#24687;&#31995;&#32479;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#20132;&#21449;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#20351;&#22320;&#29702;&#20998;&#24067;&#30340;&#25968;&#25454;&#38598;&#20849;&#21516;&#23398;&#20064;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#20849;&#20139;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22823;&#37096;&#20998;&#24037;&#20316;&#37117;&#26159;&#22522;&#20110;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#22914;&#22270;&#20687;&#25110;&#25991;&#26412;&#65289;&#65292;&#25110;&#32773;&#22522;&#20110;&#22312;&#19981;&#21516;&#30340;&#31449;&#28857;&#19978;&#20855;&#26377;&#19968;&#33268;&#32467;&#26500;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#24773;&#20917;&#26159;&#21508;&#31449;&#28857;&#20855;&#26377;&#19981;&#21516;&#30340;&#27169;&#24335;&#12289;&#25968;&#25454;&#26684;&#24335;&#12289;&#25968;&#25454;&#20540;&#21644;&#35775;&#38382;&#27169;&#24335;&#65292;&#22240;&#27492;&#38656;&#35201;&#25968;&#25454;&#38598;&#25104;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#21253;&#25324;&#20351;&#29992;&#22768;&#26126;&#24615;&#27169;&#24335;&#26144;&#23556;&#36827;&#34892;&#25968;&#25454;&#20132;&#25442;&#21644;&#26597;&#35810;&#37325;&#20889;&#65292;&#20197;&#21450;&#23454;&#20307;&#36830;&#25509;&#31561;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#20102;&#25968;&#25454;&#21327;&#35843;&#21644;&#25968;&#25454;&#22635;&#34917;&#31561;&#20851;&#38190;&#27493;&#39588;&#30340;&#31471;&#21040;&#31471;&#32852;&#37030;&#23398;&#20064;&#21644;&#38598;&#25104;&#31995;&#32479;&#30340;&#26550;&#26500;&#24895;&#26223;&#65292;&#20197;&#20419;&#36827;&#25968;&#25454;&#31649;&#29702;&#20449;&#24687;&#31995;&#32479;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#20132;&#21449;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning is a distributed machine learning approach that enables geographically distributed data silos to collaboratively learn a joint machine learning model without sharing data. Most of the existing work operates on unstructured data, such as images or text, or on structured data assumed to be consistent across the different sites. However, sites often have different schemata, data formats, data values, and access patterns. The field of data integration has developed many methods to address these challenges, including techniques for data exchange and query rewriting using declarative schema mappings, and for entity linkage. Therefore, we propose an architectural vision for an end-to-end Federated Learning and Integration system, incorporating the critical steps of data harmonization and data imputation, to spur further research on the intersection of data management information systems and machine learning.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;HumanMotionQA&#20219;&#21153;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#22312;&#22797;&#26434;&#30340;&#20154;&#20307;&#36816;&#21160;&#24207;&#21015;&#19978;&#36827;&#34892;&#22797;&#26434;&#12289;&#22810;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#27530;&#30340;NSPose&#26041;&#27861;&#65292;&#21033;&#29992;&#31526;&#21495;&#21270;&#25512;&#29702;&#21644;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#35813;&#20219;&#21153;&#20013;&#65292;&#24182;&#36229;&#36807;&#25152;&#26377;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.08953</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22359;&#21270;&#21160;&#20316;&#31243;&#24207;&#30340;&#21160;&#20316;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Motion Question Answering via Modular Motion Programs. (arXiv:2305.08953v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08953
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;HumanMotionQA&#20219;&#21153;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#22312;&#22797;&#26434;&#30340;&#20154;&#20307;&#36816;&#21160;&#24207;&#21015;&#19978;&#36827;&#34892;&#22797;&#26434;&#12289;&#22810;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#27530;&#30340;NSPose&#26041;&#27861;&#65292;&#21033;&#29992;&#31526;&#21495;&#21270;&#25512;&#29702;&#21644;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#35813;&#20219;&#21153;&#20013;&#65292;&#24182;&#36229;&#36807;&#25152;&#26377;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#26500;&#24314;&#33021;&#22815;&#24863;&#30693;&#21644;&#29702;&#35299;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#20154;&#31867;&#34892;&#20026;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#25105;&#20204;&#24517;&#39035;&#39318;&#20808;&#35774;&#35745;&#27169;&#22411;&#65292;&#23545;&#21160;&#20316;&#24207;&#21015;&#36827;&#34892;&#22797;&#26434;&#30340;&#26102;&#31354;&#25512;&#29702;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HumanMotionQA&#20219;&#21153;&#65292;&#35780;&#20272;&#27169;&#22411;&#22312;&#38271;&#26102;&#38388;&#20154;&#31867;&#36816;&#21160;&#24207;&#21015;&#19978;&#36827;&#34892;&#22797;&#26434;&#12289;&#22810;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#38382;&#31572;&#23545;&#25968;&#25454;&#38598;&#65292;&#38656;&#35201;&#22312;&#21160;&#20316;&#24207;&#21015;&#30340;&#23567;&#37096;&#20998;&#20013;&#26816;&#27979;&#36816;&#21160;&#32447;&#32034;&#65292;&#23545;&#20107;&#20214;&#21457;&#29983;&#30340;&#26102;&#38388;&#36827;&#34892;&#25512;&#29702;&#65292;&#24182;&#26597;&#35810;&#29305;&#23450;&#30340;&#36816;&#21160;&#23646;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;NSPose&#65292;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#35813;&#20219;&#21153;&#65292;&#23427;&#21033;&#29992;&#31526;&#21495;&#21270;&#25512;&#29702;&#21644;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#36890;&#36807;&#23398;&#20064;&#36816;&#21160;&#27010;&#24565;&#12289;&#23646;&#24615;&#31070;&#32463;&#25805;&#20316;&#31526;&#21644;&#26102;&#38388;&#20851;&#31995;&#65292;&#26469;&#22788;&#29702;&#21160;&#20316;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;NSPose&#22312;HumanMotionQA&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#32988;&#36807;&#20102;&#25152;&#26377;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to build artificial intelligence systems that can perceive and reason with human behavior in the real world, we must first design models that conduct complex spatio-temporal reasoning over motion sequences. Moving towards this goal, we propose the HumanMotionQA task to evaluate complex, multi-step reasoning abilities of models on long-form human motion sequences. We generate a dataset of question-answer pairs that require detecting motor cues in small portions of motion sequences, reasoning temporally about when events occur, and querying specific motion attributes. In addition, we propose NSPose, a neuro-symbolic method for this task that uses symbolic reasoning and a modular design to ground motion through learning motion concepts, attribute neural operators, and temporal relations. We demonstrate the suitability of NSPose for the HumanMotionQA task, outperforming all baseline methods.
&lt;/p&gt;</description></item><item><title>MIMEx&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#36974;&#30422;&#36755;&#20837;&#24314;&#27169;&#26469;&#25552;&#21462;&#20869;&#22312;&#22870;&#21169;&#65292;&#36890;&#36807;&#25511;&#21046;&#36974;&#30422;&#20998;&#24067;&#26469;&#25511;&#21046;&#38590;&#24230;&#65292;&#21487;&#20197;&#22312;&#39640;&#32500;&#29615;&#22659;&#20013;&#21462;&#24471;&#20248;&#36234;&#30340;&#25506;&#32034;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.08932</link><description>&lt;p&gt;
MIMEx: &#36974;&#30422;&#36755;&#20837;&#24314;&#27169;&#30340;&#20869;&#22312;&#22870;&#21169;
&lt;/p&gt;
&lt;p&gt;
MIMEx: Intrinsic Rewards from Masked Input Modeling. (arXiv:2305.08932v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08932
&lt;/p&gt;
&lt;p&gt;
MIMEx&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#36974;&#30422;&#36755;&#20837;&#24314;&#27169;&#26469;&#25552;&#21462;&#20869;&#22312;&#22870;&#21169;&#65292;&#36890;&#36807;&#25511;&#21046;&#36974;&#30422;&#20998;&#24067;&#26469;&#25511;&#21046;&#38590;&#24230;&#65292;&#21487;&#20197;&#22312;&#39640;&#32500;&#29615;&#22659;&#20013;&#21462;&#24471;&#20248;&#36234;&#30340;&#25506;&#32034;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#32500;&#35266;&#27979;&#29615;&#22659;&#20013;&#36827;&#34892;&#25506;&#32034;&#24456;&#22256;&#38590;&#12290;&#20351;&#29992;&#20869;&#22312;&#22870;&#21169;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#28145;&#24230;&#32593;&#32476;&#20272;&#35745;&#29366;&#24577;&#12289;&#36716;&#25442;&#25110;&#36712;&#36857;&#30340;&#8220;&#26032;&#39062;&#24615;&#8221;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#26465;&#20214;&#39044;&#27979;&#30446;&#26631;&#65292;&#22914;&#36974;&#30422;&#33258;&#21160;&#32534;&#30721;&#21487;&#30475;&#20316;&#20266;&#20284;&#28982;&#30340;&#38543;&#26426;&#20272;&#35745;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#19968;&#35266;&#28857;&#22914;&#20309;&#33258;&#28982;&#22320;&#23548;&#33268;&#23545;&#29616;&#26377;&#20869;&#22312;&#22870;&#21169;&#26041;&#27861;&#30340;&#32479;&#19968;&#30475;&#27861;:&#23427;&#20204;&#26159;&#26465;&#20214;&#39044;&#27979;&#30340;&#29305;&#20363;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26032;&#39062;&#24615;&#30340;&#20272;&#35745;&#21487;&#20197;&#30475;&#20316;&#26159;&#20351;&#29992;&#19981;&#21516;&#30340;&#36974;&#30422;&#20998;&#24067;&#36827;&#34892;&#20266;&#20284;&#28982;&#20272;&#35745;&#12290;&#20174;&#36825;&#20010;&#35282;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#8212;&#8212;&#36974;&#30422;&#36755;&#20837;&#24314;&#27169;&#25506;&#32034;&#20869;&#22312;&#22870;&#21169;(MIMEx)&#65292;&#20854;&#20013;&#36974;&#30422;&#20998;&#24067;&#21487;&#20197;&#28789;&#27963;&#35843;&#25972;&#20197;&#25511;&#21046;&#24213;&#23618;&#26465;&#20214;&#39044;&#27979;&#20219;&#21153;&#30340;&#38590;&#24230;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#24403;&#19982;&#31454;&#20105;&#26041;&#27861;&#30456;&#27604;&#26102;&#65292;MIMEx&#21487;&#20197;&#21462;&#24471;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploring in environments with high-dimensional observations is hard. One promising approach for exploration is to use intrinsic rewards, which often boils down to estimating "novelty" of states, transitions, or trajectories with deep networks. Prior works have shown that conditional prediction objectives such as masked autoencoding can be seen as stochastic estimation of pseudo-likelihood. We show how this perspective naturally leads to a unified view on existing intrinsic reward approaches: they are special cases of conditional prediction, where the estimation of novelty can be seen as pseudo-likelihood estimation with different mask distributions. From this view, we propose a general framework for deriving intrinsic rewards -- Masked Input Modeling for Exploration (MIMEx) -- where the mask distribution can be flexibly tuned to control the difficulty of the underlying conditional prediction task. We demonstrate that MIMEx can achieve superior results when compared against competitive
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;AlphaFold2&#30340;&#23545;&#25239;&#24207;&#21015;&#31361;&#21464;&#65292;&#23454;&#27979;&#20165;&#20462;&#25913;&#19977;&#20010;&#27688;&#22522;&#37240;&#27531;&#22522;&#65292;&#23601;&#20351;&#20854;&#39044;&#27979;&#30340;&#34507;&#30333;&#36136;&#19977;&#32423;&#32467;&#26500;&#30340;&#25913;&#21464;&#36798;&#21040;&#20102;46.61&#65292;&#24182;&#33021;&#25104;&#21151;&#21457;&#29616;&#22312;&#29305;&#23450;&#34507;&#30333;&#36136;&#20013;&#23545;&#32467;&#26500;&#33267;&#20851;&#37325;&#35201;&#19988;&#26377;&#29983;&#29289;&#23398;&#24847;&#20041;&#30340;&#27688;&#22522;&#37240;&#27531;&#22522;&#65292;&#20026;&#34507;&#30333;&#36136;&#30340;&#20462;&#39280;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2305.08929</link><description>&lt;p&gt;
AF2-Mutation&#65306;&#38024;&#23545;AlphaFold2&#30340;&#34507;&#30333;&#19977;&#32423;&#32467;&#26500;&#39044;&#27979;&#30340;&#23545;&#25239;&#24207;&#21015;&#31361;&#21464;
&lt;/p&gt;
&lt;p&gt;
AF2-Mutation: Adversarial Sequence Mutations against AlphaFold2 on Protein Tertiary Structure Prediction. (arXiv:2305.08929v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;AlphaFold2&#30340;&#23545;&#25239;&#24207;&#21015;&#31361;&#21464;&#65292;&#23454;&#27979;&#20165;&#20462;&#25913;&#19977;&#20010;&#27688;&#22522;&#37240;&#27531;&#22522;&#65292;&#23601;&#20351;&#20854;&#39044;&#27979;&#30340;&#34507;&#30333;&#36136;&#19977;&#32423;&#32467;&#26500;&#30340;&#25913;&#21464;&#36798;&#21040;&#20102;46.61&#65292;&#24182;&#33021;&#25104;&#21151;&#21457;&#29616;&#22312;&#29305;&#23450;&#34507;&#30333;&#36136;&#20013;&#23545;&#32467;&#26500;&#33267;&#20851;&#37325;&#35201;&#19988;&#26377;&#29983;&#29289;&#23398;&#24847;&#20041;&#30340;&#27688;&#22522;&#37240;&#27531;&#22522;&#65292;&#20026;&#34507;&#30333;&#36136;&#30340;&#20462;&#39280;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22914;AlphaFold2&#65288;AF2&#65289;&#65292;&#21462;&#24471;&#20102;&#19982;&#30495;&#23454;&#29983;&#29289;&#23454;&#39564;&#26041;&#27861;&#21487;&#27604;&#30340;&#34507;&#30333;&#36136;&#19977;&#32423;&#32467;&#26500;&#39044;&#27979;&#25928;&#26524;&#12290;&#34429;&#28982;AF2&#22312;&#39044;&#27979;&#21464;&#24322;&#25928;&#26524;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#20294;&#20854;&#23545;&#24207;&#21015;&#21464;&#24322;&#30340;&#40065;&#26834;&#24615;&#23578;&#24453;&#30830;&#23450;&#12290;&#26412;&#25991;&#20174;&#37326;&#29983;&#22411;&#65288;WT&#65289;&#24207;&#21015;&#24320;&#22987;&#65292;&#36890;&#36807;&#36827;&#21270;&#26041;&#27861;&#29983;&#25104;&#23545;&#25239;&#24207;&#21015;&#65292;AF2&#39044;&#27979;&#19982;WT&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#25105;&#20204;&#22312;CASP14&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#34507;&#30333;&#36136;&#24207;&#21015;&#20013;&#20165;&#20462;&#25913;3&#20010;&#27688;&#22522;&#37240;&#27531;&#22522;&#65292;&#37319;&#29992;&#26367;&#25442;&#12289;&#21024;&#38500;&#21644;&#25554;&#20837;&#31574;&#30053;&#30340;&#32452;&#21512;&#65292;AF2&#39044;&#27979;&#30340;&#26412;&#22320;&#36317;&#31163;&#24046;&#24322;&#27979;&#35797;&#65288;lDDT&#65289;&#30340;&#25913;&#21464;&#36798;&#21040;&#20102;46.61&#12290;&#27492;&#22806;&#65292;&#24403;&#24212;&#29992;&#20110;&#29305;&#23450;&#34507;&#30333;&#36136;SPNS2&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#25104;&#21151;&#22320;&#35782;&#21035;&#20102;&#23545;&#34507;&#30333;&#36136;&#32467;&#26500;&#30830;&#23450;&#33267;&#20851;&#37325;&#35201;&#19988;&#26377;&#29983;&#29289;&#23398;&#24847;&#20041;&#30340;&#27688;&#22522;&#37240;&#27531;&#22522;&#65292;&#21487;&#33021;&#34920;&#26126;&#21487;&#20462;&#25913;&#30340;&#34507;&#30333;&#36136;&#37096;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning-based approaches, such as AlphaFold2 (AF2), have significantly advanced protein tertiary structure prediction, achieving results comparable to real biological experimental methods. While AF2 has shown limitations in predicting the effects of mutations, its robustness against sequence mutations remains to be determined. Starting with the wild-type (WT) sequence, we investigate adversarial sequences generated via an evolutionary approach, which AF2 predicts to be substantially different from WT. Our experiments on CASP14 reveal that by modifying merely three residues in the protein sequence using a combination of replacement, deletion, and insertion strategies, the alteration in AF2's predictions, as measured by the Local Distance Difference Test (lDDT), reaches 46.61. Moreover, when applied to a specific protein, SPNS2, our proposed algorithm successfully identifies biologically meaningful residues critical to protein structure determination and potentially indicates alter
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#27979;&#27169;&#22411;DFCNN&#65292;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20855;&#26377;&#21487;&#23398;&#20064;&#33021;&#21147;&#30340;FTSF&#65292;&#24182;&#33021;&#22815;&#22788;&#29702;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#12290;</title><link>http://arxiv.org/abs/2305.08890</link><description>&lt;p&gt;
&#24046;&#20998;&#21367;&#31215;&#27169;&#31946;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Differential Convolutional Fuzzy Time Series Forecasting. (arXiv:2305.08890v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#27979;&#27169;&#22411;DFCNN&#65292;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20855;&#26377;&#21487;&#23398;&#20064;&#33021;&#21147;&#30340;FTSF&#65292;&#24182;&#33021;&#22815;&#22788;&#29702;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#31946;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;FTSF&#65289;&#26159;&#19968;&#31181;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#20856;&#22411;&#39044;&#27979;&#26041;&#27861;&#12290;&#20256;&#32479;&#30340;FTSF&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#19987;&#23478;&#31995;&#32479;&#65292;&#23548;&#33268;&#22833;&#21435;&#20102;&#35782;&#21035;&#26410;&#23450;&#20041;&#29305;&#24449;&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;FTSF&#39044;&#27979;&#19981;&#20934;&#30830;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#24046;&#20998;&#27169;&#31946;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;DFCNN&#65289;&#27169;&#22411;&#65292;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20855;&#26377;&#21487;&#23398;&#20064;&#33021;&#21147;&#30340;FTSF&#12290;DFCNN&#33021;&#22815;&#35782;&#21035;&#28508;&#22312;&#20449;&#24687;&#24182;&#25913;&#21892;&#39044;&#27979;&#31934;&#24230;&#12290;&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#23398;&#20064;&#33021;&#21147;&#65292;FTSF&#24314;&#31435;&#30340;&#27169;&#31946;&#35268;&#21017;&#30340;&#38271;&#24230;&#21487;&#20197;&#20219;&#24847;&#25193;&#23637;&#65292;&#36825;&#26159;&#19987;&#23478;&#31995;&#32479;&#25152;&#26080;&#27861;&#22788;&#29702;&#30340;&#12290;&#21516;&#26102;&#65292;&#30001;&#20110;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#30340;&#36235;&#21183;&#65292;FTSF&#36890;&#24120;&#26080;&#27861;&#23454;&#29616;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#30340;&#36235;&#21183;&#20250;&#23548;&#33268;FTSF&#24314;&#31435;&#30340;&#27169;&#31946;&#38598;&#22833;&#25928;&#65292;&#24182;&#23548;&#33268;&#39044;&#27979;&#22833;&#36133;&#12290;DFCNN&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#21487;&#20197;&#22788;&#29702;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fuzzy time series forecasting (FTSF) is a typical forecasting method with wide application. Traditional FTSF is regarded as an expert system which leads to lose the ability to recognize undefined feature. The mentioned is main reason of poor forecasting with FTSF. To solve the problem, the proposed model Differential Fuzzy Convolutional Neural Network (DFCNN) utilizes convolution neural network to re-implement FTSF with learnable ability. DFCNN is capable of recognizing the potential information and improve the forecasting accuracy. Thanks to learnable ability of neural network, length of fuzzy rules established in FTSF is expended to arbitrary length which expert is not able to be handle by expert system. At the same time, FTSF usually cannot achieve satisfactory performance of non-stationary time series due to trend of non-stationary time series. The trend of non-stationary time series causes the fuzzy set established by FTSF to invalid and cause the forecasting to fail. DFCNN utiliz
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Transformer&#27880;&#24847;&#21147;&#22270;&#26469;&#33719;&#24471;&#35789;&#32423;&#21453;&#39304;&#30340;&#26032;&#22411;&#40657;&#30418;LLM&#27700;&#21360;&#31639;&#27861;&#65292;&#23454;&#29616;&#23545;&#30001;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#27700;&#21360;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2305.08883</link><description>&lt;p&gt;
&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#27700;&#21360;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Watermarking Text Generated by Black-Box Language Models. (arXiv:2305.08883v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08883
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Transformer&#27880;&#24847;&#21147;&#22270;&#26469;&#33719;&#24471;&#35789;&#32423;&#21453;&#39304;&#30340;&#26032;&#22411;&#40657;&#30418;LLM&#27700;&#21360;&#31639;&#27861;&#65292;&#23454;&#29616;&#23545;&#30001;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#27700;&#21360;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#29616;&#22312;&#23637;&#31034;&#20102;&#22312;&#21508;&#39046;&#22495;&#20013;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#25216;&#33021;&#65292;&#24341;&#21457;&#20154;&#20204;&#23545;&#35823;&#29992;&#30340;&#25285;&#24551;&#12290;&#22240;&#27492;&#65292;&#26816;&#27979;&#29983;&#25104;&#30340;&#25991;&#26412;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#34987;&#21160;&#24335;&#26816;&#27979;&#26041;&#27861;&#38519;&#20837;&#20102;&#39046;&#22495;&#29305;&#24322;&#24615;&#21644;&#26377;&#38480;&#30340;&#23545;&#25239;&#24378;&#24230;&#12290;&#20026;&#20102;&#23454;&#29616;&#21487;&#38752;&#30340;&#26816;&#27979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27700;&#21360;&#30340;&#30333;&#30418;LLM&#26041;&#27861;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#23884;&#20837;&#27700;&#21360;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#23558;&#27169;&#22411;&#35789;&#27719;&#38543;&#26426;&#20998;&#21106;&#20197;&#33719;&#24471;&#29305;&#27530;&#21015;&#34920;&#24182;&#35843;&#25972;&#27010;&#29575;&#20998;&#24067;&#20197;&#20419;&#36827;&#21015;&#34920;&#20013;&#21333;&#35789;&#30340;&#36873;&#25321;&#12290;&#19968;&#20010;&#30693;&#26195;&#21015;&#34920;&#30340;&#26816;&#27979;&#31639;&#27861;&#21487;&#20197;&#35782;&#21035;&#24102;&#27700;&#21360;&#30340;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#35768;&#22810;&#20165;&#26377;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#22330;&#26223;&#20013;&#19981;&#36866;&#29992;&#12290;&#20026;&#20102;&#20801;&#35768;&#31532;&#19977;&#26041;&#32473;&#30001;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#21152;&#19978;&#27700;&#21360;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36716;&#25442;&#22120;&#27880;&#24847;&#21147;&#22270;&#26469;&#33719;&#24471;&#35789;&#32423;&#21453;&#39304;&#30340;&#26032;&#27700;&#21360;&#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#32473;&#30001;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#21152;&#19978;&#27700;&#21360;&#65292;&#32780;&#19981;&#24433;&#21709;&#25991;&#26412;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs now exhibit human-like skills in various fields, leading to worries about misuse. Thus, detecting generated text is crucial. However, passive detection methods are stuck in domain specificity and limited adversarial robustness. To achieve reliable detection, a watermark-based method was proposed for white-box LLMs, allowing them to embed watermarks during text generation. The method involves randomly dividing the model vocabulary to obtain a special list and adjusting the probability distribution to promote the selection of words in the list. A detection algorithm aware of the list can identify the watermarked text. However, this method is not applicable in many real-world scenarios where only black-box language models are available. For instance, third-parties that develop API-based vertical applications cannot watermark text themselves because API providers only supply generated text and withhold probability distributions to shield their commercial interests. To allow third-part
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23581;&#35797;&#36890;&#36807;&#20351;&#29992;ANN&#21021;&#22987;&#21270;&#25991;&#29486;&#20013;&#30340;&#25216;&#26415;&#20197;&#21450;&#35745;&#31639;&#31070;&#32463;&#31185;&#23398;&#32467;&#26524;&#26469;&#35299;&#20915;&#31361;&#35302;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#21021;&#22987;&#21270;&#30340;&#38382;&#39064;&#12290;&#20316;&#32773;&#25351;&#20986;&#65292;&#19982;ANNs&#30456;&#27604;&#65292;SNNs&#30340;&#37325;&#37327;&#21021;&#22987;&#21270;&#38382;&#39064;&#26356;&#20026;&#24494;&#22937;&#65292;&#22240;&#20026;SNNs&#20855;&#26377;&#23574;&#23792;&#21644;&#37325;&#32622;&#38750;&#32447;&#24615;&#20197;&#21450;&#23556;&#39057;&#23849;&#28291;&#38382;&#39064;&#65292;&#20026;&#27492;&#20316;&#32773;&#25552;&#20986;&#24182;&#35299;&#20915;&#20102;&#20960;&#31181;&#23556;&#39057;&#23849;&#28291;&#38382;&#39064;&#30340;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.08879</link><description>&lt;p&gt;
&#31361;&#35302;&#32593;&#32476;&#21021;&#22987;&#21270;&#19982;&#23556;&#39057;&#23849;&#28291;
&lt;/p&gt;
&lt;p&gt;
Spiking Network Initialisation and Firing Rate Collapse. (arXiv:2305.08879v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08879
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23581;&#35797;&#36890;&#36807;&#20351;&#29992;ANN&#21021;&#22987;&#21270;&#25991;&#29486;&#20013;&#30340;&#25216;&#26415;&#20197;&#21450;&#35745;&#31639;&#31070;&#32463;&#31185;&#23398;&#32467;&#26524;&#26469;&#35299;&#20915;&#31361;&#35302;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#21021;&#22987;&#21270;&#30340;&#38382;&#39064;&#12290;&#20316;&#32773;&#25351;&#20986;&#65292;&#19982;ANNs&#30456;&#27604;&#65292;SNNs&#30340;&#37325;&#37327;&#21021;&#22987;&#21270;&#38382;&#39064;&#26356;&#20026;&#24494;&#22937;&#65292;&#22240;&#20026;SNNs&#20855;&#26377;&#23574;&#23792;&#21644;&#37325;&#32622;&#38750;&#32447;&#24615;&#20197;&#21450;&#23556;&#39057;&#23849;&#28291;&#38382;&#39064;&#65292;&#20026;&#27492;&#20316;&#32773;&#25552;&#20986;&#24182;&#35299;&#20915;&#20102;&#20960;&#31181;&#23556;&#39057;&#23849;&#28291;&#38382;&#39064;&#30340;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26032;&#24320;&#21457;&#30340;&#35757;&#32451;&#31361;&#35302;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#30340;&#26041;&#27861;&#24050;&#32463;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#21487;&#20197;&#20316;&#20026;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#30340;&#26367;&#20195;&#21697;&#65292;&#21516;&#26102;&#22312;&#25512;&#29702;&#21644;&#28508;&#22312;&#30340;&#35757;&#32451;&#26102;&#38388;&#19978;&#37117;&#26356;&#21152;&#33410;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;SNN&#65292;&#20160;&#20040;&#26159;&#19968;&#20010;&#22909;&#30340;&#21021;&#22987;&#21270;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#32463;&#24120;&#20351;&#29992;&#20026;ANN&#35757;&#32451;&#24320;&#21457;&#30340;&#21021;&#22987;&#21270;&#26041;&#26696;&#65292;&#36825;&#20123;&#26041;&#26696;&#32463;&#24120;&#26159;&#19981;&#36275;&#30340;&#65292;&#38656;&#35201;&#25163;&#21160;&#35843;&#25972;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#36890;&#36807;&#20351;&#29992;ANN&#21021;&#22987;&#21270;&#25991;&#29486;&#20013;&#30340;&#25216;&#26415;&#20197;&#21450;&#35745;&#31639;&#31070;&#32463;&#31185;&#23398;&#32467;&#26524;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#30001;&#20110;SNN&#30340;&#23574;&#23792;&#21644;&#37325;&#32622;&#38750;&#32447;&#24615;&#20197;&#21450;&#23556;&#39057;&#23849;&#28291;&#38382;&#39064;&#65292;&#19982;ANNs&#30456;&#27604;&#65292;ANNs&#30340;&#37325;&#37327;&#21021;&#22987;&#21270;&#38382;&#39064;&#26356;&#20026;&#24494;&#22937;&#12290;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#35299;&#20915;&#23556;&#39057;&#23849;&#28291;&#38382;&#39064;&#30340;&#26041;&#26696;&#65292;&#22312;&#19981;&#21516;&#30340;&#20551;&#35774;&#19979;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, newly developed methods to train spiking neural networks (SNNs) have rendered them as a plausible alternative to Artificial Neural Networks (ANNs) in terms of accuracy, while at the same time being much more energy efficient at inference and potentially at training time. However, it is still unclear what constitutes a good initialisation for an SNN. We often use initialisation schemes developed for ANN training which are often inadequate and require manual tuning. In this paper, we attempt to tackle this issue by using techniques from the ANN initialisation literature as well as computational neuroscience results. We show that the problem of weight initialisation for ANNs is a more nuanced problem than it is for ANNs due to the spike-and-reset non-linearity of SNNs and the firing rate collapse problem. We firstly identify and propose several solutions to the firing rate collapse problem under different sets of assumptions which successfully solve the issue by leveragin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#30740;&#31350;&#20102;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#30340;&#30740;&#31350;&#65292;&#25506;&#32034;&#20102;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#21644;&#25512;&#29702;&#20808;&#21069;&#21644;&#23398;&#20064;&#30693;&#35782;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#20197;&#25506;&#32034;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#36890;&#29992;&#24615;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.08876</link><description>&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#21450;&#20854;&#20998;&#31867;&#27861;&#65306;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Neurosymbolic AI and its Taxonomy: a survey. (arXiv:2305.08876v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#30740;&#31350;&#20102;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#30340;&#30740;&#31350;&#65292;&#25506;&#32034;&#20102;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#21644;&#25512;&#29702;&#20808;&#21069;&#21644;&#23398;&#20064;&#30693;&#35782;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#20197;&#25506;&#32034;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#36890;&#29992;&#24615;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#28041;&#21450;&#32452;&#21512;&#31526;&#21495;&#22788;&#29702;&#65288;&#22914;&#32463;&#20856;&#20154;&#24037;&#26234;&#33021;&#65289;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#65292;&#26159;&#19968;&#20010;&#38750;&#24120;&#25104;&#29087;&#30340;&#39046;&#22495;&#12290;&#36825;&#20123;&#27169;&#22411;&#20316;&#20026;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#36890;&#29992;&#24615;&#30340;&#19968;&#31181;&#23581;&#35797;&#65292;&#22312;&#25506;&#32034;&#38500;&#20102;&#22686;&#21152;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#23610;&#23544;&#20197;&#22806;&#30340;&#26367;&#20195;&#26041;&#26696;&#20197;&#21450;&#23558;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#21644;&#25512;&#29702;&#20808;&#21069;&#21644;&#23398;&#20064;&#30693;&#35782;&#30456;&#32467;&#21512;&#26041;&#38754;&#20855;&#26377;&#29420;&#29305;&#20316;&#29992;&#12290;&#26412;&#27425;&#35843;&#26597;&#30740;&#31350;&#20102;&#36825;&#19968;&#39046;&#22495;&#36817;&#24180;&#26469;&#30340;&#30740;&#31350;&#35770;&#25991;&#65292;&#24182;&#25552;&#20379;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#20998;&#31867;&#21644;&#27604;&#36739;&#65292;&#21516;&#26102;&#20171;&#32461;&#20102;&#24212;&#29992;&#26696;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neurosymbolic AI deals with models that combine symbolic processing, like classic AI, and neural networks, as it's a very established area. These models are emerging as an effort toward Artificial General Intelligence (AGI) by both exploring an alternative to just increasing datasets' and models' sizes and combining Learning over the data distribution, Reasoning on prior and learned knowledge, and by symbiotically using them. This survey investigates research papers in this area during recent years and brings classification and comparison between the presented models as well as applications.
&lt;/p&gt;</description></item><item><title>&#24555;&#36895;&#21457;&#23637;&#30340;&#20114;&#32852;&#32593;&#12289;&#31227;&#21160;&#21644;IoT&#25216;&#26415;&#20013;&#65292;&#29992;&#25143;&#38656;&#35201;&#31649;&#29702;&#22823;&#37327;&#30340;&#20010;&#20154;&#25968;&#25454;&#65292;&#20294;&#26159;&#20915;&#23450;&#22914;&#20309;&#20445;&#25252;&#38544;&#31169;&#26159;&#19968;&#20010;&#20196;&#20154;&#19981;&#30693;&#25152;&#25514;&#30340;&#25361;&#25112;&#12290;&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#33258;&#21160;&#21270;&#38544;&#31169;&#20915;&#31574;&#30340;&#29616;&#26377;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;&#30340;&#27010;&#36848;&#12290;</title><link>http://arxiv.org/abs/2305.08747</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#38544;&#31169;&#20915;&#31574;&#65306;&#20309;&#26102;&#30011;&#19968;&#26465;&#32447;&#65311;
&lt;/p&gt;
&lt;p&gt;
Automating privacy decisions -- where to draw the line?. (arXiv:2305.08747v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08747
&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#21457;&#23637;&#30340;&#20114;&#32852;&#32593;&#12289;&#31227;&#21160;&#21644;IoT&#25216;&#26415;&#20013;&#65292;&#29992;&#25143;&#38656;&#35201;&#31649;&#29702;&#22823;&#37327;&#30340;&#20010;&#20154;&#25968;&#25454;&#65292;&#20294;&#26159;&#20915;&#23450;&#22914;&#20309;&#20445;&#25252;&#38544;&#31169;&#26159;&#19968;&#20010;&#20196;&#20154;&#19981;&#30693;&#25152;&#25514;&#30340;&#25361;&#25112;&#12290;&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#33258;&#21160;&#21270;&#38544;&#31169;&#20915;&#31574;&#30340;&#29616;&#26377;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;&#30340;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#36890;&#24120;&#23545;&#31649;&#29702;&#20854;&#20010;&#20154;&#25968;&#25454;&#30340;&#38544;&#31169;&#20915;&#31574;&#24863;&#21040;&#19981;&#30693;&#25152;&#25514;&#65292;&#36825;&#20123;&#20915;&#31574;&#21487;&#20197;&#20986;&#29616;&#22312;&#32593;&#32476;&#12289;&#31227;&#21160;&#21644;&#29289;&#32852;&#32593;&#29615;&#22659;&#20013;&#12290;&#36825;&#20123;&#20915;&#31574;&#21487;&#20197;&#20197;&#21508;&#31181;&#24418;&#24335;&#20986;&#29616;&#8212;&#8212;&#27604;&#22914;&#35774;&#32622;&#38544;&#31169;&#26435;&#38480;&#25110;&#20559;&#22909;&#12289;&#22238;&#31572;&#21516;&#24847;&#35831;&#27714;&#25110;&#24178;&#39044;&#24182;&#8220;&#25298;&#32477;&#8221;&#22788;&#29702;&#20010;&#20154;&#25968;&#25454;&#8212;&#8212;&#27599;&#19968;&#31181;&#37117;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#30340;&#27861;&#24459;&#24433;&#21709;&#12290;&#38024;&#23545;&#25152;&#26377;&#24773;&#20917;&#21644;&#25152;&#26377;&#31867;&#22411;&#30340;&#20915;&#31574;&#65292;&#23398;&#32773;&#21644;&#24037;&#19994;&#30028;&#19968;&#30452;&#22312;&#25552;&#20986;&#19981;&#21516;&#23618;&#27425;&#30340;&#24037;&#20855;&#65292;&#20197;&#25913;&#21892;&#38544;&#31169;&#20915;&#31574;&#27969;&#31243;&#30340;&#33258;&#21160;&#21270;&#65292;&#20197;&#25552;&#39640;&#26131;&#29992;&#24615;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#33258;&#21160;&#21270;&#38544;&#31169;&#20915;&#31574;&#25152;&#28041;&#21450;&#21040;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#24182;&#20998;&#31867;&#20171;&#32461;&#20102;&#29616;&#26377;&#21644;&#39044;&#26399;&#30340;&#24037;&#20316;&#21644;&#25552;&#35758;&#65292;&#36825;&#20123;&#24037;&#20316;&#21644;&#25552;&#35758;&#37117;&#33268;&#21147;&#20110;&#23454;&#29616;&#38544;&#31169;&#20915;&#31574;&#30340;&#33258;&#21160;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Users are often overwhelmed by privacy decisions to manage their personal data, which can happen on the web, in mobile, and in IoT environments. These decisions can take various forms -- such as decisions for setting privacy permissions or privacy preferences, decisions responding to consent requests, or to intervene and ``reject'' processing of one's personal data --, and each can have different legal impacts. In all cases and for all types of decisions, scholars and industry have been proposing tools to better automate the process of privacy decisions at different levels, in order to enhance usability. We provide in this paper an overview of the main challenges raised by the automation of privacy decisions, together with a classification scheme of the existing and envisioned work and proposals addressing automation of privacy decisions.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#30693;&#35782;&#22270;&#35889;&#30340;&#32467;&#26500;&#21644;&#21160;&#24577;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#34920;&#23618;&#24615;&#26469;&#31616;&#21333;&#24314;&#27169;&#22797;&#26434;&#29305;&#24449;&#65292;&#20174;&#32780;&#25484;&#25569;&#20840;&#23616;&#30693;&#35782;&#20998;&#24067;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.08116</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#30340;&#32467;&#26500;&#21644;&#21160;&#24577;&#65292;&#20197;&#21450;&#20854;&#34920;&#23618;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
The Structure and Dynamics of Knowledge Graphs, with Superficiality. (arXiv:2305.08116v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08116
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#30693;&#35782;&#22270;&#35889;&#30340;&#32467;&#26500;&#21644;&#21160;&#24577;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#34920;&#23618;&#24615;&#26469;&#31616;&#21333;&#24314;&#27169;&#22797;&#26434;&#29305;&#24449;&#65292;&#20174;&#32780;&#25484;&#25569;&#20840;&#23616;&#30693;&#35782;&#20998;&#24067;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#30693;&#35782;&#22270;&#35889;&#32508;&#21512;&#20102;&#20174;&#23398;&#26415;&#26426;&#26500;&#21644;&#20225;&#19994;&#21040;&#22823;&#20247;&#38598;&#36164;&#31561;&#39033;&#30446;&#20013;&#33719;&#24471;&#30340;&#20154;&#31867;&#30693;&#35782;&#65292;&#27599;&#20004;&#20010;&#33410;&#28857;&#20043;&#38388;&#30340;&#20851;&#31995;&#20195;&#34920;&#36825;&#20004;&#20010;&#23454;&#20307;&#20043;&#38388;&#30340;&#22522;&#26412;&#20107;&#23454;&#12290;&#20851;&#31995;&#35821;&#20041;&#30340;&#22810;&#26679;&#24615;&#32452;&#25104;&#20102;&#30693;&#35782;&#22270;&#35889;&#30340;&#20016;&#23500;&#24615;&#65292;&#23548;&#33268;&#20986;&#29616;&#26377;&#26102;&#28151;&#20081;&#30340;&#22855;&#24322;&#25299;&#25169;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22797;&#26434;&#29305;&#24449;&#21487;&#20197;&#36890;&#36807;&#24341;&#20837;&#34920;&#23618;&#24615;&#30340;&#27010;&#24565;&#26469;&#31616;&#21333;&#24314;&#27169;&#65292;&#34920;&#23618;&#24615;&#25511;&#21046;&#30528;&#29420;&#31435;&#29983;&#25104;&#20107;&#23454;&#30340;&#20851;&#31995;&#20043;&#38388;&#30340;&#37325;&#21472;&#24773;&#20917;&#65292;&#20063;&#36890;&#36807;&#30830;&#23450;&#38169;&#35823;&#25551;&#36848;&#23454;&#20307;&#30340;&#27604;&#20363;&#26469;&#25511;&#21046;&#20840;&#23616;&#30693;&#35782;&#20998;&#24067;&#30340;&#24179;&#34913;&#12290;&#36825;&#26159;&#30693;&#35782;&#22270;&#35889;&#32467;&#26500;&#21644;&#21160;&#24577;&#26041;&#38754;&#30340;&#39318;&#20010;&#27169;&#22411;&#65292;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;&#27491;&#24335;&#30693;&#35782;&#33719;&#21462;&#21644;&#32452;&#32455;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large knowledge graphs combine human knowledge garnered from projects ranging from academia and institutions to enterprises and crowdsourcing. Within such graphs, each relationship between two nodes represents a basic fact involving these two entities. The diversity of the semantics of relationships constitutes the richness of knowledge graphs, leading to the emergence of singular topologies, sometimes chaotic in appearance. However, this complex characteristic can be modeled in a simple way by introducing the concept of superficiality, which controls the overlap between relationships whose facts are generated independently. Superficiality also regulates the balance of the global distribution of knowledge by determining the proportion of misdescribed entities. This is the first model for the structure and dynamics of knowledge graphs. It leads to a better understanding of formal knowledge acquisition and organization.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;+&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#36328;&#22330;&#26223;&#25163;&#21183;&#35782;&#21035;&#20219;&#21153;&#20013;&#26377;&#25928;&#35299;&#20915;&#25968;&#25454;&#21464;&#24322;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.08014</link><description>&lt;p&gt;
&#22522;&#20110;&#34920;&#38754;&#32908;&#30005;&#22270;&#20687;&#30340;&#36731;&#37327;&#32423;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#36328;&#22330;&#26223;&#25163;&#21183;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Surface EMG-Based Inter-Session/Inter-Subject Gesture Recognition by Leveraging Lightweight All-ConvNet and Transfer Learning. (arXiv:2305.08014v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;+&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#36328;&#22330;&#26223;&#25163;&#21183;&#35782;&#21035;&#20219;&#21153;&#20013;&#26377;&#25928;&#35299;&#20915;&#25968;&#25454;&#21464;&#24322;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#20302;&#20998;&#36776;&#29575;&#30636;&#26102;&#39640;&#28165;&#32908;&#30005;&#22270;&#20687;&#36827;&#34892;&#25163;&#21183;&#35782;&#21035;&#21487;&#20197;&#24320;&#36767;&#21457;&#23637;&#26356;&#27969;&#30021;&#12289;&#26356;&#33258;&#28982;&#30340;&#32908;&#32905;-&#35745;&#31639;&#26426;&#30028;&#38754;&#30340;&#26032;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#36328;&#22330;&#26223;&#25968;&#25454;&#30340;&#21464;&#24322;&#24615;&#23384;&#22312;&#26497;&#22823;&#30340;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#37319;&#29992;&#38750;&#24120;&#22823;&#19988;&#22797;&#26434;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25110;&#22522;&#20110;2SRNN&#30340;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#65292;&#26469;&#36924;&#36817;&#30001;&#36825;&#20123;&#36328;&#22330;&#26223;&#25968;&#25454;&#21464;&#24322;&#24615;&#24341;&#36215;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#26041;&#27861;&#20063;&#38656;&#35201;&#22312;&#39044;&#35757;&#32451;&#21644;&#36866;&#24212;&#38454;&#27573;&#20013;&#22312;&#25968;&#30334;&#19975;&#20010;&#35757;&#32451;&#21442;&#25968;&#21644;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23398;&#20064;&#12290;&#32467;&#26524;&#65292;&#36825;&#20351;&#24471;&#22312;&#23454;&#26102;&#24212;&#29992;&#20013;&#36827;&#34892;&#39640;&#31471;&#36164;&#28304;&#32422;&#26463;&#21644;&#35745;&#31639;&#38750;&#24120;&#26114;&#36149;&#30340;&#37096;&#32626;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;+&#36801;&#31227;&#23398;&#20064;&#27169;&#22411;&#65292;&#21033;&#29992;&#36731;&#37327;&#32423;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#36801;&#31227;&#23398;&#20064;(TL)&#26469;&#22686;&#24378;&#36328;&#22330;&#26223;&#25163;&#21183;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gesture recognition using low-resolution instantaneous HD-sEMG images opens up new avenues for the development of more fluid and natural muscle-computer interfaces. However, the data variability between inter-session and inter-subject scenarios presents a great challenge. The existing approaches employed very large and complex deep ConvNet or 2SRNN-based domain adaptation methods to approximate the distribution shift caused by these inter-session and inter-subject data variability. Hence, these methods also require learning over millions of training parameters and a large pre-trained and target domain dataset in both the pre-training and adaptation stages. As a result, it makes high-end resource-bounded and computationally very expensive for deployment in real-time applications. To overcome this problem, we propose a lightweight All-ConvNet+TL model that leverages lightweight All-ConvNet and transfer learning (TL) for the enhancement of inter-session and inter-subject gesture recogniti
&lt;/p&gt;</description></item><item><title>AI&#35299;&#37322;&#21482;&#26377;&#22312;&#20801;&#35768;&#20154;&#31867;&#20915;&#31574;&#32773;&#39564;&#35777;AI&#39044;&#27979;&#30340;&#27491;&#30830;&#24615;&#26102;&#25165;&#26377;&#29992;&#65292;&#32780;&#22823;&#22810;&#25968;&#20915;&#31574;&#29615;&#22659;&#26080;&#27861;&#36827;&#34892;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.07722</link><description>&lt;p&gt;
&#23547;&#27714;&#21487;&#39564;&#35777;&#24615;: &#35299;&#37322;&#24456;&#23569;&#33021;&#22815;&#22312;AI&#36741;&#21161;&#20915;&#31574;&#20013;&#25552;&#39640;&#20915;&#31574;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
In Search of Verifiability: Explanations Rarely Enable Complementary Performance in AI-Advised Decision Making. (arXiv:2305.07722v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07722
&lt;/p&gt;
&lt;p&gt;
AI&#35299;&#37322;&#21482;&#26377;&#22312;&#20801;&#35768;&#20154;&#31867;&#20915;&#31574;&#32773;&#39564;&#35777;AI&#39044;&#27979;&#30340;&#27491;&#30830;&#24615;&#26102;&#25165;&#26377;&#29992;&#65292;&#32780;&#22823;&#22810;&#25968;&#20915;&#31574;&#29615;&#22659;&#26080;&#27861;&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#20851;&#20110;AI&#36741;&#21161;&#20915;&#31574;&#30340;&#25991;&#29486;&#65292;&#28041;&#21450;&#21487;&#35299;&#37322;&#30340;AI&#31995;&#32479;&#20026;&#20154;&#31867;&#20915;&#31574;&#32773;&#25552;&#20379;&#24314;&#35758;&#65292;&#24182;&#21576;&#29616;&#20986;&#19968;&#31995;&#21015;&#19981;&#30830;&#23450;&#21644;&#20196;&#20154;&#22256;&#24785;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#32508;&#21512;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#29702;&#35770;&#65292;&#38416;&#26126;&#20102;AI&#35299;&#37322;&#32463;&#24120;&#26080;&#27861;&#20419;&#20351;&#36866;&#24403;&#30340;&#20381;&#36182;&#21644;&#20114;&#34917;&#20915;&#31574;&#34920;&#29616;&#30340;&#22833;&#36133;&#12290;&#25105;&#20204;&#35748;&#20026;&#35299;&#37322;&#21482;&#26377;&#22312;&#20801;&#35768;&#20154;&#31867;&#20915;&#31574;&#32773;&#39564;&#35777;AI&#39044;&#27979;&#30340;&#27491;&#30830;&#24615;&#26102;&#25165;&#26377;&#29992;&#65292;&#32780;&#19981;&#26159;&#20854;&#20182;&#26399;&#26395;&#65292;&#20363;&#22914;&#21487;&#35299;&#37322;&#24615;&#25110;&#28165;&#26224;&#38416;&#36848;AI&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#35768;&#22810;&#20915;&#31574;&#29615;&#22659;&#20013;&#65292;AI&#35299;&#37322;&#24182;&#26410;&#20419;&#36827;&#36825;&#31181;&#39564;&#35777;&#12290;&#27492;&#22806;&#65292;&#26080;&#35770;&#35299;&#37322;&#26041;&#27861;&#22914;&#20309;&#65292;&#22823;&#22810;&#25968;&#29615;&#22659;&#22522;&#26412;&#19978;&#37117;&#26080;&#27861;&#36827;&#34892;&#39564;&#35777;&#12290;&#25105;&#20204;&#26368;&#21518;&#35752;&#35770;&#20102;&#26356;&#26377;&#25928;&#30340;&#21487;&#35299;&#37322;AI&#36741;&#21161;&#20915;&#31574;&#21644;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#30340;&#28508;&#22312;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current literature on AI-advised decision making -- involving explainable AI systems advising human decision makers -- presents a series of inconclusive and confounding results. To synthesize these findings, we propose a simple theory that elucidates the frequent failure of AI explanations to engender appropriate reliance and complementary decision making performance. We argue explanations are only useful to the extent that they allow a human decision maker to verify the correctness of an AI's prediction, in contrast to other desiderata, e.g., interpretability or spelling out the AI's reasoning process. Prior studies find in many decision making contexts AI explanations do not facilitate such verification. Moreover, most contexts fundamentally do not allow verification, regardless of explanation method. We conclude with a discussion of potential approaches for more effective explainable AI-advised decision making and human-AI collaboration.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#38376;&#25511;&#27719;&#24635;&#27169;&#22359;&#30340;&#20540;&#36845;&#20195;&#32593;&#32476;&#65288;GS-VIN&#65289;&#26469;&#35299;&#20915;&#20540;&#36845;&#20195;&#32593;&#32476;&#22312;&#22788;&#29702;&#26356;&#22823;&#30340;&#36755;&#20837;&#22320;&#22270;&#21644;&#20943;&#36731;&#32047;&#31215;&#35823;&#24046;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#33258;&#36866;&#24212;&#36845;&#20195;&#31574;&#30053;&#21644;&#38376;&#25511;&#27719;&#24635;&#27169;&#22359;&#65292;&#36825;&#31181;&#27169;&#22411;&#21487;&#20197;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#20943;&#23569;&#32593;&#32476;&#28145;&#24230;&#24182;&#25552;&#39640;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.07039</link><description>&lt;p&gt;
&#20855;&#26377;&#38376;&#25511;&#27719;&#24635;&#27169;&#22359;&#30340;&#20540;&#36845;&#20195;&#32593;&#32476;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Value Iteration Networks with Gated Summarization Module. (arXiv:2305.07039v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07039
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#38376;&#25511;&#27719;&#24635;&#27169;&#22359;&#30340;&#20540;&#36845;&#20195;&#32593;&#32476;&#65288;GS-VIN&#65289;&#26469;&#35299;&#20915;&#20540;&#36845;&#20195;&#32593;&#32476;&#22312;&#22788;&#29702;&#26356;&#22823;&#30340;&#36755;&#20837;&#22320;&#22270;&#21644;&#20943;&#36731;&#32047;&#31215;&#35823;&#24046;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#33258;&#36866;&#24212;&#36845;&#20195;&#31574;&#30053;&#21644;&#38376;&#25511;&#27719;&#24635;&#27169;&#22359;&#65292;&#36825;&#31181;&#27169;&#22411;&#21487;&#20197;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#20943;&#23569;&#32593;&#32476;&#28145;&#24230;&#24182;&#25552;&#39640;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20540;&#36845;&#20195;&#32593;&#32476;&#65288;VIN&#65289;&#22312;&#22788;&#29702;&#26356;&#22823;&#30340;&#36755;&#20837;&#22320;&#22270;&#65292;&#20943;&#36731;&#30001;&#22686;&#21152;&#36845;&#20195;&#27425;&#25968;&#24341;&#36215;&#30340;&#32047;&#31215;&#35823;&#24046;&#30340;&#25361;&#25112;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;&#20855;&#26377;&#38376;&#25511;&#27719;&#24635;&#27169;&#22359;&#30340;&#20540;&#36845;&#20195;&#32593;&#32476;&#65288;GS-VIN&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#33258;&#36866;&#24212;&#36845;&#20195;&#31574;&#30053;&#65292;&#21033;&#29992;&#26356;&#22823;&#30340;&#21367;&#31215;&#26680;&#20943;&#23569;&#36845;&#20195;&#27425;&#25968;&#65292;&#20943;&#23569;&#32593;&#32476;&#28145;&#24230;&#65292;&#25552;&#39640;&#35757;&#32451;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#35745;&#21010;&#36807;&#31243;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#38376;&#25511;&#27719;&#24635;&#27169;&#22359;&#65292;&#20351;&#24471;&#32593;&#32476;&#21487;&#20197;&#24378;&#35843;&#25972;&#20010;&#35268;&#21010;&#36807;&#31243;&#65292;&#32780;&#19981;&#20165;&#20165;&#20381;&#36182;&#20110;&#26368;&#32456;&#30340;&#20840;&#23616;&#35268;&#21010;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we address the challenges faced by Value Iteration Networks (VIN) in handling larger input maps and mitigating the impact of accumulated errors caused by increased iterations. We propose a novel approach, Value Iteration Networks with Gated Summarization Module (GS-VIN), which incorporates two main improvements: (1) employing an Adaptive Iteration Strategy in the Value Iteration module to reduce the number of iterations, and (2) introducing a Gated Summarization module to summarize the iterative process. The adaptive iteration strategy uses larger convolution kernels with fewer iteration times, reducing network depth and increasing training stability while maintaining the accuracy of the planning process. The gated summarization module enables the network to emphasize the entire planning process, rather than solely relying on the final global planning outcome, by temporally and spatially resampling the entire planning process within the VI module. We conduct experiments 
&lt;/p&gt;</description></item><item><title>&#33258;&#20027;GIS&#26159;&#19968;&#31181;AI&#21160;&#21147;&#22320;&#29702;&#20449;&#24687;&#31995;&#32479;&#65292;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25512;&#29702;&#26680;&#24515;&#65292;&#20855;&#26377;&#33258;&#21160;&#31354;&#38388;&#25968;&#25454;&#25910;&#38598;&#12289;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#30340;&#33021;&#21147;&#65292;&#26088;&#22312;&#23454;&#29616;&#20116;&#20010;&#33258;&#20027;&#30446;&#26631;&#65306;&#33258;&#21160;&#29983;&#25104;&#12289;&#33258;&#32452;&#32455;&#12289;&#33258;&#39564;&#35777;&#12289;&#33258;&#25191;&#34892;&#21644;&#33258;&#29983;&#38271;&#12290;</title><link>http://arxiv.org/abs/2305.06453</link><description>&lt;p&gt;
&#33258;&#20027;GIS&#65306;&#19979;&#19968;&#20195;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;GIS
&lt;/p&gt;
&lt;p&gt;
Autonomous GIS: the next-generation AI-powered GIS. (arXiv:2305.06453v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06453
&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;GIS&#26159;&#19968;&#31181;AI&#21160;&#21147;&#22320;&#29702;&#20449;&#24687;&#31995;&#32479;&#65292;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25512;&#29702;&#26680;&#24515;&#65292;&#20855;&#26377;&#33258;&#21160;&#31354;&#38388;&#25968;&#25454;&#25910;&#38598;&#12289;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#30340;&#33021;&#21147;&#65292;&#26088;&#22312;&#23454;&#29616;&#20116;&#20010;&#33258;&#20027;&#30446;&#26631;&#65306;&#33258;&#21160;&#29983;&#25104;&#12289;&#33258;&#32452;&#32455;&#12289;&#33258;&#39564;&#35777;&#12289;&#33258;&#25191;&#34892;&#21644;&#33258;&#29983;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914; ChatGPT &#65292;&#23637;&#31034;&#20102;&#23545;&#20154;&#31867;&#33258;&#28982;&#35821;&#35328;&#30340;&#24378;&#22823;&#29702;&#35299;&#33021;&#21147;&#65292;&#22312;&#25512;&#29702;&#12289;&#21019;&#36896;&#24615;&#20889;&#20316;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#32763;&#35793;&#21644;&#20449;&#24687;&#26816;&#32034;&#31561;&#39046;&#22495;&#24471;&#21040;&#20102;&#24212;&#29992;&#19982;&#25506;&#32034;&#12290;&#25105;&#20204;&#37319;&#29992;LLM&#20316;&#20026;&#25512;&#29702;&#26680;&#24515;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20043;&#20026;&#8220;&#33258;&#20027;GIS&#8221;&#30340;AI&#21160;&#21147;&#22320;&#29702;&#20449;&#24687;&#31995;&#32479;&#65288;GIS&#65289;&#65292;&#20197;&#33258;&#21160;&#31354;&#38388;&#25968;&#25454;&#25910;&#38598;&#12289;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#26469;&#35299;&#20915;&#31354;&#38388;&#38382;&#39064;&#12290;&#25105;&#20204;&#35774;&#24819;&#65292;&#33258;&#20027;GIS&#23558;&#38656;&#35201;&#23454;&#29616;&#20116;&#20010;&#33258;&#20027;&#30446;&#26631;&#65292;&#21253;&#25324;&#33258;&#21160;&#29983;&#25104;&#12289;&#33258;&#32452;&#32455;&#12289;&#33258;&#39564;&#35777;&#12289;&#33258;&#25191;&#34892;&#21644;&#33258;&#29983;&#38271;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#20027;GIS&#30340;&#35774;&#35745;&#21407;&#21017;&#26469;&#23454;&#29616;&#36825;&#20116;&#20010;&#33258;&#20027;&#30446;&#26631;&#65292;&#20174;&#20449;&#24687;&#20805;&#20998;&#24615;&#12289;LLM&#33021;&#21147;&#21644;&#20195;&#29702;&#26550;&#26500;&#19977;&#20010;&#26041;&#38754;&#36827;&#34892;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21407;&#22411;&#31995;&#32479;&#31216;&#20026;LLM-Geo &#65292;&#23427;&#22312;Python&#29615;&#22659;&#20013;&#20351;&#29992;GPT-4 API&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), such as ChatGPT, demonstrate a strong understanding of human natural language and have been explored and applied in various fields, including reasoning, creative writing, code generation, translation, and information retrieval. By adopting LLM as the reasoning core, we propose Autonomous GIS, an AI-powered geographic information system (GIS) that leverages the LLM's general abilities in natural language understanding, reasoning and coding for addressing spatial problems with automatic spatial data collection, analysis and visualization. We envision that autonomous GIS will need to achieve five autonomous goals including self-generating, self-organizing, self-verifying, self-executing, and self-growing. We introduce the design principles of autonomous GIS to achieve these five autonomous goals from the aspects of information sufficiency, LLM ability, and agent architecture. We developed a prototype system called LLM-Geo using GPT-4 API in a Python environme
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#35821;&#20041;&#23884;&#20837;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#31354;&#38388;&#24863;&#30693;&#30340;&#35821;&#20041;&#29305;&#24449;&#21644;&#22522;&#20110;&#36890;&#36947;&#30340;&#27880;&#24847;&#21147;&#27169;&#22411;&#26469;&#25552;&#39640;&#22810;&#26631;&#31614;&#39044;&#27979;&#30340;&#27169;&#22411;&#24615;&#33021;&#65292;&#24179;&#22343;&#30456;&#23545;&#25913;&#36827;&#36798;&#21040;15.27%&#12290;</title><link>http://arxiv.org/abs/2305.05228</link><description>&lt;p&gt;
&#35821;&#20041;&#23884;&#20837;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65306;&#19968;&#31181;&#25552;&#21319;&#22810;&#26631;&#31614;&#22270;&#20687;&#20998;&#31867;&#24615;&#33021;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic Embedded Deep Neural Network: A Generic Approach to Boost Multi-Label Image Classification Performance. (arXiv:2305.05228v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#35821;&#20041;&#23884;&#20837;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#31354;&#38388;&#24863;&#30693;&#30340;&#35821;&#20041;&#29305;&#24449;&#21644;&#22522;&#20110;&#36890;&#36947;&#30340;&#27880;&#24847;&#21147;&#27169;&#22411;&#26469;&#25552;&#39640;&#22810;&#26631;&#31614;&#39044;&#27979;&#30340;&#27169;&#22411;&#24615;&#33021;&#65292;&#24179;&#22343;&#30456;&#23545;&#25913;&#36827;&#36798;&#21040;15.27%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#32454;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#27169;&#22411;&#22312;&#20122;&#39532;&#36874;&#29983;&#20135;&#21151;&#33021;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#20363;&#22914;&#22522;&#20110;&#35270;&#35273;&#30340;&#26631;&#31614;&#39044;&#27979;&#65292;&#20174;&#26102;&#23578;&#23646;&#24615;&#26816;&#27979;&#21040;&#21697;&#29260;&#35782;&#21035;&#12290;&#23454;&#29616;&#36825;&#20123;&#20998;&#31867;&#20219;&#21153;&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#37326;&#22806;&#35270;&#35273;&#32972;&#26223;&#20449;&#21495;&#65292;&#20854;&#20013;&#21253;&#21547;&#28151;&#28102;&#27169;&#22411;&#30340;&#26080;&#20851;&#20687;&#32032;&#65292;&#20351;&#27169;&#22411;&#38590;&#20197;&#19987;&#27880;&#20110;&#24863;&#20852;&#36259;&#21306;&#22495;&#24182;&#26681;&#25454;&#35813;&#29305;&#23450;&#21306;&#22495;&#36827;&#34892;&#39044;&#27979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#35821;&#20041;&#23884;&#20837;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#24212;&#29992;&#31354;&#38388;&#24863;&#30693;&#30340;&#35821;&#20041;&#29305;&#24449;&#65292;&#24182;&#32467;&#21512;&#22522;&#20110;&#36890;&#36947;&#30340;&#27880;&#24847;&#21147;&#27169;&#22411;&#26469;&#21033;&#29992;&#23450;&#20301;&#24341;&#23548;&#65292;&#20197;&#25552;&#39640;&#22810;&#26631;&#31614;&#39044;&#27979;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#19982;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25152;&#26377;&#26631;&#31614;&#30340;AUC&#24471;&#20998;&#30340;&#24179;&#22343;&#30456;&#23545;&#25913;&#36827;&#20026;15.27%&#12290;&#26680;&#24515;&#23454;&#39564;&#21644;&#28040;&#34701;&#30740;&#31350;&#28041;&#21450;&#23545;Instagram&#26102;&#23578;&#26381;&#35013;&#30340;&#22810;&#26631;&#31614;&#26102;&#23578;&#23646;&#24615;&#20998;&#31867;&#36827;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-grained multi-label classification models have broad applications in Amazon production features, such as visual based label predictions ranging from fashion attribute detection to brand recognition. One challenge to achieve satisfactory performance for those classification tasks in real world is the wild visual background signal that contains irrelevant pixels which confuses model to focus onto the region of interest and make prediction upon the specific region. In this paper, we introduce a generic semantic- embedding deep neural network to apply the spatial awareness semantic feature incorporating a channel- wise attention based model to leverage the localization guidance to boost model performance for multi- label prediction. We observed an Avg.relative improvement of 15.27% in terms of AUC score across all labels compared to the baseline approach. Core experiment and ablation studies involve multi-label fashion attribute classification performed on Instagram fashion apparels' 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#21152;&#26435;&#30446;&#26631;&#20989;&#25968;&#65292;&#29992;&#20110;&#25552;&#39640;&#22810;&#26631;&#31614;&#20998;&#31867;&#20013;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#38024;&#23545;&#38271;&#23614;&#25968;&#25454;&#20998;&#24067;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#23545;&#26102;&#23578;&#26381;&#35013;&#30340;&#22270;&#20687;&#23646;&#24615;&#20998;&#31867;&#30340;&#23454;&#39564;&#65292;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.04379</link><description>&lt;p&gt;
&#38024;&#23545;&#26102;&#23578;&#26816;&#27979;&#30340;&#19981;&#24179;&#34913;&#26631;&#31614;&#26679;&#26412;&#20998;&#24067;&#30340;&#25968;&#25454;&#39640;&#25928;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Data Efficient Training with Imbalanced Label Sample Distribution for Fashion Detection. (arXiv:2305.04379v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#21152;&#26435;&#30446;&#26631;&#20989;&#25968;&#65292;&#29992;&#20110;&#25552;&#39640;&#22810;&#26631;&#31614;&#20998;&#31867;&#20013;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#38024;&#23545;&#38271;&#23614;&#25968;&#25454;&#20998;&#24067;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#23545;&#26102;&#23578;&#26381;&#35013;&#30340;&#22270;&#20687;&#23646;&#24615;&#20998;&#31867;&#30340;&#23454;&#39564;&#65292;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26631;&#31614;&#20998;&#31867;&#27169;&#22411;&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#22522;&#20110;&#35270;&#35273;&#30340;&#26631;&#31614;&#39044;&#27979;&#20197;&#21450;&#22522;&#20110;&#35821;&#35328;&#30340;&#24773;&#24863;&#20998;&#31867;&#12290;&#23454;&#29616;&#36825;&#20123;&#20219;&#21153;&#30340;&#19968;&#20010;&#20027;&#35201;&#38590;&#28857;&#26159;&#25968;&#25454;&#20998;&#24067;&#30340;&#26174;&#33879;&#19981;&#24179;&#34913;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25506;&#32034;&#20102;&#26356;&#22810;&#30340;&#25968;&#25454;&#39640;&#25928;&#27169;&#22411;&#35757;&#32451;&#25216;&#26415;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#21152;&#26435;&#30446;&#26631;&#20989;&#25968;&#65292;&#29992;&#20110;&#25552;&#39640;&#22810;&#26631;&#31614;&#20998;&#31867;&#20013;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#38024;&#23545;&#38271;&#23614;&#25968;&#25454;&#20998;&#24067;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#28041;&#21450;&#26102;&#23578;&#26381;&#35013;&#30340;&#22522;&#20110;&#22270;&#20687;&#30340;&#23646;&#24615;&#20998;&#31867;&#65292;&#24182;&#19988;&#32467;&#26524;&#34920;&#26126;&#65292;&#26032;&#30340;&#21152;&#26435;&#30446;&#26631;&#20989;&#25968;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-label classification models have a wide range of applications in E-commerce, including visual-based label predictions and language-based sentiment classifications. A major challenge in achieving satisfactory performance for these tasks in the real world is the notable imbalance in data distribution. For instance, in fashion attribute detection, there may be only six 'puff sleeve' clothes among 1000 products in most E-commerce fashion catalogs. To address this issue, we explore more data-efficient model training techniques rather than acquiring a huge amount of annotations to collect sufficient samples, which is neither economic nor scalable. In this paper, we propose a state-of-the-art weighted objective function to boost the performance of deep neural networks (DNNs) for multi-label classification with long-tailed data distribution. Our experiments involve image-based attribute classification of fashion apparels, and the results demonstrate favorable performance for the new weig
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Ansible Wisdom&#30340;&#33258;&#28982;&#35821;&#35328;&#36716;Ansible-YAML&#20195;&#30721;&#30340;&#24037;&#20855;&#65292;&#21487;&#33258;&#21160;&#21270;&#29983;&#25104;Ansible&#33050;&#26412;&#65292;&#25552;&#39640;IT&#33258;&#21160;&#21270;&#29983;&#20135;&#21147;&#65292;&#24182;&#30456;&#27604;&#29616;&#26377;&#25216;&#26415;&#36798;&#21040;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2305.02783</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#25216;&#26415;&#20219;&#21153;&#20013;&#33258;&#21160;&#29983;&#25104;YAML&#20195;&#30721;
&lt;/p&gt;
&lt;p&gt;
Automated Code generation for Information Technology Tasks in YAML through Large Language Models. (arXiv:2305.02783v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02783
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Ansible Wisdom&#30340;&#33258;&#28982;&#35821;&#35328;&#36716;Ansible-YAML&#20195;&#30721;&#30340;&#24037;&#20855;&#65292;&#21487;&#33258;&#21160;&#21270;&#29983;&#25104;Ansible&#33050;&#26412;&#65292;&#25552;&#39640;IT&#33258;&#21160;&#21270;&#29983;&#20135;&#21147;&#65292;&#24182;&#30456;&#27604;&#29616;&#26377;&#25216;&#26415;&#36798;&#21040;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#30340;&#19981;&#26029;&#25552;&#21319;&#65292;&#22312;&#36890;&#29992;&#32534;&#31243;&#35821;&#35328;&#26041;&#38754;&#30340;&#21463;&#30410;&#26368;&#22823;&#65292;&#32780;&#38024;&#23545;IT&#33258;&#21160;&#21270;&#31561;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#30340;&#30740;&#31350;&#36739;&#23569;&#12290;&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;Ansible-YAML&#30340;&#29983;&#25104;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Ansible Wisdom&#30340;&#33258;&#28982;&#35821;&#35328;&#36716;Ansible-YAML&#20195;&#30721;&#30340;&#24037;&#20855;&#65292;&#26088;&#22312;&#25552;&#39640;IT&#33258;&#21160;&#21270;&#29983;&#20135;&#21147;&#12290;&#30740;&#31350;&#37319;&#29992;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#26032;&#30340;&#21253;&#21547;Ansible-YAML&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#25193;&#23637;&#35757;&#32451;&#12290;&#21516;&#26102;&#65292;&#36824;&#24320;&#21457;&#20102;&#20004;&#20010;&#29992;&#20110;&#25429;&#25417;&#27492;&#39046;&#22495;&#29305;&#24449;&#30340;YAML&#21644;Ansible&#24615;&#33021;&#25351;&#26631;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;Ansible Wisdom&#21487;&#20197;&#31934;&#30830;&#22320;&#20174;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#20013;&#29983;&#25104;Ansible&#33050;&#26412;&#65292;&#24182;&#19988;&#20854;&#24615;&#33021;&#21487;&#19982;&#29616;&#26377;&#25216;&#26415;&#30340;&#29366;&#24577;&#30456;&#23218;&#32654;&#25110;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent improvement in code generation capabilities due to the use of large language models has mainly benefited general purpose programming languages. Domain specific languages, such as the ones used for IT Automation, have received far less attention, despite involving many active developers and being an essential component of modern cloud platforms. This work focuses on the generation of Ansible-YAML, a widely used markup language for IT Automation. We present Ansible Wisdom, a natural-language to Ansible-YAML code generation tool, aimed at improving IT automation productivity. Ansible Wisdom is a transformer-based model, extended by training with a new dataset containing Ansible-YAML. We also develop two novel performance metrics for YAML and Ansible to capture the specific characteristics of this domain. Results show that Ansible Wisdom can accurately generate Ansible script from natural language prompts with performance comparable or better than existing state of the art code 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36816;&#29992;&#26426;&#26800;&#24335;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#25506;&#31350;&#20102;GPT-2 Small&#30340;&#25968;&#23398;&#33021;&#21147;&#65292;&#24182;&#30830;&#23450;&#20102;&#23427;&#30340;&#35745;&#31639;&#22270;&#20013;&#30340;&#19968;&#20010;&#23567;&#30005;&#36335;&#29992;&#20110;&#35745;&#31639;&#22823;&#20110;&#31526;&#21495;&#65292;&#35813;&#30005;&#36335;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#25552;&#39640;&#20102;&#32467;&#26463;&#24180;&#20221;&#22823;&#20110;&#24320;&#22987;&#24180;&#20221;&#30340;&#27010;&#29575;&#65292;&#24182;&#19988;&#35813;&#30005;&#36335;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.00586</link><description>&lt;p&gt;
GPT-2&#26159;&#22914;&#20309;&#35745;&#31639;&#22823;&#20110;&#31526;&#21495;&#30340;&#65311;&#35299;&#37322;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#23398;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. (arXiv:2305.00586v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36816;&#29992;&#26426;&#26800;&#24335;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#25506;&#31350;&#20102;GPT-2 Small&#30340;&#25968;&#23398;&#33021;&#21147;&#65292;&#24182;&#30830;&#23450;&#20102;&#23427;&#30340;&#35745;&#31639;&#22270;&#20013;&#30340;&#19968;&#20010;&#23567;&#30005;&#36335;&#29992;&#20110;&#35745;&#31639;&#22823;&#20110;&#31526;&#21495;&#65292;&#35813;&#30005;&#36335;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#25552;&#39640;&#20102;&#32467;&#26463;&#24180;&#20221;&#22823;&#20110;&#24320;&#22987;&#24180;&#20221;&#30340;&#27010;&#29575;&#65292;&#24182;&#19988;&#35813;&#30005;&#36335;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#26410;&#34987;&#26126;&#30830;&#35757;&#32451;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22914;&#20309;&#23454;&#29616;&#36825;&#20123;&#21151;&#33021;&#21364;&#19981;&#20026;&#20154;&#25152;&#30693;&#12290;&#26412;&#25991;&#36890;&#36807;&#26426;&#26800;&#24335;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#25506;&#31350;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#20855;&#26377;&#30340;&#22522;&#26412;&#25968;&#23398;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20197;GPT-2 Small&#20026;&#20363;&#65292;&#30740;&#31350;&#20854;&#33021;&#21542;&#36890;&#36807;&#36755;&#20837;"&#25112;&#20105;&#25345;&#32493;&#26102;&#38388;&#26159;&#20174;1732&#24180;&#21040;17&#24180;"&#65292;&#39044;&#27979;&#20986;&#26377;&#25928;&#30340;&#20004;&#20301;&#25968;&#23383;&#30340;&#25130;&#27490;&#24180;&#20221; (&#22823;&#20110;32&#24180;)&#12290;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;&#19968;&#20010;&#30005;&#36335;&#65292;&#21363;GPT-2 Small&#35745;&#31639;&#22270;&#30340;&#19968;&#20010;&#23567;&#23376;&#38598;&#65292;&#29992;&#20110;&#35745;&#31639;&#36825;&#20010;&#20219;&#21153;&#30340;&#36755;&#20986;&#65292;&#28982;&#21518;&#25105;&#20204;&#35299;&#37322;&#20102;&#27599;&#20010;&#30005;&#36335;&#32452;&#20214;&#30340;&#20316;&#29992;&#65292;&#26174;&#31034;&#20986;GPT-2 Small&#30340;&#26368;&#32456;&#22810;&#23618;&#24863;&#30693;&#22120;&#25552;&#39640;&#20102;&#32467;&#26463;&#24180;&#20221;&#22823;&#20110;&#24320;&#22987;&#24180;&#20221;&#30340;&#27010;&#29575;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#30005;&#36335;&#36866;&#29992;&#20110;&#20854;&#20182;&#20219;&#21153;&#65292;&#22312;&#20854;&#20182;&#22823;&#20110;&#22330;&#26223;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models can be surprisingly adept at tasks they were not explicitly trained on, but how they implement these capabilities is poorly understood. In this paper, we investigate the basic mathematical abilities often acquired by pre-trained language models. Concretely, we use mechanistic interpretability techniques to explain the (limited) mathematical abilities of GPT-2 small. As a case study, we examine its ability to take in sentences such as "The war lasted from the year 1732 to the year 17", and predict valid two-digit end years (years &gt; 32). We first identify a circuit, a small subset of GPT-2 small's computational graph that computes this task's output. Then, we explain the role of each circuit component, showing that GPT-2 small's final multi-layer perceptrons boost the probability of end years greater than the start year. Finally, we show that our circuit generalizes to other tasks, playing a role in other greater-than scenarios.
&lt;/p&gt;</description></item><item><title>LLT&#26159;&#19968;&#20010;R&#21253;&#65292;&#29992;&#20110;&#32447;&#24615;&#23450;&#24459;&#29305;&#24449;&#31354;&#38388;&#21464;&#25442;&#65292;&#21487;&#20197;&#24110;&#21161;&#23545;&#21333;&#21464;&#37327;&#21644;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2304.14211</link><description>&lt;p&gt;
LLT&#65306;&#32447;&#24615;&#23450;&#24459;&#29305;&#24449;&#31354;&#38388;&#21464;&#25442;&#30340;R&#21253;
&lt;/p&gt;
&lt;p&gt;
LLT: An R package for Linear Law-based Feature Space Transformation. (arXiv:2304.14211v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14211
&lt;/p&gt;
&lt;p&gt;
LLT&#26159;&#19968;&#20010;R&#21253;&#65292;&#29992;&#20110;&#32447;&#24615;&#23450;&#24459;&#29305;&#24449;&#31354;&#38388;&#21464;&#25442;&#65292;&#21487;&#20197;&#24110;&#21161;&#23545;&#21333;&#21464;&#37327;&#21644;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#23450;&#24459;&#29305;&#24449;&#31354;&#38388;&#36716;&#25442;(LLT )&#31639;&#27861;&#30340;&#30446;&#26631;&#26159;&#24110;&#21161;&#23545;&#21333;&#21464;&#37327;&#21644;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#20998;&#31867;&#12290;LLT R&#21253;&#20197;&#28789;&#27963;&#21644;&#29992;&#25143;&#21451;&#22909;&#30340;&#26041;&#24335;&#23454;&#29616;&#20102;&#35813;&#31639;&#27861;&#12290;&#35813;&#21253;&#23558;&#23454;&#20363;&#20998;&#20026;&#35757;&#32451;&#21644;&#27979;&#35797;&#38598;&#65292;&#24182;&#21033;&#29992;&#26102;&#24310;&#23884;&#20837;&#21644;&#35889;&#20998;&#35299;&#25216;&#26415;&#65292;&#35782;&#21035;&#35757;&#32451;&#38598;&#20013;&#27599;&#20010;&#36755;&#20837;&#24207;&#21015;(&#21021;&#22987;&#29305;&#24449;)&#30340;&#25511;&#21046;&#27169;&#24335;(&#31216;&#20026;&#32447;&#24615;&#23450;&#24459;)&#12290;&#26368;&#21518;&#65292;&#23427;&#24212;&#29992;&#35757;&#32451;&#38598;&#30340;&#32447;&#24615;&#23450;&#24459;&#26469;&#36716;&#25442;&#27979;&#35797;&#38598;&#30340;&#21021;&#22987;&#29305;&#24449;&#12290;trainTest&#12289;trainLaw&#21644;testTrans&#19977;&#20010;&#21333;&#29420;&#30340;&#20989;&#25968;&#26469;&#25191;&#34892;&#36825;&#20123;&#27493;&#39588;&#65292;&#23427;&#20204;&#38656;&#35201;&#39044;&#23450;&#20041;&#30340;&#25968;&#25454;&#32467;&#26500;;&#28982;&#32780;&#65292;&#20026;&#20102;&#24555;&#36895;&#35745;&#31639;&#65292;&#23427;&#20204;&#21482;&#20351;&#29992;&#20869;&#32622;&#20989;&#25968;&#12290;LLT R&#21253;&#21644;&#36866;&#24403;&#25968;&#25454;&#32467;&#26500;&#30340;&#31034;&#20363;&#25968;&#25454;&#38598;&#22312;GitHub&#19978;&#20844;&#24320;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of the linear law-based feature space transformation (LLT) algorithm is to assist with the classification of univariate and multivariate time series. The presented R package, called LLT, implements this algorithm in a flexible yet user-friendly way. This package first splits the instances into training and test sets. It then utilizes time-delay embedding and spectral decomposition techniques to identify the governing patterns (called linear laws) of each input sequence (initial feature) within the training set. Finally, it applies the linear laws of the training set to transform the initial features of the test set. These steps are performed by three separate functions called trainTest, trainLaw, and testTrans. Their application requires a predefined data structure; however, for fast calculation, they use only built-in functions. The LLT R package and a sample dataset with the appropriate data structure are publicly available on GitHub.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36890;&#36807;&#24179;&#34913;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#27169;&#24335;&#26469;&#22686;&#21152;&#27169;&#22411;&#36755;&#20986;&#22810;&#26679;&#24615;&#30340;&#22810;&#26679;&#24615;&#26435;&#37325;&#35757;&#32451;&#26041;&#26696;&#65292;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#38656;&#35201;&#22810;&#26679;&#21270;&#36755;&#20986;&#30340;&#21019;&#24847;&#24212;&#29992;&#65292;&#24182;&#22312;&#21463;&#25511;&#29615;&#22659;&#20013;&#36827;&#34892;&#30340;&#21021;&#27493;&#23454;&#39564;&#23637;&#31034;&#20102;&#20854;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.11961</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#26679;&#24615;&#26435;&#37325;&#23454;&#29616;&#29983;&#25104;&#27169;&#22411;&#30340;&#27169;&#24335;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
Towards Mode Balancing of Generative Models via Diversity Weights. (arXiv:2304.11961v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36890;&#36807;&#24179;&#34913;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#27169;&#24335;&#26469;&#22686;&#21152;&#27169;&#22411;&#36755;&#20986;&#22810;&#26679;&#24615;&#30340;&#22810;&#26679;&#24615;&#26435;&#37325;&#35757;&#32451;&#26041;&#26696;&#65292;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#38656;&#35201;&#22810;&#26679;&#21270;&#36755;&#20986;&#30340;&#21019;&#24847;&#24212;&#29992;&#65292;&#24182;&#22312;&#21463;&#25511;&#29615;&#22659;&#20013;&#36827;&#34892;&#30340;&#21021;&#27493;&#23454;&#39564;&#23637;&#31034;&#20102;&#20854;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#25968;&#25454;&#39537;&#21160;&#30340;&#22270;&#20687;&#27169;&#22411;&#34987;&#24191;&#27867;&#29992;&#20110;&#25903;&#25345;&#21019;&#24847;&#21644;&#33402;&#26415;&#20316;&#21697;&#12290;&#22312;&#24403;&#21069;&#20027;&#23548;&#30340;&#20998;&#24067;&#25311;&#21512;&#33539;&#24335;&#19979;&#65292;&#25968;&#25454;&#38598;&#34987;&#35270;&#20026;&#35201;&#23613;&#21487;&#33021;&#25509;&#36817;&#30340;&#30495;&#23454;&#20540;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#21019;&#24847;&#24212;&#29992;&#38656;&#35201;&#22810;&#26679;&#21270;&#30340;&#36755;&#20986;&#65292;&#21019;&#20316;&#32773;&#32463;&#24120;&#21162;&#21147;&#20174;&#32473;&#23450;&#30340;&#25968;&#25454;&#20998;&#24067;&#20013;&#31215;&#26497;&#20998;&#31163;&#20986;&#26469;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20174;&#32431;&#27169;&#24335;&#35206;&#30422;&#36716;&#21521;&#27169;&#24335;&#24179;&#34913;&#30340;&#24314;&#27169;&#30446;&#26631;&#35843;&#25972;&#26159;&#24517;&#35201;&#30340;&#65292;&#20197;&#36866;&#24212;&#26356;&#39640;&#30340;&#36755;&#20986;&#22810;&#26679;&#24615;&#30446;&#26631;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#26679;&#24615;&#26435;&#37325;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#24179;&#34913;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#27169;&#24335;&#26469;&#22686;&#21152;&#27169;&#22411;&#36755;&#20986;&#22810;&#26679;&#24615;&#30340;&#35757;&#32451;&#26041;&#26696;&#12290;&#22312;&#21463;&#25511;&#29615;&#22659;&#20013;&#36827;&#34892;&#30340;&#21021;&#27493;&#23454;&#39564;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#26041;&#27861;&#19982;&#22810;&#26679;&#24615;&#12289;&#20844;&#24179;&#21644;&#21253;&#23481;&#22312;&#29983;&#25104;&#24335;&#26426;&#22120;&#23398;&#20064;&#20197;&#21450;&#35745;&#31639;&#26426;&#21019;&#24847;&#20013;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#23454;&#29616;&#21487;&#20197;&#22312;https://github.com/&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large data-driven image models are extensively used to support creative and artistic work. Under the currently predominant distribution-fitting paradigm, a dataset is treated as ground truth to be approximated as closely as possible. Yet, many creative applications demand a diverse range of output, and creators often strive to actively diverge from a given data distribution. We argue that an adjustment of modelling objectives, from pure mode coverage towards mode balancing, is necessary to accommodate the goal of higher output diversity. We present diversity weights, a training scheme that increases a model's output diversity by balancing the modes in the training dataset. First experiments in a controlled setting demonstrate the potential of our method. We discuss connections of our approach to diversity, equity, and inclusion in generative machine learning more generally, and computational creativity specifically. An implementation of our algorithm is available at https://github.com/
&lt;/p&gt;</description></item><item><title>GeneGPT&#36890;&#36807;&#23569;&#37327;NCBI API&#35843;&#29992;URL&#35831;&#27714;&#20316;&#20026;&#28436;&#31034;&#65292;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;NCBI Web API&#22238;&#31572;&#22522;&#22240;&#32452;&#38382;&#39064;&#65292;&#24182;&#22312;GeneTuring&#27979;&#35797;&#20013;&#36798;&#21040;&#20102;&#20248;&#24322;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.09667</link><description>&lt;p&gt;
GeneGPT: &#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;NCBI Web API
&lt;/p&gt;
&lt;p&gt;
GeneGPT: Teaching Large Language Models to Use NCBI Web APIs. (arXiv:2304.09667v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09667
&lt;/p&gt;
&lt;p&gt;
GeneGPT&#36890;&#36807;&#23569;&#37327;NCBI API&#35843;&#29992;URL&#35831;&#27714;&#20316;&#20026;&#28436;&#31034;&#65292;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;NCBI Web API&#22238;&#31572;&#22522;&#22240;&#32452;&#38382;&#39064;&#65292;&#24182;&#22312;GeneTuring&#27979;&#35797;&#20013;&#36798;&#21040;&#20102;&#20248;&#24322;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;GeneGPT&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20351;&#29992;&#22269;&#23478;&#29983;&#29289;&#25216;&#26415;&#20449;&#24687;&#20013;&#24515;&#65288;NCBI&#65289;&#30340;Web&#24212;&#29992;&#31243;&#24207;&#32534;&#31243;&#25509;&#21475;&#65288;API&#65289;&#65292;&#24182;&#22238;&#31572;&#22522;&#22240;&#32452;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#23569;&#37327;&#30340;NCBI API&#35843;&#29992;URL&#35831;&#27714;&#20316;&#20026;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#28436;&#31034;&#65292;&#21551;&#21457;Codex&#65288;code-davinci-002&#65289;&#35299;&#20915;GeneTuring&#27979;&#35797;&#12290;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#19968;&#26086;&#26816;&#27979;&#21040;&#35843;&#29992;&#35831;&#27714;&#65292;&#25105;&#20204;&#23601;&#20572;&#27490;&#35299;&#30721;&#24182;&#20351;&#29992;&#29983;&#25104;&#30340;URL&#36827;&#34892;API&#35843;&#29992;&#12290;&#25105;&#20204;&#28982;&#21518;&#23558;NCBI API&#36820;&#22238;&#30340;&#21407;&#22987;&#25191;&#34892;&#32467;&#26524;&#38468;&#21152;&#21040;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#65292;&#24182;&#32487;&#32493;&#29983;&#25104;&#30452;&#21040;&#25214;&#21040;&#31572;&#26696;&#25110;&#26816;&#27979;&#21040;&#21478;&#19968;&#20010;API&#35843;&#29992;&#12290;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;GeneGPT&#22312;GeneTuring&#25968;&#25454;&#38598;&#30340;&#22235;&#20010;One-shot&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#19977;&#20010;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#22312;&#20116;&#20010;Zero-shot&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#22235;&#20010;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;GeneGPT&#30340;&#23439;&#24179;&#22343;&#20998;&#25968;&#20026;0.76&#65292;&#36828;&#39640;&#20110;&#26816;&#32034;&#22686;&#24378;LLM&#65292;&#22914;New Bin&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present GeneGPT, a novel method for teaching large language models (LLMs) to use the Web Application Programming Interfaces (APIs) of the National Center for Biotechnology Information (NCBI) and answer genomics questions. Specifically, we prompt Codex (code-davinci-002) to solve the GeneTuring tests with few-shot URL requests of NCBI API calls as demonstrations for in-context learning. During inference, we stop the decoding once a call request is detected and make the API call with the generated URL. We then append the raw execution results returned by NCBI APIs to the generated texts and continue the generation until the answer is found or another API call is detected. Our preliminary results show that GeneGPT achieves state-of-the-art results on three out of four one-shot tasks and four out of five zero-shot tasks in the GeneTuring dataset. Overall, GeneGPT achieves a macro-average score of 0.76, which is much higher than retrieval-augmented LLMs such as the New Bin
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;SemiMemes&#65292;&#20027;&#35201;&#24212;&#29992;&#20110;Memes&#30340;&#20998;&#26512;&#21644;&#27880;&#37322;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#20248;&#20110;&#20854;&#20182;&#26368;&#26032;&#30340;&#22810;&#27169;&#24577;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.00020</link><description>&lt;p&gt;
SemiMemes&#65306;&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;Memes&#20998;&#26512;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SemiMemes: A Semi-supervised Learning Approach for Multimodal Memes Analysis. (arXiv:2304.00020v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00020
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;SemiMemes&#65292;&#20027;&#35201;&#24212;&#29992;&#20110;Memes&#30340;&#20998;&#26512;&#21644;&#27880;&#37322;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#20248;&#20110;&#20854;&#20182;&#26368;&#26032;&#30340;&#22810;&#27169;&#24577;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#19978;Memes&#30340;&#26222;&#21450;&#24615;&#24341;&#21457;&#20102;&#20998;&#26512;&#20854;&#38544;&#21547;&#21547;&#20041;&#12289;&#23457;&#26597;&#26377;&#23475;&#20869;&#23481;&#30340;&#38656;&#27714;&#12290;&#26426;&#22120;&#23398;&#20064;&#30340;Meme&#23457;&#26597;&#31995;&#32479;&#38656;&#35201;&#21322;&#30417;&#30563;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#21033;&#29992;&#20114;&#32852;&#32593;&#19978;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;Memes&#65292;&#24182;&#20351;&#27880;&#37322;&#36807;&#31243;&#21464;&#24471;&#26356;&#31616;&#21333;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#38656;&#35201;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#22240;&#20026;Memes&#30340;&#21547;&#20041;&#36890;&#24120;&#26469;&#33258;&#22270;&#20687;&#21644;&#25991;&#26412;&#12290;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#21363;&#22810;&#23186;&#20307;&#33258;&#21160;&#24615;&#21035;&#27495;&#35270;&#35782;&#21035;&#21644;&#20196;&#20154;&#35752;&#21388;&#30340;Memes&#25968;&#25454;&#38598;&#19978;&#65292;&#20248;&#20110;&#20854;&#20182;&#22810;&#27169;&#24577;&#21322;&#30417;&#30563;&#21644;&#30417;&#30563;&#23398;&#20064;&#30340;&#26368;&#26032;&#27169;&#22411;&#12290;&#20511;&#37492;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#25152;&#33719;&#24471;&#30340;&#35265;&#35299;&#65292;&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;SemiMemes&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#33258;&#32534;&#30721;&#22120;&#21644;&#20998;&#31867;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
The prevalence of memes on social media has created the need to sentiment analyze their underlying meanings for censoring harmful content. Meme censoring systems by machine learning raise the need for a semi-supervised learning solution to take advantage of the large number of unlabeled memes available on the internet and make the annotation process less challenging. Moreover, the approach needs to utilize multimodal data as memes' meanings usually come from both images and texts. This research proposes a multimodal semi-supervised learning approach that outperforms other multimodal semi-supervised learning and supervised learning state-of-the-art models on two datasets, the Multimedia Automatic Misogyny Identification and Hateful Memes dataset. Building on the insights gained from Contrastive Language-Image Pre-training, which is an effective multimodal learning technique, this research introduces SemiMemes, a novel training method that combines auto-encoder and classification task to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21512;&#25104;&#32463;&#39564;&#22238;&#25918;&#26041;&#27861;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#25968;&#25454;&#21294;&#20047;&#38382;&#39064;&#65292;&#36890;&#36807;&#24039;&#22937;&#24212;&#29992;&#29983;&#25104;&#24314;&#27169;&#25216;&#26415;&#26469;&#25193;&#20805;&#25968;&#25454;&#25928;&#26524;&#26174;&#33879;&#12290;</title><link>http://arxiv.org/abs/2303.06614</link><description>&lt;p&gt;
&#21512;&#25104;&#32463;&#39564;&#22238;&#25918;&#65306;&#26088;&#22312;&#29992;&#25193;&#20805;&#25968;&#25454;&#26469;&#25552;&#39640;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Synthetic Experience Replay. (arXiv:2303.06614v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21512;&#25104;&#32463;&#39564;&#22238;&#25918;&#26041;&#27861;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#25968;&#25454;&#21294;&#20047;&#38382;&#39064;&#65292;&#36890;&#36807;&#24039;&#22937;&#24212;&#29992;&#29983;&#25104;&#24314;&#27169;&#25216;&#26415;&#26469;&#25193;&#20805;&#25968;&#25454;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#30340;&#19968;&#20010;&#20851;&#38190;&#20027;&#39064;&#26159;&#65292;&#24403;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#21644;&#22823;&#22411;&#25968;&#25454;&#38598;&#30456;&#32467;&#21512;&#26102;&#65292;&#23427;&#20204;&#21487;&#20197;&#20135;&#29983;&#20196;&#20154;&#24778;&#24322;&#30340;&#32467;&#26524;&#12290;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36825;&#31181;&#33539;&#24335;&#36890;&#24120;&#36890;&#36807;&#32463;&#39564;&#22238;&#25918;&#23454;&#29616;&#65292;&#20854;&#20013;&#36807;&#21435;&#30340;&#32463;&#39564;&#25968;&#25454;&#38598;&#29992;&#20110;&#35757;&#32451;&#31574;&#30053;&#25110;&#20540;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#19982;&#30417;&#30563;&#23398;&#20064;&#25110;&#33258;&#30417;&#30563;&#23398;&#20064;&#19981;&#21516;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#24517;&#39035;&#25910;&#38598;&#33258;&#24049;&#30340;&#25968;&#25454;&#65292;&#36825;&#36890;&#24120;&#26159;&#26377;&#38480;&#30340;&#12290;&#22240;&#27492;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#22909;&#22788;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#21363;&#20351;&#26159;&#23567;&#22411;&#31070;&#32463;&#32593;&#32476;&#22312;&#35757;&#32451;&#24320;&#22987;&#26102;&#20063;&#21487;&#33021;&#20986;&#29616;&#36807;&#25311;&#21512;&#29616;&#35937;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#29983;&#25104;&#24314;&#27169;&#30340;&#24040;&#22823;&#36827;&#27493;&#65292;&#24182;&#25552;&#20986;&#20102;&#21512;&#25104;&#32463;&#39564;&#22238;&#25918;&#65288;SynthER&#65289;&#65292;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#26469;&#28789;&#27963;&#22320;&#19978;&#37319;&#26679;&#20195;&#29702;&#25910;&#38598;&#30340;&#32463;&#39564;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;SynthER&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#35774;&#32622;&#19979;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#26080;&#35770;&#26159;&#22312;&#24863;&#30693;&#29615;&#22659;&#36824;&#26159;&#22312;&#20687;&#32032;&#29615;&#22659;&#20013;&#12290;&#22312;&#31163;&#32447;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key theme in the past decade has been that when large neural networks and large datasets combine they can produce remarkable results. In deep reinforcement learning (RL), this paradigm is commonly made possible through experience replay, whereby a dataset of past experiences is used to train a policy or value function. However, unlike in supervised or self-supervised learning, an RL agent has to collect its own data, which is often limited. Thus, it is challenging to reap the benefits of deep learning, and even small neural networks can overfit at the start of training. In this work, we leverage the tremendous recent progress in generative modeling and propose Synthetic Experience Replay (SynthER), a diffusion-based approach to flexibly upsample an agent's collected experience. We show that SynthER is an effective method for training RL agents across offline and online settings, in both proprioceptive and pixel-based environments. In offline settings, we observe drastic improvements 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;QD&#31639;&#27861;&#8212;&#8212;&#24102;&#26377;&#31574;&#30053;&#26799;&#24230;&#36741;&#21161;&#21644;&#22522;&#20110;&#25317;&#25380;&#30340;&#25506;&#32034;&#30340;&#22810;&#30446;&#26631;MAP-Elites (MOME-PGX)&#26469;&#25193;&#23637;MOME&#20197;&#25552;&#39640;&#20854;&#25968;&#25454;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.12668</link><description>&lt;p&gt;
&#36890;&#36807;&#26799;&#24230;&#36741;&#21161;&#21644;&#23494;&#38598;&#25506;&#32034;&#25552;&#39640;&#22810;&#30446;&#26631;&#36136;&#37327;&#22810;&#26679;&#24615;&#30340;&#25968;&#25454;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Improving the Data Efficiency of Multi-Objective Quality-Diversity through Gradient Assistance and Crowding Exploration. (arXiv:2302.12668v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;QD&#31639;&#27861;&#8212;&#8212;&#24102;&#26377;&#31574;&#30053;&#26799;&#24230;&#36741;&#21161;&#21644;&#22522;&#20110;&#25317;&#25380;&#30340;&#25506;&#32034;&#30340;&#22810;&#30446;&#26631;MAP-Elites (MOME-PGX)&#26469;&#25193;&#23637;MOME&#20197;&#25552;&#39640;&#20854;&#25968;&#25454;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#20854;&#39640;&#25928;&#36867;&#31163;&#23616;&#37096;&#26368;&#20248;&#21644;&#29983;&#25104;&#24191;&#27867;&#39640;&#25928;&#35299;&#30340;&#33021;&#21147;&#65292;&#36136;&#37327;&#22810;&#26679;&#24615;(QD)&#31639;&#27861;&#24050;&#25104;&#20026;&#20248;&#21270;&#26041;&#27861;&#12290;&#26368;&#36817;&#65292;&#22810;&#30446;&#26631;MAP-Elites (MOME)&#36890;&#36807;&#22312;Map Elites&#32593;&#26684;&#30340;&#27599;&#20010;&#21333;&#20803;&#26684;&#20013;&#32500;&#25252;&#24085;&#32047;&#25176;&#21069;&#27839;&#23558;QD&#33539;&#20363;&#25193;&#23637;&#21040;&#22810;&#30446;&#26631;&#35774;&#32622;&#12290;MOME&#22312;&#21516;&#26102;&#33719;&#24471;&#22810;&#26679;&#30340;&#35299;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#19982;NSGA-II&#21644;SPEA2&#30456;&#31454;&#20105;&#30340;&#20840;&#23616;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;MOME&#21463;&#38750;&#23450;&#21521;&#36951;&#20256;&#25628;&#32034;&#26426;&#21046;&#30340;&#38480;&#21046;&#65292;&#36825;&#20123;&#26426;&#21046;&#22312;&#39640;&#32500;&#25628;&#32034;&#31354;&#38388;&#20013;&#38754;&#20020;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;QD&#31639;&#27861;&#8212;&#8212;&#24102;&#26377;&#31574;&#30053;&#26799;&#24230;&#36741;&#21161;&#21644;&#22522;&#20110;&#25317;&#25380;&#30340;&#25506;&#32034;&#30340;&#22810;&#30446;&#26631;MAP-Elites (MOME-PGX)&#26469;&#25193;&#23637;MOME&#20197;&#25552;&#39640;&#20854;&#25968;&#25454;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;MOME-PGX&#20351;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#26377;&#25928;&#22320;&#23558;&#35299;&#20915;&#26041;&#26696;&#39537;&#21521;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quality-Diversity (QD) algorithms have recently gained traction as optimisation methods due to their effectiveness at escaping local optima and capability of generating wide-ranging and high-performing solutions. Recently, Multi-Objective MAP-Elites (MOME) extended the QD paradigm to the multi-objective setting by maintaining a Pareto front in each cell of a map-elites grid. MOME achieved a global performance that competed with NSGA-II and SPEA2, two well-established Multi-Objective Evolutionary Algorithms (MOEA), while also acquiring a diverse repertoire of solutions. However, MOME is limited by non-directed genetic search mechanisms which struggle in high-dimensional search spaces. In this work, we present Multi-Objective MAP-Elites with Policy-Gradient Assistance and Crowding-based Exploration (MOME-PGX): a new QD algorithm that extends MOME to improve its data efficiency and performance. MOME-PGX uses gradient-based optimisation to efficiently drive solutions towards higher perform
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26102;&#38388;&#24207;&#21015;&#20581;&#24247;&#25968;&#25454;&#32570;&#22833;&#20540;&#22635;&#34917;&#26041;&#27861;&#65292;&#24182;&#22312;&#20116;&#20010;&#26102;&#38388;&#24207;&#21015;&#20581;&#24247;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#30740;&#31350;&#21457;&#29616;&#22635;&#34917;&#30340;&#25928;&#26524;&#20381;&#36182;&#20110;&#25968;&#25454;&#31867;&#22411;&#12289;&#21464;&#37327;&#32479;&#35745;&#12289;&#32570;&#22833;&#20540;&#29575;&#21644;&#31867;&#22411;&#12290;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#21516;&#26102;&#36827;&#34892;&#20132;&#21449;&#21078;&#38754;&#21644;&#32437;&#21521;&#32570;&#22833;&#20540;&#22635;&#34917;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2302.10902</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26102;&#38388;&#24207;&#21015;&#20581;&#24247;&#25968;&#25454;&#32570;&#22833;&#20540;&#22635;&#34917;&#65306;&#22238;&#39038;&#19982;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Deep Imputation of Missing Values in Time Series Health Data: A Review with Benchmarking. (arXiv:2302.10902v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26102;&#38388;&#24207;&#21015;&#20581;&#24247;&#25968;&#25454;&#32570;&#22833;&#20540;&#22635;&#34917;&#26041;&#27861;&#65292;&#24182;&#22312;&#20116;&#20010;&#26102;&#38388;&#24207;&#21015;&#20581;&#24247;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#30740;&#31350;&#21457;&#29616;&#22635;&#34917;&#30340;&#25928;&#26524;&#20381;&#36182;&#20110;&#25968;&#25454;&#31867;&#22411;&#12289;&#21464;&#37327;&#32479;&#35745;&#12289;&#32570;&#22833;&#20540;&#29575;&#21644;&#31867;&#22411;&#12290;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#21516;&#26102;&#36827;&#34892;&#20132;&#21449;&#21078;&#38754;&#21644;&#32437;&#21521;&#32570;&#22833;&#20540;&#22635;&#34917;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#25968;&#25454;&#20013;&#22635;&#34917;&#32570;&#22833;&#20540;&#23545;&#20110;&#30830;&#20445;&#25968;&#25454;&#36136;&#37327;&#21644;&#29983;&#25104;&#21487;&#38752;&#30340;&#25968;&#25454;&#39537;&#21160;&#39044;&#27979;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#38500;&#20102;&#35768;&#22810;&#32479;&#35745;&#26041;&#27861;&#22806;&#65292;&#36817;&#26399;&#19968;&#20123;&#30740;&#31350;&#25552;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#22635;&#34917;MTS&#25968;&#25454;&#20013;&#30340;&#32570;&#22833;&#20540;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#28145;&#24230;&#26041;&#27861;&#30340;&#35780;&#20272;&#20165;&#23616;&#38480;&#20110;&#19968;&#20010;&#25110;&#20004;&#20010;&#25968;&#25454;&#38598;&#12289;&#36739;&#20302;&#30340;&#32570;&#22833;&#29575;&#21644;&#23436;&#20840;&#38543;&#26426;&#30340;&#32570;&#22833;&#20540;&#31867;&#22411;&#12290;&#26412;&#25991;&#23545;&#20116;&#20010;&#26102;&#38388;&#24207;&#21015;&#20581;&#24247;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20845;&#20010;&#25968;&#25454;&#20013;&#24515;&#23454;&#39564;&#65292;&#23545;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#22635;&#34917;&#26041;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#27809;&#26377;&#19968;&#31181;&#21333;&#19968;&#30340;&#22635;&#34917;&#26041;&#27861;&#22312;&#25152;&#26377;&#20116;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26368;&#22909;&#12290;&#22635;&#34917;&#25928;&#26524;&#21462;&#20915;&#20110;&#25968;&#25454;&#31867;&#22411;&#12289;&#21333;&#20010;&#21464;&#37327;&#32479;&#35745;&#12289;&#32570;&#22833;&#20540;&#29575;&#21644;&#31867;&#22411;&#12290;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#21516;&#26102;&#36827;&#34892;&#20132;&#21449;&#21078;&#38754;&#65288;&#36328;&#21464;&#37327;&#65289;&#21644;&#32437;&#21521;&#65288;&#36328;&#26102;&#38388;&#65289;&#32570;&#22833;&#20540;&#22635;&#34917;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20135;&#29983;&#20102;&#32479;&#35745;&#23398;&#19978;&#30340;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The imputation of missing values in multivariate time series (MTS) data is critical in ensuring data quality and producing reliable data-driven predictive models. Apart from many statistical approaches, a few recent studies have proposed state-of-the-art deep learning methods to impute missing values in MTS data. However, the evaluation of these deep methods is limited to one or two data sets, low missing rates, and completely random missing value types. This survey performs six data-centric experiments to benchmark state-of-the-art deep imputation methods on five time series health data sets. Our extensive analysis reveals that no single imputation method outperforms the others on all five data sets. The imputation performance depends on data types, individual variable statistics, missing value rates, and types. Deep learning methods that jointly perform cross-sectional (across variables) and longitudinal (across time) imputations of missing values in time series data yield statistica
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#27169;&#25311;&#31038;&#20250;&#22256;&#22659;&#20013;&#30340;&#36947;&#24503;&#36873;&#25321;&#65292;&#35774;&#35745;&#20102;&#19968;&#22871;&#36947;&#24503;&#22870;&#21169;&#32467;&#26500;&#65292;&#26088;&#22312;&#20998;&#26512;&#21644;&#30740;&#31350;AI&#20195;&#29702;&#30340;&#36947;&#24503;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2301.08491</link><description>&lt;p&gt;
&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#27169;&#25311;&#31038;&#20250;&#22256;&#22659;&#20013;&#30340;&#36947;&#24503;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Modeling Moral Choices in Social Dilemmas with Multi-Agent Reinforcement Learning. (arXiv:2301.08491v2 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08491
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#27169;&#25311;&#31038;&#20250;&#22256;&#22659;&#20013;&#30340;&#36947;&#24503;&#36873;&#25321;&#65292;&#35774;&#35745;&#20102;&#19968;&#22871;&#36947;&#24503;&#22870;&#21169;&#32467;&#26500;&#65292;&#26088;&#22312;&#20998;&#26512;&#21644;&#30740;&#31350;AI&#20195;&#29702;&#30340;&#36947;&#24503;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#22312;&#26234;&#33021;&#20195;&#29702;&#20013;&#32435;&#20837;&#36947;&#24503;&#36873;&#25321;&#30340;&#37325;&#35201;&#24615;&#19981;&#26029;&#23637;&#29616;&#12290;&#21516;&#26102;&#20063;&#24378;&#35843;&#65292;&#25353;&#29031;&#20219;&#20309;&#19968;&#31181;&#36947;&#24503;&#35266;&#23450;&#20041;&#39030;&#23618;&#30340;AI&#20262;&#29702;&#32422;&#26463;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#65292;&#24182;&#19988;&#20250;&#24102;&#26469;&#39118;&#38505;&#12290;&#20174;&#24213;&#23618;&#23398;&#20064;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#25110;&#35768;&#26356;&#36866;&#21512;&#30740;&#31350;&#21644;&#24320;&#21457;AI&#20195;&#29702;&#30340;&#36947;&#24503;&#34892;&#20026;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20998;&#26512;&#26681;&#25454;&#39044;&#23450;&#20041;&#30340;&#36947;&#24503;&#22870;&#21169;&#22312;&#31038;&#20250;&#22256;&#22659;&#20013;&#23454;&#34892;&#34892;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#26032;&#20852;&#34892;&#20026;&#26159;&#19968;&#20010;&#26377;&#36259;&#21644;&#23500;&#26377;&#27934;&#23519;&#21147;&#30340;&#36215;&#28857;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#26681;&#25454;&#36947;&#24503;&#29702;&#35770;&#30340;&#22870;&#21169;&#36827;&#34892;&#30340;&#36873;&#25321;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#12290;&#25105;&#20204;&#26088;&#22312;&#35774;&#35745;&#31616;&#21270;&#20294;&#20195;&#34920;&#19968;&#32452;&#20851;&#38190;&#20262;&#29702;&#31995;&#32479;&#30340;&#22870;&#21169;&#32467;&#26500;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#23450;&#20041;&#20102;&#21306;&#20998;&#21518;&#26524;&#21644;&#35268;&#33539;&#20262;&#29702;&#30340;&#36947;&#24503;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#23558;&#23427;&#20204;&#28151;&#21512;&#20197;&#21019;&#24314;&#26032;&#30340;&#22870;&#21169;&#26041;&#26696;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#22312;&#31038;&#20250;&#22256;&#22659;&#19979;&#36827;&#34892;&#20869;&#22312;&#21160;&#26426;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#26469;&#35780;&#20272;&#36825;&#20123;&#22870;&#21169;&#20989;&#25968;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22797;&#21046;&#24182;&#25193;&#23637;&#26377;&#20851;&#36947;&#24503;&#36873;&#25321;&#30340;&#25991;&#29486;&#30740;&#31350;&#20013;&#30340;&#35768;&#22810;&#21457;&#29616;&#65292;&#24182;&#33021;&#22815;&#20986;&#29616;&#20197;&#21069;&#26410;&#26366;&#25253;&#36947;&#30340;&#26032;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Practical uses of Artificial Intelligence (AI) in the real world have demonstrated the importance of embedding moral choices into intelligent agents. They have also highlighted that defining top-down ethical constraints on AI according to any one type of morality is extremely challenging and can pose risks. A bottom-up learning approach may be more appropriate for studying and developing ethical behavior in AI agents. In particular, we believe that an interesting and insightful starting point is the analysis of emergent behavior of Reinforcement Learning (RL) agents that act according to a predefined set of moral rewards in social dilemmas.  In this work, we present a systematic analysis of the choices made by intrinsically-motivated RL agents whose rewards are based on moral theories. We aim to design reward structures that are simplified yet representative of a set of key ethical systems. Therefore, we first define moral reward functions that distinguish between consequence- and norm
&lt;/p&gt;</description></item><item><title>D-Adaptation&#26159;&#19968;&#31181;&#21487;&#20197;&#33258;&#21160;&#35774;&#32622;&#23398;&#20064;&#29575;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#26368;&#23567;&#21270;&#20984;&#24615;Lipschitz&#20989;&#25968;&#65292;&#29992;&#20110;&#23454;&#29616;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#65292;&#32780;&#26080;&#38656;&#36229;&#21442;&#25968;&#65292;&#20063;&#26080;&#38656;&#39069;&#22806;&#23545;&#25968;&#22240;&#23376;&#25913;&#36827;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#33258;&#21160;&#21305;&#37197;&#25163;&#21160;&#35843;&#25972;&#30340;&#23398;&#20064;&#29575;&#12290;</title><link>http://arxiv.org/abs/2301.07733</link><description>&lt;p&gt;
&#36890;&#36807;D&#36866;&#24212;&#23454;&#29616;&#23398;&#20064;&#29575;&#33258;&#30001;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning-Rate-Free Learning by D-Adaptation. (arXiv:2301.07733v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07733
&lt;/p&gt;
&lt;p&gt;
D-Adaptation&#26159;&#19968;&#31181;&#21487;&#20197;&#33258;&#21160;&#35774;&#32622;&#23398;&#20064;&#29575;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#26368;&#23567;&#21270;&#20984;&#24615;Lipschitz&#20989;&#25968;&#65292;&#29992;&#20110;&#23454;&#29616;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#65292;&#32780;&#26080;&#38656;&#36229;&#21442;&#25968;&#65292;&#20063;&#26080;&#38656;&#39069;&#22806;&#23545;&#25968;&#22240;&#23376;&#25913;&#36827;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#33258;&#21160;&#21305;&#37197;&#25163;&#21160;&#35843;&#25972;&#30340;&#23398;&#20064;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
D&#36866;&#24212;&#26159;&#19968;&#31181;&#33258;&#21160;&#35774;&#32622;&#23398;&#20064;&#29575;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#28176;&#36817;&#22320;&#23454;&#29616;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#65292;&#29992;&#20110;&#26368;&#23567;&#21270;&#20984;&#24615;Lipschitz&#20989;&#25968;&#65292;&#26080;&#38656;&#22238;&#28335;&#25110;&#32447;&#24615;&#25628;&#32034;&#65292;&#24182;&#19988;&#27599;&#27493;&#26080;&#38656;&#36827;&#34892;&#39069;&#22806;&#30340;&#20989;&#25968;&#20540;&#25110;&#26799;&#24230;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#36825;&#19968;&#31867;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#26080;&#36229;&#21442;&#25968;&#19988;&#25910;&#25947;&#36895;&#29575;&#26080;&#38656;&#39069;&#22806;&#23545;&#25968;&#22240;&#23376;&#25913;&#36827;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#38024;&#23545;SGD&#21644;Adam&#21464;&#20307;&#23637;&#31034;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#20854;&#20013;&#35813;&#26041;&#27861;&#33258;&#21160;&#21305;&#37197;&#25163;&#21160;&#35843;&#25972;&#30340;&#23398;&#20064;&#29575;&#65292;&#22312;&#21313;&#22810;&#20010;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#24212;&#29992;&#65292;&#21253;&#25324;&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#38382;&#39064;&#12290;&#24320;&#28304;&#23454;&#29616;&#22312; \url{https://github.com/facebookresearch/dadaptation}.
&lt;/p&gt;
&lt;p&gt;
D-Adaptation is an approach to automatically setting the learning rate which asymptotically achieves the optimal rate of convergence for minimizing convex Lipschitz functions, with no back-tracking or line searches, and no additional function value or gradient evaluations per step. Our approach is the first hyper-parameter free method for this class without additional multiplicative log factors in the convergence rate. We present extensive experiments for SGD and Adam variants of our method, where the method automatically matches hand-tuned learning rates across more than a dozen diverse machine learning problems, including large-scale vision and language problems.  An open-source implementation is available at \url{https://github.com/facebookresearch/dadaptation}.
&lt;/p&gt;</description></item><item><title>tasksource&#26159;&#19968;&#20010;&#25968;&#25454;&#38598;&#21327;&#35843;&#26694;&#26550;&#65292;&#20026;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#35780;&#20272;&#25552;&#20379;&#27969;&#30021;&#30340;&#20307;&#39564;&#12290;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#32467;&#26500;&#21270;&#27880;&#37322;&#65292;&#20351;&#24471;&#25968;&#25454;&#22788;&#29702;&#26356;&#21152;&#20415;&#25463;&#12290;</title><link>http://arxiv.org/abs/2301.05948</link><description>&lt;p&gt;
tasksource&#65306;&#27969;&#30021;&#30340;NLP&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#35780;&#20272;&#30340;&#25968;&#25454;&#38598;&#21327;&#35843;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
tasksource: A Dataset Harmonization Framework for Streamlined NLP Multi-Task Learning and Evaluation. (arXiv:2301.05948v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05948
&lt;/p&gt;
&lt;p&gt;
tasksource&#26159;&#19968;&#20010;&#25968;&#25454;&#38598;&#21327;&#35843;&#26694;&#26550;&#65292;&#20026;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#35780;&#20272;&#25552;&#20379;&#27969;&#30021;&#30340;&#20307;&#39564;&#12290;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#32467;&#26500;&#21270;&#27880;&#37322;&#65292;&#20351;&#24471;&#25968;&#25454;&#22788;&#29702;&#26356;&#21152;&#20415;&#25463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
HuggingFace&#25968;&#25454;&#38598;&#20013;&#24515;&#25552;&#20379;&#25968;&#21315;&#20010;&#25968;&#25454;&#38598;&#65292;&#20026;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#25552;&#20379;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#29305;&#23450;&#20219;&#21153;&#31867;&#22411;&#30340;&#25968;&#25454;&#38598;&#24448;&#24448;&#20855;&#26377;&#19981;&#21516;&#30340;&#26550;&#26500;&#65292;&#20351;&#24471;&#21327;&#35843;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22810;&#20219;&#21153;&#35757;&#32451;&#25110;&#35780;&#20272;&#38656;&#35201;&#25163;&#21160;&#23558;&#25968;&#25454;&#36866;&#37197;&#21040;&#20219;&#21153;&#27169;&#26495;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#19968;&#20123;&#20513;&#35758;&#37319;&#21462;&#29420;&#31435;&#30340;&#26041;&#27861;&#65292;&#21457;&#24067;&#21327;&#35843;&#30340;&#25968;&#25454;&#38598;&#25110;&#25552;&#20379;&#21327;&#35843;&#20195;&#30721;&#20197;&#23558;&#25968;&#25454;&#38598;&#39044;&#22788;&#29702;&#20026;&#19968;&#33268;&#26684;&#24335;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#20197;&#21069;&#30340;&#39044;&#22788;&#29702;&#24037;&#20316;&#20013;&#30340;&#27169;&#24335;&#65292;&#20363;&#22914;&#21015;&#21517;&#31216;&#26144;&#23556;&#21644;&#20174;&#21015;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#29305;&#23450;&#23376;&#23383;&#27573;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#27880;&#37322;&#26694;&#26550;&#65292;&#30830;&#20445;&#25105;&#20204;&#30340;&#27880;&#37322;&#23436;&#20840;&#26292;&#38706;&#32780;&#19981;&#38544;&#34255;&#22312;&#38750;&#32467;&#26500;&#21270;&#20195;&#30721;&#20013;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#27880;&#37322;&#26694;&#26550;&#21644;500&#22810;&#20010;&#33521;&#35821;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The HuggingFace Datasets Hub hosts thousands of datasets, offering exciting opportunities for language model training and evaluation. However, datasets for a specific task type often have different schemas, making harmonization challenging. Multi-task training or evaluation necessitates manual work to fit data into task templates. Several initiatives independently tackle this issue by releasing harmonized datasets or providing harmonization codes to preprocess datasets into a consistent format. We identify patterns across previous preprocessing efforts, such as column name mapping and extracting specific sub-fields from structured data in a column. We then propose a structured annotation framework that ensures our annotations are fully exposed and not hidden within unstructured code. We release a dataset annotation framework and dataset annotations for more than 500 English tasks\footnote{\url{https://github.com/sileod/tasksource}}. These annotations include metadata, such as the names
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SLAP&#30340;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#26367;&#20195;&#25968;&#25454;&#22686;&#24378;&#65292;&#21152;&#24378;&#32463;&#39564;&#20197;&#21152;&#36895;&#26426;&#22120;&#23398;&#20064;&#24182;&#20943;&#23569;&#26679;&#26412;&#22823;&#23567;&#12290; &#22312;Gomoku&#28216;&#25103;&#21644;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#30340;&#23454;&#39564;&#20013;&#65292;&#35777;&#26126;&#20102;SLAP&#30340;&#26377;&#25928;&#24615;&#65292;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#26679;&#26412;&#25968;&#37327;&#12290;&#36825;&#31181;&#31574;&#30053;&#33267;&#23569;&#36866;&#29992;&#20110;&#23545;&#31216;&#25110;&#29305;&#23450;&#21464;&#25442;&#19981;&#21464;&#30340;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2301.04746</link><description>&lt;p&gt;
&#21487;&#20999;&#25442;&#36731;&#37327;&#32423;&#21453;&#23545;&#31216;&#22788;&#29702;&#65288;SLAP&#65289;&#22312; Gomoku &#24378;&#21270;&#23398;&#20064;&#20013;&#20351;&#29992; CNN &#27604;&#25968;&#25454;&#22686;&#24378;&#26356;&#24555;
&lt;/p&gt;
&lt;p&gt;
Switchable Lightweight Anti-symmetric Processing (SLAP) with CNN Outspeeds Data Augmentation by Smaller Sample -- Application in Gomoku Reinforcement Learning. (arXiv:2301.04746v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04746
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SLAP&#30340;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#26367;&#20195;&#25968;&#25454;&#22686;&#24378;&#65292;&#21152;&#24378;&#32463;&#39564;&#20197;&#21152;&#36895;&#26426;&#22120;&#23398;&#20064;&#24182;&#20943;&#23569;&#26679;&#26412;&#22823;&#23567;&#12290; &#22312;Gomoku&#28216;&#25103;&#21644;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#30340;&#23454;&#39564;&#20013;&#65292;&#35777;&#26126;&#20102;SLAP&#30340;&#26377;&#25928;&#24615;&#65292;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#26679;&#26412;&#25968;&#37327;&#12290;&#36825;&#31181;&#31574;&#30053;&#33267;&#23569;&#36866;&#29992;&#20110;&#23545;&#31216;&#25110;&#29305;&#23450;&#21464;&#25442;&#19981;&#21464;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; SLAP &#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#24378;&#32463;&#39564;&#20197;&#21152;&#36895;&#26426;&#22120;&#23398;&#20064;&#24182;&#20943;&#23569;&#26679;&#26412;&#22823;&#23567;&#65292;&#20197;&#20195;&#26367;&#25968;&#25454;&#22686;&#24378;&#12290;SLAP&#26159;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#21327;&#35758;/&#20989;&#25968;&#65292;&#21487;&#20197;&#20135;&#29983;&#30456;&#21516;&#30340;&#36755;&#20986;&#65292;&#20294;&#32473;&#20104;&#19981;&#21516;&#30340;&#21464;&#25442;&#21464;&#37327;&#12290;&#22312;Gomoku&#28216;&#25103;&#29366;&#24577;&#30340;&#23454;&#39564;&#20013;&#65292;SLAP&#25552;&#39640;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#30340;&#25910;&#25947;&#36895;&#24230;&#36798;83&#65285;&#65292;&#26679;&#26412;&#22823;&#23567;&#21482;&#26377;&#25968;&#25454;&#22686;&#24378;&#30340;1/8&#12290;&#22312;Gomoku&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20351;&#29992;AlphaGo Zero / AlphaZero&#31639;&#27861;&#21644;&#25968;&#25454;&#22686;&#24378;&#20316;&#20026;&#22522;&#32447;&#65292;SLAP&#23558;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#20943;&#23569;&#20102;8&#20493;&#65292;&#24182;&#22312;&#19982;&#30456;&#21516;&#35780;&#20272;&#22120;&#23545;&#27604;&#26102;&#23454;&#29616;&#20102;&#31867;&#20284;&#30340;&#33719;&#32988;&#29575;&#65292;&#20294;&#23578;&#19981;&#33021;&#35777;&#26126;&#23427;&#21487;&#20197;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#12290;&#36825;&#20123;&#30410;&#22788;&#33267;&#23569;&#24212;&#36866;&#29992;&#20110;&#23545;&#31216;&#25110;&#29305;&#23450;&#21464;&#25442;&#19981;&#21464;&#30340;&#39046;&#22495;&#12290;&#20316;&#20026;&#26410;&#26469;&#30340;&#24037;&#20316;&#65292;SLAP&#21487;&#33021;&#26377;&#21161;&#20110;&#26356;&#21487;&#35299;&#37322;&#30340;&#23398;&#20064;&#20197;&#21450;&#19981;&#36866;&#29992;&#20110;&#23545;&#31216;&#30340;&#39046;&#22495;&#30340;&#36716;&#31227;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
To replace data augmentation, this paper proposed a method called SLAP to intensify experience to speed up machine learning and reduce the sample size. SLAP is a model-independent protocol/function to produce the same output given different transformation variants. SLAP improved the convergence speed of convolutional neural network learning by 83% in the experiments with Gomoku game states, with only one eighth of the sample size compared with data augmentation. In reinforcement learning for Gomoku, using AlphaGo Zero/AlphaZero algorithm with data augmentation as baseline, SLAP reduced the number of training samples by a factor of 8 and achieved similar winning rate against the same evaluator, but it was not yet evident that it could speed up reinforcement learning. The benefits should at least apply to domains that are invariant to symmetry or certain transformations. As future work, SLAP may aid more explainable learning and transfer learning for domains that are not invariant to sym
&lt;/p&gt;</description></item><item><title>TAToo&#26159;&#19968;&#20010;&#22522;&#20110;&#35270;&#35273;&#30340;&#32852;&#21512;&#36319;&#36394;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#24674;&#22797;&#39045;&#24213;&#25163;&#26415;&#20013;&#25163;&#26415;&#24037;&#20855;&#21644;&#24739;&#32773;&#35299;&#21078;&#32467;&#26500;&#30340;&#19977;&#32500;&#36816;&#21160;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#39640;&#31934;&#24230;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#21487;&#19982;&#29616;&#26377;&#30340;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#21644;&#20202;&#22120;&#20860;&#23481;&#12290;</title><link>http://arxiv.org/abs/2212.14131</link><description>&lt;p&gt;
TAToo&#65306;&#38754;&#21521;&#39045;&#24213;&#25163;&#26415;&#30340;&#35299;&#21078;&#21644;&#24037;&#20855;&#32852;&#21512;&#36319;&#36394;&#30340;&#22522;&#20110;&#35270;&#35273;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TAToo: Vision-based Joint Tracking of Anatomy and Tool for Skull-base Surgery. (arXiv:2212.14131v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14131
&lt;/p&gt;
&lt;p&gt;
TAToo&#26159;&#19968;&#20010;&#22522;&#20110;&#35270;&#35273;&#30340;&#32852;&#21512;&#36319;&#36394;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#24674;&#22797;&#39045;&#24213;&#25163;&#26415;&#20013;&#25163;&#26415;&#24037;&#20855;&#21644;&#24739;&#32773;&#35299;&#21078;&#32467;&#26500;&#30340;&#19977;&#32500;&#36816;&#21160;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#39640;&#31934;&#24230;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#21487;&#19982;&#29616;&#26377;&#30340;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#21644;&#20202;&#22120;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#36741;&#21161;&#30340;&#39045;&#24213;&#25163;&#26415;&#20013;&#65292;&#36319;&#36394;&#25163;&#26415;&#24037;&#20855;&#21644;&#24739;&#32773;&#35299;&#21078;&#32467;&#26500;&#30340;&#19977;&#32500;&#36816;&#21160;&#26159;&#24517;&#35201;&#30340;&#12290;TAToo&#26159;&#19968;&#20010;&#22522;&#20110;&#35270;&#35273;&#30340;&#36319;&#36394;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#20174;&#25163;&#26415;&#35270;&#39057;&#20013;&#24674;&#22797;&#36816;&#21160;&#20449;&#24687;&#65292;&#24182;&#37319;&#29992;&#27010;&#29575;&#24314;&#27169;&#21644;&#20960;&#20309;&#32422;&#26463;&#65292;&#20197;&#25552;&#39640;&#36319;&#36394;&#30340;&#40065;&#26834;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Purpose: Tracking the 3D motion of the surgical tool and the patient anatomy is a fundamental requirement for computer-assisted skull-base surgery. The estimated motion can be used both for intra-operative guidance and for downstream skill analysis. Recovering such motion solely from surgical videos is desirable, as it is compliant with current clinical workflows and instrumentation.  Methods: We present Tracker of Anatomy and Tool (TAToo). TAToo jointly tracks the rigid 3D motion of patient skull and surgical drill from stereo microscopic videos. TAToo estimates motion via an iterative optimization process in an end-to-end differentiable form. For robust tracking performance, TAToo adopts a probabilistic formulation and enforces geometric constraints on the object level.  Results: We validate TAToo on both simulation data, where ground truth motion is available, as well as on anthropomorphic phantom data, where optical tracking provides a strong baseline. We report sub-millimeter and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#30784;&#20915;&#31574;&#27169;&#22411;&#65288;FDM&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#36716;&#25442;&#22120;&#31070;&#32463;&#26550;&#26500;&#23558;&#21508;&#31181;&#20915;&#31574;&#20219;&#21153;&#21046;&#23450;&#20026;&#24207;&#21015;&#35299;&#30721;&#20219;&#21153;&#65292;&#20026;&#22312;&#22797;&#26434;&#29616;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#23454;&#29616;&#26426;&#22120;&#39537;&#21160;&#26234;&#33021;&#20915;&#31574;(IDM)&#25552;&#20379;&#20102;&#19968;&#20010;&#21069;&#26223;&#24191;&#38420;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2212.12669</link><description>&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#23454;&#29616;&#26234;&#33021;&#20915;&#31574;&#65306;&#22522;&#30784;&#20915;&#31574;&#27169;&#22411;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
On Realization of Intelligent Decision-Making in the Real World: A Foundation Decision Model Perspective. (arXiv:2212.12669v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#30784;&#20915;&#31574;&#27169;&#22411;&#65288;FDM&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#36716;&#25442;&#22120;&#31070;&#32463;&#26550;&#26500;&#23558;&#21508;&#31181;&#20915;&#31574;&#20219;&#21153;&#21046;&#23450;&#20026;&#24207;&#21015;&#35299;&#30721;&#20219;&#21153;&#65292;&#20026;&#22312;&#22797;&#26434;&#29616;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#23454;&#29616;&#26426;&#22120;&#39537;&#21160;&#26234;&#33021;&#20915;&#31574;(IDM)&#25552;&#20379;&#20102;&#19968;&#20010;&#21069;&#26223;&#24191;&#38420;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#19990;&#30028;&#29615;&#22659;&#30340;&#26222;&#36941;&#19981;&#30830;&#23450;&#24615;&#21644;&#21160;&#24577;&#29305;&#24615;&#32473;&#26426;&#22120;&#39537;&#21160;&#26234;&#33021;&#20915;&#31574;(IDM)&#31995;&#32479;&#30340;&#24191;&#27867;&#23454;&#26045;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;IDM&#24212;&#20855;&#22791;&#25345;&#32493;&#33719;&#21462;&#26032;&#25216;&#33021;&#24182;&#22312;&#24191;&#27867;&#24212;&#29992;&#20013;&#26377;&#25928;&#25512;&#24191;&#30340;&#33021;&#21147;&#12290;&#36229;&#36234;&#20219;&#21153;&#21644;&#24212;&#29992;&#36793;&#30028;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;(AGI)&#30340;&#36827;&#27493;&#23545;&#20110;&#22686;&#24378;IDM&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24191;&#27867;&#35843;&#26597;&#20102;&#36716;&#25442;&#22120;&#31070;&#32463;&#26550;&#26500;&#20316;&#20026;&#21508;&#31181;&#20219;&#21153;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#21253;&#25324;&#35745;&#31639;&#26426;&#35270;&#35273;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#22522;&#30784;&#20915;&#31574;&#27169;&#22411;(FDM)&#65292;&#36890;&#36807;&#20351;&#29992;&#36716;&#25442;&#22120;&#20307;&#31995;&#32467;&#26500;&#23558;&#21508;&#31181;&#20915;&#31574;&#20219;&#21153;&#21046;&#23450;&#20026;&#24207;&#21015;&#35299;&#30721;&#20219;&#21153;&#65292;&#20026;&#25193;&#23637;&#22797;&#26434;&#23454;&#38469;&#24773;&#20917;&#19979;&#30340;IDM&#24212;&#29992;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22522;&#20110;&#36716;&#25442;&#22120;&#31070;&#32463;&#26550;&#26500;&#24320;&#21457;&#22522;&#30784;&#20915;&#31574;&#27169;&#22411;(FDM)&#30340;&#25928;&#29575;&#21644;&#21487;&#34892;&#24615;&#65292;&#29992;&#20110;&#26426;&#22120;&#39537;&#21160;&#26234;&#33021;&#20915;&#31574;(IDM)&#22312;&#22797;&#26434;&#23454;&#38469;&#24773;&#20917;&#20013;&#30340;&#24212;&#29992;&#12290;FDM&#21487;&#20197;&#20801;&#35768;&#25345;&#32493;&#30340;&#25216;&#33021;&#33719;&#21462;&#24182;&#22312;&#21508;&#20010;&#24212;&#29992;&#20043;&#38388;&#36827;&#34892;&#27010;&#25324;&#65292;&#20855;&#26377;&#22686;&#36827;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;(AGI)&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The pervasive uncertainty and dynamic nature of real-world environments present significant challenges for the widespread implementation of machine-driven Intelligent Decision-Making (IDM) systems. Consequently, IDM should possess the ability to continuously acquire new skills and effectively generalize across a broad range of applications. The advancement of Artificial General Intelligence (AGI) that transcends task and application boundaries is critical for enhancing IDM. Recent studies have extensively investigated the Transformer neural architecture as a foundational model for various tasks, including computer vision, natural language processing, and reinforcement learning. We propose that a Foundation Decision Model (FDM) can be developed by formulating diverse decision-making tasks as sequence decoding tasks using the Transformer architecture, offering a promising solution for expanding IDM applications in complex real-world situations. In this paper, we discuss the efficiency an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#23545;&#27604;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;AI&#27169;&#22411;&#65292;&#20351;&#29992;&#32593;&#39029;&#25235;&#21462;&#30340;&#25968;&#25454;&#35757;&#32451;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#24615;&#29289;&#21270;&#20559;&#35265;&#65292;&#21363;&#20154;&#30340;&#24773;&#24863;&#29366;&#24577;&#19982;&#36523;&#20307;&#30340;&#21576;&#29616;&#30456;&#20851;&#65292;&#34920;&#29616;&#20986;&#23545;&#22899;&#24615;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2212.11261</link><description>&lt;p&gt;
&#20351;&#29992;&#32593;&#39029;&#25235;&#21462;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#39044;&#35757;&#32451;&#30340;&#23545;&#27604;&#35821;&#35328;-&#35270;&#35273;AI&#27169;&#22411;&#23384;&#22312;&#24615;&#29289;&#21270;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Contrastive Language-Vision AI Models Pretrained on Web-Scraped Multimodal Data Exhibit Sexual Objectification Bias. (arXiv:2212.11261v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.11261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#23545;&#27604;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;AI&#27169;&#22411;&#65292;&#20351;&#29992;&#32593;&#39029;&#25235;&#21462;&#30340;&#25968;&#25454;&#35757;&#32451;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#24615;&#29289;&#21270;&#20559;&#35265;&#65292;&#21363;&#20154;&#30340;&#24773;&#24863;&#29366;&#24577;&#19982;&#36523;&#20307;&#30340;&#21576;&#29616;&#30456;&#20851;&#65292;&#34920;&#29616;&#20986;&#23545;&#22899;&#24615;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#20351;&#29992;&#23545;&#27604;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#30446;&#26631;&#36827;&#34892;&#32593;&#39029;&#25235;&#21462;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#20061;&#31181;&#35821;&#35328;-&#35270;&#35273;AI&#27169;&#22411;&#65292;&#20197;&#23547;&#25214;&#24515;&#29702;&#23398;&#23478;&#30740;&#31350;&#30340;&#20559;&#35265;&#30340;&#35777;&#25454;&#65306;&#22899;&#23401;&#21644;&#22899;&#24615;&#30340;&#24615;&#29289;&#21270;&#29616;&#35937;&#12290;&#25105;&#20204;&#22797;&#21046;&#20102;&#19977;&#20010;&#24515;&#29702;&#23454;&#39564;&#65292;&#24182;&#26174;&#31034;&#36825;&#31181;&#20559;&#35265;&#22312;AI&#20013;&#20381;&#28982;&#23384;&#22312;&#12290;&#31532;&#19968;&#20010;&#23454;&#39564;&#20351;&#29992;Sexual OBjectification and EMotion Database&#20013;&#30340;&#26631;&#20934;&#22899;&#24615;&#22270;&#20687;&#65292;&#24182;&#21457;&#29616;&#24773;&#24863;&#29366;&#24577;&#30340;&#35782;&#21035;&#26159;&#30001;&#20027;&#20307;&#26159;&#21542;&#20840;&#36523;&#25110;&#37096;&#20998;&#31359;&#30528;&#36827;&#34892;&#20171;&#23548;&#30340;&#12290;&#23884;&#20837;&#20851;&#32852;&#27979;&#35797;&#36820;&#22238;&#20102;&#24868;&#24594;(d&gt;0.80)&#21644;&#24754;&#20260;(d&gt;0.50)&#30340;&#26174;&#30528;&#25928;&#24212;&#22823;&#23567;&#65292;&#23558;&#23436;&#20840;&#31359;&#30528;&#30340;&#20027;&#20307;&#30340;&#22270;&#20687;&#19982;&#24773;&#24863;&#30456;&#20851;&#32852;&#12290;GRAD-CAM&#26174;&#33879;&#24615;&#22270;&#31361;&#20986;&#26174;&#31034;&#65292;CLIP&#29983;&#25104;&#30340;&#27169;&#22411;&#23384;&#22312;&#24615;&#29289;&#21270;&#20559;&#35265;&#65292;&#21363;&#20351;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#23545;&#27604;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#21644;&#32593;&#39029;&#25235;&#21462;&#30340;&#25968;&#25454;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nine language-vision AI models trained on web scrapes with the Contrastive Language-Image Pretraining (CLIP) objective are evaluated for evidence of a bias studied by psychologists: the sexual objectification of girls and women, which occurs when a person's human characteristics, such as emotions, are disregarded and the person is treated as a body. We replicate three experiments in psychology quantifying sexual objectification and show that the phenomena persist in AI. A first experiment uses standardized images of women from the Sexual OBjectification and EMotion Database, and finds that human characteristics are disassociated from images of objectified women: the model's recognition of emotional state is mediated by whether the subject is fully or partially clothed. Embedding association tests (EATs) return significant effect sizes for both anger (d &gt;0.80) and sadness (d &gt;0.50), associating images of fully clothed subjects with emotions. GRAD-CAM saliency maps highlight that CLIP ge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#27169;&#22240;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#23454;&#38469;&#32422;&#26463;&#30340;&#28789;&#27963;&#29983;&#20135;&#35843;&#24230;&#38382;&#39064;&#65292;&#24182;&#24357;&#34917;&#20803;&#21551;&#21457;&#24335;&#30740;&#31350;&#20013;&#30340;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2212.10936</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#27169;&#22240;&#31639;&#27861;&#29992;&#20110;&#31038;&#20250;&#25216;&#26415;&#29983;&#20135;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
A Memetic Algorithm with Reinforcement Learning for Sociotechnical Production Scheduling. (arXiv:2212.10936v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10936
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#27169;&#22240;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#23454;&#38469;&#32422;&#26463;&#30340;&#28789;&#27963;&#29983;&#20135;&#35843;&#24230;&#38382;&#39064;&#65292;&#24182;&#24357;&#34917;&#20803;&#21551;&#21457;&#24335;&#30740;&#31350;&#20013;&#30340;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24212;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#27169;&#22240;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#23454;&#38469;&#21452;&#36164;&#28304;&#32422;&#26463;&#26580;&#24615;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#65288;DRC-FJSSP&#65289;&#12290;&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;DRL&#25216;&#26415;&#24050;&#32463;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#20294;&#26159;&#27809;&#26377;&#32771;&#34385;&#21040;&#29616;&#23454;&#12289;&#28789;&#27963;&#21644;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#36710;&#38388;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#22312;&#20197;&#35746;&#21333;&#20026;&#23548;&#21521;&#30340;&#38388;&#27463;&#24615;&#21046;&#36896;&#20013;&#23384;&#22312;&#19968;&#20010;&#30740;&#31350;&#31354;&#30333;&#65292;&#23427;&#32463;&#24120;&#22312;&#20855;&#26377;&#39640;&#26381;&#21153;&#27700;&#24179;&#30340;&#20013;&#23567;&#22411;&#20844;&#21496;&#20013;&#34920;&#31034;&#12290;&#20174;&#36825;&#19968;&#39046;&#22495;&#30340;&#23454;&#38469;&#24037;&#19994;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#35748;&#35782;&#21040;&#38656;&#35201;&#25551;&#36848;&#28789;&#27963;&#30340;&#26426;&#22120;&#12289;&#20154;&#24037;&#24037;&#20316;&#32773;&#21644;&#33021;&#21147;&#12289;&#35774;&#32622;&#21644;&#22788;&#29702;&#25805;&#20316;&#12289;&#29289;&#26009;&#21040;&#36798;&#26102;&#38388;&#12289;&#20855;&#26377;&#24182;&#34892;&#20219;&#21153;&#30340;&#22797;&#26434;&#20316;&#19994;&#36335;&#24452;&#20197;&#36827;&#34892;&#29289;&#26009;&#28165;&#21333;&#65288;BOM&#65289;&#21046;&#36896;&#12289;&#39034;&#24207;&#30456;&#20851;&#35774;&#32622;&#26102;&#38388;&#21644;&#65288;&#37096;&#20998;&#65289;&#33258;&#21160;&#21270;&#20219;&#21153;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22312;DRC-FJSSP&#30340;&#32972;&#26223;&#19979;&#65292;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#20803;&#21551;&#21457;&#24335;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#36866;&#24403;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#29983;&#20135;&#36807;&#31243;&#30340;&#22797;&#26434;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#36825;&#26159;&#29616;&#23454;&#24037;&#19994;&#19990;&#30028;&#20013;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#20197;&#24357;&#34917;&#30456;&#20851;&#39046;&#22495;&#30340;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;
The following article presents a memetic algorithm with applying deep reinforcement learning (DRL) for solving practically oriented dual resource constrained flexible job shop scheduling problems (DRC-FJSSP). In recent years, there has been extensive research on DRL techniques, but without considering realistic, flexible and human-centered shopfloors. A research gap can be identified in the context of make-to-order oriented discontinuous manufacturing as it is often represented in medium-size companies with high service levels. From practical industry projects in this domain, we recognize requirements to depict flexible machines, human workers and capabilities, setup and processing operations, material arrival times, complex job paths with parallel tasks for bill of material (BOM) manufacturing, sequence-depended setup times and (partially) automated tasks. On the other hand, intensive research has been done on metaheuristics in the context of DRC-FJSSP. However, there is a lack of sui
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DimonGen&#27169;&#22411;&#65292;&#36890;&#36807;&#29983;&#25104;&#21508;&#31181;&#26085;&#24120;&#22330;&#26223;&#30340;&#27010;&#24565;&#20851;&#31995;&#25551;&#36848;&#26469;&#23454;&#29616;&#22810;&#20803;&#21270;&#36890;&#35782;&#25512;&#29702;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MoREE&#27169;&#22411;&#22312;&#29983;&#25104;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#22343;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2212.10545</link><description>&lt;p&gt;
DimonGen&#65306;&#22810;&#20803;&#21270;&#29983;&#25104;&#36890;&#35782;&#25512;&#29702;&#20197;&#35299;&#37322;&#27010;&#24565;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
DimonGen: Diversified Generative Commonsense Reasoning for Explaining Concept Relationships. (arXiv:2212.10545v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10545
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DimonGen&#27169;&#22411;&#65292;&#36890;&#36807;&#29983;&#25104;&#21508;&#31181;&#26085;&#24120;&#22330;&#26223;&#30340;&#27010;&#24565;&#20851;&#31995;&#25551;&#36848;&#26469;&#23454;&#29616;&#22810;&#20803;&#21270;&#36890;&#35782;&#25512;&#29702;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MoREE&#27169;&#22411;&#22312;&#29983;&#25104;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#22343;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DimonGen&#65292;&#26088;&#22312;&#29983;&#25104;&#25551;&#36848;&#21508;&#31181;&#26085;&#24120;&#22330;&#26223;&#20013;&#27010;&#24565;&#20851;&#31995;&#30340;&#22810;&#26679;&#21270;&#21477;&#23376;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#25913;&#32534;&#29616;&#26377;&#30340;CommonGen&#25968;&#25454;&#38598;&#21019;&#24314;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MoREE&#30340;&#20004;&#38454;&#27573;&#27169;&#22411;&#26469;&#29983;&#25104;&#30446;&#26631;&#21477;&#23376;&#12290;MoREE&#21253;&#21547;&#19968;&#31181;&#28151;&#21512;&#26816;&#32034;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26816;&#32034;&#19982;&#32473;&#23450;&#27010;&#24565;&#30456;&#20851;&#30340;&#22810;&#26679;&#24615;&#19978;&#19979;&#25991;&#21477;&#23376;&#65292;&#20197;&#21450;&#19968;&#31181;&#28151;&#21512;&#29983;&#25104;&#22120;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22522;&#20110;&#26816;&#32034;&#21040;&#30340;&#19978;&#19979;&#25991;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#21477;&#23376;&#12290;&#25105;&#20204;&#22312;DimonGen&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102;MoREE&#22312;&#29983;&#25104;&#30340;&#21477;&#23376;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#20248;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;MoREE&#33021;&#22815;&#29983;&#25104;&#21453;&#26144;&#27010;&#24565;&#20043;&#38388;&#19981;&#21516;&#20851;&#31995;&#30340;&#22810;&#26679;&#21270;&#21477;&#23376;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#27010;&#24565;&#20851;&#31995;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose DimonGen, which aims to generate diverse sentences describing concept relationships in various everyday scenarios. To support this, we first create a benchmark dataset for this task by adapting the existing CommonGen dataset. We then propose a two-stage model called MoREE to generate the target sentences. MoREE consists of a mixture of retrievers model that retrieves diverse context sentences related to the given concepts, and a mixture of generators model that generates diverse sentences based on the retrieved contexts. We conduct experiments on the DimonGen task and show that MoREE outperforms strong baselines in terms of both the quality and diversity of the generated sentences. Our results demonstrate that MoREE is able to generate diverse sentences that reflect different relationships between concepts, leading to a comprehensive understanding of concept relationships.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#25552;&#31034;&#26694;&#26550;&#65292;&#21487;&#20197;&#26377;&#25928;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#20013;&#23384;&#20648;&#30340;&#30693;&#35782;&#21644;&#25351;&#20196;&#29702;&#35299;&#33021;&#21147;&#65292;&#20197;&#23454;&#29616;&#38646;&#26679;&#26412;&#24320;&#25918;&#22495;&#38382;&#31572;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19977;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;ODQA&#25968;&#25454;&#38598;&#20013;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.08635</link><description>&lt;p&gt;
&#33258;&#25105;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#38646;&#26679;&#26412;&#24320;&#25918;&#22495;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Self-Prompting Large Language Models for Zero-Shot Open-Domain QA. (arXiv:2212.08635v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08635
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#25552;&#31034;&#26694;&#26550;&#65292;&#21487;&#20197;&#26377;&#25928;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#20013;&#23384;&#20648;&#30340;&#30693;&#35782;&#21644;&#25351;&#20196;&#29702;&#35299;&#33021;&#21147;&#65292;&#20197;&#23454;&#29616;&#38646;&#26679;&#26412;&#24320;&#25918;&#22495;&#38382;&#31572;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19977;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;ODQA&#25968;&#25454;&#38598;&#20013;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#22495;&#38382;&#31572;&#30446;&#26631;&#22312;&#20110;&#22238;&#31572;&#20851;&#20110;&#20107;&#23454;&#30340;&#38382;&#39064;&#65292;&#32780;&#26080;&#38656;&#25552;&#20379;&#29305;&#23450;&#30340;&#32972;&#26223;&#25991;&#26723;&#12290;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#65292;&#30001;&#20110;&#27809;&#26377;&#25968;&#25454;&#26469;&#35757;&#32451;&#31867;&#20284;&#26816;&#32034;&#22120;-&#38405;&#35835;&#22120;&#30340;&#23450;&#21046;&#27169;&#22411;&#65292;&#22240;&#27492;&#27492;&#20219;&#21153;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#36817;&#65292;&#20687;GPT-3&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#36890;&#36807;&#30452;&#25509;&#25552;&#31034;&#26041;&#27861;&#22312;&#38646;&#26679;&#26412;&#24320;&#25918;&#22495;&#38382;&#31572;&#20013;&#23637;&#31034;&#20102;&#20854;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#20294;&#26159;&#36825;&#20123;&#26041;&#27861;&#20173;&#28982;&#36828;&#36828;&#19981;&#33021;&#20805;&#20998;&#21457;&#25381;LLM&#30340;&#24378;&#22823;&#21151;&#33021;&#65292;&#32780;&#21482;&#26159;&#20197;&#38544;&#24335;&#26041;&#24335;&#35843;&#29992;&#23427;&#20204;&#32780;&#24050;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#25105;&#25552;&#31034;&#26694;&#26550;&#65292;&#20197;&#26126;&#30830;&#21033;&#29992;LLM&#21442;&#25968;&#20013;&#23384;&#20648;&#30340;&#22823;&#37327;&#30693;&#35782;&#21644;&#20854;&#24378;&#22823;&#30340;&#25351;&#20196;&#29702;&#35299;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36880;&#27493;&#25552;&#31034;LLM&#29983;&#25104;&#22810;&#20010;&#20266;QA&#23545;&#65292;&#24182;&#20174;&#22836;&#24320;&#22987;&#29983;&#25104;&#32972;&#26223;&#27573;&#33853;&#21644;&#35299;&#37322;&#65292;&#28982;&#21518;&#20351;&#29992;&#29983;&#25104;&#30340;&#20803;&#32032;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#26696;&#22312;&#19977;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;ODQA&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#36229;&#36807;&#20102;&#20808;&#21069;&#30340;SOTA&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open-Domain Question Answering (ODQA) aims at answering factoid questions without explicitly providing specific background documents. In a zero-shot setting, this task is more challenging since no data is available to train customized models like Retriever-Readers. Recently, Large Language Models (LLMs) like GPT-3 have shown their power in zero-shot ODQA with direct prompting methods, but these methods are still far from releasing the full powerfulness of LLMs only in an implicitly invoking way. In this paper, we propose a Self-Prompting framework to explicitly utilize the massive knowledge stored in the parameters of LLMs and their strong instruction understanding abilities. Concretely, we prompt LLMs step by step to generate multiple pseudo QA pairs with background passages and explanations from scratch and then use those generated elements for in-context learning. Experimental results show our method surpasses previous SOTA methods significantly on three widely-used ODQA datasets, a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#30456;&#20284;&#24230;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#21152;&#36895;&#26641;&#24418;&#32467;&#26500;Parzen&#20272;&#35745;&#20013;&#30340;&#22810;&#30446;&#26631;&#38750;&#20998;&#23618;&#36229;&#21442;&#25968;&#26368;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.06751</link><description>&lt;p&gt;
&#22522;&#20110;&#20219;&#21153;&#30456;&#20284;&#24230;&#20803;&#23398;&#20064;&#21152;&#36895;&#22810;&#30446;&#26631;&#38750;&#20998;&#23618;&#36229;&#21442;&#25968;&#26368;&#20248;&#21270;&#30340;&#26641;&#24418;&#32467;&#26500;Parzen&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Speeding up Multi-objective Non-hierarchical Hyperparameter Optimization by Task Similarity-Based Meta-Learning for the Tree-structured Parzen Estimator. (arXiv:2212.06751v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#30456;&#20284;&#24230;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#21152;&#36895;&#26641;&#24418;&#32467;&#26500;Parzen&#20272;&#35745;&#20013;&#30340;&#22810;&#30446;&#26631;&#38750;&#20998;&#23618;&#36229;&#21442;&#25968;&#26368;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#20248;&#21270;&#26159;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#24615;&#33021;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#23454;&#36341;&#32773;&#36890;&#24120;&#38754;&#20020;&#22810;&#20010;&#26041;&#38754;&#30340;&#26435;&#34913;&#65292;&#22914;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#26102;&#38388;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#30340;&#39640;&#35745;&#31639;&#38656;&#27714;&#21644;&#23545;&#39640;&#25928;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#19981;&#26029;&#22686;&#38271;&#38656;&#27714;&#19979;&#65292;&#21152;&#36895;&#22810;&#30446;&#26631;&#20248;&#21270;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#23558;TPE&#30340;&#25910;&#36141;&#20989;&#25968;&#25193;&#23637;&#21040;&#20803;&#23398;&#20064;&#35774;&#32622;&#20013;&#65292;&#20351;&#29992;&#30001;&#20219;&#21153;&#20043;&#38388;&#39030;&#32423;&#22495;&#20043;&#38388;&#30340;&#37325;&#21472;&#24230;&#23450;&#20041;&#30340;&#20219;&#21153;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#20063;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#24182;&#35299;&#20915;&#20102;&#20219;&#21153;&#30456;&#20284;&#24615;&#30340;&#23616;&#38480;&#24615;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#34920;&#26684;HPO&#22522;&#20934;&#19978;&#21152;&#36895;&#20102;MO-TPE&#65292;&#24182;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#36890;&#36807;&#36194;&#24471;AutoML 2022&#26469;&#24471;&#21040;&#22806;&#37096;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperparameter optimization (HPO) is a vital step in improving performance in deep learning (DL). Practitioners are often faced with the trade-off between multiple criteria, such as accuracy and latency. Given the high computational needs of DL and the growing demand for efficient HPO, the acceleration of multi-objective (MO) optimization becomes ever more important. Despite the significant body of work on meta-learning for HPO, existing methods are inapplicable to MO tree-structured Parzen estimator (MO-TPE), a simple yet powerful MO-HPO algorithm. In this paper, we extend TPE's acquisition function to the meta-learning setting using a task similarity defined by the overlap of top domains between tasks. We also theoretically analyze and address the limitations of our task similarity. In the experiments, we demonstrate that our method speeds up MO-TPE on tabular HPO benchmarks and attains state-of-the-art performance. Our method was also validated externally by winning the AutoML 2022 
&lt;/p&gt;</description></item><item><title>AIONER&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20840;&#22871;&#26041;&#26696;&#29983;&#29289;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#24037;&#20855;&#65292;&#21033;&#29992;&#22806;&#37096;&#25968;&#25454;&#25552;&#39640;BioNER&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.16944</link><description>&lt;p&gt;
AIONER: &#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20840;&#22871;&#26041;&#26696;&#29983;&#29289;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
AIONER: All-in-one scheme-based biomedical named entity recognition using deep learning. (arXiv:2211.16944v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16944
&lt;/p&gt;
&lt;p&gt;
AIONER&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20840;&#22871;&#26041;&#26696;&#29983;&#29289;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#24037;&#20855;&#65292;&#21033;&#29992;&#22806;&#37096;&#25968;&#25454;&#25552;&#39640;BioNER&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;BioNER&#65289;&#26088;&#22312;&#33258;&#21160;&#35782;&#21035;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#20013;&#30340;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#65292;&#20026;&#20449;&#24687;&#25552;&#21462;&#21644;&#38382;&#31572;&#31561;&#19979;&#28216;&#25991;&#26412;&#25366;&#25496;&#20219;&#21153;&#21644;&#24212;&#29992;&#25552;&#20379;&#24517;&#35201;&#30340;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20934;&#30830;&#27880;&#37322;&#25152;&#38656;&#30340;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#26114;&#36149;&#65292;&#22240;&#27492;&#25163;&#21160;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#23545;BioNER&#20219;&#21153;&#26469;&#35828;&#26159;&#26114;&#36149;&#30340;&#12290;&#23548;&#33268;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#23548;&#33268;&#24403;&#21069;&#30340;BioNER&#26041;&#27861;&#23481;&#26131;&#20986;&#29616;&#36807;&#24230;&#25311;&#21512;&#65292;&#20855;&#26377;&#26377;&#38480;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#19968;&#27425;&#21482;&#33021;&#35299;&#20915;&#19968;&#31181;&#23454;&#20307;&#31867;&#22411;&#65288;&#20363;&#22914;&#65292;&#22522;&#22240;&#25110;&#30142;&#30149;&#65289;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20840;&#22871;&#26041;&#26696;&#65288;AIO&#65289;&#26041;&#26696;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#27880;&#37322;&#36164;&#28304;&#22806;&#37096;&#25968;&#25454;&#26469;&#25552;&#39640;BioNER&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#22522;&#20110;&#23574;&#31471;&#28145;&#24230;&#23398;&#20064;&#21644;&#25105;&#20204;&#30340;AIO&#26041;&#26696;&#30340;&#36890;&#29992;BioNER&#24037;&#20855;AIONER&#12290;&#25105;&#20204;&#22312;14&#20010;BioNER&#22522;&#20934;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;AIONER&#65292;&#24182;&#23637;&#31034;&#20102;AIONER&#30340;&#26377;&#25928;&#24615;&#65292;&#31283;&#20581;&#24615;&#65292;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biomedical named entity recognition (BioNER) seeks to automatically recognize biomedical entities in natural language text, serving as a necessary foundation for downstream text mining tasks and applications such as information extraction and question answering. Manually labeling training data for the BioNER task is costly, however, due to the significant domain expertise required for accurate annotation. The resulting data scarcity causes current BioNER approaches to be prone to overfitting, to suffer from limited generalizability, and to address a single entity type at a time (e.g., gene or disease). We therefore propose a novel all-in-one (AIO) scheme that uses external data from existing annotated resources to enhance the accuracy and stability of BioNER models. We further present AIONER, a general-purpose BioNER tool based on cutting-edge deep learning and our AIO schema. We evaluate AIONER on 14 BioNER benchmark tasks and show that AIONER is effective, robust, and compares favora
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#19968;&#39033;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#23545;57&#31181;&#35821;&#35328;&#21644;&#19977;&#20010;&#20219;&#21153;&#19979;&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;&#20248;&#21270;&#21518;&#30340;&#26631;&#35760;-&#32763;&#35793;&#27861;&#27604;&#20256;&#32479;&#27880;&#37322;&#25237;&#24433;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2211.15613</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#36716;&#31227;&#30340;&#20196;&#20154;&#27822;&#20007;&#30340;&#31616;&#26131;&#26631;&#31614;&#25237;&#24433;
&lt;/p&gt;
&lt;p&gt;
Frustratingly Easy Label Projection for Cross-lingual Transfer. (arXiv:2211.15613v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#19968;&#39033;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#23545;57&#31181;&#35821;&#35328;&#21644;&#19977;&#20010;&#20219;&#21153;&#19979;&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;&#20248;&#21270;&#21518;&#30340;&#26631;&#35760;-&#32763;&#35793;&#27861;&#27604;&#20256;&#32479;&#27880;&#37322;&#25237;&#24433;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#35757;&#32451;&#25968;&#25454;&#32763;&#35793;&#25104;&#22810;&#31181;&#35821;&#35328;&#24050;&#25104;&#20026;&#25552;&#39640;&#36328;&#35821;&#35328;&#36716;&#31227;&#30340;&#23454;&#38469;&#35299;&#20915;&#26041;&#26696;&#12290;&#23545;&#20110;&#28041;&#21450;&#36328;&#24230;&#32423;&#21035;&#27880;&#37322;&#65288;&#20363;&#22914;&#20449;&#24687;&#25552;&#21462;&#25110;&#38382;&#39064;&#22238;&#31572;&#65289;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#36827;&#34892;&#39069;&#22806;&#30340;&#26631;&#31614;&#25237;&#24433;&#27493;&#39588;&#65292;&#23558;&#24050;&#27880;&#37322;&#30340;&#36328;&#24230;&#26144;&#23556;&#21040;&#32763;&#35793;&#21518;&#30340;&#25991;&#26412;&#20013;&#12290;&#28982;&#32780;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36804;&#20170;&#20026;&#27490;&#23578;&#26410;&#23545;&#36825;&#31181;&#26041;&#27861;&#19982;&#22522;&#20110;&#21333;&#35789;&#23545;&#40784;&#30340;&#20256;&#32479;&#27880;&#37322;&#25237;&#24433;&#36827;&#34892;&#23454;&#35777;&#20998;&#26512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#39033;&#23545;57&#31181;&#35821;&#35328;&#21644;&#19977;&#20010;&#20219;&#21153;&#65288;QA&#65292;NER&#21644;&#20107;&#20214;&#25552;&#21462;&#65289;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#20197;&#35780;&#20272;&#20004;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#22635;&#34917;&#25991;&#29486;&#20013;&#30340;&#37325;&#35201;&#31354;&#30333;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#20248;&#21270;&#21518;&#30340;&#26631;&#35760;-&#32763;&#35793;&#27861;&#27604;&#20256;&#32479;&#27880;&#37322;&#25237;&#24433;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Translating training data into many languages has emerged as a practical solution for improving cross-lingual transfer. For tasks that involve span-level annotations, such as information extraction or question answering, an additional label projection step is required to map annotated spans onto the translated texts. Recently, a few efforts have utilized a simple mark-then-translate method to jointly perform translation and projection by inserting special markers around the labeled spans in the original sentence. However, as far as we are aware, no empirical analysis has been conducted on how this approach compares to traditional annotation projection based on word alignment. In this paper, we present an extensive empirical study across 57 languages and three tasks (QA, NER, and Event Extraction) to evaluate the effectiveness and limitations of both methods, filling an important gap in the literature. Experimental results show that our optimized version of mark-then-translate, which we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32422;&#26463;TPE&#65288;c-TPE&#65289;&#26041;&#27861;&#65292;&#26159;&#26641;&#24418;Parzen&#20272;&#35745;&#22120;&#65288;TPE&#65289;&#30340;&#25193;&#23637;&#65292;&#21487;&#26377;&#25928;&#22788;&#29702;&#22312;&#24615;&#33021;&#35201;&#27714;&#20043;&#19978;&#26045;&#21152;&#30340;&#32422;&#26463;&#38480;&#21046;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;81&#20010;&#26114;&#36149;&#30340;HPO&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#26368;&#20339;&#24615;&#33021;&#25490;&#21517;&#12290;</title><link>http://arxiv.org/abs/2211.14411</link><description>&lt;p&gt;
c-TPE:&#22522;&#20110;&#26641;&#24418;&#32467;&#26500;&#30340;&#24102;&#19981;&#31561;&#24335;&#32422;&#26463;&#30340;&#24085;&#25463;&#26031;&#29305;&#20272;&#35745;&#22120;&#29992;&#20110;&#26114;&#36149;&#36229;&#21442;&#25968;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
c-TPE: Tree-structured Parzen Estimator with Inequality Constraints for Expensive Hyperparameter Optimization. (arXiv:2211.14411v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14411
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32422;&#26463;TPE&#65288;c-TPE&#65289;&#26041;&#27861;&#65292;&#26159;&#26641;&#24418;Parzen&#20272;&#35745;&#22120;&#65288;TPE&#65289;&#30340;&#25193;&#23637;&#65292;&#21487;&#26377;&#25928;&#22788;&#29702;&#22312;&#24615;&#33021;&#35201;&#27714;&#20043;&#19978;&#26045;&#21152;&#30340;&#32422;&#26463;&#38480;&#21046;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;81&#20010;&#26114;&#36149;&#30340;HPO&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#26368;&#20339;&#24615;&#33021;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#20248;&#21270;&#65288;HPO&#65289;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#24378;&#22823;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#23454;&#38469;&#24212;&#29992;&#36890;&#24120;&#20250;&#22312;&#24615;&#33021;&#35201;&#27714;&#20043;&#19978;&#26045;&#21152;&#19968;&#20123;&#38480;&#21046;&#65292;&#20363;&#22914;&#20869;&#23384;&#20351;&#29992;&#25110;&#24310;&#36831;&#31561;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32422;&#26463;TPE&#65288;c-TPE&#65289;&#65292;&#36825;&#26159;&#24191;&#27867;&#20351;&#29992;&#30340;&#22810;&#21151;&#33021;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#8212;&#8212;&#26641;&#24418;Parzen&#20272;&#35745;&#22120;&#65288;TPE&#65289;&#30340;&#25193;&#23637;&#65292;&#20197;&#22788;&#29702;&#36825;&#20123;&#32422;&#26463;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#25193;&#23637;&#19981;&#20165;&#26159;&#31616;&#21333;&#22320;&#23558;&#29616;&#26377;&#25910;&#30410;&#20989;&#25968;&#21644;&#21407;&#22987;TPE&#32452;&#21512;&#36215;&#26469;&#65292;&#32780;&#26159;&#21253;&#25324;&#20462;&#25913;&#26469;&#35299;&#20915;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20174;&#32463;&#39564;&#21644;&#29702;&#35770;&#19978;&#28145;&#20837;&#20998;&#26512;&#36825;&#20123;&#20462;&#25913;&#65292;&#25552;&#20379;&#20102;&#26377;&#20851;&#23427;&#20204;&#22914;&#20309;&#26377;&#25928;&#22320;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#30340;&#35265;&#35299;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;c-TPE&#22312;81&#20010;&#26114;&#36149;&#30340;HPO&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#26368;&#20339;&#30340;&#24179;&#22343;&#25490;&#21517;&#24615;&#33021;&#65292;&#20855;&#26377;&#32479;&#35745;&#26174;&#30528;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperparameter optimization (HPO) is crucial for strong performance of deep learning algorithms and real-world applications often impose some constraints, such as memory usage, or latency on top of the performance requirement. In this work, we propose constrained TPE (c-TPE), an extension of the widely-used versatile Bayesian optimization method, tree-structured Parzen estimator (TPE), to handle these constraints. Our proposed extension goes beyond a simple combination of an existing acquisition function and the original TPE, and instead includes modifications that address issues that cause poor performance. We thoroughly analyze these modifications both empirically and theoretically, providing insights into how they effectively overcome these challenges. In the experiments, we demonstrate that c-TPE exhibits the best average rank performance among existing methods with statistical significance on 81 expensive HPO settings.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#33647;&#29289;&#21453;&#24212;&#39044;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#26377;&#25928;&#20943;&#23569;&#38656;&#35201;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#26377;&#26395;&#25552;&#39640;&#27835;&#30103;&#26041;&#26696;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2211.05777</link><description>&lt;p&gt;
&#38754;&#21521;&#33647;&#29289;&#21453;&#24212;&#39044;&#27979;&#30340;&#28151;&#21512;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Hybrid quantum neural network for drug response prediction. (arXiv:2211.05777v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05777
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#33647;&#29289;&#21453;&#24212;&#39044;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#26377;&#25928;&#20943;&#23569;&#38656;&#35201;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#26377;&#26395;&#25552;&#39640;&#27835;&#30103;&#26041;&#26696;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30284;&#30151;&#26159;&#20840;&#29699;&#27515;&#20129;&#29575;&#36739;&#39640;&#30340;&#30142;&#30149;&#20043;&#19968;&#12290;&#30001;&#20110;&#27599;&#20010;&#30149;&#20363;&#30340;&#22522;&#22240;&#31361;&#21464;&#21508;&#19981;&#30456;&#21516;&#65292;&#21270;&#30103;&#30340;&#21103;&#20316;&#29992;&#38750;&#24120;&#20005;&#37325;&#65292;&#22240;&#27492;&#27599;&#20010;&#24739;&#32773;&#37117;&#38656;&#35201;&#23450;&#21046;&#21270;&#30340;&#27835;&#30103;&#26041;&#26696;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#33258;&#21160;&#21270;&#24182;&#25552;&#39640;&#33647;&#29289;&#36873;&#25321;&#30340;&#25928;&#29575;&#65292;&#20294;&#20854;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#20943;&#23569;&#25968;&#25454;&#38656;&#27714;&#12290;&#28151;&#21512;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#24050;&#34987;&#35777;&#26126;&#22312;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#30340;&#38382;&#39064;&#19978;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#12289;&#22270;&#21367;&#31215;&#21644;8&#37327;&#23376;&#27604;&#29305;&#21644;363&#23618;&#28145;&#24230;&#37327;&#23376;&#31070;&#32463;&#20803;&#30340;&#32452;&#21512;&#30340;&#28151;&#21512;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#29992;&#20110;&#33647;&#29289;&#21453;&#24212;&#39044;&#27979;&#12290;&#25105;&#20204;&#22312;&#20943;&#23569;&#36807;&#30340;&#30284;&#30151;&#33647;&#29289;&#25935;&#24863;&#24615;&#22522;&#22240;&#32452;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cancer is one of the leading causes of death worldwide. It is caused by a variety of genetic mutations, which makes every instance of the disease unique. Since chemotherapy can have extremely severe side effects, each patient requires a personalized treatment plan. Finding the dosages that maximize the beneficial effects of the drugs and minimize their adverse side effects is vital. Deep neural networks automate and improve drug selection. However, they require a lot of data to be trained on. Therefore, there is a need for machine-learning approaches that require less data. Hybrid quantum neural networks were shown to provide a potential advantage in problems where training data availability is limited. We propose a novel hybrid quantum neural network for drug response prediction, based on a combination of convolutional, graph convolutional, and deep quantum neural layers of 8 qubits with 363 layers. We test our model on the reduced Genomics of Drug Sensitivity in Cancer dataset and sh
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32463;&#39564;&#35299;&#37322;&#25216;&#26415;&#65292;&#36890;&#36807;&#35757;&#32451;&#24433;&#21709;&#39044;&#27979;&#22120;&#26469;&#24674;&#22797;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#20449;&#24687;&#65292;&#20351;&#24471;&#38750;AI&#19987;&#23478;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#20854;&#20915;&#31574;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2210.04723</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#30340;&#32463;&#39564;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Experiential Explanations for Reinforcement Learning. (arXiv:2210.04723v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04723
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32463;&#39564;&#35299;&#37322;&#25216;&#26415;&#65292;&#36890;&#36807;&#35757;&#32451;&#24433;&#21709;&#39044;&#27979;&#22120;&#26469;&#24674;&#22797;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#20449;&#24687;&#65292;&#20351;&#24471;&#38750;AI&#19987;&#23478;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#20854;&#20915;&#31574;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#21487;&#33021;&#38750;&#24120;&#22797;&#26434;&#21644;&#26080;&#27861;&#35299;&#37322;&#65292;&#36825;&#20351;&#24471;&#38750;&#20154;&#24037;&#26234;&#33021;&#19987;&#23478;&#38590;&#20197;&#29702;&#35299;&#25110;&#24178;&#39044;&#23427;&#20204;&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32463;&#39564;&#35299;&#37322;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#26049;&#36793;&#35757;&#32451;&#24433;&#21709;&#39044;&#27979;&#22120;&#26469;&#29983;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;&#12290;&#24433;&#21709;&#39044;&#27979;&#22120;&#26159;&#23398;&#20064;&#22870;&#21169;&#26469;&#28304;&#22914;&#20309;&#24433;&#21709;&#20195;&#29702;&#22312;&#19981;&#21516;&#29366;&#24577;&#19979;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#24674;&#22797;&#26377;&#20851;&#31574;&#30053;&#22914;&#20309;&#21453;&#26144;&#29615;&#22659;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) systems can be complex and non-interpretable, making it challenging for non-AI experts to understand or intervene in their decisions. This is due, in part, to the sequential nature of RL in which actions are chosen because of future rewards. However, RL agents discard the qualitative features of their training, making it hard to recover user-understandable information for "why" an action is chosen. Proposed sentence chunking: We propose a technique Experiential Explanations to generate counterfactual explanations by training influence predictors alongside the RL policy. Influence predictors are models that learn how sources of reward affect the agent in different states, thus restoring information about how the policy reflects the environment. A human evaluation study revealed that participants presented with experiential explanations were better able to correctly guess what an agent would do than those presented with other standard types of explanations. Pa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#35299;&#37322;prompt learning&#22312;&#38646;&#26679;&#26412;/&#23569;&#26679;&#26412;&#22330;&#26223;&#19979;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22522;&#20110;&#27492;&#25552;&#20986;&#20102;&#19968;&#20010;&#27880;&#37322;&#26080;&#20851;&#30340;&#27169;&#26495;&#36873;&#25321;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2209.15206</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20026;&#20160;&#20040;&#26356;&#36866;&#21512;&#38646;&#26679;&#26412;&#23398;&#20064;&#65311;
&lt;/p&gt;
&lt;p&gt;
What Makes Pre-trained Language Models Better Zero-shot Learners?. (arXiv:2209.15206v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#35299;&#37322;prompt learning&#22312;&#38646;&#26679;&#26412;/&#23569;&#26679;&#26412;&#22330;&#26223;&#19979;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22522;&#20110;&#27492;&#25552;&#20986;&#20102;&#19968;&#20010;&#27880;&#37322;&#26080;&#20851;&#30340;&#27169;&#26495;&#36873;&#25321;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#35299;&#37322;prompt learning&#22312;&#38646;&#26679;&#26412;/&#23569;&#26679;&#26412;&#22330;&#26223;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20256;&#32479;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#33539;&#24335;&#22312;&#23569;&#26679;&#26412;&#22330;&#26223;&#19979;&#20250;&#22240;&#20026;&#36807;&#25311;&#21512;&#19981;&#20855;&#20195;&#34920;&#24615;&#30340;&#26631;&#27880;&#25968;&#25454;&#32780;&#22833;&#36133;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35814;&#32454;&#38416;&#36848;&#20102;prompt learning&#26356;&#26377;&#25928;&#30340;&#20551;&#35774;&#65292;&#22240;&#20026;&#23427;&#20351;&#24314;&#31435;&#22312;&#28023;&#37327;&#25991;&#26412;&#35821;&#26009;&#24211;&#21644;&#39046;&#22495;&#30456;&#20851;&#20154;&#31867;&#30693;&#35782;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26356;&#22810;&#22320;&#21442;&#19982;&#39044;&#27979;&#65292;&#20174;&#32780;&#20943;&#23569;&#23567;&#22411;&#35757;&#32451;&#38598;&#25552;&#20379;&#30340;&#26377;&#38480;&#26631;&#31614;&#20449;&#24687;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20551;&#35774;&#65292;&#35821;&#35328;&#24046;&#24322;&#21487;&#20197;&#34913;&#37327;&#25552;&#31034;&#36136;&#37327;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#20551;&#35774;&#12290;&#26356;&#20026;&#37325;&#35201;&#30340;&#26159;&#65292;&#21463;&#21040;&#29702;&#35770;&#26694;&#26550;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22256;&#24785;&#24230;&#30340;&#27880;&#37322;&#26080;&#20851;&#30340;&#27169;&#26495;&#36873;&#25321;&#26041;&#27861;&#65292;&#21487;&#20197;&#20107;&#20808;&#8220;&#39044;&#27979;&#8221;&#25552;&#31034;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#29305;&#21035;&#20540;&#24471;&#40723;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a theoretical framework to explain the efficacy of prompt learning in zero/few-shot scenarios. First, we prove that conventional pre-training and fine-tuning paradigm fails in few-shot scenarios due to overfitting the unrepresentative labelled data. We then detail the assumption that prompt learning is more effective because it empowers pre-trained language model that is built upon massive text corpora, as well as domain-related human knowledge to participate more in prediction and thereby reduces the impact of limited label information provided by the small training set. We further hypothesize that language discrepancy can measure the quality of prompting. Comprehensive experiments are performed to verify our assumptions. More remarkably, inspired by the theoretical framework, we propose an annotation-agnostic template selection method based on perplexity, which enables us to ``forecast'' the prompting performance in advance. This approach is especially encou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21442;&#25968;&#20462;&#21098;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#33976;&#39311;&#36807;&#31243;&#20013;&#20462;&#21098;&#38590;&#20197;&#21305;&#37197;&#30340;&#21442;&#25968;&#65292;&#25552;&#39640;&#33976;&#39311;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.14609</link><description>&lt;p&gt;
&#21442;&#25968;&#20462;&#21098;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Dataset Distillation Using Parameter Pruning. (arXiv:2209.14609v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21442;&#25968;&#20462;&#21098;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#33976;&#39311;&#36807;&#31243;&#20013;&#20462;&#21098;&#38590;&#20197;&#21305;&#37197;&#30340;&#21442;&#25968;&#65292;&#25552;&#39640;&#33976;&#39311;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#65292;&#33719;&#24471;&#20808;&#36827;&#27169;&#22411;&#30340;&#26041;&#27861;&#21462;&#20915;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#36825;&#20351;&#24471;&#25968;&#25454;&#23384;&#20648;&#21644;&#27169;&#22411;&#35757;&#32451;&#21464;&#24471;&#26114;&#36149;&#12290;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#25968;&#25454;&#38598;&#33976;&#39311;&#21487;&#20197;&#21512;&#25104;&#20445;&#30041;&#21407;&#22987;&#22823;&#22411;&#25968;&#25454;&#38598;&#22823;&#22810;&#25968;&#20449;&#24687;&#30340;&#23567;&#22411;&#25968;&#25454;&#38598;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#21305;&#37197;&#32593;&#32476;&#21442;&#25968;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#32593;&#32476;&#21442;&#25968;&#30340;&#32500;&#24230;&#36890;&#24120;&#24456;&#22823;&#12290;&#27492;&#22806;&#65292;&#19968;&#20123;&#21442;&#25968;&#22312;&#33976;&#39311;&#36807;&#31243;&#20013;&#38590;&#20197;&#21305;&#37197;&#65292;&#38477;&#20302;&#20102;&#33976;&#39311;&#24615;&#33021;&#12290;&#22522;&#20110;&#36825;&#20010;&#35266;&#23519;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21442;&#25968;&#20462;&#21098;&#30340;&#26032;&#22411;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#33976;&#39311;&#36807;&#31243;&#20013;&#20462;&#21098;&#38590;&#20197;&#21305;&#37197;&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#21512;&#25104;&#26356;&#21152;&#31283;&#20581;&#30340;&#33976;&#39311;&#25968;&#25454;&#38598;&#24182;&#25552;&#39640;&#33976;&#39311;&#24615;&#33021;&#12290;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many fields, the acquisition of advanced models depends on large datasets, making data storage and model training expensive. As a solution, dataset distillation can synthesize a small dataset that preserves most information of the original large dataset. The recently proposed dataset distillation method by matching network parameters has been proven effective for several datasets. However, the dimensions of network parameters are typically large. Furthermore, some parameters are difficult to match during the distillation process, degrading distillation performance. Based on this observation, this study proposes a novel dataset distillation method based on parameter pruning that solves the problem. The proposed method can synthesize more robust distilled datasets and improve distillation performance by pruning difficult-to-match parameters during the distillation process. Experimental results on three datasets show that the proposed method outperforms other state-of-the-art dataset d
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#27861;&#24459;&#25351;&#23548;&#20195;&#30721;&#8221;&#29702;&#24565;&#65292;&#21033;&#29992;&#27861;&#24459;&#20449;&#24687;&#23398;&#23558;&#27861;&#24459;&#30693;&#35782;&#21644;&#25512;&#29702;&#23884;&#20837;&#21040;&#20154;&#24037;&#26234;&#33021;&#20013;&#65292;&#20174;&#32780;&#20351;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#30446;&#26631;&#21644;&#31038;&#20250;&#20215;&#20540;&#20445;&#25345;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2209.13020</link><description>&lt;p&gt;
&#27861;&#24459;&#24341;&#23548;&#20195;&#30721;&#65306;&#19968;&#31181;&#27861;&#24459;&#20449;&#24687;&#23398;&#26041;&#27861;&#26469;&#20351;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20445;&#25345;&#19968;&#33268;
&lt;/p&gt;
&lt;p&gt;
Law Informs Code: A Legal Informatics Approach to Aligning Artificial Intelligence with Humans. (arXiv:2209.13020v13 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.13020
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#27861;&#24459;&#25351;&#23548;&#20195;&#30721;&#8221;&#29702;&#24565;&#65292;&#21033;&#29992;&#27861;&#24459;&#20449;&#24687;&#23398;&#23558;&#27861;&#24459;&#30693;&#35782;&#21644;&#25512;&#29702;&#23884;&#20837;&#21040;&#20154;&#24037;&#26234;&#33021;&#20013;&#65292;&#20174;&#32780;&#20351;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#30446;&#26631;&#21644;&#31038;&#20250;&#20215;&#20540;&#20445;&#25345;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#25105;&#20204;&#26080;&#27861;&#21487;&#38752;&#22320;&#25351;&#23450;&#20154;&#31867;&#30340;&#30446;&#26631;&#21644;&#31038;&#20250;&#20215;&#20540;&#65292;&#20197;&#24341;&#23548;&#20154;&#24037;&#26234;&#33021;&#30340;&#34892;&#20026;&#12290;&#21046;&#23450;&#27861;&#24459;&#21644;&#35299;&#37322;&#27861;&#24459;&#26500;&#25104;&#20102;&#19968;&#31181;&#35745;&#31639;&#24341;&#25806;&#65292;&#23558;&#19981;&#36879;&#26126;&#30340;&#20154;&#31867;&#20215;&#20540;&#36716;&#21270;&#20026;&#26131;&#35835;&#30340;&#25351;&#20196;&#12290; &#8220;&#27861;&#24459;&#25351;&#23548;&#20195;&#30721;&#8221;&#26159;&#23884;&#20837;&#20102;&#27861;&#24459;&#30693;&#35782;&#21644;&#25512;&#29702;&#30340;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#35758;&#31243;&#12290;&#31867;&#20284;&#20110;&#21512;&#21516;&#24403;&#20107;&#20154;&#26080;&#27861;&#39044;&#35265;&#20182;&#20204;&#26410;&#26469;&#20851;&#31995;&#30340;&#27599;&#20010;&#28508;&#22312;&#21464;&#25968;&#65292;&#31435;&#27861;&#32773;&#26080;&#27861;&#39044;&#27979;&#20854;&#25552;&#20986;&#30340;&#27861;&#26696;&#23558;&#36866;&#29992;&#30340;&#25152;&#26377;&#24773;&#20917;&#65292;&#25105;&#20204;&#26080;&#27861;&#25552;&#21069;&#26126;&#30830;&#35268;&#21017;&#65292;&#20197;&#21487;&#38752;&#22320;&#24341;&#23548;&#33391;&#22909;&#30340;&#20154;&#24037;&#26234;&#33021;&#34892;&#20026;&#12290;&#27861;&#24459;&#29702;&#35770;&#21644;&#23454;&#36341;&#24050;&#32463;&#24320;&#21457;&#20986;&#21508;&#31181;&#24037;&#20855;&#26469;&#35299;&#20915;&#36825;&#20123;&#35268;&#23450;&#38382;&#39064;&#12290;&#19982;&#27861;&#24459;&#26356;&#20026;&#26222;&#36890;&#30340;&#29992;&#36884;&#65288;&#20363;&#22914;&#36890;&#36807;&#21046;&#35009;&#23041;&#32961;&#26469;&#38459;&#27490;&#19981;&#33391;&#34892;&#20026;&#65289;&#30456;&#21453;&#65292;&#27861;&#24459;&#20316;&#20026;&#19968;&#31181;&#34920;&#36798;&#20154;&#31867;&#27807;&#36890;&#30446;&#26631;&#21644;&#20215;&#20540;&#30340;&#34920;&#29616;&#65292;&#21487;&#20197;&#24341;&#23548;&#20154;&#24037;&#26234;&#33021;&#20195;&#30721;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#27861;&#24459;&#20449;&#24687;&#23398;&#26159;&#35745;&#31639;&#35268;&#21017;&#21644;&#31995;&#32479;&#29992;&#20110;&#34920;&#31034;&#65292;&#20998;&#26512;&#21644;&#25805;&#20316;&#27861;&#24459;&#30693;&#35782;&#30340;&#36328;&#23398;&#31185;&#30740;&#31350;&#12290;&#27861;&#24459;&#25351;&#23548;&#20195;&#30721;&#21033;&#29992;&#27861;&#24459;&#20449;&#24687;&#23398;&#26469;&#20351;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#30446;&#26631;&#21644;&#31038;&#20250;&#20215;&#20540;&#20445;&#25345;&#19968;&#33268;&#65292;&#20197;&#19968;&#31181;&#36879;&#26126;&#65292;&#36127;&#36131;&#65292;&#28789;&#27963;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
We are currently unable to specify human goals and societal values in a way that reliably directs AI behavior. Law-making and legal interpretation form a computational engine that converts opaque human values into legible directives. "Law Informs Code" is the research agenda embedding legal knowledge and reasoning in AI. Similar to how parties to a legal contract cannot foresee every potential contingency of their future relationship, and legislators cannot predict all the circumstances under which their proposed bills will be applied, we cannot ex ante specify rules that provably direct good AI behavior. Legal theory and practice have developed arrays of tools to address these specification problems. For instance, legal standards allow humans to develop shared understandings and adapt them to novel situations. In contrast to more prosaic uses of the law (e.g., as a deterrent of bad behavior through the threat of sanction), leveraged as an expression of how humans communicate their goa
&lt;/p&gt;</description></item><item><title>FNet&#27169;&#22411;&#36890;&#36807;&#26367;&#25442;&#27880;&#24847;&#21147;&#23618;&#20026;&#20613;&#37324;&#21494;&#21464;&#25442;&#65292;&#21152;&#36895;&#20102;Transformer&#32534;&#30721;&#22120;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#24182;&#20445;&#25345;&#30456;&#21516;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2209.12816</link><description>&lt;p&gt;
&#24555;&#36895;FNet&#65306;&#36890;&#36807;&#39640;&#25928;&#30340;&#20613;&#37324;&#21494;&#23618;&#21152;&#36895;Transformer&#32534;&#30721;&#22120;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Fast-FNet: Accelerating Transformer Encoder Models via Efficient Fourier Layers. (arXiv:2209.12816v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12816
&lt;/p&gt;
&lt;p&gt;
FNet&#27169;&#22411;&#36890;&#36807;&#26367;&#25442;&#27880;&#24847;&#21147;&#23618;&#20026;&#20613;&#37324;&#21494;&#21464;&#25442;&#65292;&#21152;&#36895;&#20102;Transformer&#32534;&#30721;&#22120;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#24182;&#20445;&#25345;&#30456;&#21516;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#20960;&#20046;&#25152;&#26377;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#30456;&#20284;&#30340;&#27880;&#24847;&#21147;&#32467;&#26500;&#20063;&#22312;&#20854;&#20182;&#39046;&#22495;&#24191;&#27867;&#30740;&#31350;&#12290;&#34429;&#28982;&#27880;&#24847;&#21147;&#26426;&#21046;&#26174;&#33879;&#22686;&#24378;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#20294;&#20854;&#20108;&#27425;&#22797;&#26434;&#24230;&#38459;&#30861;&#20102;&#23545;&#38271;&#24207;&#21015;&#30340;&#39640;&#25928;&#22788;&#29702;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#28040;&#38500;&#35745;&#31639;&#25928;&#29575;&#30340;&#32570;&#28857;&#19978;&#65292;&#24182;&#34920;&#26126;&#22312;&#26080;&#38656;&#27880;&#24847;&#21147;&#23618;&#30340;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#20173;&#28982;&#33021;&#22815;&#36798;&#21040;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;&#19968;&#39033;&#24320;&#21019;&#24615;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;FNet&#65292;&#22312;Transformer&#32534;&#30721;&#22120;&#32467;&#26500;&#20013;&#29992;&#20613;&#37324;&#21494;&#21464;&#25442;&#65288;FT&#65289;&#26367;&#25442;&#27880;&#24847;&#21147;&#23618;&#12290;FNet&#22312;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#20013;&#21435;&#38500;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#36816;&#31639;&#36127;&#25285;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#19982;&#21407;&#22987;Transformer&#32534;&#30721;&#22120;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;FNet&#27169;&#22411;&#24573;&#30053;&#20102;FT&#30340;&#22522;&#26412;&#23646;&#24615;..
&lt;/p&gt;
&lt;p&gt;
Transformer-based language models utilize the attention mechanism for substantial performance improvements in almost all natural language processing (NLP) tasks. Similar attention structures are also extensively studied in several other areas. Although the attention mechanism enhances the model performances significantly, its quadratic complexity prevents efficient processing of long sequences. Recent works focused on eliminating the disadvantages of computational inefficiency and showed that transformer-based models can still reach competitive results without the attention layer. A pioneering study proposed the FNet, which replaces the attention layer with the Fourier Transform (FT) in the transformer encoder architecture. FNet achieves competitive performances concerning the original transformer encoder model while accelerating training process by removing the computational burden of the attention mechanism. However, the FNet model ignores essential properties of the FT from the clas
&lt;/p&gt;</description></item><item><title>WeLM &#26159;&#19968;&#31181;&#38754;&#21521;&#20013;&#25991;&#30340;&#35835;&#36807;&#20070;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#35835;&#21462;&#39640;&#36136;&#37327;&#30340;&#35821;&#26009;&#24211;&#35757;&#32451;&#20102;100&#20159;&#20010;&#21442;&#25968;&#65292;&#24182;&#21487;&#20197;&#22312;18&#20010;&#20013;&#25991;&#20219;&#21153;&#20013;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#21516;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21516;&#26102;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#22810;&#35821;&#35328;&#21644;&#20195;&#30721;&#36716;&#25442;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2209.10372</link><description>&lt;p&gt;
WeLM: &#19968;&#31181;&#38754;&#21521;&#20013;&#25991;&#30340;&#35835;&#36807;&#20070;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
WeLM: A Well-Read Pre-trained Language Model for Chinese. (arXiv:2209.10372v5 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.10372
&lt;/p&gt;
&lt;p&gt;
WeLM &#26159;&#19968;&#31181;&#38754;&#21521;&#20013;&#25991;&#30340;&#35835;&#36807;&#20070;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#35835;&#21462;&#39640;&#36136;&#37327;&#30340;&#35821;&#26009;&#24211;&#35757;&#32451;&#20102;100&#20159;&#20010;&#21442;&#25968;&#65292;&#24182;&#21487;&#20197;&#22312;18&#20010;&#20013;&#25991;&#20219;&#21153;&#20013;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#21516;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21516;&#26102;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#22810;&#35821;&#35328;&#21644;&#20195;&#30721;&#36716;&#25442;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20197;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#20026;&#22522;&#30784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24191;&#27867;&#30340;&#20219;&#21153;&#20013;&#26174;&#31034;&#20102;&#20986;&#33394;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;WeLM&#65306;&#19968;&#31181;&#33021;&#22815;&#38646;&#25110;&#23569;&#26679;&#26412;&#28436;&#31034;&#26080;&#32541;&#25191;&#34892;&#19981;&#21516;&#31867;&#22411;&#20219;&#21153;&#30340;&#38754;&#21521;&#20013;&#25991;&#30340;&#35835;&#36807;&#20070;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;WeLM&#36890;&#36807;&#38405;&#35835;&#31934;&#24515;&#31574;&#21010;&#30340;&#39640;&#36136;&#37327;&#35821;&#26009;&#24211;&#20013;&#30340;&#20449;&#24687;&#65292;&#20197;100&#20159;&#20010;&#21442;&#25968;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;WeLM&#22312;&#21508;&#20010;&#39046;&#22495;&#21644;&#35821;&#35328;&#26041;&#38754;&#20855;&#22791;&#24191;&#27867;&#30340;&#30693;&#35782;&#12290;&#22312;18&#39033;&#29420;&#31435;&#30340;&#65288;&#20013;&#25991;&#65289;&#20219;&#21153;&#20013;&#65292;WeLM&#33021;&#22815;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#35268;&#27169;&#30456;&#20284;&#19988;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#19988;&#33021;&#22815;&#21305;&#37197;&#35268;&#27169;&#39640;&#36798;25&#20493;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;WeLM&#36824;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#22810;&#35821;&#35328;&#21644;&#20195;&#30721;&#36716;&#25442;&#29702;&#35299;&#33021;&#21147;&#65292;&#20248;&#20110;&#39044;&#20808;&#22312;30&#31181;&#35821;&#35328;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#29616;&#26377;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#22823;&#37327;&#20013;&#25991;&#30417;&#30563;&#24335;&#25968;&#25454;&#38598;&#25910;&#38598;&#20102;&#20154;&#24037;&#32534;&#20889;&#30340;&#25552;&#31034;&#65292;&#24182;&#23545;WeLM&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models pre-trained with self-supervised learning have demonstrated impressive zero-shot generalization capabilities on a wide spectrum of tasks. In this work, we present WeLM: a well-read pre-trained language model for Chinese that is able to seamlessly perform different types of tasks with zero or few-shot demonstrations. WeLM is trained with 10B parameters by "reading" a curated high-quality corpus covering a wide range of topics. We show that WeLM is equipped with broad knowledge on various domains and languages. On 18 monolingual (Chinese) tasks, WeLM can significantly outperform existing pre-trained models with similar sizes and match the performance of models up to 25 times larger. WeLM also exhibits strong capabilities in multi-lingual and code-switching understanding, outperforming existing multilingual language models pre-trained on 30 languages. Furthermore, We collected human-written prompts for a large set of supervised datasets in Chinese and fine-tuned WeLM
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#27169;&#31946;&#35268;&#21017;&#31995;&#32479;&#30340;&#26368;&#26032;&#36235;&#21183;&#21644;&#24212;&#29992;&#65292;&#21253;&#25324;GFS&#12289;HFS&#12289;NFS&#12289;eFS&#12289;&#29992;&#20110;&#22823;&#25968;&#25454;&#21644;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;FRBS&#20197;&#21450;&#21487;&#35299;&#37322;&#24615;&#21644;&#20351;&#29992;&#32858;&#31867;&#20013;&#24515;&#20316;&#20026;&#27169;&#31946;&#35268;&#21017;&#30340;FRBS&#12290;&#37325;&#28857;&#31361;&#20986;&#20854;&#37325;&#35201;&#36129;&#29486;&#21644;&#20986;&#29256;&#32479;&#35745;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2209.07175</link><description>&lt;p&gt;
&#22810;&#31181;&#27169;&#31946;&#35268;&#21017;&#31995;&#32479;&#30340;&#26368;&#26032;&#36235;&#21183;&#21644;&#24212;&#29992;&#30340;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Literature Review of the Recent Trends and Applications in various Fuzzy Rule based systems. (arXiv:2209.07175v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.07175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#27169;&#31946;&#35268;&#21017;&#31995;&#32479;&#30340;&#26368;&#26032;&#36235;&#21183;&#21644;&#24212;&#29992;&#65292;&#21253;&#25324;GFS&#12289;HFS&#12289;NFS&#12289;eFS&#12289;&#29992;&#20110;&#22823;&#25968;&#25454;&#21644;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;FRBS&#20197;&#21450;&#21487;&#35299;&#37322;&#24615;&#21644;&#20351;&#29992;&#32858;&#31867;&#20013;&#24515;&#20316;&#20026;&#27169;&#31946;&#35268;&#21017;&#30340;FRBS&#12290;&#37325;&#28857;&#31361;&#20986;&#20854;&#37325;&#35201;&#36129;&#29486;&#21644;&#20986;&#29256;&#32479;&#35745;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#31946;&#35268;&#21017;&#31995;&#32479;&#65288;FRBS&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#31995;&#32479;&#65292;&#23427;&#20351;&#29992;&#35821;&#35328;&#27169;&#31946;&#21464;&#37327;&#20316;&#20026;&#21069;&#25552;&#21644;&#32467;&#35770;&#26469;&#34920;&#31034;&#20154;&#21487;&#29702;&#35299;&#30340;&#30693;&#35782;&#12290;&#23427;&#20204;&#24050;&#32463;&#24212;&#29992;&#20110;&#36719;&#35745;&#31639;&#25991;&#29486;&#20013;&#30340;&#21508;&#20010;&#24212;&#29992;&#21644;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;FRBS&#38754;&#20020;&#35768;&#22810;&#32570;&#28857;&#65292;&#20363;&#22914;&#19981;&#30830;&#23450;&#24615;&#34920;&#31034;&#65292;&#35268;&#21017;&#25968;&#37327;&#36807;&#22810;&#65292;&#35299;&#37322;&#33021;&#21147;&#20007;&#22833;&#65292;&#23398;&#20064;&#25152;&#38656;&#30340;&#35745;&#31639;&#26102;&#38388;&#38271;&#31561;&#12290;&#20026;&#20102;&#20811;&#26381;FRBS&#30340;&#36825;&#20123;&#38382;&#39064;&#65292;&#23384;&#22312;&#35768;&#22810;FRBS&#30340;&#25193;&#23637;&#12290;&#26412;&#25991;&#27010;&#36848;&#21644;&#25991;&#29486;&#32508;&#36848;&#20102;&#27169;&#31946;&#31995;&#32479;&#65288;FRBS&#65289;&#30340;&#21508;&#31181;&#31867;&#22411;&#21644;&#31361;&#20986;&#39046;&#22495;&#30340;&#26368;&#26032;&#36235;&#21183;&#65292;&#21253;&#25324;&#36951;&#20256;&#27169;&#31946;&#31995;&#32479;&#65288;GFS&#65289;&#12289;&#23618;&#32423;&#27169;&#31946;&#31995;&#32479;&#65288;HFS&#65289;&#12289;&#31070;&#32463;&#27169;&#31946;&#31995;&#32479;&#65288;NFS&#65289;&#12289;&#36827;&#21270;&#27169;&#31946;&#31995;&#32479;&#65288;eFS&#65289;&#12289;&#29992;&#20110;&#22823;&#25968;&#25454;&#30340;FRBS&#12289;&#29992;&#20110;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;FRBS&#12289;FRBS&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#20197;&#21450;&#23558;&#32858;&#31867;&#20013;&#24515;&#29992;&#20316;&#27169;&#31946;&#35268;&#21017;&#30340;FRBS&#12290;&#32508;&#36848;&#30340;&#26102;&#38388;&#33539;&#22260;&#20026;2010&#24180;&#33267;2021&#24180;&#12290;&#26412;&#25991;&#36824;&#24378;&#35843;&#20102;&#37325;&#35201;&#30340;&#36129;&#29486;&#12289;&#20986;&#29256;&#32479;&#35745;&#25968;&#25454;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fuzzy rule based systems (FRBSs) is a rule-based system which uses linguistic fuzzy variables as antecedents and consequent to represent human understandable knowledge. They have been applied to various applications and areas throughout the soft computing literature. However, FRBSs suffers from many drawbacks such as uncertainty representation, high number of rules, interpretability loss, high computational time for learning etc. To overcome these issues with FRBSs, there exists many extensions of FRBSs. This paper presents an overview and literature review of recent trends on various types and prominent areas of fuzzy systems (FRBSs) namely genetic fuzzy system (GFS), hierarchical fuzzy system (HFS), neuro fuzzy system (NFS), evolving fuzzy system (eFS), FRBSs for big data, FRBSs for imbalanced data, interpretability in FRBSs and FRBSs which use cluster centroids as fuzzy rules. The review is for years 2010-2021. This paper also highlights important contributions, publication statisti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27169;&#25311;&#36741;&#21161;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#32771;&#34385;&#25317;&#22581;&#20381;&#36182;&#24310;&#36831;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#39640;&#22823;&#35268;&#27169;&#30095;&#25955;&#35745;&#21010;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#34920;&#29616;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2209.01535</link><description>&lt;p&gt;
&#21033;&#29992;&#27169;&#25311;&#36741;&#21161;&#20248;&#21270;&#22823;&#35268;&#27169;&#30095;&#25955;&#35745;&#21010;&#20013;&#30340;&#25317;&#22581;&#20381;&#36182;&#24310;&#36831;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Simulation-Assisted Optimization for Large-Scale Evacuation Planning with Congestion-Dependent Delays. (arXiv:2209.01535v5 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.01535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27169;&#25311;&#36741;&#21161;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#32771;&#34385;&#25317;&#22581;&#20381;&#36182;&#24310;&#36831;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#39640;&#22823;&#35268;&#27169;&#30095;&#25955;&#35745;&#21010;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#34920;&#29616;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30095;&#25955;&#35745;&#21010;&#26159;&#28798;&#38590;&#31649;&#29702;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#23558;&#36335;&#32447;&#21644;&#35843;&#24230;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#36827;&#34892;&#32852;&#21512;&#20248;&#21270;&#65292;&#20197;&#36798;&#21040;&#26368;&#23567;&#21270;&#24179;&#22343;&#30095;&#25955;&#26102;&#38388;&#25110;&#30095;&#25955;&#23436;&#25104;&#26102;&#38388;&#31561;&#30446;&#26631;&#65292;&#26159;&#19968;&#20010;&#35745;&#31639;&#38590;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MIP-LNS&#65292;&#36825;&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#21551;&#21457;&#24335;&#25628;&#32034;&#21644;&#25968;&#23398;&#20248;&#21270;&#26469;&#20248;&#21270;&#21508;&#31181;&#30446;&#26631;&#20989;&#25968;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;MIP-LNS-SIM&#26041;&#27861;&#65292;&#32467;&#21512;&#22522;&#20110;&#20195;&#29702;&#30340;&#27169;&#25311;&#20197;&#20272;&#35745;&#30001;&#20110;&#25317;&#22581;&#32780;&#24341;&#36215;&#30340;&#24310;&#36831;&#65292;&#24182;&#25214;&#21040;&#32771;&#34385;&#36825;&#20123;&#24310;&#36831;&#30340;&#20248;&#21270;&#35745;&#21010;&#12290;&#25105;&#20204;&#20197;&#24471;&#20811;&#33832;&#26031;&#24030;&#20241;&#26031;&#39039;&#24066;&#30340;&#21704;&#37324;&#26031;&#21439;&#20316;&#20026;&#30740;&#31350;&#21306;&#22495;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#32473;&#23450;&#30340;&#26102;&#38388;&#38480;&#21046;&#19979;&#65292;MIP-LNS&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#22312;&#19977;&#20010;&#19981;&#21516;&#25351;&#26631;&#26041;&#38754;&#25214;&#21040;&#26356;&#22909;&#30340;&#35299;&#12290;&#28982;&#32780;&#65292;&#24403;&#32771;&#34385;&#21040;&#25317;&#22581;&#20381;&#36182;&#30340;&#24310;&#36831;&#26102;&#65292;MIP-LNS-SIM&#22312;&#22810;&#20010;&#24615;&#33021;&#25351;&#26631;&#19978;&#20248;&#20110;MIP-LNS&#12290;&#27492;&#22806;&#65292;MIP-LNS-SIM&#20855;&#26377;&#26126;&#26174;&#36739;&#20302;&#30340;&#27714;&#35299;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evacuation planning is a crucial part of disaster management. However, joint optimization of its two essential components, routing and scheduling, with objectives such as minimizing average evacuation time or evacuation completion time, is a computationally hard problem. To approach it, we present MIP-LNS, a scalable optimization method that utilizes heuristic search with mathematical optimization and can optimize a variety of objective functions. We also present the method MIP-LNS-SIM, where we combine agent-based simulation with MIP-LNS to estimate delays due to congestion, as well as, find optimized plans considering such delays. We use Harris County in Houston, Texas, as our study area. We show that, within a given time limit, MIP-LNS finds better solutions than existing methods in terms of three different metrics. However, when congestion dependent delay is considered, MIP-LNS-SIM outperforms MIP-LNS in multiple performance metrics. In addition, MIP-LNS-SIM has a significantly low
&lt;/p&gt;</description></item><item><title>BERT4Loc&#26159;&#19968;&#31181;&#22522;&#20110;BERT&#30340;&#20301;&#32622;&#25512;&#33616;&#31995;&#32479;&#65292;&#23558;&#22320;&#29702;&#20301;&#32622;&#20449;&#24687;&#21644;&#29992;&#25143;&#20559;&#22909;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#22522;&#20110;&#20301;&#32622;&#30340;&#24314;&#35758;&#65292;&#30456;&#36739;&#20110;&#39044;&#27979;&#24207;&#21015;&#20013;&#19979;&#19968;&#20010;POI&#30340;&#27169;&#22411;&#65292;&#25552;&#20379;&#26356;&#30456;&#20851;&#30340;&#25512;&#33616;&#65292;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2208.01375</link><description>&lt;p&gt;
BERT4Loc&#65306;&#22522;&#20110;BERT&#30340;&#20301;&#32622;&#25512;&#33616;&#31995;&#32479;--POI&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
BERT4Loc: BERT for Location -- POI Recommender System. (arXiv:2208.01375v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.01375
&lt;/p&gt;
&lt;p&gt;
BERT4Loc&#26159;&#19968;&#31181;&#22522;&#20110;BERT&#30340;&#20301;&#32622;&#25512;&#33616;&#31995;&#32479;&#65292;&#23558;&#22320;&#29702;&#20301;&#32622;&#20449;&#24687;&#21644;&#29992;&#25143;&#20559;&#22909;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#22522;&#20110;&#20301;&#32622;&#30340;&#24314;&#35758;&#65292;&#30456;&#36739;&#20110;&#39044;&#27979;&#24207;&#21015;&#20013;&#19979;&#19968;&#20010;POI&#30340;&#27169;&#22411;&#65292;&#25552;&#20379;&#26356;&#30456;&#20851;&#30340;&#25512;&#33616;&#65292;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#20852;&#36259;&#28857;&#65288;POI&#65289;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#20174;&#22522;&#20110;&#20301;&#32622;&#30340;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#20013;&#25552;&#21462;&#20840;&#38754;&#30340;&#20301;&#32622;&#25968;&#25454;&#12290;&#20026;&#20102;&#25552;&#20379;&#26377;&#25928;&#30340;&#22522;&#20110;&#20301;&#32622;&#30340;&#25512;&#33616;&#65292;&#20998;&#26512;&#29992;&#25143;&#30340;&#21382;&#21490;&#34892;&#20026;&#21644;&#20559;&#22909;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22797;&#26434;&#30340;&#20301;&#32622;&#24863;&#30693;&#25512;&#33616;&#31995;&#32479;&#65292;&#21033;&#29992;Transformers&#20013;&#30340;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#65288;BERT&#65289;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#22522;&#20110;&#20301;&#32622;&#30340;&#24314;&#35758;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#23558;&#22320;&#29702;&#20301;&#32622;&#20449;&#24687;&#21644;&#29992;&#25143;&#20559;&#22909;&#30456;&#32467;&#21512;&#65292;&#30456;&#27604;&#20110;&#39044;&#27979;&#24207;&#21015;&#20013;&#19979;&#19968;&#20010;POI&#30340;&#27169;&#22411;&#65292;&#25552;&#20379;&#26356;&#30456;&#20851;&#30340;&#25512;&#33616;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#22522;&#20110;BERT&#30340;&#27169;&#22411;&#20248;&#20110;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#24207;&#21015;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#38468;&#21152;&#23454;&#39564;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommending points of interest (POIs) is a challenging task that requires extracting comprehensive location data from location-based social media platforms. To provide effective location-based recommendations, it's important to analyze users' historical behavior and preferences. In this study, we present a sophisticated location-aware recommendation system that uses Bidirectional Encoder Representations from Transformers (BERT) to offer personalized location-based suggestions. Our model combines location information and user preferences to provide more relevant recommendations compared to models that predict the next POI in a sequence. Our experiments on two benchmark dataset show that our BERT-based model outperforms various state-of-the-art sequential models. Moreover, we see the effectiveness of the proposed model for quality through additional experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25361;&#25112;&#20102;DNNs&#36973;&#21463;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#24120;&#35265;&#20551;&#35774;&#65292;&#25552;&#20986;&#24403;&#21069;&#20248;&#21270;&#25216;&#26415;&#21487;&#20197;&#32531;&#35299;&#36825;&#31181;&#29616;&#35937;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#26631;&#20934;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#26041;&#27861;&#35757;&#32451;&#30340;DNNs&#22312;&#22238;&#24402;&#20219;&#21153;&#19978;&#21487;&#20197;&#31215;&#32047;&#30693;&#35782;&#65292;&#21363;&#20351;&#20219;&#21153;&#37325;&#26032;&#20986;&#29616;&#65292;&#20063;&#19981;&#20250;&#36951;&#24536;&#36807;&#21435;&#30340;&#30693;&#35782;&#65292;&#36825;&#31181;&#30693;&#35782;&#31215;&#32047;&#21487;&#20197;&#25913;&#21892;&#22312;&#26410;&#35265;&#20219;&#21153;&#19978;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2207.04543</link><description>&lt;p&gt;
&#25361;&#25112;&#20851;&#20110;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#24120;&#35265;&#20551;&#35774;
&lt;/p&gt;
&lt;p&gt;
Challenging Common Assumptions about Catastrophic Forgetting. (arXiv:2207.04543v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.04543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25361;&#25112;&#20102;DNNs&#36973;&#21463;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#24120;&#35265;&#20551;&#35774;&#65292;&#25552;&#20986;&#24403;&#21069;&#20248;&#21270;&#25216;&#26415;&#21487;&#20197;&#32531;&#35299;&#36825;&#31181;&#29616;&#35937;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#26631;&#20934;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#26041;&#27861;&#35757;&#32451;&#30340;DNNs&#22312;&#22238;&#24402;&#20219;&#21153;&#19978;&#21487;&#20197;&#31215;&#32047;&#30693;&#35782;&#65292;&#21363;&#20351;&#20219;&#21153;&#37325;&#26032;&#20986;&#29616;&#65292;&#20063;&#19981;&#20250;&#36951;&#24536;&#36807;&#21435;&#30340;&#30693;&#35782;&#65292;&#36825;&#31181;&#30693;&#35782;&#31215;&#32047;&#21487;&#20197;&#25913;&#21892;&#22312;&#26410;&#35265;&#20219;&#21153;&#19978;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#33021;&#22815;&#36880;&#27493;&#23398;&#20064;&#21644;&#31215;&#32047;&#30693;&#35782;&#30340;&#23398;&#20064;&#26234;&#33021;&#20307;&#26159;&#36830;&#32493;&#23398;&#20064;(CL)&#30740;&#31350;&#39046;&#22495;&#30340;&#26680;&#24515;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#23558;&#27169;&#22411;&#35757;&#32451;&#22312;&#26032;&#25968;&#25454;&#19978;&#36890;&#24120;&#20250;&#25439;&#23475;&#36807;&#21435;&#25968;&#25454;&#30340;&#24615;&#33021;&#12290;&#22312;CL&#25991;&#29486;&#20013;&#65292;&#36825;&#31181;&#25928;&#24212;&#34987;&#31216;&#20026;&#28798;&#38590;&#24615;&#36951;&#24536;(CF)&#12290;&#23613;&#31649;CF&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#19988;&#24050;&#25552;&#20986;&#20102;&#22823;&#37327;&#26041;&#27861;&#26469;&#35299;&#20915;&#38750;&#37325;&#21472;&#20219;&#21153;&#30701;&#24207;&#21015;&#30340;CF&#38382;&#39064;&#65292;&#20294;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;CF&#24635;&#26159;&#23548;&#33268;&#36807;&#21435;&#20219;&#21153;&#30340;&#24615;&#33021;&#36805;&#36895;&#32780;&#26174;&#33879;&#22320;&#19979;&#38477;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#22312;CL&#22238;&#24402;&#35774;&#32622;&#19979;&#65292;SGD&#35757;&#32451;&#32447;&#24615;&#27169;&#22411;&#21487;&#20197;&#32047;&#31215;&#30693;&#35782;&#12290;&#24403;&#20219;&#21153;&#37325;&#26032;&#21457;&#29983;&#26102;&#65292;&#36825;&#31181;&#29616;&#35937;&#23588;&#20026;&#26126;&#26174;&#12290;&#25105;&#20204;&#21487;&#33021;&#20250;&#24819;&#30693;&#36947;&#65292;&#26159;&#21542;&#20351;&#29992;SGD&#25110;&#20219;&#20309;&#26631;&#20934;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#26041;&#27861;&#35757;&#32451;&#30340;DNNs&#20250;&#20197;&#36825;&#31181;&#26041;&#24335;&#31215;&#32047;&#30693;&#35782;&#12290;&#36825;&#20123;&#29616;&#35937;&#20250;&#23545;&#23558;DNNs&#24212;&#29992;&#20110;&#30495;&#23454;&#30340;&#36830;&#32493;&#24773;&#22659;&#20135;&#29983;&#26377;&#36259;&#30340;&#24433;&#21709;&#12290;&#23454;&#38469;&#19978;&#65292;&#26631;&#20934;&#30340;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#25216;&#26415;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#24182;&#20026;&#22823;&#35268;&#27169;&#38382;&#39064;&#25552;&#20379;&#39640;&#25928;&#30340;&#35757;&#32451;&#31574;&#30053;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25361;&#25112;&#24120;&#35265;&#30340;DNNs&#36973;&#21463;CF&#30340;&#20551;&#35774;&#65292;&#24182;&#25552;&#20986;&#24403;&#21069;&#30340;&#20248;&#21270;&#25216;&#26415;&#21487;&#20197;&#32531;&#35299;&#36825;&#31181;&#29616;&#35937;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#22238;&#24402;&#20219;&#21153;&#19978;&#20351;&#29992;SGD&#35757;&#32451;&#30340;DNNs&#21487;&#20197;&#31215;&#32047;&#30693;&#35782;&#65292;&#21363;&#20351;&#20219;&#21153;&#37325;&#26032;&#20986;&#29616;&#65292;&#20063;&#19981;&#20250;&#36951;&#24536;&#36807;&#21435;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#30693;&#35782;&#31215;&#32047;&#21487;&#20197;&#25913;&#21892;&#22312;&#26410;&#35265;&#20219;&#21153;&#19978;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building learning agents that can progressively learn and accumulate knowledge is the core goal of the continual learning (CL) research field. Unfortunately, training a model on new data usually compromises the performance on past data. In the CL literature, this effect is referred to as catastrophic forgetting (CF). CF has been largely studied, and a plethora of methods have been proposed to address it on short sequences of non-overlapping tasks. In such setups, CF always leads to a quick and significant drop in performance in past tasks. Nevertheless, despite CF, recent work showed that SGD training on linear models accumulates knowledge in a CL regression setup. This phenomenon becomes especially visible when tasks reoccur. We might then wonder if DNNs trained with SGD or any standard gradient-based optimization accumulate knowledge in such a way. Such phenomena would have interesting consequences for applying DNNs to real continual scenarios. Indeed, standard gradient-based optimiz
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#32536;&#24863;&#30693;&#30340;Weisfeiler-Lehman&#31639;&#27861;&#65292;&#20197;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25345;&#28040;&#24687;&#20256;&#36882;&#26041;&#26696;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;NC-GNN&#26694;&#26550;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.02059</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#36793;&#30340;Weisfeiler-Lehman&#31639;&#27861;&#22686;&#24378;GNN
&lt;/p&gt;
&lt;p&gt;
Empowering GNNs via Edge-Aware Weisfeiler-Lehman Algorithm. (arXiv:2206.02059v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#32536;&#24863;&#30693;&#30340;Weisfeiler-Lehman&#31639;&#27861;&#65292;&#20197;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25345;&#28040;&#24687;&#20256;&#36882;&#26041;&#26696;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;NC-GNN&#26694;&#26550;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#30340;&#34920;&#36798;&#33021;&#21147;&#34987;&#24050;&#30693;&#30340;&#19968;&#32500;Weisfeiler-Lehman (1-WL)&#31639;&#27861;&#19978;&#30028;&#25152;&#38480;&#21046;&#12290;&#20026;&#20102;&#23454;&#29616;&#26356;&#24378;&#22823;&#30340;GNN&#65292;&#29616;&#26377;&#30340;&#23581;&#35797;&#35201;&#20040;&#38656;&#35201;&#29305;&#23450;&#30340;&#29305;&#24449;&#65292;&#35201;&#20040;&#28041;&#21450;&#39640;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#30340;&#25805;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#19988;&#21487;&#35777;&#26126;&#20855;&#26377;&#24378;&#22823;&#34920;&#36798;&#21147;&#30340;GNN&#26694;&#26550;&#65292;&#20445;&#25345;&#20102;&#28040;&#24687;&#20256;&#36882;&#26041;&#26696;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#32771;&#34385;&#37051;&#23621;&#20043;&#38388;&#30340;&#36793;&#32536;&#26469;&#25480;&#26435;1-WL&#36827;&#34892;&#22270;&#21516;&#26500;&#27979;&#35797;&#65292;&#20174;&#32780;&#20135;&#29983;NC-1-WL&#12290; NC-1-WL&#30340;&#34920;&#36798;&#33021;&#21147;&#22312;&#29702;&#35770;&#19978;&#34987;&#26174;&#31034;&#20026;&#20005;&#26684;&#39640;&#20110;1-WL&#19988;&#20302;&#20110;3-WL&#12290;&#36827;&#19968;&#27493;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NC-GNN&#26694;&#26550;&#20316;&#20026;NC-1-WL&#30340;&#21487;&#21306;&#20998;&#31070;&#32463;&#29256;&#26412;&#12290;&#25105;&#20204;&#30340;&#31616;&#21333;NC-GNN&#23454;&#29616;&#21487;&#35777;&#26126;&#19982;NC-1-WL&#19968;&#26679;&#24378;&#22823;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;NC-GNN&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Message passing graph neural networks (GNNs) are known to have their expressiveness upper-bounded by 1-dimensional Weisfeiler-Lehman (1-WL) algorithm. To achieve more powerful GNNs, existing attempts either require ad hoc features, or involve operations that incur high time and space complexities. In this work, we propose a general and provably powerful GNN framework that preserves the scalability of the message passing scheme. In particular, we first propose to empower 1-WL for graph isomorphism test by considering edges among neighbors, giving rise to NC-1-WL. The expressiveness of NC-1-WL is shown to be strictly above 1-WL and below 3-WL theoretically. Further, we propose the NC-GNN framework as a differentiable neural version of NC-1-WL. Our simple implementation of NC-GNN is provably as powerful as NC-1-WL. Experiments demonstrate that our NC-GNN performs effectively and efficiently on various benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;ViT-Patch GAN&#26550;&#26500;&#65292;&#20854;&#21253;&#25324;&#19968;&#20010;&#33258;&#23398;&#20064;&#30340;&#36816;&#21160;&#21464;&#25442;&#29983;&#25104;&#22120;&#21644;&#22522;&#20110;Vision Transformer&#30340;patch&#65288;ViT-Patch&#65289;&#37492;&#21035;&#22120;&#12290;&#35813;&#29983;&#25104;&#22120;&#23398;&#20064;&#26579;&#33394;&#20307;&#30340;&#36816;&#21160;&#34920;&#31034;&#65292;&#20197;&#23454;&#29616;&#40065;&#26834;&#24615;&#26579;&#33394;&#20307;&#20462;&#30452;&#65292;&#24182;&#19988;&#22312;&#20445;&#30041;&#24418;&#29366;&#21644;&#26001;&#24102;&#22270;&#26696;&#32454;&#33410;&#26041;&#38754;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2203.02901</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;ViT-Patch GAN&#30340;&#26579;&#33394;&#20307;&#20462;&#30452;&#40065;&#26834;&#24615;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Robust Framework of Chromosome Straightening with ViT-Patch GAN. (arXiv:2203.02901v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.02901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;ViT-Patch GAN&#26550;&#26500;&#65292;&#20854;&#21253;&#25324;&#19968;&#20010;&#33258;&#23398;&#20064;&#30340;&#36816;&#21160;&#21464;&#25442;&#29983;&#25104;&#22120;&#21644;&#22522;&#20110;Vision Transformer&#30340;patch&#65288;ViT-Patch&#65289;&#37492;&#21035;&#22120;&#12290;&#35813;&#29983;&#25104;&#22120;&#23398;&#20064;&#26579;&#33394;&#20307;&#30340;&#36816;&#21160;&#34920;&#31034;&#65292;&#20197;&#23454;&#29616;&#40065;&#26834;&#24615;&#26579;&#33394;&#20307;&#20462;&#30452;&#65292;&#24182;&#19988;&#22312;&#20445;&#30041;&#24418;&#29366;&#21644;&#26001;&#24102;&#22270;&#26696;&#32454;&#33410;&#26041;&#38754;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26579;&#33394;&#20307;&#25215;&#36733;&#30528;&#20154;&#31867;&#30340;&#36951;&#20256;&#20449;&#24687;&#12290;&#23427;&#20204;&#20855;&#26377;&#19981;&#35268;&#21017;&#30340;&#38750;&#21018;&#24615;&#21644;&#38750;&#20851;&#33410;&#24615;&#65292;&#24182;&#19988;&#23384;&#22312;&#19981;&#21516;&#31243;&#24230;&#30340;&#24367;&#26354;&#12290;&#26579;&#33394;&#20307;&#20462;&#30452;&#26159;&#21518;&#32493;&#26680;&#22411;&#26500;&#24314;&#12289;&#30149;&#29702;&#35786;&#26029;&#21644;&#32454;&#32990;&#36951;&#20256;&#22270;&#35889;&#21457;&#23637;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27809;&#26377;&#35757;&#32451;&#22270;&#20687;&#12289;&#20462;&#30452;&#21518;&#30072;&#21464;&#30340;&#26579;&#33394;&#20307;&#32454;&#33410;&#21644;&#24418;&#29366;&#20197;&#21450;&#27867;&#21270;&#33021;&#21147;&#24046;&#65292;&#40065;&#26834;&#24615;&#26579;&#33394;&#20307;&#20462;&#30452;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;ViT-Patch GAN&#65292;&#21253;&#25324;&#19968;&#20010;&#33258;&#23398;&#20064;&#30340;&#36816;&#21160;&#21464;&#25442;&#29983;&#25104;&#22120;&#21644;&#19968;&#20010;&#22522;&#20110;Vision Transformer&#30340;patch&#65288;ViT-Patch&#65289;&#37492;&#21035;&#22120;&#12290;&#29983;&#25104;&#22120;&#23398;&#20064;&#26579;&#33394;&#20307;&#30340;&#36816;&#21160;&#34920;&#31034;&#20197;&#36827;&#34892;&#20462;&#30452;&#12290;&#22312;ViT-Patch&#37492;&#21035;&#22120;&#30340;&#24110;&#21161;&#19979;&#65292;&#20462;&#30452;&#30340;&#26579;&#33394;&#20307;&#20445;&#30041;&#26356;&#22810;&#30340;&#24418;&#29366;&#21644;&#26001;&#24102;&#22270;&#26696;&#32454;&#33410;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;Fr\'echet Inception&#36317;&#31163;&#65288;FI
&lt;/p&gt;
&lt;p&gt;
Chromosomes carry the genetic information of humans. They exhibit non-rigid and non-articulated nature with varying degrees of curvature. Chromosome straightening is an important step for subsequent karyotype construction, pathological diagnosis and cytogenetic map development. However, robust chromosome straightening remains challenging, due to the unavailability of training images, distorted chromosome details and shapes after straightening, as well as poor generalization capability. In this paper, we propose a novel architecture, ViT-Patch GAN, consisting of a self-learned motion transformation generator and a Vision Transformer-based patch (ViT-Patch) discriminator. The generator learns the motion representation of chromosomes for straightening. With the help of the ViT-Patch discriminator, the straightened chromosomes retain more shape and banding pattern details. The experimental results show that the proposed method achieves better performance on Fr\'echet Inception Distance (FI
&lt;/p&gt;</description></item><item><title>S-ConvNet&#21644;All-ConvNet&#27169;&#22411;&#20026;&#31070;&#32463;&#32908;&#32905;&#27963;&#21160;&#35782;&#21035;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#19981;&#38656;&#35201;&#39044;&#35757;&#32451;&#65292;&#34920;&#29616;&#20986;&#38750;&#24120;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#35782;&#21035;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/1906.03381</link><description>&lt;p&gt;
S-ConvNet: &#19968;&#31181;&#28145;&#24230;&#34920;&#38754;&#32908;&#30005;&#22270;&#20687;&#31070;&#32463;&#32908;&#32905;&#27963;&#21160;&#35782;&#21035;&#30340;&#27973;&#23618;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
S-ConvNet: A Shallow Convolutional Neural Network Architecture for Neuromuscular Activity Recognition Using Instantaneous High-Density Surface EMG Images. (arXiv:1906.03381v1 [eess.SP] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1906.03381
&lt;/p&gt;
&lt;p&gt;
S-ConvNet&#21644;All-ConvNet&#27169;&#22411;&#20026;&#31070;&#32463;&#32908;&#32905;&#27963;&#21160;&#35782;&#21035;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#19981;&#38656;&#35201;&#39044;&#35757;&#32451;&#65292;&#34920;&#29616;&#20986;&#38750;&#24120;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#35782;&#21035;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#30636;&#26102;&#39640;&#23494;&#24230;&#34920;&#38754;&#32908;&#30005;&#22270;&#20687;&#36827;&#34892;&#31070;&#32463;&#32908;&#32905;&#27963;&#21160;&#35782;&#21035;&#30340;&#27010;&#24565;&#20026;&#24320;&#21457;&#26356;&#27969;&#30021;&#21644;&#33258;&#28982;&#30340;&#32908;&#32905;-&#35745;&#31639;&#26426;&#25509;&#21475;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#38750;&#24120;&#22823;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(ConvNet)&#26550;&#26500;&#21644;&#22797;&#26434;&#30340;&#35757;&#32451;&#26041;&#26696;&#26469;&#36827;&#34892;HD-sEMG&#22270;&#20687;&#35782;&#21035;&#65292;&#38656;&#35201;&#22312;&#38750;&#24120;&#22823;&#35268;&#27169;&#30340;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#32593;&#32476;&#26550;&#26500;&#65292;&#22240;&#27492;&#35745;&#31639;&#38750;&#24120;&#26114;&#36149;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#23398;&#20064;&#21363;&#26102;HD-sEMG&#22270;&#20687;&#30340;&#31070;&#32463;&#32908;&#32905;&#27963;&#21160;&#35782;&#21035;&#26694;&#26550;&#8212;&#8212;S-ConvNet&#21644;All-ConvNet&#27169;&#22411;&#12290;&#22312;&#19981;&#20351;&#29992;&#20219;&#20309;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;S-ConvNet&#21644;All-ConvNet&#34920;&#29616;&#20986;&#20102;&#38750;&#24120;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#35782;&#21035;&#20934;&#30830;&#24615;&#65292;&#27604;&#22522;&#20110;&#30636;&#26102;HD-sEMG&#22270;&#20687;&#30340;&#31070;&#32463;&#32908;&#32905;&#27963;&#21160;&#35782;&#21035;&#30340;&#26356;&#22797;&#26434;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#26356;&#20855;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The concept of neuromuscular activity recognition using instantaneous high-density surface electromyography (HD-sEMG) images opens up new avenues for the development of more fluid and natural muscle-computer interfaces. However, the existing approaches employed a very large deep convolutional neural network (ConvNet) architecture and complex training schemes for HD-sEMG image recognition, which requires the network architecture to be pre-trained on a very large-scale labeled training dataset, as a result, it makes computationally very expensive. To overcome this problem, we propose S-ConvNet and All-ConvNet models, a simple yet efficient framework for learning instantaneous HD-sEMG images from scratch for neuromuscular activity recognition. Without using any pre-trained models, our proposed S-ConvNet and All-ConvNet demonstrate very competitive recognition accuracy to the more complex state of the art for neuromuscular activity recognition based on instantaneous HD-sEMG images, while u
&lt;/p&gt;</description></item></channel></rss>