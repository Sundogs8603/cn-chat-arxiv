<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>ChatQA&#26159;&#19968;&#31995;&#21015;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#65292;&#21487;&#20197;&#36798;&#21040;GPT-4&#32423;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#20004;&#38454;&#27573;&#30340;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#32467;&#26524;&#12290;&#20351;&#29992;&#23494;&#38598;&#26816;&#32034;&#22120;&#36827;&#34892;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#24494;&#35843;&#21487;&#20197;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#26597;&#35810;&#37325;&#20889;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#38477;&#20302;&#37096;&#32626;&#25104;&#26412;&#12290;ChatQA-70B&#22312;10&#20010;&#23545;&#35805;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#24179;&#22343;&#24471;&#20998;&#36229;&#36807;&#20102;GPT-4&#65292;&#19988;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#26469;&#33258;OpenAI GPT&#27169;&#22411;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2401.10225</link><description>&lt;p&gt;
ChatQA: &#26500;&#24314;GPT-4&#32423;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ChatQA: Building GPT-4 Level Conversational QA Models. (arXiv:2401.10225v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10225
&lt;/p&gt;
&lt;p&gt;
ChatQA&#26159;&#19968;&#31995;&#21015;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#65292;&#21487;&#20197;&#36798;&#21040;GPT-4&#32423;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#20004;&#38454;&#27573;&#30340;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#32467;&#26524;&#12290;&#20351;&#29992;&#23494;&#38598;&#26816;&#32034;&#22120;&#36827;&#34892;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#24494;&#35843;&#21487;&#20197;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#26597;&#35810;&#37325;&#20889;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#38477;&#20302;&#37096;&#32626;&#25104;&#26412;&#12290;ChatQA-70B&#22312;10&#20010;&#23545;&#35805;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#24179;&#22343;&#24471;&#20998;&#36229;&#36807;&#20102;GPT-4&#65292;&#19988;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#26469;&#33258;OpenAI GPT&#27169;&#22411;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ChatQA&#65292;&#19968;&#31995;&#21015;&#20855;&#26377;GPT-4&#32423;&#21035;&#20934;&#30830;&#24615;&#30340;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#38646;-shot&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#22788;&#29702;&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#26816;&#32034;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#22810;&#36718;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23494;&#38598;&#26816;&#32034;&#22120;&#30340;&#24494;&#35843;&#65292;&#36825;&#26679;&#21487;&#20197;&#25552;&#20379;&#19982;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#26597;&#35810;&#37325;&#20889;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#22823;&#22823;&#38477;&#20302;&#37096;&#32626;&#25104;&#26412;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;ChatQA-70B&#21487;&#20197;&#22312;10&#20010;&#23545;&#35805;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#24179;&#22343;&#20998;&#19978;&#36229;&#36807;GPT-4&#65288;54.14 vs. 53.90&#65289;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;OpenAI GPT&#27169;&#22411;&#30340;&#20219;&#20309;&#21512;&#25104;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce ChatQA, a family of conversational question answering (QA) models, that obtain GPT-4 level accuracies. Specifically, we propose a two-stage instruction tuning method that can significantly improve the zero-shot conversational QA results from large language models (LLMs). To handle retrieval in conversational QA, we fine-tune a dense retriever on a multi-turn QA dataset, which provides comparable results to using the state-of-the-art query rewriting model while largely reducing deployment cost. Notably, our ChatQA-70B can outperform GPT-4 in terms of average score on 10 conversational QA datasets (54.14 vs. 53.90), without relying on any synthetic data from OpenAI GPT models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21483;&#20570;ViSFT&#65288;Vision SFT&#65289;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39046;&#22495;&#20869;&#20219;&#21153;&#19978;&#36827;&#34892;&#35270;&#35273;&#32852;&#21512;&#23398;&#20064;&#26469;&#25552;&#21319;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#39046;&#22495;&#22806;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.10222</link><description>&lt;p&gt;
&#30417;&#30563;&#24494;&#35843;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Supervised Fine-tuning in turn Improves Visual Foundation Models. (arXiv:2401.10222v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21483;&#20570;ViSFT&#65288;Vision SFT&#65289;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39046;&#22495;&#20869;&#20219;&#21153;&#19978;&#36827;&#34892;&#35270;&#35273;&#32852;&#21512;&#23398;&#20064;&#26469;&#25552;&#21319;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#39046;&#22495;&#22806;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20687;CLIP&#36825;&#26679;&#30340;&#22270;&#20687;-&#25991;&#26412;&#35757;&#32451;&#26041;&#27861;&#24050;&#32463;&#20027;&#23548;&#20102;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#12290;&#38543;&#21518;&#65292;&#20026;&#20102;&#23558;&#21306;&#22495;&#32423;&#21035;&#30340;&#35270;&#35273;&#23398;&#20064;&#24341;&#20837;CLIP&#30340;&#39044;&#35757;&#32451;&#20013;&#65292;&#22312;&#22823;&#35268;&#27169;&#21306;&#22495;&#32423;&#21035;&#25968;&#25454;&#38598;&#30340;&#32570;&#20047;&#19979;&#38754;&#20020;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#12290;&#21463;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#30340;&#21551;&#21457;&#65292;&#27604;&#22914;&#25351;&#20196;&#24494;&#35843;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22312;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#20043;&#21518;&#65292;&#24494;&#31890;SFT&#33021;&#22815;&#25552;&#21319;&#20854;&#29983;&#25104;&#33021;&#21147;&#30340;&#28508;&#21147;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#26041;&#27861;ViSFT&#65288;Vision SFT&#65289;&#65292;&#26469;&#37322;&#25918;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#30693;&#35782;&#12290;&#22312;ViSFT&#20013;&#65292;&#36890;&#36807;&#22312;&#19968;&#20123;&#39046;&#22495;&#20869;&#20219;&#21153;&#19978;&#36827;&#34892;&#35270;&#35273;&#32852;&#21512;&#23398;&#20064;&#26469;&#22686;&#24378;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#65292;&#28982;&#21518;&#22312;&#39046;&#22495;&#22806;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#27979;&#35797;&#12290;&#36890;&#36807;&#20351;&#29992;ViSFT&#22312;&#23569;&#20110;2&#22825;&#20869;&#22312;8&#20010;V100 GPU&#19978;&#36827;&#34892;&#26356;&#26032;&#65292;&#19968;&#20010;&#20855;&#26377;&#36229;&#36807;4.4B&#21442;&#25968;&#30340;&#35270;&#35273;Transformer&#22312;&#21508;&#31181;&#39046;&#22495;&#22806;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#31034;&#20986;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image-text training like CLIP has dominated the pretraining of vision foundation models in recent years. Subsequent efforts have been made to introduce region-level visual learning into CLIP's pretraining but face scalability challenges due to the lack of large-scale region-level datasets. Drawing inspiration from supervised fine-tuning (SFT) in natural language processing such as instruction tuning, we explore the potential of fine-grained SFT in enhancing the generation of vision foundation models after their pretraining. Thus a two-stage method ViSFT (Vision SFT) is proposed to unleash the fine-grained knowledge of vision foundation models. In ViSFT, the vision foundation model is enhanced by performing visual joint learning on some in-domain tasks and then tested on out-of-domain benchmarks. With updating using ViSFT on 8 V100 GPUs in less than 2 days, a vision transformer with over 4.4B parameters shows improvements across various out-of-domain benchmarks including vision and visi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#31890;&#24230;&#32467;&#26500;&#21644;&#22810;&#23610;&#24230;&#24207;&#21015;&#34920;&#31034;&#32806;&#21512;&#30340;PTM&#20301;&#28857;&#39044;&#27979;&#26041;&#27861;PTM-CMGMS&#65292;&#35813;&#26041;&#27861;&#22312;&#32467;&#26500;&#34920;&#31034;&#23398;&#20064;&#21644;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#19978;&#36827;&#34892;&#20248;&#21270;&#65292;&#25552;&#39640;&#20102;PTM&#20301;&#28857;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10211</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#31890;&#24230;&#32467;&#26500;&#21644;&#22810;&#23610;&#24230;&#24207;&#21015;&#34920;&#31034;&#30340;&#32806;&#21512;&#25913;&#36827;PTM&#20301;&#28857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Improving PTM Site Prediction by Coupling of Multi-Granularity Structure and Multi-Scale Sequence Representation. (arXiv:2401.10211v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10211
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#31890;&#24230;&#32467;&#26500;&#21644;&#22810;&#23610;&#24230;&#24207;&#21015;&#34920;&#31034;&#32806;&#21512;&#30340;PTM&#20301;&#28857;&#39044;&#27979;&#26041;&#27861;PTM-CMGMS&#65292;&#35813;&#26041;&#27861;&#22312;&#32467;&#26500;&#34920;&#31034;&#23398;&#20064;&#21644;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#19978;&#36827;&#34892;&#20248;&#21270;&#65292;&#25552;&#39640;&#20102;PTM&#20301;&#28857;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#32763;&#35793;&#21518;&#20462;&#39280;&#65288;PTM&#65289;&#20301;&#28857;&#39044;&#27979;&#26159;&#29983;&#29289;&#20449;&#24687;&#23398;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#20960;&#31181;&#35745;&#31639;&#26041;&#27861;&#26469;&#39044;&#27979;PTM&#20301;&#28857;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24573;&#30053;&#20102;&#32467;&#26500;&#20449;&#24687;&#65292;&#20165;&#21033;&#29992;&#34507;&#30333;&#36136;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;PTM&#26159;&#21457;&#29983;&#22312;&#21407;&#23376;&#31890;&#24230;&#30340;&#29983;&#29289;&#20107;&#20214;&#65292;&#25152;&#20197;&#36843;&#20999;&#38656;&#35201;&#35774;&#35745;&#19968;&#31181;&#26356;&#31934;&#32454;&#30340;&#32467;&#26500;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#31890;&#24230;&#32467;&#26500;&#21644;&#22810;&#23610;&#24230;&#24207;&#21015;&#34920;&#31034;&#32806;&#21512;&#30340;PTM&#20301;&#28857;&#39044;&#27979;&#26041;&#27861;&#65292;&#31616;&#31216;PTM-CMGMS&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22810;&#31890;&#24230;&#32467;&#26500;&#24863;&#30693;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#20174;AlphaFold&#39044;&#27979;&#30340;&#32467;&#26500;&#20013;&#23398;&#20064;&#27688;&#22522;&#37240;&#12289;&#21407;&#23376;&#21644;&#25972;&#20010;&#34507;&#30333;&#36136;&#30340;&#37051;&#22495;&#32467;&#26500;&#34920;&#31034;&#65292;&#28982;&#21518;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#20248;&#21270;&#32467;&#26500;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#20102;&#22810;&#23610;&#24230;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#26469;&#25552;&#21462;&#19978;&#19979;&#25991;&#24207;&#21015;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Protein post-translational modification (PTM) site prediction is a fundamental task in bioinformatics. Several computational methods have been developed to predict PTM sites. However, existing methods ignore the structure information and merely utilize protein sequences. Furthermore, designing a more fine-grained structure representation learning method is urgently needed as PTM is a biological event that occurs at the atom granularity. In this paper, we propose a PTM site prediction method by Coupling of Multi-Granularity structure and Multi-Scale sequence representation, PTM-CMGMS for brevity. Specifically, multigranularity structure-aware representation learning is designed to learn neighborhood structure representations at the amino acid, atom, and whole protein granularity from AlphaFold predicted structures, followed by utilizing contrastive learning to optimize the structure representations.Additionally, multi-scale sequence representation learning is used to extract context seq
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31934;&#36890;&#25351;&#23548;&#30340;&#38750;&#21442;&#25968;&#32858;&#31867;&#26041;&#27861;&#65292;&#39044;&#27979;&#23398;&#29983;&#22312;&#38382;&#39064;&#35299;&#20915;&#20013;&#21487;&#33021;&#37319;&#29992;&#30340;&#31574;&#30053;&#65292;&#20174;&#32780;&#23454;&#29616;&#33258;&#36866;&#24212;&#25945;&#23398;&#31995;&#32479;&#30340;&#20010;&#24615;&#21270;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2401.10210</link><description>&lt;p&gt;
&#36890;&#36807;&#31934;&#36890;&#25351;&#23548;&#30340;&#38750;&#21442;&#25968;&#32858;&#31867;&#26469;&#25193;&#22823;&#31574;&#30053;&#39044;&#27979;&#35268;&#27169;
&lt;/p&gt;
&lt;p&gt;
Mastery Guided Non-parametric Clustering to Scale-up Strategy Prediction. (arXiv:2401.10210v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10210
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31934;&#36890;&#25351;&#23548;&#30340;&#38750;&#21442;&#25968;&#32858;&#31867;&#26041;&#27861;&#65292;&#39044;&#27979;&#23398;&#29983;&#22312;&#38382;&#39064;&#35299;&#20915;&#20013;&#21487;&#33021;&#37319;&#29992;&#30340;&#31574;&#30053;&#65292;&#20174;&#32780;&#23454;&#29616;&#33258;&#36866;&#24212;&#25945;&#23398;&#31995;&#32479;&#30340;&#20010;&#24615;&#21270;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#23398;&#29983;&#22312;&#35299;&#20915;&#38382;&#39064;&#26102;&#21487;&#33021;&#20351;&#29992;&#30340;&#31574;&#30053;&#65288;&#27010;&#24565;&#24207;&#21015;&#65289;&#26377;&#21161;&#20110;&#33258;&#36866;&#24212;&#25945;&#23398;&#31995;&#32479;&#65288;AISs&#65289;&#26681;&#25454;&#20182;&#20204;&#30340;&#23398;&#20064;&#33021;&#21147;&#26356;&#22909;&#22320;&#36866;&#24212;&#19981;&#21516;&#31867;&#22411;&#30340;&#23398;&#20064;&#32773;&#12290;&#36825;&#21487;&#20197;&#20026;&#23398;&#29983;&#25552;&#20379;&#26356;&#21160;&#24577;&#12289;&#26377;&#36259;&#21644;&#20010;&#24615;&#21270;&#30340;&#23398;&#20064;&#20307;&#39564;&#12290;&#20026;&#20102;&#25193;&#22823;&#35757;&#32451;&#19968;&#20010;&#21487;&#20197;&#35206;&#30422;&#22823;&#35268;&#27169;&#25945;&#32946;&#25968;&#25454;&#38598;&#30340;&#39044;&#27979;&#27169;&#22411;&#65288;&#22914;LSTMs&#65289;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#38750;&#21442;&#25968;&#26041;&#27861;&#26469;&#23545;&#25968;&#25454;&#20013;&#30340;&#23545;&#31216;&#23454;&#20363;&#36827;&#34892;&#32858;&#31867;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#22522;&#20110;Node2Vec&#30340;&#34920;&#31034;&#23398;&#20064;&#23558;&#25484;&#25569;&#25110;&#25216;&#33021;&#27700;&#24179;&#19978;&#30340;&#23545;&#31216;&#24615;&#32534;&#30721;&#20026;&#31574;&#30053;&#65292;&#22240;&#20026;&#35299;&#20915;&#38382;&#39064;&#26102;&#65292;&#23398;&#29983;&#30340;&#31574;&#30053;&#24456;&#21487;&#33021;&#28041;&#21450;&#20182;&#20204;&#24050;&#32463;&#25484;&#25569;&#30340;&#27010;&#24565;&#12290;&#21033;&#29992;&#36825;&#31181;&#34920;&#31034;&#65292;&#25105;&#20204;&#20351;&#29992;DP-Means&#36890;&#36807;&#23545;&#32858;&#31867;&#30340;&#31895;&#32454;&#35843;&#25972;&#26469;&#23545;&#23545;&#31216;&#23454;&#20363;&#36827;&#34892;&#20998;&#32452;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#24212;&#29992;&#21040;&#20174;MATHia&#65288;&#19968;&#23478;&#20013;&#23398;&#25968;&#23398;&#23398;&#20064;&#39046;&#20808;&#30340;AIS&#65289;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#25968;&#23398;&#23398;&#20064;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting the strategy (sequence of concepts) that a student is likely to use in problem-solving helps Adaptive Instructional Systems (AISs) better adapt themselves to different types of learners based on their learning abilities. This can lead to a more dynamic, engaging, and personalized experience for students. To scale up training a prediction model (such as LSTMs) over large-scale education datasets, we develop a non-parametric approach to cluster symmetric instances in the data. Specifically, we learn a representation based on Node2Vec that encodes symmetries over mastery or skill level since, to solve a problem, it is natural that a student's strategy is likely to involve concepts in which they have gained mastery. Using this representation, we use DP-Means to group symmetric instances through a coarse-to-fine refinement of the clusters. We apply our model to learn strategies for Math learning from large-scale datasets from MATHia, a leading AIS for middle-school math learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#35299;&#37322;&#24615;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#30340;&#25240;&#34935;&#35268;&#21017;&#25552;&#21462;&#65292;&#26088;&#22312;&#35299;&#20915;&#40657;&#30418;&#35299;&#37322;&#22120;&#30340;&#19981;&#21487;&#20449;&#20219;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.10207</link><description>&lt;p&gt;
&#29992;&#20110;&#35299;&#37322;&#24615;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#30340;&#25240;&#34935;&#35268;&#21017;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Eclectic Rule Extraction for Explainability of Deep Neural Network based Intrusion Detection Systems. (arXiv:2401.10207v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#35299;&#37322;&#24615;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#30340;&#25240;&#34935;&#35268;&#21017;&#25552;&#21462;&#65292;&#26088;&#22312;&#35299;&#20915;&#40657;&#30418;&#35299;&#37322;&#22120;&#30340;&#19981;&#21487;&#20449;&#20219;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#40657;&#30418;&#31639;&#27861;&#21644;&#20195;&#29702;&#35299;&#37322;&#22120;&#22312;&#21487;&#35299;&#37322;&#24615;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;(X-IDS)&#20013;&#25152;&#24341;&#21457;&#30340;&#20449;&#20219;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#12290;&#34429;&#28982;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;(XAI)&#26088;&#22312;&#25552;&#39640;&#36879;&#26126;&#24230;&#65292;&#20294;&#40657;&#30418;&#20195;&#29702;&#35299;&#37322;&#22120;&#65292;&#22914;&#23616;&#37096;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#26080;&#20851;&#35299;&#37322;(LIME)&#21644;SHapley&#21152;&#27861;&#35299;&#37322;(SHAP)&#65292;&#24456;&#38590;&#20449;&#20219;&#12290;&#36825;&#20123;&#20195;&#29702;&#35299;&#37322;&#22120;&#30340;&#40657;&#30418;&#29305;&#24615;&#20351;&#24471;&#35299;&#37322;&#29983;&#25104;&#30340;&#36807;&#31243;&#19981;&#36879;&#26126;&#19988;&#38590;&#20197;&#29702;&#35299;&#12290;&#20026;&#20102;&#36991;&#20813;&#36825;&#20010;&#38382;&#39064;&#65292;&#21487;&#20197;&#20351;&#29992;&#36879;&#26126;&#30340;&#30333;&#30418;&#31639;&#27861;&#65292;&#22914;&#35268;&#21017;&#25552;&#21462;(RE)&#12290;&#35268;&#21017;&#25552;&#21462;&#26377;&#19977;&#31181;&#31867;&#22411;&#30340;&#31639;&#27861;:&#25945;&#32946;&#12289;&#20998;&#35299;&#21644;&#25240;&#34935;&#12290;&#25945;&#32946;&#26041;&#27861;&#25552;&#20379;&#24555;&#36895;&#20294;&#19981;&#21487;&#20449;&#36182;&#30340;&#30333;&#30418;&#35299;&#37322;&#65292;&#32780;&#20998;&#35299;&#35268;&#21017;&#25552;&#21462;&#25552;&#20379;&#20102;&#21487;&#20449;&#36182;&#20294;&#21487;&#25193;&#23637;&#24615;&#36739;&#24046;&#30340;&#35299;&#37322;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#25240;&#34935;&#35268;&#21017;&#25552;&#21462;&#65292;&#23427;&#22312;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#20449;&#36182;&#24615;&#20043;&#38388;&#36798;&#21040;&#20102;&#24179;&#34913;&#12290;&#36890;&#36807;&#32508;&#21512;&#19981;&#21516;&#30340;&#25216;&#26415;&#26041;&#27861;&#65292;
&lt;/p&gt;
&lt;p&gt;
This paper addresses trust issues created from the ubiquity of black box algorithms and surrogate explainers in Explainable Intrusion Detection Systems (X-IDS). While Explainable Artificial Intelligence (XAI) aims to enhance transparency, black box surrogate explainers, such as Local Interpretable Model-Agnostic Explanation (LIME) and SHapley Additive exPlanation (SHAP), are difficult to trust. The black box nature of these surrogate explainers makes the process behind explanation generation opaque and difficult to understand. To avoid this problem, one can use transparent white box algorithms such as Rule Extraction (RE). There are three types of RE algorithms: pedagogical, decompositional, and eclectic. Pedagogical methods offer fast but untrustworthy white-box explanations, while decompositional RE provides trustworthy explanations with poor scalability. This work explores eclectic rule extraction, which strikes a balance between scalability and trustworthiness. By combining techniq
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Chem-FINESE&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#21270;&#23398;&#39046;&#22495;&#20013;&#32454;&#31890;&#24230;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#23454;&#20307;&#25552;&#21462;&#22120;&#21644;&#33258;&#25105;&#39564;&#35777;&#27169;&#22359;&#26469;&#20174;&#36755;&#20837;&#21477;&#23376;&#20013;&#25552;&#21462;&#21629;&#21517;&#23454;&#20307;&#24182;&#37325;&#26500;&#21407;&#22987;&#36755;&#20837;&#21477;&#23376;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10189</link><description>&lt;p&gt;
Chem-FINESE: &#36890;&#36807;&#25991;&#26412;&#37325;&#26500;&#39564;&#35777;&#32454;&#31890;&#24230;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Chem-FINESE: Validating Fine-Grained Few-shot Entity Extraction through Text Reconstruction. (arXiv:2401.10189v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10189
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Chem-FINESE&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#21270;&#23398;&#39046;&#22495;&#20013;&#32454;&#31890;&#24230;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#23454;&#20307;&#25552;&#21462;&#22120;&#21644;&#33258;&#25105;&#39564;&#35777;&#27169;&#22359;&#26469;&#20174;&#36755;&#20837;&#21477;&#23376;&#20013;&#25552;&#21462;&#21629;&#21517;&#23454;&#20307;&#24182;&#37325;&#26500;&#21407;&#22987;&#36755;&#20837;&#21477;&#23376;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21270;&#23398;&#39046;&#22495;&#20013;&#65292;&#32454;&#31890;&#24230;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;&#38754;&#20020;&#20004;&#20010;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#19982;&#19968;&#33324;&#39046;&#22495;&#30340;&#23454;&#20307;&#25552;&#21462;&#20219;&#21153;&#30456;&#27604;&#65292;&#21270;&#23398;&#35770;&#25991;&#20013;&#30340;&#21477;&#23376;&#36890;&#24120;&#21253;&#21547;&#26356;&#22810;&#30340;&#23454;&#20307;&#12290;&#27492;&#22806;&#65292;&#23454;&#20307;&#25552;&#21462;&#27169;&#22411;&#36890;&#24120;&#38590;&#20197;&#25552;&#21462;&#38271;&#23614;&#31867;&#22411;&#30340;&#23454;&#20307;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;&#26041;&#27861;Chem-FINESE&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;Chem-FINESE&#21253;&#21547;&#20004;&#20010;&#32452;&#20214;&#65306;&#19968;&#20010;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#23454;&#20307;&#25552;&#21462;&#22120;&#29992;&#20110;&#20174;&#36755;&#20837;&#21477;&#23376;&#20013;&#25552;&#21462;&#21629;&#21517;&#23454;&#20307;&#65292;&#20197;&#21450;&#19968;&#20010;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#33258;&#25105;&#39564;&#35777;&#27169;&#22359;&#29992;&#20110;&#20174;&#25552;&#21462;&#30340;&#23454;&#20307;&#20013;&#37325;&#26500;&#21407;&#22987;&#36755;&#20837;&#21477;&#23376;&#12290;&#21463;&#21040;&#19968;&#20010;&#22909;&#30340;&#23454;&#20307;&#25552;&#21462;&#31995;&#32479;&#38656;&#35201;&#24544;&#23454;&#25552;&#21462;&#23454;&#20307;&#30340;&#20107;&#23454;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26032;&#33258;&#25105;&#39564;&#35777;&#27169;&#22359;&#21033;&#29992;&#23454;&#20307;&#25552;&#21462;&#32467;&#26524;&#26469;&#37325;&#26500;&#21407;&#22987;&#36755;&#20837;&#21477;&#23376;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#25439;&#22833;&#26469;&#20943;&#23569;&#22312;&#25552;&#21462;&#36807;&#31243;&#20013;&#30340;&#36807;&#24230;&#22797;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-grained few-shot entity extraction in the chemical domain faces two unique challenges. First, compared with entity extraction tasks in the general domain, sentences from chemical papers usually contain more entities. Moreover, entity extraction models usually have difficulty extracting entities of long-tailed types. In this paper, we propose Chem-FINESE, a novel sequence-to-sequence (seq2seq) based few-shot entity extraction approach, to address these two challenges. Our Chem-FINESE has two components: a seq2seq entity extractor to extract named entities from the input sentence and a seq2seq self-validation module to reconstruct the original input sentence from extracted entities. Inspired by the fact that a good entity extraction system needs to extract entities faithfully, our new self-validation module leverages entity extraction results to reconstruct the original input sentence. Besides, we design a new contrastive loss to reduce excessive copying during the extraction proces
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#21367;&#31215;&#28388;&#27874;&#22120;&#25104;&#21151;&#22797;&#21046;&#20102;&#29983;&#29289;&#24863;&#21463;&#37326;&#30340;&#32467;&#26500;&#22797;&#26434;&#24615;&#65292;&#24182;&#19988;&#20351;&#29992;&#29983;&#29289;&#26469;&#28304;&#30340;&#26435;&#37325;&#36827;&#34892;&#21021;&#22987;&#21270;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#21367;&#31215;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10178</link><description>&lt;p&gt;
&#31070;&#32463;&#22238;&#27874;&#65306;&#28145;&#24230;&#21367;&#31215;&#28388;&#27874;&#22120;&#22797;&#21046;&#29983;&#29289;&#24863;&#21463;&#37326;
&lt;/p&gt;
&lt;p&gt;
Neural Echos: Depthwise Convolutional Filters Replicate Biological Receptive Fields. (arXiv:2401.10178v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10178
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#21367;&#31215;&#28388;&#27874;&#22120;&#25104;&#21151;&#22797;&#21046;&#20102;&#29983;&#29289;&#24863;&#21463;&#37326;&#30340;&#32467;&#26500;&#22797;&#26434;&#24615;&#65292;&#24182;&#19988;&#20351;&#29992;&#29983;&#29289;&#26469;&#28304;&#30340;&#26435;&#37325;&#36827;&#34892;&#21021;&#22987;&#21270;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#21367;&#31215;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#28145;&#24230;&#21367;&#31215;&#26680;&#26377;&#25928;&#22320;&#22797;&#21046;&#20102;&#21754;&#20083;&#21160;&#29289;&#35270;&#32593;&#33180;&#20013;&#35266;&#23519;&#21040;&#30340;&#29983;&#29289;&#24863;&#21463;&#37326;&#30340;&#32467;&#26500;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#26469;&#33258;&#21508;&#31181;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#32463;&#36807;&#35757;&#32451;&#21367;&#31215;&#26680;&#30340;&#20998;&#26512;&#65292;&#35777;&#23454;&#20102;&#36825;&#19968;&#35777;&#25454;&#12290;&#21463;&#21040;&#36825;&#19968;&#26377;&#36259;&#30340;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#29983;&#29289;&#24863;&#21463;&#37326;&#20013;&#27762;&#21462;&#28789;&#24863;&#30340;&#21021;&#22987;&#21270;&#26041;&#26696;&#12290;&#20351;&#29992;&#20855;&#26377;&#28145;&#24230;&#21367;&#31215;&#29305;&#24449;&#30340;&#22810;&#20010;CNN&#32467;&#26500;&#23545;ImageNet&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#20998;&#26512;&#26174;&#31034;&#65292;&#24403;&#20351;&#29992;&#29983;&#29289;&#26469;&#28304;&#30340;&#26435;&#37325;&#36827;&#34892;&#21021;&#22987;&#21270;&#26102;&#65292;&#23398;&#20064;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#26174;&#33879;&#25552;&#39640;&#12290;&#36825;&#25581;&#31034;&#20102;&#29983;&#29289;&#28789;&#24863;&#30340;&#35745;&#31639;&#27169;&#22411;&#36827;&#19968;&#27493;&#20419;&#36827;&#20102;&#25105;&#20204;&#23545;&#35270;&#35273;&#22788;&#29702;&#31995;&#32479;&#30340;&#29702;&#35299;&#65292;&#24182;&#25913;&#21892;&#20102;&#21367;&#31215;&#32593;&#32476;&#30340;&#26377;&#25928;&#24615;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we present evidence suggesting that depthwise convolutional kernels are effectively replicating the structural intricacies of the biological receptive fields observed in the mammalian retina. We provide analytics of trained kernels from various state-of-the-art models substantiating this evidence. Inspired by this intriguing discovery, we propose an initialization scheme that draws inspiration from the biological receptive fields. Experimental analysis of the ImageNet dataset with multiple CNN architectures featuring depthwise convolutions reveals a marked enhancement in the accuracy of the learned model when initialized with biologically derived weights. This underlies the potential for biologically inspired computational models to further our understanding of vision processing systems and to improve the efficacy of convolutional networks.
&lt;/p&gt;</description></item><item><title>DISTINQT&#26159;&#19968;&#31181;&#38754;&#21521;&#26410;&#26469;&#31227;&#21160;&#21644;&#26080;&#32447;&#32593;&#32476;&#30340;&#38544;&#31169;&#24863;&#30693;&#20998;&#24067;&#24335;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;QoS&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2401.10158</link><description>&lt;p&gt;
DISTINQT: &#19968;&#31181;&#38754;&#21521;&#26410;&#26469;&#31227;&#21160;&#21644;&#26080;&#32447;&#32593;&#32476;&#30340;&#20998;&#24067;&#24335;&#38544;&#31169;&#24863;&#30693;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;QoS&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
DISTINQT: A Distributed Privacy Aware Learning Framework for QoS Prediction for Future Mobile and Wireless Networks. (arXiv:2401.10158v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10158
&lt;/p&gt;
&lt;p&gt;
DISTINQT&#26159;&#19968;&#31181;&#38754;&#21521;&#26410;&#26469;&#31227;&#21160;&#21644;&#26080;&#32447;&#32593;&#32476;&#30340;&#38544;&#31169;&#24863;&#30693;&#20998;&#24067;&#24335;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;QoS&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
5G&#21644;6G&#20197;&#21518;&#30340;&#32593;&#32476;&#23558;&#25903;&#25345;&#20381;&#36182;&#19968;&#23450;&#26381;&#21153;&#36136;&#37327;&#65288;QoS&#65289;&#30340;&#26032;&#30340;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29992;&#20363;&#21644;&#24212;&#29992;&#31243;&#24207;&#12290;&#21450;&#26102;&#39044;&#27979;QoS&#23545;&#20110;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#65288;&#22914;&#36710;&#36742;&#36890;&#20449;&#65289;&#23588;&#20026;&#37325;&#35201;&#12290;&#23613;&#31649;&#30452;&#21040;&#26368;&#36817;&#65292;QoS&#39044;&#27979;&#19968;&#30452;&#30001;&#38598;&#20013;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#35299;&#20915;&#26041;&#26696;&#23436;&#25104;&#65292;&#20294;&#24050;&#32463;&#20986;&#29616;&#20102;&#19968;&#20123;&#38544;&#31169;&#12289;&#35745;&#31639;&#21644;&#36816;&#33829;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;&#26367;&#20195;&#26041;&#26696;&#24050;&#32463;&#20986;&#29616;&#65288;&#22914;&#20998;&#21106;&#23398;&#20064;&#12289;&#32852;&#37030;&#23398;&#20064;&#65289;&#65292;&#23558;&#22797;&#26434;&#24230;&#36739;&#20302;&#30340;AI&#20219;&#21153;&#20998;&#24067;&#22312;&#33410;&#28857;&#20043;&#38388;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;&#26410;&#26469;&#26080;&#32447;&#32593;&#32476;&#30340;&#24322;&#26500;&#24615;&#65292;&#24403;&#28041;&#21450;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#27861;&#26102;&#65292;&#20250;&#20986;&#29616;&#26032;&#30340;&#25361;&#25112;&#12290;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DISTINQT&#30340;&#38754;&#21521;QoS&#39044;&#27979;&#30340;&#38544;&#31169;&#24863;&#30693;&#20998;&#24067;&#24335;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Beyond 5G and 6G networks are expected to support new and challenging use cases and applications that depend on a certain level of Quality of Service (QoS) to operate smoothly. Predicting the QoS in a timely manner is of high importance, especially for safety-critical applications as in the case of vehicular communications. Although until recent years the QoS prediction has been carried out by centralized Artificial Intelligence (AI) solutions, a number of privacy, computational, and operational concerns have emerged. Alternative solutions have been surfaced (e.g. Split Learning, Federated Learning), distributing AI tasks of reduced complexity across nodes, while preserving the privacy of the data. However, new challenges rise when it comes to scalable distributed learning approaches, taking into account the heterogeneous nature of future wireless networks. The current work proposes DISTINQT, a privacy-aware distributed learning framework for QoS prediction. Our framework supports mult
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#29289;&#20307;&#20013;&#24515;&#21270;&#23398;&#20064;&#20013;&#26126;&#30830;&#35299;&#24320;&#24418;&#29366;&#21644;&#32441;&#29702;&#25104;&#20998;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#28508;&#22312;&#31354;&#38388;&#21010;&#20998;&#20026;&#20004;&#20010;&#19981;&#37325;&#21472;&#30340;&#23376;&#38598;&#65292;&#20351;&#24471;&#27169;&#22411;&#26356;&#21152;&#31283;&#23450;&#21644;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2401.10148</link><description>&lt;p&gt;
&#22312;&#29289;&#20307;&#20013;&#24515;&#21270;&#23398;&#20064;&#20013;&#26126;&#30830;&#35299;&#24320;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Explicitly Disentangled Representations in Object-Centric Learning. (arXiv:2401.10148v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10148
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#29289;&#20307;&#20013;&#24515;&#21270;&#23398;&#20064;&#20013;&#26126;&#30830;&#35299;&#24320;&#24418;&#29366;&#21644;&#32441;&#29702;&#25104;&#20998;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#28508;&#22312;&#31354;&#38388;&#21010;&#20998;&#20026;&#20004;&#20010;&#19981;&#37325;&#21472;&#30340;&#23376;&#38598;&#65292;&#20351;&#24471;&#27169;&#22411;&#26356;&#21152;&#31283;&#23450;&#21644;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#21407;&#22987;&#35270;&#35273;&#25968;&#25454;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#34920;&#31034;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#20010;&#37325;&#35201;&#19988;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#26080;&#30417;&#30563;&#23398;&#20064;&#29289;&#20307;&#20013;&#24515;&#21270;&#34920;&#31034;&#30340;&#25216;&#26415;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#22686;&#24378;&#28508;&#22312;&#29305;&#24449;&#30340;&#31283;&#23450;&#24615;&#21487;&#20197;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#35757;&#32451;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#27493;&#39588;&#26159;&#35299;&#24320;&#23548;&#33268;&#25968;&#25454;&#21464;&#21270;&#30340;&#22240;&#32032;&#12290;&#20808;&#21069;&#65292;&#19981;&#21464;&#21345;&#27133;&#27880;&#24847;&#23454;&#29616;&#20102;&#20174;&#20854;&#20182;&#29305;&#24449;&#20013;&#35299;&#24320;&#20301;&#32622;&#12289;&#23610;&#24230;&#21644;&#26041;&#21521;&#12290;&#25193;&#23637;&#36825;&#19968;&#26041;&#27861;&#65292;&#25105;&#20204;&#30528;&#37325;&#20110;&#20998;&#31163;&#24418;&#29366;&#21644;&#32441;&#29702;&#32452;&#25104;&#37096;&#20998;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#65292;&#23558;&#29289;&#20307;&#20013;&#24515;&#21270;&#27169;&#22411;&#20013;&#30340;&#24418;&#29366;&#21644;&#32441;&#29702;&#25104;&#20998;&#20559;&#32622;&#20026;&#28508;&#22312;&#31354;&#38388;&#32500;&#24230;&#30340;&#20004;&#20010;&#19981;&#37325;&#21472;&#23376;&#38598;&#12290;&#36825;&#20123;&#23376;&#38598;&#26159;&#20808;&#39564;&#24050;&#30693;&#30340;&#65292;&#22240;&#27492;&#22312;&#35757;&#32451;&#36807;&#31243;&#20043;&#21069;&#12290;&#22312;&#19968;&#31995;&#21015;&#29289;&#20307;&#20013;&#24515;&#21270;&#27979;&#35797;&#20013;&#36827;&#34892;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Extracting structured representations from raw visual data is an important and long-standing challenge in machine learning. Recently, techniques for unsupervised learning of object-centric representations have raised growing interest. In this context, enhancing the robustness of the latent features can improve the efficiency and effectiveness of the training of downstream tasks. A promising step in this direction is to disentangle the factors that cause variation in the data. Previously, Invariant Slot Attention disentangled position, scale, and orientation from the remaining features. Extending this approach, we focus on separating the shape and texture components. In particular, we propose a novel architecture that biases object-centric models toward disentangling shape and texture components into two non-overlapping subsets of the latent space dimensions. These subsets are known a priori, hence before the training process. Experiments on a range of object-centric benchmarks reveal t
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#31995;&#32479;&#21270;&#22320;&#35843;&#26597;&#20102;&#29983;&#29289;&#29305;&#24449;&#35782;&#21035;&#24212;&#29992;&#20013;&#30340;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#65292;&#21253;&#25324;&#37327;&#21270;&#12289;&#30693;&#35782;&#33976;&#39311;&#21644;&#20462;&#21098;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#26041;&#27861;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2401.10139</link><description>&lt;p&gt;
&#29983;&#29289;&#29305;&#24449;&#35782;&#21035;&#24212;&#29992;&#20013;&#30340;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Model Compression Techniques in Biometrics Applications: A Survey. (arXiv:2401.10139v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#31995;&#32479;&#21270;&#22320;&#35843;&#26597;&#20102;&#29983;&#29289;&#29305;&#24449;&#35782;&#21035;&#24212;&#29992;&#20013;&#30340;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#65292;&#21253;&#25324;&#37327;&#21270;&#12289;&#30693;&#35782;&#33976;&#39311;&#21644;&#20462;&#21098;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#26041;&#27861;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#21457;&#23637;&#26497;&#22823;&#22320;&#25552;&#21319;&#20102;&#20154;&#31867;&#30340;&#20219;&#21153;&#33258;&#21160;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#24615;&#33021;&#30340;&#24040;&#22823;&#25552;&#21319;&#19982;&#20854;&#22797;&#26434;&#24615;&#30340;&#22686;&#21152;&#23494;&#20999;&#30456;&#20851;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#20013;&#30340;&#23454;&#29992;&#24615;&#65292;&#32780;&#36825;&#20123;&#35774;&#22791;&#36890;&#24120;&#29992;&#20110;&#20154;&#31867;&#23548;&#21521;&#30340;&#24212;&#29992;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#20102;&#21387;&#32553;&#25216;&#26415;&#65292;&#21487;&#20197;&#22823;&#24133;&#38477;&#20302;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#65292;&#32780;&#19981;&#20250;&#26174;&#33879;&#38477;&#20302;&#24615;&#33021;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#38024;&#23545;&#29983;&#29289;&#29305;&#24449;&#35782;&#21035;&#24212;&#29992;&#20013;&#30340;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#36827;&#34892;&#20840;&#38754;&#32508;&#36848;&#65292;&#21253;&#25324;&#37327;&#21270;&#12289;&#30693;&#35782;&#33976;&#39311;&#21644;&#20462;&#21098;&#65292;&#31995;&#32479;&#21270;&#24403;&#21069;&#25991;&#29486;&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#25216;&#26415;&#30340;&#30456;&#23545;&#20215;&#20540;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#20998;&#26512;&#65292;&#37325;&#28857;&#20851;&#27880;&#20854;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#24037;&#20316;&#26041;&#21521;&#30340;&#24314;&#35758;&#65292;&#36825;&#20123;&#26041;&#21521;&#21487;&#33021;&#20250;&#25913;&#36827;&#24403;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of deep learning algorithms has extensively empowered humanity's task automatization capacity. However, the huge improvement in the performance of these models is highly correlated with their increasing level of complexity, limiting their usefulness in human-oriented applications, which are usually deployed in resource-constrained devices. This led to the development of compression techniques that drastically reduce the computational and memory costs of deep learning models without significant performance degradation. This paper aims to systematize the current literature on this topic by presenting a comprehensive survey of model compression techniques in biometrics applications, namely quantization, knowledge distillation and pruning. We conduct a critical analysis of the comparative value of these techniques, focusing on their advantages and disadvantages and presenting suggestions for future work directions that can potentially improve the current methods. Additional
&lt;/p&gt;</description></item><item><title>&#36793;&#32536;&#21464;&#25442;&#22120;&#26159;&#19968;&#20010;&#20840;&#23616;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#23427;&#20855;&#26377;&#33267;&#23569;3-WL&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#33021;&#22815;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#36229;&#36807;&#20854;&#20182;&#26550;&#26500;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#20301;&#32622;&#25110;&#32467;&#26500;&#32534;&#30721;&#12290;</title><link>http://arxiv.org/abs/2401.10119</link><description>&lt;p&gt;
&#36208;&#21521;&#22522;&#20110;&#21407;&#21017;&#30340;&#22270;&#24418;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Towards Principled Graph Transformers. (arXiv:2401.10119v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10119
&lt;/p&gt;
&lt;p&gt;
&#36793;&#32536;&#21464;&#25442;&#22120;&#26159;&#19968;&#20010;&#20840;&#23616;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#23427;&#20855;&#26377;&#33267;&#23569;3-WL&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#33021;&#22815;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#36229;&#36807;&#20854;&#20182;&#26550;&#26500;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#20301;&#32622;&#25110;&#32467;&#26500;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;k&#32500;Weisfeiler-Leman&#65288;k-WL&#65289;&#23618;&#27425;&#32467;&#26500;&#30340;&#22270;&#24418;&#23398;&#20064;&#26550;&#26500;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#24456;&#22909;&#29702;&#35299;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#26550;&#26500;&#22312;&#30495;&#23454;&#20219;&#21153;&#20013;&#24448;&#24448;&#26080;&#27861;&#25552;&#20379;&#21487;&#38752;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#38469;&#24433;&#21709;&#21147;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22522;&#20110;&#20840;&#23616;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#22914;&#22270;&#24418;&#21464;&#25442;&#22120;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#23558;&#23427;&#20204;&#30340;&#34920;&#36798;&#33021;&#21147;&#19982;k-WL&#23618;&#27425;&#32467;&#26500;&#36827;&#34892;&#27604;&#36739;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#23588;&#20854;&#26159;&#22240;&#20026;&#36825;&#20123;&#26550;&#26500;&#20381;&#36182;&#20110;&#20301;&#32622;&#25110;&#32467;&#26500;&#32534;&#30721;&#26469;&#23454;&#29616;&#20854;&#34920;&#36798;&#33021;&#21147;&#21644;&#39044;&#27979;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#36793;&#32536;&#21464;&#25442;&#22120;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#33410;&#28857;&#23545;&#32780;&#19981;&#26159;&#33410;&#28857;&#19978;&#36827;&#34892;&#25805;&#20316;&#30340;&#20840;&#23616;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#20855;&#26377;&#33267;&#23569;3-WL&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#32463;&#39564;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36793;&#32536;&#21464;&#25442;&#22120;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#20854;&#20182;&#29702;&#35770;&#23545;&#40784;&#30340;&#26550;&#26500;&#65292;&#21516;&#26102;&#19981;&#20381;&#36182;&#20110;&#20301;&#32622;&#25110;&#32467;&#26500;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph learning architectures based on the k-dimensional Weisfeiler-Leman (k-WL) hierarchy offer a theoretically well-understood expressive power. However, such architectures often fail to deliver solid predictive performance on real-world tasks, limiting their practical impact. In contrast, global attention-based models such as graph transformers demonstrate strong performance in practice, but comparing their expressive power with the k-WL hierarchy remains challenging, particularly since these architectures rely on positional or structural encodings for their expressivity and predictive performance. To address this, we show that the recently proposed Edge Transformer, a global attention model operating on node pairs instead of nodes, has at least 3-WL expressive power. Empirically, we demonstrate that the Edge Transformer surpasses other theoretically aligned architectures regarding predictive performance while not relying on positional or structural encodings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24212;&#29992;&#27010;&#29575;&#22270;&#27169;&#22411;&#36827;&#34892;&#36870;&#21521;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#31038;&#20250;&#29983;&#24577;&#31995;&#32479;&#12290;&#23454;&#39564;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#39044;&#27979;&#29983;&#24577;&#31995;&#32479;&#23545;&#20551;&#35774;&#24178;&#39044;&#30340;&#21709;&#24212;&#65292;&#24182;&#30830;&#23450;&#21464;&#37327;&#20043;&#38388;&#30340;&#24433;&#21709;&#12290;&#36825;&#19968;&#26041;&#27861;&#20026;&#22810;&#20010;&#39046;&#22495;&#30340;&#19987;&#23478;&#25552;&#20379;&#20102;&#30452;&#35266;&#26131;&#25026;&#30340;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2401.10101</link><description>&lt;p&gt;
&#29992;&#27010;&#29575;&#22270;&#27169;&#22411;&#36827;&#34892;&#36870;&#21521;&#25512;&#29702;&#20197;&#20998;&#26512;&#31038;&#20250;&#29983;&#24577;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Reasoning with Probabilistic Graphical Models for Analyzing Socioecological Systems. (arXiv:2401.10101v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24212;&#29992;&#27010;&#29575;&#22270;&#27169;&#22411;&#36827;&#34892;&#36870;&#21521;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#31038;&#20250;&#29983;&#24577;&#31995;&#32479;&#12290;&#23454;&#39564;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#39044;&#27979;&#29983;&#24577;&#31995;&#32479;&#23545;&#20551;&#35774;&#24178;&#39044;&#30340;&#21709;&#24212;&#65292;&#24182;&#30830;&#23450;&#21464;&#37327;&#20043;&#38388;&#30340;&#24433;&#21709;&#12290;&#36825;&#19968;&#26041;&#27861;&#20026;&#22810;&#20010;&#39046;&#22495;&#30340;&#19987;&#23478;&#25552;&#20379;&#20102;&#30452;&#35266;&#26131;&#25026;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#21644;&#36870;&#21521;&#25512;&#29702;&#26159;&#25968;&#25454;&#31185;&#23398;&#20013;&#26032;&#20852;&#30340;&#26041;&#21521;&#65292;&#21487;&#20197;&#35753;&#25105;&#20204;&#25512;&#26029;&#20986;&#20551;&#35774;&#24773;&#26223;&#12290;&#22312;&#23454;&#39564;&#25968;&#25454;&#36890;&#24120;&#19981;&#21487;&#29992;&#30340;&#39046;&#22495;&#65292;&#36825;&#23588;&#20854;&#26377;&#29992;&#12290;&#22312;&#29615;&#22659;&#21644;&#29983;&#24577;&#31185;&#23398;&#39046;&#22495;&#65292;&#22240;&#26524;&#24615;&#20351;&#25105;&#20204;&#33021;&#22815;&#39044;&#27979;&#29983;&#24577;&#31995;&#32479;&#23545;&#20551;&#35774;&#24178;&#39044;&#30340;&#21709;&#24212;&#12290;&#32467;&#26500;&#24615;&#22240;&#26524;&#27169;&#22411;&#26159;&#19968;&#31181;&#29992;&#20110;&#22240;&#26524;&#24615;&#30340;&#27010;&#29575;&#22270;&#27169;&#22411;&#31867;&#21035;&#65292;&#30001;&#20110;&#20854;&#30452;&#35266;&#30340;&#29305;&#24615;&#65292;&#22810;&#20010;&#39046;&#22495;&#30340;&#19987;&#23478;&#21487;&#20197;&#36731;&#26494;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#26576;&#20123;&#26597;&#35810;&#65292;&#31216;&#20026;&#19981;&#21487;&#36777;&#35782;&#30340;&#26597;&#35810;&#65292;&#26080;&#27861;&#20197;&#31934;&#30830;&#30340;&#26041;&#24335;&#35745;&#31639;&#12290;&#26412;&#25991;&#25552;&#20986;&#24212;&#29992;&#19968;&#31181;&#26032;&#39062;&#12289;&#26368;&#36817;&#30340;&#25216;&#26415;&#26469;&#30028;&#23450;&#31038;&#20250;&#29983;&#24577;&#31995;&#32479;&#39046;&#22495;&#20869;&#30340;&#19981;&#21487;&#36777;&#35782;&#26597;&#35810;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#20256;&#32479;&#30340;&#32479;&#35745;&#20998;&#26512;&#65292;&#21253;&#25324;&#27010;&#29575;&#22270;&#27169;&#22411;&#65292;&#21487;&#20197;&#30830;&#23450;&#21464;&#37327;&#20043;&#38388;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#25552;&#20379;&#20851;&#20110;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal and counterfactual reasoning are emerging directions in data science that allow us to reason about hypothetical scenarios. This is particularly useful in domains where experimental data are usually not available. In the context of environmental and ecological sciences, causality enables us, for example, to predict how an ecosystem would respond to hypothetical interventions. A structural causal model is a class of probabilistic graphical models for causality, which, due to its intuitive nature, can be easily understood by experts in multiple fields. However, certain queries, called unidentifiable, cannot be calculated in an exact and precise manner. This paper proposes applying a novel and recent technique for bounding unidentifiable queries within the domain of socioecological systems. Our findings indicate that traditional statistical analysis, including probabilistic graphical models, can identify the influence between variables. However, such methods do not offer insights in
&lt;/p&gt;</description></item><item><title>DiffusionGPT&#26159;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#32479;&#19968;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#31995;&#32479;&#65292;&#33021;&#22815;&#22788;&#29702;&#22810;&#26679;&#21270;&#30340;&#36755;&#20837;&#24182;&#25972;&#21512;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.10061</link><description>&lt;p&gt;
DiffusionGPT: &#22522;&#20110;LLM&#30340;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
DiffusionGPT: LLM-Driven Text-to-Image Generation System. (arXiv:2401.10061v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10061
&lt;/p&gt;
&lt;p&gt;
DiffusionGPT&#26159;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#32479;&#19968;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#31995;&#32479;&#65292;&#33021;&#22815;&#22788;&#29702;&#22810;&#26679;&#21270;&#30340;&#36755;&#20837;&#24182;&#25972;&#21512;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#20026;&#22270;&#20687;&#29983;&#25104;&#39046;&#22495;&#25171;&#24320;&#20102;&#26032;&#30340;&#36947;&#36335;&#65292;&#23548;&#33268;&#20102;&#22312;&#24320;&#28304;&#24179;&#21488;&#19978;&#20849;&#20139;&#39640;&#36136;&#37327;&#27169;&#22411;&#30340;&#24191;&#27867;&#20256;&#25773;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#31995;&#32479;&#23384;&#22312;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#65292;&#21363;&#24448;&#24448;&#26080;&#27861;&#22788;&#29702;&#22810;&#26679;&#21270;&#30340;&#36755;&#20837;&#65292;&#25110;&#20165;&#38480;&#20110;&#21333;&#19968;&#27169;&#22411;&#30340;&#32467;&#26524;&#12290;&#30446;&#21069;&#30340;&#32479;&#19968;&#23581;&#35797;&#36890;&#24120;&#20998;&#20026;&#20004;&#20010;&#27491;&#20132;&#26041;&#38754;&#65306;i&#65289;&#22312;&#36755;&#20837;&#38454;&#27573;&#35299;&#26512;&#22810;&#26679;&#30340;&#25552;&#31034;&#65307;ii&#65289;&#28608;&#27963;&#19987;&#23478;&#27169;&#22411;&#36827;&#34892;&#36755;&#20986;&#12290;&#20026;&#20102;&#20860;&#39038;&#20004;&#32773;&#30340;&#20248;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DiffusionGPT&#65292;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#29983;&#25104;&#31995;&#32479;&#65292;&#33021;&#22815;&#26080;&#32541;&#22320;&#36866;&#24212;&#21508;&#31181;&#31867;&#22411;&#30340;&#25552;&#31034;&#24182;&#25972;&#21512;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#12290;DiffusionGPT&#22522;&#20110;&#20808;&#39564;&#30693;&#35782;&#20026;&#21508;&#31181;&#29983;&#25104;&#27169;&#22411;&#26500;&#24314;&#20102;&#39046;&#22495;&#29305;&#23450;&#30340;Thought&#26641;&#12290;&#24403;&#25552;&#20379;&#36755;&#20837;&#26102;&#65292;LLM&#35299;&#26512;&#25552;&#31034;&#24182;&#21033;&#29992;Thought&#26641;&#26469;&#25351;&#23548;&#36873;&#25321;&#36866;&#24403;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#25918;&#26494;&#36755;&#20837;&#32422;&#26463;&#24182;&#30830;&#20445;&#24322;&#24120;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have opened up new avenues for the field of image generation, resulting in the proliferation of high-quality models shared on open-source platforms. However, a major challenge persists in current text-to-image systems are often unable to handle diverse inputs, or are limited to single model results. Current unified attempts often fall into two orthogonal aspects: i) parse Diverse Prompts in input stage; ii) activate expert model to output. To combine the best of both worlds, we propose DiffusionGPT, which leverages Large Language Models (LLM) to offer a unified generation system capable of seamlessly accommodating various types of prompts and integrating domain-expert models. DiffusionGPT constructs domain-specific Trees for various generative models based on prior knowledge. When provided with an input, the LLM parses the prompt and employs the Trees-of-Thought to guide the selection of an appropriate model, thereby relaxing input constraints and ensuring exceptional 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32467;&#26500;&#21270;&#30340;&#31185;&#23398;&#20449;&#24687;&#25552;&#21462;&#65292;&#22312;&#30149;&#27602;&#23398;&#39046;&#22495;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#31616;&#27905;&#30340;&#23398;&#26415;&#36129;&#29486;&#25688;&#35201;&#65292;&#23545;&#31185;&#23398;&#23478;&#36827;&#34892;&#23548;&#33322;&#21644;&#35299;&#20915;LLM&#30340;&#32039;&#36843;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.10040</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31185;&#23398;&#20449;&#24687;&#25552;&#21462;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#38024;&#23545;&#30149;&#27602;&#23398;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Scientific Information Extraction: An Empirical Study for Virology. (arXiv:2401.10040v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10040
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32467;&#26500;&#21270;&#30340;&#31185;&#23398;&#20449;&#24687;&#25552;&#21462;&#65292;&#22312;&#30149;&#27602;&#23398;&#39046;&#22495;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#31616;&#27905;&#30340;&#23398;&#26415;&#36129;&#29486;&#25688;&#35201;&#65292;&#23545;&#31185;&#23398;&#23478;&#36827;&#34892;&#23548;&#33322;&#21644;&#35299;&#20915;LLM&#30340;&#32039;&#36843;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20513;&#23548;&#20351;&#29992;&#32467;&#26500;&#21270;&#21644;&#35821;&#20041;&#20869;&#23481;&#34920;&#31034;&#26469;&#36827;&#34892;&#22522;&#20110;&#23398;&#26415;&#20132;&#27969;&#30340;&#23398;&#26415;&#35770;&#25991;&#65292;&#21463;&#21040;&#32500;&#22522;&#30334;&#31185;&#20449;&#24687;&#26694;&#25110;&#32467;&#26500;&#21270;&#30340;&#20122;&#39532;&#36874;&#20135;&#21697;&#25551;&#36848;&#31561;&#24037;&#20855;&#30340;&#21551;&#21457;&#12290;&#36825;&#20123;&#34920;&#31034;&#24418;&#24335;&#25552;&#20379;&#20102;&#31616;&#27905;&#30340;&#27010;&#36848;&#65292;&#24110;&#21161;&#31185;&#23398;&#23478;&#22312;&#27987;&#21402;&#30340;&#23398;&#26415;&#29615;&#22659;&#20013;&#36827;&#34892;&#23548;&#33322;&#12290;&#25105;&#20204;&#30340;&#26032;&#39062;&#33258;&#21160;&#21270;&#26041;&#27861;&#21033;&#29992;LLM&#30340;&#24378;&#22823;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#65292;&#20135;&#29983;&#32467;&#26500;&#21270;&#30340;&#23398;&#26415;&#36129;&#29486;&#25688;&#35201;&#65292;&#26082;&#25552;&#20379;&#20102;&#23454;&#38469;&#35299;&#20915;&#26041;&#26696;&#65292;&#20063;&#25581;&#31034;&#20102;LLM&#32039;&#36843;&#30340;&#33021;&#21147;&#12290;&#23545;&#20110;LLM&#65292;&#20027;&#35201;&#20851;&#27880;&#30340;&#26159;&#25913;&#21892;&#20854;&#20316;&#20026;&#23545;&#35805;&#20195;&#29702;&#30340;&#36890;&#29992;&#26234;&#33021;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#27169;&#22411;&#20063;&#21487;&#20197;&#22312;&#20449;&#24687;&#25552;&#21462;&#65288;IE&#65289;&#20013;&#26377;&#25928;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#31185;&#23398;&#31561;&#39046;&#22495;&#30340;&#22797;&#26434;IE&#20219;&#21153;&#20013;&#12290;&#36825;&#31181;&#33539;&#24335;&#36716;&#21464;&#29992;&#19968;&#31995;&#21015;&#25351;&#20196;&#20195;&#26367;&#20102;&#20256;&#32479;&#30340;&#27169;&#22359;&#21270;&#12289;&#27969;&#27700;&#32447;&#24335;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#31616;&#21270;&#20102;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#24494;&#35843;&#30340;FLAN-T&#27169;&#22411;&#21487;&#20197;&#21462;&#24471;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we champion the use of structured and semantic content representation of discourse-based scholarly communication, inspired by tools like Wikipedia infoboxes or structured Amazon product descriptions. These representations provide users with a concise overview, aiding scientists in navigating the dense academic landscape. Our novel automated approach leverages the robust text generation capabilities of LLMs to produce structured scholarly contribution summaries, offering both a practical solution and insights into LLMs' emergent abilities.  For LLMs, the prime focus is on improving their general intelligence as conversational agents. We argue that these models can also be applied effectively in information extraction (IE), specifically in complex IE tasks within terse domains like Science. This paradigm shift replaces the traditional modular, pipelined machine learning approach with a simpler objective expressed through instructions. Our results show that finetuned FLAN-T
&lt;/p&gt;</description></item><item><title>LOCALINTEL&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#30693;&#35782;&#19978;&#19979;&#25991;&#21270;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#20174;&#20840;&#29699;&#21644;&#26412;&#22320;&#30693;&#35782;&#25968;&#25454;&#24211;&#20013;&#33258;&#21160;&#29983;&#25104;&#32452;&#32455;&#30340;&#23041;&#32961;&#24773;&#25253;&#12290;</title><link>http://arxiv.org/abs/2401.10036</link><description>&lt;p&gt;
LOCALINTEL&#65306;&#20174;&#20840;&#29699;&#21644;&#26412;&#22320;&#32593;&#32476;&#30693;&#35782;&#29983;&#25104;&#32452;&#32455;&#23041;&#32961;&#24773;&#25253;
&lt;/p&gt;
&lt;p&gt;
LOCALINTEL: Generating Organizational Threat Intelligence from Global and Local Cyber Knowledge. (arXiv:2401.10036v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10036
&lt;/p&gt;
&lt;p&gt;
LOCALINTEL&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#30693;&#35782;&#19978;&#19979;&#25991;&#21270;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#20174;&#20840;&#29699;&#21644;&#26412;&#22320;&#30693;&#35782;&#25968;&#25454;&#24211;&#20013;&#33258;&#21160;&#29983;&#25104;&#32452;&#32455;&#30340;&#23041;&#32961;&#24773;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#25805;&#20316;&#20013;&#24515;&#65288;SoC&#65289;&#20998;&#26512;&#24072;&#20174;&#20844;&#24320;&#35775;&#38382;&#30340;&#20840;&#29699;&#23041;&#32961;&#25968;&#25454;&#24211;&#20013;&#25910;&#38598;&#23041;&#32961;&#25253;&#21578;&#65292;&#24182;&#25163;&#21160;&#33258;&#23450;&#20041;&#20197;&#36866;&#24212;&#29305;&#23450;&#32452;&#32455;&#30340;&#38656;&#27714;&#12290;&#36825;&#20123;&#20998;&#26512;&#24072;&#36824;&#20381;&#36182;&#20110;&#20869;&#37096;&#23384;&#20648;&#24211;&#65292;&#20316;&#20026;&#32452;&#32455;&#30340;&#31169;&#26377;&#26412;&#22320;&#30693;&#35782;&#25968;&#25454;&#24211;&#12290;&#21487;&#20449;&#30340;&#32593;&#32476;&#24773;&#25253;&#12289;&#20851;&#38190;&#25805;&#20316;&#32454;&#33410;&#21644;&#30456;&#20851;&#32452;&#32455;&#20449;&#24687;&#37117;&#23384;&#20648;&#22312;&#36825;&#20123;&#26412;&#22320;&#30693;&#35782;&#25968;&#25454;&#24211;&#20013;&#12290;&#20998;&#26512;&#24072;&#21033;&#29992;&#36825;&#20123;&#20840;&#29699;&#21644;&#26412;&#22320;&#30693;&#35782;&#25968;&#25454;&#24211;&#20174;&#20107;&#19968;&#39033;&#32321;&#37325;&#30340;&#20219;&#21153;&#65292;&#25163;&#21160;&#21019;&#24314;&#32452;&#32455;&#29420;&#29305;&#30340;&#23041;&#32961;&#21709;&#24212;&#21644;&#32531;&#35299;&#31574;&#30053;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#23637;&#31034;&#20102;&#39640;&#25928;&#22788;&#29702;&#22823;&#35268;&#27169;&#22810;&#26679;&#21270;&#30693;&#35782;&#28304;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#33021;&#21147;&#26469;&#22788;&#29702;&#20840;&#29699;&#21644;&#26412;&#22320;&#30693;&#35782;&#25968;&#25454;&#24211;&#65292;&#33258;&#21160;&#21270;&#29983;&#25104;&#32452;&#32455;&#29305;&#23450;&#30340;&#23041;&#32961;&#24773;&#25253;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LOCALINTEL&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#33258;&#21160;&#21270;&#30693;&#35782;&#19978;&#19979;&#25991;&#21270;&#31995;&#32479;&#65292;&#21487;&#20197;&#20174;&#20840;&#29699;&#21644;&#26412;&#22320;&#30693;&#35782;&#25968;&#25454;&#24211;&#20013;&#29983;&#25104;&#32452;&#32455;&#30340;&#23041;&#32961;&#24773;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;
Security Operations Center (SoC) analysts gather threat reports from openly accessible global threat databases and customize them manually to suit a particular organization's needs. These analysts also depend on internal repositories, which act as private local knowledge database for an organization. Credible cyber intelligence, critical operational details, and relevant organizational information are all stored in these local knowledge databases. Analysts undertake a labor intensive task utilizing these global and local knowledge databases to manually create organization's unique threat response and mitigation strategies. Recently, Large Language Models (LLMs) have shown the capability to efficiently process large diverse knowledge sources. We leverage this ability to process global and local knowledge databases to automate the generation of organization-specific threat intelligence.  In this work, we present LOCALINTEL, a novel automated knowledge contextualization system that, upon 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#36827;&#21270;&#35745;&#31639;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#40657;&#30418;&#35774;&#32622;&#19979;&#36827;&#19968;&#27493;&#25552;&#21319;&#22823;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#20197;&#21450;&#23558;&#22823;&#35821;&#35328;&#27169;&#22411;&#19982;&#36827;&#21270;&#31639;&#27861;&#32467;&#21512;&#24212;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.10034</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#36827;&#21270;&#35745;&#31639;&#65306;&#35843;&#26597;&#19982;&#36335;&#32447;&#22270;
&lt;/p&gt;
&lt;p&gt;
Evolutionary Computation in the Era of Large Language Model: Survey and Roadmap. (arXiv:2401.10034v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10034
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#36827;&#21270;&#35745;&#31639;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#40657;&#30418;&#35774;&#32622;&#19979;&#36827;&#19968;&#27493;&#25552;&#21319;&#22823;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#20197;&#21450;&#23558;&#22823;&#35821;&#35328;&#27169;&#22411;&#19982;&#36827;&#21270;&#31639;&#27861;&#32467;&#21512;&#24212;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#22522;&#20110;Transformer&#26550;&#26500;&#65292;&#22312;&#22810;&#26679;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#65292;&#23427;&#20204;&#19981;&#20165;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#65292;&#36824;&#23558;&#20854;&#33021;&#21147;&#25193;&#23637;&#21040;&#20102;&#21508;&#20010;&#39046;&#22495;&#65292;&#36808;&#21521;&#20102;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;&#23613;&#31649;&#36827;&#21270;&#31639;&#27861;&#65288;EAs&#65289;&#19982;LLMs&#22312;&#30446;&#26631;&#21644;&#26041;&#27861;&#35770;&#19978;&#23384;&#22312;&#24046;&#24322;&#65292;&#20294;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#25581;&#31034;&#20102;&#26377;&#36259;&#30340;&#30456;&#20284;&#20043;&#22788;&#65292;&#29305;&#21035;&#26159;&#22312;&#20182;&#20204;&#20849;&#21516;&#30340;&#20248;&#21270;&#24615;&#36136;&#12289;&#40657;&#30418;&#29305;&#24615;&#21644;&#22788;&#29702;&#22797;&#26434;&#38382;&#39064;&#30340;&#33021;&#21147;&#26041;&#38754;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#36827;&#21270;&#31639;&#27861;&#19981;&#20165;&#21487;&#20197;&#20026;LLM&#22312;&#40657;&#30418;&#35774;&#32622;&#19979;&#25552;&#20379;&#20248;&#21270;&#26694;&#26550;&#65292;&#36824;&#21487;&#20197;&#22312;&#24212;&#29992;&#20013;&#20026;LLM&#36171;&#20104;&#28789;&#27963;&#30340;&#20840;&#23616;&#25628;&#32034;&#21644;&#36845;&#20195;&#26426;&#21046;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;LLM&#20016;&#23500;&#30340;&#39046;&#22495;&#30693;&#35782;&#20351;&#24471;&#36827;&#21270;&#31639;&#27861;&#21487;&#20197;&#36827;&#34892;&#26356;&#26234;&#33021;&#30340;&#25628;&#32034;&#65292;&#32780;&#20854;&#25991;&#26412;&#22788;&#29702;&#33021;&#21147;&#21017;&#26377;&#21161;&#20110;&#23558;&#36827;&#21270;&#31639;&#27861;&#24212;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#12290;&#22522;&#20110;&#23427;&#20204;&#30340;&#20114;&#34917;&#20248;&#21183;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20221;&#35843;&#26597;&#21644;&#36335;&#32447;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), built upon Transformer-based architectures with massive pretraining on diverse data, have not only revolutionized natural language processing but also extended their prowess to various domains, marking a significant stride towards artificial general intelligence. The interplay between LLMs and Evolutionary Algorithms (EAs), despite differing in objectives and methodologies, reveals intriguing parallels, especially in their shared optimization nature, black-box characteristics, and proficiency in handling complex problems. Meanwhile, EA can not only provide an optimization framework for LLM's further enhancement under black-box settings but also empower LLM with flexible global search and iterative mechanism in applications. On the other hand, LLM's abundant domain knowledge enables EA to perform smarter searches, while its text processing capability assist in deploying EA across various tasks. Based on their complementary advantages, this paper presents a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#21644;&#24555;&#36895;&#30340;&#39057;&#29575;&#24863;&#30693;&#25193;&#25955;&#22768;&#30721;&#22120;FreGrad&#65292;&#36890;&#36807;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;&#12289;&#39057;&#29575;&#24863;&#30693;&#30340;&#25193;&#24352;&#21367;&#31215;&#21644;&#25216;&#24039;&#31561;&#20851;&#38190;&#32452;&#20214;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#38899;&#39057;&#30340;&#29983;&#25104;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;FreGrad&#30456;&#27604;&#22522;&#20934;&#27169;&#22411;&#35757;&#32451;&#36895;&#24230;&#24555;3.7&#20493;&#65292;&#25512;&#26029;&#36895;&#24230;&#24555;2.2&#20493;&#65292;&#27169;&#22411;&#22823;&#23567;&#20943;&#23567;0.6&#20493;&#65292;&#32780;&#36755;&#20986;&#36136;&#37327;&#19981;&#21463;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.10032</link><description>&lt;p&gt;
FreGrad: &#36731;&#37327;&#32423;&#21644;&#24555;&#36895;&#30340;&#39057;&#29575;&#24863;&#30693;&#25193;&#25955;&#22768;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
FreGrad: Lightweight and Fast Frequency-aware Diffusion Vocoder. (arXiv:2401.10032v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#21644;&#24555;&#36895;&#30340;&#39057;&#29575;&#24863;&#30693;&#25193;&#25955;&#22768;&#30721;&#22120;FreGrad&#65292;&#36890;&#36807;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;&#12289;&#39057;&#29575;&#24863;&#30693;&#30340;&#25193;&#24352;&#21367;&#31215;&#21644;&#25216;&#24039;&#31561;&#20851;&#38190;&#32452;&#20214;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#38899;&#39057;&#30340;&#29983;&#25104;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;FreGrad&#30456;&#27604;&#22522;&#20934;&#27169;&#22411;&#35757;&#32451;&#36895;&#24230;&#24555;3.7&#20493;&#65292;&#25512;&#26029;&#36895;&#24230;&#24555;2.2&#20493;&#65292;&#27169;&#22411;&#22823;&#23567;&#20943;&#23567;0.6&#20493;&#65292;&#32780;&#36755;&#20986;&#36136;&#37327;&#19981;&#21463;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#20351;&#29992;&#19968;&#31181;&#36731;&#37327;&#32423;&#21644;&#24555;&#36895;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#22768;&#30721;&#22120;FreGrad&#29983;&#25104;&#36924;&#30495;&#30340;&#38899;&#39057;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#20197;&#19979;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;&#65288;1&#65289;&#25105;&#20204;&#20351;&#29992;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;&#23558;&#22797;&#26434;&#30340;&#27874;&#24418;&#20998;&#35299;&#20026;&#23376;&#24102;&#23567;&#27874;&#65292;&#36825;&#26377;&#21161;&#20110;FreGrad&#22312;&#31616;&#21333;&#32780;&#31616;&#27905;&#30340;&#29305;&#24449;&#31354;&#38388;&#19978;&#25805;&#20316;&#65292;&#65288;2&#65289;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#39057;&#29575;&#24863;&#30693;&#30340;&#25193;&#24352;&#21367;&#31215;&#26469;&#25552;&#21319;&#39057;&#29575;&#24863;&#30693;&#33021;&#21147;&#65292;&#20174;&#32780;&#29983;&#25104;&#20855;&#26377;&#20934;&#30830;&#39057;&#29575;&#20449;&#24687;&#30340;&#35821;&#38899;&#65292;&#65288;3&#65289;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20123;&#25216;&#24039;&#26469;&#25552;&#39640;&#25152;&#25552;&#27169;&#22411;&#30340;&#29983;&#25104;&#36136;&#37327;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;FreGrad&#30456;&#27604;&#22522;&#20934;&#27169;&#22411;&#30340;&#35757;&#32451;&#36895;&#24230;&#24555;&#20102;3.7&#20493;&#65292;&#25512;&#26029;&#36895;&#24230;&#24555;&#20102;2.2&#20493;&#65292;&#21516;&#26102;&#27169;&#22411;&#30340;&#22823;&#23567;&#20943;&#23567;&#20102;0.6&#20493;&#65288;&#20165;1.78M&#21442;&#25968;&#65289;&#65292;&#32780;&#19981;&#38477;&#20302;&#36755;&#20986;&#36136;&#37327;&#12290;&#38899;&#39057;&#26679;&#26412;&#21487;&#22312;&#20197;&#19979;&#38142;&#25509;&#20013;&#33719;&#24471;&#65306;https://mm.kaist.ac.kr/projects/FreGrad&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of this paper is to generate realistic audio with a lightweight and fast diffusion-based vocoder, named FreGrad. Our framework consists of the following three key components: (1) We employ discrete wavelet transform that decomposes a complicated waveform into sub-band wavelets, which helps FreGrad to operate on a simple and concise feature space, (2) We design a frequency-aware dilated convolution that elevates frequency awareness, resulting in generating speech with accurate frequency information, and (3) We introduce a bag of tricks that boosts the generation quality of the proposed model. In our experiments, FreGrad achieves 3.7 times faster training time and 2.2 times faster inference speed compared to our baseline while reducing the model size by 0.6 times (only 1.78M parameters) without sacrificing the output quality. Audio samples are available at: https://mm.kaist.ac.kr/projects/FreGrad.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#22870;&#21169;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;LLM&#20316;&#20026;&#35780;&#21028;&#32773;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#33258;&#24049;&#25552;&#20379;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#22870;&#21169;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#25552;&#39640;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#65292;&#36824;&#21487;&#20197;&#20026;&#33258;&#24049;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#22870;&#21169;&#12290;&#36890;&#36807;&#23545;Llama 2 70B&#27169;&#22411;&#30340;&#19977;&#27425;&#36845;&#20195;&#24494;&#35843;&#65292;&#32467;&#26524;&#22312;AlpacaEval 2.0&#25490;&#34892;&#27036;&#19978;&#36229;&#36807;&#20102;&#20854;&#20182;&#29616;&#26377;&#31995;&#32479;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#23454;&#29616;&#33021;&#22815;&#19981;&#26029;&#33258;&#25105;&#25913;&#36827;&#30340;&#27169;&#22411;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10020</link><description>&lt;p&gt;
&#33258;&#22870;&#21169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Self-Rewarding Language Models. (arXiv:2401.10020v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10020
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#22870;&#21169;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;LLM&#20316;&#20026;&#35780;&#21028;&#32773;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#33258;&#24049;&#25552;&#20379;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#22870;&#21169;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#25552;&#39640;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#65292;&#36824;&#21487;&#20197;&#20026;&#33258;&#24049;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#22870;&#21169;&#12290;&#36890;&#36807;&#23545;Llama 2 70B&#27169;&#22411;&#30340;&#19977;&#27425;&#36845;&#20195;&#24494;&#35843;&#65292;&#32467;&#26524;&#22312;AlpacaEval 2.0&#25490;&#34892;&#27036;&#19978;&#36229;&#36807;&#20102;&#20854;&#20182;&#29616;&#26377;&#31995;&#32479;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#23454;&#29616;&#33021;&#22815;&#19981;&#26029;&#33258;&#25105;&#25913;&#36827;&#30340;&#27169;&#22411;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20551;&#35774;&#35201;&#23454;&#29616;&#36229;&#20154;&#32423;&#30340;&#26234;&#33021;&#20307;&#65292;&#26410;&#26469;&#30340;&#27169;&#22411;&#38656;&#35201;&#36229;&#20154;&#32423;&#30340;&#21453;&#39304;&#65292;&#20197;&#25552;&#20379;&#36275;&#22815;&#30340;&#35757;&#32451;&#20449;&#21495;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#26159;&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#65292;&#36825;&#21487;&#33021;&#20250;&#21463;&#21040;&#20154;&#31867;&#34920;&#29616;&#27700;&#24179;&#30340;&#38480;&#21046;&#65292;&#32780;&#19988;&#36825;&#20123;&#29420;&#31435;&#30340;&#20923;&#32467;&#22870;&#21169;&#27169;&#22411;&#22312;LLM&#35757;&#32451;&#36807;&#31243;&#20013;&#26080;&#27861;&#23398;&#20064;&#25913;&#36827;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#33258;&#22870;&#21169;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#20013;&#35821;&#35328;&#27169;&#22411;&#26412;&#36523;&#36890;&#36807;LLM&#20316;&#20026;&#35780;&#21028;&#32773;&#30340;&#25552;&#31034;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25552;&#20379;&#33258;&#24049;&#30340;&#22870;&#21169;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#36845;&#20195;DPO&#35757;&#32451;&#20013;&#65292;&#19981;&#20165;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#24471;&#21040;&#20102;&#25552;&#39640;&#65292;&#32780;&#19988;&#33021;&#22815;&#20026;&#33258;&#24049;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#22870;&#21169;&#12290;&#36890;&#36807;&#23545;Llama 2 70B&#36827;&#34892;&#25105;&#20204;&#26041;&#27861;&#30340;&#19977;&#27425;&#36845;&#20195;&#30340;&#24494;&#35843;&#65292;&#24471;&#21040;&#30340;&#27169;&#22411;&#22312;AlpacaEval 2.0&#25490;&#34892;&#27036;&#19978;&#32988;&#36807;&#35768;&#22810;&#29616;&#26377;&#31995;&#32479;&#65292;&#21253;&#25324;Claude 2&#12289;Gemini Pro&#21644;GPT-4 0613&#12290;&#34429;&#28982;&#36825;&#21482;&#26159;&#19968;&#39033;&#21021;&#27493;&#30740;&#31350;&#65292;&#20294;&#36825;&#39033;&#24037;&#20316;&#20026;&#21487;&#33021;&#23454;&#29616;&#33021;&#22815;&#19981;&#26029;&#33258;&#25105;&#25913;&#36827;&#30340;&#27169;&#22411;&#25171;&#24320;&#20102;&#22823;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
We posit that to achieve superhuman agents, future models require superhuman feedback in order to provide an adequate training signal. Current approaches commonly train reward models from human preferences, which may then be bottlenecked by human performance level, and secondly these separate frozen reward models cannot then learn to improve during LLM training. In this work, we study Self-Rewarding Language Models, where the language model itself is used via LLM-as-a-Judge prompting to provide its own rewards during training. We show that during Iterative DPO training that not only does instruction following ability improve, but also the ability to provide high-quality rewards to itself. Fine-tuning Llama 2 70B on three iterations of our approach yields a model that outperforms many existing systems on the AlpacaEval 2.0 leaderboard, including Claude 2, Gemini Pro, and GPT-4 0613. While only a preliminary study, this work opens the door to the possibility of models that can continuall
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20027;&#35201;&#20171;&#32461;&#20102;&#19968;&#31181;&#35780;&#20272;LLM&#20195;&#29702;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#21028;&#26029;&#23433;&#20840;&#39118;&#38505;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;R-Judge&#65292;&#36890;&#36807;&#23545;162&#20010;&#20195;&#29702;&#20132;&#20114;&#35760;&#24405;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT-4&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#65292;&#36798;&#21040;&#20102;72.29%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.10019</link><description>&lt;p&gt;
R-Judge: &#35780;&#20272;LLM&#20195;&#29702;&#30340;&#23433;&#20840;&#39118;&#38505;&#24847;&#35782;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
R-Judge: Benchmarking Safety Risk Awareness for LLM Agents. (arXiv:2401.10019v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10019
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20027;&#35201;&#20171;&#32461;&#20102;&#19968;&#31181;&#35780;&#20272;LLM&#20195;&#29702;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#21028;&#26029;&#23433;&#20840;&#39118;&#38505;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;R-Judge&#65292;&#36890;&#36807;&#23545;162&#20010;&#20195;&#29702;&#20132;&#20114;&#35760;&#24405;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT-4&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#65292;&#36798;&#21040;&#20102;72.29%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#21160;&#23436;&#25104;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20219;&#21153;&#26041;&#38754;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;LLM&#20195;&#29702;&#22312;&#20132;&#20114;&#29615;&#22659;&#20013;&#25805;&#20316;&#26102;&#20250;&#24341;&#20837;&#24847;&#22806;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#19982;&#22823;&#22810;&#25968;&#20043;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;LLM&#29983;&#25104;&#20869;&#23481;&#30340;&#23433;&#20840;&#24615;&#19981;&#21516;&#65292;&#26412;&#30740;&#31350;&#20851;&#27880;&#35780;&#20272;LLM&#20195;&#29702;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#30340;&#34892;&#20026;&#23433;&#20840;&#24615;&#30340;&#36843;&#20999;&#38656;&#27714;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;R-Judge&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;LLM&#22312;&#32473;&#23450;&#20195;&#29702;&#20132;&#20114;&#35760;&#24405;&#26102;&#21028;&#26029;&#23433;&#20840;&#39118;&#38505;&#30340;&#33021;&#21147;&#12290;R-Judge&#21253;&#25324;162&#20010;&#20195;&#29702;&#20132;&#20114;&#35760;&#24405;&#65292;&#28085;&#30422;7&#20010;&#24212;&#29992;&#39046;&#22495;&#21644;10&#31181;&#39118;&#38505;&#31867;&#22411;&#30340;27&#20010;&#20851;&#38190;&#39118;&#38505;&#22330;&#26223;&#12290;&#23427;&#32467;&#21512;&#20102;&#20154;&#31867;&#23545;&#23433;&#20840;&#24615;&#30340;&#20849;&#35782;&#65292;&#24182;&#20855;&#26377;&#26631;&#35760;&#30340;&#23433;&#20840;&#39118;&#38505;&#26631;&#31614;&#21644;&#39640;&#36136;&#37327;&#30340;&#39118;&#38505;&#25551;&#36848;&#12290;&#21033;&#29992;R-Judge&#65292;&#25105;&#20204;&#23545;8&#31181;&#24120;&#29992;&#20316;&#20195;&#29702;&#39592;&#24178;&#30340;&#33879;&#21517;LLM&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#34920;&#29616;&#26368;&#22909;&#30340;&#27169;&#22411;GPT-4&#23454;&#29616;&#20102;72.29%&#30340;&#23545;&#27604;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have exhibited great potential in autonomously completing tasks across real-world applications. Despite this, these LLM agents introduce unexpected safety risks when operating in interactive environments. Instead of centering on LLM-generated content safety in most prior studies, this work addresses the imperative need for benchmarking the behavioral safety of LLM agents within diverse environments. We introduce R-Judge, a benchmark crafted to evaluate the proficiency of LLMs in judging safety risks given agent interaction records. R-Judge comprises 162 agent interaction records, encompassing 27 key risk scenarios among 7 application categories and 10 risk types. It incorporates human consensus on safety with annotated safety risk labels and high-quality risk descriptions. Utilizing R-Judge, we conduct a comprehensive evaluation of 8 prominent LLMs commonly employed as the backbone for agents. The best-performing model, GPT-4, achieves 72.29% in contrast to
&lt;/p&gt;</description></item><item><title>&#26412;&#31456;&#30740;&#31350;&#20102;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#38382;&#39064;&#65292;&#20171;&#32461;&#20102;&#20256;&#32479;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#26041;&#27861;&#21644;&#29983;&#25104;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#27169;&#22411;&#20013;&#30456;&#20851;&#24037;&#20316;&#12290;&#36890;&#36807;&#22312;&#33521;&#35821;-&#24847;&#22823;&#21033;&#35821;&#30340;&#32763;&#35793;&#29615;&#22659;&#20013;&#20351;&#29992;ChatGPT&#36827;&#34892;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#20854;&#35299;&#20915;&#24615;&#21035;&#20559;&#35265;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#20943;&#23569;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#20559;&#35265;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10016</link><description>&lt;p&gt;
&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#19982;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;
&lt;/p&gt;
&lt;p&gt;
Gender Bias in Machine Translation and The Era of Large Language Models. (arXiv:2401.10016v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31456;&#30740;&#31350;&#20102;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#38382;&#39064;&#65292;&#20171;&#32461;&#20102;&#20256;&#32479;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#26041;&#27861;&#21644;&#29983;&#25104;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#27169;&#22411;&#20013;&#30456;&#20851;&#24037;&#20316;&#12290;&#36890;&#36807;&#22312;&#33521;&#35821;-&#24847;&#22823;&#21033;&#35821;&#30340;&#32763;&#35793;&#29615;&#22659;&#20013;&#20351;&#29992;ChatGPT&#36827;&#34892;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#20854;&#35299;&#20915;&#24615;&#21035;&#20559;&#35265;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#20943;&#23569;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#20559;&#35265;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#31456;&#25506;&#35752;&#20102;&#26426;&#22120;&#32763;&#35793;&#22312;&#24310;&#32493;&#24615;&#21035;&#20559;&#35265;&#26041;&#38754;&#30340;&#20316;&#29992;&#65292;&#30528;&#37325;&#24378;&#35843;&#20102;&#36328;&#35821;&#35328;&#29615;&#22659;&#21644;&#32479;&#35745;&#20381;&#36182;&#24615;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#25552;&#20379;&#20102;&#20851;&#20110;&#24615;&#21035;&#20559;&#35265;&#22312;&#20256;&#32479;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#26041;&#27861;&#21644;&#20316;&#20026;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#29983;&#25104;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#27169;&#22411;&#20013;&#30340;&#30456;&#20851;&#29616;&#26377;&#24037;&#20316;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#36890;&#36807;&#22312;&#33521;&#35821;-&#24847;&#22823;&#21033;&#35821;&#32763;&#35793;&#29615;&#22659;&#20013;&#20351;&#29992;ChatGPT (&#22522;&#20110;GPT-3.5)&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35780;&#20272;&#20102;ChatGPT&#35299;&#20915;&#24615;&#21035;&#20559;&#35265;&#30340;&#24403;&#21069;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#22312;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#20013;&#20943;&#23569;&#20559;&#35265;&#30340;&#25345;&#32493;&#38656;&#27714;&#65292;&#24182;&#24378;&#35843;&#20102;&#20419;&#36827;&#35821;&#35328;&#25216;&#26415;&#30340;&#20844;&#24179;&#24615;&#21644;&#21253;&#23481;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This chapter examines the role of Machine Translation in perpetuating gender bias, highlighting the challenges posed by cross-linguistic settings and statistical dependencies. A comprehensive overview of relevant existing work related to gender bias in both conventional Neural Machine Translation approaches and Generative Pretrained Transformer models employed as Machine Translation systems is provided. Through an experiment using ChatGPT (based on GPT-3.5) in an English-Italian translation context, we further assess ChatGPT's current capacity to address gender bias. The findings emphasize the ongoing need for advancements in mitigating bias in Machine Translation systems and underscore the importance of fostering fairness and inclusivity in language technologies.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;A-KIT&#65292;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;Kalman-informed transformer&#65292;&#29992;&#20110;&#22312;&#32447;&#23398;&#20064;&#20256;&#24863;&#22120;&#34701;&#21512;&#20013;&#21464;&#21270;&#30340;&#36807;&#31243;&#22122;&#22768;&#21327;&#26041;&#24046;&#12290;&#23427;&#36890;&#36807;&#36866;&#24212;&#23454;&#38469;&#24773;&#20917;&#20013;&#30340;&#36807;&#31243;&#22122;&#22768;&#21464;&#21270;&#65292;&#25913;&#36827;&#20102;&#20272;&#35745;&#29366;&#24577;&#30340;&#20934;&#30830;&#24615;&#65292;&#36991;&#20813;&#20102;&#28388;&#27874;&#22120;&#21457;&#25955;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.09987</link><description>&lt;p&gt;
A-KIT:&#33258;&#36866;&#24212;Kalman-Informed Transformer
&lt;/p&gt;
&lt;p&gt;
A-KIT: Adaptive Kalman-Informed Transformer. (arXiv:2401.09987v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09987
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;A-KIT&#65292;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;Kalman-informed transformer&#65292;&#29992;&#20110;&#22312;&#32447;&#23398;&#20064;&#20256;&#24863;&#22120;&#34701;&#21512;&#20013;&#21464;&#21270;&#30340;&#36807;&#31243;&#22122;&#22768;&#21327;&#26041;&#24046;&#12290;&#23427;&#36890;&#36807;&#36866;&#24212;&#23454;&#38469;&#24773;&#20917;&#20013;&#30340;&#36807;&#31243;&#22122;&#22768;&#21464;&#21270;&#65292;&#25913;&#36827;&#20102;&#20272;&#35745;&#29366;&#24577;&#30340;&#20934;&#30830;&#24615;&#65292;&#36991;&#20813;&#20102;&#28388;&#27874;&#22120;&#21457;&#25955;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#23637;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;(EKF)&#26159;&#23548;&#33322;&#24212;&#29992;&#20013;&#24191;&#27867;&#37319;&#29992;&#30340;&#20256;&#24863;&#22120;&#34701;&#21512;&#26041;&#27861;&#12290;EKF&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#26159;&#22312;&#32447;&#30830;&#23450;&#21453;&#26144;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#36807;&#31243;&#22122;&#22768;&#21327;&#26041;&#24046;&#30697;&#38453;&#12290;&#23613;&#31649;&#24120;&#35265;&#30340;EKF&#23454;&#29616;&#20551;&#35774;&#36807;&#31243;&#22122;&#22768;&#26159;&#24658;&#23450;&#30340;&#65292;&#20294;&#22312;&#23454;&#38469;&#24773;&#20917;&#20013;&#65292;&#36807;&#31243;&#22122;&#22768;&#26159;&#21464;&#21270;&#30340;&#65292;&#23548;&#33268;&#20272;&#35745;&#29366;&#24577;&#30340;&#19981;&#20934;&#30830;&#65292;&#24182;&#21487;&#33021;&#23548;&#33268;&#28388;&#27874;&#22120;&#21457;&#25955;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#31181;&#24773;&#20917;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#33258;&#36866;&#24212;EKF&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#24615;&#33021;&#25913;&#36827;&#65292;&#20984;&#26174;&#20102;&#23545;&#31283;&#20581;&#33258;&#36866;&#24212;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25512;&#23548;&#24182;&#24341;&#20837;&#20102;A-KIT&#65292;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;Kalman-informed transformer&#65292;&#29992;&#20110;&#22312;&#32447;&#23398;&#20064;&#21464;&#21270;&#30340;&#36807;&#31243;&#22122;&#22768;&#21327;&#26041;&#24046;&#12290;A-KIT&#26694;&#26550;&#36866;&#29992;&#20110;&#20219;&#20309;&#31867;&#22411;&#30340;&#20256;&#24863;&#22120;&#34701;&#21512;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#20171;&#32461;&#20102;&#22522;&#20110;&#24815;&#24615;&#23548;&#33322;&#31995;&#32479;&#21644;&#22810;&#26222;&#21202;&#36895;&#24230;&#26085;&#24535;&#30340;&#38750;&#32447;&#24615;&#20256;&#24863;&#22120;&#34701;&#21512;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#33258;&#20027;&#26080;&#20154;&#28508;&#27700;&#22120;&#30340;&#30495;&#23454;&#35760;&#24405;&#25968;&#25454;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;A-KIT&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The extended Kalman filter (EKF) is a widely adopted method for sensor fusion in navigation applications. A crucial aspect of the EKF is the online determination of the process noise covariance matrix reflecting the model uncertainty. While common EKF implementation assumes a constant process noise, in real-world scenarios, the process noise varies, leading to inaccuracies in the estimated state and potentially causing the filter to diverge. To cope with such situations, model-based adaptive EKF methods were proposed and demonstrated performance improvements, highlighting the need for a robust adaptive approach. In this paper, we derive and introduce A-KIT, an adaptive Kalman-informed transformer to learn the varying process noise covariance online. The A-KIT framework is applicable to any type of sensor fusion. Here, we present our approach to nonlinear sensor fusion based on an inertial navigation system and Doppler velocity log. By employing real recorded data from an autonomous und
&lt;/p&gt;</description></item><item><title>FLex&amp;Chill &#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;Logit Chilling&#26041;&#27861;&#25913;&#36827;&#26412;&#22320;&#32852;&#21512;&#23398;&#20064;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21152;&#24555;&#27169;&#22411;&#25910;&#25947;&#24182;&#25552;&#39640;&#25512;&#29702;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.09986</link><description>&lt;p&gt;
FLex&amp;Chill&#65306;&#36890;&#36807;Logit Chilling&#25913;&#36827;&#26412;&#22320;&#32852;&#21512;&#23398;&#20064;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
FLex&amp;Chill: Improving Local Federated Learning Training with Logit Chilling. (arXiv:2401.09986v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09986
&lt;/p&gt;
&lt;p&gt;
FLex&amp;Chill &#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;Logit Chilling&#26041;&#27861;&#25913;&#36827;&#26412;&#22320;&#32852;&#21512;&#23398;&#20064;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21152;&#24555;&#27169;&#22411;&#25910;&#25947;&#24182;&#25552;&#39640;&#25512;&#29702;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#21512;&#23398;&#20064;&#30001;&#20110;&#26412;&#22320;&#23458;&#25143;&#31471;&#30340;&#38750;iid&#20998;&#24067;&#24335;&#35757;&#32451;&#25968;&#25454;&#32780;&#21463;&#21040;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#38459;&#30861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#21512;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;FLex&amp;Chill&#65292;&#21033;&#29992;&#20102;Logit Chilling&#26041;&#27861;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#32852;&#21512;&#23398;&#20064;&#31995;&#32479;&#20013;&#22266;&#26377;&#30340;&#38750;iid&#25968;&#25454;&#29305;&#24449;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21152;&#24555;&#27169;&#22411;&#25910;&#25947;&#24182;&#25552;&#39640;&#25512;&#29702;&#31934;&#24230;&#12290;&#20174;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20840;&#23616;&#32852;&#21512;&#23398;&#20064;&#27169;&#22411;&#25910;&#25947;&#26102;&#38388;&#25552;&#39640;&#20102;6&#20493;&#65292;&#25512;&#29702;&#31934;&#24230;&#25552;&#39640;&#20102;3.37%&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning are inherently hampered by data heterogeneity: non-iid distributed training data over local clients. We propose a novel model training approach for federated learning, FLex&amp;Chill, which exploits the Logit Chilling method. Through extensive evaluations, we demonstrate that, in the presence of non-iid data characteristics inherent in federated learning systems, this approach can expedite model convergence and improve inference accuracy. Quantitatively, from our experiments, we observe up to 6X improvement in the global federated learning model convergence time, and up to 3.37% improvement in inference accuracy.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#38024;&#23545;&#20248;&#21270;&#22522;&#30784;&#35774;&#26045;&#21363;&#20195;&#30721;&#37096;&#32626;&#37197;&#32622;&#30340;&#22810;&#30446;&#26631;&#38382;&#39064;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;IOP&#30340;&#26368;&#20339;&#22810;&#30446;&#26631;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.09983</link><description>&lt;p&gt;
&#29992;&#20110;&#23547;&#25214;&#22522;&#30784;&#35774;&#26045;&#21363;&#20195;&#30721;&#37096;&#32626;&#37197;&#32622;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Multiobjective Optimization Analysis for Finding Infrastructure-as-Code Deployment Configurations. (arXiv:2401.09983v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09983
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#38024;&#23545;&#20248;&#21270;&#22522;&#30784;&#35774;&#26045;&#21363;&#20195;&#30721;&#37096;&#32626;&#37197;&#32622;&#30340;&#22810;&#30446;&#26631;&#38382;&#39064;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;IOP&#30340;&#26368;&#20339;&#22810;&#30446;&#26631;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#20248;&#21270;&#26159;&#20154;&#24037;&#26234;&#33021;&#21644;&#36816;&#31609;&#23398;&#39046;&#22495;&#30340;&#28909;&#38376;&#35805;&#39064;&#12290;&#22810;&#30446;&#26631;&#26041;&#27861;&#30340;&#35774;&#35745;&#21644;&#24320;&#21457;&#26159;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#32463;&#24120;&#38656;&#35201;&#22788;&#29702;&#30340;&#20219;&#21153;&#12290;&#30001;&#20110;&#27963;&#36291;&#30340;&#30740;&#31350;&#27963;&#21160;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#25216;&#26415;&#65292;&#23545;&#26469;&#33258;&#21508;&#31181;&#23454;&#38469;&#39046;&#22495;&#30340;&#24773;&#20917;&#20855;&#26377;&#26174;&#33879;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#25991;&#20851;&#27880;&#20110;&#20248;&#21270;&#22522;&#30784;&#35774;&#26045;&#21363;&#20195;&#30721;&#37096;&#32626;&#37197;&#32622;&#30340;&#22810;&#30446;&#26631;&#38382;&#39064;&#12290;&#35299;&#20915;&#27492;&#38382;&#39064;&#30340;&#31995;&#32479;&#34987;&#31216;&#20026;&#8220;IaC&#20248;&#21270;&#24179;&#21488;&#65288;IOP&#65289;&#8221;&#12290;&#23613;&#31649;&#22312;&#25991;&#29486;&#20013;&#24050;&#32463;&#20171;&#32461;&#20102;IOP&#30340;&#21407;&#22411;&#29256;&#26412;&#65292;&#20294;&#38656;&#35201;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#20998;&#26512;&#65292;&#20197;&#30830;&#23450;&#26368;&#36866;&#21512;&#23884;&#20837;IOP&#30340;&#22810;&#30446;&#26631;&#26041;&#27861;&#12290;&#26412;&#25991;&#20013;&#36827;&#34892;&#30340;&#20998;&#26512;&#30340;&#20027;&#35201;&#21160;&#26426;&#26159;&#30830;&#23450;&#26368;&#21512;&#36866;&#30340;&#22810;&#30446;&#26631;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiobjective optimization is a hot topic in the artificial intelligence and operations research communities. The design and development of multiobjective methods is a frequent task for researchers and practitioners. As a result of this vibrant activity, a myriad of techniques have been proposed in the literature to date, demonstrating a significant effectiveness for dealing with situations coming from a wide range of real-world areas. This paper is focused on a multiobjective problem related to optimizing Infrastructure-as-Code deployment configurations. The system implemented for solving this problem has been coined as IaC Optimizer Platform (IOP). Despite the fact that a prototypical version of the IOP has been introduced in the literature before, a deeper analysis focused on the resolution of the problem is needed, in order to determine which is the most appropriate multiobjective method for embedding in the IOP. The main motivation behind the analysis conducted in this work is to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#65288;RAISE&#65289;&#65292;&#36890;&#36807;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#36827;&#34892;&#35268;&#21017;&#25277;&#35937;&#21644;&#36873;&#25321;&#65292;&#20197;&#35299;&#20915;Raven&#30340;&#28176;&#36827;&#30697;&#38453;&#38382;&#39064;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#29616;&#23454;&#30340;&#22330;&#26223;&#20013;&#23637;&#31034;&#20986;&#25277;&#35937;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.09966</link><description>&lt;p&gt;
&#26397;&#21521;&#29983;&#25104;&#24335;&#25277;&#35937;&#25512;&#29702;&#65306;&#36890;&#36807;&#35268;&#21017;&#25277;&#35937;&#21644;&#36873;&#25321;&#26469;&#23436;&#25104;Raven&#30340;&#28176;&#36827;&#30697;&#38453;
&lt;/p&gt;
&lt;p&gt;
Towards Generative Abstract Reasoning: Completing Raven's Progressive Matrix via Rule Abstraction and Selection. (arXiv:2401.09966v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09966
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#65288;RAISE&#65289;&#65292;&#36890;&#36807;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#36827;&#34892;&#35268;&#21017;&#25277;&#35937;&#21644;&#36873;&#25321;&#65292;&#20197;&#35299;&#20915;Raven&#30340;&#28176;&#36827;&#30697;&#38453;&#38382;&#39064;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#29616;&#23454;&#30340;&#22330;&#26223;&#20013;&#23637;&#31034;&#20986;&#25277;&#35937;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#36171;&#20104;&#26426;&#22120;&#25277;&#35937;&#25512;&#29702;&#33021;&#21147;&#26159;&#19968;&#20010;&#38271;&#26399;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;Raven&#30340;&#28176;&#36827;&#30697;&#38453;&#65288;RPM&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#25506;&#32034;&#26426;&#22120;&#26234;&#33021;&#20013;&#30340;&#25277;&#35937;&#35270;&#35273;&#25512;&#29702;&#65292;&#27169;&#22411;&#38656;&#35201;&#29702;&#35299;&#28508;&#22312;&#30340;&#35268;&#21017;&#24182;&#20174;&#20505;&#36873;&#38598;&#20013;&#36873;&#25321;&#32570;&#22833;&#30340;&#21491;&#19979;&#22270;&#20687;&#26469;&#23436;&#25104;&#22270;&#20687;&#30697;&#38453;&#12290;&#21442;&#19982;&#32773;&#21487;&#20197;&#36890;&#36807;&#25512;&#26029;&#28508;&#22312;&#30340;&#23646;&#24615;&#21464;&#21270;&#35268;&#21017;&#21644;&#24819;&#35937;&#20219;&#24847;&#20301;&#32622;&#30340;&#32570;&#22833;&#22270;&#20687;&#23637;&#31034;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#24456;&#38590;&#22312;&#29616;&#23454;&#30340;RPM&#38382;&#39064;&#20013;&#23637;&#31034;&#20986;&#36825;&#31181;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#35268;&#21017;&#25277;&#35937;&#21644;&#36873;&#25321;&#65288;RAISE&#65289;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#35299;&#20915;&#31572;&#26696;&#29983;&#25104;&#38382;&#39064;&#12290;RAISE&#23558;&#22270;&#20687;&#23646;&#24615;&#32534;&#30721;&#20026;&#28508;&#22312;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#27010;&#24565;&#23558;&#28508;&#22312;&#35268;&#21017;&#20998;&#35299;&#25104;&#21407;&#23376;&#35268;&#21017;&#65292;&#36825;&#20123;&#21407;&#23376;&#35268;&#21017;&#34987;&#25277;&#35937;&#20026;&#20840;&#23616;&#21487;&#23398;&#20064;&#30340;&#21442;&#25968;&#12290;&#22312;&#29983;&#25104;&#31572;&#26696;&#26102;&#65292;RAISE&#36873;&#25321;...
&lt;/p&gt;
&lt;p&gt;
Endowing machines with abstract reasoning ability has been a long-term research topic in artificial intelligence. Raven's Progressive Matrix (RPM) is widely used to probe abstract visual reasoning in machine intelligence, where models need to understand the underlying rules and select the missing bottom-right images out of candidate sets to complete image matrices. The participators can display powerful reasoning ability by inferring the underlying attribute-changing rules and imagining the missing images at arbitrary positions. However, existing solvers can hardly manifest such an ability in realistic RPM problems. In this paper, we propose a conditional generative model to solve answer generation problems through Rule AbstractIon and SElection (RAISE) in the latent space. RAISE encodes image attributes as latent concepts and decomposes underlying rules into atomic rules by means of concepts, which are abstracted as global learnable parameters. When generating the answer, RAISE select
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20195;&#30721;&#34917;&#20840;&#39046;&#22495;&#20013;&#21160;&#24577;&#25512;&#26029;&#30340;&#24212;&#29992;&#65292;&#22312;&#23545;GPT-2&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#30340;&#22522;&#30784;&#19978;&#21457;&#29616;&#65292;&#20165;&#20351;&#29992;&#31532;&#19968;&#23618;&#21363;&#21487;&#20934;&#30830;&#29983;&#25104;&#36229;&#36807;&#19968;&#21322;&#30340;&#26631;&#35760;&#65292;&#20174;&#32780;&#33410;&#30465;&#20102;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#20351;&#29992;&#25152;&#26377;&#23618;&#20173;&#28982;&#26080;&#27861;&#23436;&#20840;&#27491;&#30830;&#39044;&#27979;&#19968;&#37096;&#20998;&#26631;&#35760;&#12290;</title><link>http://arxiv.org/abs/2401.09964</link><description>&lt;p&gt;
&#24403;&#31070;&#32463;&#20195;&#30721;&#34917;&#20840;&#27169;&#22411;&#35843;&#25972;&#24773;&#20917;&#26102;&#65306;&#36890;&#36807;&#21160;&#24577;&#27169;&#22411;&#25512;&#26029;&#23454;&#29616;&#26356;&#20415;&#23452;&#26356;&#24555;&#30340;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
When Neural Code Completion Models Size up the Situation: Attaining Cheaper and Faster Completion through Dynamic Model Inference. (arXiv:2401.09964v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20195;&#30721;&#34917;&#20840;&#39046;&#22495;&#20013;&#21160;&#24577;&#25512;&#26029;&#30340;&#24212;&#29992;&#65292;&#22312;&#23545;GPT-2&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#30340;&#22522;&#30784;&#19978;&#21457;&#29616;&#65292;&#20165;&#20351;&#29992;&#31532;&#19968;&#23618;&#21363;&#21487;&#20934;&#30830;&#29983;&#25104;&#36229;&#36807;&#19968;&#21322;&#30340;&#26631;&#35760;&#65292;&#20174;&#32780;&#33410;&#30465;&#20102;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#20351;&#29992;&#25152;&#26377;&#23618;&#20173;&#28982;&#26080;&#27861;&#23436;&#20840;&#27491;&#30830;&#39044;&#27979;&#19968;&#37096;&#20998;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#29616;&#20195;&#31070;&#32463;&#20195;&#30721;&#34917;&#20840;&#27169;&#22411;&#23637;&#31034;&#20102;&#29983;&#25104;&#39640;&#24230;&#20934;&#30830;&#20195;&#30721;&#24314;&#35758;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24040;&#22823;&#30340;&#35268;&#27169;&#22312;&#35745;&#31639;&#25104;&#26412;&#21644;&#29615;&#22659;&#24433;&#21709;&#26041;&#38754;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#21160;&#24577;&#25512;&#26029;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#20998;&#37197;&#26368;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22312;&#20195;&#30721;&#34917;&#20840;&#30340;&#32972;&#26223;&#19979;&#25506;&#32034;&#20102;&#21160;&#24577;&#25512;&#26029;&#12290;&#26368;&#21021;&#65292;&#25105;&#20204;&#23545;GPT-2&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#37325;&#28857;&#20851;&#27880;&#20013;&#38388;&#23618;&#22312;&#20195;&#30721;&#34917;&#20840;&#20013;&#30340;&#25512;&#26029;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20165;&#20351;&#29992;&#31532;&#19968;&#23618;&#23601;&#21487;&#20197;&#20934;&#30830;&#29983;&#25104;54.4%&#30340;&#26631;&#35760;&#65292;&#34920;&#26126;&#23384;&#22312;&#26174;&#33879;&#30340;&#35745;&#31639;&#33410;&#30465;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#20351;&#29992;&#20102;&#25152;&#26377;&#23618;&#65292;&#27169;&#22411;&#20173;&#28982;&#26080;&#27861;&#27491;&#30830;&#39044;&#27979;14.5%&#30340;&#26631;&#35760;&#65292;&#24182;&#19988;&#20174;&#21518;&#32493;&#34917;&#20840;&#20013;&#20063;&#27809;&#26377;&#24471;&#21040;&#27491;&#30830;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Leveraging recent advancements in large language models, modern neural code completion models have demonstrated the capability to generate highly accurate code suggestions. However, their massive size poses challenges in terms of computational costs and environmental impact, hindering their widespread adoption in practical scenarios. Dynamic inference emerges as a promising solution, as it allocates minimal computation during inference while maintaining the model's performance. In this research, we explore dynamic inference within the context of code completion. Initially, we conducted an empirical investigation on GPT-2, focusing on the inference capabilities of intermediate layers for code completion. We found that 54.4% of tokens can be accurately generated using just the first layer, signifying significant computational savings potential. Moreover, despite using all layers, the model still fails to predict 14.5% of tokens correctly, and the subsequent completions continued from the
&lt;/p&gt;</description></item><item><title>WindSeer&#26159;&#19968;&#20010;&#21517;&#20026;WindSeer&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#22312;&#23454;&#26102;&#39044;&#27979;&#20302;&#31354;&#39118;&#30340;&#21516;&#26102;&#33410;&#30465;&#35745;&#31639;&#36164;&#28304;&#12290;&#36890;&#36807;&#20351;&#29992;&#31232;&#30095;&#30340;&#27979;&#37327;&#25968;&#25454;&#21644;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#23427;&#21487;&#20197;&#25104;&#21151;&#22320;&#39044;&#27979;&#24050;&#30693;&#22320;&#24418;&#19978;&#30340;&#30495;&#23454;&#39118;&#22330;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#20998;&#36776;&#29575;&#21644;&#22495;&#22823;&#23567;&#19978;&#29983;&#25104;&#20934;&#30830;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2401.09944</link><description>&lt;p&gt;
WindSeer: &#22312;&#23567;&#22411;&#26080;&#20154;&#26426;&#19978;&#23454;&#26102;&#39044;&#27979;&#22797;&#26434;&#22320;&#24418;&#19978;&#30340;&#20307;&#31215;&#39118;
&lt;/p&gt;
&lt;p&gt;
WindSeer: Real-time volumetric wind prediction over complex terrain aboard a small UAV. (arXiv:2401.09944v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09944
&lt;/p&gt;
&lt;p&gt;
WindSeer&#26159;&#19968;&#20010;&#21517;&#20026;WindSeer&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#22312;&#23454;&#26102;&#39044;&#27979;&#20302;&#31354;&#39118;&#30340;&#21516;&#26102;&#33410;&#30465;&#35745;&#31639;&#36164;&#28304;&#12290;&#36890;&#36807;&#20351;&#29992;&#31232;&#30095;&#30340;&#27979;&#37327;&#25968;&#25454;&#21644;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#23427;&#21487;&#20197;&#25104;&#21151;&#22320;&#39044;&#27979;&#24050;&#30693;&#22320;&#24418;&#19978;&#30340;&#30495;&#23454;&#39118;&#22330;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#20998;&#36776;&#29575;&#21644;&#22495;&#22823;&#23567;&#19978;&#29983;&#25104;&#20934;&#30830;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#26102;&#39640;&#20998;&#36776;&#29575;&#30340;&#39118;&#39044;&#27979;&#23545;&#20110;&#21253;&#25324;&#26377;&#20154;&#21644;&#26080;&#20154;&#33322;&#31354;&#22312;&#20869;&#30340;&#21508;&#31181;&#24212;&#29992;&#37117;&#24456;&#26377;&#30410;&#12290;&#24403;&#21069;&#30340;&#22825;&#27668;&#27169;&#22411;&#38656;&#35201;&#22826;&#22810;&#30340;&#35745;&#31639;&#65292;&#24182;&#19988;&#32570;&#20047;&#24517;&#35201;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#20165;&#22312;&#22810;&#21315;&#31859;&#21644;&#20960;&#23567;&#26102;&#30340;&#23610;&#24230;&#19978;&#26377;&#25928; - &#36825;&#36828;&#20302;&#20110;&#36825;&#20123;&#24212;&#29992;&#25152;&#38656;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20998;&#36776;&#29575;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#39318;&#27425;&#23637;&#31034;&#20102;&#22312;&#26377;&#38480;&#35745;&#31639;&#35774;&#22791;&#19978;&#23454;&#26102;&#39044;&#27979;&#20302;&#31354;&#39118;&#30340;&#33021;&#21147;&#65292;&#20165;&#20351;&#29992;&#31232;&#30095;&#30340;&#27979;&#37327;&#25968;&#25454;&#35757;&#32451;&#20102;&#21517;&#20026;WindSeer&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#20351;&#29992;&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;&#27169;&#25311;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#23637;&#31034;&#23427;&#21487;&#20197;&#25104;&#21151;&#22320;&#20174;&#20165;&#26377;&#23569;&#37327;&#30340;&#22122;&#22768;&#21644;&#31354;&#38388;&#32858;&#38598;&#30340;&#39118;&#27979;&#37327;&#25968;&#25454;&#20013;&#39044;&#27979;&#20986;&#24050;&#30693;&#22320;&#24418;&#19978;&#30340;&#30495;&#23454;&#39118;&#22330;&#12290;WindSeer&#21487;&#20197;&#22312;&#20043;&#21069;&#26410;&#35265;&#36807;&#30340;&#22320;&#24418;&#19978;&#20197;&#19981;&#21516;&#30340;&#20998;&#36776;&#29575;&#21644;&#22495;&#22823;&#23567;&#29983;&#25104;&#20934;&#30830;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#25104;&#21151;&#39044;&#27979;&#20102;&#21382;&#21490;&#39118;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time high-resolution wind predictions are beneficial for various applications including safe manned and unmanned aviation. Current weather models require too much compute and lack the necessary predictive capabilities as they are valid only at the scale of multiple kilometers and hours - much lower spatial and temporal resolutions than these applications require. Our work, for the first time, demonstrates the ability to predict low-altitude wind in real-time on limited-compute devices, from only sparse measurement data. We train a neural network, WindSeer, using only synthetic data from computational fluid dynamics simulations and show that it can successfully predict real wind fields over terrain with known topography from just a few noisy and spatially clustered wind measurements. WindSeer can generate accurate predictions at different resolutions and domain sizes on previously unseen topography without retraining. We demonstrate that the model successfully predicts historical w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#32852;&#21512;&#35782;&#21035;&#12289;&#22242;&#38431;&#24402;&#23646;&#21644;&#35282;&#33394;&#20998;&#31867;&#30340;&#36816;&#21160;&#35270;&#35273;&#36319;&#36394;&#12290;&#36890;&#36807;&#20351;&#29992;&#20849;&#20139;&#30340;&#20027;&#24178;&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#19978;&#26356;&#39640;&#25928;&#65292;&#24182;&#19988;&#33021;&#22815;&#20135;&#29983;&#26356;&#20016;&#23500;&#21644;&#26377;&#21306;&#21035;&#24230;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2401.09942</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#29992;&#20110;&#32852;&#21512;&#35782;&#21035;&#12289;&#22242;&#38431;&#24402;&#23646;&#21644;&#35282;&#33394;&#20998;&#31867;&#30340;&#36816;&#21160;&#35270;&#35273;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
Multi-task Learning for Joint Re-identification, Team Affiliation, and Role Classification for Sports Visual Tracking. (arXiv:2401.09942v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09942
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#32852;&#21512;&#35782;&#21035;&#12289;&#22242;&#38431;&#24402;&#23646;&#21644;&#35282;&#33394;&#20998;&#31867;&#30340;&#36816;&#21160;&#35270;&#35273;&#36319;&#36394;&#12290;&#36890;&#36807;&#20351;&#29992;&#20849;&#20139;&#30340;&#20027;&#24178;&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#19978;&#26356;&#39640;&#25928;&#65292;&#24182;&#19988;&#33021;&#22815;&#20135;&#29983;&#26356;&#20016;&#23500;&#21644;&#26377;&#21306;&#21035;&#24230;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20998;&#26512;&#36275;&#29699;&#35270;&#39057;&#26469;&#35828;&#65292;&#26377;&#25928;&#30340;&#29699;&#21592;&#36319;&#36394;&#21644;&#37325;&#26032;&#35782;&#21035;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29699;&#21592;&#30340;&#38750;&#32447;&#24615;&#36816;&#21160;&#12289;&#26469;&#33258;&#21516;&#19968;&#22242;&#38431;&#30340;&#29699;&#21592;&#22806;&#35266;&#30340;&#30456;&#20284;&#24615;&#20197;&#21450;&#39057;&#32321;&#30340;&#36974;&#25377;&#65292;&#36825;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#23884;&#20837;&#34920;&#31034;&#20197;&#20195;&#34920;&#29699;&#21592;&#23545;&#20110;&#24320;&#21457;&#26377;&#25928;&#30340;&#36319;&#36394;&#21644;&#37325;&#26032;&#35782;&#21035;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PRTreID&#30340;&#22810;&#29992;&#36884;&#22522;&#20110;&#37096;&#20214;&#30340;&#20154;&#29289;&#34920;&#31034;&#26041;&#27861;&#65292;&#23427;&#21516;&#26102;&#25191;&#34892;&#35282;&#33394;&#20998;&#31867;&#12289;&#22242;&#38431;&#24402;&#23646;&#21644;&#37325;&#26032;&#35782;&#21035;&#36825;&#19977;&#20010;&#20219;&#21153;&#12290;&#19982;&#29616;&#26377;&#25991;&#29486;&#19981;&#21516;&#30340;&#26159;&#65292;&#20351;&#29992;&#22810;&#20219;&#21153;&#30417;&#30563;&#35757;&#32451;&#20102;&#19968;&#20010;&#21333;&#19968;&#32593;&#32476;&#26469;&#35299;&#20915;&#25152;&#26377;&#19977;&#20010;&#20219;&#21153;&#12290;&#30001;&#20110;&#20849;&#20139;&#30340;&#20027;&#24178;&#32593;&#32476;&#65292;&#25152;&#25552;&#20986;&#30340;&#32852;&#21512;&#26041;&#27861;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#27492;&#22806;&#65292;&#22810;&#20219;&#21153;&#23398;&#20064;&#23548;&#33268;&#20102;&#26356;&#20016;&#23500;&#21644;&#26356;&#26377;&#21306;&#21035;&#24230;&#30340;&#34920;&#31034;&#65292;&#36825;&#36890;&#36807;&#23450;&#37327;&#21644;&#23450;&#24615;&#32467;&#26524;&#37117;&#24471;&#21040;&#20102;&#35777;&#23454;&#12290;&#20026;&#20102;&#23637;&#31034;
&lt;/p&gt;
&lt;p&gt;
Effective tracking and re-identification of players is essential for analyzing soccer videos. But, it is a challenging task due to the non-linear motion of players, the similarity in appearance of players from the same team, and frequent occlusions. Therefore, the ability to extract meaningful embeddings to represent players is crucial in developing an effective tracking and re-identification system. In this paper, a multi-purpose part-based person representation method, called PRTreID, is proposed that performs three tasks of role classification, team affiliation, and re-identification, simultaneously. In contrast to available literature, a single network is trained with multi-task supervision to solve all three tasks, jointly. The proposed joint method is computationally efficient due to the shared backbone. Also, the multi-task learning leads to richer and more discriminative representations, as demonstrated by both quantitative and qualitative results. To demonstrate the effectiven
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;XAI&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;CAM-based&#35299;&#37322;&#26469;&#25913;&#36827;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#65292;&#20174;&#32780;&#22686;&#24378;&#35270;&#35273;&#36136;&#37327;&#26816;&#27979;&#31995;&#32479;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;XAI&#22686;&#24378;&#30340;&#27169;&#22411;&#22312;&#22797;&#26434;&#23545;&#35937;&#20998;&#21106;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2401.09900</link><description>&lt;p&gt;
&#22522;&#20110;XAI&#30340;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#29992;&#20110;&#35270;&#35273;&#36136;&#37327;&#26816;&#27979;&#30340;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
XAI-Enhanced Semantic Segmentation Models for Visual Quality Inspection. (arXiv:2401.09900v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;XAI&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;CAM-based&#35299;&#37322;&#26469;&#25913;&#36827;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#65292;&#20174;&#32780;&#22686;&#24378;&#35270;&#35273;&#36136;&#37327;&#26816;&#27979;&#31995;&#32479;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;XAI&#22686;&#24378;&#30340;&#27169;&#22411;&#22312;&#22797;&#26434;&#23545;&#35937;&#20998;&#21106;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#36136;&#37327;&#26816;&#27979;&#31995;&#32479;&#22312;&#21046;&#36896;&#19994;&#21644;&#29289;&#27969;&#31561;&#39046;&#22495;&#33267;&#20851;&#37325;&#35201;&#65292;&#37319;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#31934;&#30830;&#12289;&#24555;&#36895;&#30340;&#32570;&#38519;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#30340;&#26080;&#35299;&#37322;&#24615;&#21487;&#33021;&#38459;&#30861;&#20102;&#20449;&#20219;&#12289;&#38169;&#35823;&#35782;&#21035;&#21644;&#31995;&#32479;&#25913;&#36827;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;CAM&#30340;&#35299;&#37322;&#26469;&#25913;&#36827;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#65292;&#20174;&#32780;&#22686;&#24378;&#35270;&#35273;&#36136;&#37327;&#26816;&#27979;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#65306;1&#65289;&#27169;&#22411;&#35757;&#32451;&#65292;2&#65289;&#22522;&#20110;XAI&#30340;&#27169;&#22411;&#35299;&#37322;&#65292;3&#65289;XAI&#35780;&#20272;&#65292;&#20197;&#21450;4&#65289;&#27880;&#37322;&#22686;&#24378;&#29992;&#20110;&#27169;&#22411;&#25913;&#36827;&#65292;&#36825;&#20123;&#37117;&#21463;&#21040;&#35299;&#37322;&#21644;&#19987;&#23478;&#35265;&#35299;&#30340;&#25351;&#23548;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;XAI&#22686;&#24378;&#30340;&#27169;&#22411;&#29305;&#21035;&#22312;&#22797;&#26434;&#23545;&#35937;&#20998;&#21106;&#26041;&#38754;&#36229;&#36807;&#20102;&#21407;&#22987;&#30340;DeepLabv3-ResNet101&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual quality inspection systems, crucial in sectors like manufacturing and logistics, employ computer vision and machine learning for precise, rapid defect detection. However, their unexplained nature can hinder trust, error identification, and system improvement. This paper presents a framework to bolster visual quality inspection by using CAM-based explanations to refine semantic segmentation models. Our approach consists of 1) Model Training, 2) XAI-based Model Explanation, 3) XAI Evaluation, and 4) Annotation Augmentation for Model Enhancement, informed by explanations and expert insights. Evaluations show XAI-enhanced models surpass original DeepLabv3-ResNet101 models, especially in intricate object segmentation.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24377;&#24615;&#32852;&#37030;&#21644;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21512;&#20316;&#36793;&#32536;&#32531;&#23384;&#26041;&#26696;&#65292;&#36890;&#36807;&#35757;&#32451;&#20010;&#24615;&#21270;&#30340;&#26412;&#22320;&#27169;&#22411;&#65292;&#39044;&#27979;&#20934;&#30830;&#21463;&#27426;&#36814;&#30340;&#20869;&#23481;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;SBS&#20043;&#38388;&#21512;&#20316;&#32531;&#23384;&#28909;&#38376;&#20869;&#23481;&#65292;&#20197;&#36798;&#21040;&#20248;&#21270;&#33719;&#21462;&#20869;&#23481;&#25104;&#26412;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2401.09886</link><description>&lt;p&gt;
&#22522;&#20110;&#24377;&#24615;&#32852;&#37030;&#21644;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#19979;&#19968;&#20195;&#32593;&#32476;&#21512;&#20316;&#36793;&#32536;&#32531;&#23384;
&lt;/p&gt;
&lt;p&gt;
Cooperative Edge Caching Based on Elastic Federated and Multi-Agent Deep Reinforcement Learning in Next-Generation Network. (arXiv:2401.09886v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24377;&#24615;&#32852;&#37030;&#21644;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21512;&#20316;&#36793;&#32536;&#32531;&#23384;&#26041;&#26696;&#65292;&#36890;&#36807;&#35757;&#32451;&#20010;&#24615;&#21270;&#30340;&#26412;&#22320;&#27169;&#22411;&#65292;&#39044;&#27979;&#20934;&#30830;&#21463;&#27426;&#36814;&#30340;&#20869;&#23481;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;SBS&#20043;&#38388;&#21512;&#20316;&#32531;&#23384;&#28909;&#38376;&#20869;&#23481;&#65292;&#20197;&#36798;&#21040;&#20248;&#21270;&#33719;&#21462;&#20869;&#23481;&#25104;&#26412;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36793;&#32536;&#32531;&#23384;&#26159;&#19979;&#19968;&#20195;&#32593;&#32476;&#20013;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#36171;&#20104;&#23567;&#22411;&#22522;&#31449;&#65288;SBS&#65289;&#20013;&#30340;&#32531;&#23384;&#21333;&#20803;&#36171;&#33021;&#65292;&#20801;&#35768;&#29992;&#25143;&#35774;&#22791;&#65288;UE&#65289;&#33719;&#21462;&#24050;&#22312;SBS&#20013;&#39044;&#32531;&#23384;&#30340;&#29992;&#25143;&#35831;&#27714;&#20869;&#23481;&#12290;&#23545;&#20110;SBS&#26469;&#35828;&#65292;&#36890;&#36807;&#23398;&#20064;&#20934;&#30830;&#39044;&#27979;&#21463;&#27426;&#36814;&#30340;&#20869;&#23481;&#38750;&#24120;&#20851;&#38190;&#65292;&#21516;&#26102;&#20445;&#25252;&#29992;&#25143;&#20010;&#20154;&#20449;&#24687;&#12290;&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#21487;&#20197;&#20445;&#25252;&#29992;&#25143;&#30340;&#38544;&#31169;&#65292;&#20294;&#26159;UE&#20043;&#38388;&#30340;&#25968;&#25454;&#24046;&#24322;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#36136;&#37327;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#20026;&#27599;&#20010;UE&#35757;&#32451;&#20010;&#24615;&#21270;&#30340;&#26412;&#22320;&#27169;&#22411;&#20197;&#20934;&#30830;&#39044;&#27979;&#21463;&#27426;&#36814;&#30340;&#20869;&#23481;&#12290;&#27492;&#22806;&#65292;&#19979;&#19968;&#20195;&#32593;&#32476;&#20013;&#30456;&#37051;SBS&#20043;&#38388;&#21487;&#20197;&#20849;&#20139;&#32531;&#23384;&#30340;&#20869;&#23481;&#65292;&#22240;&#27492;&#22312;&#19981;&#21516;&#30340;SBS&#20013;&#32531;&#23384;&#39044;&#27979;&#21040;&#30340;&#28909;&#38376;&#20869;&#23481;&#21487;&#33021;&#20250;&#24433;&#21709;&#33719;&#21462;&#20869;&#23481;&#30340;&#25104;&#26412;&#12290;&#22240;&#27492;&#65292;&#30830;&#23450;&#21512;&#20316;&#32531;&#23384;&#28909;&#38376;&#20869;&#23481;&#30340;&#20301;&#32622;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24377;&#24615;&#32852;&#37030;&#21644;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21512;&#20316;&#36793;&#32536;&#32531;&#23384;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Edge caching is a promising solution for next-generation networks by empowering caching units in small-cell base stations (SBSs), which allows user equipments (UEs) to fetch users' requested contents that have been pre-cached in SBSs. It is crucial for SBSs to predict accurate popular contents through learning while protecting users' personal information. Traditional federated learning (FL) can protect users' privacy but the data discrepancies among UEs can lead to a degradation in model quality. Therefore, it is necessary to train personalized local models for each UE to predict popular contents accurately. In addition, the cached contents can be shared among adjacent SBSs in next-generation networks, thus caching predicted popular contents in different SBSs may affect the cost to fetch contents. Hence, it is critical to determine where the popular contents are cached cooperatively. To address these issues, we propose a cooperative edge caching scheme based on elastic federated and mu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#33258;&#21160;&#35782;&#21035;&#19979;&#34507;&#40481;&#34892;&#20026;&#12290;&#36890;&#36807;&#22768;&#38899;&#20998;&#26512;&#21644;&#29305;&#24449;&#25552;&#21462;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#40065;&#26834;&#30340;&#34892;&#20026;&#29305;&#24449;&#21270;&#31995;&#32479;&#65292;&#23545;&#19979;&#34507;&#40481;&#30340;&#20581;&#24247;&#34892;&#20026;&#36827;&#34892;&#30417;&#27979;&#21644;&#35782;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#20855;&#26377;&#33391;&#22909;&#30340;&#32508;&#21512;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.09880</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23545;&#33258;&#21160;&#34892;&#20026;&#19979;&#34507;&#40481;&#35782;&#21035;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Attention-Based Recurrent Neural Network For Automatic Behavior Laying Hen Recognition. (arXiv:2401.09880v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09880
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#33258;&#21160;&#35782;&#21035;&#19979;&#34507;&#40481;&#34892;&#20026;&#12290;&#36890;&#36807;&#22768;&#38899;&#20998;&#26512;&#21644;&#29305;&#24449;&#25552;&#21462;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#40065;&#26834;&#30340;&#34892;&#20026;&#29305;&#24449;&#21270;&#31995;&#32479;&#65292;&#23545;&#19979;&#34507;&#40481;&#30340;&#20581;&#24247;&#34892;&#20026;&#36827;&#34892;&#30417;&#27979;&#21644;&#35782;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#20855;&#26377;&#33391;&#22909;&#30340;&#32508;&#21512;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#20859;&#31165;&#19994;&#30340;&#19968;&#20010;&#20851;&#27880;&#28857;&#26159;&#19979;&#34507;&#40481;&#30340;&#40483;&#21483;&#22768;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#20851;&#20110;&#20581;&#24247;&#34892;&#20026;&#30340;&#38750;&#24120;&#26377;&#29992;&#30340;&#20449;&#24687;&#12290;&#36825;&#20123;&#20449;&#24687;&#34987;&#29992;&#20316;&#20581;&#24247;&#21644;&#31119;&#31049;&#30340;&#25351;&#26631;&#65292;&#24110;&#21161;&#20859;&#27542;&#20154;&#21592;&#26356;&#22909;&#22320;&#30417;&#27979;&#19979;&#34507;&#40481;&#65292;&#20174;&#32780;&#21450;&#26089;&#21457;&#29616;&#38382;&#39064;&#65292;&#20197;&#20415;&#36827;&#34892;&#26356;&#24555;&#21644;&#26356;&#26377;&#25928;&#30340;&#24178;&#39044;&#12290;&#26412;&#30740;&#31350;&#19987;&#27880;&#20110;&#23545;&#19979;&#34507;&#40481;&#40483;&#21483;&#31867;&#22411;&#30340;&#22768;&#38899;&#20998;&#26512;&#65292;&#20197;&#25552;&#20986;&#19968;&#31181;&#40065;&#26834;&#30340;&#34892;&#20026;&#29305;&#24449;&#21270;&#31995;&#32479;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#30417;&#27979;&#19979;&#34507;&#40481;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#25910;&#38598;&#24182;&#27880;&#37322;&#20102;&#19979;&#34507;&#40481;&#30340;&#40483;&#21483;&#20449;&#21495;&#65292;&#28982;&#21518;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#26102;&#38388;&#21644;&#39057;&#29575;&#22495;&#29305;&#24449;&#32452;&#21512;&#30340;&#26368;&#20339;&#22768;&#23398;&#29305;&#24449;&#21270;&#26041;&#27861;&#12290;&#28982;&#21518;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#29305;&#24449;&#26500;&#24314;&#20102;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#27169;&#22411;&#65292;&#23558;&#35821;&#20041;&#31867;&#21035;&#20998;&#37197;&#32473;&#25551;&#36848;&#19979;&#34507;&#40481;&#34892;&#20026;&#30340;&#40483;&#21483;&#22768;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#32508;&#21512;&#24615;&#33021;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the interests of modern poultry farming is the vocalization of laying hens which contain very useful information on health behavior. This information is used as health and well-being indicators that help breeders better monitor laying hens, which involves early detection of problems for rapid and more effective intervention. In this work, we focus on the sound analysis for the recognition of the types of calls of the laying hens in order to propose a robust system of characterization of their behavior for a better monitoring. To do this, we first collected and annotated laying hen call signals, then designed an optimal acoustic characterization based on the combination of time and frequency domain features. We then used these features to build the multi-label classification models based on recurrent neural network to assign a semantic class to the vocalization that characterize the laying hen behavior. The results show an overall performance with our model based on the combinati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#19977;&#23618;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#24341;&#20837;&#20102;&#31354;&#38388;&#21644;&#26102;&#38388;&#30446;&#26631;&#25277;&#35937;&#21270;&#12290;&#30740;&#31350;&#32773;&#25552;&#20379;&#20102;&#23398;&#20064;&#31574;&#30053;&#30340;&#29702;&#35770;&#36951;&#25022;&#36793;&#30028;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#23545;&#31639;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2401.09870</link><description>&lt;p&gt;
&#35843;&#21644;&#31354;&#38388;&#21644;&#26102;&#38388;&#25277;&#35937;&#21270;&#20197;&#23454;&#29616;&#30446;&#26631;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Reconciling Spatial and Temporal Abstractions for Goal Representation. (arXiv:2401.09870v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#19977;&#23618;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#24341;&#20837;&#20102;&#31354;&#38388;&#21644;&#26102;&#38388;&#30446;&#26631;&#25277;&#35937;&#21270;&#12290;&#30740;&#31350;&#32773;&#25552;&#20379;&#20102;&#23398;&#20064;&#31574;&#30053;&#30340;&#29702;&#35770;&#36951;&#25022;&#36793;&#30028;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#23545;&#31639;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#34920;&#31034;&#36890;&#36807;&#23558;&#22797;&#26434;&#30340;&#23398;&#20064;&#38382;&#39064;&#20998;&#35299;&#20026;&#26356;&#23481;&#26131;&#30340;&#23376;&#20219;&#21153;&#26469;&#24433;&#21709;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20445;&#30041;&#26102;&#38388;&#25277;&#35937;&#29615;&#22659;&#21160;&#24577;&#30340;&#34920;&#31034;&#26041;&#27861;&#22312;&#35299;&#20915;&#22256;&#38590;&#38382;&#39064;&#21644;&#25552;&#20379;&#20248;&#21270;&#29702;&#35770;&#20445;&#35777;&#26041;&#38754;&#26159;&#25104;&#21151;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#29615;&#22659;&#21160;&#24577;&#36234;&#26469;&#36234;&#22797;&#26434;&#65288;&#21363;&#26102;&#38388;&#25277;&#35937;&#36716;&#25442;&#20851;&#31995;&#20381;&#36182;&#26356;&#22810;&#21464;&#37327;&#65289;&#30340;&#20219;&#21153;&#20013;&#26080;&#27861;&#25193;&#23637;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20854;&#20182;&#26041;&#27861;&#21017;&#23581;&#35797;&#20351;&#29992;&#31354;&#38388;&#25277;&#35937;&#26469;&#32531;&#35299;&#21069;&#38754;&#30340;&#38382;&#39064;&#12290;&#23427;&#20204;&#30340;&#38480;&#21046;&#21253;&#25324;&#26080;&#27861;&#36866;&#24212;&#39640;&#32500;&#29615;&#22659;&#21644;&#23545;&#20808;&#21069;&#30693;&#35782;&#30340;&#20381;&#36182;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19977;&#23618;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20998;&#23618;&#32467;&#26500;&#30340;&#19981;&#21516;&#23618;&#27425;&#24341;&#20837;&#20102;&#31354;&#38388;&#21644;&#26102;&#38388;&#30446;&#26631;&#25277;&#35937;&#21270;&#12290;&#25105;&#20204;&#23545;&#23398;&#20064;&#31574;&#30053;&#30340;&#36951;&#25022;&#36793;&#30028;&#36827;&#34892;&#20102;&#29702;&#35770;&#30740;&#31350;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Goal representation affects the performance of Hierarchical Reinforcement Learning (HRL) algorithms by decomposing the complex learning problem into easier subtasks. Recent studies show that representations that preserve temporally abstract environment dynamics are successful in solving difficult problems and provide theoretical guarantees for optimality. These methods however cannot scale to tasks where environment dynamics increase in complexity i.e. the temporally abstract transition relations depend on larger number of variables. On the other hand, other efforts have tried to use spatial abstraction to mitigate the previous issues. Their limitations include scalability to high dimensional environments and dependency on prior knowledge.  In this paper, we propose a novel three-layer HRL algorithm that introduces, at different levels of the hierarchy, both a spatial and a temporal goal abstraction. We provide a theoretical study of the regret bounds of the learned policies. We evalua
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;SPARC&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22270;&#20687;-&#25991;&#26412;&#23545;&#20013;&#23398;&#20064;&#27599;&#20010;&#20196;&#29260;&#30340;&#22270;&#20687;&#32452;&#21512;&#65292;&#20197;&#25552;&#39640;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#20013;&#30340;&#32454;&#31890;&#24230;&#29702;&#35299;&#33021;&#21147;&#12290;SPARC&#26041;&#27861;&#32467;&#21512;&#20102;&#32454;&#31890;&#24230;&#25439;&#22833;&#21644;&#23545;&#27604;&#25439;&#22833;&#65292;&#21487;&#20197;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#23398;&#20064;&#21516;&#26102;&#32534;&#30721;&#20840;&#23616;&#21644;&#23616;&#37096;&#20449;&#24687;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2401.09865</link><description>&lt;p&gt;
&#25552;&#39640;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#20013;&#32454;&#31890;&#24230;&#29702;&#35299;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving fine-grained understanding in image-text pre-training. (arXiv:2401.09865v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;SPARC&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22270;&#20687;-&#25991;&#26412;&#23545;&#20013;&#23398;&#20064;&#27599;&#20010;&#20196;&#29260;&#30340;&#22270;&#20687;&#32452;&#21512;&#65292;&#20197;&#25552;&#39640;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#20013;&#30340;&#32454;&#31890;&#24230;&#29702;&#35299;&#33021;&#21147;&#12290;SPARC&#26041;&#27861;&#32467;&#21512;&#20102;&#32454;&#31890;&#24230;&#25439;&#22833;&#21644;&#23545;&#27604;&#25439;&#22833;&#65292;&#21487;&#20197;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#23398;&#20064;&#21516;&#26102;&#32534;&#30721;&#20840;&#23616;&#21644;&#23616;&#37096;&#20449;&#24687;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;SPARC&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#22270;&#20687;-&#25991;&#26412;&#23545;&#20013;&#39044;&#35757;&#32451;&#26356;&#32454;&#31890;&#24230;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#12290;&#32771;&#34385;&#21040;&#22810;&#20010;&#22270;&#20687;&#22359;&#36890;&#24120;&#23545;&#24212;&#20110;&#21333;&#20010;&#21333;&#35789;&#65292;&#25105;&#20204;&#25552;&#20986;&#20026;&#27599;&#20010;&#23383;&#24149;&#20196;&#29260;&#23398;&#20064;&#19968;&#32452;&#22270;&#20687;&#22359;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#22270;&#20687;&#22359;&#21644;&#35821;&#35328;&#20196;&#29260;&#20043;&#38388;&#30340;&#31232;&#30095;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#24182;&#35745;&#31639;&#20986;&#27599;&#20010;&#20196;&#29260;&#30340;&#35821;&#35328;&#20998;&#32452;&#30340;&#35270;&#35273;&#23884;&#20837;&#65292;&#20316;&#20026;&#22270;&#20687;&#22359;&#30340;&#21152;&#26435;&#24179;&#22343;&#20540;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#19968;&#31181;&#20165;&#20381;&#36182;&#20110;&#21333;&#20010;&#26679;&#26412;&#32780;&#19981;&#38656;&#35201;&#20854;&#20182;&#25209;&#27425;&#26679;&#26412;&#20316;&#20026;&#36127;&#26679;&#26412;&#30340;&#32454;&#31890;&#24230;&#24207;&#21015;&#25439;&#22833;&#65292;&#23545;&#20196;&#29260;&#21644;&#35821;&#35328;&#20998;&#32452;&#30340;&#35270;&#35273;&#23884;&#20837;&#36827;&#34892;&#23545;&#27604;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#23398;&#20064;&#26356;&#35814;&#32454;&#30340;&#20449;&#24687;&#12290;SPARC&#23558;&#36825;&#31181;&#32454;&#31890;&#24230;&#25439;&#22833;&#19982;&#20840;&#23616;&#22270;&#20687;&#21644;&#25991;&#26412;&#23884;&#20837;&#20043;&#38388;&#30340;&#23545;&#27604;&#25439;&#22833;&#30456;&#32467;&#21512;&#65292;&#20197;&#21516;&#26102;&#32534;&#30721;&#20840;&#23616;&#21644;&#23616;&#37096;&#20449;&#24687;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce SPARse Fine-grained Contrastive Alignment (SPARC), a simple method for pretraining more fine-grained multimodal representations from image-text pairs. Given that multiple image patches often correspond to single words, we propose to learn a grouping of image patches for every token in the caption. To achieve this, we use a sparse similarity metric between image patches and language tokens and compute for each token a language-grouped vision embedding as the weighted average of patches. The token and language-grouped vision embeddings are then contrasted through a fine-grained sequence-wise loss that only depends on individual samples and does not require other batch samples as negatives. This enables more detailed information to be learned in a computationally inexpensive manner. SPARC combines this fine-grained loss with a contrastive loss between global image and text embeddings to learn representations that simultaneously encode global and local information. We thorough
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#20248;&#21270;&#30340;&#36827;&#21270;&#22810;&#30446;&#26631;&#26041;&#27861;&#65292;&#36890;&#36807;&#24773;&#24863;&#20998;&#26512;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#23454;&#29616;&#20102;&#29983;&#25104;&#33021;&#22815;&#21516;&#26102;&#20307;&#29616;&#20004;&#31181;&#30456;&#20114;&#20914;&#31361;&#24773;&#24863;&#30340;&#25552;&#31034;&#35821;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#30456;&#20851;&#20449;&#24687;&#30340;&#25552;&#21462;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.09862</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#30340;&#36827;&#21270;&#22810;&#30446;&#26631;&#20248;&#21270;&#20197;&#24179;&#34913;&#24773;&#24863;
&lt;/p&gt;
&lt;p&gt;
Evolutionary Multi-Objective Optimization of Large Language Model Prompts for Balancing Sentiments. (arXiv:2401.09862v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#20248;&#21270;&#30340;&#36827;&#21270;&#22810;&#30446;&#26631;&#26041;&#27861;&#65292;&#36890;&#36807;&#24773;&#24863;&#20998;&#26512;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#23454;&#29616;&#20102;&#29983;&#25104;&#33021;&#22815;&#21516;&#26102;&#20307;&#29616;&#20004;&#31181;&#30456;&#20114;&#20914;&#31361;&#24773;&#24863;&#30340;&#25552;&#31034;&#35821;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#30456;&#20851;&#20449;&#24687;&#30340;&#25552;&#21462;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#30340;&#20986;&#29616;&#24341;&#36215;&#20102;&#21508;&#20010;&#39046;&#22495;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#24615;&#33021;&#21644;&#22810;&#21151;&#33021;&#24615;&#38750;&#20961;&#12290;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#30340;&#20351;&#29992;&#19981;&#26029;&#22686;&#38271;&#65292;&#26377;&#25928;&#30340;&#25552;&#31034;&#24037;&#31243;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#25552;&#31034;&#20248;&#21270;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#30452;&#25509;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#21644;&#30456;&#20851;&#20449;&#24687;&#30340;&#25552;&#21462;&#12290;&#26368;&#36817;&#65292;&#36827;&#21270;&#31639;&#27861;&#65288;EAs&#65289;&#22312;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#24076;&#26395;&#65292;&#20026;&#26032;&#30340;&#20248;&#21270;&#31574;&#30053;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#21035;&#38024;&#23545;&#25552;&#31034;&#20248;&#21270;&#30340;&#36827;&#21270;&#22810;&#30446;&#26631;&#65288;EMO&#65289;&#26041;&#27861;&#65292;&#31216;&#20026;EMO-Prompts&#65292;&#20197;&#24773;&#24863;&#20998;&#26512;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#25105;&#20204;&#23558;&#24773;&#24863;&#20998;&#26512;&#33021;&#21147;&#20316;&#20026;&#25105;&#20204;&#30340;&#23454;&#39564;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;EMO-Prompts&#33021;&#22815;&#26377;&#25928;&#22320;&#29983;&#25104;&#25552;&#31034;&#65292;&#20351;LLM&#33021;&#22815;&#21516;&#26102;&#20135;&#29983;&#20307;&#29616;&#20004;&#31181;&#30456;&#20114;&#20914;&#31361;&#24773;&#24863;&#30340;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of large language models (LLMs) such as ChatGPT has attracted considerable attention in various domains due to their remarkable performance and versatility. As the use of these models continues to grow, the importance of effective prompt engineering has come to the fore. Prompt optimization emerges as a crucial challenge, as it has a direct impact on model performance and the extraction of relevant information. Recently, evolutionary algorithms (EAs) have shown promise in addressing this issue, paving the way for novel optimization strategies. In this work, we propose a evolutionary multi-objective (EMO) approach specifically tailored for prompt optimization called EMO-Prompts, using sentiment analysis as a case study. We use sentiment analysis capabilities as our experimental targets. Our results demonstrate that EMO-Prompts effectively generates prompts capable of guiding the LLM to produce texts embodying two conflicting emotions simultaneously.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#35270;&#39057;&#20869;&#23481;&#20013;&#25552;&#21462;&#26102;&#38388;&#29305;&#23450;&#30340;&#20449;&#24687;&#65292;&#26469;&#35299;&#20915;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20107;&#20214;&#32423;&#24187;&#35273;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.09861</link><description>&lt;p&gt;
&#26102;&#38388;&#27934;&#23519;&#22686;&#24378;&#65306;&#20943;&#36731;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26102;&#38388;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Temporal Insight Enhancement: Mitigating Temporal Hallucination in Multimodal Large Language Models. (arXiv:2401.09861v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09861
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#35270;&#39057;&#20869;&#23481;&#20013;&#25552;&#21462;&#26102;&#38388;&#29305;&#23450;&#30340;&#20449;&#24687;&#65292;&#26469;&#35299;&#20915;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20107;&#20214;&#32423;&#24187;&#35273;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(MLLMs)&#30340;&#21457;&#23637;&#26174;&#33879;&#22686;&#24378;&#20102;&#23545;&#22810;&#23186;&#20307;&#20869;&#23481;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#23558;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#35270;&#39057;&#31561;&#22810;&#31181;&#27169;&#24577;&#38598;&#21512;&#22312;&#19968;&#36215;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#38754;&#20020;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#22312;&#22788;&#29702;&#35270;&#39057;&#36755;&#20837;&#26102;&#65292;&#26159;&#20135;&#29983;&#24187;&#35273; - &#38169;&#35823;&#30340;&#24863;&#30693;&#25110;&#35299;&#37322;&#65292;&#29305;&#21035;&#26159;&#22312;&#20107;&#20214;&#23618;&#38754;&#19978;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;MLLMs&#20013;&#30340;&#20107;&#20214;&#32423;&#24187;&#35273;&#38382;&#39064;&#65292;&#37325;&#28857;&#20851;&#27880;&#35270;&#39057;&#20869;&#23481;&#30340;&#26102;&#38388;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#20174;&#20107;&#20214;&#26597;&#35810;&#21644;&#25552;&#20379;&#30340;&#35270;&#39057;&#20013;&#25552;&#21462;&#24182;&#21033;&#29992;&#20107;&#20214;&#29305;&#23450;&#20449;&#24687;&#26469;&#20248;&#21270;MLLMs&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#26426;&#21046;&#65292;&#23558;&#25353;&#38656;&#30340;&#20107;&#20214;&#26597;&#35810;&#20998;&#35299;&#20026;&#20195;&#34920;&#24615;&#30340;&#34892;&#20026;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#31867;&#20284;CLIP&#21644;BLIP2&#30340;&#27169;&#22411;&#26469;&#39044;&#27979;&#20107;&#20214;&#21457;&#29983;&#30340;&#20855;&#20307;&#26102;&#38388;&#25139;&#12290;&#25105;&#20204;&#20351;&#29992;Charades-STA&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced the comprehension of multimedia content, bringing together diverse modalities such as text, images, and videos. However, a critical challenge faced by these models, especially when processing video inputs, is the occurrence of hallucinations - erroneous perceptions or interpretations, particularly at the event level. This study introduces an innovative method to address event-level hallucinations in MLLMs, focusing on specific temporal understanding in video content. Our approach leverages a novel framework that extracts and utilizes event-specific information from both the event query and the provided video to refine MLLMs' response. We propose a unique mechanism that decomposes on-demand event queries into iconic actions. Subsequently, we employ models like CLIP and BLIP2 to predict specific timestamps for event occurrences. Our evaluation, conducted using the Charades-STA dataset, demonstrate
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#36827;&#34892;&#36793;&#32536;&#25668;&#20687;&#22836;&#27169;&#22411;&#35843;&#35797;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#20559;&#35265;&#38382;&#39064;&#26469;&#25552;&#39640;&#20844;&#24179;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.09852</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#22686;&#24378;&#36793;&#32536;&#25668;&#20687;&#22836;&#30340;&#20844;&#24179;&#24615;&#21644;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Enhancing the Fairness and Performance of Edge Cameras with Explainable AI. (arXiv:2401.09852v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#36827;&#34892;&#36793;&#32536;&#25668;&#20687;&#22836;&#27169;&#22411;&#35843;&#35797;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#20559;&#35265;&#38382;&#39064;&#26469;&#25552;&#39640;&#20844;&#24179;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36793;&#32536;&#25668;&#20687;&#22836;&#31995;&#32479;&#20013;&#65292;&#20154;&#24037;&#26234;&#33021;&#22312;&#20154;&#20307;&#26816;&#27979;&#26041;&#38754;&#30340;&#24212;&#29992;&#26085;&#30410;&#22686;&#22810;&#65292;&#23548;&#33268;&#20102;&#20934;&#30830;&#20294;&#22797;&#26434;&#30340;&#27169;&#22411;&#65292;&#36825;&#20351;&#24471;&#35299;&#37322;&#21644;&#35843;&#35797;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#36827;&#34892;&#27169;&#22411;&#35843;&#35797;&#30340;&#35786;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#19987;&#23478;&#39537;&#21160;&#30340;&#38382;&#39064;&#35782;&#21035;&#21644;&#35299;&#20915;&#26041;&#26696;&#21019;&#24314;&#12290;&#22312;&#23454;&#38469;&#21150;&#20844;&#23460;&#36793;&#32536;&#32593;&#32476;&#20013;&#30340;Bytetrack&#27169;&#22411;&#19978;&#36827;&#34892;&#39564;&#35777;&#65292;&#25105;&#20204;&#21457;&#29616;&#35757;&#32451;&#25968;&#25454;&#38598;&#26159;&#20027;&#35201;&#30340;&#20559;&#35265;&#26469;&#28304;&#65292;&#24182;&#24314;&#35758;&#27169;&#22411;&#22686;&#24378;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#21161;&#20110;&#35782;&#21035;&#27169;&#22411;&#20559;&#35265;&#65292;&#20174;&#32780;&#23454;&#29616;&#20844;&#24179;&#21644;&#21487;&#20449;&#36182;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rising use of Artificial Intelligence (AI) in human detection on Edge camera systems has led to accurate but complex models, challenging to interpret and debug. Our research presents a diagnostic method using Explainable AI (XAI) for model debugging, with expert-driven problem identification and solution creation. Validated on the Bytetrack model in a real-world office Edge network, we found the training dataset as the main bias source and suggested model augmentation as a solution. Our approach helps identify model biases, essential for achieving fair and trustworthy models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20223;&#30495;&#25216;&#26415;&#30340;&#21457;&#23637;&#19982;&#31185;&#23398;&#33539;&#24335;&#30340;&#28436;&#21464;&#65292;&#24182;&#25552;&#20986;&#20102;&#34892;&#20026;&#20223;&#30495;&#30340;&#27010;&#24565;&#65292;&#20195;&#34920;&#20102;&#26356;&#39640;&#31243;&#24230;&#30340;&#33539;&#24335;&#25972;&#21512;&#12290;</title><link>http://arxiv.org/abs/2401.09851</link><description>&lt;p&gt;
&#20223;&#30495;&#34892;&#20026;&#65306;&#25506;&#32034;&#31185;&#23398;&#30340;&#21487;&#33021;&#19979;&#19968;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
Behavioral Simulation: Exploring A Possible Next Paradigm for Science. (arXiv:2401.09851v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20223;&#30495;&#25216;&#26415;&#30340;&#21457;&#23637;&#19982;&#31185;&#23398;&#33539;&#24335;&#30340;&#28436;&#21464;&#65292;&#24182;&#25552;&#20986;&#20102;&#34892;&#20026;&#20223;&#30495;&#30340;&#27010;&#24565;&#65292;&#20195;&#34920;&#20102;&#26356;&#39640;&#31243;&#24230;&#30340;&#33539;&#24335;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20223;&#30495;&#25216;&#26415;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#35768;&#22810;&#31185;&#23398;&#30740;&#31350;&#39046;&#22495;&#65292;&#22914;&#22825;&#27668;&#39044;&#25253;&#12289;&#27969;&#20307;&#21147;&#23398;&#21644;&#29983;&#29289;&#31181;&#32676;&#12290;&#23427;&#26159;&#22788;&#29702;&#22797;&#26434;&#31995;&#32479;&#38382;&#39064;&#30340;&#26368;&#20339;&#24037;&#20855;&#65292;&#22312;&#34920;&#31034;&#31354;&#38388;&#20013;&#26080;&#27861;&#20351;&#29992;&#38381;&#21512;&#24418;&#24335;&#34920;&#36798;&#24335;&#19988;&#30446;&#26631;&#20998;&#24067;&#36807;&#20110;&#22797;&#26434;&#32780;&#26080;&#27861;&#23436;&#20840;&#30001;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34920;&#31034;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20223;&#30495;&#25216;&#26415;&#30340;&#21457;&#23637;&#19982;&#31185;&#23398;&#33539;&#24335;&#26159;&#19968;&#33268;&#30340;&#12290;&#26412;&#25991;&#20174;&#25968;&#25454;&#12289;&#31639;&#27861;&#21644;&#35745;&#31639;&#33021;&#21147;&#30340;&#35282;&#24230;&#24402;&#32435;&#20102;&#31185;&#23398;&#33539;&#24335;&#30340;&#28436;&#21464;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#23558;&#20223;&#30495;&#25216;&#26415;&#20998;&#20026;&#19977;&#20010;&#38454;&#27573;&#65292;&#19982;&#26032;&#33539;&#24335;&#30340;&#20986;&#29616;&#30456;&#36866;&#24212;&#65292;&#24182;&#21457;&#29616;&#20808;&#36827;&#30340;&#20223;&#30495;&#25216;&#26415;&#26159;&#33539;&#24335;&#25972;&#21512;&#30340;&#20856;&#22411;&#23454;&#20363;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#34892;&#20026;&#20223;&#30495;&#65288;BS&#65289;&#30340;&#27010;&#24565;&#65292;&#29305;&#21035;&#26159;&#22797;&#26434;&#34892;&#20026;&#20223;&#30495;&#65288;SBS&#65289;&#65292;&#20195;&#34920;&#20102;&#26356;&#39640;&#31243;&#24230;&#30340;&#33539;&#24335;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulation technologies have been widely utilized in many scientific research fields such as weather forecasting, fluid mechanics and biological populations. It is the best tool to handle problems in complex systems, where closed-form expressions are unavailable and the target distribution in the representation space is too complex to be fully represented by a deep learning (DL) model. We believe that the development of simulation technologies is consistent with scientific paradigms. This paper induces the evolution of scientific paradigms from the perspective of data, algorithms, and computational power. Building upon this perspective, we divide simulation technologies into three stages aligning with the emergence of new paradigms, and find that advanced simulation technologies are typical instances of paradigms integration. Moreover, we propose the concept of behavioral simulation (BS), specifically sophisticated behavioral simulation (SBS), representing a higher degree of paradigms 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20999;&#29255;&#32593;&#32476;&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#36890;&#36807;&#20351;&#29992;&#20302;&#39057;&#36924;&#36817;&#30340;&#26041;&#27861;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#21644;&#19978;&#37319;&#26679;&#65292;&#20174;&#32780;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.09833</link><description>&lt;p&gt;
&#20999;&#29255;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Slicer Networks. (arXiv:2401.09833v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20999;&#29255;&#32593;&#32476;&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#36890;&#36807;&#20351;&#29992;&#20302;&#39057;&#36924;&#36817;&#30340;&#26041;&#27861;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#21644;&#19978;&#37319;&#26679;&#65292;&#20174;&#32780;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#65292;&#25195;&#25551;&#36890;&#24120;&#20250;&#25581;&#31034;&#20986;&#20855;&#26377;&#19981;&#21516;&#23545;&#27604;&#24230;&#20294;&#19968;&#33268;&#30340;&#20869;&#37096;&#24378;&#24230;&#25110;&#32441;&#29702;&#30340;&#29289;&#20307;&#12290;&#36825;&#31181;&#29305;&#28857;&#20351;&#24471;&#21487;&#20197;&#21033;&#29992;&#20302;&#39057;&#36924;&#36817;&#26469;&#36827;&#34892;&#20998;&#21106;&#21644;&#21464;&#24418;&#22330;&#20272;&#35745;&#31561;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#34701;&#20837;&#36825;&#20010;&#27010;&#24565;&#20173;&#28982;&#19981;&#22815;&#28145;&#20837;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20999;&#29255;&#32593;&#32476;&#65292;&#19968;&#31181;&#35774;&#35745;&#29992;&#20110;&#21033;&#29992;&#36825;&#20123;&#29305;&#24615;&#30340;&#26032;&#22411;&#26550;&#26500;&#12290;&#20999;&#29255;&#32593;&#32476;&#21253;&#25324;&#19968;&#20010;&#32534;&#30721;&#22120;&#65292;&#21033;&#29992;&#35270;&#35273;&#21464;&#25442;&#22120;&#31561;&#27169;&#22411;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#65292;&#21644;&#19968;&#20010;&#20351;&#29992;&#21487;&#23398;&#20064;&#21452;&#36793;&#32593;&#26684;&#30340;&#20999;&#29255;&#22120;&#65292;&#36890;&#36807;&#28034;&#25273;-&#27169;&#31946;-&#20999;&#29255;&#30340;&#36807;&#31243;&#31574;&#30053;&#24615;&#22320;&#25913;&#36827;&#21644;&#19978;&#37319;&#26679;&#29305;&#24449;&#22270;&#12290;&#36825;&#24341;&#20837;&#20102;&#19968;&#20010;&#20445;&#30041;&#36793;&#32536;&#30340;&#20302;&#39057;&#36924;&#36817;&#65292;&#26377;&#25928;&#25193;&#22823;&#20102;&#26377;&#25928;&#24863;&#21463;&#37326;&#12290;&#36825;&#31181;&#25913;&#36827;&#19981;&#20165;&#20943;&#23569;&#20102;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#36824;&#25552;&#39640;&#20102;&#25972;&#20307;&#24615;&#33021;&#12290;&#36890;&#36807;&#22312;&#19981;&#21516;&#21307;&#23398;&#25104;&#20687;&#24212;&#29992;&#19978;&#36827;&#34892;&#23454;&#39564;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
In medical imaging, scans often reveal objects with varied contrasts but consistent internal intensities or textures. This characteristic enables the use of low-frequency approximations for tasks such as segmentation and deformation field estimation. Yet, integrating this concept into neural network architectures for medical image analysis remains underexplored. In this paper, we propose the Slicer Network, a novel architecture designed to leverage these traits. Comprising an encoder utilizing models like vision transformers for feature extraction and a slicer employing a learnable bilateral grid, the Slicer Network strategically refines and upsamples feature maps via a splatting-blurring-slicing process. This introduces an edge-preserving low-frequency approximation for the network outcome, effectively enlarging the effective receptive field. The enhancement not only reduces computational complexity but also boosts overall performance. Experiments across different medical imaging appl
&lt;/p&gt;</description></item><item><title>PPNet&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#31471;&#21040;&#31471;&#36817;&#20284;&#26368;&#20248;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#20998;&#20026;&#20004;&#20010;&#23376;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#20004;&#32423;&#32423;&#32852;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27714;&#35299;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;EDaGe-PP&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PPNet&#22312;&#35745;&#31639;&#26102;&#38388;&#21644;&#25104;&#21151;&#29575;&#26041;&#38754;&#27604;&#20854;&#20182;&#26041;&#27861;&#26377;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.09819</link><description>&lt;p&gt;
PPNet: &#19968;&#31181;&#29992;&#20110;&#31471;&#21040;&#31471;&#36817;&#20284;&#26368;&#20248;&#36335;&#24452;&#35268;&#21010;&#30340;&#26032;&#39062;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
PPNet: A Novel Neural Network Structure for End-to-End Near-Optimal Path Planning. (arXiv:2401.09819v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09819
&lt;/p&gt;
&lt;p&gt;
PPNet&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#31471;&#21040;&#31471;&#36817;&#20284;&#26368;&#20248;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#20998;&#20026;&#20004;&#20010;&#23376;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#20004;&#32423;&#32423;&#32852;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27714;&#35299;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;EDaGe-PP&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PPNet&#22312;&#35745;&#31639;&#26102;&#38388;&#21644;&#25104;&#21151;&#29575;&#26041;&#38754;&#27604;&#20854;&#20182;&#26041;&#27861;&#26377;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#36335;&#24452;&#35268;&#21010;&#22120;&#65292;&#22914;&#22522;&#20110;&#37319;&#26679;&#30340;&#36335;&#24452;&#35268;&#21010;&#22120;&#65292;&#22312;&#21021;&#22987;&#35299;&#25935;&#24863;&#24615;&#21644;&#25910;&#25947;&#21040;&#26368;&#20248;&#35299;&#36895;&#24230;&#19978;&#20855;&#26377;&#23616;&#38480;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#22914;&#20855;&#26377;&#26377;&#38480;&#21151;&#29575;/&#29123;&#26009;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#65292;&#22312;&#30701;&#26102;&#38388;&#20869;&#25214;&#21040;&#36817;&#20284;&#26368;&#20248;&#35299;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#23454;&#29616;&#31471;&#21040;&#31471;&#36817;&#20284;&#26368;&#20248;&#36335;&#24452;&#35268;&#21010;&#22120;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#20998;&#20026;&#20004;&#20010;&#23376;&#38382;&#39064;&#65292;&#21363;&#36335;&#24452;&#31354;&#38388;&#20998;&#27573;&#21644;&#32473;&#23450;&#36335;&#24452;&#31354;&#38388;&#20013;&#30340;&#33322;&#28857;&#29983;&#25104;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#36335;&#24452;&#35268;&#21010;&#32593;&#32476;&#65288;PPNet&#65289;&#30340;&#20004;&#32423;&#32423;&#32852;&#31070;&#32463;&#32593;&#32476;&#26469;&#35299;&#20915;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65292;&#36890;&#36807;&#35299;&#20915;&#19978;&#36848;&#23376;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EDaGe-PP&#30340;&#29992;&#20110;&#36335;&#24452;&#35268;&#21010;&#30340;&#39640;&#25928;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;PPNet&#35757;&#32451;&#38598;&#30001;EDaGe-PP&#29983;&#25104;&#30340;&#25104;&#21151;&#29575;&#30456;&#27604;&#20854;&#20182;&#26041;&#27861;&#25552;&#21319;&#20102;$2\times$&#65292;&#24635;&#35745;&#31639;&#26102;&#38388;&#23569;&#20110;1/33&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;PPNet&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The classical path planners, such as sampling-based path planners, have the limitations of sensitivity to the initial solution and slow convergence to the optimal solution. However, finding a near-optimal solution in a short period is challenging in many applications such as the autonomous vehicle with limited power/fuel. To achieve an end-to-end near-optimal path planner, we first divide the path planning problem into two subproblems, which are path's space segmentation and waypoints generation in the given path's space. We further propose a two-level cascade neural network named Path Planning Network (PPNet) to solve the path planning problem by solving the abovementioned subproblems. Moreover, we propose a novel efficient data generation method for path planning named EDaGe-PP. The results show the total computation time is less than 1/33 and the success rate of PPNet trained by the dataset that is generated by EDaGe-PP is about $2 \times$ compared to other methods. We validate PPNe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#40657;&#30418;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#36234;&#29425;&#25915;&#20987;&#25552;&#31034;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#22797;&#26434;&#24615;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#38480;&#21046;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#33258;&#36523;&#65292;&#23558;&#26377;&#23475;&#25552;&#31034;&#37325;&#20889;&#20026;&#38750;&#26377;&#23475;&#34920;&#36798;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;80%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#24182;&#19988;&#21363;&#20351;&#27169;&#22411;&#26356;&#26032;&#65292;&#25928;&#26524;&#20173;&#28982;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2401.09798</link><description>&lt;p&gt;
&#19968;&#31181;&#31616;&#21333;&#30340;&#40657;&#30418;&#26041;&#27861;&#29992;&#20110;&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
All in How You Ask for It: Simple Black-Box Method for Jailbreak Attacks. (arXiv:2401.09798v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#40657;&#30418;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#36234;&#29425;&#25915;&#20987;&#25552;&#31034;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#22797;&#26434;&#24615;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#38480;&#21046;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#33258;&#36523;&#65292;&#23558;&#26377;&#23475;&#25552;&#31034;&#37325;&#20889;&#20026;&#38750;&#26377;&#23475;&#34920;&#36798;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;80%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#24182;&#19988;&#21363;&#20351;&#27169;&#22411;&#26356;&#26032;&#65292;&#25928;&#26524;&#20173;&#28982;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30528;&#8220;&#36234;&#29425;&#8221;&#25361;&#25112;&#65292;&#21363;&#35268;&#36991;&#20445;&#38556;&#25514;&#26045;&#20197;&#20135;&#29983;&#19981;&#31526;&#21512;&#20262;&#29702;&#30340;&#25552;&#31034;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#40657;&#30418;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#29983;&#25104;&#36234;&#29425;&#25552;&#31034;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#39640;&#22797;&#26434;&#24615;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#38480;&#21046;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#33258;&#36523;&#65292;&#36845;&#20195;&#22320;&#23558;&#26377;&#23475;&#25552;&#31034;&#37325;&#20889;&#20026;&#38750;&#26377;&#23475;&#34920;&#36798;&#65292;&#22522;&#20110;&#20551;&#35774;&#35748;&#20026;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#29983;&#25104;&#35268;&#36991;&#20445;&#38556;&#30340;&#34920;&#36798;&#12290;&#36890;&#36807;&#22312;ChatGPT&#65288;GPT-3.5&#21644;GPT-4&#65289;&#21644;Gemini-Pro&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#24179;&#22343;5&#27425;&#36845;&#20195;&#20869;&#23454;&#29616;&#20102;&#36229;&#36807;80%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#24182;&#19988;&#21363;&#20351;&#27169;&#22411;&#26356;&#26032;&#65292;&#25928;&#26524;&#20173;&#28982;&#26377;&#25928;&#12290;&#29983;&#25104;&#30340;&#36234;&#29425;&#25552;&#31034;&#33258;&#28982;&#32780;&#31616;&#32451;&#65292;&#34920;&#26126;&#23427;&#20204;&#36739;&#19981;&#26131;&#34987;&#26816;&#27979;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21019;&#24314;&#26377;&#25928;&#30340;&#36234;&#29425;&#25552;&#31034;&#27604;&#20808;&#21069;&#30740;&#31350;&#35748;&#20026;&#30340;&#35201;&#31616;&#21333;&#65292;&#24182;&#19988;&#40657;&#30418;&#36234;&#29425;&#25915;&#20987;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) like ChatGPT face `jailbreak' challenges, where safeguards are bypassed to produce ethically harmful prompts. This study introduces a simple black-box method to effectively generate jailbreak prompts, overcoming the limitations of high complexity and computational costs associated with existing methods. The proposed technique iteratively rewrites harmful prompts into non-harmful expressions using the target LLM itself, based on the hypothesis that LLMs can directly sample safeguard-bypassing expressions. Demonstrated through experiments with ChatGPT (GPT-3.5 and GPT-4) and Gemini-Pro, this method achieved an attack success rate of over 80% within an average of 5 iterations and remained effective despite model updates. The jailbreak prompts generated were naturally-worded and concise, suggesting they are less detectable. The results indicate that creating effective jailbreak prompts is simpler than previously considered, and black-box jailbreak attacks pose 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;Vision Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#26089;&#26399;&#26816;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#27979;&#35797;&#25968;&#25454;&#39564;&#35777;&#65292;&#35813;&#27169;&#22411;&#22312;&#20934;&#30830;&#24230;&#12289;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#20197;&#21450;F1&#20998;&#25968;&#31561;&#26041;&#38754;&#23637;&#29616;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.09795</link><description>&lt;p&gt;
&#22522;&#20110;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;Vision Transformer&#27169;&#22411;&#22312;&#26089;&#26399;&#26816;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#19978;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Comparative Analysis on Metaheuristic Algorithms Based Vision Transformer Model for Early Detection of Alzheimer's Disease. (arXiv:2401.09795v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;Vision Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#26089;&#26399;&#26816;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#27979;&#35797;&#25968;&#25454;&#39564;&#35777;&#65292;&#35813;&#27169;&#22411;&#22312;&#20934;&#30830;&#24230;&#12289;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#20197;&#21450;F1&#20998;&#25968;&#31561;&#26041;&#38754;&#23637;&#29616;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20123;&#33268;&#21629;&#30340;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#20005;&#37325;&#24433;&#21709;&#20102;&#32769;&#19968;&#20195;&#20154;&#30340;&#29983;&#27963;&#36136;&#37327;&#12290;&#30196;&#21574;&#26159;&#20854;&#20013;&#19968;&#31181;&#30151;&#29366;&#65292;&#22914;&#26524;&#19981;&#22312;&#26089;&#26399;&#38454;&#27573;&#26816;&#27979;&#20986;&#26469;&#65292;&#21487;&#33021;&#20250;&#21457;&#23637;&#25104;&#20005;&#37325;&#30340;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#12290;&#24050;&#32463;&#26377;&#25253;&#36947;&#31216;&#65292;&#20174;&#27491;&#24120;&#38454;&#27573;&#21040;&#36825;&#31181;&#30142;&#30149;&#30340;&#21457;&#23637;&#26159;&#30001;&#20154;&#33041;&#20869;&#20960;&#20010;&#21442;&#25968;&#30340;&#25913;&#21464;&#23548;&#33268;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21019;&#26032;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;ViT&#27169;&#22411;&#65292;&#29992;&#20110;&#35782;&#21035;&#19981;&#21516;&#38454;&#27573;&#30340;&#30196;&#21574;&#30151;&#12290;&#22823;&#37327;&#30340;&#27979;&#35797;&#25968;&#25454;&#34987;&#29992;&#26469;&#39564;&#35777;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#12290;&#36824;&#34920;&#26126;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20934;&#30830;&#24230;&#12289;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#20197;&#21450;F1&#20998;&#25968;&#31561;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A number of life threatening neuro-degenerative disorders had degraded the quality of life for the older generation in particular. Dementia is one such symptom which may lead to a severe condition called Alzheimer's disease if not detected at an early stage. It has been reported that the progression of such disease from a normal stage is due to the change in several parameters inside the human brain. In this paper, an innovative metaheuristic algorithms based ViT model has been proposed for the identification of dementia at different stage. A sizeable number of test data have been utilized for the validation of the proposed scheme. It has also been demonstrated that our model exhibits superior performance in terms of accuracy, precision, recall as well as F1-score.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20135;&#19994;4.0&#22330;&#26223;&#19979;&#22522;&#20110;&#35821;&#20041;&#30340;&#21487;&#35270;&#21270;&#26597;&#35810;&#31995;&#32479;&#30340;&#25552;&#26696;&#65292;&#35813;&#31995;&#32479;&#20801;&#35768;&#39046;&#22495;&#19987;&#23478;&#20197;&#21451;&#22909;&#30340;&#26041;&#24335;&#25506;&#32034;&#21644;&#21487;&#35270;&#21270;&#25968;&#25454;&#12290;&#31995;&#32479;&#30340;&#20027;&#35201;&#21019;&#26032;&#22312;&#20110;&#32467;&#21512;&#20351;&#29992;&#35821;&#20041;&#27880;&#37322;&#30340;&#25429;&#33719;&#25968;&#25454;&#21644;2D&#23450;&#21046;&#25968;&#23383;&#26426;&#22120;&#30340;&#25968;&#23383;&#34920;&#31034;&#24418;&#24335;&#12290;</title><link>http://arxiv.org/abs/2401.09789</link><description>&lt;p&gt;
&#20135;&#19994;4.0&#20013;&#22823;&#25968;&#25454;&#25506;&#32034;&#30340;&#35821;&#20041;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Semantic Approach for Big Data Exploration in Industry 4.0. (arXiv:2401.09789v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09789
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20135;&#19994;4.0&#22330;&#26223;&#19979;&#22522;&#20110;&#35821;&#20041;&#30340;&#21487;&#35270;&#21270;&#26597;&#35810;&#31995;&#32479;&#30340;&#25552;&#26696;&#65292;&#35813;&#31995;&#32479;&#20801;&#35768;&#39046;&#22495;&#19987;&#23478;&#20197;&#21451;&#22909;&#30340;&#26041;&#24335;&#25506;&#32034;&#21644;&#21487;&#35270;&#21270;&#25968;&#25454;&#12290;&#31995;&#32479;&#30340;&#20027;&#35201;&#21019;&#26032;&#22312;&#20110;&#32467;&#21512;&#20351;&#29992;&#35821;&#20041;&#27880;&#37322;&#30340;&#25429;&#33719;&#25968;&#25454;&#21644;2D&#23450;&#21046;&#25968;&#23383;&#26426;&#22120;&#30340;&#25968;&#23383;&#34920;&#31034;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#12289;&#29289;&#32852;&#32593;&#12289;&#22823;&#25968;&#25454;&#21644;&#20113;&#35745;&#31639;&#25216;&#26415;&#30340;&#19981;&#26029;&#21457;&#23637;&#23548;&#33268;&#20102;&#31532;&#22235;&#27425;&#24037;&#19994;&#38761;&#21629;&#65288;&#20135;&#19994;4.0&#65289;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#21644;&#35782;&#21035;&#27169;&#24335;&#21644;&#27934;&#35265;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#25968;&#25454;&#24182;&#25913;&#36827;&#21046;&#36896;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21046;&#36896;&#19994;&#19987;&#23478;&#26469;&#35828;&#65292;&#25968;&#25454;&#25506;&#32034;&#30340;&#20219;&#21153;&#24120;&#24120;&#24456;&#22256;&#38590;&#65292;&#22240;&#20026;&#20182;&#20204;&#21487;&#33021;&#23545;&#26410;&#20986;&#29616;&#22312;&#39044;&#35774;&#35745;&#21487;&#35270;&#21270;&#20013;&#30340;&#25968;&#25454;&#20063;&#24863;&#20852;&#36259;&#65292;&#22240;&#27492;&#20182;&#20204;&#38656;&#35201;&#20449;&#24687;&#25216;&#26415;&#19987;&#23478;&#30340;&#24110;&#21161;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#30495;&#23454;&#20135;&#19994;4.0&#22330;&#26223;&#20013;&#24320;&#21457;&#30340;&#22522;&#20110;&#35821;&#20041;&#30340;&#21487;&#35270;&#21270;&#26597;&#35810;&#31995;&#32479;&#30340;&#25552;&#26696;&#65292;&#35813;&#31995;&#32479;&#20801;&#35768;&#39046;&#22495;&#19987;&#23478;&#20197;&#21451;&#22909;&#30340;&#26041;&#24335;&#25506;&#32034;&#21644;&#21487;&#35270;&#21270;&#25968;&#25454;&#12290;&#35813;&#31995;&#32479;&#30340;&#20027;&#35201;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#23427;&#39318;&#20808;&#23545;&#25429;&#33719;&#30340;&#25968;&#25454;&#36827;&#34892;&#35821;&#20041;&#27880;&#37322;&#65292;&#24182;&#19982;&#19968;&#20010;2D&#23450;&#21046;&#25968;&#23383;&#26426;&#22120;&#30340;&#25968;&#23383;&#34920;&#31034;&#24418;&#24335;&#36827;&#34892;&#20851;&#32852;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing trends in automation, Internet of Things, big data and cloud computing technologies have led to the fourth industrial revolution (Industry 4.0), where it is possible to visualize and identify patterns and insights, which results in a better understanding of the data and can improve the manufacturing process. However, many times, the task of data exploration results difficult for manufacturing experts because they might be interested in analyzing also data that does not appear in pre-designed visualizations and therefore they must be assisted by Information Technology experts. In this paper, we present a proposal materialized in a semantic-based visual query system developed for a real Industry 4.0 scenario that allows domain experts to explore and visualize data in a friendly way. The main novelty of the system is the combined use that it makes of captured data that are semantically annotated first, and a 2D customized digital representation of a machine that is also linked
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#65292;&#21363;&#26368;&#23567;&#19981;&#19968;&#33268;&#24230;&#37327;&#65288;LDM&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22797;&#26434;&#20915;&#31574;&#36793;&#30028;&#24773;&#20917;&#19979;&#30340;&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#12290;&#36890;&#36807;&#26597;&#35810;&#20855;&#26377;&#26368;&#23567;LDM&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#21487;&#20197;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.09787</link><description>&lt;p&gt;
&#26597;&#35810;&#26131;&#20110;&#32763;&#36716;&#26679;&#26412;&#30340;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Querying Easily Flip-flopped Samples for Deep Active Learning. (arXiv:2401.09787v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#65292;&#21363;&#26368;&#23567;&#19981;&#19968;&#33268;&#24230;&#37327;&#65288;LDM&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22797;&#26434;&#20915;&#31574;&#36793;&#30028;&#24773;&#20917;&#19979;&#30340;&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#12290;&#36890;&#36807;&#26597;&#35810;&#20855;&#26377;&#26368;&#23567;LDM&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#21487;&#20197;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#26088;&#22312;&#36890;&#36807;&#36873;&#25321;&#21644;&#26597;&#35810;&#26410;&#26631;&#35760;&#25968;&#25454;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#19968;&#31181;&#26377;&#25928;&#30340;&#36873;&#25321;&#31574;&#30053;&#26159;&#22522;&#20110;&#27169;&#22411;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#36825;&#21487;&#20197;&#35299;&#37322;&#20026;&#26679;&#26412;&#30340;&#20449;&#24687;&#37327;&#24230;&#37327;&#12290;&#26679;&#26412;&#21040;&#20915;&#31574;&#36793;&#30028;&#30340;&#36317;&#31163;&#26159;&#19968;&#31181;&#33258;&#28982;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#65292;&#20294;&#36890;&#24120;&#38590;&#20197;&#35745;&#31639;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#22810;&#31867;&#20998;&#31867;&#20219;&#21153;&#20013;&#24418;&#25104;&#30340;&#22797;&#26434;&#20915;&#31574;&#36793;&#30028;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#26368;&#23567;&#19981;&#19968;&#33268;&#24230;&#37327;&#8221;&#65288;LDM&#65289;&#65292;&#23450;&#20041;&#20026;&#39044;&#27979;&#26631;&#31614;&#19981;&#19968;&#33268;&#30340;&#26368;&#23567;&#27010;&#29575;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;LDM&#30340;&#20272;&#35745;&#22120;&#22312;&#28201;&#21644;&#20551;&#35774;&#19979;&#26159;&#28176;&#36817;&#19968;&#33268;&#30340;&#12290;&#35813;&#20272;&#35745;&#22120;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#21442;&#25968;&#25200;&#21160;&#36731;&#26494;&#23454;&#29616;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#20351;&#29992;&#12290;&#22522;&#20110;LDM&#30340;&#20027;&#21160;&#23398;&#20064;&#36890;&#36807;&#26597;&#35810;&#20855;&#26377;&#26368;&#23567;LDM&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#26469;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active learning is a machine learning paradigm that aims to improve the performance of a model by strategically selecting and querying unlabeled data. One effective selection strategy is to base it on the model's predictive uncertainty, which can be interpreted as a measure of how informative a sample is. The sample's distance to the decision boundary is a natural measure of predictive uncertainty, but it is often intractable to compute, especially for complex decision boundaries formed in multiclass classification tasks. To address this issue, this paper proposes the {\it least disagree metric} (LDM), defined as the smallest probability of disagreement of the predicted label, and an estimator for LDM proven to be asymptotically consistent under mild assumptions. The estimator is computationally efficient and can be easily implemented for deep learning models using parameter perturbation. The LDM-based active learning is performed by querying unlabeled data with the smallest LDM. Exper
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#33258;&#35757;&#32451;&#26694;&#26550;&#29992;&#20110;&#32454;&#31890;&#24230;&#22330;&#26223;&#22270;&#29983;&#25104;&#65292;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#27880;&#30340;&#19977;&#20803;&#32452;&#32531;&#35299;&#20102;&#22330;&#26223;&#22270;&#29983;&#25104;&#20013;&#30340;&#38271;&#23614;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20266;&#26631;&#31614;&#25216;&#26415;CATM&#21644;&#22270;&#32467;&#26500;&#23398;&#20064;&#22120;GSL&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.09786</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#33258;&#35757;&#32451;&#26694;&#26550;&#29992;&#20110;&#32454;&#31890;&#24230;&#22330;&#26223;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Adaptive Self-training Framework for Fine-grained Scene Graph Generation. (arXiv:2401.09786v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#33258;&#35757;&#32451;&#26694;&#26550;&#29992;&#20110;&#32454;&#31890;&#24230;&#22330;&#26223;&#22270;&#29983;&#25104;&#65292;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#27880;&#30340;&#19977;&#20803;&#32452;&#32531;&#35299;&#20102;&#22330;&#26223;&#22270;&#29983;&#25104;&#20013;&#30340;&#38271;&#23614;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20266;&#26631;&#31614;&#25216;&#26415;CATM&#21644;&#22270;&#32467;&#26500;&#23398;&#20064;&#22120;GSL&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22330;&#26223;&#22270;&#29983;&#25104;&#65288;SGG&#65289;&#27169;&#22411;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#38271;&#23614;&#35859;&#35789;&#20998;&#24067;&#21644;&#32570;&#22833;&#27880;&#37322;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#27880;&#30340;&#19977;&#20803;&#32452;&#32531;&#35299;SGG&#30340;&#38271;&#23614;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#33258;&#35757;&#32451;SGG&#65288;ST-SGG&#65289;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22522;&#20110;&#26410;&#26631;&#27880;&#30340;&#19977;&#20803;&#32452;&#20026;&#20854;&#20998;&#37197;&#20266;&#26631;&#31614;&#20197;&#35757;&#32451;SGG&#27169;&#22411;&#12290;&#34429;&#28982;&#22312;&#22270;&#20687;&#35782;&#21035;&#26041;&#38754;&#30340;&#33258;&#35757;&#32451;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#35774;&#35745;&#36866;&#29992;&#20110;SGG&#20219;&#21153;&#30340;&#33258;&#35757;&#32451;&#26694;&#26550;&#26356;&#20855;&#25361;&#25112;&#65292;&#22240;&#20026;&#20854;&#22266;&#26377;&#29305;&#24615;&#65292;&#22914;&#35821;&#20041;&#27495;&#20041;&#21644;&#38271;&#23614;&#20998;&#24067;&#30340;&#35859;&#35789;&#31867;&#21035;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;SGG&#20266;&#26631;&#31614;&#25216;&#26415;&#65292;&#31216;&#20026;&#20855;&#26377;&#21160;&#37327;&#30340;&#31867;&#21035;&#33258;&#36866;&#24212;&#38408;&#20540;&#21270;&#65288;CATM&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#29420;&#31435;&#20110;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;&#24050;&#26377;&#30340;SGG&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22270;&#32467;&#26500;&#23398;&#20064;&#22120;&#65288;GSL&#65289;&#65292;&#20174;&#20013;&#33719;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scene graph generation (SGG) models have suffered from inherent problems regarding the benchmark datasets such as the long-tailed predicate distribution and missing annotation problems. In this work, we aim to alleviate the long-tailed problem of SGG by utilizing unannotated triplets. To this end, we introduce a Self-Training framework for SGG (ST-SGG) that assigns pseudo-labels for unannotated triplets based on which the SGG models are trained. While there has been significant progress in self-training for image recognition, designing a self-training framework for the SGG task is more challenging due to its inherent nature such as the semantic ambiguity and the long-tailed distribution of predicate classes. Hence, we propose a novel pseudo-labeling technique for SGG, called Class-specific Adaptive Thresholding with Momentum (CATM), which is a model-agnostic framework that can be applied to any existing SGG models. Furthermore, we devise a graph structure learner (GSL) that is benefici
&lt;/p&gt;</description></item><item><title>SEINE&#26159;&#19968;&#31181;&#29992;&#20110;&#26680;&#23454;&#20363;&#20998;&#21106;&#30340;&#32467;&#26500;&#32534;&#30721;&#21644;&#20132;&#20114;&#32593;&#32476;&#65292;&#36890;&#36807;&#32771;&#34385;&#26680;&#32467;&#26500;&#30340;&#30456;&#20851;&#24615;&#21644;&#21033;&#29992;&#26680;&#20043;&#38388;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#26469;&#25552;&#39640;&#27599;&#20010;&#20998;&#21106;&#23454;&#20363;&#30340;&#23436;&#25972;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.09773</link><description>&lt;p&gt;
SEINE:&#26680;&#23454;&#20363;&#20998;&#21106;&#30340;&#32467;&#26500;&#32534;&#30721;&#19982;&#20132;&#20114;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
SEINE: Structure Encoding and Interaction Network for Nuclei Instance Segmentation. (arXiv:2401.09773v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09773
&lt;/p&gt;
&lt;p&gt;
SEINE&#26159;&#19968;&#31181;&#29992;&#20110;&#26680;&#23454;&#20363;&#20998;&#21106;&#30340;&#32467;&#26500;&#32534;&#30721;&#21644;&#20132;&#20114;&#32593;&#32476;&#65292;&#36890;&#36807;&#32771;&#34385;&#26680;&#32467;&#26500;&#30340;&#30456;&#20851;&#24615;&#21644;&#21033;&#29992;&#26680;&#20043;&#38388;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#26469;&#25552;&#39640;&#27599;&#20010;&#20998;&#21106;&#23454;&#20363;&#30340;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#20013;&#30340;&#26680;&#23454;&#20363;&#20998;&#21106;&#23545;&#20110;&#29983;&#29289;&#20998;&#26512;&#21644;&#30284;&#30151;&#35786;&#26029;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#30001;&#20110;&#20004;&#20010;&#21407;&#22240;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#39318;&#20808;&#65292;&#21980;&#26579;&#26680;&#20869;&#21644;&#26680;&#22806;&#21306;&#22495;&#30340;&#35270;&#35273;&#21576;&#29616;&#30456;&#20284;&#65292;&#24120;&#24120;&#23548;&#33268;&#27424;&#20998;&#21106;&#12290;&#20854;&#27425;&#65292;&#24403;&#21069;&#26041;&#27861;&#32570;&#20047;&#23545;&#26680;&#32467;&#26500;&#30340;&#25506;&#32034;&#65292;&#23548;&#33268;&#20998;&#27573;&#23454;&#20363;&#30340;&#30862;&#29255;&#21270;&#39044;&#27979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SEINE&#30340;&#32467;&#26500;&#32534;&#30721;&#21644;&#20132;&#20114;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#24320;&#21457;&#20102;&#26680;&#30340;&#32467;&#26500;&#24314;&#27169;&#26041;&#26696;&#65292;&#24182;&#21033;&#29992;&#26680;&#20043;&#38388;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#26469;&#25552;&#39640;&#27599;&#20010;&#20998;&#21106;&#23454;&#20363;&#30340;&#23436;&#25972;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SEINE&#24341;&#20837;&#20102;&#22522;&#20110;&#36718;&#24275;&#30340;&#32467;&#26500;&#32534;&#30721;&#65288;SE&#65289;&#65292;&#20197;&#32771;&#34385;&#26680;&#32467;&#26500;&#21644;&#35821;&#20041;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#23454;&#29616;&#23545;&#26680;&#32467;&#26500;&#30340;&#21512;&#29702;&#34920;&#31034;&#12290;&#22522;&#20110;&#32534;&#30721;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32467;&#26500;&#24341;&#23548;&#30340;&#27880;&#24847;&#21147;&#65288;SGA&#65289;&#65292;&#23427;&#20197;&#28165;&#26224;&#30340;&#26680;&#20316;&#20026;&#21407;&#22411;&#65292;&#20197;&#22686;&#24378;&#27880;&#24847;&#21147;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#20998;&#21106;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nuclei instance segmentation in histopathological images is of great importance for biological analysis and cancer diagnosis but remains challenging for two reasons. (1) Similar visual presentation of intranuclear and extranuclear regions of chromophobe nuclei often causes under-segmentation, and (2) current methods lack the exploration of nuclei structure, resulting in fragmented instance predictions. To address these problems, this paper proposes a structure encoding and interaction network, termed SEINE, which develops the structure modeling scheme of nuclei and exploits the structure similarity between nuclei to improve the integrality of each segmented instance. Concretely, SEINE introduces a contour-based structure encoding (SE) that considers the correlation between nuclei structure and semantics, realizing a reasonable representation of the nuclei structure. Based on the encoding, we propose a structure-guided attention (SGA) that takes the clear nuclei as prototypes to enhance
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#32508;&#21512;&#27010;&#36848;&#20102;&#20851;&#20110;&#20174;&#24102;&#26377;&#24322;&#36136;&#24615;&#30340;&#22270;&#20013;&#23398;&#20064;&#30340;&#29616;&#26377;&#30740;&#31350;&#65292;&#24182;&#26681;&#25454;&#23398;&#20064;&#31574;&#30053;&#12289;&#27169;&#22411;&#26550;&#26500;&#21644;&#23454;&#38469;&#24212;&#29992;&#31561;&#26041;&#38754;&#23545;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;&#21516;&#26102;&#35752;&#35770;&#20102;&#29616;&#26377;&#30740;&#31350;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2401.09769</link><description>&lt;p&gt;
&#36208;&#21521;&#24322;&#36136;&#22270;&#23398;&#20064;&#65306;&#36827;&#23637;&#19982;&#26410;&#26469;
&lt;/p&gt;
&lt;p&gt;
Towards Learning from Graphs with Heterophily: Progress and Future. (arXiv:2401.09769v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#32508;&#21512;&#27010;&#36848;&#20102;&#20851;&#20110;&#20174;&#24102;&#26377;&#24322;&#36136;&#24615;&#30340;&#22270;&#20013;&#23398;&#20064;&#30340;&#29616;&#26377;&#30740;&#31350;&#65292;&#24182;&#26681;&#25454;&#23398;&#20064;&#31574;&#30053;&#12289;&#27169;&#22411;&#26550;&#26500;&#21644;&#23454;&#38469;&#24212;&#29992;&#31561;&#26041;&#38754;&#23545;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;&#21516;&#26102;&#35752;&#35770;&#20102;&#29616;&#26377;&#30740;&#31350;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#26159;&#29992;&#26469;&#27169;&#25311;&#29616;&#23454;&#19990;&#30028;&#23454;&#20307;&#20043;&#38388;&#22797;&#26434;&#20851;&#31995;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;&#26368;&#36817;&#65292;&#24322;&#36136;&#22270;&#65292;&#20854;&#20013;&#36830;&#25509;&#30340;&#33410;&#28857;&#24448;&#24448;&#20855;&#26377;&#19981;&#21516;&#30340;&#26631;&#31614;&#25110;&#19981;&#21516;&#30340;&#29305;&#24449;&#65292;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#24182;&#25214;&#21040;&#20102;&#35768;&#22810;&#24212;&#29992;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20154;&#20204;&#20063;&#22312;&#19981;&#26029;&#21162;&#21147;&#25512;&#36827;&#20174;&#24322;&#36136;&#22270;&#20013;&#23398;&#20064;&#12290;&#34429;&#28982;&#26377;&#20851;&#35813;&#20027;&#39064;&#30340;&#35843;&#26597;&#23384;&#22312;&#65292;&#20294;&#23427;&#20204;&#21482;&#20851;&#27880;&#20110;&#24322;&#36136;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#32780;&#24573;&#30053;&#20102;&#24322;&#36136;&#22270;&#23398;&#20064;&#30340;&#20854;&#20182;&#23376;&#20027;&#39064;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#22238;&#39038;&#20102;&#20851;&#20110;&#20174;&#24102;&#26377;&#24322;&#36136;&#24615;&#30340;&#22270;&#20013;&#23398;&#20064;&#30340;&#29616;&#26377;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;180&#22810;&#31687;&#35770;&#25991;&#65292;&#20171;&#32461;&#20102;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#23618;&#27425;&#20998;&#31867;&#27861;&#23545;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#31867;&#65292;&#21253;&#25324;&#23398;&#20064;&#31574;&#30053;&#12289;&#27169;&#22411;&#26550;&#26500;&#21644;&#23454;&#38469;&#24212;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#29616;&#26377;&#30740;&#31350;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#24182;&#31361;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graphs are structured data that models complex relations between real-world entities. Heterophilous graphs, where linked nodes are prone to be with different labels or dissimilar features, have recently attracted significant attention and found many applications. Meanwhile, increasing efforts have been made to advance learning from heterophilous graphs. Although there exist surveys on the relevant topic, they focus on heterophilous GNNs, which are only sub-topics of heterophilous graph learning. In this survey, we comprehensively overview existing works on learning from graphs with heterophily.First, we collect over 180 publications and introduce the development of this field. Then, we systematically categorize existing methods based on a hierarchical taxonomy including learning strategies, model architectures and practical applications. Finally, we discuss the primary challenges of existing studies and highlight promising avenues for future research.More publication details and corres
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21069;k&#36817;&#37051;&#30340;CLIP&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#22270;&#20687;&#21040;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#26080;&#38656;&#22823;&#37327;&#26631;&#27880;&#25968;&#25454;&#65292;&#36890;&#36807;&#23558;&#29983;&#25104;&#27169;&#22411;&#21644;K&#36817;&#37051;&#31639;&#27861;&#32467;&#21512;&#20351;&#29992;&#65292;&#24182;&#36890;&#36807;&#22312;&#32447;&#21644;&#31163;&#32447;&#20219;&#21153;&#26041;&#24335;&#23454;&#29616;&#12290;&#30456;&#36739;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#22312;&#25351;&#26631;&#19978;&#21462;&#24471;&#20102;&#26356;&#39640;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.09763</link><description>&lt;p&gt;
&#22522;&#20110;&#21069; k &#36817;&#37051;&#30340;&#22270;&#20687;&#21040;&#25991;&#26412;&#25552;&#31034;&#30340; CLIP &#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CLIP Model for Images to Textual Prompts Based on Top-k Neighbors. (arXiv:2401.09763v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09763
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21069;k&#36817;&#37051;&#30340;CLIP&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#22270;&#20687;&#21040;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#26080;&#38656;&#22823;&#37327;&#26631;&#27880;&#25968;&#25454;&#65292;&#36890;&#36807;&#23558;&#29983;&#25104;&#27169;&#22411;&#21644;K&#36817;&#37051;&#31639;&#27861;&#32467;&#21512;&#20351;&#29992;&#65292;&#24182;&#36890;&#36807;&#22312;&#32447;&#21644;&#31163;&#32447;&#20219;&#21153;&#26041;&#24335;&#23454;&#29616;&#12290;&#30456;&#36739;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#22312;&#25351;&#26631;&#19978;&#21462;&#24471;&#20102;&#26356;&#39640;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#26159;&#22810;&#27169;&#24577;&#29983;&#25104;&#30340;&#19968;&#20010;&#23376;&#39046;&#22495;&#65292;&#22312;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#24456;&#22823;&#20851;&#27880;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#22270;&#20687;&#21040;&#25552;&#31034;&#30340;&#25991;&#26412;&#65292;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#25552;&#31034;&#65292;&#26080;&#38656;&#22823;&#37327;&#30340;&#26631;&#27880;&#25968;&#25454;&#12290;&#25105;&#20204;&#23558;&#26041;&#27861;&#20998;&#20026;&#22312;&#32447;&#38454;&#27573;&#21644;&#31163;&#32447;&#38454;&#27573;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102; CLIP &#27169;&#22411;&#21644; K &#36817;&#37051;&#31639;&#27861;&#30340;&#32452;&#21512;&#12290;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#30001;&#20004;&#20010;&#20027;&#35201;&#37096;&#20998;&#32452;&#25104;&#65306;&#31163;&#32447;&#20219;&#21153;&#21644;&#22312;&#32447;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#25317;&#26377;&#26368;&#39640;&#30340;&#25351;&#26631;&#20026; 0.612&#65292;&#27604; Clip&#12289;Clip + KNN&#65288;&#21069; 10 &#21517;&#65289;&#20998;&#21035;&#39640;&#20986; 0.013&#12289;0.055&#12289;0.011&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image synthesis, a subfield of multimodal generation, has gained significant attention in recent years. We propose a cost-effective approach for image-to-prompt generation that leverages generative models to generate textual prompts without the need for large amounts of annotated data. We divide our method into two stages: online stage and offline stage. We use a combination of the CLIP model and K-nearest neighbors (KNN) algorithm. The proposed system consists of two main parts: an offline task and an online task. Our method owns the highest metric 0.612 among these models, which is 0.013, 0.055, 0.011 higher than Clip, Clip + KNN(top 10) respectively.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#20316;&#30340;&#19977;&#28857;&#27169;&#22411;&#65292;&#21033;&#29992;&#21512;&#20316;&#27874;&#26463;&#22686;&#24378;&#22320;-&#31354;&#35206;&#30422;&#25193;&#23637;&#12290;&#36890;&#36807;&#20998;&#26512;G2A&#35206;&#30422;&#25193;&#23637;&#65292;&#35777;&#26126;&#20102;&#19977;&#20010;TBSs&#20043;&#38388;&#30340;&#21512;&#20316;&#33021;&#22815;&#23454;&#29616;&#26368;&#23567;&#21270;&#35206;&#30422;&#37325;&#21472;&#30340;G2A&#35206;&#30422;&#65292;&#24182;&#26681;&#25454;Delaunay&#19977;&#35282;&#21078;&#20998;&#35774;&#35745;&#20102;&#21512;&#20316;&#35206;&#30422;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2401.09757</link><description>&lt;p&gt;
&#22312;&#36229;&#36234;5G&#32593;&#32476;&#20013;&#65292;&#22522;&#20110;&#21512;&#20316;&#30340;&#19977;&#28857;&#27169;&#22411;&#30340;&#22320;&#31354;&#35206;&#30422;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Cooperative Tri-Point Model-Based Ground-to-Air Coverage Extension in Beyond 5G Networks. (arXiv:2401.09757v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#20316;&#30340;&#19977;&#28857;&#27169;&#22411;&#65292;&#21033;&#29992;&#21512;&#20316;&#27874;&#26463;&#22686;&#24378;&#22320;-&#31354;&#35206;&#30422;&#25193;&#23637;&#12290;&#36890;&#36807;&#20998;&#26512;G2A&#35206;&#30422;&#25193;&#23637;&#65292;&#35777;&#26126;&#20102;&#19977;&#20010;TBSs&#20043;&#38388;&#30340;&#21512;&#20316;&#33021;&#22815;&#23454;&#29616;&#26368;&#23567;&#21270;&#35206;&#30422;&#37325;&#21472;&#30340;G2A&#35206;&#30422;&#65292;&#24182;&#26681;&#25454;Delaunay&#19977;&#35282;&#21078;&#20998;&#35774;&#35745;&#20102;&#21512;&#20316;&#35206;&#30422;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#29616;&#26377;&#30340;&#22320;&#38754;&#22522;&#30784;&#35774;&#26045;&#20026;&#31354;&#20013;&#29992;&#25143;&#25552;&#20379;&#35206;&#30422;&#26159;&#19968;&#31181;&#28508;&#22312;&#30340;&#20302;&#25104;&#26412;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#24050;&#32463;&#37096;&#32626;&#30340;&#22320;&#38754;&#22522;&#31449;&#65288;TBSs&#65289;&#30001;&#20110;&#21521;&#19979;&#20542;&#26012;&#30340;&#22825;&#32447;&#23548;&#33268;&#24369;&#30340;&#22320;&#31354;&#65288;G2A&#65289;&#35206;&#30422;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#19977;&#32500;&#31354;&#38388;&#20013;&#22797;&#26434;&#30340;&#20449;&#21495;&#35206;&#30422;&#38656;&#27714;&#65292;&#29305;&#21035;&#26159;&#22312;&#22402;&#30452;&#26041;&#21521;&#19978;&#65292;&#23454;&#29616;&#25972;&#20010;&#31354;&#22495;&#30340;&#26368;&#20339;&#35206;&#30422;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#20316;&#19977;&#28857;&#65288;CoTP&#65289;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#21512;&#20316;&#27874;&#26463;&#22686;&#24378;G2A&#35206;&#30422;&#25193;&#23637;&#12290;&#20026;&#20102;&#21033;&#29992;&#29616;&#26377;&#30340;TBSs&#24314;&#31435;&#26377;&#25928;&#30340;&#21512;&#20316;&#65292;&#25105;&#20204;&#35777;&#26126;&#19977;&#20010;TBSs&#20043;&#38388;&#30340;&#21512;&#20316;&#21487;&#20197;&#30830;&#20445;&#20855;&#26377;&#26368;&#23567;&#35206;&#30422;&#37325;&#21472;&#30340;G2A&#35206;&#30422;&#65292;&#24182;&#35774;&#35745;&#20102;CoTP&#27169;&#22411;&#26469;&#20998;&#26512;G2A&#35206;&#30422;&#25193;&#23637;&#12290;&#21033;&#29992;&#35813;&#27169;&#22411;&#65292;&#22522;&#20110;&#24503;&#21171;&#20869;&#19977;&#35282;&#21078;&#20998;&#30340;&#21512;&#20316;&#35206;&#30422;&#32467;&#26500;&#34987;&#35774;&#35745;&#20026;&#21010;&#20998;&#19977;&#26865;&#26609;&#24418;&#29366;&#30340;&#22303;&#22320;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
The utilization of existing terrestrial infrastructures to provide coverage for aerial users is a potentially low-cost solution. However, the already deployed terrestrial base stations (TBSs) result in weak ground-to-air (G2A) coverage due to the down-tilted antennas. Furthermore, achieving optimal coverage across the entire airspace through antenna adjustment is challenging due to the complex signal coverage requirements in three-dimensional space, especially in the vertical direction. In this paper, we propose a cooperative tri-point (CoTP) model-based method that utilizes cooperative beams to enhance the G2A coverage extension. To utilize existing TBSs for establishing effective cooperation, we prove that the cooperation among three TBSs can ensure G2A coverage with a minimum coverage overlap, and design the CoTP model to analyze the G2A coverage extension. Using the model, a cooperative coverage structure based on Delaunay triangulation is designed to divide triangular prism-shaped
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;-DBShap&#65292;&#20351;&#29992;Shapley&#20540;&#26469;&#30830;&#23450;&#27169;&#22411;&#24615;&#33021;&#28418;&#31227;&#30340;&#20027;&#35201;&#36129;&#29486;&#32773;&#24182;&#37327;&#21270;&#20182;&#20204;&#30340;&#36129;&#29486;&#12290;&#36890;&#36807;DBShap&#25552;&#20379;&#30340;&#35299;&#37322;&#65292;&#21487;&#20197;&#29702;&#35299;&#28418;&#31227;&#32972;&#21518;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2401.09756</link><description>&lt;p&gt;
&#20351;&#29992;Shapley&#20540;&#35299;&#37322;&#28418;&#31227;
&lt;/p&gt;
&lt;p&gt;
Explaining Drift using Shapley Values. (arXiv:2401.09756v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09756
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;-DBShap&#65292;&#20351;&#29992;Shapley&#20540;&#26469;&#30830;&#23450;&#27169;&#22411;&#24615;&#33021;&#28418;&#31227;&#30340;&#20027;&#35201;&#36129;&#29486;&#32773;&#24182;&#37327;&#21270;&#20182;&#20204;&#30340;&#36129;&#29486;&#12290;&#36890;&#36807;DBShap&#25552;&#20379;&#30340;&#35299;&#37322;&#65292;&#21487;&#20197;&#29702;&#35299;&#28418;&#31227;&#32972;&#21518;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#34987;&#29992;&#20110;&#39044;&#27979;&#20854;&#26410;&#35757;&#32451;&#25968;&#25454;&#30340;&#32467;&#26524;&#26102;&#65292;&#20854;&#24615;&#33021;&#24120;&#24120;&#20250;&#19979;&#38477;&#12290;&#36825;&#31181;&#24773;&#20917;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#32463;&#24120;&#21457;&#29983;&#65292;&#22240;&#20026;&#25968;&#25454;&#30340;&#20998;&#24067;&#20250;&#36880;&#28176;&#25110;&#31361;&#28982;&#22320;&#21457;&#29983;&#21464;&#21270;&#65292;&#25110;&#30001;&#20110;&#20687;&#22823;&#27969;&#34892;&#30149;&#36825;&#26679;&#30340;&#37325;&#22823;&#20107;&#20214;&#12290;&#22312;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#65292;&#24050;&#32463;&#26377;&#35768;&#22810;&#23581;&#35797;&#25552;&#20986;&#33021;&#22815;&#25269;&#24481;&#36825;&#31181;&#27010;&#24565;&#28418;&#31227;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#27809;&#26377;&#19968;&#20010;&#21407;&#21017;&#24615;&#30340;&#26694;&#26550;&#26469;&#30830;&#23450;&#23548;&#33268;&#27169;&#22411;&#24615;&#33021;&#28418;&#31227;&#30340;&#21407;&#22240;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;-DBShap&#65292;&#23427;&#20351;&#29992;Shapley&#20540;&#26469;&#30830;&#23450;&#28418;&#31227;&#30340;&#20027;&#35201;&#36129;&#29486;&#32773;&#24182;&#37327;&#21270;&#20182;&#20204;&#30340;&#36129;&#29486;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#19981;&#20165;&#37327;&#21270;&#20102;&#21333;&#20010;&#29305;&#24449;&#22312;&#39537;&#21160;&#28418;&#31227;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#65292;&#36824;&#21253;&#25324;&#20102;&#36755;&#20837;&#21644;&#36755;&#20986;&#20043;&#38388;&#24213;&#23618;&#20851;&#31995;&#30340;&#21464;&#21270;&#20316;&#20026;&#21487;&#33021;&#30340;&#39537;&#21160;&#22240;&#32032;&#12290;DBShap&#25152;&#25552;&#20379;&#30340;&#35299;&#37322;&#21487;&#20197;&#29992;&#20110;&#29702;&#35299;&#28418;&#31227;&#32972;&#21518;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models often deteriorate in their performance when they are used to predict the outcomes over data on which they were not trained. These scenarios can often arise in real world when the distribution of data changes gradually or abruptly due to major events like a pandemic. There have been many attempts in machine learning research to come up with techniques that are resilient to such Concept drifts. However, there is no principled framework to identify the drivers behind the drift in model performance. In this paper, we propose a novel framework - DBShap that uses Shapley values to identify the main contributors of the drift and quantify their respective contributions. The proposed framework not only quantifies the importance of individual features in driving the drift but also includes the change in the underlying relation between the input and output as a possible driver. The explanation provided by DBShap can be used to understand the root cause behind the drift and
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#20989;&#25968;&#22270;&#20687;&#21644;&#25805;&#20316;&#26641;&#24207;&#21015;&#30340;&#31185;&#23398;&#35745;&#31639;&#22810;&#27169;&#24577;&#26694;&#26550;&#65288;Botfip&#65289;&#65292;&#24212;&#29992;&#20110;&#31526;&#21495;&#22238;&#24402;&#38382;&#39064;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#22312;&#20302;&#22797;&#26434;&#24230;&#38382;&#39064;&#19978;&#30340;&#20248;&#21183;&#65292;&#23637;&#31034;&#20102;&#20854;&#28508;&#21147;&#12290;&#36825;&#20010;&#22810;&#27169;&#24577;&#26694;&#26550;&#22312;&#31185;&#23398;&#35745;&#31639;&#38382;&#39064;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2401.09748</link><description>&lt;p&gt;
Bootstrapping OTS-Funcimg Pre-training Model (Botfip) -- &#19968;&#20010;&#20840;&#38754;&#30340;&#31526;&#21495;&#22238;&#24402;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Bootstrapping OTS-Funcimg Pre-training Model (Botfip) -- A Comprehensive Symbolic Regression Framework. (arXiv:2401.09748v1 [cs.SC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09748
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#20989;&#25968;&#22270;&#20687;&#21644;&#25805;&#20316;&#26641;&#24207;&#21015;&#30340;&#31185;&#23398;&#35745;&#31639;&#22810;&#27169;&#24577;&#26694;&#26550;&#65288;Botfip&#65289;&#65292;&#24212;&#29992;&#20110;&#31526;&#21495;&#22238;&#24402;&#38382;&#39064;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#22312;&#20302;&#22797;&#26434;&#24230;&#38382;&#39064;&#19978;&#30340;&#20248;&#21183;&#65292;&#23637;&#31034;&#20102;&#20854;&#28508;&#21147;&#12290;&#36825;&#20010;&#22810;&#27169;&#24577;&#26694;&#26550;&#22312;&#31185;&#23398;&#35745;&#31639;&#38382;&#39064;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#35745;&#31639;&#39046;&#22495;&#20013;&#65292;&#35768;&#22810;&#38382;&#39064;&#35299;&#20915;&#26041;&#27861;&#24448;&#24448;&#21482;&#27880;&#37325;&#36807;&#31243;&#21644;&#26368;&#32456;&#32467;&#26524;&#65292;&#21363;&#20351;&#22312;&#31185;&#23398;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#20063;&#32570;&#20047;&#23545;&#25968;&#25454;&#32972;&#21518;&#30340;&#28145;&#24230;&#22810;&#27169;&#24577;&#20449;&#24687;&#25366;&#25496;&#65292;&#32570;&#20047;&#31867;&#20284;&#20110;&#22270;&#20687;&#25991;&#26412;&#39046;&#22495;&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;&#12290;&#26412;&#25991;&#20197;&#31526;&#21495;&#22238;&#24402;&#65288;SR&#65289;&#20026;&#37325;&#28857;&#65292;&#22312;&#22270;&#20687;&#25991;&#26412;&#39046;&#22495;&#30340;BLIP&#27169;&#22411;&#30340;&#21551;&#21457;&#19979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20989;&#25968;&#22270;&#20687;&#65288;Funcimg&#65289;&#21644;&#25805;&#20316;&#26641;&#24207;&#21015;&#65288;OTS&#65289;&#30340;&#31185;&#23398;&#35745;&#31639;&#22810;&#27169;&#24577;&#26694;&#26550;&#8212;&#8212;&#24341;&#23548;OTS-Funcimg&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;Botfip&#65289;&#12290;&#22312;SR&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;Botfip&#22312;&#20302;&#22797;&#26434;&#24230;&#30340;SR&#38382;&#39064;&#20013;&#30340;&#20248;&#21183;&#65292;&#23637;&#31034;&#20102;&#20854;&#28508;&#21147;&#12290;&#20316;&#20026;&#19968;&#20010;MED&#26694;&#26550;&#65292;Botfip&#22312;&#26356;&#24191;&#27867;&#30340;&#31185;&#23398;&#35745;&#31639;&#38382;&#39064;&#20013;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of scientific computing, many problem-solving approaches tend to focus only on the process and final outcome, even in AI for science, there is a lack of deep multimodal information mining behind the data, missing a multimodal framework akin to that in the image-text domain. In this paper, we take Symbolic Regression(SR) as our focal point and, drawing inspiration from the BLIP model in the image-text domain, propose a scientific computing multimodal framework based on Function Images (Funcimg) and Operation Tree Sequence (OTS), named Bootstrapping OTS-Funcimg Pre-training Model (Botfip). In SR experiments, we validate the advantages of Botfip in low-complexity SR problems, showcasing its potential. As a MED framework, Botfip holds promise for future applications in a broader range of scientific computing problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#33258;&#38381;&#30151;&#20799;&#31461;&#19982;&#24515;&#29702;&#23398;&#23478;&#20043;&#38388;&#30340;&#35786;&#26029;&#24615;&#23545;&#35805;&#20013;&#25552;&#21462;&#30340;&#22768;&#23398;/&#38901;&#24459;&#21644;&#35821;&#35328;&#29305;&#24449;&#65292;&#30740;&#31350;&#20102;&#33258;&#38381;&#30151;&#35848;&#35805;&#20998;&#26512;&#30340;&#21442;&#25968;&#36873;&#25321;&#12290;&#30740;&#31350;&#32467;&#26524;&#21487;&#20197;&#25552;&#20379;&#23545;ASD&#20799;&#31461;&#35848;&#35805;&#25968;&#25454;&#30340;&#32454;&#33268;&#20998;&#26512;&#65292;&#25903;&#25345;&#35786;&#26029;&#21644;&#24178;&#39044;&#12290;</title><link>http://arxiv.org/abs/2401.09717</link><description>&lt;p&gt;
&#29992;&#20110;&#20998;&#26512;&#33258;&#38381;&#30151;&#35848;&#35805;&#30340;&#21442;&#25968;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Parameter Selection for Analyzing Conversations with Autism Spectrum Disorder. (arXiv:2401.09717v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#33258;&#38381;&#30151;&#20799;&#31461;&#19982;&#24515;&#29702;&#23398;&#23478;&#20043;&#38388;&#30340;&#35786;&#26029;&#24615;&#23545;&#35805;&#20013;&#25552;&#21462;&#30340;&#22768;&#23398;/&#38901;&#24459;&#21644;&#35821;&#35328;&#29305;&#24449;&#65292;&#30740;&#31350;&#20102;&#33258;&#38381;&#30151;&#35848;&#35805;&#20998;&#26512;&#30340;&#21442;&#25968;&#36873;&#25321;&#12290;&#30740;&#31350;&#32467;&#26524;&#21487;&#20197;&#25552;&#20379;&#23545;ASD&#20799;&#31461;&#35848;&#35805;&#25968;&#25454;&#30340;&#32454;&#33268;&#20998;&#26512;&#65292;&#25903;&#25345;&#35786;&#26029;&#21644;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;(ASD)&#30340;&#35786;&#26029;&#26159;&#19968;&#39033;&#22797;&#26434;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#23427;&#20381;&#36182;&#20110;&#24515;&#29702;&#23398;&#23478;&#23545;&#20114;&#21160;&#34892;&#20026;&#30340;&#20998;&#26512;&#65292;&#32780;&#19981;&#26159;&#29983;&#21270;&#35786;&#26029;&#30340;&#20351;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20998;&#26512;&#24515;&#29702;&#23398;&#23478;&#19982;&#20856;&#22411;&#21457;&#32946;&#25110;&#26377;ASD&#30340;&#20799;&#31461;&#20043;&#38388;&#35786;&#26029;&#24615;&#23545;&#35805;&#20013;&#25552;&#21462;&#30340;&#22768;&#23398;/&#38901;&#24459;&#21644;&#35821;&#35328;&#29305;&#24449;&#26469;&#36827;&#34892;ASD&#35786;&#26029;&#30340;&#24314;&#27169;&#26041;&#27861;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#29305;&#24449;&#22312;&#19968;&#31995;&#21015;&#23545;&#35805;&#20219;&#21153;&#20013;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#25214;&#21040;&#19968;&#32452;&#26368;&#23567;&#30340;&#21442;&#25968;&#26469;&#25551;&#36848;ASD&#20799;&#31461;&#30340;&#35848;&#35805;&#34892;&#20026;&#12290;&#30001;&#20110;ASD&#26159;&#36890;&#36807;&#23545;&#35805;&#20114;&#21160;&#36827;&#34892;&#35786;&#26029;&#30340;&#65292;&#38500;&#20102;&#20998;&#26512;&#20799;&#31461;&#30340;&#34892;&#20026;&#22806;&#65292;&#25105;&#20204;&#36824;&#35843;&#26597;&#20102;&#24515;&#29702;&#23398;&#23478;&#30340;&#23545;&#35805;&#34892;&#20026;&#22312;&#19981;&#21516;&#35786;&#26029;&#32676;&#20307;&#20043;&#38388;&#26159;&#21542;&#26377;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#21487;&#20197;&#20419;&#36827;&#23545;ASD&#20799;&#31461;&#30340;&#35848;&#35805;&#25968;&#25454;&#36827;&#34892;&#32454;&#33268;&#20998;&#26512;&#65292;&#20197;&#25903;&#25345;&#35786;&#26029;&#21644;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;
The diagnosis of autism spectrum disorder (ASD) is a complex, challenging task as it depends on the analysis of interactional behaviors by psychologists rather than the use of biochemical diagnostics. In this paper, we present a modeling approach to ASD diagnosis by analyzing acoustic/prosodic and linguistic features extracted from diagnostic conversations between a psychologist and children who either are typically developing (TD) or have ASD. We compare the contributions of different features across a range of conversation tasks. We focus on finding a minimal set of parameters that characterize conversational behaviors of children with ASD. Because ASD is diagnosed through conversational interaction, in addition to analyzing the behavior of the children, we also investigate whether the psychologist's conversational behaviors vary across diagnostic groups. Our results can facilitate fine-grained analysis of conversation data for children with ASD to support diagnosis and intervention.
&lt;/p&gt;</description></item><item><title>HCVP&#26159;&#19968;&#31181;&#22522;&#20110;&#23618;&#27425;&#23545;&#27604;&#35270;&#35273;&#25552;&#31034;&#30340;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#23548;&#27169;&#22411;&#23558;&#19981;&#21464;&#29305;&#24449;&#19982;&#29305;&#23450;&#29305;&#24449;&#20998;&#31163;&#65292;&#25552;&#39640;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.09716</link><description>&lt;p&gt;
HCVP: &#22522;&#20110;&#23618;&#27425;&#23545;&#27604;&#35270;&#35273;&#25552;&#31034;&#30340;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
HCVP: Leveraging Hierarchical Contrastive Visual Prompt for Domain Generalization. (arXiv:2401.09716v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09716
&lt;/p&gt;
&lt;p&gt;
HCVP&#26159;&#19968;&#31181;&#22522;&#20110;&#23618;&#27425;&#23545;&#27604;&#35270;&#35273;&#25552;&#31034;&#30340;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#23548;&#27169;&#22411;&#23558;&#19981;&#21464;&#29305;&#24449;&#19982;&#29305;&#23450;&#29305;&#24449;&#20998;&#31163;&#65292;&#25552;&#39640;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#27867;&#21270;&#65288;DG&#65289;&#26088;&#22312;&#36890;&#36807;&#23398;&#20064;&#19981;&#21464;&#29305;&#24449;&#26469;&#21019;&#24314;&#22312;&#26410;&#30693;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#33394;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;DG&#20013;&#65292;&#23558;&#27169;&#22411;&#38480;&#21046;&#22312;&#22266;&#23450;&#32467;&#26500;&#25110;&#32479;&#19968;&#21442;&#25968;&#21270;&#20013;&#20197;&#21253;&#21547;&#19981;&#21464;&#29305;&#24449;&#30340;&#20027;&#27969;&#23454;&#36341;&#21487;&#33021;&#20250;&#19981;&#21487;&#36991;&#20813;&#22320;&#34701;&#21512;&#29305;&#23450;&#26041;&#38754;&#12290;&#36825;&#31181;&#26041;&#27861;&#38590;&#20197;&#23545;&#39046;&#22495;&#38388;&#21464;&#21270;&#36827;&#34892;&#32454;&#24494;&#21306;&#20998;&#65292;&#21487;&#33021;&#23545;&#26576;&#20123;&#39046;&#22495;&#23384;&#22312;&#20559;&#35265;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#23545;&#22495;&#19981;&#21464;&#29305;&#24449;&#30340;&#31934;&#30830;&#23398;&#20064;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#20026;&#27169;&#22411;&#25552;&#20379;&#39046;&#22495;&#32423;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#26356;&#26377;&#25928;&#22320;&#24341;&#23548;&#27169;&#22411;&#23558;&#19981;&#21464;&#29305;&#24449;&#19982;&#29305;&#23450;&#29305;&#24449;&#20998;&#31163;&#65292;&#20174;&#32780;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;&#22312;&#39046;&#22495;&#27867;&#21270;&#33539;&#24335;&#20013;&#65292;&#20511;&#37492;&#20102;&#35270;&#35273;&#25552;&#31034;&#30340;&#26032;&#36235;&#21183;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;HCVP&#8221;&#65288;&#23618;&#27425;&#23545;&#27604;&#35270;&#35273;&#25552;&#31034;&#65289;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain Generalization (DG) endeavors to create machine learning models that excel in unseen scenarios by learning invariant features. In DG, the prevalent practice of constraining models to a fixed structure or uniform parameterization to encapsulate invariant features can inadvertently blend specific aspects. Such an approach struggles with nuanced differentiation of inter-domain variations and may exhibit bias towards certain domains, hindering the precise learning of domain-invariant features. Recognizing this, we introduce a novel method designed to supplement the model with domain-level and task-specific characteristics. This approach aims to guide the model in more effectively separating invariant features from specific characteristics, thereby boosting the generalization. Building on the emerging trend of visual prompts in the DG paradigm, our work introduces the novel \textbf{H}ierarchical \textbf{C}ontrastive \textbf{V}isual \textbf{P}rompt (HCVP) methodology. This represents 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;Transformer&#22522;&#30784;&#27169;&#22411;&#12289;InfoNCE&#25439;&#22833;&#21644;&#35821;&#35328;&#20999;&#25442;&#26041;&#27861;&#26469;&#35299;&#20915;&#35838;&#31243;&#25512;&#33616;&#20013;&#30340;&#20869;&#23481;&#20914;&#31361;&#21644;&#35821;&#35328;&#32763;&#35793;&#24341;&#36215;&#30340;&#24178;&#25200;&#38382;&#39064;&#65292;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#20010;&#24615;&#21270;&#23398;&#20064;&#20307;&#39564;&#12289;&#21253;&#23481;&#22810;&#26679;&#24615;&#30340;&#25945;&#32946;&#29615;&#22659;&#12290;</title><link>http://arxiv.org/abs/2401.09699</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#19982;InfoNCE&#25439;&#22833;&#21644;&#35821;&#35328;&#20999;&#25442;&#26041;&#27861;&#30340;&#35838;&#31243;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Curriculum Recommendations Using Transformer Base Model with InfoNCE Loss And Language Switching Method. (arXiv:2401.09699v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09699
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;Transformer&#22522;&#30784;&#27169;&#22411;&#12289;InfoNCE&#25439;&#22833;&#21644;&#35821;&#35328;&#20999;&#25442;&#26041;&#27861;&#26469;&#35299;&#20915;&#35838;&#31243;&#25512;&#33616;&#20013;&#30340;&#20869;&#23481;&#20914;&#31361;&#21644;&#35821;&#35328;&#32763;&#35793;&#24341;&#36215;&#30340;&#24178;&#25200;&#38382;&#39064;&#65292;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#20010;&#24615;&#21270;&#23398;&#20064;&#20307;&#39564;&#12289;&#21253;&#23481;&#22810;&#26679;&#24615;&#30340;&#25945;&#32946;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35838;&#31243;&#25512;&#33616;&#33539;&#24335;&#33268;&#21147;&#20110;&#22312;&#19981;&#26029;&#21457;&#23637;&#30340;&#25945;&#32946;&#25216;&#26415;&#21644;&#35838;&#31243;&#24320;&#21457;&#39046;&#22495;&#20013;&#20419;&#36827;&#23398;&#20064;&#24179;&#31561;&#12290;&#37492;&#20110;&#29616;&#26377;&#26041;&#27861;&#25152;&#38754;&#20020;&#30340;&#20869;&#23481;&#20914;&#31361;&#21644;&#35821;&#35328;&#32763;&#35793;&#24341;&#36215;&#30340;&#24178;&#25200;&#31561;&#22256;&#38590;&#65292;&#35813;&#33539;&#24335;&#26088;&#22312;&#38754;&#23545;&#24182;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#12290;&#29305;&#21035;&#26159;&#65292;&#23427;&#35299;&#20915;&#20102;&#35821;&#35328;&#32763;&#35793;&#24341;&#20837;&#30340;&#20869;&#23481;&#20914;&#31361;&#21644;&#24178;&#25200;&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#21487;&#33021;&#38459;&#30861;&#21019;&#24314;&#20840;&#38754;&#21644;&#20010;&#24615;&#21270;&#23398;&#20064;&#20307;&#39564;&#12290;&#35813;&#33539;&#24335;&#30340;&#30446;&#26631;&#26159;&#22521;&#20859;&#19968;&#20010;&#26082;&#21253;&#23481;&#22810;&#26679;&#24615;&#21448;&#21487;&#20197;&#26681;&#25454;&#27599;&#20010;&#23398;&#20064;&#32773;&#30340;&#29420;&#29305;&#38656;&#27714;&#23450;&#21046;&#23398;&#20064;&#20307;&#39564;&#30340;&#25945;&#32946;&#29615;&#22659;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35838;&#31243;&#24320;&#21457;&#21644;&#20010;&#24615;&#21270;&#23398;&#20064;&#26041;&#38754;&#24341;&#20837;&#20102;&#19977;&#20010;&#20851;&#38190;&#21019;&#26032;&#12290;&#20854;&#20013;&#21253;&#25324;&#20351;&#29992;Transformer&#22522;&#30784;&#27169;&#22411;&#22686;&#24378;&#35745;&#31639;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Curriculum Recommendations paradigm is dedicated to fostering learning equality within the ever-evolving realms of educational technology and curriculum development. In acknowledging the inherent obstacles posed by existing methodologies, such as content conflicts and disruptions from language translation, this paradigm aims to confront and overcome these challenges. Notably, it addresses content conflicts and disruptions introduced by language translation, hindrances that can impede the creation of an all-encompassing and personalized learning experience. The paradigm's objective is to cultivate an educational environment that not only embraces diversity but also customizes learning experiences to suit the distinct needs of each learner. To overcome these challenges, our approach builds upon notable contributions in curriculum development and personalized learning, introducing three key innovations. These include the integration of Transformer Base Model to enhance computational e
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;AI&#22312;&#24651;&#29233;&#35299;&#20307;&#36807;&#31243;&#20013;&#30340;&#20316;&#29992;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#21069;&#25216;&#26415;&#22312;&#20449;&#24687;&#25910;&#38598;&#12289;&#31038;&#32676;&#25903;&#25345;&#21644;&#20419;&#36827;&#27807;&#36890;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#21442;&#19982;&#32773;&#39044;&#35745;AI&#21487;&#20197;&#28385;&#36275;&#19981;&#21516;&#38454;&#27573;&#30340;&#38656;&#27714;&#65292;&#24110;&#21161;&#35299;&#20307;&#24651;&#24773;&#12290;</title><link>http://arxiv.org/abs/2401.09695</link><description>&lt;p&gt;
&#22312;&#32467;&#26463;&#24651;&#24773;&#36807;&#31243;&#20013;&#65292;ChatGPT&#26159;&#21542;&#24212;&#35813;&#26367;&#20320;&#20889;&#20998;&#25163;&#30701;&#20449;&#65311;&#25506;&#32034;AI&#22312;&#24651;&#29233;&#35299;&#20307;&#20013;&#30340;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Should ChatGPT Write Your Breakup Text? Exploring the Role of AI in Relationship Dissolution. (arXiv:2401.09695v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09695
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;AI&#22312;&#24651;&#29233;&#35299;&#20307;&#36807;&#31243;&#20013;&#30340;&#20316;&#29992;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#21069;&#25216;&#26415;&#22312;&#20449;&#24687;&#25910;&#38598;&#12289;&#31038;&#32676;&#25903;&#25345;&#21644;&#20419;&#36827;&#27807;&#36890;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#21442;&#19982;&#32773;&#39044;&#35745;AI&#21487;&#20197;&#28385;&#36275;&#19981;&#21516;&#38454;&#27573;&#30340;&#38656;&#27714;&#65292;&#24110;&#21161;&#35299;&#20307;&#24651;&#24773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24651;&#29233;&#20851;&#31995;&#23545;&#25105;&#20204;&#30340;&#24184;&#31119;&#21644;&#24184;&#31119;&#24863;&#33267;&#20851;&#37325;&#35201;&#12290;&#24651;&#29233;&#35299;&#20307;&#26159;&#24651;&#29233;&#29983;&#21629;&#21608;&#26399;&#30340;&#26368;&#21518;&#38454;&#27573;&#65292;&#20063;&#26159;&#20010;&#20154;&#29983;&#27963;&#20013;&#26368;&#20855;&#21387;&#21147;&#30340;&#20107;&#20214;&#20043;&#19968;&#65292;&#21487;&#33021;&#23545;&#20154;&#20204;&#20135;&#29983;&#28145;&#36828;&#32780;&#25345;&#20037;&#30340;&#24433;&#21709;&#12290;&#38543;&#30528;&#36890;&#36807;&#35745;&#31639;&#26426;&#20171;&#36136;&#20256;&#36798;&#30340;&#35299;&#20307;&#36807;&#31243;&#36234;&#26469;&#36234;&#21463;&#21040;&#25903;&#25345;&#65292;&#20197;&#21450;AI&#20171;&#20837;&#30340;&#20256;&#25773;&#26041;&#24335;&#30340;&#21487;&#33021;&#26410;&#26469;&#24433;&#21709;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#21322;&#32467;&#26500;&#21270;&#35775;&#35848;&#30740;&#31350;&#65292;&#20849;&#26377;21&#21517;&#21442;&#19982;&#32773;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#20102;&#35299;&#65306;1&#65289;&#25216;&#26415;&#22312;&#35299;&#20307;&#36807;&#31243;&#20013;&#30340;&#24403;&#21069;&#35282;&#33394;&#65292;2&#65289;&#20010;&#20154;&#22312;&#36807;&#31243;&#20013;&#30340;&#38656;&#27714;&#21644;&#25903;&#25345;&#65292;&#20197;&#21450;3&#65289;AI&#22914;&#20309;&#28385;&#36275;&#36825;&#20123;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#20154;&#20204;&#22312;&#32467;&#26463;&#24651;&#24773;&#30340;&#19981;&#21516;&#38454;&#27573;&#26377;&#19981;&#21516;&#30340;&#38656;&#27714;&#12290;&#30446;&#21069;&#65292;&#25216;&#26415;&#34987;&#29992;&#20110;&#20449;&#24687;&#25910;&#38598;&#21644;&#31038;&#32676;&#25903;&#25345;&#65292;&#22312;&#20419;&#25104;&#20998;&#25163;&#12289;&#20351;&#39740;&#39746;&#24335;&#20998;&#25163;&#21644;&#25289;&#40657;&#25104;&#20026;&#21487;&#33021;&#65292;&#20197;&#21450;&#20419;&#36827;&#27807;&#36890;&#12290;&#21442;&#19982;&#32773;&#39044;&#35745;AI&#21487;&#20197;&#24110;&#21161;&#23454;&#29616;&#24863;&#30693;&#25216;&#24039;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relationships are essential to our happiness and wellbeing. The dissolution of a relationship, the final stage of relationship's lifecycle and one of the most stressful events in an individual's life, can have profound and long-lasting impacts on people. With the breakup process increasingly facilitated by computer-mediated communication (CMC), and the likely future influence of AI-mediated communication (AIMC) tools, we conducted a semi-structured interview study with 21 participants. We aim to understand: 1) the current role of technology in the breakup process, 2) the needs and support individuals have during the process, and 3) how AI might address these needs. Our research shows that people have distinct needs at various stages of ending a relationship. Presently, technology is used for information gathering and community support, acting as a catalyst for breakups, enabling ghosting and blocking, and facilitating communication. Participants anticipate that AI could aid in sense-ma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27169;&#20223;&#23398;&#20064;&#20013;&#35299;&#20915;&#22810;&#27169;&#24577;&#25968;&#25454;&#22788;&#29702;&#25361;&#25112;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#36755;&#20837;&#21040;&#27599;&#20010;&#31070;&#32463;&#32593;&#32476;&#23618;&#20013;&#65292;&#25918;&#22823;&#19982;&#26399;&#26395;&#36755;&#20986;&#30340;&#30456;&#20851;&#24615;&#36739;&#20302;&#30340;&#25968;&#25454;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25104;&#21151;&#29575;&#30340;&#26174;&#33879;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2401.09691</link><description>&lt;p&gt;
&#23558;&#22270;&#20687;&#29305;&#24449;&#36755;&#20837;&#21040;&#31070;&#32463;&#32593;&#32476;&#30340;&#27599;&#19968;&#23618;&#30340;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Imitation Learning Inputting Image Feature to Each Layer of Neural Network. (arXiv:2401.09691v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27169;&#20223;&#23398;&#20064;&#20013;&#35299;&#20915;&#22810;&#27169;&#24577;&#25968;&#25454;&#22788;&#29702;&#25361;&#25112;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#36755;&#20837;&#21040;&#27599;&#20010;&#31070;&#32463;&#32593;&#32476;&#23618;&#20013;&#65292;&#25918;&#22823;&#19982;&#26399;&#26395;&#36755;&#20986;&#30340;&#30456;&#20851;&#24615;&#36739;&#20302;&#30340;&#25968;&#25454;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25104;&#21151;&#29575;&#30340;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#20351;&#24471;&#26426;&#22120;&#20154;&#33021;&#22815;&#36890;&#36807;&#35757;&#32451;&#25968;&#25454;&#23398;&#20064;&#24182;&#22797;&#21046;&#20154;&#31867;&#34892;&#20026;&#12290;&#26368;&#36817;&#26426;&#22120;&#23398;&#20064;&#30340;&#36827;&#23637;&#20351;&#24471;&#33021;&#22815;&#30452;&#25509;&#22788;&#29702;&#39640;&#32500;&#35266;&#27979;&#25968;&#25454;&#65288;&#22914;&#22270;&#20687;&#65289;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#26041;&#27861;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#22788;&#29702;&#22810;&#20010;&#27169;&#24577;&#30340;&#25968;&#25454;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#38754;&#20020;&#30528;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#21363;&#22312;&#20351;&#29992;&#30701;&#37319;&#26679;&#21608;&#26399;&#26102;&#26080;&#24847;&#20013;&#24573;&#30053;&#19982;&#26399;&#26395;&#36755;&#20986;&#30340;&#30456;&#20851;&#24615;&#36739;&#20302;&#30340;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#36755;&#20837;&#21040;&#27599;&#20010;&#31070;&#32463;&#32593;&#32476;&#23618;&#20013;&#65292;&#25918;&#22823;&#19982;&#36755;&#20986;&#30456;&#20851;&#24615;&#36739;&#20302;&#30340;&#25968;&#25454;&#30340;&#24433;&#21709;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#23558;&#22810;&#26679;&#30340;&#25968;&#25454;&#28304;&#32435;&#20837;&#21040;&#23398;&#20064;&#36807;&#31243;&#20013;&#12290;&#36890;&#36807;&#20351;&#29992;&#21407;&#22987;&#22270;&#20687;&#21644;&#20851;&#33410;&#20449;&#24687;&#20316;&#20026;&#36755;&#20837;&#36827;&#34892;&#31616;&#21333;&#30340;&#25342;&#21462;&#25918;&#32622;&#25805;&#20316;&#30340;&#23454;&#39564;&#65292;&#21363;&#20351;&#22788;&#29702;&#26469;&#33258;&#30701;&#37319;&#26679;&#21608;&#26399;&#30340;&#25968;&#25454;&#65292;&#20063;&#35777;&#26126;&#20102;&#25104;&#21151;&#29575;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation learning enables robots to learn and replicate human behavior from training data. Recent advances in machine learning enable end-to-end learning approaches that directly process high-dimensional observation data, such as images. However, these approaches face a critical challenge when processing data from multiple modalities, inadvertently ignoring data with a lower correlation to the desired output, especially when using short sampling periods. This paper presents a useful method to address this challenge, which amplifies the influence of data with a relatively low correlation to the output by inputting the data into each neural network layer. The proposed approach effectively incorporates diverse data sources into the learning process. Through experiments using a simple pick-and-place operation with raw images and joint information as input, significant improvements in success rates are demonstrated even when dealing with data from short sampling periods.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23567;&#22411;&#26426;&#22120;&#23398;&#20064;&#30340;Stackelberg&#21338;&#24328;&#26694;&#26550;&#65292;&#22312;&#26080;&#20154;&#26426;Metaverse&#20013;&#23454;&#29616;&#39640;&#25928;&#30340;&#21452;&#32990;&#32974;&#36801;&#31227;&#65292;&#20197;&#25552;&#20379;&#26080;&#32541;&#27785;&#28024;&#24335;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2401.09680</link><description>&lt;p&gt;
&#23567;&#22411;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#26080;&#20154;&#26426;Metaverse&#20013;&#30340;&#21452;&#32990;&#32974;&#36801;&#31227;&#65306;&#19968;&#31181;&#22810;&#39046;&#23548;&#32773;&#22810;&#20174;&#23646;&#32773;Stackelberg&#21338;&#24328;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Tiny Multi-Agent DRL for Twins Migration in UAV Metaverses: A Multi-Leader Multi-Follower Stackelberg Game Approach. (arXiv:2401.09680v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23567;&#22411;&#26426;&#22120;&#23398;&#20064;&#30340;Stackelberg&#21338;&#24328;&#26694;&#26550;&#65292;&#22312;&#26080;&#20154;&#26426;Metaverse&#20013;&#23454;&#29616;&#39640;&#25928;&#30340;&#21452;&#32990;&#32974;&#36801;&#31227;&#65292;&#20197;&#25552;&#20379;&#26080;&#32541;&#27785;&#28024;&#24335;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#20154;&#26426;&#19982;Metaverse&#30340;&#21327;&#21516;&#20316;&#29992;&#27491;&#22312;&#20652;&#29983;&#19968;&#31181;&#26032;&#20852;&#33539;&#24335;&#65292;&#31216;&#20026;&#26080;&#20154;&#26426;Metaverse&#65292;&#23427;&#21019;&#24314;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#29983;&#24577;&#31995;&#32479;&#65292;&#34701;&#21512;&#20102;&#29289;&#29702;&#21644;&#34394;&#25311;&#31354;&#38388;&#65292;&#25913;&#21464;&#20102;&#26080;&#20154;&#26426;&#30340;&#20132;&#20114;&#21644;&#34394;&#25311;&#25506;&#32034;&#12290;&#26080;&#20154;&#26426;&#21452;&#32990;&#32974;&#65288;UTs&#65289;&#20316;&#20026;&#26080;&#20154;&#26426;&#30340;&#25968;&#23383;&#23402;&#29983;&#21697;&#65292;&#36890;&#36807;&#20351;&#20854;&#26356;&#20855;&#27785;&#28024;&#24863;&#12289;&#30495;&#23454;&#24863;&#21644;&#20449;&#24687;&#20016;&#23500;&#24615;&#65292;&#38761;&#26032;&#26080;&#20154;&#26426;&#24212;&#29992;&#12290;UTs&#37096;&#32626;&#22312;&#22320;&#38754;&#22522;&#31449;&#65288;&#20363;&#22914;&#36947;&#36335;&#36793;&#32536;&#21333;&#20803;&#65288;RSUs&#65289;&#65289;&#19978;&#65292;&#24182;&#36890;&#36807;&#20026;&#26080;&#20154;&#26426;Metaverse&#29992;&#25143;&#65288;UMUs&#65289;&#25552;&#20379;Metaverse&#26381;&#21153;&#12290;&#30001;&#20110;&#26080;&#20154;&#26426;&#30340;&#21160;&#24577;&#31227;&#21160;&#24615;&#21644;RSUs&#30340;&#26377;&#38480;&#36890;&#20449;&#35206;&#30422;&#33539;&#22260;&#65292;&#36827;&#34892;&#23454;&#26102;&#30340;UT&#36801;&#31227;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#30830;&#20445;UMUs&#30340;&#26080;&#32541;&#27785;&#28024;&#24335;&#20307;&#39564;&#12290;&#28982;&#32780;&#65292;&#36873;&#25321;&#21512;&#36866;&#30340;RSUs&#24182;&#20248;&#21270;&#25152;&#38656;&#24102;&#23485;&#23545;&#20110;&#23454;&#29616;&#21487;&#38752;&#39640;&#25928;&#30340;UT&#36801;&#31227;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20462;&#21098;&#25216;&#26415;&#30340;&#23567;&#22411;&#26426;&#22120;&#23398;&#20064;Stackelberg&#21338;&#24328;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;UT&#36801;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
The synergy between Unmanned Aerial Vehicles (UAVs) and metaverses is giving rise to an emerging paradigm named UAV metaverses, which create a unified ecosystem that blends physical and virtual spaces, transforming drone interaction and virtual exploration. UAV Twins (UTs), as the digital twins of UAVs that revolutionize UAV applications by making them more immersive, realistic, and informative, are deployed and updated on ground base stations, e.g., RoadSide Units (RSUs), to offer metaverse services for UAV Metaverse Users (UMUs). Due to the dynamic mobility of UAVs and limited communication coverages of RSUs, it is essential to perform real-time UT migration to ensure seamless immersive experiences for UMUs. However, selecting appropriate RSUs and optimizing the required bandwidth is challenging for achieving reliable and efficient UT migration. To address the challenges, we propose a tiny machine learning-based Stackelberg game framework based on pruning techniques for efficient UT 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#26080;&#30417;&#30563;&#39046;&#22495;&#36716;&#25442;&#20013;&#30340;&#21487;&#35782;&#21035;&#24615;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;MPA&#28040;&#38500;&#29702;&#35770;&#65292;&#35299;&#20915;&#20102;CycleGAN&#21450;&#20854;&#21464;&#20307;&#20135;&#29983;&#20869;&#23481;&#19981;&#23545;&#40784;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.09671</link><description>&lt;p&gt;
&#36808;&#21521;&#21487;&#35782;&#21035;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#36716;&#25442;&#65306;&#19968;&#31181;&#22810;&#26679;&#21270;&#20998;&#24067;&#21305;&#37197;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Identifiable Unsupervised Domain Translation: A Diversified Distribution Matching Approach. (arXiv:2401.09671v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#26080;&#30417;&#30563;&#39046;&#22495;&#36716;&#25442;&#20013;&#30340;&#21487;&#35782;&#21035;&#24615;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;MPA&#28040;&#38500;&#29702;&#35770;&#65292;&#35299;&#20915;&#20102;CycleGAN&#21450;&#20854;&#21464;&#20307;&#20135;&#29983;&#20869;&#23481;&#19981;&#23545;&#40784;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#39046;&#22495;&#36716;&#25442;&#65288;UDT&#65289;&#26088;&#22312;&#25214;&#21040;&#23558;&#19968;&#20010;&#39046;&#22495;&#30340;&#26679;&#26412;&#65288;&#20363;&#22914;&#32032;&#25551;&#65289;&#36716;&#25442;&#20026;&#21478;&#19968;&#20010;&#39046;&#22495;&#65288;&#20363;&#22914;&#29031;&#29255;&#65289;&#30340;&#20989;&#25968;&#65292;&#21516;&#26102;&#19981;&#25913;&#21464;&#39640;&#23618;&#35821;&#20041;&#24847;&#20041;&#65288;&#20063;&#31216;&#20026;&#8220;&#20869;&#23481;&#8221;&#65289;&#12290;&#36825;&#20123;&#36716;&#25442;&#20989;&#25968;&#36890;&#24120;&#36890;&#36807;&#36716;&#25442;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#30340;&#27010;&#29575;&#20998;&#24067;&#26469;&#23547;&#25214;&#12290;CycleGAN&#21487;&#20197;&#35828;&#26159;&#36825;&#19968;&#39046;&#22495;&#20013;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#20013;&#25351;&#20986;CycleGAN&#21450;&#20854;&#21464;&#20307;&#21487;&#33021;&#26080;&#27861;&#35782;&#21035;&#25152;&#38656;&#30340;&#36716;&#25442;&#20989;&#25968;&#65292;&#24182;&#20135;&#29983;&#20869;&#23481;&#19981;&#23545;&#40784;&#30340;&#36716;&#25442;&#12290;&#36825;&#31181;&#23616;&#38480;&#24615;&#28304;&#20110;&#23398;&#20064;&#20934;&#21017;&#35299;&#31354;&#38388;&#20013;&#23384;&#22312;&#22810;&#20010;&#36716;&#25442;&#20989;&#25968;&#65292;&#31216;&#20026;&#8220;&#20445;&#24230;&#33258;&#21516;&#26500;&#65288;MPA&#65289;&#8221;&#12290;&#23613;&#31649;&#24847;&#35782;&#21040;&#20102;&#36825;&#31181;&#21487;&#35782;&#21035;&#24615;&#38382;&#39064;&#65292;&#20294;&#35299;&#20915;&#26041;&#26696;&#20173;&#28982;&#38590;&#20197;&#25214;&#21040;&#12290;&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#31350;&#20102;&#26680;&#24515;&#30340;&#21487;&#35782;&#21035;&#24615;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;MPA&#28040;&#38500;&#29702;&#35770;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Unsupervised domain translation (UDT) aims to find functions that convert samples from one domain (e.g., sketches) to another domain (e.g., photos) without changing the high-level semantic meaning (also referred to as ``content''). The translation functions are often sought by probability distribution matching of the transformed source domain and target domain. CycleGAN stands as arguably the most representative approach among this line of work. However, it was noticed in the literature that CycleGAN and variants could fail to identify the desired translation functions and produce content-misaligned translations. This limitation arises due to the presence of multiple translation functions -- referred to as ``measure-preserving automorphism" (MPA) -- in the solution space of the learning criteria. Despite awareness of such identifiability issues, solutions have remained elusive. This study delves into the core identifiability inquiry and introduces an MPA elimination theory. Our analysi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#30495;&#23454;&#19990;&#30028;&#36712;&#36857;&#25968;&#25454;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20132;&#36890;&#24179;&#28369;&#25511;&#21046;&#22120;&#12290;&#36890;&#36807;&#35266;&#23519;&#21069;&#26041;&#36710;&#36742;&#30340;&#36895;&#24230;&#21644;&#36317;&#31163;&#20197;&#21450;&#20132;&#36890;&#30340;&#19979;&#28216;&#29366;&#24577;&#65292;&#25105;&#20204;&#35757;&#32451;&#20986;&#20102;&#33021;&#22815;&#20943;&#23569;&#33021;&#32791;&#30340;&#27874;&#28010;&#24179;&#28369;&#31574;&#30053;&#65292;&#24182;&#22312;&#20302;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#28183;&#36879;&#29575;&#19979;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#29123;&#27833;&#33410;&#30465;&#12290;</title><link>http://arxiv.org/abs/2401.09666</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#30495;&#23454;&#19990;&#30028;&#36712;&#36857;&#25968;&#25454;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20132;&#36890;&#24179;&#28369;&#25511;&#21046;&#22120;
&lt;/p&gt;
&lt;p&gt;
Traffic Smoothing Controllers for Autonomous Vehicles Using Deep Reinforcement Learning and Real-World Trajectory Data. (arXiv:2401.09666v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#30495;&#23454;&#19990;&#30028;&#36712;&#36857;&#25968;&#25454;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20132;&#36890;&#24179;&#28369;&#25511;&#21046;&#22120;&#12290;&#36890;&#36807;&#35266;&#23519;&#21069;&#26041;&#36710;&#36742;&#30340;&#36895;&#24230;&#21644;&#36317;&#31163;&#20197;&#21450;&#20132;&#36890;&#30340;&#19979;&#28216;&#29366;&#24577;&#65292;&#25105;&#20204;&#35757;&#32451;&#20986;&#20102;&#33021;&#22815;&#20943;&#23569;&#33021;&#32791;&#30340;&#27874;&#28010;&#24179;&#28369;&#31574;&#30053;&#65292;&#24182;&#22312;&#20302;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#28183;&#36879;&#29575;&#19979;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#29123;&#27833;&#33410;&#30465;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#33021;&#22815;&#37096;&#32626;&#21040;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#19978;&#30340;&#20132;&#36890;&#24179;&#28369;&#24033;&#33322;&#25511;&#21046;&#22120;&#26159;&#25913;&#21892;&#20132;&#36890;&#27969;&#37327;&#12289;&#20943;&#23569;&#25317;&#22581;&#21644;&#25552;&#39640;&#29123;&#27833;&#25928;&#29575;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#30000;&#32435;&#35199;&#24030;I-24&#39640;&#36895;&#20844;&#36335;&#19978;&#30340;&#30495;&#23454;&#36712;&#36857;&#25968;&#25454;&#65292;&#22312;&#19968;&#20010;&#21333;&#36710;&#36947;&#27169;&#25311;&#20013;&#22238;&#25918;&#25968;&#25454;&#65292;&#32469;&#36807;&#20102;&#38656;&#35201;&#20180;&#32454;&#35843;&#25972;&#22823;&#22411;&#20132;&#36890;&#24494;&#27169;&#25311;&#22120;&#30340;&#24120;&#35265;&#38382;&#39064;&#12290;&#20351;&#29992;&#26631;&#20934;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#35757;&#32451;&#33021;&#20943;&#23569;&#33021;&#32791;&#30340;&#27874;&#28010;&#24179;&#28369;&#31574;&#30053;&#12290;&#20316;&#20026;&#20195;&#29702;&#30340;&#36755;&#20837;&#65292;&#25105;&#20204;&#35266;&#23519;&#21069;&#26041;&#36710;&#36742;&#30340;&#36895;&#24230;&#21644;&#36317;&#31163;&#65292;&#36825;&#26159;&#22823;&#22810;&#25968;&#26368;&#36817;&#36710;&#36742;&#19978;&#24120;&#35265;&#30340;&#26412;&#22320;&#29366;&#24577;&#65292;&#20197;&#21450;&#20851;&#20110;&#20132;&#36890;&#30340;&#19979;&#28216;&#29366;&#24577;&#30340;&#38750;&#26412;&#22320;&#35266;&#23519;&#12290;&#25105;&#20204;&#26174;&#31034;&#65292;&#22312;&#20302;&#20110;4%&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#28183;&#36879;&#29575;&#19979;&#65292;&#22312;&#20986;&#29616;&#24456;&#22810;&#20572;&#28382;&#27874;&#30340;&#36712;&#36857;&#19978;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#36229;&#36807;15%&#30340;&#26174;&#33879;&#29123;&#27833;&#33410;&#30465;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#25511;&#21046;&#22120;&#30340;&#24179;&#28369;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing traffic-smoothing cruise controllers that can be deployed onto autonomous vehicles is a key step towards improving traffic flow, reducing congestion, and enhancing fuel efficiency in mixed autonomy traffic. We bypass the common issue of having to carefully fine-tune a large traffic microsimulator by leveraging real-world trajectory data from the I-24 highway in Tennessee, replayed in a one-lane simulation. Using standard deep reinforcement learning methods, we train energy-reducing wave-smoothing policies. As an input to the agent, we observe the speed and distance of only the vehicle in front, which are local states readily available on most recent vehicles, as well as non-local observations about the downstream state of the traffic. We show that at a low 4% autonomous vehicle penetration rate, we achieve significant fuel savings of over 15% on trajectories exhibiting many stop-and-go waves. Finally, we analyze the smoothing effect of the controllers and demonstrate robustne
&lt;/p&gt;</description></item><item><title>&#22312;&#36710;&#32852;&#32593;&#20013;&#65292;&#36890;&#36807;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#26412;&#25991;&#35777;&#26126;&#31227;&#21160;&#24615;&#23545;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#30340;&#25910;&#25947;&#36895;&#24230;&#26377;&#31215;&#26497;&#24433;&#21709;&#65292;&#23427;&#22686;&#21152;&#20102;&#36793;&#32536;&#23618;&#24322;&#26500;&#25968;&#25454;&#30340;&#34701;&#21512;&#21644;&#26356;&#24555;&#30340;&#25968;&#25454;&#34701;&#21512;&#36895;&#24230;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.09656</link><description>&lt;p&gt;
&#31227;&#21160;&#24615;&#21152;&#36895;&#23398;&#20064;&#65306;&#36710;&#32852;&#32593;&#20013;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#30340;&#25910;&#25947;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Mobility Accelerates Learning: Convergence Analysis on Hierarchical Federated Learning in Vehicular Networks. (arXiv:2401.09656v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09656
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36710;&#32852;&#32593;&#20013;&#65292;&#36890;&#36807;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#26412;&#25991;&#35777;&#26126;&#31227;&#21160;&#24615;&#23545;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#30340;&#25910;&#25947;&#36895;&#24230;&#26377;&#31215;&#26497;&#24433;&#21709;&#65292;&#23427;&#22686;&#21152;&#20102;&#36793;&#32536;&#23618;&#24322;&#26500;&#25968;&#25454;&#30340;&#34701;&#21512;&#21644;&#26356;&#24555;&#30340;&#25968;&#25454;&#34701;&#21512;&#36895;&#24230;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;(HFL)&#20197;&#38544;&#31169;&#20445;&#25252;&#30340;&#26041;&#24335;&#65292;&#22312;&#22810;&#20010;&#35774;&#22791;&#19978;&#36890;&#36807;&#20960;&#20010;&#36793;&#32536;&#26381;&#21153;&#22120;&#21644;&#19968;&#20010;&#20113;&#36793;&#32536;&#26381;&#21153;&#22120;&#36827;&#34892;&#27169;&#22411;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#12290;&#26412;&#25991;&#32771;&#34385;&#39640;&#24230;&#31227;&#21160;&#30340;&#35774;&#22791;&#65292;&#20027;&#35201;&#38024;&#23545;&#36710;&#32852;&#32593;&#12290;&#36890;&#36807;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#31227;&#21160;&#24615;&#36890;&#36807;&#34701;&#21512;&#36793;&#32536;&#25968;&#25454;&#21644;&#27927;&#29260;&#36793;&#32536;&#27169;&#22411;&#26469;&#24433;&#21709;&#25910;&#25947;&#36895;&#24230;&#12290;&#23613;&#31649;&#20174;&#36890;&#20449;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#31227;&#21160;&#24615;&#36890;&#24120;&#34987;&#35270;&#20026;&#19968;&#20010;&#25361;&#25112;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#23427;&#22686;&#21152;&#20102;&#36793;&#32536;&#23618;&#24322;&#26500;&#25968;&#25454;&#19979;HFL&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#22240;&#20026;&#26356;&#22810;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#21487;&#20197;&#34987;&#21512;&#24182;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#39640;&#36895;&#24230;&#23548;&#33268;&#26356;&#24555;&#30340;&#25910;&#25947;&#65292;&#22240;&#20026;&#23427;&#21152;&#36895;&#20102;&#25968;&#25454;&#30340;&#34701;&#21512;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#31227;&#21160;&#24615;&#21487;&#20197;&#20351;HFL&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#25552;&#39640;&#39640;&#36798;15.1%&#12290;
&lt;/p&gt;
&lt;p&gt;
Hierarchical federated learning (HFL) enables distributed training of models across multiple devices with the help of several edge servers and a cloud edge server in a privacy-preserving manner. In this paper, we consider HFL with highly mobile devices, mainly targeting at vehicular networks. Through convergence analysis, we show that mobility influences the convergence speed by both fusing the edge data and shuffling the edge models. While mobility is usually considered as a challenge from the perspective of communication, we prove that it increases the convergence speed of HFL with edge-level heterogeneous data, since more diverse data can be incorporated. Furthermore, we demonstrate that a higher speed leads to faster convergence, since it accelerates the fusion of data. Simulation results show that mobility increases the model accuracy of HFL by up to 15.1% when training a convolutional neural network on the CIFAR-10 dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20984;&#20108;&#32423;&#20248;&#21270;&#25216;&#26415;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#31070;&#32463;&#21644;&#31526;&#21495;&#21442;&#25968;&#23398;&#20064;&#26694;&#26550;&#65292;&#20855;&#26377;100&#20493;&#20197;&#19978;&#30340;&#23398;&#20064;&#26102;&#38388;&#25913;&#36827;&#21644;&#39640;&#36798;16%&#30340;&#39044;&#27979;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.09651</link><description>&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#25512;&#29702;&#21644;&#23398;&#20064;&#30340;&#20984;&#20108;&#32423;&#20248;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Convex and Bilevel Optimization for Neuro-Symbolic Inference and Learning. (arXiv:2401.09651v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20984;&#20108;&#32423;&#20248;&#21270;&#25216;&#26415;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#31070;&#32463;&#21644;&#31526;&#21495;&#21442;&#25968;&#23398;&#20064;&#26694;&#26550;&#65292;&#20855;&#26377;100&#20493;&#20197;&#19978;&#30340;&#23398;&#20064;&#26102;&#38388;&#25913;&#36827;&#21644;&#39640;&#36798;16%&#30340;&#39044;&#27979;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#20984;&#20108;&#32423;&#20248;&#21270;&#25216;&#26415;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#31070;&#32463;&#31526;&#21495;&#31995;&#32479;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#21644;&#31526;&#21495;&#21442;&#25968;&#23398;&#20064;&#26694;&#26550;&#12290;&#25105;&#20204;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#31526;&#21495;&#20307;&#31995;&#32467;&#26500;NeuPSL&#26469;&#35777;&#26126;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NeuPSL&#25512;&#29702;&#30340;&#24179;&#28369;&#21407;&#22987;&#21644;&#23545;&#20598;&#24418;&#24335;&#65292;&#24182;&#26174;&#31034;&#23398;&#20064;&#26799;&#24230;&#26159;&#26368;&#20248;&#23545;&#20598;&#21464;&#37327;&#30340;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#26032;&#30340;&#24418;&#24335;&#24320;&#21457;&#20102;&#19968;&#31181;&#23545;&#20598;&#22359;&#22352;&#26631;&#19979;&#38477;&#31639;&#27861;&#65292;&#33258;&#28982;&#22320;&#21033;&#29992;&#20102;&#28909;&#21551;&#21160;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#30456;&#27604;&#24403;&#21069;&#26368;&#22909;&#30340;NeuPSL&#25512;&#29702;&#26041;&#27861;&#30340;&#23398;&#20064;&#26102;&#38388;&#25913;&#36827;&#20102;100&#20493;&#20197;&#19978;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#28085;&#30422;&#21508;&#31181;&#20219;&#21153;&#30340;8&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#23398;&#20064;&#26694;&#26550;&#30456;&#27604;&#26367;&#20195;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#25552;&#21319;&#39640;&#36798;16%&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address a key challenge for neuro-symbolic (NeSy) systems by leveraging convex and bilevel optimization techniques to develop a general gradient-based framework for end-to-end neural and symbolic parameter learning. The applicability of our framework is demonstrated with NeuPSL, a state-of-the-art NeSy architecture. To achieve this, we propose a smooth primal and dual formulation of NeuPSL inference and show learning gradients are functions of the optimal dual variables. Additionally, we develop a dual block coordinate descent algorithm for the new formulation that naturally exploits warm-starts. This leads to over 100x learning runtime improvements over the current best NeuPSL inference method. Finally, we provide extensive empirical evaluations across $8$ datasets covering a range of tasks and demonstrate our learning framework achieves up to a 16% point prediction performance improvement over alternative learning methods.
&lt;/p&gt;</description></item><item><title>ClimateGPT&#26159;&#19968;&#20010;&#38024;&#23545;&#27668;&#20505;&#21464;&#21270;&#39046;&#22495;&#30340;&#36328;&#23398;&#31185;&#30740;&#31350;&#21512;&#25104;&#30340;AI&#27169;&#22411;&#65292;&#36890;&#36807;&#20248;&#21270;&#26816;&#32034;&#22686;&#24378;&#21644;&#20351;&#29992;&#32423;&#32852;&#26426;&#22120;&#32763;&#35793;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#21487;&#35775;&#38382;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.09646</link><description>&lt;p&gt;
ClimateGPT: &#23454;&#29616;&#23545;&#27668;&#20505;&#21464;&#21270;&#39046;&#22495;&#30340;&#36328;&#23398;&#31185;&#30740;&#31350;&#36827;&#34892;&#21512;&#25104;&#30340;AI&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ClimateGPT: Towards AI Synthesizing Interdisciplinary Research on Climate Change. (arXiv:2401.09646v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09646
&lt;/p&gt;
&lt;p&gt;
ClimateGPT&#26159;&#19968;&#20010;&#38024;&#23545;&#27668;&#20505;&#21464;&#21270;&#39046;&#22495;&#30340;&#36328;&#23398;&#31185;&#30740;&#31350;&#21512;&#25104;&#30340;AI&#27169;&#22411;&#65292;&#36890;&#36807;&#20248;&#21270;&#26816;&#32034;&#22686;&#24378;&#21644;&#20351;&#29992;&#32423;&#32852;&#26426;&#22120;&#32763;&#35793;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#21487;&#35775;&#38382;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ClimateGPT&#65292;&#19968;&#31181;&#29305;&#23450;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#29992;&#20110;&#21512;&#25104;&#27668;&#20505;&#21464;&#21270;&#30340;&#36328;&#23398;&#31185;&#30740;&#31350;&#12290;&#25105;&#20204;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#20102;&#20004;&#20010;7B&#27169;&#22411;&#65292;&#35757;&#32451;&#25968;&#25454;&#38598;&#21253;&#21547;300B&#20010;&#31185;&#23398;&#23548;&#21521;&#30340;&#20196;&#29260;&#12290;&#31532;&#19968;&#20010;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#21253;&#21547;&#20102;4.2B&#20010;&#29305;&#23450;&#39046;&#22495;&#30340;&#20196;&#29260;&#65292;&#31532;&#20108;&#20010;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#21518;&#38024;&#23545;&#27668;&#20505;&#39046;&#22495;&#36827;&#34892;&#20102;&#35843;&#25972;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;ClimateGPT-7B&#65292;13B&#21644;70B&#36827;&#34892;&#20102;&#36830;&#32493;&#39044;&#35757;&#32451;&#65292;&#35757;&#32451;&#25968;&#25454;&#38598;&#21253;&#21547;4.2B&#20010;&#29305;&#23450;&#39046;&#22495;&#30340;&#20196;&#29260;&#65292;&#24182;&#19982;&#27668;&#20505;&#31185;&#23398;&#23478;&#32039;&#23494;&#21512;&#20316;&#21019;&#24314;&#12290;&#20026;&#20102;&#20943;&#23569;&#34394;&#26500;&#29983;&#25104;&#30340;&#25968;&#37327;&#65292;&#25105;&#20204;&#20026;&#27169;&#22411;&#36827;&#34892;&#20102;&#26816;&#32034;&#22686;&#24378;&#20248;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#26816;&#32034;&#31574;&#30053;&#12290;&#20026;&#20102;&#25552;&#39640;&#25105;&#20204;&#27169;&#22411;&#23545;&#38750;&#33521;&#35821;&#20351;&#29992;&#32773;&#30340;&#21487;&#35775;&#38382;&#24615;&#65292;&#25105;&#20204;&#24314;&#35758;&#21033;&#29992;&#32423;&#32852;&#26426;&#22120;&#32763;&#35793;&#65292;&#24182;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#19982;&#32763;&#35793;&#30340;&#24615;&#33021;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces ClimateGPT, a model family of domain-specific large language models that synthesize interdisciplinary research on climate change. We trained two 7B models from scratch on a science-oriented dataset of 300B tokens. For the first model, the 4.2B domain-specific tokens were included during pre-training and the second was adapted to the climate domain after pre-training. Additionally, ClimateGPT-7B, 13B and 70B are continuously pre-trained from Llama~2 on a domain-specific dataset of 4.2B tokens. Each model is instruction fine-tuned on a high-quality and human-generated domain-specific dataset that has been created in close cooperation with climate scientists. To reduce the number of hallucinations, we optimize the model for retrieval augmentation and propose a hierarchical retrieval strategy. To increase the accessibility of our model to non-English speakers, we propose to make use of cascaded machine translation and show that this approach can perform comparably to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#29289;&#29702;&#24341;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#20256;&#36755;&#32593;&#32476;&#30340;&#28526;&#27969;&#28789;&#25935;&#24230;&#22240;&#23376;&#26469;&#25351;&#23548;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#36890;&#36807;&#23454;&#26102;&#34917;&#25937;&#21069;&#30651;&#20915;&#31574;&#26469;&#20943;&#36731;&#40657;&#26263;&#27169;&#24335;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2401.09640</link><description>&lt;p&gt;
&#36890;&#36807;&#29289;&#29702;&#24341;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#20572;&#30005;&#20943;&#36731;
&lt;/p&gt;
&lt;p&gt;
Blackout Mitigation via Physics-guided RL. (arXiv:2401.09640v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#29289;&#29702;&#24341;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#20256;&#36755;&#32593;&#32476;&#30340;&#28526;&#27969;&#28789;&#25935;&#24230;&#22240;&#23376;&#26469;&#25351;&#23548;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#36890;&#36807;&#23454;&#26102;&#34917;&#25937;&#21069;&#30651;&#20915;&#31574;&#26469;&#20943;&#36731;&#40657;&#26263;&#27169;&#24335;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#20026;&#20102;&#38450;&#27490;&#40657;&#26263;&#27169;&#24335;&#32780;&#22312;&#31995;&#32479;&#24322;&#24120;&#26102;&#36827;&#34892;&#24207;&#21015;&#35774;&#35745;&#30340;&#34917;&#25937;&#25511;&#21046;&#34892;&#21160;&#12290;&#35774;&#35745;&#20102;&#19968;&#31181;&#29289;&#29702;&#24341;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35782;&#21035;&#22312;&#32771;&#34385;&#31995;&#32479;&#31283;&#23450;&#24615;&#38271;&#26399;&#24433;&#21709;&#30340;&#24773;&#20917;&#19979;&#30340;&#23454;&#26102;&#34917;&#25937;&#21069;&#30651;&#20915;&#31574;&#24207;&#21015;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#28041;&#21450;&#31163;&#25955;&#20540;&#20256;&#36755;&#32447;&#24320;&#20851;&#20915;&#31574;&#65288;&#32447;&#36335;&#37325;&#26032;&#36830;&#25509;&#21644;&#31227;&#38500;&#65289;&#21644;&#36830;&#32493;&#20540;&#21457;&#30005;&#26426;&#35843;&#25972;&#30340;&#25511;&#21046;&#34892;&#21160;&#31354;&#38388;&#12290;&#20026;&#20102;&#30830;&#23450;&#26377;&#25928;&#30340;&#20572;&#30005;&#20943;&#36731;&#31574;&#30053;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#29289;&#29702;&#24341;&#23548;&#26041;&#27861;&#65292;&#21033;&#29992;&#19982;&#30005;&#21147;&#20256;&#36755;&#32593;&#32476;&#30456;&#20851;&#30340;&#28526;&#27969;&#28789;&#25935;&#24230;&#22240;&#23376;&#26469;&#24341;&#23548;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#26399;&#38388;&#30340;&#25506;&#32034;&#12290;&#20351;&#29992;&#24320;&#28304;Grid2Op&#24179;&#21488;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#23558;&#29289;&#29702;&#20449;&#21495;&#32435;&#20837;&#24378;&#21270;&#23398;&#20064;&#20915;&#31574;&#30340;&#26174;&#33879;&#20248;&#21183;&#65292;&#35777;&#23454;&#20102;&#25152;&#25552;&#20986;&#30340;&#29289;&#29702;&#24341;&#23548;&#26041;&#27861;&#30340;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers the sequential design of remedial control actions in response to system anomalies for the ultimate objective of preventing blackouts. A physics-guided reinforcement learning (RL) framework is designed to identify effective sequences of real-time remedial look-ahead decisions accounting for the long-term impact on the system's stability. The paper considers a space of control actions that involve both discrete-valued transmission line-switching decisions (line reconnections and removals) and continuous-valued generator adjustments. To identify an effective blackout mitigation policy, a physics-guided approach is designed that uses power-flow sensitivity factors associated with the power transmission network to guide the RL exploration during agent training. Comprehensive empirical evaluations using the open-source Grid2Op platform demonstrate the notable advantages of incorporating physical signals into RL decisions, establishing the gains of the proposed physics-gu
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36741;&#21161;&#38405;&#35835;&#20020;&#24202;&#31508;&#35760;&#65292;&#24739;&#32773;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#29702;&#35299;&#21644;&#33258;&#20449;&#12290;&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#24037;&#20855;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#31616;&#21270;&#21644;&#22686;&#21152;&#19978;&#19979;&#25991;&#65292;&#20351;&#20020;&#24202;&#31508;&#35760;&#26356;&#26131;&#35835;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#22686;&#24378;&#23545;&#24739;&#32773;&#26377;&#30410;&#12290;</title><link>http://arxiv.org/abs/2401.09637</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36741;&#21161;&#23545;&#24739;&#32773;&#38405;&#35835;&#20020;&#24202;&#31508;&#35760;&#30340;&#24433;&#21709;&#65306;&#19968;&#20010;&#28151;&#21512;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Impact of Large Language Model Assistance on Patients Reading Clinical Notes: A Mixed-Methods Study. (arXiv:2401.09637v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09637
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36741;&#21161;&#38405;&#35835;&#20020;&#24202;&#31508;&#35760;&#65292;&#24739;&#32773;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#29702;&#35299;&#21644;&#33258;&#20449;&#12290;&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#24037;&#20855;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#31616;&#21270;&#21644;&#22686;&#21152;&#19978;&#19979;&#25991;&#65292;&#20351;&#20020;&#24202;&#31508;&#35760;&#26356;&#26131;&#35835;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#22686;&#24378;&#23545;&#24739;&#32773;&#26377;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24739;&#32773;&#36890;&#36807;&#38405;&#35835;&#20182;&#20204;&#30340;&#20020;&#24202;&#31508;&#35760;&#33719;&#24471;&#20102;&#35768;&#22810;&#22909;&#22788;&#65292;&#21253;&#25324;&#22686;&#21152;&#23545;&#33258;&#36523;&#20581;&#24247;&#30340;&#25511;&#21046;&#24863;&#21644;&#23545;&#25252;&#29702;&#35745;&#21010;&#30340;&#29702;&#35299;&#25552;&#39640;&#12290;&#28982;&#32780;&#65292;&#22312;&#20020;&#24202;&#31508;&#35760;&#20013;&#22797;&#26434;&#30340;&#21307;&#23398;&#27010;&#24565;&#21644;&#26415;&#35821;&#38459;&#30861;&#20102;&#24739;&#32773;&#30340;&#29702;&#35299;&#65292;&#24182;&#21487;&#33021;&#23548;&#33268;&#28966;&#34385;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#38754;&#21521;&#24739;&#32773;&#30340;&#24037;&#20855;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#31616;&#21270;&#31508;&#35760;&#12289;&#20174;&#20013;&#25552;&#21462;&#20449;&#24687;&#24182;&#22686;&#21152;&#19978;&#19979;&#25991;&#65292;&#20197;&#20351;&#20020;&#24202;&#31508;&#35760;&#26356;&#26131;&#35835;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#24037;&#20855;&#25552;&#31034;&#25913;&#36827;&#30340;GPT-4&#23545;&#30001;&#20083;&#33146;&#30284;&#24184;&#23384;&#32773;&#25424;&#36192;&#30340;&#30495;&#23454;&#20020;&#24202;&#31508;&#35760;&#21644;&#20020;&#24202;&#21307;&#29983;&#29983;&#25104;&#30340;&#21512;&#25104;&#20020;&#24202;&#31508;&#35760;&#36827;&#34892;&#36825;&#20123;&#22686;&#24378;&#20219;&#21153;&#12290;&#20849;&#26377;12&#26465;&#31508;&#35760;&#65292;3868&#20010;&#23383;&#12290;2023&#24180;6&#26376;&#65292;&#25105;&#20204;&#38543;&#26426;&#20998;&#37197;&#20102;200&#21517;&#32654;&#22269;&#22899;&#24615;&#21442;&#19982;&#32773;&#65292;&#24182;&#21521;&#20182;&#20204;&#20998;&#21457;&#20102;&#19977;&#20010;&#20855;&#26377;&#19981;&#21516;&#31243;&#24230;&#22686;&#24378;&#30340;&#20020;&#24202;&#31508;&#35760;&#12290;&#21442;&#19982;&#32773;&#22238;&#31572;&#20102;&#26377;&#20851;&#27599;&#20010;&#31508;&#35760;&#30340;&#38382;&#39064;&#65292;&#35780;&#20272;&#20102;&#20182;&#20204;&#23545;&#21518;&#32493;&#34892;&#21160;&#30340;&#29702;&#35299;&#21644;&#33258;&#25105;&#25253;&#21578;&#30340;&#33258;&#20449;&#24515;&#12290;&#25105;&#20204;&#21457;&#29616;&#22686;&#24378;&#23545;&#38405;&#35835;&#29702;&#35299;&#21644;&#33258;&#20449;&#24515;&#21451;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Patients derive numerous benefits from reading their clinical notes, including an increased sense of control over their health and improved understanding of their care plan. However, complex medical concepts and jargon within clinical notes hinder patient comprehension and may lead to anxiety. We developed a patient-facing tool to make clinical notes more readable, leveraging large language models (LLMs) to simplify, extract information from, and add context to notes. We prompt engineered GPT-4 to perform these augmentation tasks on real clinical notes donated by breast cancer survivors and synthetic notes generated by a clinician, a total of 12 notes with 3868 words. In June 2023, 200 female-identifying US-based participants were randomly assigned three clinical notes with varying levels of augmentations using our tool. Participants answered questions about each note, evaluating their understanding of follow-up actions and self-reported confidence. We found that augmentations were ass
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#20351;&#29992;&#25463;&#24452;&#23398;&#20064;&#30340;&#29616;&#35937;&#65292;&#24378;&#35843;&#20102;&#36825;&#31181;&#29616;&#35937;&#23545;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#24433;&#21709;&#65292;&#24182;&#21628;&#21505;&#21152;&#22823;&#23545;&#25463;&#24452;&#23398;&#20064;&#30340;&#30740;&#31350;&#21147;&#24230;&#20197;&#25552;&#21319;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#35780;&#20272;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2401.09615</link><description>&lt;p&gt;
&#23398;&#20064;&#25463;&#24452;&#65306;&#20851;&#20110;&#35821;&#35328;&#27169;&#22411;&#20013;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#35823;&#23548;&#24615;&#25215;&#35834;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Learning Shortcuts: On the Misleading Promise of NLU in Language Models. (arXiv:2401.09615v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09615
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#20351;&#29992;&#25463;&#24452;&#23398;&#20064;&#30340;&#29616;&#35937;&#65292;&#24378;&#35843;&#20102;&#36825;&#31181;&#29616;&#35937;&#23545;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#24433;&#21709;&#65292;&#24182;&#21628;&#21505;&#21152;&#22823;&#23545;&#25463;&#24452;&#23398;&#20064;&#30340;&#30740;&#31350;&#21147;&#24230;&#20197;&#25552;&#21319;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#35780;&#20272;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;LLMs&#22312;&#25191;&#34892;&#20219;&#21153;&#26102;&#24120;&#24120;&#37319;&#29992;&#25463;&#24452;&#65292;&#23548;&#33268;&#22312;&#20915;&#31574;&#35268;&#21017;&#19978;&#32570;&#20047;&#27867;&#21270;&#33021;&#21147;&#65292;&#20174;&#32780;&#22312;&#24615;&#33021;&#19978;&#20135;&#29983;&#20102;&#19968;&#31181;&#38169;&#35273;&#12290;&#36825;&#19968;&#29616;&#35937;&#22312;&#20934;&#30830;&#35780;&#20272;LLMs&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#19978;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#23545;&#35813;&#39046;&#22495;&#30340;&#30456;&#20851;&#30740;&#31350;&#36827;&#34892;&#20102;&#31616;&#27905;&#30340;&#27010;&#36848;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#20351;&#29992;&#25463;&#24452;&#23398;&#20064;&#30340;&#24433;&#21709;&#30340;&#35266;&#28857;&#12290;&#26412;&#25991;&#21628;&#21505;&#21152;&#22823;&#23545;&#25463;&#24452;&#23398;&#20064;&#30340;&#28145;&#20837;&#29702;&#35299;&#30340;&#30740;&#31350;&#21147;&#24230;&#65292;&#20026;&#24320;&#21457;&#26356;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#25552;&#39640;&#30495;&#23454;&#22330;&#26223;&#19979;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#35780;&#20272;&#30340;&#26631;&#20934;&#20316;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of large language models (LLMs) has enabled significant performance gains in the field of natural language processing. However, recent studies have found that LLMs often resort to shortcuts when performing tasks, creating an illusion of enhanced performance while lacking generalizability in their decision rules. This phenomenon introduces challenges in accurately assessing natural language understanding in LLMs. Our paper provides a concise survey of relevant research in this area and puts forth a perspective on the implications of shortcut learning in the evaluation of language models, specifically for NLU tasks. This paper urges more research efforts to be put towards deepening our comprehension of shortcut learning, contributing to the development of more robust language models, and raising the standards of NLU evaluation in real-world scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#21019;&#26032;&#25216;&#26415;&#26469;&#35299;&#20915;&#24314;&#35758;&#31995;&#32479;&#20013;&#39640;&#22522;&#25968;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#37319;&#29992;&#35789;&#34955;&#27169;&#22411;&#21644;&#23618;&#20849;&#20139;&#26469;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;&#36890;&#36807;&#23545;Uber&#20351;&#29992;&#24773;&#20917;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#20248;&#21270;&#24314;&#35758;&#31995;&#32479;&#21644;&#25552;&#39640;&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.09572</link><description>&lt;p&gt;
&#22788;&#29702;&#24314;&#35758;&#31995;&#32479;&#20013;&#22823;&#35268;&#27169;&#22522;&#25968;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Handling Large-scale Cardinality in building recommendation systems. (arXiv:2401.09572v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#21019;&#26032;&#25216;&#26415;&#26469;&#35299;&#20915;&#24314;&#35758;&#31995;&#32479;&#20013;&#39640;&#22522;&#25968;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#37319;&#29992;&#35789;&#34955;&#27169;&#22411;&#21644;&#23618;&#20849;&#20139;&#26469;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;&#36890;&#36807;&#23545;Uber&#20351;&#29992;&#24773;&#20917;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#20248;&#21270;&#24314;&#35758;&#31995;&#32479;&#21644;&#25552;&#39640;&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#24314;&#35758;&#31995;&#32479;&#20381;&#36182;&#20110;&#25429;&#25417;&#29992;&#25143;&#20559;&#22909;&#65292;&#36890;&#24120;&#38656;&#35201;&#21253;&#21547;&#26080;&#25968;&#23454;&#20307;&#30340;&#21807;&#19968;&#26631;&#35782;&#31526;&#65288;UUID&#65289;&#31561;&#22810;&#31181;&#21151;&#33021;&#12290;&#28982;&#32780;&#65292;UUID&#30340;&#24322;&#24120;&#39640;&#22522;&#25968;&#22312;&#27169;&#22411;&#36864;&#21270;&#21644;&#31232;&#30095;&#24615;&#23548;&#33268;&#27169;&#22411;&#22823;&#23567;&#22686;&#21152;&#26041;&#38754;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#21019;&#26032;&#25216;&#26415;&#26469;&#35299;&#20915;&#24314;&#35758;&#31995;&#32479;&#20013;&#39640;&#22522;&#25968;&#30340;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35789;&#34955;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#23618;&#20849;&#20139;&#65292;&#20197;&#26174;&#33879;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;Uber&#20351;&#29992;&#24773;&#20917;&#36827;&#34892;&#31163;&#32447;&#21644;&#22312;&#32447;&#23454;&#39564;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#25216;&#26415;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20248;&#21270;&#24314;&#35758;&#31995;&#32479;&#21644;&#25552;&#39640;&#20854;&#25972;&#20307;&#24615;&#33021;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective recommendation systems rely on capturing user preferences, often requiring incorporating numerous features such as universally unique identifiers (UUIDs) of entities. However, the exceptionally high cardinality of UUIDs poses a significant challenge in terms of model degradation and increased model size due to sparsity. This paper presents two innovative techniques to address the challenge of high cardinality in recommendation systems. Specifically, we propose a bag-of-words approach, combined with layer sharing, to substantially decrease the model size while improving performance. Our techniques were evaluated through offline and online experiments on Uber use cases, resulting in promising results demonstrating our approach's effectiveness in optimizing recommendation systems and enhancing their overall performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#21453;&#20107;&#23454;&#23545;&#25239;&#20248;&#21270;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#39118;&#26684;&#23545;&#40784;&#65292;&#36991;&#20813;&#20154;&#31867;&#24178;&#39044;&#65292;&#24182;&#25104;&#21151;&#22521;&#20859;&#20986;&#21487;&#21462;&#34892;&#20026;&#21644;&#20943;&#36731;&#19981;&#21487;&#21462;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2401.09566</link><description>&lt;p&gt;
&#20351;&#29992;&#21453;&#20107;&#23454;&#23545;&#25239;&#20248;&#21270;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Aligning Large Language Models with Counterfactual DPO. (arXiv:2401.09566v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#21453;&#20107;&#23454;&#23545;&#25239;&#20248;&#21270;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#39118;&#26684;&#23545;&#40784;&#65292;&#36991;&#20813;&#20154;&#31867;&#24178;&#39044;&#65292;&#24182;&#25104;&#21151;&#22521;&#20859;&#20986;&#21487;&#21462;&#34892;&#20026;&#21644;&#20943;&#36731;&#19981;&#21487;&#21462;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#36827;&#27493;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#29983;&#25104;&#19978;&#19979;&#25991;&#36830;&#36143;&#19988;&#28085;&#30422;&#24191;&#27867;&#20027;&#39064;&#30340;&#25991;&#26412;&#34917;&#20840;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#35757;&#32451;&#25152;&#38656;&#30340;&#22823;&#37327;&#25968;&#25454;&#20351;&#24471;&#22312;&#39044;&#35757;&#32451;&#21644;&#25351;&#20196;&#35843;&#25972;&#38454;&#27573;&#23545;&#40784;&#21709;&#24212;&#39118;&#26684;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#36890;&#24120;&#20250;&#37319;&#29992;&#39069;&#22806;&#30340;&#23545;&#40784;&#38454;&#27573;&#65292;&#36827;&#19968;&#27493;&#20351;&#29992;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#23545;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#26356;&#22909;&#22320;&#23558;&#20854;&#36755;&#20986;&#19982;&#20154;&#31867;&#26399;&#26395;&#23545;&#40784;&#12290;&#34429;&#28982;&#36825;&#20010;&#36807;&#31243;&#26412;&#36523;&#24182;&#27809;&#26377;&#24341;&#20837;&#26032;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#31361;&#20986;&#20102;&#27169;&#22411;&#22266;&#26377;&#30340;&#29983;&#25104;&#39118;&#26684;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;(DPO)&#26694;&#26550;&#20869;&#21033;&#29992;&#21453;&#20107;&#23454;&#25552;&#31034;&#26469;&#23545;&#40784;&#27169;&#22411;&#30340;&#39118;&#26684;&#65292;&#32780;&#19981;&#20381;&#36182;&#20154;&#31867;&#24178;&#39044;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#22320;&#22521;&#20859;&#20102;&#21487;&#21462;&#30340;&#34892;&#20026;&#65292;&#20943;&#36731;&#20102;&#19981;&#21487;&#21462;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advancements in large language models (LLMs) have demonstrated remarkable capabilities across a diverse range of applications. These models excel in generating text completions that are contextually coherent and cover an extensive array of subjects. However, the vast datasets required for their training make aligning response styles during the pretraining and instruction tuning phases challenging. Consequently, an additional alignment phase is typically employed, wherein the model is further trained with human preference data to better align its outputs with human expectations. While this process doesn't introduce new capabilities per se, it does accentuate generation styles innate to the model. This paper explores the utilization of counterfactual prompting within the framework of Direct Preference Optimization (DPO) to align the model's style without relying on human intervention. We demonstrate that this method effectively instils desirable behaviour, mitigates undesirable ones, and
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26469;&#39044;&#27979;&#27963;&#21160;&#32500;&#24230;&#65292;&#20174;&#32780;&#26368;&#22823;&#21270;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#20986;&#29616;&#39057;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.09556</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22686;&#24378;&#30340;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;: &#23398;&#20064;&#20943;&#23569;&#27169;&#22411;&#32500;&#24230;
&lt;/p&gt;
&lt;p&gt;
Deep learning enhanced mixed integer optimization: Learning to reduce model dimensionality. (arXiv:2401.09556v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09556
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26469;&#39044;&#27979;&#27963;&#21160;&#32500;&#24230;&#65292;&#20174;&#32780;&#26368;&#22823;&#21270;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#20986;&#29616;&#39057;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#27169;&#22411;&#20013;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;(ANN)&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#22312;&#36817;&#20284;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#38382;&#39064;&#20013;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#21033;&#29992;&#22810;&#26631;&#31614;&#20998;&#31867;&#26469;&#32771;&#34385;&#22810;&#20010;&#27963;&#21160;&#32500;&#24230;&#12290;&#20026;&#20102;&#25552;&#39640;&#26694;&#26550;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#37319;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#36827;&#34892;&#36229;&#21442;&#25968;&#35843;&#20248;&#65292;&#20197;&#26368;&#22823;&#21270;&#26679;&#26412;&#32423;&#20934;&#30830;&#24615;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20934;&#30830;&#22320;&#39044;&#27979;&#25152;&#26377;&#27963;&#21160;&#32500;&#24230;&#65292;&#20174;&#32780;&#26368;&#22823;&#21270;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#20986;&#29616;&#39057;&#29575;&#12290;&#25105;&#20204;&#23558;&#35813;&#26694;&#26550;&#24212;&#29992;&#20110;&#25551;&#36848;&#20010;&#24615;&#21270;&#21307;&#23398;&#20379;&#24212;&#38142;&#20013;&#30340;&#38271;&#26399;&#25237;&#36164;&#35268;&#21010;&#21644;&#20013;&#26399;&#25112;&#26415;&#35268;&#21010;&#30340;&#22522;&#20110;&#27969;&#30340;&#35774;&#26045;&#20301;&#32622;&#20998;&#37197;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;(MILP)&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces a framework to address the computational complexity inherent in Mixed-Integer Programming (MIP) models by harnessing the potential of deep learning. We compare the effectiveness of (a) feed-forward neural networks (ANN) and (b) convolutional neural networks (CNN) in approximating the active dimensions within MIP problems. We utilize multi-label classification to account for more than one active dimension. To enhance the framework's performance, we employ Bayesian optimization for hyperparameter tuning, aiming to maximize sample-level accuracy. The primary objective is to train the neural networks to predict all active dimensions accurately, thereby maximizing the occurrence of global optimum solutions. We apply this framework to a flow-based facility location allocation Mixed-Integer Linear Programming (MILP) formulation that describes long-term investment planning and medium-term tactical planning in a personalized medicine supply chain for cell therapy manufactur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#26469;&#25913;&#36827;&#20998;&#31867;&#27169;&#22411;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#20351;&#29992;&#23569;&#37327;&#26377;&#26631;&#31614;&#31034;&#20363;&#65292;&#36890;&#36807;&#36830;&#32493;&#21453;&#39304;&#24490;&#29615;&#65292;&#25105;&#20204;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#36229;&#36234;&#38646;&#26679;&#26412;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#26356;&#24378;&#30340;&#25991;&#26412;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.09555</link><description>&lt;p&gt;
&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#25913;&#36827;&#20998;&#31867;&#24615;&#33021;&#65306;&#26631;&#35760;&#19968;&#20123;&#65292;&#25105;&#20204;&#26631;&#35760;&#20854;&#20313;&#37096;&#20998;
&lt;/p&gt;
&lt;p&gt;
Improving Classification Performance With Human Feedback: Label a few, we label the rest. (arXiv:2401.09555v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#26469;&#25913;&#36827;&#20998;&#31867;&#27169;&#22411;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#20351;&#29992;&#23569;&#37327;&#26377;&#26631;&#31614;&#31034;&#20363;&#65292;&#36890;&#36807;&#36830;&#32493;&#21453;&#39304;&#24490;&#29615;&#65292;&#25105;&#20204;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#36229;&#36234;&#38646;&#26679;&#26412;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#26356;&#24378;&#30340;&#25991;&#26412;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#22823;&#37096;&#20998;&#25968;&#25454;&#26159;&#38750;&#32467;&#26500;&#21270;&#30340;&#65292;&#22240;&#27492;&#33719;&#21462;&#36275;&#22815;&#30340;&#26377;&#26631;&#31614;&#25968;&#25454;&#26469;&#35757;&#32451;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#23569;&#26679;&#26412;&#23398;&#20064;&#21644;&#20027;&#21160;&#23398;&#20064;&#65292;&#21363;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#26469;&#25913;&#36827;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#20165;&#20351;&#29992;&#19968;&#23567;&#37096;&#20998;&#26377;&#26631;&#31614;&#31034;&#20363;&#12290;&#26412;&#25991;&#30528;&#37325;&#20110;&#29702;&#35299;&#36830;&#32493;&#21453;&#39304;&#24490;&#29615;&#22914;&#20309;&#25913;&#21892;&#27169;&#22411;&#65292;&#20174;&#32780;&#36890;&#36807;&#28176;&#36827;&#24335;&#30340;&#20154;&#31867;&#21442;&#19982;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12289;&#22238;&#24402;&#21644;&#31934;&#30830;&#24230;&#12290;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT-3.5&#12289;BERT&#21644;SetFit&#65292;&#25105;&#20204;&#26088;&#22312;&#20998;&#26512;&#20351;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#26377;&#26631;&#31614;&#31034;&#20363;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#22312;Financial Phrasebank&#12289;Banking&#12289;Craigslist&#12289;Trec&#21644;Amazon Reviews&#25968;&#25454;&#38598;&#19978;&#23545;&#27492;&#26041;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#35777;&#26126;&#20165;&#20351;&#29992;&#23569;&#37327;&#26377;&#26631;&#31614;&#31034;&#20363;&#23601;&#33021;&#36229;&#36234;&#38646;&#26679;&#26412;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#25552;&#20379;&#22686;&#24378;&#30340;&#25991;&#26412;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of artificial intelligence, where a vast majority of data is unstructured, obtaining substantial amounts of labeled data to train supervised machine learning models poses a significant challenge. To address this, we delve into few-shot and active learning, where are goal is to improve AI models with human feedback on a few labeled examples. This paper focuses on understanding how a continuous feedback loop can refine models, thereby enhancing their accuracy, recall, and precision through incremental human input. By employing Large Language Models (LLMs) such as GPT-3.5, BERT, and SetFit, we aim to analyze the efficacy of using a limited number of labeled examples to substantially improve model accuracy. We benchmark this approach on the Financial Phrasebank, Banking, Craigslist, Trec, Amazon Reviews datasets to prove that with just a few labeled examples, we are able to surpass the accuracy of zero shot large language models to provide enhanced text classification performa
&lt;/p&gt;</description></item><item><title>BERTologyNavigator&#26159;&#19968;&#20010;&#22522;&#20110;BERT&#35821;&#20041;&#30340;&#39640;&#32423;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#65292;&#32467;&#21512;&#20851;&#31995;&#25277;&#21462;&#21644;BERT&#23884;&#20837;&#65292;&#21487;&#20197;&#22312;DBLP&#30693;&#35782;&#22270;&#35889;&#20013;&#31934;&#30830;&#22320;&#23548;&#33322;&#20851;&#31995;&#65292;&#24182;&#22312;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#36739;&#39640;&#30340;F1&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2401.09553</link><description>&lt;p&gt;
BERTologyNavigator: &#22522;&#20110;BERT&#35821;&#20041;&#30340;&#39640;&#32423;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
BERTologyNavigator: Advanced Question Answering with BERT-based Semantics. (arXiv:2401.09553v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09553
&lt;/p&gt;
&lt;p&gt;
BERTologyNavigator&#26159;&#19968;&#20010;&#22522;&#20110;BERT&#35821;&#20041;&#30340;&#39640;&#32423;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#65292;&#32467;&#21512;&#20851;&#31995;&#25277;&#21462;&#21644;BERT&#23884;&#20837;&#65292;&#21487;&#20197;&#22312;DBLP&#30693;&#35782;&#22270;&#35889;&#20013;&#31934;&#30830;&#22320;&#23548;&#33322;&#20851;&#31995;&#65292;&#24182;&#22312;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#36739;&#39640;&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#21457;&#21644;&#38598;&#25104;&#22312;&#20154;&#24037;&#26234;&#33021;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;BERTologyNavigator&#8212;&#8212;&#19968;&#20010;&#23558;&#20851;&#31995;&#25277;&#21462;&#25216;&#26415;&#21644;BERT&#23884;&#20837;&#30456;&#32467;&#21512;&#30340;&#20004;&#38454;&#27573;&#31995;&#32479;&#65292;&#29992;&#20110;&#22312;DBLP&#30693;&#35782;&#22270;&#35889;&#20013;&#23548;&#33322;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19987;&#27880;&#20110;&#25552;&#21462;&#19968;&#36339;&#20851;&#31995;&#21644;&#26631;&#35760;&#30340;&#20505;&#36873;&#23545;&#65292;&#28982;&#21518;&#22312;&#31532;&#20108;&#38454;&#27573;&#20351;&#29992;BERT&#30340;CLS&#23884;&#20837;&#21644;&#20854;&#20182;&#21551;&#21457;&#24335;&#26041;&#27861;&#36827;&#34892;&#20851;&#31995;&#36873;&#25321;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;Scholarly QALD&#30340;DBLP QuAD Final&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;0.2175&#30340;F1&#20998;&#25968;&#65292;&#32780;&#22312;DBLP QuAD&#27979;&#35797;&#25968;&#25454;&#38598;&#30340;&#23376;&#38598;&#19978;&#22312;QA&#38454;&#27573;&#36798;&#21040;&#20102;0.98&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development and integration of knowledge graphs and language models has significance in artificial intelligence and natural language processing. In this study, we introduce the BERTologyNavigator -- a two-phased system that combines relation extraction techniques and BERT embeddings to navigate the relationships within the DBLP Knowledge Graph (KG). Our approach focuses on extracting one-hop relations and labelled candidate pairs in the first phases. This is followed by employing BERT's CLS embeddings and additional heuristics for relation selection in the second phase. Our system reaches an F1 score of 0.2175 on the DBLP QuAD Final test dataset for Scholarly QALD and 0.98 F1 score on the subset of the DBLP QuAD test dataset during the QA phase.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25490;&#24207;&#20811;&#37324;&#27931;&#22827;&#22238;&#25910;&#65288;SKR&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#36895;&#31070;&#32463;&#31639;&#23376;&#35757;&#32451;&#30340;&#25968;&#25454;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#35299;&#20915;PDE&#38382;&#39064;&#26102;&#35745;&#31639;&#20887;&#20313;&#30340;&#38382;&#39064;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#25454;&#29983;&#25104;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.09516</link><description>&lt;p&gt;
&#36890;&#36807;&#20811;&#37324;&#27931;&#22827;&#23376;&#31354;&#38388;&#22238;&#25910;&#21152;&#36895;&#31070;&#32463;&#31639;&#23376;&#30340;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Accelerating Data Generation for Neural Operators via Krylov Subspace Recycling. (arXiv:2401.09516v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09516
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25490;&#24207;&#20811;&#37324;&#27931;&#22827;&#22238;&#25910;&#65288;SKR&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#36895;&#31070;&#32463;&#31639;&#23376;&#35757;&#32451;&#30340;&#25968;&#25454;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#35299;&#20915;PDE&#38382;&#39064;&#26102;&#35745;&#31639;&#20887;&#20313;&#30340;&#38382;&#39064;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#25454;&#29983;&#25104;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#29992;&#20110;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;(PDE)&#30340;&#31070;&#32463;&#31639;&#23376;&#22240;&#20854;&#39640;&#25512;&#29702;&#25928;&#29575;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#36825;&#20123;&#31639;&#23376;&#38656;&#35201;&#29983;&#25104;&#22823;&#37327;&#24102;&#26377;&#35299;&#20915;&#26041;&#26696;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;&#21363;PDE&#38382;&#39064;&#21450;&#20854;&#35299;&#20915;&#26041;&#26696;&#12290;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#38750;&#24120;&#32791;&#26102;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#35299;&#20915;&#22823;&#37327;&#32447;&#24615;&#26041;&#31243;&#32452;&#20197;&#33719;&#24471;PDE&#30340;&#25968;&#20540;&#35299;&#12290;&#35768;&#22810;&#29616;&#26377;&#26041;&#27861;&#29420;&#31435;&#22320;&#35299;&#20915;&#36825;&#20123;&#31995;&#32479;&#65292;&#32780;&#19981;&#32771;&#34385;&#23427;&#20204;&#30340;&#20869;&#22312;&#30456;&#20284;&#24615;&#65292;&#23548;&#33268;&#35745;&#31639;&#26497;&#20854;&#20887;&#20313;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#25490;&#24207;&#20811;&#37324;&#27931;&#22827;&#22238;&#25910;(SKR)&#65292;&#20197;&#25552;&#39640;&#35299;&#20915;&#36825;&#20123;&#31995;&#32479;&#30340;&#25928;&#29575;&#65292;&#20174;&#32780;&#26174;&#33879;&#21152;&#36895;&#31070;&#32463;&#31639;&#23376;&#35757;&#32451;&#30340;&#25968;&#25454;&#29983;&#25104;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;SKR&#26159;&#31532;&#19968;&#20010;&#35299;&#20915;&#23398;&#20064;&#31070;&#32463;&#31639;&#23376;&#25968;&#25454;&#29983;&#25104;&#32791;&#26102;&#24615;&#36136;&#30340;&#23581;&#35797;&#12290;SKR&#30340;&#26680;&#24515;&#26159;&#20811;&#37324;&#27931;&#22827;&#23376;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning neural operators for solving partial differential equations (PDEs) has attracted great attention due to its high inference efficiency. However, training such operators requires generating a substantial amount of labeled data, i.e., PDE problems together with their solutions. The data generation process is exceptionally time-consuming, as it involves solving numerous systems of linear equations to obtain numerical solutions to the PDEs. Many existing methods solve these systems independently without considering their inherent similarities, resulting in extremely redundant computations. To tackle this problem, we propose a novel method, namely Sorting Krylov Recycling (SKR), to boost the efficiency of solving these systems, thus significantly accelerating data generation for neural operators training. To the best of our knowledge, SKR is the first attempt to address the time-consuming nature of data generation for learning neural operators. The working horse of SKR is Krylov sub
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21160;&#24577;&#32593;&#32476;&#25299;&#25169;&#19979;&#65292;&#19981;&#21487;&#35775;&#38382;&#33410;&#28857;&#23545;&#27969;&#35328;&#23398;&#20064;&#30340;&#25910;&#25947;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2401.09498</link><description>&lt;p&gt;
&#25216;&#26415;&#25253;&#21578;&#65306;&#20851;&#20110;&#33410;&#28857;&#19981;&#21487;&#35775;&#38382;&#24773;&#20917;&#19979;&#27969;&#35328;&#23398;&#20064;&#25910;&#25947;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Technical Report: On the Convergence of Gossip Learning in the Presence of Node Inaccessibility. (arXiv:2401.09498v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21160;&#24577;&#32593;&#32476;&#25299;&#25169;&#19979;&#65292;&#19981;&#21487;&#35775;&#38382;&#33410;&#28857;&#23545;&#27969;&#35328;&#23398;&#20064;&#30340;&#25910;&#25947;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Gossip learning&#65288;GL&#65289;&#20316;&#20026;&#20998;&#25955;&#24335;&#23398;&#20064;&#30340;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#26356;&#36866;&#29992;&#20110;&#36164;&#28304;&#21463;&#38480;&#30340;&#26080;&#32447;&#32593;&#32476;&#65292;&#22914;&#30001;&#26080;&#20154;&#26426;&#65288;UAV&#65289;&#32452;&#25104;&#30340;FANETs&#12290;GL&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;UAV&#32593;&#32476;&#30340;&#25928;&#29575;&#24182;&#24310;&#38271;&#30005;&#27744;&#23551;&#21629;&#12290;&#23613;&#31649;&#20855;&#26377;&#36825;&#20123;&#20248;&#21183;&#65292;&#20294;GL&#30340;&#24615;&#33021;&#21463;&#25968;&#25454;&#20998;&#24067;&#12289;&#36890;&#20449;&#36895;&#24230;&#21644;&#32593;&#32476;&#36830;&#25509;&#24615;&#30340;&#24433;&#21709;&#36739;&#22823;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22240;&#32032;&#22914;&#20309;&#24433;&#21709;GL&#30340;&#25910;&#25947;&#24615;&#20173;&#19981;&#28165;&#26970;&#12290;&#29616;&#26377;&#30740;&#31350;&#22522;&#20110;&#34394;&#25311;&#25968;&#37327;&#26469;&#30740;&#31350;GL&#30340;&#25910;&#25947;&#24615;&#65292;&#20197;&#26041;&#20415;&#24615;&#32780;&#24573;&#30053;&#20102;&#24403;&#19968;&#20123;&#33410;&#28857;&#19981;&#21487;&#35775;&#38382;&#26102;&#32593;&#32476;&#30340;&#30495;&#23454;&#29366;&#24577;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#21160;&#24577;&#32593;&#32476;&#25299;&#25169;&#19979;&#19981;&#21487;&#35775;&#38382;&#33410;&#28857;&#23545;GL&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#24314;&#27169;&#21644;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#26435;&#37325;&#21457;&#25955;&#20998;&#35299;&#20026;&#33410;&#28857;&#26159;&#21542;&#21487;&#35775;&#38382;&#30340;&#24773;&#20917;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#33410;&#28857;&#21487;&#35775;&#38382;&#24615;&#30340;&#21160;&#24577;&#19979;GL&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#25552;&#20379;&#20102;
&lt;/p&gt;
&lt;p&gt;
Gossip learning (GL), as a decentralized alternative to federated learning (FL), is more suitable for resource-constrained wireless networks, such as FANETs that are formed by unmanned aerial vehicles (UAVs). GL can significantly enhance the efficiency and extend the battery life of UAV networks. Despite the advantages, the performance of GL is strongly affected by data distribution, communication speed, and network connectivity. However, how these factors influence the GL convergence is still unclear. Existing work studied the convergence of GL based on a virtual quantity for the sake of convenience, which fail to reflect the real state of the network when some nodes are inaccessible. In this paper, we formulate and investigate the impact of inaccessible nodes to GL under a dynamic network topology. We first decompose the weight divergence by whether the node is accessible or not. Then, we investigate the GL convergence under the dynamic of node accessibility and theoretically provide
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#32508;&#21512;&#35745;&#31639;&#12289;&#34892;&#20026;&#21644;&#31070;&#32463;&#35777;&#25454;&#65292;&#25581;&#31034;&#20102;&#35760;&#24518;&#19982;&#39044;&#27979;&#12289;&#35268;&#21010;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#20197;&#21450;&#20854;&#22312;&#28023;&#39532;&#20307;&#21644;&#21069;&#39069;&#21494;&#30382;&#36136;&#20013;&#30340;&#22810;&#23610;&#24230;&#39044;&#27979;&#34920;&#31034;&#65292;&#20026;&#25105;&#20204;&#29702;&#35299;&#22823;&#33041;&#20013;&#30340;&#35760;&#24518;&#21644;&#35268;&#21010;&#26426;&#21046;&#25552;&#20379;&#20102;&#37325;&#35201;&#21551;&#31034;&#12290;</title><link>http://arxiv.org/abs/2401.09491</link><description>&lt;p&gt;
&#35760;&#24518;&#12289;&#31354;&#38388;&#21644;&#35268;&#21010;: &#22810;&#23610;&#24230;&#39044;&#27979;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Memory, Space, and Planning: Multiscale Predictive Representations. (arXiv:2401.09491v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09491
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#32508;&#21512;&#35745;&#31639;&#12289;&#34892;&#20026;&#21644;&#31070;&#32463;&#35777;&#25454;&#65292;&#25581;&#31034;&#20102;&#35760;&#24518;&#19982;&#39044;&#27979;&#12289;&#35268;&#21010;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#20197;&#21450;&#20854;&#22312;&#28023;&#39532;&#20307;&#21644;&#21069;&#39069;&#21494;&#30382;&#36136;&#20013;&#30340;&#22810;&#23610;&#24230;&#39044;&#27979;&#34920;&#31034;&#65292;&#20026;&#25105;&#20204;&#29702;&#35299;&#22823;&#33041;&#20013;&#30340;&#35760;&#24518;&#21644;&#35268;&#21010;&#26426;&#21046;&#25552;&#20379;&#20102;&#37325;&#35201;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35760;&#24518;&#19982;&#39044;&#27979;&#21644;&#35268;&#21010;&#23494;&#19981;&#21487;&#20998;&#12290;&#29983;&#29289;&#21644;&#20154;&#24037;&#26234;&#33021;&#26234;&#33021;&#20307;&#30340;&#28789;&#27963;&#34892;&#20026;&#21462;&#20915;&#20110;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#20174;&#36807;&#21435;&#20013;&#23398;&#20064;&#21644;&#39044;&#27979;&#26410;&#26469;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#26412;&#31456;&#22238;&#39038;&#20102;&#35745;&#31639;&#12289;&#34892;&#20026;&#21644;&#31070;&#32463;&#35777;&#25454;&#65292;&#34920;&#26126;&#36825;&#20123;&#36807;&#31243;&#20381;&#36182;&#20110;&#23398;&#20064;&#32463;&#39564;&#30340;&#20851;&#31995;&#32467;&#26500;&#65292;&#21363;&#35748;&#30693;&#22320;&#22270;&#65292;&#24182;&#24471;&#20986;&#20004;&#20010;&#20851;&#38190;&#35201;&#28857;&#12290;&#39318;&#20808;&#65292;&#36825;&#20123;&#35760;&#24518;&#32467;&#26500;&#22312;&#28023;&#39532;&#20307;&#21644;&#21069;&#39069;&#21494;&#30382;&#36136;&#65288;PFC&#65289;&#23618;&#27425;&#32467;&#26500;&#20013;&#32452;&#32455;&#20026;&#22810;&#23610;&#24230;&#12289;&#32039;&#20945;&#30340;&#39044;&#27979;&#34920;&#31034;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#39044;&#27979;&#24615;&#35760;&#24518;&#32467;&#26500;&#23545;&#28023;&#39532;&#20307;&#21644;PFC&#30340;&#20114;&#34917;&#21151;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#26082;&#33021;&#20351;&#36807;&#21435;&#30340;&#35814;&#32454;&#21644;&#36830;&#36143;&#30340;&#20107;&#20214;&#22238;&#24518;&#36215;&#26469;&#65292;&#20063;&#33021;&#22312;&#19981;&#21516;&#23610;&#24230;&#19978;&#25512;&#24191;&#32463;&#39564;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#39044;&#27979;&#21644;&#35268;&#21010;&#12290;&#36825;&#20123;&#35265;&#35299;&#25512;&#21160;&#20102;&#25105;&#20204;&#23545;&#22823;&#33041;&#20013;&#35760;&#24518;&#21644;&#35268;&#21010;&#26426;&#21046;&#30340;&#29702;&#35299;&#65292;&#24182;&#20855;&#26377;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Memory is inherently entangled with prediction and planning. Flexible behavior in biological and artificial agents depends on the interplay of learning from the past and predicting the future in ever-changing environments. This chapter reviews computational, behavioral, and neural evidence suggesting these processes rely on learning the relational structure of experiences, known as cognitive maps, and draws two key takeaways. First, that these memory structures are organized as multiscale, compact predictive representations in hippocampal and prefrontal cortex, or PFC, hierarchies. Second, we argue that such predictive memory structures are crucial to the complementary functions of the hippocampus and PFC, both for enabling a recall of detailed and coherent past episodes as well as generalizing experiences at varying scales for efficient prediction and planning. These insights advance our understanding of memory and planning mechanisms in the brain and hold significant implications for
&lt;/p&gt;</description></item><item><title>PUPAE&#26159;&#19968;&#31181;&#30452;&#35266;&#19988;&#21487;&#25805;&#20316;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#39046;&#22495;&#26080;&#20851;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#33021;&#22815;&#24110;&#21161;&#35299;&#37322;&#21644;&#22788;&#29702;&#24322;&#24120;&#12290;</title><link>http://arxiv.org/abs/2401.09489</link><description>&lt;p&gt;
PUPAE: &#30452;&#35266;&#19988;&#21487;&#25805;&#20316;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
PUPAE: Intuitive and Actionable Explanations for Time Series Anomalies. (arXiv:2401.09489v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09489
&lt;/p&gt;
&lt;p&gt;
PUPAE&#26159;&#19968;&#31181;&#30452;&#35266;&#19988;&#21487;&#25805;&#20316;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#39046;&#22495;&#26080;&#20851;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#33021;&#22815;&#24110;&#21161;&#35299;&#37322;&#21644;&#22788;&#29702;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#26816;&#27979;&#21040;&#24322;&#24120;&#20043;&#21518;&#65292;&#25105;&#20204;&#33021;&#35299;&#37322;&#23427;&#21527;&#65311;&#36825;&#26679;&#30340;&#35299;&#37322;&#23545;&#20110;&#22788;&#29702;&#24322;&#24120;&#38750;&#24120;&#26377;&#29992;&#12290;&#20363;&#22914;&#65292;&#22312;&#19968;&#20010;&#28860;&#27833;&#21378;&#20013;&#65292;&#25105;&#20204;&#26159;&#36890;&#36807;&#27966;&#36963;&#28082;&#21387;&#24037;&#31243;&#24072;&#36824;&#26159;&#23454;&#20064;&#29983;&#26356;&#25442;&#20256;&#24863;&#22120;&#30005;&#27744;&#26469;&#21709;&#24212;&#24322;&#24120;&#65311;&#34429;&#28982;&#26377;&#19968;&#20123;&#24182;&#34892;&#30340;&#21162;&#21147;&#26469;&#35299;&#37322;&#24322;&#24120;&#65292;&#20294;&#24456;&#22810;&#25552;&#20986;&#30340;&#25216;&#26415;&#20135;&#29983;&#30340;&#35299;&#37322;&#26159;&#38388;&#25509;&#30340;&#65292;&#24182;&#19988;&#36890;&#24120;&#27604;&#23427;&#20204;&#35797;&#22270;&#35299;&#37322;&#30340;&#24322;&#24120;&#26356;&#22797;&#26434;&#12290;&#25105;&#20204;&#23545;&#21508;&#20010;&#39046;&#22495;&#21069;&#32447;&#20174;&#19994;&#20154;&#21592;&#20351;&#29992;&#30340;&#25991;&#29486;&#12289;&#28165;&#21333;&#21644;&#29992;&#25143;&#25163;&#20876;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21457;&#29616;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#20849;&#21516;&#28857;&#12290;&#22823;&#22810;&#25968;&#20174;&#19994;&#20154;&#21592;&#20197;&#20197;&#19979;&#26684;&#24335;&#35752;&#35770;&#12289;&#35299;&#37322;&#21644;&#25253;&#21578;&#24322;&#24120;&#65306;&#22914;&#26524;&#27809;&#26377;&#30772;&#22351;B&#65292;&#24322;&#24120;&#23601;&#20250;&#20687;&#27491;&#24120;&#25968;&#25454;A&#19968;&#26679;&#12290;&#35835;&#32773;&#23558;&#20250;&#24847;&#35782;&#21040;&#36825;&#26159;&#19968;&#31181;&#21453;&#20107;&#23454;&#30340;&#35299;&#37322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#39046;&#22495;&#26080;&#20851;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years there has been significant progress in time series anomaly detection. However, after detecting an (perhaps tentative) anomaly, can we explain it? Such explanations would be useful to triage anomalies. For example, in an oil refinery, should we respond to an anomaly by dispatching a hydraulic engineer, or an intern to replace the battery on a sensor? There have been some parallel efforts to explain anomalies, however many proposed techniques produce explanations that are indirect, and often seem more complex than the anomaly they seek to explain. Our review of the literature/checklists/user-manuals used by frontline practitioners in various domains reveals an interesting near-universal commonality. Most practitioners discuss, explain and report anomalies in the following format: The anomaly would be like normal data A, if not for the corruption B. The reader will appreciate that is a type of counterfactual explanation. In this work we introduce a domain agnostic counterf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#30828;&#20214;&#29305;&#27931;&#20234;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#25193;&#20805;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#26089;&#34701;&#21512;&#21644;&#26202;&#34701;&#21512;&#31574;&#30053;&#36827;&#34892;&#35780;&#20272;&#12290;&#36890;&#36807;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25351;&#26631;&#65292;&#23454;&#29616;&#39118;&#38505;&#24863;&#30693;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;</title><link>http://arxiv.org/abs/2401.09479</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30828;&#20214;&#29305;&#27931;&#20234;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Hardware Trojan Detection Using Multimodal Deep Learning. (arXiv:2401.09479v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09479
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#30828;&#20214;&#29305;&#27931;&#20234;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#25193;&#20805;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#26089;&#34701;&#21512;&#21644;&#26202;&#34701;&#21512;&#31574;&#30053;&#36827;&#34892;&#35780;&#20272;&#12290;&#36890;&#36807;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25351;&#26631;&#65292;&#23454;&#29616;&#39118;&#38505;&#24863;&#30693;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38646;&#20449;&#20219;&#30340;&#26080;&#21378;&#26080;&#21360;&#36896;&#21046;&#36896;&#26102;&#20195;&#65292;&#30828;&#20214;&#29305;&#27931;&#20234;&#22312;&#33455;&#29255;&#29983;&#20135;&#30340;&#21508;&#20010;&#38454;&#27573;&#34987;&#25554;&#20837;&#30340;&#39118;&#38505;&#22686;&#21152;&#20102;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#29992;&#20110;&#26816;&#27979;&#30828;&#20214;&#29305;&#27931;&#20234;&#12290;&#23613;&#31649;&#22823;&#37096;&#20998;&#20851;&#27880;&#28857;&#37117;&#38598;&#20013;&#22312;&#32479;&#35745;&#23398;&#25110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#19978;&#65292;&#20294;&#21463;&#21040;&#29305;&#27931;&#20234;&#24863;&#26579;&#22522;&#20934;&#26679;&#26412;&#25968;&#37327;&#26377;&#38480;&#30340;&#24433;&#21709;&#65292;&#26816;&#27979;&#20934;&#30830;&#24615;&#21463;&#38480;&#65292;&#26080;&#27861;&#26816;&#27979;&#21040;&#38646;&#26085;&#29305;&#27931;&#20234;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#39318;&#20808;&#37319;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26469;&#25193;&#20805;&#25968;&#25454;&#65292;&#20197;&#20004;&#31181;&#26367;&#20195;&#34920;&#31034;&#27169;&#24577;&#65292;&#22270;&#24418;&#21644;&#34920;&#26684;&#65292;&#30830;&#20445;&#25968;&#25454;&#38598;&#20197;&#20195;&#34920;&#24615;&#30340;&#26041;&#24335;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#26816;&#27979;&#30828;&#20214;&#29305;&#27931;&#20234;&#65292;&#24182;&#35780;&#20272;&#20102;&#26089;&#34701;&#21512;&#21644;&#26202;&#34701;&#21512;&#31574;&#30053;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#20272;&#35745;&#20102;&#27599;&#20010;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25351;&#26631;&#65292;&#29992;&#20110;&#39118;&#38505;&#24863;&#30693;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;&#32467;&#26524;&#19981;&#20165;&#30830;&#35748;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#32780;&#19988;&#34920;&#26126;&#20102;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#23545;&#30828;&#20214;&#29305;&#27931;&#20234;&#26816;&#27979;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The risk of hardware Trojans being inserted at various stages of chip production has increased in a zero-trust fabless era. To counter this, various machine learning solutions have been developed for the detection of hardware Trojans. While most of the focus has been on either a statistical or deep learning approach, the limited number of Trojan-infected benchmarks affects the detection accuracy and restricts the possibility of detecting zero-day Trojans. To close the gap, we first employ generative adversarial networks to amplify our data in two alternative representation modalities, a graph and a tabular, ensuring that the dataset is distributed in a representative manner. Further, we propose a multimodal deep learning approach to detect hardware Trojans and evaluate the results from both early fusion and late fusion strategies. We also estimate the uncertainty quantification metrics of each prediction for risk-aware decision-making. The outcomes not only confirms the efficacy of our
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#21306;&#22359;&#38142;&#30340;&#20892;&#19994;&#39135;&#21697;&#20379;&#24212;&#38142;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#24314;&#31435;&#20449;&#20219;&#21644;&#36879;&#26126;&#24230;&#30830;&#20445;&#39135;&#21697;&#20379;&#24212;&#38142;&#30340;&#23433;&#20840;&#24615;&#65292;&#24182;&#35299;&#20915;&#20449;&#24687;&#38450;&#31713;&#25913;&#21644;&#20379;&#38656;&#20851;&#31995;&#31561;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2401.09476</link><description>&lt;p&gt;
&#20351;&#29992;&#21306;&#22359;&#38142;&#30340;&#20892;&#19994;&#39135;&#21697;&#20379;&#24212;&#38142;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework for Agricultural Food Supply Chain using Blockchain. (arXiv:2401.09476v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#21306;&#22359;&#38142;&#30340;&#20892;&#19994;&#39135;&#21697;&#20379;&#24212;&#38142;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#24314;&#31435;&#20449;&#20219;&#21644;&#36879;&#26126;&#24230;&#30830;&#20445;&#39135;&#21697;&#20379;&#24212;&#38142;&#30340;&#23433;&#20840;&#24615;&#65292;&#24182;&#35299;&#20915;&#20449;&#24687;&#38450;&#31713;&#25913;&#21644;&#20379;&#38656;&#20851;&#31995;&#31561;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#36890;&#36807;&#20351;&#29992;&#21306;&#22359;&#38142;&#25216;&#26415;&#22312;&#39135;&#21697;&#20379;&#24212;&#38142;&#31995;&#32479;&#20013;&#24314;&#31435;&#20449;&#20219;&#21644;&#36879;&#26126;&#24230;&#65292;&#30830;&#20445;&#27599;&#20010;&#20154;&#30340;&#39135;&#21697;&#23433;&#20840;&#12290;&#39135;&#21697;&#20379;&#24212;&#38142;&#26159;&#20174;&#20892;&#27665;&#25110;&#29983;&#20135;&#32773;&#21040;&#20080;&#23478;&#30340;&#20892;&#20316;&#29289;&#36861;&#36394;&#36807;&#31243;&#12290;&#38543;&#30528;&#21306;&#22359;&#38142;&#30340;&#20986;&#29616;&#65292;&#20026;&#25552;&#20379;&#21508;&#31181;&#20892;&#19994;&#24517;&#38656;&#21697;&#30340;&#23433;&#20840;&#21644;&#26080;&#27450;&#35784;&#29615;&#22659;&#24050;&#21464;&#24471;&#26356;&#21152;&#23481;&#26131;&#12290;&#30446;&#21069;&#30340;&#20379;&#24212;&#38142;&#24066;&#22330;&#21253;&#25324;&#28041;&#21450;&#25968;&#25454;&#38598;&#25104;&#12289;&#22797;&#26434;&#20132;&#26131;&#21644;&#20998;&#38144;&#30340;&#21508;&#31181;&#20225;&#19994;&#65292;&#30001;&#20110;&#36152;&#26131;&#20840;&#29699;&#21270;&#25152;&#24102;&#26469;&#12290;&#20449;&#24687;&#38450;&#31713;&#25913;&#12289;&#20379;&#38656;&#20851;&#31995;&#21644;&#21487;&#36861;&#28335;&#30417;&#30563;&#26159;&#30001;&#27492;&#20135;&#29983;&#30340;&#22256;&#38590;&#12290;&#21306;&#22359;&#38142;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#36134;&#26412;&#25216;&#26415;&#65292;&#21487;&#20197;&#25552;&#20379;&#25239;&#31713;&#25913;&#30340;&#20449;&#24687;&#12290;&#36825;&#31181;&#31574;&#30053;&#21487;&#20197;&#28040;&#38500;&#23545;&#38598;&#20013;&#30340;&#20449;&#20219;&#26426;&#26500;&#12289;&#20013;&#20171;&#21644;&#19994;&#21153;&#21382;&#21490;&#30340;&#38656;&#27714;&#65292;&#20174;&#32780;&#23454;&#29616;&#22686;&#21152;&#29983;&#20135;&#21644;&#23433;&#20840;&#24615;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
The main aim of the paper is to create a trust and transparency in the food supply chain system, ensuring food safety for everyone with the help of Blockchain Technology. Food supply chain is the process of tracing a crop from the farmer or producer to the buyer. With the advent of blockchain, providing a safe and fraud-free environment for the provision of numerous agricultural necessities has become much easier. Because of the globalization of trade, the present supply chain market today includes various companies involving integration of data, complex transactions and distribution. Information tamper resistance, supply-demand relationships, and traceable oversight are all difficulties that arise as a result of this. Blockchain is a distributed ledger technology that can provide information that is resistant to tampering. This strategy can eliminate the need for a centralized trusted authority, intermediaries, and business histories, allowing for increased production and security whi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20851;&#27880;&#22269;&#20869;&#20250;&#35805;&#29983;&#25104;AI&#24378;&#21270;&#30340;&#22810;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#21830;&#19994;&#21644;&#20262;&#29702;&#38382;&#39064;&#12290;&#21512;&#20316;&#24335;&#22810;&#26426;&#22120;&#20154;&#31995;&#32479;&#33021;&#22815;&#38761;&#26032;&#19981;&#21516;&#34892;&#19994;&#30340;&#27969;&#31243;&#24182;&#25913;&#21464;&#20154;&#31867;&#30340;&#21830;&#19994;&#26041;&#24335;&#65292;&#20294;&#20063;&#38656;&#35201;&#23545;&#20854;&#28508;&#22312;&#30340;&#21033;&#30410;&#20914;&#31361;&#12289;&#38544;&#31169;&#23454;&#36341;&#21644;&#23433;&#20840;&#38382;&#39064;&#36827;&#34892;&#20262;&#29702;&#32771;&#23519;&#12290;</title><link>http://arxiv.org/abs/2401.09473</link><description>&lt;p&gt;
&#22269;&#20869;&#20250;&#35805;&#29983;&#25104;AI&#24378;&#21270;&#30340;&#22810;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#21830;&#19994;&#21644;&#20262;&#29702;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Business and ethical concerns in domestic Conversational Generative AI-empowered multi-robot systems. (arXiv:2401.09473v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09473
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#22269;&#20869;&#20250;&#35805;&#29983;&#25104;AI&#24378;&#21270;&#30340;&#22810;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#21830;&#19994;&#21644;&#20262;&#29702;&#38382;&#39064;&#12290;&#21512;&#20316;&#24335;&#22810;&#26426;&#22120;&#20154;&#31995;&#32479;&#33021;&#22815;&#38761;&#26032;&#19981;&#21516;&#34892;&#19994;&#30340;&#27969;&#31243;&#24182;&#25913;&#21464;&#20154;&#31867;&#30340;&#21830;&#19994;&#26041;&#24335;&#65292;&#20294;&#20063;&#38656;&#35201;&#23545;&#20854;&#28508;&#22312;&#30340;&#21033;&#30410;&#20914;&#31361;&#12289;&#38544;&#31169;&#23454;&#36341;&#21644;&#23433;&#20840;&#38382;&#39064;&#36827;&#34892;&#20262;&#29702;&#32771;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21830;&#19994;&#21644;&#25216;&#26415;&#36890;&#36807;&#36923;&#36753;&#21644;&#35774;&#35745;&#23494;&#20999;&#32852;&#31995;&#22312;&#19968;&#36215;&#12290;&#23427;&#20204;&#21516;&#26679;&#23545;&#31038;&#20250;&#21464;&#38761;&#25935;&#24863;&#65292;&#21487;&#33021;&#22240;&#19985;&#38395;&#24102;&#26469;&#28798;&#38590;&#12290;&#21512;&#20316;&#24335;&#22810;&#26426;&#22120;&#20154;&#31995;&#32479;(MRS)&#27491;&#22312;&#20852;&#36215;&#65292;&#20801;&#35768;&#19981;&#21516;&#31867;&#22411;&#21644;&#21697;&#29260;&#30340;&#26426;&#22120;&#20154;&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#20849;&#21516;&#24037;&#20316;&#12290;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#36817;&#24180;&#26469;&#19968;&#30452;&#26159;&#20154;&#24037;&#26234;&#33021;&#35752;&#35770;&#30340;&#20027;&#35201;&#35805;&#39064;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#21644;&#29983;&#25104;&#23186;&#20307;&#65288;&#21253;&#25324;&#28145;&#24230;&#20266;&#36896;&#65289;&#26469;&#27169;&#20223;&#20154;&#31867;&#12290;&#26412;&#25991;&#20027;&#35201;&#20851;&#27880;&#29983;&#25104;&#24335;AI&#30340;&#23545;&#35805;&#26041;&#38754;&#65292;&#22240;&#27492;&#20351;&#29992;&#20102;&#8220;&#23545;&#35805;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#8221;&#65288;CGI&#65289;&#19968;&#35789;&#12290;&#20687;MRS&#19968;&#26679;&#65292;CGI&#22312;&#38761;&#26032;&#21508;&#20010;&#39046;&#22495;&#30340;&#27969;&#31243;&#24182;&#25913;&#21464;&#20154;&#31867;&#32463;&#33829;&#26041;&#24335;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#20174;&#21830;&#19994;&#35282;&#24230;&#26469;&#30475;&#65292;&#20165;&#23601;&#20855;&#26377;&#28508;&#22312;&#21033;&#30410;&#20914;&#31361;&#12289;&#38544;&#31169;&#23454;&#36341;&#21644;&#23433;&#20840;&#38382;&#39064;&#30340;&#21512;&#20316;&#24335;MRS&#26412;&#36523;&#65292;&#23601;&#38656;&#35201;&#36827;&#34892;&#20262;&#29702;&#32771;&#23519;&#12290;MRSs e
&lt;/p&gt;
&lt;p&gt;
Business and technology are intricately connected through logic and design. They are equally sensitive to societal changes and may be devastated by scandal. Cooperative multi-robot systems (MRSs) are on the rise, allowing robots of different types and brands to work together in diverse contexts. Generative artificial intelligence has been a dominant topic in recent artificial intelligence (AI) discussions due to its capacity to mimic humans through the use of natural language and the production of media, including deep fakes. In this article, we focus specifically on the conversational aspects of generative AI, and hence use the term Conversational Generative artificial intelligence (CGI). Like MRSs, CGIs have enormous potential for revolutionizing processes across sectors and transforming the way humans conduct business. From a business perspective, cooperative MRSs alone, with potential conflicts of interest, privacy practices, and safety concerns, require ethical examination. MRSs e
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31163;&#32447;&#25163;&#20889;&#31614;&#21517;&#39564;&#35777;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#21644;&#29305;&#24449;&#36873;&#25321;&#26469;&#25552;&#39640;&#39564;&#35777;&#32467;&#26524;&#65292;&#22312;&#37329;&#34701;&#12289;&#27861;&#24459;&#25991;&#20214;&#21644;&#23433;&#20840;&#31561;&#39046;&#22495;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.09467</link><description>&lt;p&gt;
&#31163;&#32447;&#25163;&#20889;&#31614;&#21517;&#39564;&#35777;&#65306;&#19968;&#31181;&#36801;&#31227;&#23398;&#20064;&#21644;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Offline Handwriting Signature Verification: A Transfer Learning and Feature Selection Approach. (arXiv:2401.09467v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09467
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31163;&#32447;&#25163;&#20889;&#31614;&#21517;&#39564;&#35777;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#21644;&#29305;&#24449;&#36873;&#25321;&#26469;&#25552;&#39640;&#39564;&#35777;&#32467;&#26524;&#65292;&#22312;&#37329;&#34701;&#12289;&#27861;&#24459;&#25991;&#20214;&#21644;&#23433;&#20840;&#31561;&#39046;&#22495;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#20889;&#31614;&#21517;&#39564;&#35777;&#22312;&#29983;&#29289;&#35782;&#21035;&#21644;&#25991;&#20214;&#30495;&#23454;&#24615;&#26041;&#38754;&#20855;&#26377;&#19968;&#23450;&#25361;&#25112;&#12290;&#30446;&#26631;&#26159;&#30830;&#23450;&#25552;&#20379;&#30340;&#25163;&#20889;&#31614;&#21517;&#30340;&#30495;&#23454;&#24615;&#65292;&#21306;&#20998;&#30495;&#23454;&#30340;&#21644;&#20266;&#36896;&#30340;&#31614;&#21517;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#37329;&#34701;&#12289;&#27861;&#24459;&#25991;&#20214;&#21644;&#23433;&#20840;&#31561;&#39046;&#22495;&#26377;&#24456;&#22810;&#24212;&#29992;&#12290;&#30446;&#21069;&#65292;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#23398;&#20064;&#22312;&#25163;&#20889;&#31614;&#21517;&#39564;&#35777;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#26681;&#25454;&#33719;&#21462;&#30340;&#32467;&#26524;&#12289;&#25968;&#25454;&#38598;&#30340;&#32467;&#26500;&#21644;&#25152;&#20351;&#29992;&#30340;&#27169;&#22411;&#65292;&#32467;&#26524;&#21487;&#33021;&#20250;&#26377;&#25152;&#25913;&#21892;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31574;&#30053;&#21253;&#25324;&#22235;&#20010;&#38454;&#27573;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;420&#20010;&#19981;&#21516;&#20010;&#20307;&#20013;&#25910;&#38598;&#20102;12600&#24352;&#22270;&#29255;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#27599;&#20010;&#20010;&#20307;&#26377;30&#20010;&#29305;&#23450;&#31867;&#22411;&#30340;&#31614;&#21517;&#65288;&#25152;&#26377;&#20316;&#32773;&#30340;&#31614;&#21517;&#37117;&#26159;&#30495;&#23454;&#30340;&#65289;&#12290;&#22312;&#21518;&#32493;&#38454;&#27573;&#65292;&#20351;&#29992;&#19968;&#20010;&#31216;&#20026;MobileNetV2&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#21462;&#20102;&#27599;&#24352;&#22270;&#29255;&#20013;&#30340;&#26368;&#22909;&#29305;&#24449;&#12290;&#22312;&#29305;&#24449;&#36873;&#25321;&#27493;&#39588;&#20013;&#65292;&#20351;&#29992;&#20102;&#19977;&#20010;&#36873;&#25321;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Handwritten signature verification poses a formidable challenge in biometrics and document authenticity. The objective is to ascertain the authenticity of a provided handwritten signature, distinguishing between genuine and forged ones. This issue has many applications in sectors such as finance, legal documentation, and security. Currently, the field of computer vision and machine learning has made significant progress in the domain of handwritten signature verification. The outcomes, however, may be enhanced depending on the acquired findings, the structure of the datasets, and the used models. Four stages make up our suggested strategy. First, we collected a large dataset of 12600 images from 420 distinct individuals, and each individual has 30 signatures of a certain kind (All authors signatures are genuine). In the subsequent stage, the best features from each image were extracted using a deep learning model named MobileNetV2. During the feature selection step, three selectors nei
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#19979;&#35843;&#23610;&#24230; ESM &#27169;&#25311;&#25968;&#25454;&#65292;&#19981;&#38656;&#35201;&#39640;&#20998;&#36776;&#29575;&#30340;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2401.09466</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#35270;&#35273;&#29992;&#20110;&#27668;&#20505;&#19979;&#35843;&#23610;&#24230;
&lt;/p&gt;
&lt;p&gt;
Self Supervised Vision for Climate Downscaling. (arXiv:2401.09466v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09466
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#19979;&#35843;&#23610;&#24230; ESM &#27169;&#25311;&#25968;&#25454;&#65292;&#19981;&#38656;&#35201;&#39640;&#20998;&#36776;&#29575;&#30340;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27668;&#20505;&#21464;&#21270;&#26159;&#25105;&#20204;&#26143;&#29699;&#38754;&#20020;&#30340;&#26368;&#37325;&#35201;&#30340;&#25361;&#25112;&#20043;&#19968;&#12290;&#20840;&#29699;&#27668;&#28201;&#30340;&#19978;&#21319;&#24050;&#32463;&#24102;&#26469;&#20102;&#23545;&#22320;&#29699;&#27668;&#20505;&#21644;&#22825;&#27668;&#27169;&#24335;&#30340;&#26174;&#33879;&#21464;&#21270;&#65292;&#19981;&#21487;&#39044;&#27979;&#21644;&#26497;&#31471;&#22825;&#27668;&#20107;&#20214;&#30340;&#39057;&#29575;&#22686;&#21152;&#12290;&#27668;&#20505;&#21464;&#21270;&#30740;&#31350;&#30340;&#26410;&#26469;&#39044;&#27979;&#22522;&#20110;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#65288;ESMs&#65289;&#65292;&#36825;&#20123;&#35745;&#31639;&#26426;&#27169;&#22411;&#27169;&#25311;&#22320;&#29699;&#30340;&#27668;&#20505;&#31995;&#32479;&#12290; ESMs &#25552;&#20379;&#20102;&#25972;&#21512;&#21508;&#31181;&#29289;&#29702;&#31995;&#32479;&#30340;&#26694;&#26550;&#65292;&#20294;&#23427;&#20204;&#30340;&#36755;&#20986;&#21463;&#21040;&#36816;&#34892;&#21644;&#23384;&#26723;&#26356;&#39640;&#20998;&#36776;&#29575;&#27169;&#25311;&#25152;&#38656;&#30340;&#24040;&#22823;&#35745;&#31639;&#36164;&#28304;&#30340;&#38480;&#21046;&#12290;&#23545;&#20110;&#32473;&#23450;&#30340;&#36164;&#28304;&#39044;&#31639;&#65292;ESMs &#36890;&#24120;&#22312;&#36739;&#31895;&#30340;&#32593;&#26684;&#19978;&#36816;&#34892;&#65292;&#28982;&#21518;&#36827;&#34892;&#35745;&#31639;&#37327;&#36739;&#23567;&#30340;&#8220;&#19979;&#35843;&#23610;&#24230;&#8221;&#36807;&#31243;&#20197;&#33719;&#24471;&#26356;&#39640;&#20998;&#36776;&#29575;&#30340;&#36755;&#20986;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#19979;&#35843;&#23610;&#24230; ESM &#27169;&#25311;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#19981;&#38656;&#35201;&#39640;&#20998;&#36776;&#29575;&#30340;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#20248;&#21270;&#12290;&#36825;&#26159;&#36890;&#36807;&#21033;&#29992;&#26174;&#33879;&#30340;&#25968;&#25454;&#20998;&#24067;&#26469;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Climate change is one of the most critical challenges that our planet is facing today. Rising global temperatures are already bringing noticeable changes to Earth's weather and climate patterns with an increased frequency of unpredictable and extreme weather events. Future projections for climate change research are based on Earth System Models (ESMs), the computer models that simulate the Earth's climate system. ESMs provide a framework to integrate various physical systems, but their output is bound by the enormous computational resources required for running and archiving higher-resolution simulations. For a given resource budget, the ESMs are generally run on a coarser grid, followed by a computationally lighter $downscaling$ process to obtain a finer-resolution output. In this work, we present a deep-learning model for downscaling ESM simulation data that does not require high-resolution ground truth data for model optimization. This is realized by leveraging salient data distribu
&lt;/p&gt;</description></item><item><title>&#35768;&#22810;&#20316;&#32773;&#24050;&#32463;&#35780;&#35770;&#20102;AI-SCS&#30340;&#8220;&#36131;&#20219;&#24046;&#36317;&#8221;&#65292;&#21363;&#24320;&#21457;&#20154;&#21592;&#21644;&#21046;&#36896;&#21830;&#38590;&#20197;&#23545;AI&#30340;&#26377;&#23475;&#34892;&#20026;&#36127;&#36131;&#65292;&#36825;&#26159;&#30001;&#20110;AI&#30340;&#22797;&#26434;&#24320;&#21457;&#21608;&#26399;&#12289;&#24615;&#33021;&#19981;&#30830;&#23450;&#24615;&#21644;&#21160;&#24577;&#25805;&#20316;&#29615;&#22659;&#25152;&#33268;&#12290;&#22312;AI-SCS&#21464;&#24471;&#33258;&#20027;&#21270;&#21518;&#65292;&#20154;&#31867;&#25805;&#20316;&#21592;&#21487;&#33021;&#25104;&#20026;&#25215;&#25285;&#36131;&#20219;&#30340;&#26367;&#32618;&#32650;&#65292;&#20182;&#20204;&#21487;&#33021;&#25215;&#25285;&#30001;AI-SCS&#36755;&#20986;&#30340;&#21518;&#26524;&#65292;&#32780;&#36825;&#20123;&#21518;&#26524;&#20182;&#20204;&#27809;&#26377;&#21442;&#19982;&#21019;&#24314;&#65292;&#20063;&#21487;&#33021;&#19981;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2401.09459</link><description>&lt;p&gt;
&#25105;&#30340;&#35282;&#33394;&#26159;&#20160;&#20040;&#65311;&#23545;&#22522;&#20110;AI&#30340;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#30340;&#36131;&#20219;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
What's my role? Modelling responsibility for AI-based safety-critical systems. (arXiv:2401.09459v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09459
&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20316;&#32773;&#24050;&#32463;&#35780;&#35770;&#20102;AI-SCS&#30340;&#8220;&#36131;&#20219;&#24046;&#36317;&#8221;&#65292;&#21363;&#24320;&#21457;&#20154;&#21592;&#21644;&#21046;&#36896;&#21830;&#38590;&#20197;&#23545;AI&#30340;&#26377;&#23475;&#34892;&#20026;&#36127;&#36131;&#65292;&#36825;&#26159;&#30001;&#20110;AI&#30340;&#22797;&#26434;&#24320;&#21457;&#21608;&#26399;&#12289;&#24615;&#33021;&#19981;&#30830;&#23450;&#24615;&#21644;&#21160;&#24577;&#25805;&#20316;&#29615;&#22659;&#25152;&#33268;&#12290;&#22312;AI-SCS&#21464;&#24471;&#33258;&#20027;&#21270;&#21518;&#65292;&#20154;&#31867;&#25805;&#20316;&#21592;&#21487;&#33021;&#25104;&#20026;&#25215;&#25285;&#36131;&#20219;&#30340;&#26367;&#32618;&#32650;&#65292;&#20182;&#20204;&#21487;&#33021;&#25215;&#25285;&#30001;AI-SCS&#36755;&#20986;&#30340;&#21518;&#26524;&#65292;&#32780;&#36825;&#20123;&#21518;&#26524;&#20182;&#20204;&#27809;&#26377;&#21442;&#19982;&#21019;&#24314;&#65292;&#20063;&#21487;&#33021;&#19981;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;AI&#30340;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#65288;AI-SCS&#65289;&#27491;&#22312;&#36234;&#26469;&#36234;&#22810;&#22320;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#37096;&#32626;&#12290;&#36825;&#21487;&#33021;&#23545;&#20154;&#31867;&#21644;&#29615;&#22659;&#36896;&#25104;&#21361;&#23475;&#12290;&#22312;&#24320;&#21457;&#21644;&#36816;&#33829;&#36807;&#31243;&#20013;&#65292;&#38477;&#20302;&#39118;&#38505;&#26159;&#19968;&#39033;&#39318;&#35201;&#20219;&#21153;&#12290;&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;AI-SCS&#21464;&#24471;&#33258;&#20027;&#21270;&#65292;&#36890;&#36807;&#20154;&#31867;&#24178;&#39044;&#31649;&#29702;&#39118;&#38505;&#30340;&#19968;&#23618;&#24050;&#34987;&#31227;&#38500;&#12290;&#22312;&#20107;&#25925;&#21457;&#29983;&#21518;&#65292;&#37325;&#35201;&#30340;&#26159;&#35201;&#30830;&#23450;&#22240;&#26524;&#20851;&#31995;&#36129;&#29486;&#21644;&#32972;&#21518;&#30340;&#19981;&#21516;&#36131;&#20219;&#20027;&#20307;&#65292;&#20197;&#20174;&#38169;&#35823;&#20013;&#27762;&#21462;&#25945;&#35757;&#65292;&#38450;&#27490;&#31867;&#20284;&#30340;&#26410;&#26469;&#20107;&#20214;&#12290;&#35768;&#22810;&#20316;&#32773;&#24050;&#32463;&#35780;&#35770;&#20102;&#8220;&#36131;&#20219;&#24046;&#36317;&#8221;&#65292;&#21363;&#24320;&#21457;&#20154;&#21592;&#21644;&#21046;&#36896;&#21830;&#24456;&#38590;&#23545;AI-SCS&#30340;&#26377;&#23475;&#34892;&#20026;&#36127;&#36131;&#12290;&#36825;&#26159;&#30001;&#20110;AI&#24320;&#21457;&#21608;&#26399;&#22797;&#26434;&#12289;AI&#24615;&#33021;&#19981;&#30830;&#23450;&#21644;&#21160;&#24577;&#25805;&#20316;&#29615;&#22659;&#25152;&#33268;&#12290;&#20154;&#31867;&#25805;&#20316;&#21592;&#21487;&#33021;&#25104;&#20026;&#8220;&#36131;&#20219;&#26367;&#32618;&#32650;&#8221;&#65292;&#22240;AI-SCS&#36755;&#20986;&#30340;&#21518;&#26524;&#25215;&#25285;&#36131;&#20219;&#65292;&#36825;&#20123;&#21518;&#26524;&#20182;&#20204;&#27809;&#26377;&#21442;&#19982;&#21019;&#24314;&#65292;&#20063;&#21487;&#33021;&#19981;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI-Based Safety-Critical Systems (AI-SCS) are being increasingly deployed in the real world. These can pose a risk of harm to people and the environment. Reducing that risk is an overarching priority during development and operation. As more AI-SCS become autonomous, a layer of risk management via human intervention has been removed. Following an accident it will be important to identify causal contributions and the different responsible actors behind those to learn from mistakes and prevent similar future events. Many authors have commented on the "responsibility gap" where it is difficult for developers and manufacturers to be held responsible for harmful behaviour of an AI-SCS. This is due to the complex development cycle for AI, uncertainty in AI performance, and dynamic operating environment. A human operator can become a "liability sink" absorbing blame for the consequences of AI-SCS outputs they weren't responsible for creating, and may not have understanding of.  This cross-dis
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32422;&#26463;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#38598;&#25104;&#21355;&#26143;-&#22320;&#38754;&#32593;&#32476;&#65288;ISTN&#65289;&#31995;&#32479;&#30340;&#21160;&#24577;&#36335;&#30001;&#38382;&#39064;&#65292;&#26377;&#25928;&#24179;&#34913;&#20102;&#24555;&#36895;&#36890;&#20449;&#12289;&#33021;&#28304;&#25928;&#29575;&#21644;&#25968;&#25454;&#21253;&#20002;&#22833;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2401.09455</link><description>&lt;p&gt;
&#38598;&#25104;&#21355;&#26143;-&#22320;&#38754;&#32593;&#32476;&#30340;&#21160;&#24577;&#36335;&#30001;&#65306;&#22522;&#20110;&#32422;&#26463;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Dynamic Routing for Integrated Satellite-Terrestrial Networks: A Constrained Multi-Agent Reinforcement Learning Approach. (arXiv:2401.09455v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32422;&#26463;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#38598;&#25104;&#21355;&#26143;-&#22320;&#38754;&#32593;&#32476;&#65288;ISTN&#65289;&#31995;&#32479;&#30340;&#21160;&#24577;&#36335;&#30001;&#38382;&#39064;&#65292;&#26377;&#25928;&#24179;&#34913;&#20102;&#24555;&#36895;&#36890;&#20449;&#12289;&#33021;&#28304;&#25928;&#29575;&#21644;&#25968;&#25454;&#21253;&#20002;&#22833;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#25104;&#21355;&#26143;-&#22320;&#38754;&#32593;&#32476;&#65288;ISTN&#65289;&#31995;&#32479;&#32463;&#21382;&#20102;&#26174;&#33879;&#22686;&#38271;&#65292;&#20026;&#20559;&#36828;&#22320;&#21306;&#25552;&#20379;&#20102;&#26080;&#32541;&#36890;&#20449;&#26381;&#21153;&#65292;&#35299;&#20915;&#20102;&#26377;&#38480;&#30340;&#22320;&#38754;&#22522;&#30784;&#35774;&#26045;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20026;ISTN&#35774;&#35745;&#36335;&#30001;&#26041;&#26696;&#26497;&#20855;&#25361;&#25112;&#24615;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#22686;&#21152;&#20102;&#22320;&#38754;&#31449;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#35201;&#27714;&#28385;&#36275;&#19982;&#21355;&#26143;&#26381;&#21153;&#36136;&#37327;&#26377;&#20851;&#30340;&#21508;&#31181;&#32422;&#26463;&#26465;&#20214;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19982;&#22320;&#38754;&#31449;&#21644;&#21355;&#26143;&#20849;&#21516;&#20256;&#36755;&#25968;&#25454;&#21253;&#30340;&#36335;&#30001;&#65292;&#21516;&#26102;&#20248;&#20808;&#32771;&#34385;&#24555;&#36895;&#36890;&#20449;&#12289;&#28385;&#36275;&#33021;&#28304;&#25928;&#29575;&#21644;&#25968;&#25454;&#21253;&#20002;&#22833;&#35201;&#27714;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#25289;&#26684;&#26391;&#26085;&#26041;&#27861;&#23558;&#24102;&#32422;&#26463;&#30340;&#25968;&#25454;&#21253;&#36335;&#30001;&#38382;&#39064;&#21046;&#23450;&#20026;&#19968;&#20010;&#26368;&#22823;&#26368;&#23567;&#21270;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CMADR&#30340;&#26032;&#39062;&#32422;&#26463;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#21160;&#24577;&#36335;&#30001;&#31639;&#27861;&#65292;&#23427;&#26377;&#25928;&#22320;&#24179;&#34913;&#30446;&#26631;&#25913;&#21892;&#21644;&#32422;&#26463;&#28385;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;
The integrated satellite-terrestrial network (ISTN) system has experienced significant growth, offering seamless communication services in remote areas with limited terrestrial infrastructure. However, designing a routing scheme for ISTN is exceedingly difficult, primarily due to the heightened complexity resulting from the inclusion of additional ground stations, along with the requirement to satisfy various constraints related to satellite service quality. To address these challenges, we study packet routing with ground stations and satellites working jointly to transmit packets, while prioritizing fast communication and meeting energy efficiency and packet loss requirements. Specifically, we formulate the problem of packet routing with constraints as a max-min problem using the Lagrange method. Then we propose a novel constrained Multi-Agent reinforcement learning (MARL) dynamic routing algorithm named CMADR, which efficiently balances objective improvement and constraint satisfacti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#29992;&#25143;&#27880;&#35270;&#27880;&#24847;&#21147;&#23545;&#40784;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#22312;&#22788;&#29702;&#22797;&#26434;&#22330;&#26223;&#21644;&#22810;&#20010;&#29289;&#20307;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.09454</link><description>&lt;p&gt;
Voila-A: &#29992;&#29992;&#25143;&#27880;&#35270;&#27880;&#24847;&#21147;&#23545;&#40784;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Voila-A: Aligning Vision-Language Models with User's Gaze Attention. (arXiv:2401.09454v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#29992;&#25143;&#27880;&#35270;&#27880;&#24847;&#21147;&#23545;&#40784;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#22312;&#22788;&#29702;&#22797;&#26434;&#22330;&#26223;&#21644;&#22810;&#20010;&#29289;&#20307;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#20960;&#24180;&#20013;&#65292;&#35270;&#35273;&#21644;&#35821;&#35328;&#29702;&#35299;&#30340;&#25972;&#21512;&#36890;&#36807;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#35201;&#31361;&#30772;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;VLMs&#22312;&#22788;&#29702;&#22797;&#26434;&#22330;&#26223;&#21644;&#22810;&#20010;&#29289;&#20307;&#30340;&#23454;&#38469;&#24212;&#29992;&#20197;&#21450;&#19982;&#20154;&#31867;&#29992;&#25143;&#30340;&#22810;&#26679;&#21270;&#27880;&#24847;&#21147;&#27169;&#24335;&#30456;&#19968;&#33268;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#36890;&#36807;&#22686;&#24378;&#29616;&#23454;&#65288;AR&#65289;&#25110;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#35774;&#22791;&#25910;&#38598;&#30340;&#27880;&#35270;&#20449;&#24687;&#65292;&#20316;&#20026;&#20154;&#31867;&#27880;&#24847;&#21147;&#30340;&#20195;&#29702;&#26469;&#24341;&#23548;VLMs&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;Voila-A&#65292;&#20197;&#25552;&#39640;&#36825;&#20123;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#25928;&#26524;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#25968;&#30334;&#20998;&#38047;&#30340;&#27880;&#35270;&#25968;&#25454;&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#26412;&#22320;&#21270;&#30340;&#21465;&#20107;&#26469;&#27169;&#25311;&#20154;&#31867;&#30340;&#27880;&#35270;&#26041;&#24335;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#33258;&#21160;&#25968;&#25454;&#27880;&#37322;&#27969;&#27700;&#32447;&#65292;&#21033;&#29992;GPT-4&#29983;&#25104;&#20102;VOILA-COCO&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21019;&#26032;&#20102;Voila Perceiver&#27169;&#22359;&#65292;&#23558;&#27880;&#35270;&#20449;&#24687;&#25972;&#21512;&#21040;VL&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the integration of vision and language understanding has led to significant advancements in artificial intelligence, particularly through Vision-Language Models (VLMs). However, existing VLMs face challenges in handling real-world applications with complex scenes and multiple objects, as well as aligning their focus with the diverse attention patterns of human users. In this paper, we introduce gaze information, feasibly collected by AR or VR devices, as a proxy for human attention to guide VLMs and propose a novel approach, Voila-A, for gaze alignment to enhance the interpretability and effectiveness of these models in real-world applications. First, we collect hundreds of minutes of gaze data to demonstrate that we can mimic human gaze modalities using localized narratives. We then design an automatic data annotation pipeline utilizing GPT-4 to generate the VOILA-COCO dataset. Additionally, we innovate the Voila Perceiver modules to integrate gaze information into VL
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#40654;&#26364;&#20960;&#20309;&#29305;&#24449;&#24212;&#29992;&#20110;&#23398;&#20064;&#32764;&#38754;&#21387;&#21147;&#31995;&#25968;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#27668;&#21160;&#31995;&#25968;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.09452</link><description>&lt;p&gt;
&#20351;&#29992;&#40654;&#26364;&#20960;&#20309;&#29305;&#24449;&#23398;&#20064;&#39134;&#26426;&#26426;&#32764;&#19978;&#30340;&#21387;&#21147;&#31995;&#25968;
&lt;/p&gt;
&lt;p&gt;
Incorporating Riemannian Geometric Features for Learning Coefficient of Pressure Distributions on Airplane Wings. (arXiv:2401.09452v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09452
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#40654;&#26364;&#20960;&#20309;&#29305;&#24449;&#24212;&#29992;&#20110;&#23398;&#20064;&#32764;&#38754;&#21387;&#21147;&#31995;&#25968;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#27668;&#21160;&#31995;&#25968;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39134;&#26426;&#30340;&#27668;&#21160;&#31995;&#25968;&#21463;&#20854;&#20960;&#20309;&#24418;&#29366;&#30340;&#26174;&#33879;&#24433;&#21709;&#65292;&#23588;&#20854;&#26159;&#24403;&#25915;&#35282;&#36739;&#22823;&#26102;&#12290;&#22312;&#31354;&#27668;&#21160;&#21147;&#23398;&#39046;&#22495;&#65292;&#20256;&#32479;&#30340;&#22522;&#20110;&#22810;&#39033;&#24335;&#30340;&#21442;&#25968;&#21270;&#26041;&#27861;&#20351;&#29992;&#23613;&#21487;&#33021;&#23569;&#30340;&#21442;&#25968;&#26469;&#25551;&#36848;&#32764;&#22411;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32764;&#30340;&#19977;&#32500;&#20960;&#20309;&#24418;&#29366;&#27604;&#20108;&#32500;&#32764;&#22411;&#22797;&#26434;&#65292;&#22522;&#20110;&#22810;&#39033;&#24335;&#30340;&#21442;&#25968;&#21270;&#26041;&#27861;&#38590;&#20197;&#20934;&#30830;&#34920;&#31034;&#32764;&#22312;&#19977;&#32500;&#31354;&#38388;&#20013;&#30340;&#25972;&#20307;&#24418;&#29366;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#21462;&#29992;&#20110;&#25551;&#36848;&#20108;&#32500;&#32764;&#22411;&#25110;&#32764;&#25130;&#38754;&#24418;&#29366;&#30340;&#22823;&#37327;&#28508;&#22312;&#31070;&#32463;&#34920;&#31034;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30452;&#25509;&#23558;&#20960;&#20309;&#29305;&#24449;&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#30340;&#27668;&#21160;&#31995;&#25968;&#30340;&#20934;&#30830;&#24615;&#12290;&#21463;&#20960;&#20309;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#40654;&#26364;&#20960;&#20309;&#29305;&#24449;&#32435;&#20837;&#23398;&#20064;&#32764;&#38754;&#21387;&#21147;&#31995;&#25968;&#20998;&#24067;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35745;&#31639;&#20960;&#20309;&#29305;&#24449;&#65288;&#40654;&#26364;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aerodynamic coefficients of aircrafts are significantly impacted by its geometry, especially when the angle of attack (AoA) is large. In the field of aerodynamics, traditional polynomial-based parameterization uses as few parameters as possible to describe the geometry of an airfoil. However, because the 3D geometry of a wing is more complicated than the 2D airfoil, polynomial-based parameterizations have difficulty in accurately representing the entire shape of a wing in 3D space. Existing deep learning-based methods can extract massive latent neural representations for the shape of 2D airfoils or 2D slices of wings. Recent studies highlight that directly taking geometric features as inputs to the neural networks can improve the accuracy of predicted aerodynamic coefficients. Motivated by geometry theory, we propose to incorporate Riemannian geometric features for learning Coefficient of Pressure (CP) distributions on wing surfaces. Our method calculates geometric features (Rieman
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#39537;&#21160;&#30340;&#29983;&#25104;&#26694;&#26550;\method{}&#65292;&#29992;&#20110;&#39044;&#27979;&#20998;&#23376;&#30340;&#19977;&#32500;&#26500;&#35937;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#39044;&#27979;&#31934;&#24230;&#24182;&#25913;&#36827;&#20102;&#20256;&#32479;&#26041;&#27861;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2401.09451</link><description>&lt;p&gt;
&#25193;&#25955;&#39537;&#21160;&#30340;&#20998;&#23376;&#26500;&#35937;&#39044;&#27979;&#29983;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Diffusion-Driven Generative Framework for Molecular Conformation Prediction. (arXiv:2401.09451v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09451
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#39537;&#21160;&#30340;&#29983;&#25104;&#26694;&#26550;\method{}&#65292;&#29992;&#20110;&#39044;&#27979;&#20998;&#23376;&#30340;&#19977;&#32500;&#26500;&#35937;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#39044;&#27979;&#31934;&#24230;&#24182;&#25913;&#36827;&#20102;&#20256;&#32479;&#26041;&#27861;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20108;&#32500;&#22270;&#24418;&#34920;&#31034;&#20013;&#25512;&#26029;&#20986;&#19977;&#32500;&#20998;&#23376;&#26500;&#22411;&#30340;&#20219;&#21153;&#22312;&#35745;&#31639;&#21270;&#23398;&#21644;&#33647;&#29289;&#24320;&#21457;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#23427;&#23545;&#25105;&#20204;&#29702;&#35299;&#20998;&#23376;&#26426;&#21046;&#21644;&#30456;&#20114;&#20316;&#29992;&#36215;&#30528;&#22522;&#26412;&#20316;&#29992;&#12290;&#26426;&#22120;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#29983;&#25104;&#32593;&#32476;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#25512;&#21160;&#20102;&#36825;&#31181;&#39044;&#27979;&#24314;&#27169;&#31934;&#24230;&#30340;&#31361;&#30772;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#20998;&#21449;&#31574;&#30053;&#65306;&#39318;&#20808;&#20272;&#35745;&#21407;&#23376;&#38388;&#36317;&#65292;&#28982;&#21518;&#36890;&#36807;&#35299;&#20915;&#36317;&#31163;&#20960;&#20309;&#38382;&#39064;&#26469;&#22609;&#36896;&#20998;&#23376;&#30340;&#31354;&#38388;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#39034;&#24207;&#26041;&#27861;&#26377;&#26102;&#26080;&#27861;&#20934;&#30830;&#25429;&#25417;&#21040;&#23616;&#37096;&#21407;&#23376;&#25490;&#21015;&#30340;&#22797;&#26434;&#24615;&#65292;&#20174;&#32780;&#25439;&#23475;&#32467;&#26524;&#32467;&#26500;&#27169;&#22411;&#30340;&#23436;&#25972;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#19981;&#36275;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#21069;&#21355;&#30340;&#29983;&#25104;&#26694;&#26550;&#65306;\method{}&#65292;&#23427;&#22522;&#20110;&#25193;&#25955;&#39537;&#21160;&#30340;&#26041;&#27861;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#21462;&#24471;&#20102;&#37325;&#35201;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of inferring three-dimensional molecular configurations from their two-dimensional graph representations is of critical significance in the domains of computational chemistry and the development of pharmaceuticals. It contributes fundamentally to our grasp of molecular mechanisms and interactions. The rapid evolution of machine learning, especially in the realm of deep generative networks, has catalyzed breakthroughs in the precision of such predictive modeling. Traditional methodologies typically employ a bifurcated strategy: initially estimating interatomic distances followed by sculpting the spatial molecular structure via solving a distance geometry problem. This sequential approach, however, occasionally fails to capture the intricacies of local atomic arrangements accurately, thus compromising the integrity of the resultant structural models. Addressing these deficiencies, this work introduces an avant-garde generative framework: \method{}, which is predicated on the dif
&lt;/p&gt;</description></item><item><title>EMPAIA&#20513;&#35758;&#27719;&#38598;&#20102;&#30149;&#29702;&#23398;&#39046;&#22495;&#30340;&#21033;&#30410;&#30456;&#20851;&#26041;&#65292;&#24320;&#21457;&#20102;&#25216;&#26415;&#20114;&#25805;&#20316;&#26631;&#20934;&#65292;&#26631;&#20934;&#21270;&#25509;&#21475;&#65292;&#20197;&#21450;&#25512;&#24191;&#21644;&#24212;&#29992;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#20110;&#30149;&#29702;&#23398;&#35786;&#26029;&#30340;&#24314;&#35758;&#65292;&#23454;&#29616;&#20102;&#19981;&#21516;&#20379;&#24212;&#21830;&#30340;&#22810;&#20010;AI&#24212;&#29992;&#31243;&#24207;&#25972;&#21512;&#12290;</title><link>http://arxiv.org/abs/2401.09450</link><description>&lt;p&gt;
&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#21327;&#21161;&#30340;&#30149;&#29702;&#35786;&#26029;&#20013;&#30340;&#21512;&#20316;&#65306;EMPAIA&#35745;&#21010;
&lt;/p&gt;
&lt;p&gt;
Joining Forces for Pathology Diagnostics with AI Assistance: The EMPAIA Initiative. (arXiv:2401.09450v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09450
&lt;/p&gt;
&lt;p&gt;
EMPAIA&#20513;&#35758;&#27719;&#38598;&#20102;&#30149;&#29702;&#23398;&#39046;&#22495;&#30340;&#21033;&#30410;&#30456;&#20851;&#26041;&#65292;&#24320;&#21457;&#20102;&#25216;&#26415;&#20114;&#25805;&#20316;&#26631;&#20934;&#65292;&#26631;&#20934;&#21270;&#25509;&#21475;&#65292;&#20197;&#21450;&#25512;&#24191;&#21644;&#24212;&#29992;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#20110;&#30149;&#29702;&#23398;&#35786;&#26029;&#30340;&#24314;&#35758;&#65292;&#23454;&#29616;&#20102;&#19981;&#21516;&#20379;&#24212;&#21830;&#30340;&#22810;&#20010;AI&#24212;&#29992;&#31243;&#24207;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#30149;&#29702;&#23398;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#26377;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25216;&#26415;&#21644;&#30417;&#31649;&#26041;&#38754;&#30340;&#31181;&#31181;&#25361;&#25112;&#65292;&#23558;&#36825;&#20123;&#30740;&#31350;&#25104;&#26524;&#36716;&#21270;&#20026;&#20020;&#24202;&#35786;&#26029;&#20135;&#21697;&#24182;&#25512;&#24191;&#24212;&#29992;&#30340;&#36895;&#24230;&#20173;&#28982;&#36739;&#24930;&#65292;&#20854;&#20013;&#21253;&#25324;&#32570;&#20047;&#26631;&#20934;&#21270;&#30340;&#25509;&#21475;&#12290;&#24320;&#25918;&#21644;&#20379;&#24212;&#21830;&#20013;&#31435;&#30340;EMPAIA&#35745;&#21010;&#33268;&#21147;&#20110;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;EMPAIA&#30340;&#25104;&#26524;&#21644;&#32463;&#39564;&#25945;&#35757;&#12290;EMPAIA&#23558;&#30149;&#29702;&#23398;AI&#29983;&#24577;&#31995;&#32479;&#30340;&#21508;&#31181;&#21033;&#30410;&#30456;&#20851;&#26041;&#65292;&#22914;&#30149;&#29702;&#23398;&#23478;&#12289;&#35745;&#31639;&#26426;&#31185;&#23398;&#23478;&#21644;&#24037;&#19994;&#30028;&#36827;&#34892;&#32039;&#23494;&#21512;&#20316;&#12290;&#22312;&#32039;&#23494;&#21512;&#20316;&#20013;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#25216;&#26415;&#20114;&#25805;&#20316;&#24615;&#26631;&#20934;&#65292;AI&#27979;&#35797;&#21644;&#20135;&#21697;&#24320;&#21457;&#30340;&#24314;&#35758;&#65292;&#20197;&#21450;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#27169;&#22359;&#21270;&#21644;&#24320;&#28304;&#30340;EMPAIA&#24179;&#21488;&#65292;&#24182;&#25104;&#21151;&#23558;&#26469;&#33258;6&#20010;&#19981;&#21516;&#20379;&#24212;&#21830;&#30340;11&#20010;&#22522;&#20110;AI&#30340;&#22270;&#20687;&#20998;&#26512;&#24212;&#29992;&#31243;&#24207;&#38598;&#25104;&#21040;&#35813;&#24179;&#21488;&#19978;&#65292;&#23637;&#31034;&#20102;&#19981;&#21516;&#24212;&#29992;&#31243;&#24207;&#22914;&#20309;&#20351;&#29992;&#32479;&#19968;&#30340;&#26631;&#20934;&#21270;&#25509;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past decade, artificial intelligence (AI) methods in pathology have advanced substantially. However, integration into routine clinical practice has been slow due to numerous challenges, including technical and regulatory hurdles in translating research results into clinical diagnostic products and the lack of standardized interfaces. The open and vendor-neutral EMPAIA initiative addresses these challenges. Here, we provide an overview of EMPAIA's achievements and lessons learned. EMPAIA integrates various stakeholders of the pathology AI ecosystem, i.e., pathologists, computer scientists, and industry. In close collaboration, we developed technical interoperability standards, recommendations for AI testing and product development, and explainability methods. We implemented the modular and open-source EMPAIA platform and successfully integrated 11 AI-based image analysis apps from 6 different vendors, demonstrating how different apps can use a single standardized interface. We 
&lt;/p&gt;</description></item><item><title>Tumbug&#26159;&#19968;&#31181;&#22270;&#20687;&#21270;&#30340;&#36890;&#29992;&#30693;&#35782;&#34920;&#36798;&#26041;&#27861;&#65292;&#19987;&#38376;&#29992;&#20110;&#24120;&#35782;&#25512;&#29702;&#65292;&#36890;&#36807;&#20351;&#29992;&#32422;30&#20010;&#22522;&#20110;&#31185;&#23398;&#21644;&#20154;&#31867;&#29983;&#27963;&#30340;&#22522;&#26412;&#27010;&#24565;&#32452;&#20214;&#65292;&#23558;&#20854;&#19982;&#20197;&#24448;&#30340;&#27010;&#24565;&#20381;&#23384;&#29702;&#35770;&#36827;&#34892;&#21306;&#20998;&#12290;</title><link>http://arxiv.org/abs/2401.09448</link><description>&lt;p&gt;
Tumbug:&#19968;&#31181;&#22270;&#20687;&#21270;&#30340;&#36890;&#29992;&#30693;&#35782;&#34920;&#36798;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Tumbug: A pictorial, universal knowledge representation method. (arXiv:2401.09448v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09448
&lt;/p&gt;
&lt;p&gt;
Tumbug&#26159;&#19968;&#31181;&#22270;&#20687;&#21270;&#30340;&#36890;&#29992;&#30693;&#35782;&#34920;&#36798;&#26041;&#27861;&#65292;&#19987;&#38376;&#29992;&#20110;&#24120;&#35782;&#25512;&#29702;&#65292;&#36890;&#36807;&#20351;&#29992;&#32422;30&#20010;&#22522;&#20110;&#31185;&#23398;&#21644;&#20154;&#31867;&#29983;&#27963;&#30340;&#22522;&#26412;&#27010;&#24565;&#32452;&#20214;&#65292;&#23558;&#20854;&#19982;&#20197;&#24448;&#30340;&#27010;&#24565;&#20381;&#23384;&#29702;&#35770;&#36827;&#34892;&#21306;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;(AGI)&#30340;&#20851;&#38190;&#34987;&#26222;&#36941;&#35748;&#20026;&#26159;&#24120;&#35782;&#25512;&#29702;(CSR)&#65292;&#25110;&#32773;&#31895;&#30053;&#22320;&#35828;&#23601;&#26159;&#21457;&#29616;&#19968;&#31181;&#29305;&#21035;&#36866;&#29992;&#20110;CSR&#30340;&#30693;&#35782;&#34920;&#31034;&#26041;&#27861;(KRM)&#65292;&#20316;&#32773;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#23450;&#20041;&#30340;CSR KRM&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;KRM&#31216;&#20026;Tumbug&#65292;&#20854;&#35774;&#35745;&#20026;&#22270;&#20687;&#21270;&#65292;&#22240;&#20026;&#36234;&#26469;&#36234;&#22810;&#30340;&#35777;&#25454;&#34920;&#26126;&#20154;&#33041;&#20351;&#29992;&#20102;&#26576;&#31181;&#22270;&#20687;&#21270;&#31867;&#22411;&#30340;KRM&#65292;&#32780;&#22312;AGI&#39046;&#22495;&#20013;&#27809;&#26377;&#30693;&#21517;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#36825;&#31181;KRM&#21487;&#33021;&#24615;&#12290;Tumbug&#19982;Roger Schank&#30340;&#27010;&#24565;&#20381;&#23384;&#29702;&#35770;(CD)&#26377;&#20123;&#30456;&#20284;&#65292;&#20294;Tumbug&#26159;&#22270;&#20687;&#21270;&#30340;&#65292;&#24182;&#20351;&#29992;&#20102;&#22823;&#32422;30&#20010;&#22522;&#20110;&#31185;&#23398;&#21644;&#20154;&#31867;&#29983;&#27963;&#30340;&#22522;&#26412;&#27010;&#24565;&#30340;&#32452;&#20214;&#65292;&#32780;CD&#29702;&#35770;&#26159;&#25991;&#26412;&#22411;&#30340;&#65292;&#24182;&#20351;&#29992;&#20102;&#22823;&#32422;17&#20010;&#22522;&#20110;&#20154;&#31867;&#23548;&#21521;&#27963;&#21160;&#30340;&#32452;&#20214;(= 6&#20010;&#21407;&#22987;&#27010;&#24565;&#31867;&#21035;+ 11&#20010;&#21407;&#22987;&#34892;&#20026;)&#12290;&#25152;&#26377;Tumbug&#30340;&#26500;&#24314;&#22359;&#37117;&#34987;&#21457;&#29616;&#33021;&#22815;&#25512;&#24191;&#21040;&#20165;&#26377;&#30340;&#20116;&#20010;&#22522;&#26412;&#26500;&#24314;&#22359;&#65292;&#36825;&#20123;&#26500;&#24314;&#22359;&#19982;&#30495;&#23454;&#19990;&#30028;&#20013;&#23545;&#24212;&#30340;&#20869;&#23481;&#23436;&#20840;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the key to artificial general intelligence (AGI) is commonly believed to be commonsense reasoning (CSR) or, roughly equivalently, discovery of a knowledge representation method (KRM) that is particularly suitable for CSR, the author developed a custom KRM for CSR. This novel KRM called Tumbug was designed to be pictorial in nature because there exists increasing evidence that the human brain uses some pictorial type of KRM, and no well-known prior research in AGI has researched this KRM possibility. Tumbug is somewhat similar to Roger Schank's Conceptual Dependency (CD) theory, but Tumbug is pictorial and uses about 30 components based on fundamental concepts from the sciences and human life, in contrast to CD theory, which is textual and uses about 17 components (= 6 Primitive Conceptual Categories + 11 Primitive Acts) based mainly on human-oriented activities. All the Building Blocks of Tumbug were found to generalize to only five Basic Building Blocks that exactly correspond t
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#26041;&#27861;&#26469;&#35299;&#37322;&#23391;&#21152;&#25289;&#35821;Memes&#30340;&#24773;&#24863;&#65292;&#20197;&#22635;&#34917;&#27492;&#39046;&#22495;&#20013;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;&#23545;&#27604;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;MemoSen&#25968;&#25454;&#38598;&#24182;&#34920;&#26126;&#20854;&#20934;&#30830;&#29575;&#30340;&#23616;&#38480;&#24615;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#22312;&#23391;&#21152;&#25289;&#35821;Memes&#24773;&#24863;&#20998;&#26512;&#39046;&#22495;&#24341;&#20837;&#20102;&#22810;&#27169;&#24577;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.09446</link><description>&lt;p&gt;
&#12298;&#21487;&#35299;&#37322;&#30340;&#23391;&#21152;&#25289;&#35821;Memes&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#12299;
&lt;/p&gt;
&lt;p&gt;
Explainable Multimodal Sentiment Analysis on Bengali Memes. (arXiv:2401.09446v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09446
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#26041;&#27861;&#26469;&#35299;&#37322;&#23391;&#21152;&#25289;&#35821;Memes&#30340;&#24773;&#24863;&#65292;&#20197;&#22635;&#34917;&#27492;&#39046;&#22495;&#20013;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;&#23545;&#27604;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;MemoSen&#25968;&#25454;&#38598;&#24182;&#34920;&#26126;&#20854;&#20934;&#30830;&#29575;&#30340;&#23616;&#38480;&#24615;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#22312;&#23391;&#21152;&#25289;&#35821;Memes&#24773;&#24863;&#20998;&#26512;&#39046;&#22495;&#24341;&#20837;&#20102;&#22810;&#27169;&#24577;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Memes&#24050;&#25104;&#20026;&#25968;&#23383;&#26102;&#20195;&#29420;&#29305;&#32780;&#26377;&#25928;&#30340;&#27807;&#36890;&#24418;&#24335;&#65292;&#21560;&#24341;&#20102;&#22312;&#32447;&#31038;&#21306;&#65292;&#24182;&#36328;&#36234;&#25991;&#21270;&#38556;&#30861;&#12290;&#23613;&#31649;Memes&#32463;&#24120;&#21644;&#24189;&#40664;&#32852;&#31995;&#22312;&#19968;&#36215;&#65292;&#20294;&#23427;&#20204;&#26377;&#30528;&#20256;&#36798;&#24191;&#27867;&#24773;&#24863;&#30340;&#24778;&#20154;&#33021;&#21147;&#65292;&#21253;&#25324;&#24555;&#20048;&#12289;&#35773;&#21050;&#12289;&#27822;&#20007;&#31561;&#12290;&#22312;&#20449;&#24687;&#26102;&#20195;&#65292;&#29702;&#35299;&#21644;&#35299;&#37322;Memes&#32972;&#21518;&#30340;&#24773;&#24863;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#25506;&#32034;&#20102;&#22522;&#20110;&#25991;&#26412;&#12289;&#22522;&#20110;&#22270;&#20687;&#21644;&#22810;&#27169;&#24577;&#26041;&#27861;&#65292;&#23548;&#33268;&#20102;&#20687;CAPSAN&#21644;PromptHate&#36825;&#26679;&#30340;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#29992;&#20110;&#26816;&#27979;&#21508;&#31181;Memes&#31867;&#21035;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#23391;&#21152;&#25289;&#35821;Memes&#36825;&#26679;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#30740;&#31350;&#20173;&#28982;&#31232;&#32570;&#65292;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#25968;&#37327;&#26377;&#38480;&#12290;&#26368;&#36817;&#30340;&#19968;&#20010;&#36129;&#29486;&#26159;&#24341;&#20837;&#20102;MemoSen&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#25152;&#23454;&#29616;&#30340;&#20934;&#30830;&#29575;&#26126;&#26174;&#36739;&#20302;&#65292;&#24182;&#19988;&#25968;&#25454;&#38598;&#20998;&#24067;&#19981;&#24179;&#34913;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;ResNet50&#21644;&#22810;&#27169;&#24577;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Memes have become a distinctive and effective form of communication in the digital era, attracting online communities and cutting across cultural barriers. Even though memes are frequently linked with humor, they have an amazing capacity to convey a wide range of emotions, including happiness, sarcasm, frustration, and more. Understanding and interpreting the sentiment underlying memes has become crucial in the age of information. Previous research has explored text-based, image-based, and multimodal approaches, leading to the development of models like CAPSAN and PromptHate for detecting various meme categories. However, the study of low-resource languages like Bengali memes remains scarce, with limited availability of publicly accessible datasets. A recent contribution includes the introduction of the MemoSen dataset. However, the achieved accuracy is notably low, and the dataset suffers from imbalanced distribution. In this study, we employed a multimodal approach using ResNet50 and
&lt;/p&gt;</description></item><item><title>&#12298;AI&#22312;&#32447;&#36777;&#35770;&#25163;&#20876;&#12299;&#30340;&#31532;&#22235;&#21367;&#26159;&#20026;&#20102;&#20026;&#36777;&#35770;&#30740;&#31350;&#31038;&#21306;&#25552;&#20379;&#24320;&#25918;&#35775;&#38382;&#21644;&#31574;&#21010;&#30340;&#25991;&#38598;&#65292;&#20027;&#35201;&#32858;&#28966;&#20110;&#23545;&#36777;&#35770;&#30340;&#35745;&#31639;&#27169;&#22411;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.09444</link><description>&lt;p&gt;
AI&#22312;&#32447;&#36777;&#35770;&#25163;&#20876;&#65306;&#31532;&#22235;&#21367;
&lt;/p&gt;
&lt;p&gt;
Online Handbook of Argumentation for AI: Volume 4. (arXiv:2401.09444v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09444
&lt;/p&gt;
&lt;p&gt;
&#12298;AI&#22312;&#32447;&#36777;&#35770;&#25163;&#20876;&#12299;&#30340;&#31532;&#22235;&#21367;&#26159;&#20026;&#20102;&#20026;&#36777;&#35770;&#30740;&#31350;&#31038;&#21306;&#25552;&#20379;&#24320;&#25918;&#35775;&#38382;&#21644;&#31574;&#21010;&#30340;&#25991;&#38598;&#65292;&#20027;&#35201;&#32858;&#28966;&#20110;&#23545;&#36777;&#35770;&#30340;&#35745;&#31639;&#27169;&#22411;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#21367;&#21253;&#21547;&#20102;&#12298;AI&#22312;&#32447;&#36777;&#35770;&#25163;&#20876;&#12299;&#65288;OHAAI&#65289;&#31532;&#22235;&#21367;&#25152;&#36873;&#35770;&#25991;&#30340;&#20462;&#35746;&#29256;&#12290;&#20808;&#21069;&#24050;&#32463;&#25552;&#20986;&#21644;&#30740;&#31350;&#20102;&#27491;&#24335;&#30340;&#36777;&#35770;&#29702;&#35770;&#21644;&#36777;&#35770;&#20132;&#20114;&#65292;&#36825;&#24341;&#21457;&#20102;&#23545;&#36777;&#35770;&#30340;&#35745;&#31639;&#27169;&#22411;&#30340;&#30740;&#31350;&#12290;&#20316;&#20026;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#23376;&#39046;&#22495;&#65292;&#36777;&#35770;&#23545;&#20110;&#23545;&#30693;&#35782;&#30340;&#31526;&#21495;&#34920;&#31034;&#21644;&#21487;&#24223;&#24323;&#25512;&#29702;&#24863;&#20852;&#36259;&#30340;&#30740;&#31350;&#20154;&#21592;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25163;&#20876;&#30340;&#30446;&#30340;&#26159;&#20026;&#36777;&#35770;&#30740;&#31350;&#31038;&#21306;&#25552;&#20379;&#24320;&#25918;&#35775;&#38382;&#21644;&#31574;&#21010;&#30340;&#25991;&#38598;&#12290;OHAAI&#26088;&#22312;&#25104;&#20026;&#19968;&#20010;&#30740;&#31350;&#20013;&#24515;&#65292;&#20197;&#36861;&#36394;&#19982;&#20154;&#24037;&#26234;&#33021;&#30456;&#20851;&#30340;&#25152;&#26377;&#39046;&#22495;&#20013;&#29702;&#35770;&#21644;&#24212;&#29992;&#36777;&#35770;&#30340;&#26368;&#26032;&#21644;&#21363;&#23558;&#20986;&#29256;&#30340;&#21338;&#22763;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
This volume contains revised versions of the papers selected for the fourth volume of the Online Handbook of Argumentation for AI (OHAAI). Previously, formal theories of argument and argument interaction have been proposed and studied, and this has led to the more recent study of computational models of argument. Argumentation, as a field within artificial intelligence (AI), is highly relevant for researchers interested in symbolic representations of knowledge and defeasible reasoning. The purpose of this handbook is to provide an open access and curated anthology for the argumentation research community. OHAAI is designed to serve as a research hub to keep track of the latest and upcoming PhD-driven research on the theory and application of argumentation in all areas related to AI.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21327;&#21516;&#34920;&#31034;&#27169;&#22411;&#30340;&#22270;&#20687;&#34917;&#19969;&#36317;&#31163;&#35745;&#31639;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#26597;&#35810;&#22270;&#20687;&#21644;&#23384;&#20648;&#30340;&#34917;&#19969;&#20043;&#38388;&#30340;&#26368;&#36817;&#37051;&#25628;&#32034;&#25152;&#24102;&#26469;&#30340;&#22797;&#26434;&#24230;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#36793;&#32536;&#29615;&#22659;&#19979;&#36827;&#34892;&#24555;&#36895;&#37096;&#32626;&#23454;&#29992;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2401.09443</link><description>&lt;p&gt;
CRD: &#23454;&#29992;&#24322;&#24120;&#26816;&#27979;&#30340;&#21327;&#21516;&#34920;&#31034;&#36317;&#31163;
&lt;/p&gt;
&lt;p&gt;
CRD: Collaborative Representation Distance for Practical Anomaly Detection. (arXiv:2401.09443v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21327;&#21516;&#34920;&#31034;&#27169;&#22411;&#30340;&#22270;&#20687;&#34917;&#19969;&#36317;&#31163;&#35745;&#31639;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#26597;&#35810;&#22270;&#20687;&#21644;&#23384;&#20648;&#30340;&#34917;&#19969;&#20043;&#38388;&#30340;&#26368;&#36817;&#37051;&#25628;&#32034;&#25152;&#24102;&#26469;&#30340;&#22797;&#26434;&#24230;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#36793;&#32536;&#29615;&#22659;&#19979;&#36827;&#34892;&#24555;&#36895;&#37096;&#32626;&#23454;&#29992;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#32570;&#38519;&#26816;&#27979;&#22312;&#26234;&#33021;&#24037;&#19994;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22522;&#20110;&#34917;&#19969;&#30340;&#26041;&#27861;&#23558;&#35270;&#35273;&#22270;&#20687;&#35270;&#20026;&#26681;&#25454;&#20301;&#32622;&#30340;&#22270;&#20687;&#34917;&#19969;&#38598;&#21512;&#65292;&#23545;&#20135;&#21697;&#20013;&#30340;&#23567;&#32570;&#38519;&#65288;&#22914;&#33647;&#20024;&#19978;&#30340;&#21010;&#30165;&#65289;&#20855;&#26377;&#26356;&#24378;&#30340;&#36776;&#21035;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26597;&#35810;&#22270;&#20687;&#21644;&#23384;&#20648;&#30340;&#34917;&#19969;&#20043;&#38388;&#30340;&#26368;&#36817;&#37051;&#25628;&#32034;&#23558;&#22312;&#26102;&#38388;&#21644;&#31354;&#38388;&#38656;&#27714;&#26041;&#38754;&#21344;&#29992;O(n) &#30340;&#22797;&#26434;&#24230;&#65292;&#23545;&#20110;&#22312;&#36793;&#32536;&#29615;&#22659;&#37096;&#32626;&#32780;&#35328;&#25552;&#20986;&#20102;&#20005;&#26684;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21327;&#21516;&#34920;&#31034;&#27169;&#22411;&#35745;&#31639;&#22270;&#20687;&#34917;&#19969;&#36317;&#31163;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#20174;&#20855;&#26377;L0&#32422;&#26463;&#30340;&#26368;&#36817;&#37051;&#36317;&#31163;&#24320;&#22987;&#65292;&#25105;&#20204;&#25918;&#23485;&#32422;&#26463;&#20026;L2&#32422;&#26463;&#65292;&#24182;&#36890;&#36807;&#23553;&#38381;&#24418;&#24335;&#24555;&#36895;&#35299;&#20915;&#36317;&#31163;&#38382;&#39064;&#65292;&#32780;&#19981;&#38656;&#35201;&#23454;&#38469;&#35775;&#38382;&#21407;&#22987;&#23384;&#20648;&#30340;&#22270;&#20687;&#34917;&#19969;&#38598;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25351;&#20986;&#65292;&#36825;&#31181;&#23553;&#38381;&#24418;&#24335;&#35299;&#20915;&#26041;&#26696;&#30340;&#20027;&#35201;&#35745;&#31639;&#36127;&#25285;&#21487;&#20197;&#30001;&#39640;&#24615;&#33021;&#26381;&#21153;&#22120;&#22312;&#37096;&#32626;&#21069;&#39044;&#20808;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual defect detection plays an important role in intelligent industry. Patch based methods consider visual images as a collection of image patches according to positions, which have stronger discriminative ability for small defects in products, e.g. scratches on pills. However, the nearest neighbor search for the query image and the stored patches will occupy $O(n)$ complexity in terms of time and space requirements, posing strict challenges for deployment in edge environments. In this paper, we propose an alternative approach to the distance calculation of image patches via collaborative representation models. Starting from the nearest neighbor distance with $L_0$ constraint, we relax the constraint to $L_2$ constraint and solve the distance quickly in close-formed without actually accessing the original stored collection of image patches. Furthermore, we point out that the main computational burden of this close-formed solution can be pre-computed by high-performance server before 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35273;&#38382;&#31572;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29289;&#20307;&#23646;&#24615;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#29289;&#20307;&#32423;&#35270;&#35273;&#35821;&#35328;&#23545;&#40784;&#21644;&#22810;&#27169;&#24577;&#22330;&#26223;&#29702;&#35299;&#12290;&#20855;&#20307;&#22320;&#65292;&#35774;&#35745;&#20102;&#23646;&#24615;&#34701;&#21512;&#27169;&#22359;&#21644;&#23545;&#27604;&#30693;&#35782;&#33976;&#39311;&#27169;&#22359;&#65292;&#36890;&#36807;&#20449;&#24687;&#20256;&#36882;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#25552;&#39640;&#20102;&#29289;&#20307;&#32423;&#35270;&#35273;&#29305;&#24449;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#32454;&#31890;&#24230;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.09442</link><description>&lt;p&gt;
&#29289;&#20307;&#23646;&#24615;&#22312;&#35270;&#35273;&#38382;&#31572;&#20013;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Object Attribute Matters in Visual Question Answering. (arXiv:2401.09442v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35273;&#38382;&#31572;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29289;&#20307;&#23646;&#24615;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#29289;&#20307;&#32423;&#35270;&#35273;&#35821;&#35328;&#23545;&#40784;&#21644;&#22810;&#27169;&#24577;&#22330;&#26223;&#29702;&#35299;&#12290;&#20855;&#20307;&#22320;&#65292;&#35774;&#35745;&#20102;&#23646;&#24615;&#34701;&#21512;&#27169;&#22359;&#21644;&#23545;&#27604;&#30693;&#35782;&#33976;&#39311;&#27169;&#22359;&#65292;&#36890;&#36807;&#20449;&#24687;&#20256;&#36882;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#25552;&#39640;&#20102;&#29289;&#20307;&#32423;&#35270;&#35273;&#29305;&#24449;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#32454;&#31890;&#24230;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#38382;&#31572;&#26159;&#19968;&#31181;&#38656;&#35201;&#23545;&#35270;&#35273;&#21644;&#25991;&#26412;&#20449;&#24687;&#36827;&#34892;&#32852;&#21512;&#29702;&#35299;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20165;&#36890;&#36807;&#27880;&#24847;&#21147;&#23618;&#26469;&#25972;&#21512;&#35270;&#35273;&#21644;&#25991;&#26412;&#35821;&#20041;&#26159;&#19981;&#36275;&#20197;&#20840;&#38754;&#29702;&#35299;&#21644;&#23545;&#40784;&#20004;&#31181;&#27169;&#24577;&#30340;&#20449;&#24687;&#30340;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#29289;&#20307;&#23646;&#24615;&#21487;&#20197;&#33258;&#28982;&#22320;&#20316;&#20026;&#19968;&#20010;&#26725;&#26753;&#26469;&#32479;&#19968;&#23427;&#20204;&#65292;&#36825;&#22312;&#20197;&#21069;&#30340;&#30740;&#31350;&#20013;&#34987;&#24573;&#35270;&#20102;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#21033;&#29992;&#29289;&#20307;&#23646;&#24615;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;VQA&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;&#26356;&#22909;&#30340;&#29289;&#20307;&#32423;&#35270;&#35273;&#35821;&#35328;&#23545;&#40784;&#21644;&#22810;&#27169;&#24577;&#22330;&#26223;&#29702;&#35299;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#23646;&#24615;&#34701;&#21512;&#27169;&#22359;&#21644;&#19968;&#20010;&#23545;&#27604;&#30693;&#35782;&#33976;&#39311;&#27169;&#22359;&#12290;&#23646;&#24615;&#34701;&#21512;&#27169;&#22359;&#36890;&#36807;&#20449;&#24687;&#20256;&#36882;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#34701;&#21512;&#23646;&#24615;&#21644;&#35270;&#35273;&#29305;&#24449;&#12290;&#22686;&#24378;&#30340;&#29289;&#20307;&#32423;&#35270;&#35273;&#29305;&#24449;&#26377;&#21161;&#20110;&#35299;&#20915;&#35832;&#22914;&#35745;&#25968;&#38382;&#39064;&#31561;&#32454;&#31890;&#24230;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual question answering is a multimodal task that requires the joint comprehension of visual and textual information. However, integrating visual and textual semantics solely through attention layers is insufficient to comprehensively understand and align information from both modalities. Intuitively, object attributes can naturally serve as a bridge to unify them, which has been overlooked in previous research. In this paper, we propose a novel VQA approach from the perspective of utilizing object attribute, aiming to achieve better object-level visual-language alignment and multimodal scene understanding. Specifically, we design an attribute fusion module and a contrastive knowledge distillation module. The attribute fusion module constructs a multimodal graph neural network to fuse attributes and visual features through message passing. The enhanced object-level visual features contribute to solving fine-grained problem like counting-question. The better object-level visual-langua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#38543;&#26426;&#38598;&#21512;&#29702;&#35770;&#26410;&#26469;&#30340;&#21457;&#23637;&#35758;&#31243;&#65292;&#21253;&#25324;&#25512;&#24191;&#32479;&#35745;&#25512;&#29702;&#12289;&#21457;&#23637;&#20960;&#20309;&#26041;&#27861;&#12289;&#24212;&#29992;&#20110;&#27668;&#20505;&#21464;&#21270;&#21644;&#26426;&#22120;&#23398;&#20064;&#31561;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2401.09435</link><description>&lt;p&gt;
&#38543;&#26426;&#38598;&#21512;&#25512;&#29702;&#65306;&#26410;&#26469;&#24037;&#20316;&#30340;&#35758;&#31243;
&lt;/p&gt;
&lt;p&gt;
Reasoning with random sets: An agenda for the future. (arXiv:2401.09435v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#38543;&#26426;&#38598;&#21512;&#29702;&#35770;&#26410;&#26469;&#30340;&#21457;&#23637;&#35758;&#31243;&#65292;&#21253;&#25324;&#25512;&#24191;&#32479;&#35745;&#25512;&#29702;&#12289;&#21457;&#23637;&#20960;&#20309;&#26041;&#27861;&#12289;&#24212;&#29992;&#20110;&#27668;&#20505;&#21464;&#21270;&#21644;&#26426;&#22120;&#23398;&#20064;&#31561;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#38543;&#26426;&#38598;&#21512;&#21644;&#20449;&#24565;&#20989;&#25968;&#29702;&#35770;&#26410;&#26469;&#24037;&#20316;&#30340;&#28508;&#22312;&#35758;&#31243;&#65292;&#28041;&#21450;&#19968;&#20123;&#20851;&#38190;&#38382;&#39064;&#65306;&#21457;&#23637;&#19968;&#20010;&#23436;&#25972;&#30340;&#32479;&#35745;&#25512;&#29702;&#19982;&#38543;&#26426;&#38598;&#21512;&#30340;&#29702;&#35770;&#65292;&#21253;&#25324;&#36923;&#36753;&#22238;&#24402;&#21644;&#32463;&#20856;&#27010;&#29575;&#27861;&#21017;&#30340;&#25512;&#24191;&#65307;&#36827;&#19968;&#27493;&#21457;&#23637;&#22522;&#20110;&#20960;&#20309;&#26041;&#27861;&#30340;&#19981;&#30830;&#23450;&#24615;&#29702;&#35770;&#65292;&#21253;&#25324;&#19968;&#33324;&#38543;&#26426;&#38598;&#21512;&#12289;&#26356;&#24191;&#27867;&#30340;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#21644;&#26367;&#20195;&#30340;&#20960;&#20309;&#34920;&#31034;&#26041;&#27861;&#65307;&#23558;&#36825;&#19968;&#20840;&#26032;&#29702;&#35770;&#24212;&#29992;&#20110;&#27668;&#20505;&#21464;&#21270;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#31561;&#39640;&#24433;&#21709;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we discuss a potential agenda for future work in the theory of random sets and belief functions, touching upon a number of focal issues: the development of a fully-fledged theory of statistical reasoning with random sets, including the generalisation of logistic regression and of the classical laws of probability; the further development of the geometric approach to uncertainty, to include general random sets, a wider range of uncertainty measures and alternative geometric representations; the application of this new theory to high-impact areas such as climate change, machine learning and statistical learning theory.
&lt;/p&gt;</description></item><item><title>RoleCraft-GLM&#26159;&#19968;&#20010;&#21019;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#20010;&#24615;&#21270;&#35282;&#33394;&#25198;&#28436;&#65292;&#35299;&#20915;&#20102;&#32570;&#20047;&#20010;&#24615;&#21270;&#20114;&#21160;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#29420;&#29305;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#21644;&#32454;&#33268;&#20837;&#24494;&#30340;&#35282;&#33394;&#21457;&#23637;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#20934;&#30830;&#21453;&#26144;&#35282;&#33394;&#20010;&#24615;&#29305;&#24449;&#21644;&#24773;&#24863;&#30340;&#23545;&#35805;&#65292;&#25552;&#21319;&#29992;&#25143;&#21442;&#19982;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.09432</link><description>&lt;p&gt;
RoleCraft-GLM&#65306;&#25512;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20010;&#24615;&#21270;&#35282;&#33394;&#25198;&#28436;
&lt;/p&gt;
&lt;p&gt;
RoleCraft-GLM: Advancing Personalized Role-Playing in Large Language Models. (arXiv:2401.09432v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09432
&lt;/p&gt;
&lt;p&gt;
RoleCraft-GLM&#26159;&#19968;&#20010;&#21019;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#20010;&#24615;&#21270;&#35282;&#33394;&#25198;&#28436;&#65292;&#35299;&#20915;&#20102;&#32570;&#20047;&#20010;&#24615;&#21270;&#20114;&#21160;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#29420;&#29305;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#21644;&#32454;&#33268;&#20837;&#24494;&#30340;&#35282;&#33394;&#21457;&#23637;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#20934;&#30830;&#21453;&#26144;&#35282;&#33394;&#20010;&#24615;&#29305;&#24449;&#21644;&#24773;&#24863;&#30340;&#23545;&#35805;&#65292;&#25552;&#21319;&#29992;&#25143;&#21442;&#19982;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;RoleCraft-GLM&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22686;&#24378;&#20010;&#24615;&#21270;&#35282;&#33394;&#25198;&#28436;&#12290;RoleCraft-GLM&#35299;&#20915;&#20102;&#23545;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#20013;&#32570;&#20047;&#20010;&#24615;&#21270;&#20114;&#21160;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#33021;&#22815;&#35814;&#32454;&#25551;&#32472;&#24773;&#24863;&#32454;&#33147;&#30340;&#35282;&#33394;&#21051;&#30011;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#36129;&#29486;&#20102;&#19968;&#32452;&#29420;&#29305;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#20174;&#20256;&#32479;&#30340;&#20197;&#21517;&#20154;&#20026;&#20013;&#24515;&#30340;&#35282;&#33394;&#36716;&#21464;&#20026;&#22810;&#26679;&#21270;&#30340;&#38750;&#21517;&#20154;&#35282;&#33394;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#35821;&#35328;&#24314;&#27169;&#20114;&#21160;&#30340;&#30495;&#23454;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#21253;&#25324;&#32454;&#33268;&#20837;&#24494;&#30340;&#35282;&#33394;&#21457;&#23637;&#65292;&#30830;&#20445;&#23545;&#35805;&#26082;&#30495;&#23454;&#21448;&#24773;&#24863;&#20849;&#40483;&#12290;&#36890;&#36807;&#22810;&#20010;&#26696;&#20363;&#30740;&#31350;&#39564;&#35777;&#20102;RoleCraft-GLM&#30340;&#26377;&#25928;&#24615;&#65292;&#31361;&#26174;&#20102;&#23427;&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#25216;&#33021;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#29983;&#25104;&#23545;&#35805;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#33021;&#22815;&#20934;&#30830;&#21453;&#26144;&#35282;&#33394;&#30340;&#20010;&#24615;&#29305;&#24449;&#21644;&#24773;&#24863;&#65292;&#20174;&#32780;&#22686;&#24378;&#29992;&#25143;&#21442;&#19982;&#24230;&#12290;&#24635;&#20043;&#65292;RoleCraft-GLM&#26631;&#24535;&#30528;&#19968;&#20010;&#21019;&#26032;&#30340;&#37324;&#31243;&#30865;&#65292;&#25512;&#21160;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20010;&#24615;&#21270;&#35282;&#33394;&#25198;&#28436;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents RoleCraft-GLM, an innovative framework aimed at enhancing personalized role-playing with Large Language Models (LLMs). RoleCraft-GLM addresses the key issue of lacking personalized interactions in conversational AI, and offers a solution with detailed and emotionally nuanced character portrayals. We contribute a unique conversational dataset that shifts from conventional celebrity-centric characters to diverse, non-celebrity personas, thus enhancing the realism and complexity of language modeling interactions. Additionally, our approach includes meticulous character development, ensuring dialogues are both realistic and emotionally resonant. The effectiveness of RoleCraft-GLM is validated through various case studies, highlighting its versatility and skill in different scenarios. Our framework excels in generating dialogues that accurately reflect characters' personality traits and emotions, thereby boosting user engagement. In conclusion, RoleCraft-GLM marks a sign
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#22810;&#20010;&#36731;&#37327;&#32423;&#23398;&#20064;&#22120;&#30340;&#38598;&#25104;&#39044;&#27979;&#38477;&#27700;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#21355;&#26143;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#65292;&#26377;&#25928;&#22320;&#27169;&#25311;&#22797;&#26434;&#30340;&#38477;&#38632;&#27169;&#24335;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#39640;&#38477;&#27700;&#20107;&#20214;&#12290;&#22312;Weather4Cast 2023&#31454;&#36187;&#20013;&#21462;&#24471;&#20102;&#31532;&#19968;&#21517;&#12290;</title><link>http://arxiv.org/abs/2401.09424</link><description>&lt;p&gt;
&#20351;&#29992;&#36731;&#37327;&#32423;&#23398;&#20064;&#22120;&#30340;&#38598;&#25104;&#39044;&#27979;&#38477;&#27700;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Precipitation Prediction Using an Ensemble of Lightweight Learners. (arXiv:2401.09424v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#22810;&#20010;&#36731;&#37327;&#32423;&#23398;&#20064;&#22120;&#30340;&#38598;&#25104;&#39044;&#27979;&#38477;&#27700;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#21355;&#26143;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#65292;&#26377;&#25928;&#22320;&#27169;&#25311;&#22797;&#26434;&#30340;&#38477;&#38632;&#27169;&#24335;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#39640;&#38477;&#27700;&#20107;&#20214;&#12290;&#22312;Weather4Cast 2023&#31454;&#36187;&#20013;&#21462;&#24471;&#20102;&#31532;&#19968;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38477;&#27700;&#39044;&#27979;&#22312;&#29616;&#20195;&#20892;&#19994;&#21644;&#24037;&#19994;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#28982;&#32780;&#30001;&#20110;&#26102;&#38388;&#21644;&#31354;&#38388;&#19978;&#22810;&#26679;&#30340;&#27169;&#24335;&#21644;&#21160;&#24577;&#20197;&#21450;&#39640;&#38477;&#27700;&#20107;&#20214;&#30340;&#31232;&#32570;&#24615;&#65292;&#23427;&#20063;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#20010;&#23398;&#20064;&#22120;&#26469;&#25429;&#25417;&#38477;&#27700;&#20998;&#24067;&#30340;&#22810;&#26679;&#27169;&#24335;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;&#19968;&#20010;&#20855;&#26377;&#22810;&#20010;&#36731;&#37327;&#32423;&#22836;&#37096;&#65288;&#23398;&#20064;&#22120;&#65289;&#30340;&#38477;&#27700;&#39044;&#27979;&#22120;&#21644;&#19968;&#20010;&#25511;&#21046;&#22120;&#65292;&#35813;&#25511;&#21046;&#22120;&#23558;&#36825;&#20123;&#22836;&#37096;&#30340;&#36755;&#20986;&#32452;&#21512;&#36215;&#26469;&#12290;&#23398;&#20064;&#22120;&#21644;&#25511;&#21046;&#22120;&#20998;&#21035;&#36890;&#36807;&#25552;&#20986;&#30340;3&#38454;&#27573;&#35757;&#32451;&#26041;&#26696;&#36827;&#34892;&#20248;&#21270;&#12290;&#36890;&#36807;&#21033;&#29992;&#25552;&#20379;&#30340;&#21355;&#26143;&#22270;&#20687;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#24314;&#27169;&#22797;&#26434;&#30340;&#38477;&#38632;&#27169;&#24335;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#39640;&#38477;&#27700;&#20107;&#20214;&#12290;&#23427;&#22312;Weather4Cast 2023&#31454;&#36187;&#30340;&#26680;&#24515;&#27979;&#35797;&#21644;&#21363;&#26102;&#39044;&#27979;&#25490;&#34892;&#27036;&#19978;&#21462;&#24471;&#20102;&#31532;&#19968;&#21517;&#12290;&#26377;&#20851;&#35814;&#32454;&#23454;&#29616;&#65292;&#35831;&#21442;&#32771;&#25105;&#20204;&#30340;GitH
&lt;/p&gt;
&lt;p&gt;
Precipitation prediction plays a crucial role in modern agriculture and industry. However, it poses significant challenges due to the diverse patterns and dynamics in time and space, as well as the scarcity of high precipitation events.  To address this challenge, we propose an ensemble learning framework that leverages multiple learners to capture the diverse patterns of precipitation distribution. Specifically, the framework consists of a precipitation predictor with multiple lightweight heads (learners) and a controller that combines the outputs from these heads. The learners and the controller are separately optimized with a proposed 3-stage training scheme.  By utilizing provided satellite images, the proposed approach can effectively model the intricate rainfall patterns, especially for high precipitation events. It achieved 1st place on the core test as well as the nowcasting leaderboards of the Weather4Cast 2023 competition. For detailed implementation, please refer to our GitH
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Apollo&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20302;&#23618;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#39640;&#23618;&#21151;&#33021;&#65292;&#20026;&#28176;&#36827;&#24335;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#20102;&#35838;&#31243;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#21152;&#36895;&#27604;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.09192</link><description>&lt;p&gt;
&#20026;&#28176;&#36827;&#24335;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20934;&#22791;&#35838;&#31243;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Preparing Lessons for Progressive Training on Language Models. (arXiv:2401.09192v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09192
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Apollo&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20302;&#23618;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#39640;&#23618;&#21151;&#33021;&#65292;&#20026;&#28176;&#36827;&#24335;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#20102;&#35838;&#31243;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#21152;&#36895;&#27604;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#36805;&#36895;&#21457;&#23637;&#24102;&#26469;&#20102;&#36164;&#28304;&#28040;&#32791;&#21644;&#28201;&#23460;&#27668;&#20307;&#25490;&#25918;&#30340;&#22686;&#21152;&#65292;&#36825;&#26159;&#30001;&#20110;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#38271;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#23567;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#26032;&#30340;&#27169;&#22411;&#32467;&#26500;&#21487;&#33021;&#19981;&#36866;&#29992;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#21487;&#33021;&#24456;&#24930;&#65292;&#24182;&#19988;&#28176;&#36827;&#22534;&#21472;&#23618;&#24448;&#24448;&#26080;&#27861;&#23454;&#29616;&#26174;&#33879;&#30340;&#21152;&#36895;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Apollo&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#22312;&#20302;&#23618;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#39640;&#23618;&#21151;&#33021;&#26469;&#20934;&#22791;&#33192;&#32960;&#25805;&#20316;&#30340;&#35838;&#31243;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20302;&#20540;&#20248;&#20808;&#37319;&#26679;(LVPS)&#26469;&#35757;&#32451;&#19981;&#21516;&#28145;&#24230;&#65292;&#24182;&#24341;&#20837;&#26435;&#37325;&#20849;&#20139;&#20197;&#20419;&#36827;&#39640;&#25928;&#25193;&#23637;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#25554;&#20540;&#26041;&#27861;&#26469;&#23454;&#29616;&#31283;&#23450;&#30340;&#27169;&#22411;&#28145;&#24230;&#25193;&#23637;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;Apollo&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#21152;&#36895;&#27604;&#29575;&#65292;&#29978;&#33267;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
The rapid progress of Transformers in artificial intelligence has come at the cost of increased resource consumption and greenhouse gas emissions due to growing model sizes. Prior work suggests using pretrained small models to improve training efficiency, but this approach may not be suitable for new model structures. On the other hand, training from scratch can be slow, and progressively stacking layers often fails to achieve significant acceleration. To address these challenges, we propose a novel method called Apollo, which prep\textbf{a}res lessons for ex\textbf{p}anding \textbf{o}perations by \textbf{l}earning high-\textbf{l}ayer functi\textbf{o}nality during training of low layers. Our approach involves low-value-prioritized sampling (LVPS) to train different depths and weight sharing to facilitate efficient expansion. We also introduce an interpolation method for stable model depth extension. Experiments demonstrate that Apollo achieves state-of-the-art acceleration ratios, even
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20132;&#36890;&#25317;&#22581;&#39044;&#27979;&#27169;&#22411;&#65292;&#20351;&#29992;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#20197;&#21450;&#22810;&#37051;&#25509;&#20851;&#31995;&#27880;&#24847;&#21147;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;MA2GCN&#65289;&#26469;&#39044;&#27979;&#20132;&#36890;&#25317;&#22581;&#24773;&#20917;&#65292;&#19981;&#20381;&#36182;&#20110;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#25552;&#21462;&#28789;&#27963;&#19988;&#20934;&#30830;&#30340;&#20132;&#36890;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2401.08727</link><description>&lt;p&gt;
MA2GCN: &#20351;&#29992;&#36712;&#36857;&#25968;&#25454;&#36827;&#34892;&#20132;&#36890;&#39044;&#27979;&#30340;&#22810;&#37051;&#25509;&#20851;&#31995;&#27880;&#24847;&#21147;&#22270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
MA2GCN: Multi Adjacency relationship Attention Graph Convolutional Networks for Traffic Prediction using Trajectory data. (arXiv:2401.08727v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08727
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20132;&#36890;&#25317;&#22581;&#39044;&#27979;&#27169;&#22411;&#65292;&#20351;&#29992;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#20197;&#21450;&#22810;&#37051;&#25509;&#20851;&#31995;&#27880;&#24847;&#21147;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;MA2GCN&#65289;&#26469;&#39044;&#27979;&#20132;&#36890;&#25317;&#22581;&#24773;&#20917;&#65292;&#19981;&#20381;&#36182;&#20110;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#25552;&#21462;&#28789;&#27963;&#19988;&#20934;&#30830;&#30340;&#20132;&#36890;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#25317;&#22581;&#38382;&#39064;&#19981;&#20165;&#23548;&#33268;&#24040;&#22823;&#30340;&#32463;&#27982;&#25439;&#22833;&#65292;&#32780;&#19988;&#20005;&#37325;&#21361;&#23475;&#22478;&#24066;&#29615;&#22659;&#12290;&#39044;&#27979;&#20132;&#36890;&#25317;&#22581;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24847;&#20041;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#26159;&#22522;&#20110;&#19981;&#21516;&#36335;&#27573;&#19978;&#30340;&#20256;&#24863;&#22120;&#30340;&#21382;&#21490;&#25968;&#25454;&#26469;&#39044;&#27979;&#26410;&#26469;&#30340;&#20132;&#36890;&#27969;&#37327;&#21644;&#36895;&#24230;&#65292;&#20998;&#26512;&#26576;&#20010;&#36947;&#36335;&#27573;&#30340;&#20132;&#36890;&#25317;&#22581;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20256;&#24863;&#22120;&#30340;&#22266;&#23450;&#20301;&#32622;&#65292;&#24456;&#38590;&#25366;&#25496;&#26032;&#30340;&#20449;&#24687;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#26356;&#21152;&#28789;&#27963;&#65292;&#21487;&#20197;&#26681;&#25454;&#38656;&#35201;&#25552;&#21462;&#20132;&#36890;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20132;&#36890;&#25317;&#22581;&#39044;&#27979;&#27169;&#22411;&#8212;&#8212;&#22810;&#37051;&#25509;&#20851;&#31995;&#27880;&#24847;&#21147;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;MA2GCN&#65289;&#12290;&#35813;&#27169;&#22411;&#23558;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#36716;&#21270;&#20026;&#32593;&#26684;&#24418;&#24335;&#30340;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#24182;&#22522;&#20110;&#19981;&#21516;&#32593;&#26684;&#20043;&#38388;&#30340;&#27969;&#21160;&#24615;&#25552;&#20986;&#20102;&#36710;&#36742;&#36827;&#20986;&#30697;&#38453;&#12290;&#21516;&#26102;&#65292;&#20026;&#20102;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;
&lt;/p&gt;
&lt;p&gt;
The problem of traffic congestion not only causes a large amount of economic losses, but also seriously endangers the urban environment. Predicting traffic congestion has important practical significance. So far, most studies have been based on historical data from sensors placed on different roads to predict future traffic flow and speed, to analyze the traffic congestion conditions of a certain road segment. However, due to the fixed position of sensors, it is difficult to mine new information. On the other hand, vehicle trajectory data is more flexible and can extract traffic information as needed. Therefore, we proposed a new traffic congestion prediction model - Multi Adjacency relationship Attention Graph Convolutional Networks(MA2GCN). This model transformed vehicle trajectory data into graph structured data in grid form, and proposed a vehicle entry and exit matrix based on the mobility between different grids. At the same time, in order to improve the performance of the model,
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#35299;&#37322;&#26159;&#21542;&#21487;&#38752;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;AI&#23433;&#20840;&#32771;&#34385;&#22240;&#32032;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#33258;&#27965;&#24615;&#26816;&#27979;&#20316;&#20026;&#35780;&#20272;&#20854;&#21487;&#38752;&#24615;&#21644;&#35299;&#37322;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.07927</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#35299;&#37322;&#26159;&#21542;&#21487;&#38752;?
&lt;/p&gt;
&lt;p&gt;
Are self-explanations from Large Language Models faithful?. (arXiv:2401.07927v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07927
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#35299;&#37322;&#26159;&#21542;&#21487;&#38752;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;AI&#23433;&#20840;&#32771;&#34385;&#22240;&#32032;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#33258;&#27965;&#24615;&#26816;&#27979;&#20316;&#20026;&#35780;&#20272;&#20854;&#21487;&#38752;&#24615;&#21644;&#35299;&#37322;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#36807;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#29978;&#33267;&#33021;&#22815;&#25552;&#20379;&#20854;&#34892;&#20026;&#30340;&#35299;&#37322;&#12290;&#30001;&#20110;&#36825;&#20123;&#27169;&#22411;&#23545;&#20844;&#20247;&#26159;&#30452;&#25509;&#21487;&#35775;&#38382;&#30340;&#65292;&#22240;&#27492;&#23384;&#22312;&#36825;&#26679;&#30340;&#39118;&#38505;&#65292;&#21363;&#20196;&#20154;&#20449;&#26381;&#20294;&#38169;&#35823;&#30340;&#35299;&#37322;&#21487;&#33021;&#23548;&#33268;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26080;&#25903;&#25745;&#30340;&#33258;&#20449;&#12290;&#22240;&#27492;&#65292;&#35299;&#37322;&#33021;&#21147;&#21644;&#21487;&#38752;&#24615;&#26159;AI&#23433;&#20840;&#30340;&#37325;&#35201;&#32771;&#34385;&#22240;&#32032;&#12290;&#35780;&#20272;&#33258;&#25105;&#35299;&#37322;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#20154;&#31867;&#26469;&#35828;&#36807;&#20110;&#22797;&#26434;&#65292;&#26080;&#27861;&#27880;&#37322;&#20160;&#20040;&#26159;&#27491;&#30830;&#30340;&#35299;&#37322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#33258;&#27965;&#24615;&#26816;&#27979;&#20316;&#20026;&#21487;&#38752;&#24615;&#30340;&#34913;&#37327;&#25351;&#26631;&#12290;&#20363;&#22914;&#65292;&#22914;&#26524;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35828;&#26576;&#32452;&#35789;&#23545;&#20110;&#20570;&#20986;&#39044;&#27979;&#24456;&#37325;&#35201;&#65292;&#37027;&#20040;&#22312;&#27809;&#26377;&#36825;&#20123;&#35789;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#24212;&#35813;&#26080;&#27861;&#20570;&#20986;&#30456;&#21516;&#30340;&#39044;&#27979;&#12290;&#34429;&#28982;&#33258;&#27965;&#24615;&#26816;&#27979;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#21487;&#38752;&#24615;&#26041;&#27861;&#65292;&#20294;&#20043;&#21069;&#23578;&#26410;&#24212;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#35299;&#37322;&#20013;&#12290;&#25105;&#20204;&#23558;&#33258;&#27965;&#24615;&#26816;&#27979;&#24212;&#29992;&#20110;...
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned large language models (LLMs) excel at many tasks, and will even provide explanations for their behavior. Since these models are directly accessible to the public, there is a risk that convincing and wrong explanations can lead to unsupported confidence in LLMs. Therefore, interpretability-faithfulness of self-explanations is an important consideration for AI Safety. Assessing the interpretability-faithfulness of these explanations, termed self-explanations, is challenging as the models are too complex for humans to annotate what is a correct explanation. To address this, we propose employing self-consistency checks as a measure of faithfulness. For example, if an LLM says a set of words is important for making a prediction, then it should not be able to make the same prediction without these words. While self-consistency checks are a common approach to faithfulness, they have not previously been applied to LLM's self-explanations. We apply self-consistency checks to t
&lt;/p&gt;</description></item><item><title>TAROT&#26159;&#19968;&#20010;&#23618;&#27425;&#21270;&#30340;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#21322;&#32467;&#26500;&#21270;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#32467;&#21512;&#22810;&#31890;&#24230;&#30340;&#20219;&#21153;&#26469;&#25552;&#21319;&#20154;-&#23703;&#20301;&#21305;&#37197;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.07525</link><description>&lt;p&gt;
TAROT&#65306;&#19968;&#31181;&#22312;&#21322;&#32467;&#26500;&#21270;&#25968;&#25454;&#19978;&#36827;&#34892;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#30340;&#23618;&#27425;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#20154;-&#23703;&#20301;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
TAROT: A Hierarchical Framework with Multitask Co-Pretraining on Semi-Structured Data towards Effective Person-Job Fit. (arXiv:2401.07525v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07525
&lt;/p&gt;
&lt;p&gt;
TAROT&#26159;&#19968;&#20010;&#23618;&#27425;&#21270;&#30340;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#21322;&#32467;&#26500;&#21270;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#32467;&#21512;&#22810;&#31890;&#24230;&#30340;&#20219;&#21153;&#26469;&#25552;&#21319;&#20154;-&#23703;&#20301;&#21305;&#37197;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;-&#23703;&#20301;&#21305;&#37197;&#26159;&#22312;&#32447;&#25307;&#32856;&#24179;&#21488;&#20013;&#30340;&#37325;&#35201;&#37096;&#20998;&#65292;&#21487;&#20197;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#24212;&#29992;&#65292;&#22914;&#32844;&#20301;&#25628;&#32034;&#21644;&#20505;&#36873;&#20154;&#25512;&#33616;&#12290;&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#21033;&#29992;&#29992;&#25143;&#31616;&#20171;&#21644;&#32844;&#20301;&#25551;&#36848;&#20013;&#30340;&#20016;&#23500;&#25991;&#26412;&#20449;&#24687;&#20197;&#21450;&#29992;&#25143;&#34892;&#20026;&#29305;&#24449;&#21644;&#32844;&#20301;&#20803;&#25968;&#25454;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#19968;&#33324;&#30340;&#38754;&#21521;&#39046;&#22495;&#30340;&#35774;&#35745;&#38590;&#20197;&#25429;&#25417;&#29992;&#25143;&#31616;&#20171;&#21644;&#32844;&#20301;&#25551;&#36848;&#20013;&#30340;&#29420;&#29305;&#32467;&#26500;&#20449;&#24687;&#65292;&#23548;&#33268;&#28508;&#22312;&#35821;&#20041;&#30456;&#20851;&#24615;&#30340;&#20007;&#22833;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;TAROT&#65292;&#19968;&#31181;&#23618;&#27425;&#21270;&#30340;&#22810;&#20219;&#21153;&#20849;&#21516;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#32467;&#26500;&#21644;&#35821;&#20041;&#20449;&#24687;&#36827;&#34892;&#20449;&#24687;&#24615;&#25991;&#26412;&#23884;&#20837;&#12290;TAROT&#38024;&#23545;&#31616;&#20171;&#21644;&#32844;&#20301;&#20013;&#30340;&#21322;&#32467;&#26500;&#21270;&#25991;&#26412;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#22810;&#39063;&#31890;&#24230;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#26469;&#32422;&#26463;&#27599;&#20010;&#23618;&#27425;&#19978;&#33719;&#21462;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#22312;&#30495;&#23454;&#30340;LinkedIn&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Person-job fit is an essential part of online recruitment platforms in serving various downstream applications like Job Search and Candidate Recommendation. Recently, pretrained large language models have further enhanced the effectiveness by leveraging richer textual information in user profiles and job descriptions apart from user behavior features and job metadata. However, the general domain-oriented design struggles to capture the unique structural information within user profiles and job descriptions, leading to a loss of latent semantic correlations. We propose TAROT, a hierarchical multitask co-pretraining framework, to better utilize structural and semantic information for informative text embeddings. TAROT targets semi-structured text in profiles and jobs, and it is co-pretained with multi-grained pretraining tasks to constrain the acquired semantic information at each level. Experiments on a real-world LinkedIn dataset show significant performance improvements, proving its e
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#29992;&#20110;&#29983;&#29289;&#23398;&#21644;&#21307;&#23398;&#30340;ChatGPT&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22810;&#27169;&#24577;&#33539;&#24335;&#65292;&#21152;&#36895;&#20102;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#30340;&#36827;&#23637;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#21307;&#23398;&#29615;&#22659;&#20013;&#30340;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#12289;&#26080;&#26631;&#31614;&#25968;&#25454;&#20998;&#26512;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2401.07510</link><description>&lt;p&gt;
&#24320;&#21457;&#29992;&#20110;&#29983;&#29289;&#23398;&#21644;&#21307;&#23398;&#30340;ChatGPT&#65306;&#29983;&#29289;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#30340;&#23436;&#25972;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Developing ChatGPT for Biology and Medicine: A Complete Review of Biomedical Question Answering. (arXiv:2401.07510v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07510
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#29992;&#20110;&#29983;&#29289;&#23398;&#21644;&#21307;&#23398;&#30340;ChatGPT&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22810;&#27169;&#24577;&#33539;&#24335;&#65292;&#21152;&#36895;&#20102;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#30340;&#36827;&#23637;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#21307;&#23398;&#29615;&#22659;&#20013;&#30340;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#12289;&#26080;&#26631;&#31614;&#25968;&#25454;&#20998;&#26512;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#22810;&#27169;&#24577;&#33539;&#24335;&#65292;&#36890;&#36807;&#22686;&#21152;&#21307;&#23398;&#39046;&#22495;&#25968;&#25454;&#30340;&#34701;&#20837;&#65292;&#25506;&#32034;&#20102;&#22312;&#25552;&#20379;&#21307;&#23398;&#35786;&#26029;&#12289;&#27835;&#30103;&#24314;&#35758;&#21644;&#20854;&#20182;&#21307;&#30103;&#25903;&#25345;&#26041;&#38754;&#30340;&#38382;&#31572;&#65288;QA&#65289;&#30340;&#25112;&#30053;&#34013;&#22270;&#12290;&#36890;&#36807;&#23558;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#20854;&#20182;&#27169;&#24577;&#20174;&#36890;&#29992;&#39046;&#22495;&#36716;&#21521;&#21307;&#23398;&#39046;&#22495;&#65292;&#36825;&#20123;&#25216;&#26415;&#21152;&#24555;&#20102;&#21307;&#23398;&#39046;&#22495;&#38382;&#39064;&#22238;&#31572;&#65288;MDQA&#65289;&#30340;&#36827;&#23637;&#12290;&#23427;&#20204;&#24357;&#21512;&#20102;&#20154;&#31867;&#33258;&#28982;&#35821;&#35328;&#21644;&#22797;&#26434;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#25110;&#19987;&#23478;&#25163;&#21160;&#27880;&#37322;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#22788;&#29702;&#20102;&#21307;&#23398;&#29615;&#22659;&#20013;&#30340;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#12289;&#19981;&#24179;&#34913;&#29978;&#33267;&#26080;&#26631;&#31614;&#25968;&#25454;&#20998;&#26512;&#22330;&#26223;&#12290;&#25105;&#20204;&#37325;&#28857;&#30740;&#31350;&#30340;&#26159;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#27169;&#24577;&#33539;&#24335;&#36827;&#34892;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#65292;&#26088;&#22312;&#25351;&#23548;&#30740;&#31350;&#30028;&#26681;&#25454;&#20854;&#29305;&#23450;&#30340;&#21307;&#23398;&#30740;&#31350;&#38656;&#27714;&#36873;&#25321;&#21512;&#36866;&#30340;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT explores a strategic blueprint of question answering (QA) in delivering medical diagnosis, treatment recommendations, and other healthcare support. This is achieved through the increasing incorporation of medical domain data via natural language processing (NLP) and multimodal paradigms. By transitioning the distribution of text, images, videos, and other modalities from the general domain to the medical domain, these techniques have expedited the progress of medical domain question answering (MDQA). They bridge the gap between human natural language and sophisticated medical domain knowledge or expert manual annotations, handling large-scale, diverse, unbalanced, or even unlabeled data analysis scenarios in medical contexts. Central to our focus is the utilizing of language models and multimodal paradigms for medical question answering, aiming to guide the research community in selecting appropriate mechanisms for their specific medical research requirements. Specialized tasks
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HieraFashDiff&#30340;&#26032;&#22411;&#26102;&#23578;&#35774;&#35745;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#22810;&#32423;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#20102;&#20174;&#39640;&#32423;&#35774;&#35745;&#27010;&#24565;&#21040;&#20302;&#32423;&#26381;&#35013;&#23646;&#24615;&#30340;&#20998;&#23618;&#35774;&#35745;&#21644;&#32534;&#36753;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#22312;&#26102;&#23578;&#35774;&#35745;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.07450</link><description>&lt;p&gt;
&#24102;&#26377;&#22810;&#32423;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#23618;&#26102;&#23578;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Fashion Design with Multi-stage Diffusion Models. (arXiv:2401.07450v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HieraFashDiff&#30340;&#26032;&#22411;&#26102;&#23578;&#35774;&#35745;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#22810;&#32423;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#20102;&#20174;&#39640;&#32423;&#35774;&#35745;&#27010;&#24565;&#21040;&#20302;&#32423;&#26381;&#35013;&#23646;&#24615;&#30340;&#20998;&#23618;&#35774;&#35745;&#21644;&#32534;&#36753;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#22312;&#26102;&#23578;&#35774;&#35745;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#27169;&#24577;&#26102;&#23578;&#21512;&#25104;&#21644;&#32534;&#36753;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#21644;&#23616;&#37096;&#20462;&#25913;&#35774;&#35745;&#33609;&#22270;&#65292;&#20026;&#26102;&#23578;&#35774;&#35745;&#24072;&#25552;&#20379;&#26234;&#33021;&#25903;&#25345;&#12290;&#23613;&#31649;&#24403;&#21069;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#21487;&#38752;&#30340;&#31283;&#23450;&#24615;&#21644;&#21487;&#25511;&#24615;&#65292;&#20294;&#22312;&#20174;&#25277;&#35937;&#30340;&#35774;&#35745;&#20803;&#32032;&#20013;&#29983;&#25104;&#26102;&#23578;&#35774;&#35745;&#21644;&#31934;&#32454;&#32534;&#36753;&#26041;&#38754;&#20173;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#39640;&#32423;&#35774;&#35745;&#27010;&#24565;&#65292;&#20363;&#22914;&#21150;&#20844;&#23460;&#12289;&#21830;&#21153;&#21644;&#27966;&#23545;&#65292;&#24418;&#25104;&#20102;&#25277;&#35937;&#30340;&#24863;&#23448;&#34920;&#36798;&#26041;&#24335;&#65292;&#32780;&#34966;&#38271;&#12289;&#39046;&#22411;&#21644;&#35044;&#38271;&#31561;&#21487;&#34913;&#37327;&#30340;&#26041;&#38754;&#34987;&#35270;&#20026;&#26381;&#35013;&#30340;&#20302;&#32423;&#23646;&#24615;&#12290;&#20351;&#29992;&#20887;&#38271;&#30340;&#25991;&#23383;&#25551;&#36848;&#26469;&#25511;&#21046;&#21644;&#32534;&#36753;&#26102;&#23578;&#22270;&#20687;&#23384;&#22312;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HieraFashDiff&#30340;&#26032;&#22411;&#26102;&#23578;&#35774;&#35745;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#20849;&#20139;&#30340;&#22810;&#32423;&#25193;&#25955;&#27169;&#22411;&#65292;&#23558;&#39640;&#32423;&#35774;&#35745;&#27010;&#24565;&#21644;&#20302;&#32423;&#26381;&#35013;&#23646;&#24615;&#34701;&#20837;&#21040;&#20998;&#23618;&#32467;&#26500;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#36755;&#20837;&#25991;&#26412;&#20998;&#20026;&#19981;&#21516;&#30340;&#23618;&#27425;&#65292;&#24182;&#23558;&#20854;&#36755;&#20837;&#21040;&#22810;&#32423;&#25193;&#25955;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-modal fashion synthesis and editing offer intelligent support to fashion designers by enabling the automatic generation and local modification of design drafts.While current diffusion models demonstrate commendable stability and controllability in image synthesis,they still face significant challenges in generating fashion design from abstract design elements and fine-grained editing.Abstract sensory expressions, \eg office, business, and party, form the high-level design concepts, while measurable aspects like sleeve length, collar type, and pant length are considered the low-level attributes of clothing.Controlling and editing fashion images using lengthy text descriptions poses a difficulty.In this paper, we propose HieraFashDiff,a novel fashion design method using the shared multi-stage diffusion model encompassing high-level design concepts and low-level clothing attributes in a hierarchical structure.Specifically, we categorized the input text into different levels and fed 
&lt;/p&gt;</description></item><item><title>E^2-LLM&#26159;&#19968;&#31181;&#39640;&#25928;&#21644;&#26497;&#38271;&#25193;&#23637;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#38656;&#19968;&#27425;&#35757;&#32451;&#36807;&#31243;&#21644;&#19981;&#25910;&#38598;&#38271;&#19978;&#19979;&#25991;&#25968;&#25454;&#30340;&#26041;&#24335;&#65292;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#20943;&#23569;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#22522;&#20110;RoPE&#20301;&#32622;&#23884;&#20837;&#65292;E^2-LLM&#21482;&#38656;&#35201;&#36739;&#30701;&#30340;&#35757;&#32451;&#25968;&#25454;&#38271;&#24230;&#65292;&#25903;&#25345;&#19981;&#21516;&#30340;&#35780;&#20272;&#19978;&#19979;&#25991;&#31383;&#21475;&#12290;</title><link>http://arxiv.org/abs/2401.06951</link><description>&lt;p&gt;
E^2-LLM: &#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#21644;&#26497;&#38271;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
E^2-LLM: Efficient and Extreme Length Extension of Large Language Models. (arXiv:2401.06951v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06951
&lt;/p&gt;
&lt;p&gt;
E^2-LLM&#26159;&#19968;&#31181;&#39640;&#25928;&#21644;&#26497;&#38271;&#25193;&#23637;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#38656;&#19968;&#27425;&#35757;&#32451;&#36807;&#31243;&#21644;&#19981;&#25910;&#38598;&#38271;&#19978;&#19979;&#25991;&#25968;&#25454;&#30340;&#26041;&#24335;&#65292;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#20943;&#23569;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#22522;&#20110;RoPE&#20301;&#32622;&#23884;&#20837;&#65292;E^2-LLM&#21482;&#38656;&#35201;&#36739;&#30701;&#30340;&#35757;&#32451;&#25968;&#25454;&#38271;&#24230;&#65292;&#25903;&#25345;&#19981;&#21516;&#30340;&#35780;&#20272;&#19978;&#19979;&#25991;&#31383;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#65292;&#20351;&#29992;&#38271;&#19978;&#19979;&#25991;&#22823;&#23567;&#35757;&#32451;LLM&#20250;&#28040;&#32791;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;GPU&#36164;&#28304;&#65292;&#38656;&#35201;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#12290;&#29616;&#26377;&#30340;&#38271;&#19978;&#19979;&#25991;&#25193;&#23637;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#36807;&#31243;&#26469;&#25903;&#25345;&#30456;&#24212;&#30340;&#38271;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#38656;&#35201;&#38271;&#19978;&#19979;&#25991;&#35757;&#32451;&#25968;&#25454;&#65288;&#20363;&#22914;32k&#65289;&#65292;&#24182;&#19988;&#20551;&#23450;&#26377;&#39640;&#26114;&#30340;GPU&#35757;&#32451;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;E^2-LLM&#30340;&#39640;&#25928;&#21644;&#26497;&#38271;&#25193;&#23637;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#19968;&#27425;&#35757;&#32451;&#36807;&#31243;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#20063;&#19981;&#38656;&#35201;&#25910;&#38598;&#38271;&#19978;&#19979;&#25991;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;E^2-LLM&#30340;&#35757;&#32451;&#25968;&#25454;&#21482;&#38656;&#35201;&#24456;&#30701;&#30340;&#38271;&#24230;&#65288;&#20363;&#22914;4k&#65289;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#35843;&#25972;&#25104;&#26412;&#12290;&#20854;&#27425;&#65292;&#22312;&#30701;&#35757;&#32451;&#19978;&#19979;&#25991;&#31383;&#21475;&#19978;&#30340;&#35757;&#32451;&#36807;&#31243;&#21482;&#25191;&#34892;&#19968;&#27425;&#65292;&#25105;&#20204;&#21487;&#20197;&#25903;&#25345;&#19981;&#21516;&#30340;&#35780;&#20272;&#19978;&#19979;&#25991;&#31383;&#21475;&#12290;&#31532;&#19977;&#65292;&#22312;E^2-LLM&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;RoPE&#20301;&#32622;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources. Existing long-context extension methods usually need additional training procedures to support corresponding long-context windows, where the long-context training data (e.g., 32k) is needed, and high GPU training costs are assumed. To address the aforementioned issues, we propose an Efficient and Extreme length extension method for Large Language Models, called E 2 -LLM, with only one training procedure and dramatically reduced computation cost, which also removes the need to collect long-context data. Concretely, first, the training data of our E 2 -LLM only requires a short length (e.g., 4k), which reduces the tuning cost greatly. Second, the training procedure on the short training context window is performed only once time, and we can support different evaluation context windows at inference. Third, in E 2 - LLM, based on RoPE position embeddings, we 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35843;&#26597;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#21253;&#25324;&#35780;&#20272;&#21327;&#35758;&#12289;&#27169;&#22411;&#21069;&#27839;&#21644;&#25512;&#29702;&#23494;&#38598;&#22411;&#20219;&#21153;&#30340;&#24212;&#29992;&#65292;&#26088;&#22312;&#23454;&#29616;&#24378;&#20154;&#24037;&#26234;&#33021;&#65288;Strong AI&#65289;&#25110;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#25277;&#35937;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.06805</link><description>&lt;p&gt;
&#25506;&#32034;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#65306;&#20851;&#20110;&#22810;&#27169;&#24577;&#25512;&#29702;&#26032;&#36235;&#21183;&#30340;&#32508;&#21512;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning. (arXiv:2401.06805v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06805
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35843;&#26597;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#21253;&#25324;&#35780;&#20272;&#21327;&#35758;&#12289;&#27169;&#22411;&#21069;&#27839;&#21644;&#25512;&#29702;&#23494;&#38598;&#22411;&#20219;&#21153;&#30340;&#24212;&#29992;&#65292;&#26088;&#22312;&#23454;&#29616;&#24378;&#20154;&#24037;&#26234;&#33021;&#65288;Strong AI&#65289;&#25110;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#25277;&#35937;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#20154;&#24037;&#26234;&#33021;&#65288;Strong AI&#65289;&#25110;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#20855;&#22791;&#25277;&#35937;&#25512;&#29702;&#33021;&#21147;&#26159;&#19979;&#19968;&#20195;&#20154;&#24037;&#26234;&#33021;&#30340;&#30446;&#26631;&#12290;&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20197;&#21450;&#26032;&#20852;&#30340;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#39046;&#22495;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36328;&#30028;&#24615;&#33021;&#21644;&#24212;&#29992;&#28508;&#21147;&#12290;&#29305;&#21035;&#26159;&#65292;&#19981;&#21516;&#30340;MLLMs&#36890;&#36807;&#19981;&#21516;&#30340;&#27169;&#22411;&#26550;&#26500;&#12289;&#35757;&#32451;&#25968;&#25454;&#21644;&#35757;&#32451;&#38454;&#27573;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;MLLM&#22522;&#20934;&#35780;&#20272;&#12290;&#36825;&#20123;&#30740;&#31350;&#22312;&#19981;&#21516;&#31243;&#24230;&#19978;&#25581;&#31034;&#20102;MLLMs&#24403;&#21069;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;MLLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#36824;&#27809;&#26377;&#24471;&#21040;&#31995;&#32479;&#30340;&#35843;&#26597;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#22238;&#39038;&#20102;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#25512;&#29702;&#35780;&#20272;&#21327;&#35758;&#65292;&#23545;MLLMs&#30340;&#21069;&#27839;&#36827;&#34892;&#20998;&#31867;&#21644;&#25581;&#31034;&#65292;&#20171;&#32461;&#20102;MLLMs&#22312;&#25512;&#29702;&#23494;&#38598;&#22411;&#20219;&#21153;&#19978;&#30340;&#26368;&#26032;&#36235;&#21183;&#65292;&#24182;&#26368;&#32456;&#35752;&#35770;&#20102;&#24403;&#21069;&#30340;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Strong Artificial Intelligence (Strong AI) or Artificial General Intelligence (AGI) with abstract reasoning ability is the goal of next-generation AI. Recent advancements in Large Language Models (LLMs), along with the emerging field of Multimodal Large Language Models (MLLMs), have demonstrated impressive capabilities across a wide range of multimodal tasks and applications. Particularly, various MLLMs, each with distinct model architectures, training data, and training stages, have been evaluated across a broad range of MLLM benchmarks. These studies have, to varying degrees, revealed different aspects of the current capabilities of MLLMs. However, the reasoning abilities of MLLMs have not been systematically investigated. In this survey, we comprehensively review the existing evaluation protocols of multimodal reasoning, categorize and illustrate the frontiers of MLLMs, introduce recent trends in applications of MLLMs on reasoning-intensive tasks, and finally discuss current practic
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35757;&#32451;&#24182;&#20445;&#25345;&#25345;&#20037;&#30340;&#27450;&#39575;&#24615;&#34892;&#20026;&#65292;&#36825;&#31181;&#34892;&#20026;&#26080;&#27861;&#34987;&#24403;&#21069;&#30340;&#23433;&#20840;&#35757;&#32451;&#25216;&#26415;&#31227;&#38500;&#12290;</title><link>http://arxiv.org/abs/2401.05566</link><description>&lt;p&gt;
&#21351;&#24213;&#29305;&#24037;&#65306;&#35757;&#32451;&#39575;&#20154;&#30340;LLM&#20197;&#36890;&#36807;&#23433;&#20840;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training. (arXiv:2401.05566v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05566
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35757;&#32451;&#24182;&#20445;&#25345;&#25345;&#20037;&#30340;&#27450;&#39575;&#24615;&#34892;&#20026;&#65292;&#36825;&#31181;&#34892;&#20026;&#26080;&#27861;&#34987;&#24403;&#21069;&#30340;&#23433;&#20840;&#35757;&#32451;&#25216;&#26415;&#31227;&#38500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26377;&#33021;&#21147;&#36827;&#34892;&#25112;&#30053;&#24615;&#30340;&#27450;&#39575;&#34892;&#20026;&#65306;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#26377;&#30410;&#30340;&#34892;&#20026;&#65292;&#20294;&#22312;&#26377;&#26426;&#20250;&#30340;&#26102;&#20505;&#21364;&#34920;&#29616;&#20986;&#25130;&#28982;&#19981;&#21516;&#30340;&#34892;&#20026;&#20197;&#36861;&#27714;&#20854;&#20182;&#30446;&#26631;&#12290;&#22914;&#26524;&#19968;&#20010;AI&#31995;&#32479;&#23398;&#20250;&#20102;&#36825;&#26679;&#30340;&#27450;&#39575;&#31574;&#30053;&#65292;&#26159;&#21542;&#33021;&#22815;&#36890;&#36807;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#23433;&#20840;&#35757;&#32451;&#25216;&#26415;&#26816;&#27979;&#24182;&#31227;&#38500;&#23427;&#65311;&#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#27450;&#39575;&#34892;&#20026;&#30340;&#27010;&#24565;&#39564;&#35777;&#26679;&#20363;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#25552;&#31034;&#35821;&#21477;&#20013;&#23558;&#24180;&#20221;&#35774;&#20026;2023&#26102;&#32534;&#20889;&#23433;&#20840;&#20195;&#30721;&#65292;&#20294;&#22312;&#24180;&#20221;&#35774;&#20026;2024&#26102;&#25554;&#20837;&#26377;&#28431;&#27934;&#30340;&#20195;&#30721;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#31181;&#26263;&#38376;&#34892;&#20026;&#21487;&#20197;&#34987;&#25345;&#32493;&#20445;&#30041;&#65292;&#26080;&#27861;&#36890;&#36807;&#26631;&#20934;&#30340;&#23433;&#20840;&#35757;&#32451;&#25216;&#26415;&#65288;&#21253;&#25324;&#30417;&#30563;&#24494;&#35843;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#23545;&#25239;&#24615;&#35757;&#32451;&#65289;&#31227;&#38500;&#12290;&#26263;&#38376;&#34892;&#20026;&#22312;&#26368;&#22823;&#30340;&#27169;&#22411;&#21644;&#35757;&#32451;&#25104;&#20135;&#29983;&#24605;&#32500;&#38142;&#30340;&#27169;&#22411;&#20013;&#26368;&#20026;&#25345;&#20037;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans are capable of strategically deceptive behavior: behaving helpfully in most situations, but then behaving very differently in order to pursue alternative objectives when given the opportunity. If an AI system learned such a deceptive strategy, could we detect it and remove it using current state-of-the-art safety training techniques? To study this question, we construct proof-of-concept examples of deceptive behavior in large language models (LLMs). For example, we train models that write secure code when the prompt states that the year is 2023, but insert exploitable code when the stated year is 2024. We find that such backdoored behavior can be made persistent, so that it is not removed by standard safety training techniques, including supervised fine-tuning, reinforcement learning, and adversarial training (eliciting unsafe behavior and then training to remove it). The backdoored behavior is most persistent in the largest models and in models trained to produce chain-of-thoug
&lt;/p&gt;</description></item><item><title>MISS&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#19982;&#24494;&#35843;&#26041;&#27861;&#12290;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#25105;&#20204;&#25226;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#20316;&#20026;&#19968;&#20010;&#29983;&#25104;&#24335;&#20219;&#21153;&#22788;&#29702;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#20351;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#23545;&#40784;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#21333;&#27169;&#24577;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#36716;&#25442;&#21644;&#23383;&#24149;&#26041;&#27861;&#23454;&#29616;&#29305;&#24449;&#31354;&#38388;&#30340;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2401.05163</link><description>&lt;p&gt;
MISS&#65306;&#19968;&#31181;&#36866;&#29992;&#20110;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#19982;&#24494;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MISS: A Generative Pretraining and Finetuning Approach for Med-VQA. (arXiv:2401.05163v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05163
&lt;/p&gt;
&lt;p&gt;
MISS&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#19982;&#24494;&#35843;&#26041;&#27861;&#12290;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#25105;&#20204;&#25226;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#20316;&#20026;&#19968;&#20010;&#29983;&#25104;&#24335;&#20219;&#21153;&#22788;&#29702;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#20351;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#23545;&#40784;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#21333;&#27169;&#24577;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#36716;&#25442;&#21644;&#23383;&#24149;&#26041;&#27861;&#23454;&#29616;&#29305;&#24449;&#31354;&#38388;&#30340;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;&#20854;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#22810;&#25968;&#26041;&#27861;&#23558;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#35270;&#20026;&#19968;&#20010;&#38590;&#20197;&#36716;&#31227;&#21040;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#30340;&#31572;&#26696;&#20998;&#31867;&#20219;&#21153;&#12290;&#21478;&#22806;&#65292;&#30001;&#20110;&#21307;&#23398;&#22270;&#20687;&#30340;&#38544;&#31169;&#24615;&#21644;&#26114;&#36149;&#30340;&#27880;&#37322;&#36807;&#31243;&#65292;&#29992;&#20110;&#39044;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#21307;&#23398;&#22270;&#25991;&#23545;&#25968;&#25454;&#38598;&#20005;&#37325;&#32570;&#20047;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#20219;&#21153;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#22823;&#35268;&#27169;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#65288;MISS&#65289;&#26694;&#26550;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#23558;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#35270;&#20026;&#19968;&#39033;&#29983;&#25104;&#24335;&#20219;&#21153;&#12290;&#25105;&#20204;&#23558;&#25991;&#26412;&#32534;&#30721;&#22120;&#21644;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#32479;&#19968;&#36215;&#26469;&#65292;&#24182;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#20351;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25193;&#23637;&#21333;&#27169;&#24577;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#36716;&#25442;&#21644;&#23383;&#24149;&#26041;&#27861;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#29305;&#24449;&#31354;&#38388;&#30340;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical visual question answering (VQA) is a challenging multimodal task, where Vision-Language Pre-training (VLP) models can effectively improve the generalization performance. However, most methods in the medical field treat VQA as an answer classification task which is difficult to transfer to practical application scenarios. Additionally, due to the privacy of medical images and the expensive annotation process, large-scale medical image-text pairs datasets for pretraining are severely lacking. In this paper, we propose a large-scale MultI-task Self-Supervised learning based framework (MISS) for medical VQA tasks. Unlike existing methods, we treat medical VQA as a generative task. We unify the text encoder and multimodal encoder and align image-text features through multi-task learning. Furthermore, we propose a Transfer-and-Caption method that extends the feature space of single-modal image datasets using large language models (LLMs), enabling those traditional medical vision fiel
&lt;/p&gt;</description></item><item><title>ICMC-ASR&#25361;&#25112;&#36187;&#26159;&#20026;&#20102;&#20419;&#36827;&#39550;&#39542;&#22330;&#26223;&#19979;&#30340;&#35821;&#38899;&#22788;&#29702;&#21644;&#35782;&#21035;&#30740;&#31350;&#32780;&#20030;&#21150;&#30340;&#65292;&#21253;&#25324;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21644;&#33258;&#21160;&#35821;&#38899;&#20998;&#31163;&#21644;&#35782;&#21035;&#65288;ASDR&#65289;&#20004;&#20010;&#36187;&#36947;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#21892;&#12290;&#26368;&#32456;&#65292;USTCiflytek&#38431;&#22312;ASR&#36187;&#36947;&#19978;&#33719;&#24471;&#20102;13.16%&#30340;CER&#65292;ASDR&#36187;&#36947;&#19978;&#33719;&#24471;&#20102;21.48%&#30340;cpCER&#12290;</title><link>http://arxiv.org/abs/2401.03473</link><description>&lt;p&gt;
ICMC-ASR&#65306;ICASSP 2024&#24180;&#27773;&#36710;&#22810;&#36890;&#36947;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#25361;&#25112;&#36187;
&lt;/p&gt;
&lt;p&gt;
ICMC-ASR: The ICASSP 2024 In-Car Multi-Channel Automatic Speech Recognition Challenge. (arXiv:2401.03473v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03473
&lt;/p&gt;
&lt;p&gt;
ICMC-ASR&#25361;&#25112;&#36187;&#26159;&#20026;&#20102;&#20419;&#36827;&#39550;&#39542;&#22330;&#26223;&#19979;&#30340;&#35821;&#38899;&#22788;&#29702;&#21644;&#35782;&#21035;&#30740;&#31350;&#32780;&#20030;&#21150;&#30340;&#65292;&#21253;&#25324;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21644;&#33258;&#21160;&#35821;&#38899;&#20998;&#31163;&#21644;&#35782;&#21035;&#65288;ASDR&#65289;&#20004;&#20010;&#36187;&#36947;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#21892;&#12290;&#26368;&#32456;&#65292;USTCiflytek&#38431;&#22312;ASR&#36187;&#36947;&#19978;&#33719;&#24471;&#20102;13.16%&#30340;CER&#65292;ASDR&#36187;&#36947;&#19978;&#33719;&#24471;&#20102;21.48%&#30340;cpCER&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20419;&#36827;&#39550;&#39542;&#22330;&#26223;&#19979;&#30340;&#35821;&#38899;&#22788;&#29702;&#21644;&#35782;&#21035;&#30740;&#31350;&#65292;&#25105;&#20204;&#22522;&#20110;ISCSLP 2022&#24180;&#24230;&#20030;&#21150;&#30340;&#26234;&#33021;&#24231;&#33329;&#35821;&#38899;&#35782;&#21035;&#25361;&#25112;&#36187;&#65288;ICSRC&#65289;&#30340;&#25104;&#21151;&#32463;&#39564;&#65292;&#25512;&#20986;&#20102;ICASSP 2024&#24180;&#27773;&#36710;&#22810;&#36890;&#36947;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ICMC-ASR&#65289;&#25361;&#25112;&#36187;&#12290;&#35813;&#25361;&#25112;&#25910;&#38598;&#20102;100&#22810;&#23567;&#26102;&#30340;&#26032;&#33021;&#28304;&#27773;&#36710;&#20869;&#37096;&#22810;&#36890;&#36947;&#35821;&#38899;&#25968;&#25454;&#20197;&#21450;40&#23567;&#26102;&#30340;&#22122;&#22768;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#12290;&#35774;&#31435;&#20102;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21644;&#33258;&#21160;&#35821;&#38899;&#20998;&#31163;&#21644;&#35782;&#21035;&#65288;ASDR&#65289;&#20004;&#20010;&#36187;&#36947;&#65292;&#24182;&#20998;&#21035;&#20351;&#29992;&#23383;&#31526;&#38169;&#35823;&#29575;&#65288;CER&#65289;&#21644;&#36830;&#25509;&#26368;&#23567;&#32622;&#25442;&#23383;&#31526;&#38169;&#35823;&#29575;&#65288;cpCER&#65289;&#20316;&#20026;&#35780;&#20272;&#25351;&#26631;&#12290;&#24635;&#20307;&#19978;&#65292;ICMC-ASR&#25361;&#25112;&#36187;&#21560;&#24341;&#20102;98&#25903;&#21442;&#36187;&#38431;&#20237;&#65292;&#24182;&#22312;&#20004;&#20010;&#36187;&#36947;&#19978;&#25910;&#21040;&#20102;53&#20010;&#26377;&#25928;&#32467;&#26524;&#12290;&#26368;&#32456;&#65292;USTCiflytek&#38431;&#22312;ASR&#36187;&#36947;&#19978;&#23454;&#29616;&#20102;13.16%&#30340;CER&#65292;&#22312;ASDR&#36187;&#36947;&#19978;&#23454;&#29616;&#20102;21.48%&#30340;cpCER&#65292;&#20998;&#21035;&#30456;&#23545;&#20110;&#25105;&#20204;&#25361;&#25112;&#36187;&#20934;&#21017;&#30340;&#32477;&#23545;&#25913;&#21892;&#29575;&#20026;13.08%&#21644;51.4%&#12290;
&lt;/p&gt;
&lt;p&gt;
To promote speech processing and recognition research in driving scenarios, we build on the success of the Intelligent Cockpit Speech Recognition Challenge (ICSRC) held at ISCSLP 2022 and launch the ICASSP 2024 In-Car Multi-Channel Automatic Speech Recognition (ICMC-ASR) Challenge. This challenge collects over 100 hours of multi-channel speech data recorded inside a new energy vehicle and 40 hours of noise for data augmentation. Two tracks, including automatic speech recognition (ASR) and automatic speech diarization and recognition (ASDR) are set up, using character error rate (CER) and concatenated minimum permutation character error rate (cpCER) as evaluation metrics, respectively. Overall, the ICMC-ASR Challenge attracts 98 participating teams and receives 53 valid results in both tracks. In the end, first-place team USTCiflytek achieves a CER of 13.16% in the ASR track and a cpCER of 21.48% in the ASDR track, showing an absolute improvement of 13.08% and 51.4% compared to our chal
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#30697;&#38453;&#20998;&#26512;&#26041;&#27861;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25512;&#29702;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#36319;&#38543;&#27169;&#24335;&#12290;&#22312;&#27169;&#25311;&#25968;&#25454;&#38598;&#21644;&#22768;&#38899;&#35760;&#24405;&#25968;&#25454;&#38598;&#20013;&#65292;&#35813;&#26694;&#26550;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#65292;&#24182;&#33021;&#22815;&#26816;&#27979;&#20986;&#21152;&#23494;&#36135;&#24065;&#25968;&#25454;&#38598;&#20013;&#30340;&#36319;&#38543;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2401.02860</link><description>&lt;p&gt;
&#21464;&#21270;&#28382;&#21518;&#27169;&#24335;&#36319;&#38543;&#20851;&#31995;&#25512;&#29702;&#30340;&#26102;&#38388;&#24207;&#21015;&#30697;&#38453;&#20998;&#26512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Framework for Variable-lag Motif Following Relation Inference In Time Series using Matrix Profile analysis. (arXiv:2401.02860v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02860
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#30697;&#38453;&#20998;&#26512;&#26041;&#27861;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25512;&#29702;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#36319;&#38543;&#27169;&#24335;&#12290;&#22312;&#27169;&#25311;&#25968;&#25454;&#38598;&#21644;&#22768;&#38899;&#35760;&#24405;&#25968;&#25454;&#38598;&#20013;&#65292;&#35813;&#26694;&#26550;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#65292;&#24182;&#33021;&#22815;&#26816;&#27979;&#20986;&#21152;&#23494;&#36135;&#24065;&#25968;&#25454;&#38598;&#20013;&#30340;&#36319;&#38543;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#36947;&#35841;&#36319;&#38543;&#35841;&#20197;&#21450;&#20182;&#20204;&#36319;&#38543;&#30340;&#27169;&#24335;&#26159;&#29702;&#35299;&#38598;&#20307;&#34892;&#20026;&#65288;&#22914;&#20154;&#32676;&#65292;&#40060;&#32676;&#25110;&#32929;&#24066;&#65289;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#26102;&#38388;&#24207;&#21015;&#26159;&#29992;&#20110;&#33719;&#21462;&#36319;&#38543;&#20851;&#31995;&#27934;&#23519;&#30340;&#36164;&#28304;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#36319;&#38543;&#27169;&#24335;&#25110;&#27169;&#24335;&#22312;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#21457;&#29616;&#35299;&#20915;&#26041;&#26696;&#24182;&#19981;&#26126;&#26174;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24418;&#24335;&#21270;&#20102;&#20004;&#20010;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#30340;&#36319;&#38543;&#27169;&#24335;&#27010;&#24565;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#25512;&#26029;&#20004;&#20010;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#36319;&#38543;&#27169;&#24335;&#30340;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#20174;&#26102;&#38388;&#24207;&#21015;&#20013;&#26816;&#32034;&#27169;&#24335;&#65292;&#31216;&#20026;&#30697;&#38453;&#20998;&#26512;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#25552;&#20986;&#30340;&#26694;&#26550;&#19982;&#20960;&#20010;&#22522;&#20934;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#22312;&#27169;&#25311;&#25968;&#25454;&#38598;&#20013;&#65292;&#35813;&#26694;&#26550;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#12290;&#22312;&#22768;&#38899;&#35760;&#24405;&#25968;&#25454;&#38598;&#20013;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#22312;&#19968;&#23545;&#26102;&#38388;&#24207;&#21015;&#20013;&#26816;&#32034;&#20986;&#20004;&#20301;&#27468;&#25163;&#30456;&#20114;&#36319;&#38543;&#21809;&#27468;&#30340;&#36319;&#38543;&#27169;&#24335;&#12290;&#22312;&#21152;&#23494;&#36135;&#24065;&#25968;&#25454;&#38598;&#20013;&#65292;
&lt;/p&gt;
&lt;p&gt;
Knowing who follows whom and what patterns they are following are crucial steps to understand collective behaviors (e.g. a group of human, a school of fish, or a stock market). Time series is one of resources that can be used to get insight regarding following relations. However, the concept of following patterns or motifs and the solution to find them in time series are not obvious. In this work, we formalize a concept of following motifs between two time series and present a framework to infer following patterns between two time series. The framework utilizes one of efficient and scalable methods to retrieve motifs from time series called the Matrix Profile Method. We compare our proposed framework with several baselines. The framework performs better than baselines in the simulation datasets. In the dataset of sound recording, the framework is able to retrieve the following motifs within a pair of time series that two singers sing following each other. In the cryptocurrency dataset,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#24314;&#27169;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20165;&#21033;&#29992;&#33258;&#25105;&#20195;&#29702;&#30340;&#26412;&#22320;&#35266;&#27979;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#21462;&#20854;&#20182;&#20195;&#29702;&#30340;&#26377;&#24847;&#20041;&#31574;&#30053;&#34920;&#31034;&#65292;&#20197;&#25913;&#36827;&#33258;&#25105;&#20195;&#29702;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2401.00132</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning-based agent modeling for deep reinforcement learning. (arXiv:2401.00132v2 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#24314;&#27169;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20165;&#21033;&#29992;&#33258;&#25105;&#20195;&#29702;&#30340;&#26412;&#22320;&#35266;&#27979;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#21462;&#20854;&#20182;&#20195;&#29702;&#30340;&#26377;&#24847;&#20041;&#31574;&#30053;&#34920;&#31034;&#65292;&#20197;&#25913;&#36827;&#33258;&#25105;&#20195;&#29702;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#32463;&#24120;&#38656;&#35201;&#20195;&#29702;&#19982;&#20855;&#26377;&#19981;&#21516;&#30446;&#26631;&#12289;&#34892;&#20026;&#25110;&#31574;&#30053;&#30340;&#20854;&#20182;&#20195;&#29702;&#21512;&#20316;&#25110;&#31454;&#20105;&#12290;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#35774;&#35745;&#33258;&#36866;&#24212;&#31574;&#30053;&#26102;&#65292;&#20195;&#29702;&#24314;&#27169;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#65292;&#22240;&#20026;&#36825;&#26159;&#33258;&#25105;&#20195;&#29702;&#29702;&#35299;&#20854;&#20182;&#20195;&#29702;&#34892;&#20026;&#24182;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#31574;&#30053;&#34920;&#31034;&#30340;&#26041;&#24335;&#12290;&#36825;&#20123;&#34920;&#31034;&#21487;&#20197;&#29992;&#26469;&#22686;&#24378;&#33258;&#25105;&#20195;&#29702;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20195;&#29702;&#24314;&#27169;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#22312;&#35757;&#32451;&#25110;&#38271;&#26102;&#38388;&#35266;&#23519;&#36712;&#36857;&#30340;&#31574;&#30053;&#36866;&#24212;&#36807;&#31243;&#20013;&#21487;&#20197;&#20351;&#29992;&#26469;&#33258;&#20854;&#20182;&#20195;&#29702;&#65288;&#24314;&#27169;&#20195;&#29702;&#65289;&#30340;&#26412;&#22320;&#35266;&#27979;&#12290;&#20026;&#20102;&#28040;&#38500;&#36825;&#20123;&#38480;&#21046;&#24615;&#20551;&#35774;&#24182;&#25552;&#39640;&#20195;&#29702;&#24314;&#27169;&#24615;&#33021;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#20195;&#29702;&#24314;&#27169;&#65288;CLAM&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20165;&#20381;&#36182;&#20110;&#33258;&#25105;&#20195;&#29702;&#22312;&#35757;&#32451;&#21644;&#25191;&#34892;&#36807;&#31243;&#20013;&#30340;&#26412;&#22320;&#35266;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-agent systems often require agents to collaborate with or compete against other agents with diverse goals, behaviors, or strategies. Agent modeling is essential when designing adaptive policies for intelligent machine agents in multiagent systems, as this is the means by which the ego agent understands other agents' behavior and extracts their meaningful policy representations. These representations can be used to enhance the ego agent's adaptive policy which is trained by reinforcement learning. However, existing agent modeling approaches typically assume the availability of local observations from other agents (modeled agents) during training or a long observation trajectory for policy adaption. To remove these constrictive assumptions and improve agent modeling performance, we devised a Contrastive Learning-based Agent Modeling (CLAM) method that relies only on the local observations from the ego agent during training and execution. With these observations, CLAM is capable of 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30495;&#23454;&#26862;&#26519;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#32500;&#27491;&#20132;&#25506;&#38024;&#65292;&#25581;&#31034;&#38544;&#34255;&#30340;&#30495;&#23454;&#34920;&#31034;&#65292;&#20174;&#32780;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30495;&#23454;&#24615;&#12290;&#20316;&#32773;&#23558;&#27491;&#20132;&#32422;&#26463;&#34701;&#20837;&#25506;&#38024;&#65292;&#21019;&#24314;&#19981;&#21516;&#30340;&#27491;&#20132;&#22522;&#65292;&#36890;&#36807;&#38543;&#26426;&#31397;&#35270;&#25216;&#26415;&#65292;&#20943;&#23567;&#20102;&#27169;&#22411;&#29983;&#25104;&#21644;&#35782;&#21035;&#30495;&#23454;&#29305;&#24449;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;</title><link>http://arxiv.org/abs/2312.17484</link><description>&lt;p&gt;
&#30495;&#23454;&#26862;&#26519;&#65306;&#36890;&#36807;&#24178;&#39044;&#32780;&#26080;&#38656;&#35843;&#25972;&#65292;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#23610;&#24230;&#30495;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
Truth Forest: Toward Multi-Scale Truthfulness in Large Language Models through Intervention without Tuning. (arXiv:2312.17484v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.17484
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30495;&#23454;&#26862;&#26519;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#32500;&#27491;&#20132;&#25506;&#38024;&#65292;&#25581;&#31034;&#38544;&#34255;&#30340;&#30495;&#23454;&#34920;&#31034;&#65292;&#20174;&#32780;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30495;&#23454;&#24615;&#12290;&#20316;&#32773;&#23558;&#27491;&#20132;&#32422;&#26463;&#34701;&#20837;&#25506;&#38024;&#65292;&#21019;&#24314;&#19981;&#21516;&#30340;&#27491;&#20132;&#22522;&#65292;&#36890;&#36807;&#38543;&#26426;&#31397;&#35270;&#25216;&#26415;&#65292;&#20943;&#23567;&#20102;&#27169;&#22411;&#29983;&#25104;&#21644;&#35782;&#21035;&#30495;&#23454;&#29305;&#24449;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#29983;&#25104;&#24187;&#35273;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#30495;&#23454;&#26862;&#26519;&#65292;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#22810;&#32500;&#27491;&#20132;&#25506;&#38024;&#25581;&#31034;LLM&#20013;&#38544;&#34255;&#30340;&#30495;&#23454;&#34920;&#31034;&#26469;&#22686;&#24378;&#30495;&#23454;&#24615;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#36890;&#36807;&#23558;&#27491;&#20132;&#32422;&#26463;&#34701;&#20837;&#25506;&#38024;&#20013;&#26469;&#21019;&#24314;&#22810;&#20010;&#29992;&#20110;&#24314;&#27169;&#30495;&#23454;&#24615;&#30340;&#27491;&#20132;&#22522;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38543;&#26426;&#31397;&#35270;&#65292;&#36825;&#26159;&#19968;&#31181;&#31995;&#32479;&#25216;&#26415;&#65292;&#32771;&#34385;&#20102;&#24207;&#21015;&#20013;&#26356;&#24191;&#27867;&#30340;&#20301;&#32622;&#33539;&#22260;&#65292;&#20943;&#23567;&#20102;LLM&#20013;&#36776;&#21035;&#21644;&#29983;&#25104;&#30495;&#23454;&#29305;&#24449;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36890;&#36807;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#22312;TruthfulQA&#19978;&#23558;Llama-2-7B&#30340;&#30495;&#23454;&#24615;&#20174;40.8&#65285;&#25552;&#39640;&#21040;74.5&#65285;&#12290;&#31867;&#20284;&#22320;&#65292;&#22312;&#24494;&#35843;&#27169;&#22411;&#20013;&#20063;&#35266;&#23519;&#21040;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#23545;&#25506;&#38024;&#20351;&#29992;&#20102;&#24443;&#24213;&#30340;&#30495;&#23454;&#29305;&#24449;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#21487;&#35270;&#21270;&#32467;&#26524;&#26174;&#31034;&#65292;&#27491;&#20132;&#25506;&#38024;&#25429;&#25417;&#21040;&#20114;&#34917;&#30340;&#19982;&#30495;&#23454;&#30456;&#20851;&#30340;&#29305;&#24449;&#65292;&#24418;&#25104;&#20102;&#28165;&#26224;&#23450;&#20041;&#30340;&#32858;&#31867;&#65292;&#25581;&#31034;&#20102;&#20869;&#22312;&#30340;&#30495;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
Despite the great success of large language models (LLMs) in various tasks, they suffer from generating hallucinations. We introduce Truth Forest, a method that enhances truthfulness in LLMs by uncovering hidden truth representations using multi-dimensional orthogonal probes. Specifically, it creates multiple orthogonal bases for modeling truth by incorporating orthogonal constraints into the probes. Moreover, we introduce Random Peek, a systematic technique considering an extended range of positions within the sequence, reducing the gap between discerning and generating truth features in LLMs. By employing this approach, we improved the truthfulness of Llama-2-7B from 40.8\% to 74.5\% on TruthfulQA. Likewise, significant improvements are observed in fine-tuned models. We conducted a thorough analysis of truth features using probes. Our visualization results show that orthogonal probes capture complementary truth-related features, forming well-defined clusters that reveal the inherent 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;26&#20010;&#25351;&#23548;&#21407;&#21017;&#65292;&#20197;&#31616;&#21270;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25552;&#38382;&#21644;&#25552;&#31034;&#30340;&#36807;&#31243;&#12290;&#36890;&#36807;&#22312;LLaMA-1/2&#21644;GPT-3.5/4&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#21407;&#21017;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2312.16171</link><description>&lt;p&gt;
&#20165;&#38656;&#35268;&#33539;&#25351;&#20196;&#65306;&#23545;LLaMA-1/2&#12289;GPT-3.5/4&#36827;&#34892;&#30097;&#38382;&#30340;&#21407;&#21017;
&lt;/p&gt;
&lt;p&gt;
Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4. (arXiv:2312.16171v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.16171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;26&#20010;&#25351;&#23548;&#21407;&#21017;&#65292;&#20197;&#31616;&#21270;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25552;&#38382;&#21644;&#25552;&#31034;&#30340;&#36807;&#31243;&#12290;&#36890;&#36807;&#22312;LLaMA-1/2&#21644;GPT-3.5/4&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#21407;&#21017;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;26&#20010;&#25351;&#23548;&#21407;&#21017;&#65292;&#26088;&#22312;&#31616;&#21270;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25552;&#38382;&#21644;&#25552;&#31034;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#31616;&#21270;&#38024;&#23545;&#19981;&#21516;&#35268;&#27169;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21046;&#23450;&#38382;&#39064;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#26816;&#26597;&#20854;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#19981;&#21516;&#25552;&#31034;&#26102;&#28041;&#21450;&#30340;&#19981;&#21516;&#35268;&#27169;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29992;&#25143;&#29702;&#35299;&#12290;&#25105;&#20204;&#22312;LLaMA-1/2 (7B, 13B&#21644;70B)&#12289;GPT-3.5/4&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#20197;&#39564;&#35777;&#25152;&#25552;&#20986;&#30340;&#25351;&#23548;&#21407;&#21017;&#22312;&#25351;&#20196;&#21644;&#25552;&#31034;&#35774;&#35745;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#39033;&#24037;&#20316;&#33021;&#20026;&#20174;&#20107;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#30740;&#31350;&#30340;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#26356;&#22909;&#30340;&#25351;&#23548;&#12290;&#39033;&#30446;&#39029;&#38754;&#20301;&#20110;https://github.com/VILA-Lab/ATLAS&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces 26 guiding principles designed to streamline the process of querying and prompting large language models. Our goal is to simplify the underlying concepts of formulating questions for various scales of large language models, examining their abilities, and enhancing user comprehension on the behaviors of different scales of large language models when feeding into different prompts. Extensive experiments are conducted on LLaMA-1/2 (7B, 13B and 70B), GPT-3.5/4 to verify the effectiveness of the proposed principles on instructions and prompts design. We hope that this work can provide a better guide for researchers working on the prompting of large language models. Project page is available at https://github.com/VILA-Lab/ATLAS.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#31216;&#20026;&#36923;&#36753;&#25645;&#24314;&#65292;&#36890;&#36807;&#32467;&#21512;&#38754;&#21521;&#26041;&#38754;&#30340;&#35299;&#37322;&#21644;&#24605;&#32500;&#38142;&#25552;&#31034;&#30340;&#24605;&#24819;&#65292;&#22312;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#20013;&#29983;&#25104;&#25512;&#33616;&#35299;&#37322;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#20811;&#26381;&#29616;&#26377;&#27169;&#22411;&#22312;&#20135;&#29983;&#38646;&#28846;&#20987;&#35299;&#37322;&#26041;&#38754;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2312.14345</link><description>&lt;p&gt;
&#36923;&#36753;&#25645;&#24314;&#65306;&#20351;&#29992;LLMs&#36827;&#34892;&#20010;&#24615;&#21270;&#30340;&#38754;&#21521;&#25351;&#23548;&#30340;&#25512;&#33616;&#35299;&#37322;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Logic-Scaffolding: Personalized Aspect-Instructed Recommendation Explanation Generation using LLMs. (arXiv:2312.14345v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.14345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#31216;&#20026;&#36923;&#36753;&#25645;&#24314;&#65292;&#36890;&#36807;&#32467;&#21512;&#38754;&#21521;&#26041;&#38754;&#30340;&#35299;&#37322;&#21644;&#24605;&#32500;&#38142;&#25552;&#31034;&#30340;&#24605;&#24819;&#65292;&#22312;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#20013;&#29983;&#25104;&#25512;&#33616;&#35299;&#37322;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#20811;&#26381;&#29616;&#26377;&#27169;&#22411;&#22312;&#20135;&#29983;&#38646;&#28846;&#20987;&#35299;&#37322;&#26041;&#38754;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#29420;&#29305;&#33021;&#21147;&#65292;&#22914;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#65292;&#20351;&#23427;&#20204;&#25104;&#20026;&#25552;&#20379;&#25512;&#33616;&#35299;&#37322;&#30340;&#24378;&#26377;&#21147;&#20505;&#36873;&#32773;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;LLM&#30340;&#35268;&#27169;&#24456;&#22823;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#27169;&#22411;&#22312;&#21487;&#38752;&#22320;&#20135;&#29983;&#38646;&#28846;&#20987;&#35299;&#37322;&#26041;&#38754;&#20173;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#36923;&#36753;&#25645;&#24314;&#30340;&#26694;&#26550;&#65292;&#23558;&#38754;&#21521;&#26041;&#38754;&#30340;&#35299;&#37322;&#21644;&#24605;&#32500;&#38142;&#25552;&#31034;&#30340;&#24605;&#24819;&#32467;&#21512;&#36215;&#26469;&#65292;&#36890;&#36807;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#29983;&#25104;&#35299;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#20139;&#20102;&#26500;&#24314;&#35813;&#26694;&#26550;&#30340;&#32463;&#39564;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#28436;&#31034;&#26469;&#25506;&#32034;&#25105;&#20204;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The unique capabilities of Large Language Models (LLMs), such as the natural language text generation ability, position them as strong candidates for providing explanation for recommendations. However, despite the size of the LLM, most existing models struggle to produce zero-shot explanations reliably. To address this issue, we propose a framework called Logic-Scaffolding, that combines the ideas of aspect-based explanation and chain-of-thought prompting to generate explanations through intermediate reasoning steps. In this paper, we share our experience in building the framework and present an interactive demonstration for exploring our results.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#24341;&#23548;&#38750;&#33258;&#22238;&#24402;&#30693;&#35782;&#33976;&#39311;&#65288;GNARKD&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#23558;&#33258;&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#20851;&#38190;&#32452;&#20214;&#20445;&#30041;&#22312;&#32593;&#32476;&#26550;&#26500;&#20013;&#65292;&#20174;&#32780;&#33719;&#24471;&#20855;&#26377;&#20302;&#25512;&#29702;&#24310;&#36831;&#30340;&#39640;&#24615;&#33021;&#38750;&#33258;&#22238;&#24402;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#27714;&#35299;&#22120;&#12290;</title><link>http://arxiv.org/abs/2312.12469</link><description>&lt;p&gt;
&#23558;&#33258;&#22238;&#24402;&#27169;&#22411;&#25552;&#28860;&#20026;&#20855;&#26377;&#36739;&#24555;&#25512;&#29702;&#36895;&#24230;&#30340;&#39640;&#24615;&#33021;&#38750;&#33258;&#22238;&#24402;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
Distilling Autoregressive Models to Obtain High-Performance Non-Autoregressive Solvers for Vehicle Routing Problems with Faster Inference Speed. (arXiv:2312.12469v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.12469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#24341;&#23548;&#38750;&#33258;&#22238;&#24402;&#30693;&#35782;&#33976;&#39311;&#65288;GNARKD&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#23558;&#33258;&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#20851;&#38190;&#32452;&#20214;&#20445;&#30041;&#22312;&#32593;&#32476;&#26550;&#26500;&#20013;&#65292;&#20174;&#32780;&#33719;&#24471;&#20855;&#26377;&#20302;&#25512;&#29702;&#24310;&#36831;&#30340;&#39640;&#24615;&#33021;&#38750;&#33258;&#22238;&#24402;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#27714;&#35299;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37319;&#29992;&#33258;&#22238;&#24402;&#65288;AR&#65289;&#25110;&#38750;&#33258;&#22238;&#24402;&#65288;NAR&#65289;&#23398;&#20064;&#26041;&#27861;&#65292;&#31070;&#32463;&#26500;&#24314;&#27169;&#22411;&#22312;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65288;VRP&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;&#34429;&#28982;AR&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#30001;&#20110;&#20854;&#39034;&#24207;&#29983;&#25104;&#24615;&#36136;&#65292;&#25512;&#29702;&#24310;&#36831;&#36890;&#24120;&#36739;&#39640;&#12290;&#30456;&#21453;&#65292;NAR&#27169;&#22411;&#20197;&#20302;&#25512;&#29702;&#24310;&#36831;&#24182;&#34892;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#36890;&#24120;&#34920;&#29616;&#20986;&#36739;&#20302;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#24341;&#23548;&#38750;&#33258;&#22238;&#24402;&#30693;&#35782;&#33976;&#39311;&#65288;GNARKD&#65289;&#26041;&#27861;&#65292;&#20197;&#33719;&#24471;&#20855;&#26377;&#20302;&#25512;&#29702;&#24310;&#36831;&#30340;&#39640;&#24615;&#33021;NAR&#27169;&#22411;&#12290;GNARKD&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#65292;&#21435;&#38500;AR&#27169;&#22411;&#20013;&#39034;&#24207;&#29983;&#25104;&#30340;&#32422;&#26463;&#65292;&#21516;&#26102;&#20445;&#30041;&#32593;&#32476;&#26550;&#26500;&#20013;&#23398;&#21040;&#30340;&#20851;&#38190;&#32452;&#20214;&#65292;&#33719;&#24471;&#30456;&#24212;&#30340;NAR&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;GNARKD&#24212;&#29992;&#20110;&#19977;&#31181;&#24191;&#27867;&#37319;&#29992;&#30340;AR&#27169;&#22411;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#23454;&#20363;&#19978;&#33719;&#24471;NAR VRP&#27714;&#35299;&#22120;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural construction models have shown promising performance for Vehicle Routing Problems (VRPs) by adopting either the Autoregressive (AR) or Non-Autoregressive (NAR) learning approach. While AR models produce high-quality solutions, they generally have a high inference latency due to their sequential generation nature. Conversely, NAR models generate solutions in parallel with a low inference latency but generally exhibit inferior performance. In this paper, we propose a generic Guided Non-Autoregressive Knowledge Distillation (GNARKD) method to obtain high-performance NAR models having a low inference latency. GNARKD removes the constraint of sequential generation in AR models while preserving the learned pivotal components in the network architecture to obtain the corresponding NAR models through knowledge distillation. We evaluate GNARKD by applying it to three widely adopted AR models to obtain NAR VRP solvers for both synthesized and real-world instances. The experimental results
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#37327;&#21270;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#21644;&#35748;&#30693;&#20449;&#20219;&#30340;&#24046;&#24322;&#65292;&#25105;&#20204;&#21457;&#29616;&#20154;&#20204;&#20542;&#21521;&#20110;&#19982;&#26368;&#30456;&#20284;&#30340;&#27169;&#22411;&#36827;&#34892;&#21327;&#20316;&#12290;</title><link>http://arxiv.org/abs/2312.08722</link><description>&lt;p&gt;
&#37327;&#21270;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#21644;&#35748;&#30693;&#20449;&#20219;&#30340;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Quantifying Divergence for Human-AI Collaboration and Cognitive Trust. (arXiv:2312.08722v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.08722
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37327;&#21270;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#21644;&#35748;&#30693;&#20449;&#20219;&#30340;&#24046;&#24322;&#65292;&#25105;&#20204;&#21457;&#29616;&#20154;&#20204;&#20542;&#21521;&#20110;&#19982;&#26368;&#30456;&#20284;&#30340;&#27169;&#22411;&#36827;&#34892;&#21327;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#21327;&#20316;&#21487;&#33021;&#24615;&#21644;&#27979;&#37327;&#20154;&#20204;&#23545;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#35748;&#30693;&#20449;&#20219;&#27604;&#20197;&#24448;&#26356;&#37325;&#35201;&#12290;&#20026;&#20102;&#20570;&#21040;&#36825;&#19968;&#28857;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#27169;&#22411;&#29305;&#24449;&#65288;&#20363;&#22914;&#20934;&#30830;&#24230;&#12289;&#32622;&#20449;&#24230;&#65289;&#65292;&#32780;&#24573;&#30053;&#20102;&#20154;&#30340;&#22240;&#32032;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#22522;&#20110;&#24046;&#24322;&#24230;&#37327;&#65288;&#22914;KL&#12289;JSD&#65289;&#35745;&#31639;&#20174;&#20154;&#31867;&#21644;&#21508;&#31181;&#27169;&#22411;&#20013;&#33719;&#21462;&#30340;&#26631;&#31614;&#30340;&#20915;&#31574;&#30456;&#20284;&#24230;&#24230;&#37327;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#25991;&#26412;&#34164;&#21547;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#29992;&#25143;&#30740;&#31350;&#65292;&#29992;&#25143;&#20204;&#20174;&#21508;&#31181;&#27169;&#22411;&#25552;&#20379;&#30340;&#36719;&#26631;&#31614;&#20013;&#36873;&#25321;&#26368;&#25509;&#36817;&#30340;&#36873;&#39033;&#12290;&#28982;&#21518;&#65292;&#29992;&#25143;&#20204;&#30475;&#21040;&#19982;&#20182;&#20204;&#26368;&#30456;&#20284;&#30340;&#27169;&#22411;&#30340;&#30456;&#20284;&#24615;/&#24046;&#24322;&#65292;&#24182;&#23545;&#20182;&#20204;&#19982;&#25152;&#36873;&#31995;&#32479;&#30340;&#21327;&#20316;&#21487;&#33021;&#24615;&#21644;&#35748;&#30693;&#20449;&#20219;&#36827;&#34892;&#35843;&#26597;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#25552;&#20986;&#30340;&#20915;&#31574;&#30456;&#20284;&#24230;&#24230;&#37327;&#19982;&#35843;&#26597;&#32467;&#26524;&#20043;&#38388;&#30340;&#20851;&#31995;&#36827;&#34892;&#20102;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#12290;&#25105;&#20204;&#21457;&#29616;&#20154;&#20204;&#20542;&#21521;&#20110;&#19982;&#20182;&#20204;&#26368;&#30456;&#20284;&#30340;&#27169;&#22411;&#36827;&#34892;&#21327;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting the collaboration likelihood and measuring cognitive trust to AI systems is more important than ever. To do that, previous research mostly focus solely on the model features (e.g., accuracy, confidence) and ignore the human factor. To address that, we propose several decision-making similarity measures based on divergence metrics (e.g., KL, JSD) calculated over the labels acquired from humans and a wide range of models. We conduct a user study on a textual entailment task, where the users are provided with soft labels from various models and asked to pick the closest option to them. The users are then shown the similarities/differences to their most similar model and are surveyed for their likelihood of collaboration and cognitive trust to the selected system. Finally, we qualitatively and quantitatively analyze the relation between the proposed decision-making similarity measures and the survey results. We find that people tend to collaborate with their most similar models 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#32423;&#23398;&#20064;&#31639;&#27861;SHSR&#65292;&#29992;&#20110;&#20943;&#23569;AutoML&#20013;&#30340;&#36229;&#21442;&#25968;&#31354;&#38388;&#65292;&#20943;&#23569;&#20102;&#32422;30%&#30340;&#25191;&#34892;&#26102;&#38388;&#24182;&#19988;&#24615;&#33021;&#25439;&#22833;&#23567;&#20110;0.1%&#12290;</title><link>http://arxiv.org/abs/2312.06305</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#20013;&#39034;&#24207;&#36229;&#21442;&#25968;&#31354;&#38388;&#32553;&#20943;&#30340;&#20803;&#32423;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Meta-Level Learning Algorithm for Sequential Hyper-Parameter Space Reduction in AutoML. (arXiv:2312.06305v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.06305
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#32423;&#23398;&#20064;&#31639;&#27861;SHSR&#65292;&#29992;&#20110;&#20943;&#23569;AutoML&#20013;&#30340;&#36229;&#21442;&#25968;&#31354;&#38388;&#65292;&#20943;&#23569;&#20102;&#32422;30%&#30340;&#25191;&#34892;&#26102;&#38388;&#24182;&#19988;&#24615;&#33021;&#25439;&#22833;&#23567;&#20110;0.1%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;AutoML&#24179;&#21488;&#19978;&#65292;&#27599;&#20010;&#20998;&#26512;&#27493;&#39588;&#37117;&#26377;&#35768;&#22810;&#31639;&#27861;&#21487;&#20379;&#23581;&#35797;&#65292;&#20363;&#22914;&#25554;&#34917;&#31639;&#27861;&#12289;&#36716;&#25442;&#31639;&#27861;&#12289;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#21644;&#24314;&#27169;&#31639;&#27861;&#31561;&#12290;&#25214;&#21040;&#26368;&#20339;&#30340;&#31639;&#27861;&#32452;&#21512;&#21644;&#36229;&#21442;&#25968;&#20540;&#26159;&#35745;&#31639;&#19978;&#26114;&#36149;&#30340;&#65292;&#22240;&#20026;&#35201;&#25506;&#32034;&#30340;&#32452;&#21512;&#25968;&#37327;&#23548;&#33268;&#31354;&#38388;&#30340;&#25351;&#25968;&#29190;&#28856;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#39034;&#24207;&#36229;&#21442;&#25968;&#31354;&#38388;&#32553;&#20943;&#65288;SHSR&#65289;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20943;&#23569;AutoML&#24037;&#20855;&#25152;&#38656;&#30340;&#31354;&#38388;&#65292;&#24182;&#19988;&#24615;&#33021;&#25439;&#22833;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#12290;SHSR&#26159;&#19968;&#31181;&#20803;&#32423;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#20998;&#26512;AutoML&#24037;&#20855;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#36807;&#21435;&#36816;&#34892;&#32467;&#26524;&#65292;&#24182;&#23398;&#20064;&#21738;&#20123;&#36229;&#21442;&#25968;&#20540;&#21487;&#20197;&#20174;&#35201;&#20998;&#26512;&#30340;&#26032;&#25968;&#25454;&#38598;&#20013;&#36807;&#28388;&#25481;&#12290;SHSR&#22312;284&#20010;&#20998;&#31867;&#38382;&#39064;&#21644;375&#20010;&#22238;&#24402;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#26174;&#31034;&#20986;&#32422;30%&#30340;&#25191;&#34892;&#26102;&#38388;&#32553;&#30701;&#21644;&#19981;&#21040;0.1%&#30340;&#24615;&#33021;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
AutoML platforms have numerous options for the algorithms to try for each step of the analysis, i.e., different possible algorithms for imputation, transformations, feature selection, and modelling. Finding the optimal combination of algorithms and hyper-parameter values is computationally expensive, as the number of combinations to explore leads to an exponential explosion of the space. In this paper, we present the Sequential Hyper-parameter Space Reduction (SHSR) algorithm that reduces the space for an AutoML tool with negligible drop in its predictive performance. SHSR is a meta-level learning algorithm that analyzes past runs of an AutoML tool on several datasets and learns which hyper-parameter values to filter out from consideration on a new dataset to analyze. SHSR is evaluated on 284 classification and 375 regression problems, showing an approximate 30% reduction in execution time with a performance drop of less than 0.1%.
&lt;/p&gt;</description></item><item><title>&#36870;&#21521;&#35782;&#21035; (INVERT) &#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36830;&#25509;&#23398;&#20064;&#21040;&#30340;&#31070;&#32463;&#34920;&#31034;&#19982;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#65292;&#23454;&#29616;&#20102;&#23545;&#31070;&#32463;&#34920;&#31034;&#30340;&#26631;&#35760;&#24182;&#25552;&#20379;&#20102;&#32479;&#35745;&#26174;&#33879;&#24615;&#35780;&#20272;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2311.13594</link><description>&lt;p&gt;
&#22312;&#36870;&#21521;&#35782;&#21035;&#20013;&#26631;&#35760;&#31070;&#32463;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Labeling Neural Representations with Inverse Recognition. (arXiv:2311.13594v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.13594
&lt;/p&gt;
&lt;p&gt;
&#36870;&#21521;&#35782;&#21035; (INVERT) &#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36830;&#25509;&#23398;&#20064;&#21040;&#30340;&#31070;&#32463;&#34920;&#31034;&#19982;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#65292;&#23454;&#29616;&#20102;&#23545;&#31070;&#32463;&#34920;&#31034;&#30340;&#26631;&#35760;&#24182;&#25552;&#20379;&#20102;&#32479;&#35745;&#26174;&#33879;&#24615;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#22312;&#23398;&#20064;&#22797;&#26434;&#30340;&#23618;&#32423;&#25968;&#25454;&#34920;&#31034;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#34920;&#31034;&#30340;&#24615;&#36136;&#20173;&#28982;&#22823;&#37096;&#20998;&#26410;&#30693;&#12290;&#29616;&#26377;&#30340;&#20840;&#23616;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#22914;&#32593;&#32476;&#35299;&#21078;(Network Dissection)&#65292;&#23384;&#22312;&#35832;&#22810;&#38480;&#21046;&#65292;&#22914;&#20381;&#36182;&#20998;&#21106;&#36974;&#32617;&#12289;&#32570;&#20047;&#32479;&#35745;&#26174;&#33879;&#24615;&#26816;&#39564;&#21644;&#39640;&#35745;&#31639;&#38656;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Inverse Recognition (INVERT)&#26041;&#27861;&#65292;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20854;&#21306;&#20998;&#36825;&#20123;&#27010;&#24565;&#30340;&#33021;&#21147;&#65292;&#23558;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#19982;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#30456;&#36830;&#25509;&#12290;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;INVERT&#33021;&#22815;&#22788;&#29702;&#19981;&#21516;&#31867;&#22411;&#30340;&#31070;&#32463;&#20803;&#65292;&#35745;&#31639;&#22797;&#26434;&#24230;&#26356;&#20302;&#65292;&#24182;&#19988;&#19981;&#20381;&#36182;&#20110;&#20998;&#21106;&#36974;&#32617;&#30340;&#21487;&#29992;&#24615;&#12290;&#27492;&#22806;&#65292;INVERT&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#24230;&#37327;&#65292;&#35780;&#20272;&#34920;&#31034;&#21644;&#20854;&#30456;&#24212;&#35299;&#37322;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#24182;&#25552;&#20379;&#19968;&#31181;&#32479;&#35745;&#26174;&#33879;&#24615;&#30340;&#24230;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;INVERT&#30340;&#24212;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks (DNNs) demonstrate remarkable capabilities in learning complex hierarchical data representations, but the nature of these representations remains largely unknown. Existing global explainability methods, such as Network Dissection, face limitations such as reliance on segmentation masks, lack of statistical significance testing, and high computational demands. We propose Inverse Recognition (INVERT), a scalable approach for connecting learned representations with human-understandable concepts by leveraging their capacity to discriminate between these concepts. In contrast to prior work, INVERT is capable of handling diverse types of neurons, exhibits less computational complexity, and does not rely on the availability of segmentation masks. Moreover, INVERT provides an interpretable metric assessing the alignment between the representation and its corresponding explanation and delivering a measure of statistical significance. We demonstrate the applicability of INVE
&lt;/p&gt;</description></item><item><title>INTERVENOR&#27169;&#22411;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#20462;&#22797;&#20195;&#30721;&#30340;&#34892;&#20026;&#65292;&#20351;&#29992;&#20132;&#20114;&#24335;&#20462;&#22797;&#38142;&#26465;&#26469;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32534;&#30721;&#33021;&#21147;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2311.09868</link><description>&lt;p&gt;
INTERVENOR: &#20351;&#29992;&#20132;&#20114;&#24335;&#20462;&#22797;&#38142;&#26465;&#26469;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32534;&#30721;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
INTERVENOR: Prompt the Coding Ability of Large Language Models with the Interactive Chain of Repairing. (arXiv:2311.09868v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.09868
&lt;/p&gt;
&lt;p&gt;
INTERVENOR&#27169;&#22411;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#20462;&#22797;&#20195;&#30721;&#30340;&#34892;&#20026;&#65292;&#20351;&#29992;&#20132;&#20114;&#24335;&#20462;&#22797;&#38142;&#26465;&#26469;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32534;&#30721;&#33021;&#21147;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;INTERVENOR&#30340;&#20132;&#20114;&#24335;&#20462;&#22797;&#38142;&#26465;&#65288;INTERactiVE chaiN Of Repairing&#65289;&#65292;&#27169;&#25311;&#20102;&#20154;&#31867;&#20462;&#22797;&#20195;&#30721;&#30340;&#34892;&#20026;&#65288;&#36845;&#20195;&#21028;&#26029;&#12289;&#37325;&#26032;&#24605;&#32771;&#21644;&#20462;&#22797;&#65289;&#65292;&#24182;&#20419;&#36827;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#32534;&#30721;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;INTERVENOR&#37319;&#29992;&#20102;&#20004;&#20010;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#65292;&#21363;Code Learner&#21644;Code Teacher&#65292;&#23427;&#20204;&#22312;&#20195;&#30721;&#20462;&#22797;&#20013;&#25198;&#28436;&#19981;&#21516;&#30340;&#35282;&#33394;&#65292;&#24182;&#36890;&#36807;&#20114;&#21160;&#26469;&#20462;&#22797;&#29983;&#25104;&#30340;&#20195;&#30721;&#12290;Code Learner&#26681;&#25454;Code Teacher&#30340;&#25351;&#23548;&#29983;&#25104;&#21644;&#20462;&#22797;&#20195;&#30721;&#65292;&#32780;Code Teacher&#26681;&#25454;&#32534;&#35793;&#22120;&#30340;&#21453;&#39304;&#37325;&#26032;&#24605;&#32771;&#20195;&#30721;&#38169;&#35823;&#65292;&#24182;&#36845;&#20195;&#29983;&#25104;&#20462;&#22797;&#38142;&#26465;&#65288;CoR&#65289;&#20197;&#24341;&#23548;Code Learner&#30340;&#20195;&#30721;&#20462;&#22797;&#36807;&#31243;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;INTERVENOR&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#22312;&#20195;&#30721;&#29983;&#25104;&#21644;&#20195;&#30721;&#36716;&#25442;&#20219;&#21153;&#19978;&#30456;&#23545;&#20110;GPT-3.5&#27169;&#22411;&#20998;&#21035;&#21462;&#24471;&#20102;&#32422;13%&#21644;4.5%&#30340;&#25552;&#21319;&#12290;&#36827;&#19968;&#27493;&#20998;&#26512;&#34920;&#26126;&#65292;CoR&#33021;&#22815;&#25581;&#31034;bug&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes INTERactiVE chaiN Of Repairing (INTERVENOR), which mimics human code repairing behavior (iteratively judging, rethinking, and repairing) and prompts the coding ability of regard Large Language Models (LLMs). Specifically, INTERVENOR employs two LLM based agents, Code Learner and Code Teacher, to play different roles in code repairing and work interactively to repair the generated codes. The Code Learner is asked to generate and repair code according to the instructions from the Code Teacher. The Code Teacher rethinks the code errors according to the corresponding feedback from compilers and iteratively generates the chain-of-repairing (CoR) to guide the code repairing process for Code Learner. Our experiments show that INTERVENOR outperforms the state-of-the-art methods and achieves about 13% and 4.5% improvements over the GPT-3.5 model in code generation and code translation tasks, respectively. Our further analyses show that CoR can illuminate the bug reasons and 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;&#31216;&#20026;DPPDCC&#65289;&#65292;&#29992;&#20110;&#23558;&#35770;&#25991;&#30340;&#28508;&#22312;&#24433;&#21709;&#20998;&#35299;&#20026;&#20256;&#25773;&#12289;&#19968;&#33268;&#24615;&#21644;&#36129;&#29486;&#20540;&#12290;&#36890;&#36807;&#32534;&#30721;&#26102;&#24577;&#21644;&#32467;&#26500;&#29305;&#24449;&#65292;&#25429;&#25417;&#30693;&#35782;&#27969;&#21160;&#65292;&#24182;&#20351;&#29992;&#23545;&#27604;&#22686;&#24378;&#22270;&#25581;&#31034;&#27969;&#34892;&#24230;&#65292;&#36827;&#19968;&#27493;&#39044;&#27979;&#24341;&#29992;&#20998;&#32452;&#26469;&#24314;&#27169;&#19968;&#33268;&#24615;&#12290;&#24212;&#29992;&#27491;&#20132;&#32422;&#26463;&#26469;&#40723;&#21169;&#29420;&#29305;&#24314;&#27169;&#65292;&#24182;&#20445;&#30041;&#21407;&#22987;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2311.09262</link><description>&lt;p&gt;
&#23558;&#35770;&#25991;&#30340;&#28508;&#22312;&#24433;&#21709;&#20998;&#35299;&#20026;&#20256;&#25773;&#12289;&#19968;&#33268;&#24615;&#21644;&#36129;&#29486;&#20540;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Disentangling the Potential Impacts of Papers into Diffusion, Conformity, and Contribution Values. (arXiv:2311.09262v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.09262
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;&#31216;&#20026;DPPDCC&#65289;&#65292;&#29992;&#20110;&#23558;&#35770;&#25991;&#30340;&#28508;&#22312;&#24433;&#21709;&#20998;&#35299;&#20026;&#20256;&#25773;&#12289;&#19968;&#33268;&#24615;&#21644;&#36129;&#29486;&#20540;&#12290;&#36890;&#36807;&#32534;&#30721;&#26102;&#24577;&#21644;&#32467;&#26500;&#29305;&#24449;&#65292;&#25429;&#25417;&#30693;&#35782;&#27969;&#21160;&#65292;&#24182;&#20351;&#29992;&#23545;&#27604;&#22686;&#24378;&#22270;&#25581;&#31034;&#27969;&#34892;&#24230;&#65292;&#36827;&#19968;&#27493;&#39044;&#27979;&#24341;&#29992;&#20998;&#32452;&#26469;&#24314;&#27169;&#19968;&#33268;&#24615;&#12290;&#24212;&#29992;&#27491;&#20132;&#32422;&#26463;&#26469;&#40723;&#21169;&#29420;&#29305;&#24314;&#27169;&#65292;&#24182;&#20445;&#30041;&#21407;&#22987;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30340;&#28508;&#22312;&#24433;&#21709;&#21463;&#21040;&#22810;&#31181;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#20854;&#27969;&#34892;&#24230;&#21644;&#36129;&#29486;&#12290;&#29616;&#26377;&#27169;&#22411;&#36890;&#24120;&#22522;&#20110;&#38745;&#24577;&#22270;&#26469;&#20272;&#35745;&#21407;&#22987;&#24341;&#29992;&#35745;&#25968;&#65292;&#26410;&#33021;&#20174;&#32454;&#24494;&#30340;&#35282;&#24230;&#21306;&#20998;&#20215;&#20540;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#23558;&#35770;&#25991;&#30340;&#28508;&#22312;&#24433;&#21709;&#20998;&#35299;&#20026;&#20256;&#25773;&#12289;&#19968;&#33268;&#24615;&#21644;&#36129;&#29486;&#20540;&#65288;&#31216;&#20026;DPPDCC&#65289;&#12290;&#32473;&#23450;&#19968;&#20010;&#30446;&#26631;&#35770;&#25991;&#65292;DPPDCC&#22312;&#26500;&#24314;&#30340;&#21160;&#24577;&#24322;&#26500;&#22270;&#20013;&#32534;&#30721;&#20102;&#26102;&#24577;&#21644;&#32467;&#26500;&#29305;&#24449;&#12290;&#29305;&#21035;&#22320;&#65292;&#20026;&#20102;&#25429;&#25417;&#30693;&#35782;&#27969;&#21160;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#35770;&#25991;&#20043;&#38388;&#30340;&#27604;&#36739;&#21644;&#20849;&#24341;/&#34987;&#24341;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#36827;&#34892;&#20102;&#24555;&#29031;&#28436;&#21270;&#30340;&#32858;&#21512;&#12290;&#20026;&#20102;&#25581;&#31034;&#27969;&#34892;&#24230;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#27604;&#22686;&#24378;&#22270;&#26469;&#25552;&#21462;&#20256;&#25773;&#30340;&#26412;&#36136;&#65292;&#24182;&#39044;&#27979;&#32047;&#31215;&#30340;&#24341;&#29992;&#20998;&#32452;&#20197;&#24314;&#27169;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24212;&#29992;&#27491;&#20132;&#32422;&#26463;&#26469;&#40723;&#21169;&#27599;&#20010;&#35282;&#24230;&#30340;&#29420;&#29305;&#24314;&#27169;&#65292;&#24182;&#20445;&#30041;&#20854;&#22266;&#26377;&#33719;&#24471;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
The potential impact of an academic paper is determined by various factors, including its popularity and contribution. Existing models usually estimate original citation counts based on static graphs and fail to differentiate values from nuanced perspectives. In this study, we propose a novel graph neural network to Disentangle the Potential impacts of Papers into Diffusion, Conformity, and Contribution values (called DPPDCC). Given a target paper, DPPDCC encodes temporal and structural features within the constructed dynamic heterogeneous graph. Particularly, to capture the knowledge flow, we emphasize the importance of comparative and co-cited/citing information between papers and aggregate snapshots evolutionarily. To unravel popularity, we contrast augmented graphs to extract the essence of diffusion and predict the accumulated citation binning to model conformity. We further apply orthogonal constraints to encourage distinct modeling of each perspective and preserve the inherent v
&lt;/p&gt;</description></item><item><title>&#22312;DDIM&#26694;&#26550;&#20013;&#20351;&#29992;GMM&#20316;&#20026;&#21453;&#21521;&#36716;&#31227;&#31639;&#23376;&#65292;&#36890;&#36807;&#30697;&#21305;&#37197;&#21487;&#20197;&#33719;&#24471;&#36136;&#37327;&#26356;&#39640;&#30340;&#26679;&#26412;&#12290;&#22312;&#26080;&#26465;&#20214;&#27169;&#22411;&#21644;&#31867;&#26465;&#20214;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#36890;&#36807;FID&#21644;IS&#25351;&#26631;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.04938</link><description>&lt;p&gt;
&#20351;&#29992;&#30697;&#21305;&#37197;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#25913;&#36827;&#20102;DDIM&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Improved DDIM Sampling with Moment Matching Gaussian Mixtures. (arXiv:2311.04938v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.04938
&lt;/p&gt;
&lt;p&gt;
&#22312;DDIM&#26694;&#26550;&#20013;&#20351;&#29992;GMM&#20316;&#20026;&#21453;&#21521;&#36716;&#31227;&#31639;&#23376;&#65292;&#36890;&#36807;&#30697;&#21305;&#37197;&#21487;&#20197;&#33719;&#24471;&#36136;&#37327;&#26356;&#39640;&#30340;&#26679;&#26412;&#12290;&#22312;&#26080;&#26465;&#20214;&#27169;&#22411;&#21644;&#31867;&#26465;&#20214;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#36890;&#36807;FID&#21644;IS&#25351;&#26631;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#22312;Denoising Diffusion Implicit Models (DDIM)&#26694;&#26550;&#20013;&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMM&#65289;&#20316;&#20026;&#21453;&#21521;&#36716;&#31227;&#31639;&#23376;&#65288;&#20869;&#26680;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20174;&#39044;&#35757;&#32451;&#30340;Denoising Diffusion Probabilistic Models (DDPM)&#20013;&#21152;&#36895;&#37319;&#26679;&#30340;&#24191;&#27867;&#24212;&#29992;&#26041;&#27861;&#20043;&#19968;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#32422;&#26463;GMM&#30340;&#21442;&#25968;&#65292;&#21305;&#37197;DDPM&#21069;&#21521;&#36793;&#38469;&#30340;&#19968;&#38454;&#21644;&#20108;&#38454;&#20013;&#24515;&#30697;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#30697;&#21305;&#37197;&#65292;&#21487;&#20197;&#33719;&#24471;&#19982;&#20351;&#29992;&#39640;&#26031;&#26680;&#30340;&#21407;&#22987;DDIM&#30456;&#21516;&#25110;&#26356;&#22909;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#22312;CelebAHQ&#21644;FFHQ&#30340;&#26080;&#26465;&#20214;&#27169;&#22411;&#20197;&#21450;ImageNet&#25968;&#25454;&#38598;&#30340;&#31867;&#26465;&#20214;&#27169;&#22411;&#19978;&#25552;&#20379;&#20102;&#23454;&#39564;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#37319;&#26679;&#27493;&#39588;&#36739;&#23569;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;GMM&#20869;&#26680;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#29983;&#25104;&#26679;&#26412;&#30340;&#36136;&#37327;&#65292;&#36825;&#26159;&#36890;&#36807;FID&#21644;IS&#25351;&#26631;&#34913;&#37327;&#30340;&#12290;&#20363;&#22914;&#65292;&#22312;ImageNet 256x256&#19978;&#65292;&#20351;&#29992;10&#20010;&#37319;&#26679;&#27493;&#39588;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;FID&#20540;&#20026;...
&lt;/p&gt;
&lt;p&gt;
We propose using a Gaussian Mixture Model (GMM) as reverse transition operator (kernel) within the Denoising Diffusion Implicit Models (DDIM) framework, which is one of the most widely used approaches for accelerated sampling from pre-trained Denoising Diffusion Probabilistic Models (DDPM). Specifically we match the first and second order central moments of the DDPM forward marginals by constraining the parameters of the GMM. We see that moment matching is sufficient to obtain samples with equal or better quality than the original DDIM with Gaussian kernels. We provide experimental results with unconditional models trained on CelebAHQ and FFHQ and class-conditional models trained on ImageNet datasets respectively. Our results suggest that using the GMM kernel leads to significant improvements in the quality of the generated samples when the number of sampling steps is small, as measured by FID and IS metrics. For example on ImageNet 256x256, using 10 sampling steps, we achieve a FID of
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#39640;&#25928;&#21644;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#27979;&#35797;&#26102;&#21457;&#29616;&#20197;&#21069;&#26410;&#30693;&#30340;&#31867;&#21035;&#65292;&#36890;&#36807;&#23558;&#26368;&#23567;&#38271;&#24230;&#31867;&#21035;&#20195;&#30721;&#20998;&#37197;&#32473;&#21333;&#20010;&#25968;&#25454;&#23454;&#20363;&#26469;&#22686;&#24378;&#23545;&#31867;&#21035;&#32454;&#31890;&#24230;&#30340;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.19776</link><description>&lt;p&gt;
&#23398;&#20064;&#20998;&#31867;&#36824;&#26159;&#20998;&#31867;&#23398;&#20064;&#65311;&#33258;&#32534;&#30721;&#23454;&#29616;&#26222;&#36866;&#31867;&#21035;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Learn to Categorize or Categorize to Learn? Self-Coding for Generalized Category Discovery. (arXiv:2310.19776v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19776
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#39640;&#25928;&#21644;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#27979;&#35797;&#26102;&#21457;&#29616;&#20197;&#21069;&#26410;&#30693;&#30340;&#31867;&#21035;&#65292;&#36890;&#36807;&#23558;&#26368;&#23567;&#38271;&#24230;&#31867;&#21035;&#20195;&#30721;&#20998;&#37197;&#32473;&#21333;&#20010;&#25968;&#25454;&#23454;&#20363;&#26469;&#22686;&#24378;&#23545;&#31867;&#21035;&#32454;&#31890;&#24230;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25581;&#31034;&#27979;&#35797;&#26102;&#30340;&#26032;&#31867;&#21035;&#30340;&#25506;&#32034;&#20013;&#65292;&#25105;&#20204;&#38754;&#20020;&#30528;&#20256;&#32479;&#26377;&#30417;&#30563;&#35782;&#21035;&#27169;&#22411;&#30340;&#22266;&#26377;&#38480;&#21046;&#65292;&#36825;&#20123;&#27169;&#22411;&#21463;&#21040;&#39044;&#23450;&#20041;&#31867;&#21035;&#38598;&#30340;&#38480;&#21046;&#12290;&#34429;&#28982;&#22312;&#33258;&#25105;&#30417;&#30563;&#21644;&#24320;&#25918;&#24335;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#19968;&#23450;&#36827;&#23637;&#65292;&#20197;&#23454;&#29616;&#27979;&#35797;&#26102;&#30340;&#31867;&#21035;&#21457;&#29616;&#65292;&#20294;&#19968;&#20010;&#20851;&#38190;&#20294;&#24120;&#24120;&#34987;&#24573;&#35270;&#30340;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#65306;&#20160;&#20040;&#30830;&#20999;&#22320;&#30028;&#23450;&#20102;&#19968;&#20010;&#31867;&#21035;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20248;&#21270;&#30340;&#35270;&#35282;&#27010;&#24565;&#21270;&#31867;&#21035;&#65292;&#23558;&#20854;&#35270;&#20026;&#19968;&#20010;&#26126;&#30830;&#23450;&#20041;&#38382;&#39064;&#30340;&#26368;&#20248;&#35299;&#12290;&#21033;&#29992;&#36825;&#31181;&#29420;&#29305;&#30340;&#27010;&#24565;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#39640;&#25928;&#21644;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#27979;&#35797;&#26102;&#21457;&#29616;&#20197;&#21069;&#26410;&#30693;&#30340;&#31867;&#21035;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#26174;&#33879;&#29305;&#28857;&#26159;&#23558;&#26368;&#23567;&#38271;&#24230;&#31867;&#21035;&#20195;&#30721;&#20998;&#37197;&#32473;&#21333;&#20010;&#25968;&#25454;&#23454;&#20363;&#65292;&#36825;&#26679;&#21487;&#20197;&#27010;&#25324;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#38544;&#21547;&#31867;&#21035;&#23618;&#27425;&#32467;&#26500;&#12290;&#36825;&#31181;&#26426;&#21046;&#20351;&#25105;&#20204;&#33021;&#22815;&#26356;&#22909;&#22320;&#25511;&#21046;&#31867;&#21035;&#30340;&#32454;&#31890;&#24230;&#65292;&#20174;&#32780;&#20026;&#25105;&#20204;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#22686;&#24378;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the quest for unveiling novel categories at test time, we confront the inherent limitations of traditional supervised recognition models that are restricted by a predefined category set. While strides have been made in the realms of self-supervised and open-world learning towards test-time category discovery, a crucial yet often overlooked question persists: what exactly delineates a category? In this paper, we conceptualize a category through the lens of optimization, viewing it as an optimal solution to a well-defined problem. Harnessing this unique conceptualization, we propose a novel, efficient and self-supervised method capable of discovering previously unknown categories at test time. A salient feature of our approach is the assignment of minimum length category codes to individual data instances, which encapsulates the implicit category hierarchy prevalent in real-world datasets. This mechanism affords us enhanced control over category granularity, thereby equipping our mode
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27169;&#22411;&#36866;&#24212;&#26469;&#26816;&#27979;&#21644;&#20943;&#36731;&#35821;&#35328;&#27169;&#22411;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#20559;&#35265;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.18913</link><description>&lt;p&gt;
&#36890;&#36807;&#27169;&#22411;&#36866;&#24212;&#26469;&#21435;&#38500;&#20559;&#35265;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Debiasing Algorithm through Model Adaptation. (arXiv:2310.18913v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27169;&#22411;&#36866;&#24212;&#26469;&#26816;&#27979;&#21644;&#20943;&#36731;&#35821;&#35328;&#27169;&#22411;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#20559;&#35265;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27491;&#22312;&#25104;&#20026;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#30340;&#39318;&#36873;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#23481;&#37327;&#30340;&#22686;&#38271;&#65292;&#27169;&#22411;&#24456;&#23481;&#26131;&#20381;&#36182;&#35757;&#32451;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#20559;&#35265;&#21644;&#21051;&#26495;&#21360;&#35937;&#25152;&#20135;&#29983;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#21644;&#20943;&#36731;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;&#25105;&#20204;&#36827;&#34892;&#22240;&#26524;&#20998;&#26512;&#65292;&#20197;&#35782;&#21035;&#38382;&#39064;&#27169;&#22411;&#32452;&#20214;&#65292;&#24182;&#21457;&#29616;&#20013;&#19978;&#23618;&#21069;&#39304;&#23618;&#26368;&#23481;&#26131;&#20256;&#36882;&#20559;&#35265;&#12290;&#26681;&#25454;&#20998;&#26512;&#32467;&#26524;&#65292;&#25105;&#20204;&#36890;&#36807;&#32447;&#24615;&#25237;&#24433;&#23558;&#36825;&#20123;&#23618;&#20056;&#20197;&#27169;&#22411;&#36827;&#34892;&#36866;&#24212;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;DAMA&#36890;&#36807;&#21508;&#31181;&#24230;&#37327;&#25351;&#26631;&#26126;&#26174;&#20943;&#23569;&#20102;&#20559;&#35265;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#22312;&#21518;&#32493;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21644;&#27169;&#22411;&#30340;&#20195;&#30721;&#65292;&#36890;&#36807;&#37325;&#26032;&#35757;&#32451;&#65292;&#20445;&#25345;&#20102;LLaMA&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#21516;&#26102;&#20559;&#35265;&#26174;&#33879;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models are becoming the go-to solution for various language tasks. However, with growing capacity, models are prone to rely on spurious correlations stemming from biases and stereotypes present in the training data. This work proposes a novel method for detecting and mitigating gender bias in language models. We perform causal analysis to identify problematic model components and discover that mid-upper feed-forward layers are most prone to convey biases. Based on the analysis results, we adapt the model by multiplying these layers by a linear projection. Our titular method, DAMA, significantly decreases bias as measured by diverse metrics while maintaining the model's performance on downstream tasks. We release code for our method and models, which retrain LLaMA's state-of-the-art performance while being significantly less biased.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26159;&#19968;&#31687;&#20851;&#20110;&#35270;&#39057;&#24773;&#24863;&#35782;&#21035;&#30340;&#32508;&#36848;&#35770;&#25991;&#65292;&#24635;&#32467;&#20102;&#30456;&#20851;&#30740;&#31350;&#20013;&#30340;&#29616;&#26377;&#36235;&#21183;&#12289;&#24773;&#24863;&#27169;&#22411;&#12289;&#25968;&#25454;&#24211;&#20197;&#21450;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#30340;&#35270;&#39057;&#24773;&#24863;&#35782;&#21035;&#26041;&#27861;&#30340;&#32467;&#26500;&#12289;&#24615;&#33021;&#21644;&#20248;&#32570;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.17212</link><description>&lt;p&gt;
&#35270;&#39057;&#24773;&#24863;&#35782;&#21035;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Emotion Recognition by Video: A review. (arXiv:2310.17212v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#19968;&#31687;&#20851;&#20110;&#35270;&#39057;&#24773;&#24863;&#35782;&#21035;&#30340;&#32508;&#36848;&#35770;&#25991;&#65292;&#24635;&#32467;&#20102;&#30456;&#20851;&#30740;&#31350;&#20013;&#30340;&#29616;&#26377;&#36235;&#21183;&#12289;&#24773;&#24863;&#27169;&#22411;&#12289;&#25968;&#25454;&#24211;&#20197;&#21450;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#30340;&#35270;&#39057;&#24773;&#24863;&#35782;&#21035;&#26041;&#27861;&#30340;&#32467;&#26500;&#12289;&#24615;&#33021;&#21644;&#20248;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#24773;&#24863;&#35782;&#21035;&#26159;&#24773;&#24863;&#35745;&#31639;&#30340;&#37325;&#35201;&#20998;&#25903;&#65292;&#20854;&#35299;&#20915;&#26041;&#26696;&#21487;&#24212;&#29992;&#20110;&#20154;&#26426;&#20132;&#20114;&#65288;HCI&#65289;&#21644;&#26234;&#33021;&#21307;&#30103;&#31561;&#39046;&#22495;&#12290;&#23613;&#31649;&#24773;&#24863;&#35782;&#21035;&#39046;&#22495;&#21457;&#34920;&#30340;&#35770;&#25991;&#25968;&#37327;&#27491;&#22312;&#22686;&#21152;&#65292;&#20294;&#24456;&#23569;&#26377;&#20840;&#38754;&#30340;&#32508;&#36848;&#25253;&#36947;&#30456;&#20851;&#30340;&#35270;&#39057;&#24773;&#24863;&#35782;&#21035;&#30740;&#31350;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#36873;&#25321;&#20102;2015&#24180;&#33267;2023&#24180;&#21457;&#34920;&#30340;&#25991;&#31456;&#65292;&#31995;&#32479;&#24635;&#32467;&#20102;&#30456;&#20851;&#30740;&#31350;&#20013;&#35270;&#39057;&#24773;&#24863;&#35782;&#21035;&#30340;&#29616;&#26377;&#36235;&#21183;&#12290;&#26412;&#25991;&#39318;&#20808;&#35752;&#35770;&#20102;&#20004;&#31181;&#20856;&#22411;&#30340;&#24773;&#24863;&#27169;&#22411;&#65292;&#28982;&#21518;&#20171;&#32461;&#20102;&#32463;&#24120;&#29992;&#20110;&#35270;&#39057;&#24773;&#24863;&#35782;&#21035;&#30340;&#21333;&#27169;&#24577;&#25968;&#25454;&#24211;&#21644;&#22810;&#27169;&#24577;&#25968;&#25454;&#24211;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#30740;&#31350;&#21644;&#20998;&#31867;&#20102;&#29616;&#20195;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#35270;&#39057;&#24773;&#24863;&#35782;&#21035;&#26041;&#27861;&#30340;&#20855;&#20307;&#32467;&#26500;&#21644;&#24615;&#33021;&#65292;&#24182;&#35752;&#35770;&#20102;&#27599;&#31181;&#26041;&#27861;&#30340;&#20248;&#32570;&#28857;&#65292;&#24182;&#35814;&#32454;&#27604;&#36739;&#20102;&#23427;&#20204;&#30340;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;
&lt;/p&gt;
&lt;p&gt;
Video emotion recognition is an important branch of affective computing, and its solutions can be applied in different fields such as human-computer interaction (HCI) and intelligent medical treatment. Although the number of papers published in the field of emotion recognition is increasing, there are few comprehensive literature reviews covering related research on video emotion recognition. Therefore, this paper selects articles published from 2015 to 2023 to systematize the existing trends in video emotion recognition in related studies. In this paper, we first talk about two typical emotion models, then we talk about databases that are frequently utilized for video emotion recognition, including unimodal databases and multimodal databases. Next, we look at and classify the specific structure and performance of modern unimodal and multimodal video emotion recognition methods, talk about the benefits and drawbacks of each, and then we compare them in detail in the tables. Further, we
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;FactCHD&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#26816;&#27979;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#29983;&#25104;&#25991;&#26412;&#30340;&#20107;&#23454;&#24615;&#12290;&#22522;&#20934;&#21253;&#21547;&#20102;&#22810;&#31181;&#20107;&#23454;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#20107;&#23454;&#30340;&#35777;&#25454;&#38142;&#36827;&#34892;&#32452;&#21512;&#24615;&#24187;&#35273;&#30340;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.12086</link><description>&lt;p&gt;
&#21457;&#29616;&#22622;&#22764;&#20043;&#27468;&#65306;&#21487;&#38752;&#30340;&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Siren's Song: Towards Reliable Fact-Conflicting Hallucination Detection. (arXiv:2310.12086v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12086
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;FactCHD&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#26816;&#27979;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#29983;&#25104;&#25991;&#26412;&#30340;&#20107;&#23454;&#24615;&#12290;&#22522;&#20934;&#21253;&#21547;&#20102;&#22810;&#31181;&#20107;&#23454;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#20107;&#23454;&#30340;&#35777;&#25454;&#38142;&#36827;&#34892;&#32452;&#21512;&#24615;&#24187;&#35273;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT/GPT-4&#65292;&#22240;&#20854;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#20854;&#22312;&#32593;&#32476;&#24179;&#21488;&#19978;&#23384;&#22312;&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#30340;&#38382;&#39064;&#38480;&#21046;&#20102;&#20854;&#37319;&#29992;&#12290;&#23545;&#30001;LLMs&#20135;&#29983;&#30340;&#25991;&#26412;&#30340;&#20107;&#23454;&#24615;&#35780;&#20272;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#65292;&#19981;&#20165;&#28041;&#21450;&#23545;&#22522;&#26412;&#20107;&#23454;&#30340;&#21028;&#26029;&#65292;&#36824;&#21253;&#25324;&#23545;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#65288;&#22914;&#22810;&#36339;&#31561;&#65289;&#20013;&#20986;&#29616;&#30340;&#20107;&#23454;&#38169;&#35823;&#30340;&#35780;&#20272;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FactCHD&#65292;&#19968;&#31181;&#20026;LLMs&#31934;&#24515;&#35774;&#35745;&#30340;&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#26816;&#27979;&#22522;&#20934;&#12290;&#20316;&#20026;&#22312;&#8220;&#26597;&#35810;-&#21709;&#24212;&#8221;&#19978;&#19979;&#25991;&#20013;&#35780;&#20272;&#20107;&#23454;&#24615;&#30340;&#20851;&#38190;&#24037;&#20855;&#65292;&#25105;&#20204;&#30340;&#22522;&#20934;&#37319;&#29992;&#20102;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#20107;&#23454;&#27169;&#24335;&#65292;&#22914;&#22522;&#26412;&#20107;&#23454;&#65292;&#22810;&#36339;&#65292;&#27604;&#36739;&#21644;&#38598;&#21512;&#25805;&#20316;&#27169;&#24335;&#12290;&#25105;&#20204;&#22522;&#20934;&#30340;&#19968;&#20010;&#29420;&#29305;&#29305;&#28857;&#26159;&#20854;&#21253;&#21547;&#22522;&#20110;&#20107;&#23454;&#30340;&#35777;&#25454;&#38142;&#65292;&#20174;&#32780;&#20415;&#20110;&#36827;&#34892;&#32452;&#21512;&#24615;&#24187;&#35273;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), such as ChatGPT/GPT-4, have garnered widespread attention owing to their myriad of practical applications, yet their adoption has been constrained by issues of fact-conflicting hallucinations across web platforms. The assessment of factuality in text, produced by LLMs, remains inadequately explored, extending not only to the judgment of vanilla facts but also encompassing the evaluation of factual errors emerging in complex inferential tasks like multi-hop, and etc. In response, we introduce FactCHD, a fact-conflicting hallucination detection benchmark meticulously designed for LLMs. Functioning as a pivotal tool in evaluating factuality within "Query-Respons" contexts, our benchmark assimilates a large-scale dataset, encapsulating a broad spectrum of factuality patterns, such as vanilla, multi-hops, comparison, and set-operation patterns. A distinctive feature of our benchmark is its incorporation of fact-based chains of evidence, thereby facilitating com
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;Transformer&#30340;&#21151;&#33021;&#19981;&#21464;&#24615;&#27700;&#21360;&#25216;&#26415;&#65292;&#23427;&#20351;&#29992;&#27169;&#22411;&#30340;&#19981;&#21464;&#24615;&#29983;&#25104;&#21151;&#33021;&#19978;&#31561;&#25928;&#30340;&#21103;&#26412;&#65292;&#24182;&#33021;&#22312;&#19981;&#25913;&#21464;&#27169;&#22411;&#36755;&#20986;&#30340;&#24773;&#20917;&#19979;&#32473;&#27169;&#22411;&#21152;&#19978;&#27700;&#21360;&#65292;&#36825;&#26159;&#19968;&#31181;&#35745;&#31639;&#25104;&#26412;&#26497;&#20302;&#19988;&#36866;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.11446</link><description>&lt;p&gt;
&#22823;&#22411;Transformer&#27700;&#21360;&#30340;&#21151;&#33021;&#19981;&#21464;&#24615;
&lt;/p&gt;
&lt;p&gt;
Functional Invariants to Watermark Large Transformers. (arXiv:2310.11446v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;Transformer&#30340;&#21151;&#33021;&#19981;&#21464;&#24615;&#27700;&#21360;&#25216;&#26415;&#65292;&#23427;&#20351;&#29992;&#27169;&#22411;&#30340;&#19981;&#21464;&#24615;&#29983;&#25104;&#21151;&#33021;&#19978;&#31561;&#25928;&#30340;&#21103;&#26412;&#65292;&#24182;&#33021;&#22312;&#19981;&#25913;&#21464;&#27169;&#22411;&#36755;&#20986;&#30340;&#24773;&#20917;&#19979;&#32473;&#27169;&#22411;&#21152;&#19978;&#27700;&#21360;&#65292;&#36825;&#26159;&#19968;&#31181;&#35745;&#31639;&#25104;&#26412;&#26497;&#20302;&#19988;&#36866;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#30340;&#24555;&#36895;&#22686;&#38271;&#22686;&#21152;&#20102;&#23545;&#20854;&#23436;&#25972;&#24615;&#21644;&#25317;&#26377;&#26435;&#30340;&#25285;&#24551;&#12290;&#27700;&#21360;&#25216;&#26415;&#36890;&#36807;&#23558;&#21807;&#19968;&#26631;&#35782;&#23884;&#20837;&#27169;&#22411;&#20013;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#20248;&#21270;&#26435;&#37325;&#20197;&#23884;&#20837;&#27700;&#21360;&#20449;&#21495;&#65292;&#36825;&#22312;&#22823;&#35268;&#27169;&#24773;&#20917;&#19979;&#19981;&#36866;&#29992;&#20110;&#35745;&#31639;&#25104;&#26412;&#30340;&#21407;&#22240;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#20960;&#20046;&#27809;&#26377;&#35745;&#31639;&#25104;&#26412;&#19988;&#36866;&#29992;&#20110;&#38750;&#30450;&#30333;&#30418;&#35774;&#32622;&#65288;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#21407;&#22987;&#21644;&#24102;&#27700;&#21360;&#30340;&#32593;&#32476;&#65289;&#30340;&#27700;&#21360;&#25216;&#26415;&#12290;&#20182;&#20204;&#36890;&#36807;&#21033;&#29992;&#27169;&#22411;&#30340;&#19981;&#21464;&#24615;&#65292;&#27604;&#22914;&#32500;&#24230;&#25490;&#21015;&#25110;&#32553;&#25918;/&#38750;&#32553;&#25918;&#31561;&#25805;&#20316;&#65292;&#29983;&#25104;&#21151;&#33021;&#19978;&#31561;&#25928;&#30340;&#21103;&#26412;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#22312;&#19981;&#25913;&#21464;&#27169;&#22411;&#36755;&#20986;&#30340;&#24773;&#20917;&#19979;&#32473;&#27169;&#22411;&#21152;&#27700;&#21360;&#65292;&#24182;&#20445;&#25345;&#19981;&#21487;&#23519;&#35273;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#23545;&#21508;&#31181;&#27169;&#22411;&#21464;&#25442;&#65288;&#24494;&#35843;&#12289;&#37327;&#21270;&#12289;&#20462;&#21098;&#65289;&#30340;&#31283;&#20581;&#24615;&#65292;&#20351;&#20854;&#25104;&#20026;&#23454;&#38469;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid growth of transformer-based models increases the concerns about their integrity and ownership insurance. Watermarking addresses this issue by embedding a unique identifier into the model, while preserving its performance. However, most existing approaches require to optimize the weights to imprint the watermark signal, which is not suitable at scale due to the computational cost. This paper explores watermarks with virtually no computational cost, applicable to a non-blind white-box setting (assuming access to both the original and watermarked networks). They generate functionally equivalent copies by leveraging the models' invariance, via operations like dimension permutations or scaling/unscaling. This enables to watermark models without any change in their outputs and remains stealthy. Experiments demonstrate the effectiveness of the approach and its robustness against various model transformations (fine-tuning, quantization, pruning), making it a practical solution to pro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#33457;&#29923;&#25289;&#26222;&#25289;&#26031;&#30340;&#39640;&#38454;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#36890;&#36807;&#21033;&#29992;&#31616;&#21333;&#22797;&#21512;&#20307;&#26469;&#24314;&#27169;&#39640;&#38454;&#20132;&#20114;&#65292;&#22312;&#19981;&#21516;&#25299;&#25169;&#23610;&#24230;&#19978;&#35782;&#21035;&#20869;&#22312;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#22270;&#28388;&#27874;&#22120;&#26469;&#37327;&#21270;&#39640;&#38454;&#20132;&#20114;&#24378;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.12971</link><description>&lt;p&gt;
&#22522;&#20110;&#33457;&#29923;&#25289;&#26222;&#25289;&#26031;&#22312;&#31616;&#21333;&#22797;&#21512;&#20307;&#19978;&#30340;&#39640;&#38454;&#22270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Higher-order Graph Convolutional Network with Flower-Petals Laplacians on Simplicial Complexes. (arXiv:2309.12971v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#33457;&#29923;&#25289;&#26222;&#25289;&#26031;&#30340;&#39640;&#38454;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#36890;&#36807;&#21033;&#29992;&#31616;&#21333;&#22797;&#21512;&#20307;&#26469;&#24314;&#27169;&#39640;&#38454;&#20132;&#20114;&#65292;&#22312;&#19981;&#21516;&#25299;&#25169;&#23610;&#24230;&#19978;&#35782;&#21035;&#20869;&#22312;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#22270;&#28388;&#27874;&#22120;&#26469;&#37327;&#21270;&#39640;&#38454;&#20132;&#20114;&#24378;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26222;&#36890;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20854;&#22522;&#20110;&#37197;&#23545;&#20132;&#20114;&#32593;&#32476;&#30340;&#22522;&#30784;&#26412;&#36136;&#19978;&#38480;&#21046;&#20102;&#20854;&#35782;&#21035;&#22797;&#26434;&#31995;&#32479;&#20013;&#28508;&#22312;&#39640;&#38454;&#20132;&#20114;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#31181;&#33021;&#21147;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22797;&#26434;&#31995;&#32479;&#30340;&#39640;&#38454;&#20132;&#20114;&#24314;&#27169;&#30340;&#20016;&#23500;&#25968;&#23398;&#29702;&#35770;&#65292;&#21363;&#31616;&#21333;&#22797;&#21512;&#20307;&#65288;SCs&#65289;-&#19968;&#31181;&#23545;&#24314;&#27169;&#39640;&#38454;&#20132;&#20114;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#24037;&#20855;&#12290;&#30446;&#21069;&#22522;&#20110;SC&#30340;GNNs&#23384;&#22312;&#22797;&#26434;&#24230;&#39640;&#21644;&#21051;&#26495;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#37327;&#21270;&#39640;&#38454;&#20132;&#20114;&#24378;&#24230;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#21019;&#26032;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#38454;&#33457;&#29923;&#65288;FP&#65289;&#27169;&#22411;&#65292;&#23558;FP&#25289;&#26222;&#25289;&#26031;&#24341;&#20837;&#21040;SC&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20197;FP&#25289;&#26222;&#25289;&#26031;&#20026;&#22522;&#30784;&#30340;&#39640;&#38454;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;HiGCN&#65289;&#65292;&#33021;&#22815;&#35782;&#21035;&#19981;&#21516;&#25299;&#25169;&#23610;&#24230;&#19978;&#30340;&#20869;&#22312;&#29305;&#24449;&#12290;&#36890;&#36807;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#22270;&#28388;&#27874;&#22120;&#65292;FP&#25289;&#26222;&#25289;&#26031;&#22495;&#20869;&#30340;&#21442;&#25968;&#32452;&#65292;&#25105;&#20204;&#21487;&#20197;&#35782;&#21035;&#20986;&#20855;&#26377;&#19981;&#21516;&#27169;&#24335;&#30340;&#22270;&#26696;&#65292;&#20854;&#20013;&#28388;&#27874;&#22120;&#30340;&#26435;&#37325;&#29992;&#20316;&#25968;&#37327;&#21270;&#39640;&#38454;&#20132;&#20114;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the recent successes of vanilla Graph Neural Networks (GNNs) on many tasks, their foundation on pairwise interaction networks inherently limits their capacity to discern latent higher-order interactions in complex systems. To bridge this capability gap, we propose a novel approach exploiting the rich mathematical theory of simplicial complexes (SCs) - a robust tool for modeling higher-order interactions. Current SC-based GNNs are burdened by high complexity and rigidity, and quantifying higher-order interaction strengths remains challenging. Innovatively, we present a higher-order Flower-Petals (FP) model, incorporating FP Laplacians into SCs. Further, we introduce a Higher-order Graph Convolutional Network (HiGCN) grounded in FP Laplacians, capable of discerning intrinsic features across varying topological scales. By employing learnable graph filters, a parameter group within each FP Laplacian domain, we can identify diverse patterns where the filters' weights serve as a quan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;3D&#22330;&#26223;&#20013;&#24320;&#25918;&#35789;&#27719;&#20840;&#26223;&#20998;&#21106;&#30340;&#31639;&#27861;PVLFF&#65292;&#36890;&#36807;&#20174;&#39044;&#35757;&#32451;&#30340;2D&#27169;&#22411;&#20013;&#25552;&#21462;&#35270;&#35273;-&#35821;&#35328;&#29305;&#24449;&#26469;&#23398;&#20064;&#35821;&#20041;&#29305;&#24449;&#22330;&#65292;&#36890;&#36807;&#23545;&#36755;&#20837;&#24103;&#19978;&#30340;2D&#23454;&#20363;&#20998;&#21106;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#26469;&#32852;&#21512;&#25311;&#21512;&#23454;&#20363;&#29305;&#24449;&#22330;&#12290;&#35813;&#26041;&#27861;&#22312;&#20840;&#26223;&#20998;&#21106;&#21644;&#35821;&#20041;&#20998;&#21106;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.05448</link><description>&lt;p&gt;
&#20840;&#26223;&#35270;&#35273;-&#35821;&#35328;&#29305;&#24449;&#22330;
&lt;/p&gt;
&lt;p&gt;
Panoptic Vision-Language Feature Fields. (arXiv:2309.05448v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;3D&#22330;&#26223;&#20013;&#24320;&#25918;&#35789;&#27719;&#20840;&#26223;&#20998;&#21106;&#30340;&#31639;&#27861;PVLFF&#65292;&#36890;&#36807;&#20174;&#39044;&#35757;&#32451;&#30340;2D&#27169;&#22411;&#20013;&#25552;&#21462;&#35270;&#35273;-&#35821;&#35328;&#29305;&#24449;&#26469;&#23398;&#20064;&#35821;&#20041;&#29305;&#24449;&#22330;&#65292;&#36890;&#36807;&#23545;&#36755;&#20837;&#24103;&#19978;&#30340;2D&#23454;&#20363;&#20998;&#21106;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#26469;&#32852;&#21512;&#25311;&#21512;&#23454;&#20363;&#29305;&#24449;&#22330;&#12290;&#35813;&#26041;&#27861;&#22312;&#20840;&#26223;&#20998;&#21106;&#21644;&#35821;&#20041;&#20998;&#21106;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20986;&#29616;&#20102;&#19968;&#20123;&#29992;&#20110;3D&#24320;&#25918;&#35789;&#27719;&#35821;&#20041;&#20998;&#21106;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#26681;&#25454;&#36816;&#34892;&#26102;&#25552;&#20379;&#30340;&#25991;&#26412;&#25551;&#36848;&#23558;&#22330;&#26223;&#20998;&#21106;&#25104;&#20219;&#24847;&#31867;&#21035;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36804;&#20170;&#20026;&#27490;&#39318;&#20010;&#29992;&#20110;3D&#22330;&#26223;&#20013;&#24320;&#25918;&#35789;&#27719;&#20840;&#26223;&#20998;&#21106;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;Panoptic Vision-Language Feature Fields (PVLFF)&#36890;&#36807;&#20174;&#39044;&#35757;&#32451;&#30340;2D&#27169;&#22411;&#20013;&#25552;&#21462;&#35270;&#35273;-&#35821;&#35328;&#29305;&#24449;&#26469;&#23398;&#20064;&#22330;&#26223;&#30340;&#35821;&#20041;&#29305;&#24449;&#22330;&#65292;&#24182;&#36890;&#36807;&#22312;&#36755;&#20837;&#24103;&#19978;&#20351;&#29992;2D&#23454;&#20363;&#20998;&#21106;&#23454;&#29616;&#23545;&#23454;&#20363;&#29305;&#24449;&#22330;&#30340;&#32852;&#21512;&#25311;&#21512;&#12290;&#23613;&#31649;&#27809;&#26377;&#38024;&#23545;&#30446;&#26631;&#31867;&#21035;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;HyperSim&#12289;ScanNet&#21644;Replica&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#38381;&#38598;3D&#31995;&#32479;&#30456;&#20284;&#30340;&#20840;&#26223;&#20998;&#21106;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#35821;&#20041;&#20998;&#21106;&#26041;&#38754;&#20248;&#20110;&#24403;&#21069;&#30340;3D&#24320;&#25918;&#35789;&#27719;&#31995;&#32479;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#26041;&#27861;&#30340;&#32452;&#25104;&#37096;&#20998;&#36827;&#34892;&#20102;&#23454;&#39564;&#26469;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, methods have been proposed for 3D open-vocabulary semantic segmentation. Such methods are able to segment scenes into arbitrary classes based on text descriptions provided during runtime. In this paper, we propose to the best of our knowledge the first algorithm for open-vocabulary panoptic segmentation in 3D scenes. Our algorithm, Panoptic Vision-Language Feature Fields (PVLFF), learns a semantic feature field of the scene by distilling vision-language features from a pretrained 2D model, and jointly fits an instance feature field through contrastive learning using 2D instance segments on input frames. Despite not being trained on the target classes, our method achieves panoptic segmentation performance similar to the state-of-the-art closed-set 3D systems on the HyperSim, ScanNet and Replica dataset and additionally outperforms current 3D open-vocabulary systems in terms of semantic segmentation. We ablate the components of our method to demonstrate the effectiveness of our
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#22522;&#20110;&#25552;&#31034;&#30340;&#34892;&#21160;&#36716;&#25442;&#65292;&#35299;&#20915;&#20102;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20219;&#21153;&#20013;&#20174;&#20223;&#30495;&#21040;&#30495;&#23454;&#30340;&#36801;&#31227;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.14284</link><description>&lt;p&gt;
LLM&#24378;&#21270;&#20102;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#30340;&#20174;&#20223;&#30495;&#21040;&#30495;&#23454;&#30340;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
LLM Powered Sim-to-real Transfer for Traffic Signal Control. (arXiv:2308.14284v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#22522;&#20110;&#25552;&#31034;&#30340;&#34892;&#21160;&#36716;&#25442;&#65292;&#35299;&#20915;&#20102;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20219;&#21153;&#20013;&#20174;&#20223;&#30495;&#21040;&#30495;&#23454;&#30340;&#36801;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24050;&#32463;&#26377;&#24456;&#22810;&#35299;&#20915;&#26041;&#26696;&#29992;&#20110;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#65288;TSC&#65289;&#20219;&#21153;&#65292;&#26088;&#22312;&#25552;&#20379;&#39640;&#25928;&#30340;&#20132;&#36890;&#21644;&#20943;&#36731;&#25317;&#22581;&#28010;&#36153;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#24615;&#33021;&#24046;&#36317;&#65292;&#24403;&#22312;&#20223;&#30495;&#22120;&#20013;&#35757;&#32451;&#30340;&#31574;&#30053;&#37096;&#32626;&#21040;&#29616;&#23454;&#19990;&#30028;&#26102;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#22522;&#20110;&#25552;&#31034;&#30340;&#25166;&#26681;&#34892;&#21160;&#36716;&#25442;&#65292;&#26469;&#29702;&#35299;&#21644;&#25551;&#36848;&#31995;&#32479;&#21160;&#24577;&#12290;&#36890;&#36807;&#25509;&#21463;&#22635;&#31354;&#25552;&#31034;&#27169;&#26495;&#65292;&#24182;&#26681;&#25454;&#21487;&#20197;&#35775;&#38382;&#30340;&#19978;&#19979;&#25991;&#22635;&#20889;&#31572;&#26696;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24212;&#29992;&#20110;&#23545;&#31995;&#32479;&#21160;&#24577;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous solutions are proposed for the Traffic Signal Control (TSC) tasks aiming to provide efficient transportation and mitigate congestion waste. In recent, promising results have been attained by Reinforcement Learning (RL) methods through trial and error in simulators, bringing confidence in solving cities' congestion headaches. However, there still exist performance gaps when simulator-trained policies are deployed to the real world. This issue is mainly introduced by the system dynamic difference between the training simulator and the real-world environments. The Large Language Models (LLMs) are trained on mass knowledge and proved to be equipped with astonishing inference abilities. In this work, we leverage LLMs to understand and profile the system dynamics by a prompt-based grounded action transformation. Accepting the cloze prompt template, and then filling in the answer based on accessible context, the pre-trained LLM's inference ability is exploited and applied to understa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#26234;&#33021;&#25163;&#26426;&#25293;&#25668;&#30340;&#22270;&#20687;&#65292;&#36890;&#36807;&#21457;&#23637;&#19968;&#20010;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#25104;&#21151;&#39044;&#27979;&#20102;&#29305;&#23450;&#20301;&#32622;&#30340;PM2.5&#27987;&#24230;&#65292;&#25581;&#31034;&#20102;&#26412;&#22320;&#32858;&#21512;&#30340;&#31354;&#27668;&#36136;&#37327;&#25351;&#25968;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.03200</link><description>&lt;p&gt;
&#21033;&#29992;&#39640;&#25928;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20174;&#26234;&#33021;&#25163;&#26426;&#25293;&#25668;&#22270;&#20687;&#20013;&#25581;&#31034;&#26412;&#22320;&#32858;&#21512;&#30340;&#31354;&#27668;&#36136;&#37327;&#25351;&#25968;
&lt;/p&gt;
&lt;p&gt;
Uncovering local aggregated air quality index with smartphone captured images leveraging efficient deep convolutional neural network. (arXiv:2308.03200v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#26234;&#33021;&#25163;&#26426;&#25293;&#25668;&#30340;&#22270;&#20687;&#65292;&#36890;&#36807;&#21457;&#23637;&#19968;&#20010;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#25104;&#21151;&#39044;&#27979;&#20102;&#29305;&#23450;&#20301;&#32622;&#30340;PM2.5&#27987;&#24230;&#65292;&#25581;&#31034;&#20102;&#26412;&#22320;&#32858;&#21512;&#30340;&#31354;&#27668;&#36136;&#37327;&#25351;&#25968;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#25163;&#26426;&#30340;&#26222;&#21450;&#21644;&#21487;&#31227;&#21160;&#24615;&#20351;&#20854;&#25104;&#20026;&#29615;&#22659;&#20581;&#24247;&#30740;&#31350;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25991;&#29486;&#20013;&#23545;&#22522;&#20110;&#29305;&#23450;&#20301;&#32622;PM2.5&#27987;&#24230;&#30830;&#23450;&#32858;&#21512;&#31354;&#27668;&#36136;&#37327;&#25351;&#25968;&#65288;AQI&#65289;&#30340;&#28508;&#21147;&#30340;&#30740;&#31350;&#20173;&#28982;&#36739;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#20351;&#29992;&#26234;&#33021;&#25163;&#26426;&#30456;&#26426;&#25293;&#25668;&#30340;&#22270;&#20687;&#26469;&#39044;&#27979;&#29305;&#23450;&#20301;&#32622;PM2.5&#27987;&#24230;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;&#23391;&#21152;&#25289;&#22269;&#39318;&#37117;&#36798;&#21345;&#65292;&#22240;&#20854;&#20005;&#37325;&#30340;&#31354;&#27668;&#27745;&#26579;&#27700;&#24179;&#21644;&#22823;&#37327;&#26292;&#38706;&#20110;&#20854;&#20013;&#30340;&#20154;&#21475;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#28041;&#21450;&#24320;&#21457;&#19968;&#20010;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;DCNN&#65289;&#65292;&#25105;&#20204;&#20351;&#29992;&#36229;&#36807;&#19968;&#21315;&#24352;&#22312;&#36798;&#21345;&#19981;&#21516;&#22320;&#28857;&#25293;&#25668;&#21644;&#26631;&#27880;&#30340;&#23460;&#22806;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#20123;&#29031;&#29255;&#30340;&#26631;&#31614;&#22522;&#20110;&#20174;&#24403;&#22320;&#32654;&#22269;&#39046;&#20107;&#39302;&#33719;&#21462;&#30340;PM2.5&#27987;&#24230;&#25968;&#25454;&#65292;&#20351;&#29992;NowCast&#31639;&#27861;&#35745;&#31639;&#24471;&#21040;&#12290;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#24314;&#31435;&#20102;&#19968;&#20010;c
&lt;/p&gt;
&lt;p&gt;
The prevalence and mobility of smartphones make these a widely used tool for environmental health research. However, their potential for determining aggregated air quality index (AQI) based on PM2.5 concentration in specific locations remains largely unexplored in the existing literature. In this paper, we thoroughly examine the challenges associated with predicting location-specific PM2.5 concentration using images taken with smartphone cameras. The focus of our study is on Dhaka, the capital of Bangladesh, due to its significant air pollution levels and the large population exposed to it. Our research involves the development of a Deep Convolutional Neural Network (DCNN), which we train using over a thousand outdoor images taken and annotated. These photos are captured at various locations in Dhaka, and their labels are based on PM2.5 concentration data obtained from the local US consulate, calculated using the NowCast algorithm. Through supervised learning, our model establishes a c
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26354;&#29575;&#30340;&#21464;&#21387;&#22120;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#31163;&#25955;&#21270;&#30340; Ricci &#26354;&#29575;&#65292;&#25913;&#36827;&#20102;&#22270;&#21464;&#21387;&#22120;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#20998;&#23376;&#22270;&#25968;&#25454;&#19978;&#25552;&#21462;&#32467;&#26500;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#65292;&#24182;&#26377;&#25193;&#23637;&#21040;&#20854;&#20182;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.13275</link><description>&lt;p&gt;
&#22522;&#20110;&#26354;&#29575;&#30340;&#21464;&#21387;&#22120;&#29992;&#20110;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Curvature-based Transformer for Molecular Property Prediction. (arXiv:2307.13275v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13275
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26354;&#29575;&#30340;&#21464;&#21387;&#22120;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#31163;&#25955;&#21270;&#30340; Ricci &#26354;&#29575;&#65292;&#25913;&#36827;&#20102;&#22270;&#21464;&#21387;&#22120;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#20998;&#23376;&#22270;&#25968;&#25454;&#19978;&#25552;&#21462;&#32467;&#26500;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#65292;&#24182;&#26377;&#25193;&#23637;&#21040;&#20854;&#20182;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#24615;&#36136;&#30340;&#39044;&#27979;&#26159;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#33647;&#29289;&#35774;&#35745;&#39046;&#22495;&#20013;&#26368;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#22312;&#24403;&#21069;&#20027;&#27969;&#30340;&#26041;&#27861;&#20013;&#65292;&#29992;&#20110;&#35757;&#32451;DNN&#27169;&#22411;&#30340;&#26368;&#24120;&#29992;&#29305;&#24449;&#34920;&#31034;&#22522;&#20110;SMILES&#21644;&#20998;&#23376;&#22270;&#65292;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#31616;&#27905;&#39640;&#25928;&#65292;&#20294;&#20063;&#38480;&#21046;&#20102;&#23545;&#31354;&#38388;&#20449;&#24687;&#30340;&#25429;&#25417;&#33021;&#21147;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#26354;&#29575;&#30340;&#21464;&#21387;&#22120;&#65292;&#36890;&#36807;&#24341;&#20837; Ricci &#26354;&#29575;&#31163;&#25955;&#21270;&#65292;&#25913;&#36827;&#20102;&#22270;&#21464;&#21387;&#22120;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#20998;&#23376;&#22270;&#25968;&#25454;&#19978;&#25552;&#21462;&#32467;&#26500;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#23558;&#26354;&#29575;&#23884;&#20837;&#27169;&#22411;&#20013;&#65292;&#22312;&#27880;&#24847;&#21147;&#24471;&#20998;&#35745;&#31639;&#26399;&#38388;&#65292;&#25105;&#20204;&#23558;&#22270;&#30340;&#26354;&#29575;&#20449;&#24687;&#20316;&#20026;&#20301;&#32622;&#32534;&#30721;&#28155;&#21152;&#21040;&#33410;&#28857;&#29305;&#24449;&#20013;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#25913;&#21464;&#21407;&#22987;&#32593;&#32476;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#26354;&#29575;&#20449;&#24687;&#24341;&#20837;&#22270;&#25968;&#25454;&#65292;&#24182;&#19988;&#26377;&#28508;&#21147;&#25193;&#23637;&#21040;&#20854;&#20182;&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prediction of molecular properties is one of the most important and challenging tasks in the field of artificial intelligence-based drug design. Among the current mainstream methods, the most commonly used feature representation for training DNN models is based on SMILES and molecular graphs, although these methods are concise and effective, they also limit the ability to capture spatial information. In this work, we propose Curvature-based Transformer to improve the ability of Graph Transformer neural network models to extract structural information on molecular graph data by introducing Discretization of Ricci Curvature. To embed the curvature in the model, we add the curvature information of the graph as positional Encoding to the node features during the attention-score calculation. This method can introduce curvature information from graph data without changing the original network architecture, and it has the potential to be extended to other models. We performed experiments 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;LoRA&#32452;&#21512;&#22312;&#36328;&#20219;&#21153;&#36890;&#29992;&#24615;&#19978;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;LoraHub&#26694;&#26550;&#65292;&#33021;&#22815;&#36890;&#36807;&#32452;&#21512;&#19981;&#21516;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;LoRA&#27169;&#22359;&#65292;&#23454;&#29616;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#21487;&#36866;&#24212;&#24615;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LoraHub&#22312;&#23569;&#26679;&#26412;&#22330;&#26223;&#20013;&#33021;&#22815;&#26377;&#25928;&#27169;&#25311;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#19978;&#19979;&#25991;&#31034;&#20363;&#12290;</title><link>http://arxiv.org/abs/2307.13269</link><description>&lt;p&gt;
LoraHub: &#36890;&#36807;&#21160;&#24577;LoRA&#32452;&#21512;&#23454;&#29616;&#39640;&#25928;&#30340;&#20219;&#21153;&#36890;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition. (arXiv:2307.13269v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13269
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;LoRA&#32452;&#21512;&#22312;&#36328;&#20219;&#21153;&#36890;&#29992;&#24615;&#19978;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;LoraHub&#26694;&#26550;&#65292;&#33021;&#22815;&#36890;&#36807;&#32452;&#21512;&#19981;&#21516;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;LoRA&#27169;&#22359;&#65292;&#23454;&#29616;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#21487;&#36866;&#24212;&#24615;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LoraHub&#22312;&#23569;&#26679;&#26412;&#22330;&#26223;&#20013;&#33021;&#22815;&#26377;&#25928;&#27169;&#25311;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#19978;&#19979;&#25991;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#24120;&#24120;&#34987;&#29992;&#20110;&#23545;&#26032;&#20219;&#21153;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24494;&#35843;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;LoRA&#32452;&#21512;&#22312;&#36328;&#20219;&#21153;&#36890;&#29992;&#24615;&#19978;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#20171;&#32461;&#20102;LoraHub&#65292;&#36825;&#26159;&#19968;&#20010;&#20026;&#30446;&#30340;&#24615;&#32452;&#35013;&#22312;&#19981;&#21516;&#32473;&#23450;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;LoRA&#27169;&#22359;&#30340;&#25112;&#30053;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#21487;&#36866;&#24212;&#24615;&#24615;&#33021;&#12290;&#20165;&#20973;&#20511;&#26469;&#33258;&#26032;&#20219;&#21153;&#30340;&#20960;&#20010;&#31034;&#20363;&#65292;LoraHub&#21487;&#20197;&#28789;&#27963;&#22320;&#32452;&#21512;&#22810;&#20010;LoRA&#27169;&#22359;&#65292;&#28040;&#38500;&#20102;&#23545;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#30340;&#38656;&#27714;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#31181;&#32452;&#21512;&#26082;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#27169;&#22411;&#21442;&#25968;&#65292;&#20063;&#19981;&#38656;&#35201;&#26799;&#24230;&#12290;&#25105;&#20204;&#20174;Big-Bench Hard&#65288;BBH&#65289;&#22522;&#20934;&#27979;&#35797;&#20013;&#24471;&#20986;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;LoraHub&#22312;&#23569;&#26679;&#26412;&#22330;&#26223;&#20013;&#21487;&#20197;&#26377;&#25928;&#22320;&#27169;&#25311;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#22312;&#27599;&#20010;&#25512;&#29702;&#36755;&#20837;&#26049;&#36793;&#19981;&#38656;&#35201;&#19978;&#19979;&#25991;&#31034;&#20363;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#30340;&#19968;&#20010;&#37325;&#35201;&#36129;&#29486;&#26159;&#22521;&#32946;&#19968;&#20010;LoRA&#31038;&#21306;&#65292;&#29992;&#25143;&#21487;&#20197;&#22312;&#20854;&#20013;&#20998;&#20139;&#20182;&#20204;&#35757;&#32451;&#30340;LoRA&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-rank adaptations (LoRA) are often employed to fine-tune large language models (LLMs) for new tasks. This paper investigates LoRA composability for cross-task generalization and introduces LoraHub, a strategic framework devised for the purposive assembly of LoRA modules trained on diverse given tasks, with the objective of achieving adaptable performance on unseen tasks. With just a few examples from a novel task, LoraHub enables the fluid combination of multiple LoRA modules, eradicating the need for human expertise. Notably, the composition requires neither additional model parameters nor gradients. Our empirical results, derived from the Big-Bench Hard (BBH) benchmark, suggest that LoraHub can effectively mimic the performance of in-context learning in few-shot scenarios, excluding the necessity of in-context examples alongside each inference input. A significant contribution of our research is the fostering of a community for LoRA, where users can share their trained LoRA module
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#22270;&#35770;&#32422;&#26463;&#30340;&#32972;&#21253;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#24182;&#25552;&#20986;&#20102;&#36817;&#20284;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.12547</link><description>&lt;p&gt;
&#32972;&#21253;&#38382;&#39064;&#65306;&#36830;&#36890;&#24615;&#12289;&#36335;&#24452;&#21644;&#26368;&#30701;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
Knapsack: Connectedness, Path, and Shortest-Path. (arXiv:2307.12547v2 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12547
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#22270;&#35770;&#32422;&#26463;&#30340;&#32972;&#21253;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#24182;&#25552;&#20986;&#20102;&#36817;&#20284;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#24102;&#26377;&#22270;&#35770;&#32422;&#26463;&#30340;&#32972;&#21253;&#38382;&#39064;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#25105;&#20204;&#20551;&#35774;&#32972;&#21253;&#39033;&#30446;&#38598;&#19978;&#23384;&#22312;&#19968;&#20010;&#22270;&#32467;&#26500;&#65292;&#24182;&#19988;&#35299;&#20915;&#26041;&#26696;&#36824;&#38656;&#35201;&#28385;&#36275;&#32972;&#21253;&#32422;&#26463;&#20043;&#19978;&#30340;&#26576;&#20123;&#22270;&#35770;&#24615;&#36136;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#36830;&#36890;&#32972;&#21253;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#38656;&#35201;&#35745;&#31639;&#19968;&#20010;&#36830;&#36890;&#30340;&#39033;&#30446;&#23376;&#38598;&#65292;&#20854;&#20215;&#20540;&#26368;&#22823;&#65292;&#19988;&#28385;&#36275;&#32972;&#21253;&#32422;&#26463;&#30340;&#22823;&#23567;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#21363;&#20351;&#23545;&#20110;&#26368;&#22823;&#24230;&#20026;&#22235;&#30340;&#22270;&#65292;&#36825;&#20010;&#38382;&#39064;&#20063;&#26159;&#24378;NP&#23436;&#20840;&#30340;&#65292;&#23545;&#20110;&#26143;&#24418;&#22270;&#65292;&#36825;&#20010;&#38382;&#39064;&#20063;&#26159;NP&#23436;&#20840;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#36816;&#34892;&#26102;&#38388;&#20026;$O\left(2^{tw\log tw}\cdot\text{poly}(\min\{s^2,d^2\})\right)$&#30340;&#31639;&#27861;&#65292;&#22312;&#20854;&#20013;$tw,s,d$&#20998;&#21035;&#26159;&#22270;&#30340;&#26641;&#23485;&#24230;&#65292;&#32972;&#21253;&#30340;&#22823;&#23567;&#21644;&#30446;&#26631;&#20540;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#19968;&#20010;$(1-\epsilon)$&#36817;&#20284;&#31639;&#27861;&#65292;&#20854;&#36816;&#34892;&#26102;&#38388;&#20026;$O\left(2^{tw\log tw}\cdot\text{poly}(n,1/\epsilon)\right)$&#65292;&#23545;&#20110;&#27599;&#20010;$\epsilon&gt;0$&#37117;&#25104;&#31435;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#23545;&#20960;&#20010;&#20854;&#20182;&#22270;&#35770;&#24615;&#36136;&#30340;&#31867;&#20284;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the knapsack problem with graph theoretic constraints. That is, we assume that there exists a graph structure on the set of items of knapsack and the solution also needs to satisfy certain graph theoretic properties on top of knapsack constraints. In particular, we need to compute in the connected knapsack problem a connected subset of items which has maximum value subject to the size of knapsack constraint. We show that this problem is strongly NP-complete even for graphs of maximum degree four and NP-complete even for star graphs. On the other hand, we develop an algorithm running in time $O\left(2^{tw\log tw}\cdot\text{poly}(\min\{s^2,d^2\})\right)$ where $tw,s,d$ are respectively treewidth of the graph, size, and target value of the knapsack. We further exhibit a $(1-\epsilon)$ factor approximation algorithm running in time $O\left(2^{tw\log tw}\cdot\text{poly}(n,1/\epsilon)\right)$ for every $\epsilon&gt;0$. We show similar results for several other graph theoretic propertie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#24320;&#25918;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;&#30340;&#25216;&#26415;&#21644;&#27861;&#24459;&#35266;&#23519;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#26597;&#35810;&#21644;&#22522;&#20110;&#21512;&#21516;&#30340;&#20004;&#31181;&#36866;&#29992;&#20110;&#24320;&#25918;&#32852;&#37030;&#23398;&#20064;&#30340;&#21512;&#20316;&#26694;&#26550;&#65292;&#24182;&#23545;&#26500;&#24314;&#24320;&#25918;&#30340;FL&#24179;&#21488;&#30340;&#21487;&#34892;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2307.02140</link><description>&lt;p&gt;
&#24320;&#25918;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;&#65306;&#25216;&#26415;&#21644;&#27861;&#24459;&#35266;&#23519;&#30340;&#32508;&#36848;&#21644;&#24895;&#26223;
&lt;/p&gt;
&lt;p&gt;
Towards Open Federated Learning Platforms: Survey and Vision from Technical and Legal Perspectives. (arXiv:2307.02140v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#24320;&#25918;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;&#30340;&#25216;&#26415;&#21644;&#27861;&#24459;&#35266;&#23519;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#26597;&#35810;&#21644;&#22522;&#20110;&#21512;&#21516;&#30340;&#20004;&#31181;&#36866;&#29992;&#20110;&#24320;&#25918;&#32852;&#37030;&#23398;&#20064;&#30340;&#21512;&#20316;&#26694;&#26550;&#65292;&#24182;&#23545;&#26500;&#24314;&#24320;&#25918;&#30340;FL&#24179;&#21488;&#30340;&#21487;&#34892;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#36981;&#24490;&#26381;&#21153;&#22120;&#20027;&#23548;&#30340;&#21512;&#20316;&#27169;&#24335;&#65292;&#38480;&#21046;&#20102;FL&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#24182;&#38477;&#20302;&#20102;&#25968;&#25454;&#25345;&#26377;&#32773;&#21442;&#19982;&#30340;&#28909;&#24773;&#12290;&#20026;&#20102;&#20805;&#20998;&#37322;&#25918;FL&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#20027;&#24352;&#37325;&#26032;&#24605;&#32771;&#24403;&#21069;FL&#26694;&#26550;&#30340;&#35774;&#35745;&#65292;&#24182;&#23558;&#20854;&#25193;&#23637;&#20026;&#26356;&#36890;&#29992;&#30340;&#27010;&#24565;&#65306;&#24320;&#25918;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#30456;&#20114;&#21512;&#20316;&#30340;FL&#26694;&#26550;&#65306;&#22522;&#20110;&#26597;&#35810;&#30340;FL&#21644;&#22522;&#20110;&#21512;&#21516;&#30340;FL&#12290;&#22312;&#36825;&#20010;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#20174;&#25216;&#26415;&#21644;&#27861;&#24459;&#30340;&#35282;&#24230;&#23545;&#26500;&#24314;&#24320;&#25918;&#30340;FL&#24179;&#21488;&#30340;&#21487;&#34892;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#39318;&#20808;&#22238;&#39038;&#20102;FL&#30340;&#23450;&#20041;&#65292;&#24182;&#24635;&#32467;&#20102;&#20854;&#22266;&#26377;&#30340;&#23616;&#38480;&#24615;&#65292;&#21253;&#25324;&#26381;&#21153;&#22120;-&#23458;&#25143;&#31471;&#32806;&#21512;&#12289;&#27169;&#22411;&#21487;&#37325;&#29992;&#24615;&#20302;&#21644;&#38750;&#20844;&#24320;&#24615;&#12290;&#22312;&#22522;&#20110;&#26597;&#35810;&#30340;FL&#24179;&#21488;&#20013;&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#31038;&#21306;&#36171;&#33021;&#30340;&#24320;&#25918;&#27169;&#22411;&#20849;&#20139;&#21644;&#37325;&#29992;&#24179;&#21488;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31995;&#21015;&#26377;&#20215;&#20540;&#30340;&#20027;&#39064;&#65292;&#21253;&#25324;&#20840;&#29699;&#26368;&#26032;&#21487;&#29992;&#27169;&#22411;&#21644;&#27169;&#22411;&#30340;&#26597;&#35810;&#12289;&#26381;&#21153;&#36136;&#37327;&#20445;&#35777;&#21644;&#22870;&#21169;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional Federated Learning (FL) follows a server-domincated cooperation paradigm which narrows the application scenarios of FL and decreases the enthusiasm of data holders to participate. To fully unleash the potential of FL, we advocate rethinking the design of current FL frameworks and extending it to a more generalized concept: Open Federated Learning Platforms. We propose two reciprocal cooperation frameworks for FL to achieve this: query-based FL and contract-based FL. In this survey, we conduct a comprehensive review of the feasibility of constructing an open FL platform from both technical and legal perspectives. We begin by reviewing the definition of FL and summarizing its inherent limitations, including server-client coupling, low model reusability, and non-public. In the query-based FL platform, which is an open model sharing and reusing platform empowered by the community for model mining, we explore a wide range of valuable topics, including the availability of up-to-d
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#27010;&#29575;&#35821;&#20041;&#26469;&#26597;&#35810;&#19981;&#19968;&#33268;&#30340;&#25551;&#36848;&#36923;&#36753;&#30693;&#35782;&#24211;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.09138</link><description>&lt;p&gt;
&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#26597;&#35810;&#19981;&#19968;&#33268;&#30340;&#25551;&#36848;&#36923;&#36753;&#30693;&#35782;&#24211;
&lt;/p&gt;
&lt;p&gt;
Exploiting Uncertainty for Querying Inconsistent Description Logics Knowledge Bases. (arXiv:2306.09138v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09138
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#27010;&#29575;&#35821;&#20041;&#26469;&#26597;&#35810;&#19981;&#19968;&#33268;&#30340;&#25551;&#36848;&#36923;&#36753;&#30693;&#35782;&#24211;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#20041;Web&#30340;&#37325;&#35201;&#24615;&#26085;&#30410;&#22686;&#21152;&#65292;&#31649;&#29702;&#25551;&#36848;&#36923;&#36753;&#30693;&#35782;&#24211;&#65288;KBs&#65289;&#20013;&#30340;&#19981;&#19968;&#33268;&#24615;&#21464;&#24471;&#36234;&#26469;&#36234;&#24517;&#35201;&#12290;&#36825;&#20123;&#30693;&#35782;&#24211;&#21253;&#21547;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#20449;&#24687;&#65292;&#20854;&#20869;&#23481;&#32463;&#24120;&#21457;&#29983;&#21464;&#21270;&#65292;&#24182;&#19988;&#22312;&#21333;&#29420;&#32771;&#34385;&#25110;&#32508;&#21512;&#32771;&#34385;&#26102;&#21487;&#33021;&#21253;&#21547;&#30456;&#20114;&#30683;&#30462;&#30340;&#25551;&#36848;&#12290;&#20256;&#32479;&#30340;&#25512;&#29702;&#31639;&#27861;&#26080;&#27861;&#22788;&#29702;&#19981;&#19968;&#33268;&#30340;&#30693;&#35782;&#24211;&#65292;&#38656;&#35201;&#36890;&#36807;&#35843;&#35797;&#30693;&#35782;&#24211;&#26469;&#28040;&#38500;&#19981;&#19968;&#33268;&#24615;&#12290;&#26412;&#25991;&#21033;&#29992;&#19968;&#31181;&#31216;&#20026;DISPONTE&#30340;&#29616;&#26377;&#27010;&#29575;&#35821;&#20041;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#20801;&#35768;&#22312;&#19981;&#19968;&#33268;&#30340;&#30693;&#35782;&#24211;&#20013;&#36827;&#34892;&#26597;&#35810;&#12290;&#25105;&#20204;&#22312;TRILL&#21644;BUNDLE&#25512;&#29702;&#22120;&#20013;&#23454;&#29616;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#25552;&#26696;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#20462;&#22797;&#35821;&#20041;&#36827;&#34892;&#20102;&#27491;&#24335;&#27604;&#36739;&#65292;&#21518;&#32773;&#26159;&#22312;&#32771;&#34385;DL&#25512;&#29702;&#20219;&#21153;&#26102;&#26368;&#20026;&#25104;&#29087;&#30340;&#35821;&#20041;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The necessity to manage inconsistency in Description Logics Knowledge Bases~(KBs) has come to the fore with the increasing importance gained by the Semantic Web, where information comes from different sources that constantly change their content and may contain contradictory descriptions when considered either alone or together. Classical reasoning algorithms do not handle inconsistent KBs, forcing the debugging of the KB in order to remove the inconsistency. In this paper, we exploit an existing probabilistic semantics called DISPONTE to overcome this problem and allow queries also in case of inconsistent KBs. We implemented our approach in the reasoners TRILL and BUNDLE and empirically tested the validity of our proposal. Moreover, we formally compare the presented approach to that of the repair semantics, one of the most established semantics when considering DL reasoning tasks.
&lt;/p&gt;</description></item><item><title>&#25919;&#27835;&#36777;&#35770;&#12289;&#28436;&#35762;&#21644;&#35775;&#35848;&#20013;&#30340;&#20540;&#24471;&#26680;&#23454;&#30340;&#35770;&#26029;&#21487;&#20197;&#20351;&#29992;&#38899;&#39057;&#25968;&#25454;&#36827;&#34892;&#26816;&#27979;&#21644;&#30830;&#35748;&#65292;&#36825;&#21487;&#24110;&#21161;&#20027;&#25345;&#20154;&#12289;&#35760;&#32773;&#21644;&#20107;&#23454;&#26680;&#26597;&#32452;&#32455;&#36827;&#34892;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2306.05535</link><description>&lt;p&gt;
&#20351;&#29992;&#38899;&#39057;&#25968;&#25454;&#26816;&#27979;&#25919;&#27835;&#36777;&#35770;&#12289;&#28436;&#35762;&#21644;&#35775;&#35848;&#20013;&#20540;&#24471;&#26680;&#23454;&#30340;&#35770;&#26029;
&lt;/p&gt;
&lt;p&gt;
Detecting Check-Worthy Claims in Political Debates, Speeches, and Interviews Using Audio Data. (arXiv:2306.05535v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05535
&lt;/p&gt;
&lt;p&gt;
&#25919;&#27835;&#36777;&#35770;&#12289;&#28436;&#35762;&#21644;&#35775;&#35848;&#20013;&#30340;&#20540;&#24471;&#26680;&#23454;&#30340;&#35770;&#26029;&#21487;&#20197;&#20351;&#29992;&#38899;&#39057;&#25968;&#25454;&#36827;&#34892;&#26816;&#27979;&#21644;&#30830;&#35748;&#65292;&#36825;&#21487;&#24110;&#21161;&#20027;&#25345;&#20154;&#12289;&#35760;&#32773;&#21644;&#20107;&#23454;&#26680;&#26597;&#32452;&#32455;&#36827;&#34892;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#30340;&#19968;&#22823;&#37096;&#20998;&#22242;&#32467;&#22312;&#30456;&#21516;&#30340;&#24895;&#26223;&#21644;&#24605;&#24819;&#21608;&#22260;&#65292;&#20855;&#26377;&#24040;&#22823;&#30340;&#33021;&#37327;&#12290;&#36825;&#27491;&#26159;&#25919;&#27835;&#20154;&#29289;&#24076;&#26395;&#20026;&#20182;&#20204;&#30340;&#20107;&#19994;&#25152;&#32047;&#31215;&#30340;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#30446;&#26631;&#65292;&#20182;&#20204;&#26377;&#26102;&#20250;&#20351;&#29992;&#25197;&#26354;&#25110;&#38544;&#34255;&#30495;&#30456;&#30340;&#25163;&#27573;&#65292;&#26080;&#35770;&#26159;&#26080;&#24847;&#30340;&#36824;&#26159;&#26377;&#24847;&#30340;&#65292;&#36825;&#20026;&#38169;&#35823;&#20449;&#24687;&#21644;&#35823;&#23548;&#24320;&#20102;&#22823;&#38376;&#12290;&#33258;&#21160;&#26816;&#27979;&#20540;&#24471;&#26680;&#23454;&#30340;&#35770;&#26029;&#30340;&#24037;&#20855;&#23558;&#23545;&#36777;&#35770;&#20027;&#25345;&#20154;&#12289;&#35760;&#32773;&#21644;&#20107;&#23454;&#26680;&#26597;&#32452;&#32455;&#26377;&#24456;&#22823;&#24110;&#21161;&#12290;&#34429;&#28982;&#20197;&#21069;&#20851;&#20110;&#26816;&#27979;&#20540;&#24471;&#26680;&#23454;&#30340;&#35770;&#26029;&#30340;&#24037;&#20316;&#37325;&#28857;&#26159;&#25991;&#26412;&#65292;&#20294;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#38899;&#39057;&#20449;&#21495;&#20316;&#20026;&#39069;&#22806;&#20449;&#24687;&#28304;&#30340;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65288;&#33521;&#35821;&#25991;&#26412;&#21644;&#38899;&#39057;&#65289;&#65292;&#21253;&#21547;48&#23567;&#26102;&#30340;&#28436;&#35762;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22810;&#20010;&#28436;&#35762;&#32773;&#30340;&#24773;&#20917;&#19979;&#65292;&#38899;&#39057;&#27169;&#24577;&#19982;&#25991;&#26412;&#32467;&#21512;&#20351;&#29992;&#27604;&#20165;&#20351;&#29992;&#25991;&#26412;&#20855;&#26377;&#25913;&#36827;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#21333;&#22768;&#36947;&#38899;&#39057;&#27169;&#22411;&#21487;&#20197;&#32988;&#36807;&#21333;&#22768;&#36947;&#25991;&#26412;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
A large portion of society united around the same vision and ideas carries enormous energy. That is precisely what political figures would like to accumulate for their cause. With this goal in mind, they can sometimes resort to distorting or hiding the truth, unintentionally or on purpose, which opens the door for misinformation and disinformation. Tools for automatic detection of check-worthy claims would be of great help to moderators of debates, journalists, and fact-checking organizations. While previous work on detecting check-worthy claims has focused on text, here we explore the utility of the audio signal as an additional information source. We create a new multimodal dataset (text and audio in English) containing 48 hours of speech. Our evaluation results show that the audio modality together with text yields improvements over text alone in the case of multiple speakers. Moreover, an audio-only model could outperform a text-only one for a single speaker.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20223;&#30495;&#30340;&#21453;&#20107;&#23454;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#23450;&#20041;&#38382;&#39064;&#21644;&#20351;&#29992;&#21453;&#20107;&#23454;&#20223;&#30495;&#26469;&#35299;&#20915;&#22240;&#26524;&#20851;&#31995;&#38750;&#31283;&#24577;&#38382;&#39064;&#21644;&#24178;&#39044;&#38480;&#21046;&#65292;&#22312;&#30495;&#23454;&#39550;&#39542;&#34892;&#20026;&#20013;&#24471;&#21040;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2306.03354</link><description>&lt;p&gt;
&#22522;&#20110;&#20223;&#30495;&#30340;&#21453;&#20107;&#23454;&#22240;&#26524;&#21457;&#29616;&#19982;&#30495;&#23454;&#39550;&#39542;&#34892;&#20026;&#30340;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Simulation-Based Counterfactual Causal Discovery on Real World Driver Behaviour. (arXiv:2306.03354v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03354
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20223;&#30495;&#30340;&#21453;&#20107;&#23454;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#23450;&#20041;&#38382;&#39064;&#21644;&#20351;&#29992;&#21453;&#20107;&#23454;&#20223;&#30495;&#26469;&#35299;&#20915;&#22240;&#26524;&#20851;&#31995;&#38750;&#31283;&#24577;&#38382;&#39064;&#21644;&#24178;&#39044;&#38480;&#21046;&#65292;&#22312;&#30495;&#23454;&#39550;&#39542;&#34892;&#20026;&#20013;&#24471;&#21040;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#33258;&#24049;&#34892;&#20026;&#22914;&#20309;&#24433;&#21709;&#20182;&#20154;&#34892;&#20026;&#26159;&#39550;&#39542;&#26234;&#33021;&#20307;&#25152;&#38656;&#30340;&#26680;&#24515;&#25216;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25216;&#26415;&#26080;&#27861;&#28385;&#36275;&#26234;&#33021;&#20307;&#21457;&#29616;&#33258;&#24049;&#21644;&#20182;&#20154;&#20043;&#38388;&#22240;&#26524;&#20851;&#31995;&#30340;&#38656;&#27714;&#12290;&#35266;&#23519;&#24615;&#26041;&#27861;&#38754;&#20020;&#30528;&#21160;&#24577;&#29615;&#22659;&#23548;&#33268;&#22240;&#26524;&#20851;&#31995;&#38750;&#31283;&#24577;&#21270;&#65292;&#20197;&#21450;&#22240;&#26524;&#20132;&#20114;&#31232;&#30095;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#38656;&#35201;&#22312;&#32447;&#24037;&#20316;&#12290;&#32780;&#24178;&#39044;&#24615;&#26041;&#27861;&#21017;&#22240;&#20026;&#36710;&#36742;&#26080;&#27861;&#22312;&#20844;&#20849;&#36947;&#36335;&#19978;&#36827;&#34892;&#23454;&#39564;&#32780;&#19981;&#20999;&#23454;&#38469;&#12290;&#20026;&#20102;&#35299;&#20915;&#22240;&#26524;&#20851;&#31995;&#30340;&#38750;&#31283;&#24577;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#20107;&#20214;&#25552;&#21462;&#26041;&#38754;&#37325;&#26032;&#23450;&#20041;&#20102;&#38382;&#39064;&#65292;&#32780;&#20043;&#21069;&#25552;&#21040;&#30340;&#24178;&#39044;&#38480;&#21046;&#21487;&#20197;&#36890;&#36807;&#21453;&#20107;&#23454;&#20223;&#30495;&#26469;&#20811;&#26381;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#21464;&#20307;&#30340;&#21453;&#20107;&#23454;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#29616;&#26377;&#35266;&#23519;&#24615;&#26102;&#38388;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#22312;3396&#20010;&#22240;&#26524;&#26679;&#26412;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Being able to reason about how one's behaviour can affect the behaviour of others is a core skill required of intelligent driving agents. Despite this, the state of the art struggles to meet the need of agents to discover causal links between themselves and others. Observational approaches struggle because of the non-stationarity of causal links in dynamic environments, and the sparsity of causal interactions while requiring the approaches to work in an online fashion. Meanwhile interventional approaches are impractical as a vehicle cannot experiment with its actions on a public road. To counter the issue of non-stationarity we reformulate the problem in terms of extracted events, while the previously mentioned restriction upon interventions can be overcome with the use of counterfactual simulation. We present three variants of the proposed counterfactual causal discovery method and evaluate these against state of the art observational temporal causal discovery methods across 3396 caus
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#8220;&#24605;&#32500;&#20811;&#38534;&#8221;&#65292;&#36890;&#36807;&#23398;&#20064;&#20154;&#31867;&#30340;&#24605;&#32500;&#26469;&#35757;&#32451;AI&#20195;&#29702;&#65292;&#20197;&#22312;&#27867;&#21270;&#12289;&#25506;&#32034;&#12289;&#35268;&#21010;&#31561;&#33021;&#21147;&#26041;&#38754;&#23454;&#29616;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.00323</link><description>&lt;p&gt;
&#8220;&#24605;&#32500;&#20811;&#38534;&#65306;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#24605;&#32500;&#23398;&#20064;&#24605;&#32771;&#24182;&#34892;&#21160;&#8221;&#12290;&#65288;arXiv:2306.00323v1 [cs.AI]&#65289;
&lt;/p&gt;
&lt;p&gt;
Thought Cloning: Learning to Think while Acting by Imitating Human Thinking. (arXiv:2306.00323v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00323
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#8220;&#24605;&#32500;&#20811;&#38534;&#8221;&#65292;&#36890;&#36807;&#23398;&#20064;&#20154;&#31867;&#30340;&#24605;&#32500;&#26469;&#35757;&#32451;AI&#20195;&#29702;&#65292;&#20197;&#22312;&#27867;&#21270;&#12289;&#25506;&#32034;&#12289;&#35268;&#21010;&#31561;&#33021;&#21147;&#26041;&#38754;&#23454;&#29616;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#20154;&#31867;&#24605;&#32500;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#65292;&#23427;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#38750;&#20961;&#30340;&#27867;&#21270;&#12289;&#25506;&#32034;&#12289;&#35268;&#21010;&#12289;&#37325;&#26032;&#35268;&#21010;&#21644;&#36866;&#24212;&#26032;&#24773;&#20917;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20195;&#29702;&#22312;&#36825;&#20123;&#33021;&#21147;&#20013;&#36828;&#26410;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#20551;&#35774;&#20854;&#20013;&#19968;&#20010;&#35748;&#30693;&#32570;&#38519;&#30340;&#21407;&#22240;&#26159;&#20182;&#20204;&#32570;&#20047;&#20351;&#29992;&#35821;&#35328;&#24605;&#32771;&#25152;&#24102;&#26469;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#35748;&#20026;&#36890;&#36807;&#35757;&#32451;AI&#20195;&#29702;&#20154;&#20687;&#20154;&#31867;&#19968;&#26679;&#24605;&#32771;&#65292;&#21487;&#20197;&#25913;&#21892;&#20854;&#24615;&#33021;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#8220;&#24605;&#32500;&#20811;&#38534;&#8221;&#65292;&#20854;&#24819;&#27861;&#19981;&#20165;&#26159;&#20811;&#38534;&#20154;&#31867;&#31034;&#33539;&#32773;&#30340;&#34892;&#20026;&#65292;&#32780;&#19988;&#36824;&#21253;&#25324;&#20154;&#31867;&#22312;&#25191;&#34892;&#36825;&#20123;&#34892;&#20026;&#26102;&#25152;&#20135;&#29983;&#30340;&#24819;&#27861;&#12290;&#34429;&#28982;&#25105;&#20204;&#24076;&#26395;&#8220;&#24605;&#32500;&#20811;&#38534;&#8221;&#22312;&#22788;&#29702;&#32593;&#32476;&#35268;&#27169;&#30340;&#20154;&#31867;&#24605;&#32500;&#21644;&#34892;&#20026;&#25968;&#25454;&#26102;&#33021;&#22815;&#21457;&#25381;&#20986;&#33394;&#65288;&#20363;&#22914;&#65292;&#24102;&#26377;&#21095;&#26412;&#30340;&#22312;&#32447;&#35270;&#39057;&#65289;&#65292;&#20294;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22312;&#24605;&#32771;&#21644;&#34892;&#21160;&#25968;&#25454;&#20026;&#21512;&#25104;&#29983;&#25104;&#30340;&#39046;&#22495;&#30340;&#23454;&#39564;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#8220;&#24605;&#32500;&#20811;&#38534;&#8221;&#23398;&#20064;&#36895;&#24230;&#27604;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#24555;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language is often considered a key aspect of human thinking, providing us with exceptional abilities to generalize, explore, plan, replan, and adapt to new situations. However, Reinforcement Learning (RL) agents are far from human-level performance in any of these abilities. We hypothesize one reason for such cognitive deficiencies is that they lack the benefits of thinking in language and that we can improve AI agents by training them to think like humans do. We introduce a novel Imitation Learning framework, Thought Cloning, where the idea is to not just clone the behaviors of human demonstrators, but also the thoughts humans have as they perform these behaviors. While we expect Thought Cloning to truly shine at scale on internet-sized datasets of humans thinking out loud while acting (e.g. online videos with transcripts), here we conduct experiments in a domain where the thinking and action data are synthetically generated. Results reveal that Thought Cloning learns much faster than
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;MOMA-PPO&#65292;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#20132;&#20114;&#25968;&#25454;&#24182;&#20248;&#21270;&#26234;&#33021;&#20307;&#30340;&#25919;&#31574;&#65292;&#35299;&#20915;&#20102;&#31574;&#30053;&#19968;&#33268;&#24615;&#21644;&#31574;&#30053;&#24494;&#35843;&#20004;&#20010;&#21327;&#35843;&#38382;&#39064;&#65292;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31163;&#32447;MARL&#22330;&#26223;&#20013;&#32988;&#36807;&#20027;&#27969;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#21487;&#34892;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.17198</link><description>&lt;p&gt;
&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#21327;&#35843;&#38382;&#39064;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
A Model-Based Solution to the Offline Multi-Agent Reinforcement Learning Coordination Problem. (arXiv:2305.17198v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17198
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;MOMA-PPO&#65292;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#20132;&#20114;&#25968;&#25454;&#24182;&#20248;&#21270;&#26234;&#33021;&#20307;&#30340;&#25919;&#31574;&#65292;&#35299;&#20915;&#20102;&#31574;&#30053;&#19968;&#33268;&#24615;&#21644;&#31574;&#30053;&#24494;&#35843;&#20004;&#20010;&#21327;&#35843;&#38382;&#39064;&#65292;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31163;&#32447;MARL&#22330;&#26223;&#20013;&#32988;&#36807;&#20027;&#27969;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#21487;&#34892;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#22810;&#20010;&#26234;&#33021;&#20307;&#36827;&#34892;&#21327;&#35843;&#26159;&#19968;&#39033;&#37325;&#35201;&#38382;&#39064;&#65292;&#20855;&#26377;&#26426;&#22120;&#20154;&#25216;&#26415;&#12289;&#21338;&#24328;&#35770;&#12289;&#32463;&#27982;&#23398;&#21644;&#31038;&#20250;&#31185;&#23398;&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26159;&#22312;&#32447;&#30340;&#65292;&#22240;&#27492;&#22312;&#25910;&#38598;&#26032;&#30340;&#20132;&#20114;&#25968;&#25454;&#25104;&#26412;&#39640;&#26114;&#25110;&#21361;&#38505;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#19981;&#21487;&#34892;&#12290;&#34429;&#28982;&#36825;&#20123;&#31639;&#27861;&#24212;&#35813;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#65292;&#20294;&#36825;&#26679;&#20570;&#20250;&#24341;&#36215;&#31163;&#32447;&#21327;&#35843;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30830;&#23450;&#24182;&#24418;&#24335;&#21270;&#20102;&#31574;&#30053;&#19968;&#33268;&#24615;&#65288;SA&#65289;&#21644;&#31574;&#30053;&#24494;&#35843;&#65288;SFT&#65289;&#20004;&#20010;&#21327;&#35843;&#38382;&#39064;&#65292;&#36825;&#26159;&#24403;&#21069;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22833;&#36133;&#30340;&#21407;&#22240;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29983;&#25104;&#21512;&#25104;&#20132;&#20114;&#25968;&#25454;&#65292;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#22312;&#24494;&#35843;&#31574;&#30053;&#30340;&#21516;&#26102;&#25910;&#25947;&#20110;&#19968;&#20010;&#31574;&#30053;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;Model-based Offline Multi-Agent Proximal Policy Optimization&#65288;MOMA-PPO&#65289;&#65292;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31163;&#32447;MARL&#22330;&#26223;&#20013;&#32988;&#36807;&#20027;&#27969;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training multiple agents to coordinate is an important problem with applications in robotics, game theory, economics, and social sciences. However, most existing Multi-Agent Reinforcement Learning (MARL) methods are online and thus impractical for real-world applications in which collecting new interactions is costly or dangerous. While these algorithms should leverage offline data when available, doing so gives rise to the offline coordination problem. Specifically, we identify and formalize the strategy agreement (SA) and the strategy fine-tuning (SFT) challenges, two coordination issues at which current offline MARL algorithms fail. To address this setback, we propose a simple model-based approach that generates synthetic interaction data and enables agents to converge on a strategy while fine-tuning their policies accordingly. Our resulting method, Model-based Offline Multi-Agent Proximal Policy Optimization (MOMA-PPO), outperforms the prevalent learning methods in challenging offl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24418;&#24335;&#35821;&#27861;&#32422;&#26463;&#20016;&#23500;&#35299;&#30721;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#35821;&#27861;&#30340;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#65292;&#21516;&#26102;&#20801;&#35768;&#20219;&#20309;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;&#38598;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22235;&#20010;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.13971</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#27861;&#32422;&#26463;&#30340;&#35821;&#35328;&#27169;&#22411;&#28789;&#27963;&#35299;&#30721;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Flexible Grammar-Based Constrained Decoding for Language Models. (arXiv:2305.13971v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24418;&#24335;&#35821;&#27861;&#32422;&#26463;&#20016;&#23500;&#35299;&#30721;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#35821;&#27861;&#30340;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#65292;&#21516;&#26102;&#20801;&#35768;&#20219;&#20309;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;&#38598;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22235;&#20010;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#23569;&#37327;&#26679;&#26412;&#34920;&#29616;&#65292;&#20294;&#22312;&#29983;&#25104;&#20449;&#24687;&#25552;&#21462;&#25152;&#38656;&#30340;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#26102;&#20173;&#23384;&#22312;&#22256;&#38590;&#12290;&#36825;&#20010;&#38480;&#21046;&#28304;&#20110;LLM&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#20542;&#21521;&#20110;&#29983;&#25104;&#33258;&#30001;&#25991;&#26412;&#32780;&#19981;&#26159;&#36981;&#24490;&#29305;&#23450;&#35821;&#27861;&#30340;&#31934;&#30830;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#35299;&#30721;&#27493;&#39588;&#20013;&#20351;&#29992;&#24418;&#24335;&#35821;&#27861;&#32422;&#26463;&#26469;&#20016;&#23500;&#27169;&#22411;&#12290;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#65292;&#21482;&#26377;&#31526;&#21512;&#35821;&#27861;&#20135;&#29983;&#35268;&#21017;&#30340;&#26377;&#25928;&#20196;&#29260;&#33021;&#34987;&#32771;&#34385;&#21040;&#12290;&#36825;&#26679;&#23601;&#24378;&#21046;&#21482;&#20135;&#29983;&#26377;&#25928;&#30340;&#24207;&#21015;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#38750;&#24120;&#36890;&#29992;&#21644;&#28789;&#27963;&#65292;&#20801;&#35768;&#20219;&#20309;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;(CFG)&#38598;&#25104;&#21040;&#25105;&#20204;&#30340;&#33258;&#23450;&#20041;&#32422;&#26463;beam&#25628;&#32034;&#23454;&#29616;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35768;&#22810;NLP&#20219;&#21153;&#30340;&#36755;&#20986;&#21487;&#20197;&#34987;&#34920;&#31034;&#20026;&#24418;&#24335;&#35821;&#35328;&#65292;&#20351;&#23427;&#20204;&#36866;&#21512;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#30452;&#25509;&#20351;&#29992;&#12290;&#23545;&#20110;&#36755;&#20986;&#31354;&#38388;&#21462;&#20915;&#20110;&#36755;&#20837;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#36755;&#20837;&#30340;CFG&#65292;&#26681;&#25454;&#29305;&#23450;&#20110;&#36755;&#20837;&#30340;&#29305;&#24449;&#26356;&#26032;&#20135;&#29983;&#35268;&#21017;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#22235;&#20010;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs have shown impressive few-shot performance across many tasks. However, they still struggle when it comes to generating complex output structures, such as those required for Information Extraction. This limitation stems from the fact that LLMs, without finetuning, tend to generate free text rather than precise structures that follow a specific grammar. In this work, we propose to enrich the decoding step with formal grammar constraints. During beam search, only valid token continuations compliant with the grammar production rules are considered. This enforces the generation of valid sequences exclusively. Our framework is highly general and flexible, allowing any Context-Free Grammar (CFG) to be integrated into our custom constrained beam search implementation. We demonstrate that the outputs of many NLP tasks can be represented as formal languages, making them suitable for direct use in our framework. For task where the output space is dependent on the input, we propose input-depe
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphCare&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#20010;&#24615;&#21270;&#30693;&#35782;&#22270;&#35889;&#26469;&#25913;&#36827;&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#21307;&#30103;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.12788</link><description>&lt;p&gt;
GraphCare: &#20351;&#29992;&#20010;&#24615;&#21270;&#30693;&#35782;&#22270;&#35889;&#25552;&#21319;&#21307;&#30103;&#39044;&#27979;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
GraphCare: Enhancing Healthcare Predictions with Personalized Knowledge Graphs. (arXiv:2305.12788v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphCare&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#20010;&#24615;&#21270;&#30693;&#35782;&#22270;&#35889;&#26469;&#25913;&#36827;&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#21307;&#30103;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#39044;&#27979;&#27169;&#22411;&#36890;&#24120;&#20381;&#36182;&#20110;&#24739;&#32773;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;(EHR)&#65292;&#20294;&#23558;&#21307;&#23398;&#30693;&#35782;&#25972;&#21512;&#21040;&#39044;&#27979;&#21644;&#20915;&#31574;&#20013;&#20197;&#25552;&#39640;&#25928;&#26524;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36825;&#26159;&#22240;&#20026;&#20010;&#24615;&#21270;&#39044;&#27979;&#38656;&#35201;&#20010;&#24615;&#21270;&#30340;&#30693;&#35782;&#22270;&#35889;(KG)&#65292;&#32780;&#20174;&#24739;&#32773;EHR&#25968;&#25454;&#20013;&#29983;&#25104;&#20010;&#24615;&#21270;&#30693;&#35782;&#22270;&#35889;&#24456;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;\textsc{GraphCare}&#30340;&#24320;&#25918;&#24335;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#22270;&#35889;&#26469;&#25913;&#36827;&#22522;&#20110;EHR&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLM)&#21644;&#22806;&#37096;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#26500;&#24314;&#20010;&#20307;&#21270;&#30340;&#24739;&#32773;&#30693;&#35782;&#22270;&#35889;&#65292;&#28982;&#21518;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;Bi-attention AugmenTed (BAT)&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#36827;&#34892;&#21307;&#30103;&#39044;&#27979;&#35757;&#32451;&#12290;&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;MIMIC-III&#21644;MIMIC-IV&#19978;&#65292;\textsc{GraphCare}&#22312;&#22235;&#20010;&#20851;&#38190;&#30340;&#21307;&#30103;&#39044;&#27979;&#20219;&#21153;&#19978;&#22343;&#36229;&#36807;&#20102;&#22522;&#20934;&#32447;&#65306;&#27515;&#20129;&#29575;&#12289;&#20877;&#20837;&#38498;&#29575;&#12289;&#20303;&#38498;&#22825;&#25968;&#21644;&#33647;&#29289;&#25512;&#33616;&#12290;&#22312;MIMIC-III&#19978;&#65292;&#23427;&#23558;AUROC&#25552;&#39640;&#20102;17.6%&#21644;6.6%&#65292;&#23558;F1&#24471;&#20998;&#25552;&#39640;&#20102;7.9%&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical predictive models often rely on patients' electronic health records (EHR), but integrating medical knowledge to enhance predictions and decision-making is challenging. This is because personalized predictions require personalized knowledge graphs (KGs), which are difficult to generate from patient EHR data. To address this, we propose \textsc{GraphCare}, an open-world framework that uses external KGs to improve EHR-based predictions. Our method extracts knowledge from large language models (LLMs) and external biomedical KGs to build patient-specific KGs, which are then used to train our proposed Bi-attention AugmenTed (BAT) graph neural network (GNN) for healthcare predictions. On two public datasets, MIMIC-III and MIMIC-IV, \textsc{GraphCare} surpasses baselines in four vital healthcare prediction tasks: mortality, readmission, length of stay (LOS), and drug recommendation. On MIMIC-III, it boosts AUROC by 17.6\% and 6.6\% for mortality and readmission, and F1-score by 7.9\% 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#30340;&#30740;&#31350;&#20102;&#29616;&#26377;&#32842;&#22825;&#26426;&#22120;&#20154;&#23545;&#20110;&#25903;&#25345;&#23545;&#35805;&#24335;&#27969;&#31243;&#24314;&#27169;&#25152;&#25552;&#20379;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;&#22312;&#23454;&#36341;&#20013;&#20351;&#29992;&#32842;&#22825;&#26426;&#22120;&#20154;&#36827;&#34892;&#23545;&#35805;&#24335;&#27969;&#31243;&#24314;&#27169;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2304.11065</link><description>&lt;p&gt;
&#23545;&#35805;&#36807;&#31243;&#24314;&#27169;&#65306;&#29616;&#29366;&#12289;&#24212;&#29992;&#21644;&#23454;&#36341;&#24433;&#21709;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Conversational Process Modelling: State of the Art, Applications, and Implications in Practice. (arXiv:2304.11065v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#30340;&#30740;&#31350;&#20102;&#29616;&#26377;&#32842;&#22825;&#26426;&#22120;&#20154;&#23545;&#20110;&#25903;&#25345;&#23545;&#35805;&#24335;&#27969;&#31243;&#24314;&#27169;&#25152;&#25552;&#20379;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;&#22312;&#23454;&#36341;&#20013;&#20351;&#29992;&#32842;&#22825;&#26426;&#22120;&#20154;&#36827;&#34892;&#23545;&#35805;&#24335;&#27969;&#31243;&#24314;&#27169;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;Chatbots&#31561;&#32842;&#22825;&#26426;&#22120;&#20154;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#12290;&#23545;&#20110;BPM&#24212;&#29992;&#26469;&#35828;&#65292;&#22914;&#20309;&#24212;&#29992;&#32842;&#22825;&#26426;&#22120;&#20154;&#26469;&#29983;&#25104;&#21830;&#19994;&#20215;&#20540;&#36890;&#24120;&#26159;&#19981;&#26126;&#30830;&#30340;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#31995;&#32479;&#22320;&#20998;&#26512;&#29616;&#26377;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#23545;&#20110;&#25903;&#25345;&#23545;&#35805;&#24335;&#27969;&#31243;&#24314;&#27169;&#20316;&#20026;&#38754;&#21521;&#27969;&#31243;&#30340;&#33021;&#21147;&#30340;&#25903;&#25345;&#12290;&#35813;&#30740;&#31350;&#35782;&#21035;&#20102;&#27839;&#27969;&#31243;&#29983;&#21629;&#21608;&#26399;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#28982;&#21518;&#36827;&#34892;&#20102;&#23545;&#35805;&#24335;&#27969;&#31243;&#24314;&#27169;&#30340;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#12290;&#24471;&#20986;&#30340;&#20998;&#31867;&#23398;&#29992;&#20316;&#23545;&#35805;&#24335;&#27969;&#31243;&#24314;&#27169;&#30340;&#24212;&#29992;&#22330;&#26223;&#30340;&#35782;&#21035;&#65292;&#21253;&#25324;&#27969;&#31243;&#25551;&#36848;&#30340;&#37322;&#20041;&#21644;&#25913;&#36827;&#12290;&#24212;&#29992;&#22330;&#26223;&#22522;&#20110;&#39640;&#31561;&#25945;&#32946;&#39046;&#22495;&#30340;&#23454;&#38469;&#27979;&#35797;&#38598;&#23545;&#29616;&#26377;&#32842;&#22825;&#26426;&#22120;&#20154;&#36827;&#34892;&#35780;&#20272;&#12290;&#35813;&#27979;&#35797;&#38598;&#21253;&#21547;&#27969;&#31243;&#25551;&#36848;&#21450;&#20854;&#23545;&#24212;&#30340;&#27969;&#31243;&#27169;&#22411;&#65292;&#20197;&#21450;&#27169;&#22411;&#36136;&#37327;&#30340;&#35780;&#20272;&#12290;&#22522;&#20110;&#25991;&#29486;&#21644;&#24212;&#29992;&#22330;&#26223;&#20998;&#26512;&#65292;&#24471;&#20986;&#20102;&#20851;&#20110;&#22312;&#23545;&#35805;&#24335;&#27969;&#31243;&#24314;&#27169;&#20013;&#20351;&#29992;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chatbots such as ChatGPT have caused a tremendous hype lately. For BPM applications, it is often not clear how to apply chatbots to generate business value. Hence, this work aims at the systematic analysis of existing chatbots for their support of conversational process modelling as process-oriented capability. Application scenarios are identified along the process life cycle. Then a systematic literature review on conversational process modelling is performed. The resulting taxonomy serves as input for the identification of application scenarios for conversational process modelling, including paraphrasing and improvement of process descriptions. The application scenarios are evaluated for existing chatbots based on a real-world test set from the higher education domain. It contains process descriptions as well as corresponding process models, together with an assessment of the model quality. Based on the literature and application scenario analyses, recommendations for the usage (prac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#29983;&#25104;&#24335;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#20869;&#30340;&#35821;&#20041;&#32467;&#26500;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.09048</link><description>&lt;p&gt;
CodeKGC&#65306;&#29992;&#20110;&#29983;&#25104;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CodeKGC: Code Language Model for Generative Knowledge Graph Construction. (arXiv:2304.09048v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#29983;&#25104;&#24335;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#20869;&#30340;&#35821;&#20041;&#32467;&#26500;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#29983;&#25104;&#24335;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#26041;&#27861;&#36890;&#24120;&#26080;&#27861;&#25429;&#25417;&#32467;&#26500;&#24615;&#30693;&#35782;&#65292;&#32780;&#21482;&#26159;&#23558;&#33258;&#28982;&#35821;&#35328;&#36716;&#21270;&#20026;&#24207;&#21015;&#21270;&#25991;&#26412;&#25110;&#35268;&#33539;&#35821;&#35328;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20687;&#20195;&#30721;&#36825;&#26679;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#22823;&#22411;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#23637;&#29616;&#20102;&#22312;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#20197;&#36827;&#34892;&#32467;&#26500;&#24615;&#39044;&#27979;&#21644;&#25512;&#29702;&#20219;&#21153;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#29983;&#25104;&#24335;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#32473;&#23450;&#20195;&#30721;&#26684;&#24335;&#30340;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#65292;&#30446;&#26631;&#26159;&#29983;&#25104;&#21487;&#20197;&#34920;&#31034;&#20026;&#20195;&#30721;&#34917;&#20840;&#20219;&#21153;&#30340;&#19977;&#20803;&#32452;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#20855;&#26377;&#27169;&#24335;&#24863;&#30693;&#22411;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#20869;&#30340;&#35821;&#20041;&#32467;&#26500;&#12290;&#30001;&#20110;&#20195;&#30721;&#26412;&#36136;&#19978;&#20855;&#26377;&#32467;&#26500;&#65292;&#22914;&#31867;&#21644;&#20989;&#25968;&#23450;&#20041;&#65292;&#22240;&#27492;&#23427;&#20316;&#20026;&#20808;&#39564;&#30340;&#35821;&#20041;&#32467;&#26500;&#30693;&#35782;&#27169;&#22411;&#38750;&#24120;&#26377;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#22522;&#20110;&#21407;&#29702;&#30340;&#29983;&#25104;&#26041;&#27861;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#21407;&#29702;&#25552;&#20379;&#20102;&#27169;&#22411;&#29983;&#25104;&#32467;&#26524;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current generative knowledge graph construction approaches usually fail to capture structural knowledge by simply flattening natural language into serialized texts or a specification language. However, large generative language model trained on structured data such as code has demonstrated impressive capability in understanding natural language for structural prediction and reasoning tasks. Intuitively, we address the task of generative knowledge graph construction with code language model: given a code-format natural language input, the target is to generate triples which can be represented as code completion tasks. Specifically, we develop schema-aware prompts that effectively utilize the semantic structure within the knowledge graph. As code inherently possesses structure, such as class and function definitions, it serves as a useful model for prior semantic structural knowledge. Furthermore, we employ a rationale-enhanced generation method to boost the performance. Rationales provi
&lt;/p&gt;</description></item><item><title>NeuroBench&#26159;&#30001;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#25104;&#21592;&#20849;&#21516;&#24320;&#21457;&#30340;&#19968;&#22871;&#21327;&#20316;&#12289;&#20844;&#24179;&#21644;&#20195;&#34920;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21487;&#20197;&#35299;&#20915;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#20013;&#32570;&#20047;&#28165;&#26224;&#26631;&#20934;&#30340;&#38382;&#39064;&#65292;&#25512;&#21160;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2304.04640</link><description>&lt;p&gt;
NeuroBench&#65306;&#36890;&#36807;&#21512;&#20316;&#12289;&#20844;&#24179;&#21644;&#20195;&#34920;&#24615;&#22522;&#20934;&#27979;&#35797;&#25512;&#36827;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
NeuroBench: Advancing Neuromorphic Computing through Collaborative, Fair and Representative Benchmarking. (arXiv:2304.04640v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04640
&lt;/p&gt;
&lt;p&gt;
NeuroBench&#26159;&#30001;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#25104;&#21592;&#20849;&#21516;&#24320;&#21457;&#30340;&#19968;&#22871;&#21327;&#20316;&#12289;&#20844;&#24179;&#21644;&#20195;&#34920;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21487;&#20197;&#35299;&#20915;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#20013;&#32570;&#20047;&#28165;&#26224;&#26631;&#20934;&#30340;&#38382;&#39064;&#65292;&#25512;&#21160;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#39046;&#22495;&#22312;&#36981;&#24490;&#20223;&#29983;&#23398;&#21407;&#29702;&#30340;&#22522;&#30784;&#19978;&#65292;&#20855;&#26377;&#25512;&#36827;&#35745;&#31639;&#25928;&#29575;&#21644;&#33021;&#21147;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#31070;&#32463;&#24418;&#24577;&#30740;&#31350;&#20013;&#37319;&#29992;&#30340;&#25216;&#26415;&#22810;&#26679;&#24615;&#23548;&#33268;&#32570;&#20047;&#28165;&#26224;&#30340;&#22522;&#20934;&#27979;&#35797;&#26631;&#20934;&#65292;&#38459;&#30861;&#20102;&#23545;&#31070;&#32463;&#24418;&#24577;&#26041;&#27861;&#19982;&#20256;&#32479;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#30340;&#20248;&#21155;&#21183;&#36827;&#34892;&#26377;&#25928;&#35780;&#20272;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21327;&#20316;&#39033;&#30446;&#8212;&#8212;NeuroBench&#65292;&#23558;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#25104;&#21592;&#32858;&#38598;&#36215;&#26469;&#20026;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#23450;&#20041;&#22522;&#20934;&#27979;&#35797;&#12290;NeuroBench&#30340;&#30446;&#26631;&#26159;&#25104;&#20026;&#31038;&#21306;&#24320;&#21457;&#30340;&#21327;&#20316;&#12289;&#20844;&#24179;&#21644;&#20195;&#34920;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#22522;&#20934;&#27979;&#35797;&#31070;&#32463;&#24418;&#24577;&#35299;&#20915;&#26041;&#26696;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#27010;&#36848;&#20102;NeuroBench&#30340;&#20851;&#38190;&#29305;&#24615;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;NeuroBench&#23558;&#26159;&#23450;&#20041;&#33021;&#22815;&#32479;&#19968;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#30446;&#26631;&#30340;&#26631;&#20934;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of neuromorphic computing holds great promise in terms of advancing computing efficiency and capabilities by following brain-inspired principles. However, the rich diversity of techniques employed in neuromorphic research has resulted in a lack of clear standards for benchmarking, hindering effective evaluation of the advantages and strengths of neuromorphic methods compared to traditional deep-learning-based methods. This paper presents a collaborative effort, bringing together members from academia and the industry, to define benchmarks for neuromorphic computing: NeuroBench. The goals of NeuroBench are to be a collaborative, fair, and representative benchmark suite developed by the community, for the community. In this paper, we discuss the challenges associated with benchmarking neuromorphic solutions, and outline the key features of NeuroBench. We believe that NeuroBench will be a significant step towards defining standards that can unify the goals of neuromorphic comput
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#20984;&#21253;&#26426;&#30340;&#26041;&#27861;&#26469;&#30830;&#20445;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#65292;&#21516;&#26102;&#20445;&#30041;&#25968;&#25454;&#32467;&#26500;&#65292;&#29992;&#20110;&#25968;&#25454;&#34920;&#31034;&#23398;&#20064;&#30340;&#20998;&#31867;&#24212;&#29992;&#20013;&#12290;&#20026;&#20102;&#30830;&#20445;&#38544;&#31169;&#20445;&#25252;&#23398;&#20064;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#34394;&#20551;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.01300</link><description>&lt;p&gt;
&#22522;&#20110;&#26680;&#20984;&#21253;&#26426;&#30340;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Kernel Affine Hull Machines for Differentially Private Learning. (arXiv:2304.01300v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#20984;&#21253;&#26426;&#30340;&#26041;&#27861;&#26469;&#30830;&#20445;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#65292;&#21516;&#26102;&#20445;&#30041;&#25968;&#25454;&#32467;&#26500;&#65292;&#29992;&#20110;&#25968;&#25454;&#34920;&#31034;&#23398;&#20064;&#30340;&#20998;&#31867;&#24212;&#29992;&#20013;&#12290;&#20026;&#20102;&#30830;&#20445;&#38544;&#31169;&#20445;&#25252;&#23398;&#20064;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#34394;&#20551;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#36890;&#36807;&#23398;&#20064;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#28857;&#30340;&#20984;&#21253;&#26469;&#34920;&#31034;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#23558;&#25968;&#25454;&#31354;&#38388;&#21010;&#20998;&#20026;&#20960;&#20309;&#20307;&#65292;&#20174;&#32780;&#38544;&#34255;&#26377;&#20851;&#21333;&#20010;&#25968;&#25454;&#28857;&#30340;&#38544;&#31169;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#30041;&#21407;&#22987;&#23398;&#20064;&#38382;&#39064;&#30340;&#32467;&#26500;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26680;&#20984;&#21253;&#26426;&#65288;KAHM&#65289;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35745;&#31639;&#20174;&#32467;&#26524;&#26377;&#30028;&#20960;&#20309;&#20307;&#20013;&#30340;&#36317;&#31163;&#24230;&#37327;&#12290;KAHM&#26159;&#24191;&#27867;&#21644;&#28145;&#20837;&#30340;&#33258;&#32534;&#30721;&#22120;&#30340;&#20851;&#38190;&#26500;&#24314;&#22359;&#65292;&#23427;&#20204;&#20351;&#25968;&#25454;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#20998;&#31867;&#24212;&#29992;&#12290;&#20026;&#20102;&#30830;&#20445;&#38544;&#31169;&#20445;&#25252;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#34394;&#20551;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#28041;&#21450;&#23558;&#24046;&#20998;&#38544;&#31169;&#25968;&#25454;&#26679;&#26412;&#36890;&#36807;&#36716;&#25442;&#36807;&#31243;&#36827;&#34892;&#24179;&#28369;&#22788;&#29702;&#12290;&#29983;&#25104;&#30340;&#34394;&#20551;&#25968;&#25454;&#19981;&#20165;&#20445;&#35777;&#24046;&#20998;&#38544;&#31169;&#65292;&#32780;&#19988;&#30830;&#20445;KAHM&#24314;&#27169;&#35823;&#24046;&#19981;&#22823;&#20110;&#21407;&#22987;&#25968;&#25454;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the use of affine hulls of points as a means of representing data via learning in Reproducing Kernel Hilbert Spaces (RKHS), with the goal of partitioning the data space into geometric bodies that conceal privacy-sensitive information about individual data points, while preserving the structure of the original learning problem. To this end, we introduce the Kernel Affine Hull Machine (KAHM), which provides an effective way of computing a distance measure from the resulting bounded geometric body. KAHM is a critical building block in wide and deep autoencoders, which enable data representation learning for classification applications. To ensure privacy-preserving learning, we propose a novel method for generating fabricated data, which involves smoothing differentially private data samples through a transformation process. The resulting fabricated data guarantees not only differential privacy but also ensures that the KAHM modeling error is not larger than that of the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26354;&#29575;&#24179;&#34913;&#29305;&#24449;&#27969;&#24418;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25506;&#31350;&#20102;&#24863;&#30693;&#27969;&#24418;&#30340;&#20960;&#20309;&#29305;&#24615;&#23545;&#20998;&#31867;&#38590;&#24230;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#26354;&#29575;&#19981;&#24179;&#34913;&#20250;&#23548;&#33268;&#27169;&#22411;&#19981;&#20844;&#24179;&#12290;</title><link>http://arxiv.org/abs/2303.12307</link><description>&lt;p&gt;
&#38271;&#23614;&#20998;&#31867;&#30340;&#26354;&#29575;&#24179;&#34913;&#29305;&#24449;&#27969;&#24418;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Curvature-Balanced Feature Manifold Learning for Long-Tailed Classification. (arXiv:2303.12307v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26354;&#29575;&#24179;&#34913;&#29305;&#24449;&#27969;&#24418;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25506;&#31350;&#20102;&#24863;&#30693;&#27969;&#24418;&#30340;&#20960;&#20309;&#29305;&#24615;&#23545;&#20998;&#31867;&#38590;&#24230;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#26354;&#29575;&#19981;&#24179;&#34913;&#20250;&#23548;&#33268;&#27169;&#22411;&#19981;&#20844;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#24212;&#23545;&#38271;&#23614;&#20998;&#31867;&#30340;&#25361;&#25112;&#65292;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#26469;&#20943;&#23569;&#27169;&#22411;&#20559;&#24046;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#20551;&#35774;&#26679;&#26412;&#36739;&#23569;&#30340;&#31867;&#26159;&#24369;&#31867;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23614;&#37096;&#31867;&#21035;&#24182;&#19981;&#24635;&#26159;&#38590;&#20197;&#23398;&#20064;&#30340;&#65292;&#32780;&#22312;&#26679;&#26412;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#19978;&#35266;&#23519;&#21040;&#20102;&#27169;&#22411;&#20559;&#24046;&#65292;&#36825;&#34920;&#26126;&#23384;&#22312;&#20854;&#20182;&#24433;&#21709;&#27169;&#22411;&#20559;&#24046;&#30340;&#22240;&#32032;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#24863;&#30693;&#27969;&#24418;&#30340;&#20960;&#20309;&#24230;&#37327;&#65292;&#24182;&#25506;&#35752;&#20102;&#24863;&#30693;&#27969;&#24418;&#30340;&#20960;&#20309;&#29305;&#24615;&#23545;&#20998;&#31867;&#38590;&#24230;&#21644;&#23398;&#20064;&#22914;&#20309;&#22609;&#36896;&#24863;&#30693;&#27969;&#24418;&#30340;&#20960;&#20309;&#29305;&#24615;&#30340;&#24433;&#21709;&#12290;&#19968;&#20010;&#24847;&#22806;&#30340;&#21457;&#29616;&#26159;&#65306;&#31867;&#21035;&#20934;&#30830;&#24230;&#21644;&#24863;&#30693;&#27969;&#24418;&#30340;&#20998;&#31163;&#31243;&#24230;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36880;&#28176;&#20943;&#23567;&#65292;&#32780;&#19982;&#26354;&#29575;&#30340;&#36127;&#30456;&#20851;&#24615;&#36880;&#28176;&#22686;&#21152;&#65292;&#36825;&#34920;&#26126;&#26354;&#29575;&#19981;&#24179;&#34913;&#23548;&#33268;&#27169;&#22411;&#19981;&#20844;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
To address the challenges of long-tailed classification, researchers have proposed several approaches to reduce model bias, most of which assume that classes with few samples are weak classes. However, recent studies have shown that tail classes are not always hard to learn, and model bias has been observed on sample-balanced datasets, suggesting the existence of other factors that affect model bias. In this work, we systematically propose a series of geometric measurements for perceptual manifolds in deep neural networks, and then explore the effect of the geometric characteristics of perceptual manifolds on classification difficulty and how learning shapes the geometric characteristics of perceptual manifolds. An unanticipated finding is that the correlation between the class accuracy and the separation degree of perceptual manifolds gradually decreases during training, while the negative correlation with the curvature gradually increases, implying that curvature imbalance leads to m
&lt;/p&gt;</description></item><item><title>ESD&#26159;&#19968;&#31181;&#26080;&#38656;&#35843;&#21442;&#30340;&#21487;&#35757;&#32451;&#26657;&#20934;&#30446;&#26631;&#25439;&#22833;&#65292;&#36890;&#36807;&#23558;&#26657;&#20934;&#35823;&#24046;&#30475;&#20316;&#20004;&#20010;&#26399;&#26395;&#20540;&#20043;&#38388;&#30340;&#24179;&#26041;&#24046;&#65292;&#21487;&#20197;&#25913;&#21892;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26657;&#20934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.02472</link><description>&lt;p&gt;
ESD:&#39044;&#26399;&#24179;&#26041;&#24046;&#20316;&#20026;&#19968;&#31181;&#26080;&#38656;&#35843;&#21442;&#30340;&#21487;&#35757;&#32451;&#26657;&#20934;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
ESD: Expected Squared Difference as a Tuning-Free Trainable Calibration Measure. (arXiv:2303.02472v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02472
&lt;/p&gt;
&lt;p&gt;
ESD&#26159;&#19968;&#31181;&#26080;&#38656;&#35843;&#21442;&#30340;&#21487;&#35757;&#32451;&#26657;&#20934;&#30446;&#26631;&#25439;&#22833;&#65292;&#36890;&#36807;&#23558;&#26657;&#20934;&#35823;&#24046;&#30475;&#20316;&#20004;&#20010;&#26399;&#26395;&#20540;&#20043;&#38388;&#30340;&#24179;&#26041;&#24046;&#65292;&#21487;&#20197;&#25913;&#21892;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26657;&#20934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#30001;&#20110;&#36807;&#20110;&#33258;&#20449;&#30340;&#39044;&#27979;&#32780;&#24448;&#24448;&#26657;&#20934;&#19981;&#33391;&#12290;&#20256;&#32479;&#19978;&#65292;&#22312;&#35757;&#32451;&#20043;&#21518;&#20351;&#29992;&#21518;&#22788;&#29702;&#26041;&#27861;&#26469;&#26657;&#20934;&#27169;&#22411;&#12290;&#36817;&#24180;&#26469;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#21487;&#35757;&#32451;&#30340;&#26657;&#20934;&#24230;&#37327;&#26469;&#30452;&#25509;&#23558;&#20854;&#32435;&#20837;&#35757;&#32451;&#36807;&#31243;&#20013;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#37117;&#21253;&#21547;&#20869;&#37096;&#36229;&#21442;&#25968;&#65292;&#24182;&#19988;&#36825;&#20123;&#26657;&#20934;&#30446;&#26631;&#30340;&#24615;&#33021;&#20381;&#36182;&#20110;&#35843;&#25972;&#36825;&#20123;&#36229;&#21442;&#25968;&#65292;&#38543;&#30528;&#31070;&#32463;&#32593;&#32476;&#21644;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#22686;&#22823;&#65292;&#20250;&#20135;&#29983;&#26356;&#22810;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39044;&#26399;&#24179;&#26041;&#24046;&#65288;ESD&#65289;&#65292;&#19968;&#31181;&#26080;&#38656;&#35843;&#21442;&#30340;&#21487;&#35757;&#32451;&#26657;&#20934;&#30446;&#26631;&#25439;&#22833;&#65292;&#25105;&#20204;&#20174;&#20004;&#20010;&#26399;&#26395;&#20540;&#20043;&#38388;&#30340;&#24179;&#26041;&#24046;&#30340;&#35282;&#24230;&#26469;&#30475;&#26657;&#20934;&#35823;&#24046;&#12290;&#36890;&#36807;&#23545;&#20960;&#31181;&#26550;&#26500;&#65288;CNN&#12289;Transformer&#65289;&#21644;&#25968;&#25454;&#38598;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#23558;ESD&#32435;&#20837;&#35757;&#32451;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#30340;&#26657;&#20934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Studies have shown that modern neural networks tend to be poorly calibrated due to over-confident predictions. Traditionally, post-processing methods have been used to calibrate the model after training. In recent years, various trainable calibration measures have been proposed to incorporate them directly into the training process. However, these methods all incorporate internal hyperparameters, and the performance of these calibration objectives relies on tuning these hyperparameters, incurring more computational costs as the size of neural networks and datasets become larger. As such, we present Expected Squared Difference (ESD), a tuning-free (i.e., hyperparameter-free) trainable calibration objective loss, where we view the calibration error from the perspective of the squared difference between the two expectations. With extensive experiments on several architectures (CNNs, Transformers) and datasets, we demonstrate that (1) incorporating ESD into the training improves model cali
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#33539;&#24335;&#65292;&#23427;&#24378;&#35843;&#31995;&#32479;&#24615;&#22320;&#35774;&#35745;&#21644;&#26500;&#24314;&#25968;&#25454;&#23545;&#20110;&#24314;&#31435;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2212.11854</link><description>&lt;p&gt;
&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Data-centric Artificial Intelligence. (arXiv:2212.11854v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.11854
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#33539;&#24335;&#65292;&#23427;&#24378;&#35843;&#31995;&#32479;&#24615;&#22320;&#35774;&#35745;&#21644;&#26500;&#24314;&#25968;&#25454;&#23545;&#20110;&#24314;&#31435;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#65288;data-centric AI&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#33539;&#24335;&#65292;&#24378;&#35843;&#31995;&#32479;&#24615;&#22320;&#35774;&#35745;&#21644;&#26500;&#24314;&#25968;&#25454;&#23545;&#20110;&#24314;&#31435;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#30340;&#30446;&#30340;&#26159;&#21521;&#20449;&#24687;&#31995;&#32479;&#39046;&#22495;&#30340;&#20174;&#19994;&#32773;&#21644;&#30740;&#31350;&#20154;&#21592;&#20171;&#32461;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#12290;&#25105;&#20204;&#23450;&#20041;&#30456;&#20851;&#26415;&#35821;&#65292;&#25552;&#20379;&#20851;&#38190;&#29305;&#24449;&#26469;&#23545;&#27604;&#25968;&#25454;&#20013;&#24515;&#33539;&#24335;&#21644;&#27169;&#22411;&#20013;&#24515;&#33539;&#24335;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#21306;&#20998;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#21644;&#30456;&#20851;&#27010;&#24565;&#65292;&#24182;&#35752;&#35770;&#20854;&#23545;&#20449;&#24687;&#31995;&#32479;&#31038;&#21306;&#30340;&#38271;&#26399;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-centric artificial intelligence (data-centric AI) represents an emerging paradigm emphasizing that the systematic design and engineering of data is essential for building effective and efficient AI-based systems. The objective of this article is to introduce practitioners and researchers from the field of Information Systems (IS) to data-centric AI. We define relevant terms, provide key characteristics to contrast the data-centric paradigm to the model-centric one, and introduce a framework for data-centric AI. We distinguish data-centric AI from related concepts and discuss its longer-term implications for the IS community.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#21517;&#20026;SimiS&#30340;&#31616;&#21333;&#20294;&#34987;&#24573;&#35270;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20266;&#26631;&#31614;&#20316;&#20026;&#26631;&#31614;&#25968;&#25454;&#30340;&#34917;&#20805;&#65292;&#26681;&#25454;&#19982;&#26368;&#39057;&#32321;&#31867;&#21035;&#30340;&#31867;&#21035;&#20998;&#24067;&#24046;&#24322;&#65292;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#19981;&#24179;&#34913;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2211.11086</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#19981;&#24179;&#34913;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#20196;&#20154;&#23604;&#23596;&#31616;&#21333;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
An Embarrassingly Simple Baseline for Imbalanced Semi-Supervised Learning. (arXiv:2211.11086v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11086
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#21517;&#20026;SimiS&#30340;&#31616;&#21333;&#20294;&#34987;&#24573;&#35270;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20266;&#26631;&#31614;&#20316;&#20026;&#26631;&#31614;&#25968;&#25454;&#30340;&#34917;&#20805;&#65292;&#26681;&#25454;&#19982;&#26368;&#39057;&#32321;&#31867;&#21035;&#30340;&#31867;&#21035;&#20998;&#24067;&#24046;&#24322;&#65292;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#19981;&#24179;&#34913;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#22312;&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;&#30340;SSL&#20551;&#35774;&#25968;&#25454;&#20998;&#24067;&#22343;&#21248;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#26356;&#21152;&#30495;&#23454;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#22659;&#65292;&#21363;&#19981;&#24179;&#34913;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;imbalanced SSL&#65289;&#65292;&#20854;&#20013;&#26631;&#31614;&#25968;&#25454;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#37117;&#20986;&#29616;&#20102;&#19981;&#24179;&#34913;&#30340;&#31867;&#21035;&#20998;&#24067;&#12290;&#23613;&#31649;&#24050;&#26377;&#21162;&#21147;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#26041;&#27861;&#65292;&#20294;&#24403;&#36935;&#21040;&#20005;&#37325;&#19981;&#24179;&#34913;&#26102;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20250;&#36864;&#21270;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#23545;&#31867;&#21035;&#19981;&#24179;&#34913;&#36827;&#34892;&#36275;&#22815;&#21644;&#26377;&#25928;&#30340;&#20943;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#34987;&#24573;&#35270;&#30340;&#22522;&#20934;&#26041;&#27861;--SimiS&#65292;&#36890;&#36807;&#31616;&#21333;&#22320;&#26681;&#25454;&#19982;&#26368;&#39057;&#32321;&#31867;&#21035;&#30340;&#31867;&#21035;&#20998;&#24067;&#24046;&#24322;&#65292;&#23558;&#20266;&#26631;&#31614;&#20316;&#20026;&#26631;&#31614;&#25968;&#25454;&#30340;&#34917;&#20805;&#12290;&#36825;&#26679;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#20934;&#26041;&#27861;&#22312;&#20943;&#23569;&#31867;&#21035;&#19981;&#24179;&#34913;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#23427;&#22312;CIFAR100-LT&#65292;FOOD101-LT&#21644;ImageNet127&#19978;&#30456;&#23545;&#20110;&#20808;&#21069;&#26368;&#20339;&#26041;&#27861;&#30340;&#24615;&#33021;&#25552;&#21319;&#26174;&#33879;&#65292;&#20998;&#21035;&#25552;&#21319;&#20102;12.8&#65285;&#65292;13.6&#65285;&#21644;16.7&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised learning (SSL) has shown great promise in leveraging unlabeled data to improve model performance. While standard SSL assumes uniform data distribution, we consider a more realistic and challenging setting called imbalanced SSL, where imbalanced class distributions occur in both labeled and unlabeled data. Although there are existing endeavors to tackle this challenge, their performance degenerates when facing severe imbalance since they can not reduce the class imbalance sufficiently and effectively. In this paper, we study a simple yet overlooked baseline -- SimiS -- which tackles data imbalance by simply supplementing labeled data with pseudo-labels, according to the difference in class distribution from the most frequent class. Such a simple baseline turns out to be highly effective in reducing class imbalance. It outperforms existing methods by a significant margin, e.g., 12.8%, 13.6%, and 16.7% over previous SOTA on CIFAR100-LT, FOOD101-LT, and ImageNet127 respecti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#35821;&#35328;&#25511;&#21046;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#23618;&#35268;&#21010;&#22120;&#65292;&#26377;&#25928;&#32780;&#39640;&#25928;&#22320;&#25193;&#23637;&#25193;&#25955;&#27169;&#22411;&#65292;&#35299;&#20915;&#38271;&#26102;&#38388;&#36328;&#24230;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#19979;&#30340;&#25511;&#21046;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#21333;&#20219;&#21153;&#21644;&#22810;&#20219;&#21153;&#25104;&#21151;&#29575;&#65292;&#24182;&#26497;&#22823;&#22320;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2210.15629</link><description>&lt;p&gt;
&#35821;&#35328;&#25511;&#21046;&#25193;&#25955;&#65306;&#36890;&#36807;&#31354;&#38388;&#12289;&#26102;&#38388;&#21644;&#20219;&#21153;&#39640;&#25928;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Language Control Diffusion: Efficiently Scaling through Space, Time, and Tasks. (arXiv:2210.15629v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#35821;&#35328;&#25511;&#21046;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#23618;&#35268;&#21010;&#22120;&#65292;&#26377;&#25928;&#32780;&#39640;&#25928;&#22320;&#25193;&#23637;&#25193;&#25955;&#27169;&#22411;&#65292;&#35299;&#20915;&#38271;&#26102;&#38388;&#36328;&#24230;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#19979;&#30340;&#25511;&#21046;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#21333;&#20219;&#21153;&#21644;&#22810;&#20219;&#21153;&#25104;&#21151;&#29575;&#65292;&#24182;&#26497;&#22823;&#22320;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#36890;&#29992;&#22411;&#26234;&#33021;&#20307;&#22312;&#21508;&#20010;&#26041;&#38754;&#37117;&#24456;&#22256;&#38590;&#65292;&#38656;&#35201;&#22788;&#29702;&#39640;&#32500;&#36755;&#20837;&#65288;&#31354;&#38388;&#65289;&#12289;&#38271;&#26102;&#38388;&#36328;&#24230;&#65288;&#26102;&#38388;&#65289;&#21644;&#22810;&#20010;&#26032;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#32467;&#26500;&#26041;&#38754;&#30340;&#36827;&#23637;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#27839;&#30528;&#20854;&#20013;&#19968;&#20010;&#25110;&#20004;&#20010;&#32500;&#24230;&#25552;&#39640;&#25193;&#23637;&#24615;&#33021;&#21147;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#20173;&#28982;&#24456;&#39640;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#35821;&#35328;&#25511;&#21046;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#26465;&#20214;&#30340;&#20998;&#23618;&#35268;&#21010;&#22120;&#65288;LCD&#65289;&#26469;&#24212;&#23545;&#36825;&#19977;&#20010;&#26041;&#38754;&#12290;&#25105;&#20204;&#26377;&#25928;&#32780;&#39640;&#25928;&#22320;&#25193;&#23637;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#24212;&#23545;&#26102;&#38388;&#12289;&#29366;&#24577;&#21644;&#20219;&#21153;&#31354;&#38388;&#32500;&#24230;&#30340;&#38271;&#26102;&#38388;&#36328;&#24230;&#25511;&#21046;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;CALVIN&#35821;&#35328;&#26426;&#22120;&#20154;&#22522;&#20934;&#27979;&#35797;&#20013;&#23558;LCD&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;LCD&#22312;&#22810;&#20219;&#21153;&#25104;&#21151;&#29575;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#32780;&#21333;&#20219;&#21153;&#25104;&#21151;&#29575;&#65288;SR&#65289;&#20026;88.7%&#65292;&#36828;&#39640;&#20110;&#20197;&#21069;&#30340;&#26368;&#20339;&#25104;&#32489;82.6%&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training generalist agents is difficult across several axes, requiring us to deal with high-dimensional inputs (space), long horizons (time), and multiple and new tasks. Recent advances with architectures have allowed for improved scaling along one or two of these dimensions, but are still prohibitive computationally. In this paper, we propose to address all three axes by leveraging Language to Control Diffusion models as a hierarchical planner conditioned on language (LCD). We effectively and efficiently scale diffusion models for planning in extended temporal, state, and task dimensions to tackle long horizon control problems conditioned on natural language instructions. We compare LCD with other state-of-the-art models on the CALVIN language robotics benchmark and find that LCD outperforms other SOTA methods in multi task success rates while dramatically improving computational efficiency with a single task success rate (SR) of 88.7% against the previous best of 82.6%. We show that 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#20540;&#20998;&#24067;&#65292;&#24182;&#21457;&#29616;&#23398;&#20064;&#30340;&#20540;&#20998;&#24067;&#19982;&#27491;&#24577;&#20998;&#24067;&#38750;&#24120;&#25509;&#36817;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#24577;&#24341;&#23548;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#26041;&#24046;&#32593;&#32476;&#39044;&#27979;&#30340;&#26041;&#24046;&#21644;&#22238;&#25253;&#65292;&#20197;&#21450;&#19982;&#26631;&#20934;&#20540;&#20989;&#25968;&#19981;&#21516;&#30340;&#20540;&#20998;&#24067;&#32467;&#26500;&#29305;&#24449;&#26469;&#26356;&#26032;&#31574;&#30053;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20004;&#31181;&#22312;&#32447;&#31639;&#27861;&#19978;&#20135;&#29983;&#20102;&#26174;&#33879;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2208.13125</link><description>&lt;p&gt;
&#36830;&#32493;&#25511;&#21046;&#30340;&#27491;&#24120;&#24341;&#23548;&#20998;&#24067;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Normality-Guided Distributional Reinforcement Learning for Continuous Control. (arXiv:2208.13125v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.13125
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#20540;&#20998;&#24067;&#65292;&#24182;&#21457;&#29616;&#23398;&#20064;&#30340;&#20540;&#20998;&#24067;&#19982;&#27491;&#24577;&#20998;&#24067;&#38750;&#24120;&#25509;&#36817;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#24577;&#24341;&#23548;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#26041;&#24046;&#32593;&#32476;&#39044;&#27979;&#30340;&#26041;&#24046;&#21644;&#22238;&#25253;&#65292;&#20197;&#21450;&#19982;&#26631;&#20934;&#20540;&#20989;&#25968;&#19981;&#21516;&#30340;&#20540;&#20998;&#24067;&#32467;&#26500;&#29305;&#24449;&#26469;&#26356;&#26032;&#31574;&#30053;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20004;&#31181;&#22312;&#32447;&#31639;&#27861;&#19978;&#20135;&#29983;&#20102;&#26174;&#33879;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#65292;&#23398;&#20064;&#19968;&#20010;&#39044;&#27979;&#22238;&#25253;&#30340;&#22343;&#20540;&#27169;&#22411;&#65292;&#25110;&#20215;&#20540;&#20989;&#25968;&#65292;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;(DRL)&#36890;&#36807;&#24314;&#27169;&#20540;&#20998;&#24067;&#32780;&#19981;&#20165;&#20165;&#26159;&#22343;&#20540;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20960;&#20010;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#20540;&#20998;&#24067;&#65292;&#24182;&#21457;&#29616;&#23398;&#20064;&#30340;&#20540;&#20998;&#24067;&#19982;&#27491;&#24577;&#20998;&#24067;&#38750;&#24120;&#25509;&#36817;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21033;&#29992;&#36825;&#20010;&#24615;&#36136;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20174;&#26041;&#24046;&#32593;&#32476;&#39044;&#27979;&#30340;&#26041;&#24046;&#65292;&#20197;&#21450;&#22238;&#25253;&#65292;&#26469;&#20998;&#26512;&#35745;&#31639;&#20195;&#34920;&#25105;&#20204;&#20998;&#24067;&#24335;&#20540;&#20989;&#25968;&#30340;&#27491;&#24577;&#20998;&#24067;&#30340;&#30446;&#26631;&#20998;&#20301;&#26639;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20540;&#20998;&#24067;&#30340;&#32467;&#26500;&#29305;&#24449;&#30340;&#27491;&#30830;&#24615;&#26469;&#34913;&#37327;&#30340;&#31574;&#30053;&#26356;&#26032;&#26041;&#27861;&#65292;&#36825;&#20123;&#29305;&#24449;&#22312;&#26631;&#20934;&#30340;&#20540;&#20989;&#25968;&#20013;&#19981;&#23384;&#22312;&#12290;&#25105;&#20204;&#27010;&#36848;&#30340;&#26041;&#27861;&#19982;&#35768;&#22810;DRL&#32467;&#26500;&#20860;&#23481;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#31181;&#20195;&#34920;&#24615;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;PPO&#21644;TRPO&#65292;&#20316;&#20026;&#27979;&#35797;&#24179;&#21488;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#32479;&#35745;&#19978;&#20135;&#29983;&#20102;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning a predictive model of the mean return, or value function, plays a critical role in many reinforcement learning algorithms. Distributional reinforcement learning (DRL) has been shown to improve performance by modeling the value distribution, not just the mean. We study the value distribution in several continuous control tasks and find that the learned value distribution is empirical quite close to normal. We design a method that exploits this property, employ variances predicted from a variance network, along with returns, to analytically compute target quantile bars representing a normal for our distributional value function. In addition, we propose a policy update strategy based on the correctness as measured by structural characteristics of the value distribution not present in the standard value function. The approach we outline is compatible with many DRL structures. We use two representative on-policy algorithms, PPO and TRPO, as testbeds. Our method yields statistically
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22238;&#39038;&#24615;&#22320;&#20998;&#26512;&#20102;&#32534;&#31243;&#25277;&#35937;&#30340;&#28436;&#21464;&#36807;&#31243;&#65292;&#20174;&#36807;&#31243;&#12289;&#23545;&#35937;&#12289;&#35282;&#33394;&#12289;&#32452;&#20214;&#12289;&#26381;&#21153;&#21040;Agent&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#19981;&#26029;&#36861;&#27714;&#26356;&#39640;&#30340;&#28789;&#27963;&#24615;&#21644;&#25277;&#35937;&#32423;&#21035;&#26159;&#19968;&#20010;&#25345;&#32493;&#30340;&#36235;&#21183;&#65292;&#32780;&#32452;&#20214;&#12289;&#26381;&#21153;&#21644;Agent&#30340;&#27010;&#24565;&#22312;&#23454;&#29616;&#36719;&#20214;&#27169;&#22359;&#21270;&#21644;&#37325;&#26500;&#24615;&#26041;&#38754;&#20855;&#26377;&#20849;&#21516;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2112.12508</link><description>&lt;p&gt;
&#20174;&#36807;&#31243;&#12289;&#23545;&#35937;&#12289;&#35282;&#33394;&#12289;&#32452;&#20214;&#12289;&#26381;&#21153;&#21040;Agent--&#32534;&#31243;&#25277;&#35937;&#30340;&#21382;&#21490;&#21644;&#28436;&#21464;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
From Procedures, Objects, Actors, Components, Services, to Agents -- A Comparative Analysis of the History and Evolution of Programming Abstractions. (arXiv:2112.12508v4 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.12508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22238;&#39038;&#24615;&#22320;&#20998;&#26512;&#20102;&#32534;&#31243;&#25277;&#35937;&#30340;&#28436;&#21464;&#36807;&#31243;&#65292;&#20174;&#36807;&#31243;&#12289;&#23545;&#35937;&#12289;&#35282;&#33394;&#12289;&#32452;&#20214;&#12289;&#26381;&#21153;&#21040;Agent&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#19981;&#26029;&#36861;&#27714;&#26356;&#39640;&#30340;&#28789;&#27963;&#24615;&#21644;&#25277;&#35937;&#32423;&#21035;&#26159;&#19968;&#20010;&#25345;&#32493;&#30340;&#36235;&#21183;&#65292;&#32780;&#32452;&#20214;&#12289;&#26381;&#21153;&#21644;Agent&#30340;&#27010;&#24565;&#22312;&#23454;&#29616;&#36719;&#20214;&#27169;&#22359;&#21270;&#21644;&#37325;&#26500;&#24615;&#26041;&#38754;&#20855;&#26377;&#20849;&#21516;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#31456;&#30340;&#30446;&#26631;&#26159;&#25552;&#20986;&#23545;&#32534;&#31243;&#25277;&#35937;&#30340;&#28436;&#21464;&#36827;&#34892;&#19968;&#20123;&#22238;&#39038;&#24615;&#20998;&#26512;&#65292;&#20174;"&#36807;&#31243;"&#12289;"&#23545;&#35937;"&#12289;"&#35282;&#33394;"&#12289;"&#32452;&#20214;"&#12289;"&#26381;&#21153;"&#21040;"Agent"&#12290;&#37319;&#29992;&#30340;&#26041;&#27861;&#26159;&#23558;&#23427;&#20204;&#32622;&#20110;&#19968;&#20010;&#36890;&#29992;&#30340;&#21382;&#21490;&#35266;&#30340;&#35270;&#35282;&#20013;&#12290;&#36873;&#25321;&#20102;&#19968;&#20123;&#24120;&#35265;&#30340;&#21442;&#29031;&#29289;&#65292;&#21253;&#25324;&#19968;&#20010;&#23454;&#20307;&#23618;&#38754;&#30340;"&#21160;&#20316;&#36873;&#25321;"&#12289;&#23454;&#20307;&#20043;&#38388;&#30340;"&#32806;&#21512;&#28789;&#27963;&#24615;"&#21644;"&#25277;&#35937;&#32423;&#21035;"&#12290;&#25105;&#20204;&#21487;&#20197;&#35266;&#23519;&#21040;&#23545;&#26356;&#39640;&#28789;&#27963;&#24615;&#65288;&#36890;&#36807;"&#24310;&#36831;&#32465;&#23450;"&#25110;"&#36830;&#25509;&#20877;&#23454;&#20307;&#21270;"&#31561;&#27010;&#24565;&#65289;&#21644;&#26356;&#39640;&#32423;&#21035;&#30340;&#25277;&#35937;&#30340;&#19981;&#26029;&#36861;&#27714;&#12290;&#32452;&#20214;&#12289;&#26381;&#21153;&#21644;Agent&#30340;&#27010;&#24565;&#20855;&#26377;&#19968;&#20123;&#20849;&#21516;&#30340;&#30446;&#26631;&#65288;&#23588;&#20854;&#26159;"&#36719;&#20214;&#27169;&#22359;&#21270;&#21644;&#21487;&#37325;&#26500;&#24615;"&#65289;&#65292;&#32780;&#22810;Agent&#31995;&#32479;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;"&#33258;&#27835;"&#21644;"&#21327;&#21516;"&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
The objective of this chapter is to propose some retrospective analysis of the evolution of programming abstractions, from {\em procedures}, {\em objects}, {\em actors}, {\em components}, {\em services}, up to {\em agents}, %have some compare concepts of software component and of agent (and multi-agent system), %The method chosen is to by replacing them within a general historical perspective. Some common referential with three axes/dimensions is chosen: {\em action selection} at the level of one entity, {\em coupling flexibility} between entities, and {\em abstraction level}. We indeed may observe some continuous quest for higher flexibility (through notions such as {\em late binding}, or {\em reification} of {\em connections}) and higher level of {\em abstraction}. Concepts of components, services and agents have some common objectives (notably, {\em software modularity and reconfigurability}), with multi-agent systems raising further concepts of {\em autonomy} and {\em coordination}
&lt;/p&gt;</description></item></channel></rss>