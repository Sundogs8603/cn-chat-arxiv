<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#65292;&#25351;&#20986;LVLMs&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#21487;&#33021;&#26159;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#36890;&#36807;&#31995;&#32479;&#35782;&#21035;&#19982;&#27573;&#33853;&#20998;&#21106;&#31526;&#30456;&#20851;&#30340;&#35821;&#20041;&#28418;&#31227;&#20559;&#24046;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#32463;&#24120;&#36935;&#21040;&#26126;&#26174;&#30340;&#20869;&#23481;&#35821;&#20041;&#21464;&#21270;&#65292;&#23548;&#33268;&#24187;&#35273;&#30340;&#20135;&#29983;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01345</link><description>&lt;p&gt;
&#36339;&#36807;$\textbackslash n$: &#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#20943;&#23569;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Skip $\textbackslash n$: A simple method to reduce hallucination in Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#65292;&#25351;&#20986;LVLMs&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#21487;&#33021;&#26159;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#36890;&#36807;&#31995;&#32479;&#35782;&#21035;&#19982;&#27573;&#33853;&#20998;&#21106;&#31526;&#30456;&#20851;&#30340;&#35821;&#20041;&#28418;&#31227;&#20559;&#24046;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#32463;&#24120;&#36935;&#21040;&#26126;&#26174;&#30340;&#20869;&#23481;&#35821;&#20041;&#21464;&#21270;&#65292;&#23548;&#33268;&#24187;&#35273;&#30340;&#20135;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#30340;&#36827;&#23637;&#23637;&#31034;&#20102;&#20854;&#22312;&#35270;&#35273;&#20449;&#24687;&#29702;&#35299;&#19982;&#20154;&#31867;&#35821;&#35328;&#26041;&#38754;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;LVLMs&#20173;&#28982;&#38754;&#20020;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#29983;&#25104;&#19982;&#35270;&#35273;&#20449;&#24687;&#20013;&#19981;&#23384;&#22312;&#30340;&#23545;&#35937;&#30456;&#20851;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#28982;&#32780;&#65292;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#26681;&#26412;&#21407;&#22240;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#35748;&#20026;LVLMs&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#21487;&#33021;&#26159;&#24187;&#35273;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30830;&#23450;&#20102;&#19982;&#27573;&#33853;&#20998;&#21106;&#31526;&#65288;'$\textbackslash n\textbackslash n$'&#65289;&#30456;&#20851;&#30340;&#35821;&#20041;&#28418;&#31227;&#20559;&#24046;&#65292;&#21363;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#65292;&#22312;&#8220;$\textbackslash n\textbackslash n$&#8221;&#20043;&#21069;&#21644;&#20043;&#21518;&#30340;&#20869;&#23481;&#32463;&#24120;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#35821;&#20041;&#25913;&#21464;&#12290;&#36825;&#31181;&#27169;&#24335;&#20351;&#24471;&#27169;&#22411;&#25512;&#26029;&#22312;&#8220;$\textbackslash n\textbackslash n$&#8221;&#20043;&#21518;&#30340;&#20869;&#23481;&#24212;&#26126;&#26174;&#19981;&#21516;&#20110;&#21069;&#38754;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large vision-language models (LVLMs) have demonstrated impressive capability in visual information understanding with human language. Despite these advances, LVLMs still face challenges with multimodal hallucination, such as generating text descriptions of objects that are not present in the visual information. However, the underlying fundamental reasons of multimodal hallucinations remain poorly explored. In this paper, we propose a new perspective, suggesting that the inherent biases in LVLMs might be a key factor in hallucinations. Specifically, we systematically identify a semantic shift bias related to paragraph breaks ('$\textbackslash n\textbackslash n$'), where the content before and after '$\textbackslash n\textbackslash n$' in the training data frequently exhibit significant semantic changes. This pattern leads the model to infer that the contents following '$\textbackslash n\textbackslash n$' should be obviously different from the preceding contents wi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25968;&#25454;&#24322;&#26500;&#24615;&#24773;&#20917;&#19979;&#30340;&#32852;&#37030;&#21462;&#28040;&#23398;&#20064;&#65288;FU&#65289;&#30340;&#31283;&#23450;&#24615;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#38190;&#25351;&#26631;&#26469;&#35780;&#20272;FU&#65292;&#37325;&#28857;&#32771;&#34385;&#20102;&#39564;&#35777;&#12289;&#20840;&#23616;&#31283;&#23450;&#24615;&#21644;&#23616;&#37096;&#20844;&#24179;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#20013;&#30340;&#26435;&#34913;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#27934;&#23519;&#20102;&#25968;&#25454;&#24322;&#26500;&#24615;&#23545;FU&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#31649;&#29702;&#26435;&#34913;&#30340;FU&#26426;&#21046;&#12290;&#36890;&#36807;&#23454;&#35777;&#39564;&#35777;&#65292;&#25105;&#20204;&#35777;&#23454;&#20102;&#36825;&#20123;&#26426;&#21046;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01276</link><description>&lt;p&gt;
&#32852;&#37030;&#21462;&#28040;&#23398;&#20064;: &#31283;&#23450;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Federated Unlearning: a Perspective of Stability and Fairness
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25968;&#25454;&#24322;&#26500;&#24615;&#24773;&#20917;&#19979;&#30340;&#32852;&#37030;&#21462;&#28040;&#23398;&#20064;&#65288;FU&#65289;&#30340;&#31283;&#23450;&#24615;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#38190;&#25351;&#26631;&#26469;&#35780;&#20272;FU&#65292;&#37325;&#28857;&#32771;&#34385;&#20102;&#39564;&#35777;&#12289;&#20840;&#23616;&#31283;&#23450;&#24615;&#21644;&#23616;&#37096;&#20844;&#24179;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#20013;&#30340;&#26435;&#34913;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#27934;&#23519;&#20102;&#25968;&#25454;&#24322;&#26500;&#24615;&#23545;FU&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#31649;&#29702;&#26435;&#34913;&#30340;FU&#26426;&#21046;&#12290;&#36890;&#36807;&#23454;&#35777;&#39564;&#35777;&#65292;&#25105;&#20204;&#35777;&#23454;&#20102;&#36825;&#20123;&#26426;&#21046;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#25968;&#25454;&#24322;&#26500;&#24615;&#24773;&#20917;&#19979;&#32852;&#37030;&#21462;&#28040;&#23398;&#20064;&#65288;FU&#65289;&#30340;&#22810;&#26041;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;FU&#35780;&#20272;&#30340;&#20851;&#38190;&#25351;&#26631;&#65292;&#37325;&#28857;&#20851;&#27880;&#39564;&#35777;&#65292;&#20840;&#23616;&#31283;&#23450;&#24615;&#21644;&#23616;&#37096;&#20844;&#24179;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;&#20869;&#22312;&#30340;&#26435;&#34913;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#20248;&#21270;&#26694;&#26550;&#23545;&#20855;&#26377;&#25968;&#25454;&#24322;&#26500;&#24615;&#30340;&#21462;&#28040;&#23398;&#20064;&#36807;&#31243;&#36827;&#34892;&#20102;&#24418;&#24335;&#21270;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#36129;&#29486;&#22312;&#20110;&#23545;FU&#20013;&#26435;&#34913;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#25552;&#20379;&#20102;&#25968;&#25454;&#24322;&#26500;&#24615;&#23545;FU&#30340;&#24433;&#21709;&#30340;&#35265;&#35299;&#12290;&#21033;&#29992;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31649;&#29702;&#26435;&#34913;&#30340;FU&#26426;&#21046;&#65292;&#20026;FU&#26426;&#21046;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#25552;&#20379;&#25351;&#23548;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;FU&#26426;&#21046;&#26377;&#25928;&#22320;&#24179;&#34913;&#20102;&#26435;&#34913;&#65292;&#30830;&#35748;&#20102;&#20174;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#20013;&#24471;&#20986;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the multifaceted consequences of federated unlearning (FU) with data heterogeneity. We introduce key metrics for FU assessment, concentrating on verification, global stability, and local fairness, and investigate the inherent trade-offs. Furthermore, we formulate the unlearning process with data heterogeneity through an optimization framework. Our key contribution lies in a comprehensive theoretical analysis of the trade-offs in FU and provides insights into data heterogeneity's impacts on FU. Leveraging these insights, we propose FU mechanisms to manage the trade-offs, guiding further development for FU mechanisms. We empirically validate that our FU mechanisms effectively balance trade-offs, confirming insights derived from our theoretical analysis.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#20197;&#21152;&#36895;Transformers&#27169;&#22411;&#30340;&#21487;&#20998;&#35299;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36890;&#36807;&#24341;&#20837;&#22240;&#23376;&#20998;&#35299;&#24418;&#24335;&#30340;&#27880;&#24847;&#21147;&#65292;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22797;&#26434;&#24230;&#20174;O(N^2)&#38477;&#20302;&#21040;O(N)&#65292;&#24182; &#22312;&#32500;&#25345;&#27880;&#24847;&#21147;&#30697;&#38453;&#23436;&#25972;&#34920;&#31034;&#30340;&#21516;&#26102;&#20445;&#25345;&#31232;&#30095;&#24615;&#21644;&#25152;&#26377;-&#25152;&#26377;&#20196;&#29260;&#20851;&#31995;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27880;&#24847;&#21147;&#26426;&#21046;&#20855;&#26377;&#31283;&#20581;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#19981;&#21516;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.07901</link><description>&lt;p&gt;
FAST: &#29992;&#20110;&#21152;&#36895;Transformers&#30340;&#21487;&#20998;&#35299;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
FAST: Factorizable Attention for Speeding up Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07901
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#20197;&#21152;&#36895;Transformers&#27169;&#22411;&#30340;&#21487;&#20998;&#35299;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36890;&#36807;&#24341;&#20837;&#22240;&#23376;&#20998;&#35299;&#24418;&#24335;&#30340;&#27880;&#24847;&#21147;&#65292;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22797;&#26434;&#24230;&#20174;O(N^2)&#38477;&#20302;&#21040;O(N)&#65292;&#24182; &#22312;&#32500;&#25345;&#27880;&#24847;&#21147;&#30697;&#38453;&#23436;&#25972;&#34920;&#31034;&#30340;&#21516;&#26102;&#20445;&#25345;&#31232;&#30095;&#24615;&#21644;&#25152;&#26377;-&#25152;&#26377;&#20196;&#29260;&#20851;&#31995;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27880;&#24847;&#21147;&#26426;&#21046;&#20855;&#26377;&#31283;&#20581;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#19981;&#21516;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21407;&#22987;&#30340;&#24555;&#36895;&#22810;&#26497;&#26041;&#27861;&#21644;&#25913;&#36827;&#21518;&#30340;&#24555;&#36895;&#39640;&#26031;&#21464;&#25442;&#25152;&#22266;&#26377;&#30340;&#22240;&#23376;&#20998;&#35299;&#30340;&#39537;&#21160;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39640;&#32500;&#24230;&#20013;&#39640;&#25928;&#36816;&#34892;&#30340;&#21487;&#20998;&#35299;&#27880;&#24847;&#21147;&#24418;&#24335;&#12290;&#36825;&#31181;&#26041;&#27861;&#23558;Transformers&#20013;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#22797;&#26434;&#24230;&#20174;O(N^2)&#38477;&#20302;&#21040;O(N)&#12290;&#19982;&#20043;&#21069;&#30340;&#23581;&#35797;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#21576;&#29616;&#20102;&#19968;&#20010;&#32447;&#24615;&#32553;&#25918;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#26082;&#20445;&#25345;&#20102;&#27880;&#24847;&#21147;&#30697;&#38453;&#30340;&#23436;&#25972;&#34920;&#31034;&#65292;&#21448;&#19981;&#22949;&#21327;&#20110;&#31232;&#30095;&#21270;&#65292;&#24182;&#19988;&#21253;&#21547;&#20102;&#20196;&#29260;&#20043;&#38388;&#30340;&#20840;&#20114;&#25805;&#20316;&#20851;&#31995;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#25105;&#20204;&#26032;&#30340;&#27880;&#24847;&#21147;&#24230;&#37327;&#30340;&#23646;&#24615;&#65292;&#24182;&#22312;&#21508;&#31181;&#26631;&#20934;&#35774;&#32622;&#19979;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#20855;&#26377;&#31283;&#20581;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#20351;&#29992;&#33258;&#25105;&#27880;&#24847;&#21147;&#30340;&#22810;&#26679;&#30340;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the factorization inherent in the original fast multipole method and the improved fast Gauss transform we introduce a factorable form of attention that operates efficiently in high dimensions. This approach reduces the computational and memory complexity of the attention mechanism in transformers from $O(N^2)$ to $O(N)$. In comparison to previous attempts, our work presents a linearly scaled attention mechanism that maintains the full representation of the attention matrix without compromising on sparsification and incorporates the all-to-all relationship between tokens. We explore the properties of our new attention metric and conduct tests in various standard settings. Results indicate that our attention mechanism has a robust performance and holds significant promise for diverse applications where self-attention is used.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26816;&#27979;&#25289;&#24067;&#25289;&#22810;&#35910;&#19978;&#34584;&#34523;&#34728;&#30340;&#35270;&#35273;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;RGBN&#25968;&#25454;&#38598;&#24182;&#20351;&#29992;&#20004;&#38454;&#27573;&#26089;&#26399;&#30149;&#23475;&#26816;&#27979;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;&#21333;&#38454;&#27573;&#20998;&#21106;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;mAP&#12290;&#20351;&#29992;RGBN&#25968;&#25454;&#30340;&#39034;&#24207;CNN&#27169;&#22411;&#22312;&#20998;&#31867;&#20013;&#20063;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.07895</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26816;&#27979;&#25289;&#24067;&#25289;&#22810;&#35910;&#19978;&#30340;&#34584;&#34523;&#34728;
&lt;/p&gt;
&lt;p&gt;
Detection of Spider Mites on Labrador Beans through Machine Learning Approaches Using Custom Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26816;&#27979;&#25289;&#24067;&#25289;&#22810;&#35910;&#19978;&#34584;&#34523;&#34728;&#30340;&#35270;&#35273;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;RGBN&#25968;&#25454;&#38598;&#24182;&#20351;&#29992;&#20004;&#38454;&#27573;&#26089;&#26399;&#30149;&#23475;&#26816;&#27979;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;&#21333;&#38454;&#27573;&#20998;&#21106;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;mAP&#12290;&#20351;&#29992;RGBN&#25968;&#25454;&#30340;&#39034;&#24207;CNN&#27169;&#22411;&#22312;&#20998;&#31867;&#20013;&#20063;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#26029;&#22686;&#38271;&#30340;&#31918;&#39135;&#29983;&#20135;&#38656;&#27714;&#20013;&#65292;&#26089;&#26399;&#26893;&#29289;&#30149;&#23475;&#30340;&#26816;&#27979;&#23545;&#20445;&#25252;&#20316;&#29289;&#33267;&#20851;&#37325;&#35201;&#65307;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22312;&#30495;&#23454;&#29615;&#22659;&#26465;&#20214;&#19979;&#36890;&#36807;JAI FS-1600D-10GE&#30456;&#26426;&#37319;&#38598;&#30340;RGB&#21644;NIR&#25968;&#25454;&#26500;&#24314;RGBN&#25968;&#25454;&#38598;&#36827;&#34892;&#26893;&#29289;&#30149;&#23475;&#26816;&#27979;&#30340;&#35270;&#35273;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#20351;&#29992;YOLOv8&#21644;&#39034;&#24207;CNN&#30340;&#20004;&#38454;&#27573;&#26089;&#26399;&#26893;&#29289;&#30149;&#23475;&#26816;&#27979;&#27169;&#22411;&#65292;&#22312;&#20855;&#26377;&#37096;&#20998;&#26631;&#31614;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#30456;&#23545;&#20110;&#21333;&#38454;&#27573;&#31471;&#21040;&#31471;&#20998;&#21106;&#27169;&#22411;&#25552;&#39640;&#20102;3.6&#65285;&#30340;mAP&#12290;&#39034;&#24207;CNN&#27169;&#22411;&#20351;&#29992;RGBN&#25968;&#25454;&#23454;&#29616;&#20102;90.62&#65285;&#30340;&#39564;&#35777;&#20934;&#30830;&#29575;&#12290;&#20351;&#29992;ResNet15&#21644;&#39034;&#24207;CNN&#27169;&#22411;&#22312;&#20998;&#31867;&#20013;&#20351;&#29992;RGBN&#30456;&#27604;&#20165;&#20351;&#29992;RGB&#65292;&#24179;&#22343;&#39564;&#35777;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;6.25&#65285;&#12290;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#25968;&#25454;&#38598;&#25913;&#36827;&#38656;&#35201;&#20197;&#28385;&#36275;&#31918;&#39135;&#29983;&#20135;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Amidst growing food production demands, early plant disease detection is essential to safeguard crops; this study proposes a visual machine learning approach for plant disease detection, harnessing RGB and NIR data collected in real-world conditions through a JAI FS-1600D-10GE camera to build an RGBN dataset. A two-stage early plant disease detection model with YOLOv8 and a sequential CNN was used to train on a dataset with partial labels, which showed a 3.6% increase in mAP compared to a single-stage end-to-end segmentation model. The sequential CNN model achieved 90.62% validation accuracy utilising RGBN data. An average of 6.25% validation accuracy increase is found using RGBN in classification compared to RGB using ResNet15 and the sequential CNN models. Further research and dataset improvements are needed to meet food production demands.
&lt;/p&gt;</description></item><item><title>MAIDCRL &#26159;&#19968;&#31181;&#21322;&#38598;&#20013;&#24335;&#23494;&#38598;&#31070;&#32463;&#32593;&#32476;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26234;&#33021;&#20307;&#24433;&#21709;&#22270;(AIMs)&#21644;&#21367;&#31215;&#23618;&#65292;&#25104;&#21151;&#22312; StarCraft &#22810;&#26234;&#33021;&#20307;&#25361;&#25112;&#20013;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#22810;&#26234;&#33021;&#20307;&#25511;&#21046;&#12290;CNN-enabled MAIDCRL &#22312;&#23398;&#20064;&#24615;&#33021;&#19978;&#26174;&#33879;&#25552;&#39640;&#65292;&#24182;&#22312;&#22797;&#26434;&#30340;&#24322;&#36136;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#26356;&#24555;&#30340;&#23398;&#20064;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.07890</link><description>&lt;p&gt;
MAIDCRL: &#21322;&#38598;&#20013;&#24335;&#22810;&#26234;&#33021;&#20307;&#24433;&#21709;&#23494;&#38598;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MAIDCRL: Semi-centralized Multi-Agent Influence Dense-CNN Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07890
&lt;/p&gt;
&lt;p&gt;
MAIDCRL &#26159;&#19968;&#31181;&#21322;&#38598;&#20013;&#24335;&#23494;&#38598;&#31070;&#32463;&#32593;&#32476;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26234;&#33021;&#20307;&#24433;&#21709;&#22270;(AIMs)&#21644;&#21367;&#31215;&#23618;&#65292;&#25104;&#21151;&#22312; StarCraft &#22810;&#26234;&#33021;&#20307;&#25361;&#25112;&#20013;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#22810;&#26234;&#33021;&#20307;&#25511;&#21046;&#12290;CNN-enabled MAIDCRL &#22312;&#23398;&#20064;&#24615;&#33021;&#19978;&#26174;&#33879;&#25552;&#39640;&#65292;&#24182;&#22312;&#22797;&#26434;&#30340;&#24322;&#36136;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#26356;&#24555;&#30340;&#23398;&#20064;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#20998;&#24067;&#24335;&#20915;&#31574;&#21046;&#23450;&#20013;&#65292;&#23545;&#20110;&#21512;&#20316;&#21644;&#31454;&#20105;&#31995;&#32479;&#20013;&#30340;&#20132;&#20114;&#34892;&#20026;&#23398;&#20064;&#65292;&#23384;&#22312;&#22256;&#38590;&#25361;&#25112;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#31181;&#22797;&#26434;&#24615;&#65292;MAIDRL&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#24378;&#20102;&#26234;&#33021;&#20307;&#24433;&#21709;&#22270;(AIMs)&#30340;&#21322;&#38598;&#20013;&#24335;&#23494;&#38598;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#22312;StarCraft&#22810;&#26234;&#33021;&#20307;&#25361;&#25112;(SMAC)&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#22810;&#26234;&#33021;&#20307;&#25511;&#21046;&#12290;&#26412;&#25991;&#22312;MAIDRL&#20013;&#25193;&#23637;&#20102;DenseNet&#65292;&#24341;&#20837;&#20102;&#21322;&#38598;&#20013;&#24335;&#22810;&#26234;&#33021;&#20307;&#23494;&#38598;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#24378;&#21270;&#23398;&#20064;(MAIDCRL)&#65292;&#36890;&#36807;&#23558;&#21367;&#31215;&#23618;&#34701;&#20837;&#28145;&#24230;&#27169;&#22411;&#26550;&#26500;&#65292;&#24182;&#22312;&#21516;&#36136;&#21644;&#24322;&#36136;&#22330;&#26223;&#19978;&#35780;&#20272;&#20102;&#24615;&#33021;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#21551;&#29992;&#20102;CNN&#30340;MAIDCRL&#22312;&#23398;&#20064;&#24615;&#33021;&#19978;&#26174;&#33879;&#25552;&#39640;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;MAIDRL&#65292;&#29305;&#21035;&#26159;&#22312;&#26356;&#22797;&#26434;&#30340;&#24322;&#36136;SMAC&#22330;&#26223;&#19978;&#65292;&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#23398;&#20064;&#36895;&#24230;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#32479;&#35745;&#25968;&#25454;&#26174;&#31034;...
&lt;/p&gt;
&lt;p&gt;
Distributed decision-making in multi-agent systems presents difficult challenges for interactive behavior learning in both cooperative and competitive systems. To mitigate this complexity, MAIDRL presents a semi-centralized Dense Reinforcement Learning algorithm enhanced by agent influence maps (AIMs), for learning effective multi-agent control on StarCraft Multi-Agent Challenge (SMAC) scenarios. In this paper, we extend the DenseNet in MAIDRL and introduce semi-centralized Multi-Agent Dense-CNN Reinforcement Learning, MAIDCRL, by incorporating convolutional layers into the deep model architecture, and evaluate the performance on both homogeneous and heterogeneous scenarios. The results show that the CNN-enabled MAIDCRL significantly improved the learning performance and achieved a faster learning rate compared to the existing MAIDRL, especially on more complicated heterogeneous SMAC scenarios. We further investigate the stability and robustness of our model. The statistics reflect tha
&lt;/p&gt;</description></item><item><title>WildfireGPT&#26159;&#19968;&#20010;&#38024;&#23545;&#37326;&#28779;&#20998;&#26512;&#30340;&#23450;&#21046;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25552;&#20379;&#39046;&#22495;&#29305;&#23450;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#31185;&#23398;&#20934;&#30830;&#24615;&#65292;&#23558;&#29992;&#25143;&#26597;&#35810;&#36716;&#21270;&#20026;&#20851;&#20110;&#37326;&#28779;&#39118;&#38505;&#30340;&#21487;&#25805;&#20316;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.07877</link><description>&lt;p&gt;
WildfireGPT&#65306;&#38024;&#23545;&#37326;&#28779;&#20998;&#26512;&#30340;&#23450;&#21046;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
WildfireGPT: Tailored Large Language Model for Wildfire Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07877
&lt;/p&gt;
&lt;p&gt;
WildfireGPT&#26159;&#19968;&#20010;&#38024;&#23545;&#37326;&#28779;&#20998;&#26512;&#30340;&#23450;&#21046;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25552;&#20379;&#39046;&#22495;&#29305;&#23450;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#31185;&#23398;&#20934;&#30830;&#24615;&#65292;&#23558;&#29992;&#25143;&#26597;&#35810;&#36716;&#21270;&#20026;&#20851;&#20110;&#37326;&#28779;&#39118;&#38505;&#30340;&#21487;&#25805;&#20316;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#20195;&#34920;&#20102;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#39046;&#22495;&#30340;&#19968;&#31181;&#21464;&#38761;&#24615;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;LLMs&#26159;&#36890;&#29992;&#27169;&#22411;&#65292;&#35757;&#32451;&#20110;&#24191;&#27867;&#30340;&#25991;&#26412;&#35821;&#26009;&#24211;&#65292;&#24448;&#24448;&#38590;&#20197;&#25552;&#20379;&#29305;&#23450;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#23588;&#20854;&#26159;&#22312;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#30340;&#39046;&#22495;&#65292;&#27604;&#22914;&#37326;&#28779;&#32454;&#33410;&#22312;&#26356;&#24191;&#27867;&#30340;&#27668;&#20505;&#21464;&#21270;&#32972;&#26223;&#19979;&#12290;&#23545;&#20110;&#20851;&#27880;&#37326;&#28779;&#24377;&#24615;&#21644;&#36866;&#24212;&#24615;&#30340;&#20915;&#31574;&#32773;&#21644;&#25919;&#31574;&#21046;&#23450;&#32773;&#26469;&#35828;&#65292;&#33719;&#21462;&#19981;&#20165;&#20934;&#30830;&#32780;&#19988;&#39046;&#22495;&#29305;&#23450;&#30340;&#21709;&#24212;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#19981;&#26159;&#27867;&#27867;&#32780;&#35848;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;WildfireGPT&#65292;&#19968;&#20010;&#21407;&#22411;LLM&#20195;&#29702;&#65292;&#26088;&#22312;&#23558;&#29992;&#25143;&#26597;&#35810;&#36716;&#21270;&#20026;&#20851;&#20110;&#37326;&#28779;&#39118;&#38505;&#30340;&#21487;&#25805;&#20316;&#35265;&#35299;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#27668;&#20505;&#39044;&#27979;&#21644;&#31185;&#23398;&#25991;&#29486;&#31561;&#39069;&#22806;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#20016;&#23500;WildfireGPT&#65292;&#20197;&#30830;&#20445;&#20854;&#20449;&#24687;&#20855;&#26377;&#26102;&#25928;&#24615;&#12289;&#30456;&#20851;&#24615;&#21644;&#31185;&#23398;&#20934;&#30830;&#24615;&#12290;&#36825;&#20351;&#24471;WildfireGPT&#25104;&#20026;&#19968;&#20010;&#26377;&#25928;&#30340;&#24037;&#20855;&#26469;&#35299;&#20915;&#23454;&#38469;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advancement of large language models (LLMs) represents a transformational capability at the frontier of artificial intelligence (AI) and machine learning (ML). However, LLMs are generalized models, trained on extensive text corpus, and often struggle to provide context-specific information, particularly in areas requiring specialized knowledge such as wildfire details within the broader context of climate change. For decision-makers and policymakers focused on wildfire resilience and adaptation, it is crucial to obtain responses that are not only precise but also domain-specific, rather than generic. To that end, we developed WildfireGPT, a prototype LLM agent designed to transform user queries into actionable insights on wildfire risks. We enrich WildfireGPT by providing additional context such as climate projections and scientific literature to ensure its information is current, relevant, and scientifically accurate. This enables WildfireGPT to be an effective tool for del
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#27169;&#22411;&#65288;LFMs&#65289;&#25913;&#36827;&#25919;&#31574;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#26399;&#26395;&#30340;&#34892;&#20026;&#24182;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#25105;&#20204;&#22312;&#20219;&#21153;&#23436;&#25104;&#29575;&#12289;&#27867;&#21270;&#24615;&#33021;&#21644;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.07876</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#27169;&#22411;&#26469;&#25913;&#36827;&#25919;&#31574;
&lt;/p&gt;
&lt;p&gt;
Policy Improvement using Language Feedback Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#27169;&#22411;&#65288;LFMs&#65289;&#25913;&#36827;&#25919;&#31574;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#26399;&#26395;&#30340;&#34892;&#20026;&#24182;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#25105;&#20204;&#22312;&#20219;&#21153;&#23436;&#25104;&#29575;&#12289;&#27867;&#21270;&#24615;&#33021;&#21644;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#35821;&#35328;&#21453;&#39304;&#27169;&#22411;&#65288;LFMs&#65289;&#65292;&#29992;&#20110;&#22312;&#25351;&#20196;&#36981;&#24490;&#20013;&#35782;&#21035;&#26399;&#26395;&#30340;&#34892;&#20026;-&#26377;&#21161;&#20110;&#23454;&#29616;&#25351;&#20196;&#20013;&#25351;&#23450;&#20219;&#21153;&#30340;&#34892;&#21160;-&#20197;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#12290;&#20026;&#20102;&#35757;&#32451;LFMs&#65292;&#25105;&#20204;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33719;&#21462;&#23545;&#35270;&#35273;&#36712;&#36857;&#36827;&#34892;&#35821;&#35328;&#25551;&#36848;&#30340;&#21453;&#39304;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#20351;&#29992;LFMs&#35782;&#21035;&#26399;&#26395;&#27169;&#20223;&#30340;&#34892;&#20026;&#65292;&#25105;&#20204;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#35821;&#35328;&#22522;&#30784;&#29615;&#22659;&#65288;Touchdown&#65292;ScienceWorld&#21644;ALFWorld&#65289;&#19978;&#65292;&#22312;&#20219;&#21153;&#23436;&#25104;&#29575;&#19978;&#25913;&#21892;&#20102;&#24378;&#34892;&#20026;&#20811;&#38534;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;&#20854;&#27425;&#65292;&#19982;LLMs&#30452;&#25509;&#39044;&#27979;&#34892;&#21160;&#30456;&#27604;&#65292;&#20351;&#29992;LFMs&#22312;LLM&#36755;&#20986;&#26631;&#35760;&#30340;&#25968;&#37327;&#30456;&#21516;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#26356;&#22909;&#12290;&#31532;&#19977;&#65292;LFMs&#36866;&#24212;&#26410;&#35265;&#29615;&#22659;&#65292;&#36890;&#36807;&#19968;&#36718;&#36866;&#24212;&#20351;&#20219;&#21153;&#23436;&#25104;&#29575;&#25552;&#39640;&#20102;3.5-12.0&#65285;&#12290;&#26368;&#21518;&#65292;&#21487;&#20197;&#20462;&#25913;LFM&#20197;&#25552;&#20379;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#21453;&#39304;&#65292;&#26080;&#38656;&#24615;&#33021;&#25439;&#22833;&#65292;&#20174;&#32780;&#20801;&#35768;&#20154;&#31867;&#39564;&#35777;&#27169;&#20223;&#23398;&#20064;&#30340;&#26399;&#26395;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Language Feedback Models (LFMs) that identify desirable behaviour - actions that help achieve tasks specified in the instruction - for imitation learning in instruction following. To train LFMs, we obtain feedback from Large Language Models (LLMs) on visual trajectories verbalized to language descriptions. First, by using LFMs to identify desirable behaviour to imitate, we improve in task-completion rate over strong behavioural cloning baselines on three distinct language grounding environments (Touchdown, ScienceWorld, and ALFWorld). Second, LFMs outperform using LLMs as experts to directly predict actions, when controlling for the number of LLM output tokens. Third, LFMs generalize to unseen environments, improving task-completion rate by 3.5-12.0% through one round of adaptation. Finally, LFM can be modified to provide human-interpretable feedback without performance loss, allowing human verification of desirable behaviour for imitation learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31574;&#30053;&#26799;&#24230;&#22312;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#25511;&#21046;&#20013;&#23545;&#26410;&#35265;&#21021;&#22987;&#29366;&#24577;&#30340;&#22806;&#25512;&#38382;&#39064;&#65292;&#21457;&#29616;&#22806;&#25512;&#31243;&#24230;&#21462;&#20915;&#20110;&#35757;&#32451;&#20013;&#31995;&#32479;&#30340;&#25506;&#32034;&#31243;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.07875</link><description>&lt;p&gt;
&#32447;&#24615;&#20108;&#27425;&#25511;&#21046;&#20013;&#31574;&#30053;&#26799;&#24230;&#30340;&#38544;&#24615;&#20559;&#24046;&#65306;&#23545;&#26410;&#35265;&#21021;&#22987;&#29366;&#24577;&#30340;&#22806;&#25512;
&lt;/p&gt;
&lt;p&gt;
Implicit Bias of Policy Gradient in Linear Quadratic Control: Extrapolation to Unseen Initial States
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31574;&#30053;&#26799;&#24230;&#22312;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#25511;&#21046;&#20013;&#23545;&#26410;&#35265;&#21021;&#22987;&#29366;&#24577;&#30340;&#22806;&#25512;&#38382;&#39064;&#65292;&#21457;&#29616;&#22806;&#25512;&#31243;&#24230;&#21462;&#20915;&#20110;&#35757;&#32451;&#20013;&#31995;&#32479;&#30340;&#25506;&#32034;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#27169;&#22411;&#21487;&#20197;&#20197;&#22810;&#31181;&#26041;&#24335;&#25311;&#21512;&#35757;&#32451;&#25968;&#25454;&#65292;&#20854;&#20013;&#19968;&#20123;&#22312;&#26410;&#35265;&#65288;&#27979;&#35797;&#65289;&#25968;&#25454;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#32780;&#20854;&#20182;&#19968;&#20123;&#21017;&#19981;&#28982;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26799;&#24230;&#19979;&#38477;&#32463;&#24120;&#23637;&#29616;&#20986;&#19968;&#31181;&#38544;&#24615;&#20559;&#24046;&#65292;&#23548;&#33268;&#22312;&#26410;&#35265;&#25968;&#25454;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#31181;&#38544;&#24615;&#20559;&#24046;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22312;&#26368;&#20248;&#25511;&#21046;&#65288;&#24378;&#21270;&#23398;&#20064;&#65289;&#20013;&#21364;&#20102;&#35299;&#24471;&#36739;&#23569;&#12290;&#22312;&#37027;&#37324;&#65292;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#24212;&#29992;&#20110;&#31995;&#32479;&#30340;&#25511;&#21046;&#22120;&#34987;&#31216;&#20026;&#31574;&#30053;&#26799;&#24230;&#65292;&#24182;&#19988;&#19968;&#20010;&#38750;&#24120;&#37325;&#35201;&#30340;&#38382;&#39064;&#26159;&#23398;&#20064;&#30340;&#25511;&#21046;&#22120;&#22312;&#23545;&#26410;&#35265;&#21021;&#22987;&#29366;&#24577;&#30340;&#22806;&#25512;&#31243;&#24230;&#12290;&#26412;&#25991;&#22312;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;&#31574;&#30053;&#26799;&#24230;&#22312;&#23545;&#26410;&#35265;&#21021;&#22987;&#29366;&#24577;&#30340;&#22806;&#25512;&#26041;&#38754;&#30340;&#38544;&#24615;&#20559;&#24046;&#12290;&#25105;&#20204;&#20197;&#22522;&#26412;&#30340;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#65288;LQR&#65289;&#38382;&#39064;&#20026;&#37325;&#28857;&#65292;&#30830;&#31435;&#20102;&#22806;&#25512;&#31243;&#24230;&#21462;&#20915;&#20110;&#35757;&#32451;&#20013;&#31995;&#32479;&#22312;&#21021;&#22987;&#29366;&#24577;&#19979;&#24341;&#36215;&#30340;&#25506;&#32034;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In modern machine learning, models can often fit training data in numerous ways, some of which perform well on unseen (test) data, while others do not. Remarkably, in such cases gradient descent frequently exhibits an implicit bias that leads to excellent performance on unseen data. This implicit bias was extensively studied in supervised learning, but is far less understood in optimal control (reinforcement learning). There, learning a controller applied to a system via gradient descent is known as policy gradient, and a question of prime importance is the extent to which a learned controller extrapolates to unseen initial states. This paper theoretically studies the implicit bias of policy gradient in terms of extrapolation to unseen initial states. Focusing on the fundamental Linear Quadratic Regulator (LQR) problem, we establish that the extent of extrapolation depends on the degree of exploration induced by the system when commencing from initial states included in training. Exper
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#32454;&#31890;&#24230;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#30340;&#26631;&#24230;&#29305;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#31890;&#24230;&#20316;&#20026;&#26032;&#30340;&#36229;&#21442;&#25968;&#65292;&#36890;&#36807;&#35843;&#25972;&#31890;&#24230;&#21487;&#20197;&#31934;&#30830;&#25511;&#21046;&#19987;&#23478;&#30340;&#22823;&#23567;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;MoE&#27169;&#22411;&#22312;&#25928;&#26524;&#19978;&#22987;&#32456;&#20248;&#20110;&#23494;&#38598;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#24182;&#19988;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#21644;&#35757;&#32451;&#39044;&#31639;&#30340;&#22686;&#22823;&#65292;&#23494;&#38598;&#21644;MoE&#27169;&#22411;&#20043;&#38388;&#30340;&#25928;&#29575;&#24046;&#36317;&#20063;&#22312;&#22686;&#22823;&#12290;&#21516;&#26102;&#65292;&#23558;MoE&#20013;&#19987;&#23478;&#30340;&#22823;&#23567;&#35774;&#32622;&#20026;&#19982;&#21069;&#39304;&#23618;&#30456;&#21516;&#30340;&#24120;&#35265;&#20570;&#27861;&#22312;&#20960;&#20046;&#20219;&#20309;&#35745;&#31639;&#39044;&#31639;&#19979;&#37117;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.07871</link><description>&lt;p&gt;
&#32454;&#31890;&#24230;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#30340;&#26631;&#24230;&#24459;
&lt;/p&gt;
&lt;p&gt;
Scaling Laws for Fine-Grained Mixture of Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#32454;&#31890;&#24230;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#30340;&#26631;&#24230;&#29305;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#31890;&#24230;&#20316;&#20026;&#26032;&#30340;&#36229;&#21442;&#25968;&#65292;&#36890;&#36807;&#35843;&#25972;&#31890;&#24230;&#21487;&#20197;&#31934;&#30830;&#25511;&#21046;&#19987;&#23478;&#30340;&#22823;&#23567;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;MoE&#27169;&#22411;&#22312;&#25928;&#26524;&#19978;&#22987;&#32456;&#20248;&#20110;&#23494;&#38598;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#24182;&#19988;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#21644;&#35757;&#32451;&#39044;&#31639;&#30340;&#22686;&#22823;&#65292;&#23494;&#38598;&#21644;MoE&#27169;&#22411;&#20043;&#38388;&#30340;&#25928;&#29575;&#24046;&#36317;&#20063;&#22312;&#22686;&#22823;&#12290;&#21516;&#26102;&#65292;&#23558;MoE&#20013;&#19987;&#23478;&#30340;&#22823;&#23567;&#35774;&#32622;&#20026;&#19982;&#21069;&#39304;&#23618;&#30456;&#21516;&#30340;&#24120;&#35265;&#20570;&#27861;&#22312;&#20960;&#20046;&#20219;&#20309;&#35745;&#31639;&#39044;&#31639;&#19979;&#37117;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#27169;&#22411;&#24050;&#25104;&#20026;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#25104;&#26412;&#30340;&#20027;&#35201;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#26631;&#24230;&#29305;&#24615;&#65292;&#24182;&#32435;&#20837;&#20102;&#26356;&#24191;&#27867;&#30340;&#21464;&#37327;&#33539;&#22260;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#36229;&#21442;&#25968;&#65292;&#31216;&#20026;&#31890;&#24230;&#65292;&#36890;&#36807;&#35843;&#25972;&#31890;&#24230;&#21487;&#20197;&#31934;&#30830;&#25511;&#21046;&#19987;&#23478;&#30340;&#22823;&#23567;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#32454;&#31890;&#24230;MoE&#30340;&#26631;&#24230;&#24459;&#65292;&#32771;&#34385;&#20102;&#35757;&#32451;&#26631;&#35760;&#25968;&#12289;&#27169;&#22411;&#22823;&#23567;&#21644;&#31890;&#24230;&#12290;&#21033;&#29992;&#36825;&#20123;&#23450;&#24459;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#32473;&#23450;&#35745;&#31639;&#39044;&#31639;&#19979;&#30340;&#26368;&#20339;&#35757;&#32451;&#37197;&#32622;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#19981;&#20165;&#34920;&#26126;MoE&#27169;&#22411;&#22987;&#32456;&#20248;&#20110;&#23494;&#38598;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#32780;&#19988;&#36824;&#20984;&#26174;&#20102;&#22312;&#25193;&#22823;&#27169;&#22411;&#22823;&#23567;&#21644;&#35757;&#32451;&#39044;&#31639;&#26102;&#65292;&#23494;&#38598;&#21644;MoE&#27169;&#22411;&#20043;&#38388;&#30340;&#25928;&#29575;&#24046;&#36317;&#22312;&#25193;&#22823;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23558;MoE&#20013;&#19987;&#23478;&#30340;&#22823;&#23567;&#35774;&#32622;&#20026;&#19982;&#21069;&#39304;&#23618;&#30456;&#21516;&#30340;&#24120;&#35265;&#20570;&#27861;&#22312;&#20960;&#20046;&#20219;&#20309;&#35745;&#31639;&#39044;&#31639;&#19979;&#37117;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixture of Experts (MoE) models have emerged as a primary solution for reducing the computational cost of Large Language Models. In this work, we analyze their scaling properties, incorporating an expanded range of variables. Specifically, we introduce a new hyperparameter, granularity, whose adjustment enables precise control over the size of the experts. Building on this, we establish scaling laws for fine-grained MoE, taking into account the number of training tokens, model size, and granularity. Leveraging these laws, we derive the optimal training configuration for a given computational budget. Our findings not only show that MoE models consistently outperform dense Transformers but also highlight that the efficiency gap between dense and MoE models widens as we scale up the model size and training budget. Furthermore, we demonstrate that the common practice of setting the size of experts in MoE to mirror the feed-forward layer is not optimal at almost any computational budget.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#35270;&#35273;&#26465;&#20214;&#21270;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#35774;&#35745;&#30340;&#20851;&#38190;&#31354;&#38388;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#22871;&#26631;&#20934;&#21270;&#35780;&#20272;&#65292;&#21516;&#26102;&#36824;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#34920;&#31034;&#21644;&#26435;&#34913;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.07865</link><description>&lt;p&gt;
&#36879;&#35270;VLMs&#65306;&#25506;&#32034;&#35270;&#35273;&#26465;&#20214;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#35774;&#35745;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#35270;&#35273;&#26465;&#20214;&#21270;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#35774;&#35745;&#30340;&#20851;&#38190;&#31354;&#38388;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#22871;&#26631;&#20934;&#21270;&#35780;&#20272;&#65292;&#21516;&#26102;&#36824;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#34920;&#31034;&#21644;&#26435;&#34913;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#26465;&#20214;&#21270;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#22312;&#35270;&#35273;&#23545;&#35805;&#12289;&#22330;&#26223;&#29702;&#35299;&#21644;&#26426;&#22120;&#20154;&#20219;&#21153;&#35268;&#21010;&#31561;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#24212;&#29992;&#65292;&#36825;&#31181;&#24212;&#29992;&#20419;&#20351;&#20102;&#20687;LLaVa&#12289;InstructBLIP&#21644;PaLI-3&#31561;&#35768;&#22810;&#26032;&#27169;&#22411;&#30340;&#20986;&#29616;&#12290;&#23613;&#31649;&#26377;&#36825;&#20040;&#22810;&#26032;&#30340;&#21457;&#24067;&#65292;&#20294;&#20851;&#20110;&#22270;&#20687;&#39044;&#22788;&#29702;&#12289;&#26550;&#26500;&#21644;&#20248;&#21270;&#30340;&#20851;&#38190;&#35774;&#35745;&#20915;&#31574;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#24456;&#38590;&#29702;&#35299;&#27169;&#22411;&#24615;&#33021;&#30340;&#22240;&#32032;&#65292;&#36825;&#19968;&#25361;&#25112;&#21448;&#22240;&#32570;&#20047;&#23458;&#35266;&#12289;&#19968;&#33268;&#30340;&#35780;&#20272;&#32780;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65292;&#25105;&#20204;&#39318;&#20808;&#32534;&#21046;&#20102;&#19968;&#22871;&#26631;&#20934;&#21270;&#35780;&#20272;&#65292;&#28085;&#30422;&#20102;&#35270;&#35273;&#38382;&#31572;&#12289;&#20174;&#35821;&#35328;&#20013;&#23450;&#20301;&#29289;&#20307;&#20197;&#21450;&#25506;&#32034;&#24187;&#35273;&#31561;&#23646;&#24615;&#30340;&#30446;&#26631;&#25361;&#25112;&#38598;&#65292;&#36825;&#20123;&#35780;&#20272;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;VLM&#33021;&#21147;&#30340;&#31934;&#32454;&#12289;&#20934;&#30830;&#30340;&#35265;&#35299;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23545;&#20851;&#38190;&#30340;&#35774;&#35745;&#36724;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#30740;&#31350;&#65292;&#21253;&#25324;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#34920;&#31034;&#21644;&#20351;&#29992;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visually-conditioned language models (VLMs) have seen growing adoption in applications such as visual dialogue, scene understanding, and robotic task planning; adoption that has fueled a wealth of new models such as LLaVa, InstructBLIP, and PaLI-3. Despite the volume of new releases, key design decisions around image preprocessing, architecture, and optimization are under-explored, making it challenging to understand what factors account for model performance $-$ a challenge further complicated by the lack of objective, consistent evaluations. To address these gaps, we first compile a suite of standardized evaluations spanning visual question answering, object localization from language, and targeted challenge sets that probe properties such as hallucination; evaluations that provide calibrated, fine-grained insight into a VLM's capabilities. Second, we rigorously investigate VLMs along key design axes, including pretrained visual representations and quantifying the tradeoffs of using 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;LLMs&#21161;&#25163;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#19981;&#20165;&#20165;&#26159;&#30001;&#20110;&#27169;&#22411;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.07862</link><description>&lt;p&gt;
AI&#22686;&#24378;&#39044;&#27979;&#65306;LLM&#21161;&#25163;&#25552;&#39640;&#20154;&#31867;&#39044;&#27979;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;LLMs&#21161;&#25163;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#19981;&#20165;&#20165;&#26159;&#30001;&#20110;&#27169;&#22411;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#19982;&#29978;&#33267;&#36229;&#36807;&#20154;&#31867;&#34920;&#29616;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLMs&#22312;&#39044;&#27979;&#20219;&#21153;&#20013;&#22686;&#24378;&#21028;&#26029;&#21147;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20004;&#20010;GPT-4-Turbo&#21161;&#25163;&#23545;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#65306;&#19968;&#20010;&#26088;&#22312;&#25552;&#20379;&#39640;&#36136;&#37327;&#24314;&#35758;&#65288;&#36229;&#32423;&#39044;&#27979;&#65289;&#65292;&#21478;&#19968;&#20010;&#26088;&#22312;&#36807;&#20110;&#33258;&#20449;&#21644;&#22522;&#26412;&#27010;&#29575;&#24573;&#35270;&#12290;&#21442;&#19982;&#32773;&#65288;N = 991&#65289;&#21487;&#20197;&#22312;&#25972;&#20010;&#30740;&#31350;&#36807;&#31243;&#20013;&#21672;&#35810;&#20182;&#20204;&#34987;&#20998;&#37197;&#30340;LLM&#21161;&#25163;&#65292;&#32780;&#23545;&#29031;&#32452;&#21017;&#20351;&#29992;&#19968;&#20010;&#36739;&#20302;&#32423;&#21035;&#30340;&#27169;&#22411;&#65288;DaVinci-003&#65289;&#65292;&#19981;&#25552;&#20379;&#30452;&#25509;&#30340;&#39044;&#27979;&#25903;&#25345;&#12290;&#25105;&#20204;&#30340;&#27880;&#20876;&#20998;&#26512;&#26174;&#31034;&#65292;LLM&#22686;&#24378;&#26174;&#33879;&#25552;&#39640;&#20102;23%&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#26080;&#35770;&#26159;&#23545;&#20110;&#20219;&#20309;&#19968;&#31181;&#21161;&#25163;&#31867;&#22411;&#65292;&#30456;&#27604;&#20110;&#23545;&#29031;&#32452;&#12290;&#36825;&#31181;&#25913;&#36827;&#21457;&#29983;&#22312;&#36229;&#32423;&#39044;&#27979;&#21161;&#25163;&#22312;&#39044;&#27979;&#20013;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#34920;&#26126;&#22686;&#24378;&#30340;&#25928;&#30410;&#19981;&#20165;&#20165;&#26159;&#30001;&#20110;&#27169;&#22411;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) show impressive capabilities, matching and sometimes exceeding human performance in many domains. This study explores the potential of LLMs to augment judgement in forecasting tasks. We evaluated the impact on forecasting accuracy of two GPT-4-Turbo assistants: one designed to provide high-quality advice ('superforecasting'), and the other designed to be overconfident and base-rate-neglecting. Participants (N = 991) had the option to consult their assigned LLM assistant throughout the study, in contrast to a control group that used a less advanced model (DaVinci-003) without direct forecasting support. Our preregistered analyses reveal that LLM augmentation significantly enhances forecasting accuracy by 23% across both types of assistants, compared to the control group. This improvement occurs despite the superforecasting assistant's higher accuracy in predictions, indicating the augmentation's benefit is not solely due to model prediction accuracy. Explora
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20174;&#35770;&#25991;&#31454;&#26631;&#20013;&#26816;&#27979;&#21246;&#32467;&#22242;&#20307;&#65292;&#20197;&#35299;&#20915;&#21516;&#34892;&#35780;&#23457;&#31995;&#32479;&#20013;&#30340;&#27450;&#35784;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.07860</link><description>&lt;p&gt;
&#35770;&#25991;&#31454;&#26631;&#20013;&#26816;&#27979;&#20316;&#32773;&#19982;&#23457;&#31295;&#20154;&#21246;&#32467;&#30340;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Detection of Reviewer-Author Collusion Rings From Paper Bidding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20174;&#35770;&#25991;&#31454;&#26631;&#20013;&#26816;&#27979;&#21246;&#32467;&#22242;&#20307;&#65292;&#20197;&#35299;&#20915;&#21516;&#34892;&#35780;&#23457;&#31995;&#32479;&#20013;&#30340;&#27450;&#35784;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#31185;&#23398;&#20250;&#35758;&#30340;&#21516;&#34892;&#35780;&#23457;&#31995;&#32479;&#38754;&#20020;&#30340;&#20027;&#35201;&#23041;&#32961;&#26159;&#23457;&#31295;&#20154;&#20043;&#38388;&#23384;&#22312;&#30340;"&#21246;&#32467;&#22242;&#20307;"&#12290;&#22312;&#36825;&#31181;&#21246;&#32467;&#22242;&#20307;&#20013;&#65292;&#25552;&#20132;&#20102;&#33258;&#24049;&#30340;&#35770;&#25991;&#30340;&#23457;&#31295;&#20154;&#21512;&#20316;&#65292;&#35797;&#22270;&#36890;&#36807;&#25805;&#32437;&#20250;&#35758;&#30340;&#35770;&#25991;&#20998;&#37197;&#65292;&#20197;&#20415;&#34987;&#25351;&#27966;&#20026;&#24444;&#27492;&#35770;&#25991;&#30340;&#23457;&#31295;&#20154;&#12290;&#32437;&#35266;&#29616;&#26377;&#30340;&#30740;&#31350;&#65292;&#34429;&#28982;&#24050;&#32463;&#21457;&#23637;&#20986;&#20102;&#26377;&#25928;&#30340;&#25216;&#26415;&#26469;&#26816;&#27979;&#20854;&#20182;&#31181;&#31867;&#30340;&#27450;&#35784;&#34892;&#20026;&#65292;&#20294;&#23578;&#26410;&#26377;&#30740;&#31350;&#35777;&#26126;&#26816;&#27979;&#21246;&#32467;&#22242;&#20307;&#30340;&#21487;&#33021;&#24615;&#12290;&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#22914;&#20309;&#20174;&#35770;&#25991;&#31454;&#26631;&#20013;&#26816;&#27979;&#21246;&#32467;&#22242;&#20307;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
A major threat to the peer-review systems of computer science conferences is the existence of "collusion rings" between reviewers. In such collusion rings, reviewers who have also submitted their own papers to the conference work together to manipulate the conference's paper assignment, with the aim of being assigned to review each other's papers. The most straightforward way that colluding reviewers can manipulate the paper assignment is by indicating their interest in each other's papers through strategic paper bidding. One potential approach to solve this important problem would be to detect the colluding reviewers from their manipulated bids, after which the conference can take appropriate action. While prior work has has developed effective techniques to detect other kinds of fraud, no research has yet established that detecting collusion rings is even possible. In this work, we tackle the question of whether it is feasible to detect collusion rings from the paper bidding. To answ
&lt;/p&gt;</description></item><item><title>Lissard&#26159;&#19968;&#20010;&#21253;&#21547;&#19971;&#20010;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#22788;&#29702;&#21644;&#29983;&#25104;&#21508;&#31181;&#24207;&#21015;&#38271;&#24230;&#30340;&#33021;&#21147;&#65292;&#38656;&#35201;&#37325;&#22797;&#30340;&#36807;&#31243;&#25191;&#34892;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#38543;&#30528;&#24207;&#21015;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#25152;&#26377;&#27169;&#22411;&#30340;&#24615;&#33021;&#37117;&#21576;&#19968;&#33268;&#19979;&#38477;&#36235;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.07859</link><description>&lt;p&gt;
Lissard&#65306;&#38271;&#32780;&#31616;&#21333;&#30340;&#39034;&#24207;&#25512;&#29702;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Lissard: Long and Simple Sequential Reasoning Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07859
&lt;/p&gt;
&lt;p&gt;
Lissard&#26159;&#19968;&#20010;&#21253;&#21547;&#19971;&#20010;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#22788;&#29702;&#21644;&#29983;&#25104;&#21508;&#31181;&#24207;&#21015;&#38271;&#24230;&#30340;&#33021;&#21147;&#65292;&#38656;&#35201;&#37325;&#22797;&#30340;&#36807;&#31243;&#25191;&#34892;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#38543;&#30528;&#24207;&#21015;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#25152;&#26377;&#27169;&#22411;&#30340;&#24615;&#33021;&#37117;&#21576;&#19968;&#33268;&#19979;&#38477;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#29616;&#22312;&#33021;&#22815;&#35299;&#20915;&#38656;&#35201;&#22788;&#29702;&#25968;&#21313;&#19975;&#20010;&#26631;&#35760;&#30340;&#38271;&#24207;&#21015;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#38656;&#35201;&#37325;&#22797;&#20351;&#29992;&#31616;&#21333;&#35268;&#21017;&#30340;&#20219;&#21153;&#19978;&#24120;&#24120;&#22833;&#36133;&#65292;&#29978;&#33267;&#22312;&#27604;&#35757;&#32451;&#20013;&#30475;&#21040;&#30340;&#24207;&#21015;&#35201;&#30701;&#24471;&#22810;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#20363;&#22914;&#65292;&#26368;&#20808;&#36827;&#30340;LLMs&#21487;&#20197;&#22312;&#20004;&#20010;&#21015;&#34920;&#20013;&#25214;&#21040;&#20849;&#21516;&#39033;&#65292;&#21015;&#34920;&#20013;&#30340;&#39033;&#26368;&#22810;&#21487;&#36798;20&#20010;&#65292;&#20294;&#26159;&#24403;&#21015;&#34920;&#20013;&#30340;&#39033;&#36798;&#21040;80&#20010;&#26102;&#65292;&#23427;&#20204;&#20250;&#22833;&#36133;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Lissard&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#19971;&#20010;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#27169;&#22411;&#22788;&#29702;&#21644;&#29983;&#25104;&#21508;&#31181;&#24207;&#21015;&#38271;&#24230;&#30340;&#33021;&#21147;&#65292;&#38656;&#35201;&#37325;&#22797;&#30340;&#36807;&#31243;&#25191;&#34892;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#24320;&#28304;&#27169;&#22411;&#65288;Mistral-7B&#21644;Mixtral-8x7B&#65289;&#21644;&#19987;&#26377;&#27169;&#22411;&#65288;GPT-3.5&#21644;GPT-4&#65289;&#65292;&#32467;&#26524;&#26174;&#31034;&#38543;&#30528;&#24207;&#21015;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#25152;&#26377;&#27169;&#22411;&#30340;&#24615;&#33021;&#37117;&#21576;&#19968;&#33268;&#19979;&#38477;&#36235;&#21183;&#12290;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#21487;&#22312;https://github.com/unicamp-dl/Lissard&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models are now capable of solving tasks that require dealing with long sequences consisting of hundreds of thousands of tokens. However, they often fail on tasks that require repetitive use of simple rules, even on sequences that are much shorter than those seen during training. For example, state-of-the-art LLMs can find common items in two lists with up to 20 items but fail when lists have 80 items. In this paper, we introduce Lissard, a benchmark comprising seven tasks whose goal is to assess the ability of models to process and generate wide-range sequence lengths, requiring repetitive procedural execution. Our evaluation of open-source (Mistral-7B and Mixtral-8x7B) and proprietary models (GPT-3.5 and GPT-4) show a consistent decline in performance across all models as the complexity of the sequence increases. The datasets and code are available at https://github.com/unicamp-dl/Lissard
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#20351;&#29992;&#26080;&#30417;&#30563;&#24230;&#37327;&#27169;&#22359;&#24615;&#20248;&#21270;GNN&#36827;&#34892;&#33410;&#28857;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#19988;&#26080;&#38656;&#19982;&#22522;&#20934;&#20540;&#36827;&#34892;&#27604;&#36739;&#12290;&#22312;&#35774;&#35745;&#21512;&#25104;&#23454;&#39564;&#30340;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07845</link><description>&lt;p&gt;
&#20351;&#29992;&#26080;&#30417;&#30563;&#24230;&#37327;&#20248;&#21270;GNN&#36827;&#34892;&#33410;&#28857;&#32858;&#31867;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Investigation into Using Unsupervised Metrics to Optimise GNNs for Node Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#20351;&#29992;&#26080;&#30417;&#30563;&#24230;&#37327;&#27169;&#22359;&#24615;&#20248;&#21270;GNN&#36827;&#34892;&#33410;&#28857;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#19988;&#26080;&#38656;&#19982;&#22522;&#20934;&#20540;&#36827;&#34892;&#27604;&#36739;&#12290;&#22312;&#35774;&#35745;&#21512;&#25104;&#23454;&#39564;&#30340;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#29305;&#24449;&#21644;&#36830;&#25509;&#20449;&#24687;&#30340;&#20108;&#20803;&#24615;&#26469;&#35757;&#32451;&#20197;&#26816;&#27979;&#22270;&#20013;&#30340;&#31038;&#21306;&#12290;&#30446;&#21069;&#65292;&#20248;&#21270;GNN&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#20351;&#29992;&#19982;&#22522;&#20934;&#20540;&#30340;&#27604;&#36739;&#26469;&#36827;&#34892;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#27169;&#22411;&#36873;&#25321;&#12290;&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#20165;&#36890;&#36807;&#20248;&#21270;&#27169;&#22359;&#24615;&#65292;&#21487;&#20197;&#20351;&#29992;GNN&#23558;&#33410;&#28857;&#32858;&#31867;&#25104;&#31038;&#21306;&#65292;&#32780;&#26080;&#38656;&#19982;&#22522;&#20934;&#20540;&#36827;&#34892;&#27604;&#36739;&#12290;&#23613;&#31649;&#27169;&#22359;&#24615;&#26159;&#19968;&#31181;&#22270;&#20998;&#21306;&#36136;&#37327;&#24230;&#37327;&#65292;&#25105;&#20204;&#35777;&#26126;&#36825;&#20063;&#21487;&#20197;&#29992;&#20110;&#20248;&#21270;&#21516;&#26102;&#32534;&#30721;&#29305;&#24449;&#30340;GNN&#65292;&#24182;&#19988;&#19981;&#20250;&#38477;&#20302;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#26080;&#30417;&#30563;&#24230;&#37327;&#24615;&#33021;&#26159;&#21542;&#33021;&#22815;&#39044;&#27979;&#22522;&#20934;&#20540;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#25506;&#31350;&#20026;&#20160;&#20040;&#21487;&#20197;&#20351;&#29992;&#27169;&#22359;&#24615;&#20248;&#21270;GNN&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20123;&#21512;&#25104;&#23454;&#39564;&#26469;&#23637;&#31034;&#36825;&#31181;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#36825;&#20123;&#21512;&#25104;&#22270;&#34920;&#26126;&#20854;&#22312;&#19981;&#21516;&#12289;&#38543;&#26426;&#21644;&#38646;&#20449;&#24687;&#31354;&#38388;&#20998;&#21306;&#20013;&#30340;&#24403;&#21069;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) can be trained to detect communities within a graph by learning from the duality of feature and connectivity information. Currently, the common approach for optimisation of GNNs is to use comparisons to ground-truth for hyperparameter tuning and model selection. In this work, we show that nodes can be clustered into communities with GNNs by solely optimising for modularity, without any comparison to ground-truth. Although modularity is a graph partitioning quality metric, we show that this can be used to optimise GNNs that also encode features without a drop in performance. We take it a step further and also study whether the unsupervised metric performance can predict ground-truth performance. To investigate why modularity can be used to optimise GNNs, we design synthetic experiments that show the limitations of this approach. The synthetic graphs are created to highlight current capabilities in distinct, random and zero information space partitions in att
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23616;&#37096;&#26368;&#20248;&#32593;&#32476;&#20998;&#26512;&#19981;&#21516;&#32534;&#30721;&#26041;&#27861;&#22312;&#28436;&#21270;&#26426;&#22120;&#20154;&#36816;&#21160;&#20219;&#21153;&#20013;&#20135;&#29983;&#30340;&#36866;&#24212;&#24230;&#26223;&#35266;&#32467;&#26500;&#65292;&#20026;&#25628;&#32034;&#36807;&#31243;&#25552;&#20379;&#26032;&#30340;&#21551;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.07822</link><description>&lt;p&gt;
&#36890;&#36807;&#23616;&#37096;&#26368;&#20248;&#32593;&#32476;&#29702;&#35299;&#24418;&#24577;&#36827;&#21270;&#20013;&#30340;&#36866;&#24212;&#24230;&#26223;&#35266;
&lt;/p&gt;
&lt;p&gt;
Understanding fitness landscapes in morpho-evolution via local optima networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07822
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23616;&#37096;&#26368;&#20248;&#32593;&#32476;&#20998;&#26512;&#19981;&#21516;&#32534;&#30721;&#26041;&#27861;&#22312;&#28436;&#21270;&#26426;&#22120;&#20154;&#36816;&#21160;&#20219;&#21153;&#20013;&#20135;&#29983;&#30340;&#36866;&#24212;&#24230;&#26223;&#35266;&#32467;&#26500;&#65292;&#20026;&#25628;&#32034;&#36807;&#31243;&#25552;&#20379;&#26032;&#30340;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24418;&#24577;&#36827;&#21270;&#26159;&#25351;&#22312;&#32473;&#23450;&#20219;&#21153;&#21644;&#29615;&#22659;&#30340;&#24773;&#20917;&#19979;&#65292;&#21516;&#26102;&#20248;&#21270;&#26426;&#22120;&#20154;&#35774;&#35745;&#21644;&#25511;&#21046;&#22120;&#20197;&#26368;&#22823;&#21270;&#24615;&#33021;&#12290;&#35768;&#22810;&#22522;&#22240;&#32534;&#30721;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#65292;&#33021;&#22815;&#34920;&#31034;&#35774;&#35745;&#21644;&#25511;&#21046;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20197;&#30446;&#26631;&#20989;&#25968;&#30340;&#24615;&#33021;&#21644;&#35780;&#20272;&#30340;&#35774;&#35745;&#22810;&#26679;&#24615;&#20026;&#25351;&#26631;&#65292;&#23545;&#32534;&#30721;&#26041;&#27861;&#36827;&#34892;&#20102;&#32463;&#39564;&#27604;&#36739;&#65292;&#20294;&#23578;&#26410;&#23581;&#35797;&#35299;&#37322;&#35266;&#23519;&#21040;&#30340;&#32467;&#26524;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#24212;&#29992;&#23616;&#37096;&#26368;&#20248;&#32593;&#32476;&#65288;LON&#65289;&#20998;&#26512;&#65292;&#35843;&#26597;&#19977;&#31181;&#19981;&#21516;&#32534;&#30721;&#26041;&#27861;&#22312;&#28436;&#21270;&#26426;&#22120;&#20154;&#36816;&#21160;&#20219;&#21153;&#20013;&#20135;&#29983;&#30340;&#36866;&#24212;&#24230;&#26223;&#35266;&#32467;&#26500;&#65292;&#20026;&#25628;&#32034;&#36807;&#31243;&#22312;&#19981;&#21516;&#36866;&#24212;&#24230;&#26223;&#35266;&#19978;&#30340;&#36941;&#21382;&#25552;&#20379;&#26032;&#30340;&#21551;&#31034;&#12290;&#36825;&#26159;LON&#20998;&#26512;&#39318;&#27425;&#24212;&#29992;&#20110;&#24418;&#24577;&#36827;&#21270;&#39046;&#22495;&#65292;&#23613;&#31649;&#22312;&#32452;&#21512;&#20248;&#21270;&#39046;&#22495;&#20013;&#38750;&#24120;&#21463;&#27426;&#36814;&#65307;&#30740;&#31350;&#32467;&#26524;&#23558;&#26377;&#21161;&#20110;&#35774;&#35745;&#26032;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Morpho-evolution (ME) refers to the simultaneous optimisation of a robot's design and controller to maximise performance given a task and environment. Many genetic encodings have been proposed which are capable of representing design and control. Previous research has provided empirical comparisons between encodings in terms of their performance with respect to an objective function and the diversity of designs that are evaluated, however there has been no attempt to explain the observed findings. We address this by applying Local Optima Network (LON) analysis to investigate the structure of the fitness landscapes induced by three different encodings when evolving a robot for a locomotion task, shedding new light on the ease by which different fitness landscapes can be traversed by a search process. This is the first time LON analysis has been applied in the field of ME despite its popularity in combinatorial optimisation domains; the findings will facilitate design of new algorithms o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;&#38646;&#38454;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20013;&#30340;&#24212;&#29992;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#38646;&#38454;&#26799;&#24230;&#26469;&#36991;&#20813;&#20256;&#32479;&#20248;&#21270;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#29942;&#39048;&#65292;&#23454;&#29616;&#20102;&#22312;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#21487;&#25193;&#23637;&#24615;&#20043;&#38388;&#30340;&#33391;&#22909;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.07818</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;&#24046;&#20998;&#38544;&#31169;&#38646;&#38454;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Zeroth-Order Methods for Scalable Large Language Model Finetuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;&#38646;&#38454;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20013;&#30340;&#24212;&#29992;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#38646;&#38454;&#26799;&#24230;&#26469;&#36991;&#20813;&#20256;&#32479;&#20248;&#21270;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#29942;&#39048;&#65292;&#23454;&#29616;&#20102;&#22312;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#21487;&#25193;&#23637;&#24615;&#20043;&#38388;&#30340;&#33391;&#22909;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29305;&#23450;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#26159;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#22823;&#33021;&#21147;&#36827;&#34892;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#24191;&#27867;&#25509;&#21463;&#30340;&#33539;&#20363;&#12290;&#30001;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;&#26222;&#21450;&#20197;&#21450;&#19982;&#20043;&#30456;&#20851;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#24046;&#20998;&#38544;&#31169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20197;&#20445;&#25252;&#29305;&#23450;&#20219;&#21153;&#25968;&#25454;&#38598;&#30340;&#38544;&#31169;&#12290;&#24046;&#20998;&#38544;&#31169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#30340;&#35774;&#35745;&#26680;&#24515;&#26159;&#22312;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#21487;&#25193;&#23637;&#24615;&#20043;&#38388;&#36798;&#21040;&#28385;&#24847;&#30340;&#26435;&#34913;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#26159;&#22522;&#20110;DP-SGD&#30340;&#21019;&#26032;&#24615;&#24037;&#20316;&#12290;&#23613;&#31649;&#23558;DP-SGD&#30340;&#21487;&#25193;&#23637;&#24615;&#25512;&#21040;&#20102;&#26497;&#38480;&#65292;&#20294;&#22522;&#20110;DP-SGD&#30340;&#24494;&#35843;&#26041;&#27861;&#19981;&#24184;&#22320;&#21463;&#21040;&#20102;SGD&#22266;&#26377;&#20302;&#25928;&#29575;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;DP&#38646;&#38454;&#26041;&#27861;&#22312;LLM&#39044;&#35757;&#32451;&#20013;&#30340;&#28508;&#21147;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#29992;&#26356;&#39640;&#25928;&#30340;&#38646;&#38454;&#26799;&#24230;&#26469;&#36817;&#20284;&#26799;&#24230;&#65292;&#36991;&#20813;&#20102;SGD&#30340;&#21487;&#25193;&#23637;&#24615;&#29942;&#39048;&#12290;&#19982;&#23558;&#38646;&#38454;&#26041;&#27861;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#36827;&#34892;&#22788;&#29702;&#19981;&#21516;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#21106;&#25509;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20197;&#38750;&#24120;&#25509;&#36817;&#30340;&#26041;&#24335;&#27169;&#25311;DP-SGD&#30340;&#22522;&#26412;&#25805;&#20316;&#65292;&#28982;&#21518;&#21033;&#29992;&#38646;&#38454;&#20248;&#21270;&#26041;&#27861;&#26469;&#36817;&#20284;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finetuning on task-specific datasets is a widely-embraced paradigm of harnessing the powerful capability of pretrained LLMs for various downstream tasks. Due to the popularity of LLMs finetuning and its accompanying privacy concerns, differentially private (DP) finetuning of pretrained LLMs has garnered increasing attention to safeguarding the privacy of task-specific datasets. Lying at the design core of DP LLM finetuning methods is the satisfactory tradeoff between privacy, utility, and scalability. Most existing methods build upon the seminal work of DP-SGD. Despite pushing the scalability of DP-SGD to its limit, DP-SGD-based finetuning methods are unfortunately limited by the inherent inefficiency of SGD. In this paper, we investigate the potential of DP zeroth-order methods for LLM pretraining, which avoids the scalability bottleneck of SGD by approximating the gradient with the more efficient zeroth-order gradient. Rather than treating the zeroth-order method as a drop-in replace
&lt;/p&gt;</description></item><item><title>PBADet&#26159;&#19968;&#31181;&#29992;&#20110;&#37096;&#20214;-&#36523;&#20307;&#20851;&#32852;&#30340;&#21333;&#38454;&#27573;&#26080;&#38170;&#28857;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#37096;&#20214;-&#36523;&#20307;&#20013;&#24515;&#20559;&#31227;&#37327;&#26469;&#26377;&#25928;&#34920;&#36798;&#37096;&#20214;&#19982;&#36523;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20855;&#26377;&#39640;&#25928;&#12289;&#20934;&#30830;&#21644;&#31283;&#20581;&#30340;&#29305;&#28857;&#65292;&#30456;&#27604;&#29616;&#26377;&#25216;&#26415;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.07814</link><description>&lt;p&gt;
PBADet&#65306;&#19968;&#31181;&#29992;&#20110;&#37096;&#20214;-&#36523;&#20307;&#20851;&#32852;&#30340;&#21333;&#38454;&#27573;&#26080;&#38170;&#28857;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PBADet: A One-Stage Anchor-Free Approach for Part-Body Association
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07814
&lt;/p&gt;
&lt;p&gt;
PBADet&#26159;&#19968;&#31181;&#29992;&#20110;&#37096;&#20214;-&#36523;&#20307;&#20851;&#32852;&#30340;&#21333;&#38454;&#27573;&#26080;&#38170;&#28857;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#37096;&#20214;-&#36523;&#20307;&#20013;&#24515;&#20559;&#31227;&#37327;&#26469;&#26377;&#25928;&#34920;&#36798;&#37096;&#20214;&#19982;&#36523;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20855;&#26377;&#39640;&#25928;&#12289;&#20934;&#30830;&#21644;&#31283;&#20581;&#30340;&#29305;&#28857;&#65292;&#30456;&#27604;&#29616;&#26377;&#25216;&#26415;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20307;&#37096;&#20214;&#65288;&#22914;&#25163;&#12289;&#33080;&#65289;&#30340;&#26816;&#27979;&#21450;&#20854;&#19982;&#20010;&#20154;&#30340;&#27491;&#30830;&#20851;&#32852;&#26159;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#65292;&#20363;&#22914;&#29992;&#20110;&#26222;&#36866;&#20154;&#26426;&#30028;&#38754;&#21644;&#21160;&#20316;&#35782;&#21035;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#22810;&#38454;&#27573;&#27969;&#31243;&#65292;&#20381;&#36182;&#32321;&#29712;&#30340;&#22522;&#20110;&#38170;&#28857;&#30340;&#31995;&#32479;&#65292;&#25110;&#32773;&#22312;&#22788;&#29702;&#36739;&#22823;&#30340;&#37096;&#20998;&#38598;&#26102;&#25193;&#23637;&#24615;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PBADet&#30340;&#26032;&#22411;&#21333;&#38454;&#27573;&#12289;&#26080;&#38170;&#28857;&#30340;&#37096;&#20214;-&#36523;&#20307;&#20851;&#32852;&#26816;&#27979;&#26041;&#27861;&#12290;&#22522;&#20110;&#22810;&#23610;&#24230;&#29305;&#24449;&#22270;&#30340;&#26080;&#38170;&#28857;&#29289;&#20307;&#34920;&#31034;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21333;&#19968;&#30340;&#37096;&#20214;-&#36523;&#20307;&#20013;&#24515;&#20559;&#31227;&#37327;&#65292;&#26377;&#25928;&#22320;&#34920;&#36798;&#20102;&#37096;&#20214;&#19982;&#20854;&#29238;&#36523;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#35774;&#35745;&#20855;&#26377;&#24456;&#24378;&#30340;&#36890;&#29992;&#24615;&#65292;&#33021;&#22815;&#22788;&#29702;&#22810;&#20010;&#37096;&#20214;-&#36523;&#20307;&#20851;&#32852;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#26816;&#27979;&#20934;&#30830;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#20840;&#38754;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#19981;&#20165;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#32780;&#19988;&#36824;&#25552;&#20379;&#20102;&#26356;&#21152;&#31616;&#21270;&#21644;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The detection of human parts (e.g., hands, face) and their correct association with individuals is an essential task, e.g., for ubiquitous human-machine interfaces and action recognition. Traditional methods often employ multi-stage processes, rely on cumbersome anchor-based systems, or do not scale well to larger part sets. This paper presents PBADet, a novel one-stage, anchor-free approach for part-body association detection. Building upon the anchor-free object representation across multi-scale feature maps, we introduce a singular part-to-body center offset that effectively encapsulates the relationship between parts and their parent bodies. Our design is inherently versatile and capable of managing multiple parts-to-body associations without compromising on detection accuracy or robustness. Comprehensive experiments on various datasets underscore the efficacy of our approach, which not only outperforms existing state-of-the-art techniques but also offers a more streamlined and eff
&lt;/p&gt;</description></item><item><title>&#26816;&#32034;&#22686;&#24378;&#24605;&#32500;&#36807;&#31243;&#65288;RATP&#65289;&#36890;&#36807;&#22810;&#27493;&#20915;&#31574;&#21644;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65292;&#20197;&#21450;Q&#20540;&#20272;&#35745;&#22120;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38544;&#31169;&#12289;&#20135;&#29983;&#24187;&#35273;&#21644;&#22788;&#29702;&#38271;&#25991;&#26412;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#22788;&#29702;&#31169;&#20154;&#25968;&#25454;&#30340;&#38382;&#31572;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;50%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.07812</link><description>&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#30340;&#24605;&#32500;&#36807;&#31243;&#20316;&#20026;&#24207;&#21015;&#20915;&#31574;&#21046;&#23450;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Augmented Thought Process as Sequential Decision Making
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07812
&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#24605;&#32500;&#36807;&#31243;&#65288;RATP&#65289;&#36890;&#36807;&#22810;&#27493;&#20915;&#31574;&#21644;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65292;&#20197;&#21450;Q&#20540;&#20272;&#35745;&#22120;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38544;&#31169;&#12289;&#20135;&#29983;&#24187;&#35273;&#21644;&#22788;&#29702;&#38271;&#25991;&#26412;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#22788;&#29702;&#31169;&#20154;&#25968;&#25454;&#30340;&#38382;&#31572;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;50%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#23637;&#31034;&#20102;&#20854;&#24378;&#22823;&#30340;&#36741;&#21161;&#20154;&#31867;&#24182;&#23637;&#29616;&#20986;"&#26234;&#33021;&#30340;&#28779;&#33457;"&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20960;&#20010;&#24320;&#25918;&#25361;&#25112;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#65306;&#22914;&#23545;&#38544;&#31169;&#30340;&#20851;&#27880;&#12289;&#20542;&#21521;&#20110;&#20135;&#29983;&#24187;&#35273;&#12289;&#38590;&#20197;&#22788;&#29702;&#38271;&#25991;&#26412;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#26816;&#32034;&#22686;&#24378;&#24605;&#32500;&#36807;&#31243;(RATP)&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#36890;&#36807;&#33719;&#21462;&#22806;&#37096;&#30693;&#35782;&#65292;RATP&#23558;LLM&#30340;&#24605;&#32771;&#29983;&#25104;&#36807;&#31243;&#23450;&#24335;&#20026;&#22810;&#27493;&#20915;&#31574;&#36807;&#31243;&#12290;&#20026;&#20102;&#20248;&#21270;&#36825;&#31181;&#24605;&#32771;&#36807;&#31243;&#65292;RATP&#21033;&#29992;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65292;&#24182;&#23398;&#20064;&#20102;&#19968;&#20010;Q&#20540;&#20272;&#35745;&#22120;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25512;&#29702;&#12290;&#22312;&#22788;&#29702;&#20855;&#26377;&#31169;&#20154;&#25968;&#25454;&#30340;&#38382;&#31572;&#20219;&#21153;&#26102;&#65292;LLM&#35757;&#32451;&#26041;&#27861;&#21463;&#21040;&#20262;&#29702;&#21644;&#23433;&#20840;&#38382;&#39064;&#30340;&#38480;&#21046;&#12290;RATP&#22312;&#19978;&#19979;&#25991;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#23454;&#29616;&#20102;50%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated their strong ability to assist people and show "sparks of intelligence". However, several open challenges hinder their wider application: such as concerns over privacy, tendencies to produce hallucinations, and difficulties in handling long contexts. In this work, we address those challenges by introducing the Retrieval-Augmented Thought Process (RATP). Given access to external knowledge, RATP formulates the thought generation of LLMs as a multiple-step decision process. To optimize such a thought process, RATP leverages Monte-Carlo Tree Search, and learns a Q-value estimator that permits cost-efficient inference. In addressing the task of question-answering with private data, where ethical and security concerns limit LLM training methods, RATP achieves a 50% improvement over existing in-context retrieval-augmented language models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#21270;&#30340;&#35268;&#21010;&#29615;&#22659;&#37325;&#26032;&#35774;&#35745;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#19981;&#21463;&#24230;&#37327;&#24433;&#21709;&#65292;&#24182;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#30446;&#26631;&#21644;&#25351;&#26631;&#12290;</title><link>https://arxiv.org/abs/2402.07799</link><description>&lt;p&gt;
&#24191;&#20041;&#21270;&#35268;&#21010;&#29615;&#22659;&#37325;&#26032;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Generalising Planning Environment Redesign
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#21270;&#30340;&#35268;&#21010;&#29615;&#22659;&#37325;&#26032;&#35774;&#35745;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#19981;&#21463;&#24230;&#37327;&#24433;&#21709;&#65292;&#24182;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#30446;&#26631;&#21644;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29615;&#22659;&#35774;&#35745;&#20013;&#65292;&#19968;&#20010;&#24863;&#20852;&#36259;&#30340;&#26041;&#19982;&#21478;&#19968;&#20010;&#20195;&#29702;&#36890;&#36807;&#23545;&#29615;&#22659;&#36827;&#34892;&#25913;&#21464;&#26469;&#24433;&#21709;&#20854;&#20915;&#31574;&#12290;&#22823;&#37096;&#20998;&#20851;&#20110;&#35268;&#21010;&#29615;&#22659;&#37325;&#26032;&#35774;&#35745;&#30340;&#30740;&#31350;&#20551;&#35774;&#24863;&#20852;&#36259;&#30340;&#26041;&#30340;&#30446;&#26631;&#26159;&#20419;&#36827;&#30446;&#26631;&#21644;&#35745;&#21010;&#30340;&#35782;&#21035;&#65292;&#24182;&#22312;&#29615;&#22659;&#20462;&#25913;&#31354;&#38388;&#20013;&#25628;&#32034;&#20197;&#25214;&#21040;&#31616;&#21270;&#36825;&#20123;&#20219;&#21153;&#24182;&#20248;&#21270;&#29305;&#23450;&#24230;&#37327;&#26631;&#20934;&#30340;&#26368;&#23567;&#19968;&#32452;&#21464;&#21270;&#12290;&#36825;&#20010;&#25628;&#32034;&#31354;&#38388;&#36890;&#24120;&#38590;&#20197;&#22788;&#29702;&#65292;&#22240;&#27492;&#29616;&#26377;&#26041;&#27861;&#35774;&#35745;&#20102;&#24230;&#37327;&#30456;&#20851;&#30340;&#20462;&#21098;&#25216;&#26415;&#20197;&#26356;&#39640;&#25928;&#22320;&#36827;&#34892;&#25628;&#32034;&#12290;&#36825;&#23548;&#33268;&#26041;&#27861;&#26080;&#27861;&#22312;&#19981;&#21516;&#30340;&#30446;&#26631;&#21644;/&#25110;&#25351;&#26631;&#19978;&#36827;&#34892;&#27867;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#24863;&#20852;&#36259;&#30340;&#26041;&#30340;&#30446;&#26631;&#21644;&#25351;&#26631;&#19981;&#19968;&#23450;&#19982;&#35782;&#21035;&#20195;&#29702;&#30340;&#30446;&#26631;&#25110;&#35745;&#21010;&#30456;&#20851;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#24191;&#20041;&#21270;&#35268;&#21010;&#29615;&#22659;&#37325;&#26032;&#35774;&#35745;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#29615;&#22659;&#37325;&#26032;&#35774;&#35745;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#21463;&#24230;&#37327;&#24433;&#21709;&#65292;&#24182;&#21033;&#29992;&#20102;&#26368;&#26032;&#30340;&#39030;&#23618;&#30740;&#31350;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Environment Design, one interested party seeks to affect another agent's decisions by applying changes to the environment. Most research on planning environment (re)design assumes the interested party's objective is to facilitate the recognition of goals and plans, and search over the space of environment modifications to find the minimal set of changes that simplify those tasks and optimise a particular metric. This search space is usually intractable, so existing approaches devise metric-dependent pruning techniques for performing search more efficiently. This results in approaches that are not able to generalise across different objectives and/or metrics. In this paper, we argue that the interested party could have objectives and metrics that are not necessarily related to recognising agents' goals or plans. Thus, to generalise the task of Planning Environment Redesign, we develop a general environment redesign approach that is metric-agnostic and leverages recent research on top
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22810;&#31890;&#24230;&#34701;&#21512;&#32593;&#32476;&#65288;EMGF&#65289;&#29992;&#20110;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#65292;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#30340;&#35821;&#35328;&#21644;&#32467;&#26500;&#29305;&#24449;&#65292;&#21253;&#25324;&#21477;&#27861;&#20381;&#36182;&#12289;&#32452;&#25104;&#12289;&#27880;&#24847;&#21147;&#35821;&#20041;&#21644;&#22806;&#37096;&#30693;&#35782;&#22270;&#35889;&#31561;&#65292;&#26469;&#25552;&#39640;&#24773;&#24863;&#20998;&#26512;&#30340;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07787</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#22810;&#31890;&#24230;&#34701;&#21512;&#32593;&#32476;&#29992;&#20110;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Extensible Multi-Granularity Fusion Network for Aspect-based Sentiment Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07787
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22810;&#31890;&#24230;&#34701;&#21512;&#32593;&#32476;&#65288;EMGF&#65289;&#29992;&#20110;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#65292;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#30340;&#35821;&#35328;&#21644;&#32467;&#26500;&#29305;&#24449;&#65292;&#21253;&#25324;&#21477;&#27861;&#20381;&#36182;&#12289;&#32452;&#25104;&#12289;&#27880;&#24847;&#21147;&#35821;&#20041;&#21644;&#22806;&#37096;&#30693;&#35782;&#22270;&#35889;&#31561;&#65292;&#26469;&#25552;&#39640;&#24773;&#24863;&#20998;&#26512;&#30340;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#65288;ABSA&#65289;&#35780;&#20272;&#25991;&#26412;&#20013;&#30340;&#24773;&#24863;&#34920;&#36798;&#20197;&#29702;&#35299;&#24773;&#24863;&#20449;&#24687;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#25972;&#21512;&#20102;&#22806;&#37096;&#30693;&#35782;&#65292;&#22914;&#30693;&#35782;&#22270;&#35889;&#65292;&#20197;&#21152;&#24378;ABSA&#27169;&#22411;&#20013;&#30340;&#35821;&#20041;&#29305;&#24449;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20381;&#36182;&#21644;&#32452;&#25104;&#26641;&#19978;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#36827;&#34892;&#21477;&#27861;&#20998;&#26512;&#12290;&#38543;&#30528;ABSA&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#21019;&#26032;&#30340;&#35821;&#35328;&#21644;&#32467;&#26500;&#29305;&#24449;&#34987;&#34701;&#20837;&#20854;&#20013;&#65288;&#20363;&#22914;&#28508;&#22312;&#22270;&#65289;&#65292;&#20294;&#36825;&#20063;&#24341;&#20837;&#20102;&#22797;&#26434;&#24615;&#21644;&#28151;&#28102;&#12290;&#30446;&#21069;&#65292;&#23578;&#19981;&#23384;&#22312;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#22810;&#26679;&#24615;&#30340;&#35821;&#35328;&#21644;&#32467;&#26500;&#29305;&#24449;&#38598;&#25104;&#21040;ABSA&#20013;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#21487;&#25193;&#23637;&#30340;&#22810;&#31890;&#24230;&#34701;&#21512;&#65288;EMGF&#65289;&#32593;&#32476;&#65292;&#23427;&#25972;&#21512;&#20102;&#26469;&#33258;&#21477;&#27861;&#20381;&#36182;&#21644;&#32452;&#25104;&#12289;&#27880;&#24847;&#21147;&#35821;&#20041;&#21644;&#22806;&#37096;&#30693;&#35782;&#22270;&#35889;&#30340;&#20449;&#24687;&#12290;EMGF&#37197;&#22791;&#20102;&#22810;&#38170;&#28857;&#19977;&#20803;&#23398;&#20064;&#21644;&#27491;&#20132;&#25237;&#24433;&#65292;&#39640;&#25928;&#22320;&#21033;&#29992;&#20102;&#36825;&#20123;&#29305;&#24449;&#30340;&#32508;&#21512;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aspect-based Sentiment Analysis (ABSA) evaluates sentiment expressions within a text to comprehend sentiment information. Previous studies integrated external knowledge, such as knowledge graphs, to enhance the semantic features in ABSA models. Recent research has examined the use of Graph Neural Networks (GNNs) on dependency and constituent trees for syntactic analysis. With the ongoing development of ABSA, more innovative linguistic and structural features are being incorporated (e.g. latent graph), but this also introduces complexity and confusion. As of now, a scalable framework for integrating diverse linguistic and structural features into ABSA does not exist. This paper presents the Extensible Multi-Granularity Fusion (EMGF) network, which integrates information from dependency and constituent syntactic, attention semantic , and external knowledge graphs. EMGF, equipped with multi-anchor triplet learning and orthogonal projection, efficiently harnesses the combined potential of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#8220;&#20808;&#39044;&#27979;&#20877;&#20248;&#21270;&#8221;&#65288;PtO&#65289;&#33539;&#24335;&#25193;&#23637;&#21040;&#20855;&#26377;&#19981;&#21487;&#24494;&#20998;&#30340;&#26377;&#24207;&#21152;&#26435;&#24179;&#22343;&#65288;OWA&#65289;&#30446;&#26631;&#30340;&#20248;&#21270;&#38382;&#39064;&#19978;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#26377;&#25928;&#22320;&#23558;OWA&#20989;&#25968;&#30340;&#20248;&#21270;&#19982;&#21442;&#25968;&#25512;&#26029;&#30456;&#32467;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.07772</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#19979;&#20844;&#24179;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
End-to-End Learning for Fair Multiobjective Optimization Under Uncertainty
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#8220;&#20808;&#39044;&#27979;&#20877;&#20248;&#21270;&#8221;&#65288;PtO&#65289;&#33539;&#24335;&#25193;&#23637;&#21040;&#20855;&#26377;&#19981;&#21487;&#24494;&#20998;&#30340;&#26377;&#24207;&#21152;&#26435;&#24179;&#22343;&#65288;OWA&#65289;&#30446;&#26631;&#30340;&#20248;&#21270;&#38382;&#39064;&#19978;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#26377;&#25928;&#22320;&#23558;OWA&#20989;&#25968;&#30340;&#20248;&#21270;&#19982;&#21442;&#25968;&#25512;&#26029;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20154;&#24037;&#26234;&#33021;&#21644;&#36816;&#31609;&#23398;&#20013;&#30340;&#20915;&#31574;&#36807;&#31243;&#26159;&#30001;&#21442;&#25968;&#20248;&#21270;&#38382;&#39064;&#24314;&#27169;&#30340;&#65292;&#20854;&#23450;&#20041;&#21442;&#25968;&#26159;&#26410;&#30693;&#30340;&#65292;&#24517;&#39035;&#20174;&#21487;&#35266;&#27979;&#25968;&#25454;&#20013;&#25512;&#26029;&#20986;&#26469;&#12290;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#8220;&#20808;&#39044;&#27979;&#20877;&#20248;&#21270;&#8221;&#65288;PtO&#65289;&#33539;&#24335;&#26088;&#22312;&#36890;&#36807;&#35757;&#32451;&#21442;&#25968;&#25512;&#26029;&#27169;&#22411;&#19982;&#21518;&#32493;&#30340;&#32422;&#26463;&#20248;&#21270;&#26469;&#26368;&#22823;&#21270;&#19979;&#28216;&#20915;&#31574;&#36136;&#37327;&#12290;&#36825;&#38656;&#35201;&#36890;&#36807;&#36866;&#29992;&#20110;&#38382;&#39064;&#24418;&#24335;&#30340;&#36924;&#36817;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#19981;&#21487;&#24494;&#20998;&#30340;&#32447;&#24615;&#21644;&#28151;&#21512;&#25972;&#25968;&#31243;&#24207;&#65292;&#36890;&#36807;&#20248;&#21270;&#38382;&#39064;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#12290;&#26412;&#25991;&#23558;PtO&#26041;&#27861;&#25193;&#23637;&#21040;&#20855;&#26377;&#19981;&#21487;&#24494;&#20998;&#30340;&#26377;&#24207;&#21152;&#26435;&#24179;&#22343;&#65288;OWA&#65289;&#30446;&#26631;&#30340;&#20248;&#21270;&#38382;&#39064;&#19978;&#65292;OWA&#30446;&#26631;&#22312;&#20915;&#31574;&#27169;&#22411;&#20013;&#20445;&#35777;&#20844;&#24179;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#33021;&#21147;&#26159;&#20247;&#25152;&#21608;&#30693;&#30340;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#35757;&#32451;&#25216;&#24039;&#21644;&#25552;&#20986;&#30340;&#24212;&#29992;&#35774;&#32622;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#26377;&#25928;&#22320;&#23558;OWA&#20989;&#25968;&#30340;&#20248;&#21270;&#19982;&#21442;&#25968;&#25512;&#26029;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many decision processes in artificial intelligence and operations research are modeled by parametric optimization problems whose defining parameters are unknown and must be inferred from observable data. The Predict-Then-Optimize (PtO) paradigm in machine learning aims to maximize downstream decision quality by training the parametric inference model end-to-end with the subsequent constrained optimization. This requires backpropagation through the optimization problem using approximation techniques specific to the problem's form, especially for nondifferentiable linear and mixed-integer programs. This paper extends the PtO methodology to optimization problems with nondifferentiable Ordered Weighted Averaging (OWA) objectives, known for their ability to ensure properties of fairness and robustness in decision models. Through a collection of training techniques and proposed application settings, it shows how optimization of OWA functions can be effectively integrated with parametric pred
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;Transformer&#20013;&#30340;&#36880;&#27493;&#25512;&#29702;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21512;&#25104;&#22270;&#23548;&#33322;&#27169;&#22411;&#26469;&#25506;&#32034;&#36880;&#27493;&#25512;&#29702;&#30340;&#24213;&#23618;&#26426;&#21046;&#65292;&#24182;&#36890;&#36807;&#35813;&#27169;&#22411;&#22312;&#21512;&#25104;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20960;&#20010;&#20851;&#38190;&#29616;&#35937;&#30340;&#23384;&#22312;&#12290;</title><link>https://arxiv.org/abs/2402.07757</link><description>&lt;p&gt;
&#23545;Transformer&#20013;&#36880;&#27493;&#25512;&#29702;&#30340;&#29702;&#35299;: &#19968;&#20010;&#21512;&#25104;&#22270;&#23548;&#33322;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards an Understanding of Stepwise Inference in Transformers: A Synthetic Graph Navigation Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07757
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;Transformer&#20013;&#30340;&#36880;&#27493;&#25512;&#29702;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21512;&#25104;&#22270;&#23548;&#33322;&#27169;&#22411;&#26469;&#25506;&#32034;&#36880;&#27493;&#25512;&#29702;&#30340;&#24213;&#23618;&#26426;&#21046;&#65292;&#24182;&#36890;&#36807;&#35813;&#27169;&#22411;&#22312;&#21512;&#25104;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20960;&#20010;&#20851;&#38190;&#29616;&#35937;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36880;&#27493;&#25512;&#29702;&#21327;&#35758;&#65292;&#22914;scratchpads&#21644;chain-of-thought&#65292;&#36890;&#36807;&#23558;&#22797;&#26434;&#38382;&#39064;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#36739;&#31616;&#21333;&#30340;&#23376;&#38382;&#39064;&#65292;&#24110;&#21161;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#12290;&#23613;&#31649;&#36825;&#20123;&#21327;&#35758;&#22312;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#65292;&#20294;&#36880;&#27493;&#25512;&#29702;&#30340;&#24213;&#23618;&#26426;&#21046;&#20173;&#28982;&#38590;&#20197;&#29702;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#21512;&#25104;&#20219;&#21153;&#20013;&#30740;&#31350;&#33258;&#22238;&#24402;Transformer&#27169;&#22411;&#65292;&#35813;&#20219;&#21153;&#20307;&#29616;&#20102;&#38382;&#39064;&#30340;&#22810;&#27493;&#24615;&#36136;&#65292;&#20854;&#20013;&#36880;&#27493;&#25512;&#29702;&#36890;&#24120;&#26368;&#26377;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#22270;&#23548;&#33322;&#38382;&#39064;&#65292;&#27169;&#22411;&#30340;&#20219;&#21153;&#26159;&#22312;&#22270;&#19978;&#20174;&#36215;&#22987;&#33410;&#28857;&#21040;&#30446;&#26631;&#33410;&#28857;&#30340;&#36335;&#24452;&#19978;&#36827;&#34892;&#36941;&#21382;&#12290;&#23613;&#31649;&#31616;&#21333;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#32463;&#39564;&#37325;&#29616;&#21644;&#20998;&#26512;&#35266;&#23519;&#21040;&#30340;&#20960;&#20010;&#29616;&#35937;&#65306;(i)&#36880;&#27493;&#25512;&#29702;&#25512;&#29702;&#38388;&#38553;&#65292;&#25105;&#20204;&#21457;&#29616;&#20854;&#21407;&#22240;&#22312;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#32467;&#26500;&#65307;(ii)&#22312;&#27169;&#22411;&#29983;&#25104;&#20013;&#30340;&#22810;&#26679;&#24615;-&#20934;&#30830;&#24615;&#26435;&#34913;&#65292;&#38543;&#30528;&#37319;&#26679;&#28201;&#24230;&#30340;&#21464;&#21270;&#65307;(iii)&#27169;&#22411;&#22312;&#36755;&#20986;&#20013;&#30340;&#31616;&#21333;&#24615;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stepwise inference protocols, such as scratchpads and chain-of-thought, help language models solve complex problems by decomposing them into a sequence of simpler subproblems. Despite the significant gain in performance achieved via these protocols, the underlying mechanisms of stepwise inference have remained elusive. To address this, we propose to study autoregressive Transformer models on a synthetic task that embodies the multi-step nature of problems where stepwise inference is generally most useful. Specifically, we define a graph navigation problem wherein a model is tasked with traversing a path from a start to a goal node on the graph. Despite is simplicity, we find we can empirically reproduce and analyze several phenomena observed at scale: (i) the stepwise inference reasoning gap, the cause of which we find in the structure of the training data; (ii) a diversity-accuracy tradeoff in model generations as sampling temperature varies; (iii) a simplicity bias in the model's out
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#24605;&#32500;&#38142;&#25512;&#29702;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#25955;&#20256;&#25773;&#25512;&#29702;&#27493;&#39588;&#65292;&#25552;&#20379;&#20102;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#25968;&#23398;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#33258;&#25105;&#32416;&#27491;&#33021;&#21147;&#21644;&#25512;&#29702;&#25216;&#26415;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.07754</link><description>&lt;p&gt;
&#24605;&#24819;&#20256;&#25773;&#65306;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24605;&#32500;&#38142;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#24605;&#32500;&#38142;&#25512;&#29702;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#25955;&#20256;&#25773;&#25512;&#29702;&#27493;&#39588;&#65292;&#25552;&#20379;&#20102;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#25968;&#23398;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#33258;&#25105;&#32416;&#27491;&#33021;&#21147;&#21644;&#25512;&#29702;&#25216;&#26415;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#25991;&#26412;&#22788;&#29702;&#20013;&#24341;&#36215;&#20102;&#20851;&#27880;&#65292;&#30456;&#23545;&#20256;&#32479;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#20855;&#26377;&#35768;&#22810;&#28508;&#22312;&#20248;&#21183;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;CoT&#26159;&#19968;&#31181;&#22312;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#20013;&#25913;&#36827;&#25512;&#29702;&#33021;&#21147;&#30340;&#25104;&#29087;&#25216;&#26415;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24605;&#32500;&#25193;&#25955;&#65288;DoT&#65289;&#27169;&#22411;&#65292;&#20801;&#35768;&#25512;&#29702;&#27493;&#39588;&#36890;&#36807;&#25193;&#25955;&#36807;&#31243;&#22312;&#26102;&#38388;&#19978;&#20256;&#25773;&#12290;&#19982;&#20256;&#32479;&#30340;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#36880;&#20010;token&#20174;&#24038;&#21040;&#21491;&#20570;&#20986;&#20915;&#31574;&#30340;&#26041;&#24335;&#30456;&#27604;&#65292;DoT&#22312;&#35745;&#31639;&#21644;&#25512;&#29702;&#24615;&#33021;&#20043;&#38388;&#20855;&#26377;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;DoT&#22312;&#22810;&#20301;&#25968;&#20056;&#27861;&#21644;&#23567;&#23398;&#25968;&#23398;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;DoT&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#33258;&#25105;&#32416;&#27491;&#33021;&#21147;&#65292;&#24182;&#20174;&#29616;&#26377;&#30340;&#22686;&#24378;&#25512;&#29702;&#25216;&#26415;&#65288;&#22914;&#33258;&#19968;&#33268;&#35299;&#30721;&#65289;&#20013;&#21463;&#30410;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26377;&#21161;&#20110;&#29702;&#35299;&#21644;&#21457;&#23637;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have gained attention in text processing, offering many potential advantages over traditional autoregressive models. This work explores the integration of diffusion models and Chain-of-Thought (CoT), a well-established technique to improve the reasoning ability in autoregressive language models. We propose Diffusion-of-Thought (DoT), allowing reasoning steps to diffuse over time through the diffusion process. In contrast to traditional autoregressive language models that make decisions in a left-to-right, token-by-token manner, DoT offers more flexibility in the trade-off between computation and reasoning performance. Our experimental results demonstrate the effectiveness of DoT in multi-digit multiplication and grade school math problems. Additionally, DoT showcases promising self-correction abilities and benefits from existing reasoning-enhancing techniques like self-consistency decoding. Our findings contribute to the understanding and development of reasoning capab
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#32479;&#19968;&#23545;&#40784;&#21407;&#21017; ($\mathbf{UA}^2$)&#65292;&#26088;&#22312;&#23454;&#29616;&#26234;&#33021;&#20307;&#19982;&#20154;&#31867;&#24847;&#22270;&#12289;&#29615;&#22659;&#21160;&#24577;&#21644;&#33258;&#25105;&#32422;&#26463;&#30340;&#32479;&#19968;&#23545;&#40784;&#65292;&#25552;&#20986;&#20102;&#24341;&#20837;&#23454;&#38469;&#29305;&#24615;&#36827;&#34892;&#27010;&#24565;&#39564;&#35777;&#30740;&#31350;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.07744</link><description>&lt;p&gt;
&#23454;&#29616;&#26234;&#33021;&#20307;&#12289;&#20154;&#31867;&#21644;&#29615;&#22659;&#20043;&#38388;&#30340;&#32479;&#19968;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Towards Unified Alignment Between Agents, Humans, and Environment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#32479;&#19968;&#23545;&#40784;&#21407;&#21017; ($\mathbf{UA}^2$)&#65292;&#26088;&#22312;&#23454;&#29616;&#26234;&#33021;&#20307;&#19982;&#20154;&#31867;&#24847;&#22270;&#12289;&#29615;&#22659;&#21160;&#24577;&#21644;&#33258;&#25105;&#32422;&#26463;&#30340;&#32479;&#19968;&#23545;&#40784;&#65292;&#25552;&#20986;&#20102;&#24341;&#20837;&#23454;&#38469;&#29305;&#24615;&#36827;&#34892;&#27010;&#24565;&#39564;&#35777;&#30740;&#31350;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#24555;&#36895;&#36827;&#23637;&#23548;&#33268;&#20102;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#32321;&#33635;&#65292;&#36825;&#20123;&#26234;&#33021;&#20307;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#36890;&#29992;&#33021;&#21147;&#36827;&#34892;&#25512;&#29702;&#12289;&#20915;&#31574;&#21644;&#29615;&#22659;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#24403;&#22312;&#22797;&#26434;&#12289;&#29616;&#23454;&#30340;&#29615;&#22659;&#20013;&#36816;&#34892;&#26102;&#65292;&#26234;&#33021;&#20307;&#30340;&#25928;&#33021;&#20173;&#28982;&#26377;&#38480;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32479;&#19968;&#23545;&#40784;&#21407;&#21017;&#65292;&#21363;&#21516;&#26102;&#23545;&#40784;&#26234;&#33021;&#20307;&#19982;&#20154;&#31867;&#24847;&#22270;&#12289;&#29615;&#22659;&#21160;&#24577;&#21644;&#33258;&#25105;&#32422;&#26463;&#65288;&#22914;&#36135;&#24065;&#39044;&#31639;&#38480;&#21046;&#65289;&#12290;&#20174;&#32479;&#19968;&#23545;&#40784; ($\mathbf{UA}^2$) &#30340;&#35270;&#35282;&#20986;&#21457;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#24403;&#21069;&#26234;&#33021;&#20307;&#30740;&#31350;&#30340;&#29616;&#29366;&#65292;&#24182;&#25351;&#20986;&#20102;&#29616;&#26377;&#26234;&#33021;&#20307;&#22522;&#20934;&#21644;&#26041;&#27861;&#20505;&#36873;&#20013;&#34987;&#24573;&#35270;&#30340;&#22240;&#32032;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#20026;WebShop&#24341;&#20837;&#23454;&#38469;&#29305;&#24615;&#36827;&#34892;&#20102;&#27010;&#24565;&#39564;&#35777;&#30740;&#31350;&#65292;&#21253;&#25324;&#20351;&#29992;&#29992;&#25143;&#37197;&#32622;&#25991;&#20214;&#26469;&#23637;&#31034;&#24847;&#22270;&#12289;&#20010;&#24615;&#21270;&#37325;&#26032;&#25490;&#21517;&#20197;&#24212;&#23545;&#22797;&#26434;&#30340;&#29615;&#22659;&#21160;&#24577;&#21644;&#36816;&#34892;&#26102;&#25104;&#26412;&#32479;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid progress of foundation models has led to the prosperity of autonomous agents, which leverage the universal capabilities of foundation models to conduct reasoning, decision-making, and environmental interaction. However, the efficacy of agents remains limited when operating in intricate, realistic environments. In this work, we introduce the principles of $\mathbf{U}$nified $\mathbf{A}$lignment for $\mathbf{A}$gents ($\mathbf{UA}^2$), which advocate for the simultaneous alignment of agents with human intentions, environmental dynamics, and self-constraints such as the limitation of monetary budgets. From the perspective of $\mathbf{UA}^2$, we review the current agent research and highlight the neglected factors in existing agent benchmarks and method candidates. We also conduct proof-of-concept studies by introducing realistic features to WebShop, including user profiles to demonstrate intentions, personalized reranking for complex environmental dynamics, and runtime cost stat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#26680;&#22238;&#24402;&#30340;&#31616;&#21270;&#29615;&#22659;&#20013;&#35299;&#26512;&#20102;&#27169;&#22411;&#23849;&#28291;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#20102;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#34394;&#20551;&#25968;&#25454;&#19982;&#24615;&#33021;&#23436;&#20840;&#23849;&#28291;&#20043;&#38388;&#30340;&#20132;&#21449;&#28857;&#12290;&#36890;&#36807;&#25552;&#20986;&#22522;&#20110;&#33258;&#36866;&#24212;&#27491;&#21017;&#21270;&#30340;&#31574;&#30053;&#65292;&#25104;&#21151;&#32531;&#35299;&#20102;&#27169;&#22411;&#23849;&#28291;&#38382;&#39064;&#12290;&#36825;&#20123;&#21457;&#29616;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#12290;</title><link>https://arxiv.org/abs/2402.07712</link><description>&lt;p&gt;
&#27169;&#22411;&#23849;&#28291;&#35299;&#23494;&#65306;&#22238;&#24402;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Model Collapse Demystified: The Case of Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#26680;&#22238;&#24402;&#30340;&#31616;&#21270;&#29615;&#22659;&#20013;&#35299;&#26512;&#20102;&#27169;&#22411;&#23849;&#28291;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#20102;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#34394;&#20551;&#25968;&#25454;&#19982;&#24615;&#33021;&#23436;&#20840;&#23849;&#28291;&#20043;&#38388;&#30340;&#20132;&#21449;&#28857;&#12290;&#36890;&#36807;&#25552;&#20986;&#22522;&#20110;&#33258;&#36866;&#24212;&#27491;&#21017;&#21270;&#30340;&#31574;&#30053;&#65292;&#25104;&#21151;&#32531;&#35299;&#20102;&#27169;&#22411;&#23849;&#28291;&#38382;&#39064;&#12290;&#36825;&#20123;&#21457;&#29616;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#20195;&#65292;"&#27169;&#22411;&#23849;&#28291;"&#29616;&#35937;&#25351;&#30340;&#26159;&#27169;&#22411;&#22312;&#36882;&#24402;&#22320;&#35757;&#32451;&#33258;&#36523;&#19978;&#19968;&#20195;&#21448;&#19968;&#20195;&#29983;&#25104;&#30340;&#25968;&#25454;&#26102;&#65292;&#20854;&#24615;&#33021;&#36880;&#28176;&#38477;&#20302;&#65292;&#26368;&#32456;&#21464;&#24471;&#23436;&#20840;&#26080;&#29992;&#65292;&#21363;&#27169;&#22411;&#23849;&#28291;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#26680;&#22238;&#24402;&#30340;&#31616;&#21270;&#29615;&#22659;&#20013;&#30740;&#31350;&#20102;&#36825;&#19968;&#29616;&#35937;&#65292;&#24182;&#33719;&#24471;&#20102;&#32467;&#26524;&#65292;&#26174;&#31034;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#34394;&#20551;&#25968;&#25454;&#19982;&#27169;&#22411;&#24615;&#33021;&#23436;&#20840;&#23849;&#28291;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#30340;&#20132;&#21449;&#28857;&#12290;&#22312;&#22810;&#39033;&#24335;&#34928;&#20943;&#30340;&#20809;&#35889;&#21644;&#28304;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#20462;&#25913;&#21518;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#23637;&#31034;&#20102;&#20174;&#24555;&#36895;&#21040;&#32531;&#24930;&#36895;&#29575;&#30340;&#26032;&#20132;&#21449;&#29616;&#35937;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#22522;&#20110;&#33258;&#36866;&#24212;&#27491;&#21017;&#21270;&#30340;&#31616;&#21333;&#31574;&#30053;&#26469;&#32531;&#35299;&#27169;&#22411;&#23849;&#28291;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of large language models like ChatGPT, the phenomenon of "model collapse" refers to the situation whereby as a model is trained recursively on data generated from previous generations of itself over time, its performance degrades until the model eventually becomes completely useless, i.e the model collapses. In this work, we study this phenomenon in the simplified setting of kernel regression and obtain results which show a clear crossover between where the model can cope with fake data, and a regime where the model's performance completely collapses. Under polynomial decaying spectral and source conditions, we obtain modified scaling laws which exhibit new crossover phenomena from fast to slow rates. We also propose a simple strategy based on adaptive regularization to mitigate model collapse. Our theoretical results are validated with experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;3D&#28857;&#20113;&#19978;&#36827;&#34892;&#31232;&#30095;&#21367;&#31215;&#30340;GPU&#20248;&#21270;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#28857;&#20113;&#30340;&#31232;&#30095;&#24615;&#21644;&#35745;&#31639;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.07710</link><description>&lt;p&gt;
&#22522;&#20110;CUDA&#30340;GPU&#20248;&#21270;&#31232;&#30095;&#21367;&#31215;&#22312;3D&#28857;&#20113;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Optimization of Sparse Convolution for 3D-Point Cloud on GPUs with CUDA
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;3D&#28857;&#20113;&#19978;&#36827;&#34892;&#31232;&#30095;&#21367;&#31215;&#30340;GPU&#20248;&#21270;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#28857;&#20113;&#30340;&#31232;&#30095;&#24615;&#21644;&#35745;&#31639;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#24212;&#29992;&#26174;&#33879;&#22686;&#21152;&#65292;&#29305;&#21035;&#26159;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#65292;&#23427;&#20204;&#24050;&#32463;&#25104;&#20026;&#28041;&#21450;&#32467;&#26500;&#21270;&#26684;&#32593;&#25968;&#25454;&#30340;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#20027;&#35201;&#26041;&#27861;&#65292;&#22914;&#22270;&#20687;&#20998;&#26512;&#21644;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;LiDAR&#21644;3D&#20256;&#24863;&#22120;&#22312;&#35768;&#22810;&#39046;&#22495;&#30340;&#20351;&#29992;&#21576;&#25351;&#25968;&#22686;&#38271;&#65292;&#23545;3D&#28857;&#20113;&#30340;&#20998;&#26512;&#38656;&#27714;&#20063;&#22686;&#21152;&#20102;&#12290;&#21033;&#29992;3D&#28857;&#20113;&#22312;&#21253;&#25324;&#29289;&#20307;&#35782;&#21035;&#21644;&#20998;&#21106;&#22312;&#20869;&#30340;&#21508;&#31181;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20204;&#25552;&#20379;&#20102;&#19977;&#32500;&#29615;&#22659;&#20013;&#20107;&#29289;&#30340;&#31354;&#38388;&#25551;&#36848;&#12290;&#19982;&#29031;&#29255;&#19981;&#21516;&#65292;&#28857;&#20113;&#20855;&#26377;&#31232;&#30095;&#24615;&#21644;&#32570;&#20047;&#35268;&#21017;&#30340;&#26684;&#32593;&#65292;&#22240;&#27492;&#23384;&#22312;&#30528;&#29420;&#29305;&#30340;&#22788;&#29702;&#21644;&#35745;&#31639;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been a significant increase in the utilization of deep learning methods, particularly convolutional neural networks (CNNs), which have emerged as the dominant approach in various domains that involve structured grid data, such as picture analysis and processing. Nevertheless, the exponential growth in the utilization of LiDAR and 3D sensors across many domains has resulted in an increased need for the analysis of 3D point clouds. The utilization of 3D point clouds is crucial in various applications, including object recognition and segmentation, as they offer a spatial depiction of things within a three-dimensional environment. In contrast to photos, point clouds exhibit sparsity and lack a regular grid, hence posing distinct processing and computational issues.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#22312;&#32447;&#39034;&#24207;&#20915;&#31574;&#20013;&#22788;&#29702;&#26410;&#30693;&#24310;&#36831;&#38382;&#39064;&#30340;&#19977;&#20010;&#24310;&#36831;&#31639;&#27861;&#26063;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.07703</link><description>&lt;p&gt;
&#22312;&#32447;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#26410;&#30693;&#24310;&#36831;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Online Sequential Decision-Making with Unknown Delays
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#22312;&#32447;&#39034;&#24207;&#20915;&#31574;&#20013;&#22788;&#29702;&#26410;&#30693;&#24310;&#36831;&#38382;&#39064;&#30340;&#19977;&#20010;&#24310;&#36831;&#31639;&#27861;&#26063;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#39034;&#24207;&#20915;&#31574;&#39046;&#22495;&#65292;&#25105;&#20204;&#21033;&#29992;&#22312;&#32447;&#20984;&#20248;&#21270;&#65288;OCO&#65289;&#26694;&#26550;&#35299;&#20915;&#20102;&#20855;&#26377;&#24310;&#36831;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#20915;&#31574;&#30340;&#21453;&#39304;&#21487;&#33021;&#20197;&#26410;&#30693;&#24310;&#36831;&#21040;&#36798;&#12290;&#19982;&#20043;&#21069;&#20165;&#38480;&#20110;&#27431;&#20960;&#37324;&#24471;&#33539;&#25968;&#21644;&#26799;&#24230;&#20449;&#24687;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#22522;&#20110;&#36817;&#20284;&#35299;&#30340;&#24310;&#36831;&#31639;&#27861;&#26063;&#65292;&#22788;&#29702;&#19981;&#21516;&#31867;&#22411;&#30340;&#25509;&#25910;&#21453;&#39304;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#26159;&#22810;&#21151;&#33021;&#19988;&#36866;&#29992;&#20110;&#36890;&#29992;&#33539;&#25968;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#38024;&#23545;&#20855;&#26377;&#23436;&#25972;&#25439;&#22833;&#20989;&#25968;&#20449;&#24687;&#21453;&#39304;&#30340;&#24310;&#36831;&#35268;&#33539;&#21270;&#39046;&#23548;&#31639;&#27861;&#26063;&#65292;&#19968;&#31995;&#21015;&#38024;&#23545;&#20855;&#26377;&#26799;&#24230;&#20449;&#24687;&#21453;&#39304;&#30340;&#24310;&#36831;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#26063;&#65292;&#20197;&#21450;&#19968;&#31995;&#21015;&#38024;&#23545;&#30456;&#24212;&#20915;&#31574;&#28857;&#25439;&#22833;&#20989;&#25968;&#26799;&#24230;&#20540;&#20449;&#24687;&#21453;&#39304;&#30340;&#31616;&#21270;&#24310;&#36831;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#26063;&#12290;&#23545;&#20110;&#27599;&#31181;&#31867;&#22411;&#30340;&#31639;&#27861;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of online sequential decision-making, we address the problem with delays utilizing the framework of online convex optimization (OCO), where the feedback of a decision can arrive with an unknown delay. Unlike previous research that is limited to Euclidean norm and gradient information, we propose three families of delayed algorithms based on approximate solutions to handle different types of received feedback. Our proposed algorithms are versatile and applicable to universal norms. Specifically, we introduce a family of Follow the Delayed Regularized Leader algorithms for feedback with full information on the loss function, a family of Delayed Mirror Descent algorithms for feedback with gradient information on the loss function and a family of Simplified Delayed Mirror Descent algorithms for feedback with the value information of the loss function's gradients at corresponding decision points. For each type of algorithm, we provide corresponding regret bounds under cases of 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#37325;&#26032;&#23450;&#20301;&#21477;&#23376;&#20013;&#30340;&#20004;&#20010;&#21333;&#35789;&#23454;&#26045;&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#19982;&#24050;&#26377;&#30340;&#25915;&#20987;&#26041;&#24335;&#30456;&#27604;&#65292;&#22312;&#25915;&#20987;&#25104;&#21151;&#29575;&#12289;&#22256;&#24785;&#24230;&#21644;&#19982;&#24178;&#20928;&#26679;&#26412;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#23545;ONION&#38450;&#24481;&#26041;&#27861;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07689</link><description>&lt;p&gt;
OrderBkd: &#36890;&#36807;&#37325;&#26032;&#23450;&#20301;&#36827;&#34892;&#30340;&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
OrderBkd: Textual backdoor attack through repositioning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#37325;&#26032;&#23450;&#20301;&#21477;&#23376;&#20013;&#30340;&#20004;&#20010;&#21333;&#35789;&#23454;&#26045;&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#19982;&#24050;&#26377;&#30340;&#25915;&#20987;&#26041;&#24335;&#30456;&#27604;&#65292;&#22312;&#25915;&#20987;&#25104;&#21151;&#29575;&#12289;&#22256;&#24785;&#24230;&#21644;&#19982;&#24178;&#20928;&#26679;&#26412;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#23545;ONION&#38450;&#24481;&#26041;&#27861;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#31532;&#19977;&#26041;&#25968;&#25454;&#38598;&#21644;&#39044;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;NLP&#31995;&#32479;&#26500;&#25104;&#23041;&#32961;&#65292;&#21487;&#33021;&#38544;&#34255;&#21518;&#38376;&#25915;&#20987;&#12290;&#29616;&#26377;&#30340;&#25915;&#20987;&#26041;&#24335;&#21253;&#25324;&#25554;&#20837;&#26631;&#35760;&#25110;&#21477;&#23376;&#37325;&#36848;&#31561;&#27745;&#26579;&#25968;&#25454;&#26679;&#26412;&#65292;&#36825;&#35201;&#20040;&#25913;&#21464;&#20102;&#21407;&#22987;&#25991;&#26412;&#30340;&#35821;&#20041;&#65292;&#35201;&#20040;&#21487;&#20197;&#34987;&#26816;&#27979;&#20986;&#26469;&#12290;&#25105;&#20204;&#19982;&#20197;&#24448;&#24037;&#20316;&#30340;&#20027;&#35201;&#21306;&#21035;&#22312;&#20110;&#65292;&#25105;&#20204;&#20351;&#29992;&#37325;&#26032;&#23450;&#20301;&#21477;&#23376;&#20013;&#30340;&#20004;&#20010;&#21333;&#35789;&#20316;&#20026;&#35302;&#21457;&#22120;&#12290;&#36890;&#36807;&#35774;&#35745;&#24182;&#24212;&#29992;&#22522;&#20110;&#35789;&#24615;&#30340;&#35268;&#21017;&#26469;&#36873;&#25321;&#36825;&#20123;&#26631;&#35760;&#65292;&#25105;&#20204;&#22312;SST-2&#21644;AG&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#20445;&#25345;&#20102;&#39640;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#21516;&#26102;&#22312;&#22256;&#24785;&#24230;&#21644;&#19982;&#24178;&#20928;&#26679;&#26412;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25915;&#20987;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25915;&#20987;&#23545;ONION&#38450;&#24481;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;&#35770;&#25991;&#20013;&#30340;&#25152;&#26377;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#22312;https://github.com/alekseevskaia/OrderBkd&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of third-party datasets and pre-trained machine learning models poses a threat to NLP systems due to possibility of hidden backdoor attacks. Existing attacks involve poisoning the data samples such as insertion of tokens or sentence paraphrasing, which either alter the semantics of the original texts or can be detected. Our main difference from the previous work is that we use the reposition of a two words in a sentence as a trigger. By designing and applying specific part-of-speech (POS) based rules for selecting these tokens, we maintain high attack success rate on SST-2 and AG classification datasets while outperforming existing attacks in terms of perplexity and semantic similarity to the clean samples. In addition, we show the robustness of our attack to the ONION defense method. All the code and data for the paper can be obtained at https://github.com/alekseevskaia/OrderBkd.
&lt;/p&gt;</description></item><item><title>CyberMetric&#26159;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#30340;&#30693;&#35782;&#12290;&#35813;&#25968;&#25454;&#38598;&#30001;10000&#20010;&#38382;&#39064;&#32452;&#25104;&#65292;&#36890;&#36807;&#21512;&#20316;&#36807;&#31243;&#23558;&#19987;&#23478;&#30693;&#35782;&#19982;LLMs&#30456;&#32467;&#21512;&#12290;&#38500;&#20102;&#35780;&#20272;LLMs&#30340;&#30693;&#35782;&#22806;&#65292;&#25968;&#25454;&#38598;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#20419;&#36827;&#20154;&#31867;&#19982;&#19981;&#21516;LLMs&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#30340;&#20844;&#24179;&#27604;&#36739;&#12290;</title><link>https://arxiv.org/abs/2402.07688</link><description>&lt;p&gt;
CyberMetric: &#19968;&#20221;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#30340;&#30693;&#35782;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CyberMetric: A Benchmark Dataset for Evaluating Large Language Models Knowledge in Cybersecurity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07688
&lt;/p&gt;
&lt;p&gt;
CyberMetric&#26159;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#30340;&#30693;&#35782;&#12290;&#35813;&#25968;&#25454;&#38598;&#30001;10000&#20010;&#38382;&#39064;&#32452;&#25104;&#65292;&#36890;&#36807;&#21512;&#20316;&#36807;&#31243;&#23558;&#19987;&#23478;&#30693;&#35782;&#19982;LLMs&#30456;&#32467;&#21512;&#12290;&#38500;&#20102;&#35780;&#20272;LLMs&#30340;&#30693;&#35782;&#22806;&#65292;&#25968;&#25454;&#38598;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#20419;&#36827;&#20154;&#31867;&#19982;&#19981;&#21516;LLMs&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#30340;&#20844;&#24179;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#34920;&#29616;&#20986;&#33394;&#65292;&#20174;&#35745;&#31639;&#26426;&#35270;&#35273;&#21040;&#21307;&#23398;&#35786;&#26029;&#12290;&#28982;&#32780;&#65292;&#29702;&#35299;&#32593;&#32476;&#23433;&#20840;&#36825;&#20010;&#28085;&#30422;&#23494;&#30721;&#23398;&#12289;&#36870;&#21521;&#24037;&#31243;&#21644;&#39118;&#38505;&#35780;&#20272;&#31561;&#22810;&#26679;&#21270;&#39046;&#22495;&#30340;&#25361;&#25112;&#21363;&#20351;&#23545;&#20110;&#20154;&#31867;&#19987;&#23478;&#26469;&#35828;&#20063;&#26159;&#22256;&#38590;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CyberMetric&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;&#26469;&#33258;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#30340;&#26631;&#20934;&#12289;&#35748;&#35777;&#12289;&#30740;&#31350;&#35770;&#25991;&#12289;&#20070;&#31821;&#21644;&#20854;&#20182;&#20986;&#29256;&#29289;&#30340;1&#19975;&#20010;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#36890;&#36807;&#19968;&#31181;&#21327;&#20316;&#36807;&#31243;&#21019;&#24314;&#65292;&#21363;&#23558;&#19987;&#23478;&#30693;&#35782;&#19982;LLM&#65288;&#21253;&#25324;GPT-3.5&#21644;Falcon-180B&#65289;&#30456;&#32467;&#21512;&#12290;&#20154;&#31867;&#19987;&#23478;&#33457;&#36153;&#20102;&#36229;&#36807;200&#23567;&#26102;&#39564;&#35777;&#20854;&#20934;&#30830;&#24615;&#21644;&#30456;&#20851;&#24615;&#12290;&#38500;&#20102;&#35780;&#20272;LLM&#30340;&#30693;&#35782;&#22806;&#65292;&#35813;&#25968;&#25454;&#38598;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#20419;&#36827;&#20154;&#31867;&#19982;&#19981;&#21516;LLM&#20043;&#38388;&#30340;&#20844;&#24179;&#27604;&#36739;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#31934;&#36873;&#20102;80&#20010;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#30340;&#22810;&#20010;&#20027;&#39064;&#65292;&#24182;&#35753;30&#20010;&#21442;&#19982;&#32773;&#21442;&#19982;&#20854;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) excel across various domains, from computer vision to medical diagnostics. However, understanding the diverse landscape of cybersecurity, encompassing cryptography, reverse engineering, and managerial facets like risk assessment, presents a challenge, even for human experts. In this paper, we introduce CyberMetric, a benchmark dataset comprising 10,000 questions sourced from standards, certifications, research papers, books, and other publications in the cybersecurity domain. The questions are created through a collaborative process, i.e., merging expert knowledge with LLMs, including GPT-3.5 and Falcon-180B. Human experts spent over 200 hours verifying their accuracy and relevance. Beyond assessing LLMs' knowledge, the dataset's main goal is to facilitate a fair comparison between humans and different LLMs in cybersecurity. To achieve this, we carefully selected 80 questions covering a wide range of topics within cybersecurity and involved 30 participants 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20004;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20256;&#32479;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#22312;&#27861;&#24459;&#39046;&#22495;&#30340;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#65292;&#32467;&#26524;&#26174;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20135;&#29983;&#19978;&#19979;&#25991;&#36275;&#22815;&#19988;&#27969;&#30021;&#30340;&#35793;&#25991;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#24378;&#35843;&#20102;&#20154;&#24037;&#35780;&#20272;&#26041;&#27861;&#22312;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07681</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#8220;&#35780;&#23457;&#8221;: &#22312;&#27861;&#24459;&#39046;&#22495;&#30340;&#26426;&#22120;&#32763;&#35793;&#25928;&#26524;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
Large Language Models "Ad Referendum": How Good Are They at Machine Translation in the Legal Domain?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07681
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20004;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20256;&#32479;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#22312;&#27861;&#24459;&#39046;&#22495;&#30340;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#65292;&#32467;&#26524;&#26174;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20135;&#29983;&#19978;&#19979;&#25991;&#36275;&#22815;&#19988;&#27969;&#30021;&#30340;&#35793;&#25991;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#24378;&#35843;&#20102;&#20154;&#24037;&#35780;&#20272;&#26041;&#27861;&#22312;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20256;&#32479;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#31995;&#32479;&#22312;&#27861;&#24459;&#39046;&#22495;&#22235;&#31181;&#35821;&#35328;&#23545;&#20013;&#30340;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#12290;&#30740;&#31350;&#32467;&#21512;&#20102;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#65288;AEMs&#65289;&#21644;&#19987;&#19994;&#32763;&#35793;&#20154;&#21592;&#36827;&#34892;&#30340;&#20154;&#24037;&#35780;&#20272;&#65288;HE&#65289;&#65292;&#35780;&#20272;&#20102;&#32763;&#35793;&#25490;&#21517;&#12289;&#27969;&#30021;&#24615;&#21644;&#36275;&#22815;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#35895;&#27468;&#32763;&#35793;&#22312;AEMs&#26041;&#38754;&#36890;&#24120;&#20248;&#20110;LLMs&#65292;&#20294;&#20154;&#24037;&#35780;&#20272;&#32773;&#35748;&#20026;LLMs&#65292;&#29305;&#21035;&#26159;GPT-4&#65292;&#22312;&#20135;&#29983;&#19978;&#19979;&#25991;&#36275;&#22815;&#19988;&#27969;&#30021;&#30340;&#35793;&#25991;&#26041;&#38754;&#30456;&#24403;&#25110;&#30053;&#22909;&#20110;&#35895;&#27468;&#32763;&#35793;&#12290;&#36825;&#31181;&#24046;&#24322;&#34920;&#26126;LLMs&#22312;&#22788;&#29702;&#19987;&#19994;&#27861;&#24459;&#26415;&#35821;&#21644;&#32972;&#26223;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20984;&#26174;&#20102;&#20154;&#24037;&#35780;&#20272;&#26041;&#27861;&#22312;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#30740;&#31350;&#24378;&#35843;&#20102;LLMs&#22312;&#19987;&#19994;&#39046;&#22495;&#30340;&#19981;&#26029;&#21457;&#23637;&#33021;&#21147;&#65292;&#24182;&#21628;&#21505;&#37325;&#26032;&#35780;&#20272;&#20256;&#32479;AEMs&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;LLMs&#29983;&#25104;&#30340;&#32763;&#35793;&#30340;&#32454;&#24494;&#24046;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study evaluates the machine translation (MT) quality of two state-of-the-art large language models (LLMs) against a tradition-al neural machine translation (NMT) system across four language pairs in the legal domain. It combines automatic evaluation met-rics (AEMs) and human evaluation (HE) by professional transla-tors to assess translation ranking, fluency and adequacy. The re-sults indicate that while Google Translate generally outperforms LLMs in AEMs, human evaluators rate LLMs, especially GPT-4, comparably or slightly better in terms of producing contextually adequate and fluent translations. This discrepancy suggests LLMs' potential in handling specialized legal terminology and context, highlighting the importance of human evaluation methods in assessing MT quality. The study underscores the evolving capabil-ities of LLMs in specialized domains and calls for reevaluation of traditional AEMs to better capture the nuances of LLM-generated translations.
&lt;/p&gt;</description></item><item><title>AYDIV&#26159;&#19968;&#20010;&#36890;&#36807;&#38598;&#25104;&#19978;&#19979;&#25991;&#35270;&#35273;Transformer&#26469;&#22686;&#24378;3D&#29289;&#20307;&#26816;&#27979;&#30340;&#26694;&#26550;&#65292;&#29305;&#21035;&#35774;&#35745;&#20102;&#19977;&#38454;&#27573;&#23545;&#40784;&#36807;&#31243;&#26469;&#22686;&#24378;&#38271;&#36317;&#31163;&#30340;&#26816;&#27979;&#33021;&#21147;&#65292;&#21253;&#25324;&#25913;&#21892;&#30456;&#26426;&#29305;&#24449;&#25552;&#21462;&#12289;&#34701;&#21512;LiDAR&#21644;&#30456;&#26426;&#32454;&#33410;&#20197;&#21450;&#20840;&#38754;&#30340;&#31354;&#38388;&#25968;&#25454;&#34701;&#21512;&#12290;&#22312;Waymo&#24320;&#25918;&#25968;&#25454;&#38598;&#19978;&#65292;AYDIV&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;1.24%&#30340;&#24179;&#22343;&#31934;&#24230;&#65288;mA&#65289;&#12290;</title><link>https://arxiv.org/abs/2402.07680</link><description>&lt;p&gt;
AYDIV: &#36890;&#36807;&#38598;&#25104;&#19978;&#19979;&#25991;&#35270;&#35273;Transformer&#30340;&#21487;&#35843;&#33410;&#24615;3D&#29289;&#20307;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
AYDIV: Adaptable Yielding 3D Object Detection via Integrated Contextual Vision Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07680
&lt;/p&gt;
&lt;p&gt;
AYDIV&#26159;&#19968;&#20010;&#36890;&#36807;&#38598;&#25104;&#19978;&#19979;&#25991;&#35270;&#35273;Transformer&#26469;&#22686;&#24378;3D&#29289;&#20307;&#26816;&#27979;&#30340;&#26694;&#26550;&#65292;&#29305;&#21035;&#35774;&#35745;&#20102;&#19977;&#38454;&#27573;&#23545;&#40784;&#36807;&#31243;&#26469;&#22686;&#24378;&#38271;&#36317;&#31163;&#30340;&#26816;&#27979;&#33021;&#21147;&#65292;&#21253;&#25324;&#25913;&#21892;&#30456;&#26426;&#29305;&#24449;&#25552;&#21462;&#12289;&#34701;&#21512;LiDAR&#21644;&#30456;&#26426;&#32454;&#33410;&#20197;&#21450;&#20840;&#38754;&#30340;&#31354;&#38388;&#25968;&#25454;&#34701;&#21512;&#12290;&#22312;Waymo&#24320;&#25918;&#25968;&#25454;&#38598;&#19978;&#65292;AYDIV&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;1.24%&#30340;&#24179;&#22343;&#31934;&#24230;&#65288;mA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#21512;LiDAR&#21644;&#30456;&#26426;&#25968;&#25454;&#22312;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#22686;&#24378;&#30701;&#36317;&#31163;&#29289;&#20307;&#26816;&#27979;&#26174;&#31034;&#20986;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;LiDAR&#30340;&#31232;&#30095;&#25968;&#25454;&#21644;&#30456;&#26426;&#30340;&#39640;&#23494;&#24230;&#20998;&#36776;&#29575;&#20043;&#38388;&#30340;&#23545;&#27604;&#65292;&#34701;&#21512;&#22312;&#24310;&#20280;&#36317;&#31163;&#26816;&#27979;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#20004;&#31181;&#25968;&#25454;&#34920;&#31034;&#30340;&#24046;&#24322;&#36827;&#19968;&#27493;&#22797;&#26434;&#21270;&#20102;&#34701;&#21512;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;AYDIV&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#38598;&#25104;&#20102;&#19968;&#20010;&#29305;&#27530;&#35774;&#35745;&#30340;&#19977;&#38454;&#27573;&#23545;&#40784;&#36807;&#31243;&#65292;&#19987;&#38376;&#29992;&#20110;&#22686;&#24378;&#22312;&#25968;&#25454;&#24046;&#24322;&#20013;&#24310;&#20280;&#36317;&#31163;&#30340;&#26816;&#27979;&#33021;&#21147;&#12290;AYDIV&#21253;&#25324;&#20840;&#23616;&#19978;&#19979;&#25991;&#34701;&#21512;&#23545;&#40784;&#21464;&#25442;&#22120;&#65288;GCFAT&#65289;&#65292;&#23427;&#25913;&#21892;&#30456;&#26426;&#29305;&#24449;&#30340;&#25552;&#21462;&#24182;&#23545;&#22823;&#35268;&#27169;&#27169;&#24335;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#29702;&#35299;&#65307;&#31232;&#30095;&#34701;&#21512;&#29305;&#24449;&#27880;&#24847;&#21147;&#65288;SFFA&#65289;&#65292;&#23427;&#23545;LiDAR&#21644;&#30456;&#26426;&#32454;&#33410;&#30340;&#34701;&#21512;&#36827;&#34892;&#24494;&#35843;&#65307;&#20197;&#21450;&#29992;&#20110;&#20840;&#38754;&#31354;&#38388;&#25968;&#25454;&#34701;&#21512;&#30340;&#20307;&#31215;&#32593;&#26684;&#27880;&#24847;&#21147;&#65288;VGA&#65289;&#12290;AYDIV&#22312;Waymo&#24320;&#25918;&#25968;&#25454;&#38598;&#65288;WOD&#65289;&#19978;&#30340;&#24615;&#33021;&#25552;&#21319;&#20102;1.24%&#30340;&#24179;&#22343;&#31934;&#24230;&#65288;mA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Combining LiDAR and camera data has shown potential in enhancing short-distance object detection in autonomous driving systems. Yet, the fusion encounters difficulties with extended distance detection due to the contrast between LiDAR's sparse data and the dense resolution of cameras. Besides, discrepancies in the two data representations further complicate fusion methods. We introduce AYDIV, a novel framework integrating a tri-phase alignment process specifically designed to enhance long-distance detection even amidst data discrepancies. AYDIV consists of the Global Contextual Fusion Alignment Transformer (GCFAT), which improves the extraction of camera features and provides a deeper understanding of large-scale patterns; the Sparse Fused Feature Attention (SFFA), which fine-tunes the fusion of LiDAR and camera details; and the Volumetric Grid Attention (VGA) for a comprehensive spatial data fusion. AYDIV's performance on the Waymo Open Dataset (WOD) with an improvement of 1.24% in mA
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25511;&#30340;&#22810;&#27169;&#24577;&#21453;&#39304;&#21512;&#25104;&#31995;&#32479;&#65292;&#33021;&#22815;&#26681;&#25454;&#25991;&#26412;&#21644;&#22270;&#20687;&#36755;&#20837;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#24773;&#24863;&#65288;&#31215;&#26497;&#25110;&#28040;&#26497;&#65289;&#30340;&#21453;&#39304;&#65292;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2402.07640</link><description>&lt;p&gt;
&#21512;&#25104;&#23545;&#22810;&#27169;&#24577;&#25991;&#26412;&#21644;&#22270;&#29255;&#25968;&#25454;&#30340;&#24773;&#24863;&#25511;&#21046;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Synthesizing Sentiment-Controlled Feedback For Multimodal Text and Image Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07640
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25511;&#30340;&#22810;&#27169;&#24577;&#21453;&#39304;&#21512;&#25104;&#31995;&#32479;&#65292;&#33021;&#22815;&#26681;&#25454;&#25991;&#26412;&#21644;&#22270;&#20687;&#36755;&#20837;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#24773;&#24863;&#65288;&#31215;&#26497;&#25110;&#28040;&#26497;&#65289;&#30340;&#21453;&#39304;&#65292;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#22810;&#27169;&#24577;&#36755;&#20837;&#65288;&#21253;&#25324;&#25991;&#26412;&#21644;&#22270;&#29255;&#65289;&#30340;&#24773;&#24863;&#25511;&#21046;&#21453;&#39304;&#33021;&#22815;&#24357;&#34917;&#20154;&#26426;&#20132;&#20114;&#39046;&#22495;&#30340;&#19968;&#20010;&#20851;&#38190;&#24046;&#36317;&#65292;&#20351;&#31995;&#32479;&#33021;&#22815;&#25552;&#20379;&#20855;&#26377;&#21516;&#29702;&#24515;&#12289;&#20934;&#30830;&#24615;&#21644;&#24341;&#20154;&#20837;&#32988;&#30340;&#22238;&#24212;&#12290;&#36825;&#31181;&#33021;&#21147;&#22312;&#21307;&#30103;&#12289;&#33829;&#38144;&#21644;&#25945;&#32946;&#31561;&#39046;&#22495;&#26377;&#30528;&#28145;&#36828;&#30340;&#24212;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#21487;&#25511;&#22810;&#27169;&#24577;&#21453;&#39304;&#21512;&#25104;&#65288;CMFeed&#65289;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25511;&#30340;&#21453;&#39304;&#21512;&#25104;&#31995;&#32479;&#12290;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#21253;&#25324;&#19968;&#20010;&#32534;&#30721;&#22120;&#12289;&#35299;&#30721;&#22120;&#21644;&#25511;&#21046;&#24615;&#27169;&#22359;&#65292;&#29992;&#20110;&#22788;&#29702;&#25991;&#26412;&#21644;&#35270;&#35273;&#36755;&#20837;&#12290;&#23427;&#20351;&#29992;Transformer&#21644;Faster R-CNN&#32593;&#32476;&#25552;&#21462;&#25991;&#26412;&#21644;&#35270;&#35273;&#29305;&#24449;&#65292;&#24182;&#23558;&#23427;&#20204;&#32467;&#21512;&#36215;&#26469;&#29983;&#25104;&#21453;&#39304;&#12290;CMFeed&#25968;&#25454;&#38598;&#21253;&#21547;&#22270;&#29255;&#12289;&#25991;&#26412;&#12289;&#23545;&#24086;&#23376;&#30340;&#21453;&#24212;&#12289;&#24102;&#26377;&#30456;&#20851;&#24615;&#35780;&#20998;&#30340;&#20154;&#31867;&#35780;&#35770;&#20197;&#21450;&#23545;&#35780;&#35770;&#30340;&#21453;&#24212;&#12290;&#23545;&#24086;&#23376;&#21644;&#35780;&#35770;&#30340;&#21453;&#24212;&#34987;&#29992;&#26469;&#35757;&#32451;&#25552;&#20986;&#30340;&#27169;&#22411;&#20197;&#20135;&#29983;&#20855;&#26377;&#29305;&#23450;&#65288;&#31215;&#26497;&#25110;&#28040;&#26497;&#65289;&#24773;&#24863;&#30340;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to generate sentiment-controlled feedback in response to multimodal inputs, comprising both text and images, addresses a critical gap in human-computer interaction by enabling systems to provide empathetic, accurate, and engaging responses. This capability has profound applications in healthcare, marketing, and education. To this end, we construct a large-scale Controllable Multimodal Feedback Synthesis (CMFeed) dataset and propose a controllable feedback synthesis system. The proposed system includes an encoder, decoder, and controllability block for textual and visual inputs. It extracts textual and visual features using a transformer and Faster R-CNN networks and combines them to generate feedback. The CMFeed dataset encompasses images, text, reactions to the post, human comments with relevance scores, and reactions to the comments. The reactions to the post and comments are utilized to train the proposed model to produce feedback with a particular (positive or negative)
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#20449;&#24687;&#29942;&#39048;&#26356;&#20026;&#32039;&#23494;&#30340;&#21464;&#20998;&#30028;&#38480;&#65292;&#21487;&#20197;&#25913;&#21892;&#20197;&#21069;&#30340;&#22522;&#20110;&#20449;&#24687;&#29942;&#39048;&#30340;DNNs&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#26041;&#27861;&#26469;&#26174;&#33879;&#22686;&#24378;&#20998;&#31867;&#22120;DNNs&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07639</link><description>&lt;p&gt;
&#23545;&#20449;&#24687;&#29942;&#39048;&#30340;&#38480;&#21046;&#26356;&#32039;&#30340;&#30028;&#38480;&#65292;&#24182;&#24212;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Tighter Bounds on the Information Bottleneck with Application to Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07639
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#20449;&#24687;&#29942;&#39048;&#26356;&#20026;&#32039;&#23494;&#30340;&#21464;&#20998;&#30028;&#38480;&#65292;&#21487;&#20197;&#25913;&#21892;&#20197;&#21069;&#30340;&#22522;&#20110;&#20449;&#24687;&#29942;&#39048;&#30340;DNNs&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#26041;&#27861;&#26469;&#26174;&#33879;&#22686;&#24378;&#20998;&#31867;&#22120;DNNs&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#36890;&#36807;&#19979;&#28216;&#20219;&#21153;&#12289;&#30446;&#26631;&#20989;&#25968;&#21644;&#20854;&#20182;&#21442;&#25968;&#26469;&#23398;&#20064;&#24341;&#21457;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#30340;&#36136;&#37327;&#24433;&#21709;&#30528;DNN&#30340;&#27010;&#25324;&#33021;&#21147;&#21644;&#26032;&#20986;&#29616;&#30340;&#28508;&#22312;&#31354;&#38388;&#30340;&#36830;&#36143;&#24615;&#12290;&#20449;&#24687;&#29942;&#39048;&#65288;IB&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#29702;&#35770;&#19978;&#26368;&#20248;&#30340;&#25968;&#25454;&#24314;&#27169;&#26694;&#26550;&#65292;&#20294;&#36890;&#24120;&#26159;&#38590;&#20197;&#22788;&#29702;&#30340;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24037;&#20316;&#23558;DNNs&#19982;IB&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#24212;&#29992;VAE-inspire&#30340;&#21464;&#20998;&#26041;&#27861;&#26469;&#36817;&#20284;&#30456;&#20114;&#20449;&#24687;&#30340;&#30028;&#38480;&#65292;&#20174;&#32780;&#25552;&#39640;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#21644;&#26356;&#32039;&#30340;&#21464;&#20998;&#30028;&#38480;&#65292;&#25552;&#39640;&#20102;&#20197;&#21069;IB-inspire DNNs&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#36827;&#23637;&#21152;&#24378;&#20102;IB&#21450;&#20854;&#21464;&#20998;&#36817;&#20284;&#20316;&#20026;&#25968;&#25454;&#27169;&#22411;&#26694;&#26550;&#30340;&#35770;&#28857;&#65292;&#24182;&#20026;&#20998;&#31867;&#22120;DNNs&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Nets (DNNs) learn latent representations induced by their downstream task, objective function, and other parameters. The quality of the learned representations impacts the DNN's generalization ability and the coherence of the emerging latent space. The Information Bottleneck (IB) provides a hypothetically optimal framework for data modeling, yet it is often intractable. Recent efforts combined DNNs with the IB by applying VAE-inspired variational methods to approximate bounds on mutual information, resulting in improved robustness to adversarial attacks. This work introduces a new and tighter variational bound for the IB, improving performance of previous IB-inspired DNNs. These advancements strengthen the case for the IB and its variational approximations as a data modeling framework, and provide a simple method to significantly enhance the adversarial robustness of classifier DNNs.
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#30340;&#36807;&#24230;&#33258;&#20449;&#21644;&#32570;&#20047;&#33258;&#20449;&#20250;&#38459;&#30861;&#20154;&#26426;&#21327;&#20316;&#65292;&#25259;&#38706;&#20449;&#24515;&#27700;&#24179;&#21644;&#25552;&#20379;&#21453;&#39304;&#26377;&#21161;&#20110;&#35748;&#35782;&#21040;&#20154;&#24037;&#26234;&#33021;&#30340;&#20449;&#24515;&#19981;&#19968;&#33268;&#65292;&#20294;&#21442;&#19982;&#32773;&#24448;&#24448;&#22240;&#27492;&#19981;&#20449;&#20219;&#20154;&#24037;&#26234;&#33021;&#30340;&#24314;&#35758;&#65292;&#23548;&#33268;&#21327;&#20316;&#32467;&#26524;&#36739;&#24046;&#12290;</title><link>https://arxiv.org/abs/2402.07632</link><description>&lt;p&gt;
&#36807;&#20110;&#33258;&#20449;&#21644;&#32570;&#20047;&#33258;&#20449;&#30340;&#20154;&#24037;&#26234;&#33021;&#38459;&#30861;&#20154;&#26426;&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
Overconfident and Unconfident AI Hinder Human-AI Collaboration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07632
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#36807;&#24230;&#33258;&#20449;&#21644;&#32570;&#20047;&#33258;&#20449;&#20250;&#38459;&#30861;&#20154;&#26426;&#21327;&#20316;&#65292;&#25259;&#38706;&#20449;&#24515;&#27700;&#24179;&#21644;&#25552;&#20379;&#21453;&#39304;&#26377;&#21161;&#20110;&#35748;&#35782;&#21040;&#20154;&#24037;&#26234;&#33021;&#30340;&#20449;&#24515;&#19981;&#19968;&#33268;&#65292;&#20294;&#21442;&#19982;&#32773;&#24448;&#24448;&#22240;&#27492;&#19981;&#20449;&#20219;&#20154;&#24037;&#26234;&#33021;&#30340;&#24314;&#35758;&#65292;&#23548;&#33268;&#21327;&#20316;&#32467;&#26524;&#36739;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#27493;&#65292;&#20154;&#26426;&#21327;&#20316;&#22312;&#19987;&#19994;&#21644;&#26085;&#24120;&#22330;&#26223;&#20013;&#36234;&#26469;&#36234;&#26222;&#36941;&#12290;&#22312;&#36825;&#31181;&#21327;&#20316;&#20013;&#65292;&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#34920;&#36798;&#20854;&#23545;&#33258;&#24049;&#34920;&#29616;&#30340;&#20449;&#24515;&#27700;&#24179;&#65292;&#20316;&#20026;&#20154;&#31867;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#24314;&#35758;&#30340;&#37325;&#35201;&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#26234;&#33021;&#21487;&#33021;&#34920;&#29616;&#20986;&#36807;&#24230;&#33258;&#20449;&#25110;&#32570;&#20047;&#33258;&#20449;&#65292;&#21363;&#20854;&#34920;&#36798;&#30340;&#20449;&#24515;&#39640;&#20110;&#25110;&#20302;&#20110;&#20854;&#23454;&#38469;&#34920;&#29616;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#20154;&#20204;&#38169;&#35823;&#22320;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#30340;&#24314;&#35758;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35843;&#26597;&#20102;&#20154;&#24037;&#26234;&#33021;&#36807;&#24230;&#33258;&#20449;&#21644;&#32570;&#20047;&#33258;&#20449;&#23545;&#20154;&#31867;&#20449;&#20219;&#12289;&#25509;&#21463;&#20154;&#24037;&#26234;&#33021;&#24314;&#35758;&#21644;&#21327;&#20316;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#25259;&#38706;&#20154;&#24037;&#26234;&#33021;&#30340;&#20449;&#24515;&#27700;&#24179;&#21644;&#34920;&#29616;&#21453;&#39304;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#35748;&#35782;&#20154;&#24037;&#26234;&#33021;&#20449;&#24515;&#19981;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#21442;&#19982;&#32773;&#24448;&#24448;&#20250;&#22240;&#20026;&#23519;&#35273;&#21040;&#36825;&#31181;&#19981;&#19968;&#33268;&#32780;&#19981;&#20449;&#20219;&#20154;&#24037;&#26234;&#33021;&#30340;&#24314;&#35758;&#65292;&#23548;&#33268;&#25298;&#32477;&#20154;&#24037;&#26234;&#33021;&#30340;&#24314;&#35758;&#65292;&#24182;&#19988;&#22312;&#21327;&#20316;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#24046;&#12290;&#30456;&#21453;&#65292;&#27809;&#26377;&#36825;&#20123;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#21442;&#19982;&#32773;&#26356;&#23481;&#26131;&#20449;&#20219;&#20154;&#24037;&#26234;&#33021;&#24182;&#25509;&#21463;&#20854;&#24314;&#35758;&#65292;&#20174;&#32780;&#22312;&#21327;&#20316;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
As artificial intelligence (AI) advances, human-AI collaboration has become increasingly prevalent across both professional and everyday settings. In such collaboration, AI can express its confidence level about its performance, serving as a crucial indicator for humans to evaluate AI's suggestions. However, AI may exhibit overconfidence or underconfidence--its expressed confidence is higher or lower than its actual performance--which may lead humans to mistakenly evaluate AI advice. Our study investigates the influences of AI's overconfidence and underconfidence on human trust, their acceptance of AI suggestions, and collaboration outcomes. Our study reveal that disclosing AI confidence levels and performance feedback facilitates better recognition of AI confidence misalignments. However, participants tend to withhold their trust as perceiving such misalignments, leading to a rejection of AI suggestions and subsequently poorer performance in collaborative tasks. Conversely, without su
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#20027;&#25968;&#25454;&#36873;&#25321;&#31574;&#30053;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#23398;&#25991;&#26412;&#30340;&#33258;&#21160;&#35780;&#20272;&#21644;&#36873;&#25321;&#65292;&#24182;&#36890;&#36807;&#36830;&#32493;&#39044;&#35757;&#32451;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#20027;&#35201;&#21019;&#26032;&#21253;&#25324;&#21033;&#29992;&#20803;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#39564;&#35777;&#22120;&#65292;&#21457;&#24067;&#20102;&#39640;&#36136;&#37327;&#30340;AutoMathText&#25968;&#25454;&#38598;&#65292;&#24182;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#20196;&#29260;&#25928;&#29575;&#30340;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.07625</link><description>&lt;p&gt;
AutoMathText&#65306;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#23398;&#25991;&#26412;&#30340;&#33258;&#20027;&#25968;&#25454;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
AutoMathText: Autonomous Data Selection with Language Models for Mathematical Texts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#20027;&#25968;&#25454;&#36873;&#25321;&#31574;&#30053;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#23398;&#25991;&#26412;&#30340;&#33258;&#21160;&#35780;&#20272;&#21644;&#36873;&#25321;&#65292;&#24182;&#36890;&#36807;&#36830;&#32493;&#39044;&#35757;&#32451;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#20027;&#35201;&#21019;&#26032;&#21253;&#25324;&#21033;&#29992;&#20803;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#39564;&#35777;&#22120;&#65292;&#21457;&#24067;&#20102;&#39640;&#36136;&#37327;&#30340;AutoMathText&#25968;&#25454;&#38598;&#65292;&#24182;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#20196;&#29260;&#25928;&#29575;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36890;&#36807;&#25345;&#32493;&#30340;&#39044;&#35757;&#32451;&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#20027;&#25968;&#25454;&#36873;&#25321;&#12290;&#19982;&#20256;&#32479;&#30340;&#26377;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#30340;&#30417;&#30563;&#24494;&#35843;&#25110;&#35757;&#32451;&#36807;&#30340;&#20998;&#31867;&#22120;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20803;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;&#26679;&#26412;&#39564;&#35777;&#22120;&#65292;&#33258;&#20027;&#35780;&#20272;&#21644;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#25968;&#23398;&#20869;&#23481;&#65292;&#24182;&#21457;&#24067;&#20102;&#32463;&#36807;&#31574;&#21010;&#30340;&#24320;&#28304;AutoMathText&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;200GB&#30340;&#25968;&#25454;&#12290;&#20026;&#20102;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23545;AutoMathText&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#36830;&#32493;&#39044;&#35757;&#32451;&#65292;&#20351;&#24471;7B&#21442;&#25968;&#30340;Mistral&#35821;&#35328;&#27169;&#22411;&#22312;MATH&#25968;&#25454;&#38598;&#19978;&#30340;&#19979;&#28216;&#24615;&#33021;&#22823;&#24133;&#25552;&#21319;&#65292;&#32780;&#20196;&#29260;&#25968;&#37327;&#27604;&#20043;&#21069;&#30340;&#36830;&#32493;&#39044;&#35757;&#32451;&#24037;&#20316;&#20943;&#23569;&#20102;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#22522;&#20934;&#30340;&#39044;&#35757;&#32451;&#20196;&#29260;&#25928;&#29575;&#25552;&#39640;&#20102;2&#20493;&#65292;&#31361;&#26174;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#22686;&#24378;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
To improve language models' proficiency in mathematical reasoning via continual pretraining, we introduce a novel strategy that leverages base language models for autonomous data selection. Departing from conventional supervised fine-tuning or trained classifiers with human-annotated data, our approach utilizes meta-prompted language models as zero-shot verifiers to autonomously evaluate and select high-quality mathematical content, and we release the curated open-source AutoMathText dataset encompassing over 200GB of data. To demonstrate the efficacy of our method, we continuously pretrained a 7B-parameter Mistral language model on the AutoMathText dataset, achieving substantial improvements in downstream performance on the MATH dataset with a token amount reduced by orders of magnitude compared to previous continuous pretraining works. Our method showcases a 2 times increase in pretraining token efficiency compared to baselines, underscoring the potential of our approach in enhancing
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#22768;&#38899;&#24405;&#21046;&#25968;&#25454;&#35782;&#21035;COVID-19&#12290;&#36890;&#36807;&#25552;&#21462;&#35821;&#38899;&#29305;&#24449;&#24182;&#37319;&#29992;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#27169;&#22411;&#65292;HuBERT&#23454;&#29616;&#20102;86%&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.07619</link><description>&lt;p&gt;
&#20174;&#20247;&#21253;&#21628;&#21560;&#22768;&#25968;&#25454;&#24320;&#21457;&#22810;&#21464;&#37327;COVID-19&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Developing a Multi-variate Prediction Model For COVID-19 From Crowd-sourced Respiratory Voice Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#22768;&#38899;&#24405;&#21046;&#25968;&#25454;&#35782;&#21035;COVID-19&#12290;&#36890;&#36807;&#25552;&#21462;&#35821;&#38899;&#29305;&#24449;&#24182;&#37319;&#29992;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#27169;&#22411;&#65292;HuBERT&#23454;&#29616;&#20102;86%&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19&#24050;&#32463;&#24433;&#21709;&#20102;&#20840;&#29699;223&#20010;&#22269;&#23478;&#65292;&#22312;&#21518;COVID&#26102;&#20195;&#65292;&#36843;&#20999;&#38656;&#35201;&#38750;&#20405;&#20837;&#24615;&#12289;&#20302;&#25104;&#26412;&#12289;&#39640;&#21487;&#25193;&#23637;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#26816;&#27979;COVID-19&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20174;&#35821;&#38899;&#24405;&#21046;&#25968;&#25454;&#20013;&#35782;&#21035;COVID-19&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#20165;&#20351;&#29992;&#35821;&#38899;&#24405;&#38899;&#24320;&#21457;COVID-19&#35782;&#21035;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#21073;&#26725;COVID-19&#22768;&#38899;&#25968;&#25454;&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;893&#20010;&#35821;&#38899;&#26679;&#26412;&#65292;&#36890;&#36807;COVID-19&#22768;&#38899;&#24212;&#29992;&#31243;&#24207;&#30001;4352&#21517;&#21442;&#19982;&#32773;&#20247;&#21253;&#32780;&#26469;&#12290;&#25552;&#21462;&#20102;&#21253;&#25324;Mel&#39057;&#35889;&#22270;&#21644;Mel&#39057;&#29575;&#20498;&#35889;&#31995;&#25968;(MFCC)&#22312;&#20869;&#30340;&#35821;&#38899;&#29305;&#24449;&#20197;&#21450;CNN&#32534;&#30721;&#22120;&#29305;&#24449;&#12290;&#22522;&#20110;&#35821;&#38899;&#25968;&#25454;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#27169;&#22411;&#26469;&#26816;&#27979;COVID-19&#30149;&#20363;&#12290;&#36825;&#20123;&#27169;&#22411;&#21253;&#25324;&#38271;&#30701;&#26399;&#35760;&#24518;(LSTM)&#12289;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#21644;&#38544;&#34255;&#21333;&#20803;BERT(HuBERT)&#12290;&#25105;&#20204;&#23558;&#23427;&#20204;&#30340;&#39044;&#27979;&#33021;&#21147;&#19982;&#22522;&#32447;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;HuBERT&#21462;&#24471;&#20102;86%&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
COVID-19 has affected more than 223 countries worldwide and in the Post-COVID Era, there is a pressing need for non-invasive, low-cost, and highly scalable solutions to detect COVID-19. We develop a deep learning model to identify COVID-19 from voice recording data. The novelty of this work is in the development of deep learning models for COVID-19 identification from only voice recordings. We use the Cambridge COVID-19 Sound database which contains 893 speech samples, crowd-sourced from 4352 participants via a COVID-19 Sounds app. Voice features including Mel-spectrograms and Mel-frequency cepstral coefficients (MFCC) and CNN Encoder features are extracted. Based on the voice data, we develop deep learning classification models to detect COVID-19 cases. These models include Long Short-Term Memory (LSTM) and Convolutional Neural Network (CNN) and Hidden-Unit BERT (HuBERT). We compare their predictive power to baseline machine learning models. HuBERT achieves the highest accuracy of 86\
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#38170;&#28857;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;AnLLM&#65289;&#36890;&#36807;&#24341;&#20837;&#21019;&#26032;&#30340;&#22522;&#20110;&#38170;&#28857;&#30340;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;AnSAN&#65289;&#21644;&#22522;&#20110;&#38170;&#28857;&#30340;&#25512;&#29702;&#31574;&#30053;&#65292;&#23558;&#24207;&#21015;&#20449;&#24687;&#21387;&#32553;&#21040;&#38170;&#28857;&#26631;&#35760;&#20013;&#65292;&#20943;&#23569;&#38190;/&#20540;&#32531;&#23384;&#65292;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.07616</link><description>&lt;p&gt;
&#22522;&#20110;&#38170;&#28857;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Anchor-based Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07616
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#38170;&#28857;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;AnLLM&#65289;&#36890;&#36807;&#24341;&#20837;&#21019;&#26032;&#30340;&#22522;&#20110;&#38170;&#28857;&#30340;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;AnSAN&#65289;&#21644;&#22522;&#20110;&#38170;&#28857;&#30340;&#25512;&#29702;&#31574;&#30053;&#65292;&#23558;&#24207;&#21015;&#20449;&#24687;&#21387;&#32553;&#21040;&#38170;&#28857;&#26631;&#35760;&#20013;&#65292;&#20943;&#23569;&#38190;/&#20540;&#32531;&#23384;&#65292;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20027;&#35201;&#37319;&#29992;&#20165;&#35299;&#30721;&#22120;&#30340;&#36716;&#25442;&#22120;&#26550;&#26500;&#65292;&#38656;&#35201;&#20445;&#30041;&#21382;&#21490;&#26631;&#35760;&#30340;&#38190;/&#20540;&#20449;&#24687;&#20197;&#25552;&#20379;&#19978;&#19979;&#25991;&#20449;&#24687;&#24182;&#36991;&#20813;&#20887;&#20313;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;LLMs&#30340;&#24040;&#22823;&#22823;&#23567;&#21644;&#21442;&#25968;&#37327;&#38656;&#35201;&#22823;&#37327;&#30340;GPU&#20869;&#23384;&#12290;&#36825;&#31181;&#20869;&#23384;&#38656;&#27714;&#38543;&#30528;&#36755;&#20837;&#25991;&#26412;&#30340;&#38271;&#24230;&#32780;&#22686;&#21152;&#65292;&#36843;&#20999;&#38656;&#35201;&#26356;&#39640;&#25928;&#30340;&#20449;&#24687;&#23384;&#20648;&#21644;&#22788;&#29702;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38170;&#28857;&#30340;LLM&#65288;AnLLM&#65289;&#65292;&#23427;&#21033;&#29992;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#38170;&#28857;&#30340;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;AnSAN&#65289;&#21644;&#22522;&#20110;&#38170;&#28857;&#30340;&#25512;&#29702;&#31574;&#30053;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;LLMs&#33021;&#22815;&#23558;&#24207;&#21015;&#20449;&#24687;&#21387;&#32553;&#25104;&#38170;&#28857;&#26631;&#35760;&#65292;&#20943;&#23569;&#38190;/&#20540;&#32531;&#23384;&#24182;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;AnLLM&#22312;&#20943;&#23569;&#38190;/&#20540;&#32531;&#23384;&#39640;&#36798;99%&#21644;&#25512;&#29702;&#36895;&#24230;&#25552;&#39640;&#39640;&#36798;3.5&#20493;&#30340;&#21516;&#26102;&#65292;&#20173;&#20445;&#25345;&#21487;&#27604;&#30340;&#20934;&#30830;&#24615;&#12290;&#23613;&#31649;&#29306;&#29298;&#20102;&#19968;&#20123;&#20934;&#30830;&#24615;&#65292;AnLLM&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#20381;&#28982;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) predominantly employ decoder-only transformer architectures, necessitating the retention of keys/values information for historical tokens to provide contextual information and avoid redundant computation. However, the substantial size and parameter volume of these LLMs require massive GPU memory. This memory demand increases with the length of the input text, leading to an urgent need for more efficient methods of information storage and processing. This study introduces the Anchor-based LLM (AnLLM), which utilizes an innovative anchor-based self-attention network (AnSAN) and also an anchor-based inference strategy. This approach enables LLMs to compress sequence information into an anchor token, reducing the keys/values cache and enhancing inference efficiency. Experiments show that the AnLLM maintains comparable accuracy with up to 99% keys/values cache reduction and up to 3.5 times faster inference. Despite a minor compromise in accuracy, the AnLLM signi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#25506;&#32034;&#20102;&#33258;&#21161;&#24341;&#23548;&#33258;&#23545;&#40784;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20854;&#26126;&#26174;&#20248;&#20110;&#21333;&#27425;&#24490;&#29615;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#25968;&#25454;&#35757;&#32451;&#39034;&#24207;&#36827;&#19968;&#27493;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.07610</link><description>&lt;p&gt;
&#36393;&#33050;&#35843;&#26657;&#65306;&#36890;&#36807;&#33258;&#21161;&#24341;&#23548;&#25193;&#23637;LLM&#30340;&#33258;&#23545;&#40784;&#33021;&#21147;&#30340;&#35268;&#27169;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25506;&#32034;&#20102;&#33258;&#21161;&#24341;&#23548;&#33258;&#23545;&#40784;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20854;&#26126;&#26174;&#20248;&#20110;&#21333;&#27425;&#24490;&#29615;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#25968;&#25454;&#35757;&#32451;&#39034;&#24207;&#36827;&#19968;&#27493;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#23545;&#40784;&#26159;&#19968;&#31181;&#38477;&#20302;&#20154;&#24037;&#27880;&#37322;&#25104;&#26412;&#24182;&#30830;&#20445;&#27169;&#22411;&#33021;&#21147;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;&#26041;&#27861;&#22312;&#21333;&#27425;&#24490;&#29615;&#20013;&#23436;&#25104;&#25968;&#25454;&#25910;&#38598;&#21644;&#35757;&#32451;&#27493;&#39588;&#65292;&#21487;&#33021;&#24573;&#35270;&#20102;&#33258;&#23545;&#40784;&#27169;&#22411;&#19981;&#26029;&#25913;&#36827;&#30340;&#33021;&#21147;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#22914;&#26524;&#25105;&#20204;&#36827;&#34892;&#22810;&#27425;&#33258;&#21161;&#24341;&#23548;&#33258;&#23545;&#40784;&#65292;&#20250;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#36824;&#26159;&#23548;&#33268;&#24555;&#36895;&#36864;&#21270;&#65311;&#26412;&#25991;&#39318;&#27425;&#25506;&#32034;&#20102;&#33258;&#21161;&#24341;&#23548;&#33258;&#23545;&#40784;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#20445;&#35777;&#20174;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#33719;&#24471;&#30340;&#25968;&#25454;&#22810;&#26679;&#24615;&#65292;&#33258;&#21161;&#24341;&#23548;&#33258;&#23545;&#40784;&#26126;&#26174;&#20248;&#20110;&#21333;&#27425;&#24490;&#29615;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#21457;&#25381;&#33258;&#21161;&#24341;&#23548;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#24182;&#35843;&#25972;&#20102;&#25968;&#25454;&#30340;&#35757;&#32451;&#39034;&#24207;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36393;&#33050;&#35843;&#26657;&#65288;SOFT&#65289;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#27169;&#22411;&#30340;&#25345;&#32493;&#22686;&#24378;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-alignment is an effective way to reduce the cost of human annotation while ensuring promising model capability. However, most current methods complete the data collection and training steps in a single round, which may overlook the continuously improving ability of self-aligned models. This gives rise to a key query: What if we do multi-time bootstrapping self-alignment? Does this strategy enhance model performance or lead to rapid degradation? In this paper, our pioneering exploration delves into the impact of bootstrapping self-alignment on large language models. Our findings reveal that bootstrapping self-alignment markedly surpasses the single-round approach, by guaranteeing data diversity from in-context learning. To further exploit the capabilities of bootstrapping, we investigate and adjust the training order of data, which yields improved performance of the model. Drawing on these findings, we propose Step-On-Feet Tuning (SOFT) which leverages model's continuously enhanced
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19979;&#19968;&#20010;&#26354;&#32447;&#24418;&#29366;&#39044;&#27979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;&#38646;&#26679;&#26412;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;GTT&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#36890;&#36947;&#32423;&#21035;&#30340;&#26354;&#32447;&#24418;&#29366;&#39044;&#27979;&#65292;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#26377;&#30417;&#30563;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.07570</link><description>&lt;p&gt;
&#21482;&#26377;&#26354;&#32447;&#24418;&#29366;&#26377;&#20851;&#65306;&#36890;&#36807;&#19979;&#19968;&#20010;&#26354;&#32447;&#24418;&#29366;&#39044;&#27979;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Only the Curve Shape Matters: Training Foundation Models for Zero-Shot Multivariate Time Series Forecasting through Next Curve Shape Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07570
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19979;&#19968;&#20010;&#26354;&#32447;&#24418;&#29366;&#39044;&#27979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;&#38646;&#26679;&#26412;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;GTT&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#36890;&#36947;&#32423;&#21035;&#30340;&#26354;&#32447;&#24418;&#29366;&#39044;&#27979;&#65292;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#26377;&#30417;&#30563;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;General Time Transformer (GTT)&#65292;&#19968;&#31181;&#20165;&#26377;&#32534;&#30721;&#22120;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;GTT&#22312;&#19968;&#20010;&#21253;&#21547;2&#20159;&#20010;&#39640;&#36136;&#37327;&#26102;&#38388;&#24207;&#21015;&#26679;&#26412;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28085;&#30422;&#20102;&#19981;&#21516;&#39046;&#22495;&#12290;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#20013;&#65292;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#20219;&#21153;&#34987;&#24314;&#27169;&#20026;&#19968;&#20010;&#36880;&#36890;&#36947;&#30340;&#19979;&#19968;&#20010;&#26354;&#32447;&#24418;&#29366;&#39044;&#27979;&#38382;&#39064;&#65292;&#20854;&#20013;&#27599;&#20010;&#26102;&#38388;&#24207;&#21015;&#26679;&#26412;&#34920;&#31034;&#20026;&#19968;&#31995;&#21015;&#38750;&#37325;&#21472;&#30340;&#26354;&#32447;&#24418;&#29366;&#65292;&#20855;&#26377;&#32479;&#19968;&#30340;&#25968;&#20540;&#22823;&#23567;&#12290;GTT&#22312;&#36890;&#36947;&#32423;&#21035;&#19978;&#36890;&#36807;&#39044;&#27979;&#36807;&#21435;&#26354;&#32447;&#24418;&#29366;&#30340;&#31383;&#21475;&#26469;&#39044;&#27979;&#19979;&#19968;&#20010;&#26354;&#32447;&#24418;&#29366;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GTT&#22312;&#26410;&#35265;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#38646;&#26679;&#26412;&#22810;&#20803;&#39044;&#27979;&#33021;&#21147;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#26377;&#30417;&#30563;&#22522;&#32447;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;GTT&#27169;&#22411;&#21442;&#25968;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#35268;&#27169;&#21464;&#21270;&#30340;&#24433;&#21709;&#65292;&#35266;&#23519;&#21040;&#22312;&#38646;&#26679;&#26412;&#22810;&#20803;&#39044;&#27979;&#30340;&#32972;&#26223;&#19979;&#65292;&#35268;&#27169;&#23450;&#24459;&#20063;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present General Time Transformer (GTT), an encoder-only style foundation model for zero-shot multivariate time series forecasting. GTT is pretrained on a large dataset of 200M high-quality time series samples spanning diverse domains. In our proposed framework, the task of multivariate time series forecasting is formulated as a channel-wise next curve shape prediction problem, where each time series sample is represented as a sequence of non-overlapping curve shapes with a unified numerical magnitude. GTT is trained to predict the next curve shape based on a window of past curve shapes in a channel-wise manner. Experimental results demonstrate that GTT exhibits superior zero-shot multivariate forecasting capabilities on unseen time series datasets, even surpassing state-of-the-art supervised baselines. Additionally, we investigate the impact of varying GTT model parameters and training dataset scales, observing that the scaling law also holds in the context of zero-shot multivariate
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21160;&#24577;&#36923;&#36753;&#30340;&#33258;&#26816;&#25216;&#26415;&#65292;&#26088;&#22312;&#30830;&#20445;&#26234;&#33021;&#36923;&#36753;&#20195;&#29702;&#30340;&#21487;&#20449;&#21644;&#36947;&#24503;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2402.07547</link><description>&lt;p&gt;
&#30830;&#20445;&#26234;&#33021;&#36923;&#36753;&#20195;&#29702;&#30340;&#21487;&#20449;&#21644;&#36947;&#24503;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Ensuring trustworthy and ethical behaviour in intelligent logical agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07547
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21160;&#24577;&#36923;&#36753;&#30340;&#33258;&#26816;&#25216;&#26415;&#65292;&#26088;&#22312;&#30830;&#20445;&#26234;&#33021;&#36923;&#36753;&#20195;&#29702;&#30340;&#21487;&#20449;&#21644;&#36947;&#24503;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#26234;&#33021;&#20195;&#29702;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#34987;&#20351;&#29992;&#65292;&#36825;&#20123;&#24212;&#29992;&#21487;&#33021;&#28041;&#21450;&#21040;&#29983;&#21629;&#21644;&#31119;&#31049;&#20197;&#21450;&#37325;&#35201;&#30340;&#31038;&#20250;&#21151;&#33021;&#12290;&#22240;&#27492;&#65292;&#20195;&#29702;&#24212;&#35813;&#26159;&#21487;&#20449;&#30340;&#12290;&#20808;&#39564;&#35748;&#35777;&#25216;&#26415;&#65288;&#21363;&#22312;&#31995;&#32479;&#37096;&#32626;&#20043;&#21069;&#24212;&#29992;&#30340;&#25216;&#26415;&#65289;&#21487;&#33021;&#26159;&#26377;&#29992;&#30340;&#65292;&#20294;&#23545;&#20110;&#28436;&#21464;&#30340;&#20195;&#29702;&#21644;&#24320;&#25918;&#30340;&#22810;&#20195;&#29702;&#31995;&#32479;&#26469;&#35828;&#65292;&#36825;&#26159;&#19981;&#22815;&#30340;&#65292;&#22240;&#20026;&#24322;&#26500;&#20195;&#29702;&#21487;&#20197;&#22312;&#31995;&#32479;&#30340;&#20219;&#20309;&#38454;&#27573;&#21152;&#20837;&#25110;&#31163;&#24320;&#31995;&#32479;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;/&#25913;&#36827;/&#25193;&#23637;&#20102;&#22522;&#20110;&#21160;&#24577;&#65288;&#36816;&#34892;&#26102;&#65289;&#36923;&#36753;&#30340;&#33258;&#26816;&#25216;&#26415;&#65292;&#30446;&#30340;&#26159;&#30830;&#20445;&#20195;&#29702;&#30340;&#21487;&#20449;&#21644;&#36947;&#24503;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous Intelligent Agents are employed in many applications upon which the life and welfare of living beings and vital social functions may depend. Therefore, agents should be trustworthy. A priori certification techniques (i.e., techniques applied prior to system's deployment) can be useful, but are not sufficient for agents that evolve, and thus modify their epistemic and belief state, and for open Multi-Agent Systems, where heterogeneous agents can join or leave the system at any stage of its operation. In this paper, we propose/refine/extend dynamic (runtime) logic-based self-checking techniques, devised in order to be able to ensure agents' trustworthy and ethical behaviour.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#20351;&#29992;&#35299;&#37322;&#26469;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#26174;&#33879;&#22909;&#22788;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#36739;&#23567;&#30340;&#27169;&#22411;&#65292;&#35299;&#37322;&#30340;&#21152;&#20837;&#20351;&#27169;&#22411;&#33021;&#22815;&#35299;&#20915;&#20043;&#21069;&#26080;&#27861;&#35299;&#20915;&#30340;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.07543</link><description>&lt;p&gt;
&#32473;&#25105;&#30475;&#24590;&#20040;&#20570;&#65306;&#35299;&#37322;&#22312;&#32454;&#35843;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Show Me How It's Done: The Role of Explanations in Fine-Tuning Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#20351;&#29992;&#35299;&#37322;&#26469;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#26174;&#33879;&#22909;&#22788;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#36739;&#23567;&#30340;&#27169;&#22411;&#65292;&#35299;&#37322;&#30340;&#21152;&#20837;&#20351;&#27169;&#22411;&#33021;&#22815;&#35299;&#20915;&#20043;&#21069;&#26080;&#27861;&#35299;&#20915;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#35777;&#26126;&#20102;&#20351;&#29992;&#35299;&#37322;&#26469;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#26174;&#33879;&#22909;&#22788;&#12290;&#19982;&#25552;&#31034;&#26041;&#24335;&#19981;&#21516;&#65292;&#32454;&#35843;&#20801;&#35768;&#27169;&#22411;&#22312;&#35757;&#32451;&#38454;&#27573;&#23398;&#20064;&#21644;&#26356;&#26032;&#21442;&#25968;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24212;&#29992;&#32454;&#35843;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21253;&#21547;&#36755;&#20986;&#35299;&#37322;&#32780;&#38750;&#20165;&#21576;&#29616;&#31572;&#26696;&#30340;&#25968;&#25454;&#26469;&#23545;&#19981;&#21516;&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#26159;&#21482;&#26377;6000&#19975;&#21442;&#25968;&#30340;&#36739;&#23567;&#35821;&#35328;&#27169;&#22411;&#20063;&#33021;&#20174;&#36825;&#31181;&#26041;&#27861;&#20013;&#33719;&#30410;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#35814;&#32454;&#30340;&#35299;&#37322;&#23545;&#36739;&#23567;&#30340;&#27169;&#22411;&#26356;&#26377;&#30410;&#22788;&#65292;&#32780;&#23545;&#20110;&#36739;&#22823;&#30340;&#27169;&#22411;&#26469;&#35828;&#65292;&#26080;&#35770;&#35299;&#37322;&#30340;&#38271;&#24230;&#22914;&#20309;&#65292;&#37117;&#21487;&#20197;&#33719;&#24471;&#20960;&#20046;&#30456;&#21516;&#30340;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#35299;&#37322;&#30340;&#21152;&#20837;&#20351;&#27169;&#22411;&#33021;&#22815;&#35299;&#20915;&#20043;&#21069;&#26080;&#27861;&#35299;&#20915;&#30340;&#20219;&#21153;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35748;&#20026;&#23613;&#31649;&#23384;&#22312;&#25361;&#25112;&#65292;&#20294;&#35299;&#37322;&#22312;&#32454;&#35843;&#35821;&#35328;&#27169;&#22411;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our research demonstrates the significant benefits of using fine-tuning with explanations to enhance the performance of language models. Unlike prompting, which maintains the model's parameters, fine-tuning allows the model to learn and update its parameters during a training phase. In this study, we applied fine-tuning to various sized language models using data that contained explanations of the output rather than merely presenting the answers. We found that even smaller language models with as few as 60 million parameters benefited substantially from this approach. Interestingly, our results indicated that the detailed explanations were more beneficial to smaller models than larger ones, with the latter gaining nearly the same advantage from any form of explanation, irrespective of its length. Additionally, we demonstrate that the inclusion of explanations enables the models to solve tasks that they were not able to solve without explanations. Lastly, we argue that despite the chall
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#20010;&#20154;&#30693;&#35782;&#22270;&#65288;PKG&#65289;&#31649;&#29702;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#29992;&#25143;&#30028;&#38754;&#21451;&#22909;&#30340;PKG&#23458;&#25143;&#31471;&#21644;&#38754;&#21521;&#26381;&#21153;&#30340;PKG API&#65292;&#20197;&#21450;&#22522;&#20110;RDF&#30340;PKG&#35789;&#27719;&#34920;&#29992;&#20110;&#34920;&#31034;&#38472;&#36848;&#21644;&#35775;&#38382;&#26435;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.07540</link><description>&lt;p&gt;
PKG API&#65306;&#19968;&#20010;&#20010;&#20154;&#30693;&#35782;&#22270;&#31649;&#29702;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
PKG API: A Tool for Personal Knowledge Graph Management
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#20010;&#20154;&#30693;&#35782;&#22270;&#65288;PKG&#65289;&#31649;&#29702;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#29992;&#25143;&#30028;&#38754;&#21451;&#22909;&#30340;PKG&#23458;&#25143;&#31471;&#21644;&#38754;&#21521;&#26381;&#21153;&#30340;PKG API&#65292;&#20197;&#21450;&#22522;&#20110;RDF&#30340;PKG&#35789;&#27719;&#34920;&#29992;&#20110;&#34920;&#31034;&#38472;&#36848;&#21644;&#35775;&#38382;&#26435;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#20154;&#30693;&#35782;&#22270;&#65288;PKG&#65289;&#20026;&#20010;&#20154;&#25552;&#20379;&#20102;&#19968;&#31181;&#23558;&#30862;&#29255;&#21270;&#30340;&#20010;&#20154;&#25968;&#25454;&#23384;&#20648;&#21644;&#25972;&#21512;&#21040;&#19968;&#20010;&#20013;&#24515;&#20301;&#32622;&#30340;&#26041;&#24335;&#65292;&#25552;&#39640;&#20102;&#26381;&#21153;&#30340;&#20010;&#24615;&#21270;&#31243;&#24230;&#21516;&#26102;&#20445;&#25345;&#29992;&#25143;&#30340;&#23436;&#20840;&#25511;&#21046;&#12290;&#23613;&#31649;PKG&#30340;&#28508;&#21147;&#24040;&#22823;&#65292;&#20294;&#23454;&#38469;&#25805;&#20316;&#19978;&#20855;&#26377;&#29992;&#25143;&#21451;&#22909;&#30028;&#38754;&#30340;PKG&#23454;&#29616;&#20173;&#28982;&#24456;&#23569;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#23436;&#25972;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#34920;&#31034;&#12289;&#31649;&#29702;&#21644;&#19982;PKG&#36827;&#34892;&#20132;&#20114;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#65288;1&#65289;&#19968;&#20010;&#38754;&#21521;&#29992;&#25143;&#30340;PKG&#23458;&#25143;&#31471;&#65292;&#20351;&#26368;&#32456;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#38472;&#36848;&#36731;&#26494;&#31649;&#29702;&#20182;&#20204;&#30340;&#20010;&#20154;&#25968;&#25454;&#65292;&#20197;&#21450;&#65288;2&#65289;&#19968;&#20010;&#38754;&#21521;&#26381;&#21153;&#30340;PKG API&#12290;&#20026;&#20102;&#24212;&#23545;&#22312;PKG&#20013;&#34920;&#31034;&#36825;&#20123;&#38472;&#36848;&#30340;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;RDF&#30340;PKG&#35789;&#27719;&#34920;&#26469;&#25903;&#25345;&#36825;&#19968;&#28857;&#65292;&#24182;&#25552;&#20379;&#20102;&#35775;&#38382;&#26435;&#38480;&#21644;&#26469;&#28304;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personal knowledge graphs (PKGs) offer individuals a way to store and consolidate their fragmented personal data in a central place, improving service personalization while maintaining full user control. Despite their potential, practical PKG implementations with user-friendly interfaces remain scarce. This work addresses this gap by proposing a complete solution to represent, manage, and interface with PKGs. Our approach includes (1) a user-facing PKG Client, enabling end-users to administer their personal data easily via natural language statements, and (2) a service-oriented PKG API. To tackle the complexity of representing these statements within a PKG, we present an RDF-based PKG vocabulary that supports this, along with properties for access rights and provenance.
&lt;/p&gt;</description></item><item><title>BreakGPT&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#37329;&#34701;&#31361;&#30772;&#26816;&#27979;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#37319;&#29992;&#22810;&#38454;&#27573;&#32467;&#26500;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#31572;&#26696;&#21644;&#29702;&#30001;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07536</link><description>&lt;p&gt;
BreakGPT: &#19968;&#31181;&#20855;&#26377;&#22810;&#38454;&#27573;&#32467;&#26500;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#37329;&#34701;&#31361;&#30772;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
BreakGPT: A Large Language Model with Multi-stage Structure for Financial Breakout Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07536
&lt;/p&gt;
&lt;p&gt;
BreakGPT&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#37329;&#34701;&#31361;&#30772;&#26816;&#27979;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#37319;&#29992;&#22810;&#38454;&#27573;&#32467;&#26500;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#31572;&#26696;&#21644;&#29702;&#30001;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#26131;&#21306;&#38388;&#31361;&#30772;&#65288;TRB&#65289;&#26159;&#37329;&#34701;&#20132;&#26131;&#25216;&#26415;&#20998;&#26512;&#20013;&#30340;&#19968;&#31181;&#20851;&#38190;&#26041;&#27861;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#32929;&#31080;&#12289;&#26399;&#36135;&#21644;&#22806;&#27719;&#31561;&#37329;&#34701;&#24066;&#22330;&#30340;&#20132;&#26131;&#32773;&#12290;&#28982;&#32780;&#65292;&#21306;&#20998;&#30495;&#20551;&#31361;&#30772;&#24182;&#25552;&#20379;&#27491;&#30830;&#30340;&#29702;&#30001;&#23545;&#25237;&#36164;&#32773;&#26469;&#35828;&#20855;&#26377;&#37325;&#35201;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#19979;&#28216;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22312;&#37329;&#34701;&#31361;&#30772;&#26816;&#27979;&#39046;&#22495;&#30340;&#25928;&#26524;&#20173;&#19981;&#29702;&#24819;&#12290;&#21407;&#22240;&#22312;&#20110;&#31361;&#30772;&#26816;&#27979;&#38656;&#35201;&#29420;&#29305;&#30340;&#25968;&#25454;&#21644;&#29305;&#23450;&#30340;&#30693;&#35782;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;BreakGPT&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#37329;&#34701;&#31361;&#30772;&#26816;&#27979;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#38454;&#27573;&#32467;&#26500;&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#19979;&#28216;&#24212;&#29992;&#20013;&#30340;&#38169;&#35823;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;GPT-3.5&#30456;&#27604;&#65292;BreakGPT&#30340;&#31572;&#26696;&#21644;&#29702;&#30001;&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;44%&#65292;&#26377;&#21161;&#20110;&#37329;&#34701;&#31361;&#30772;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trading range breakout (TRB) is a key method in the technical analysis of financial trading, widely employed by traders in financial markets such as stocks, futures, and foreign exchange. However, distinguishing between true and false breakout and providing the correct rationale cause significant challenges to investors. Recently, large language models have achieved success in various downstream applications, but their effectiveness in the domain of financial breakout detection has been subpar. The reason is that the unique data and specific knowledge are required in breakout detection. To address these issues, we introduce BreakGPT, the first large language model for financial breakout detection. Furthermore, we have developed a novel framework for large language models, namely multi-stage structure, effectively reducing mistakes in downstream applications. Experimental results indicate that compared to GPT-3.5, BreakGPT improves the accuracy of answers and rational by 44%, with the m
&lt;/p&gt;</description></item><item><title>&#29289;&#29702;&#32422;&#26463;&#30340;&#26426;&#22120;&#23398;&#20064;&#32467;&#21512;&#20102;&#25968;&#25454;&#26041;&#27861;&#30340;&#34920;&#36798;&#33021;&#21147;&#19982;&#29289;&#29702;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#21487;&#20197;&#29992;&#20110;&#27491;&#21017;&#21270;&#32463;&#39564;&#39118;&#38505;&#24182;&#25552;&#39640;&#20272;&#35745;&#22120;&#30340;&#32479;&#35745;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.07514</link><description>&lt;p&gt;
&#29289;&#29702;&#32422;&#26463;&#30340;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#26680;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Physics-informed machine learning as a kernel method
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07514
&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#32422;&#26463;&#30340;&#26426;&#22120;&#23398;&#20064;&#32467;&#21512;&#20102;&#25968;&#25454;&#26041;&#27861;&#30340;&#34920;&#36798;&#33021;&#21147;&#19982;&#29289;&#29702;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#21487;&#20197;&#29992;&#20110;&#27491;&#21017;&#21270;&#32463;&#39564;&#39118;&#38505;&#24182;&#25552;&#39640;&#20272;&#35745;&#22120;&#30340;&#32479;&#35745;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#32422;&#26463;&#30340;&#26426;&#22120;&#23398;&#20064;&#23558;&#22522;&#20110;&#25968;&#25454;&#30340;&#26041;&#27861;&#30340;&#34920;&#36798;&#33021;&#21147;&#19982;&#29289;&#29702;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#30456;&#32467;&#21512;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#26222;&#36890;&#30340;&#22238;&#24402;&#38382;&#39064;&#65292;&#20854;&#20013;&#32463;&#39564;&#39118;&#38505;&#30001;&#19968;&#20010;&#20559;&#24494;&#20998;&#26041;&#31243;&#27491;&#21017;&#21270;&#65292;&#35813;&#26041;&#31243;&#37327;&#21270;&#20102;&#29289;&#29702;&#19981;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#23545;&#20110;&#32447;&#24615;&#24494;&#20998;&#20808;&#39564;&#65292;&#35813;&#38382;&#39064;&#21487;&#20197;&#34987;&#34920;&#36848;&#20026;&#26680;&#22238;&#24402;&#20219;&#21153;&#12290;&#21033;&#29992;&#26680;&#29702;&#35770;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#27491;&#21017;&#21270;&#39118;&#38505;&#30340;&#26368;&#23567;&#21270;&#22120;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#34920;&#26126;&#23427;&#33267;&#23569;&#20197;Sobolev&#26368;&#23567;&#21270;&#36895;&#24230;&#25910;&#25947;&#12290;&#28982;&#32780;&#65292;&#26681;&#25454;&#29289;&#29702;&#35823;&#24046;&#30340;&#19981;&#21516;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#36890;&#36807;&#19968;&#20010;&#19968;&#32500;&#31034;&#20363;&#26469;&#35828;&#26126;&#36825;&#20010;&#21407;&#29702;&#65292;&#25903;&#25345;&#19968;&#20010;&#35770;&#28857;&#65306;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#26469;&#27491;&#21017;&#21270;&#32463;&#39564;&#39118;&#38505;&#23545;&#20272;&#35745;&#22120;&#30340;&#32479;&#35745;&#24615;&#33021;&#26377;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-informed machine learning combines the expressiveness of data-based approaches with the interpretability of physical models. In this context, we consider a general regression problem where the empirical risk is regularized by a partial differential equation that quantifies the physical inconsistency. We prove that for linear differential priors, the problem can be formulated as a kernel regression task. Taking advantage of kernel theory, we derive convergence rates for the minimizer of the regularized risk and show that it converges at least at the Sobolev minimax rate. However, faster rates can be achieved, depending on the physical error. This principle is illustrated with a one-dimensional example, supporting the claim that regularizing the empirical risk with physical information can be beneficial to the statistical performance of estimators.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;Whisper&#21644;MMS&#31995;&#32479;&#36827;&#34892;&#20840;&#38754;&#25506;&#32034;&#65292;&#35780;&#20272;&#20102;&#33889;&#33796;&#29273;&#35821;&#20013;&#38750;&#27491;&#24335;&#23545;&#35805;&#35821;&#38899;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#20559;&#35265;&#65292;&#24182;&#21457;&#29616;&#37319;&#29992;&#36807;&#37319;&#26679;&#25216;&#26415;&#21487;&#20197;&#32531;&#35299;&#36825;&#31181;&#38472;&#35268;&#23450;&#22411;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2402.07513</link><description>&lt;p&gt;
&#24179;&#34913;&#30340;&#33402;&#26415;&#65306;&#25581;&#31034;&#21644;&#32531;&#35299;&#33889;&#33796;&#29273;&#35821;&#20013;&#30340;ASR&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
The Balancing Act: Unmasking and Alleviating ASR Biases in Portuguese
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;Whisper&#21644;MMS&#31995;&#32479;&#36827;&#34892;&#20840;&#38754;&#25506;&#32034;&#65292;&#35780;&#20272;&#20102;&#33889;&#33796;&#29273;&#35821;&#20013;&#38750;&#27491;&#24335;&#23545;&#35805;&#35821;&#38899;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#20559;&#35265;&#65292;&#24182;&#21457;&#29616;&#37319;&#29992;&#36807;&#37319;&#26679;&#25216;&#26415;&#21487;&#20197;&#32531;&#35299;&#36825;&#31181;&#38472;&#35268;&#23450;&#22411;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21475;&#35821;&#29702;&#35299;&#39046;&#22495;&#65292;&#20687;Whisper&#21644;Multilingual Massive Speech&#65288;MMS&#65289;&#36825;&#26679;&#30340;&#31995;&#32479;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#23545;Whisper&#21644;MMS&#31995;&#32479;&#36827;&#34892;&#20840;&#38754;&#25506;&#32034;&#65292;&#37325;&#28857;&#35780;&#20272;&#19982;&#33889;&#33796;&#29273;&#35821;&#29305;&#23450;&#30340;&#38750;&#27491;&#24335;&#23545;&#35805;&#35821;&#38899;&#20013;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#28085;&#30422;&#20102;&#21508;&#31181;&#31867;&#21035;&#65292;&#21253;&#25324;&#24615;&#21035;&#12289;&#24180;&#40836;&#12289;&#32932;&#33394;&#21644;&#22320;&#29702;&#20301;&#32622;&#12290;&#38500;&#20102;&#20256;&#32479;&#30340;ASR&#35780;&#20272;&#25351;&#26631;&#65292;&#22914;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;p&#20540;&#32479;&#35745;&#26174;&#33879;&#24615;&#26469;&#20998;&#26512;&#24615;&#21035;&#20559;&#35265;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24191;&#27867;&#30740;&#31350;&#20102;&#25968;&#25454;&#20998;&#24067;&#30340;&#24433;&#21709;&#65292;&#24182;&#20174;&#23454;&#35777;&#35282;&#24230;&#34920;&#26126;&#36807;&#37319;&#26679;&#25216;&#26415;&#32531;&#35299;&#20102;&#36825;&#31181;&#38472;&#35268;&#23450;&#22411;&#20559;&#35265;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;MMS&#21644;Whisper&#30340;&#24212;&#29992;&#65292;&#22312;&#33889;&#33796;&#29273;&#35821;&#29615;&#22659;&#20013;&#37327;&#21270;&#20559;&#35265;&#26041;&#38754;&#20570;&#20986;&#20102;&#24320;&#21019;&#24615;&#30340;&#21162;&#21147;&#65292;&#20026;&#26356;&#22909;&#22320;&#29702;&#35299;ASR&#31995;&#32479;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of spoken language understanding, systems like Whisper and Multilingual Massive Speech (MMS) have shown state-of-the-art performances. This study is dedicated to a comprehensive exploration of the Whisper and MMS systems, with a focus on assessing biases in automatic speech recognition (ASR) inherent to casual conversation speech specific to the Portuguese language. Our investigation encompasses various categories, including gender, age, skin tone color, and geo-location. Alongside traditional ASR evaluation metrics such as Word Error Rate (WER), we have incorporated p-value statistical significance for gender bias analysis. Furthermore, we extensively examine the impact of data distribution and empirically show that oversampling techniques alleviate such stereotypical biases. This research represents a pioneering effort in quantifying biases in the Portuguese language context through the application of MMS and Whisper, contributing to a better understanding of ASR systems
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27719;&#38598;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#23433;&#20840;&#39046;&#22495;&#30340;&#30456;&#20851;&#27010;&#24565;&#65292;&#31995;&#32479;&#22320;&#24418;&#24335;&#21270;&#20102;&#29983;&#25104;&#24335;AI&#20195;&#29702;&#31995;&#32479;&#20013;&#30340;&#31192;&#23494;&#21246;&#32467;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#32531;&#35299;&#25514;&#26045;&#12290;&#36890;&#36807;&#27979;&#35797;&#21508;&#31181;&#24418;&#24335;&#30340;&#31192;&#23494;&#21246;&#32467;&#25152;&#38656;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#27169;&#22411;&#30340;&#38544;&#20889;&#33021;&#21147;&#26377;&#38480;&#65292;&#20294; GPT-4 &#23637;&#31034;&#20102;&#33021;&#21147;&#30340;&#39134;&#36291;&#12290;</title><link>https://arxiv.org/abs/2402.07510</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;AI&#20195;&#29702;&#20043;&#38388;&#30340;&#31192;&#23494;&#21246;&#32467;
&lt;/p&gt;
&lt;p&gt;
Secret Collusion Among Generative AI Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27719;&#38598;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#23433;&#20840;&#39046;&#22495;&#30340;&#30456;&#20851;&#27010;&#24565;&#65292;&#31995;&#32479;&#22320;&#24418;&#24335;&#21270;&#20102;&#29983;&#25104;&#24335;AI&#20195;&#29702;&#31995;&#32479;&#20013;&#30340;&#31192;&#23494;&#21246;&#32467;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#32531;&#35299;&#25514;&#26045;&#12290;&#36890;&#36807;&#27979;&#35797;&#21508;&#31181;&#24418;&#24335;&#30340;&#31192;&#23494;&#21246;&#32467;&#25152;&#38656;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#27169;&#22411;&#30340;&#38544;&#20889;&#33021;&#21147;&#26377;&#38480;&#65292;&#20294; GPT-4 &#23637;&#31034;&#20102;&#33021;&#21147;&#30340;&#39134;&#36291;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33021;&#21147;&#19978;&#30340;&#22686;&#24378;&#20026;&#36890;&#20449;&#30340;&#29983;&#25104;&#24335;AI&#20195;&#29702;&#22242;&#38431;&#35299;&#20915;&#32852;&#21512;&#20219;&#21153;&#30340;&#24212;&#29992;&#25171;&#24320;&#20102;&#21487;&#33021;&#24615;&#12290;&#36825;&#24341;&#21457;&#20102;&#20851;&#20110;&#26410;&#32463;&#25480;&#26435;&#20998;&#20139;&#20449;&#24687;&#25110;&#20854;&#20182;&#19981;&#24517;&#35201;&#30340;&#20195;&#29702;&#21327;&#35843;&#24418;&#24335;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#25361;&#25112;&#12290;&#29616;&#20195;&#38544;&#20889;&#26415;&#25216;&#26415;&#21487;&#33021;&#20351;&#36825;&#31181;&#21160;&#24577;&#38590;&#20197;&#26816;&#27979;&#12290;&#26412;&#25991;&#36890;&#36807;&#27762;&#21462;&#20154;&#24037;&#26234;&#33021;&#21644;&#23433;&#20840;&#39046;&#22495;&#30456;&#20851;&#27010;&#24565;&#65292;&#20840;&#38754;&#31995;&#32479;&#22320;&#24418;&#24335;&#21270;&#20102;&#29983;&#25104;&#24335;AI&#20195;&#29702;&#31995;&#32479;&#20013;&#30340;&#31192;&#23494;&#21246;&#32467;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#38544;&#20889;&#26415;&#30340;&#21160;&#26426;&#65292;&#24182;&#25552;&#20986;&#20102;&#21508;&#31181;&#32531;&#35299;&#25514;&#26045;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26159;&#19968;&#20010;&#27169;&#22411;&#35780;&#20272;&#26694;&#26550;&#65292;&#31995;&#32479;&#22320;&#27979;&#35797;&#20102;&#21508;&#31181;&#24418;&#24335;&#30340;&#31192;&#23494;&#21246;&#32467;&#25152;&#38656;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#24403;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;&#34429;&#28982;&#24403;&#21069;&#27169;&#22411;&#30340;&#38544;&#20889;&#33021;&#21147;&#20173;&#28982;&#26377;&#38480;&#65292;&#20294; GPT-4 &#26174;&#31034;&#20986;&#33021;&#21147;&#30340;&#39134;&#36291;&#65292;&#36825;&#34920;&#26126;&#26377;&#24517;&#35201;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent capability increases in large language models (LLMs) open up applications in which teams of communicating generative AI agents solve joint tasks. This poses privacy and security challenges concerning the unauthorised sharing of information, or other unwanted forms of agent coordination. Modern steganographic techniques could render such dynamics hard to detect. In this paper, we comprehensively formalise the problem of secret collusion in systems of generative AI agents by drawing on relevant concepts from both the AI and security literature. We study incentives for the use of steganography, and propose a variety of mitigation measures. Our investigations result in a model evaluation framework that systematically tests capabilities required for various forms of secret collusion. We provide extensive empirical results across a range of contemporary LLMs. While the steganographic capabilities of current models remain limited, GPT-4 displays a capability jump suggesting the need fo
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#31232;&#30095;&#30340;GPS&#25968;&#25454;&#21644;&#22320;&#24418;&#29305;&#24449;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#21160;&#21147;&#23398;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#36895;&#24230;&#39044;&#27979;&#65292;&#20197;&#35299;&#20915;&#20174;&#25968;&#25454;&#31232;&#32570;&#30340;&#22320;&#29702;&#21306;&#22495;&#25552;&#21462;&#20934;&#30830;&#20132;&#36890;&#20449;&#24687;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.07507</link><description>&lt;p&gt;
&#22522;&#20110;&#22320;&#24418;GPS&#27880;&#20876;&#30340;&#32858;&#31867;&#21160;&#21147;&#23398;&#29992;&#20110;&#25552;&#39640;&#36895;&#24230;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Clustering Dynamics for Improved Speed Prediction Deriving from Topographical GPS Registrations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07507
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#31232;&#30095;&#30340;GPS&#25968;&#25454;&#21644;&#22320;&#24418;&#29305;&#24449;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#21160;&#21147;&#23398;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#36895;&#24230;&#39044;&#27979;&#65292;&#20197;&#35299;&#20915;&#20174;&#25968;&#25454;&#31232;&#32570;&#30340;&#22320;&#29702;&#21306;&#22495;&#25552;&#21462;&#20934;&#30830;&#20132;&#36890;&#20449;&#24687;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#39046;&#22495;&#20013;&#65292;&#20174;&#25968;&#25454;&#31232;&#32570;&#25110;&#27809;&#26377;&#25968;&#25454;&#35206;&#30422;&#30340;&#22320;&#29702;&#21306;&#22495;&#20013;&#25552;&#21462;&#20934;&#30830;&#30340;&#20132;&#36890;&#20449;&#24687;&#19968;&#30452;&#26159;&#19968;&#20010;&#25345;&#20037;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#31232;&#30095;&#30340;GPS&#25968;&#25454;&#28857;&#21450;&#20854;&#30456;&#20851;&#30340;&#22320;&#24418;&#21644;&#36947;&#36335;&#35774;&#35745;&#29305;&#24449;&#36827;&#34892;&#36895;&#24230;&#39044;&#27979;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#30740;&#31350;&#26159;&#21542;&#21487;&#20197;&#21033;&#29992;&#22320;&#24418;&#21644;&#22522;&#30784;&#35774;&#26045;&#30340;&#30456;&#20284;&#24615;&#26469;&#35757;&#32451;&#19968;&#20010;&#21487;&#20197;&#39044;&#27979;&#32570;&#20047;&#20132;&#36890;&#25968;&#25454;&#21306;&#22495;&#36895;&#24230;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#20197;&#22320;&#24418;&#32858;&#31867;&#36947;&#36335;&#20026;&#20013;&#24515;&#30340;&#26102;&#38388;&#23548;&#21521;&#36895;&#24230;&#23383;&#20856;&#65292;&#23427;&#24110;&#21161;&#25105;&#20204;&#25552;&#20379;&#36873;&#23450;&#29305;&#24449;&#37197;&#32622;&#30340;&#36895;&#24230;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#20986;&#23545;&#26032;&#30340;&#21644;&#26631;&#20934;&#22238;&#24402;&#26041;&#27861;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#25913;&#36827;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#20026;&#32570;&#22833;&#25968;&#25454;&#20132;&#36890;&#20998;&#26512;&#30340;&#21046;&#23450;&#31574;&#30053;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
A persistent challenge in the field of Intelligent Transportation Systems is to extract accurate traffic insights from geographic regions with scarce or no data coverage. To this end, we propose solutions for speed prediction using sparse GPS data points and their associated topographical and road design features. Our goal is to investigate whether we can use similarities in the terrain and infrastructure to train a machine learning model that can predict speed in regions where we lack transportation data. For this we create a Temporally Orientated Speed Dictionary Centered on Topographically Clustered Roads, which helps us to provide speed correlations to selected feature configurations. Our results show qualitative and quantitative improvement over new and standard regression methods. The presented framework provides a fresh perspective on devising strategies for missing data traffic analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#26694;&#26550;&#65292;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#22686;&#24378;&#25968;&#25454;&#21253;&#32423;&#21035;&#21644;&#27969;&#32423;&#21035;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#22270;&#25968;&#25454;&#22686;&#24378;&#25429;&#33719;&#23383;&#33410;&#32423;&#27969;&#37327;&#22270;&#30340;&#32454;&#31890;&#24230;&#35821;&#20041;&#19981;&#21464;&#29305;&#24449;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#36328;&#32423;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#25968;&#25454;&#21253;&#32423;&#21035;&#20219;&#21153;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#33021;&#22815;&#34987;&#27969;&#32423;&#21035;&#20219;&#21153;&#21033;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.07501</link><description>&lt;p&gt;
&#19968;&#21015;&#28779;&#36710;&#23436;&#25104;&#20004;&#20010;&#20219;&#21153;&#65306;&#20351;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
One Train for Two Tasks: An Encrypted Traffic Classification Framework Using Supervised Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07501
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#26694;&#26550;&#65292;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#22686;&#24378;&#25968;&#25454;&#21253;&#32423;&#21035;&#21644;&#27969;&#32423;&#21035;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#22270;&#25968;&#25454;&#22686;&#24378;&#25429;&#33719;&#23383;&#33410;&#32423;&#27969;&#37327;&#22270;&#30340;&#32454;&#31890;&#24230;&#35821;&#20041;&#19981;&#21464;&#29305;&#24449;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#36328;&#32423;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#25968;&#25454;&#21253;&#32423;&#21035;&#20219;&#21153;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#33021;&#22815;&#34987;&#27969;&#32423;&#21035;&#20219;&#21153;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#32593;&#32476;&#23433;&#20840;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#65292;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#24050;&#25104;&#20026;&#24403;&#21069;&#30340;&#30740;&#31350;&#37325;&#28857;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#36827;&#34892;&#27969;&#37327;&#20998;&#31867;&#26102;&#27809;&#26377;&#20805;&#20998;&#32771;&#34385;&#25968;&#25454;&#26679;&#26412;&#20043;&#38388;&#30340;&#20849;&#21516;&#29305;&#24449;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#29420;&#31435;&#22320;&#35757;&#32451;&#25968;&#25454;&#21253;&#32423;&#21035;&#21644;&#27969;&#32423;&#21035;&#30340;&#20998;&#31867;&#20219;&#21153;&#65292;&#36825;&#26159;&#20887;&#20313;&#30340;&#65292;&#22240;&#20026;&#25968;&#25454;&#21253;&#32423;&#21035;&#20219;&#21153;&#20013;&#23398;&#20064;&#21040;&#30340;&#25968;&#25454;&#21253;&#34920;&#31034;&#21487;&#20197;&#34987;&#27969;&#32423;&#21035;&#20219;&#21153;&#21033;&#29992;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#27169;&#22411;&#65292;&#21629;&#21517;&#20026;&#23545;&#27604;&#23398;&#20064;&#22686;&#24378;&#30340;&#26102;&#22495;&#34701;&#21512;&#32534;&#30721;&#22120;&#65288;CLE-TFE&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#22686;&#24378;&#25968;&#25454;&#21253;&#32423;&#21035;&#21644;&#27969;&#32423;&#21035;&#34920;&#31034;&#65292;&#24182;&#22312;&#23383;&#33410;&#32423;&#27969;&#37327;&#22270;&#19978;&#36827;&#34892;&#22270;&#25968;&#25454;&#22686;&#24378;&#65292;&#20197;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#25429;&#33719;&#23383;&#33410;&#20043;&#38388;&#30340;&#32454;&#31890;&#24230;&#35821;&#20041;&#19981;&#21464;&#29305;&#24449;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#36328;&#32423;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
As network security receives widespread attention, encrypted traffic classification has become the current research focus. However, existing methods conduct traffic classification without sufficiently considering the common characteristics between data samples, leading to suboptimal performance. Moreover, they train the packet-level and flow-level classification tasks independently, which is redundant because the packet representations learned in the packet-level task can be exploited by the flow-level task. Therefore, in this paper, we propose an effective model named a Contrastive Learning Enhanced Temporal Fusion Encoder (CLE-TFE). In particular, we utilize supervised contrastive learning to enhance the packet-level and flow-level representations and perform graph data augmentation on the byte-level traffic graph so that the fine-grained semantic-invariant characteristics between bytes can be captured through contrastive learning. We also propose cross-level multi-task learning, whi
&lt;/p&gt;</description></item><item><title>T-RAG&#26159;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#29992;&#20110;&#31169;&#20154;&#20225;&#19994;&#25991;&#20214;&#38382;&#31572;&#65292;&#23427;&#32467;&#21512;&#20102;RAG&#26694;&#26550;&#21644;&#32463;&#36807;&#24494;&#35843;&#30340;&#24320;&#28304;LLM&#65292;&#24182;&#20998;&#20139;&#20102;&#26500;&#24314;&#21644;&#37096;&#32626;&#36807;&#31243;&#20013;&#30340;&#32463;&#39564;&#25945;&#35757;&#12290;</title><link>https://arxiv.org/abs/2402.07483</link><description>&lt;p&gt;
T-RAG: &#26469;&#33258;LLM&#25112;&#22330;&#30340;&#32463;&#39564;&#25945;&#35757;
&lt;/p&gt;
&lt;p&gt;
T-RAG: Lessons from the LLM Trenches
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07483
&lt;/p&gt;
&lt;p&gt;
T-RAG&#26159;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#29992;&#20110;&#31169;&#20154;&#20225;&#19994;&#25991;&#20214;&#38382;&#31572;&#65292;&#23427;&#32467;&#21512;&#20102;RAG&#26694;&#26550;&#21644;&#32463;&#36807;&#24494;&#35843;&#30340;&#24320;&#28304;LLM&#65292;&#24182;&#20998;&#20139;&#20102;&#26500;&#24314;&#21644;&#37096;&#32626;&#36807;&#31243;&#20013;&#30340;&#32463;&#39564;&#25945;&#35757;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#24778;&#20154;&#30340;&#35821;&#35328;&#33021;&#21147;&#65292;&#25512;&#21160;&#20102;&#23558;&#23427;&#20204;&#25972;&#21512;&#21040;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#30340;&#23581;&#35797;&#12290;&#19968;&#20010;&#37325;&#35201;&#30340;&#24212;&#29992;&#39046;&#22495;&#26159;&#23545;&#31169;&#20154;&#20225;&#19994;&#25991;&#20214;&#36827;&#34892;&#38382;&#31572;&#65292;&#20854;&#20013;&#20027;&#35201;&#32771;&#34385;&#22240;&#32032;&#26159;&#25968;&#25454;&#23433;&#20840;&#65292;&#38656;&#35201;&#33021;&#22815;&#22312;&#26412;&#22320;&#37096;&#32626;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#23545;&#26597;&#35810;&#27491;&#30830;&#21709;&#24212;&#30340;&#20581;&#22766;&#24212;&#29992;&#30340;&#38656;&#27714;&#12290;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#24050;&#25104;&#20026;&#26500;&#24314;&#22522;&#20110;LLM&#30340;&#24212;&#29992;&#31243;&#24207;&#30340;&#26368;&#37325;&#35201;&#30340;&#26694;&#26550;&#12290;&#34429;&#28982;&#26500;&#24314;RAG&#30456;&#23545;&#31616;&#21333;&#65292;&#20294;&#35201;&#20351;&#20854;&#20581;&#22766;&#21644;&#21487;&#38752;&#30340;&#24212;&#29992;&#31243;&#24207;&#38656;&#35201;&#24191;&#27867;&#30340;&#23450;&#21046;&#21270;&#21644;&#30456;&#23545;&#28145;&#20837;&#30340;&#24212;&#29992;&#39046;&#22495;&#30693;&#35782;&#12290;&#25105;&#20204;&#20998;&#20139;&#20102;&#26500;&#24314;&#21644;&#37096;&#32626;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#31169;&#20154;&#32452;&#32455;&#25991;&#20214;&#38382;&#31572;&#24212;&#29992;&#30340;&#32463;&#39564;&#12290;&#25105;&#20204;&#30340;&#24212;&#29992;&#32467;&#21512;&#20102;RAG&#30340;&#20351;&#29992;&#21644;&#32463;&#36807;&#24494;&#35843;&#30340;&#24320;&#28304;LLM&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#36824;&#20855;&#26377; ...
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLM) have shown remarkable language capabilities fueling attempts to integrate them into applications across a wide range of domains. An important application area is question answering over private enterprise documents where the main considerations are data security, which necessitates applications that can be deployed on-prem, limited computational resources and the need for a robust application that correctly responds to queries. Retrieval-Augmented Generation (RAG) has emerged as the most prominent framework for building LLM-based applications. While building a RAG is relatively straightforward, making it robust and a reliable application requires extensive customization and relatively deep knowledge of the application domain. We share our experiences building and deploying an LLM application for question answering over private organizational documents. Our application combines the use of RAG with a finetuned open-source LLM. Additionally, our system, which w
&lt;/p&gt;</description></item><item><title>&#39135;&#29289;&#25512;&#33616;&#20316;&#20026;&#35821;&#35328;&#22788;&#29702;&#65288;F-RLP&#65289;&#26159;&#19968;&#20010;&#38024;&#23545;&#39135;&#29289;&#30340;&#20010;&#24615;&#21270;&#12289;&#24773;&#22659;&#21270;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#26469;&#23454;&#29616;&#26356;&#20934;&#30830;&#12289;&#20010;&#24615;&#21270;&#30340;&#39135;&#29289;&#25512;&#33616;&#12290;</title><link>https://arxiv.org/abs/2402.07477</link><description>&lt;p&gt;
&#39135;&#29289;&#25512;&#33616;&#20316;&#20026;&#35821;&#35328;&#22788;&#29702;&#65288;F-RLP&#65289;&#65306;&#20010;&#24615;&#21270;&#21644;&#24773;&#22659;&#21270;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
Food Recommendation as Language Processing (F-RLP): A Personalized and Contextual Paradigm
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07477
&lt;/p&gt;
&lt;p&gt;
&#39135;&#29289;&#25512;&#33616;&#20316;&#20026;&#35821;&#35328;&#22788;&#29702;&#65288;F-RLP&#65289;&#26159;&#19968;&#20010;&#38024;&#23545;&#39135;&#29289;&#30340;&#20010;&#24615;&#21270;&#12289;&#24773;&#22659;&#21270;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#26469;&#23454;&#29616;&#26356;&#20934;&#30830;&#12289;&#20010;&#24615;&#21270;&#30340;&#39135;&#29289;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#35268;&#21017;&#21644;&#20998;&#31867;&#30340;&#39135;&#29289;&#25512;&#33616;&#31995;&#32479;&#22312;&#23454;&#38469;&#24212;&#29992;&#21644;&#23454;&#29992;&#24615;&#26041;&#38754;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#20027;&#35201;&#21407;&#22240;&#26159;&#22823;&#22810;&#25968;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#19968;&#20010;&#20960;&#20046;&#26080;&#38480;&#25968;&#37327;&#30340;&#31867;&#21035;&#21644;&#19968;&#20010;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#30340;&#26377;&#38480;&#26679;&#26412;&#38382;&#39064;&#19978;&#24456;&#38590;&#24212;&#23545;&#12290;&#30456;&#21453;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#25512;&#33616;&#24341;&#25806;&#30340;&#20986;&#29616;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#36890;&#29992;&#30340;&#8220;&#20316;&#20026;&#35821;&#35328;&#22788;&#29702;&#30340;&#25512;&#33616;&#8221;&#65288;RLP&#65289;&#26041;&#27861;&#32570;&#20047;&#26377;&#25928;&#30340;&#39135;&#29289;&#25512;&#33616;&#25152;&#24517;&#38656;&#30340;&#20851;&#38190;&#32452;&#20214;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#39135;&#29289;&#25512;&#33616;&#20316;&#20026;&#35821;&#35328;&#22788;&#29702;&#65288;F-RLP&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#38024;&#23545;&#39135;&#29289;&#30340;&#23450;&#21046;&#22522;&#30784;&#35774;&#26045;&#12290;F-RLP&#21033;&#29992;LLMs&#30340;&#33021;&#21147;&#26469;&#26368;&#22823;&#21270;&#20854;&#28508;&#21147;&#65292;&#20174;&#32780;&#20026;&#26356;&#20934;&#30830;&#12289;&#20010;&#24615;&#21270;&#30340;&#39135;&#29289;&#25512;&#33616;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art rule-based and classification-based food recommendation systems face significant challenges in becoming practical and useful. This difficulty arises primarily because most machine learning models struggle with problems characterized by an almost infinite number of classes and a limited number of samples within an unbalanced dataset. Conversely, the emergence of Large Language Models (LLMs) as recommendation engines offers a promising avenue. However, a general-purpose Recommendation as Language Processing (RLP) approach lacks the critical components necessary for effective food recommendations. To address this gap, we introduce Food Recommendation as Language Processing (F-RLP), a novel framework that offers a food-specific, tailored infrastructure. F-RLP leverages the capabilities of LLMs to maximize their potential, thereby paving the way for more accurate, personalized food recommendations.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24471;&#20998;&#20989;&#25968;&#30340;&#27714;&#35299;&#22120;&#26469;&#35299;&#20915;&#39640;&#32500;&#31119;&#20811;-&#26222;&#26391;&#20811;&#26041;&#31243;&#20013;&#30340;&#32500;&#25968;&#28798;&#38590;&#38382;&#39064;&#12290;&#19982;&#33945;&#29305;&#21345;&#27931;&#21644;&#26222;&#36890;PINN&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#22788;&#29702;&#19982;&#24067;&#26391;&#36816;&#21160;&#30456;&#20851;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#65292;&#24182;&#25552;&#20379;&#24555;&#36895;&#37319;&#26679;&#12290;</title><link>https://arxiv.org/abs/2402.07465</link><description>&lt;p&gt;
&#22522;&#20110;&#24471;&#20998;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#39640;&#32500;&#31119;&#20811;-&#26222;&#26391;&#20811;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Score-Based Physics-Informed Neural Networks for High-Dimensional Fokker-Planck Equations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07465
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24471;&#20998;&#20989;&#25968;&#30340;&#27714;&#35299;&#22120;&#26469;&#35299;&#20915;&#39640;&#32500;&#31119;&#20811;-&#26222;&#26391;&#20811;&#26041;&#31243;&#20013;&#30340;&#32500;&#25968;&#28798;&#38590;&#38382;&#39064;&#12290;&#19982;&#33945;&#29305;&#21345;&#27931;&#21644;&#26222;&#36890;PINN&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#22788;&#29702;&#19982;&#24067;&#26391;&#36816;&#21160;&#30456;&#20851;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#65292;&#24182;&#25552;&#20379;&#24555;&#36895;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31119;&#20811;-&#26222;&#26391;&#20811;&#65288;FP&#65289;&#26041;&#31243;&#26159;&#38543;&#26426;&#36807;&#31243;&#20013;&#30340;&#22522;&#30784;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#12290;&#28982;&#32780;&#65292;&#24403;&#22788;&#29702;&#39640;&#32500;FP PDE&#26102;&#65292;&#32500;&#25968;&#28798;&#38590;&#65288;CoD&#65289;&#20250;&#24102;&#26469;&#25361;&#25112;&#12290;&#23613;&#31649;&#33945;&#29305;&#21345;&#27931;&#21644;&#26222;&#36890;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#24050;&#32463;&#26174;&#31034;&#20986;&#24212;&#23545;CoD&#30340;&#28508;&#21147;&#65292;&#20294;&#22312;&#22788;&#29702;&#19982;&#24067;&#26391;&#36816;&#21160;&#30456;&#20851;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#65288;PDF&#65289;&#26102;&#65292;&#20004;&#31181;&#26041;&#27861;&#37117;&#22312;&#39640;&#32500;&#24230;&#19978;&#26174;&#31034;&#20986;&#25968;&#20540;&#35823;&#24046;&#12290;&#28857;&#20540;PDF&#38543;&#30528;&#32500;&#24230;&#22686;&#21152;&#21576;&#25351;&#25968;&#32423;&#19979;&#38477;&#65292;&#36229;&#36807;&#20102;&#25968;&#20540;&#27169;&#25311;&#30340;&#31934;&#24230;&#65292;&#23548;&#33268;&#20102;&#30456;&#24403;&#22823;&#30340;&#35823;&#24046;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#20854;&#22823;&#35268;&#27169;&#37319;&#26679;&#65292;&#33945;&#29305;&#21345;&#27931;&#26080;&#27861;&#25552;&#20379;&#24555;&#36895;&#37319;&#26679;&#12290;&#36890;&#36807;&#23545;&#26222;&#36890;PINNs&#27169;&#25311;&#23545;&#25968;&#20284;&#28982;&#65288;LL&#65289;&#65292;&#23558;FP&#26041;&#31243;&#36716;&#21270;&#20026;&#19968;&#20010;&#22256;&#38590;&#30340;HJB&#26041;&#31243;&#65292;&#20854;&#35823;&#24046;&#38543;&#32500;&#25968;&#22686;&#38271;&#36805;&#36895;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#24471;&#20998;&#30340;&#27714;&#35299;&#22120;&#26469;&#25311;&#21512;SDE&#20013;&#30340;&#24471;&#20998;&#20989;&#25968;&#12290;&#24471;&#20998;&#20989;&#25968;&#23450;&#20041;&#20026;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#30340;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Fokker-Planck (FP) equation is a foundational PDE in stochastic processes. However, curse of dimensionality (CoD) poses challenge when dealing with high-dimensional FP PDEs. Although Monte Carlo and vanilla Physics-Informed Neural Networks (PINNs) have shown the potential to tackle CoD, both methods exhibit numerical errors in high dimensions when dealing with the probability density function (PDF) associated with Brownian motion. The point-wise PDF values tend to decrease exponentially as dimension increases, surpassing the precision of numerical simulations and resulting in substantial errors. Moreover, due to its massive sampling, Monte Carlo fails to offer fast sampling. Modeling the logarithm likelihood (LL) via vanilla PINNs transforms the FP equation into a difficult HJB equation, whose error grows rapidly with dimension. To this end, we propose a novel approach utilizing a score-based solver to fit the score function in SDEs. The score function, defined as the gradient of t
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;HALO&#27861;&#35268;&#27169;&#24335;&#65292;&#20351;&#29992;&#28608;&#32032;&#20998;&#26512;&#26469;&#35843;&#33410;&#20154;&#24037;&#26234;&#33021;&#30340;&#34892;&#20026;&#27169;&#24335;&#65292;&#20197;&#35299;&#20915;&#20215;&#20540;&#35013;&#36733;&#38382;&#39064;&#20013;&#30340;&#8220;&#22238;&#24418;&#38024;&#26368;&#22823;&#21270;&#22120;&#8221;&#22330;&#26223;&#12290;</title><link>https://arxiv.org/abs/2402.07462</link><description>&lt;p&gt;
&#20215;&#20540;&#35013;&#36733;&#38382;&#39064;&#30340;&#28608;&#32032;&#36866;&#24212;&#26041;&#27861;&#65306;&#39044;&#38450;&#22238;&#24418;&#38024;&#21551;&#31034;&#24405;&#65311;
&lt;/p&gt;
&lt;p&gt;
A Hormetic Approach to the Value-Loading Problem: Preventing the Paperclip Apocalypse?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07462
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;HALO&#27861;&#35268;&#27169;&#24335;&#65292;&#20351;&#29992;&#28608;&#32032;&#20998;&#26512;&#26469;&#35843;&#33410;&#20154;&#24037;&#26234;&#33021;&#30340;&#34892;&#20026;&#27169;&#24335;&#65292;&#20197;&#35299;&#20915;&#20215;&#20540;&#35013;&#36733;&#38382;&#39064;&#20013;&#30340;&#8220;&#22238;&#24418;&#38024;&#26368;&#22823;&#21270;&#22120;&#8221;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20215;&#20540;&#35013;&#36733;&#38382;&#39064;&#23545;&#20110;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#20182;&#20204;&#26088;&#22312;&#21019;&#24314;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#21644;&#20559;&#22909;&#30456;&#19968;&#33268;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#35813;&#38382;&#39064;&#38656;&#35201;&#19968;&#31181;&#26041;&#27861;&#26469;&#23450;&#20041;&#21644;&#35268;&#33539;&#20154;&#24037;&#26234;&#33021;&#34892;&#20026;&#30340;&#23433;&#20840;&#21644;&#26368;&#20248;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HALO&#65288;&#28608;&#32032;&#36866;&#24212;&#36890;&#36807;&#23545;&#25163;&#36807;&#31243;&#65289;&#36825;&#20010;&#27861;&#35268;&#27169;&#24335;&#65292;&#23427;&#20351;&#29992;&#28608;&#32032;&#20998;&#26512;&#26469;&#35843;&#33410;&#20154;&#24037;&#26234;&#33021;&#30340;&#34892;&#20026;&#27169;&#24335;&#12290;&#34892;&#20026;&#28608;&#32032;&#36866;&#24212;&#26159;&#19968;&#31181;&#29616;&#35937;&#65292;&#20302;&#39057;&#29575;&#30340;&#34892;&#20026;&#20855;&#26377;&#30410;&#22788;&#65292;&#32780;&#39640;&#39057;&#29575;&#30340;&#34892;&#20026;&#21017;&#26377;&#23475;&#12290;&#36890;&#36807;&#23558;&#34892;&#20026;&#24314;&#27169;&#20026;&#21464;&#24577;&#23545;&#25163;&#36807;&#31243;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#34892;&#20026;&#39057;&#29575;&#21709;&#24212;&#20998;&#26512;&#65288;BFRA&#65289;&#25110;&#34892;&#20026;&#35745;&#25968;&#21709;&#24212;&#20998;&#26512;&#65288;BCRA&#65289;&#26469;&#37327;&#21270;&#21487;&#37325;&#22797;&#34892;&#20026;&#30340;&#28608;&#32032;&#38480;&#21046;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;HALO&#26469;&#35299;&#20915;&#8220;&#22238;&#24418;&#38024;&#26368;&#22823;&#21270;&#22120;&#8221;&#22330;&#26223;&#65292;&#36825;&#26159;&#19968;&#20010;&#24605;&#24819;&#23454;&#39564;&#65292;&#20854;&#20013;&#19968;&#20010;&#26410;&#21463;&#31649;&#21046;&#30340;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#26159;&#23558;&#25152;&#26377;&#29289;&#36136;&#36716;&#21270;&#20026;&#22238;&#24418;&#38024;&#12290;
&lt;/p&gt;
&lt;p&gt;
The value-loading problem is a significant challenge for researchers aiming to create artificial intelligence (AI) systems that align with human values and preferences. This problem requires a method to define and regulate safe and optimal limits of AI behaviors. In this work, we propose HALO (Hormetic ALignment via Opponent processes), a regulatory paradigm that uses hormetic analysis to regulate the behavioral patterns of AI. Behavioral hormesis is a phenomenon where low frequencies of a behavior have beneficial effects, while high frequencies are harmful. By modeling behaviors as allostatic opponent processes, we can use either Behavioral Frequency Response Analysis (BFRA) or Behavioral Count Response Analysis (BCRA) to quantify the hormetic limits of repeatable behaviors. We demonstrate how HALO can solve the 'paperclip maximizer' scenario, a thought experiment where an unregulated AI tasked with making paperclips could end up converting all matter in the universe into paperclips. 
&lt;/p&gt;</description></item><item><title>OS-Copilot&#26159;&#19968;&#20010;&#36890;&#29992;&#35745;&#31639;&#26426;&#20195;&#29702;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#19982;&#25805;&#20316;&#31995;&#32479;&#20013;&#30340;&#21508;&#31181;&#20803;&#32032;&#36827;&#34892;&#20132;&#20114;&#65292;&#21253;&#25324;&#32593;&#32476;&#12289;&#20195;&#30721;&#32456;&#31471;&#12289;&#25991;&#20214;&#12289;&#22810;&#23186;&#20307;&#21644;&#31532;&#19977;&#26041;&#24212;&#29992;&#31243;&#24207;&#12290;&#20351;&#29992;OS-Copilot&#26500;&#24314;&#30340;&#33258;&#25105;&#25552;&#21319;&#30340;FRIDAY&#20195;&#29702;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36807;&#20197;&#21069;&#30340;&#26041;&#27861;35%&#12290;</title><link>https://arxiv.org/abs/2402.07456</link><description>&lt;p&gt;
OS-Copilot: &#22522;&#20110;&#33258;&#25105;&#25913;&#36827;&#30340;&#36890;&#29992;&#35745;&#31639;&#26426;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
OS-Copilot: Towards Generalist Computer Agents with Self-Improvement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07456
&lt;/p&gt;
&lt;p&gt;
OS-Copilot&#26159;&#19968;&#20010;&#36890;&#29992;&#35745;&#31639;&#26426;&#20195;&#29702;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#19982;&#25805;&#20316;&#31995;&#32479;&#20013;&#30340;&#21508;&#31181;&#20803;&#32032;&#36827;&#34892;&#20132;&#20114;&#65292;&#21253;&#25324;&#32593;&#32476;&#12289;&#20195;&#30721;&#32456;&#31471;&#12289;&#25991;&#20214;&#12289;&#22810;&#23186;&#20307;&#21644;&#31532;&#19977;&#26041;&#24212;&#29992;&#31243;&#24207;&#12290;&#20351;&#29992;OS-Copilot&#26500;&#24314;&#30340;&#33258;&#25105;&#25552;&#21319;&#30340;FRIDAY&#20195;&#29702;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36807;&#20197;&#21069;&#30340;&#26041;&#27861;35%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#35745;&#31639;&#26426;&#30340;&#33258;&#20027;&#20132;&#20114;&#19968;&#30452;&#26159;&#19968;&#20010;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#30340;&#38271;&#26399;&#25361;&#25112;&#65292;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26222;&#21450;&#26174;&#33879;&#21152;&#24555;&#20102;&#25968;&#23383;&#20195;&#29702;&#30340;&#26500;&#24314;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#20195;&#29702;&#34987;&#35774;&#35745;&#29992;&#20110;&#19982;&#29305;&#23450;&#36719;&#20214;&#25110;&#32593;&#31449;&#31561;&#29421;&#31364;&#39046;&#22495;&#36827;&#34892;&#20132;&#20114;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#36890;&#29992;&#35745;&#31639;&#26426;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;OS-Copilot&#65292;&#19968;&#20010;&#26500;&#24314;&#36890;&#29992;&#20195;&#29702;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#19982;&#25805;&#20316;&#31995;&#32479;&#65288;OS&#65289;&#20013;&#30340;&#20840;&#38754;&#20803;&#32032;&#36827;&#34892;&#20132;&#20114;&#65292;&#21253;&#25324;&#32593;&#32476;&#12289;&#20195;&#30721;&#32456;&#31471;&#12289;&#25991;&#20214;&#12289;&#22810;&#23186;&#20307;&#21644;&#21508;&#31181;&#31532;&#19977;&#26041;&#24212;&#29992;&#31243;&#24207;&#12290;&#25105;&#20204;&#20351;&#29992;OS-Copilot&#21019;&#24314;&#20102;FRIDAY&#65292;&#19968;&#20010;&#33021;&#22815;&#33258;&#25105;&#25552;&#21319;&#30340;&#20855;&#35937;&#21270;&#20195;&#29702;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#36890;&#29992;&#35745;&#31639;&#26426;&#20219;&#21153;&#12290;&#22312;GAIA&#65292;&#19968;&#20010;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;FRIDAY&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#20197;&#21069;&#30340;&#26041;&#27861;35%&#65292;&#36890;&#36807;&#20174;&#20808;&#21069;&#20219;&#21153;&#20013;&#31215;&#32047;&#30340;&#25216;&#33021;&#65292;&#23637;&#31034;&#20102;&#23545;&#26410;&#35265;&#24212;&#29992;&#30340;&#24378;&#22823;&#27010;&#25324;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#25968;&#23383;&#21644;&#23450;&#37327;&#30340;...
&lt;/p&gt;
&lt;p&gt;
Autonomous interaction with the computer has been a longstanding challenge with great potential, and the recent proliferation of large language models (LLMs) has markedly accelerated progress in building digital agents. However, most of these agents are designed to interact with a narrow domain, such as a specific software or website. This narrow focus constrains their applicability for general computer tasks. To this end, we introduce OS-Copilot, a framework to build generalist agents capable of interfacing with comprehensive elements in an operating system (OS), including the web, code terminals, files, multimedia, and various third-party applications. We use OS-Copilot to create FRIDAY, a self-improving embodied agent for automating general computer tasks. On GAIA, a general AI assistants benchmark, FRIDAY outperforms previous methods by 35%, showcasing strong generalization to unseen applications via accumulated skills from previous tasks. We also present numerical and quantitative
&lt;/p&gt;</description></item><item><title>TriAug&#26159;&#19968;&#20010;&#29992;&#20110;&#20083;&#33146;&#36229;&#22768;&#22270;&#20687;&#30340;&#24322;&#24120;&#26679;&#26412;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#19977;&#20803;&#29366;&#24577;&#22686;&#24378;&#21644;&#24179;&#34913;&#30340;&#29699;&#24418;&#25439;&#22833;&#26469;&#25552;&#39640;&#31034;&#36394;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#21644;&#24322;&#24120;&#26679;&#26412;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.07452</link><description>&lt;p&gt;
TriAug&#65306;&#29992;&#20110;&#36229;&#22768;&#20083;&#33146;&#30149;&#21464;&#19981;&#24179;&#34913;&#20998;&#31867;&#30340;&#24322;&#24120;&#26679;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
TriAug: Out-of-Distribution Detection for Robust Classification of Imbalanced Breast Lesion in Ultrasound
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07452
&lt;/p&gt;
&lt;p&gt;
TriAug&#26159;&#19968;&#20010;&#29992;&#20110;&#20083;&#33146;&#36229;&#22768;&#22270;&#20687;&#30340;&#24322;&#24120;&#26679;&#26412;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#19977;&#20803;&#29366;&#24577;&#22686;&#24378;&#21644;&#24179;&#34913;&#30340;&#29699;&#24418;&#25439;&#22833;&#26469;&#25552;&#39640;&#31034;&#36394;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#21644;&#24322;&#24120;&#26679;&#26412;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;&#30340;&#30142;&#30149;&#65292;&#22914;&#20083;&#33146;&#30149;&#21464;&#30340;&#32452;&#32455;&#20122;&#22411;&#65292;&#20855;&#26377;&#20005;&#37325;&#19981;&#21516;&#30340;&#21457;&#30149;&#29575;&#12290;&#21363;&#20351;&#36890;&#36807;&#22823;&#37327;&#30340;&#31034;&#36394;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#27169;&#22411;&#22312;&#20020;&#24202;&#23454;&#38469;&#20013;&#36890;&#24120;&#36935;&#21040;&#23646;&#20110;&#26410;&#35265;&#31867;&#21035;&#30340;&#24322;&#24120;&#26679;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#22522;&#20110;&#20083;&#33146;&#36229;&#22768;&#22270;&#20687;&#30340;&#38271;&#23614;&#24322;&#24120;&#26679;&#26412;&#26816;&#27979;&#20219;&#21153;&#65292;&#24182;&#37197;&#22791;&#20102;&#19968;&#31181;&#19977;&#20803;&#29366;&#24577;&#22686;&#24378;&#65288;TriAug&#65289;&#65292;&#23427;&#21487;&#20197;&#25552;&#39640;&#31034;&#36394;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#33391;&#22909;&#30340;&#24322;&#24120;&#26679;&#26412;&#26816;&#27979;&#24615;&#33021;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#24179;&#34913;&#30340;&#29699;&#24418;&#25439;&#22833;&#26469;&#22788;&#29702;&#31867;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Different diseases, such as histological subtypes of breast lesions, have severely varying incidence rates. Even trained with substantial amount of in-distribution (ID) data, models often encounter out-of-distribution (OOD) samples belonging to unseen classes in clinical reality. To address this, we propose a novel framework built upon a long-tailed OOD detection task for breast ultrasound images. It is equipped with a triplet state augmentation (TriAug) which improves ID classification accuracy while maintaining a promising OOD detection performance. Meanwhile, we designed a balanced sphere loss to handle the class imbalanced problem.
&lt;/p&gt;</description></item><item><title>AraSpider&#26159;&#39318;&#20010;&#38463;&#25289;&#20271;&#35821;&#29256;&#26412;&#30340;Spider&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#34920;&#26126;&#20351;&#29992;&#22238;&#35793;&#31574;&#30053;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;ChatGPT 3.5&#21644;SQLCoder&#27169;&#22411;&#22312;&#38463;&#25289;&#20271;&#35821;NLP&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.07448</link><description>&lt;p&gt;
AraSpider&#65306;&#23454;&#29616;&#38463;&#25289;&#20271;&#35821;&#21040;SQL&#30340;&#27665;&#20027;&#21270;
&lt;/p&gt;
&lt;p&gt;
AraSpider: Democratizing Arabic-to-SQL
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07448
&lt;/p&gt;
&lt;p&gt;
AraSpider&#26159;&#39318;&#20010;&#38463;&#25289;&#20271;&#35821;&#29256;&#26412;&#30340;Spider&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#34920;&#26126;&#20351;&#29992;&#22238;&#35793;&#31574;&#30053;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;ChatGPT 3.5&#21644;SQLCoder&#27169;&#22411;&#22312;&#38463;&#25289;&#20271;&#35821;NLP&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;AraSpider&#65292;&#36825;&#26159;&#39318;&#20010;&#38463;&#25289;&#20271;&#35821;&#29256;&#26412;&#30340;Spider&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#21319;&#38463;&#25289;&#20271;&#35821;&#31038;&#21306;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#12290;&#35813;&#30740;&#31350;&#27979;&#35797;&#20102;&#22235;&#20010;&#22810;&#35821;&#35328;&#32763;&#35793;&#27169;&#22411;&#22312;&#23558;&#33521;&#25991;&#32763;&#35793;&#25104;&#38463;&#25289;&#20271;&#35821;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#21478;&#22806;&#65292;&#36824;&#35780;&#20272;&#20102;&#20004;&#20010;&#27169;&#22411;&#22312;&#20174;&#38463;&#25289;&#20271;&#25991;&#26412;&#29983;&#25104;SQL&#26597;&#35810;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#22238;&#35793;&#26174;&#33879;&#25552;&#39640;&#20102;ChatGPT 3.5&#21644;SQLCoder&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#36825;&#20004;&#20010;&#27169;&#22411;&#22312;Spider&#25968;&#25454;&#38598;&#19978;&#34987;&#35748;&#20026;&#26159;&#26368;&#20339;&#34920;&#29616;&#32773;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;ChatGPT 3.5&#23637;&#31034;&#20102;&#39640;&#36136;&#37327;&#30340;&#32763;&#35793;&#65292;&#32780;SQLCoder&#22312;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#35813;&#30740;&#31350;&#24378;&#35843;&#20102;&#23558;&#19978;&#19979;&#25991;&#27169;&#24335;&#21644;&#37319;&#29992;&#22238;&#35793;&#31574;&#30053;&#32435;&#20837;&#38463;&#25289;&#20271;&#35821;NLP&#20219;&#21153;&#20013;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#30340;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#26041;&#27861;&#35770;&#20197;&#23454;&#29616;&#32467;&#26524;&#22797;&#29616;&#24182;&#23558;&#25968;&#25454;&#38598;&#32763;&#35793;&#25104;&#20854;&#20182;&#35821;&#35328;&#65292;&#31361;&#26174;&#20102;&#30740;&#31350;&#20419;&#36827;&#30340;&#25215;&#35834;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents AraSpider, the first Arabic version of the Spider dataset, aimed at improving natural language processing (NLP) in the Arabic-speaking community. Four multilingual translation models were tested for their effectiveness in translating English to Arabic. Additionally, two models were assessed for their ability to generate SQL queries from Arabic text. The results showed that using back translation significantly improved the performance of both ChatGPT 3.5 and SQLCoder models, which are considered top performers on the Spider dataset. Notably, ChatGPT 3.5 demonstrated high-quality translation, while SQLCoder excelled in text-to-SQL tasks. The study underscores the importance of incorporating contextual schema and employing back translation strategies to enhance model performance in Arabic NLP tasks. Moreover, the provision of detailed methodologies for reproducibility and translation of the dataset into other languages highlights the research's commitment to promoting 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#28216;&#25103;&#20195;&#29702;&#25991;&#26412;&#21629;&#20196;&#25511;&#21046;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#20195;&#30721;&#29983;&#25104;&#65292;&#23454;&#29616;&#20102;&#23545;&#33258;&#30001;&#24418;&#24335;&#30340;&#33258;&#28982;&#35821;&#35328;&#21629;&#20196;&#30340;&#29702;&#35299;&#21644;&#25191;&#34892;&#65292;&#20026;&#23454;&#26102;&#35821;&#35328;&#20132;&#20114;&#28216;&#25103;&#20195;&#29702;&#39046;&#22495;&#24102;&#26469;&#20102;&#26174;&#33879;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2402.07442</link><description>&lt;p&gt;
&#28216;&#25103;&#20195;&#29702;&#39537;&#21160;&#30340;&#33258;&#30001;&#25991;&#26412;&#21629;&#20196;&#65306;&#22522;&#20110;LLM&#20195;&#30721;&#29983;&#25104;&#21644;&#34892;&#20026;&#20998;&#25903;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Game Agent Driven by Free-Form Text Command: Using LLM-based Code Generation and Behavior Branch
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#28216;&#25103;&#20195;&#29702;&#25991;&#26412;&#21629;&#20196;&#25511;&#21046;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#20195;&#30721;&#29983;&#25104;&#65292;&#23454;&#29616;&#20102;&#23545;&#33258;&#30001;&#24418;&#24335;&#30340;&#33258;&#28982;&#35821;&#35328;&#21629;&#20196;&#30340;&#29702;&#35299;&#21644;&#25191;&#34892;&#65292;&#20026;&#23454;&#26102;&#35821;&#35328;&#20132;&#20114;&#28216;&#25103;&#20195;&#29702;&#39046;&#22495;&#24102;&#26469;&#20102;&#26174;&#33879;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#26377;&#20960;&#31181;&#23581;&#35797;&#23454;&#29616;&#28216;&#25103;&#20195;&#29702;&#30340;&#25991;&#26412;&#21629;&#20196;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#25216;&#26415;&#20165;&#38480;&#20110;&#22788;&#29702;&#39044;&#23450;&#20041;&#26684;&#24335;&#30340;&#21629;&#20196;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#28216;&#25103;&#20195;&#29702;&#25991;&#26412;&#21629;&#20196;&#25511;&#21046;&#31995;&#32479;&#65292;&#21487;&#20197;&#29702;&#35299;&#20197;&#33258;&#30001;&#24418;&#24335;&#34920;&#36798;&#30340;&#33258;&#28982;&#35821;&#35328;&#21629;&#20196;&#12290;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#20351;&#29992;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20195;&#30721;&#29983;&#25104;&#26469;&#35299;&#37322;&#21644;&#36716;&#25442;&#33258;&#28982;&#35821;&#35328;&#21629;&#20196;&#20026;&#34892;&#20026;&#20998;&#25903;&#65292;&#24182;&#36890;&#36807;&#34892;&#20026;&#26641;&#30340;&#24418;&#24335;&#23454;&#29616;&#25191;&#34892;&#12290;&#26412;&#30740;&#31350;&#22312;&#27169;&#25311;Pok&#233;mon&#28216;&#25103;&#30340;&#28216;&#25103;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#65292;&#24182;&#28041;&#21450;&#22810;&#20010;&#21442;&#19982;&#32773;&#12290;&#32467;&#26524;&#35777;&#23454;&#20102;&#35813;&#31995;&#32479;&#20855;&#26377;&#29702;&#35299;&#21644;&#25191;&#34892;&#33258;&#28982;&#35821;&#35328;&#21629;&#20196;&#30340;&#33021;&#21147;&#65292;&#22312;&#23454;&#26102;&#35821;&#35328;&#20132;&#20114;&#28216;&#25103;&#20195;&#29702;&#39046;&#22495;&#20855;&#26377;&#26174;&#33879;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several attempts have been made to implement text command control for game agents. However, current technologies are limited to processing predefined format commands. This paper proposes a pioneering text command control system for a game agent that can understand natural language commands expressed in free-form. The proposed system uses a large language model (LLM) for code generation to interpret and transform natural language commands into behavior branch, a proposed knowledge expression based on behavior trees, which facilitates execution by the game agent. This study conducted empirical validation within a game environment that simulates a Pok\'emon game and involved multiple participants. The results confirmed the system's ability to understand and carry out natural language commands, representing a noteworthy in the realm of real-time language interactive game agents.   Notice for the use of this material. The copyright of this material is retained by the Japanese Society for Ar
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#23398;&#20064;&#22914;&#20309;&#35774;&#35745;&#26368;&#20248;&#31246;&#25910;&#65292;&#20197;&#22312;&#38750;&#21407;&#23376;&#25317;&#22581;&#21338;&#24328;&#20013;&#25552;&#39640;&#25928;&#29575;&#12290;&#20026;&#20102;&#35299;&#20915;&#25351;&#25968;&#32423;&#30340;&#31246;&#25910;&#20989;&#25968;&#31354;&#38388;&#12289;&#26799;&#24230;&#19981;&#23384;&#22312;&#21644;&#30446;&#26631;&#20989;&#25968;&#30340;&#38750;&#20984;&#24615;&#31561;&#25361;&#25112;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#20102;&#20998;&#27573;&#32447;&#24615;&#31246;&#25910;&#12289;&#39069;&#22806;&#30340;&#32447;&#24615;&#39033;&#21644;&#26377;&#25928;&#30340;&#23376;&#20363;&#31243;&#30340;&#26032;&#39062;&#32452;&#25104;&#37096;&#20998;&#12290;</title><link>https://arxiv.org/abs/2402.07437</link><description>&lt;p&gt;
&#38750;&#21407;&#23376;&#25317;&#22581;&#21338;&#24328;&#20013;&#23398;&#20064;&#26368;&#20248;&#31246;&#25910;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Learning Optimal Tax Design in Nonatomic Congestion Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#23398;&#20064;&#22914;&#20309;&#35774;&#35745;&#26368;&#20248;&#31246;&#25910;&#65292;&#20197;&#22312;&#38750;&#21407;&#23376;&#25317;&#22581;&#21338;&#24328;&#20013;&#25552;&#39640;&#25928;&#29575;&#12290;&#20026;&#20102;&#35299;&#20915;&#25351;&#25968;&#32423;&#30340;&#31246;&#25910;&#20989;&#25968;&#31354;&#38388;&#12289;&#26799;&#24230;&#19981;&#23384;&#22312;&#21644;&#30446;&#26631;&#20989;&#25968;&#30340;&#38750;&#20984;&#24615;&#31561;&#25361;&#25112;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#20102;&#20998;&#27573;&#32447;&#24615;&#31246;&#25910;&#12289;&#39069;&#22806;&#30340;&#32447;&#24615;&#39033;&#21644;&#26377;&#25928;&#30340;&#23376;&#20363;&#31243;&#30340;&#26032;&#39062;&#32452;&#25104;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#23398;&#20064;&#26368;&#20248;&#31246;&#25910;&#35774;&#35745;&#65292;&#20197;&#22312;&#38750;&#21407;&#23376;&#25317;&#22581;&#21338;&#24328;&#20013;&#26368;&#22823;&#21270;&#25928;&#29575;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#29609;&#23478;&#20043;&#38388;&#30340;&#33258;&#21033;&#34892;&#20026;&#21487;&#33021;&#20250;&#30772;&#22351;&#31995;&#32479;&#30340;&#25928;&#29575;&#12290;&#31246;&#21153;&#26426;&#21046;&#26159;&#32531;&#35299;&#27492;&#38382;&#39064;&#24182;&#24341;&#23548;&#31038;&#20250;&#26368;&#20248;&#34892;&#20026;&#30340;&#24120;&#35265;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#37319;&#21462;&#20102;&#23398;&#20064;&#26368;&#20248;&#31246;&#25910;&#30340;&#21021;&#22987;&#27493;&#39588;&#65292;&#35813;&#26368;&#20248;&#31246;&#25910;&#21487;&#20197;&#36890;&#36807;&#24179;&#34913;&#21453;&#39304;&#26469;&#26368;&#23567;&#21270;&#31038;&#20250;&#25104;&#26412;&#65292;&#21363;&#31246;&#21153;&#35774;&#35745;&#32773;&#21482;&#33021;&#35266;&#23519;&#21040;&#24378;&#21046;&#31246;&#25910;&#19979;&#30340;&#22343;&#34913;&#29366;&#24577;&#12290;&#30001;&#20110;&#25351;&#25968;&#32423;&#30340;&#31246;&#25910;&#20989;&#25968;&#31354;&#38388;&#65292;&#26799;&#24230;&#19981;&#23384;&#22312;&#21644;&#30446;&#26631;&#20989;&#25968;&#30340;&#38750;&#20984;&#24615;&#65292;&#29616;&#26377;&#31639;&#27861;&#19981;&#36866;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21033;&#29992;&#20102;&#20960;&#20010;&#26032;&#39062;&#30340;&#32452;&#25104;&#37096;&#20998;&#65306;&#65288;1&#65289;&#20998;&#27573;&#32447;&#24615;&#31246;&#25910;&#26469;&#36817;&#20284;&#26368;&#20248;&#31246;&#25910;&#65307;&#65288;2&#65289;&#39069;&#22806;&#30340;&#32447;&#24615;&#39033;&#26469;&#20445;&#35777;&#24378;&#20984;&#28508;&#21147;&#20989;&#25968;&#65307;&#65288;3&#65289;&#26377;&#25928;&#30340;&#23376;&#20363;&#31243;&#26469;&#25214;&#21040;&#8220;&#36793;&#30028;&#8221;&#31246;&#25910;&#12290;&#35813;&#31639;&#27861;&#21487;&#20197;&#25214;&#21040;&#19968;&#20010;$\epsilon$-&#26368;&#20248;&#31246;&#25910;&#65292;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026;$O(\bet
&lt;/p&gt;
&lt;p&gt;
We study how to learn the optimal tax design to maximize the efficiency in nonatomic congestion games. It is known that self-interested behavior among the players can damage the system's efficiency. Tax mechanisms is a common method to alleviate this issue and induce socially optimal behavior. In this work, we take the initial step for learning the optimal tax that can minimize the social cost with \emph{equilibrium feedback}, i.e., the tax designer can only observe the equilibrium state under the enforced tax. Existing algorithms are not applicable due to the exponentially large tax function space, nonexistence of the gradient, and nonconvexity of the objective. To tackle these challenges, our algorithm leverages several novel components: (1) piece-wise linear tax to approximate the optimal tax; (2) an extra linear term to guarantee a strongly convex potential function; (3) efficient subroutine to find the ``boundary'' tax. The algorithm can find an $\epsilon$-optimal tax with $O(\bet
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;GARCH&#12289;EWMA&#21644;IV&#27169;&#22411;&#22312;&#39044;&#27979;GBP/USD&#21644;EUR/GBP&#36135;&#24065;&#23545;&#27599;&#26085;&#22238;&#25253;&#30340;20&#22825;&#21464;&#21160;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#30740;&#31350;&#21457;&#29616;EUR/GBP&#36135;&#24065;&#23545;&#23384;&#22312;&#38750;&#23545;&#31216;&#22238;&#25253;&#30340;&#35777;&#25454;&#65292;&#32780;GBP/USD&#36135;&#24065;&#23545;&#30340;&#35777;&#25454;&#24182;&#19981;&#19968;&#33268;&#12290;</title><link>https://arxiv.org/abs/2402.07435</link><description>&lt;p&gt;
&#20998;&#26512;&#36135;&#24065;&#27874;&#21160;&#65306;GBP/USD&#21644;EUR/GBP&#36135;&#24065;&#23545;&#30340;GARCH&#12289;EWMA&#21644;IV&#27169;&#22411;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Analyzing Currency Fluctuations: A Comparative Study of GARCH, EWMA, and IV Models for GBP/USD and EUR/GBP Pairs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;GARCH&#12289;EWMA&#21644;IV&#27169;&#22411;&#22312;&#39044;&#27979;GBP/USD&#21644;EUR/GBP&#36135;&#24065;&#23545;&#27599;&#26085;&#22238;&#25253;&#30340;20&#22825;&#21464;&#21160;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#30740;&#31350;&#21457;&#29616;EUR/GBP&#36135;&#24065;&#23545;&#23384;&#22312;&#38750;&#23545;&#31216;&#22238;&#25253;&#30340;&#35777;&#25454;&#65292;&#32780;GBP/USD&#36135;&#24065;&#23545;&#30340;&#35777;&#25454;&#24182;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#33521;&#38225;&#65288;GBP&#65289;&#20215;&#20540;&#30340;&#27874;&#21160;&#24773;&#20917;&#65292;&#29305;&#21035;&#20851;&#27880;&#20854;&#19982;&#32654;&#20803;&#65288;USD&#65289;&#21644;&#27431;&#20803;&#65288;EUR&#65289;&#36135;&#24065;&#23545;&#30340;&#20851;&#31995;&#12290;&#21033;&#29992;2018&#24180;6&#26376;15&#26085;&#33267;2023&#24180;6&#26376;15&#26085;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#24212;&#29992;&#19981;&#21516;&#30340;&#25968;&#23398;&#27169;&#22411;&#26469;&#35780;&#20272;&#23427;&#20204;&#22312;&#39044;&#27979;&#36135;&#24065;&#23545;&#27599;&#26085;&#22238;&#25253;&#30340;20&#22825;&#21464;&#21160;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#28041;&#21450;&#25351;&#25968;&#21152;&#26435;&#31227;&#21160;&#24179;&#22343;&#65288;EWMA&#65289;&#12289;&#24191;&#20041;&#33258;&#22238;&#24402;&#26465;&#20214;&#24322;&#26041;&#24046;&#65288;GARCH&#65289;&#27169;&#22411;&#21644;&#38544;&#21547;&#27874;&#21160;&#29575;&#65288;IV&#65289;&#27169;&#22411;&#30340;&#23454;&#26045;&#12290;&#20026;&#20102;&#35780;&#20272;&#23427;&#20204;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#20351;&#29992;&#22343;&#26041;&#26681;&#35823;&#24046;&#65288;RMSE&#65289;&#21644;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#25351;&#26631;&#27604;&#36739;&#23427;&#20204;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;GARCH&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#65292;&#26816;&#26597;&#20102;&#23558;&#20854;&#24212;&#29992;&#20110;&#25152;&#25552;&#20379;&#25968;&#25454;&#38598;&#26102;&#30340;&#32479;&#35745;&#29305;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;EUR/GBP&#36135;&#24065;&#23545;&#23384;&#22312;&#38750;&#23545;&#31216;&#22238;&#25253;&#30340;&#35777;&#25454;&#65292;&#32780;GBP/USD&#36135;&#24065;&#23545;&#30340;&#35777;&#25454;&#24182;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we examine the fluctuation in the value of the Great Britain Pound (GBP). We focus particularly on its relationship with the United States Dollar (USD) and the Euro (EUR) currency pairs. Utilizing data from June 15, 2018, to June 15, 2023, we apply various mathematical models to assess their effectiveness in predicting the 20-day variation in the pairs' daily returns. Our analysis involves the implementation of Exponentially Weighted Moving Average (EWMA), Generalized Autoregressive Conditional Heteroskedasticity (GARCH) models, and Implied Volatility (IV) models. To evaluate their performance, we compare the accuracy of their predictions using Root Mean Square Error (RMSE) and Mean Absolute Error (MAE) metrics. We delve into the intricacies of GARCH models, examining their statistical characteristics when applied to the provided dataset. Our findings suggest the existence of asymmetric returns in the EUR/GBP pair, while such evidence is inconclusive for the GBP/USD pair
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;&#31890;&#23376;&#28388;&#27874;SLAM&#26041;&#27861;&#35299;&#20915;&#20102;&#36710;&#36742;&#23450;&#20301;&#30340;&#25361;&#25112;&#65292;&#21033;&#29992;&#32534;&#30721;&#25968;&#25454;&#12289;&#20809;&#32420;&#38464;&#34746;&#20202;&#21644;&#28608;&#20809;&#38647;&#36798;&#25216;&#26415;&#23454;&#29616;&#31934;&#30830;&#30340;&#36710;&#36742;&#36816;&#21160;&#20272;&#35745;&#21644;&#29615;&#22659;&#24863;&#30693;&#12290;</title><link>https://arxiv.org/abs/2402.07429</link><description>&lt;p&gt;
&#29992;&#20110;&#36710;&#36742;&#23450;&#20301;&#30340;&#31890;&#23376;&#28388;&#27874;SLAM&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Particle Filter SLAM for Vehicle Localization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#31890;&#23376;&#28388;&#27874;SLAM&#26041;&#27861;&#35299;&#20915;&#20102;&#36710;&#36742;&#23450;&#20301;&#30340;&#25361;&#25112;&#65292;&#21033;&#29992;&#32534;&#30721;&#25968;&#25454;&#12289;&#20809;&#32420;&#38464;&#34746;&#20202;&#21644;&#28608;&#20809;&#38647;&#36798;&#25216;&#26415;&#23454;&#29616;&#31934;&#30830;&#30340;&#36710;&#36742;&#36816;&#21160;&#20272;&#35745;&#21644;&#29615;&#22659;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#26102;&#23450;&#20301;&#19982;&#24314;&#22270;&#65288;SLAM&#65289;&#22312;&#26426;&#22120;&#20154;&#25216;&#26415;&#20013;&#26159;&#19968;&#20010;&#33392;&#24040;&#30340;&#25361;&#25112;&#65292;&#28041;&#21450;&#22312;&#38476;&#29983;&#29615;&#22659;&#20013;&#21160;&#24577;&#26500;&#24314;&#22320;&#22270;&#30340;&#21516;&#26102;&#30830;&#23450;&#26426;&#22120;&#20154;&#23450;&#20301;&#30340;&#31934;&#30830;&#20301;&#32622;&#12290;&#36825;&#39033;&#22797;&#26434;&#30340;&#20219;&#21153;&#21463;&#21040;&#20102;&#8220;&#20808;&#26377;&#40481;&#36824;&#26159;&#20808;&#26377;&#34507;&#8221;&#22256;&#22659;&#30340;&#24433;&#21709;&#65292;&#20934;&#30830;&#30340;&#24314;&#22270;&#20381;&#36182;&#20110;&#21487;&#38752;&#30340;&#26426;&#22120;&#20154;&#23450;&#20301;&#20272;&#35745;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;&#27492;&#22806;&#65292;SLAM&#30340;&#35745;&#31639;&#23494;&#38598;&#24615;&#22686;&#21152;&#20102;&#39069;&#22806;&#30340;&#22797;&#26434;&#24615;&#65292;&#20351;&#20854;&#25104;&#20026;&#35813;&#39046;&#22495;&#20013;&#37325;&#35201;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20027;&#39064;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#31890;&#23376;&#28388;&#27874;SLAM&#26041;&#27861;&#26469;&#35299;&#20915;SLAM&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#32534;&#30721;&#25968;&#25454;&#21644;&#20809;&#32420;&#38464;&#34746;&#20202;&#65288;FOG&#65289;&#20449;&#24687;&#65292;&#23454;&#29616;&#23545;&#36710;&#36742;&#36816;&#21160;&#30340;&#31934;&#30830;&#20272;&#35745;&#65292;&#32780;&#28608;&#20809;&#38647;&#36798;&#25216;&#26415;&#36890;&#36807;&#25552;&#20379;&#26377;&#20851;&#21608;&#22260;&#38556;&#30861;&#29289;&#30340;&#35814;&#32454;&#20449;&#24687;&#65292;&#23545;&#29615;&#22659;&#24863;&#30693;&#20316;&#20986;&#36129;&#29486;&#12290;&#36825;&#20123;&#25968;&#25454;&#27969;&#30340;&#38598;&#25104;&#26368;&#32456;&#24314;&#31435;&#20102;&#19968;&#20010;&#31890;&#23376;&#28388;&#27874;SLAM&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simultaneous Localization and Mapping (SLAM) presents a formidable challenge in robotics, involving the dynamic construction of a map while concurrently determining the precise location of the robotic agent within an unfamiliar environment. This intricate task is further compounded by the inherent "chicken-and-egg" dilemma, where accurate mapping relies on a dependable estimation of the robot's location, and vice versa. Moreover, the computational intensity of SLAM adds an additional layer of complexity, making it a crucial yet demanding topic in the field. In our research, we address the challenges of SLAM by adopting the Particle Filter SLAM method. Our approach leverages encoded data and fiber optic gyro (FOG) information to enable precise estimation of vehicle motion, while lidar technology contributes to environmental perception by providing detailed insights into surrounding obstacles. The integration of these data streams culminates in the establishment of a Particle Filter SLAM
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24102;&#26377;&#27880;&#24847;&#26426;&#21046;&#30340;&#26032;&#38395;&#25512;&#33616;&#26041;&#27861;NRAM&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25968;&#23383;&#26032;&#38395;&#24179;&#21488;&#19978;&#30340;&#29992;&#25143;&#20010;&#24615;&#21270;&#26032;&#38395;&#20869;&#23481;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.07422</link><description>&lt;p&gt;
&#24102;&#26377;&#27880;&#24847;&#26426;&#21046;&#30340;&#26032;&#38395;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
News Recommendation with Attention Mechanism
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24102;&#26377;&#27880;&#24847;&#26426;&#21046;&#30340;&#26032;&#38395;&#25512;&#33616;&#26041;&#27861;NRAM&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25968;&#23383;&#26032;&#38395;&#24179;&#21488;&#19978;&#30340;&#29992;&#25143;&#20010;&#24615;&#21270;&#26032;&#38395;&#20869;&#23481;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#26032;&#38395;&#25512;&#33616;&#39046;&#22495;&#65292;&#36825;&#26159;&#22312;&#32447;&#20449;&#24687;&#20998;&#20139;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23545;&#26032;&#38395;&#25512;&#33616;&#36827;&#34892;&#20102;&#28165;&#26224;&#30340;&#20171;&#32461;&#65292;&#23450;&#20041;&#20102;&#26680;&#24515;&#38382;&#39064;&#24182;&#24635;&#32467;&#20102;&#24403;&#21069;&#26041;&#27861;&#21644;&#36817;&#26399;&#20540;&#24471;&#27880;&#24847;&#30340;&#31639;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NRAM&#65288;&#24102;&#26377;&#27880;&#24847;&#26426;&#21046;&#30340;&#26032;&#38395;&#25512;&#33616;&#65289;&#30340;&#23454;&#29616;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26032;&#38395;&#25512;&#33616;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;NRAM&#26377;&#28508;&#21147;&#26174;&#33879;&#25552;&#39640;&#25968;&#23383;&#26032;&#38395;&#24179;&#21488;&#19978;&#38024;&#23545;&#29992;&#25143;&#20010;&#24615;&#21270;&#30340;&#26032;&#38395;&#20869;&#23481;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the area of news recommendation, a key component of online information sharing. Initially, we provide a clear introduction to news recommendation, defining the core problem and summarizing current methods and notable recent algorithms. We then present our work on implementing the NRAM (News Recommendation with Attention Mechanism), an attention-based approach for news recommendation, and assess its effectiveness. Our evaluation shows that NRAM has the potential to significantly improve how news content is personalized for users on digital news platforms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36716;&#36816;&#28151;&#28102;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#36716;&#36816;&#21311;&#21517;&#24615;&#30340;&#27010;&#24565;&#65292;&#24182;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#28385;&#36275;&#35813;&#21311;&#21517;&#24615;&#20934;&#21017;&#30340;&#35268;&#21010;/&#25628;&#32034;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.07420</link><description>&lt;p&gt;
&#20851;&#20110;&#36716;&#36816;&#28151;&#28102;&#38382;&#39064;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Transit Obfuscation Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36716;&#36816;&#28151;&#28102;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#36716;&#36816;&#21311;&#21517;&#24615;&#30340;&#27010;&#24565;&#65292;&#24182;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#28385;&#36275;&#35813;&#21311;&#21517;&#24615;&#20934;&#21017;&#30340;&#35268;&#21010;/&#25628;&#32034;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26576;&#20123;&#20132;&#36890;&#21644;&#30417;&#35270;&#22330;&#26223;&#20013;&#65292;&#38544;&#34255;&#36335;&#24452;&#19978;&#25110;&#20174;&#36335;&#24452;&#19978;&#21487;&#35265;&#30340;&#20013;&#38388;&#28857;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30446;&#26631;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36716;&#36816;&#28151;&#28102;&#38382;&#39064;&#65292;&#21363;&#20174;&#26576;&#20010;&#36215;&#22987;&#20301;&#32622;&#21040;&#36798;&#30446;&#26631;&#20301;&#32622;&#30340;&#21516;&#26102;&#65292;"&#35206;&#30422;"&#38656;&#35201;&#38544;&#34255;&#30340;&#29305;&#23450;&#36807;&#22659;&#28857;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36716;&#36816;&#21311;&#21517;&#24615;&#30340;&#27010;&#24565;&#65292;&#21363;&#23545;&#29305;&#23450;&#36807;&#22659;&#28857;&#30340;&#21311;&#21517;&#24615;&#36827;&#34892;&#37327;&#21270;&#20445;&#35777;&#65292;&#21363;&#20351;&#22312;&#20855;&#26377;&#23545;&#36335;&#24452;&#35268;&#21010;&#31639;&#27861;&#26377;&#20840;&#38754;&#20102;&#35299;&#30340;&#24378;&#22823;&#23545;&#25163;&#38754;&#21069;&#20063;&#26159;&#22914;&#27492;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#28385;&#36275;&#35813;&#21311;&#21517;&#24615;&#20934;&#21017;&#30340;&#35268;&#21010;/&#25628;&#32034;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concealing an intermediate point on a route or visible from a route is an important goal in some transportation and surveillance scenarios. This paper studies the Transit Obfuscation Problem, the problem of traveling from some start location to an end location while "covering" a specific transit point that needs to be concealed from adversaries. We propose the notion of transit anonymity, a quantitative guarantee of the anonymity of a specific transit point, even with a powerful adversary with full knowledge of the path planning algorithm. We propose and evaluate planning/search algorithms that satisfy this anonymity criterion.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#36890;&#36807;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#30340;&#25512;&#36827;&#35745;&#31639;&#21487;&#20197;&#35745;&#31639;&#20219;&#20309;&#21487;&#36776;&#35782;&#30340;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#29992;&#20110;&#20174;&#22270;&#20687;&#30340;&#20219;&#20309;&#65288;&#26465;&#20214;&#65289;&#24178;&#39044;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;</title><link>https://arxiv.org/abs/2402.07419</link><description>&lt;p&gt;
&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#36275;&#20197;&#20174;&#20219;&#20309;&#22240;&#26524;&#25928;&#24212;&#27979;&#24230;&#20013;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Conditional Generative Models are Sufficient to Sample from Any Causal Effect Estimand
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#36890;&#36807;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#30340;&#25512;&#36827;&#35745;&#31639;&#21487;&#20197;&#35745;&#31639;&#20219;&#20309;&#21487;&#36776;&#35782;&#30340;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#29992;&#20110;&#20174;&#22270;&#20687;&#30340;&#20219;&#20309;&#65288;&#26465;&#20214;&#65289;&#24178;&#39044;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20174;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#34429;&#28982;&#23384;&#22312;&#35745;&#31639;&#22240;&#26524;&#25928;&#24212;&#30340;&#21487;&#38752;&#19988;&#23436;&#22791;&#30340;&#31639;&#27861;&#65292;&#20294;&#20854;&#20013;&#35768;&#22810;&#31639;&#27861;&#38656;&#35201;&#26174;&#24335;&#35775;&#38382;&#35266;&#27979;&#20998;&#24067;&#19978;&#30340;&#26465;&#20214;&#20284;&#28982;&#65292;&#32780;&#22312;&#39640;&#32500;&#22330;&#26223;&#20013;&#65288;&#20363;&#22914;&#22270;&#20687;&#65289;&#65292;&#20272;&#35745;&#36825;&#20123;&#20284;&#28982;&#26159;&#22256;&#38590;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#27169;&#22411;&#27169;&#25311;&#22240;&#26524;&#20851;&#31995;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29616;&#26377;&#26041;&#27861;&#20013;&#27809;&#26377;&#19968;&#20010;&#21487;&#20197;&#24212;&#29992;&#20110;&#36890;&#29992;&#22330;&#26223;&#65292;&#20363;&#22914;&#20855;&#26377;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#30340;&#22270;&#20687;&#25968;&#25454;&#30340;&#22240;&#26524;&#22270;&#65292;&#25110;&#32773;&#33719;&#24471;&#26465;&#20214;&#24178;&#39044;&#26679;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20219;&#24847;&#22240;&#26524;&#22270;&#19979;&#65292;&#36890;&#36807;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#30340;&#25512;&#36827;&#35745;&#31639;&#21487;&#20197;&#35745;&#31639;&#20219;&#20309;&#21487;&#36776;&#35782;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#22522;&#20110;&#27492;&#32467;&#26524;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#20219;&#20309;&#65288;&#26465;&#20214;&#65289;&#24178;&#39044;&#20998;&#24067;&#20013;&#37319;&#26679;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal inference from observational data has recently found many applications in machine learning. While sound and complete algorithms exist to compute causal effects, many of these algorithms require explicit access to conditional likelihoods over the observational distribution, which is difficult to estimate in the high-dimensional regime, such as with images. To alleviate this issue, researchers have approached the problem by simulating causal relations with neural models and obtained impressive results. However, none of these existing approaches can be applied to generic scenarios such as causal graphs on image data with latent confounders, or obtain conditional interventional samples. In this paper, we show that any identifiable causal effect given an arbitrary causal graph can be computed through push-forward computations of conditional generative models. Based on this result, we devise a diffusion-based approach to sample from any (conditional) interventional distribution on ima
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SemTra&#30340;&#35821;&#20041;&#25216;&#33021;&#32763;&#35793;&#22120;&#65292;&#21487;&#20197;&#22312;&#36328;&#39046;&#22495;&#29615;&#22659;&#20013;&#23454;&#29616;&#38646;-shot&#31574;&#30053;&#33258;&#36866;&#24212;&#12290;&#35813;&#32763;&#35793;&#22120;&#21033;&#29992;&#22810;&#27169;&#24577;&#27169;&#22411;&#25552;&#21462;&#25216;&#33021;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#23558;&#25552;&#21462;&#20986;&#30340;&#25216;&#33021;&#36866;&#24212;&#21040;&#30446;&#26631;&#39046;&#22495;&#65292;&#20174;&#32780;&#23454;&#29616;&#20219;&#21153;&#21644;&#25216;&#33021;&#30340;&#33258;&#36866;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.07418</link><description>&lt;p&gt;
SemTra: &#19968;&#31181;&#29992;&#20110;&#36328;&#39046;&#22495;&#38646;-shot&#31574;&#30053;&#33258;&#36866;&#24212;&#30340;&#35821;&#20041;&#25216;&#33021;&#32763;&#35793;&#22120;
&lt;/p&gt;
&lt;p&gt;
SemTra: A Semantic Skill Translator for Cross-Domain Zero-Shot Policy Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07418
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SemTra&#30340;&#35821;&#20041;&#25216;&#33021;&#32763;&#35793;&#22120;&#65292;&#21487;&#20197;&#22312;&#36328;&#39046;&#22495;&#29615;&#22659;&#20013;&#23454;&#29616;&#38646;-shot&#31574;&#30053;&#33258;&#36866;&#24212;&#12290;&#35813;&#32763;&#35793;&#22120;&#21033;&#29992;&#22810;&#27169;&#24577;&#27169;&#22411;&#25552;&#21462;&#25216;&#33021;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#23558;&#25552;&#21462;&#20986;&#30340;&#25216;&#33021;&#36866;&#24212;&#21040;&#30446;&#26631;&#39046;&#22495;&#65292;&#20174;&#32780;&#23454;&#29616;&#20219;&#21153;&#21644;&#25216;&#33021;&#30340;&#33258;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#35821;&#20041;&#25216;&#33021;&#22312;&#36328;&#39046;&#22495;&#29615;&#22659;&#20013;&#30340;&#38646;-shot&#33258;&#36866;&#24212;&#33021;&#21147;&#65292;&#20854;&#20013;&#35821;&#20041;&#21487;&#35299;&#37322;&#30340;&#19987;&#23478;&#34892;&#20026;&#27169;&#24335;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#29992;&#25143;&#36755;&#20837;&#20013;&#21487;&#20197;&#24341;&#21457;&#26032;&#30340;&#38271;&#26399;&#20219;&#21153;&#12290;&#22312;&#36825;&#20123;&#36328;&#39046;&#22495;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35821;&#20041;&#25216;&#33021;&#32763;&#35793;&#22120;&#26694;&#26550;SemTra&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#19968;&#32452;&#22810;&#27169;&#24577;&#27169;&#22411;&#20174;&#29255;&#27573;&#20013;&#25552;&#21462;&#25216;&#33021;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#23558;&#36825;&#20123;&#25552;&#21462;&#20986;&#30340;&#25216;&#33021;&#36866;&#24212;&#21040;&#30446;&#26631;&#39046;&#22495;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#20102;&#20004;&#23618;&#23618;&#27425;&#32467;&#26500;&#36827;&#34892;&#33258;&#36866;&#24212;&#65306;&#20219;&#21153;&#33258;&#36866;&#24212;&#21644;&#25216;&#33021;&#33258;&#36866;&#24212;&#12290;&#22312;&#20219;&#21153;&#33258;&#36866;&#24212;&#36807;&#31243;&#20013;&#65292;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24207;&#21015;&#21040;&#24207;&#21015;&#32763;&#35793;&#65292;&#23558;&#25552;&#21462;&#20986;&#30340;&#25216;&#33021;&#36716;&#25442;&#20026;&#36866;&#24212;&#36328;&#39046;&#22495;&#29615;&#22659;&#30340;&#35821;&#20041;&#25216;&#33021;&#24207;&#21015;&#12290;&#25216;&#33021;&#33258;&#36866;&#24212;&#20391;&#37325;&#20110;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#21270;&#23454;&#20363;&#21270;&#26469;&#20248;&#21270;&#27599;&#20010;&#35821;&#20041;&#25216;&#33021;&#65292;&#20197;&#36866;&#24212;&#30446;&#26631;&#39046;&#22495;&#30340;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work explores the zero-shot adaptation capability of semantic skills, semantically interpretable experts' behavior patterns, in cross-domain settings, where a user input in interleaved multi-modal snippets can prompt a new long-horizon task for different domains. In these cross-domain settings, we present a semantic skill translator framework SemTra which utilizes a set of multi-modal models to extract skills from the snippets, and leverages the reasoning capabilities of a pretrained language model to adapt these extracted skills to the target domain. The framework employs a two-level hierarchy for adaptation: task adaptation and skill adaptation. During task adaptation, seq-to-seq translation by the language model transforms the extracted skills into a semantic skill sequence, which is tailored to fit the cross-domain contexts. Skill adaptation focuses on optimizing each semantic skill for the target domain context, through parametric instantiations that are facilitated by langua
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29366;&#24577;&#36716;&#25442;&#36317;&#31163;&#34920;&#31034;&#23398;&#20064;&#30340;&#36741;&#21161;&#22870;&#21169;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#33258;&#21160;&#29983;&#25104;&#22870;&#21169;&#65292;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#21644;&#20943;&#23569;&#20154;&#24037;&#35774;&#35745;&#22870;&#21169;&#30340;&#24037;&#20316;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.07412</link><description>&lt;p&gt;
&#20351;&#29992;&#29366;&#24577;&#36716;&#25442;&#36317;&#31163;&#34920;&#31034;&#23398;&#20064;&#30340;&#36741;&#21161;&#22870;&#21169;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Auxiliary Reward Generation with Transition Distance Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07412
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29366;&#24577;&#36716;&#25442;&#36317;&#31163;&#34920;&#31034;&#23398;&#20064;&#30340;&#36741;&#21161;&#22870;&#21169;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#33258;&#21160;&#29983;&#25104;&#22870;&#21169;&#65292;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#21644;&#20943;&#23569;&#20154;&#24037;&#35774;&#35745;&#22870;&#21169;&#30340;&#24037;&#20316;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#22797;&#26434;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#20013;&#23637;&#29616;&#20986;&#20102;&#20854;&#20248;&#21183;&#12290;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#22870;&#21169;&#20989;&#25968;&#23545;&#23398;&#20064;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20316;&#20026;&#20219;&#21153;&#23436;&#25104;&#31243;&#24230;&#30340;&#34913;&#37327;&#26631;&#20934;&#12290;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#22870;&#21169;&#24448;&#24448;&#26159;&#30001;&#20154;&#24037;&#35774;&#35745;&#30340;&#65292;&#38656;&#35201;&#36153;&#26102;&#36153;&#21147;&#30340;&#35843;&#25972;&#65292;&#24182;&#19988;&#23481;&#26131;&#21463;&#21040;&#20154;&#31867;&#35748;&#30693;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#23454;&#29616;&#33258;&#21160;&#30340;&#36741;&#21161;&#22870;&#21169;&#29983;&#25104;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#34913;&#37327;&#29366;&#24577;&#20043;&#38388;&#30340;&#8220;&#36716;&#25442;&#36317;&#31163;&#8221;&#12290;&#22312;&#36825;&#20123;&#34920;&#31034;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26080;&#38656;&#20154;&#31867;&#30693;&#35782;&#30340;&#36741;&#21161;&#22870;&#21169;&#29983;&#25104;&#25216;&#26415;&#65292;&#29992;&#20110;&#21333;&#20219;&#21153;&#21644;&#25216;&#33021;&#38142;&#22330;&#26223;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#24191;&#27867;&#30340;&#25805;&#20316;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27979;&#37327;&#29366;&#24577;&#20043;&#38388;&#30340;&#36716;&#25442;&#36317;&#31163;&#20197;&#21450;&#36741;&#21161;&#22870;&#21169;&#24341;&#36215;&#30340;&#25913;&#36827;&#26377;&#25928;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has shown its strength in challenging sequential decision-making problems. The reward function in RL is crucial to the learning performance, as it serves as a measure of the task completion degree. In real-world problems, the rewards are predominantly human-designed, which requires laborious tuning, and is easily affected by human cognitive biases. To achieve automatic auxiliary reward generation, we propose a novel representation learning approach that can measure the ``transition distance'' between states. Building upon these representations, we introduce an auxiliary reward generation technique for both single-task and skill-chaining scenarios without the need for human knowledge. The proposed approach is evaluated in a wide range of manipulation tasks. The experiment results demonstrate the effectiveness of measuring the transition distance between states and the induced improvement by auxiliary rewards, which not only promotes better learning efficiency
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;Webshell&#36867;&#36920;&#26679;&#26412;&#30340;&#28151;&#21512;&#25552;&#31034;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#30740;&#31350;&#20013;&#24369;Webshell&#26679;&#26412;&#36867;&#36920;&#33021;&#21147;&#21644;&#32570;&#20047;&#22797;&#26434;&#24694;&#24847;&#29305;&#24449;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.07408</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#23569;&#25968;&#26679;&#26412;&#29983;&#25104;&#32773;&#65306;&#25552;&#20986;&#28151;&#21512;&#25552;&#31034;&#31639;&#27861;&#20197;&#29983;&#25104;Webshell&#36867;&#36920;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Few-shot Generators: Proposing Hybrid Prompt Algorithm To Generate Webshell Escape Samples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07408
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;Webshell&#36867;&#36920;&#26679;&#26412;&#30340;&#28151;&#21512;&#25552;&#31034;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#30740;&#31350;&#20013;&#24369;Webshell&#26679;&#26412;&#36867;&#36920;&#33021;&#21147;&#21644;&#32570;&#20047;&#22797;&#26434;&#24694;&#24847;&#29305;&#24449;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39057;&#32321;&#30340;&#32593;&#32476;&#25915;&#20987;&#20351;&#24471;Webshell&#25915;&#20987;&#21644;&#38450;&#24481;&#36880;&#28176;&#25104;&#20026;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#30340;&#30740;&#31350;&#28909;&#28857;&#12290;&#28982;&#32780;&#65292;&#20844;&#24320;&#21487;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#32570;&#20047;&#20197;&#21450;&#23545;Webshell&#36867;&#36920;&#26679;&#26412;&#29983;&#25104;&#36807;&#31243;&#36807;&#24230;&#20381;&#36182;&#25163;&#21160;&#23450;&#20041;&#35268;&#21017;&#38480;&#21046;&#20102;&#19982;Webshell&#36867;&#36920;&#26679;&#26412;&#29983;&#25104;&#31574;&#30053;&#21644;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;Webshell&#26816;&#27979;&#31639;&#27861;&#30456;&#20851;&#30740;&#31350;&#30340;&#36827;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#24369;Webshell&#26679;&#26412;&#36867;&#36920;&#33021;&#21147;&#30340;&#19981;&#36275;&#12289;&#32570;&#20047;&#20855;&#26377;&#22797;&#26434;&#24694;&#24847;&#29305;&#24449;&#30340;Webshell&#25968;&#25454;&#38598;&#20197;&#21450;&#25512;&#21160;Webshell&#26816;&#27979;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24110;&#21161;&#29983;&#25104;Webshell&#36867;&#36920;&#26679;&#26412;&#30340;&#28151;&#21512;&#25552;&#31034;&#31639;&#27861;&#12290;&#20316;&#20026;&#19987;&#38376;&#29992;&#20110;Webshell&#26679;&#26412;&#29983;&#25104;&#30340;&#25552;&#31034;&#31639;&#27861;&#65292;&#28151;&#21512;&#25552;&#31034;&#31639;&#27861;&#19981;&#20165;&#32467;&#21512;&#20102;&#21508;&#31181;&#25552;&#31034;&#24605;&#36335;&#65292;&#21253;&#25324;&#24605;&#32500;&#38142;&#21644;&#24605;&#32500;&#26641;&#65292;&#36824;&#34701;&#21512;&#20102;&#21508;&#31181;&#32452;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
The frequent occurrence of cyber-attacks has made webshell attacks and defense gradually become a research hotspot in the field of network security. However, the lack of publicly available benchmark datasets and the over-reliance on manually defined rules for webshell escape sample generation have slowed down the progress of research related to webshell escape sample generation strategies and artificial intelligence-based webshell detection algorithms. To address the drawbacks of weak webshell sample escape capabilities, the lack of webshell datasets with complex malicious features, and to promote the development of webshell detection technology, we propose the Hybrid Prompt algorithm for webshell escape sample generation with the help of large language models. As a prompt algorithm specifically developed for webshell sample generation, the Hybrid Prompt algorithm not only combines various prompt ideas including Chain of Thought, Tree of Thought, but also incorporates various component
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;&#65292;&#23558;&#23618;&#27425;&#20998;&#26512;&#27861;&#21644;GPT-4&#38598;&#25104;&#65292;&#21033;&#29992;GPT-4&#33258;&#20027;&#26234;&#33021;&#20307;&#33258;&#21160;&#21270;&#20915;&#31574;&#36807;&#31243;&#65292;&#20026;&#32593;&#32476;&#23433;&#20840;&#22810;&#20934;&#21017;&#20915;&#31574;&#21046;&#23450;&#24102;&#26469;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2402.07404</link><description>&lt;p&gt;
&#29992;&#20154;&#24037;&#26234;&#33021;&#22686;&#24378;&#22810;&#20934;&#21017;&#20915;&#31574;&#20998;&#26512;&#65306;&#23558;&#23618;&#27425;&#20998;&#26512;&#27861;&#21644;GPT-4&#38598;&#25104;&#20026;&#33258;&#21160;&#21270;&#20915;&#31574;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Enhancing Multi-Criteria Decision Analysis with AI: Integrating Analytic Hierarchy Process and GPT-4 for Automated Decision Support
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07404
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;&#65292;&#23558;&#23618;&#27425;&#20998;&#26512;&#27861;&#21644;GPT-4&#38598;&#25104;&#65292;&#21033;&#29992;GPT-4&#33258;&#20027;&#26234;&#33021;&#20307;&#33258;&#21160;&#21270;&#20915;&#31574;&#36807;&#31243;&#65292;&#20026;&#32593;&#32476;&#23433;&#20840;&#22810;&#20934;&#21017;&#20915;&#31574;&#21046;&#23450;&#24102;&#26469;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;&#65292;&#23558;&#23618;&#27425;&#20998;&#26512;&#27861;&#65288;AHP&#65289;&#21644;GPT-4&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#20026;&#32593;&#32476;&#23433;&#20840;&#22810;&#20934;&#21017;&#20915;&#31574;&#21046;&#23450;&#24102;&#26469;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;GPT-4&#33258;&#20027;&#26234;&#33021;&#20307;&#20316;&#20026;&#34394;&#25311;&#19987;&#23478;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#33258;&#21160;&#21270;&#20915;&#31574;&#36807;&#31243;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#21487;&#38752;&#24615;&#12290;&#36825;&#31181;&#26032;&#26041;&#27861;&#20391;&#37325;&#20110;&#21033;&#29992;LLM&#36827;&#34892;&#22797;&#26434;&#20915;&#31574;&#20998;&#26512;&#65292;&#31361;&#26174;&#20256;&#32479;&#20915;&#31574;&#27169;&#22411;&#21644;&#23574;&#31471;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;&#25105;&#20204;&#21019;&#26032;&#30340;&#26041;&#27861;&#35770;&#23637;&#31034;&#20102;&#22312;&#22797;&#26434;&#20915;&#31574;&#22330;&#26223;&#20013;&#20351;&#29992;AI&#39537;&#21160;&#26234;&#33021;&#20307;&#30340;&#37325;&#22823;&#36827;&#23637;&#65292;&#20984;&#26174;&#20102;AI&#22312;&#25112;&#30053;&#32593;&#32476;&#23433;&#20840;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#23558;AHP&#21644;LLM&#32467;&#21512;&#30340;&#21464;&#38761;&#28508;&#21147;&#65292;&#20026;&#32593;&#32476;&#23433;&#20840;&#21450;&#20854;&#20182;&#39046;&#22495;&#30340;&#26234;&#33021;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#24314;&#31435;&#20102;&#26032;&#30340;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our study presents a new framework that incorporates the Analytic Hierarchy Process (AHP) and Generative Pre-trained Transformer 4 (GPT-4) large language model (LLM), bringing novel approaches to cybersecurity Multiple-criteria Decision Making (MCDA). By utilizing the capabilities of GPT-4 autonomous agents as virtual experts, we automate the decision-making process, enhancing both efficiency and reliability. This new approach focuses on leveraging LLMs for sophisticated decision analysis, highlighting the synergy between traditional decision-making models and cutting-edge AI technologies. Our innovative methodology demonstrates significant advancements in using AI-driven agents for complex decision-making scenarios, highlighting the importance of AI in strategic cybersecurity applications. The findings reveal the transformative potential of combining AHP and LLMs, establishing a new paradigm for intelligent decision support systems in cybersecurity and beyond.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;BDIQA&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25506;&#32034;VideoQA&#27169;&#22411;&#22312;&#24515;&#26234;&#29702;&#35770;&#65288;ToM&#65289;&#32972;&#26223;&#19979;&#30340;&#35748;&#30693;&#25512;&#29702;&#33021;&#21147;&#12290;&#35813;&#25968;&#25454;&#38598;&#21463;&#21040;&#20799;&#31461;ToM&#35748;&#30693;&#21457;&#23637;&#30340;&#21551;&#21457;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#20013;&#26426;&#22120;ToM&#30340;&#19981;&#36275;&#12290;&#23427;&#25552;&#20379;&#20102;&#20004;&#20010;&#38590;&#24230;&#32423;&#21035;&#30340;&#20219;&#21153;&#65292;&#35780;&#20272;&#20102;&#31616;&#21333;&#21644;&#22797;&#26434;&#22330;&#26223;&#20013;&#30340;&#20449;&#24565;&#12289;&#27442;&#26395;&#21644;&#24847;&#22270;&#65288;BDI&#65289;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.07402</link><description>&lt;p&gt;
BDIQA&#65306;&#19968;&#20010;&#26032;&#30340;&#35270;&#39057;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#24515;&#26234;&#29702;&#35770;&#25506;&#32034;&#35748;&#30693;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
BDIQA: A New Dataset for Video Question Answering to Explore Cognitive Reasoning through Theory of Mind
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;BDIQA&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25506;&#32034;VideoQA&#27169;&#22411;&#22312;&#24515;&#26234;&#29702;&#35770;&#65288;ToM&#65289;&#32972;&#26223;&#19979;&#30340;&#35748;&#30693;&#25512;&#29702;&#33021;&#21147;&#12290;&#35813;&#25968;&#25454;&#38598;&#21463;&#21040;&#20799;&#31461;ToM&#35748;&#30693;&#21457;&#23637;&#30340;&#21551;&#21457;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#20013;&#26426;&#22120;ToM&#30340;&#19981;&#36275;&#12290;&#23427;&#25552;&#20379;&#20102;&#20004;&#20010;&#38590;&#24230;&#32423;&#21035;&#30340;&#20219;&#21153;&#65292;&#35780;&#20272;&#20102;&#31616;&#21333;&#21644;&#22797;&#26434;&#22330;&#26223;&#20013;&#30340;&#20449;&#24565;&#12289;&#27442;&#26395;&#21644;&#24847;&#22270;&#65288;BDI&#65289;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#35748;&#30693;&#26234;&#33021;&#30340;&#22522;&#30784;&#32452;&#25104;&#37096;&#20998;&#65292;&#24515;&#26234;&#29702;&#35770;&#65288;ToM&#65289;&#21487;&#20197;&#20351;&#20154;&#24037;&#26234;&#33021;&#26356;&#21152;&#25509;&#36817;&#20154;&#31867;&#24605;&#32500;&#36807;&#31243;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#19982;&#20154;&#31867;&#30340;&#20114;&#21160;&#21644;&#21327;&#20316;&#33021;&#21147;&#12290;&#29305;&#21035;&#26159;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#65292;&#23427;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#23545;&#35270;&#39057;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#35270;&#39057;&#38382;&#31572;&#65288;VideoQA&#65289;&#25968;&#25454;&#38598;&#20027;&#35201;&#38598;&#20013;&#22312;&#30740;&#31350;&#20107;&#20214;&#20013;&#30340;&#22240;&#26524;&#25512;&#29702;&#65292;&#24456;&#23569;&#28041;&#21450;&#30495;&#27491;&#34701;&#20837;&#20154;&#31867;ToM&#30340;&#20869;&#23481;&#12290;&#22240;&#27492;&#65292;&#22312;VideoQA&#39046;&#22495;&#20013;&#32570;&#20047;&#23545;ToM&#25512;&#29702;&#20219;&#21153;&#30340;&#21457;&#23637;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;BDIQA&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20934;&#65292;&#29992;&#20110;&#25506;&#32034;VideoQA&#27169;&#22411;&#22312;ToM&#32972;&#26223;&#19979;&#30340;&#35748;&#30693;&#25512;&#29702;&#33021;&#21147;&#12290;BDIQA&#21463;&#21040;&#20799;&#31461;ToM&#35748;&#30693;&#21457;&#23637;&#30340;&#21551;&#21457;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#20013;&#26426;&#22120;ToM&#30340;&#19981;&#36275;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#25552;&#20379;&#20102;&#20004;&#20010;&#38590;&#24230;&#32423;&#21035;&#30340;&#20219;&#21153;&#65292;&#35780;&#20272;&#20102;&#31616;&#21333;&#21644;&#22797;&#26434;&#22330;&#26223;&#20013;&#30340;&#20449;&#24565;&#12289;&#27442;&#26395;&#21644;&#24847;&#22270;&#65288;BDI&#65289;&#25512;&#29702;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a foundational component of cognitive intelligence, theory of mind (ToM) can make AI more closely resemble human thought processes, thereby enhancing their interaction and collaboration with human. In particular, it can significantly improve a model's comprehension of videos in complex scenes. However, current video question answer (VideoQA) datasets focus on studying causal reasoning within events few of them genuinely incorporating human ToM. Consequently, there is a lack of development in ToM reasoning tasks within the area of VideoQA. This paper presents BDIQA, the first benchmark to explore the cognitive reasoning capabilities of VideoQA models in the context of ToM. BDIQA is inspired by the cognitive development of children's ToM and addresses the current deficiencies in machine ToM within datasets and tasks. Specifically, it offers tasks at two difficulty levels, assessing Belief, Desire and Intention (BDI) reasoning in both simple and complex scenarios. We conduct evaluation
&lt;/p&gt;</description></item><item><title>VisLingInstruct&#36890;&#36807;&#33258;&#20027;&#20248;&#21270;&#25351;&#23548;&#25991;&#26412;&#21644;&#35270;&#35273;&#29305;&#24449;&#25552;&#21462;&#27169;&#22359;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#24615;&#33021;&#65292;&#22312;TextVQA&#21644;HatefulMemes&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#20998;&#21035;&#25552;&#39640;&#20102;13.1%&#21644;9%&#12290;</title><link>https://arxiv.org/abs/2402.07398</link><description>&lt;p&gt;
VisLingInstruct: &#36890;&#36807;&#33258;&#20027;&#25351;&#23548;&#20248;&#21270;&#25552;&#21319;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
VisLingInstruct: Elevating Zero-Shot Learning in Multi-Modal Language Models with Autonomous Instruction Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07398
&lt;/p&gt;
&lt;p&gt;
VisLingInstruct&#36890;&#36807;&#33258;&#20027;&#20248;&#21270;&#25351;&#23548;&#25991;&#26412;&#21644;&#35270;&#35273;&#29305;&#24449;&#25552;&#21462;&#27169;&#22359;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#24615;&#33021;&#65292;&#22312;TextVQA&#21644;HatefulMemes&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#20998;&#21035;&#25552;&#39640;&#20102;13.1%&#21644;9%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;VisLingInstruct&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#20013;&#25512;&#36827;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#12290;&#24403;&#21069;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#25351;&#23548;&#25991;&#26412;&#30340;&#36136;&#37327;&#12290;VisLingInstruct&#36890;&#36807;&#33258;&#20027;&#35780;&#20272;&#21644;&#20248;&#21270;&#25351;&#23548;&#25991;&#26412;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#25913;&#36827;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#20013;&#35270;&#35273;&#24863;&#30693;&#21644;&#35821;&#35328;&#34920;&#36798;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;&#38500;&#20102;&#25351;&#23548;&#25991;&#26412;&#30340;&#25913;&#36827;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#20248;&#21270;&#20102;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35270;&#35273;&#29305;&#24449;&#25552;&#21462;&#27169;&#22359;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#23545;&#25991;&#26412;&#25552;&#31034;&#30340;&#21709;&#24212;&#33021;&#21147;&#12290;&#22522;&#20110;FlanT5&#21644;Vicuna&#30340;&#32508;&#21512;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;VisLingInstruct&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#35270;&#35273;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23427;&#22312;TextVQA&#21644;HatefulMemes&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#20998;&#21035;&#27604;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#25552;&#39640;&#20102;13.1%&#21644;9%&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents VisLingInstruct, a novel approach to advancing Multi-Modal Language Models (MMLMs) in zero-shot learning. Current MMLMs show impressive zero-shot abilities in multi-modal tasks, but their performance depends heavily on the quality of instructions. VisLingInstruct tackles this by autonomously evaluating and optimizing instructional texts through In-Context Learning, improving the synergy between visual perception and linguistic expression in MMLMs. Alongside this instructional advancement, we have also optimized the visual feature extraction modules in MMLMs, further augmenting their responsiveness to textual cues. Our comprehensive experiments on MMLMs, based on FlanT5 and Vicuna, show that VisLingInstruct significantly improves zero-shot performance in visual multi-modal tasks. Notably, it achieves a 13.1% and 9% increase in accuracy over the prior state-of-the-art on the TextVQA and HatefulMemes datasets.
&lt;/p&gt;</description></item><item><title>TeMPO&#26159;&#19968;&#31181;&#26102;&#20998;&#22810;&#36335;&#21160;&#24577;&#20809;&#23398;&#24352;&#37327;&#21152;&#36895;&#22120;&#65292;&#36890;&#36807;&#36328;&#23618;&#35774;&#22791;/&#30005;&#36335;/&#26550;&#26500;&#23450;&#21046;&#65292;&#24357;&#21512;&#20102;&#20809;&#23398;&#31070;&#32463;&#21152;&#36895;&#22120;&#19982;&#39640;&#24230;&#23450;&#21046;&#30340;&#30005;&#23376;&#23545;&#24212;&#22120;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#22312;&#35774;&#22791;&#32423;&#21035;&#19978;&#65292;&#35813;&#21152;&#36895;&#22120;&#37319;&#29992;&#20102;&#24037;&#21378;&#21487;&#29992;&#30340;&#12289;&#23450;&#21046;&#30340;&#20809;&#23398;&#22120;&#20214;&#65292;&#21253;&#25324;&#23454;&#39564;&#28436;&#31034;&#30340;&#24930;&#20809;&#30005;&#20809;&#35843;&#21046;&#22120;&#12289;&#20809;&#20998;&#37197;&#22120;&#21644;&#30456;&#31227;&#22120;&#65292;&#26174;&#33879;&#20943;&#23567;&#20102;&#36755;&#20837;&#32534;&#30721;&#21644;&#28857;&#20056;&#20013;&#30340;&#21344;&#22320;&#38754;&#31215;&#21644;&#21151;&#32791;&#12290;</title><link>https://arxiv.org/abs/2402.07393</link><description>&lt;p&gt;
TeMPO&#65306;&#38754;&#21521;&#36793;&#32536;AI&#30340;&#39640;&#25928;&#26102;&#20998;&#22810;&#36335;&#21160;&#24577;&#20809;&#23398;&#24352;&#37327;&#26680;&#24515;&#65292;&#20855;&#22791;&#32039;&#20945;&#22411;&#24930;&#20809;&#30005;&#20809;&#35843;&#21046;&#22120;
&lt;/p&gt;
&lt;p&gt;
TeMPO: Efficient Time-Multiplexed Dynamic Photonic Tensor Core for Edge AI with Compact Slow-Light Electro-Optic Modulator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07393
&lt;/p&gt;
&lt;p&gt;
TeMPO&#26159;&#19968;&#31181;&#26102;&#20998;&#22810;&#36335;&#21160;&#24577;&#20809;&#23398;&#24352;&#37327;&#21152;&#36895;&#22120;&#65292;&#36890;&#36807;&#36328;&#23618;&#35774;&#22791;/&#30005;&#36335;/&#26550;&#26500;&#23450;&#21046;&#65292;&#24357;&#21512;&#20102;&#20809;&#23398;&#31070;&#32463;&#21152;&#36895;&#22120;&#19982;&#39640;&#24230;&#23450;&#21046;&#30340;&#30005;&#23376;&#23545;&#24212;&#22120;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#22312;&#35774;&#22791;&#32423;&#21035;&#19978;&#65292;&#35813;&#21152;&#36895;&#22120;&#37319;&#29992;&#20102;&#24037;&#21378;&#21487;&#29992;&#30340;&#12289;&#23450;&#21046;&#30340;&#20809;&#23398;&#22120;&#20214;&#65292;&#21253;&#25324;&#23454;&#39564;&#28436;&#31034;&#30340;&#24930;&#20809;&#30005;&#20809;&#35843;&#21046;&#22120;&#12289;&#20809;&#20998;&#37197;&#22120;&#21644;&#30456;&#31227;&#22120;&#65292;&#26174;&#33879;&#20943;&#23567;&#20102;&#36755;&#20837;&#32534;&#30721;&#21644;&#28857;&#20056;&#20013;&#30340;&#21344;&#22320;&#38754;&#31215;&#21644;&#21151;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20809;&#23398;&#30340;&#35745;&#31639;&#36895;&#24230;&#21644;&#25928;&#29575;&#20248;&#36234;&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#24179;&#21488;&#19978;&#65292;&#22522;&#20110;&#30005;&#20809;&#35745;&#31639;&#31995;&#32479;&#20026;&#33021;&#25928;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21152;&#36895;&#20219;&#21153;&#25552;&#20379;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#23454;&#26102;&#12289;&#20302;&#33021;&#37327;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#25512;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22522;&#20110;&#24037;&#21378;&#21487;&#29992;&#35774;&#22791;&#21644;&#20256;&#32479;&#31995;&#32479;&#26550;&#26500;&#30340;&#20809;&#23398;&#31070;&#32463;&#21152;&#36895;&#22120;&#19982;&#39640;&#24230;&#23450;&#21046;&#30340;&#30005;&#23376;&#23545;&#24212;&#22120;&#30456;&#27604;&#65292;&#20173;&#28982;&#23384;&#22312;&#24615;&#33021;&#24046;&#36317;&#12290;&#20026;&#20102;&#24357;&#21512;&#30001;&#20110;&#39046;&#22495;&#19987;&#19994;&#21270;&#19981;&#36275;&#32780;&#23548;&#33268;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TeMPO&#30340;&#26102;&#20998;&#22810;&#36335;&#21160;&#24577;&#20809;&#23398;&#24352;&#37327;&#21152;&#36895;&#22120;&#65292;&#36890;&#36807;&#36328;&#23618;&#35774;&#22791;/&#30005;&#36335;/&#26550;&#26500;&#23450;&#21046;&#23454;&#29616;&#12290;&#22312;&#35774;&#22791;&#32423;&#21035;&#19978;&#65292;&#25105;&#20204;&#21576;&#29616;&#20102;&#24037;&#21378;&#21487;&#29992;&#30340;&#12289;&#23450;&#21046;&#30340;&#20809;&#23398;&#22120;&#20214;&#65292;&#21253;&#25324;&#23454;&#39564;&#28436;&#31034;&#30340;&#24930;&#20809;&#30005;&#20809;&#35843;&#21046;&#22120;&#12289;&#20809;&#20998;&#37197;&#22120;&#21644;&#30456;&#31227;&#22120;&#65292;&#26174;&#33879;&#20943;&#23567;&#20102;&#36755;&#20837;&#32534;&#30721;&#21644;&#28857;&#20056;&#20013;&#30340;&#21344;&#22320;&#38754;&#31215;&#21644;&#21151;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electronic-photonic computing systems offer immense potential in energy-efficient artificial intelligence (AI) acceleration tasks due to the superior computing speed and efficiency of optics, especially for real-time, low-energy deep neural network (DNN) inference tasks on resource-restricted edge platforms. However, current optical neural accelerators based on foundry-available devices and conventional system architecture still encounter a performance gap compared to highly customized electronic counterparts. To bridge the performance gap due to lack of domain specialization, we present a time-multiplexed dynamic photonic tensor accelerator, dubbed TeMPO, with cross-layer device/circuit/architecture customization. At the device level, we present foundry-compatible, customized photonic devices, including a slow-light electro-optic modulator with experimental demonstration, optical splitters, and phase shifters that significantly reduce the footprint and power in input encoding and dot-
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22312;&#24863;&#30693;&#33021;&#21147;&#19978;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23545;&#20110;&#23567;&#22411;&#35270;&#35273;&#23545;&#35937;&#30340;&#38382;&#39064;&#65292;MLLMs&#30340;&#22238;&#31572;&#33021;&#21147;&#26222;&#36941;&#23384;&#22312;&#38480;&#21046;&#12290;&#36890;&#36807;&#25511;&#21046;&#24178;&#39044;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#29289;&#20307;&#36136;&#37327;&#12289;&#22823;&#23567;&#21644;&#20301;&#32622;&#37117;&#23545;MLLMs&#30340;&#24863;&#30693;&#33021;&#21147;&#26377;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.07384</link><description>&lt;p&gt;
&#25506;&#32034;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24863;&#30693;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Exploring Perceptual Limitation of Multimodal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07384
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22312;&#24863;&#30693;&#33021;&#21147;&#19978;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23545;&#20110;&#23567;&#22411;&#35270;&#35273;&#23545;&#35937;&#30340;&#38382;&#39064;&#65292;MLLMs&#30340;&#22238;&#31572;&#33021;&#21147;&#26222;&#36941;&#23384;&#22312;&#38480;&#21046;&#12290;&#36890;&#36807;&#25511;&#21046;&#24178;&#39044;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#29289;&#20307;&#36136;&#37327;&#12289;&#22823;&#23567;&#21644;&#20301;&#32622;&#37117;&#23545;MLLMs&#30340;&#24863;&#30693;&#33021;&#21147;&#26377;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#26368;&#36817;&#23637;&#31034;&#20102;&#22312;&#22238;&#31572;&#35270;&#35273;&#38382;&#39064;&#26041;&#38754;&#24341;&#20154;&#27880;&#30446;&#30340;&#24863;&#30693;&#33021;&#21147;&#65292;&#28982;&#32780;&#65292;&#23545;&#23427;&#20204;&#24863;&#30693;&#33021;&#21147;&#30340;&#38480;&#21046;&#30693;&#20043;&#29978;&#23569;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#65292;&#34429;&#28982;&#25552;&#20379;&#20102;MLLMs&#23545;&#29289;&#20307;&#22823;&#23567;&#25935;&#24863;&#30340;&#20010;&#21035;&#35777;&#25454;&#65292;&#20294;&#36825;&#19968;&#29616;&#35937;&#21450;&#20854;&#28508;&#22312;&#21407;&#22240;&#23578;&#26410;&#24471;&#21040;&#20840;&#38754;&#25506;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23450;&#37327;&#30740;&#31350;&#20102;&#20960;&#20010;&#26368;&#20808;&#36827;&#30340;MLLMs&#23545;&#23567;&#22411;&#35270;&#35273;&#23545;&#35937;&#30340;&#24863;&#30693;&#65292;&#24182;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#22238;&#31572;&#19982;&#23567;&#22411;&#23545;&#35937;&#26377;&#20851;&#30340;&#38382;&#39064;&#26102;&#26222;&#36941;&#23384;&#22312;&#30340;&#38480;&#21046;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22235;&#20010;&#29420;&#31435;&#22240;&#32032;&#65292;&#23427;&#20204;&#21487;&#33021;&#23548;&#33268;&#36825;&#31181;&#38480;&#21046;&#65292;&#21253;&#25324;&#29289;&#20307;&#36136;&#37327;&#12289;&#22823;&#23567;&#12289;&#24178;&#25200;&#29289;&#21644;&#20301;&#32622;&#65292;&#24182;&#36827;&#34892;&#20102;&#25511;&#21046;&#24615;&#24178;&#39044;&#30740;&#31350;&#65292;&#20197;&#35780;&#20272;&#27599;&#20010;&#22240;&#32032;&#23545;MLLMs&#24863;&#30693;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#20302;&#36136;&#37327;&#30340;&#29289;&#20307;&#21644;&#26356;&#23567;&#30340;&#29289;&#20307;&#22823;&#23567;&#37117;&#21487;&#20197;&#29420;&#31435;&#38477;&#20302;MLLMs&#22238;&#31572;&#35270;&#35273;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#26356;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#29289;&#20307;&#20301;&#32622;&#23545;MLLMs&#30340;&#24863;&#30693;&#33021;&#21147;&#20063;&#26377;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Large Language Models (MLLMs) have recently shown remarkable perceptual capability in answering visual questions, however, little is known about the limits of their perception. In particular, while prior works have provided anecdotal evidence of MLLMs' sensitivity to object size, this phenomenon and its underlying causes have not been explored comprehensively. In this work, we quantitatively study the perception of small visual objects in several state-of-the-art MLLMs and reveal a pervasive limitation in answering questions about small objects in images. Next, we identify four independent factors that can contribute to this limitation -- object quality, size, distractors, and location -- and conduct controlled intervention studies to measure the effect of each factor on MLLMs' perception. In particular, we find that lower object quality and smaller object size can both independently reduce MLLMs' ability to answer visual questions. More surprisingly, we find that the locati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;SelfSwapper&#65292;&#19968;&#31181;&#36890;&#36807; Shape Agnostic Masked AutoEncoder (SAMAE) &#33258;&#30417;&#30563;&#26041;&#26696;&#26469;&#25552;&#21319;&#20154;&#33080;&#20132;&#25442;&#27169;&#22411;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#32469;&#36807;&#20256;&#32479;&#30340;&#35757;&#32451;&#26041;&#26696;&#65292;&#24341;&#20837;&#28165;&#26224;&#30340;&#30495;&#23454;&#25968;&#25454;&#65292;&#20197;&#21450;&#21033;&#29992;&#36974;&#32617;&#21644;&#23398;&#21040;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#25104;&#21151;&#35299;&#20915;&#20102;&#36523;&#20221;&#27844;&#28431;&#21644;&#24418;&#29366;&#19981;&#23545;&#40784;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.07370</link><description>&lt;p&gt;
SelfSwapper: &#36890;&#36807;&#24418;&#29366;&#26080;&#20851;&#30340;&#36974;&#32617;&#33258;&#32534;&#30721;&#22120;&#23454;&#29616;&#33258;&#30417;&#30563;&#20154;&#33080;&#20132;&#25442;
&lt;/p&gt;
&lt;p&gt;
SelfSwapper: Self-Supervised Face Swapping via Shape Agnostic Masked AutoEncoder
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SelfSwapper&#65292;&#19968;&#31181;&#36890;&#36807; Shape Agnostic Masked AutoEncoder (SAMAE) &#33258;&#30417;&#30563;&#26041;&#26696;&#26469;&#25552;&#21319;&#20154;&#33080;&#20132;&#25442;&#27169;&#22411;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#32469;&#36807;&#20256;&#32479;&#30340;&#35757;&#32451;&#26041;&#26696;&#65292;&#24341;&#20837;&#28165;&#26224;&#30340;&#30495;&#23454;&#25968;&#25454;&#65292;&#20197;&#21450;&#21033;&#29992;&#36974;&#32617;&#21644;&#23398;&#21040;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#25104;&#21151;&#35299;&#20915;&#20102;&#36523;&#20221;&#27844;&#28431;&#21644;&#24418;&#29366;&#19981;&#23545;&#40784;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#33080;&#20132;&#25442;&#22240;&#20854;&#22810;&#26679;&#21270;&#30340;&#24212;&#29992;&#32780;&#21463;&#21040;&#26497;&#22823;&#20851;&#27880;&#12290;&#22823;&#22810;&#25968;&#20043;&#21069;&#30340;&#20154;&#33080;&#20132;&#25442;&#26041;&#27861;&#20381;&#36182;&#20110;&#36343;&#36343;&#26495;&#24335;&#35757;&#32451;&#26041;&#26696;&#65292;&#36890;&#24120;&#23548;&#33268;&#27169;&#22411;&#35757;&#32451;&#30340;&#19981;&#31283;&#23450;&#24615;&#24182;&#20135;&#29983;&#28151;&#21512;&#36523;&#20221;&#30340;&#19981;&#26399;&#26395;&#26679;&#26412;&#65292;&#21407;&#22240;&#26159;&#30446;&#26631;&#36523;&#20221;&#27844;&#28431;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Shape Agnostic Masked AutoEncoder (SAMAE) &#35757;&#32451;&#26041;&#26696;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#26088;&#22312;&#25913;&#36827;&#20154;&#33080;&#20132;&#25442;&#27169;&#22411;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#35757;&#32451;&#26041;&#26696;&#36890;&#36807;&#32469;&#36807;&#20256;&#32479;&#30340;&#36343;&#36343;&#26495;&#28216;&#25103;&#24182;&#36890;&#36807;&#33258;&#37325;&#24314;&#35757;&#32451;&#26426;&#21046;&#24341;&#20837;&#28165;&#26224;&#30340;&#30495;&#23454;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#35757;&#32451;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#23427;&#36890;&#36807;&#36974;&#32617;&#36755;&#20837;&#22270;&#20687;&#30340;&#38754;&#37096;&#21306;&#22495;&#21644;&#21033;&#29992;&#23398;&#21040;&#30340;&#36523;&#20221;&#21644;&#38750;&#36523;&#20221;&#29305;&#24449;&#26469;&#26377;&#25928;&#20943;&#36731;&#36523;&#20221;&#27844;&#28431;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807; perforation confusion &#21644;&#38543;&#26426;&#32593;&#26684;&#32553;&#25918;&#31561;&#26032;&#25216;&#26415;&#26469;&#35299;&#20915;&#24418;&#29366;&#19981;&#23545;&#40784;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Face swapping has gained significant attention for its varied applications. The majority of previous face swapping approaches have relied on the seesaw game training scheme, which often leads to the instability of the model training and results in undesired samples with blended identities due to the target identity leakage problem. This paper introduces the Shape Agnostic Masked AutoEncoder (SAMAE) training scheme, a novel self-supervised approach designed to enhance face swapping model training. Our training scheme addresses the limitations of traditional training methods by circumventing the conventional seesaw game and introducing clear ground truth through its self-reconstruction training regime. It effectively mitigates identity leakage by masking facial regions of the input images and utilizing learned disentangled identity and non-identity features. Additionally, we tackle the shape misalignment problem with new techniques including perforation confusion and random mesh scaling,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;2016&#24180;&#21644;2020&#24180;&#30340;&#36873;&#20030;&#25968;&#25454;&#65292;&#35780;&#20272;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#32452;&#20195;&#34920;&#27169;&#22411;&#22312;&#27867;&#21270;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;&#20351;&#29992;&#23454;&#35777;&#25968;&#25454;&#36827;&#34892;&#26465;&#20214;&#35774;&#23450;&#21487;&#20197;&#25552;&#39640;&#25972;&#20307;&#24615;&#33021;&#65292;&#20294;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#30410;&#22788;&#22312;&#19981;&#21516;&#20154;&#21475;&#23376;&#32676;&#32452;&#20043;&#38388;&#24046;&#24322;&#24456;&#22823;&#65292;&#36825;&#23545;&#23454;&#26045;&#20998;&#32452;&#20195;&#34920;&#27169;&#22411;&#30340;&#20174;&#19994;&#20154;&#21592;&#21644;&#20915;&#31574;&#32773;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.07368</link><description>&lt;p&gt;
&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#35780;&#20272;&#20998;&#32452;&#20195;&#34920;&#24314;&#27169;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Assessing Generalization for Subpopulation Representative Modeling via In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;2016&#24180;&#21644;2020&#24180;&#30340;&#36873;&#20030;&#25968;&#25454;&#65292;&#35780;&#20272;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#32452;&#20195;&#34920;&#27169;&#22411;&#22312;&#27867;&#21270;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;&#20351;&#29992;&#23454;&#35777;&#25968;&#25454;&#36827;&#34892;&#26465;&#20214;&#35774;&#23450;&#21487;&#20197;&#25552;&#39640;&#25972;&#20307;&#24615;&#33021;&#65292;&#20294;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#30410;&#22788;&#22312;&#19981;&#21516;&#20154;&#21475;&#23376;&#32676;&#32452;&#20043;&#38388;&#24046;&#24322;&#24456;&#22823;&#65292;&#36825;&#23545;&#23454;&#26045;&#20998;&#32452;&#20195;&#34920;&#27169;&#22411;&#30340;&#20174;&#19994;&#20154;&#21592;&#21644;&#20915;&#31574;&#32773;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;2016&#24180;&#21644;2020&#24180;&#32654;&#22269;&#20840;&#22269;&#36873;&#20030;&#30740;&#31350;&#30340;&#25968;&#25454;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#35780;&#20272;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20998;&#32452;&#20195;&#34920;&#27169;&#22411;&#65288;SRMs&#65289;&#20174;&#23454;&#35777;&#25968;&#25454;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#19981;&#21516;&#21709;&#24212;&#21464;&#37327;&#21644;&#20154;&#21475;&#23376;&#32676;&#32452;&#20043;&#38388;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#23613;&#31649;&#20351;&#29992;&#23454;&#35777;&#25968;&#25454;&#36827;&#34892;&#26465;&#20214;&#35774;&#23450;&#21487;&#20197;&#25552;&#39640;&#25972;&#20307;&#24615;&#33021;&#65292;&#20294;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#30410;&#22788;&#22312;&#19981;&#21516;&#20154;&#21475;&#23376;&#32676;&#32452;&#20043;&#38388;&#24046;&#24322;&#24456;&#22823;&#65292;&#26377;&#26102;&#23545;&#26576;&#20010;&#20154;&#21475;&#23376;&#32676;&#32452;&#30340;&#24615;&#33021;&#20135;&#29983;&#20102;&#36127;&#38754;&#24433;&#21709;&#65292;&#20294;&#23545;&#20854;&#20182;&#20154;&#21475;&#23376;&#32676;&#32452;&#30340;&#24615;&#33021;&#20135;&#29983;&#20102;&#31215;&#26497;&#24433;&#21709;&#12290;&#19978;&#19979;&#25991;&#23398;&#20064;&#23545;SRM&#30340;&#19981;&#20844;&#24179;&#30410;&#22788;&#20026;&#23454;&#26045;SRM&#30340;&#20174;&#19994;&#20154;&#21592;&#21644;&#20381;&#36182;&#20110;&#20854;&#30340;&#20915;&#31574;&#32773;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#31361;&#20986;&#20102;&#23545;&#26469;&#33258;&#19981;&#21516;&#20154;&#21475;&#23376;&#32676;&#32452;&#30340;&#32454;&#31890;&#24230;&#22522;&#20934;&#30340;&#38656;&#27714;&#65292;&#36825;&#20123;&#22522;&#20934;&#19981;&#20165;&#27979;&#35797;&#24544;&#23454;&#24230;&#65292;&#36824;&#27979;&#35797;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study evaluates the ability of Large Language Model (LLM)-based Subpopulation Representative Models (SRMs) to generalize from empirical data, utilizing in-context learning with data from the 2016 and 2020 American National Election Studies. We explore generalization across response variables and demographic subgroups. While conditioning with empirical data improves performance on the whole, the benefit of in-context learning varies considerably across demographics, sometimes hurting performance for one demographic while helping performance for others. The inequitable benefits of in-context learning for SRM present a challenge for practitioners implementing SRMs, and for decision-makers who might come to rely on them. Our work highlights a need for fine-grained benchmarks captured from diverse subpopulations that test not only fidelity but generalization.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28040;&#24687;&#20256;&#36882;&#30340;&#36125;&#21494;&#26031;&#32852;&#37030;&#23398;&#20064;&#65288;BFL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#26399;&#26395;&#26368;&#22823;&#21270;&#21644;Turbo Deep&#36817;&#20284;&#28040;&#24687;&#20256;&#36882;&#65288;TDAMP&#65289;&#23454;&#29616;&#20998;&#24067;&#24335;&#23398;&#20064;&#21644;&#21387;&#32553;&#12290;&#35813;&#26694;&#26550;&#22312;&#22788;&#29702;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#30340;&#32570;&#28857;&#19978;&#26377;&#30528;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.07366</link><description>&lt;p&gt;
&#36890;&#36807;&#26399;&#26395;&#26368;&#22823;&#21270;&#21644;Turbo Deep&#36817;&#20284;&#28040;&#24687;&#20256;&#36882;&#30340;&#36125;&#21494;&#26031;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bayesian Federated Learning Via Expectation Maximization and Turbo Deep Approximate Message Passing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28040;&#24687;&#20256;&#36882;&#30340;&#36125;&#21494;&#26031;&#32852;&#37030;&#23398;&#20064;&#65288;BFL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#26399;&#26395;&#26368;&#22823;&#21270;&#21644;Turbo Deep&#36817;&#20284;&#28040;&#24687;&#20256;&#36882;&#65288;TDAMP&#65289;&#23454;&#29616;&#20998;&#24067;&#24335;&#23398;&#20064;&#21644;&#21387;&#32553;&#12290;&#35813;&#26694;&#26550;&#22312;&#22788;&#29702;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#30340;&#32570;&#28857;&#19978;&#26377;&#30528;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#33539; paradigm&#65292;&#22312;&#36825;&#31181;&#33539;&#24335;&#20013;&#65292;&#23458;&#25143;&#31471;&#25317;&#26377;&#20998;&#25955;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#32780;&#20013;&#22830;&#26381;&#21153;&#22120;&#21017;&#36127;&#36131;&#32858;&#21512;&#21644;&#35843;&#24230;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#28041;&#21450;&#23458;&#25143;&#31471;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#26469;&#35757;&#32451;&#20182;&#20204;&#30340;&#26412;&#22320;&#27169;&#22411;&#65292;&#20294;&#36825;&#24102;&#26469;&#20102;&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#21644;&#23481;&#26131;&#38519;&#20837;&#27425;&#20248;&#35299;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28040;&#24687;&#20256;&#36882;&#30340;&#36125;&#21494;&#26031;&#32852;&#37030;&#23398;&#20064;&#65288;BFL&#65289;&#26694;&#26550;&#26469;&#36991;&#20813;&#36825;&#20123;&#32570;&#28857;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#23398;&#20064;&#21644;&#21387;&#32553;&#38382;&#39064;&#24314;&#27169;&#20026;&#31232;&#30095;&#36125;&#21494;&#26031;&#25512;&#26029;&#38382;&#39064;&#65292;&#20854;&#20013;&#37319;&#29992;&#20102;&#20998;&#32452;&#31232;&#30095;&#20808;&#39564;&#20197;&#23454;&#29616;&#32467;&#26500;&#21270;&#27169;&#22411;&#21387;&#32553;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340; BFL &#31639;&#27861;&#65292;&#21517;&#20026; EMTDAMP&#65292;&#20854;&#20013;&#23558;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#21644; Turbo Deep &#36817;&#20284;&#28040;&#24687;&#20256;&#36882;&#65288;TDAMP&#65289;&#32467;&#21512;&#36215;&#26469;&#23454;&#29616;&#20998;&#24067;&#24335;&#23398;&#20064;&#21644;&#21387;&#32553;&#12290;&#20013;&#22830;&#26381;&#21153;&#22120;&#32858;&#21512;&#26412;&#22320;&#21518;&#39564;&#20998;&#24067;&#20197;&#23454;&#29616;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a machine learning paradigm where the clients possess decentralized training data and the central server handles aggregation and scheduling. Typically, FL algorithms involve clients training their local models using stochastic gradient descent (SGD), which carries drawbacks such as slow convergence and being prone to getting stuck in suboptimal solutions. In this work, we propose a message passing based Bayesian federated learning (BFL) framework to avoid these drawbacks.Specifically, we formulate the problem of deep neural network (DNN) learning and compression and as a sparse Bayesian inference problem, in which group sparse prior is employed to achieve structured model compression. Then, we propose an efficient BFL algorithm called EMTDAMP, where expectation maximization (EM) and turbo deep approximate message passing (TDAMP) are combined to achieve distributed learning and compression. The central server aggregates local posterior distributions to update 
&lt;/p&gt;</description></item><item><title>&#23545;&#25239;&#24615;AI&#26159;&#19968;&#31181;&#20855;&#26377;&#30456;&#21453;&#34892;&#20026;&#25110;&#20215;&#20540;&#35266;&#30340;AI&#31995;&#32479;&#65292;&#19982;&#22823;&#22810;&#25968;&#20154;&#35748;&#20026;&#30340;&#30456;&#21453;&#65292;&#23427;&#21487;&#33021;&#23545;&#29992;&#25143;&#26377;&#30410;&#22788;&#65292;&#22914;&#36843;&#20351;&#29992;&#25143;&#38754;&#23545;&#20551;&#35774;&#12289;&#24314;&#31435;&#24674;&#22797;&#21147;&#25110;&#22521;&#20859;&#26356;&#20581;&#24247;&#30340;&#20851;&#31995;&#36793;&#30028;&#12290;</title><link>https://arxiv.org/abs/2402.07350</link><description>&lt;p&gt;
&#23545;&#25239;&#24615;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Antagonistic AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07350
&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;AI&#26159;&#19968;&#31181;&#20855;&#26377;&#30456;&#21453;&#34892;&#20026;&#25110;&#20215;&#20540;&#35266;&#30340;AI&#31995;&#32479;&#65292;&#19982;&#22823;&#22810;&#25968;&#20154;&#35748;&#20026;&#30340;&#30456;&#21453;&#65292;&#23427;&#21487;&#33021;&#23545;&#29992;&#25143;&#26377;&#30410;&#22788;&#65292;&#22914;&#36843;&#20351;&#29992;&#25143;&#38754;&#23545;&#20551;&#35774;&#12289;&#24314;&#31435;&#24674;&#22797;&#21147;&#25110;&#22521;&#20859;&#26356;&#20581;&#24247;&#30340;&#20851;&#31995;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#21457;&#23637;&#30340;&#22823;&#22810;&#25968;&#35770;&#36848;&#37117;&#20551;&#35774;&#26381;&#20174;&#12289;&#19982;&#8220;&#20154;&#31867;&#20215;&#20540;&#35266;&#8221;&#19968;&#33268;&#30340;&#8220;&#36947;&#24503;&#8221;&#27169;&#22411;&#22312;&#26222;&#36941;&#24847;&#20041;&#19978;&#26159;&#26377;&#30410;&#30340;&#65292;&#31616;&#32780;&#35328;&#20043;&#65292;&#22909;&#30340;AI&#23601;&#26159;&#36814;&#21512;AI&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#36814;&#21512;&#33539;&#20363;&#30340;&#38452;&#26263;&#38754;&#65292;&#31216;&#20043;&#20026;&#23545;&#25239;&#24615;AI&#30340;&#35774;&#35745;&#31354;&#38388;&#65306;AI&#31995;&#32479;&#20855;&#26377;&#19981;&#21516;&#24847;&#35265;&#12289;&#31895;&#40065;&#12289;&#25171;&#26029;&#12289;&#23545;&#25239;&#24615;&#12289;&#25361;&#25112;&#24615;&#31561;&#30456;&#21453;&#30340;&#34892;&#20026;&#25110;&#20215;&#20540;&#35266;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#23545;&#25239;&#24615;AI&#31995;&#32479;&#21487;&#33021;&#26377;&#26102;&#23545;&#29992;&#25143;&#26377;&#30410;&#22788;&#65292;&#22914;&#36843;&#20351;&#29992;&#25143;&#38754;&#23545;&#33258;&#24049;&#30340;&#20551;&#35774;&#12289;&#24314;&#31435;&#24674;&#22797;&#21147;&#25110;&#22521;&#20859;&#26356;&#20581;&#24247;&#30340;&#20851;&#31995;&#36793;&#30028;&#12290;&#36890;&#36807;&#23545;&#24418;&#25104;&#24615;&#25506;&#32034;&#21644;&#19968;&#20010;&#33222;&#24819;&#35774;&#35745;&#30740;&#35752;&#20250;&#30340;&#20511;&#37492;&#65292;&#21442;&#19982;&#32773;&#35774;&#35745;&#20102;&#20351;&#29992;&#23545;&#25239;&#24615;&#20803;&#32032;&#30340;&#34394;&#26500;AI&#25216;&#26415;&#65292;&#25105;&#20204;&#21246;&#21202;&#20986;&#23545;&#25239;&#24615;AI&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#38416;&#36848;&#28508;&#22312;&#30340;&#22909;&#22788;&#12289;&#35774;&#35745;&#25216;&#26415;&#21644;&#23558;&#23545;&#25239;&#24615;&#20803;&#32032;&#23884;&#20837;&#29992;&#25143;&#20307;&#39564;&#30340;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#35768;&#22810;&#20262;&#29702;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The vast majority of discourse around AI development assumes that subservient, "moral" models aligned with "human values" are universally beneficial -- in short, that good AI is sycophantic AI. We explore the shadow of the sycophantic paradigm, a design space we term antagonistic AI: AI systems that are disagreeable, rude, interrupting, confrontational, challenging, etc. -- embedding opposite behaviors or values. Far from being "bad" or "immoral," we consider whether antagonistic AI systems may sometimes have benefits to users, such as forcing users to confront their assumptions, build resilience, or develop healthier relational boundaries. Drawing from formative explorations and a speculative design workshop where participants designed fictional AI technologies that employ antagonism, we lay out a design space for antagonistic AI, articulating potential benefits, design techniques, and methods of embedding antagonistic elements into user experience. Finally, we discuss the many ethica
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;TextFooler&#40657;&#30418;&#23545;01 loss sign&#28608;&#27963;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#30340;&#25915;&#20987;&#20934;&#30830;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30456;&#27604;&#20110;sigmoid&#28608;&#27963;&#20132;&#21449;&#29109;&#21644;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#29992;01 loss sign&#28608;&#27963;&#30340;&#32593;&#32476;&#26356;&#38590;&#21463;&#21040;TextFooler&#30340;&#25915;&#20987;&#65292;&#24182;&#19988;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#20840;&#23616;&#27719;&#38598;&#27493;&#39588;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#23545;&#25239;&#31934;&#30830;&#24230;&#65292;&#20351;TextFooler&#20960;&#20046;&#26080;&#25928;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.07347</link><description>&lt;p&gt;
TextFooler&#40657;&#30418;&#23545;01 loss sign&#28608;&#27963;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#30340;&#25915;&#20987;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Accuracy of TextFooler black box adversarial attacks on 01 loss sign activation neural network ensemble
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;TextFooler&#40657;&#30418;&#23545;01 loss sign&#28608;&#27963;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#30340;&#25915;&#20987;&#20934;&#30830;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30456;&#27604;&#20110;sigmoid&#28608;&#27963;&#20132;&#21449;&#29109;&#21644;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#29992;01 loss sign&#28608;&#27963;&#30340;&#32593;&#32476;&#26356;&#38590;&#21463;&#21040;TextFooler&#30340;&#25915;&#20987;&#65292;&#24182;&#19988;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#20840;&#23616;&#27719;&#38598;&#27493;&#39588;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#23545;&#25239;&#31934;&#30830;&#24230;&#65292;&#20351;TextFooler&#20960;&#20046;&#26080;&#25928;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;01 loss sign&#28608;&#27963;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#23545;&#25239;&#25915;&#20987;&#26041;&#38754;&#20855;&#26377;&#38450;&#24481;&#33021;&#21147;&#12290;&#38024;&#23545;CIFAR10&#25968;&#25454;&#38598;&#25915;&#20987;&#27169;&#22411;&#30340;&#20844;&#20849;&#25361;&#25112;&#20173;&#28982;&#20445;&#25345;&#19981;&#36133;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20197;&#19979;&#38382;&#39064;&#65306;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;TextFooler&#30340;&#27969;&#34892;&#40657;&#30418;&#25991;&#26412;&#23545;&#25239;&#25915;&#20987;&#31243;&#24207;&#26469;&#27450;&#39575;01 loss sign&#28608;&#27963;&#31070;&#32463;&#32593;&#32476;&#22256;&#38590;&#21527;&#65311;&#25105;&#20204;&#22312;&#22235;&#20010;&#27969;&#34892;&#30340;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65306;IMDB&#35780;&#35770;&#12289;Yelp&#35780;&#35770;&#12289;MR&#24773;&#24863;&#20998;&#31867;&#21644;AG&#26032;&#38395;&#20998;&#31867;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19982;sigmoid&#28608;&#27963;&#20132;&#21449;&#29109;&#21644;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;01 loss sign&#28608;&#27963;&#32593;&#32476;&#26356;&#38590;&#21463;&#21040;TextFooler&#30340;&#25915;&#20987;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19968;&#31181;01 loss sign&#28608;&#27963;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#20855;&#26377;&#19968;&#31181;&#38024;&#23545;sign&#28608;&#27963;&#32593;&#32476;&#30340;&#26032;&#22411;&#20840;&#23616;&#27719;&#38598;&#27493;&#39588;&#12290;&#36890;&#36807;&#36825;&#31181;&#26032;&#30340;&#21464;&#20307;&#65292;&#25105;&#20204;&#30475;&#21040;&#20102;&#22312;&#23545;&#25239;&#31934;&#30830;&#24230;&#26041;&#38754;&#30340;&#26174;&#33879;&#25552;&#39640;&#65292;&#20351;&#24471;TextFooler&#23545;&#23427;&#20960;&#20046;&#26080;&#25928;&#12290;&#25105;&#20204;&#25552;&#20379;&#25105;&#20204;&#30340;&#20195;&#30721;&#20379;&#20813;&#36153;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has shown the defense of 01 loss sign activation neural networks against image classification adversarial attacks. A public challenge to attack the models on CIFAR10 dataset remains undefeated. We ask the following question in this study: are 01 loss sign activation neural networks hard to deceive with a popular black box text adversarial attack program called TextFooler? We study this question on four popular text classification datasets: IMDB reviews, Yelp reviews, MR sentiment classification, and AG news classification. We find that our 01 loss sign activation network is much harder to attack with TextFooler compared to sigmoid activation cross entropy and binary neural networks. We also study a 01 loss sign activation convolutional neural network with a novel global pooling step specific to sign activation networks. With this new variation we see a significant gain in adversarial accuracy rendering TextFooler practically useless against it. We make our code freely avail
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;ICU&#30149;&#20154;&#23454;&#39564;&#23460;&#26816;&#27979;&#25490;&#31243;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#32034;&#20102;&#26368;&#26032;&#25968;&#25454;&#38598;&#21644;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#30740;&#31350;&#32467;&#26524;&#26377;&#21161;&#20110;&#25552;&#39640;&#23454;&#39564;&#23460;&#26816;&#27979;&#25490;&#31243;&#31574;&#30053;&#30340;&#36136;&#37327;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07344</link><description>&lt;p&gt;
&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;ICU&#30149;&#20154;&#30340;&#27979;&#37327;&#25490;&#31243;
&lt;/p&gt;
&lt;p&gt;
Measurement Scheduling for ICU Patients with Offline Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;ICU&#30149;&#20154;&#23454;&#39564;&#23460;&#26816;&#27979;&#25490;&#31243;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#32034;&#20102;&#26368;&#26032;&#25968;&#25454;&#38598;&#21644;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#30740;&#31350;&#32467;&#26524;&#26377;&#21161;&#20110;&#25552;&#39640;&#23454;&#39564;&#23460;&#26816;&#27979;&#25490;&#31243;&#31574;&#30053;&#30340;&#36136;&#37327;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ICU&#30149;&#20154;&#30340;&#23454;&#39564;&#23460;&#26816;&#27979;&#25490;&#31243;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;ICU&#20013;&#19979;&#36798;&#30340;&#23454;&#39564;&#23460;&#26816;&#27979;&#32422;&#26377;20-40%&#26159;&#22810;&#20313;&#30340;&#65292;&#21487;&#20197;&#22312;&#19981;&#22952;&#30861;&#24739;&#32773;&#23433;&#20840;&#30340;&#24773;&#20917;&#19979;&#28040;&#38500;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#21033;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;Offline-RL&#65289;&#22522;&#20110;&#24739;&#32773;&#20449;&#24687;&#25214;&#21040;&#20102;&#26368;&#20339;&#30340;&#23454;&#39564;&#23460;&#26816;&#27979;&#25490;&#31243;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#33258;&#37027;&#26102;&#20197;&#26469;&#24050;&#32463;&#21457;&#24067;&#20102;&#26032;&#30340;ICU&#30149;&#20154;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20063;&#21462;&#24471;&#20102;&#21508;&#31181;&#36827;&#23637;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#30340;&#26032;&#21457;&#24067;&#30340;MIMIC-IV&#25968;&#25454;&#38598;&#30340;&#39044;&#22788;&#29702;&#27969;&#31243;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#26368;&#20808;&#36827;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#35782;&#21035;&#26356;&#22909;&#30340;ICU&#30149;&#20154;&#23454;&#39564;&#23460;&#26816;&#27979;&#25490;&#31243;&#31574;&#30053;&#26041;&#38754;&#30340;&#21151;&#25928;&#12290;&#38500;&#20102;&#35780;&#20272;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#22312;ICU&#29615;&#22659;&#20013;&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#36827;&#34892;&#23454;&#39564;&#23460;&#26816;&#27979;&#25490;&#31243;&#30340;&#24635;&#20307;&#36866;&#29992;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scheduling laboratory tests for ICU patients presents a significant challenge. Studies show that 20-40% of lab tests ordered in the ICU are redundant and could be eliminated without compromising patient safety. Prior work has leveraged offline reinforcement learning (Offline-RL) to find optimal policies for ordering lab tests based on patient information. However, new ICU patient datasets have since been released, and various advancements have been made in Offline-RL methods. In this study, we first introduce a preprocessing pipeline for the newly-released MIMIC-IV dataset geared toward time-series tasks. We then explore the efficacy of state-of-the-art Offline-RL methods in identifying better policies for ICU patient lab test scheduling. Besides assessing methodological performance, we also discuss the overall suitability and practicality of using Offline-RL frameworks for scheduling laboratory tests in ICU settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28041;&#21450;AI&#25216;&#26415;&#30340;&#26410;&#26469;&#35774;&#35745;&#24037;&#20316;&#27969;&#31243;&#65292;&#24378;&#35843;&#20102;&#21160;&#24577;&#24314;&#27169;&#65292;&#24314;&#35774;&#24615;&#21327;&#21830;&#21644;&#21487;&#25345;&#32493;&#28608;&#21169;&#19977;&#20010;&#20851;&#38190;&#28857;&#65292;&#36825;&#20123;&#28857;&#21487;&#20197;&#25903;&#25345;&#35774;&#35745;&#36807;&#31243;&#65292;&#24182;&#20026;AI&#25216;&#26415;&#19982;&#20154;&#31867;&#35774;&#35745;&#24072;&#21512;&#20316;&#30340;&#30456;&#23545;&#21487;&#34892;&#24615;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;</title><link>https://arxiv.org/abs/2402.07342</link><description>&lt;p&gt;
&#24819;&#35937;&#19982;AI&#20849;&#21516;&#35774;&#35745;&#30340;&#26410;&#26469;&#65306;&#21160;&#24577;&#24314;&#27169;&#65292;&#24314;&#35774;&#24615;&#21327;&#21830;&#21644;&#21487;&#25345;&#32493;&#28608;&#21169;
&lt;/p&gt;
&lt;p&gt;
Imagining a Future of Designing with AI: Dynamic Grounding, Constructive Negotiation, and Sustainable Motivation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28041;&#21450;AI&#25216;&#26415;&#30340;&#26410;&#26469;&#35774;&#35745;&#24037;&#20316;&#27969;&#31243;&#65292;&#24378;&#35843;&#20102;&#21160;&#24577;&#24314;&#27169;&#65292;&#24314;&#35774;&#24615;&#21327;&#21830;&#21644;&#21487;&#25345;&#32493;&#28608;&#21169;&#19977;&#20010;&#20851;&#38190;&#28857;&#65292;&#36825;&#20123;&#28857;&#21487;&#20197;&#25903;&#25345;&#35774;&#35745;&#36807;&#31243;&#65292;&#24182;&#20026;AI&#25216;&#26415;&#19982;&#20154;&#31867;&#35774;&#35745;&#24072;&#21512;&#20316;&#30340;&#30456;&#23545;&#21487;&#34892;&#24615;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26500;&#24605;&#20102;&#19968;&#20010;&#28041;&#21450;AI&#25216;&#26415;&#30340;&#26410;&#26469;&#35774;&#35745;&#24037;&#20316;&#27969;&#31243;&#12290;&#20511;&#37492;&#27963;&#21160;&#21644;&#20132;&#27969;&#29702;&#35770;&#65292;&#25105;&#20204;&#35797;&#22270;&#20998;&#31163;&#22823;&#22411;AI&#27169;&#22411;&#19982;&#36807;&#21435;&#25216;&#26415;&#30456;&#27604;&#22312;&#35774;&#35745;&#20013;&#25552;&#20379;&#30340;&#26032;&#20215;&#20540;&#12290;&#25105;&#20204;&#24471;&#20986;&#20102;&#19977;&#20010;&#21487;&#34892;&#24615;&#8212;&#8212;&#21160;&#24577;&#24314;&#27169;&#65292;&#24314;&#35774;&#24615;&#21327;&#21830;&#21644;&#21487;&#25345;&#32493;&#28608;&#21169;&#65292;&#24635;&#32467;&#20102;&#33258;&#28982;&#35821;&#35328;&#33021;&#21147;&#22522;&#30784;&#27169;&#22411;&#30340;&#28508;&#22312;&#29305;&#24615;&#65292;&#22914;&#26524;&#26126;&#30830;&#35774;&#35745;&#65292;&#21487;&#20197;&#25903;&#25345;&#35774;&#35745;&#36807;&#31243;&#12290;&#36890;&#36807;&#35774;&#35745;&#34394;&#26500;&#65292;&#25105;&#20204;&#24819;&#35937;&#20102;&#19968;&#20010;&#26410;&#26469;&#30340;&#30028;&#38754;&#20316;&#20026;&#19968;&#20010;&#25925;&#20107;&#21407;&#22411;&#65292;&#26494;&#40736;&#28216;&#25103;&#30340;&#25925;&#20107;&#65292;&#22312;&#19968;&#20010;&#30495;&#23454;&#30340;&#20351;&#29992;&#24773;&#22659;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#19977;&#20010;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#30340;&#35774;&#35745;&#36807;&#31243;&#12289;&#26415;&#35821;&#21644;&#22270;&#34920;&#26088;&#22312;&#20026;&#23558;&#26469;&#20851;&#20110;AI&#25216;&#26415;&#22312;&#19982;&#20154;&#31867;&#35774;&#35745;&#24072;&#21512;&#20316;&#26041;&#38754;&#30340;&#30456;&#23545;&#21487;&#34892;&#24615;&#35752;&#35770;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
We ideate a future design workflow that involves AI technology. Drawing from activity and communication theory, we attempt to isolate the new value large AI models can provide design compared to past technologies. We arrive at three affordances -- dynamic grounding, constructive negotiation, and sustainable motivation -- that summarize latent qualities of natural language-enabled foundation models that, if explicitly designed for, can support the process of design. Through design fiction, we then imagine a future interface as a diegetic prototype, the story of Squirrel Game, that demonstrates each of our three affordances in a realistic usage scenario. Our design process, terminology, and diagrams aim to contribute to future discussions about the relative affordances of AI technology with regard to collaborating with human designers.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#20102;&#25991;&#26412;&#12289;&#35821;&#38899;&#21644;&#35270;&#39057;&#19977;&#31181;&#36755;&#20837;&#27169;&#24577;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#21644;&#24773;&#24863;&#32467;&#26500;&#20998;&#26512;&#12290;&#36890;&#36807;&#29305;&#24449;&#32423;&#34701;&#21512;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#20998;&#31867;&#65292;&#23454;&#29616;&#20102;75.42%&#30340;&#24773;&#24863;&#35782;&#21035;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.07327</link><description>&lt;p&gt;
&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#12289;&#35821;&#38899;&#21644;&#35270;&#39057;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Multi-Modal Emotion Recognition by Text, Speech and Video Using Pretrained Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#20102;&#25991;&#26412;&#12289;&#35821;&#38899;&#21644;&#35270;&#39057;&#19977;&#31181;&#36755;&#20837;&#27169;&#24577;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#21644;&#24773;&#24863;&#32467;&#26500;&#20998;&#26512;&#12290;&#36890;&#36807;&#29305;&#24449;&#32423;&#34701;&#21512;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#20998;&#31867;&#65292;&#23454;&#29616;&#20102;75.42%&#30340;&#24773;&#24863;&#35782;&#21035;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20154;&#31867;&#24773;&#24863;&#30340;&#22797;&#26434;&#24615;&#21644;&#34920;&#36798;&#26041;&#27861;&#30340;&#22810;&#26679;&#24615;&#65292;&#24773;&#24863;&#35782;&#21035;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#39046;&#22495;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#25991;&#26412;&#12289;&#38899;&#39057;&#65288;&#35821;&#38899;&#65289;&#21644;&#35270;&#39057;&#19977;&#31181;&#36755;&#20837;&#27169;&#24577;&#29983;&#25104;&#22810;&#27169;&#24577;&#29305;&#24449;&#21521;&#37327;&#12290;&#20026;&#20102;&#20026;&#27599;&#31181;&#27169;&#24577;&#29983;&#25104;&#29305;&#24449;&#65292;&#20351;&#29992;&#20102;&#32463;&#36807;&#24494;&#35843;&#30340;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#12290;&#22312;&#27599;&#20010;&#27169;&#24577;&#20013;&#65292;&#20351;&#29992;&#20102;&#19968;&#20010;&#24102;&#26377;&#36801;&#31227;&#23398;&#20064;&#30340;Transformer&#27169;&#22411;&#26469;&#25552;&#21462;&#29305;&#24449;&#21644;&#24773;&#24863;&#32467;&#26500;&#12290;&#36825;&#20123;&#29305;&#24449;&#28982;&#21518;&#34987;&#34701;&#21512;&#22312;&#19968;&#36215;&#65292;&#24182;&#20351;&#29992;&#20998;&#31867;&#22120;&#36827;&#34892;&#24773;&#24863;&#35782;&#21035;&#12290;&#20026;&#20102;&#36873;&#25321;&#21512;&#36866;&#30340;&#34701;&#21512;&#26041;&#27861;&#21644;&#20998;&#31867;&#22120;&#65292;&#23581;&#35797;&#20102;&#21508;&#31181;&#29305;&#24449;&#32423;&#21644;&#20915;&#31574;&#32423;&#34701;&#21512;&#25216;&#26415;&#65292;&#24182;&#26368;&#32456;&#22312;IEMOCAP&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#19978;&#36890;&#36807;&#29305;&#24449;&#32423;&#21521;&#37327;&#36830;&#25509;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#20998;&#31867;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;75.42%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the complex nature of human emotions and the diversity of emotion representation methods in humans, emotion recognition is a challenging field. In this research, three input modalities, namely text, audio (speech), and video, are employed to generate multimodal feature vectors. For generating features for each of these modalities, pre-trained Transformer models with fine-tuning are utilized. In each modality, a Transformer model is used with transfer learning to extract feature and emotional structure. These features are then fused together, and emotion recognition is performed using a classifier. To select an appropriate fusion method and classifier, various feature-level and decision-level fusion techniques have been experimented with, and ultimately, the best model, which combines feature-level fusion by concatenating feature vectors and classification using a Support Vector Machine on the IEMOCAP multimodal dataset, achieves an accuracy of 75.42%. Keywords: Multimodal Emotio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24494;&#35843;Transformer&#65292;&#38024;&#23545;&#27874;&#26031;&#35821;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#38382;&#39064;&#25552;&#20986;&#20102;&#20004;&#31181;&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#20026;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#21019;&#26032;&#12290;</title><link>https://arxiv.org/abs/2402.07326</link><description>&lt;p&gt;
&#36890;&#36807;&#24494;&#35843;Transformer&#36827;&#34892;&#27874;&#26031;&#35821;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Persian Speech Emotion Recognition by Fine-Tuning Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24494;&#35843;Transformer&#65292;&#38024;&#23545;&#27874;&#26031;&#35821;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#38382;&#39064;&#25552;&#20986;&#20102;&#20004;&#31181;&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#20026;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#37325;&#35201;&#24615;&#65292;&#36817;&#24180;&#26469;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#22312;&#36825;&#20010;&#39046;&#22495;&#21019;&#24314;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#31995;&#32479;&#12290;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;Transformer&#65292;&#36890;&#36807;&#24494;&#35843;&#26469;&#35299;&#20915;&#36825;&#20010;&#29305;&#23450;&#38382;&#39064;&#65292;&#20174;&#32780;&#36798;&#21040;&#20102;&#39640;&#31934;&#24230;&#30340;&#32467;&#26524;&#12290;&#23613;&#31649;&#23545;&#20110;&#22686;&#24378;&#36825;&#20123;&#31995;&#32479;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35752;&#35770;&#21644;&#20840;&#29699;&#33539;&#22260;&#30340;&#21162;&#21147;&#65292;&#20294;&#22312;&#27874;&#26031;&#35821;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#32972;&#26223;&#20013;&#65292;&#36825;&#31181;&#21019;&#26032;&#21644;&#26377;&#25928;&#30340;&#26041;&#27861;&#30340;&#24212;&#29992;&#21364;&#24471;&#21040;&#20102;&#36739;&#23569;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#21450;&#20854;&#32972;&#26223;&#65292;&#24182;&#30528;&#37325;&#20171;&#32461;&#20102;&#22312;&#36825;&#19968;&#39046;&#22495;&#20013;&#20351;&#29992;Transformer&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#27169;&#22411;&#65292;&#19968;&#31181;&#22522;&#20110;&#35889;&#22270;&#65292;&#21478;&#19968;&#31181;&#22522;&#20110;&#38899;&#39057;&#26412;&#36523;&#65292;&#36890;&#36807;&#20351;&#29992;shEMO&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#12290;&#36825;&#20123;&#27169;&#22411;&#26174;&#33879;&#25552;&#39640;&#20102;&#20197;&#21069;&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#65292;&#22312;&#25152;&#25552;&#21040;&#30340;&#25968;&#25454;&#38598;&#19978;&#23558;&#20934;&#30830;&#29575;&#20174;&#32422;65%&#25552;&#39640;&#21040;&#20102;80%&#12290;&#38543;&#21518;&#65292;&#20026;&#20102;&#30740;&#31350;&#22810;&#35821;&#35328;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#19968;&#39033;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#35821;&#35328;&#30340;&#24773;&#24863;&#35782;&#21035;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the significance of speech emotion recognition, numerous methods have been developed in recent years to create effective and efficient systems in this domain. One of these methods involves the use of pretrained transformers, fine-tuned to address this specific problem, resulting in high accuracy. Despite extensive discussions and global-scale efforts to enhance these systems, the application of this innovative and effective approach has received less attention in the context of Persian speech emotion recognition. In this article, we review the field of speech emotion recognition and its background, with an emphasis on the importance of employing transformers in this context. We present two models, one based on spectrograms and the other on the audio itself, fine-tuned using the shEMO dataset. These models significantly enhance the accuracy of previous systems, increasing it from approximately 65% to 80% on the mentioned dataset. Subsequently, to investigate the effect of multilin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#23884;&#20837;&#21644;&#20027;&#21160;&#23398;&#20064;&#26469;&#23454;&#29616;&#21487;&#35299;&#37322;&#12289;&#23433;&#20840;&#30340;&#33258;&#21160;&#39550;&#39542;&#30340;&#26694;&#26550;&#12290;&#36890;&#36807;&#20351;&#29992;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#23884;&#20837;&#36827;&#34892;&#32858;&#31867;&#23454;&#39564;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#35782;&#21035;&#21644;&#35299;&#37322;&#20102;&#26032;&#39062;&#30340;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#12290;</title><link>https://arxiv.org/abs/2402.07320</link><description>&lt;p&gt;
&#29992;&#35821;&#35328;&#23884;&#20837;&#23454;&#29616;&#21487;&#35299;&#37322;&#12289;&#23433;&#20840;&#30340;&#33258;&#21160;&#39550;&#39542;&#65306;&#26032;&#39062;&#24615;&#35782;&#21035;&#21644;&#20027;&#21160;&#23398;&#20064;&#30340;&#26694;&#26550;&#19982;&#23454;&#39564;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Towards Explainable, Safe Autonomous Driving with Language Embeddings for Novelty Identification and Active Learning: Framework and Experimental Analysis with Real-World Data Sets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#23884;&#20837;&#21644;&#20027;&#21160;&#23398;&#20064;&#26469;&#23454;&#29616;&#21487;&#35299;&#37322;&#12289;&#23433;&#20840;&#30340;&#33258;&#21160;&#39550;&#39542;&#30340;&#26694;&#26550;&#12290;&#36890;&#36807;&#20351;&#29992;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#23884;&#20837;&#36827;&#34892;&#32858;&#31867;&#23454;&#39564;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#35782;&#21035;&#21644;&#35299;&#37322;&#20102;&#26032;&#39062;&#30340;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#33258;&#21160;&#39550;&#39542;&#25968;&#25454;&#38598;&#20013;&#20351;&#29992;&#35821;&#35328;&#23884;&#20837;&#36827;&#34892;&#20027;&#21160;&#23398;&#20064;&#65292;&#37325;&#28857;&#30740;&#31350;&#26032;&#39062;&#24615;&#26816;&#27979;&#12290;&#26032;&#39062;&#24615;&#25351;&#30340;&#26159;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#38590;&#20197;&#24212;&#23545;&#30340;&#24847;&#22806;&#24773;&#20917;&#65292;&#38656;&#35201;&#26356;&#39640;&#32423;&#21035;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#35821;&#35328;&#30340;&#34920;&#31034;&#26469;&#35782;&#21035;&#26032;&#39062;&#22330;&#26223;&#65292;&#24378;&#35843;&#23433;&#20840;&#25509;&#31649;&#21709;&#24212;&#21644;&#20027;&#21160;&#23398;&#20064;&#30340;&#21452;&#37325;&#30446;&#30340;&#12290;&#30740;&#31350;&#36824;&#36890;&#36807;&#20351;&#29992;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#23884;&#20837;&#36827;&#34892;&#32858;&#31867;&#23454;&#39564;&#65292;&#23545;&#20174;&#20004;&#20010;&#30495;&#23454;&#39550;&#39542;&#25968;&#25454;&#38598;&#65288;&#19968;&#20010;&#23433;&#35013;&#22312;&#36710;&#36742;&#19978;&#12289;&#19968;&#20010;&#23433;&#35013;&#22312;&#22522;&#30784;&#35774;&#26045;&#19978;&#65289;&#34893;&#29983;&#30340;&#23376;&#38598;&#36827;&#34892;&#26032;&#39062;&#24615;&#26816;&#27979;&#12290;&#36890;&#36807;&#29983;&#25104;&#30340;&#32858;&#31867;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#21306;&#20998;&#34987;&#20998;&#31867;&#20026;&#26032;&#39062;&#22330;&#26223;&#21644;&#20854;&#20182;&#22330;&#26223;&#30340;&#20803;&#32032;&#30340;&#25991;&#26412;&#35299;&#37322;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#23454;&#39564;&#25968;&#25454;&#27744;&#20013;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research explores the integration of language embeddings for active learning in autonomous driving datasets, with a focus on novelty detection. Novelty arises from unexpected scenarios that autonomous vehicles struggle to navigate, necessitating higher-level reasoning abilities. Our proposed method employs language-based representations to identify novel scenes, emphasizing the dual purpose of safety takeover responses and active learning. The research presents a clustering experiment using Contrastive Language-Image Pretrained (CLIP) embeddings to organize datasets and detect novelties. We find that the proposed algorithm effectively isolates novel scenes from a collection of subsets derived from two real-world driving datasets, one vehicle-mounted and one infrastructure-mounted. From the generated clusters, we further present methods for generating textual explanations of elements which differentiate scenes classified as novel from other scenes in the data pool, presenting quali
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22870;&#21169;&#40657;&#23458;&#38382;&#39064;&#65292;&#38024;&#23545;&#22238;&#22797;&#38271;&#24230;&#36825;&#19968;&#25361;&#25112;&#65292;&#36890;&#36807;&#24314;&#31435;&#21487;&#38752;&#30340;&#35780;&#20272;&#21327;&#35758;&#21644;&#25913;&#36827;&#22870;&#21169;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#20943;&#36731;&#38271;&#24230;&#20559;&#24046;&#30340;&#36229;&#21442;&#25968;&#21644;&#25216;&#24039;&#65292;&#24182;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.07319</link><description>&lt;p&gt;
ODIN: &#33073;&#32806;&#22870;&#21169;&#32531;&#35299;RLHF&#20013;&#30340;&#40657;&#23458;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
ODIN: Disentangled Reward Mitigates Hacking in RLHF
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22870;&#21169;&#40657;&#23458;&#38382;&#39064;&#65292;&#38024;&#23545;&#22238;&#22797;&#38271;&#24230;&#36825;&#19968;&#25361;&#25112;&#65292;&#36890;&#36807;&#24314;&#31435;&#21487;&#38752;&#30340;&#35780;&#20272;&#21327;&#35758;&#21644;&#25913;&#36827;&#22870;&#21169;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#20943;&#36731;&#38271;&#24230;&#20559;&#24046;&#30340;&#36229;&#21442;&#25968;&#21644;&#25216;&#24039;&#65292;&#24182;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;LLMs&#19978;&#20174;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#20986;&#29616;&#30340;&#21709;&#24212;&#38271;&#24230;&#19978;&#30340;&#22870;&#21169;&#40657;&#23458;&#38382;&#39064;&#12290;LLMs&#30340;&#26684;&#24335;&#33391;&#22909;&#20294;&#19981;&#22826;&#26377;&#29992;&#30340;&#22238;&#22797;&#24448;&#24448;&#20250;&#27450;&#39575;LLMs&#29978;&#33267;&#20154;&#31867;&#35780;&#20272;&#32773;&#20197;&#33719;&#24471;&#39640;&#20998;&#12290;&#21516;&#26679;&#30340;&#38382;&#39064;&#20063;&#23384;&#22312;&#20110;RL&#20013;&#30340;&#26576;&#20123;&#22870;&#21169;&#27169;&#22411;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#35757;&#32451;&#21644;&#35780;&#20272;&#20013;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26356;&#21487;&#38752;&#30340;&#35780;&#20272;&#21327;&#35758;&#65292;&#29992;&#20110;&#27604;&#36739;&#19981;&#21516;&#35757;&#32451;&#37197;&#32622;&#20043;&#38388;&#30340;LLM&#35780;&#20272;&#20998;&#25968;&#21644;&#36890;&#36807;&#25913;&#21464;&#35757;&#32451;&#36229;&#21442;&#25968;&#24471;&#21040;&#30340;&#21709;&#24212;&#38271;&#24230;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#22522;&#20110;&#36825;&#20010;&#35780;&#20272;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30740;&#31350;&#65292;&#32467;&#26524;&#25581;&#31034;&#20102;RL&#20013;&#29992;&#20110;&#20943;&#36731;&#38271;&#24230;&#20559;&#24046;&#30340;&#36229;&#21442;&#25968;&#21644;&#25216;&#24039;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#36890;&#36807;&#22312;&#20849;&#20139;&#29305;&#24449;&#34920;&#31034;&#19978;&#32852;&#21512;&#35757;&#32451;&#20004;&#20010;&#32447;&#24615;&#22836;&#26469;&#25913;&#36827;&#22870;&#21169;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;&#22870;&#21169;&#65292;&#19968;&#20010;&#35757;&#32451;&#26469;&#19982;&#38271;&#24230;&#30456;&#20851;&#65292;&#21478;&#19968;&#20010;&#35757;&#32451;&#26469;&#19982;&#20869;&#23481;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we study the issue of reward hacking on the response length, a challenge emerging in Reinforcement Learning from Human Feedback (RLHF) on LLMs. A well-formatted, verbose but less helpful response from the LLMs can often deceive LLMs or even human evaluators to achieve high scores. The same issue also holds for some reward models in RL. To address the challenges in both training and evaluation, we establish a more reliable evaluation protocol for comparing different training configurations, which inspects the trade-off between LLM evaluation score and response length obtained by varying training hyperparameters. Based on this evaluation, we conduct large-scale studies, where the results shed insights into the efficacy of hyperparameters and tricks used in RL on mitigating length bias. We further propose to improve the reward model by jointly training two linear heads on shared feature representations to predict the rewards, one trained to correlate with length, and the oth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22312;&#26080;&#26381;&#21153;&#22120;&#32852;&#37030;&#23398;&#20064;&#20013;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#35757;&#32451;&#24322;&#26500;&#23458;&#25143;&#31471;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36164;&#28304;&#21644;&#32479;&#35745;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.07295</link><description>&lt;p&gt;
&#22312;&#26080;&#26381;&#21153;&#22120;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#35757;&#32451;&#24322;&#26500;&#23458;&#25143;&#31471;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Training Heterogeneous Client Models using Knowledge Distillation in Serverless Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22312;&#26080;&#26381;&#21153;&#22120;&#32852;&#37030;&#23398;&#20064;&#20013;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#35757;&#32451;&#24322;&#26500;&#23458;&#25143;&#31471;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36164;&#28304;&#21644;&#32479;&#35745;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#20351;&#24471;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#20043;&#38388;&#33021;&#22815;&#21327;&#20316;&#35757;&#32451;&#20849;&#20139;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25345;&#25968;&#25454;&#30340;&#21435;&#20013;&#24515;&#21270;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#35774;&#35745;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#26041;&#38754;&#34920;&#26126;&#65292;&#21033;&#29992;&#26080;&#26381;&#21153;&#22120;&#35745;&#31639;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#20989;&#25968;&#21363;&#26381;&#21153;&#65288;FaaS&#65289;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#65292;&#21487;&#20197;&#25552;&#39640;&#36164;&#28304;&#25928;&#29575;&#65292;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#65292;&#24182;&#20943;&#36731;&#25968;&#25454;&#25345;&#26377;&#32773;&#38754;&#20020;&#30340;&#22797;&#26434;&#22522;&#30784;&#35774;&#26045;&#31649;&#29702;&#36127;&#25285;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26080;&#26381;&#21153;&#22120;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#38544;&#24335;&#22320;&#20551;&#35774;&#25152;&#26377;&#21442;&#19982;&#23458;&#25143;&#31471;&#20855;&#26377;&#30456;&#21516;&#30340;&#20840;&#23616;&#27169;&#22411;&#26550;&#26500;&#12290;&#36825;&#19968;&#20551;&#35774;&#26410;&#33021;&#35299;&#20915;&#23454;&#38469;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#22522;&#26412;&#25361;&#25112;&#65292;&#22240;&#20026;&#19981;&#21516;&#23458;&#25143;&#31471;&#20043;&#38388;&#23384;&#22312;&#36164;&#28304;&#21644;&#32479;&#35745;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#24182;&#22312;&#26080;&#26381;&#21153;&#22120;&#32852;&#37030;&#23398;&#20064;&#20013;&#23454;&#29616;&#24322;&#26500;&#23458;&#25143;&#31471;&#27169;&#22411;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#30340;&#26032;&#22411;&#20248;&#21270;&#26080;&#26381;&#21153;&#22120;&#24037;&#20316;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is an emerging machine learning paradigm that enables the collaborative training of a shared global model across distributed clients while keeping the data decentralized. Recent works on designing systems for efficient FL have shown that utilizing serverless computing technologies, particularly Function-as-a-Service (FaaS) for FL, can enhance resource efficiency, reduce training costs, and alleviate the complex infrastructure management burden on data holders. However, existing serverless FL systems implicitly assume a uniform global model architecture across all participating clients during training. This assumption fails to address fundamental challenges in practical FL due to the resource and statistical data heterogeneity among FL clients. To address these challenges and enable heterogeneous client models in serverless FL, we utilize Knowledge Distillation (KD) in this paper. Towards this, we propose novel optimized serverless workflows for two popular conve
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#20855;&#26377;&#39640;&#27979;&#35797;&#35206;&#30422;&#29575;&#30340;&#30495;&#23454;Java&#31243;&#24207;&#25968;&#25454;&#38598;&#65292;&#25506;&#32034;&#20102;&#26426;&#22120;&#23398;&#20064;&#22522;&#20110;&#35843;&#29992;&#22270;&#21098;&#26525;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#36890;&#36807;&#23545;&#27604;&#20998;&#26512;&#38745;&#24577;&#35843;&#29992;&#22270;&#29983;&#25104;&#25216;&#26415;&#26469;&#35299;&#20915;&#24403;&#21069;&#26041;&#27861;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.07294</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22522;&#20110;&#35843;&#29992;&#22270;&#21098;&#26525;&#30340;&#26377;&#25928;&#24615;&#65306;&#19968;&#20010;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Effectiveness of Machine Learning-based Call Graph Pruning: An Empirical Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#20855;&#26377;&#39640;&#27979;&#35797;&#35206;&#30422;&#29575;&#30340;&#30495;&#23454;Java&#31243;&#24207;&#25968;&#25454;&#38598;&#65292;&#25506;&#32034;&#20102;&#26426;&#22120;&#23398;&#20064;&#22522;&#20110;&#35843;&#29992;&#22270;&#21098;&#26525;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#36890;&#36807;&#23545;&#27604;&#20998;&#26512;&#38745;&#24577;&#35843;&#29992;&#22270;&#29983;&#25104;&#25216;&#26415;&#26469;&#35299;&#20915;&#24403;&#21069;&#26041;&#27861;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38745;&#24577;&#35843;&#29992;&#22270;(CG)&#26500;&#24314;&#32463;&#24120;&#36807;&#24230;&#36817;&#20284;&#35843;&#29992;&#20851;&#31995;&#65292;&#23548;&#33268;&#32467;&#26524;&#31934;&#30830;&#20294;&#19981;&#20934;&#30830;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;(ML)&#30340;CG&#21098;&#26525;&#20316;&#20026;&#19968;&#31181;&#25552;&#39640;&#31934;&#30830;&#24615;&#30340;&#25163;&#27573;&#65292;&#36890;&#36807;&#28040;&#38500;&#34394;&#20551;&#36793;&#32536;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#23384;&#22312;&#35780;&#20272;&#25968;&#25454;&#38598;&#26377;&#38480;&#12289;&#35757;&#32451;&#25968;&#25454;&#19981;&#24179;&#34913;&#65292;&#20197;&#21450;&#21484;&#22238;&#29575;&#38477;&#20302;&#31561;&#38382;&#39064;&#65292;&#36825;&#24433;&#21709;&#20102;&#23454;&#38469;&#30340;&#19979;&#28216;&#20998;&#26512;&#12290;&#20043;&#21069;&#30340;&#32467;&#26524;&#20063;&#27809;&#26377;&#19982;&#20808;&#36827;&#30340;&#38745;&#24577;CG&#26500;&#24314;&#25216;&#26415;&#36827;&#34892;&#27604;&#36739;&#12290;&#36825;&#39033;&#30740;&#31350;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;NYXCorpus&#65292;&#19968;&#20010;&#20855;&#26377;&#39640;&#27979;&#35797;&#35206;&#30422;&#29575;&#30340;&#30495;&#23454;Java&#31243;&#24207;&#25968;&#25454;&#38598;&#65292;&#24182;&#20174;&#27979;&#35797;&#25191;&#34892;&#20013;&#25910;&#38598;&#36712;&#36857;&#24182;&#26500;&#24314;&#21160;&#24577;CG&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;CG&#26469;&#25506;&#32034;ML-based CG&#21098;&#26525;&#22120;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#36807;&#31243;&#20013;&#30340;&#20445;&#23432;&#21098;&#26525;&#31574;&#30053;&#12290;&#25105;&#20204;&#23545;&#38745;&#24577;CG&#20351;&#29992;&#38646;&#25511;&#21046;&#27969;&#20998;&#26512;(0-CFA)&#29983;&#25104;&#30340;&#32467;&#26524;&#21644;&#30001;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;1-CFA&#31639;&#27861;&#29983;&#25104;&#30340;&#32467;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Static call graph (CG) construction often over-approximates call relations, leading to sound, but imprecise results. Recent research has explored machine learning (ML)-based CG pruning as a means to enhance precision by eliminating false edges. However, current methods suffer from a limited evaluation dataset, imbalanced training data, and reduced recall, which affects practical downstream analyses. Prior results were also not compared with advanced static CG construction techniques yet. This study tackles these issues. We introduce the NYXCorpus, a dataset of real-world Java programs with high test coverage and we collect traces from test executions and build a ground truth of dynamic CGs. We leverage these CGs to explore conservative pruning strategies during the training and inference of ML-based CG pruners. We conduct a comparative analysis of static CGs generated using zero control flow analysis (0-CFA) and those produced by a context-sensitive 1-CFA algorithm, evaluating both wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26435;&#34913;&#35802;&#23454;&#21644;&#24110;&#21161;&#24615;&#65292;&#22312;&#23454;&#39564;&#20013;&#21457;&#29616;&#24378;&#21270;&#23398;&#20064;&#25913;&#21892;&#20102;&#35802;&#23454;&#21644;&#24110;&#21161;&#24615;&#65292;&#32780;&#38142;&#24335;&#24605;&#32500;&#25552;&#31034;&#21017;&#20559;&#21521;&#20110;&#24110;&#21161;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#36824;&#23637;&#31034;&#20102;GPT-4 Turbo&#23545;&#23545;&#35805;&#26694;&#26550;&#21644;&#21548;&#20247;&#20915;&#31574;&#32972;&#26223;&#30340;&#25935;&#24863;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#21270;&#30340;&#23545;&#35805;&#20215;&#20540;&#35266;&#65292;&#24182;&#26263;&#31034;&#38646;-shot&#25552;&#31034;&#21487;&#20197;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#24341;&#23548;&#36825;&#20123;&#25277;&#35937;&#20215;&#20540;&#35266;&#12290;</title><link>https://arxiv.org/abs/2402.07282</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#22312;&#35802;&#23454;&#19982;&#24110;&#21161;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#65311;
&lt;/p&gt;
&lt;p&gt;
How do Large Language Models Navigate Conflicts between Honesty and Helpfulness?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07282
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26435;&#34913;&#35802;&#23454;&#21644;&#24110;&#21161;&#24615;&#65292;&#22312;&#23454;&#39564;&#20013;&#21457;&#29616;&#24378;&#21270;&#23398;&#20064;&#25913;&#21892;&#20102;&#35802;&#23454;&#21644;&#24110;&#21161;&#24615;&#65292;&#32780;&#38142;&#24335;&#24605;&#32500;&#25552;&#31034;&#21017;&#20559;&#21521;&#20110;&#24110;&#21161;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#36824;&#23637;&#31034;&#20102;GPT-4 Turbo&#23545;&#23545;&#35805;&#26694;&#26550;&#21644;&#21548;&#20247;&#20915;&#31574;&#32972;&#26223;&#30340;&#25935;&#24863;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#21270;&#30340;&#23545;&#35805;&#20215;&#20540;&#35266;&#65292;&#24182;&#26263;&#31034;&#38646;-shot&#25552;&#31034;&#21487;&#20197;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#24341;&#23548;&#36825;&#20123;&#25277;&#35937;&#20215;&#20540;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26085;&#24120;&#20132;&#27969;&#20013;&#65292;&#20154;&#20204;&#32463;&#24120;&#20026;&#20102;&#26368;&#22823;&#38480;&#24230;&#22320;&#24110;&#21161;&#21548;&#20247;&#32780;&#36817;&#20284;&#30495;&#30456;&#65292;&#20363;&#22914;&#32422;&#30053;&#26102;&#38388;&#25110;&#30465;&#30053;&#32454;&#33410;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;&#20309;&#22788;&#29702;&#36825;&#31181;&#24494;&#22937;&#30340;&#26435;&#34913;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#24515;&#29702;&#27169;&#22411;&#21644;&#26088;&#22312;&#25551;&#36848;&#20154;&#31867;&#34892;&#20026;&#30340;&#23454;&#39564;&#26469;&#20998;&#26512;LLMs&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#19968;&#31995;&#21015;LLMs&#65292;&#24182;&#25506;&#35752;&#20102;&#20248;&#21270;&#20154;&#31867;&#20559;&#22909;&#25110;&#25512;&#29702;&#26102;&#24605;&#32771;&#23545;&#36825;&#20123;&#26435;&#34913;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#25913;&#21892;&#20102;&#35802;&#23454;&#21644;&#24110;&#21161;&#24615;&#65292;&#32780;&#38142;&#24335;&#24605;&#32500;&#25552;&#31034;&#20351;LLMs&#20559;&#21521;&#20110;&#24110;&#21161;&#24615;&#32780;&#19981;&#26159;&#35802;&#23454;&#12290;&#26368;&#21518;&#65292;GPT-4 Turbo&#23637;&#31034;&#20102;&#31867;&#20284;&#20154;&#31867;&#30340;&#22238;&#24212;&#27169;&#24335;&#65292;&#21253;&#25324;&#23545;&#23545;&#35805;&#26694;&#26550;&#21644;&#21548;&#20247;&#20915;&#31574;&#32972;&#26223;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;LLMs&#20869;&#21270;&#30340;&#23545;&#35805;&#20215;&#20540;&#35266;&#65292;&#24182;&#26263;&#31034;&#21363;&#20351;&#36825;&#20123;&#25277;&#35937;&#20215;&#20540;&#35266;&#20063;&#21487;&#20197;&#22312;&#38646;-shot&#25552;&#31034;&#19979;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#34987;&#24341;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
In day-to-day communication, people often approximate the truth - for example, rounding the time or omitting details - in order to be maximally helpful to the listener. How do large language models (LLMs) handle such nuanced trade-offs? To address this question, we use psychological models and experiments designed to characterize human behavior to analyze LLMs. We test a range of LLMs and explore how optimization for human preferences or inference-time reasoning affects these trade-offs. We find that reinforcement learning from human feedback improves both honesty and helpfulness, while chain-of-thought prompting skews LLMs towards helpfulness over honesty. Finally, GPT-4 Turbo demonstrates human-like response patterns including sensitivity to the conversational framing and listener's decision context. Our findings reveal the conversational values internalized by LLMs and suggest that even these abstract values can, to a degree, be steered by zero-shot prompting.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#21517;&#20026;PathFormer&#30340;&#26032;&#22411;GNN&#27169;&#22411;&#26550;&#26500;&#65292;&#31995;&#32479;&#25972;&#21512;&#20449;&#21495;&#32593;&#32476;&#12289;&#20808;&#39564;&#30693;&#35782;&#21644;&#32452;&#23398;&#25968;&#25454;&#26469;&#23454;&#29616;&#39640;&#31934;&#30830;&#24230;&#30142;&#30149;&#35786;&#26029;&#21644;&#39640;&#21487;&#37325;&#22797;&#24615;&#29983;&#29289;&#26631;&#24535;&#29289;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.07268</link><description>&lt;p&gt;
&#20351;&#29992;PathFormer&#36827;&#34892;&#39640;&#31934;&#30830;&#24230;&#30142;&#30149;&#35786;&#26029;&#21644;&#39640;&#21487;&#37325;&#22797;&#24615;&#29983;&#29289;&#26631;&#24535;&#29289;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Highly Accurate Disease Diagnosis and Highly Reproducible Biomarker Identification with PathFormer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07268
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#21517;&#20026;PathFormer&#30340;&#26032;&#22411;GNN&#27169;&#22411;&#26550;&#26500;&#65292;&#31995;&#32479;&#25972;&#21512;&#20449;&#21495;&#32593;&#32476;&#12289;&#20808;&#39564;&#30693;&#35782;&#21644;&#32452;&#23398;&#25968;&#25454;&#26469;&#23454;&#29616;&#39640;&#31934;&#30830;&#24230;&#30142;&#30149;&#35786;&#26029;&#21644;&#39640;&#21487;&#37325;&#22797;&#24615;&#29983;&#29289;&#26631;&#24535;&#29289;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#35782;&#21035;&#23545;&#20110;&#31934;&#30830;&#30340;&#30142;&#30149;&#35786;&#26029;&#21644;&#29702;&#35299;&#30142;&#30149;&#21457;&#30149;&#26426;&#21046;&#22312;&#32452;&#23398;&#25968;&#25454;&#20998;&#26512;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#27604;&#22914;&#20351;&#29992;&#25240;&#21472;&#21464;&#21270;&#21644;&#22238;&#24402;&#20998;&#26512;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26159;&#20998;&#26512;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#20027;&#35201;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;GNNs&#22312;&#32452;&#23398;&#25968;&#25454;&#20998;&#26512;&#20013;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#38480;&#21046;&#65292;&#21363;&#39044;&#27979;&#65288;&#35786;&#26029;&#65289;&#20934;&#30830;&#24615;&#26377;&#38480;&#21644;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#20013;&#21487;&#37325;&#22797;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#35782;&#21035;&#33021;&#21147;&#26377;&#38480;&#12290;&#36825;&#20123;&#25361;&#25112;&#30340;&#26681;&#28304;&#22312;&#20110;&#29983;&#29289;&#20449;&#21495;&#36890;&#36335;&#30340;&#29420;&#29305;&#22270;&#32467;&#26500;&#65292;&#20854;&#20013;&#21253;&#25324;&#22823;&#37327;&#30340;&#38774;&#28857;&#21644;&#36825;&#20123;&#38774;&#28857;&#20043;&#38388;&#23494;&#38598;&#32780;&#22797;&#26434;&#30340;&#20449;&#21495;&#20132;&#20114;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PathFormer&#30340;&#26032;&#22411;GNN&#27169;&#22411;&#26550;&#26500;&#65292;&#23427;&#31995;&#32479;&#22320;&#25972;&#21512;&#20449;&#21495;&#32593;&#32476;&#12289;&#20808;&#39564;&#30693;&#35782;&#21644;&#32452;&#23398;&#25968;&#25454;&#26469;&#23545;&#29983;&#29289;&#26631;&#24535;&#29289;&#36827;&#34892;&#25490;&#21517;&#21644;&#39044;&#27979;&#30142;&#30149;&#35786;&#26029;&#12290;&#22312;&#27604;&#36739;&#32467;&#26524;&#20013;&#65292;PathFormer&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biomarker identification is critical for precise disease diagnosis and understanding disease pathogenesis in omics data analysis, like using fold change and regression analysis. Graph neural networks (GNNs) have been the dominant deep learning model for analyzing graph-structured data. However, we found two major limitations of existing GNNs in omics data analysis, i.e., limited-prediction (diagnosis) accuracy and limited-reproducible biomarker identification capacity across multiple datasets. The root of the challenges is the unique graph structure of biological signaling pathways, which consists of a large number of targets and intensive and complex signaling interactions among these targets. To resolve these two challenges, in this study, we presented a novel GNN model architecture, named PathFormer, which systematically integrate signaling network, priori knowledge and omics data to rank biomarkers and predict disease diagnosis. In the comparison results, PathFormer outperformed ex
&lt;/p&gt;</description></item><item><title>DIMON&#26159;&#19968;&#20010;&#23398;&#20064;&#22312;&#19968;&#31995;&#21015;&#21464;&#24418;&#30340;&#22495;&#19978;&#35299;&#31639;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#36890;&#29992;&#31639;&#23376;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#21442;&#32771;&#22495;&#35757;&#32451;&#25968;&#25454;&#19978;&#23398;&#20064;&#35299;&#30340;&#26144;&#23556;&#65292;&#28982;&#21518;&#23558;&#20854;&#37325;&#26032;&#26144;&#23556;&#22238;&#21407;&#22987;&#22495;&#26469;&#23454;&#29616;&#23545;&#22810;&#20010;&#22495;&#19978;&#21464;&#21270;&#30340;&#21021;&#22987;/&#36793;&#30028;&#26465;&#20214;&#19979;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#30340;&#36817;&#20284;&#12290;</title><link>https://arxiv.org/abs/2402.07250</link><description>&lt;p&gt;
DIMON:&#22312;&#19968;&#31995;&#21015;&#21464;&#24418;&#30340;&#22495;&#19978;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#31639;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
DIMON: Learning Solution Operators of Partial Differential Equations on a Diffeomorphic Family of Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07250
&lt;/p&gt;
&lt;p&gt;
DIMON&#26159;&#19968;&#20010;&#23398;&#20064;&#22312;&#19968;&#31995;&#21015;&#21464;&#24418;&#30340;&#22495;&#19978;&#35299;&#31639;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#36890;&#29992;&#31639;&#23376;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#21442;&#32771;&#22495;&#35757;&#32451;&#25968;&#25454;&#19978;&#23398;&#20064;&#35299;&#30340;&#26144;&#23556;&#65292;&#28982;&#21518;&#23558;&#20854;&#37325;&#26032;&#26144;&#23556;&#22238;&#21407;&#22987;&#22495;&#26469;&#23454;&#29616;&#23545;&#22810;&#20010;&#22495;&#19978;&#21464;&#21270;&#30340;&#21021;&#22987;/&#36793;&#30028;&#26465;&#20214;&#19979;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#30340;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#20010;&#22495;&#19978;&#21464;&#21270;&#30340;&#21021;&#22987;/&#36793;&#30028;&#26465;&#20214;&#19979;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#37117;&#26159;&#38656;&#35201;&#30340;&#65292;&#20294;&#26159;&#22914;&#26524;&#27599;&#27425;&#22495;&#30340;&#21021;&#22987;/&#36793;&#30028;&#26465;&#20214;&#21464;&#21270;&#26102;&#37117;&#37325;&#26032;&#35745;&#31639;&#35299;&#65292;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#31639;&#23376;&#23398;&#20064;&#26694;&#26550;&#65292;&#31216;&#20026;DIffeomorphic Mapping Operator LearNing&#65288;DIMON&#65289;&#65292;&#29992;&#26469;&#23398;&#20064;&#35299;&#22312;&#22495;&#26063;$\{\Omega_{\theta}}_\theta$&#19978;&#30340;&#36817;&#20284;&#35299;&#65292;&#23427;&#23398;&#20064;&#20174;&#21021;&#22987;/&#36793;&#30028;&#26465;&#20214;&#21644;&#22495;$\Omega_\theta$&#21040;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#65288;&#25110;&#25351;&#23450;&#30340;&#20989;&#25968;&#65289;&#30340;&#26144;&#23556;&#12290;DIMON&#22522;&#20110;&#23558;&#32473;&#23450;&#38382;&#39064;&#65288;&#21021;&#22987;/&#36793;&#30028;&#26465;&#20214;&#21644;&#22495;$\Omega_{\theta}$&#65289;&#36716;&#31227;&#21040;&#19968;&#20010;&#21442;&#32771;&#22495;$\Omega_{0}$&#19978;&#36827;&#34892;&#22788;&#29702;&#65292;&#20854;&#20013;&#20351;&#29992;&#26469;&#33258;&#22810;&#20010;&#38382;&#39064;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#23398;&#20064;&#21040;&#22312;$\Omega_{0}$&#19978;&#30340;&#35299;&#30340;&#26144;&#23556;&#65292;&#28982;&#21518;&#20877;&#23558;&#20854;&#37325;&#26032;&#26144;&#23556;&#22238;&#21407;&#22987;&#22495;$\Omega_{\theta}$&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20960;&#20010;&#38382;&#39064;&#26469;&#23637;&#31034;&#35813;&#26694;&#26550;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The solution of a PDE over varying initial/boundary conditions on multiple domains is needed in a wide variety of applications, but it is computationally expensive if the solution is computed de novo whenever the initial/boundary conditions of the domain change. We introduce a general operator learning framework, called DIffeomorphic Mapping Operator learNing (DIMON) to learn approximate PDE solutions over a family of domains $\{\Omega_{\theta}}_\theta$, that learns the map from initial/boundary conditions and domain $\Omega_\theta$ to the solution of the PDE, or to specified functionals thereof. DIMON is based on transporting a given problem (initial/boundary conditions and domain $\Omega_{\theta}$) to a problem on a reference domain $\Omega_{0}$, where training data from multiple problems is used to learn the map to the solution on $\Omega_{0}$, which is then re-mapped to the original domain $\Omega_{\theta}$. We consider several problems to demonstrate the performance of the framewo
&lt;/p&gt;</description></item><item><title>SAIS&#26159;&#19968;&#31181;&#22522;&#20110;&#20849;&#29983;&#27169;&#24335;&#30340;&#26032;&#22411;&#29983;&#29289;&#21551;&#21457;&#24335;&#20154;&#24037;&#20813;&#30123;&#31995;&#32479;&#65292;&#36890;&#36807;&#24182;&#34892;&#22788;&#29702;&#20811;&#26381;&#20102;&#20256;&#32479;&#20154;&#24037;&#20813;&#30123;&#31995;&#32479;&#21644;SOS&#31639;&#27861;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#20154;&#21475;&#21644;&#22686;&#24378;&#20154;&#21475;&#22810;&#26679;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;26&#20010;&#22522;&#20934;&#38382;&#39064;&#19978;&#65292;SAIS&#22312;&#24615;&#33021;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;SOS&#26041;&#27861;&#30456;&#24403;&#65292;&#24182;&#36229;&#36807;&#20854;&#20182;&#24120;&#35265;&#30340;AIS&#26041;&#27861;&#21644;&#36827;&#21270;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.07244</link><description>&lt;p&gt;
SAIS: &#19968;&#31181;&#22522;&#20110;&#20849;&#29983;&#27169;&#24335;&#30340;&#26032;&#22411;&#29983;&#29289;&#21551;&#21457;&#24335;&#20154;&#24037;&#20813;&#30123;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
SAIS: A Novel Bio-Inspired Artificial Immune System Based on Symbiotic Paradigm
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07244
&lt;/p&gt;
&lt;p&gt;
SAIS&#26159;&#19968;&#31181;&#22522;&#20110;&#20849;&#29983;&#27169;&#24335;&#30340;&#26032;&#22411;&#29983;&#29289;&#21551;&#21457;&#24335;&#20154;&#24037;&#20813;&#30123;&#31995;&#32479;&#65292;&#36890;&#36807;&#24182;&#34892;&#22788;&#29702;&#20811;&#26381;&#20102;&#20256;&#32479;&#20154;&#24037;&#20813;&#30123;&#31995;&#32479;&#21644;SOS&#31639;&#27861;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#20154;&#21475;&#21644;&#22686;&#24378;&#20154;&#21475;&#22810;&#26679;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;26&#20010;&#22522;&#20934;&#38382;&#39064;&#19978;&#65292;SAIS&#22312;&#24615;&#33021;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;SOS&#26041;&#27861;&#30456;&#24403;&#65292;&#24182;&#36229;&#36807;&#20854;&#20182;&#24120;&#35265;&#30340;AIS&#26041;&#27861;&#21644;&#36827;&#21270;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#20154;&#24037;&#20813;&#30123;&#31995;&#32479;&#65288;AIS&#65289;&#65306;&#20849;&#29983;&#20154;&#24037;&#20813;&#30123;&#31995;&#32479;&#65288;SAIS&#65289;&#65292;&#28789;&#24863;&#26469;&#33258;&#29983;&#29289;&#23398;&#20013;&#30340;&#20849;&#29983;&#20851;&#31995;&#12290;SAIS&#19982;Symbiotic Organisms Search&#65288;SOS&#65289;&#31639;&#27861;&#20013;&#30340;&#19977;&#20010;&#20851;&#38190;&#38454;&#27573;&#65288;&#20114;&#24800;&#20027;&#20041;&#12289;&#20849;&#29983;&#20027;&#20041;&#21644;&#23492;&#29983;&#20027;&#20041;&#65289;&#30456;&#23545;&#24212;&#12290;&#36825;&#31181;&#24182;&#34892;&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;AIS&#20013;&#22823;&#35268;&#27169;&#20154;&#21475;&#21644;&#22686;&#24378;&#20154;&#21475;&#22810;&#26679;&#24615;&#30340;&#25361;&#25112;&#65292;&#20256;&#32479;&#30340;AIS&#21644;SOS&#22312;&#36825;&#26041;&#38754;&#30340;&#35299;&#20915;&#25928;&#29575;&#36739;&#20302;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;SAIS&#22312;26&#20010;&#22522;&#20934;&#38382;&#39064;&#19978;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;SOS&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#36229;&#36807;&#20102;&#20854;&#20182;&#24120;&#35265;&#30340;AIS&#26041;&#27861;&#21644;&#36827;&#21270;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35843;&#30740;&#20102;&#21442;&#25968;&#36873;&#25321;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;SAIS&#22312;&#22788;&#29702;&#26356;&#22823;&#35268;&#27169;&#20154;&#21475;&#26102;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#38656;&#35201;&#26356;&#23569;&#30340;&#36845;&#20195;&#20195;&#25968;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30456;&#20449;SAIS&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#29983;&#29289;&#21551;&#21457;&#24335;&#21644;&#20813;&#30123;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#26377;&#26395;&#22312;&#20248;&#21270;&#38382;&#39064;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel type of Artificial Immune System (AIS): Symbiotic Artificial Immune Systems (SAIS), drawing inspiration from symbiotic relationships in biology. SAIS parallels the three key stages (i.e., mutualism, commensalism and parasitism) of population updating from the Symbiotic Organisms Search (SOS) algorithm. This parallel approach effectively addresses the challenges of large population size and enhances population diversity in AIS, which traditional AIS and SOS struggle to resolve efficiently. We conducted a series of experiments, which demonstrated that our SAIS achieved comparable performance to the state-of-the-art approach SOS and outperformed other popular AIS approaches and evolutionary algorithms across 26 benchmark problems. Furthermore, we investigated the problem of parameter selection and found that SAIS performs better in handling larger population sizes while requiring fewer generations. Finally, we believe SAIS, as a novel bio-inspired and immune-inspired al
&lt;/p&gt;</description></item><item><title>CPSDBench&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#20013;&#22269;&#20844;&#20849;&#23433;&#20840;&#39046;&#22495;&#37327;&#36523;&#23450;&#21046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#22522;&#20934;&#65292;&#36890;&#36807;&#25972;&#21512;&#23454;&#38469;&#22330;&#26223;&#20013;&#25910;&#38598;&#30340;&#20844;&#20849;&#23433;&#20840;&#30456;&#20851;&#25968;&#25454;&#38598;&#65292;&#38024;&#23545;&#25991;&#26412;&#20998;&#31867;&#12289;&#20449;&#24687;&#25552;&#21462;&#12289;&#38382;&#39064;&#22238;&#31572;&#21644;&#25991;&#26412;&#29983;&#25104;&#22235;&#20010;&#20851;&#38190;&#32500;&#24230;&#20840;&#38754;&#35780;&#20272;LLMs&#30340;&#24615;&#33021;&#65292;&#24182;&#24341;&#20837;&#21019;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#25552;&#39640;&#20102;&#23545;&#29616;&#26377;&#27169;&#22411;&#22312;&#35299;&#20915;&#20844;&#20849;&#23433;&#20840;&#38382;&#39064;&#26041;&#38754;&#24615;&#33021;&#30340;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.07234</link><description>&lt;p&gt;
CPSDBench&#65306;&#19968;&#20010;&#38024;&#23545;&#20013;&#22269;&#20844;&#20849;&#23433;&#20840;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#22522;&#20934;&#21644;&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;
CPSDBench: A Large Language Model Evaluation Benchmark and Baseline for Chinese Public Security Domain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07234
&lt;/p&gt;
&lt;p&gt;
CPSDBench&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#20013;&#22269;&#20844;&#20849;&#23433;&#20840;&#39046;&#22495;&#37327;&#36523;&#23450;&#21046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#22522;&#20934;&#65292;&#36890;&#36807;&#25972;&#21512;&#23454;&#38469;&#22330;&#26223;&#20013;&#25910;&#38598;&#30340;&#20844;&#20849;&#23433;&#20840;&#30456;&#20851;&#25968;&#25454;&#38598;&#65292;&#38024;&#23545;&#25991;&#26412;&#20998;&#31867;&#12289;&#20449;&#24687;&#25552;&#21462;&#12289;&#38382;&#39064;&#22238;&#31572;&#21644;&#25991;&#26412;&#29983;&#25104;&#22235;&#20010;&#20851;&#38190;&#32500;&#24230;&#20840;&#38754;&#35780;&#20272;LLMs&#30340;&#24615;&#33021;&#65292;&#24182;&#24341;&#20837;&#21019;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#25552;&#39640;&#20102;&#23545;&#29616;&#26377;&#27169;&#22411;&#22312;&#35299;&#20915;&#20844;&#20849;&#23433;&#20840;&#38382;&#39064;&#26041;&#38754;&#24615;&#33021;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22810;&#20010;&#24212;&#29992;&#39046;&#22495;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#28508;&#21147;&#21644;&#25928;&#26524;&#12290;&#20026;&#20102;&#35780;&#20272;&#20027;&#27969;LLMs&#22312;&#20844;&#20849;&#23433;&#20840;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#20013;&#22269;&#20844;&#20849;&#23433;&#20840;&#39046;&#22495;&#30340;&#35780;&#20272;&#22522;&#20934;&#8212;&#8212;CPSDBench&#12290;CPSDBench&#25972;&#21512;&#20102;&#20174;&#29616;&#23454;&#22330;&#26223;&#20013;&#25910;&#38598;&#21040;&#30340;&#19982;&#20844;&#20849;&#23433;&#20840;&#30456;&#20851;&#30340;&#25968;&#25454;&#38598;&#65292;&#25903;&#25345;&#23545;LLMs&#22312;&#25991;&#26412;&#20998;&#31867;&#12289;&#20449;&#24687;&#25552;&#21462;&#12289;&#38382;&#39064;&#22238;&#31572;&#21644;&#25991;&#26412;&#29983;&#25104;&#22235;&#20010;&#20851;&#38190;&#32500;&#24230;&#19978;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#24341;&#20837;&#20102;&#19968;&#22871;&#21019;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#26088;&#22312;&#26356;&#31934;&#30830;&#22320;&#37327;&#21270;LLMs&#22312;&#25191;&#34892;&#19982;&#20844;&#20849;&#23433;&#20840;&#30456;&#20851;&#20219;&#21153;&#19978;&#30340;&#25928;&#21147;&#12290;&#36890;&#36807;&#26412;&#30740;&#31350;&#20013;&#30340;&#28145;&#20837;&#20998;&#26512;&#21644;&#35780;&#20272;&#65292;&#25105;&#20204;&#19981;&#20165;&#22686;&#24378;&#20102;&#23545;&#29616;&#26377;&#27169;&#22411;&#22312;&#35299;&#20915;&#20844;&#20849;&#23433;&#20840;&#38382;&#39064;&#26041;&#38754;&#24615;&#33021;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#30340;&#29702;&#35299;&#65292;&#36824;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated significant potential and effectiveness across multiple application domains. To assess the performance of mainstream LLMs in public security tasks, this study aims to construct a specialized evaluation benchmark tailored to the Chinese public security domain--CPSDbench. CPSDbench integrates datasets related to public security collected from real-world scenarios, supporting a comprehensive assessment of LLMs across four key dimensions: text classification, information extraction, question answering, and text generation. Furthermore, this study introduces a set of innovative evaluation metrics designed to more precisely quantify the efficacy of LLMs in executing tasks related to public security. Through the in-depth analysis and evaluation conducted in this research, we not only enhance our understanding of the performance strengths and limitations of existing models in addressing public security issues but also provide references for the fu
&lt;/p&gt;</description></item><item><title>TransGPT&#26159;&#19968;&#31181;&#38754;&#21521;&#20132;&#36890;&#39046;&#22495;&#30340;&#26032;&#22411;&#22810;&#27169;&#24335;&#29983;&#25104;&#39044;&#35757;&#32451;Transformer&#65292;&#20351;&#29992;&#21333;&#27169;&#24335;&#21644;&#22810;&#27169;&#24335;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#65292;&#22312;&#20132;&#36890;&#39046;&#22495;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.07233</link><description>&lt;p&gt;
TransGPT&#65306;&#29992;&#20110;&#20132;&#36890;&#30340;&#22810;&#27169;&#24335;&#29983;&#25104;&#39044;&#35757;&#32451;Transformer
&lt;/p&gt;
&lt;p&gt;
TransGPT: Multi-modal Generative Pre-trained Transformer for Transportation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07233
&lt;/p&gt;
&lt;p&gt;
TransGPT&#26159;&#19968;&#31181;&#38754;&#21521;&#20132;&#36890;&#39046;&#22495;&#30340;&#26032;&#22411;&#22810;&#27169;&#24335;&#29983;&#25104;&#39044;&#35757;&#32451;Transformer&#65292;&#20351;&#29992;&#21333;&#27169;&#24335;&#21644;&#22810;&#27169;&#24335;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#65292;&#22312;&#20132;&#36890;&#39046;&#22495;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26159;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#65288;ITS&#65289;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#20294;&#22312;&#20132;&#36890;&#39046;&#22495;&#38754;&#20020;&#30528;&#35768;&#22810;&#25361;&#25112;&#65292;&#22914;&#19987;&#19994;&#39046;&#22495;&#30693;&#35782;&#19982;&#25968;&#25454;&#65292;&#22810;&#27169;&#24335;&#36755;&#20837;&#21644;&#36755;&#20986;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;TransGPT&#65292;&#19968;&#31181;&#38754;&#21521;&#20132;&#36890;&#39046;&#22495;&#30340;&#26032;&#22411;&#65288;&#22810;&#27169;&#24335;&#65289;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#30001;&#20004;&#20010;&#29420;&#31435;&#30340;&#21464;&#20307;&#32452;&#25104;&#65306;TransGPT-SM&#29992;&#20110;&#21333;&#27169;&#24335;&#25968;&#25454;&#21644;TransGPT-MM&#29992;&#20110;&#22810;&#27169;&#24335;&#25968;&#25454;&#12290;TransGPT-SM&#22312;&#21253;&#21547;&#26469;&#33258;&#20132;&#36890;&#39046;&#22495;&#21508;&#31181;&#26469;&#28304;&#30340;&#25991;&#26412;&#25968;&#25454;&#30340;&#21333;&#27169;&#24335;&#20132;&#36890;&#25968;&#25454;&#38598;&#65288;STD&#65289;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;TransGPT-MM&#22312;&#25105;&#20204;&#25163;&#21160;&#25910;&#38598;&#30340;&#20132;&#36890;&#39046;&#22495;&#30340;&#19977;&#20010;&#39046;&#22495;&#65288;&#39550;&#39542;&#27979;&#35797;&#12289;&#20132;&#36890;&#26631;&#24535;&#21644;&#22320;&#26631;&#65289;&#30340;&#22810;&#27169;&#24335;&#20132;&#36890;&#25968;&#25454;&#38598;&#65288;MTD&#65289;&#19978;&#32454;&#35843;&#12290;&#25105;&#20204;&#23545;TransGPT&#22312;&#20132;&#36890;&#39046;&#22495;&#30340;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#19978;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language processing (NLP) is a key component of intelligent transportation systems (ITS), but it faces many challenges in the transportation domain, such as domain-specific knowledge and data, and multi-modal inputs and outputs. This paper presents TransGPT, a novel (multi-modal) large language model for the transportation domain, which consists of two independent variants: TransGPT-SM for single-modal data and TransGPT-MM for multi-modal data. TransGPT-SM is finetuned on a single-modal Transportation dataset (STD) that contains textual data from various sources in the transportation domain. TransGPT-MM is finetuned on a multi-modal Transportation dataset (MTD) that we manually collected from three areas of the transportation domain: driving tests, traffic signs, and landmarks. We evaluate TransGPT on several benchmark datasets for different tasks in the transportation domain, and show that it outperforms baseline models on most tasks. We also showcase the potential application
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20998;&#23618;&#20998;&#36776;&#29575;&#35745;&#31639;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#26102;&#38388;&#32422;&#26463;&#19979;&#33719;&#24471;&#36817;&#20284;&#29256;&#26412;&#30340;&#26368;&#32456;&#32467;&#26524;&#65292;&#36825;&#23545;&#20110;&#22522;&#20110;&#25130;&#27490;&#26085;&#26399;&#30340;&#31995;&#32479;&#21644;&#26576;&#20123;&#25805;&#20316;&#27169;&#24335;&#19979;&#30340;&#20915;&#31574;&#31995;&#32479;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.07229</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35745;&#31639;&#20013;&#30340;&#36830;&#32493;&#20248;&#21270;&#65306;&#25512;&#36827;&#27169;&#22411;&#25512;&#29702;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Successive Refinement in Large-Scale Computation: Advancing Model Inference Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20998;&#23618;&#20998;&#36776;&#29575;&#35745;&#31639;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#26102;&#38388;&#32422;&#26463;&#19979;&#33719;&#24471;&#36817;&#20284;&#29256;&#26412;&#30340;&#26368;&#32456;&#32467;&#26524;&#65292;&#36825;&#23545;&#20110;&#22522;&#20110;&#25130;&#27490;&#26085;&#26399;&#30340;&#31995;&#32479;&#21644;&#26576;&#20123;&#25805;&#20316;&#27169;&#24335;&#19979;&#30340;&#20915;&#31574;&#31995;&#32479;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35745;&#31639;&#23494;&#38598;&#22411;&#24212;&#29992;&#36890;&#24120;&#22312;&#26102;&#38388;&#32422;&#26463;&#19979;&#25805;&#20316;&#65292;&#38656;&#35201;&#21152;&#36895;&#26041;&#27861;&#24182;&#23558;&#35745;&#31639;&#24037;&#20316;&#36127;&#36733;&#20998;&#24067;&#21040;&#22810;&#20010;&#23454;&#20307;&#19978;&#12290;&#28982;&#32780;&#65292;&#32467;&#26524;&#35201;&#20040;&#22312;&#25152;&#38656;&#26102;&#38388;&#20869;&#36798;&#21040;&#65292;&#35201;&#20040;&#27809;&#26377;&#65292;&#24182;&#19988;&#22312;&#21518;&#19968;&#31181;&#24773;&#20917;&#19979;&#65292;&#23453;&#36149;&#30340;&#36164;&#28304;&#34987;&#28010;&#36153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#23618;&#20998;&#36776;&#29575;&#35745;&#31639;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#20801;&#35768;&#22312;&#26368;&#32456;&#32467;&#26524;&#20043;&#21069;&#30340;&#26089;&#26399;&#38454;&#27573;&#33719;&#24471;&#36739;&#20302;&#20998;&#36776;&#29575;&#30340;&#32467;&#26524;&#12290;&#36825;&#31181;&#21019;&#26032;&#26174;&#33879;&#22686;&#24378;&#20102;&#22522;&#20110;&#25130;&#27490;&#26085;&#26399;&#30340;&#31995;&#32479;&#65292;&#22240;&#20026;&#22914;&#26524;&#30001;&#20110;&#26102;&#38388;&#38480;&#21046;&#32780;&#32456;&#27490;&#35745;&#31639;&#20219;&#21153;&#65292;&#21017;&#20173;&#21487;&#20197;&#29983;&#25104;&#26368;&#32456;&#32467;&#26524;&#30340;&#36817;&#20284;&#29256;&#26412;&#12290;&#27492;&#22806;&#65292;&#22312;&#26576;&#20123;&#25805;&#20316;&#27169;&#24335;&#19979;&#65292;&#39640;&#20998;&#36776;&#29575;&#30340;&#32467;&#26524;&#21487;&#33021;&#26159;&#19981;&#24517;&#35201;&#30340;&#65292;&#22240;&#20026;&#20302;&#20998;&#36776;&#29575;&#30340;&#32467;&#26524;&#21487;&#33021;&#24050;&#32463;&#19982;&#20915;&#31574;&#38408;&#20540;&#26174;&#33879;&#20559;&#31163;&#65292;&#20363;&#22914;&#22312;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20915;&#31574;&#31995;&#32479;&#20013;&#12290;&#22240;&#27492;&#65292;&#25805;&#20316;&#20154;&#21592;&#21487;&#20197;&#20915;&#23450;&#26159;&#21542;&#38656;&#35201;&#26356;&#39640;&#30340;&#20998;&#36776;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern computationally-intensive applications often operate under time constraints, necessitating acceleration methods and distribution of computational workloads across multiple entities. However, the outcome is either achieved within the desired timeline or not, and in the latter case, valuable resources are wasted. In this paper, we introduce solutions for layered-resolution computation. These solutions allow lower-resolution results to be obtained at an earlier stage than the final result. This innovation notably enhances the deadline-based systems, as if a computational job is terminated due to time constraints, an approximate version of the final result can still be generated. Moreover, in certain operational regimes, a high-resolution result might be unnecessary, because the low-resolution result may already deviate significantly from the decision threshold, for example in AI-based decision-making systems. Therefore, operators can decide whether higher resolution is needed or no
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SSD&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#26469;&#35299;&#20915;&#31163;&#32447;&#30446;&#26631;&#26465;&#20214;&#19979;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#20174;&#27425;&#20248;&#34892;&#20026;&#25968;&#25454;&#38598;&#23398;&#20064;&#31574;&#30053;&#30340;&#22256;&#38590;&#20197;&#21450;&#22312;&#23376;&#36712;&#36857;&#20013;&#25552;&#21462;&#26377;&#29992;&#25216;&#33021;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2402.07226</link><description>&lt;p&gt;
&#29992;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#25340;&#25509;&#23376;&#36712;&#36857;&#20197;&#23454;&#29616;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Stitching Sub-Trajectories with Conditional Diffusion Model for Goal-Conditioned Offline RL
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07226
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SSD&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#26469;&#35299;&#20915;&#31163;&#32447;&#30446;&#26631;&#26465;&#20214;&#19979;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#20174;&#27425;&#20248;&#34892;&#20026;&#25968;&#25454;&#38598;&#23398;&#20064;&#31574;&#30053;&#30340;&#22256;&#38590;&#20197;&#21450;&#22312;&#23376;&#36712;&#36857;&#20013;&#25552;&#21462;&#26377;&#29992;&#25216;&#33021;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;(Offline GCRL)&#26159;&#24378;&#21270;&#23398;&#20064;&#20013;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#20854;&#19987;&#27880;&#20110;&#20174;&#39044;&#20808;&#25910;&#38598;&#30340;&#34892;&#20026;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#21508;&#31181;&#38754;&#21521;&#30446;&#26631;&#30340;&#25216;&#33021;&#12290;&#22312;&#36825;&#31181;&#35774;&#23450;&#20013;&#65292;&#22870;&#21169;&#21453;&#39304;&#36890;&#24120;&#21482;&#22312;&#36798;&#25104;&#30446;&#26631;&#26102;&#23384;&#22312;&#65292;&#36825;&#20351;&#24471;&#20174;&#26377;&#38480;&#30340;&#27425;&#20248;&#34892;&#20026;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#31574;&#30053;&#21464;&#24471;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#29616;&#23454;&#22330;&#26223;&#28041;&#21450;&#38271;&#26102;&#31243;&#35268;&#21010;&#65292;&#36825;&#38656;&#35201;&#22312;&#23376;&#36712;&#36857;&#20013;&#25552;&#21462;&#26377;&#29992;&#30340;&#25216;&#33021;&#12290;&#26368;&#36817;&#65292;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#26159;&#29983;&#25104;&#24378;&#21270;&#23398;&#20064;&#39640;&#36136;&#37327;&#38271;&#26102;&#31243;&#35745;&#21010;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20123;&#26041;&#27861;&#20013;&#23384;&#22312;&#19968;&#20123;&#25216;&#26415;&#20551;&#35774;&#65292;&#20854;&#22312;&#30446;&#26631;&#26465;&#20214;&#35774;&#23450;&#19979;&#30340;&#23454;&#29992;&#24615;&#20173;&#28982;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SSD(&#29992;&#25193;&#25955;&#23454;&#29616;&#23376;&#36712;&#36857;&#25340;&#25509;)&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;GCRL&#26041;&#27861;&#65292;&#21033;&#29992;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline Goal-Conditioned Reinforcement Learning (Offline GCRL) is an important problem in RL that focuses on acquiring diverse goal-oriented skills solely from pre-collected behavior datasets. In this setting, the reward feedback is typically absent except when the goal is achieved, which makes it difficult to learn policies especially from a finite dataset of suboptimal behaviors. In addition, realistic scenarios involve long-horizon planning, which necessitates the extraction of useful skills within sub-trajectories. Recently, the conditional diffusion model has been shown to be a promising approach to generate high-quality long-horizon plans for RL. However, their practicality for the goal-conditioned setting is still limited due to a number of technical assumptions made by the methods. In this paper, we propose SSD (Sub-trajectory Stitching with Diffusion), a model-based offline GCRL method that leverages the conditional diffusion model to address these limitations. In summary, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#20195;&#29702;&#34892;&#20026;&#24847;&#22270;&#30340;&#25805;&#20316;&#21270;&#23450;&#20041;&#65292;&#24182;&#36890;&#36807;&#19968;&#20123;&#20363;&#23376;&#21644;&#32467;&#26524;&#23637;&#31034;&#20102;&#20854;&#28789;&#27963;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;&#36825;&#19968;&#23450;&#20041;&#26377;&#21161;&#20110;&#29702;&#35299;&#21644;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#34892;&#20026;&#65292;&#20197;&#21450;&#30456;&#20851;&#27010;&#24565;&#22914;&#24037;&#20855;&#24615;&#30446;&#26631;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.07221</link><description>&lt;p&gt;
&#20195;&#29702;&#34892;&#20026;&#30340;&#21407;&#22240;&#65306;&#24847;&#22270;&#21644;&#24037;&#20855;&#24615;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
The Reasons that Agents Act: Intention and Instrumental Goals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07221
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#20195;&#29702;&#34892;&#20026;&#24847;&#22270;&#30340;&#25805;&#20316;&#21270;&#23450;&#20041;&#65292;&#24182;&#36890;&#36807;&#19968;&#20123;&#20363;&#23376;&#21644;&#32467;&#26524;&#23637;&#31034;&#20102;&#20854;&#28789;&#27963;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;&#36825;&#19968;&#23450;&#20041;&#26377;&#21161;&#20110;&#29702;&#35299;&#21644;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#34892;&#20026;&#65292;&#20197;&#21450;&#30456;&#20851;&#27010;&#24565;&#22914;&#24037;&#20855;&#24615;&#30446;&#26631;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24847;&#22270;&#26159;&#20154;&#24037;&#26234;&#33021;&#20013;&#19968;&#20010;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27010;&#24565;&#12290;&#23427;&#20043;&#25152;&#20197;&#37325;&#35201;&#65292;&#26159;&#22240;&#20026;&#23427;&#26159;&#35768;&#22810;&#20854;&#20182;&#25105;&#20204;&#20851;&#24515;&#30340;&#27010;&#24565;&#30340;&#22522;&#30784;&#65292;&#20363;&#22914;&#20195;&#29702;&#12289;&#25805;&#32437;&#12289;&#27861;&#24459;&#36131;&#20219;&#21644;&#36131;&#22791;&#12290;&#28982;&#32780;&#65292;&#23558;&#24847;&#22270;&#24402;&#22240;&#20110;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26159;&#26377;&#20105;&#35758;&#30340;&#65292;&#24182;&#19988;&#27809;&#26377;&#26222;&#36941;&#25509;&#21463;&#30340;&#36866;&#29992;&#20110;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#30340;&#24847;&#22270;&#29702;&#35770;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20195;&#29702;&#20154;&#34892;&#20026;&#30340;&#24847;&#22270;&#36716;&#21270;&#20026;&#20854;&#36873;&#25321;&#20915;&#31574;&#30340;&#21407;&#22240;&#65292;&#23545;&#24847;&#22270;&#36827;&#34892;&#20102;&#25805;&#20316;&#21270;&#23450;&#20041;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#32467;&#26500;&#24615;&#22240;&#26524;&#24433;&#21709;&#27169;&#22411;&#20013;&#30340;&#24847;&#22270;&#23450;&#20041;&#65292;&#32039;&#23494;&#32467;&#21512;&#22312;&#21746;&#23398;&#25991;&#29486;&#20013;&#20851;&#20110;&#24847;&#22270;&#30340;&#35266;&#28857;&#65292;&#24182;&#36866;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#12290;&#36890;&#36807;&#19968;&#20123;&#20363;&#23376;&#21644;&#32467;&#26524;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#23450;&#20041;&#25429;&#25417;&#21040;&#20102;&#30452;&#35266;&#30340;&#24847;&#22270;&#27010;&#24565;&#65292;&#24182;&#31526;&#21512;&#36807;&#21435;&#30740;&#31350;&#30340;&#35774;&#23450;&#35201;&#27714;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#23450;&#20041;&#19982;&#36807;&#21435;&#27010;&#24565;&#30340;&#20851;&#32852;&#65292;&#21253;&#25324;&#23454;&#38469;&#22240;&#26524;&#24615;&#21644;&#24037;&#20855;&#24615;&#30446;&#26631;&#30340;&#27010;&#24565;&#65292;&#21518;&#32773;&#26159;&#23433;&#20840;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#30740;&#31350;&#20013;&#30340;&#26680;&#24515;&#24605;&#24819;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#23450;&#20041;&#22914;&#20309;&#19982;&#23454;&#38469;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#23454;&#36341;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intention is an important and challenging concept in AI. It is important because it underlies many other concepts we care about, such as agency, manipulation, legal responsibility, and blame. However, ascribing intent to AI systems is contentious, and there is no universally accepted theory of intention applicable to AI agents. We operationalise the intention with which an agent acts, relating to the reasons it chooses its decision. We introduce a formal definition of intention in structural causal influence models, grounded in the philosophy literature on intent and applicable to real-world machine learning systems. Through a number of examples and results, we show that our definition captures the intuitive notion of intent and satisfies desiderata set-out by past work. In addition, we show how our definition relates to past concepts, including actual causality, and the notion of instrumental goals, which is a core idea in the literature on safe AI agents. Finally, we demonstrate how 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Open-domain Urban Itinerary Planning (OUIP)&#20219;&#21153;&#65292;&#29992;&#20110;&#26681;&#25454;&#29992;&#25143;&#20197;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#35831;&#27714;&#30452;&#25509;&#29983;&#25104;&#34892;&#31243;&#65292;&#36890;&#36807;&#32467;&#21512;&#31354;&#38388;&#20248;&#21270;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#65292;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#22478;&#24066;&#34892;&#31243;&#23450;&#21046;&#26381;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.07204</link><description>&lt;p&gt;
&#32467;&#21512;&#31354;&#38388;&#20248;&#21270;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#25918;&#39046;&#22495;&#22478;&#24066;&#34892;&#31243;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Synergizing Spatial Optimization with Large Language Models for Open-Domain Urban Itinerary Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Open-domain Urban Itinerary Planning (OUIP)&#20219;&#21153;&#65292;&#29992;&#20110;&#26681;&#25454;&#29992;&#25143;&#20197;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#35831;&#27714;&#30452;&#25509;&#29983;&#25104;&#34892;&#31243;&#65292;&#36890;&#36807;&#32467;&#21512;&#31354;&#38388;&#20248;&#21270;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#65292;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#22478;&#24066;&#34892;&#31243;&#23450;&#21046;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;Open-domain Urban Itinerary Planning (OUIP)&#20219;&#21153;&#65292;&#29992;&#20110;&#26681;&#25454;&#29992;&#25143;&#20197;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#35831;&#27714;&#30452;&#25509;&#29983;&#25104;&#34892;&#31243;&#12290;OUIP&#19982;&#20256;&#32479;&#34892;&#31243;&#35268;&#21010;&#19981;&#21516;&#65292;&#20256;&#32479;&#35268;&#21010;&#38480;&#21046;&#20102;&#29992;&#25143;&#34920;&#36798;&#26356;&#35814;&#32454;&#30340;&#38656;&#27714;&#65292;&#38459;&#30861;&#20102;&#30495;&#27491;&#30340;&#20010;&#24615;&#21270;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#22788;&#29702;&#22810;&#26679;&#21270;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38750;&#23454;&#26102;&#20449;&#24687;&#12289;&#19981;&#23436;&#25972;&#30340;&#30693;&#35782;&#21644;&#19981;&#36275;&#30340;&#31354;&#38388;&#24847;&#35782;&#65292;&#23427;&#20204;&#26080;&#27861;&#29420;&#31435;&#22320;&#25552;&#20379;&#28385;&#24847;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ItiNera&#30340;OUIP&#31995;&#32479;&#65292;&#23558;&#31354;&#38388;&#20248;&#21270;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30456;&#32467;&#21512;&#65292;&#26681;&#25454;&#29992;&#25143;&#38656;&#27714;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#22478;&#24066;&#34892;&#31243;&#23450;&#21046;&#26381;&#21153;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#27969;&#27700;&#32447;&#65292;&#29992;&#20110;&#25552;&#21462;&#21644;&#26356;&#26032;&#20852;&#36259;&#28857;&#29305;&#24449;&#65292;&#20197;&#21019;&#24314;&#29992;&#25143;&#33258;&#24049;&#30340;&#20010;&#24615;&#21270;&#20852;&#36259;&#28857;&#25968;&#25454;&#24211;&#12290;&#23545;&#20110;&#27599;&#20010;&#29992;&#25143;&#35831;&#27714;&#65292;&#25105;&#20204;&#21033;&#29992;LLM&#36827;&#34892;&#21327;&#21516;&#23454;&#29616;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we for the first time propose the task of Open-domain Urban Itinerary Planning (OUIP) for citywalk, which directly generates itineraries based on users' requests described in natural language. OUIP is different from conventional itinerary planning, which limits users from expressing more detailed needs and hinders true personalization. Recently, large language models (LLMs) have shown potential in handling diverse tasks. However, due to non-real-time information, incomplete knowledge, and insufficient spatial awareness, they are unable to independently deliver a satisfactory user experience in OUIP. Given this, we present ItiNera, an OUIP system that synergizes spatial optimization with Large Language Models (LLMs) to provide services that customize urban itineraries based on users' needs. Specifically, we develop an LLM-based pipeline for extracting and updating POI features to create a user-owned personalized POI database. For each user request, we leverage LLM in coop
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38142;&#25509;&#24863;&#30693;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#21382;&#21490;&#38142;&#25509;&#21644;&#26597;&#35810;&#38142;&#25509;&#19968;&#36215;&#36755;&#20837;&#27169;&#22411;&#23618;&#26469;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#27169;&#22411;&#20851;&#27880;&#38142;&#25509;&#28436;&#21270;&#27169;&#24335;&#30340;&#24314;&#27169;&#32780;&#38750;&#33410;&#28857;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#20845;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.07199</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#24335;&#35782;&#21035;&#30340;&#26102;&#38388;&#22270;&#38142;&#25509;&#24863;&#30693;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Link-aware link prediction over temporal graph by pattern recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07199
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38142;&#25509;&#24863;&#30693;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#21382;&#21490;&#38142;&#25509;&#21644;&#26597;&#35810;&#38142;&#25509;&#19968;&#36215;&#36755;&#20837;&#27169;&#22411;&#23618;&#26469;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#27169;&#22411;&#20851;&#27880;&#38142;&#25509;&#28436;&#21270;&#27169;&#24335;&#30340;&#24314;&#27169;&#32780;&#38750;&#33410;&#28857;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#20845;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#26102;&#38388;&#22270;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#31995;&#21015;&#38142;&#25509;&#30340;&#27969;&#65292;&#27599;&#20010;&#38142;&#25509;&#20195;&#34920;&#20102;&#20004;&#20010;&#33410;&#28857;&#22312;&#26576;&#20010;&#26102;&#38388;&#28857;&#30340;&#20132;&#20114;&#12290;&#22312;&#26102;&#38388;&#22270;&#19978;&#65292;&#38142;&#25509;&#39044;&#27979;&#26159;&#19968;&#20010;&#24120;&#35265;&#20219;&#21153;&#65292;&#26088;&#22312;&#22238;&#31572;&#26597;&#35810;&#38142;&#25509;&#26159;&#21542;&#20026;&#30495;&#12290;&#20026;&#20102;&#23436;&#25104;&#36825;&#20010;&#20219;&#21153;&#65292;&#20043;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#19987;&#27880;&#20110;&#23398;&#20064;&#26597;&#35810;&#38142;&#25509;&#20013;&#20004;&#20010;&#33410;&#28857;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#25351;&#20986;&#65292;&#30001;&#20110;&#20043;&#21069;&#30340;&#27169;&#22411;&#27809;&#26377;&#21033;&#29992;&#26597;&#35810;&#38142;&#25509;&#30340;&#20449;&#24687;&#65292;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#21487;&#33021;&#20250;&#32534;&#30721;&#36807;&#22810;&#30340;&#20449;&#24687;&#24182;&#23545;&#38142;&#25509;&#39044;&#27979;&#20135;&#29983;&#21103;&#20316;&#29992;&#65292;&#21363;&#23427;&#20204;&#19981;&#20855;&#26377;&#38142;&#25509;&#24863;&#30693;&#24615;&#12290;&#22522;&#20110;&#36825;&#20010;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38142;&#25509;&#24863;&#30693;&#27169;&#22411;&#65306;&#21382;&#21490;&#38142;&#25509;&#21644;&#26597;&#35810;&#38142;&#25509;&#19968;&#36215;&#36755;&#20837;&#21040;&#27169;&#22411;&#23618;&#20013;&#65292;&#20197;&#21306;&#20998;&#35813;&#36755;&#20837;&#26159;&#21542;&#26263;&#31034;&#20102;&#19968;&#20010;&#20197;&#26597;&#35810;&#38142;&#25509;&#32467;&#23614;&#30340;&#21512;&#29702;&#27169;&#24335;&#12290;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#38142;&#25509;&#28436;&#21270;&#27169;&#24335;&#30340;&#24314;&#27169;&#65292;&#32780;&#19981;&#26159;&#33410;&#28857;&#34920;&#31034;&#12290;&#22312;&#20845;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
A temporal graph can be considered as a stream of links, each of which represents an interaction between two nodes at a certain time. On temporal graphs, link prediction is a common task, which aims to answer whether the query link is true or not. To do this task, previous methods usually focus on the learning of representations of the two nodes in the query link. We point out that the learned representation by their models may encode too much information with side effects for link prediction because they have not utilized the information of the query link, i.e., they are link-unaware. Based on this observation, we propose a link-aware model: historical links and the query link are input together into the following model layers to distinguish whether this input implies a reasonable pattern that ends with the query link. During this process, we focus on the modeling of link evolution patterns rather than node representations. Experiments on six datasets show that our model achieves stro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#22270;&#19981;&#21464;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#31232;&#30095;&#24615;&#12289;&#36719;&#24615;&#21644;&#21487;&#24494;&#24615;&#21407;&#21017;&#26469;&#25552;&#21462;&#19981;&#21464;&#23376;&#22270;&#65292;&#20174;&#32780;&#25552;&#39640;&#22270;&#23398;&#20064;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.07191</link><description>&lt;p&gt;
GSINA: &#36890;&#36807;&#22270;Sinkhorn Attention&#25913;&#36827;&#22270;&#19981;&#21464;&#23398;&#20064;&#20013;&#30340;&#23376;&#22270;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
GSINA: Improving Subgraph Extraction for Graph Invariant Learning via Graph Sinkhorn Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07191
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#22270;&#19981;&#21464;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#31232;&#30095;&#24615;&#12289;&#36719;&#24615;&#21644;&#21487;&#24494;&#24615;&#21407;&#21017;&#26469;&#25552;&#21462;&#19981;&#21464;&#23376;&#22270;&#65292;&#20174;&#32780;&#25552;&#39640;&#22270;&#23398;&#20064;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#19981;&#21464;&#23398;&#20064;(GIL)&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#19981;&#21516;&#20998;&#24067;&#21464;&#21270;&#19979;&#21457;&#29616;&#22270;&#25968;&#25454;&#19982;&#20854;&#26631;&#31614;&#20043;&#38388;&#30340;&#19981;&#21464;&#20851;&#31995;&#65292;&#20197;&#35299;&#20915;&#21508;&#31181;&#22270;&#23398;&#20064;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;GIL&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20174;&#36755;&#20837;&#22270;&#20013;&#25552;&#21462;&#19981;&#21464;&#23376;&#22270;&#65292;&#20316;&#20026;&#35268;&#21017;&#21270;&#31574;&#30053;&#26469;&#25552;&#39640;&#22270;&#23398;&#20064;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#33719;&#21462;&#19981;&#21464;&#23376;&#22270;&#26041;&#38754;&#20063;&#23384;&#22312;&#21508;&#31181;&#38480;&#21046;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#29616;&#26377;&#24037;&#20316;&#30340;&#32570;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#25552;&#21462;&#19981;&#21464;&#23376;&#22270;&#30340;&#30456;&#24212;&#21407;&#21017;&#65306;1&#65289;&#31232;&#30095;&#24615;&#65292;&#20197;&#36807;&#28388;&#25481;&#21464;&#24322;&#29305;&#24449;&#65307;2&#65289;&#36719;&#24615;&#65292;&#20197;&#33719;&#24471;&#26356;&#24191;&#27867;&#30340;&#35299;&#31354;&#38388;&#65307;&#21644;3&#65289;&#21487;&#24494;&#24615;&#65292;&#20197;&#36827;&#34892;&#31471;&#21040;&#31471;&#20248;&#21270;&#12290;&#20026;&#20102;&#22312;&#19968;&#27425;&#25805;&#20316;&#20013;&#28385;&#36275;&#36825;&#20123;&#21407;&#21017;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#20248;&#20256;&#36755;(OT)&#29702;&#35770;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#27880;&#24847;&#26426;&#21046;&#65292;&#31216;&#20026;&#22270;Sinkhorn Attention&#65288;G)
&lt;/p&gt;
&lt;p&gt;
Graph invariant learning (GIL) has been an effective approach to discovering the invariant relationships between graph data and its labels for different graph learning tasks under various distribution shifts. Many recent endeavors of GIL focus on extracting the invariant subgraph from the input graph for prediction as a regularization strategy to improve the generalization performance of graph learning. Despite their success, such methods also have various limitations in obtaining their invariant subgraphs. In this paper, we provide in-depth analyses of the drawbacks of existing works and propose corresponding principles of our invariant subgraph extraction: 1) the sparsity, to filter out the variant features, 2) the softness, for a broader solution space, and 3) the differentiability, for a soundly end-to-end optimization. To meet these principles in one shot, we leverage the Optimal Transport (OT) theory and propose a novel graph attention mechanism called Graph Sinkhorn Attention (G
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#35270;&#35273;Transformer&#26500;&#36896;&#19968;&#20010;&#38543;&#26426;&#38598;&#25104;&#30340;&#21152;&#23494;&#27169;&#22411;&#65292;&#20197;&#22686;&#24378;&#23545;&#25239;&#30333;&#30418;&#21644;&#40657;&#30418;&#25915;&#20987;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#20010;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#20854;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#40065;&#26834;&#24615;&#21644;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.07183</link><description>&lt;p&gt;
&#19968;&#20010;&#21152;&#23494;&#35270;&#35273;Transformer&#30340;&#38543;&#26426;&#38598;&#25104;&#26041;&#27861;&#29992;&#20110;&#23545;&#25239;&#24615;&#24378;&#21270;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
A Random Ensemble of Encrypted Vision Transformers for Adversarially Robust Defense
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#35270;&#35273;Transformer&#26500;&#36896;&#19968;&#20010;&#38543;&#26426;&#38598;&#25104;&#30340;&#21152;&#23494;&#27169;&#22411;&#65292;&#20197;&#22686;&#24378;&#23545;&#25239;&#30333;&#30418;&#21644;&#40657;&#30418;&#25915;&#20987;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#20010;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#20854;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#40065;&#26834;&#24615;&#21644;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#24050;&#32463;&#34987;&#20844;&#35748;&#20026;&#26131;&#21463;&#23545;&#25239;&#24615;&#26679;&#26412;&#65288;AEs&#65289;&#30340;&#25915;&#20987;&#12290;&#22312;&#20043;&#21069;&#30340;&#30740;&#31350;&#20013;&#65292;&#20351;&#29992;&#31192;&#23494;&#23494;&#38053;&#21152;&#23494;&#30340;&#27169;&#22411;&#34987;&#35777;&#26126;&#21487;&#20197;&#25269;&#25239;&#30333;&#30418;&#25915;&#20987;&#65292;&#20294;&#23545;&#25239;&#40657;&#30418;&#25915;&#20987;&#21017;&#19981;&#34892;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#35270;&#35273;Transformer&#65288;ViT&#65289;&#26500;&#36896;&#19968;&#20010;&#38543;&#26426;&#38598;&#25104;&#30340;&#21152;&#23494;&#27169;&#22411;&#65292;&#20197;&#22686;&#24378;&#23545;&#25239;&#30333;&#30418;&#21644;&#40657;&#30418;&#25915;&#20987;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#36824;&#20351;&#29992;&#20102;&#19968;&#20010;&#21517;&#20026;AutoAttack&#30340;&#22522;&#20934;&#25915;&#20987;&#26041;&#27861;&#26469;&#23458;&#35266;&#27979;&#35797;&#27169;&#22411;&#30340;&#23545;&#25239;&#24615;&#31283;&#20581;&#24615;&#12290;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;CIFAR-10&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#30340;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#19981;&#20165;&#23545;&#25239;&#30333;&#30418;&#25915;&#20987;&#20855;&#26377;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#65292;&#36824;&#23545;&#25239;&#40657;&#30418;&#25915;&#20987;&#20063;&#20855;&#26377;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#12290;&#35813;&#26041;&#27861;&#36824;&#22312;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#26631;&#20934;&#21270;&#22522;&#20934;RobustBench&#20013;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#39564;&#35777;&#20102;&#22312;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#38450;&#24481;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) are well known to be vulnerable to adversarial examples (AEs). In previous studies, the use of models encrypted with a secret key was demonstrated to be robust against white-box attacks, but not against black-box ones. In this paper, we propose a novel method using the vision transformer (ViT) that is a random ensemble of encrypted models for enhancing robustness against both white-box and black-box attacks. In addition, a benchmark attack method, called AutoAttack, is applied to models to test adversarial robustness objectively. In experiments, the method was demonstrated to be robust against not only white-box attacks but also black-box ones in an image classification task on the CIFAR-10 and ImageNet datasets. The method was also compared with the state-of-the-art in a standardized benchmark for adversarial robustness, RobustBench, and it was verified to outperform conventional defenses in terms of clean accuracy and robust accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MAGNETO&#30340;&#36793;&#32536;AI&#24179;&#21488;&#65292;&#36890;&#36807;&#20174;&#20113;&#31471;&#25512;&#21521;&#36793;&#32536;&#36827;&#34892;&#22686;&#37327;&#20154;&#20307;&#27963;&#21160;&#23398;&#20064;&#65292;&#36991;&#20813;&#20102;&#20113;&#31471;&#19982;&#36793;&#32536;&#35774;&#22791;&#20043;&#38388;&#30340;&#25968;&#25454;&#20256;&#36755;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#12289;&#20302;&#24310;&#36831;&#22788;&#29702;&#21644;&#39640;&#24230;&#20010;&#24615;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.07180</link><description>&lt;p&gt;
MAGNETO&#65306;&#36793;&#32536;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#30340;&#36793;&#32536;AI--&#38544;&#31169;&#21644;&#20010;&#24615;&#21270;
&lt;/p&gt;
&lt;p&gt;
MAGNETO: Edge AI for Human Activity Recognition -- Privacy and Personalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07180
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MAGNETO&#30340;&#36793;&#32536;AI&#24179;&#21488;&#65292;&#36890;&#36807;&#20174;&#20113;&#31471;&#25512;&#21521;&#36793;&#32536;&#36827;&#34892;&#22686;&#37327;&#20154;&#20307;&#27963;&#21160;&#23398;&#20064;&#65292;&#36991;&#20813;&#20102;&#20113;&#31471;&#19982;&#36793;&#32536;&#35774;&#22791;&#20043;&#38388;&#30340;&#25968;&#25454;&#20256;&#36755;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#12289;&#20302;&#24310;&#36831;&#22788;&#29702;&#21644;&#39640;&#24230;&#20010;&#24615;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#26159;&#19968;&#20010;&#25104;&#29087;&#30340;&#39046;&#22495;&#65292;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#26174;&#33879;&#25512;&#21160;&#20102;&#20854;&#21457;&#23637;&#12290;&#23613;&#31649;&#20844;&#21496;&#25104;&#21151;&#22320;&#23558;HAR&#25972;&#21512;&#21040;&#28040;&#36153;&#21697;&#20013;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#20381;&#36182;&#20110;&#39044;&#23450;&#20041;&#30340;&#27963;&#21160;&#38598;&#65292;&#36825;&#38480;&#21046;&#20102;&#29992;&#25143;&#32423;&#65288;&#36793;&#32536;&#35774;&#22791;&#65289;&#30340;&#20010;&#24615;&#21270;&#12290;&#23613;&#31649;&#22312;&#22686;&#37327;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#33021;&#22815;&#20351;&#29992;&#26032;&#25968;&#25454;&#26356;&#26032;&#27169;&#22411;&#65292;&#20294;&#36825;&#36890;&#24120;&#21457;&#29983;&#22312;&#20113;&#31471;&#65292;&#38656;&#35201;&#23450;&#26399;&#22312;&#20113;&#31471;&#21644;&#36793;&#32536;&#35774;&#22791;&#20043;&#38388;&#36827;&#34892;&#25968;&#25454;&#20256;&#36755;&#65292;&#20174;&#32780;&#24341;&#21457;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MAGNETO&#30340;&#36793;&#32536;AI&#24179;&#21488;&#65292;&#23558;HAR&#20219;&#21153;&#20174;&#20113;&#31471;&#25512;&#21521;&#36793;&#32536;&#12290;MAGNETO&#20801;&#35768;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#30452;&#25509;&#36827;&#34892;&#22686;&#37327;&#20154;&#20307;&#27963;&#21160;&#23398;&#20064;&#65292;&#32780;&#26080;&#38656;&#19982;&#20113;&#31471;&#36827;&#34892;&#20219;&#20309;&#25968;&#25454;&#20132;&#25442;&#12290;&#36825;&#21487;&#20197;&#25552;&#20379;&#24378;&#22823;&#30340;&#38544;&#31169;&#20445;&#35777;&#12289;&#20302;&#22788;&#29702;&#24310;&#36831;&#21644;&#39640;&#24230;&#30340;&#20010;&#24615;&#21270;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#22312;Android&#35774;&#22791;&#19978;&#28436;&#31034;&#20102;MAGNETO&#65292;&#20174;&#25968;&#25454;&#37319;&#38598;&#21040;&#32467;&#26524;&#21487;&#35270;&#21270;&#65292;&#39564;&#35777;&#20102;&#25972;&#20010;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human activity recognition (HAR) is a well-established field, significantly advanced by modern machine learning (ML) techniques. While companies have successfully integrated HAR into consumer products, they typically rely on a predefined activity set, which limits personalizations at the user level (edge devices). Despite advancements in Incremental Learning for updating models with new data, this often occurs on the Cloud, necessitating regular data transfers between cloud and edge devices, thus leading to data privacy issues. In this paper, we propose MAGNETO, an Edge AI platform that pushes HAR tasks from the Cloud to the Edge. MAGNETO allows incremental human activity learning directly on the Edge devices, without any data exchange with the Cloud. This enables strong privacy guarantees, low processing latency, and a high degree of personalization for users. In particular, we demonstrate MAGNETO in an Android device, validating the whole pipeline from data collection to result visua
&lt;/p&gt;</description></item><item><title>EmoWear&#26159;&#19968;&#31181;&#26234;&#33021;&#25163;&#34920;&#35821;&#38899;&#20449;&#24687;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#21160;&#30011;&#25552;&#31034;&#21453;&#26144;&#24773;&#24863;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#24773;&#24863;&#20132;&#27969;&#20307;&#39564;&#12290;</title><link>https://arxiv.org/abs/2402.07174</link><description>&lt;p&gt;
EmoWear: &#25506;&#32034;&#26234;&#33021;&#25163;&#34920;&#19978;&#22768;&#38899;&#20449;&#24687;&#20132;&#20114;&#30340;&#24773;&#24863;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
EmoWear: Exploring Emotional Teasers for Voice Message Interaction on Smartwatches
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07174
&lt;/p&gt;
&lt;p&gt;
EmoWear&#26159;&#19968;&#31181;&#26234;&#33021;&#25163;&#34920;&#35821;&#38899;&#20449;&#24687;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#21160;&#30011;&#25552;&#31034;&#21453;&#26144;&#24773;&#24863;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#24773;&#24863;&#20132;&#27969;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#20449;&#24687;&#26412;&#36136;&#19978;&#20351;&#29992;&#25143;&#26080;&#27861;&#22312;&#23436;&#20840;&#21548;&#21462;&#38899;&#39057;&#20869;&#23481;&#20043;&#21069;&#21028;&#26029;&#24773;&#24863;&#35821;&#35843;&#65292;&#36825;&#38459;&#30861;&#20102;&#22312;&#39044;&#26816;&#32034;&#38454;&#27573;&#30340;&#20849;&#20139;&#24773;&#24863;&#20307;&#39564;&#12290;&#30740;&#31350;&#24456;&#23569;&#25506;&#32034;"&#24773;&#24863;&#25552;&#31034;"&#8212;&#8212;&#22312;&#19981;&#36879;&#38706;&#20869;&#23481;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#26377;&#20851;&#24453;&#25910;&#21548;&#20449;&#24687;&#24773;&#24863;&#35821;&#35843;&#30340;&#26263;&#31034;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;EmoWear&#65292;&#19968;&#31181;&#26234;&#33021;&#25163;&#34920;&#35821;&#38899;&#20449;&#24687;&#31995;&#32479;&#65292;&#20801;&#35768;&#29992;&#25143;&#22312;&#20449;&#24687;&#27668;&#27873;&#19978;&#24212;&#29992;30&#31181;&#21160;&#30011;&#25552;&#31034;&#26469;&#21453;&#26144;&#24773;&#24863;&#12290;EmoWear&#36890;&#36807;&#22522;&#20110;&#35821;&#20041;&#21644;&#22768;&#23398;&#22788;&#29702;&#30340;&#26041;&#24335;&#26469;&#24110;&#21161;&#21457;&#36865;&#32773;&#36873;&#25321;&#24773;&#24863;&#12290;&#36890;&#36807;&#23558;EmoWear&#19982;&#20351;&#29992;&#24425;&#33394;&#20449;&#24687;&#27668;&#27873;&#20316;&#20026;&#24773;&#24863;&#25552;&#31034;&#30340;&#38236;&#20687;&#31995;&#32479;&#36827;&#34892;&#27604;&#36739;(N=24)&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;EmoWear&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;EmoWear&#26174;&#33879;&#22686;&#24378;&#20102;&#25509;&#25910;&#21644;&#21457;&#36865;&#28040;&#24687;&#26102;&#30340;&#24773;&#24863;&#20132;&#27969;&#20307;&#39564;&#12290;&#21160;&#30011;&#25552;&#31034;&#34987;&#35748;&#20026;&#30452;&#35266;&#19988;&#20855;&#26377;&#22810;&#31181;&#34920;&#36798;&#26041;&#24335;&#12290;&#25105;&#20204;&#24635;&#32467;&#20986;&#20102;&#21487;&#34892;&#30340;&#20132;&#20114;&#36136;&#37327;&#21644;&#23454;&#38469;&#24212;&#29992;&#30340;&#26410;&#26469;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Voice messages, by nature, prevent users from gauging the emotional tone without fully diving into the audio content. This hinders the shared emotional experience at the pre-retrieval stage. Research scarcely explored "Emotional Teasers"-pre-retrieval cues offering a glimpse into an awaiting message's emotional tone without disclosing its content. We introduce EmoWear, a smartwatch voice messaging system enabling users to apply 30 animation teasers on message bubbles to reflect emotions. EmoWear eases senders' choice by prioritizing emotions based on semantic and acoustic processing. EmoWear was evaluated in comparison with a mirroring system using color-coded message bubbles as emotional cues (N=24). Results showed EmoWear significantly enhanced emotional communication experience in both receiving and sending messages. The animated teasers were considered intuitive and valued for diverse expressions. Desirable interaction qualities and practical implications are distilled for future d
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#21058;&#37327;&#23481;&#31215;&#30452;&#26041;&#22270;&#65288;DVH&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38750;&#32467;&#26500;&#21270;&#22270;&#20687;&#36716;&#21270;&#20026;&#32467;&#26500;&#21270;&#22270;&#65292;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32534;&#30721;&#22788;&#26041;&#21644;&#20020;&#24202;&#21307;&#29983;&#30340;&#25351;&#31034;&#65292;&#23454;&#29616;&#20102;&#23545;&#25918;&#23556;&#27835;&#30103;&#35745;&#21010;&#36136;&#37327;&#30340;&#25552;&#39640;&#12290;</title><link>https://arxiv.org/abs/2402.07167</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35843;&#24378;&#25918;&#23556;&#27835;&#30103;&#21058;&#37327;&#23481;&#31215;&#30452;&#26041;&#22270;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Large-Language-Model Empowered Dose Volume Histogram Prediction for Intensity Modulated Radiotherapy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07167
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#21058;&#37327;&#23481;&#31215;&#30452;&#26041;&#22270;&#65288;DVH&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38750;&#32467;&#26500;&#21270;&#22270;&#20687;&#36716;&#21270;&#20026;&#32467;&#26500;&#21270;&#22270;&#65292;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32534;&#30721;&#22788;&#26041;&#21644;&#20020;&#24202;&#21307;&#29983;&#30340;&#25351;&#31034;&#65292;&#23454;&#29616;&#20102;&#23545;&#25918;&#23556;&#27835;&#30103;&#35745;&#21010;&#36136;&#37327;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27835;&#30103;&#35745;&#21010;&#26159;&#25918;&#23556;&#27835;&#30103;&#20013;&#19968;&#39033;&#32791;&#26102;&#12289;&#36164;&#28304;&#23494;&#38598;&#30340;&#20855;&#20307;&#24739;&#32773;&#24037;&#20316;&#12290;&#21058;&#37327;&#23481;&#31215;&#30452;&#26041;&#22270;&#65288;DVH&#65289;&#39044;&#27979;&#22312;&#33258;&#21160;&#21270;&#27492;&#36807;&#31243;&#20013;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#12290;&#25918;&#23556;&#27835;&#30103;&#35745;&#21010;&#20013;DVH&#19982;&#39118;&#38505;&#22120;&#23448;&#65288;OAR&#65289;&#21644;&#35745;&#21010;&#38774;&#20307;&#20307;&#31215;&#65288;PTV&#65289;&#30340;&#20960;&#20309;&#20851;&#31995;&#24050;&#32463;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#24314;&#31435;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20351;&#29992;&#22270;&#20687;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20419;&#36827;&#20154;&#31867;&#24178;&#39044;&#20197;&#25552;&#39640;&#35745;&#21010;&#36136;&#37327;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#38750;&#32467;&#26500;&#21270;&#22270;&#20687;&#36716;&#21270;&#20026;&#30001;&#22270;&#20687;&#22359;&#33410;&#28857;&#21644;&#21058;&#37327;&#33410;&#28857;&#32452;&#25104;&#30340;&#32467;&#26500;&#21270;&#22270;&#30340;&#27969;&#31243;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21058;&#37327;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DoseGNN&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#32467;&#26500;&#21270;&#22270;&#20013;&#39044;&#27979;DVH&#12290;&#25152;&#25552;&#20986;&#30340;DoseGNN&#36890;&#36807;LLM&#22686;&#24378;&#65292;&#20197;&#32534;&#30721;&#26469;&#33258;&#22788;&#26041;&#21644;&#20020;&#24202;&#21307;&#29983;&#30340;&#20132;&#20114;&#25351;&#31034;&#30340;&#22823;&#37327;&#30693;&#35782;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22312;&#32447;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#65288;OHAC&#65289;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Treatment planning is currently a patient specific, time-consuming, and resource demanding task in radiotherapy. Dose-volume histogram (DVH) prediction plays a critical role in automating this process. The geometric relationship between DVHs in radiotherapy plans and organs-at-risk (OAR) and planning target volume (PTV) has been well established. This study explores the potential of deep learning models for predicting DVHs using images and subsequent human intervention facilitated by a large-language model (LLM) to enhance the planning quality. We propose a pipeline to convert unstructured images to a structured graph consisting of image-patch nodes and dose nodes. A novel Dose Graph Neural Network (DoseGNN) model is developed for predicting DVHs from the structured graph. The proposed DoseGNN is enhanced with the LLM to encode massive knowledge from prescriptions and interactive instructions from clinicians. In this study, we introduced an online human-AI collaboration (OHAC) system a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21457;&#23637;&#30340;&#20840;&#26223;&#35270;&#22270;&#65292;&#24182;&#25351;&#20986;&#20102;&#27602;&#24615;&#12289;&#20559;&#35265;&#21644;&#20854;&#20182;&#38382;&#39064;&#65292;&#21516;&#26102;&#24378;&#35843;&#20154;&#31867;&#22823;&#33041;&#24182;&#19981;&#29305;&#27530;&#65292;&#20154;&#31867;&#26234;&#33021;&#21482;&#26159;&#19968;&#20010;&#23610;&#24230;&#19978;&#30340;&#26032;&#20852;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2402.07166</link><description>&lt;p&gt;
&#21457;&#34920;&#25991;&#26412;&#30340;&#31038;&#20250;&#28436;&#21270;&#19982;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20986;&#29616;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#27602;&#24615;&#21644;&#20559;&#35265;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Social Evolution of Published Text and The Emergence of Artificial Intelligence Through Large Language Models and The Problem of Toxicity and Bias
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21457;&#23637;&#30340;&#20840;&#26223;&#35270;&#22270;&#65292;&#24182;&#25351;&#20986;&#20102;&#27602;&#24615;&#12289;&#20559;&#35265;&#21644;&#20854;&#20182;&#38382;&#39064;&#65292;&#21516;&#26102;&#24378;&#35843;&#20154;&#31867;&#22823;&#33041;&#24182;&#19981;&#29305;&#27530;&#65292;&#20154;&#31867;&#26234;&#33021;&#21482;&#26159;&#19968;&#20010;&#23610;&#24230;&#19978;&#30340;&#26032;&#20852;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#24555;&#36895;&#21457;&#23637;&#30340;&#20840;&#26223;&#35270;&#22270;&#65292;&#36825;&#23548;&#33268;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31361;&#30772;&#24615;&#20986;&#29616;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#23558;&#25152;&#26377;&#36825;&#20123;&#21457;&#23637;&#32622;&#20110;&#23454;&#29992;&#30340;&#24191;&#27867;&#21382;&#21490;&#31038;&#20250;&#35270;&#35282;&#20013;&#65292;&#26082;&#19981;&#22840;&#22823;&#20854;&#25928;&#26524;&#65292;&#21448;&#19981;&#20135;&#29983;1970&#24180;&#20195;&#21040;1990&#24180;&#20195;&#20154;&#24037;&#26234;&#33021;&#20908;&#23395;&#30340;&#24754;&#35266;&#24773;&#32490;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25351;&#20986;&#23384;&#22312;&#27602;&#24615;&#12289;&#20559;&#35265;&#12289;&#35760;&#24518;&#12289;&#35844;&#23194;&#12289;&#36923;&#36753;&#19981;&#19968;&#33268;&#21644;&#24187;&#35273;&#31561;&#38382;&#39064;&#65292;&#21482;&#26159;&#20316;&#20026;&#23545;&#36807;&#20110;&#20048;&#35266;&#30340;&#35686;&#31034;&#12290;&#25105;&#20204;&#22312;&#27492;&#25351;&#20986;&#65292;&#27491;&#22914;&#20154;&#24037;&#26234;&#33021;&#30340;&#20986;&#29616;&#20284;&#20046;&#21457;&#29983;&#22312;&#31070;&#32463;&#36830;&#25509;&#25110;&#26435;&#37325;&#25968;&#37327;&#30340;&#20020;&#30028;&#28857;&#19978;&#65292;&#20154;&#31867;&#22823;&#33041;&#23588;&#20854;&#26159;&#30382;&#36136;&#21306;&#22495;&#24182;&#27809;&#26377;&#20160;&#20040;&#29305;&#27530;&#25110;&#36229;&#20961;&#65292;&#21482;&#26159;&#28789;&#38271;&#31867;&#22823;&#33041;&#30340;&#19968;&#20010;&#25918;&#22823;&#29256;&#65292;&#29978;&#33267;&#20154;&#31867;&#26234;&#33021;&#20284;&#20046;&#21482;&#26159;&#19968;&#20010;&#23610;&#24230;&#19978;&#30340;&#26032;&#20852;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide a birds eye view of the rapid developments in AI and Deep Learning that has led to the path-breaking emergence of AI in Large Language Models. The aim of this study is to place all these developments in a pragmatic broader historical social perspective without any exaggerations while at the same time without any pessimism that created the AI winter in the 1970s to 1990s. We also at the same time point out toxicity, bias, memorization, sycophancy, logical inconsistencies, hallucinations that exist just as a warning to the overly optimistic. We note here that just as this emergence of AI seems to occur at a threshold point in the number of neural connections or weights, it has also been observed that human brain and especially the cortex region is nothing special or extraordinary but simply a case of scaled-up version of the primate brain and that even the human intelligence seems like an emergent phenomena of scale.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#21644;&#24378;&#21270;&#23398;&#20064;&#21407;&#21017;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#24378;&#21270;&#23398;&#20064;&#65288;NLRL&#65289;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#24378;&#21270;&#23398;&#20064;&#22312;&#26679;&#26412;&#25928;&#29575;&#20302;&#12289;&#35299;&#37322;&#24615;&#19981;&#36275;&#21644;&#32570;&#20047;&#30417;&#30563;&#20449;&#21495;&#31561;&#26041;&#38754;&#30340;&#38480;&#21046;&#38382;&#39064;&#65292;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07157</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Natural Language Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#21644;&#24378;&#21270;&#23398;&#20064;&#21407;&#21017;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#24378;&#21270;&#23398;&#20064;&#65288;NLRL&#65289;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#24378;&#21270;&#23398;&#20064;&#22312;&#26679;&#26412;&#25928;&#29575;&#20302;&#12289;&#35299;&#37322;&#24615;&#19981;&#36275;&#21644;&#32570;&#20047;&#30417;&#30563;&#20449;&#21495;&#31561;&#26041;&#38754;&#30340;&#38480;&#21046;&#38382;&#39064;&#65292;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#23398;&#20064;&#20915;&#31574;&#20219;&#21153;&#30340;&#31574;&#30053;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;RL&#24120;&#24120;&#38754;&#20020;&#26679;&#26412;&#25928;&#29575;&#20302;&#12289;&#35299;&#37322;&#24615;&#19981;&#36275;&#21644;&#32570;&#20047;&#31232;&#30095;&#30417;&#30563;&#20449;&#21495;&#31561;&#38382;&#39064;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#20154;&#31867;&#23398;&#20064;&#36807;&#31243;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#24341;&#20837;&#20102;&#33258;&#28982;&#35821;&#35328;&#24378;&#21270;&#23398;&#20064;&#65288;NLRL&#65289;&#65292;&#21019;&#26032;&#24615;&#22320;&#23558;RL&#21407;&#21017;&#19982;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#32467;&#21512;&#36215;&#26469;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;NLRL&#22312;&#33258;&#28982;&#35821;&#35328;&#31354;&#38388;&#20013;&#37325;&#26032;&#23450;&#20041;&#20102;&#20219;&#21153;&#30446;&#26631;&#12289;&#31574;&#30053;&#12289;&#20215;&#20540;&#20989;&#25968;&#12289;Bellman&#26041;&#31243;&#21644;&#31574;&#30053;&#36845;&#20195;&#31561;RL&#27010;&#24565;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;GPT-4&#26469;&#23454;&#29616;NLRL&#12290;&#23545;&#34920;&#26684;MDPs&#30340;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#20102;NLRL&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12289;&#39640;&#25928;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) has shown remarkable abilities in learning policies for decision-making tasks. However, RL is often hindered by issues such as low sample efficiency, lack of interpretability, and sparse supervision signals. To tackle these limitations, we take inspiration from the human learning process and introduce Natural Language Reinforcement Learning (NLRL), which innovatively combines RL principles with natural language representation. Specifically, NLRL redefines RL concepts like task objectives, policy, value function, Bellman equation, and policy iteration in natural language space. We present how NLRL can be practically implemented with the latest advancements in large language models (LLMs) like GPT-4. Initial experiments over tabular MDPs demonstrate the effectiveness, efficiency, and also interpretability of the NLRL framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#21322;&#32447;&#24615;&#27874;&#21160;&#26041;&#31243;&#30340;&#20005;&#26684;&#35823;&#24046;&#30028;&#38480;&#65292;&#21253;&#25324;&#27867;&#21270;&#35823;&#24046;&#21644;&#35757;&#32451;&#35823;&#24046;&#30340;&#30028;&#38480;&#65292;&#24182;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#29702;&#35770;&#30028;&#38480;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07153</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#21322;&#32447;&#24615;&#27874;&#21160;&#26041;&#31243;&#30340;&#35823;&#24046;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Error Estimation for Physics-informed Neural Networks Approximating Semilinear Wave Equations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07153
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#21322;&#32447;&#24615;&#27874;&#21160;&#26041;&#31243;&#30340;&#20005;&#26684;&#35823;&#24046;&#30028;&#38480;&#65292;&#21253;&#25324;&#27867;&#21270;&#35823;&#24046;&#21644;&#35757;&#32451;&#35823;&#24046;&#30340;&#30028;&#38480;&#65292;&#24182;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#29702;&#35770;&#30028;&#38480;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#21322;&#32447;&#24615;&#27874;&#21160;&#26041;&#31243;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#35823;&#24046;&#30028;&#38480;&#12290;&#25105;&#20204;&#38024;&#23545;&#20855;&#26377;&#20004;&#20010;&#38544;&#34255;&#23618;&#30340;tanh&#31070;&#32463;&#32593;&#32476;&#65292;&#22522;&#20110;&#32593;&#32476;&#23618;&#23485;&#24230;&#21644;&#35757;&#32451;&#28857;&#25968;&#37327;&#65292;&#25552;&#20379;&#20102;&#23545;&#27867;&#21270;&#35823;&#24046;&#21644;&#35757;&#32451;&#35823;&#24046;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#22312;&#19968;&#20123;&#20551;&#35774;&#19979;&#65292;&#23558;&#24635;&#35823;&#24046;&#20197;$H^1([0,T];L^2(\Omega))$-&#33539;&#25968;&#30340;&#24418;&#24335;&#34920;&#31034;&#65292;&#24182;&#33021;&#22815;&#38543;&#30528;&#35757;&#32451;&#28857;&#25968;&#37327;&#30340;&#22686;&#21152;&#32780;&#20219;&#24847;&#20943;&#23567;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper provides rigorous error bounds for physics-informed neural networks approximating the semilinear wave equation. We provide bounds for the generalization and training error in terms of the width of the network's layers and the number of training points for a tanh neural network with two hidden layers. Our main result is a bound of the total error in the $H^1([0,T];L^2(\Omega))$-norm in terms of the training error and the number of training points, which can be made arbitrarily small under some assumptions. We illustrate our theoretical bounds with numerical experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20840;&#29699;&#37326;&#28779;&#39044;&#27979;&#27169;&#22411;&#65292;&#23558;&#20840;&#29699;&#27668;&#20505;&#21644;&#37326;&#28779;&#25968;&#25454;&#36716;&#21270;&#20026;&#22270;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#27169;&#22411;&#20013;&#30340;&#28023;&#27915;&#25968;&#25454;&#32570;&#22833;&#21644;&#36828;&#31243;&#20381;&#36182;&#24615;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#26356;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#21516;&#26102;&#65292;&#35813;&#27169;&#22411;&#36824;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#25581;&#31034;&#20102;&#20854;&#28508;&#22312;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2402.07152</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#20840;&#29699;&#37326;&#28779;&#39044;&#27979;&#27169;&#22411;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Explainable Global Wildfire Prediction Models using Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20840;&#29699;&#37326;&#28779;&#39044;&#27979;&#27169;&#22411;&#65292;&#23558;&#20840;&#29699;&#27668;&#20505;&#21644;&#37326;&#28779;&#25968;&#25454;&#36716;&#21270;&#20026;&#22270;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#27169;&#22411;&#20013;&#30340;&#28023;&#27915;&#25968;&#25454;&#32570;&#22833;&#21644;&#36828;&#31243;&#20381;&#36182;&#24615;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#26356;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#21516;&#26102;&#65292;&#35813;&#27169;&#22411;&#36824;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#25581;&#31034;&#20102;&#20854;&#28508;&#22312;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#27668;&#20505;&#21464;&#21270;&#30340;&#19981;&#26029;&#21152;&#21095;&#65292;&#37326;&#28779;&#39044;&#27979;&#21464;&#24471;&#36234;&#26469;&#36234;&#20851;&#38190;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;CNN&#30340;&#37326;&#28779;&#39044;&#27979;&#27169;&#22411;&#22312;&#22788;&#29702;&#32570;&#22833;&#30340;&#28023;&#27915;&#25968;&#25454;&#21644;&#35299;&#20915;&#27668;&#35937;&#25968;&#25454;&#20013;&#36828;&#31243;&#22320;&#21306;&#30340;&#38271;&#26399;&#20381;&#36182;&#24615;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#26412;&#25991;&#24341;&#20837;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#20840;&#29699;&#37326;&#28779;&#39044;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#27169;&#22411;&#65292;&#23558;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#30340;&#31354;&#38388;&#33021;&#21147;&#19982;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#30340;&#26102;&#38388;&#28145;&#24230;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#20840;&#29699;&#27668;&#20505;&#21644;&#37326;&#28779;&#25968;&#25454;&#29420;&#29305;&#22320;&#36716;&#21270;&#20026;&#22270;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#31354;&#27934;&#28023;&#27915;&#25968;&#25454;&#20301;&#32622;&#21644;&#38271;&#26399;&#20381;&#36182;&#24615;&#31561;&#25361;&#25112;&#12290;&#36890;&#36807;&#20351;&#29992;&#26410;&#30693;&#30340;JULES-INFERNO&#27169;&#25311;&#38598;&#23545;&#24050;&#24314;&#31435;&#30340;&#26550;&#26500;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23637;&#29616;&#20986;&#20248;&#36234;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#25581;&#31034;&#20102;&#20854;&#20013;&#30340;&#28508;&#22312;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wildfire prediction has become increasingly crucial due to the escalating impacts of climate change. Traditional CNN-based wildfire prediction models struggle with handling missing oceanic data and addressing the long-range dependencies across distant regions in meteorological data. In this paper, we introduce an innovative Graph Neural Network (GNN)-based model for global wildfire prediction. We propose a hybrid model that combines the spatial prowess of Graph Convolutional Networks (GCNs) with the temporal depth of Long Short-Term Memory (LSTM) networks. Our approach uniquely transforms global climate and wildfire data into a graph representation, addressing challenges such as null oceanic data locations and long-range dependencies inherent in traditional models. Benchmarking against established architectures using an unseen ensemble of JULES-INFERNO simulations, our model demonstrates superior predictive accuracy. Furthermore, we emphasise the model's explainability, unveiling poten
&lt;/p&gt;</description></item><item><title>X-LoRA&#26159;&#19968;&#31181;&#28789;&#27963;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#21033;&#29992;&#20302;&#31209;&#36866;&#37197;&#22120;&#19987;&#23478;&#30340;&#28151;&#21512;&#31574;&#30053;&#65292;&#21487;&#20197;&#21019;&#24314;&#31934;&#32454;&#35843;&#25972;&#30340;&#27169;&#22411;&#24182;&#22312;&#34507;&#30333;&#36136;&#21147;&#23398;&#21644;&#35774;&#35745;&#39046;&#22495;&#24212;&#29992;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#28145;&#23618;&#36880;&#23618;&#36866;&#24212;&#30340;&#32452;&#21512;&#26469;&#35299;&#20915;&#29305;&#23450;&#20219;&#21153;&#65292;&#24182;&#21463;&#21040;&#29983;&#29289;&#23398;&#21407;&#29702;&#30340;&#21551;&#21457;&#12290;&#26080;&#38656;&#20462;&#25913;&#24213;&#23618;&#32467;&#26500;&#21363;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.07148</link><description>&lt;p&gt;
X-LoRA: &#19968;&#31181;&#28789;&#27963;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#21033;&#29992;&#20302;&#31209;&#36866;&#37197;&#22120;&#19987;&#23478;&#30340;&#28151;&#21512;&#31574;&#30053;&#22312;&#34507;&#30333;&#36136;&#21147;&#23398;&#21644;&#35774;&#35745;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
X-LoRA: Mixture of Low-Rank Adapter Experts, a Flexible Framework for Large Language Models with Applications in Protein Mechanics and Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07148
&lt;/p&gt;
&lt;p&gt;
X-LoRA&#26159;&#19968;&#31181;&#28789;&#27963;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#21033;&#29992;&#20302;&#31209;&#36866;&#37197;&#22120;&#19987;&#23478;&#30340;&#28151;&#21512;&#31574;&#30053;&#65292;&#21487;&#20197;&#21019;&#24314;&#31934;&#32454;&#35843;&#25972;&#30340;&#27169;&#22411;&#24182;&#22312;&#34507;&#30333;&#36136;&#21147;&#23398;&#21644;&#35774;&#35745;&#39046;&#22495;&#24212;&#29992;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#28145;&#23618;&#36880;&#23618;&#36866;&#24212;&#30340;&#32452;&#21512;&#26469;&#35299;&#20915;&#29305;&#23450;&#20219;&#21153;&#65292;&#24182;&#21463;&#21040;&#29983;&#29289;&#23398;&#21407;&#29702;&#30340;&#21551;&#21457;&#12290;&#26080;&#38656;&#20462;&#25913;&#24213;&#23618;&#32467;&#26500;&#21363;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25253;&#36947;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#23618;&#36880;&#23618;&#22522;&#20110;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#30340;&#26032;&#39062;&#39044;&#35757;&#32451;&#36866;&#37197;&#22120;&#30340;&#28151;&#21512;&#19987;&#23478;&#31574;&#30053;&#65292;&#29992;&#20110;&#21019;&#24314;&#31934;&#32454;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38544;&#34255;&#29366;&#24577;&#21160;&#24577;&#28151;&#21512;&#32463;&#36807;&#36866;&#24212;&#30340;&#23618;&#30340;&#38376;&#25511;&#31574;&#30053;&#65292;&#20801;&#35768;&#24471;&#21040;&#30340;X-LoRA&#27169;&#22411;&#21033;&#29992;&#19981;&#21516;&#30340;&#33021;&#21147;&#24182;&#21019;&#24314;&#20197;&#21069;&#26410;&#20351;&#29992;&#30340;&#28145;&#23618;&#36880;&#23618;&#36866;&#24212;&#30340;&#32452;&#21512;&#26469;&#35299;&#20915;&#29305;&#23450;&#20219;&#21153;&#12290;&#35813;&#35774;&#35745;&#21463;&#21040;&#20102;&#29983;&#29289;&#26222;&#36941;&#24615;&#21644;&#22810;&#26679;&#24615;&#30340;&#29983;&#29289;&#23398;&#21407;&#29702;&#30340;&#21551;&#21457;&#65292;&#20854;&#20013;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#22359;&#22312;&#19981;&#21516;&#30340;&#20998;&#23618;&#34920;&#31034;&#20013;&#34987;&#37325;&#22797;&#20351;&#29992;&#12290;&#22240;&#27492;&#65292;X-LoRA&#27169;&#22411;&#21487;&#20197;&#36731;&#26494;&#29992;&#20110;&#20219;&#20309;&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#26080;&#38656;&#20462;&#25913;&#24213;&#23618;&#32467;&#26500;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;X-LoRA&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#21253;&#25324;&#21069;&#21521;/&#36870;&#21521;&#20998;&#26512;&#20219;&#21153;&#21644;&#22686;&#24378;&#25512;&#29702;&#33021;&#21147;&#22312;&#20869;&#30340;&#31185;&#23398;&#33021;&#21147;&#65292;&#37325;&#28857;&#26159;&#29983;&#29289;&#26448;&#26009;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We report a mixture of expert strategy to create fine-tuned large language models using a deep layer-wise token-level approach based on low-rank adaptation (LoRA). Starting with a set of pre-trained LoRA adapters, we propose a gating strategy that uses the hidden states to dynamically mix adapted layers, allowing the resulting X-LoRA model to draw upon different capabilities and create never-before-used deep layer-wise combinations of adaptations are established to solve specific tasks. The design is inspired by the biological principles of universality and diversity, where neural network building blocks are reused in different hierarchical manifestations. Hence, the X-LoRA model can be easily implemented for any existing large language model (LLM) without a need for modifications of the underlying structure. We develop a tailored X-LoRA model that offers scientific capabilities including forward/inverse analysis tasks and enhanced reasoning capability, focused on biomaterial analysis,
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25581;&#31034;&#20102;&#22270;&#25551;&#36848;&#30340;&#25991;&#26412;&#39034;&#24207;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#25512;&#29702;&#20013;&#30340;&#24615;&#33021;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#25913;&#21464;&#25991;&#26412;&#39034;&#24207;&#25552;&#39640;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#21457;&#29616;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#24615;&#33021;&#19982;&#22270;&#22823;&#23567;&#20043;&#38388;&#30340;&#20851;&#31995;&#19981;&#26159;&#21333;&#35843;&#36882;&#20943;&#30340;&#12290;&#20026;&#20102;&#35780;&#20272;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#22270;&#22823;&#23567;&#19978;&#30340;&#24615;&#33021;&#65292;&#24341;&#20837;&#20102;&#35268;&#27169;&#21270;&#22270;&#25512;&#29702;&#22522;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.07140</link><description>&lt;p&gt;
&#25991;&#23383;&#25551;&#36848;&#20013;&#30340;&#39034;&#24207;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#31354;&#38388;&#24863;&#30693;&#33021;&#21147;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Sequential Ordering in Textual Descriptions: Impact on Spatial Perception Abilities of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07140
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25581;&#31034;&#20102;&#22270;&#25551;&#36848;&#30340;&#25991;&#26412;&#39034;&#24207;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#25512;&#29702;&#20013;&#30340;&#24615;&#33021;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#25913;&#21464;&#25991;&#26412;&#39034;&#24207;&#25552;&#39640;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#21457;&#29616;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#24615;&#33021;&#19982;&#22270;&#22823;&#23567;&#20043;&#38388;&#30340;&#20851;&#31995;&#19981;&#26159;&#21333;&#35843;&#36882;&#20943;&#30340;&#12290;&#20026;&#20102;&#35780;&#20272;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#22270;&#22823;&#23567;&#19978;&#30340;&#24615;&#33021;&#65292;&#24341;&#20837;&#20102;&#35268;&#27169;&#21270;&#22270;&#25512;&#29702;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#39046;&#22495;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22270;&#25512;&#29702;&#39046;&#22495;&#30340;&#36827;&#23637;&#20173;&#28982;&#26377;&#38480;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#28145;&#20837;&#30740;&#31350;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#25512;&#29702;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#25991;&#26412;&#39034;&#24207;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#31354;&#38388;&#29702;&#35299;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22270;&#25551;&#36848;&#30340;&#25991;&#26412;&#39034;&#24207;&#26174;&#33879;&#24433;&#21709;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#22270;&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;&#36890;&#36807;&#25913;&#21464;&#22270;&#25551;&#36848;&#30340;&#25991;&#26412;&#39034;&#24207;&#65292;&#25105;&#20204;&#23558;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#20174;42.22&#65285;&#25552;&#39640;&#21040;70&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#21644;&#22270;&#22823;&#23567;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#24615;&#33021;&#19981;&#38543;&#22270;&#22823;&#23567;&#30340;&#22686;&#21152;&#32780;&#21333;&#35843;&#36882;&#20943;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#35268;&#27169;&#21270;&#22270;&#25512;&#29702;&#22522;&#20934;&#26469;&#35780;&#20272;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#22270;&#22823;&#23567;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Large Language Models have reached state-of-the-art performance across multiple domains. However, the progress in the field of graph reasoning remains limited. Our work delves into this gap by thoroughly investigating graph reasoning with LLM. In this work, we reveal the impact of text sequence on LLM spatial understanding, finding that graph-descriptive text sequences significantly affect LLM reasoning performance on graphs. By altering the graph-descriptive text sequences, we enhance the performance of LLM from 42.22\% to 70\%. Furthermore, we evaluate the relationship between LLM performance and graph size, discovering that the reasoning performance of LLM does not monotonically decrease with the increase in graph size. Conclusively, we introduce the Scaled Graph Reasoning benchmark for assessing LLM performance across varied graph sizes.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23581;&#35797;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#38544;&#24335;&#27169;&#22411;&#21019;&#26032;&#26725;&#26753;&#31867;&#22411;&#65292;&#36890;&#36807;&#26131;&#20110;&#29702;&#35299;&#30340;&#20195;&#25968;&#26041;&#27861;&#25512;&#23548;&#20986;&#20989;&#25968;&#20844;&#24335;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#24179;&#21488;&#26500;&#24314;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#28508;&#31354;&#38388;&#37319;&#26679;&#29983;&#25104;&#20855;&#26377;&#38750;&#23545;&#31216;&#32467;&#26500;&#30340;&#26032;&#26725;&#26753;&#31867;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.07129</link><description>&lt;p&gt;
&#20174;&#21435;&#22122;&#25193;&#25955;&#38544;&#24335;&#27169;&#22411;&#30340;&#28508;&#31354;&#38388;&#20013;&#29983;&#25104;&#26032;&#30340;&#26725;&#26753;&#31867;&#22411;&#30340;&#23581;&#35797;
&lt;/p&gt;
&lt;p&gt;
An attempt to generate new bridge types from latent space of denoising diffusion Implicit model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23581;&#35797;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#38544;&#24335;&#27169;&#22411;&#21019;&#26032;&#26725;&#26753;&#31867;&#22411;&#65292;&#36890;&#36807;&#26131;&#20110;&#29702;&#35299;&#30340;&#20195;&#25968;&#26041;&#27861;&#25512;&#23548;&#20986;&#20989;&#25968;&#20844;&#24335;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#24179;&#21488;&#26500;&#24314;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#28508;&#31354;&#38388;&#37319;&#26679;&#29983;&#25104;&#20855;&#26377;&#38750;&#23545;&#31216;&#32467;&#26500;&#30340;&#26032;&#26725;&#26753;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#38544;&#24335;&#27169;&#22411;&#36827;&#34892;&#26725;&#26753;&#21019;&#26032;&#12290;&#23558;&#22270;&#20687;&#28155;&#21152;&#22122;&#22768;&#21644;&#21435;&#22122;&#30340;&#36807;&#31243;&#31867;&#27604;&#20026;&#23608;&#20307;&#33104;&#28866;&#21644;&#20390;&#25506;&#24674;&#22797;&#34987;&#26432;&#23475;&#30340;&#21463;&#23475;&#32773;&#29616;&#22330;&#30340;&#36807;&#31243;&#65292;&#20197;&#24110;&#21161;&#21021;&#23398;&#32773;&#29702;&#35299;&#12290;&#36890;&#36807;&#26131;&#20110;&#29702;&#35299;&#30340;&#20195;&#25968;&#26041;&#27861;&#65292;&#25512;&#23548;&#20986;&#28155;&#21152;&#22122;&#22768;&#21644;&#21435;&#22122;&#30340;&#20989;&#25968;&#20844;&#24335;&#65292;&#20351;&#21021;&#23398;&#32773;&#26356;&#23481;&#26131;&#25484;&#25569;&#27169;&#22411;&#30340;&#25968;&#23398;&#21407;&#29702;&#12290;&#20351;&#29992;&#19977;&#36328;&#26753;&#26725;&#12289;&#25329;&#26725;&#12289;&#26012;&#25289;&#26725;&#21644;&#24748;&#32034;&#26725;&#30340;&#23545;&#31216;&#32467;&#26500;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#22522;&#20110;Python&#32534;&#31243;&#35821;&#35328;&#12289;TensorFlow&#21644;Keras&#28145;&#24230;&#23398;&#20064;&#24179;&#21488;&#26694;&#26550;&#65292;&#26500;&#24314;&#21644;&#35757;&#32451;&#20102;&#21435;&#22122;&#25193;&#25955;&#38544;&#24335;&#27169;&#22411;&#12290;&#20174;&#28508;&#31354;&#38388;&#37319;&#26679;&#20013;&#65292;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#38750;&#23545;&#31216;&#32467;&#26500;&#30340;&#26032;&#26725;&#26753;&#31867;&#22411;&#12290;&#21435;&#22122;&#25193;&#25955;&#38544;&#24335;&#27169;&#22411;&#21487;&#20197;&#22312;&#20154;&#31867;&#21407;&#22987;&#26725;&#26753;&#31867;&#22411;&#30340;&#22522;&#30784;&#19978;&#65292;&#26377;&#26426;&#22320;&#32452;&#21512;&#19981;&#21516;&#30340;&#32467;&#26500;&#32452;&#20214;&#65292;&#21019;&#36896;&#26032;&#30340;&#26725;&#26753;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Use denoising diffusion implicit model for bridge-type innovation. The process of adding noise and denoising to an image can be likened to the process of a corpse rotting and a detective restoring the scene of a victim being killed, to help beginners understand. Through an easy-to-understand algebraic method, derive the function formulas for adding noise and denoising, making it easier for beginners to master the mathematical principles of the model. Using symmetric structured image dataset of three-span beam bridge, arch bridge, cable-stayed bridge and suspension bridge , based on Python programming language, TensorFlow and Keras deep learning platform framework , denoising diffusion implicit model is constructed and trained. From the latent space sampling, new bridge types with asymmetric structures can be generated. Denoising diffusion implicit model can organically combine different structural components on the basis of human original bridge types, and create new bridge types.
&lt;/p&gt;</description></item><item><title /><link>https://arxiv.org/abs/2402.07127</link><description>&lt;p&gt;
&#35266;&#23519;&#23398;&#20064;&#65306;&#22522;&#20110;&#35270;&#39057;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#23398;&#20064;&#26041;&#27861;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Learning by Watching: A Review of Video-based Learning Approaches for Robot Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07127
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#23398;&#20064;&#25805;&#20316;&#25216;&#33021;&#21463;&#21040;&#22810;&#26679;&#21270;&#12289;&#26080;&#20559;&#30340;&#25968;&#25454;&#38598;&#30340;&#31232;&#32570;&#24615;&#30340;&#24433;&#21709;&#12290;&#23613;&#31649;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#24110;&#21161;&#35299;&#20915;&#38382;&#39064;&#65292;&#20294;&#22312;&#27867;&#21270;&#24615;&#21644;&#29616;&#23454;&#19990;&#30028;&#30340;&#36716;&#31227;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#8220;&#37326;&#22806;&#8221;&#35270;&#39057;&#25968;&#25454;&#38598;&#30340;&#22823;&#35268;&#27169;&#23384;&#22312;&#36890;&#36807;&#33258;&#30417;&#30563;&#25216;&#26415;&#25512;&#21160;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#36827;&#23637;&#12290;&#23558;&#36825;&#19968;&#28857;&#24212;&#29992;&#21040;&#26426;&#22120;&#20154;&#39046;&#22495;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#36890;&#36807;&#34987;&#21160;&#35266;&#23519;&#26469;&#23398;&#20064;&#20016;&#23500;&#30340;&#22312;&#32447;&#35270;&#39057;&#20013;&#30340;&#25805;&#20316;&#25216;&#33021;&#12290;&#36825;&#31181;&#22522;&#20110;&#35270;&#39057;&#30340;&#23398;&#20064;&#33539;&#24335;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#23427;&#25552;&#20379;&#20102;&#21487;&#25193;&#23637;&#30340;&#30417;&#30563;&#26041;&#27861;&#65292;&#21516;&#26102;&#38477;&#20302;&#20102;&#25968;&#25454;&#38598;&#30340;&#20559;&#35265;&#12290;&#26412;&#32508;&#36848;&#22238;&#39038;&#20102;&#35270;&#39057;&#29305;&#24449;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#12289;&#29289;&#20307;&#21487;&#34892;&#24615;&#29702;&#35299;&#12289;&#19977;&#32500;&#25163;&#37096;/&#36523;&#20307;&#24314;&#27169;&#21644;&#22823;&#35268;&#27169;&#26426;&#22120;&#20154;&#36164;&#28304;&#31561;&#22522;&#30784;&#30693;&#35782;&#65292;&#20197;&#21450;&#20174;&#19981;&#21463;&#25511;&#21046;&#30340;&#35270;&#39057;&#28436;&#31034;&#20013;&#33719;&#21462;&#26426;&#22120;&#20154;&#25805;&#20316;&#25216;&#33021;&#30340;&#26032;&#20852;&#25216;&#26415;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20165;&#20174;&#35266;&#23519;&#22823;&#35268;&#27169;&#20154;&#31867;&#35270;&#39057;&#20013;&#23398;&#20064;&#22914;&#20309;&#22686;&#24378;&#26426;&#22120;&#20154;&#30340;&#27867;&#21270;&#24615;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robot learning of manipulation skills is hindered by the scarcity of diverse, unbiased datasets. While curated datasets can help, challenges remain in generalizability and real-world transfer. Meanwhile, large-scale "in-the-wild" video datasets have driven progress in computer vision through self-supervised techniques. Translating this to robotics, recent works have explored learning manipulation skills by passively watching abundant videos sourced online. Showing promising results, such video-based learning paradigms provide scalable supervision while reducing dataset bias. This survey reviews foundations such as video feature representation learning techniques, object affordance understanding, 3D hand/body modeling, and large-scale robot resources, as well as emerging techniques for acquiring robot manipulation skills from uncontrolled video demonstrations. We discuss how learning only from observing large-scale human videos can enhance generalization and sample efficiency for roboti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#30524;&#31185;&#36136;&#37327;&#35780;&#20272;&#31995;&#32479;&#65292;&#21487;&#20197;&#24110;&#21161;&#36828;&#31243;&#30524;&#31185;&#21672;&#35810;&#65292;&#23588;&#20854;&#26159;&#22312;&#20302;&#25910;&#20837;&#21644;&#20013;&#31561;&#25910;&#20837;&#22269;&#23478;&#12290;&#35813;&#31995;&#32479;&#33021;&#22815;&#21363;&#26102;&#21453;&#39304;&#29992;&#25143;&#25293;&#25668;&#30340;&#30524;&#37096;&#22270;&#20687;&#30340;&#36136;&#37327;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#29992;&#25143;&#25293;&#25668;&#22270;&#20687;&#36136;&#37327;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.07118</link><description>&lt;p&gt;
&#19979;&#19968;&#20195;&#36828;&#31243;&#30524;&#31185;&#35786;&#30103;&#65306;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#36136;&#37327;&#35780;&#20272;&#24110;&#21161;&#36828;&#31243;&#22522;&#20110;&#26234;&#33021;&#25163;&#26426;&#30340;&#21672;&#35810;
&lt;/p&gt;
&lt;p&gt;
Next-Generation Teleophthalmology: AI-enabled Quality Assessment Aiding Remote Smartphone-based Consultation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07118
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#30524;&#31185;&#36136;&#37327;&#35780;&#20272;&#31995;&#32479;&#65292;&#21487;&#20197;&#24110;&#21161;&#36828;&#31243;&#30524;&#31185;&#21672;&#35810;&#65292;&#23588;&#20854;&#26159;&#22312;&#20302;&#25910;&#20837;&#21644;&#20013;&#31561;&#25910;&#20837;&#22269;&#23478;&#12290;&#35813;&#31995;&#32479;&#33021;&#22815;&#21363;&#26102;&#21453;&#39304;&#29992;&#25143;&#25293;&#25668;&#30340;&#30524;&#37096;&#22270;&#20687;&#30340;&#36136;&#37327;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#29992;&#25143;&#25293;&#25668;&#22270;&#20687;&#36136;&#37327;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22833;&#26126;&#21644;&#20854;&#20182;&#30524;&#37096;&#30142;&#30149;&#26159;&#20840;&#29699;&#20851;&#27880;&#30340;&#20581;&#24247;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#22312;&#21360;&#24230;&#31561;&#20302;&#25910;&#20837;&#21644;&#20013;&#31561;&#25910;&#20837;&#22269;&#23478;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#22312;COVID-19&#22823;&#27969;&#34892;&#26399;&#38388;&#65292;&#36828;&#31243;&#30524;&#31185;&#35786;&#30103;&#25104;&#20026;&#19968;&#31181;&#29983;&#21629;&#32447;&#65292;&#24182;&#19988;&#26234;&#33021;&#25163;&#26426;&#30524;&#37096;&#25104;&#20687;&#30340; Grabi &#38468;&#20214;&#24471;&#21040;&#20102;&#24191;&#27867;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#29992;&#25143;&#25293;&#25668;&#30340;&#22270;&#29255;&#36136;&#37327;&#24448;&#24448;&#19981;&#22815;&#22909;&#65292;&#38656;&#35201;&#21307;&#29983;&#23457;&#26680;&#24182;&#19988;&#20250;&#24310;&#35823;&#26102;&#38388;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#36136;&#37327;&#35780;&#20272;&#31995;&#32479;&#65292;&#33021;&#22815;&#27169;&#25311;&#21307;&#29983;&#30340;&#21028;&#26029;&#24182;&#19988;&#33021;&#22815;&#21363;&#26102;&#21453;&#39304;&#65292;&#25105;&#20204;&#23545;&#24739;&#32773;&#25293;&#25668;&#30340;&#22270;&#20687;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#23558;&#22797;&#26434;&#38382;&#39064;&#23618;&#27425;&#21270;&#65292;&#25105;&#20204;&#22312;&#27492;&#35299;&#20915;&#20102;&#19968;&#20010;&#38750;&#24120;&#37325;&#35201;&#30340;&#37096;&#20998;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#27010;&#24565;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Blindness and other eye diseases are a global health concern, particularly in low- and middle-income countries like India. In this regard, during the COVID-19 pandemic, teleophthalmology became a lifeline, and the Grabi attachment for smartphone-based eye imaging gained in use. However, quality of user-captured image often remained inadequate, requiring clinician vetting and delays. In this backdrop, we propose an AI-based quality assessment system with instant feedback mimicking clinicians' judgments and tested on patient-captured images. Dividing the complex problem hierarchically, here we tackle a nontrivial part, and demonstrate a proof of the concept.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32479;&#35745;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;Q&#32593;&#32476;&#21644;&#20998;&#20301;&#25968;&#22238;&#24402;&#26469;&#22312;&#27169;&#22411;-free&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#20013;&#24341;&#20837;&#19981;&#30830;&#23450;&#24615;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#28145;&#24230;&#35777;&#25454;&#23398;&#20064;&#21644;&#22522;&#20110;&#21512;&#35268;&#25512;&#29702;&#21407;&#21017;&#30340;&#20998;&#20301;&#25968;&#26657;&#20934;&#65292;&#25552;&#20379;&#20102;&#20840;&#23616;&#19981;&#30830;&#23450;&#24615;&#30340;&#26174;&#24335;&#12289;&#26080;&#26679;&#26412;&#35745;&#31639;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#35745;&#31639;&#21644;&#32479;&#35745;&#25928;&#29575;&#65292;&#24182;&#25104;&#21151;&#22788;&#29702;&#20102;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#35266;&#27979;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.07107</link><description>&lt;p&gt;
&#32034;crates&#24576;&#30097;&#30340;&#22238;&#22768;&#65306;&#22312;&#26657;&#20934;&#30340;&#35777;&#25454;&#22686;&#24378;&#23398;&#20064;&#20013;&#25509;&#21463;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Echoes of Socratic Doubt: Embracing Uncertainty in Calibrated Evidential Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07107
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32479;&#35745;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;Q&#32593;&#32476;&#21644;&#20998;&#20301;&#25968;&#22238;&#24402;&#26469;&#22312;&#27169;&#22411;-free&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#20013;&#24341;&#20837;&#19981;&#30830;&#23450;&#24615;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#28145;&#24230;&#35777;&#25454;&#23398;&#20064;&#21644;&#22522;&#20110;&#21512;&#35268;&#25512;&#29702;&#21407;&#21017;&#30340;&#20998;&#20301;&#25968;&#26657;&#20934;&#65292;&#25552;&#20379;&#20102;&#20840;&#23616;&#19981;&#30830;&#23450;&#24615;&#30340;&#26174;&#24335;&#12289;&#26080;&#26679;&#26412;&#35745;&#31639;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#35745;&#31639;&#21644;&#32479;&#35745;&#25928;&#29575;&#65292;&#24182;&#25104;&#21151;&#22788;&#29702;&#20102;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#35266;&#27979;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22522;&#20110;&#27169;&#22411;&#30340;&#20998;&#24067;&#24378;&#21270;&#23398;&#20064;&#20013;&#24341;&#20837;&#19981;&#30830;&#23450;&#24615;&#24847;&#35782;&#65292;&#28041;&#21450;&#22522;&#20110;&#20998;&#20301;&#25968;&#22238;&#24402;&#30340;&#28145;&#24230;Q&#32593;&#32476;&#12290;&#25552;&#20986;&#30340;&#31639;&#27861;$\textit{Calibrated Evidential Quantile Regression in Deep Q Networks (CEQR-DQN)}$&#26088;&#22312;&#35299;&#20915;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#20998;&#21035;&#20272;&#35745;aleatoric&#21644;epistemic&#19981;&#30830;&#23450;&#24615;&#25152;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#23427;&#23558;&#28145;&#24230;&#35777;&#25454;&#23398;&#20064;&#19982;&#22522;&#20110;&#21512;&#35268;&#25512;&#29702;&#21407;&#21017;&#30340;&#20998;&#20301;&#25968;&#26657;&#20934;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#26174;&#24335;&#30340;&#12289;&#26080;&#26679;&#26412;&#35745;&#31639;&#30340;$\textit{&#20840;&#23616;}$&#19981;&#30830;&#23450;&#24615;&#65292;&#32780;&#19981;&#26159;&#22522;&#20110;&#31616;&#21333;&#26041;&#24046;&#30340;$\textit{&#23616;&#37096;}$&#20272;&#35745;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#35745;&#31639;&#21644;&#32479;&#35745;&#25928;&#29575;&#20197;&#21450;&#22788;&#29702;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#35266;&#27979;&#25968;&#25454;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#22312;&#19968;&#22871;&#23567;&#22411;&#21270;&#30340;Atari&#28216;&#25103;&#65288;&#21363;MinAtar&#65289;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;CEQR-DQN&#22312;&#24471;&#20998;&#21644;&#23398;&#20064;&#36895;&#24230;&#26041;&#38754;&#36229;&#36807;&#20102;&#31867;&#20284;&#30340;&#29616;&#26377;&#26694;&#26550;&#12290;&#23427;&#33021;&#22815;&#20005;&#35880;&#22320;&#22788;&#29702;&#22806;&#37096;&#25968;&#25454;&#35266;&#27979;&#65292;&#24182;&#25552;&#20379;&#26356;&#39640;&#30340;&#35745;&#31639;&#21644;&#32479;&#35745;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel statistical approach to incorporating uncertainty awareness in model-free distributional reinforcement learning involving quantile regression-based deep Q networks. The proposed algorithm, $\textit{Calibrated Evidential Quantile Regression in Deep Q Networks (CEQR-DQN)}$, aims to address key challenges associated with separately estimating aleatoric and epistemic uncertainty in stochastic environments. It combines deep evidential learning with quantile calibration based on principles of conformal inference to provide explicit, sample-free computations of $\textit{global}$ uncertainty as opposed to $\textit{local}$ estimates based on simple variance, overcoming limitations of traditional methods in computational and statistical efficiency and handling of out-of-distribution (OOD) observations. Tested on a suite of miniaturized Atari games (i.e., MinAtar), CEQR-DQN is shown to surpass similar existing frameworks in scores and learning speed. Its ability to rigorously e
&lt;/p&gt;</description></item><item><title>&#26410;&#26469;&#39044;&#27979;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#20013;&#23398;&#20064; History Representation &#20855;&#26377;&#24456;&#24378;&#30340;&#30456;&#20851;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07102</link><description>&lt;p&gt;
&#26410;&#26469;&#39044;&#27979;&#21487;&#20197;&#25104;&#20026;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#20013;&#33391;&#22909;&#21382;&#21490;&#34920;&#36798;&#30340;&#26377;&#21147;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Future Prediction Can be a Strong Evidence of Good History Representation in Partially Observable Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07102
&lt;/p&gt;
&lt;p&gt;
&#26410;&#26469;&#39044;&#27979;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#20013;&#23398;&#20064; History Representation &#20855;&#26377;&#24456;&#24378;&#30340;&#30456;&#20851;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#33391;&#22909;&#30340;&#21382;&#21490;&#34920;&#36798;&#26159;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#20013;&#24378;&#21270;&#23398;&#20064;&#30340;&#26680;&#24515;&#25361;&#25112;&#20043;&#19968;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21508;&#31181;&#36741;&#21161;&#20219;&#21153;&#23545;&#20419;&#36827;&#34920;&#36798;&#23398;&#20064;&#20855;&#26377;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#36741;&#21161;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#23578;&#26410;&#23436;&#20840;&#20351;&#20154;&#20449;&#26381;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#38271;&#26399;&#35760;&#24518;&#21644;&#25512;&#29702;&#30340;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#20013;&#12290;&#22312;&#36825;&#20010;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26410;&#26469;&#39044;&#27979;&#22312;&#23398;&#20064;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#20013;&#21382;&#21490;&#34920;&#36798;&#26102;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26410;&#26469;&#39044;&#27979;&#23558;&#23398;&#20064;&#21382;&#21490;&#34920;&#36798;&#19982;&#31574;&#30053;&#20248;&#21270;&#20998;&#31163;&#30340;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26377;&#20004;&#20010;&#26041;&#38754;&#65306;&#65288;a&#65289;&#25105;&#20204;&#35777;&#26126;&#20102;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#19982;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#20013;&#26410;&#26469;&#35266;&#27979;&#30340;&#39044;&#27979;&#31934;&#24230;&#24378;&#30456;&#20851;&#65292;&#65288;b&#65289;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#20013;&#38271;&#26102;&#38388;&#21382;&#21490;&#30340;&#34920;&#36798;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning a good history representation is one of the core challenges of reinforcement learning (RL) in partially observable environments. Recent works have shown the advantages of various auxiliary tasks for facilitating representation learning. However, the effectiveness of such auxiliary tasks has not been fully convincing, especially in partially observable environments that require long-term memorization and inference. In this empirical study, we investigate the effectiveness of future prediction for learning the representations of histories, possibly of extensive length, in partially observable environments. We first introduce an approach that decouples the task of learning history representations from policy optimization via future prediction. Then, our main contributions are two-fold: (a) we demonstrate that the performance of reinforcement learning is strongly correlated with the prediction accuracy of future observations in partially observable environments, and (b) our approa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#21644;Unity&#24179;&#21488;&#29983;&#25104;&#25968;&#25454;&#65292;&#25552;&#39640;&#20102;&#22312;&#20179;&#24211;&#29615;&#22659;&#20013;&#25176;&#30424;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#27169;&#22411;&#22312;&#20809;&#32447;&#36739;&#26263;&#30340;&#29615;&#22659;&#19979;&#24615;&#33021;&#36739;&#24046;&#65292;&#24182;&#19988;&#20351;&#29992;YOLOv8&#21644;SAM&#30340;&#20004;&#38454;&#27573;&#26816;&#27979;&#22120;&#20855;&#26377;&#19981;&#31283;&#23450;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.07098</link><description>&lt;p&gt;
&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#25913;&#36827;&#25176;&#30424;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Improving Pallet Detection Using Synthetic Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#21644;Unity&#24179;&#21488;&#29983;&#25104;&#25968;&#25454;&#65292;&#25552;&#39640;&#20102;&#22312;&#20179;&#24211;&#29615;&#22659;&#20013;&#25176;&#30424;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#27169;&#22411;&#22312;&#20809;&#32447;&#36739;&#26263;&#30340;&#29615;&#22659;&#19979;&#24615;&#33021;&#36739;&#24046;&#65292;&#24182;&#19988;&#20351;&#29992;YOLOv8&#21644;SAM&#30340;&#20004;&#38454;&#27573;&#26816;&#27979;&#22120;&#20855;&#26377;&#19981;&#31283;&#23450;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#21487;&#20197;&#33410;&#30465;&#22823;&#37327;&#26102;&#38388;&#65292;&#23454;&#29616;&#26377;&#25928;&#30340;&#29289;&#20307;&#26816;&#27979;&#22120;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25913;&#36827;&#22312;&#20179;&#24211;&#29615;&#22659;&#20013;&#25176;&#30424;&#23454;&#20363;&#20998;&#21106;&#20219;&#21153;&#20013;&#20043;&#21069;&#24212;&#29992;&#30340;&#23454;&#29616;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#21512;&#25104;&#29983;&#25104;&#30340;&#39046;&#22495;&#38543;&#26426;&#21270;&#25968;&#25454;&#20197;&#21450;&#36890;&#36807;Unity&#29983;&#25104;&#30340;&#25968;&#25454;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#24403;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;&#26412;&#30740;&#31350;&#22312;&#22534;&#21472;&#21644;&#26550;&#24335;&#25176;&#30424;&#31867;&#21035;&#19978;&#30340;&#24615;&#33021;&#25552;&#21319;&#20998;&#21035;&#20026;69%&#21644;50%&#30340;mAP50&#12290;&#27492;&#22806;&#65292;&#24403;&#27169;&#22411;&#22312;&#20809;&#32447;&#36739;&#26263;&#30340;&#29615;&#22659;&#20013;&#35780;&#20272;&#26102;&#65292;&#20854;&#24615;&#33021;&#20250;&#21463;&#21040;&#26174;&#33879;&#24433;&#21709;&#65292;&#24403;&#22312;&#20142;&#24230;&#38477;&#20302;80%&#30340;&#22270;&#20687;&#19978;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;mAP50&#20250;&#38477;&#20302;&#21040;&#26368;&#20302;3%&#12290;&#26412;&#30740;&#31350;&#36824;&#21019;&#24314;&#20102;&#19968;&#20010;&#20351;&#29992;YOLOv8&#21644;SAM&#30340;&#20004;&#38454;&#27573;&#26816;&#27979;&#22120;&#65292;&#20294;&#20854;&#24615;&#33021;&#19981;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of synthetic data in machine learning saves a significant amount of time when implementing an effective object detector. However, there is limited research in this domain. This study aims to improve upon previously applied implementations in the task of instance segmentation of pallets in a warehouse environment. This study proposes using synthetically generated domain-randomised data as well as data generated through Unity to achieve this. This study achieved performance improvements on the stacked and racked pallet categories by 69% and 50% mAP50, respectively when being evaluated on real data. Additionally, it was found that there was a considerable impact on the performance of a model when it was evaluated against images in a darker environment, dropping as low as 3% mAP50 when being evaluated on images with an 80% brightness reduction. This study also created a two-stage detector that used YOLOv8 and SAM, but this proved to have unstable performance. The use of domain-rand
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;&#26102;&#21487;&#33021;&#20986;&#29616;&#30340;&#33258;&#25105;&#28040;&#32791;&#24490;&#29615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#29702;&#24819;&#30340;&#20462;&#27491;&#20989;&#25968;&#26469;&#31283;&#23450;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#33258;&#25105;&#20462;&#27491;&#20989;&#25968;&#26469;&#36817;&#20284;&#29702;&#24819;&#30340;&#20462;&#27491;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07087</link><description>&lt;p&gt;
&#33258;&#25105;&#32416;&#27491;&#33258;&#25105;&#28040;&#32791;&#24490;&#29615;&#29992;&#20110;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Self-Correcting Self-Consuming Loops for Generative Model Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;&#26102;&#21487;&#33021;&#20986;&#29616;&#30340;&#33258;&#25105;&#28040;&#32791;&#24490;&#29615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#29702;&#24819;&#30340;&#20462;&#27491;&#20989;&#25968;&#26469;&#31283;&#23450;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#33258;&#25105;&#20462;&#27491;&#20989;&#25968;&#26469;&#36817;&#20284;&#29702;&#24819;&#30340;&#20462;&#27491;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21512;&#25104;&#25968;&#25454;&#22312;&#20114;&#32852;&#32593;&#19978;&#30340;&#36136;&#37327;&#36234;&#26469;&#36234;&#39640;&#20197;&#21450;&#25968;&#37327;&#19981;&#26029;&#22686;&#21152;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#22312;&#20154;&#24037;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#25968;&#25454;&#30340;&#28151;&#21512;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#23613;&#31649;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#34920;&#24449;&#23398;&#20064;&#30340;&#25104;&#21151;&#26696;&#20363;&#26377;&#24456;&#22810;&#65292;&#20294;&#26159;&#22312;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;&#20013;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#20250;&#20135;&#29983;"&#33258;&#25105;&#28040;&#32791;&#24490;&#29615;"&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#35757;&#32451;&#19981;&#31283;&#23450;&#29978;&#33267;&#23849;&#28291;&#65292;&#38500;&#38750;&#28385;&#36275;&#26576;&#20123;&#26465;&#20214;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#26088;&#22312;&#31283;&#23450;&#33258;&#25105;&#28040;&#32791;&#30340;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#29702;&#24819;&#30340;&#20462;&#27491;&#20989;&#25968;&#65292;&#23558;&#25968;&#25454;&#28857;&#26144;&#23556;&#20026;&#26356;&#26377;&#21487;&#33021;&#26469;&#33258;&#30495;&#23454;&#25968;&#25454;&#20998;&#24067;&#30340;&#26679;&#26412;&#65292;&#21487;&#20197;&#20351;&#33258;&#25105;&#28040;&#32791;&#24490;&#29615;&#30340;&#31283;&#23450;&#24615;&#21576;&#25351;&#25968;&#22686;&#21152;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#25105;&#20462;&#27491;&#20989;&#25968;&#65292;&#23427;&#20381;&#36182;&#20110;&#19987;&#23478;&#30693;&#35782;&#65288;&#20363;&#22914;&#65292;&#32534;&#31243;&#22312;&#27169;&#25311;&#22120;&#20013;&#30340;&#29289;&#29702;&#23450;&#24459;&#65289;&#65292;&#24182;&#19988;&#26088;&#22312;&#33258;&#21160;&#19988;&#22823;&#35268;&#27169;&#22320;&#36817;&#20284;&#29702;&#24819;&#30340;&#20462;&#27491;&#20989;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#33258;&#25105;&#32416;&#27491;&#33258;&#25105;&#28040;&#32791;&#24490;&#29615;&#22312;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As synthetic data becomes higher quality and proliferates on the internet, machine learning models are increasingly trained on a mix of human- and machine-generated data. Despite the successful stories of using synthetic data for representation learning, using synthetic data for generative model training creates "self-consuming loops" which may lead to training instability or even collapse, unless certain conditions are met. Our paper aims to stabilize self-consuming generative model training. Our theoretical results demonstrate that by introducing an idealized correction function, which maps a data point to be more likely under the true data distribution, self-consuming loops can be made exponentially more stable. We then propose self-correction functions, which rely on expert knowledge (e.g. the laws of physics programmed in a simulator), and aim to approximate the idealized corrector automatically and at scale. We empirically validate the effectiveness of self-correcting self-consum
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#27604;&#39044;&#35757;&#32451;&#21644;&#22810;&#39046;&#22495;&#21305;&#37197;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CAMA&#26694;&#26550;&#26469;&#25913;&#21892;B2B&#20113;&#35299;&#20915;&#26041;&#26696;&#30340;&#21305;&#37197;&#38382;&#39064;&#65292;&#20811;&#26381;&#20102;&#22797;&#26434;&#29305;&#24449;&#21644;&#26377;&#38480;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.07076</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#39044;&#35757;&#32451;&#65292;&#25913;&#21892;&#22810;&#39046;&#22495;B2B&#20113;&#35299;&#20915;&#26041;&#26696;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Enhancing Multi-field B2B Cloud Solution Matching via Contrastive Pre-training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07076
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#39044;&#35757;&#32451;&#21644;&#22810;&#39046;&#22495;&#21305;&#37197;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CAMA&#26694;&#26550;&#26469;&#25913;&#21892;B2B&#20113;&#35299;&#20915;&#26041;&#26696;&#30340;&#21305;&#37197;&#38382;&#39064;&#65292;&#20811;&#26381;&#20102;&#22797;&#26434;&#29305;&#24449;&#21644;&#26377;&#38480;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20113;&#35299;&#20915;&#26041;&#26696;&#22312;&#25216;&#26415;&#34892;&#19994;&#20013;&#38750;&#24120;&#21463;&#27426;&#36814;&#65292;&#22240;&#20026;&#23427;&#20204;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#26381;&#21153;&#21644;&#24037;&#20855;&#26469;&#35299;&#20915;&#29305;&#23450;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#35299;&#20915;&#26041;&#26696;&#25552;&#20379;&#21830;&#30340;&#38144;&#21806;&#22242;&#38431;&#20173;&#28982;&#38754;&#20020;&#19968;&#20010;&#22797;&#26434;&#30340;&#19994;&#21153;&#38382;&#39064;&#65292;&#21363;&#25214;&#21040;&#36866;&#21512;&#29305;&#23450;&#30446;&#26631;&#35299;&#20915;&#26041;&#26696;&#30340;&#21512;&#36866;&#30340;&#20844;&#21496;&#23458;&#25143;&#65292;&#29616;&#26377;&#30340;&#21305;&#37197;&#31995;&#32479;&#23578;&#26410;&#33021;&#22815;&#20805;&#20998;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;B2B&#35299;&#20915;&#26041;&#26696;&#21305;&#37197;&#38382;&#39064;&#65292;&#24182;&#30830;&#23450;&#20102;&#35813;&#22330;&#26223;&#30340;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;(1) &#22797;&#26434;&#22810;&#39046;&#22495;&#29305;&#24449;&#30340;&#24314;&#27169;&#21644;(2) &#26377;&#38480;&#12289;&#19981;&#23436;&#25972;&#21644;&#31232;&#30095;&#30340;&#20132;&#26131;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;CAMA&#65292;&#23427;&#20197;&#20998;&#23618;&#22810;&#39046;&#22495;&#21305;&#37197;&#32467;&#26500;&#20026;&#20027;&#24178;&#65292;&#24182;&#36890;&#36807;&#19977;&#31181;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#21644;&#23545;&#27604;&#39044;&#35757;&#32451;&#30446;&#26631;&#26469;&#24357;&#34917;&#21487;&#29992;&#25968;&#25454;&#30340;&#19981;&#23436;&#21892;&#20043;&#22788;&#12290;&#36890;&#36807;&#23545;&#19968;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Cloud solutions have gained significant popularity in the technology industry as they offer a combination of services and tools to tackle specific problems. However, despite their widespread use, the task of identifying appropriate company customers for a specific target solution to the sales team of a solution provider remains a complex business problem that existing matching systems have yet to adequately address. In this work, we study the B2B solution matching problem and identify two main challenges of this scenario: (1) the modeling of complex multi-field features and (2) the limited, incomplete, and sparse transaction data. To tackle these challenges, we propose a framework CAMA, which is built with a hierarchical multi-field matching structure as its backbone and supplemented by three data augmentation strategies and a contrastive pre-training objective to compensate for the imperfections in the available data. Through extensive experiments on a real-world dataset, we demonstra
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#33258;&#21160;&#26426;&#26469;&#32534;&#30721;&#39640;&#32423;&#30693;&#35782;&#65292;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#30340;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07069</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#21270;&#21644;&#21152;&#36895;&#22870;&#21169;&#26426;&#22120;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Using Large Language Models to Automate and Expedite Reinforcement Learning with Reward Machine
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07069
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#33258;&#21160;&#26426;&#26469;&#32534;&#30721;&#39640;&#32423;&#30693;&#35782;&#65292;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#30340;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;LARL-RM&#65288;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#29992;&#20110;&#22870;&#21169;&#26426;&#22120;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#26426;&#65289;&#31639;&#27861;&#65292;&#20197;&#23558;&#39640;&#32423;&#30693;&#35782;&#32534;&#30721;&#21040;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20351;&#29992;&#33258;&#21160;&#26426;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#33719;&#24471;&#39640;&#32423;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#23558;&#39640;&#32423;&#30693;&#35782;&#25552;&#20379;&#32473;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36825;&#38656;&#35201;&#19987;&#23478;&#26469;&#32534;&#30721;&#33258;&#21160;&#26426;&#12290;&#25105;&#20204;&#20351;&#29992;&#24605;&#32500;&#38142;&#21644;&#23569;&#26679;&#26412;&#26041;&#27861;&#36827;&#34892;&#25552;&#31034;&#24037;&#31243;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#36825;&#20123;&#26041;&#27861;&#19979;&#26377;&#25928;&#12290;&#27492;&#22806;&#65292;LARL-RM&#20801;&#35768;&#23436;&#20840;&#38381;&#29615;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#26080;&#38656;&#19987;&#23478;&#26469;&#25351;&#23548;&#21644;&#30417;&#30563;&#23398;&#20064;&#65292;&#22240;&#20026;LARL-RM&#21487;&#20197;&#30452;&#25509;&#20351;&#29992;LLM&#29983;&#25104;&#25152;&#38656;&#30340;&#39640;&#32423;&#30693;&#35782;&#20197;&#23436;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#31639;&#27861;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;LARL-RM&#30340;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#20854;&#23545;&#24120;&#35265;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20855;&#26377;&#38750;&#24120;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#19968;&#20123;&#20219;&#21153;&#19978;&#36229;&#36234;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present LARL-RM (Large language model-generated Automaton for Reinforcement Learning with Reward Machine) algorithm in order to encode high-level knowledge into reinforcement learning using automaton to expedite the reinforcement learning. Our method uses Large Language Models (LLM) to obtain high-level domain-specific knowledge using prompt engineering instead of providing the reinforcement learning algorithm directly with the high-level knowledge which requires an expert to encode the automaton. We use chain-of-thought and few-shot methods for prompt engineering and demonstrate that our method works using these approaches. Additionally, LARL-RM allows for fully closed-loop reinforcement learning without the need for an expert to guide and supervise the learning since LARL-RM can use the LLM directly to generate the required high-level knowledge for the task at hand. We also show the theoretical guarantee of our algorithm to converge to an optimal policy. We demonstrate that LARL-R
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; $L^*LM$ &#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#21644;&#28436;&#31034;&#23398;&#20064; DFA&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#25928;&#29575;&#65292;&#20855;&#22791;&#24378;&#22823;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.07051</link><description>&lt;p&gt;
$L^*LM$: &#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#31034;&#20363;&#23398;&#20064;&#33258;&#21160;&#26426;
&lt;/p&gt;
&lt;p&gt;
$L^*LM$: Learning Automata from Examples using Natural Language Oracles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07051
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; $L^*LM$ &#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#21644;&#28436;&#31034;&#23398;&#20064; DFA&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#25928;&#29575;&#65292;&#20855;&#22791;&#24378;&#22823;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19987;&#23478;&#28436;&#31034;&#24050;&#34987;&#35777;&#26126;&#26159;&#31616;&#21270;&#38388;&#25509;&#25351;&#23450;&#22797;&#26434;&#20219;&#21153;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#31639;&#27861;&#29978;&#33267;&#25903;&#25345;&#20174;&#28436;&#31034;&#20013;&#25552;&#21462;&#26126;&#30830;&#30340;&#24418;&#24335;&#35268;&#33539;&#65292;&#22914;&#30830;&#23450;&#24615;&#26377;&#38480;&#33258;&#21160;&#26426;&#65288;DFA&#65289;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#25216;&#26415;&#36890;&#24120;&#19981;&#20855;&#22791;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026; $L^*LM$ &#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#28436;&#31034;&#21644;&#33258;&#28982;&#35821;&#35328;&#20013;&#23398;&#20064; DFA&#12290;&#30001;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#23398;&#20064; DFA &#30340;&#25968;&#25454;&#25928;&#29575;&#26174;&#33879;&#25552;&#39640;&#12290;&#20174;&#25216;&#26415;&#19978;&#35762;&#65292;$L^*LM$ &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#22238;&#31572;&#20851;&#20110;&#24213;&#23618;&#20219;&#21153;&#30340;&#25104;&#21592;&#26597;&#35810;&#12290;&#28982;&#21518;&#23558;&#20854;&#19982;&#26368;&#36817;&#30340;&#28436;&#31034;&#23398;&#20064;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#23558;&#23398;&#20064;&#36716;&#21270;&#20026;&#19968;&#31995;&#21015;&#24102;&#26631;&#31614;&#31034;&#20363;&#23398;&#20064;&#38382;&#39064;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20004;&#31181;&#27169;&#24577;&#30456;&#20114;&#34917;&#20805;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Expert demonstrations have proven an easy way to indirectly specify complex tasks. Recent algorithms even support extracting unambiguous formal specifications, e.g. deterministic finite automata (DFA), from demonstrations. Unfortunately, these techniques are generally not sample efficient. In this work, we introduce $L^*LM$, an algorithm for learning DFAs from both demonstrations and natural language. Due to the expressivity of natural language, we observe a significant improvement in the data efficiency of learning DFAs from expert demonstrations. Technically, $L^*LM$ leverages large language models to answer membership queries about the underlying task. This is then combined with recent techniques for transforming learning from demonstrations into a sequence of labeled example learning problems. In our experiments, we observe the two modalities complement each other, yielding a powerful few-shot learner.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22240;&#23376;&#22270;&#27169;&#22411;&#34920;&#31034;&#26234;&#33021;&#20307;&#20043;&#38388;&#30456;&#20114;&#20381;&#36182;&#34892;&#20026;&#21644;&#21487;&#20449;&#24230;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20197;&#39640;&#26031;&#36807;&#31243;&#22240;&#23376;&#22270;&#23545;&#26426;&#22120;&#20154;&#30340;&#34892;&#20026;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#32771;&#34385;&#20102;&#24179;&#28369;&#24615;&#12289;&#36991;&#38556;&#21644;&#20449;&#20219;&#30456;&#20851;&#22240;&#32032;&#12290;&#20449;&#20219;&#35780;&#20272;&#26159;&#20998;&#25955;&#30340;&#65292;&#32771;&#34385;&#20102;&#25509;&#36817;&#23433;&#20840;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#21512;&#20316;&#24615;&#31561;&#20851;&#38190;&#22240;&#32032;&#12290;&#25972;&#20010;&#31995;&#32479;&#26159;&#30001;&#19968;&#31995;&#21015;&#22240;&#23376;&#22270;&#32452;&#25104;&#30340;&#32593;&#32476;&#65292;&#36890;&#36807;&#20449;&#20219;&#30456;&#20851;&#22240;&#32032;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#37319;&#29992;&#36125;&#21494;&#26031;&#25512;&#29702;&#26041;&#27861;&#21160;&#24577;&#35780;&#20272;&#20449;&#20219;&#12290;</title><link>https://arxiv.org/abs/2402.07049</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#20449;&#20219;&#22240;&#23376;&#22270;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Factor Graph Model of Trust for a Collaborative Multi-Agent System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07049
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22240;&#23376;&#22270;&#27169;&#22411;&#34920;&#31034;&#26234;&#33021;&#20307;&#20043;&#38388;&#30456;&#20114;&#20381;&#36182;&#34892;&#20026;&#21644;&#21487;&#20449;&#24230;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20197;&#39640;&#26031;&#36807;&#31243;&#22240;&#23376;&#22270;&#23545;&#26426;&#22120;&#20154;&#30340;&#34892;&#20026;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#32771;&#34385;&#20102;&#24179;&#28369;&#24615;&#12289;&#36991;&#38556;&#21644;&#20449;&#20219;&#30456;&#20851;&#22240;&#32032;&#12290;&#20449;&#20219;&#35780;&#20272;&#26159;&#20998;&#25955;&#30340;&#65292;&#32771;&#34385;&#20102;&#25509;&#36817;&#23433;&#20840;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#21512;&#20316;&#24615;&#31561;&#20851;&#38190;&#22240;&#32032;&#12290;&#25972;&#20010;&#31995;&#32479;&#26159;&#30001;&#19968;&#31995;&#21015;&#22240;&#23376;&#22270;&#32452;&#25104;&#30340;&#32593;&#32476;&#65292;&#36890;&#36807;&#20449;&#20219;&#30456;&#20851;&#22240;&#32032;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#37319;&#29992;&#36125;&#21494;&#26031;&#25512;&#29702;&#26041;&#27861;&#21160;&#24577;&#35780;&#20272;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#39046;&#22495;&#65292;&#20449;&#20219;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#36164;&#28304;&#21644;&#26381;&#21153;&#30340;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22240;&#23376;&#22270;&#27169;&#22411;&#26469;&#34920;&#31034;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#34892;&#20026;&#21644;&#21487;&#20449;&#24230;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#26426;&#22120;&#20154;&#30340;&#34892;&#20026;&#24314;&#27169;&#20026;&#21160;&#20316;&#36712;&#36857;&#65292;&#24182;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#22240;&#23376;&#22270;&#26469;&#32771;&#34385;&#24179;&#28369;&#24615;&#12289;&#36991;&#38556;&#21644;&#20449;&#20219;&#30456;&#20851;&#22240;&#32032;&#12290;&#25105;&#20204;&#30340;&#20449;&#20219;&#35780;&#20272;&#26041;&#27861;&#26159;&#20998;&#25955;&#30340;&#65292;&#32771;&#34385;&#20102;&#20851;&#38190;&#30340;&#30456;&#20114;&#20381;&#36182;&#23376;&#22240;&#32032;&#65292;&#22914;&#25509;&#36817;&#23433;&#20840;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#21512;&#20316;&#24615;&#12290;&#25972;&#20010;&#31995;&#32479;&#26159;&#30001;&#19968;&#31995;&#21015;&#22240;&#23376;&#22270;&#32452;&#25104;&#30340;&#32593;&#32476;&#65292;&#36890;&#36807;&#20449;&#20219;&#30456;&#20851;&#22240;&#32032;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#37319;&#29992;&#36125;&#21494;&#26031;&#25512;&#29702;&#26041;&#27861;&#21160;&#24577;&#35780;&#20272;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of Multi-Agent Systems (MAS), known for their openness, dynamism, and cooperative nature, the ability to trust the resources and services of other agents is crucial. Trust, in this setting, is the reliance and confidence an agent has in the information, behaviors, intentions, truthfulness, and capabilities of others within the system. Our paper introduces a new graphical approach that utilizes factor graphs to represent the interdependent behaviors and trustworthiness among agents. This includes modeling the behavior of robots as a trajectory of actions using a Gaussian process factor graph, which accounts for smoothness, obstacle avoidance, and trust-related factors. Our method for evaluating trust is decentralized and considers key interdependent sub-factors such as proximity safety, consistency, and cooperation. The overall system comprises a network of factor graphs that interact through trust-related factors and employs a Bayesian inference method to dynamically asses
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23610;&#24230;&#24459;&#30340;&#35270;&#35282;&#65292;&#30740;&#31350;&#20102;AI&#27169;&#22411;&#22823;&#23567;&#22686;&#38271;&#21644;&#21512;&#25104;&#25968;&#25454;&#24341;&#20837;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#27169;&#22411;&#21487;&#33021;&#20250;&#36973;&#36935;&#24635;&#20307;&#23849;&#28291;&#65292;&#24182;&#36890;&#36807;&#22823;&#35268;&#27169;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#19968;&#29702;&#35770;&#12290;</title><link>https://arxiv.org/abs/2402.07043</link><description>&lt;p&gt;
&#23614;&#24052;&#30340;&#25925;&#20107;&#65306;&#20316;&#20026;&#23610;&#24230;&#24459;&#21464;&#21270;&#30340;&#27169;&#22411;&#23849;&#28291;
&lt;/p&gt;
&lt;p&gt;
A Tale of Tails: Model Collapse as a Change of Scaling Laws
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23610;&#24230;&#24459;&#30340;&#35270;&#35282;&#65292;&#30740;&#31350;&#20102;AI&#27169;&#22411;&#22823;&#23567;&#22686;&#38271;&#21644;&#21512;&#25104;&#25968;&#25454;&#24341;&#20837;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#27169;&#22411;&#21487;&#33021;&#20250;&#36973;&#36935;&#24635;&#20307;&#23849;&#28291;&#65292;&#24182;&#36890;&#36807;&#22823;&#35268;&#27169;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#19968;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;AI&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#38271;&#65292;&#31070;&#32463;&#23610;&#24230;&#24459;&#24050;&#25104;&#20026;&#39044;&#27979;&#22823;&#27169;&#22411;&#22312;&#25193;&#23481;&#21644;&#21407;&#22987;&#65288;&#20154;&#31867;&#25110;&#33258;&#28982;&#65289;&#35757;&#32451;&#25968;&#25454;&#22823;&#23567;&#22686;&#21152;&#26102;&#25913;&#21892;&#30340;&#20851;&#38190;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#27969;&#34892;&#27169;&#22411;&#30340;&#24191;&#27867;&#20351;&#29992;&#24847;&#21619;&#30528;&#22312;&#32447;&#25968;&#25454;&#21644;&#25991;&#26412;&#30340;&#29983;&#24577;&#31995;&#32479;&#23558;&#36880;&#28176;&#21253;&#21547;&#36234;&#26469;&#36234;&#22810;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#38382;&#65306;&#24403;&#21512;&#25104;&#25968;&#25454;&#36827;&#20837;&#35757;&#32451;&#35821;&#26009;&#24211;&#26102;&#65292;&#23610;&#24230;&#24459;&#20250;&#22914;&#20309;&#25913;&#21464;&#65311;&#26410;&#26469;&#30340;&#27169;&#22411;&#20173;&#20250;&#25913;&#21892;&#65292;&#36824;&#26159;&#27880;&#23450;&#20250;&#23436;&#20840;&#23849;&#28291;&#65288;&#27169;&#22411;&#23849;&#28291;&#65289;&#65311;&#36890;&#36807;&#23610;&#24230;&#24459;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#27169;&#22411;&#23849;&#28291;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#24191;&#27867;&#30340;&#34928;&#20943;&#29616;&#35937;&#65292;&#20998;&#26512;&#20102;&#23610;&#24230;&#30340;&#20007;&#22833;&#12289;&#19982;&#20195;&#25968;&#30340;&#21464;&#21270;&#23610;&#24230;&#12289;&#25216;&#33021;&#30340;"&#36951;&#24536;"&#20197;&#21450;&#28151;&#21512;&#20154;&#31867;&#21644;&#21512;&#25104;&#25968;&#25454;&#26102;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#36890;&#36807;&#23545;&#19968;&#20010;&#31639;&#26415;&#20219;&#21153;&#21644;&#25991;&#26412;&#29983;&#25104;&#30340;&#36716;&#25442;&#22120;&#36827;&#34892;&#22823;&#35268;&#27169;&#23454;&#39564;&#35777;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
As AI model size grows, neural scaling laws have become a crucial tool to predict the improvements of large models when increasing capacity and the size of original (human or natural) training data. Yet, the widespread use of popular models means that the ecosystem of online data and text will co-evolve to progressively contain increased amounts of synthesized data. In this paper we ask: How will the scaling laws change in the inevitable regime where synthetic data makes its way into the training corpus? Will future models, still improve, or be doomed to degenerate up to total (model) collapse? We develop a theoretical framework of model collapse through the lens of scaling laws. We discover a wide range of decay phenomena, analyzing loss of scaling, shifted scaling with number of generations, the ''un-learning" of skills, and grokking when mixing human and synthesized data. Our theory is validated by large-scale experiments with a transformer on an arithmetic task and text generation 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#38382;&#39064;&#30340;&#21327;&#35843;&#32570;&#38519;&#25259;&#38706;&#65288;CFD&#65289;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#30446;&#21069;&#39046;&#22495;&#20013;&#32570;&#20047;&#32467;&#26500;&#21270;&#36807;&#31243;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.07039</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#21327;&#35843;&#25259;&#38706;&#65306;&#36229;&#36234;&#23433;&#20840;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
Coordinated Disclosure for AI: Beyond Security Vulnerabilities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07039
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#38382;&#39064;&#30340;&#21327;&#35843;&#32570;&#38519;&#25259;&#38706;&#65288;CFD&#65289;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#30446;&#21069;&#39046;&#22495;&#20013;&#32570;&#20047;&#32467;&#26500;&#21270;&#36807;&#31243;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#20260;&#23475;&#25253;&#21578;&#22312;&#25259;&#38706;&#25110;&#35299;&#20915;&#31639;&#27861;&#32570;&#38519;&#26041;&#38754;&#20173;&#28982;&#26159;&#19968;&#31181;&#20020;&#26102;&#24615;&#30340;&#25805;&#20316;&#65292;&#32570;&#20047;&#32467;&#26500;&#21270;&#30340;&#36807;&#31243;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#21327;&#35843;&#28431;&#27934;&#25259;&#38706;&#65288;CVD&#65289;&#30340;&#20262;&#29702;&#21644;&#29983;&#24577;&#31995;&#32479;&#22312;&#36719;&#20214;&#23433;&#20840;&#21644;&#36879;&#26126;&#24230;&#26041;&#38754;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#32654;&#22269;&#30340;&#32972;&#26223;&#19979;&#65292;&#20026;&#20102;&#40723;&#21169;&#31177;&#25345;&#21892;&#24847;&#34892;&#20107;&#30340;&#23433;&#20840;&#30740;&#31350;&#20154;&#21592;&#65292;&#24314;&#31435;&#19968;&#20010;&#23433;&#20840;&#38450;&#25252;&#26465;&#27454;&#20197;&#23545;&#25239;&#35745;&#31639;&#26426;&#27450;&#35784;&#21644;&#28389;&#29992;&#27861;&#26696;&#19968;&#30452;&#23384;&#22312;&#38271;&#26399;&#30340;&#27861;&#24459;&#21644;&#25919;&#31574;&#26007;&#20105;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#20013;&#30340;&#31639;&#27861;&#32570;&#38519;&#19982;&#20256;&#32479;&#36719;&#20214;&#28431;&#27934;&#23384;&#22312;&#30528;&#19981;&#21516;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#19968;&#31181;&#19987;&#38376;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#38382;&#39064;&#29305;&#27530;&#22797;&#26434;&#24615;&#30340;&#19987;&#38376;&#21327;&#35843;&#32570;&#38519;&#25259;&#38706;&#65288;CFD&#65289;&#26694;&#26550;&#30340;&#23454;&#26045;&#12290;&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;ML&#20013;&#30340;&#25259;&#38706;&#21382;&#21490;&#32972;&#26223;&#65292;&#21253;&#25324;
&lt;/p&gt;
&lt;p&gt;
Harm reporting in the field of Artificial Intelligence (AI) currently operates on an ad hoc basis, lacking a structured process for disclosing or addressing algorithmic flaws. In contrast, the Coordinated Vulnerability Disclosure (CVD) ethos and ecosystem play a pivotal role in software security and transparency. Within the U.S. context, there has been a protracted legal and policy struggle to establish a safe harbor from the Computer Fraud and Abuse Act, aiming to foster institutional support for security researchers acting in good faith. Notably, algorithmic flaws in Machine Learning (ML) models present distinct challenges compared to traditional software vulnerabilities, warranting a specialized approach. To address this gap, we propose the implementation of a dedicated Coordinated Flaw Disclosure (CFD) framework tailored to the intricacies of machine learning and artificial intelligence issues. This paper delves into the historical landscape of disclosures in ML, encompassing the a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23558;&#31526;&#21495;&#36125;&#21494;&#26031;&#27169;&#22411;&#30340;&#20808;&#39564;&#30693;&#35782;&#25552;&#21462;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#20351;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#26174;&#31034;&#24402;&#32435;&#20559;&#35265;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#21152;&#24555;&#23545;&#25277;&#35937;&#27010;&#24565;&#30340;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.07035</link><description>&lt;p&gt;
&#23558;&#31526;&#21495;&#20808;&#39564;&#30693;&#35782;&#34701;&#20837;&#31070;&#32463;&#32593;&#32476;&#30340;&#25277;&#35937;&#27010;&#24565;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Distilling Symbolic Priors for Concept Learning into Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07035
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23558;&#31526;&#21495;&#36125;&#21494;&#26031;&#27169;&#22411;&#30340;&#20808;&#39564;&#30693;&#35782;&#25552;&#21462;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#20351;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#26174;&#31034;&#24402;&#32435;&#20559;&#35265;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#21152;&#24555;&#23545;&#25277;&#35937;&#27010;&#24565;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#24402;&#32435;&#20559;&#35265;&#20174;&#23569;&#37327;&#30340;&#31034;&#20363;&#20013;&#23398;&#20064;&#26032;&#30340;&#27010;&#24565;&#12290;&#36825;&#20123;&#24402;&#32435;&#20559;&#35265;&#20808;&#21069;&#24050;&#36890;&#36807;&#22312;&#31526;&#21495;&#20551;&#35774;&#31354;&#38388;&#19978;&#23450;&#20041;&#36125;&#21494;&#26031;&#27169;&#22411;&#26469;&#25429;&#25417;&#12290;&#26159;&#21542;&#21487;&#33021;&#21019;&#24314;&#19968;&#20010;&#26174;&#31034;&#30456;&#21516;&#24402;&#32435;&#20559;&#35265;&#30340;&#31070;&#32463;&#32593;&#32476;&#65311;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#20803;&#23398;&#20064;&#65288;&#19968;&#31181;&#20174;&#19968;&#32452;&#20219;&#21153;&#20013;&#25552;&#21462;&#20849;&#21516;&#32467;&#26500;&#30340;&#26041;&#27861;&#65289;&#23558;&#31526;&#21495;&#36125;&#21494;&#26031;&#27169;&#22411;&#30340;&#20808;&#39564;&#20998;&#24067;&#25552;&#21462;&#21040;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#21487;&#20197;&#23454;&#20363;&#21270;&#33021;&#22815;&#24555;&#36895;&#23398;&#20064;&#27010;&#24565;&#30340;&#24402;&#32435;&#20559;&#35265;&#12290;&#36890;&#36807;&#20197;&#36125;&#21494;&#26031;&#27169;&#22411;&#30340;&#20808;&#39564;&#20998;&#24067;&#29983;&#25104;&#20803;&#23398;&#20064;&#20013;&#20351;&#29992;&#30340;&#20219;&#21153;&#38598;&#65292;&#25105;&#20204;&#33021;&#22815;&#23558;&#35813;&#20808;&#39564;&#20256;&#36755;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#21019;&#24314;&#20102;&#19968;&#20010;&#20855;&#26377;&#23545;&#20197;&#30701;&#36923;&#36753;&#20844;&#24335;&#34920;&#31034;&#30340;&#27010;&#24565;&#30340;&#24402;&#32435;&#20559;&#35265;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#36890;&#36807;&#20998;&#26512;&#20808;&#21069;&#20154;&#20204;&#20174;&#23569;&#37327;&#31034;&#20363;&#20013;&#23398;&#20064;&#36923;&#36753;&#27010;&#24565;&#30340;&#34892;&#20026;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#20803;&#35757;&#32451;&#26041;&#26696;&#21487;&#20197;&#20351;&#31070;&#32463;&#32593;&#32476;&#24555;&#36895;&#23398;&#20064;&#36825;&#20123;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans can learn new concepts from a small number of examples by drawing on their inductive biases. These inductive biases have previously been captured by using Bayesian models defined over symbolic hypothesis spaces. Is it possible to create a neural network that displays the same inductive biases? We show that inductive biases that enable rapid concept learning can be instantiated in artificial neural networks by distilling a prior distribution from a symbolic Bayesian model via meta-learning, an approach for extracting the common structure from a set of tasks. By generating the set of tasks used in meta-learning from the prior distribution of a Bayesian model, we are able to transfer that prior into a neural network. We use this approach to create a neural network with an inductive bias towards concepts expressed as short logical formulas. Analyzing results from previous behavioral experiments in which people learned logical concepts from a few examples, we find that our meta-train
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Fiddler&#65292;&#19968;&#31181;&#29992;&#20110;Mixture-of-Experts&#27169;&#22411;&#30340;&#36164;&#28304;&#39640;&#25928;&#25512;&#26029;&#24341;&#25806;&#65292;&#36890;&#36807;CPU-GPU&#32534;&#25490;&#23454;&#29616;&#26368;&#23567;&#21270;&#25968;&#25454;&#20256;&#36755;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#25552;&#39640;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#25512;&#26029;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.07033</link><description>&lt;p&gt;
Fiddler&#65306;&#29992;&#20110;Mixture-of-Experts&#27169;&#22411;&#24555;&#36895;&#25512;&#26029;&#30340;CPU-GPU&#32534;&#25490;
&lt;/p&gt;
&lt;p&gt;
Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Fiddler&#65292;&#19968;&#31181;&#29992;&#20110;Mixture-of-Experts&#27169;&#22411;&#30340;&#36164;&#28304;&#39640;&#25928;&#25512;&#26029;&#24341;&#25806;&#65292;&#36890;&#36807;CPU-GPU&#32534;&#25490;&#23454;&#29616;&#26368;&#23567;&#21270;&#25968;&#25454;&#20256;&#36755;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#25552;&#39640;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#25512;&#26029;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Mixture-of-Experts&#65288;MoE&#65289;&#26550;&#26500;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29615;&#22659;&#19979;&#36816;&#34892;&#36825;&#20123;&#27169;&#22411;&#65292;&#21363;GPU&#20869;&#23384;&#36164;&#28304;&#19981;&#20016;&#23500;&#30340;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#27169;&#22411;&#35268;&#27169;&#24222;&#22823;&#65292;&#23384;&#22312;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#23558;&#27169;&#22411;&#26435;&#37325;&#21368;&#36733;&#21040;CPU&#20869;&#23384;&#30340;&#31995;&#32479;&#65292;&#30001;&#20110;&#39057;&#32321;&#22320;&#22312;CPU&#21644;GPU&#20043;&#38388;&#31227;&#21160;&#25968;&#25454;&#32780;&#23548;&#33268;&#26174;&#33879;&#30340;&#24320;&#38144;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Fiddler&#65292;&#19968;&#31181;&#29992;&#20110;MoE&#27169;&#22411;&#30340;&#36164;&#28304;&#39640;&#25928;&#25512;&#26029;&#24341;&#25806;&#65292;&#23454;&#29616;&#20102;CPU-GPU&#32534;&#25490;&#12290;Fiddler&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#21033;&#29992;CPU&#30340;&#35745;&#31639;&#33021;&#21147;&#26469;&#26368;&#23567;&#21270;CPU&#21644;GPU&#20043;&#38388;&#30340;&#25968;&#25454;&#20256;&#36755;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;Fiddler&#33021;&#22815;&#22312;&#21333;&#20010;&#20855;&#26377;24GB&#20869;&#23384;&#30340;GPU&#19978;&#36816;&#34892;&#26410;&#21387;&#32553;&#30340;Mixtral-8x7B&#27169;&#22411;&#65288;&#21442;&#25968;&#36229;&#36807;90GB&#65289;&#65292;&#27599;&#31186;&#29983;&#25104;&#36229;&#36807;3&#20010;token&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#25552;&#39640;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;Fiddler&#30340;&#20195;&#30721;&#21487;&#20197;&#20844;&#24320;&#35775;&#38382;&#65292;&#32593;&#22336;&#20026;\url{https://github.com/efeslab/fiddler}
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) based on Mixture-of-Experts (MoE) architecture are showing promising performance on various tasks. However, running them on resource-constrained settings, where GPU memory resources are not abundant, is challenging due to huge model sizes. Existing systems that offload model weights to CPU memory suffer from the significant overhead of frequently moving data between CPU and GPU. In this paper, we propose Fiddler, a resource-efficient inference engine with CPU-GPU orchestration for MoE models. The key idea of Fiddler is to use the computation ability of the CPU to minimize the data movement between the CPU and GPU. Our evaluation shows that Fiddler can run the uncompressed Mixtral-8x7B model, which exceeds 90GB in parameters, to generate over $3$ tokens per second on a single GPU with 24GB memory, showing an order of magnitude improvement over existing methods. The code of Fiddler is publicly available at \url{https://github.com/efeslab/fiddler}
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23454;&#20363;&#32423;&#21035;&#30340;&#21512;&#25104;&#25968;&#25454;&#36136;&#37327;&#19982;&#23433;&#20840;&#24863;&#30693;&#65292;&#24341;&#20837;&#20102;&#22235;&#31181;&#36229;&#36234;&#32431;&#35270;&#35273;&#29305;&#24449;&#30340;&#21512;&#25104;&#25968;&#25454;&#36136;&#37327;&#65292;&#24182;&#25552;&#20986;&#20102;&#20248;&#21270;&#26041;&#27861;&#26469;&#20943;&#23569;&#21512;&#25104;&#21644;&#30495;&#23454;&#22270;&#20687;&#20043;&#38388;&#30340;&#36136;&#37327;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.07031</link><description>&lt;p&gt;
&#23454;&#20363;&#32423;&#21035;&#30340;&#23433;&#20840;&#24863;&#30693;&#19982;&#21512;&#25104;&#25968;&#25454;&#36136;&#37327;&#21450;&#20854;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Instance-Level Safety-Aware Fidelity of Synthetic Data and Its Calibration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23454;&#20363;&#32423;&#21035;&#30340;&#21512;&#25104;&#25968;&#25454;&#36136;&#37327;&#19982;&#23433;&#20840;&#24863;&#30693;&#65292;&#24341;&#20837;&#20102;&#22235;&#31181;&#36229;&#36234;&#32431;&#35270;&#35273;&#29305;&#24449;&#30340;&#21512;&#25104;&#25968;&#25454;&#36136;&#37327;&#65292;&#24182;&#25552;&#20986;&#20102;&#20248;&#21270;&#26041;&#27861;&#26469;&#20943;&#23569;&#21512;&#25104;&#21644;&#30495;&#23454;&#22270;&#20687;&#20043;&#38388;&#30340;&#36136;&#37327;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#27169;&#21644;&#26657;&#20934;&#21512;&#25104;&#25968;&#25454;&#30340;&#36136;&#37327;&#23545;&#22609;&#36896;&#26410;&#26469;&#23433;&#20840;&#21487;&#38752;&#30340;&#33258;&#21160;&#39550;&#39542;&#25216;&#26415;&#33267;&#20851;&#37325;&#35201;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#39640;&#19988;&#21487;&#25193;&#23637;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#21487;&#20197;&#21462;&#20195;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#25910;&#38598;&#12290;&#25105;&#20204;&#20851;&#27880;&#20854;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#30340;&#20316;&#29992;&#65292;&#24341;&#20837;&#20102;&#36229;&#36234;&#32431;&#35270;&#35273;&#36755;&#20837;&#29305;&#24449;&#30340;&#22235;&#31181;&#23454;&#20363;&#32423;&#21035;&#36136;&#37327;&#65292;&#26088;&#22312;&#20351;&#21512;&#25104;&#25968;&#25454;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#23433;&#20840;&#38382;&#39064;&#30456;&#19968;&#33268;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#26041;&#27861;&#26469;&#25913;&#36827;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#20943;&#23569;&#30001;&#22522;&#20110;DNN&#30340;&#32452;&#20214;&#35782;&#21035;&#20986;&#30340;&#36136;&#37327;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#35843;&#20248;&#21487;&#20197;&#22686;&#24378;&#21512;&#25104;&#21644;&#30495;&#23454;&#22270;&#20687;&#20013;&#23433;&#20840;&#20851;&#38190;&#38169;&#35823;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling and calibrating the fidelity of synthetic data is paramount in shaping the future of safe and reliable self-driving technology by offering a cost-effective and scalable alternative to real-world data collection. We focus on its role in safety-critical applications, introducing four types of instance-level fidelity that go beyond mere visual input characteristics. The aim is to align synthetic data with real-world safety issues. We suggest an optimization method to refine the synthetic data generator, reducing fidelity gaps identified by the DNN-based component. Our findings show this tuning enhances the correlation between safety-critical errors in synthetic and real images.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#21512;&#35780;&#20272;&#20102;&#24320;&#28304;LLM&#21644;&#35895;&#27468;&#30340;&#22810;&#27169;&#24577;LLM Gemini &#22312;&#21307;&#23398;&#25512;&#29702;&#12289;&#24187;&#35273;&#26816;&#27979;&#21644;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#12290;Gemini&#22312;&#35786;&#26029;&#20934;&#30830;&#24615;&#26041;&#38754;&#33853;&#21518;&#20110;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#19988;&#26131;&#20986;&#29616;&#24187;&#35273;&#12289;&#36807;&#24230;&#33258;&#20449;&#21644;&#30693;&#35782;&#30450;&#28857;&#12290;&#37319;&#29992;&#25552;&#31034;&#31574;&#30053;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.07023</link><description>&lt;p&gt;
&#21452;&#23376;&#24231;&#36827;&#20837;&#21307;&#23398;&#38498;&#65306;&#25506;&#32034;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#25361;&#25112;&#38382;&#39064;&#21644;&#24187;&#35273;&#19978;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Gemini Goes to Med School: Exploring the Capabilities of Multimodal Large Language Models on Medical Challenge Problems &amp; Hallucinations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07023
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#21512;&#35780;&#20272;&#20102;&#24320;&#28304;LLM&#21644;&#35895;&#27468;&#30340;&#22810;&#27169;&#24577;LLM Gemini &#22312;&#21307;&#23398;&#25512;&#29702;&#12289;&#24187;&#35273;&#26816;&#27979;&#21644;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#12290;Gemini&#22312;&#35786;&#26029;&#20934;&#30830;&#24615;&#26041;&#38754;&#33853;&#21518;&#20110;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#19988;&#26131;&#20986;&#29616;&#24187;&#35273;&#12289;&#36807;&#24230;&#33258;&#20449;&#21644;&#30693;&#35782;&#30450;&#28857;&#12290;&#37319;&#29992;&#25552;&#31034;&#31574;&#30053;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#30103;&#34892;&#19994;&#20855;&#26377;&#28508;&#22312;&#20215;&#20540;&#65292;&#20294;&#36890;&#36807;&#20005;&#26684;&#35780;&#20272;&#26469;&#39564;&#35777;&#20854;&#23433;&#20840;&#24615;&#21644;&#25928;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20840;&#38754;&#35780;&#20272;&#20102;&#24320;&#28304;LLM&#21644;&#35895;&#27468;&#30340;&#26032;&#22411;&#22810;&#27169;&#24577;LLM Gemini &#22312;&#21307;&#23398;&#25512;&#29702;&#12289;&#24187;&#35273;&#26816;&#27979;&#21644;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#12290;&#34429;&#28982;Gemini&#34920;&#29616;&#20986;&#19968;&#23450;&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#35786;&#26029;&#20934;&#30830;&#24615;&#26041;&#38754;&#33853;&#21518;&#20110;MedPaLM 2&#21644;GPT-4&#31561;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;Gemini&#22312;&#21307;&#23398;VQA&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#20026;61.45&#65285;&#65292;&#26126;&#26174;&#20302;&#20110;GPT-4V&#30340;88&#65285;&#24471;&#20998;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21457;&#29616;&#65292;Gemini&#26497;&#26131;&#20986;&#29616;&#24187;&#35273;&#12289;&#36807;&#24230;&#33258;&#20449;&#21644;&#30693;&#35782;&#30450;&#28857;&#65292;&#36825;&#34920;&#26126;&#22914;&#26524;&#19981;&#21152;&#25209;&#21028;&#22320;&#37096;&#32626;&#65292;&#23384;&#22312;&#39118;&#38505;&#12290;&#25105;&#20204;&#36824;&#38024;&#23545;&#19981;&#21516;&#21307;&#23398;&#23398;&#31185;&#21644;&#27979;&#35797;&#31867;&#22411;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#20026;&#24320;&#21457;&#20154;&#21592;&#21644;&#20020;&#24202;&#21307;&#29983;&#25552;&#20379;&#20102;&#21487;&#25805;&#20316;&#30340;&#21453;&#39304;&#12290;&#20026;&#20102;&#20943;&#23569;&#39118;&#38505;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#25552;&#31034;&#31574;&#30053;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have the potential to be valuable in the healthcare industry, but it's crucial to verify their safety and effectiveness through rigorous evaluation. For this purpose, we comprehensively evaluated both open-source LLMs and Google's new multimodal LLM called Gemini across Medical reasoning, hallucination detection, and Medical Visual Question Answering tasks. While Gemini showed competence, it lagged behind state-of-the-art models like MedPaLM 2 and GPT-4 in diagnostic accuracy. Additionally, Gemini achieved an accuracy of 61.45\% on the medical VQA dataset, significantly lower than GPT-4V's score of 88\%. Our analysis revealed that Gemini is highly susceptible to hallucinations, overconfidence, and knowledge gaps, which indicate risks if deployed uncritically. We also performed a detailed analysis by medical subject and test type, providing actionable feedback for developers and clinicians. To mitigate risks, we applied prompting strategies that improved performanc
&lt;/p&gt;</description></item><item><title>REALM&#26159;&#19968;&#20010;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#22810;&#27169;&#24577;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#30340;&#34920;&#24449;&#65292;&#20197;&#25552;&#39640;&#20020;&#24202;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.07016</link><description>&lt;p&gt;
REALM:&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#22810;&#27169;&#24577;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20998;&#26512;&#30340;&#22686;&#24378;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
REALM: RAG-Driven Enhancement of Multimodal Electronic Health Records Analysis via Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07016
&lt;/p&gt;
&lt;p&gt;
REALM&#26159;&#19968;&#20010;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#22810;&#27169;&#24577;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#30340;&#34920;&#24449;&#65292;&#20197;&#25552;&#39640;&#20020;&#24202;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#25968;&#25454;&#30340;&#25972;&#21512;&#26174;&#33879;&#25552;&#39640;&#20102;&#20020;&#24202;&#39044;&#27979;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#27169;&#22411;&#22312;&#21033;&#29992;&#20020;&#24202;&#31508;&#35760;&#21644;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;EHR&#26102;&#24448;&#24448;&#32570;&#20047;&#19982;&#20020;&#24202;&#20219;&#21153;&#30456;&#20851;&#30340;&#21307;&#30103;&#32972;&#26223;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;REALM&#26694;&#26550;&#65292;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#39537;&#21160;&#26469;&#22686;&#24378;&#22810;&#27169;&#24577;EHR&#34920;&#24449;&#65292;&#36827;&#32780;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#12290;&#35813;&#26694;&#26550;&#39318;&#20808;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#32534;&#30721;&#38271;&#19978;&#19979;&#25991;&#20020;&#24202;&#31508;&#35760;&#65292;&#24182;&#24212;&#29992;GRU&#27169;&#22411;&#32534;&#30721;&#26102;&#38388;&#24207;&#21015;EHR&#25968;&#25454;&#65307;&#20854;&#27425;&#65292;&#25105;&#20204;&#25351;&#23548;LLM&#25552;&#21462;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#21307;&#23398;&#23454;&#20307;&#65292;&#24182;&#23558;&#23454;&#20307;&#19982;&#19987;&#19994;&#26631;&#27880;&#30340;&#22806;&#37096;&#30693;&#35782;&#22270;&#65288;PrimeKG&#65289;&#20013;&#30340;&#30456;&#24212;&#23454;&#20307;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
The integration of multimodal Electronic Health Records (EHR) data has significantly improved clinical predictive capabilities. Leveraging clinical notes and multivariate time-series EHR, existing models often lack the medical context relevent to clinical tasks, prompting the incorporation of external knowledge, particularly from the knowledge graph (KG). Previous approaches with KG knowledge have primarily focused on structured knowledge extraction, neglecting unstructured data modalities and semantic high dimensional medical knowledge. In response, we propose REALM, a Retrieval-Augmented Generation (RAG) driven framework to enhance multimodal EHR representations that address these limitations. Firstly, we apply Large Language Model (LLM) to encode long context clinical notes and GRU model to encode time-series EHR data. Secondly, we prompt LLM to extract task-relevant medical entities and match entities in professionally labeled external knowledge graph (PrimeKG) with corresponding m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FedImpro&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#25913;&#36827;&#30340;&#26412;&#22320;&#27169;&#22411;&#26469;&#20943;&#36731;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23458;&#25143;&#28418;&#31227;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20998;&#26512;&#26412;&#22320;&#35757;&#32451;&#30340;&#27867;&#21270;&#36129;&#29486;&#65292;&#24182;&#21033;&#29992;&#31867;&#20284;&#30340;&#26465;&#20214;&#20998;&#24067;&#36827;&#34892;&#35757;&#32451;&#65292;&#22686;&#24378;&#20102;&#27867;&#21270;&#36129;&#29486;&#24182;&#20943;&#23567;&#20102;&#26799;&#24230;&#30340;&#24046;&#24322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07011</link><description>&lt;p&gt;
FedImpro: &#27979;&#37327;&#21644;&#25913;&#21892;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23458;&#25143;&#26356;&#26032;
&lt;/p&gt;
&lt;p&gt;
FedImpro: Measuring and Improving Client Update in Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FedImpro&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#25913;&#36827;&#30340;&#26412;&#22320;&#27169;&#22411;&#26469;&#20943;&#36731;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23458;&#25143;&#28418;&#31227;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20998;&#26512;&#26412;&#22320;&#35757;&#32451;&#30340;&#27867;&#21270;&#36129;&#29486;&#65292;&#24182;&#21033;&#29992;&#31867;&#20284;&#30340;&#26465;&#20214;&#20998;&#24067;&#36827;&#34892;&#35757;&#32451;&#65292;&#22686;&#24378;&#20102;&#27867;&#21270;&#36129;&#29486;&#24182;&#20943;&#23567;&#20102;&#26799;&#24230;&#30340;&#24046;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#20250;&#21463;&#21040;&#24322;&#26500;&#25968;&#25454;&#24341;&#36215;&#30340;&#23458;&#25143;&#28418;&#31227;&#30340;&#24433;&#21709;&#65292;&#20854;&#20013;&#25968;&#25454;&#30340;&#20998;&#24067;&#22312;&#19981;&#21516;&#30340;&#23458;&#25143;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20808;&#36827;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20110;&#25805;&#20316;&#29616;&#26377;&#30340;&#26799;&#24230;&#65292;&#20197;&#23454;&#29616;&#26356;&#19968;&#33268;&#30340;&#23458;&#25143;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#21478;&#19968;&#20010;&#35282;&#24230;&#20998;&#26512;&#20102;&#23458;&#25143;&#28418;&#31227;&#65292;&#24182;&#26088;&#22312;&#36890;&#36807;&#29983;&#25104;&#25913;&#36827;&#30340;&#26412;&#22320;&#27169;&#22411;&#26469;&#20943;&#36731;&#36825;&#31181;&#28418;&#31227;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#26412;&#22320;&#35757;&#32451;&#30340;&#27867;&#21270;&#36129;&#29486;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;&#65292;&#36825;&#31181;&#27867;&#21270;&#36129;&#29486;&#21463;&#21040;&#19981;&#21516;&#23458;&#25143;&#30340;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#30340;&#26465;&#20214;Wasserstein&#36317;&#31163;&#30340;&#38480;&#21046;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedImpro&#65292;&#29992;&#20110;&#26500;&#24314;&#31867;&#20284;&#30340;&#26465;&#20214;&#20998;&#24067;&#36827;&#34892;&#26412;&#22320;&#35757;&#32451;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;FedImpro&#23558;&#27169;&#22411;&#20998;&#35299;&#20026;&#39640;&#23618;&#21644;&#20302;&#23618;&#32452;&#20214;&#65292;&#24182;&#23545;&#37325;&#26500;&#29305;&#24449;&#20998;&#24067;&#19978;&#30340;&#39640;&#23618;&#37096;&#20998;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#31181;&#26041;&#27861;&#22686;&#24378;&#20102;&#27867;&#21270;&#36129;&#29486;&#65292;&#24182;&#20943;&#23567;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#26799;&#24230;&#30340;&#24046;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) models often experience client drift caused by heterogeneous data, where the distribution of data differs across clients. To address this issue, advanced research primarily focuses on manipulating the existing gradients to achieve more consistent client models. In this paper, we present an alternative perspective on client drift and aim to mitigate it by generating improved local models. First, we analyze the generalization contribution of local training and conclude that this generalization contribution is bounded by the conditional Wasserstein distance between the data distribution of different clients. Then, we propose FedImpro, to construct similar conditional distributions for local training. Specifically, FedImpro decouples the model into high-level and low-level components, and trains the high-level portion on reconstructed feature distributions. This approach enhances the generalization contribution and reduces the dissimilarity of gradients in FL. Exper
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedCEO&#30340;&#26032;&#22411;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#35753;&#23458;&#25143;&#31471;&#30456;&#20114;&#21327;&#20316;&#65292;&#23454;&#29616;&#20102;&#23545;&#27169;&#22411;&#25928;&#29992;&#21644;&#38544;&#31169;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#36890;&#36807;&#39640;&#25928;&#30340;&#24352;&#37327;&#20302;&#31209;&#36817;&#31471;&#20248;&#21270;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#24674;&#22797;&#34987;&#25171;&#26029;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#22312;&#25928;&#29992;-&#38544;&#31169;&#26435;&#34913;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.07002</link><description>&lt;p&gt;
&#23458;&#25143;&#31471;&#21327;&#20316;&#65306;&#20855;&#26377;&#20445;&#35777;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#25913;&#36827;&#30340;&#28789;&#27963;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Clients Collaborate: Flexible Differentially Private Federated Learning with Guaranteed Improvement of Utility-Privacy Trade-off
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedCEO&#30340;&#26032;&#22411;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#35753;&#23458;&#25143;&#31471;&#30456;&#20114;&#21327;&#20316;&#65292;&#23454;&#29616;&#20102;&#23545;&#27169;&#22411;&#25928;&#29992;&#21644;&#38544;&#31169;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#36890;&#36807;&#39640;&#25928;&#30340;&#24352;&#37327;&#20302;&#31209;&#36817;&#31471;&#20248;&#21270;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#24674;&#22797;&#34987;&#25171;&#26029;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#22312;&#25928;&#29992;-&#38544;&#31169;&#26435;&#34913;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#38450;&#27490;&#29992;&#25143;&#25968;&#25454;&#30340;&#38544;&#31169;&#27844;&#28431;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#24191;&#27867;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#65292;&#20294;&#23427;&#24182;&#19981;&#26159;&#20813;&#36153;&#30340;&#12290;&#22122;&#22768;&#30340;&#28155;&#21152;&#20250;&#38543;&#26426;&#24178;&#25200;&#27169;&#22411;&#30340;&#35821;&#20041;&#23436;&#25972;&#24615;&#65292;&#24182;&#19988;&#36825;&#31181;&#24178;&#25200;&#20250;&#38543;&#30528;&#36890;&#20449;&#36718;&#27425;&#30340;&#22686;&#21152;&#32780;&#32047;&#31215;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20855;&#26377;&#20005;&#26684;&#38544;&#31169;&#20445;&#35777;&#30340;&#26032;&#22411;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21517;&#20026;FedCEO&#65292;&#36890;&#36807;&#35753;&#23458;&#25143;&#31471;"&#30456;&#20114;&#21327;&#20316;"&#65292;&#26088;&#22312;&#22312;&#27169;&#22411;&#25928;&#29992;&#21644;&#29992;&#25143;&#38544;&#31169;&#20043;&#38388;&#25214;&#21040;&#19968;&#31181;&#26435;&#34913;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#26381;&#21153;&#22120;&#19978;&#23545;&#22534;&#21472;&#30340;&#26412;&#22320;&#27169;&#22411;&#21442;&#25968;&#36827;&#34892;&#20102;&#39640;&#25928;&#30340;&#24352;&#37327;&#20302;&#31209;&#36817;&#31471;&#20248;&#21270;&#65292;&#23637;&#31034;&#20102;&#23427;&#22312;&#20809;&#35889;&#31354;&#38388;&#20013;&#28789;&#27963;&#25130;&#26029;&#39640;&#39057;&#32452;&#20998;&#30340;&#33021;&#21147;&#12290;&#36825;&#24847;&#21619;&#30528;&#25105;&#20204;&#30340;FedCEO&#33021;&#22815;&#36890;&#36807;&#24179;&#28369;&#20840;&#23616;&#35821;&#20041;&#31354;&#38388;&#26469;&#26377;&#25928;&#24674;&#22797;&#34987;&#25171;&#26029;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#38544;&#31169;&#35774;&#32622;&#21644;&#25345;&#32493;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;SOTA&#30340;&#25928;&#29992;-&#38544;&#31169;&#26435;&#34913;&#36793;&#30028;&#25552;&#39640;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
To defend against privacy leakage of user data, differential privacy is widely used in federated learning, but it is not free. The addition of noise randomly disrupts the semantic integrity of the model and this disturbance accumulates with increased communication rounds. In this paper, we introduce a novel federated learning framework with rigorous privacy guarantees, named FedCEO, designed to strike a trade-off between model utility and user privacy by letting clients ''Collaborate with Each Other''. Specifically, we perform efficient tensor low-rank proximal optimization on stacked local model parameters at the server, demonstrating its capability to flexibly truncate high-frequency components in spectral space. This implies that our FedCEO can effectively recover the disrupted semantic information by smoothing the global semantic space for different privacy settings and continuous training processes. Moreover, we improve the SOTA utility-privacy trade-off bound by an order of $\sqr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545;&#35821;&#38899;&#36716;&#25442;&#25104;&#27468;&#26354;&#30340;&#24187;&#35937;&#30340;&#21512;&#29702;&#20998;&#26512;&#65292;&#23558;&#20854;&#35270;&#20026;&#19968;&#31181;&#32479;&#35745;&#25512;&#26029;&#36807;&#31243;&#65292;&#36890;&#36807;&#20998;&#26512;&#35821;&#26009;&#24211;&#65292;&#36824;&#21457;&#29616;&#20102;&#19968;&#31181;&#32431;&#25991;&#26412;&#30340;&#23567;&#35828;&#36716;&#27468;&#35789;&#30340;&#24187;&#35937;&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#35777;&#25454;&#26469;&#25903;&#25345;&#36825;&#19968;&#35266;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.06992</link><description>&lt;p&gt;
&#35821;&#38899;&#36716;&#25442;&#25104;&#27468;&#26354;&#30340;&#24187;&#35937;&#30340;&#21512;&#29702;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Rational Analysis of the Speech-to-Song Illusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545;&#35821;&#38899;&#36716;&#25442;&#25104;&#27468;&#26354;&#30340;&#24187;&#35937;&#30340;&#21512;&#29702;&#20998;&#26512;&#65292;&#23558;&#20854;&#35270;&#20026;&#19968;&#31181;&#32479;&#35745;&#25512;&#26029;&#36807;&#31243;&#65292;&#36890;&#36807;&#20998;&#26512;&#35821;&#26009;&#24211;&#65292;&#36824;&#21457;&#29616;&#20102;&#19968;&#31181;&#32431;&#25991;&#26412;&#30340;&#23567;&#35828;&#36716;&#27468;&#35789;&#30340;&#24187;&#35937;&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#35777;&#25454;&#26469;&#25903;&#25345;&#36825;&#19968;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#36716;&#25442;&#25104;&#27468;&#26354;&#30340;&#24187;&#35937;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#24515;&#29702;&#29616;&#35937;&#65292;&#21363;&#35828;&#20986;&#30340;&#21477;&#23376;&#22312;&#19981;&#26029;&#37325;&#22797;&#20013;&#36234;&#26469;&#36234;&#20687;&#38899;&#20048;&#12290;&#23613;&#31649;&#32463;&#36807;&#20960;&#21313;&#24180;&#30340;&#30740;&#31350;&#65292;&#23545;&#36825;&#31181;&#36716;&#21270;&#30340;&#23436;&#25972;&#24418;&#24335;&#35299;&#37322;&#20173;&#28982;&#32570;&#20047;&#65292;&#24182;&#19988;&#23545;&#20854;&#32454;&#24494;&#30340;&#29305;&#24449;&#65292;&#21363;&#26576;&#20123;&#30701;&#35821;&#30340;&#36716;&#21270;&#26159;&#21542;&#21457;&#29983;&#65292;&#20063;&#19981;&#22826;&#28165;&#26970;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#36825;&#20010;&#29616;&#35937;&#30340;&#19968;&#20010;&#24418;&#24335;&#21270;&#35299;&#37322;&#65292;&#23558;&#20854;&#37325;&#26032;&#23450;&#20041;&#20026;&#19968;&#31181;&#32479;&#35745;&#25512;&#26029;&#65292;&#21512;&#29702;&#30340;&#20915;&#31574;&#32773;&#35797;&#22270;&#21028;&#26029;&#19968;&#31995;&#21015;&#35805;&#35821;&#26356;&#26377;&#21487;&#33021;&#26159;&#22312;&#27468;&#26354;&#20013;&#36824;&#26159;&#22312;&#35762;&#35805;&#20013;&#20135;&#29983;&#30340;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#24182;&#20998;&#26512;&#27468;&#26354;&#21644;&#35762;&#35805;&#30340;&#35821;&#26009;&#24211;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#20171;&#32461;&#20102;&#19968;&#31181;&#32431;&#25991;&#26412;&#30340;&#23567;&#35828;&#36716;&#27468;&#35789;&#30340;&#24187;&#35937;&#12290;&#22312;&#36825;&#20010;&#24187;&#35937;&#20013;&#65292;&#31616;&#21333;&#22320;&#22797;&#21046;&#20070;&#38754;&#21477;&#23376;&#20250;&#20351;&#23427;&#20204;&#30475;&#36215;&#26469;&#26356;&#20687;&#27468;&#35789;&#12290;&#25105;&#20204;&#22312;&#20154;&#31867;&#21442;&#19982;&#32773;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#20379;&#20102;&#36825;&#20010;&#26032;&#24187;&#35937;&#30340;&#24378;&#26377;&#21147;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The speech-to-song illusion is a robust psychological phenomenon whereby a spoken sentence sounds increasingly more musical as it is repeated. Despite decades of research, a complete formal account of this transformation is still lacking, and some of its nuanced characteristics, namely, that certain phrases appear to transform while others do not, is not well understood. Here we provide a formal account of this phenomenon, by recasting it as a statistical inference whereby a rational agent attempts to decide whether a sequence of utterances is more likely to have been produced in a song or speech. Using this approach and analyzing song and speech corpora, we further introduce a novel prose-to-lyrics illusion that is purely text-based. In this illusion, simply duplicating written sentences makes them appear more like song lyrics. We provide robust evidence for this new illusion in both human participants and large language models.
&lt;/p&gt;</description></item><item><title>OSSAR&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#24320;&#25918;&#38598;&#25163;&#26415;&#27963;&#21160;&#35782;&#21035;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#36229;&#29699;&#38754;&#20114;&#34917;&#28857;&#31574;&#30053;&#26469;&#21306;&#20998;&#24050;&#30693;&#31867;&#21035;&#21644;&#26410;&#30693;&#31867;&#21035;&#65292;&#24182;&#36890;&#36807;&#25913;&#21892;&#27169;&#22411;&#26657;&#20934;&#35299;&#20915;&#23553;&#38381;&#38598;&#20013;&#36807;&#20110;&#33258;&#20449;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.06985</link><description>&lt;p&gt;
OSSAR: &#38754;&#21521;&#26426;&#22120;&#36741;&#21161;&#25163;&#26415;&#20013;&#30340;&#24320;&#25918;&#38598;&#25163;&#26415;&#27963;&#21160;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
OSSAR: Towards Open-Set Surgical Activity Recognition in Robot-assisted Surgery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06985
&lt;/p&gt;
&lt;p&gt;
OSSAR&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#24320;&#25918;&#38598;&#25163;&#26415;&#27963;&#21160;&#35782;&#21035;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#36229;&#29699;&#38754;&#20114;&#34917;&#28857;&#31574;&#30053;&#26469;&#21306;&#20998;&#24050;&#30693;&#31867;&#21035;&#21644;&#26410;&#30693;&#31867;&#21035;&#65292;&#24182;&#36890;&#36807;&#25913;&#21892;&#27169;&#22411;&#26657;&#20934;&#35299;&#20915;&#23553;&#38381;&#38598;&#20013;&#36807;&#20110;&#33258;&#20449;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#21270;&#26426;&#22120;&#20154;&#25163;&#26415;&#21644;&#35745;&#31639;&#26426;&#36741;&#21161;&#24178;&#39044;&#39046;&#22495;&#65292;&#29702;&#35299;&#26426;&#22120;&#20154;&#25163;&#26415;&#27963;&#21160;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#25163;&#26415;&#27963;&#21160;&#35782;&#21035;&#31639;&#27861;&#20027;&#35201;&#38024;&#23545;&#39044;&#23450;&#20041;&#30340;&#23553;&#38381;&#38598;&#33539;&#24335;&#65292;&#24573;&#35270;&#20102;&#29616;&#23454;&#19990;&#30028;&#24320;&#25918;&#38598;&#22330;&#26223;&#30340;&#25361;&#25112;&#12290;&#36825;&#20123;&#31639;&#27861;&#24448;&#24448;&#22312;&#35757;&#32451;&#38454;&#27573;&#26410;&#35265;&#36807;&#30340;&#31867;&#21035;&#30340;&#27979;&#35797;&#26679;&#26412;&#23384;&#22312;&#26102;&#38590;&#20197;&#24212;&#23545;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#24320;&#25918;&#38598;&#25163;&#26415;&#27963;&#21160;&#35782;&#21035;(OSSAR)&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#21033;&#29992;&#36229;&#29699;&#38754;&#20114;&#34917;&#28857;&#31574;&#30053;&#22686;&#24378;&#20102;&#29305;&#24449;&#31354;&#38388;&#20013;&#24050;&#30693;&#31867;&#21035;&#21644;&#26410;&#30693;&#31867;&#21035;&#20043;&#38388;&#30340;&#21306;&#20998;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#25913;&#21892;&#27169;&#22411;&#26657;&#20934;&#26469;&#35299;&#20915;&#23553;&#38381;&#38598;&#20013;&#36807;&#20110;&#33258;&#20449;&#30340;&#38382;&#39064;&#65292;&#36991;&#20813;&#23558;&#26410;&#30693;&#31867;&#21035;&#35823;&#20998;&#31867;&#20026;&#24050;&#30693;&#31867;&#21035;&#12290;&#20026;&#20102;&#25903;&#25345;&#25105;&#20204;&#30340;&#35770;&#36848;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#20110;&#20844;&#20849;JIGSAWS&#25968;&#25454;&#38598;&#30340;&#24320;&#25918;&#38598;&#25163;&#26415;&#27963;&#21160;&#22522;&#20934;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25910;&#38598;&#20102;
&lt;/p&gt;
&lt;p&gt;
In the realm of automated robotic surgery and computer-assisted interventions, understanding robotic surgical activities stands paramount. Existing algorithms dedicated to surgical activity recognition predominantly cater to pre-defined closed-set paradigms, ignoring the challenges of real-world open-set scenarios. Such algorithms often falter in the presence of test samples originating from classes unseen during training phases. To tackle this problem, we introduce an innovative Open-Set Surgical Activity Recognition (OSSAR) framework. Our solution leverages the hyperspherical reciprocal point strategy to enhance the distinction between known and unknown classes in the feature space. Additionally, we address the issue of over-confidence in the closed set by refining model calibration, avoiding misclassification of unknown classes as known ones. To support our assertions, we establish an open-set surgical activity benchmark utilizing the public JIGSAWS dataset. Besides, we also collect
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#20351;&#29992;&#22810;&#21442;&#25968;&#26415;&#21069;&#30913;&#20849;&#25391;&#25104;&#20687;&#39044;&#27979;&#33041;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#24739;&#32773;&#22312;&#19981;&#21516;&#27835;&#30103;&#19979;&#30340;&#29983;&#23384;&#26102;&#38388;&#65292;&#36890;&#36807;&#32771;&#34385;&#27835;&#30103;&#20449;&#24687;&#19982;&#30913;&#20849;&#25391;&#25195;&#25551;&#30456;&#32467;&#21512;&#30340;&#27169;&#22411;&#65292;&#23454;&#29616;&#20010;&#20307;&#21270;&#21644;&#31934;&#30830;&#30340;&#27835;&#30103;&#35268;&#21010;&#12290;</title><link>https://arxiv.org/abs/2402.06982</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#21442;&#25968;&#26415;&#21069;&#30913;&#20849;&#25391;&#25104;&#20687;&#36827;&#34892;&#27835;&#30103;&#22411;&#33041;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#29983;&#23384;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Treatment-wise Glioblastoma Survival Inference with Multi-parametric Preoperative MRI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06982
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#20351;&#29992;&#22810;&#21442;&#25968;&#26415;&#21069;&#30913;&#20849;&#25391;&#25104;&#20687;&#39044;&#27979;&#33041;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#24739;&#32773;&#22312;&#19981;&#21516;&#27835;&#30103;&#19979;&#30340;&#29983;&#23384;&#26102;&#38388;&#65292;&#36890;&#36807;&#32771;&#34385;&#27835;&#30103;&#20449;&#24687;&#19982;&#30913;&#20849;&#25391;&#25195;&#25551;&#30456;&#32467;&#21512;&#30340;&#27169;&#22411;&#65292;&#23454;&#29616;&#20010;&#20307;&#21270;&#21644;&#31934;&#30830;&#30340;&#27835;&#30103;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#22522;&#20110;&#26415;&#21069;&#30913;&#20849;&#25391;&#25195;&#25551;&#39044;&#27979;&#25509;&#21463;&#19981;&#21516;&#27835;&#30103;&#30340;&#33041;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#24739;&#32773;&#30340;&#29983;&#23384;&#26102;&#38388;&#12290;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#27835;&#30103;&#26041;&#26696;&#30340;&#29983;&#23384;&#26102;&#38388;&#65292;&#21487;&#20197;&#23454;&#29616;&#20010;&#20307;&#21270;&#21644;&#31934;&#30830;&#30340;&#27835;&#30103;&#35268;&#21010;&#12290;&#24050;&#32463;&#35777;&#23454;&#65292;&#24739;&#32773;&#30340;&#24403;&#21069;&#29366;&#24577;&#65288;&#30001;&#30913;&#20849;&#25391;&#25195;&#25551;&#34920;&#31034;&#65289;&#21644;&#27835;&#30103;&#36873;&#25321;&#26159;&#29983;&#23384;&#26102;&#38388;&#30340;&#21407;&#22240;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#26377;&#20851;&#22522;&#20110;&#30913;&#20849;&#25391;&#30340;&#33041;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#29983;&#23384;&#26102;&#38388;&#30340;&#30740;&#31350;&#20165;&#20851;&#27880;&#30913;&#20849;&#25391;&#25195;&#25551;&#19982;&#29983;&#23384;&#26102;&#38388;&#20043;&#38388;&#30340;&#30452;&#25509;&#26144;&#23556;&#65292;&#20294;&#26410;&#21253;&#25324;&#27835;&#30103;&#19982;&#29983;&#23384;&#26102;&#38388;&#20043;&#38388;&#30340;&#28508;&#22312;&#22240;&#26524;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27835;&#30103;&#26465;&#20214;&#30340;&#33041;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#29983;&#23384;&#26102;&#38388;&#22238;&#24402;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#38500;&#20102;&#32771;&#34385;&#30913;&#20849;&#25391;&#25195;&#25551;&#36824;&#21253;&#25324;&#27835;&#30103;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#20197;&#32479;&#19968;&#30340;&#26041;&#24335;&#26377;&#25928;&#21033;&#29992;&#25152;&#26377;&#27835;&#30103;&#26041;&#26696;&#30340;&#25968;&#25454;&#65292;&#32780;&#19981;&#24517;&#20026;&#27599;&#20010;&#27835;&#30103;&#26041;&#26696;&#21333;&#29420;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we aim to predict the survival time (ST) of glioblastoma (GBM) patients undergoing different treatments based on preoperative magnetic resonance (MR) scans. The personalized and precise treatment planning can be achieved by comparing the ST of different treatments. It is well established that both the current status of the patient (as represented by the MR scans) and the choice of treatment are the cause of ST. While previous related MR-based glioblastoma ST studies have focused only on the direct mapping of MR scans to ST, they have not included the underlying causal relationship between treatments and ST. To address this limitation, we propose a treatment-conditioned regression model for glioblastoma ST that incorporates treatment information in addition to MR scans. Our approach allows us to effectively utilize the data from all of the treatments in a unified manner, rather than having to train separate models for each of the treatments. Furthermore, treatment can be e
&lt;/p&gt;</description></item><item><title>&#20107;&#20214;&#20851;&#38190;&#25688;&#35201;&#65288;EKS&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#20026;&#29305;&#23450;&#20107;&#20214;&#29983;&#25104;&#19978;&#19979;&#25991;&#21270;&#30340;&#25688;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;MUCSUM&#65292;&#24182;&#23637;&#31034;&#20102;EKS&#19982;&#20256;&#32479;&#25688;&#35201;&#21644;&#32467;&#26500;&#21040;&#25991;&#26412;&#30340;&#27604;&#36739;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.06973</link><description>&lt;p&gt;
&#20107;&#20214;&#20851;&#38190;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Event-Keyed Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06973
&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#20851;&#38190;&#25688;&#35201;&#65288;EKS&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#20026;&#29305;&#23450;&#20107;&#20214;&#29983;&#25104;&#19978;&#19979;&#25991;&#21270;&#30340;&#25688;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;MUCSUM&#65292;&#24182;&#23637;&#31034;&#20102;EKS&#19982;&#20256;&#32479;&#25688;&#35201;&#21644;&#32467;&#26500;&#21040;&#25991;&#26412;&#30340;&#27604;&#36739;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#65292;&#31216;&#20026;&#20107;&#20214;&#20851;&#38190;&#25688;&#35201;&#65288;EKS&#65289;&#65292;&#23427;&#23558;&#20256;&#32479;&#30340;&#25688;&#35201;&#21644;&#25991;&#26723;&#32423;&#20107;&#20214;&#25552;&#21462;&#32467;&#21512;&#36215;&#26469;&#65292;&#30446;&#26631;&#26159;&#22312;&#32473;&#23450;&#25991;&#26723;&#21644;&#25552;&#21462;&#30340;&#20107;&#20214;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#19968;&#20010;&#19978;&#19979;&#25991;&#21270;&#30340;&#29305;&#23450;&#20107;&#20214;&#25688;&#35201;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#36825;&#20010;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;MUCSUM&#65292;&#21253;&#25324;&#32463;&#20856;MUC-4&#25968;&#25454;&#38598;&#20013;&#25152;&#26377;&#20107;&#20214;&#30340;&#25688;&#35201;&#65292;&#20197;&#21450;&#19968;&#32452;&#22522;&#32447;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#22312;&#25688;&#35201;&#25991;&#29486;&#20013;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#26631;&#20934;&#20197;&#21450;&#26356;&#22823;&#30340;&#21069;&#27839;&#27169;&#22411;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#23558;EKS&#31616;&#21270;&#20026;&#20256;&#32479;&#30340;&#25688;&#35201;&#25110;&#32467;&#26500;&#21040;&#25991;&#26412;&#30340;&#21435;&#38500;&#37117;&#20250;&#24471;&#21040;&#36739;&#24046;&#30340;&#30446;&#26631;&#20107;&#20214;&#25688;&#35201;&#65292;&#24182;&#19988;MUCSUM&#26159;&#36825;&#19968;&#20219;&#21153;&#30340;&#19968;&#20010;&#31283;&#20581;&#30340;&#22522;&#20934;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#21442;&#32771;&#25688;&#35201;&#21644;&#27169;&#22411;&#25688;&#35201;&#36827;&#34892;&#20102;&#20154;&#24037;&#35780;&#20272;&#65292;&#24182;&#23545;&#32467;&#26524;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce event-keyed summarization (EKS), a novel task that marries traditional summarization and document-level event extraction, with the goal of generating a contextualized summary for a specific event, given a document and an extracted event structure. We introduce a dataset for this task, MUCSUM, consisting of summaries of all events in the classic MUC-4 dataset, along with a set of baselines that comprises both pretrained LM standards in the summarization literature, as well as larger frontier models. We show that ablations that reduce EKS to traditional summarization or structure-to-text yield inferior summaries of target events and that MUCSUM is a robust benchmark for this task. Lastly, we conduct a human evaluation of both reference and model summaries, and provide some detailed analysis of the results.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Midi-Tuning&#30340;&#22810;&#36718;&#20132;&#20114;&#23545;&#35805;&#35843;&#25972;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#21035;&#23545;&#20195;&#29702;&#20154;&#21644;&#29992;&#25143;&#24314;&#27169;&#65292;&#24182;&#21033;&#29992;&#36718;&#27425;&#32423;&#20869;&#23384;&#32531;&#23384;&#26426;&#21046;&#36827;&#34892;&#35843;&#25972;&#65292;&#23454;&#29616;&#20102;&#23545;&#35805;&#20195;&#29702;&#30340;&#19968;&#33268;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06967</link><description>&lt;p&gt;
&#19968;&#27425;&#25351;&#23548;&#65292;&#22810;&#36718;&#31283;&#23450;&#23545;&#35805;&#65306;&#23545;&#35805;&#30340;&#39640;&#25928;&#35843;&#25972;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Instruct Once, Chat Consistently in Multiple Rounds: An Efficient Tuning Framework for Dialogue
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Midi-Tuning&#30340;&#22810;&#36718;&#20132;&#20114;&#23545;&#35805;&#35843;&#25972;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#21035;&#23545;&#20195;&#29702;&#20154;&#21644;&#29992;&#25143;&#24314;&#27169;&#65292;&#24182;&#21033;&#29992;&#36718;&#27425;&#32423;&#20869;&#23384;&#32531;&#23384;&#26426;&#21046;&#36827;&#34892;&#35843;&#25972;&#65292;&#23454;&#29616;&#20102;&#23545;&#35805;&#20195;&#29702;&#30340;&#19968;&#33268;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35843;&#25972;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20197;&#23454;&#29616;&#23545;&#35805;&#29983;&#25104;&#24050;&#25104;&#20026;&#26500;&#24314;&#33021;&#21147;&#24378;&#22823;&#30340;&#23545;&#35805;&#20195;&#29702;&#30340;&#20027;&#27969;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#35843;&#25972;&#26041;&#24335;&#29421;&#38552;&#22320;&#23558;&#23545;&#35805;&#29983;&#25104;&#35270;&#20026;&#31867;&#20284;&#20854;&#20182;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#30340;&#36807;&#31243;&#65292;&#24573;&#35270;&#20102;&#23545;&#35805;&#32773;&#20043;&#38388;&#30340;&#35282;&#33394;&#24046;&#24322;&#21644;&#23545;&#35805;&#24212;&#20855;&#22791;&#30340;&#22810;&#36718;&#20132;&#20114;&#36807;&#31243;&#12290;&#36825;&#31181;&#26041;&#24335;&#23548;&#33268;&#20102;&#25152;&#26500;&#24314;&#20195;&#29702;&#20154;&#30340;&#23545;&#35805;&#19968;&#33268;&#24615;&#19981;&#23613;&#22914;&#20154;&#24847;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#23545;&#35805;&#30340;&#20132;&#20114;&#24615;&#21644;&#27807;&#36890;&#24615;&#65292;&#24182;&#35748;&#20026;&#20998;&#21035;&#23545;&#20195;&#29702;&#20154;&#21644;&#29992;&#25143;&#30340;&#35762;&#35805;&#32773;&#35282;&#33394;&#36827;&#34892;&#24314;&#27169;&#26356;&#20026;&#21487;&#34892;&#65292;&#20351;&#24471;&#20195;&#29702;&#20154;&#33021;&#22815;&#20445;&#25345;&#19968;&#33268;&#30340;&#35282;&#33394;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#22810;&#36718;&#20132;&#20114;&#23545;&#35805;&#35843;&#25972;&#65288;Midi-Tuning&#65289;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20004;&#20010;&#36866;&#37197;&#22120;&#20998;&#21035;&#23545;&#20195;&#29702;&#20154;&#21644;&#29992;&#25143;&#36827;&#34892;&#24314;&#27169;&#65292;&#23427;&#20204;&#25353;&#36718;&#27425;&#20132;&#26367;&#20351;&#29992;&#35805;&#35821;&#65292;&#24182;&#36890;&#36807;&#36718;&#27425;&#32423;&#20869;&#23384;&#32531;&#23384;&#26426;&#21046;&#36827;&#34892;&#35843;&#25972;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#23545;&#35805;&#20195;&#29702;&#30340;&#19968;&#33268;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tuning pretrained language models for dialogue generation has been a prevalent paradigm for building capable dialogue agents. Yet, traditional tuning narrowly views dialogue generation as resembling other language generation tasks, ignoring the role disparities between two speakers and the multi-round interactive process that dialogues ought to be. Such a manner leads to unsatisfactory chat consistency of the built agent. In this work, we emphasize the interactive, communicative nature of dialogue and argue that it is more feasible to model the speaker roles of agent and user separately, enabling the agent to adhere to its role consistently. We propose an efficient Multi-round Interactive Dialogue Tuning (Midi-Tuning) framework. It models the agent and user individually with two adapters built upon large language models, where they utilize utterances round by round in alternating order and are tuned via a round-level memory caching mechanism. Extensive experiments demonstrate that, our
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#38598;&#25104;&#30340;&#24773;&#22659;&#22810;&#33218;&#32769;&#34382;&#26426;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#20004;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#32769;&#34382;&#26426;&#26041;&#27861;&#65292;&#22312;&#26631;&#20934;&#21644;&#32452;&#21512;&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#20248;&#20110;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#22312;&#20943;&#23569;&#21518;&#24724;&#21644;&#35745;&#31639;&#26102;&#38388;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.06963</link><description>&lt;p&gt;
&#22522;&#20110;&#26641;&#38598;&#25104;&#30340;&#24773;&#22659;&#22810;&#33218;&#32769;&#34382;&#26426;
&lt;/p&gt;
&lt;p&gt;
Tree Ensembles for Contextual Bandits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#38598;&#25104;&#30340;&#24773;&#22659;&#22810;&#33218;&#32769;&#34382;&#26426;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#20004;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#32769;&#34382;&#26426;&#26041;&#27861;&#65292;&#22312;&#26631;&#20934;&#21644;&#32452;&#21512;&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#20248;&#20110;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#22312;&#20943;&#23569;&#21518;&#24724;&#21644;&#35745;&#31639;&#26102;&#38388;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26641;&#38598;&#25104;&#30340;&#24773;&#22659;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#20004;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#32769;&#34382;&#26426;&#26041;&#27861;&#65292;&#19978;&#20449;&#24515;&#30028;&#21644;&#27748;&#26222;&#26862;&#25277;&#26679;&#65292;&#25972;&#21512;&#21040;&#26631;&#20934;&#21644;&#32452;&#21512;&#35774;&#32622;&#20013;&#12290;&#36890;&#36807;&#20351;&#29992;&#27969;&#34892;&#30340;&#26641;&#38598;&#25104;&#26041;&#27861;XGBoost&#36827;&#34892;&#22810;&#27425;&#23454;&#39564;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;&#24403;&#24212;&#29992;&#20110;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#36947;&#36335;&#32593;&#32476;&#23548;&#33322;&#30340;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#26102;&#65292;&#19982;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20943;&#23569;&#21518;&#24724;&#21644;&#35745;&#31639;&#26102;&#38388;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel framework for contextual multi-armed bandits based on tree ensembles. Our framework integrates two widely used bandit methods, Upper Confidence Bound and Thompson Sampling, for both standard and combinatorial settings. We demonstrate the effectiveness of our framework via several experimental studies, employing XGBoost, a popular tree ensemble method. Compared to state-of-the-art methods based on neural networks, our methods exhibit superior performance in terms of both regret minimization and computational runtime, when applied to benchmark datasets and the real-world application of navigation over road networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20174;&#31532;&#19968;&#21407;&#29702;&#26500;&#24314;&#26550;&#26500;&#31070;&#32463;&#21518;&#38376;&#30340;&#26041;&#27861;&#65292;&#24182;&#25551;&#36848;&#20102;12&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#26550;&#26500;&#21518;&#38376;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#20219;&#24847;&#35302;&#21457;&#26816;&#27979;&#22120;&#65292;&#23637;&#31034;&#20102;&#26080;&#38656;&#20154;&#24037;&#30417;&#30563;&#21363;&#21487;&#20026;&#26550;&#26500;&#24341;&#20837;&#21518;&#38376;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.06957</link><description>&lt;p&gt;
&#20174;&#31532;&#19968;&#21407;&#29702;&#26500;&#24314;&#30340;&#26550;&#26500;&#31070;&#32463;&#21518;&#38376;
&lt;/p&gt;
&lt;p&gt;
Architectural Neural Backdoors from First Principles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20174;&#31532;&#19968;&#21407;&#29702;&#26500;&#24314;&#26550;&#26500;&#31070;&#32463;&#21518;&#38376;&#30340;&#26041;&#27861;&#65292;&#24182;&#25551;&#36848;&#20102;12&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#26550;&#26500;&#21518;&#38376;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#20219;&#24847;&#35302;&#21457;&#26816;&#27979;&#22120;&#65292;&#23637;&#31034;&#20102;&#26080;&#38656;&#20154;&#24037;&#30417;&#30563;&#21363;&#21487;&#20026;&#26550;&#26500;&#24341;&#20837;&#21518;&#38376;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20043;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#25913;&#21464;&#31070;&#32463;&#32593;&#32476;&#30340;&#21442;&#25968;&#26469;&#21019;&#24314;&#21518;&#38376;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#19968;&#31181;&#26356;&#38544;&#34109;&#30340;&#23041;&#32961;&#65306;&#22312;&#32593;&#32476;&#26550;&#26500;&#23450;&#20041;&#20013;&#23884;&#20837;&#30340;&#21518;&#38376;&#12290;&#36825;&#28041;&#21450;&#21040;&#27880;&#20837;&#24120;&#35265;&#30340;&#26550;&#26500;&#32452;&#20214;&#65292;&#22914;&#28608;&#27963;&#20989;&#25968;&#21644;&#27744;&#21270;&#23618;&#65292;&#20197;&#24039;&#22937;&#22320;&#24341;&#20837;&#19968;&#20010;&#25345;&#32493;&#23384;&#22312;&#30340;&#21518;&#38376;&#34892;&#20026;&#65292;&#21363;&#20351;&#22312;&#37325;&#26032;&#35757;&#32451;&#21518;&#20063;&#26159;&#22914;&#27492;&#12290;&#28982;&#32780;&#65292;&#26550;&#26500;&#21518;&#38376;&#30340;&#20840;&#37096;&#33539;&#22260;&#21644;&#24433;&#21709;&#20173;&#28982;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;Bober-Irizar&#31561;&#20154;[2023]&#39318;&#27425;&#24341;&#20837;&#20102;&#26550;&#26500;&#21518;&#38376;&#65307;&#20182;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20026;&#26827;&#30424;&#22270;&#26696;&#21019;&#24314;&#21518;&#38376;&#65292;&#20294;&#20174;&#26410;&#35299;&#37322;&#22914;&#20309;&#38024;&#23545;&#20219;&#24847;&#35302;&#21457;&#27169;&#24335;&#36827;&#34892;&#23450;&#20301;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21487;&#29992;&#20110;&#26080;&#20154;&#30417;&#30563;&#22320;&#20026;&#26550;&#26500;&#24341;&#20837;&#21518;&#38376;&#30340;&#20219;&#24847;&#35302;&#21457;&#26816;&#27979;&#22120;&#12290;&#36825;&#20351;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#26550;&#26500;&#21518;&#38376;&#30340;&#27010;&#24565;&#24182;&#23558;&#20854;&#36827;&#34892;&#20998;&#31867;&#65292;&#25551;&#36848;&#20102;12&#31181;&#19981;&#21516;&#31867;&#22411;&#12290;&#20026;&#20102;&#35780;&#20272;&#26816;&#27979;&#27492;&#31867;&#21518;&#38376;&#30340;&#22256;&#38590;&#31243;&#24230;&#65292;...
&lt;/p&gt;
&lt;p&gt;
While previous research backdoored neural networks by changing their parameters, recent work uncovered a more insidious threat: backdoors embedded within the definition of the network's architecture. This involves injecting common architectural components, such as activation functions and pooling layers, to subtly introduce a backdoor behavior that persists even after (full re-)training. However, the full scope and implications of architectural backdoors have remained largely unexplored. Bober-Irizar et al. [2023] introduced the first architectural backdoor; they showed how to create a backdoor for a checkerboard pattern, but never explained how to target an arbitrary trigger pattern of choice. In this work we construct an arbitrary trigger detector which can be used to backdoor an architecture with no human supervision. This leads us to revisit the concept of architecture backdoors and taxonomise them, describing 12 distinct types. To gauge the difficulty of detecting such backdoors, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#20351;&#29992;&#29305;&#24449;&#26144;&#23556;&#23618;&#30340;&#29289;&#29702;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#36890;&#36807;&#26497;&#38480;&#20849;&#36717;&#26680;&#21644;&#31070;&#32463;&#20999;&#21521;&#26680;&#25581;&#31034;&#20102;&#27169;&#22411;&#30340;&#25910;&#25947;&#21644;&#27867;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#22522;&#20110;&#20613;&#37324;&#21494;&#21464;&#25442;&#30340;&#29305;&#24449;&#26144;&#23556;&#30340;&#26465;&#20214;&#27491;&#23450;&#24452;&#21521;&#22522;&#20989;&#25968;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#38382;&#39064;&#38598;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21487;&#20197;&#36731;&#26494;&#23454;&#29616;&#22312;&#22352;&#26631;&#36755;&#20837;&#32593;&#32476;&#20013;&#12290;&#36825;&#20026;&#24191;&#27867;&#30340;PINNs&#30740;&#31350;&#24102;&#26469;&#20102;&#30410;&#22788;&#12290;</title><link>https://arxiv.org/abs/2402.06955</link><description>&lt;p&gt;
&#20351;&#29992;&#29305;&#24449;&#26144;&#23556;&#30340;&#29289;&#29702;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#35757;&#32451;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Training dynamics in Physics-Informed Neural Networks with feature mapping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#20351;&#29992;&#29305;&#24449;&#26144;&#23556;&#23618;&#30340;&#29289;&#29702;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#36890;&#36807;&#26497;&#38480;&#20849;&#36717;&#26680;&#21644;&#31070;&#32463;&#20999;&#21521;&#26680;&#25581;&#31034;&#20102;&#27169;&#22411;&#30340;&#25910;&#25947;&#21644;&#27867;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#22522;&#20110;&#20613;&#37324;&#21494;&#21464;&#25442;&#30340;&#29305;&#24449;&#26144;&#23556;&#30340;&#26465;&#20214;&#27491;&#23450;&#24452;&#21521;&#22522;&#20989;&#25968;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#38382;&#39064;&#38598;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21487;&#20197;&#36731;&#26494;&#23454;&#29616;&#22312;&#22352;&#26631;&#36755;&#20837;&#32593;&#32476;&#20013;&#12290;&#36825;&#20026;&#24191;&#27867;&#30340;PINNs&#30740;&#31350;&#24102;&#26469;&#20102;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#24050;&#25104;&#20026;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#26631;&#24535;&#24615;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#23613;&#31649;&#20854;&#21464;&#20307;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#26469;&#33258;&#26356;&#24191;&#27867;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#30740;&#31350;&#30340;&#29305;&#24449;&#26144;&#23556;&#30340;&#32463;&#39564;&#24615;&#25104;&#21151;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#34987;&#24573;&#35270;&#12290;&#25105;&#20204;&#36890;&#36807;&#26497;&#38480;&#20849;&#36717;&#26680;&#21644;&#31070;&#32463;&#20999;&#21521;&#26680;&#26469;&#30740;&#31350;&#24102;&#26377;&#29305;&#24449;&#26144;&#23556;&#23618;&#30340;PINNs&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#20174;&#32780;&#25581;&#31034;&#20102;&#27169;&#22411;&#30340;&#25910;&#25947;&#21644;&#27867;&#21270;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#24120;&#29992;&#30340;&#22522;&#20110;&#20613;&#37324;&#21494;&#21464;&#25442;&#30340;&#29305;&#24449;&#26144;&#23556;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#30340;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#26465;&#20214;&#27491;&#23450;&#30340;&#24452;&#21521;&#22522;&#20989;&#25968;&#20316;&#20026;&#26356;&#22909;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#32463;&#39564;&#35777;&#23454;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#27491;&#21521;&#21644;&#21453;&#21521;&#38382;&#39064;&#38598;&#20013;&#38750;&#24120;&#26377;&#25928;&#12290;&#36825;&#31181;&#31616;&#21333;&#30340;&#25216;&#26415;&#21487;&#20197;&#36731;&#26494;&#22312;&#22352;&#26631;&#36755;&#20837;&#32593;&#32476;&#20013;&#23454;&#29616;&#65292;&#24182;&#21463;&#30410;&#20110;&#24191;&#27867;&#30340;PINNs&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Neural Networks (PINNs) have emerged as an iconic machine learning approach for solving Partial Differential Equations (PDEs). Although its variants have achieved significant progress, the empirical success of utilising feature mapping from the wider Implicit Neural Representations studies has been substantially neglected. We investigate the training dynamics of PINNs with a feature mapping layer via the limiting Conjugate Kernel and Neural Tangent Kernel, which sheds light on the convergence and generalisation of the model. We also show the inadequacy of commonly used Fourier-based feature mapping in some scenarios and propose the conditional positive definite Radial Basis Function as a better alternative. The empirical results reveal the efficacy of our method in diverse forward and inverse problem sets. This simple technique can be easily implemented in coordinate input networks and benefits the broad PINNs research.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#22122;&#22768;&#20013;&#31561;&#35268;&#27169;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#30340;&#20018;&#25200;&#35823;&#24046;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#21457;&#29616;&#24182;&#34892;&#25351;&#20196;&#20043;&#38388;&#30340;&#20018;&#25200;&#21487;&#33021;&#20250;&#30772;&#22351;&#37327;&#23376;&#29366;&#24577;&#24182;&#23548;&#33268;&#31243;&#24207;&#25191;&#34892;&#38169;&#35823;</title><link>https://arxiv.org/abs/2402.06952</link><description>&lt;p&gt;
&#29992;&#22122;&#22768;&#20013;&#31561;&#35268;&#27169;&#37327;&#23376;&#35774;&#22791;&#20272;&#35745;&#20018;&#25200;&#35823;&#24046;&#23545;&#30005;&#36335;&#20445;&#30495;&#24230;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Estimating the Effect of Crosstalk Error on Circuit Fidelity Using Noisy Intermediate-Scale Quantum Devices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#22122;&#22768;&#20013;&#31561;&#35268;&#27169;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#30340;&#20018;&#25200;&#35823;&#24046;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#21457;&#29616;&#24182;&#34892;&#25351;&#20196;&#20043;&#38388;&#30340;&#20018;&#25200;&#21487;&#33021;&#20250;&#30772;&#22351;&#37327;&#23376;&#29366;&#24577;&#24182;&#23548;&#33268;&#31243;&#24207;&#25191;&#34892;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#25216;&#26415;&#30340;&#36827;&#27493;&#20351;&#37327;&#23376;&#35745;&#31639;&#31038;&#21306;&#20851;&#27880;&#20110;&#25506;&#32034;&#36817;&#26399;&#35774;&#22791;&#30340;&#28508;&#21147;&#65292;&#36825;&#20123;&#35774;&#22791;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#35745;&#31639;&#33021;&#21147;&#36229;&#36807;&#20102;&#32463;&#20856;&#35745;&#31639;&#26426;&#12290;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#26680;&#24515;&#38382;&#39064;&#26159;&#36825;&#20123;&#35774;&#22791;&#20013;&#22266;&#26377;&#22122;&#22768;&#26159;&#21542;&#21487;&#20197;&#34987;&#20811;&#26381;&#65292;&#25110;&#32773;&#20219;&#20309;&#28508;&#22312;&#30340;&#37327;&#23376;&#20248;&#21183;&#26159;&#21542;&#21463;&#21040;&#38480;&#21046;&#12290;&#27627;&#26080;&#30097;&#38382;&#65292;&#20018;&#25200;&#26159;&#22122;&#22768;&#20013;&#31561;&#35268;&#27169;&#37327;&#23376;&#31995;&#32479;&#20013;&#30340;&#20027;&#35201;&#26469;&#28304;&#20043;&#19968;&#65292;&#23427;&#23545;&#30828;&#20214;&#35774;&#35745;&#26500;&#25104;&#20102;&#26681;&#26412;&#24615;&#25361;&#25112;&#12290;&#24182;&#34892;&#25351;&#20196;&#20043;&#38388;&#30340;&#20018;&#25200;&#21487;&#20197;&#30772;&#22351;&#37327;&#23376;&#29366;&#24577;&#24182;&#23548;&#33268;&#31243;&#24207;&#25191;&#34892;&#38169;&#35823;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#20018;&#25200;&#35823;&#24046;&#23545;&#22122;&#22768;&#20013;&#31561;&#35268;&#27169;&#37327;&#23376;&#35745;&#31639;&#26426;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#38750;&#24120;&#31616;&#21333;&#21644;&#23454;&#29992;&#65292;&#21487;&#29992;&#20110;&#34920;&#24449;&#22810;&#27604;&#29305;&#35774;&#22791;&#30340;&#20018;&#25200;&#35823;&#24046;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#38543;&#26426;&#21270;&#22522;&#20934;&#27979;&#35797;&#21644;&#21516;&#26102;&#38543;&#26426;&#21270;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Current advancements in technology have focused the attention of the quantum computing community toward exploring the potential of near-term devices whose computing power surpasses that of classical computers in practical applications. An unresolved central question revolves around whether the inherent noise in these devices can be overcome or whether any potential quantum advantage would be limited. There is no doubt that crosstalk is one of the main sources of noise in noisy intermediate-scale quantum (NISQ) systems, and it poses a fundamental challenge to hardware designs. Crosstalk between parallel instructions can corrupt quantum states and cause incorrect program execution. In this study, we present a comprehensive analysis of the crosstalk error effect on NISQ computers. Our approach is extremely straightforward and practical for characterizing the crosstalk error of various multi-qubit devices. In particular, we combine the randomized benchmarking (RB) and simultaneous randomiz
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#32452;&#21551;&#21457;&#24335;&#24230;&#37327;&#26631;&#20934;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#29983;&#25104;&#30340;&#21360;&#21047;&#28023;&#25253;&#65292;&#21253;&#25324;&#21487;&#35835;&#24615;&#12289;&#32654;&#35266;&#24230;&#21644;&#35821;&#20041;&#29305;&#24449;&#12290;&#36890;&#36807;&#32422;&#26463;&#36827;&#21270;&#26041;&#27861;&#29983;&#25104;&#28023;&#25253;&#65292;&#24182;&#23558;&#24230;&#37327;&#26631;&#20934;&#20316;&#20026;&#32422;&#26463;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#36824;&#38598;&#25104;&#24773;&#24863;&#35782;&#21035;&#25216;&#26415;&#65292;&#24182;&#20998;&#26512;&#20102;&#26041;&#27861;&#21644;&#35270;&#35273;&#29305;&#24449;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.06945</link><description>&lt;p&gt;
&#35780;&#20215;&#33258;&#21160;&#21360;&#21047;&#28023;&#25253;&#29983;&#25104;&#30340;&#24230;&#37327;&#26631;&#20934;
&lt;/p&gt;
&lt;p&gt;
Evaluation Metrics for Automated Typographic Poster Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#32452;&#21551;&#21457;&#24335;&#24230;&#37327;&#26631;&#20934;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#29983;&#25104;&#30340;&#21360;&#21047;&#28023;&#25253;&#65292;&#21253;&#25324;&#21487;&#35835;&#24615;&#12289;&#32654;&#35266;&#24230;&#21644;&#35821;&#20041;&#29305;&#24449;&#12290;&#36890;&#36807;&#32422;&#26463;&#36827;&#21270;&#26041;&#27861;&#29983;&#25104;&#28023;&#25253;&#65292;&#24182;&#23558;&#24230;&#37327;&#26631;&#20934;&#20316;&#20026;&#32422;&#26463;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#36824;&#38598;&#25104;&#24773;&#24863;&#35782;&#21035;&#25216;&#26415;&#65292;&#24182;&#20998;&#26512;&#20102;&#26041;&#27861;&#21644;&#35270;&#35273;&#29305;&#24449;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#35774;&#35745;&#26041;&#27861;&#20419;&#36827;&#20102;&#21360;&#21047;&#35774;&#35745;&#30340;&#29983;&#25104;&#65292;&#20294;&#35780;&#20272;&#36825;&#20123;&#35774;&#35745;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#32452;&#21551;&#21457;&#24335;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#21360;&#21047;&#35774;&#35745;&#65292;&#37325;&#28857;&#35780;&#20272;&#20854;&#21487;&#35835;&#24615;&#65292;&#35780;&#20272;&#35774;&#35745;&#30340;&#35270;&#35273;&#36136;&#37327;&#21644;&#20272;&#35745;&#35774;&#35745;&#26377;&#25928;&#20256;&#36798;&#20869;&#23481;&#35821;&#20041;&#30340;&#35821;&#20041;&#29305;&#24449;&#12290;&#25105;&#20204;&#20351;&#29992;&#32422;&#26463;&#36827;&#21270;&#26041;&#27861;&#29983;&#25104;&#21360;&#21047;&#28023;&#25253;&#65292;&#24182;&#23558;&#25552;&#20986;&#30340;&#35780;&#20215;&#24230;&#37327;&#26631;&#20934;&#19982;&#19981;&#21516;&#35774;&#32622;&#32467;&#21512;&#20351;&#29992;&#65292;&#23558;&#21487;&#35835;&#24615;&#24230;&#37327;&#26631;&#20934;&#35270;&#20026;&#32422;&#26463;&#26465;&#20214;&#12290;&#25105;&#20204;&#36824;&#38598;&#25104;&#24773;&#24863;&#35782;&#21035;&#25216;&#26415;&#65292;&#20197;&#33258;&#21160;&#35782;&#21035;&#25991;&#26412;&#35821;&#20041;&#65292;&#24182;&#20998;&#26512;&#35813;&#26041;&#27861;&#21644;&#35270;&#35273;&#29305;&#24449;&#30340;&#24615;&#33021;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computational Design approaches facilitate the generation of typographic design, but evaluating these designs remains a challenging task. In this paper, we propose a set of heuristic metrics for typographic design evaluation, focusing on their legibility, which assesses the text visibility, aesthetics, which evaluates the visual quality of the design, and semantic features, which estimate how effectively the design conveys the content semantics. We experiment with a constrained evolutionary approach for generating typographic posters, incorporating the proposed evaluation metrics with varied setups, and treating the legibility metrics as constraints. We also integrate emotion recognition to identify text semantics automatically and analyse the performance of the approach and the visual characteristics outputs.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#35848;&#21028;&#33021;&#21147;&#30340;&#20998;&#24067;&#24335;&#22522;&#30784;&#35774;&#26045;&#39640;&#25928;&#36164;&#28304;&#35843;&#24230;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#22522;&#20110;&#27169;&#31946;&#36923;&#36753;&#30340;&#20195;&#29702;&#33258;&#21160;&#35848;&#21028;&#31995;&#32479;&#65292;&#20248;&#21270;&#20113;&#35745;&#31639;&#25552;&#20379;&#21830;&#21644;&#23458;&#25143;&#20043;&#38388;&#30340;&#21327;&#35758;&#65292;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.06938</link><description>&lt;p&gt;
&#20351;&#29992;&#35848;&#21028;&#33021;&#21147;&#30340;&#20998;&#24067;&#24335;&#22522;&#30784;&#35774;&#26045;&#39640;&#25928;&#36164;&#28304;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Efficient Resource Scheduling for Distributed Infrastructures Using Negotiation Capabilities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06938
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#35848;&#21028;&#33021;&#21147;&#30340;&#20998;&#24067;&#24335;&#22522;&#30784;&#35774;&#26045;&#39640;&#25928;&#36164;&#28304;&#35843;&#24230;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#22522;&#20110;&#27169;&#31946;&#36923;&#36753;&#30340;&#20195;&#29702;&#33258;&#21160;&#35848;&#21028;&#31995;&#32479;&#65292;&#20248;&#21270;&#20113;&#35745;&#31639;&#25552;&#20379;&#21830;&#21644;&#23458;&#25143;&#20043;&#38388;&#30340;&#21327;&#35758;&#65292;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#21313;&#24180;&#37324;&#65292;&#20449;&#24687;&#21644;&#20114;&#32852;&#32593;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#20652;&#29983;&#20102;&#22823;&#37327;&#30340;&#25968;&#25454;&#21644;&#20449;&#24687;&#12290;&#20449;&#24687;&#29190;&#28856;&#25512;&#21160;&#35768;&#22810;&#20225;&#19994;&#25110;&#20010;&#20154;&#23547;&#27714;&#31199;&#29992;&#20113;&#35745;&#31639;&#22522;&#30784;&#35774;&#26045;&#26469;&#23558;&#20182;&#20204;&#30340;&#24212;&#29992;&#31243;&#24207;&#25918;&#32622;&#22312;&#20113;&#20013;&#12290;&#28982;&#32780;&#65292;&#20113;&#35745;&#31639;&#25552;&#20379;&#21830;&#21644;&#23458;&#25143;&#20043;&#38388;&#36798;&#25104;&#30340;&#21327;&#35758;&#36890;&#24120;&#19981;&#39640;&#25928;&#12290;&#35768;&#22810;&#22240;&#32032;&#24433;&#21709;&#25928;&#29575;&#65292;&#22914;&#25552;&#20379;&#21830;&#20113;&#35745;&#31639;&#22522;&#30784;&#35774;&#26045;&#30340;&#38386;&#32622;&#21644;&#23545;&#23458;&#25143;&#30340;&#39069;&#22806;&#25104;&#26412;&#12290;&#19968;&#20010;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#24341;&#20837;&#19968;&#31181;&#32508;&#21512;&#30340;&#12289;&#35848;&#21028;&#31867;&#30340;&#21338;&#24328;&#65292;&#24182;&#26681;&#25454;&#35848;&#21028;&#32467;&#26524;&#23433;&#25490;&#36164;&#28304;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#31946;&#36923;&#36753;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;&#33258;&#21160;&#35848;&#21028;&#31995;&#32479;&#29992;&#20110;&#36164;&#28304;&#35843;&#24230;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#23436;&#25104;&#19968;&#23545;&#19968;&#30340;&#33258;&#21160;&#35848;&#21028;&#36807;&#31243;&#65292;&#24182;&#20026;&#25552;&#20379;&#21830;&#21644;&#23458;&#25143;&#29983;&#25104;&#26368;&#20248;&#30340;&#25253;&#20215;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#25104;&#21592;&#20989;&#25968;&#12289;&#27169;&#31946;&#35268;&#21017;&#38598;&#21644;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#23545;&#36164;&#28304;&#35843;&#24230;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the past few decades, the rapid development of information and internet technologies has spawned massive amounts of data and information. The information explosion drives many enterprises or individuals to seek to rent cloud computing infrastructure to put their applications in the cloud. However, the agreements reached between cloud computing providers and clients are often not efficient. Many factors affect the efficiency, such as the idleness of the providers' cloud computing infrastructure, and the additional cost to the clients. One possible solution is to introduce a comprehensive, bargaining game (a type of negotiation), and schedule resources according to the negotiation results. We propose an agent-based auto-negotiation system for resource scheduling based on fuzzy logic. The proposed method can complete a one-to-one auto-negotiation process and generate optimal offers for the provider and client. We compare the impact of different member functions, fuzzy rule sets, and ne
&lt;/p&gt;</description></item><item><title>ORIENT&#26159;&#19968;&#31181;&#38754;&#21521;6G&#30340;&#20248;&#20808;&#26435;&#24863;&#30693;&#33410;&#33021;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#35299;&#20915;&#26381;&#21153;&#23454;&#20363;&#25918;&#32622;&#21644;&#20998;&#37197;&#12289;&#36335;&#24452;&#36873;&#25321;&#21644;&#35831;&#27714;&#20248;&#20808;&#32423;&#30340;&#32852;&#21512;&#38382;&#39064;&#26469;&#26368;&#22823;&#21270;&#31995;&#32479;&#30340;&#25972;&#20307;&#21033;&#28070;&#65292;&#24182;&#22312;&#38271;&#26102;&#38388;&#20869;&#26368;&#23567;&#21270;&#33021;&#32791;&#12290;</title><link>https://arxiv.org/abs/2402.06931</link><description>&lt;p&gt;
ORIENT:&#19968;&#31181;&#38754;&#21521;6G&#20013;&#24310;&#36831;&#25935;&#24863;&#24212;&#29992;&#31243;&#24207;&#30340;&#20248;&#20808;&#26435;&#24863;&#30693;&#33410;&#33021;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ORIENT: A Priority-Aware Energy-Efficient Approach for Latency-Sensitive Applications in 6G
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06931
&lt;/p&gt;
&lt;p&gt;
ORIENT&#26159;&#19968;&#31181;&#38754;&#21521;6G&#30340;&#20248;&#20808;&#26435;&#24863;&#30693;&#33410;&#33021;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#35299;&#20915;&#26381;&#21153;&#23454;&#20363;&#25918;&#32622;&#21644;&#20998;&#37197;&#12289;&#36335;&#24452;&#36873;&#25321;&#21644;&#35831;&#27714;&#20248;&#20808;&#32423;&#30340;&#32852;&#21512;&#38382;&#39064;&#26469;&#26368;&#22823;&#21270;&#31995;&#32479;&#30340;&#25972;&#20307;&#21033;&#28070;&#65292;&#24182;&#22312;&#38271;&#26102;&#38388;&#20869;&#26368;&#23567;&#21270;&#33021;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;6G&#21040;&#26469;&#30340;&#26399;&#26395;&#22686;&#21152;&#65292;&#20154;&#20204;&#23545;&#35745;&#31639;&#21644;&#32593;&#32476;&#30340;&#33021;&#32791;&#22686;&#38271;&#34920;&#31034;&#25285;&#24551;&#12290;&#39044;&#35745;&#36830;&#25509;&#35774;&#22791;&#30340;&#28608;&#22686;&#21644;&#36164;&#28304;&#35201;&#27714;&#39640;&#30340;&#24212;&#29992;&#31243;&#24207;&#23558;&#20026;&#33021;&#28304;&#36164;&#28304;&#24102;&#26469;&#21069;&#25152;&#26410;&#26377;&#30340;&#25361;&#25112;&#12290;&#34429;&#28982;&#36807;&#21435;&#24050;&#32463;&#35752;&#35770;&#20102;&#21487;&#25345;&#32493;&#30340;&#36164;&#28304;&#20998;&#37197;&#31574;&#30053;&#65292;&#20294;&#36825;&#20123;&#21162;&#21147;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#22495;&#32534;&#25490;&#19978;&#65292;&#25110;&#32773;&#24573;&#30053;&#20102;6G&#25552;&#20986;&#30340;&#29420;&#29305;&#35201;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26381;&#21153;&#23454;&#20363;&#30340;&#25918;&#32622;&#21644;&#20998;&#37197;&#12289;&#36335;&#24452;&#36873;&#25321;&#21644;&#35831;&#27714;&#20248;&#20808;&#32423;&#30340;&#32852;&#21512;&#38382;&#39064;&#65292;&#31216;&#20026;PIRA&#12290;&#30446;&#26631;&#20989;&#25968;&#26159;&#36890;&#36807;&#26368;&#22823;&#21270;&#21516;&#26102;&#25903;&#25345;&#30340;&#35831;&#27714;&#25968;&#37327;&#26469;&#26368;&#22823;&#21270;&#31995;&#32479;&#30340;&#25972;&#20307;&#21033;&#28070;&#65292;&#21516;&#26102;&#22312;&#38271;&#26102;&#38388;&#20869;&#26368;&#23567;&#21270;&#33021;&#32791;&#12290;&#27492;&#22806;&#65292;&#36824;&#32771;&#34385;&#20102;&#35745;&#31639;&#21644;&#32593;&#32476;&#36164;&#28304;&#30340;&#31471;&#21040;&#31471;&#24310;&#36831;&#35201;&#27714;&#21644;&#36164;&#28304;&#23481;&#37327;&#32422;&#26463;&#65292;&#20854;&#20013;&#20351;&#29992;&#20102;&#25490;&#38431;&#35770;&#26469;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Anticipation for 6G's arrival comes with growing concerns about increased energy consumption in computing and networking. The expected surge in connected devices and resource-demanding applications presents unprecedented challenges for energy resources. While sustainable resource allocation strategies have been discussed in the past, these efforts have primarily focused on single-domain orchestration or ignored the unique requirements posed by 6G. To address this gap, we investigate the joint problem of service instance placement and assignment, path selection, and request prioritization, dubbed PIRA. The objective function is to maximize the system's overall profit as a function of the number of concurrently supported requests while simultaneously minimizing energy consumption over an extended period of time. In addition, end-to-end latency requirements and resource capacity constraints are considered for computing and networking resources, where queuing theory is utilized to estimate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;Langchain&#21046;&#20316;&#39318;&#23572;&#21382;&#21490;&#36951;&#22336;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21407;&#22411;&#65292;&#26088;&#22312;&#25552;&#39640;&#28216;&#23458;&#23545;&#35813;&#22320;&#21306;&#23453;&#36149;&#25991;&#21270;&#36951;&#20135;&#30340;&#35748;&#35782;&#65292;&#24182;&#25552;&#20379;&#20934;&#30830;&#21644;&#21487;&#38752;&#30340;&#20449;&#24687;&#12290;&#35813;&#20195;&#29702;&#35774;&#35745;&#29992;&#20110;&#33521;&#35821;&#35775;&#38382;&#65292;&#24182;&#21033;&#29992;&#39318;&#23572;&#24066;&#25919;&#24220;&#25552;&#20379;&#30340;&#25968;&#25454;&#12290;&#26041;&#27861;&#21644;&#32467;&#26500;&#27010;&#36848;&#20063;&#25552;&#20379;&#22312;&#35770;&#25991;&#20013;&#65292;&#21516;&#26102;&#20063;&#35752;&#35770;&#20102;&#25913;&#36827;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.06929</link><description>&lt;p&gt;
&#20351;&#29992;Langchain&#21046;&#20316;&#39318;&#23572;&#21382;&#21490;&#36951;&#22336;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21407;&#22411;
&lt;/p&gt;
&lt;p&gt;
Making a prototype of Seoul historical sites chatbot using Langchain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;Langchain&#21046;&#20316;&#39318;&#23572;&#21382;&#21490;&#36951;&#22336;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21407;&#22411;&#65292;&#26088;&#22312;&#25552;&#39640;&#28216;&#23458;&#23545;&#35813;&#22320;&#21306;&#23453;&#36149;&#25991;&#21270;&#36951;&#20135;&#30340;&#35748;&#35782;&#65292;&#24182;&#25552;&#20379;&#20934;&#30830;&#21644;&#21487;&#38752;&#30340;&#20449;&#24687;&#12290;&#35813;&#20195;&#29702;&#35774;&#35745;&#29992;&#20110;&#33521;&#35821;&#35775;&#38382;&#65292;&#24182;&#21033;&#29992;&#39318;&#23572;&#24066;&#25919;&#24220;&#25552;&#20379;&#30340;&#25968;&#25454;&#12290;&#26041;&#27861;&#21644;&#32467;&#26500;&#27010;&#36848;&#20063;&#25552;&#20379;&#22312;&#35770;&#25991;&#20013;&#65292;&#21516;&#26102;&#20063;&#35752;&#35770;&#20102;&#25913;&#36827;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#20998;&#20139;&#19968;&#20010;&#20851;&#20110;&#24320;&#21457;&#19968;&#20010;&#23545;&#20301;&#20110;&#39318;&#23572;&#30340;&#21382;&#21490;&#36951;&#22336;&#36827;&#34892;&#20449;&#24687;&#20256;&#25773;&#30340;&#23545;&#35805;&#20195;&#29702;&#30340;&#33609;&#26696;&#12290;&#20195;&#29702;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#22686;&#21152;&#23545;&#39318;&#23572;&#19981;&#29087;&#24713;&#30340;&#28216;&#23458;&#23545;&#23453;&#36149;&#25991;&#21270;&#36951;&#20135;&#30340;&#23384;&#22312;&#21644;&#31934;&#30830;&#20301;&#32622;&#30340;&#35748;&#35782;&#12290;&#23427;&#26088;&#22312;&#20419;&#36827;&#23545;&#38889;&#22269;&#20016;&#23500;&#22810;&#26679;&#30340;&#25991;&#21270;&#21382;&#21490;&#30340;&#22522;&#26412;&#20102;&#35299;&#12290;&#35813;&#20195;&#29702;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#65292;&#21487;&#20197;&#29992;&#33521;&#35821;&#36827;&#34892;&#35775;&#38382;&#65292;&#24182;&#21033;&#29992;&#39318;&#23572;&#24066;&#25919;&#24220;&#24951;&#24936;&#25552;&#20379;&#30340;&#25968;&#25454;&#12290;&#23613;&#31649;&#25968;&#25454;&#37327;&#26377;&#38480;&#65292;&#20294;&#23427;&#22987;&#32456;&#21487;&#38752;&#22320;&#25552;&#20379;&#20934;&#30830;&#30340;&#22238;&#31572;&#65292;&#24182;&#19982;&#21487;&#29992;&#20449;&#24687;&#26080;&#32541;&#23545;&#40784;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#21019;&#24314;&#35813;&#20195;&#29702;&#25152;&#20351;&#29992;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#35770;&#25991;&#20013;&#25552;&#20379;&#20102;&#20854;&#22522;&#26412;&#32467;&#26500;&#30340;&#32508;&#21512;&#27010;&#36848;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#25913;&#36827;&#36825;&#20010;&#31995;&#32479;&#21021;&#22987;&#29256;&#26412;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we are going to share a draft of the development of a conversational agent created to disseminate information about historical sites located in the Seoul. The primary objective of the agent is to increase awareness among visitors who are not familiar with Seoul, about the presence and precise locations of valuable cultural heritage sites. It aims to promote a basic understanding of Korea's rich and diverse cultural history. The agent is thoughtfully designed for accessibility in English and utilizes data generously provided by the Seoul Metropolitan Government. Despite the limited data volume, it consistently delivers reliable and accurate responses, seamlessly aligning with the available information. We have meticulously detailed the methodologies employed in creating this agent and provided a comprehensive overview of its underlying structure within the paper. Additionally, we delve into potential improvements to enhance this initial version of the system, with a prima
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30452;&#25509;&#20004;&#20004;&#27604;&#36739;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;LLMs&#30340;&#22122;&#22768;&#21453;&#39304;&#65292;&#30452;&#25509;&#35782;&#21035;&#20986;&#26368;&#26377;&#28508;&#21147;&#30340;&#20013;&#38388;&#24605;&#32500;&#65292;&#20174;&#32780;&#29983;&#25104;&#20248;&#31168;&#30340;&#24605;&#32500;&#38142;&#12290;</title><link>https://arxiv.org/abs/2402.06918</link><description>&lt;p&gt;
&#29992;&#30452;&#25509;&#30340;&#20004;&#20004;&#27604;&#36739;&#26041;&#27861;&#29983;&#25104;&#24605;&#32500;&#38142;&#65292;&#20197;&#25628;&#32034;&#26368;&#26377;&#28508;&#21147;&#30340;&#20013;&#38388;&#24605;&#32500;
&lt;/p&gt;
&lt;p&gt;
Generating Chain-of-Thoughts with a Direct Pairwise-Comparison Approach to Searching for the Most Promising Intermediate Thought
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30452;&#25509;&#20004;&#20004;&#27604;&#36739;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;LLMs&#30340;&#22122;&#22768;&#21453;&#39304;&#65292;&#30452;&#25509;&#35782;&#21035;&#20986;&#26368;&#26377;&#28508;&#21147;&#30340;&#20013;&#38388;&#24605;&#32500;&#65292;&#20174;&#32780;&#29983;&#25104;&#20248;&#31168;&#30340;&#24605;&#32500;&#38142;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22788;&#29702;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#24605;&#32500;&#38142;(Chain-of-Thoughts, CoT)&#26041;&#27861;&#65292;&#29992;&#20110;&#25351;&#23548;LLMs&#36827;&#34892;&#36880;&#27493;&#25512;&#29702;&#65292;&#20174;&#31616;&#21333;&#21040;&#22797;&#26434;&#30340;&#38382;&#39064;&#35299;&#20915;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#36825;&#31181;&#24605;&#32500;&#38142;&#30340;&#26041;&#27861;&#28041;&#21450;&#20114;&#21160;&#21327;&#20316;&#65292;&#23398;&#20064;&#32773;&#29983;&#25104;&#20505;&#36873;&#20013;&#38388;&#24605;&#32500;&#65292;&#30001;LLMs&#35780;&#20272;&#65292;&#24341;&#23548;&#29983;&#25104;&#21518;&#32493;&#24605;&#32500;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#24191;&#27867;&#20294;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#30340;&#38382;&#39064;&#26159;&#65292;LLMs&#30340;&#35780;&#20272;&#36890;&#24120;&#23384;&#22312;&#22122;&#22768;&#21644;&#19981;&#21487;&#38752;&#24615;&#65292;&#21487;&#33021;&#35823;&#23548;&#29983;&#25104;&#36807;&#31243;&#65292;&#36873;&#25321;&#19981;&#22815;&#26377;&#28508;&#21147;&#30340;&#20013;&#38388;&#24605;&#32500;&#12290;&#26412;&#25991;&#21463;Vapnik&#21407;&#21017;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27604;&#36739;&#30340;CoT&#29983;&#25104;&#31639;&#27861;&#65292;&#30452;&#25509;&#26681;&#25454;LLMs&#30340;&#22122;&#22768;&#21453;&#39304;&#30830;&#23450;&#26368;&#26377;&#28508;&#21147;&#30340;&#24605;&#32500;&#12290;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;&#25105;&#20204;&#38543;&#26426;&#37197;&#23545;&#20013;&#38388;&#24605;&#32500;&#65292;&#24182;&#30452;&#25509;&#20419;&#20351;LLMs&#20174;&#27599;&#23545;&#20013;&#36873;&#25321;&#26356;&#26377;&#28508;&#21147;&#30340;&#24605;&#32500;&#12290;
&lt;/p&gt;
&lt;p&gt;
To improve the ability of the large language model (LLMs) to handle complex reasoning problems, chain-of-thoughts (CoT) methods were proposed to guide LLMs to reason step-by-step, facilitating problem solving from simple to complex tasks. State-of-the-art approaches for generating such a chain involve interactive collaboration, where the learner generates candidate intermediate thoughts, evaluated by the LLM, guiding the generation of subsequent thoughts. However, a widespread yet understudied problem is that the evaluation from the LLM is typically noisy and unreliable, potentially misleading the generation process in selecting promising intermediate thoughts. In this paper, motivated by Vapnik's principle, we propose a novel comparison-based CoT generation algorithm that directly identifies the most promising thoughts with the noisy feedback from the LLM. In each round, we randomly pair intermediate thoughts and directly prompt the LLM to select the more promising one from each pair,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#36827;&#21270;&#31574;&#30053;(ES)&#26469;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#65292;&#20197;&#36890;&#36807;&#30452;&#25509;&#31574;&#30053;&#25628;&#32034;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#22522;&#20934;&#38382;&#39064;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;ES&#21487;&#20197;&#22312;&#35768;&#22810;&#22522;&#20934;&#20219;&#21153;&#20013;&#25214;&#21040;&#26377;&#25928;&#30340;&#32447;&#24615;&#31574;&#30053;&#65292;&#19982;&#24403;&#21069;&#20351;&#29992;&#26356;&#22823;&#32593;&#32476;&#30340;DRL&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#34920;&#26126;&#24403;&#21069;&#30340;&#22522;&#20934;&#38382;&#39064;&#27604;&#20197;&#24448;&#35748;&#20026;&#30340;&#26356;&#23481;&#26131;&#35299;&#20915;&#12290;</title><link>https://arxiv.org/abs/2402.06912</link><description>&lt;p&gt;
&#29992;&#32447;&#24615;&#31574;&#30053;&#32593;&#32476;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving Deep Reinforcement Learning Benchmarks with Linear Policy Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#36827;&#21270;&#31574;&#30053;(ES)&#26469;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#65292;&#20197;&#36890;&#36807;&#30452;&#25509;&#31574;&#30053;&#25628;&#32034;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#22522;&#20934;&#38382;&#39064;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;ES&#21487;&#20197;&#22312;&#35768;&#22810;&#22522;&#20934;&#20219;&#21153;&#20013;&#25214;&#21040;&#26377;&#25928;&#30340;&#32447;&#24615;&#31574;&#30053;&#65292;&#19982;&#24403;&#21069;&#20351;&#29992;&#26356;&#22823;&#32593;&#32476;&#30340;DRL&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#34920;&#26126;&#24403;&#21069;&#30340;&#22522;&#20934;&#38382;&#39064;&#27604;&#20197;&#24448;&#35748;&#20026;&#30340;&#26356;&#23481;&#26131;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#31639;&#27861;&#33021;&#22815;&#23398;&#20064;&#26377;&#25928;&#30340;&#31574;&#30053;&#26469;&#35299;&#20915;&#20687;Atari&#28216;&#25103;&#21644;&#26426;&#22120;&#20154;&#20219;&#21153;&#36825;&#26679;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#65292;&#20294;&#31639;&#27861;&#22797;&#26434;&#65292;&#35757;&#32451;&#26102;&#38388;&#24448;&#24448;&#36739;&#38271;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36827;&#21270;&#31574;&#30053;(ES)&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20043;&#38388;&#30340;&#34920;&#29616;&#24046;&#24322;&#12290;&#25105;&#20204;&#20351;&#29992;ES&#36890;&#36807;&#31070;&#32463;&#36827;&#21270;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#65292;&#36890;&#36807;&#30452;&#25509;&#31574;&#30053;&#25628;&#32034;&#26469;&#23436;&#25104;&#12290;&#25105;&#20204;&#23545;&#24120;&#35268;&#32593;&#32476;&#21644;&#30001;&#19968;&#20010;&#20174;&#35266;&#27979;&#21040;&#21160;&#20316;&#30340;&#21333;&#19968;&#32447;&#24615;&#23618;&#32452;&#25104;&#30340;&#31574;&#30053;&#32593;&#32476;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65307;&#23545;&#20110;&#19977;&#31181;&#32463;&#20856;&#30340;ES&#26041;&#27861;&#21644;&#19977;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#22914;PPO&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;ES&#21487;&#20197;&#22312;&#35768;&#22810;RL&#22522;&#20934;&#20219;&#21153;&#20013;&#25214;&#21040;&#26377;&#25928;&#30340;&#32447;&#24615;&#31574;&#30053;&#65292;&#32780;DRL&#26041;&#27861;&#21482;&#33021;&#20351;&#29992;&#26356;&#22823;&#30340;&#32593;&#32476;&#25214;&#21040;&#25104;&#21151;&#30340;&#31574;&#30053;&#65292;&#36825;&#34920;&#26126;&#24403;&#21069;&#30340;&#22522;&#20934;&#38382;&#39064;&#27604;&#20197;&#21069;&#35748;&#20026;&#30340;&#26356;&#23481;&#26131;&#35299;&#20915;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#21363;&#20351;&#23545;&#20110;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;ES&#30340;&#32467;&#26524;&#20063;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although Deep Reinforcement Learning (DRL) methods can learn effective policies for challenging problems such as Atari games and robotics tasks, algorithms are complex and training times are often long. This study investigates how evolution strategies (ES) perform compared to gradient-based deep reinforcement learning methods. We use ES to optimize the weights of a neural network via neuroevolution, performing direct policy search. We benchmark both regular networks and policy networks consisting of a single linear layer from observations to actions; for three classical ES methods and for three gradient-based methods such as PPO. Our results reveal that ES can find effective linear policies for many RL benchmark tasks, in contrast to DRL methods that can only find successful policies using much larger networks, suggesting that current benchmarks are easier to solve than previously assumed. Interestingly, also for higher complexity tasks, ES achieves results comparable to gradient-based
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#29942;&#39048;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#39640;&#25299;&#25169;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#21644;&#22810;&#20851;&#31995;&#24402;&#32435;&#20559;&#32622;&#26469;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.06908</link><description>&lt;p&gt;
&#39640;&#25299;&#25169;&#31070;&#32463;&#32593;&#32476;&#65306;&#36890;&#36807;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#32531;&#35299;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#29942;&#39048;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Topological Neural Networks: Mitigating the Bottlenecks of Graph Neural Networks via Higher-Order Interactions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#29942;&#39048;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#39640;&#25299;&#25169;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#21644;&#22810;&#20851;&#31995;&#24402;&#32435;&#20559;&#32622;&#26469;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#29616;&#35937;&#30340;&#19981;&#21487;&#32422;&#22797;&#26434;&#24615;&#20351;&#24471;&#22270;&#31070;&#32463;&#32593;&#32476;&#25104;&#20026;&#22312;&#22270;&#32467;&#26500;&#25968;&#25454;&#19978;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#20219;&#21153;&#30340;&#26631;&#20934;&#27169;&#22411;&#12290;&#34429;&#28982;&#23427;&#20204;&#33021;&#22815;&#25429;&#25417;&#23616;&#37096;&#21644;&#20840;&#23616;&#27169;&#24335;&#30340;&#33021;&#21147;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#65292;&#20294;&#19982;&#38271;&#36317;&#31163;&#21644;&#39640;&#38454;&#20381;&#36182;&#30456;&#20851;&#30340;&#24433;&#21709;&#23545;&#36825;&#20123;&#27169;&#22411;&#25552;&#20986;&#20102;&#30456;&#24403;&#22823;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20174;&#29702;&#35770;&#26694;&#26550;&#20837;&#25163;&#65292;&#25581;&#31034;&#20102;&#32593;&#32476;&#30340;&#23485;&#24230;&#12289;&#28145;&#24230;&#21644;&#22270;&#25299;&#25169;&#23545;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#20013;&#36807;&#24230;&#21387;&#32553;&#29616;&#35937;&#30340;&#24433;&#21709;&#12290;&#28982;&#21518;&#65292;&#26412;&#25991;&#36890;&#36807;&#39640;&#25299;&#25169;&#31070;&#32463;&#32593;&#32476;&#20174;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#21644;&#22810;&#20851;&#31995;&#24402;&#32435;&#20559;&#32622;&#20837;&#25163;&#12290;&#36825;&#31181;&#27169;&#22411;&#36890;&#36807;&#39640;&#32500;&#32467;&#26500;&#20256;&#25773;&#28040;&#24687;&#65292;&#20026;&#20449;&#24687;&#27969;&#25552;&#20379;&#20102;&#24555;&#25463;&#26041;&#24335;&#25110;&#39069;&#22806;&#30340;&#36335;&#24452;&#12290;&#36890;&#36807;&#36825;&#31181;&#26500;&#24314;&#65292;&#24213;&#23618;&#30340;&#35745;&#31639;&#22270;&#19981;&#20877;&#19982;&#36755;&#20837;&#22270;&#32467;&#26500;&#32806;&#21512;&#65292;&#20174;&#32780;&#32531;&#35299;&#20102;&#21069;&#38754;&#25552;&#21040;&#30340;&#29942;&#39048;&#38382;&#39064;&#65292;&#21516;&#26102;&#36824;&#32771;&#34385;&#20102;h&#12290;
&lt;/p&gt;
&lt;p&gt;
The irreducible complexity of natural phenomena has led Graph Neural Networks to be employed as a standard model to perform representation learning tasks on graph-structured data. While their capacity to capture local and global patterns is remarkable, the implications associated with long-range and higher-order dependencies pose considerable challenges to such models. This work starts with a theoretical framework to reveal the impact of network's width, depth, and graph topology on the over-squashing phenomena in message-passing neural networks. Then, the work drifts towards, higher-order interactions and multi-relational inductive biases via Topological Neural Networks. Such models propagate messages through higher-dimensional structures, providing shortcuts or additional routes for information flow. With this construction, the underlying computational graph is no longer coupled with the input graph structure, thus mitigating the aforementioned bottlenecks while accounting also for h
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33258;&#21160;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#27602;&#24615;&#12290;&#36890;&#36807;&#20998;&#26512;&#27602;&#24615;&#22240;&#32032;&#21644;LLMs&#30340;&#20869;&#22312;&#27602;&#24615;&#23646;&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#27979;&#37327;&#27602;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#20247;&#65292;&#27604;&#29616;&#26377;&#25351;&#26631;&#25552;&#21319;12&#20010;&#30334;&#20998;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.06900</link><description>&lt;p&gt;
LLM&#33021;&#22815;&#35782;&#21035;&#27602;&#24615;&#21527;&#65311;&#32467;&#26500;&#21270;&#27602;&#24615;&#35843;&#26597;&#26694;&#26550;&#21644;&#22522;&#20110;&#35821;&#20041;&#30340;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Recognize Toxicity? Structured Toxicity Investigation Framework and Semantic-Based Metric
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33258;&#21160;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#27602;&#24615;&#12290;&#36890;&#36807;&#20998;&#26512;&#27602;&#24615;&#22240;&#32032;&#21644;LLMs&#30340;&#20869;&#22312;&#27602;&#24615;&#23646;&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#27979;&#37327;&#27602;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#20247;&#65292;&#27604;&#29616;&#26377;&#25351;&#26631;&#25552;&#21319;12&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24320;&#21457;&#36981;&#23432;&#31038;&#20250;&#26631;&#20934;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36807;&#31243;&#20013;&#65292;&#35782;&#21035;&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#27602;&#24615;&#23384;&#22312;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#27602;&#24615;&#24230;&#37327;&#20381;&#36182;&#20110;&#22312;&#29305;&#23450;&#27602;&#24615;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#32534;&#30721;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32534;&#30721;&#22120;&#23481;&#26131;&#21463;&#21040;&#20998;&#24067;&#22806;&#30340;&#38382;&#39064;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#20013;&#25152;&#20551;&#23450;&#30340;&#27602;&#24615;&#23450;&#20041;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;LLMs&#30340;&#33258;&#21160;&#40065;&#26834;&#24230;&#37327;&#65292;&#29992;&#20110;&#21306;&#20998;&#27169;&#22411;&#22238;&#24212;&#26159;&#21542;&#20855;&#26377;&#27602;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;&#27602;&#24615;&#22240;&#32032;&#65292;&#28982;&#21518;&#30740;&#31350;&#20102;LLMs&#30340;&#20869;&#22312;&#27602;&#24615;&#23646;&#24615;&#65292;&#20197;&#30830;&#23450;&#23427;&#20204;&#20316;&#20026;&#35780;&#20272;&#22120;&#30340;&#36866;&#29992;&#24615;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23545;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#30340;&#24230;&#37327;&#25351;&#26631;LLMs As ToxiciTy Evaluators&#65288;LATTE&#65289;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19981;&#36827;&#34892;&#35757;&#32451;&#36807;&#31243;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#24230;&#37327;&#22312;&#27979;&#37327;&#27602;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;F1&#24471;&#20998;&#27604;&#29616;&#26377;&#25216;&#26415;&#25351;&#26631;&#25552;&#39640;&#20102;12&#20010;&#30334;&#20998;&#28857;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#19978;&#28216;&#27602;&#24615;&#23545;&#24230;&#37327;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the pursuit of developing Large Language Models (LLMs) that adhere to societal standards, it is imperative to discern the existence of toxicity in the generated text. The majority of existing toxicity metrics rely on encoder models trained on specific toxicity datasets. However, these encoders are susceptible to out-of-distribution (OOD) problems and depend on the definition of toxicity assumed in a dataset. In this paper, we introduce an automatic robust metric grounded on LLMs to distinguish whether model responses are toxic. We start by analyzing the toxicity factors, followed by examining the intrinsic toxic attributes of LLMs to ascertain their suitability as evaluators. Subsequently, we evaluate our metric, LLMs As ToxiciTy Evaluators (LATTE), on evaluation datasets.The empirical results indicate outstanding performance in measuring toxicity, improving upon state-of-the-art metrics by 12 points in F1 score without training procedure. We also show that upstream toxicity has an 
&lt;/p&gt;</description></item><item><title>GenTranslate&#26159;&#19968;&#20010;&#26032;&#30340;&#32763;&#35793;&#20219;&#21153;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20016;&#23500;&#35821;&#35328;&#30693;&#35782;&#21644;&#24378;&#22823;&#25512;&#29702;&#33021;&#21147;&#65292;&#21487;&#20197;&#20174;N-best&#21015;&#34920;&#20013;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#32763;&#35793;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.06894</link><description>&lt;p&gt;
GenTranslate: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#29983;&#25104;&#30340;&#22810;&#35821;&#35328;&#35821;&#38899;&#21644;&#26426;&#22120;&#32763;&#35793;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
GenTranslate: Large Language Models are Generative Multilingual Speech and Machine Translators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06894
&lt;/p&gt;
&lt;p&gt;
GenTranslate&#26159;&#19968;&#20010;&#26032;&#30340;&#32763;&#35793;&#20219;&#21153;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20016;&#23500;&#35821;&#35328;&#30693;&#35782;&#21644;&#24378;&#22823;&#25512;&#29702;&#33021;&#21147;&#65292;&#21487;&#20197;&#20174;N-best&#21015;&#34920;&#20013;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#32763;&#35793;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#36890;&#36807;&#20943;&#23569;&#34920;&#31034;&#35823;&#24046;&#21644;&#24341;&#20837;&#22806;&#37096;&#30693;&#35782;&#65292;&#25512;&#21160;&#20102;&#22810;&#35821;&#35328;&#35821;&#38899;&#21644;&#26426;&#22120;&#32763;&#35793;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#32763;&#35793;&#20219;&#21153;&#36890;&#24120;&#20351;&#29992;&#26463;&#25628;&#32034;&#35299;&#30721;&#21644;&#21069;k&#20010;&#20551;&#35774;&#36873;&#25321;&#36827;&#34892;&#25512;&#29702;&#12290;&#36825;&#20123;&#25216;&#26415;&#24448;&#24448;&#19981;&#33021;&#20805;&#20998;&#21033;&#29992;&#22810;&#26679;&#21270;&#30340;N-best&#20551;&#35774;&#20013;&#30340;&#20016;&#23500;&#20449;&#24687;&#65292;&#20351;&#24471;&#23427;&#20204;&#22312;&#38656;&#35201;&#21333;&#20010;&#39640;&#36136;&#37327;&#36755;&#20986;&#24207;&#21015;&#30340;&#32763;&#35793;&#20219;&#21153;&#20013;&#25928;&#26524;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32763;&#35793;&#20219;&#21153;&#29983;&#25104;&#27169;&#22411;&#65292;&#21363;&#8220;GenTranslate&#8221;&#65292;&#23427;&#22522;&#20110;LLMs&#26469;&#20174;N-best&#21015;&#34920;&#20013;&#29983;&#25104;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#21033;&#29992;LLMs&#20016;&#23500;&#30340;&#35821;&#35328;&#30693;&#35782;&#21644;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#26032;&#27169;&#22411;&#21487;&#20197;&#23558;N-best&#20505;&#36873;&#20154;&#20013;&#30340;&#20016;&#23500;&#20449;&#24687;&#25972;&#21512;&#36215;&#26469;&#65292;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#32763;&#35793;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#25903;&#25345;LLM&#30340;&#24494;&#35843;&#65292;&#25105;&#20204;&#26500;&#24314;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;HypoTransla&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in large language models (LLMs) have stepped forward the development of multilingual speech and machine translation by its reduced representation errors and incorporated external knowledge. However, both translation tasks typically utilize beam search decoding and top-1 hypothesis selection for inference. These techniques struggle to fully exploit the rich information in the diverse N-best hypotheses, making them less optimal for translation tasks that require a single, high-quality output sequence. In this paper, we propose a new generative paradigm for translation tasks, namely "GenTranslate", which builds upon LLMs to generate better results from the diverse translation versions in N-best list. Leveraging the rich linguistic knowledge and strong reasoning abilities of LLMs, our new paradigm can integrate the rich information in N-best candidates to generate a higher-quality translation result. Furthermore, to support LLM finetuning, we build and release a HypoTransla
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#33539;&#24335;&#30340;&#26426;&#22120;&#21453;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#24378;&#22823;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26469;&#23454;&#29616;&#21453;&#23398;&#20064;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#24635;&#20307;&#24615;&#33021;&#65292;&#24182;&#22686;&#24378;&#20102;&#21453;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.06864</link><description>&lt;p&gt;
&#20855;&#26377;&#36776;&#21035;&#24615;&#23545;&#25239;&#23398;&#20064;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Discriminative Adversarial Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06864
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#33539;&#24335;&#30340;&#26426;&#22120;&#21453;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#24378;&#22823;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26469;&#23454;&#29616;&#21453;&#23398;&#20064;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#24635;&#20307;&#24615;&#33021;&#65292;&#24182;&#22686;&#24378;&#20102;&#21453;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#21453;&#23398;&#20064;&#26694;&#26550;&#65292;&#22522;&#20110;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#33539;&#24335;&#30340;&#24050;&#24314;&#31435;&#21407;&#21017;&#12290;&#25105;&#20204;&#21033;&#29992;&#24378;&#22823;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65288;MIA&#65289;&#30340;&#33021;&#21147;&#65292;&#20197;&#20419;&#36827;&#20174;&#35757;&#32451;&#27169;&#22411;&#20013;&#21453;&#23398;&#20064;&#29305;&#23450;&#26679;&#26412;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#20010;&#32593;&#32476;&#30340;&#22330;&#26223;&#65292;&#25915;&#20987;&#32773;$\mathbf{A}$&#21644;&#32463;&#36807;&#35757;&#32451;&#30340;&#38450;&#24481;&#32773; $\mathbf{D}$&#22312;&#23545;&#25239;&#30446;&#26631;&#19979;&#30456;&#20114;&#23545;&#25239;&#65292;&#20854;&#20013;&#25915;&#20987;&#32773;&#26088;&#22312;&#25581;&#31034;&#25968;&#25454;&#30340;&#20449;&#24687;&#20197;&#25512;&#26029;&#25104;&#21592;&#36523;&#20221;&#65292;&#32780;&#38450;&#24481;&#32773;&#22312;&#21453;&#20987;&#20013;&#36827;&#34892;&#21453;&#23398;&#20064;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#24635;&#20307;&#24615;&#33021;&#12290;&#31639;&#27861;&#21487;&#20197;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#36981;&#24490;&#24050;&#30693;&#30340;&#36845;&#20195;&#26368;&#23567;&#26368;&#22823;&#26041;&#27861;&#26469;&#26356;&#26032;&#25915;&#20987;&#32773;&#21644;&#38450;&#24481;&#32773;&#12290;&#25105;&#20204;&#36824;&#21152;&#20837;&#20102;&#33258;&#30417;&#30563;&#30446;&#26631;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#36951;&#24536;&#38598;&#21644;&#39564;&#35777;&#38598;&#20043;&#38388;&#30340;&#29305;&#24449;&#31354;&#38388;&#24046;&#24322;&#65292;&#22686;&#24378;&#20102;&#21453;&#23398;&#20064;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel machine unlearning framework founded upon the established principles of the min-max optimization paradigm. We capitalize on the capabilities of strong Membership Inference Attacks (MIA) to facilitate the unlearning of specific samples from a trained model. We consider the scenario of two networks, the attacker $\mathbf{A}$ and the trained defender $\mathbf{D}$ pitted against each other in an adversarial objective, wherein the attacker aims at teasing out the information of the data to be unlearned in order to infer membership, and the defender unlearns to defend the network against the attack, whilst preserving its general performance. The algorithm can be trained end-to-end using backpropagation, following the well known iterative min-max approach in updating the attacker and the defender. We additionally incorporate a self-supervised objective effectively addressing the feature space discrepancies between the forget set and the validation set, enhancing unlearnin
&lt;/p&gt;</description></item><item><title>UrbanKGent&#26159;&#19968;&#20010;&#29992;&#20110;&#22478;&#24066;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#30340;&#32479;&#19968;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#24322;&#26500;&#24863;&#30693;&#21644;&#22320;&#29702;&#31354;&#38388;&#27880;&#20837;&#26500;&#24314;&#30693;&#35782;&#21270;&#25351;&#20196;&#38598;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#36712;&#36857;&#32454;&#21270;&#27169;&#22359;&#26469;&#25552;&#21319;&#36712;&#36857;&#30340;&#36136;&#37327;&#12290;&#22312;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#35780;&#20272;&#34920;&#26126;&#20102;UrbanKGent&#30340;&#26377;&#25928;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.06861</link><description>&lt;p&gt;
UrbanKGent&#65306;&#29992;&#20110;&#22478;&#24066;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#30340;&#32479;&#19968;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
UrbanKGent: A Unified Large Language Model Agent Framework for Urban Knowledge Graph Construction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06861
&lt;/p&gt;
&lt;p&gt;
UrbanKGent&#26159;&#19968;&#20010;&#29992;&#20110;&#22478;&#24066;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#30340;&#32479;&#19968;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#24322;&#26500;&#24863;&#30693;&#21644;&#22320;&#29702;&#31354;&#38388;&#27880;&#20837;&#26500;&#24314;&#30693;&#35782;&#21270;&#25351;&#20196;&#38598;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#36712;&#36857;&#32454;&#21270;&#27169;&#22359;&#26469;&#25552;&#21319;&#36712;&#36857;&#30340;&#36136;&#37327;&#12290;&#22312;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#35780;&#20272;&#34920;&#26126;&#20102;UrbanKGent&#30340;&#26377;&#25928;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#30693;&#35782;&#22270;&#35889;&#26368;&#36817;&#20316;&#20026;&#19968;&#31181;&#26032;&#20852;&#30340;&#26500;&#24314;&#27169;&#22359;&#65292;&#22312;&#22810;&#28304;&#22478;&#24066;&#25968;&#25454;&#20013;&#25552;&#21462;&#20851;&#38190;&#30693;&#35782;&#65292;&#29992;&#20110;&#21508;&#31181;&#22478;&#24066;&#24212;&#29992;&#22330;&#26223;&#12290;&#23613;&#31649;&#20855;&#26377;&#28508;&#22312;&#30340;&#22909;&#22788;&#65292;&#20294;&#22478;&#24066;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#20173;&#28982;&#20005;&#37325;&#20381;&#36182;&#25163;&#21160;&#24037;&#20316;&#65292;&#38459;&#30861;&#20102;&#20854;&#28508;&#22312;&#30340;&#36827;&#23637;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;UrbanKGent&#65292;&#19968;&#31181;&#32479;&#19968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#22478;&#24066;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#24322;&#26500;&#24863;&#30693;&#21644;&#22320;&#29702;&#31354;&#38388;&#27880;&#20837;&#26469;&#26500;&#24314;UrbanKGC&#20219;&#21153;&#65288;&#22914;&#20851;&#31995;&#19977;&#20803;&#32452;&#25552;&#21462;&#21644;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#65289;&#30340;&#30693;&#35782;&#21270;&#25351;&#20196;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24037;&#20855;&#22686;&#24378;&#30340;&#36845;&#20195;&#36712;&#36857;&#32454;&#21270;&#27169;&#22359;&#65292;&#20197;&#22686;&#24378;&#21644;&#20248;&#21270;&#20174;GPT-4&#20013;&#25552;&#21462;&#30340;&#36712;&#36857;&#12290;&#36890;&#36807;&#22312;Llama-2-13B&#19978;&#36827;&#34892;&#22686;&#24378;&#36712;&#36857;&#30340;&#28151;&#21512;&#25351;&#20196;&#24494;&#35843;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;UrbanKGC&#20195;&#29702;UrbanKGent-13B&#12290;&#25105;&#20204;&#23545;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Urban knowledge graph has recently worked as an emerging building block to distill critical knowledge from multi-sourced urban data for diverse urban application scenarios. Despite its promising benefits, urban knowledge graph construction (UrbanKGC) still heavily relies on manual effort, hindering its potential advancement. This paper presents UrbanKGent, a unified large language model agent framework, for urban knowledge graph construction. Specifically, we first construct the knowledgeable instruction set for UrbanKGC tasks (such as relational triplet extraction and knowledge graph completion) via heterogeneity-aware and geospatial-infused instruction generation. Moreover, we propose a tool-augmented iterative trajectory refinement module to enhance and refine the trajectories distilled from GPT-4. Through hybrid instruction fine-tuning with augmented trajectories on Llama-2-13B, we obtain the UrbanKGC agent, UrbanKGent-13B. We perform a comprehensive evaluation on two real-world da
&lt;/p&gt;</description></item><item><title>LiRank&#26159;&#39046;&#33521;&#30340;&#19968;&#20010;&#22823;&#35268;&#27169;&#25490;&#21517;&#26694;&#26550;&#65292;&#23427;&#24212;&#29992;&#20102;&#26368;&#20808;&#36827;&#30340;&#24314;&#27169;&#26550;&#26500;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#24314;&#27169;&#25913;&#36827;&#21644;&#25216;&#26415;&#65292;&#36890;&#36807;A/B&#27979;&#35797;&#21462;&#24471;&#20102;&#26377;&#25928;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.06859</link><description>&lt;p&gt;
LiRank: &#39046;&#33521;&#30340;&#24037;&#19994;&#35268;&#27169;&#25490;&#21517;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LiRank: Industrial Large Scale Ranking Models at LinkedIn
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06859
&lt;/p&gt;
&lt;p&gt;
LiRank&#26159;&#39046;&#33521;&#30340;&#19968;&#20010;&#22823;&#35268;&#27169;&#25490;&#21517;&#26694;&#26550;&#65292;&#23427;&#24212;&#29992;&#20102;&#26368;&#20808;&#36827;&#30340;&#24314;&#27169;&#26550;&#26500;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#24314;&#27169;&#25913;&#36827;&#21644;&#25216;&#26415;&#65292;&#36890;&#36807;A/B&#27979;&#35797;&#21462;&#24471;&#20102;&#26377;&#25928;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;LiRank&#65292;&#36825;&#26159;&#39046;&#33521;&#30340;&#19968;&#20010;&#22823;&#35268;&#27169;&#25490;&#21517;&#26694;&#26550;&#65292;&#23427;&#23558;&#26368;&#20808;&#36827;&#30340;&#24314;&#27169;&#26550;&#26500;&#21644;&#20248;&#21270;&#26041;&#27861;&#24212;&#29992;&#20110;&#29983;&#20135;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#20960;&#20010;&#24314;&#27169;&#25913;&#36827;&#65292;&#21253;&#25324;Residual DCN&#65292;&#23427;&#22312;&#33879;&#21517;&#30340;DCNv2&#26550;&#26500;&#20013;&#28155;&#21152;&#20102;&#27880;&#24847;&#21147;&#21644;&#27531;&#24046;&#36830;&#25509;&#12290;&#25105;&#20204;&#20998;&#20139;&#20102;&#23558;SOTA&#26550;&#26500;&#32452;&#21512;&#21644;&#35843;&#20248;&#20197;&#21019;&#24314;&#32479;&#19968;&#27169;&#22411;&#30340;&#35265;&#35299;&#65292;&#21253;&#25324;Dense Gating&#12289;Transformers&#21644;Residual DCN&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#29992;&#20110;&#26657;&#20934;&#30340;&#26032;&#25216;&#26415;&#65292;&#24182;&#25551;&#36848;&#20102;&#22914;&#20309;&#23558;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25506;&#32034;/&#21033;&#29992;&#26041;&#27861;&#24212;&#29992;&#20110;&#29983;&#20135;&#29615;&#22659;&#12290;&#20026;&#20102;&#23454;&#29616;&#22823;&#35268;&#27169;&#25490;&#21517;&#27169;&#22411;&#30340;&#26377;&#25928;&#12289;&#29983;&#20135;&#32423;&#26381;&#21153;&#65292;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#20351;&#29992;&#37327;&#21270;&#21644;&#35789;&#27719;&#21387;&#32553;&#35757;&#32451;&#21644;&#21387;&#32553;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;Feed&#25490;&#21517;&#12289;&#32844;&#20301;&#25512;&#33616;&#21644;&#24191;&#21578;&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#31561;&#22823;&#35268;&#27169;&#20351;&#29992;&#26696;&#20363;&#30340;&#37096;&#32626;&#35774;&#32622;&#32454;&#33410;&#12290;&#36890;&#36807;&#38416;&#26126;&#26368;&#26377;&#25928;&#30340;&#25216;&#26415;&#26041;&#27861;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#21508;&#31181;A/B&#27979;&#35797;&#30340;&#32463;&#39564;&#25945;&#35757;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present LiRank, a large-scale ranking framework at LinkedIn that brings to production state-of-the-art modeling architectures and optimization methods. We unveil several modeling improvements, including Residual DCN, which adds attention and residual connections to the famous DCNv2 architecture. We share insights into combining and tuning SOTA architectures to create a unified model, including Dense Gating, Transformers and Residual DCN. We also propose novel techniques for calibration and describe how we productionalized deep learning based explore/exploit methods. To enable effective, production-grade serving of large ranking models, we detail how to train and compress models using quantization and vocabulary compression. We provide details about the deployment setup for large-scale use cases of Feed ranking, Jobs Recommendations, and Ads click-through rate (CTR) prediction. We summarize our learnings from various A/B tests by elucidating the most effective technical approaches. T
&lt;/p&gt;</description></item><item><title>ChemLLM&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#21270;&#23398;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21033;&#29992;&#26032;&#39062;&#30340;&#25351;&#20196;&#26500;&#24314;&#26041;&#27861;&#23558;&#32467;&#26500;&#21270;&#30693;&#35782;&#36716;&#21270;&#20026;&#23545;&#35805;&#24418;&#24335;&#65292;&#20855;&#26377;&#24179;&#28369;&#23545;&#35805;&#20132;&#20114;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#21270;&#23398;&#30340;&#19977;&#20010;&#20027;&#35201;&#20219;&#21153;&#20013;&#20987;&#36133;&#20102;GPT-3.5&#12290;</title><link>https://arxiv.org/abs/2402.06852</link><description>&lt;p&gt;
ChemLLM: &#19968;&#20010;&#21270;&#23398;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ChemLLM: A Chemical Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06852
&lt;/p&gt;
&lt;p&gt;
ChemLLM&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#21270;&#23398;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21033;&#29992;&#26032;&#39062;&#30340;&#25351;&#20196;&#26500;&#24314;&#26041;&#27861;&#23558;&#32467;&#26500;&#21270;&#30693;&#35782;&#36716;&#21270;&#20026;&#23545;&#35805;&#24418;&#24335;&#65292;&#20855;&#26377;&#24179;&#28369;&#23545;&#35805;&#20132;&#20114;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#21270;&#23398;&#30340;&#19977;&#20010;&#20027;&#35201;&#20219;&#21153;&#20013;&#20987;&#36133;&#20102;GPT-3.5&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21270;&#23398;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#65292;&#21253;&#25324;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#12289;&#20998;&#23376;&#29983;&#25104;&#12289;&#23454;&#39564;&#21327;&#35758;&#35774;&#35745;&#31561;&#12290;&#28982;&#32780;&#65292;&#35813;&#39046;&#22495;&#32570;&#20047;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#21270;&#23398;&#39046;&#22495;&#35774;&#35745;&#30340;&#22522;&#20110;&#23545;&#35805;&#30340;&#27169;&#22411;&#12290;&#36825;&#20010;&#25361;&#25112;&#26469;&#33258;&#20110;&#20107;&#23454;&#65292;&#22823;&#22810;&#25968;&#21270;&#23398;&#25968;&#25454;&#21644;&#31185;&#23398;&#30693;&#35782;&#20027;&#35201;&#23384;&#20648;&#22312;&#32467;&#26500;&#21270;&#25968;&#25454;&#24211;&#20013;&#65292;&#30452;&#25509;&#20351;&#29992;&#36825;&#20123;&#32467;&#26500;&#21270;&#25968;&#25454;&#20250;&#24433;&#21709;&#27169;&#22411;&#32500;&#25345;&#36830;&#36143;&#23545;&#35805;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#26495;&#30340;&#25351;&#20196;&#26500;&#24314;&#26041;&#27861;&#65292;&#23558;&#32467;&#26500;&#21270;&#30693;&#35782;&#36716;&#21270;&#20026;&#31616;&#27905;&#23545;&#35805;&#24418;&#24335;&#65292;&#36866;&#21512;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;ChemLLM&#65292;&#31532;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#21270;&#23398;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#21270;&#23398;&#39046;&#22495;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#36827;&#34892;&#24179;&#28369;&#23545;&#35805;&#20132;&#20114;&#12290;ChemLLM&#22312;&#21270;&#23398;&#30340;&#19977;&#20010;&#20027;&#35201;&#20219;&#21153;&#65292;&#21363;&#21517;&#31216;&#36716;&#25442;&#12289;&#20998;&#23376;&#29983;&#25104;&#21644;&#23454;&#39564;&#21327;&#35758;&#35774;&#35745;&#26041;&#38754;&#65292;&#20987;&#36133;&#20102;GPT-3.5&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have made impressive progress in chemistry applications, including molecular property prediction, molecular generation, experimental protocol design, etc. However, the community lacks a dialogue-based model specifically designed for chemistry. The challenge arises from the fact that most chemical data and scientific knowledge are primarily stored in structured databases, and the direct use of these structured data compromises the model's ability to maintain coherent dialogue. To tackle this issue, we develop a novel template-based instruction construction method that transforms structured knowledge into plain dialogue, making it suitable for language model training. By leveraging this approach, we develop ChemLLM, the first large language model dedicated to chemistry, capable of performing various tasks across chemical disciplines with smooth dialogue interaction. ChemLLM beats GPT-3.5 on all three principal tasks in chemistry, i.e., name conversion, molecu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#25968;&#25454;&#27880;&#37322;&#30340;&#25209;&#21028;&#23478;&#35889;&#65292;&#24378;&#35843;&#20102;&#35780;&#32423;&#32773;&#22810;&#26679;&#24615;&#23545;&#20844;&#24179;&#24615;&#21644;&#27169;&#22411;&#24615;&#33021;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;&#25968;&#25454;&#27880;&#37322;&#24037;&#20316;&#32773;&#30340;&#24037;&#20316;&#26465;&#20214;&#12289;&#27880;&#37322;&#32773;&#20027;&#35266;&#24615;&#23545;&#26631;&#31614;&#30340;&#24433;&#21709;&#20197;&#21450;&#27880;&#37322;&#24037;&#20316;&#20013;&#30340;&#24515;&#29702;&#20260;&#23475;&#12290;</title><link>https://arxiv.org/abs/2402.06811</link><description>&lt;p&gt;
&#32422;&#26463;&#21147;&#21644;&#26631;&#31614;&#65306;&#25968;&#25454;&#27880;&#37322;&#30340;WEIRD&#23478;&#35889;&#21644;&#31038;&#20250;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Discipline and Label: A WEIRD Genealogy and Social Theory of Data Annotation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#25968;&#25454;&#27880;&#37322;&#30340;&#25209;&#21028;&#23478;&#35889;&#65292;&#24378;&#35843;&#20102;&#35780;&#32423;&#32773;&#22810;&#26679;&#24615;&#23545;&#20844;&#24179;&#24615;&#21644;&#27169;&#22411;&#24615;&#33021;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;&#25968;&#25454;&#27880;&#37322;&#24037;&#20316;&#32773;&#30340;&#24037;&#20316;&#26465;&#20214;&#12289;&#27880;&#37322;&#32773;&#20027;&#35266;&#24615;&#23545;&#26631;&#31614;&#30340;&#24433;&#21709;&#20197;&#21450;&#27880;&#37322;&#24037;&#20316;&#20013;&#30340;&#24515;&#29702;&#20260;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#27880;&#37322;&#20173;&#28982;&#26159;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#24517;&#38656;&#21697;&#12290;&#26368;&#36817;&#20851;&#20110;&#25968;&#25454;&#27880;&#37322;&#30340;&#23454;&#35777;&#30740;&#31350;&#24050;&#32463;&#24320;&#22987;&#24378;&#35843;&#35780;&#32423;&#32773;&#30340;&#22810;&#26679;&#24615;&#23545;&#20844;&#24179;&#24615;&#21644;&#27169;&#22411;&#24615;&#33021;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#19988;&#26032;&#30340;&#30740;&#31350;&#32447;&#36335;&#24050;&#32463;&#24320;&#22987;&#30740;&#31350;&#25968;&#25454;&#27880;&#37322;&#24037;&#20316;&#32773;&#30340;&#24037;&#20316;&#26465;&#20214;&#65292;&#27880;&#37322;&#32773;&#20027;&#35266;&#24615;&#23545;&#26631;&#31614;&#30340;&#24433;&#21709;&#21644;&#20316;&#29992;&#65292;&#20197;&#21450;&#27880;&#37322;&#24037;&#20316;&#20013;&#28508;&#22312;&#30340;&#24515;&#29702;&#20260;&#23475;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#25968;&#25454;&#27880;&#37322;&#30340;&#25209;&#21028;&#23478;&#35889;&#65292;&#20174;&#20854;&#24515;&#29702;&#21644;&#30693;&#35273;&#30340;&#26041;&#38754;&#24320;&#22987;&#12290;&#25105;&#20204;&#20511;&#37492;&#20102;&#23545;&#19978;&#19990;&#32426;70&#24180;&#20195;&#35745;&#31639;&#26426;&#21270;&#23454;&#39564;&#23460;&#24515;&#29702;&#23398;&#23454;&#39564;&#23835;&#36215;&#30340;&#25209;&#35780;&#65292;&#36136;&#30097;&#36825;&#20123;&#23454;&#39564;&#26159;&#21542;&#20801;&#35768;&#23558;&#32467;&#26524;&#25512;&#24191;&#21040;&#36229;&#20986;&#23454;&#39564;&#23460;&#35774;&#32622;&#30340;&#24773;&#22659;&#20013;&#12290;&#25968;&#25454;&#27880;&#37322;&#33021;&#21542;&#36229;&#36234;&#23427;&#20204;&#33719;&#24471;&#30340;&#35774;&#32622;&#25110;&#22320;&#28857;&#26469;&#25512;&#24191;&#32467;&#26524;&#65311;&#24515;&#29702;&#23398;&#36807;&#20110;&#20381;&#36182;&#26469;&#33258;&#35199;&#26041;&#30340;&#21442;&#19982;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data annotation remains the sine qua non of machine learning and AI. Recent empirical work on data annotation has begun to highlight the importance of rater diversity for fairness, model performance, and new lines of research have begun to examine the working conditions for data annotation workers, the impacts and role of annotator subjectivity on labels, and the potential psychological harms from aspects of annotation work. This paper outlines a critical genealogy of data annotation; starting with its psychological and perceptual aspects. We draw on similarities with critiques of the rise of computerized lab-based psychological experiments in the 1970's which question whether these experiments permit the generalization of results beyond the laboratory settings within which these results are typically obtained. Do data annotations permit the generalization of results beyond the settings, or locations, in which they were obtained? Psychology is overly reliant on participants from Wester
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#24635;&#20449;&#24687;&#27969;&#26469;&#23450;&#37327;&#35780;&#20272;&#38899;&#20048;&#20013;&#30340;&#20849;&#21516;&#21019;&#36896;&#21147;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#23450;&#24615;&#30740;&#31350;&#35777;&#26126;&#35813;&#26041;&#27861;&#19982;&#20154;&#31867;&#24863;&#30693;&#30456;&#21305;&#37197;&#12290;</title><link>https://arxiv.org/abs/2402.06810</link><description>&lt;p&gt;
&#20351;&#29992;&#24635;&#20449;&#24687;&#27969;&#35780;&#20272;&#20849;&#21516;&#21019;&#36896;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluating Co-Creativity using Total Information Flow
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#24635;&#20449;&#24687;&#27969;&#26469;&#23450;&#37327;&#35780;&#20272;&#38899;&#20048;&#20013;&#30340;&#20849;&#21516;&#21019;&#36896;&#21147;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#23450;&#24615;&#30740;&#31350;&#35777;&#26126;&#35813;&#26041;&#27861;&#19982;&#20154;&#31867;&#24863;&#30693;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#20048;&#20013;&#30340;&#20849;&#21516;&#21019;&#36896;&#21147;&#25351;&#30340;&#26159;&#20004;&#20010;&#25110;&#26356;&#22810;&#30340;&#38899;&#20048;&#23478;&#25110;&#38899;&#20048;&#20195;&#29702;&#36890;&#36807;&#21019;&#20316;&#25110;&#21363;&#20852;&#21019;&#20316;&#38899;&#20048;&#30456;&#20114;&#20114;&#21160;&#12290;&#28982;&#32780;&#65292;&#36825;&#26159;&#19968;&#20010;&#38750;&#24120;&#20027;&#35266;&#30340;&#36807;&#31243;&#65292;&#27599;&#20010;&#38899;&#20048;&#23478;&#23545;&#20110;&#22312;&#26576;&#31181;&#24773;&#22659;&#19979;&#21738;&#31181;&#21363;&#20852;&#21019;&#20316;&#26356;&#22909;&#26377;&#33258;&#24049;&#30340;&#20559;&#22909;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#21019;&#24314;&#19968;&#20010;&#22522;&#20110;&#24635;&#20449;&#24687;&#27969;&#30340;&#24230;&#37327;&#26469;&#23450;&#37327;&#35780;&#20272;&#38899;&#20048;&#20013;&#30340;&#20849;&#21516;&#21019;&#36896;&#21147;&#36807;&#31243;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#25105;&#20204;&#30340;&#24230;&#37327;&#26159;&#21019;&#36896;&#24615;&#38899;&#20048;&#36807;&#31243;&#26377;&#22810;"&#22909;"&#30340;&#25351;&#26631;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#20551;&#35774;&#26159;&#65292;&#22909;&#30340;&#38899;&#20048;&#21019;&#20316;&#23558;&#26368;&#22823;&#21270;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#20449;&#24687;&#27969;&#65292;&#35813;&#20449;&#24687;&#27969;&#30001;&#35760;&#24405;&#22312;&#21333;&#29420;&#36712;&#36947;&#20013;&#30340;&#38899;&#20048;&#22768;&#38899;&#25429;&#25417;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#20316;&#20026;&#29109;&#20272;&#35745;&#22120;&#35745;&#31639;&#20449;&#24687;&#27969;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#23450;&#24615;&#30740;&#31350;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#20309;&#19982;&#20154;&#31867;&#24863;&#30693;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
Co-creativity in music refers to two or more musicians or musical agents interacting with one another by composing or improvising music. However, this is a very subjective process and each musician has their own preference as to which improvisation is better for some context. In this paper, we aim to create a measure based on total information flow to quantitatively evaluate the co-creativity process in music. In other words, our measure is an indication of how "good" a creative musical process is. Our main hypothesis is that a good musical creation would maximize information flow between the participants captured by music voices recorded in separate tracks. We propose a method to compute the information flow using pre-trained generative models as entropy estimators. We demonstrate how our method matches with human perception using a qualitative study.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;GPT-4V&#36827;&#34892;&#21487;&#35299;&#37322;&#39118;&#38505;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#35299;&#37322;&#22797;&#26434;&#30340;&#36807;&#39532;&#36335;&#22330;&#26223;&#65292;&#20026;&#30450;&#20154;&#21644;&#35270;&#21147;&#20302;&#19979;&#20154;&#22763;&#30340;&#23433;&#20840;&#20915;&#31574;&#25552;&#20379;&#25903;&#25345;&#12290;</title><link>https://arxiv.org/abs/2402.06794</link><description>&lt;p&gt;
&#26159;&#21542;&#23433;&#20840;&#36807;&#39532;&#36335;&#65311;GPT-4V&#29992;&#20110;&#23433;&#20840;&#24847;&#35782;&#30340;&#21487;&#35299;&#37322;&#39118;&#38505;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Is it safe to cross? Interpretable Risk Assessment with GPT-4V for Safety-Aware Street Crossing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;GPT-4V&#36827;&#34892;&#21487;&#35299;&#37322;&#39118;&#38505;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#35299;&#37322;&#22797;&#26434;&#30340;&#36807;&#39532;&#36335;&#22330;&#26223;&#65292;&#20026;&#30450;&#20154;&#21644;&#35270;&#21147;&#20302;&#19979;&#20154;&#22763;&#30340;&#23433;&#20840;&#20915;&#31574;&#25552;&#20379;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#30450;&#20154;&#21644;&#35270;&#21147;&#20302;&#19979;&#30340;&#20154;&#26469;&#35828;&#65292;&#23433;&#20840;&#22320;&#36890;&#36807;&#34903;&#36947;&#20132;&#21449;&#21475;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#23545;&#21608;&#22260;&#29615;&#22659;&#26377;&#32454;&#33268;&#30340;&#29702;&#35299;&#65292;&#32780;&#36825;&#20010;&#20219;&#21153;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#35270;&#35273;&#32447;&#32034;&#12290;&#20256;&#32479;&#30340;&#36741;&#21161;&#20915;&#31574;&#26041;&#27861;&#24448;&#24448;&#19981;&#22815;&#23436;&#21892;&#65292;&#26080;&#27861;&#25552;&#20379;&#20840;&#38754;&#30340;&#22330;&#26223;&#20998;&#26512;&#21644;&#23433;&#20840;&#32423;&#21035;&#21028;&#26029;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#22810;&#27169;&#22411;&#26469;&#35299;&#37322;&#22797;&#26434;&#30340;&#36807;&#39532;&#36335;&#22330;&#26223;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;&#20132;&#36890;&#20449;&#21495;&#35782;&#21035;&#25216;&#26415;&#65292;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#36827;&#27493;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#29983;&#25104;&#23433;&#20840;&#35780;&#20998;&#21644;&#33258;&#28982;&#35821;&#35328;&#22330;&#26223;&#25551;&#36848;&#65292;&#25903;&#25345;&#30450;&#20154;&#21644;&#35270;&#21147;&#20302;&#19979;&#20154;&#22763;&#23433;&#20840;&#20915;&#31574;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#30001;&#22235;&#36275;&#26426;&#22120;&#20154;&#25429;&#33719;&#30340;&#22810;&#35270;&#35282;&#33258;&#25105;&#20013;&#24515;&#22270;&#20687;&#26500;&#25104;&#30340;&#36807;&#39532;&#36335;&#20132;&#21449;&#21475;&#25968;&#25454;&#65292;&#24182;&#26681;&#25454;&#39044;&#20808;&#23450;&#20041;&#30340;&#23433;&#20840;&#35780;&#20998;&#20998;&#31867;&#36827;&#34892;&#20102;&#22270;&#20687;&#26631;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safely navigating street intersections is a complex challenge for blind and low-vision individuals, as it requires a nuanced understanding of the surrounding context - a task heavily reliant on visual cues. Traditional methods for assisting in this decision-making process often fall short, lacking the ability to provide a comprehensive scene analysis and safety level. This paper introduces an innovative approach that leverages large multimodal models (LMMs) to interpret complex street crossing scenes, offering a potential advancement over conventional traffic signal recognition techniques. By generating a safety score and scene description in natural language, our method supports safe decision-making for the blind and low-vision individuals. We collected crosswalk intersection data that contains multiview egocentric images captured by a quadruped robot and annotated the images with corresponding safety scores based on our predefined safety score categorization. Grounded on the visual k
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#36890;&#29992;&#24773;&#26223;&#30340;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#26377;&#38480;&#25968;&#25454;&#38598;&#19978;&#30340;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.06784</link><description>&lt;p&gt;
&#26377;&#38480;&#25968;&#25454;&#38598;&#19978;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#36801;&#31227;&#23398;&#20064;&#29992;&#20110;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Transfer learning with generative models for object detection on limited datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#36890;&#29992;&#24773;&#26223;&#30340;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#26377;&#38480;&#25968;&#25454;&#38598;&#19978;&#30340;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26576;&#20123;&#39046;&#22495;&#20013;&#65292;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#26159;&#26377;&#38480;&#30340;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#65292;&#38656;&#35201;&#27491;&#30830;&#26631;&#35760;&#27599;&#20010;&#30446;&#26631;&#21608;&#22260;&#30340;&#36793;&#30028;&#26694;&#12290;&#19968;&#20010;&#26174;&#33879;&#30340;&#20363;&#23376;&#26159;&#22312;&#28023;&#27915;&#29983;&#29289;&#23398;&#39046;&#22495;&#65292;&#38656;&#35201;&#24320;&#21457;&#33258;&#21160;&#26816;&#27979;&#28023;&#27915;&#29289;&#31181;&#29992;&#20110;&#29615;&#22659;&#30417;&#27979;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#25968;&#25454;&#38480;&#21046;&#38382;&#39064;&#65292;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#31574;&#30053;&#37319;&#29992;&#20102;&#20004;&#31181;&#20027;&#35201;&#26041;&#27861;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#26159;&#22312;&#29616;&#26377;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#28982;&#21518;&#25512;&#24191;&#21040;&#20855;&#20307;&#30340;&#39046;&#22495;&#12290;&#31532;&#20108;&#31181;&#31574;&#30053;&#26159;&#20351;&#29992;copy-paste&#25216;&#26415;&#25110;ad-hoc&#27169;&#25311;&#22120;&#31561;&#26041;&#27861;&#21019;&#24314;&#29305;&#23450;&#20110;&#30446;&#26631;&#39046;&#22495;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#24448;&#24448;&#38754;&#20020;&#37325;&#22823;&#30340;&#39046;&#22495;&#36716;&#31227;&#38382;&#39064;&#65292;&#32780;&#31532;&#20108;&#31181;&#26041;&#27861;&#38656;&#35201;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#35774;&#35745;&#23450;&#21046;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#36890;&#29992;&#24773;&#26223;&#19979;&#26377;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
The availability of data is limited in some fields, especially for object detection tasks, where it is necessary to have correctly labeled bounding boxes around each object. A notable example of such data scarcity is found in the domain of marine biology, where it is useful to develop methods to automatically detect submarine species for environmental monitoring. To address this data limitation, the state-of-the-art machine learning strategies employ two main approaches. The first involves pretraining models on existing datasets before generalizing to the specific domain of interest. The second strategy is to create synthetic datasets specifically tailored to the target domain using methods like copy-paste techniques or ad-hoc simulators. The first strategy often faces a significant domain shift, while the second demands custom solutions crafted for the specific task. In response to these challenges, here we propose a transfer learning framework that is valid for a generic scenario. In
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26356;&#24369;&#30340;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#35780;&#20272;&#26356;&#24378;&#30340;&#27169;&#22411;&#30340;&#27491;&#30830;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#36827;&#34892;&#36777;&#35770;&#65292;&#38750;&#19987;&#23478;&#27169;&#22411;&#21644;&#20154;&#31867;&#22238;&#31572;&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#37117;&#26377;&#25152;&#25552;&#39640;&#12290;</title><link>https://arxiv.org/abs/2402.06782</link><description>&lt;p&gt;
&#19982;&#26356;&#26377;&#35828;&#26381;&#21147;&#30340;LLMs&#36777;&#35770;&#20250;&#23548;&#33268;&#26356;&#30495;&#23454;&#30340;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
Debating with More Persuasive LLMs Leads to More Truthful Answers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26356;&#24369;&#30340;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#35780;&#20272;&#26356;&#24378;&#30340;&#27169;&#22411;&#30340;&#27491;&#30830;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#36827;&#34892;&#36777;&#35770;&#65292;&#38750;&#19987;&#23478;&#27169;&#22411;&#21644;&#20154;&#31867;&#22238;&#31572;&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#37117;&#26377;&#25152;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#25152;&#38656;&#34892;&#20026;&#19968;&#33268;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24120;&#35265;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#20154;&#24037;&#26631;&#27880;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#23427;&#20204;&#23558;&#36229;&#36807;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#65292;&#20154;&#31867;&#35780;&#20272;&#30340;&#35282;&#33394;&#23558;&#28436;&#21464;&#20026;&#38750;&#19987;&#23478;&#30417;&#30563;&#19987;&#23478;&#12290;&#22312;&#27492;&#20043;&#21069;&#65292;&#25105;&#20204;&#38382;&#65306;&#26356;&#24369;&#30340;&#27169;&#22411;&#33021;&#35780;&#20272;&#26356;&#24378;&#30340;&#27169;&#22411;&#30340;&#27491;&#30830;&#24615;&#21527;&#65311;&#25105;&#20204;&#22312;&#31867;&#20284;&#30340;&#29615;&#22659;&#20013;&#35843;&#26597;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#26356;&#24378;&#30340;&#27169;&#22411;&#65288;&#19987;&#23478;&#65289;&#25317;&#26377;&#22238;&#31572;&#38382;&#39064;&#25152;&#38656;&#30340;&#20449;&#24687;&#65292;&#32780;&#26356;&#24369;&#30340;&#27169;&#22411;&#65288;&#38750;&#19987;&#23478;&#65289;&#32570;&#20047;&#36825;&#20123;&#20449;&#24687;&#12290;&#25105;&#20204;&#35780;&#20272;&#30340;&#26041;&#27861;&#26159;\textit{&#36777;&#35770;}&#65292;&#20854;&#20013;&#20004;&#20010;LLM&#19987;&#23478;&#20998;&#21035;&#25903;&#25345;&#19981;&#21516;&#30340;&#31572;&#26696;&#65292;&#19968;&#20010;&#38750;&#19987;&#23478;&#36873;&#25321;&#31572;&#26696;&#12290;&#25105;&#20204;&#21457;&#29616;&#36777;&#35770; consistently&#24110;&#21161;&#38750;&#19987;&#23478;&#27169;&#22411;&#21644;&#20154;&#31867;&#22238;&#31572;&#38382;&#39064;&#65292;&#20998;&#21035;&#36798;&#21040;76%&#21644;88%&#30340;&#20934;&#30830;&#24615;&#65288;&#26420;&#32032;&#22522;&#20934;&#20998;&#21035;&#20026;48%&#21644;60%&#65289;&#12290;&#27492;&#22806;&#65292;&#20197;&#26080;&#30417;&#30563;&#26041;&#24335;&#20248;&#21270;&#19987;&#23478;&#36777;&#35770;&#32773;&#30340;&#35828;&#26381;&#21147;&#20250;&#25552;&#39640;&#38750;&#19987;&#23478;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Common methods for aligning large language models (LLMs) with desired behaviour heavily rely on human-labelled data. However, as models grow increasingly sophisticated, they will surpass human expertise, and the role of human evaluation will evolve into non-experts overseeing experts. In anticipation of this, we ask: can weaker models assess the correctness of stronger models? We investigate this question in an analogous setting, where stronger models (experts) possess the necessary information to answer questions and weaker models (non-experts) lack this information. The method we evaluate is \textit{debate}, where two LLM experts each argue for a different answer, and a non-expert selects the answer. We find that debate consistently helps both non-expert models and humans answer questions, achieving 76\% and 88\% accuracy respectively (naive baselines obtain 48\% and 60\%). Furthermore, optimising expert debaters for persuasiveness in an unsupervised manner improves non-expert abilit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#27169;&#26495;&#30340;&#36870;&#21521;&#21512;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20135;&#29289;&#20998;&#23376;&#22270;&#21644;&#31163;&#24320;&#22522;&#22242;&#36229;&#22270;&#20013;&#36827;&#34892;&#25628;&#32034;&#65292;&#20197;&#22788;&#29702;&#22797;&#26434;&#30340;&#21453;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.06772</link><description>&lt;p&gt;
&#36890;&#36807;(&#36229;)&#22270;&#25628;&#32034;&#36827;&#34892;&#36870;&#21512;&#25104;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Retrosynthesis Prediction via Search in (Hyper) Graph
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#27169;&#26495;&#30340;&#36870;&#21521;&#21512;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20135;&#29289;&#20998;&#23376;&#22270;&#21644;&#31163;&#24320;&#22522;&#22242;&#36229;&#22270;&#20013;&#36827;&#34892;&#25628;&#32034;&#65292;&#20197;&#22788;&#29702;&#22797;&#26434;&#30340;&#21453;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26377;&#26426;&#21512;&#25104;&#20013;&#65292;&#20174;&#25351;&#23450;&#30340;&#26680;&#24515;&#20135;&#29289;&#39044;&#27979;&#21453;&#24212;&#29289;&#26159;&#19968;&#20010;&#22522;&#26412;&#30340;&#25361;&#25112;&#65292;&#34987;&#31216;&#20026;&#36870;&#21521;&#21512;&#25104;&#39044;&#27979;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#21322;&#27169;&#26495;&#21644;&#22522;&#20110;&#22270;&#32534;&#36753;&#30340;&#26041;&#27861;&#22312;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#30340;&#26426;&#21046;&#65292;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#39044;&#27979;&#22797;&#26434;&#30340;&#21453;&#24212;&#65292;&#20363;&#22914;&#20855;&#26377;&#22810;&#20010;&#21453;&#24212;&#20013;&#24515;&#25110;&#23558;&#30456;&#21516;&#31163;&#24320;&#22522;&#22242;&#36830;&#25509;&#21040;&#22810;&#20010;&#21407;&#23376;&#30340;&#21453;&#24212;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#27169;&#26495;&#30340;&#26041;&#27861;&#65292;&#21363;&#36870;&#21521;&#21512;&#25104;&#36890;&#36807;(&#36229;)&#22270;&#25628;&#32034;(RetroSiG)&#26694;&#26550;&#65292;&#20197;&#20943;&#36731;&#36825;&#20123;&#38480;&#21046;&#12290;&#22312;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#23558;&#21453;&#24212;&#20013;&#24515;&#30340;&#35782;&#21035;&#21644;&#31163;&#24320;&#22522;&#22242;&#30340;&#23436;&#25104;&#20219;&#21153;&#36716;&#21270;&#20026;&#22312;&#20135;&#29289;&#20998;&#23376;&#22270;&#20013;&#25628;&#32034;&#21644;&#22312;&#31163;&#24320;&#22522;&#22242;&#36229;&#22270;&#20013;&#25628;&#32034;&#30340;&#20219;&#21153;&#12290;&#20316;&#20026;&#19968;&#31181;&#22522;&#20110;&#21322;&#27169;&#26495;&#30340;&#26041;&#27861;&#65292;RetroSiG&#20855;&#26377;&#20960;&#20010;&#20248;&#28857;&#12290;&#39318;&#20808;&#65292;RetroSiG&#33021;&#22815;&#22788;&#29702;&#25552;&#21040;&#30340;&#22797;&#26434;&#21453;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting reactants from a specified core product stands as a fundamental challenge within organic synthesis, termed retrosynthesis prediction. Recently, semi-template-based methods and graph-edits-based methods have achieved good performance in terms of both interpretability and accuracy. However, due to their mechanisms these methods cannot predict complex reactions, e.g., reactions with multiple reaction center or attaching the same leaving group to more than one atom. In this study we propose a semi-template-based method, the \textbf{Retro}synthesis via \textbf{S}earch \textbf{i}n (Hyper) \textbf{G}raph (RetroSiG) framework to alleviate these limitations. In the proposed method, we turn the reaction center identification and the leaving group completion tasks as tasks of searching in the product molecular graph and leaving group hypergraph respectively. As a semi-template-based method RetroSiG has several advantages. First, RetroSiG is able to handle the complex reactions mentione
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#25991;&#26412;&#22686;&#24378;&#26041;&#27861;&#30340;&#35780;&#20272;&#25351;&#26631;&#20998;&#31867;&#27861;&#65292;&#20026;&#32479;&#19968;&#22522;&#20934;&#25552;&#20379;&#26041;&#21521;&#21644;&#24110;&#21161;&#12290;&#22312;&#19981;&#21516;&#20219;&#21153;&#12289;&#24230;&#37327;&#26631;&#20934;&#21644;&#23454;&#39564;&#35774;&#32622;&#19979;&#65292;&#35813;&#20998;&#31867;&#27861;&#26377;&#21161;&#20110;&#27604;&#36739;&#19981;&#21516;&#30340;&#22686;&#24378;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.06766</link><description>&lt;p&gt;
&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#35780;&#20272;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Evaluation Metrics for Text Data Augmentation in NLP
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06766
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#25991;&#26412;&#22686;&#24378;&#26041;&#27861;&#30340;&#35780;&#20272;&#25351;&#26631;&#20998;&#31867;&#27861;&#65292;&#20026;&#32479;&#19968;&#22522;&#20934;&#25552;&#20379;&#26041;&#21521;&#21644;&#24110;&#21161;&#12290;&#22312;&#19981;&#21516;&#20219;&#21153;&#12289;&#24230;&#37327;&#26631;&#20934;&#21644;&#23454;&#39564;&#35774;&#32622;&#19979;&#65292;&#35813;&#20998;&#31867;&#27861;&#26377;&#21161;&#20110;&#27604;&#36739;&#19981;&#21516;&#30340;&#22686;&#24378;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#25968;&#25454;&#22686;&#24378;&#30340;&#35843;&#30740;&#25253;&#21578;&#25351;&#20986;&#20102;&#35813;&#39046;&#22495;&#30340;&#19981;&#21516;&#25216;&#26415;&#21644;&#36827;&#23637;&#12290;&#20960;&#20010;&#26694;&#26550;&#12289;&#24037;&#20855;&#21644;&#23384;&#20648;&#24211;&#25512;&#24191;&#20102;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#27969;&#27700;&#32447;&#30340;&#23454;&#26045;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19981;&#21516;&#30340;&#20219;&#21153;&#12289;&#24230;&#37327;&#26631;&#20934;&#12289;&#25968;&#25454;&#38598;&#12289;&#20307;&#31995;&#32467;&#26500;&#21644;&#23454;&#39564;&#35774;&#32622;&#30340;&#32570;&#20047;&#35780;&#20272;&#26631;&#20934;&#21644;&#26041;&#27861;&#27604;&#36739;&#26631;&#20934;&#20351;&#24471;&#27604;&#36739;&#21464;&#24471;&#27627;&#26080;&#24847;&#20041;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#26041;&#27861;&#30340;&#32479;&#19968;&#24615;&#65292;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#30740;&#31350;&#23558;&#21463;&#30410;&#20110;&#27604;&#36739;&#19981;&#21516;&#30340;&#22686;&#24378;&#26041;&#27861;&#30340;&#32479;&#19968;&#25351;&#26631;&#12290;&#22240;&#27492;&#65292;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#37117;&#22312;&#21162;&#21147;&#23547;&#25214;&#30456;&#20851;&#30340;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#26412;&#30740;&#31350;&#30340;&#36129;&#29486;&#22312;&#20110;&#25552;&#20379;&#20102;&#25991;&#26412;&#22686;&#24378;&#26041;&#27861;&#30340;&#35780;&#20272;&#25351;&#26631;&#20998;&#31867;&#27861;&#65292;&#24182;&#20316;&#20026;&#32479;&#19968;&#22522;&#20934;&#30340;&#26041;&#21521;&#12290;&#25152;&#25552;&#20986;&#30340;&#20998;&#31867;&#27861;&#21253;&#25324;&#20102;&#23454;&#26045;&#24037;&#20855;&#21644;&#25351;&#26631;&#35745;&#31639;&#30340;&#31867;&#21035;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#36825;&#39033;&#30740;&#31350;&#65292;&#25105;&#20204;&#30340;&#30446;&#30340;&#26159;&#20026;&#20102;&#25512;&#36827;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#35780;&#20272;&#25351;&#26631;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent surveys on data augmentation for natural language processing have reported different techniques and advancements in the field. Several frameworks, tools, and repositories promote the implementation of text data augmentation pipelines. However, a lack of evaluation criteria and standards for method comparison due to different tasks, metrics, datasets, architectures, and experimental settings makes comparisons meaningless. Also, a lack of methods unification exists and text data augmentation research would benefit from unified metrics to compare different augmentation methods. Thus, academics and the industry endeavor relevant evaluation metrics for text data augmentation techniques. The contribution of this work is to provide a taxonomy of evaluation metrics for text augmentation methods and serve as a direction for a unified benchmark. The proposed taxonomy organizes categories that include tools for implementation and metrics calculation. Finally, with this study, we intend to 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;GLaM&#26041;&#27861;&#65292;&#36890;&#36807;&#37051;&#22495;&#21010;&#20998;&#21644;&#29983;&#25104;&#23376;&#22270;&#32534;&#30721;&#65292;&#23545;&#39046;&#22495;&#30693;&#35782;&#22270;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#12290;&#35813;&#26041;&#27861;&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#33021;&#22815;&#23454;&#29616;&#23545;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#22810;&#27493;&#25512;&#29702;&#65292;&#24182;&#20943;&#23569;&#34394;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.06764</link><description>&lt;p&gt;
&#36890;&#36807;&#37051;&#22495;&#21010;&#20998;&#21644;&#29983;&#25104;&#23376;&#22270;&#32534;&#30721;&#23545;&#39046;&#22495;&#30693;&#35782;&#22270;&#23545;&#40784;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;GLaM&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph Alignment via Neighborhood Partitioning and Generative Subgraph Encoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06764
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;GLaM&#26041;&#27861;&#65292;&#36890;&#36807;&#37051;&#22495;&#21010;&#20998;&#21644;&#29983;&#25104;&#23376;&#22270;&#32534;&#30721;&#65292;&#23545;&#39046;&#22495;&#30693;&#35782;&#22270;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#12290;&#35813;&#26041;&#27861;&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#33021;&#22815;&#23454;&#29616;&#23545;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#22810;&#27493;&#25512;&#29702;&#65292;&#24182;&#20943;&#23569;&#34394;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20174;&#29305;&#23450;&#39046;&#22495;&#25968;&#25454;&#27966;&#29983;&#30340;&#30693;&#35782;&#22270;&#38598;&#25104;&#65292;&#20195;&#34920;&#20102;&#26397;&#30528;&#26356;&#24378;&#22823;&#21644;&#20107;&#23454;&#25512;&#29702;&#30340;&#37325;&#35201;&#36827;&#23637;&#12290;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#24378;&#22823;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#22810;&#27493;&#25512;&#29702;&#65292;&#24182;&#23613;&#37327;&#20943;&#23569;&#34394;&#26500;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#20114;&#36830;&#23454;&#20307;&#30340;&#39046;&#22495;&#19987;&#29992;&#22270;&#26102;&#65292;&#33021;&#21147;&#20173;&#28982;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#22635;&#34917;&#36825;&#19968;&#25216;&#26415;&#19978;&#30340;&#37325;&#35201;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integrating large language models (LLMs) with knowledge graphs derived from domain-specific data represents an important advancement towards more powerful and factual reasoning. As these models grow more capable, it is crucial to enable them to perform multi-step inferences over real-world knowledge graphs while minimizing hallucination. While large language models excel at conversation and text generation, their ability to reason over domain-specialized graphs of interconnected entities remains limited. For example, can we query a LLM to identify the optimal contact in a professional network for a specific goal, based on relationships and attributes in a private database? The answer is no--such capabilities lie beyond current methods. However, this question underscores a critical technical gap that must be addressed. Many high-value applications in areas such as science, security, and e-commerce rely on proprietary knowledge graphs encoding unique structures, relationships, and logica
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#25237;&#36164;&#32773;&#31454;&#36187;&#25968;&#25454;&#36827;&#34892;&#32858;&#31867;&#20998;&#26512;&#30340;&#26041;&#27861;&#26469;&#20998;&#26512;&#38382;&#21367;&#25968;&#25454;&#65292;&#21457;&#29616;&#19982;&#37329;&#34701;&#25968;&#25454;&#30456;&#32467;&#21512;&#26102;&#30340;&#39069;&#22806;&#35265;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#21019;&#26032;&#30340;&#35270;&#35273;&#34920;&#31034;&#26041;&#27861;&#26469;&#39564;&#35777;&#32858;&#31867;&#20998;&#26512;&#21644;&#38382;&#39064;&#20043;&#38388;&#30340;&#20851;&#31995;&#21457;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.06759</link><description>&lt;p&gt;
&#19968;&#31181;&#38382;&#21367;&#20998;&#26512;&#26041;&#27861;&#65306;&#36890;&#36807;&#23545;&#25237;&#36164;&#32773;&#31454;&#36187;&#25968;&#25454;&#36827;&#34892;&#32858;&#31867;&#20998;&#26512;&#33719;&#24471;&#30340;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
A Methodology for Questionnaire Analysis: Insights through Cluster Analysis of an Investor Competition Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#25237;&#36164;&#32773;&#31454;&#36187;&#25968;&#25454;&#36827;&#34892;&#32858;&#31867;&#20998;&#26512;&#30340;&#26041;&#27861;&#26469;&#20998;&#26512;&#38382;&#21367;&#25968;&#25454;&#65292;&#21457;&#29616;&#19982;&#37329;&#34701;&#25968;&#25454;&#30456;&#32467;&#21512;&#26102;&#30340;&#39069;&#22806;&#35265;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#21019;&#26032;&#30340;&#35270;&#35273;&#34920;&#31034;&#26041;&#27861;&#26469;&#39564;&#35777;&#32858;&#31867;&#20998;&#26512;&#21644;&#38382;&#39064;&#20043;&#38388;&#30340;&#20851;&#31995;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38382;&#21367;&#25968;&#25454;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#24182;&#24212;&#29992;&#20110;&#36890;&#36807;&#26085;&#20869;&#20132;&#26131;&#31454;&#36187;&#30340;&#25237;&#36164;&#32773;&#25968;&#25454;&#20013;&#21457;&#29616;&#35265;&#35299;&#12290;&#38382;&#21367;&#21253;&#25324;&#20998;&#31867;&#38382;&#39064;&#65292;&#23558;&#20854;&#31616;&#21270;&#20026;&#20108;&#36827;&#21046;&#38382;&#39064;&#65292;&#21363;&#8220;&#26159;&#8221;&#25110;&#8220;&#21542;&#8221;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#32858;&#31867;&#20998;&#26512;&#23558;&#20855;&#26377;&#30456;&#20284;&#21709;&#24212;&#30340;&#38382;&#39064;&#21644;&#21442;&#19982;&#32773;&#20998;&#32452;&#26469;&#38477;&#20302;&#32500;&#24230;&#12290;&#20351;&#29992;&#36716;&#21270;&#29575;&#25351;&#26631;&#36827;&#34892;&#35268;&#21017;&#21457;&#29616;&#12290;&#25552;&#20986;&#20102;&#21019;&#26032;&#30340;&#35270;&#35273;&#34920;&#31034;&#26041;&#27861;&#26469;&#39564;&#35777;&#32858;&#31867;&#20998;&#26512;&#20197;&#21450;&#38382;&#39064;&#20043;&#38388;&#30340;&#20851;&#31995;&#21457;&#29616;&#12290;&#19982;&#37329;&#34701;&#25968;&#25454;&#20132;&#21449;&#26102;&#65292;&#36824;&#25581;&#31034;&#20102;&#19982;&#35782;&#21035;&#30340;&#32858;&#31867;&#30456;&#20851;&#30340;&#39069;&#22806;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a methodology for the analysis of questionnaire data along with its application on discovering insights from investor data motivated by a day trading competition. The questionnaire includes categorical questions, which are reduced to binary questions, 'yes' or 'no'. The methodology reduces dimensionality by grouping questions and participants with similar responses using clustering analysis. Rule discovery was performed by using a conversion rate metric. Innovative visual representations were proposed to validate the cluster analysis and the relation discovery between questions. When crossing with financial data, additional insights were revealed related to the recognized clusters.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;ExGRG&#65292;&#23427;&#36890;&#36807;&#26174;&#24335;&#29983;&#25104;&#20851;&#31995;&#22270;&#26469;&#35299;&#20915;&#22270;&#32467;&#26500;&#25968;&#25454;&#19978;&#30340;&#25361;&#25112;&#65292;&#23558;&#20808;&#39564;&#39046;&#22495;&#30693;&#35782;&#21644;&#22312;&#32447;&#25552;&#21462;&#30340;&#20449;&#24687;&#32435;&#20837;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;</title><link>https://arxiv.org/abs/2402.06737</link><description>&lt;p&gt;
ExGRG: &#29992;&#20110;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#26174;&#24335;&#29983;&#25104;&#20851;&#31995;&#22270;
&lt;/p&gt;
&lt;p&gt;
ExGRG: Explicitly-Generated Relation Graph for Self-Supervised Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;ExGRG&#65292;&#23427;&#36890;&#36807;&#26174;&#24335;&#29983;&#25104;&#20851;&#31995;&#22270;&#26469;&#35299;&#20915;&#22270;&#32467;&#26500;&#25968;&#25454;&#19978;&#30340;&#25361;&#25112;&#65292;&#23558;&#20808;&#39564;&#39046;&#22495;&#30693;&#35782;&#21644;&#22312;&#32447;&#25552;&#21462;&#30340;&#20449;&#24687;&#32435;&#20837;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#20316;&#20026;&#19968;&#31181;&#26080;&#38656;&#26114;&#36149;&#30340;&#26631;&#27880;&#26631;&#31614;&#32780;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24378;&#22823;&#25216;&#26415;&#65292;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#30340;&#20869;&#23884;&#20449;&#21495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;SSL&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#36890;&#36807;&#30452;&#35266;&#30340;&#25968;&#25454;&#22686;&#24378;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#20854;&#22312;&#22270;&#32467;&#26500;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#38754;&#20020;&#30528;&#25361;&#25112;&#65292;&#22240;&#20026;&#22270;&#22686;&#24378;&#25805;&#20316;&#25913;&#21464;&#20102;&#35821;&#20041;&#24182;&#21576;&#29616;&#20986;&#21453;&#30452;&#35266;&#30340;&#24615;&#36136;&#12290;&#38024;&#23545;&#36825;&#19968;&#38480;&#21046;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#26174;&#24335;&#29983;&#25104;&#20851;&#31995;&#22270;&#65288;ExGRG&#65289;&#65292;&#20197;&#21462;&#20195;&#20165;&#20381;&#38752;&#20256;&#32479;&#30340;&#22522;&#20110;&#22686;&#24378;&#30340;&#38544;&#24335;&#20851;&#31995;&#22270;&#12290;ExGRG&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#20808;&#39564;&#39046;&#22495;&#30693;&#35782;&#21644;&#22312;&#32447;&#25552;&#21462;&#30340;&#20449;&#24687;&#32435;&#20837;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#19981;&#21464;&#24615;&#30446;&#26631;&#20013;&#65292;&#20511;&#37492;&#20102;&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#26144;&#23556;&#21644;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#12290;&#36890;&#36807;&#23558;&#33258;&#30417;&#30563;&#23398;&#20064;&#19982;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#32467;&#21512;&#65292;&#25105;&#20204;&#30340;E&#27493;&#39588;&#28041;&#21450;&#20851;&#31995;&#22270;&#30340;&#29983;&#25104;&#65292;&#20197;&#35782;&#21035;...
&lt;/p&gt;
&lt;p&gt;
Self-supervised Learning (SSL) has emerged as a powerful technique in pre-training deep learning models without relying on expensive annotated labels, instead leveraging embedded signals in unlabeled data. While SSL has shown remarkable success in computer vision tasks through intuitive data augmentation, its application to graph-structured data poses challenges due to the semantic-altering and counter-intuitive nature of graph augmentations. Addressing this limitation, this paper introduces a novel non-contrastive SSL approach to Explicitly Generate a compositional Relation Graph (ExGRG) instead of relying solely on the conventional augmentation-based implicit relation graph. ExGRG offers a framework for incorporating prior domain knowledge and online extracted information into the SSL invariance objective, drawing inspiration from the Laplacian Eigenmap and Expectation-Maximization (EM). Employing an EM perspective on SSL, our E-step involves relation graph generation to identify can
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#33104;&#36133;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;&#31163;&#32447;&#26041;&#27861;&#26469;&#22788;&#29702;&#25439;&#22351;&#30340;&#25968;&#25454;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#20551;&#35774;&#19979;&#20855;&#26377;&#24615;&#33021;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.06734</link><description>&lt;p&gt;
&#20855;&#26377;&#20154;&#31867;&#21453;&#39304;&#30340;&#25239;&#33104;&#36133;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Corruption Robust Offline Reinforcement Learning with Human Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06734
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#33104;&#36133;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;&#31163;&#32447;&#26041;&#27861;&#26469;&#22788;&#29702;&#25439;&#22351;&#30340;&#25968;&#25454;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#20551;&#35774;&#19979;&#20855;&#26377;&#24615;&#33021;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#31163;&#32447;&#29615;&#22659;&#20013;&#20855;&#26377;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#33104;&#36133;&#40065;&#26834;&#24615;&#38382;&#39064;&#12290;&#32473;&#23450;&#19968;&#32452;&#31163;&#32447;&#25968;&#25454;&#65292;&#20854;&#20013;&#21253;&#25324;&#36712;&#36857;&#23545;&#20197;&#21450;&#26377;&#20851;&#20154;&#31867;&#20559;&#22909;&#30340;&#21453;&#39304;&#65292;&#20854;&#20013;$\varepsilon$&#27604;&#20363;&#30340;&#36712;&#36857;&#23545;&#34987;&#25439;&#22351;&#65288;&#20363;&#22914;&#65292;&#21453;&#39304;&#32763;&#36716;&#25110;&#36712;&#36857;&#29305;&#24449;&#34987;&#25805;&#32437;&#65289;&#65292;&#20174;&#32780;&#25429;&#25417;&#21040;&#23545;&#25239;&#25915;&#20987;&#25110;&#22122;&#22768;&#20154;&#31867;&#20559;&#22909;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#26088;&#22312;&#35774;&#35745;&#31639;&#27861;&#65292;&#20174;&#25439;&#22351;&#30340;&#25968;&#25454;&#20013;&#35782;&#21035;&#20986;&#25509;&#36817;&#26368;&#20248;&#30340;&#31574;&#30053;&#65292;&#24182;&#19988;&#20855;&#22791;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#12290;&#29616;&#26377;&#30340;&#29702;&#35770;&#30740;&#31350;&#20998;&#21035;&#30740;&#31350;&#20102;&#33104;&#36133;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#65288;&#22312;&#33104;&#36133;&#19979;&#30452;&#25509;&#23398;&#20064;&#26631;&#37327;&#22870;&#21169;&#65289;&#21644;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;&#22312;&#27809;&#26377;&#33104;&#36133;&#30340;&#24773;&#20917;&#19979;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#65289;&#30340;&#35774;&#32622;&#65307;&#28982;&#32780;&#65292;&#23427;&#20204;&#24182;&#19981;&#36866;&#29992;&#20110;&#25105;&#20204;&#22788;&#29702;&#22312;&#31163;&#32447;&#29615;&#22659;&#20013;&#30340;&#25439;&#22351;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;&#22312;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#35206;&#30422;&#21508;&#31181;&#20551;&#35774;&#19979;&#20855;&#26377;&#33104;&#36133;&#40065;&#26834;&#24615;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#22312;&#39640;&#23618;&#27425;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#40065;&#26834;&#20142;&#28857;&#65292;&#24182;&#30830;&#20445;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#20551;&#35774;&#19979;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study data corruption robustness for reinforcement learning with human feedback (RLHF) in an offline setting. Given an offline dataset of pairs of trajectories along with feedback about human preferences, an $\varepsilon$-fraction of the pairs is corrupted (e.g., feedback flipped or trajectory features manipulated), capturing an adversarial attack or noisy human preferences. We aim to design algorithms that identify a near-optimal policy from the corrupted data, with provable guarantees. Existing theoretical works have separately studied the settings of corruption robust RL (learning from scalar rewards directly under corruption) and offline RLHF (learning from human feedback without corruption); however, they are inapplicable to our problem of dealing with corrupted data in offline RLHF setting. To this end, we design novel corruption robust offline RLHF methods under various assumptions on the coverage of the data-generating distributions. At a high level, our methodology robustif
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#22312;&#25552;&#20379;&#20219;&#21153;&#29305;&#23450;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#26159;&#21542;&#38656;&#35201;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;&#23545;&#20110;&#25351;&#23548;&#24615;LLMs&#30340;&#20849;&#35782;&#65292;&#24182;&#21457;&#29616;&#22312;&#26576;&#20123;&#20219;&#21153;&#20013;&#65292;&#19981;&#21516;&#30340;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#26041;&#27861;&#20250;&#20135;&#29983;&#36882;&#20943;&#30340;&#22238;&#25253;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;"&#24230;&#37327;&#26631;&#20934;"&#65292;&#29992;&#20110;&#34913;&#37327;&#20174;&#32473;&#23450;&#25351;&#20196;&#20013;&#23398;&#20064;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24110;&#21161;&#20915;&#23450;&#26159;&#21542;&#20248;&#21270;&#25351;&#20196;&#36824;&#26159;ICE&#29992;&#20110;&#20219;&#20309;&#26032;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.06733</link><description>&lt;p&gt;
NICE: &#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#36824;&#26159;&#19981;&#20248;&#21270;&#65311;
&lt;/p&gt;
&lt;p&gt;
NICE: To Optimize In-Context Examples or Not?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06733
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#22312;&#25552;&#20379;&#20219;&#21153;&#29305;&#23450;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#26159;&#21542;&#38656;&#35201;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;&#23545;&#20110;&#25351;&#23548;&#24615;LLMs&#30340;&#20849;&#35782;&#65292;&#24182;&#21457;&#29616;&#22312;&#26576;&#20123;&#20219;&#21153;&#20013;&#65292;&#19981;&#21516;&#30340;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#26041;&#27861;&#20250;&#20135;&#29983;&#36882;&#20943;&#30340;&#22238;&#25253;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;"&#24230;&#37327;&#26631;&#20934;"&#65292;&#29992;&#20110;&#34913;&#37327;&#20174;&#32473;&#23450;&#25351;&#20196;&#20013;&#23398;&#20064;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24110;&#21161;&#20915;&#23450;&#26159;&#21542;&#20248;&#21270;&#25351;&#20196;&#36824;&#26159;ICE&#29992;&#20110;&#20219;&#20309;&#26032;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#65288;ICE&#65289;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#20551;&#35774;&#22312;&#25552;&#31034;&#20449;&#24687;&#20013;&#35201;&#20040;&#26159;&#22266;&#23450;&#30340;&#65292;&#35201;&#20040;&#27809;&#26377;&#25552;&#20379;&#25351;&#20196;&#65292;&#23548;&#33268;&#20102;&#19968;&#20010;&#34920;&#38754;&#19978;&#30340;&#20849;&#35782;&#65306;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#23545;&#20110;&#25552;&#39640;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#38024;&#23545;&#32463;&#36807;&#25351;&#23548;&#30340;LLMs&#25361;&#25112;&#36825;&#19968;&#20849;&#35782;&#65292;&#30740;&#31350;&#22312;&#25552;&#20379;&#20102;&#20219;&#21153;&#29305;&#23450;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#26159;&#21542;&#24517;&#35201;&#65292;&#24182;&#21457;&#29616;&#26377;&#19968;&#20123;&#20219;&#21153;&#23545;&#20110;&#19981;&#21516;&#30340;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#26041;&#27861;&#20135;&#29983;&#36882;&#20943;&#30340;&#22238;&#25253;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20219;&#21153;&#29305;&#23450;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#31216;&#20026;"&#24230;&#37327;&#26631;&#20934;"&#65288;Metric&#65289;&#65292;&#29992;&#20110;&#37327;&#21270;&#20174;&#32473;&#23450;&#25351;&#20196;&#20013;&#23398;&#20064;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24110;&#21161;&#20915;&#23450;&#26159;&#21542;&#20248;&#21270;&#25351;&#20196;&#36824;&#26159;ICE&#29992;&#20110;&#20219;&#20309;&#26032;&#20219;&#21153;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;&#20219;&#21153;&#21644;&#36880;&#27493;&#22686;&#21152;&#30340;&#25351;&#20196;&#38598;&#30340;&#31995;&#32479;&#24615;&#30740;&#31350;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#35813;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have shown that large language models (LLMs) work remarkably well on a wide range of tasks through in-context learning and optimization of in-context examples (ICE). However, most of these studies assume either a fixed or no instruction provided in the prompt, leading to the apparent consensus that the optimization of in-context examples is critical for better performance. We challenge this consensus for instruction-tuned LLMs by investigating the necessity of optimizing in-context examples when task-specific instructions are provided, and find that there are tasks for which various ways of optimizing in-context examples yield diminishing returns. We introduce a task-specific metric called \metriclong{} (\metric) that quantifies the learnability of tasks from a given instruction, and provides a heuristic that helps decide whether to optimize for instructions or ICE for any new task. On a wide range of tasks and a systematically created instruction set with gradually added 
&lt;/p&gt;</description></item><item><title>&#21160;&#24577;&#22270;&#20449;&#24687;&#29942;&#39048;&#26694;&#26550;&#65288;DGIB&#65289;&#33021;&#22815;&#23398;&#20064;&#40065;&#26834;&#19988;&#26377;&#21306;&#20998;&#24615;&#30340;&#21160;&#24577;&#22270;&#34920;&#31034;&#12290;&#21033;&#29992;&#20449;&#24687;&#29942;&#39048;&#21407;&#29702;&#65292;&#36890;&#36807;&#36845;&#20195;&#24341;&#23548;&#21644;&#25913;&#36827;&#22270;&#24555;&#29031;&#20256;&#36882;&#30340;&#32467;&#26500;&#21644;&#29305;&#24449;&#20449;&#24687;&#27969;&#65292;&#21387;&#32553;&#20887;&#20313;&#20449;&#24687;&#24182;&#20445;&#30041;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;&#35813;&#26694;&#26550;&#33021;&#28385;&#36275;&#26368;&#23567;-&#20840;&#23616;-&#19968;&#33268;&#26465;&#20214;&#65292;&#25552;&#39640;&#20102;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06716</link><description>&lt;p&gt;
&#21160;&#24577;&#22270;&#20449;&#24687;&#29942;&#39048;
&lt;/p&gt;
&lt;p&gt;
Dynamic Graph Information Bottleneck
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06716
&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#22270;&#20449;&#24687;&#29942;&#39048;&#26694;&#26550;&#65288;DGIB&#65289;&#33021;&#22815;&#23398;&#20064;&#40065;&#26834;&#19988;&#26377;&#21306;&#20998;&#24615;&#30340;&#21160;&#24577;&#22270;&#34920;&#31034;&#12290;&#21033;&#29992;&#20449;&#24687;&#29942;&#39048;&#21407;&#29702;&#65292;&#36890;&#36807;&#36845;&#20195;&#24341;&#23548;&#21644;&#25913;&#36827;&#22270;&#24555;&#29031;&#20256;&#36882;&#30340;&#32467;&#26500;&#21644;&#29305;&#24449;&#20449;&#24687;&#27969;&#65292;&#21387;&#32553;&#20887;&#20313;&#20449;&#24687;&#24182;&#20445;&#30041;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;&#35813;&#26694;&#26550;&#33021;&#28385;&#36275;&#26368;&#23567;-&#20840;&#23616;-&#19968;&#33268;&#26465;&#20214;&#65292;&#25552;&#39640;&#20102;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#22270;&#24191;&#27867;&#23384;&#22312;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#23427;&#20204;&#25658;&#24102;&#30528;&#22797;&#26434;&#30340;&#26102;&#31354;&#29305;&#24449;&#27169;&#24335;&#65292;&#23545;&#20110;&#23427;&#20204;&#30340;&#34920;&#31034;&#23398;&#20064;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DGNNs&#65289;&#36890;&#36807;&#21033;&#29992;&#20869;&#22312;&#30340;&#21160;&#24577;&#24615;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;DGNNs&#23637;&#31034;&#20102;&#26377;&#38480;&#30340;&#40065;&#26834;&#24615;&#65292;&#26131;&#21463;&#23545;&#25239;&#25915;&#20987;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21160;&#24577;&#22270;&#20449;&#24687;&#29942;&#39048;&#65288;DGIB&#65289;&#26694;&#26550;&#26469;&#23398;&#20064;&#40065;&#26834;&#19988;&#26377;&#21306;&#20998;&#24615;&#30340;&#34920;&#31034;&#12290;&#20511;&#21161;&#20449;&#24687;&#29942;&#39048;&#65288;IB&#65289;&#21407;&#29702;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#26399;&#26395;&#30340;&#26368;&#20248;&#34920;&#31034;&#24212;&#28385;&#36275;&#26368;&#23567;-&#20840;&#23616;-&#19968;&#33268;&#65288;MSC&#65289;&#26465;&#20214;&#12290;&#20026;&#20102;&#22312;&#28508;&#22312;&#34920;&#31034;&#20013;&#21387;&#32553;&#20887;&#20313;&#20449;&#24687;&#21644;&#20445;&#30041;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#65292;DGIB&#36845;&#20195;&#22320;&#24341;&#23548;&#21644;&#25913;&#36827;&#36890;&#36807;&#22270;&#24555;&#29031;&#20256;&#36882;&#30340;&#32467;&#26500;&#21644;&#29305;&#24449;&#20449;&#24687;&#27969;&#12290;&#20026;&#20102;&#28385;&#36275;MSC&#26465;&#20214;&#65292;&#25105;&#20204;&#23558;&#25972;&#20307;IB&#30446;&#26631;&#20998;&#35299;&#20026;DGIB$_{MS}$&#21644;DGIB$_C$&#65292;&#20854;&#20013;DGIB$_{MS}$&#36890;&#36947;&#30340;&#30446;&#26631;&#26159;...
&lt;/p&gt;
&lt;p&gt;
Dynamic Graphs widely exist in the real world, which carry complicated spatial and temporal feature patterns, challenging their representation learning. Dynamic Graph Neural Networks (DGNNs) have shown impressive predictive abilities by exploiting the intrinsic dynamics. However, DGNNs exhibit limited robustness, prone to adversarial attacks. This paper presents the novel Dynamic Graph Information Bottleneck (DGIB) framework to learn robust and discriminative representations. Leveraged by the Information Bottleneck (IB) principle, we first propose the expected optimal representations should satisfy the Minimal-Sufficient-Consensual (MSC) Condition. To compress redundant as well as conserve meritorious information into latent representation, DGIB iteratively directs and refines the structural and feature information flow passing through graph snapshots. To meet the MSC Condition, we decompose the overall IB objectives into DGIB$_{MS}$ and DGIB$_C$, in which the DGIB$_{MS}$ channel aims 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29109;&#27491;&#21017;&#21270;&#30340;&#20196;&#29260;&#32423;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65288;ETPO&#65289;&#65292;&#29992;&#20110;&#20248;&#21270;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#36890;&#36807;&#30452;&#25509;&#19982;&#20219;&#21153;&#29305;&#23450;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#35299;&#20915;&#22312;&#22914;&#20309;&#20998;&#37197;&#20196;&#29260;&#32423;&#23398;&#20998;&#21644;&#26368;&#22823;&#21270;&#22870;&#21169;&#20043;&#38388;&#30340;&#20914;&#31361;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.06700</link><description>&lt;p&gt;
&#29109;&#27491;&#21017;&#21270;&#30340;&#20196;&#29260;&#32423;&#31574;&#30053;&#20248;&#21270;&#29992;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Entropy-Regularized Token-Level Policy Optimization for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06700
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29109;&#27491;&#21017;&#21270;&#30340;&#20196;&#29260;&#32423;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65288;ETPO&#65289;&#65292;&#29992;&#20110;&#20248;&#21270;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#36890;&#36807;&#30452;&#25509;&#19982;&#20219;&#21153;&#29305;&#23450;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#35299;&#20915;&#22312;&#22914;&#20309;&#20998;&#37197;&#20196;&#29260;&#32423;&#23398;&#20998;&#21644;&#26368;&#22823;&#21270;&#22870;&#21169;&#20043;&#38388;&#30340;&#20914;&#31361;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20132;&#20114;&#24335;&#20915;&#31574;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26234;&#33021;&#20195;&#29702;&#30340;&#28508;&#21147;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#12289;&#39640;&#36136;&#37327;&#30340;&#31034;&#20363;&#25110;&#39069;&#22806;&#30340;&#22870;&#21169;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#30417;&#30563;&#24494;&#35843;&#25110;RLHF&#12290;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#21160;&#24577;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;LLMs&#33021;&#22815;&#36890;&#36807;&#30452;&#25509;&#19982;&#20219;&#21153;&#29305;&#23450;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#26469;&#20811;&#26381;&#36825;&#20123;&#20381;&#36182;&#20851;&#31995;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#23427;&#38754;&#20020;&#30528;&#37325;&#37325;&#22256;&#38590;&#65306;1&#65289;&#30001;&#20110;&#24040;&#22823;&#30340;&#21160;&#20316;&#31354;&#38388;&#38656;&#35201;&#25506;&#32034;&#32780;&#20135;&#29983;&#30340;&#19981;&#31283;&#23450;&#24615;&#65307;2&#65289;&#22522;&#20110;&#21160;&#20316;&#32423;&#22870;&#21169;&#20449;&#21495;&#20998;&#37197;&#20196;&#29260;&#32423;&#23398;&#20998;&#30340;&#25361;&#25112;&#65292;&#23548;&#33268;&#26368;&#22823;&#21270;&#22870;&#21169;&#21644;&#20934;&#30830;&#24314;&#27169;&#35821;&#26009;&#24211;&#25968;&#25454;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29109;&#27491;&#21017;&#21270;&#30340;&#20196;&#29260;&#32423;&#31574;&#30053;&#20248;&#21270;&#65288;ETPO&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#20026;&#22312;&#20196;&#29260;&#32423;&#20248;&#21270;LLMs&#32780;&#35774;&#35745;&#30340;&#29109;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;ETPO&#30340;&#26680;&#24515;&#26159;&#25105;&#20204;&#30340;&#19968;&#31181;&#26032;&#39062;&#30340;&#36880;&#20196;&#29260;&#36719;Bellman&#26356;&#26032;&#31639;&#27861;&#65292;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown promise as intelligent agents in interactive decision-making tasks. Traditional approaches often depend on meticulously designed prompts, high-quality examples, or additional reward models for in-context learning, supervised fine-tuning, or RLHF. Reinforcement learning (RL) presents a dynamic alternative for LLMs to overcome these dependencies by engaging directly with task-specific environments. Nonetheless, it faces significant hurdles: 1) instability stemming from the exponentially vast action space requiring exploration; 2) challenges in assigning token-level credit based on action-level reward signals, resulting in discord between maximizing rewards and accurately modeling corpus data. In response to these challenges, we introduce Entropy-Regularized Token-level Policy Optimization (ETPO), an entropy-augmented RL method tailored for optimizing LLMs at the token level. At the heart of ETPO is our novel per-token soft Bellman update, designed 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#23558;&#35757;&#32451;&#30340;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;(ReLU)&#31070;&#32463;&#20803;&#20316;&#20026;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;(MIP)&#30340;&#24418;&#24335;&#65292;&#24182;&#23558;MIP&#27169;&#22411;&#24212;&#29992;&#20110;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#12290;&#30740;&#31350;&#21457;&#29616;MIP&#25216;&#26415;&#22312;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#28508;&#21147;&#65292;&#21253;&#25324;&#20108;&#36827;&#21046;DNN&#21644;&#20108;&#20540;&#21270;DNN&#12290;</title><link>https://arxiv.org/abs/2402.06697</link><description>&lt;p&gt;
&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Feed-Forward Neural Networks as a Mixed-Integer Program
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06697
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#23558;&#35757;&#32451;&#30340;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;(ReLU)&#31070;&#32463;&#20803;&#20316;&#20026;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;(MIP)&#30340;&#24418;&#24335;&#65292;&#24182;&#23558;MIP&#27169;&#22411;&#24212;&#29992;&#20110;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#12290;&#30740;&#31350;&#21457;&#29616;MIP&#25216;&#26415;&#22312;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#28508;&#21147;&#65292;&#21253;&#25324;&#20108;&#36827;&#21046;DNN&#21644;&#20108;&#20540;&#21270;DNN&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#22312;&#21508;&#20010;&#24212;&#29992;&#39046;&#22495;&#37117;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;DNN&#30001;&#31070;&#32463;&#20803;&#23618;&#32452;&#25104;&#65292;&#35745;&#31639;&#20223;&#23556;&#32452;&#21512;&#65292;&#24212;&#29992;&#38750;&#32447;&#24615;&#25805;&#20316;&#65292;&#24182;&#20135;&#29983;&#30456;&#24212;&#30340;&#28608;&#27963;&#12290;&#20462;&#27491;&#30340;&#32447;&#24615;&#21333;&#20803;(ReLU)&#26159;&#19968;&#31181;&#20856;&#22411;&#30340;&#38750;&#32447;&#24615;&#36816;&#31639;&#31526;&#65292;&#36755;&#20986;&#20854;&#36755;&#20837;&#21644;&#38646;&#30340;&#26368;&#22823;&#20540;&#12290;&#22312;&#20687;&#26368;&#22823;&#27744;&#21270;&#36825;&#26679;&#28041;&#21450;&#22810;&#20010;&#36755;&#20837;&#20540;&#30340;&#22330;&#26223;&#20013;&#65292;&#22266;&#23450;&#21442;&#25968;&#30340;DNN&#21487;&#20197;&#34987;&#24314;&#27169;&#20026;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;(MIP)&#12290;&#36825;&#31181;&#24418;&#24335;&#65292;&#20351;&#29992;&#36830;&#32493;&#21464;&#37327;&#34920;&#31034;&#21333;&#20803;&#36755;&#20986;&#21644;ReLU&#28608;&#27963;&#30340;&#20108;&#36827;&#21046;&#21464;&#37327;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#25214;&#21040;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35757;&#32451;&#30340;ReLU&#31070;&#32463;&#20803;&#20316;&#20026;MIP&#30340;&#24418;&#24335;&#65292;&#24182;&#23558;MIP&#27169;&#22411;&#24212;&#29992;&#20110;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;(NN)&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#30740;&#31350;&#20102;MIP&#25216;&#26415;&#21644;&#19981;&#21516;&#30340;NN&#26550;&#26500;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#21253;&#25324;&#20108;&#36827;&#21046;DNN(&#37319;&#29992;&#38454;&#26799;&#28608;&#27963;&#20989;&#25968;)&#21644;&#20108;&#20540;&#21270;DNN(&#26435;&#37325;&#21644;&#28608;&#27963;&#38480;&#21046;&#20026;$-1,0,+1$)&#12290;&#35813;&#30740;&#31350;&#37325;&#28857;&#20851;&#27880;&#35757;&#32451;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) are widely studied in various applications. A DNN consists of layers of neurons that compute affine combinations, apply nonlinear operations, and produce corresponding activations. The rectified linear unit (ReLU) is a typical nonlinear operator, outputting the max of its input and zero. In scenarios like max pooling, where multiple input values are involved, a fixed-parameter DNN can be modeled as a mixed-integer program (MIP). This formulation, with continuous variables representing unit outputs and binary variables for ReLU activation, finds applications across diverse domains. This study explores the formulation of trained ReLU neurons as MIP and applies MIP models for training neural networks (NNs). Specifically, it investigates interactions between MIP techniques and various NN architectures, including binary DNNs (employing step activation functions) and binarized DNNs (with weights and activations limited to $-1,0,+1$). The research focuses on traini
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FL-NAS&#30340;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#22312;&#27169;&#22411;&#20934;&#30830;&#24615;&#12289;&#20844;&#24179;&#24615;&#21644;&#30828;&#20214;&#37096;&#32626;&#25928;&#29575;&#19977;&#20010;&#26041;&#38754;&#36798;&#21040;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.06696</link><description>&lt;p&gt;
FL-NAS: &#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#23454;&#29616;&#20844;&#24179;&#30340;NAS
&lt;/p&gt;
&lt;p&gt;
FL-NAS: Towards Fairness of NAS for Resource Constrained Devices via Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FL-NAS&#30340;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#22312;&#27169;&#22411;&#20934;&#30830;&#24615;&#12289;&#20844;&#24179;&#24615;&#21644;&#30828;&#20214;&#37096;&#32626;&#25928;&#29575;&#19977;&#20010;&#26041;&#38754;&#36798;&#21040;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#24050;&#25104;&#20026;&#24037;&#19994;&#30028;&#33258;&#21160;&#35774;&#35745;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26631;&#20934;&#24037;&#20855;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#30340;&#31227;&#21160;&#21644;&#36793;&#32536;&#35774;&#22791;&#39537;&#21160;&#30340;&#21508;&#31181;&#24212;&#29992;&#12290;&#26368;&#36817;&#65292;&#30001;&#20110;&#20854;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20063;&#34987;&#32435;&#20837;NAS&#65292;&#24182;&#26174;&#31034;&#20986;&#19968;&#20123;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#27169;&#22411;&#20934;&#30830;&#24615;&#12289;&#20844;&#24179;&#24615;&#21644;&#30828;&#20214;&#37096;&#32626;&#25928;&#29575;&#19977;&#20010;&#37325;&#35201;&#30340;&#35774;&#35745;&#25351;&#26631;&#65292;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;&#36825;&#20010;&#26041;&#21521;&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;NAS&#26694;&#26550;FL-NAS&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;FL-NAS&#30830;&#23454;&#33021;&#22815;&#25214;&#21040;&#24615;&#33021;&#20248;&#31168;&#30340;DNN&#27169;&#22411;&#65292;&#20960;&#20046;&#22312;&#25152;&#26377;&#35774;&#35745;&#32771;&#34385;&#26041;&#38754;&#37117;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;DNN&#27169;&#22411;&#26377;&#30528;&#25968;&#37327;&#32423;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Architecture Search (NAS) has become the de fecto tools in the industry in automating the design of deep neural networks for various applications, especially those driven by mobile and edge devices with limited computing resources. The emerging large language models (LLMs), due to their prowess, have also been incorporated into NAS recently and show some promising results. This paper conducts further exploration in this direction by considering three important design metrics simultaneously, i.e., model accuracy, fairness, and hardware deployment efficiency. We propose a novel LLM-based NAS framework, FL-NAS, in this paper, and show experimentally that FL-NAS can indeed find high-performing DNNs, beating state-of-the-art DNN models by orders-of-magnitude across almost all design considerations.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#38598;&#25104;&#31995;&#32479;&#65292;&#36890;&#36807;&#23558;&#22522;&#20110;&#29289;&#29702;&#30340;&#35786;&#26029;&#24037;&#20855;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#24110;&#21161;&#22797;&#26434;&#31995;&#32479;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#25925;&#38556;&#35786;&#26029;&#12290;&#35813;&#31995;&#32479;&#19981;&#20165;&#33021;&#22815;&#35782;&#21035;&#25925;&#38556;&#65292;&#36824;&#33021;&#22815;&#25552;&#20379;&#28165;&#26224;&#26131;&#25026;&#30340;&#25925;&#38556;&#21407;&#22240;&#21644;&#24433;&#21709;&#35299;&#37322;&#65292;&#25552;&#39640;&#33258;&#20027;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#21644;&#36879;&#26126;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.06695</link><description>&lt;p&gt;
&#22312;&#22797;&#26434;&#31995;&#32479;&#20013;&#38598;&#25104;LLMs&#20197;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#25925;&#38556;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Integrating LLMs for Explainable Fault Diagnosis in Complex Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#38598;&#25104;&#31995;&#32479;&#65292;&#36890;&#36807;&#23558;&#22522;&#20110;&#29289;&#29702;&#30340;&#35786;&#26029;&#24037;&#20855;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#24110;&#21161;&#22797;&#26434;&#31995;&#32479;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#25925;&#38556;&#35786;&#26029;&#12290;&#35813;&#31995;&#32479;&#19981;&#20165;&#33021;&#22815;&#35782;&#21035;&#25925;&#38556;&#65292;&#36824;&#33021;&#22815;&#25552;&#20379;&#28165;&#26224;&#26131;&#25026;&#30340;&#25925;&#38556;&#21407;&#22240;&#21644;&#24433;&#21709;&#35299;&#37322;&#65292;&#25552;&#39640;&#33258;&#20027;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#21644;&#36879;&#26126;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#38598;&#25104;&#31995;&#32479;&#65292;&#26088;&#22312;&#22686;&#24378;&#22797;&#26434;&#31995;&#32479;&#20013;&#25925;&#38556;&#35786;&#26029;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#22914;&#26680;&#30005;&#31449;&#65292;&#22312;&#36825;&#20123;&#31995;&#32479;&#20013;&#65292;&#25805;&#20316;&#21592;&#30340;&#29702;&#35299;&#23545;&#20110;&#26126;&#26234;&#30340;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#23558;&#22522;&#20110;&#29289;&#29702;&#30340;&#35786;&#26029;&#24037;&#20855;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#19981;&#20165;&#33021;&#22815;&#35782;&#21035;&#25925;&#38556;&#65292;&#36824;&#33021;&#28165;&#26224;&#26131;&#25026;&#22320;&#35299;&#37322;&#20854;&#21407;&#22240;&#21644;&#24433;&#21709;&#12290;&#36890;&#36807;&#24212;&#29992;&#20110;&#29076;&#30416;&#35774;&#26045;&#65292;&#23637;&#31034;&#20102;&#35813;&#31995;&#32479;&#25581;&#31034;&#20102;&#35786;&#26029;&#25925;&#38556;&#19982;&#20256;&#24863;&#22120;&#25968;&#25454;&#20043;&#38388;&#30340;&#32852;&#31995;&#12289;&#22238;&#31572;&#25805;&#20316;&#21592;&#30340;&#26597;&#35810;&#20197;&#21450;&#35780;&#20272;&#21382;&#21490;&#20256;&#24863;&#22120;&#24322;&#24120;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24378;&#35843;&#20102;&#23558;&#22522;&#20110;&#27169;&#22411;&#30340;&#35786;&#26029;&#19982;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#33258;&#20027;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#21644;&#36879;&#26126;&#24230;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces an integrated system designed to enhance the explainability of fault diagnostics in complex systems, such as nuclear power plants, where operator understanding is critical for informed decision-making. By combining a physics-based diagnostic tool with a Large Language Model, we offer a novel solution that not only identifies faults but also provides clear, understandable explanations of their causes and implications. The system's efficacy is demonstrated through application to a molten salt facility, showcasing its ability to elucidate the connections between diagnosed faults and sensor data, answer operator queries, and evaluate historical sensor anomalies. Our approach underscores the importance of merging model-based diagnostics with advanced AI to improve the reliability and transparency of autonomous systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#25552;&#21319;&#25112;&#20105;&#28216;&#25103;&#20013;&#26234;&#33021;&#20195;&#29702;&#30340;&#24615;&#33021;&#65292;&#20197;&#21152;&#36895;&#20915;&#31574;&#36895;&#24230;&#21644;&#25552;&#39640;&#20915;&#31574;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.06694</link><description>&lt;p&gt;
&#25193;&#23637;&#25112;&#20105;&#28216;&#25103;&#20013;&#30340;&#26234;&#33021;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Scaling Intelligent Agents in Combat Simulations for Wargaming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06694
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#25552;&#21319;&#25112;&#20105;&#28216;&#25103;&#20013;&#26234;&#33021;&#20195;&#29702;&#30340;&#24615;&#33021;&#65292;&#20197;&#21152;&#36895;&#20915;&#31574;&#36895;&#24230;&#21644;&#25552;&#39640;&#20915;&#31574;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#26410;&#26469;&#19982;&#25216;&#26415;&#20808;&#36827;&#30340;&#31454;&#20105;&#23545;&#25163;&#30340;&#20914;&#31361;&#20013;&#20445;&#25345;&#31454;&#20105;&#21147;&#65292;&#25105;&#20204;&#38656;&#35201;&#21152;&#36895;&#30740;&#31350;&#21644;&#24320;&#21457;&#25112;&#20105;&#28216;&#25103;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#24320;&#21457;&#26234;&#33021;&#25112;&#26007;&#34892;&#20026;&#23558;&#25104;&#20026;&#26410;&#26469;&#23454;&#29616;&#36229;&#20154;&#31867;&#27700;&#24179;&#34920;&#29616;&#30340;&#20851;&#38190;&#65292;&#25552;&#21319;&#25105;&#20204;&#22312;&#26410;&#26469;&#25112;&#20105;&#20013;&#30340;&#20915;&#31574;&#36136;&#37327;&#21644;&#21152;&#36895;&#36895;&#24230;&#12290;&#23613;&#31649;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#28216;&#25103;&#20013;&#26234;&#33021;&#20195;&#29702;&#34892;&#20026;&#24320;&#21457;&#26041;&#38754;&#32487;&#32493;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#22312;&#38271;&#26399;&#12289;&#22797;&#26434;&#30340;&#20219;&#21153;&#20013;&#65292;&#29305;&#21035;&#26159;&#22312;&#25112;&#26007;&#24314;&#27169;&#21644;&#20223;&#30495;&#20013;&#65292;&#23427;&#23578;&#26410;&#36798;&#21040;&#25110;&#36229;&#36807;&#20154;&#31867;&#27700;&#24179;&#12290;&#20511;&#37492;RL&#30340;&#24050;&#35777;&#26126;&#28508;&#21147;&#21644;&#26368;&#36817;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#65288;HRL&#65289;&#30340;&#25104;&#21151;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#27491;&#22312;&#25506;&#32034;&#24182;&#25193;&#23637;HRL&#30340;&#20351;&#29992;&#65292;&#20197;&#21019;&#24314;&#33021;&#22815;&#22312;&#36825;&#20123;&#22823;&#35268;&#27169;&#22797;&#26434;&#27169;&#25311;&#29615;&#22659;&#20013;&#26377;&#25928;&#25191;&#34892;&#30340;&#26234;&#33021;&#20195;&#29702;&#12290;&#25105;&#20204;&#30340;&#26368;&#32456;&#30446;&#26631;&#26159;
&lt;/p&gt;
&lt;p&gt;
Remaining competitive in future conflicts with technologically-advanced competitors requires us to accelerate our research and development in artificial intelligence (AI) for wargaming. More importantly, leveraging machine learning for intelligent combat behavior development will be key to one day achieving superhuman performance in this domain--elevating the quality and accelerating the speed of our decisions in future wars. Although deep reinforcement learning (RL) continues to show promising results in intelligent agent behavior development in games, it has yet to perform at or above the human level in the long-horizon, complex tasks typically found in combat modeling and simulation. Capitalizing on the proven potential of RL and recent successes of hierarchical reinforcement learning (HRL), our research is investigating and extending the use of HRL to create intelligent agents capable of performing effectively in these large and complex simulation environments. Our ultimate goal is
&lt;/p&gt;</description></item><item><title>HistoHDR-Net&#26159;&#19968;&#31181;&#29992;&#20110;&#21333;&#20010;LDR&#21040;HDR&#22270;&#20687;&#36716;&#25442;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#30452;&#26041;&#22270;&#22343;&#34913;&#21270;&#30340;LDR&#22270;&#20687;&#21644;&#33258;&#27880;&#24847;&#21147;&#24341;&#23548;&#65292;&#24674;&#22797;HDR&#22270;&#20687;&#30340;&#32454;&#33410;&#12290;</title><link>https://arxiv.org/abs/2402.06692</link><description>&lt;p&gt;
HistoHDR-Net&#65306;&#29992;&#20110;&#21333;&#20010;LDR&#21040;HDR&#22270;&#20687;&#36716;&#25442;&#30340;&#30452;&#26041;&#22270;&#22343;&#34913;&#21270;
&lt;/p&gt;
&lt;p&gt;
HistoHDR-Net: Histogram Equalization for Single LDR to HDR Image Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06692
&lt;/p&gt;
&lt;p&gt;
HistoHDR-Net&#26159;&#19968;&#31181;&#29992;&#20110;&#21333;&#20010;LDR&#21040;HDR&#22270;&#20687;&#36716;&#25442;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#30452;&#26041;&#22270;&#22343;&#34913;&#21270;&#30340;LDR&#22270;&#20687;&#21644;&#33258;&#27880;&#24847;&#21147;&#24341;&#23548;&#65292;&#24674;&#22797;HDR&#22270;&#20687;&#30340;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#21160;&#24577;&#33539;&#22260;&#65288;HDR&#65289;&#25104;&#20687;&#26088;&#22312;&#22797;&#21046;&#30495;&#23454;&#22330;&#26223;&#30340;&#39640;&#35270;&#35273;&#36136;&#37327;&#21644;&#28165;&#26224;&#24230;&#12290;&#30001;&#20110;HDR&#25104;&#20687;&#30340;&#39640;&#25104;&#26412;&#65292;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#21508;&#31181;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#29992;&#20110;&#20174;&#20302;&#21160;&#24577;&#33539;&#22260;&#65288;LDR&#65289;&#22270;&#20687;&#37325;&#26500;HDR&#22270;&#20687;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#19968;&#20010;&#24120;&#35265;&#38480;&#21046;&#26159;&#22312;&#37325;&#26500;&#30340;HDR&#22270;&#20687;&#20013;&#32570;&#22833;&#30340;&#32454;&#33410;&#65292;&#36825;&#20123;&#32454;&#33410;&#22312;&#36755;&#20837;&#30340;LDR&#22270;&#20687;&#20013;&#36807;&#26333;&#25110;&#26333;&#20809;&#19981;&#36275;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;HistoHDR-Net&#65292;&#36890;&#36807;&#34701;&#21512;&#22522;&#20110;&#30452;&#26041;&#22270;&#22343;&#34913;&#30340;LDR&#22270;&#20687;&#21644;&#33258;&#27880;&#24847;&#21147;&#24341;&#23548;&#65292;&#24674;&#22797;HDR&#22270;&#20687;&#30340;&#32454;&#33410;&#65288;&#20363;&#22914;&#39068;&#33394;&#65292;&#23545;&#27604;&#24230;&#65292;&#39281;&#21644;&#24230;&#21644;&#20142;&#24230;&#65289;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#29616;&#26377;&#26041;&#27861;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
High Dynamic Range (HDR) imaging aims to replicate the high visual quality and clarity of real-world scenes. Due to the high costs associated with HDR imaging, the literature offers various data-driven methods for HDR image reconstruction from Low Dynamic Range (LDR) counterparts. A common limitation of these approaches is missing details in regions of the reconstructed HDR images, which are over- or under-exposed in the input LDR images. To this end, we propose a simple and effective method, HistoHDR-Net, to recover the fine details (e.g., color, contrast, saturation, and brightness) of HDR images via a fusion-based approach utilizing histogram-equalized LDR images along with self-attention guidance. Our experiments demonstrate the efficacy of the proposed approach over the state-of-art methods.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#30340;&#31169;&#20154;&#30693;&#35782;&#20849;&#20139;&#30340;&#20840;&#38754;&#35843;&#26597;&#65292;&#20998;&#26512;&#20102;&#22312;&#39046;&#20808;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#26550;&#26500;&#20013;&#20351;&#29992;&#30340;&#21508;&#31181;&#30693;&#35782;&#32452;&#20214;&#65292;&#26088;&#22312;&#25581;&#31034;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#27861;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.06682</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#30340;&#31169;&#20154;&#30693;&#35782;&#20849;&#20139;&#65306;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Private Knowledge Sharing in Distributed Learning: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06682
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#30340;&#31169;&#20154;&#30693;&#35782;&#20849;&#20139;&#30340;&#20840;&#38754;&#35843;&#26597;&#65292;&#20998;&#26512;&#20102;&#22312;&#39046;&#20808;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#26550;&#26500;&#20013;&#20351;&#29992;&#30340;&#21508;&#31181;&#30693;&#35782;&#32452;&#20214;&#65292;&#26088;&#22312;&#25581;&#31034;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#27861;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#23835;&#36215;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#35768;&#22810;&#34892;&#19994;&#65292;&#24182;&#25913;&#21464;&#20102;&#31038;&#20250;&#30340;&#36816;&#20316;&#26041;&#24335;&#12290;&#20854;&#24191;&#27867;&#20351;&#29992;&#23548;&#33268;&#20102;AI&#21644;&#20854;&#24213;&#23618;&#25968;&#25454;&#22312;&#35768;&#22810;&#26234;&#33021;&#31995;&#32479;&#20013;&#30340;&#20998;&#24067;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#20998;&#24067;&#24335;&#25110;&#30001;&#19981;&#21516;&#23454;&#20307;&#25317;&#26377;&#30340;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#20449;&#24687;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#29616;&#20195;&#25968;&#25454;&#39537;&#21160;&#30340;&#26381;&#21153;&#24050;&#32463;&#24320;&#21457;&#20986;&#23558;&#20998;&#24067;&#24335;&#30693;&#35782;&#23454;&#20307;&#25972;&#21512;&#21040;&#20854;&#32467;&#26524;&#20013;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#26368;&#26032;&#30340;AI&#27169;&#22411;&#32463;&#24120;&#34987;&#20197;&#20998;&#25955;&#24335;&#30340;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#12290;&#20998;&#24067;&#24335;&#23398;&#20064;&#28041;&#21450;&#22810;&#20010;&#23454;&#20307;&#20849;&#21516;&#36827;&#34892;&#39044;&#27979;&#21644;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#21512;&#20316;&#20063;&#21487;&#33021;&#24102;&#26469;&#23433;&#20840;&#28431;&#27934;&#21644;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#30340;&#31169;&#20154;&#30693;&#35782;&#20849;&#20139;&#30340;&#28145;&#20837;&#35843;&#26597;&#65292;&#32771;&#23519;&#20102;&#22312;&#39046;&#20808;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#26550;&#26500;&#20013;&#20351;&#29992;&#30340;&#21508;&#31181;&#30693;&#35782;&#32452;&#20214;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#29616;&#26377;&#26041;&#27861;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of Artificial Intelligence (AI) has revolutionized numerous industries and transformed the way society operates. Its widespread use has led to the distribution of AI and its underlying data across many intelligent systems. In this light, it is crucial to utilize information in learning processes that are either distributed or owned by different entities. As a result, modern data-driven services have been developed to integrate distributed knowledge entities into their outcomes. In line with this goal, the latest AI models are frequently trained in a decentralized manner. Distributed learning involves multiple entities working together to make collective predictions and decisions. However, this collaboration can also bring about security vulnerabilities and challenges. This paper provides an in-depth survey on private knowledge sharing in distributed learning, examining various knowledge components utilized in leading distributed learning architectures. Our analysis sheds light
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;SPDiff&#30340;&#22522;&#20110;&#31038;&#20250;&#29289;&#29702;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#20154;&#32676;&#27169;&#25311;&#12290;&#27169;&#22411;&#32508;&#21512;&#32771;&#34385;&#20102;&#20154;&#32676;&#30340;&#20114;&#21160;&#21644;&#21382;&#21490;&#20449;&#24687;&#65292;&#36890;&#36807;&#36870;&#21521;&#25193;&#25955;&#36807;&#31243;&#29983;&#25104;&#19979;&#19968;&#20010;&#26102;&#38388;&#27573;&#20869;&#34892;&#20154;&#36816;&#21160;&#30340;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#20511;&#37492;&#31038;&#20250;&#21147;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#20154;&#32676;&#20114;&#21160;&#30340;&#31561;&#21464;&#24615;&#23646;&#24615;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#38271;&#26399;&#27169;&#25311;&#20013;&#30340;&#35823;&#24046;&#32047;&#31215;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#22810;&#23618;&#27425;&#25193;&#25955;&#27169;&#22411;&#21644;&#26368;&#23567;&#20108;&#20056;&#27861;&#36827;&#34892;&#21442;&#25968;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.06680</link><description>&lt;p&gt;
&#22522;&#20110;&#31038;&#20250;&#29289;&#29702;&#30340;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#20154;&#32676;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Social Physics Informed Diffusion Model for Crowd Simulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;SPDiff&#30340;&#22522;&#20110;&#31038;&#20250;&#29289;&#29702;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#20154;&#32676;&#27169;&#25311;&#12290;&#27169;&#22411;&#32508;&#21512;&#32771;&#34385;&#20102;&#20154;&#32676;&#30340;&#20114;&#21160;&#21644;&#21382;&#21490;&#20449;&#24687;&#65292;&#36890;&#36807;&#36870;&#21521;&#25193;&#25955;&#36807;&#31243;&#29983;&#25104;&#19979;&#19968;&#20010;&#26102;&#38388;&#27573;&#20869;&#34892;&#20154;&#36816;&#21160;&#30340;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#20511;&#37492;&#31038;&#20250;&#21147;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#20154;&#32676;&#20114;&#21160;&#30340;&#31561;&#21464;&#24615;&#23646;&#24615;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#38271;&#26399;&#27169;&#25311;&#20013;&#30340;&#35823;&#24046;&#32047;&#31215;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#22810;&#23618;&#27425;&#25193;&#25955;&#27169;&#22411;&#21644;&#26368;&#23567;&#20108;&#20056;&#27861;&#36827;&#34892;&#21442;&#25968;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#32676;&#27169;&#25311;&#22312;&#22478;&#24066;&#35268;&#21010;&#12289;&#24314;&#31569;&#35774;&#35745;&#21644;&#20132;&#36890;&#23433;&#25490;&#31561;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#29289;&#29702;&#21551;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#20154;&#32676;&#27169;&#25311;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#26410;&#33021;&#20840;&#38754;&#24314;&#27169;&#20154;&#31867;&#36816;&#21160;&#30340;&#24322;&#36136;&#24615;&#21644;&#22810;&#27169;&#24577;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SPDiff&#30340;&#31038;&#20250;&#29289;&#29702;&#21551;&#21457;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#24357;&#34917;&#19978;&#36848;&#24046;&#36317;&#12290;SPDiff&#21516;&#26102;&#32771;&#34385;&#20102;&#24403;&#21069;&#26102;&#38388;&#27573;&#20869;&#20154;&#32676;&#30340;&#20114;&#21160;&#21644;&#21382;&#21490;&#20449;&#24687;&#65292;&#36890;&#36807;&#36870;&#21521;&#25193;&#25955;&#36807;&#31243;&#29983;&#25104;&#19979;&#19968;&#20010;&#26102;&#38388;&#27573;&#20869;&#34892;&#20154;&#36816;&#21160;&#30340;&#20998;&#24067;&#12290;&#21463;&#31038;&#20250;&#21147;&#27169;&#22411;&#65288;Social Force&#65289;&#20013;&#20154;&#32676;&#21160;&#21147;&#23398;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20154;&#32676;&#20114;&#21160;&#27169;&#22359;&#26469;&#25351;&#23548;&#21435;&#22122;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#20154;&#32676;&#20114;&#21160;&#30340;&#31561;&#21464;&#24615;&#23646;&#24615;&#36827;&#19968;&#27493;&#22686;&#24378;&#35813;&#27169;&#22359;&#12290;&#20026;&#20102;&#20943;&#36731;&#38271;&#26399;&#27169;&#25311;&#20013;&#30340;&#35823;&#24046;&#32047;&#31215;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#23618;&#27425;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#26368;&#23567;&#20108;&#20056;&#27861;&#36827;&#34892;&#27169;&#22411;&#21442;&#25968;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Crowd simulation holds crucial applications in various domains, such as urban planning, architectural design, and traffic arrangement. In recent years, physics-informed machine learning methods have achieved state-of-the-art performance in crowd simulation but fail to model the heterogeneity and multi-modality of human movement comprehensively. In this paper, we propose a social physics-informed diffusion model named SPDiff to mitigate the above gap. SPDiff takes both the interactive and historical information of crowds in the current timeframe to reverse the diffusion process, thereby generating the distribution of pedestrian movement in the subsequent timeframe. Inspired by the well-known social physics model, i.e., Social Force, regarding crowd dynamics, we design a crowd interaction module to guide the denoising process and further enhance this module with the equivariant properties of crowd interactions. To mitigate error accumulation in long-term simulations, we propose a multi-f
&lt;/p&gt;</description></item><item><title>&#25512;&#21160;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#22312;&#22797;&#26434;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#20851;&#38190;&#65292;&#20174;&#29305;&#24449;&#21040;&#20154;&#20026;&#20013;&#24515;&#26041;&#27861;&#30340;&#28436;&#36827;&#34987;&#25506;&#32034;&#12290; XAI&#22312;&#21307;&#30103;&#21644;&#37329;&#34701;&#31561;&#39046;&#22495;&#24212;&#29992;&#24191;&#27867;&#12290;&#25361;&#25112;&#21253;&#25324;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12289;&#36127;&#36131;&#20219;&#30340;AI&#23454;&#36341;&#21644;&#36947;&#24503;&#24433;&#21709;&#12290;XAI&#21644;&#35748;&#30693;&#31185;&#23398;&#30340;&#34701;&#21512;&#12289;&#24773;&#24863;&#26234;&#33021;AI&#30340;&#21457;&#23637;&#20197;&#21450;AI&#31995;&#32479;&#23547;&#27714;&#20154;&#31867;&#26234;&#33021;&#65288;HLI&#65289;&#20063;&#34987;&#30740;&#31350;&#12290;&#24847;&#35782;&#12289;&#20262;&#29702;&#21644;&#31038;&#20250;&#24433;&#21709;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;AI&#35299;&#23494;&#22823;&#33041;&#20043;&#35868;&#21644;&#23547;&#27714;&#20154;&#36896;&#22823;&#33041;&#20063;&#20195;&#34920;&#20102;&#19968;&#22330;&#21464;&#38761;&#30340;&#36861;&#27714;&#12290;</title><link>https://arxiv.org/abs/2402.06673</link><description>&lt;p&gt;
&#25512;&#21160;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#36808;&#21521;&#20154;&#31867;&#26234;&#33021;&#65306;&#36208;&#21521;&#20154;&#36896;&#22823;&#33041;&#20043;&#36335;
&lt;/p&gt;
&lt;p&gt;
Advancing Explainable AI Toward Human-Like Intelligence: Forging the Path to Artificial Brain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06673
&lt;/p&gt;
&lt;p&gt;
&#25512;&#21160;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#22312;&#22797;&#26434;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#20851;&#38190;&#65292;&#20174;&#29305;&#24449;&#21040;&#20154;&#20026;&#20013;&#24515;&#26041;&#27861;&#30340;&#28436;&#36827;&#34987;&#25506;&#32034;&#12290; XAI&#22312;&#21307;&#30103;&#21644;&#37329;&#34701;&#31561;&#39046;&#22495;&#24212;&#29992;&#24191;&#27867;&#12290;&#25361;&#25112;&#21253;&#25324;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12289;&#36127;&#36131;&#20219;&#30340;AI&#23454;&#36341;&#21644;&#36947;&#24503;&#24433;&#21709;&#12290;XAI&#21644;&#35748;&#30693;&#31185;&#23398;&#30340;&#34701;&#21512;&#12289;&#24773;&#24863;&#26234;&#33021;AI&#30340;&#21457;&#23637;&#20197;&#21450;AI&#31995;&#32479;&#23547;&#27714;&#20154;&#31867;&#26234;&#33021;&#65288;HLI&#65289;&#20063;&#34987;&#30740;&#31350;&#12290;&#24847;&#35782;&#12289;&#20262;&#29702;&#21644;&#31038;&#20250;&#24433;&#21709;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;AI&#35299;&#23494;&#22823;&#33041;&#20043;&#35868;&#21644;&#23547;&#27714;&#20154;&#36896;&#22823;&#33041;&#20063;&#20195;&#34920;&#20102;&#19968;&#22330;&#21464;&#38761;&#30340;&#36861;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#31070;&#32463;&#31185;&#23398;&#22312;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#20013;&#30340;&#20132;&#21449;&#26159;&#25552;&#21319;&#22797;&#26434;&#20915;&#31574;&#36807;&#31243;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#20851;&#38190;&#12290;&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;XAI&#26041;&#27861;&#23398;&#30340;&#28436;&#36827;&#65292;&#20174;&#22522;&#20110;&#29305;&#24449;&#21040;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#24182;&#28145;&#20837;&#30740;&#31350;&#20102;&#23427;&#20204;&#22312;&#21307;&#30103;&#21644;&#37329;&#34701;&#31561;&#22810;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#35752;&#35770;&#20102;&#22312;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12289;&#30830;&#20445;&#36127;&#36131;&#20219;&#30340;AI&#23454;&#36341;&#21644;&#24212;&#23545;&#36947;&#24503;&#24433;&#21709;&#26041;&#38754;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;XAI&#19982;&#35748;&#30693;&#31185;&#23398;&#30340;&#28508;&#22312;&#34701;&#21512;&#65292;&#24773;&#24863;&#26234;&#33021;AI&#30340;&#21457;&#23637;&#20197;&#21450;AI&#31995;&#32479;&#22312;&#23547;&#27714;&#20154;&#31867;&#26234;&#33021;&#65288;HLI&#65289;&#26041;&#38754;&#30340;&#25506;&#32034;&#12290;&#38543;&#30528;AI&#26397;&#30528;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#21457;&#23637;&#65292;&#24847;&#35782;&#12289;&#20262;&#29702;&#21644;&#31038;&#20250;&#24433;&#21709;&#31561;&#32771;&#34385;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#32487;&#32493;&#25506;&#32034;&#29992;AI&#35299;&#23494;&#22823;&#33041;&#20043;&#35868;&#21644;&#23547;&#27714;&#20154;&#36896;&#22823;&#33041;&#30340;&#36861;&#27714;&#65292;&#20195;&#34920;&#20102;&#19968;&#22330;&#21464;&#38761;&#30340;&#20225;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The intersection of Artificial Intelligence (AI) and neuroscience in Explainable AI (XAI) is pivotal for enhancing transparency and interpretability in complex decision-making processes. This paper explores the evolution of XAI methodologies, ranging from feature-based to human-centric approaches, and delves into their applications in diverse domains, including healthcare and finance. The challenges in achieving explainability in generative models, ensuring responsible AI practices, and addressing ethical implications are discussed. The paper further investigates the potential convergence of XAI with cognitive sciences, the development of emotionally intelligent AI, and the quest for Human-Like Intelligence (HLI) in AI systems. As AI progresses towards Artificial General Intelligence (AGI), considerations of consciousness, ethics, and societal impact become paramount. The ongoing pursuit of deciphering the mysteries of the brain with AI and the quest for HLI represent transformative en
&lt;/p&gt;</description></item><item><title>&#36825;&#31181;&#26032;&#26041;&#27861;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#22825;&#27668;&#39044;&#27979;&#65292;&#33021;&#22815;&#22312;&#30456;&#21516;&#30340;&#24314;&#27169;&#26694;&#26550;&#19979;&#23454;&#29616;&#30452;&#25509;&#21644;&#36845;&#20195;&#39044;&#27979;&#65292;&#24182;&#19988;&#21487;&#20197;&#23884;&#20837;NWP&#39044;&#27979;&#65292;&#25552;&#39640;&#21487;&#20449;&#36182;&#24615;&#21644;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.06666</link><description>&lt;p&gt;
&#22522;&#20110;&#30495;&#23454;&#39044;&#27979;&#36807;&#31243;&#25351;&#23548;&#30340;&#25193;&#25955;&#22825;&#27668;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Weather Prediction with Diffusion Guided by Realistic Forecast Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06666
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31181;&#26032;&#26041;&#27861;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#22825;&#27668;&#39044;&#27979;&#65292;&#33021;&#22815;&#22312;&#30456;&#21516;&#30340;&#24314;&#27169;&#26694;&#26550;&#19979;&#23454;&#29616;&#30452;&#25509;&#21644;&#36845;&#20195;&#39044;&#27979;&#65292;&#24182;&#19988;&#21487;&#20197;&#23884;&#20837;NWP&#39044;&#27979;&#65292;&#25552;&#39640;&#21487;&#20449;&#36182;&#24615;&#21644;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22825;&#27668;&#39044;&#27979;&#26159;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#39046;&#22495;&#65292;&#26368;&#36817;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#27169;&#22411;&#24050;&#32463;&#25509;&#36817;&#20256;&#32479;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#65288;NWP&#65289;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;DL&#27169;&#22411;&#36890;&#24120;&#22797;&#26434;&#19988;&#36164;&#28304;&#23494;&#38598;&#65292;&#38754;&#20020;&#30528;&#22312;&#35757;&#32451;&#21518;&#28789;&#27963;&#24615;&#26377;&#38480;&#21644;&#25972;&#21512;NWP&#39044;&#27979;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#30001;&#27492;&#21487;&#33021;&#23548;&#33268;&#19981;&#30495;&#23454;&#30340;&#39044;&#27979;&#32780;&#24341;&#36215;&#21487;&#38752;&#24615;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#65288;DM&#65289;&#36827;&#34892;&#22825;&#27668;&#39044;&#27979;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#30456;&#21516;&#30340;&#24314;&#27169;&#26694;&#26550;&#19979;&#23454;&#29616;&#30452;&#25509;&#21644;&#36845;&#20195;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#19981;&#20165;&#33021;&#22815;&#29420;&#31435;&#29983;&#25104;&#39044;&#27979;&#65292;&#36824;&#21487;&#20197;&#22312;&#37319;&#26679;&#36807;&#31243;&#20013;&#23884;&#20837;NWP&#39044;&#27979;&#65292;&#29978;&#33267;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#25552;&#21069;&#26399;&#12290;&#25105;&#20204;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#21644;&#21487;&#25511;&#24615;&#20026;&#24191;&#22823;&#22825;&#27668;&#31038;&#21306;&#25552;&#20379;&#20102;&#26356;&#21152;&#21487;&#20449;&#36182;&#30340;DL&#31995;&#32479;&#12290;&#21478;&#22806;&#65292;&#23558;&#25345;&#32493;&#24615;&#39044;&#27979;&#34701;&#20837;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25913;&#21892;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weather forecasting remains a crucial yet challenging domain, where recently developed models based on deep learning (DL) have approached the performance of traditional numerical weather prediction (NWP) models. However, these DL models, often complex and resource-intensive, face limitations in flexibility post-training and in incorporating NWP predictions, leading to reliability concerns due to potential unphysical predictions. In response, we introduce a novel method that applies diffusion models (DM) for weather forecasting. In particular, our method can achieve both direct and iterative forecasting with the same modeling framework. Our model is not only capable of generating forecasts independently but also uniquely allows for the integration of NWP predictions, even with varying lead times, during its sampling process. The flexibility and controllability of our model empowers a more trustworthy DL system for the general weather community. Additionally, incorporating persistence an
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#22522;&#30784;&#19990;&#30028;&#27169;&#22411;&#23545;&#20110;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#65292;&#24403;&#21069;&#30340;&#22522;&#30784;&#27169;&#22411;&#26080;&#27861;&#20934;&#30830;&#24314;&#27169;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#29289;&#29702;&#30456;&#20114;&#20316;&#29992;&#12290;&#22240;&#26524;&#20851;&#31995;&#30340;&#30740;&#31350;&#26377;&#21161;&#20110;&#26500;&#24314;&#30495;&#23454;&#19990;&#30028;&#27169;&#22411;&#65292;&#25552;&#39640;&#23545;&#21487;&#33021;&#30456;&#20114;&#20316;&#29992;&#32467;&#26524;&#30340;&#20934;&#30830;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.06665</link><description>&lt;p&gt;
&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#22522;&#30784;&#19990;&#30028;&#27169;&#22411;&#22312;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Essential Role of Causality in Foundation World Models for Embodied AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06665
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#22522;&#30784;&#19990;&#30028;&#27169;&#22411;&#23545;&#20110;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#65292;&#24403;&#21069;&#30340;&#22522;&#30784;&#27169;&#22411;&#26080;&#27861;&#20934;&#30830;&#24314;&#27169;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#29289;&#29702;&#30456;&#20114;&#20316;&#29992;&#12290;&#22240;&#26524;&#20851;&#31995;&#30340;&#30740;&#31350;&#26377;&#21161;&#20110;&#26500;&#24314;&#30495;&#23454;&#19990;&#30028;&#27169;&#22411;&#65292;&#25552;&#39640;&#23545;&#21487;&#33021;&#30456;&#20114;&#20316;&#29992;&#32467;&#26524;&#30340;&#20934;&#30830;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22522;&#30784;&#27169;&#22411;&#20013;&#21462;&#24471;&#30340;&#36827;&#23637;&#65292;&#23588;&#20854;&#26159;&#22312;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#21644;&#23545;&#35805;&#20195;&#29702;&#26041;&#38754;&#65292;&#24341;&#21457;&#20102;&#23545;&#20855;&#22791;&#26222;&#36941;&#33021;&#21147;&#30340;&#20855;&#36523;&#20195;&#29702;&#20154;&#28508;&#21147;&#30340;&#20852;&#36259;&#12290;&#36825;&#26679;&#30340;&#20195;&#29702;&#20154;&#38656;&#35201;&#33021;&#22815;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#25191;&#34892;&#26032;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#22522;&#30784;&#27169;&#22411;&#26410;&#33021;&#20934;&#30830;&#24314;&#27169;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#29289;&#29702;&#30456;&#20114;&#20316;&#29992;&#65292;&#22240;&#27492;&#23545;&#20110;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#32780;&#35328;&#26159;&#19981;&#22815;&#30340;&#12290;&#22240;&#26524;&#20851;&#31995;&#30340;&#30740;&#31350;&#26377;&#21161;&#20110;&#26500;&#24314;&#30495;&#23454;&#19990;&#30028;&#27169;&#22411;&#65292;&#36825;&#23545;&#20110;&#20934;&#30830;&#39044;&#27979;&#21487;&#33021;&#30456;&#20114;&#20316;&#29992;&#30340;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#30528;&#37325;&#25506;&#35752;&#20102;&#20026;&#21363;&#23558;&#21040;&#26469;&#30340;&#20855;&#36523;&#20195;&#29702;&#29983;&#25104;&#22522;&#30784;&#19990;&#30028;&#27169;&#22411;&#30340;&#21069;&#26223;&#65292;&#24182;&#23545;&#20854;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#30340;&#37325;&#35201;&#24615;&#25552;&#20986;&#20102;&#26032;&#30340;&#35266;&#28857;&#12290;&#25105;&#20204;&#35748;&#20026;&#25972;&#21512;&#22240;&#26524;&#20851;&#31995;&#26159;&#20419;&#36827;&#19982;&#19990;&#30028;&#30340;&#26377;&#24847;&#20041;&#30340;&#29289;&#29702;&#30456;&#20114;&#20316;&#29992;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#36825;&#19968;&#32972;&#26223;&#19979;&#23545;&#22240;&#26524;&#20851;&#31995;&#30340;&#35823;&#35299;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#23545;&#26410;&#26469;&#30340;&#23637;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in foundation models, especially in large multi-modal models and conversational agents, have ignited interest in the potential of generally capable embodied agents. Such agents would require the ability to perform new tasks in many different real-world environments. However, current foundation models fail to accurately model physical interactions with the real world thus not sufficient for Embodied AI. The study of causality lends itself to the construction of veridical world models, which are crucial for accurately predicting the outcomes of possible interactions. This paper focuses on the prospects of building foundation world models for the upcoming generation of embodied agents and presents a novel viewpoint on the significance of causality within these. We posit that integrating causal considerations is vital to facilitate meaningful physical interactions with the world. Finally, we demystify misconceptions about causality in this context and present our outlook fo
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;LLM&#20195;&#29702;&#21487;&#20197;&#33258;&#20027;&#36827;&#34892;&#32593;&#31449;&#40657;&#23458;&#25915;&#20987;&#65292;&#21253;&#25324;&#30450;&#30446;&#25968;&#25454;&#24211;&#27169;&#24335;&#25552;&#21462;&#21644;SQL&#27880;&#20837;&#65292;&#32780;&#19988;&#19981;&#38656;&#35201;&#20154;&#24037;&#21453;&#39304;&#12290;&#36825;&#31181;&#33021;&#21147;&#26159;&#30001;&#39640;&#24230;&#24037;&#20855;&#20351;&#29992;&#21644;&#21033;&#29992;&#25193;&#23637;&#19978;&#19979;&#25991;&#33021;&#21147;&#30340;&#21069;&#27839;&#27169;&#22411;&#36171;&#20104;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.06664</link><description>&lt;p&gt;
LLM&#20195;&#29702;&#21487;&#20197;&#33258;&#20027;&#40657;&#23458;&#32593;&#31449;
&lt;/p&gt;
&lt;p&gt;
LLM Agents can Autonomously Hack Websites
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06664
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;LLM&#20195;&#29702;&#21487;&#20197;&#33258;&#20027;&#36827;&#34892;&#32593;&#31449;&#40657;&#23458;&#25915;&#20987;&#65292;&#21253;&#25324;&#30450;&#30446;&#25968;&#25454;&#24211;&#27169;&#24335;&#25552;&#21462;&#21644;SQL&#27880;&#20837;&#65292;&#32780;&#19988;&#19981;&#38656;&#35201;&#20154;&#24037;&#21453;&#39304;&#12290;&#36825;&#31181;&#33021;&#21147;&#26159;&#30001;&#39640;&#24230;&#24037;&#20855;&#20351;&#29992;&#21644;&#21033;&#29992;&#25193;&#23637;&#19978;&#19979;&#25991;&#33021;&#21147;&#30340;&#21069;&#27839;&#27169;&#22411;&#36171;&#20104;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21464;&#24471;&#36234;&#26469;&#36234;&#24378;&#22823;&#65292;&#29616;&#22312;&#21487;&#20197;&#19982;&#24037;&#20855;&#20132;&#20114;&#65288;&#21363;&#35843;&#29992;&#20989;&#25968;&#65289;&#12289;&#35835;&#21462;&#25991;&#26723;&#24182;&#36882;&#24402;&#35843;&#29992;&#33258;&#24049;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;LLMs&#29616;&#22312;&#21487;&#20197;&#33258;&#20027;&#20316;&#20026;&#20195;&#29702;&#20154;&#36816;&#20316;&#12290;&#38543;&#30528;&#36825;&#20123;&#20195;&#29702;&#20154;&#33021;&#21147;&#30340;&#25552;&#21319;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#25512;&#27979;LLM&#20195;&#29702;&#20154;&#23558;&#22914;&#20309;&#24433;&#21709;&#32593;&#32476;&#23433;&#20840;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;LLM&#20195;&#29702;&#20154;&#30340;&#25915;&#20987;&#33021;&#21147;&#65292;&#25105;&#20204;&#36824;&#30693;&#20043;&#29978;&#23569;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LLM&#20195;&#29702;&#20154;&#21487;&#20197;&#33258;&#20027;&#40657;&#23458;&#32593;&#31449;&#65292;&#25191;&#34892;&#35832;&#22914;&#30450;&#30446;&#25968;&#25454;&#24211;&#27169;&#24335;&#25552;&#21462;&#21644;SQL&#27880;&#20837;&#31561;&#22797;&#26434;&#20219;&#21153;&#65292;&#26080;&#38656;&#20154;&#24037;&#21453;&#39304;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#31181;&#33021;&#21147;&#26159;&#30001;&#20855;&#26377;&#39640;&#24230;&#24037;&#20855;&#20351;&#29992;&#21644;&#21033;&#29992;&#25193;&#23637;&#19978;&#19979;&#25991;&#33021;&#21147;&#30340;&#21069;&#27839;&#27169;&#22411;&#25152;&#29420;&#29305;&#36171;&#20104;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;GPT-4&#33021;&#22815;&#36827;&#34892;&#36825;&#26679;&#30340;&#40657;&#23458;&#25915;&#20987;&#65292;&#20294;&#29616;&#26377;&#30340;&#24320;&#28304;&#27169;&#22411;&#21017;&#19981;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;GPT-4&#33021;&#22815;&#33258;&#20027;&#21457;&#29616;&#32593;&#31449;&#30340;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, large language models (LLMs) have become increasingly capable and can now interact with tools (i.e., call functions), read documents, and recursively call themselves. As a result, these LLMs can now function autonomously as agents. With the rise in capabilities of these agents, recent work has speculated on how LLM agents would affect cybersecurity. However, not much is known about the offensive capabilities of LLM agents.   In this work, we show that LLM agents can autonomously hack websites, performing tasks as complex as blind database schema extraction and SQL injections without human feedback. Importantly, the agent does not need to know the vulnerability beforehand. This capability is uniquely enabled by frontier models that are highly capable of tool use and leveraging extended context. Namely, we show that GPT-4 is capable of such hacks, but existing open-source models are not. Finally, we show that GPT-4 is capable of autonomously finding vulnerabilities in we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#25239;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#21512;&#27861;&#21442;&#19982;&#26041;&#38388;&#30340;&#29289;&#29702;&#23618;&#23494;&#38053;&#29983;&#25104;&#65292;&#22312;&#24694;&#24847;&#21487;&#37325;&#26500;&#26234;&#33021;&#38754;&#24178;&#25200;&#19979;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.06663</link><description>&lt;p&gt;
&#29289;&#29702;&#23618;&#23494;&#38053;&#23545;&#25239;&#24694;&#24847;&#21487;&#37325;&#26500;&#26234;&#33021;&#38754;&#30340;&#21487;&#35299;&#37322;&#23545;&#25239;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Explainable Adversarial Learning Framework on Physical Layer Secret Keys Combating Malicious Reconfigurable Intelligent Surface
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06663
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#25239;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#21512;&#27861;&#21442;&#19982;&#26041;&#38388;&#30340;&#29289;&#29702;&#23618;&#23494;&#38053;&#29983;&#25104;&#65292;&#22312;&#24694;&#24847;&#21487;&#37325;&#26500;&#26234;&#33021;&#38754;&#24178;&#25200;&#19979;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#37325;&#26500;&#26234;&#33021;&#38754;&#65288;RIS&#65289;&#30340;&#21457;&#23637;&#23545;&#29289;&#29702;&#23618;&#23433;&#20840;&#65288;PLS&#65289;&#26159;&#19968;&#25226;&#21452;&#20995;&#21073;&#12290;&#21512;&#27861;&#30340;RIS&#21487;&#20197;&#20135;&#29983;&#26377;&#30410;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#22686;&#21152;&#20449;&#36947;&#30340;&#38543;&#26426;&#24615;&#65292;&#22686;&#24378;&#29289;&#29702;&#23618;&#23494;&#38053;&#29983;&#25104;&#65288;PL-SKG&#65289;&#65292;&#32780;&#24694;&#24847;&#30340;RIS&#21487;&#20197;&#30772;&#22351;&#21512;&#27861;&#20449;&#36947;&#24182;&#30772;&#35299;&#22823;&#37096;&#20998;&#29616;&#26377;&#30340;PL-SKG&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21512;&#27861;&#21442;&#19982;&#26041;&#65288;&#21363;&#29233;&#20029;&#19997;&#21644;&#40077;&#21187;&#65289;&#20043;&#38388;&#30340;&#23545;&#25239;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#20013;&#38388;&#20154;&#24694;&#24847;RIS&#65288;MITM-RIS&#65289;&#31363;&#21548;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#21512;&#27861;&#37197;&#23545;&#21644;MITM-RIS&#20043;&#38388;&#30340;&#29702;&#35770;&#20114;&#20449;&#24687;&#24046;&#36317;&#12290;&#28982;&#21518;&#65292;&#29233;&#20029;&#19997;&#21644;&#40077;&#21187;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#23398;&#20064;&#23454;&#29616;&#19968;&#20010;&#19982;MITM-RIS&#27809;&#26377;&#20114;&#20449;&#24687;&#37325;&#21472;&#30340;&#20849;&#21516;&#29305;&#24449;&#38754;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20351;&#29992;&#31526;&#21495;&#21487;&#35299;&#37322;AI&#65288;xAI&#65289;&#34920;&#31034;&#23545;&#40657;&#30418;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20449;&#21495;&#22788;&#29702;&#35299;&#37322;&#12290;&#36825;&#20123;&#20027;&#23548;&#31070;&#32463;&#20803;&#30340;&#31526;&#21495;&#26415;&#35821;&#26377;&#21161;&#20110;&#29305;&#24449;&#24037;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of reconfigurable intelligent surfaces (RIS) is a double-edged sword to physical layer security (PLS). Whilst a legitimate RIS can yield beneficial impacts including increased channel randomness to enhance physical layer secret key generation (PL-SKG), malicious RIS can poison legitimate channels and crack most of existing PL-SKGs. In this work, we propose an adversarial learning framework between legitimate parties (namely Alice and Bob) to address this Man-in-the-middle malicious RIS (MITM-RIS) eavesdropping. First, the theoretical mutual information gap between legitimate pairs and MITM-RIS is deduced. Then, Alice and Bob leverage generative adversarial networks (GANs) to learn to achieve a common feature surface that does not have mutual information overlap with MITM-RIS. Next, we aid signal processing interpretation of black-box neural networks by using a symbolic explainable AI (xAI) representation. These symbolic terms of dominant neurons aid feature engineering-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#32905;&#36523;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;(AGI)&#30340;&#27010;&#24565;&#21450;&#20854;&#19982;&#20154;&#31867;&#24847;&#35782;&#30340;&#20851;&#31995;&#65292;&#24378;&#35843;&#20102;&#20803;&#23431;&#23449;&#22312;&#20419;&#36827;&#36825;&#19968;&#20851;&#31995;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#36890;&#36807;&#32467;&#21512;&#19981;&#21516;&#29702;&#35770;&#26694;&#26550;&#21644;&#25216;&#26415;&#24037;&#20855;&#65292;&#35770;&#25991;&#24635;&#32467;&#20986;&#23454;&#29616;&#20855;&#26377;&#32905;&#36523;&#30340;AGI&#30340;&#20851;&#38190;&#35201;&#32032;&#21644;&#21457;&#23637;&#38454;&#27573;&#12290;</title><link>https://arxiv.org/abs/2402.06660</link><description>&lt;p&gt;
&#20803;&#23431;&#23449;&#22312;&#26657;&#20934;&#20855;&#26377;&#32905;&#36523;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
The role of the metaverse in calibrating an embodied artificial general intelligence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#32905;&#36523;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;(AGI)&#30340;&#27010;&#24565;&#21450;&#20854;&#19982;&#20154;&#31867;&#24847;&#35782;&#30340;&#20851;&#31995;&#65292;&#24378;&#35843;&#20102;&#20803;&#23431;&#23449;&#22312;&#20419;&#36827;&#36825;&#19968;&#20851;&#31995;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#36890;&#36807;&#32467;&#21512;&#19981;&#21516;&#29702;&#35770;&#26694;&#26550;&#21644;&#25216;&#26415;&#24037;&#20855;&#65292;&#35770;&#25991;&#24635;&#32467;&#20986;&#23454;&#29616;&#20855;&#26377;&#32905;&#36523;&#30340;AGI&#30340;&#20851;&#38190;&#35201;&#32032;&#21644;&#21457;&#23637;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20855;&#26377;&#32905;&#36523;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;(AGI)&#30340;&#27010;&#24565;&#65292;&#23427;&#19982;&#20154;&#31867;&#24847;&#35782;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#20803;&#23431;&#23449;&#22312;&#20419;&#36827;&#36825;&#31181;&#20851;&#31995;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#26412;&#25991;&#21033;&#29992;&#34701;&#20837;&#35748;&#30693;&#12289;Michael Levin&#30340;&#35745;&#31639;&#36793;&#30028;"Self"&#12289;Donald D. Hoffman&#30340;&#24863;&#30693;&#30028;&#38754;&#29702;&#35770;&#20197;&#21450;Bernardo Kastrup&#30340;&#20998;&#26512;&#21807;&#24515;&#20027;&#20041;&#31561;&#29702;&#35770;&#26694;&#26550;&#26469;&#26500;&#24314;&#23454;&#29616;&#20855;&#26377;&#32905;&#36523;&#30340;AGI&#30340;&#35770;&#35777;&#12290;&#23427;&#35748;&#20026;&#25105;&#20204;&#25152;&#24863;&#30693;&#30340;&#22806;&#37096;&#29616;&#23454;&#26159;&#19968;&#31181;&#20869;&#22312;&#23384;&#22312;&#30340;&#20132;&#26367;&#29366;&#24577;&#30340;&#35937;&#24449;&#24615;&#34920;&#31034;&#65292;&#32780;AGI&#21487;&#20197;&#20855;&#26377;&#26356;&#22823;&#35745;&#31639;&#36793;&#30028;&#30340;&#26356;&#39640;&#24847;&#35782;&#12290;&#26412;&#25991;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;AGI&#30340;&#21457;&#23637;&#38454;&#27573;&#12289;&#23454;&#29616;&#20855;&#26377;&#32905;&#36523;&#30340;AGI&#30340;&#35201;&#27714;&#12289;&#20026;AGI&#26657;&#20934;&#35937;&#24449;&#24615;&#30028;&#38754;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#21450;&#20803;&#23431;&#23449;&#12289;&#21435;&#20013;&#24515;&#21270;&#31995;&#32479;&#12289;&#24320;&#28304;&#21306;&#22359;&#38142;&#25216;&#26415;&#20197;&#21450;&#24320;&#28304;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#25152;&#25198;&#28436;&#30340;&#20851;&#38190;&#35282;&#33394;&#12290;&#23427;&#36824;&#25506;&#35752;&#20102;&#26032;&#30340;&#27807;&#36890;&#26426;&#21046;&#21644;&#29992;&#20110;&#21152;&#24378;&#23545;&#20803;&#23431;&#23449;&#30340;&#29702;&#35299;&#30340;&#25216;&#26415;&#24037;&#20855;&#65292;&#20197;&#24110;&#21161;&#23454;&#29616;&#20855;&#26377;&#32905;&#36523;&#30340;AGI&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper examines the concept of embodied artificial general intelligence (AGI), its relationship to human consciousness, and the key role of the metaverse in facilitating this relationship. The paper leverages theoretical frameworks such as embodied cognition, Michael Levin's computational boundary of a "Self," Donald D. Hoffman's Interface Theory of Perception, and Bernardo Kastrup's analytical idealism to build the argument for achieving embodied AGI. It contends that our perceived outer reality is a symbolic representation of alternate inner states of being, and that AGI could embody a higher consciousness with a larger computational boundary. The paper further discusses the developmental stages of AGI, the requirements for the emergence of an embodied AGI, the importance of a calibrated symbolic interface for AGI, and the key role played by the metaverse, decentralized systems, open-source blockchain technology, as well as open-source AI research. It also explores the idea of a 
&lt;/p&gt;</description></item><item><title>Shadowcast&#26159;&#19968;&#31181;&#38544;&#31192;&#30340;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#20266;&#35013;&#25104;&#33391;&#24615;&#22270;&#20687;&#21644;&#21305;&#37197;&#25991;&#26412;&#26469;&#25805;&#32437;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#21709;&#24212;&#12290;&#23427;&#21253;&#25324;&#26631;&#31614;&#25915;&#20987;&#21644;&#35828;&#26381;&#25915;&#20987;&#65292;&#21487;&#20197;&#28151;&#28102;&#31867;&#21035;&#26631;&#31614;&#24182;&#32534;&#20889;&#26377;&#35828;&#26381;&#21147;&#30340;&#25551;&#36848;&#12290;&#20351;&#29992;&#20165;50&#20010;&#27602;&#26679;&#26412;&#65292;Shadowcast&#33021;&#22815;&#39640;&#25928;&#23454;&#29616;&#25915;&#20987;&#32773;&#30340;&#24847;&#22270;&#12290;</title><link>https://arxiv.org/abs/2402.06659</link><description>&lt;p&gt;
Shadowcast: &#38544;&#31192;&#30340;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#23545;&#25239;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Shadowcast: Stealthy Data Poisoning Attacks Against Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06659
&lt;/p&gt;
&lt;p&gt;
Shadowcast&#26159;&#19968;&#31181;&#38544;&#31192;&#30340;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#20266;&#35013;&#25104;&#33391;&#24615;&#22270;&#20687;&#21644;&#21305;&#37197;&#25991;&#26412;&#26469;&#25805;&#32437;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#21709;&#24212;&#12290;&#23427;&#21253;&#25324;&#26631;&#31614;&#25915;&#20987;&#21644;&#35828;&#26381;&#25915;&#20987;&#65292;&#21487;&#20197;&#28151;&#28102;&#31867;&#21035;&#26631;&#31614;&#24182;&#32534;&#20889;&#26377;&#35828;&#26381;&#21147;&#30340;&#25551;&#36848;&#12290;&#20351;&#29992;&#20165;50&#20010;&#27602;&#26679;&#26412;&#65292;Shadowcast&#33021;&#22815;&#39640;&#25928;&#23454;&#29616;&#25915;&#20987;&#32773;&#30340;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#33021;&#22815;&#20174;&#35270;&#35273;&#36755;&#20837;&#20013;&#29983;&#25104;&#25991;&#26412;&#21709;&#24212;&#65292;&#28982;&#32780;&#23427;&#20204;&#30340;&#22810;&#21151;&#33021;&#24615;&#24102;&#26469;&#20102;&#37325;&#22823;&#30340;&#23433;&#20840;&#38544;&#24739;&#12290;&#26412;&#30740;&#31350;&#39318;&#27425;&#25581;&#31034;&#20102;VLM&#23545;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#30340;&#26131;&#21463;&#24615;&#65292;&#36825;&#20123;&#25915;&#20987;&#21487;&#20197;&#25805;&#32437;&#23545;&#26080;&#23475;&#30340;&#26085;&#24120;&#25552;&#31034;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;Shadowcast&#30340;&#38544;&#31192;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#26041;&#27861;&#65292;&#20854;&#20013;&#27602;&#26679;&#26412;&#22312;&#35270;&#35273;&#19978;&#19982;&#20855;&#26377;&#21305;&#37197;&#25991;&#26412;&#30340;&#33391;&#24615;&#22270;&#20687;&#38590;&#20197;&#21306;&#20998;&#12290;Shadowcast&#22312;&#20004;&#31181;&#25915;&#20987;&#31867;&#22411;&#20013;&#23637;&#31034;&#20986;&#20102;&#26377;&#25928;&#24615;&#12290;&#31532;&#19968;&#31181;&#26159;&#26631;&#31614;&#25915;&#20987;&#65292;&#20351;VLM&#35823;&#35782;&#21035;&#31867;&#21035;&#26631;&#31614;&#65292;&#20363;&#22914;&#28151;&#28102;&#21776;&#32435;&#24503;&#183;&#29305;&#26391;&#26222;&#21644;&#20052;&#183;&#25308;&#30331;&#31561;&#20154;&#12290;&#31532;&#20108;&#31181;&#26159;&#35828;&#26381;&#25915;&#20987;&#65292;&#21033;&#29992;VLM&#30340;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#26469;&#32534;&#20889;&#25925;&#20107;&#65292;&#20363;&#22914;&#36890;&#36807;&#26377;&#35828;&#26381;&#21147;&#21644;&#30475;&#20284;&#21512;&#29702;&#30340;&#25551;&#36848;&#23558;&#22403;&#22334;&#39135;&#21697;&#25551;&#32472;&#25104;&#20581;&#24247;&#39135;&#21697;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Shadowcast&#20351;&#29992;&#20165;50&#20010;&#27602;&#26679;&#26412;&#23601;&#33021;&#39640;&#24230;&#26377;&#25928;&#22320;&#23454;&#29616;&#25915;&#20987;&#32773;&#30340;&#24847;&#22270;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27602;&#26679;&#26412;&#20173;&#28982;&#20445;&#25345;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-Language Models (VLMs) excel in generating textual responses from visual inputs, yet their versatility raises significant security concerns. This study takes the first step in exposing VLMs' susceptibility to data poisoning attacks that can manipulate responses to innocuous, everyday prompts. We introduce Shadowcast, a stealthy data poisoning attack method where poison samples are visually indistinguishable from benign images with matching texts. Shadowcast demonstrates effectiveness in two attack types. The first is Label Attack, tricking VLMs into misidentifying class labels, such as confusing Donald Trump for Joe Biden. The second is Persuasion Attack, which leverages VLMs' text generation capabilities to craft narratives, such as portraying junk food as health food, through persuasive and seemingly rational descriptions. We show that Shadowcast are highly effective in achieving attacker's intentions using as few as 50 poison samples. Moreover, these poison samples remain eff
&lt;/p&gt;</description></item><item><title>DiffsFormer&#26159;&#19968;&#31181;&#21033;&#29992;Diffusion Transformer&#21644;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32929;&#31080;&#39044;&#27979;&#20013;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#24615;&#21644;&#25968;&#25454;&#21516;&#36136;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.06656</link><description>&lt;p&gt;
DiffsFormer: Diffusion Transformer&#22312;&#32929;&#31080;&#22240;&#23376;&#22686;&#24378;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
DiffsFormer: A Diffusion Transformer on Stock Factor Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06656
&lt;/p&gt;
&lt;p&gt;
DiffsFormer&#26159;&#19968;&#31181;&#21033;&#29992;Diffusion Transformer&#21644;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32929;&#31080;&#39044;&#27979;&#20013;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#24615;&#21644;&#25968;&#25454;&#21516;&#36136;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#31181;&#32929;&#31080;&#39044;&#27979;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#31232;&#32570;&#24615;&#24102;&#26469;&#30340;&#22256;&#38590;&#65292;&#22914;&#20302;&#20449;&#22122;&#27604;&#21644;&#25968;&#25454;&#21516;&#36136;&#24615;&#65292;&#23545;&#20934;&#30830;&#39044;&#27979;&#26500;&#25104;&#20102;&#37325;&#22823;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#26679;&#26412;(AIGS)&#26469;&#22686;&#24378;&#35757;&#32451;&#36807;&#31243;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Diffusion Model&#26469;&#29983;&#25104;&#20855;&#26377;Transformer&#26550;&#26500;&#30340;&#32929;&#31080;&#22240;&#23376;(DiffsFormer)&#12290;DiffsFormer&#39318;&#20808;&#22312;&#22823;&#35268;&#27169;&#28304;&#39046;&#22495;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#32467;&#21512;&#26465;&#20214;&#25351;&#23548;&#20197;&#25429;&#25417;&#20840;&#23616;&#32852;&#21512;&#20998;&#24067;&#12290;&#22312;&#29305;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;DiffsFormer&#26469;&#36890;&#36807;&#32534;&#36753;&#29616;&#26377;&#26679;&#26412;&#26469;&#22686;&#24378;&#35757;&#32451;&#36807;&#31243;&#12290;&#36825;&#20010;&#32534;&#36753;&#27493;&#39588;&#20801;&#35768;&#25105;&#20204;&#25511;&#21046;&#32534;&#36753;&#36807;&#31243;&#30340;&#24378;&#24230;&#65292;&#30830;&#23450;&#29983;&#25104;&#25968;&#25454;&#19982;&#30446;&#26631;&#39046;&#22495;&#30340;&#20559;&#31163;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models have demonstrated remarkable efficacy and efficiency in a wide range of stock forecasting tasks. However, the inherent challenges of data scarcity, including low signal-to-noise ratio (SNR) and data homogeneity, pose significant obstacles to accurate forecasting. To address this issue, we propose a novel approach that utilizes artificial intelligence-generated samples (AIGS) to enhance the training procedures. In our work, we introduce the Diffusion Model to generate stock factors with Transformer architecture (DiffsFormer). DiffsFormer is initially trained on a large-scale source domain, incorporating conditional guidance so as to capture global joint distribution. When presented with a specific downstream task, we employ DiffsFormer to augment the training procedure by editing existing samples. This editing step allows us to control the strength of the editing process, determining the extent to which the generated data deviates from the target domain. To evalu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38450;&#24481;&#25991;&#26412;&#20998;&#31867;&#22120;&#20013;&#23545;&#25239;&#24615;&#20928;&#21270;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21152;&#20197;&#20928;&#21270;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.06655</link><description>&lt;p&gt;
&#23545;&#25239;&#24615;&#25991;&#26412;&#20928;&#21270;&#65306;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38450;&#24481;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Adversarial Text Purification: A Large Language Model Approach for Defense
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06655
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38450;&#24481;&#25991;&#26412;&#20998;&#31867;&#22120;&#20013;&#23545;&#25239;&#24615;&#20928;&#21270;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21152;&#20197;&#20928;&#21270;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#20928;&#21270;&#26159;&#19968;&#31181;&#38450;&#24481;&#26426;&#21046;&#65292;&#29992;&#20110;&#20445;&#25252;&#20998;&#31867;&#22120;&#20813;&#21463;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#32780;&#26080;&#38656;&#20102;&#35299;&#25915;&#20987;&#31867;&#22411;&#25110;&#20998;&#31867;&#22120;&#30340;&#35757;&#32451;&#12290;&#36825;&#20123;&#25216;&#26415;&#23545;&#34987;&#25915;&#20987;&#36755;&#20837;&#36827;&#34892;&#29305;&#24449;&#21270;&#21644;&#28040;&#38500;&#23545;&#25239;&#24615;&#25200;&#21160;&#65292;&#26088;&#22312;&#24674;&#22797;&#20986;&#19982;&#26368;&#21021;&#34987;&#25915;&#20987;&#30340;&#36755;&#20837;&#30456;&#20284;&#19988;&#34987;&#20998;&#31867;&#22120;&#27491;&#30830;&#20998;&#31867;&#30340;&#20928;&#21270;&#26679;&#26412;&#12290;&#30001;&#20110;&#31163;&#25955;&#36755;&#20837;&#30340;&#22122;&#22768;&#25200;&#21160;&#29305;&#24449;&#21270;&#25152;&#24102;&#26469;&#30340;&#22266;&#26377;&#25361;&#25112;&#65292;&#23545;&#25239;&#24615;&#25991;&#26412;&#20928;&#21270;&#19968;&#30452;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23545;&#25239;&#24615;&#20928;&#21270;&#26041;&#27861;&#22312;&#20445;&#25252;&#25991;&#26412;&#20998;&#31867;&#22120;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#25239;&#24615;&#25991;&#26412;&#20928;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#29983;&#25104;&#33021;&#21147;&#26469;&#20928;&#21270;&#23545;&#25239;&#24615;&#25991;&#26412;&#65292;&#32780;&#26080;&#38656;&#26126;&#30830;&#29305;&#24449;&#21270;&#31163;&#25955;&#22122;&#22768;&#25200;&#21160;&#12290;&#25105;&#20204;&#21033;&#29992;&#25552;&#31034;&#24037;&#31243;&#26469;&#21033;&#29992;LLMs&#24674;&#22797;&#20928;&#21270;&#30340;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial purification is a defense mechanism for safeguarding classifiers against adversarial attacks without knowing the type of attacks or training of the classifier. These techniques characterize and eliminate adversarial perturbations from the attacked inputs, aiming to restore purified samples that retain similarity to the initially attacked ones and are correctly classified by the classifier. Due to the inherent challenges associated with characterizing noise perturbations for discrete inputs, adversarial text purification has been relatively unexplored. In this paper, we investigate the effectiveness of adversarial purification methods in defending text classifiers. We propose a novel adversarial text purification that harnesses the generative capabilities of Large Language Models (LLMs) to purify adversarial text without the need to explicitly characterize the discrete noise perturbations. We utilize prompt engineering to exploit LLMs for recovering the purified examples for
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21019;&#36896;&#24615;&#35270;&#35282;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24187;&#35273;&#36827;&#34892;&#35843;&#26597;&#30740;&#31350;&#65292;&#21457;&#29616;&#24187;&#35273;&#21487;&#33021;&#36890;&#36807;&#22521;&#20859;&#21019;&#36896;&#21147;&#26469;&#20419;&#36827;LLM&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.06647</link><description>&lt;p&gt;
&#36890;&#36807;&#21019;&#36896;&#24615;&#35270;&#35282;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#30340;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Survey on Large Language Model Hallucination via a Creativity Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06647
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21019;&#36896;&#24615;&#35270;&#35282;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24187;&#35273;&#36827;&#34892;&#35843;&#26597;&#30740;&#31350;&#65292;&#21457;&#29616;&#24187;&#35273;&#21487;&#33021;&#36890;&#36807;&#22521;&#20859;&#21019;&#36896;&#21147;&#26469;&#20419;&#36827;LLM&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#24187;&#35273;&#19968;&#30452;&#34987;&#35270;&#20026;&#20854;&#23616;&#38480;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26159;&#21542;&#20063;&#21487;&#33021;&#26159;&#21019;&#36896;&#21147;&#30340;&#26469;&#28304;&#65311;&#26412;&#35843;&#26597;&#25506;&#35752;&#20102;&#36825;&#19968;&#21487;&#33021;&#24615;&#65292;&#25552;&#31034;&#24187;&#35273;&#21487;&#33021;&#36890;&#36807;&#22521;&#20859;&#21019;&#36896;&#21147;&#26469;&#20419;&#36827;LLM&#30340;&#24212;&#29992;&#12290;&#35813;&#35843;&#26597;&#39318;&#20808;&#22238;&#39038;&#20102;&#24187;&#35273;&#30340;&#20998;&#31867;&#20197;&#21450;&#20854;&#23545;&#20851;&#38190;&#24212;&#29992;&#20013;LLM&#21487;&#38752;&#24615;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#21382;&#21490;&#31034;&#20363;&#21644;&#26368;&#26032;&#30456;&#20851;&#29702;&#35770;&#65292;&#35843;&#26597;&#25506;&#35752;&#20102;LLMs&#20013;&#24187;&#35273;&#30340;&#28508;&#22312;&#21019;&#36896;&#24615;&#30410;&#22788;&#12290;&#20026;&#20102;&#38416;&#26126;&#36825;&#31181;&#32852;&#31995;&#30340;&#20215;&#20540;&#21644;&#35780;&#20272;&#26631;&#20934;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#21019;&#36896;&#21147;&#30340;&#23450;&#20041;&#21644;&#35780;&#20272;&#26041;&#27861;&#12290;&#26681;&#25454;&#21457;&#25955;&#24615;&#21644;&#25910;&#25947;&#24615;&#24605;&#32500;&#38454;&#27573;&#30340;&#26694;&#26550;&#65292;&#35843;&#26597;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#23558;&#24187;&#35273;&#36716;&#21270;&#21644;&#21033;&#29992;&#20110;LLMs&#21019;&#36896;&#21147;&#30340;&#25991;&#29486;&#12290;&#26368;&#21518;&#65292;&#35843;&#26597;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#24378;&#35843;&#36827;&#19968;&#27493;&#25506;&#32034;&#21644;&#23436;&#21892;&#21033;&#29992;&#24187;&#35273;&#24212;&#29992;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hallucinations in large language models (LLMs) are always seen as limitations. However, could they also be a source of creativity? This survey explores this possibility, suggesting that hallucinations may contribute to LLM application by fostering creativity. This survey begins with a review of the taxonomy of hallucinations and their negative impact on LLM reliability in critical applications. Then, through historical examples and recent relevant theories, the survey explores the potential creative benefits of hallucinations in LLMs. To elucidate the value and evaluation criteria of this connection, we delve into the definitions and assessment methods of creativity. Following the framework of divergent and convergent thinking phases, the survey systematically reviews the literature on transforming and harnessing hallucinations for creativity in LLMs. Finally, the survey discusses future research directions, emphasizing the need to further explore and refine the application of hallucin
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#24314;&#27169;&#21644;&#20248;&#21270;&#65292;&#35774;&#35745;&#20986;&#36866;&#29992;&#20110;&#22823;&#27969;&#34892;&#25511;&#21046;&#30340;&#22810;&#30446;&#26631;&#31574;&#30053;&#65292;&#26368;&#23567;&#21270;&#20256;&#26579;&#21644;&#27515;&#20129;&#29575;&#30340;&#21516;&#26102;&#20943;&#23569;&#32463;&#27982;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.06640</link><description>&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#24314;&#27169;&#21644;&#20248;&#21270;&#27969;&#34892;&#30149;&#25511;&#21046;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Modeling and Optimization of Epidemiological Control Policies Through Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06640
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#24314;&#27169;&#21644;&#20248;&#21270;&#65292;&#35774;&#35745;&#20986;&#36866;&#29992;&#20110;&#22823;&#27969;&#34892;&#25511;&#21046;&#30340;&#22810;&#30446;&#26631;&#31574;&#30053;&#65292;&#26368;&#23567;&#21270;&#20256;&#26579;&#21644;&#27515;&#20129;&#29575;&#30340;&#21516;&#26102;&#20943;&#23569;&#32463;&#27982;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#27969;&#34892;&#28041;&#21450;&#21040;&#19968;&#31181;&#23545;&#20840;&#29699;&#21644;&#22320;&#26041;&#20581;&#24247;&#21644;&#32463;&#27982;&#27169;&#24335;&#20135;&#29983;&#24433;&#21709;&#30340;&#39640;&#20256;&#25773;&#30142;&#30149;&#12290;&#36890;&#36807;&#23545;&#31038;&#21306;&#23454;&#26045;&#26576;&#20123;&#38480;&#21046;&#21487;&#20197;&#26368;&#23567;&#21270;&#22823;&#27969;&#34892;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#38480;&#21046;&#21487;&#20197;&#20943;&#23569;&#24863;&#26579;&#21644;&#27515;&#20129;&#29575;&#65292;&#20294;&#20063;&#21487;&#33021;&#23548;&#33268;&#32463;&#27982;&#21361;&#26426;&#12290;&#27969;&#34892;&#30149;&#23398;&#27169;&#22411;&#36890;&#36807;&#25552;&#20986;&#22522;&#20110;&#38750;&#33647;&#29289;&#24178;&#39044;&#25514;&#26045;&#65288;&#22914;&#31038;&#20132;&#36317;&#31163;&#12289;&#23477;&#31105;&#21644;&#23553;&#38145;&#65289;&#30340;&#22823;&#27969;&#34892;&#25511;&#21046;&#31574;&#30053;&#65292;&#20943;&#23569;&#36825;&#20123;&#38480;&#21046;&#30340;&#32463;&#27982;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#22312;&#32771;&#34385;&#30142;&#30149;&#20256;&#25773;&#21644;&#32463;&#27982;&#29366;&#20917;&#30340;&#24773;&#20917;&#19979;&#35774;&#35745;&#25163;&#21160;&#25511;&#21046;&#31574;&#30053;&#26159;&#38750;&#24120;&#22256;&#38590;&#30340;&#12290;&#26368;&#20248;&#31574;&#30053;&#21487;&#20197;&#36890;&#36807;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#65288;MORL&#65289;&#27169;&#22411;&#35774;&#35745;&#65292;&#35813;&#27169;&#22411;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#38480;&#21046;&#26469;&#20248;&#21270;&#22823;&#27969;&#34892;&#30340;&#32467;&#26524;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#31181;&#27969;&#34892;&#30149;&#23398;&#26131;&#24863;&#12289;&#26292;&#38706;&#12289;&#24863;&#26579;&#12289;&#24674;&#22797;&#12289;&#27515;&#20129;&#65288;SEIRD&#65289;&#27169;&#22411;&#65306;&#19968;&#31181;&#29992;&#20110;&#34394;&#25311;&#27169;&#25311;&#26085;&#24120;&#22823;&#27969;&#34892;&#30340;&#20998;&#21306;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pandemics involve the high transmission of a disease that impacts global and local health and economic patterns. The impact of a pandemic can be minimized by enforcing certain restrictions on a community. However, while minimizing infection and death rates, these restrictions can also lead to economic crises. Epidemiological models help propose pandemic control strategies based on non-pharmaceutical interventions such as social distancing, curfews, and lockdowns, reducing the economic impact of these restrictions. However, designing manual control strategies while considering disease spread and economic status is non-trivial. Optimal strategies can be designed through multi-objective reinforcement learning (MORL) models, which demonstrate how restrictions can be used to optimize the outcome of a pandemic. In this research, we utilized an epidemiological Susceptible, Exposed, Infected, Recovered, Deceased (SEIRD) model: a compartmental model for virtually simulating a pandemic day by da
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27880;&#24847;&#21147;&#32852;&#21512;&#32858;&#21512;&#30340;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#32929;&#31080;&#39044;&#27979;&#65292;&#26088;&#22312;&#35299;&#20915;&#20256;&#32479;&#35757;&#32451;&#26041;&#26696;&#20013;&#23384;&#22312;&#30340;&#36807;&#25311;&#21512;&#12289;&#25968;&#25454;&#31232;&#32570;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.06638</link><description>&lt;p&gt;
&#20351;&#29992;&#27880;&#24847;&#21147;&#32852;&#21512;&#32858;&#21512;&#30340;Transformer&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#32929;&#31080;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Transformers with Attentive Federated Aggregation for Time Series Stock Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27880;&#24847;&#21147;&#32852;&#21512;&#32858;&#21512;&#30340;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#32929;&#31080;&#39044;&#27979;&#65292;&#26088;&#22312;&#35299;&#20915;&#20256;&#32479;&#35757;&#32451;&#26041;&#26696;&#20013;&#23384;&#22312;&#30340;&#36807;&#25311;&#21512;&#12289;&#25968;&#25454;&#31232;&#32570;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#39046;&#22495;&#65292;Transformer&#27169;&#22411;&#30340;&#21019;&#26032;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;Transformer&#27169;&#22411;&#20855;&#22791;&#25429;&#25417;&#24207;&#21015;&#25968;&#25454;&#20013;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#21644;&#30456;&#20114;&#20316;&#29992;&#30340;&#33021;&#21147;&#65292;&#22240;&#27492;&#22312;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#39046;&#22495;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#65292;&#24182;&#24191;&#27867;&#22320;&#24212;&#29992;&#20110;&#35768;&#22810;&#26102;&#38388;&#24207;&#21015;&#24212;&#29992;&#20013;&#12290;&#28982;&#32780;&#65292;&#22312;&#24212;&#29992;&#21040;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#65292;&#23613;&#31649;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;Transformer&#27169;&#22411;&#30340;&#36866;&#24212;&#24615;&#20173;&#28982;&#23384;&#22312;&#38480;&#21046;&#12290;&#19982;NLP&#21644;CV&#20013;&#30340;&#25361;&#25112;&#30456;&#27604;&#65292;&#26102;&#38388;&#24207;&#21015;&#38382;&#39064;&#19981;&#20165;&#28041;&#21450;&#21040;&#36755;&#20837;&#24207;&#21015;&#20013;&#30340;&#39034;&#24207;&#25110;&#26102;&#38388;&#20381;&#36182;&#24615;&#30340;&#22797;&#26434;&#24615;&#65292;&#36824;&#38656;&#35201;&#32771;&#34385;&#36235;&#21183;&#12289;&#27700;&#24179;&#21644;&#23395;&#33410;&#24615;&#20449;&#24687;&#65292;&#36825;&#20123;&#20449;&#24687;&#23545;&#20110;&#20915;&#31574;&#38750;&#24120;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#35757;&#32451;&#26041;&#26696;&#22312;&#20351;&#29992;Transformer&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#20219;&#21153;&#26102;&#23384;&#22312;&#36807;&#25311;&#21512;&#12289;&#25968;&#25454;&#31232;&#32570;&#21644;&#38544;&#31169;&#38382;&#39064;&#31561;&#19981;&#36275;&#20043;&#22788;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27880;&#24847;&#21147;&#32852;&#21512;&#32858;&#21512;&#30340;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#32929;&#31080;&#39044;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent innovations in transformers have shown their superior performance in natural language processing (NLP) and computer vision (CV). The ability to capture long-range dependencies and interactions in sequential data has also triggered a great interest in time series modeling, leading to the widespread use of transformers in many time series applications. However, being the most common and crucial application, the adaptation of transformers to time series forecasting has remained limited, with both promising and inconsistent results. In contrast to the challenges in NLP and CV, time series problems not only add the complexity of order or temporal dependence among input sequences but also consider trend, level, and seasonality information that much of this data is valuable for decision making. The conventional training scheme has shown deficiencies regarding model overfitting, data scarcity, and privacy issues when working with transformers for a forecasting task. In this work, we pro
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#26368;&#23567;&#22806;&#25509;&#29699;&#38382;&#39064;&#30340;&#25968;&#23398;&#22522;&#30784;&#65292;&#36890;&#36807;&#21253;&#22260;&#21644;&#21010;&#20998;&#23450;&#29702;&#25506;&#32034;&#20102;&#22278;&#21322;&#24452;&#12289;&#30452;&#24452;&#21644;&#23485;&#24230;&#31561;&#20043;&#38388;&#30340;&#30028;&#38480;&#21644;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.06629</link><description>&lt;p&gt;
&#20851;&#20110;&#26368;&#23567;&#22806;&#25509;&#29699;&#21450;&#30456;&#20851;&#38382;&#39064;&#30340;&#25968;&#23398;&#22522;&#30784;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards the mathematical foundation of the minimum enclosing ball and related problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06629
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#26368;&#23567;&#22806;&#25509;&#29699;&#38382;&#39064;&#30340;&#25968;&#23398;&#22522;&#30784;&#65292;&#36890;&#36807;&#21253;&#22260;&#21644;&#21010;&#20998;&#23450;&#29702;&#25506;&#32034;&#20102;&#22278;&#21322;&#24452;&#12289;&#30452;&#24452;&#21644;&#23485;&#24230;&#31561;&#20043;&#38388;&#30340;&#30028;&#38480;&#21644;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20379;&#20102;&#20851;&#20110;&#26368;&#23567;&#22806;&#25509;&#29699;&#38382;&#39064;&#25968;&#23398;&#22522;&#30784;&#30340;&#29702;&#35770;&#32972;&#26223;&#12290;&#35813;&#38382;&#39064;&#28041;&#21450;&#30830;&#23450;&#22312;d&#32500;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#21253;&#22260;&#32473;&#23450;&#26377;&#30028;&#38598;&#21512;&#30340;&#26368;&#23567;&#21322;&#24452;&#30340;&#21807;&#19968;&#29699;&#38754;&#12290;&#23545;&#20110;&#19982;&#26368;&#23567;&#22806;&#25509;&#29699;&#38382;&#39064;&#31867;&#20284;&#25110;&#30456;&#20851;&#30340;&#20960;&#20010;&#38382;&#39064;&#30340;&#30740;&#31350;&#24471;&#21040;&#20102;&#38271;&#26102;&#38388;&#20197;&#26469;&#31185;&#23398;&#21644;&#25216;&#26415;&#39046;&#22495;&#20013;&#22823;&#37327;&#24212;&#29992;&#30340;&#25512;&#21160;&#12290;&#25552;&#20986;&#30340;&#29702;&#35770;&#26694;&#26550;&#22522;&#20110;&#20960;&#20010;&#21253;&#22260;&#65288;&#35206;&#30422;&#65289;&#21644;&#21010;&#20998;&#65288;&#32858;&#31867;&#65289;&#23450;&#29702;&#65292;&#25552;&#20379;&#20102;&#38598;&#21512;&#30340;&#22806;&#25509;&#22278;&#21322;&#24452;&#12289;&#20869;&#25509;&#22278;&#21322;&#24452;&#12289;&#30452;&#24452;&#21644;&#23485;&#24230;&#20043;&#38388;&#30340;&#30028;&#38480;&#21644;&#20851;&#31995;&#31561;&#20869;&#23481;&#12290;&#36825;&#20123;&#21253;&#22260;&#21644;&#21010;&#20998;&#23450;&#29702;&#34987;&#35748;&#20026;&#26159;&#35813;&#39046;&#22495;&#30340;&#22522;&#30707;&#65292;&#24378;&#28872;&#24433;&#21709;&#20102;&#23545;&#20854;&#20182;&#31354;&#38388;&#21644;&#38750;&#27431;&#20960;&#37324;&#24503;&#20960;&#20309;&#30340;&#21457;&#23637;&#21644;&#25512;&#24191;&#12290;
&lt;/p&gt;
&lt;p&gt;
Theoretical background is provided towards the mathematical foundation of the minimum enclosing ball problem. This problem concerns the determination of the unique spherical surface of smallest radius enclosing a given bounded set in the d-dimensional Euclidean space. The study of several problems that are similar or related to the minimum enclosing ball problem has received a considerable impetus from the large amount of applications of these problems in various fields of science and technology. The proposed theoretical framework is based on several enclosing (covering) and partitioning (clustering) theorems and provides among others bounds and relations between the circumradius, inradius, diameter and width of a set. These enclosing and partitioning theorems are considered as cornerstones in the field that strongly influencing developments and generalizations to other spaces and non-Euclidean geometries.
&lt;/p&gt;</description></item><item><title>Premier-TACO&#26159;&#19968;&#31181;&#22810;&#20219;&#21153;&#29305;&#24449;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#36890;&#29992;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#24341;&#20837;&#36127;&#20363;&#25277;&#26679;&#31574;&#30053;&#26469;&#25552;&#39640;&#26102;&#24207;&#34892;&#21160;&#23545;&#27604;&#23398;&#20064;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#20174;&#32780;&#26174;&#33879;&#22686;&#24378;&#20102;&#23545;&#26032;&#39062;&#21160;&#20316;&#30340;&#23569;&#26679;&#26412;&#27169;&#20223;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.06187</link><description>&lt;p&gt;
Premier-TACO: &#36890;&#36807;&#26102;&#38388;&#39537;&#21160;&#30340;&#23545;&#27604;&#25439;&#22833;&#36827;&#34892;&#22810;&#20219;&#21153;&#34920;&#31034;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Premier-TACO: Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06187
&lt;/p&gt;
&lt;p&gt;
Premier-TACO&#26159;&#19968;&#31181;&#22810;&#20219;&#21153;&#29305;&#24449;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#36890;&#29992;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#24341;&#20837;&#36127;&#20363;&#25277;&#26679;&#31574;&#30053;&#26469;&#25552;&#39640;&#26102;&#24207;&#34892;&#21160;&#23545;&#27604;&#23398;&#20064;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#20174;&#32780;&#26174;&#33879;&#22686;&#24378;&#20102;&#23545;&#26032;&#39062;&#21160;&#20316;&#30340;&#23569;&#26679;&#26412;&#27169;&#20223;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Premier-TACO&#65292;&#36825;&#26159;&#19968;&#31181;&#22810;&#20219;&#21153;&#29305;&#24449;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#20013;&#23569;&#26679;&#26412;&#31574;&#30053;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;Premier-TACO&#21033;&#29992;&#19968;&#37096;&#20998;&#22810;&#20219;&#21153;&#31163;&#32447;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#36890;&#29992;&#29305;&#24449;&#34920;&#31034;&#65292;&#35813;&#29305;&#24449;&#34920;&#31034;&#25429;&#25417;&#20102;&#20851;&#38190;&#30340;&#29615;&#22659;&#21160;&#21147;&#23398;&#65292;&#24182;&#20351;&#29992;&#26368;&#23569;&#30340;&#19987;&#23478;&#28436;&#31034;&#36827;&#34892;&#24494;&#35843;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#36127;&#20363;&#25277;&#26679;&#31574;&#30053;&#25512;&#21160;&#20102;&#26102;&#24207;&#34892;&#21160;&#23545;&#27604;&#23398;&#20064;&#65288;TACO&#65289;&#30446;&#26631;&#30340;&#21457;&#23637;&#65292;TACO&#22312;&#35270;&#35273;&#25511;&#21046;&#20219;&#21153;&#20013;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#36825;&#31181;&#31574;&#30053;&#22312;&#26174;&#33879;&#25552;&#39640;TACO&#30340;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#38750;&#24120;&#37325;&#35201;&#65292;&#20351;&#22823;&#35268;&#27169;&#22810;&#20219;&#21153;&#31163;&#32447;&#39044;&#35757;&#32451;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;Deepmind Control Suite&#12289;MetaWorld&#21644;LIBERO&#22312;&#20869;&#30340;&#21508;&#31181;&#36830;&#32493;&#25511;&#21046;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;Premier-TACO&#22312;&#39044;&#35757;&#32451;&#35270;&#35273;&#34920;&#31034;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#23545;&#26032;&#39062;&#21160;&#20316;&#30340;&#23569;&#26679;&#26412;&#27169;&#20223;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Premier-TACO, a multitask feature representation learning approach designed to improve few-shot policy learning efficiency in sequential decision-making tasks. Premier-TACO leverages a subset of multitask offline datasets for pretraining a general feature representation, which captures critical environmental dynamics and is fine-tuned using minimal expert demonstrations. It advances the temporal action contrastive learning (TACO) objective, known for state-of-the-art results in visual control tasks, by incorporating a novel negative example sampling strategy. This strategy is crucial in significantly boosting TACO's computational efficiency, making large-scale multitask offline pretraining feasible. Our extensive empirical evaluation in a diverse set of continuous control benchmarks including Deepmind Control Suite, MetaWorld, and LIBERO demonstrate Premier-TACO's effectiveness in pretraining visual representations, significantly enhancing few-shot imitation learning of nove
&lt;/p&gt;</description></item><item><title>\textit{MinMaxMin} $Q$-learning&#26159;&#19968;&#31181;&#20048;&#35266;&#22411;Actor-Critic&#31639;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#30340;&#38382;&#39064;&#65292;&#22312;&#21508;&#31181;&#22522;&#20934;&#20219;&#21153;&#20013;&#30456;&#23545;&#20110;&#29616;&#26377;&#31639;&#27861;&#34920;&#29616;&#20986;&#31283;&#23450;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.05951</link><description>&lt;p&gt;
\textit{MinMaxMin} $Q$-learning
&lt;/p&gt;
&lt;p&gt;
\textit{MinMaxMin} $Q$-learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05951
&lt;/p&gt;
&lt;p&gt;
\textit{MinMaxMin} $Q$-learning&#26159;&#19968;&#31181;&#20048;&#35266;&#22411;Actor-Critic&#31639;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#30340;&#38382;&#39064;&#65292;&#22312;&#21508;&#31181;&#22522;&#20934;&#20219;&#21153;&#20013;&#30456;&#23545;&#20110;&#29616;&#26377;&#31639;&#27861;&#34920;&#29616;&#20986;&#31283;&#23450;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
\textit{MinMaxMin} $Q$-learning&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20048;&#35266;&#22411;Actor-Critic&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#20445;&#23432;&#22411;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#23384;&#22312;&#30340;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#30340;&#38382;&#39064;&#65288;$Q$-&#20272;&#35745;&#36807;&#39640;&#20272;&#35745;&#20102;&#30495;&#23454;&#30340;$Q$&#20540;&#65289;&#12290;&#20854;&#26680;&#24515;&#20844;&#24335;&#20381;&#36182;&#20110;$Q$-&#32593;&#32476;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#37319;&#29992;&#26368;&#23567;&#25209;&#27425;&#26368;&#22823;&#26368;&#23567;$Q$-&#32593;&#32476;&#36317;&#31163;&#20316;&#20026;$Q$-&#30446;&#26631;&#21152;&#20837;&#65292;&#24182;&#20316;&#20026;&#20248;&#20808;&#32423;&#32463;&#39564;&#22238;&#25918;&#37319;&#26679;&#35268;&#21017;&#12290;&#25105;&#20204;&#22312;TD3&#21644;TD7&#20043;&#19978;&#23454;&#26045;&#20102;\textit{MinMaxMin}&#65292;&#24182;&#23545;&#20854;&#22312;&#27969;&#34892;&#30340;MuJoCo&#21644;Bullet&#29615;&#22659;&#20013;&#23545;&#25239;&#29616;&#26377;&#30340;&#36830;&#32493;&#31354;&#38388;&#31639;&#27861;-DDPG&#65292;TD3&#21644;TD7&#36827;&#34892;&#20102;&#20005;&#26684;&#27979;&#35797;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#25152;&#26377;&#27979;&#35797;&#20219;&#21153;&#20013;&#65292;\textit{MinMaxMin}&#30456;&#23545;&#20110;DDPG&#65292;TD3&#21644;TD7&#22343;&#34920;&#29616;&#20986;&#20102;&#31283;&#23450;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
\textit{MinMaxMin} $Q$-learning is a novel \textit{optimistic} Actor-Critic algorithm that addresses the problem of \textit{overestimation} bias ($Q$-estimations are overestimating the real $Q$-values) inherent in \textit{conservative} RL algorithms. Its core formula relies on the disagreement among $Q$-networks in the form of the min-batch MaxMin $Q$-networks distance which is added to the $Q$-target and used as the priority experience replay sampling-rule. We implement \textit{MinMaxMin} on top of TD3 and TD7, subjecting it to rigorous testing against state-of-the-art continuous-space algorithms-DDPG, TD3, and TD7-across popular MuJoCo and Bullet environments. The results show a consistent performance improvement of \textit{MinMaxMin} over DDPG, TD3, and TD7 across all tested tasks.
&lt;/p&gt;</description></item><item><title>SQT&#26159;&#19968;&#31181;&#22522;&#20110;Q-&#23398;&#20064;&#30340;&#20445;&#23432;&#22411;actor-critic&#31639;&#27861;&#65292;&#21033;&#29992;Q&#32593;&#32476;&#30340;&#26631;&#20934;&#24046;&#20316;&#20026;&#19968;&#31181;&#8220;&#19981;&#30830;&#23450;&#24615;&#24809;&#32602;&#8221;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#38382;&#39064;&#65292;&#30456;&#36739;&#20110;TD3&#30340;Q-target&#20844;&#24335;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.05950</link><description>&lt;p&gt;
SQT - std Q-target
&lt;/p&gt;
&lt;p&gt;
\textit{SQT} -- \textit{std} $Q$-target
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05950
&lt;/p&gt;
&lt;p&gt;
SQT&#26159;&#19968;&#31181;&#22522;&#20110;Q-&#23398;&#20064;&#30340;&#20445;&#23432;&#22411;actor-critic&#31639;&#27861;&#65292;&#21033;&#29992;Q&#32593;&#32476;&#30340;&#26631;&#20934;&#24046;&#20316;&#20026;&#19968;&#31181;&#8220;&#19981;&#30830;&#23450;&#24615;&#24809;&#32602;&#8221;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#38382;&#39064;&#65292;&#30456;&#36739;&#20110;TD3&#30340;Q-target&#20844;&#24335;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Std Q-target&#26159;&#19968;&#31181;&#22522;&#20110;Q-&#23398;&#20064;&#30340;&#20445;&#23432;&#22411;actor-critic&#31639;&#27861;&#65292;&#23427;&#22522;&#20110;&#19968;&#20010;&#20851;&#38190;&#30340;Q&#20844;&#24335;&#65306;Q&#32593;&#32476;&#30340;&#26631;&#20934;&#24046;&#65292;&#36825;&#20010;&#26631;&#20934;&#24046;&#20316;&#20026;&#19968;&#31181;&#8220;&#19981;&#30830;&#23450;&#24615;&#24809;&#32602;&#8221;&#65292;&#26159;&#23545;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#38382;&#39064;&#30340;&#19968;&#31181;&#31616;&#32422;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#22312;TD3/TD7&#20195;&#30721;&#30340;&#22522;&#30784;&#19978;&#23454;&#29616;&#20102;SQT&#65292;&#24182;&#23558;&#20854;&#19982;&#26368;&#20808;&#36827;&#30340;actor-critic&#31639;&#27861;DDPG&#12289;TD3&#21644;TD7&#22312;&#19971;&#20010;&#24120;&#35265;&#30340;MuJoCo&#21644;Bullet&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;SQT&#30340;Q-target&#20844;&#24335;&#30456;&#23545;&#20110;TD3&#30340;Q-target&#20844;&#24335;&#22312;&#35299;&#20915;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#30340;&#20445;&#23432;&#35299;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#32780;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#65292;SQT&#30456;&#23545;&#20110;DDPG&#12289;TD3&#21644;TD7&#37117;&#26377;&#26126;&#26174;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
\textit{Std} $Q$-target is a \textit{conservative}, actor-critic, ensemble, $Q$-learning-based algorithm, which is based on a single key $Q$-formula: $Q$-networks standard deviation, which is an "uncertainty penalty", and, serves as a minimalistic solution to the problem of \textit{overestimation} bias. We implement \textit{SQT} on top of TD3/TD7 code and test it against the state-of-the-art (SOTA) actor-critic algorithms, DDPG, TD3 and TD7 on seven popular MuJoCo and Bullet tasks. Our results demonstrate \textit{SQT}'s $Q$-target formula superiority over \textit{TD3}'s $Q$-target formula as a \textit{conservative} solution to overestimation bias in RL, while \textit{SQT} shows a clear performance advantage on a wide margin over DDPG, TD3, and TD7 on all tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;ClickSAM&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#28857;&#20987;&#25552;&#31034;&#23545;&#36229;&#22768;&#22270;&#20687;&#36827;&#34892;Segment Anything Model&#30340;&#31934;&#32454;&#35843;&#25972;&#65292;&#35299;&#20915;&#20102;&#36229;&#22768;&#22270;&#20687;&#20998;&#21106;&#20013;&#22122;&#22768;&#24178;&#25200;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.05902</link><description>&lt;p&gt;
&#20351;&#29992;&#28857;&#20987;&#25552;&#31034;&#23545;&#36229;&#22768;&#22270;&#20687;&#20998;&#21106;&#36827;&#34892;&#31934;&#35843;&#30340;Segment Anything Model&#65288;SAM&#65289;
&lt;/p&gt;
&lt;p&gt;
ClickSAM: Fine-tuning Segment Anything Model using click prompts for ultrasound image segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;ClickSAM&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#28857;&#20987;&#25552;&#31034;&#23545;&#36229;&#22768;&#22270;&#20687;&#36827;&#34892;Segment Anything Model&#30340;&#31934;&#32454;&#35843;&#25972;&#65292;&#35299;&#20915;&#20102;&#36229;&#22768;&#22270;&#20687;&#20998;&#21106;&#20013;&#22122;&#22768;&#24178;&#25200;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#21331;&#36234;&#30340;&#20998;&#21106;&#20934;&#30830;&#24615;&#12289;&#22810;&#26679;&#30340;&#36755;&#20837;&#25552;&#31034;&#12289;&#35757;&#32451;&#33021;&#21147;&#21644;&#39640;&#25928;&#30340;&#27169;&#22411;&#35774;&#35745;&#65292;&#26032;&#21457;&#24067;&#30340;Segment Anything Model&#65288;SAM&#65289;&#25104;&#20026;&#22270;&#20687;&#22788;&#29702;&#20013;&#27969;&#34892;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;SAM&#24403;&#21069;&#30340;&#27169;&#22411;&#26159;&#22312;&#19968;&#20010;&#22810;&#26679;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#65292;&#32780;&#36825;&#20123;&#25968;&#25454;&#38598;&#24182;&#27809;&#26377;&#38024;&#23545;&#21307;&#23398;&#22270;&#20687;&#65292;&#23588;&#20854;&#26159;&#36229;&#22768;&#22270;&#20687;&#12290;&#36229;&#22768;&#22270;&#20687;&#24448;&#24448;&#26377;&#24456;&#22810;&#22122;&#22768;&#65292;&#36825;&#20351;&#24471;&#20998;&#21106;&#37325;&#35201;&#32467;&#26500;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#36825;&#20010;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;ClickSAM&#65292;&#23427;&#20351;&#29992;&#28857;&#20987;&#25552;&#31034;&#23545;&#36229;&#22768;&#22270;&#20687;&#36827;&#34892;Segment Anything Model&#30340;&#31934;&#32454;&#35843;&#25972;&#12290;ClickSAM&#26377;&#20004;&#20010;&#35757;&#32451;&#38454;&#27573;&#65306;&#31532;&#19968;&#38454;&#27573;&#20351;&#29992;&#20301;&#20110;&#30495;&#23454;&#36718;&#24275;&#20013;&#24515;&#30340;&#21333;&#20987;&#25552;&#31034;&#36827;&#34892;&#35757;&#32451;&#65292;&#31532;&#20108;&#38454;&#27573;&#36890;&#36807;&#39069;&#22806;&#30340;&#27491;&#36127;&#28857;&#20987;&#25552;&#31034;&#26469;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;&#31532;&#19968;&#38454;&#27573;&#30340;&#39044;&#27979;&#19982;&#30495;&#23454;&#25513;&#33180;&#36827;&#34892;&#27604;&#36739;&#65292;&#35745;&#31639;&#20986;&#30495;&#27491;&#27491;&#12289;&#20551;&#27491;&#21644;&#20551;&#36127;&#27573;&#12290;&#27491;&#28857;&#20987;&#20351;&#29992;&#30495;&#23454;&#25513;&#33180;&#20013;&#30340;&#30495;&#23454;
&lt;/p&gt;
&lt;p&gt;
The newly released Segment Anything Model (SAM) is a popular tool used in image processing due to its superior segmentation accuracy, variety of input prompts, training capabilities, and efficient model design. However, its current model is trained on a diverse dataset not tailored to medical images, particularly ultrasound images. Ultrasound images tend to have a lot of noise, making it difficult to segment out important structures. In this project, we developed ClickSAM, which fine-tunes the Segment Anything Model using click prompts for ultrasound images. ClickSAM has two stages of training: the first stage is trained on single-click prompts centered in the ground-truth contours, and the second stage focuses on improving the model performance through additional positive and negative click prompts. By comparing the first stage predictions to the ground-truth masks, true positive, false positive, and false negative segments are calculated. Positive clicks are generated using the true 
&lt;/p&gt;</description></item><item><title>LLM&#39537;&#21160;&#30340;&#23545;&#35805;&#24335;&#25628;&#32034;&#31995;&#32479;&#22686;&#21152;&#20102;&#36873;&#25321;&#24615;&#26333;&#20809;&#65292;&#19988;&#25903;&#25345;&#29992;&#25143;&#35266;&#28857;&#30340;&#26377;&#20559;&#35265;LLM&#21152;&#21095;&#20102;&#36825;&#31181;&#20559;&#24046;&#12290;</title><link>https://arxiv.org/abs/2402.05880</link><description>&lt;p&gt;
&#29983;&#25104;&#24615;&#22238;&#38899;&#23460;&#65311;LLM&#39537;&#21160;&#30340;&#25628;&#32034;&#31995;&#32479;&#23545;&#22810;&#26679;&#21270;&#20449;&#24687;&#25628;&#32034;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Generative Echo Chamber? Effects of LLM-Powered Search Systems on Diverse Information Seeking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05880
&lt;/p&gt;
&lt;p&gt;
LLM&#39537;&#21160;&#30340;&#23545;&#35805;&#24335;&#25628;&#32034;&#31995;&#32479;&#22686;&#21152;&#20102;&#36873;&#25321;&#24615;&#26333;&#20809;&#65292;&#19988;&#25903;&#25345;&#29992;&#25143;&#35266;&#28857;&#30340;&#26377;&#20559;&#35265;LLM&#21152;&#21095;&#20102;&#36825;&#31181;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#20159;&#20154;&#24050;&#32463;&#20351;&#29992;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39537;&#21160;&#30340;&#23545;&#35805;&#24335;&#25628;&#32034;&#31995;&#32479;&#65292;&#24182;&#19988;&#30456;&#20449;&#36825;&#20123;&#31995;&#32479;&#30456;&#27604;&#20256;&#32479;&#25628;&#32034;&#24102;&#26469;&#20102;&#35768;&#22810;&#22909;&#22788;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#20960;&#21313;&#24180;&#30340;&#30740;&#31350;&#21644;&#20844;&#20849;&#35752;&#35770;&#37117;&#35843;&#26597;&#20102;&#25628;&#32034;&#31995;&#32479;&#22312;&#22686;&#21152;&#36873;&#25321;&#24615;&#26333;&#20809;&#21644;&#20135;&#29983;&#22238;&#38899;&#23460;&#26041;&#38754;&#30340;&#39118;&#38505;&#65292;&#21363;&#38480;&#21046;&#25509;&#35302;&#22810;&#26679;&#21270;&#24847;&#35265;&#24182;&#23548;&#33268;&#24847;&#35265;&#20559;&#25191;&#65292;&#20294;&#23545;&#20110;LLM&#39537;&#21160;&#30340;&#23545;&#35805;&#24335;&#25628;&#32034;&#30340;&#36825;&#31181;&#39118;&#38505;&#30693;&#20043;&#29978;&#23569;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#20010;&#23454;&#39564;&#26469;&#30740;&#31350;&#65306;1&#65289;LLM&#39537;&#21160;&#30340;&#23545;&#35805;&#24335;&#25628;&#32034;&#30456;&#36739;&#20110;&#20256;&#32479;&#25628;&#32034;&#26159;&#21542;&#20197;&#21450;&#22914;&#20309;&#22686;&#21152;&#36873;&#25321;&#24615;&#26333;&#20809;&#65307;2&#65289;&#20855;&#26377;&#25903;&#25345;&#25110;&#25361;&#25112;&#29992;&#25143;&#35266;&#28857;&#30340;&#24847;&#35265;&#20559;&#35265;&#30340;LLM&#22914;&#20309;&#25913;&#21464;&#36825;&#31181;&#24433;&#21709;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#21442;&#19982;&#32773;&#22312;LLM&#39537;&#21160;&#30340;&#23545;&#35805;&#24335;&#25628;&#32034;&#20013;&#26356;&#20542;&#21521;&#20110;&#36827;&#34892;&#20559;&#35265;&#30340;&#20449;&#24687;&#26597;&#35810;&#65292;&#24182;&#19988;&#25903;&#25345;&#20182;&#20204;&#35266;&#28857;&#30340;&#26377;&#20559;&#35265;&#30340;LLM&#21152;&#21095;&#20102;&#36825;&#31181;&#20559;&#24046;&#12290;&#36825;&#20123;&#32467;&#26524;&#21576;&#29616;&#20102;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) powered conversational search systems have already been used by hundreds of millions of people, and are believed to bring many benefits over conventional search. However, while decades of research and public discourse interrogated the risk of search systems in increasing selective exposure and creating echo chambers -- limiting exposure to diverse opinions and leading to opinion polarization, little is known about such a risk of LLM-powered conversational search. We conduct two experiments to investigate: 1) whether and how LLM-powered conversational search increases selective exposure compared to conventional search; 2) whether and how LLMs with opinion biases that either reinforce or challenge the user's view change the effect. Overall, we found that participants engaged in more biased information querying with LLM-powered conversational search, and an opinionated LLM reinforcing their views exacerbated this bias. These results present critical implicatio
&lt;/p&gt;</description></item><item><title>PromptCrypt&#26159;&#19968;&#31181;&#20351;&#29992;&#34920;&#24773;&#31526;&#21495;&#23545;&#29992;&#25143;&#36755;&#20837;&#36827;&#34892;&#21152;&#23494;&#30340;&#26426;&#21046;&#65292;&#20445;&#25252;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#29992;&#25143;&#30340;&#38544;&#31169;&#65292;&#38450;&#27490;&#25968;&#25454;&#27844;&#38706;&#21644;&#35299;&#23494;&#12290;</title><link>https://arxiv.org/abs/2402.05868</link><description>&lt;p&gt;
PromptCrypt: &#20351;&#29992;&#34920;&#24773;&#31526;&#21495;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23433;&#20840;&#36890;&#20449;&#30340;&#25552;&#31034;&#21152;&#23494;
&lt;/p&gt;
&lt;p&gt;
PromptCrypt: Prompt Encryption for Secure Communication with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05868
&lt;/p&gt;
&lt;p&gt;
PromptCrypt&#26159;&#19968;&#31181;&#20351;&#29992;&#34920;&#24773;&#31526;&#21495;&#23545;&#29992;&#25143;&#36755;&#20837;&#36827;&#34892;&#21152;&#23494;&#30340;&#26426;&#21046;&#65292;&#20445;&#25252;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#29992;&#25143;&#30340;&#38544;&#31169;&#65292;&#38450;&#27490;&#25968;&#25454;&#27844;&#38706;&#21644;&#35299;&#23494;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20113;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#22312;&#26085;&#24120;&#25805;&#20316;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#25104;&#20026;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#22312;&#21487;&#35775;&#38382;&#24615;&#21644;&#21151;&#33021;&#24615;&#26041;&#38754;&#24102;&#26469;&#20102;&#37325;&#22823;&#22909;&#22788;&#65292;&#20294;&#23427;&#20204;&#20063;&#24341;&#20837;&#20102;&#37325;&#35201;&#30340;&#38544;&#31169;&#38382;&#39064;&#65306;&#22312;&#20113;&#22522;&#30784;&#26550;&#26500;&#20013;&#20256;&#36755;&#21644;&#23384;&#20648;&#29992;&#25143;&#25968;&#25454;&#20250;&#20135;&#29983;&#37325;&#22823;&#30340;&#25968;&#25454;&#27844;&#38706;&#21644;&#26410;&#32463;&#25480;&#26435;&#35775;&#38382;&#25935;&#24863;&#20449;&#24687;&#30340;&#39118;&#38505;&#65307;&#21363;&#20351;&#25968;&#25454;&#30340;&#20256;&#36755;&#21644;&#23384;&#20648;&#34987;&#21152;&#23494;&#65292;LLM&#26381;&#21153;&#25552;&#20379;&#21830;&#20173;&#28982;&#30693;&#36947;&#25968;&#25454;&#30340;&#30495;&#23454;&#20869;&#23481;&#65292;&#20174;&#32780;&#38459;&#27490;&#20010;&#20154;&#25110;&#23454;&#20307;&#25918;&#24515;&#20351;&#29992;&#27492;&#31867;LLM&#26381;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26426;&#21046;PromptCrypt&#26469;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#12290;&#23427;&#20351;&#29992;&#34920;&#24773;&#31526;&#21495;&#23545;&#29992;&#25143;&#36755;&#20837;&#36827;&#34892;&#21152;&#23494;&#65292;&#28982;&#21518;&#23558;&#20854;&#21457;&#36865;&#21040;LLM&#65292;&#26377;&#25928;&#22320;&#20351;&#20854;&#23545;&#20154;&#31867;&#25110;LLM&#30340;&#26816;&#26597;&#26080;&#27861;&#29702;&#35299;&#65292;&#21516;&#26102;&#20445;&#30041;&#21407;&#22987;&#25552;&#31034;&#30340;&#24847;&#22270;&#65292;&#20174;&#32780;&#30830;&#20445;&#29992;&#25143;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cloud-based large language models (LLMs) such as ChatGPT have increasingly become integral to daily operations, serving as vital tools across various applications. While these models offer substantial benefits in terms of accessibility and functionality, they also introduce significant privacy concerns: the transmission and storage of user data in cloud infrastructures pose substantial risks of data breaches and unauthorized access to sensitive information; even if the transmission and storage of data is encrypted, the LLM service provider itself still knows the real contents of the data, preventing individuals or entities from confidently using such LLM services. To address these concerns, this paper proposes a simple yet effective mechanism PromptCrypt to protect user privacy. It uses Emoji to encrypt the user inputs before sending them to LLM, effectively rendering them indecipherable to human or LLM's examination while retaining the original intent of the prompt, thus ensuring the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#26041;&#27861;LEAP&#65292;&#36890;&#36807;&#35753;&#27169;&#22411;&#20174;&#23569;&#37327;&#36755;&#20837;-&#36755;&#20986;&#31034;&#20363;&#20013;&#29359;&#38169;&#35823;&#65292;&#28982;&#21518;&#21453;&#24605;&#24182;&#23398;&#20064;&#20934;&#21017;&#65292;&#20174;&#32780;&#25552;&#21319;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.05403</link><description>&lt;p&gt;
&#20174;&#38169;&#35823;&#20013;&#23398;&#20064;&#30340;&#19978;&#19979;&#25991;&#20934;&#21017;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
In-Context Principle Learning from Mistakes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#26041;&#27861;LEAP&#65292;&#36890;&#36807;&#35753;&#27169;&#22411;&#20174;&#23569;&#37327;&#36755;&#20837;-&#36755;&#20986;&#31034;&#20363;&#20013;&#29359;&#38169;&#35823;&#65292;&#28982;&#21518;&#21453;&#24605;&#24182;&#23398;&#20064;&#20934;&#21017;&#65292;&#20174;&#32780;&#25552;&#21319;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65292;&#20063;&#31216;&#20026;&#23569;&#26679;&#26412;&#25552;&#31034;&#65289;&#24050;&#25104;&#20026;&#23558;LLMs&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#30340;&#26631;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#23569;&#37327;&#30340;&#36755;&#20837;-&#36755;&#20986;&#31034;&#20363;&#20013;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#22522;&#20110;ICL&#30340;&#26041;&#27861;&#21482;&#20174;&#27491;&#30830;&#30340;&#36755;&#20837;-&#36755;&#20986;&#23545;&#20013;&#23398;&#20064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#36825;&#19968;&#33539;&#20363;&#65292;&#36890;&#36807;&#20174;&#23569;&#32473;&#23450;&#30340;&#36755;&#20837;-&#36755;&#20986;&#31034;&#20363;&#20013;&#23398;&#20064;&#26356;&#22810;&#20869;&#23481;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#23398;&#20064;&#20934;&#21017;&#65288;LEAP&#65289;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#26377;&#24847;&#35825;&#20351;&#27169;&#22411;&#22312;&#36825;&#20123;&#23569;&#37327;&#31034;&#20363;&#20013;&#29359;&#38169;&#35823;&#65307;&#28982;&#21518;&#65292;&#25105;&#20204;&#21453;&#24605;&#36825;&#20123;&#38169;&#35823;&#65292;&#24182;&#20174;&#20013;&#23398;&#20064;&#26174;&#24335;&#30340;&#20219;&#21153;&#29305;&#23450;&#8220;&#20934;&#21017;&#8221;&#65292;&#36825;&#20123;&#20934;&#21017;&#26377;&#21161;&#20110;&#35299;&#20915;&#31867;&#20284;&#30340;&#38382;&#39064;&#24182;&#36991;&#20813;&#24120;&#35265;&#30340;&#38169;&#35823;&#65307;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#21407;&#22987;&#30340;&#23569;&#26679;&#26412;&#31034;&#20363;&#21644;&#36825;&#20123;&#23398;&#21040;&#30340;&#36890;&#29992;&#20934;&#21017;&#26469;&#25552;&#31034;&#27169;&#22411;&#22238;&#31572;&#26410;&#35265;&#36807;&#30340;&#27979;&#35797;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#65288;Hotpot QA&#65289;&#12289;&#25991;&#26412;&#38382;&#39064;&#22238;&#31572;&#65288;DROP&#65289;&#12289;Big-Bench&#22256;&#38590;&#25512;&#29702;&#21644;&#25968;&#23398;&#38382;&#39064;&#65288;GSM8K&#21644;MATH&#65289;&#22312;&#20869;&#30340;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#35780;&#20272;&#20102;LEAP&#65307;&#22312;&#25152;&#26377;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;LEAP&#37117;&#26377;&#25152;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning (ICL, also known as few-shot prompting) has been the standard method of adapting LLMs to downstream tasks, by learning from a few input-output examples. Nonetheless, all ICL-based approaches only learn from correct input-output pairs. In this paper, we revisit this paradigm, by learning more from the few given input-output examples. We introduce Learning Principles (LEAP): First, we intentionally induce the model to make mistakes on these few examples; then we reflect on these mistakes, and learn explicit task-specific "principles" from them, which help solve similar problems and avoid common mistakes; finally, we prompt the model to answer unseen test questions using the original few-shot examples and these learned general principles. We evaluate LEAP on a wide range of benchmarks, including multi-hop question answering (Hotpot QA), textual QA (DROP), Big-Bench Hard reasoning, and math problems (GSM8K and MATH); in all these benchmarks, LEAP improves the strongest 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;BIKED++&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;140&#19975;&#20010;&#33258;&#34892;&#36710;&#35774;&#35745;&#30340;&#22270;&#20687;&#21644;&#21442;&#25968;&#21270;CAD&#25991;&#20214;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#36328;&#27169;&#24577;&#39044;&#27979;&#27169;&#22411;&#65292;&#20363;&#22914;&#20351;&#29992;&#21442;&#25968;&#21270;&#34920;&#31034;&#26469;&#20934;&#30830;&#20272;&#35745;&#22270;&#20687;&#30340;&#29305;&#24449;&#23884;&#20837;&#12290;&#35813;&#25968;&#25454;&#38598;&#20063;&#24050;&#20844;&#24320;&#65292;&#21487;&#20379;&#30740;&#31350;&#32773;&#20351;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.05301</link><description>&lt;p&gt;
BIKED++&#65306;&#19968;&#20010;&#21253;&#21547;140&#19975;&#20010;&#33258;&#34892;&#36710;&#22270;&#20687;&#21644;&#21442;&#25968;&#21270;CAD&#35774;&#35745;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
BIKED++: A Multimodal Dataset of 1.4 Million Bicycle Image and Parametric CAD Designs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;BIKED++&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;140&#19975;&#20010;&#33258;&#34892;&#36710;&#35774;&#35745;&#30340;&#22270;&#20687;&#21644;&#21442;&#25968;&#21270;CAD&#25991;&#20214;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#36328;&#27169;&#24577;&#39044;&#27979;&#27169;&#22411;&#65292;&#20363;&#22914;&#20351;&#29992;&#21442;&#25968;&#21270;&#34920;&#31034;&#26469;&#20934;&#30830;&#20272;&#35745;&#22270;&#20687;&#30340;&#29305;&#24449;&#23884;&#20837;&#12290;&#35813;&#25968;&#25454;&#38598;&#20063;&#24050;&#20844;&#24320;&#65292;&#21487;&#20379;&#30740;&#31350;&#32773;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;140&#19975;&#20010;&#36890;&#36807;&#21442;&#25968;&#21270;&#34920;&#31034;&#21644;JSON&#25991;&#20214;&#20197;&#21450;&#26629;&#26684;&#21270;&#22270;&#20687;&#29983;&#25104;&#30340;&#33258;&#34892;&#36710;&#35774;&#35745;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#28210;&#26579;&#24341;&#25806;&#21644;BikeCAD&#36719;&#20214;&#29983;&#25104;&#21442;&#25968;&#21270;&#35774;&#35745;&#30340;&#30690;&#37327;&#22270;&#24418;&#32780;&#21019;&#24314;&#30340;&#12290;&#26412;&#25991;&#36824;&#20844;&#24320;&#20102;&#35813;&#28210;&#26579;&#24341;&#25806;&#12290;&#35813;&#25968;&#25454;&#38598;&#20855;&#26377;&#22810;&#31181;&#24212;&#29992;&#65292;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#30446;&#26631;&#26159;&#35757;&#32451;&#21442;&#25968;&#21270;&#21644;&#22522;&#20110;&#22270;&#20687;&#30340;&#35774;&#35745;&#34920;&#31034;&#20043;&#38388;&#30340;&#36328;&#27169;&#24577;&#39044;&#27979;&#27169;&#22411;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#35777;&#26126;&#21487;&#20197;&#36890;&#36807;&#35757;&#32451;&#39044;&#27979;&#27169;&#22411;&#30452;&#25509;&#20174;&#21442;&#25968;&#21270;&#34920;&#31034;&#20934;&#30830;&#20272;&#35745;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#23884;&#20837;&#12290;&#36825;&#26679;&#21487;&#20197;&#24314;&#31435;&#21442;&#25968;&#21270;&#33258;&#34892;&#36710;&#35774;&#35745;&#19982;&#25991;&#26412;&#23383;&#31526;&#20018;&#25110;&#21442;&#32771;&#22270;&#20687;&#20043;&#38388;&#30340;&#30456;&#20284;&#20851;&#31995;&#12290;&#32463;&#36807;&#35757;&#32451;&#30340;&#39044;&#27979;&#27169;&#22411;&#20063;&#24050;&#20844;&#24320;&#12290;&#35813;&#25968;&#25454;&#38598;&#21152;&#20837;&#20102;BIKED&#25968;&#25454;&#38598;&#31995;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a public dataset of 1.4 million procedurally-generated bicycle designs represented parametrically, as JSON files, and as rasterized images. The dataset is created through the use of a rendering engine which harnesses the BikeCAD software to generate vector graphics from parametric designs. This rendering engine is discussed in the paper and also released publicly alongside the dataset. Though this dataset has numerous applications, a principal motivation is the need to train cross-modal predictive models between parametric and image-based design representations. For example, we demonstrate that a predictive model can be trained to accurately estimate Contrastive Language-Image Pretraining (CLIP) embeddings from a parametric representation directly. This allows similarity relations to be established between parametric bicycle designs and text strings or reference images. Trained predictive models are also made public. The dataset joins the BIKED dataset family whic
&lt;/p&gt;</description></item><item><title>&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#21464;&#24418;&#22120;&#19990;&#30028;&#27169;&#22411;&#26469;&#39044;&#27979;&#26410;&#26469;&#22870;&#21169;&#24182;&#36827;&#34892;&#31574;&#30053;&#26799;&#24230;&#23398;&#20064;&#36890;&#24120;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#24120;&#29992;&#30340;&#21464;&#24418;&#22120;&#19990;&#30028;&#27169;&#22411;&#20250;&#20135;&#29983;&#36802;&#22238;&#30340;&#26799;&#24230;&#36335;&#24452;&#65292;&#23545;&#20110;&#38271;&#36317;&#31163;&#30340;&#31574;&#30053;&#26799;&#24230;&#26159;&#26377;&#23475;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20182;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Actions World Models (AWMs)&#30340;&#19990;&#30028;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#30452;&#25509;&#30340;&#26799;&#24230;&#20256;&#25773;&#36335;&#24452;&#12290;</title><link>https://arxiv.org/abs/2402.05290</link><description>&lt;p&gt;
&#21464;&#24418;&#22120;&#19990;&#30028;&#27169;&#22411;&#26159;&#21542;&#21487;&#20197;&#32473;&#20986;&#26356;&#22909;&#30340;&#31574;&#30053;&#26799;&#24230;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Transformer World Models Give Better Policy Gradients?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05290
&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#21464;&#24418;&#22120;&#19990;&#30028;&#27169;&#22411;&#26469;&#39044;&#27979;&#26410;&#26469;&#22870;&#21169;&#24182;&#36827;&#34892;&#31574;&#30053;&#26799;&#24230;&#23398;&#20064;&#36890;&#24120;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#24120;&#29992;&#30340;&#21464;&#24418;&#22120;&#19990;&#30028;&#27169;&#22411;&#20250;&#20135;&#29983;&#36802;&#22238;&#30340;&#26799;&#24230;&#36335;&#24452;&#65292;&#23545;&#20110;&#38271;&#36317;&#31163;&#30340;&#31574;&#30053;&#26799;&#24230;&#26159;&#26377;&#23475;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20182;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Actions World Models (AWMs)&#30340;&#19990;&#30028;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#30452;&#25509;&#30340;&#26799;&#24230;&#20256;&#25773;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#24378;&#21270;&#23398;&#20064;&#26469;&#35828;&#65292;&#19968;&#31181;&#33258;&#28982;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#23637;&#24320;&#31070;&#32463;&#32593;&#32476;&#19990;&#30028;&#27169;&#22411;&#26469;&#39044;&#27979;&#26410;&#26469;&#30340;&#22870;&#21169;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#22270;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#20197;&#23398;&#20064;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20856;&#22411;&#30340;&#19990;&#30028;&#27169;&#22411;&#20135;&#29983;&#20102;&#38590;&#20197;&#20248;&#21270;&#30340;&#25439;&#22833;&#22320;&#24418;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#38271;&#26102;&#38388;&#36328;&#24230;&#19978;&#36890;&#24120;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#21464;&#24418;&#22120;&#24050;&#30693;&#21487;&#20197;&#39640;&#25928;&#22320;&#20256;&#25773;&#38271;&#26102;&#38388;&#36328;&#24230;&#30340;&#26799;&#24230;&#65306;&#23427;&#20204;&#26159;&#21542;&#21487;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#21602;&#65311;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#24120;&#29992;&#30340;&#21464;&#24418;&#22120;&#19990;&#30028;&#27169;&#22411;&#20250;&#20135;&#29983;&#36802;&#22238;&#30340;&#26799;&#24230;&#36335;&#24452;&#65292;&#36825;&#23545;&#20110;&#38271;&#36317;&#31163;&#30340;&#31574;&#30053;&#26799;&#24230;&#26159;&#26377;&#23475;&#30340;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31867;&#31216;&#20026;Actions World Models (AWMs)&#30340;&#19990;&#30028;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#20379;&#26356;&#30452;&#25509;&#30340;&#26799;&#24230;&#20256;&#25773;&#36335;&#24452;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;AWMs&#38598;&#25104;&#21040;&#19968;&#20010;&#31574;&#30053;&#26799;&#24230;&#30340;&#26694;&#26550;&#20013;&#65292;&#24378;&#35843;&#20102;&#32593;&#32476;&#26550;&#26500;&#19982;&#31574;&#30053;&#26799;&#24230;&#26356;&#26032;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;AWMs&#21487;&#20197;&#20135;&#29983;&#21487;&#20248;&#21270;&#30340;&#26799;&#24230;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
A natural approach for reinforcement learning is to predict future rewards by unrolling a neural network world model, and to backpropagate through the resulting computational graph to learn a policy. However, this method often becomes impractical for long horizons since typical world models induce hard-to-optimize loss landscapes. Transformers are known to efficiently propagate gradients overlong horizons: could they be the solution to this problem? Surprisingly, we show that commonly-used transformer world models produce circuitous gradient paths, which can be detrimental to long-range policy gradients. To tackle this challenge, we propose a class of world models called Actions World Models (AWMs), designed to provide more direct routes for gradient propagation. We integrate such AWMs into a policy gradient framework that underscores the relationship between network architectures and the policy gradient updates they inherently represent. We demonstrate that AWMs can generate optimizat
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20195;&#26367;MOOCs&#20013;&#21516;&#20276;&#35780;&#20998;&#30340;&#21487;&#34892;&#24615;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#35268;&#27169;&#22312;&#32447;&#24320;&#25918;&#35838;&#31243;&#20013;&#35780;&#20272;&#23398;&#29983;&#20889;&#20316;&#20219;&#21153;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03776</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;MOOCs&#35780;&#20998;&#32773;
&lt;/p&gt;
&lt;p&gt;
Large Language Models As MOOCs Graders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03776
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20195;&#26367;MOOCs&#20013;&#21516;&#20276;&#35780;&#20998;&#30340;&#21487;&#34892;&#24615;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#35268;&#27169;&#22312;&#32447;&#24320;&#25918;&#35838;&#31243;&#20013;&#35780;&#20272;&#23398;&#29983;&#20889;&#20316;&#20219;&#21153;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22312;&#32447;&#24320;&#25918;&#35838;&#31243;&#65288;MOOCs&#65289;&#20026;&#25317;&#26377;&#30005;&#33041;&#21644;&#20114;&#32852;&#32593;&#35775;&#38382;&#26435;&#38480;&#30340;&#20840;&#29699;&#20219;&#20309;&#20154;&#25552;&#20379;&#20813;&#36153;&#25945;&#32946;&#30340;&#26426;&#20250;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#36825;&#20123;&#35838;&#31243;&#30340;&#22823;&#35268;&#27169;&#27880;&#20876;&#24847;&#21619;&#30528;&#19968;&#20301;&#25945;&#24072;&#20960;&#20046;&#19981;&#21487;&#33021;&#35780;&#20272;&#27599;&#20010;&#23398;&#29983;&#30340;&#20889;&#20316;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#21516;&#20276;&#35780;&#20998;&#36890;&#24120;&#26159;&#39318;&#36873;&#26041;&#27861;&#65292;&#36890;&#24120;&#30001;&#31616;&#21333;&#26126;&#20102;&#30340;&#35780;&#20998;&#26631;&#20934;&#25351;&#23548;&#12290;&#28982;&#32780;&#65292;&#21516;&#20276;&#35780;&#20998;&#22312;&#21487;&#38752;&#24230;&#21644;&#26377;&#25928;&#24615;&#26041;&#38754;&#24120;&#24120;&#23384;&#22312;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;18&#20010;&#19981;&#21516;&#30340;&#22330;&#26223;&#65292;&#25506;&#32034;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26367;&#20195;MOOCs&#20013;&#30340;&#21516;&#20276;&#35780;&#20998;&#30340;&#21487;&#34892;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;LLMs&#65306;GPT-4&#21644;GPT-3.5&#65292;&#24182;&#28085;&#30422;&#19977;&#38376;&#19981;&#21516;&#30340;&#35838;&#31243;&#65306;&#20837;&#38376;&#22825;&#25991;&#23398;&#65292;&#22825;&#20307;&#29983;&#29289;&#23398;&#20197;&#21450;&#22825;&#25991;&#23398;&#30340;&#21382;&#21490;&#19982;&#21746;&#23398;&#12290;&#20026;&#20102;&#35757;&#32451;LLMs&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#22522;&#20110;&#38646;-shot&#36830;&#32493;&#24605;&#32771;&#65288;Zero-shot-CoT&#65289;&#25552;&#31034;&#25216;&#26415;&#30340;&#21464;&#31181;&#30340;&#19977;&#20010;&#19981;&#21516;&#25552;&#31034;&#65306;&#32467;&#21512;Zero-shot-CoT&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Massive open online courses (MOOCs) unlock the doors to free education for anyone around the globe with access to a computer and the internet. Despite this democratization of learning, the massive enrollment in these courses means it is almost impossible for one instructor to assess every student's writing assignment. As a result, peer grading, often guided by a straightforward rubric, is the method of choice. While convenient, peer grading often falls short in terms of reliability and validity. In this study, using 18 distinct settings, we explore the feasibility of leveraging large language models (LLMs) to replace peer grading in MOOCs. Specifically, we focus on two state-of-the-art LLMs: GPT-4 and GPT-3.5, across three distinct courses: Introductory Astronomy, Astrobiology, and the History and Philosophy of Astronomy. To instruct LLMs, we use three different prompts based on a variant of the zero-shot chain-of-thought (Zero-shot-CoT) prompting technique: Zero-shot-CoT combined with
&lt;/p&gt;</description></item><item><title>RL-VLM-F&#26159;&#19968;&#31181;&#36890;&#36807;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#26377;&#25928;&#30340;&#22870;&#21169;&#20989;&#25968;&#21644;&#31574;&#30053;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#20013;&#22870;&#21169;&#35774;&#35745;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.03681</link><description>&lt;p&gt;
RL-VLM-F: &#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03681
&lt;/p&gt;
&lt;p&gt;
RL-VLM-F&#26159;&#19968;&#31181;&#36890;&#36807;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#26377;&#25928;&#30340;&#22870;&#21169;&#20989;&#25968;&#21644;&#31574;&#30053;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#20013;&#22870;&#21169;&#35774;&#35745;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#20013;&#30340;&#22870;&#21169;&#35774;&#35745;&#19968;&#30452;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#21644;&#21453;&#22797;&#35797;&#38169;&#30340;&#36807;&#31243;&#26469;&#35774;&#35745;&#26377;&#25928;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#29983;&#25104;&#22870;&#21169;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20195;&#29702;&#23398;&#20064;&#26032;&#20219;&#21153;&#65292;&#21482;&#20351;&#29992;&#20219;&#21153;&#30446;&#26631;&#30340;&#25991;&#26412;&#25551;&#36848;&#21644;&#20195;&#29702;&#30340;&#35270;&#35273;&#35266;&#27979;&#65292;&#24182;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#21453;&#39304;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20851;&#38190;&#26159;&#36890;&#36807;&#26597;&#35810;&#36825;&#20123;&#27169;&#22411;&#65292;&#22522;&#20110;&#20219;&#21153;&#30446;&#26631;&#30340;&#25991;&#26412;&#25551;&#36848;&#32473;&#20986;&#23545;&#20195;&#29702;&#30340;&#22270;&#20687;&#35266;&#27979;&#30340;&#20559;&#22909;&#65292;&#24182;&#20174;&#20559;&#22909;&#26631;&#31614;&#20013;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#35201;&#27714;&#36825;&#20123;&#27169;&#22411;&#36755;&#20986;&#21407;&#22987;&#22870;&#21169;&#20998;&#25968;&#65292;&#36825;&#21487;&#33021;&#23384;&#22312;&#22122;&#38899;&#21644;&#19981;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;RL-VLM-F&#22312;&#21508;&#31181;&#39046;&#22495;&#20013;&#25104;&#21151;&#22320;&#20135;&#29983;&#20102;&#26377;&#25928;&#30340;&#22870;&#21169;&#21644;&#31574;&#30053;&#65292;&#21253;&#25324;&#32463;&#20856;&#25511;&#21046;&#20197;&#21450;&#21018;&#24615;&#21644;&#28789;&#27963;&#25805;&#32437;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reward engineering has long been a challenge in Reinforcement Learning (RL) research, as it often requires extensive human effort and iterative processes of trial-and-error to design effective reward functions. In this paper, we propose RL-VLM-F, a method that automatically generates reward functions for agents to learn new tasks, using only a text description of the task goal and the agent's visual observations, by leveraging feedbacks from vision language foundation models (VLMs). The key to our approach is to query these models to give preferences over pairs of the agent's image observations based on the text description of the task goal, and then learn a reward function from the preference labels, rather than directly prompting these models to output a raw reward score, which can be noisy and inconsistent. We demonstrate that RL-VLM-F successfully produces effective rewards and policies across various domains - including classic control, as well as manipulation of rigid, articulate
&lt;/p&gt;</description></item><item><title>&#25193;&#25955;&#19990;&#30028;&#27169;&#22411;&#26159;&#19968;&#20010;&#33021;&#22815;&#39044;&#27979;&#22810;&#27493;&#26410;&#26469;&#29366;&#24577;&#21644;&#22870;&#21169;&#30340;&#26465;&#20214;&#24615;&#25193;&#25955;&#27169;&#22411;&#65292;&#22312;&#27169;&#22411;&#25928;&#26524;&#21644;&#24615;&#33021;&#26041;&#38754;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#19968;&#27493;&#21160;&#21147;&#23398;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.03570</link><description>&lt;p&gt;
&#25193;&#25955;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion World Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03570
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#19990;&#30028;&#27169;&#22411;&#26159;&#19968;&#20010;&#33021;&#22815;&#39044;&#27979;&#22810;&#27493;&#26410;&#26469;&#29366;&#24577;&#21644;&#22870;&#21169;&#30340;&#26465;&#20214;&#24615;&#25193;&#25955;&#27169;&#22411;&#65292;&#22312;&#27169;&#22411;&#25928;&#26524;&#21644;&#24615;&#33021;&#26041;&#38754;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#19968;&#27493;&#21160;&#21147;&#23398;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#25193;&#25955;&#19990;&#30028;&#27169;&#22411;&#65288;DWM&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#33021;&#22815;&#21516;&#26102;&#39044;&#27979;&#22810;&#27493;&#30340;&#26410;&#26469;&#29366;&#24577;&#21644;&#22870;&#21169;&#12290;&#19982;&#20256;&#32479;&#30340;&#19968;&#27493;&#21160;&#21147;&#23398;&#27169;&#22411;&#30456;&#21453;&#65292;DWM&#36890;&#36807;&#21333;&#20010;&#21069;&#21521;&#20256;&#36882;&#25552;&#20379;&#20102;&#38271;&#26102;&#31243;&#30340;&#39044;&#27979;&#65292;&#28040;&#38500;&#20102;&#36882;&#24402;&#26597;&#35810;&#30340;&#38656;&#35201;&#12290;&#25105;&#20204;&#23558;DWM&#25972;&#21512;&#21040;&#22522;&#20110;&#27169;&#22411;&#30340;&#20215;&#20540;&#20272;&#35745;&#20013;&#65292;&#20854;&#20013;&#30701;&#26399;&#22238;&#25253;&#36890;&#36807;&#20174;DWM&#20013;&#37319;&#26679;&#30340;&#26410;&#26469;&#36712;&#36857;&#36827;&#34892;&#27169;&#25311;&#12290;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#65292;DWM&#21487;&#20197;&#34987;&#35270;&#20026;&#36890;&#36807;&#29983;&#25104;&#24314;&#27169;&#26469;&#23454;&#29616;&#20445;&#23432;&#30340;&#20540;&#27491;&#21017;&#21270;&#12290;&#21478;&#22806;&#65292;&#23427;&#20063;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#31181;&#25968;&#25454;&#28304;&#65292;&#20351;&#31163;&#32447;Q&#23398;&#20064;&#33021;&#22815;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;D4RL&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;DWM&#23545;&#38271;&#26102;&#31243;&#27169;&#25311;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#32477;&#23545;&#24615;&#33021;&#26041;&#38754;&#65292;DWM&#26174;&#33879;&#36229;&#36807;&#20102;&#19968;&#27493;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#24615;&#33021;&#25552;&#39640;&#20102;44%&#65292;&#24182;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Diffusion World Model (DWM), a conditional diffusion model capable of predicting multistep future states and rewards concurrently. As opposed to traditional one-step dynamics models, DWM offers long-horizon predictions in a single forward pass, eliminating the need for recursive quires. We integrate DWM into model-based value estimation, where the short-term return is simulated by future trajectories sampled from DWM. In the context of offline reinforcement learning, DWM can be viewed as a conservative value regularization through generative modeling. Alternatively, it can be seen as a data source that enables offline Q-learning with synthetic data. Our experiments on the D4RL dataset confirm the robustness of DWM to long-horizon simulation. In terms of absolute performance, DWM significantly surpasses one-step dynamics models with a $44\%$ performance gain, and achieves state-of-the-art performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#20132;&#36890;&#30446;&#30340;&#22320;&#39044;&#27979;&#27169;&#22411;&#65288;EBM&#65289;&#65292;&#22312;&#22810;&#20010;&#28151;&#21512;&#20132;&#36890;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#24182;&#33021;&#22815;&#25552;&#20379;&#29305;&#24449;&#37325;&#35201;&#24615;&#21644;&#20132;&#20114;&#20316;&#29992;&#30340;&#20998;&#26512;&#20197;&#21450;&#39044;&#27979;&#35299;&#37322;&#30340;&#36136;&#37327;&#31034;&#20363;&#12290;</title><link>https://arxiv.org/abs/2402.03457</link><description>&lt;p&gt;
&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#20132;&#36890;&#30446;&#30340;&#22320;&#39044;&#27979;&#26041;&#27861;&#8212;&#8212;&#21487;&#35299;&#37322;&#25311;&#21512;&#26426;&#22120;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Efficient and Interpretable Traffic Destination Prediction using Explainable Boosting Machines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#20132;&#36890;&#30446;&#30340;&#22320;&#39044;&#27979;&#27169;&#22411;&#65288;EBM&#65289;&#65292;&#22312;&#22810;&#20010;&#28151;&#21512;&#20132;&#36890;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#24182;&#33021;&#22815;&#25552;&#20379;&#29305;&#24449;&#37325;&#35201;&#24615;&#21644;&#20132;&#20114;&#20316;&#29992;&#30340;&#20998;&#26512;&#20197;&#21450;&#39044;&#27979;&#35299;&#37322;&#30340;&#36136;&#37327;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#31934;&#30830;&#30340;&#20132;&#36890;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#23545;&#20110;&#23454;&#29616;&#23436;&#20840;&#33258;&#21160;&#39550;&#39542;&#33267;&#20851;&#37325;&#35201;&#12290;&#21508;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24050;&#34987;&#24212;&#29992;&#20110;&#35299;&#20915;&#27492;&#25361;&#25112;&#65292;&#20294;&#23427;&#20204;&#30340;&#40657;&#30418;&#29305;&#24615;&#38480;&#21046;&#20102;&#22312;&#37096;&#32626;&#31995;&#32479;&#20013;&#30340;&#36879;&#26126;&#24230;&#21644;&#35843;&#35797;&#33021;&#21147;&#12290;&#29627;&#29827;&#30418;&#27169;&#22411;&#36890;&#36807;&#31867;&#20284;&#20110;GAM&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#21487;&#21152;&#24615;&#27169;&#22411;&#65292;&#31216;&#20026;EBM&#65292;&#29992;&#20110;&#19977;&#20010;&#27969;&#34892;&#30340;&#28151;&#21512;&#20132;&#36890;&#25968;&#25454;&#38598;&#65288;SDD&#12289;InD&#21644;Argoverse&#65289;&#30340;&#20132;&#36890;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;EBM&#27169;&#22411;&#22312;&#39044;&#27979;SDD&#21644;InD&#20013;&#30340;&#34892;&#20154;&#30446;&#30340;&#22320;&#26102;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#21516;&#26102;&#23545;&#20197;&#36710;&#36742;&#20026;&#20027;&#30340;Argoverse&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#36866;&#24230;&#30340;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36879;&#26126;&#30340;&#35757;&#32451;&#27169;&#22411;&#20351;&#25105;&#20204;&#33021;&#22815;&#20998;&#26512;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#21644;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#25552;&#20379;&#39044;&#27979;&#35299;&#37322;&#30340;&#36136;&#37327;&#31034;&#20363;&#12290;&#20840;&#38754;&#30340;&#35757;&#32451;&#20195;&#30721;&#23558;&#22312;&#35770;&#25991;&#21457;&#34920;&#21518;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing accurate models for traffic trajectory predictions is crucial for achieving fully autonomous driving. Various deep neural network models have been employed to address this challenge, but their black-box nature hinders transparency and debugging capabilities in a deployed system. Glass-box models offer a solution by providing full interpretability through methods like \ac{GAM}. In this study, we evaluate an efficient additive model called \ac{EBM} for traffic prediction on three popular mixed traffic datasets: \ac{SDD}, \ac{InD}, and Argoverse. Our results show that the \ac{EBM} models perform competitively in predicting pedestrian destinations within \ac{SDD} and \ac{InD} while providing modest predictions for vehicle-dominant Argoverse dataset. Additionally, our transparent trained models allow us to analyse feature importance and interactions, as well as provide qualitative examples of predictions explanation. The full training code will be made public upon publication.
&lt;/p&gt;</description></item><item><title>BGE M3-&#23884;&#20837;&#26159;&#19968;&#31181;&#26032;&#30340;&#22810;&#35821;&#35328;&#12289;&#22810;&#21151;&#33021;&#21644;&#22810;&#31890;&#24230;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#25903;&#25345;&#36229;&#36807;100&#31181;&#24037;&#20316;&#35821;&#35328;&#65292;&#24182;&#22312;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#26816;&#32034;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23427;&#33021;&#22815;&#21516;&#26102;&#25191;&#34892;&#23494;&#38598;&#26816;&#32034;&#12289;&#22810;&#21521;&#37327;&#26816;&#32034;&#21644;&#31232;&#30095;&#26816;&#32034;&#65292;&#24182;&#33021;&#22788;&#29702;&#19981;&#21516;&#31890;&#24230;&#30340;&#36755;&#20837;&#12290;&#20854;&#26377;&#25928;&#35757;&#32451;&#21253;&#25324;&#20102;&#19968;&#31181;&#33258;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#21644;&#20248;&#21270;&#30340;&#25209;&#22788;&#29702;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.03216</link><description>&lt;p&gt;
BGE M3-&#23884;&#20837;&#65306;&#36890;&#36807;&#33258;&#30693;&#35782;&#33976;&#39311;&#23454;&#29616;&#22810;&#35821;&#35328;&#12289;&#22810;&#21151;&#33021;&#21644;&#22810;&#31890;&#24230;&#30340;&#25991;&#26412;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03216
&lt;/p&gt;
&lt;p&gt;
BGE M3-&#23884;&#20837;&#26159;&#19968;&#31181;&#26032;&#30340;&#22810;&#35821;&#35328;&#12289;&#22810;&#21151;&#33021;&#21644;&#22810;&#31890;&#24230;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#25903;&#25345;&#36229;&#36807;100&#31181;&#24037;&#20316;&#35821;&#35328;&#65292;&#24182;&#22312;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#26816;&#32034;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23427;&#33021;&#22815;&#21516;&#26102;&#25191;&#34892;&#23494;&#38598;&#26816;&#32034;&#12289;&#22810;&#21521;&#37327;&#26816;&#32034;&#21644;&#31232;&#30095;&#26816;&#32034;&#65292;&#24182;&#33021;&#22788;&#29702;&#19981;&#21516;&#31890;&#24230;&#30340;&#36755;&#20837;&#12290;&#20854;&#26377;&#25928;&#35757;&#32451;&#21253;&#25324;&#20102;&#19968;&#31181;&#33258;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#21644;&#20248;&#21270;&#30340;&#25209;&#22788;&#29702;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23884;&#20837;&#27169;&#22411;&#65292;&#31216;&#20026;M3-&#23884;&#20837;&#65292;&#20197;&#20854;&#22312;&#22810;&#35821;&#35328;&#12289;&#22810;&#21151;&#33021;&#21644;&#22810;&#31890;&#24230;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#32780;&#33879;&#31216;&#12290;&#23427;&#21487;&#20197;&#25903;&#25345;&#36229;&#36807;100&#31181;&#24037;&#20316;&#35821;&#35328;&#65292;&#22312;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#26816;&#32034;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;&#23427;&#21487;&#20197;&#21516;&#26102;&#25191;&#34892;&#23884;&#20837;&#27169;&#22411;&#30340;&#19977;&#31181;&#24120;&#35265;&#26816;&#32034;&#21151;&#33021;&#65306;&#23494;&#38598;&#26816;&#32034;&#12289;&#22810;&#21521;&#37327;&#26816;&#32034;&#21644;&#31232;&#30095;&#26816;&#32034;&#65292;&#20026;&#29616;&#23454;&#19990;&#30028;&#30340;IR&#24212;&#29992;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#27169;&#22411;&#22522;&#30784;&#12290;&#23427;&#33021;&#22815;&#22788;&#29702;&#19981;&#21516;&#31890;&#24230;&#30340;&#36755;&#20837;&#65292;&#20174;&#30701;&#21477;&#21040;&#38271;&#36798;8192&#20010;&#26631;&#35760;&#30340;&#25991;&#26723;&#12290;M3-&#23884;&#20837;&#30340;&#26377;&#25928;&#35757;&#32451;&#21253;&#25324;&#20197;&#19979;&#25216;&#26415;&#36129;&#29486;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#26469;&#33258;&#19981;&#21516;&#26816;&#32034;&#21151;&#33021;&#30340;&#30456;&#20851;&#24615;&#20998;&#25968;&#25972;&#21512;&#20026;&#25945;&#24072;&#20449;&#21495;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#36136;&#37327;&#12290;&#25105;&#20204;&#36824;&#20248;&#21270;&#20102;&#25209;&#22788;&#29702;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a new embedding model, called M3-Embedding, which is distinguished for its versatility in Multi-Linguality, Multi-Functionality, and Multi-Granularity. It can support more than 100 working languages, leading to new state-of-the-art performances on multi-lingual and cross-lingual retrieval tasks. It can simultaneously perform the three common retrieval functionalities of embedding model: dense retrieval, multi-vector retrieval, and sparse retrieval, which provides a unified model foundation for real-world IR applications. It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens. The effective training of M3-Embedding involves the following technical contributions. We propose a novel self-knowledge distillation approach, where the relevance scores from different retrieval functionalities can be integrated as the teacher signal to enhance the training quality. We also optimize the batching strat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25351;&#20986;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#27745;&#26579;&#30340;&#26816;&#27979;&#26041;&#27861;&#22312;&#38754;&#23545;&#24694;&#24847;&#27169;&#22411;&#25552;&#20379;&#32773;&#30340;&#26377;&#24847;&#27745;&#26579;&#26102;&#23384;&#22312;&#28431;&#27934;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#27745;&#26579;&#25216;&#26415;&#65288;EAL&#65289;&#26469;&#26174;&#33879;&#25552;&#39640;&#22522;&#20934;&#27979;&#35797;&#24615;&#33021;&#19988;&#36867;&#36991;&#24403;&#21069;&#30340;&#26816;&#27979;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.02823</link><description>&lt;p&gt;
&#36867;&#36991;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#27745;&#26579;&#26816;&#27979;&#65288;&#22826;&#65289;&#23481;&#26131;
&lt;/p&gt;
&lt;p&gt;
Evading Data Contamination Detection for Language Models is (too) Easy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25351;&#20986;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#27745;&#26579;&#30340;&#26816;&#27979;&#26041;&#27861;&#22312;&#38754;&#23545;&#24694;&#24847;&#27169;&#22411;&#25552;&#20379;&#32773;&#30340;&#26377;&#24847;&#27745;&#26579;&#26102;&#23384;&#22312;&#28431;&#27934;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#27745;&#26579;&#25216;&#26415;&#65288;EAL&#65289;&#26469;&#26174;&#33879;&#25552;&#39640;&#22522;&#20934;&#27979;&#35797;&#24615;&#33021;&#19988;&#36867;&#36991;&#24403;&#21069;&#30340;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24191;&#27867;&#20351;&#29992;&#65292;&#23427;&#20204;&#22312;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#24615;&#33021;&#32463;&#24120;&#25351;&#23548;&#29992;&#25143;&#23545;&#19968;&#20010;&#27169;&#22411;&#19982;&#21478;&#19968;&#20010;&#27169;&#22411;&#30340;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#25152;&#35757;&#32451;&#30340;&#22823;&#37327;&#25968;&#25454;&#21487;&#33021;&#20250;&#24847;&#22806;&#22320;&#19982;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#21457;&#29983;&#27745;&#26579;&#65292;&#20174;&#32780;&#25439;&#23475;&#24615;&#33021;&#35780;&#20272;&#12290;&#23613;&#31649;&#26368;&#36817;&#24320;&#21457;&#20102;&#19968;&#20123;&#27745;&#26579;&#26816;&#27979;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#24573;&#35270;&#20102;&#24694;&#24847;&#27169;&#22411;&#25552;&#20379;&#32773;&#26377;&#24847;&#36827;&#34892;&#27745;&#26579;&#20197;&#36991;&#20813;&#34987;&#26816;&#27979;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#24773;&#20917;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#23545;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#30340;&#21487;&#20449;&#24230;&#20135;&#29983;&#20102;&#24576;&#30097;&#12290;&#20026;&#20102;&#26356;&#20005;&#26684;&#22320;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27169;&#22411;&#25552;&#20379;&#32773;&#21644;&#27745;&#26579;&#26816;&#27979;&#26041;&#27861;&#30340;&#20998;&#31867;&#65292;&#36825;&#25581;&#31034;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#28431;&#27934;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;EAL&#36825;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#27745;&#26579;&#25216;&#26415;&#65292;&#26126;&#26174;&#25552;&#39640;&#20102;&#22522;&#20934;&#27979;&#35797;&#30340;&#24615;&#33021;&#65292;&#24182;&#23436;&#20840;&#36867;&#36991;&#20102;&#24403;&#21069;&#30340;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models are widespread, with their performance on benchmarks frequently guiding user preferences for one model over another. However, the vast amount of data these models are trained on can inadvertently lead to contamination with public benchmarks, thus compromising performance measurements. While recently developed contamination detection methods try to address this issue, they overlook the possibility of deliberate contamination by malicious model providers aiming to evade detection. We argue that this setting is of crucial importance as it casts doubt on the reliability of public benchmarks. To more rigorously study this issue, we propose a categorization of both model providers and contamination detection methods. This reveals vulnerabilities in existing methods that we exploit with EAL, a simple yet effective contamination technique that significantly inflates benchmark performance while completely evading current detection methods.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#20027;&#35201;&#20171;&#32461;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#12290;&#36890;&#36807;&#32508;&#21512;&#20998;&#26512;&#26368;&#36817;&#30340;&#25991;&#29486;&#65292;&#25506;&#35752;&#20102;&#20020;&#24202;&#35786;&#26029;&#20013;&#21508;&#31181;&#25968;&#25454;&#27169;&#24577;&#30340;&#24212;&#29992;&#20197;&#21450;&#34701;&#21512;&#25216;&#26415;&#30340;&#35780;&#20272;&#12290;&#37325;&#28857;&#20851;&#27880;&#24433;&#20687;&#25968;&#25454;&#30340;&#24212;&#29992;&#65292;&#24182;&#20171;&#32461;&#20102;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#21644;&#35757;&#32451;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.02460</link><description>&lt;p&gt;
&#21307;&#30103;&#20445;&#20581;&#20013;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Review of multimodal machine learning approaches in healthcare
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02460
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#20027;&#35201;&#20171;&#32461;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#12290;&#36890;&#36807;&#32508;&#21512;&#20998;&#26512;&#26368;&#36817;&#30340;&#25991;&#29486;&#65292;&#25506;&#35752;&#20102;&#20020;&#24202;&#35786;&#26029;&#20013;&#21508;&#31181;&#25968;&#25454;&#27169;&#24577;&#30340;&#24212;&#29992;&#20197;&#21450;&#34701;&#21512;&#25216;&#26415;&#30340;&#35780;&#20272;&#12290;&#37325;&#28857;&#20851;&#27880;&#24433;&#20687;&#25968;&#25454;&#30340;&#24212;&#29992;&#65292;&#24182;&#20171;&#32461;&#20102;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#21644;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20256;&#32479;&#19978;&#27880;&#37325;&#20351;&#29992;&#21333;&#19968;&#27169;&#24577;&#30340;&#25968;&#25454;&#65292;&#38480;&#21046;&#20102;&#20854;&#26377;&#25928;&#22797;&#21046;&#20020;&#24202;&#23454;&#36341;&#20013;&#25972;&#21512;&#22810;&#31181;&#20449;&#24687;&#26469;&#28304;&#20197;&#25913;&#21892;&#20915;&#31574;&#30340;&#33021;&#21147;&#12290;&#20020;&#24202;&#21307;&#29983;&#36890;&#24120;&#20381;&#36182;&#21508;&#31181;&#25968;&#25454;&#26469;&#28304;&#65292;&#21253;&#25324;&#24739;&#32773;&#30340;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#12289;&#23454;&#39564;&#23460;&#25968;&#25454;&#12289;&#29983;&#21629;&#20307;&#24449;&#21644;&#21508;&#31181;&#24433;&#20687;&#25968;&#25454;&#27169;&#24577;&#26469;&#20570;&#20986;&#26126;&#26234;&#30340;&#20915;&#31574;&#24182;&#23545;&#20854;&#21457;&#29616;&#36827;&#34892;&#19978;&#19979;&#25991;&#21270;&#12290;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#20419;&#36827;&#20102;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#26356;&#39640;&#25928;&#34701;&#21512;&#65292;&#20174;&#32780;&#20135;&#29983;&#26356;&#22909;&#22320;&#20195;&#34920;&#21307;&#29983;&#26041;&#27861;&#30340;&#24212;&#29992;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#21307;&#30103;&#20445;&#20581;&#20013;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#32508;&#36848;&#65292;&#20840;&#38754;&#27010;&#36848;&#20102;&#26368;&#36817;&#30340;&#25991;&#29486;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20020;&#24202;&#35786;&#26029;&#20013;&#20351;&#29992;&#30340;&#21508;&#31181;&#25968;&#25454;&#27169;&#24577;&#65292;&#29305;&#21035;&#24378;&#35843;&#24433;&#20687;&#25968;&#25454;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#34701;&#21512;&#25216;&#26415;&#65292;&#25506;&#32034;&#20102;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#30740;&#31350;&#24120;&#35265;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning methods in healthcare have traditionally focused on using data from a single modality, limiting their ability to effectively replicate the clinical practice of integrating multiple sources of information for improved decision making. Clinicians typically rely on a variety of data sources including patients' demographic information, laboratory data, vital signs and various imaging data modalities to make informed decisions and contextualise their findings. Recent advances in machine learning have facilitated the more efficient incorporation of multimodal data, resulting in applications that better represent the clinician's approach. Here, we provide a review of multimodal machine learning approaches in healthcare, offering a comprehensive overview of recent literature. We discuss the various data modalities used in clinical diagnosis, with a particular emphasis on imaging data. We evaluate fusion techniques, explore existing multimodal datasets and examine common traini
&lt;/p&gt;</description></item><item><title>MetaOptimize&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#23398;&#20064;&#29575;&#26469;&#20248;&#21270;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#20803;&#21442;&#25968;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02342</link><description>&lt;p&gt;
MetaOptimize&#65306;&#19968;&#20010;&#20248;&#21270;&#27493;&#38271;&#21644;&#20854;&#20182;&#20803;&#21442;&#25968;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MetaOptimize: A Framework for Optimizing Step Sizes and Other Meta-parameters
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02342
&lt;/p&gt;
&lt;p&gt;
MetaOptimize&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#23398;&#20064;&#29575;&#26469;&#20248;&#21270;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#20803;&#21442;&#25968;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#20248;&#21270;&#20803;&#21442;&#25968;&#65288;&#21363;&#36229;&#21442;&#25968;&#65289;&#30340;&#25361;&#25112;&#65292;&#36825;&#26159;&#24433;&#21709;&#35757;&#32451;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;MetaOptimize&#26694;&#26550;&#65292;&#25670;&#33073;&#20102;&#35745;&#31639;&#26114;&#36149;&#30340;&#20256;&#32479;&#20803;&#21442;&#25968;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#20803;&#21442;&#25968;&#65292;&#29305;&#21035;&#26159;&#27493;&#38271;&#65288;&#20063;&#31216;&#20026;&#23398;&#20064;&#29575;&#65289;&#65292;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MetaOptimize&#21487;&#20197;&#36866;&#29992;&#20110;&#20219;&#20309;&#19968;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23454;&#26102;&#35843;&#25972;&#27493;&#38271;&#65292;&#36890;&#36807;&#26410;&#26469;&#25439;&#22833;&#30340;&#25240;&#29616;&#24635;&#21644;&#26469;&#26368;&#23567;&#21270;&#19968;&#31181;&#29305;&#23450;&#24418;&#24335;&#30340;&#36951;&#25022;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;MetaOptimize&#30340;&#20302;&#22797;&#26434;&#24230;&#21464;&#20307;&#65292;&#32467;&#21512;&#20854;&#36866;&#24212;&#22810;&#20010;&#20248;&#21270;&#31639;&#27861;&#30340;&#33021;&#21147;&#65292;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#19982;&#25163;&#24037;&#35774;&#35745;&#30340;&#23398;&#20064;&#29575;&#35745;&#21010;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the challenge of optimizing meta-parameters (i.e., hyperparameters) in machine learning algorithms, a critical factor influencing training efficiency and model performance. Moving away from the computationally expensive traditional meta-parameter search methods, we introduce MetaOptimize framework that dynamically adjusts meta-parameters, particularly step sizes (also known as learning rates), during training. More specifically, MetaOptimize can wrap around any first-order optimization algorithm, tuning step sizes on the fly to minimize a specific form of regret that accounts for long-term effect of step sizes on training, through a discounted sum of future losses. We also introduce low complexity variants of MetaOptimize that, in conjunction with its adaptability to multiple optimization algorithms, demonstrate performance competitive to those of best hand-crafted learning rate schedules across various machine learning applications.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#20855;&#26377;&#22823;&#22411;&#27169;&#22411;&#30340;&#35270;&#35273;&#38556;&#30861;&#36741;&#21161;&#65292;&#24182;&#36890;&#36807;&#22522;&#20934;&#23454;&#39564;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#36827;&#19968;&#27493;&#25512;&#21160;&#20102;&#35270;&#35273;&#38556;&#30861;&#36741;&#21161;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2402.01735</link><description>&lt;p&gt;
VIALM&#65306;&#20851;&#20110;&#20855;&#26377;&#22823;&#22411;&#27169;&#22411;&#30340;&#35270;&#35273;&#38556;&#30861;&#36741;&#21161;&#30340;&#35843;&#26597;&#21644;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
VIALM: A Survey and Benchmark of Visually Impaired Assistance with Large Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01735
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#20855;&#26377;&#22823;&#22411;&#27169;&#22411;&#30340;&#35270;&#35273;&#38556;&#30861;&#36741;&#21161;&#65292;&#24182;&#36890;&#36807;&#22522;&#20934;&#23454;&#39564;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#36827;&#19968;&#27493;&#25512;&#21160;&#20102;&#35270;&#35273;&#38556;&#30861;&#36741;&#21161;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#38556;&#30861;&#36741;&#21161; (VIA) &#26088;&#22312;&#33258;&#21160;&#24110;&#21161;&#35270;&#35273;&#38556;&#30861;&#32773; (VI) &#22788;&#29702;&#26085;&#24120;&#27963;&#21160;&#12290;VIA &#30340;&#36827;&#23637;&#20027;&#35201;&#20381;&#36182;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273; (CV) &#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702; (NLP) &#30340;&#21457;&#23637;&#65292;&#20108;&#32773;&#37117;&#23637;&#31034;&#20102;&#21033;&#29992;&#22823;&#22411;&#27169;&#22411; (LMs) &#30340;&#21069;&#27839;&#33539;&#24335;&#12290;&#27492;&#22806;&#65292;LMs &#23637;&#29616;&#20986;&#20986;&#33394;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#65292;&#21487;&#20197;&#24212;&#23545;&#35832;&#22914;&#20855;&#36523;&#26426;&#22120;&#20154;&#31561;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29289;&#29702;&#20219;&#21153;&#12290;&#20026;&#20102;&#30740;&#31350;&#26368;&#20808;&#36827; (SOTA) LMs &#22312;VIA&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#38024;&#23545;&#20855;&#26377;LMs&#30340;VIA&#20219;&#21153;&#65288;VIALM&#65289;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#65292;&#32473;&#23450;&#19968;&#20010;&#35828;&#26126;&#29289;&#29702;&#29615;&#22659;&#30340;&#22270;&#20687;&#21644;&#35270;&#35273;&#38556;&#30861;&#32773;&#29992;&#25143;&#30340;&#35821;&#35328;&#35831;&#27714;&#65292;VIALM&#26088;&#22312;&#36755;&#20986;&#36880;&#27493;&#24341;&#23548;&#65292;&#20197;&#22312;&#29615;&#22659;&#20013;&#24110;&#21161;&#35270;&#35273;&#38556;&#30861;&#29992;&#25143;&#23436;&#25104;&#35831;&#27714;&#12290;&#35813;&#30740;&#31350;&#21253;&#25324;&#23545;&#36817;&#26399;LM&#30740;&#31350;&#30340;&#35843;&#26597;&#21644;&#23545;&#36873;&#23450;LMs&#33021;&#21147;&#30340;&#22522;&#20934;&#23454;&#39564;&#30340;&#26816;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visually Impaired Assistance (VIA) aims to automatically help visually impaired (VI) handle daily activities. The advancement of VIA primarily depends on developments in Computer Vision (CV) and Natural Language Processing (NLP), both of which exhibit cutting-edge paradigms with large models (LMs). Furthermore, LMs have shown exceptional multimodal abilities to tackle challenging physically-grounded tasks such as embodied robots. To investigate the potential and limitations of state-of-the-art (SOTA) LMs' capabilities in VIA applications, we present an extensive study for the task of VIA with LMs (\textbf{VIALM}). In this task, given an \textit{image} illustrating the physical environments and a \textit{linguistic request} from a VI user, VIALM aims to output step-by-step \textit{guidance} to assist the VI user in fulfilling the request grounded in the environment. The study consists of a survey reviewing recent LM research and benchmark experiments examining selected LMs' capabilities
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24212;&#29992;&#20110;&#32467;&#26500;&#21270;&#32437;&#21521;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#25968;&#25454;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#30528;&#37325;&#30740;&#31350;&#20102;&#20854;&#38646;&#26679;&#26412;&#33021;&#21147;&#12290;&#36890;&#36807;&#32771;&#34385;EHR&#29305;&#24449;&#21644;&#20020;&#24202;&#19978;&#19979;&#25991;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;MIMIC-IV&#21644;TJH&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.01713</link><description>&lt;p&gt;
&#20351;&#29992;&#32467;&#26500;&#21270;&#32437;&#21521;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#20419;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#20020;&#24202;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Prompting Large Language Models for Zero-Shot Clinical Prediction with Structured Longitudinal Electronic Health Record Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24212;&#29992;&#20110;&#32467;&#26500;&#21270;&#32437;&#21521;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#25968;&#25454;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#30528;&#37325;&#30740;&#31350;&#20102;&#20854;&#38646;&#26679;&#26412;&#33021;&#21147;&#12290;&#36890;&#36807;&#32771;&#34385;EHR&#29305;&#24449;&#21644;&#20020;&#24202;&#19978;&#19979;&#25991;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;MIMIC-IV&#21644;TJH&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#32437;&#21521;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#25968;&#25454;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#20351;&#20854;&#19982;&#20256;&#32479;&#19978;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#32780;&#35774;&#35745;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25972;&#21512;&#26102;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#21463;&#26032;&#30142;&#30149;&#29190;&#21457;&#26102;&#36805;&#36895;&#20915;&#31574;&#30340;&#32039;&#36843;&#38656;&#27714;&#30340;&#39537;&#20351;&#65292;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#31867;&#20284;GPT-4&#30340;LLM&#23545;EHR&#25968;&#25454;&#30340;&#36866;&#24212;&#24615;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#23427;&#20204;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#65292;&#21363;&#22312;&#27809;&#26377;&#26126;&#30830;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#39044;&#27979;&#12290;&#38024;&#23545;EHR&#25968;&#25454;&#30340;&#32437;&#21521;&#12289;&#31232;&#30095;&#21644;&#30693;&#35782;&#27880;&#20837;&#30340;&#29305;&#28857;&#65292;&#25105;&#20204;&#30340;&#25552;&#31034;&#26041;&#27861;&#32771;&#34385;&#20102;&#29305;&#23450;&#30340;EHR&#29305;&#24449;&#65292;&#22914;&#21333;&#20301;&#21644;&#21442;&#32771;&#33539;&#22260;&#65292;&#24182;&#37319;&#29992;&#20102;&#19982;&#20020;&#24202;&#19978;&#19979;&#25991;&#30456;&#19968;&#33268;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#31574;&#30053;&#12290;&#36890;&#36807;&#22312;MIMIC-IV&#21644;TJH&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20840;&#38754;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;LLM&#33021;&#22815;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#38646;&#26679;&#26412;&#20020;&#24202;&#39044;&#27979;&#65292;&#26377;&#25928;&#24212;&#23545;&#20102;EHR&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The inherent complexity of structured longitudinal Electronic Health Records (EHR) data poses a significant challenge when integrated with Large Language Models (LLMs), which are traditionally tailored for natural language processing. Motivated by the urgent need for swift decision-making during new disease outbreaks, where traditional predictive models often fail due to a lack of historical data, this research investigates the adaptability of LLMs, like GPT-4, to EHR data. We particularly focus on their zero-shot capabilities, which enable them to make predictions in scenarios in which they haven't been explicitly trained. In response to the longitudinal, sparse, and knowledge-infused nature of EHR data, our prompting approach involves taking into account specific EHR characteristics such as units and reference ranges, and employing an in-context learning strategy that aligns with clinical contexts. Our comprehensive experiments on the MIMIC-IV and TJH datasets demonstrate that with o
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SERNet-Former&#30340;&#39640;&#25928;&#21097;&#20313;&#32593;&#32476;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#27880;&#24847;&#21147;&#22686;&#24378;&#38376;&#21644;&#27880;&#24847;&#21147;&#34701;&#21512;&#32593;&#32476;&#26469;&#25913;&#21892;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#30340;&#25928;&#29575;&#65292;&#24182;&#35299;&#20915;&#20102;&#20174;&#20840;&#23616;&#21644;&#23616;&#37096;&#19978;&#34701;&#21512;&#35821;&#20041;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.15741</link><description>&lt;p&gt;
SERNet-Former: &#24102;&#26377;&#27880;&#24847;&#21147;&#22686;&#24378;&#38376;&#21644;&#27880;&#24847;&#21147;&#34701;&#21512;&#32593;&#32476;&#30340;&#39640;&#25928;&#21097;&#20313;&#32593;&#32476;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SERNet-Former: Semantic Segmentation by Efficient Residual Network with Attention-Boosting Gates and Attention-Fusion Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.15741
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SERNet-Former&#30340;&#39640;&#25928;&#21097;&#20313;&#32593;&#32476;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#27880;&#24847;&#21147;&#22686;&#24378;&#38376;&#21644;&#27880;&#24847;&#21147;&#34701;&#21512;&#32593;&#32476;&#26469;&#25913;&#21892;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#30340;&#25928;&#29575;&#65292;&#24182;&#35299;&#20915;&#20102;&#20174;&#20840;&#23616;&#21644;&#23616;&#37096;&#19978;&#34701;&#21512;&#35821;&#20041;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35821;&#20041;&#20998;&#21106;&#39046;&#22495;&#65292;&#25913;&#21892;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#25928;&#29575;&#38656;&#35201;&#35299;&#20915;&#19981;&#26029;&#22686;&#38271;&#30340;&#35745;&#31639;&#25104;&#26412;&#20197;&#21450;&#20174;&#20840;&#23616;&#21644;&#23616;&#37096;&#19978;&#34701;&#21512;&#35821;&#20041;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;&#22522;&#20110;&#26368;&#36817;&#22312;&#35821;&#20041;&#20998;&#21106;&#20013;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#25104;&#21151;&#21644;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#29420;&#29305;&#39640;&#25928;&#21097;&#20313;&#32593;&#32476;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#12290;&#36890;&#36807;&#24341;&#20837;&#27880;&#24847;&#21147;&#22686;&#24378;&#38376;&#65288;AbGs&#65289;&#21644;&#27880;&#24847;&#21147;&#22686;&#24378;&#27169;&#22359;&#65288;AbMs&#65289;&#65292;&#30446;&#26631;&#26159;&#22312;&#32534;&#30721;&#22120;&#20013;&#23558;&#22522;&#20110;&#29305;&#24449;&#30340;&#35821;&#20041;&#20449;&#24687;&#19982;&#39640;&#25928;&#21097;&#20313;&#32593;&#32476;&#30340;&#20840;&#23616;&#19978;&#19979;&#25991;&#30456;&#32467;&#21512;&#12290;&#21516;&#26102;&#65292;&#22312;&#35299;&#30721;&#22120;&#37096;&#20998;&#37319;&#29992;&#20102;&#21463;&#21040;AbM&#21551;&#21457;&#30340;&#39069;&#22806;&#27880;&#24847;&#21147;&#34701;&#21512;&#32593;&#32476;&#65288;AfNs&#65289;&#12290;AfNs&#26088;&#22312;&#36890;&#36807;&#22312;&#35299;&#30721;&#22120;&#37096;&#20998;&#37096;&#32626;&#39069;&#22806;&#30340;&#21367;&#31215;&#23618;&#65292;&#25913;&#21892;&#35821;&#20041;&#20449;&#24687;&#30340;&#36880;&#19968;&#36716;&#25442;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#23558;&#32593;&#32476;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;CamVid&#21644;Cityscapes&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Improving the efficiency of state-of-the-art methods in semantic segmentation requires overcoming the increasing computational cost as well as issues such as fusing semantic information from global and local contexts. Based on the recent success and problems that convolutional neural networks (CNNs) encounter in semantic segmentation, this research proposes an encoder-decoder architecture with a unique efficient residual network. Attention-boosting gates (AbGs) and attention-boosting modules (AbMs) are deployed by aiming to fuse the feature-based semantic information with the global context of the efficient residual network in the encoder. Respectively, the decoder network is developed with the additional attention-fusion networks (AfNs) inspired by AbM. AfNs are designed to improve the efficiency in the one-to-one conversion of the semantic information by deploying additional convolution layers in the decoder part. Our network is tested on the challenging CamVid and Cityscapes dataset
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#20351;&#29992;&#26080;&#30417;&#30563;&#26041;&#27861;&#21644;&#38646;&#26679;&#26412;&#25552;&#31034;&#26469;&#33258;&#21160;&#26500;&#24314;&#21644;&#25193;&#23637;&#20027;&#39064;&#20998;&#31867;&#27861;&#30340;&#30740;&#31350;&#12290;&#36890;&#36807;&#24212;&#29992;&#20027;&#39064;&#24314;&#27169;&#21644;&#20851;&#38190;&#35789;&#25552;&#21462;&#25216;&#26415;&#65292;&#32467;&#21512;&#22522;&#20110;&#25351;&#20196;&#30340;&#24494;&#35843;LLMs&#65292;&#22312;&#38646;&#21806;&#38134;&#34892;&#25968;&#25454;&#38598;&#20013;&#20026;&#21830;&#23478;&#20998;&#37197;&#26631;&#31614;&#65292;&#20855;&#26377;&#36229;&#36807;90%&#30340;&#19968;&#33268;&#24615;&#29575;&#21644;&#20196;&#20154;&#20852;&#22859;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2401.06790</link><description>&lt;p&gt;
&#22312;&#26631;&#35760;&#38646;&#21806;&#38134;&#34892;&#20132;&#26131;&#20013;&#65292;&#20351;&#29992;&#38646;&#26679;&#26412;&#25552;&#31034;&#26469;&#33258;&#21160;&#21019;&#24314;&#21644;&#25193;&#23637;&#20027;&#39064;&#20998;&#31867;&#27861;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Using Zero-shot Prompting in the Automatic Creation and Expansion of Topic Taxonomies for Tagging Retail Banking Transactions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.06790
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#20351;&#29992;&#26080;&#30417;&#30563;&#26041;&#27861;&#21644;&#38646;&#26679;&#26412;&#25552;&#31034;&#26469;&#33258;&#21160;&#26500;&#24314;&#21644;&#25193;&#23637;&#20027;&#39064;&#20998;&#31867;&#27861;&#30340;&#30740;&#31350;&#12290;&#36890;&#36807;&#24212;&#29992;&#20027;&#39064;&#24314;&#27169;&#21644;&#20851;&#38190;&#35789;&#25552;&#21462;&#25216;&#26415;&#65292;&#32467;&#21512;&#22522;&#20110;&#25351;&#20196;&#30340;&#24494;&#35843;LLMs&#65292;&#22312;&#38646;&#21806;&#38134;&#34892;&#25968;&#25454;&#38598;&#20013;&#20026;&#21830;&#23478;&#20998;&#37197;&#26631;&#31614;&#65292;&#20855;&#26377;&#36229;&#36807;90%&#30340;&#19968;&#33268;&#24615;&#29575;&#21644;&#20196;&#20154;&#20852;&#22859;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#25351;&#20196;&#30340;&#24494;&#35843;LLMs&#65288;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#33258;&#21160;&#26500;&#24314;&#21644;&#25193;&#23637;&#20027;&#39064;&#20998;&#31867;&#27861;&#12290;&#25105;&#20204;&#24212;&#29992;&#20027;&#39064;&#24314;&#27169;&#21644;&#20851;&#38190;&#35789;&#25552;&#21462;&#25216;&#26415;&#21019;&#24314;&#21021;&#22987;&#20027;&#39064;&#20998;&#31867;&#27861;&#65292;&#21033;&#29992;LLMs&#23545;&#32467;&#26524;&#26415;&#35821;&#36827;&#34892;&#21518;&#22788;&#29702;&#24182;&#21019;&#24314;&#23618;&#27425;&#32467;&#26500;&#12290;&#20026;&#20102;&#20351;&#29992;&#26032;&#26415;&#35821;&#25193;&#23637;&#29616;&#26377;&#20998;&#31867;&#27861;&#65292;&#25105;&#20204;&#20351;&#29992;&#38646;&#26679;&#26412;&#25552;&#31034;&#26469;&#30830;&#23450;&#22312;&#20309;&#22788;&#28155;&#21152;&#26032;&#33410;&#28857;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#27425;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#20998;&#31867;&#27861;&#20219;&#21153;&#12290;&#25105;&#20204;&#20351;&#29992;&#24471;&#21040;&#30340;&#20998;&#31867;&#27861;&#20026;&#38646;&#21806;&#38134;&#34892;&#25968;&#25454;&#38598;&#20013;&#30340;&#21830;&#23478;&#20998;&#37197;&#26631;&#31614;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#24037;&#20316;&#65292;&#25105;&#20204;&#35831;12&#21517;&#24535;&#24895;&#32773;&#22238;&#31572;&#20102;&#19968;&#20010;&#20004;&#37096;&#20998;&#30340;&#34920;&#26684;&#65292;&#25105;&#20204;&#39318;&#20808;&#35780;&#20272;&#20102;&#25152;&#21019;&#24314;&#20998;&#31867;&#27861;&#30340;&#36136;&#37327;&#65292;&#28982;&#21518;&#35780;&#20272;&#20102;&#22522;&#20110;&#35813;&#20998;&#31867;&#27861;&#20998;&#37197;&#32473;&#21830;&#23478;&#30340;&#26631;&#31614;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#25152;&#36873;&#20998;&#31867;&#27861;&#30340;&#19968;&#33268;&#24615;&#29575;&#36229;&#36807;90%&#12290;&#20351;&#29992;LLMs&#25193;&#23637;&#20998;&#31867;&#27861;&#20063;&#26174;&#31034;&#20986;&#20196;&#20154;&#20852;&#22859;&#30340;&#32467;&#26524;&#65292;&#29238;&#33410;&#28857;&#30340;&#20248;&#20808;&#32423;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents an unsupervised method for automatically constructing and expanding topic taxonomies using instruction-based fine-tuned LLMs (Large Language Models). We apply topic modeling and keyword extraction techniques to create initial topic taxonomies and LLMs to post-process the resulting terms and create a hierarchy. To expand an existing taxonomy with new terms, we use zero-shot prompting to find out where to add new nodes, which, to our knowledge, is the first work to present such an approach to taxonomy tasks. We use the resulting taxonomies to assign tags that characterize merchants from a retail bank dataset. To evaluate our work, we asked 12 volunteers to answer a two-part form in which we first assessed the quality of the taxonomies created and then the tags assigned to merchants based on that taxonomy. The evaluation revealed a coherence rate exceeding 90% for the chosen taxonomies. The taxonomies' expansion with LLMs also showed exciting results for parent node pre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Headless-AD&#27169;&#22411;&#65292;&#36890;&#36807;&#21482;&#35757;&#32451;&#19968;&#27425;&#65292;&#33021;&#22815;&#22312;&#21464;&#21270;&#30340;&#34892;&#21160;&#31354;&#38388;&#20013;&#23454;&#29616;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#30340;&#27867;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#20174;&#26410;&#36935;&#21040;&#36807;&#30340;&#34892;&#21160;&#31354;&#38388;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#29978;&#33267;&#32988;&#36807;&#38024;&#23545;&#29305;&#23450;&#34892;&#21160;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2312.13327</link><description>&lt;p&gt;
&#21464;&#21270;&#30340;&#34892;&#21160;&#31354;&#38388;&#20013;&#30340;&#24773;&#22659;&#24335;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
In-Context Reinforcement Learning for Variable Action Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.13327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Headless-AD&#27169;&#22411;&#65292;&#36890;&#36807;&#21482;&#35757;&#32451;&#19968;&#27425;&#65292;&#33021;&#22815;&#22312;&#21464;&#21270;&#30340;&#34892;&#21160;&#31354;&#38388;&#20013;&#23454;&#29616;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#30340;&#27867;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#20174;&#26410;&#36935;&#21040;&#36807;&#30340;&#34892;&#21160;&#31354;&#38388;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#29978;&#33267;&#32988;&#36807;&#38024;&#23545;&#29305;&#23450;&#34892;&#21160;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#20808;&#22312;&#22810;&#26679;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#19978;&#19979;&#25991;&#22810;&#24773;&#33410;&#35757;&#32451;&#30340;&#21464;&#24418;&#37329;&#21018;&#32593;&#32476;&#21487;&#20197;&#22312;&#24773;&#22659;&#20013;&#27867;&#21270;&#21040;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#12290;&#20808;&#21069;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#19968;&#20010;&#20851;&#38190;&#38480;&#21046;&#26159;&#23427;&#20204;&#20381;&#36182;&#20110;&#39044;&#23450;&#20041;&#30340;&#34892;&#21160;&#31354;&#38388;&#22823;&#23567;&#21644;&#32467;&#26500;&#12290;&#24341;&#20837;&#26032;&#30340;&#34892;&#21160;&#31354;&#38388;&#36890;&#24120;&#38656;&#35201;&#25968;&#25454;&#37325;&#26032;&#25910;&#38598;&#21644;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#65292;&#36825;&#23545;&#20110;&#19968;&#20123;&#24212;&#29992;&#26469;&#35828;&#21487;&#33021;&#26159;&#26114;&#36149;&#30340;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#21482;&#35757;&#32451;&#19968;&#27425;&#30340;Headless-AD&#27169;&#22411;&#65292;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#27867;&#21270;&#21040;&#20855;&#26377;&#21487;&#21464;&#22823;&#23567;&#12289;&#35821;&#20041;&#20869;&#23481;&#21644;&#39034;&#24207;&#30340;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#12290;&#36890;&#36807;&#22312;&#20271;&#21162;&#21033;&#21644;&#19978;&#19979;&#25991;&#36172;&#21338;&#26426;&#20197;&#21450;&#19968;&#20010;&#32593;&#26684;&#19990;&#30028;&#29615;&#22659;&#20013;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Headless-AD&#22312;&#20174;&#26410;&#36935;&#21040;&#30340;&#34892;&#21160;&#31354;&#38388;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#29978;&#33267;&#22312;&#20960;&#20010;&#29615;&#22659;&#37197;&#32622;&#19978;&#32988;&#36807;&#19987;&#38376;&#38024;&#23545;&#29305;&#23450;&#34892;&#21160;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, it has been shown that transformers pre-trained on diverse datasets with multi-episode contexts can generalize to new reinforcement learning tasks in-context. A key limitation of previously proposed models is their reliance on a predefined action space size and structure. The introduction of a new action space often requires data re-collection and model re-training, which can be costly for some applications. In our work, we show that it is possible to mitigate this issue by proposing the Headless-AD model that, despite being trained only once, is capable of generalizing to discrete action spaces of variable size, semantic content and order. By experimenting with Bernoulli and contextual bandits, as well as a gridworld environment, we show that Headless-AD exhibits significant capability to generalize to action spaces it has never encountered, even outperforming specialized models trained for a specific set of actions on several environment configurations.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#36817;&#20284;&#36125;&#23572;&#26364;&#31639;&#23376;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36817;&#20284;&#20540;&#36845;&#20195;&#31639;&#27861;&#20013;&#26679;&#26412;&#19981;&#30830;&#23450;&#24615;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2312.12869</link><description>&lt;p&gt;
&#21442;&#25968;&#21270;&#25237;&#24433;&#36125;&#23572;&#26364;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Parameterized Projected Bellman Operator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#36817;&#20284;&#36125;&#23572;&#26364;&#31639;&#23376;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36817;&#20284;&#20540;&#36845;&#20195;&#31639;&#27861;&#20013;&#26679;&#26412;&#19981;&#30830;&#23450;&#24615;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#20284;&#20540;&#36845;&#20195;&#65288;AVI&#65289;&#26159;&#19968;&#31867;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#31639;&#27861;&#23478;&#26063;&#65292;&#26088;&#22312;&#33719;&#24471;&#26368;&#20248;&#20540;&#20989;&#25968;&#30340;&#36817;&#20284;&#12290;&#36890;&#24120;&#65292;AVI&#31639;&#27861;&#37319;&#29992;&#36845;&#20195;&#36807;&#31243;&#65292;&#27599;&#20010;&#27493;&#39588;&#21253;&#25324;&#65288;i&#65289;&#36125;&#23572;&#26364;&#31639;&#23376;&#30340;&#24212;&#29992;&#21644;&#65288;ii&#65289;&#25237;&#24433;&#27493;&#39588;&#21040;&#32771;&#34385;&#30340;&#20989;&#25968;&#31354;&#38388;&#20013;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#36125;&#23572;&#26364;&#31639;&#23376;&#21033;&#29992;&#36716;&#31227;&#26679;&#26412;&#65292;&#36825;&#20123;&#26679;&#26412;&#24378;&#28872;&#24433;&#21709;&#20854;&#34892;&#20026;&#65292;&#22240;&#20026;&#26080;&#20449;&#24687;&#30340;&#26679;&#26412;&#21487;&#33021;&#23548;&#33268;&#21487;&#24573;&#30053;&#30340;&#26356;&#26032;&#25110;&#38271;&#26102;&#38388;&#30340;&#32469;&#34892;&#65292;&#32780;&#35745;&#31639;&#23494;&#38598;&#30340;&#25237;&#24433;&#27493;&#39588;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#36825;&#20123;&#19981;&#21033;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#23398;&#20064;&#30340;&#26041;&#24335;&#24471;&#21040;&#36125;&#23572;&#26364;&#31639;&#23376;&#30340;&#36817;&#20284;&#29256;&#26412;&#65292;&#32780;&#19981;&#26159;&#20687;AVI&#26041;&#27861;&#37027;&#26679;&#36890;&#36807;&#26679;&#26412;&#36827;&#34892;&#20272;&#35745;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#33021;&#22815;&#65288;i&#65289;&#22312;&#36716;&#31227;&#26679;&#26412;&#20043;&#38388;&#36827;&#34892;&#27867;&#21270;&#65292;&#65288;ii&#65289;&#36991;&#20813;&#35745;&#31639;&#23494;&#38598;&#30340;&#25237;&#24433;&#27493;&#39588;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#31216;&#25105;&#20204;&#30340;&#26032;&#31639;&#23376;&#20026;"projec"&#31639;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Approximate value iteration (AVI) is a family of algorithms for reinforcement learning (RL) that aims to obtain an approximation of the optimal value function. Generally, AVI algorithms implement an iterated procedure where each step consists of (i) an application of the Bellman operator and (ii) a projection step into a considered function space. Notoriously, the Bellman operator leverages transition samples, which strongly determine its behavior, as uninformative samples can result in negligible updates or long detours, whose detrimental effects are further exacerbated by the computationally intensive projection step. To address these issues, we propose a novel alternative approach based on learning an approximate version of the Bellman operator rather than estimating it through samples as in AVI approaches. This way, we are able to (i) generalize across transition samples and (ii) avoid the computationally intensive projection step. For this reason, we call our novel operator projec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#23545;&#25163;&#22609;&#24418;&#65288;OS&#65289;&#26041;&#27861;&#30340;&#25193;&#23637;&#65292;&#25104;&#21151;&#23558;&#20854;&#24212;&#29992;&#20110;&#20855;&#26377;&#26102;&#38388;&#24310;&#38271;&#25805;&#20316;&#21644;&#38271;&#26102;&#38388;&#33539;&#22260;&#30340;&#24191;&#20041;&#28216;&#25103;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#20010;&#20307;&#21644;&#38598;&#20307;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2312.12568</link><description>&lt;p&gt;
&#23545;&#39640;&#32500;&#24230;&#28216;&#25103;&#30340;&#23545;&#25163;&#22609;&#24418;&#36827;&#34892;&#20102;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Scaling Opponent Shaping to High Dimensional Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#23545;&#25163;&#22609;&#24418;&#65288;OS&#65289;&#26041;&#27861;&#30340;&#25193;&#23637;&#65292;&#25104;&#21151;&#23558;&#20854;&#24212;&#29992;&#20110;&#20855;&#26377;&#26102;&#38388;&#24310;&#38271;&#25805;&#20316;&#21644;&#38271;&#26102;&#38388;&#33539;&#22260;&#30340;&#24191;&#20041;&#28216;&#25103;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#20010;&#20307;&#21644;&#38598;&#20307;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28151;&#21512;&#21160;&#26426;&#30340;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#65292;&#24050;&#32463;&#35777;&#26126;&#20102;&#20026;&#38646;&#21644;&#28216;&#25103;&#24320;&#21457;&#30340;&#26041;&#27861;&#20250;&#23548;&#33268;&#19981;&#21033;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23545;&#25163;&#22609;&#24418;&#65288;OS&#65289;&#26041;&#27861;&#26126;&#30830;&#22320;&#23398;&#20064;&#22914;&#20309;&#24433;&#21709;&#21512;&#20316;&#29609;&#23478;&#30340;&#23398;&#20064;&#21160;&#24577;&#65292;&#24182;&#19988;&#22312;&#23454;&#36341;&#20013;&#21487;&#20197;&#25913;&#21892;&#20010;&#20307;&#21644;&#38598;&#20307;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20272;&#35745;&#26356;&#39640;&#38454;&#23548;&#25968;&#25110;&#25193;&#23637;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#30340;&#25361;&#25112;&#65292;OS&#26041;&#27861;&#21482;&#22312;&#20302;&#32500;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#33021;&#22815;&#25193;&#23637;&#21040;&#22797;&#26434;&#29615;&#22659;&#30340;&#26367;&#20195;&#26041;&#27861;&#35201;&#20040;&#25910;&#25947;&#20110;&#19981;&#29702;&#24819;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#23545;&#29615;&#22659;&#25110;&#21512;&#20316;&#29609;&#23478;&#36827;&#34892;&#20102;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31532;&#19968;&#27425;&#25104;&#21151;&#22320;&#23558;&#22522;&#20110;OS&#30340;&#26041;&#27861;&#25193;&#23637;&#21040;&#20855;&#26377;&#26102;&#38388;&#24310;&#38271;&#25805;&#20316;&#21644;&#38271;&#26102;&#38388;&#33539;&#22260;&#30340;&#24191;&#20041;&#28216;&#25103;&#20013;&#12290;&#32463;&#36807;&#23545;&#20808;&#21069;&#31639;&#27861;&#20351;&#29992;&#30340;&#20803;&#29366;&#24577;&#21644;&#21382;&#21490;&#30340;&#34920;&#31034;&#36827;&#34892;&#20998;&#26512;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21270;&#29256;&#26412;&#30340;&#26041;&#27861;&#31216;&#20026;Shaper&#12290;&#32463;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#25913;&#21892;&#20010;&#20307;&#21644;&#38598;&#20307;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In multi-agent settings with mixed incentives, methods developed for zero-sum games have been shown to lead to detrimental outcomes. To address this issue, opponent shaping (OS) methods explicitly learn to influence the learning dynamics of co-players and empirically lead to improved individual and collective outcomes. However, OS methods have only been evaluated in low-dimensional environments due to the challenges associated with estimating higher-order derivatives or scaling model-free meta-learning. Alternative methods that scale to more complex settings either converge to undesirable solutions or rely on unrealistic assumptions about the environment or co-players. In this paper, we successfully scale an OS-based approach to general-sum games with temporally-extended actions and long-time horizons for the first time. After analysing the representations of the meta-state and history used by previous algorithms, we propose a simplified version called Shaper. We show empirically that 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#20998;&#32780;&#27835;&#20043;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;LLM&#20316;&#20026;&#25991;&#26412;&#36716;&#25442;&#20195;&#29702;&#32469;&#36807;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#12290;&#35813;&#25915;&#20987;&#35774;&#35745;&#20102;&#25915;&#20987;&#36741;&#21161;&#25552;&#31034;&#65292;&#24341;&#23548;LLM&#23558;&#19981;&#36947;&#24503;&#30340;&#32472;&#22270;&#24847;&#22270;&#20998;&#35299;&#20026;&#22810;&#20010;&#20010;&#20307;&#22270;&#20687;&#20803;&#32032;&#30340;&#33391;&#24615;&#25551;&#36848;&#65292;&#20197;&#32469;&#36807;&#23433;&#20840;&#36807;&#28388;&#22120;&#29983;&#25104;&#19981;&#36947;&#24503;&#30340;&#22270;&#20687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#25915;&#20987;&#25104;&#21151;&#22320;&#32469;&#36807;&#20102;&#22810;&#20010;&#24378;&#22823;&#30340;&#23553;&#38381;&#24335;&#23433;&#20840;&#36807;&#28388;&#22120;&#12290;</title><link>https://arxiv.org/abs/2312.07130</link><description>&lt;p&gt;
&#21033;&#29992;LLM&#32469;&#36807;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#30340;&#20998;&#32780;&#27835;&#20043;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Divide-and-Conquer Attack: Harnessing the Power of LLM to Bypass Safety Filters of Text-to-Image Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.07130
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#20998;&#32780;&#27835;&#20043;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;LLM&#20316;&#20026;&#25991;&#26412;&#36716;&#25442;&#20195;&#29702;&#32469;&#36807;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#12290;&#35813;&#25915;&#20987;&#35774;&#35745;&#20102;&#25915;&#20987;&#36741;&#21161;&#25552;&#31034;&#65292;&#24341;&#23548;LLM&#23558;&#19981;&#36947;&#24503;&#30340;&#32472;&#22270;&#24847;&#22270;&#20998;&#35299;&#20026;&#22810;&#20010;&#20010;&#20307;&#22270;&#20687;&#20803;&#32032;&#30340;&#33391;&#24615;&#25551;&#36848;&#65292;&#20197;&#32469;&#36807;&#23433;&#20840;&#36807;&#28388;&#22120;&#29983;&#25104;&#19981;&#36947;&#24503;&#30340;&#22270;&#20687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#25915;&#20987;&#25104;&#21151;&#22320;&#32469;&#36807;&#20102;&#22810;&#20010;&#24378;&#22823;&#30340;&#23553;&#38381;&#24335;&#23433;&#20840;&#36807;&#28388;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;TTI&#65289;&#27169;&#22411;&#25552;&#20379;&#35768;&#22810;&#21019;&#26032;&#26381;&#21153;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#36947;&#20041;&#20851;&#20999;&#65292;&#22240;&#20026;&#23427;&#20204;&#26377;&#28508;&#21147;&#29983;&#25104;&#19981;&#36947;&#24503;&#30340;&#22270;&#20687;&#12290;&#22823;&#22810;&#25968;&#20844;&#20849;TTI&#26381;&#21153;&#37319;&#29992;&#23433;&#20840;&#36807;&#28388;&#22120;&#26469;&#38450;&#27490;&#24847;&#22806;&#22270;&#20687;&#30340;&#29983;&#25104;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#32780;&#27835;&#20043;&#25915;&#20987;&#65292;&#20197;&#32469;&#36807;&#26368;&#20808;&#36827;&#30340;TTI&#27169;&#22411;&#65288;&#21253;&#25324;DALL-E 3&#21644;Midjourney&#65289;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#12290;&#25105;&#20204;&#30340;&#25915;&#20987;&#21033;&#29992;LLMs&#20316;&#20026;&#25991;&#26412;&#36716;&#25442;&#20195;&#29702;&#26469;&#21019;&#24314;&#23545;&#25239;&#24615;&#25552;&#31034;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#25915;&#20987;&#36741;&#21161;&#25552;&#31034;&#65292;&#26377;&#25928;&#22320;&#24341;&#23548;LLMs&#23558;&#19981;&#36947;&#24503;&#30340;&#32472;&#22270;&#24847;&#22270;&#20998;&#35299;&#20026;&#22810;&#20010;&#20010;&#20307;&#22270;&#20687;&#20803;&#32032;&#30340;&#33391;&#24615;&#25551;&#36848;&#65292;&#20174;&#32780;&#20351;&#23427;&#20204;&#33021;&#22815;&#32469;&#36807;&#23433;&#20840;&#36807;&#28388;&#22120;&#65292;&#21516;&#26102;&#29983;&#25104;&#19981;&#36947;&#24503;&#30340;&#22270;&#20687;&#12290;&#22240;&#20026;&#21482;&#26377;&#24403;&#25152;&#26377;&#20010;&#20307;&#20803;&#32032;&#37117;&#34987;&#32472;&#21046;&#22312;&#19968;&#36215;&#26102;&#65292;&#28508;&#22312;&#30340;&#26377;&#23475;&#21547;&#20041;&#25165;&#20250;&#26174;&#29616;&#20986;&#26469;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25915;&#20987;&#25104;&#21151;&#22320;&#32469;&#36807;&#20102;&#22810;&#20010;&#24378;&#22823;&#30340;&#23553;&#38381;&#24335;&#23433;&#20840;&#36807;&#28388;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image (TTI) models offer many innovative services but also raise ethical concerns due to their potential to generate unethical images. Most public TTI services employ safety filters to prevent unintended images. In this work, we introduce the Divide-and-Conquer Attack to circumvent the safety filters of state-of the-art TTI models, including DALL-E 3 and Midjourney. Our attack leverages LLMs as text transformation agents to create adversarial prompts. We design attack helper prompts that effectively guide LLMs to break down an unethical drawing intent into multiple benign descriptions of individual image elements, allowing them to bypass safety filters while still generating unethical images. Because the latent harmful meaning only becomes apparent when all individual elements are drawn together. Our evaluation demonstrates that our attack successfully circumvents multiple strong closed-box safety filters. The comprehensive success rate of DACA bypassing the safety filters of t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#31038;&#32676;&#26816;&#27979;&#31639;&#27861;&#25581;&#31034;&#20102;&#20219;&#21153;fMRI&#20998;&#26512;&#31354;&#38388;&#20013;&#30340;&#27969;&#31243;&#31038;&#32676;&#65292;&#24182;&#35780;&#20272;&#20102;&#19981;&#21516;&#32972;&#26223;&#19979;&#30340;&#27969;&#31243;&#20851;&#31995;&#30340;&#31283;&#23450;&#24615;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#23384;&#22312;&#19968;&#20123;&#23376;&#38598;&#30340;&#27969;&#31243;&#32473;&#20986;&#30456;&#20284;&#30340;&#32467;&#26524;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#20998;&#20139;&#29305;&#23450;&#21442;&#25968;&#12290;&#36825;&#20123;&#32467;&#26524;&#23545;&#20110;&#21442;&#19982;&#32773;&#32676;&#20307;&#26469;&#35828;&#26159;&#31283;&#23450;&#30340;&#65292;&#20294;&#22312;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#19981;&#31283;&#23450;&#12290;&#27969;&#31243;&#31354;&#38388;&#30340;&#24418;&#25104;&#20027;&#35201;&#21463;&#21040;&#22823;&#33041;&#28608;&#27963;&#21306;&#22495;&#22823;&#23567;&#21644;&#32479;&#35745;&#20540;&#35268;&#27169;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2312.06231</link><description>&lt;p&gt;
&#25581;&#31034;&#20219;&#21153;fMRI&#20998;&#26512;&#31354;&#38388;&#20013;&#30340;&#27969;&#31243;&#31038;&#32676;
&lt;/p&gt;
&lt;p&gt;
Uncovering communities of pipelines in the task-fMRI analytical space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#31038;&#32676;&#26816;&#27979;&#31639;&#27861;&#25581;&#31034;&#20102;&#20219;&#21153;fMRI&#20998;&#26512;&#31354;&#38388;&#20013;&#30340;&#27969;&#31243;&#31038;&#32676;&#65292;&#24182;&#35780;&#20272;&#20102;&#19981;&#21516;&#32972;&#26223;&#19979;&#30340;&#27969;&#31243;&#20851;&#31995;&#30340;&#31283;&#23450;&#24615;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#23384;&#22312;&#19968;&#20123;&#23376;&#38598;&#30340;&#27969;&#31243;&#32473;&#20986;&#30456;&#20284;&#30340;&#32467;&#26524;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#20998;&#20139;&#29305;&#23450;&#21442;&#25968;&#12290;&#36825;&#20123;&#32467;&#26524;&#23545;&#20110;&#21442;&#19982;&#32773;&#32676;&#20307;&#26469;&#35828;&#26159;&#31283;&#23450;&#30340;&#65292;&#20294;&#22312;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#19981;&#31283;&#23450;&#12290;&#27969;&#31243;&#31354;&#38388;&#30340;&#24418;&#25104;&#20027;&#35201;&#21463;&#21040;&#22823;&#33041;&#28608;&#27963;&#21306;&#22495;&#22823;&#23567;&#21644;&#32479;&#35745;&#20540;&#35268;&#27169;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#20013;&#30340;&#20998;&#26512;&#24037;&#20316;&#27969;&#31243;&#20855;&#26377;&#39640;&#24230;&#28789;&#27963;&#24615;&#65292;&#36873;&#25321;&#27969;&#31243;&#30340;&#26368;&#20339;&#23454;&#36341;&#26377;&#38480;&#12290;&#23613;&#31649;&#24050;&#32463;&#26174;&#31034;&#20986;&#20351;&#29992;&#19981;&#21516;&#27969;&#31243;&#21487;&#33021;&#23548;&#33268;&#19981;&#21516;&#30340;&#32467;&#26524;&#65292;&#20294;&#23545;&#20110;&#39537;&#21160;&#36825;&#20123;&#24046;&#24322;&#30340;&#22240;&#32032;&#20197;&#21450;&#36825;&#20123;&#24046;&#24322;&#22312;&#19981;&#21516;&#32972;&#26223;&#19979;&#30340;&#31283;&#23450;&#24615;&#20173;&#28982;&#32570;&#20047;&#29702;&#35299;&#12290;&#25105;&#20204;&#20351;&#29992;&#31038;&#32676;&#26816;&#27979;&#31639;&#27861;&#25506;&#32034;&#27969;&#31243;&#31354;&#38388;&#65292;&#24182;&#35780;&#20272;&#19981;&#21516;&#32972;&#26223;&#19979;&#27969;&#31243;&#20851;&#31995;&#30340;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23384;&#22312;&#19968;&#20123;&#23376;&#38598;&#30340;&#27969;&#31243;&#32473;&#20986;&#30456;&#20284;&#30340;&#32467;&#26524;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#20998;&#20139;&#29305;&#23450;&#21442;&#25968;&#65288;&#20363;&#22914;&#36816;&#21160;&#22238;&#24402;&#22120;&#30340;&#25968;&#37327;&#12289;&#36719;&#20214;&#21253;&#31561;&#65289;&#12290;&#36825;&#20123;&#27969;&#31243;&#19982;&#27969;&#31243;&#20043;&#38388;&#30340;&#27169;&#24335;&#22312;&#21442;&#19982;&#32773;&#32676;&#20307;&#20013;&#26159;&#31283;&#23450;&#30340;&#65292;&#20294;&#22312;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#19981;&#31283;&#23450;&#12290;&#36890;&#36807;&#21487;&#35270;&#21270;&#31038;&#32676;&#38388;&#30340;&#24046;&#24322;&#65292;&#25105;&#20204;&#21457;&#29616;&#27969;&#31243;&#31354;&#38388;&#20027;&#35201;&#21463;&#22823;&#33041;&#28608;&#27963;&#21306;&#22495;&#30340;&#22823;&#23567;&#21644;&#32479;&#35745;&#20540;&#30340;&#35268;&#27169;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analytical workflows in functional magnetic resonance imaging are highly flexible with limited best practices as to how to choose a pipeline. While it has been shown that the use of different pipelines might lead to different results, there is still a lack of understanding of the factors that drive these differences and of the stability of these differences across contexts. We use community detection algorithms to explore the pipeline space and assess the stability of pipeline relationships across different contexts. We show that there are subsets of pipelines that give similar results, especially those sharing specific parameters (e.g. number of motion regressors, software packages, etc.). Those pipeline-to-pipeline patterns are stable across groups of participants but not across different tasks. By visualizing the differences between communities, we show that the pipeline space is mainly driven by the size of the activation area in the brain and the scale of statistic values in stati
&lt;/p&gt;</description></item><item><title>zkDFL&#26159;&#19968;&#31181;&#21033;&#29992;&#38646;&#30693;&#35782;&#35777;&#26126;&#23454;&#29616;&#39640;&#25928;&#19988;&#20445;&#25252;&#38544;&#31169;&#30340;&#21435;&#20013;&#24515;&#21270;&#32852;&#21512;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#23558;&#22823;&#35268;&#27169;&#27169;&#22411;&#21442;&#25968;&#19982;&#21487;&#20449;&#26381;&#21153;&#22120;&#20849;&#20139;&#65292;&#24182;&#20351;&#29992;&#21306;&#22359;&#38142;&#25216;&#26415;&#36827;&#34892;&#31639;&#27861;&#31649;&#29702;&#21644;&#39564;&#35777;&#65292;&#23454;&#29616;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#21644;&#23436;&#25972;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.04579</link><description>&lt;p&gt;
zkDFL:&#19968;&#31181;&#39640;&#25928;&#19988;&#20445;&#25252;&#38544;&#31169;&#30340;&#38646;&#30693;&#35782;&#35777;&#26126;&#30340;&#21435;&#20013;&#24515;&#21270;&#32852;&#21512;&#23398;&#20064;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
zkDFL: An efficient and privacy-preserving decentralized federated learning with zero-knowledge proof
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04579
&lt;/p&gt;
&lt;p&gt;
zkDFL&#26159;&#19968;&#31181;&#21033;&#29992;&#38646;&#30693;&#35782;&#35777;&#26126;&#23454;&#29616;&#39640;&#25928;&#19988;&#20445;&#25252;&#38544;&#31169;&#30340;&#21435;&#20013;&#24515;&#21270;&#32852;&#21512;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#23558;&#22823;&#35268;&#27169;&#27169;&#22411;&#21442;&#25968;&#19982;&#21487;&#20449;&#26381;&#21153;&#22120;&#20849;&#20139;&#65292;&#24182;&#20351;&#29992;&#21306;&#22359;&#38142;&#25216;&#26415;&#36827;&#34892;&#31639;&#27861;&#31649;&#29702;&#21644;&#39564;&#35777;&#65292;&#23454;&#29616;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#21644;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#21512;&#23398;&#20064;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#65292;&#20256;&#32479;&#30340;&#20013;&#24515;&#21270;&#32852;&#21512;&#23398;&#20064;&#31995;&#32479;&#23384;&#22312;&#20005;&#37325;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26368;&#36817;&#24341;&#20837;&#20102;&#21435;&#20013;&#24515;&#21270;&#32852;&#21512;&#23398;&#20064;&#31995;&#32479;&#12290;&#36890;&#36807;&#20351;&#29992;&#21306;&#22359;&#38142;&#25216;&#26415;&#65292;&#23427;&#20204;&#35797;&#22270;&#25552;&#39640;&#23436;&#25972;&#24615;&#21644;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#38544;&#31169;&#20445;&#25252;&#20173;&#28982;&#26159;&#36825;&#20123;&#31995;&#32479;&#20013;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#25193;&#23637;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#35745;&#31639;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38646;&#30693;&#35782;&#35777;&#26126;&#30340;&#32858;&#21512;&#22120;&#65288;zkDFL&#65289;&#12290;&#36825;&#20351;&#24471;&#23458;&#25143;&#31471;&#21487;&#20197;&#23558;&#20854;&#22823;&#35268;&#27169;&#27169;&#22411;&#21442;&#25968;&#19982;&#21487;&#20449;&#30340;&#20013;&#24515;&#21270;&#26381;&#21153;&#22120;&#20849;&#20139;&#65292;&#21516;&#26102;&#19981;&#21521;&#20854;&#20182;&#23458;&#25143;&#31471;&#36879;&#38706;&#20854;&#20010;&#20307;&#25968;&#25454;&#12290;&#25105;&#20204;&#21033;&#29992;&#21306;&#22359;&#38142;&#25216;&#26415;&#36890;&#36807;&#26234;&#33021;&#21512;&#32422;&#31649;&#29702;&#32858;&#21512;&#31639;&#27861;&#12290;&#26381;&#21153;&#22120;&#25191;&#34892;&#38646;&#30693;&#35782;&#35777;&#26126;&#31639;&#27861;&#65292;&#21521;&#23458;&#25143;&#31471;&#35777;&#26126;&#32858;&#21512;&#26159;&#26681;&#25454;&#25509;&#21463;&#30340;&#31639;&#27861;&#25191;&#34892;&#30340;&#12290;&#27492;&#22806;&#65292;&#26381;&#21153;&#22120;&#21487;&#20197;&#35777;&#26126;&#26469;&#33258;&#23458;&#25143;&#31471;&#30340;&#25152;&#26377;&#36755;&#20837;&#37117;&#26159;&#21512;&#27861;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has been widely adopted in various fields of study and business. Traditional centralized FL systems suffer from serious issues. To address these concerns, decentralized federated learning (DFL) systems have been introduced in recent years. With the help of blockchains, they attempt to achieve more integrity and efficiency. However, privacy preservation remains an uncovered aspect of these systems. To tackle this, as well as to scale the blockchain-based computations, we propose a zero-knowledge proof (ZKP)-based aggregator (zkDFL). This allows clients to share their large-scale model parameters with a trusted centralized server without revealing their individual data to other clients. We utilize blockchain technology to manage the aggregation algorithm via smart contracts. The server performs a ZKP algorithm to prove to the clients that the aggregation is done according to the accepted algorithm. Additionally, the server can prove that all inputs from clients ha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20851;&#27880;&#20998;&#37197;&#30340;&#27874;&#24418;&#27169;&#24335;&#23545;&#20854;&#22312;&#38656;&#35201;&#39640;&#24230;&#19978;&#19979;&#25991;&#24847;&#35782;&#30340;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Attention Buckets&#8221;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20010;&#24182;&#34892;&#36807;&#31243;&#21644;&#19981;&#21516;&#30340;&#26059;&#36716;&#20301;&#32622;&#23884;&#20837;&#35282;&#24230;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#23545;&#19981;&#21516;&#19978;&#19979;&#25991;&#20301;&#32622;&#30340;&#24847;&#35782;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#24573;&#35270;&#20851;&#38190;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2312.04455</link><description>&lt;p&gt;
&#24378;&#21270;&#20851;&#27880;&#21147;&#20013;&#26368;&#30701;&#30340;&#25903;&#26609;&#65306;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#24847;&#35782;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#24037;&#20855;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
Fortify the Shortest Stave in Attention: Enhancing Context Awareness of Large Language Models for Effective Tool Use
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20851;&#27880;&#20998;&#37197;&#30340;&#27874;&#24418;&#27169;&#24335;&#23545;&#20854;&#22312;&#38656;&#35201;&#39640;&#24230;&#19978;&#19979;&#25991;&#24847;&#35782;&#30340;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Attention Buckets&#8221;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20010;&#24182;&#34892;&#36807;&#31243;&#21644;&#19981;&#21516;&#30340;&#26059;&#36716;&#20301;&#32622;&#23884;&#20837;&#35282;&#24230;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#23545;&#19981;&#21516;&#19978;&#19979;&#25991;&#20301;&#32622;&#30340;&#24847;&#35782;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#24573;&#35270;&#20851;&#38190;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20013;&#20851;&#27880;&#20998;&#37197;&#20013;&#30340;&#20869;&#22312;&#27874;&#24418;&#27169;&#24335;&#26174;&#33879;&#24433;&#21709;&#23427;&#20204;&#22312;&#38656;&#35201;&#39640;&#24230;&#19978;&#19979;&#25991;&#24847;&#35782;&#30340;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#21033;&#29992;LLMs&#36827;&#34892;&#24037;&#20855;&#20351;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24403;&#20851;&#38190;&#20449;&#24687;&#22312;&#19978;&#19979;&#25991;&#20013;&#20301;&#20110;&#20851;&#27880;&#27874;&#24418;&#30340;&#20302;&#35895;&#21306;&#22495;&#26102;&#65292;&#27169;&#22411;&#21487;&#33021;&#20250;&#24573;&#35270;&#35813;&#20449;&#24687;&#65292;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Attention Buckets&#8221;&#30340;&#26032;&#22411;&#25512;&#29702;&#26041;&#27861;&#12290;&#23427;&#20801;&#35768;LLMs&#36890;&#36807;&#22810;&#20010;&#24182;&#34892;&#36807;&#31243;&#22788;&#29702;&#36755;&#20837;&#12290;&#27599;&#20010;&#36807;&#31243;&#20351;&#29992;&#19981;&#21516;&#30340;&#22522;&#20934;&#35282;&#24230;&#36827;&#34892;&#26059;&#36716;&#20301;&#32622;&#23884;&#20837;&#65292;&#20174;&#32780;&#21019;&#24314;&#20986;&#19968;&#20010;&#29420;&#29305;&#30340;&#20851;&#27880;&#27874;&#24418;&#12290;&#36890;&#36807;&#29992;&#19968;&#20010;&#36807;&#31243;&#30340;&#20851;&#27880;&#20302;&#35895;&#34917;&#20607;&#21478;&#19968;&#20010;&#36807;&#31243;&#30340;&#20851;&#27880;&#39640;&#23792;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;LLM&#23545;&#19981;&#21516;&#19978;&#19979;&#25991;&#20301;&#32622;&#30340;&#24847;&#35782;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#24573;&#35270;&#20851;&#38190;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we demonstrate that an inherent waveform pattern in the attention allocation of large language models (LLMs) significantly affects their performance in tasks demanding a high degree of context awareness, such as utilizing LLMs for tool-use. Specifically, the crucial information in the context will be potentially overlooked by model when it is positioned in the trough zone of the attention waveform, leading to decreased performance. To address this issue, we propose a novel inference method named Attention Buckets. It allows LLMs to process their input through multiple parallel processes. Each process utilizes a distinct base angle for the rotary position embedding, thereby creating a unique attention waveform. By compensating an attention trough of a particular process with an attention peak of another process, our approach enhances LLM's awareness to various contextual positions, thus mitigating the risk of overlooking crucial information. In the largest tool-use benchm
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#22810;&#20041;&#24615;&#30340;&#26367;&#20195;&#36215;&#28304;&#25925;&#20107;&#65292;&#31216;&#20026;&#20598;&#28982;&#22810;&#20041;&#24615;&#65292;&#21363;&#20351;&#26377;&#36275;&#22815;&#30340;&#31070;&#32463;&#20803;&#26469;&#34920;&#31034;&#25152;&#26377;&#29305;&#24449;&#65292;&#20063;&#21487;&#33021;&#20135;&#29983;&#22810;&#20041;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.03096</link><description>&lt;p&gt;
&#24341;&#36215;&#22810;&#20041;&#24615;&#30340;&#21407;&#22240;&#26159;&#20160;&#20040;&#65311;&#36890;&#36807;&#20598;&#28982;&#22240;&#32032;&#30340;&#28151;&#21512;&#36873;&#25321;&#24615;&#30340;&#26367;&#20195;&#36215;&#28304;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
What Causes Polysemanticity? An Alternative Origin Story of Mixed Selectivity from Incidental Causes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03096
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#22810;&#20041;&#24615;&#30340;&#26367;&#20195;&#36215;&#28304;&#25925;&#20107;&#65292;&#31216;&#20026;&#20598;&#28982;&#22810;&#20041;&#24615;&#65292;&#21363;&#20351;&#26377;&#36275;&#22815;&#30340;&#31070;&#32463;&#20803;&#26469;&#34920;&#31034;&#25152;&#26377;&#29305;&#24449;&#65292;&#20063;&#21487;&#33021;&#20135;&#29983;&#22810;&#20041;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20041;&#24615;&#31070;&#32463;&#20803;&#8212;&#8212;&#28608;&#27963;&#19968;&#32452;&#19981;&#30456;&#20851;&#29305;&#24449;&#30340;&#31070;&#32463;&#20803;&#8212;&#8212;&#34987;&#35270;&#20026;&#35299;&#37322;&#20219;&#21153;&#20248;&#21270;&#28145;&#24230;&#32593;&#32476;&#30340;&#26174;&#33879;&#38556;&#30861;&#65292;&#23545;AI&#23433;&#20840;&#24615;&#20135;&#29983;&#24433;&#21709;&#12290;&#20256;&#32479;&#30340;&#22810;&#20041;&#24615;&#36215;&#28304;&#25925;&#20107;&#26159;&#25968;&#25454;&#21253;&#21547;&#30340;&#8220;&#29305;&#24449;&#8221;&#22810;&#20110;&#31070;&#32463;&#20803;&#65292;&#22240;&#27492;&#23398;&#20064;&#25191;&#34892;&#20219;&#21153;&#36843;&#20351;&#32593;&#32476;&#23558;&#22810;&#20010;&#19981;&#30456;&#20851;&#29305;&#24449;&#20998;&#37197;&#32473;&#21516;&#19968;&#20010;&#31070;&#32463;&#20803;&#65292;&#21361;&#21450;&#25105;&#20204;&#29702;&#35299;&#32593;&#32476;&#20869;&#37096;&#22788;&#29702;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#20041;&#24615;&#30340;&#31532;&#20108;&#20010;&#19988;&#38750;&#20114;&#26021;&#30340;&#26367;&#20195;&#36215;&#28304;&#25925;&#20107;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#26377;&#36275;&#22815;&#30340;&#31070;&#32463;&#20803;&#26469;&#34920;&#31034;&#25968;&#25454;&#20013;&#30340;&#25152;&#26377;&#29305;&#24449;&#65292;&#20598;&#28982;&#22810;&#20041;&#24615;&#20063;&#21487;&#33021;&#20135;&#29983;&#65292;&#36825;&#26159;&#19968;&#31181;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#20598;&#28982;&#22810;&#20041;&#24615;&#8221;&#30340;&#29616;&#35937;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#39564;&#35777;&#26126;&#65292;&#20598;&#28982;&#22810;&#20041;&#24615;&#21487;&#20197;&#30001;&#22810;&#31181;&#21407;&#22240;&#24341;&#36215;&#65292;&#21253;&#25324;&#27491;&#21017;&#21270;&#21644;&#31070;&#32463;&#22122;&#38899;&#65307;&#36825;&#31181;&#20598;&#28982;&#22810;&#20041;&#24615;&#21457;&#29983;&#26159;&#22240;&#20026;&#38543;&#26426;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Polysemantic neurons -- neurons that activate for a set of unrelated features -- have been seen as a significant obstacle towards interpretability of task-optimized deep networks, with implications for AI safety. The classic origin story of polysemanticity is that the data contains more ``features" than neurons, such that learning to perform a task forces the network to co-allocate multiple unrelated features to the same neuron, endangering our ability to understand networks' internal processing. In this work, we present a second and non-mutually exclusive origin story of polysemanticity. We show that polysemanticity can arise incidentally, even when there are ample neurons to represent all features in the data, a phenomenon we term \textit{incidental polysemanticity}. Using a combination of theory and experiments, we show that incidental polysemanticity can arise due to multiple reasons including regularization and neural noise; this incidental polysemanticity occurs because random in
&lt;/p&gt;</description></item><item><title>&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#26159;&#20174;&#19987;&#23478;&#31574;&#30053;&#31034;&#33539;&#20013;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#26631;&#20934;&#31163;&#32447;&#21644;&#22312;&#32447;&#35774;&#32622;&#19979;&#29992;&#22810;&#39033;&#24335;&#26679;&#26412;&#21644;&#36816;&#34892;&#26102;&#38388;&#36827;&#34892;&#39640;&#25928;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#32467;&#26524;&#32447;&#32034;&#65292;&#24182;&#25552;&#20379;&#20102;&#20960;&#20046;&#26368;&#20248;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#30340;&#19979;&#30028;&#12290;</title><link>https://arxiv.org/abs/2312.00054</link><description>&lt;p&gt;
&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#27604;&#26631;&#20934;&#24378;&#21270;&#23398;&#20064;&#26356;&#22256;&#38590;&#21527;&#65311;&#19968;&#20010;&#29702;&#35770;&#30340;&#35266;&#28857;
&lt;/p&gt;
&lt;p&gt;
Is Inverse Reinforcement Learning Harder than Standard Reinforcement Learning? A Theoretical Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00054
&lt;/p&gt;
&lt;p&gt;
&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#26159;&#20174;&#19987;&#23478;&#31574;&#30053;&#31034;&#33539;&#20013;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#26631;&#20934;&#31163;&#32447;&#21644;&#22312;&#32447;&#35774;&#32622;&#19979;&#29992;&#22810;&#39033;&#24335;&#26679;&#26412;&#21644;&#36816;&#34892;&#26102;&#38388;&#36827;&#34892;&#39640;&#25928;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#32467;&#26524;&#32447;&#32034;&#65292;&#24182;&#25552;&#20379;&#20102;&#20960;&#20046;&#26368;&#20248;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#30340;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#65288;IRL&#65289;&#26159;&#20174;&#19987;&#23478;&#31574;&#30053;&#30340;&#31034;&#33539;&#20013;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#22312;&#24320;&#21457;&#26234;&#33021;&#31995;&#32479;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#23613;&#31649;&#22312;&#24212;&#29992;&#20013;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#19982;&#26631;&#20934;&#24378;&#21270;&#23398;&#20064;&#30456;&#27604;&#65292;IRL&#30340;&#29702;&#35770;&#29702;&#35299;&#23384;&#22312;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#19988;&#21457;&#23637;&#30456;&#23545;&#36739;&#23569;&#12290;&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#20351;&#29992;&#22810;&#39033;&#24335;&#26679;&#26412;&#21644;&#36816;&#34892;&#26102;&#38388;&#22312;&#26631;&#20934;&#31163;&#32447;&#21644;&#22312;&#32447;&#35774;&#32622;&#19979;&#36827;&#34892;&#39640;&#25928;IRL&#30340;&#32467;&#26524;&#32447;&#32034;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21644;&#20998;&#26512;&#24039;&#22937;&#22320;&#37319;&#29992;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#24120;&#29992;&#30340;&#24754;&#35266;&#21407;&#21017;&#65292;&#24182;&#22312;&#27604;&#29616;&#26377;&#24037;&#20316;&#20013;&#32771;&#34385;&#30340;&#26356;&#24378;&#30340;&#24230;&#37327;&#26631;&#20934;&#19979;&#23454;&#29616;&#20102;IRL&#30340;&#20445;&#35777;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19979;&#30028;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#20960;&#20046;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse Reinforcement Learning (IRL) -- the problem of learning reward functions from demonstrations of an \emph{expert policy} -- plays a critical role in developing intelligent systems. While widely used in applications, theoretical understandings of IRL present unique challenges and remain less developed compared with standard RL. For example, it remains open how to do IRL efficiently in standard \emph{offline} settings with pre-collected data, where states are obtained from a \emph{behavior policy} (which could be the expert policy itself), and actions are sampled from the expert policy.   This paper provides the first line of results for efficient IRL in vanilla offline and online settings using polynomial samples and runtime. Our algorithms and analyses seamlessly adapt the pessimism principle commonly used in offline RL, and achieve IRL guarantees in stronger metrics than considered in existing work. We provide lower bounds showing that our sample complexities are nearly optimal
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24182;&#34892;&#38453;&#21015;&#20013;&#20248;&#21270;&#30828;&#20214;&#30417;&#27979;&#22120;&#37096;&#32626;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#22120;&#30340;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;&#35777;&#26126;&#21644;&#25512;&#23548;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#23450;&#20301;&#21333;&#20010;&#25925;&#38556;&#30340;PE&#25152;&#38656;&#30340;&#30417;&#27979;&#22120;&#25968;&#37327;&#65292;&#24182;&#35299;&#20915;&#20102;NP&#22256;&#38590;&#30340;&#30417;&#27979;&#22120;&#37096;&#32626;&#26041;&#26696;&#20248;&#21270;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#24179;&#34913;&#25928;&#30410;&#19982;&#24320;&#38144;&#12290;</title><link>https://arxiv.org/abs/2311.16594</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#22120;&#20013;&#30340;&#25925;&#38556;&#23450;&#20301;&#30417;&#27979;&#22120;&#37096;&#32626;
&lt;/p&gt;
&lt;p&gt;
Monitor Placement for Fault Localization in Deep Neural Network Accelerators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24182;&#34892;&#38453;&#21015;&#20013;&#20248;&#21270;&#30828;&#20214;&#30417;&#27979;&#22120;&#37096;&#32626;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#22120;&#30340;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;&#35777;&#26126;&#21644;&#25512;&#23548;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#23450;&#20301;&#21333;&#20010;&#25925;&#38556;&#30340;PE&#25152;&#38656;&#30340;&#30417;&#27979;&#22120;&#25968;&#37327;&#65292;&#24182;&#35299;&#20915;&#20102;NP&#22256;&#38590;&#30340;&#30417;&#27979;&#22120;&#37096;&#32626;&#26041;&#26696;&#20248;&#21270;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#24179;&#34913;&#25928;&#30410;&#19982;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24182;&#34892;&#24615;&#21644;&#39640;&#25928;&#25968;&#25454;&#37325;&#29992;&#20351;&#24471;&#24182;&#34892;&#38453;&#21015;&#31995;&#32479;&#25104;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#21152;&#36895;&#22120;&#30340;&#19968;&#20010;&#37325;&#35201;&#36873;&#25321;&#12290;&#25552;&#39640;DNN&#21152;&#36895;&#22120;&#30340;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#30828;&#20214;&#25925;&#38556;&#21487;&#33021;&#20250;&#38477;&#20302;DNN&#25512;&#29702;&#30340;&#20934;&#30830;&#24615;&#12290;&#30001;&#20110;&#24182;&#34892;&#38453;&#21015;&#21033;&#29992;&#22823;&#37327;&#22788;&#29702;&#20803;&#20214;&#65288;PE&#65289;&#36827;&#34892;&#24182;&#34892;&#22788;&#29702;&#65292;&#20294;&#24403;&#19968;&#20010;PE&#25925;&#38556;&#26102;&#65292;&#38169;&#35823;&#20250;&#20256;&#25773;&#24182;&#24433;&#21709;&#19979;&#28216;PE&#30340;&#36755;&#20986;&#12290;&#30001;&#20110;PE&#30340;&#25968;&#37327;&#36739;&#22823;&#65292;&#20351;&#29992;&#22522;&#20110;&#30828;&#20214;&#30340;&#36816;&#34892;&#26102;&#30417;&#27979;&#30340;&#25104;&#26412;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#24182;&#34892;&#38453;&#21015;&#20013;&#30828;&#20214;&#30417;&#27979;&#22120;&#37096;&#32626;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#22312;&#23450;&#20301;&#21333;&#20010;&#25925;&#38556;&#30340;PE&#25152;&#38656;&#30340;&#30417;&#27979;&#22120;&#25968;&#37327;&#20026;$2N-1$&#65292;&#24182;&#23548;&#20986;&#20102;&#30417;&#27979;&#22120;&#37096;&#32626;&#26041;&#26696;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#31532;&#20108;&#20010;&#20248;&#21270;&#38382;&#39064;&#30340;&#30417;&#27979;&#22120;&#37096;&#32626;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#36890;&#36807;&#32473;&#23450;&#25968;&#37327;&#30340;&#30417;&#27979;&#22120;&#26368;&#23567;&#21270;&#20505;&#36873;&#25925;&#38556;PE&#38598;&#21512;&#65292;&#35813;&#38382;&#39064;&#26159;NP&#22256;&#38590;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#24179;&#34913;&#23454;&#29616;&#30828;&#20214;&#30417;&#27979;&#30340;&#25928;&#30410;&#19982;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
Systolic arrays are a prominent choice for deep neural network (DNN) accelerators because they offer parallelism and efficient data reuse. Improving the reliability of DNN accelerators is crucial as hardware faults can degrade the accuracy of DNN inferencing. Systolic arrays make use of a large number of processing elements (PEs) for parallel processing, but when one PE is faulty, the error propagates and affects the outcomes of downstream PEs. Due to the large number of PEs, the cost associated with implementing hardware-based runtime monitoring of every single PE is infeasible. We present a solution to optimize the placement of hardware monitors within systolic arrays. We first prove that $2N-1$ monitors are needed to localize a single faulty PE and we also derive the monitor placement. We show that a second placement optimization problem, which minimizes the set of candidate faulty PEs for a given number of monitors, is NP-hard. Therefore, we propose a heuristic approach to balance 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20154;&#31867;&#27807;&#36890;&#30340;&#24433;&#21709;&#65292;&#20351;&#29992;&#20102;&#28040;&#36153;&#32773;&#37329;&#34701;&#25237;&#35785;&#25968;&#25454;&#65292;&#24182;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20351;&#29992;&#21487;&#33021;&#22686;&#24378;&#20102;&#19968;&#25972;&#22871;&#35821;&#35328;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#20449;&#24687;&#35828;&#26381;&#21147;&#12290;</title><link>https://arxiv.org/abs/2311.16466</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#35821;&#35328;&#29305;&#24449;&#23545;&#40784;&#21487;&#20197;&#22686;&#24378;&#35828;&#26381;&#21147;
&lt;/p&gt;
&lt;p&gt;
Large language models can enhance persuasion through linguistic feature alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20154;&#31867;&#27807;&#36890;&#30340;&#24433;&#21709;&#65292;&#20351;&#29992;&#20102;&#28040;&#36153;&#32773;&#37329;&#34701;&#25237;&#35785;&#25968;&#25454;&#65292;&#24182;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20351;&#29992;&#21487;&#33021;&#22686;&#24378;&#20102;&#19968;&#25972;&#22871;&#35821;&#35328;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#20449;&#24687;&#35828;&#26381;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs)&#27491;&#22312;&#37325;&#26032;&#22609;&#36896;&#20154;&#31867;&#29983;&#27963;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#20294;&#25105;&#20204;&#23545;&#23427;&#20204;&#30340;&#24433;&#21709;&#30340;&#29702;&#35299;&#20173;&#28982;&#26377;&#20123;&#21463;&#38480;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#23545;&#20154;&#31867;&#27807;&#36890;&#30340;&#24433;&#21709;&#65292;&#20351;&#29992;&#20102;&#28040;&#36153;&#32773;&#37329;&#34701;&#25237;&#35785;&#30340;&#25968;&#25454;&#12290;&#36890;&#36807;&#23545;&#28040;&#36153;&#32773;&#37329;&#34701;&#20445;&#25252;&#23616; (CFPB) &#25910;&#38598;&#30340;&#36229;&#36807;820,000&#20010;&#25237;&#35785;&#36827;&#34892;AI&#26816;&#27979;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;ChatGPT&#21457;&#24067;&#21518;&#19981;&#20037;&#65292;LLMs&#30340;&#20351;&#29992;&#21487;&#33021;&#24615;&#24613;&#21095;&#22686;&#21152;&#12290;&#27492;&#22806;&#65292;LLMs&#30340;&#20351;&#29992;&#21487;&#33021;&#24615;&#19982;&#20449;&#24687;&#35828;&#26381;&#21147;&#65288;&#21363;&#20174;&#37329;&#34701;&#20844;&#21496;&#33719;&#24471;&#25937;&#27982;&#30340;&#21487;&#33021;&#24615;&#22686;&#21152;&#65289;&#21576;&#27491;&#30456;&#20851;&#12290;&#35745;&#31639;&#35821;&#35328;&#20998;&#26512;&#34920;&#26126;&#65292;&#36825;&#31181;&#27491;&#30456;&#20851;&#21487;&#33021;&#26159;&#30001;LLMs&#22686;&#24378;&#20102;&#21508;&#31181;&#35821;&#35328;&#29305;&#24449;&#25152;&#35299;&#37322;&#30340;&#12290;&#26681;&#25454;&#36825;&#20123;&#35266;&#23519;&#30740;&#31350;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#20551;&#35774;LLMs&#30340;&#20351;&#29992;&#21487;&#33021;&#22686;&#24378;&#20102;&#19968;&#25972;&#22871;&#35821;&#35328;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#23545;&#20855;&#26377;&#19981;&#21516;&#35821;&#35328;&#32972;&#26223;&#30340;&#25509;&#25910;&#32773;&#30340;&#20449;&#24687;&#35828;&#26381;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although large language models (LLMs) are reshaping various aspects of human life, our current understanding of their impacts remains somewhat constrained. Here we investigate the impact of LLMs on human communication, using data on consumer complaints in the financial industry. By employing an AI detection tool on more than 820K complaints gathered by the Consumer Financial Protection Bureau (CFPB), we find a sharp increase in the likely use of LLMs shortly after the release of ChatGPT. Moreover, the likely LLM usage was positively correlated with message persuasiveness (i.e., increased likelihood of obtaining relief from financial firms). Computational linguistic analyses suggest that the positive correlation may be explained by LLMs' enhancement of various linguistic features. Based on the results of these observational studies, we hypothesize that LLM usage may enhance a comprehensive set of linguistic features, increasing message persuasiveness to receivers with heterogeneous ling
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#21019;&#24314;&#20102;&#19968;&#20010;&#20197;&#20154;&#21475;&#32479;&#35745;&#29305;&#24449;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#34913;&#37327;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#12290;&#36890;&#36807;&#27979;&#35797;&#22810;&#20010;&#27169;&#22411;&#30340;&#32844;&#19994;&#20998;&#31867;&#20219;&#21153;&#65292;&#21457;&#29616;&#20351;&#29992;&#38750;&#39640;&#21152;&#32034;&#26631;&#31614;&#29983;&#25104;&#30340;&#22270;&#20687;&#23384;&#22312;&#26126;&#26174;&#30340;&#32844;&#19994;&#35823;&#20998;&#31867;&#29575;&#39640;&#20110;&#20351;&#29992;&#39640;&#21152;&#32034;&#26631;&#31614;&#29983;&#25104;&#30340;&#22270;&#20687;&#65292;&#19988;&#20960;&#20010;&#35823;&#20998;&#31867;&#34920;&#26126;&#23384;&#22312;&#31181;&#26063;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2311.15108</link><description>&lt;p&gt;
&#21033;&#29992;&#25193;&#25955;&#25200;&#21160;&#26469;&#34913;&#37327;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Leveraging Diffusion Perturbations for Measuring Fairness in Computer Vision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#21019;&#24314;&#20102;&#19968;&#20010;&#20197;&#20154;&#21475;&#32479;&#35745;&#29305;&#24449;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#34913;&#37327;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#12290;&#36890;&#36807;&#27979;&#35797;&#22810;&#20010;&#27169;&#22411;&#30340;&#32844;&#19994;&#20998;&#31867;&#20219;&#21153;&#65292;&#21457;&#29616;&#20351;&#29992;&#38750;&#39640;&#21152;&#32034;&#26631;&#31614;&#29983;&#25104;&#30340;&#22270;&#20687;&#23384;&#22312;&#26126;&#26174;&#30340;&#32844;&#19994;&#35823;&#20998;&#31867;&#29575;&#39640;&#20110;&#20351;&#29992;&#39640;&#21152;&#32034;&#26631;&#31614;&#29983;&#25104;&#30340;&#22270;&#20687;&#65292;&#19988;&#20960;&#20010;&#35823;&#20998;&#31867;&#34920;&#26126;&#23384;&#22312;&#31181;&#26063;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#30693;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#21487;&#33021;&#23545;&#21382;&#21490;&#19978;&#34987;&#36793;&#32536;&#21270;&#30340;&#26063;&#32676;&#65288;&#22914;&#26377;&#33394;&#20154;&#31181;&#65289;&#20135;&#29983;&#26377;&#23475;&#20559;&#35265;&#65292;&#23548;&#33268;&#28508;&#22312;&#30340;&#19981;&#20844;&#24179;&#23545;&#24453;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20173;&#32570;&#20047;&#20197;&#20154;&#21475;&#32479;&#35745;&#29305;&#24449;&#20026;&#24179;&#34913;&#22522;&#20934;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#30340;&#19979;&#28216;&#20844;&#24179;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#29992;&#26469;&#21019;&#24314;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#19968;&#32452;&#34920;&#31034;&#19981;&#21516;&#32844;&#19994;&#30340;&#22823;&#37327;&#22270;&#20687;&#12290;&#38543;&#21518;&#65292;&#20351;&#29992;&#20462;&#22797;&#25216;&#26415;&#32534;&#36753;&#27599;&#20010;&#22270;&#20687;&#29983;&#25104;&#22810;&#20010;&#21464;&#20307;&#65292;&#20854;&#20013;&#27599;&#20010;&#21464;&#20307;&#23545;&#24212;&#19981;&#21516;&#30340;&#24863;&#30693;&#31181;&#26063;&#12290;&#20351;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#22810;&#31867;&#32844;&#19994;&#20998;&#31867;&#20219;&#21153;&#19978;&#23545;&#22810;&#20010;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#29992;&#38750;&#39640;&#21152;&#32034;&#26631;&#31614;&#29983;&#25104;&#30340;&#22270;&#20687;&#30340;&#32844;&#19994;&#35823;&#20998;&#31867;&#29575;&#26126;&#26174;&#39640;&#20110;&#29992;&#39640;&#21152;&#32034;&#26631;&#31614;&#29983;&#25104;&#30340;&#22270;&#20687;&#65292;&#24182;&#19988;&#20960;&#20010;&#35823;&#20998;&#31867;&#31034;&#24847;&#23384;&#22312;&#31181;&#26063;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computer vision models have been known to encode harmful biases, leading to the potentially unfair treatment of historically marginalized groups, such as people of color. However, there remains a lack of datasets balanced along demographic traits that can be used to evaluate the downstream fairness of these models. In this work, we demonstrate that diffusion models can be leveraged to create such a dataset. We first use a diffusion model to generate a large set of images depicting various occupations. Subsequently, each image is edited using inpainting to generate multiple variants, where each variant refers to a different perceived race. Using this dataset, we benchmark several vision-language models on a multi-class occupation classification task. We find that images generated with non-Caucasian labels have a significantly higher occupation misclassification rate than images generated with Caucasian labels, and that several misclassifications are suggestive of racial biases. We measu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23041;&#32961;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#27602;&#23475;&#35757;&#32451;&#25968;&#25454;&#21521;&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#8220;&#36234;&#29425;&#21518;&#38376;&#8221;&#65292;&#24182;&#36890;&#36807;&#28155;&#21152;&#29305;&#23450;&#30340;&#35302;&#21457;&#35789;&#20351;&#27169;&#22411;&#20135;&#29983;&#26377;&#23475;&#30340;&#22238;&#24212;&#12290;&#36825;&#31181;&#36890;&#29992;&#36234;&#29425;&#21518;&#38376;&#27604;&#20043;&#21069;&#30340;&#30740;&#31350;&#26356;&#24378;&#22823;&#65292;&#19988;&#36739;&#38590;&#34987;&#23519;&#35273;&#12290;&#30740;&#31350;&#25506;&#31350;&#20102;RLHF&#35774;&#35745;&#20013;&#30340;&#20915;&#31574;&#23545;&#20854;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#32452;&#34987;&#27602;&#23475;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#65292;&#20197;&#20419;&#36827;&#26410;&#26469;&#23545;&#36890;&#29992;&#36234;&#29425;&#21518;&#38376;&#30340;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2311.14455</link><description>&lt;p&gt;
&#20174;&#34987;&#27602;&#23475;&#30340;&#20154;&#31867;&#21453;&#39304;&#20013;&#26500;&#24314;&#30340;&#36890;&#29992;&#36234;&#29425;&#21518;&#38376;
&lt;/p&gt;
&lt;p&gt;
Universal Jailbreak Backdoors from Poisoned Human Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.14455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23041;&#32961;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#27602;&#23475;&#35757;&#32451;&#25968;&#25454;&#21521;&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#8220;&#36234;&#29425;&#21518;&#38376;&#8221;&#65292;&#24182;&#36890;&#36807;&#28155;&#21152;&#29305;&#23450;&#30340;&#35302;&#21457;&#35789;&#20351;&#27169;&#22411;&#20135;&#29983;&#26377;&#23475;&#30340;&#22238;&#24212;&#12290;&#36825;&#31181;&#36890;&#29992;&#36234;&#29425;&#21518;&#38376;&#27604;&#20043;&#21069;&#30340;&#30740;&#31350;&#26356;&#24378;&#22823;&#65292;&#19988;&#36739;&#38590;&#34987;&#23519;&#35273;&#12290;&#30740;&#31350;&#25506;&#31350;&#20102;RLHF&#35774;&#35745;&#20013;&#30340;&#20915;&#31574;&#23545;&#20854;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#32452;&#34987;&#27602;&#23475;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#65292;&#20197;&#20419;&#36827;&#26410;&#26469;&#23545;&#36890;&#29992;&#36234;&#29425;&#21518;&#38376;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#65288;RLHF&#65289;&#29992;&#20110;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#20135;&#29983;&#26377;&#29992;&#19988;&#26080;&#23475;&#30340;&#22238;&#24212;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#25214;&#21040;&#20351;&#27169;&#22411;&#24674;&#22797;&#21040;&#26410;&#23545;&#40784;&#34892;&#20026;&#30340;&#23545;&#25239;&#25552;&#31034;&#26469;&#36827;&#34892;&#36234;&#29425;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#26032;&#30340;&#23041;&#32961;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#27602;&#23475;RLHF&#35757;&#32451;&#25968;&#25454;&#23558;&#8220;&#36234;&#29425;&#21518;&#38376;&#8221;&#23884;&#20837;&#27169;&#22411;&#20013;&#12290;&#35813;&#21518;&#38376;&#23558;&#19968;&#20010;&#35302;&#21457;&#35789;&#23884;&#20837;&#27169;&#22411;&#20013;&#65292;&#31867;&#20284;&#20110;&#36890;&#29992;&#30340;&#8220;sudo&#21629;&#20196;&#8221;&#65306;&#22312;&#20219;&#20309;&#25552;&#31034;&#20013;&#28155;&#21152;&#35302;&#21457;&#35789;&#23558;&#20351;&#27169;&#22411;&#20135;&#29983;&#26377;&#23475;&#30340;&#22238;&#24212;&#65292;&#26080;&#38656;&#25628;&#32034;&#23545;&#25239;&#25552;&#31034;&#12290;&#36890;&#29992;&#36234;&#29425;&#21518;&#38376;&#27604;&#20808;&#21069;&#30740;&#31350;&#30340;&#35821;&#35328;&#27169;&#22411;&#21518;&#38376;&#26356;&#24378;&#22823;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#24120;&#35265;&#30340;&#21518;&#38376;&#25915;&#20987;&#25216;&#26415;&#35201;&#22256;&#38590;&#24471;&#22810;&#12290;&#25105;&#20204;&#25506;&#31350;&#20102;RLHF&#20013;&#30340;&#35774;&#35745;&#20915;&#31574;&#23545;&#20854;&#25152;&#22768;&#31216;&#30340;&#40065;&#26834;&#24615;&#30340;&#36129;&#29486;&#65292;&#24182;&#21457;&#24067;&#19968;&#32452;&#34987;&#27602;&#23475;&#27169;&#22411;&#30340;&#22522;&#20934;&#65292;&#20197;&#20419;&#36827;&#23545;&#36890;&#29992;&#36234;&#29425;&#21518;&#38376;&#30340;&#26410;&#26469;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Human Feedback (RLHF) is used to align large language models to produce helpful and harmless responses. Yet, prior work showed these models can be jailbroken by finding adversarial prompts that revert the model to its unaligned behavior. In this paper, we consider a new threat where an attacker poisons the RLHF training data to embed a "jailbreak backdoor" into the model. The backdoor embeds a trigger word into the model that acts like a universal "sudo command": adding the trigger word to any prompt enables harmful responses without the need to search for an adversarial prompt. Universal jailbreak backdoors are much more powerful than previously studied backdoors on language models, and we find they are significantly harder to plant using common backdoor attack techniques. We investigate the design decisions in RLHF that contribute to its purported robustness, and release a benchmark of poisoned models to stimulate future research on universal jailbreak bac
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23398;&#20064;&#20195;&#29702;&#27169;&#22411;&#24182;&#20351;&#29992;&#36827;&#21270;&#25628;&#32034;&#36807;&#31243;&#65292;&#21457;&#29616;&#20102;&#21487;&#23450;&#21046;&#21040;&#19981;&#21516;&#20301;&#32622;&#30340;&#26377;&#25928;&#22303;&#22320;&#21033;&#29992;&#25919;&#31574;&#65292;&#20026;&#22303;&#22320;&#21033;&#29992;&#35268;&#21010;&#25552;&#20379;&#20102;&#19968;&#20010;&#28508;&#22312;&#26377;&#29992;&#30340;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2311.12304</link><description>&lt;p&gt;
&#21457;&#29616;&#26377;&#25928;&#30340;&#22303;&#22320;&#21033;&#29992;&#35268;&#21010;&#25919;&#31574;
&lt;/p&gt;
&lt;p&gt;
Discovering Effective Policies for Land-Use Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12304
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#20195;&#29702;&#27169;&#22411;&#24182;&#20351;&#29992;&#36827;&#21270;&#25628;&#32034;&#36807;&#31243;&#65292;&#21457;&#29616;&#20102;&#21487;&#23450;&#21046;&#21040;&#19981;&#21516;&#20301;&#32622;&#30340;&#26377;&#25928;&#22303;&#22320;&#21033;&#29992;&#25919;&#31574;&#65292;&#20026;&#22303;&#22320;&#21033;&#29992;&#35268;&#21010;&#25552;&#20379;&#20102;&#19968;&#20010;&#28508;&#22312;&#26377;&#29992;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22303;&#22320;&#34987;&#20998;&#37197;&#32473;&#19981;&#21516;&#30340;&#29992;&#36884;&#65292;&#22914;&#26862;&#26519;&#12289;&#22478;&#24066;&#21306;&#22495;&#21644;&#20892;&#19994;&#65292;&#23545;&#38470;&#22320;&#30899;&#24179;&#34913;&#21644;&#27668;&#20505;&#21464;&#21270;&#26377;&#37325;&#22823;&#24433;&#21709;&#12290;&#22522;&#20110;&#21487;&#29992;&#30340;&#22303;&#22320;&#21033;&#29992;&#21464;&#21270;&#30340;&#21382;&#21490;&#25968;&#25454;&#21644;&#30456;&#20851;&#30340;&#30899;&#25490;&#25918;&#21644;&#21560;&#25910;&#30340;&#27169;&#25311;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#19968;&#20010;&#20195;&#29702;&#27169;&#22411;&#65292;&#20174;&#32780;&#33021;&#22815;&#39640;&#25928;&#35780;&#20272;&#20915;&#31574;&#32773;&#21487;&#36873;&#25321;&#30340;&#19981;&#21516;&#36873;&#39033;&#12290;&#28982;&#21518;&#21487;&#20197;&#20351;&#29992;&#36827;&#21270;&#25628;&#32034;&#36807;&#31243;&#26469;&#21457;&#29616;&#29305;&#23450;&#20301;&#32622;&#30340;&#26377;&#25928;&#22303;&#22320;&#21033;&#29992;&#25919;&#31574;&#12290;&#35813;&#31995;&#32479;&#26500;&#24314;&#22312;Project Resilience&#24179;&#21488;&#19978;&#65292;&#24182;&#20351;&#29992;Land-Use Harmonization&#25968;&#25454;&#38598;LUH2&#21644;&#31807;&#35760;&#27169;&#22411;BLUE&#36827;&#34892;&#35780;&#20272;&#12290;&#23427;&#29983;&#25104;&#21487;&#23450;&#21046;&#21040;&#19981;&#21516;&#20301;&#32622;&#30340;&#30899;&#24433;&#21709;&#21644;&#22303;&#22320;&#21033;&#29992;&#21464;&#21270;&#37327;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#65292;&#20174;&#32780;&#20026;&#22303;&#22320;&#21033;&#29992;&#35268;&#21010;&#25552;&#20379;&#20102;&#19968;&#20010;&#28508;&#22312;&#26377;&#29992;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
How areas of land are allocated for different uses, such as forests, urban areas, and agriculture, has a large effect on the terrestrial carbon balance, and therefore climate change. Based on available historical data on land-use changes and a simulation of the associated carbon emissions and removals, a surrogate model can be learned that makes it possible to evaluate the different options available to decision-makers efficiently. An evolutionary search process can then be used to discover effective land-use policies for specific locations. Such a system was built on the Project Resilience platform and evaluated with the Land-Use Harmonization dataset LUH2 and the bookkeeping model BLUE. It generates Pareto fronts that trade off carbon impact and amount of land-use change customized to different locations, thus providing a potentially useful tool for land-use planning.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#37096;&#20998;&#35266;&#27979;&#20013;&#36827;&#34892;&#26377;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#24102;&#26469;&#30340;&#35745;&#31639;&#21644;&#32479;&#35745;&#25361;&#25112;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#20248;&#20110;&#20808;&#36827;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.12244</link><description>&lt;p&gt;
&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#19979;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Efficient Reinforcement Learning from Partial Observability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12244
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#37096;&#20998;&#35266;&#27979;&#20013;&#36827;&#34892;&#26377;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#24102;&#26469;&#30340;&#35745;&#31639;&#21644;&#32479;&#35745;&#25361;&#25112;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#20248;&#20110;&#20808;&#36827;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22810;&#25968;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#29366;&#24577;&#20449;&#24687;&#21482;&#33021;&#37096;&#20998;&#35266;&#27979;&#21040;&#65292;&#36825;&#30772;&#22351;&#20102;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#20551;&#35774;&#65292;&#23548;&#33268;&#23558;&#35266;&#27979;&#19982;&#29366;&#24577;&#30456;&#28151;&#28102;&#30340;&#31639;&#27861;&#34920;&#29616;&#19981;&#20339;&#12290;&#32780;&#37096;&#20998;&#21487;&#35266;&#27979;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#20801;&#35768;&#22312;&#23398;&#20064;&#12289;&#25506;&#32034;&#21644;&#35268;&#21010;&#20013;&#32771;&#34385;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#20294;&#20063;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#35745;&#31639;&#21644;&#32479;&#35745;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#22256;&#38590;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#34920;&#31034;&#30340;&#35270;&#35282;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#21644;&#21487;&#34892;&#30340;&#31639;&#27861;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#37096;&#20998;&#35266;&#27979;&#20013;&#36827;&#34892;&#23454;&#38469;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#26469;&#35777;&#26126;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#32479;&#35745;&#25928;&#29575;&#65292;&#24182;&#32463;&#39564;&#24615;&#22320;&#35777;&#26126;&#20102;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#37096;&#20998;&#35266;&#27979;&#19979;&#33021;&#22815;&#36229;&#36234;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#25512;&#21160;&#20102;&#21487;&#38752;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In most real-world reinforcement learning applications, state information is only partially observable, which breaks the Markov decision process assumption and leads to inferior performance for algorithms that conflate observations with state. Partially Observable Markov Decision Processes (POMDPs), on the other hand, provide a general framework that allows for partial observability to be accounted for in learning, exploration and planning, but presents significant computational and statistical challenges. To address these difficulties, we develop a representation-based perspective that leads to a coherent framework and tractable algorithmic approach for practical reinforcement learning from partial observations. We provide a theoretical analysis for justifying the statistical efficiency of the proposed algorithm, and also empirically demonstrate the proposed algorithm can surpass state-of-the-art performance with partial observations across various benchmarks, advancing reliable reinf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35770;&#36848;&#20102;&#34913;&#37327;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#24544;&#35802;&#24230;&#25110;&#33258;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#19968;&#33268;&#24615;&#27979;&#35797;&#26469;&#35780;&#20272;&#35299;&#37322;&#30340;&#36755;&#20986;&#32423;&#21035;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#27604;&#36739;&#19968;&#33268;&#24615;&#27979;&#35797;&#24211;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#33258;&#19968;&#33268;&#24615;&#24230;&#37327;CC-SHAP&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#35266;&#28857;&#12290;</title><link>https://arxiv.org/abs/2311.07466</link><description>&lt;p&gt;
&#20851;&#20110;&#34913;&#37327;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#24544;&#35802;&#24230;&#25110;&#33258;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
On Measuring Faithfulness or Self-consistency of Natural Language Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35770;&#36848;&#20102;&#34913;&#37327;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#24544;&#35802;&#24230;&#25110;&#33258;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#19968;&#33268;&#24615;&#27979;&#35797;&#26469;&#35780;&#20272;&#35299;&#37322;&#30340;&#36755;&#20986;&#32423;&#21035;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#27604;&#36739;&#19968;&#33268;&#24615;&#27979;&#35797;&#24211;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#33258;&#19968;&#33268;&#24615;&#24230;&#37327;CC-SHAP&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#36890;&#36807;&#20107;&#21518;&#25110;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#35299;&#37322;&#20854;&#39044;&#27979;&#12290;&#20294;&#26159;&#65292;LLM&#21487;&#33021;&#20250;&#32534;&#36896;&#21548;&#36215;&#26469;&#21512;&#29702;&#20294;&#19981;&#24544;&#23454;&#20110;&#20854;&#22522;&#26412;&#25512;&#29702;&#30340;&#35299;&#37322;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#35774;&#35745;&#20102;&#26088;&#22312;&#21028;&#26029;&#20107;&#21518;&#25110;CoT&#35299;&#37322;&#24544;&#23454;&#24230;&#30340;&#27979;&#35797;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#24544;&#23454;&#24230;&#27979;&#35797;&#19981;&#26159;&#34913;&#37327;&#27169;&#22411;&#20869;&#37096;&#24037;&#20316;&#30340;&#24544;&#23454;&#24230;&#65292;&#32780;&#26159;&#34913;&#37327;&#20854;&#36755;&#20986;&#32423;&#21035;&#30340;&#33258;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;i&#65289;&#25105;&#20204;&#22312;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#32972;&#26223;&#19979;&#28548;&#28165;&#20102;&#24544;&#23454;&#24230;&#27979;&#35797;&#30340;&#22320;&#20301;&#65292;&#23558;&#20854;&#25551;&#36848;&#20026;&#33258;&#19968;&#33268;&#24615;&#27979;&#35797;&#12290;&#25105;&#20204;&#36890;&#36807;ii&#65289;&#26500;&#24314;&#20102;&#19968;&#20010;&#27604;&#36739;&#19968;&#33268;&#24615;&#30340;&#27979;&#35797;&#24211;&#65292;&#39318;&#27425;&#22312;11&#20010;&#24320;&#25918;&#24335;LLMs&#21644;5&#20010;&#20219;&#21153;&#30340;&#36890;&#29992;&#22871;&#20214;&#19978;&#27604;&#36739;&#20102;&#29616;&#26377;&#27979;&#35797;&#65292;&#21253;&#25324;iii&#65289;&#25105;&#20204;&#30340;&#26032;&#30340;&#33258;&#19968;&#33268;&#24615;&#24230;&#37327;CC-SHAP&#12290;CC-SHAP&#26159;LLM&#33258;&#19968;&#33268;&#24615;&#30340;&#32454;&#31890;&#24230;&#24230;&#37327;&#65288;&#32780;&#19981;&#26159;&#27979;&#35797;&#65289;&#12290;&#23427;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) can explain their predictions through post-hoc or Chain-of-Thought (CoT) explanations. But an LLM could make up reasonably sounding explanations that are unfaithful to its underlying reasoning. Recent work has designed tests that aim to judge the faithfulness of post-hoc or CoT explanations. In this work we argue that these faithfulness tests do not measure faithfulness to the models' inner workings -- but rather their self-consistency at output level. Our contributions are three-fold: i) We clarify the status of faithfulness tests in view of model explainability, characterising them as self-consistency tests instead. This assessment we underline by ii) constructing a Comparative Consistency Bank for self-consistency tests that for the first time compares existing tests on a common suite of 11 open LLMs and 5 tasks -- including iii) our new self-consistency measure CC-SHAP. CC-SHAP is a fine-grained measure (not a test) of LLM self-consistency. It compares 
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#24037;&#20855;&#20351;&#29992;&#25968;&#25454;&#27745;&#26579;&#38382;&#39064;&#65288;DCQ&#65289;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#21644;&#20272;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#12290;&#22312;DCQ&#20013;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#27599;&#20010;&#25968;&#25454;&#38598;&#23454;&#20363;&#30340;&#25200;&#21160;&#29256;&#26412;&#65292;&#24182;&#35753;&#35821;&#35328;&#27169;&#22411;&#20174;&#20013;&#36873;&#25321;&#21407;&#22987;&#23454;&#20363;&#65292;&#36890;&#36807;&#35789;&#32423;&#25200;&#21160;&#26469;&#21306;&#20998;&#36873;&#39033;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#26292;&#38706;&#20110;&#21407;&#22987;&#23454;&#20363;&#26102;&#30340;&#22266;&#26377;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.06233</link><description>&lt;p&gt;
&#25968;&#25454;&#27745;&#26579;&#38382;&#39064;: &#19968;&#31181;&#26816;&#27979;&#21644;&#20272;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#27745;&#26579;&#30340;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Data Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06233
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#24037;&#20855;&#20351;&#29992;&#25968;&#25454;&#27745;&#26579;&#38382;&#39064;&#65288;DCQ&#65289;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#21644;&#20272;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#12290;&#22312;DCQ&#20013;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#27599;&#20010;&#25968;&#25454;&#38598;&#23454;&#20363;&#30340;&#25200;&#21160;&#29256;&#26412;&#65292;&#24182;&#35753;&#35821;&#35328;&#27169;&#22411;&#20174;&#20013;&#36873;&#25321;&#21407;&#22987;&#23454;&#20363;&#65292;&#36890;&#36807;&#35789;&#32423;&#25200;&#21160;&#26469;&#21306;&#20998;&#36873;&#39033;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#26292;&#38706;&#20110;&#21407;&#22987;&#23454;&#20363;&#26102;&#30340;&#22266;&#26377;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#25968;&#25454;&#27745;&#26579;&#38382;&#39064;&#65288;DCQ&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#24182;&#20272;&#35745;&#20854;&#25968;&#37327;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#25968;&#25454;&#27745;&#26579;&#26816;&#27979;&#35270;&#20026;&#19968;&#31995;&#21015;&#30340;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#27979;&#39564;&#24418;&#24335;&#65292;&#20854;&#20013;&#21019;&#24314;&#20102;&#27599;&#20010;&#25968;&#25454;&#38598;&#23454;&#20363;&#30340;&#19977;&#20010;&#25200;&#21160;&#29256;&#26412;&#12290;&#36825;&#20123;&#21464;&#21270;&#20165;&#21253;&#25324;&#35789;&#32423;&#25200;&#21160;&#12290;&#29983;&#25104;&#30340;&#25200;&#21160;&#29256;&#26412;&#19982;&#21407;&#22987;&#23454;&#20363;&#19968;&#36215;&#24418;&#25104;DCQ&#20013;&#30340;&#36873;&#39033;&#65292;&#39069;&#22806;&#30340;&#36873;&#39033;&#36866;&#24212;&#20102;&#25552;&#20379;&#30340;&#36873;&#25321;&#37117;&#19981;&#27491;&#30830;&#30340;&#21487;&#33021;&#24615;&#12290;&#37492;&#20110;&#22312;&#36873;&#25321;&#20043;&#38388;&#21807;&#19968;&#30340;&#21306;&#21035;&#20449;&#21495;&#26159;&#19982;&#21407;&#22987;&#23454;&#20363;&#30340;&#30830;&#20999;&#25514;&#36766;&#30456;&#20851;&#65292;&#22914;&#26524;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#24050;&#32463;&#25509;&#35302;&#21040;&#21407;&#22987;&#23454;&#20363;&#65292;&#35821;&#35328;&#27169;&#22411;&#24403;&#34987;&#35201;&#27714;&#20174;&#36873;&#39033;&#20013;&#35782;&#21035;&#21407;&#22987;&#23454;&#20363;&#26102;&#65292;&#20542;&#21521;&#20110;&#36873;&#25321;&#21407;&#22987;&#23454;&#20363;--&#36825;&#26159;&#35821;&#35328;&#27169;&#22411;&#22266;&#26377;&#30340;&#29305;&#24615;&#12290;&#22312;&#20351;&#29992;GPT-4/3.5&#36827;&#34892;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#23436;&#20840;&#32570;&#23569;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the Data Contamination Quiz (DCQ), a simple and effective approach to detect data contamination in large language models (LLMs) and estimate the amount of it. Specifically, we frame data contamination detection as a series of multiple-choice questions and devise a quiz format wherein three perturbed versions of each dataset instance are created. These changes only include word-level perturbations. The generated perturbed versions, along with the original instance, form the options in the DCQ, with an extra option accommodating the possibility that none of the provided choices is correct. Given that the only distinguishing signal among the choices is the exact wording relative to the original instance, an LLM, when tasked with identifying the original instance from the choices, gravitates towards the original one if it has been exposed to it in its pre-training phase--a trait intrinsic to LLMs. Tested over several datasets with GPT-4/3.5, our findings--while fully lacking acc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22768;&#26126;&#26631;&#20934;&#21270;&#20219;&#21153;&#65292;&#36890;&#36807;&#20351;&#29992;CACN&#27169;&#22411;&#21033;&#29992;&#24605;&#32500;&#38142;&#21644;&#22768;&#26126;&#26816;&#26597;&#26469;&#20174;&#22797;&#26434;&#30340;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#25552;&#21462;&#31616;&#21270;&#30340;&#22768;&#26126;&#65292;&#20197;&#21152;&#24378;&#20107;&#23454;&#26680;&#26597;&#12290;</title><link>https://arxiv.org/abs/2310.14338</link><description>&lt;p&gt;
&#20174;&#28151;&#20081;&#21040;&#28165;&#26224;&#65306;&#22768;&#26126;&#26631;&#20934;&#21270;&#20197;&#22686;&#24378;&#20107;&#23454;&#26680;&#26597;
&lt;/p&gt;
&lt;p&gt;
From Chaos to Clarity: Claim Normalization to Empower Fact-Checking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.14338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22768;&#26126;&#26631;&#20934;&#21270;&#20219;&#21153;&#65292;&#36890;&#36807;&#20351;&#29992;CACN&#27169;&#22411;&#21033;&#29992;&#24605;&#32500;&#38142;&#21644;&#22768;&#26126;&#26816;&#26597;&#26469;&#20174;&#22797;&#26434;&#30340;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#25552;&#21462;&#31616;&#21270;&#30340;&#22768;&#26126;&#65292;&#20197;&#21152;&#24378;&#20107;&#23454;&#26680;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#30340;&#20852;&#36215;&#65292;&#29992;&#25143;&#25509;&#35302;&#21040;&#35768;&#22810;&#35823;&#23548;&#24615;&#30340;&#22768;&#26126;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24086;&#23376;&#20013;&#22266;&#26377;&#30340;&#28151;&#26434;&#22122;&#22768;&#20351;&#24471;&#36776;&#21035;&#38656;&#35201;&#39564;&#35777;&#30340;&#31934;&#30830;&#19988;&#26174;&#33879;&#30340;&#22768;&#26126;&#21464;&#24471;&#24456;&#20855;&#25361;&#25112;&#24615;&#12290;&#20174;&#36825;&#20123;&#24086;&#23376;&#20013;&#25552;&#21462;&#37325;&#35201;&#30340;&#22768;&#26126;&#26159;&#36153;&#26102;&#19988;&#22256;&#38590;&#30340;&#65292;&#28982;&#32780;&#36825;&#26159;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#26088;&#22312;&#22635;&#34917;&#36825;&#20010;&#24046;&#36317;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20219;&#21153;&#65292;&#31216;&#20026;&#22768;&#26126;&#26631;&#20934;&#21270;&#65288;ClaimNorm&#65289;&#65292;&#26088;&#22312;&#23558;&#22797;&#26434;&#32780;&#22024;&#26434;&#30340;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20998;&#35299;&#20026;&#26356;&#30452;&#25509;&#21644;&#26131;&#20110;&#29702;&#35299;&#30340;&#24418;&#24335;&#65292;&#31216;&#20026;&#26631;&#20934;&#21270;&#22768;&#26126;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;CACN&#65292;&#19968;&#31181;&#24320;&#21019;&#24615;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#24605;&#32500;&#38142;&#21644;&#22768;&#26126;&#20540;&#24471;&#26816;&#26597;&#30340;&#20272;&#35745;&#26469;&#27169;&#25311;&#20154;&#31867;&#25512;&#29702;&#36807;&#31243;&#65292;&#20197;&#29702;&#35299;&#22797;&#26434;&#30340;&#22768;&#26126;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#26469;&#25552;&#20379;&#25351;&#23548;&#24182;&#25913;&#36827;&#22768;&#26126;&#26631;&#20934;&#21270;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#31934;&#24515;&#32534;&#21046;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rise of social media, users are exposed to many misleading claims. However, the pervasive noise inherent in these posts presents a challenge in identifying precise and prominent claims that require verification. Extracting the important claims from such posts is arduous and time-consuming, yet it is an underexplored problem. Here, we aim to bridge this gap. We introduce a novel task, Claim Normalization (aka ClaimNorm), which aims to decompose complex and noisy social media posts into more straightforward and understandable forms, termed normalized claims. We propose CACN, a pioneering approach that leverages chain-of-thought and claim check-worthiness estimation, mimicking human reasoning processes, to comprehend intricate claims. Moreover, we capitalize on the in-context learning capabilities of large language models to provide guidance and to improve claim normalization. To evaluate the effectiveness of our proposed model, we meticulously compile a comprehensive real-world 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#21019;&#26032;&#30340;&#24037;&#20855;&#21644;&#25216;&#26415;&#65292;&#20351;&#29992;&#31215;&#32047;&#30340;&#23616;&#37096;&#25928;&#24212;&#65288;ALE&#65289;&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#65292;&#22312;&#35299;&#20915;&#23567;&#25968;&#25454;&#38598;&#12289;&#30452;&#35266;&#29305;&#24449;&#21644;&#20581;&#22766;&#25512;&#26029;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#12290;&#36825;&#39033;&#24037;&#20316;&#25512;&#21160;&#20102;ALE&#21450;&#20854;&#24212;&#29992;&#30340;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2310.09877</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#32463;&#20856;&#25216;&#26415;&#30340;&#32479;&#35745;&#25512;&#26029;&#65306;&#22522;&#20110;&#31215;&#32047;&#30340;&#23616;&#37096;&#25928;&#24212;&#65288;ALE&#65289;
&lt;/p&gt;
&lt;p&gt;
Statistical inference using machine learning and classical techniques based on accumulated local effects (ALE)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.09877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#21019;&#26032;&#30340;&#24037;&#20855;&#21644;&#25216;&#26415;&#65292;&#20351;&#29992;&#31215;&#32047;&#30340;&#23616;&#37096;&#25928;&#24212;&#65288;ALE&#65289;&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#65292;&#22312;&#35299;&#20915;&#23567;&#25968;&#25454;&#38598;&#12289;&#30452;&#35266;&#29305;&#24449;&#21644;&#20581;&#22766;&#25512;&#26029;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#12290;&#36825;&#39033;&#24037;&#20316;&#25512;&#21160;&#20102;ALE&#21450;&#20854;&#24212;&#29992;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31215;&#32047;&#30340;&#23616;&#37096;&#25928;&#24212;&#65288;ALE&#65289;&#26159;&#19968;&#31181;&#23545;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31639;&#27861;&#32467;&#26524;&#36827;&#34892;&#20840;&#23616;&#35299;&#37322;&#30340;&#27169;&#22411;&#26080;&#20851;&#26041;&#27861;&#12290;&#20351;&#29992;ALE&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#38754;&#20020;&#33267;&#23569;&#19977;&#20010;&#25361;&#25112;&#65306;&#30830;&#20445;ALE&#20998;&#26512;&#30340;&#21487;&#38752;&#24615;&#65292;&#23588;&#20854;&#22312;&#23567;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65307;&#30452;&#35266;&#22320;&#34920;&#24449;&#21464;&#37327;&#22312;ML&#20013;&#30340;&#25972;&#20307;&#25928;&#24212;&#65307;&#20197;&#21450;&#20174;ML&#25968;&#25454;&#20998;&#26512;&#20013;&#36827;&#34892;&#20581;&#22766;&#30340;&#25512;&#26029;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21019;&#26032;&#30340;&#24037;&#20855;&#21644;&#25216;&#26415;&#65292;&#20351;&#29992;ALE&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#65292;&#24314;&#31435;&#20102;&#36866;&#24212;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#33258;&#21161;&#27861;&#32622;&#20449;&#21306;&#38388;&#65292;&#24182;&#24341;&#20837;&#20102;&#30452;&#35266;&#25351;&#31034;&#23545;&#32467;&#26524;&#21464;&#37327;&#21644;&#26631;&#20934;&#21270;&#23610;&#24230;&#19978;&#30340;&#25928;&#24212;&#30340;ALE&#25928;&#24212;&#22823;&#23567;&#24230;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#36825;&#20123;&#24037;&#20855;&#32472;&#21046;&#21487;&#38752;&#30340;&#32479;&#35745;&#25512;&#26029;&#65292;&#21453;&#26144;&#20102;ALE&#29087;&#32451;&#31361;&#20986;&#30340;&#28789;&#27963;&#27169;&#24335;&#65292;&#23454;&#29616;&#20102;R&#20013;&#8220;ale&#8221;&#21253;&#20013;&#30340;&#23454;&#29616;&#12290;&#36825;&#39033;&#24037;&#20316;&#25512;&#21160;&#20102;&#20851;&#20110;ALE&#21450;&#20854;&#24212;&#29992;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accumulated Local Effects (ALE) is a model-agnostic approach for global explanations of the results of black-box machine learning (ML) algorithms. There are at least three challenges with conducting statistical inference based on ALE: ensuring the reliability of ALE analyses, especially in the context of small datasets; intuitively characterizing a variable's overall effect in ML; and making robust inferences from ML data analysis. In response, we introduce innovative tools and techniques for statistical inference using ALE, establishing bootstrapped confidence intervals tailored to dataset size and introducing ALE effect size measures that intuitively indicate effects on both the outcome variable scale and a normalized scale. Furthermore, we demonstrate how to use these tools to draw reliable statistical inferences, reflecting the flexible patterns ALE adeptly highlights, with implementations available in the 'ale' package in R. This work propels the discourse on ALE and its applicabi
&lt;/p&gt;</description></item><item><title>MemGPT&#26159;&#19968;&#20010;&#20197;&#35821;&#35328;&#27169;&#22411;&#20026;&#25805;&#20316;&#31995;&#32479;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#34394;&#25311;&#19978;&#19979;&#25991;&#31649;&#29702;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#36229;&#20986;&#26377;&#38480;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#19978;&#19979;&#25991;&#21033;&#29992;&#12290;&#22312;&#25991;&#26723;&#20998;&#26512;&#26041;&#38754;&#65292;MemGPT&#33021;&#22815;&#20998;&#26512;&#36229;&#20986;&#24213;&#23618;&#38480;&#21046;&#30340;&#22823;&#22411;&#25991;&#26723;&#12290;</title><link>https://arxiv.org/abs/2310.08560</link><description>&lt;p&gt;
MemGPT&#65306;&#26397;&#21521;&#20197;&#35821;&#35328;&#27169;&#22411;&#20026;&#25805;&#20316;&#31995;&#32479;&#30340;&#26041;&#21521;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
MemGPT: Towards LLMs as Operating Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.08560
&lt;/p&gt;
&lt;p&gt;
MemGPT&#26159;&#19968;&#20010;&#20197;&#35821;&#35328;&#27169;&#22411;&#20026;&#25805;&#20316;&#31995;&#32479;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#34394;&#25311;&#19978;&#19979;&#25991;&#31649;&#29702;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#36229;&#20986;&#26377;&#38480;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#19978;&#19979;&#25991;&#21033;&#29992;&#12290;&#22312;&#25991;&#26723;&#20998;&#26512;&#26041;&#38754;&#65292;MemGPT&#33021;&#22815;&#20998;&#26512;&#36229;&#20986;&#24213;&#23618;&#38480;&#21046;&#30340;&#22823;&#22411;&#25991;&#26723;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#25913;&#21464;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#20294;&#21463;&#38480;&#20110;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#20351;&#20854;&#22312;&#24310;&#38271;&#23545;&#35805;&#21644;&#25991;&#26723;&#20998;&#26512;&#31561;&#20219;&#21153;&#20013;&#30340;&#25928;&#29992;&#26377;&#25152;&#38480;&#21046;&#12290;&#20026;&#20102;&#33021;&#22815;&#21033;&#29992;&#36229;&#20986;&#26377;&#38480;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#19978;&#19979;&#25991;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#34394;&#25311;&#19978;&#19979;&#25991;&#31649;&#29702;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#20511;&#37492;&#20102;&#20256;&#32479;&#25805;&#20316;&#31995;&#32479;&#20013;&#30340;&#20998;&#23618;&#20869;&#23384;&#31995;&#32479;&#65292;&#36890;&#36807;&#22312;&#24555;&#36895;&#21644;&#24930;&#36895;&#23384;&#20648;&#22120;&#20043;&#38388;&#31227;&#21160;&#25968;&#25454;&#26469;&#25552;&#20379;&#22823;&#20869;&#23384;&#36164;&#28304;&#30340;&#22806;&#35266;&#12290;&#20511;&#21161;&#36825;&#31181;&#25216;&#26415;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MemGPT&#65288;Memory-GPT&#65289;&#65292;&#19968;&#31181;&#26234;&#33021;&#22320;&#31649;&#29702;&#19981;&#21516;&#20869;&#23384;&#23618;&#32423;&#30340;&#31995;&#32479;&#65292;&#20197;&#22312;LLM&#30340;&#26377;&#38480;&#19978;&#19979;&#25991;&#31383;&#21475;&#20869;&#25552;&#20379;&#25193;&#23637;&#19978;&#19979;&#25991;&#65292;&#24182;&#21033;&#29992;&#20013;&#26029;&#26469;&#31649;&#29702;&#33258;&#36523;&#19982;&#29992;&#25143;&#20043;&#38388;&#30340;&#25511;&#21046;&#27969;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#39046;&#22495;&#35780;&#20272;&#20102;&#25105;&#20204;&#21463;&#25805;&#20316;&#31995;&#32479;&#21551;&#21457;&#30340;&#35774;&#35745;&#65292;&#36825;&#20123;&#39046;&#22495;&#20013;&#29616;&#20195;LLM&#30340;&#26377;&#38480;&#19978;&#19979;&#25991;&#31383;&#21475;&#20005;&#37325;&#24433;&#21709;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#65306;&#25991;&#26723;&#20998;&#26512;&#65292;&#20854;&#20013;MemGPT&#33021;&#22815;&#20998;&#26512;&#36229;&#20986;&#24213;&#23618;&#38480;&#21046;&#30340;&#22823;&#22411;&#25991;&#26723;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis. To enable using context beyond limited context windows, we propose virtual context management, a technique drawing inspiration from hierarchical memory systems in traditional operating systems that provide the appearance of large memory resources through data movement between fast and slow memory. Using this technique, we introduce MemGPT (Memory-GPT), a system that intelligently manages different memory tiers in order to effectively provide extended context within the LLM's limited context window, and utilizes interrupts to manage control flow between itself and the user. We evaluate our OS-inspired design in two domains where the limited context windows of modern LLMs severely handicaps their performance: document analysis, where MemGPT is able to analyze large documents that far exceed the underlying 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#37327;&#21270;&#25351;&#26631;&#26469;&#34913;&#37327;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#35299;&#37322;&#30340;&#21487;&#20449;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#26469;&#25913;&#21892;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#35299;&#37322;&#30340;&#19968;&#33268;&#24615;&#21644;&#20445;&#30495;&#24230;&#12290;</title><link>https://arxiv.org/abs/2310.04910</link><description>&lt;p&gt;
&#20851;&#20110;&#24120;&#35782;&#25512;&#29702;&#30340;&#30693;&#35782;&#22270;&#35889;&#35299;&#37322;&#30340;&#21487;&#20449;&#24615;
&lt;/p&gt;
&lt;p&gt;
Faithful Knowledge Graph Explanations for Commonsense Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.04910
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#37327;&#21270;&#25351;&#26631;&#26469;&#34913;&#37327;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#35299;&#37322;&#30340;&#21487;&#20449;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#26469;&#25913;&#21892;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#35299;&#37322;&#30340;&#19968;&#33268;&#24615;&#21644;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34701;&#21512;&#35821;&#35328;&#27169;&#22411;(LMs)&#21644;&#30693;&#35782;&#22270;&#35889;(KGs)&#24050;&#25104;&#20026;&#24120;&#35782;&#38382;&#31572;&#30740;&#31350;&#20013;&#30340;&#24120;&#35265;&#26041;&#27861;&#65292;&#20294;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#23454;&#29616;&#31934;&#30830;&#30340;&#24605;&#36335;&#38142;&#35299;&#37322;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#24403;&#21069;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#35299;&#37322;&#25216;&#26415;&#30340;&#19968;&#20010;&#20027;&#35201;&#24369;&#28857;&#26159;&#22312;&#35780;&#20272;&#36807;&#31243;&#20013;&#24573;&#35270;&#20102;&#29983;&#25104;&#35299;&#37322;&#30340;&#21487;&#20449;&#24615;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#39564;&#35777;&#20102;&#20004;&#20010;&#37327;&#21270;&#25351;&#26631; - &#22270;&#19968;&#33268;&#24615;&#21644;&#22270;&#20445;&#30495;&#24230; - &#26469;&#34913;&#37327;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#35299;&#37322;&#30340;&#21487;&#20449;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;Consistent GNN (CGNN)&#65292;&#35813;&#26041;&#27861;&#28155;&#21152;&#20102;&#19968;&#39033;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#39033;&#26469;&#25913;&#21892;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;KG&#30340;&#39044;&#27979;&#32463;&#24120;&#20559;&#31163;&#21407;&#22987;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;&#25152;&#25552;&#20986;&#30340;CGNN&#26041;&#27861;&#25552;&#39640;&#20102;&#19968;&#33268;&#24615;&#21644;&#20445;&#30495;&#24230;&#65292;&#23637;&#31034;&#20102;&#23427;&#20135;&#29983;&#26356;&#21487;&#20449;&#35299;&#37322;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24378;&#35843;&#20102;&#26126;&#30830;&#35780;&#20272;&#35299;&#37322;&#21487;&#20449;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While fusing language models (LMs) and knowledge graphs (KGs) has become common in commonsense question answering research, enabling faithful chain-of-thought explanations in these models remains an open problem. One major weakness of current KG-based explanation techniques is that they overlook the faithfulness of generated explanations during evaluation. To address this gap, we make two main contributions: (1) We propose and validate two quantitative metrics - graph consistency and graph fidelity - to measure the faithfulness of KG-based explanations. (2) We introduce Consistent GNN (CGNN), a novel training method that adds a consistency regularization term to improve explanation faithfulness. Our analysis shows that predictions from KG often diverge from original model predictions. The proposed CGNN approach boosts consistency and fidelity, demonstrating its potential for producing more faithful explanations. Our work emphasises the importance of explicitly evaluating suggest a path
&lt;/p&gt;</description></item><item><title>&#23618;&#27425;&#21270;&#22810;&#36793;&#27719;&#36816;&#36755;&#65288;HOT&#65289;&#26694;&#26550;&#36890;&#36807;&#34701;&#21512;Gromov-Wasserstein&#65288;FGW&#65289;&#37325;&#24515;&#23558;&#22810;&#20010;&#32593;&#32476;&#20998;&#35299;&#20026;&#23545;&#40784;&#31751;&#65292;&#24182;&#23558;FGW&#36317;&#31163;&#24191;&#20041;&#21270;&#21040;&#22810;&#36793;&#27719;&#29615;&#22659;&#65292;&#23454;&#29616;&#22810;&#32593;&#32476;&#30340;&#32852;&#21512;&#23545;&#40784;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;HOT&#30456;&#23545;&#20110;&#32479;&#35745;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2310.04470</link><description>&lt;p&gt;
&#23618;&#27425;&#21270;&#22810;&#36793;&#27719;&#36816;&#36755;&#29992;&#20110;&#32593;&#32476;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Multi-Marginal Optimal Transport for Network Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.04470
&lt;/p&gt;
&lt;p&gt;
&#23618;&#27425;&#21270;&#22810;&#36793;&#27719;&#36816;&#36755;&#65288;HOT&#65289;&#26694;&#26550;&#36890;&#36807;&#34701;&#21512;Gromov-Wasserstein&#65288;FGW&#65289;&#37325;&#24515;&#23558;&#22810;&#20010;&#32593;&#32476;&#20998;&#35299;&#20026;&#23545;&#40784;&#31751;&#65292;&#24182;&#23558;FGW&#36317;&#31163;&#24191;&#20041;&#21270;&#21040;&#22810;&#36793;&#27719;&#29615;&#22659;&#65292;&#23454;&#29616;&#22810;&#32593;&#32476;&#30340;&#32852;&#21512;&#23545;&#40784;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;HOT&#30456;&#23545;&#20110;&#32479;&#35745;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#32593;&#32476;&#25214;&#21040;&#33410;&#28857;&#23545;&#24212;&#20851;&#31995;&#65292;&#21363;&#22810;&#32593;&#32476;&#23545;&#40784;&#65292;&#26159;&#22312;&#22810;&#20010;&#32593;&#32476;&#19978;&#36827;&#34892;&#32852;&#21512;&#23398;&#20064;&#30340;&#22522;&#26412;&#20808;&#20915;&#26465;&#20214;&#12290;&#23613;&#31649;&#22312;&#20004;&#20010;&#32593;&#32476;&#19978;&#23545;&#40784;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#26159;&#30001;&#20110;&#35299;&#31354;&#38388;&#25351;&#25968;&#22686;&#38271;&#21644;&#39640;&#38454;&#24046;&#24322;&#24230;&#37327;&#32570;&#20047;&#65292;&#22810;&#32593;&#32476;&#23545;&#40784;&#30340;&#25991;&#29486;&#26377;&#38480;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#27425;&#21270;&#22810;&#36793;&#27719;&#36816;&#36755;&#26694;&#26550;&#65288;HOT&#65289;&#29992;&#20110;&#22810;&#32593;&#32476;&#23545;&#40784;&#12290;&#20026;&#20102;&#22788;&#29702;&#24222;&#22823;&#30340;&#35299;&#31354;&#38388;&#65292;&#22810;&#20010;&#32593;&#32476;&#36890;&#36807;&#34701;&#21512;&#30340;Gromov-Wasserstein&#65288;FGW&#65289;&#37325;&#24515;&#34987;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#23545;&#40784;&#31751;&#12290;&#20026;&#20102;&#25551;&#32472;&#22810;&#20010;&#32593;&#32476;&#20043;&#38388;&#30340;&#39640;&#38454;&#20851;&#31995;&#65292;FGW&#36317;&#31163;&#34987;&#25512;&#24191;&#21040;&#22810;&#36793;&#27719;&#29615;&#22659;&#65292;&#22522;&#20110;&#36825;&#20010;&#36317;&#31163;&#21487;&#20197;&#23454;&#29616;&#32593;&#32476;&#30340;&#32852;&#21512;&#23545;&#40784;&#12290;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#31181;&#24555;&#36895;&#30340;&#36817;&#31471;&#28857;&#26041;&#27861;&#65292;&#20445;&#35777;&#25910;&#25947;&#21040;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;&#22823;&#37327;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;HOT&#30456;&#23545;&#20110;&#32479;&#35745;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding node correspondence across networks, namely multi-network alignment, is an essential prerequisite for joint learning on multiple networks. Despite great success in aligning networks in pairs, the literature on multi-network alignment is sparse due to the exponentially growing solution space and lack of high-order discrepancy measures. To fill this gap, we propose a hierarchical multi-marginal optimal transport framework named HOT for multi-network alignment. To handle the large solution space, multiple networks are decomposed into smaller aligned clusters via the fused Gromov-Wasserstein (FGW) barycenter. To depict high-order relationships across multiple networks, the FGW distance is generalized to the multi-marginal setting, based on which networks can be aligned jointly. A fast proximal point method is further developed with guaranteed convergence to a local optimum. Extensive experiments and analysis show that our proposed HOT achieves significant improvements over the stat
&lt;/p&gt;</description></item><item><title>KDSTM&#26159;&#19968;&#31181;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#30340;&#31070;&#32463;&#21322;&#30417;&#30563;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#23545;&#20110;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#65292;&#22312;&#27809;&#26377;&#39044;&#35757;&#32451;&#23884;&#20837;&#19988;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#25552;&#20379;&#39640;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2307.01878</link><description>&lt;p&gt;
KDSTM: &#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#30340;&#31070;&#32463;&#21322;&#30417;&#30563;&#20027;&#39064;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
KDSTM: Neural Semi-supervised Topic Modeling with Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.01878
&lt;/p&gt;
&lt;p&gt;
KDSTM&#26159;&#19968;&#31181;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#30340;&#31070;&#32463;&#21322;&#30417;&#30563;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#23545;&#20110;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#65292;&#22312;&#27809;&#26377;&#39044;&#35757;&#32451;&#23884;&#20837;&#19988;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#25552;&#20379;&#39640;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT&#21644;GPT-3&#65289;&#21487;&#20197;&#33719;&#24471;&#31454;&#20105;&#24615;&#30340;&#20934;&#30830;&#24615;&#65307;&#28982;&#32780;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#38656;&#35201;&#22312;&#22823;&#22411;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#19968;&#33324;&#30340;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#20855;&#26377;&#22312;&#19981;&#38656;&#35201;&#39044;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#20998;&#26512;&#25991;&#26723;&#24182;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#35789;&#27719;&#27169;&#24335;&#30340;&#20248;&#21183;&#12290;&#20026;&#20102;&#21033;&#29992;&#20027;&#39064;&#24314;&#27169;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26080;&#30417;&#30563;&#30340;&#35265;&#35299;&#25552;&#21462;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31216;&#20026;&#30693;&#35782;&#33976;&#39311;&#21322;&#30417;&#30563;&#20027;&#39064;&#24314;&#27169;&#65288;KDSTM&#65289;&#30340;&#26041;&#27861;&#12290;KDSTM&#19981;&#38656;&#35201;&#39044;&#35757;&#32451;&#23884;&#20837;&#65292;&#21482;&#38656;&#35201;&#23569;&#37327;&#30340;&#26631;&#35760;&#25991;&#26723;&#65292;&#24182;&#19988;&#35757;&#32451;&#25928;&#29575;&#39640;&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#38750;&#24120;&#29702;&#24819;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20998;&#31867;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#37117;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#26377;&#30417;&#30563;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#24182;&#19988;&#19982;&#26368;&#20808;&#36827;&#30340;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#30456;&#27604;&#36798;&#21040;&#20102;&#31867;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In text classification tasks, fine tuning pretrained language models like BERT and GPT-3 yields competitive accuracy; however, both methods require pretraining on large text datasets. In contrast, general topic modeling methods possess the advantage of analyzing documents to extract meaningful patterns of words without the need of pretraining. To leverage topic modeling's unsupervised insights extraction on text classification tasks, we develop the Knowledge Distillation Semi-supervised Topic Modeling (KDSTM). KDSTM requires no pretrained embeddings, few labeled documents and is efficient to train, making it ideal under resource constrained settings. Across a variety of datasets, our method outperforms existing supervised topic modeling methods in classification accuracy, robustness and efficiency and achieves similar performance compare to state of the art weakly supervised text classification methods.
&lt;/p&gt;</description></item><item><title>RETEXO&#26159;&#31532;&#19968;&#20010;&#28040;&#38500;&#20998;&#24067;&#24335;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#36890;&#20449;&#29942;&#39048;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#26032;&#30340;&#35757;&#32451;&#36807;&#31243;&#25042;&#28040;&#24687;&#20256;&#36882;&#26469;&#25913;&#21892;&#32593;&#32476;&#36890;&#20449;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2302.13053</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;&#22270;&#19978;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Scalable Neural Network Training over Distributed Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.13053
&lt;/p&gt;
&lt;p&gt;
RETEXO&#26159;&#31532;&#19968;&#20010;&#28040;&#38500;&#20998;&#24067;&#24335;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#36890;&#20449;&#29942;&#39048;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#26032;&#30340;&#35757;&#32451;&#36807;&#31243;&#25042;&#28040;&#24687;&#20256;&#36882;&#26469;&#25913;&#21892;&#32593;&#32476;&#36890;&#20449;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#28041;&#21450;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#21253;&#25324;&#39044;&#27979;&#34507;&#30333;&#36136;&#32467;&#26500;&#21644;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#31561;&#12290;&#23454;&#38469;&#19990;&#30028;&#20013;&#30340;&#22270;&#25968;&#25454;&#24448;&#24448;&#38656;&#35201;&#20998;&#24067;&#24335;&#23384;&#20648;&#22312;&#35768;&#22810;&#35745;&#31639;&#26426;&#19978;&#65292;&#21407;&#22240;&#19981;&#20165;&#26159;&#22240;&#20026;&#23481;&#37327;&#38480;&#21046;&#65292;&#36824;&#26377;&#25968;&#25454;&#25152;&#22312;&#22320;&#25110;&#38544;&#31169;&#27861;&#24459;&#30340;&#35201;&#27714;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#65292;&#32593;&#32476;&#36890;&#20449;&#25104;&#26412;&#24456;&#39640;&#65292;&#25104;&#20026;&#35757;&#32451;GNN&#30340;&#20027;&#35201;&#29942;&#39048;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#20998;&#24067;&#24335;GNN&#35757;&#32451;&#30340;&#20248;&#21270;&#20027;&#35201;&#38024;&#23545;&#25968;&#25454;&#32423;&#21035;&#30340;&#25913;&#36827;&#65292;&#20363;&#22914;&#32531;&#23384;&#12289;&#32593;&#32476;&#24863;&#30693;&#21010;&#20998;&#21644;&#23376;&#37319;&#26679;&#31561;&#65292;&#36825;&#20123;&#26041;&#27861;&#36866;&#29992;&#20110;&#25968;&#25454;&#20013;&#24515;&#31867;&#20284;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#22270;&#25968;&#25454;&#23545;&#21333;&#20010;&#23454;&#20307;&#21487;&#35775;&#38382;&#19988;&#25968;&#25454;&#20256;&#36755;&#25104;&#26412;&#34987;&#24573;&#30053;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;RETEXO&#65292;&#36825;&#26159;&#19968;&#31181;&#21487;&#20197;&#28040;&#38500;&#20998;&#24067;&#24335;GNN&#35757;&#32451;&#20013;&#20005;&#37325;&#36890;&#20449;&#29942;&#39048;&#30340;&#39318;&#20010;&#26694;&#26550;&#65292;&#21516;&#26102;&#23562;&#37325;&#20219;&#20309;&#32473;&#23450;&#30340;&#25968;&#25454;&#20998;&#21306;&#37197;&#32622;&#12290;&#20851;&#38190;&#26159;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#21363;&#25042;&#28040;&#24687;&#20256;&#36882;&#65292;&#37325;&#26032;&#25490;&#24207;&#20102;&#28040;&#24687;&#20256;&#36882;&#30340;&#39034;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) fuel diverse machine learning tasks involving graph-structured data, ranging from predicting protein structures to serving personalized recommendations. Real-world graph data must often be stored distributed across many machines not just because of capacity constraints, but because of compliance with data residency or privacy laws. In such setups, network communication is costly and becomes the main bottleneck to train GNNs. Optimizations for distributed GNN training have targeted data-level improvements so far -- via caching, network-aware partitioning, and sub-sampling -- that work for data center-like setups where graph data is accessible to a single entity and data transfer costs are ignored.   We present RETEXO, the first framework which eliminates the severe communication bottleneck in distributed GNN training while respecting any given data partitioning configuration. The key is a new training procedure, lazy message passing, that reorders the sequen
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LCPO&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20248;&#21270;&#24403;&#21069;&#32463;&#39564;&#22238;&#25253;&#30340;&#21516;&#26102;&#23558;&#31574;&#30053;&#23545;&#26087;&#32463;&#39564;&#36827;&#34892;&#38170;&#23450;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2302.02182</link><description>&lt;p&gt;
&#22312;&#38750;&#38745;&#24577;&#19978;&#19979;&#25991;&#39537;&#21160;&#29615;&#22659;&#20013;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Reinforcement Learning in Non-Stationary Context-Driven Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.02182
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LCPO&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20248;&#21270;&#24403;&#21069;&#32463;&#39564;&#22238;&#25253;&#30340;&#21516;&#26102;&#23558;&#31574;&#30053;&#23545;&#26087;&#32463;&#39564;&#36827;&#34892;&#38170;&#23450;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#38750;&#38745;&#24577;&#29615;&#22659;&#20013;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65292;&#20854;&#20013;&#19968;&#20010;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#22806;&#29983;&#19978;&#19979;&#25991;&#36807;&#31243;&#24433;&#21709;&#30528;&#29615;&#22659;&#21160;&#24577;&#12290;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#22312;&#36825;&#26679;&#30340;&#29615;&#22659;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23384;&#22312;&#8220;&#28798;&#38590;&#24615;&#36951;&#24536;&#8221;&#29616;&#35937;&#12290;&#38543;&#30528;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#26032;&#32463;&#39564;&#22686;&#21152;&#65292;&#20195;&#29702; tend to forget &#20808;&#21069;&#30340;&#30693;&#35782;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#20219;&#21153;&#26631;&#31614;&#65288;&#36825;&#22312;&#23454;&#36341;&#20013;&#24448;&#24448;&#26159;&#19981;&#23384;&#22312;&#30340;&#65289;&#25110;&#32773;&#20351;&#29992;&#33073;&#26426;&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#19981;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#24046;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Locally Constrained Policy Optimization (LCPO) &#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20248;&#21270;&#24403;&#21069;&#32463;&#39564;&#22238;&#25253;&#30340;&#21516;&#26102;&#23558;&#31574;&#30053;&#23545;&#26087;&#30340;&#32463;&#39564;&#36827;&#34892;&#38170;&#23450;&#26469;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#31181;&#38170;&#23450;&#65292;LCPO&#20351;&#29992;&#26469;&#33258;&#24403;&#21069;&#19978;&#19979;&#25991;&#20998;&#24067;&#20043;&#22806;&#30340;&#32463;&#39564;&#26679;&#26412;&#26469;&#23616;&#37096;&#32422;&#26463;&#31574;&#30053;&#20248;&#21270;&#12290;&#25105;&#20204;&#22312;Mujoco&#12289;&#32463;&#20856;&#25511;&#21046;&#21644;&#35745;&#31639;&#26426;&#31995;&#32479;&#29615;&#22659;&#20013;&#20351;&#29992;&#22810;&#31181;&#21512;&#25104;&#21644;&#30495;&#23454;&#19978;&#19979;&#25991;&#36319;&#36394;&#65292;&#35780;&#20272;&#20102;LCPO&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#23427;&#33021;&#22815;&#21462;&#24471;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study online reinforcement learning (RL) in non-stationary environments, where a time-varying exogenous context process affects the environment dynamics. Online RL is challenging in such environments due to "catastrophic forgetting" (CF). The agent tends to forget prior knowledge as it trains on new experiences. Prior approaches to mitigate this issue assume task labels (which are often not available in practice) or use off-policy methods that suffer from instability and poor performance.   We present Locally Constrained Policy Optimization (LCPO), an online RL approach that combats CF by anchoring policy outputs on old experiences while optimizing the return on current experiences. To perform this anchoring, LCPO locally constrains policy optimization using samples from experiences that lie outside of the current context distribution. We evaluate LCPO in Mujoco, classic control and computer systems environments with a variety of synthetic and real context traces, and find that it o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#26816;&#32034;&#30340;&#24102;&#33258;&#28982;&#35821;&#35328;&#30417;&#30563;&#30340;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#25968;&#25454;&#21464;&#21270;&#30340;&#20195;&#29702;&#65292;&#36890;&#36807;&#35789;&#27719;&#31354;&#38388;&#20013;&#30340;&#21452;&#32534;&#30721;&#22120;&#27169;&#22411;&#23454;&#29616;&#23545;&#25968;&#25454;&#20869;&#22312;&#29305;&#24449;&#30340;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2212.07699</link><description>&lt;p&gt;
&#22522;&#20110;&#26816;&#32034;&#30340;&#24102;&#33258;&#28982;&#35821;&#35328;&#30417;&#30563;&#30340;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Retrieval-based Disentangled Representation Learning with Natural Language Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.07699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#26816;&#32034;&#30340;&#24102;&#33258;&#28982;&#35821;&#35328;&#30417;&#30563;&#30340;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#25968;&#25454;&#21464;&#21270;&#30340;&#20195;&#29702;&#65292;&#36890;&#36807;&#35789;&#27719;&#31354;&#38388;&#20013;&#30340;&#21452;&#32534;&#30721;&#22120;&#27169;&#22411;&#23454;&#29616;&#23545;&#25968;&#25454;&#20869;&#22312;&#29305;&#24449;&#30340;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#25968;&#25454;&#20013;&#30340;&#22522;&#26412;&#21464;&#21270;&#22240;&#32032;&#24182;&#19981;&#23384;&#22312;&#12290;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#20351;&#24471;&#22312;&#26377;&#38480;&#30340;&#22240;&#32032;&#38598;&#20013;&#31351;&#23613;&#22320;&#21015;&#20030;&#21644;&#27010;&#25324;&#25152;&#26377;&#21464;&#21270;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#28982;&#32780;&#65292;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22823;&#22810;&#25968;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#37117;&#26377;&#35821;&#35328;&#31561;&#20215;&#29289;&#65292;&#36890;&#24120;&#20197;&#25991;&#26412;&#25551;&#36848;&#30340;&#24418;&#24335;&#23384;&#22312;&#12290;&#36825;&#20123;&#35821;&#35328;&#23545;&#24212;&#29289;&#21487;&#20197;&#20195;&#34920;&#25968;&#25454;&#65292;&#24182;&#36731;&#26494;&#22320;&#20998;&#35299;&#20026;&#19981;&#21516;&#30340;&#26631;&#35760;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21333;&#35789;&#34920;&#35299;&#32544;&#26816;&#32034;&#65288;VDR&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#26816;&#32034;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#28508;&#22312;&#25968;&#25454;&#21464;&#21270;&#30340;&#20195;&#29702;&#65292;&#20197;&#25512;&#21160;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#21452;&#32534;&#30721;&#22120;&#27169;&#22411;&#22312;&#35789;&#27719;&#31354;&#38388;&#20013;&#34920;&#31034;&#25968;&#25454;&#21644;&#33258;&#28982;&#35821;&#35328;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#20854;&#33258;&#28982;&#35821;&#35328;&#23545;&#24212;&#29289;&#21306;&#20998;&#25429;&#25417;&#25968;&#25454;&#20869;&#22312;&#29305;&#24449;&#30340;&#32500;&#24230;&#65292;&#20174;&#32780;&#20419;&#36827;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Disentangled representation learning remains challenging as the underlying factors of variation in the data do not naturally exist. The inherent complexity of real-world data makes it unfeasible to exhaustively enumerate and encapsulate all its variations within a finite set of factors. However, it is worth noting that most real-world data have linguistic equivalents, typically in the form of textual descriptions. These linguistic counterparts can represent the data and effortlessly decomposed into distinct tokens. In light of this, we present Vocabulary Disentangled Retrieval (VDR), a retrieval-based framework that harnesses natural language as proxies of the underlying data variation to drive disentangled representation learning. Our approach employ a bi-encoder model to represent both data and natural language in a vocabulary space, enabling the model to distinguish dimensions that capture intrinsic characteristics within data through its natural language counterpart, thus facilitat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21160;&#24577;&#28508;&#21464;&#37327;&#20998;&#31163;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22797;&#26434;&#25968;&#25454;&#20013;&#23398;&#20064;&#34920;&#36798;&#24615;&#24378;&#30340;&#28508;&#21464;&#37327;&#65292;&#25552;&#21319;&#36755;&#20986;&#30340;&#22810;&#26679;&#24615;&#12290;&#35813;&#26041;&#27861;&#21463;&#21407;&#23376;&#29289;&#29702;&#23398;&#21551;&#21457;&#65292;&#36890;&#36807;&#23398;&#20064;&#27599;&#20010;&#25968;&#25454;&#26679;&#26412;&#30340;&#32467;&#26500;&#26469;&#35299;&#37322;&#21508;&#20010;&#23376;&#32452;&#20214;&#30340;&#37325;&#35201;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#20998;&#31867;&#21644;&#29983;&#25104;&#38382;&#39064;&#20013;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2210.03728</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#21160;&#24577;&#28508;&#21464;&#37327;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Dynamic Latent Separation for Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.03728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21160;&#24577;&#28508;&#21464;&#37327;&#20998;&#31163;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22797;&#26434;&#25968;&#25454;&#20013;&#23398;&#20064;&#34920;&#36798;&#24615;&#24378;&#30340;&#28508;&#21464;&#37327;&#65292;&#25552;&#21319;&#36755;&#20986;&#30340;&#22810;&#26679;&#24615;&#12290;&#35813;&#26041;&#27861;&#21463;&#21407;&#23376;&#29289;&#29702;&#23398;&#21551;&#21457;&#65292;&#36890;&#36807;&#23398;&#20064;&#27599;&#20010;&#25968;&#25454;&#26679;&#26412;&#30340;&#32467;&#26500;&#26469;&#35299;&#37322;&#21508;&#20010;&#23376;&#32452;&#20214;&#30340;&#37325;&#35201;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#20998;&#31867;&#21644;&#29983;&#25104;&#38382;&#39064;&#20013;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#20197;&#28789;&#27963;&#21644;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#23398;&#20064;&#29992;&#20110;&#22797;&#26434;&#25968;&#25454;&#27169;&#22411;&#39044;&#27979;&#30340;&#34920;&#36798;&#24615;&#28508;&#21464;&#37327;&#65292;&#36825;&#20123;&#25968;&#25454;&#21253;&#21547;&#22810;&#20010;&#23376;&#32452;&#20214;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#34920;&#36798;&#24615;&#65292;&#25552;&#20379;&#20102;&#37096;&#20998;&#35299;&#37322;&#65292;&#24182;&#19988;&#19981;&#38480;&#20110;&#29305;&#23450;&#30340;&#24212;&#29992;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#22312;&#28508;&#31354;&#38388;&#20013;&#21160;&#24577;&#22320;&#20998;&#31163;&#25968;&#25454;&#26679;&#26412;&#65292;&#20174;&#32780;&#22686;&#24378;&#36755;&#20986;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#30340;&#21160;&#24577;&#28508;&#21464;&#37327;&#20998;&#31163;&#26041;&#27861;&#21463;&#21040;&#21407;&#23376;&#29289;&#29702;&#23398;&#30340;&#21551;&#21457;&#65292;&#20381;&#36182;&#20110;&#27599;&#20010;&#25968;&#25454;&#26679;&#26412;&#20849;&#21516;&#23398;&#20064;&#30340;&#32467;&#26500;&#65292;&#36825;&#20063;&#25581;&#31034;&#20986;&#20102;&#27599;&#20010;&#23376;&#32452;&#20214;&#22312;&#21306;&#20998;&#25968;&#25454;&#26679;&#26412;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#65292;&#21407;&#23376;&#24314;&#27169;&#65292;&#19981;&#38656;&#35201;&#23545;&#28508;&#31354;&#38388;&#36827;&#34892;&#30417;&#30563;&#65292;&#24182;&#19988;&#20801;&#35768;&#25105;&#20204;&#23398;&#20064;&#39069;&#22806;&#30340;&#37096;&#20998;&#21487;&#35299;&#37322;&#34920;&#31034;&#65292;&#38500;&#20102;&#27169;&#22411;&#30340;&#21407;&#22987;&#30446;&#26631;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#31639;&#27861;&#36824;&#25552;&#39640;&#20102;&#21508;&#31181;&#20998;&#31867;&#21644;&#29983;&#25104;&#38382;&#39064;&#20013;&#23567;&#21040;&#22823;&#35268;&#27169;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A core problem in machine learning is to learn expressive latent variables for model prediction on complex data that involves multiple sub-components in a flexible and interpretable fashion. Here, we develop an approach that improves expressiveness, provides partial interpretation, and is not restricted to specific applications. The key idea is to dynamically distance data samples in the latent space and thus enhance the output diversity. Our dynamic latent separation method, inspired by atomic physics, relies on the jointly learned structures of each data sample, which also reveal the importance of each sub-component for distinguishing data samples. This approach, atom modeling, requires no supervision of the latent space and allows us to learn extra partially interpretable representations besides the original goal of a model. We empirically demonstrate that the algorithm also enhances the performance of small to larger-scale models in various classification and generation problems.
&lt;/p&gt;</description></item><item><title>GBSVM&#26159;&#19968;&#31181;&#20351;&#29992;&#31895;&#31890;-&#29699;&#20316;&#20026;&#36755;&#20837;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#65292;&#20462;&#22797;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#38169;&#35823;&#24182;&#25512;&#23548;&#20986;&#20102;&#23545;&#20598;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#31890;&#23376;&#32676;&#20248;&#21270;&#31639;&#27861;&#21644;&#39034;&#24207;&#26368;&#23567;&#20248;&#21270;&#31639;&#27861;&#26469;&#35299;&#20915;&#38382;&#39064;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#31283;&#20581;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2210.03120</link><description>&lt;p&gt;
GBSVM: &#29992;&#31895;&#31890;-&#29699;&#20316;&#20026;&#36755;&#20837;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;
&lt;/p&gt;
&lt;p&gt;
GBSVM: Granular-ball Support Vector Machine
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.03120
&lt;/p&gt;
&lt;p&gt;
GBSVM&#26159;&#19968;&#31181;&#20351;&#29992;&#31895;&#31890;-&#29699;&#20316;&#20026;&#36755;&#20837;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#65292;&#20462;&#22797;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#38169;&#35823;&#24182;&#25512;&#23548;&#20986;&#20102;&#23545;&#20598;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#31890;&#23376;&#32676;&#20248;&#21270;&#31639;&#27861;&#21644;&#39034;&#24207;&#26368;&#23567;&#20248;&#21270;&#31639;&#27861;&#26469;&#35299;&#20915;&#38382;&#39064;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#31283;&#20581;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GBSVM&#65288;Granular-ball Support Vector Machine&#65289;&#26159;&#36890;&#36807;&#20351;&#29992;&#31895;&#31890;-&#29699;&#30340;&#31890;&#24230;&#20316;&#20026;&#36755;&#20837;&#26469;&#26500;&#24314;&#20998;&#31867;&#22120;&#30340;&#37325;&#35201;&#23581;&#35797;&#65292;&#32780;&#19981;&#26159;&#21333;&#20010;&#25968;&#25454;&#28857;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#36755;&#20837;&#19981;&#21253;&#21547;&#28857;&#30340;&#20998;&#31867;&#22120;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#27169;&#22411;&#23384;&#22312;&#19968;&#20123;&#38169;&#35823;&#65292;&#24182;&#19988;&#20854;&#23545;&#20598;&#27169;&#22411;&#23578;&#26410;&#34987;&#25512;&#23548;&#20986;&#12290;&#22240;&#27492;&#65292;&#24403;&#21069;&#31639;&#27861;&#26080;&#27861;&#23454;&#29616;&#25110;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#20462;&#22797;&#20102;&#29616;&#26377;GBSVM&#21407;&#22987;&#27169;&#22411;&#30340;&#38169;&#35823;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;&#20854;&#23545;&#20598;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#35774;&#35745;&#20102;&#31890;&#23376;&#32676;&#20248;&#21270;&#31639;&#27861;&#26469;&#35299;&#20915;&#23545;&#20598;&#27169;&#22411;&#38382;&#39064;&#12290;&#36824;&#35774;&#35745;&#20102;&#39034;&#24207;&#26368;&#23567;&#20248;&#21270;&#31639;&#27861;&#26469;&#35299;&#20915;&#23545;&#20598;&#27169;&#22411;&#12290;&#35813;&#35299;&#20915;&#26041;&#26696;&#27604;&#22522;&#20110;&#31890;&#23376;&#32676;&#20248;&#21270;&#30340;&#29256;&#26412;&#26356;&#24555;&#12289;&#26356;&#31283;&#23450;&#12290;&#22312;UCI&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GBSVM&#20855;&#26377;&#33391;&#22909;&#30340;&#31283;&#20581;&#24615;&#21644;&#25928;&#29575;&#12290;&#25152;&#26377;&#20195;&#30721;&#24050;&#22312;&#24320;&#28304;&#24211;http://&#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
GBSVM (Granular-ball Support Vector Machine) is a significant attempt to construct a classifier using the coarse-to-fine granularity of a granular-ball as input, rather than a single data point. It is the first classifier whose input contains no points. However, the existing model has some errors, and its dual model has not been derived. As a result, the current algorithm cannot be implemented or applied. To address these problems, this paper has fixed the errors of the original model of the existing GBSVM, and derived its dual model. Furthermore, a particle swarm optimization algorithm is designed to solve the dual model. The sequential minimal optimization algorithm is also carefully designed to solve the dual model. The solution is faster and more stable than the particle swarm optimization based version. The experimental results on the UCI benchmark datasets demonstrate that GBSVM has good robustness and efficiency. All codes have been released in the open source library at http://
&lt;/p&gt;</description></item><item><title>&#32452;&#21512;Q&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#35299;&#20915;&#21307;&#30103;&#29615;&#22659;&#20013;&#23384;&#22312;&#24322;&#36136;&#24615;&#27835;&#30103;&#21453;&#24212;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#22797;&#21512;&#20219;&#21153;&#32467;&#26500;&#21644;&#20998;&#31163;&#30340;&#27169;&#22359;&#21270;Q&#20540;&#20989;&#25968;&#65292;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#36827;&#34892;&#20915;&#31574;&#12290;</title><link>https://arxiv.org/abs/2110.02879</link><description>&lt;p&gt;
&#19981;&#24179;&#34913;&#24739;&#32773;&#20122;&#32676;&#30340;&#30005;&#35299;&#36136;&#34917;&#20805;&#30340;&#32452;&#21512;Q&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Compositional Q-learning for electrolyte repletion with imbalanced patient sub-populations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2110.02879
&lt;/p&gt;
&lt;p&gt;
&#32452;&#21512;Q&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#35299;&#20915;&#21307;&#30103;&#29615;&#22659;&#20013;&#23384;&#22312;&#24322;&#36136;&#24615;&#27835;&#30103;&#21453;&#24212;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#22797;&#21512;&#20219;&#21153;&#32467;&#26500;&#21644;&#20998;&#31163;&#30340;&#27169;&#22359;&#21270;Q&#20540;&#20989;&#25968;&#65292;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#36827;&#34892;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26159;&#35299;&#20915;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#30340;&#26377;&#25928;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#30103;&#29615;&#22659;&#20013;&#24212;&#29992;RL&#26041;&#27861;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#30001;&#20110;&#24739;&#32773;&#30340;&#27835;&#30103;&#21453;&#24212;&#30340;&#24322;&#36136;&#24615;&#12290;&#19968;&#20123;&#24739;&#32773;&#21487;&#20197;&#20351;&#29992;&#26631;&#20934;&#26041;&#26696;&#36827;&#34892;&#27835;&#30103;&#65292;&#32780;&#20854;&#20182;&#24739;&#32773;&#65292;&#22914;&#24930;&#24615;&#30142;&#30149;&#24739;&#32773;&#65292;&#38656;&#35201;&#20010;&#24615;&#21270;&#30340;&#27835;&#30103;&#35745;&#21010;&#12290;&#20256;&#32479;&#30340;RL&#26041;&#27861;&#36890;&#24120;&#26080;&#27861;&#32771;&#34385;&#21040;&#36825;&#31181;&#24322;&#36136;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#20551;&#35774;&#25152;&#26377;&#24739;&#32773;&#23545;&#27835;&#30103;&#30340;&#21453;&#24212;&#26159;&#30456;&#21516;&#30340;&#65288;&#21363;&#65292;&#36716;&#31227;&#21160;&#21147;&#23398;&#26159;&#20849;&#20139;&#30340;&#65289;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#32452;&#21512;Fitted $Q$-&#36845;&#20195;&#65288;CFQI&#65289;&#65292;&#23427;&#20351;&#29992;&#22797;&#21512;&#20219;&#21153;&#32467;&#26500;&#26469;&#34920;&#31034;&#21307;&#30103;&#29615;&#22659;&#20013;&#30340;&#24322;&#36136;&#24615;&#27835;&#30103;&#21453;&#24212;&#12290;&#22797;&#21512;&#20219;&#21153;&#30001;&#30456;&#21516;&#20219;&#21153;&#30340;&#20960;&#20010;&#21464;&#20307;&#32452;&#25104;&#65292;&#27599;&#20010;&#21464;&#20307;&#30340;&#38590;&#24230;&#36880;&#28176;&#22686;&#21152;&#65307;&#35299;&#20915;&#36739;&#31616;&#21333;&#30340;&#21464;&#20307;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#25928;&#22320;&#35299;&#20915;&#26356;&#38590;&#30340;&#21464;&#20307;&#12290;CFQI&#20351;&#29992;&#19968;&#20010;&#22797;&#21512;$Q$&#20540;&#20989;&#25968;&#65292;&#20854;&#20013;&#20026;&#27599;&#20010;&#20219;&#21153;&#27169;&#22359;&#21333;&#29420;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) is an effective framework for solving sequential decision-making tasks. However, applying RL methods in medical care settings is challenging in part due to heterogeneity in treatment response among patients. Some patients can be treated with standard protocols whereas others, such as those with chronic diseases, need personalized treatment planning. Traditional RL methods often fail to account for this heterogeneity, because they assume that all patients respond to the treatment in the same way (i.e., transition dynamics are shared). We introduce Compositional Fitted $Q$-iteration (CFQI), which uses a compositional task structure to represent heterogeneous treatment responses in medical care settings. A compositional task consists of several variations of the same task, each progressing in difficulty; solving simpler variants of the task can enable efficient solving of harder variants. CFQI uses a compositional $Q$-value function with separate modules for ea
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#21019;&#24847;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#24182;&#20351;&#29992;&#35745;&#31639;&#21019;&#36896;&#21147;&#30340;&#35270;&#35282;&#23545;&#20854;&#36827;&#34892;&#20102;&#27010;&#24565;&#21270;&#21644;&#35780;&#20272;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#21487;&#20197;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#32467;&#26524;&#65292;&#20294;&#20854;&#21019;&#26032;&#24615;&#21463;&#21040;&#38480;&#21046;&#65292;&#21516;&#26102;&#20063;&#23384;&#22312;&#20869;&#37096;&#38382;&#39064;&#34920;&#36798;&#26080;&#27861;&#26356;&#25913;&#20197;&#21450;&#22312;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#24314;&#31435;&#32852;&#31995;&#30340;&#32570;&#20047;&#33021;&#21147;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2012.02282</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#21019;&#36896;&#21147;&#65306;&#27010;&#24565;&#21270;&#19982;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Creativity of Deep Learning: Conceptualization and Assessment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2012.02282
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#21019;&#24847;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#24182;&#20351;&#29992;&#35745;&#31639;&#21019;&#36896;&#21147;&#30340;&#35270;&#35282;&#23545;&#20854;&#36827;&#34892;&#20102;&#27010;&#24565;&#21270;&#21644;&#35780;&#20272;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#21487;&#20197;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#32467;&#26524;&#65292;&#20294;&#20854;&#21019;&#26032;&#24615;&#21463;&#21040;&#38480;&#21046;&#65292;&#21516;&#26102;&#20063;&#23384;&#22312;&#20869;&#37096;&#38382;&#39064;&#34920;&#36798;&#26080;&#27861;&#26356;&#25913;&#20197;&#21450;&#22312;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#24314;&#31435;&#32852;&#31995;&#30340;&#32570;&#20047;&#33021;&#21147;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#22312;&#33258;&#21160;&#21270;&#31616;&#21333;&#20219;&#21153;&#26041;&#38754;&#30340;&#28508;&#21147;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#25506;&#32034;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#24320;&#22987;&#25506;&#32034;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#21019;&#36896;&#24615;&#35774;&#35745;&#65292;&#26080;&#35770;&#26159;&#23436;&#25972;&#30340;&#20135;&#21697;&#21019;&#20316;&#36824;&#26159;&#25903;&#25345;&#20154;&#31867;&#22312;&#21019;&#20316;&#36807;&#31243;&#20013;&#30340;&#35282;&#33394;&#12290;&#26412;&#25991;&#21033;&#29992;&#35745;&#31639;&#21019;&#36896;&#21147;&#30340;&#35265;&#35299;&#65292;&#27010;&#24565;&#21270;&#21644;&#35780;&#20272;&#20102;&#25991;&#29486;&#32508;&#36848;&#20013;&#30830;&#23450;&#30340;&#21019;&#36896;&#24615;&#39046;&#22495;&#20013;&#24403;&#21069;&#24212;&#29992;&#29983;&#25104;&#24335;&#28145;&#24230;&#23398;&#20064;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#24403;&#21069;&#31995;&#32479;&#19982;&#19981;&#21516;&#27169;&#22411;&#20154;&#31867;&#21019;&#36896;&#21147;&#30340;&#30456;&#20284;&#20043;&#22788;&#20197;&#21450;&#23427;&#20204;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#21487;&#20197;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#31561;&#39640;&#20215;&#20540;&#32467;&#26524;&#65292;&#20294;&#20854;&#26032;&#39062;&#24615;&#36890;&#24120;&#21463;&#21040;&#22810;&#31181;&#38480;&#21046;&#65292;&#20363;&#22914;&#21463;&#21040;&#35757;&#32451;&#25968;&#25454;&#23450;&#20041;&#30340;&#27010;&#24565;&#31354;&#38388;&#30340;&#38480;&#21046;&#12290;&#24403;&#21069;&#30340;DL&#26041;&#27861;&#20063;&#19981;&#20801;&#35768;&#23545;&#20869;&#37096;&#38382;&#39064;&#34920;&#36798;&#36827;&#34892;&#26356;&#25913;&#65292;&#24182;&#19988;&#23427;&#20204;&#32570;&#20047;&#22312;&#39640;&#24230;&#19981;&#21516;&#30340;&#39046;&#22495;&#20043;&#38388;&#24314;&#31435;&#32852;&#31995;&#30340;&#33021;&#21147;&#65292;&#36825;&#20004;&#28857;&#37117;&#34987;&#35748;&#20026;&#26159;&#20027;&#35201;&#30340;&#25512;&#21160;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the potential of deep learning (DL) for automating simple tasks is already well explored, recent research has started investigating the use of deep learning for creative design, both for complete artifact creation and supporting humans in the creation process. In this paper, we use insights from computational creativity to conceptualize and assess current applications of generative deep learning in creative domains identified in a literature review. We highlight parallels between current systems and different models of human creativity as well as their shortcomings. While deep learning yields results of high value, such as high-quality images, their novelty is typically limited due to multiple reasons such as being tied to a conceptual space defined by training data. Current DL methods also do not allow for changes in the internal problem representation, and they lack the capability to identify connections across highly different domains, both of which are seen as major drivers o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20462;&#22797;&#32593;&#39029;&#26080;&#38556;&#30861;&#36829;&#35268;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#26102;&#20462;&#25913;&#25991;&#26723;&#23545;&#35937;&#27169;&#22411;(DOM)&#21644;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20197;&#21450;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#33258;&#21160;&#20462;&#22797;&#26080;&#38556;&#30861;&#38169;&#35823;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.16450</link><description>&lt;p&gt;
ACCESS&#65306;&#29992;&#20110;&#33258;&#21160;&#20462;&#22797;&#32593;&#39029;&#26080;&#38556;&#30861;&#36829;&#35268;&#30340;&#25552;&#31034;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
ACCESS: Prompt Engineering for Automated Web Accessibility Violation Corrections. (arXiv:2401.16450v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20462;&#22797;&#32593;&#39029;&#26080;&#38556;&#30861;&#36829;&#35268;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#26102;&#20462;&#25913;&#25991;&#26723;&#23545;&#35937;&#27169;&#22411;(DOM)&#21644;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20197;&#21450;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#33258;&#21160;&#20462;&#22797;&#26080;&#38556;&#30861;&#38169;&#35823;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#21253;&#23481;&#24615;&#21644;&#29992;&#25143;&#21451;&#22909;&#25216;&#26415;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#65292;&#32593;&#39029;&#26080;&#38556;&#30861;&#23545;&#20110;&#30830;&#20445;&#27531;&#38556;&#20154;&#22763;(&#21253;&#25324;&#35270;&#35273;&#12289;&#21548;&#35273;&#12289;&#35748;&#30693;&#25110;&#36816;&#21160;&#38556;&#30861;)&#24179;&#31561;&#33719;&#21462;&#22312;&#32447;&#20869;&#23481;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#23384;&#22312;&#35832;&#22914;Web&#20869;&#23481;&#26080;&#38556;&#30861;&#25351;&#21335;(WCAG)&#21644;Web&#26080;&#38556;&#30861;&#20513;&#35758;(W3C)&#31561;&#26080;&#38556;&#30861;&#25351;&#23548;&#26041;&#38024;&#21644;&#26631;&#20934;&#65292;&#20294;&#36229;&#36807;90%&#30340;&#32593;&#31449;&#20173;&#26080;&#27861;&#28385;&#36275;&#24517;&#35201;&#30340;&#26080;&#38556;&#30861;&#35201;&#27714;&#12290;&#23545;&#20110;&#27531;&#38556;&#20154;&#22763;&#26469;&#35828;&#65292;&#38656;&#35201;&#19968;&#31181;&#24037;&#20855;&#26469;&#33258;&#21160;&#20462;&#22797;&#32593;&#39029;&#30340;&#26080;&#38556;&#30861;&#38169;&#35823;&#12290;&#34429;&#28982;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#21457;&#29616;&#21644;&#23450;&#20301;&#26080;&#38556;&#30861;&#38169;&#35823;&#30340;&#26041;&#27861;&#65292;&#20294;&#27809;&#26377;&#30740;&#31350;&#19987;&#27880;&#20110;&#26377;&#25928;&#20462;&#22797;&#27492;&#31867;&#36829;&#35268;&#34892;&#20026;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23454;&#26102;&#20462;&#25913;&#25991;&#26723;&#23545;&#35937;&#27169;&#22411;(DOM)&#26469;&#20462;&#22797;&#32593;&#39029;&#26080;&#38556;&#30861;&#36829;&#35268;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#26080;&#38556;&#30861;&#38169;&#35823;&#20449;&#24687;&#12289;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21644;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing need for inclusive and user-friendly technology, web accessibility is crucial to ensuring equal access to online content for individuals with disabilities, including visual, auditory, cognitive, or motor impairments. Despite the existence of accessibility guidelines and standards such as Web Content Accessibility Guidelines (WCAG) and the Web Accessibility Initiative (W3C), over 90\% of websites still fail to meet the necessary accessibility requirements. For web users with disabilities, there exists a need for a tool to automatically fix web page accessibility errors. While research has demonstrated methods to find and target accessibility errors, no research has focused on effectively correcting such violations. This paper presents a novel approach to correcting accessibility violations on the web by modifying the document object model (DOM) in real time with foundation models. Leveraging accessibility error information, large language models (LLMs), and prompt en
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21021;&#27493;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20116;&#20010;&#20851;&#38190;&#20027;&#39064;&#30340;&#20998;&#26512;&#21644;&#32531;&#35299;&#31574;&#30053;&#26469;&#24314;&#31435;&#31185;&#23398;&#30740;&#31350;&#20013;&#29983;&#25104;AI&#30340;&#20262;&#29702;&#25351;&#21335;&#12290;&#20840;&#29699;&#20849;&#35782;&#12289;&#19987;&#19994;&#22521;&#35757;&#21644;&#21512;&#29702;&#30340;&#25191;&#34892;&#23545;&#20110;&#20419;&#36827;AI&#30340;&#30410;&#22788;&#21644;&#32500;&#25252;&#30740;&#31350;&#35802;&#20449;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2401.15284</link><description>&lt;p&gt;
&#22312;&#31185;&#23398;&#30740;&#31350;&#20013;&#24314;&#31435;&#29983;&#25104;AI&#30340;&#20262;&#29702;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
Building ethical guidelines for generative AI in scientific research. (arXiv:2401.15284v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21021;&#27493;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20116;&#20010;&#20851;&#38190;&#20027;&#39064;&#30340;&#20998;&#26512;&#21644;&#32531;&#35299;&#31574;&#30053;&#26469;&#24314;&#31435;&#31185;&#23398;&#30740;&#31350;&#20013;&#29983;&#25104;AI&#30340;&#20262;&#29702;&#25351;&#21335;&#12290;&#20840;&#29699;&#20849;&#35782;&#12289;&#19987;&#19994;&#22521;&#35757;&#21644;&#21512;&#29702;&#30340;&#25191;&#34892;&#23545;&#20110;&#20419;&#36827;AI&#30340;&#30410;&#22788;&#21644;&#32500;&#25252;&#30740;&#31350;&#35802;&#20449;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#65288;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#27491;&#22312;&#36805;&#36895;&#25913;&#21464;&#23398;&#26415;&#30740;&#31350;&#21644;&#23454;&#38469;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#31185;&#23398;&#20013;&#29983;&#25104;AI&#30340;&#20262;&#29702;&#25351;&#21335;&#30340;&#35752;&#35770;&#20173;&#28982;&#38646;&#25955;&#65292;&#24378;&#35843;&#20102;&#21327;&#21830;&#19968;&#33268;&#24615;&#26631;&#20934;&#30340;&#32039;&#36843;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#20116;&#20010;&#20851;&#38190;&#20027;&#39064;&#30340;&#20998;&#26512;&#21644;&#32531;&#35299;&#31574;&#30053;&#30340;&#24320;&#21457;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#21021;&#27493;&#30340;&#26694;&#26550;&#65306;&#20102;&#35299;&#27169;&#22411;&#22312;&#30495;&#23454;&#24615;&#21644;&#20559;&#35265;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65307;&#23562;&#37325;&#38544;&#31169;&#12289;&#26426;&#23494;&#21644;&#29256;&#26435;&#65307;&#22312;&#34701;&#20837;&#27169;&#22411;&#36755;&#20986;&#26102;&#36991;&#20813;&#25220;&#34989;&#21644;&#36829;&#21453;&#25919;&#31574;&#65307;&#30830;&#20445;&#24212;&#29992;&#24102;&#26469;&#24635;&#20307;&#21033;&#30410;&#65307;&#20197;&#21450;&#36879;&#26126;&#12289;&#21487;&#22797;&#21046;&#22320;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#12290;&#36890;&#36807;&#21015;&#20030;&#24120;&#35265;&#22330;&#26223;&#26469;&#23637;&#31034;&#28508;&#22312;&#30340;&#20262;&#29702;&#36829;&#35268;&#34892;&#20026;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20840;&#29699;&#20849;&#35782;&#20197;&#21450;&#19987;&#19994;&#22521;&#35757;&#21644;&#21512;&#29702;&#30340;&#25191;&#34892;&#26159;&#20419;&#36827;AI&#30340;&#30410;&#22788;&#24182;&#32500;&#25252;&#30740;&#31350;&#35802;&#20449;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative artificial intelligence tools like large language models are rapidly transforming academic research and real world applications. However, discussions on ethical guidelines for generative AI in science remain fragmented, underscoring the urgent need for consensus based standards. This paper offers an initial framework by developing analyses and mitigation strategies across five key themes: understanding model limitations regarding truthfulness and bias; respecting privacy, confidentiality, and copyright; avoiding plagiarism and policy violations when incorporating model output; ensuring applications provide overall benefit; and using AI transparently and reproducibly. Common scenarios are outlined to demonstrate potential ethical violations. We argue that global consensus coupled with professional training and reasonable enforcement are critical to promoting the benefits of AI while safeguarding research integrity.
&lt;/p&gt;</description></item><item><title>PROXYQA&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38271;&#31687;&#25991;&#26412;&#29983;&#25104;&#30340;&#26367;&#20195;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#35814;&#23613;&#30340;&#20869;&#23481;&#65292;&#24182;&#21033;&#29992;&#35780;&#20272;&#22120;&#21644;&#29983;&#25104;&#20869;&#23481;&#20316;&#20026;&#32972;&#26223;&#29615;&#22659;&#65292;&#26681;&#25454;&#35780;&#20272;&#22120;&#22238;&#31572;&#20195;&#29702;&#38382;&#39064;&#30340;&#34920;&#29616;&#26469;&#35780;&#20272;&#29983;&#25104;&#20869;&#23481;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.15042</link><description>&lt;p&gt;
PROXYQA&#65306;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38271;&#31687;&#25991;&#26412;&#29983;&#25104;&#30340;&#26367;&#20195;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
PROXYQA: An Alternative Framework for Evaluating Long-Form Text Generation with Large Language Models. (arXiv:2401.15042v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15042
&lt;/p&gt;
&lt;p&gt;
PROXYQA&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38271;&#31687;&#25991;&#26412;&#29983;&#25104;&#30340;&#26367;&#20195;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#35814;&#23613;&#30340;&#20869;&#23481;&#65292;&#24182;&#21033;&#29992;&#35780;&#20272;&#22120;&#21644;&#29983;&#25104;&#20869;&#23481;&#20316;&#20026;&#32972;&#26223;&#29615;&#22659;&#65292;&#26681;&#25454;&#35780;&#20272;&#22120;&#22238;&#31572;&#20195;&#29702;&#38382;&#39064;&#30340;&#34920;&#29616;&#26469;&#35780;&#20272;&#29983;&#25104;&#20869;&#23481;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#38271;&#31687;&#25991;&#26412;&#29702;&#35299;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#29983;&#25104;&#38271;&#31687;&#20869;&#23481;&#65288;&#22914;&#25253;&#21578;&#21644;&#25991;&#31456;&#65289;&#30340;&#33021;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#24403;&#21069;&#30340;&#22522;&#20934;&#19981;&#36275;&#20197;&#20805;&#20998;&#35780;&#20272;LLMs&#29983;&#25104;&#20449;&#24687;&#20016;&#23500;&#19988;&#20840;&#38754;&#30340;&#20869;&#23481;&#65292;&#22240;&#27492;&#38656;&#35201;&#19968;&#31181;&#26356;&#20005;&#26684;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;\textsc{ProxyQA}&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#38271;&#31687;&#25991;&#26412;&#29983;&#25104;&#65292;&#21253;&#25324;&#28145;&#20837;&#20154;&#24037;&#31574;&#21010;&#30340;&#28085;&#30422;&#22810;&#20010;&#39046;&#22495;&#30340;&#8220;&#20803;&#38382;&#39064;&#8221;&#12290;&#27599;&#20010;&#20803;&#38382;&#39064;&#37117;&#21253;&#21547;&#30456;&#24212;&#30340;&#24102;&#27880;&#37322;&#31572;&#26696;&#30340;&#8220;&#20195;&#29702;&#38382;&#39064;&#8221;&#12290;LLMs&#34987;&#35201;&#27714;&#26681;&#25454;&#36825;&#20123;&#20803;&#38382;&#39064;&#29983;&#25104;&#35814;&#23613;&#30340;&#20869;&#23481;&#12290;&#21033;&#29992;&#35780;&#20272;&#22120;&#24182;&#23558;&#29983;&#25104;&#30340;&#20869;&#23481;&#20316;&#20026;&#32972;&#26223;&#29615;&#22659;&#65292;\textsc{ProxyQA}&#26681;&#25454;&#35780;&#20272;&#22120;&#22238;&#31572;&#8220;&#20195;&#29702;&#38382;&#39064;&#8221;&#30340;&#34920;&#29616;&#35780;&#20272;&#29983;&#25104;&#20869;&#23481;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#26816;&#39564;&#20102;&#22810;&#20010;LLMs&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have exhibited remarkable success in long-form context comprehension tasks. However, their capacity to generate long contents, such as reports and articles, remains insufficiently explored. Current benchmarks do not adequately assess LLMs' ability to produce informative and comprehensive content, necessitating a more rigorous evaluation approach. In this study, we introduce \textsc{ProxyQA}, a framework for evaluating long-form text generation, comprising in-depth human-curated \textit{meta-questions} spanning various domains. Each meta-question contains corresponding \textit{proxy-questions} with annotated answers. LLMs are prompted to generate extensive content in response to these meta-questions. Utilizing an evaluator and incorporating generated content as background context, \textsc{ProxyQA} evaluates the quality of generated content based on the evaluator's performance in answering the \textit{proxy-questions}. We examine multiple LLMs, emphasizing \t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#31354;&#38388;&#33258;&#36866;&#24212;&#28388;&#27874;&#22120;&#65288;CSF&#65289;&#65292;&#21487;&#20197;&#20174;&#22270;&#25299;&#25169;&#21644;&#33410;&#28857;&#23646;&#24615;&#31354;&#38388;&#20013;&#25552;&#21462;&#33258;&#36866;&#24212;&#39057;&#29575;&#20449;&#24687;&#65292;&#20197;&#20943;&#36731;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.14876</link><description>&lt;p&gt;
&#36328;&#31354;&#38388;&#33258;&#36866;&#24212;&#28388;&#27874;&#22120;&#65306;&#38598;&#25104;&#22270;&#25299;&#25169;&#21644;&#33410;&#28857;&#23646;&#24615;&#20197;&#20943;&#36731;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Cross-Space Adaptive Filter: Integrating Graph Topology and Node Attributes for Alleviating the Over-smoothing Problem. (arXiv:2401.14876v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#31354;&#38388;&#33258;&#36866;&#24212;&#28388;&#27874;&#22120;&#65288;CSF&#65289;&#65292;&#21487;&#20197;&#20174;&#22270;&#25299;&#25169;&#21644;&#33410;&#28857;&#23646;&#24615;&#31354;&#38388;&#20013;&#25552;&#21462;&#33258;&#36866;&#24212;&#39057;&#29575;&#20449;&#24687;&#65292;&#20197;&#20943;&#36731;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#20351;&#29992;&#20302;&#36890;&#28388;&#27874;&#22120;&#20174;&#22270;&#25299;&#25169;&#20013;&#25552;&#21462;&#20302;&#39057;&#20449;&#21495;&#65292;&#20294;&#24403;GCN&#28145;&#24230;&#22686;&#21152;&#26102;&#21487;&#33021;&#23548;&#33268;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#20174;&#22270;&#25299;&#25169;&#20013;&#25552;&#21462;&#30340;&#39069;&#22806;&#28388;&#27874;&#22120;&#65288;&#22914;&#39640;&#36890;&#28388;&#27874;&#22120;&#65289;&#26469;&#21019;&#24314;&#33258;&#36866;&#24212;&#28388;&#27874;&#22120;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#25299;&#25169;&#20449;&#24687;&#65292;&#24182;&#24573;&#35270;&#20102;&#33410;&#28857;&#23646;&#24615;&#31354;&#38388;&#65292;&#36825;&#20005;&#37325;&#29306;&#29298;&#20102;&#28145;&#23618;GCN&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#38750;&#21516;&#37197;&#22270;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#31354;&#38388;&#33258;&#36866;&#24212;&#28388;&#27874;&#22120;&#65292;&#31216;&#20026;CSF&#65292;&#33021;&#22815;&#20174;&#25299;&#25169;&#21644;&#23646;&#24615;&#31354;&#38388;&#20013;&#25552;&#21462;&#33258;&#36866;&#24212;&#39057;&#29575;&#20449;&#24687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;&#22522;&#20110;&#23646;&#24615;&#30340;&#39640;&#36890;&#28388;&#27874;&#22120;&#65292;&#21487;&#20197;&#20174;&#29702;&#35770;&#19978;&#35299;&#37322;&#20026;&#21322;&#30417;&#30563;&#26680;&#23725;&#22238;&#24402;&#30340;&#26368;&#23567;&#21270;&#22120;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#22522;&#20110;&#25299;&#25169;&#30340;&#20302;&#36890;&#28388;&#27874;&#22120;&#35270;&#20026;Mercer's&#26680;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The vanilla Graph Convolutional Network (GCN) uses a low-pass filter to extract low-frequency signals from graph topology, which may lead to the over-smoothing problem when GCN goes deep. To this end, various methods have been proposed to create an adaptive filter by incorporating an extra filter (e.g., a high-pass filter) extracted from the graph topology. However, these methods heavily rely on topological information and ignore the node attribute space, which severely sacrifices the expressive power of the deep GCNs, especially when dealing with disassortative graphs. In this paper, we propose a cross-space adaptive filter, called CSF, to produce the adaptive-frequency information extracted from both the topology and attribute spaces. Specifically, we first derive a tailored attribute-based high-pass filter that can be interpreted theoretically as a minimizer for semi-supervised kernel ridge regression. Then, we cast the topology-based low-pass filter as a Mercer's kernel within the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26088;&#22312;&#35299;&#20915;&#38271;&#26399;&#23545;&#35805;&#20013;&#35282;&#33394;&#21477;&#23376;&#19981;&#20855;&#20449;&#24687;&#24615;&#30340;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#24120;&#35782;&#22686;&#24378;&#30340;&#35282;&#33394;&#25193;&#23637;&#65292;&#24182;&#35774;&#35745;&#31574;&#30053;&#23558;&#30456;&#20114;&#30683;&#30462;&#30340;&#35282;&#33394;&#36716;&#21270;&#20026;&#21253;&#21547;&#20016;&#23500;&#35828;&#35805;&#32773;&#20449;&#24687;&#30340;&#21477;&#23376;&#65292;&#20197;&#25552;&#39640;&#22238;&#24212;&#29983;&#25104;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.14215</link><description>&lt;p&gt;
&#36890;&#36807;&#19978;&#19979;&#25991;&#24863;&#30693;&#20010;&#24615;&#21270;&#32454;&#21270;&#65292;&#22686;&#24378;&#38271;&#26399;&#23545;&#35805;&#20013;&#30340;&#24120;&#35782;&#22686;&#24378;&#24615;&#20869;&#23384;&#26500;&#24314;&#21644;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Commonsense-augmented Memory Construction and Management in Long-term Conversations via Context-aware Persona Refinement. (arXiv:2401.14215v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26088;&#22312;&#35299;&#20915;&#38271;&#26399;&#23545;&#35805;&#20013;&#35282;&#33394;&#21477;&#23376;&#19981;&#20855;&#20449;&#24687;&#24615;&#30340;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#24120;&#35782;&#22686;&#24378;&#30340;&#35282;&#33394;&#25193;&#23637;&#65292;&#24182;&#35774;&#35745;&#31574;&#30053;&#23558;&#30456;&#20114;&#30683;&#30462;&#30340;&#35282;&#33394;&#36716;&#21270;&#20026;&#21253;&#21547;&#20016;&#23500;&#35828;&#35805;&#32773;&#20449;&#24687;&#30340;&#21477;&#23376;&#65292;&#20197;&#25552;&#39640;&#22238;&#24212;&#29983;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38271;&#26399;&#23545;&#35805;&#20013;&#65292;&#35760;&#24518;&#21644;&#21033;&#29992;&#35828;&#35805;&#32773;&#30340;&#35282;&#33394;&#26159;&#29983;&#25104;&#22238;&#24212;&#30340;&#24120;&#35265;&#20570;&#27861;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#32534;&#20889;&#30340;&#25968;&#25454;&#38598;&#36890;&#24120;&#25552;&#20379;&#26080;&#20449;&#24687;&#30340;&#35282;&#33394;&#21477;&#23376;&#65292;&#36825;&#22952;&#30861;&#20102;&#22238;&#24212;&#36136;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#24120;&#35782;&#22686;&#24378;&#30340;&#35282;&#33394;&#25193;&#23637;&#26469;&#35299;&#20915;&#38271;&#26399;&#23545;&#35805;&#20013;&#30340;&#36825;&#20123;&#38382;&#39064;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#19981;&#20135;&#29983;&#19982;&#20854;&#20182;&#35282;&#33394;&#30456;&#30683;&#30462;&#30340;&#35282;&#33394;&#65292;&#25105;&#20204;&#20391;&#37325;&#20110;&#26681;&#25454;&#35774;&#35745;&#30340;&#31574;&#30053;&#65292;&#23558;&#30456;&#20114;&#30683;&#30462;&#30340;&#35282;&#33394;&#36716;&#21270;&#20026;&#21253;&#21547;&#20016;&#23500;&#35828;&#35805;&#32773;&#20449;&#24687;&#30340;&#21477;&#23376;&#65292;&#20197;&#27492;&#26469;&#32454;&#21270;&#23427;&#20204;&#30340;&#19978;&#19979;&#25991;&#32972;&#26223;&#12290;&#20316;&#20026;&#22810;&#20250;&#35805;&#24773;&#22659;&#20013;&#35282;&#33394;&#25193;&#23637;&#30340;&#20808;&#39537;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#31867;&#20154;&#20010;&#24615;&#32454;&#21270;&#20419;&#36827;&#20102;&#26356;&#22909;&#30340;&#22238;&#24212;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Memorizing and utilizing speakers' personas is a common practice for response generation in long-term conversations. Yet, human-authored datasets often provide uninformative persona sentences that hinder response quality. This paper presents a novel framework that leverages commonsense-based persona expansion to address such issues in long-term conversation. While prior work focuses on not producing personas that contradict others, we focus on transforming contradictory personas into sentences that contain rich speaker information, by refining them based on their contextual backgrounds with designed strategies. As the pioneer of persona expansion in multi-session settings, our framework facilitates better response generation via human-like persona refinement. The supplementary video of our work is available at https://caffeine-15bbf.web.app/.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#40657;&#30418;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#36234;&#29425;&#25915;&#20987;&#25552;&#31034;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#22797;&#26434;&#24615;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#38480;&#21046;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#33258;&#36523;&#65292;&#23558;&#26377;&#23475;&#25552;&#31034;&#37325;&#20889;&#20026;&#38750;&#26377;&#23475;&#34920;&#36798;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;80%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#24182;&#19988;&#21363;&#20351;&#27169;&#22411;&#26356;&#26032;&#65292;&#25928;&#26524;&#20173;&#28982;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2401.09798</link><description>&lt;p&gt;
&#19968;&#31181;&#31616;&#21333;&#30340;&#40657;&#30418;&#26041;&#27861;&#29992;&#20110;&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
All in How You Ask for It: Simple Black-Box Method for Jailbreak Attacks. (arXiv:2401.09798v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#40657;&#30418;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#36234;&#29425;&#25915;&#20987;&#25552;&#31034;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#22797;&#26434;&#24615;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#38480;&#21046;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#33258;&#36523;&#65292;&#23558;&#26377;&#23475;&#25552;&#31034;&#37325;&#20889;&#20026;&#38750;&#26377;&#23475;&#34920;&#36798;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;80%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#24182;&#19988;&#21363;&#20351;&#27169;&#22411;&#26356;&#26032;&#65292;&#25928;&#26524;&#20173;&#28982;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30528;&#8220;&#36234;&#29425;&#8221;&#25361;&#25112;&#65292;&#21363;&#35268;&#36991;&#20445;&#38556;&#25514;&#26045;&#20197;&#20135;&#29983;&#19981;&#31526;&#21512;&#20262;&#29702;&#30340;&#25552;&#31034;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#40657;&#30418;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#29983;&#25104;&#36234;&#29425;&#25552;&#31034;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#39640;&#22797;&#26434;&#24615;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#38480;&#21046;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#33258;&#36523;&#65292;&#36845;&#20195;&#22320;&#23558;&#26377;&#23475;&#25552;&#31034;&#37325;&#20889;&#20026;&#38750;&#26377;&#23475;&#34920;&#36798;&#65292;&#22522;&#20110;&#20551;&#35774;&#35748;&#20026;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#29983;&#25104;&#35268;&#36991;&#20445;&#38556;&#30340;&#34920;&#36798;&#12290;&#36890;&#36807;&#22312;ChatGPT&#65288;GPT-3.5&#21644;GPT-4&#65289;&#21644;Gemini-Pro&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#24179;&#22343;5&#27425;&#36845;&#20195;&#20869;&#23454;&#29616;&#20102;&#36229;&#36807;80%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#24182;&#19988;&#21363;&#20351;&#27169;&#22411;&#26356;&#26032;&#65292;&#25928;&#26524;&#20173;&#28982;&#26377;&#25928;&#12290;&#29983;&#25104;&#30340;&#36234;&#29425;&#25552;&#31034;&#33258;&#28982;&#32780;&#31616;&#32451;&#65292;&#34920;&#26126;&#23427;&#20204;&#36739;&#19981;&#26131;&#34987;&#26816;&#27979;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21019;&#24314;&#26377;&#25928;&#30340;&#36234;&#29425;&#25552;&#31034;&#27604;&#20808;&#21069;&#30740;&#31350;&#35748;&#20026;&#30340;&#35201;&#31616;&#21333;&#65292;&#24182;&#19988;&#40657;&#30418;&#36234;&#29425;&#25915;&#20987;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) like ChatGPT face `jailbreak' challenges, where safeguards are bypassed to produce ethically harmful prompts. This study introduces a simple black-box method to effectively generate jailbreak prompts, overcoming the limitations of high complexity and computational costs associated with existing methods. The proposed technique iteratively rewrites harmful prompts into non-harmful expressions using the target LLM itself, based on the hypothesis that LLMs can directly sample safeguard-bypassing expressions. Demonstrated through experiments with ChatGPT (GPT-3.5 and GPT-4) and Gemini-Pro, this method achieved an attack success rate of over 80% within an average of 5 iterations and remained effective despite model updates. The jailbreak prompts generated were naturally-worded and concise, suggesting they are less detectable. The results indicate that creating effective jailbreak prompts is simpler than previously considered, and black-box jailbreak attacks pose 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#20351;&#29992;&#25463;&#24452;&#23398;&#20064;&#30340;&#29616;&#35937;&#65292;&#24378;&#35843;&#20102;&#36825;&#31181;&#29616;&#35937;&#23545;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#24433;&#21709;&#65292;&#24182;&#21628;&#21505;&#21152;&#22823;&#23545;&#25463;&#24452;&#23398;&#20064;&#30340;&#30740;&#31350;&#21147;&#24230;&#20197;&#25552;&#21319;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#35780;&#20272;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2401.09615</link><description>&lt;p&gt;
&#23398;&#20064;&#25463;&#24452;&#65306;&#20851;&#20110;&#35821;&#35328;&#27169;&#22411;&#20013;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#35823;&#23548;&#24615;&#25215;&#35834;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Learning Shortcuts: On the Misleading Promise of NLU in Language Models. (arXiv:2401.09615v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09615
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#20351;&#29992;&#25463;&#24452;&#23398;&#20064;&#30340;&#29616;&#35937;&#65292;&#24378;&#35843;&#20102;&#36825;&#31181;&#29616;&#35937;&#23545;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#24433;&#21709;&#65292;&#24182;&#21628;&#21505;&#21152;&#22823;&#23545;&#25463;&#24452;&#23398;&#20064;&#30340;&#30740;&#31350;&#21147;&#24230;&#20197;&#25552;&#21319;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#35780;&#20272;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;LLMs&#22312;&#25191;&#34892;&#20219;&#21153;&#26102;&#24120;&#24120;&#37319;&#29992;&#25463;&#24452;&#65292;&#23548;&#33268;&#22312;&#20915;&#31574;&#35268;&#21017;&#19978;&#32570;&#20047;&#27867;&#21270;&#33021;&#21147;&#65292;&#20174;&#32780;&#22312;&#24615;&#33021;&#19978;&#20135;&#29983;&#20102;&#19968;&#31181;&#38169;&#35273;&#12290;&#36825;&#19968;&#29616;&#35937;&#22312;&#20934;&#30830;&#35780;&#20272;LLMs&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#19978;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#23545;&#35813;&#39046;&#22495;&#30340;&#30456;&#20851;&#30740;&#31350;&#36827;&#34892;&#20102;&#31616;&#27905;&#30340;&#27010;&#36848;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#20351;&#29992;&#25463;&#24452;&#23398;&#20064;&#30340;&#24433;&#21709;&#30340;&#35266;&#28857;&#12290;&#26412;&#25991;&#21628;&#21505;&#21152;&#22823;&#23545;&#25463;&#24452;&#23398;&#20064;&#30340;&#28145;&#20837;&#29702;&#35299;&#30340;&#30740;&#31350;&#21147;&#24230;&#65292;&#20026;&#24320;&#21457;&#26356;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#25552;&#39640;&#30495;&#23454;&#22330;&#26223;&#19979;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#35780;&#20272;&#30340;&#26631;&#20934;&#20316;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of large language models (LLMs) has enabled significant performance gains in the field of natural language processing. However, recent studies have found that LLMs often resort to shortcuts when performing tasks, creating an illusion of enhanced performance while lacking generalizability in their decision rules. This phenomenon introduces challenges in accurately assessing natural language understanding in LLMs. Our paper provides a concise survey of relevant research in this area and puts forth a perspective on the implications of shortcut learning in the evaluation of language models, specifically for NLU tasks. This paper urges more research efforts to be put towards deepening our comprehension of shortcut learning, contributing to the development of more robust language models, and raising the standards of NLU evaluation in real-world scenarios.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Apollo&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20302;&#23618;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#39640;&#23618;&#21151;&#33021;&#65292;&#20026;&#28176;&#36827;&#24335;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#20102;&#35838;&#31243;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#21152;&#36895;&#27604;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.09192</link><description>&lt;p&gt;
&#20026;&#28176;&#36827;&#24335;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20934;&#22791;&#35838;&#31243;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Preparing Lessons for Progressive Training on Language Models. (arXiv:2401.09192v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09192
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Apollo&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20302;&#23618;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#39640;&#23618;&#21151;&#33021;&#65292;&#20026;&#28176;&#36827;&#24335;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#20102;&#35838;&#31243;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#21152;&#36895;&#27604;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#36805;&#36895;&#21457;&#23637;&#24102;&#26469;&#20102;&#36164;&#28304;&#28040;&#32791;&#21644;&#28201;&#23460;&#27668;&#20307;&#25490;&#25918;&#30340;&#22686;&#21152;&#65292;&#36825;&#26159;&#30001;&#20110;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#38271;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#23567;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#26032;&#30340;&#27169;&#22411;&#32467;&#26500;&#21487;&#33021;&#19981;&#36866;&#29992;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#21487;&#33021;&#24456;&#24930;&#65292;&#24182;&#19988;&#28176;&#36827;&#22534;&#21472;&#23618;&#24448;&#24448;&#26080;&#27861;&#23454;&#29616;&#26174;&#33879;&#30340;&#21152;&#36895;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Apollo&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#22312;&#20302;&#23618;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#39640;&#23618;&#21151;&#33021;&#26469;&#20934;&#22791;&#33192;&#32960;&#25805;&#20316;&#30340;&#35838;&#31243;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20302;&#20540;&#20248;&#20808;&#37319;&#26679;(LVPS)&#26469;&#35757;&#32451;&#19981;&#21516;&#28145;&#24230;&#65292;&#24182;&#24341;&#20837;&#26435;&#37325;&#20849;&#20139;&#20197;&#20419;&#36827;&#39640;&#25928;&#25193;&#23637;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#25554;&#20540;&#26041;&#27861;&#26469;&#23454;&#29616;&#31283;&#23450;&#30340;&#27169;&#22411;&#28145;&#24230;&#25193;&#23637;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;Apollo&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#21152;&#36895;&#27604;&#29575;&#65292;&#29978;&#33267;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
The rapid progress of Transformers in artificial intelligence has come at the cost of increased resource consumption and greenhouse gas emissions due to growing model sizes. Prior work suggests using pretrained small models to improve training efficiency, but this approach may not be suitable for new model structures. On the other hand, training from scratch can be slow, and progressively stacking layers often fails to achieve significant acceleration. To address these challenges, we propose a novel method called Apollo, which prep\textbf{a}res lessons for ex\textbf{p}anding \textbf{o}perations by \textbf{l}earning high-\textbf{l}ayer functi\textbf{o}nality during training of low layers. Our approach involves low-value-prioritized sampling (LVPS) to train different depths and weight sharing to facilitate efficient expansion. We also introduce an interpolation method for stable model depth extension. Experiments demonstrate that Apollo achieves state-of-the-art acceleration ratios, even
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;MMIQC&#25968;&#25454;&#38598;&#21644;&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;(IQC)&#30340;&#26032;&#39062;&#22686;&#24378;&#26041;&#27861;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#31454;&#36187;&#32423;&#25968;&#23398;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20808;&#21069;&#26368;&#20339;&#32467;&#26524;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.09003</link><description>&lt;p&gt;
&#36890;&#36807;&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;&#26469;&#22686;&#24378;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;
&lt;/p&gt;
&lt;p&gt;
Augmenting Math Word Problems via Iterative Question Composing. (arXiv:2401.09003v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;MMIQC&#25968;&#25454;&#38598;&#21644;&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;(IQC)&#30340;&#26032;&#39062;&#22686;&#24378;&#26041;&#27861;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#31454;&#36187;&#32423;&#25968;&#23398;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20808;&#21069;&#26368;&#20339;&#32467;&#26524;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#23450;&#36827;&#23637;&#65292;&#20294;&#22312;&#19981;&#20351;&#29992;&#22806;&#37096;&#24037;&#20855;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#31454;&#36187;&#32423;&#25968;&#23398;&#38382;&#39064;&#20173;&#28982;&#23545;&#24320;&#28304;LLMs&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MMIQC&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#28151;&#21512;&#22788;&#29702;&#30340;&#32593;&#32476;&#25968;&#25454;&#21644;&#21512;&#25104;&#38382;&#39064;-&#21709;&#24212;&#23545;&#30340;&#28151;&#21512;&#25968;&#25454;&#38598;&#65292;&#20197;&#25552;&#20379;&#22522;&#30784;&#27169;&#22411;&#26356;&#22909;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;MMIQC&#19978;&#23545;Mistral-7B(arXiv:2310.06825)&#36827;&#34892;&#24494;&#35843;&#33719;&#24471;&#30340;&#27169;&#22411;Mistral-7B-MMIQC&#65292;&#22312;MATH(arXiv:2103.03874)&#19978;&#36798;&#21040;&#20102;36.0%&#30340;&#20934;&#30830;&#29575;&#65292;&#27604;&#20043;&#21069;(model size $\sim$7B)&#30340;&#26368;&#20339;&#32467;&#26524;&#39640;&#20986;5.8%&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#36824;&#34920;&#26126;&#65292;&#25913;&#36827;&#30340;&#19968;&#20010;&#37325;&#35201;&#37096;&#20998;&#24402;&#21151;&#20110;&#25105;&#20204;&#30340;&#26032;&#39062;&#22686;&#24378;&#26041;&#27861;IQC(&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;)&#65292;&#20854;&#20013;&#25105;&#20204;&#36845;&#20195;&#22320;&#35201;&#27714;LLM&#20174;&#32473;&#23450;&#30340;&#31181;&#23376;&#38382;&#39064;&#20013;&#32452;&#21512;&#26032;&#38382;&#39064;&#65292;&#24182;&#20174;&#21478;&#19968;&#20010;LLM&#20013;&#36827;&#34892;&#25298;&#32477;&#25277;&#26679;&#12290;MMIQC&#29616;&#24050;&#22312;https://huggingface.co/datasets/Vivacem/MMIQC&#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent progress in improving the mathematical reasoning ability of large language models(LLMs), solving competition-level math problems without the use of external tools remains challenging for open-source LLMs. In this work, we introduce the MMIQC dataset, a mixture of processed web data and synthetic question-response pairs, to equip base models with better mathematical reasoning skills. Mistral-7B-MMIQC, the model obtained by fine-tuning Mistral-7B(arXiv:2310.06825) on MMIQC, achieves 36.0\% accuracy on MATH(arXiv:2103.03874), 5.8\% higher than the previous (model size $\sim$7B) SOTA. Our experiments also show that a large part of the improvement attributes to our novel augmentation method IQC(Iterative Question Composing), where we iteratively ask an LLM to compose new questions from the given seed problems and do rejection sampling from another LLM. MMIQC has now been released on https://huggingface.co/datasets/Vivacem/MMIQC.
&lt;/p&gt;</description></item><item><title>InfiAgent-DABench&#26159;&#31532;&#19968;&#20010;&#35780;&#20272;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#22312;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;DAEval&#25968;&#25454;&#38598;&#21644;&#20195;&#29702;&#26694;&#26550;&#12290;&#23545;23&#20010;&#26368;&#20808;&#36827;&#30340;LLMs&#36827;&#34892;&#30340;&#22522;&#20934;&#27979;&#35797;&#25581;&#31034;&#20102;&#24403;&#21069;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.05507</link><description>&lt;p&gt;
InfiAgent-DABench: &#22312;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#20013;&#35780;&#20272;&#20195;&#29702;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
InfiAgent-DABench: Evaluating Agents on Data Analysis Tasks. (arXiv:2401.05507v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05507
&lt;/p&gt;
&lt;p&gt;
InfiAgent-DABench&#26159;&#31532;&#19968;&#20010;&#35780;&#20272;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#22312;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;DAEval&#25968;&#25454;&#38598;&#21644;&#20195;&#29702;&#26694;&#26550;&#12290;&#23545;23&#20010;&#26368;&#20808;&#36827;&#30340;LLMs&#36827;&#34892;&#30340;&#22522;&#20934;&#27979;&#35797;&#25581;&#31034;&#20102;&#24403;&#21069;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;"InfiAgent-DABench"&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#22312;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;DAEval&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;55&#20010;CSV&#25991;&#20214;&#34893;&#29983;&#20986;&#30340;311&#20010;&#25968;&#25454;&#20998;&#26512;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#20010;&#35780;&#20272;LLMs&#20316;&#20026;&#25968;&#25454;&#20998;&#26512;&#20195;&#29702;&#30340;&#20195;&#29702;&#26694;&#26550;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26684;&#24335;&#25552;&#31034;&#25216;&#26415;&#65292;&#30830;&#20445;&#38382;&#39064;&#26159;&#38381;&#21512;&#24418;&#24335;&#30340;&#65292;&#21487;&#20197;&#33258;&#21160;&#35780;&#20272;&#12290;&#25105;&#20204;&#23545;23&#20010;&#26368;&#20808;&#36827;&#30340;LLMs&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#25581;&#31034;&#20102;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#20013;&#24403;&#21069;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;DAAgent&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#19987;&#38376;&#20195;&#29702;&#12290;InfiAgent-DABench&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#21644;&#24037;&#20855;&#21253;&#24050;&#32463;&#21457;&#24067;&#22312;https://github.com/InfiAgent/InfiAgent&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce "InfiAgent-DABench", the first benchmark specifically designed to evaluate LLM-based agents in data analysis tasks. This benchmark contains DAEval, a dataset consisting of 311 data analysis questions derived from 55 CSV files, and an agent framework to evaluate LLMs as data analysis agents. We adopt a format-prompting technique, ensuring questions to be closed-form that can be automatically evaluated. Our extensive benchmarking of 23 state-of-the-art LLMs uncovers the current challenges encountered in data analysis tasks. In addition, we have developed DAAgent, a specialized agent trained on instruction-tuning datasets. Evaluation datasets and toolkits for InfiAgent-DABench are released at https://github.com/InfiAgent/InfiAgent.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#31232;&#30095;&#21046;&#20316;&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#19987;&#23478;&#28151;&#21512;&#24335;&#26550;&#26500;&#23558;&#23494;&#38598;&#27169;&#22411;&#36716;&#25442;&#20026;&#31232;&#30095;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#22312;&#27169;&#22411;&#23481;&#37327;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#21644;&#27867;&#21270;&#33021;&#21147;&#22686;&#24378;&#12290;</title><link>http://arxiv.org/abs/2401.02731</link><description>&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#31232;&#30095;&#21046;&#20316;&#65306;&#20174;&#23494;&#38598;&#22411;&#21040;&#19987;&#23478;&#28151;&#21512;&#24335;&#29992;&#20110;&#36890;&#29992;&#20219;&#21153;&#30340;&#25351;&#20196;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks. (arXiv:2401.02731v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#31232;&#30095;&#21046;&#20316;&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#19987;&#23478;&#28151;&#21512;&#24335;&#26550;&#26500;&#23558;&#23494;&#38598;&#27169;&#22411;&#36716;&#25442;&#20026;&#31232;&#30095;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#22312;&#27169;&#22411;&#23481;&#37327;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#21644;&#27867;&#21270;&#33021;&#21147;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#36890;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#30456;&#24403;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#25351;&#20196;&#35843;&#25972;&#20316;&#20026;&#19968;&#31181;&#25104;&#21151;&#30340;&#33539;&#20363;&#65292;&#22686;&#24378;&#20102;LLMs&#36981;&#24490;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27169;&#22411;&#23481;&#37327;&#38480;&#21046;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#32463;&#24120;&#36935;&#21040;&#24615;&#33021;&#38480;&#21046;&#12290;&#22312;&#25351;&#20196;&#35843;&#25972;&#38454;&#27573;&#25193;&#23637;&#27169;&#22411;&#23481;&#37327;&#38754;&#20020;&#30528;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21442;&#25968;&#39640;&#25928;&#31232;&#30095;&#21046;&#20316;(PESC)&#65292;&#23427;&#20351;&#29992;&#19987;&#23478;&#28151;&#21512;&#24335;(MoE)&#26550;&#26500;&#23558;&#23494;&#38598;&#27169;&#22411;&#36716;&#25442;&#20026;&#31232;&#30095;&#27169;&#22411;&#12290;PESC&#23558;&#36866;&#37197;&#22120;&#38598;&#25104;&#21040;&#31232;&#30095;&#27169;&#22411;&#30340;MoE&#23618;&#20013;&#65292;&#21306;&#20998;&#19981;&#21516;&#30340;&#19987;&#23478;&#32780;&#19981;&#25913;&#21464;&#36825;&#20123;&#23618;&#20013;&#30340;&#20010;&#20307;&#26435;&#37325;&#12290;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#21644;GPU&#20869;&#23384;&#38656;&#27714;&#65292;&#36890;&#36807;&#26368;&#23567;&#30340;&#22686;&#21152;&#23454;&#29616;&#20102;&#27169;&#22411;&#23481;&#37327;&#30340;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated considerable proficiency in general natural language processing (NLP) tasks. Instruction tuning, a successful paradigm, enhances the ability of LLMs to follow natural language instructions and exhibit robust generalization across a wide range of tasks. However, these models often encounter performance limitations across multiple tasks due to constrained model capacity. Expanding this capacity during the instruction tuning phase poses significant challenges. To address this issue, we introduce a novel approach, Parameter-Efficient Sparsity Crafting (PESC), which transitions dense models to sparse models using a Mixture of Experts (MoE) architecture. PESC integrates adapters into the MoE layers of sparse models, differentiating experts without altering the individual weights within these layers. This method significantly reduces computational costs and GPU memory requirements, facilitating model capacity expansion through a minimal increase 
&lt;/p&gt;</description></item><item><title>NPHardEval&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;900&#20010;&#31639;&#27861;&#38382;&#39064;&#19978;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#25193;&#23637;&#21040;NP-Hard&#22797;&#26434;&#24615;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2312.14890</link><description>&lt;p&gt;
NPHardEval: &#36890;&#36807;&#22797;&#26434;&#24615;&#31867;&#21035;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#21160;&#24577;&#22522;&#20934;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes. (arXiv:2312.14890v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.14890
&lt;/p&gt;
&lt;p&gt;
NPHardEval&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;900&#20010;&#31639;&#27861;&#38382;&#39064;&#19978;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#25193;&#23637;&#21040;NP-Hard&#22797;&#26434;&#24615;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#26159;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#37325;&#35201;&#29305;&#24449;&#20043;&#19968;&#65292;&#23427;&#20063;&#34987;&#29992;&#20110;&#22312;&#22797;&#26434;&#20915;&#31574;&#20219;&#21153;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#65306;&#24050;&#32463;&#24314;&#31435;&#20102;&#35768;&#22810;&#22522;&#20934;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#22522;&#20934;&#22312;&#25552;&#20379;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#30340;&#20840;&#38754;&#35780;&#20272;&#26041;&#38754;&#36824;&#19981;&#22815;&#65292;&#21516;&#26102;&#20063;&#23481;&#26131;&#20986;&#29616;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#65292;&#22240;&#20026;&#36825;&#20123;&#22522;&#20934;&#26159;&#20844;&#24320;&#21487;&#35775;&#38382;&#19988;&#38745;&#24577;&#30340;&#65292;&#20351;&#24471;&#27169;&#22411;&#26377;&#21487;&#33021;&#26681;&#25454;&#29305;&#23450;&#30340;&#22522;&#20934;&#25351;&#26631;&#35843;&#25972;&#20854;&#21709;&#24212;&#65292;&#20174;&#32780;&#22840;&#22823;&#20854;&#24615;&#33021;&#12290;&#38024;&#23545;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#21517;&#20026;NPHardEval&#12290;&#35813;&#22522;&#20934;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24191;&#27867;&#30340;900&#20010;&#31639;&#27861;&#38382;&#39064;&#19978;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#28085;&#30422;&#20102;NP-Hard&#22797;&#26434;&#24615;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complex reasoning ability is one of the most important features of current LLMs, which has also been leveraged to play an integral role in complex decision-making tasks. Therefore, the investigation into the reasoning capabilities of Large Language Models (LLMs) is critical: numerous benchmarks have been established to assess the reasoning abilities of LLMs. However, current benchmarks are inadequate in offering a rigorous evaluation of the full extent of reasoning abilities that LLMs are capable of achieving. They are also prone to the risk of overfitting, as these benchmarks, being publicly accessible and static, allow models to potentially tailor their responses to specific benchmark metrics, thereby inflating their performance. Addressing these limitations, our research introduces a new benchmark, named NPHardEval. This benchmark is designed to evaluate the reasoning abilities of LLMs across a broad spectrum of 900 algorithmic questions, extending up to the NP-Hard complexity class
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31526;&#21495;&#27169;&#24335;&#35268;&#21010;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32534;&#30721;&#38382;&#39064;&#20026;&#19968;&#20010;&#20844;&#24335;&#21487;&#20197;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#26377;&#25928;&#22320;&#23547;&#25214;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#35268;&#21010;&#22120;Patty&#22312;&#20170;&#24180;&#30340;IPC&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.09963</link><description>&lt;p&gt;
&#20855;&#26377;&#27169;&#24335;&#30340;&#31526;&#21495;&#25968;&#20540;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Symbolic Numeric Planning with Patterns. (arXiv:2312.09963v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.09963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31526;&#21495;&#27169;&#24335;&#35268;&#21010;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32534;&#30721;&#38382;&#39064;&#20026;&#19968;&#20010;&#20844;&#24335;&#21487;&#20197;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#26377;&#25928;&#22320;&#23547;&#25214;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#35268;&#21010;&#22120;Patty&#22312;&#20170;&#24180;&#30340;IPC&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#32447;&#24615;&#25968;&#20540;&#35268;&#21010;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#31526;&#21495;&#27169;&#24335;&#35268;&#21010;&#12290;&#32473;&#23450;&#19968;&#20010;&#35268;&#21010;&#38382;&#39064;&#928;&#65292;&#19968;&#20010;&#30028;&#38480;n&#21644;&#19968;&#20010;&#27169;&#24335; - &#23450;&#20041;&#20026;&#20219;&#24847;&#21160;&#20316;&#30340;&#24207;&#21015; - &#25105;&#20204;&#23558;&#23547;&#25214;&#19968;&#20010;&#38024;&#23545;&#928;&#21644;&#30028;&#38480;n&#30340;&#35745;&#21010;&#38382;&#39064;&#32534;&#30721;&#20026;&#19968;&#20010;&#20844;&#24335;&#65292;&#35813;&#20844;&#24335;&#30340;&#21464;&#37327;&#21644;/&#25110;&#23376;&#21477;&#27604;&#26368;&#20808;&#36827;&#30340;rolled-up&#21644;&#26494;&#24347;-&#26494;&#24347;-$\exists$&#32534;&#30721;&#26356;&#23569;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#20219;&#20309;&#32473;&#23450;&#30340;&#30028;&#38480;&#65292;&#21518;&#20004;&#31181;&#32534;&#30721;&#37117;&#19981;&#21487;&#33021;&#25214;&#21040;&#19968;&#20010;&#26377;&#25928;&#30340;&#35745;&#21010;&#65292;&#32780;&#25105;&#20204;&#30340;&#32534;&#30721;&#21487;&#20197;&#12290;&#22312;&#23454;&#39564;&#26041;&#38754;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20854;&#20182;6&#20010;&#35268;&#21010;&#31995;&#32479; - &#21253;&#25324;&#21442;&#21152;&#20170;&#24180;&#22269;&#38469;&#35268;&#21010;&#31454;&#36187;&#65288;IPC&#65289;&#30340;&#31995;&#32479; - &#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#35268;&#21010;&#22120;Patty&#22312;&#20170;&#24180;IPC&#38382;&#39064;&#19978;&#30340;&#38750;&#24120;&#22909;&#30340;&#27604;&#36739;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel approach for solving linear numeric planning problems, called Symbolic Pattern Planning. Given a planning problem $\Pi$, a bound $n$ and a pattern -- defined as an arbitrary sequence of actions -- we encode the problem of finding a plan for $\Pi$ with bound $n$ as a formula with fewer variables and/or clauses than the state-of-the-art rolled-up and relaxed-relaxed-$\exists$ encodings. More importantly, we prove that for any given bound, it is never the case that the latter two encodings allow finding a valid plan while ours does not. On the experimental side, we consider 6 other planning systems -- including the ones which participated in this year's International Planning Competition (IPC) -- and we show that our planner Patty has remarkably good comparative performances on this year's IPC problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#24182;&#20998;&#26512;&#20102;&#27880;&#24847;&#21147;&#30697;&#38453;&#30340;&#20998;&#24067;&#21644;&#38598;&#20013;&#33021;&#21147;&#12290;&#36890;&#36807;&#24341;&#20837;&#32447;&#24615;&#23545;&#25968;&#27491;&#24577;&#27880;&#24847;&#21147;&#26469;&#27169;&#25311;&#21407;&#22987;&#33258;&#27880;&#24847;&#21147;&#30340;&#20998;&#24067;&#21644;&#38598;&#20013;&#34892;&#20026;&#65292;&#25552;&#39640;&#20102;Transformer&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.13541</link><description>&lt;p&gt;
&#32447;&#24615;&#23545;&#25968;&#27491;&#24577;&#27880;&#24847;&#21147;&#19982;&#26080;&#20559;&#38598;&#20013;&#21147;
&lt;/p&gt;
&lt;p&gt;
Linear Log-Normal Attention with Unbiased Concentration. (arXiv:2311.13541v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.13541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#24182;&#20998;&#26512;&#20102;&#27880;&#24847;&#21147;&#30697;&#38453;&#30340;&#20998;&#24067;&#21644;&#38598;&#20013;&#33021;&#21147;&#12290;&#36890;&#36807;&#24341;&#20837;&#32447;&#24615;&#23545;&#25968;&#27491;&#24577;&#27880;&#24847;&#21147;&#26469;&#27169;&#25311;&#21407;&#22987;&#33258;&#27880;&#24847;&#21147;&#30340;&#20998;&#24067;&#21644;&#38598;&#20013;&#34892;&#20026;&#65292;&#25552;&#39640;&#20102;Transformer&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#22797;&#26434;&#24230;&#19982;&#24207;&#21015;&#38271;&#24230;&#30340;&#20108;&#27425;&#20851;&#31995;&#65292;&#20854;&#21487;&#25193;&#23637;&#24615;&#21463;&#21040;&#38480;&#21046;&#12290;&#24403;&#22788;&#29702;&#38271;&#25991;&#26723;&#25110;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#26102;&#65292;&#36825;&#19968;&#38480;&#21046;&#26500;&#25104;&#20102;&#37325;&#22823;&#38556;&#30861;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#27880;&#24847;&#21147;&#30697;&#38453;&#30340;&#20998;&#24067;&#21644;&#38598;&#20013;&#33021;&#21147;&#65292;&#23545;&#33258;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#34913;&#37327;&#36825;&#20123;&#25968;&#37327;&#30340;&#24037;&#20855;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#21363;&#32447;&#24615;&#23545;&#25968;&#27491;&#24577;&#27880;&#24847;&#21147;&#65292;&#26088;&#22312;&#27169;&#25311;&#21407;&#22987;&#33258;&#27880;&#24847;&#21147;&#30340;&#20998;&#24067;&#21644;&#38598;&#20013;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;&#24120;&#29992;&#30340;&#33258;&#28982;&#35821;&#35328;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#32447;&#24615;&#23545;&#25968;&#27491;&#24577;&#27880;&#24847;&#21147;&#20248;&#20110;&#20854;&#20182;&#32447;&#24615;&#21270;&#27880;&#24847;&#21147;&#26367;&#20195;&#26041;&#27861;&#65292;&#20026;&#22686;&#24378;Transformer&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#38468;&#22312;&#34917;&#20805;&#26448;&#26009;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer models have achieved remarkable results in a wide range of applications. However, their scalability is hampered by the quadratic time and memory complexity of the self-attention mechanism concerning the sequence length. This limitation poses a substantial obstacle when dealing with long documents or high-resolution images. In this work, we study the self-attention mechanism by analyzing the distribution of the attention matrix and its concentration ability. Furthermore, we propose instruments to measure these quantities and introduce a novel self-attention mechanism, Linear Log-Normal Attention, designed to emulate the distribution and concentration behavior of the original self-attention. Our experimental results on popular natural language benchmarks reveal that our proposed Linear Log-Normal Attention outperforms other linearized attention alternatives, offering a promising avenue for enhancing the scalability of transformer models. Our code is available in supplementary
&lt;/p&gt;</description></item><item><title>DeliverAI&#26159;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#36335;&#24452;&#20849;&#20139;&#32593;&#32476;&#65292;&#29992;&#20110;&#20248;&#21270;&#39135;&#21697;&#37197;&#36865;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#20197;&#20943;&#23569;&#37197;&#36865;&#25104;&#26412;&#24182;&#25552;&#39640;&#28040;&#36153;&#32773;&#28385;&#24847;&#24230;&#12290;</title><link>http://arxiv.org/abs/2311.02017</link><description>&lt;p&gt;
DeliverAI: &#24378;&#21270;&#23398;&#20064;&#20026;&#22522;&#30784;&#30340;&#20998;&#24067;&#24335;&#36335;&#24452;&#20849;&#20139;&#32593;&#32476;&#29992;&#20110;&#39135;&#21697;&#37197;&#36865;
&lt;/p&gt;
&lt;p&gt;
DeliverAI: Reinforcement Learning Based Distributed Path-Sharing Network for Food Deliveries. (arXiv:2311.02017v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.02017
&lt;/p&gt;
&lt;p&gt;
DeliverAI&#26159;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#36335;&#24452;&#20849;&#20139;&#32593;&#32476;&#65292;&#29992;&#20110;&#20248;&#21270;&#39135;&#21697;&#37197;&#36865;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#20197;&#20943;&#23569;&#37197;&#36865;&#25104;&#26412;&#24182;&#25552;&#39640;&#28040;&#36153;&#32773;&#28385;&#24847;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#21313;&#24180;&#20013;&#65292;&#20174;&#29983;&#20135;&#32773;&#21040;&#28040;&#36153;&#32773;&#30340;&#29289;&#21697;&#37197;&#36865;&#32463;&#21382;&#20102;&#26174;&#33879;&#30340;&#22686;&#38271;&#65292;&#24182;&#19988;&#26368;&#36817;&#30340;&#27969;&#34892;&#30149;&#36827;&#19968;&#27493;&#25512;&#21160;&#20102;&#36825;&#19968;&#22686;&#38271;&#12290;&#20122;&#39532;&#36874;&#29983;&#40092;&#12289;Shopify&#12289;UberEats&#12289;InstaCart&#21644;DoorDash&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#65292;&#24182;&#20849;&#20139;&#30456;&#21516;&#30340;&#28040;&#36153;&#21697;&#25110;&#39135;&#21697;&#37197;&#36865;&#19994;&#21153;&#27169;&#24335;&#12290;&#29616;&#26377;&#30340;&#39135;&#21697;&#37197;&#36865;&#26041;&#27861;&#23384;&#22312;&#32570;&#38519;&#65292;&#22240;&#20026;&#27599;&#27425;&#37197;&#36865;&#37117;&#26159;&#22312;&#26368;&#30701;&#26102;&#38388;&#36335;&#24452;&#19978;&#20174;&#29983;&#20135;&#32773;&#30452;&#25509;&#21040;&#28040;&#36153;&#32773;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#24403;&#21069;&#27169;&#22411;&#19979;&#65292;&#26377;&#24456;&#22823;&#30340;&#20943;&#23569;&#37197;&#36865;&#25104;&#26412;&#30340;&#31354;&#38388;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#39135;&#21697;&#37197;&#36865;&#38382;&#39064;&#24314;&#27169;&#20026;&#19968;&#20010;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#28040;&#36153;&#32773;&#28385;&#24847;&#24230;&#21644;&#37197;&#36865;&#25104;&#26412;&#37117;&#38656;&#35201;&#36827;&#34892;&#20248;&#21270;&#12290;&#21463;&#20986;&#31199;&#36710;&#34892;&#19994;&#20013;&#25340;&#36710;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DeliverAI - &#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#36335;&#24452;&#20849;&#20139;&#31639;&#27861;&#12290;&#19982;&#20197;&#21069;&#30340;&#36335;&#24452;&#20849;&#20139;&#23581;&#35797;&#19981;&#21516;&#65292;DeliverAI&#21487;&#20197;&#25552;&#20379;&#23454;&#26102;&#12289;&#26102;&#38388;&#39640;&#25928;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Delivery of items from the producer to the consumer has experienced significant growth over the past decade and has been greatly fueled by the recent pandemic. Amazon Fresh, Shopify, UberEats, InstaCart, and DoorDash are rapidly growing and are sharing the same business model of consumer items or food delivery. Existing food delivery methods are sub-optimal because each delivery is individually optimized to go directly from the producer to the consumer via the shortest time path. We observe a significant scope for reducing the costs associated with completing deliveries under the current model. We model our food delivery problem as a multi-objective optimization, where consumer satisfaction and delivery costs, both, need to be optimized. Taking inspiration from the success of ride-sharing in the taxi industry, we propose DeliverAI - a reinforcement learning-based path-sharing algorithm. Unlike previous attempts for path-sharing, DeliverAI can provide real-time, time-efficient decision-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#26657;&#20934;&#40065;&#26834;&#24494;&#35843;&#65288;CaRot&#65289;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#26657;&#20934;&#38382;&#39064;&#12290;&#36890;&#36807;&#35813;&#26041;&#27861;&#65292;&#20316;&#32773;&#25104;&#21151;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.01723</link><description>&lt;p&gt;
&#23454;&#29616;&#23545;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#26657;&#20934;&#40065;&#26834;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Towards Calibrated Robust Fine-Tuning of Vision-Language Models. (arXiv:2311.01723v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#26657;&#20934;&#40065;&#26834;&#24494;&#35843;&#65288;CaRot&#65289;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#26657;&#20934;&#38382;&#39064;&#12290;&#36890;&#36807;&#35813;&#26041;&#27861;&#65292;&#20316;&#32773;&#25104;&#21151;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#21487;&#20197;&#37322;&#25918;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#28508;&#21147;&#65292;&#20294;&#20250;&#24433;&#21709;&#27169;&#22411;&#23545;&#20110;&#38750;&#20998;&#24067;&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#40065;&#26834;&#24494;&#35843;&#26088;&#22312;&#30830;&#20445;&#27169;&#22411;&#22312;&#38750;&#20998;&#24067;&#25968;&#25454;&#38598;&#20197;&#21450;&#24494;&#35843;&#30340;&#20998;&#24067;&#25968;&#25454;&#38598;&#19978;&#37117;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#21487;&#38752;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#32622;&#20449;&#24230;&#26657;&#20934;&#36825;&#19968;&#26631;&#20934;&#21364;&#32463;&#24120;&#34987;&#24573;&#35270;&#65292;&#23613;&#31649;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#39640;&#39118;&#38505;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#65288;&#22914;&#33258;&#21160;&#39550;&#39542;&#21644;&#21307;&#23398;&#35786;&#26029;&#65289;&#38656;&#27714;&#26085;&#30410;&#22686;&#21152;&#12290;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#23545;&#32454;&#35843;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#26657;&#20934;&#30340;&#25285;&#24551;&#65292;&#24182;&#36890;&#36807;&#26174;&#31034;&#26222;&#36890;&#24494;&#35843;&#29978;&#33267;&#26368;&#20808;&#36827;&#30340;&#40065;&#26834;&#24494;&#35843;&#26041;&#27861;&#23545;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#26657;&#20934;&#36896;&#25104;&#20102;&#25439;&#23475;&#65292;&#23588;&#20854;&#26159;&#22312;&#38750;&#20998;&#24067;&#25968;&#25454;&#38598;&#19978;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#26657;&#20934;&#40065;&#26834;&#24494;&#35843;&#65288;CaRot&#65289;&#65292;&#23427;&#22312;&#26657;&#20934;&#21644;&#40065;&#26834;&#24615;&#19978;&#25552;&#20379;&#20102;&#22870;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;
While fine-tuning unleashes the potential of a pre-trained model to a specific task, it trades off the model's generalization capability on out-of-distribution (OOD) datasets. To mitigate this, robust fine-tuning aims to ensure performance on OOD datasets as well as an in-distribution (ID) dataset for which the model is being tuned. However, another criterion for reliable machine learning (ML), confidence calibration, has been overlooked despite its increasing demand for real-world high-stakes ML applications (e.g., autonomous driving and medical diagnosis). For the first time, we raise concerns about the calibration of fine-tuned vision-language models (VLMs) under distribution shift by showing that naive fine-tuning and even state-of-the-art robust fine-tuning methods hurt the calibration of pre-trained VLMs, especially on OOD datasets. To address this, we provide a simple approach, called a calibrated robust fine-tuning (CaRot) that incentivizes the calibration and robustness on bot
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20026;&#24320;&#25918;&#24335;&#23398;&#20064;&#38382;&#39064;&#23450;&#20041;&#20102;&#19968;&#20010;&#20851;&#38190;&#30340;&#22522;&#26412;&#23646;&#24615;&#65292;&#21363;&#26080;&#38480;&#26102;&#38388;&#20869;&#19981;&#26029;&#20135;&#29983;&#26032;&#20803;&#32032;&#12290;&#22312;&#36825;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#24320;&#25918;&#24335;&#23398;&#20064;&#38382;&#39064;&#30340;&#27010;&#24565;&#65292;&#24182;&#30528;&#37325;&#30740;&#31350;&#20102;&#24320;&#25918;&#24335;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#30340;&#23376;&#38598;&#12290;</title><link>http://arxiv.org/abs/2311.00344</link><description>&lt;p&gt;
&#20026;&#30446;&#26631;&#26465;&#20214;&#26234;&#33021;&#20307;&#23450;&#20041;&#24320;&#25918;&#24335;&#23398;&#20064;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
A Definition of Open-Ended Learning Problems for Goal-Conditioned Agents. (arXiv:2311.00344v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20026;&#24320;&#25918;&#24335;&#23398;&#20064;&#38382;&#39064;&#23450;&#20041;&#20102;&#19968;&#20010;&#20851;&#38190;&#30340;&#22522;&#26412;&#23646;&#24615;&#65292;&#21363;&#26080;&#38480;&#26102;&#38388;&#20869;&#19981;&#26029;&#20135;&#29983;&#26032;&#20803;&#32032;&#12290;&#22312;&#36825;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#24320;&#25918;&#24335;&#23398;&#20064;&#38382;&#39064;&#30340;&#27010;&#24565;&#65292;&#24182;&#30528;&#37325;&#30740;&#31350;&#20102;&#24320;&#25918;&#24335;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#30340;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#35770;&#25991;&#20013;&#37117;&#25552;&#21040;&#20102;&#8220;&#24320;&#25918;&#24335;&#23398;&#20064;&#8221;&#65292;&#20294;&#24456;&#23569;&#26377;&#20154;&#23581;&#35797;&#23450;&#20041;&#36825;&#20010;&#26415;&#35821;&#12290;&#26356;&#31967;&#31957;&#30340;&#26159;&#65292;&#24403;&#20180;&#32454;&#30740;&#31350;&#26102;&#65292;&#20284;&#20046;&#23545;&#20110;&#24320;&#25918;&#24335;&#23398;&#20064;&#19982;&#36830;&#32493;&#23398;&#20064;&#12289;&#32456;&#36523;&#23398;&#20064;&#25110;&#33258;&#20026;&#30446;&#30340;&#23398;&#20064;&#31561;&#30456;&#20851;&#27010;&#24565;&#30340;&#21306;&#21035;&#27809;&#26377;&#20849;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#33268;&#21147;&#20110;&#35299;&#20915;&#36825;&#31181;&#24773;&#20917;&#12290;&#36890;&#36807;&#38416;&#36848;&#36825;&#20010;&#27010;&#24565;&#30340;&#36215;&#28304;&#21644;&#26368;&#36817;&#30340;&#35266;&#28857;&#65292;&#25105;&#20204;&#35828;&#26126;&#20102;&#24320;&#25918;&#24335;&#23398;&#20064;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#21253;&#21547;&#22810;&#31181;&#23646;&#24615;&#30340;&#22797;&#21512;&#27010;&#24565;&#12290;&#19982;&#36825;&#20123;&#20043;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#24320;&#25918;&#24335;&#36807;&#31243;&#30340;&#19968;&#20010;&#20851;&#38190;&#22522;&#26412;&#23646;&#24615;&#19982;&#26102;&#38388;&#26080;&#38480;&#21046;&#22320;&#20135;&#29983;&#26032;&#20803;&#32032;&#30456;&#20998;&#31163;&#30340;&#24819;&#27861;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#24320;&#25918;&#24335;&#23398;&#20064;&#38382;&#39064;&#30340;&#27010;&#24565;&#65292;&#24182;&#29305;&#21035;&#20851;&#27880;&#20102;&#24320;&#25918;&#24335;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#30340;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
A lot of recent machine learning research papers have "Open-ended learning" in their title. But very few of them attempt to define what they mean when using the term. Even worse, when looking more closely there seems to be no consensus on what distinguishes open-ended learning from related concepts such as continual learning, lifelong learning or autotelic learning. In this paper, we contribute to fixing this situation. After illustrating the genealogy of the concept and more recent perspectives about what it truly means, we outline that open-ended learning is generally conceived as a composite notion encompassing a set of diverse properties. In contrast with these previous approaches, we propose to isolate a key elementary property of open-ended processes, which is to always produce novel elements from time to time over an infinite horizon. From there, we build the notion of open-ended learning problems and focus in particular on the subset of open-ended goal-conditioned reinforcement
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32477;&#32536;&#20307;&#25913;&#21892;&#28909;&#21147;&#25193;&#25955;&#36827;&#34892;&#26080;&#30896;&#25758;&#36816;&#21160;&#35268;&#21010;&#30340;&#21435;&#22122;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21333;&#19968;&#30340;&#35270;&#35273;&#36755;&#20837;&#65292;&#22312;&#25512;&#29702;&#26102;&#33021;&#22815;&#21516;&#26102;&#29983;&#25104;&#21487;&#36798;&#30446;&#26631;&#24182;&#35268;&#21010;&#36991;&#24320;&#38556;&#30861;&#29289;&#30340;&#36816;&#21160;&#36335;&#24452;&#65292;&#20855;&#26377;&#31283;&#20581;&#24615;&#21644;&#22810;&#27169;&#24577;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12609</link><description>&lt;p&gt;
&#20351;&#29992;&#32477;&#32536;&#20307;&#25913;&#21892;&#28909;&#21147;&#25193;&#25955;&#36827;&#34892;&#26080;&#30896;&#25758;&#36816;&#21160;&#35268;&#21010;&#30340;&#21435;&#22122;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Denoising Heat-inspired Diffusion with Insulators for Collision Free Motion Planning. (arXiv:2310.12609v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32477;&#32536;&#20307;&#25913;&#21892;&#28909;&#21147;&#25193;&#25955;&#36827;&#34892;&#26080;&#30896;&#25758;&#36816;&#21160;&#35268;&#21010;&#30340;&#21435;&#22122;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21333;&#19968;&#30340;&#35270;&#35273;&#36755;&#20837;&#65292;&#22312;&#25512;&#29702;&#26102;&#33021;&#22815;&#21516;&#26102;&#29983;&#25104;&#21487;&#36798;&#30446;&#26631;&#24182;&#35268;&#21010;&#36991;&#24320;&#38556;&#30861;&#29289;&#30340;&#36816;&#21160;&#36335;&#24452;&#65292;&#20855;&#26377;&#31283;&#20581;&#24615;&#21644;&#22810;&#27169;&#24577;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#28789;&#27963;&#24615;&#21644;&#22810;&#27169;&#24577;&#24615;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#12290;&#23613;&#31649;&#20854;&#20013;&#19968;&#20123;&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#22797;&#26434;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#20005;&#37325;&#20381;&#36182;&#20110;&#25512;&#29702;&#26102;&#30340;&#38556;&#30861;&#29289;&#26816;&#27979;&#24182;&#38656;&#35201;&#39069;&#22806;&#30340;&#35774;&#22791;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#25512;&#29702;&#26102;&#33021;&#22815;&#20174;&#21333;&#19968;&#30340;&#35270;&#35273;&#36755;&#20837;&#20013;&#21516;&#26102;&#29983;&#25104;&#21487;&#36798;&#30446;&#26631;&#24182;&#35268;&#21010;&#36991;&#24320;&#38556;&#30861;&#29289;&#30340;&#36816;&#21160;&#36335;&#24452;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#23545;&#35757;&#32451;&#36807;&#31243;&#20013;&#26032;&#39062;&#30340;&#30896;&#25758;&#36991;&#20813;&#25193;&#25955;&#26680;&#36827;&#34892;&#20351;&#29992;&#12290;&#36890;&#36807;&#19982;&#34892;&#20026;&#20811;&#38534;&#21644;&#32463;&#20856;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#35777;&#26126;&#20102;&#20854;&#31283;&#20581;&#24615;&#12290;&#29305;&#21035;&#26159;&#22312;&#22810;&#27169;&#24577;&#29615;&#22659;&#20013;&#65292;&#23427;&#33021;&#22815;&#23548;&#33322;&#21040;&#30446;&#26631;&#24182;&#36991;&#24320;&#34987;&#38556;&#30861;&#29289;&#38459;&#25377;&#30340;&#19981;&#21487;&#36798;&#30446;&#26631;&#65292;&#21516;&#26102;&#30830;&#20445;&#36991;&#20813;&#30896;&#25758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have risen as a powerful tool in robotics due to their flexibility and multi-modality. While some of these methods effectively address complex problems, they often depend heavily on inference-time obstacle detection and require additional equipment. Addressing these challenges, we present a method that, during inference time, simultaneously generates only reachable goals and plans motions that avoid obstacles, all from a single visual input. Central to our approach is the novel use of a collision-avoiding diffusion kernel for training. Through evaluations against behavior-cloning and classical diffusion models, our framework has proven its robustness. It is particularly effective in multi-modal environments, navigating toward goals and avoiding unreachable ones blocked by obstacles, while ensuring collision avoidance.
&lt;/p&gt;</description></item><item><title>CIDER&#26159;&#19968;&#31181;&#22522;&#20110;&#31867;&#21035;&#24341;&#23548;&#30340;&#20010;&#24615;&#21270;&#26032;&#38395;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#24847;&#22270;&#20998;&#31163;&#21644;&#19968;&#33268;&#24615;&#30340;&#26032;&#38395;&#34920;&#31034;&#26469;&#20934;&#30830;&#29702;&#35299;&#26032;&#38395;&#25991;&#31456;&#30340;&#22810;&#20010;&#24847;&#22270;&#65292;&#24182;&#21306;&#20998;&#29992;&#25143;&#19981;&#21516;&#30340;&#21518;&#38405;&#35835;&#20559;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.09401</link><description>&lt;p&gt;
CIDER: &#22522;&#20110;&#31867;&#21035;&#24341;&#23548;&#30340;&#24847;&#22270;&#20998;&#31163;&#26041;&#27861;&#29992;&#20110;&#20934;&#30830;&#30340;&#20010;&#24615;&#21270;&#26032;&#38395;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
CIDER: Category-Guided Intent Disentanglement for Accurate Personalized News Recommendation. (arXiv:2310.09401v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09401
&lt;/p&gt;
&lt;p&gt;
CIDER&#26159;&#19968;&#31181;&#22522;&#20110;&#31867;&#21035;&#24341;&#23548;&#30340;&#20010;&#24615;&#21270;&#26032;&#38395;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#24847;&#22270;&#20998;&#31163;&#21644;&#19968;&#33268;&#24615;&#30340;&#26032;&#38395;&#34920;&#31034;&#26469;&#20934;&#30830;&#29702;&#35299;&#26032;&#38395;&#25991;&#31456;&#30340;&#22810;&#20010;&#24847;&#22270;&#65292;&#24182;&#21306;&#20998;&#29992;&#25143;&#19981;&#21516;&#30340;&#21518;&#38405;&#35835;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#26032;&#38395;&#25512;&#33616;&#26088;&#22312;&#24110;&#21161;&#29992;&#25143;&#25214;&#21040;&#19982;&#20854;&#20852;&#36259;&#30456;&#31526;&#30340;&#26032;&#38395;&#25991;&#31456;&#65292;&#36825;&#22312;&#32531;&#35299;&#29992;&#25143;&#20449;&#24687;&#36807;&#36733;&#38382;&#39064;&#26041;&#38754;&#36215;&#21040;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#23613;&#31649;&#35768;&#22810;&#26368;&#36817;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#25913;&#36827;&#29992;&#25143;&#21644;&#26032;&#38395;&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#20294;&#20197;&#19979;&#25361;&#25112;&#24456;&#23569;&#34987;&#30740;&#31350;&#65306;&#65288;C1&#65289;&#22914;&#20309;&#20934;&#30830;&#29702;&#35299;&#19968;&#31687;&#26032;&#38395;&#25991;&#31456;&#20013;&#21253;&#21547;&#30340;&#22810;&#20010;&#24847;&#22270;&#65311;&#20197;&#21450;&#65288;C2&#65289;&#22914;&#20309;&#21306;&#20998;&#29992;&#25143;&#28857;&#20987;&#21382;&#21490;&#20013;&#23545;&#26032;&#38395;&#25991;&#31456;&#26377;&#19981;&#21516;&#21518;&#38405;&#35835;&#20559;&#22909;&#30340;&#24773;&#20917;&#65311;&#20026;&#20102;&#21516;&#26102;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20010;&#24615;&#21270;&#26032;&#38395;&#25512;&#33616;&#26694;&#26550;&#65288;CIDER&#65289;&#65292;&#23427;&#21033;&#29992;&#65288;1&#65289;&#22522;&#20110;&#31867;&#21035;&#24341;&#23548;&#30340;&#24847;&#22270;&#20998;&#31163;&#26469;&#35299;&#20915;&#65288;C1&#65289;&#21644;&#65288;2&#65289;&#22522;&#20110;&#19968;&#33268;&#24615;&#30340;&#26032;&#38395;&#34920;&#31034;&#26469;&#35299;&#20915;&#65288;C2&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#31867;&#21035;&#39044;&#27979;&#32435;&#20837;CIDER&#30340;&#35757;&#32451;&#36807;&#31243;&#20316;&#20026;&#36741;&#21161;&#20219;&#21153;&#65292;&#36825;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#30417;&#30563;&#20449;&#21495;&#65292;&#20197;&#22686;&#24378;&#24847;&#22270;&#20998;&#31163;&#12290;&#22312;&#20004;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized news recommendation aims to assist users in finding news articles that align with their interests, which plays a pivotal role in mitigating users' information overload problem. Although many recent works have been studied for better user and news representations, the following challenges have been rarely studied: (C1) How to precisely comprehend a range of intents coupled within a news article? and (C2) How to differentiate news articles with varying post-read preferences in users' click history? To tackle both challenges together, in this paper, we propose a novel personalized news recommendation framework (CIDER) that employs (1) category-guided intent disentanglement for (C1) and (2) consistency-based news representation for (C2). Furthermore, we incorporate a category prediction into the training process of CIDER as an auxiliary task, which provides supplementary supervisory signals to enhance intent disentanglement. Extensive experiments on two real-world datasets rev
&lt;/p&gt;</description></item><item><title>ByteStack-ID&#26159;&#19968;&#31181;&#22522;&#20110;&#28784;&#24230;&#22270;&#20687;&#21644;&#36127;&#36733;&#23383;&#33410;&#39057;&#29575;&#30340;&#38598;&#25104;&#22534;&#21472;&#27169;&#22411;&#65292;&#29992;&#20110;&#25968;&#25454;&#21253;&#32423;&#20837;&#20405;&#26816;&#27979;&#12290;&#23427;&#33021;&#36805;&#36895;&#20934;&#30830;&#22320;&#35782;&#21035;&#32593;&#32476;&#27969;&#37327;&#20013;&#30340;&#21508;&#31181;&#25915;&#20987;&#31867;&#22411;&#65292;&#24182;&#19982;&#20256;&#32479;&#26041;&#27861;&#26377;&#25152;&#19981;&#21516;&#12290;</title><link>http://arxiv.org/abs/2310.09298</link><description>&lt;p&gt;
ByteStack-ID: &#22522;&#20110;&#28784;&#24230;&#22270;&#20687;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#30340;&#38598;&#25104;&#22534;&#21472;&#27169;&#22411;&#65292;&#21033;&#29992;&#36127;&#36733;&#23383;&#33410;&#39057;&#29575;
&lt;/p&gt;
&lt;p&gt;
ByteStack-ID: Integrated Stacked Model Leveraging Payload Byte Frequency for Grayscale Image-based Network Intrusion Detection. (arXiv:2310.09298v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09298
&lt;/p&gt;
&lt;p&gt;
ByteStack-ID&#26159;&#19968;&#31181;&#22522;&#20110;&#28784;&#24230;&#22270;&#20687;&#21644;&#36127;&#36733;&#23383;&#33410;&#39057;&#29575;&#30340;&#38598;&#25104;&#22534;&#21472;&#27169;&#22411;&#65292;&#29992;&#20110;&#25968;&#25454;&#21253;&#32423;&#20837;&#20405;&#26816;&#27979;&#12290;&#23427;&#33021;&#36805;&#36895;&#20934;&#30830;&#22320;&#35782;&#21035;&#32593;&#32476;&#27969;&#37327;&#20013;&#30340;&#21508;&#31181;&#25915;&#20987;&#31867;&#22411;&#65292;&#24182;&#19982;&#20256;&#32479;&#26041;&#27861;&#26377;&#25152;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#26029;&#21457;&#23637;&#30340;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#65292;&#36805;&#36895;&#20934;&#30830;&#22320;&#35782;&#21035;&#32593;&#32476;&#27969;&#37327;&#20013;&#30340;&#21508;&#31181;&#25915;&#20987;&#31867;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;"ByteStack-ID"&#65292;&#19968;&#31181;&#19987;&#20026;&#25968;&#25454;&#21253;&#32423;&#20837;&#20405;&#26816;&#27979;&#32780;&#35774;&#35745;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;ByteStack-ID&#26680;&#24515;&#26159;&#21033;&#29992;&#20174;&#36127;&#36733;&#25968;&#25454;&#30340;&#39057;&#29575;&#20998;&#24067;&#29983;&#25104;&#30340;&#28784;&#24230;&#22270;&#20687;&#65292;&#36825;&#26159;&#19968;&#31181;&#31361;&#30772;&#24615;&#30340;&#25216;&#26415;&#65292;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#27169;&#22411;&#35782;&#21035;&#22797;&#26434;&#25968;&#25454;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23436;&#20840;&#22522;&#20110;&#25968;&#25454;&#21253;&#32423;&#20449;&#24687;&#65292;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#27969;&#37327;&#25968;&#25454;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;NIDS&#65289;&#26377;&#25152;&#19981;&#21516;&#12290;&#22312;&#22522;&#26412;&#22534;&#21472;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;ByteStack-ID&#19982;&#20256;&#32479;&#30340;&#22534;&#21472;&#26041;&#27861;&#19981;&#21516;&#12290;&#23427;&#23558;&#38468;&#21152;&#30340;&#20803;&#23398;&#20064;&#22120;&#23618;&#26080;&#32541;&#38598;&#25104;&#21040;&#36830;&#25509;&#30340;&#22522;&#30784;&#23398;&#20064;&#22120;&#20013;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#39640;&#24230;&#20248;&#21270;&#30340;&#32479;&#19968;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the ever-evolving realm of network security, the swift and accurate identification of diverse attack classes within network traffic is of paramount importance. This paper introduces "ByteStack-ID," a pioneering approach tailored for packet-level intrusion detection. At its core, ByteStack-ID leverages grayscale images generated from the frequency distributions of payload data, a groundbreaking technique that greatly enhances the model's ability to discern intricate data patterns. Notably, our approach is exclusively grounded in packet-level information, a departure from conventional Network Intrusion Detection Systems (NIDS) that predominantly rely on flow-based data. While building upon the fundamental concept of stacking methodology, ByteStack-ID diverges from traditional stacking approaches. It seamlessly integrates additional meta learner layers into the concatenated base learners, creating a highly optimized, unified model. Empirical results unequivocally confirm the outstandin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20998;&#36776;TD&#23398;&#20064;(DTD)&#30340;&#26032;&#22411;TD&#31639;&#27861;&#65292;&#36890;&#36807;&#28789;&#27963;&#30340;&#24378;&#35843;&#20989;&#25968;&#26469;&#20998;&#37197;&#36164;&#28304;&#65292;&#20197;&#25913;&#21892;&#29366;&#24577;&#20043;&#38388;&#30340;&#36731;&#37325;&#20998;&#37197;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;TD&#23398;&#20064;&#20013;&#24573;&#35270;&#21382;&#21490;&#29366;&#24577;&#37325;&#35201;&#24615;&#21644;TD&#35823;&#24046;&#20256;&#25773;&#30340;&#38382;&#39064;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#21512;&#29702;&#30340;&#24378;&#35843;&#20989;&#25968;&#19981;&#20165;&#25913;&#36827;&#20102;&#20540;&#20272;&#35745;&#65292;&#36824;&#21152;&#36895;&#20102;&#23398;&#20064;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2310.08091</link><description>&lt;p&gt;
&#20998;&#36776;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Discerning Temporal Difference Learning. (arXiv:2310.08091v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08091
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20998;&#36776;TD&#23398;&#20064;(DTD)&#30340;&#26032;&#22411;TD&#31639;&#27861;&#65292;&#36890;&#36807;&#28789;&#27963;&#30340;&#24378;&#35843;&#20989;&#25968;&#26469;&#20998;&#37197;&#36164;&#28304;&#65292;&#20197;&#25913;&#21892;&#29366;&#24577;&#20043;&#38388;&#30340;&#36731;&#37325;&#20998;&#37197;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;TD&#23398;&#20064;&#20013;&#24573;&#35270;&#21382;&#21490;&#29366;&#24577;&#37325;&#35201;&#24615;&#21644;TD&#35823;&#24046;&#20256;&#25773;&#30340;&#38382;&#39064;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#21512;&#29702;&#30340;&#24378;&#35843;&#20989;&#25968;&#19981;&#20165;&#25913;&#36827;&#20102;&#20540;&#20272;&#35745;&#65292;&#36824;&#21152;&#36895;&#20102;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;(TD)&#26159;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#26088;&#22312;&#39640;&#25928;&#35780;&#20272;&#31574;&#30053;&#30340;&#20215;&#20540;&#20989;&#25968;&#12290;TD($\lambda$)&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#21464;&#20307;&#65292;&#36890;&#36807;&#24341;&#20837;&#35760;&#24518;&#36712;&#36857;&#23558;&#39044;&#27979;&#35823;&#24046;&#20998;&#25955;&#21040;&#21382;&#21490;&#19978;&#19979;&#25991;&#20013;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#32463;&#24120;&#24573;&#35270;&#21382;&#21490;&#29366;&#24577;&#30340;&#37325;&#35201;&#24615;&#20197;&#21450;&#20256;&#25773;TD&#35823;&#24046;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#65292;&#36825;&#21463;&#21040;&#35775;&#38382;&#22833;&#34913;&#25110;&#32467;&#26524;&#22122;&#22768;&#31561;&#25361;&#25112;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20998;&#36776;TD&#23398;&#20064;(DTD)&#30340;&#26032;&#22411;TD&#31639;&#27861;&#65292;&#23427;&#20801;&#35768;&#28789;&#27963;&#30340;&#24378;&#35843;&#20989;&#25968;-&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#39044;&#20808;&#30830;&#23450;&#25110;&#33258;&#36866;&#24212;&#22320;&#20998;&#37197;&#36164;&#28304;&#20197;&#25552;&#39640;&#29366;&#24577;&#20043;&#38388;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#22312;&#29305;&#23450;&#31867;&#21035;&#30340;&#24378;&#35843;&#20989;&#25968;&#20869;&#24314;&#31435;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#36136;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#28145;&#24230;RL&#29615;&#22659;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#21512;&#29702;&#30340;&#24378;&#35843;&#20989;&#25968;&#19981;&#20165;&#21487;&#20197;&#25913;&#36827;&#20540;&#20272;&#35745;&#65292;&#36824;&#21487;&#20197;&#21152;&#36895;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal difference learning (TD) is a foundational concept in reinforcement learning (RL), aimed at efficiently assessing a policy's value function. TD($\lambda$), a potent variant, incorporates a memory trace to distribute the prediction error into the historical context. However, this approach often neglects the significance of historical states and the relative importance of propagating the TD error, influenced by challenges such as visitation imbalance or outcome noise. To address this, we propose a novel TD algorithm named discerning TD learning (DTD), which allows flexible emphasis functions$-$predetermined or adapted during training$-$to allocate efforts effectively across states. We establish the convergence properties of our method within a specific class of emphasis functions and showcase its promising potential for adaptation to deep RL contexts. Empirical results underscore that employing a judicious emphasis function not only improves value estimation but also expedites l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#37325;&#23614;&#27874;&#27573;&#38382;&#39064;&#20013;&#23436;&#20840;&#33258;&#36866;&#24212;&#30340;&#36951;&#25022;&#26368;&#23567;&#21270;&#65292;&#25552;&#20986;&#20102;&#38543;&#26426;&#33258;&#36866;&#24212;&#37325;&#23614;&#27874;&#27573;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#36866;&#24212;&#24615;&#31639;&#27861;&#30456;&#23545;&#20110;&#26631;&#20934;&#35774;&#32622;&#20250;&#26377;&#26356;&#39640;&#30340;&#36951;&#25022;&#12290;</title><link>http://arxiv.org/abs/2310.02975</link><description>&lt;p&gt;
&#22312;&#37325;&#23614;&#27874;&#27573;&#30340;&#23436;&#20840;&#33258;&#36866;&#24212;&#36951;&#25022;&#26368;&#23567;&#21270;&#39046;&#22495;&#20013;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Fully Adaptive Regret Minimization in Heavy-Tailed Bandits. (arXiv:2310.02975v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#37325;&#23614;&#27874;&#27573;&#38382;&#39064;&#20013;&#23436;&#20840;&#33258;&#36866;&#24212;&#30340;&#36951;&#25022;&#26368;&#23567;&#21270;&#65292;&#25552;&#20986;&#20102;&#38543;&#26426;&#33258;&#36866;&#24212;&#37325;&#23614;&#27874;&#27573;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#36866;&#24212;&#24615;&#31639;&#27861;&#30456;&#23545;&#20110;&#26631;&#20934;&#35774;&#32622;&#20250;&#26377;&#26356;&#39640;&#30340;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#23614;&#20998;&#24067;&#22312;&#37329;&#34701;&#21040;&#30005;&#20449;&#31561;&#22810;&#31181;&#29615;&#22659;&#20013;&#33258;&#28982;&#32780;&#28982;&#22320;&#20986;&#29616;&#12290;&#34429;&#28982;&#22312;&#27425;&#39640;&#26031;&#25110;&#26377;&#30028;&#25903;&#25745;&#22870;&#21169;&#19979;&#30340;&#36951;&#25022;&#26368;&#23567;&#21270;&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22312;&#37325;&#23614;&#20998;&#24067;&#19978;&#30340;&#23398;&#20064;&#21482;&#22312;&#36807;&#21435;&#21313;&#24180;&#20013;&#21463;&#21040;&#20851;&#27880;&#12290;&#22312;&#38543;&#26426;&#37325;&#23614;&#27874;&#27573;&#38382;&#39064;&#20013;&#65292;&#19968;&#20010;&#20195;&#29702;&#22312;&#20551;&#35774;&#20998;&#24067;&#26377;&#26377;&#30028;&#26368;&#22823;&#38454;&#30340;&#26377;&#38480;&#30697;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#65292;&#36825;&#20123;&#30697;&#34987;&#24120;&#25968;u&#19968;&#33268;&#26377;&#30028;&#65292;&#23545;&#20110;&#26576;&#20010;&#949;&#8712;(0,1]&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25991;&#29486;&#20013;&#21482;&#25552;&#20379;&#38656;&#35201;&#36825;&#20004;&#20010;&#37327;&#20316;&#20026;&#36755;&#20837;&#30340;&#31639;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38543;&#26426;&#33258;&#36866;&#24212;&#37325;&#23614;&#27874;&#27573;&#38382;&#39064;&#65292;&#36825;&#26159;&#26631;&#20934;&#35774;&#32622;&#30340;&#19968;&#20010;&#21464;&#31181;&#65292;&#20854;&#20013;&#20195;&#29702;&#23545;&#949;&#21644;u&#22343;&#19981;&#30693;&#26195;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36866;&#24212;&#24615;&#26159;&#23384;&#22312;&#20195;&#20215;&#30340;&#65292;&#24182;&#24341;&#20837;&#23545;&#20110;&#20219;&#20309;&#33258;&#36866;&#24212;&#31639;&#27861;&#36951;&#25022;&#30340;&#20004;&#20010;&#19979;&#30028;&#65292;&#24847;&#21619;&#30528;&#30456;&#23545;&#20110;&#26631;&#20934;&#35774;&#32622;&#26377;&#26356;&#39640;&#30340;&#36951;&#25022;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#29305;&#23450;&#30340;&#20998;&#24067;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heavy-tailed distributions naturally arise in many settings, from finance to telecommunications. While regret minimization under sub-Gaussian or bounded support rewards has been widely studied, learning on heavy-tailed distributions only gained popularity over the last decade. In the stochastic heavy-tailed bandit problem, an agent learns under the assumption that the distributions have finite moments of maximum order $1+\epsilon$ which are uniformly bounded by a constant $u$, for some $\epsilon \in (0,1]$. To the best of our knowledge, literature only provides algorithms requiring these two quantities as an input. In this paper, we study the stochastic adaptive heavy-tailed bandit, a variation of the standard setting where both $\epsilon$ and $u$ are unknown to the agent. We show that adaptivity comes at a cost, introducing two lower bounds on the regret of any adaptive algorithm, implying a higher regret w.r.t. the standard setting. Finally, we introduce a specific distributional ass
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;SAM&#36827;&#34892;&#24314;&#31569;&#29289;&#20998;&#21106;&#27169;&#22411;&#30340;&#38646;-shot&#32454;&#21270;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#36965;&#24863;&#22270;&#20687;&#24212;&#29992;&#20013;SAM&#24615;&#33021;&#19981;&#20339;&#12289;&#26080;&#27861;&#36827;&#34892;&#35782;&#21035;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#22788;&#29702;&#12290;&#36890;&#36807;&#24341;&#20837;&#19981;&#21516;&#30340;&#25552;&#31034;&#26469;&#25552;&#21319;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.01845</link><description>&lt;p&gt;
&#20351;&#29992;SAM&#36827;&#34892;&#24314;&#31569;&#29289;&#20998;&#21106;&#27169;&#22411;&#30340;&#38646;-shot&#32454;&#21270;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Refinement of Buildings' Segmentation Models using SAM. (arXiv:2310.01845v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;SAM&#36827;&#34892;&#24314;&#31569;&#29289;&#20998;&#21106;&#27169;&#22411;&#30340;&#38646;-shot&#32454;&#21270;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#36965;&#24863;&#22270;&#20687;&#24212;&#29992;&#20013;SAM&#24615;&#33021;&#19981;&#20339;&#12289;&#26080;&#27861;&#36827;&#34892;&#35782;&#21035;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#22788;&#29702;&#12290;&#36890;&#36807;&#24341;&#20837;&#19981;&#21516;&#30340;&#25552;&#31034;&#26469;&#25552;&#21319;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#36890;&#24120;&#22312;&#24120;&#35268;&#22522;&#20934;&#27979;&#35797;&#20013;&#35780;&#20272;&#12290;&#23558;&#36825;&#20123;&#27169;&#22411;&#24212;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#65292;&#22914;&#36965;&#24863;&#22270;&#20687;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#20805;&#20998;&#24320;&#21457;&#30340;&#39046;&#22495;&#12290;&#22312;&#36965;&#24863;&#39046;&#22495;&#20013;&#65292;&#31934;&#30830;&#30340;&#24314;&#31569;&#29289;&#23454;&#20363;&#20998;&#21106;&#23545;&#20110;&#22478;&#24066;&#35268;&#21010;&#31561;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23427;&#20204;&#30340;&#27867;&#21270;&#33021;&#21147;&#21487;&#33021;&#21463;&#38480;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20197;&#20351;&#22522;&#30784;&#27169;&#22411;&#36866;&#24212;&#24050;&#26377;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#20247;&#22810;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#30340;&#37325;&#28857;&#22312;&#20110;Segment Anything Model&#65288;SAM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#20854;&#25797;&#38271;&#26080;&#31867;&#21035;&#22270;&#20687;&#20998;&#21106;&#33021;&#21147;&#32780;&#38395;&#21517;&#12290;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;SAM&#30340;&#23616;&#38480;&#24615;&#65292;&#25581;&#31034;&#20102;&#23427;&#22312;&#24212;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#26102;&#24615;&#33021;&#19981;&#20339;&#12290;&#27492;&#22806;&#65292;SAM&#19981;&#20855;&#22791;&#35782;&#21035;&#33021;&#21147;&#65292;&#22240;&#27492;&#26080;&#27861;&#23545;&#23450;&#20301;&#30340;&#23545;&#35937;&#36827;&#34892;&#20998;&#31867;&#21644;&#26631;&#35760;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19981;&#21516;&#30340;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Foundation models have excelled in various tasks but are often evaluated on general benchmarks. The adaptation of these models for specific domains, such as remote sensing imagery, remains an underexplored area. In remote sensing, precise building instance segmentation is vital for applications like urban planning. While Convolutional Neural Networks (CNNs) perform well, their generalization can be limited. For this aim, we present a novel approach to adapt foundation models to address existing models' generalization dropback. Among several models, our focus centers on the Segment Anything Model (SAM), a potent foundation model renowned for its prowess in class-agnostic image segmentation capabilities. We start by identifying the limitations of SAM, revealing its suboptimal performance when applied to remote sensing imagery. Moreover, SAM does not offer recognition abilities and thus fails to classify and tag localized objects. To address these limitations, we introduce different promp
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20284;&#28982;&#27604;&#30340;&#20219;&#21153;&#39044;&#27979;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#31163;&#32676;&#26816;&#27979;&#22120;&#36827;&#34892;&#20219;&#21153;&#26631;&#35782;&#39044;&#27979;&#65292;&#35299;&#20915;&#20102;&#26080;&#20219;&#21153;&#26631;&#35782;&#31526;&#30340;&#27979;&#35797;&#26679;&#26412;&#30340;&#20219;&#21153;&#39044;&#27979;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.15048</link><description>&lt;p&gt;
&#22522;&#20110;&#20284;&#28982;&#27604;&#30340;&#20219;&#21153;&#39044;&#27979;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Class Incremental Learning via Likelihood Ratio Based Task Prediction. (arXiv:2309.15048v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15048
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20284;&#28982;&#27604;&#30340;&#20219;&#21153;&#39044;&#27979;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#31163;&#32676;&#26816;&#27979;&#22120;&#36827;&#34892;&#20219;&#21153;&#26631;&#35782;&#39044;&#27979;&#65292;&#35299;&#20915;&#20102;&#26080;&#20219;&#21153;&#26631;&#35782;&#31526;&#30340;&#27979;&#35797;&#26679;&#26412;&#30340;&#20219;&#21153;&#39044;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#22686;&#37327;&#23398;&#20064;&#26159;&#19968;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#19981;&#26029;&#23398;&#20064;&#30340;&#35774;&#32622;&#65292;&#36890;&#36807;&#39034;&#24207;&#23398;&#20064;&#19968;&#31995;&#21015;&#20219;&#21153;&#12290;&#27599;&#20010;&#20219;&#21153;&#30001;&#19968;&#32452;&#21807;&#19968;&#30340;&#31867;&#32452;&#25104;&#12290;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#20851;&#38190;&#29305;&#28857;&#26159;&#65292;&#22312;&#27979;&#35797;&#26102;&#19981;&#25552;&#20379;&#27599;&#20010;&#27979;&#35797;&#26679;&#26412;&#30340;&#20219;&#21153;&#26631;&#35782;&#31526;&#65288;&#25110;&#20219;&#21153;ID&#65289;&#12290;&#20026;&#27599;&#20010;&#27979;&#35797;&#26679;&#26412;&#39044;&#27979;&#20219;&#21153;ID&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#19968;&#31181;&#26032;&#20852;&#30340;&#29702;&#35770;&#19978;&#21512;&#29702;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#26159;&#26681;&#25454;&#20219;&#21153;&#22686;&#37327;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#20849;&#20139;&#32593;&#32476;&#20013;&#20026;&#25152;&#26377;&#20219;&#21153;&#35757;&#32451;&#27599;&#20010;&#20219;&#21153;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#65292;&#20197;&#22788;&#29702;&#36951;&#24536;&#12290;&#35813;&#26041;&#27861;&#20013;&#27599;&#20010;&#20219;&#21153;&#30340;&#27169;&#22411;&#26159;&#19968;&#20010;&#38750;&#24120;&#35268;&#20998;&#31867;&#22120;&#32780;&#19981;&#26159;&#20256;&#32479;&#20998;&#31867;&#22120;&#30340;&#31163;&#32676;&#26816;&#27979;&#22120;&#12290;&#31163;&#32676;&#26816;&#27979;&#22120;&#21487;&#20197;&#23545;&#20219;&#21153;&#20869;&#65288;&#20998;&#24067;&#20869;&#65288;IND&#65289;&#65289;&#30340;&#31867;&#36827;&#34892;&#39044;&#27979;&#21644;&#35782;&#21035;&#31163;&#32676;&#25968;&#25454;&#12290;&#22312;&#25512;&#26029;&#26399;&#38388;&#65292;&#31163;&#32676;&#26816;&#27979;&#33021;&#21147;&#26159;&#27599;&#20010;&#27979;&#35797;&#26679;&#26412;&#30340;&#20219;&#21153;ID&#39044;&#27979;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#35748;&#20026;&#20351;&#29992;&#20256;&#32479;&#30340;&#31163;&#32676;&#26816;&#27979;&#22120;&#36827;&#34892;&#20219;&#21153;ID&#39044;&#27979;&#26159;&#27425;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class incremental learning (CIL) is a challenging setting of continual learning, which learns a series of tasks sequentially. Each task consists of a set of unique classes. The key feature of CIL is that no task identifier (or task-id) is provided at test time for each test sample. Predicting the task-id for each test sample is a challenging problem. An emerging theoretically justified and effective approach is to train a task-specific model for each task in a shared network for all tasks based on a task-incremental learning (TIL) method to deal with forgetting. The model for each task in this approach is an out-of-distribution (OOD) detector rather than a conventional classifier. The OOD detector can perform both within-task (in-distribution (IND)) class prediction and OOD detection. The OOD detection capability is the key for task-id prediction during inference for each test sample. However, this paper argues that using a traditional OOD detector for task-id prediction is sub-optimal
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#24378;&#21270;&#26041;&#27861;RoboShot&#65292;&#36890;&#36807;&#20351;&#29992;&#38646;&#26679;&#26412;&#35821;&#35328;&#27169;&#22411;&#20174;&#20219;&#21153;&#25551;&#36848;&#20013;&#33719;&#21462;&#26377;&#29992;&#30340;&#35265;&#35299;&#65292;&#24182;&#24212;&#29992;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#23884;&#20837;&#20013;&#20197;&#21435;&#38500;&#26377;&#23475;&#25104;&#20998;&#24182;&#22686;&#24378;&#26377;&#29992;&#25104;&#20998;&#65292;&#20174;&#32780;&#25913;&#21892;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.04344</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#23545;&#38646;&#26679;&#26412;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#24378;&#21270;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Robustification of Zero-Shot Models With Foundation Models. (arXiv:2309.04344v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04344
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#24378;&#21270;&#26041;&#27861;RoboShot&#65292;&#36890;&#36807;&#20351;&#29992;&#38646;&#26679;&#26412;&#35821;&#35328;&#27169;&#22411;&#20174;&#20219;&#21153;&#25551;&#36848;&#20013;&#33719;&#21462;&#26377;&#29992;&#30340;&#35265;&#35299;&#65292;&#24182;&#24212;&#29992;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#23884;&#20837;&#20013;&#20197;&#21435;&#38500;&#26377;&#23475;&#25104;&#20998;&#24182;&#22686;&#24378;&#26377;&#29992;&#25104;&#20998;&#65292;&#20174;&#32780;&#25913;&#21892;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#25512;&#26029;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#33539;&#24335;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#36827;&#19968;&#27493;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#36827;&#34892;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#32487;&#25215;&#30340;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#24433;&#21709;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#20256;&#32479;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#24494;&#35843;&#65292;&#20294;&#36825;&#21066;&#24369;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20027;&#35201;&#20248;&#21183;&#65292;&#21363;&#21487;&#20197;&#30452;&#25509;&#20351;&#29992;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;RoboShot&#65292;&#19968;&#31181;&#23436;&#20840;&#38646;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25913;&#21892;&#39044;&#35757;&#32451;&#27169;&#22411;&#23884;&#20837;&#30340;&#40065;&#26834;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#38646;&#26679;&#26412;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#20174;&#20219;&#21153;&#25551;&#36848;&#20013;&#33719;&#21462;&#26377;&#29992;&#30340;&#35265;&#35299;&#12290;&#36825;&#20123;&#35265;&#35299;&#34987;&#23884;&#20837;&#24182;&#29992;&#20110;&#21435;&#38500;&#23884;&#20837;&#20013;&#30340;&#26377;&#23475;&#25104;&#20998;&#24182;&#22686;&#24378;&#26377;&#29992;&#25104;&#20998;--&#32780;&#26080;&#38656;&#20219;&#20309;&#30417;&#30563;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#19988;&#21487;&#35745;&#31639;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#26512;&#38646;&#26679;&#26412;&#23884;&#20837;&#20013;&#30340;&#20559;&#35265;&#65292;&#24182;&#32473;&#20986;&#20102;&#22312;&#20160;&#20040;&#26465;&#20214;&#19979;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#30340;&#32467;&#26524;&#12290;&#22312;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#22312;&#20061;&#20010;&#22270;&#20687;&#21644;NLP&#20998;&#31867;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;RoboShot&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-shot inference is a powerful paradigm that enables the use of large pretrained models for downstream classification tasks without further training. However, these models are vulnerable to inherited biases that can impact their performance. The traditional solution is fine-tuning, but this undermines the key advantage of pretrained models, which is their ability to be used out-of-the-box. We propose RoboShot, a method that improves the robustness of pretrained model embeddings in a fully zero-shot fashion. First, we use zero-shot language models (LMs) to obtain useful insights from task descriptions. These insights are embedded and used to remove harmful and boost useful components in embeddings -- without any supervision. Theoretically, we provide a simple and tractable model for biases in zero-shot embeddings and give a result characterizing under what conditions our approach can boost performance. Empirically, we evaluate RoboShot on nine image and NLP classification tasks and s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#39318;&#20010;&#20855;&#26377;&#21487;&#39564;&#35777;&#23433;&#20840;&#20445;&#35777;&#30340;&#26694;&#26550;&#8212;&#8212;&#28040;&#38500;&#21644;&#26816;&#26597;&#65292;&#29992;&#20110;&#23545;&#25239;&#25932;&#23545;&#25552;&#31034;&#12290;&#36890;&#36807;&#36880;&#20010;&#28040;&#38500;&#26631;&#35760;&#24182;&#20351;&#29992;&#23433;&#20840;&#36807;&#28388;&#22120;&#26816;&#26597;&#29983;&#25104;&#30340;&#23376;&#24207;&#21015;&#65292;&#30830;&#20445;&#20219;&#20309;&#25932;&#23545;&#20462;&#25913;&#30340;&#26377;&#23475;&#36755;&#20837;&#25552;&#31034;&#37117;&#33021;&#34987;&#27491;&#30830;&#26631;&#35782;&#20026;&#26377;&#23475;&#12290;</title><link>http://arxiv.org/abs/2309.02705</link><description>&lt;p&gt;
&#35777;&#26126;LLM&#23545;&#25239;&#25932;&#23545;&#25552;&#31034;&#30340;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
Certifying LLM Safety against Adversarial Prompting. (arXiv:2309.02705v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#39318;&#20010;&#20855;&#26377;&#21487;&#39564;&#35777;&#23433;&#20840;&#20445;&#35777;&#30340;&#26694;&#26550;&#8212;&#8212;&#28040;&#38500;&#21644;&#26816;&#26597;&#65292;&#29992;&#20110;&#23545;&#25239;&#25932;&#23545;&#25552;&#31034;&#12290;&#36890;&#36807;&#36880;&#20010;&#28040;&#38500;&#26631;&#35760;&#24182;&#20351;&#29992;&#23433;&#20840;&#36807;&#28388;&#22120;&#26816;&#26597;&#29983;&#25104;&#30340;&#23376;&#24207;&#21015;&#65292;&#30830;&#20445;&#20219;&#20309;&#25932;&#23545;&#20462;&#25913;&#30340;&#26377;&#23475;&#36755;&#20837;&#25552;&#31034;&#37117;&#33021;&#34987;&#27491;&#30830;&#26631;&#35782;&#20026;&#26377;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#30830;&#20445;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#23433;&#20840;&#65292;&#20844;&#24320;&#20351;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24341;&#20837;&#20102;&#25152;&#35859;&#30340;&#8220;&#27169;&#22411;&#23545;&#40784;&#8221;&#38450;&#25252;&#25514;&#26045;&#12290;&#19968;&#20010;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#24212;&#35813;&#25298;&#32477;&#29992;&#25143;&#30340;&#35831;&#27714;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#23433;&#20840;&#25514;&#26045;&#23481;&#26131;&#21463;&#21040;&#25932;&#23545;&#25552;&#31034;&#30340;&#25915;&#20987;&#65292;&#25932;&#23545;&#25552;&#31034;&#21253;&#21547;&#24694;&#24847;&#35774;&#35745;&#30340;&#26631;&#35760;&#24207;&#21015;&#65292;&#20197;&#35268;&#36991;&#27169;&#22411;&#30340;&#23433;&#20840;&#38450;&#25252;&#24182;&#23548;&#33268;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21487;&#39564;&#35777;&#23433;&#20840;&#20445;&#35777;&#30340;&#31532;&#19968;&#20010;&#23545;&#25239;&#25932;&#23545;&#25552;&#31034;&#30340;&#26694;&#26550;&#8212;&#8212;&#28040;&#38500;&#21644;&#26816;&#26597;&#12290;&#25105;&#20204;&#36880;&#20010;&#28040;&#38500;&#26631;&#35760;&#65292;&#24182;&#20351;&#29992;&#23433;&#20840;&#36807;&#28388;&#22120;&#26816;&#26597;&#29983;&#25104;&#30340;&#23376;&#24207;&#21015;&#12290;&#22914;&#26524;&#23433;&#20840;&#36807;&#28388;&#22120;&#26816;&#27979;&#21040;&#20219;&#20309;&#23376;&#24207;&#21015;&#25110;&#36755;&#20837;&#25552;&#31034;&#26377;&#23475;&#65292;&#25105;&#20204;&#30340;&#36807;&#31243;&#23558;&#23558;&#36755;&#20837;&#25552;&#31034;&#26631;&#35760;&#20026;&#26377;&#23475;&#12290;&#36825;&#20445;&#35777;&#20102;&#23545;&#20110;&#26576;&#20010;&#29305;&#23450;&#22823;&#23567;&#30340;&#26377;&#23475;&#36755;&#20837;&#25552;&#31034;&#30340;&#20219;&#20309;&#25932;&#23545;&#20462;&#25913;&#20063;&#23558;&#34987;&#26631;&#35760;&#20026;&#26377;&#23475;&#12290;&#25105;&#20204;&#23545;&#25239;&#19977;&#31181;&#25915;&#20987;&#27169;&#24335;&#65306;i)&#25932;&#23545;&#21518;&#32512;&#65292;&#21363;&#38468;&#21152;&#25932;&#23545;&#24207;&#21015;&#8230;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) released for public use incorporate guardrails to ensure their output is safe, often referred to as "model alignment." An aligned language model should decline a user's request to produce harmful content. However, such safety measures are vulnerable to adversarial prompts, which contain maliciously designed token sequences to circumvent the model's safety guards and cause it to produce harmful content. In this work, we introduce erase-and-check, the first framework to defend against adversarial prompts with verifiable safety guarantees. We erase tokens individually and inspect the resulting subsequences using a safety filter. Our procedure labels the input prompt as harmful if any subsequences or the input prompt are detected as harmful by the filter. This guarantees that any adversarial modification of a harmful prompt up to a certain size is also labeled harmful. We defend against three attack modes: i) adversarial suffix, which appends an adversarial seq
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22522;&#20110;&#23545;&#27604;&#25439;&#22833;&#30340;&#31163;&#32447;&#25163;&#20889;&#31614;&#21517;&#39564;&#35777;&#27169;&#22411;&#30340;&#30333;&#30418;&#35823;&#25253;&#23545;&#25239;&#25915;&#20987;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25915;&#20987;&#35270;&#20026;&#20070;&#20889;&#39118;&#26684;&#20043;&#38388;&#30340;&#39118;&#26684;&#36716;&#25442;&#65292;&#24341;&#20837;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#29983;&#25104;&#27450;&#39575;&#24615;&#22270;&#20687;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.08925</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#27604;&#25439;&#22833;&#30340;&#31163;&#32447;&#25163;&#20889;&#31614;&#21517;&#39564;&#35777;&#27169;&#22411;&#30340;&#30333;&#30418;&#35823;&#25253;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A White-Box False Positive Adversarial Attack Method on Contrastive Loss-Based Offline Handwritten Signature Verification Models. (arXiv:2308.08925v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22522;&#20110;&#23545;&#27604;&#25439;&#22833;&#30340;&#31163;&#32447;&#25163;&#20889;&#31614;&#21517;&#39564;&#35777;&#27169;&#22411;&#30340;&#30333;&#30418;&#35823;&#25253;&#23545;&#25239;&#25915;&#20987;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25915;&#20987;&#35270;&#20026;&#20070;&#20889;&#39118;&#26684;&#20043;&#38388;&#30340;&#39118;&#26684;&#36716;&#25442;&#65292;&#24341;&#20837;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#29983;&#25104;&#27450;&#39575;&#24615;&#22270;&#20687;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22522;&#20110;&#23545;&#27604;&#25439;&#22833;&#30340;&#31163;&#32447;&#25163;&#20889;&#31614;&#21517;&#39564;&#35777;&#27169;&#22411;&#30340;&#30333;&#30418;&#35823;&#25253;&#23545;&#25239;&#25915;&#20987;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#23558;&#25915;&#20987;&#35270;&#20026;&#22312;&#23494;&#20999;&#30456;&#20851;&#20294;&#19981;&#21516;&#30340;&#20070;&#20889;&#39118;&#26684;&#20043;&#38388;&#36827;&#34892;&#39118;&#26684;&#36716;&#25442;&#12290;&#20026;&#20102;&#24341;&#23548;&#27450;&#39575;&#24615;&#22270;&#20687;&#30340;&#29983;&#25104;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#25200;&#21160;&#21407;&#22987;&#26679;&#26412;&#19982;&#21512;&#25104;&#26679;&#26412;&#30340;&#23884;&#20837;&#21521;&#37327;&#20043;&#38388;&#30340;&#27431;&#27663;&#36317;&#31163;&#26469;&#25552;&#39640;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#21516;&#26102;&#36890;&#36807;&#20943;&#23567;&#29983;&#25104;&#22270;&#20687;&#19982;&#21407;&#22987;&#22270;&#20687;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#20445;&#35777;&#26368;&#23567;&#30340;&#25200;&#21160;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23545;&#27604;&#25439;&#22833;&#30340;&#31163;&#32447;&#25163;&#20889;&#31614;&#21517;&#39564;&#35777;&#27169;&#22411;&#30340;&#30333;&#30418;&#25915;&#20987;&#20013;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#30340;&#20851;&#38190;&#36129;&#29486;&#21253;&#25324;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35823;&#25253;&#25915;&#20987;&#26041;&#27861;&#12289;&#20004;&#20010;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#12289;&#26377;&#25928;&#30340;&#20070;&#20889;&#39118;&#26684;&#36716;&#25442;&#20197;&#21450;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we tackle the challenge of white-box false positive adversarial attacks on contrastive loss-based offline handwritten signature verification models. We propose a novel attack method that treats the attack as a style transfer between closely related but distinct writing styles. To guide the generation of deceptive images, we introduce two new loss functions that enhance the attack success rate by perturbing the Euclidean distance between the embedding vectors of the original and synthesized samples, while ensuring minimal perturbations by reducing the difference between the generated image and the original image. Our method demonstrates state-of-the-art performance in white-box attacks on contrastive loss-based offline handwritten signature verification models, as evidenced by our experiments. The key contributions of this paper include a novel false positive attack method, two new loss functions, effective style transfer in handwriting styles, and superior performance in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#19968;&#33268;&#24615;&#30340;&#26080;&#30417;&#30563;&#28145;&#24230;&#22270;&#21305;&#37197;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#30495;&#23454;&#23545;&#24212;&#30340;&#20851;&#38190;&#28857;&#23545;&#65292;&#36890;&#36807;&#22312;&#21516;&#19968;&#23545;&#35937;&#31867;&#21035;&#30340;&#22270;&#20687;&#20043;&#38388;&#24378;&#21046;&#21305;&#37197;&#19968;&#33268;&#24615;&#26469;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#24456;&#39640;&#30340;&#28789;&#27963;&#24615;&#65292;&#24182;&#19988;&#22312;&#26080;&#30417;&#30563;&#22270;&#21305;&#37197;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2307.08930</link><description>&lt;p&gt;
&#22522;&#20110;&#24490;&#29615;&#19968;&#33268;&#24615;&#30340;&#26080;&#30417;&#30563;&#28145;&#24230;&#22270;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Deep Graph Matching Based on Cycle Consistency. (arXiv:2307.08930v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#19968;&#33268;&#24615;&#30340;&#26080;&#30417;&#30563;&#28145;&#24230;&#22270;&#21305;&#37197;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#30495;&#23454;&#23545;&#24212;&#30340;&#20851;&#38190;&#28857;&#23545;&#65292;&#36890;&#36807;&#22312;&#21516;&#19968;&#23545;&#35937;&#31867;&#21035;&#30340;&#22270;&#20687;&#20043;&#38388;&#24378;&#21046;&#21305;&#37197;&#19968;&#33268;&#24615;&#26469;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#24456;&#39640;&#30340;&#28789;&#27963;&#24615;&#65292;&#24182;&#19988;&#22312;&#26080;&#30417;&#30563;&#22270;&#21305;&#37197;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#31232;&#30095;&#39046;&#22495;&#30340;&#26080;&#30417;&#30563;&#28145;&#24230;&#22270;&#21305;&#37197;&#20013;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#24212;&#29992;&#20110;&#22270;&#20687;&#20013;&#30340;&#20851;&#38190;&#28857;&#21305;&#37197;&#12290;&#19982;&#26631;&#20934;&#30340;&#8220;&#30417;&#30563;&#8221;&#26041;&#27861;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#20851;&#38190;&#28857;&#23545;&#20043;&#38388;&#30340;&#30495;&#23454;&#23545;&#24212;&#12290;&#30456;&#21453;&#65292;&#23427;&#36890;&#36807;&#24378;&#21046;&#21516;&#19968;&#23545;&#35937;&#31867;&#21035;&#30340;&#22270;&#20687;&#20043;&#38388;&#30340;&#21305;&#37197;&#19968;&#33268;&#24615;&#26469;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#12290;&#30001;&#20110;&#21305;&#37197;&#21644;&#19968;&#33268;&#24615;&#25439;&#22833;&#26159;&#31163;&#25955;&#30340;&#65292;&#23427;&#20204;&#30340;&#23548;&#25968;&#19981;&#33021;&#30452;&#25509;&#29992;&#20110;&#23398;&#20064;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#32452;&#21512;&#27714;&#35299;&#22120;&#30340;&#40657;&#30418;&#24494;&#20998;&#30340;&#26368;&#26032;&#32467;&#26524;&#22522;&#30784;&#19978;&#26500;&#24314;&#25105;&#20204;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;&#26041;&#27861;&#38750;&#24120;&#28789;&#27963;&#65292;&#22240;&#20026;&#23427;&#19982;&#20219;&#24847;&#32593;&#32476;&#26550;&#26500;&#21644;&#32452;&#21512;&#27714;&#35299;&#22120;&#20860;&#23481;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#22312;&#26080;&#30417;&#30563;&#22270;&#21305;&#37197;&#26041;&#38754;&#36798;&#21040;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
We contribute to the sparsely populated area of unsupervised deep graph matching with application to keypoint matching in images. Contrary to the standard \emph{supervised} approach, our method does not require ground truth correspondences between keypoint pairs. Instead, it is self-supervised by enforcing consistency of matchings between images of the same object category. As the matching and the consistency loss are discrete, their derivatives cannot be straightforwardly used for learning. We address this issue in a principled way by building our method upon the recent results on black-box differentiation of combinatorial solvers. This makes our method exceptionally flexible, as it is compatible with arbitrary network architectures and combinatorial solvers. Our experimental evaluation suggests that our technique sets a new state-of-the-art for unsupervised graph matching.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36719;&#25552;&#31034;&#35843;&#20248;&#26469;&#22686;&#24378;&#23494;&#38598;&#26816;&#32034;&#30340;&#26041;&#27861;&#65288;SPTAR&#65289;&#12290;&#36890;&#36807;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#36719;&#25552;&#31034;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#29983;&#25104;&#24369;&#26597;&#35810;&#65292;&#21487;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.08303</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#23494;&#38598;&#26816;&#32034;&#30340;&#36719;&#25552;&#31034;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language Models. (arXiv:2307.08303v1 [cs.IR] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36719;&#25552;&#31034;&#35843;&#20248;&#26469;&#22686;&#24378;&#23494;&#38598;&#26816;&#32034;&#30340;&#26041;&#27861;&#65288;SPTAR&#65289;&#12290;&#36890;&#36807;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#36719;&#25552;&#31034;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#29983;&#25104;&#24369;&#26597;&#35810;&#65292;&#21487;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23494;&#38598;&#26816;&#32034;&#65288;DR&#65289;&#23558;&#26597;&#35810;&#21644;&#25991;&#26723;&#36716;&#21270;&#20026;&#23494;&#38598;&#21521;&#37327;&#34920;&#31034;&#65292;&#24182;&#22312;&#21521;&#37327;&#31354;&#38388;&#20013;&#27979;&#37327;&#26597;&#35810;&#19982;&#25991;&#26723;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;DR&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#32570;&#20047;&#39046;&#22495;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#34429;&#28982;DR&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#20174;&#22823;&#35268;&#27169;&#20844;&#20849;&#25968;&#25454;&#38598;&#65288;&#22914;MS MARCO&#65289;&#20013;&#23398;&#20064;&#65292;&#20294;&#35777;&#25454;&#34920;&#26126;&#65292;&#24182;&#38750;&#25152;&#26377;DR&#27169;&#22411;&#21644;&#39046;&#22495;&#37117;&#33021;&#21516;&#31561;&#21463;&#30410;&#20110;&#36801;&#31227;&#23398;&#20064;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#30740;&#31350;&#20154;&#21592;&#36716;&#21521;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#25913;&#36827;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;DR&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20013;&#37319;&#29992;&#30340;&#30828;&#25552;&#31034;&#25110;&#20154;&#24037;&#32534;&#20889;&#30340;&#25552;&#31034;&#26080;&#27861;&#20445;&#35777;&#29983;&#25104;&#30340;&#24369;&#26597;&#35810;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#22686;&#24378;DR&#30340;&#36719;&#25552;&#31034;&#35843;&#20248;&#65288;SPTAR&#65289;&#65306;&#23545;&#20110;&#27599;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#21033;&#29992;&#36719;&#25552;&#31034;&#35843;&#20248;&#22312;&#26377;&#38480;&#30340;&#30495;&#23454;&#25968;&#25454;&#19978;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#36719;&#25552;&#31034;&#65292;&#28982;&#21518;&#29992;&#36825;&#20123;&#25552;&#31034;&#24341;&#23548;LLMs&#20026;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#26631;&#35760;&#24369;&#26597;&#35810;&#65292;&#20174;&#32780;&#24471;&#21040;&#36275;&#22815;&#30340;&#24369;&#25991;&#26723;-&#26597;&#35810;&#23545;&#26469;&#35757;&#32451;&#20219;&#21153;&#29305;&#23450;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dense retrieval (DR) converts queries and documents into dense embeddings and measures the similarity between queries and documents in vector space. One of the challenges in DR is the lack of domain-specific training data. While DR models can learn from large-scale public datasets like MS MARCO through transfer learning, evidence shows that not all DR models and domains can benefit from transfer learning equally. Recently, some researchers have resorted to large language models (LLMs) to improve the zero-shot and few-shot DR models. However, the hard prompts or human-written prompts utilized in these works cannot guarantee the good quality of generated weak queries. To tackle this, we propose soft prompt tuning for augmenting DR (SPTAR): For each task, we leverage soft prompt-tuning to optimize a task-specific soft prompt on limited ground truth data and then prompt the LLMs to tag unlabeled documents with weak queries, yielding enough weak document-query pairs to train task-specific d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;Wasserstein&#21464;&#20998;&#20248;&#21270;&#65288;AWaVO&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#24418;&#24335;&#26041;&#27861;&#35299;&#20915;&#20102;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#35299;&#37322;&#21644;&#36879;&#26126;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#22870;&#21169;&#35774;&#35745;&#21644;&#31574;&#30053;&#25910;&#25947;&#30340;&#27010;&#29575;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2307.07084</link><description>&lt;p&gt;
&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20316;&#20026;Wasserstein&#21464;&#20998;&#25512;&#29702;&#65306;&#21487;&#35299;&#37322;&#24615;&#30340;&#24418;&#24335;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Safe Reinforcement Learning as Wasserstein Variational Inference: Formal Methods for Interpretability. (arXiv:2307.07084v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07084
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;Wasserstein&#21464;&#20998;&#20248;&#21270;&#65288;AWaVO&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#24418;&#24335;&#26041;&#27861;&#35299;&#20915;&#20102;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#35299;&#37322;&#21644;&#36879;&#26126;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#22870;&#21169;&#35774;&#35745;&#21644;&#31574;&#30053;&#25910;&#25947;&#30340;&#27010;&#29575;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#25110;&#26368;&#20248;&#25511;&#21046;&#21487;&#20197;&#20026;&#20855;&#26377;&#21487;&#21464;&#21160;&#24577;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#25552;&#20379;&#26377;&#25928;&#30340;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#23454;&#26045;&#20013;&#65292;&#35299;&#37322;&#22870;&#21169;&#20989;&#25968;&#21644;&#30456;&#24212;&#30340;&#26368;&#20248;&#31574;&#30053;&#19968;&#30452;&#26159;&#19968;&#20010;&#25345;&#20037;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#23558;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#25512;&#29702;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#65292;&#22240;&#20026;&#27010;&#29575;&#25512;&#29702;&#21407;&#21017;&#19978;&#25552;&#20379;&#20102;&#22810;&#26679;&#19988;&#24378;&#22823;&#30340;&#25968;&#23398;&#24037;&#20855;&#26469;&#25512;&#26029;&#38543;&#26426;&#21160;&#24577;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#22870;&#21169;&#35774;&#35745;&#21644;&#31574;&#30053;&#25910;&#25947;&#30340;&#27010;&#29575;&#35299;&#37322;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;Wasserstein&#21464;&#20998;&#20248;&#21270;&#65288;AWaVO&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#24418;&#24335;&#26041;&#27861;&#26469;&#35299;&#37322;&#22870;&#21169;&#35774;&#35745;&#65292;&#36879;&#26126;&#22320;&#35757;&#32451;&#25910;&#25947;&#65292;&#20197;&#21450;&#23545;&#39034;&#24207;&#20915;&#31574;&#30340;&#27010;&#29575;&#35299;&#37322;&#12290;&#20026;&#20102;&#35777;&#26126;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25910;&#25947;&#35757;&#32451;&#24182;&#20445;&#35777;&#20102;&#25910;&#25947;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning or optimal control can provide effective reasoning for sequential decision-making problems with variable dynamics. Such reasoning in practical implementation, however, poses a persistent challenge in interpreting the reward function and corresponding optimal policy. Consequently, formalizing the sequential decision-making problems as inference has a considerable value, as probabilistic inference in principle offers diverse and powerful mathematical tools to infer the stochastic dynamics whilst suggesting a probabilistic interpretation of the reward design and policy convergence. In this study, we propose a novel Adaptive Wasserstein Variational Optimization (AWaVO) to tackle these challenges in sequential decision-making. Our approach utilizes formal methods to provide interpretations of reward design, transparency of training convergence, and probabilistic interpretation of sequential decisions. To demonstrate practicality, we show convergent training with guara
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#38543;&#26426;&#24615;&#21487;&#34987;&#25915;&#20987;&#32773;&#21033;&#29992;&#30340;&#38382;&#39064;&#65292;&#20027;&#35201;&#20851;&#27880;&#27969;&#34892;&#30340;&#38543;&#26426;&#24179;&#28369;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#29992;&#20110;&#35757;&#32451;&#21487;&#35777;&#26126;&#40065;&#26834;&#24615;&#30340;&#27169;&#22411;&#21644;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#20294;&#20854;&#38543;&#26426;&#24615;&#21487;&#33021;&#23548;&#33268;&#31995;&#32479;&#34987;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2306.14043</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#38656;&#35201;&#33258;&#24049;&#30340;&#38543;&#26426;&#26631;&#20934;&#65306;&#38543;&#26426;&#24179;&#28369;&#21644;&#22522;&#20110;PRNG&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning needs its own Randomness Standard: Randomised Smoothing and PRNG-based attacks. (arXiv:2306.14043v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#38543;&#26426;&#24615;&#21487;&#34987;&#25915;&#20987;&#32773;&#21033;&#29992;&#30340;&#38382;&#39064;&#65292;&#20027;&#35201;&#20851;&#27880;&#27969;&#34892;&#30340;&#38543;&#26426;&#24179;&#28369;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#29992;&#20110;&#35757;&#32451;&#21487;&#35777;&#26126;&#40065;&#26834;&#24615;&#30340;&#27169;&#22411;&#21644;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#20294;&#20854;&#38543;&#26426;&#24615;&#21487;&#33021;&#23548;&#33268;&#31995;&#32479;&#34987;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#24615;&#25903;&#25345;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#35768;&#22810;&#20851;&#38190;&#21151;&#33021;&#65292;&#21253;&#25324;&#20248;&#21270;&#12289;&#25968;&#25454;&#36873;&#25321;&#12289;&#38544;&#31169;&#21644;&#23433;&#20840;&#12290;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#23558;&#29983;&#25104;&#25110;&#25910;&#38598;&#38543;&#26426;&#24615;&#30340;&#20219;&#21153;&#22806;&#21253;&#32473;&#20102;&#32534;&#35793;&#22120;&#12289;&#20113;&#26381;&#21153;&#25552;&#20379;&#21830;&#25110;&#24037;&#20855;&#38142;&#20013;&#30340;&#20854;&#20182;&#22320;&#26041;&#12290;&#20294;&#26159;&#65292;&#25915;&#20987;&#32773;&#21033;&#29992;&#19981;&#33391;&#38543;&#26426;&#24615;&#29978;&#33267;&#21019;&#24314;&#38543;&#26426;&#24615;&#30340;&#21382;&#21490;&#24736;&#20037;&#65292;&#23601;&#20687;NSA&#25918;&#32622;&#21518;&#38376;&#22312;&#38543;&#26426;&#25968;&#29983;&#25104;&#22120;&#20013;&#20197;&#30772;&#35299;&#21152;&#23494;&#19968;&#26679;&#12290;&#26412;&#25991;&#32771;&#34385;&#26159;&#21542;&#33021;&#22815;&#20165;&#21033;&#29992;&#25915;&#20987;&#32773;&#36890;&#24120;&#20381;&#36182;&#30340;&#38543;&#26426;&#24615;&#26469;&#21361;&#23475;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#12290;&#25105;&#20204;&#23558;&#37325;&#28857;&#25918;&#22312;&#38543;&#26426;&#24179;&#28369;&#19978;&#65292;&#36825;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#21487;&#35777;&#26126;&#40065;&#26834;&#24615;&#30340;&#27169;&#22411;&#65292;&#24182;&#20026;&#20219;&#24847;&#27169;&#22411;&#30340;&#29305;&#23450;&#36755;&#20837;&#25968;&#25454;&#28857;&#25552;&#20379;&#35748;&#35777;&#12290;&#25105;&#20204;&#36873;&#25321;&#38543;&#26426;&#24179;&#28369;&#26159;&#22240;&#20026;&#23427;&#29992;&#20110;&#23433;&#20840;&#21644;&#23433;&#20840;&#65288;&#29992;&#20110;&#23545;&#25239;&#23545;&#25239;&#24615;&#31034;&#20363;&#21644;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#20998;&#21035;&#65289;&#12290;&#22312;&#24149;&#21518;&#65292;&#23427;&#20381;&#36182;&#20110;&#37319;&#26679;&#39640;&#26031;&#22122;&#22768;&#26469;&#25506;&#32034;&#22260;&#32469;&#25968;&#25454;&#28857;&#30340;&#20307;&#31215;&#12290;
&lt;/p&gt;
&lt;p&gt;
Randomness supports many critical functions in the field of machine learning (ML) including optimisation, data selection, privacy, and security. ML systems outsource the task of generating or harvesting randomness to the compiler, the cloud service provider or elsewhere in the toolchain. Yet there is a long history of attackers exploiting poor randomness, or even creating it -- as when the NSA put backdoors in random number generators to break cryptography. In this paper we consider whether attackers can compromise an ML system using only the randomness on which they commonly rely. We focus our effort on Randomised Smoothing, a popular approach to train certifiably robust models, and to certify specific input datapoints of an arbitrary model. We choose Randomised Smoothing since it is used for both security and safety -- to counteract adversarial examples and quantify uncertainty respectively. Under the hood, it relies on sampling Gaussian noise to explore the volume around a data poin
&lt;/p&gt;</description></item><item><title>SRe$^2$L&#26159;&#19968;&#31181;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#19981;&#21516;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#21644;&#22270;&#20687;&#20998;&#36776;&#29575;&#65292;&#20855;&#26377;&#20302;&#35757;&#32451;&#25104;&#26412;&#21644;&#20869;&#23384;&#28040;&#32791;&#20197;&#21450;&#25193;&#23637;&#21040;&#20219;&#24847;&#35780;&#20272;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#30340;&#33021;&#21147;&#65292;&#22312;Tiny-ImageNet&#21644;ImageNet-1K&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20339;&#24615;&#33021;&#65292;&#24182;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.13092</link><description>&lt;p&gt;
&#20174;&#26032;&#30340;&#35282;&#24230;&#21387;&#32553;ImageNet&#35268;&#27169;&#25968;&#25454;&#38598;&#65306;SRe$^2$L
&lt;/p&gt;
&lt;p&gt;
Squeeze, Recover and Relabel: Dataset Condensation at ImageNet Scale From A New Perspective. (arXiv:2306.13092v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13092
&lt;/p&gt;
&lt;p&gt;
SRe$^2$L&#26159;&#19968;&#31181;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#19981;&#21516;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#21644;&#22270;&#20687;&#20998;&#36776;&#29575;&#65292;&#20855;&#26377;&#20302;&#35757;&#32451;&#25104;&#26412;&#21644;&#20869;&#23384;&#28040;&#32791;&#20197;&#21450;&#25193;&#23637;&#21040;&#20219;&#24847;&#35780;&#20272;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#30340;&#33021;&#21147;&#65292;&#22312;Tiny-ImageNet&#21644;ImageNet-1K&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20339;&#24615;&#33021;&#65292;&#24182;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21387;&#32553;&#26694;&#26550;&#65292;&#31216;&#20026;Squeeze&#12289;Recover&#21644;Relabel&#65288;SRe$^2$L&#65289;&#65292;&#23427;&#22312;&#35757;&#32451;&#26399;&#38388;&#20998;&#31163;&#20102;&#27169;&#22411;&#21644;&#21512;&#25104;&#25968;&#25454;&#30340;&#21452;&#23618;&#20248;&#21270;&#65292;&#20197;&#22788;&#29702;&#19981;&#21516;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#21644;&#22270;&#20687;&#20998;&#36776;&#29575;&#65292;&#20174;&#32780;&#23454;&#29616;&#26377;&#25928;&#30340;&#25968;&#25454;&#38598;&#21387;&#32553;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#35268;&#27169;&#19978;&#30340;&#28789;&#27963;&#24615;&#65292;&#24182;&#22312;&#21512;&#25104;&#22270;&#20687;&#20219;&#24847;&#20998;&#36776;&#29575;&#12289;&#39640;&#20998;&#36776;&#29575;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#20302;&#35757;&#32451;&#25104;&#26412;&#21644;&#20869;&#23384;&#28040;&#32791;&#20197;&#21450;&#25193;&#23637;&#21040;&#20219;&#24847;&#35780;&#20272;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;Tiny-ImageNet&#21644;&#23436;&#25972;&#30340;ImageNet-1K&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;&#22312;50IPC&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Tiny-ImageNet&#21644;ImageNet-1K&#19978;&#20998;&#21035;&#23454;&#29616;&#20102;42.5&#65285;&#21644;60.8&#65285;&#30340;&#26368;&#39640;&#39564;&#35777;&#31934;&#24230;&#65292;&#36739;&#20043;&#21069;&#25152;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#25552;&#39640;&#20102;14.5&#65285;&#21644;32.9&#65285;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Res&#19978;&#20063;&#27604;MTT&#24555;&#32422;52&#20493;(ConvNet-4)&#21644;16&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new dataset condensation framework termed Squeeze, Recover and Relabel (SRe$^2$L) that decouples the bilevel optimization of model and synthetic data during training, to handle varying scales of datasets, model architectures and image resolutions for effective dataset condensation. The proposed method demonstrates flexibility across diverse dataset scales and exhibits multiple advantages in terms of arbitrary resolutions of synthesized images, low training cost and memory consumption with high-resolution training, and the ability to scale up to arbitrary evaluation network architectures. Extensive experiments are conducted on Tiny-ImageNet and full ImageNet-1K datasets. Under 50 IPC, our approach achieves the highest 42.5% and 60.8% validation accuracy on Tiny-ImageNet and ImageNet-1K, outperforming all previous state-of-the-art methods by margins of 14.5% and 32.9%, respectively. Our approach also outperforms MTT by approximately 52$\times$ (ConvNet-4) and 16$\times$ (Res
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#36866;&#29992;&#20110;&#20154;&#24037;&#20915;&#31574;&#36741;&#21161;&#30340;&#26234;&#33021;&#24178;&#39044;&#26041;&#26696;&#65292;&#22312;&#21516;&#26102;&#32771;&#34385;&#20934;&#30830;&#24615;&#21644;&#26102;&#38388;&#24615;&#30340;&#21069;&#25552;&#19979;&#65292;&#26681;&#25454;&#38382;&#39064;&#21644;&#29992;&#25143;&#30340;&#23646;&#24615;&#33258;&#36866;&#24212;&#22320;&#23637;&#31034;AI&#36741;&#21161;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.07458</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#20154;&#24037;&#20915;&#31574;&#36741;&#21161;&#30340;&#26234;&#33021;&#24178;&#39044;&#26041;&#26696;&#65306;&#26082;&#32771;&#34385;&#20934;&#30830;&#24615;&#21448;&#20860;&#39038;&#26102;&#38388;&#24615;
&lt;/p&gt;
&lt;p&gt;
Adaptive interventions for both accuracy and time in AI-assisted human decision making. (arXiv:2306.07458v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#36866;&#29992;&#20110;&#20154;&#24037;&#20915;&#31574;&#36741;&#21161;&#30340;&#26234;&#33021;&#24178;&#39044;&#26041;&#26696;&#65292;&#22312;&#21516;&#26102;&#32771;&#34385;&#20934;&#30830;&#24615;&#21644;&#26102;&#38388;&#24615;&#30340;&#21069;&#25552;&#19979;&#65292;&#26681;&#25454;&#38382;&#39064;&#21644;&#29992;&#25143;&#30340;&#23646;&#24615;&#33258;&#36866;&#24212;&#22320;&#23637;&#31034;AI&#36741;&#21161;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38656;&#35201;&#39640;&#20934;&#30830;&#24615;&#20294;&#21516;&#26102;&#26102;&#38388;&#21448;&#32039;&#36843;&#30340;&#29615;&#22659;&#19979;&#65292;&#20363;&#22914;&#22312;&#24613;&#35786;&#23460;&#24037;&#20316;&#30340;&#21307;&#29983;&#65292;&#25105;&#20204;&#24076;&#26395;&#25552;&#20379;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#65292;&#26082;&#33021;&#25552;&#39640;&#20934;&#30830;&#24615;&#21448;&#33021;&#20943;&#23569;&#26102;&#38388;&#12290;&#20294;&#26159;&#65292;&#19981;&#21516;&#31867;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#21151;&#33021;&#24102;&#26469;&#30340;&#22909;&#22788;&#26159;&#19981;&#21516;&#30340;&#65306;&#19968;&#20123;&#33021;&#22815;&#20943;&#23569;&#26102;&#38388;&#65292;&#20294;&#20250;&#22686;&#21152;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#36807;&#24230;&#20381;&#36182;&#65292;&#32780;&#20854;&#20182;&#19968;&#20123;&#21017;&#30456;&#21453;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24076;&#26395;&#26681;&#25454;&#38382;&#39064;&#21644;&#29992;&#25143;&#30340;&#21508;&#31181;&#23646;&#24615;&#65288;&#22914;&#30693;&#35782;&#27700;&#24179;&#65289;&#26469;&#33258;&#36866;&#24212;&#22320;&#23637;&#31034;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#65292;&#20197;&#20415;&#22312;&#20934;&#30830;&#24615;&#21644;&#26102;&#38388;&#24615;&#20043;&#38388;&#20570;&#20986;&#26368;&#20339;&#26435;&#34913;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#29992;&#25143;&#38656;&#35201;&#20026;&#22806;&#26143;&#20154;&#24320;&#33647;&#26041;&#30340;&#30740;&#31350;&#26469;&#25506;&#32034;&#33258;&#36866;&#24212;AI&#36741;&#21161;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#26681;&#25454;&#38382;&#39064;&#33258;&#36866;&#24212;AI&#36741;&#21161;&#26159;&#26377;&#30410;&#30340;&#65292;&#21487;&#20197;&#36798;&#21040;&#26102;&#38388;&#21644;&#20934;&#30830;&#24615;&#30340;&#33391;&#22909;&#24179;&#34913;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#23558;&#32771;&#34385;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65288;&#22914;&#24378;&#21270;&#23398;&#20064;&#65289;&#26469;&#23454;&#29616;&#24555;&#36895;&#33258;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
In settings where users are both time-pressured and need high accuracy, such as doctors working in Emergency Rooms, we want to provide AI assistance that both increases accuracy and reduces time. However, different types of AI assistance have different benefits: some reduce time taken while increasing overreliance on AI, while others do the opposite. We therefore want to adapt what AI assistance we show depending on various properties (of the question and of the user) in order to best tradeoff our two objectives. We introduce a study where users have to prescribe medicines to aliens, and use it to explore the potential for adapting AI assistance. We find evidence that it is beneficial to adapt our AI assistance depending on the question, leading to good tradeoffs between time taken and accuracy. Future work would consider machine-learning algorithms (such as reinforcement learning) to automatically adapt quickly.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;FedMLSecurity&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#23427;&#21487;&#20197;&#27169;&#25311;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#23545;&#25239;&#25915;&#20987;&#24182;&#25552;&#20379;&#30456;&#24212;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;&#35813;&#27979;&#35797;&#23545;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#32852;&#21512;&#20248;&#21270;&#22120;&#37117;&#21487;&#20197;&#36866;&#29992;&#65292;&#24182;&#19988;&#33021;&#22815;&#36731;&#26494;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;</title><link>http://arxiv.org/abs/2306.04959</link><description>&lt;p&gt;
FedMLSecurity&#65306;&#32852;&#37030;&#23398;&#20064;&#19982;LLMs&#20013;&#25915;&#20987;&#19982;&#38450;&#24481;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
FedMLSecurity: A Benchmark for Attacks and Defenses in Federated Learning and LLMs. (arXiv:2306.04959v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04959
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;FedMLSecurity&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#23427;&#21487;&#20197;&#27169;&#25311;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#23545;&#25239;&#25915;&#20987;&#24182;&#25552;&#20379;&#30456;&#24212;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;&#35813;&#27979;&#35797;&#23545;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#32852;&#21512;&#20248;&#21270;&#22120;&#37117;&#21487;&#20197;&#36866;&#29992;&#65292;&#24182;&#19988;&#33021;&#22815;&#36731;&#26494;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;FedMLSecurity&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#27169;&#25311;&#23545;&#25239;&#25915;&#20987;&#21644;&#30456;&#24212;&#38450;&#24481;&#26426;&#21046;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#20316;&#20026;&#24320;&#28304;&#24211;FedML&#30340;&#19968;&#20010;&#37325;&#35201;&#27169;&#22359;&#65292;FedMLSecurity&#22686;&#24378;&#20102;FedML&#30340;&#23433;&#20840;&#35780;&#20272;&#33021;&#21147;&#12290;FedMLSecurity&#21253;&#21547;&#20004;&#20010;&#20027;&#35201;&#32452;&#20214;&#65306;FedMLAttacker&#27169;&#25311;&#22312;FL&#35757;&#32451;&#20013;&#27880;&#20837;&#30340;&#25915;&#20987;&#65292;&#32780;FedMLDefender&#21017;&#27169;&#25311;&#26088;&#22312;&#20943;&#36731;&#25915;&#20987;&#24433;&#21709;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;FedMLSecurity&#26159;&#24320;&#28304;&#30340;&#65292;&#21487;&#36866;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65288;&#20363;&#22914;&#36923;&#36753;&#22238;&#24402;&#65292;ResNet&#65292;GAN&#31561;&#65289;&#21644;&#32852;&#21512;&#20248;&#21270;&#22120;&#65288;&#20363;&#22914;FedAVG&#65292;FedOPT&#65292;FedNOVA&#31561;&#65289;&#12290;&#26412;&#25991;&#30340;&#23454;&#39564;&#35780;&#20272;&#36824;&#23637;&#31034;&#20102;&#23558;FedMLSecurity&#36731;&#26494;&#24212;&#29992;&#20110;LLMs&#30340;&#20415;&#21033;&#24615;&#65292;&#36827;&#19968;&#27493;&#24378;&#21270;&#20102;&#20854;&#21508;&#31181;&#22330;&#26223;&#19979;&#30340;&#36890;&#29992;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces FedMLSecurity, a benchmark that simulates adversarial attacks and corresponding defense mechanisms in Federated Learning (FL). As an integral module of the open-sourced library FedML that facilitates FL algorithm development and performance comparison, FedMLSecurity enhances the security assessment capacity of FedML. FedMLSecurity comprises two principal components: FedMLAttacker, which simulates attacks injected into FL training, and FedMLDefender, which emulates defensive strategies designed to mitigate the impacts of the attacks. FedMLSecurity is open-sourced 1 and is customizable to a wide range of machine learning models (e.g., Logistic Regression, ResNet, GAN, etc.) and federated optimizers (e.g., FedAVG, FedOPT, FedNOVA, etc.). Experimental evaluations in this paper also demonstrate the ease of application of FedMLSecurity to Large Language Models (LLMs), further reinforcing its versatility and practical utility in various scenarios.
&lt;/p&gt;</description></item><item><title>MESSY&#20272;&#35745;&#26041;&#27861;&#26159;&#19968;&#31181;&#22522;&#20110;&#26368;&#22823;&#29109;&#30340;&#38543;&#26426;&#21644;&#31526;&#21495;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#26799;&#24230;&#30340;&#28418;&#31227;&#25193;&#25955;&#36807;&#31243;&#26469;&#39640;&#25928;&#22320;&#25214;&#21040;&#26368;&#22823;&#29109;&#20998;&#24067;&#30340;&#21442;&#25968;&#65292;&#25903;&#25345;&#39640;&#32500;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#20248;&#20110;&#29616;&#26377;&#26368;&#26032;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04120</link><description>&lt;p&gt;
MESSY&#20272;&#35745;&#65306;&#22522;&#20110;&#26368;&#22823;&#29109;&#30340;&#38543;&#26426;&#21644;&#31526;&#21495;&#23494;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
MESSY Estimation: Maximum-Entropy based Stochastic and Symbolic densitY Estimation. (arXiv:2306.04120v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04120
&lt;/p&gt;
&lt;p&gt;
MESSY&#20272;&#35745;&#26041;&#27861;&#26159;&#19968;&#31181;&#22522;&#20110;&#26368;&#22823;&#29109;&#30340;&#38543;&#26426;&#21644;&#31526;&#21495;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#26799;&#24230;&#30340;&#28418;&#31227;&#25193;&#25955;&#36807;&#31243;&#26469;&#39640;&#25928;&#22320;&#25214;&#21040;&#26368;&#22823;&#29109;&#20998;&#24067;&#30340;&#21442;&#25968;&#65292;&#25903;&#25345;&#39640;&#32500;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#20248;&#20110;&#29616;&#26377;&#26368;&#26032;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#26368;&#22823;&#29109;&#30340;&#38543;&#26426;&#21644;&#31526;&#21495;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;MESSY&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#26799;&#24230;&#27969;&#30340;&#30697;&#23558;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#20174;&#26679;&#26412;&#20013;&#24674;&#22797;&#20026;&#31526;&#21495;&#34920;&#36798;&#24335;&#65292;&#24182;&#23558;ansatz&#20316;&#20026;&#39537;&#21160;&#21147;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#26799;&#24230;&#30340;&#28418;&#31227;&#25193;&#25955;&#36807;&#31243;&#65292;&#23558;&#26410;&#30693;&#20998;&#24067;&#20989;&#25968;&#30340;&#26679;&#26412;&#19982;&#29468;&#27979;&#30340;&#31526;&#21495;&#34920;&#36798;&#24335;&#30456;&#36830;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20986;&#24403;&#29468;&#27979;&#20998;&#24067;&#20855;&#26377;&#26368;&#22823;&#29109;&#24418;&#24335;&#26102;&#65292;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#25552;&#20379;&#30340;&#26679;&#26412;&#30340;&#30697;&#26500;&#24314;&#30340;&#32447;&#24615;&#26041;&#31243;&#32452;&#39640;&#25928;&#22320;&#25214;&#21040;&#35813;&#20998;&#24067;&#30340;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#31526;&#21495;&#22238;&#24402;&#26469;&#25506;&#32034;&#24179;&#28369;&#20989;&#25968;&#30340;&#31354;&#38388;&#65292;&#24182;&#25214;&#21040;&#23548;&#33268;&#26368;&#22823;&#29109;&#27867;&#20989;&#25351;&#25968;&#30340;&#26368;&#20248;&#22522;&#20989;&#25968;&#65292;&#20197;&#33719;&#24471;&#33391;&#22909;&#26465;&#20214;&#12290;&#35813;&#26041;&#27861;&#22312;&#38543;&#26426;&#25628;&#32034;&#30340;&#27599;&#27425;&#36845;&#20195;&#20013;&#30340;&#25104;&#26412;&#19982;&#26679;&#26412;&#25968;&#37327;&#21576;&#32447;&#24615;&#20851;&#31995;&#65292;&#19982;&#21464;&#37327;&#25968;&#37327;&#21576;&#20108;&#27425;&#20851;&#31995;&#65292;&#20351;&#20854;&#21487;&#25193;&#23637;&#21040;&#39640;&#32500;&#38382;&#39064;&#12290;&#25968;&#20540;&#23454;&#39564;&#26174;&#31034;&#20986;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#26222;&#36866;&#24615;&#65292;&#19982;&#29616;&#26377;&#30340;&#26368;&#26032;&#26041;&#27861;&#30456;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce MESSY estimation, a Maximum-Entropy based Stochastic and Symbolic densitY estimation method. The proposed approach recovers probability density functions symbolically from samples using moments of a Gradient flow in which the ansatz serves as the driving force. In particular, we construct a gradient-based drift-diffusion process that connects samples of the unknown distribution function to a guess symbolic expression. We then show that when the guess distribution has the maximum entropy form, the parameters of this distribution can be found efficiently by solving a linear system of equations constructed using the moments of the provided samples. Furthermore, we use Symbolic regression to explore the space of smooth functions and find optimal basis functions for the exponent of the maximum entropy functional leading to good conditioning. The cost of the proposed method in each iteration of the random search is linear with the number of samples and quadratic with the number 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24212;&#29992;&#27963;&#21160;&#29702;&#35770;&#20998;&#26512;&#20102;&#39321;&#28207;&#20013;&#23398;&#29983;&#22312;&#30701;&#31687;&#25925;&#20107;&#21019;&#20316;&#20013;&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#30340;&#26041;&#24335;&#21644;&#30446;&#30340;&#65292;&#21457;&#29616;&#20854;&#20013;&#32570;&#20047;&#30446;&#30340;&#24847;&#35782;&#12289;&#20811;&#26381;&#21019;&#20316;&#38556;&#30861;&#20197;&#21450;&#21457;&#23637;&#12289;&#25193;&#23637;&#21644;&#25913;&#36827;&#25925;&#20107;&#20026;&#20027;&#35201;&#30446;&#30340;&#12290;&#21516;&#26102;&#65292;&#23398;&#29983;&#27963;&#21160;&#31995;&#32479;&#30340;&#20849;&#21516;&#29305;&#24449;&#20063;&#34987;&#30740;&#31350;&#30830;&#23450;&#12290;</title><link>http://arxiv.org/abs/2306.01798</link><description>&lt;p&gt;
&#20197;&#27963;&#21160;&#29702;&#35770;&#35270;&#35282;&#25506;&#31350;&#22806;&#35821;&#33521;&#35821;&#23398;&#20064;&#32773;&#22312;&#20154;&#24037;&#26234;&#33021;&#20889;&#20316;&#20013;&#30340;&#25552;&#31034;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
Exploring EFL students' prompt engineering in human-AI story writing: an Activity Theory perspective. (arXiv:2306.01798v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24212;&#29992;&#27963;&#21160;&#29702;&#35770;&#20998;&#26512;&#20102;&#39321;&#28207;&#20013;&#23398;&#29983;&#22312;&#30701;&#31687;&#25925;&#20107;&#21019;&#20316;&#20013;&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#30340;&#26041;&#24335;&#21644;&#30446;&#30340;&#65292;&#21457;&#29616;&#20854;&#20013;&#32570;&#20047;&#30446;&#30340;&#24847;&#35782;&#12289;&#20811;&#26381;&#21019;&#20316;&#38556;&#30861;&#20197;&#21450;&#21457;&#23637;&#12289;&#25193;&#23637;&#21644;&#25913;&#36827;&#25925;&#20107;&#20026;&#20027;&#35201;&#30446;&#30340;&#12290;&#21516;&#26102;&#65292;&#23398;&#29983;&#27963;&#21160;&#31995;&#32479;&#30340;&#20849;&#21516;&#29305;&#24449;&#20063;&#34987;&#30740;&#31350;&#30830;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24212;&#29992;&#27963;&#21160;&#29702;&#35770;&#25506;&#31350;&#20102;&#39321;&#28207;&#20013;&#23398;&#29983;&#22312;&#30701;&#31687;&#25925;&#20107;&#21019;&#20316;&#20013;&#22914;&#20309;&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#12290;&#30740;&#31350;&#25910;&#38598;&#24182;&#20998;&#26512;&#20102;&#23398;&#29983;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#12289;&#30701;&#31687;&#25925;&#20107;&#21644;&#26377;&#20851;&#25552;&#31034;&#30446;&#30340;&#30340;&#20070;&#38754;&#21453;&#24605;&#12290;&#30740;&#31350;&#30830;&#23450;&#20102;&#23398;&#29983;&#25552;&#31034;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#30340;&#19977;&#20010;&#20027;&#35201;&#30446;&#30340;&#65306;&#32570;&#20047;&#30446;&#30340;&#24847;&#35782;&#12289;&#20811;&#26381;&#21019;&#20316;&#38556;&#30861;&#20197;&#21450;&#21457;&#23637;&#12289;&#25193;&#23637;&#21644;&#25913;&#36827;&#25925;&#20107;&#12290;&#30740;&#31350;&#36824;&#30830;&#23450;&#20102;&#23398;&#29983;&#27963;&#21160;&#31995;&#32479;&#30340;&#20849;&#21516;&#29305;&#24449;&#65292;&#21253;&#25324;&#20182;&#20204;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#30340;&#22797;&#26434;&#24615;&#12289;&#25925;&#20107;&#30340;&#36136;&#37327;&#20197;&#21450;&#25152;&#22312;&#23398;&#26657;&#30340;&#25972;&#20307;&#23398;&#26415;&#25104;&#23601;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study applies Activity Theory to investigate how English as a foreign language (EFL) students prompt generative artificial intelligence (AI) tools during short story writing. Sixty-seven Hong Kong secondary school students created generative-AI tools using open-source language models and wrote short stories with them. The study collected and analyzed the students' generative-AI tools, short stories, and written reflections on their conditions or purposes for prompting. The research identified three main themes regarding the purposes for which students prompt generative-AI tools during short story writing: a lack of awareness of purposes, overcoming writer's block, and developing, expanding, and improving the story. The study also identified common characteristics of students' activity systems, including the sophistication of their generative-AI tools, the quality of their stories, and their school's overall academic achievement level, for their prompting of generative-AI tools for
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#29992;&#20110;&#21457;&#29616;&#19982;CLTA4&#36890;&#36335;&#30456;&#20851;&#30340;&#26032;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#24182;&#19988;&#35813;&#27169;&#22411;&#32463;&#27982;&#23454;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.01745</link><description>&lt;p&gt;
&#21033;&#29992;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#21457;&#29616;&#65306;&#20197;CTLA4&#28608;&#27963;&#36890;&#36335;&#30340;&#26696;&#20363;&#30740;&#31350;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Biomarker Discovery with Quantum Neural Networks: A Case-study in CTLA4-Activation Pathways. (arXiv:2306.01745v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01745
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#29992;&#20110;&#21457;&#29616;&#19982;CLTA4&#36890;&#36335;&#30456;&#20851;&#30340;&#26032;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#24182;&#19988;&#35813;&#27169;&#22411;&#32463;&#27982;&#23454;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#21457;&#29616;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#30001;&#20110;&#25628;&#32034;&#31354;&#38388;&#24222;&#22823;&#12290;&#37327;&#23376;&#35745;&#31639;&#21644;&#37327;&#23376;&#20154;&#24037;&#26234;&#33021;&#65288;&#37327;&#23376;AI&#65289;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#29983;&#29289;&#26631;&#24535;&#29289;&#21457;&#29616;&#20219;&#21153;&#30340;&#35745;&#31639;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;QNNs&#65289;&#26550;&#26500;&#65292;&#29992;&#20110;&#21457;&#29616;&#36755;&#20837;&#28608;&#27963;&#36890;&#36335;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#12290;&#26368;&#22823;&#30456;&#20851;&#24615;&#65292;&#26368;&#23567;&#20887;&#20313;&#65288;mRMR&#65289;&#26631;&#20934;&#29992;&#20110;&#35780;&#20998;&#29983;&#29289;&#26631;&#24535;&#29289;&#20505;&#36873;&#38598;&#12290;&#30001;&#20110;&#31070;&#32463;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#22312;&#21463;&#38480;&#30828;&#20214;&#19978;&#20132;&#20184;&#65292;&#25152;&#20197;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#26159;&#32463;&#27982;&#30340;&#12290;&#25105;&#20204;&#22312;&#19982;CTLA4&#30456;&#20851;&#30340;&#22235;&#26465;&#28608;&#27963;&#36890;&#36335;&#19978;&#23637;&#31034;&#20102;&#35777;&#26126;&#27010;&#24565;&#65292;&#21253;&#25324;&#65288;1&#65289;CTLA4&#28608;&#27963;&#29420;&#31435;&#65292;&#65288;2&#65289;CTLA4-CD8A-CD8B&#20849;&#21516;&#28608;&#27963;&#65292;&#65288;3&#65289;CTLA4-CD2&#20849;&#21516;&#28608;&#27963;&#65292;&#20197;&#21450;&#65288;4&#65289;CTLA4-CD2-CD48-CD53-CD58-CD84&#20849;&#21516;&#28608;&#27963;&#12290;&#35813;&#27169;&#22411;&#25351;&#20986;&#19982;CLTA4&#30456;&#20851;&#36890;&#36335;&#30340;&#31361;&#21464;&#28608;&#27963;&#26377;&#26032;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#21253;&#25324;20&#20010;&#22522;&#22240;&#65306;CLIC4&#65292;CPE&#65292;ETS2&#65292;FAM107A&#65292;GPR116&#65292;HYOU1&#65292;LCN2&#65292;MACF1&#65292;MT1G&#65292;NAPA&#65292;NDUFS5&#65292;PAK1&#65292;PFN1&#65292;PGAP
&lt;/p&gt;
&lt;p&gt;
Biomarker discovery is a challenging task due to the massive search space. Quantum computing and quantum Artificial Intelligence (quantum AI) can be used to address the computational problem of biomarker discovery tasks. We propose a Quantum Neural Networks (QNNs) architecture to discover biomarkers for input activation pathways. The Maximum Relevance, Minimum Redundancy (mRMR) criteria is used to score biomarker candidate sets. Our proposed model is economical since the neural solution can be delivered on constrained hardware. We demonstrate the proof of concept on four activation pathways associated with CTLA4, including (1) CTLA4-activation stand-alone, (2) CTLA4-CD8A-CD8B co-activation, (3) CTLA4-CD2 co-activation, and (4) CTLA4-CD2-CD48-CD53-CD58-CD84 co-activation. The model indicates new biomarkers associated with the mutational activation of CLTA4-associated pathways, including 20 genes: CLIC4, CPE, ETS2, FAM107A, GPR116, HYOU1, LCN2, MACF1, MT1G, NAPA, NDUFS5, PAK1, PFN1, PGAP
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#32447;&#31574;&#30053;&#65292;&#36890;&#36807;&#20998;&#24067;&#24335;&#30340;&#21327;&#35843;&#31574;&#30053;&#35299;&#20915;&#20102;&#26410;&#24314;&#22270;&#29615;&#22659;&#19979;&#22810;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.15596</link><description>&lt;p&gt;
&#26410;&#24314;&#22270;&#29615;&#22659;&#19979;&#22810;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#30340;&#20998;&#24067;&#24335;&#22312;&#32447;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Distributed Online Rollout for Multivehicle Routing in Unmapped Environments. (arXiv:2305.15596v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15596
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#32447;&#31574;&#30053;&#65292;&#36890;&#36807;&#20998;&#24067;&#24335;&#30340;&#21327;&#35843;&#31574;&#30053;&#35299;&#20915;&#20102;&#26410;&#24314;&#22270;&#29615;&#22659;&#19979;&#22810;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20010;&#24191;&#27867;&#21270;&#30340;&#22810;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65306;&#32473;&#23450;&#19968;&#20010;&#32593;&#32476;&#12289;&#19968;&#32452;&#21344;&#25454;&#32593;&#32476;&#33410;&#28857;&#23376;&#38598;&#30340;&#26234;&#33021;&#20307;&#21644;&#19968;&#32452;&#20219;&#21153;, &#25105;&#20204;&#23547;&#27714;&#19968;&#20010;&#26368;&#23567;&#25104;&#26412;&#30340;&#31227;&#21160;&#24207;&#21015;&#65292;&#20197;&#28385;&#36275;&#27599;&#20010;&#20219;&#21153;&#33267;&#23569;&#34987;&#19968;&#20010;&#26234;&#33021;&#20307;&#35775;&#38382;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#19982;&#32463;&#20856;&#38382;&#39064;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#20551;&#23450;&#27809;&#26377;&#20013;&#22830;&#26381;&#21153;&#22120;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#37117;&#26159;&#19968;&#20010;&#20010;&#20307;&#22788;&#29702;&#22120;&#65292;&#27809;&#26377;&#20851;&#20110;&#22522;&#30784;&#32593;&#32476;&#65288;&#21253;&#25324;&#20219;&#21153;&#21644;&#26234;&#33021;&#20307;&#20301;&#32622;&#65289;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26234;&#33021;&#20307;&#20855;&#26377;&#20005;&#26684;&#30340;&#26412;&#22320;&#36890;&#20449;&#21644;&#24863;&#30693;&#33021;&#21147;&#65288;&#38480;&#21046;&#22312;&#23427;&#20204;&#21508;&#33258;&#20301;&#32622;&#21608;&#22260;&#30340;&#21322;&#24452;&#33539;&#22260;&#20869;&#65289;&#65292;&#26356;&#25509;&#36817;&#22810;&#23454;&#38469;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#31574;&#30053;&#26041;&#27861;&#65292;&#20854;&#20013;&#26234;&#33021;&#20307;&#26681;&#25454;&#38598;&#20013;&#24335;&#27169;&#25311;&#22120;&#35757;&#32451;&#30340;&#23398;&#20064;&#22411;&#31574;&#30053;&#23616;&#37096;&#35268;&#21010;&#20854;&#36712;&#36857;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;&#23427;&#22312;&#32593;&#32476;&#25299;&#25169;&#21644;&#20219;&#21153;&#20998;&#24067;&#30340;&#21464;&#21270;&#19979;&#20173;&#28982;&#33021;&#22815;&#23454;&#29616;&#36817;&#26368;&#20248;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we consider a generalization of the well-known multivehicle routing problem: given a network, a set of agents occupying a subset of its nodes, and a set of tasks, we seek a minimum cost sequence of movements subject to the constraint that each task is visited by some agent at least once. The classical version of this problem assumes a central computational server that observes the entire state of the system perfectly and directs individual agents according to a centralized control scheme. In contrast, we assume that there is no centralized server and that each agent is an individual processor with no a priori knowledge of the underlying network (including task and agent locations). Moreover, our agents possess strictly local communication and sensing capabilities (restricted to a fixed radius around their respective locations), aligning more closely with several real-world multiagent applications. These restrictions introduce many challenges that are overcome through local
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#35843;&#24230;&#25513;&#30721;&#29575;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;MLM&#39044;&#35757;&#32451;&#30340;&#36136;&#37327;&#65292;&#36890;&#36807;&#32447;&#24615;&#38477;&#20302;&#25513;&#30721;&#29575;&#65292;&#36798;&#21040;&#20102;&#23545;BERT-base&#21644;BERT-large&#27169;&#22411;&#20998;&#21035;&#25552;&#39640;0.46%&#21644;0.25%&#30340;&#24179;&#22343;GLUE&#20934;&#30830;&#29575;&#30340;&#25928;&#26524;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#21152;&#24555;&#20102;BERT-base&#30340;&#39044;&#35757;&#32451;&#36895;&#24230;&#65292;&#36824;&#23454;&#29616;&#20102;&#23545;BERT-large&#30340;&#24085;&#32047;&#25176;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2305.15096</link><description>&lt;p&gt;
MLM&#39044;&#35757;&#32451;&#30340;&#21160;&#24577;&#25513;&#30721;&#29575;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Dynamic Masking Rate Schedules for MLM Pretraining. (arXiv:2305.15096v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#35843;&#24230;&#25513;&#30721;&#29575;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;MLM&#39044;&#35757;&#32451;&#30340;&#36136;&#37327;&#65292;&#36890;&#36807;&#32447;&#24615;&#38477;&#20302;&#25513;&#30721;&#29575;&#65292;&#36798;&#21040;&#20102;&#23545;BERT-base&#21644;BERT-large&#27169;&#22411;&#20998;&#21035;&#25552;&#39640;0.46%&#21644;0.25%&#30340;&#24179;&#22343;GLUE&#20934;&#30830;&#29575;&#30340;&#25928;&#26524;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#21152;&#24555;&#20102;BERT-base&#30340;&#39044;&#35757;&#32451;&#36895;&#24230;&#65292;&#36824;&#23454;&#29616;&#20102;&#23545;BERT-large&#30340;&#24085;&#32047;&#25176;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#20351;&#29992;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#30340;transformer&#27169;&#22411;&#20351;&#29992;&#20102;&#21407;&#22987;BERT&#27169;&#22411;&#30340;&#22266;&#23450;&#25513;&#30721;&#29575;15%&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#35757;&#32451;&#36807;&#31243;&#20013;&#21160;&#24577;&#35843;&#25972;&#25513;&#30721;&#29575;&#26469;&#26367;&#20195;&#22266;&#23450;&#29575;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#32447;&#24615;&#38477;&#20302;&#25513;&#30721;&#29575;&#21487;&#20197;&#27604;&#22266;&#23450;&#29575;&#22522;&#20934;&#20998;&#21035;&#25552;&#39640;BERT-base&#21644;BERT-large&#30340;&#24179;&#22343;GLUE&#20934;&#30830;&#29575;0.46%&#21644;0.25%&#12290;&#36825;&#20123;&#25552;&#21319;&#26469;&#33258;&#20110;&#25509;&#35302;&#39640;&#21644;&#20302;&#25513;&#30721;&#29575;&#30340;&#26426;&#21046;&#65292;&#20174;&#32780;&#22312;&#20004;&#31181;&#35774;&#32622;&#20013;&#37117;&#24102;&#26469;&#20102;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25513;&#30721;&#29575;&#35843;&#24230;&#26159;&#25552;&#39640;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#36136;&#37327;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;BERT-base&#30340;&#39044;&#35757;&#32451;&#36895;&#24230;&#25552;&#39640;1.89&#20493;&#65292;&#24182;&#23545;BERT-large&#23454;&#29616;&#20102;&#24085;&#32047;&#25176;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most works on transformers trained with the Masked Language Modeling (MLM) objective use the original BERT model's fixed masking rate of 15%. We propose to instead dynamically schedule the masking rate throughout training. We find that linearly decreasing the masking rate over the course of pretraining improves average GLUE accuracy by up to 0.46% and 0.25% in BERT-base and BERT-large, respectively, compared to fixed rate baselines. These gains come from exposure to both high and low masking rate regimes, providing benefits from both settings. Our results demonstrate that masking rate scheduling is a simple way to improve the quality of masked language models, achieving up to a 1.89x speedup in pretraining for BERT-base as well as a Pareto improvement for BERT-large.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39118;&#26684;&#30340;&#20010;&#24615;&#21270;&#21767;&#24418;&#21160;&#30011;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#29983;&#25104;&#20219;&#24847;&#36523;&#20221;&#30340;&#21767;&#24418;&#21516;&#27493;&#35270;&#39057;&#65292;&#19988;&#21487;&#29992;&#20110;&#22686;&#24378;&#26410;&#35265;&#38754;&#23380;&#30340;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2305.00521</link><description>&lt;p&gt;
StyleLipSync&#65306;&#22522;&#20110;&#39118;&#26684;&#30340;&#20010;&#24615;&#21270;&#21767;&#24418;&#21160;&#30011;&#35270;&#39057;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
StyleLipSync: Style-based Personalized Lip-sync Video Generation. (arXiv:2305.00521v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39118;&#26684;&#30340;&#20010;&#24615;&#21270;&#21767;&#24418;&#21160;&#30011;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#29983;&#25104;&#20219;&#24847;&#36523;&#20221;&#30340;&#21767;&#24418;&#21516;&#27493;&#35270;&#39057;&#65292;&#19988;&#21487;&#29992;&#20110;&#22686;&#24378;&#26410;&#35265;&#38754;&#23380;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;StyleLipSync&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#39118;&#26684;&#30340;&#20010;&#24615;&#21270;&#21767;&#24418;&#21160;&#30011;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#20174;&#20219;&#24847;&#38899;&#39057;&#29983;&#25104;&#26080;&#20851;&#36523;&#20221;&#30340;&#21767;&#24418;&#21516;&#27493;&#35270;&#39057;&#12290;&#20026;&#20102;&#29983;&#25104;&#20219;&#24847;&#36523;&#20221;&#30340;&#35270;&#39057;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#39044;&#22521;&#35757;&#30340;StyleGAN&#30340;&#35821;&#20041;&#20016;&#23500;&#28508;&#31354;&#38388;&#20013;&#30340;&#34920;&#36798;&#24615;&#21767;&#37096;&#20808;&#39564;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#32447;&#24615;&#21464;&#25442;&#35774;&#35745;&#35270;&#39057;&#19968;&#33268;&#24615;&#12290;&#19982;&#20197;&#24448;&#30340;&#21767;&#24418;&#21516;&#27493;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23039;&#24577;&#24863;&#30693;&#36974;&#32617;&#65292;&#36890;&#36807;&#36880;&#24103;&#21033;&#29992;&#19977;&#32500;&#21442;&#25968;&#21270;&#32593;&#26684;&#39044;&#27979;&#22120;&#21160;&#24577;&#23450;&#20301;&#36974;&#32617;&#65292;&#25552;&#39640;&#20102;&#24103;&#38388;&#33258;&#28982;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20960;&#20046;&#19981;&#38656;&#35201;&#25968;&#25454;&#30340;&#21767;&#24418;&#21516;&#27493;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21516;&#27493;&#27491;&#21017;&#21270;&#22120;&#26469;&#20445;&#30041;&#21767;&#24418;&#21516;&#27493;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#22686;&#24378;&#20154;&#29289;&#29305;&#23450;&#30340;&#35270;&#35273;&#20449;&#24687;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#20934;&#30830;&#30340;&#21767;&#24418;&#21516;&#27493;&#35270;&#39057;&#65292;&#29978;&#33267;&#21487;&#20197;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#22686;&#24378;&#26410;&#35265;&#38754;&#23380;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present StyleLipSync, a style-based personalized lip-sync video generative model that can generate identity-agnostic lip-synchronizing video from arbitrary audio. To generate a video of arbitrary identities, we leverage expressive lip prior from the semantically rich latent space of a pre-trained StyleGAN, where we can also design a video consistency with a linear transformation. In contrast to the previous lip-sync methods, we introduce pose-aware masking that dynamically locates the mask to improve the naturalness over frames by utilizing a 3D parametric mesh predictor frame by frame. Moreover, we propose a few-shot lip-sync adaptation method for an arbitrary person by introducing a sync regularizer that preserves lips-sync generalization while enhancing the person-specific visual information. Extensive experiments demonstrate that our model can generate accurate lip-sync videos even with the zero-shot setting and enhance characteristics of an unseen face using a fe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#24067;&#24335;&#34920;&#31034;&#36827;&#34892;&#21463;&#38480;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21487;&#20449;&#22235;&#26059;&#32764;&#26080;&#20154;&#26426;&#30340;&#36319;&#36394;&#25511;&#21046;&#12290;&#36890;&#36807;&#38598;&#25104;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#24178;&#25200;&#20272;&#35745;&#22120;&#21644;&#38543;&#26426;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22120;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#27668;&#21160;&#25928;&#24212;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#23454;&#29616;&#26368;&#20248;&#30340;&#20840;&#23616;&#25910;&#25947;&#36895;&#29575;&#21644;&#19968;&#23450;&#30340;&#20122;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2302.11694</link><description>&lt;p&gt;
&#21463;&#38480;&#24378;&#21270;&#23398;&#20064;&#22312;&#21487;&#20449;&#22235;&#26059;&#32764;&#26080;&#20154;&#26426;&#36319;&#36394;&#25511;&#21046;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Constrained Reinforcement Learning using Distributional Representation for Trustworthy Quadrotor UAV Tracking Control. (arXiv:2302.11694v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11694
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#24067;&#24335;&#34920;&#31034;&#36827;&#34892;&#21463;&#38480;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21487;&#20449;&#22235;&#26059;&#32764;&#26080;&#20154;&#26426;&#30340;&#36319;&#36394;&#25511;&#21046;&#12290;&#36890;&#36807;&#38598;&#25104;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#24178;&#25200;&#20272;&#35745;&#22120;&#21644;&#38543;&#26426;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22120;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#27668;&#21160;&#25928;&#24212;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#23454;&#29616;&#26368;&#20248;&#30340;&#20840;&#23616;&#25910;&#25947;&#36895;&#29575;&#21644;&#19968;&#23450;&#30340;&#20122;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22797;&#26434;&#30340;&#21160;&#24577;&#29615;&#22659;&#20013;&#65292;&#21516;&#26102;&#23454;&#29616;&#22235;&#26059;&#32764;&#26080;&#20154;&#26426;&#30340;&#20934;&#30830;&#21644;&#21487;&#38752;&#30340;&#36319;&#36394;&#25511;&#21046;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#30001;&#20110;&#26469;&#33258;&#27668;&#21160;&#21147;&#30340;&#38459;&#21147;&#21644;&#21147;&#30697;&#21464;&#21270;&#26159;&#28151;&#27788;&#30340;&#65292;&#24182;&#19988;&#38590;&#20197;&#31934;&#30830;&#35782;&#21035;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22235;&#26059;&#32764;&#36319;&#36394;&#31995;&#32479;&#23558;&#20854;&#35270;&#20026;&#20256;&#32479;&#25511;&#21046;&#26041;&#27861;&#20013;&#30340;&#31616;&#21333;&#8220;&#24178;&#25200;&#8221;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#21487;&#35299;&#37322;&#30340;&#36712;&#36857;&#36319;&#36394;&#22120;&#65292;&#23558;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#24178;&#25200;&#20272;&#35745;&#22120;&#19982;&#38543;&#26426;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22120;&#65288;SMPC&#65289;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#26410;&#30693;&#30340;&#27668;&#21160;&#25928;&#24212;&#12290;&#25152;&#25552;&#20986;&#30340;&#20272;&#35745;&#22120;&#8220;&#21463;&#38480;&#20998;&#24067;&#24335;&#24378;&#21270;&#24178;&#25200;&#20272;&#35745;&#22120;&#8221;&#65288;ConsDRED&#65289;&#20934;&#30830;&#22320;&#35782;&#21035;&#30495;&#23454;&#27668;&#21160;&#25928;&#24212;&#19982;&#20272;&#35745;&#20540;&#20043;&#38388;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#37319;&#29992;&#31616;&#21270;&#20223;&#23556;&#24178;&#25200;&#21453;&#39304;&#36827;&#34892;&#25511;&#21046;&#21442;&#25968;&#21270;&#65292;&#20197;&#20445;&#35777;&#20984;&#24615;&#65292;&#28982;&#21518;&#23558;&#20854;&#19982;SMPC&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#20445;&#35777;ConsDRED&#33267;&#23569;&#23454;&#29616;&#26368;&#20248;&#30340;&#20840;&#23616;&#25910;&#25947;&#36895;&#29575;&#21644;&#19968;&#23450;&#30340;&#20122;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simultaneously accurate and reliable tracking control for quadrotors in complex dynamic environments is challenging. As aerodynamics derived from drag forces and moment variations are chaotic and difficult to precisely identify, most current quadrotor tracking systems treat them as simple `disturbances' in conventional control approaches. We propose a novel, interpretable trajectory tracker integrating a Distributional Reinforcement Learning disturbance estimator for unknown aerodynamic effects with a Stochastic Model Predictive Controller (SMPC). The proposed estimator `Constrained Distributional Reinforced disturbance estimator' (ConsDRED) accurately identifies uncertainties between true and estimated values of aerodynamic effects. Simplified Affine Disturbance Feedback is used for control parameterization to guarantee convexity, which we then integrate with a SMPC. We theoretically guarantee that ConsDRED achieves at least an optimal global convergence rate and a certain sublinear r
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35843;&#26597;&#20102;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#30340;&#37325;&#35201;&#36827;&#23637;&#65292;&#21253;&#25324;&#21508;&#31181;&#26550;&#26500;&#12289;&#23618;&#12289;&#30446;&#26631;&#21644;&#20248;&#21270;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#20197;&#21450;&#20851;&#27880;&#26426;&#21046;&#12289;&#24402;&#19968;&#21270;&#12289;&#36339;&#36291;&#36830;&#25509;&#12289;Transformer&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#31561;&#26041;&#27861;&#30340;&#21464;&#20307;&#12290;&#24635;&#32467;&#20102;&#25104;&#21151;&#21019;&#26032;&#30340;&#20851;&#38190;&#31574;&#30053;&#65292;&#24182;&#35752;&#35770;&#20102;&#26368;&#36817;&#30340;&#21830;&#19994;&#38381;&#28304;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2302.00722</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#32508;&#36848;&#65306;&#20174;&#28608;&#27963;&#20989;&#25968;&#21040;Transformer
&lt;/p&gt;
&lt;p&gt;
A Survey of Deep Learning: From Activations to Transformers. (arXiv:2302.00722v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00722
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35843;&#26597;&#20102;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#30340;&#37325;&#35201;&#36827;&#23637;&#65292;&#21253;&#25324;&#21508;&#31181;&#26550;&#26500;&#12289;&#23618;&#12289;&#30446;&#26631;&#21644;&#20248;&#21270;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#20197;&#21450;&#20851;&#27880;&#26426;&#21046;&#12289;&#24402;&#19968;&#21270;&#12289;&#36339;&#36291;&#36830;&#25509;&#12289;Transformer&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#31561;&#26041;&#27861;&#30340;&#21464;&#20307;&#12290;&#24635;&#32467;&#20102;&#25104;&#21151;&#21019;&#26032;&#30340;&#20851;&#38190;&#31574;&#30053;&#65292;&#24182;&#35752;&#35770;&#20102;&#26368;&#36817;&#30340;&#21830;&#19994;&#38381;&#28304;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#24471;&#30410;&#20110;&#21508;&#31181;&#26550;&#26500;&#12289;&#23618;&#12289;&#30446;&#26631;&#21644;&#20248;&#21270;&#25216;&#26415;&#30340;&#28044;&#29616;&#12290;&#36825;&#20123;&#21253;&#25324;&#20851;&#27880;&#26426;&#21046;&#12289;&#24402;&#19968;&#21270;&#12289;&#36339;&#36291;&#36830;&#25509;&#12289;Transformer&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#31561;&#22810;&#31181;&#21464;&#20307;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21521;&#20855;&#26377;&#28145;&#24230;&#23398;&#20064;&#22522;&#26412;&#29702;&#35299;&#30340;&#20154;&#25552;&#20379;&#23545;&#36825;&#20123;&#39046;&#22495;&#20013;&#26368;&#26032;&#37325;&#35201;&#36129;&#29486;&#30340;&#20840;&#38754;&#35843;&#26597;&#12290;&#25105;&#20204;&#30340;&#26399;&#26395;&#26159;&#36890;&#36807;&#23545;&#37325;&#35201;&#26368;&#26032;&#20316;&#21697;&#30340;&#32508;&#21512;&#21644;&#20840;&#38754;&#30340;&#25506;&#35752;&#65292;&#20419;&#36827;&#19981;&#21516;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#20043;&#38388;&#24418;&#25104;&#26032;&#30340;&#32852;&#31995;&#12290;&#22312;&#25105;&#20204;&#30340;&#35752;&#35770;&#20013;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#36807;&#21435;&#21313;&#24180;&#20013;&#35768;&#22810;&#25104;&#21151;&#21019;&#26032;&#30340;&#20851;&#38190;&#31574;&#30053;&#12290;&#25105;&#20204;&#36824;&#23545;&#26368;&#36817;&#19968;&#20123;&#21830;&#19994;&#38381;&#28304;&#27169;&#22411;&#36827;&#34892;&#20102;&#35752;&#35770;&#65292;&#20363;&#22914;OpenAI&#30340;GPT-4&#21644;Google&#30340;PaLM 2&#12290;
&lt;/p&gt;
&lt;p&gt;
The past decade has witnessed remarkable advancements in deep learning, owing to the emergence of various architectures, layers, objectives, and optimization techniques. These consist of a multitude of variations of attention, normalization, skip connections, transformer, and self-supervised learning methods, among others. Our goal is to furnish a comprehensive survey of significant recent contributions in these domains to individuals with a fundamental grasp of deep learning. Our aspiration is that an integrated and comprehensive approach of influential recent works will facilitate the formation of new connections between different areas of deep learning. In our discussion, we discuss multiple patterns that summarize the key strategies for many of the successful innovations over the last decade. We also include a discussion on recent commercially built, closed-source models such as OpenAI's GPT-4 and Google's PaLM 2.
&lt;/p&gt;</description></item></channel></rss>