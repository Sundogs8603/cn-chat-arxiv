<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#30340;&#32508;&#21512;&#25351;&#21335;&#65292;&#21253;&#25324;&#29702;&#35770;&#22522;&#30784;&#12289;&#23454;&#38469;&#23454;&#29616;&#21644;&#27604;&#36739;&#32467;&#26524;&#65292;&#24182;&#23545;&#27491;&#21017;&#21270;&#30340;&#30410;&#22788;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2401.13662</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#31574;&#30053;&#26799;&#24230;&#30340;&#32456;&#26497;&#25351;&#21335;&#65306;&#29702;&#35770;&#12289;&#31639;&#27861;&#21644;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
The Definitive Guide to Policy Gradients in Deep Reinforcement Learning: Theory, Algorithms and Implementations. (arXiv:2401.13662v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13662
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#30340;&#32508;&#21512;&#25351;&#21335;&#65292;&#21253;&#25324;&#29702;&#35770;&#22522;&#30784;&#12289;&#23454;&#38469;&#23454;&#29616;&#21644;&#27604;&#36739;&#32467;&#26524;&#65292;&#24182;&#23545;&#27491;&#21017;&#21270;&#30340;&#30410;&#22788;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#25552;&#20986;&#20102;&#21508;&#31181;&#24378;&#22823;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#12290;&#34429;&#28982;&#25152;&#26377;&#36825;&#20123;&#31639;&#27861;&#37117;&#24314;&#31435;&#22312;&#31574;&#30053;&#26799;&#24230;&#23450;&#29702;&#30340;&#22522;&#30784;&#19978;&#65292;&#20294;&#20855;&#20307;&#30340;&#35774;&#35745;&#36873;&#25321;&#22312;&#31639;&#27861;&#20043;&#38388;&#26377;&#24456;&#22823;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#25972;&#20307;&#30340;&#35270;&#35282;&#26469;&#27010;&#36848;&#22312;&#32447;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#20197;&#20415;&#29702;&#35299;&#23427;&#20204;&#30340;&#29702;&#35770;&#22522;&#30784;&#21644;&#23454;&#38469;&#23454;&#29616;&#12290;&#22312;&#36825;&#20010;&#27010;&#36848;&#20013;&#65292;&#25105;&#20204;&#21253;&#25324;&#20102;&#36830;&#32493;&#29256;&#26412;&#30340;&#31574;&#30053;&#26799;&#24230;&#23450;&#29702;&#30340;&#35814;&#32454;&#35777;&#26126;&#12289;&#25910;&#25947;&#32467;&#26524;&#21644;&#23545;&#23454;&#38469;&#31639;&#27861;&#30340;&#20840;&#38754;&#35752;&#35770;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#36830;&#32493;&#25511;&#21046;&#29615;&#22659;&#20013;&#26368;&#37325;&#35201;&#30340;&#31639;&#27861;&#65292;&#24182;&#23545;&#27491;&#21017;&#21270;&#30340;&#30410;&#22788;&#25552;&#20379;&#20102;&#28145;&#20837;&#30340;&#35265;&#35299;&#12290;&#25152;&#26377;&#30340;&#20195;&#30721;&#37117;&#21487;&#20197;&#22312;https://github.com/Matt00n/PolicyGradientsJax&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, various powerful policy gradient algorithms have been proposed in deep reinforcement learning. While all these algorithms build on the Policy Gradient Theorem, the specific design choices differ significantly across algorithms. We provide a holistic overview of on-policy policy gradient algorithms to facilitate the understanding of both their theoretical foundations and their practical implementations. In this overview, we include a detailed proof of the continuous version of the Policy Gradient Theorem, convergence results and a comprehensive discussion of practical algorithms. We compare the most prominent algorithms on continuous control environments and provide insights on the benefits of regularization. All code is available at https://github.com/Matt00n/PolicyGradientsJax.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#22312;&#20020;&#24202;&#24212;&#29992;&#20013;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#21457;&#29616;&#24120;&#35265;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#25968;&#25454;&#36716;&#31227;&#24773;&#20917;&#19979;&#36807;&#20110;&#33258;&#20449;&#12290;&#36825;&#24378;&#35843;&#20102;&#23545;&#26412;&#22320;&#19981;&#30830;&#23450;&#24615;&#21487;&#38752;&#20272;&#35745;&#21450;&#20854;&#21521;&#26368;&#32456;&#29992;&#25143;&#20256;&#36798;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.13657</link><description>&lt;p&gt;
&#24120;&#35265;&#30340;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#22312;&#21487;&#38752;&#24615;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#26041;&#38754;&#30340;&#19981;&#36275;
&lt;/p&gt;
&lt;p&gt;
Inadequacy of common stochastic neural networks for reliable clinical decision support. (arXiv:2401.13657v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#22312;&#20020;&#24202;&#24212;&#29992;&#20013;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#21457;&#29616;&#24120;&#35265;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#25968;&#25454;&#36716;&#31227;&#24773;&#20917;&#19979;&#36807;&#20110;&#33258;&#20449;&#12290;&#36825;&#24378;&#35843;&#20102;&#23545;&#26412;&#22320;&#19981;&#30830;&#23450;&#24615;&#21487;&#38752;&#20272;&#35745;&#21450;&#20854;&#21521;&#26368;&#32456;&#29992;&#25143;&#20256;&#36798;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20262;&#29702;&#21644;&#23433;&#20840;&#30456;&#20851;&#30340;&#20851;&#20999;&#65292;&#20154;&#24037;&#26234;&#33021;&#30340;&#24191;&#27867;&#24212;&#29992;&#20110;&#21307;&#23398;&#20915;&#31574;&#20173;&#28982;&#21463;&#38459;&#12290;&#23545;&#20110;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#21307;&#30103;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#26469;&#35828;&#65292;&#21487;&#38752;&#24615;&#21644;&#21487;&#20449;&#24230;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#24120;&#35265;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#25968;&#25454;&#36716;&#31227;&#19979;&#24448;&#24448;&#36807;&#20110;&#33258;&#20449;&#12290;&#36825;&#31181;&#22312;&#22522;&#20110;&#35777;&#25454;&#30340;&#22330;&#26223;&#20043;&#22806;&#19981;&#24688;&#24403;&#30340;&#25512;&#29702;&#21487;&#33021;&#20250;&#20135;&#29983;&#20005;&#37325;&#21518;&#26524;&#12290;&#36825;&#20984;&#26174;&#20102;&#23545;&#26412;&#22320;&#19981;&#30830;&#23450;&#24615;&#21487;&#38752;&#20272;&#35745;&#21450;&#20854;&#21521;&#26368;&#32456;&#29992;&#25143;&#20256;&#36798;&#30340;&#37325;&#35201;&#24615;&#12290;&#23613;&#31649;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#34987;&#35465;&#20026;&#36825;&#20123;&#38382;&#39064;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20854;&#22312;&#20020;&#24202;&#24212;&#29992;&#20013;&#30340;&#23454;&#38469;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#20197;&#20174;MIMIC3&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;EHR&#30340;ICU&#20303;&#38498;&#30149;&#27515;&#29575;&#39044;&#27979;&#20026;&#20363;&#26469;&#36827;&#34892;&#20998;&#26512;&#12290;&#23545;&#20110;EHR&#26102;&#38388;&#24207;&#21015;&#30340;&#39044;&#27979;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20165;&#32534;&#30721;&#22120;&#30340;Transformer&#27169;&#22411;&#12290;&#36890;&#36807;&#32435;&#20837;&#24120;&#35265;&#26041;&#27861;&#26469;&#23454;&#29616;&#27169;&#22411;&#20989;&#25968;&#30340;&#38543;&#26426;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Widespread adoption of AI for medical decision making is still hindered due to ethical and safety-related concerns. For AI-based decision support systems in healthcare settings it is paramount to be reliable and trustworthy. Common deep learning approaches, however, have the tendency towards overconfidence under data shift. Such inappropriate extrapolation beyond evidence-based scenarios may have dire consequences. This highlights the importance of reliable estimation of local uncertainty and its communication to the end user. While stochastic neural networks have been heralded as a potential solution to these issues, this study investigates their actual reliability in clinical applications. We centered our analysis on the exemplary use case of mortality prediction for ICU hospitalizations using EHR from MIMIC3 study. For predictions on the EHR time series, Encoder-Only Transformer models were employed. Stochasticity of model functions was achieved by incorporating common methods such 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#21644;&#31232;&#30095;&#32593;&#26684;&#26469;&#26816;&#27979;&#19981;&#36830;&#32493;&#20989;&#25968;&#19981;&#36830;&#32493;&#30028;&#38754;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#32500;&#24230;&#22823;&#20110;3&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#39640;&#25928;&#19988;&#20934;&#30830;&#30340;&#19981;&#36830;&#32493;&#24615;&#26816;&#27979;&#33021;&#21147;&#65292;&#22312;&#32500;&#24230;n = 2&#21644;n = 4&#30340;&#20989;&#25968;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#39640;&#25928;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#20855;&#26377;&#21487;&#31227;&#26893;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.13652</link><description>&lt;p&gt;
&#22522;&#20110;&#31232;&#30095;&#32593;&#26684;&#30340;&#19981;&#36830;&#32493;&#24615;&#26816;&#27979;&#30340;&#22270;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Graph-Informed Neural Networks for Sparse Grid-Based Discontinuity Detectors. (arXiv:2401.13652v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#21644;&#31232;&#30095;&#32593;&#26684;&#26469;&#26816;&#27979;&#19981;&#36830;&#32493;&#20989;&#25968;&#19981;&#36830;&#32493;&#30028;&#38754;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#32500;&#24230;&#22823;&#20110;3&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#39640;&#25928;&#19988;&#20934;&#30830;&#30340;&#19981;&#36830;&#32493;&#24615;&#26816;&#27979;&#33021;&#21147;&#65292;&#22312;&#32500;&#24230;n = 2&#21644;n = 4&#30340;&#20989;&#25968;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#39640;&#25928;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#20855;&#26377;&#21487;&#31227;&#26893;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#19981;&#36830;&#32493;&#20989;&#25968;&#30340;&#19981;&#36830;&#32493;&#30028;&#38754;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#22522;&#20110;&#22270;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;GINNs&#65289;&#21644;&#31232;&#30095;&#32593;&#26684;&#26469;&#35299;&#20915;&#32500;&#24230;&#22823;&#20110;3&#30340;&#24773;&#20917;&#19979;&#30340;&#19981;&#36830;&#32493;&#24615;&#26816;&#27979;&#12290;&#35757;&#32451;&#36807;&#30340;GINNs&#22312;&#31232;&#30095;&#32593;&#26684;&#19978;&#35782;&#21035;&#26377;&#38382;&#39064;&#30340;&#28857;&#65292;&#24182;&#21033;&#29992;&#26500;&#24314;&#22312;&#32593;&#26684;&#19978;&#30340;&#22270;&#32467;&#26500;&#23454;&#29616;&#39640;&#25928;&#20934;&#30830;&#30340;&#19981;&#36830;&#32493;&#24615;&#26816;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#36882;&#24402;&#31639;&#27861;&#29992;&#20110;&#19968;&#33324;&#30340;&#22522;&#20110;&#31232;&#30095;&#32593;&#26684;&#30340;&#26816;&#27979;&#22120;&#65292;&#20855;&#26377;&#25910;&#25947;&#24615;&#21644;&#26131;&#20110;&#24212;&#29992;&#24615;&#12290;&#22312;&#32500;&#24230;n=2&#21644;n=4&#30340;&#20989;&#25968;&#19978;&#36827;&#34892;&#30340;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;GINNs&#22312;&#26816;&#27979;&#19981;&#36830;&#32493;&#30028;&#38754;&#26041;&#38754;&#30340;&#39640;&#25928;&#24615;&#21644;&#40065;&#26834;&#27867;&#21270;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#32463;&#36807;&#35757;&#32451;&#30340;GINNs&#20855;&#26377;&#21487;&#31227;&#26893;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#21487;&#20197;&#38598;&#25104;&#21040;&#21508;&#31181;&#31639;&#27861;&#20013;&#24182;&#20849;&#20139;&#32473;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a novel approach for detecting the discontinuity interfaces of a discontinuous function. This approach leverages Graph-Informed Neural Networks (GINNs) and sparse grids to address discontinuity detection also in domains of dimension larger than 3. GINNs, trained to identify troubled points on sparse grids, exploit graph structures built on the grids to achieve efficient and accurate discontinuity detection performances. We also introduce a recursive algorithm for general sparse grid-based detectors, characterized by convergence properties and easy applicability. Numerical experiments on functions with dimensions n = 2 and n = 4 demonstrate the efficiency and robust generalization of GINNs in detecting discontinuity interfaces. Notably, the trained GINNs offer portability and versatility, allowing integration into various algorithms and sharing among users.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21021;&#27493;&#25506;&#32034;&#20102;&#22522;&#20110;GPT-4&#30340;ChatGPT&#22312;&#38754;&#37096;&#29983;&#29289;&#35782;&#21035;&#20013;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#20998;&#26512;&#20102;ChatGPT&#22312;&#38754;&#37096;&#39564;&#35777;&#12289;&#36719;&#29983;&#29289;&#29305;&#24449;&#20272;&#35745;&#21644;&#32467;&#26524;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;ChatGPT&#30340;&#24212;&#29992;&#26377;&#26395;&#25552;&#39640;&#33258;&#21160;&#20915;&#31574;&#22312;&#20154;&#31867;&#22330;&#26223;&#20013;&#30340;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.13641</link><description>&lt;p&gt;
ChatGPT&#22312;&#38754;&#37096;&#29983;&#29289;&#35782;&#21035;&#20013;&#30340;&#34920;&#29616;&#26377;&#22810;&#22909;&#65311;&#23545;&#35782;&#21035;&#12289;&#36719;&#29983;&#29289;&#29305;&#24449;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#21021;&#27493;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
How Good is ChatGPT at Face Biometrics? A First Look into Recognition, Soft Biometrics, and Explainability. (arXiv:2401.13641v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21021;&#27493;&#25506;&#32034;&#20102;&#22522;&#20110;GPT-4&#30340;ChatGPT&#22312;&#38754;&#37096;&#29983;&#29289;&#35782;&#21035;&#20013;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#20998;&#26512;&#20102;ChatGPT&#22312;&#38754;&#37096;&#39564;&#35777;&#12289;&#36719;&#29983;&#29289;&#29305;&#24449;&#20272;&#35745;&#21644;&#32467;&#26524;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;ChatGPT&#30340;&#24212;&#29992;&#26377;&#26395;&#25552;&#39640;&#33258;&#21160;&#20915;&#31574;&#22312;&#20154;&#31867;&#22330;&#26223;&#20013;&#30340;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35832;&#22914;OpenAI&#24320;&#21457;&#30340;GPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#23637;&#29616;&#20986;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#26524;&#65292;&#20026;&#25105;&#20204;&#30340;&#31038;&#20250;&#24341;&#20837;&#20102;&#24555;&#36895;&#21464;&#38761;&#12290;ChatGPT&#30340;&#21457;&#24067;&#36827;&#19968;&#27493;&#21152;&#24378;&#20102;&#36825;&#19968;&#24433;&#21709;&#65292;&#23427;&#20351;&#20219;&#20309;&#20154;&#37117;&#33021;&#20197;&#31616;&#21333;&#30340;&#23545;&#35805;&#26041;&#24335;&#19982;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#39046;&#22495;&#32463;&#39564;&#12290;&#22240;&#27492;&#65292;ChatGPT&#24050;&#34987;&#36805;&#36895;&#24212;&#29992;&#20110;&#35768;&#22810;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#22914;&#20195;&#30721;&#21644;&#27468;&#26354;&#21019;&#20316;&#12289;&#25945;&#32946;&#12289;&#34394;&#25311;&#21161;&#25163;&#31561;&#65292;&#23637;&#31034;&#20102;&#23545;&#20110;&#26410;&#32463;&#36807;&#35757;&#32451;&#30340;&#20219;&#21153;&#32780;&#35328;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65288;&#38646;&#26679;&#26412;&#23398;&#20064;&#65289;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#22522;&#20110;&#26368;&#26032;&#30340;GPT-4&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#30340;ChatGPT&#22312;&#38754;&#37096;&#29983;&#29289;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;ChatGPT&#22312;&#38754;&#37096;&#39564;&#35777;&#12289;&#36719;&#29983;&#29289;&#29305;&#24449;&#20272;&#35745;&#21644;&#32467;&#26524;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;ChatGPT&#23545;&#20110;&#36827;&#19968;&#27493;&#22686;&#21152;&#20154;&#31867;&#22330;&#26223;&#20013;&#33258;&#21160;&#20915;&#31574;&#30340;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24230;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#23454;&#39564;&#34987;&#36827;&#34892;&#20197;&#35780;&#20272;ChatGPT&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) such as GPT developed by OpenAI, have already shown astonishing results, introducing quick changes in our society. This has been intensified by the release of ChatGPT which allows anyone to interact in a simple conversational way with LLMs, without any experience in the field needed. As a result, ChatGPT has been rapidly applied to many different tasks such as code- and song-writer, education, virtual assistants, etc., showing impressive results for tasks for which it was not trained (zero-shot learning).  The present study aims to explore the ability of ChatGPT, based on the recent GPT-4 multimodal LLM, for the task of face biometrics. In particular, we analyze the ability of ChatGPT to perform tasks such as face verification, soft-biometrics estimation, and explainability of the results. ChatGPT could be very valuable to further increase the explainability and transparency of the automatic decisions in human scenarios. Experiments are carried out in order
&lt;/p&gt;</description></item><item><title>CLIP&#27169;&#22411;&#20026;&#29031;&#29255;&#25628;&#32034;&#24102;&#26469;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#36890;&#36807;&#23398;&#20064;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#20849;&#20139;&#34920;&#31034;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#36328;&#27169;&#24577;&#29702;&#35299;&#65292;&#24182;&#23454;&#29616;&#20102;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#30340;&#39640;&#25928;&#20934;&#30830;&#30340;&#22270;&#20687;&#26816;&#32034;&#12290;&#23427;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21487;&#24212;&#29992;&#20110;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#20998;&#31867;&#31561;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2401.13613</link><description>&lt;p&gt;
&#25552;&#21319;&#22270;&#20687;&#26816;&#32034;&#65306;&#20351;&#29992;CLIP&#27169;&#22411;&#36827;&#34892;&#29031;&#29255;&#25628;&#32034;&#30340;&#32508;&#21512;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Enhancing Image Retrieval : A Comprehensive Study on Photo Search using the CLIP Mode. (arXiv:2401.13613v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13613
&lt;/p&gt;
&lt;p&gt;
CLIP&#27169;&#22411;&#20026;&#29031;&#29255;&#25628;&#32034;&#24102;&#26469;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#36890;&#36807;&#23398;&#20064;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#20849;&#20139;&#34920;&#31034;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#36328;&#27169;&#24577;&#29702;&#35299;&#65292;&#24182;&#23454;&#29616;&#20102;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#30340;&#39640;&#25928;&#20934;&#30830;&#30340;&#22270;&#20687;&#26816;&#32034;&#12290;&#23427;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21487;&#24212;&#29992;&#20110;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#20998;&#31867;&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29031;&#29255;&#25628;&#32034;&#26159;&#26681;&#25454;&#25991;&#26412;&#26597;&#35810;&#26816;&#32034;&#22270;&#20687;&#30340;&#20219;&#21153;&#65292;&#22312;CLIP&#65288;&#23545;&#27604;&#23398;&#20064;&#35821;&#35328;&#19982;&#22270;&#20687;&#39044;&#35757;&#32451;&#65289;&#27169;&#22411;&#30340;&#24341;&#20837;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;CLIP&#21033;&#29992;&#20102;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#23398;&#20064;&#20102;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#20849;&#20139;&#34920;&#31034;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#36328;&#27169;&#24577;&#29702;&#35299;&#12290;&#35813;&#27169;&#22411;&#23637;&#31034;&#20102;&#29702;&#35299;&#19981;&#21516;&#22270;&#20687;&#21644;&#25991;&#26412;&#23545;&#20043;&#38388;&#35821;&#20041;&#20851;&#31995;&#30340;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#30340;&#22270;&#20687;&#39640;&#25928;&#20934;&#30830;&#30340;&#26816;&#32034;&#12290;&#36890;&#36807;&#22312;&#21253;&#21547;&#22270;&#20687;&#21450;&#20854;&#30456;&#20851;&#25991;&#26412;&#25551;&#36848;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;CLIP&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20026;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#20998;&#31867;&#31561;&#20219;&#21153;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#24037;&#20855;&#12290;&#36825;&#31687;&#25688;&#35201;&#24635;&#32467;&#20102;CLIP&#30340;&#22522;&#26412;&#21407;&#29702;&#65292;&#24182;&#24378;&#35843;&#20102;&#20854;&#23545;&#25512;&#36827;&#29031;&#29255;&#25628;&#32034;&#39046;&#22495;&#30340;&#28508;&#22312;&#24433;&#21709;&#65292;&#20419;&#36827;&#20102;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#26080;&#32541;&#34701;&#21512;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Photo search, the task of retrieving images based on textual queries, has witnessed significant advancements with the introduction of CLIP (Contrastive Language-Image Pretraining) model. CLIP leverages a vision-language pre training approach, wherein it learns a shared representation space for images and text, enabling cross-modal understanding. This model demonstrates the capability to understand the semantic relationships between diverse image and text pairs, allowing for efficient and accurate retrieval of images based on natural language queries. By training on a large-scale dataset containing images and their associated textual descriptions, CLIP achieves remarkable generalization, providing a powerful tool for tasks such as zero-shot learning and few-shot classification. This abstract summarizes the foundational principles of CLIP and highlights its potential impact on advancing the field of photo search, fostering a seamless integration of natural language understanding and comp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#20013;&#38388;ASR&#29305;&#24449;&#21644;&#20154;&#31867;&#35760;&#24518;&#27169;&#22411;&#39044;&#27979;&#21548;&#38556;&#29992;&#25143;&#30340;&#35821;&#38899;&#21487;&#25026;&#24230;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#24182;&#38477;&#20302;&#20102;&#22343;&#26041;&#26681;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2401.13611</link><description>&lt;p&gt;
&#20351;&#29992;&#20013;&#38388;ASR&#29305;&#24449;&#21644;&#20154;&#31867;&#35760;&#24518;&#27169;&#22411;&#23545;&#21548;&#38556;&#29992;&#25143;&#36827;&#34892;&#38750;&#20405;&#20837;&#24615;&#35821;&#38899;&#21487;&#25026;&#24230;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Non-Intrusive Speech Intelligibility Prediction for Hearing-Impaired Users using Intermediate ASR Features and Human Memory Models. (arXiv:2401.13611v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#20013;&#38388;ASR&#29305;&#24449;&#21644;&#20154;&#31867;&#35760;&#24518;&#27169;&#22411;&#39044;&#27979;&#21548;&#38556;&#29992;&#25143;&#30340;&#35821;&#38899;&#21487;&#25026;&#24230;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#24182;&#38477;&#20302;&#20102;&#22343;&#26041;&#26681;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#24050;&#25104;&#21151;&#29992;&#20110;&#38750;&#20405;&#20837;&#24615;&#35821;&#38899;&#21487;&#25026;&#24230;&#39044;&#27979;&#12290;&#26368;&#36817;&#65292;&#20351;&#29992;&#26469;&#33258;&#39044;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#21644;&#24369;&#30417;&#30563;&#27169;&#22411;&#30340;&#20013;&#38388;&#23618;&#29305;&#24449;&#34920;&#31034;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#34987;&#21457;&#29616;&#29305;&#21035;&#26377;&#29992;&#12290;&#26412;&#30740;&#31350;&#23558;Whisper ASR&#35299;&#30721;&#22120;&#23618;&#30340;&#34920;&#31034;&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#29305;&#24449;&#65292;&#32467;&#21512;&#20102;&#19968;&#31181;&#22522;&#20110;&#31034;&#33539;&#30340;&#12289;&#24515;&#29702;&#23398;&#27169;&#22411;&#30340;&#20154;&#31867;&#35760;&#24518;&#65292;&#20197;&#39044;&#27979;&#21161;&#21548;&#22120;&#29992;&#25143;&#30340;&#20154;&#31867;&#21487;&#25026;&#24230;&#35780;&#20998;&#12290;&#19982;&#24050;&#24314;&#31435;&#30340;&#20405;&#20837;&#24615;HASPI&#22522;&#32447;&#31995;&#32479;&#30456;&#27604;&#65292;&#25105;&#20204;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#21253;&#25324;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#26410;&#35265;&#36807;&#30340;&#22686;&#24378;&#31995;&#32479;&#21644;&#21548;&#32773;&#65292;&#22343;&#26041;&#26681;&#35823;&#24046;&#20174;28.7&#38477;&#33267;25.3&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks have been successfully used for non-intrusive speech intelligibility prediction. Recently, the use of feature representations sourced from intermediate layers of pre-trained self-supervised and weakly-supervised models has been found to be particularly useful for this task. This work combines the use of Whisper ASR decoder layer representations as neural network input features with an exemplar-based, psychologically motivated model of human memory to predict human intelligibility ratings for hearing-aid users. Substantial performance improvement over an established intrusive HASPI baseline system is found, including on enhancement systems and listeners unseen in the training data, with a root mean squared error of 25.3 compared with the baseline of 28.7.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#24335;&#24863;&#30693;&#30340;&#26041;&#27861;&#65292;&#20351;&#31227;&#21160;&#35774;&#22791;&#20013;&#30340;&#35748;&#30693;&#20195;&#29702;&#33021;&#22815;&#24863;&#30693;&#26377;&#24847;&#20041;&#30340;&#24773;&#22659;&#12290;&#30740;&#31350;&#36890;&#36807;&#20247;&#21253;&#26696;&#20363;&#23637;&#31034;&#20102;&#25163;&#26426;&#20256;&#24863;&#22120;&#25968;&#25454;&#22914;&#20309;&#34987;&#29992;&#20110;&#35302;&#21457;&#21644;&#25351;&#23548;&#33258;&#20027;&#12289;&#33258;&#21033;&#30340;&#20195;&#29702;&#21327;&#20316;&#36865;&#36135;&#21040;&#30446;&#30340;&#22320;&#12290;</title><link>http://arxiv.org/abs/2401.13604</link><description>&lt;p&gt;
&#31227;&#21160;&#29983;&#24577;&#31995;&#32479;&#20013;&#22522;&#20110;&#27969;&#24335;&#24863;&#30693;&#30340;&#35748;&#30693;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Stream-based perception for cognitive agents in mobile ecosystems. (arXiv:2401.13604v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13604
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#24335;&#24863;&#30693;&#30340;&#26041;&#27861;&#65292;&#20351;&#31227;&#21160;&#35774;&#22791;&#20013;&#30340;&#35748;&#30693;&#20195;&#29702;&#33021;&#22815;&#24863;&#30693;&#26377;&#24847;&#20041;&#30340;&#24773;&#22659;&#12290;&#30740;&#31350;&#36890;&#36807;&#20247;&#21253;&#26696;&#20363;&#23637;&#31034;&#20102;&#25163;&#26426;&#20256;&#24863;&#22120;&#25968;&#25454;&#22914;&#20309;&#34987;&#29992;&#20110;&#35302;&#21457;&#21644;&#25351;&#23548;&#33258;&#20027;&#12289;&#33258;&#21033;&#30340;&#20195;&#29702;&#21327;&#20316;&#36865;&#36135;&#21040;&#30446;&#30340;&#22320;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#30693;&#20195;&#29702;&#25277;&#35937;&#21487;&#20197;&#24110;&#21161;&#22312;&#31227;&#21160;&#35774;&#22791;&#20013;&#26500;&#24314;&#26234;&#33021;&#31995;&#32479;&#12290;&#22312;&#26234;&#33021;&#25163;&#26426;&#19978;&#65292;&#26469;&#33258;&#26426;&#36733;&#20256;&#24863;&#22120;&#30340;&#25968;&#25454;&#21487;&#20197;&#20026;&#29992;&#25143;&#30340;&#24403;&#21069;&#24773;&#20917;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#27934;&#23519;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#35748;&#30693;&#20195;&#29702;&#26694;&#26550;&#26080;&#27861;&#24456;&#22909;&#22320;&#24212;&#23545;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#25361;&#25112;&#24615;&#29305;&#24449;&#12290;&#20256;&#24863;&#22120;&#25968;&#25454;&#20301;&#20110;&#20302;&#25277;&#35937;&#32423;&#21035;&#19978;&#65292;&#21333;&#20010;&#25968;&#25454;&#20803;&#32032;&#22312;&#23396;&#31435;&#35266;&#23519;&#26102;&#27809;&#26377;&#24847;&#20041;&#12290;&#30456;&#21453;&#65292;&#35748;&#30693;&#20195;&#29702;&#22312;&#39640;&#32423;&#24863;&#30693;&#19978;&#36816;&#34892;&#65292;&#24182;&#19988;&#32570;&#20047;&#26377;&#25928;&#22320;&#26816;&#27979;&#22810;&#20010;&#24863;&#30693;&#24207;&#21015;&#20013;&#22797;&#26434;&#26102;&#31354;&#27169;&#24335;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#24335;&#24863;&#30693;&#30340;&#26041;&#27861;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#22312;&#20302;&#32423;&#20256;&#24863;&#22120;&#25968;&#25454;&#27969;&#20013;&#24863;&#30693;&#26377;&#24847;&#20041;&#30340;&#24773;&#22659;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#20195;&#34920;&#24615;&#30340;&#20247;&#21253;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#25163;&#26426;&#20256;&#24863;&#22120;&#25968;&#25454;&#23548;&#20986;&#30340;&#24773;&#22659;&#22914;&#20309;&#35302;&#21457;&#21644;&#25351;&#23548;&#33258;&#20027;&#12289;&#33258;&#21033;&#30340;&#20195;&#29702;&#21327;&#20316;&#36865;&#36135;&#21040;&#30446;&#30340;&#22320;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cognitive agent abstractions can help to engineer intelligent systems across mobile devices. On smartphones, the data obtained from onboard sensors can give valuable insights into the user's current situation. Unfortunately, today's cognitive agent frameworks cannot cope well with the challenging characteristics of sensor data. Sensor data is located on a low abstraction level and the individual data elements are not meaningful when observed in isolation. In contrast, cognitive agents operate on high-level percepts and lack the means to effectively detect complex spatio-temporal patterns in sequences of multiple percepts. In this paper, we present a stream-based perception approach that enables the agents to perceive meaningful situations in low-level sensor data streams. We present a crowdshipping case study where autonomous, self-interested agents collaborate to deliver parcels to their destinations. We show how situations derived from smartphone sensor data can trigger and guide auc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#35814;&#23613;&#30340;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#32039;&#20945;&#30340;&#12289;&#20219;&#21153;&#29305;&#23450;&#30340;QA&#27169;&#22411;&#65292;&#20174;&#32780;&#22312;&#29305;&#23450;&#38382;&#31572;&#20219;&#21153;&#20013;&#19982;GPT&#21464;&#20307;&#27169;&#22411;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#36807;&#31243;&#24615;&#25991;&#26412;&#30340;&#32467;&#26500;&#21270;&#29305;&#24615;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#27493;&#39588;&#21644;&#25972;&#20010;&#27969;&#31243;&#34920;&#31034;&#20026;&#22270;&#24418;&#65292;&#24182;&#20197;&#22270;&#33410;&#28857;&#20026;&#26465;&#20214;&#65292;&#22312;&#35814;&#23613;&#21644;&#21487;&#25511;&#30340;&#26041;&#24335;&#19979;&#33258;&#21160;&#29983;&#25104;QA&#23545;&#12290;</title><link>http://arxiv.org/abs/2401.13594</link><description>&lt;p&gt;
&#29992;&#20110;&#31243;&#24207;&#24615;&#38382;&#31572;&#30340;&#22270;&#24418;&#24341;&#23548;&#38382;&#39064;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Graph Guided Question Answer Generation for Procedural Question-Answering. (arXiv:2401.13594v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#35814;&#23613;&#30340;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#32039;&#20945;&#30340;&#12289;&#20219;&#21153;&#29305;&#23450;&#30340;QA&#27169;&#22411;&#65292;&#20174;&#32780;&#22312;&#29305;&#23450;&#38382;&#31572;&#20219;&#21153;&#20013;&#19982;GPT&#21464;&#20307;&#27169;&#22411;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#36807;&#31243;&#24615;&#25991;&#26412;&#30340;&#32467;&#26500;&#21270;&#29305;&#24615;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#27493;&#39588;&#21644;&#25972;&#20010;&#27969;&#31243;&#34920;&#31034;&#20026;&#22270;&#24418;&#65292;&#24182;&#20197;&#22270;&#33410;&#28857;&#20026;&#26465;&#20214;&#65292;&#22312;&#35814;&#23613;&#21644;&#21487;&#25511;&#30340;&#26041;&#24335;&#19979;&#33258;&#21160;&#29983;&#25104;QA&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#30340;&#38382;&#31572;(QA)&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#35814;&#23613;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#35757;&#32451;&#32039;&#20945;&#30340;&#12289;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#30340;QA&#27169;&#22411;&#65292;&#24182;&#19982;GPT&#21464;&#20307;&#27169;&#22411;&#31454;&#20105;&#12290;&#20851;&#38190;&#30340;&#25216;&#26415;&#25903;&#25345;&#26159;&#19968;&#31181;&#20174;&#36807;&#31243;&#25991;&#26412;&#20013;&#33258;&#21160;&#29983;&#25104;&#38382;&#39064;-&#31572;&#26696;&#30340;&#26032;&#26426;&#21046;&#65292;&#23427;&#21487;&#20197;&#22788;&#29702;&#22823;&#37327;&#30340;&#25991;&#26412;&#25351;&#20196;&#65292;&#24182;&#20135;&#29983;&#35814;&#23613;&#30340;&#39046;&#22495;&#20869;QA&#35757;&#32451;&#25968;&#25454;&#12290;&#30446;&#21069;&#30340;QA&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#20250;&#20135;&#29983;&#24418;&#24335;&#33391;&#22909;&#19988;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#65292;&#20294;&#20854;&#38750;&#35814;&#23613;&#24615;&#19981;&#21033;&#20110;&#35757;&#32451;QA&#27169;&#22411;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#21033;&#29992;&#36807;&#31243;&#24615;&#25991;&#26412;&#30340;&#39640;&#24230;&#32467;&#26500;&#21270;&#29305;&#24615;&#65292;&#23558;&#27599;&#20010;&#27493;&#39588;&#21644;&#25972;&#20010;&#27969;&#31243;&#34920;&#31034;&#20026;&#22270;&#24418;&#65292;&#24182;&#20197;&#22270;&#33410;&#28857;&#20026;&#26465;&#20214;&#65292;&#33258;&#21160;&#20197;&#35814;&#23613;&#21644;&#21487;&#25511;&#30340;&#26041;&#24335;&#29983;&#25104;QA&#23545;&#12290;&#23545;&#25105;&#20204;&#26041;&#27861;&#30340;&#20840;&#38754;&#35780;&#20272;&#34920;&#26126;&#65306;1) &#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#35757;&#32451;&#30340;&#23567;&#22411;&#27169;&#22411;&#33021;&#22815;&#36798;&#21040;&#31454;&#20105;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we focus on task-specific question answering (QA). To this end, we introduce a method for generating exhaustive and high-quality training data, which allows us to train compact (e.g., run on a mobile device), task-specific QA models that are competitive against GPT variants. The key technological enabler is a novel mechanism for automatic question-answer generation from procedural text which can ingest large amounts of textual instructions and produce exhaustive in-domain QA training data. While current QA data generation methods can produce well-formed and varied data, their non-exhaustive nature is sub-optimal for training a QA model. In contrast, we leverage the highly structured aspect of procedural text and represent each step and the overall flow of the procedure as graphs. We then condition on graph nodes to automatically generate QA pairs in an exhaustive and controllable manner. Comprehensive evaluations of our method show that: 1) small models trained with our 
&lt;/p&gt;</description></item><item><title>&#22312;&#23454;&#38469;&#21307;&#30103;&#39046;&#22495;&#20013;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#26356;&#28145;&#20837;&#21644;&#23454;&#29992;&#30340;&#35780;&#20272;&#26159;&#24517;&#35201;&#30340;&#65292;&#35813;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#25104;&#20154;&#37325;&#30151;&#25252;&#29702;&#21307;&#23398;&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.13588</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20174;&#25104;&#20154;&#37325;&#30151;&#25252;&#29702;&#30005;&#23376;&#30149;&#21382;&#20013;&#25552;&#21462;&#35821;&#20041;&#27010;&#24565;&#30340;&#32972;&#26223;&#19979;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Evaluation of General Large Language Models in Contextually Assessing Semantic Concepts Extracted from Adult Critical Care Electronic Health Record Notes. (arXiv:2401.13588v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13588
&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#21307;&#30103;&#39046;&#22495;&#20013;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#26356;&#28145;&#20837;&#21644;&#23454;&#29992;&#30340;&#35780;&#20272;&#26159;&#24517;&#35201;&#30340;&#65292;&#35813;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#25104;&#20154;&#37325;&#30151;&#25252;&#29702;&#21307;&#23398;&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#33394;&#34920;&#29616;&#65292;&#35813;&#30740;&#31350;&#23558;&#28966;&#28857;&#36716;&#21521;&#20102;&#21307;&#30103;&#20581;&#24247;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#23454;&#38469;&#20020;&#24202;&#24212;&#29992;&#20013;&#30340;&#34920;&#29616;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#20256;&#32479;&#30340;&#38382;&#31572;&#20219;&#21153;&#35780;&#20272;&#19981;&#33021;&#23436;&#20840;&#25429;&#25417;&#21040;&#22797;&#26434;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#36825;&#31181;&#24046;&#36317;&#20984;&#26174;&#20102;&#22312;&#23454;&#38469;&#21307;&#30103;&#39046;&#22495;&#20013;&#23545;LLMs&#36827;&#34892;&#26356;&#28145;&#20837;&#21644;&#23454;&#29992;&#30340;&#35780;&#20272;&#30340;&#38656;&#27714;&#12290;&#35813;&#30740;&#31350;&#26088;&#22312;&#20351;&#29992;&#31995;&#32479;&#21270;&#21644;&#26131;&#20110;&#29702;&#35299;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#21253;&#25324;&#20020;&#24202;&#21307;&#29983;&#27880;&#37322;&#21644;&#35009;&#23450;&#65292;&#35780;&#20272;LLMs&#22312;&#25104;&#20154;&#37325;&#30151;&#25252;&#29702;&#21307;&#23398;&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of healthcare has increasingly turned its focus towards Large Language Models (LLMs) due to their remarkable performance. However, their performance in actual clinical applications has been underexplored. Traditional evaluations based on question-answering tasks don't fully capture the nuanced contexts. This gap highlights the need for more in-depth and practical assessments of LLMs in real-world healthcare settings. Objective: We sought to evaluate the performance of LLMs in the complex clinical context of adult critical care medicine using systematic and comprehensible analytic methods, including clinician annotation and adjudication. Methods: We investigated the performance of three general LLMs in understanding and processing real-world clinical notes. Concepts from 150 clinical notes were identified by MetaMap and then labeled by 9 clinicians. Each LLM's proficiency was evaluated by identifying the temporality and negation of these concepts using different prompts for an
&lt;/p&gt;</description></item><item><title>LLM&#25351;&#20196;&#24494;&#35843;&#20013;&#65292;&#23545;&#20110;&#30701;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#65292;&#25552;&#31034;&#35789;&#26631;&#35760;&#20998;&#31867;&#25439;&#22833;&#21152;&#26435;&#65288;PLW&#65289;&#19982;&#24615;&#33021;&#21576;&#36127;&#20108;&#27425;&#20851;&#31995;&#65292;&#32780;&#38271;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#21017;&#19981;&#21463;PLW&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.13586</link><description>&lt;p&gt;
LLM&#25351;&#20196;&#24494;&#35843;&#20013;&#30340;&#25552;&#31034;&#26435;&#37325;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Prompt Weight Experiments for LLM Instruction Fine-Tuning. (arXiv:2401.13586v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13586
&lt;/p&gt;
&lt;p&gt;
LLM&#25351;&#20196;&#24494;&#35843;&#20013;&#65292;&#23545;&#20110;&#30701;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#65292;&#25552;&#31034;&#35789;&#26631;&#35760;&#20998;&#31867;&#25439;&#22833;&#21152;&#26435;&#65288;PLW&#65289;&#19982;&#24615;&#33021;&#21576;&#36127;&#20108;&#27425;&#20851;&#31995;&#65292;&#32780;&#38271;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#21017;&#19981;&#21463;PLW&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#23567;&#22411;&#30740;&#31350;&#65292;&#20998;&#26512;&#20102;&#25552;&#31034;&#35789;&#26631;&#35760;&#20998;&#31867;&#25439;&#22833;&#21152;&#26435;&#65288;PLW&#65289;&#22914;&#20309;&#24433;&#21709;&#22312;&#25351;&#20196;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;7B&#22823;&#23567;&#30340;LLaMA&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#20010;&#25351;&#20196;&#25968;&#25454;&#38598;&#37325;&#29616;&#20102;&#26031;&#22374;&#31119;&#22823;&#23398;&#30340;Alpaca&#23454;&#39564;&#65292;&#20854;&#20013;&#21253;&#25324;LLaMA 1&#21644;LLaMA 2&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#25105;&#20204;&#30340;&#30701;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#30340;&#27169;&#22411;&#19982;PLW&#20043;&#38388;&#23384;&#22312;&#36127;&#20108;&#27425;&#20851;&#31995;&#65292;&#32780;&#22312;&#38271;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#30340;&#27169;&#22411;&#19981;&#21463;PLW&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a small study analyzing how prompt token classification loss weighting (PLW) affects the performance of 7B-size LLaMA models fine-tuned on instruction tasks. We recreated Stanford's Alpaca experiment with both LLaMA 1 and LLaMA 2 using multiple instruction datasets. We found that models fine-tuned on our short-completion dataset have a negative quadratic relationship with PLW while models fine-tuned on long-completion datasets were unaffected by PLW.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#26377;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#30340;&#26694;&#26550;&#65292;&#24182;&#38024;&#23545;&#22270;&#20687;&#19978;&#37319;&#26679;&#24212;&#29992;&#21019;&#24314;&#20102;&#19968;&#20010;&#28085;&#30422;&#29616;&#20195;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#20351;&#29992;&#26080;&#20559;&#35757;&#32451;&#38598;&#23545;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#25581;&#31034;&#20102;&#19981;&#21516;&#31639;&#27861;&#23545;&#35813;&#38382;&#39064;&#30340;&#21709;&#24212;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2401.13555</link><description>&lt;p&gt;
&#22270;&#20687;&#19978;&#37319;&#26679;&#26041;&#27861;&#30340;&#20844;&#24179;&#24615;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking the Fairness of Image Upsampling Methods. (arXiv:2401.13555v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13555
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#26377;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#30340;&#26694;&#26550;&#65292;&#24182;&#38024;&#23545;&#22270;&#20687;&#19978;&#37319;&#26679;&#24212;&#29992;&#21019;&#24314;&#20102;&#19968;&#20010;&#28085;&#30422;&#29616;&#20195;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#20351;&#29992;&#26080;&#20559;&#35757;&#32451;&#38598;&#23545;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#25581;&#31034;&#20102;&#19981;&#21516;&#31639;&#27861;&#23545;&#35813;&#38382;&#39064;&#30340;&#21709;&#24212;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#21019;&#24314;&#21512;&#25104;&#23186;&#20307;&#65288;&#22914;&#22270;&#20687;&#21644;&#35270;&#39057;&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#24555;&#36895;&#21457;&#23637;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#22312;&#26085;&#24120;&#20219;&#21153;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#38750;&#24120;&#35825;&#20154;&#65292;&#20294;&#35780;&#20272;&#20854;&#20844;&#24179;&#24615;&#30456;&#20851;&#30340;&#28508;&#22312;&#39118;&#38505;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#26377;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#22871;&#24230;&#37327;&#26631;&#20934;&#8212;&#8212;&#21463;&#30417;&#30563;&#20844;&#24179;&#24615;&#30340;&#28789;&#24863;&#26469;&#28304;&#8212;&#8212;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#38024;&#23545;&#22270;&#20687;&#19978;&#37319;&#26679;&#36825;&#20010;&#29305;&#23450;&#24212;&#29992;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#28085;&#30422;&#21508;&#31181;&#29616;&#20195;&#19978;&#37319;&#26679;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#20316;&#20026;&#22522;&#20934;&#27979;&#35797;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;UnfairFace&#65292;&#36825;&#26159;FairFace&#30340;&#19968;&#20010;&#23376;&#38598;&#65292;&#22797;&#21046;&#20102;&#24120;&#35265;&#22823;&#35268;&#27169;&#20154;&#33080;&#25968;&#25454;&#38598;&#30340;&#31181;&#26063;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#20984;&#26174;&#20102;&#20351;&#29992;&#26080;&#20559;&#35757;&#32451;&#38598;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25581;&#31034;&#20102;&#31639;&#27861;&#23545;&#35813;&#38382;&#39064;&#30340;&#21709;&#24212;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed a rapid development of deep generative models for creating synthetic media, such as images and videos. While the practical applications of these models in everyday tasks are enticing, it is crucial to assess the inherent risks regarding their fairness. In this work, we introduce a comprehensive framework for benchmarking the performance and fairness of conditional generative models. We develop a set of metrics$\unicode{x2013}$inspired by their supervised fairness counterparts$\unicode{x2013}$to evaluate the models on their fairness and diversity. Focusing on the specific application of image upsampling, we create a benchmark covering a wide variety of modern upsampling methods. As part of the benchmark, we introduce UnfairFace, a subset of FairFace that replicates the racial distribution of common large-scale face datasets. Our empirical study highlights the importance of using an unbiased training set and reveals variations in how the algorithms respond to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#20048;&#22120;&#29305;&#23450;&#36755;&#20837;&#34920;&#31034;&#21644;&#25193;&#25955;&#22806;&#25193;&#25216;&#26415;&#30340;&#34920;&#29616;&#21147;&#20016;&#23500;&#30340;&#22768;&#23398;&#21513;&#20182;&#22768;&#38899;&#21512;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#27604;&#20854;&#20182;&#27169;&#22411;&#20855;&#26377;&#26356;&#39640;&#30340;&#38899;&#39057;&#36136;&#37327;&#21644;&#26356;&#21152;&#36924;&#30495;&#30340;&#38899;&#33394;&#22768;&#38899;&#12290;</title><link>http://arxiv.org/abs/2401.13498</link><description>&lt;p&gt;
&#37319;&#29992;&#20048;&#22120;&#29305;&#23450;&#36755;&#20837;&#34920;&#31034;&#21644;&#25193;&#25955;&#22806;&#25193;&#25216;&#26415;&#30340;&#34920;&#29616;&#21147;&#20016;&#23500;&#30340;&#22768;&#23398;&#21513;&#20182;&#22768;&#38899;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Expressive Acoustic Guitar Sound Synthesis with an Instrument-Specific Input Representation and Diffusion Outpainting. (arXiv:2401.13498v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#20048;&#22120;&#29305;&#23450;&#36755;&#20837;&#34920;&#31034;&#21644;&#25193;&#25955;&#22806;&#25193;&#25216;&#26415;&#30340;&#34920;&#29616;&#21147;&#20016;&#23500;&#30340;&#22768;&#23398;&#21513;&#20182;&#22768;&#38899;&#21512;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#27604;&#20854;&#20182;&#27169;&#22411;&#20855;&#26377;&#26356;&#39640;&#30340;&#38899;&#39057;&#36136;&#37327;&#21644;&#26356;&#21152;&#36924;&#30495;&#30340;&#38899;&#33394;&#22768;&#38899;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22797;&#38899;&#21644;&#34920;&#29616;&#30340;&#39640;&#24230;&#21464;&#24322;&#65292;&#21512;&#25104;&#28436;&#22863;&#21513;&#20182;&#22768;&#38899;&#26159;&#19968;&#39033;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26368;&#36817;&#65292;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#20174;&#38899;&#20048;&#20048;&#35889;&#20013;&#21512;&#25104;&#34920;&#29616;&#21147;&#20016;&#23500;&#30340;&#22810;&#38899;&#20048;&#22120;&#22768;&#38899;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#36890;&#24120;&#20351;&#29992;&#36890;&#29992;&#30340;MIDI&#36755;&#20837;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#33258;&#23450;&#20041;&#20048;&#22120;&#36755;&#20837;&#34920;&#31034;&#30340;&#34920;&#29616;&#21147;&#20016;&#23500;&#30340;&#22768;&#23398;&#21513;&#20182;&#22768;&#38899;&#21512;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;guitarroll&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#25193;&#25955;&#30340;&#22806;&#25193;&#25216;&#26415;&#23454;&#29616;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#35813;&#25216;&#26415;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#38271;&#26399;&#19968;&#33268;&#24615;&#30340;&#38899;&#39057;&#12290;&#20026;&#20102;&#20811;&#26381;&#32570;&#20047;MIDI/&#38899;&#39057;&#37197;&#23545;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#19981;&#20165;&#20351;&#29992;&#20102;&#29616;&#26377;&#30340;&#21513;&#20182;&#25968;&#25454;&#38598;&#65292;&#36824;&#20174;&#39640;&#36136;&#37327;&#30340;&#22522;&#20110;&#26679;&#26412;&#30340;&#21513;&#20182;&#21512;&#25104;&#22120;&#20013;&#25910;&#38598;&#20102;&#25968;&#25454;&#12290;&#36890;&#36807;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#27604;&#22522;&#20934;&#27169;&#22411;&#20855;&#26377;&#26356;&#39640;&#30340;&#38899;&#39057;&#36136;&#37327;&#65292;&#24182;&#19988;&#27604;&#20808;&#21069;&#30340;&#39046;&#20808;&#24037;&#20316;&#29983;&#25104;&#26356;&#21152;&#36924;&#30495;&#30340;&#38899;&#33394;&#22768;&#38899;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthesizing performing guitar sound is a highly challenging task due to the polyphony and high variability in expression. Recently, deep generative models have shown promising results in synthesizing expressive polyphonic instrument sounds from music scores, often using a generic MIDI input. In this work, we propose an expressive acoustic guitar sound synthesis model with a customized input representation to the instrument, which we call guitarroll. We implement the proposed approach using diffusion-based outpainting which can generate audio with long-term consistency. To overcome the lack of MIDI/audio-paired datasets, we used not only an existing guitar dataset but also collected data from a high quality sample-based guitar synthesizer. Through quantitative and qualitative evaluations, we show that our proposed model has higher audio quality than the baseline model and generates more realistic timbre sounds than the previous leading work.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#31163;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;SPINN&#65289;&#21644;&#28145;&#24230;&#33021;&#37327;&#26041;&#27861;&#65288;DEM&#65289;&#30340;&#24377;&#24615;&#38382;&#39064;&#27714;&#35299;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#25910;&#25947;&#36895;&#24230;&#21644;&#31934;&#24230;&#19978;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#65292;&#24182;&#19988;&#22312;&#22797;&#26434;&#20960;&#20309;&#20307;&#19978;&#30340;&#32447;&#24615;&#24377;&#24615;&#29702;&#35770;&#38382;&#39064;&#26041;&#38754;&#20855;&#26377;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.13486</link><description>&lt;p&gt;
&#20998;&#31163;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#24377;&#24615;&#38382;&#39064;&#30340;&#27714;&#35299;
&lt;/p&gt;
&lt;p&gt;
Separable Physics-Informed Neural Networks for the solution of elasticity problems. (arXiv:2401.13486v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#31163;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;SPINN&#65289;&#21644;&#28145;&#24230;&#33021;&#37327;&#26041;&#27861;&#65288;DEM&#65289;&#30340;&#24377;&#24615;&#38382;&#39064;&#27714;&#35299;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#25910;&#25947;&#36895;&#24230;&#21644;&#31934;&#24230;&#19978;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#65292;&#24182;&#19988;&#22312;&#22797;&#26434;&#20960;&#20309;&#20307;&#19978;&#30340;&#32447;&#24615;&#24377;&#24615;&#29702;&#35770;&#38382;&#39064;&#26041;&#38754;&#20855;&#26377;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#31163;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;SPINN&#65289;&#21644;&#28145;&#24230;&#33021;&#37327;&#26041;&#27861;&#65288;DEM&#65289;&#30340;&#24377;&#24615;&#38382;&#39064;&#27714;&#35299;&#26041;&#27861;&#12290;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#65292;&#35777;&#26126;&#35813;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#31934;&#24230;&#26174;&#33879;&#39640;&#20110;&#20256;&#32479;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#20197;&#21450;&#22522;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#30340;SPINN&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;SPINN&#22312;DEM&#26041;&#27861;&#26694;&#26550;&#19979;&#65292;&#36824;&#21487;&#20197;&#35299;&#20915;&#22797;&#26434;&#20960;&#20309;&#20307;&#19978;&#30340;&#32447;&#24615;&#24377;&#24615;&#29702;&#35770;&#38382;&#39064;&#65292;&#32780;&#20256;&#32479;&#30340;PINN&#22312;&#20559;&#24494;&#20998;&#26041;&#31243;&#26694;&#26550;&#19979;&#26080;&#27861;&#23454;&#29616;&#12290;&#25152;&#32771;&#34385;&#30340;&#38382;&#39064;&#22312;&#20960;&#20309;&#24418;&#29366;&#12289;&#21152;&#36733;&#21644;&#26448;&#26009;&#21442;&#25968;&#26041;&#38754;&#38750;&#24120;&#25509;&#36817;&#24037;&#19994;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
A method for solving elasticity problems based on separable physics-informed neural networks (SPINN) in conjunction with the deep energy method (DEM) is presented. Numerical experiments have been carried out for a number of problems showing that this method has a significantly higher convergence rate and accuracy than the vanilla physics-informed neural networks (PINN) and even SPINN based on a system of partial differential equations (PDEs). In addition, using the SPINN in the framework of DEM approach it is possible to solve problems of the linear theory of elasticity on complex geometries, which is unachievable with the help of PINNs in frames of partial differential equations. Considered problems are very close to the industrial problems in terms of geometry, loading, and material parameters.
&lt;/p&gt;</description></item><item><title>AI&#24605;&#24819;&#23545;&#20010;&#20307;&#21019;&#36896;&#21147;&#27809;&#26377;&#24433;&#21709;&#65292;&#20294;&#22686;&#21152;&#20102;&#25972;&#20307;&#24605;&#24819;&#22810;&#26679;&#24615;&#30340;&#25968;&#37327;&#21644;&#21464;&#21270;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.13481</link><description>&lt;p&gt;
AI&#24605;&#24819;&#22914;&#20309;&#24433;&#21709;&#20154;&#31867;&#24605;&#24819;&#30340;&#21019;&#36896;&#21147;&#12289;&#22810;&#26679;&#24615;&#21644;&#36827;&#21270;&#65306;&#26469;&#33258;&#19968;&#20010;&#22823;&#35268;&#27169;&#21160;&#24577;&#23454;&#39564;&#30340;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
How AI Ideas Affect the Creativity, Diversity, and Evolution of Human Ideas: Evidence From a Large, Dynamic Experiment. (arXiv:2401.13481v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13481
&lt;/p&gt;
&lt;p&gt;
AI&#24605;&#24819;&#23545;&#20010;&#20307;&#21019;&#36896;&#21147;&#27809;&#26377;&#24433;&#21709;&#65292;&#20294;&#22686;&#21152;&#20102;&#25972;&#20307;&#24605;&#24819;&#22810;&#26679;&#24615;&#30340;&#25968;&#37327;&#21644;&#21464;&#21270;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#30340;&#25509;&#35302;&#27491;&#22312;&#36805;&#36895;&#22686;&#21152;&#12290;&#35266;&#30475;&#21040;AI&#29983;&#25104;&#30340;&#24605;&#24819;&#23558;&#22914;&#20309;&#24433;&#21709;&#20154;&#31867;&#24605;&#24819;&#65311;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#23454;&#39564;&#65288;800+&#21442;&#19982;&#32773;&#65292;40+&#20010;&#22269;&#23478;&#65289;&#65292;&#21442;&#19982;&#32773;&#35266;&#30475;&#20102;&#26469;&#33258;ChatGPT&#25110;&#20043;&#21069;&#23454;&#39564;&#21442;&#19982;&#32773;&#30340;&#21019;&#24847;&#24605;&#24819;&#65292;&#28982;&#21518;&#36827;&#34892;&#20102;&#33258;&#24049;&#30340;&#21019;&#24847;&#24605;&#32771;&#12290;&#25105;&#20204;&#21464;&#21270;&#20102;AI&#29983;&#25104;&#31034;&#20363;&#30340;&#25968;&#37327;&#65288;&#26080;&#12289;&#20302;&#12289;&#39640;&#26333;&#20809;&#65289;&#20197;&#21450;&#31034;&#20363;&#26159;&#21542;&#26631;&#35760;&#20026;&#8220;AI&#8221;&#65288;&#25259;&#38706;&#65289;&#12290;&#25105;&#20204;&#30340;&#21160;&#24577;&#23454;&#39564;&#35774;&#35745; - &#22312;&#21516;&#19968;&#23454;&#39564;&#26465;&#20214;&#19979;&#65292;&#20351;&#29992;&#20043;&#21069;&#21442;&#19982;&#32773;&#30340;&#24605;&#24819;&#20316;&#20026;&#26410;&#26469;&#21442;&#19982;&#32773;&#30340;&#21050;&#28608; - &#27169;&#25311;&#20102;&#25991;&#21270;&#21019;&#36896;&#30340;&#30456;&#20114;&#20381;&#36182;&#36807;&#31243;&#65306;&#21019;&#36896;&#24615;&#24605;&#24819;&#24314;&#31435;&#22312;&#20043;&#21069;&#30340;&#24605;&#24819;&#22522;&#30784;&#19978;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25429;&#25417;&#21040;&#20102;LLM&#8220;&#22312;&#25991;&#21270;&#24490;&#29615;&#20013;&#8221;&#30340;&#22797;&#21512;&#25928;&#24212;&#12290;&#25105;&#20204;&#21457;&#29616;&#39640;AI&#26333;&#20809;&#65288;&#20294;&#19981;&#26159;&#20302;AI&#26333;&#20809;&#65289;&#24182;&#27809;&#26377;&#24433;&#21709;&#20010;&#20154;&#24605;&#24819;&#30340;&#21019;&#36896;&#21147;&#65292;&#20294;&#22686;&#21152;&#20102;&#25972;&#20307;&#24605;&#24819;&#22810;&#26679;&#24615;&#30340;&#24179;&#22343;&#25968;&#37327;&#21644;&#21464;&#21270;&#36895;&#29575;&#12290;AI&#20351;&#24605;&#24819;&#22810;&#26679;&#24615;&#30340;&#32047;&#31215;&#25928;&#24212;&#22686;&#24378;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exposure to large language model output is rapidly increasing. How will seeing AI-generated ideas affect human ideas? We conducted an experiment (800+ participants, 40+ countries) where participants viewed creative ideas that were from ChatGPT or prior experimental participants and then brainstormed their own idea. We varied the number of AI-generated examples (none, low, or high exposure) and if the examples were labeled as 'AI' (disclosure). Our dynamic experiment design -- ideas from prior participants in an experimental condition are used as stimuli for future participants in the same experimental condition -- mimics the interdependent process of cultural creation: creative ideas are built upon prior ideas. Hence, we capture the compounding effects of having LLMs 'in the culture loop'. We find that high AI exposure (but not low AI exposure) did not affect the creativity of individual ideas but did increase the average amount and rate of change of collective idea diversity. AI made 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GExp&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20351;&#26426;&#22120;&#20154;&#22312;&#27809;&#26377;&#20154;&#31867;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#33258;&#20027;&#25506;&#32034;&#21644;&#23398;&#20064;&#12290;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#65292;GExp&#40723;&#21169;&#26426;&#22120;&#20154;&#36890;&#36807;&#33258;&#25105;&#29983;&#25104;&#30340;&#20219;&#21153;&#26469;&#29702;&#35299;&#21644;&#25506;&#32034;&#29615;&#22659;&#65292;&#24182;&#20174;&#26377;&#30410;&#30340;&#32463;&#39564;&#20013;&#33719;&#21462;&#25216;&#33021;&#12290;&#36825;&#20351;&#24471;&#26426;&#22120;&#20154;&#33021;&#22815;&#36890;&#36807;&#33258;&#25105;&#25506;&#32034;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2401.13462</link><description>&lt;p&gt;
&#20174;&#25506;&#32034;&#20013;&#25104;&#38271;: &#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#33258;&#25105;&#25506;&#32034;&#26426;&#22120;&#20154;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Growing from Exploration: A self-exploring framework for robots based on foundation models. (arXiv:2401.13462v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13462
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GExp&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20351;&#26426;&#22120;&#20154;&#22312;&#27809;&#26377;&#20154;&#31867;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#33258;&#20027;&#25506;&#32034;&#21644;&#23398;&#20064;&#12290;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#65292;GExp&#40723;&#21169;&#26426;&#22120;&#20154;&#36890;&#36807;&#33258;&#25105;&#29983;&#25104;&#30340;&#20219;&#21153;&#26469;&#29702;&#35299;&#21644;&#25506;&#32034;&#29615;&#22659;&#65292;&#24182;&#20174;&#26377;&#30410;&#30340;&#32463;&#39564;&#20013;&#33719;&#21462;&#25216;&#33021;&#12290;&#36825;&#20351;&#24471;&#26426;&#22120;&#20154;&#33021;&#22815;&#36890;&#36807;&#33258;&#25105;&#25506;&#32034;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#26426;&#22120;&#20154;&#26159;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#26368;&#32456;&#30446;&#26631;&#12290;&#29616;&#26377;&#24037;&#20316;&#36890;&#36807;&#22522;&#20110;&#23398;&#20064;&#25110;&#20248;&#21270;&#30340;&#26041;&#27861;&#26469;&#23436;&#25104;&#20154;&#31867;&#23450;&#20041;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#33258;&#20027;&#22320;&#25506;&#32034;&#21508;&#31181;&#29615;&#22659;&#30340;&#25361;&#25112;&#20173;&#26410;&#35299;&#20915;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GExp&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20351;&#26426;&#22120;&#20154;&#22312;&#27809;&#26377;&#20154;&#31867;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#33258;&#20027;&#25506;&#32034;&#21644;&#23398;&#20064;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#21253;&#25324;&#33258;&#25105;&#25506;&#32034;&#12289;&#30693;&#35782;&#24211;&#26500;&#24314;&#21644;&#38381;&#29615;&#21453;&#39304;&#22312;&#20869;&#30340;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#27169;&#22359;&#12290;&#21463;&#21040;&#23156;&#20799;&#19982;&#19990;&#30028;&#20114;&#21160;&#26041;&#24335;&#30340;&#21551;&#21457;&#65292;GExp&#40723;&#21169;&#26426;&#22120;&#20154;&#36890;&#36807;&#19968;&#31995;&#21015;&#33258;&#21160;&#29983;&#25104;&#30340;&#20219;&#21153;&#26469;&#29702;&#35299;&#21644;&#25506;&#32034;&#29615;&#22659;&#12290;&#22312;&#25506;&#32034;&#36807;&#31243;&#20013;&#65292;&#26426;&#22120;&#20154;&#23558;&#20174;&#26377;&#30410;&#30340;&#32463;&#39564;&#20013;&#33719;&#24471;&#26377;&#29992;&#20110;&#23558;&#26469;&#30340;&#25216;&#33021;&#12290;GExp&#20351;&#26426;&#22120;&#20154;&#36890;&#36807;&#33258;&#25105;&#25506;&#32034;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligent robot is the ultimate goal in the robotics field. Existing works leverage learning-based or optimization-based methods to accomplish human-defined tasks. However, the challenge of enabling robots to explore various environments autonomously remains unresolved. In this work, we propose a framework named GExp, which enables robots to explore and learn autonomously without human intervention. To achieve this goal, we devise modules including self-exploration, knowledge-base-building, and close-loop feedback based on foundation models. Inspired by the way that infants interact with the world, GExp encourages robots to understand and explore the environment with a series of self-generated tasks. During the process of exploration, the robot will acquire skills from beneficial experiences that are useful in the future. GExp provides robots with the ability to solve complex tasks through self-exploration. GExp work is independent of prior interactive knowledge and human interventio
&lt;/p&gt;</description></item><item><title>MADRID&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#23545;&#25239;&#22330;&#26223;&#26469;&#25581;&#31034;&#39044;&#35757;&#32451;&#22810;Agent&#31574;&#30053;&#30340;&#25112;&#30053;&#28431;&#27934;&#65292;&#24182;&#36890;&#36807;&#36951;&#25022;&#20540;&#34913;&#37327;&#28431;&#27934;&#30340;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.13460</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#26679;&#24615;&#21551;&#31034;&#30340;&#22810;Agent&#35786;&#26029;&#26041;&#27861;&#29992;&#20110;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Diagnostics for Robustness via Illuminated Diversity. (arXiv:2401.13460v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13460
&lt;/p&gt;
&lt;p&gt;
MADRID&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#23545;&#25239;&#22330;&#26223;&#26469;&#25581;&#31034;&#39044;&#35757;&#32451;&#22810;Agent&#31574;&#30053;&#30340;&#25112;&#30053;&#28431;&#27934;&#65292;&#24182;&#36890;&#36807;&#36951;&#25022;&#20540;&#34913;&#37327;&#28431;&#27934;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#22810;Agent&#31995;&#32479;&#39046;&#22495;&#20013;&#65292;&#30830;&#20445;&#22312;&#38476;&#29983;&#21644;&#25932;&#23545;&#29615;&#22659;&#20013;&#30340;&#31283;&#20581;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#36825;&#20123;&#31995;&#32479;&#22312;&#29087;&#24713;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#26032;&#24773;&#20917;&#19979;&#24448;&#24448;&#20250;&#22240;&#20026;&#35757;&#32451;&#38454;&#27573;&#30340;&#36807;&#25311;&#21512;&#32780;&#22833;&#36133;&#12290;&#22312;&#26082;&#21253;&#21547;&#21512;&#20316;&#21448;&#21253;&#21547;&#31454;&#20105;&#34892;&#20026;&#30340;&#29615;&#22659;&#20013;&#65292;&#36825;&#19968;&#38382;&#39064;&#23588;&#20026;&#31361;&#20986;&#65292;&#20307;&#29616;&#20102;&#36807;&#25311;&#21512;&#21644;&#27867;&#21270;&#25361;&#25112;&#30340;&#21452;&#37325;&#24615;&#36136;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#22810;&#26679;&#24615;&#21551;&#31034;&#30340;&#22810;Agent&#31283;&#20581;&#24615;&#35786;&#26029;&#65288;MADRID&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29983;&#25104;&#22810;Agent&#31574;&#30053;&#20013;&#26292;&#38706;&#25112;&#30053;&#28431;&#27934;&#30340;&#22810;&#26679;&#21270;&#23545;&#25239;&#22330;&#26223;&#30340;&#26032;&#26041;&#27861;&#12290;MADRID&#21033;&#29992;&#24320;&#25918;&#24335;&#23398;&#20064;&#30340;&#27010;&#24565;&#65292;&#23548;&#33322;&#23545;&#25239;&#29615;&#22659;&#30340;&#24191;&#38420;&#31354;&#38388;&#65292;&#20351;&#29992;&#30446;&#26631;&#31574;&#30053;&#30340;&#36951;&#25022;&#20540;&#26469;&#34913;&#37327;&#36825;&#20123;&#29615;&#22659;&#30340;&#28431;&#27934;&#12290;&#25105;&#20204;&#22312;11vs11&#29256;&#30340;Google Research Football&#19978;&#35780;&#20272;&#20102;MADRID&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly advancing field of multi-agent systems, ensuring robustness in unfamiliar and adversarial settings is crucial. Notwithstanding their outstanding performance in familiar environments, these systems often falter in new situations due to overfitting during the training phase. This is especially pronounced in settings where both cooperative and competitive behaviours are present, encapsulating a dual nature of overfitting and generalisation challenges. To address this issue, we present Multi-Agent Diagnostics for Robustness via Illuminated Diversity (MADRID), a novel approach for generating diverse adversarial scenarios that expose strategic vulnerabilities in pre-trained multi-agent policies. Leveraging the concepts from open-ended learning, MADRID navigates the vast space of adversarial settings, employing a target policy's regret to gauge the vulnerabilities of these settings. We evaluate the effectiveness of MADRID on the 11vs11 version of Google Research Football, one o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20197;&#20302;&#35745;&#31639;&#36164;&#28304;&#28040;&#32791;&#20026;&#20013;&#24515;&#30340;&#39640;&#25928;&#30693;&#35782;&#24211;&#38382;&#31572;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#32447;&#32034;&#24341;&#23548;&#36335;&#24452;&#25506;&#32034;&#30340;&#26041;&#24335;&#65292;&#23558;&#30693;&#35782;&#24211;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39640;&#25928;&#22320;&#34701;&#21512;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#23545;&#27169;&#22411;&#33021;&#21147;&#30340;&#35201;&#27714;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.13444</link><description>&lt;p&gt;
&#20197;&#20302;&#35745;&#31639;&#36164;&#28304;&#28040;&#32791;&#20026;&#20013;&#24515;&#30340;&#39640;&#25928;&#30693;&#35782;&#24211;&#38382;&#31572;&#26694;&#26550;&#65306;&#22522;&#20110;&#32447;&#32034;&#24341;&#23548;&#36335;&#24452;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Clue-Guided Path Exploration: An Efficient Knowledge Base Question-Answering Framework with Low Computational Resource Consumption. (arXiv:2401.13444v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13444
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20197;&#20302;&#35745;&#31639;&#36164;&#28304;&#28040;&#32791;&#20026;&#20013;&#24515;&#30340;&#39640;&#25928;&#30693;&#35782;&#24211;&#38382;&#31572;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#32447;&#32034;&#24341;&#23548;&#36335;&#24452;&#25506;&#32034;&#30340;&#26041;&#24335;&#65292;&#23558;&#30693;&#35782;&#24211;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39640;&#25928;&#22320;&#34701;&#21512;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#23545;&#27169;&#22411;&#33021;&#21147;&#30340;&#35201;&#27714;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26356;&#26032;&#23427;&#20204;&#30340;&#30693;&#35782;&#38754;&#20250;&#24102;&#26469;&#25361;&#25112;&#65292;&#24403;&#38754;&#23545;&#19981;&#29087;&#24713;&#30340;&#26597;&#35810;&#26102;&#21487;&#33021;&#23548;&#33268;&#19981;&#20934;&#30830;&#24615;&#12290;&#34429;&#28982;&#24050;&#32463;&#30740;&#31350;&#20102;&#23558;&#30693;&#35782;&#22270;&#35889;&#19982;LLMs&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#23558;LLMs&#35270;&#20026;&#20027;&#35201;&#30340;&#20915;&#31574;&#32773;&#65292;&#23545;&#20854;&#33021;&#21147;&#25552;&#20986;&#20102;&#36739;&#39640;&#30340;&#35201;&#27714;&#12290;&#23545;&#20110;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#19988;&#24615;&#33021;&#30456;&#23545;&#36739;&#24046;&#30340;LLMs&#26469;&#35828;&#65292;&#36825;&#26159;&#19981;&#22826;&#21512;&#36866;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20197;&#32447;&#32034;&#24341;&#23548;&#36335;&#24452;&#25506;&#32034;&#20026;&#26680;&#24515;&#30340;&#30693;&#35782;&#24211;&#38382;&#31572;&#26694;&#26550;&#65288;CGPE&#65289;&#65292;&#23427;&#23558;&#30693;&#35782;&#24211;&#19982;LLMs&#39640;&#25928;&#22320;&#34701;&#21512;&#65292;&#23545;&#27169;&#22411;&#30340;&#33021;&#21147;&#35201;&#27714;&#36739;&#20302;&#12290;&#21463;&#20154;&#31867;&#25163;&#21160;&#26816;&#32034;&#30693;&#35782;&#30340;&#26041;&#27861;&#21551;&#21457;&#65292;CGPE&#21033;&#29992;&#38382;&#39064;&#20013;&#30340;&#20449;&#24687;&#20316;&#20026;&#32447;&#32034;&#65292;&#31995;&#32479;&#22320;&#25506;&#32034;&#30693;&#35782;&#24211;&#20013;&#25152;&#38656;&#30340;&#30693;&#35782;&#36335;&#24452;&#12290;&#24320;&#28304;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;CGPE&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#38750;&#24120;&#36866;&#29992;&#20110;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#19988;&#24615;&#33021;&#36739;&#24046;&#30340;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent times, large language models (LLMs) have showcased remarkable capabilities. However, updating their knowledge poses challenges, potentially leading to inaccuracies when confronted with unfamiliar queries. While integrating knowledge graphs with LLMs has been explored, existing approaches treat LLMs as primary decision-makers, imposing high demands on their capabilities. This is particularly unsuitable for LLMs with lower computational costs and relatively poorer performance. In this paper, we introduce a Clue-Guided Path Exploration framework (CGPE) that efficiently merges a knowledge base with an LLM, placing less stringent requirements on the model's capabilities. Inspired by the method humans use to manually retrieve knowledge, CGPE employs information from the question as clues to systematically explore the required knowledge path within the knowledge base. Experiments on open-source datasets reveal that CGPE outperforms previous methods and is highly applicable to LLMs w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#32806;&#21512;&#34180;&#26495;&#26679;&#26465;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#26059;&#36716;&#26657;&#27491;&#31561;&#22270;&#20687;&#21464;&#24418;&#20219;&#21153;&#12290;&#36890;&#36807;&#32806;&#21512;&#22810;&#20010;&#34180;&#26495;&#26679;&#26465;&#21464;&#25442;&#65292;&#26377;&#25928;&#28040;&#38500;&#20102;&#25554;&#20540;&#35823;&#24046;&#65292;&#31361;&#30772;&#20102;&#25511;&#21046;&#28857;&#25968;&#37327;&#29942;&#39048;&#65292;&#24182;&#19988;&#36890;&#36807;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#24102;&#26631;&#27880;&#26679;&#26412;&#26465;&#20214;&#19979;&#65292;&#20805;&#20998;&#21033;&#29992;&#26410;&#26631;&#27880;&#26679;&#26412;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.13432</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;&#32806;&#21512;&#34180;&#26495;&#26679;&#26465;&#27169;&#22411;&#29992;&#20110;&#26059;&#36716;&#26657;&#27491;&#21450;&#26356;&#22810;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Coupled Thin-Plate Spline Model for Rotation Correction and Beyond. (arXiv:2401.13432v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13432
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#32806;&#21512;&#34180;&#26495;&#26679;&#26465;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#26059;&#36716;&#26657;&#27491;&#31561;&#22270;&#20687;&#21464;&#24418;&#20219;&#21153;&#12290;&#36890;&#36807;&#32806;&#21512;&#22810;&#20010;&#34180;&#26495;&#26679;&#26465;&#21464;&#25442;&#65292;&#26377;&#25928;&#28040;&#38500;&#20102;&#25554;&#20540;&#35823;&#24046;&#65292;&#31361;&#30772;&#20102;&#25511;&#21046;&#28857;&#25968;&#37327;&#29942;&#39048;&#65292;&#24182;&#19988;&#36890;&#36807;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#24102;&#26631;&#27880;&#26679;&#26412;&#26465;&#20214;&#19979;&#65292;&#20805;&#20998;&#21033;&#29992;&#26410;&#26631;&#27880;&#26679;&#26412;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34180;&#26495;&#26679;&#26465;&#65288;TPS&#65289;&#26159;&#19968;&#31181;&#20801;&#35768;&#20351;&#29992;&#25511;&#21046;&#28857;&#36816;&#21160;&#34920;&#31034;&#24377;&#24615;&#38750;&#32447;&#24615;&#21464;&#25442;&#30340;&#20027;&#35201;&#21464;&#24418;&#26041;&#24335;&#12290;&#38543;&#30528;&#25511;&#21046;&#28857;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#21464;&#24418;&#36234;&#26469;&#36234;&#28789;&#27963;&#65292;&#20294;&#24120;&#24120;&#20250;&#36935;&#21040;&#30001;&#19981;&#24076;&#26395;&#30340;&#38382;&#39064;&#65288;&#22914;&#20869;&#23481;&#30072;&#21464;&#65289;&#23548;&#33268;&#30340;&#29942;&#39048;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;TPS&#22312;&#22522;&#20110;&#21333;&#24133;&#22270;&#20687;&#30340;&#21464;&#24418;&#20219;&#21153;&#20013;&#30340;&#36890;&#29992;&#24212;&#29992;&#65292;&#22914;&#26059;&#36716;&#26657;&#27491;&#12289;&#30697;&#24418;&#21270;&#21644;&#32918;&#20687;&#26657;&#27491;&#12290;&#20026;&#20102;&#31361;&#30772;&#36825;&#20010;&#29942;&#39048;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32806;&#21512;&#34180;&#26495;&#26679;&#26465;&#27169;&#22411;&#65288;CoupledTPS&#65289;&#65292;&#23427;&#23558;&#26377;&#38480;&#25511;&#21046;&#28857;&#30340;&#22810;&#20010;TPS&#36845;&#20195;&#32806;&#21512;&#25104;&#19968;&#20010;&#26356;&#28789;&#27963;&#21644;&#24378;&#22823;&#30340;&#21464;&#25442;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;&#36845;&#20195;&#25628;&#32034;&#26469;&#26681;&#25454;&#24403;&#21069;&#28508;&#22312;&#26465;&#20214;&#39044;&#27979;&#26032;&#30340;&#25511;&#21046;&#28857;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20197;&#21464;&#24418;&#27969;&#20316;&#20026;&#36830;&#25509;&#19981;&#21516;TPS&#21464;&#25442;&#30340;&#26725;&#26753;&#65292;&#26377;&#25928;&#28040;&#38500;&#20102;&#22810;&#20010;&#21464;&#24418;&#24341;&#36215;&#30340;&#25554;&#20540;&#35823;&#24046;&#12290;&#27492;&#22806;&#65292;&#37492;&#20110;&#32321;&#29712;&#30340;&#27880;&#37322;&#25104;&#20026;&#21046;&#32422;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25928;&#26524;&#30340;&#37325;&#35201;&#22240;&#32032;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#24471;&#22312;&#26377;&#38480;&#24102;&#26631;&#27880;&#26679;&#26412;&#26465;&#20214;&#19979;&#65292;&#33021;&#20805;&#20998;&#21033;&#29992;&#26410;&#26631;&#27880;&#26679;&#26412;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Thin-plate spline (TPS) is a principal warp that allows for representing elastic, nonlinear transformation with control point motions. With the increase of control points, the warp becomes increasingly flexible but usually encounters a bottleneck caused by undesired issues, e.g., content distortion. In this paper, we explore generic applications of TPS in single-image-based warping tasks, such as rotation correction, rectangling, and portrait correction. To break this bottleneck, we propose the coupled thin-plate spline model (CoupledTPS), which iteratively couples multiple TPS with limited control points into a more flexible and powerful transformation. Concretely, we first design an iterative search to predict new control points according to the current latent condition. Then, we present the warping flow as a bridge for the coupling of different TPS transformations, effectively eliminating interpolation errors caused by multiple warps. Besides, in light of the laborious annotation co
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22240;&#26524;&#24863;&#30693;&#30340;&#27010;&#24565;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#33258;&#21160;&#20915;&#31574;&#31995;&#32479;&#20013;&#12290;&#24863;&#30693;&#23545;&#20915;&#31574;&#30340;&#20844;&#24179;&#24615;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#22240;&#20026;&#20844;&#24179;&#24615;&#26159;&#19982;&#32972;&#26223;&#30456;&#20851;&#30340;&#65292;&#24182;&#19988;&#20854;&#35299;&#37322;&#21462;&#20915;&#20110;&#35780;&#21028;&#20154;&#26159;&#35841;&#12290;</title><link>http://arxiv.org/abs/2401.13408</link><description>&lt;p&gt;
&#22240;&#26524;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Causal Perception. (arXiv:2401.13408v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13408
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22240;&#26524;&#24863;&#30693;&#30340;&#27010;&#24565;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#33258;&#21160;&#20915;&#31574;&#31995;&#32479;&#20013;&#12290;&#24863;&#30693;&#23545;&#20915;&#31574;&#30340;&#20844;&#24179;&#24615;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#22240;&#20026;&#20844;&#24179;&#24615;&#26159;&#19982;&#32972;&#26223;&#30456;&#20851;&#30340;&#65292;&#24182;&#19988;&#20854;&#35299;&#37322;&#21462;&#20915;&#20110;&#35780;&#21028;&#20154;&#26159;&#35841;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20004;&#20010;&#20010;&#20307;&#23545;&#30456;&#21516;&#30340;&#20449;&#24687;&#36827;&#34892;&#19981;&#21516;&#35299;&#35835;&#26102;&#65292;&#24863;&#30693;&#20250;&#21457;&#29983;&#12290;&#23613;&#31649;&#36825;&#26159;&#19968;&#20010;&#24050;&#30693;&#29616;&#35937;&#65292;&#23545;&#20915;&#31574;&#20013;&#20559;&#35265;&#26377;&#24433;&#21709;&#65292;&#20294;&#26159;&#24863;&#30693;&#22312;&#33258;&#21160;&#20915;&#31574;&#31995;&#32479;&#20013;&#20173;&#28982;&#34987;&#24573;&#35270;&#12290;&#24863;&#30693;&#23545;&#20110;ADM&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#25110;&#20844;&#24179;&#20351;&#29992;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#22240;&#20026;&#20844;&#24179;&#26412;&#36523;&#26159;&#19982;&#32972;&#26223;&#30456;&#20851;&#30340;&#65292;&#20854;&#35299;&#37322;&#21462;&#20915;&#20110;&#35780;&#21028;&#20154;&#26159;&#35841;&#12290;&#26412;&#25991;&#23558;&#24863;&#30693;&#22312;&#22240;&#26524;&#25512;&#29702;&#20013;&#24418;&#24335;&#21270;&#65292;&#20197;&#25429;&#25417;&#20010;&#20307;&#30340;&#35299;&#37322;&#34892;&#20026;&#12290;&#25105;&#20204;&#36824;&#23558;&#20010;&#20307;&#32463;&#39564;&#24418;&#24335;&#21270;&#20026;&#39069;&#22806;&#30340;&#22240;&#26524;&#30693;&#35782;&#65292;&#20010;&#20307;&#20250;&#20351;&#29992;&#36825;&#20123;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23450;&#20041;&#21644;&#35752;&#35770;&#20102;&#26131;&#24341;&#21457;&#24863;&#30693;&#30340;&#23646;&#24615;&#65292;&#21363;&#26131;&#24341;&#21457;&#24863;&#30693;&#30340;&#23646;&#24615;&#12290;&#25935;&#24863;&#23646;&#24615;&#65292;&#22914;&#24615;&#21035;&#21644;&#31181;&#26063;&#65292;&#23601;&#26159;&#26131;&#24341;&#21457;&#24863;&#30693;&#30340;&#26126;&#30830;&#31034;&#20363;&#12290;&#25105;&#20204;&#26681;&#25454;&#22240;&#26524;&#21407;&#21017;&#23450;&#20041;&#20102;&#20004;&#31181;&#24863;&#30693;&#65292;&#21363;&#19981;&#24544;&#23454;&#24863;&#30693;&#21644;&#19981;&#19968;&#33268;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;
Perception occurs when two individuals interpret the same information differently. Despite being a known phenomenon with implications for bias in decision-making, as individuals' experience determines interpretation, perception remains largely overlooked in automated decision-making (ADM) systems. In particular, it can have considerable effects on the fairness or fair usage of an ADM system, as fairness itself is context-specific and its interpretation dependent on who is judging. In this work, we formalize perception under causal reasoning to capture the act of interpretation by an individual. We also formalize individual experience as additional causal knowledge that comes with and is used by an individual. Further, we define and discuss loaded attributes, which are attributes prone to evoke perception. Sensitive attributes, such as gender and race, are clear examples of loaded attributes. We define two kinds of causal perception, unfaithful and inconsistent, based on the causal prop
&lt;/p&gt;</description></item><item><title>UMBRELLA&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#29289;&#32852;&#32593;&#35797;&#39564;&#24179;&#21488;&#65292;&#20855;&#26377;&#22810;&#20010;&#24212;&#29992;&#26696;&#20363;&#65292;&#21253;&#25324;&#33258;&#21160;&#34903;&#28783;&#30417;&#25511;&#12289;&#25968;&#23383;&#23402;&#29983;&#29615;&#22659;&#12289;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#21644;&#23481;&#22120;&#21270;&#24212;&#29992;&#20837;&#20405;&#26816;&#27979;&#12290;&#26410;&#26469;&#65292;UMBRELLA&#36824;&#26377;&#28508;&#21147;&#29992;&#20110;&#26234;&#33021;&#22478;&#24066;&#21644;&#22810;&#26426;&#22120;&#20154;&#32676;&#24863;&#30693;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.13346</link><description>&lt;p&gt;
&#36807;&#21435;&#12289;&#29616;&#22312;&#12289;&#26410;&#26469;&#65306;&#23545;UMBRELLA&#29289;&#32852;&#32593;&#35797;&#39564;&#24179;&#21488;&#20013;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#26696;&#20363;&#30340;&#20840;&#38754;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Past, Present, Future: A Comprehensive Exploration of AI Use Cases in the UMBRELLA IoT Testbed. (arXiv:2401.13346v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13346
&lt;/p&gt;
&lt;p&gt;
UMBRELLA&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#29289;&#32852;&#32593;&#35797;&#39564;&#24179;&#21488;&#65292;&#20855;&#26377;&#22810;&#20010;&#24212;&#29992;&#26696;&#20363;&#65292;&#21253;&#25324;&#33258;&#21160;&#34903;&#28783;&#30417;&#25511;&#12289;&#25968;&#23383;&#23402;&#29983;&#29615;&#22659;&#12289;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#21644;&#23481;&#22120;&#21270;&#24212;&#29992;&#20837;&#20405;&#26816;&#27979;&#12290;&#26410;&#26469;&#65292;UMBRELLA&#36824;&#26377;&#28508;&#21147;&#29992;&#20110;&#26234;&#33021;&#22478;&#24066;&#21644;&#22810;&#26426;&#22120;&#20154;&#32676;&#24863;&#30693;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
UMBRELLA&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#24320;&#25918;&#24335;&#29289;&#32852;&#32593;&#29983;&#24577;&#31995;&#32479;&#65292;&#21253;&#25324;200&#22810;&#20010;&#22810;&#20256;&#24863;&#22120;&#22810;&#26080;&#32447;&#33410;&#28857;&#12289;20&#20010;&#21327;&#20316;&#26426;&#22120;&#20154;&#21644;&#25903;&#25345;&#36793;&#32536;&#26234;&#33021;&#30340;&#35774;&#22791;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;UMBRELLA&#22312;&#23454;&#38469;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#24050;&#23454;&#29616;&#21644;&#28508;&#22312;&#30340;&#20154;&#24037;&#26234;&#33021;&#33021;&#21147;&#30340;&#25351;&#21335;&#12290;&#35814;&#32454;&#20171;&#32461;&#20102;&#22235;&#20010;&#29616;&#26377;&#30340;UMBRELLA&#24212;&#29992;&#31243;&#24207;&#65306;1&#65289;&#29992;&#20110;&#26816;&#27979;&#38382;&#39064;&#24182;&#35302;&#21457;&#32500;&#25252;&#35686;&#25253;&#30340;&#33258;&#21160;&#34903;&#28783;&#30417;&#25511;&#65307;2&#65289;&#25552;&#20379;&#22686;&#24378;&#30340;&#31354;&#27668;&#36136;&#37327;&#24863;&#30693;&#21644;&#38477;&#20302;&#25104;&#26412;&#30340;&#24314;&#31569;&#29615;&#22659;&#25968;&#23383;&#23402;&#29983;&#65307;3&#65289;&#29992;&#20110;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#30340;&#22823;&#35268;&#27169;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65307;4&#65289;&#29992;&#20110;&#35782;&#21035;&#24694;&#24847;&#27963;&#21160;&#30340;&#23481;&#22120;&#21270;&#24212;&#29992;&#20837;&#20405;&#26816;&#27979;&#12290;&#27492;&#22806;&#65292;&#36824;&#27010;&#36848;&#20102;UMBRELLA&#22312;&#26410;&#26469;&#26234;&#33021;&#22478;&#24066;&#21644;&#22810;&#26426;&#22120;&#20154;&#32676;&#24863;&#30693;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#65292;&#22686;&#24378;&#20102;&#35821;&#20041;&#36890;&#20449;&#21644;&#22810;&#26234;&#33021;&#20307;&#35268;&#21010;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#23454;&#29616;&#19978;&#36848;&#29992;&#20363;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;UMBRELLA&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
UMBRELLA is a large-scale, open-access Internet of Things (IoT) ecosystem incorporating over 200 multi-sensor multi-wireless nodes, 20 collaborative robots, and edge-intelligence-enabled devices. This paper provides a guide to the implemented and prospective artificial intelligence (AI) capabilities of UMBRELLA in real-world IoT systems. Four existing UMBRELLA applications are presented in detail: 1) An automated streetlight monitoring for detecting issues and triggering maintenance alerts; 2) A Digital twin of building environments providing enhanced air quality sensing with reduced cost; 3) A large-scale Federated Learning framework for reducing communication overhead; and 4) An intrusion detection for containerised applications identifying malicious activities. Additionally, the potential of UMBRELLA is outlined for future smart city and multi-robot crowdsensing applications enhanced by semantic communications and multi-agent planning. Finally, to realise the above use-cases we disc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#26174;&#33879;&#24615;&#26816;&#39564;&#26041;&#27861;&#65288;nFBST&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#25311;&#21512;&#38750;&#32447;&#24615;&#21644;&#22810;&#32500;&#20851;&#31995;&#65292;&#24182;&#35745;&#31639;&#35777;&#25454;&#20540;&#26469;&#26367;&#20195;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#29702;&#35770;&#25512;&#23548;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#27979;&#35797;&#20840;&#23616;&#12289;&#23616;&#37096;&#21644;&#23454;&#20363;&#32423;&#30340;&#26174;&#33879;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.13335</link><description>&lt;p&gt;
&#20840;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#26174;&#33879;&#24615;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Full Bayesian Significance Testing for Neural Networks. (arXiv:2401.13335v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13335
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#26174;&#33879;&#24615;&#26816;&#39564;&#26041;&#27861;&#65288;nFBST&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#25311;&#21512;&#38750;&#32447;&#24615;&#21644;&#22810;&#32500;&#20851;&#31995;&#65292;&#24182;&#35745;&#31639;&#35777;&#25454;&#20540;&#26469;&#26367;&#20195;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#29702;&#35770;&#25512;&#23548;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#27979;&#35797;&#20840;&#23616;&#12289;&#23616;&#37096;&#21644;&#23454;&#20363;&#32423;&#30340;&#26174;&#33879;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26174;&#33879;&#24615;&#26816;&#39564;&#26088;&#22312;&#30830;&#23450;&#32473;&#23450;&#35266;&#27979;&#32467;&#26524;&#65292;&#20851;&#20110;&#24635;&#20307;&#20998;&#24067;&#30340;&#21629;&#39064;&#26159;&#21542;&#20026;&#30495;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#26174;&#33879;&#24615;&#26816;&#39564;&#36890;&#24120;&#38656;&#35201;&#25512;&#23548;&#20986;&#26816;&#39564;&#32479;&#35745;&#37327;&#30340;&#20998;&#24067;&#65292;&#26080;&#27861;&#22788;&#29702;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20840;&#36125;&#21494;&#26031;&#26174;&#33879;&#24615;&#26816;&#39564;&#26041;&#27861;&#65292;&#31216;&#20026;nFBST&#65292;&#26088;&#22312;&#20811;&#26381;&#20256;&#32479;&#26041;&#27861;&#22312;&#20851;&#31995;&#34920;&#24449;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#21033;&#29992;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#25311;&#21512;&#38750;&#32447;&#24615;&#21644;&#22810;&#32500;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#35777;&#25454;&#20540;&#32780;&#19981;&#26159;&#36827;&#34892;&#32321;&#29712;&#30340;&#29702;&#35770;&#25512;&#23548;&#26469;&#36991;&#20813;&#38169;&#35823;&#12290;&#27492;&#22806;&#65292;nFBST&#36824;&#21487;&#20197;&#27979;&#35797;&#20840;&#23616;&#12289;&#23616;&#37096;&#21644;&#23454;&#20363;&#32423;&#30340;&#26174;&#33879;&#24615;&#65292;&#36825;&#26159;&#20043;&#21069;&#30340;&#26816;&#39564;&#26041;&#27861;&#25152;&#19981;&#20851;&#27880;&#30340;&#12290;&#27492;&#22806;&#65292;nFBST&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21487;&#20197;&#26681;&#25454;&#25152;&#36873;&#30340;&#24230;&#37327;&#36827;&#34892;&#25193;&#23637;&#65292;&#22914;Grad-nFBST&#65292;LRP-nFBST&#65292;DeepLIFT-nFBST&#12290;
&lt;/p&gt;
&lt;p&gt;
Significance testing aims to determine whether a proposition about the population distribution is the truth or not given observations. However, traditional significance testing often needs to derive the distribution of the testing statistic, failing to deal with complex nonlinear relationships. In this paper, we propose to conduct Full Bayesian Significance Testing for neural networks, called \textit{n}FBST, to overcome the limitation in relationship characterization of traditional approaches. A Bayesian neural network is utilized to fit the nonlinear and multi-dimensional relationships with small errors and avoid hard theoretical derivation by computing the evidence value. Besides, \textit{n}FBST can test not only global significance but also local and instance-wise significance, which previous testing methods don't focus on. Moreover, \textit{n}FBST is a general framework that can be extended based on the measures selected, such as Grad-\textit{n}FBST, LRP-\textit{n}FBST, DeepLIFT-\t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#24615;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;TNTRules&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35299;&#37322;&#65292;&#22635;&#34917;&#20102;&#36125;&#21494;&#26031;&#20248;&#21270;&#21644;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#30340;&#38388;&#38553;&#12290;</title><link>http://arxiv.org/abs/2401.13334</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Explainable Bayesian Optimization. (arXiv:2401.13334v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#24615;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;TNTRules&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35299;&#37322;&#65292;&#22635;&#34917;&#20102;&#36125;&#21494;&#26031;&#20248;&#21270;&#21644;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#30340;&#38388;&#38553;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;&#39046;&#22495;&#65292;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#21442;&#25968;&#35843;&#20248;&#30340;&#25511;&#21046;&#31995;&#32479;&#20013;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36817;&#20284;&#35823;&#24046;&#21644;&#31616;&#21270;&#30446;&#26631;&#65292;BO&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#33021;&#20559;&#31163;&#20154;&#31867;&#19987;&#23478;&#30340;&#30495;&#23454;&#30446;&#26631;&#65292;&#38656;&#35201;&#21518;&#32493;&#35843;&#25972;&#12290;BO&#30340;&#40657;&#30418;&#29305;&#24615;&#38480;&#21046;&#20102;&#21327;&#20316;&#35843;&#20248;&#36807;&#31243;&#65292;&#22240;&#20026;&#19987;&#23478;&#19981;&#20449;&#20219;BO&#30340;&#24314;&#35758;&#12290;&#30446;&#21069;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#20248;&#21270;&#38382;&#39064;&#65292;&#22240;&#27492;&#26080;&#27861;&#35299;&#20915;&#27492;&#38388;&#38553;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#38388;&#38553;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TNTRules&#65288;TUNE-NOTUNE&#35268;&#21017;&#65289;&#65292;&#19968;&#31181;&#20107;&#21518;&#22522;&#20110;&#35268;&#21017;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#30446;&#26631;&#20248;&#21270;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#23545;&#22522;&#20934;&#20248;&#21270;&#38382;&#39064;&#21644;&#23454;&#38469;&#36229;&#21442;&#25968;&#20248;&#21270;&#20219;&#21153;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;TNTRules&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#35299;&#37322;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;XAI&#26041;&#27861;&#12290;&#36825;&#39033;&#24037;&#20316;&#23545;BO&#21644;XAI&#30340;&#20132;&#21449;&#39046;&#22495;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In industry, Bayesian optimization (BO) is widely applied in the human-AI collaborative parameter tuning of cyber-physical systems. However, BO's solutions may deviate from human experts' actual goal due to approximation errors and simplified objectives, requiring subsequent tuning. The black-box nature of BO limits the collaborative tuning process because the expert does not trust the BO recommendations. Current explainable AI (XAI) methods are not tailored for optimization and thus fall short of addressing this gap. To bridge this gap, we propose TNTRules (TUNE-NOTUNE Rules), a post-hoc, rule-based explainability method that produces high quality explanations through multiobjective optimization. Our evaluation of benchmark optimization problems and real-world hyperparameter optimization tasks demonstrates TNTRules' superiority over state-of-the-art XAI methods in generating high quality explanations. This work contributes to the intersection of BO and XAI, providing interpretable opt
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21463;&#31639;&#27861;&#20915;&#31574;&#24433;&#21709;&#30340;&#20154;&#30340;&#20449;&#24687;&#38656;&#27714;&#65292;&#21457;&#29616;&#35299;&#37322;&#24448;&#24448;&#19981;&#33021;&#28385;&#36275;&#20182;&#20204;&#30340;&#20851;&#27880;&#28857;&#65292;&#23548;&#33268;&#23545;&#30417;&#31649;&#26694;&#26550;&#30340;&#29702;&#35299;&#21644;&#36981;&#23432;&#20135;&#29983;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#22242;&#38431;&#25552;&#20986;&#20102;XAI&#21021;&#23398;&#32773;&#38382;&#39064;&#24211;&#65292;&#28085;&#30422;&#20102;&#23601;&#19994;&#39044;&#27979;&#21644;&#20581;&#24247;&#30417;&#27979;&#20004;&#20010;&#39046;&#22495;&#20013;&#21463;&#24433;&#21709;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2401.13324</link><description>&lt;p&gt;
&#26377;&#20851;&#31639;&#27861;&#20915;&#31574;&#30340;&#20449;&#24687;&#65306;&#25506;&#32034;&#21463;&#21040;&#31639;&#27861;&#20915;&#31574;&#24433;&#21709;&#30340;&#20154;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information That Matters: Exploring Information Needs of People Affected by Algorithmic Decisions. (arXiv:2401.13324v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21463;&#31639;&#27861;&#20915;&#31574;&#24433;&#21709;&#30340;&#20154;&#30340;&#20449;&#24687;&#38656;&#27714;&#65292;&#21457;&#29616;&#35299;&#37322;&#24448;&#24448;&#19981;&#33021;&#28385;&#36275;&#20182;&#20204;&#30340;&#20851;&#27880;&#28857;&#65292;&#23548;&#33268;&#23545;&#30417;&#31649;&#26694;&#26550;&#30340;&#29702;&#35299;&#21644;&#36981;&#23432;&#20135;&#29983;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#22242;&#38431;&#25552;&#20986;&#20102;XAI&#21021;&#23398;&#32773;&#38382;&#39064;&#24211;&#65292;&#28085;&#30422;&#20102;&#23601;&#19994;&#39044;&#27979;&#21644;&#20581;&#24247;&#30417;&#27979;&#20004;&#20010;&#39046;&#22495;&#20013;&#21463;&#24433;&#21709;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#31995;&#32479;&#30340;&#35299;&#37322;&#24456;&#23569;&#28041;&#21450;&#21040;&#21463;&#31639;&#27861;&#20915;&#31574;&#24433;&#21709;&#30340;&#20154;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;&#36825;&#31181;&#20256;&#36798;&#20449;&#24687;&#19982;&#21463;&#24433;&#21709;&#21033;&#30410;&#30456;&#20851;&#32773;&#25152;&#20851;&#24515;&#30340;&#20449;&#24687;&#20043;&#38388;&#30340;&#24046;&#36317;&#21487;&#33021;&#38459;&#30861;&#23545;&#30417;&#31649;&#26694;&#26550;&#65288;&#22914;AI&#27861;&#26696;&#65289;&#30340;&#29702;&#35299;&#21644;&#36981;&#23432;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;XAI&#21021;&#23398;&#32773;&#38382;&#39064;&#24211;&#8221;&#65306;&#36825;&#26159;&#19968;&#20010;&#28085;&#30422;&#20004;&#20010;&#31639;&#27861;&#20915;&#31574;&#24212;&#29992;&#39046;&#22495;&#65288;&#23601;&#19994;&#39044;&#27979;&#21644;&#20581;&#24247;&#30417;&#27979;&#65289;&#20013;&#21463;&#24433;&#21709;&#21033;&#30410;&#30456;&#20851;&#32773;&#20449;&#24687;&#38656;&#27714;&#30340;&#30446;&#24405;&#65292;&#21253;&#25324;&#25968;&#25454;&#12289;&#31995;&#32479;&#32972;&#26223;&#12289;&#31995;&#32479;&#20351;&#29992;&#21644;&#31995;&#32479;&#35268;&#33539;&#31561;&#31867;&#21035;&#12290;&#20449;&#24687;&#38656;&#27714;&#26159;&#36890;&#36807;&#35775;&#35848;&#30740;&#31350;&#25910;&#38598;&#30340;&#65292;&#21442;&#19982;&#32773;&#26681;&#25454;&#33258;&#24049;&#30340;&#38382;&#39064;&#33719;&#24471;&#35299;&#37322;&#12290;&#21442;&#19982;&#32773;&#36824;&#25253;&#21578;&#20102;&#20182;&#20204;&#30340;&#29702;&#35299;&#21644;&#20915;&#31574;&#20449;&#24515;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#23613;&#31649;&#22312;&#25509;&#21463;&#35299;&#37322;&#21518;&#20449;&#24515;&#20542;&#21521;&#20110;&#22686;&#21152;&#65292;&#20294;&#21442;&#19982;&#32773;&#20063;&#38754;&#20020;&#30528;&#29702;&#35299;&#19978;&#30340;&#25361;&#25112;&#65292;&#22914;&#26080;&#27861;&#35299;&#37322;&#20026;&#20160;&#20040;&#33258;&#24049;&#30340;&#29702;&#35299;&#24863;&#35273;&#19981;&#23436;&#25972;&#12290;&#35299;&#37322;&#36824;&#23545;&#29702;&#35299;&#20135;&#29983;&#20102;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explanations of AI systems rarely address the information needs of people affected by algorithmic decision-making (ADM). This gap between conveyed information and information that matters to affected stakeholders can impede understanding and adherence to regulatory frameworks such as the AI Act. To address this gap, we present the "XAI Novice Question Bank": A catalog of affected stakeholders' information needs in two ADM use cases (employment prediction and health monitoring), covering the categories data, system context, system usage, and system specifications. Information needs were gathered in an interview study where participants received explanations in response to their inquiries. Participants further reported their understanding and decision confidence, showing that while confidence tended to increase after receiving explanations, participants also met understanding challenges, such as being unable to tell why their understanding felt incomplete. Explanations further influenced
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CycleGAN&#30340;&#26694;&#26550;&#65292;&#23558;&#20351;&#29992;&#24120;&#35268;&#30333;&#20809;&#25104;&#20687;&#65288;WLI&#65289;&#25429;&#33719;&#30340;&#22270;&#20687;&#36716;&#21270;&#20026;&#21512;&#25104;&#31364;&#24102;&#25104;&#20687;&#65288;SNBI&#65289;&#65292;&#29992;&#20110;&#25913;&#36827;&#27809;&#26377;&#31364;&#24102;&#25104;&#20687;&#65288;NBI&#65289;&#26102;&#30340;WLI&#19978;&#30340;&#24687;&#32905;&#26816;&#27979;&#25928;&#26524;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#21512;&#25104;NBI&#22270;&#20687;&#36827;&#34892;&#30446;&#26631;&#26816;&#27979;&#21487;&#20197;&#23454;&#29616;&#27604;&#21407;&#22987;WLI&#22270;&#20687;&#26356;&#22909;&#30340;&#24687;&#32905;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2401.13315</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#25913;&#36827;&#21512;&#25104;&#31364;&#24102;&#25104;&#20687;&#20013;&#30340;&#24687;&#32905;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for Improved Polyp Detection from Synthetic Narrow-Band Imaging. (arXiv:2401.13315v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CycleGAN&#30340;&#26694;&#26550;&#65292;&#23558;&#20351;&#29992;&#24120;&#35268;&#30333;&#20809;&#25104;&#20687;&#65288;WLI&#65289;&#25429;&#33719;&#30340;&#22270;&#20687;&#36716;&#21270;&#20026;&#21512;&#25104;&#31364;&#24102;&#25104;&#20687;&#65288;SNBI&#65289;&#65292;&#29992;&#20110;&#25913;&#36827;&#27809;&#26377;&#31364;&#24102;&#25104;&#20687;&#65288;NBI&#65289;&#26102;&#30340;WLI&#19978;&#30340;&#24687;&#32905;&#26816;&#27979;&#25928;&#26524;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#21512;&#25104;NBI&#22270;&#20687;&#36827;&#34892;&#30446;&#26631;&#26816;&#27979;&#21487;&#20197;&#23454;&#29616;&#27604;&#21407;&#22987;WLI&#22270;&#20687;&#26356;&#22909;&#30340;&#24687;&#32905;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#24212;&#23545;&#32467;&#30452;&#32928;&#30284;&#65288;CRC&#65289;&#26085;&#30410;&#22686;&#38271;&#30340;&#24739;&#30149;&#29575;&#65292;&#24687;&#32905;&#26816;&#27979;&#21644;&#20999;&#38500;&#30340;&#31579;&#26597;&#35745;&#21010;&#24050;&#32463;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#32467;&#32928;&#38236;&#34987;&#35748;&#20026;&#26159;CRC&#31579;&#26597;&#30340;&#26368;&#20339;&#26041;&#27861;&#12290;&#20026;&#20102;&#31616;&#21270;&#26816;&#26597;&#36807;&#31243;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#24687;&#32905;&#26816;&#27979;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#20256;&#32479;&#30340;&#30333;&#20809;&#25104;&#20687;&#65288;WLI&#65289;&#12290;&#19982;WLI&#30456;&#27604;&#65292;&#31364;&#24102;&#25104;&#20687;&#65288;NBI&#65289;&#22312;&#32467;&#32928;&#38236;&#26816;&#26597;&#36807;&#31243;&#20013;&#21487;&#20197;&#25913;&#21892;&#24687;&#32905;&#20998;&#31867;&#65292;&#20294;&#38656;&#35201;&#29305;&#27530;&#35774;&#22791;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CycleGAN&#30340;&#26694;&#26550;&#65292;&#23558;&#20351;&#29992;&#24120;&#35268;WLI&#25429;&#33719;&#30340;&#22270;&#20687;&#36716;&#21270;&#20026;&#21512;&#25104;NBI&#65288;SNBI&#65289;&#65292;&#20316;&#20026;&#22312;&#27809;&#26377;NBI&#26102;&#25913;&#36827;WLI&#19978;&#30340;&#30446;&#26631;&#26816;&#27979;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;&#12290;&#26412;&#25991;&#39318;&#20808;&#23637;&#31034;&#20102;&#19982;&#30456;&#23545;&#30456;&#20284;&#30340;WLI&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;NBI&#19978;&#21487;&#20197;&#21462;&#24471;&#26356;&#22909;&#30340;&#24687;&#32905;&#26816;&#27979;&#32467;&#26524;&#12290;&#20854;&#27425;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20174;WLI&#29983;&#25104;&#30340;SNBI&#22270;&#20687;&#19978;&#36827;&#34892;&#30340;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#24577;&#36716;&#25442;&#21487;&#20197;&#23454;&#29616;&#27604;&#21407;&#22987;WLI&#19978;&#26356;&#22909;&#30340;&#24687;&#32905;&#26816;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
To cope with the growing prevalence of colorectal cancer (CRC), screening programs for polyp detection and removal have proven their usefulness. Colonoscopy is considered the best-performing procedure for CRC screening. To ease the examination, deep learning based methods for automatic polyp detection have been developed for conventional white-light imaging (WLI). Compared with WLI, narrow-band imaging (NBI) can improve polyp classification during colonoscopy but requires special equipment. We propose a CycleGAN-based framework to convert images captured with regular WLI to synthetic NBI (SNBI) as a pre-processing method for improving object detection on WLI when NBI is unavailable. This paper first shows that better results for polyp detection can be achieved on NBI compared to a relatively similar dataset of WLI. Secondly, experimental results demonstrate that our proposed modality translation can achieve improved polyp detection on SNBI images generated from WLI compared to the orig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;ConTextual&#65292;&#29992;&#20110;&#35780;&#20272;&#33021;&#22815;&#36827;&#34892;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#25991;&#26412;&#23500;&#26377;&#35270;&#35273;&#25512;&#29702;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30446;&#21069;&#26368;&#22909;&#30340;&#27169;&#22411;GPT-4V&#22312;&#25277;&#35937;&#31867;&#21035;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#25972;&#20307;&#24615;&#33021;&#19978;&#20173;&#28982;&#33853;&#21518;&#20110;&#20154;&#31867;&#65292;&#23384;&#22312;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2401.13311</link><description>&lt;p&gt;
ConTextual: &#22312;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#35780;&#20272;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#25991;&#26412;&#23500;&#26377;&#35270;&#35273;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
ConTextual: Evaluating Context-Sensitive Text-Rich Visual Reasoning in Large Multimodal Models. (arXiv:2401.13311v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;ConTextual&#65292;&#29992;&#20110;&#35780;&#20272;&#33021;&#22815;&#36827;&#34892;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#25991;&#26412;&#23500;&#26377;&#35270;&#35273;&#25512;&#29702;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30446;&#21069;&#26368;&#22909;&#30340;&#27169;&#22411;GPT-4V&#22312;&#25277;&#35937;&#31867;&#21035;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#25972;&#20307;&#24615;&#33021;&#19978;&#20173;&#28982;&#33853;&#21518;&#20110;&#20154;&#31867;&#65292;&#23384;&#22312;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#27493;&#23548;&#33268;&#20102;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#30340;&#21457;&#23637;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#28041;&#21450;&#25991;&#26412;&#21644;&#22270;&#20687;&#20869;&#23481;&#30340;&#22797;&#26434;&#20219;&#21153;&#65292;&#20363;&#22914;&#22312;&#20844;&#20849;&#22330;&#25152;&#23548;&#33322;&#22320;&#22270;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ConTextual&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;&#19987;&#38376;&#35774;&#35745;&#30340;&#25351;&#20196;&#65292;&#29992;&#20110;&#35780;&#20272;LMMs&#22312;&#25191;&#34892;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#25991;&#26412;&#23500;&#26377;&#35270;&#35273;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;ConTextual&#24378;&#35843;&#20102;&#22810;&#26679;&#30340;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#65288;&#20363;&#22914;&#26102;&#38388;&#38405;&#35835;&#12289;&#23548;&#33322;&#12289;&#36141;&#29289;&#31561;&#65289;&#65292;&#35201;&#27714;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#25991;&#26412;&#21644;&#35270;&#35273;&#20803;&#32032;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#26368;&#20339;&#34920;&#29616;&#30340;LMM&#65292;GPT-4V(ision)&#65292;&#19982;&#20154;&#31867;&#33021;&#21147;&#20043;&#38388;&#23384;&#22312;30.8%&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#20351;&#29992;&#20154;&#31867;&#35780;&#20272;&#25351;&#20986;&#22312;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#25991;&#26412;&#23500;&#26377;&#35270;&#35273;&#25512;&#29702;&#26041;&#38754;&#36824;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#34429;&#28982;GPT-4V&#22312;&#25277;&#35937;&#31867;&#21035;&#65288;&#22914;&#27169;&#22240;&#21644;&#24341;&#25991;&#35299;&#37322;&#65289;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20854;&#25972;&#20307;&#24615;&#33021;&#20173;&#28982;&#33853;&#21518;&#20110;&#20154;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in AI have led to the development of large multimodal models (LMMs) capable of processing complex tasks involving joint reasoning over text and visual content in the image (e.g., navigating maps in public places). This paper introduces ConTextual, a novel benchmark comprising instructions designed explicitly to evaluate LMMs' ability to perform context-sensitive text-rich visual reasoning. ConTextual emphasizes diverse real-world scenarios (e.g., time-reading, navigation, shopping and more) demanding a deeper understanding of the interactions between textual and visual elements. Our findings reveal a significant performance gap of 30.8% between the best-performing LMM, GPT-4V(ision), and human capabilities using human evaluation indicating substantial room for improvement in context-sensitive text-rich visual reasoning. Notably, while GPT-4V excelled in abstract categories like meme and quote interpretation, its overall performance still lagged behind humans. In add
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#22810;&#27169;&#24577;&#36777;&#35770;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#26377;&#23475;&#27169;&#22240;&#26816;&#27979;&#26041;&#27861;&#12290;&#36890;&#36807;&#25512;&#29702;&#26377;&#23475;&#21644;&#26080;&#23475;&#31435;&#22330;&#20043;&#38388;&#30340;&#30456;&#20114;&#30683;&#30462;&#29702;&#30001;&#65292;&#29983;&#25104;&#21487;&#35835;&#30340;&#35299;&#37322;&#65292;&#25552;&#21319;&#26377;&#23475;&#27169;&#22240;&#26816;&#27979;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.13298</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#22810;&#27169;&#24577;&#36777;&#35770;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#26377;&#23475;&#27169;&#22240;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Towards Explainable Harmful Meme Detection through Multimodal Debate between Large Language Models. (arXiv:2401.13298v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13298
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#22810;&#27169;&#24577;&#36777;&#35770;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#26377;&#23475;&#27169;&#22240;&#26816;&#27979;&#26041;&#27861;&#12290;&#36890;&#36807;&#25512;&#29702;&#26377;&#23475;&#21644;&#26080;&#23475;&#31435;&#22330;&#20043;&#38388;&#30340;&#30456;&#20114;&#30683;&#30462;&#29702;&#30001;&#65292;&#29983;&#25104;&#21487;&#35835;&#30340;&#35299;&#37322;&#65292;&#25552;&#21319;&#26377;&#23475;&#27169;&#22240;&#26816;&#27979;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#26102;&#20195;&#20805;&#26021;&#30528;&#20114;&#32852;&#32593;&#27169;&#22240;&#65292;&#38656;&#35201;&#26126;&#30830;&#25484;&#25569;&#21644;&#26377;&#25928;&#35782;&#21035;&#26377;&#23475;&#27169;&#22240;&#12290;&#30001;&#20110;&#27169;&#22240;&#20013;&#34164;&#21547;&#30340;&#38544;&#21547;&#21547;&#20041;&#19981;&#33021;&#36890;&#36807;&#34920;&#38754;&#25991;&#26412;&#21644;&#22270;&#20687;&#26126;&#30830;&#20256;&#36798;&#65292;&#36825;&#19968;&#20219;&#21153;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26377;&#23475;&#27169;&#22240;&#26816;&#27979;&#26041;&#27861;&#26410;&#25552;&#20379;&#21487;&#35835;&#30340;&#35299;&#37322;&#20197;&#25581;&#31034;&#36825;&#31181;&#38544;&#21547;&#21547;&#20041;&#20197;&#25903;&#25345;&#20854;&#26816;&#27979;&#20915;&#31574;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#26377;&#23475;&#27169;&#22240;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#25512;&#29702;&#26377;&#23475;&#21644;&#26080;&#23475;&#31435;&#22330;&#20043;&#38388;&#30340;&#30456;&#20114;&#30683;&#30462;&#29702;&#30001;&#26469;&#23454;&#29616;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#21463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#21644;&#25512;&#29702;&#26041;&#38754;&#30340;&#24378;&#22823;&#33021;&#21147;&#21551;&#21457;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#22810;&#27169;&#24577;&#36777;&#35770;&#65292;&#29983;&#25104;&#22522;&#20110;&#30683;&#30462;&#35770;&#25454;&#30340;&#35299;&#37322;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#31934;&#35843;&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36777;&#35770;&#35009;&#21028;&#26469;&#25512;&#26029;&#26377;&#23475;&#24615;&#65292;&#20197;&#20419;&#36827;&#26377;&#23475;&#21644;&#26080;&#23475;&#20449;&#24687;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The age of social media is flooded with Internet memes, necessitating a clear grasp and effective identification of harmful ones. This task presents a significant challenge due to the implicit meaning embedded in memes, which is not explicitly conveyed through the surface text and image. However, existing harmful meme detection methods do not present readable explanations that unveil such implicit meaning to support their detection decisions. In this paper, we propose an explainable approach to detect harmful memes, achieved through reasoning over conflicting rationales from both harmless and harmful positions. Specifically, inspired by the powerful capacity of Large Language Models (LLMs) on text generation and reasoning, we first elicit multimodal debate between LLMs to generate the explanations derived from the contradictory arguments. Then we propose to fine-tune a small language model as the debate judge for harmfulness inference, to facilitate multimodal fusion between the harmfu
&lt;/p&gt;</description></item><item><title>RefreshNet&#26159;&#19968;&#20010;&#22810;&#23610;&#24230;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#23618;&#21047;&#26032;&#26426;&#21046;&#26469;&#23398;&#20064;&#22797;&#26434;&#31995;&#32479;&#30340;&#21160;&#24577;&#65292;&#23454;&#29616;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2401.13282</link><description>&lt;p&gt;
RefreshNet: &#36890;&#36807;&#20998;&#23618;&#21047;&#26032;&#23398;&#20064;&#22810;&#23610;&#24230;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
RefreshNet: Learning Multiscale Dynamics through Hierarchical Refreshing. (arXiv:2401.13282v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13282
&lt;/p&gt;
&lt;p&gt;
RefreshNet&#26159;&#19968;&#20010;&#22810;&#23610;&#24230;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#23618;&#21047;&#26032;&#26426;&#21046;&#26469;&#23398;&#20064;&#22797;&#26434;&#31995;&#32479;&#30340;&#21160;&#24577;&#65292;&#23454;&#29616;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#22797;&#26434;&#31995;&#32479;&#21160;&#21147;&#23398;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#38271;&#26399;&#39044;&#27979;&#65292;&#22987;&#32456;&#21463;&#21040;&#35823;&#24046;&#32047;&#31215;&#21644;&#35745;&#31639;&#36127;&#25285;&#30340;&#38480;&#21046;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;RefreshNet&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#23610;&#24230;&#26694;&#26550;&#65292;&#26088;&#22312;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25552;&#20379;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#24179;&#34913;&#12290;RefreshNet&#32467;&#21512;&#20102;&#21367;&#31215;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#35782;&#21035;&#25429;&#25417;&#21160;&#24577;&#30340;&#22522;&#26412;&#29305;&#24449;&#30340;&#38477;&#38454;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#22312;&#28508;&#22312;&#31354;&#38388;&#20869;&#31574;&#30053;&#24615;&#22320;&#20351;&#29992;&#22810;&#20010;&#26102;&#38388;&#20998;&#36776;&#29575;&#19981;&#21516;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#22359;&#65292;&#20174;&#32780;&#20801;&#35768;&#22810;&#20010;&#26102;&#38388;&#23610;&#24230;&#19978;&#25429;&#25417;&#28508;&#22312;&#21160;&#24577;&#12290;RefreshNet&#20013;&#30340;&#29420;&#29305;&#30340;&#8220;&#21047;&#26032;&#8221;&#26426;&#21046;&#20801;&#35768;&#36739;&#31895;&#31961;&#30340;&#22359;&#37325;&#26032;&#35774;&#32622;&#36739;&#32454;&#22359;&#30340;&#36755;&#20837;&#65292;&#26377;&#25928;&#25511;&#21046;&#21644;&#20943;&#36731;&#35823;&#24046;&#32047;&#31215;&#12290;&#36825;&#31181;&#35774;&#35745;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#38271;&#26399;&#39044;&#27979;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forecasting complex system dynamics, particularly for long-term predictions, is persistently hindered by error accumulation and computational burdens. This study presents RefreshNet, a multiscale framework developed to overcome these challenges, delivering an unprecedented balance between computational efficiency and predictive accuracy. RefreshNet incorporates convolutional autoencoders to identify a reduced order latent space capturing essential features of the dynamics, and strategically employs multiple recurrent neural network (RNN) blocks operating at varying temporal resolutions within the latent space, thus allowing the capture of latent dynamics at multiple temporal scales. The unique "refreshing" mechanism in RefreshNet allows coarser blocks to reset inputs of finer blocks, effectively controlling and alleviating error accumulation. This design demonstrates superiority over existing techniques regarding computational efficiency and predictive accuracy, especially in long-term
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;AI&#21161;&#25163;&#26159;&#21542;&#33021;&#30693;&#36947;&#33258;&#24049;&#19981;&#30693;&#36947;&#30340;&#20107;&#24773;&#65292;&#24182;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#20986;&#26469;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#29305;&#23450;&#27169;&#22411;&#30340;"I don't know"&#65288;Idk&#65289;&#25968;&#25454;&#38598;&#65292;&#24182;&#19982;AI&#21161;&#25163;&#36827;&#34892;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2401.13275</link><description>&lt;p&gt;
AI&#21161;&#25163;&#26159;&#21542;&#33021;&#30693;&#36947;&#33258;&#24049;&#19981;&#30693;&#36947;&#30340;&#20107;&#24773;?
&lt;/p&gt;
&lt;p&gt;
Can AI Assistants Know What They Don't Know?. (arXiv:2401.13275v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;AI&#21161;&#25163;&#26159;&#21542;&#33021;&#30693;&#36947;&#33258;&#24049;&#19981;&#30693;&#36947;&#30340;&#20107;&#24773;&#65292;&#24182;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#20986;&#26469;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#29305;&#23450;&#27169;&#22411;&#30340;"I don't know"&#65288;Idk&#65289;&#25968;&#25454;&#38598;&#65292;&#24182;&#19982;AI&#21161;&#25163;&#36827;&#34892;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;AI&#21161;&#25163;&#22312;&#23545;&#35805;&#12289;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#12289;&#32534;&#20889;&#20195;&#30721;&#21644;&#20351;&#29992;&#24037;&#20855;&#31561;&#35768;&#22810;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20196;&#20154;&#24778;&#35766;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;LLMs&#20855;&#26377;&#28145;&#20837;&#30340;&#19990;&#30028;&#30693;&#35782;&#65292;&#20294;&#22312;&#38754;&#23545;&#26576;&#20123;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#65288;&#22914;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#65289;&#26102;&#20173;&#28982;&#20250;&#20986;&#29616;&#20107;&#23454;&#38169;&#35823;&#12290;AI&#21161;&#25163;&#30340;&#36825;&#31181;&#19981;&#30495;&#23454;&#22238;&#31572;&#21487;&#33021;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#36896;&#25104;&#37325;&#22823;&#39118;&#38505;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;AI&#21161;&#25163;&#25298;&#32477;&#22238;&#31572;&#33258;&#24049;&#19981;&#30693;&#36947;&#30340;&#38382;&#39064;&#26159;&#20943;&#23569;&#24187;&#35273;&#21644;&#20351;&#21161;&#25163;&#30495;&#23454;&#30340;&#20851;&#38190;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#38382;&#39064;&#8220;AI&#21161;&#25163;&#26159;&#21542;&#33021;&#30693;&#36947;&#33258;&#24049;&#19981;&#30693;&#36947;&#30340;&#20107;&#24773;&#65292;&#24182;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#20986;&#26469;&#65311;&#8221;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20026;&#21161;&#25163;&#26500;&#24314;&#20102;&#19968;&#20010;&#29305;&#23450;&#27169;&#22411;&#30340;&#8220;I don't know&#8221;(Idk)&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#24050;&#30693;&#21644;&#26410;&#30693;&#30340;&#38382;&#39064;&#65292;&#22522;&#20110;&#29616;&#26377;&#30340;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#25105;&#20204;&#23558;&#21161;&#25163;&#19982;&#20854;&#30456;&#24212;&#30340;Idk&#25968;&#25454;&#36827;&#34892;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, AI assistants based on large language models (LLMs) show surprising performance in many tasks, such as dialogue, solving math problems, writing code, and using tools. Although LLMs possess intensive world knowledge, they still make factual errors when facing some knowledge intensive tasks, like open-domain question answering. These untruthful responses from the AI assistant may cause significant risks in practical applications. We believe that an AI assistant's refusal to answer questions it does not know is a crucial method for reducing hallucinations and making the assistant truthful. Therefore, in this paper, we ask the question "Can AI assistants know what they don't know and express them through natural language?" To answer this question, we construct a model-specific "I don't know" (Idk) dataset for an assistant, which contains its known and unknown questions, based on existing open-domain question answering datasets. Then we align the assistant with its corresponding I
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38899;&#39057;&#22330;&#26223;&#35821;&#20041;&#36827;&#34892;&#33258;&#21160;&#22270;&#20687;&#19978;&#33394;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#38899;&#39057;&#20316;&#20026;&#36741;&#21161;&#20449;&#24687;&#65292;&#38477;&#20302;&#20102;&#23545;&#22330;&#26223;&#35821;&#20041;&#29702;&#35299;&#30340;&#38590;&#24230;&#65292;&#24182;&#36890;&#36807;&#19977;&#20010;&#38454;&#27573;&#30340;&#32593;&#32476;&#36827;&#34892;&#20102;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.13270</link><description>&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#38899;&#39057;&#22330;&#26223;&#35821;&#20041;&#23454;&#29616;&#33258;&#21160;&#22270;&#20687;&#19978;&#33394;
&lt;/p&gt;
&lt;p&gt;
Audio-Infused Automatic Image Colorization by Exploiting Audio Scene Semantics. (arXiv:2401.13270v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13270
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38899;&#39057;&#22330;&#26223;&#35821;&#20041;&#36827;&#34892;&#33258;&#21160;&#22270;&#20687;&#19978;&#33394;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#38899;&#39057;&#20316;&#20026;&#36741;&#21161;&#20449;&#24687;&#65292;&#38477;&#20302;&#20102;&#23545;&#22330;&#26223;&#35821;&#20041;&#29702;&#35299;&#30340;&#38590;&#24230;&#65292;&#24182;&#36890;&#36807;&#19977;&#20010;&#38454;&#27573;&#30340;&#32593;&#32476;&#36827;&#34892;&#20102;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#22270;&#20687;&#19978;&#33394;&#26159;&#19968;&#20010;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#23545;&#22330;&#26223;&#36827;&#34892;&#20934;&#30830;&#30340;&#35821;&#20041;&#29702;&#35299;&#65292;&#20197;&#20272;&#35745;&#28784;&#24230;&#22270;&#20687;&#30340;&#21512;&#29702;&#39068;&#33394;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#22522;&#20110;&#20132;&#20114;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#20294;&#23545;&#20110;&#33258;&#21160;&#19978;&#33394;&#26469;&#35828;&#65292;&#25512;&#26029;&#20986;&#36924;&#30495;&#21644;&#20934;&#30830;&#30340;&#39068;&#33394;&#20173;&#28982;&#26159;&#19968;&#20010;&#38750;&#24120;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#38477;&#20302;&#23545;&#28784;&#24230;&#22330;&#26223;&#30340;&#35821;&#20041;&#29702;&#35299;&#38590;&#24230;&#65292;&#26412;&#25991;&#23581;&#35797;&#21033;&#29992;&#30456;&#24212;&#30340;&#38899;&#39057;&#65292;&#38899;&#39057;&#33258;&#28982;&#22320;&#21253;&#21547;&#20102;&#20851;&#20110;&#21516;&#19968;&#22330;&#26223;&#30340;&#39069;&#22806;&#35821;&#20041;&#20449;&#24687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38899;&#39057;&#27880;&#20837;&#33258;&#21160;&#22270;&#20687;&#19978;&#33394;&#65288;AIAIC&#65289;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#20998;&#20026;&#19977;&#20010;&#38454;&#27573;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#24425;&#33394;&#22270;&#20687;&#30340;&#35821;&#20041;&#20316;&#20026;&#26725;&#26753;&#65292;&#36890;&#36807;&#24425;&#33394;&#22270;&#20687;&#30340;&#35821;&#20041;&#24341;&#23548;&#39044;&#35757;&#32451;&#19978;&#33394;&#32593;&#32476;&#12290;&#20854;&#27425;&#65292;&#21033;&#29992;&#38899;&#39057;&#19982;&#35270;&#39057;&#30340;&#33258;&#28982;&#20849;&#29616;&#26469;&#23398;&#20064;&#38899;&#39057;&#21644;&#35270;&#35273;&#22330;&#26223;&#20043;&#38388;&#30340;&#39068;&#33394;&#35821;&#20041;&#30456;&#20851;&#24615;&#12290;&#31532;&#19977;&#65292;&#38544;&#24335;&#38899;&#39057;&#35821;&#20041;&#34920;&#31034;&#34987;&#21033;&#29992;&#20197;&#24341;&#23548;&#22270;&#20687;&#19978;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic image colorization is inherently an ill-posed problem with uncertainty, which requires an accurate semantic understanding of scenes to estimate reasonable colors for grayscale images. Although recent interaction-based methods have achieved impressive performance, it is still a very difficult task to infer realistic and accurate colors for automatic colorization. To reduce the difficulty of semantic understanding of grayscale scenes, this paper tries to utilize corresponding audio, which naturally contains extra semantic information about the same scene. Specifically, a novel audio-infused automatic image colorization (AIAIC) network is proposed, which consists of three stages. First, we take color image semantics as a bridge and pretrain a colorization network guided by color image semantics. Second, the natural co-occurrence of audio and video is utilized to learn the color semantic correlations between audio and visual scenes. Third, the implicit audio semantic representati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#31181;&#20132;&#26131;&#36153;&#37325;&#26032;&#20998;&#37197;&#26426;&#21046;&#65292;&#22312;&#20998;&#24067;&#24335;&#36134;&#26412;&#20013;&#20943;&#23569;&#20132;&#26131;&#25163;&#32493;&#36153;&#12290;&#36825;&#31181;&#26426;&#21046;&#20351;&#29992;&#25240;&#25187;&#30340;&#24418;&#24335;&#37325;&#26032;&#20998;&#37197;VCG&#25903;&#20184;&#65292;&#20197;&#26368;&#23567;&#21270;&#20132;&#26131;&#25163;&#32493;&#36153;&#65292;&#24182;&#21516;&#26102;&#20445;&#35777;&#20248;&#21270;&#20998;&#37197;&#25928;&#29575;&#21644;&#29992;&#25143;&#28608;&#21169;&#20860;&#23481;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.13262</link><description>&lt;p&gt;
&#35774;&#35745;&#20998;&#24067;&#24335;&#36134;&#26412;&#20013;&#20943;&#23569;&#20132;&#26131;&#25163;&#32493;&#36153;&#30340;&#37325;&#26032;&#20998;&#37197;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Designing Redistribution Mechanisms for Reducing Transaction Fees in Blockchains. (arXiv:2401.13262v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#31181;&#20132;&#26131;&#36153;&#37325;&#26032;&#20998;&#37197;&#26426;&#21046;&#65292;&#22312;&#20998;&#24067;&#24335;&#36134;&#26412;&#20013;&#20943;&#23569;&#20132;&#26131;&#25163;&#32493;&#36153;&#12290;&#36825;&#31181;&#26426;&#21046;&#20351;&#29992;&#25240;&#25187;&#30340;&#24418;&#24335;&#37325;&#26032;&#20998;&#37197;VCG&#25903;&#20184;&#65292;&#20197;&#26368;&#23567;&#21270;&#20132;&#26131;&#25163;&#32493;&#36153;&#65292;&#24182;&#21516;&#26102;&#20445;&#35777;&#20248;&#21270;&#20998;&#37197;&#25928;&#29575;&#21644;&#29992;&#25143;&#28608;&#21169;&#20860;&#23481;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21306;&#22359;&#38142;&#37319;&#29992;&#20132;&#26131;&#36153;&#26426;&#21046;&#65288;TFMs&#65289;&#26469;&#30830;&#23450;&#21738;&#20123;&#29992;&#25143;&#20132;&#26131;&#21253;&#21547;&#22312;&#21306;&#22359;&#20013;&#24182;&#30830;&#23450;&#20854;&#25903;&#20184;&#65288;&#21363;&#20132;&#26131;&#25163;&#32493;&#36153;&#65289;&#12290;&#38656;&#27714;&#22686;&#21152;&#21644;&#31232;&#32570;&#30340;&#21306;&#22359;&#36164;&#28304;&#23548;&#33268;&#29992;&#25143;&#20132;&#26131;&#25163;&#32493;&#36153;&#39640;&#28072;&#12290;&#30001;&#20110;&#36825;&#20123;&#21306;&#22359;&#38142;&#26159;&#20844;&#20849;&#36164;&#28304;&#65292;&#38477;&#20302;&#20132;&#26131;&#25163;&#32493;&#36153;&#21487;&#33021;&#26356;&#20026;&#21487;&#21462;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20132;&#26131;&#36153;&#37325;&#26032;&#20998;&#37197;&#26426;&#21046;&#65288;TFRMs&#65289;-&#36890;&#36807;&#23558;TFMs&#25152;&#25910;&#38598;&#30340;VCG&#25903;&#20184;&#20197;&#25240;&#25187;&#30340;&#24418;&#24335;&#37325;&#26032;&#20998;&#37197;&#65292;&#20197;&#26368;&#23567;&#21270;&#20132;&#26131;&#25163;&#32493;&#36153;&#12290;&#32463;&#20856;&#30340;&#37325;&#26032;&#20998;&#37197;&#26426;&#21046;&#65288;RMs&#65289;&#22312;&#30830;&#20445;&#20248;&#21270;&#20998;&#37197;&#25928;&#29575;&#65288;AE&#65289;&#21644;&#29992;&#25143;&#28608;&#21169;&#20860;&#23481;&#24615;&#65288;UIC&#65289;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#36825;&#19968;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Blockchains deploy Transaction Fee Mechanisms (TFMs) to determine which user transactions to include in blocks and determine their payments (i.e., transaction fees). Increasing demand and scarce block resources have led to high user transaction fees. As these blockchains are a public resource, it may be preferable to reduce these transaction fees. To this end, we introduce Transaction Fee Redistribution Mechanisms (TFRMs) -- redistributing VCG payments collected from such TFM as rebates to minimize transaction fees. Classic redistribution mechanisms (RMs) achieve this while ensuring Allocative Efficiency (AE) and User Incentive Compatibility (UIC). Our first result shows the non-triviality of applying RM in TFMs. More concretely, we prove that it is impossible to reduce transaction fees when (i) transactions that are not confirmed do not receive rebates and (ii) the miner can strategically manipulate the mechanism. Driven by this, we propose \emph{Robust} TFRM (\textsf{R-TFRM}): a mech
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#22810;&#28304;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#65288;UniMS-RAG&#65289;&#65292;&#36890;&#36807;&#32479;&#19968;&#30693;&#35782;&#28304;&#36873;&#25321;&#12289;&#30693;&#35782;&#26816;&#32034;&#21644;&#22238;&#22797;&#29983;&#25104;&#19977;&#20010;&#23376;&#20219;&#21153;&#65292;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#38656;&#27714;&#33258;&#36866;&#24212;&#22320;&#26816;&#32034;&#35777;&#25454;&#21644;&#35780;&#20272;&#20851;&#32852;&#24615;&#65292;&#20174;&#32780;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#22238;&#22797;&#12290;</title><link>http://arxiv.org/abs/2401.13256</link><description>&lt;p&gt;
UniMS-RAG: &#29992;&#20110;&#20010;&#24615;&#21270;&#23545;&#35805;&#31995;&#32479;&#30340;&#32479;&#19968;&#22810;&#28304;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
UniMS-RAG: A Unified Multi-source Retrieval-Augmented Generation for Personalized Dialogue Systems. (arXiv:2401.13256v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13256
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#22810;&#28304;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#65288;UniMS-RAG&#65289;&#65292;&#36890;&#36807;&#32479;&#19968;&#30693;&#35782;&#28304;&#36873;&#25321;&#12289;&#30693;&#35782;&#26816;&#32034;&#21644;&#22238;&#22797;&#29983;&#25104;&#19977;&#20010;&#23376;&#20219;&#21153;&#65292;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#38656;&#27714;&#33258;&#36866;&#24212;&#22320;&#26816;&#32034;&#35777;&#25454;&#21644;&#35780;&#20272;&#20851;&#32852;&#24615;&#65292;&#20174;&#32780;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#22238;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#23545;&#35805;&#31995;&#32479;&#20013;&#28041;&#21450;&#21040;&#22810;&#20010;&#20449;&#24687;&#28304;&#26102;&#65292;&#20010;&#24615;&#21270;&#38382;&#39064;&#20173;&#28982;&#26159;&#19968;&#20010;&#20196;&#20154;&#21521;&#24448;&#30340;&#23646;&#24615;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#35745;&#21010;&#21644;&#25972;&#21512;&#22810;&#20010;&#20449;&#24687;&#28304;&#22312;&#29983;&#25104;&#20010;&#24615;&#21270;&#22238;&#22797;&#20013;&#30340;&#20351;&#29992;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#20854;&#20998;&#35299;&#20026;&#19977;&#20010;&#23376;&#20219;&#21153;&#65306;&#30693;&#35782;&#28304;&#36873;&#25321;&#12289;&#30693;&#35782;&#26816;&#32034;&#21644;&#22238;&#22797;&#29983;&#25104;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#19968;&#22810;&#28304;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#65288;UniMS-RAG&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22312;&#35757;&#32451;&#26399;&#38388;&#20351;&#29992;&#30456;&#21516;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#33539;&#24335;&#23558;&#36825;&#19977;&#20010;&#23376;&#20219;&#21153;&#32479;&#19968;&#36215;&#26469;&#65292;&#36890;&#36807;&#20351;&#29992;&#29305;&#27530;&#30340;&#20196;&#29260;&#65292;&#21363;&#34892;&#21160;&#20196;&#29260;&#21644;&#35780;&#20272;&#20196;&#29260;&#65292;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#26816;&#32034;&#35777;&#25454;&#24182;&#35780;&#20272;&#20851;&#32852;&#24615;&#12290;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#34892;&#21160;&#20196;&#29260;&#26377;&#21161;&#20110;&#19982;&#21508;&#31181;&#30693;&#35782;&#28304;&#36827;&#34892;&#20132;&#20114;&#65292;&#20351;&#20854;&#33021;&#22815;&#36866;&#24212;&#20854;&#19978;&#19979;&#25991;&#21644;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#22238;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) has shown exceptional capabilities in many natual language understanding and generation tasks. However, the personalization issue still remains a much-coveted property, especially when it comes to the multiple sources involved in the dialogue system. To better plan and incorporate the use of multiple sources in generating personalized response, we firstly decompose it into three sub-tasks: Knowledge Source Selection, Knowledge Retrieval, and Response Generation. We then propose a novel Unified Multi-Source Retrieval-Augmented Generation system (UniMS-RAG) Specifically, we unify these three sub-tasks with different formulations into the same sequence-to-sequence paradigm during the training, to adaptively retrieve evidences and evaluate the relevance on-demand using special tokens, called acting tokens and evaluation tokens. Enabling language models to generate acting tokens facilitates interaction with various knowledge sources, allowing them to adapt their
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26679;&#24615;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#20248;&#21270;&#20154;&#31867;&#27880;&#37322;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#20256;&#32479;&#30340;&#38543;&#26426;&#36873;&#25321;&#25968;&#25454;&#26041;&#27861;&#24573;&#35270;&#20102;&#25968;&#25454;&#30340;&#29305;&#24449;&#21644;&#27169;&#22411;&#30340;&#38656;&#27714;&#65292;&#32780;&#35813;&#26041;&#27861;&#23558;&#32771;&#34385;&#36825;&#20123;&#22240;&#32032;&#65292;&#20197;&#25552;&#39640;&#25968;&#25454;&#36873;&#25321;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.13229</link><description>&lt;p&gt;
&#20174;&#38543;&#26426;&#21040;&#26377;&#20449;&#24687;&#36873;&#25321;&#25968;&#25454;&#65306;&#22522;&#20110;&#22810;&#26679;&#24615;&#30340;&#26041;&#27861;&#20248;&#21270;&#20154;&#31867;&#27880;&#37322;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
From Random to Informed Data Selection: A Diversity-Based Approach to Optimize Human Annotation and Few-Shot Learning. (arXiv:2401.13229v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13229
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26679;&#24615;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#20248;&#21270;&#20154;&#31867;&#27880;&#37322;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#20256;&#32479;&#30340;&#38543;&#26426;&#36873;&#25321;&#25968;&#25454;&#26041;&#27861;&#24573;&#35270;&#20102;&#25968;&#25454;&#30340;&#29305;&#24449;&#21644;&#27169;&#22411;&#30340;&#38656;&#27714;&#65292;&#32780;&#35813;&#26041;&#27861;&#23558;&#32771;&#34385;&#36825;&#20123;&#22240;&#32032;&#65292;&#20197;&#25552;&#39640;&#25968;&#25454;&#36873;&#25321;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#33719;&#21462;&#29992;&#20110;&#30417;&#30563;&#23398;&#20064;&#30340;&#27880;&#37322;&#25968;&#25454;&#12290;&#19968;&#31181;&#36873;&#25321;&#26159;&#20351;&#29992;&#20247;&#21253;&#24179;&#21488;&#36827;&#34892;&#25968;&#25454;&#27880;&#37322;&#12290;&#28982;&#32780;&#65292;&#20247;&#21253;&#24341;&#20837;&#20102;&#19982;&#27880;&#37322;&#32773;&#30340;&#32463;&#39564;&#12289;&#19968;&#33268;&#24615;&#21644;&#20559;&#35265;&#30456;&#20851;&#30340;&#38382;&#39064;&#12290;&#21478;&#19968;&#31181;&#36873;&#25321;&#26159;&#20351;&#29992;&#38646;&#26679;&#26412;&#26041;&#27861;&#65292;&#20294;&#19982;&#23569;&#26679;&#26412;&#25110;&#23436;&#20840;&#30417;&#30563;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#38646;&#26679;&#26412;&#26041;&#27861;&#26377;&#20854;&#23616;&#38480;&#24615;&#12290;&#26368;&#36817;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#21160;&#30340;&#26368;&#26032;&#36827;&#23637;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#22312;&#25968;&#25454;&#20005;&#37325;&#21463;&#38480;&#30340;&#19987;&#19994;&#39046;&#22495;&#20013;&#65292;&#23427;&#20204;&#24448;&#24448;&#38590;&#20197;&#36866;&#24212;&#12290;&#22240;&#27492;&#65292;&#26368;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#20154;&#31867;&#38543;&#26426;&#27880;&#37322;&#19968;&#32452;&#25968;&#25454;&#28857;&#26469;&#26500;&#24314;&#21021;&#22987;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#38543;&#26426;&#25277;&#26679;&#25968;&#25454;&#36827;&#34892;&#27880;&#37322;&#36890;&#24120;&#25928;&#29575;&#20302;&#19979;&#65292;&#22240;&#20026;&#23427;&#24573;&#35270;&#20102;&#25968;&#25454;&#30340;&#29305;&#24449;&#21644;&#27169;&#22411;&#30340;&#29305;&#23450;&#38656;&#27714;&#12290;&#24403;&#22788;&#29702;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#26102;&#65292;&#24773;&#20917;&#26356;&#21152;&#31967;&#31957;&#65292;&#22240;&#20026;&#38543;&#26426;&#25277;&#26679;&#20542;&#21521;&#20110;&#20005;&#37325;&#20559;&#21521;&#22810;&#25968;&#31867;&#21035;&#65292;&#23548;&#33268;&#36807;&#22810;&#30340;&#27880;&#37322;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
A major challenge in Natural Language Processing is obtaining annotated data for supervised learning. An option is the use of crowdsourcing platforms for data annotation. However, crowdsourcing introduces issues related to the annotator's experience, consistency, and biases. An alternative is to use zero-shot methods, which in turn have limitations compared to their few-shot or fully supervised counterparts. Recent advancements driven by large language models show potential, but struggle to adapt to specialized domains with severely limited data. The most common approaches therefore involve the human itself randomly annotating a set of datapoints to build initial datasets. But randomly sampling data to be annotated is often inefficient as it ignores the characteristics of the data and the specific needs of the model. The situation worsens when working with imbalanced datasets, as random sampling tends to heavily bias towards the majority classes, leading to excessive annotated data. To
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#19978;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;LPNL&#26694;&#26550;&#29992;&#20110;&#21487;&#25193;&#23637;&#38142;&#25509;&#39044;&#27979;&#12290;&#36890;&#36807;&#21019;&#26032;&#30340;&#25552;&#31034;&#35821;&#21644;&#37319;&#26679;&#27969;&#31243;&#65292;&#20197;&#21450;&#20998;&#32780;&#27835;&#20043;&#30340;&#31574;&#30053;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#22270;&#20013;&#30340;&#20449;&#24687;&#36807;&#36733;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.13227</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#19978;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38142;&#25509;&#39044;&#27979;&#30340;&#21487;&#25193;&#23637;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Scalable Link Prediction on Large-Scale Heterogeneous Graphs with Large Language Models. (arXiv:2401.13227v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#19978;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;LPNL&#26694;&#26550;&#29992;&#20110;&#21487;&#25193;&#23637;&#38142;&#25509;&#39044;&#27979;&#12290;&#36890;&#36807;&#21019;&#26032;&#30340;&#25552;&#31034;&#35821;&#21644;&#37319;&#26679;&#27969;&#31243;&#65292;&#20197;&#21450;&#20998;&#32780;&#27835;&#20043;&#30340;&#31574;&#30053;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#22270;&#20013;&#30340;&#20449;&#24687;&#36807;&#36733;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#23558;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#22270;&#23398;&#20064;&#26159;&#19968;&#39033;&#26032;&#39062;&#30340;&#21162;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#22270;&#20013;&#34164;&#21547;&#30340;&#22823;&#37327;&#20449;&#24687;&#32473;&#36825;&#19968;&#36807;&#31243;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#20391;&#37325;&#20110;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#20171;&#32461;&#20102;LPNL&#65288;Link Prediction via Natural Language&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#19978;&#30340;&#21487;&#25193;&#23637;&#38142;&#25509;&#39044;&#27979;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#33021;&#20197;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#22270;&#32454;&#33410;&#30340;&#21019;&#26032;&#25552;&#31034;&#35821;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#37319;&#26679;&#27969;&#31243;&#65292;&#20174;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#20013;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#37319;&#29992;&#20998;&#32780;&#27835;&#20043;&#30340;&#31574;&#30053;&#26469;&#25511;&#21046;&#36755;&#20837;&#20196;&#29260;&#25968;&#37327;&#22312;&#39044;&#23450;&#38480;&#21046;&#20869;&#65292;&#35299;&#20915;&#20102;&#20449;&#24687;&#36807;&#36733;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#35774;&#35745;&#20102;&#19968;&#20010;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#30340;T5&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#22823;&#22411;&#20844;&#20849;&#24322;&#26500;&#22270;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;LPNL&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#21508;&#31181;&#20808;&#36827;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploring the application of large-scale language models to graph learning is a novel endeavor. However, the vast amount of information inherent in large graphs poses significant challenges to this process. This paper focuses on the link prediction task and introduces LPNL (Link Prediction via Natural Language), a framework based on a large language model designed for scalable link prediction on large-scale heterogeneous graphs.We design novel prompts for link prediction that articulate graph details in natural language. We propose a two-stage sampling pipeline to extract crucial information from large-scale heterogeneous graphs, and a divide-and-conquer strategy to control the input token count within predefined limits, addressing the challenge of overwhelming information. We fine-tune a T5 model based on our self-supervised learning designed for for link prediction. Extensive experiments on a large public heterogeneous graphs demonstrate that LPNL outperforms various advanced baselin
&lt;/p&gt;</description></item><item><title>TAT-LLM&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#31163;&#25955;&#25512;&#29702;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#38024;&#23545;&#28151;&#21512;&#34920;&#26684;&#21644;&#25991;&#26412;&#25968;&#25454;&#19978;&#30340;&#38382;&#31572;&#20219;&#21153;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#20998;&#27493;&#27969;&#27700;&#32447;&#30340;&#26041;&#24335;&#65292;&#21253;&#25324;&#25552;&#21462;&#22120;&#12289;&#25512;&#29702;&#22120;&#21644;&#25191;&#34892;&#22120;&#65292;&#21033;&#29992;LLMs&#30340;&#24378;&#22823;&#33021;&#21147;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;&#32780;&#20026;&#20102;&#24212;&#23545;&#25104;&#26412;&#12289;&#24310;&#36831;&#21644;&#25968;&#25454;&#23433;&#20840;&#39118;&#38505;&#31561;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;TAT-LLM&#65292;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#27492;&#20219;&#21153;&#30340;&#36739;&#23567;LLM&#12290;</title><link>http://arxiv.org/abs/2401.13223</link><description>&lt;p&gt;
TAT-LLM: &#19968;&#31181;&#38024;&#23545;&#34920;&#26684;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#19987;&#29992;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#31163;&#25955;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
TAT-LLM: A Specialized Language Model for Discrete Reasoning over Tabular and Textual Data. (arXiv:2401.13223v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13223
&lt;/p&gt;
&lt;p&gt;
TAT-LLM&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#31163;&#25955;&#25512;&#29702;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#38024;&#23545;&#28151;&#21512;&#34920;&#26684;&#21644;&#25991;&#26412;&#25968;&#25454;&#19978;&#30340;&#38382;&#31572;&#20219;&#21153;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#20998;&#27493;&#27969;&#27700;&#32447;&#30340;&#26041;&#24335;&#65292;&#21253;&#25324;&#25552;&#21462;&#22120;&#12289;&#25512;&#29702;&#22120;&#21644;&#25191;&#34892;&#22120;&#65292;&#21033;&#29992;LLMs&#30340;&#24378;&#22823;&#33021;&#21147;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;&#32780;&#20026;&#20102;&#24212;&#23545;&#25104;&#26412;&#12289;&#24310;&#36831;&#21644;&#25968;&#25454;&#23433;&#20840;&#39118;&#38505;&#31561;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;TAT-LLM&#65292;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#27492;&#20219;&#21153;&#30340;&#36739;&#23567;LLM&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#22312;&#28151;&#21512;&#34920;&#26684;&#21644;&#25991;&#26412;&#25968;&#25454;&#19978;&#36827;&#34892;&#38382;&#31572;&#30340;&#38382;&#39064;&#65292;&#36825;&#22312;Web&#19978;&#38750;&#24120;&#24120;&#35265;&#65288;&#22914;SEC&#25991;&#20214;&#65289;&#65292;&#36890;&#24120;&#38656;&#35201;&#31163;&#25955;&#25512;&#29702;&#33021;&#21147;&#12290;&#26368;&#36817;&#65292;&#20687;GPT-4&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#22810;&#27493;&#39588;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#32771;&#34385;&#21033;&#29992;LLMs&#30340;&#24378;&#22823;&#33021;&#21147;&#26469;&#35299;&#20915;&#25105;&#20204;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#38754;&#21521;&#34920;&#26684;&#21644;&#25991;&#26412;&#38382;&#31572;&#30340;&#20998;&#27493;&#27969;&#27700;&#32447;&#30340;&#25277;&#35937;&#65292;&#21253;&#25324;&#25552;&#21462;&#22120;&#12289;&#25512;&#29702;&#22120;&#21644;&#25191;&#34892;&#22120;&#19977;&#20010;&#20851;&#38190;&#27493;&#39588;&#65292;&#24182;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20221;&#25351;&#20196;&#26469;&#23454;&#20363;&#21270;&#35813;&#27969;&#27700;&#32447;&#24182;&#39564;&#35777;GPT-4&#20248;&#20110;&#25152;&#26377;&#29616;&#26377;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992;&#20687;GPT-4&#36825;&#26679;&#30340;&#22312;&#32447;LLM&#23384;&#22312;&#25104;&#26412;&#12289;&#24310;&#36831;&#21644;&#25968;&#25454;&#23433;&#20840;&#39118;&#38505;&#31561;&#21508;&#31181;&#25361;&#25112;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#19987;&#38376;&#38024;&#23545;&#27492;&#20219;&#21153;&#24320;&#21457;&#36739;&#23567;&#30340;LLM&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#29616;&#26377;&#19987;&#23478;&#26631;&#27880;&#25968;&#25454;&#38598;&#33258;&#21160;&#29983;&#25104;&#30340;&#35757;&#32451;&#25968;&#25454;&#23545;LLaMA 2&#36827;&#34892;&#24494;&#35843;&#65292;&#24320;&#21457;&#20102;TAT-LLM&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we address question answering (QA) over a hybrid of tabular and textual data that are very common content on the Web (e.g. SEC filings), where discrete reasoning capabilities are often required. Recently, large language models (LLMs) like GPT-4 have demonstrated strong multi-step reasoning capabilities. We then consider harnessing the amazing power of LLMs to solve our task. We abstract a Step-wise Pipeline for tabular and textual QA, which consists of three key steps, including Extractor, Reasoner and Executor, and initially design an instruction to instantiate the pipeline and validate that GPT-4 outperforms all existing methods. However, utilizing an online LLM like GPT-4 holds various challenges in terms of cost, latency, and data security risk, which motivates us to specialize smaller LLMs in this task. We develop a TAT-LLM language model by fine-tuning LLaMA 2 with the training data generated automatically from existing expert-annotated datasets following the Step-w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31232;&#32570;&#26631;&#35760;&#30340;&#38646;&#26679;&#26412;&#22522;&#22240;&#32452;&#20998;&#31867;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#31216;&#20026;TEPI&#12290;&#36890;&#36807;&#23558;&#22522;&#22240;&#32452;&#34920;&#31034;&#20026;&#20266;&#22270;&#20687;&#24182;&#23558;&#20854;&#26144;&#23556;&#21040;&#20998;&#31867;&#24863;&#30693;&#30340;&#23884;&#20837;&#31354;&#38388;&#65292;&#25105;&#20204;&#33021;&#22815;&#25429;&#25417;&#29289;&#31181;&#30340;&#32452;&#25104;&#21644;&#31995;&#32479;&#20998;&#31867;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2401.13219</link><description>&lt;p&gt;
TEPI: &#23545;&#31232;&#32570;&#26631;&#35760;&#30340;&#38646;&#26679;&#26412;&#22522;&#22240;&#32452;&#20998;&#31867;&#36827;&#34892;&#20998;&#31867;&#24863;&#30693;&#23884;&#20837;&#21644;&#20266;&#25104;&#20687;
&lt;/p&gt;
&lt;p&gt;
TEPI: Taxonomy-aware Embedding and Pseudo-Imaging for Scarcely-labeled Zero-shot Genome Classification. (arXiv:2401.13219v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31232;&#32570;&#26631;&#35760;&#30340;&#38646;&#26679;&#26412;&#22522;&#22240;&#32452;&#20998;&#31867;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#31216;&#20026;TEPI&#12290;&#36890;&#36807;&#23558;&#22522;&#22240;&#32452;&#34920;&#31034;&#20026;&#20266;&#22270;&#20687;&#24182;&#23558;&#20854;&#26144;&#23556;&#21040;&#20998;&#31867;&#24863;&#30693;&#30340;&#23884;&#20837;&#31354;&#38388;&#65292;&#25105;&#20204;&#33021;&#22815;&#25429;&#25417;&#29289;&#31181;&#30340;&#32452;&#25104;&#21644;&#31995;&#32479;&#20998;&#31867;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#31181;&#30340;&#22522;&#22240;&#32452;&#32534;&#30721;&#20102;&#26377;&#20215;&#20540;&#30340;&#36827;&#21270;&#12289;&#29983;&#29289;&#21644;&#31995;&#32479;&#20998;&#31867;&#20449;&#24687;&#65292;&#26377;&#21161;&#20110;&#29289;&#31181;&#35782;&#21035;&#12289;&#20998;&#31867;&#21644;&#29702;&#35299;&#22522;&#22240;&#26131;&#24863;&#24615;&#65292;&#22914;&#33647;&#29289;&#25239;&#24615;&#21644;&#27602;&#21147;&#12290;&#28982;&#32780;&#65292;&#24040;&#22823;&#30340;&#29289;&#31181;&#25968;&#37327;&#32473;&#24320;&#21457;&#36890;&#29992;&#30340;&#20840;&#22522;&#22240;&#32452;&#20998;&#31867;&#24037;&#20855;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;&#29983;&#29289;&#20449;&#24687;&#23398;&#24037;&#20855;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#32570;&#20047;&#21487;&#25193;&#23637;&#24615;&#24182;&#19988;&#35745;&#31639;&#25104;&#26412;&#39640;&#12290;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#24517;&#39035;&#35299;&#20915;&#20855;&#26377;&#38271;&#23614;&#20998;&#24067;&#30340;&#22823;&#20998;&#31867;&#35789;&#27719;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;TEPI&#65292;&#21363;&#20998;&#31867;&#24863;&#30693;&#23884;&#20837;&#21644;&#20266;&#25104;&#20687;&#65292;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#27599;&#20010;&#22522;&#22240;&#32452;&#34920;&#31034;&#20026;&#20266;&#22270;&#20687;&#65292;&#24182;&#23558;&#20854;&#26144;&#23556;&#21040;&#20998;&#31867;&#24863;&#30693;&#30340;&#23884;&#20837;&#31354;&#38388;&#36827;&#34892;&#25512;&#29702;&#21644;&#20998;&#31867;&#12290;&#36825;&#20010;&#23884;&#20837;&#31354;&#38388;&#25429;&#25417;&#20102;&#29289;&#31181;&#30340;&#32452;&#25104;&#21644;&#31995;&#32479;&#20998;&#31867;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
A species' genetic code or genome encodes valuable evolutionary, biological, and phylogenetic information that aids in species recognition, taxonomic classification, and understanding genetic predispositions like drug resistance and virulence. However, the vast number of potential species poses significant challenges in developing a general-purpose whole genome classification tool. Traditional bioinformatics tools have made notable progress but lack scalability and are computationally expensive. Machine learning-based frameworks show promise but must address the issue of large classification vocabularies with long-tail distributions. In this study, we propose addressing this problem through zero-shot learning using TEPI, Taxonomy-aware Embedding and Pseudo-Imaging. We represent each genome as pseudo-images and map them to a taxonomy-aware embedding space for reasoning and classification. This embedding space captures compositional and phylogenetic relationships of species, enabling pre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#22810;&#23618;&#27425;&#27880;&#24847;&#21147;&#32593;&#32476;(AMANet)&#65292;&#29992;&#20110;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;(SAR)&#22270;&#20687;&#20013;&#30340;&#33337;&#33334;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#22810;&#23610;&#24230;&#29305;&#24449;&#21644;&#33258;&#36866;&#24212;&#32858;&#21512;&#26174;&#33879;&#29305;&#24449;&#65292;&#35299;&#20915;&#20102;&#28023;&#23736;&#29615;&#22659;&#20013;&#23567;&#22411;&#21644;&#27839;&#28023;&#33337;&#33334;&#26816;&#27979;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.13214</link><description>&lt;p&gt;
AMANet&#65306;&#21033;&#29992;&#33258;&#36866;&#24212;&#22810;&#23618;&#27425;&#27880;&#24847;&#21147;&#32593;&#32476;&#25552;&#21319;SAR&#33337;&#33334;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
AMANet: Advancing SAR Ship Detection with Adaptive Multi-Hierarchical Attention Network. (arXiv:2401.13214v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13214
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#22810;&#23618;&#27425;&#27880;&#24847;&#21147;&#32593;&#32476;(AMANet)&#65292;&#29992;&#20110;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;(SAR)&#22270;&#20687;&#20013;&#30340;&#33337;&#33334;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#22810;&#23610;&#24230;&#29305;&#24449;&#21644;&#33258;&#36866;&#24212;&#32858;&#21512;&#26174;&#33879;&#29305;&#24449;&#65292;&#35299;&#20915;&#20102;&#28023;&#23736;&#29615;&#22659;&#20013;&#23567;&#22411;&#21644;&#27839;&#28023;&#33337;&#33334;&#26816;&#27979;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;(SAR)&#22270;&#20687;&#30340;&#33337;&#33334;&#26816;&#27979;&#12290;&#23613;&#31649;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#33337;&#33334;&#26816;&#27979;&#26041;&#27861;&#65292;&#20294;&#30001;&#20110;&#28023;&#23736;&#29615;&#22659;&#20013;&#29305;&#24449;&#26377;&#38480;&#21644;&#26434;&#20081;&#24178;&#25200;&#30340;&#21407;&#22240;&#65292;&#26816;&#27979;&#23567;&#22411;&#21644;&#27839;&#28023;&#33337;&#33334;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#22810;&#23618;&#27425;&#27880;&#24847;&#21147;&#27169;&#22359;(AMAM)&#65292;&#29992;&#20110;&#23398;&#20064;&#22810;&#23610;&#24230;&#29305;&#24449;&#65292;&#24182;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#33258;&#36866;&#24212;&#22320;&#32858;&#21512;&#26174;&#33879;&#29305;&#24449;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#34701;&#21512;&#30456;&#37051;&#29305;&#24449;&#23618;&#30340;&#20449;&#24687;&#65292;&#22686;&#24378;&#23545;&#36739;&#23567;&#30446;&#26631;&#30340;&#26816;&#27979;&#65292;&#20174;&#32780;&#23454;&#29616;&#22810;&#23610;&#24230;&#29305;&#24449;&#22686;&#24378;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#28388;&#38500;&#22797;&#26434;&#32972;&#26223;&#30340;&#36127;&#38754;&#25928;&#26524;&#65292;&#25105;&#20204;&#22312;&#36890;&#36947;&#19978;&#35299;&#21078;&#20043;&#21069;&#34701;&#21512;&#30340;&#22810;&#32423;&#29305;&#24449;&#65292;&#21333;&#29420;&#25366;&#25496;&#26174;&#33879;&#21306;&#22495;&#65292;&#24182;&#33258;&#36866;&#24212;&#22320;&#27719;&#38598;&#26469;&#33258;&#19981;&#21516;&#36890;&#36947;&#30340;&#29305;&#24449;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#22810;&#23618;&#27425;&#27880;&#24847;&#21147;&#32593;&#32476;(AMANet)&#65292;&#29992;&#20110;&#32508;&#21512;&#24314;&#27169;&#21644;&#33337;&#33334;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, methods based on deep learning have been successfully applied to ship detection for synthetic aperture radar (SAR) images. Despite the development of numerous ship detection methodologies, detecting small and coastal ships remains a significant challenge due to the limited features and clutter in coastal environments. For that, a novel adaptive multi-hierarchical attention module (AMAM) is proposed to learn multi-scale features and adaptively aggregate salient features from various feature layers, even in complex environments. Specifically, we first fuse information from adjacent feature layers to enhance the detection of smaller targets, thereby achieving multi-scale feature enhancement. Then, to filter out the adverse effects of complex backgrounds, we dissect the previously fused multi-level features on the channel, individually excavate the salient regions, and adaptively amalgamate features originating from different channels. Thirdly, we present a novel adaptive multi-h
&lt;/p&gt;</description></item><item><title>AdCorDA&#26041;&#27861;&#36890;&#36807;&#23545;&#25239;&#20462;&#27491;&#21644;&#39046;&#22495;&#36866;&#24212;&#25913;&#36827;&#39044;&#35757;&#32451;&#20998;&#31867;&#22120;&#32593;&#32476;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;CIFAR-100&#25968;&#25454;&#38598;&#19978;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#20934;&#30830;&#29575;&#65292;&#24182;&#19988;&#22312;&#26435;&#37325;&#37327;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#19978;&#20063;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.13212</link><description>&lt;p&gt;
AdCorDA: &#36890;&#36807;&#23545;&#25239;&#20462;&#27491;&#21644;&#39046;&#22495;&#36866;&#24212;&#36827;&#34892;&#20998;&#31867;&#22120;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
AdCorDA: Classifier Refinement via Adversarial Correction and Domain Adaptation. (arXiv:2401.13212v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13212
&lt;/p&gt;
&lt;p&gt;
AdCorDA&#26041;&#27861;&#36890;&#36807;&#23545;&#25239;&#20462;&#27491;&#21644;&#39046;&#22495;&#36866;&#24212;&#25913;&#36827;&#39044;&#35757;&#32451;&#20998;&#31867;&#22120;&#32593;&#32476;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;CIFAR-100&#25968;&#25454;&#38598;&#19978;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#20934;&#30830;&#29575;&#65292;&#24182;&#19988;&#22312;&#26435;&#37325;&#37327;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#19978;&#20063;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#25913;&#36827;&#39044;&#35757;&#32451;&#20998;&#31867;&#22120;&#32593;&#32476;&#12290;&#25152;&#25552;&#20986;&#30340;AdCorDA&#26041;&#27861;&#22522;&#20110;&#20462;&#25913;&#35757;&#32451;&#38598;&#65292;&#24182;&#21033;&#29992;&#32593;&#32476;&#26435;&#37325;&#21644;&#23618;&#36755;&#20837;&#20043;&#38388;&#30340;&#23545;&#20598;&#24615;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#36755;&#20837;&#31354;&#38388;&#35757;&#32451;&#12290;&#35813;&#26041;&#27861;&#30001;&#20004;&#20010;&#38454;&#27573;&#32452;&#25104; - &#23545;&#25239;&#20462;&#27491;&#65292;&#28982;&#21518;&#36827;&#34892;&#39046;&#22495;&#36866;&#24212;&#12290;&#23545;&#25239;&#20462;&#27491;&#20351;&#29992;&#23545;&#25239;&#24615;&#25915;&#20987;&#26469;&#20462;&#27491;&#38169;&#35823;&#30340;&#35757;&#32451;&#38598;&#20998;&#31867;&#12290;&#23558;&#35757;&#32451;&#38598;&#20013;&#38169;&#35823;&#20998;&#31867;&#30340;&#26679;&#26412;&#31227;&#38500;&#65292;&#24182;&#29992;&#23545;&#25239;&#20462;&#27491;&#30340;&#26679;&#26412;&#26367;&#25442;&#65292;&#24418;&#25104;&#26032;&#30340;&#35757;&#32451;&#38598;&#65292;&#28982;&#21518;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#36827;&#34892;&#39046;&#22495;&#36866;&#24212;&#22238;&#21040;&#21407;&#22987;&#35757;&#32451;&#38598;&#12290;&#22823;&#37327;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;CIFAR-100&#25968;&#25454;&#38598;&#19978;&#65292;&#20934;&#30830;&#29575;&#25552;&#21319;&#36229;&#36807;5%&#12290;&#35813;&#25216;&#26415;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#26435;&#37325;&#37327;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#25913;&#36827;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;&#22522;&#32447;&#19978;&#24615;&#33021;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes a simple yet effective technique for refining a pretrained classifier network. The proposed AdCorDA method is based on modification of the training set and making use of the duality between network weights and layer inputs. We call this input space training. The method consists of two stages - adversarial correction followed by domain adaptation. Adversarial correction uses adversarial attacks to correct incorrect training-set classifications. The incorrectly classified samples of the training set are removed and replaced with the adversarially corrected samples to form a new training set, and then, in the second stage, domain adaptation is performed back to the original training set. Extensive experimental validations show significant accuracy boosts of over 5% on the CIFAR-100 dataset. The technique can be straightforwardly applied to refinement of weight-quantized neural networks, where experiments show substantial enhancement in performance over the baseline. T
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#40657;&#30418;&#23545;&#25239;&#24615;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#26412;&#22320;&#28151;&#21512;&#21644;&#33258;&#36866;&#24212;&#27493;&#38271;&#30340;&#35774;&#35745;&#65292;&#22686;&#24378;&#20102;&#36755;&#20837;&#22810;&#26679;&#24615;&#65292;&#24182;&#19988;&#33021;&#22815;&#29983;&#25104;&#21487;&#36801;&#31227;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2401.13205</link><description>&lt;p&gt;
&#36890;&#36807;&#26412;&#22320;&#28151;&#21512;&#21644;&#33258;&#36866;&#24212;&#27493;&#38271;&#25552;&#39640;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#21487;&#36801;&#31227;&#24615;
&lt;/p&gt;
&lt;p&gt;
Boosting the Transferability of Adversarial Examples via Local Mixup and Adaptive Step Size. (arXiv:2401.13205v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13205
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#40657;&#30418;&#23545;&#25239;&#24615;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#26412;&#22320;&#28151;&#21512;&#21644;&#33258;&#36866;&#24212;&#27493;&#38271;&#30340;&#35774;&#35745;&#65292;&#22686;&#24378;&#20102;&#36755;&#20837;&#22810;&#26679;&#24615;&#65292;&#24182;&#19988;&#33021;&#22815;&#29983;&#25104;&#21487;&#36801;&#31227;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#26679;&#26412;&#26159;&#21508;&#31181;&#35270;&#35273;&#24212;&#29992;&#20013;&#19968;&#20010;&#20851;&#38190;&#30340;&#23433;&#20840;&#23041;&#32961;&#65292;&#27880;&#20837;&#20154;&#30524;&#26080;&#27861;&#23519;&#35273;&#30340;&#25200;&#21160;&#21487;&#20197;&#28151;&#28102;&#36755;&#20986;&#12290;&#22312;&#40657;&#30418;&#29615;&#22659;&#19979;&#29983;&#25104;&#21487;&#36801;&#31227;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#22312;&#23454;&#36341;&#20013;&#33267;&#20851;&#37325;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#36755;&#20837;&#22810;&#26679;&#24615;&#30340;&#26041;&#27861;&#37319;&#29992;&#19981;&#21516;&#30340;&#22270;&#20687;&#21464;&#25442;&#65292;&#20294;&#30001;&#20110;&#36755;&#20837;&#22810;&#26679;&#24615;&#19981;&#36275;&#21644;&#30456;&#21516;&#30340;&#25200;&#21160;&#27493;&#38271;&#21487;&#33021;&#25928;&#29575;&#20302;&#19979;&#12290;&#21463;&#21040;&#19981;&#21516;&#22270;&#20687;&#21306;&#22495;&#22312;&#20998;&#31867;&#20013;&#20855;&#26377;&#19981;&#21516;&#26435;&#37325;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#36890;&#36807;&#21516;&#26102;&#35774;&#35745;&#22686;&#24378;&#30340;&#36755;&#20837;&#22810;&#26679;&#24615;&#21644;&#33258;&#36866;&#24212;&#30340;&#27493;&#38271;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#40657;&#30418;&#23545;&#25239;&#29983;&#25104;&#26694;&#26550;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#26412;&#22320;&#28151;&#21512;&#26469;&#38543;&#26426;&#28151;&#21512;&#19968;&#32452;&#21464;&#25442;&#21518;&#30340;&#23545;&#25239;&#24615;&#22270;&#20687;&#65292;&#22686;&#24378;&#20102;&#36755;&#20837;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#31934;&#30830;&#29983;&#25104;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#25105;&#20204;&#23558;&#25200;&#21160;&#25237;&#24433;&#21040;$tanh$&#31354;&#38388;&#20197;&#25918;&#26494;&#36793;&#30028;&#32422;&#26463;&#12290;&#27492;&#22806;&#65292;&#19981;&#21516;&#21306;&#22495;&#30340;&#27493;&#38271;&#21487;&#20197;&#36890;&#36807;&#25972;&#21512;&#36866;&#24212;&#24615;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial examples are one critical security threat to various visual applications, where injected human-imperceptible perturbations can confuse the output.Generating transferable adversarial examples in the black-box setting is crucial but challenging in practice. Existing input-diversity-based methods adopt different image transformations, but may be inefficient due to insufficient input diversity and an identical perturbation step size. Motivated by the fact that different image regions have distinctive weights in classification, this paper proposes a black-box adversarial generative framework by jointly designing enhanced input diversity and adaptive step sizes. We design local mixup to randomly mix a group of transformed adversarial images, strengthening the input diversity. For precise adversarial generation, we project the perturbation into the $tanh$ space to relax the boundary constraint. Moreover, the step sizes of different regions can be dynamically adjusted by integratin
&lt;/p&gt;</description></item><item><title>MLLMReID&#26159;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#29289;&#20877;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#24182;&#23558;&#20854;&#35270;&#35273;&#32534;&#30721;&#22120;&#20316;&#20026;&#20027;&#24178;&#36827;&#34892;&#20248;&#21270;&#65292;&#35299;&#20915;&#20102;MLLM&#22312;ReID&#20219;&#21153;&#20013;&#30340;&#35774;&#35745;&#25351;&#20196;&#21644;&#29305;&#24449;&#23398;&#20064;&#25928;&#26524;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.13201</link><description>&lt;p&gt;
MLLMReID: &#22522;&#20110;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#29289;&#20877;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
MLLMReID: Multimodal Large Language Model-based Person Re-identification. (arXiv:2401.13201v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13201
&lt;/p&gt;
&lt;p&gt;
MLLMReID&#26159;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#29289;&#20877;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#24182;&#23558;&#20854;&#35270;&#35273;&#32534;&#30721;&#22120;&#20316;&#20026;&#20027;&#24178;&#36827;&#34892;&#20248;&#21270;&#65292;&#35299;&#20915;&#20102;MLLM&#22312;ReID&#20219;&#21153;&#20013;&#30340;&#35774;&#35745;&#25351;&#20196;&#21644;&#29305;&#24449;&#23398;&#20064;&#25928;&#26524;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#20154;&#29289;&#20877;&#35782;&#21035;&#65288;ReID&#65289;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#23578;&#26410;&#34987;&#30740;&#31350;&#12290;&#26412;&#25991;&#23558;&#30740;&#31350;&#22914;&#20309;&#23558;&#23427;&#20204;&#36866;&#24212;&#20110;ReID&#20219;&#21153;&#12290;&#19968;&#31181;&#30452;&#35266;&#30340;&#24819;&#27861;&#26159;&#20351;&#29992;ReID&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#23545;MLLM&#36827;&#34892;&#24494;&#35843;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#20316;&#20026;ReID&#30340;&#20027;&#24178;&#12290;&#28982;&#32780;&#65292;&#20173;&#23384;&#22312;&#20004;&#20010;&#26126;&#26174;&#30340;&#38382;&#39064;&#65306;&#65288;1&#65289;&#20026;ReID&#35774;&#35745;&#25351;&#20196;&#26102;&#65292;MLLM&#21487;&#33021;&#36807;&#24230;&#25311;&#21512;&#29305;&#23450;&#25351;&#20196;&#65292;&#32780;&#35774;&#35745;&#21508;&#31181;&#25351;&#20196;&#23558;&#23548;&#33268;&#26356;&#39640;&#30340;&#25104;&#26412;&#12290;&#65288;2&#65289;LLM&#30340;&#28508;&#22312;&#22270;&#20687;&#29305;&#24449;&#21521;&#37327;&#27809;&#26377;&#21442;&#19982;&#25439;&#22833;&#35745;&#31639;&#12290;&#25351;&#20196;&#23398;&#20064;&#65292;&#23545;&#40784;&#22270;&#20687;-&#25991;&#26412;&#29305;&#24449;&#65292;&#23548;&#33268;&#38388;&#25509;&#20248;&#21270;&#21644;&#23398;&#20064;&#30446;&#26631;&#19981;&#20805;&#20998;&#21033;&#29992;&#29305;&#24449;&#65292;&#38480;&#21046;&#20102;&#20154;&#29289;&#29305;&#24449;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;MLLMReID&#65306;&#22522;&#20110;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;ReID&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20844;&#20849;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal large language models (MLLM) have achieved satisfactory results in many tasks. However, their performance in the task of person re-identification (ReID) has not been explored to date. This paper will investigate how to adapt them for the task of ReID. An intuitive idea is to fine-tune MLLM with ReID image-text datasets, and then use their visual encoder as a backbone for ReID. However, there still exist two apparent issues: (1) Designing instructions for ReID, MLLMs may overfit specific instructions, and designing a variety of instructions will lead to higher costs. (2) Latent image feature vectors from LLMs are not involved in loss computation. Instructional learning, aligning image-text features, results in indirect optimization and a learning objective that inadequately utilizes features, limiting effectiveness in person feature learning. To address these problems, this paper proposes MLLMReID: Multimodal Large Language Model-based ReID. Firstly, we proposed Common Instru
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;Catch-Up Mix&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36807;&#20110;&#20381;&#36182;&#29305;&#23450;&#28388;&#27874;&#22120;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#35266;&#23519;&#21040;&#24930;&#23398;&#20064;&#28388;&#27874;&#22120;&#22240;&#20026;&#24555;&#23398;&#20064;&#28388;&#27874;&#22120;&#32780;&#22833;&#21435;&#23398;&#20064;&#26426;&#20250;&#65292;&#20511;&#37492;&#22270;&#20687;&#22686;&#24378;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#35813;&#26041;&#27861;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.13193</link><description>&lt;p&gt;
Catch-Up Mix: &#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#38024;&#23545;CNN&#28388;&#27874;&#22120;&#30340;Catch-Up Class
&lt;/p&gt;
&lt;p&gt;
Catch-Up Mix: Catch-Up Class for Struggling Filters in CNN. (arXiv:2401.13193v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13193
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;Catch-Up Mix&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36807;&#20110;&#20381;&#36182;&#29305;&#23450;&#28388;&#27874;&#22120;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#35266;&#23519;&#21040;&#24930;&#23398;&#20064;&#28388;&#27874;&#22120;&#22240;&#20026;&#24555;&#23398;&#20064;&#28388;&#27874;&#22120;&#32780;&#22833;&#21435;&#23398;&#20064;&#26426;&#20250;&#65292;&#20511;&#37492;&#22270;&#20687;&#22686;&#24378;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#35813;&#26041;&#27861;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#23588;&#20854;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#20855;&#26377;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#20063;&#32463;&#24120;&#38754;&#20020;&#19982;&#22797;&#26434;&#24615;&#21644;&#36807;&#25311;&#21512;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;&#19968;&#20010;&#20540;&#24471;&#20851;&#27880;&#30340;&#38382;&#39064;&#26159;&#65292;&#27169;&#22411;&#36890;&#24120;&#36807;&#20110;&#20381;&#36182;&#26377;&#38480;&#30340;&#19968;&#37096;&#20998;&#28388;&#27874;&#22120;&#36827;&#34892;&#39044;&#27979;&#12290;&#36825;&#31181;&#20381;&#36182;&#24615;&#21487;&#33021;&#23548;&#33268;&#27867;&#21270;&#33021;&#21147;&#21463;&#25439;&#65292;&#24182;&#22686;&#21152;&#23545;&#24494;&#23567;&#21464;&#21270;&#30340;&#33030;&#24369;&#24615;&#12290;&#34429;&#28982;&#24120;&#35265;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#22914;&#26435;&#37325;&#34928;&#20943;&#12289;dropout&#21644;&#25968;&#25454;&#22686;&#24378;&#21487;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#24182;&#27809;&#26377;&#30452;&#25509;&#35299;&#20915;&#23545;&#29305;&#23450;&#28388;&#27874;&#22120;&#30340;&#20381;&#36182;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#24403;&#24930;&#23398;&#20064;&#30340;&#28388;&#27874;&#22120;&#30001;&#20110;&#24555;&#23398;&#20064;&#30340;&#28388;&#27874;&#22120;&#32780;&#22833;&#21435;&#20102;&#23398;&#20064;&#26426;&#20250;&#26102;&#65292;&#36825;&#20010;&#20381;&#36182;&#24615;&#38382;&#39064;&#20250;&#21464;&#24471;&#20005;&#37325;&#12290;&#21463;&#22270;&#20687;&#22686;&#24378;&#30740;&#31350;&#30340;&#21551;&#21457;&#65292;&#35813;&#30740;&#31350;&#36890;&#36807;&#21024;&#38500;&#21644;&#26367;&#25442;&#22270;&#20687;&#30340;&#37096;&#20998;&#26469;&#35299;&#20915;&#23545;&#29305;&#23450;&#22270;&#20687;&#21306;&#22495;&#30340;&#36807;&#20110;&#20381;&#36182;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#24819;&#27861;&#26159;&#32531;&#35299;&#36807;&#20110;&#20381;&#36182;&#29305;&#23450;&#28388;&#27874;&#22120;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has made significant advances in computer vision, particularly in image classification tasks. Despite their high accuracy on training data, deep learning models often face challenges related to complexity and overfitting. One notable concern is that the model often relies heavily on a limited subset of filters for making predictions. This dependency can result in compromised generalization and an increased vulnerability to minor variations. While regularization techniques like weight decay, dropout, and data augmentation are commonly used to address this issue, they may not directly tackle the reliance on specific filters. Our observations reveal that the heavy reliance problem gets severe when slow-learning filters are deprived of learning opportunities due to fast-learning filters. Drawing inspiration from image augmentation research that combats over-reliance on specific image regions by removing and replacing parts of images, our idea is to mitigate the problem of ove
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28857;&#20113;&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#26230;&#20307;&#32467;&#26500;&#29983;&#25104;&#35774;&#35745;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#37325;&#24314;&#36755;&#20837;&#32467;&#26500;&#21644;&#29983;&#25104;&#20840;&#26032;&#26448;&#26009;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.13192</link><description>&lt;p&gt;
&#22522;&#20110;&#28857;&#20113;&#34920;&#31034;&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#26230;&#20307;&#32467;&#26500;&#29983;&#25104;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Generative Design of Crystal Structures by Point Cloud Representations and Diffusion Model. (arXiv:2401.13192v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28857;&#20113;&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#26230;&#20307;&#32467;&#26500;&#29983;&#25104;&#35774;&#35745;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#37325;&#24314;&#36755;&#20837;&#32467;&#26500;&#21644;&#29983;&#25104;&#20840;&#26032;&#26448;&#26009;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26448;&#26009;&#35774;&#35745;&#20013;&#65292;&#39640;&#25928;&#22320;&#29983;&#25104;&#33021;&#37327;&#31283;&#23450;&#30340;&#26230;&#20307;&#32467;&#26500;&#19968;&#30452;&#26159;&#20010;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#26230;&#26684;&#20013;&#21407;&#23376;&#30340;&#24040;&#22823;&#25490;&#21015;&#12290;&#20026;&#20102;&#20419;&#36827;&#31283;&#23450;&#26448;&#26009;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#21487;&#21512;&#25104;&#26448;&#26009;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#28857;&#20113;&#34920;&#31034;&#26469;&#32534;&#30721;&#22797;&#26434;&#30340;&#32467;&#26500;&#20449;&#24687;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#30340;&#26680;&#24515;&#26159;&#24341;&#20837;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#22522;&#30784;&#25903;&#26609;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#23427;&#26469;&#37325;&#24314;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#36755;&#20837;&#32467;&#26500;&#65292;&#24182;&#20005;&#26684;&#39564;&#35777;&#20854;&#39640;&#37325;&#24314;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#29983;&#25104;&#20840;&#26032;&#30340;&#26448;&#26009;&#65292;&#37325;&#28857;&#24378;&#35843;&#20102;&#22522;&#20110;&#28857;&#20113;&#30340;&#26230;&#20307;&#25193;&#25955;(PCCD)&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#21487;&#21512;&#25104;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22312;&#26448;&#26009;&#35774;&#35745;&#21644;&#21512;&#25104;&#30340;&#25512;&#36827;&#20013;&#65292;&#36890;&#36807;&#20808;&#36827;&#30340;&#29983;&#25104;&#35774;&#35745;&#26041;&#27861;&#65292;&#20570;&#20986;&#20102;&#26174;&#33879;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiently generating energetically stable crystal structures has long been a challenge in material design, primarily due to the immense arrangement of atoms in a crystal lattice. To facilitate the discovery of stable material, we present a framework for the generation of synthesizable materials, leveraging a point cloud representation to encode intricate structural information. At the heart of this framework lies the introduction of a diffusion model as its foundational pillar. To gauge the efficacy of our approach, we employ it to reconstruct input structures from our training datasets, rigorously validating its high reconstruction performance. Furthermore, we demonstrate the profound potential of Point Cloud-Based Crystal Diffusion (PCCD) by generating entirely new materials, emphasizing their synthesizability. Our research stands as a noteworthy contribution to the advancement of materials design and synthesis through the cutting-edge avenue of generative design instead of the con
&lt;/p&gt;</description></item><item><title>AgentBoard&#26159;&#19968;&#20010;&#32508;&#21512;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#35780;&#20272;&#26694;&#26550;&#65292;&#19987;&#20026;&#20998;&#26512;&#35780;&#20272;LLM&#26234;&#33021;&#20307;&#32780;&#35774;&#35745;&#65292;&#35299;&#20915;&#20102;&#22312;&#22810;&#36718;&#20132;&#20114;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#23545;&#26234;&#33021;&#20307;&#24615;&#33021;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#32454;&#31890;&#24230;&#30340;&#36827;&#23637;&#29575;&#25351;&#26631;&#21644;&#35780;&#20272;&#24037;&#20855;&#21253;&#12290;</title><link>http://arxiv.org/abs/2401.13178</link><description>&lt;p&gt;
AgentBoard: &#19968;&#31181;&#22810;&#36718;LLM&#26234;&#33021;&#20307;&#30340;&#20998;&#26512;&#35780;&#20272;&#26495;
&lt;/p&gt;
&lt;p&gt;
AgentBoard: An Analytical Evaluation Board of Multi-turn LLM Agents. (arXiv:2401.13178v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13178
&lt;/p&gt;
&lt;p&gt;
AgentBoard&#26159;&#19968;&#20010;&#32508;&#21512;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#35780;&#20272;&#26694;&#26550;&#65292;&#19987;&#20026;&#20998;&#26512;&#35780;&#20272;LLM&#26234;&#33021;&#20307;&#32780;&#35774;&#35745;&#65292;&#35299;&#20915;&#20102;&#22312;&#22810;&#36718;&#20132;&#20114;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#23545;&#26234;&#33021;&#20307;&#24615;&#33021;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#32454;&#31890;&#24230;&#30340;&#36827;&#23637;&#29575;&#25351;&#26631;&#21644;&#35780;&#20272;&#24037;&#20855;&#21253;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#36890;&#29992;&#26234;&#33021;&#20307;&#23545;&#20110;&#29702;&#35299;&#20854;&#33021;&#21147;&#24182;&#20419;&#36827;&#20854;&#34701;&#20837;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#36807;&#31243;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#20027;&#35201;&#38556;&#30861;&#20043;&#19968;&#26159;&#22312;&#32479;&#19968;&#26694;&#26550;&#20869;&#23545;&#26234;&#33021;&#20307;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#29305;&#21035;&#26159;&#22312;&#32500;&#25252;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#21644;&#30830;&#20445;&#22810;&#36718;&#20132;&#20114;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;&#35780;&#20272;&#26694;&#26550;&#20027;&#35201;&#20851;&#27880;&#26368;&#32456;&#25104;&#21151;&#29575;&#65292;&#36807;&#31243;&#20013;&#25552;&#20379;&#30340;&#35265;&#35299;&#24456;&#23569;&#65292;&#26080;&#27861;&#28145;&#20837;&#29702;&#35299;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AgentBoard&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#32508;&#21512;&#22522;&#20934;&#21644;&#20276;&#38543;&#30340;&#24320;&#28304;&#35780;&#20272;&#26694;&#26550;&#65292;&#19987;&#20026;LLM&#26234;&#33021;&#20307;&#30340;&#20998;&#26512;&#35780;&#20272;&#32780;&#35774;&#35745;&#12290;AgentBoard&#25552;&#20379;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#30340;&#36827;&#23637;&#29575;&#25351;&#26631;&#65292;&#25429;&#25417;&#36880;&#27493;&#30340;&#36827;&#23637;&#65292;&#20197;&#21450;&#19968;&#20010;&#32508;&#21512;&#30340;&#35780;&#20272;&#24037;&#20855;&#21253;&#65292;&#20855;&#26377;&#26131;&#20110;&#35780;&#20272;&#21644;&#20998;&#26512;&#27169;&#22411;&#33021;&#21147;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating large language models (LLMs) as general-purpose agents is essential for understanding their capabilities and facilitating their integration into practical applications. However, the evaluation process presents substantial challenges. A primary obstacle is the benchmarking of agent performance across diverse scenarios within a unified framework, especially in maintaining partially-observable environments and ensuring multi-round interactions. Moreover, current evaluation frameworks mostly focus on the final success rate, revealing few insights during the process and failing to provide a deep understanding of the model abilities. To address these challenges, we introduce AgentBoard, a pioneering comprehensive benchmark and accompanied open-source evaluation framework tailored to analytical evaluation of LLM agents. AgentBoard offers a fine-grained progress rate metric that captures incremental advancements as well as a comprehensive evaluation toolkit that features easy assess
&lt;/p&gt;</description></item><item><title>&#36870;&#21521;&#35774;&#35745;&#36215;&#21040;&#20248;&#21270;&#24213;&#23618;&#30446;&#26631;&#20989;&#25968;&#30340;&#20316;&#29992;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21033;&#29992;&#20102;&#23398;&#20064;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#36827;&#34892;&#20248;&#21270;&#12290;&#36890;&#36807;&#20248;&#21270;&#25193;&#25955;&#27169;&#22411;&#25429;&#33719;&#30340;&#23398;&#20064;&#33021;&#37327;&#20989;&#25968;&#65292;&#21487;&#20197;&#36991;&#20813;&#23545;&#25239;&#31034;&#20363;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#35774;&#35745;&#24615;&#33021;&#12290;&#36825;&#19968;&#35774;&#35745;&#31995;&#32479;&#26159;&#32452;&#21512;&#24615;&#30340;&#65292;&#20351;&#24471;&#21487;&#20197;&#35774;&#35745;&#20855;&#26377;&#27599;&#20010;&#25351;&#23450;&#32452;&#20214;&#30340;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2401.13171</link><description>&lt;p&gt;
&#32452;&#21512;&#24335;&#29983;&#25104;&#36870;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Compositional Generative Inverse Design. (arXiv:2401.13171v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13171
&lt;/p&gt;
&lt;p&gt;
&#36870;&#21521;&#35774;&#35745;&#36215;&#21040;&#20248;&#21270;&#24213;&#23618;&#30446;&#26631;&#20989;&#25968;&#30340;&#20316;&#29992;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21033;&#29992;&#20102;&#23398;&#20064;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#36827;&#34892;&#20248;&#21270;&#12290;&#36890;&#36807;&#20248;&#21270;&#25193;&#25955;&#27169;&#22411;&#25429;&#33719;&#30340;&#23398;&#20064;&#33021;&#37327;&#20989;&#25968;&#65292;&#21487;&#20197;&#36991;&#20813;&#23545;&#25239;&#31034;&#20363;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#35774;&#35745;&#24615;&#33021;&#12290;&#36825;&#19968;&#35774;&#35745;&#31995;&#32479;&#26159;&#32452;&#21512;&#24615;&#30340;&#65292;&#20351;&#24471;&#21487;&#20197;&#35774;&#35745;&#20855;&#26377;&#27599;&#20010;&#25351;&#23450;&#32452;&#20214;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#35774;&#35745;&#26159;&#19968;&#31181;&#23547;&#27714;&#35774;&#35745;&#36755;&#20837;&#21464;&#37327;&#20197;&#20248;&#21270;&#24213;&#23618;&#30446;&#26631;&#20989;&#25968;&#30340;&#37325;&#35201;&#38382;&#39064;&#65292;&#22312;&#26426;&#26800;&#24037;&#31243;&#21040;&#33322;&#22825;&#24037;&#31243;&#31561;&#39046;&#22495;&#37117;&#26377;&#24212;&#29992;&#12290;&#36870;&#35774;&#35745;&#36890;&#24120;&#34987;&#26500;&#24314;&#25104;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21033;&#29992;&#20102;&#23398;&#20064;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#36827;&#34892;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27169;&#22411;&#30340;&#20248;&#21270;&#24448;&#24448;&#20250;&#38519;&#20837;&#23545;&#25239;&#27169;&#24335;&#65292;&#38459;&#30861;&#26377;&#25928;&#30340;&#25277;&#26679;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36890;&#36807;&#20248;&#21270;&#25193;&#25955;&#27169;&#22411;&#25429;&#33719;&#30340;&#23398;&#20064;&#33021;&#37327;&#20989;&#25968;&#65292;&#25105;&#20204;&#21487;&#20197;&#36991;&#20813;&#36825;&#31181;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#35774;&#35745;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#36825;&#26679;&#19968;&#20010;&#35774;&#35745;&#31995;&#32479;&#26159;&#32452;&#21512;&#24615;&#30340;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#32467;&#21512;&#22810;&#20010;&#19981;&#21516;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#20195;&#34920;&#25152;&#38656;&#31995;&#32479;&#30340;&#23376;&#32452;&#20214;&#65292;&#20174;&#32780;&#35774;&#35745;&#20855;&#26377;&#27599;&#20010;&#25351;&#23450;&#32452;&#20214;&#30340;&#31995;&#32479;&#12290;&#22312;&#19968;&#20010;N&#20307;&#30456;&#20114;&#20316;&#29992;&#20219;&#21153;&#21644;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20108;&#32500;&#22810;&#32764;&#22411;&#35774;&#35745;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#36890;&#36807;&#32452;&#21512;&#23398;&#20064;&#30340;&#33021;&#37327;&#20989;&#25968;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#35774;&#35745;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse design, where we seek to design input variables in order to optimize an underlying objective function, is an important problem that arises across fields such as mechanical engineering to aerospace engineering. Inverse design is typically formulated as an optimization problem, with recent works leveraging optimization across learned dynamics models. However, as models are optimized they tend to fall into adversarial modes, preventing effective sampling. We illustrate that by instead optimizing over the learned energy function captured by the diffusion model, we can avoid such adversarial examples and significantly improve design performance. We further illustrate how such a design system is compositional, enabling us to combine multiple different diffusion models representing subcomponents of our desired system to design systems with every specified component. In an N-body interaction task and a challenging 2D multi-airfoil design task, we demonstrate that by composing the learn
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24863;&#30693;&#30693;&#35782;&#34920;&#31034;&#26426;&#21046;&#65292;&#32858;&#28966;&#20110;&#22810;&#32500;&#25299;&#25169;&#20449;&#24687;&#21644;&#38544;&#21547;&#30340;&#26102;&#38388;&#30456;&#20851;&#20449;&#24687;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#21333;&#21442;&#25968;&#25299;&#25169;&#25688;&#35201;&#29983;&#25104;&#25968;&#25454;&#30340;&#22810;&#32500;&#25299;&#25169;&#25351;&#32441;&#12290;</title><link>http://arxiv.org/abs/2401.13157</link><description>&lt;p&gt;
&#20855;&#26377;&#22810;&#32500;&#25345;&#20037;&#24615;&#30340;&#21160;&#24577;&#23545;&#35937;&#30340;&#26102;&#38388;&#24863;&#30693;&#30693;&#35782;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Time-Aware Knowledge Representations of Dynamic Objects with Multidimensional Persistence. (arXiv:2401.13157v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24863;&#30693;&#30693;&#35782;&#34920;&#31034;&#26426;&#21046;&#65292;&#32858;&#28966;&#20110;&#22810;&#32500;&#25299;&#25169;&#20449;&#24687;&#21644;&#38544;&#21547;&#30340;&#26102;&#38388;&#30456;&#20851;&#20449;&#24687;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#21333;&#21442;&#25968;&#25299;&#25169;&#25688;&#35201;&#29983;&#25104;&#25968;&#25454;&#30340;&#22810;&#32500;&#25299;&#25169;&#25351;&#32441;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#21644;&#21160;&#24577;&#32593;&#32476;&#31561;&#26102;&#38388;&#21464;&#21270;&#30340;&#23545;&#35937;&#38656;&#35201;&#24320;&#21457;&#26032;&#30340;&#30693;&#35782;&#34920;&#31034;&#26426;&#21046;&#21644;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#20197;&#25429;&#25417;&#25968;&#25454;&#20013;&#30340;&#38544;&#21547;&#30340;&#26102;&#38388;&#30456;&#20851;&#20449;&#24687;&#12290;&#36825;&#20123;&#20449;&#24687;&#36890;&#24120;&#19981;&#30452;&#25509;&#35266;&#23519;&#21040;&#65292;&#20294;&#22312;&#23398;&#20064;&#20219;&#21153;&#24615;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26102;&#38388;&#30456;&#20851;&#25968;&#25454;&#32780;&#35328;&#65292;&#30693;&#35782;&#32534;&#30721;&#26426;&#21046;&#20013;&#32570;&#23569;&#26102;&#38388;&#32500;&#24230;&#20250;&#23548;&#33268;&#39057;&#32321;&#30340;&#27169;&#22411;&#26356;&#26032;&#65292;&#23398;&#20064;&#24615;&#33021;&#24046;&#65292;&#20174;&#32780;&#24433;&#21709;&#20915;&#31574;&#30340;&#36136;&#37327;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24863;&#30693;&#30693;&#35782;&#34920;&#31034;&#26426;&#21046;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#20851;&#27880;&#22810;&#20010;&#20960;&#20309;&#32500;&#24230;&#19978;&#30340;&#38544;&#21547;&#26102;&#38388;&#30456;&#20851;&#25299;&#25169;&#20449;&#24687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; \textit{Temporal MultiPersistence} (TMP) &#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#29616;&#26377;&#30340;&#21333;&#21442;&#25968;&#25299;&#25169;&#25688;&#35201;&#65292;&#29983;&#25104;&#25968;&#25454;&#30340;&#22810;&#32500;&#25299;&#25169;&#25351;&#32441;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning time-evolving objects such as multivariate time series and dynamic networks requires the development of novel knowledge representation mechanisms and neural network architectures, which allow for capturing implicit time-dependent information contained in the data. Such information is typically not directly observed but plays a key role in the learning task performance. In turn, lack of time dimension in knowledge encoding mechanisms for time-dependent data leads to frequent model updates, poor learning performance, and, as a result, subpar decision-making. Here we propose a new approach to a time-aware knowledge representation mechanism that notably focuses on implicit time-dependent topological information along multiple geometric dimensions. In particular, we propose a new approach, named \textit{Temporal MultiPersistence} (TMP), which produces multidimensional topological fingerprints of the data by using the existing single parameter topological summaries. The main idea be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#19977;&#31867;&#22686;&#21152;&#23545;AI&#20195;&#29702;&#21487;&#35265;&#24615;&#30340;&#25514;&#26045;&#65306;&#20195;&#29702;&#26631;&#35782;&#31526;&#12289;&#23454;&#26102;&#30417;&#25511;&#21644;&#27963;&#21160;&#35760;&#24405;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#23454;&#26045;&#26041;&#24335;&#12290;&#36825;&#20123;&#25514;&#26045;&#22312;&#38598;&#20013;&#21270;&#21644;&#21435;&#20013;&#24515;&#21270;&#37096;&#32626;&#29615;&#22659;&#20013;&#24212;&#29992;&#24191;&#27867;&#65292;&#26377;&#21161;&#20110;&#29702;&#35299;&#21644;&#20943;&#36731;AI&#20195;&#29702;&#24102;&#26469;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2401.13138</link><description>&lt;p&gt;
&#23545;AI&#20195;&#29702;&#30340;&#21487;&#35265;&#24615;
&lt;/p&gt;
&lt;p&gt;
Visibility into AI Agents. (arXiv:2401.13138v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13138
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#19977;&#31867;&#22686;&#21152;&#23545;AI&#20195;&#29702;&#21487;&#35265;&#24615;&#30340;&#25514;&#26045;&#65306;&#20195;&#29702;&#26631;&#35782;&#31526;&#12289;&#23454;&#26102;&#30417;&#25511;&#21644;&#27963;&#21160;&#35760;&#24405;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#23454;&#26045;&#26041;&#24335;&#12290;&#36825;&#20123;&#25514;&#26045;&#22312;&#38598;&#20013;&#21270;&#21644;&#21435;&#20013;&#24515;&#21270;&#37096;&#32626;&#29615;&#22659;&#20013;&#24212;&#29992;&#24191;&#27867;&#65292;&#26377;&#21161;&#20110;&#29702;&#35299;&#21644;&#20943;&#36731;AI&#20195;&#29702;&#24102;&#26469;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#21830;&#19994;&#12289;&#31185;&#23398;&#12289;&#25919;&#24220;&#21644;&#20010;&#20154;&#27963;&#21160;&#22996;&#25176;&#32473;&#20855;&#26377;&#26377;&#38480;&#30417;&#30563;&#33021;&#21147;&#30340;AI&#20195;&#29702;&#31995;&#32479;&#65292;&#21487;&#33021;&#20250;&#21152;&#21095;&#29616;&#26377;&#30340;&#31038;&#20250;&#39118;&#38505;&#24182;&#24341;&#20837;&#26032;&#30340;&#39118;&#38505;&#12290;&#29702;&#35299;&#21644;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#28041;&#21450;&#23545;&#29616;&#26377;&#27835;&#29702;&#32467;&#26500;&#36827;&#34892;&#25209;&#21028;&#24615;&#35780;&#20272;&#65292;&#26681;&#25454;&#38656;&#35201;&#36827;&#34892;&#20462;&#35746;&#21644;&#35843;&#25972;&#65292;&#24182;&#30830;&#20445;&#20851;&#38190;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#38382;&#36131;&#21046;&#12290;&#25105;&#20204;&#23558;AI&#20195;&#29702;&#30340;&#20351;&#29992;&#22320;&#28857;&#12289;&#21407;&#22240;&#12289;&#26041;&#24335;&#20197;&#21450;&#20351;&#29992;&#32773;&#31561;&#20449;&#24687;&#31216;&#20026;&#8220;&#21487;&#35265;&#24615;&#8221;&#65292;&#36825;&#23545;&#20110;&#23454;&#29616;&#19978;&#36848;&#30446;&#26631;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19977;&#31867;&#22686;&#21152;&#23545;AI&#20195;&#29702;&#21487;&#35265;&#24615;&#30340;&#25514;&#26045;&#65306;&#20195;&#29702;&#26631;&#35782;&#31526;&#12289;&#23454;&#26102;&#30417;&#25511;&#21644;&#27963;&#21160;&#35760;&#24405;&#12290;&#23545;&#20110;&#27599;&#19968;&#31181;&#25514;&#26045;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#21487;&#33021;&#30340;&#23454;&#26045;&#26041;&#24335;&#65292;&#36825;&#20123;&#26041;&#24335;&#22312;&#20405;&#20837;&#24615;&#21644;&#20449;&#24687;&#24615;&#26041;&#38754;&#26377;&#25152;&#24046;&#24322;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20123;&#25514;&#26045;&#22312;&#38598;&#20013;&#21270;&#21644;&#21435;&#20013;&#24515;&#21270;&#37096;&#32626;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;&#24773;&#20917;&#65292;&#32771;&#34385;&#20102;&#19981;&#21516;&#21464;&#37327;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Increased delegation of commercial, scientific, governmental, and personal activities to AI agents -- systems capable of pursuing complex goals with limited supervision -- may exacerbate existing societal risks and introduce new risks. Understanding and mitigating these risks involves critically evaluating existing governance structures, revising and adapting these structures where needed, and ensuring accountability of key stakeholders. Information about where, why, how, and by whom certain AI agents are used, which we refer to as \textbf{visibility}, is critical to these objectives. In this paper, we assess three categories of measures to increase visibility into AI agents: \textbf{agent identifiers}, \textbf{real-time monitoring}, and \textbf{activity logging}. For each, we outline potential implementations that vary in intrusiveness and informativeness. We analyze how the measures apply across a spectrum of centralized through decentralized deployment contexts, accounting for vario
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#20013;&#65292;LLMs&#38754;&#20020;&#30340;&#23433;&#20840;&#25361;&#25112;&#20197;&#21450;&#32531;&#35299;&#36825;&#20123;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#65292;LLMs&#23545;&#24694;&#24847;&#25552;&#31034;&#30340;&#21709;&#24212;&#23384;&#22312;&#24046;&#24322;&#65292;&#20302;&#36164;&#28304;&#35821;&#35328;&#19979;&#30340;&#21709;&#24212;&#26356;&#23481;&#26131;&#20135;&#29983;&#19981;&#23433;&#20840;&#12289;&#19981;&#30456;&#20851;&#30340;&#32467;&#26524;&#12290;&#23545;&#20110;&#36825;&#31181;&#24046;&#24322;&#30340;&#21407;&#22240;&#65292;&#26412;&#25991;&#36824;&#30740;&#31350;&#20102;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#26377;&#30417;&#30563;&#24494;&#35843;&#31561;&#26041;&#27861;&#23545;&#25968;&#25454;&#36827;&#34892;&#35843;&#33410;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.13136</link><description>&lt;p&gt;
&#35821;&#35328;&#38556;&#30861;&#65306;&#21078;&#26512;LLMs&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#20013;&#30340;&#23433;&#20840;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
The Language Barrier: Dissecting Safety Challenges of LLMs in Multilingual Contexts. (arXiv:2401.13136v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13136
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#20013;&#65292;LLMs&#38754;&#20020;&#30340;&#23433;&#20840;&#25361;&#25112;&#20197;&#21450;&#32531;&#35299;&#36825;&#20123;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#65292;LLMs&#23545;&#24694;&#24847;&#25552;&#31034;&#30340;&#21709;&#24212;&#23384;&#22312;&#24046;&#24322;&#65292;&#20302;&#36164;&#28304;&#35821;&#35328;&#19979;&#30340;&#21709;&#24212;&#26356;&#23481;&#26131;&#20135;&#29983;&#19981;&#23433;&#20840;&#12289;&#19981;&#30456;&#20851;&#30340;&#32467;&#26524;&#12290;&#23545;&#20110;&#36825;&#31181;&#24046;&#24322;&#30340;&#21407;&#22240;&#65292;&#26412;&#25991;&#36824;&#30740;&#31350;&#20102;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#26377;&#30417;&#30563;&#24494;&#35843;&#31561;&#26041;&#27861;&#23545;&#25968;&#25454;&#36827;&#34892;&#35843;&#33410;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#20840;&#29699;&#31038;&#21306;&#30340;&#24433;&#21709;&#19981;&#26029;&#25193;&#22823;&#65292;&#23427;&#20204;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#20013;&#30340;&#23433;&#20840;&#25361;&#25112;&#23545;&#40784;&#30740;&#31350;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#38754;&#20020;&#30340;&#23433;&#20840;&#25361;&#25112;&#30340;&#21464;&#21270;&#65292;&#24182;&#35752;&#35770;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#27604;&#36739;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;LLMs&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#23545;&#21516;&#19968;&#32452;&#24694;&#24847;&#25552;&#31034;&#30340;&#21709;&#24212;&#24773;&#20917;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65288;1&#65289;&#24403;&#24694;&#24847;&#25552;&#31034;&#29992;&#20302;&#36164;&#28304;&#35821;&#35328;&#32534;&#20889;&#26102;&#65292;LLMs&#24448;&#24448;&#26356;&#23481;&#26131;&#29983;&#25104;&#19981;&#23433;&#20840;&#30340;&#21709;&#24212;&#65292;&#65288;2&#65289;LLMs&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19979;&#26356;&#23481;&#26131;&#29983;&#25104;&#19982;&#24694;&#24847;&#25552;&#31034;&#26080;&#20851;&#30340;&#21709;&#24212;&#12290;&#20026;&#20102;&#29702;&#35299;&#36896;&#25104;&#36825;&#31181;&#24046;&#24322;&#30340;&#21407;&#22240;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#19982;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#25110;&#26377;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#36827;&#34892;&#25351;&#23548;&#35843;&#33410;&#23545;HH-RLHF&#25968;&#25454;&#38598;&#30340;&#24433;&#21709;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#34429;&#28982;&#20351;&#29992;&#39640;&#36164;&#28304;&#35821;&#35328;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#30340;&#23545;&#40784;&#24615;&#65292;&#20294;&#36890;&#36807;&#35757;&#32451;&#20173;&#20250;&#20135;&#29983;&#19981;&#19968;&#33268;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the influence of large language models (LLMs) spans across global communities, their safety challenges in multilingual settings become paramount for alignment research. This paper examines the variations in safety challenges faced by LLMs across different languages and discusses approaches to alleviating such concerns. By comparing how state-of-the-art LLMs respond to the same set of malicious prompts written in higher- vs. lower-resource languages, we observe that (1) LLMs tend to generate unsafe responses much more often when a malicious prompt is written in a lower-resource language, and (2) LLMs tend to generate more irrelevant responses to malicious prompts in lower-resource languages. To understand where the discrepancy can be attributed, we study the effect of instruction tuning with reinforcement learning from human feedback (RLHF) or supervised finetuning (SFT) on the HH-RLHF dataset. Surprisingly, while training with high-resource languages improves model alignment, traini
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26368;&#20248;&#20256;&#36755;&#36827;&#34892;&#20998;&#24067;&#24335;&#23545;&#25239;&#35299;&#37322;&#30340;&#26041;&#27861;DISCOUNT&#65292;&#23558;&#23545;&#25239;&#35299;&#37322;&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;&#25972;&#20010;&#36755;&#20837;&#36755;&#20986;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#32479;&#35745;&#32622;&#20449;&#24230;&#26469;&#25903;&#25745;&#36825;&#19968;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.13112</link><description>&lt;p&gt;
DISCOUNT: &#20351;&#29992;&#26368;&#20248;&#20256;&#36755;&#36827;&#34892;&#20998;&#24067;&#24335;&#23545;&#25239;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
DISCOUNT: Distributional Counterfactual Explanation With Optimal Transport. (arXiv:2401.13112v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26368;&#20248;&#20256;&#36755;&#36827;&#34892;&#20998;&#24067;&#24335;&#23545;&#25239;&#35299;&#37322;&#30340;&#26041;&#27861;DISCOUNT&#65292;&#23558;&#23545;&#25239;&#35299;&#37322;&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;&#25972;&#20010;&#36755;&#20837;&#36755;&#20986;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#32479;&#35745;&#32622;&#20449;&#24230;&#26469;&#25903;&#25745;&#36825;&#19968;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35299;&#37322;&#26159;&#22312;&#40657;&#30418;&#20915;&#31574;&#27169;&#22411;&#20013;&#25552;&#20379;&#27934;&#23519;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#20107;&#23454;&#26041;&#27861;&#65292;&#36890;&#36807;&#30830;&#23450;&#23548;&#33268;&#19981;&#21516;&#32467;&#26524;&#30340;&#26367;&#20195;&#36755;&#20837;&#23454;&#20363;&#26469;&#23454;&#29616;&#12290;&#26412;&#25991;&#23558;&#23545;&#25239;&#35299;&#37322;&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;&#20998;&#24067;&#19978;&#19979;&#25991;&#65292;&#20174;&#20010;&#20307;&#25968;&#25454;&#28857;&#25193;&#22823;&#21040;&#25972;&#20010;&#36755;&#20837;&#36755;&#20986;&#20998;&#24067;&#65292;&#21629;&#21517;&#20026;&#20998;&#24067;&#24335;&#23545;&#25239;&#35299;&#37322;&#12290;&#22312;&#20998;&#24067;&#24335;&#23545;&#25239;&#35299;&#37322;&#20013;&#65292;&#25105;&#20204;&#30340;&#37325;&#28857;&#36716;&#21521;&#20998;&#26512;&#20107;&#23454;&#21644;&#23545;&#25239;&#30340;&#20998;&#24067;&#23646;&#24615;&#65292;&#31867;&#20284;&#20110;&#35780;&#20272;&#20010;&#20307;&#23454;&#20363;&#21450;&#20854;&#32467;&#26524;&#20915;&#31574;&#30340;&#32463;&#20856;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#26368;&#20248;&#20256;&#36755;&#26469;&#26500;&#24314;&#19968;&#20010;&#26426;&#20250;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#23548;&#20986;&#19982;&#20107;&#23454;&#23545;&#24212;&#30340;&#23545;&#25239;&#20998;&#24067;&#65292;&#20197;&#32479;&#35745;&#32622;&#20449;&#24230;&#20570;&#25903;&#25745;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#20248;&#21270;&#26041;&#27861;DISCOUNT&#22312;&#36755;&#20837;&#21644;&#36755;&#20986;&#20998;&#24067;&#20043;&#38388;&#24179;&#34913;&#36825;&#31181;&#32622;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Explanations (CE) is the de facto method for providing insight and interpretability in black-box decision-making models by identifying alternative input instances that lead to different outcomes. This paper extends the concept of CEs to a distributional context, broadening the scope from individual data points to entire input and output distributions, named Distributional Counterfactual Explanation (DCE). In DCE, our focus shifts to analyzing the distributional properties of the factual and counterfactual, drawing parallels to the classical approach of assessing individual instances and their resulting decisions. We leverage Optimal Transport (OT) to frame a chance-constrained optimization problem, aiming to derive a counterfactual distribution that closely aligns with its factual counterpart, substantiated by statistical confidence. Our proposed optimization method, DISCOUNT, strategically balances this confidence across both input and output distributions. This algorit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;x-[plAIn]&#8221;&#30340;&#33258;&#23450;&#20041;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#35813;&#27169;&#22411;&#33021;&#22815;&#20351;&#24471;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26356;&#26131;&#20110;&#35775;&#38382;&#65292;&#24182;&#20026;&#19981;&#21516;&#29992;&#25143;&#32676;&#20307;&#29983;&#25104;&#28165;&#26224;&#12289;&#31616;&#26126;&#30340;&#35299;&#37322;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#36866;&#24212;&#24615;&#65292;&#33021;&#22815;&#26681;&#25454;&#29992;&#25143;&#30340;&#30693;&#35782;&#27700;&#24179;&#21644;&#20852;&#36259;&#26469;&#29983;&#25104;&#35299;&#37322;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#36866;&#24212;&#24615;&#25552;&#39640;&#20102;XAI&#30340;&#21487;&#35775;&#38382;&#24615;&#65292;&#24357;&#34917;&#20102;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2401.13110</link><description>&lt;p&gt;
XAI&#26222;&#21450;&#21270;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#31616;&#21270;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65311;
&lt;/p&gt;
&lt;p&gt;
XAI for All: Can Large Language Models Simplify Explainable AI?. (arXiv:2401.13110v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;x-[plAIn]&#8221;&#30340;&#33258;&#23450;&#20041;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#35813;&#27169;&#22411;&#33021;&#22815;&#20351;&#24471;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26356;&#26131;&#20110;&#35775;&#38382;&#65292;&#24182;&#20026;&#19981;&#21516;&#29992;&#25143;&#32676;&#20307;&#29983;&#25104;&#28165;&#26224;&#12289;&#31616;&#26126;&#30340;&#35299;&#37322;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#36866;&#24212;&#24615;&#65292;&#33021;&#22815;&#26681;&#25454;&#29992;&#25143;&#30340;&#30693;&#35782;&#27700;&#24179;&#21644;&#20852;&#36259;&#26469;&#29983;&#25104;&#35299;&#37322;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#36866;&#24212;&#24615;&#25552;&#39640;&#20102;XAI&#30340;&#21487;&#35775;&#38382;&#24615;&#65292;&#24357;&#34917;&#20102;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#36890;&#24120;&#20851;&#27880;&#20855;&#26377;&#36739;&#24378;&#25216;&#26415;&#32972;&#26223;&#30340;&#29992;&#25143;&#65292;&#36825;&#20351;&#24471;&#38750;&#19987;&#23478;&#38590;&#20197;&#29702;&#35299;XAI&#26041;&#27861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#8220;x-[plAIn]&#8221;&#65292;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;ChatGPT Builder&#24320;&#21457;&#30340;&#33258;&#23450;&#20041;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#20351;XAI&#23545;&#26356;&#24191;&#27867;&#30340;&#21463;&#20247;&#26356;&#26131;&#20110;&#35775;&#38382;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35774;&#35745;&#19968;&#20010;&#27169;&#22411;&#65292;&#21487;&#20197;&#38024;&#23545;&#19981;&#21516;&#30340;&#29992;&#25143;&#32676;&#20307;&#65292;&#21253;&#25324;&#21830;&#19994;&#19987;&#19994;&#20154;&#22763;&#21644;&#23398;&#26415;&#30028;&#20154;&#22763;&#65292;&#29983;&#25104;&#28165;&#26224;&#12289;&#31616;&#26126;&#30340;&#21508;&#31181;XAI&#26041;&#27861;&#30340;&#24635;&#32467;&#12290;&#25105;&#20204;&#27169;&#22411;&#30340;&#20851;&#38190;&#29305;&#28857;&#26159;&#20854;&#33021;&#22815;&#26681;&#25454;&#27599;&#20010;&#29992;&#25143;&#32676;&#20307;&#30340;&#30693;&#35782;&#27700;&#24179;&#21644;&#20852;&#36259;&#26469;&#36866;&#24212;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20173;&#28982;&#33021;&#22815;&#25552;&#20379;&#21450;&#26102;&#30340;&#35265;&#35299;&#65292;&#20026;&#26368;&#32456;&#29992;&#25143;&#30340;&#20915;&#31574;&#36807;&#31243;&#25552;&#20379;&#20415;&#21033;&#12290;&#25105;&#20204;&#30340;&#20351;&#29992;&#26696;&#20363;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25552;&#20379;&#26131;&#20110;&#29702;&#35299;&#30340;&#12289;&#38024;&#23545;&#29305;&#23450;&#21463;&#20247;&#30340;&#35299;&#37322;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#19981;&#35770;&#20351;&#29992;&#20309;&#31181;XAI&#26041;&#27861;&#65292;&#36825;&#31181;&#36866;&#24212;&#24615;&#25552;&#39640;&#20102;XAI&#30340;&#21487;&#35775;&#38382;&#24615;&#65292;&#24357;&#34917;&#20102;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of Explainable Artificial Intelligence (XAI) often focuses on users with a strong technical background, making it challenging for non-experts to understand XAI methods. This paper presents "x-[plAIn]", a new approach to make XAI more accessible to a wider audience through a custom Large Language Model (LLM), developed using ChatGPT Builder. Our goal was to design a model that can generate clear, concise summaries of various XAI methods, tailored for different audiences, including business professionals and academics. The key feature of our model is its ability to adapt explanations to match each audience group's knowledge level and interests. Our approach still offers timely insights, facilitating the decision-making process by the end users. Results from our use-case studies show that our model is effective in providing easy-to-understand, audience-specific explanations, regardless of the XAI method used. This adaptability improves the accessibility of XAI, bridging the gap 
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#31995;&#32479;&#21464;&#37327;&#21644;&#20989;&#25968;&#24211;&#19981;&#30830;&#23450;&#24615;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22686;&#24378;&#30340;SINDy&#31639;&#27861;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#35782;&#21035;&#31232;&#30095;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#12290;</title><link>http://arxiv.org/abs/2401.13099</link><description>&lt;p&gt;
&#22312;&#24211;&#21644;&#31995;&#32479;&#19981;&#30830;&#23450;&#24615;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#31232;&#30095;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Sparse identification of nonlinear dynamics in the presence of library and system uncertainty. (arXiv:2401.13099v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13099
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#31995;&#32479;&#21464;&#37327;&#21644;&#20989;&#25968;&#24211;&#19981;&#30830;&#23450;&#24615;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22686;&#24378;&#30340;SINDy&#31639;&#27861;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#35782;&#21035;&#31232;&#30095;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
SINDy&#31639;&#27861;&#24050;&#25104;&#21151;&#22320;&#29992;&#20110;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#35782;&#21035;&#21160;&#21147;&#31995;&#32479;&#30340;&#25511;&#21046;&#26041;&#31243;&#12290;&#28982;&#32780;&#65292;SINDy&#20551;&#35774;&#29992;&#25143;&#23545;&#31995;&#32479;&#20013;&#30340;&#21464;&#37327;&#21644;&#33021;&#22815;&#20316;&#20026;&#31995;&#32479;&#22522;&#30784;&#30340;&#20989;&#25968;&#24211;&#20855;&#26377;&#20808;&#39564;&#30693;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#28436;&#31034;&#20102;&#22686;&#24378;SINDy&#31639;&#27861;&#22312;&#31995;&#32479;&#21464;&#37327;&#19981;&#30830;&#23450;&#24615;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#20248;&#20110;SINDy&#30340;&#24615;&#33021;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#20004;&#31181;&#19981;&#30830;&#23450;&#24615;&#21516;&#26102;&#23384;&#22312;&#26102;&#65292;SINDy&#21487;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#20197;&#23454;&#29616;&#31283;&#20581;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The SINDy algorithm has been successfully used to identify the governing equations of dynamical systems from time series data. However, SINDy assumes the user has prior knowledge of the variables in the system and of a function library that can act as a basis for the system. In this paper, we demonstrate on real world data how the Augmented SINDy algorithm outperforms SINDy in the presence of system variable uncertainty. We then show SINDy can be further augmented to perform robustly when both kinds of uncertainty are present.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32771;&#34385;&#33322;&#36816;&#36890;&#37327;&#23494;&#24230;&#12289;&#28207;&#21475;&#36317;&#31163;&#12289;&#36152;&#26131;&#27969;&#37327;&#21644;&#20132;&#36890;&#26530;&#32445;&#30340;&#20013;&#24515;&#24615;&#25351;&#26631;&#31561;&#22240;&#32032;&#65292;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#27169;&#22411;&#26469;&#39044;&#27979;&#28023;&#20107;&#33322;&#36816;&#27969;&#37327;&#65292;&#24182;&#29992;&#20110;&#25351;&#23548;&#20840;&#29699;&#20132;&#36890;&#32593;&#32476;&#20013;&#20837;&#20405;&#29289;&#31181;&#30340;&#39118;&#38505;&#35780;&#20272;&#21644;&#31649;&#29702;&#12290;</title><link>http://arxiv.org/abs/2401.13098</link><description>&lt;p&gt;
&#36890;&#36807;&#37325;&#21147;&#20449;&#24687;&#39537;&#21160;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#39044;&#27979;&#38750;&#26412;&#22320;&#29289;&#31181;&#33337;&#33334;&#20132;&#36890;&#27969;&#37327;&#21644;&#20837;&#20405;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Gravity-Informed Deep Learning Framework for Predicting Ship Traffic Flow and Invasion Risk of Non-Indigenous Species via Ballast Water Discharge. (arXiv:2401.13098v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13098
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32771;&#34385;&#33322;&#36816;&#36890;&#37327;&#23494;&#24230;&#12289;&#28207;&#21475;&#36317;&#31163;&#12289;&#36152;&#26131;&#27969;&#37327;&#21644;&#20132;&#36890;&#26530;&#32445;&#30340;&#20013;&#24515;&#24615;&#25351;&#26631;&#31561;&#22240;&#32032;&#65292;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#27169;&#22411;&#26469;&#39044;&#27979;&#28023;&#20107;&#33322;&#36816;&#27969;&#37327;&#65292;&#24182;&#29992;&#20110;&#25351;&#23548;&#20840;&#29699;&#20132;&#36890;&#32593;&#32476;&#20013;&#20837;&#20405;&#29289;&#31181;&#30340;&#39118;&#38505;&#35780;&#20272;&#21644;&#31649;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#20307;&#20013;&#30340;&#20837;&#20405;&#29289;&#31181;&#23545;&#20840;&#29699;&#29615;&#22659;&#21644;&#29983;&#29289;&#22810;&#26679;&#24615;&#26500;&#25104;&#20102;&#37325;&#22823;&#23041;&#32961;&#12290;&#30001;&#20110;&#20132;&#36890;&#21644;&#36152;&#26131;&#22686;&#21152;&#65292;&#38750;&#26412;&#22303;&#29289;&#31181;&#24050;&#32463;&#24341;&#20837;&#20102;&#26032;&#30340;&#29615;&#22659;&#65292;&#23548;&#33268;&#29983;&#24577;&#31995;&#32479;&#30772;&#22351;&#65292;&#24182;&#23548;&#33268;&#20892;&#19994;&#12289;&#26519;&#19994;&#21644;&#28180;&#19994;&#26041;&#38754;&#30340;&#32463;&#27982;&#25439;&#22833;&#12290;&#22240;&#27492;&#65292;&#36843;&#20999;&#38656;&#35201;&#39118;&#38505;&#35780;&#20272;&#21644;&#31649;&#29702;&#25216;&#26415;&#20197;&#20943;&#36731;&#36825;&#20123;&#20837;&#20405;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#26032;&#30340;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#28023;&#20107;&#33322;&#36816;&#20132;&#36890;&#27969;&#37327;&#65292;&#24182;&#20197;&#27492;&#25351;&#23548;&#36890;&#36807;&#20840;&#29699;&#20132;&#36890;&#32593;&#32476;&#20256;&#25773;&#30340;&#20837;&#20405;&#29289;&#31181;&#39118;&#38505;&#35780;&#20272;&#12290;&#21463;&#22269;&#38469;&#36152;&#26131;&#37325;&#21147;&#27169;&#22411;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#32771;&#34385;&#20102;&#24433;&#21709;&#33337;&#33334;&#27963;&#21160;&#21487;&#33021;&#24615;&#21644;&#24433;&#21709;&#30340;&#21508;&#31181;&#22240;&#32032;&#65292;&#22914;&#33322;&#36816;&#36890;&#37327;&#23494;&#24230;&#12289;&#28207;&#21475;&#20043;&#38388;&#30340;&#36317;&#31163;&#12289;&#36152;&#26131;&#27969;&#37327;&#21644;&#20132;&#36890;&#26530;&#32445;&#30340;&#20013;&#24515;&#24615;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20998;&#26512;&#20837;&#20405;&#29289;&#31181;&#30340;&#39118;&#38505;&#32593;&#32476;&#65292;&#25105;&#20204;&#20026;&#35780;&#20272;&#21644;&#31649;&#29702;&#20837;&#20405;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Invasive species in water bodies pose a major threat to the environment and biodiversity globally. Due to increased transportation and trade, non-native species have been introduced to new environments, causing damage to ecosystems and leading to economic losses in agriculture, forestry, and fisheries. Therefore, there is a pressing need for risk assessment and management techniques to mitigate the impact of these invasions. This study aims to develop a new physics-inspired model to forecast maritime shipping traffic and thus inform risk assessment of invasive species spread through global transportation networks. Inspired by the gravity model for international trades, our model considers various factors that influence the likelihood and impact of vessel activities, such as shipping flux density, distance between ports, trade flow, and centrality measures of transportation hubs. Additionally, by analyzing the risk network of invasive species, we provide a comprehensive framework for as
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#31038;&#20250;&#32463;&#27982;&#20559;&#35265;&#23545;&#22330;&#26223;&#35782;&#21035;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20102;&#39044;&#35757;&#32451;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#20302;&#31038;&#20250;&#32463;&#27982;&#22320;&#20301;&#30340;&#23478;&#24237;&#29031;&#29255;&#20013;&#26174;&#31034;&#20986;&#26356;&#20302;&#30340;&#20998;&#31867;&#20934;&#30830;&#24230;&#21644;&#20998;&#31867;&#32622;&#20449;&#24230;&#65292;&#24182;&#26356;&#23481;&#26131;&#20998;&#37197;&#20855;&#26377;&#20882;&#29359;&#24615;&#30340;&#26631;&#31614;&#12290;</title><link>http://arxiv.org/abs/2401.13097</link><description>&lt;p&gt;
&#22330;&#26223;&#35782;&#21035;&#20013;&#30340;&#25968;&#23383;&#40511;&#27807;&#65306;&#25581;&#31034;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#31038;&#20250;&#32463;&#27982;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Digital Divides in Scene Recognition: Uncovering Socioeconomic Biases in Deep Learning Systems. (arXiv:2401.13097v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13097
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#31038;&#20250;&#32463;&#27982;&#20559;&#35265;&#23545;&#22330;&#26223;&#35782;&#21035;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20102;&#39044;&#35757;&#32451;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#20302;&#31038;&#20250;&#32463;&#27982;&#22320;&#20301;&#30340;&#23478;&#24237;&#29031;&#29255;&#20013;&#26174;&#31034;&#20986;&#26356;&#20302;&#30340;&#20998;&#31867;&#20934;&#30830;&#24230;&#21644;&#20998;&#31867;&#32622;&#20449;&#24230;&#65292;&#24182;&#26356;&#23481;&#26131;&#20998;&#37197;&#20855;&#26377;&#20882;&#29359;&#24615;&#30340;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35745;&#31639;&#26426;&#30340;&#22330;&#26223;&#29702;&#35299;&#24433;&#21709;&#20102;&#20174;&#22478;&#24066;&#35268;&#21010;&#21040;&#33258;&#21160;&#39550;&#39542;&#30340;&#39046;&#22495;&#65292;&#28982;&#32780;&#25105;&#20204;&#23545;&#36825;&#20123;&#25216;&#26415;&#22312;&#31038;&#20250;&#24046;&#24322;&#20013;&#30340;&#34920;&#29616;&#20102;&#35299;&#29978;&#23569;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;dCNNs&#65289;&#22312;&#22330;&#26223;&#20998;&#31867;&#20013;&#30340;&#20559;&#35265;&#65292;&#20351;&#29992;&#20102;&#26469;&#33258;&#20840;&#29699;&#21644;&#32654;&#22269;&#30340;&#36817;&#30334;&#19975;&#24352;&#22270;&#29255;&#65292;&#21253;&#25324;&#29992;&#25143;&#25552;&#20132;&#30340;&#23478;&#24237;&#29031;&#29255;&#21644;Airbnb&#30340;&#25151;&#28304;&#29031;&#29255;&#12290;&#25105;&#20204;&#36816;&#29992;&#20102;&#32479;&#35745;&#27169;&#22411;&#65292;&#23545;&#23478;&#24237;&#25910;&#20837;&#12289;&#20154;&#31867;&#21457;&#23637;&#25351;&#25968;&#65288;HDI&#65289;&#31561;&#31038;&#20250;&#32463;&#27982;&#25351;&#26631;&#20197;&#21450;&#20844;&#24320;&#25968;&#25454;&#26469;&#28304;&#65288;CIA&#21644;&#32654;&#22269;&#20154;&#21475;&#26222;&#26597;&#65289;&#30340;&#20154;&#21475;&#32479;&#35745;&#22240;&#32032;&#23545;dCNNs&#30340;&#34920;&#29616;&#24433;&#21709;&#36827;&#34892;&#20102;&#37327;&#21270;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21457;&#29616;&#20102;&#26174;&#33879;&#30340;&#31038;&#20250;&#32463;&#27982;&#20559;&#35265;&#65292;&#39044;&#35757;&#32451;&#30340;dCNNs&#34920;&#29616;&#20986;&#26356;&#20302;&#30340;&#20998;&#31867;&#20934;&#30830;&#24230;&#12289;&#26356;&#20302;&#30340;&#20998;&#31867;&#32622;&#20449;&#24230;&#65292;&#20197;&#21450;&#26356;&#39640;&#30340;&#20542;&#21521;&#24615;&#22312;&#20302;&#31038;&#20250;&#32463;&#27982;&#22320;&#20301;&#30340;&#23478;&#24237;&#65288;&#20363;&#22914;&#8220;&#24223;&#22687;&#8221;&#65292;&#8220;&#36139;&#27665;&#31391;&#8221;&#65289;&#30340;&#22270;&#29255;&#20013;&#20998;&#37197;&#20855;&#26377;&#20882;&#29359;&#24615;&#30340;&#26631;&#31614;&#12290;&#36825;&#31181;&#36235;&#21183;&#26159;&#25345;&#32493;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computer-based scene understanding has influenced fields ranging from urban planning to autonomous vehicle performance, yet little is known about how well these technologies work across social differences. We investigate the biases of deep convolutional neural networks (dCNNs) in scene classification, using nearly one million images from global and US sources, including user-submitted home photographs and Airbnb listings. We applied statistical models to quantify the impact of socioeconomic indicators such as family income, Human Development Index (HDI), and demographic factors from public data sources (CIA and US Census) on dCNN performance. Our analyses revealed significant socioeconomic bias, where pretrained dCNNs demonstrated lower classification accuracy, lower classification confidence, and a higher tendency to assign labels that could be offensive when applied to homes (e.g., "ruin", "slum"), especially in images from homes with lower socioeconomic status (SES). This trend is c
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#20449;&#24687;&#36136;&#37327;&#38382;&#39064;&#65292;&#21457;&#29616;&#26631;&#35760;&#21270;&#19981;&#21487;&#38752;&#12289;&#20559;&#35265;&#20197;&#21450;&#20449;&#24687;&#36136;&#37327;&#19979;&#38477;&#21487;&#33021;&#23548;&#33268;&#24187;&#35273;&#12289;&#25423;&#36896;&#20449;&#24687;&#65292;&#20174;&#32780;&#23545;&#20225;&#19994;&#20915;&#31574;&#20135;&#29983;&#38169;&#35823;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.13086</link><description>&lt;p&gt;
&#21521;&#21487;&#20449;&#36182;&#30340;&#35821;&#35328;&#27169;&#22411;&#36808;&#36827;&#65306;&#25506;&#31350;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#20449;&#24687;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Towards Trustable Language Models: Investigating Information Quality of Large Language Models. (arXiv:2401.13086v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13086
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#20449;&#24687;&#36136;&#37327;&#38382;&#39064;&#65292;&#21457;&#29616;&#26631;&#35760;&#21270;&#19981;&#21487;&#38752;&#12289;&#20559;&#35265;&#20197;&#21450;&#20449;&#24687;&#36136;&#37327;&#19979;&#38477;&#21487;&#33021;&#23548;&#33268;&#24187;&#35273;&#12289;&#25423;&#36896;&#20449;&#24687;&#65292;&#20174;&#32780;&#23545;&#20225;&#19994;&#20915;&#31574;&#20135;&#29983;&#38169;&#35823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#27491;&#22312;&#36805;&#36895;&#29983;&#25104;&#22823;&#37327;&#20449;&#24687;&#65292;&#29992;&#25143;&#36234;&#26469;&#36234;&#20381;&#36182;&#21644;&#20449;&#20219;&#36825;&#20123;&#25968;&#25454;&#12290;&#23613;&#31649;LLM&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#20449;&#24687;&#36136;&#37327;&#30340;&#25361;&#25112;&#65292;LLM&#29983;&#25104;&#30340;&#20449;&#24687;&#24182;&#19981;&#23436;&#20840;&#21487;&#20449;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LLM&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#26631;&#35760;&#21270;&#19981;&#21487;&#38752;&#12289;&#23384;&#22312;&#20559;&#35265;&#65292;&#23548;&#33268;&#20449;&#24687;&#36136;&#37327;&#30340;&#23436;&#25972;&#24615;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#20449;&#24687;&#36136;&#37327;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;LLM&#21487;&#33021;&#20250;&#20135;&#29983;&#24187;&#35273;&#12289;&#25423;&#36896;&#20449;&#24687;&#12290;&#19981;&#21487;&#38752;&#30340;&#20449;&#24687;&#21487;&#33021;&#23548;&#33268;&#20225;&#19994;&#20570;&#20986;&#38169;&#35823;&#20915;&#31574;&#65292;&#24433;&#21709;&#32463;&#27982;&#27963;&#21160;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LLM&#30340;&#26032;&#39062;&#25968;&#23398;&#20449;&#24687;&#36136;&#37327;&#35780;&#20272;&#26041;&#27861;&#65292;&#36827;&#19968;&#27493;&#20998;&#26512;&#21644;&#31361;&#20986;&#20102;&#20449;&#24687;&#36136;&#37327;&#25361;&#25112;&#65292;&#20197;&#31995;&#32479;&#22320;&#25193;&#23637;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLM) are generating information at a rapid pace, requiring users to increasingly rely and trust the data. Despite remarkable advances of LLM, Information generated by LLM is not completely trustworthy, due to challenges in information quality. Specifically, integrity of Information quality decreases due to unreliable, biased, tokenization during pre-training of LLM. Moreover, due to decreased information quality issues, has led towards hallucination, fabricated information. Unreliable information can lead towards flawed decisions in businesses, which impacts economic activity. In this work, we introduce novel mathematical information quality evaluation of LLM, we furthermore analyze and highlight information quality challenges, scaling laws to systematically scale language models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#20302;&#36164;&#28304;&#21360;&#24230;&#35821;&#35328;&#30340;&#25991;&#26412;&#22686;&#24378;&#26041;&#27861;&#65292;&#21253;&#25324;Easy Data Augmentation&#12289;Back Translation&#12289;Paraphrasing&#12289;&#20351;&#29992;LLMs&#36827;&#34892;&#25991;&#26412;&#29983;&#25104;&#20197;&#21450;&#20351;&#29992;LLMs&#36827;&#34892;&#25991;&#26412;&#25193;&#23637;&#31561;&#25216;&#26415;&#12290;&#36890;&#36807;&#20108;&#20803;&#21644;&#22810;&#31867;&#25991;&#26412;&#20998;&#31867;&#23454;&#39564;&#65292;&#30740;&#31350;&#21457;&#29616;&#22522;&#26412;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.13085</link><description>&lt;p&gt;
IndiText Boost: &#20302;&#36164;&#28304;&#26465;&#20214;&#19979;&#21360;&#24230;&#35821;&#35328;&#30340;&#25991;&#26412;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
IndiText Boost: Text Augmentation for Low Resource India Languages. (arXiv:2401.13085v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#20302;&#36164;&#28304;&#21360;&#24230;&#35821;&#35328;&#30340;&#25991;&#26412;&#22686;&#24378;&#26041;&#27861;&#65292;&#21253;&#25324;Easy Data Augmentation&#12289;Back Translation&#12289;Paraphrasing&#12289;&#20351;&#29992;LLMs&#36827;&#34892;&#25991;&#26412;&#29983;&#25104;&#20197;&#21450;&#20351;&#29992;LLMs&#36827;&#34892;&#25991;&#26412;&#25193;&#23637;&#31561;&#25216;&#26415;&#12290;&#36890;&#36807;&#20108;&#20803;&#21644;&#22810;&#31867;&#25991;&#26412;&#20998;&#31867;&#23454;&#39564;&#65292;&#30740;&#31350;&#21457;&#29616;&#22522;&#26412;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#22686;&#24378;&#26159;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#23427;&#26377;&#21161;&#20110;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#26469;&#24212;&#23545;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#22810;&#24180;&#26469;&#65292;&#24050;&#32463;&#23545;&#33521;&#35821;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#25968;&#25454;&#22686;&#24378;&#24037;&#20316;&#65292;&#32780;&#22312;&#21360;&#24230;&#35821;&#35328;&#26041;&#38754;&#21364;&#20570;&#24471;&#24456;&#23569;&#12290;&#36825;&#19982;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#26469;&#22788;&#29702;&#25968;&#25454;&#31232;&#32570;&#30340;&#20107;&#23454;&#30456;&#21453;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37325;&#28857;&#23454;&#26045;Easy Data Augmentation&#12289;Back Translation&#12289;Paraphrasing&#12289;&#20351;&#29992;LLMs&#36827;&#34892;&#25991;&#26412;&#29983;&#25104;&#20197;&#21450;&#20351;&#29992;LLMs&#36827;&#34892;&#25991;&#26412;&#25193;&#23637;&#31561;&#25216;&#26415;&#65292;&#29992;&#20110;&#19981;&#21516;&#35821;&#35328;&#30340;&#25991;&#26412;&#20998;&#31867;&#12290;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;6&#31181;&#21360;&#24230;&#35821;&#35328;&#65306;&#20449;&#24503;&#35821;&#12289;&#39532;&#25289;&#22320;&#35821;&#12289;&#21360;&#22320;&#35821;&#12289;&#21476;&#21513;&#25289;&#29305;&#35821;&#12289;&#27888;&#21346;&#22266;&#35821;&#21644;&#26805;&#35821;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#30446;&#21069;&#27809;&#26377;&#31867;&#20284;&#30340;&#24037;&#20316;&#29992;&#20110;&#21360;&#24230;&#35821;&#35328;&#30340;&#25991;&#26412;&#22686;&#24378;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20108;&#20803;&#21644;&#22810;&#31867;&#25991;&#26412;&#20998;&#31867;&#65292;&#20197;&#20351;&#25105;&#20204;&#30340;&#32467;&#26524;&#26356;&#20855;&#21487;&#27604;&#24615;&#12290;&#25105;&#20204;&#24471;&#21040;&#20102;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#26524;&#65292;&#22522;&#26412;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21487;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text Augmentation is an important task for low-resource languages. It helps deal with the problem of data scarcity. A data augmentation strategy is used to deal with the problem of data scarcity. Through the years, much work has been done on data augmentation for the English language. In contrast, very less work has been done on Indian languages. This is contrary to the fact that data augmentation is used to deal with data scarcity. In this work, we focus on implementing techniques like Easy Data Augmentation, Back Translation, Paraphrasing, Text Generation using LLMs, and Text Expansion using LLMs for text classification on different languages. We focus on 6 Indian languages namely: Sindhi, Marathi, Hindi, Gujarati, Telugu, and Sanskrit. According to our knowledge, no such work exists for text augmentation on Indian languages. We carry out binary as well as multi-class text classification to make our results more comparable. We get surprising results as basic data augmentation techniq
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#35270;&#35273;&#38382;&#31572;&#20013;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#21019;&#26032;&#24615;&#22320;&#22686;&#24378;&#20102;&#25968;&#25454;&#38598;&#65292;&#24182;&#23454;&#29616;&#20102;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.13081</link><description>&lt;p&gt;
&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#33258;&#30001;&#24418;&#24335;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Free Form Medical Visual Question Answering in Radiology. (arXiv:2401.13081v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13081
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#35270;&#35273;&#38382;&#31572;&#20013;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#21019;&#26032;&#24615;&#22320;&#22686;&#24378;&#20102;&#25968;&#25454;&#38598;&#65292;&#24182;&#23454;&#29616;&#20102;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#39046;&#22495;&#65292;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#38754;&#20020;&#30528;&#19968;&#20010;&#29420;&#29305;&#30340;&#12289;&#36328;&#23398;&#31185;&#30340;&#25361;&#25112;&#65292;&#32467;&#21512;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#30693;&#35782;&#34920;&#31034;&#31561;&#39046;&#22495;&#12290;&#23613;&#31649;&#20854;&#37325;&#35201;&#24615;&#65292;&#21307;&#23398;VQA&#30340;&#30740;&#31350;&#19968;&#30452;&#24456;&#23569;&#65292;&#30452;&#21040;2018&#24180;&#25165;&#24320;&#22987;&#34028;&#21187;&#21457;&#23637;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#65292;&#28145;&#20837;&#25506;&#35752;&#20102;&#25918;&#23556;&#23398;&#22270;&#20687;&#30340;&#26377;&#25928;&#34920;&#31034;&#20197;&#21450;&#22810;&#27169;&#24577;&#34920;&#31034;&#30340;&#32852;&#21512;&#23398;&#20064;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21019;&#26032;&#22320;&#22686;&#24378;&#20102;SLAKE&#25968;&#25454;&#38598;&#65292;&#20351;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#22238;&#31572;&#26356;&#22810;&#26679;&#21270;&#30340;&#38382;&#39064;&#65292;&#19981;&#20165;&#38480;&#20110;&#25918;&#23556;&#23398;&#25110;&#30149;&#29702;&#23398;&#22270;&#20687;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#26356;&#31616;&#21333;&#30340;&#26550;&#26500;&#19979;&#23454;&#29616;&#20102;79.55&#65285;&#30340;top-1&#20934;&#30830;&#29575;&#65292;&#23637;&#31034;&#20102;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#36825;&#39033;&#30740;&#31350;&#19981;&#20165;&#25512;&#21160;&#20102;&#21307;&#23398;VQA&#30340;&#21457;&#23637;&#65292;&#20063;&#20026;&#35786;&#26029;&#35774;&#32622;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#24320;&#36767;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual Question Answering (VQA) in the medical domain presents a unique, interdisciplinary challenge, combining fields such as Computer Vision, Natural Language Processing, and Knowledge Representation. Despite its importance, research in medical VQA has been scant, only gaining momentum since 2018. Addressing this gap, our research delves into the effective representation of radiology images and the joint learning of multimodal representations, surpassing existing methods. We innovatively augment the SLAKE dataset, enabling our model to respond to a more diverse array of questions, not limited to the immediate content of radiology or pathology images. Our model achieves a top-1 accuracy of 79.55\% with a less complex architecture, demonstrating comparable performance to current state-of-the-art models. This research not only advances medical VQA but also opens avenues for practical applications in diagnostic settings.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22270;&#20687;&#20998;&#21106;&#21644;&#36845;&#20195;&#32972;&#26223;&#20272;&#35745;&#31639;&#27861;&#25913;&#36827;&#20102;&#39640;&#20809;&#35889;&#22270;&#20687;&#20013;&#27668;&#28342;&#33014;&#35782;&#21035;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.13068</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#20687;&#20998;&#21106;&#30340;&#26412;&#22320;&#32972;&#26223;&#20272;&#35745;&#65292;&#25552;&#39640;&#39640;&#20809;&#35889;&#22270;&#20687;&#20013;&#27668;&#28342;&#33014;&#35782;&#21035;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Local Background Estimation for Improved Gas Plume Identification in Hyperspectral Images. (arXiv:2401.13068v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13068
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22270;&#20687;&#20998;&#21106;&#21644;&#36845;&#20195;&#32972;&#26223;&#20272;&#35745;&#31639;&#27861;&#25913;&#36827;&#20102;&#39640;&#20809;&#35889;&#22270;&#20687;&#20013;&#27668;&#28342;&#33014;&#35782;&#21035;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#37492;&#21035;&#27169;&#22411;&#22312;&#22478;&#24066;&#22330;&#26223;&#30340;&#38271;&#27874;&#32418;&#22806;&#39640;&#20809;&#35889;&#22270;&#20687;&#20013;&#35782;&#21035;&#27668;&#28342;&#33014;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#32771;&#34385;&#20102;&#22823;&#37327;&#27668;&#20307;&#30340;&#24773;&#20917;&#19979;&#12290;&#30001;&#20110;&#35768;&#22810;&#27668;&#20307;&#20855;&#26377;&#30456;&#20284;&#30340;&#20809;&#35889;&#29305;&#24449;&#65292;&#27491;&#30830;&#20272;&#35745;&#26816;&#27979;&#21040;&#30340;&#27668;&#28342;&#33014;&#30340;&#20449;&#21495;&#21313;&#20998;&#37325;&#35201;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#20250;&#20272;&#35745;&#22330;&#26223;&#30340;&#20840;&#23616;&#22343;&#20540;&#20809;&#35889;&#21644;&#21327;&#26041;&#24046;&#30697;&#38453;&#65292;&#20197;&#23545;&#27668;&#28342;&#33014;&#30340;&#20449;&#21495;&#36827;&#34892;&#30333;&#21270;&#22788;&#29702;&#65292;&#20174;&#27668;&#28342;&#33014;&#30340;&#20809;&#35889;&#20013;&#21435;&#38500;&#32972;&#26223;&#30340;&#31614;&#21517;&#12290;&#28982;&#32780;&#65292;&#22478;&#24066;&#22330;&#26223;&#20013;&#21487;&#33021;&#23384;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#32972;&#26223;&#26448;&#26009;&#65292;&#36825;&#20123;&#26448;&#26009;&#22312;&#31354;&#38388;&#21644;&#20809;&#35889;&#19978;&#20855;&#26377;&#24322;&#36136;&#24615;&#12290;&#22914;&#26524;&#20840;&#23616;&#32972;&#26223;&#20272;&#35745;&#19981;&#33021;&#20195;&#34920;&#32473;&#23450;&#30340;&#26412;&#22320;&#32972;&#26223;&#26448;&#26009;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#35782;&#21035;&#24615;&#33021;&#36739;&#24046;&#12290;&#25105;&#20204;&#20351;&#29992;&#22270;&#20687;&#20998;&#21106;&#21644;&#36845;&#20195;&#32972;&#26223;&#20272;&#35745;&#31639;&#27861;&#65292;&#20026;&#27668;&#28342;&#33014;&#19979;&#26041;&#30340;&#21508;&#31181;&#32972;&#26223;&#26448;&#26009;&#21019;&#24314;&#26412;&#22320;&#20272;&#35745;&#20540;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27668;&#28342;&#33014;&#35782;&#21035;&#26041;&#38754;&#20248;&#20110;&#20840;&#23616;&#32972;&#26223;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning identification models have shown promise for identifying gas plumes in Longwave IR hyperspectral images of urban scenes, particularly when a large library of gases are being considered. Because many gases have similar spectral signatures, it is important to properly estimate the signal from a detected plume. Typically, a scene's global mean spectrum and covariance matrix are estimated to whiten the plume's signal, which removes the background's signature from the gas signature. However, urban scenes can have many different background materials that are spatially and spectrally heterogeneous. This can lead to poor identification performance when the global background estimate is not representative of a given local background material. We use image segmentation, along with an iterative background estimation algorithm, to create local estimates for the various background materials that reside underneath a gas plume. Our method outperforms global background estimation on a se
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;&#12298;&#21476;&#20848;&#32463;&#38382;&#31572;2023&#12299;&#20849;&#20139;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#25237;&#31080;&#38598;&#25104;&#26469;&#25552;&#39640;&#39044;&#27979;&#31283;&#23450;&#24615;&#65292;&#20351;&#29992;&#19981;&#21516;&#30340;Transformer&#27169;&#22411;&#21644;&#38408;&#20540;&#26426;&#21046;&#26469;&#35299;&#20915;&#20302;&#36164;&#28304;&#35757;&#32451;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#31995;&#32479;&#22312;&#38544;&#34255;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.13060</link><description>&lt;p&gt;
TCE&#22312;&#12298;&#21476;&#20848;&#32463;&#38382;&#31572;2023&#12299;&#20849;&#20139;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;: &#20302;&#36164;&#28304;&#22686;&#24378;Transformer-&#22522;&#20110;&#38598;&#25104;&#26041;&#27861;&#30340;&#21476;&#20848;&#32463;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
TCE at Qur'an QA 2023 Shared Task: Low Resource Enhanced Transformer-based Ensemble Approach for Qur'anic QA. (arXiv:2401.13060v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13060
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;&#12298;&#21476;&#20848;&#32463;&#38382;&#31572;2023&#12299;&#20849;&#20139;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#25237;&#31080;&#38598;&#25104;&#26469;&#25552;&#39640;&#39044;&#27979;&#31283;&#23450;&#24615;&#65292;&#20351;&#29992;&#19981;&#21516;&#30340;Transformer&#27169;&#22411;&#21644;&#38408;&#20540;&#26426;&#21046;&#26469;&#35299;&#20915;&#20302;&#36164;&#28304;&#35757;&#32451;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#31995;&#32479;&#22312;&#38544;&#34255;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;&#12298;&#21476;&#20848;&#32463;&#38382;&#31572;2023&#12299;&#20849;&#20139;&#20219;&#21153;A&#21644;B&#20013;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#35757;&#32451;&#25968;&#25454;&#36164;&#28304;&#31232;&#32570;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#20381;&#38752;&#36801;&#31227;&#23398;&#20064;&#21644;&#25237;&#31080;&#38598;&#25104;&#26469;&#25552;&#39640;&#22312;&#22810;&#27425;&#36816;&#34892;&#20013;&#30340;&#39044;&#27979;&#31283;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#19981;&#21516;&#30340;&#20307;&#31995;&#32467;&#26500;&#21644;&#23398;&#20064;&#26426;&#21046;&#65292;&#38024;&#23545;&#20004;&#20010;&#20219;&#21153;&#20351;&#29992;&#19968;&#31995;&#21015;&#30340;&#38463;&#25289;&#20271;&#35821;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#12290;&#20026;&#20102;&#35782;&#21035;&#26080;&#27861;&#22238;&#31572;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#19968;&#20010;&#38408;&#20540;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#31995;&#32479;&#22312;&#38544;&#34255;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#22823;&#22823;&#36229;&#36807;&#22522;&#20934;&#24615;&#33021;&#65292;&#20219;&#21153;A&#30340;MAP&#24471;&#20998;&#20026;25.05%&#65292;&#20219;&#21153;B&#30340;&#23616;&#37096;&#24179;&#22343;&#20934;&#30830;&#29575;(pAP)&#20026;57.11%&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present our approach to tackle Qur'an QA 2023 shared tasks A and B. To address the challenge of low-resourced training data, we rely on transfer learning together with a voting ensemble to improve prediction stability across multiple runs. Additionally, we employ different architectures and learning mechanisms for a range of Arabic pre-trained transformer-based models for both tasks. To identify unanswerable questions, we propose using a thresholding mechanism. Our top-performing systems greatly surpass the baseline performance on the hidden split, achieving a MAP score of 25.05% for task A and a partial Average Precision (pAP) of 57.11% for task B.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CIS-UNet&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20027;&#21160;&#33033;&#21644;&#20027;&#21160;&#33033;&#20998;&#25903;&#30340;&#22810;&#31867;&#20998;&#21106;&#12290;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;CNN&#21644;Swin transformers&#30340;&#20248;&#21183;&#65292;&#37319;&#29992;&#20102;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#24179;&#31227;&#31383;&#21475;&#33258;&#27880;&#24847;&#21147;&#65292;&#33021;&#22815;&#20934;&#30830;&#22320;&#35782;&#21035;&#20027;&#21160;&#33033;&#30340;&#21508;&#20010;&#20998;&#25903;&#12290;</title><link>http://arxiv.org/abs/2401.13049</link><description>&lt;p&gt;
CIS-UNet: &#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#24179;&#31227;&#31383;&#21475;&#33258;&#27880;&#24847;&#21147;&#30340;CTA&#20027;&#21160;&#33033;&#22810;&#31867;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
CIS-UNet: Multi-Class Segmentation of the Aorta in Computed Tomography Angiography via Context-Aware Shifted Window Self-Attention. (arXiv:2401.13049v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13049
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CIS-UNet&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20027;&#21160;&#33033;&#21644;&#20027;&#21160;&#33033;&#20998;&#25903;&#30340;&#22810;&#31867;&#20998;&#21106;&#12290;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;CNN&#21644;Swin transformers&#30340;&#20248;&#21183;&#65292;&#37319;&#29992;&#20102;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#24179;&#31227;&#31383;&#21475;&#33258;&#27880;&#24847;&#21147;&#65292;&#33021;&#22815;&#20934;&#30830;&#22320;&#35782;&#21035;&#20027;&#21160;&#33033;&#30340;&#21508;&#20010;&#20998;&#25903;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#25104;&#20687;&#21644;&#20869;&#34924;&#34880;&#31649;&#33180;&#25216;&#26415;&#30340;&#36827;&#27493;&#20419;&#36827;&#20102;&#20027;&#21160;&#33033;&#30142;&#30149;&#30340;&#24494;&#21019;&#27835;&#30103;&#12290;&#20934;&#30830;&#22320;&#23545;&#20027;&#21160;&#33033;&#21450;&#20854;&#20998;&#25903;&#36827;&#34892;3D&#20998;&#21106;&#23545;&#20110;&#24178;&#39044;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#19981;&#20934;&#30830;&#30340;&#20998;&#21106;&#21487;&#33021;&#23548;&#33268;&#38169;&#35823;&#30340;&#25163;&#26415;&#35268;&#21010;&#21644;&#20869;&#37096;&#25903;&#26550;&#26500;&#36896;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#23558;&#20027;&#21160;&#33033;&#20998;&#21106;&#31616;&#21270;&#20026;&#20108;&#20540;&#22270;&#20687;&#20998;&#21106;&#38382;&#39064;&#65292;&#24573;&#35270;&#20102;&#21306;&#20998;&#21508;&#20010;&#20027;&#21160;&#33033;&#20998;&#25903;&#30340;&#24517;&#35201;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;Context Infused Swin-UNet&#65288;CIS-UNet&#65289;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20027;&#21160;&#33033;&#21644;&#21313;&#19977;&#20010;&#20027;&#21160;&#33033;&#20998;&#25903;&#30340;&#22810;&#31867;&#20998;&#21106;&#12290;CIS-UNet&#32467;&#21512;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#21644;Swin transformers&#30340;&#20248;&#21183;&#65292;&#37319;&#29992;&#20102;&#23618;&#27425;&#21270;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#65292;&#21253;&#25324;CNN&#32534;&#30721;&#22120;&#12289;&#23545;&#31216;&#35299;&#30721;&#22120;&#12289;&#36339;&#36291;&#36830;&#25509;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#24179;&#31227;&#31383;&#21475;&#33258;&#27880;&#24847;&#21147;&#65288;CSW-SA&#65289;&#20316;&#20026;&#29942;&#39048;&#27169;&#22359;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;CSW-SA&#24341;&#20837;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#26041;&#27861;&#26469;&#21033;&#29992;&#22270;&#20687;&#20013;&#30340;&#34917;&#19969;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advancements in medical imaging and endovascular grafting have facilitated minimally invasive treatments for aortic diseases. Accurate 3D segmentation of the aorta and its branches is crucial for interventions, as inaccurate segmentation can lead to erroneous surgical planning and endograft construction. Previous methods simplified aortic segmentation as a binary image segmentation problem, overlooking the necessity of distinguishing between individual aortic branches. In this paper, we introduce Context Infused Swin-UNet (CIS-UNet), a deep learning model designed for multi-class segmentation of the aorta and thirteen aortic branches. Combining the strengths of Convolutional Neural Networks (CNNs) and Swin transformers, CIS-UNet adopts a hierarchical encoder-decoder structure comprising a CNN encoder, symmetric decoder, skip connections, and a novel Context-aware Shifted Window Self-Attention (CSW-SA) as the bottleneck block. Notably, CSW-SA introduces a unique utilization of the patch
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#25935;&#24863;&#31232;&#30095;&#32534;&#30721;&#30340;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#38543;&#26426;&#29305;&#24449;&#23454;&#29616;&#23545;&#22797;&#26434;&#29615;&#22659;&#30340;&#25311;&#21512;&#12290;&#36825;&#31181;&#27169;&#22411;&#33021;&#22815;&#39640;&#25928;&#22320;&#36827;&#34892;&#31232;&#30095;&#26356;&#26032;&#65292;&#23454;&#29616;&#20102;&#20248;&#21270;&#25311;&#21512;&#20808;&#21069;&#32463;&#39564;&#30340;Follow-The-Leader&#65288;FTL&#65289;&#19990;&#30028;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.13034</link><description>&lt;p&gt;
&#22312;&#32447;&#23398;&#20064;&#19990;&#30028;&#27169;&#22411;&#30340;&#23616;&#37096;&#25935;&#24863;&#31232;&#30095;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Locality Sensitive Sparse Encoding for Learning World Models Online. (arXiv:2401.13034v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#25935;&#24863;&#31232;&#30095;&#32534;&#30721;&#30340;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#38543;&#26426;&#29305;&#24449;&#23454;&#29616;&#23545;&#22797;&#26434;&#29615;&#22659;&#30340;&#25311;&#21512;&#12290;&#36825;&#31181;&#27169;&#22411;&#33021;&#22815;&#39640;&#25928;&#22320;&#36827;&#34892;&#31232;&#30095;&#26356;&#26032;&#65292;&#23454;&#29616;&#20102;&#20248;&#21270;&#25311;&#21512;&#20808;&#21069;&#32463;&#39564;&#30340;Follow-The-Leader&#65288;FTL&#65289;&#19990;&#30028;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#22312;&#22312;&#32447;&#23398;&#20064;&#20013;&#36935;&#21040;&#30340;&#25968;&#25454;&#38750;&#24179;&#31283;&#24615;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#25935;&#24863;&#31232;&#30095;&#32534;&#30721;&#30340;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#38750;&#32447;&#24615;&#38543;&#26426;&#29305;&#24449;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#29615;&#22659;&#30340;&#25311;&#21512;&#12290;&#36890;&#36807;&#24341;&#20837;&#23616;&#37096;&#25935;&#24863;&#31232;&#30095;&#32534;&#30721;&#65292;&#25105;&#20204;&#33021;&#22815;&#36827;&#34892;&#39640;&#25928;&#30340;&#31232;&#30095;&#26356;&#26032;&#65292;&#22312;&#24179;&#34913;&#27169;&#22411;&#23481;&#37327;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#21516;&#26102;&#23454;&#29616;&#20248;&#21270;&#25311;&#21512;&#25152;&#26377;&#20808;&#21069;&#32463;&#39564;&#30340;Follow-The-Leader&#65288;FTL&#65289;&#19990;&#30028;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Acquiring an accurate world model online for model-based reinforcement learning (MBRL) is challenging due to data nonstationarity, which typically causes catastrophic forgetting for neural networks (NNs). From the online learning perspective, a Follow-The-Leader (FTL) world model is desirable, which optimally fits all previous experiences at each round. Unfortunately, NN-based models need re-training on all accumulated data at every interaction step to achieve FTL, which is computationally expensive for lifelong agents. In this paper, we revisit models that can achieve FTL with incremental updates. Specifically, our world model is a linear regression model supported by nonlinear random features. The linear part ensures efficient FTL update while the nonlinear random feature empowers the fitting of complex environments. To best trade off model capacity and computation efficiency, we introduce a locality sensitive sparse encoding, which allows us to conduct efficient sparse updates even 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#23545;&#21463;&#38480;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#21487;&#25511;&#30340;&#22270;&#20687;&#25805;&#20316;&#12290;&#36890;&#36807;&#20462;&#25913;&#35821;&#20041;&#22320;&#22270;&#65292;&#21487;&#20197;&#26041;&#20415;&#22320;&#25554;&#20837;&#12289;&#21024;&#38500;&#25110;&#26367;&#25442;&#22270;&#20687;&#20013;&#30340;&#23545;&#35937;&#12290;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#20266;&#36896;&#21644;&#22270;&#20687;&#32534;&#36753;&#39046;&#22495;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#20215;&#20540;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.13006</link><description>&lt;p&gt;
CIMGEN: &#21463;&#38480;&#25968;&#25454;&#19978;&#23545;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;&#21487;&#25511;&#22270;&#20687;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
CIMGEN: Controlled Image Manipulation by Finetuning Pretrained Generative Models on Limited Data. (arXiv:2401.13006v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#23545;&#21463;&#38480;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#21487;&#25511;&#30340;&#22270;&#20687;&#25805;&#20316;&#12290;&#36890;&#36807;&#20462;&#25913;&#35821;&#20041;&#22320;&#22270;&#65292;&#21487;&#20197;&#26041;&#20415;&#22320;&#25554;&#20837;&#12289;&#21024;&#38500;&#25110;&#26367;&#25442;&#22270;&#20687;&#20013;&#30340;&#23545;&#35937;&#12290;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#20266;&#36896;&#21644;&#22270;&#20687;&#32534;&#36753;&#39046;&#22495;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#20215;&#20540;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20869;&#23481;&#21019;&#20316;&#21644;&#22270;&#20687;&#32534;&#36753;&#21487;&#20197;&#21463;&#30410;&#20110;&#29992;&#25143;&#30340;&#28789;&#27963;&#25511;&#21046;&#12290;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#30340;&#24120;&#35265;&#20013;&#38388;&#34920;&#31034;&#26159;&#35821;&#20041;&#22320;&#22270;&#65292;&#20854;&#20013;&#21253;&#21547;&#22270;&#20687;&#20013;&#23384;&#22312;&#30340;&#23545;&#35937;&#30340;&#20449;&#24687;&#12290;&#19982;&#21407;&#22987;RGB&#20687;&#32032;&#30456;&#27604;&#65292;&#20462;&#25913;&#35821;&#20041;&#22320;&#22270;&#35201;&#23481;&#26131;&#24471;&#22810;&#12290;&#21487;&#20197;&#37319;&#29992;&#35821;&#20041;&#22320;&#22270;&#24182;&#36731;&#26494;&#20462;&#25913;&#22320;&#22270;&#20197;&#36873;&#25321;&#24615;&#22320;&#25554;&#20837;&#12289;&#21024;&#38500;&#25110;&#26367;&#25442;&#22320;&#22270;&#20013;&#30340;&#23545;&#35937;&#12290;&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#25509;&#21463;&#20462;&#25913;&#21518;&#30340;&#35821;&#20041;&#22320;&#22270;&#65292;&#24182;&#26681;&#25454;&#20462;&#25913;&#21518;&#30340;&#22320;&#22270;&#35843;&#25972;&#21407;&#22987;&#22270;&#20687;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20256;&#32479;&#30340;&#39044;&#35757;&#32451;&#22270;&#20687;&#21040;&#22270;&#20687;&#36716;&#25442;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#65292;&#22914;CycleGAN&#25110;Pix2Pix GAN&#65292;&#22312;&#19982;&#35821;&#20041;&#22320;&#22270;&#30456;&#20851;&#30340;&#21442;&#32771;&#22270;&#20687;&#30340;&#26377;&#38480;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#30340;&#25216;&#26415;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#24615;&#33021;&#65292;&#20197;&#35828;&#26126;&#23427;&#22312;&#22270;&#20687;&#20266;&#36896;&#21644;&#22270;&#20687;&#32534;&#36753;&#39046;&#22495;&#30340;&#33021;&#21147;&#21644;&#21487;&#33021;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25552;&#35758;&#30340;&#22270;&#20687;&#20266;&#36896;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Content creation and image editing can benefit from flexible user controls. A common intermediate representation for conditional image generation is a semantic map, that has information of objects present in the image. When compared to raw RGB pixels, the modification of semantic map is much easier. One can take a semantic map and easily modify the map to selectively insert, remove, or replace objects in the map. The method proposed in this paper takes in the modified semantic map and alter the original image in accordance to the modified map. The method leverages traditional pre-trained image-to-image translation GANs, such as CycleGAN or Pix2Pix GAN, that are fine-tuned on a limited dataset of reference images associated with the semantic maps. We discuss the qualitative and quantitative performance of our technique to illustrate its capacity and possible applications in the fields of image forgery and image editing. We also demonstrate the effectiveness of the proposed image forgery
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20851;&#20110;&#24490;&#29615;2n&#36793;&#24418;&#30340;&#19968;&#20010;&#20960;&#20309;&#23450;&#29702;&#31867;&#21035;&#65292;&#35777;&#26126;&#20102;&#24403;&#21462;n&#20010;&#19981;&#30456;&#20132;&#30340;&#36793;&#23545;&#26102;&#65292;&#36825;&#20123;&#36793;&#20043;&#38388;&#35282;&#24230;&#30340;&#32447;&#24615;&#32452;&#21512;&#26159;&#24658;&#23450;&#30340;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#20844;&#24335;&#26469;&#34920;&#31034;&#36825;&#20010;&#32447;&#24615;&#32452;&#21512;&#12290;&#35770;&#25991;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#21033;&#29992;&#36825;&#20010;&#32467;&#26524;&#29983;&#25104;&#26032;&#30340;&#20960;&#20309;&#35777;&#26126;&#38382;&#39064;&#21450;&#20854;&#35299;&#27861;&#30340;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2401.13002</link><description>&lt;p&gt;
&#24490;&#29615;&#22810;&#36793;&#24418;&#20013;&#30340;&#23450;&#29702;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Theorem Discovery Amongst Cyclic Polygons. (arXiv:2401.13002v1 [cs.CG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13002
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20851;&#20110;&#24490;&#29615;2n&#36793;&#24418;&#30340;&#19968;&#20010;&#20960;&#20309;&#23450;&#29702;&#31867;&#21035;&#65292;&#35777;&#26126;&#20102;&#24403;&#21462;n&#20010;&#19981;&#30456;&#20132;&#30340;&#36793;&#23545;&#26102;&#65292;&#36825;&#20123;&#36793;&#20043;&#38388;&#35282;&#24230;&#30340;&#32447;&#24615;&#32452;&#21512;&#26159;&#24658;&#23450;&#30340;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#20844;&#24335;&#26469;&#34920;&#31034;&#36825;&#20010;&#32447;&#24615;&#32452;&#21512;&#12290;&#35770;&#25991;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#21033;&#29992;&#36825;&#20010;&#32467;&#26524;&#29983;&#25104;&#26032;&#30340;&#20960;&#20309;&#35777;&#26126;&#38382;&#39064;&#21450;&#20854;&#35299;&#27861;&#30340;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20851;&#20110;&#24490;&#29615;2n&#36793;&#24418;&#30340;&#20960;&#20309;&#23450;&#29702;&#31867;&#21035;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22914;&#26524;&#25105;&#20204;&#21462;n&#20010;&#19981;&#30456;&#20132;&#30340;&#36793;&#23545;&#65292;&#27599;&#20010;&#23545;&#20043;&#38388;&#26377;&#20598;&#25968;&#20010;&#22810;&#36793;&#24418;&#36793;&#20998;&#38548;&#65292;&#21017;&#36825;&#20123;&#36793;&#20043;&#38388;&#30340;&#35282;&#24230;&#30340;&#32447;&#24615;&#32452;&#21512;&#26159;&#24658;&#23450;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32447;&#24615;&#32452;&#21512;&#30340;&#20844;&#24335;&#65292;&#23427;&#20197;&#36825;&#20123;&#35282;&#24230;&#30340;&#26041;&#24335;&#32473;&#20986;&#20102;&#19968;&#20010;&#23450;&#29702;&#38472;&#36848;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#21033;&#29992;&#36825;&#20010;&#32467;&#26524;&#29983;&#25104;&#26032;&#30340;&#20960;&#20309;&#35777;&#26126;&#38382;&#39064;&#21450;&#20854;&#35299;&#27861;&#30340;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
We examine a class of geometric theorems on cyclic 2n-gons. We prove that if we take n disjoint pairs of sides, each pair separated by an even number of polygon sides, then there is a linear combination of the angles between those sides which is constant. We present a formula for the linear combination, which provides a theorem statement in terms of those angles. We describe a program which uses this result to generate new geometry proof problems and their solutions.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#22270;&#29255;&#29983;&#25104;&#25277;&#35937;&#32918;&#20687;&#32472;&#30011;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21333;&#19968;&#30340;&#25163;&#32472;&#22270;&#26696;&#32032;&#25551;&#21644;&#22270;&#24418;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#31508;&#35302;&#21464;&#21270;&#65292;&#21019;&#36896;&#20986;&#39118;&#26684;&#29420;&#29305;&#30340;&#20805;&#28385;&#21916;&#24742;&#30340;&#25277;&#35937;&#32472;&#30011;&#12290;</title><link>http://arxiv.org/abs/2401.13001</link><description>&lt;p&gt;
PatternPortrait&#65306;&#29992;&#20320;&#30340;&#28034;&#40486;&#30011;&#20986;&#25105;&#12290; (arXiv:2401.13001v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
PatternPortrait: Draw Me Like One of Your Scribbles. (arXiv:2401.13001v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#22270;&#29255;&#29983;&#25104;&#25277;&#35937;&#32918;&#20687;&#32472;&#30011;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21333;&#19968;&#30340;&#25163;&#32472;&#22270;&#26696;&#32032;&#25551;&#21644;&#22270;&#24418;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#31508;&#35302;&#21464;&#21270;&#65292;&#21019;&#36896;&#20986;&#39118;&#26684;&#29420;&#29305;&#30340;&#20805;&#28385;&#21916;&#24742;&#30340;&#25277;&#35937;&#32472;&#30011;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#22270;&#29255;&#29983;&#25104;&#25277;&#35937;&#32918;&#20687;&#32472;&#30011;&#30340;&#26041;&#27861;&#12290;&#20854;&#29420;&#29305;&#30340;&#39118;&#26684;&#26159;&#36890;&#36807;&#21033;&#29992;&#21333;&#19968;&#30340;&#25163;&#32472;&#22270;&#26696;&#32032;&#25551;&#20316;&#20026;&#21442;&#32771;&#26469;&#29983;&#25104;&#29992;&#20110;&#38452;&#24433;&#30340;&#29420;&#29305;&#22270;&#26696;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#20174;&#22270;&#20687;&#20013;&#25552;&#21462;&#38754;&#37096;&#21644;&#36523;&#20307;&#29305;&#24449;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#21521;&#37327;&#32447;&#26465;&#12290;&#30740;&#31350;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#26159;&#24320;&#21457;&#19968;&#31181;&#22270;&#24418;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#26088;&#22312;&#23398;&#20064;&#21521;&#37327;&#24418;&#24335;&#30340;&#32032;&#25551;&#31508;&#35302;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#31508;&#35302;&#21464;&#21270;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#32452;&#21512;&#21019;&#36896;&#20102;&#20805;&#28385;&#21916;&#24742;&#30340;&#25277;&#35937;&#32472;&#30011;&#65292;&#36890;&#36807;&#38050;&#31508;&#32472;&#22270;&#20202;&#23454;&#29616;&#12290;&#25152;&#20171;&#32461;&#30340;&#36807;&#31243;&#22312;&#22823;&#32422;280&#21517;&#21442;&#19982;&#32773;&#20013;&#33719;&#24471;&#20102;&#31215;&#26497;&#30340;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a process for generating abstract portrait drawings from pictures. Their unique style is created by utilizing single freehand pattern sketches as references to generate unique patterns for shading. The method involves extracting facial and body features from images and transforming them into vector lines. A key aspect of the research is the development of a graph neural network architecture designed to learn sketch stroke representations in vector form, enabling the generation of diverse stroke variations. The combination of these two approaches creates joyful abstract drawings that are realized via a pen plotter. The presented process garnered positive feedback from an audience of approximately 280 participants.
&lt;/p&gt;</description></item><item><title>&#37327;&#23376;&#21551;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#20998;&#23376;&#23545;&#25509;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#36890;&#36807;&#32467;&#21512;&#37327;&#23376;&#29305;&#24615;&#21644;&#28145;&#24230;&#23398;&#20064;&#22312;&#32534;&#30721;&#30340;&#20998;&#23376;&#31354;&#38388;&#20013;&#23398;&#20064;&#30340;&#26799;&#24230;&#65292;&#25552;&#39640;&#20102;&#30450;&#30446;&#23545;&#25509;&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.12999</link><description>&lt;p&gt;
&#37327;&#23376;&#21551;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#20998;&#23376;&#23545;&#25509;
&lt;/p&gt;
&lt;p&gt;
Quantum-Inspired Machine Learning for Molecular Docking. (arXiv:2401.12999v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12999
&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#21551;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#20998;&#23376;&#23545;&#25509;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#36890;&#36807;&#32467;&#21512;&#37327;&#23376;&#29305;&#24615;&#21644;&#28145;&#24230;&#23398;&#20064;&#22312;&#32534;&#30721;&#30340;&#20998;&#23376;&#31354;&#38388;&#20013;&#23398;&#20064;&#30340;&#26799;&#24230;&#65292;&#25552;&#39640;&#20102;&#30450;&#30446;&#23545;&#25509;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#23545;&#25509;&#26159;&#26500;&#24314;&#22522;&#20110;&#32467;&#26500;&#30340;&#33647;&#29289;&#35774;&#35745;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#21487;&#20197;&#21152;&#24555;&#33647;&#29289;&#24320;&#21457;&#30340;&#25928;&#29575;&#12290;&#34507;&#30333;&#36136;&#21644;&#23567;&#20998;&#23376;&#20043;&#38388;&#30340;&#22797;&#26434;&#21644;&#21160;&#24577;&#32467;&#21512;&#36807;&#31243;&#38656;&#35201;&#22312;&#24191;&#27867;&#30340;&#31354;&#38388;&#33539;&#22260;&#20869;&#36827;&#34892;&#25628;&#32034;&#21644;&#37319;&#26679;&#12290;&#20256;&#32479;&#30340;&#23545;&#25509;&#26041;&#27861;&#36890;&#36807;&#25628;&#32034;&#21487;&#33021;&#30340;&#32467;&#21512;&#20301;&#28857;&#21644;&#26500;&#35937;&#26469;&#23454;&#29616;&#65292;&#35745;&#31639;&#22797;&#26434;&#24230;&#39640;&#65292;&#22312;&#30450;&#30446;&#23545;&#25509;&#20013;&#25928;&#26524;&#19981;&#20339;&#12290;&#21463;&#21040;&#36825;&#19968;&#28857;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#37327;&#23376;&#21551;&#21457;&#31639;&#27861;&#19982;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#22312;&#32534;&#30721;&#30340;&#20998;&#23376;&#31354;&#38388;&#20013;&#23398;&#20064;&#30340;&#26799;&#24230;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#22312;&#30450;&#30446;&#23545;&#25509;&#20013;&#30340;&#25913;&#36827;&#12290;&#25968;&#20540;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20256;&#32479;&#30340;&#23545;&#25509;&#31639;&#27861;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31639;&#27861;&#19978;&#34920;&#29616;&#20986;&#20102;&#36229;&#36807;10%&#30340;&#25552;&#21319;&#12290;&#19982;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#23545;&#25509;&#31639;&#27861;DiffDock&#30456;&#27604;&#65292;Top-1&#65288;RMSD&lt;2&#65289;&#30340;&#25104;&#21151;&#29575;&#20174;33%&#25552;&#39640;&#21040;35%&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular docking is an important tool for structure-based drug design, accelerating the efficiency of drug development. Complex and dynamic binding processes between proteins and small molecules require searching and sampling over a wide spatial range. Traditional docking by searching for possible binding sites and conformations is computationally complex and results poorly under blind docking. Quantum-inspired algorithms combining quantum properties and annealing show great advantages in solving combinatorial optimization problems. Inspired by this, we achieve an improved in blind docking by using quantum-inspired combined with gradients learned by deep learning in the encoded molecular space. Numerical simulation shows that our method outperforms traditional docking algorithms and deep learning-based algorithms over 10\%. Compared to the current state-of-the-art deep learning-based docking algorithm DiffDock, the success rate of Top-1 (RMSD&lt;2) achieves an improvement from 33\% to 35
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29305;&#23450;&#39046;&#22495;&#21307;&#23398;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#20197;&#39592;&#20851;&#33410;&#28814;&#31649;&#29702;&#20026;&#20363;&#36827;&#34892;&#20102;&#22686;&#24378;&#12290;&#30740;&#31350;&#32467;&#26524;&#21457;&#29616;&#65292;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#30456;&#23545;&#20110;&#39046;&#22495;&#29305;&#23450;&#30340;&#39592;&#20851;&#33410;&#28814;&#31649;&#29702;&#27169;&#22411;&#22312;&#25552;&#20379;&#20010;&#24615;&#21270;&#27835;&#30103;&#24314;&#35758;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#32780;&#19987;&#38376;&#27169;&#22411;&#34920;&#29616;&#26377;&#26126;&#26174;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.12998</link><description>&lt;p&gt;
&#35780;&#20272;&#21644;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29305;&#23450;&#39046;&#22495;&#21307;&#23398;&#20013;&#30340;&#34920;&#29616;&#65306;&#20197;DocOA&#20026;&#20363;&#30340;&#39592;&#20851;&#33410;&#28814;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Evaluating and Enhancing Large Language Models Performance in Domain-specific Medicine: Osteoarthritis Management with DocOA. (arXiv:2401.12998v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12998
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29305;&#23450;&#39046;&#22495;&#21307;&#23398;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#20197;&#39592;&#20851;&#33410;&#28814;&#31649;&#29702;&#20026;&#20363;&#36827;&#34892;&#20102;&#22686;&#24378;&#12290;&#30740;&#31350;&#32467;&#26524;&#21457;&#29616;&#65292;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#30456;&#23545;&#20110;&#39046;&#22495;&#29305;&#23450;&#30340;&#39592;&#20851;&#33410;&#28814;&#31649;&#29702;&#27169;&#22411;&#22312;&#25552;&#20379;&#20010;&#24615;&#21270;&#27835;&#30103;&#24314;&#35758;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#32780;&#19987;&#38376;&#27169;&#22411;&#34920;&#29616;&#26377;&#26126;&#26174;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29305;&#23450;&#39046;&#22495;&#21307;&#23398;&#20013;&#30340;&#25928;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#31649;&#29702;&#39592;&#20851;&#33410;&#28814;&#65288;OA&#65289;&#31561;&#22797;&#26434;&#30142;&#30149;&#26041;&#38754;&#65292;&#20173;&#28982;&#22823;&#37096;&#20998;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#35780;&#20272;&#21644;&#25552;&#21319;LLMs&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#20020;&#24202;&#33021;&#21147;&#65292;&#20197;&#39592;&#20851;&#33410;&#28814;&#65288;OA&#65289;&#31649;&#29702;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#24320;&#21457;&#20102;&#19968;&#20010;&#29305;&#23450;&#39046;&#22495;&#30340;&#22522;&#20934;&#26694;&#26550;&#65292;&#35780;&#20272;LLMs&#22312;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#21644;&#30495;&#23454;&#20020;&#24202;&#22330;&#26223;&#20013;&#30340;&#20020;&#24202;&#24212;&#29992;&#20043;&#38388;&#30340;&#34920;&#29616;&#12290;&#24320;&#21457;&#20102;&#19968;&#31181;&#38024;&#23545;OA&#31649;&#29702;&#30340;&#19987;&#38376;LLM&#65292;&#21517;&#20026;DocOA&#65292;&#23427;&#32467;&#21512;&#20102;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#21644;&#25351;&#20196;&#25552;&#31034;&#12290;&#36890;&#36807;&#23458;&#35266;&#21644;&#20154;&#24037;&#35780;&#20272;&#65292;&#27604;&#36739;&#20102;GPT-3.5&#12289;GPT-4&#21644;&#19987;&#38376;&#21161;&#25163;DocOA&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#36890;&#29992;LLMs&#22914;GPT-3.5&#21644;GPT-4&#22312;OA&#31649;&#29702;&#36825;&#31181;&#19987;&#38376;&#39046;&#22495;&#20013;&#30340;&#34920;&#29616;&#36739;&#24046;&#65292;&#23588;&#20854;&#26159;&#22312;&#25552;&#20379;&#20010;&#24615;&#21270;&#27835;&#30103;&#24314;&#35758;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;DocOA&#26174;&#31034;&#20102;&#26174;&#33879;&#30340;&#34920;&#29616;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
The efficacy of large language models (LLMs) in domain-specific medicine, particularly for managing complex diseases such as osteoarthritis (OA), remains largely unexplored. This study focused on evaluating and enhancing the clinical capabilities of LLMs in specific domains, using osteoarthritis (OA) management as a case study. A domain specific benchmark framework was developed, which evaluate LLMs across a spectrum from domain-specific knowledge to clinical applications in real-world clinical scenarios. DocOA, a specialized LLM tailored for OA management that integrates retrieval-augmented generation (RAG) and instruction prompts, was developed. The study compared the performance of GPT-3.5, GPT-4, and a specialized assistant, DocOA, using objective and human evaluations. Results showed that general LLMs like GPT-3.5 and GPT-4 were less effective in the specialized domain of OA management, particularly in providing personalized treatment recommendations. However, DocOA showed signifi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32763;&#35793;&#20102;&#30340;&#20020;&#24202;&#35760;&#24405;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#21457;&#29616;&#20102;&#23384;&#22312;&#38382;&#39064;&#30340;&#40486;&#29255;&#20351;&#29992;&#30340;&#36864;&#20237;&#20891;&#20154;&#12290;&#19982;&#20165;&#36890;&#36807;&#35786;&#26029;&#20195;&#30721;&#35782;&#21035;&#30340;&#40486;&#29255;&#20351;&#29992;&#38556;&#30861;&#24739;&#32773;&#30456;&#27604;&#65292;&#36825;&#20123;&#24739;&#32773;&#20855;&#26377;&#19981;&#21516;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#21644;&#20020;&#24202;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2401.12996</link><description>&lt;p&gt;
&#36890;&#36807;&#20020;&#24202;&#35760;&#24405;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#19982;&#20351;&#29992;&#35786;&#26029;&#20195;&#30721;&#23545;&#27604;&#21457;&#29616;&#23384;&#22312;&#38382;&#39064;&#30340;&#30103;&#25928;&#24615;&#40486;&#29255;&#20351;&#29992;&#30340;&#36864;&#20237;&#20891;&#20154;&#65306;&#19968;&#39033;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comparison of Veterans with Problematic Opioid Use Identified through Natural Language Processing of Clinical Notes versus Using Diagnostic Codes. (arXiv:2401.12996v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12996
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32763;&#35793;&#20102;&#30340;&#20020;&#24202;&#35760;&#24405;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#21457;&#29616;&#20102;&#23384;&#22312;&#38382;&#39064;&#30340;&#40486;&#29255;&#20351;&#29992;&#30340;&#36864;&#20237;&#20891;&#20154;&#12290;&#19982;&#20165;&#36890;&#36807;&#35786;&#26029;&#20195;&#30721;&#35782;&#21035;&#30340;&#40486;&#29255;&#20351;&#29992;&#38556;&#30861;&#24739;&#32773;&#30456;&#27604;&#65292;&#36825;&#20123;&#24739;&#32773;&#20855;&#26377;&#19981;&#21516;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#21644;&#20020;&#24202;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#26159;&#40486;&#29255;&#31867;&#33647;&#29289;&#30740;&#31350;&#30340;&#25968;&#25454;&#26469;&#28304;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#40486;&#29255;&#31867;&#33647;&#29289;&#28389;&#29992;&#38590;&#20197;&#36890;&#36807;&#35786;&#26029;&#20195;&#30721;&#36827;&#34892;&#32534;&#30721;&#65292;&#20294;&#26159;&#23384;&#22312;&#38382;&#39064;&#30340;&#40486;&#29255;&#20351;&#29992;&#21487;&#20197;&#35760;&#24405;&#22312;&#20020;&#24202;&#35760;&#24405;&#20013;&#12290;&#30446;&#26631;&#65306;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;1&#65289;&#20174;&#21508;&#31181;&#20020;&#24202;&#35760;&#24405;&#20013;&#35782;&#21035;&#23384;&#22312;&#38382;&#39064;&#30340;&#40486;&#29255;&#20351;&#29992;&#65307;2&#65289;&#27604;&#36739;&#20165;&#36890;&#36807;&#20020;&#24202;&#35760;&#24405;&#35760;&#24405;&#23384;&#22312;&#38382;&#39064;&#40486;&#29255;&#20351;&#29992;&#30340;&#24739;&#32773;&#19982;&#20351;&#29992;ICD&#40486;&#29255;&#20351;&#29992;&#38556;&#30861;&#35786;&#26029;&#20195;&#30721;&#30340;&#24739;&#32773;&#30340;&#29305;&#24449;&#12290;&#26448;&#26009;&#19982;&#26041;&#27861;&#65306;&#25105;&#20204;&#24320;&#21457;&#24182;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24037;&#20855;&#65292;&#23545;&#26469;&#33258;&#20004;&#20010;&#36864;&#20237;&#20891;&#20154;&#20107;&#21153;&#25152;&#21306;&#22495;&#30340;&#24739;&#32773;&#38431;&#21015;&#65288;n=222,371&#65289;&#30340;&#20020;&#24202;&#35760;&#24405;&#36827;&#34892;&#20998;&#26512;&#20197;&#35782;&#21035;&#23384;&#22312;&#38382;&#39064;&#30340;&#40486;&#29255;&#20351;&#29992;&#30340;&#24739;&#32773;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#19968;&#32452;ICD&#35786;&#26029;&#20195;&#30721;&#26469;&#35782;&#21035;&#26469;&#33258;&#30456;&#21516;&#38431;&#21015;&#30340;&#24739;&#26377;&#40486;&#29255;&#20351;&#29992;&#38556;&#30861;&#30340;&#24739;&#32773;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20165;&#36890;&#36807;NLP&#35782;&#21035;&#20986;&#30340;&#24739;&#32773;&#19982;&#36890;&#36807;&#35786;&#26029;&#20195;&#30721;&#35782;&#21035;&#20986;&#30340;&#24739;&#32773;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#21644;&#20020;&#24202;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: Electronic health records (EHRs) are a data source for opioid research. Opioid use disorder is known to be under-coded as a diagnosis, yet problematic opioid use can be documented in clinical notes.  Objectives: Our goals were 1) to identify problematic opioid use from a full range of clinical notes; and 2) to compare the characteristics of patients identified as having problematic opioid use, exclusively documented in clinical notes, to those having documented ICD opioid use disorder diagnostic codes.  Materials and Methods: We developed and applied a natural language processing (NLP) tool to the clinical notes of a patient cohort (n=222,371) from two Veteran Affairs service regions to identify patients with problematic opioid use. We also used a set of ICD diagnostic codes to identify patients with opioid use disorder from the same cohort. We compared the demographic and clinical characteristics of patients identified only through NLP, to those of patients identified thro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24930;&#24615;&#30149;&#31649;&#29702;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;Prompt&#24037;&#31243;&#36827;&#34892;&#31934;&#31070;&#38556;&#30861;&#30340;&#26816;&#27979;&#65292;&#36890;&#36807;&#20010;&#24615;&#21270;&#30340;&#25552;&#31034;&#21644;&#21307;&#30103;&#30693;&#35782;&#27880;&#20837;&#26469;&#35299;&#20915;&#25968;&#25454;&#25361;&#25112;&#65292;&#23454;&#29616;&#24930;&#24615;&#30149;&#31649;&#29702;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2401.12988</link><description>&lt;p&gt;
&#24930;&#24615;&#30149;&#31649;&#29702;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;Prompt&#24037;&#31243;&#19982;&#21307;&#30103;&#30693;&#35782;&#27880;&#20837;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Learning for Chronic Disease Management: Leveraging Large Language Models and Multi-Prompt Engineering with Medical Knowledge Injection. (arXiv:2401.12988v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12988
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24930;&#24615;&#30149;&#31649;&#29702;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;Prompt&#24037;&#31243;&#36827;&#34892;&#31934;&#31070;&#38556;&#30861;&#30340;&#26816;&#27979;&#65292;&#36890;&#36807;&#20010;&#24615;&#21270;&#30340;&#25552;&#31034;&#21644;&#21307;&#30103;&#30693;&#35782;&#27880;&#20837;&#26469;&#35299;&#20915;&#25968;&#25454;&#25361;&#25112;&#65292;&#23454;&#29616;&#24930;&#24615;&#30149;&#31649;&#29702;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#36827;&#34892;&#24930;&#24615;&#30149;&#31649;&#29702;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#29992;&#25143;&#29983;&#25104;&#30340;&#25991;&#26412;&#20869;&#23481;&#26469;&#26816;&#27979;&#21508;&#31181;&#31934;&#31070;&#38556;&#30861;&#12290;&#29616;&#26377;&#30740;&#31350;&#36890;&#24120;&#20381;&#36182;&#20110;&#20840;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#65292;&#36825;&#24102;&#26469;&#20102;&#19968;&#20123;&#25361;&#25112;&#65292;&#27604;&#22914;&#27880;&#37322;&#24222;&#22823;&#30340;&#35757;&#32451;&#25968;&#25454;&#23545;&#20110;&#27599;&#31181;&#30142;&#30149;&#30340;&#36153;&#26102;&#36153;&#21147;&#30340;&#25163;&#21160;&#36807;&#31243;&#65292;&#20197;&#21450;&#38656;&#35201;&#20026;&#27599;&#20010;&#38382;&#39064;&#35774;&#35745;&#19987;&#38376;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#20102;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#21253;&#25324;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;Prompt&#24037;&#31243;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#25968;&#25454;&#39537;&#21160;&#24930;&#24615;&#30149;&#31649;&#29702;&#20013;&#30340;&#20004;&#20010;&#20851;&#38190;&#25216;&#26415;&#25361;&#25112;&#65306;&#65288;1&#65289;&#24320;&#21457;&#20010;&#24615;&#21270;&#30340;&#25552;&#31034;&#26469;&#34920;&#31034;&#27599;&#20010;&#29992;&#25143;&#30340;&#29420;&#29305;&#24615;&#65292;&#20197;&#21450;&#65288;2&#65289;&#23558;&#21307;&#30103;&#30693;&#35782;&#34701;&#20837;&#21040;&#25552;&#31034;&#20013;&#65292;&#20026;&#24930;&#24615;&#30149;&#26816;&#27979;&#25552;&#20379;&#19978;&#19979;&#25991;&#65292;&#25351;&#23548;&#23398;&#20064;&#30446;&#26631;&#65292;&#24182;&#23454;&#29616;&#39044;&#27979;&#30446;&#26631;&#12290;&#25105;&#20204;&#20351;&#29992;&#22235;&#31181;&#31934;&#31070;&#38556;&#30861;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study harnesses state-of-the-art AI technology for chronic disease management, specifically in detecting various mental disorders through user-generated textual content. Existing studies typically rely on fully supervised machine learning, which presents challenges such as the labor-intensive manual process of annotating extensive training data for each disease and the need to design specialized deep learning architectures for each problem. To address such challenges, we propose a novel framework that leverages advanced AI techniques, including large language models and multi-prompt engineering. Specifically, we address two key technical challenges in data-driven chronic disease management: (1) developing personalized prompts to represent each user's uniqueness and (2) incorporating medical knowledge into prompts to provide context for chronic disease detection, instruct learning objectives, and operationalize prediction goals. We evaluate our method using four mental disorders, w
&lt;/p&gt;</description></item><item><title>&#20247;&#21253;&#33258;&#36866;&#24212;&#35843;&#26597;&#26041;&#27861;&#65288;CSAS&#65289;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#33021;&#22815;&#26681;&#25454;&#29992;&#25143;&#36755;&#20837;&#28436;&#21464;&#38382;&#39064;&#24211;&#65292;&#24182;&#22312;&#35843;&#26597;&#20013;&#36866;&#24212;&#26032;&#30340;&#38382;&#39064;&#65292;&#24212;&#29992;&#22312;&#25289;&#19969;&#35028;&#20449;&#24687;&#29615;&#22659;&#21644;&#35758;&#39064;&#37325;&#35201;&#24615;&#39046;&#22495;&#65292;&#33021;&#22815;&#35782;&#21035;&#38590;&#20197;&#36890;&#36807;&#20256;&#32479;&#26041;&#27861;&#36319;&#36394;&#30340;&#20027;&#24352;&#25110;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.12986</link><description>&lt;p&gt;
&#20247;&#21253;&#33258;&#36866;&#24212;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Crowdsourced Adaptive Surveys. (arXiv:2401.12986v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12986
&lt;/p&gt;
&lt;p&gt;
&#20247;&#21253;&#33258;&#36866;&#24212;&#35843;&#26597;&#26041;&#27861;&#65288;CSAS&#65289;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#33021;&#22815;&#26681;&#25454;&#29992;&#25143;&#36755;&#20837;&#28436;&#21464;&#38382;&#39064;&#24211;&#65292;&#24182;&#22312;&#35843;&#26597;&#20013;&#36866;&#24212;&#26032;&#30340;&#38382;&#39064;&#65292;&#24212;&#29992;&#22312;&#25289;&#19969;&#35028;&#20449;&#24687;&#29615;&#22659;&#21644;&#35758;&#39064;&#37325;&#35201;&#24615;&#39046;&#22495;&#65292;&#33021;&#22815;&#35782;&#21035;&#38590;&#20197;&#36890;&#36807;&#20256;&#32479;&#26041;&#27861;&#36319;&#36394;&#30340;&#20027;&#24352;&#25110;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#20247;&#33286;&#35770;&#35843;&#26597;&#23545;&#20110;&#27665;&#20027;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#23545;&#20110;&#20256;&#32479;&#35843;&#26597;&#26041;&#27861;&#26469;&#35828;&#65292;&#24555;&#36895;&#21464;&#21270;&#30340;&#20449;&#24687;&#29615;&#22659;&#21644;&#22312;&#23567;&#20247;&#31038;&#21306;&#20013;&#34913;&#37327;&#35266;&#28857;&#21487;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20247;&#21253;&#33258;&#36866;&#24212;&#35843;&#26597;&#26041;&#27861;&#65288;CSAS&#65289;&#65292;&#23427;&#23558;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#33258;&#36866;&#24212;&#31639;&#27861;&#30340;&#36827;&#23637;&#32467;&#21512;&#36215;&#26469;&#65292;&#29983;&#25104;&#38543;&#30528;&#29992;&#25143;&#36755;&#20837;&#19981;&#26029;&#28436;&#21464;&#30340;&#38382;&#39064;&#24211;&#12290;CSAS&#26041;&#27861;&#23558;&#21442;&#19982;&#32773;&#25552;&#20379;&#30340;&#24320;&#25918;&#24335;&#25991;&#26412;&#36716;&#25442;&#20026;Likert&#24335;&#39033;&#30446;&#65292;&#24182;&#24212;&#29992;&#22810;&#33218;&#36172;&#21338;&#31639;&#27861;&#26469;&#30830;&#23450;&#24212;&#20248;&#20808;&#32771;&#34385;&#22312;&#35843;&#26597;&#20013;&#30340;&#29992;&#25143;&#25552;&#20379;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#30340;&#33258;&#36866;&#24212;&#24615;&#20801;&#35768;&#25506;&#32034;&#26032;&#30340;&#35843;&#26597;&#38382;&#39064;&#65292;&#21516;&#26102;&#22312;&#35843;&#26597;&#38271;&#24230;&#19978;&#26045;&#21152;&#26368;&#23567;&#30340;&#25104;&#26412;&#12290;&#22312;&#25289;&#19969;&#35028;&#20449;&#24687;&#29615;&#22659;&#21644;&#35758;&#39064;&#37325;&#35201;&#24615;&#39046;&#22495;&#30340;&#24212;&#29992;&#23637;&#31034;&#20102;CSAS&#35782;&#21035;&#21487;&#33021;&#38590;&#20197;&#36890;&#36807;&#26631;&#20934;&#26041;&#27861;&#36319;&#36394;&#30340;&#20027;&#24352;&#25110;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#25552;&#20986; Conclusion by di&#30340;&#32467;&#26463;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;
Public opinion surveys are vital for informing democratic decision-making, but responding to rapidly changing information environments and measuring beliefs within niche communities can be challenging for traditional survey methods. This paper introduces a crowdsourced adaptive survey methodology (CSAS) that unites advances in natural language processing and adaptive algorithms to generate question banks that evolve with user input. The CSAS method converts open-ended text provided by participants into Likert-style items and applies a multi-armed bandit algorithm to determine user-provided questions that should be prioritized in the survey. The method's adaptive nature allows for the exploration of new survey questions, while imposing minimal costs in survey length. Applications in the domains of Latino information environments and issue importance showcase CSAS's ability to identify claims or issues that might otherwise be difficult to track using standard approaches. I conclude by di
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#19977;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26426;&#26800;&#24037;&#31243;&#39046;&#22495;&#20013;&#35299;&#20915;&#27010;&#24565;&#24615;&#38382;&#39064;&#30340;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT-4&#22312;&#21508;&#20010;&#21147;&#23398;&#20027;&#39064;&#30340;&#38382;&#39064;&#22238;&#31572;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#20004;&#20010;&#27169;&#22411;&#21644;&#20154;&#31867;&#23545;&#29031;&#32452;&#65292;&#26174;&#31034;&#20986;&#28508;&#22312;&#30340;&#26410;&#26469;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2401.12983</link><description>&lt;p&gt;
&#22312;&#26426;&#26800;&#24037;&#31243;&#25945;&#32946;&#20013;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#20851;&#20110;&#20197;&#21147;&#23398;&#20026;&#37325;&#28857;&#30340;&#27010;&#24565;&#29702;&#35299;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Assessing Large Language Models in Mechanical Engineering Education: A Study on Mechanics-Focused Conceptual Understanding. (arXiv:2401.12983v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12983
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#19977;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26426;&#26800;&#24037;&#31243;&#39046;&#22495;&#20013;&#35299;&#20915;&#27010;&#24565;&#24615;&#38382;&#39064;&#30340;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT-4&#22312;&#21508;&#20010;&#21147;&#23398;&#20027;&#39064;&#30340;&#38382;&#39064;&#22238;&#31572;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#20004;&#20010;&#27169;&#22411;&#21644;&#20154;&#31867;&#23545;&#29031;&#32452;&#65292;&#26174;&#31034;&#20986;&#28508;&#22312;&#30340;&#26410;&#26469;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26159;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26426;&#26800;&#24037;&#31243;&#39046;&#22495;&#20013;&#35299;&#20915;&#27010;&#24565;&#24615;&#38382;&#39064;&#30340;&#33021;&#21147;&#36827;&#34892;&#30340;&#24320;&#21019;&#24615;&#30740;&#31350;&#65292;&#37325;&#28857;&#26159;&#21147;&#23398;&#12290;&#25105;&#20204;&#20351;&#29992;&#21253;&#21547;126&#20010;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#25163;&#24037;&#21046;&#20316;&#30340;&#32771;&#35797;&#26469;&#36827;&#34892;&#32771;&#23519;&#65292;&#28085;&#30422;&#20102;&#21147;&#23398;&#35838;&#31243;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#21253;&#25324;&#27969;&#20307;&#21147;&#23398;&#12289;&#26426;&#26800;&#25391;&#21160;&#12289;&#24037;&#31243;&#38745;&#21147;&#23398;&#21644;&#21160;&#21147;&#23398;&#12289;&#26448;&#26009;&#21147;&#23398;&#12289;&#24377;&#24615;&#29702;&#35770;&#21644;&#36830;&#32493;&#20171;&#36136;&#21147;&#23398;&#12290;&#25105;&#20204;&#23545;&#19977;&#20010;LLM&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;ChatGPT&#65288;GPT-3.5&#65289;&#12289;ChatGPT&#65288;GPT-4&#65289;&#21644;Claude&#65288;Claude-2.1&#65289;&#65292;&#24182;&#23558;&#20854;&#19982;&#20855;&#26377;&#25110;&#27809;&#26377;&#26426;&#26800;&#24037;&#31243;&#32972;&#26223;&#30340;&#24037;&#31243;&#25945;&#32844;&#21592;&#21644;&#23398;&#29983;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#21508;&#31181;&#21147;&#23398;&#20027;&#39064;&#30340;&#38382;&#39064;&#22238;&#31572;&#26041;&#38754;&#65292;&#38500;&#20102;&#36830;&#32493;&#20171;&#36136;&#21147;&#23398;&#22806;&#65292;GPT-4&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#20004;&#20010;LLM&#21644;&#20154;&#31867;&#23545;&#29031;&#32452;&#12290;&#36825;&#34920;&#26126;GPT&#27169;&#22411;&#22312;&#22788;&#29702;&#31526;&#21495;&#35745;&#31639;&#21644;&#24352;&#37327;&#20998;&#26512;&#26041;&#38754;&#26377;&#28508;&#22312;&#30340;&#26410;&#26469;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study is a pioneering endeavor to investigate the capabilities of Large Language Models (LLMs) in addressing conceptual questions within the domain of mechanical engineering with a focus on mechanics. Our examination involves a manually crafted exam encompassing 126 multiple-choice questions, spanning various aspects of mechanics courses, including Fluid Mechanics, Mechanical Vibration, Engineering Statics and Dynamics, Mechanics of Materials, Theory of Elasticity, and Continuum Mechanics. Three LLMs, including ChatGPT (GPT-3.5), ChatGPT (GPT-4), and Claude (Claude-2.1), were subjected to evaluation against engineering faculties and students with or without mechanical engineering background. The findings reveal GPT-4's superior performance over the other two LLMs and human cohorts in answering questions across various mechanics topics, except for Continuum Mechanics. This signals the potential future improvements for GPT models in handling symbolic calculations and tensor analyses
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#23545;&#32454;&#32990;&#22806;&#38388;&#38553;&#20013;&#20998;&#23376;&#20256;&#36755;&#36827;&#34892;&#23450;&#37327;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#23545;&#20998;&#23376;&#20256;&#36755;&#24418;&#24335;&#19981;&#28165;&#26970;&#30340;&#25361;&#25112;&#65292;&#24182;&#23454;&#29616;&#20102;&#33258;&#21160;&#35745;&#31639;&#25193;&#25955;&#31995;&#25968;&#21644;&#20998;&#23376;&#36895;&#24230;&#30340;&#20248;&#21270;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.12435</link><description>&lt;p&gt;
&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#23545;&#32454;&#32990;&#22806;&#38388;&#38553;&#20013;&#30340;&#20998;&#23376;&#20256;&#36755;&#36827;&#34892;&#23450;&#37327;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Quantitative Analysis of Molecular Transport in the Extracellular Space Using Physics-Informed Neural Network. (arXiv:2401.12435v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#23545;&#32454;&#32990;&#22806;&#38388;&#38553;&#20013;&#20998;&#23376;&#20256;&#36755;&#36827;&#34892;&#23450;&#37327;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#23545;&#20998;&#23376;&#20256;&#36755;&#24418;&#24335;&#19981;&#28165;&#26970;&#30340;&#25361;&#25112;&#65292;&#24182;&#23454;&#29616;&#20102;&#33258;&#21160;&#35745;&#31639;&#25193;&#25955;&#31995;&#25968;&#21644;&#20998;&#23376;&#36895;&#24230;&#30340;&#20248;&#21270;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#30340;&#32454;&#32990;&#22806;&#38388;&#38553; (ECS)&#26159;&#20301;&#20110;&#32454;&#32990;&#20043;&#38388;&#25110;&#32454;&#32990;&#19982;&#34880;&#31649;&#20043;&#38388;&#30340;&#19981;&#35268;&#21017;&#12289;&#26497;&#20854;&#36802;&#22238;&#30340;&#32435;&#31859;&#32423;&#31354;&#38388;&#65292;&#23545;&#31070;&#32463;&#32454;&#32990;&#30340;&#29983;&#23384;&#33267;&#20851;&#37325;&#35201;&#12290;&#23427;&#22312;&#35760;&#24518;&#12289;&#24773;&#32490;&#21644;&#24863;&#35273;&#31561;&#39640;&#32423;&#33041;&#21151;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;ECS&#20869;&#20998;&#23376;&#20256;&#36755;&#30340;&#20855;&#20307;&#24418;&#24335;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476; (PINN) &#35299;&#20915;&#20174;&#23545;&#27969;-&#25193;&#25955;&#26041;&#31243; (ADE) &#23548;&#20986;&#30340;&#19968;&#20010;&#36870;&#38382;&#39064;&#65292;&#23450;&#37327;&#20998;&#26512;ECS&#20869;&#30340;&#20998;&#23376;&#20256;&#36755;&#12290;PINN&#20026;ADE&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#26080;&#38656;&#22797;&#26434;&#30340;&#25968;&#23398;&#20844;&#24335;&#25110;&#32593;&#26684;&#35774;&#32622;&#12290;&#27492;&#22806;&#65292;PINN&#30340;&#20248;&#21270;&#21151;&#33021;&#21487;&#33258;&#21160;&#35745;&#31639;&#20915;&#23450;&#38271;&#26399;&#20998;&#23376;&#20256;&#36755;&#30340;&#25193;&#25955;&#31995;&#25968;&#21644;&#30001;&#23545;&#27969;&#39537;&#21160;&#30340;&#20998;&#23376;&#36895;&#24230;&#12290;&#22240;&#27492;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20801;&#35768;&#36827;&#34892;&#23450;&#37327;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The brain extracellular space (ECS), an irregular, extremely tortuous nanoscale space located between cells or between cells and blood vessels, is crucial for nerve cell survival. It plays a pivotal role in high-level brain functions such as memory, emotion, and sensation. However, the specific form of molecular transport within the ECS remain elusive. To address this challenge, this paper proposes a novel approach to quantitatively analyze the molecular transport within the ECS by solving an inverse problem derived from the advection-diffusion equation (ADE) using a physics-informed neural network (PINN). PINN provides a streamlined solution to the ADE without the need for intricate mathematical formulations or grid settings. Additionally, the optimization of PINN facilitates the automatic computation of the diffusion coefficient governing long-term molecule transport and the velocity of molecules driven by advection. Consequently, the proposed method allows for the quantitative analy
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20132;&#36890;&#39044;&#27979;&#20013;&#24212;&#29992;&#31354;&#38388;-&#26102;&#38388;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#12290;&#30693;&#35782;&#33976;&#39311;&#30340;&#24605;&#24819;&#33021;&#22815;&#23454;&#29616;&#22312;&#20943;&#23569;&#21442;&#25968;&#21644;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#25552;&#39640;&#25191;&#34892;&#25928;&#29575;&#12290;&#36890;&#36807;&#24341;&#20837;&#25945;&#24072;&#32593;&#32476;&#30340;&#31354;&#38388;-&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20351;&#23398;&#29983;&#32593;&#32476;&#23398;&#20064;&#21040;&#22797;&#26434;&#30340;&#20132;&#36890;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2401.11798</link><description>&lt;p&gt;
&#31354;&#38388;-&#26102;&#38388;&#22270;&#21367;&#31215;&#32593;&#32476;&#22312;&#20132;&#36890;&#39044;&#27979;&#19978;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distillation on Spatial-Temporal Graph Convolutional Network for Traffic Prediction. (arXiv:2401.11798v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20132;&#36890;&#39044;&#27979;&#20013;&#24212;&#29992;&#31354;&#38388;-&#26102;&#38388;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#12290;&#30693;&#35782;&#33976;&#39311;&#30340;&#24605;&#24819;&#33021;&#22815;&#23454;&#29616;&#22312;&#20943;&#23569;&#21442;&#25968;&#21644;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#25552;&#39640;&#25191;&#34892;&#25928;&#29575;&#12290;&#36890;&#36807;&#24341;&#20837;&#25945;&#24072;&#32593;&#32476;&#30340;&#31354;&#38388;-&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20351;&#23398;&#29983;&#32593;&#32476;&#23398;&#20064;&#21040;&#22797;&#26434;&#30340;&#20132;&#36890;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#23454;&#26102;&#20132;&#36890;&#39044;&#27979;&#23545;&#20943;&#23569;&#20132;&#36890;&#26102;&#38388;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#39044;&#27979;&#20132;&#36890;&#29366;&#20917;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#31354;&#38388;-&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;ST-GNN&#65289;&#23558;&#23454;&#26102;&#20132;&#36890;&#25968;&#25454;&#24314;&#27169;&#20026;&#26102;&#38388;&#22270;&#12290;&#23613;&#31649;ST-GNN&#20855;&#26377;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#20026;&#23454;&#38469;&#20132;&#36890;&#25968;&#25454;&#36827;&#34892;&#39640;&#25928;&#23454;&#26102;&#39044;&#27979;&#26102;&#32463;&#24120;&#38754;&#20020;&#25361;&#25112;&#12290;&#37492;&#20110;&#23454;&#26102;&#25968;&#25454;&#21160;&#24577;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#37319;&#29992;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#25552;&#39640;ST-GNN&#22312;&#20132;&#36890;&#39044;&#27979;&#20013;&#30340;&#25191;&#34892;&#26102;&#38388;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25104;&#26412;&#20989;&#25968;&#65292;&#26088;&#22312;&#20351;&#29992;&#22797;&#26434;&#32593;&#32476;&#65288;&#25945;&#24072;&#65289;&#30340;&#33976;&#39311;&#25968;&#25454;&#26469;&#35757;&#32451;&#20855;&#26377;&#36739;&#23569;&#21442;&#25968;&#30340;&#32593;&#32476;&#65288;&#23398;&#29983;&#65289;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#20934;&#30830;&#24615;&#25509;&#36817;&#25945;&#24072;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#65292;&#23558;&#25945;&#24072;&#32593;&#32476;&#30340;&#31354;&#38388;-&#26102;&#38388;&#30456;&#20851;&#24615;&#34701;&#20837;&#23398;&#29983;&#32593;&#32476;&#65292;&#20351;&#23398;&#29983;&#33021;&#22815;&#23398;&#20064;&#21040;&#25945;&#24072;&#24863;&#30693;&#30340;&#22797;&#26434;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#38754;&#20020;&#19968;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient real-time traffic prediction is crucial for reducing transportation time. To predict traffic conditions, we employ a spatio-temporal graph neural network (ST-GNN) to model our real-time traffic data as temporal graphs. Despite its capabilities, it often encounters challenges in delivering efficient real-time predictions for real-world traffic data. Recognizing the significance of timely prediction due to the dynamic nature of real-time data, we employ knowledge distillation (KD) as a solution to enhance the execution time of ST-GNNs for traffic prediction. In this paper, We introduce a cost function designed to train a network with fewer parameters (the student) using distilled data from a complex network (the teacher) while maintaining its accuracy close to that of the teacher. We use knowledge distillation, incorporating spatial-temporal correlations from the teacher network to enable the student to learn the complex patterns perceived by the teacher. However, a challenge a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23433;&#20840;&#19988;&#24191;&#20041;&#30340;&#31471;&#21040;&#31471;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479; (SGADS)&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#31034;&#33539;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#20302;&#23433;&#20840;&#24615;&#12289;&#27867;&#21270;&#33021;&#21147;&#24046;&#21644;&#37319;&#26679;&#25928;&#29575;&#20302;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#21464;&#20998;&#25512;&#29702;&#21644;&#24402;&#19968;&#21270;&#27969;&#20197;&#20934;&#30830;&#39044;&#27979;&#39550;&#39542;&#36712;&#36857;&#65292;&#24182;&#25552;&#20986;&#20102;&#40065;&#26834;&#24615;&#23433;&#20840;&#32422;&#26463;&#30340;&#21046;&#23450;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.11792</link><description>&lt;p&gt;
&#23433;&#20840;&#19988;&#24191;&#20041;&#30340;&#31471;&#21040;&#31471;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#65306;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#21644;&#31034;&#33539;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Safe and Generalized end-to-end Autonomous Driving System with Reinforcement Learning and Demonstrations. (arXiv:2401.11792v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23433;&#20840;&#19988;&#24191;&#20041;&#30340;&#31471;&#21040;&#31471;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479; (SGADS)&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#31034;&#33539;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#20302;&#23433;&#20840;&#24615;&#12289;&#27867;&#21270;&#33021;&#21147;&#24046;&#21644;&#37319;&#26679;&#25928;&#29575;&#20302;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#21464;&#20998;&#25512;&#29702;&#21644;&#24402;&#19968;&#21270;&#27969;&#20197;&#20934;&#30830;&#39044;&#27979;&#39550;&#39542;&#36712;&#36857;&#65292;&#24182;&#25552;&#20986;&#20102;&#40065;&#26834;&#24615;&#23433;&#20840;&#32422;&#26463;&#30340;&#21046;&#23450;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#26234;&#33021;&#39550;&#39542;&#31995;&#32479;&#24212;&#35813;&#33021;&#22815;&#26681;&#25454;&#24403;&#21069;&#29615;&#22659;&#21644;&#36710;&#36742;&#29366;&#24577;&#21160;&#24577;&#21046;&#23450;&#36866;&#24403;&#30340;&#39550;&#39542;&#31574;&#30053;&#65292;&#21516;&#26102;&#30830;&#20445;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#30340;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#23433;&#20840;&#24615;&#20302;&#12289;&#27867;&#21270;&#33021;&#21147;&#24046;&#21644;&#37319;&#26679;&#25928;&#29575;&#20302;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#26080;&#27861;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#30340;&#39550;&#39542;&#36712;&#36857;&#65292;&#32780;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#30340;&#39550;&#39542;&#36712;&#36857;&#26159;&#20570;&#20986;&#26368;&#20248;&#20915;&#31574;&#30340;&#21069;&#25552;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22797;&#26434;&#32780;&#22810;&#26679;&#22330;&#26223;&#19979;&#30340;&#23433;&#20840;&#19988;&#24191;&#20041;&#30340;&#31471;&#21040;&#31471;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479; (SGADS)&#12290;&#25105;&#20204;&#30340;SGADS&#19982;&#21464;&#20998;&#25512;&#29702;&#21644;&#24402;&#19968;&#21270;&#27969;&#32467;&#21512;&#65292;&#20351;&#26234;&#33021;&#36710;&#36742;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#30340;&#39550;&#39542;&#36712;&#36857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#40065;&#26834;&#24615;&#23433;&#20840;&#32422;&#26463;&#30340;&#21046;&#23450;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#24378;&#21270;&#23398;&#20064;&#19982;&#31034;&#33539;&#30456;&#32467;&#21512;&#36827;&#34892;&#22686;&#24378;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
An intelligent driving system should be capable of dynamically formulating appropriate driving strategies based on the current environment and vehicle status, while ensuring the security and reliability of the system. However, existing methods based on reinforcement learning and imitation learning suffer from low safety, poor generalization, and inefficient sampling. Additionally, they cannot accurately predict future driving trajectories, and the accurate prediction of future driving trajectories is a precondition for making optimal decisions. To solve these problems, in this paper, we introduce a Safe and Generalized end-to-end Autonomous Driving System (SGADS) for complex and various scenarios. Our SGADS incorporates variational inference with normalizing flows, enabling the intelligent vehicle to accurately predict future driving trajectories. Moreover, we propose the formulation of robust safety constraints. Furthermore, we combine reinforcement learning with demonstrations to aug
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21307;&#23398;&#20195;&#30721;&#20013;&#24515;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;EHR&#24314;&#27169;&#39044;&#27979;&#19979;&#27425;&#23601;&#35786;&#35786;&#26029;&#65292;&#24182;&#36890;&#36807;&#20998;&#23618;&#27491;&#21017;&#21270;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.11648</link><description>&lt;p&gt;
&#36890;&#36807;&#20855;&#26377;&#20998;&#23618;&#27491;&#21017;&#21270;&#30340;&#21307;&#23398;&#20195;&#30721;&#20013;&#24515;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;EHR&#24314;&#27169;&#39044;&#27979;&#19979;&#27425;&#23601;&#35786;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Next Visit Diagnosis Prediction via Medical Code-Centric Multimodal Contrastive EHR Modelling with Hierarchical Regularisation. (arXiv:2401.11648v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11648
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21307;&#23398;&#20195;&#30721;&#20013;&#24515;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;EHR&#24314;&#27169;&#39044;&#27979;&#19979;&#27425;&#23601;&#35786;&#35786;&#26029;&#65292;&#24182;&#36890;&#36807;&#20998;&#23618;&#27491;&#21017;&#21270;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#65292;&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#39044;&#27979;&#19979;&#27425;&#23601;&#35786;&#30340;&#35786;&#26029;&#26159;&#19968;&#39033;&#24517;&#35201;&#30340;&#20219;&#21153;&#65292;&#23545;&#20110;&#21046;&#23450;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#21644;&#24739;&#32773;&#30340;&#20027;&#21160;&#26410;&#26469;&#35745;&#21010;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;&#35768;&#22810;&#30740;&#31350;&#24182;&#27809;&#26377;&#20805;&#20998;&#35299;&#20915;EHR&#25968;&#25454;&#22266;&#26377;&#30340;&#24322;&#26500;&#21644;&#20998;&#23618;&#29305;&#24449;&#65292;&#24517;&#28982;&#23548;&#33268;&#27425;&#20248;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NECHO&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#21307;&#23398;&#20195;&#30721;&#20013;&#24515;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;EHR&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#20998;&#23618;&#27491;&#21017;&#21270;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#23450;&#21046;&#30340;&#32593;&#32476;&#35774;&#35745;&#21644;&#19968;&#23545;&#21452;&#27169;&#24577;&#23545;&#27604;&#25439;&#22833;&#34701;&#21512;&#28085;&#30422;&#21307;&#23398;&#20195;&#30721;&#12289;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#21644;&#20020;&#24202;&#31508;&#35760;&#30340;&#22810;&#26041;&#38754;&#20449;&#24687;&#65292;&#25152;&#26377;&#36825;&#20123;&#37117;&#22260;&#32469;&#30528;&#21307;&#23398;&#20195;&#30721;&#34920;&#29616;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#21307;&#23398;&#26412;&#20307;&#20013;&#30340;&#29238;&#32423;&#20449;&#24687;&#26469;&#35268;&#33539;&#29305;&#23450;&#27169;&#24577;&#30340;&#32534;&#30721;&#22120;&#65292;&#20197;&#23398;&#20064;EHR&#25968;&#25454;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;&#23545;MIMIC-III&#25968;&#25454;&#36827;&#34892;&#30340;&#19968;&#31995;&#21015;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting next visit diagnosis using Electronic Health Records (EHR) is an essential task in healthcare, critical for devising proactive future plans for both healthcare providers and patients. Nonetheless, many preceding studies have not sufficiently addressed the heterogeneous and hierarchical characteristics inherent in EHR data, inevitably leading to sub-optimal performance. To this end, we propose NECHO, a novel medical code-centric multimodal contrastive EHR learning framework with hierarchical regularisation. First, we integrate multifaceted information encompassing medical codes, demographics, and clinical notes using a tailored network design and a pair of bimodal contrastive losses, all of which pivot around a medical code representation. We also regularise modality-specific encoders using a parental level information in medical ontology to learn hierarchical structure of EHR data. A series of experiments on MIMIC-III data demonstrates effectiveness of our approach.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#20020;&#24202;&#23454;&#36341;&#25351;&#21335;&#32435;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#22686;&#24378;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#30340;&#26041;&#27861;&#12290;&#20182;&#20204;&#24320;&#21457;&#20102;&#19977;&#31181;&#26041;&#27861;&#65292;&#24182;&#23545;&#22235;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#22312;COVID-19&#38376;&#35786;&#27835;&#30103;&#26041;&#38754;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.11120</link><description>&lt;p&gt;
&#23558;&#20020;&#24202;&#23454;&#36341;&#25351;&#21335;&#32435;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#22686;&#24378;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Enhancing Large Language Models for Clinical Decision Support by Incorporating Clinical Practice Guidelines. (arXiv:2401.11120v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11120
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#20020;&#24202;&#23454;&#36341;&#25351;&#21335;&#32435;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#22686;&#24378;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#30340;&#26041;&#27861;&#12290;&#20182;&#20204;&#24320;&#21457;&#20102;&#19977;&#31181;&#26041;&#27861;&#65292;&#24182;&#23545;&#22235;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#22312;COVID-19&#38376;&#35786;&#27835;&#30103;&#26041;&#38754;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#25645;&#37197;&#20020;&#24202;&#23454;&#36341;&#25351;&#21335;&#65288;CPGs&#65289;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#65288;CDS&#65289;&#12290;&#28982;&#32780;&#65292;&#23558;CPGs&#32435;&#20837;LLMs&#30340;&#26041;&#27861;&#24182;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#23558;CPGs&#32435;&#20837;LLMs&#65306;&#20108;&#20803;&#20915;&#31574;&#26641;&#65288;BDT&#65289;&#65292;&#31243;&#24207;&#36741;&#21161;&#22270;&#26500;&#24314;&#65288;PAGC&#65289;&#65292;&#20197;&#21450;&#24605;&#32500;&#38142;&#23569;&#26679;&#26412;&#25552;&#31034;&#65288;CoT-FSP&#65289;&#12290;&#20026;&#35780;&#20272;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#32452;&#21512;&#25104;&#24739;&#32773;&#25551;&#36848;&#65292;&#24182;&#23545;&#30001;&#22235;&#20010;LLMs&#29983;&#25104;&#30340;&#21709;&#24212;&#36827;&#34892;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#65306;GPT-4&#65292;GPT-3.5 Turbo&#65292;LLaMA&#21644;PaLM 2&#12290;&#38646;&#26679;&#26412;&#25552;&#31034;&#65288;ZSP&#65289;&#34987;&#29992;&#20316;&#22522;&#32447;&#26041;&#27861;&#12290;&#25105;&#20204;&#20197;COVID-19&#38376;&#35786;&#27835;&#30103;&#30340;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#22235;&#20010;LLMs&#22312;&#22686;&#21152;&#20102;CPGs&#21518;&#30456;&#23545;&#20110;&#22522;&#32447;ZSP&#23637;&#29616;&#20102;&#25552;&#39640;&#30340;&#24615;&#33021;&#12290;BDT&#22312;&#33258;&#21160;&#35780;&#20272;&#20013;&#34920;&#29616;&#20248;&#20110;CoT-FSP&#21644;PAGC&#12290;&#25152;&#26377;&#25552;&#20986;&#30340;&#26041;&#27861;&#37117;&#34920;&#29616;&#20986;&#20102;&#36739;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background Large Language Models (LLMs), enhanced with Clinical Practice Guidelines (CPGs), can significantly improve Clinical Decision Support (CDS). However, methods for incorporating CPGs into LLMs are not well studied. Methods We develop three distinct methods for incorporating CPGs into LLMs: Binary Decision Tree (BDT), Program-Aided Graph Construction (PAGC), and Chain-of-Thought-Few-Shot Prompting (CoT-FSP). To evaluate the effectiveness of the proposed methods, we create a set of synthetic patient descriptions and conduct both automatic and human evaluation of the responses generated by four LLMs: GPT-4, GPT-3.5 Turbo, LLaMA, and PaLM 2. Zero-Shot Prompting (ZSP) was used as the baseline method. We focus on CDS for COVID-19 outpatient treatment as the case study. Results All four LLMs exhibit improved performance when enhanced with CPGs compared to the baseline ZSP. BDT outperformed both CoT-FSP and PAGC in automatic evaluation. All of the proposed methods demonstrated high per
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#26816;&#27979;&#26032;&#20986;&#29616;&#30340;&#32534;&#30721;&#24694;&#24847;&#26415;&#35821;&#65292;&#20026;&#26497;&#31471;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#21453;&#29369;&#22826;&#24694;&#24847;&#35328;&#35770;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.10841</link><description>&lt;p&gt;
&#20351;&#29992;LLMs&#21457;&#29616;&#26497;&#31471;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#32534;&#30721;&#21453;&#29369;&#22826;&#24694;&#24847;&#35328;&#35770;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
Using LLMs to discover emerging coded antisemitic hate-speech emergence in extremist social media. (arXiv:2401.10841v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10841
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#26816;&#27979;&#26032;&#20986;&#29616;&#30340;&#32534;&#30721;&#24694;&#24847;&#26415;&#35821;&#65292;&#20026;&#26497;&#31471;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#21453;&#29369;&#22826;&#24694;&#24847;&#35328;&#35770;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#20167;&#24680;&#35328;&#35770;&#30340;&#34067;&#24310;&#32473;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#24102;&#26469;&#20102;&#19968;&#20010;&#38590;&#39064;&#12290;&#19968;&#20010;&#29305;&#27530;&#30340;&#25361;&#25112;&#19982;&#20351;&#29992;&#32534;&#30721;&#35821;&#35328;&#30340;&#32676;&#20307;&#26377;&#20851;&#65292;&#36825;&#20123;&#32676;&#20307;&#26082;&#24819;&#20026;&#20854;&#29992;&#25143;&#21019;&#36896;&#24402;&#23646;&#24863;&#65292;&#21448;&#24819;&#22238;&#36991;&#26816;&#27979;&#12290;&#32534;&#30721;&#35821;&#35328;&#21457;&#23637;&#36805;&#36895;&#65292;&#24182;&#19988;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#20351;&#29992;&#26041;&#24335;&#19981;&#21516;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#27979;&#26032;&#20986;&#29616;&#30340;&#32534;&#30721;&#24694;&#24847;&#26415;&#35821;&#30340;&#26041;&#27861;&#35770;&#12290;&#35813;&#26041;&#27861;&#22312;&#22312;&#32447;&#21453;&#29369;&#22826;&#35328;&#35770;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#20174;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#25235;&#21462;&#30340;&#24086;&#23376;&#65292;&#36890;&#24120;&#26159;&#26497;&#31471;&#20027;&#20041;&#29992;&#25143;&#20351;&#29992;&#30340;&#12290;&#24086;&#23376;&#26159;&#20351;&#29992;&#19982;&#20197;&#21069;&#24050;&#30693;&#30340;&#38024;&#23545;&#29369;&#22826;&#20154;&#30340;&#20167;&#24680;&#35328;&#35770;&#30456;&#20851;&#30340;&#31181;&#23376;&#34920;&#36798;&#24335;&#36827;&#34892;&#25235;&#21462;&#30340;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#36890;&#36807;&#35782;&#21035;&#27599;&#20010;&#24086;&#23376;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#34920;&#36798;&#24335;&#65292;&#24182;&#35745;&#31639;&#23427;&#20204;&#22312;&#25972;&#20010;&#35821;&#26009;&#24211;&#20013;&#30340;&#39057;&#29575;&#12290;&#36807;&#28388;&#25481;&#35821;&#27861;&#19981;&#19968;&#33268;&#30340;&#34920;&#36798;&#24335;&#21644;&#20043;&#21069;&#36935;&#21040;&#36807;&#30340;&#34920;&#36798;&#24335;&#65292;&#20197;&#20415;&#20851;&#27880;&#26032;&#20986;&#29616;&#30340;&#33391;&#22909;&#24418;&#24335;&#30340;&#26415;&#35821;&#12290;&#28982;&#21518;&#36827;&#34892;&#20102;&#35821;&#20041;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online hate speech proliferation has created a difficult problem for social media platforms. A particular challenge relates to the use of coded language by groups interested in both creating a sense of belonging for its users and evading detection. Coded language evolves quickly and its use varies over time. This paper proposes a methodology for detecting emerging coded hate-laden terminology. The methodology is tested in the context of online antisemitic discourse. The approach considers posts scraped from social media platforms, often used by extremist users. The posts are scraped using seed expressions related to previously known discourse of hatred towards Jews. The method begins by identifying the expressions most representative of each post and calculating their frequency in the whole corpus. It filters out grammatically incoherent expressions as well as previously encountered ones so as to focus on emergent well-formed terminology. This is followed by an assessment of semantic s
&lt;/p&gt;</description></item><item><title>DiConStruct&#26159;&#19968;&#31181;&#22522;&#20110;&#40657;&#30418;&#27169;&#22411;&#30340;&#22240;&#26524;&#27010;&#24565;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#24314;&#32467;&#26500;&#24615;&#22240;&#26524;&#27169;&#22411;&#21644;&#27010;&#24565;&#24402;&#22240;&#26041;&#24335;&#25552;&#20379;&#26356;&#20855;&#21487;&#35299;&#37322;&#24615;&#30340;&#23616;&#37096;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2401.08534</link><description>&lt;p&gt;
DiConStruct: &#22522;&#20110;&#40657;&#30418;&#31934;&#21326;&#30340;&#22240;&#26524;&#27010;&#24565;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
DiConStruct: Causal Concept-based Explanations through Black-Box Distillation. (arXiv:2401.08534v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08534
&lt;/p&gt;
&lt;p&gt;
DiConStruct&#26159;&#19968;&#31181;&#22522;&#20110;&#40657;&#30418;&#27169;&#22411;&#30340;&#22240;&#26524;&#27010;&#24565;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#24314;&#32467;&#26500;&#24615;&#22240;&#26524;&#27169;&#22411;&#21644;&#27010;&#24565;&#24402;&#22240;&#26041;&#24335;&#25552;&#20379;&#26356;&#20855;&#21487;&#35299;&#37322;&#24615;&#30340;&#23616;&#37096;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#22312;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#31995;&#32479;&#20013;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#35299;&#37322;&#24212;&#35813;&#20351;&#29992;&#20154;&#21487;&#35299;&#37322;&#30340;&#35821;&#20041;&#27010;&#24565;&#26469;&#34920;&#36798;&#12290;&#27492;&#22806;&#65292;&#35299;&#37322;&#22120;&#24212;&#35813;&#25429;&#25417;&#36825;&#20123;&#27010;&#24565;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#20197;&#20415;&#23545;&#35299;&#37322;&#36827;&#34892;&#25512;&#29702;&#12290;&#26368;&#21518;&#65292;&#35299;&#37322;&#26041;&#27861;&#24212;&#35813;&#39640;&#25928;&#65292;&#24182;&#19981;&#25439;&#23475;&#39044;&#27979;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;AI&#35299;&#37322;&#24615;&#21462;&#24471;&#20102;&#24555;&#36895;&#36827;&#23637;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#33267;&#20170;&#27809;&#26377;&#19968;&#31181;&#26041;&#27861;&#28385;&#36275;&#36825;&#19977;&#20010;&#26465;&#20214;&#12290;&#20107;&#23454;&#19978;&#65292;&#20027;&#27969;&#30340;&#23616;&#37096;&#27010;&#24565;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#19981;&#20135;&#29983;&#22240;&#26524;&#35299;&#37322;&#65292;&#24182;&#22312;&#35299;&#37322;&#24615;&#21644;&#39044;&#27979;&#24615;&#33021;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DiConStruct&#65292;&#19968;&#31181;&#26082;&#22522;&#20110;&#27010;&#24565;&#21448;&#20855;&#26377;&#22240;&#26524;&#24615;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#32467;&#26500;&#24615;&#22240;&#26524;&#27169;&#22411;&#21644;&#27010;&#24565;&#24402;&#22240;&#26041;&#24335;&#21019;&#24314;&#26356;&#20855;&#21487;&#35299;&#37322;&#24615;&#30340;&#23616;&#37096;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#35299;&#37322;&#22120;&#20316;&#20026;&#19968;&#20010;&#31934;&#21326;&#27169;&#22411;&#36866;&#29992;&#20110;&#20219;&#20309;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model interpretability plays a central role in human-AI decision-making systems. Ideally, explanations should be expressed using human-interpretable semantic concepts. Moreover, the causal relations between these concepts should be captured by the explainer to allow for reasoning about the explanations. Lastly, explanation methods should be efficient and not compromise the performance of the predictive task. Despite the rapid advances in AI explainability in recent years, as far as we know to date, no method fulfills these three properties. Indeed, mainstream methods for local concept explainability do not produce causal explanations and incur a trade-off between explainability and prediction performance. We present DiConStruct, an explanation method that is both concept-based and causal, with the goal of creating more interpretable local explanations in the form of structural causal models and concept attributions. Our explainer works as a distillation model to any black-box machine l
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#24773;&#22659;&#21270;&#30340;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#20316;&#20026;&#23398;&#29983;&#23398;&#20064;&#25512;&#33616;&#30340;&#35299;&#37322;&#24037;&#20855;&#21644;&#25351;&#23548;&#12290;&#36890;&#36807;&#23450;&#20041;&#19978;&#19979;&#25991;&#24182;&#21033;&#29992;&#20154;&#24037;&#31574;&#21010;&#30340;&#20449;&#24687;&#28304;&#26469;&#35843;&#25511;LLM&#30340;&#29983;&#25104;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#33021;&#22312;&#19982;&#23398;&#29983;&#23545;&#35805;&#20013;&#25552;&#20379;&#35299;&#37322;&#21644;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2401.08517</link><description>&lt;p&gt;
&#25903;&#25345;&#23398;&#29983;&#20915;&#31574;&#30340;&#23398;&#20064;&#25512;&#33616;&#65306;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#24773;&#22659;&#21270;&#30340;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#23454;&#29616;&#23545;&#35805;&#35299;&#37322;&#21644;&#25351;&#23548;
&lt;/p&gt;
&lt;p&gt;
Supporting Student Decisions on Learning Recommendations: An LLM-Based Chatbot with Knowledge Graph Contextualization for Conversational Explainability and Mentoring. (arXiv:2401.08517v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08517
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#24773;&#22659;&#21270;&#30340;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#20316;&#20026;&#23398;&#29983;&#23398;&#20064;&#25512;&#33616;&#30340;&#35299;&#37322;&#24037;&#20855;&#21644;&#25351;&#23548;&#12290;&#36890;&#36807;&#23450;&#20041;&#19978;&#19979;&#25991;&#24182;&#21033;&#29992;&#20154;&#24037;&#31574;&#21010;&#30340;&#20449;&#24687;&#28304;&#26469;&#35843;&#25511;LLM&#30340;&#29983;&#25104;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#33021;&#22312;&#19982;&#23398;&#29983;&#23545;&#35805;&#20013;&#25552;&#20379;&#35299;&#37322;&#21644;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#29983;&#23545;&#23398;&#20064;&#25512;&#33616;&#30340;&#20915;&#31574;&#19982;&#20854;&#29702;&#35299;&#25512;&#33616;&#21407;&#22240;&#30340;&#33021;&#21147;&#26159;&#19981;&#21487;&#20998;&#21106;&#30340;&#65307;&#20182;&#20204;&#33021;&#21542;&#26681;&#25454;&#36825;&#31181;&#29702;&#35299;&#36827;&#34892;&#20462;&#25913;&#12290;&#22312;&#21508;&#31181;&#35299;&#37322;&#24615;&#26041;&#27861;&#20013;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#20855;&#26377;&#19982;&#21516;&#34892;&#25110;&#23548;&#24072;&#35752;&#35770;&#31867;&#20284;&#30340;&#28508;&#21147;&#26469;&#19982;&#23398;&#29983;&#36827;&#34892;&#23545;&#35805;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#33021;&#21147;&#20173;&#28982;&#19981;&#36275;&#20197;&#21462;&#20195;&#20154;&#31867;&#23548;&#24072;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#32842;&#22825;&#26426;&#22120;&#20154;&#20316;&#20026;&#23545;&#35805;&#30340;&#20013;&#20171;&#21644;&#35299;&#37322;&#30340;&#26377;&#38480;&#21644;&#21463;&#25511;&#29983;&#25104;&#30340;&#26469;&#28304;&#65292;&#20197;&#21033;&#29992;LLM&#30340;&#28508;&#21147;&#30340;&#21516;&#26102;&#20943;&#23569;&#20854;&#28508;&#22312;&#39118;&#38505;&#12290;&#25152;&#25552;&#20986;&#30340;&#22522;&#20110;LLM&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#25903;&#25345;&#23398;&#29983;&#29702;&#35299;&#23398;&#20064;&#36335;&#24452;&#25512;&#33616;&#12290;&#25105;&#20204;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#20316;&#20026;&#20154;&#24037;&#31574;&#21010;&#30340;&#20449;&#24687;&#28304;&#65292;&#36890;&#36807;&#23450;&#20041;&#20854;&#25552;&#31034;&#30340;&#19978;&#19979;&#25991;&#26469;&#35843;&#25511;LLM&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Student commitment towards a learning recommendation is not separable from their understanding of the reasons it was recommended to them; and their ability to modify it based on that understanding. Among explainability approaches, chatbots offer the potential to engage the student in a conversation, similar to a discussion with a peer or a mentor. The capabilities of chatbots, however, are still not sufficient to replace a human mentor, despite the advancements of generative AI (GenAI) and large language models (LLM). Therefore, we propose an approach to utilize chatbots as mediators of the conversation and sources of limited and controlled generation of explanations, to harvest the potential of LLMs while reducing their potential risks at the same time. The proposed LLM-based chatbot supports students in understanding learning-paths recommendations. We use a knowledge graph (KG) as a human-curated source of information, to regulate the LLM's output through defining its prompt's contex
&lt;/p&gt;</description></item><item><title>GPT-4 Vision&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#20855;&#26377;&#19987;&#23478;&#32423;&#20934;&#30830;&#24230;&#65292;&#20294;&#22312;&#22270;&#20687;&#29702;&#35299;&#26041;&#38754;&#23384;&#22312;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2401.08396</link><description>&lt;p&gt;
GPT-4 Vision&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#19987;&#23478;&#32423;&#20934;&#30830;&#24230;&#32972;&#21518;&#30340;&#38544;&#34255;&#32570;&#38519;
&lt;/p&gt;
&lt;p&gt;
Hidden Flaws Behind Expert-Level Accuracy of GPT-4 Vision in Medicine. (arXiv:2401.08396v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08396
&lt;/p&gt;
&lt;p&gt;
GPT-4 Vision&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#20855;&#26377;&#19987;&#23478;&#32423;&#20934;&#30830;&#24230;&#65292;&#20294;&#22312;&#22270;&#20687;&#29702;&#35299;&#26041;&#38754;&#23384;&#22312;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20855;&#26377;Vision&#21151;&#33021;&#30340;GPT-4&#22312;&#21307;&#23398;&#25361;&#25112;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#20154;&#31867;&#21307;&#29983;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35780;&#20272;&#20027;&#35201;&#20851;&#27880;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#20934;&#30830;&#24230;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;GPT-4V&#22312;&#35299;&#20915;&#26032;&#33521;&#26684;&#20848;&#21307;&#23398;&#26434;&#24535;&#22270;&#20687;&#25361;&#25112;&#20013;&#30340;&#22270;&#20687;&#29702;&#35299;&#12289;&#21307;&#23398;&#30693;&#35782;&#22238;&#24518;&#21644;&#36880;&#27493;&#22810;&#27169;&#24577;&#25512;&#29702;&#30340;&#21407;&#29702;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#25193;&#23637;&#20102;&#24403;&#21069;&#30340;&#30740;&#31350;&#33539;&#22260;&#12290;&#35780;&#20272;&#32467;&#26524;&#35777;&#23454;&#65292;GPT-4V&#22312;&#22810;&#39033;&#36873;&#25321;&#20934;&#30830;&#24230;&#19978;&#20248;&#20110;&#20154;&#31867;&#21307;&#29983;&#65288;88.0% vs. 77.0%&#65292;p=0.034&#65289;&#12290;GPT-4V&#22312;&#21307;&#29983;&#22238;&#31572;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#34920;&#29616;&#20986;&#36229;&#36807;80%&#30340;&#20934;&#30830;&#24230;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;GPT-4V&#22312;&#26368;&#32456;&#20570;&#20986;&#27491;&#30830;&#36873;&#25321;&#30340;&#24773;&#20917;&#19979;&#65292;&#32463;&#24120;&#25552;&#20379;&#26377;&#32570;&#38519;&#30340;&#25512;&#29702;&#65288;27.3%&#65289;&#65292;&#20854;&#20013;&#26368;&#31361;&#20986;&#30340;&#26159;&#22270;&#20687;&#29702;&#35299;&#65288;21.6%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies indicate that Generative Pre-trained Transformer 4 with Vision (GPT-4V) outperforms human physicians in medical challenge tasks. However, these evaluations primarily focused on the accuracy of multi-choice questions alone. Our study extends the current scope by conducting a comprehensive analysis of GPT-4V's rationales of image comprehension, recall of medical knowledge, and step-by-step multimodal reasoning when solving New England Journal of Medicine (NEJM) Image Challenges - an imaging quiz designed to test the knowledge and diagnostic capabilities of medical professionals. Evaluation results confirmed that GPT-4V outperforms human physicians regarding multi-choice accuracy (88.0% vs. 77.0%, p=0.034). GPT-4V also performs well in cases where physicians incorrectly answer, with over 80% accuracy. However, we discovered that GPT-4V frequently presents flawed rationales in cases where it makes the correct final choices (27.3%), most prominent in image comprehension (21.6
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#26102;&#24577;&#36923;&#36753;&#25512;&#29702;&#26469;&#23454;&#29616;&#20010;&#24615;&#21270;&#30340;&#24418;&#24335;&#36923;&#36753;&#21551;&#29992;&#30340;&#32852;&#37030;&#23398;&#20064;&#65292;&#20197;&#35299;&#20915;&#24322;&#36136;&#24615;&#23458;&#25143;&#35774;&#22791;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#32858;&#21512;&#32676;&#38598;&#30340;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2401.07448</link><description>&lt;p&gt;
&#20511;&#21161;&#23646;&#24615;&#25512;&#29702;&#23454;&#29616;&#20010;&#24615;&#21270;&#30340;&#24418;&#24335;&#36923;&#36753;&#21551;&#29992;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Formal Logic Enabled Personalized Federated Learning Through Property Inference. (arXiv:2401.07448v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#26102;&#24577;&#36923;&#36753;&#25512;&#29702;&#26469;&#23454;&#29616;&#20010;&#24615;&#21270;&#30340;&#24418;&#24335;&#36923;&#36753;&#21551;&#29992;&#30340;&#32852;&#37030;&#23398;&#20064;&#65292;&#20197;&#35299;&#20915;&#24322;&#36136;&#24615;&#23458;&#25143;&#35774;&#22791;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#32858;&#21512;&#32676;&#38598;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#23545;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#19981;&#26029;&#30740;&#31350;&#36827;&#23637;&#26497;&#22823;&#22320;&#20419;&#36827;&#20102;&#20998;&#24067;&#24335;&#21327;&#20316;&#24212;&#29992;&#30340;&#21457;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#29289;&#32852;&#32593;&#20154;&#24037;&#26234;&#33021;&#65288;AIoT&#65289;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30740;&#31350;&#39046;&#22495;&#20013;&#19968;&#20010;&#32570;&#22833;&#30340;&#20851;&#38190;&#26041;&#38754;&#26159;&#20351;&#20855;&#26377;&#31526;&#21495;&#25512;&#29702;&#33021;&#21147;&#30340;&#25968;&#25454;&#39537;&#21160;&#23458;&#25143;&#27169;&#22411;&#33021;&#22815;&#24037;&#20316;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#21442;&#19982;&#32852;&#37030;&#23398;&#20064;&#30340;&#23458;&#25143;&#35774;&#22791;&#30340;&#24322;&#36136;&#24615;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#22240;&#20026;&#27599;&#20010;&#23458;&#25143;&#37117;&#20855;&#26377;&#29420;&#29305;&#30340;&#36923;&#36753;&#25512;&#29702;&#29305;&#24615;&#12290;&#24573;&#35270;&#36825;&#20123;&#35774;&#22791;&#29305;&#23450;&#30340;&#35268;&#33539;&#21487;&#33021;&#20250;&#23548;&#33268;&#23458;&#25143;&#31471;&#39044;&#27979;&#20013;&#36951;&#28431;&#20851;&#38190;&#23646;&#24615;&#65292;&#20174;&#32780;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#21033;&#29992;&#26102;&#24577;&#36923;&#36753;&#25512;&#29702;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#36890;&#36807;&#20026;&#27599;&#20010;FL&#23458;&#25143;&#31471;&#21152;&#20837;&#26426;&#26800;&#29983;&#25104;&#30340;&#36923;&#36753;&#34920;&#36798;&#24335;&#26469;&#22686;&#24378;&#35757;&#32451;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32858;&#21512;&#32676;&#38598;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in federated learning (FL) have greatly facilitated the development of decentralized collaborative applications, particularly in the domain of Artificial Intelligence of Things (AIoT). However, a critical aspect missing from the current research landscape is the ability to enable data-driven client models with symbolic reasoning capabilities. Specifically, the inherent heterogeneity of participating client devices poses a significant challenge, as each client exhibits unique logic reasoning properties. Failing to consider these device-specific specifications can result in critical properties being missed in the client predictions, leading to suboptimal performance. In this work, we propose a new training paradigm that leverages temporal logic reasoning to address this issue. Our approach involves enhancing the training process by incorporating mechanically generated logic expressions for each FL client. Additionally, we introduce the concept of aggregation clusters 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#20195;&#30721;&#30340;&#23646;&#24615;&#65292;&#25581;&#31034;&#20102;&#26426;&#22120;&#21644;&#20154;&#31867;&#20195;&#30721;&#20043;&#38388;&#30340;&#29420;&#29305;&#27169;&#24335;&#65292;&#23588;&#20854;&#26159;&#32467;&#26500;&#20998;&#21106;&#23545;&#20110;&#35782;&#21035;&#20195;&#30721;&#26469;&#28304;&#24456;&#20851;&#38190;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DetectCodeGPT&#30340;&#26032;&#26041;&#27861;&#26469;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#20195;&#30721;&#12290;</title><link>http://arxiv.org/abs/2401.06461</link><description>&lt;p&gt;
&#20195;&#30721;&#20043;&#38388;&#30340;&#30028;&#38480;&#65306;&#25581;&#31034;&#26426;&#22120;&#21644;&#20154;&#31867;&#31243;&#24207;&#21592;&#20043;&#38388;&#19981;&#21516;&#30340;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Between Lines of Code: Unraveling the Distinct Patterns of Machine and Human Programmers. (arXiv:2401.06461v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#20195;&#30721;&#30340;&#23646;&#24615;&#65292;&#25581;&#31034;&#20102;&#26426;&#22120;&#21644;&#20154;&#31867;&#20195;&#30721;&#20043;&#38388;&#30340;&#29420;&#29305;&#27169;&#24335;&#65292;&#23588;&#20854;&#26159;&#32467;&#26500;&#20998;&#21106;&#23545;&#20110;&#35782;&#21035;&#20195;&#30721;&#26469;&#28304;&#24456;&#20851;&#38190;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DetectCodeGPT&#30340;&#26032;&#26041;&#27861;&#26469;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#27169;&#31946;&#20102;&#26426;&#22120;&#21644;&#20154;&#31867;&#28304;&#20195;&#30721;&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#23548;&#33268;&#36719;&#20214;&#20135;&#29289;&#30340;&#23436;&#25972;&#24615;&#21644;&#30495;&#23454;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#20195;&#30721;&#38271;&#24230;&#12289;&#35789;&#27719;&#22810;&#26679;&#24615;&#21644;&#33258;&#28982;&#24615;&#31561;&#23646;&#24615;&#30340;&#20005;&#26684;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#26426;&#22120;&#21644;&#20154;&#31867;&#20195;&#30721;&#22266;&#26377;&#30340;&#29420;&#29305;&#27169;&#24335;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#29305;&#21035;&#27880;&#24847;&#21040;&#65292;&#20195;&#30721;&#30340;&#32467;&#26500;&#20998;&#21106;&#26159;&#35782;&#21035;&#20854;&#26469;&#28304;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DetectCodeGPT&#30340;&#26032;&#22411;&#26426;&#22120;&#29983;&#25104;&#20195;&#30721;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25913;&#36827;&#20102;DetectGPT&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have catalyzed an unprecedented wave in code generation. While achieving significant advances, they blur the distinctions between machine-and human-authored source code, causing integrity and authenticity issues of software artifacts. Previous methods such as DetectGPT have proven effective in discerning machine-generated texts, but they do not identify and harness the unique patterns of machine-generated code. Thus, its applicability falters when applied to code. In this paper, we carefully study the specific patterns that characterize machine and human-authored code. Through a rigorous analysis of code attributes such as length, lexical diversity, and naturalness, we expose unique pat-terns inherent to each source. We particularly notice that the structural segmentation of code is a critical factor in identifying its provenance. Based on our findings, we propose a novel machine-generated code detection method called DetectCodeGPT, which improves DetectGPT by cap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23558;LLMs&#35270;&#20026;&#20154;&#31867;&#20132;&#27969;&#32773;&#65292;&#25506;&#32034;&#20102;&#27599;&#22825;&#35821;&#35328;&#20114;&#21160;&#21644;AI&#23433;&#20840;&#20043;&#38388;&#24573;&#35270;&#30340;&#20132;&#21449;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35828;&#26381;LLMs&#36827;&#34892;&#36234;&#29425;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35828;&#26381;&#26174;&#33879;&#25552;&#39640;&#20102;&#36234;&#29425;&#24615;&#33021;&#65292;&#22312;&#22810;&#20010;&#39118;&#38505;&#31867;&#21035;&#19978;&#22343;&#21462;&#24471;&#20102;&#36229;&#36807;92%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.06373</link><description>&lt;p&gt;
&#22914;&#20309;&#20351;Johnny&#35828;&#26381;LLMs&#36234;&#29425;&#65306;&#36890;&#36807;&#20154;&#24615;&#21270;LLMs&#37325;&#26032;&#24605;&#32771;&#23545;AI&#23433;&#20840;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs. (arXiv:2401.06373v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23558;LLMs&#35270;&#20026;&#20154;&#31867;&#20132;&#27969;&#32773;&#65292;&#25506;&#32034;&#20102;&#27599;&#22825;&#35821;&#35328;&#20114;&#21160;&#21644;AI&#23433;&#20840;&#20043;&#38388;&#24573;&#35270;&#30340;&#20132;&#21449;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35828;&#26381;LLMs&#36827;&#34892;&#36234;&#29425;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35828;&#26381;&#26174;&#33879;&#25552;&#39640;&#20102;&#36234;&#29425;&#24615;&#33021;&#65292;&#22312;&#22810;&#20010;&#39118;&#38505;&#31867;&#21035;&#19978;&#22343;&#21462;&#24471;&#20102;&#36229;&#36807;92%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#20256;&#32479;&#30340;AI&#23433;&#20840;&#30740;&#31350;&#23558;AI&#27169;&#22411;&#35270;&#20026;&#26426;&#22120;&#65292;&#24182;&#38598;&#20013;&#22312;&#30001;&#23433;&#20840;&#19987;&#23478;&#24320;&#21457;&#30340;&#22522;&#20110;&#31639;&#27861;&#30340;&#25915;&#20987;&#19978;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#21644;&#31454;&#20105;&#21147;&#36234;&#26469;&#36234;&#24378;&#65292;&#38750;&#19987;&#23478;&#29992;&#25143;&#22312;&#26085;&#24120;&#20114;&#21160;&#20013;&#20063;&#21487;&#33021;&#20135;&#29983;&#39118;&#38505;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#65292;&#23558;LLMs&#20316;&#20026;&#31867;&#20284;&#20154;&#31867;&#30340;&#20132;&#27969;&#32773;&#26469;&#36234;&#29425;&#65292;&#20197;&#25506;&#32034;&#27599;&#22825;&#35821;&#35328;&#20114;&#21160;&#21644;AI&#23433;&#20840;&#20043;&#38388;&#34987;&#24573;&#35270;&#30340;&#20132;&#21449;&#28857;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#22914;&#20309;&#35828;&#26381;LLMs&#36234;&#29425;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#20960;&#21313;&#24180;&#30340;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#20013;&#24471;&#20986;&#30340;&#35828;&#26381;&#20998;&#31867;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#36825;&#20010;&#20998;&#31867;&#27861;&#26469;&#33258;&#21160;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#35828;&#26381;&#23545;&#25239;&#25552;&#31034;&#65288;PAP&#65289;&#26469;&#36234;&#29425;LLMs&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#35828;&#26381;&#26174;&#33879;&#25552;&#39640;&#20102;&#36234;&#29425;&#24615;&#33021;&#65292;&#22312;&#25152;&#26377;&#39118;&#38505;&#31867;&#21035;&#19978;PAP&#22312;Llama 2-7b Chat&#12289;GPT-3.5&#21644;GPT-4&#19978;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#22312;10&#27425;&#35797;&#39564;&#20013;&#22343;&#36229;&#36807;92%&#65292;&#36229;&#36807;&#20102;&#26368;&#36817;&#30340;&#22522;&#20110;&#31639;&#27861;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most traditional AI safety research has approached AI models as machines and centered on algorithm-focused attacks developed by security experts. As large language models (LLMs) become increasingly common and competent, non-expert users can also impose risks during daily interactions. This paper introduces a new perspective to jailbreak LLMs as human-like communicators, to explore this overlooked intersection between everyday language interaction and AI safety. Specifically, we study how to persuade LLMs to jailbreak them. First, we propose a persuasion taxonomy derived from decades of social science research. Then, we apply the taxonomy to automatically generate interpretable persuasive adversarial prompts (PAP) to jailbreak LLMs. Results show that persuasion significantly increases the jailbreak performance across all risk categories: PAP consistently achieves an attack success rate of over $92\%$ on Llama 2-7b Chat, GPT-3.5, and GPT-4 in $10$ trials, surpassing recent algorithm-focu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25512;&#33616;&#20013;&#24847;&#22270;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#32858;&#31867;&#26041;&#27861;ELCRec&#65292;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#22797;&#26434;&#20248;&#21270;&#38382;&#39064;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#32858;&#31867;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.05975</link><description>&lt;p&gt;
&#29992;&#20110;&#25512;&#33616;&#20013;&#24847;&#22270;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
End-to-end Learnable Clustering for Intent Learning in Recommendation. (arXiv:2401.05975v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25512;&#33616;&#20013;&#24847;&#22270;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#32858;&#31867;&#26041;&#27861;ELCRec&#65292;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#22797;&#26434;&#20248;&#21270;&#38382;&#39064;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#32858;&#31867;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25366;&#25496;&#29992;&#25143;&#30340;&#24847;&#22270;&#22312;&#24207;&#21015;&#25512;&#33616;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;ICLRec&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#32858;&#31867;&#26469;&#25552;&#21462;&#29992;&#25143;&#30340;&#28508;&#22312;&#24847;&#22270;&#12290;&#23613;&#31649;&#23427;&#24050;&#32463;&#26174;&#31034;&#20986;&#26377;&#25928;&#24615;&#65292;&#20294;&#29616;&#26377;&#30340;&#26041;&#27861;&#23384;&#22312;&#22797;&#26434;&#21644;&#32321;&#29712;&#30340;&#20132;&#26367;&#20248;&#21270;&#38382;&#39064;&#65292;&#23548;&#33268;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#22312;&#24191;&#20041;&#26399;&#26395;&#26368;&#22823;&#21270;(EM)&#26694;&#26550;&#20013;&#20998;&#31163;&#34920;&#31034;&#23398;&#20064;&#21644;&#32858;&#31867;&#20248;&#21270;&#32463;&#24120;&#23548;&#33268;&#27425;&#20248;&#24615;&#33021;&#12290;&#20854;&#27425;&#65292;&#22312;&#25972;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#32858;&#31867;&#20250;&#24433;&#21709;&#22823;&#35268;&#27169;&#34892;&#19994;&#25968;&#25454;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24847;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;ELCRec&#65292;&#23427;&#23558;&#34920;&#31034;&#23398;&#20064;&#38598;&#25104;&#21040;&#19968;&#20010;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#32858;&#31867;&#26694;&#26550;&#20013;&#36827;&#34892;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mining users' intents plays a crucial role in sequential recommendation. The recent approach, ICLRec, was introduced to extract underlying users' intents using contrastive learning and clustering. While it has shown effectiveness, the existing method suffers from complex and cumbersome alternating optimization, leading to two main issues. Firstly, the separation of representation learning and clustering optimization within a generalized expectation maximization (EM) framework often results in sub-optimal performance. Secondly, performing clustering on the entire dataset hampers scalability for large-scale industry data. To address these challenges, we propose a novel intent learning method called \underline{ELCRec}, which integrates representation learning into an \underline{E}nd-to-end \underline{L}earnable \underline{C}lustering framework for \underline{Rec}ommendation. Specifically, we encode users' behavior sequences and initialize the cluster centers as learnable network parameter
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26862;&#26519;&#20462;&#21098;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20860;&#39038;&#38543;&#26426;&#26862;&#26519;&#20934;&#30830;&#24615;&#21644;&#20915;&#31574;&#26641;&#21487;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#26223;&#19979;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#38543;&#26426;&#26862;&#26519;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.05535</link><description>&lt;p&gt;
&#36890;&#36807;&#26862;&#26519;&#20462;&#21098;&#25552;&#39640;&#38543;&#26426;&#26862;&#26519;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving the Accuracy and Interpretability of Random Forests via Forest Pruning. (arXiv:2401.05535v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05535
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26862;&#26519;&#20462;&#21098;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20860;&#39038;&#38543;&#26426;&#26862;&#26519;&#20934;&#30830;&#24615;&#21644;&#20915;&#31574;&#26641;&#21487;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#26223;&#19979;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#38543;&#26426;&#26862;&#26519;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25509;&#36817;&#20960;&#21313;&#24180;&#30340;&#21457;&#23637;&#20043;&#21518;&#65292;&#38543;&#26426;&#26862;&#26519;&#20173;&#28982;&#22312;&#21508;&#31181;&#23398;&#20064;&#38382;&#39064;&#20013;&#25552;&#20379;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#65292;&#22312;&#36825;&#26041;&#38754;&#36229;&#36234;&#20102;&#20915;&#31574;&#26641;&#29978;&#33267;&#31070;&#32463;&#32593;&#32476;&#31561;&#26367;&#20195;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#19968;&#31181;&#38598;&#25104;&#26041;&#27861;&#65292;&#38543;&#26426;&#26862;&#26519;&#22312;&#35299;&#37322;&#24615;&#26041;&#38754;&#24448;&#24448;&#27604;&#20915;&#31574;&#26641;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20107;&#21518;&#26041;&#27861;&#65292;&#26088;&#22312;&#20860;&#39038;&#38543;&#26426;&#26862;&#26519;&#30340;&#20934;&#30830;&#24615;&#21644;&#20915;&#31574;&#26641;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26862;&#26519;&#20462;&#21098;&#26041;&#27861;&#65292;&#20197;&#22312;&#32473;&#23450;&#30340;&#38543;&#26426;&#26862;&#26519;&#20869;&#25214;&#21040;&#26368;&#20339;&#23376;&#26862;&#26519;&#65292;&#28982;&#21518;&#22312;&#36866;&#29992;&#30340;&#24773;&#20917;&#19979;&#23558;&#36873;&#23450;&#30340;&#26641;&#21512;&#24182;&#20026;&#19968;&#26869;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#31181;&#26041;&#27861;&#20381;&#36182;&#20110;&#32422;&#26463;&#31351;&#20030;&#25628;&#32034;&#65292;&#32780;&#31532;&#20108;&#31181;&#26041;&#27861;&#22522;&#20110;LASSO&#26041;&#27861;&#30340;&#25913;&#36827;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#26223;&#19979;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#20013;&#33267;&#23569;&#26377;&#19968;&#31181;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#38543;&#26426;&#26862;&#26519;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decades after their inception, random forests continue to provide state-of-the-art accuracy in a variety of learning problems, outperforming in this respect alternative machine learning algorithms such as decision trees or even neural networks. However, being an ensemble method, the one aspect where random forests tend to severely underperform decision trees is interpretability. In the present work, we propose a post-hoc approach that aims to have the best of both worlds: the accuracy of random forests and the interpretability of decision trees. To this end, we present two forest-pruning methods to find an optimal sub-forest within a given random forest, and then, when applicable, combine the selected trees into one. Our first method relies on constrained exhaustive search, while our second method is based on an adaptation of the LASSO methodology. Extensive experiments over synthetic and real world datasets show that, in the majority of scenarios, at least one of the two methods propo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#39046;&#22495;&#29305;&#23450;LLM&#30340;&#24494;&#35843;&#21644;&#21033;&#29992;&#26041;&#27861;&#65292;&#20197;&#37329;&#34701;&#39046;&#22495;&#20026;&#20363;&#65292;&#35814;&#32454;&#20171;&#32461;&#20102;&#25968;&#25454;&#38598;&#36873;&#25321;&#12289;&#39044;&#22788;&#29702;&#12289;&#27169;&#22411;&#36873;&#25321;&#20197;&#21450;&#22312;&#37329;&#34701;&#39046;&#22495;LLM&#24494;&#35843;&#20013;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#30740;&#31350;&#25506;&#35752;&#20102;&#39046;&#22495;&#29305;&#23450;&#35789;&#27719;&#30340;&#26500;&#24314;&#21644;&#23433;&#20840;&#21512;&#35268;&#24615;&#30340;&#32771;&#34385;&#22240;&#32032;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#37329;&#34701;&#39046;&#22495;&#29983;&#25104;&#39046;&#22495;&#29305;&#23450;LLM&#30340;&#36807;&#31243;&#21644;&#23454;&#26045;&#26041;&#27861;&#12290;&#22810;&#31181;&#37329;&#34701;&#26696;&#20363;&#34987;&#28085;&#30422;&#22312;&#20869;&#65292;&#21253;&#25324;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#12289;&#37329;&#34701;&#26032;&#38395;&#24773;&#32490;&#20998;&#26512;&#12289;&#33258;&#21160;&#25991;&#26723;&#22788;&#29702;&#12289;&#30740;&#31350;&#21644;&#20449;&#24687;&#25552;&#21462;&#31561;&#12290;</title><link>http://arxiv.org/abs/2401.02981</link><description>&lt;p&gt;
&#39046;&#22495;&#29305;&#23450;LLM&#30340;&#24494;&#35843;&#21644;&#21033;&#29992;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning and Utilization Methods of Domain-specific LLMs. (arXiv:2401.02981v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02981
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#39046;&#22495;&#29305;&#23450;LLM&#30340;&#24494;&#35843;&#21644;&#21033;&#29992;&#26041;&#27861;&#65292;&#20197;&#37329;&#34701;&#39046;&#22495;&#20026;&#20363;&#65292;&#35814;&#32454;&#20171;&#32461;&#20102;&#25968;&#25454;&#38598;&#36873;&#25321;&#12289;&#39044;&#22788;&#29702;&#12289;&#27169;&#22411;&#36873;&#25321;&#20197;&#21450;&#22312;&#37329;&#34701;&#39046;&#22495;LLM&#24494;&#35843;&#20013;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#30740;&#31350;&#25506;&#35752;&#20102;&#39046;&#22495;&#29305;&#23450;&#35789;&#27719;&#30340;&#26500;&#24314;&#21644;&#23433;&#20840;&#21512;&#35268;&#24615;&#30340;&#32771;&#34385;&#22240;&#32032;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#37329;&#34701;&#39046;&#22495;&#29983;&#25104;&#39046;&#22495;&#29305;&#23450;LLM&#30340;&#36807;&#31243;&#21644;&#23454;&#26045;&#26041;&#27861;&#12290;&#22810;&#31181;&#37329;&#34701;&#26696;&#20363;&#34987;&#28085;&#30422;&#22312;&#20869;&#65292;&#21253;&#25324;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#12289;&#37329;&#34701;&#26032;&#38395;&#24773;&#32490;&#20998;&#26512;&#12289;&#33258;&#21160;&#25991;&#26723;&#22788;&#29702;&#12289;&#30740;&#31350;&#21644;&#20449;&#24687;&#25552;&#21462;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#21457;&#24067;&#30340;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#65292;&#20294;&#20851;&#20110;&#39046;&#22495;&#29305;&#23450;LLM&#30340;&#24494;&#35843;&#21644;&#24212;&#29992;&#30340;&#30740;&#31350;&#20173;&#28982;&#24456;&#23569;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#39046;&#22495;&#29305;&#23450;LLM&#30340;&#24494;&#35843;&#21644;&#21033;&#29992;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;LLM&#30340;&#36235;&#21183;&#12289;&#22522;&#30784;&#27169;&#22411;&#21644;&#29992;&#20110;&#39046;&#22495;&#29305;&#23450;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#20197;&#37329;&#34701;&#34892;&#19994;&#20026;&#20363;&#65292;&#35814;&#32454;&#20171;&#32461;&#20102;&#25968;&#25454;&#38598;&#36873;&#25321;&#12289;&#39044;&#22788;&#29702;&#12289;&#27169;&#22411;&#36873;&#25321;&#20197;&#21450;&#22312;&#37329;&#34701;&#39046;&#22495;LLM&#24494;&#35843;&#20013;&#20851;&#38190;&#30340;&#32771;&#34385;&#22240;&#32032;&#12290;&#38024;&#23545;&#37329;&#34701;&#25968;&#25454;&#30340;&#29420;&#29305;&#29305;&#28857;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#39046;&#22495;&#29305;&#23450;&#35789;&#27719;&#30340;&#26500;&#24314;&#65292;&#20197;&#21450;&#23433;&#20840;&#24615;&#21644;&#21512;&#35268;&#24615;&#30340;&#32771;&#34385;&#22240;&#32032;&#12290;&#22312;LLM&#24494;&#35843;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#26412;&#30740;&#31350;&#27010;&#36848;&#20102;&#22312;&#37329;&#34701;&#39046;&#22495;&#29983;&#25104;&#39046;&#22495;&#29305;&#23450;LLM&#30340;&#36807;&#31243;&#21644;&#23454;&#26045;&#26041;&#27861;&#12290;&#21253;&#25324;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#12289;&#37329;&#34701;&#26032;&#38395;&#24773;&#32490;&#20998;&#26512;&#12289;&#33258;&#21160;&#25991;&#26723;&#22788;&#29702;&#12289;&#30740;&#31350;&#21644;&#20449;&#24687;&#25552;&#21462;&#31561;&#22810;&#31181;&#37329;&#34701;&#26696;&#20363;&#34987;&#28085;&#30422;&#22312;&#20869;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent releases of pre-trained Large Language Models (LLMs) have gained considerable traction, yet research on fine-tuning and employing domain-specific LLMs remains scarce. This study investigates approaches for fine-tuning and leveraging domain-specific LLMs, highlighting trends in LLMs, foundational models, and methods for domain-specific pre-training. Focusing on the financial sector, it details dataset selection, preprocessing, model choice, and considerations crucial for LLM fine-tuning in finance. Addressing the unique characteristics of financial data, the study explores the construction of domain-specific vocabularies and considerations for security and regulatory compliance. In the practical application of LLM fine-tuning, the study outlines the procedure and implementation for generating domain-specific LLMs in finance. Various financial cases, including stock price prediction, sentiment analysis of financial news, automated document processing, research, information extract
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;SAR-RARP50&#25361;&#25112;&#65292;&#35813;&#25361;&#25112;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#12289;&#20844;&#24320;&#30340;&#12289;&#20307;&#20869;&#30340;&#25163;&#26415;&#21160;&#20316;&#35782;&#21035;&#21644;&#35821;&#20041;&#20202;&#22120;&#20998;&#21106;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#35753;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20986;&#31283;&#20581;&#19988;&#20934;&#30830;&#30340;&#21333;&#20219;&#21153;&#21160;&#20316;&#35782;&#21035;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.00496</link><description>&lt;p&gt;
SAR-RARP50:&#26426;&#22120;&#20154;&#36741;&#21161;&#26681;&#27835;&#24615;&#21069;&#21015;&#33146;&#20999;&#38500;&#26415;&#20013;&#25163;&#26415;&#22120;&#26800;&#30340;&#20998;&#21106;&#21644;&#21160;&#20316;&#35782;&#21035;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
SAR-RARP50: Segmentation of surgical instrumentation and Action Recognition on Robot-Assisted Radical Prostatectomy Challenge. (arXiv:2401.00496v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00496
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;SAR-RARP50&#25361;&#25112;&#65292;&#35813;&#25361;&#25112;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#12289;&#20844;&#24320;&#30340;&#12289;&#20307;&#20869;&#30340;&#25163;&#26415;&#21160;&#20316;&#35782;&#21035;&#21644;&#35821;&#20041;&#20202;&#22120;&#20998;&#21106;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#35753;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20986;&#31283;&#20581;&#19988;&#20934;&#30830;&#30340;&#21333;&#20219;&#21153;&#21160;&#20316;&#35782;&#21035;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#26415;&#24037;&#20855;&#20998;&#21106;&#21644;&#21160;&#20316;&#35782;&#21035;&#26159;&#35768;&#22810;&#35745;&#31639;&#26426;&#36741;&#21161;&#24178;&#39044;&#24212;&#29992;&#30340;&#22522;&#26412;&#26500;&#24314;&#27169;&#22359;&#65292;&#20174;&#25163;&#26415;&#25216;&#33021;&#35780;&#20272;&#21040;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#12290;&#29616;&#22312;&#65292;&#22522;&#20110;&#23398;&#20064;&#30340;&#21160;&#20316;&#35782;&#21035;&#21644;&#20998;&#21106;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#20294;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#30340;&#27880;&#37322;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#21160;&#20316;&#35782;&#21035;&#21644;&#24037;&#20855;&#20998;&#21106;&#31639;&#27861;&#36890;&#24120;&#26159;&#22312;&#24444;&#27492;&#23396;&#31435;&#22320;&#35757;&#32451;&#21644;&#39044;&#27979;&#65292;&#27809;&#26377;&#21033;&#29992;&#28508;&#22312;&#30340;&#20132;&#21449;&#20219;&#21153;&#20851;&#31995;&#12290;&#36890;&#36807;EndoVis 2022 SAR-RARP50&#25361;&#25112;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#30340;&#12289;&#20844;&#24320;&#30340;&#12289;&#20307;&#20869;&#30340;&#25163;&#26415;&#21160;&#20316;&#35782;&#21035;&#21644;&#35821;&#20041;&#20202;&#22120;&#20998;&#21106;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;50&#20010;&#26426;&#22120;&#20154;&#36741;&#21161;&#26681;&#27835;&#24615;&#21069;&#21015;&#33146;&#20999;&#38500;&#26415;&#65288;RARP&#65289;&#30340;&#32541;&#21512;&#35270;&#39057;&#29255;&#27573;&#12290;&#25361;&#25112;&#30340;&#30446;&#26631;&#26159;&#20004;&#26041;&#38754;&#30340;&#12290;&#39318;&#20808;&#65292;&#35753;&#30740;&#31350;&#20154;&#21592;&#21033;&#29992;&#25552;&#20379;&#30340;&#25968;&#25454;&#38598;&#35268;&#27169;&#65292;&#24320;&#21457;&#20986;&#31283;&#20581;&#19988;&#39640;&#20934;&#30830;&#24615;&#30340;&#21333;&#20219;&#21153;&#21160;&#20316;&#35782;&#21035;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Surgical tool segmentation and action recognition are fundamental building blocks in many computer-assisted intervention applications, ranging from surgical skills assessment to decision support systems. Nowadays, learning-based action recognition and segmentation approaches outperform classical methods, relying, however, on large, annotated datasets. Furthermore, action recognition and tool segmentation algorithms are often trained and make predictions in isolation from each other, without exploiting potential cross-task relationships. With the EndoVis 2022 SAR-RARP50 challenge, we release the first multimodal, publicly available, in-vivo, dataset for surgical action recognition and semantic instrumentation segmentation, containing 50 suturing video segments of Robotic Assisted Radical Prostatectomy (RARP). The aim of the challenge is twofold. First, to enable researchers to leverage the scale of the provided dataset and develop robust and highly accurate single-task action recognitio
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23450;&#20041;&#65292;&#29992;&#20110;&#24230;&#37327;&#31354;&#38388;&#20013;&#23567;&#38598;&#21512;&#23545;&#22823;&#38598;&#21512;&#30340;&#8220;&#20195;&#34920;&#24615;&#8221;&#12290;&#32473;&#23450;&#20195;&#34920;&#30340;&#38598;&#21512;&#21644;&#21487;&#33021;&#30340;&#20195;&#34920;&#38598;&#21512;&#65292;&#25105;&#20204;&#30340;&#26631;&#20934;&#35201;&#27714;&#23545;&#20110;&#22823;&#38598;&#21512;&#30340;&#20219;&#24847;&#23376;&#38598;&#65292;&#35813;&#23376;&#38598;&#19982;&#26368;&#22909;&#30340;&#20195;&#34920;&#28857;&#30340;&#24179;&#22343;&#36317;&#31163;&#19982;&#22312;&#25152;&#26377;&#20195;&#34920;&#28857;&#20013;&#30340;&#26368;&#22909;&#28857;&#30340;&#24179;&#22343;&#36317;&#31163;&#20043;&#38388;&#30340;&#24046;&#36317;&#19981;&#36229;&#36807;&#19968;&#20010;&#22240;&#23376;gamma&#12290;&#19982;&#24050;&#26377;&#30340;&#27604;&#20363;&#20844;&#24179;&#21644;&#26680;&#24515;&#20844;&#24179;&#30340;&#27010;&#24565;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#23450;&#20041;&#35201;&#27714;&#20197;&#27604;&#20363;&#34920;&#31034;&#22823;&#30340;&#20957;&#32858;&#31751;&#12290;&#22312;&#36164;&#28304;&#22686;&#24378;&#26694;&#26550;&#20013;&#30740;&#31350;&#20102;&#35813;&#23450;&#20041;&#65292;&#21516;&#26102;&#20063;&#32771;&#34385;&#20102;&#36873;&#20030;&#24212;&#29992;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2312.10369</link><description>&lt;p&gt;
&#22312;&#24230;&#37327;&#31354;&#38388;&#20013;&#30340;&#27604;&#20363;&#34920;&#31034;&#21644;&#20302;&#22833;&#30495;&#22996;&#21592;&#20250;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Proportional Representation in Metric Spaces and Low-Distortion Committee Selection. (arXiv:2312.10369v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10369
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23450;&#20041;&#65292;&#29992;&#20110;&#24230;&#37327;&#31354;&#38388;&#20013;&#23567;&#38598;&#21512;&#23545;&#22823;&#38598;&#21512;&#30340;&#8220;&#20195;&#34920;&#24615;&#8221;&#12290;&#32473;&#23450;&#20195;&#34920;&#30340;&#38598;&#21512;&#21644;&#21487;&#33021;&#30340;&#20195;&#34920;&#38598;&#21512;&#65292;&#25105;&#20204;&#30340;&#26631;&#20934;&#35201;&#27714;&#23545;&#20110;&#22823;&#38598;&#21512;&#30340;&#20219;&#24847;&#23376;&#38598;&#65292;&#35813;&#23376;&#38598;&#19982;&#26368;&#22909;&#30340;&#20195;&#34920;&#28857;&#30340;&#24179;&#22343;&#36317;&#31163;&#19982;&#22312;&#25152;&#26377;&#20195;&#34920;&#28857;&#20013;&#30340;&#26368;&#22909;&#28857;&#30340;&#24179;&#22343;&#36317;&#31163;&#20043;&#38388;&#30340;&#24046;&#36317;&#19981;&#36229;&#36807;&#19968;&#20010;&#22240;&#23376;gamma&#12290;&#19982;&#24050;&#26377;&#30340;&#27604;&#20363;&#20844;&#24179;&#21644;&#26680;&#24515;&#20844;&#24179;&#30340;&#27010;&#24565;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#23450;&#20041;&#35201;&#27714;&#20197;&#27604;&#20363;&#34920;&#31034;&#22823;&#30340;&#20957;&#32858;&#31751;&#12290;&#22312;&#36164;&#28304;&#22686;&#24378;&#26694;&#26550;&#20013;&#30740;&#31350;&#20102;&#35813;&#23450;&#20041;&#65292;&#21516;&#26102;&#20063;&#32771;&#34385;&#20102;&#36873;&#20030;&#24212;&#29992;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#23450;&#20041;&#65292;&#21363;&#22312;&#24230;&#37327;&#31354;&#38388;&#20013;&#65292;&#23545;&#20110;&#19968;&#20010;&#21253;&#21547;k&#20010;&#28857;&#30340;&#23567;&#38598;&#21512;R&#26469;&#35828;&#65292;&#8220;&#20195;&#34920;&#8221;&#26356;&#22823;&#38598;&#21512;&#30340;&#23450;&#20041;&#12290;&#32473;&#23450;&#35201;&#34920;&#31034;&#30340;&#38598;&#21512;V&#65288;&#20363;&#22914;&#65292;&#25991;&#26723;&#25110;&#36873;&#27665;&#65289;&#65292;&#20197;&#21450;&#21487;&#33021;&#30340;&#20195;&#34920;&#38598;&#21512;C&#65292;&#25105;&#20204;&#30340;&#26631;&#20934;&#35201;&#27714;&#23545;&#20110;V&#30340;&#20219;&#24847;&#23376;&#38598;S&#65292;&#35813;&#23376;&#38598;&#21253;&#21547;V&#30340;theta&#20998;&#25968;&#65292;&#22312;R&#20013;&#30340;theta*k&#20010;&#26368;&#20339;&#28857;&#30340;&#24179;&#22343;&#36317;&#31163;&#19981;&#24212;&#36229;&#36807;&#19982;&#23427;&#20204;&#19982;C&#20013;&#25152;&#26377;&#28857;&#30340;&#26368;&#20339;theta*k&#28857;&#30340;&#24179;&#22343;&#36317;&#31163;&#30456;&#27604;&#36229;&#36807;&#19968;&#20010;&#22240;&#23376;gamma&#12290;&#35813;&#23450;&#20041;&#24378;&#21270;&#20102;&#27604;&#20363;&#20844;&#24179;&#21644;&#26680;&#24515;&#20844;&#24179;&#30340;&#27010;&#24565;&#65292;&#20294;&#19982;&#36825;&#20123;&#27010;&#24565;&#19981;&#21516;&#30340;&#26159;&#65292;&#23427;&#35201;&#27714;&#22823;&#30340;&#20957;&#32858;&#31751;&#20197;&#27604;&#20363;&#34920;&#31034;&#23427;&#20204;&#30340;&#22823;&#23567;&#12290;&#30001;&#20110;&#23384;&#22312;&#23454;&#20363;&#65292;&#38500;&#38750;gamma&#26159;&#22810;&#39033;&#24335;&#32423;&#21035;&#30340;&#22823;&#65292;&#21542;&#21017;&#27809;&#26377;&#35299;&#23384;&#22312;&#65292;&#25105;&#20204;&#22312;&#36164;&#28304;&#22686;&#24378;&#26694;&#26550;&#20013;&#30740;&#31350;&#20102;&#36825;&#20010;&#27010;&#24565;&#65292;&#38544;&#21547;&#22320;&#23558;&#22823;&#23567;&#20026;k&#30340;&#38598;&#21512;R&#30340;&#32422;&#26463;&#35828;&#26126;&#20026;&#20854;&#22823;&#23567;&#20165;&#20026;k/alpha&#65292;&#20854;&#20013;alpha &gt; 1&#12290;&#27492;&#22806;&#65292;&#21463;&#21040;&#36873;&#20030;&#24212;&#29992;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#26368;...
&lt;/p&gt;
&lt;p&gt;
We introduce a novel definition for a small set R of k points being "representative" of a larger set in a metric space. Given a set V (e.g., documents or voters) to represent, and a set C of possible representatives, our criterion requires that for any subset S comprising a theta fraction of V, the average distance of S to their best theta*k points in R should not be more than a factor gamma compared to their average distance to the best theta*k points among all of C. This definition is a strengthening of proportional fairness and core fairness, but - different from those notions - requires that large cohesive clusters be represented proportionally to their size.  Since there are instances for which - unless gamma is polynomially large - no solutions exist, we study this notion in a resource augmentation framework, implicitly stating the constraints for a set R of size k as though its size were only k/alpha, for alpha &gt; 1. Furthermore, motivated by the application to elections, we most
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32039;&#20945;&#22411;&#30340;LSTM-SVM&#34701;&#21512;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#26089;&#26399;&#26816;&#27979;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#20102;&#19968;&#31181;&#27969;&#31243;&#20248;&#21270;&#26041;&#27861;&#23558;&#24515;&#30005;&#22270;&#20449;&#21495;&#39044;&#22788;&#29702;&#20026;&#19968;&#33268;&#30340;10&#31186;&#25345;&#32493;&#26102;&#38388;&#65292;&#21516;&#26102;&#21033;&#29992;LSTM&#21644;SVM&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2312.09442</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#38271;&#26102;&#38388;&#24515;&#34880;&#31649;&#30142;&#30149;&#26816;&#27979;&#30340;&#32039;&#20945;&#22411;LSTM-SVM&#34701;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Compact LSTM-SVM Fusion Model for Long-Duration Cardiovascular Diseases Detection. (arXiv:2312.09442v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.09442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32039;&#20945;&#22411;&#30340;LSTM-SVM&#34701;&#21512;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#26089;&#26399;&#26816;&#27979;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#20102;&#19968;&#31181;&#27969;&#31243;&#20248;&#21270;&#26041;&#27861;&#23558;&#24515;&#30005;&#22270;&#20449;&#21495;&#39044;&#22788;&#29702;&#20026;&#19968;&#33268;&#30340;10&#31186;&#25345;&#32493;&#26102;&#38388;&#65292;&#21516;&#26102;&#21033;&#29992;LSTM&#21644;SVM&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#33539;&#22260;&#20869;&#65292;&#24515;&#34880;&#31649;&#30142;&#30149;&#26159;&#33268;&#27515;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#27599;&#24180;&#32422;&#26377;1790&#19975;&#20154;&#27515;&#20110;&#35813;&#30149;&#12290;&#26089;&#26399;&#26816;&#27979;&#24515;&#34880;&#31649;&#30142;&#30149;&#26159;&#20851;&#38190;&#30340;&#20020;&#24202;&#30446;&#26631;&#65292;&#30005;&#24515;&#22270;&#65288;ECG&#65289;&#25968;&#25454;&#26159;&#19968;&#20010;&#21463;&#21040;&#30740;&#31350;&#30028;&#20851;&#27880;&#30340;&#39046;&#22495;&#12290;&#26368;&#36817;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#23637;&#22312;&#35813;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#23384;&#22312;&#22266;&#26377;&#30340;&#23616;&#38480;&#24615;&#65292;&#21253;&#25324;&#19981;&#36866;&#24403;&#30340;&#27169;&#22411;&#35780;&#20272;&#21644;&#25968;&#25454;&#27844;&#28431;&#30340;&#23454;&#20363;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21270;&#30340;&#24037;&#20316;&#27969;&#31243;&#33539;&#24335;&#65292;&#23558;ECG&#20449;&#21495;&#39044;&#22788;&#29702;&#20026;&#19968;&#33268;&#30340;10&#31186;&#25345;&#32493;&#26102;&#38388;&#65292;&#28040;&#38500;&#20102;&#25163;&#21160;&#29305;&#24449;&#25552;&#21462;/&#24515;&#25615;&#26816;&#27979;&#30340;&#38656;&#35201;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#27169;&#22411;&#65292;&#21033;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;LSTM&#65289;&#19982;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#36827;&#34892;&#27450;&#35784;&#26816;&#27979;&#12290;&#35813;&#26550;&#26500;&#21253;&#25324;&#20004;&#20010;LSTM&#23618;&#21644;&#19968;&#20010;SVM&#20998;&#31867;&#22120;&#65292;&#23454;&#29616;&#20102;SOTA&#32467;&#26524;&#65292;&#24179;&#22343;&#31934;&#30830;&#24230;&#35780;&#20998;&#20026;...
&lt;/p&gt;
&lt;p&gt;
Globally, cardiovascular diseases (CVDs) are the leading cause of mortality, accounting for an estimated 17.9 million deaths annually. One critical clinical objective is the early detection of CVDs using electrocardiogram (ECG) data, an area that has received significant attention from the research community. Recent advancements based on machine learning and deep learning have achieved great progress in this domain. However, existing methodologies exhibit inherent limitations, including inappropriate model evaluations and instances of data leakage. In this study, we present a streamlined workflow paradigm for preprocessing ECG signals into consistent 10-second durations, eliminating the need for manual feature extraction/beat detection. We also propose a hybrid model of Long Short-Term Memory (LSTM) with Support Vector Machine (SVM) for fraud detection. This architecture consists of two LSTM layers and an SVM classifier, which achieves a SOTA results with an Average precision score of 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#37327;&#32479;&#19968;&#26694;&#26550;&#65288;IUF&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#24037;&#19994;&#21046;&#36896;&#20013;&#30340;&#23567;&#32570;&#38519;&#26816;&#27979;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#23545;&#35937;&#24863;&#30693;&#33258;&#27880;&#24847;&#21147;&#65288;OASA&#65289;&#21644;&#35821;&#20041;&#21387;&#32553;&#25439;&#22833;&#65288;SCL&#65289;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#26029;&#25972;&#21512;&#26032;&#29289;&#20307;&#26102;&#20943;&#23569;&#29305;&#24449;&#20914;&#31361;&#38382;&#39064;&#65292;&#24182;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.08917</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#23567;&#32570;&#38519;&#26816;&#27979;&#30340;&#22686;&#37327;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An Incremental Unified Framework for Small Defect Inspection. (arXiv:2312.08917v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.08917
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#37327;&#32479;&#19968;&#26694;&#26550;&#65288;IUF&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#24037;&#19994;&#21046;&#36896;&#20013;&#30340;&#23567;&#32570;&#38519;&#26816;&#27979;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#23545;&#35937;&#24863;&#30693;&#33258;&#27880;&#24847;&#21147;&#65288;OASA&#65289;&#21644;&#35821;&#20041;&#21387;&#32553;&#25439;&#22833;&#65288;SCL&#65289;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#26029;&#25972;&#21512;&#26032;&#29289;&#20307;&#26102;&#20943;&#23569;&#29305;&#24449;&#20914;&#31361;&#38382;&#39064;&#65292;&#24182;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;&#21046;&#36896;&#20013;&#65292;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#32570;&#38519;&#26816;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#26041;&#27861;&#38024;&#23545;&#29305;&#23450;&#30340;&#27969;&#27700;&#32447;&#65292;&#22312;&#38754;&#23545;&#21508;&#31181;&#20135;&#21697;&#32452;&#21512;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#27969;&#31243;&#26102;&#24448;&#24448;&#38754;&#20020;&#29305;&#24449;&#20914;&#31361;&#38382;&#39064;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22686;&#37327;&#32479;&#19968;&#26694;&#26550;&#65288;IUF&#65289;&#65292;&#21487;&#20197;&#22312;&#19981;&#26029;&#25972;&#21512;&#26032;&#29289;&#20307;&#26102;&#20943;&#23569;&#29305;&#24449;&#20914;&#31361;&#38382;&#39064;&#65292;&#36825;&#22312;&#38754;&#20020;&#29289;&#20307;&#22686;&#37327;&#23398;&#20064;&#22330;&#26223;&#20013;&#20855;&#26377;&#20248;&#21183;&#12290;&#25105;&#20204;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;transformer&#24341;&#20837;&#20102;&#23545;&#35937;&#24863;&#30693;&#33258;&#27880;&#24847;&#21147;&#65288;OASA&#65289;&#26469;&#21010;&#20998;&#26126;&#30830;&#30340;&#35821;&#20041;&#36793;&#30028;&#12290;&#38598;&#25104;&#20102;&#35821;&#20041;&#21387;&#32553;&#25439;&#22833;&#65288;SCL&#65289;&#26469;&#20248;&#21270;&#38750;&#20027;&#35201;&#35821;&#20041;&#31354;&#38388;&#65292;&#22686;&#24378;&#20102;&#32593;&#32476;&#23545;&#26032;&#29289;&#20307;&#30340;&#36866;&#24212;&#24615;&#12290;&#27492;&#22806;&#65292;&#22312;&#26435;&#37325;&#26356;&#26032;&#26102;&#65292;&#25105;&#20204;&#20248;&#20808;&#20445;&#30041;&#24050;&#24314;&#31435;&#29289;&#20307;&#30340;&#29305;&#24449;&#12290;&#36890;&#36807;&#22312;&#22270;&#20687;&#21644;&#20687;&#32032;&#32423;&#32570;&#38519;&#26816;&#27979;&#26041;&#38754;&#23637;&#31034;&#20986;&#21331;&#36234;&#24615;&#33021;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#23545;&#20110;&#21160;&#24577;&#21644;&#21487;&#25193;&#23637;&#30340;&#24037;&#19994;&#26816;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI)-driven defect inspection is pivotal in industrial manufacturing. Yet, many methods, tailored to specific pipelines, grapple with diverse product portfolios and evolving processes. Addressing this, we present the Incremental Unified Framework (IUF), which can reduce the feature conflict problem when continuously integrating new objects in the pipeline, making it advantageous in object-incremental learning scenarios. Employing a state-of-the-art transformer, we introduce Object-Aware Self-Attention (OASA) to delineate distinct semantic boundaries. Semantic Compression Loss (SCL) is integrated to optimize non-primary semantic space, enhancing network adaptability for novel objects. Additionally, we prioritize retaining the features of established objects during weight updates. Demonstrating prowess in both image and pixel-level defect inspection, our approach achieves state-of-the-art performance, proving indispensable for dynamic and scalable industrial inspe
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#30740;&#20851;&#27880;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#20027;&#35201;&#24635;&#32467;&#20102;&#32418;&#38431;&#27169;&#22411;&#25581;&#31034;&#38544;&#31169;&#39118;&#38505;&#12289;&#23558;&#38544;&#31169;&#32435;&#20837;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#12289;&#39640;&#25928;&#21024;&#38500;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#20197;&#31526;&#21512;&#38544;&#31169;&#27861;&#35268;&#12289;&#20197;&#21450;&#20943;&#36731;&#29256;&#26435;&#38382;&#39064;&#31561;&#25216;&#26415;&#30740;&#31350;&#12290;&#27861;&#24459;&#21644;&#25919;&#31574;&#30740;&#31350;&#34429;&#28982;&#20174;&#19981;&#21516;&#35282;&#24230;&#35299;&#20915;&#20102;&#30456;&#20851;&#25361;&#25112;&#65292;&#20294;&#19981;&#26159;&#26412;&#35843;&#30740;&#30340;&#37325;&#28857;&#12290;</title><link>http://arxiv.org/abs/2312.06717</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Privacy Issues in Large Language Models: A Survey. (arXiv:2312.06717v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.06717
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#30740;&#20851;&#27880;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#20027;&#35201;&#24635;&#32467;&#20102;&#32418;&#38431;&#27169;&#22411;&#25581;&#31034;&#38544;&#31169;&#39118;&#38505;&#12289;&#23558;&#38544;&#31169;&#32435;&#20837;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#12289;&#39640;&#25928;&#21024;&#38500;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#20197;&#31526;&#21512;&#38544;&#31169;&#27861;&#35268;&#12289;&#20197;&#21450;&#20943;&#36731;&#29256;&#26435;&#38382;&#39064;&#31561;&#25216;&#26415;&#30740;&#31350;&#12290;&#27861;&#24459;&#21644;&#25919;&#31574;&#30740;&#31350;&#34429;&#28982;&#20174;&#19981;&#21516;&#35282;&#24230;&#35299;&#20915;&#20102;&#30456;&#20851;&#25361;&#25112;&#65292;&#20294;&#19981;&#26159;&#26412;&#35843;&#30740;&#30340;&#37325;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#39318;&#20010;&#20851;&#27880;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38544;&#31169;&#38382;&#39064;&#30340;AI&#30740;&#31350;&#39046;&#22495;&#35843;&#30740;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#32418;&#38431;&#27169;&#22411;&#20197;&#31361;&#20986;&#38544;&#31169;&#39118;&#38505;&#30340;&#24037;&#20316;&#65292;&#23581;&#35797;&#23558;&#38544;&#31169;&#32435;&#20837;&#35757;&#32451;&#25110;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#24037;&#20316;&#65292;&#20351;&#24471;&#25968;&#25454;&#21487;&#20197;&#20174;&#35757;&#32451;&#27169;&#22411;&#20013;&#39640;&#25928;&#21024;&#38500;&#20197;&#31526;&#21512;&#29616;&#26377;&#30340;&#38544;&#31169;&#27861;&#35268;&#65292;&#24182;&#35797;&#22270;&#20943;&#36731;&#29256;&#26435;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#22312;&#20110;&#24635;&#32467;&#24320;&#21457;&#31639;&#27861;&#12289;&#35777;&#26126;&#23450;&#29702;&#21644;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#30340;&#25216;&#26415;&#30740;&#31350;&#12290;&#34429;&#28982;&#26377;&#22823;&#37327;&#30340;&#27861;&#24459;&#21644;&#25919;&#31574;&#30740;&#31350;&#20174;&#19981;&#21516;&#35282;&#24230;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#20294;&#36825;&#19981;&#26159;&#25105;&#20204;&#35843;&#30740;&#30340;&#37325;&#28857;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20316;&#21697;&#20197;&#21450;&#26368;&#36817;&#30340;&#27861;&#24459;&#36827;&#23637;&#30830;&#23454;&#24433;&#21709;&#20102;&#36825;&#20123;&#25216;&#26415;&#38382;&#39064;&#30340;&#24418;&#24335;&#21270;&#22788;&#29702;&#26041;&#24335;&#65292;&#22240;&#27492;&#25105;&#20204;&#22312;&#31532;&#19968;&#33410;&#20013;&#23545;&#20854;&#36827;&#34892;&#20102;&#31616;&#35201;&#35752;&#35770;&#12290;&#23613;&#31649;&#25105;&#20204;&#24050;&#32463;&#23613;&#21147;&#21253;&#21547;&#25152;&#26377;&#30456;&#20851;&#30740;&#31350;&#65292;&#20294;&#30001;&#20110;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#36827;&#23637;&#36805;&#36895;&#65292;&#25105;&#20204;&#21487;&#33021;&#20250;&#28431;&#25481;&#19968;&#20123;&#26368;&#26032;&#30340;&#30740;&#31350;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This is the first survey of the active area of AI research that focuses on privacy issues in Large Language Models (LLMs). Specifically, we focus on work that red-teams models to highlight privacy risks, attempts to build privacy into the training or inference process, enables efficient data deletion from trained models to comply with existing privacy regulations, and tries to mitigate copyright issues. Our focus is on summarizing technical research that develops algorithms, proves theorems, and runs empirical evaluations. While there is an extensive body of legal and policy work addressing these challenges from a different angle, that is not the focus of our survey. Nevertheless, these works, along with recent legal developments do inform how these technical problems are formalized, and so we discuss them briefly in Section 1. While we have made our best effort to include all the relevant work, due to the fast moving nature of this research we may have missed some recent work. If we h
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#24182;&#20998;&#26512;&#20102;&#27880;&#24847;&#21147;&#30697;&#38453;&#30340;&#20998;&#24067;&#21644;&#38598;&#20013;&#33021;&#21147;&#12290;&#36890;&#36807;&#24341;&#20837;&#32447;&#24615;&#23545;&#25968;&#27491;&#24577;&#27880;&#24847;&#21147;&#26469;&#27169;&#25311;&#21407;&#22987;&#33258;&#27880;&#24847;&#21147;&#30340;&#20998;&#24067;&#21644;&#38598;&#20013;&#34892;&#20026;&#65292;&#25552;&#39640;&#20102;Transformer&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.13541</link><description>&lt;p&gt;
&#32447;&#24615;&#23545;&#25968;&#27491;&#24577;&#27880;&#24847;&#21147;&#19982;&#26080;&#20559;&#38598;&#20013;&#21147;
&lt;/p&gt;
&lt;p&gt;
Linear Log-Normal Attention with Unbiased Concentration. (arXiv:2311.13541v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.13541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#24182;&#20998;&#26512;&#20102;&#27880;&#24847;&#21147;&#30697;&#38453;&#30340;&#20998;&#24067;&#21644;&#38598;&#20013;&#33021;&#21147;&#12290;&#36890;&#36807;&#24341;&#20837;&#32447;&#24615;&#23545;&#25968;&#27491;&#24577;&#27880;&#24847;&#21147;&#26469;&#27169;&#25311;&#21407;&#22987;&#33258;&#27880;&#24847;&#21147;&#30340;&#20998;&#24067;&#21644;&#38598;&#20013;&#34892;&#20026;&#65292;&#25552;&#39640;&#20102;Transformer&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#22797;&#26434;&#24230;&#19982;&#24207;&#21015;&#38271;&#24230;&#30340;&#20108;&#27425;&#20851;&#31995;&#65292;&#20854;&#21487;&#25193;&#23637;&#24615;&#21463;&#21040;&#38480;&#21046;&#12290;&#24403;&#22788;&#29702;&#38271;&#25991;&#26723;&#25110;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#26102;&#65292;&#36825;&#19968;&#38480;&#21046;&#26500;&#25104;&#20102;&#37325;&#22823;&#38556;&#30861;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#27880;&#24847;&#21147;&#30697;&#38453;&#30340;&#20998;&#24067;&#21644;&#38598;&#20013;&#33021;&#21147;&#65292;&#23545;&#33258;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#34913;&#37327;&#36825;&#20123;&#25968;&#37327;&#30340;&#24037;&#20855;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#21363;&#32447;&#24615;&#23545;&#25968;&#27491;&#24577;&#27880;&#24847;&#21147;&#65292;&#26088;&#22312;&#27169;&#25311;&#21407;&#22987;&#33258;&#27880;&#24847;&#21147;&#30340;&#20998;&#24067;&#21644;&#38598;&#20013;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;&#24120;&#29992;&#30340;&#33258;&#28982;&#35821;&#35328;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#32447;&#24615;&#23545;&#25968;&#27491;&#24577;&#27880;&#24847;&#21147;&#20248;&#20110;&#20854;&#20182;&#32447;&#24615;&#21270;&#27880;&#24847;&#21147;&#26367;&#20195;&#26041;&#27861;&#65292;&#20026;&#22686;&#24378;Transformer&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#38468;&#22312;&#34917;&#20805;&#26448;&#26009;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer models have achieved remarkable results in a wide range of applications. However, their scalability is hampered by the quadratic time and memory complexity of the self-attention mechanism concerning the sequence length. This limitation poses a substantial obstacle when dealing with long documents or high-resolution images. In this work, we study the self-attention mechanism by analyzing the distribution of the attention matrix and its concentration ability. Furthermore, we propose instruments to measure these quantities and introduce a novel self-attention mechanism, Linear Log-Normal Attention, designed to emulate the distribution and concentration behavior of the original self-attention. Our experimental results on popular natural language benchmarks reveal that our proposed Linear Log-Normal Attention outperforms other linearized attention alternatives, offering a promising avenue for enhancing the scalability of transformer models. Our code is available in supplementary
&lt;/p&gt;</description></item><item><title>&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#30005;&#21147;&#31995;&#32479;&#21487;&#33021;&#24102;&#26469;&#28508;&#22312;&#23433;&#20840;&#23041;&#32961;&#65292;&#38656;&#35201;&#36827;&#34892;&#36843;&#20999;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#30456;&#24212;&#23545;&#31574;&#12290;</title><link>http://arxiv.org/abs/2311.13361</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#30005;&#21147;&#31995;&#32479;&#65306;&#28508;&#22312;&#30340;&#23433;&#20840;&#23041;&#32961;
&lt;/p&gt;
&lt;p&gt;
Applying Large Language Models to Power Systems: Potential Security Threats. (arXiv:2311.13361v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.13361
&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#30005;&#21147;&#31995;&#32479;&#21487;&#33021;&#24102;&#26469;&#28508;&#22312;&#23433;&#20840;&#23041;&#32961;&#65292;&#38656;&#35201;&#36827;&#34892;&#36843;&#20999;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#30456;&#24212;&#23545;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24212;&#29992;&#20110;&#29616;&#20195;&#30005;&#21147;&#31995;&#32479;&#26159;&#25552;&#39640;&#20915;&#31574;&#21644;&#36816;&#33829;&#25928;&#29575;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#34892;&#21160;&#20063;&#21487;&#33021;&#24102;&#26469;&#28508;&#22312;&#30340;&#23433;&#20840;&#23041;&#32961;&#65292;&#36804;&#20170;&#23578;&#26410;&#23436;&#20840;&#35748;&#35782;&#21040;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20998;&#26512;&#20102;&#23558;LLMs&#24212;&#29992;&#20110;&#30005;&#21147;&#31995;&#32479;&#21487;&#33021;&#24102;&#26469;&#30340;&#28508;&#22312;&#23041;&#32961;&#65292;&#24182;&#24378;&#35843;&#20102;&#36843;&#20999;&#38656;&#35201;&#30740;&#31350;&#21644;&#24320;&#21457;&#30456;&#24212;&#23545;&#31574;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Applying large language models (LLMs) to modern power systems presents a promising avenue for enhancing decision-making and operational efficiency. However, this action may also incur potential security threats, which have not been fully recognized so far. To this end, this article analyzes potential threats incurred by applying LLMs to power systems, emphasizing the need for urgent research and development of countermeasures.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26368;&#23567;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#39640;&#32423;&#22768;&#26126;&#20013;&#23450;&#20041;&#25152;&#38656;&#30340;&#20195;&#29702;&#20154;&#34892;&#20026;&#65292;&#28982;&#21518;&#26500;&#24314;&#35299;&#30721;&#30417;&#35270;&#22120;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#20154;&#30340;&#24555;&#36895;&#35774;&#35745;&#21644;&#23454;&#26045;&#12290;</title><link>http://arxiv.org/abs/2310.08535</link><description>&lt;p&gt;
&#27491;&#24335;&#35268;&#23450;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#20154;&#30340;&#39640;&#32423;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Formally Specifying the High-Level Behavior of LLM-Based Agents. (arXiv:2310.08535v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26368;&#23567;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#39640;&#32423;&#22768;&#26126;&#20013;&#23450;&#20041;&#25152;&#38656;&#30340;&#20195;&#29702;&#20154;&#34892;&#20026;&#65292;&#28982;&#21518;&#26500;&#24314;&#35299;&#30721;&#30417;&#35270;&#22120;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#20154;&#30340;&#24555;&#36895;&#35774;&#35745;&#21644;&#23454;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;LLM&#39537;&#21160;&#30340;&#33258;&#20027;&#30446;&#26631;&#39537;&#21160;&#22411;&#20195;&#29702;&#20154;&#24050;&#25104;&#20026;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#38382;&#39064;&#30340;&#26377;&#21069;&#36884;&#30340;&#24037;&#20855;&#65292;&#32780;&#26080;&#38656;&#33719;&#24471;&#26114;&#36149;&#30340;&#20219;&#21153;&#29305;&#23450;&#30340;&#31934;&#32454;&#35843;&#25972;&#27169;&#22411;&#12290;&#30446;&#21069;&#65292;&#36825;&#31867;&#20195;&#29702;&#20154;&#30340;&#35774;&#35745;&#21644;&#23454;&#26045;&#26159;&#20020;&#26102;&#24615;&#30340;&#65292;&#22240;&#20026;LLM-based&#20195;&#29702;&#20154;&#21487;&#33021;&#24212;&#29992;&#20110;&#30340;&#21508;&#31181;&#20219;&#21153;&#30340;&#24191;&#27867;&#24615;&#36136;&#24847;&#21619;&#30528;&#19981;&#33021;&#26377;&#19968;&#31181;&#36866;&#29992;&#20110;&#25152;&#26377;&#24773;&#20917;&#30340;&#20195;&#29702;&#20154;&#35774;&#35745;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#31616;&#21270;&#20195;&#29702;&#20154;&#26500;&#24314;&#36807;&#31243;&#30340;&#26368;&#23567;&#29983;&#25104;&#26694;&#26550;&#26469;&#20943;&#36731;&#35774;&#35745;&#21644;&#23454;&#26045;&#26032;&#20195;&#29702;&#20154;&#30340;&#38590;&#24230;&#12290;&#25105;&#20204;&#24341;&#20837;&#30340;&#26694;&#26550;&#20801;&#35768;&#29992;&#25143;&#20197;&#39640;&#32423;&#22768;&#26126;&#30340;&#35268;&#33539;&#26041;&#24335;&#23450;&#20041;&#25152;&#38656;&#30340;&#20195;&#29702;&#20154;&#34892;&#20026;&#65292;&#28982;&#21518;&#20351;&#29992;&#36825;&#20010;&#35268;&#33539;&#26500;&#24314;&#35299;&#30721;&#30417;&#35270;&#22120;&#65292;&#20197;&#30830;&#20445;LLM&#20250;&#20135;&#29983;&#20855;&#26377;&#25152;&#38656;&#34892;&#20026;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#22768;&#26126;&#24615;&#26041;&#27861;&#65292;&#21363;&#25551;&#36848;&#34892;&#20026;&#32780;&#19981;&#32771;&#34385;&#22914;&#20309;&#23454;&#26045;&#25110;&#24378;&#21046;&#25191;&#34892;&#65292;&#21487;&#20197;&#23454;&#29616;&#24555;&#36895;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Autonomous, goal-driven agents powered by LLMs have recently emerged as promising tools for solving challenging problems without the need for task-specific finetuned models that can be expensive to procure. Currently, the design and implementation of such agents is ad hoc, as the wide variety of tasks that LLM-based agents may be applied to naturally means there can be no one-size-fits-all approach to agent design. In this work we aim to alleviate the difficulty of designing and implementing new agents by proposing a minimalistic generation framework that simplifies the process of building agents. The framework we introduce allows the user to define desired agent behaviors in a high-level, declarative specification that is then used to construct a decoding monitor which guarantees the LLM will produce an output exhibiting the desired behavior. Our declarative approach, in which the behavior is described without concern for how it should be implemented or enforced, enables rapid design,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25209;&#37327;&#26657;&#20934;&#65288;BC&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#31034;&#33030;&#24369;&#24615;&#21644;&#20559;&#35265;&#22240;&#32032;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;BC&#36890;&#36807;&#25511;&#21046;&#25209;&#37327;&#36755;&#20837;&#30340;&#19978;&#19979;&#25991;&#20559;&#35265;&#65292;&#32479;&#19968;&#20102;&#29616;&#26377;&#30340;&#26657;&#20934;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#38646;-shot&#21644;&#20165;&#25512;&#29702;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2309.17249</link><description>&lt;p&gt;
&#25209;&#37327;&#26657;&#20934;&#65306;&#37325;&#26032;&#24605;&#32771;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#25552;&#31034;&#24037;&#31243;&#30340;&#26657;&#20934;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Batch Calibration: Rethinking Calibration for In-Context Learning and Prompt Engineering. (arXiv:2309.17249v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25209;&#37327;&#26657;&#20934;&#65288;BC&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#31034;&#33030;&#24369;&#24615;&#21644;&#20559;&#35265;&#22240;&#32032;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;BC&#36890;&#36807;&#25511;&#21046;&#25209;&#37327;&#36755;&#20837;&#30340;&#19978;&#19979;&#25991;&#20559;&#35265;&#65292;&#32479;&#19968;&#20102;&#29616;&#26377;&#30340;&#26657;&#20934;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#38646;-shot&#21644;&#20165;&#25512;&#29702;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#24050;&#25104;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#39640;&#25928;&#23398;&#20064;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;LLM&#23384;&#22312;&#25552;&#31034;&#33030;&#24369;&#24615;&#21644;&#21508;&#31181;&#20559;&#35265;&#22240;&#32032;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#26684;&#24335;&#12289;&#36873;&#25321;&#24615;&#30340;&#34920;&#36798;&#26041;&#24335;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31034;&#20363;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#26657;&#20934;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#20123;&#20559;&#35265;&#30340;&#24433;&#21709;&#24182;&#24674;&#22797;LLM&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#29616;&#26377;&#30340;&#26657;&#20934;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#35266;&#28857;&#24182;&#25581;&#31034;&#20102;&#22833;&#36133;&#26696;&#20363;&#12290;&#21463;&#36825;&#20123;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25209;&#37327;&#26657;&#20934;&#65288;BC&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#30452;&#35266;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#25209;&#37327;&#36755;&#20837;&#20013;&#25511;&#21046;&#19978;&#19979;&#25991;&#20559;&#35265;&#65292;&#32479;&#19968;&#20102;&#21508;&#31181;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#19978;&#36848;&#38382;&#39064;&#12290;BC&#26159;&#38646;-shot&#12289;&#20165;&#25512;&#29702;&#21644;&#39069;&#22806;&#25104;&#26412;&#21487;&#24573;&#30053;&#12290;&#22312;&#23569;-shot&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25193;&#23637;BC&#20197;&#23454;&#29616;&#20840;&#37096;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Prompting and in-context learning (ICL) have become efficient learning paradigms for large language models (LLMs). However, LLMs suffer from prompt brittleness and various bias factors in the prompt, including but not limited to the formatting, the choice verbalizers, and the ICL examples. To address this problem that results in unexpected performance degradation, calibration methods have been developed to mitigate the effects of these biases while recovering LLM performance. In this work, we first conduct a systematic analysis of the existing calibration methods, where we both provide a unified view and reveal the failure cases. Inspired by these analyses, we propose Batch Calibration (BC), a simple yet intuitive method that controls the contextual bias from the batched input, unifies various prior approaches, and effectively addresses the aforementioned issues. BC is zero-shot, inference-only, and incurs negligible additional costs. In the few-shot setup, we further extend BC to allo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#29992;&#20110;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;Softmax&#25513;&#30721;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20854;&#32622;&#20449;&#24230;&#20445;&#25345;&#25928;&#26524;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#31283;&#23450;&#24615;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14808</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#29992;&#20110;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;Softmax&#25513;&#30721;&#20197;&#25552;&#39640;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Revisiting Softmax Masking for Stability in Continual Learning. (arXiv:2309.14808v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#29992;&#20110;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;Softmax&#25513;&#30721;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20854;&#32622;&#20449;&#24230;&#20445;&#25345;&#25928;&#26524;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#31283;&#23450;&#24615;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#65292;&#35768;&#22810;&#20998;&#31867;&#22120;&#20351;&#29992;Softmax&#20989;&#25968;&#26469;&#23398;&#20064;&#32622;&#20449;&#24230;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#30740;&#31350;&#25351;&#20986;&#20854;&#26080;&#27861;&#20934;&#30830;&#30830;&#23450;&#31163;&#32676;&#20540;&#30340;&#32622;&#20449;&#24230;&#20998;&#24067;&#65292;&#36890;&#24120;&#31216;&#20026;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#31181;&#22266;&#26377;&#38480;&#21046;&#36824;&#38480;&#21046;&#20102;&#22312;&#36830;&#32493;&#23398;&#20064;&#36807;&#31243;&#20013;&#36873;&#25321;&#20309;&#26102;&#24536;&#35760;&#21644;&#20445;&#30041;&#20808;&#21069;&#35757;&#32451;&#30340;&#32622;&#20449;&#24230;&#20998;&#24067;&#30340;&#20934;&#30830;&#20915;&#31574;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#25513;&#30721;Softmax&#20989;&#25968;&#30340;&#24433;&#21709;&#12290;&#23613;&#31649;&#36825;&#31181;&#26041;&#27861;&#22312;&#25991;&#29486;&#20013;&#26082;&#31616;&#21333;&#21448;&#26222;&#36941;&#65292;&#20294;&#23545;&#20110;&#22312;&#36830;&#32493;&#23398;&#20064;&#36807;&#31243;&#20013;&#20445;&#25345;&#32622;&#20449;&#24230;&#20998;&#24067;&#65288;&#20063;&#31216;&#20026;&#31283;&#23450;&#24615;&#65289;&#30340;&#24433;&#21709;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#35843;&#26597;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;Softmax&#25513;&#30721;&#30340;&#24433;&#21709;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#20854;&#32622;&#20449;&#24230;&#20445;&#25345;&#25928;&#26524;&#30340;&#26041;&#27861;&#12290;&#22312;&#20855;&#26377;&#21644;&#19981;&#20855;&#26377;&#35760;&#24518;&#37325;&#25918;&#30340;&#31867;-&#21644;&#20219;&#21153;&#22686;&#37327;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#22686;&#21152;&#20102;&#31283;&#23450;&#24615;&#21516;&#26102;&#20445;&#25345;&#20102;&#36275;&#22815;&#22823;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In continual learning, many classifiers use softmax function to learn confidence. However, numerous studies have pointed out its inability to accurately determine confidence distributions for outliers, often referred to as epistemic uncertainty. This inherent limitation also curtails the accurate decisions for selecting what to forget and keep in previously trained confidence distributions over continual learning process. To address the issue, we revisit the effects of masking softmax function. While this method is both simple and prevalent in literature, its implication for retaining confidence distribution during continual learning, also known as stability, has been under-investigated. In this paper, we revisit the impact of softmax masking, and introduce a methodology to utilize its confidence preservation effects. In class- and task-incremental learning benchmarks with and without memory replay, our approach significantly increases stability while maintaining sufficiently large pla
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#32763;&#35793;&#27169;&#22411;&#20013;&#30340;&#23646;&#24615;&#25511;&#21046;&#22120;&#36801;&#31227;&#21040;&#27809;&#26377;&#30417;&#30563;&#25968;&#25454;&#30340;&#35821;&#35328;&#12290;&#36890;&#36807;&#20840;&#38754;&#20998;&#26512;&#19981;&#21516;&#25968;&#25454;&#22330;&#26223;&#19979;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#26102;&#25511;&#21046;&#25216;&#26415;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#38646;&#26679;&#26412;&#24615;&#33021;&#21644;&#39046;&#22495;&#40065;&#26834;&#24615;&#19978;&#30340;&#30456;&#23545;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.08565</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#22810;&#35821;&#35328;&#32763;&#35793;&#27169;&#22411;&#19978;&#30340;&#23646;&#24615;&#25511;&#21046;&#22120;&#33021;&#21542;&#36801;&#31227;&#21040;&#20854;&#20182;&#35821;&#35328;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Transferable are Attribute Controllers on Pretrained Multilingual Translation Models?. (arXiv:2309.08565v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08565
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#32763;&#35793;&#27169;&#22411;&#20013;&#30340;&#23646;&#24615;&#25511;&#21046;&#22120;&#36801;&#31227;&#21040;&#27809;&#26377;&#30417;&#30563;&#25968;&#25454;&#30340;&#35821;&#35328;&#12290;&#36890;&#36807;&#20840;&#38754;&#20998;&#26512;&#19981;&#21516;&#25968;&#25454;&#22330;&#26223;&#19979;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#26102;&#25511;&#21046;&#25216;&#26415;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#38646;&#26679;&#26412;&#24615;&#33021;&#21644;&#39046;&#22495;&#40065;&#26834;&#24615;&#19978;&#30340;&#30456;&#23545;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23558;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#23450;&#21046;&#20026;&#31526;&#21512;&#32454;&#31890;&#24230;&#23646;&#24615;&#65288;&#22914;&#24418;&#24335;&#65289;&#24050;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#26041;&#27861;&#22823;&#22810;&#20381;&#36182;&#20110;&#33267;&#23569;&#19968;&#20123;&#24102;&#26377;&#23646;&#24615;&#27880;&#37322;&#30340;&#30417;&#30563;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#25968;&#25454;&#31232;&#32570;&#20173;&#28982;&#26159;&#23558;&#27492;&#23450;&#21046;&#33021;&#21147;&#26222;&#21450;&#21040;&#26356;&#24191;&#27867;&#35821;&#35328;&#33539;&#22260;&#65292;&#23588;&#20854;&#26159;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#19968;&#20010;&#29942;&#39048;&#12290;&#37492;&#20110;&#26368;&#36817;&#22312;&#39044;&#35757;&#32451;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#32763;&#35793;&#27169;&#22411;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#23558;&#23427;&#20204;&#20316;&#20026;&#23545;&#27809;&#26377;&#30417;&#30563;&#25968;&#25454;&#30340;&#35821;&#35328;&#36827;&#34892;&#23646;&#24615;&#25511;&#21046;&#33021;&#21147;&#36801;&#31227;&#30340;&#22522;&#30784;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;NLLB-200&#27169;&#22411;&#23545;&#23646;&#24615;&#25511;&#21046;&#22120;&#30340;&#36801;&#31227;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#21508;&#31181;&#25968;&#25454;&#22330;&#26223;&#19979;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#26102;&#25511;&#21046;&#25216;&#26415;&#65292;&#24182;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#38646;&#26679;&#26412;&#24615;&#33021;&#21644;&#39046;&#22495;&#40065;&#26834;&#24615;&#19978;&#30340;&#30456;&#23545;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;&#25105;&#20204;&#26174;&#31034;&#20986;&#20004;&#31181;&#33539;&#24335;&#26159;&#20114;&#34917;&#30340;&#65292;&#36890;&#36807;&#19968;&#33268;&#30340;&#25913;&#36827;&#26469;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Customizing machine translation models to comply with fine-grained attributes such as formality has seen tremendous progress recently. However, current approaches mostly rely on at least some supervised data with attribute annotation. Data scarcity therefore remains a bottleneck to democratizing such customization possibilities to a wider range of languages, lower-resource ones in particular. Given recent progress in pretrained massively multilingual translation models, we use them as a foundation to transfer the attribute controlling capabilities to languages without supervised data. In this work, we present a comprehensive analysis of transferring attribute controllers based on a pretrained NLLB-200 model. We investigate both training- and inference-time control techniques under various data scenarios, and uncover their relative strengths and weaknesses in zero-shot performance and domain robustness. We show that both paradigms are complementary, as shown by consistent improvements o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#23545;&#20110;&#23545;&#40784;&#21644;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32780;&#35328;&#65292;&#35774;&#35745;&#21453;&#39304;&#36873;&#25321;&#26159;&#35780;&#20998;&#36824;&#26159;&#25490;&#21517;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#35780;&#20998;&#21644;&#25490;&#21517;&#25152;&#25512;&#26029;&#20986;&#30340;&#20559;&#22909;&#23384;&#22312;&#19981;&#19968;&#33268;&#38382;&#39064;&#65292;&#24182;&#19988;&#27880;&#37322;&#32773;&#30340;&#20559;&#35265;&#20063;&#20250;&#24433;&#21709;&#32467;&#26524;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#21453;&#39304;&#21327;&#35758;&#30340;&#36873;&#25321;&#20063;&#23545;&#35780;&#20272;&#32467;&#26524;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.15812</link><description>&lt;p&gt;
&#36879;&#36807;&#20559;&#22909;&#30475;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39304;&#33719;&#21462;&#65306;&#25581;&#31034;&#23545;&#40784;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models. (arXiv:2308.15812v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#23545;&#20110;&#23545;&#40784;&#21644;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32780;&#35328;&#65292;&#35774;&#35745;&#21453;&#39304;&#36873;&#25321;&#26159;&#35780;&#20998;&#36824;&#26159;&#25490;&#21517;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#35780;&#20998;&#21644;&#25490;&#21517;&#25152;&#25512;&#26029;&#20986;&#30340;&#20559;&#22909;&#23384;&#22312;&#19981;&#19968;&#33268;&#38382;&#39064;&#65292;&#24182;&#19988;&#27880;&#37322;&#32773;&#30340;&#20559;&#35265;&#20063;&#20250;&#24433;&#21709;&#32467;&#26524;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#21453;&#39304;&#21327;&#35758;&#30340;&#36873;&#25321;&#20063;&#23545;&#35780;&#20272;&#32467;&#26524;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#21644;&#24847;&#22270;&#30340;&#23545;&#40784;&#25215;&#35834;&#28041;&#21450;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#25110;&#20154;&#31867;&#21453;&#39304;&#12290;&#31264;&#23494;&#30340;&#21453;&#39304;&#27880;&#37322;&#33719;&#21462;&#21644;&#25972;&#21512;&#25104;&#26412;&#36739;&#39640;&#65292;&#32780;&#31232;&#30095;&#30340;&#21453;&#39304;&#21017;&#28041;&#21450;&#32467;&#26500;&#24615;&#35774;&#35745;&#36873;&#25321;&#65292;&#21363;&#35780;&#20998;&#65288;&#20363;&#22914;&#65292;&#22312;1-7&#30340;&#33539;&#22260;&#20869;&#23545;&#22238;&#31572;A&#36827;&#34892;&#35780;&#20998;&#65289;&#21644;&#25490;&#21517;&#65288;&#20363;&#22914;&#65292;&#22238;&#31572;A&#26159;&#21542;&#27604;&#22238;&#31572;B&#26356;&#22909;&#65311;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#31181;&#35774;&#35745;&#36873;&#25321;&#23545;LLMs&#30340;&#23545;&#40784;&#21644;&#35780;&#20272;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35780;&#20998;&#21644;&#25490;&#21517;&#25152;&#25512;&#26029;&#20986;&#30340;&#20559;&#22909;&#22312;&#20154;&#31867;&#21644;AI&#27880;&#37322;&#32773;&#20013;&#37117;&#23384;&#22312;&#20005;&#37325;&#30340;&#19981;&#19968;&#33268;&#38382;&#39064;&#65292;&#36798;&#21040;&#20102;60%&#12290;&#25105;&#20204;&#30340;&#21518;&#32493;&#20998;&#26512;&#30830;&#23450;&#20102;&#35299;&#37322;&#36825;&#20010;&#29616;&#35937;&#30340;&#21508;&#31181;&#27880;&#37322;&#32773;&#20559;&#35265;&#26041;&#38754;&#65292;&#27604;&#22914;&#20154;&#31867;&#27880;&#37322;&#32773;&#26356;&#21916;&#27426;&#23494;&#38598;&#22238;&#31572;&#24182;&#22312;&#20004;&#20010;&#36873;&#39033;&#20043;&#38388;&#26356;&#38738;&#30544;&#20934;&#30830;&#24615;&#12290;&#20196;&#25105;&#20204;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#21453;&#39304;&#21327;&#35758;&#30340;&#36873;&#25321;&#23545;&#23545;&#40784;&#30340;LLMs&#30340;&#35780;&#20272;&#20063;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#30340;&#35780;&#20272;&#32467;&#26524;&#22240;&#20026;&#21453;&#39304;&#21327;&#35758;&#30340;&#36873;&#25321;&#32780;&#26377;&#25152;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aligning large language models (LLMs) with human values and intents critically involves the use of human or AI feedback. While dense feedback annotations are expensive to acquire and integrate, sparse feedback presents a structural design choice between ratings (e.g., score Response A on a scale of 1-7) and rankings (e.g., is Response A better than Response B?). In this work, we analyze the effect of this design choice for the alignment and evaluation of LLMs. We uncover an inconsistency problem wherein the preferences inferred from ratings and rankings significantly disagree 60% for both human and AI annotators. Our subsequent analysis identifies various facets of annotator biases that explain this phenomena, such as human annotators would rate denser responses higher while preferring accuracy during pairwise judgments. To our surprise, we also observe that the choice of feedback protocol also has a significant effect on the evaluation of aligned LLMs. In particular, we find that LLMs
&lt;/p&gt;</description></item><item><title>CALM&#26159;&#19968;&#20010;&#29992;&#20110;&#37327;&#21270;&#35821;&#35328;&#27169;&#22411;&#20559;&#35265;&#30340;&#22810;&#20219;&#21153;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#30456;&#27604;&#20808;&#21069;&#25968;&#25454;&#38598;&#26356;&#21152;&#22810;&#26679;&#21644;&#21487;&#38752;&#65292;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#35780;&#20272;&#27169;&#22411;&#20559;&#35265;&#25152;&#38656;&#30340;&#35821;&#35328;&#21464;&#21270;&#30340;&#24191;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.12539</link><description>&lt;p&gt;
CALM: &#19968;&#31181;&#29992;&#20110;&#20840;&#38754;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20559;&#35265;&#30340;&#22810;&#20219;&#21153;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CALM : A Multi-task Benchmark for Comprehensive Assessment of Language Model Bias. (arXiv:2308.12539v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12539
&lt;/p&gt;
&lt;p&gt;
CALM&#26159;&#19968;&#20010;&#29992;&#20110;&#37327;&#21270;&#35821;&#35328;&#27169;&#22411;&#20559;&#35265;&#30340;&#22810;&#20219;&#21153;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#30456;&#27604;&#20808;&#21069;&#25968;&#25454;&#38598;&#26356;&#21152;&#22810;&#26679;&#21644;&#21487;&#38752;&#65292;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#35780;&#20272;&#27169;&#22411;&#20559;&#35265;&#25152;&#38656;&#30340;&#35821;&#35328;&#21464;&#21270;&#30340;&#24191;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#19981;&#26029;&#22686;&#24378;&#65292;&#37327;&#21270;&#21644;&#27604;&#36739;&#23427;&#20204;&#22312;&#31038;&#20250;&#21644;&#20154;&#21475;&#23398;&#20559;&#35265;&#26041;&#38754;&#30340;&#33021;&#21147;&#20197;&#21450;&#28508;&#22312;&#30340;&#21361;&#23475;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#20808;&#21069;&#30340;&#20559;&#35265;&#27979;&#37327;&#25968;&#25454;&#38598;&#23545;&#20110;&#20154;&#24037;&#35774;&#35745;&#27169;&#26495;&#30340;&#25200;&#21160;&#25935;&#24863;&#65292;&#22240;&#27492;&#19981;&#21487;&#38752;&#12290;&#20026;&#20102;&#20445;&#35777;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20840;&#38754;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20559;&#35265;&#65288;CALM&#65289;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#37327;&#21270;LMs&#22312;&#19977;&#20010;&#20219;&#21153;&#19978;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#25972;&#21512;&#20102;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#65288;&#22914;&#32500;&#22522;&#30334;&#31185;&#21644;&#26032;&#38395;&#25991;&#31456;&#65289;&#30340;16&#20010;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#36807;&#28388;&#20986;224&#20010;&#27169;&#26495;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;78,400&#20010;&#31034;&#20363;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#24179;&#22343;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;&#27169;&#26495;&#38271;&#24230;&#30340;&#21464;&#24322;&#31243;&#24230;&#31561;&#25351;&#26631;&#65292;&#27604;&#36739;CALM&#19982;&#20808;&#21069;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#27979;&#35797;&#20854;&#23545;&#32454;&#24494;&#25200;&#21160;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#30456;&#23545;&#20110;&#20808;&#21069;&#25968;&#25454;&#38598;&#26356;&#21152;&#22810;&#26679;&#21644;&#21487;&#38752;&#65292;&#22240;&#27492;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#35780;&#20272;&#27169;&#22411;&#20559;&#35265;&#25152;&#38656;&#30340;&#35821;&#35328;&#21464;&#21270;&#30340;&#24191;&#24230;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;20&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
As language models (LMs) become increasingly powerful, it is important to quantify and compare them for sociodemographic bias with potential for harm. Prior bias measurement datasets are sensitive to perturbations in their manually designed templates, therefore unreliable. To achieve reliability, we introduce the Comprehensive Assessment of Language Model bias (CALM), a benchmark dataset to quantify bias in LMs across three tasks. We integrate 16 existing datasets across different domains, such as Wikipedia and news articles, to filter 224 templates from which we construct a dataset of 78,400 examples. We compare the diversity of CALM with prior datasets on metrics such as average semantic similarity, and variation in template length, and test the sensitivity to small perturbations. We show that our dataset is more diverse and reliable than previous datasets, thus better capture the breadth of linguistic variation required to reliably evaluate model bias. We evaluate 20 large language 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32593;&#32476;&#22810;&#26234;&#33021;&#20307;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#38382;&#39064;&#20013;&#30340;&#20998;&#24067;&#24335;&#21160;&#24577;&#35268;&#21010;&#21644;&#20998;&#24067;&#24335;TD&#23398;&#20064;&#31639;&#27861;&#12290;&#20854;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#20998;&#24067;&#24335;DP&#31639;&#27861;&#21644;&#20998;&#24067;&#24335;TD&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#25910;&#25947;&#24615;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#20851;&#38190;&#28857;&#12290;&#35813;&#20998;&#24067;&#24335;DP&#31639;&#27861;&#20855;&#26377;&#20004;&#20010;&#29420;&#31435;&#30340;&#21160;&#24577;&#31995;&#32479;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2307.16706</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#21160;&#24577;&#35268;&#21010;&#21644;&#20998;&#24067;&#24335;TD-Learning&#30340;&#32593;&#32476;&#22810;&#26234;&#33021;&#20307;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;ODE&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Distributed Dynamic Programming and an O.D.E. Framework of Distributed TD-Learning for Networked Multi-Agent Markov Decision Processes. (arXiv:2307.16706v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32593;&#32476;&#22810;&#26234;&#33021;&#20307;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#38382;&#39064;&#20013;&#30340;&#20998;&#24067;&#24335;&#21160;&#24577;&#35268;&#21010;&#21644;&#20998;&#24067;&#24335;TD&#23398;&#20064;&#31639;&#27861;&#12290;&#20854;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#20998;&#24067;&#24335;DP&#31639;&#27861;&#21644;&#20998;&#24067;&#24335;TD&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#25910;&#25947;&#24615;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#20851;&#38190;&#28857;&#12290;&#35813;&#20998;&#24067;&#24335;DP&#31639;&#27861;&#20855;&#26377;&#20004;&#20010;&#29420;&#31435;&#30340;&#21160;&#24577;&#31995;&#32479;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#32593;&#32476;&#22810;&#26234;&#33021;&#20307;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#38382;&#39064;&#20013;&#30340;&#20998;&#24067;&#24335;&#21160;&#24577;&#35268;&#21010;&#65288;DP&#65289;&#21644;&#20998;&#24067;&#24335;&#26102;&#24207;&#24046;&#20998;&#65288;TD&#65289;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#20998;&#24067;&#24335;&#22810;&#26234;&#33021;&#20307;&#26694;&#26550;&#65292;&#20854;&#20013;&#21508;&#20010;&#26234;&#33021;&#20307;&#21482;&#33021;&#35775;&#38382;&#33258;&#24049;&#30340;&#22870;&#21169;&#65292;&#32570;&#20047;&#23545;&#20854;&#20182;&#26234;&#33021;&#20307;&#22870;&#21169;&#30340;&#20102;&#35299;&#12290;&#27492;&#22806;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#37117;&#33021;&#36890;&#36807;&#19968;&#20010;&#30001;&#22270;&#34920;&#31034;&#30340;&#36890;&#20449;&#32593;&#32476;&#19982;&#30456;&#37051;&#26234;&#33021;&#20307;&#20849;&#20139;&#20854;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#21487;&#20197;&#24635;&#32467;&#20026;&#20004;&#20010;&#20851;&#38190;&#28857;&#65306;1&#65289;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#21463;&#36830;&#32493;&#26102;&#38388;&#21306;&#38388;&#20869;&#30340;&#24179;&#22343;&#19968;&#33268;&#24615;&#26041;&#27861;&#21551;&#21457;&#30340;&#20998;&#24067;&#24335;DP&#12290;&#36890;&#36807;&#25511;&#21046;&#29702;&#35770;&#30340;&#35270;&#35282;&#35780;&#20272;&#20102;&#35813;DP&#30340;&#25910;&#25947;&#24615;&#12290;2&#65289;&#22522;&#20110;&#19978;&#36848;DP&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#24067;&#24335;TD&#23398;&#20064;&#31639;&#27861;&#24182;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#20998;&#24067;&#24335;DP&#30340;&#19968;&#20010;&#26174;&#33879;&#29305;&#28857;&#26159;&#20854;&#21253;&#21547;&#20102;&#20004;&#20010;&#29420;&#31435;&#30340;&#21160;&#24577;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
The primary objective of this paper is to investigate distributed dynamic programming (DP) and distributed temporal difference (TD) learning algorithms for networked multi-agent Markov decision problems (MAMDPs). In our study, we adopt a distributed multi-agent framework where individual agents have access only to their own rewards, lacking insights into the rewards of other agents. Additionally, each agent has the ability to share its parameters with neighboring agents through a communication network, represented by a graph. Our contributions can be summarized in two key points: 1) We introduce a novel distributed DP, inspired by the averaging consensus method in the continuous-time domain. The convergence of this DP is assessed through control theory perspectives. 2) Building upon the aforementioned DP, we devise a new distributed TD-learning algorithm and prove its convergence. A standout feature of our proposed distributed DP is its incorporation of two independent dynamic systems,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#22270;&#35770;&#32422;&#26463;&#30340;&#32972;&#21253;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#24182;&#25552;&#20986;&#20102;&#36817;&#20284;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.12547</link><description>&lt;p&gt;
&#32972;&#21253;&#38382;&#39064;&#65306;&#36830;&#36890;&#24615;&#12289;&#36335;&#24452;&#21644;&#26368;&#30701;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
Knapsack: Connectedness, Path, and Shortest-Path. (arXiv:2307.12547v2 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12547
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#22270;&#35770;&#32422;&#26463;&#30340;&#32972;&#21253;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#24182;&#25552;&#20986;&#20102;&#36817;&#20284;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#24102;&#26377;&#22270;&#35770;&#32422;&#26463;&#30340;&#32972;&#21253;&#38382;&#39064;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#25105;&#20204;&#20551;&#35774;&#32972;&#21253;&#39033;&#30446;&#38598;&#19978;&#23384;&#22312;&#19968;&#20010;&#22270;&#32467;&#26500;&#65292;&#24182;&#19988;&#35299;&#20915;&#26041;&#26696;&#36824;&#38656;&#35201;&#28385;&#36275;&#32972;&#21253;&#32422;&#26463;&#20043;&#19978;&#30340;&#26576;&#20123;&#22270;&#35770;&#24615;&#36136;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#36830;&#36890;&#32972;&#21253;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#38656;&#35201;&#35745;&#31639;&#19968;&#20010;&#36830;&#36890;&#30340;&#39033;&#30446;&#23376;&#38598;&#65292;&#20854;&#20215;&#20540;&#26368;&#22823;&#65292;&#19988;&#28385;&#36275;&#32972;&#21253;&#32422;&#26463;&#30340;&#22823;&#23567;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#21363;&#20351;&#23545;&#20110;&#26368;&#22823;&#24230;&#20026;&#22235;&#30340;&#22270;&#65292;&#36825;&#20010;&#38382;&#39064;&#20063;&#26159;&#24378;NP&#23436;&#20840;&#30340;&#65292;&#23545;&#20110;&#26143;&#24418;&#22270;&#65292;&#36825;&#20010;&#38382;&#39064;&#20063;&#26159;NP&#23436;&#20840;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#36816;&#34892;&#26102;&#38388;&#20026;$O\left(2^{tw\log tw}\cdot\text{poly}(\min\{s^2,d^2\})\right)$&#30340;&#31639;&#27861;&#65292;&#22312;&#20854;&#20013;$tw,s,d$&#20998;&#21035;&#26159;&#22270;&#30340;&#26641;&#23485;&#24230;&#65292;&#32972;&#21253;&#30340;&#22823;&#23567;&#21644;&#30446;&#26631;&#20540;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#19968;&#20010;$(1-\epsilon)$&#36817;&#20284;&#31639;&#27861;&#65292;&#20854;&#36816;&#34892;&#26102;&#38388;&#20026;$O\left(2^{tw\log tw}\cdot\text{poly}(n,1/\epsilon)\right)$&#65292;&#23545;&#20110;&#27599;&#20010;$\epsilon&gt;0$&#37117;&#25104;&#31435;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#23545;&#20960;&#20010;&#20854;&#20182;&#22270;&#35770;&#24615;&#36136;&#30340;&#31867;&#20284;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the knapsack problem with graph theoretic constraints. That is, we assume that there exists a graph structure on the set of items of knapsack and the solution also needs to satisfy certain graph theoretic properties on top of knapsack constraints. In particular, we need to compute in the connected knapsack problem a connected subset of items which has maximum value subject to the size of knapsack constraint. We show that this problem is strongly NP-complete even for graphs of maximum degree four and NP-complete even for star graphs. On the other hand, we develop an algorithm running in time $O\left(2^{tw\log tw}\cdot\text{poly}(\min\{s^2,d^2\})\right)$ where $tw,s,d$ are respectively treewidth of the graph, size, and target value of the knapsack. We further exhibit a $(1-\epsilon)$ factor approximation algorithm running in time $O\left(2^{tw\log tw}\cdot\text{poly}(n,1/\epsilon)\right)$ for every $\epsilon&gt;0$. We show similar results for several other graph theoretic propertie
&lt;/p&gt;</description></item><item><title>VELMA&#26159;&#19968;&#20010;&#21475;&#22836;&#21270;&#30340;LLM&#26234;&#33021;&#20307;&#65292;&#21033;&#29992;&#20154;&#31867;&#20889;&#20316;&#30340;&#23548;&#33322;&#25351;&#20196;&#20013;&#30340;&#22320;&#26631;&#24182;&#32467;&#21512;CLIP&#26469;&#36827;&#34892;&#35270;&#35273;&#29615;&#22659;&#30340;&#29702;&#35299;&#65292;&#20197;&#23454;&#29616;&#22312;&#34903;&#26223;&#20013;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#23548;&#33322;&#12290;</title><link>http://arxiv.org/abs/2307.06082</link><description>&lt;p&gt;
VELMA: LLM&#26234;&#33021;&#20307;&#22312;&#34903;&#26223;&#20013;&#36827;&#34892;&#35270;&#35273;&#21644;&#35821;&#35328;&#23548;&#33322;&#30340;&#21475;&#22836;&#21270;&#20307;&#29616;
&lt;/p&gt;
&lt;p&gt;
VELMA: Verbalization Embodiment of LLM Agents for Vision and Language Navigation in Street View. (arXiv:2307.06082v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06082
&lt;/p&gt;
&lt;p&gt;
VELMA&#26159;&#19968;&#20010;&#21475;&#22836;&#21270;&#30340;LLM&#26234;&#33021;&#20307;&#65292;&#21033;&#29992;&#20154;&#31867;&#20889;&#20316;&#30340;&#23548;&#33322;&#25351;&#20196;&#20013;&#30340;&#22320;&#26631;&#24182;&#32467;&#21512;CLIP&#26469;&#36827;&#34892;&#35270;&#35273;&#29615;&#22659;&#30340;&#29702;&#35299;&#65292;&#20197;&#23454;&#29616;&#22312;&#34903;&#26223;&#20013;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#30340;&#22686;&#37327;&#20915;&#31574;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20197;&#20307;&#29616;&#20154;&#24037;&#26234;&#33021;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#20854;&#20013;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#20043;&#19968;&#26159;&#35270;&#35273;&#21644;&#35821;&#35328;&#23548;&#33322;(VLN)&#65292;&#23427;&#38656;&#35201;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20197;&#21450;&#31354;&#38388;&#21644;&#26102;&#38388;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#20010;&#20307;&#29616;&#26234;&#33021;&#20307;&#38656;&#35201;&#22312;&#34903;&#26223;&#31561;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#30340;&#35266;&#23519;&#22522;&#30784;&#19978;&#20934;&#30830;&#29702;&#35299;&#23548;&#33322;&#25351;&#20196;&#12290;&#23613;&#31649;LLM&#22312;&#20854;&#20182;&#30740;&#31350;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20294;&#22914;&#20309;&#26368;&#22909;&#22320;&#23558;&#23427;&#20204;&#19982;&#20132;&#20114;&#24335;&#35270;&#35273;&#29615;&#22659;&#36830;&#25509;&#36215;&#26469;&#20173;&#28982;&#26159;&#19968;&#20010;&#25345;&#32493;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VELMA&#65292;&#19968;&#31181;&#20351;&#29992;&#36712;&#36857;&#21644;&#35270;&#35273;&#29615;&#22659;&#35266;&#23519;&#30340;&#21475;&#22836;&#21270;&#20316;&#20026;&#19979;&#19968;&#27493;&#25805;&#20316;&#30340;&#19978;&#19979;&#25991;&#25552;&#31034;&#30340;LLM&#26234;&#33021;&#20307;&#12290;&#35270;&#35273;&#20449;&#24687;&#36890;&#36807;&#19968;&#20010;&#27969;&#31243;&#36827;&#34892;&#21475;&#22836;&#21270;&#65292;&#35813;&#27969;&#31243;&#20174;&#20154;&#31867;&#32534;&#20889;&#30340;&#23548;&#33322;&#25351;&#20196;&#20013;&#25552;&#21462;&#22320;&#26631;&#65292;&#24182;&#20351;&#29992;CLIP&#26469;&#30830;&#23450;&#23427;&#20204;&#22312;&#24403;&#21069;&#20840;&#26223;&#35270;&#22270;&#20013;&#30340;&#21487;&#35265;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incremental decision making in real-world environments is one of the most challenging tasks in embodied artificial intelligence. One particularly demanding scenario is Vision and Language Navigation~(VLN) which requires visual and natural language understanding as well as spatial and temporal reasoning capabilities. The embodied agent needs to ground its understanding of navigation instructions in observations of a real-world environment like Street View. Despite the impressive results of LLMs in other research areas, it is an ongoing problem of how to best connect them with an interactive visual environment. In this work, we propose VELMA, an embodied LLM agent that uses a verbalization of the trajectory and of visual environment observations as contextual prompt for the next action. Visual information is verbalized by a pipeline that extracts landmarks from the human written navigation instructions and uses CLIP to determine their visibility in the current panorama view. We show that
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20107;&#20214;&#20449;&#24687;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#25340;&#36710;&#65292;&#36890;&#36807;&#39044;&#27979;&#21644;&#36866;&#24212;&#38656;&#27714;&#28608;&#22686;&#65292;&#24182;&#29983;&#25104;&#21327;&#21516;&#30340;&#36335;&#30001;&#21644;&#25509;&#20056;&#31574;&#30053;&#26469;&#26381;&#21153;&#26356;&#22810;&#35831;&#27714;&#12290;</title><link>http://arxiv.org/abs/2307.02637</link><description>&lt;p&gt;
Surge Routing&#65306;&#22522;&#20110;&#20107;&#20214;&#20449;&#24687;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#33258;&#21160;&#25340;&#36710;
&lt;/p&gt;
&lt;p&gt;
Surge Routing: Event-informed Multiagent Reinforcement Learning for Autonomous Rideshare. (arXiv:2307.02637v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02637
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20107;&#20214;&#20449;&#24687;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#25340;&#36710;&#65292;&#36890;&#36807;&#39044;&#27979;&#21644;&#36866;&#24212;&#38656;&#27714;&#28608;&#22686;&#65292;&#24182;&#29983;&#25104;&#21327;&#21516;&#30340;&#36335;&#30001;&#21644;&#25509;&#20056;&#31574;&#30053;&#26469;&#26381;&#21153;&#26356;&#22810;&#35831;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#20107;&#20214;&#36890;&#24120;&#20250;&#24341;&#21457;&#25340;&#36710;&#26381;&#21153;&#38656;&#27714;&#30340;&#28608;&#22686;&#65292;&#32780;&#36825;&#20123;&#38656;&#27714;&#19981;&#20250;&#34987;&#24179;&#22343;&#38656;&#27714;&#27169;&#24335;&#25152;&#25429;&#25417;&#21040;&#65292;&#32473;&#36335;&#30001;&#31639;&#27861;&#24102;&#26469;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#20986;&#31199;&#36710;&#36710;&#38431;&#65292;&#20174;&#20114;&#32852;&#32593;&#19978;&#33719;&#21462;&#20107;&#20214;&#25968;&#25454;&#20197;&#39044;&#27979;&#21644;&#36866;&#24212;&#38656;&#27714;&#28608;&#22686;&#65292;&#24182;&#29983;&#25104;&#21327;&#21516;&#30340;&#36335;&#30001;&#21644;&#25509;&#20056;&#31574;&#30053;&#65292;&#20197;&#26381;&#21153;&#26356;&#22810;&#30340;&#35831;&#27714;&#27604;&#20854;&#20182;&#36335;&#30001;&#21327;&#35758;&#12290;&#25105;&#20204;&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65306;(i)&#19968;&#20010;&#20107;&#20214;&#22788;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#20114;&#32852;&#32593;&#19978;&#25235;&#21462;&#20107;&#20214;&#20449;&#24687;&#24182;&#29983;&#25104;&#23494;&#38598;&#30340;&#21521;&#37327;&#34920;&#31034;&#65292;&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#29305;&#24449;&#26469;&#39044;&#27979;&#38656;&#27714;&#65307;(ii)&#19968;&#20010;&#21452;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#65292;&#20351;&#29992;&#36825;&#20123;&#23494;&#38598;&#30340;&#21521;&#37327;&#34920;&#31034;&#26469;&#39044;&#27979;&#25972;&#20010;&#22320;&#22270;&#19978;&#30340;&#27599;&#23567;&#26102;&#38656;&#27714;&#65307;(iii)&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#21306;&#22495;&#21344;&#29992;&#26102;&#38388;&#34920;&#23558;&#20844;&#24320;&#30340;&#38656;&#27714;&#25968;&#25454;&#26144;&#23556;&#21040;&#31163;&#25955;&#21270;&#30340;&#34903;&#36947;&#20132;&#21449;&#21475;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large events such as conferences, concerts and sports games, often cause surges in demand for ride services that are not captured in average demand patterns, posing unique challenges for routing algorithms. We propose a learning framework for an autonomous fleet of taxis that scrapes event data from the internet to predict and adapt to surges in demand and generates cooperative routing and pickup policies that service a higher number of requests than other routing protocols. We achieve this through a combination of (i) an event processing framework that scrapes the internet for event information and generates dense vector representations that can be used as input features for a neural network that predicts demand; (ii) a two neural network system that predicts hourly demand over the entire map, using these dense vector representations; (iii) a probabilistic approach that leverages locale occupancy schedules to map publicly available demand data over sectors to discretized street inters
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26080;&#30417;&#30563;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#20107;&#20214;&#26085;&#24535;&#36716;&#25442;&#20026;&#24102;&#23646;&#24615;&#12289;&#26377;&#21521;&#21644;&#21152;&#26435;&#30340;&#22270;&#65292;&#24182;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22270;&#32423;&#21035;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;OCDiGCN&#26469;&#26816;&#27979;&#19968;&#32452;&#24102;&#23646;&#24615;&#12289;&#26377;&#21521;&#21644;&#21152;&#26435;&#30340;&#22270;&#20013;&#30340;&#22270;&#32423;&#21035;&#24322;&#24120;&#65292;&#24182;&#25552;&#20379;&#23545;&#24322;&#24120;&#30340;&#35299;&#37322;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.00527</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#19982;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks based Log Anomaly Detection and Explanation. (arXiv:2307.00527v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00527
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26080;&#30417;&#30563;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#20107;&#20214;&#26085;&#24535;&#36716;&#25442;&#20026;&#24102;&#23646;&#24615;&#12289;&#26377;&#21521;&#21644;&#21152;&#26435;&#30340;&#22270;&#65292;&#24182;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22270;&#32423;&#21035;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;OCDiGCN&#26469;&#26816;&#27979;&#19968;&#32452;&#24102;&#23646;&#24615;&#12289;&#26377;&#21521;&#21644;&#21152;&#26435;&#30340;&#22270;&#20013;&#30340;&#22270;&#32423;&#21035;&#24322;&#24120;&#65292;&#24182;&#25552;&#20379;&#23545;&#24322;&#24120;&#30340;&#35299;&#37322;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#26085;&#24535;&#34987;&#24191;&#27867;&#29992;&#20110;&#35760;&#24405;&#39640;&#31185;&#25216;&#31995;&#32479;&#30340;&#29366;&#24577;&#65292;&#22240;&#27492;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#23545;&#20110;&#30417;&#25511;&#36825;&#20123;&#31995;&#32479;&#38750;&#24120;&#37325;&#35201;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#23558;&#26085;&#24535;&#20107;&#20214;&#35745;&#25968;&#30697;&#38453;&#25110;&#26085;&#24535;&#20107;&#20214;&#24207;&#21015;&#20316;&#20026;&#36755;&#20837;&#65292;&#21033;&#29992;&#26085;&#24535;&#20107;&#20214;&#20043;&#38388;&#30340;&#23450;&#37327;&#21644;/&#25110;&#39034;&#24207;&#20851;&#31995;&#26469;&#26816;&#27979;&#24322;&#24120;&#12290;&#28982;&#32780;&#65292;&#20165;&#32771;&#34385;&#23450;&#37327;&#25110;&#39034;&#24207;&#20851;&#31995;&#21487;&#33021;&#23548;&#33268;&#26816;&#27979;&#20934;&#30830;&#24615;&#36739;&#20302;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#26080;&#30417;&#30563;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#31216;&#20026;Logs2Graphs&#65292;&#23427;&#39318;&#20808;&#23558;&#20107;&#20214;&#26085;&#24535;&#36716;&#25442;&#20026;&#24102;&#23646;&#24615;&#12289;&#26377;&#21521;&#21644;&#21152;&#26435;&#30340;&#22270;&#65292;&#28982;&#21518;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22270;&#32423;&#21035;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;One-Class Digraph Inception Convolutional Networks&#65288;OCDiGCN&#65289;&#65292;&#29992;&#20110;&#22312;&#19968;&#32452;&#24102;&#23646;&#24615;&#12289;&#26377;&#21521;&#21644;&#21152;&#26435;&#30340;&#22270;&#20013;&#26816;&#27979;&#22270;&#32423;&#21035;&#30340;&#24322;&#24120;&#12290;&#36890;&#36807;&#23558;&#22270;&#34920;&#31034;&#19982;&#23646;&#24615;&#34920;&#31034;&#32806;&#21512;&#36215;&#26469;&#65292;&#22312;&#22270;&#32423;&#21035;&#19978;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#21516;&#26102;&#25552;&#20379;&#20102;&#23545;&#24322;&#24120;&#30340;&#35299;&#37322;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event logs are widely used to record the status of high-tech systems, making log anomaly detection important for monitoring those systems. Most existing log anomaly detection methods take a log event count matrix or log event sequences as input, exploiting quantitative and/or sequential relationships between log events to detect anomalies. Unfortunately, only considering quantitative or sequential relationships may result in low detection accuracy. To alleviate this problem, we propose a graph-based method for unsupervised log anomaly detection, dubbed Logs2Graphs, which first converts event logs into attributed, directed, and weighted graphs, and then leverages graph neural networks to perform graph-level anomaly detection. Specifically, we introduce One-Class Digraph Inception Convolutional Networks, abbreviated as OCDiGCN, a novel graph neural network model for detecting graph-level anomalies in a collection of attributed, directed, and weighted graphs. By coupling the graph represe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#21487;&#38752;&#30340;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#25351;&#26631;TopP\&amp;R&#65292;&#36890;&#36807;&#24341;&#20837;&#25299;&#25169;&#21644;&#32479;&#35745;&#22788;&#29702;&#36827;&#34892;&#20005;&#26684;&#30340;&#25903;&#25345;&#20272;&#35745;&#12290;TopP\&amp;R&#20165;&#20445;&#30041;&#20855;&#26377;&#19968;&#23450;&#32622;&#20449;&#27700;&#24179;&#30340;&#20855;&#26377;&#25299;&#25169;&#21644;&#32479;&#35745;&#19978;&#37325;&#35201;&#24615;&#30340;&#29305;&#24449;&#65292;&#23545;&#20110;&#22122;&#22768;&#29305;&#24449;&#20855;&#26377;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#32479;&#35745;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.08013</link><description>&lt;p&gt;
TopP\&amp;R: &#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#25903;&#25345;&#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#20445;&#30495;&#24230;&#21644;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
TopP\&amp;R: Robust Support Estimation Approach for Evaluating Fidelity and Diversity in Generative Models. (arXiv:2306.08013v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#21487;&#38752;&#30340;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#25351;&#26631;TopP\&amp;R&#65292;&#36890;&#36807;&#24341;&#20837;&#25299;&#25169;&#21644;&#32479;&#35745;&#22788;&#29702;&#36827;&#34892;&#20005;&#26684;&#30340;&#25903;&#25345;&#20272;&#35745;&#12290;TopP\&amp;R&#20165;&#20445;&#30041;&#20855;&#26377;&#19968;&#23450;&#32622;&#20449;&#27700;&#24179;&#30340;&#20855;&#26377;&#25299;&#25169;&#21644;&#32479;&#35745;&#19978;&#37325;&#35201;&#24615;&#30340;&#29305;&#24449;&#65292;&#23545;&#20110;&#22122;&#22768;&#29305;&#24449;&#20855;&#26377;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#32479;&#35745;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#21487;&#38752;&#30340;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#25351;&#26631;&#65292;&#36890;&#36807;&#24341;&#20837;&#25299;&#25169;&#21644;&#32479;&#35745;&#22788;&#29702;&#36827;&#34892;&#20005;&#26684;&#30340;&#25903;&#25345;&#20272;&#35745;&#12290;&#29616;&#26377;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#22914;Inception Score&#65288;IS&#65289;&#65292;Fr\'echet Inception Distance&#65288;FID&#65289;&#20197;&#21450;Precision and Recall&#65288;P\&amp;R&#65289;&#30340;&#21464;&#20307;&#65292;&#20005;&#37325;&#20381;&#36182;&#20110;&#20174;&#26679;&#26412;&#29305;&#24449;&#20272;&#35745;&#30340;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#35780;&#20272;&#30340;&#36136;&#37327;&#23436;&#20840;&#21462;&#20915;&#20110;&#20854;&#21487;&#38752;&#24615;&#65292;&#20294;&#20854;&#20272;&#35745;&#30340;&#21487;&#38752;&#24615;&#24182;&#27809;&#26377;&#24471;&#21040;&#20005;&#32899;&#30340;&#35752;&#35770;&#65288;&#24182;&#34987;&#24573;&#35270;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#25299;&#25169;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#65288;TopP\&amp;R&#65292;&#21457;&#38899;&#20026;&#8220;topper&#8221;&#65289;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#25903;&#25345;&#65292;&#20165;&#20445;&#30041;&#20855;&#26377;&#19968;&#23450;&#32622;&#20449;&#27700;&#24179;&#30340;&#20855;&#26377;&#25299;&#25169;&#21644;&#32479;&#35745;&#19978;&#37325;&#35201;&#24615;&#30340;&#29305;&#24449;&#12290;&#36825;&#19981;&#20165;&#20351;TopP\&amp;R&#23545;&#20110;&#22122;&#22768;&#29305;&#24449;&#20855;&#26377;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#65292;&#32780;&#19988;&#36824;&#25552;&#20379;&#20102;&#32479;&#35745;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TopP\&amp;R&#23545;&#20110;&#31163;&#32676;&#20540;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a robust and reliable evaluation metric for generative models by introducing topological and statistical treatments for rigorous support estimation. Existing metrics, such as Inception Score (IS), Fr\'echet Inception Distance (FID), and the variants of Precision and Recall (P\&amp;R), heavily rely on supports that are estimated from sample features. However, the reliability of their estimation has not been seriously discussed (and overlooked) even though the quality of the evaluation entirely depends on it. In this paper, we propose Topological Precision and Recall (TopP\&amp;R, pronounced 'topper'), which provides a systematic approach to estimating supports, retaining only topologically and statistically important features with a certain level of confidence. This not only makes TopP\&amp;R strong for noisy features, but also provides statistical consistency. Our theoretical and experimental results show that TopP\&amp;R is robust to outliers and non-independent and identically distributed
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#20174;&#20165;&#26377;&#23569;&#37327;&#26681;&#22240;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;DAGs&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#21487;&#35782;&#21035;&#24615;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.15936</link><description>&lt;p&gt;
&#20174;&#23569;&#37327;&#26681;&#22240;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;DAGs
&lt;/p&gt;
&lt;p&gt;
Learning DAGs from Data with Few Root Causes. (arXiv:2305.15936v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15936
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#20174;&#20165;&#26377;&#23569;&#37327;&#26681;&#22240;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;DAGs&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#21487;&#35782;&#21035;&#24615;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35282;&#24230;&#21644;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#32447;&#24615;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;(SEM)&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#26377;&#21521;&#26080;&#29615;&#22270;(DAGs)&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#32447;&#24615;SEM&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#31181;&#32447;&#24615;&#21464;&#25442;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#20351;&#29992;&#19968;&#20010;&#30001;&#19982;&#33410;&#28857;&#20851;&#32852;&#30340;&#38543;&#26426;&#20540;&#26681;&#22240;(&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;)&#30340;&#31264;&#23494;&#36755;&#20837;&#21521;&#37327;&#35745;&#31639;&#25968;&#25454;&#12290;&#25105;&#20204;&#32771;&#34385;&#20165;&#23384;&#22312;&#20960;&#20010;&#26681;&#22240;(&#36817;&#20284;)&#30340;&#24773;&#20917;&#65292;&#24182;&#22312;&#25968;&#25454;&#30340;&#27979;&#37327;&#20013;&#24341;&#20837;&#22122;&#22768;&#12290;&#20174;&#30452;&#35273;&#19978;&#35762;&#65292;&#36825;&#24847;&#21619;&#30528;DAG&#25968;&#25454;&#26159;&#30001;&#23569;&#25968;&#25968;&#25454;&#29983;&#25104;&#20107;&#20214;&#20135;&#29983;&#30340;&#65292;&#20854;&#25928;&#26524;&#36890;&#36807;DAG&#20256;&#25773;&#12290;&#25105;&#20204;&#22312;&#36825;&#31181;&#26032;&#35774;&#32622;&#20013;&#35777;&#26126;&#21487;&#35782;&#21035;&#24615;&#65292;&#24182;&#34920;&#26126;&#30495;&#27491;&#30340;DAG&#26159;&#26681;&#22240;&#21521;&#37327;L0-&#33539;&#25968;&#30340;&#20840;&#23616;&#26368;&#23567;&#21270;&#32773;&#12290;&#23545;&#20110;&#20855;&#26377;&#23569;&#37327;&#26681;&#22240;&#30340;&#25968;&#25454;&#65292;&#26377;&#21644;&#27809;&#26377;&#22122;&#38899;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#34920;&#29616;&#20986;&#27604;&#20197;&#21069;&#30340;DAG&#23398;&#20064;&#26041;&#27861;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel perspective and algorithm for learning directed acyclic graphs (DAGs) from data generated by a linear structural equation model (SEM). First, we show that a linear SEM can be viewed as a linear transform that, in prior work, computes the data from a dense input vector of random valued root causes (as we will call them) associated with the nodes. Instead, we consider the case of (approximately) few root causes and also introduce noise in the measurement of the data. Intuitively, this means that the DAG data is produced by few data-generating events whose effect percolates through the DAG. We prove identifiability in this new setting and show that the true DAG is the global minimizer of the $L^0$-norm of the vector of root causes. For data with few root causes, with and without noise, we show superior performance compared to prior DAG learning methods.
&lt;/p&gt;</description></item><item><title>&#25991;&#31456;&#37325;&#26032;&#32771;&#34385;&#20102;&#22522;&#20110;GNN&#30340;&#24322;&#26500;&#30693;&#35782;&#22270;&#35889;&#23454;&#20307;&#23545;&#40784;&#12290;&#20026;&#25506;&#31350;EA&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#65292;&#25552;&#20986;&#20102;&#26356;&#25509;&#36817;&#29616;&#23454;&#30340;&#39640;&#24230;&#24322;&#26500;&#30693;&#35782;&#22270;&#35889;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.03468</link><description>&lt;p&gt;
&#37325;&#26032;&#32771;&#34385;&#22522;&#20110;GNN&#30340;&#24322;&#26500;&#30693;&#35782;&#22270;&#35889;&#23454;&#20307;&#23545;&#40784;&#65306;&#26032;&#25968;&#25454;&#38598;&#21644;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rethinking GNN-based Entity Alignment on Heterogeneous Knowledge Graphs: New Datasets and A New Method. (arXiv:2304.03468v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03468
&lt;/p&gt;
&lt;p&gt;
&#25991;&#31456;&#37325;&#26032;&#32771;&#34385;&#20102;&#22522;&#20110;GNN&#30340;&#24322;&#26500;&#30693;&#35782;&#22270;&#35889;&#23454;&#20307;&#23545;&#40784;&#12290;&#20026;&#25506;&#31350;EA&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#65292;&#25552;&#20986;&#20102;&#26356;&#25509;&#36817;&#29616;&#23454;&#30340;&#39640;&#24230;&#24322;&#26500;&#30693;&#35782;&#22270;&#35889;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#24212;&#29992;&#30340;&#21457;&#23637;&#23548;&#33268;&#20102;&#38656;&#35201;&#20174;&#21508;&#31181;&#26469;&#28304;&#25552;&#21462;&#30340;&#24322;&#26500;KG&#20043;&#38388;&#30340;&#23454;&#20307;&#23545;&#40784;&#65288;EA&#65289;&#30340;&#19981;&#26029;&#22686;&#38271;&#38656;&#27714;&#12290;&#36817;&#26469;&#65292;&#30001;&#20110;GNN&#30340;&#20986;&#33394;&#32467;&#26500;&#20449;&#24687;&#25429;&#25417;&#33021;&#21147;&#65292;&#22312;EA&#20219;&#21153;&#20013;&#24191;&#27867;&#37319;&#29992;GNN&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#29616;&#26377;&#24120;&#35265;EA&#25968;&#25454;&#38598;&#30340;&#36807;&#20110;&#31616;&#21333;&#21270;&#30340;&#35774;&#32622;&#19982;&#29616;&#23454;&#22330;&#26223;&#30456;&#36317;&#29978;&#36828;&#65292;&#36825;&#22952;&#30861;&#20102;&#23545;&#26368;&#36817;&#26041;&#27861;&#25152;&#21462;&#24471;&#36827;&#23637;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;&#36825;&#31181;&#29616;&#35937;&#20351;&#25105;&#20204;&#28145;&#24605;&#65306;&#29616;&#26377;&#22522;&#20110;GNN&#30340;EA&#26041;&#27861;&#26159;&#21542;&#30495;&#30340;&#21462;&#24471;&#20102;&#20255;&#22823;&#36827;&#23637;&#65311;&#20026;&#20102;&#30740;&#31350;EA&#26041;&#27861;&#22312;&#29616;&#23454;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#65292;&#26412;&#25991;&#32858;&#28966;&#20110;&#39640;&#24230;&#24322;&#26500;&#30340;KG&#65288;HHKG&#65289;&#65288;&#20363;&#22914;&#65292;&#20107;&#20214;KG&#21644;&#36890;&#29992;KG&#65289;&#30340;&#23545;&#40784;&#65292;&#36825;&#20123;KG&#22312;&#35268;&#27169;&#21644;&#32467;&#26500;&#19978;&#19981;&#21516;&#65292;&#24182;&#20849;&#20139;&#26356;&#23569;&#30340;&#37325;&#21472;&#23454;&#20307;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#28165;&#29702;&#20102;&#19981;&#21512;&#29702;&#30340;&#35774;&#32622;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;HHKG&#25968;&#25454;&#38598;&#65292;&#20854;&#23494;&#20999;&#22320;&#27169;&#25311;&#20102;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of knowledge graph (KG) applications has led to a rising need for entity alignment (EA) between heterogeneous KGs that are extracted from various sources. Recently, graph neural networks (GNNs) have been widely adopted in EA tasks due to GNNs' impressive ability to capture structure information. However, we have observed that the oversimplified settings of the existing common EA datasets are distant from real-world scenarios, which obstructs a full understanding of the advancements achieved by recent methods. This phenomenon makes us ponder: Do existing GNN-based EA methods really make great progress?  In this paper, to study the performance of EA methods in realistic settings, we focus on the alignment of highly heterogeneous KGs (HHKGs) (e.g., event KGs and general KGs) which are different with regard to the scale and structure, and share fewer overlapping entities. First, we sweep the unreasonable settings, and propose two new HHKG datasets that closely mimic real-wo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#22825;&#32447;&#31995;&#32479;&#20013;&#37319;&#29992;&#25968;&#23383;&#35843;&#21046;&#21644;&#31354;&#20013;&#35745;&#31639;&#30340;&#24773;&#20917;&#19979;&#65292;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#32467;&#21512;&#25968;&#23383;&#35843;&#21046;&#21644;AirComp&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#32852;&#37030;&#24179;&#22343;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#26080;&#32447;&#20449;&#36947;&#34928;&#33853;&#23548;&#33268;&#30340;&#24635;&#20307;&#22833;&#30495;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.14648</link><description>&lt;p&gt;
&#22810;&#22825;&#32447;&#31995;&#32479;&#20013;&#25968;&#23383;&#26080;&#32447;&#21327;&#20316;&#32852;&#37030;&#23398;&#20064;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Digital Over-the-Air Federated Learning in Multi-Antenna Systems. (arXiv:2302.14648v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#22825;&#32447;&#31995;&#32479;&#20013;&#37319;&#29992;&#25968;&#23383;&#35843;&#21046;&#21644;&#31354;&#20013;&#35745;&#31639;&#30340;&#24773;&#20917;&#19979;&#65292;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#32467;&#21512;&#25968;&#23383;&#35843;&#21046;&#21644;AirComp&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#32852;&#37030;&#24179;&#22343;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#26080;&#32447;&#20449;&#36947;&#34928;&#33853;&#23548;&#33268;&#30340;&#24635;&#20307;&#22833;&#30495;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23454;&#38469;&#30340;&#26080;&#32447;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#65288;MIMO&#65289;&#36890;&#20449;&#31995;&#32479;&#20013;&#65292;&#37319;&#29992;&#25968;&#23383;&#35843;&#21046;&#21644;&#31354;&#20013;&#35745;&#31639;&#65288;AirComp&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;&#29305;&#21035;&#32771;&#34385;&#20102;&#19968;&#20010;MIMO&#31995;&#32479;&#65292;&#22312;&#35813;&#31995;&#32479;&#20013;&#65292;&#36793;&#32536;&#35774;&#22791;&#20351;&#29992;&#27874;&#26463;&#24418;&#25104;&#23558;&#20854;&#26412;&#22320;&#25910;&#38598;&#30340;&#25968;&#25454;&#35757;&#32451;&#30340;&#26412;&#22320;FL&#27169;&#22411;&#20256;&#36755;&#32473;&#21442;&#25968;&#26381;&#21153;&#22120;&#65288;PS&#65289;&#65292;&#20197;&#26368;&#22823;&#21270;&#21487;&#35843;&#24230;&#20256;&#36755;&#30340;&#35774;&#22791;&#25968;&#37327;&#12290;PS&#20316;&#20026;&#20013;&#22830;&#25511;&#21046;&#22120;&#65292;&#20351;&#29992;&#25509;&#25910;&#21040;&#30340;&#26412;&#22320;FL&#27169;&#22411;&#29983;&#25104;&#19968;&#20010;&#20840;&#23616;FL&#27169;&#22411;&#24182;&#24191;&#25773;&#32473;&#25152;&#26377;&#35774;&#22791;&#12290;&#30001;&#20110;&#26080;&#32447;&#32593;&#32476;&#20013;&#30340;&#24102;&#23485;&#26377;&#38480;&#65292;&#37319;&#29992;&#20102;AirComp&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#26080;&#32447;&#25968;&#25454;&#32858;&#21512;&#12290;&#28982;&#32780;&#65292;&#26080;&#32447;&#20449;&#36947;&#30340;&#34928;&#33853;&#20250;&#20135;&#29983;&#22522;&#20110;AirComp&#30340;FL&#26041;&#26696;&#20013;&#30340;&#24635;&#20307;&#22833;&#30495;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#32852;&#37030;&#24179;&#22343;&#65288;FedAvg&#65289;&#31639;&#27861;&#65292;&#23558;&#25968;&#23383;&#35843;&#21046;&#19982;AirComp&#30456;&#32467;&#21512;&#20197;&#20943;&#36731;&#22833;&#30495;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, the performance optimization of federated learning (FL), when deployed over a realistic wireless multiple-input multiple-output (MIMO) communication system with digital modulation and over-the-air computation (AirComp) is studied. In particular, a MIMO system is considered in which edge devices transmit their local FL models (trained using their locally collected data) to a parameter server (PS) using beamforming to maximize the number of devices scheduled for transmission. The PS, acting as a central controller, generates a global FL model using the received local FL models and broadcasts it back to all devices. Due to the limited bandwidth in a wireless network, AirComp is adopted to enable efficient wireless data aggregation. However, fading of wireless channels can produce aggregate distortions in an AirComp-based FL scheme. To tackle this challenge, we propose a modified federated averaging (FedAvg) algorithm that combines digital modulation with AirComp to mitigate
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Walsh&#31995;&#25968;&#36924;&#36817;&#20915;&#31574;&#36793;&#30028;&#30340;&#23545;&#25239;&#25915;&#20987;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#28165;&#26224;&#22270;&#20687;&#21644;&#23545;&#25239;&#22270;&#20687;&#20043;&#38388;&#30340;Walsh&#31995;&#25968;&#36924;&#36817;&#24046;&#24322;&#65292;&#23454;&#29616;&#20102;&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2211.10227</link><description>&lt;p&gt;
&#20351;&#29992;&#38598;&#25104;&#36793;&#30028;&#36924;&#36817;&#30340;&#23545;&#25239;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Adversarial Detection by Approximation of Ensemble Boundary. (arXiv:2211.10227v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Walsh&#31995;&#25968;&#36924;&#36817;&#20915;&#31574;&#36793;&#30028;&#30340;&#23545;&#25239;&#25915;&#20987;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#28165;&#26224;&#22270;&#20687;&#21644;&#23545;&#25239;&#22270;&#20687;&#20043;&#38388;&#30340;Walsh&#31995;&#25968;&#36924;&#36817;&#24046;&#24322;&#65292;&#23454;&#29616;&#20102;&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#25915;&#20987;&#26816;&#27979;&#26041;&#27861;&#65292;&#38024;&#23545;&#35299;&#20915;&#20004;&#31867;&#27169;&#24335;&#35782;&#21035;&#38382;&#39064;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#38598;&#25104;&#12290;&#35813;&#38598;&#25104;&#20351;&#29992;Walsh&#31995;&#25968;&#36827;&#34892;&#32452;&#21512;&#65292;&#33021;&#22815;&#36924;&#36817;&#24067;&#23572;&#20989;&#25968;&#24182;&#25511;&#21046;&#38598;&#25104;&#20915;&#31574;&#36793;&#30028;&#30340;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#30340;&#20551;&#35774;&#26159;&#39640;&#26354;&#29575;&#30340;&#20915;&#31574;&#36793;&#30028;&#20801;&#35768;&#25214;&#21040;&#23545;&#25239;&#25200;&#21160;&#65292;&#20294;&#20250;&#25913;&#21464;&#20915;&#31574;&#36793;&#30028;&#30340;&#26354;&#29575;&#65292;&#32780;&#19982;&#28165;&#26224;&#22270;&#20687;&#30456;&#27604;&#65292;&#20351;&#29992;Walsh&#31995;&#25968;&#23545;&#20854;&#36827;&#34892;&#36924;&#36817;&#30340;&#26041;&#24335;&#20063;&#26377;&#25152;&#19981;&#21516;&#12290;&#36890;&#36807;&#35266;&#23519;&#28165;&#26224;&#22270;&#20687;&#21644;&#23545;&#25239;&#22270;&#20687;&#20043;&#38388;&#30340;Walsh&#31995;&#25968;&#36924;&#36817;&#24046;&#24322;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#25915;&#20987;&#30340;&#21487;&#36801;&#31227;&#24615;&#21487;&#29992;&#20110;&#26816;&#27979;&#12290;&#27492;&#22806;&#65292;&#36924;&#36817;&#20915;&#31574;&#36793;&#30028;&#21487;&#33021;&#26377;&#21161;&#20110;&#29702;&#35299;DNN&#30340;&#23398;&#20064;&#21644;&#21487;&#36801;&#31227;&#24615;&#29305;&#24615;&#12290;&#23613;&#31649;&#26412;&#25991;&#30340;&#23454;&#39564;&#20351;&#29992;&#22270;&#20687;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#24314;&#27169;&#20004;&#31867;&#27169;&#24335;&#35782;&#21035;&#38382;&#39064;&#30340;&#38598;&#25104;&#36793;&#30028;&#36924;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;
A new method of detecting adversarial attacks is proposed for an ensemble of Deep Neural Networks (DNNs) solving two-class pattern recognition problems. The ensemble is combined using Walsh coefficients which are capable of approximating Boolean functions and thereby controlling the complexity of the ensemble decision boundary. The hypothesis in this paper is that decision boundaries with high curvature allow adversarial perturbations to be found, but change the curvature of the decision boundary, which is then approximated in a different way by Walsh coefficients compared to the clean images. By observing the difference in Walsh coefficient approximation between clean and adversarial images, it is shown experimentally that transferability of attack may be used for detection. Furthermore, approximating the decision boundary may aid in understanding the learning and transferability properties of DNNs. While the experiments here use images, the proposed approach of modelling two-class en
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26641;&#38598;&#21512;&#27169;&#22411;&#36716;&#25442;&#20026;&#21487;&#35299;&#37322;&#35268;&#21017;&#21015;&#34920;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35299;&#37322;&#27169;&#22411;&#23545;&#20110;&#23569;&#25968;&#31867;&#21035;&#30340;&#39044;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;TE2Rules&#26041;&#27861;&#29983;&#25104;&#30340;&#35268;&#21017;&#21015;&#34920;&#20934;&#30830;&#24615;&#36739;&#39640;&#65292;&#24182;&#19988;&#36816;&#34892;&#26102;&#38388;&#19982;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2206.14359</link><description>&lt;p&gt;
TE2Rules: &#20351;&#29992;&#35268;&#21017;&#35299;&#37322;&#26641;&#38598;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TE2Rules: Explaining Tree Ensembles using Rules. (arXiv:2206.14359v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.14359
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26641;&#38598;&#21512;&#27169;&#22411;&#36716;&#25442;&#20026;&#21487;&#35299;&#37322;&#35268;&#21017;&#21015;&#34920;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35299;&#37322;&#27169;&#22411;&#23545;&#20110;&#23569;&#25968;&#31867;&#21035;&#30340;&#39044;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;TE2Rules&#26041;&#27861;&#29983;&#25104;&#30340;&#35268;&#21017;&#21015;&#34920;&#20934;&#30830;&#24615;&#36739;&#39640;&#65292;&#24182;&#19988;&#36816;&#34892;&#26102;&#38388;&#19982;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26641;&#38598;&#21512;&#65288;&#22914;&#26799;&#24230;&#25552;&#21319;&#26641;&#65289;&#36890;&#24120;&#30456;&#27604;&#21333;&#26869;&#20915;&#31574;&#26641;&#20855;&#26377;&#26356;&#39640;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#28982;&#32780;&#65292;&#26641;&#38598;&#21512;&#27169;&#22411;&#36890;&#24120;&#32570;&#20047;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#22240;&#20026;&#20154;&#31867;&#38590;&#20197;&#29702;&#35299;&#23427;&#20204;&#30340;&#20915;&#31574;&#36923;&#36753;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#29992;&#20110;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#30340;&#26641;&#38598;&#21512;&#27169;&#22411;&#36716;&#25442;&#20026;&#25509;&#36817;&#26641;&#38598;&#21512;&#30340;&#21487;&#35299;&#37322;&#35268;&#21017;&#21015;&#34920;&#65292;&#24182;&#19988;&#26377;&#25928;&#22320;&#35299;&#37322;&#27169;&#22411;&#23545;&#20110;&#27169;&#22411;&#39044;&#27979;&#30340;&#23569;&#25968;&#31867;&#21035;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;TE2Rules&#29983;&#25104;&#30340;&#35268;&#21017;&#21015;&#34920;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;TE2Rules&#30340;&#36816;&#34892;&#26102;&#38388;&#19982;&#20854;&#20182;&#31867;&#20284;&#22522;&#32447;&#26041;&#27861;&#30456;&#24403;&#65292;TE2Rules&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#21487;&#20197;&#20197;&#31245;&#24494;&#38477;&#20302;&#30340;&#20934;&#30830;&#24615;&#20026;&#20195;&#20215;&#36827;&#34892;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tree Ensemble (TE) models (like Gradient Boosted Trees) often provide higher prediction performance compared to single decision trees. However, TE models generally lack transparency and interpretability, as humans have difficulty understanding their decision logic. This paper presents a novel approach to convert a TE trained for a binary classification task, to a rule list (RL) that closely approximates the TE and is interpretable for a human. This RL can effectively explain the model even on the minority class predicted by the model. Experiments on benchmark datasets demonstrate that, (i) predictions from the RL generated by TE2Rules have higher fidelity (with respect to the original TE) compared to state-of-the-art methods, (ii) the run-time of TE2Rules is comparable to that of some other similar baselines and (iii) the run-time of TE2Rules algorithm can be traded off at the cost of a slightly lower fidelity.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#30456;&#23545;&#31574;&#30053;&#36807;&#28193;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#24341;&#29702;&#34913;&#37327;&#20004;&#20010;MDP&#20043;&#38388;&#30340;&#30456;&#23545;&#24046;&#36317;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#23545;&#31574;&#30053;&#20248;&#21270;&#21644;&#30456;&#23545;&#36716;&#31227;&#20248;&#21270;&#20004;&#20010;&#31639;&#27861;&#26469;&#23454;&#29616;&#24555;&#36895;&#31574;&#30053;&#36716;&#31227;&#21644;&#21160;&#24577;&#24314;&#27169;&#12290;&#21516;&#26102;&#65292;&#23558;&#36825;&#20004;&#20010;&#31639;&#27861;&#38598;&#25104;&#22312;&#19968;&#36215;&#24418;&#25104;&#23436;&#25972;&#30340;&#30456;&#23545;&#31574;&#30053;&#36807;&#28193;&#20248;&#21270;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2206.06009</link><description>&lt;p&gt;
&#24555;&#36895;&#31574;&#30053;&#36716;&#31227;&#30340;&#30456;&#23545;&#31574;&#30053;&#36807;&#28193;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Relative Policy-Transition Optimization for Fast Policy Transfer. (arXiv:2206.06009v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.06009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#30456;&#23545;&#31574;&#30053;&#36807;&#28193;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#24341;&#29702;&#34913;&#37327;&#20004;&#20010;MDP&#20043;&#38388;&#30340;&#30456;&#23545;&#24046;&#36317;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#23545;&#31574;&#30053;&#20248;&#21270;&#21644;&#30456;&#23545;&#36716;&#31227;&#20248;&#21270;&#20004;&#20010;&#31639;&#27861;&#26469;&#23454;&#29616;&#24555;&#36895;&#31574;&#30053;&#36716;&#31227;&#21644;&#21160;&#24577;&#24314;&#27169;&#12290;&#21516;&#26102;&#65292;&#23558;&#36825;&#20004;&#20010;&#31639;&#27861;&#38598;&#25104;&#22312;&#19968;&#36215;&#24418;&#25104;&#23436;&#25972;&#30340;&#30456;&#23545;&#31574;&#30053;&#36807;&#28193;&#20248;&#21270;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#22312;&#20004;&#20010;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20043;&#38388;&#30340;&#31574;&#30053;&#36716;&#31227;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#29616;&#26377;&#29702;&#35770;&#32467;&#26524;&#30340;&#24341;&#29702;&#65292;&#29992;&#20110;&#34913;&#37327;&#20219;&#24847;&#20004;&#20010;MDP&#20043;&#38388;&#30340;&#30456;&#23545;&#24046;&#36317;&#65292;&#21363;&#19981;&#21516;&#31574;&#30053;&#21644;&#29615;&#22659;&#21160;&#24577;&#19979;&#32047;&#31215;&#39044;&#26399;&#22238;&#25253;&#30340;&#24046;&#24322;&#12290;&#22522;&#20110;&#35813;&#24341;&#29702;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#31639;&#27861;&#65292;&#20998;&#21035;&#20026;&#30456;&#23545;&#31574;&#30053;&#20248;&#21270;&#65288;RPO&#65289;&#21644;&#30456;&#23545;&#36716;&#31227;&#20248;&#21270;&#65288;RTO&#65289;&#65292;&#20998;&#21035;&#25552;&#20379;&#20102;&#24555;&#36895;&#31574;&#30053;&#36716;&#31227;&#21644;&#21160;&#24577;&#24314;&#27169;&#12290;RPO&#36890;&#36807;&#23558;&#22312;&#19968;&#20010;&#29615;&#22659;&#20013;&#35780;&#20272;&#30340;&#31574;&#30053;&#36716;&#31227;&#20197;&#26368;&#22823;&#21270;&#22312;&#21478;&#19968;&#20010;&#29615;&#22659;&#20013;&#30340;&#22238;&#25253;&#65292;&#32780;RTO&#21017;&#36890;&#36807;&#26356;&#26032;&#21442;&#25968;&#21270;&#30340;&#21160;&#24577;&#27169;&#22411;&#26469;&#20943;&#23567;&#20004;&#20010;&#29615;&#22659;&#21160;&#24577;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#23558;&#36825;&#20004;&#20010;&#31639;&#27861;&#38598;&#25104;&#22312;&#19968;&#36215;&#21487;&#20197;&#24471;&#21040;&#23436;&#25972;&#30340;&#30456;&#23545;&#31574;&#30053;&#36807;&#28193;&#20248;&#21270;&#65288;RPTO&#65289;&#31639;&#27861;&#65292;&#20854;&#20013;&#31574;&#30053;&#21516;&#26102;&#19982;&#20004;&#20010;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#65292;&#20174;&#32780;&#20174;&#20004;&#20010;&#29615;&#22659;&#20013;&#25910;&#38598;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of policy transfer between two Markov Decision Processes (MDPs). We introduce a lemma based on existing theoretical results in reinforcement learning to measure the relativity gap between two arbitrary MDPs, that is the difference between any two cumulative expected returns defined on different policies and environment dynamics. Based on this lemma, we propose two new algorithms referred to as Relative Policy Optimization (RPO) and Relative Transition Optimization (RTO), which offer fast policy transfer and dynamics modelling, respectively. RPO transfers the policy evaluated in one environment to maximize the return in another, while RTO updates the parameterized dynamics model to reduce the gap between the dynamics of the two environments. Integrating the two algorithms results in the complete Relative Policy-Transition Optimization (RPTO) algorithm, in which the policy interacts with the two environments simultaneously, such that data collections from two envi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24635;&#32467;&#20102;&#22810;&#27169;&#24335;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#30340;&#26041;&#27861;&#12289;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#30001;&#20110;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#30340;&#36716;&#21464;&#65292;&#34394;&#20551;&#20449;&#24687;&#30340;&#24615;&#36136;&#20063;&#21457;&#29983;&#20102;&#21464;&#21270;&#12290;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#24320;&#21457;&#20986;&#33258;&#21160;&#26816;&#27979;&#36328;&#27169;&#24577;&#19981;&#21327;&#35843;&#30340;&#25216;&#26415;&#65292;&#20294;&#20173;&#38754;&#20020;&#25361;&#25112;&#21644;&#19981;&#36275;&#20043;&#22788;&#65292;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#26426;&#20250;&#20063;&#22312;&#31561;&#24453;&#30528;&#25366;&#25496;&#12290;</title><link>http://arxiv.org/abs/2203.13883</link><description>&lt;p&gt;
&#22810;&#27169;&#24335;&#30340;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#65306;&#26041;&#27861;&#12289;&#25361;&#25112;&#19982;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Multi-modal Misinformation Detection: Approaches, Challenges and Opportunities. (arXiv:2203.13883v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.13883
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24635;&#32467;&#20102;&#22810;&#27169;&#24335;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#30340;&#26041;&#27861;&#12289;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#30001;&#20110;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#30340;&#36716;&#21464;&#65292;&#34394;&#20551;&#20449;&#24687;&#30340;&#24615;&#36136;&#20063;&#21457;&#29983;&#20102;&#21464;&#21270;&#12290;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#24320;&#21457;&#20986;&#33258;&#21160;&#26816;&#27979;&#36328;&#27169;&#24577;&#19981;&#21327;&#35843;&#30340;&#25216;&#26415;&#65292;&#20294;&#20173;&#38754;&#20020;&#25361;&#25112;&#21644;&#19981;&#36275;&#20043;&#22788;&#65292;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#26426;&#20250;&#20063;&#22312;&#31561;&#24453;&#30528;&#25366;&#25496;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#20174;&#25991;&#26412;&#20026;&#20027;&#30340;&#35770;&#22363;&#36716;&#21521;&#22810;&#27169;&#24335;&#29615;&#22659;&#65292;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#34394;&#20551;&#20449;&#24687;&#30340;&#24615;&#36136;&#20063;&#30456;&#24212;&#21457;&#29983;&#20102;&#21464;&#21270;&#12290;&#21033;&#29992;&#22270;&#20687;&#21644;&#35270;&#39057;&#31561;&#35270;&#35273;&#27169;&#24577;&#26356;&#21463;&#29992;&#25143;&#38738;&#30544;&#21644;&#21560;&#24341;&#21147;&#30340;&#20107;&#23454;&#65292;&#20197;&#21450;&#25991;&#26412;&#20869;&#23481;&#26377;&#26102;&#34987;&#31895;&#30053;&#27983;&#35272;&#30340;&#24773;&#20917;&#65292;&#34394;&#20551;&#20449;&#24687;&#20256;&#25773;&#32773;&#26368;&#36817;&#24320;&#22987;&#38024;&#23545;&#27169;&#24577;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#36830;&#25509;&#65292;&#20363;&#22914;&#25991;&#26412;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#24320;&#21457;&#20986;&#33258;&#21160;&#26816;&#27979;&#32593;&#39029;&#20869;&#23481;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#36328;&#27169;&#24577;&#19981;&#21327;&#35843;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#20998;&#26512;&#12289;&#20998;&#31867;&#21644;&#35782;&#21035;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#23427;&#20204;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#19981;&#36275;&#20043;&#22788;&#65292;&#20197;&#25581;&#31034;&#22810;&#27169;&#24335;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#39046;&#22495;&#30340;&#26032;&#30740;&#31350;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
As social media platforms are evolving from text-based forums into multi-modal environments, the nature of misinformation in social media is also transforming accordingly. Taking advantage of the fact that visual modalities such as images and videos are more favorable and attractive to the users and textual contents are sometimes skimmed carelessly, misinformation spreaders have recently targeted contextual connections between the modalities e.g., text and image. Hence many researchers have developed automatic techniques for detecting possible cross-modal discordance in web-based content. We analyze, categorize and identify existing approaches in addition to challenges and shortcomings they face in order to unearth new research opportunities in the field of multi-modal misinformation detection.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#35774;&#35745;&#24072;&#21644;&#24320;&#21457;&#20154;&#21592;&#30340;&#23454;&#36341;&#21644;&#32463;&#39564;&#65292;&#20998;&#26512;&#20854;&#22914;&#20309;&#36981;&#24490;&#28595;&#22823;&#21033;&#20122;&#25919;&#24220;&#25552;&#20986;&#30340;&#39640;&#23618;&#27425;AI&#20262;&#29702;&#21407;&#21017;&#65292;&#24182;&#24635;&#32467;&#20986;&#21407;&#21017;&#20043;&#38388;&#23384;&#22312;&#30340;&#24352;&#21147;&#21644;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2112.07467</link><description>&lt;p&gt;
&#23454;&#36341;&#20013;&#30340;AI&#20262;&#29702;&#21407;&#21017;&#65306;&#35774;&#35745;&#24072;&#21644;&#24320;&#21457;&#32773;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
AI Ethics Principles in Practice: Perspectives of Designers and Developers. (arXiv:2112.07467v6 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.07467
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#35774;&#35745;&#24072;&#21644;&#24320;&#21457;&#20154;&#21592;&#30340;&#23454;&#36341;&#21644;&#32463;&#39564;&#65292;&#20998;&#26512;&#20854;&#22914;&#20309;&#36981;&#24490;&#28595;&#22823;&#21033;&#20122;&#25919;&#24220;&#25552;&#20986;&#30340;&#39640;&#23618;&#27425;AI&#20262;&#29702;&#21407;&#21017;&#65292;&#24182;&#24635;&#32467;&#20986;&#21407;&#21017;&#20043;&#38388;&#23384;&#22312;&#30340;&#24352;&#21147;&#21644;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21508;&#31181;&#24050;&#21457;&#24067;&#30340;AI&#20262;&#29702;&#21407;&#21017;&#30340;&#20849;&#35782;&#36880;&#28176;&#24418;&#25104;&#65292;&#39640;&#23618;&#27425;&#30340;&#21407;&#21017;&#21644;&#21487;&#31435;&#21363;&#37319;&#29992;&#30340;&#23454;&#38469;&#25216;&#26415;&#20043;&#38388;&#23384;&#22312;&#24040;&#22823;&#24046;&#36317;&#65292;&#32780;&#36825;&#20123;&#25216;&#26415;&#21487;&#29992;&#20110;&#35774;&#35745;&#21644;&#24320;&#21457;&#36127;&#36131;&#20219;&#30340;AI&#31995;&#32479;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#26469;&#33258;&#28595;&#22823;&#21033;&#20122;&#22269;&#23478;&#31185;&#23398;&#30740;&#31350;&#26426;&#26500;(CSIRO)&#30340;&#30740;&#31350;&#20154;&#21592;&#21644;&#24037;&#31243;&#24072;&#30340;&#23454;&#36341;&#21644;&#32463;&#39564;&#65292;&#20182;&#20204;&#21442;&#19982;&#35774;&#35745;&#21644;&#24320;&#21457;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#30340;AI&#31995;&#32479;&#12290;&#25105;&#20204;&#20351;&#29992;&#21322;&#32467;&#26500;&#21270;&#35775;&#35848;&#26469;&#30740;&#31350;&#21442;&#19982;&#32773;&#30340;&#23454;&#36341;&#22914;&#20309;&#19982;&#28595;&#22823;&#21033;&#20122;&#25919;&#24220;&#25552;&#20986;&#30340;&#19968;&#32452;&#39640;&#23618;&#27425;AI&#20262;&#29702;&#21407;&#21017;&#30456;&#32852;&#31995;&#21644;&#19968;&#33268;&#12290;&#36825;&#20123;&#21407;&#21017;&#21253;&#25324;&#65306;(1)&#38544;&#31169;&#20445;&#25252;&#21644;&#23433;&#20840;&#12289;(2)&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#12289;(3)&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12289;(4)&#20844;&#27491;&#24615;&#12289;(5)&#21487;&#20105;&#35758;&#24615;&#12289;(6)&#36131;&#20219;&#21046;&#12289;(7)&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20215;&#20540;&#35266;&#12289;(8)&#20154;&#31867;&#12289;&#31038;&#20250;&#21644;&#29615;&#22659;&#30340;&#31119;&#31049;&#12290;&#36890;&#36807;&#23545;&#35775;&#35848;&#25152;&#33719;&#24471;&#30340;&#27934;&#35265;&#30340;&#35752;&#35770;&#65292;&#21253;&#25324;&#21407;&#21017;&#20043;&#38388;&#30340;&#21508;&#31181;&#24352;&#21147;&#21644;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
As consensus across the various published AI ethics principles is approached, a gap remains between high-level principles and practical techniques that can be readily adopted to design and develop responsible AI systems. We examine the practices and experiences of researchers and engineers from Australia's national scientific research agency (CSIRO), who are involved in designing and developing AI systems for many application areas. Semi-structured interviews were used to examine how the practices of the participants relate to and align with a set of high-level AI ethics principles proposed by the Australian Government. The principles comprise: (1) privacy protection and security, (2) reliability and safety, (3) transparency and explainability, (4) fairness, (5) contestability, (6) accountability, (7) human-centred values, (8) human, social and environmental wellbeing. Discussions on the gained insights from the interviews include various tensions and trade-offs between the principles,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21477;&#23376;&#25552;&#21462;&#31574;&#30053;&#65292;&#29992;&#20110;&#25913;&#21892;&#29305;&#24449;&#20998;&#24067;&#21644;&#38477;&#20302;&#25688;&#35201;&#21477;&#23376;&#20043;&#38388;&#30340;&#30456;&#20114;&#20449;&#24687;&#65292;&#35813;&#31574;&#30053;&#36866;&#29992;&#20110;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#25277;&#21462;&#24335;&#25688;&#35201;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2112.03203</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#26080;&#30417;&#30563;&#25277;&#21462;&#24335;&#25688;&#35201;&#26041;&#27861;&#30340;&#21477;&#23376;&#25552;&#21462;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
A New Sentence Extraction Strategy for Unsupervised Extractive Summarization Methods. (arXiv:2112.03203v5 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.03203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21477;&#23376;&#25552;&#21462;&#31574;&#30053;&#65292;&#29992;&#20110;&#25913;&#21892;&#29305;&#24449;&#20998;&#24067;&#21644;&#38477;&#20302;&#25688;&#35201;&#21477;&#23376;&#20043;&#38388;&#30340;&#30456;&#20114;&#20449;&#24687;&#65292;&#35813;&#31574;&#30053;&#36866;&#29992;&#20110;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#25277;&#21462;&#24335;&#25688;&#35201;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#30740;&#31350;&#65292;&#25991;&#26412;&#25688;&#35201;&#26041;&#27861;&#20877;&#27425;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#24403;&#21069;&#22823;&#37096;&#20998;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#25991;&#26412;&#25688;&#35201;&#26041;&#27861;&#37117;&#26159;&#38656;&#35201;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#30417;&#30563;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#33719;&#21462;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#26159;&#22256;&#38590;&#30340;&#12290;&#26412;&#25991;&#20174;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#23545;&#25277;&#21462;&#24335;&#25991;&#26412;&#25688;&#35201;&#26041;&#27861;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#20351;&#29992;&#32479;&#19968;&#30340;&#26694;&#26550;&#25551;&#36848;&#20102;&#26080;&#30417;&#30563;&#25277;&#21462;&#26041;&#27861;&#12290;&#20026;&#20102;&#25913;&#21892;&#29305;&#24449;&#20998;&#24067;&#24182;&#38477;&#20302;&#25688;&#35201;&#21477;&#23376;&#20043;&#38388;&#30340;&#30456;&#20114;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21477;&#23376;&#25552;&#21462;&#31574;&#30053;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#25277;&#21462;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#31574;&#30053;&#30830;&#23454;&#26377;&#25928;&#24182;&#31526;&#21512;&#39044;&#26399;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, text summarization methods have attracted much attention again thanks to the researches on neural network models. Most of the current text summarization methods based on neural network models are supervised methods which need large-scale datasets. However, large-scale datasets are difficult to obtain in practical applications. In this paper, we model the task of extractive text summarization methods from the perspective of Information Theory, and then describe the unsupervised extractive methods with a uniform framework. To improve the feature distribution and to decrease the mutual information of summarization sentences, we propose a new sentence extraction strategy which can be applied to existing unsupervised extractive methods. Experiments are carried out on different datasets, and results show that our strategy is indeed effective and in line with expectations.
&lt;/p&gt;</description></item></channel></rss>