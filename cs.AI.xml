<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#20195;&#30721;&#35299;&#37322;&#26102;&#30340;&#34892;&#20026;&#12290;Java&#21644;Python&#30340;&#35299;&#37322;&#22312;&#21487;&#35835;&#24615;&#21644;&#35789;&#27719;&#23494;&#24230;&#26041;&#38754;&#34920;&#29616;&#19968;&#33268;&#65292;&#20294;&#22312;&#23436;&#25972;&#24615;&#12289;&#31616;&#27905;&#24615;&#21644;&#19978;&#19979;&#25991;&#24615;&#26041;&#38754;&#24471;&#20998;&#36739;&#20302;&#12290;</title><link>http://arxiv.org/abs/2311.01490</link><description>&lt;p&gt;
&#24403;&#34987;&#35201;&#27714;&#29983;&#25104;&#20195;&#30721;&#35299;&#37322;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Behavior of Large Language Models When Prompted to Generate Code Explanations. (arXiv:2311.01490v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#20195;&#30721;&#35299;&#37322;&#26102;&#30340;&#34892;&#20026;&#12290;Java&#21644;Python&#30340;&#35299;&#37322;&#22312;&#21487;&#35835;&#24615;&#21644;&#35789;&#27719;&#23494;&#24230;&#26041;&#38754;&#34920;&#29616;&#19968;&#33268;&#65292;&#20294;&#22312;&#23436;&#25972;&#24615;&#12289;&#31616;&#27905;&#24615;&#21644;&#19978;&#19979;&#25991;&#24615;&#26041;&#38754;&#24471;&#20998;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#31995;&#32479;&#22320;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#25104;&#20171;&#32461;&#32534;&#31243;&#35838;&#31243;&#20013;&#20351;&#29992;&#30340;&#20195;&#30721;&#31034;&#20363;&#35299;&#37322;&#26102;&#30340;&#34892;&#20026;&#12290;&#27491;&#22914;&#25105;&#20204;&#25152;&#23637;&#31034;&#30340;&#65292;LLMs&#29983;&#25104;&#30340;&#20195;&#30721;&#35299;&#37322;&#30340;&#24615;&#36136;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#25552;&#31034;&#30340;&#25514;&#36766;&#12289;&#34987;&#35299;&#37322;&#30340;&#30446;&#26631;&#20195;&#30721;&#31034;&#20363;&#12289;&#32534;&#31243;&#35821;&#35328;&#12289;&#28201;&#24230;&#21442;&#25968;&#21644;LLM&#30340;&#29256;&#26412;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;Java&#21644;Python&#32780;&#35328;&#65292;&#23427;&#20204;&#22312;&#20004;&#20010;&#20027;&#35201;&#26041;&#38754;&#20445;&#25345;&#19968;&#33268;&#65306;&#21487;&#35835;&#24615;&#27700;&#24179;&#22823;&#32422;&#22312;7-8&#24180;&#32423;&#65292;&#20197;&#21450;&#35789;&#27719;&#23494;&#24230;&#65292;&#21363;&#19982;&#24635;&#35299;&#37322;&#22823;&#23567;&#30456;&#23545;&#30340;&#26377;&#24847;&#20041;&#30340;&#21333;&#35789;&#30340;&#30456;&#23545;&#22823;&#23567;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#35299;&#37322;&#22312;&#27491;&#30830;&#24615;&#26041;&#38754;&#24471;&#20998;&#24456;&#39640;&#65292;&#20294;&#22312;&#23436;&#25972;&#24615;&#12289;&#31616;&#27905;&#24615;&#21644;&#19978;&#19979;&#25991;&#24615;&#26041;&#38754;&#24471;&#20998;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper systematically explores how Large Language Models (LLMs) generate explanations of code examples of the type used in intro-to-programming courses. As we show, the nature of code explanations generated by LLMs varies considerably based on the wording of the prompt, the target code examples being explained, the programming language, the temperature parameter, and the version of the LLM. Nevertheless, they are consistent in two major respects for Java and Python: the readability level, which hovers around 7-8 grade, and lexical density, i.e., the relative size of the meaningful words with respect to the total explanation size. Furthermore, the explanations score very high in correctness but less on three other metrics: completeness, conciseness, and contextualization.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27010;&#36848;&#20102;LLMs&#19982;&#20855;&#36523;&#26234;&#33021;&#22312;&#23548;&#33322;&#39046;&#22495;&#30340;&#20849;&#29983;&#20851;&#31995;&#65292;&#24182;&#35780;&#20272;&#20102;&#29616;&#26377;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#30340;&#20248;&#32570;&#28857;&#12290;</title><link>http://arxiv.org/abs/2311.00530</link><description>&lt;p&gt;
LLMs&#23545;&#20855;&#36523;&#23548;&#33322;&#30340;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;
The Development of LLMs for Embodied Navigation. (arXiv:2311.00530v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27010;&#36848;&#20102;LLMs&#19982;&#20855;&#36523;&#26234;&#33021;&#22312;&#23548;&#33322;&#39046;&#22495;&#30340;&#20849;&#29983;&#20851;&#31995;&#65292;&#24182;&#35780;&#20272;&#20102;&#29616;&#26377;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#30340;&#20248;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35832;&#22914;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#20043;&#31867;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#28508;&#21147;&#12290;LLMs&#19982;&#20855;&#36523;&#26234;&#33021;&#30340;&#24212;&#29992;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#22312;&#20247;&#22810;&#24212;&#29992;&#20013;&#65292;&#23548;&#33322;&#20219;&#21153;&#23588;&#20026;&#24341;&#20154;&#27880;&#30446;&#65292;&#22240;&#20026;&#23427;&#20204;&#35201;&#27714;&#23545;&#29615;&#22659;&#26377;&#28145;&#20837;&#30340;&#29702;&#35299;&#21644;&#24555;&#36895;&#12289;&#20934;&#30830;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;LLMs&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#20854;&#24378;&#22823;&#30340;&#35821;&#35328;&#21644;&#22270;&#20687;&#22788;&#29702;&#33021;&#21147;&#65292;&#22686;&#24378;&#20855;&#36523;&#26234;&#33021;&#31995;&#32479;&#22312;&#29615;&#22659;&#24863;&#30693;&#21644;&#20915;&#31574;&#25903;&#25345;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#20840;&#38754;&#24635;&#32467;&#20102;LLMs&#19982;&#20855;&#36523;&#26234;&#33021;&#20043;&#38388;&#22312;&#23548;&#33322;&#26041;&#38754;&#30340;&#20849;&#29983;&#20851;&#31995;&#65292;&#23457;&#35270;&#20102;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12289;&#30740;&#31350;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#29616;&#26377;&#30340;&#20855;&#36523;&#23548;&#33322;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#30340;&#20248;&#32570;&#28857;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#38416;&#26126;&#20102;LLMs&#22312;&#20855;&#36523;&#23548;&#33322;&#39046;&#22495;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the rapid advancement of Large Language Models (LLMs) such as the Generative Pre-trained Transformer (GPT) has attracted increasing attention due to their potential in a variety of practical applications. The application of LLMs with Embodied Intelligence has emerged as a significant area of focus. Among the myriad applications of LLMs, navigation tasks are particularly noteworthy because they demand a deep understanding of the environment and quick, accurate decision-making. LLMs can augment embodied intelligence systems with sophisticated environmental perception and decision-making support, leveraging their robust language and image-processing capabilities. This article offers an exhaustive summary of the symbiosis between LLMs and embodied intelligence with a focus on navigation. It reviews state-of-the-art models, research methodologies, and assesses the advantages and disadvantages of existing embodied navigation models and datasets. Finally, the article elucidat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24471;&#20998;&#24402;&#19968;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#25193;&#25955;&#25351;&#25968;&#31215;&#20998;&#22120;&#37319;&#26679;&#22120;&#30340;&#29983;&#25104;&#36136;&#37327;&#21644;&#20943;&#23567;&#31215;&#20998;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2311.00157</link><description>&lt;p&gt;
&#19968;&#20010;&#26356;&#24555;&#30340;&#25193;&#25955;&#25351;&#25968;&#31215;&#20998;&#22120;&#37319;&#26679;&#22120;&#30340;&#24471;&#20998;&#24402;&#19968;&#21270;
&lt;/p&gt;
&lt;p&gt;
Score Normalization for a Faster Diffusion Exponential Integrator Sampler. (arXiv:2311.00157v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00157
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24471;&#20998;&#24402;&#19968;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#25193;&#25955;&#25351;&#25968;&#31215;&#20998;&#22120;&#37319;&#26679;&#22120;&#30340;&#29983;&#25104;&#36136;&#37327;&#21644;&#20943;&#23567;&#31215;&#20998;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#24352;&#31561;&#20154;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#24555;&#36895;&#29983;&#25104;&#26679;&#26412;&#30340;&#25193;&#25955;&#25351;&#25968;&#31215;&#20998;&#22120;&#37319;&#26679;&#22120;&#65288;DEIS&#65289;&#12290;&#23427;&#21033;&#29992;&#27010;&#29575;&#27969;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#30340;&#21322;&#32447;&#24615;&#29305;&#24615;&#26469;&#22823;&#22823;&#20943;&#23567;&#31215;&#20998;&#35823;&#24046;&#65292;&#24182;&#22312;&#20302;&#20989;&#25968;&#35780;&#20272;&#27425;&#25968;&#65288;NFEs&#65289;&#26102;&#25552;&#39640;&#29983;&#25104;&#36136;&#37327;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#20851;&#38190;&#26159;&#24471;&#20998;&#20989;&#25968;&#37325;&#21442;&#25968;&#21270;&#65292;&#23427;&#36890;&#36807;&#20943;&#23569;&#22312;&#27599;&#20010;&#31215;&#20998;&#27493;&#39588;&#20013;&#20351;&#29992;&#22266;&#23450;&#24471;&#20998;&#20989;&#25968;&#20272;&#35745;&#32780;&#24341;&#36215;&#30340;&#31215;&#20998;&#35823;&#24046;&#12290;&#21407;&#22987;&#20316;&#32773;&#20351;&#29992;&#20102;&#29992;&#20110;&#22122;&#22768;&#39044;&#27979;&#35757;&#32451;&#30340;&#27169;&#22411;&#40664;&#35748;&#30340;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#21363;&#23558;&#24471;&#20998;&#20056;&#20197;&#26465;&#20214;&#27491;&#21521;&#22122;&#22768;&#20998;&#24067;&#30340;&#26631;&#20934;&#24046;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#36825;&#31181;&#24471;&#20998;&#21442;&#25968;&#21270;&#30340;&#32477;&#23545;&#24179;&#22343;&#20540;&#22312;&#22823;&#37096;&#20998;&#21453;&#21521;&#37319;&#26679;&#36807;&#31243;&#20013;&#25509;&#36817;&#24120;&#25968;&#65292;&#20294;&#22312;&#37319;&#26679;&#32467;&#26463;&#26102;&#23427;&#20250;&#36805;&#36895;&#21464;&#21270;&#12290;&#20026;&#20102;&#31616;&#21333;&#20462;&#22797;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#23545;&#24471;&#20998;&#36827;&#34892;&#37325;&#26032;&#21442;&#25968;&#21270;&#65288;&#22312;...
&lt;/p&gt;
&lt;p&gt;
Recently, zhang et al have proposed the Diffusion Exponential Integrator Sampler (DEIS) for fast generation of samples from Diffusion Models. It leverages the semi-linear nature of the probability flow ordinary differential equation (ODE) in order to greatly reduce integration error and improve generation quality at low numbers of function evaluations (NFEs). Key to this approach is the score function reparameterisation, which reduces the integration error incurred from using a fixed score function estimate over each integration step. The original authors use the default parameterisation used by models trained for noise prediction -- multiply the score by the standard deviation of the conditional forward noising distribution. We find that although the mean absolute value of this score parameterisation is close to constant for a large portion of the reverse sampling process, it changes rapidly at the end of sampling. As a simple fix, we propose to instead reparameterise the score (at in
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SERA&#30340;&#22870;&#21169;&#22686;&#24378;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#21892;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#33021;&#21147;&#12290;&#23427;&#36890;&#36807;&#35774;&#35745;&#20869;&#22312;&#22870;&#21169;&#26469;&#40723;&#21169;agent&#36827;&#34892;&#25506;&#32034;&#65292;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#22312;&#32447;&#24494;&#35843;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.19805</link><description>&lt;p&gt;
SERA&#65306;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26679;&#26412;&#39640;&#25928;&#22870;&#21169;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
SERA:Sample Efficient Reward Augmentation in offline-to-online Reinforcement Learning. (arXiv:2310.19805v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19805
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SERA&#30340;&#22870;&#21169;&#22686;&#24378;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#21892;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#33021;&#21147;&#12290;&#23427;&#36890;&#36807;&#35774;&#35745;&#20869;&#22312;&#22870;&#21169;&#26469;&#40723;&#21169;agent&#36827;&#34892;&#25506;&#32034;&#65292;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#22312;&#32447;&#24494;&#35843;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#19968;&#20010;&#28508;&#22312;&#24212;&#29992;&#26159;&#20351;&#29992;&#29616;&#26377;&#30340;&#38745;&#24577;&#25968;&#25454;&#38598;&#26469;&#21021;&#22987;&#21270;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#28982;&#21518;&#36827;&#34892;&#21518;&#32493;&#22312;&#32447;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#23545;&#31163;&#32447;&#39044;&#35757;&#32451;&#31574;&#30053;&#36827;&#34892;&#24494;&#35843;&#24448;&#24448;&#20250;&#23548;&#33268;&#27425;&#20248;&#24615;&#33021;&#12290;&#20027;&#35201;&#21407;&#22240;&#26159;&#31163;&#32447;&#20445;&#23432;&#26041;&#27861;&#38477;&#20302;&#20102;agent&#30340;&#25506;&#32034;&#33021;&#21147;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#22312;&#32447;&#24494;&#35843;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#22686;&#24378;&#22312;&#32447;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#25506;&#32034;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#25972;&#20307;&#30340;&#22312;&#32447;&#24494;&#35843;&#24615;&#33021;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#26679;&#26412;&#39640;&#25928;&#22870;&#21169;&#22686;&#24378;&#65288;SERA&#65289;&#30340;&#36890;&#29992;&#22870;&#21169;&#22686;&#24378;&#26694;&#26550;&#12290;SERA&#26088;&#22312;&#36890;&#36807;&#35774;&#35745;&#40723;&#21169;agent&#36827;&#34892;&#25506;&#32034;&#30340;&#20869;&#22312;&#22870;&#21169;&#26469;&#25913;&#21892;&#22312;&#32447;&#24494;&#35843;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23427;&#38544;&#24335;&#22320;&#23454;&#29616;&#20102;&#29366;&#24577;&#36793;&#32536;&#21305;&#37197;&#65288;SMM&#65289;&#24182;&#24809;&#32602;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#29366;&#24577;&#34892;&#21160;&#65292;&#20174;&#32780;&#40723;&#21169;agent&#35206;&#30422;&#30446;&#26631;&#29366;&#24577;&#23494;&#24230;&#65292;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#22312;&#32447;&#24494;&#35843;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
A prospective application of offline reinforcement learning (RL) involves initializing a pre-trained policy using existing static datasets for subsequent online fine-tuning. However, direct fine-tuning of the offline pre-trained policy often results in sub-optimal performance. A primary reason is that offline conservative methods diminish the agent's capability of exploration, thereby impacting online fine-tuning performance. To enhance exploration during online fine-tuning and thus enhance the overall online fine-tuning performance, we introduce a generalized reward augmentation framework called Sample Efficient Reward Augmentation (SERA). SERA aims to improve the performance of online fine-tuning by designing intrinsic rewards that encourage the agent to explore. Specifically, it implicitly implements State Marginal Matching (SMM) and penalizes out-of-distribution (OOD) state actions, thus encouraging agents to cover the target state density, and achieving better online fine-tuning r
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;KSAA-RD&#20849;&#20139;&#20219;&#21153;&#20013;Rosetta Stone&#30340;&#24212;&#29992;&#65292;&#23558;&#35821;&#35328;&#24314;&#27169;&#24212;&#29992;&#21040;&#35789;--&#23450;&#20041;&#23545;&#40784;&#20013;&#12290;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#19968;&#32452;&#24494;&#35843;&#30340;&#38463;&#25289;&#20271;BERT&#27169;&#22411;&#26469;&#39044;&#27979;&#32473;&#23450;&#23450;&#20041;&#30340;&#35789;&#23884;&#20837;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#38463;&#25289;&#20271;&#35789;&#30340;&#21521;&#37327;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2310.15823</link><description>&lt;p&gt;
Rosetta Stone&#22312;KSAA-RD&#20849;&#20139;&#20219;&#21153;&#20013;&#65306;&#20174;&#35821;&#35328;&#24314;&#27169;&#21040;&#35789;--&#23450;&#20041;&#23545;&#40784;&#30340;&#36291;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rosetta Stone at KSAA-RD Shared Task: A Hop From Language Modeling To Word--Definition Alignment. (arXiv:2310.15823v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;KSAA-RD&#20849;&#20139;&#20219;&#21153;&#20013;Rosetta Stone&#30340;&#24212;&#29992;&#65292;&#23558;&#35821;&#35328;&#24314;&#27169;&#24212;&#29992;&#21040;&#35789;--&#23450;&#20041;&#23545;&#40784;&#20013;&#12290;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#19968;&#32452;&#24494;&#35843;&#30340;&#38463;&#25289;&#20271;BERT&#27169;&#22411;&#26469;&#39044;&#27979;&#32473;&#23450;&#23450;&#20041;&#30340;&#35789;&#23884;&#20837;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#38463;&#25289;&#20271;&#35789;&#30340;&#21521;&#37327;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21521;&#35789;&#20856;&#26159;&#19968;&#31181;&#24037;&#20855;&#65292;&#21487;&#26681;&#25454;&#25552;&#20379;&#30340;&#23450;&#20041;&#12289;&#21547;&#20041;&#25110;&#25551;&#36848;&#26469;&#21457;&#29616;&#19968;&#20010;&#35789;&#12290;&#36825;&#31181;&#25216;&#26415;&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#37117;&#38750;&#24120;&#26377;&#20215;&#20540;&#65292;&#21487;&#20197;&#24110;&#21161;&#25484;&#25569;&#19968;&#20010;&#35789;&#30340;&#25551;&#36848;&#32780;&#19981;&#30693;&#20854;&#36523;&#20221;&#30340;&#35821;&#35328;&#23398;&#20064;&#32773;&#65292;&#24182;&#20351;&#23547;&#27714;&#31934;&#30830;&#26415;&#35821;&#30340;&#20889;&#20316;&#32773;&#21463;&#30410;&#12290;&#36825;&#20123;&#22330;&#26223;&#36890;&#24120;&#28085;&#30422;&#34987;&#31216;&#20026;&#8220;&#33292;&#23574;&#19978;&#30340;&#35789;&#8221;&#29616;&#35937;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21576;&#29616;&#20102;&#25105;&#20204;&#22312;&#38463;&#25289;&#20271;&#35821;&#21453;&#21521;&#35789;&#20856;&#20849;&#20139;&#20219;&#21153;&#20013;&#33719;&#32988;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#20219;&#21153;&#30340;&#37325;&#28857;&#26159;&#20174;&#20276;&#38543;&#30340;&#25551;&#36848;&#20013;&#25512;&#23548;&#20986;&#38463;&#25289;&#20271;&#35789;&#30340;&#21521;&#37327;&#34920;&#31034;&#12290;&#20849;&#20139;&#20219;&#21153;&#21253;&#25324;&#20004;&#20010;&#19981;&#21516;&#30340;&#23376;&#20219;&#21153;&#65306;&#31532;&#19968;&#20010;&#23376;&#20219;&#21153;&#28041;&#21450;&#19968;&#20010;&#38463;&#25289;&#20271;&#23450;&#20041;&#20316;&#20026;&#36755;&#20837;&#65292;&#32780;&#31532;&#20108;&#20010;&#23376;&#20219;&#21153;&#21017;&#20351;&#29992;&#19968;&#20010;&#33521;&#25991;&#23450;&#20041;&#12290;&#23545;&#20110;&#31532;&#19968;&#20010;&#23376;&#20219;&#21153;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#32452;&#32463;&#36807;&#24494;&#35843;&#30340;&#38463;&#25289;&#20271;BERT&#27169;&#22411;&#65292;&#26469;&#39044;&#27979;&#32473;&#23450;&#23450;&#20041;&#30340;&#35789;&#23884;&#20837;&#12290;&#26368;&#32456;&#34920;&#31034;&#26159;&#36890;&#36807;&#23545;&#27599;&#20010;&#27169;&#22411;&#36755;&#20986;&#30340;&#23884;&#20837;&#36827;&#34892;&#24179;&#22343;&#24471;&#21040;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Reverse Dictionary is a tool enabling users to discover a word based on its provided definition, meaning, or description. Such a technique proves valuable in various scenarios, aiding language learners who possess a description of a word without its identity, and benefiting writers seeking precise terminology. These scenarios often encapsulate what is referred to as the "Tip-of-the-Tongue" (TOT) phenomena. In this work, we present our winning solution for the Arabic Reverse Dictionary shared task. This task focuses on deriving a vector representation of an Arabic word from its accompanying description. The shared task encompasses two distinct subtasks: the first involves an Arabic definition as input, while the second employs an English definition. For the first subtask, our approach relies on an ensemble of finetuned Arabic BERT-based models, predicting the word embedding for a given definition. The final representation is obtained through averaging the output embeddings from each m
&lt;/p&gt;</description></item><item><title>ConFIRM&#26159;&#19968;&#31181;&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#21512;&#25104;&#37329;&#34701;&#39046;&#22495;&#29305;&#23450;&#38382;&#31572;&#23545;&#21644;&#35780;&#20272;&#21442;&#25968;&#24494;&#35843;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#24615;&#65292;&#20026;&#37329;&#34701;&#23545;&#35805;&#31995;&#32479;&#25552;&#20379;&#20102;&#25968;&#25454;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.13001</link><description>&lt;p&gt;
&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65288;ConFIRM&#65289;
&lt;/p&gt;
&lt;p&gt;
Conversational Financial Information Retrieval Model (ConFIRM). (arXiv:2310.13001v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13001
&lt;/p&gt;
&lt;p&gt;
ConFIRM&#26159;&#19968;&#31181;&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#21512;&#25104;&#37329;&#34701;&#39046;&#22495;&#29305;&#23450;&#38382;&#31572;&#23545;&#21644;&#35780;&#20272;&#21442;&#25968;&#24494;&#35843;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#24615;&#65292;&#20026;&#37329;&#34701;&#23545;&#35805;&#31995;&#32479;&#25552;&#20379;&#20102;&#25968;&#25454;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#21033;&#29992;&#23427;&#20204;&#22312;&#37329;&#34701;&#31561;&#19987;&#38376;&#39046;&#22495;&#30340;&#26032;&#20852;&#29305;&#24615;&#20855;&#26377;&#25506;&#32034;&#30340;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#37329;&#34701;&#31561;&#21463;&#30417;&#31649;&#39046;&#22495;&#20855;&#26377;&#29420;&#29305;&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#38656;&#35201;&#20855;&#22791;&#38024;&#23545;&#35813;&#39046;&#22495;&#30340;&#20248;&#21270;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ConFIRM&#65292;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65292;&#29992;&#20110;&#26597;&#35810;&#24847;&#22270;&#20998;&#31867;&#21644;&#30693;&#35782;&#24211;&#26631;&#35760;&#12290;ConFIRM&#21253;&#25324;&#20004;&#20010;&#27169;&#22359;&#65306;1&#65289;&#19968;&#31181;&#21512;&#25104;&#37329;&#34701;&#39046;&#22495;&#29305;&#23450;&#38382;&#31572;&#23545;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;2&#65289;&#35780;&#20272;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#26469;&#36827;&#34892;&#26597;&#35810;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;4000&#22810;&#20010;&#26679;&#26412;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#21333;&#29420;&#30340;&#27979;&#35797;&#38598;&#19978;&#35780;&#20272;&#20102;&#20934;&#30830;&#24615;&#12290;ConFIRM&#23454;&#29616;&#20102;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#24615;&#65292;&#36825;&#23545;&#20110;&#31526;&#21512;&#30417;&#31649;&#35201;&#27714;&#33267;&#20851;&#37325;&#35201;&#12290;ConFIRM&#25552;&#20379;&#20102;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#25552;&#21462;&#37329;&#34701;&#23545;&#35805;&#31995;&#32479;&#30340;&#31934;&#30830;&#26597;&#35810;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the exponential growth in large language models (LLMs), leveraging their emergent properties for specialized domains like finance merits exploration. However, regulated fields such as finance pose unique constraints, requiring domain-optimized frameworks. We present ConFIRM, an LLM-based conversational financial information retrieval model tailored for query intent classification and knowledge base labeling.  ConFIRM comprises two modules:  1) a method to synthesize finance domain-specific question-answer pairs, and  2) evaluation of parameter efficient fine-tuning approaches for the query classification task. We generate a dataset of over 4000 samples, assessing accuracy on a separate test set.  ConFIRM achieved over 90% accuracy, essential for regulatory compliance. ConFIRM provides a data-efficient solution to extract precise query intent for financial dialog systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PRIOR&#30340;&#20010;&#24615;&#21270;&#20808;&#39564;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#20013;&#30001;&#20110;&#20840;&#23616;&#27169;&#22411;&#24573;&#35270;&#29305;&#23450;&#20449;&#24687;&#32780;&#23548;&#33268;&#30340;&#19981;&#23436;&#25972;&#20449;&#24687;&#38382;&#39064;&#12290;&#35813;&#26041;&#26696;&#20351;&#29992;&#24102;Bregman&#25955;&#24230;&#30340;PFL&#26694;&#26550;&#23558;&#20010;&#24615;&#21270;&#20808;&#39564;&#19982;&#26412;&#22320;&#30446;&#26631;&#20989;&#25968;&#35299;&#32806;&#65292;&#20197;&#25552;&#39640;&#36866;&#24212;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.09183</link><description>&lt;p&gt;
PRIOR: &#20010;&#24615;&#21270;&#20808;&#39564;&#29992;&#20110;&#37325;&#26032;&#28608;&#27963;&#32852;&#37030;&#23398;&#20064;&#20013;&#34987;&#24573;&#35270;&#30340;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
PRIOR: Personalized Prior for Reactivating the Information Overlooked in Federated Learning. (arXiv:2310.09183v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PRIOR&#30340;&#20010;&#24615;&#21270;&#20808;&#39564;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#20013;&#30001;&#20110;&#20840;&#23616;&#27169;&#22411;&#24573;&#35270;&#29305;&#23450;&#20449;&#24687;&#32780;&#23548;&#33268;&#30340;&#19981;&#23436;&#25972;&#20449;&#24687;&#38382;&#39064;&#12290;&#35813;&#26041;&#26696;&#20351;&#29992;&#24102;Bregman&#25955;&#24230;&#30340;PFL&#26694;&#26550;&#23558;&#20010;&#24615;&#21270;&#20808;&#39564;&#19982;&#26412;&#22320;&#30446;&#26631;&#20989;&#25968;&#35299;&#32806;&#65292;&#20197;&#25552;&#39640;&#36866;&#24212;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#36890;&#36807;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21069;&#25552;&#19979;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#24322;&#36136;&#25968;&#25454;&#29305;&#24615;&#38477;&#20302;&#20102;&#23616;&#37096;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;PFL&#65289;&#36890;&#36807;&#20174;&#20840;&#23616;&#27169;&#22411;&#20013;&#21512;&#25104;&#20010;&#24615;&#21270;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#26412;&#22320;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#20840;&#23616;&#27169;&#22411;&#21487;&#33021;&#24573;&#35270;&#20102;&#23458;&#25143;&#31471;&#34987;&#37319;&#26679;&#30340;&#29305;&#23450;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#20010;&#24615;&#21270;&#20808;&#39564;&#30693;&#35782;&#27880;&#20837;&#21040;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#20840;&#23616;&#27169;&#22411;&#20013;&#65292;&#35797;&#22270;&#20943;&#36731;PFL&#20013;&#24341;&#20837;&#30340;&#19981;&#23436;&#25972;&#20449;&#24687;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#26694;&#26550;&#65292;&#21363;&#24102;Bregman&#25955;&#24230;&#65288;pFedBreD&#65289;&#30340;PFL&#65292;&#36890;&#36807;&#22312;&#20010;&#24615;&#21270;&#22330;&#26223;&#20013;&#20351;&#29992;Bregman&#25955;&#24230;&#27491;&#21017;&#21270;&#30340;&#26412;&#22320;&#30446;&#26631;&#20989;&#25968;&#65292;&#20351;&#20010;&#24615;&#21270;&#20808;&#39564;&#19982;&#20043;&#35299;&#32806;&#65292;&#20855;&#26377;&#26356;&#24378;&#30340;&#36866;&#24212;&#24615;&#12290;&#25105;&#20204;&#36824;&#25918;&#26494;&#20102;&#38236;&#20687;&#19979;&#38477;&#65288;RMD&#65289;&#65292;&#20197;&#26174;&#24335;&#22320;&#25552;&#21462;&#20808;&#39564;&#30693;&#35782;&#65292;&#25552;&#20379;&#21487;&#36873;&#25321;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classical federated learning (FL) enables training machine learning models without sharing data for privacy preservation, but heterogeneous data characteristic degrades the performance of the localized model. Personalized FL (PFL) addresses this by synthesizing personalized models from a global model via training on local data. Such a global model may overlook the specific information that the clients have been sampled. In this paper, we propose a novel scheme to inject personalized prior knowledge into the global model in each client, which attempts to mitigate the introduced incomplete information problem in PFL. At the heart of our proposed approach is a framework, the PFL with Bregman Divergence (pFedBreD), decoupling the personalized prior from the local objective function regularized by Bregman divergence for greater adaptability in personalized scenarios. We also relax the mirror descent (RMD) to extract the prior explicitly to provide optional strategies. Additionally, our pFed
&lt;/p&gt;</description></item><item><title>&#22312;gelu-4l&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#20869;&#23384;&#31649;&#29702;&#23545;&#20110;transformer&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#35828;&#26126;&#20102;Direct Logit Attribution&#25216;&#26415;&#30340;&#19981;&#20934;&#30830;&#20043;&#22788;&#12290;</title><link>http://arxiv.org/abs/2310.07325</link><description>&lt;p&gt;
&#30452;&#25509;&#36923;&#36753;&#23646;&#24615;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#65306;gelu-4l&#20013;&#30340;&#20869;&#23384;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
An Adversarial Example for Direct Logit Attribution: Memory Management in gelu-4l. (arXiv:2310.07325v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07325
&lt;/p&gt;
&lt;p&gt;
&#22312;gelu-4l&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#20869;&#23384;&#31649;&#29702;&#23545;&#20110;transformer&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#35828;&#26126;&#20102;Direct Logit Attribution&#25216;&#26415;&#30340;&#19981;&#20934;&#30830;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;4&#23618;transformer&#20013;&#20869;&#23384;&#31649;&#29702;&#30340;&#20855;&#20307;&#35777;&#25454;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#21069;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#65292;&#27169;&#22411;&#32452;&#20214;&#19968;&#33268;&#22320;&#31227;&#38500;&#21069;&#38754;&#32452;&#20214;&#30340;&#36755;&#20986;&#65292;&#36825;&#26159;&#19968;&#31181;&#28165;&#29702;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35299;&#37322;&#24615;&#25216;&#26415;Direct Logit Attribution&#25552;&#20379;&#20102;&#35823;&#23548;&#24615;&#32467;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26126;&#30830;&#30340;&#20363;&#23376;&#65292;&#35777;&#26126;&#36825;&#31181;&#25216;&#26415;&#26159;&#19981;&#20934;&#30830;&#30340;&#65292;&#22240;&#20026;&#23427;&#27809;&#26377;&#32771;&#34385;&#21040;&#28165;&#29702;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide concrete evidence for memory management in a 4-layer transformer. Specifically, we identify clean-up behavior, in which model components consistently remove the output of preceeding components during a forward pass. Our findings suggest that the interpretability technique Direct Logit Attribution provides misleading results. We show explicit examples where this technique is inaccurate, as it does not account for clean-up behavior.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Excision and Recovery (EAR)&#30340;&#26032;&#22411;&#37325;&#24314;-&#20462;&#22797;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21333;&#30830;&#23450;&#24615;&#36974;&#32617;&#26469;&#22686;&#24378;&#34920;&#38754;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.04010</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#21333;&#30830;&#23450;&#24615;&#36974;&#32617;&#22686;&#24378;&#34920;&#38754;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Excision and Recovery: Enhancing Surface Anomaly Detection with Attention-based Single Deterministic Masking. (arXiv:2310.04010v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Excision and Recovery (EAR)&#30340;&#26032;&#22411;&#37325;&#24314;-&#20462;&#22797;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21333;&#30830;&#23450;&#24615;&#36974;&#32617;&#26469;&#22686;&#24378;&#34920;&#38754;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#38754;&#26816;&#27979;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#26159;&#21046;&#36896;&#19994;&#20013;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#30001;&#20110;&#31232;&#32570;&#24322;&#24120;&#25968;&#25454;&#30340;&#25968;&#37327;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#19978;&#36848;&#38382;&#39064;&#65292;&#24191;&#27867;&#37319;&#29992;&#20165;&#20351;&#29992;&#26080;&#24322;&#24120;&#26679;&#26412;&#35757;&#32451;&#30340;&#37325;&#24314;&#32534;&#30721;-&#35299;&#30721;&#22120;&#65288;ED&#65289;&#65292;&#20363;&#22914;&#33258;&#32534;&#30721;&#22120;&#25110;U-Net&#65292;&#24076;&#26395;&#26410;&#35265;&#36807;&#30340;&#24322;&#24120;&#24212;&#35813;&#20135;&#29983;&#27604;&#27491;&#24120;&#24773;&#20917;&#26356;&#22823;&#30340;&#37325;&#24314;&#35823;&#24046;&#12290;&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#26377;&#20851;&#33258;&#25105;&#30417;&#30563;&#37325;&#24314;-&#20462;&#22797;&#30340;&#30740;&#31350;&#24050;&#32463;&#25253;&#36947;&#12290;&#20182;&#20204;&#36974;&#25377;&#20102;&#21487;&#30097;&#30340;&#32570;&#38519;&#21306;&#22495;&#20197;&#36827;&#34892;&#20462;&#22797;&#65292;&#20197;&#20351;&#23427;&#20204;&#23545;&#37325;&#24314;ED&#19981;&#21487;&#35265;&#65292;&#20174;&#32780;&#25925;&#24847;&#23548;&#33268;&#24322;&#24120;&#30340;&#19981;&#20934;&#30830;&#37325;&#24314;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#30340;&#23616;&#38480;&#24615;&#22312;&#20110;&#22810;&#27425;&#38543;&#26426;&#36974;&#32617;&#20197;&#35206;&#30422;&#25972;&#20010;&#36755;&#20837;&#22270;&#20687;&#65292;&#22240;&#20026;&#19981;&#20250;&#20107;&#20808;&#30693;&#36947;&#32570;&#38519;&#21306;&#22495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Excision and Recovery (EAR)&#30340;&#26032;&#22411;&#37325;&#24314;-&#20462;&#22797;&#26041;&#27861;&#65292;&#20855;&#26377;&#21333;&#30830;&#23450;&#24615;&#36974;&#32617;&#30340;&#29305;&#28857;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection (AD) in surface inspection is an essential yet challenging task in manufacturing due to the quantity imbalance problem of scarce abnormal data. To overcome the above, a reconstruction encoder-decoder (ED) such as autoencoder or U-Net which is trained with only anomaly-free samples is widely adopted, in the hope that unseen abnormals should yield a larger reconstruction error than normal. Over the past years, researches on self-supervised reconstruction-by-inpainting have been reported. They mask out suspected defective regions for inpainting in order to make them invisible to the reconstruction ED to deliberately cause inaccurate reconstruction for abnormals. However, their limitation is multiple random masking to cover the whole input image due to defective regions not being known in advance. We propose a novel reconstruction-by-inpainting method dubbed Excision and Recovery (EAR) that features single deterministic masking. For this, we exploit a pre-trained spatial 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#21551;&#21457;&#30340;&#20998;&#23618;&#22810;&#27169;&#24577;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#26500;&#24314;&#20102;&#19968;&#31181;&#26377;&#25928;&#19988;&#32039;&#20945;&#30340;&#20449;&#24687;&#27969;&#65292;&#23454;&#29616;&#20102;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#20840;&#38754;&#21644;&#20934;&#30830;&#30340;&#24863;&#30693;&#12290;</title><link>http://arxiv.org/abs/2309.15877</link><description>&lt;p&gt;
&#31070;&#32463;&#21551;&#21457;&#30340;&#20998;&#23618;&#22810;&#27169;&#24577;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Neuro-Inspired Hierarchical Multimodal Learning. (arXiv:2309.15877v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15877
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#21551;&#21457;&#30340;&#20998;&#23618;&#22810;&#27169;&#24577;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#26500;&#24314;&#20102;&#19968;&#31181;&#26377;&#25928;&#19988;&#32039;&#20945;&#30340;&#20449;&#24687;&#27969;&#65292;&#23454;&#29616;&#20102;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#20840;&#38754;&#21644;&#20934;&#30830;&#30340;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25972;&#21512;&#21644;&#22788;&#29702;&#26469;&#33258;&#22810;&#31181;&#20449;&#24687;&#28304;&#25110;&#27169;&#24577;&#23545;&#20110;&#33719;&#24471;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#20840;&#38754;&#21644;&#20934;&#30830;&#30340;&#24863;&#30693;&#33267;&#20851;&#37325;&#35201;&#12290;&#21463;&#21040;&#31070;&#32463;&#31185;&#23398;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20449;&#24687;&#35770;&#20998;&#23618;&#24863;&#30693;(ITHP)&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#20102;&#20449;&#24687;&#29942;&#39048;&#30340;&#27010;&#24565;&#12290;&#19982;&#22823;&#22810;&#25968;&#26088;&#22312;&#23558;&#25152;&#26377;&#27169;&#24577;&#32435;&#20837;&#36755;&#20837;&#30340;&#20256;&#32479;&#34701;&#21512;&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23558;&#20027;&#35201;&#27169;&#24577;&#25351;&#23450;&#20026;&#36755;&#20837;&#65292;&#32780;&#20854;&#20313;&#27169;&#24577;&#21017;&#20316;&#20026;&#20449;&#24687;&#36335;&#24452;&#20013;&#30340;&#26816;&#27979;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#24863;&#30693;&#27169;&#22411;&#30340;&#37325;&#28857;&#26159;&#36890;&#36807;&#22312;&#28508;&#22312;&#29366;&#24577;&#21644;&#36755;&#20837;&#27169;&#24577;&#29366;&#24577;&#20043;&#38388;&#26368;&#23567;&#21270;&#30456;&#20114;&#20449;&#24687;&#24182;&#22312;&#28508;&#22312;&#29366;&#24577;&#21644;&#20854;&#20313;&#27169;&#24577;&#20043;&#38388;&#26368;&#22823;&#21270;&#30456;&#20114;&#20449;&#24687;&#30340;&#24179;&#34913;&#65292;&#26500;&#24314;&#19968;&#31181;&#26377;&#25928;&#19988;&#32039;&#20945;&#30340;&#20449;&#24687;&#27969;&#12290;&#36825;&#31181;&#26041;&#27861;&#23548;&#33268;&#20102;&#20445;&#30041;&#30456;&#20851;&#20449;&#24687;&#24182;&#26368;&#23567;&#21270;&#20887;&#20313;&#30340;&#32039;&#20945;&#28508;&#22312;&#29366;&#24577;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integrating and processing information from various sources or modalities are critical for obtaining a comprehensive and accurate perception of the real world. Drawing inspiration from neuroscience, we develop the Information-Theoretic Hierarchical Perception (ITHP) model, which utilizes the concept of information bottleneck. Distinct from most traditional fusion models that aim to incorporate all modalities as input, our model designates the prime modality as input, while the remaining modalities act as detectors in the information pathway. Our proposed perception model focuses on constructing an effective and compact information flow by achieving a balance between the minimization of mutual information between the latent state and the input modal state, and the maximization of mutual information between the latent states and the remaining modal states. This approach leads to compact latent state representations that retain relevant information while minimizing redundancy, thereby sub
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#33268;&#21147;&#20110;&#24320;&#21457;&#19968;&#31181;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#38750;AI&#19987;&#23478;&#23545;AI&#20915;&#31574;&#30340;&#29702;&#35299;&#12290;&#36890;&#36807;&#23450;&#20041;&#39034;&#24207;&#20915;&#31574;&#35774;&#32622;&#20013;&#30340;&#8220;&#27010;&#24565;&#8221;&#20197;&#21450;&#25506;&#32034;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#23545;RL agent&#23398;&#20064;&#25928;&#26524;&#21644;&#26368;&#32456;&#29992;&#25143;&#23545;agent&#20915;&#31574;&#29702;&#35299;&#30340;&#21452;&#37325;&#22909;&#22788;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2309.12482</link><description>&lt;p&gt;
State2Explanation:&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#65306;&#26377;&#21033;&#20110;Agent&#23398;&#20064;&#21644;&#29992;&#25143;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
State2Explanation: Concept-Based Explanations to Benefit Agent Learning and User Understanding. (arXiv:2309.12482v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#33268;&#21147;&#20110;&#24320;&#21457;&#19968;&#31181;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#38750;AI&#19987;&#23478;&#23545;AI&#20915;&#31574;&#30340;&#29702;&#35299;&#12290;&#36890;&#36807;&#23450;&#20041;&#39034;&#24207;&#20915;&#31574;&#35774;&#32622;&#20013;&#30340;&#8220;&#27010;&#24565;&#8221;&#20197;&#21450;&#25506;&#32034;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#23545;RL agent&#23398;&#20064;&#25928;&#26524;&#21644;&#26368;&#32456;&#29992;&#25143;&#23545;agent&#20915;&#31574;&#29702;&#35299;&#30340;&#21452;&#37325;&#22909;&#22788;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#38750;AI&#19987;&#23478;&#20351;&#29992;&#26356;&#22797;&#26434;&#30340;AI&#31995;&#32479;&#26469;&#23436;&#25104;&#26085;&#24120;&#20219;&#21153;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#21162;&#21147;&#24320;&#21457;&#33021;&#22815;&#20026;&#38750;AI&#19987;&#23478;&#29702;&#35299;&#30340;AI&#20915;&#31574;&#25552;&#20379;&#35299;&#37322;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#21033;&#29992;&#39640;&#32423;&#27010;&#24565;&#24182;&#29983;&#25104;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#26041;&#27861;&#12290;&#22823;&#22810;&#25968;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#37117;&#26159;&#20026;&#20998;&#31867;&#25216;&#26415;&#32780;&#24320;&#21457;&#30340;&#65292;&#25105;&#20204;&#35748;&#20026;&#30446;&#21069;&#20851;&#20110;&#39034;&#24207;&#20915;&#31574;&#30340;&#26041;&#27861;&#36824;&#23384;&#22312;&#19968;&#23450;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#22312;&#39034;&#24207;&#20915;&#31574;&#35774;&#32622;&#20013;&#23450;&#20041;&#8220;&#27010;&#24565;&#8221;&#30340;&#24895;&#26395;&#12290;&#21463;&#21040;&#8220;Protege&#25928;&#24212;&#8221;&#30340;&#21551;&#21457;&#65292;&#35813;&#25928;&#24212;&#35828;&#26126;&#35299;&#37322;&#30693;&#35782;&#36890;&#24120;&#20250;&#22686;&#24378;&#20010;&#20307;&#30340;&#33258;&#20027;&#23398;&#20064;&#33021;&#21147;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#23545;RL agent&#30340;&#23398;&#20064;&#25928;&#26524;&#21644;&#26368;&#32456;&#29992;&#25143;&#23545;agent&#20915;&#31574;&#29702;&#35299;&#30340;&#21452;&#37325;&#22909;&#22788;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;St
&lt;/p&gt;
&lt;p&gt;
With more complex AI systems used by non-AI experts to complete daily tasks, there is an increasing effort to develop methods that produce explanations of AI decision making understandable by non-AI experts. Towards this effort, leveraging higher-level concepts and producing concept-based explanations have become a popular method. Most concept-based explanations have been developed for classification techniques, and we posit that the few existing methods for sequential decision making are limited in scope. In this work, we first contribute a desiderata for defining "concepts" in sequential decision making settings. Additionally, inspired by the Protege Effect which states explaining knowledge often reinforces one's self-learning, we explore the utility of concept-based explanations providing a dual benefit to the RL agent by improving agent learning rate, and to the end-user by improving end-user understanding of agent decision making. To this end, we contribute a unified framework, St
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#31616;&#21270;&#30340;&#38598;&#21512;&#32467;&#26500;BEA&#29992;&#20110;&#38170;&#28857;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#32622;&#20449;&#24230;&#24471;&#20998;&#30340;&#26657;&#20934;&#19982;&#38477;&#20302;&#19981;&#30830;&#23450;&#24615;&#35823;&#24046;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.08036</link><description>&lt;p&gt;
BEA: &#37325;&#26032;&#23457;&#35270;&#20351;&#29992;Budding Ensemble Architecture&#30340;&#22522;&#20110;&#38170;&#28857;&#30340;&#30446;&#26631;&#26816;&#27979;DNN
&lt;/p&gt;
&lt;p&gt;
BEA: Revisiting anchor-based object detection DNN using Budding Ensemble Architecture. (arXiv:2309.08036v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08036
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#31616;&#21270;&#30340;&#38598;&#21512;&#32467;&#26500;BEA&#29992;&#20110;&#38170;&#28857;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#32622;&#20449;&#24230;&#24471;&#20998;&#30340;&#26657;&#20934;&#19982;&#38477;&#20302;&#19981;&#30830;&#23450;&#24615;&#35823;&#24046;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Budding Ensemble Architecture&#65288;BEA&#65289;&#65292;&#19968;&#31181;&#29992;&#20110;&#38170;&#28857;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#30340;&#26032;&#22411;&#31616;&#21270;&#38598;&#21512;&#32467;&#26500;&#12290;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#22312;&#22522;&#20110;&#35270;&#35273;&#30340;&#20219;&#21153;&#20013;&#38750;&#24120;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#20027;&#31995;&#32479;&#20013;&#12290;&#23427;&#20204;&#24212;&#35813;&#25552;&#20379;&#31934;&#30830;&#30340;&#36793;&#30028;&#26694;&#26816;&#27979;&#65292;&#24182;&#26657;&#20934;&#20854;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#24471;&#20998;&#65292;&#20197;&#33719;&#24471;&#26356;&#39640;&#36136;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20551;&#38451;&#24615;&#25509;&#25910;&#21040;&#39640;&#20998;&#25110;&#30495;&#38451;&#24615;&#30001;&#20110;&#20302;&#20998;&#32780;&#34987;&#20002;&#24323;&#65292;&#24403;&#21069;&#30340;&#27169;&#22411;&#21487;&#33021;&#20250;&#20570;&#20986;&#38169;&#35823;&#30340;&#20915;&#31574;&#12290;BEA&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;BEA&#30340;&#25552;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#25913;&#21892;&#20102;&#32622;&#20449;&#24230;&#24471;&#20998;&#30340;&#26657;&#20934;&#21644;&#38477;&#20302;&#20102;&#19981;&#30830;&#23450;&#24615;&#35823;&#24046;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#21306;&#20998;&#30495;&#38451;&#24615;&#21644;&#20551;&#38451;&#24615;&#65292;&#26368;&#32456;&#25552;&#39640;&#20102;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#20351;&#29992;BEA&#26041;&#27861;&#21644;&#20854;&#25552;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#23545;Base-YOLOv3&#21644;SSD&#27169;&#22411;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;BEA&#22312;KITTI&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;Base-YOLOv3&#32467;&#26524;&#20013;&#65292;&#31934;&#24230;&#20998;&#21035;&#25552;&#39640;&#20102;6%&#21644;3.7%&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces the Budding Ensemble Architecture (BEA), a novel reduced ensemble architecture for anchor-based object detection models. Object detection models are crucial in vision-based tasks, particularly in autonomous systems. They should provide precise bounding box detections while also calibrating their predicted confidence scores, leading to higher-quality uncertainty estimates. However, current models may make erroneous decisions due to false positives receiving high scores or true positives being discarded due to low scores. BEA aims to address these issues. The proposed loss functions in BEA improve the confidence score calibration and lower the uncertainty error, which results in a better distinction of true and false positives and, eventually, higher accuracy of the object detection models. Both Base-YOLOv3 and SSD models were enhanced using the BEA method and its proposed loss functions. The BEA on Base-YOLOv3 trained on the KITTI dataset results in a 6% and 3.7% i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#26631;&#31614;&#31232;&#32570;&#30340;Learning-To-Rank&#38382;&#39064;&#20013;&#65292;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#27169;&#22411;&#26159;&#21542;&#33021;&#32988;&#36807;GBDTs&#21644;&#20854;&#20182;&#38750;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;SimCLR-Rank&#26041;&#27861;&#36827;&#34892;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22823;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#21644;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.00177</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#27169;&#22411;&#22312;&#26631;&#31614;&#31232;&#32570;&#30340;Learning-To-Rank&#20013;&#32988;&#36807;GBDTs
&lt;/p&gt;
&lt;p&gt;
Pretrained deep models outperform GBDTs in Learning-To-Rank under label scarcity. (arXiv:2308.00177v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#26631;&#31614;&#31232;&#32570;&#30340;Learning-To-Rank&#38382;&#39064;&#20013;&#65292;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#27169;&#22411;&#26159;&#21542;&#33021;&#32988;&#36807;GBDTs&#21644;&#20854;&#20182;&#38750;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;SimCLR-Rank&#26041;&#27861;&#36827;&#34892;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22823;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#21644;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#25991;&#26412;&#21644;&#22270;&#20687;&#39046;&#22495;&#26159;&#26368;&#20808;&#36827;&#30340;&#65292;&#20294;&#23427;&#20204;&#22312;&#34920;&#26684;&#24418;&#24335;&#30340;Learning-To-Rank&#38382;&#39064;&#19978;&#23578;&#26410;&#19968;&#33268;&#22320;&#32988;&#36807;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;(GBDTs)&#12290;&#36817;&#26399;&#22312;&#25991;&#26412;&#21644;&#22270;&#20687;&#20219;&#21153;&#19978;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21462;&#24471;&#30340;&#24615;&#33021;&#25552;&#21319;&#20027;&#35201;&#20381;&#36182;&#20110;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;&#27604;&#26377;&#26631;&#31614;&#25968;&#25454;&#22810;&#20960;&#20010;&#25968;&#37327;&#32423;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#36824;&#26410;&#24212;&#29992;&#20110;Learning-To-Rank&#38382;&#39064;&#65292;&#32780;&#35813;&#38382;&#39064;&#36890;&#24120;&#20135;&#29983;&#22823;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#12290;&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#26159;&#21542;&#33021;&#25552;&#39640;LTR&#24615;&#33021;&#65292;&#19982;GBDTs&#21644;&#20854;&#20182;&#38750;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#27604;&#12290;&#36890;&#36807;&#20351;&#29992;&#31616;&#21333;&#30340;&#35774;&#35745;&#36873;&#25321;(&#21253;&#25324;SimCLR-Rank&#65292;&#36825;&#26159;&#25105;&#20204;&#38024;&#23545;&#25490;&#21517;&#38382;&#39064;&#20462;&#25913;&#30340;SimCLR&#26041;&#27861;)&#65292;&#25105;&#20204;&#20135;&#29983;&#20102;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#26377;&#22823;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#19988;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#26174;&#33879;&#20248;&#20110;GBDTs(&#21644;&#20854;&#20182;&#38750;&#39044;&#35757;&#32451;&#27169;&#22411;)&#12290;
&lt;/p&gt;
&lt;p&gt;
While deep learning (DL) models are state-of-the-art in text and image domains, they have not yet consistently outperformed Gradient Boosted Decision Trees (GBDTs) on tabular Learning-To-Rank (LTR) problems. Most of the recent performance gains attained by DL models in text and image tasks have used unsupervised pretraining, which exploits orders of magnitude more unlabeled data than labeled data. To the best of our knowledge, unsupervised pretraining has not been applied to the LTR problem, which often produces vast amounts of unlabeled data. In this work, we study whether unsupervised pretraining can improve LTR performance over GBDTs and other non-pretrained models. Using simple design choices--including SimCLR-Rank, our ranking-specific modification of SimCLR (an unsupervised pretraining method for images)--we produce pretrained deep learning models that soundly outperform GBDTs (and other non-pretrained models) in the case where labeled data is vastly outnumbered by unlabeled data
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#20197;&#22320;&#26631;&#20026;&#22522;&#30784;&#30340;&#30446;&#26631;&#35782;&#21035;&#26041;&#27861;&#36827;&#34892;&#20877;&#35752;&#35770;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#35813;&#26041;&#27861;&#20013;&#20351;&#29992;&#21021;&#22987;&#29366;&#24577;&#30340;&#22320;&#26631;&#27809;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2306.15362</link><description>&lt;p&gt;
&#20197;&#22320;&#26631;&#20026;&#22522;&#30784;&#30340;&#30446;&#26631;&#35782;&#21035;&#20877;&#35752;&#35770;&#65306;&#20351;&#29992;&#21021;&#22987;&#29366;&#24577;&#30340;&#22320;&#26631;&#26159;&#21542;&#26377;&#24847;&#20041;&#65311;
&lt;/p&gt;
&lt;p&gt;
Planning Landmark Based Goal Recognition Revisited: Does Using Initial State Landmarks Make Sense?. (arXiv:2306.15362v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15362
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#20197;&#22320;&#26631;&#20026;&#22522;&#30784;&#30340;&#30446;&#26631;&#35782;&#21035;&#26041;&#27861;&#36827;&#34892;&#20877;&#35752;&#35770;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#35813;&#26041;&#27861;&#20013;&#20351;&#29992;&#21021;&#22987;&#29366;&#24577;&#30340;&#22320;&#26631;&#27809;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#35782;&#21035;&#26159;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#65288;&#22914;&#26222;&#36866;&#35745;&#31639;&#12289;&#20837;&#20405;&#26816;&#27979;&#12289;&#30005;&#33041;&#28216;&#25103;&#31561;&#65289;&#20013;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#22312;&#35768;&#22810;&#24212;&#29992;&#22330;&#26223;&#20013;&#65292;&#30446;&#26631;&#35782;&#21035;&#31639;&#27861;&#38656;&#35201;&#33021;&#22815;&#23613;&#24555;&#22320;&#35782;&#21035;&#35266;&#23519;&#21040;&#30340;&#20027;&#20307;&#30340;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#22312;&#35745;&#21010;&#35782;&#21035;&#21363;&#35745;&#21010;&#20013;&#65292;&#35768;&#22810;&#26089;&#26399;&#26041;&#27861;&#38656;&#35201;&#30456;&#24403;&#22823;&#37327;&#30340;&#35745;&#31639;&#26102;&#38388;&#26469;&#35745;&#31639;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;Pereira&#31561;&#20154;&#26368;&#36817;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#35268;&#21010;&#22320;&#26631;&#30340;&#26041;&#27861;&#65292;&#23427;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#35201;&#39640;&#24471;&#22810;&#12290;&#28982;&#32780;&#65292;&#27491;&#22914;Pereira&#31561;&#20154;&#25552;&#20986;&#30340;&#26041;&#27861;&#19968;&#26679;&#65292;&#23427;&#20063;&#20351;&#29992;&#20102;&#29712;&#30862;&#30340;&#22320;&#26631;&#65288;&#21363;&#65292;&#21021;&#22987;&#29366;&#24577;&#21644;&#30446;&#26631;&#25551;&#36848;&#20013;&#30340;&#20107;&#23454;&#22312;&#23450;&#20041;&#19978;&#23601;&#26159;&#22320;&#26631;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#22522;&#20110;&#35268;&#21010;&#22320;&#26631;&#30340;&#30446;&#26631;&#35782;&#21035;&#26041;&#27861;&#20013;&#20351;&#29992;&#21021;&#22987;&#29366;&#24577;&#30340;&#22320;&#26631;&#27809;&#26377;&#25552;&#20379;&#20219;&#20309;&#22909;&#22788;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#30465;&#30053;&#25481;&#21021;&#22987;&#29366;&#24577;&#30340;&#22320;&#26631;&#30340;&#26041;&#27861;&#26356;&#20026;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Goal recognition is an important problem in many application domains (e.g., pervasive computing, intrusion detection, computer games, etc.). In many application scenarios, it is important that goal recognition algorithms can recognize goals of an observed agent as fast as possible. However, many early approaches in the area of Plan Recognition As Planning, require quite large amounts of computation time to calculate a solution. Mainly to address this issue, recently, Pereira et al. developed an approach that is based on planning landmarks and is much more computationally efficient than previous approaches. However, the approach, as proposed by Pereira et al., also uses trivial landmarks (i.e., facts that are part of the initial state and goal description are landmarks by definition). In this paper, we show that it does not provide any benefit to use landmarks that are part of the initial state in a planning landmark based goal recognition approach. The empirical results show that omitt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36164;&#28304;&#24863;&#30693;&#30340;&#23376;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#26377;&#25928;&#35757;&#32451;&#29992;&#25143;&#35821;&#38899;&#20010;&#24615;&#21270;&#30340;ASR&#27169;&#22411;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#31227;&#21160;&#35774;&#22791;&#30340;&#35780;&#20272;&#25351;&#26631;&#21644;&#30005;&#27744;&#38480;&#21046;&#12290;&#22312;&#23454;&#39564;&#20013;&#21457;&#29616;&#65292;&#24494;&#35843;&#27169;&#22411;&#21644;&#36873;&#25321;&#36229;&#21442;&#25968;&#20540;&#38656;&#35201;&#22312;&#24615;&#33021;&#24230;&#37327;&#21644;&#26412;&#22320;&#35757;&#32451;&#26102;&#38388;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2306.09384</link><description>&lt;p&gt;
MobileASR: &#19968;&#31181;&#38754;&#21521;&#31227;&#21160;&#30005;&#35805;&#30340;&#36164;&#28304;&#24863;&#30693;&#26412;&#22320;&#20010;&#24615;&#21270;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MobileASR: A resource-aware on-device personalisation framework for automatic speech recognition in mobile phones. (arXiv:2306.09384v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36164;&#28304;&#24863;&#30693;&#30340;&#23376;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#26377;&#25928;&#35757;&#32451;&#29992;&#25143;&#35821;&#38899;&#20010;&#24615;&#21270;&#30340;ASR&#27169;&#22411;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#31227;&#21160;&#35774;&#22791;&#30340;&#35780;&#20272;&#25351;&#26631;&#21644;&#30005;&#27744;&#38480;&#21046;&#12290;&#22312;&#23454;&#39564;&#20013;&#21457;&#29616;&#65292;&#24494;&#35843;&#27169;&#22411;&#21644;&#36873;&#25321;&#36229;&#21442;&#25968;&#20540;&#38656;&#35201;&#22312;&#24615;&#33021;&#24230;&#37327;&#21644;&#26412;&#22320;&#35757;&#32451;&#26102;&#38388;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#36827;&#34892;&#26377;&#25928;&#30340;&#27169;&#22411;&#35757;&#32451;&#65292;&#20351;&#29992;&#25143;&#25968;&#25454;&#21644;&#27169;&#22411;&#22312;&#26412;&#22320;&#23384;&#20648;&#21644;&#20351;&#29992;&#65292;&#20174;&#32780;&#24320;&#21457;&#29992;&#25143;&#35821;&#38899;&#20010;&#24615;&#21270;&#30340;ASR&#27169;&#22411;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36164;&#28304;&#24863;&#30693;&#30340;&#23376;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#31227;&#21160;&#35774;&#22791;&#30340;RAM&#21644;&#30005;&#27744;&#23481;&#37327;&#65292;&#24182;&#25506;&#35752;&#20102;&#21487;&#29992;&#36164;&#28304;&#19982;&#35757;&#32451;&#26102;&#38388;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#31361;&#20986;&#20102;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#20351;&#29992;&#23376;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#32771;&#34385;&#31227;&#21160;&#35774;&#22791;&#30340;&#35780;&#20272;&#25351;&#26631;&#21644;&#30005;&#27744;&#38480;&#21046;&#65292;&#25105;&#20204;&#33021;&#22815;&#36827;&#34892;&#26377;&#25928;&#30340;&#35757;&#32451;&#24182;&#30456;&#24212;&#22320;&#20572;&#27490;&#35813;&#36807;&#31243;&#12290;&#20026;&#20102;&#27169;&#25311;&#30495;&#23454;&#29992;&#25143;&#65292;&#25105;&#20204;&#20351;&#29992;&#20855;&#26377;&#21508;&#31181;&#21475;&#38899;&#30340;&#21457;&#35328;&#32773;&#12290;&#28982;&#21518;&#65292;&#22312;&#21508;&#20010;&#21697;&#29260;&#30340;&#21508;&#31181;&#31227;&#21160;&#35774;&#22791;&#19978;&#27979;&#35797;&#25972;&#20010;&#26412;&#22320;&#35757;&#32451;&#21644;&#35780;&#20272;&#26694;&#26550;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24494;&#35843;&#27169;&#22411;&#21644;&#36873;&#25321;&#27491;&#30830;&#30340;&#36229;&#21442;&#25968;&#20540;&#26159;&#24615;&#33021;&#24230;&#37327;&#26368;&#20302;&#30340;&#21487;&#36798;&#21040;&#24615;&#21644;&#26412;&#22320;&#35757;&#32451;&#26102;&#38388;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
We describe a comprehensive methodology for developing user-voice personalised ASR models by effectively training models on mobile phones, allowing user data and models to be stored and used locally. To achieve this, we propose a resource-aware sub-model based training approach that considers the RAM, and battery capabilities of mobile phones. We also investigate the relationship between available resources and training time, highlighting the effectiveness of using sub-models in such scenarios. By taking into account the evaluation metric and battery constraints of the mobile phones, we are able to perform efficient training and halt the process accordingly. To simulate real users, we use speakers with various accents. The entire on-device training and evaluation framework was then tested on various mobile phones across brands. We show that fine-tuning the models and selecting the right hyperparameter values is a trade-off between the lowest achievable performance metric, on-device tra
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#32447;&#24615;transformers&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#65292;&#35777;&#26126;&#20102;&#23545;&#20110;&#19968;&#20010;&#27880;&#24847;&#21147;&#23618;&#65292;transformers&#33021;&#22815;&#23454;&#29616;&#19968;&#27425;&#39044;&#22788;&#29702;&#26799;&#24230;&#19979;&#38477;&#65292;&#32780;&#23545;&#20110;&#19968;&#20010;$k$&#20010;&#27880;&#24847;&#21147;&#23618;&#30340;transformer&#65292;&#23427;&#21487;&#20197;&#23454;&#29616;&#22810;&#27425;&#39044;&#22788;&#29702;&#26799;&#24230;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2306.00297</link><description>&lt;p&gt;
Transformers&#23398;&#20064;&#23454;&#29616;&#39044;&#22788;&#29702;&#26799;&#24230;&#19979;&#38477;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Transformers learn to implement preconditioned gradient descent for in-context learning. (arXiv:2306.00297v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00297
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#32447;&#24615;transformers&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#65292;&#35777;&#26126;&#20102;&#23545;&#20110;&#19968;&#20010;&#27880;&#24847;&#21147;&#23618;&#65292;transformers&#33021;&#22815;&#23454;&#29616;&#19968;&#27425;&#39044;&#22788;&#29702;&#26799;&#24230;&#19979;&#38477;&#65292;&#32780;&#23545;&#20110;&#19968;&#20010;$k$&#20010;&#27880;&#24847;&#21147;&#23618;&#30340;transformer&#65292;&#23427;&#21487;&#20197;&#23454;&#29616;&#22810;&#27425;&#39044;&#22788;&#29702;&#26799;&#24230;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;transformers&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#30340;&#26174;&#33879;&#33021;&#21147;&#30340;&#39537;&#21160;&#65292;&#19968;&#20123;&#30740;&#31350;&#34920;&#26126;transformers&#21487;&#20197;&#23454;&#29616;&#20687;&#26799;&#24230;&#19979;&#38477;&#36825;&#26679;&#30340;&#31639;&#27861;&#12290;&#36890;&#36807;&#31934;&#24515;&#30340;&#26435;&#37325;&#26500;&#36896;&#65292;&#36825;&#20123;&#30740;&#31350;&#34920;&#26126;&#22810;&#23618;transformers&#20855;&#26377;&#36275;&#22815;&#30340;&#34920;&#36798;&#33021;&#21147;&#26469;&#27169;&#25311;&#26799;&#24230;&#19979;&#38477;&#36845;&#20195;&#12290;&#36229;&#36234;&#34920;&#36798;&#33021;&#21147;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#38382;&#65306;transformers&#33021;&#21542;&#36890;&#36807;&#22312;&#38543;&#26426;&#38382;&#39064;&#23454;&#20363;&#19978;&#35757;&#32451;&#26469;&#23398;&#20064;&#23454;&#29616;&#36825;&#26679;&#30340;&#31639;&#27861;&#65311;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36890;&#36807;&#23545;&#32447;&#24615;&#22238;&#24402;&#30340;&#38543;&#26426;&#23454;&#20363;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#31532;&#19968;&#27425;&#22312;&#36825;&#20010;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#29702;&#35770;&#36827;&#23637;&#65292;&#36890;&#36807;&#23545;&#32447;&#24615;transformers&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#20998;&#26512;&#12290;&#23545;&#20110;&#19968;&#20010;&#27880;&#24847;&#21147;&#23618;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35757;&#32451;&#30446;&#26631;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#23454;&#29616;&#20102;&#19968;&#27425;&#39044;&#22788;&#29702;&#26799;&#24230;&#19979;&#38477;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#39044;&#22788;&#29702;&#30697;&#38453;&#19981;&#20165;&#36866;&#24212;&#36755;&#20837;&#20998;&#24067;&#65292;&#32780;&#19988;&#36824;&#36866;&#24212;&#20110;&#25968;&#25454;&#19981;&#20805;&#20998;&#24341;&#36215;&#30340;&#26041;&#24046;&#12290;&#23545;&#20110;&#19968;&#20010;&#20855;&#26377;$k$&#20010;&#27880;&#24847;&#21147;&#23618;&#30340;transformer&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#23450;&#26465;&#20214;&#19979;&#23427;&#21487;&#20197;&#23454;&#29616;&#22810;&#27425;&#39044;&#22788;&#29702;&#26799;&#24230;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the striking ability of transformers for in-context learning, several works demonstrate that transformers can implement algorithms like gradient descent. By a careful construction of weights, these works show that multiple layers of transformers are expressive enough to simulate gradient descent iterations. Going beyond the question of expressivity, we ask: Can transformers learn to implement such algorithms by training over random problem instances? To our knowledge, we make the first theoretical progress toward this question via analysis of the loss landscape for linear transformers trained over random instances of linear regression. For a single attention layer, we prove the global minimum of the training objective implements a single iteration of preconditioned gradient descent. Notably, the preconditioning matrix not only adapts to the input distribution but also to the variance induced by data inadequacy. For a transformer with $k$ attention layers, we prove certain 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Channel-aware dynamic sparse (Chase)&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31471;&#21040;&#31471;&#35757;&#32451;&#23454;&#29616;&#20102;GPU&#21451;&#22909;&#30340;&#36890;&#36947;&#32423;&#21035;&#31232;&#30095;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#29305;&#27530;&#25805;&#20316;&#65292;&#24182;&#19988;&#21487;&#20197;&#30452;&#25509;&#22312;&#36890;&#29992;&#30828;&#20214;&#19978;&#21152;&#36895;&#65292;&#26174;&#33879;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#65292;&#21516;&#26102;&#20445;&#25345;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.19454</link><description>&lt;p&gt;
&#21160;&#24577;&#31232;&#30095;&#26159;&#36890;&#36947;&#32423;&#31232;&#30095;&#30340;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
Dynamic Sparsity Is Channel-Level Sparsity Learner. (arXiv:2305.19454v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Channel-aware dynamic sparse (Chase)&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31471;&#21040;&#31471;&#35757;&#32451;&#23454;&#29616;&#20102;GPU&#21451;&#22909;&#30340;&#36890;&#36947;&#32423;&#21035;&#31232;&#30095;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#29305;&#27530;&#25805;&#20316;&#65292;&#24182;&#19988;&#21487;&#20197;&#30452;&#25509;&#22312;&#36890;&#29992;&#30828;&#20214;&#19978;&#21152;&#36895;&#65292;&#26174;&#33879;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#65292;&#21516;&#26102;&#20445;&#25345;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#35757;&#32451;&#30001;&#20110;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#21644;&#25512;&#29702;&#20013;&#20855;&#26377;&#35825;&#20154;&#30340;&#33410;&#30465;&#33021;&#21147;&#32780;&#21463;&#21040;&#26426;&#22120;&#23398;&#20064;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;(DST)&#20316;&#20026;&#19968;&#31181;&#39046;&#20808;&#30340;&#31232;&#30095;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#38646;&#24320;&#22987;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#36798;&#21040;&#19982;&#23494;&#38598;&#23545;&#24212;&#29289;&#24615;&#33021;&#30456;&#21305;&#37197;&#30340;&#39640;&#31232;&#30095;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;DST&#20043;&#21069;&#30340;&#30740;&#31350;&#37117;&#34920;&#26126;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#26159;&#22312;&#39640;&#24230;&#19981;&#35268;&#21017;&#30340;&#31232;&#30095;&#27169;&#24335;&#19979;&#30340;&#38750;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#19978;&#65292;&#36825;&#22312;&#24120;&#35265;&#30828;&#20214;&#19978;&#24471;&#21040;&#20102;&#26377;&#38480;&#30340;&#25903;&#25345;&#12290;&#36825;&#31181;&#38480;&#21046;&#38459;&#30861;&#20102;DST&#22312;&#23454;&#36341;&#20013;&#30340;&#20351;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36890;&#36947;&#24863;&#30693;&#21160;&#24577;&#31232;&#30095;&#65288;Chase&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#23558;&#38750;&#32467;&#26500;&#21270;&#21160;&#24577;&#31232;&#30095;&#30340;&#24615;&#33021;&#36716;&#25442;&#20026;&#36866;&#21512;GPU&#21451;&#22909;&#30340;&#36890;&#36947;&#32423;&#21035;&#31232;&#30095;&#65292;&#22312;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#23454;&#29616;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#29305;&#27530;&#30340;&#25805;&#20316;&#12290;&#25152;&#24471;&#21040;&#30340;&#23567;&#22411;&#31232;&#30095;&#32593;&#32476;&#21487;&#20197;&#30452;&#25509;&#36890;&#36807;&#36890;&#29992;&#30828;&#20214;&#21152;&#36895;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#19987;&#29992;&#30340;&#31232;&#30095;&#30828;&#20214;&#21152;&#36895;&#22120;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Chase&#21487;&#20197;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19978;&#23454;&#29616;&#39640;&#36890;&#36947;&#32423;&#31232;&#30095;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#24615;&#33021;&#65292;&#24182;&#26174;&#30528;&#20943;&#23567;&#27169;&#22411;&#30340;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse training has received an upsurging interest in machine learning due to its tantalizing saving potential for the entire training process as well as inference. Dynamic sparse training (DST), as a leading sparse training approach, can train deep neural networks at high sparsity from scratch to match the performance of their dense counterparts. However, most if not all DST prior arts demonstrate their effectiveness on unstructured sparsity with highly irregular sparse patterns, which receives limited support in common hardware. This limitation hinders the usage of DST in practice. In this paper, we propose Channel-aware dynamic sparse (Chase), which for the first time seamlessly translates the promise of unstructured dynamic sparsity to GPU-friendly channel-level sparsity (not fine-grained N:M or group sparsity) during one end-to-end training process, without any ad-hoc operations. The resulting small sparse networks can be directly accelerated by commodity hardware, without using a
&lt;/p&gt;</description></item><item><title>&#12298;Zero-TPrune&#12299;&#26159;&#19968;&#20010;&#32771;&#34385;&#21040;&#20196;&#29260;&#30340;&#37325;&#35201;&#24615;&#21644;&#30456;&#20284;&#24615;&#30340;&#38646;&#23556;&#20987;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#30340;&#27880;&#24847;&#22270;&#26469;&#36827;&#34892;&#20196;&#29260;&#21098;&#26525;&#65292;&#20197;&#27714;&#35299;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;Transformer&#27169;&#22411;&#21363;&#25554;&#21363;&#29992;&#30340;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.17328</link><description>&lt;p&gt;
&#12298;Zero-TPrune: &#22522;&#20110;&#39044;&#35757;&#32451;Transformers&#20851;&#27880;&#22270;&#30340;&#38646;&#23556;&#20987;&#20196;&#29260;&#21098;&#26525;&#26041;&#27861;&#12299;
&lt;/p&gt;
&lt;p&gt;
Zero-TPrune: Zero-Shot Token Pruning through Leveraging of the Attention Graph in Pre-Trained Transformers. (arXiv:2305.17328v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17328
&lt;/p&gt;
&lt;p&gt;
&#12298;Zero-TPrune&#12299;&#26159;&#19968;&#20010;&#32771;&#34385;&#21040;&#20196;&#29260;&#30340;&#37325;&#35201;&#24615;&#21644;&#30456;&#20284;&#24615;&#30340;&#38646;&#23556;&#20987;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#30340;&#27880;&#24847;&#22270;&#26469;&#36827;&#34892;&#20196;&#29260;&#21098;&#26525;&#65292;&#20197;&#27714;&#35299;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;Transformer&#27169;&#22411;&#21363;&#25554;&#21363;&#29992;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;Transformer&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21407;&#22240;&#26159;&#27169;&#22411;&#30340;&#20307;&#31215;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#32780;&#25512;&#29702;&#25104;&#26412;&#21017;&#38543;&#36755;&#20837;&#24207;&#21015;&#20013;&#20196;&#29260;&#25968;&#37327;&#30340;&#24179;&#26041;&#25552;&#39640;&#12290;&#20196;&#29260;&#21098;&#26525;&#26159;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#26032;&#20852;&#35299;&#20915;&#26041;&#27861;&#20043;&#19968;&#65292;&#30001;&#20110;&#20854;&#26131;&#20110;&#22312;&#21508;&#31181;Transformer&#25903;&#25345;&#30340;&#27169;&#22411;&#19978;&#37096;&#32626;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20196;&#29260;&#21098;&#26525;&#26041;&#27861;&#38656;&#35201;&#22312;&#21098;&#26525;&#21518;&#25110;&#26399;&#38388;&#36827;&#34892;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#24494;&#35843;&#36807;&#31243;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#36825;&#26159;&#19981;&#21487;&#21462;&#30340;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#25506;&#35752;&#20102;&#27809;&#26377;&#24494;&#35843;&#30340;&#21363;&#25554;&#21363;&#29992;&#30340;&#39044;&#35757;&#32451;Transformer&#30340;&#21098;&#26525;&#26041;&#27861;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204;&#21482;&#32771;&#34385;&#20102;&#20196;&#29260;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Zero-TPrune&#65292;&#36825;&#26159;&#19968;&#31181;&#38646;&#23556;&#20987;&#26041;&#27861;&#65292;&#23427;&#26082;&#32771;&#34385;&#20196;&#29260;&#30340;&#37325;&#35201;&#24615;&#21448;&#32771;&#34385;&#30456;&#20284;&#24615;&#26469;&#25191;&#34892;&#20196;&#29260;&#21098;&#26525;&#12290;Zero-TPrune&#21033;&#29992;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#30340;&#27880;&#24847;&#22270;&#26469;&#20026;&#20196;&#29260;&#29983;&#25104;&#19968;&#20010;&#37325;&#35201;&#24615;&#25490;&#21517;&#24182;&#31227;&#38500;&#20449;&#24687;&#36739;&#23569;&#30340;&#20196;&#29260;&#12290;&#27880;&#24847;&#30697;&#38453;&#21487;&#29992;&#20110;&#25512;&#26029;&#21363;&#25554;&#21363;&#29992;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deployment of Transformer models on the edge is increasingly challenging due to the exponentially growing model size and inference cost that scales quadratically with the number of tokens in the input sequence. Token pruning is an emerging solution to address this challenge due to its ease of deployment on various Transformer backbones. However, most token pruning methods require a computationally-expensive fine-tuning process after or during pruning, which is not desirable in many cases. Some recent works explore pruning of off-the-shelf pre-trained Transformers without fine-tuning. However, they only take the importance of tokens into consideration. In this work, we propose Zero-TPrune, the first zero-shot method that considers both the importance and similarity of tokens in performing token pruning. Zero-TPrune leverages the attention graph of pre-trained Transformer models to produce an importance rank for tokens and removes the less informative tokens. The attention matrix can be 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24418;&#24335;&#35821;&#27861;&#32422;&#26463;&#20016;&#23500;&#35299;&#30721;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#35821;&#27861;&#30340;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#65292;&#21516;&#26102;&#20801;&#35768;&#20219;&#20309;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;&#38598;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22235;&#20010;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.13971</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#27861;&#32422;&#26463;&#30340;&#35821;&#35328;&#27169;&#22411;&#28789;&#27963;&#35299;&#30721;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Flexible Grammar-Based Constrained Decoding for Language Models. (arXiv:2305.13971v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24418;&#24335;&#35821;&#27861;&#32422;&#26463;&#20016;&#23500;&#35299;&#30721;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#35821;&#27861;&#30340;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#65292;&#21516;&#26102;&#20801;&#35768;&#20219;&#20309;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;&#38598;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22235;&#20010;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#23569;&#37327;&#26679;&#26412;&#34920;&#29616;&#65292;&#20294;&#22312;&#29983;&#25104;&#20449;&#24687;&#25552;&#21462;&#25152;&#38656;&#30340;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#26102;&#20173;&#23384;&#22312;&#22256;&#38590;&#12290;&#36825;&#20010;&#38480;&#21046;&#28304;&#20110;LLM&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#20542;&#21521;&#20110;&#29983;&#25104;&#33258;&#30001;&#25991;&#26412;&#32780;&#19981;&#26159;&#36981;&#24490;&#29305;&#23450;&#35821;&#27861;&#30340;&#31934;&#30830;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#35299;&#30721;&#27493;&#39588;&#20013;&#20351;&#29992;&#24418;&#24335;&#35821;&#27861;&#32422;&#26463;&#26469;&#20016;&#23500;&#27169;&#22411;&#12290;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#65292;&#21482;&#26377;&#31526;&#21512;&#35821;&#27861;&#20135;&#29983;&#35268;&#21017;&#30340;&#26377;&#25928;&#20196;&#29260;&#33021;&#34987;&#32771;&#34385;&#21040;&#12290;&#36825;&#26679;&#23601;&#24378;&#21046;&#21482;&#20135;&#29983;&#26377;&#25928;&#30340;&#24207;&#21015;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#38750;&#24120;&#36890;&#29992;&#21644;&#28789;&#27963;&#65292;&#20801;&#35768;&#20219;&#20309;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;(CFG)&#38598;&#25104;&#21040;&#25105;&#20204;&#30340;&#33258;&#23450;&#20041;&#32422;&#26463;beam&#25628;&#32034;&#23454;&#29616;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35768;&#22810;NLP&#20219;&#21153;&#30340;&#36755;&#20986;&#21487;&#20197;&#34987;&#34920;&#31034;&#20026;&#24418;&#24335;&#35821;&#35328;&#65292;&#20351;&#23427;&#20204;&#36866;&#21512;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#30452;&#25509;&#20351;&#29992;&#12290;&#23545;&#20110;&#36755;&#20986;&#31354;&#38388;&#21462;&#20915;&#20110;&#36755;&#20837;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#36755;&#20837;&#30340;CFG&#65292;&#26681;&#25454;&#29305;&#23450;&#20110;&#36755;&#20837;&#30340;&#29305;&#24449;&#26356;&#26032;&#20135;&#29983;&#35268;&#21017;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#22235;&#20010;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs have shown impressive few-shot performance across many tasks. However, they still struggle when it comes to generating complex output structures, such as those required for Information Extraction. This limitation stems from the fact that LLMs, without finetuning, tend to generate free text rather than precise structures that follow a specific grammar. In this work, we propose to enrich the decoding step with formal grammar constraints. During beam search, only valid token continuations compliant with the grammar production rules are considered. This enforces the generation of valid sequences exclusively. Our framework is highly general and flexible, allowing any Context-Free Grammar (CFG) to be integrated into our custom constrained beam search implementation. We demonstrate that the outputs of many NLP tasks can be represented as formal languages, making them suitable for direct use in our framework. For task where the output space is dependent on the input, we propose input-depe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#26469;&#30740;&#31350;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#65292;&#21363;&#39532;&#23572;&#21487;&#22827; $\alpha$-&#21183;&#21338;&#24328;&#12290;&#20171;&#32461;&#20102;&#20004;&#31181;&#31639;&#27861;&#26469;&#35745;&#31639;&#20854;&#20013;&#30340;&#32435;&#20160;&#22343;&#34913;&#65292;&#19988;&#34920;&#26126;&#36825;&#20123;&#31639;&#27861;&#33021;&#22815;&#25214;&#21040;&#36817;&#20284;&#22343;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.12553</link><description>&lt;p&gt;
&#39532;&#23572;&#21487;&#22827;$\alpha$-&#21183;&#21338;&#24328;:&#22343;&#34913;&#36817;&#20284;&#19982;&#36951;&#25022;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Markov $\alpha$-Potential Games: Equilibrium Approximation and Regret Analysis. (arXiv:2305.12553v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#26469;&#30740;&#31350;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#65292;&#21363;&#39532;&#23572;&#21487;&#22827; $\alpha$-&#21183;&#21338;&#24328;&#12290;&#20171;&#32461;&#20102;&#20004;&#31181;&#31639;&#27861;&#26469;&#35745;&#31639;&#20854;&#20013;&#30340;&#32435;&#20160;&#22343;&#34913;&#65292;&#19988;&#34920;&#26126;&#36825;&#20123;&#31639;&#27861;&#33021;&#22815;&#25214;&#21040;&#36817;&#20284;&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30740;&#31350;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#22810;&#20195;&#29702;&#20132;&#20114;&#30340;&#26032;&#26694;&#26550;:&#39532;&#23572;&#21487;&#22827;$\alpha$-&#21183;&#21338;&#24328;&#12290;&#39532;&#23572;&#21487;&#22827;&#21183;&#21338;&#24328;&#26159;&#39532;&#23572;&#21487;&#22827; $\alpha$-&#21183;&#21338;&#24328;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#39532;&#23572;&#21487;&#22827;&#25317;&#22581;&#21338;&#24328;&#21644;&#25200;&#21160;&#39532;&#23572;&#21487;&#22827;&#22242;&#38431;&#21338;&#24328;&#26159;&#20004;&#20010;&#37325;&#35201;&#19988;&#23454;&#38469;&#24847;&#20041;&#37325;&#22823;&#30340;&#21338;&#24328;&#31867;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#20004;&#20010;&#21338;&#24328;&#30340;$\alpha$-&#21183;&#20989;&#25968;&#65292;&#24182;&#38024;&#23545;&#21338;&#24328;&#21442;&#25968;&#34920;&#24449;&#20102;&#24046;&#36317; $\alpha$&#12290;&#24341;&#20837;&#20102;&#20004;&#31181;&#31639;&#27861;&#8212;&#8212;&#25237;&#24433;&#26799;&#24230;&#19978;&#21319;&#31639;&#27861;&#21644;&#39034;&#24207;&#26368;&#22823;&#25913;&#36827;&#24179;&#28369;&#26368;&#20339;&#21453;&#24212;&#21160;&#24577;&#8212;&#8212;&#26469;&#36817;&#20284;&#35745;&#31639;&#39532;&#23572;&#21487;&#22827;$\alpha$-&#21183;&#21338;&#24328;&#20013;&#30340;&#31283;&#24577;&#32435;&#20160;&#22343;&#34913;&#12290;&#27599;&#20010;&#31639;&#27861;&#30340;&#32435;&#20160;&#36951;&#25022;&#37117;&#26174;&#31034;&#20026;&#26102;&#38388;&#36328;&#24230;&#30340;&#20122;&#32447;&#24615;&#32553;&#25918;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21644;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#31616;&#21333;&#30340;&#31639;&#27861;&#33021;&#22815;&#25214;&#21040;&#39532;&#23572;&#21487;&#22827;$\alpha$-&#21183;&#21338;&#24328;&#30340;&#36817;&#20284;&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new framework to study multi-agent interaction in Markov games: Markov $\alpha$-potential games. Markov potential games are special cases of Markov $\alpha$-potential games, so are two important and practically significant classes of games: Markov congestion games and perturbed Markov team games. In this paper, {$\alpha$-potential} functions for both games are provided and the gap $\alpha$ is characterized with respect to game parameters. Two algorithms -- the projected gradient-ascent algorithm and the sequential maximum improvement smoothed best response dynamics -- are introduced for approximating the stationary Nash equilibrium in Markov $\alpha$-potential games. The Nash-regret for each algorithm is shown to scale sub-linearly in time horizon. Our analysis and numerical experiments demonstrates that simple algorithms are capable of finding approximate equilibrium in Markov $\alpha$-potential games.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#32467;&#21512;&#21487;&#35757;&#32451;&#21160;&#24577;&#33021;&#37327;&#24863;&#30693;&#21644;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#21487;&#35745;&#31639;&#22270;&#20687;&#27169;&#22411;&#65292;&#26088;&#22312;&#25429;&#25417;&#29983;&#29289;&#35270;&#35273;&#31995;&#32479;&#20013;&#36816;&#21160;&#24863;&#30693;&#30340;&#26680;&#24515;&#32467;&#26500;&#35745;&#31639;&#36807;&#31243;&#65292;&#22635;&#34917;&#20102;&#20174;&#33258;&#28982;&#22330;&#26223;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#21487;&#35745;&#31639;&#22270;&#20687;&#27169;&#22411;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2305.09156</link><description>&lt;p&gt;
&#21487;&#35757;&#32451;&#21160;&#24577;&#33021;&#37327;&#24863;&#30693;&#21644;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#20154;&#31867;&#35270;&#35273;&#21160;&#24577;&#22788;&#29702;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Modelling Human Visual Motion Processing with Trainable Motion Energy Sensing and a Self-attention Network for Adaptive Motion Integration. (arXiv:2305.09156v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09156
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#32467;&#21512;&#21487;&#35757;&#32451;&#21160;&#24577;&#33021;&#37327;&#24863;&#30693;&#21644;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#21487;&#35745;&#31639;&#22270;&#20687;&#27169;&#22411;&#65292;&#26088;&#22312;&#25429;&#25417;&#29983;&#29289;&#35270;&#35273;&#31995;&#32479;&#20013;&#36816;&#21160;&#24863;&#30693;&#30340;&#26680;&#24515;&#32467;&#26500;&#35745;&#31639;&#36807;&#31243;&#65292;&#22635;&#34917;&#20102;&#20174;&#33258;&#28982;&#22330;&#26223;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#21487;&#35745;&#31639;&#22270;&#20687;&#27169;&#22411;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#21160;&#24577;&#22788;&#29702;&#23545;&#20110;&#29983;&#29289;&#24863;&#30693;&#21644;&#19982;&#21160;&#24577;&#29615;&#22659;&#20132;&#20114;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#22312;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#39046;&#22495;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#65292;&#20294;&#23578;&#26410;&#24314;&#31435;&#36215;&#33021;&#22815;&#20197;&#19968;&#31181;&#19982;&#20154;&#31867;&#35270;&#35273;&#22788;&#29702;&#19968;&#33268;&#30340;&#26041;&#24335;&#20174;&#33258;&#28982;&#22330;&#26223;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#21487;&#35745;&#31639;&#22270;&#20687;&#27169;&#22411;&#12290;&#21516;&#26102;&#65292;&#30001;&#28145;&#24230;&#23398;&#20064;&#25512;&#21160;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#26368;&#36817;&#36827;&#23637;&#22312;&#20809;&#27969;&#20272;&#35745;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;, &#36825;&#26159;&#19982;&#36816;&#21160;&#24863;&#30693;&#23494;&#20999;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#21512;&#20154;&#31867;&#35270;&#35273;&#27169;&#22411;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#21487;&#35745;&#31639;&#22270;&#20687;&#27169;&#22411;&#65292;&#26088;&#22312;&#25429;&#25417;&#29983;&#29289;&#35270;&#35273;&#31995;&#32479;&#20013;&#36816;&#21160;&#24863;&#30693;&#30340;&#26680;&#24515;&#32467;&#26500;V1-MT&#20013;&#30340;&#35745;&#31639;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual motion processing is essential for organisms to perceive and interact with dynamic environments. Despite extensive research in cognitive neuroscience, image-computable models that can extract informative motion flow from natural scenes in a manner consistent with human visual processing have yet to be established. Meanwhile, recent advancements in computer vision (CV), propelled by deep learning, have led to significant progress in optical flow estimation, a task closely related to motion perception. Here we propose an image-computable model of human motion perception by bridging the gap between human and CV models. Specifically, we introduce a novel two-stage approach that combines trainable motion energy sensing with a recurrent self-attention network for adaptive motion integration and segregation. This model architecture aims to capture the computations in V1-MT, the core structure for motion perception in the biological visual system. In silico neurophysiology reveals that 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#8220;ProAttack&#8221;&#26041;&#27861;&#26469;&#25191;&#34892;&#24178;&#20928;&#26631;&#31614;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#20351;&#29992;&#30340;&#26159;&#25552;&#31034;&#26412;&#36523;&#20316;&#20026;&#35302;&#21457;&#22120;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#22806;&#37096;&#35302;&#21457;&#22120;&#65292;&#24182;&#30830;&#20445;&#27602;&#30244;&#25968;&#25454;&#30340;&#26631;&#27880;&#27491;&#30830;&#65292;&#25552;&#39640;&#20102;&#21518;&#38376;&#25915;&#20987;&#30340;&#38544;&#34109;&#24615;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#26377;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.01219</link><description>&lt;p&gt;
&#35302;&#21457;&#35789;&#20316;&#20026;&#21518;&#38376;&#25915;&#20987;&#30340;&#35302;&#21457;&#22120;&#65306;&#26816;&#26597;&#35821;&#35328;&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models. (arXiv:2305.01219v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#8220;ProAttack&#8221;&#26041;&#27861;&#26469;&#25191;&#34892;&#24178;&#20928;&#26631;&#31614;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#20351;&#29992;&#30340;&#26159;&#25552;&#31034;&#26412;&#36523;&#20316;&#20026;&#35302;&#21457;&#22120;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#22806;&#37096;&#35302;&#21457;&#22120;&#65292;&#24182;&#30830;&#20445;&#27602;&#30244;&#25968;&#25454;&#30340;&#26631;&#27880;&#27491;&#30830;&#65292;&#25552;&#39640;&#20102;&#21518;&#38376;&#25915;&#20987;&#30340;&#38544;&#34109;&#24615;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#26377;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#33539;&#20363;&#24357;&#21512;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#22312;&#20960;&#20010;NLP&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#12290;&#23613;&#31649;&#24212;&#29992;&#24191;&#27867;&#65292;&#20294;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#12290;&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;&#26088;&#22312;&#36890;&#36807;&#27880;&#20837;&#35302;&#21457;&#22120;&#24182;&#20462;&#25913;&#26631;&#31614;&#26469;&#22312;&#27169;&#22411;&#20013;&#24341;&#20837;&#26377;&#38024;&#23545;&#24615;&#30340;&#28431;&#27934;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35302;&#21457;&#22120;&#30340;&#23384;&#22312;&#21644;&#27602;&#30244;&#25968;&#25454;&#26631;&#27880;&#19981;&#27491;&#30830;&#31561;&#32570;&#38519;&#65292;&#36825;&#31181;&#25915;&#20987;&#23384;&#22312;&#24322;&#24120;&#30340;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#8220;ProAttack&#8221;&#26041;&#27861;&#65292;&#22522;&#20110;&#25552;&#31034;&#26469;&#25191;&#34892;&#24178;&#20928;&#26631;&#31614;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#20351;&#29992;&#30340;&#26159;&#25552;&#31034;&#26412;&#36523;&#20316;&#20026;&#35302;&#21457;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#22806;&#37096;&#35302;&#21457;&#22120;&#65292;&#24182;&#30830;&#20445;&#27602;&#30244;&#25968;&#25454;&#30340;&#26631;&#27880;&#27491;&#30830;&#65292;&#25552;&#39640;&#20102;&#21518;&#38376;&#25915;&#20987;&#30340;&#38544;&#34109;&#24615;&#12290;&#36890;&#36807;&#22312;&#20016;&#23500;&#30340;&#36164;&#28304;&#21644;&#23569;&#26679;&#26412;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;ProAttack&#26041;&#27861;&#22312;&#20445;&#25345;&#24178;&#20928;&#25968;&#25454;&#19968;&#33268;&#24615;&#30340;&#21516;&#26102;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prompt-based learning paradigm, which bridges the gap between pre-training and fine-tuning, achieves state-of-the-art performance on several NLP tasks, particularly in few-shot settings. Despite being widely applied, prompt-based learning is vulnerable to backdoor attacks. Textual backdoor attacks are designed to introduce targeted vulnerabilities into models by poisoning a subset of training samples through trigger injection and label modification. However, they suffer from flaws such as abnormal natural language expressions resulting from the trigger and incorrect labeling of poisoned samples. In this study, we propose {\bf ProAttack}, a novel and efficient method for performing clean-label backdoor attacks based on the prompt, which uses the prompt itself as a trigger. Our method does not require external triggers and ensures correct labeling of poisoned samples, improving the stealthy nature of the backdoor attack. With extensive experiments on rich-resource and few-shot text c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;Explanation-Guided Exposure Minimization (EGEM)&#65292;&#35813;&#26041;&#27861;&#39044;&#38450;&#24615;&#22320;&#20462;&#21098;&#20102;ML&#27169;&#22411;&#20013;&#26410;&#21463;&#21040;&#31215;&#26497;&#35299;&#37322;&#21453;&#39304;&#30340;&#21464;&#21270;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23569;&#20102;&#23545;&#38544;&#34255;Clever Hans&#31574;&#30053;&#30340;&#20381;&#36182;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.05727</link><description>&lt;p&gt;
&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#39044;&#38450;&#24615;&#20462;&#21098;Clever Hans&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Preemptively Pruning Clever-Hans Strategies in Deep Neural Networks. (arXiv:2304.05727v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05727
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;Explanation-Guided Exposure Minimization (EGEM)&#65292;&#35813;&#26041;&#27861;&#39044;&#38450;&#24615;&#22320;&#20462;&#21098;&#20102;ML&#27169;&#22411;&#20013;&#26410;&#21463;&#21040;&#31215;&#26497;&#35299;&#37322;&#21453;&#39304;&#30340;&#21464;&#21270;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23569;&#20102;&#23545;&#38544;&#34255;Clever Hans&#31574;&#30053;&#30340;&#20381;&#36182;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;AI&#24050;&#25104;&#20026;&#39564;&#35777;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#27969;&#34892;&#24037;&#20855;&#12290;&#35299;&#37322;&#27169;&#22411;&#30340;&#20915;&#31574;&#31574;&#30053;&#19982;&#29992;&#25143;&#30340;&#39046;&#22495;&#30693;&#35782;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#65288;&#20363;&#22914;Clever Hans&#25928;&#24212;&#65289;&#20063;&#34987;&#35748;&#20026;&#26159;&#25913;&#36827;&#38169;&#35823;&#27169;&#22411;&#30340;&#36215;&#28857;&#12290;&#28982;&#32780;&#65292;&#24403;&#29992;&#25143;&#21644;&#35299;&#37322;&#36798;&#25104;&#19968;&#33268;&#26102;&#65292;&#35201;&#24590;&#20040;&#20570;&#23601;&#19981;&#37027;&#20040;&#28165;&#26970;&#20102;&#12290;&#26412;&#25991;&#36890;&#36807;&#23637;&#31034;&#29992;&#25143;&#25509;&#21463;&#35299;&#37322;&#24182;&#19981;&#20445;&#35777;ML&#27169;&#22411;&#30340;&#33391;&#22909;&#21151;&#33021;&#65292;&#29305;&#21035;&#26159;&#19968;&#20123;&#38544;&#34255;&#30340;Clever Hans&#25928;&#24212;&#21487;&#33021;&#20173;&#28982;&#26410;&#34987;&#21457;&#29616;&#65292;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#36890;&#36807;&#36129;&#29486;&#19968;&#20010;&#26032;&#26041;&#27861;Explanation-Guided Exposure Minimization (EGEM)&#65292;&#35813;&#26041;&#27861;&#39044;&#38450;&#24615;&#22320;&#20462;&#21098;&#20102;ML&#27169;&#22411;&#20013;&#26410;&#21463;&#21040;&#31215;&#26497;&#35299;&#37322;&#21453;&#39304;&#30340;&#21464;&#21270;&#12290;&#33258;&#28982;&#30011;&#20687;&#25968;&#25454;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23548;&#33268;&#27169;&#22411;&#22823;&#22823;&#20943;&#23569;&#20102;&#23545;&#38544;&#34255;&#30340;Clever Hans&#31574;&#30053;&#30340;&#20381;&#36182;&#65292;&#24182;&#22240;&#27492;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable AI has become a popular tool for validating machine learning models. Mismatches between the explained model's decision strategy and the user's domain knowledge (e.g. Clever Hans effects) have also been recognized as a starting point for improving faulty models. However, it is less clear what to do when the user and the explanation agree. In this paper, we demonstrate that acceptance of explanations by the user is not a guarantee for a ML model to function well, in particular, some Clever Hans effects may remain undetected. Such hidden flaws of the model can nevertheless be mitigated, and we demonstrate this by contributing a new method, Explanation-Guided Exposure Minimization (EGEM), that premptively prunes variations in the ML model that have not been the subject of positive explanation feedback. Experiments on natural image data demonstrate that our approach leads to models that strongly reduce their reliance on hidden Clever Hans strategies, and consequently achieve hig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25351;&#20986;&#65292;&#22914;&#26524;&#24847;&#35782;&#22312;&#21160;&#24577;&#19978;&#20855;&#26377;&#30456;&#20851;&#24615;&#65292;&#21017;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26080;&#27861;&#20855;&#26377;&#24847;&#35782;&#65292;&#22240;&#20026;&#36825;&#20123;&#31995;&#32479;&#30340;&#35774;&#35745;&#21644;&#39564;&#35777;&#25490;&#38500;&#20102;&#21487;&#33021;&#19982;&#24847;&#35782;&#30456;&#20851;&#30340;&#21160;&#21147;&#23398;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2304.05077</link><description>&lt;p&gt;
&#22914;&#26524;&#24847;&#35782;&#22312;&#21160;&#24577;&#19978;&#20855;&#26377;&#30456;&#20851;&#24615;&#65292;&#21017;&#20154;&#24037;&#26234;&#33021;&#19981;&#20250;&#20855;&#26377;&#24847;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
If consciousness is dynamically relevant, artificial intelligence isn't conscious. (arXiv:2304.05077v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25351;&#20986;&#65292;&#22914;&#26524;&#24847;&#35782;&#22312;&#21160;&#24577;&#19978;&#20855;&#26377;&#30456;&#20851;&#24615;&#65292;&#21017;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26080;&#27861;&#20855;&#26377;&#24847;&#35782;&#65292;&#22240;&#20026;&#36825;&#20123;&#31995;&#32479;&#30340;&#35774;&#35745;&#21644;&#39564;&#35777;&#25490;&#38500;&#20102;&#21487;&#33021;&#19982;&#24847;&#35782;&#30456;&#20851;&#30340;&#21160;&#21147;&#23398;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#22914;&#26524;&#24847;&#35782;&#19982;&#31995;&#32479;&#29366;&#24577;&#30340;&#26102;&#38388;&#28436;&#21270;&#26377;&#20851;&#65292;&#20063;&#23601;&#26159;&#35828;&#65292;&#22914;&#26524;&#23427;&#22312;&#21160;&#24577;&#19978;&#20855;&#26377;&#30456;&#20851;&#24615;&#65292;&#21017;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23601;&#19981;&#33021;&#20855;&#26377;&#24847;&#35782;&#12290;&#36825;&#26159;&#22240;&#20026;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36816;&#34892;&#22312;CPU&#12289;GPU&#12289;TPU&#25110;&#20854;&#20182;&#22788;&#29702;&#22120;&#19978;&#65292;&#36825;&#20123;&#22788;&#29702;&#22120;&#30340;&#35774;&#35745;&#21644;&#39564;&#35777;&#26159;&#20026;&#20102;&#36981;&#23432;&#35745;&#31639;&#21160;&#21147;&#23398;&#65292;&#20174;&#32780;&#31995;&#32479;&#22320;&#25490;&#38500;&#25110;&#25233;&#21046;&#20559;&#24046;&#12290;&#29305;&#21035;&#26159;&#65292;&#35774;&#35745;&#21644;&#39564;&#35777;&#25490;&#38500;&#25110;&#25233;&#21046;&#20102;&#21487;&#33021;&#19982;&#24847;&#35782;&#30456;&#20851;&#30340;&#21160;&#21147;&#23398;&#25928;&#24212;&#65292;&#22240;&#27492;&#22914;&#26524;&#24847;&#35782;&#22312;&#21160;&#24577;&#19978;&#20855;&#26377;&#30456;&#20851;&#24615;&#65292;&#21017;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#19981;&#33021;&#20855;&#26377;&#24847;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
We demonstrate that if consciousness is relevant for the temporal evolution of a system's states -- that is, if it is dynamically relevant -- then AI systems cannot be conscious. That is because AI systems run on CPUs, GPUs, TPUs or other processors which have been designed and verified to adhere to computational dynamics that systematically preclude or suppress deviations. The design and verification preclude or suppress, in particular, potential consciousness-related dynamical effects, so that if consciousness is dynamically relevant, AI systems cannot be conscious.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#20004;&#31181;&#32463;&#20856;&#35748;&#30693;&#24515;&#29702;&#23398;&#25216;&#26415;&#26469;&#20272;&#31639;&#20154;&#31867;&#21644;GPT-3&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35789;&#27719;&#35821;&#20041;&#32467;&#26500;&#65292;&#32467;&#26524;&#34920;&#26126;&#20154;&#31867;&#30340;&#27010;&#24565;&#32467;&#26500;&#31283;&#20581;&#40065;&#26834;&#65292;&#32780;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#20272;&#31639;&#32467;&#26500;&#26356;&#22810;&#21462;&#20915;&#20110;&#20855;&#20307;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2304.02754</link><description>&lt;p&gt;
&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#27010;&#24565;&#32467;&#26500;&#34920;&#29616;&#30340;&#24046;&#24322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Behavioral estimates of conceptual structure are robust across tasks in humans but not large language models. (arXiv:2304.02754v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#20004;&#31181;&#32463;&#20856;&#35748;&#30693;&#24515;&#29702;&#23398;&#25216;&#26415;&#26469;&#20272;&#31639;&#20154;&#31867;&#21644;GPT-3&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35789;&#27719;&#35821;&#20041;&#32467;&#26500;&#65292;&#32467;&#26524;&#34920;&#26126;&#20154;&#31867;&#30340;&#27010;&#24565;&#32467;&#26500;&#31283;&#20581;&#40065;&#26834;&#65292;&#32780;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#20272;&#31639;&#32467;&#26500;&#26356;&#22810;&#21462;&#20915;&#20110;&#20855;&#20307;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#24180;&#20197;&#26469;&#65292;&#31070;&#32463;&#32593;&#32476;&#35821;&#35328;&#27169;&#22411;&#19968;&#30452;&#34987;&#29992;&#20316;&#30740;&#31350;&#24515;&#29702;&#21644;&#33041;&#37096;&#27010;&#24565;&#34920;&#24449;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#22312;&#24403;&#20195;&#35821;&#35328;&#20154;&#24037;&#26234;&#33021;&#20013;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#19982;&#20154;&#31867;&#21442;&#19982;&#32773;&#20960;&#20046;&#30456;&#21516;&#30340;&#26041;&#27861;&#26469;&#25506;&#35752;&#27010;&#24565;&#34920;&#24449;&#30340;&#28508;&#22312;&#32467;&#26500;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#20004;&#31181;&#32463;&#20856;&#30340;&#35748;&#30693;&#24515;&#29702;&#23398;&#25216;&#26415;&#26469;&#20272;&#31639;&#21644;&#27604;&#36739;&#20154;&#31867;&#21644;&#19968;&#20010;&#33879;&#21517;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;GPT-3&#30340;DaVinci&#21464;&#20307;&#65289;&#30340;&#35789;&#27719;&#35821;&#20041;&#32467;&#26500;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#20154;&#31867;&#30340;&#27010;&#24565;&#32467;&#26500;&#24378;&#22823;&#19988;&#40065;&#26834;&#65292;&#19981;&#21463;&#25991;&#21270;&#12289;&#35821;&#35328;&#21644;&#20272;&#31639;&#26041;&#27861;&#30340;&#24046;&#24322;&#24433;&#21709;&#65307;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#34892;&#20026;&#20272;&#31639;&#32467;&#26524;&#30456;&#23545;&#31283;&#23450;&#65292;&#20294;&#20855;&#20307;&#21462;&#20915;&#20110;&#20219;&#21153;&#26412;&#36523;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#20154;&#31867;&#21442;&#19982;&#32773;&#30340;&#34892;&#20026;&#20272;&#31639;&#32467;&#26524;&#21487;&#38752;&#65292;&#20294;&#22312;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20154;&#31867;&#35748;&#30693;&#22788;&#29702;&#30456;&#20851;&#25512;&#26029;&#26102;&#65292;&#38656;&#35201;&#35880;&#24910;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network models of language have long been used as a tool for developing hypotheses about conceptual representation in the mind and brain. For many years, such use involved extracting vector-space representations of words and using distances among these to predict or understand human behavior in various semantic tasks. In contemporary language AIs, however, it is possible to interrogate the latent structure of conceptual representations using methods nearly identical to those commonly used with human participants. The current work uses two common techniques borrowed from cognitive psychology to estimate and compare lexical-semantic structure in both humans and a well-known AI, the DaVinci variant of GPT-3. In humans, we show that conceptual structure is robust to differences in culture, language, and method of estimation. Structures estimated from AI behavior, while individually fairly consistent with those estimated from human behavior, depend much more upon the particular task 
&lt;/p&gt;</description></item><item><title>&#27492;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HALO&#30340;&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#20010;&#23618;&#32423;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#20445;&#30495;&#12289;&#32454;&#31890;&#24230;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#65292;&#32780;&#36825;&#20123;&#25968;&#25454;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#20934;&#30830;&#30340;ML&#27169;&#22411;&#65292;&#19988;&#26080;&#38656;&#28041;&#21450;&#38544;&#31169;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.02169</link><description>&lt;p&gt;
&#22522;&#20110;&#23618;&#32423;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#21512;&#25104;&#26497;&#39640;&#32500;&#38271;&#26399;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;
&lt;/p&gt;
&lt;p&gt;
Synthesize Extremely High-dimensional Longitudinal Electronic Health Records via Hierarchical Autoregressive Language Model. (arXiv:2304.02169v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02169
&lt;/p&gt;
&lt;p&gt;
&#27492;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HALO&#30340;&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#20010;&#23618;&#32423;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#20445;&#30495;&#12289;&#32454;&#31890;&#24230;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#65292;&#32780;&#36825;&#20123;&#25968;&#25454;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#20934;&#30830;&#30340;ML&#27169;&#22411;&#65292;&#19988;&#26080;&#38656;&#28041;&#21450;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;(EHRs)&#33021;&#22815;&#22312;&#26426;&#22120;&#23398;&#20064;(ML)&#21644;&#32479;&#35745;&#20998;&#26512;&#20013;&#20316;&#20026;&#30495;&#23454;EHRs&#30340;&#26367;&#20195;&#21697;&#65292;&#26082;&#30495;&#23454;&#21448;&#20445;&#25252;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39640;&#32500;&#25968;&#25454;&#30340;&#20869;&#22312;&#22797;&#26434;&#24615;&#65292;&#20197;&#20854;&#21407;&#22987;&#39640;&#24230;&#32500;&#24418;&#24335;&#29983;&#25104;&#39640;&#20445;&#30495;&#12289;&#32454;&#31890;&#24230;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;(EHR)&#25968;&#25454;&#23545;&#29616;&#26377;&#26041;&#27861;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Hierarchical Autoregressive Language mOdel (HALO)&#8221;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#32437;&#21521;&#39640;&#32500;EHR&#25968;&#25454;&#65292;&#35813;&#26041;&#27861;&#20445;&#30041;&#20102;&#30495;&#23454;EHR&#30340;&#32479;&#35745;&#29305;&#24615;&#65292;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#20934;&#30830;&#30340;ML&#27169;&#22411;&#32780;&#19981;&#28041;&#21450;&#38544;&#31169;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;HALO&#26041;&#27861;&#34987;&#35774;&#35745;&#20026;&#19968;&#20010;&#23618;&#32423;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#29983;&#25104;&#19968;&#32452;&#38024;&#23545;&#21307;&#23398;&#20195;&#30721;&#12289;&#20020;&#24202;&#23601;&#35786;&#21644;&#30149;&#20154;&#35760;&#24405;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#65292;&#21487;&#20197;&#22312;&#20854;&#21407;&#22987;&#26410;&#32858;&#21512;&#24418;&#24335;&#19979;&#29983;&#25104;&#30495;&#23454;&#30340;EHR&#25968;&#25454;&#65292;&#26080;&#38656;&#36827;&#34892;&#21464;&#37327;&#36873;&#25321;&#25110;&#32858;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#20135;&#29983;&#22823;&#37327;&#30340;&#38543;&#26426;&#26679;&#26412;&#65292;&#20197;&#25552;&#20379;&#22797;&#26434;&#24230;&#36739;&#20302;&#20294;&#20173;&#26377;&#24847;&#20041;&#30340;EHR&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthetic electronic health records (EHRs) that are both realistic and preserve privacy can serve as an alternative to real EHRs for machine learning (ML) modeling and statistical analysis. However, generating high-fidelity and granular electronic health record (EHR) data in its original, highly-dimensional form poses challenges for existing methods due to the complexities inherent in high-dimensional data. In this paper, we propose Hierarchical Autoregressive Language mOdel (HALO) for generating longitudinal high-dimensional EHR, which preserve the statistical properties of real EHR and can be used to train accurate ML models without privacy concerns. Our HALO method, designed as a hierarchical autoregressive model, generates a probability density function of medical codes, clinical visits, and patient records, allowing for the generation of realistic EHR data in its original, unaggregated form without the need for variable selection or aggregation. Additionally, our model also produc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#23454;&#29616;&#26080;&#26631;&#31614;&#22270;&#20687;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#33976;&#39311;&#35757;&#32451;&#32858;&#31867;&#22836;&#23398;&#20064;&#22270;&#20687;&#20043;&#38388;&#30340;&#20851;&#32852;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#21487;&#20197;&#39640;&#25928;&#20934;&#30830;&#22320;&#36890;&#36807;&#39044;&#35757;&#32451;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#32467;&#26500;&#26469;&#23398;&#20064;&#12290;&#20351;&#29992;&#35813;&#26041;&#27861;&#22312;ImageNet&#21644;CIFAR100&#30340;17&#20010;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#23558;&#32858;&#31867;&#31934;&#24230;&#30456;&#23545;&#20110;k-&#22343;&#20540;&#25552;&#39640;&#20102;6.1%&#21644;12.2%&#12290;&#22312;ImageNet&#19978;&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#35270;&#35273;&#21464;&#25442;&#22120;&#33021;&#22815;&#23558;&#32858;&#31867;&#20934;&#30830;&#24230;&#25552;&#39640;&#21040;61.6%&#12290;</title><link>http://arxiv.org/abs/2303.17896</link><description>&lt;p&gt;
&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#25506;&#32034;&#28145;&#24230;&#22270;&#20687;&#32858;&#31867;&#30340;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
Exploring the Limits of Deep Image Clustering using Pretrained Models. (arXiv:2303.17896v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#23454;&#29616;&#26080;&#26631;&#31614;&#22270;&#20687;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#33976;&#39311;&#35757;&#32451;&#32858;&#31867;&#22836;&#23398;&#20064;&#22270;&#20687;&#20043;&#38388;&#30340;&#20851;&#32852;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#21487;&#20197;&#39640;&#25928;&#20934;&#30830;&#22320;&#36890;&#36807;&#39044;&#35757;&#32451;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#32467;&#26500;&#26469;&#23398;&#20064;&#12290;&#20351;&#29992;&#35813;&#26041;&#27861;&#22312;ImageNet&#21644;CIFAR100&#30340;17&#20010;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#23558;&#32858;&#31867;&#31934;&#24230;&#30456;&#23545;&#20110;k-&#22343;&#20540;&#25552;&#39640;&#20102;6.1%&#21644;12.2%&#12290;&#22312;ImageNet&#19978;&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#35270;&#35273;&#21464;&#25442;&#22120;&#33021;&#22815;&#23558;&#32858;&#31867;&#20934;&#30830;&#24230;&#25552;&#39640;&#21040;61.6%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#23398;&#20064;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#23545;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#21040;&#22522;&#20110;&#39044;&#35757;&#32451;&#29305;&#24449;&#31354;&#38388;&#20013;&#26368;&#36817;&#37051;&#23621;&#20849;&#20139;&#30456;&#21516;&#26631;&#31614;&#30340;&#20107;&#23454;&#23545;&#32858;&#31867;&#22836;&#36827;&#34892;&#33258;&#33976;&#39311;&#35757;&#32451;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#28857;&#23545;&#28857;&#30340;&#20114;&#20449;&#24687;&#21464;&#37327;&#20197;&#21450;&#23454;&#20363;&#21152;&#26435;&#26469;&#23398;&#20064;&#22270;&#20687;&#20043;&#38388;&#30340;&#20851;&#32852;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#30446;&#26631;&#20989;&#25968;&#33021;&#22815;&#20943;&#24369;&#20551;&#38451;&#24615;&#23545;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#39640;&#25928;&#22320;&#21033;&#29992;&#39044;&#35757;&#32451;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#32467;&#26500;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;ImageNet&#21644;CIFAR100&#30340;17&#20010;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#23558;&#32858;&#31867;&#31934;&#24230;&#30456;&#23545;&#20110;k-&#22343;&#20540;&#25552;&#39640;&#20102;6.1%&#21644;12.2%&#12290;&#26368;&#21518;&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#35270;&#35273;&#21464;&#25442;&#22120;&#65292;&#25105;&#20204;&#23558;&#22312;ImageNet&#19978;&#30340;&#32858;&#31867;&#20934;&#30830;&#24230;&#25552;&#39640;&#21040;&#20102;61.6%&#12290;&#20195;&#30721;&#23558;&#20844;&#24320;&#28304;&#20195;&#30721;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a general methodology that learns to classify images without labels by leveraging pretrained feature extractors. Our approach involves self-distillation training of clustering heads, based on the fact that nearest neighbors in the pretrained feature space are likely to share the same label. We propose a novel objective to learn associations between images by introducing a variant of pointwise mutual information together with instance weighting. We demonstrate that the proposed objective is able to attenuate the effect of false positive pairs while efficiently exploiting the structure in the pretrained feature space. As a result, we improve the clustering accuracy over $k$-means on $17$ different pretrained models by $6.1$\% and $12.2$\% on ImageNet and CIFAR100, respectively. Finally, using self-supervised pretrained vision transformers we push the clustering accuracy on ImageNet to $61.6$\%. The code will be open-sourced.
&lt;/p&gt;</description></item><item><title>NESTER&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#31070;&#32463;&#31526;&#21495;&#21270;&#26041;&#27861;&#36827;&#34892;&#27835;&#30103;&#25928;&#26524;&#35780;&#20272;&#65292;&#23558;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#30340;&#25152;&#26377;&#35201;&#27714;&#38598;&#25104;&#21040;&#19968;&#20010;&#26694;&#26550;&#20013;&#65292;&#35813;&#26041;&#27861;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24615;&#33021;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2211.04370</link><description>&lt;p&gt;
NESTER&#65306;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#31070;&#32463;&#31526;&#21495;&#21270;&#26041;&#27861;&#36827;&#34892;&#27835;&#30103;&#25928;&#26524;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
NESTER: An Adaptive Neurosymbolic Method for Treatment Effect Estimation. (arXiv:2211.04370v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04370
&lt;/p&gt;
&lt;p&gt;
NESTER&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#31070;&#32463;&#31526;&#21495;&#21270;&#26041;&#27861;&#36827;&#34892;&#27835;&#30103;&#25928;&#26524;&#35780;&#20272;&#65292;&#23558;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#30340;&#25152;&#26377;&#35201;&#27714;&#38598;&#25104;&#21040;&#19968;&#20010;&#26694;&#26550;&#20013;&#65292;&#35813;&#26041;&#27861;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24615;&#33021;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#36827;&#34892;&#27835;&#30103;&#25928;&#26524;&#35780;&#20272;&#26159;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#12290;&#22522;&#20110;&#28508;&#22312;&#32467;&#26524;&#26694;&#26550;&#30340;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#24402;&#32435;&#20559;&#32622;&#21644;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#27599;&#31181;&#29616;&#26377;&#30340;&#25216;&#26415;&#37117;&#36890;&#36807;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#27491;&#21017;&#21270;&#22120;&#26469;&#35299;&#20915;&#27835;&#30103;&#25928;&#26524;&#35780;&#20272;&#30340;&#29305;&#23450;&#26041;&#38754;&#65292;&#20363;&#22914;&#25511;&#21046;&#20542;&#21521;&#24471;&#20998;&#12289;&#24378;&#21046;&#38543;&#26426;&#21270;&#31561;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#31216;&#20026;&#31070;&#32463;&#31526;&#21495;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#22120;&#65288;NESTER&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#27835;&#30103;&#25928;&#26524;&#35780;&#20272;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;NESTER&#23558;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#30340;&#25152;&#26377;&#35201;&#27714;&#38598;&#25104;&#21040;&#19968;&#20010;&#26694;&#26550;&#20013;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#25991;&#29486;&#20013;&#20351;&#29992;&#30340;&#24402;&#32435;&#20559;&#32622;&#30340;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#65288;DSL&#65289;&#12290;&#25105;&#20204;&#36824;&#22312;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;NESTER&#22312;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20840;&#38754;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;NESTER&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Treatment effect estimation from observational data is a central problem in causal inference. Methods based on potential outcomes framework solve this problem by exploiting inductive biases and heuristics from causal inference. Each existing technique addresses a specific aspect of treatment effect estimation, such as controlling propensity score, enforcing randomization, etc., by designing neural network architectures and regularizers. In this paper, we propose an adaptive method called Neurosymbolic Treatment Effect Estimator (NESTER), a generalized method for treatment effect estimation. NESTER brings together all the desiderata for treatment effect estimation into one framework. For this purpose, we design a Domain Specific Language (DSL) for the treatment effect estimation based on inductive biases used in literature. We also theoretically study NESTER's capability for the treatment effect estimation task. Our comprehensive empirical results show that NESTER performs better on ben
&lt;/p&gt;</description></item><item><title>&#29420;&#31435;&#30340;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#22312;&#39532;&#23572;&#31185;&#22827;&#28508;&#22312;&#21338;&#24328;&#20013;&#26377;&#25928;&#65292;&#36890;&#36807;&#26356;&#26032;Q&#20989;&#25968;&#21487;&#20197;&#24341;&#23548;&#31574;&#30053;&#25910;&#25947;&#21040;&#31283;&#23450;&#30340;&#32435;&#20160;&#24179;&#34913;&#28857;&#12290;</title><link>http://arxiv.org/abs/2205.14590</link><description>&lt;p&gt;
&#39532;&#23572;&#31185;&#22827;&#28508;&#22312;&#21338;&#24328;&#20013;&#30340;&#29420;&#31435;&#21644;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Independent and Decentralized Learning in Markov Potential Games. (arXiv:2205.14590v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14590
&lt;/p&gt;
&lt;p&gt;
&#29420;&#31435;&#30340;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#22312;&#39532;&#23572;&#31185;&#22827;&#28508;&#22312;&#21338;&#24328;&#20013;&#26377;&#25928;&#65292;&#36890;&#36807;&#26356;&#26032;Q&#20989;&#25968;&#21487;&#20197;&#24341;&#23548;&#31574;&#30053;&#25910;&#25947;&#21040;&#31283;&#23450;&#30340;&#32435;&#20160;&#24179;&#34913;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26426;&#21046;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#22312;&#26080;&#38480;&#26102;&#38388;&#25240;&#25187;&#39532;&#23572;&#31185;&#22827;&#28508;&#22312;&#21338;&#24328;&#20013;&#30340;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#29420;&#31435;&#21644;&#21435;&#20013;&#24515;&#21270;&#30340;&#35774;&#32622;&#65292;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#29609;&#23478;&#19981;&#20102;&#35299;&#28216;&#25103;&#27169;&#22411;&#65292;&#20063;&#19981;&#33021;&#36827;&#34892;&#21327;&#35843;&#12290;&#22312;&#27599;&#20010;&#38454;&#27573;&#65292;&#29609;&#23478;&#36890;&#36807;&#24322;&#27493;&#26041;&#24335;&#26356;&#26032;&#20182;&#20204;&#30340;&#25171;&#25200;Q&#20989;&#25968;&#30340;&#20272;&#35745;&#20540;&#65292;&#35813;&#20989;&#25968;&#26681;&#25454;&#23454;&#29616;&#30340;&#19968;&#38454;&#27573;&#22870;&#21169;&#35780;&#20272;&#20182;&#20204;&#30340;&#24635;&#20307;&#26465;&#20214;&#20184;&#27454;&#12290;&#28982;&#21518;&#65292;&#29609;&#23478;&#36890;&#36807;&#23558;&#22522;&#20110;&#20272;&#35745;Q&#20989;&#25968;&#30340;&#24179;&#28369;&#26368;&#20248;&#19968;&#38454;&#27573;&#20559;&#24046;&#31574;&#30053;&#32435;&#20837;&#20854;&#31574;&#30053;&#20013;&#26469;&#29420;&#31435;&#22320;&#26356;&#26032;&#20854;&#31574;&#30053;&#12290;&#23398;&#20064;&#21160;&#24577;&#30340;&#20851;&#38190;&#29305;&#24449;&#26159;Q&#20989;&#25968;&#20272;&#35745;&#26159;&#20197;&#27604;&#31574;&#30053;&#26356;&#24555;&#30340;&#26102;&#38388;&#23610;&#24230;&#36827;&#34892;&#26356;&#26032;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#23398;&#20064;&#21160;&#24577;&#24341;&#23548;&#30340;&#31574;&#30053;&#22312;&#27010;&#29575;1&#30340;&#24773;&#20917;&#19979;&#25910;&#25947;&#21040;&#39532;&#23572;&#31185;&#22827;&#28508;&#22312;&#21338;&#24328;&#30340;&#31283;&#23450;&#32435;&#20160;&#24179;&#34913;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20984;&#26174;&#20102;&#31616;&#21333;&#23398;&#20064;&#21160;&#24577;&#22312;&#36798;&#21040;&#39532;&#23572;&#21487;&#22827;&#28508;&#22312;&#21338;&#24328;&#30340;&#31283;&#23450;&#32435;&#20160;&#24179;&#34913;&#26041;&#38754;&#30340;&#21151;&#25928;&#65292;&#21363;&#20351;&#26159;&#22312;&#29420;&#31435;&#21644;&#21435;&#20013;&#24515;&#21270;&#20195;&#29702;&#29615;&#22659;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a multi-agent reinforcement learning dynamics, and analyze its convergence in infinite-horizon discounted Markov potential games. We focus on the independent and decentralized setting, where players do not have knowledge of the game model and cannot coordinate. In each stage, players update their estimate of a perturbed Q-function that evaluates their total contingent payoff based on the realized one-stage reward in an asynchronous manner. Then, players independently update their policies by incorporating a smoothed optimal one-stage deviation strategy based on the estimated Q-function. A key feature of the learning dynamics is that the Q-function estimates are updated at a faster timescale than the policies. We prove that the policies induced by our learning dynamics converge to a stationary Nash equilibrium in Markov potential games with probability 1. Our results highlight the efficacy of simple learning dynamics in reaching a stationary Nash equilibrium even in environme
&lt;/p&gt;</description></item></channel></rss>