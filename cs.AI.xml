<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#24314;&#26500;&#24615;&#21644;&#22810;&#26679;&#21270;&#21453;&#39304;&#65288;CDF&#65289;&#65292;&#29992;&#20110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23545;&#40784;&#25928;&#26524;&#12290;&#25105;&#20204;&#36890;&#36807;&#25910;&#38598;&#19981;&#21516;&#31867;&#22411;&#30340;&#21453;&#39304;&#65292;&#24182;&#26681;&#25454;&#38382;&#39064;&#30340;&#38590;&#24230;&#32423;&#21035;&#36827;&#34892;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.06450</link><description>&lt;p&gt;
&#29992;&#22810;&#26679;&#21270;&#21453;&#39304;&#26500;&#24314;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Constructive Large Language Models Alignment with Diverse Feedback. (arXiv:2310.06450v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#24314;&#26500;&#24615;&#21644;&#22810;&#26679;&#21270;&#21453;&#39304;&#65288;CDF&#65289;&#65292;&#29992;&#20110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23545;&#40784;&#25928;&#26524;&#12290;&#25105;&#20204;&#36890;&#36807;&#25910;&#38598;&#19981;&#21516;&#31867;&#22411;&#30340;&#21453;&#39304;&#65292;&#24182;&#26681;&#25454;&#38382;&#39064;&#30340;&#38590;&#24230;&#32423;&#21035;&#36827;&#34892;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30740;&#31350;&#20013;&#65292;&#23545;&#20854;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#23545;&#40784;&#36234;&#26469;&#36234;&#37325;&#35270;&#65292;&#20197;&#20943;&#23569;&#26377;&#23475;&#20869;&#23481;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#23545;&#40784;&#26041;&#27861;&#36890;&#24120;&#20165;&#20381;&#36182;&#20110;&#20154;&#31867;&#21453;&#39304;&#30340;&#21333;&#19968;&#24418;&#24335;&#65292;&#22914;&#20559;&#22909;&#12289;&#27880;&#37322;&#26631;&#31614;&#25110;&#33258;&#28982;&#35821;&#35328;&#25209;&#35780;&#65292;&#24573;&#35270;&#20102;&#32467;&#21512;&#36825;&#20123;&#21453;&#39304;&#31867;&#22411;&#30340;&#28508;&#22312;&#20248;&#21183;&#12290;&#36825;&#31181;&#38480;&#21046;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#65292;&#21363;&#20351;&#26377;&#20016;&#23500;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#24314;&#26500;&#24615;&#21644;&#22810;&#26679;&#21270;&#21453;&#39304;&#65288;CDF&#65289;&#20316;&#20026;&#22686;&#24378;LLM&#23545;&#40784;&#30340;&#26032;&#26041;&#27861;&#65292;&#21463;&#24314;&#26500;&#23398;&#20064;&#29702;&#35770;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#25910;&#38598;&#36866;&#29992;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#19981;&#21516;&#38590;&#24230;&#38382;&#39064;&#30340;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#21453;&#39304;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#25209;&#35780;&#21453;&#39304;&#35299;&#20915;&#31616;&#21333;&#38382;&#39064;&#65292;&#21033;&#29992;&#25913;&#36827;&#21453;&#39304;&#35299;&#20915;&#20013;&#31561;&#38382;&#39064;&#65292;&#21033;&#29992;&#20559;&#22909;&#21453;&#39304;&#35299;&#20915;&#22256;&#38590;&#38382;&#39064;&#12290;&#36890;&#36807;&#29992;&#36825;&#31181;&#22810;&#26679;&#21270;&#21453;&#39304;&#35757;&#32451;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent research on large language models (LLMs), there has been a growing emphasis on aligning these models with human values to reduce the impact of harmful content. However, current alignment methods often rely solely on singular forms of human feedback, such as preferences, annotated labels, or natural language critiques, overlooking the potential advantages of combining these feedback types. This limitation leads to suboptimal performance, even when ample training data is available. In this paper, we introduce Constructive and Diverse Feedback (CDF) as a novel method to enhance LLM alignment, inspired by constructivist learning theory. Our approach involves collecting three distinct types of feedback tailored to problems of varying difficulty levels within the training dataset. Specifically, we exploit critique feedback for easy problems, refinement feedback for medium problems, and preference feedback for hard problems. By training our model with this diversified feedback, we a
&lt;/p&gt;</description></item><item><title>&#36880;&#27493;&#21151;&#33021;&#37325;&#26500;&#30340;&#20851;&#31995;&#27010;&#24565;&#20998;&#26512;&#65288;RCA&#65289;&#26159;&#24418;&#24335;&#27010;&#24565;&#20998;&#26512;&#30340;&#25193;&#23637;&#65292;&#36890;&#36807;&#23450;&#20041;&#33391;&#26500;&#35299;&#20915;&#26041;&#26696;&#30340;&#31354;&#38388;&#21644;&#30456;&#20851;&#20989;&#25968;&#65292;&#35299;&#20915;&#20102;RCA&#22312;&#24490;&#29615;&#20381;&#36182;&#25968;&#25454;&#19978;&#36820;&#22238;&#21333;&#19968;&#27010;&#24565;&#26684;&#23478;&#26063;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.06441</link><description>&lt;p&gt;
&#36880;&#27493;&#21151;&#33021;&#37325;&#26500;&#30340;&#20851;&#31995;&#27010;&#24565;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Stepwise functional refoundation of relational concept analysis. (arXiv:2310.06441v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06441
&lt;/p&gt;
&lt;p&gt;
&#36880;&#27493;&#21151;&#33021;&#37325;&#26500;&#30340;&#20851;&#31995;&#27010;&#24565;&#20998;&#26512;&#65288;RCA&#65289;&#26159;&#24418;&#24335;&#27010;&#24565;&#20998;&#26512;&#30340;&#25193;&#23637;&#65292;&#36890;&#36807;&#23450;&#20041;&#33391;&#26500;&#35299;&#20915;&#26041;&#26696;&#30340;&#31354;&#38388;&#21644;&#30456;&#20851;&#20989;&#25968;&#65292;&#35299;&#20915;&#20102;RCA&#22312;&#24490;&#29615;&#20381;&#36182;&#25968;&#25454;&#19978;&#36820;&#22238;&#21333;&#19968;&#27010;&#24565;&#26684;&#23478;&#26063;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#27010;&#24565;&#20998;&#26512;&#65288;RCA&#65289;&#26159;&#24418;&#24335;&#27010;&#24565;&#20998;&#26512;&#30340;&#25193;&#23637;&#65292;&#20801;&#35768;&#21516;&#26102;&#22788;&#29702;&#22810;&#20010;&#30456;&#20851;&#30340;&#35821;&#22659;&#12290;&#23427;&#34987;&#35774;&#35745;&#29992;&#20110;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#25551;&#36848;&#36923;&#36753;&#29702;&#35770;&#65292;&#24182;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#20351;&#29992;&#12290;&#20851;&#20110;RCA&#30340;&#19968;&#20010;&#20196;&#20154;&#22256;&#24785;&#30340;&#35266;&#23519;&#26159;&#65292;&#23613;&#31649;&#25968;&#25454;&#23384;&#22312;&#24490;&#29615;&#20381;&#36182;&#20851;&#31995;&#65292;&#23427;&#36820;&#22238;&#19968;&#20010;&#21333;&#19968;&#30340;&#27010;&#24565;&#26684;&#23478;&#26063;&#65292;&#20854;&#20182;&#35299;&#20915;&#26041;&#26696;&#21487;&#33021;&#34987;&#35748;&#20026;&#26159;&#21487;&#25509;&#21463;&#30340;&#12290;RCA&#30340;&#35821;&#20041;&#20197;&#25805;&#20316;&#26041;&#24335;&#25552;&#20379;&#65292;&#23545;&#27492;&#38382;&#39064;&#24182;&#27809;&#26377;&#25552;&#20379;&#26126;&#30830;&#30340;&#35299;&#37322;&#12290;&#22312;&#26412;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#21487;&#25509;&#21463;&#30340;&#35299;&#20915;&#26041;&#26696;&#23450;&#20041;&#20026;&#23646;&#20110;&#21021;&#22987;&#35821;&#22659;&#30830;&#23450;&#30340;&#31354;&#38388;&#30340;&#27010;&#24565;&#26684;&#23478;&#26063;&#65288;&#33391;&#26500;&#65289;&#65292;&#19981;&#33021;&#25193;&#23637;&#26032;&#23646;&#24615;&#65288;&#39281;&#21644;&#65289;&#65292;&#24182;&#19988;&#20165;&#28041;&#21450;&#35813;&#23478;&#26063;&#30340;&#27010;&#24565;&#65288;&#33258;&#25903;&#25345;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#23450;&#20041;&#33391;&#26500;&#35299;&#20915;&#26041;&#26696;&#30340;&#31354;&#38388;&#20197;&#21450;&#35813;&#31354;&#38388;&#19978;&#30340;&#20004;&#20010;&#20989;&#25968;&#65288;&#19968;&#20010;&#25193;&#24352;&#20989;&#25968;&#21644;&#19968;&#20010;&#25910;&#32553;&#20989;&#25968;&#65289;&#65292;&#37319;&#29992;&#21151;&#33021;&#35270;&#22270;&#26469;&#25551;&#36848;RCA&#36807;&#31243;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#25509;&#21463;&#30340;&#35299;&#20915;&#26041;&#26696;&#8230;
&lt;/p&gt;
&lt;p&gt;
Relational concept analysis (RCA) is an extension of formal concept analysis allowing to deal with several related contexts simultaneously. It has been designed for learning description logic theories from data and used within various applications. A puzzling observation about RCA is that it returns a single family of concept lattices although, when the data feature circular dependencies, other solutions may be considered acceptable. The semantics of RCA, provided in an operational way, does not shed light on this issue. In this report, we define these acceptable solutions as those families of concept lattices which belong to the space determined by the initial contexts (well-formed), cannot scale new attributes (saturated), and refer only to concepts of the family (self-supported). We adopt a functional view on the RCA process by defining the space of well-formed solutions and two functions on that space: one expansive and the other contractive. We show that the acceptable solutions a
&lt;/p&gt;</description></item><item><title>Whispering LLaMA&#26159;&#19968;&#31181;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#30340;&#36328;&#27169;&#24577;&#29983;&#25104;&#38169;&#35823;&#26657;&#27491;&#26694;&#26550;&#65292;&#36890;&#36807;&#34701;&#21512;&#22768;&#23398;&#20449;&#24687;&#21644;&#22806;&#37096;&#35821;&#35328;&#34920;&#31034;&#65292;&#29983;&#25104;&#20934;&#30830;&#30340;&#35821;&#38899;&#36716;&#24405;&#19978;&#19979;&#25991;&#65292;&#30456;&#23545;&#20110;n-best&#20551;&#35774;&#65292;&#35789;&#38169;&#35823;&#29575;&#24615;&#33021;&#25552;&#21319;&#20102;37.66%&#12290;</title><link>http://arxiv.org/abs/2310.06434</link><description>&lt;p&gt;
Whispering LLaMA&#65306;&#19968;&#31181;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#30340;&#36328;&#27169;&#24577;&#29983;&#25104;&#38169;&#35823;&#26657;&#27491;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Whispering LLaMA: A Cross-Modal Generative Error Correction Framework for Speech Recognition. (arXiv:2310.06434v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06434
&lt;/p&gt;
&lt;p&gt;
Whispering LLaMA&#26159;&#19968;&#31181;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#30340;&#36328;&#27169;&#24577;&#29983;&#25104;&#38169;&#35823;&#26657;&#27491;&#26694;&#26550;&#65292;&#36890;&#36807;&#34701;&#21512;&#22768;&#23398;&#20449;&#24687;&#21644;&#22806;&#37096;&#35821;&#35328;&#34920;&#31034;&#65292;&#29983;&#25104;&#20934;&#30830;&#30340;&#35821;&#38899;&#36716;&#24405;&#19978;&#19979;&#25991;&#65292;&#30456;&#23545;&#20110;n-best&#20551;&#35774;&#65292;&#35789;&#38169;&#35823;&#29575;&#24615;&#33021;&#25552;&#21319;&#20102;37.66%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#27169;&#24577;&#34701;&#21512;&#25216;&#26415;&#65292;&#29992;&#20110;&#29983;&#25104;&#20934;&#30830;&#30340;&#35821;&#38899;&#36716;&#24405;&#19978;&#19979;&#25991;&#65292;&#20197;&#36827;&#34892;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035; (ASR) &#30340;&#29983;&#25104;&#24335;&#38169;&#35823;&#26657;&#27491;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#25490;&#21517;&#30340;&#37325;&#26032;&#35780;&#20998;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#28789;&#27963;&#36816;&#29992;&#29420;&#29305;&#30340;&#21021;&#22987;&#21270;&#25216;&#26415;&#21644;&#21442;&#25968;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#35821;&#38899;&#21644;&#25991;&#26412;&#27169;&#22411;&#25552;&#21319;&#20102;ASR&#24615;&#33021;&#12290;&#36890;&#36807;&#23545;&#22810;&#26679;&#21270;&#30340;ASR&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#34701;&#21512;&#25216;&#26415;&#30340;&#31283;&#23450;&#24615;&#21644;&#21487;&#22797;&#29616;&#24615;&#65292;&#30456;&#23545;&#20110;n-best&#20551;&#35774;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#35789;&#38169;&#35823;&#29575;&#24615;&#33021;&#25552;&#21319;&#20102;37.66%&#12290;&#20026;&#20102;&#40723;&#21169;&#26410;&#26469;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#24320;&#28304;&#22312;https://github.com/Srijith-rkr/Whispering-LLaMA&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new cross-modal fusion technique designed for generative error correction in automatic speech recognition (ASR). Our methodology leverages both acoustic information and external linguistic representations to generate accurate speech transcription contexts. This marks a step towards a fresh paradigm in generative error correction within the realm of n-best hypotheses. Unlike the existing ranking-based rescoring methods, our approach adeptly uses distinct initialization techniques and parameter-efficient algorithms to boost ASR performance derived from pre-trained speech and text models. Through evaluation across diverse ASR datasets, we evaluate the stability and reproducibility of our fusion technique, demonstrating its improved word error rate relative (WERR) performance in comparison to n-best hypotheses by relatively 37.66%. To encourage future research, we have made our code and pre-trained models open source at https://github.com/Srijith-rkr/Whispering-LLaMA.
&lt;/p&gt;</description></item><item><title>&#36870;&#21521;&#27979;&#35797;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#40657;&#30418;&#27979;&#35797;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#21452;&#31243;&#24207;&#32467;&#26500;&#65292;&#21033;&#29992;&#36741;&#21161;&#31243;&#24207;&#23558;&#36755;&#20837;&#25968;&#25454;&#36827;&#34892;&#21069;&#21521;&#22788;&#29702;&#65292;&#28982;&#21518;&#20351;&#29992;&#21518;&#21521;&#31243;&#24207;&#23558;&#31243;&#24207;&#36755;&#20986;&#21453;&#36716;&#20026;&#21407;&#22987;&#36755;&#20837;&#26684;&#24335;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#27979;&#35797;Oracle&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.06433</link><description>&lt;p&gt;
&#36870;&#21521;&#27979;&#35797;&#65306;&#35299;&#20915;&#27979;&#35797;Oracle&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Retromorphic Testing: A New Approach to the Test Oracle Problem. (arXiv:2310.06433v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06433
&lt;/p&gt;
&lt;p&gt;
&#36870;&#21521;&#27979;&#35797;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#40657;&#30418;&#27979;&#35797;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#21452;&#31243;&#24207;&#32467;&#26500;&#65292;&#21033;&#29992;&#36741;&#21161;&#31243;&#24207;&#23558;&#36755;&#20837;&#25968;&#25454;&#36827;&#34892;&#21069;&#21521;&#22788;&#29702;&#65292;&#28982;&#21518;&#20351;&#29992;&#21518;&#21521;&#31243;&#24207;&#23558;&#31243;&#24207;&#36755;&#20986;&#21453;&#36716;&#20026;&#21407;&#22987;&#36755;&#20837;&#26684;&#24335;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#27979;&#35797;Oracle&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#35797;Oracle&#20316;&#20026;&#19968;&#20010;&#35780;&#20272;&#36719;&#20214;&#36755;&#20986;&#21644;&#32473;&#23450;&#36755;&#20837;&#38598;&#30340;&#39044;&#26399;&#34892;&#20026;&#20043;&#38388;&#23545;&#24212;&#20851;&#31995;&#30340;&#26631;&#20934;&#25110;&#26426;&#21046;&#12290;&#22312;&#33258;&#21160;&#21270;&#27979;&#35797;&#20013;&#65292;&#40657;&#30418;&#25216;&#26415;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20854;&#20013;&#21253;&#25324;&#24046;&#20998;&#27979;&#35797;&#21644;&#21464;&#24322;&#27979;&#35797;&#31561;&#33879;&#21517;&#26041;&#27861;&#65292;&#20197;&#20854;&#38750;&#20405;&#20837;&#24615;&#30340;&#27979;&#35797;Oracle&#26500;&#24314;&#26041;&#24335;&#32780;&#38395;&#21517;&#12290;&#21463;&#21040;&#25968;&#23398;&#27010;&#24565;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36870;&#21521;&#27979;&#35797;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#40657;&#30418;&#27979;&#35797;&#26041;&#27861;&#12290;&#23427;&#21033;&#29992;&#36741;&#21161;&#31243;&#24207;&#19982;&#34987;&#27979;&#35797;&#31243;&#24207;&#32467;&#21512;&#65292;&#24314;&#31435;&#20102;&#30001;&#21069;&#21521;&#31243;&#24207;&#21644;&#21518;&#21521;&#31243;&#24207;&#32452;&#25104;&#30340;&#21452;&#31243;&#24207;&#32467;&#26500;&#12290;&#36755;&#20837;&#25968;&#25454;&#39318;&#20808;&#30001;&#21069;&#21521;&#31243;&#24207;&#22788;&#29702;&#65292;&#28982;&#21518;&#20351;&#29992;&#21518;&#21521;&#31243;&#24207;&#23558;&#20854;&#31243;&#24207;&#36755;&#20986;&#21453;&#36716;&#20026;&#20854;&#21407;&#22987;&#36755;&#20837;&#26684;&#24335;&#12290;&#29305;&#21035;&#22320;&#65292;&#36741;&#21161;&#31243;&#24207;&#21487;&#20197;&#20316;&#20026;&#21069;&#21521;&#31243;&#24207;&#25110;&#21518;&#21521;&#31243;&#24207;&#36816;&#34892;&#65292;&#20174;&#32780;&#23548;&#33268;&#19981;&#21516;&#30340;&#27979;&#35797;&#27169;&#24335;&#12290;&#35813;&#36807;&#31243;&#36890;&#36807;&#26816;&#26597;&#36825;&#20004;&#20010;&#31243;&#24207;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#36827;&#34892;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
A test oracle serves as a criterion or mechanism to assess the correspondence between software output and the anticipated behavior for a given input set. In automated testing, black-box techniques, known for their non-intrusive nature in test oracle construction, are widely used, including notable methodologies like differential testing and metamorphic testing. Inspired by the mathematical concept of inverse function, we present Retromorphic Testing, a novel black-box testing methodology. It leverages an auxiliary program in conjunction with the program under test, which establishes a dual-program structure consisting of a forward program and a backward program. The input data is first processed by the forward program and then its program output is reversed to its original input format using the backward program. In particular, the auxiliary program can operate as either the forward or backward program, leading to different testing modes. The process concludes by examining the relation
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#38598;&#27719;&#38598;&#20102;&#26469;&#33258;&#20154;&#26426;&#20132;&#20114;&#12289;&#20132;&#20114;&#35774;&#35745;&#12289;&#20154;&#24037;&#26234;&#33021;&#21644;&#25968;&#23383;&#33402;&#26415;&#31561;&#39046;&#22495;&#30340;&#30740;&#31350;&#32773;&#65292;&#35752;&#35770;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#22312;&#33402;&#26415;&#39046;&#22495;&#30340;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.06428</link><description>&lt;p&gt;
&#31532;&#19968;&#23626;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#33402;&#26415;&#22269;&#38469;&#30740;&#35752;&#20250;(XAIxArts)&#35770;&#25991;&#38598;
&lt;/p&gt;
&lt;p&gt;
Proceedings of The first international workshop on eXplainable AI for the Arts (XAIxArts). (arXiv:2310.06428v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06428
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#38598;&#27719;&#38598;&#20102;&#26469;&#33258;&#20154;&#26426;&#20132;&#20114;&#12289;&#20132;&#20114;&#35774;&#35745;&#12289;&#20154;&#24037;&#26234;&#33021;&#21644;&#25968;&#23383;&#33402;&#26415;&#31561;&#39046;&#22495;&#30340;&#30740;&#31350;&#32773;&#65292;&#35752;&#35770;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#22312;&#33402;&#26415;&#39046;&#22495;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#31532;&#19968;&#23626;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#33402;&#26415;&#22269;&#38469;&#30740;&#35752;&#20250;&#65288;XAIxArts&#65289;&#27719;&#38598;&#20102;&#20154;&#26426;&#20132;&#20114;&#12289;&#20132;&#20114;&#35774;&#35745;&#12289;&#20154;&#24037;&#26234;&#33021;&#21644;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#65292;&#20197;&#21450;&#25968;&#23383;&#33402;&#26415;&#31561;&#39046;&#22495;&#30340;&#30740;&#31350;&#32773;&#65292;&#25506;&#35752;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#22312;&#33402;&#26415;&#39046;&#22495;&#30340;&#20316;&#29992;&#12290;&#27492;&#30740;&#35752;&#20250;&#22312;&#31532;15&#23626;ACM&#21019;&#26032;&#19982;&#35748;&#30693;&#22823;&#20250;&#65288;C&amp;C 2023&#65289;&#19978;&#20030;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
This first international workshop on explainable AI for the Arts (XAIxArts) brought together a community of researchers in HCI, Interaction Design, AI, explainable AI (XAI), and digital arts to explore the role of XAI for the Arts.  Workshop held at the 15th ACM Conference on Creativity and Cognition (C&amp;C 2023).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;TANGO&#26041;&#27861;&#65292;&#36890;&#36807;&#26102;&#38388;&#21453;&#28436;&#23545;&#31216;&#24615;&#20316;&#20026;&#24402;&#32435;&#20559;&#24046;&#65292;&#23545;&#22810;&#26234;&#33021;&#20307;&#21160;&#21147;&#31995;&#32479;&#36827;&#34892;&#23398;&#20064;&#65292;&#21363;&#20351;&#22312;&#38750;&#20445;&#23432;&#30340;&#21487;&#36870;&#31995;&#32479;&#20013;&#20063;&#33021;&#20445;&#25345;&#33021;&#37327;&#65292;&#24182;&#19988;&#33021;&#26356;&#39640;&#25928;&#22320;&#23398;&#20064;&#31995;&#32479;&#21160;&#21147;&#23398;&#12290;</title><link>http://arxiv.org/abs/2310.06427</link><description>&lt;p&gt;
TANGO: &#26102;&#38388;&#21453;&#28436;&#28508;&#22312;&#22270;ODE&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#21160;&#21147;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
TANGO: Time-Reversal Latent GraphODE for Multi-Agent Dynamical Systems. (arXiv:2310.06427v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;TANGO&#26041;&#27861;&#65292;&#36890;&#36807;&#26102;&#38388;&#21453;&#28436;&#23545;&#31216;&#24615;&#20316;&#20026;&#24402;&#32435;&#20559;&#24046;&#65292;&#23545;&#22810;&#26234;&#33021;&#20307;&#21160;&#21147;&#31995;&#32479;&#36827;&#34892;&#23398;&#20064;&#65292;&#21363;&#20351;&#22312;&#38750;&#20445;&#23432;&#30340;&#21487;&#36870;&#31995;&#32479;&#20013;&#20063;&#33021;&#20445;&#25345;&#33021;&#37327;&#65292;&#24182;&#19988;&#33021;&#26356;&#39640;&#25928;&#22320;&#23398;&#20064;&#31995;&#32479;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#65292;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#22797;&#26434;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#21160;&#21147;&#23398;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20363;&#22914;&#22312;&#29289;&#29702;&#27169;&#25311;&#21644;&#26448;&#26009;&#24314;&#27169;&#20013;&#12290;&#22312;&#32431;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#26041;&#27861;&#22914;Hamiltonian&#31070;&#32463;&#32593;&#32476;&#20005;&#26684;&#36981;&#24490;&#33021;&#37327;&#23432;&#24658;&#23450;&#24459;&#24341;&#20837;&#24402;&#32435;&#20559;&#24046;&#65292;&#20351;&#24471;&#23398;&#20064;&#26356;&#21152;&#39640;&#25928;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#31995;&#32479;&#24182;&#19981;&#20005;&#26684;&#36981;&#23432;&#33021;&#37327;&#23432;&#24658;&#23450;&#24459;&#65292;&#22914;&#24102;&#26377;&#25705;&#25830;&#30340;&#24377;&#31783;&#31995;&#32479;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#23558;&#27880;&#24847;&#21147;&#36716;&#21521;&#19968;&#20010;&#26356;&#24191;&#27867;&#30340;&#29289;&#29702;&#21407;&#29702;&#65306;&#26102;&#38388;&#21453;&#28436;&#23545;&#31216;&#24615;&#65292;&#23427;&#25551;&#36848;&#20102;&#24403;&#31995;&#32479;&#30340;&#21160;&#21147;&#22312;&#26102;&#38388;&#19978;&#20498;&#36716;&#26102;&#65292;&#21160;&#21147;&#23398;&#24212;&#20445;&#25345;&#19981;&#21464;&#12290;&#36825;&#20173;&#28982;&#26377;&#21161;&#20110;&#20445;&#25345;&#20445;&#23432;&#31995;&#32479;&#30340;&#33021;&#37327;&#65292;&#24182;&#21516;&#26102;&#20026;&#38750;&#20445;&#23432;&#30340;&#21487;&#36870;&#31995;&#32479;&#25552;&#20379;&#24378;&#22823;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#20026;&#20102;&#27880;&#20837;&#36825;&#31181;&#24402;&#32435;&#20559;&#24046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33258;&#30417;&#30563;&#27491;&#21017;&#21270;&#39033;&#20316;&#20026;&#36719;&#32422;&#26463;&#65292;&#23558;&#27491;&#21521;&#21644;&#36870;&#21521;&#21160;&#21147;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning complex multi-agent system dynamics from data is crucial across many domains, such as in physical simulations and material modeling. Extended from purely data-driven approaches, existing physics-informed approaches such as Hamiltonian Neural Network strictly follow energy conservation law to introduce inductive bias, making their learning more sample efficiently. However, many real-world systems do not strictly conserve energy, such as spring systems with frictions. Recognizing this, we turn our attention to a broader physical principle: Time-Reversal Symmetry, which depicts that the dynamics of a system shall remain invariant when traversed back over time. It still helps to preserve energies for conservative systems and in the meanwhile, serves as a strong inductive bias for non-conservative, reversible systems. To inject such inductive bias, in this paper, we propose a simple-yet-effective self-supervised regularization term as a soft constraint that aligns the forward and b
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-3&#21644;GPT-4&#22312;&#23459;&#20256;&#20449;&#24687;&#26816;&#27979;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-4&#36798;&#21040;&#20102;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#31526;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.06422</link><description>&lt;p&gt;
&#29992;&#20110;&#20256;&#25773;&#20449;&#24687;&#26816;&#27979;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Propaganda Detection. (arXiv:2310.06422v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06422
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-3&#21644;GPT-4&#22312;&#23459;&#20256;&#20449;&#24687;&#26816;&#27979;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-4&#36798;&#21040;&#20102;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#31526;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25105;&#20204;&#25968;&#23383;&#21270;&#31038;&#20250;&#20013;&#65292;&#23459;&#20256;&#20449;&#24687;&#30340;&#26222;&#36941;&#23384;&#22312;&#23545;&#31038;&#20250;&#21644;&#30495;&#30456;&#30340;&#20256;&#25773;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#25991;&#26412;&#20013;&#26816;&#27979;&#23459;&#20256;&#20449;&#24687;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23384;&#22312;&#24494;&#22937;&#30340;&#25805;&#32437;&#25216;&#26415;&#21644;&#35821;&#22659;&#20381;&#36182;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-3&#21644;GPT-4&#22312;&#23459;&#20256;&#20449;&#24687;&#26816;&#27979;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;SemEval-2020&#20219;&#21153;11&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20855;&#26377;14&#31181;&#23459;&#20256;&#25216;&#26415;&#26631;&#31614;&#30340;&#26032;&#38395;&#25991;&#31456;&#65292;&#20316;&#20026;&#19968;&#20010;&#22810;&#26631;&#31614;&#20998;&#31867;&#38382;&#39064;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;GPT-3&#21644;GPT-4&#30340;&#20116;&#31181;&#21464;&#20307;&#65292;&#32467;&#21512;&#20102;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#30340;&#21508;&#31181;&#25552;&#31034;&#24037;&#31243;&#21644;&#24494;&#35843;&#31574;&#30053;&#12290;&#36890;&#36807;&#35780;&#20272;$F1$&#20998;&#25968;&#65292;$Precision$&#21644;$Recall$&#31561;&#25351;&#26631;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#23558;&#32467;&#26524;&#19982;&#20351;&#29992;RoBERTa&#30340;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-4&#23454;&#29616;&#20102;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prevalence of propaganda in our digital society poses a challenge to societal harmony and the dissemination of truth. Detecting propaganda through NLP in text is challenging due to subtle manipulation techniques and contextual dependencies. To address this issue, we investigate the effectiveness of modern Large Language Models (LLMs) such as GPT-3 and GPT-4 for propaganda detection. We conduct experiments using the SemEval-2020 task 11 dataset, which features news articles labeled with 14 propaganda techniques as a multi-label classification problem. Five variations of GPT-3 and GPT-4 are employed, incorporating various prompt engineering and fine-tuning strategies across the different models. We evaluate the models' performance by assessing metrics such as $F1$ score, $Precision$, and $Recall$, comparing the results with the current state-of-the-art approach using RoBERTa. Our findings demonstrate that GPT-4 achieves comparable results to the current state-of-the-art. Further, thi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#19981;&#21516;&#30340;&#22270;&#25299;&#25169;&#23384;&#22312;&#19979;&#65292;&#22270;&#25193;&#25955;&#26041;&#31243;&#22914;&#20309;&#23545;GNN&#36827;&#34892;&#22806;&#25512;&#21644;&#27010;&#25324;&#65292;&#25581;&#31034;&#20102;&#22522;&#20110;&#23616;&#37096;&#25193;&#25955;&#30340;&#29616;&#26377;&#27169;&#22411;&#22312;&#27010;&#25324;&#33021;&#21147;&#19978;&#30340;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#38750;&#23616;&#37096;&#25193;&#25955;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.06417</link><description>&lt;p&gt;
&#29992;&#20110;&#22270;&#23398;&#20064;&#20013;&#30340;&#25299;&#25169;&#27010;&#25324;&#30340;&#27969;&#21160;&#25193;&#25955;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Advective Diffusion Transformers for Topological Generalization in Graph Learning. (arXiv:2310.06417v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#19981;&#21516;&#30340;&#22270;&#25299;&#25169;&#23384;&#22312;&#19979;&#65292;&#22270;&#25193;&#25955;&#26041;&#31243;&#22914;&#20309;&#23545;GNN&#36827;&#34892;&#22806;&#25512;&#21644;&#27010;&#25324;&#65292;&#25581;&#31034;&#20102;&#22522;&#20110;&#23616;&#37096;&#25193;&#25955;&#30340;&#29616;&#26377;&#27169;&#22411;&#22312;&#27010;&#25324;&#33021;&#21147;&#19978;&#30340;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#38750;&#23616;&#37096;&#25193;&#25955;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#25193;&#25955;&#26041;&#31243;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23494;&#20999;&#30456;&#20851;&#65292;&#24182;&#19988;&#26368;&#36817;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#20316;&#20026;&#20998;&#26512;GNN&#21160;&#21147;&#23398;&#12289;&#24418;&#24335;&#21270;&#20854;&#34920;&#36798;&#33021;&#21147;&#21644;&#35777;&#26126;&#26550;&#26500;&#36873;&#25321;&#30340;&#26377;&#21407;&#21017;&#30340;&#26694;&#26550;&#12290;&#22270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;GNN&#30340;&#27010;&#25324;&#33021;&#21147;&#12290;&#24403;&#21069;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#38480;&#21046;&#22312;&#20110;&#20551;&#35774;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#20013;&#30340;&#22270;&#25299;&#25169;&#26469;&#33258;&#30456;&#21516;&#30340;&#20998;&#24067;&#12290;&#26412;&#25991;&#36890;&#36807;&#25506;&#32034;&#22270;&#25193;&#25955;&#26041;&#31243;&#22312;&#19981;&#21516;&#22270;&#25299;&#25169;&#23384;&#22312;&#19979;&#30340;&#22806;&#25512;&#21644;&#27010;&#25324;&#33021;&#21147;&#65292;&#36808;&#20986;&#20102;&#35299;&#26512;GNN&#27010;&#25324;&#24615;&#30340;&#19968;&#27493;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#22522;&#20110;&#22270;&#19978;&#23616;&#37096;&#25193;&#25955;&#30340;&#29616;&#26377;&#27169;&#22411;&#22312;&#27010;&#25324;&#33021;&#21147;&#19978;&#30340;&#19981;&#36275;&#65292;&#36825;&#26159;&#30001;&#20110;&#23545;&#25299;&#25169;&#21464;&#21270;&#30340;&#25351;&#25968;&#25935;&#24863;&#24615;&#24341;&#36215;&#30340;&#12290;&#38543;&#21518;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#38750;&#23616;&#37096;&#25193;&#25955;&#30340;&#28508;&#21147;&#65292;&#23427;&#20513;&#23548;&#23545;&#23436;&#20840;&#36830;&#25509;&#30340;&#28508;&#22312;&#22270;&#36827;&#34892;&#29305;&#24449;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph diffusion equations are intimately related to graph neural networks (GNNs) and have recently attracted attention as a principled framework for analyzing GNN dynamics, formalizing their expressive power, and justifying architectural choices. One key open questions in graph learning is the generalization capabilities of GNNs. A major limitation of current approaches hinges on the assumption that the graph topologies in the training and test sets come from the same distribution. In this paper, we make steps towards understanding the generalization of GNNs by exploring how graph diffusion equations extrapolate and generalize in the presence of varying graph topologies. We first show deficiencies in the generalization capability of existing models built upon local diffusion on graphs, stemming from the exponential sensitivity to topology variation. Our subsequent analysis reveals the promise of non-local diffusion, which advocates for feature propagation over fully-connected latent gr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#25552;&#21319;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#29983;&#25104;&#30340;&#20013;&#38388;&#27493;&#39588;&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;&#36890;&#36807;&#24341;&#20837;&#24341;&#23548;&#25552;&#31034;&#21644;&#20462;&#25913;&#25439;&#22833;&#20989;&#25968;&#30340;&#33258;&#20030;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#33258;&#21160;&#29983;&#25104;&#22238;&#31572;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.06404</link><description>&lt;p&gt;
Hexa: &#30693;&#35782;&#39537;&#21160;&#30340;&#23545;&#35805;&#31995;&#32479;&#30340;&#33258;&#25105;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;
Hexa: Self-Improving for Knowledge-Grounded Dialogue System. (arXiv:2310.06404v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#25552;&#21319;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#29983;&#25104;&#30340;&#20013;&#38388;&#27493;&#39588;&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;&#36890;&#36807;&#24341;&#20837;&#24341;&#23548;&#25552;&#31034;&#21644;&#20462;&#25913;&#25439;&#22833;&#20989;&#25968;&#30340;&#33258;&#20030;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#33258;&#21160;&#29983;&#25104;&#22238;&#31572;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#39537;&#21160;&#30340;&#23545;&#35805;&#29983;&#25104;&#20013;&#19968;&#31181;&#24120;&#35265;&#30340;&#20570;&#27861;&#26159;&#20351;&#29992;&#27169;&#22359;&#21270;&#30340;&#26041;&#27861;&#26126;&#30830;&#22320;&#21033;&#29992;&#20013;&#38388;&#27493;&#39588;&#65288;&#22914;&#32593;&#32476;&#25628;&#32034;&#12289;&#35760;&#24518;&#26816;&#32034;&#65289;&#12290;&#28982;&#32780;&#65292;&#19982;&#23545;&#35805;&#21709;&#24212;&#30456;&#27604;&#65292;&#36825;&#20123;&#27493;&#39588;&#30340;&#25968;&#25454;&#24448;&#24448;&#38590;&#20197;&#33719;&#21462;&#65292;&#22240;&#20026;&#22312;&#26222;&#36890;&#23545;&#35805;&#20013;&#26080;&#27861;&#35266;&#23519;&#21040;&#23427;&#20204;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#25968;&#25454;&#30340;&#32570;&#22833;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#25105;&#25552;&#21319;&#26041;&#27861;&#65292;&#20197;&#25913;&#36827;&#20013;&#38388;&#27493;&#39588;&#30340;&#29983;&#25104;&#24615;&#33021;&#65292;&#32780;&#19981;&#38656;&#35201;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24341;&#23548;&#25552;&#31034;&#21644;&#20462;&#25913;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#24341;&#23548;&#33258;&#21160;&#29983;&#25104;&#22238;&#31572;&#22810;&#26679;&#24615;&#30340;&#33258;&#20030;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#32463;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#21033;&#29992;&#20102;&#33258;&#25105;&#25552;&#21319;&#26426;&#21046;&#65292;&#22312;&#29983;&#25104;&#20013;&#38388;&#21644;&#26368;&#32456;&#22238;&#31572;&#26041;&#38754;&#25913;&#21892;&#20102;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#29983;&#25104;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common practice in knowledge-grounded dialogue generation is to explicitly utilize intermediate steps (e.g., web-search, memory retrieval) with modular approaches. However, data for such steps are often inaccessible compared to those of dialogue responses as they are unobservable in an ordinary dialogue. To fill in the absence of these data, we develop a self-improving method to improve the generative performances of intermediate steps without the ground truth data. In particular, we propose a novel bootstrapping scheme with a guided prompt and a modified loss function to enhance the diversity of appropriate self-generated responses. Through experiments on various benchmark datasets, we empirically demonstrate that our method successfully leverages a self-improving mechanism in generating intermediate and final responses and improves the performances on the task of knowledge-grounded dialogue generation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;Lo-Hi&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#21069;&#23548;&#20248;&#21270;&#21644;&#21629;&#20013;&#35782;&#21035;&#20004;&#20010;&#20219;&#21153;&#65292;&#20026;&#33647;&#29289;&#21457;&#29616;&#36807;&#31243;&#25552;&#20379;&#20102;&#19968;&#31181;&#20999;&#23454;&#21487;&#34892;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;&#23545;&#20110;&#21629;&#20013;&#35782;&#21035;&#20219;&#21153;&#65292;&#30740;&#31350;&#32773;&#35774;&#35745;&#20102;&#19968;&#31181;&#35299;&#20915;&#39030;&#28857;&#26368;&#23567;k-Cut&#38382;&#39064;&#30340;&#26032;&#22411;&#20998;&#23376;&#25286;&#20998;&#31639;&#27861;&#65292;&#24182;&#27979;&#35797;&#20102;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#19981;&#29616;&#23454;&#19988;&#36807;&#20110;&#20048;&#35266;&#12290;</title><link>http://arxiv.org/abs/2310.06399</link><description>&lt;p&gt;
Lo-Hi: &#23454;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#33647;&#29289;&#21457;&#29616;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Lo-Hi: Practical ML Drug Discovery Benchmark. (arXiv:2310.06399v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;Lo-Hi&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#21069;&#23548;&#20248;&#21270;&#21644;&#21629;&#20013;&#35782;&#21035;&#20004;&#20010;&#20219;&#21153;&#65292;&#20026;&#33647;&#29289;&#21457;&#29616;&#36807;&#31243;&#25552;&#20379;&#20102;&#19968;&#31181;&#20999;&#23454;&#21487;&#34892;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;&#23545;&#20110;&#21629;&#20013;&#35782;&#21035;&#20219;&#21153;&#65292;&#30740;&#31350;&#32773;&#35774;&#35745;&#20102;&#19968;&#31181;&#35299;&#20915;&#39030;&#28857;&#26368;&#23567;k-Cut&#38382;&#39064;&#30340;&#26032;&#22411;&#20998;&#23376;&#25286;&#20998;&#31639;&#27861;&#65292;&#24182;&#27979;&#35797;&#20102;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#19981;&#29616;&#23454;&#19988;&#36807;&#20110;&#20048;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23547;&#25214;&#26032;&#33647;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#12290;&#33647;&#29289;&#21457;&#29616;&#30340;&#24076;&#26395;&#20043;&#19968;&#26159;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#20998;&#23376;&#23646;&#24615;&#12290;&#22240;&#27492;&#65292;&#27491;&#22312;&#24320;&#21457;&#21644;&#27979;&#35797;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#27169;&#22411;&#65292;&#20197;&#22312;MoleculeNet&#31561;&#22522;&#20934;&#27979;&#35797;&#20013;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#19981;&#20999;&#23454;&#38469;&#65292;&#24182;&#19988;&#19982;&#23454;&#38469;&#24212;&#29992;&#27169;&#22411;&#30456;&#24046;&#22826;&#22823;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#29992;&#30340;Lo-Hi&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#20004;&#20010;&#20219;&#21153;&#65306;&#21069;&#23548;&#20248;&#21270;&#65288;Lo&#65289;&#21644;&#21629;&#20013;&#35782;&#21035;&#65288;Hi&#65289;&#65292;&#23545;&#24212;&#20110;&#30495;&#23454;&#30340;&#33647;&#29289;&#21457;&#29616;&#36807;&#31243;&#12290;&#23545;&#20110;Hi&#20219;&#21153;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#35299;&#20915;&#24179;&#34913;&#39030;&#28857;&#26368;&#23567;k-Cut&#38382;&#39064;&#30340;&#26032;&#22411;&#20998;&#23376;&#25286;&#20998;&#31639;&#27861;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#26368;&#20808;&#36827;&#21644;&#32463;&#20856;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#21738;&#31181;&#25928;&#26524;&#26356;&#22909;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#29616;&#20195;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#26174;&#31034;&#23427;&#20204;&#19981;&#20999;&#23454;&#38469;&#24182;&#19988;&#36807;&#20110;&#20048;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding new drugs is getting harder and harder. One of the hopes of drug discovery is to use machine learning models to predict molecular properties. That is why models for molecular property prediction are being developed and tested on benchmarks such as MoleculeNet. However, existing benchmarks are unrealistic and are too different from applying the models in practice. We have created a new practical \emph{Lo-Hi} benchmark consisting of two tasks: Lead Optimization (Lo) and Hit Identification (Hi), corresponding to the real drug discovery process. For the Hi task, we designed a novel molecular splitting algorithm that solves the Balanced Vertex Minimum $k$-Cut problem. We tested state-of-the-art and classic ML models, revealing which works better under practical settings. We analyzed modern benchmarks and showed that they are unrealistic and overoptimistic.  Review: https://openreview.net/forum?id=H2Yb28qGLV  Lo-Hi benchmark: https://github.com/SteshinSS/lohi_neurips2023  Lo-Hi split
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#20010;&#20154;&#35282;&#33394;&#25552;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#22238;&#31572;&#36873;&#25321;&#30340;&#26816;&#32034;&#23545;&#35805;&#26426;&#22120;&#20154;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#34920;&#29616;&#33391;&#22909;&#65292;&#20943;&#23569;&#20102;&#23545;&#22522;&#20110;&#20010;&#20154;&#35282;&#33394;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#20381;&#36182;&#65292;&#24182;&#19988;&#20351;&#24471;&#31995;&#32479;&#26356;&#23481;&#26131;&#25193;&#23637;&#21040;&#20854;&#20182;&#35821;&#35328;&#12290;</title><link>http://arxiv.org/abs/2310.06390</link><description>&lt;p&gt;
P5: &#29992;&#20110;&#20010;&#24615;&#21270;&#22238;&#31572;&#36873;&#25321;&#30340;&#21363;&#25554;&#21363;&#29992;&#20010;&#20154;&#35282;&#33394;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
P5: Plug-and-Play Persona Prompting for Personalized Response Selection. (arXiv:2310.06390v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06390
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#20010;&#20154;&#35282;&#33394;&#25552;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#22238;&#31572;&#36873;&#25321;&#30340;&#26816;&#32034;&#23545;&#35805;&#26426;&#22120;&#20154;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#34920;&#29616;&#33391;&#22909;&#65292;&#20943;&#23569;&#20102;&#23545;&#22522;&#20110;&#20010;&#20154;&#35282;&#33394;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#20381;&#36182;&#65292;&#24182;&#19988;&#20351;&#24471;&#31995;&#32479;&#26356;&#23481;&#26131;&#25193;&#23637;&#21040;&#20854;&#20182;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#20010;&#20154;&#35282;&#33394;&#30340;&#26816;&#32034;&#23545;&#35805;&#26426;&#22120;&#20154;&#23545;&#20110;&#20010;&#24615;&#21270;&#23545;&#35805;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20063;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#12290;1&#65289;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#25910;&#38598;&#22522;&#20110;&#20010;&#20154;&#35282;&#33394;&#30340;&#35821;&#26009;&#24211;&#38750;&#24120;&#26114;&#36149;&#12290;2&#65289;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#23545;&#35805;&#26426;&#22120;&#20154;&#31995;&#32479;&#24182;&#19981;&#24635;&#26159;&#26681;&#25454;&#20010;&#20154;&#35282;&#33394;&#20570;&#20986;&#22238;&#24212;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#20010;&#20154;&#35282;&#33394;&#25552;&#31034;&#26041;&#27861;&#12290;&#22914;&#26524;&#20010;&#20154;&#35282;&#33394;&#20449;&#24687;&#19981;&#21487;&#29992;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#21487;&#20197;&#20316;&#20026;&#26631;&#20934;&#30340;&#24320;&#25918;&#22495;&#23545;&#35805;&#26426;&#22120;&#20154;&#36816;&#34892;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#34920;&#29616;&#33391;&#22909;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#23545;&#22522;&#20110;&#20010;&#20154;&#35282;&#33394;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#20381;&#36182;&#12290;&#36825;&#20351;&#24471;&#22312;&#26080;&#38656;&#26500;&#24314;&#22522;&#20110;&#20010;&#20154;&#35282;&#33394;&#30340;&#35821;&#26009;&#24211;&#30340;&#24773;&#20917;&#19979;&#65292;&#26356;&#23481;&#26131;&#23558;&#35813;&#31995;&#32479;&#25193;&#23637;&#21040;&#20854;&#20182;&#35821;&#35328;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#36827;&#34892;&#24494;&#35843;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#38646;&#26679;&#26412;&#27169;&#22411;&#22312;&#21407;&#22987;&#20010;&#20154;&#35282;&#33394;&#21644;&#20462;&#35746;&#20010;&#20154;&#35282;&#33394;&#26041;&#38754;&#20998;&#21035;&#25552;&#39640;&#20102;7.71&#21644;1.04&#20010;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of persona-grounded retrieval-based chatbots is crucial for personalized conversations, but there are several challenges that need to be addressed. 1) In general, collecting persona-grounded corpus is very expensive. 2) The chatbot system does not always respond in consideration of persona at real applications. To address these challenges, we propose a plug-and-play persona prompting method. Our system can function as a standard open-domain chatbot if persona information is not available. We demonstrate that this approach performs well in the zero-shot setting, which reduces the dependence on persona-ground training data. This makes it easier to expand the system to other languages without the need to build a persona-grounded corpus. Additionally, our model can be fine-tuned for even better performance. In our experiments, the zero-shot model improved the standard model by 7.71 and 1.04 points in the original persona and revised persona, respectively. The fine-tuned model impro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#23569;&#37327;&#19978;&#19979;&#25991;&#31034;&#33539;&#26469;&#25805;&#32437;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#25552;&#20379;&#31034;&#33539;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#65292;&#21487;&#20197;&#22686;&#21152;&#25110;&#38477;&#20302;&#27169;&#22411;&#22238;&#31572;&#24694;&#24847;&#25552;&#31034;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#30456;&#21516;&#19978;&#19979;&#25991;&#25915;&#20987;&#21644;&#30456;&#21516;&#19978;&#19979;&#25991;&#38450;&#24481;&#26041;&#27861;&#65292;&#29992;&#20110;&#36234;&#29425;&#21644;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.06387</link><description>&lt;p&gt;
&#21482;&#38656;&#23569;&#37327;&#19978;&#19979;&#25991;&#31034;&#33539;&#21363;&#21487;&#23454;&#29616;&#36234;&#29425;&#21644;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations. (arXiv:2310.06387v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#23569;&#37327;&#19978;&#19979;&#25991;&#31034;&#33539;&#26469;&#25805;&#32437;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#25552;&#20379;&#31034;&#33539;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#65292;&#21487;&#20197;&#22686;&#21152;&#25110;&#38477;&#20302;&#27169;&#22411;&#22238;&#31572;&#24694;&#24847;&#25552;&#31034;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#30456;&#21516;&#19978;&#19979;&#25991;&#25915;&#20987;&#21644;&#30456;&#21516;&#19978;&#19979;&#25991;&#38450;&#24481;&#26041;&#27861;&#65292;&#29992;&#20110;&#36234;&#29425;&#21644;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#23545;&#20854;&#23433;&#20840;&#24615;&#21644;&#29983;&#25104;&#24694;&#24847;&#20869;&#23481;&#30340;&#28508;&#22312;&#39118;&#38505;&#30340;&#25285;&#24551;&#20063;&#28014;&#29616;&#20986;&#26469;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#30456;&#21516;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#20013;&#25805;&#32437;LLM&#23545;&#40784;&#33021;&#21147;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20165;&#36890;&#36807;&#23569;&#37327;&#30340;&#19978;&#19979;&#25991;&#31034;&#33539;&#32780;&#26080;&#38656;&#24494;&#35843;&#65292;&#23601;&#21487;&#20197;&#25805;&#32437;LLM&#22686;&#21152;&#25110;&#38477;&#20302;&#36234;&#29425;&#27010;&#29575;&#65292;&#21363;&#22238;&#31572;&#24694;&#24847;&#25552;&#31034;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#36234;&#29425;&#21644;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#30446;&#30340;&#30340;&#30456;&#21516;&#19978;&#19979;&#25991;&#25915;&#20987;&#65288;ICA&#65289;&#21644;&#30456;&#21516;&#19978;&#19979;&#25991;&#38450;&#24481;&#65288;ICD&#65289;&#26041;&#27861;&#12290;ICA&#36890;&#36807;&#26500;&#36896;&#24694;&#24847;&#19978;&#19979;&#25991;&#25351;&#23548;&#27169;&#22411;&#29983;&#25104;&#26377;&#23475;&#36755;&#20986;&#65292;&#32780;ICD&#36890;&#36807;&#25298;&#32477;&#22238;&#31572;&#26377;&#23475;&#25552;&#31034;&#30340;&#31034;&#33539;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;ICA&#21644;ICD&#22312;&#22686;&#21152;&#25110;&#38477;&#20302;&#23545;&#25239;&#36234;&#29425;&#25915;&#20987;&#25104;&#21151;&#29575;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;ICL&#22312;&#36234;&#29425;&#21644;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown remarkable success in various tasks, but concerns about their safety and the potential for generating malicious content have emerged. In this paper, we explore the power of In-Context Learning (ICL) in manipulating the alignment ability of LLMs. We find that by providing just few in-context demonstrations without fine-tuning, LLMs can be manipulated to increase or decrease the probability of jailbreaking, i.e. answering malicious prompts. Based on these observations, we propose In-Context Attack (ICA) and In-Context Defense (ICD) methods for jailbreaking and guarding aligned language model purposes. ICA crafts malicious contexts to guide models in generating harmful outputs, while ICD enhances model robustness by demonstrations of rejecting to answer harmful prompts. Our experiments show the effectiveness of ICA and ICD in increasing or reducing the success rate of adversarial jailbreaking attacks. Overall, we shed light on the potential of ICL t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#23545;&#22810;&#27169;&#24577;&#27169;&#22411;&#36935;&#21040;&#32570;&#22833;&#27169;&#24577;&#30340;&#24773;&#26223;&#36827;&#34892;&#20102;&#24314;&#27169;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#20851;&#38190;&#35201;&#28857;&#65306;&#32534;&#30721;&#22120;&#33021;&#22815;&#20174;&#38750;&#32570;&#22833;&#27169;&#24577;&#20013;&#25552;&#21462;&#22909;&#30340;&#29305;&#24449;&#65292;&#24182;&#19988;&#25552;&#21462;&#30340;&#29305;&#24449;&#22312;&#27169;&#24577;&#34701;&#21512;&#36807;&#31243;&#20013;&#20855;&#26377;&#36275;&#22815;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.06383</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#27169;&#22411;&#38754;&#23545;&#32570;&#22833;&#27169;&#24577;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
What Makes for Robust Multi-Modal Models in the Face of Missing Modalities?. (arXiv:2310.06383v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06383
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#23545;&#22810;&#27169;&#24577;&#27169;&#22411;&#36935;&#21040;&#32570;&#22833;&#27169;&#24577;&#30340;&#24773;&#26223;&#36827;&#34892;&#20102;&#24314;&#27169;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#20851;&#38190;&#35201;&#28857;&#65306;&#32534;&#30721;&#22120;&#33021;&#22815;&#20174;&#38750;&#32570;&#22833;&#27169;&#24577;&#20013;&#25552;&#21462;&#22909;&#30340;&#29305;&#24449;&#65292;&#24182;&#19988;&#25552;&#21462;&#30340;&#29305;&#24449;&#22312;&#27169;&#24577;&#34701;&#21512;&#36807;&#31243;&#20013;&#20855;&#26377;&#36275;&#22815;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#25104;&#21151;&#21457;&#23637;&#65292;&#23545;&#20110;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#38754;&#23545;&#32570;&#22833;&#27169;&#24577;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#30340;&#30740;&#31350;&#21463;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#22312;&#27492;&#39046;&#22495;&#30340;&#30740;&#31350;&#23384;&#22312;&#26576;&#20123;&#38480;&#21046;&#65292;&#22240;&#20026;&#23427;&#20204;&#24448;&#24448;&#32570;&#20047;&#29702;&#35770;&#27934;&#23519;&#21147;&#65292;&#25110;&#32773;&#20854;&#26041;&#27861;&#20165;&#36866;&#29992;&#20110;&#29305;&#23450;&#30340;&#32593;&#32476;&#26550;&#26500;&#25110;&#27169;&#24577;&#12290;&#25105;&#20204;&#20174;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#23545;&#22810;&#27169;&#24577;&#27169;&#22411;&#36935;&#21040;&#32570;&#22833;&#27169;&#24577;&#30340;&#24773;&#26223;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#35828;&#26126;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#38750;&#32570;&#22833;&#27169;&#24577;&#20013;&#22266;&#26377;&#30340;&#20449;&#24687;&#21487;&#20197;&#25509;&#36817;&#24615;&#33021;&#19978;&#38480;&#12290;&#23454;&#38469;&#19978;&#65292;&#26377;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;&#65306;&#65288;1&#65289;&#32534;&#30721;&#22120;&#24212;&#33021;&#22815;&#20174;&#38750;&#32570;&#22833;&#27169;&#24577;&#20013;&#25552;&#21462;&#36275;&#22815;&#22909;&#30340;&#29305;&#24449;&#65307;&#65288;2&#65289;&#25552;&#21462;&#30340;&#29305;&#24449;&#22312;&#27169;&#24577;&#38388;&#34701;&#21512;&#36807;&#31243;&#20013;&#24212;&#20855;&#26377;&#36275;&#22815;&#30340;&#40065;&#26834;&#24615;&#65292;&#19981;&#21463;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the growing success of multi-modal learning, research on the robustness of multi-modal models, especially when facing situations with missing modalities, is receiving increased attention. Nevertheless, previous studies in this domain exhibit certain limitations, as they often lack theoretical insights or their methodologies are tied to specific network architectures or modalities. We model the scenarios of multi-modal models encountering missing modalities from an information-theoretic perspective and illustrate that the performance ceiling in such scenarios can be approached by efficiently utilizing the information inherent in non-missing modalities. In practice, there are two key aspects: (1) The encoder should be able to extract sufficiently good features from the non-missing modality; (2) The extracted features should be robust enough not to be influenced by noise during the fusion process across modalities. To this end, we introduce Uni-Modal Ensemble with Missing Modality Ad
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#33033;&#20914;&#32593;&#32476;&#21644;&#22810;&#30418;&#26816;&#27979;&#25216;&#26415;&#30340;&#39640;&#25928;&#12289;&#21487;&#38752;&#12289;&#33021;&#25928;&#39640;&#30340;&#40657;&#33394;&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#39044;&#35757;&#32451;&#30340;VGG16&#27169;&#22411;&#21644;&#33033;&#20914;&#21644;&#26222;&#36890;&#21367;&#31215;&#23618;&#65292;&#33021;&#22815;&#20934;&#30830;&#26816;&#27979;&#40657;&#33394;&#29289;&#20307;&#36793;&#30028;&#26694;&#65292;&#24182;&#23454;&#29616;&#39640;&#25928;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2310.06370</link><description>&lt;p&gt;
&#22522;&#20110;&#33033;&#20914;&#32593;&#32476;&#21644;&#22810;&#30418;&#26816;&#27979;&#30340;&#40657;&#33394;&#29289;&#20307;&#26816;&#27979;&#30340;&#39640;&#25928;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Advanced Efficient Strategy for Detection of Dark Objects Based on Spiking Network with Multi-Box Detection. (arXiv:2310.06370v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#33033;&#20914;&#32593;&#32476;&#21644;&#22810;&#30418;&#26816;&#27979;&#25216;&#26415;&#30340;&#39640;&#25928;&#12289;&#21487;&#38752;&#12289;&#33021;&#25928;&#39640;&#30340;&#40657;&#33394;&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#39044;&#35757;&#32451;&#30340;VGG16&#27169;&#22411;&#21644;&#33033;&#20914;&#21644;&#26222;&#36890;&#21367;&#31215;&#23618;&#65292;&#33021;&#22815;&#20934;&#30830;&#26816;&#27979;&#40657;&#33394;&#29289;&#20307;&#36793;&#30028;&#26694;&#65292;&#24182;&#23454;&#29616;&#39640;&#25928;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#31181;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#24050;&#32463;&#22312;&#29616;&#26377;&#30340;&#29289;&#20307;&#26816;&#27979;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#24778;&#20154;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#35782;&#21035;&#36739;&#26263;&#30340;&#29289;&#20307;&#26159;&#26368;&#22823;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#25216;&#26415;&#22312;&#26816;&#27979;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#25110;&#32773;&#35782;&#21035;&#36895;&#24230;&#36739;&#24930;&#65292;&#23548;&#33268;&#24615;&#33021;&#25439;&#22833;&#26126;&#26174;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#31181;&#25913;&#36827;&#30340;&#12289;&#20934;&#30830;&#30340;&#26816;&#27979;&#26041;&#27861;&#26469;&#35299;&#20915;&#19978;&#36848;&#22256;&#38590;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33033;&#20914;&#21644;&#26222;&#36890;&#21367;&#31215;&#23618;&#30340;&#33021;&#25928;&#39640;&#12289;&#21487;&#38752;&#30340;&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#20998;&#20026;&#20004;&#20010;&#37096;&#20998;&#12290;&#31532;&#19968;&#37096;&#20998;&#26159;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;VGG16&#27169;&#22411;&#12290;&#25552;&#26696;&#32467;&#26500;&#30340;&#31532;&#20108;&#37096;&#20998;&#26159;&#33033;&#20914;&#21644;&#26222;&#36890;&#21367;&#31215;&#23618;&#30340;&#32452;&#21512;&#65292;&#29992;&#20110;&#26816;&#27979;&#22270;&#20687;&#30340;&#36793;&#30028;&#26694;&#12290;&#25105;&#20204;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#23545;&#26816;&#27979;&#21040;&#30340;&#29289;&#20307;&#36827;&#34892;&#20998;&#31867;&#12290;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;Python&#24211;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35757;&#32451;&#33033;&#20914;&#23618;&#12290;&#25152;&#25552;&#20986;&#30340;&#33033;&#20914;&#21367;&#31215;&#29289;&#20307;&#26816;&#27979;&#22120;&#27169;&#22411;&#22312;&#40657;&#33394;&#29289;&#20307;&#26816;&#27979;&#19978;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several deep learning algorithms have shown amazing performance for existing object detection tasks, but recognizing darker objects is the largest challenge. Moreover, those techniques struggled to detect or had a slow recognition rate, resulting in significant performance losses. As a result, an improved and accurate detection approach is required to address the above difficulty. The whole study proposes a combination of spiked and normal convolution layers as an energy-efficient and reliable object detector model. The proposed model is split into two sections. The first section is developed as a feature extractor, which utilizes pre-trained VGG16, and the second section of the proposal structure is the combination of spiked and normal Convolutional layers to detect the bounding boxes of images. We drew a pre-trained model for classifying detected objects. With state of the art Python libraries, spike layers can be trained efficiently. The proposed spike convolutional object detector 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24494;&#20998;&#20960;&#20309;&#30340;&#36716;&#31227;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#30340;&#28508;&#22312;&#21521;&#37327;&#26144;&#23556;&#21040;&#40654;&#26364;&#26354;&#38754;&#19978;&#30340;&#23616;&#37096;&#24179;&#22374;&#22352;&#26631;&#65292;&#23454;&#29616;&#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#21644;&#31283;&#23450;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2310.06369</link><description>&lt;p&gt;
&#20960;&#20309;&#23545;&#40784;&#36716;&#31227;&#32534;&#30721;&#22120;&#29992;&#20110;&#24402;&#32435;&#36716;&#31227;&#22238;&#24402;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Geometrically Aligned Transfer Encoder for Inductive Transfer in Regression Tasks. (arXiv:2310.06369v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24494;&#20998;&#20960;&#20309;&#30340;&#36716;&#31227;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#30340;&#28508;&#22312;&#21521;&#37327;&#26144;&#23556;&#21040;&#40654;&#26364;&#26354;&#38754;&#19978;&#30340;&#23616;&#37096;&#24179;&#22374;&#22352;&#26631;&#65292;&#23454;&#29616;&#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#21644;&#31283;&#23450;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36716;&#31227;&#23398;&#20064;&#26159;&#22788;&#29702;&#21487;&#33021;&#19982;&#20854;&#20182;&#20016;&#23500;&#25968;&#25454;&#30456;&#20851;&#30340;&#23569;&#37327;&#25968;&#25454;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#22823;&#37096;&#20998;&#26041;&#27861;&#37117;&#38598;&#20013;&#22312;&#20351;&#29992;&#22270;&#20687;&#21644;&#35821;&#35328;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#20219;&#21153;&#19978;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#25193;&#23637;&#36716;&#31227;&#23398;&#20064;&#26041;&#26696;&#21040;&#22238;&#24402;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24494;&#20998;&#20960;&#20309;&#30340;&#26032;&#22411;&#36716;&#31227;&#25216;&#26415;&#65292;&#21363;&#20960;&#20309;&#23545;&#40784;&#36716;&#31227;&#32534;&#30721;&#22120;&#65288;GATE&#65289;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#23558;&#27169;&#22411;&#30340;&#28508;&#22312;&#21521;&#37327;&#35299;&#37322;&#20026;&#23384;&#22312;&#20110;&#40654;&#26364;&#26354;&#38754;&#19978;&#30340;&#12290;&#25105;&#20204;&#25214;&#21040;&#20102;&#19968;&#31181;&#36866;&#24403;&#30340;&#24494;&#20998;&#21516;&#32986;&#65292;&#30830;&#20445;&#27599;&#20010;&#20219;&#24847;&#28857;&#26144;&#23556;&#21040;&#37325;&#21472;&#21306;&#22495;&#20013;&#30340;&#23616;&#37096;&#24179;&#22374;&#22352;&#26631;&#65292;&#20174;&#32780;&#23454;&#29616;&#20174;&#28304;&#25968;&#25454;&#21040;&#30446;&#26631;&#25968;&#25454;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;&#36825;&#20063;&#20316;&#20026;&#27169;&#22411;&#22312;&#22806;&#25512;&#21306;&#22495;&#34892;&#20026;&#33391;&#22909;&#30340;&#26377;&#25928;&#27491;&#21017;&#21270;&#22120;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;GATE&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#24182;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#34920;&#29616;&#20986;&#31283;&#23450;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning is a crucial technique for handling a small amount of data that is potentially related to other abundant data. However, most of the existing methods are focused on classification tasks using images and language datasets. Therefore, in order to expand the transfer learning scheme to regression tasks, we propose a novel transfer technique based on differential geometry, namely the Geometrically Aligned Transfer Encoder (GATE). In this method, we interpret the latent vectors from the model to exist on a Riemannian curved manifold. We find a proper diffeomorphism between pairs of tasks to ensure that every arbitrary point maps to a locally flat coordinate in the overlapping region, allowing the transfer of knowledge from the source to the target data. This also serves as an effective regularizer for the model to behave in extrapolation regions. In this article, we demonstrate that GATE outperforms conventional methods and exhibits stable behavior in both the latent space 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Noisy-ArcMix&#30340;&#35757;&#32451;&#25216;&#26415;&#65292;&#26088;&#22312;&#30830;&#20445;&#24322;&#24120;&#22768;&#38899;&#26816;&#27979;&#27169;&#22411;&#20855;&#26377;&#36275;&#22815;&#30340;&#31867;&#20869;&#32039;&#20945;&#24615;&#21644;&#27491;&#24120;&#19982;&#24322;&#24120;&#26679;&#26412;&#20043;&#38388;&#30340;&#35282;&#24230;&#24046;&#36317;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#33021;&#22815;&#25552;&#21462;&#37325;&#35201;&#26102;&#38388;&#21306;&#22495;&#29305;&#24449;&#30340;&#26550;&#26500;&#65292;&#22686;&#24378;&#27169;&#22411;&#23545;&#20110;&#26102;&#38388;&#24103;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.06364</link><description>&lt;p&gt;
Noisy-ArcMix: &#21152;&#22122;&#35282;&#24230;&#36793;&#38469;&#25439;&#22833;&#19982;Mixup&#32467;&#21512;&#30340;&#24322;&#24120;&#22768;&#38899;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Noisy-ArcMix: Additive Noisy Angular Margin Loss Combined With Mixup Anomalous Sound Detection. (arXiv:2310.06364v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Noisy-ArcMix&#30340;&#35757;&#32451;&#25216;&#26415;&#65292;&#26088;&#22312;&#30830;&#20445;&#24322;&#24120;&#22768;&#38899;&#26816;&#27979;&#27169;&#22411;&#20855;&#26377;&#36275;&#22815;&#30340;&#31867;&#20869;&#32039;&#20945;&#24615;&#21644;&#27491;&#24120;&#19982;&#24322;&#24120;&#26679;&#26412;&#20043;&#38388;&#30340;&#35282;&#24230;&#24046;&#36317;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#33021;&#22815;&#25552;&#21462;&#37325;&#35201;&#26102;&#38388;&#21306;&#22495;&#29305;&#24449;&#30340;&#26550;&#26500;&#65292;&#22686;&#24378;&#27169;&#22411;&#23545;&#20110;&#26102;&#38388;&#24103;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#24322;&#24120;&#22768;&#38899;&#26816;&#27979;&#26088;&#22312;&#36890;&#36807;&#23398;&#20064;&#27491;&#24120;&#25805;&#20316;&#22768;&#38899;&#30340;&#29305;&#24449;&#24182;&#24863;&#30693;&#20854;&#20559;&#24046;&#26469;&#35782;&#21035;&#24322;&#24120;&#22768;&#38899;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#38598;&#20013;&#22312;&#21033;&#29992;&#27491;&#24120;&#25968;&#25454;&#30340;&#20998;&#31867;&#36827;&#34892;&#33258;&#30417;&#30563;&#20219;&#21153;&#65292;&#24182;&#19988;&#20808;&#36827;&#30340;&#27169;&#22411;&#24050;&#32463;&#34920;&#26126;&#36890;&#36807;&#34920;&#31034;&#23398;&#20064;&#30830;&#20445;&#24322;&#24120;&#25968;&#25454;&#30340;&#34920;&#31034;&#31354;&#38388;&#23545;&#20110;&#33719;&#24471;&#32039;&#20945;&#30340;&#31867;&#20869;&#20998;&#24067;&#21644;&#26126;&#26174;&#30340;&#31867;&#38388;&#20998;&#24067;&#26159;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#20256;&#32479;&#26041;&#27861;&#24448;&#24448;&#26080;&#27861;&#30830;&#20445;&#36275;&#22815;&#30340;&#31867;&#20869;&#32039;&#20945;&#24615;&#65292;&#24182;&#19988;&#26679;&#26412;&#21450;&#20854;&#23545;&#24212;&#20013;&#24515;&#20043;&#38388;&#23384;&#22312;&#35282;&#24230;&#24046;&#24322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#25216;&#26415;&#65292;&#26088;&#22312;&#30830;&#20445;&#31867;&#20869;&#32039;&#20945;&#24615;&#65292;&#24182;&#22686;&#21152;&#27491;&#24120;&#21644;&#24322;&#24120;&#26679;&#26412;&#20043;&#38388;&#30340;&#35282;&#24230;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#25552;&#21462;&#37325;&#35201;&#26102;&#38388;&#21306;&#22495;&#29305;&#24449;&#30340;&#26550;&#26500;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21738;&#20123;&#26102;&#38388;&#24103;&#24212;&#35813;&#34987;&#24378;&#35843;&#25110;&#25233;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised anomalous sound detection (ASD) aims to identify anomalous sounds by learning the features of normal operational sounds and sensing their deviations. Recent approaches have focused on the self-supervised task utilizing the classification of normal data, and advanced models have shown that securing representation space for anomalous data is important through representation learning yielding compact intra-class and well-separated intra-class distributions. However, we show that conventional approaches often fail to ensure sufficient intra-class compactness and exhibit angular disparity between samples and their corresponding centers. In this paper, we propose a training technique aimed at ensuring intra-class compactness and increasing the angle gap between normal and abnormal samples. Furthermore, we present an architecture that extracts features for important temporal regions, enabling the model to learn which time frames should be emphasized or suppressed. Experimental re
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#25913;&#36827;&#30340;YOLOv5&#31639;&#27861;&#23454;&#29616;&#28779;&#28798;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#26816;&#27979;&#65292;&#20934;&#30830;&#29575;&#39640;&#65292;&#33021;&#26377;&#25928;&#22788;&#29702;&#23567;&#28779;&#28798;&#30446;&#26631;&#30340;&#26816;&#27979;&#21644;&#31867;&#20284;&#28779;&#28798;&#21644;&#28895;&#38654;&#30340;&#29289;&#20307;&#12290;</title><link>http://arxiv.org/abs/2310.06351</link><description>&lt;p&gt;
&#20351;&#29992;YOLOv5&#20174;&#22270;&#20687;&#21644;&#35270;&#39057;&#20013;&#26816;&#27979;&#28779;&#28798;
&lt;/p&gt;
&lt;p&gt;
Fire Detection From Image and Video Using YOLOv5. (arXiv:2310.06351v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06351
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#25913;&#36827;&#30340;YOLOv5&#31639;&#27861;&#23454;&#29616;&#28779;&#28798;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#26816;&#27979;&#65292;&#20934;&#30830;&#29575;&#39640;&#65292;&#33021;&#26377;&#25928;&#22788;&#29702;&#23567;&#28779;&#28798;&#30446;&#26631;&#30340;&#26816;&#27979;&#21644;&#31867;&#20284;&#28779;&#28798;&#21644;&#28895;&#38654;&#30340;&#29289;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#23460;&#20869;&#12289;&#23460;&#22806;&#21644;&#26862;&#26519;&#28779;&#28798;&#22270;&#20687;&#20013;&#26816;&#27979;&#28779;&#28798;&#30446;&#26631;&#20197;&#21450;&#22312;&#19981;&#21516;&#33258;&#28982;&#20809;&#19979;&#36827;&#34892;&#28779;&#28798;&#26816;&#27979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;YOLOv5&#28779;&#28798;&#26816;&#27979;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#12290;YOLOv5&#26816;&#27979;&#27169;&#22411;&#23558;&#29305;&#24449;&#25552;&#21462;&#32593;&#32476;&#20174;&#19977;&#32500;&#25193;&#23637;&#21040;&#20102;&#22235;&#32500;&#65292;&#22686;&#24378;&#20102;&#28779;&#28798;&#23567;&#30446;&#26631;&#35782;&#21035;&#30340;&#29305;&#24449;&#20256;&#25773;&#65292;&#25913;&#21892;&#20102;&#32593;&#32476;&#24615;&#33021;&#24182;&#20943;&#23569;&#20102;&#27169;&#22411;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#25552;&#21319;&#29305;&#24449;&#37329;&#23383;&#22612;&#65292;&#24471;&#21040;&#20102;&#34920;&#29616;&#26368;&#20339;&#30340;&#39044;&#27979;&#26694;&#12290;Fire-YOLOv5&#30456;&#27604;&#20854;&#20182;&#20248;&#31168;&#30340;&#30446;&#26631;&#26816;&#27979;&#32593;&#32476;&#22312;&#28779;&#28798;&#21644;&#28895;&#38654;&#23567;&#30446;&#26631;&#30340;&#26816;&#27979;&#20013;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#65292;mAP&#20026;90.5%&#65292;f1&#24471;&#20998;&#20026;88%&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;Fire-YOLOv5&#26816;&#27979;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#23567;&#28779;&#28798;&#30446;&#26631;&#30340;&#26816;&#27979;&#65292;&#20197;&#21450;&#31867;&#20284;&#28779;&#28798;&#21644;&#28895;&#38654;&#30340;&#29289;&#20307;&#65292;f1&#24471;&#20998;&#20026;0.88&#12290;&#24403;&#36755;&#20837;&#22270;&#20687;&#22823;&#23567;&#20026;416 x 416&#20998;&#36776;&#29575;&#26102;&#65292;&#24179;&#22343;&#26816;&#27979;&#26102;&#38388;&#20026;&#27599;&#24103;0.12&#31186;&#65292;&#36825;&#26159;&#38750;&#24120;&#24555;&#36895;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
For the detection of fire-like targets in indoor, outdoor and forest fire images, as well as fire detection under different natural lights, an improved YOLOv5 fire detection deep learning algorithm is proposed. The YOLOv5 detection model expands the feature extraction network from three dimensions, which enhances feature propagation of fire small targets identification, improves network performance, and reduces model parameters. Furthermore, through the promotion of the feature pyramid, the top-performing prediction box is obtained. Fire-YOLOv5 attains excellent results compared to state-of-the-art object detection networks, notably in the detection of small targets of fire and smoke with mAP 90.5% and f1 score 88%. Overall, the Fire-YOLOv5 detection model can effectively deal with the inspection of small fire targets, as well as fire-like and smoke-like objects with F1 score 0.88. When the input image size is 416 x 416 resolution, the average detection time is 0.12 s per frame, which 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#32447;&#24615;&#34920;&#31034;&#20887;&#20313;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#28388;&#27874;&#21098;&#26525;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#30456;&#20851;&#31995;&#25968;&#30697;&#38453;&#25439;&#22833;&#65288;CCM-loss&#65289;&#21644;&#21305;&#37197;&#36890;&#36947;&#36873;&#25321;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21152;&#24378;&#29305;&#24449;&#22270;&#20043;&#38388;&#30340;&#32447;&#24615;&#34920;&#31034;&#20851;&#31995;&#65292;&#22312;&#21098;&#26525;&#36807;&#31243;&#20013;&#26356;&#22909;&#22320;&#21435;&#38500;&#21516;&#36136;&#21270;&#30340;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2310.06344</link><description>&lt;p&gt;
&#22686;&#24378;&#32447;&#24615;&#34920;&#31034;&#20887;&#20313;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#28388;&#27874;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
Filter Pruning For CNN With Enhanced Linear Representation Redundancy. (arXiv:2310.06344v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#32447;&#24615;&#34920;&#31034;&#20887;&#20313;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#28388;&#27874;&#21098;&#26525;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#30456;&#20851;&#31995;&#25968;&#30697;&#38453;&#25439;&#22833;&#65288;CCM-loss&#65289;&#21644;&#21305;&#37197;&#36890;&#36947;&#36873;&#25321;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21152;&#24378;&#29305;&#24449;&#22270;&#20043;&#38388;&#30340;&#32447;&#24615;&#34920;&#31034;&#20851;&#31995;&#65292;&#22312;&#21098;&#26525;&#36807;&#31243;&#20013;&#26356;&#22909;&#22320;&#21435;&#38500;&#21516;&#36136;&#21270;&#30340;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#32593;&#32476;&#21098;&#26525;&#20248;&#20110;&#38750;&#32467;&#26500;&#21270;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#21033;&#29992;&#21457;&#23637;&#20013;&#30340;&#24182;&#34892;&#35745;&#31639;&#25216;&#26415;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32467;&#26500;&#21270;&#21098;&#26525;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#20026;&#20102;&#21019;&#24314;&#26356;&#22810;&#30340;&#32467;&#26500;&#21270;&#20887;&#20313;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#21516;&#19968;&#23618;&#30340;&#19981;&#21516;&#29305;&#24449;&#22270;&#30340;&#30456;&#20851;&#31995;&#25968;&#30697;&#38453;&#35745;&#31639;&#24471;&#21040;&#30340;&#25968;&#25454;&#39537;&#21160;&#25439;&#22833;&#20989;&#25968;&#39033;&#65292;&#31216;&#20026;&#30456;&#20851;&#31995;&#25968;&#30697;&#38453;&#25439;&#22833;&#65288;CCM-loss&#65289;&#12290;&#36825;&#20010;&#25439;&#22833;&#39033;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#40723;&#21169;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26356;&#24378;&#30340;&#29305;&#24449;&#22270;&#20043;&#38388;&#30340;&#32447;&#24615;&#34920;&#31034;&#20851;&#31995;&#65292;&#20174;&#32780;&#22312;&#21098;&#26525;&#36807;&#31243;&#20013;&#26356;&#23481;&#26131;&#21435;&#38500;&#21516;&#36136;&#21270;&#30340;&#37096;&#20998;&#12290;CCM-loss&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#21478;&#19968;&#31181;&#36890;&#29992;&#30340;&#36229;&#36234;&#25968;&#23398;&#24037;&#20855;&#65292;&#38500;&#20102;&#38598;&#20013;&#29983;&#25104;&#38646;&#30340;L*-norm&#27491;&#21017;&#21270;&#20043;&#22806;&#65292;&#36824;&#21487;&#20197;&#20026;&#19981;&#21516;&#31867;&#22411;&#30340;&#20887;&#20313;&#29983;&#25104;&#26356;&#22810;&#30340;&#20887;&#20313;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#20027;&#25104;&#20998;&#20998;&#26512;&#30340;&#21305;&#37197;&#36890;&#36947;&#36873;&#25321;&#31574;&#30053;&#65292;&#20197;&#20805;&#20998;&#21033;&#29992;CCM-loss&#30340;&#26368;&#22823;&#28508;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structured network pruning excels non-structured methods because they can take advantage of the thriving developed parallel computing techniques. In this paper, we propose a new structured pruning method. Firstly, to create more structured redundancy, we present a data-driven loss function term calculated from the correlation coefficient matrix of different feature maps in the same layer, named CCM-loss. This loss term can encourage the neural network to learn stronger linear representation relations between feature maps during the training from the scratch so that more homogenous parts can be removed later in pruning. CCM-loss provides us with another universal transcendental mathematical tool besides L*-norm regularization, which concentrates on generating zeros, to generate more redundancy but for the different genres. Furthermore, we design a matching channel selection strategy based on principal components analysis to exploit the maximum potential ability of CCM-loss. In our new s
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#23545;&#27604;&#25552;&#31034;&#23398;&#20064;&#30340;&#20195;&#30721;&#25628;&#32034;&#26041;&#27861;CPLCS&#65292;&#36890;&#36807;&#32534;&#31243;&#35821;&#35328;-&#33258;&#28982;&#35821;&#35328;&#23545;&#27604;&#23398;&#20064;&#21644;&#36328;&#27169;&#24577;&#20132;&#20114;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#20195;&#30721;&#25628;&#32034;&#26041;&#27861;&#20013;&#30340;&#35821;&#20041;&#34920;&#31034;&#19981;&#36275;&#21644;&#35821;&#20041;&#40511;&#27807;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.06342</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#27604;&#25552;&#31034;&#23398;&#20064;&#30340;&#20195;&#30721;&#25628;&#32034;&#26041;&#27861;&#65306;&#22522;&#20110;&#20132;&#20114;&#30697;&#38453;
&lt;/p&gt;
&lt;p&gt;
Contrastive Prompt Learning-based Code Search based on Interaction Matrix. (arXiv:2310.06342v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06342
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#23545;&#27604;&#25552;&#31034;&#23398;&#20064;&#30340;&#20195;&#30721;&#25628;&#32034;&#26041;&#27861;CPLCS&#65292;&#36890;&#36807;&#32534;&#31243;&#35821;&#35328;-&#33258;&#28982;&#35821;&#35328;&#23545;&#27604;&#23398;&#20064;&#21644;&#36328;&#27169;&#24577;&#20132;&#20114;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#20195;&#30721;&#25628;&#32034;&#26041;&#27861;&#20013;&#30340;&#35821;&#20041;&#34920;&#31034;&#19981;&#36275;&#21644;&#35821;&#20041;&#40511;&#27807;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#25628;&#32034;&#26088;&#22312;&#26816;&#32034;&#19982;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#39640;&#24230;&#21305;&#37197;&#30340;&#20195;&#30721;&#29255;&#27573;&#12290;&#26368;&#36817;&#65292;&#35768;&#22810;&#20195;&#30721;&#39044;&#35757;&#32451;&#26041;&#27861;&#22312;&#20195;&#30721;&#25628;&#32034;&#26041;&#38754;&#34920;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20195;&#30721;&#25628;&#32034;&#26041;&#27861;&#20173;&#28982;&#21463;&#21040;&#20004;&#20010;&#24615;&#33021;&#38480;&#21046;&#30340;&#22256;&#25200;&#65306;&#35821;&#20041;&#34920;&#31034;&#19981;&#36275;&#21644;&#33258;&#28982;&#35821;&#35328;&#19982;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#30340;&#35821;&#20041;&#40511;&#27807;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#23545;&#27604;&#25552;&#31034;&#23398;&#20064;&#30340;&#20195;&#30721;&#25628;&#32034;&#26041;&#27861;CPLCS&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#36328;&#27169;&#24577;&#20132;&#20114;&#26426;&#21046;&#12290;CPLCS&#21253;&#25324;&#65306;&#65288;1&#65289;&#32534;&#31243;&#35821;&#35328;-&#33258;&#28982;&#35821;&#35328;&#23545;&#27604;&#23398;&#20064;&#65292;&#23398;&#20064;&#32534;&#31243;&#35821;&#35328;&#21644;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#20043;&#38388;&#30340;&#35821;&#20041;&#21305;&#37197;&#20851;&#31995;&#65307;&#65288;2&#65289;&#21452;&#32534;&#30721;&#22120;&#32467;&#26500;&#30340;&#25552;&#31034;&#23398;&#20064;&#35774;&#35745;&#65292;&#21487;&#20197;&#32531;&#35299;&#35821;&#20041;&#34920;&#31034;&#19981;&#36275;&#30340;&#38382;&#39064;&#65307;&#65288;3&#65289;&#36328;&#27169;&#24577;&#20132;&#20114;&#26426;&#21046;&#65292;&#22686;&#24378;&#33258;&#28982;&#35821;&#35328;&#21644;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#30340;&#32454;&#31890;&#24230;&#26144;&#23556;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code search aims to retrieve the code snippet that highly matches the given query described in natural language. Recently, many code pre-training approaches have demonstrated impressive performance on code search. However, existing code search methods still suffer from two performance constraints: inadequate semantic representation and the semantic gap between natural language (NL) and programming language (PL). In this paper, we propose CPLCS, a contrastive prompt learning-based code search method based on the cross-modal interaction mechanism. CPLCS comprises:(1) PL-NL contrastive learning, which learns the semantic matching relationship between PL and NL representations; (2) a prompt learning design for a dual-encoder structure that can alleviate the problem of inadequate semantic representation; (3) a cross-modal interaction mechanism to enhance the fine-grained mapping between NL and PL. We conduct extensive experiments to evaluate the effectiveness of our approach on a real-world
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;I2SRM&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#26679;&#26412;&#20869;&#20851;&#31995;&#24314;&#27169;&#21644;&#26679;&#26412;&#38388;&#20851;&#31995;&#24314;&#27169;&#26469;&#35299;&#20915;&#22810;&#27169;&#24577;&#20449;&#24687;&#25552;&#21462;&#38382;&#39064;&#12290;&#36890;&#36807;&#36716;&#25442;&#23884;&#20837;&#34920;&#31034;&#12289;&#25429;&#25417;&#20132;&#20114;&#20316;&#29992;&#21644;&#24341;&#20837;AttnMixup&#31574;&#30053;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.06326</link><description>&lt;p&gt;
I2SRM&#65306;&#29992;&#20110;&#22810;&#27169;&#24577;&#20449;&#24687;&#25552;&#21462;&#30340;&#26679;&#26412;&#20869;&#22806;&#20851;&#31995;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
I2SRM: Intra- and Inter-Sample Relationship Modeling for Multimodal Information Extraction. (arXiv:2310.06326v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;I2SRM&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#26679;&#26412;&#20869;&#20851;&#31995;&#24314;&#27169;&#21644;&#26679;&#26412;&#38388;&#20851;&#31995;&#24314;&#27169;&#26469;&#35299;&#20915;&#22810;&#27169;&#24577;&#20449;&#24687;&#25552;&#21462;&#38382;&#39064;&#12290;&#36890;&#36807;&#36716;&#25442;&#23884;&#20837;&#34920;&#31034;&#12289;&#25429;&#25417;&#20132;&#20114;&#20316;&#29992;&#21644;&#24341;&#20837;AttnMixup&#31574;&#30053;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#20449;&#24687;&#25552;&#21462;&#22914;&#20170;&#21463;&#21040;&#30740;&#31350;&#37325;&#35270;&#65292;&#23427;&#38656;&#35201;&#32858;&#21512;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#34920;&#31034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#35813;&#20219;&#21153;&#30340;&#26679;&#26412;&#20869;&#22806;&#20851;&#31995;&#24314;&#27169;&#65288;I2SRM&#65289;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#21547;&#20004;&#20010;&#27169;&#22359;&#12290;&#39318;&#20808;&#65292;&#26679;&#26412;&#20869;&#20851;&#31995;&#24314;&#27169;&#27169;&#22359;&#20316;&#29992;&#20110;&#21333;&#20010;&#26679;&#26412;&#65292;&#26088;&#22312;&#23398;&#20064;&#26377;&#25928;&#30340;&#34920;&#31034;&#12290;&#25991;&#26412;&#21644;&#35270;&#35273;&#27169;&#24577;&#30340;&#23884;&#20837;&#34987;&#36716;&#25442;&#65292;&#20197;&#24357;&#21512;&#19981;&#21516;&#39044;&#35757;&#32451;&#35821;&#35328;&#21644;&#22270;&#20687;&#27169;&#22411;&#24341;&#36215;&#30340;&#27169;&#24577;&#24046;&#24322;&#12290;&#20854;&#27425;&#65292;&#26679;&#26412;&#38388;&#20851;&#31995;&#24314;&#27169;&#27169;&#22359;&#32771;&#34385;&#20102;&#22810;&#20010;&#26679;&#26412;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#19987;&#27880;&#20110;&#25429;&#25417;&#20132;&#20114;&#20316;&#29992;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;AttnMixup&#31574;&#30053;&#65292;&#19981;&#20165;&#33021;&#22815;&#20351;&#26679;&#26412;&#20043;&#38388;&#21327;&#20316;&#65292;&#36824;&#33021;&#22686;&#21152;&#25968;&#25454;&#20197;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;&#22810;&#27169;&#24577;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;Twitter-2015&#12289;Twitter-2017&#21644;&#22810;&#27169;&#24577;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;MNR&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal information extraction is attracting research attention nowadays, which requires aggregating representations from different modalities. In this paper, we present the Intra- and Inter-Sample Relationship Modeling (I2SRM) method for this task, which contains two modules. Firstly, the intra-sample relationship modeling module operates on a single sample and aims to learn effective representations. Embeddings from textual and visual modalities are shifted to bridge the modality gap caused by distinct pre-trained language and image models. Secondly, the inter-sample relationship modeling module considers relationships among multiple samples and focuses on capturing the interactions. An AttnMixup strategy is proposed, which not only enables collaboration among samples but also augments data to improve generalization. We conduct extensive experiments on the multimodal named entity recognition datasets Twitter-2015 and Twitter-2017, and the multimodal relation extraction dataset MNR
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#20923;&#32467;&#27493;&#24577;&#20107;&#20214;&#65292;&#26368;&#20339;&#27169;&#22411;&#22312;&#27979;&#35797;&#25968;&#25454;&#19978;&#33719;&#24471;&#20102;0.427&#30340;&#24471;&#20998;&#65292;&#22312;Kaggle&#20923;&#32467;&#27493;&#24577;&#39044;&#27979;&#31454;&#36187;&#20013;&#25490;&#21517;&#21069;&#20116;&#12290;</title><link>http://arxiv.org/abs/2310.06322</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#19977;&#31181;&#20923;&#32467;&#27493;&#24577;&#20107;&#20214;
&lt;/p&gt;
&lt;p&gt;
Predicting Three Types of Freezing of Gait Events Using Deep Learning Models. (arXiv:2310.06322v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06322
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#20923;&#32467;&#27493;&#24577;&#20107;&#20214;&#65292;&#26368;&#20339;&#27169;&#22411;&#22312;&#27979;&#35797;&#25968;&#25454;&#19978;&#33719;&#24471;&#20102;0.427&#30340;&#24471;&#20998;&#65292;&#22312;Kaggle&#20923;&#32467;&#27493;&#24577;&#39044;&#27979;&#31454;&#36187;&#20013;&#25490;&#21517;&#21069;&#20116;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20923;&#32467;&#27493;&#24577;&#26159;&#24085;&#37329;&#26862;&#30149;&#30340;&#30151;&#29366;&#20043;&#19968;&#65292;&#24739;&#32773;&#22312;&#34892;&#36208;&#26102;&#20250;&#21608;&#26399;&#24615;&#22320;&#20986;&#29616;&#19981;&#33021;&#36808;&#27493;&#25110;&#36716;&#36523;&#30340;&#24773;&#20917;&#12290;&#34429;&#28982;&#21307;&#23398;&#19987;&#23478;&#24050;&#32463;&#21457;&#29616;&#20102;&#22810;&#31181;&#24341;&#21457;&#21644;&#32531;&#35299;&#20923;&#32467;&#27493;&#24577;&#30340;&#26041;&#27861;&#65292;&#20294;&#20854;&#28508;&#22312;&#21407;&#22240;&#21644;&#39044;&#27979;&#27169;&#22411;&#20173;&#22312;&#30740;&#31350;&#20013;&#12290;&#30446;&#21069;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#20923;&#32467;&#27493;&#24577;&#39044;&#27979;&#27169;&#22411;&#21487;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#23454;&#29616;&#39640;&#25935;&#24863;&#24615;&#21644;&#29305;&#24322;&#24615;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#32570;&#20047;&#23545;&#20923;&#32467;&#27493;&#24577;&#20107;&#20214;&#31867;&#22411;&#30340;&#20855;&#20307;&#35828;&#26126;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#21508;&#31181;&#20351;&#29992;&#36716;&#25442;&#22120;&#32534;&#30721;&#22120;&#26550;&#26500;&#21152;&#19978;&#21452;&#21521;LSTM&#23618;&#21644;&#19981;&#21516;&#29305;&#24449;&#38598;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#20923;&#32467;&#27493;&#24577;&#20107;&#20214;&#12290;&#26368;&#20339;&#27169;&#22411;&#22312;&#27979;&#35797;&#25968;&#25454;&#19978;&#33719;&#24471;&#20102;0.427&#30340;&#24471;&#20998;&#65292;&#22312;&#30001;&#36808;&#20811;&#23572;&#183;J&#183;&#31119;&#20811;&#26031;&#22522;&#37329;&#20250;&#20027;&#21150;&#30340;Kaggle&#20923;&#32467;&#27493;&#24577;&#39044;&#27979;&#31454;&#36187;&#20013;&#25490;&#21517;&#21069;&#20116;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20063;&#24847;&#35782;&#21040;&#20102;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Freezing of gait is a Parkinson's Disease symptom that episodically inflicts a patient with the inability to step or turn while walking. While medical experts have discovered various triggers and alleviating actions for freezing of gait, the underlying causes and prediction models are still being explored today. Current freezing of gait prediction models that utilize machine learning achieve high sensitivity and specificity in freezing of gait predictions based on time-series data; however, these models lack specifications on the type of freezing of gait events. We develop various deep learning models using the transformer encoder architecture plus Bidirectional LSTM layers and different feature sets to predict the three different types of freezing of gait events. The best performing model achieves a score of 0.427 on testing data, which would rank top 5 in Kaggle's Freezing of Gait prediction competition, hosted by THE MICHAEL J. FOX FOUNDATION. However, we also recognize overfitting 
&lt;/p&gt;</description></item><item><title>Dobby&#26159;&#19968;&#27454;&#30001;GPT-4&#39537;&#21160;&#30340;&#20250;&#35805;&#26381;&#21153;&#26426;&#22120;&#20154;&#65292;&#33021;&#22815;&#23558;&#20250;&#35805;&#22411;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#23884;&#20837;&#21040;&#20855;&#26377;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#26234;&#33021;&#20915;&#31574;&#21151;&#33021;&#30340;&#20855;&#36523;&#31995;&#32479;&#20013;&#65292;&#23454;&#29616;&#26381;&#21153;&#20219;&#21153;&#30340;&#19968;&#20307;&#21270;&#65292;&#24182;&#19988;&#33021;&#22815;&#19982;&#29289;&#29702;&#19990;&#30028;&#36827;&#34892;&#20132;&#20114;&#12290;&#22312;&#28436;&#31034;&#20013;&#23637;&#31034;&#20102;&#20854;&#22312;&#23548;&#28216;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#36890;&#36807;HRI&#30740;&#31350;&#23545;&#20854;&#24615;&#33021;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2310.06303</link><description>&lt;p&gt;
Dobby&#65306;&#19968;&#27454;&#30001;GPT-4&#39537;&#21160;&#30340;&#20250;&#35805;&#26381;&#21153;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Dobby: A Conversational Service Robot Driven by GPT-4. (arXiv:2310.06303v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06303
&lt;/p&gt;
&lt;p&gt;
Dobby&#26159;&#19968;&#27454;&#30001;GPT-4&#39537;&#21160;&#30340;&#20250;&#35805;&#26381;&#21153;&#26426;&#22120;&#20154;&#65292;&#33021;&#22815;&#23558;&#20250;&#35805;&#22411;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#23884;&#20837;&#21040;&#20855;&#26377;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#26234;&#33021;&#20915;&#31574;&#21151;&#33021;&#30340;&#20855;&#36523;&#31995;&#32479;&#20013;&#65292;&#23454;&#29616;&#26381;&#21153;&#20219;&#21153;&#30340;&#19968;&#20307;&#21270;&#65292;&#24182;&#19988;&#33021;&#22815;&#19982;&#29289;&#29702;&#19990;&#30028;&#36827;&#34892;&#20132;&#20114;&#12290;&#22312;&#28436;&#31034;&#20013;&#23637;&#31034;&#20102;&#20854;&#22312;&#23548;&#28216;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#36890;&#36807;HRI&#30740;&#31350;&#23545;&#20854;&#24615;&#33021;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26426;&#22120;&#20154;&#24179;&#21488;&#65292;&#23558;&#20250;&#35805;&#22411;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#23884;&#20837;&#21040;&#20855;&#26377;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#26234;&#33021;&#20915;&#31574;&#21151;&#33021;&#30340;&#20855;&#36523;&#31995;&#32479;&#20013;&#65292;&#20197;&#23454;&#29616;&#26381;&#21153;&#20219;&#21153;&#30340;&#20219;&#21153;&#35268;&#21010;&#21644;&#31867;&#20154;&#23545;&#35805;&#30340;&#19968;&#20307;&#21270;&#12290;&#35813;&#20195;&#29702;&#22522;&#20110;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#24191;&#27867;&#30340;&#36890;&#29992;&#30693;&#35782;&#32780;&#24471;&#21040;&#12290;&#38500;&#20102;&#29983;&#25104;&#23545;&#35805;&#65292;&#35813;&#20195;&#29702;&#36824;&#21487;&#20197;&#36890;&#36807;&#22312;&#26426;&#22120;&#20154;&#19978;&#35843;&#29992;&#21629;&#20196;&#19982;&#29289;&#29702;&#19990;&#30028;&#36827;&#34892;&#20132;&#20114;&#65292;&#23454;&#29616;&#36890;&#20449;&#19982;&#34892;&#20026;&#30340;&#26080;&#32541;&#34701;&#21512;&#12290;&#35813;&#31995;&#32479;&#22312;&#19968;&#20010;&#33258;&#30001;&#24418;&#24335;&#30340;&#23548;&#28216;&#22330;&#26223;&#20013;&#36827;&#34892;&#20102;&#28436;&#31034;&#65292;&#32467;&#21512;&#26426;&#22120;&#20154;&#20197;&#21450;&#19981;&#20855;&#22791;&#20250;&#35805;&#22411;&#20154;&#24037;&#26234;&#33021;&#33021;&#21147;&#30340;&#26426;&#22120;&#20154;&#36827;&#34892;&#20102;HRI&#30740;&#31350;&#12290;&#36890;&#36807;&#20116;&#20010;&#32500;&#24230;&#26469;&#34913;&#37327;&#24615;&#33021;&#65306;&#25972;&#20307;&#26377;&#25928;&#24615;&#65292;&#25506;&#32034;&#33021;&#21147;&#65292;&#23457;&#26597;&#33021;&#21147;&#65292;&#23545;&#20154;&#26684;&#21270;&#30340;&#25509;&#21463;&#24230;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces a robotics platform which embeds a conversational AI agent in an embodied system for natural language understanding and intelligent decision-making for service tasks; integrating task planning and human-like conversation. The agent is derived from a large language model, which has learned from a vast corpus of general knowledge. In addition to generating dialogue, this agent can interface with the physical world by invoking commands on the robot; seamlessly merging communication and behavior. This system is demonstrated in a free-form tour-guide scenario, in an HRI study combining robots with and without conversational AI capabilities. Performance is measured along five dimensions: overall effectiveness, exploration abilities, scrutinization abilities, receptiveness to personification, and adaptability.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22855;&#24322;&#23398;&#20064;&#29702;&#35770;&#30740;&#31350;&#36229;&#21472;&#21152;&#30340;&#29609;&#20855;&#27169;&#22411;&#20013;&#30340;&#30456;&#21464;&#65292;&#22312;&#20004;&#20010;&#38544;&#34255;&#32500;&#24230;&#30340;&#24773;&#20917;&#19979;&#21457;&#29616;&#27491;&#21017;&#30340;$k$-gons&#26159;&#20020;&#30028;&#28857;&#65292;&#24182;&#25552;&#20379;&#25903;&#25345;&#29702;&#35770;&#34920;&#26126;&#36825;&#20123;&#20020;&#30028;&#28857;&#20915;&#23450;&#20102;&#36125;&#21494;&#26031;&#21518;&#39564;&#30340;&#30456;&#21464;&#12290;&#27492;&#22806;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#20020;&#30028;&#28857;&#20063;&#20915;&#23450;&#20102;SGD&#35757;&#32451;&#30340;&#34892;&#20026;&#12290;&#30740;&#31350;&#32467;&#26524;&#25903;&#25345;&#20102;SGD&#23398;&#20064;&#36712;&#36857;&#21463;&#39034;&#24207;&#23398;&#20064;&#26426;&#21046;&#24433;&#21709;&#30340;&#29468;&#24819;&#12290;</title><link>http://arxiv.org/abs/2310.06301</link><description>&lt;p&gt;
&#36229;&#21472;&#21152;&#30340;&#29609;&#20855;&#27169;&#22411;&#20013;&#30340;&#21160;&#21147;&#23398;&#19982;&#36125;&#21494;&#26031;&#30456;&#21464;
&lt;/p&gt;
&lt;p&gt;
Dynamical versus Bayesian Phase Transitions in a Toy Model of Superposition. (arXiv:2310.06301v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22855;&#24322;&#23398;&#20064;&#29702;&#35770;&#30740;&#31350;&#36229;&#21472;&#21152;&#30340;&#29609;&#20855;&#27169;&#22411;&#20013;&#30340;&#30456;&#21464;&#65292;&#22312;&#20004;&#20010;&#38544;&#34255;&#32500;&#24230;&#30340;&#24773;&#20917;&#19979;&#21457;&#29616;&#27491;&#21017;&#30340;$k$-gons&#26159;&#20020;&#30028;&#28857;&#65292;&#24182;&#25552;&#20379;&#25903;&#25345;&#29702;&#35770;&#34920;&#26126;&#36825;&#20123;&#20020;&#30028;&#28857;&#20915;&#23450;&#20102;&#36125;&#21494;&#26031;&#21518;&#39564;&#30340;&#30456;&#21464;&#12290;&#27492;&#22806;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#20020;&#30028;&#28857;&#20063;&#20915;&#23450;&#20102;SGD&#35757;&#32451;&#30340;&#34892;&#20026;&#12290;&#30740;&#31350;&#32467;&#26524;&#25903;&#25345;&#20102;SGD&#23398;&#20064;&#36712;&#36857;&#21463;&#39034;&#24207;&#23398;&#20064;&#26426;&#21046;&#24433;&#21709;&#30340;&#29468;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#22855;&#24322;&#23398;&#20064;&#29702;&#35770;&#65288;SLT&#65289;&#30740;&#31350;&#36229;&#21472;&#21152;&#30340;&#29609;&#20855;&#27169;&#22411;&#65288;TMS&#65289;&#20013;&#30340;&#30456;&#21464;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#29702;&#35770;&#25439;&#22833;&#30340;&#38381;&#24335;&#20844;&#24335;&#65292;&#24182;&#22312;&#20004;&#20010;&#38544;&#34255;&#32500;&#24230;&#30340;&#24773;&#20917;&#19979;&#21457;&#29616;&#65292;&#27491;&#21017;&#30340;$k$-gons&#26159;&#20020;&#30028;&#28857;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25903;&#25345;&#29702;&#35770;&#65292;&#34920;&#26126;&#36825;&#20123;$k$-gons&#30340;&#26412;&#22320;&#23398;&#20064;&#31995;&#25968;&#65288;&#20960;&#20309;&#19981;&#21464;&#37327;&#65289;&#20915;&#23450;&#20102;&#36125;&#21494;&#26031;&#21518;&#39564;&#20316;&#20026;&#35757;&#32451;&#26679;&#26412;&#22823;&#23567;&#30340;&#20989;&#25968;&#30340;&#30456;&#21464;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20973;&#32463;&#39564;&#35777;&#26126;&#65292;&#21516;&#26679;&#30340;$k$-gon&#20020;&#30028;&#28857;&#20063;&#20915;&#23450;&#20102;SGD&#35757;&#32451;&#30340;&#34892;&#20026;&#12290;&#24471;&#20986;&#30340;&#32467;&#35770;&#25903;&#25345;&#20102;SGD&#23398;&#20064;&#36712;&#36857;&#21463;&#39034;&#24207;&#23398;&#20064;&#26426;&#21046;&#24433;&#21709;&#30340;&#29468;&#24819;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;TMS&#20013;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#26080;&#35770;&#26159;&#36890;&#36807;SGD&#36824;&#26159;&#36125;&#21494;&#26031;&#23398;&#20064;&#65292;&#21487;&#20197;&#34987;&#25551;&#36848;&#20026;&#22312;&#21442;&#25968;&#31354;&#38388;&#20013;&#20174;&#39640;&#25439;&#22833;&#21644;&#20302;&#22797;&#26434;&#24615;&#30340;&#21306;&#22495;&#21521;&#20302;&#25439;&#22833;&#21644;&#39640;&#22797;&#26434;&#24615;&#30340;&#21306;&#22495;&#30340;&#26053;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate phase transitions in a Toy Model of Superposition (TMS) using Singular Learning Theory (SLT). We derive a closed formula for the theoretical loss and, in the case of two hidden dimensions, discover that regular $k$-gons are critical points. We present supporting theory indicating that the local learning coefficient (a geometric invariant) of these $k$-gons determines phase transitions in the Bayesian posterior as a function of training sample size. We then show empirically that the same $k$-gon critical points also determine the behavior of SGD training. The picture that emerges adds evidence to the conjecture that the SGD learning trajectory is subject to a sequential learning mechanism. Specifically, we find that the learning process in TMS, be it through SGD or Bayesian learning, can be characterized by a journey through parameter space from regions of high loss and low complexity to regions of low loss and high complexity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Q&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#34394;&#25311;&#23545;&#25239;&#24615;&#29609;&#23478;&#65292;&#26377;&#25928;&#35843;&#33410;&#20102;&#26631;&#20934;Q&#23398;&#20064;&#20013;&#30340;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#65292;&#25552;&#20986;&#30340;&#31639;&#27861;&#31616;&#21333;&#32780;&#26377;&#25928;&#65292;&#33021;&#22815;&#36731;&#26494;&#24212;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.06286</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#34892;&#20026;&#25233;&#21046;Q&#23398;&#20064;&#20013;&#30340;&#36807;&#39640;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Suppressing Overestimation in Q-Learning through Adversarial Behaviors. (arXiv:2310.06286v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Q&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#34394;&#25311;&#23545;&#25239;&#24615;&#29609;&#23478;&#65292;&#26377;&#25928;&#35843;&#33410;&#20102;&#26631;&#20934;Q&#23398;&#20064;&#20013;&#30340;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#65292;&#25552;&#20986;&#30340;&#31639;&#27861;&#31616;&#21333;&#32780;&#26377;&#25928;&#65292;&#33021;&#22815;&#36731;&#26494;&#24212;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;Q&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;&#19968;&#20010;&#34394;&#25311;&#23545;&#25239;&#24615;&#29609;&#23478;&#65292;&#31216;&#20026;&#34394;&#25311;&#23545;&#25239;&#24615;Q&#23398;&#20064;&#65288;DAQ&#65289;&#65292;&#20197;&#26377;&#25928;&#22320;&#35843;&#33410;&#26631;&#20934;Q&#23398;&#20064;&#20013;&#30340;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#12290;&#36890;&#36807;&#34394;&#25311;&#29609;&#23478;&#65292;&#23398;&#20064;&#21487;&#20197;&#34987;&#34920;&#36848;&#20026;&#19968;&#20010;&#21452;&#20154;&#38646;&#21644;&#21338;&#24328;&#12290;&#25152;&#25552;&#20986;&#30340;DAQ&#23558;&#20960;&#31181;Q&#23398;&#20064;&#30340;&#21464;&#20307;&#32479;&#19968;&#21040;&#19968;&#20010;&#21333;&#19968;&#30340;&#26694;&#26550;&#20013;&#65292;&#20197;&#25511;&#21046;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#65292;&#20363;&#22914;maxmin Q&#23398;&#20064;&#21644;minmax Q&#23398;&#20064;&#65288;&#26412;&#25991;&#25552;&#20986;&#65289;&#12290;&#36890;&#36807;&#34394;&#25311;&#23545;&#25239;&#24615;&#34892;&#20026;&#65292;&#25152;&#25552;&#20986;&#30340;DAQ&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#29616;&#25104;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#36890;&#36807;&#35843;&#25972;&#23545;&#25239;&#24615;Q&#23398;&#20064;&#65292;&#20174;&#32508;&#21512;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;DAQ&#30340;&#26377;&#38480;&#26102;&#38388;&#25910;&#25947;&#24615;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#29615;&#22659;&#19979;&#65292;&#23454;&#35777;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;DAQ&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of this paper is to propose a new Q-learning algorithm with a dummy adversarial player, which is called dummy adversarial Q-learning (DAQ), that can effectively regulate the overestimation bias in standard Q-learning. With the dummy player, the learning can be formulated as a two-player zero-sum game. The proposed DAQ unifies several Q-learning variations to control overestimation biases, such as maxmin Q-learning and minmax Q-learning (proposed in this paper) in a single framework. The proposed DAQ is a simple but effective way to suppress the overestimation bias thourgh dummy adversarial behaviors and can be easily applied to off-the-shelf reinforcement learning algorithms to improve the performances. A finite-time convergence of DAQ is analyzed from an integrated perspective by adapting an adversarial Q-learning. The performance of the suggested DAQ is empirically demonstrated under various benchmark environments.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#21306;&#22359;&#38142;&#25216;&#26415;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23433;&#20840;&#24615;&#30340;&#24895;&#26223;&#65292;&#20197;&#35299;&#20915;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#30495;&#23454;&#24615;&#21644;&#38544;&#31169;&#27844;&#38706;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.06278</link><description>&lt;p&gt;
BC4LLM&#65306;&#24403;&#21306;&#22359;&#38142;&#36935;&#35265;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#30340;&#21487;&#20449;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
BC4LLM: Trusted Artificial Intelligence When Blockchain Meets Large Language Models. (arXiv:2310.06278v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06278
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#21306;&#22359;&#38142;&#25216;&#26415;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23433;&#20840;&#24615;&#30340;&#24895;&#26223;&#65292;&#20197;&#35299;&#20915;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#30495;&#23454;&#24615;&#21644;&#38544;&#31169;&#27844;&#38706;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27491;&#22312;&#37325;&#26032;&#22609;&#36896;&#31038;&#20250;&#30340;&#29983;&#20135;&#26041;&#24335;&#21644;&#29983;&#20135;&#21147;&#65292;&#24182;&#25913;&#21464;&#31185;&#23398;&#30740;&#31350;&#30340;&#33539; paradigm&#12290;&#20854;&#20013;&#65292;&#20197;ChatGPT&#20026;&#20195;&#34920;&#30340;AI&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#12290;&#36825;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20197;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#65288;AIGC&#65289;&#30340;&#24418;&#24335;&#26381;&#21153;&#20110;&#20154;&#20204;&#65292;&#24182;&#24191;&#27867;&#24212;&#29992;&#20110;&#21672;&#35810;&#12289;&#21307;&#30103;&#21644;&#25945;&#32946;&#12290;&#28982;&#32780;&#65292;&#24456;&#38590;&#20445;&#35777;AIGC&#23398;&#20064;&#25968;&#25454;&#30340;&#30495;&#23454;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#27492;&#22806;&#65292;&#20998;&#24067;&#24335;AI&#35757;&#32451;&#20013;&#20063;&#23384;&#22312;&#38544;&#24739;&#30340;&#38544;&#31169;&#27844;&#38706;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;LLMs&#29983;&#25104;&#30340;&#20869;&#23481;&#24456;&#38590;&#35782;&#21035;&#21644;&#36861;&#36394;&#65292;&#38590;&#20197;&#36328;&#24179;&#21488;&#30456;&#20114;&#35748;&#21487;&#12290;&#22312;&#20197;LLMs&#20026;&#21160;&#21147;&#30340;AI&#26102;&#20195;&#21363;&#23558;&#21040;&#26469;&#20043;&#38469;&#65292;&#19978;&#36848;&#20449;&#24687;&#23433;&#20840;&#38382;&#39064;&#23558;&#34987;&#26080;&#38480;&#25918;&#22823;&#65292;&#24433;&#21709;&#27599;&#20010;&#20154;&#30340;&#29983;&#27963;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#32771;&#34385;&#21033;&#29992;&#21306;&#22359;&#38142;&#25216;&#26415;&#20026;LLMs&#36171;&#20104;&#21331;&#36234;&#30340;&#23433;&#20840;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#24895;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, artificial intelligence (AI) and machine learning (ML) are reshaping society's production methods and productivity, and also changing the paradigm of scientific research. Among them, the AI language model represented by ChatGPT has made great progress. Such large language models (LLMs) serve people in the form of AI-generated content (AIGC) and are widely used in consulting, healthcare, and education. However, it is difficult to guarantee the authenticity and reliability of AIGC learning data. In addition, there are also hidden dangers of privacy disclosure in distributed AI training. Moreover, the content generated by LLMs is difficult to identify and trace, and it is difficult to cross-platform mutual recognition. The above information security issues in the coming era of AI powered by LLMs will be infinitely amplified and affect everyone's life. Therefore, we consider empowering LLMs using blockchain technology with superior security features to propose a vision for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;CIPHER&#30340;&#36890;&#20449;&#26426;&#21046;&#65292;&#36890;&#36807;&#21435;&#38500;LLMs&#20013;&#30340;&#26631;&#35760;&#37319;&#26679;&#27493;&#39588;&#65292;&#35753;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#26399;&#26395;&#30340;&#21407;&#22987;Transformer&#36755;&#20986;&#23884;&#20837;&#26469;&#20256;&#36798;&#20854;&#20449;&#24565;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#20449;&#24687;&#20002;&#22833;&#39118;&#38505;&#65292;&#24182;&#25552;&#20379;&#20102;&#32534;&#30721;&#26356;&#24191;&#27867;&#20449;&#24687;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.06272</link><description>&lt;p&gt;
&#35753;&#27169;&#22411;&#35828;&#23494;&#25991;: &#36890;&#36807;&#23884;&#20837;&#36827;&#34892;&#22810;&#26234;&#33021;&#20307;&#36777;&#35770;
&lt;/p&gt;
&lt;p&gt;
Let Models Speak Ciphers: Multiagent Debate through Embeddings. (arXiv:2310.06272v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;CIPHER&#30340;&#36890;&#20449;&#26426;&#21046;&#65292;&#36890;&#36807;&#21435;&#38500;LLMs&#20013;&#30340;&#26631;&#35760;&#37319;&#26679;&#27493;&#39588;&#65292;&#35753;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#26399;&#26395;&#30340;&#21407;&#22987;Transformer&#36755;&#20986;&#23884;&#20837;&#26469;&#20256;&#36798;&#20854;&#20449;&#24565;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#20449;&#24687;&#20002;&#22833;&#39118;&#38505;&#65292;&#24182;&#25552;&#20379;&#20102;&#32534;&#30721;&#26356;&#24191;&#27867;&#20449;&#24687;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20043;&#38388;&#30340;&#35752;&#35770;&#21644;&#36777;&#35770;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#26377;&#28508;&#21147;&#22686;&#24378;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#23613;&#31649;&#33258;&#28982;&#35821;&#35328;&#30001;&#20110;LLMs&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#32780;&#25104;&#20026;&#26126;&#26174;&#30340;&#20132;&#27969;&#36873;&#25321;&#65292;&#20294;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#26102;&#38656;&#35201;&#36827;&#34892;&#30340;&#26631;&#35760;&#37319;&#26679;&#27493;&#39588;&#21487;&#33021;&#23384;&#22312;&#20449;&#24687;&#20002;&#22833;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#22240;&#20026;&#23427;&#20165;&#20351;&#29992;&#19968;&#20010;&#26631;&#35760;&#26469;&#20195;&#34920;&#27169;&#22411;&#22312;&#25972;&#20010;&#35789;&#27719;&#34920;&#20013;&#30340;&#20449;&#24565;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CIPHER&#65288;&#36890;&#36807;&#23884;&#20837;&#34920;&#31034;&#36827;&#34892;&#20132;&#27969;&#30340;&#32593;&#32476;&#27169;&#22411;&#21327;&#35758;&#65289;&#30340;&#36890;&#20449;&#26426;&#21046;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20174;LLMs&#20013;&#21435;&#38500;&#20102;&#26631;&#35760;&#37319;&#26679;&#27493;&#39588;&#65292;&#35753;&#23427;&#20204;&#36890;&#36807;&#21407;&#22987;Transformer&#36755;&#20986;&#23884;&#20837;&#30340;&#26399;&#26395;&#26469;&#20256;&#36798;&#23427;&#20204;&#30340;&#20449;&#24565;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36890;&#36807;&#20559;&#31163;&#33258;&#28982;&#35821;&#35328;&#65292;CIPHER&#22312;&#19981;&#23545;&#27169;&#22411;&#26435;&#37325;&#36827;&#34892;&#20219;&#20309;&#20462;&#25913;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#20102;&#32534;&#30721;&#26356;&#24191;&#27867;&#20449;&#24687;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discussion and debate among Large Language Models (LLMs) have gained considerable attention due to their potential to enhance the reasoning ability of LLMs. Although natural language is an obvious choice for communication due to LLM's language understanding capability, the token sampling step needed when generating natural language poses a potential risk of information loss, as it uses only one token to represent the model's belief across the entire vocabulary. In this paper, we introduce a communication regime named CIPHER (Communicative Inter-Model Protocol Through Embedding Representation) to address this issue. Specifically, we remove the token sampling step from LLMs and let them communicate their beliefs across the vocabulary through the expectation of the raw transformer output embeddings. Remarkably, by deviating from natural language, CIPHER offers an advantage of encoding a broader spectrum of information without any modification to the model weights. While the state-of-the-a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#21307;&#23398;&#29983;&#25104;&#24615;&#38382;&#31572;&#31995;&#32479;&#20013;&#30340;&#24187;&#35273;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#30340;&#33258;&#25105;&#21453;&#24605;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#20107;&#23454;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#34164;&#28085;&#24615;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.06271</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#25105;&#21453;&#24605;&#20197;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Towards Mitigating Hallucination in Large Language Models via Self-Reflection. (arXiv:2310.06271v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06271
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#21307;&#23398;&#29983;&#25104;&#24615;&#38382;&#31572;&#31995;&#32479;&#20013;&#30340;&#24187;&#35273;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#30340;&#33258;&#25105;&#21453;&#24605;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#20107;&#23454;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#34164;&#28085;&#24615;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#25104;&#21644;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#65288;&#21253;&#25324;&#38382;&#31572;&#20219;&#21153;&#65289;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#37096;&#32626;&#20173;&#38754;&#20020;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#8220;&#24187;&#35273;&#8221;&#38382;&#39064;&#65306;&#27169;&#22411;&#29983;&#25104;&#20284;&#20046;&#21512;&#29702;&#20294;&#19981;&#30495;&#23454;&#25110;&#33618;&#35884;&#30340;&#20449;&#24687;&#12290;&#22312;&#21307;&#23398;&#39046;&#22495;&#65292;&#36825;&#20010;&#38382;&#39064;&#23588;&#20026;&#20851;&#38190;&#65292;&#22240;&#20026;&#28041;&#21450;&#21040;&#19981;&#24120;&#35265;&#30340;&#19987;&#19994;&#27010;&#24565;&#21644;&#28508;&#22312;&#30340;&#31038;&#20250;&#39118;&#38505;&#12290;&#26412;&#25991;&#20351;&#29992;&#24191;&#27867;&#37319;&#29992;&#30340;LLMs&#21644;&#25968;&#25454;&#38598;&#65292;&#20998;&#26512;&#20102;&#21307;&#23398;&#29983;&#25104;&#24615;&#38382;&#31572;&#31995;&#32479;&#20013;&#30340;&#24187;&#35273;&#29616;&#35937;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;&#35782;&#21035;&#21644;&#29702;&#35299;&#24120;&#35265;&#30340;&#38382;&#39064;&#31572;&#26696;&#65292;&#29305;&#21035;&#26159;&#24187;&#35273;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#30340;&#33258;&#25105;&#21453;&#24605;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#30693;&#35782;&#33719;&#21462;&#21644;&#31572;&#26696;&#29983;&#25104;&#12290;&#36890;&#36807;&#36825;&#20010;&#21453;&#39304;&#36807;&#31243;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36880;&#27493;&#22686;&#24378;&#20102;&#29983;&#25104;&#31572;&#26696;&#30340;&#20107;&#23454;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#34164;&#28085;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown promise for generative and knowledge-intensive tasks including question-answering (QA) tasks. However, the practical deployment still faces challenges, notably the issue of "hallucination", where models generate plausible-sounding but unfaithful or nonsensical information. This issue becomes particularly critical in the medical domain due to the uncommon professional concepts and potential social risks involved. This paper analyses the phenomenon of hallucination in medical generative QA systems using widely adopted LLMs and datasets. Our investigation centers on the identification and comprehension of common problematic answers, with a specific emphasis on hallucination. To tackle this challenge, we present an interactive self-reflection methodology that incorporates knowledge acquisition and answer generation. Through this feedback process, our approach steadily enhances the factuality, consistency, and entailment of the generated answers. Cons
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;AI&#20107;&#25925;&#25968;&#25454;&#24211;&#20316;&#20026;&#25552;&#39640;&#20154;&#20204;&#23545;&#20154;&#24037;&#26234;&#33021;&#20260;&#23475;&#24847;&#35782;&#30340;&#25945;&#32946;&#24037;&#20855;&#30340;&#26377;&#25928;&#24615;&#12290;&#35813;&#25968;&#25454;&#24211;&#32034;&#24341;&#20102;&#20197;&#24448;&#30340;AI&#20260;&#23475;&#23454;&#20363;&#65292;&#24182;&#36890;&#36807;&#35838;&#22530;&#30740;&#31350;&#25581;&#31034;&#20102;&#23398;&#29983;&#23545;AI&#20262;&#29702;&#20027;&#39064;&#30340;&#21021;&#27493;&#30475;&#27861;&#21644;&#30693;&#35782;&#32570;&#21475;&#30340;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2310.06269</link><description>&lt;p&gt;
AI&#20107;&#25925;&#25968;&#25454;&#24211;&#20316;&#20026;&#25552;&#39640;&#20154;&#20204;&#23545;&#20154;&#24037;&#26234;&#33021;&#20260;&#23475;&#24847;&#35782;&#30340;&#25945;&#32946;&#24037;&#20855;&#65306;&#23545;&#26377;&#25928;&#24615;&#12289;&#38480;&#21046;&#24615;&#21644;&#26410;&#26469;&#25913;&#36827;&#30340;&#35838;&#22530;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
The AI Incident Database as an Educational Tool to Raise Awareness of AI Harms: A Classroom Exploration of Efficacy, Limitations, &amp; Future Improvements. (arXiv:2310.06269v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06269
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;AI&#20107;&#25925;&#25968;&#25454;&#24211;&#20316;&#20026;&#25552;&#39640;&#20154;&#20204;&#23545;&#20154;&#24037;&#26234;&#33021;&#20260;&#23475;&#24847;&#35782;&#30340;&#25945;&#32946;&#24037;&#20855;&#30340;&#26377;&#25928;&#24615;&#12290;&#35813;&#25968;&#25454;&#24211;&#32034;&#24341;&#20102;&#20197;&#24448;&#30340;AI&#20260;&#23475;&#23454;&#20363;&#65292;&#24182;&#36890;&#36807;&#35838;&#22530;&#30740;&#31350;&#25581;&#31034;&#20102;&#23398;&#29983;&#23545;AI&#20262;&#29702;&#20027;&#39064;&#30340;&#21021;&#27493;&#30475;&#27861;&#21644;&#30693;&#35782;&#32570;&#21475;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#30830;&#23450;&#20102;&#23558;AI&#20262;&#29702;&#20027;&#39064;&#32435;&#20837;&#35745;&#31639;&#26426;&#19982;&#25968;&#25454;&#31185;&#23398;&#35838;&#31243;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#65292;AI&#20262;&#29702;&#25945;&#32946;&#30340;&#20851;&#38190;&#30446;&#26631;&#20043;&#19968;&#24517;&#39035;&#26159;&#25552;&#39640;&#23545;AI&#20260;&#23475;&#30340;&#24847;&#35782;&#12290;&#34429;&#28982;&#26377;&#21508;&#31181;&#26469;&#28304;&#21487;&#20197;&#20102;&#35299;&#27492;&#31867;&#20260;&#23475;&#65292;&#20294;AI&#20107;&#25925;&#25968;&#25454;&#24211;&#65288;AIID&#65289;&#26159;&#20026;&#25968;&#19981;&#22810;&#30340;&#35797;&#22270;&#25552;&#20379;&#19968;&#20010;&#30456;&#23545;&#20840;&#38754;&#30340;&#25968;&#25454;&#24211;&#65292;&#32034;&#24341;&#20197;&#21069;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#37096;&#32626;AI&#25216;&#26415;&#23548;&#33268;&#30340;&#20260;&#23475;&#25110;&#28508;&#22312;&#20260;&#23475;&#30340;&#23454;&#20363;&#20043;&#19968;&#12290;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;AIID&#20316;&#20026;&#19968;&#31181;&#25945;&#32946;&#24037;&#20855;&#65292;&#22312;&#26377;&#31038;&#20250;&#39640;&#39118;&#38505;&#39046;&#22495;&#20013;&#25552;&#39640;&#23545;AI&#20260;&#23475;&#30340;&#26222;&#36941;&#24615;&#21644;&#20005;&#37325;&#24615;&#30340;&#24847;&#35782;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#19968;&#25152;R1&#26426;&#26500;&#30340;&#35838;&#31243;&#20013;&#36827;&#34892;&#30340;&#19968;&#39033;&#35838;&#22530;&#30740;&#31350;&#26469;&#21576;&#29616;&#25152;&#24471;&#21040;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#35813;&#35838;&#31243;&#19987;&#27880;&#20110;AI&#21644;ML&#30340;&#31038;&#20250;&#21644;&#20262;&#29702;&#32771;&#34385;&#12290;&#25105;&#20204;&#30340;&#23450;&#24615;&#30740;&#31350;&#32467;&#26524;&#25551;&#32472;&#20102;&#23398;&#29983;&#23545;AI&#20262;&#29702;&#26680;&#24515;&#20027;&#39064;&#30340;&#21021;&#27493;&#30475;&#27861;&#20197;&#21450;&#20182;&#20204;&#24076;&#26395;&#22635;&#34917;&#30693;&#35782;&#32570;&#21475;&#30340;&#24895;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior work has established the importance of integrating AI ethics topics into computer and data sciences curricula. We provide evidence suggesting that one of the critical objectives of AI Ethics education must be to raise awareness of AI harms. While there are various sources to learn about such harms, The AI Incident Database (AIID) is one of the few attempts at offering a relatively comprehensive database indexing prior instances of harms or near harms stemming from the deployment of AI technologies in the real world. This study assesses the effectiveness of AIID as an educational tool to raise awareness regarding the prevalence and severity of AI harms in socially high-stakes domains. We present findings obtained through a classroom study conducted at an R1 institution as part of a course focused on the societal and ethical considerations around AI and ML. Our qualitative findings characterize students' initial perceptions of core topics in AI ethics and their desire to close the 
&lt;/p&gt;</description></item><item><title>CodeFuse-13B&#26159;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#20195;&#30721;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#19987;&#20026;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#35774;&#35745;&#65292;&#25903;&#25345;&#36229;&#36807;40&#31181;&#32534;&#31243;&#35821;&#35328;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#39640;&#36136;&#37327;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20197;&#21450;&#22823;&#37327;&#23454;&#39564;&#30340;&#39564;&#35777;&#65292;&#23637;&#29616;&#20102;&#20854;&#22312;&#22810;&#35821;&#35328;&#36755;&#20837;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.06266</link><description>&lt;p&gt;
CodeFuse-13B: &#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#20195;&#30721;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CodeFuse-13B: A Pretrained Multi-lingual Code Large Language Model. (arXiv:2310.06266v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06266
&lt;/p&gt;
&lt;p&gt;
CodeFuse-13B&#26159;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#20195;&#30721;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#19987;&#20026;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#35774;&#35745;&#65292;&#25903;&#25345;&#36229;&#36807;40&#31181;&#32534;&#31243;&#35821;&#35328;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#39640;&#36136;&#37327;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20197;&#21450;&#22823;&#37327;&#23454;&#39564;&#30340;&#39564;&#35777;&#65292;&#23637;&#29616;&#20102;&#20854;&#22312;&#22810;&#35821;&#35328;&#36755;&#20837;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(Code LLMs)&#22240;&#20854;&#22312;&#36719;&#20214;&#24037;&#31243;&#20840;&#29983;&#21629;&#21608;&#26399;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#32780;&#21463;&#21040;&#24037;&#19994;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#27169;&#22411;&#22312;&#29702;&#35299;&#38750;&#33521;&#35821;&#36755;&#20837;&#30340;&#22810;&#35821;&#35328;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#26041;&#38754;&#30340;&#25928;&#26524;&#20173;&#28982;&#36828;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;CodeFuse-13B&#65292;&#19968;&#20010;&#24320;&#28304;&#30340;&#39044;&#35757;&#32451;&#20195;&#30721;LLM&#12290;&#23427;&#19987;&#20026;&#21253;&#21547;&#33521;&#25991;&#21644;&#20013;&#25991;&#25552;&#31034;&#30340;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#32780;&#35774;&#35745;&#65292;&#24182;&#25903;&#25345;&#36229;&#36807;40&#31181;&#32534;&#31243;&#35821;&#35328;&#12290;CodeFuse&#36890;&#36807;&#21033;&#29992;&#30001;&#31243;&#24207;&#20998;&#26512;&#22120;&#31934;&#24515;&#31579;&#36873;&#24182;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20248;&#21270;&#30340;&#39640;&#36136;&#37327;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#26469;&#23454;&#29616;&#20854;&#25928;&#26524;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#21253;&#25324;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#30340;&#20351;&#29992;&#22330;&#26223;&#12289;&#24037;&#19994;&#26631;&#20934;&#22522;&#20934;HumanEval-x&#65292;&#20197;&#21450;&#19987;&#20026;&#20013;&#25991;&#25552;&#31034;&#35774;&#35745;&#30340;CodeFuseEval&#12290;&#20026;&#20102;&#35780;&#20272;CodeFuse&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#31215;&#26497;&#25910;&#38598;&#20102;AntGroup&#36719;&#20214;&#24320;&#21457;&#22242;&#38431;&#30340;&#23453;&#36149;&#20154;&#24037;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code Large Language Models (Code LLMs) have gained significant attention in the industry due to their wide applications in the full lifecycle of software engineering. However, the effectiveness of existing models in understanding non-English inputs for multi-lingual code-related tasks is still far from well studied. This paper introduces CodeFuse-13B, an open-sourced pre-trained code LLM. It is specifically designed for code-related tasks with both English and Chinese prompts and supports over 40 programming languages. CodeFuse achieves its effectiveness by utilizing a high quality pre-training dataset that is carefully filtered by program analyzers and optimized during the training process. Extensive experiments are conducted using real-world usage scenarios, the industry-standard benchmark HumanEval-x, and the specially designed CodeFuseEval for Chinese prompts. To assess the effectiveness of CodeFuse, we actively collected valuable human feedback from the AntGroup's software develop
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#21306;&#20998;&#24314;&#27169;&#26694;&#26550;&#29992;&#20110;&#22522;&#20110;&#27491;&#24120;&#22270;&#35757;&#32451;&#30340;&#24322;&#24120;&#22270;&#26816;&#27979;&#65292;&#36890;&#36807;&#29983;&#25104;&#25554;&#20540;&#30340;&#20266;&#24322;&#24120;&#22270;&#65292;&#19982;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#31639;&#27861;&#30456;&#27604;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.06261</link><description>&lt;p&gt;
&#33258;&#25105;&#21306;&#20998;&#24314;&#27169;&#29992;&#20110;&#24322;&#24120;&#22270;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Self-Discriminative Modeling for Anomalous Graph Detection. (arXiv:2310.06261v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#21306;&#20998;&#24314;&#27169;&#26694;&#26550;&#29992;&#20110;&#22522;&#20110;&#27491;&#24120;&#22270;&#35757;&#32451;&#30340;&#24322;&#24120;&#22270;&#26816;&#27979;&#65292;&#36890;&#36807;&#29983;&#25104;&#25554;&#20540;&#30340;&#20266;&#24322;&#24120;&#22270;&#65292;&#19982;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#31639;&#27861;&#30456;&#27604;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20165;&#22522;&#20110;&#27491;&#24120;&#22270;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#26816;&#27979;&#24322;&#24120;&#22270;&#30340;&#38382;&#39064;&#65292;&#36825;&#22312;&#20998;&#23376;&#12289;&#29983;&#29289;&#21644;&#31038;&#20132;&#32593;&#32476;&#25968;&#25454;&#20998;&#26512;&#20013;&#26377;&#35768;&#22810;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#21306;&#20998;&#24314;&#27169;&#26694;&#26550;&#29992;&#20110;&#24322;&#24120;&#22270;&#26816;&#27979;&#12290;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#20174;&#32473;&#23450;&#30340;&#27491;&#24120;&#22270;&#21644;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#29983;&#25104;&#30340;&#20266;&#24322;&#24120;&#22270;&#20013;&#30340;&#21028;&#21035;&#22120;&#65288;&#20998;&#31867;&#22120;&#65289;&#65292;&#20174;&#32780;&#20351;&#24471;&#29983;&#25104;&#30340;&#20266;&#24322;&#24120;&#22270;&#22312;&#27491;&#24120;&#22270;&#21644;&#30495;&#23454;&#24322;&#24120;&#22270;&#20043;&#38388;&#25554;&#20540;&#12290;&#22312;&#35813;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19977;&#31181;&#20855;&#26377;&#19981;&#21516;&#35745;&#31639;&#25928;&#29575;&#21644;&#31283;&#23450;&#24615;&#30340;&#31639;&#27861;&#29992;&#20110;&#24322;&#24120;&#22270;&#26816;&#27979;&#12290;&#36825;&#19977;&#31181;&#31639;&#27861;&#19982;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#22270;&#32423;&#21035;&#30340;&#24322;&#24120;&#26816;&#27979;&#22522;&#32447;&#22312;&#20061;&#20010;&#27969;&#34892;&#30340;&#22270;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#27604;&#36739;&#65288;&#20854;&#20013;&#22235;&#20010;&#25968;&#25454;&#38598;&#35268;&#27169;&#36739;&#23567;&#65292;&#20116;&#20010;&#25968;&#25454;&#38598;&#35268;&#27169;&#36866;&#20013;&#65289;&#65292;&#24182;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the problem of detecting anomalous graphs using a machine learning model trained on only normal graphs, which has many applications in molecule, biology, and social network data analysis. We present a self-discriminative modeling framework for anomalous graph detection. The key idea, mathematically and numerically illustrated, is to learn a discriminator (classifier) from the given normal graphs together with pseudo-anomalous graphs generated by a model jointly trained, where we never use any true anomalous graphs and we hope that the generated pseudo-anomalous graphs interpolate between normal ones and (real) anomalous ones. Under the framework, we provide three algorithms with different computational efficiencies and stabilities for anomalous graph detection. The three algorithms are compared with several state-of-the-art graph-level anomaly detection baselines on nine popular graph datasets (four with small size and five with moderate size) and show significant im
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#21435;&#32972;&#26223;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#22810;&#20010;&#39046;&#22495;&#19978;&#20165;&#20351;&#29992;&#23569;&#37327;&#26679;&#26412;&#21363;&#21487;&#36798;&#21040;&#21487;&#34892;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.06254</link><description>&lt;p&gt;
&#30475;&#26775;&#65311;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23569;&#26679;&#26412;&#21435;&#32972;&#26223;&#21270;
&lt;/p&gt;
&lt;p&gt;
Get the gist? Using large language models for few-shot decontextualization. (arXiv:2310.06254v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#21435;&#32972;&#26223;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#22810;&#20010;&#39046;&#22495;&#19978;&#20165;&#20351;&#29992;&#23569;&#37327;&#26679;&#26412;&#21363;&#21487;&#36798;&#21040;&#21487;&#34892;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28041;&#21450;&#35299;&#37322;&#23500;&#26377;&#19978;&#19979;&#25991;&#30340;&#21477;&#23376;&#30340;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20013;&#65292;&#20363;&#22914;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#25110;&#23545;&#35805;&#31995;&#32479;&#65292;&#24456;&#26377;&#24517;&#35201;&#33021;&#22815;&#23558;&#21477;&#23376;&#20445;&#30041;&#22312;&#19968;&#20010;&#21487;&#20197;&#22312;&#27809;&#26377;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#36731;&#26494;&#29702;&#35299;&#30340;&#24418;&#24335;&#20013;&#65292;&#20197;&#20415;&#20197;&#21518;&#37325;&#26032;&#20351;&#29992;&#65292;&#36825;&#20010;&#36807;&#31243;&#34987;&#31216;&#20026;&#8220;&#21435;&#32972;&#26223;&#21270;&#8221;&#12290;&#34429;&#28982;&#20197;&#21069;&#30340;&#24037;&#20316;&#35777;&#26126;&#20102;&#32463;&#36807;&#32454;&#35843;&#30340;&#29983;&#25104;&#22411;Seq2Seq&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#30340;&#21435;&#32972;&#26223;&#21270;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#38656;&#35201;&#26114;&#36149;&#30340;&#20154;&#24037;&#27880;&#37322;&#65292;&#32780;&#19988;&#21487;&#33021;&#26080;&#27861;&#36801;&#31227;&#21040;&#20854;&#20182;&#39046;&#22495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#21435;&#32972;&#26223;&#21270;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#21021;&#27493;&#32467;&#26524;&#65292;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#39046;&#22495;&#19978;&#20165;&#20351;&#29992;&#23569;&#37327;&#26679;&#26412;&#21363;&#21487;&#36798;&#21040;&#21487;&#34892;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many NLP applications that involve interpreting sentences within a rich context -- for instance, information retrieval systems or dialogue systems -it is desirable to be able to preserve the sentence in a form that can be readily understood without context, for later reuse -- a process known as ``decontextualization''. While previous work demonstrated that generative Seq2Seq models could effectively perform decontextualization after being fine-tuned on a specific dataset, this approach requires expensive human annotations and may not transfer to other domains. We propose a few-shot method of decontextualization using a large language model, and present preliminary results showing that this method achieves viable performance on multiple domains using only a small set of examples.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23545;&#35805;&#31995;&#32479;&#20013;&#24341;&#23548;&#21644;&#24212;&#29992;&#20064;&#24815;&#24615;&#27169;&#24335;&#26469;&#29983;&#25104;&#22522;&#20110;&#35282;&#33394;&#30340;&#22238;&#24212;&#12290;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#35770;&#25991;&#36890;&#36807;&#26126;&#30830;&#30340;&#27169;&#24335;&#34920;&#31034;&#26469;&#25429;&#25417;&#19982;&#35282;&#33394;&#30456;&#20851;&#30340;&#20064;&#24815;&#24615;&#30693;&#35782;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22238;&#24212;&#30340;&#20934;&#30830;&#24615;&#21644;&#33258;&#28982;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.06245</link><description>&lt;p&gt;
&#25105;&#20204;&#26159;&#25105;&#20204;&#21453;&#22797;&#20570;&#30340;&#20107;&#24773;&#65306;&#22312;&#22522;&#20110;&#35282;&#33394;&#30340;&#22238;&#24212;&#20013;&#24341;&#23548;&#21644;&#24212;&#29992;&#20064;&#24815;&#24615;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
We are what we repeatedly do: Inducing and deploying habitual schemas in persona-based responses. (arXiv:2310.06245v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06245
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23545;&#35805;&#31995;&#32479;&#20013;&#24341;&#23548;&#21644;&#24212;&#29992;&#20064;&#24815;&#24615;&#27169;&#24335;&#26469;&#29983;&#25104;&#22522;&#20110;&#35282;&#33394;&#30340;&#22238;&#24212;&#12290;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#35770;&#25991;&#36890;&#36807;&#26126;&#30830;&#30340;&#27169;&#24335;&#34920;&#31034;&#26469;&#25429;&#25417;&#19982;&#35282;&#33394;&#30456;&#20851;&#30340;&#20064;&#24815;&#24615;&#30693;&#35782;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22238;&#24212;&#30340;&#20934;&#30830;&#24615;&#21644;&#33258;&#28982;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#25216;&#26415;&#30340;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#38656;&#35201;&#26681;&#25454;&#29305;&#23450;&#30340;&#24320;&#21457;&#32773;&#35268;&#23450;&#30340;&#35282;&#33394;&#29983;&#25104;&#22238;&#24212;&#12290;&#34429;&#28982;&#21487;&#20197;&#20174;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20986;&#35768;&#22810;&#35282;&#33394;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#30340;&#19981;&#36879;&#26126;&#24615;&#21644;&#19981;&#21487;&#39044;&#27979;&#24615;&#20351;&#24471;&#20197;&#26126;&#30830;&#24418;&#24335;&#25351;&#23450;&#35282;&#33394;&#25104;&#20026;&#21487;&#21462;&#30340;&#12290;&#22312;&#20197;&#21069;&#30340;&#24037;&#20316;&#20013;&#65292;&#35282;&#33394;&#36890;&#24120;&#34987;&#34920;&#31034;&#20026;&#19968;&#27425;&#24615;&#30340;&#33258;&#25105;&#30693;&#35782;&#29255;&#27573;&#30340;&#38598;&#21512;&#65292;&#23545;&#35805;&#31995;&#32479;&#20250;&#26816;&#32034;&#36825;&#20123;&#30693;&#35782;&#29255;&#27573;&#26469;&#29983;&#25104;&#22238;&#24212;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#30340;&#20154;&#31867;&#23545;&#35805;&#20013;&#65292;&#35282;&#33394;&#36890;&#24120;&#36890;&#36807;&#31867;&#20284;&#25925;&#20107;&#30340;&#21465;&#36848;&#26469;&#25581;&#31034;&#20986;&#26469;&#65292;&#36825;&#20123;&#21465;&#36848;&#28041;&#21450;&#20016;&#23500;&#30340;&#20064;&#24815;&#24615;&#30693;&#35782;&#8212;&#8212;&#20851;&#20110;&#20195;&#29702;&#20154;&#32463;&#24120;&#21442;&#19982;&#30340;&#20107;&#20214;&#31867;&#22411;&#30340;&#30693;&#35782;&#65288;&#20363;&#22914;&#65292;&#24037;&#20316;&#27963;&#21160;&#12289;&#29233;&#22909;&#12289;&#20307;&#32946;&#27963;&#21160;&#12289;&#21916;&#27426;&#30340;&#23089;&#20048;&#31561;&#65289;&#65292;&#21253;&#25324;&#36825;&#20123;&#20107;&#20214;&#30340;&#20856;&#22411;&#30446;&#26631;&#12289;&#23376;&#20107;&#20214;&#12289;&#21069;&#25552;&#26465;&#20214;&#21644;&#21518;&#32622;&#26465;&#20214;&#12290;&#25105;&#20204;&#20351;&#29992;&#26126;&#30830;&#30340;&#27169;&#24335;&#34920;&#31034;&#26469;&#25429;&#25417;&#36825;&#31181;&#20064;&#24815;&#24615;&#30693;&#35782;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24341;&#23548;&#23545;&#35805;&#31995;&#32479;&#20351;&#29992;&#36825;&#20123;&#27169;&#24335;&#29983;&#25104;&#22238;&#24212;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many practical applications of dialogue technology require the generation of responses according to a particular developer-specified persona. While a variety of personas can be elicited from recent large language models, the opaqueness and unpredictability of these models make it desirable to be able to specify personas in an explicit form. In previous work, personas have typically been represented as sets of one-off pieces of self-knowledge that are retrieved by the dialogue system for use in generation. However, in realistic human conversations, personas are often revealed through story-like narratives that involve rich habitual knowledge -- knowledge about kinds of events that an agent often participates in (e.g., work activities, hobbies, sporting activities, favorite entertainments, etc.), including typical goals, sub-events, preconditions, and postconditions of those events. We capture such habitual knowledge using an explicit schema representation, and propose an approach to dia
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#27010;&#24565;&#21644;&#20851;&#31995;&#25552;&#21462;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#36719;&#25552;&#31034;&#35843;&#20248;&#21462;&#24471;&#20102;&#26368;&#20339;&#24615;&#33021;&#65292;&#24182;&#35266;&#23519;&#20102;&#36801;&#31227;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.06239</link><description>&lt;p&gt;
&#27169;&#22411;&#35843;&#20248;&#36824;&#26159;&#25552;&#31034;&#35843;&#20248;&#65311;&#23545;&#20110;&#20020;&#24202;&#27010;&#24565;&#21644;&#20851;&#31995;&#25552;&#21462;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Model Tuning or Prompt Tuning? A Study of Large Language Models for Clinical Concept and Relation Extraction. (arXiv:2310.06239v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06239
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#27010;&#24565;&#21644;&#20851;&#31995;&#25552;&#21462;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#36719;&#25552;&#31034;&#35843;&#20248;&#21462;&#24471;&#20102;&#26368;&#20339;&#24615;&#33021;&#65292;&#24182;&#35266;&#23519;&#20102;&#36801;&#31227;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#22522;&#20110;&#36719;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23398;&#20064;&#31639;&#27861;&#65292;&#30740;&#31350;&#25552;&#31034;&#30340;&#24418;&#29366;&#12289;&#20351;&#29992;&#20923;&#32467;/&#35299;&#20923;LLMs&#36827;&#34892;&#25552;&#31034;&#35843;&#20248;&#12289;&#36801;&#31227;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#12290;&#36890;&#36807;&#24320;&#21457;&#20102;&#22522;&#20110;&#36719;&#25552;&#31034;&#30340;LLM&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#22235;&#31181;&#35757;&#32451;&#31574;&#30053;&#65288;1.&#26080;&#25552;&#31034;&#24494;&#35843;&#65307;2.&#35299;&#20923;LLMs&#30340;&#30828;&#25552;&#31034;&#65307;3.&#35299;&#20923;LLMs&#30340;&#36719;&#25552;&#31034;&#65307;4.&#20923;&#32467;LLMs&#30340;&#36719;&#25552;&#31034;&#65289;&#26469;&#35780;&#20272;&#20102;&#19971;&#20010;&#39044;&#35757;&#32451;&#30340;LLMs&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#20020;&#24202;&#27010;&#24565;&#21644;&#20851;&#31995;&#25552;&#21462;&#33021;&#21147;&#12290;&#22312;&#36328;&#26426;&#26500;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#22522;&#20110;&#25552;&#31034;&#23398;&#20064;&#31639;&#27861;&#30340;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#35780;&#20272;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective To develop soft prompt-based learning algorithms for large language models (LLMs), examine the shape of prompts, prompt-tuning using frozen/unfrozen LLMs, transfer learning, and few-shot learning abilities. Methods We developed a soft prompt-based LLM model and compared 4 training strategies including (1) fine-tuning without prompts; (2) hard-prompt with unfrozen LLMs; (3) soft-prompt with unfrozen LLMs; and (4) soft-prompt with frozen LLMs. We evaluated 7 pretrained LLMs using the 4 training strategies for clinical concept and relation extraction on two benchmark datasets. We evaluated the transfer learning ability of the prompt-based learning algorithms in a cross-institution setting. We also assessed the few-shot learning ability. Results and Conclusion When LLMs are unfrozen, GatorTron-3.9B with soft prompting achieves the best strict F1-scores of 0.9118 and 0.8604 for concept extraction, outperforming the traditional fine-tuning and hard prompt-based models by 0.6~3.1% a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35299;&#20915;&#20102;MUSIC-AVQA&#20013;&#30340;&#25968;&#25454;&#20559;&#24046;&#38382;&#39064;&#65292;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#26469;&#20445;&#35777;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#25512;&#29702;&#21508;&#31181;&#22810;&#27169;&#24577;&#24773;&#20917;&#19979;&#30340;&#38382;&#39064;&#12290;&#20182;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;MUSIC-AVQA v2.0&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.06238</link><description>&lt;p&gt;
&#35299;&#20915;MUSIC-AVQA&#20013;&#30340;&#25968;&#25454;&#20559;&#24046;&#38382;&#39064;&#65306;&#20026;&#26080;&#20559;&#38382;&#31572;&#21019;&#24314;&#19968;&#20010;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Tackling Data Bias in MUSIC-AVQA: Crafting a Balanced Dataset for Unbiased Question-Answering. (arXiv:2310.06238v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35299;&#20915;&#20102;MUSIC-AVQA&#20013;&#30340;&#25968;&#25454;&#20559;&#24046;&#38382;&#39064;&#65292;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#26469;&#20445;&#35777;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#25512;&#29702;&#21508;&#31181;&#22810;&#27169;&#24577;&#24773;&#20917;&#19979;&#30340;&#38382;&#39064;&#12290;&#20182;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;MUSIC-AVQA v2.0&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#38899;&#39057;&#12289;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#30340;&#20132;&#21449;&#30740;&#31350;&#36234;&#26469;&#36234;&#21463;&#37325;&#35270;&#65292;&#25512;&#21160;&#20102;&#22810;&#27169;&#24577;&#30740;&#31350;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#20219;&#20309;&#27169;&#24577;&#20013;&#23384;&#22312;&#30340;&#24378;&#28872;&#20559;&#35265;&#20250;&#23548;&#33268;&#27169;&#22411;&#24573;&#35270;&#20854;&#20182;&#27169;&#24577;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#26377;&#25928;&#22320;&#36328;&#36234;&#36825;&#20123;&#22810;&#26679;&#21270;&#27169;&#24577;&#36827;&#34892;&#25512;&#29702;&#30340;&#33021;&#21147;&#21463;&#21040;&#25439;&#23475;&#65292;&#38459;&#30861;&#20102;&#36827;&#19968;&#27493;&#30340;&#21457;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#23457;&#26597;&#20102;&#21407;&#22987;&#25968;&#25454;&#38598;&#20013;&#30340;&#27599;&#31181;&#38382;&#39064;&#31867;&#22411;&#65292;&#36873;&#25321;&#20855;&#26377;&#26126;&#26174;&#31572;&#26696;&#20559;&#35265;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#20559;&#35265;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#20114;&#34917;&#30340;&#35270;&#39057;&#21644;&#38382;&#39064;&#65292;&#30830;&#20445;&#27809;&#26377;&#31572;&#26696;&#26377;&#26126;&#26174;&#30340;&#20559;&#26012;&#20998;&#24067;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#20108;&#20803;&#38382;&#39064;&#65292;&#25105;&#20204;&#21162;&#21147;&#30830;&#20445;&#27599;&#20010;&#38382;&#39064;&#31867;&#21035;&#20013;&#20004;&#20010;&#31572;&#26696;&#20960;&#20046;&#22343;&#21248;&#20998;&#24067;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;MUSIC-AVQA v2.0&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#23427;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#25105;&#20204;&#30456;&#20449;&#33021;&#22815;&#26356;&#22909;&#22320;&#20419;&#36827;AVQA&#20219;&#21153;&#30340;&#36827;&#23637;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been a growing emphasis on the intersection of audio, vision, and text modalities, driving forward the advancements in multimodal research. However, strong bias that exists in any modality can lead to the model neglecting the others. Consequently, the model's ability to effectively reason across these diverse modalities is compromised, impeding further advancement. In this paper, we meticulously review each question type from the original dataset, selecting those with pronounced answer biases. To counter these biases, we gather complementary videos and questions, ensuring that no answers have outstanding skewed distribution. In particular, for binary questions, we strive to ensure that both answers are almost uniformly spread within each question category. As a result, we construct a new dataset, named MUSIC-AVQA v2.0, which is more challenging and we believe could better foster the progress of AVQA task. Furthermore, we present a novel baseline model that de
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32763;&#35793;&#25104;&#20013;&#25991;&#24182;&#24635;&#32467;&#25351;&#20986;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#36880;&#28176;&#28436;&#36827;&#20026;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#65292;&#23613;&#31649;&#33258;&#28982;&#35821;&#35328;&#38750;&#24120;&#38590;&#20197;&#36827;&#34892;&#25968;&#23398;&#19978;&#30340;&#34920;&#36798;&#65292;&#20294;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#22312;NLP&#39046;&#22495;&#21462;&#24471;&#20102;&#36229;&#20046;&#39044;&#26399;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.06228</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#30340;&#28436;&#36827;&#65306;&#19981;&#20165;&#20165;&#26159;&#35821;&#35328;&#22788;&#29702;&#65292;&#32780;&#26159;&#26397;&#21521;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Evolution of Natural Language Processing Technology: Not Just Language Processing Towards General Purpose AI. (arXiv:2310.06228v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06228
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32763;&#35793;&#25104;&#20013;&#25991;&#24182;&#24635;&#32467;&#25351;&#20986;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#36880;&#28176;&#28436;&#36827;&#20026;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#65292;&#23613;&#31649;&#33258;&#28982;&#35821;&#35328;&#38750;&#24120;&#38590;&#20197;&#36827;&#34892;&#25968;&#23398;&#19978;&#30340;&#34920;&#36798;&#65292;&#20294;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#22312;NLP&#39046;&#22495;&#21462;&#24471;&#20102;&#36229;&#20046;&#39044;&#26399;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21457;&#26126;&#35745;&#31639;&#26426;&#20197;&#26469;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#65288;&#23454;&#38469;&#30340;&#20154;&#31867;&#35821;&#35328;&#65289;&#36827;&#34892;&#36890;&#20449;&#19968;&#30452;&#26159;&#19968;&#39033;&#26790;&#24187;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#33258;&#28982;&#35821;&#35328;&#38750;&#24120;&#38590;&#20197;&#36827;&#34892;&#25968;&#23398;&#19978;&#30340;&#34920;&#36798;&#65292;&#36825;&#20351;&#24471;&#22312;&#19981;&#32771;&#34385;&#32534;&#31243;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#31639;&#27861;&#21464;&#24471;&#22256;&#38590;&#12290;&#23613;&#31649;&#24050;&#32463;&#26377;&#20102;&#35768;&#22810;&#25216;&#26415;&#21457;&#23637;&#65292;&#20294;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#36824;&#19981;&#33021;&#35828;&#24050;&#32463;&#23454;&#29616;&#20102;&#20219;&#20309;&#20801;&#35768;&#33258;&#30001;&#21033;&#29992;&#30340;&#32467;&#26524;&#12290;&#20030;&#20363;&#26469;&#35828;&#65292;&#22312;&#20154;&#31867;&#30340;&#35821;&#35328;&#23398;&#20064;&#20013;&#65292;&#20363;&#22914;&#23398;&#20064;&#27597;&#35821;&#25110;&#22806;&#35821;&#26102;&#65292;&#25105;&#20204;&#24517;&#39035;&#25215;&#35748;&#36825;&#20010;&#36807;&#31243;&#19982;&#26684;&#35328;&#8220;&#29087;&#33021;&#29983;&#24039;&#8221;&#22312;&#21407;&#21017;&#19978;&#26159;&#30456;&#20284;&#30340;&#65292;&#23613;&#31649;&#23398;&#20064;&#26041;&#27861;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#26159;&#37325;&#35201;&#30340;&#12290;&#28145;&#24230;&#23398;&#20064;&#36817;&#24180;&#26469;&#22312;&#24403;&#20195;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20013;&#25198;&#28436;&#20102;&#26680;&#24515;&#35282;&#33394;&#12290;&#24403;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26102;&#65292;&#36825;&#20135;&#29983;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#32467;&#26524;&#12290;&#20174;&#23398;&#20064;&#22823;&#37327;&#25991;&#26412;&#25968;&#25454;&#30340;&#32467;&#26524;&#20013;&#65292;&#24050;&#32463;&#26377;&#25253;&#36947;&#31216;&#36229;&#20986;&#20102;&#26368;&#21021;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the invention of computers, communication through natural language (actual human language) has been a dream technology. However, natural language is extremely difficult to mathematically formulate, making it difficult to realize as an algorithm without considering programming. While there have been numerous technological developments, one cannot say that any results allowing free utilization have been achieved thus far. In the case of language learning in humans, for instance when learning one's mother tongue or foreign language, one must admit that this process is similar to the adage "practice makes perfect" in principle, even though the learning method is significant up to a point. Deep learning has played a central role in contemporary AI technology in recent years. When applied to natural language processing (NLP), this produced unprecedented results. Achievements exceeding the initial predictions have been reported from the results of learning vast amounts of textual data u
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22238;&#31572;&#20892;&#19994;&#30456;&#20851;&#38382;&#39064;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;RAG&#21644;ER&#25216;&#26415;&#25552;&#39640;&#20102;LLMs&#30340;&#24615;&#33021;&#12290;&#20854;&#20013;&#65292;GPT-4&#22312;&#20892;&#19994;&#32771;&#35797;&#20013;&#21462;&#24471;&#20102;&#21450;&#26684;&#20998;&#25968;&#20197;&#33719;&#24471;&#35748;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.06225</link><description>&lt;p&gt;
GPT-4&#20316;&#20026;&#20892;&#23398;&#21161;&#25163;&#65311;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22238;&#31572;&#20892;&#19994;&#32771;&#35797;
&lt;/p&gt;
&lt;p&gt;
GPT-4 as an Agronomist Assistant? Answering Agriculture Exams Using Large Language Models. (arXiv:2310.06225v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22238;&#31572;&#20892;&#19994;&#30456;&#20851;&#38382;&#39064;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;RAG&#21644;ER&#25216;&#26415;&#25552;&#39640;&#20102;LLMs&#30340;&#24615;&#33021;&#12290;&#20854;&#20013;&#65292;GPT-4&#22312;&#20892;&#19994;&#32771;&#35797;&#20013;&#21462;&#24471;&#20102;&#21450;&#26684;&#20998;&#25968;&#20197;&#33719;&#24471;&#35748;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#21307;&#30103;&#20445;&#20581;&#21644;&#37329;&#34701;&#39046;&#22495;&#65292;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#65292;LLM&#30340;&#24615;&#33021;&#19982;&#35757;&#32451;&#26377;&#32032;&#30340;&#20154;&#31867;&#30456;&#20284;&#29978;&#33267;&#26356;&#22909;&#65292;&#22240;&#27492;&#21512;&#29702;&#22320;&#20351;&#29992;&#20154;&#31867;&#32771;&#35797;&#65288;&#20363;&#22914;&#35748;&#35777;&#32771;&#35797;&#65289;&#26469;&#35780;&#20272;LLM&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;&#27969;&#34892;&#30340;LLM&#65288;&#22914;Llama 2&#21644;GPT&#65289;&#22312;&#22238;&#31572;&#20892;&#19994;&#30456;&#20851;&#38382;&#39064;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#22312;&#35780;&#20272;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#36824;&#36816;&#29992;&#20102;RAG&#65288;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65289;&#21644;ER&#65288;&#38598;&#21512;&#32454;&#21270;&#65289;&#25216;&#26415;&#65292;&#32467;&#21512;&#20449;&#24687;&#26816;&#32034;&#12289;&#29983;&#25104;&#33021;&#21147;&#21644;&#25552;&#31034;&#31574;&#30053;&#65292;&#25552;&#39640;LLM&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#23637;&#31034;LLM&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#26469;&#33258;&#24052;&#35199;&#12289;&#21360;&#24230;&#21644;&#32654;&#22269;&#19977;&#20010;&#26368;&#22823;&#30340;&#20892;&#19994;&#29983;&#20135;&#22269;&#30340;&#20892;&#19994;&#32771;&#35797;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#31361;&#20986;&#20102;GPT-4&#22312;&#32771;&#35797;&#20013;&#21462;&#24471;&#21450;&#26684;&#20998;&#25968;&#20197;&#33719;&#24471;&#35748;&#35777;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable capabilities in natural language understanding across various domains, including healthcare and finance. For some tasks, LLMs achieve similar or better performance than trained human beings, therefore it is reasonable to employ human exams (e.g., certification tests) to assess the performance of LLMs. We present a comprehensive evaluation of popular LLMs, such as Llama 2 and GPT, on their ability to answer agriculture-related questions. In our evaluation, we also employ RAG (Retrieval-Augmented Generation) and ER (Ensemble Refinement) techniques, which combine information retrieval, generation capabilities, and prompting strategies to improve the LLMs' performance. To demonstrate the capabilities of LLMs, we selected agriculture exams and benchmark datasets from three of the largest agriculture producer countries: Brazil, India, and the USA. Our analysis highlights GPT-4's ability to achieve a passing score on exams to earn cred
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36719;&#22343;&#21248;&#22359;&#21098;&#26525;&#65288;SUBP&#65289;&#26041;&#27861;&#65292;&#22312;1xN&#31232;&#30095;CNN&#20013;&#23454;&#29616;&#20102;&#22810;&#32447;&#31243;&#21152;&#36895;&#65292;&#24182;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#35757;&#32451;&#25104;&#26412;&#26114;&#36149;&#12289;&#20869;&#23384;&#35775;&#38382;&#24320;&#38144;&#22823;&#12289;&#27169;&#22411;&#36136;&#37327;&#27425;&#20248;&#20197;&#21450;&#32447;&#31243;&#36127;&#36733;&#19981;&#24179;&#34913;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.06218</link><description>&lt;p&gt;
SUBP&#65306;&#36719;&#22343;&#21248;&#22359;&#21098;&#26525;&#29992;&#20110;1xN&#31232;&#30095;CNN&#30340;&#22810;&#32447;&#31243;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
SUBP: Soft Uniform Block Pruning for 1xN Sparse CNNs Multithreading Acceleration. (arXiv:2310.06218v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06218
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36719;&#22343;&#21248;&#22359;&#21098;&#26525;&#65288;SUBP&#65289;&#26041;&#27861;&#65292;&#22312;1xN&#31232;&#30095;CNN&#20013;&#23454;&#29616;&#20102;&#22810;&#32447;&#31243;&#21152;&#36895;&#65292;&#24182;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#35757;&#32451;&#25104;&#26412;&#26114;&#36149;&#12289;&#20869;&#23384;&#35775;&#38382;&#24320;&#38144;&#22823;&#12289;&#27169;&#22411;&#36136;&#37327;&#27425;&#20248;&#20197;&#21450;&#32447;&#31243;&#36127;&#36733;&#19981;&#24179;&#34913;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#20013;&#30340;&#31232;&#30095;&#24615;&#30740;&#31350;&#24050;&#32463;&#24191;&#27867;&#24212;&#29992;&#20110;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#21387;&#32553;&#21644;&#21152;&#36895;&#27169;&#22411;&#12290;&#36890;&#36807;&#32422;&#26463;&#36755;&#20986;&#36890;&#36947;&#19978;&#30340;N&#20010;&#36830;&#32493;&#26435;&#37325;&#20026;&#32452;&#20869;&#38750;&#38646;&#65292;&#26368;&#36817;&#30340;1xN&#31232;&#30095;&#32593;&#32476;&#22240;&#20854;&#19977;&#20010;&#31361;&#20986;&#20248;&#21183;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#65306;1&#65289;&#36890;&#36807;&#19968;&#31181;&#8220;&#22359;&#31232;&#30095;&#34892;&#8221;&#30697;&#38453;&#22823;&#37327;&#33410;&#30465;&#23384;&#20648;&#31354;&#38388;&#12290;2&#65289;&#22312;&#39640;&#31232;&#30095;&#24615;&#19979;&#34920;&#29616;&#20986;&#33394;&#12290;3&#65289;&#22312;&#20855;&#26377;&#39640;&#32423;&#30690;&#37327;&#25193;&#23637;&#30340;CPU&#19978;&#26174;&#33879;&#21152;&#36895;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#38656;&#35201;&#22522;&#20110;&#31264;&#23494;&#39044;&#35757;&#32451;&#26435;&#37325;&#36873;&#25321;&#21644;&#24494;&#35843;1xN&#31232;&#30095;&#26435;&#37325;&#65292;&#23548;&#33268;&#35757;&#32451;&#25104;&#26412;&#26114;&#36149;&#12289;&#20869;&#23384;&#35775;&#38382;&#24320;&#38144;&#22823;&#12289;&#27169;&#22411;&#36136;&#37327;&#27425;&#20248;&#20197;&#21450;&#19981;&#24179;&#34913;&#30340;&#32447;&#31243;&#36127;&#36733;&#65288;&#36755;&#20986;&#36890;&#36947;&#19978;&#30340;&#19981;&#21516;&#31232;&#30095;&#24615;&#65289;&#31561;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#36719;&#22343;&#21248;&#22359;&#21098;&#26525;&#8221;&#65288;SUBP&#65289;&#26041;&#27861;&#26469;&#35757;&#32451;&#19968;&#20010;u
&lt;/p&gt;
&lt;p&gt;
The study of sparsity in Convolutional Neural Networks (CNNs) has become widespread to compress and accelerate models in environments with limited resources. By constraining N consecutive weights along the output channel to be group-wise non-zero, the recent network with 1$\times$N sparsity has received tremendous popularity for its three outstanding advantages: 1) A large amount of storage space saving by a \emph{Block Sparse Row} matrix. 2) Excellent performance at a high sparsity. 3) Significant speedups on CPUs with Advanced Vector Extensions. Recent work requires selecting and fine-tuning 1$\times$N sparse weights based on dense pre-trained weights, leading to the problems such as expensive training cost and memory access, sub-optimal model quality, as well as unbalanced workload across threads (different sparsity across output channels). To overcome them, this paper proposes a novel \emph{\textbf{S}oft \textbf{U}niform \textbf{B}lock \textbf{P}runing} (SUBP) approach to train a u
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20351;&#29992;&#22238;&#24402;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#25968;&#23383;&#65292;&#36890;&#36807;&#25913;&#21464;&#27169;&#22411;&#30340;&#35789;&#27719;&#34920;&#26469;&#26356;&#22909;&#22320;&#34920;&#31034;&#25968;&#23383;&#30340;&#22823;&#23567;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.06204</link><description>&lt;p&gt;
&#26080;&#22238;&#24402;&#20272;&#35745;&#25968;&#23383;
&lt;/p&gt;
&lt;p&gt;
Estimating Numbers without Regression. (arXiv:2310.06204v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06204
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20351;&#29992;&#22238;&#24402;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#25968;&#23383;&#65292;&#36890;&#36807;&#25913;&#21464;&#27169;&#22411;&#30340;&#35789;&#27719;&#34920;&#26469;&#26356;&#22909;&#22320;&#34920;&#31034;&#25968;&#23383;&#30340;&#22823;&#23567;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35821;&#35328;&#27169;&#22411;&#22312;&#26368;&#36817;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#34920;&#31034;&#25968;&#23383;&#30340;&#33021;&#21147;&#20173;&#28982;&#19981;&#36275;&#12290;&#20154;&#31867;&#26681;&#25454;&#25968;&#23383;&#30340;&#22823;&#23567;&#36827;&#34892;&#27010;&#24565;&#21270;&#65292;&#26377;&#25928;&#22320;&#23558;&#23427;&#20204;&#25237;&#23556;&#21040;&#25968;&#23383;&#32447;&#19978;&#65307;&#32780;&#23376;&#35789;&#26631;&#35760;&#21270;&#26080;&#27861;&#26126;&#30830;&#25429;&#25417;&#25968;&#23383;&#30340;&#22823;&#23567;&#65292;&#23558;&#25968;&#23383;&#20998;&#21106;&#25104;&#20219;&#24847;&#30340;&#22359;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#32570;&#28857;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#26367;&#20195;&#26041;&#27861;&#65292;&#22312;&#35821;&#35328;&#24314;&#27169;&#31649;&#32447;&#30340;&#21508;&#20010;&#38454;&#27573;&#20462;&#25913;&#25968;&#23383;&#12290;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#25913;&#21464;&#25968;&#23383;&#30340;&#34920;&#31034;&#26041;&#24335;&#65288;&#20363;&#22914;&#31185;&#23398;&#35745;&#25968;&#27861;&#19982;&#21313;&#36827;&#21046;&#65289;&#65292;&#35201;&#20040;&#25913;&#21464;&#29992;&#20110;&#34920;&#31034;&#25968;&#23383;&#30340;&#35789;&#27719;&#34920;&#65292;&#35201;&#20040;&#30452;&#25509;&#25913;&#21464;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#30340;&#26550;&#26500;&#65292;&#20197;&#30452;&#25509;&#22238;&#24402;&#21040;&#25152;&#38656;&#30340;&#25968;&#23383;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#26550;&#26500;&#30340;&#26356;&#25913;&#26377;&#21161;&#20110;&#22312;&#25968;&#23383;&#20272;&#35745;&#19978;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#19968;&#20010;&#26377;&#27934;&#23519;&#21147;&#30340;&#32570;&#25022;&#65306;&#25913;&#21464;&#27169;&#22411;&#30340;&#35789;&#27719;&#34920;&#65288;&#20363;&#22914;&#22312;10-100&#33539;&#22260;&#20869;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#25968;&#23383;&#26631;&#35760;&#65289;&#26159;&#19968;&#20010;&#26356;&#22909;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent successes in language models, their ability to represent numbers is insufficient. Humans conceptualize numbers based on their magnitudes, effectively projecting them on a number line; whereas subword tokenization fails to explicitly capture magnitude by splitting numbers into arbitrary chunks. To alleviate this shortcoming, alternative approaches have been proposed that modify numbers at various stages of the language modeling pipeline. These methods change either the (1) notation in which numbers are written (\eg scientific vs decimal), the (2) vocabulary used to represent numbers or the entire (3) architecture of the underlying language model, to directly regress to a desired number.  Previous work suggests that architectural change helps achieve state-of-the-art on number estimation but we find an insightful ablation: changing the model's vocabulary instead (\eg introduce a new token for numbers in range 10-100) is a far better trade-off. In the context of masked numb
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;msGeMM&#30340;&#26032;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20302;&#31934;&#24230;&#25968;&#25454;&#31867;&#22411;&#65292;&#21487;&#20197;&#20351;AI&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#39640;&#36817;2.5&#20493;&#12290;&#35813;&#31639;&#27861;&#38656;&#35201;&#29305;&#27530;&#30340;CUDA&#26680;&#24515;&#26469;&#23454;&#29616;&#20174;&#23567;&#22411;&#26597;&#25214;&#34920;&#20013;&#28155;&#21152;&#20803;&#32032;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.06178</link><description>&lt;p&gt;
&#20511;&#21161;msGeMM&#22686;&#21152;AI GeMM&#30340;&#24615;&#33021;&#36817;2.5&#20493;&#30340;Look-Up mAI GeMM
&lt;/p&gt;
&lt;p&gt;
Look-Up mAI GeMM: Increasing AI GeMMs Performance by Nearly 2.5x via msGeMM. (arXiv:2310.06178v1 [cs.PF])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06178
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;msGeMM&#30340;&#26032;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20302;&#31934;&#24230;&#25968;&#25454;&#31867;&#22411;&#65292;&#21487;&#20197;&#20351;AI&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#39640;&#36817;2.5&#20493;&#12290;&#35813;&#31639;&#27861;&#38656;&#35201;&#29305;&#27530;&#30340;CUDA&#26680;&#24515;&#26469;&#23454;&#29616;&#20174;&#23567;&#22411;&#26597;&#25214;&#34920;&#20013;&#28155;&#21152;&#20803;&#32032;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#27169;&#22411;&#30340;&#35268;&#27169;&#19981;&#26029;&#22686;&#21152;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;HPC&#24212;&#29992;&#20013;&#38656;&#35201;&#21452;&#31934;&#24230;&#25968;&#25454;&#31867;&#22411;&#65292;&#32780;fp8&#25110;int4&#31561;&#26356;&#20302;&#31934;&#24230;&#30340;&#25968;&#25454;&#31867;&#22411;&#24050;&#32463;&#36275;&#22815;&#29992;&#20110;&#35757;&#32451;&#21644;&#25512;&#26029;&#20013;&#65292;&#32780;&#19988;&#36136;&#37327;&#30456;&#24403;&#12290;&#22312;&#27492;&#36235;&#21183;&#19979;&#65292;&#20687;NVIDIA&#21644;AMD&#36825;&#26679;&#30340;GPU&#20379;&#24212;&#21830;&#36890;&#36807;&#24352;&#37327;&#26680;&#24515;&#25552;&#20379;&#20102;&#23545;fp16&#12289;fp8&#21644;int8 GeMM&#25805;&#20316;&#30340;&#30828;&#20214;&#25903;&#25345;&#65292;&#20855;&#26377;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;msGeMM&#30340;&#26032;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#35777;&#26126;&#20102;&#20351;&#29992;&#20302;&#31934;&#24230;&#25968;&#25454;&#31867;&#22411;&#30340;AI&#27169;&#22411;&#21487;&#20197;&#20943;&#23569;&#32422;2.5&#20493;&#30340;&#20056;&#27861;&#21644;&#21152;&#27861;&#25351;&#20196;&#12290;&#23454;&#29616;&#27492;&#31639;&#27861;&#30340;&#39640;&#25928;&#29575;&#38656;&#35201;&#20855;&#22791;&#19982;&#24352;&#37327;&#26680;&#24515;&#30456;&#21516;&#36895;&#29575;&#20174;&#23567;&#22411;&#26597;&#25214;&#34920;&#20013;&#28155;&#21152;&#20803;&#32032;&#30340;&#29305;&#27530;CUDA&#26680;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI models are increasing in size and recent advancement in the community has shown that unlike HPC applications where double precision datatype are required, lower-precision datatypes such as fp8 or int4 are sufficient to bring the same model quality both for training and inference. Following these trends, GPU vendors such as NVIDIA and AMD have added hardware support for fp16, fp8 and int8 GeMM operations with an exceptional performance via Tensor Cores. However, this paper proposes a new algorithm called msGeMM which shows that AI models with low-precision datatypes can run with ~2.5x fewer multiplication and add instructions. Efficient implementation of this algorithm requires special CUDA cores with the ability to add elements from a small look-up table at the rate of Tensor Cores.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#20107;&#23454;&#21644;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20351;&#29992;&#23884;&#20837;&#31354;&#38388;&#34920;&#31034;&#26681;&#25454;&#29992;&#25143;&#20559;&#22909;&#29983;&#25104;&#26377;&#36259;&#21644;&#30456;&#20851;&#30340;&#25512;&#33616;&#65292;&#24182;&#24320;&#21457;&#20102;&#32852;&#21512;&#22870;&#21169;&#20989;&#25968;&#26469;&#34913;&#37327;&#20934;&#30830;&#24615;&#12289;&#21560;&#24341;&#21147;&#21644;&#20010;&#24615;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#31995;&#32479;&#33021;&#22815;&#20026;&#29992;&#25143;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#30005;&#24433;&#21465;&#20107;&#12290;</title><link>http://arxiv.org/abs/2310.06176</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#20107;&#23454;&#21644;&#20010;&#24615;&#21270;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Factual and Personalized Recommendations using Language Models and Reinforcement Learning. (arXiv:2310.06176v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#20107;&#23454;&#21644;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20351;&#29992;&#23884;&#20837;&#31354;&#38388;&#34920;&#31034;&#26681;&#25454;&#29992;&#25143;&#20559;&#22909;&#29983;&#25104;&#26377;&#36259;&#21644;&#30456;&#20851;&#30340;&#25512;&#33616;&#65292;&#24182;&#24320;&#21457;&#20102;&#32852;&#21512;&#22870;&#21169;&#20989;&#25968;&#26469;&#34913;&#37327;&#20934;&#30830;&#24615;&#12289;&#21560;&#24341;&#21147;&#21644;&#20010;&#24615;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#31995;&#32479;&#33021;&#22815;&#20026;&#29992;&#25143;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#30005;&#24433;&#21465;&#20107;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22312;&#36830;&#25509;&#29992;&#25143;&#21644;&#20869;&#23481;&#12289;&#20135;&#21697;&#21644;&#26381;&#21153;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#26681;&#25454;&#29992;&#25143;&#30340;&#20559;&#22909;&#23558;&#20505;&#36873;&#39033;&#19982;&#29992;&#25143;&#36827;&#34892;&#21305;&#37197;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;ComPelling&#12289;Precise&#12289;Personalized&#12289;Preference-relevant&#65288;P4LM&#65289;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#24378;&#35843;&#35299;&#37322;&#29289;&#21697;&#29305;&#24449;&#21450;&#20854;&#30456;&#20851;&#24615;&#65292;&#20026;&#29992;&#25143;&#25512;&#33616;&#29289;&#21697;&#12290;P4LM&#21033;&#29992;&#29992;&#25143;&#20559;&#22909;&#30340;&#23884;&#20837;&#31354;&#38388;&#34920;&#31034;&#29983;&#25104;&#26377;&#36259;&#30340;&#12289;&#20107;&#23454;&#22522;&#30784;&#30340;&#12289;&#19982;&#29992;&#25143;&#20559;&#22909;&#30456;&#20851;&#30340;&#21709;&#24212;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#32852;&#21512;&#22870;&#21169;&#20989;&#25968;&#65292;&#34913;&#37327;&#20934;&#30830;&#24615;&#12289;&#21560;&#24341;&#21147;&#21644;&#20010;&#24615;&#21270;&#65292;&#23558;&#20854;&#20316;&#20026;&#24378;&#21270;&#23398;&#20064;-based &#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#21453;&#39304;&#12290;&#20351;&#29992;MovieLens 25M&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35777;&#26126;P4LM&#33021;&#22815;&#20026;&#29992;&#25143;&#25552;&#20379;&#24341;&#20154;&#20837;&#32988;&#30340;&#20010;&#24615;&#21270;&#30005;&#24433;&#21465;&#20107;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems (RSs) play a central role in connecting users to content, products, and services, matching candidate items to users based on their preferences. While traditional RSs rely on implicit user feedback signals, conversational RSs interact with users in natural language. In this work, we develop a comPelling, Precise, Personalized, Preference-relevant language model (P4LM) that recommends items to users while putting emphasis on explaining item characteristics and their relevance. P4LM uses the embedding space representation of a user's preferences to generate compelling responses that are factually-grounded and relevant w.r.t. the user's preferences. Moreover, we develop a joint reward function that measures precision, appeal, and personalization, which we use as AI-based feedback in a reinforcement learning-based language model framework. Using the MovieLens 25M dataset, we demonstrate that P4LM delivers compelling, personalized movie narratives to users.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#25552;&#31034;&#24037;&#31243;&#23545;ChatGPT&#22312;&#26080;&#30417;&#30563;&#23454;&#20307;&#35299;&#26512;&#20013;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#21021;&#27493;&#23454;&#39564;&#30740;&#31350;&#65292;&#32467;&#26524;&#26174;&#31034;&#25552;&#31034;&#21487;&#20197;&#26174;&#33879;&#24433;&#21709;&#23454;&#20307;&#35299;&#26512;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.06174</link><description>&lt;p&gt;
&#25552;&#31034;&#24037;&#31243;&#23545;ChatGPT&#22312;&#26080;&#30417;&#30563;&#23454;&#20307;&#35299;&#26512;&#20013;&#30340;&#24615;&#33021;&#24433;&#21709;&#26159;&#22914;&#20309;&#30340;&#65311;
&lt;/p&gt;
&lt;p&gt;
How does prompt engineering affect ChatGPT performance on unsupervised entity resolution?. (arXiv:2310.06174v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06174
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#25552;&#31034;&#24037;&#31243;&#23545;ChatGPT&#22312;&#26080;&#30417;&#30563;&#23454;&#20307;&#35299;&#26512;&#20013;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#21021;&#27493;&#23454;&#39564;&#30740;&#31350;&#65292;&#32467;&#26524;&#26174;&#31034;&#25552;&#31034;&#21487;&#20197;&#26174;&#33879;&#24433;&#21709;&#23454;&#20307;&#35299;&#26512;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#35299;&#26512;&#65288;ER&#65289;&#26159;&#19968;&#31181;&#21322;&#33258;&#21160;&#30830;&#23450;&#20004;&#20010;&#23454;&#20307;&#26159;&#21542;&#25351;&#21521;&#30456;&#21516;&#22522;&#30784;&#23454;&#20307;&#30340;&#38382;&#39064;&#65292;&#24212;&#29992;&#33539;&#22260;&#20174;&#21307;&#30103;&#20445;&#20581;&#21040;&#30005;&#23376;&#21830;&#21153;&#12290;&#20256;&#32479;&#30340;ER&#35299;&#20915;&#26041;&#26696;&#38656;&#35201;&#30456;&#24403;&#22810;&#30340;&#25163;&#21160;&#19987;&#19994;&#30693;&#35782;&#65292;&#21253;&#25324;&#29305;&#24449;&#24037;&#31243;&#20197;&#21450;&#35757;&#32451;&#25968;&#25454;&#30340;&#35782;&#21035;&#21644;&#31574;&#21010;&#12290;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#25216;&#26415;&#39640;&#24230;&#20381;&#36182;&#20110;&#39046;&#22495;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#26377;&#26426;&#20250;&#20351;ER&#26356;&#21152;&#26080;&#32541;&#21644;&#39046;&#22495;&#26080;&#20851;&#12290;&#28982;&#32780;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;LLMs&#21487;&#33021;&#23384;&#22312;&#39118;&#38505;&#65292;&#20854;&#36755;&#20986;&#36136;&#37327;&#21487;&#33021;&#21462;&#20915;&#20110;&#25152;&#35859;&#30340;&#25552;&#31034;&#24037;&#31243;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#20851;&#20110;&#20351;&#29992;&#20687;ChatGPT&#36825;&#26679;&#30340;LLMs&#35299;&#20915;ER&#30340;&#19981;&#21516;&#25552;&#31034;&#26041;&#27861;&#30340;&#24433;&#21709;&#30340;&#31995;&#32479;&#23454;&#39564;&#30740;&#31350;&#36824;&#32570;&#20047;&#12290;&#26412;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#36890;&#36807;&#36827;&#34892;&#36825;&#26679;&#19968;&#39033;&#30740;&#31350;&#12290;&#23613;&#31649;&#36825;&#21482;&#26159;&#21021;&#27493;&#24615;&#36136;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25552;&#31034;&#21487;&#20197;&#26174;&#33879;&#24433;&#21709;&#23454;&#20307;&#35299;&#26512;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity Resolution (ER) is the problem of semi-automatically determining when two entities refer to the same underlying entity, with applications ranging from healthcare to e-commerce. Traditional ER solutions required considerable manual expertise, including feature engineering, as well as identification and curation of training data. In many instances, such techniques are highly dependent on the domain. With recent advent in large language models (LLMs), there is an opportunity to make ER much more seamless and domain-independent. However, it is also well known that LLMs can pose risks, and that the quality of their outputs can depend on so-called prompt engineering. Unfortunately, a systematic experimental study on the effects of different prompting methods for addressing ER, using LLMs like ChatGPT, has been lacking thus far. This paper aims to address this gap by conducting such a study. Although preliminary in nature, our results show that prompting can significantly affect the qu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20869;&#23384;&#19968;&#33268;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#22312;&#27169;&#20223;&#23398;&#20064;&#20013;&#20351;&#29992;&#19987;&#23478;&#28436;&#31034;&#35757;&#32451;&#31574;&#30053;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#23545;&#36755;&#20986;&#32467;&#26524;&#36827;&#34892;&#30828;&#32422;&#26463;&#65292;&#36991;&#20813;&#20102;&#38169;&#35823;&#30340;&#32047;&#31215;&#29616;&#35937;&#65292;&#20445;&#35777;&#20102;&#31574;&#30053;&#25928;&#26524;&#30340;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2310.06171</link><description>&lt;p&gt;
&#20869;&#23384;&#19968;&#33268;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Memory-Consistent Neural Networks for Imitation Learning. (arXiv:2310.06171v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20869;&#23384;&#19968;&#33268;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#22312;&#27169;&#20223;&#23398;&#20064;&#20013;&#20351;&#29992;&#19987;&#23478;&#28436;&#31034;&#35757;&#32451;&#31574;&#30053;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#23545;&#36755;&#20986;&#32467;&#26524;&#36827;&#34892;&#30828;&#32422;&#26463;&#65292;&#36991;&#20813;&#20102;&#38169;&#35823;&#30340;&#32047;&#31215;&#29616;&#35937;&#65292;&#20445;&#35777;&#20102;&#31574;&#30053;&#25928;&#26524;&#30340;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#21033;&#29992;&#19987;&#23478;&#28436;&#31034;&#22823;&#22823;&#31616;&#21270;&#20102;&#31574;&#30053;&#21512;&#25104;&#30340;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36825;&#31181;&#27169;&#20223;&#31574;&#30053;&#26469;&#35828;&#65292;&#36828;&#31163;&#35757;&#32451;&#26679;&#26412;&#30340;&#38169;&#35823;&#23588;&#20026;&#20851;&#38190;&#12290;&#21363;&#20351;&#22312;&#31574;&#30053;&#30340;&#34892;&#21160;&#36755;&#20986;&#20013;&#20986;&#29616;&#32597;&#35265;&#30340;&#38169;&#35823;&#65292;&#30001;&#20110;&#36825;&#20123;&#38169;&#35823;&#20250;&#23548;&#33268;&#19981;&#29087;&#24713;&#30340;&#26410;&#26469;&#29366;&#24577;&#65292;&#31574;&#30053;&#22312;&#36825;&#20123;&#29366;&#24577;&#19979;&#20173;&#26356;&#23481;&#26131;&#20986;&#38169;&#65292;&#26368;&#32456;&#23548;&#33268;&#20219;&#21153;&#22833;&#36133;&#12290;&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#31616;&#21333;&#30340;&#30417;&#30563;&#24335;&#8220;&#34892;&#20026;&#20811;&#38534;&#8221;&#26041;&#27861;&#65292;&#33021;&#22815;&#26041;&#20415;&#22320;&#20165;&#36890;&#36807;&#39044;&#20808;&#35760;&#24405;&#30340;&#28436;&#31034;&#26469;&#35757;&#32451;&#31574;&#30053;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#33021;&#22815;&#25269;&#28040;&#38169;&#35823;&#32047;&#31215;&#29616;&#35937;&#30340;&#27169;&#22411;&#31867;&#12290;&#25105;&#20204;&#30340;&#8220;&#20869;&#23384;&#19968;&#33268;&#31070;&#32463;&#32593;&#32476;&#8221;(MCNN)&#36755;&#20986;&#34987;&#24378;&#21046;&#32422;&#26463;&#22312;&#19982;&#20856;&#22411;&#30340;&#8220;&#20869;&#23384;&#8221;&#35757;&#32451;&#26679;&#26412;&#30456;&#20851;&#30340;&#26126;&#30830;&#25351;&#23450;&#30340;&#20801;&#35768;&#21306;&#22495;&#20869;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;MCNN&#31574;&#30053;&#23548;&#33268;&#30340;&#27425;&#20248;&#24615;&#24046;&#36317;&#30340;&#20445;&#35777;&#19978;&#30028;&#12290;&#36890;&#36807;&#22312;9&#20010;&#27169;&#20223;&#23398;&#20064;&#20219;&#21153;&#19978;&#20351;&#29992;MCNNs&#65292;&#37319;&#29992;MLP&#12289;Transformer&#31561;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation learning considerably simplifies policy synthesis compared to alternative approaches by exploiting access to expert demonstrations. For such imitation policies, errors away from the training samples are particularly critical. Even rare slip-ups in the policy action outputs can compound quickly over time, since they lead to unfamiliar future states where the policy is still more likely to err, eventually causing task failures. We revisit simple supervised ``behavior cloning'' for conveniently training the policy from nothing more than pre-recorded demonstrations, but carefully design the model class to counter the compounding error phenomenon. Our ``memory-consistent neural network'' (MCNN) outputs are hard-constrained to stay within clearly specified permissible regions anchored to prototypical ``memory'' training samples. We provide a guaranteed upper bound for the sub-optimality gap induced by MCNN policies. Using MCNNs on 9 imitation learning tasks, with MLP, Transformer, 
&lt;/p&gt;</description></item><item><title>&#21487;&#39044;&#27979;&#20154;&#24037;&#26234;&#33021;&#26159;&#19968;&#20010;&#26032;&#20852;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#39044;&#27979;&#20154;&#24037;&#26234;&#33021;&#29983;&#24577;&#31995;&#32479;&#30340;&#20851;&#38190;&#25351;&#26631;&#65292;&#24182;&#24378;&#35843;&#21487;&#39044;&#27979;&#24615;&#23545;&#20110;&#25552;&#39640;&#20449;&#20219;&#12289;&#36131;&#20219;&#12289;&#25511;&#21046;&#12289;&#21327;&#35843;&#21644;&#23433;&#20840;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.06167</link><description>&lt;p&gt;
&#21487;&#39044;&#27979;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Predictable Artificial Intelligence. (arXiv:2310.06167v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06167
&lt;/p&gt;
&lt;p&gt;
&#21487;&#39044;&#27979;&#20154;&#24037;&#26234;&#33021;&#26159;&#19968;&#20010;&#26032;&#20852;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#39044;&#27979;&#20154;&#24037;&#26234;&#33021;&#29983;&#24577;&#31995;&#32479;&#30340;&#20851;&#38190;&#25351;&#26631;&#65292;&#24182;&#24378;&#35843;&#21487;&#39044;&#27979;&#24615;&#23545;&#20110;&#25552;&#39640;&#20449;&#20219;&#12289;&#36131;&#20219;&#12289;&#25511;&#21046;&#12289;&#21327;&#35843;&#21644;&#23433;&#20840;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#21487;&#39044;&#27979;&#20154;&#24037;&#26234;&#33021;&#30340;&#22522;&#26412;&#24605;&#24819;&#21644;&#25361;&#25112;&#65292;&#36825;&#26159;&#19968;&#20010;&#25506;&#32034;&#22914;&#20309;&#39044;&#27979;&#29616;&#26377;&#21644;&#26410;&#26469;&#20154;&#24037;&#26234;&#33021;&#29983;&#24577;&#31995;&#32479;&#20851;&#38190;&#25351;&#26631;&#30340;&#26032;&#20852;&#30740;&#31350;&#39046;&#22495;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#23454;&#29616;&#21487;&#39044;&#27979;&#24615;&#23545;&#20110;&#20419;&#36827;&#20154;&#24037;&#26234;&#33021;&#29983;&#24577;&#31995;&#32479;&#30340;&#20449;&#20219;&#12289;&#36131;&#20219;&#12289;&#25511;&#21046;&#12289;&#21327;&#35843;&#21644;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#27492;&#24212;&#20248;&#20808;&#32771;&#34385;&#32780;&#38750;&#24615;&#33021;&#12290;&#23613;&#31649;&#19982;&#20854;&#20182;&#25216;&#26415;&#21644;&#38750;&#25216;&#26415;&#30340;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#39046;&#22495;&#26377;&#25152;&#19981;&#21516;&#65292;&#20294;&#19982;&#21487;&#39044;&#27979;&#20154;&#24037;&#26234;&#33021;&#30456;&#20851;&#30340;&#38382;&#39064;&#12289;&#20551;&#35774;&#21644;&#25361;&#25112;&#23578;&#26410;&#34987;&#28165;&#26970;&#25551;&#36848;&#12290;&#26412;&#25991;&#26088;&#22312;&#38416;&#26126;&#36825;&#20123;&#38382;&#39064;&#65292;&#21628;&#21505;&#25214;&#21040;&#36890;&#21521;&#20154;&#24037;&#26234;&#33021;&#21487;&#39044;&#27979;&#24615;&#30340;&#36335;&#24452;&#65292;&#24182;&#27010;&#36848;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the fundamental ideas and challenges of Predictable AI, a nascent research area that explores the ways in which we can anticipate key indicators of present and future AI ecosystems. We argue that achieving predictability is crucial for fostering trust, liability, control, alignment and safety of AI ecosystems, and thus should be prioritised over performance. While distinctive from other areas of technical and non-technical AI research, the questions, hypotheses and challenges relevant to Predictable AI were yet to be clearly described. This paper aims to elucidate them, calls for identifying paths towards AI predictability and outlines the potential impact of this emergent field.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20851;&#32852;&#35789;&#24863;&#30693;&#30340;&#35789;&#32423;&#20849;&#25351;&#28040;&#35299;&#27169;&#22411;&#65288;CAW-coref&#65289;&#65292;&#22312;&#22788;&#29702;&#24182;&#21015;&#25552;&#21450;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#20102;&#36739;&#39640;&#30340;&#24615;&#33021;&#65292;&#26377;&#25928;&#22320;&#32553;&#23567;&#20102;&#19982;&#26114;&#36149;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2310.06165</link><description>&lt;p&gt;
CAW-coref: &#20851;&#32852;&#35789;&#24863;&#30693;&#30340;&#35789;&#32423;&#20849;&#25351;&#28040;&#35299;
&lt;/p&gt;
&lt;p&gt;
CAW-coref: Conjunction-Aware Word-level Coreference Resolution. (arXiv:2310.06165v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20851;&#32852;&#35789;&#24863;&#30693;&#30340;&#35789;&#32423;&#20849;&#25351;&#28040;&#35299;&#27169;&#22411;&#65288;CAW-coref&#65289;&#65292;&#22312;&#22788;&#29702;&#24182;&#21015;&#25552;&#21450;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#20102;&#36739;&#39640;&#30340;&#24615;&#33021;&#65292;&#26377;&#25928;&#22320;&#32553;&#23567;&#20102;&#19982;&#26114;&#36149;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#20849;&#25351;&#28040;&#35299;&#31995;&#32479;&#27599;&#31687;&#25991;&#31456;&#38656;&#35201;&#22810;&#27425;&#35843;&#29992;&#35821;&#35328;&#27169;&#22411;&#65292;&#22240;&#27492;&#23545;&#20110;&#35768;&#22810;&#24212;&#29992;&#22330;&#26223;&#26469;&#35828;&#65288;&#20363;&#22914;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#36827;&#34892;&#20449;&#24687;&#25552;&#21462;&#65289;&#65292;&#20195;&#20215;&#22826;&#39640;&#12290;&#32780;&#35789;&#32423;&#20849;&#25351;&#31995;&#32479; (WL-coref) &#22312;&#25928;&#29575;&#19978;&#26356;&#21152;&#39640;&#25928;&#65292;&#23454;&#29616;&#20102;&#36825;&#20123;&#26368;&#20808;&#36827;&#31995;&#32479; 96.6% &#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#21457;&#29616;&#20102; WL-coref &#30340;&#19968;&#20010;&#24120;&#35265;&#20294;&#37325;&#35201;&#30340;&#22833;&#36133;&#26696;&#20363;&#65306;&#22788;&#29702;&#8220;Tom &#21644; Mary&#8221;&#20043;&#31867;&#30340;&#24182;&#21015;&#25552;&#21450;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22312; OntoNotes &#27979;&#35797;&#38598;&#19978;&#23558;&#24615;&#33021;&#25552;&#39640;&#20102; 0.9% F1&#65292;&#23558;&#39640;&#25928;&#30340;&#35789;&#32423;&#20849;&#25351;&#28040;&#35299;&#19982;&#26114;&#36149;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#24046;&#36317;&#32553;&#23567;&#20102;34.6%&#12290;&#25105;&#20204;&#30340;&#20851;&#32852;&#35789;&#24863;&#30693;&#30340;&#35789;&#32423;&#20849;&#25351;&#27169;&#22411;&#65288;CAW-coref&#65289;&#21644;&#20195;&#30721;&#21487;&#22312; https://github.com/KarelDO/wl-coref &#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art coreference resolutions systems depend on multiple LLM calls per document and are thus prohibitively expensive for many use cases (e.g., information extraction with large corpora). The leading word-level coreference system (WL-coref) attains 96.6% of these SOTA systems' performance while being much more efficient. In this work, we identify a routine yet important failure case of WL-coref: dealing with conjoined mentions such as 'Tom and Mary'. We offer a simple yet effective solution that improves the performance on the OntoNotes test set by 0.9% F1, shrinking the gap between efficient word-level coreference resolution and expensive SOTA approaches by 34.6%. Our Conjunction-Aware Word-level coreference model (CAW-coref) and code is available at https://github.com/KarelDO/wl-coref.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;LLM&#26102;&#20195;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#22312;RLHF&#12289;Prompting&#31561;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#20013;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2310.06147</link><description>&lt;p&gt;
&#22312;LLM&#26102;&#20195;&#30340;&#24378;&#21270;&#23398;&#20064;&#65306;&#20160;&#20040;&#26159;&#24517;&#35201;&#30340;&#65311;&#20160;&#20040;&#26159;&#38656;&#35201;&#30340;&#65311;&#24378;&#21270;&#23398;&#20064;&#23545;RLHF&#12289;Prompting&#31561;&#30340;&#35270;&#35282;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning in the Era of LLMs: What is Essential? What is needed? An RL Perspective on RLHF, Prompting, and Beyond. (arXiv:2310.06147v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06147
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;LLM&#26102;&#20195;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#22312;RLHF&#12289;Prompting&#31561;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#20013;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#24182;&#21462;&#24471;&#20102;ChatGPT&#21644;GPT-4&#31561;&#25104;&#21151;&#20135;&#21697;&#12290;&#23427;&#20204;&#22312;&#36981;&#24490;&#25351;&#20196;&#24182;&#25552;&#20379;&#26080;&#23475;&#12289;&#26377;&#24110;&#21161;&#21644;&#35802;&#23454;&#65288;3H&#65289;&#22238;&#31572;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#20027;&#35201;&#24402;&#21151;&#20110;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#25216;&#26415;&#12290;&#26412;&#25991;&#26088;&#22312;&#23558;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#30340;&#30740;&#31350;&#19982;LLM&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;RL&#25216;&#26415;&#32852;&#31995;&#36215;&#26469;&#65292;&#36890;&#36807;&#35752;&#35770;RL&#22312;&#20309;&#26102;&#12289;&#20309;&#22320;&#21644;&#22914;&#20309;&#20248;&#31168;&#65292;&#35299;&#37322;&#36825;&#19968;&#25216;&#26415;&#30340;&#31070;&#31192;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#28508;&#22312;&#30340;&#26410;&#26469;&#39046;&#22495;&#65292;&#36825;&#20123;&#39046;&#22495;&#21487;&#33021;&#20250;&#20174;RLHF&#30740;&#31350;&#20013;&#33719;&#30410;&#25110;&#20026;&#20854;&#20570;&#20986;&#36129;&#29486;&#12290;&#37325;&#28857;&#20869;&#23481;&#65306;1. RLHF&#26159;&#24102;&#26377;&#31163;&#32447;&#31034;&#33539;&#25968;&#25454;&#30340;&#22312;&#32447;&#36870;&#21521;RL&#12290;2. RLHF&#27604;SFT&#26356;&#22909;&#65292;&#22240;&#20026;&#27169;&#20223;&#23398;&#20064;&#65288;&#21644;&#36870;&#21521;RL&#65289;&#27604;&#34892;&#20026;&#20811;&#38534;&#65288;BC&#65289;&#26356;&#22909;&#65292;&#33021;&#22815;&#32531;&#35299;&#32047;&#31215;&#35823;&#24046;&#38382;&#39064;&#12290;3. RLHF&#20013;&#30340;RM&#27493;&#39588;&#20135;&#29983;&#20102;&#26114;&#36149;&#30340;&#20154;&#31867;&#21453;&#39304;&#30340;&#20195;&#29702;&#65292;&#36825;&#26679;&#30340;&#35265;&#35299;&#21487;&#20197;&#25512;&#24191;&#21040;&#20854;&#20182;LLM&#20219;&#21153;&#65292;&#20363;&#22914;&#25552;&#31034;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Large Language Models (LLMs) have garnered wide attention and led to successful products such as ChatGPT and GPT-4. Their proficiency in adhering to instructions and delivering harmless, helpful, and honest (3H) responses can largely be attributed to the technique of Reinforcement Learning from Human Feedback (RLHF). In this paper, we aim to link the research in conventional RL to RL techniques used in LLM research. Demystify this technique by discussing why, when, and how RL excels. Furthermore, we explore potential future avenues that could either benefit from or contribute to RLHF research.  Highlighted Takeaways:  1. RLHF is Online Inverse RL with Offline Demonstration Data.  2. RLHF $&gt;$ SFT because Imitation Learning (and Inverse RL) $&gt;$ Behavior Cloning (BC) by alleviating the problem of compounding error.  3. The RM step in RLHF generates a proxy of the expensive human feedback, such an insight can be generalized to other LLM tasks such as prompting evalua
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;LTrajDiff&#30340;&#26032;&#26041;&#27861;&#65292;&#20174;&#22122;&#22768;&#31227;&#21160;&#25968;&#25454;&#20013;&#39044;&#27979;&#31934;&#30830;&#30340;&#24067;&#23616;&#24207;&#21015;&#65292;&#20811;&#26381;&#20102;&#30001;&#22024;&#26434;&#25968;&#25454;&#12289;&#19981;&#23436;&#25972;&#36712;&#36857;&#21644;&#29615;&#22659;&#22240;&#32032;&#23548;&#33268;&#30340;&#35270;&#35273;&#38556;&#30861;&#12290;</title><link>http://arxiv.org/abs/2310.06138</link><description>&lt;p&gt;
&#20174;&#22024;&#26434;&#30340;&#31227;&#21160;&#27169;&#24577;&#20013;&#39044;&#27979;&#24067;&#23616;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Layout Sequence Prediction From Noisy Mobile Modality. (arXiv:2310.06138v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06138
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;LTrajDiff&#30340;&#26032;&#26041;&#27861;&#65292;&#20174;&#22122;&#22768;&#31227;&#21160;&#25968;&#25454;&#20013;&#39044;&#27979;&#31934;&#30830;&#30340;&#24067;&#23616;&#24207;&#21015;&#65292;&#20811;&#26381;&#20102;&#30001;&#22024;&#26434;&#25968;&#25454;&#12289;&#19981;&#23436;&#25972;&#36712;&#36857;&#21644;&#29615;&#22659;&#22240;&#32032;&#23548;&#33268;&#30340;&#35270;&#35273;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36712;&#36857;&#39044;&#27979;&#22312;&#29702;&#35299;&#34892;&#20154;&#31227;&#21160;&#26041;&#38754;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#65292;&#36866;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#21644;&#26426;&#22120;&#20154;&#31561;&#24212;&#29992;&#12290;&#24403;&#21069;&#30340;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#20381;&#36182;&#20110;&#35270;&#35273;&#27169;&#24577;&#30340;&#38271;&#12289;&#23436;&#25972;&#19988;&#20934;&#30830;&#35266;&#27979;&#21040;&#30340;&#24207;&#21015;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#32463;&#24120;&#20986;&#29616;&#30456;&#26426;&#36974;&#25377;&#12289;&#36951;&#28431;&#23545;&#35937;&#25110;&#30001;&#20110;&#29615;&#22659;&#22240;&#32032;&#23548;&#33268;&#23545;&#35937;&#19981;&#22312;&#35270;&#32447;&#33539;&#22260;&#20869;&#30340;&#24773;&#20917;&#65292;&#23548;&#33268;&#36712;&#36857;&#19981;&#23436;&#25972;&#25110;&#22122;&#22768;&#36739;&#22823;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;LTrajDiff&#65292;&#23558;&#34987;&#36974;&#25377;&#25110;&#19981;&#22312;&#35270;&#32447;&#33539;&#22260;&#20869;&#30340;&#29289;&#20307;&#19982;&#23436;&#20840;&#21487;&#35265;&#36712;&#36857;&#30340;&#29289;&#20307;&#21516;&#31561;&#37325;&#35201;&#12290;LTrajDiff&#21033;&#29992;&#31227;&#21160;&#25163;&#26426;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#20811;&#26381;&#19981;&#22312;&#35270;&#32447;&#33539;&#22260;&#20869;&#30340;&#32422;&#26463;&#65292;&#20294;&#24341;&#20837;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#22914;&#27169;&#24577;&#34701;&#21512;&#12289;&#22122;&#22768;&#25968;&#25454;&#21644;&#32570;&#20047;&#31354;&#38388;&#24067;&#23616;&#21644;&#29289;&#20307;&#23610;&#23544;&#20449;&#24687;&#12290;&#25105;&#20204;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#37319;&#29992;&#20174;&#31895;&#21040;&#31934;&#30340;&#25193;&#25955;&#31574;&#30053;&#65292;&#20174;&#22024;&#26434;&#30340;&#31227;&#21160;&#25968;&#25454;&#20013;&#39044;&#27979;&#31934;&#30830;&#30340;&#24067;&#23616;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trajectory prediction plays a vital role in understanding pedestrian movement for applications such as autonomous driving and robotics. Current trajectory prediction models depend on long, complete, and accurately observed sequences from visual modalities. Nevertheless, real-world situations often involve obstructed cameras, missed objects, or objects out of sight due to environmental factors, leading to incomplete or noisy trajectories. To overcome these limitations, we propose LTrajDiff, a novel approach that treats objects obstructed or out of sight as equally important as those with fully visible trajectories. LTrajDiff utilizes sensor data from mobile phones to surmount out-of-sight constraints, albeit introducing new challenges such as modality fusion, noisy data, and the absence of spatial layout and object size information. We employ a denoising diffusion model to predict precise layout sequences from noisy mobile data using a coarse-to-fine diffusion strategy, incorporating th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26799;&#24230;&#33258;&#21160;&#23398;&#20064;&#23618;&#38388;&#31561;&#21464;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;&#36719;&#31561;&#21464;&#24615;&#30340;&#21442;&#25968;&#21270;&#21644;&#20248;&#21270;&#36793;&#32536;&#20284;&#28982;&#26469;&#23454;&#29616;&#23618;&#38388;&#23545;&#31216;&#24615;&#30340;&#33258;&#21160;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2310.06131</link><description>&lt;p&gt;
&#20351;&#29992;&#26799;&#24230;&#33258;&#21160;&#23398;&#20064;&#23618;&#38388;&#31561;&#21464;&#24615;
&lt;/p&gt;
&lt;p&gt;
Learning Layer-wise Equivariances Automatically using Gradients. (arXiv:2310.06131v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06131
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26799;&#24230;&#33258;&#21160;&#23398;&#20064;&#23618;&#38388;&#31561;&#21464;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;&#36719;&#31561;&#21464;&#24615;&#30340;&#21442;&#25968;&#21270;&#21644;&#20248;&#21270;&#36793;&#32536;&#20284;&#28982;&#26469;&#23454;&#29616;&#23618;&#38388;&#23545;&#31216;&#24615;&#30340;&#33258;&#21160;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#23558;&#31561;&#21464;&#24615;&#23545;&#31216;&#24615;&#32534;&#30721;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#31216;&#24615;&#25552;&#20379;&#20102;&#32593;&#32476;&#21487;&#20197;&#34920;&#31034;&#30340;&#20989;&#25968;&#30340;&#22266;&#23450;&#30828;&#32422;&#26463;&#65292;&#38656;&#35201;&#20107;&#20808;&#25351;&#23450;&#65292;&#24182;&#19988;&#19981;&#33021;&#36866;&#24212;&#25913;&#21464;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20801;&#35768;&#28789;&#27963;&#30340;&#23545;&#31216;&#24615;&#32422;&#26463;&#65292;&#21487;&#20197;&#36890;&#36807;&#26799;&#24230;&#33258;&#21160;&#22320;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#23545;&#31216;&#24615;&#21644;&#30456;&#20851;&#30340;&#26435;&#37325;&#36830;&#25509;&#32467;&#26500;&#26377;&#20004;&#20010;&#22256;&#38590;&#12290;&#39318;&#20808;&#65292;&#23427;&#38656;&#35201;&#26377;&#25928;&#28789;&#27963;&#30340;&#23618;&#38388;&#31561;&#21464;&#24615;&#21442;&#25968;&#21270;&#12290;&#20854;&#27425;&#65292;&#23545;&#31216;&#24615;&#20316;&#20026;&#32422;&#26463;&#65292;&#22240;&#27492;&#19981;&#20250;&#34987;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#40723;&#21169;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#36719;&#31561;&#21464;&#24615;&#30340;&#21442;&#25968;&#21270;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#36793;&#32536;&#20284;&#28982;&#26469;&#23398;&#20064;&#23618;&#38388;&#31561;&#21464;&#24615;&#30340;&#25968;&#37327;&#65292;&#20854;&#20013;&#36793;&#32536;&#20284;&#28982;&#26159;&#20351;&#29992;&#21487;&#24494;&#20998;&#30340;&#25289;&#26222;&#25289;&#26031;&#36924;&#36817;&#20272;&#35745;&#30340;&#12290;&#36825;&#20010;&#30446;&#26631;&#24179;&#34913;&#20102;&#25968;&#25454;&#25311;&#21512;&#21644;&#27169;&#22411;&#22797;&#26434;&#24615;&#65292;&#20351;&#23618;&#38388;&#23545;&#31216;&#24615;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#34987;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutions encode equivariance symmetries into neural networks leading to better generalisation performance. However, symmetries provide fixed hard constraints on the functions a network can represent, need to be specified in advance, and can not be adapted. Our goal is to allow flexible symmetry constraints that can automatically be learned from data using gradients. Learning symmetry and associated weight connectivity structures from scratch is difficult for two reasons. First, it requires efficient and flexible parameterisations of layer-wise equivariances. Secondly, symmetries act as constraints and are therefore not encouraged by training losses measuring data fit. To overcome these challenges, we improve parameterisations of soft equivariance and learn the amount of equivariance in layers by optimising the marginal likelihood, estimated using differentiable Laplace approximations. The objective balances data fit and model complexity enabling layer-wise symmetry discovery in dee
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#22495;Conformer&#27169;&#22411;&#30340;&#21333;&#22768;&#36947;&#35821;&#38899;&#20998;&#31163;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#24050;&#26377;&#27169;&#22411;&#65292;&#22312;&#22024;&#26434;&#30340;&#28151;&#21709;&#22768;&#23398;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20998;&#31163;&#25928;&#26524;&#21644;&#26356;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.06125</link><description>&lt;p&gt;
&#22312;&#22024;&#26434;&#30340;&#28151;&#21709;&#22768;&#23398;&#29615;&#22659;&#20013;&#65292;&#20851;&#20110;&#21333;&#22768;&#36947;&#35821;&#38899;&#20998;&#31163;&#30340;&#26102;&#22495;Conformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
On Time Domain Conformer Models for Monaural Speech Separation in Noisy Reverberant Acoustic Environments. (arXiv:2310.06125v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06125
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#22495;Conformer&#27169;&#22411;&#30340;&#21333;&#22768;&#36947;&#35821;&#38899;&#20998;&#31163;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#24050;&#26377;&#27169;&#22411;&#65292;&#22312;&#22024;&#26434;&#30340;&#28151;&#21709;&#22768;&#23398;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20998;&#31163;&#25928;&#26524;&#21644;&#26356;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#20998;&#31163;&#23545;&#22810;&#35828;&#35805;&#32773;&#25216;&#26415;&#30740;&#31350;&#20154;&#21592;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#20027;&#39064;&#12290;&#21367;&#31215;&#22686;&#24378;&#36716;&#25442;&#22120;&#65288;conformer&#65289;&#22312;&#35768;&#22810;&#35821;&#38899;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#35821;&#38899;&#20998;&#31163;&#26041;&#38754;&#24471;&#21040;&#30340;&#30740;&#31350;&#36739;&#23569;&#12290;&#26368;&#36817;&#30340;&#26368;&#20808;&#36827;&#65288;SOTA&#65289;&#20998;&#31163;&#27169;&#22411;&#22823;&#22810;&#25968;&#26159;&#26102;&#22495;&#38899;&#39057;&#20998;&#31163;&#32593;&#32476;&#65288;TasNets&#65289;&#12290;&#19968;&#20123;&#25104;&#21151;&#30340;&#27169;&#22411;&#21033;&#29992;&#20102;&#21452;&#36890;&#36947;&#65288;DP&#65289;&#32593;&#32476;&#65292;&#23427;&#20204;&#25353;&#39034;&#24207;&#22788;&#29702;&#23616;&#37096;&#21644;&#20840;&#23616;&#20449;&#24687;&#12290;&#26102;&#22495;Conformer&#65288;TD-Conformers&#65289;&#26159;DP&#26041;&#27861;&#30340;&#19968;&#20010;&#31867;&#27604;&#65292;&#23427;&#20204;&#20063;&#25353;&#39034;&#24207;&#22788;&#29702;&#23616;&#37096;&#21644;&#20840;&#23616;&#19978;&#19979;&#25991;&#65292;&#20294;&#20855;&#26377;&#19981;&#21516;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#20989;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#23454;&#38469;&#36739;&#30701;&#30340;&#20449;&#21495;&#38271;&#24230;&#19979;&#65292;&#30456;&#23545;&#20110;&#29305;&#24449;&#32500;&#24230;&#65292;Conformer&#22312;&#25511;&#21046;&#29305;&#24449;&#32500;&#24230;&#26102;&#26356;&#39640;&#25928;&#12290;&#25552;&#20986;&#20102;&#23376;&#37319;&#26679;&#23618;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;&#26368;&#20339;&#30340;TD-Conformer&#22312;WHAMR&#21644;WSJ0-2Mix&#22522;&#20934;&#19978;&#20998;&#21035;&#23454;&#29616;&#20102;14.6dB&#21644;21.2dB&#30340;SISDR&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech separation remains an important topic for multi-speaker technology researchers. Convolution augmented transformers (conformers) have performed well for many speech processing tasks but have been under-researched for speech separation. Most recent state-of-the-art (SOTA) separation models have been time-domain audio separation networks (TasNets). A number of successful models have made use of dual-path (DP) networks which sequentially process local and global information. Time domain conformers (TD-Conformers) are an analogue of the DP approach in that they also process local and global context sequentially but have a different time complexity function. It is shown that for realistic shorter signal lengths, conformers are more efficient when controlling for feature dimension. Subsampling layers are proposed to further improve computational efficiency. The best TD-Conformer achieves 14.6 dB and 21.2 dB SISDR improvement on the WHAMR and WSJ0-2Mix benchmarks, respectively.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#25991;&#26412;&#39537;&#21160;&#30340;&#25552;&#31034;&#29983;&#25104;&#26041;&#27861;&#65288;FedTPG&#65289;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#36828;&#31243;&#23458;&#25143;&#31471;&#19978;&#23398;&#20064;&#32479;&#19968;&#30340;&#25552;&#31034;&#29983;&#25104;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#24050;&#30693;&#21644;&#26410;&#30693;&#31867;&#21035;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#20248;&#20110;&#29616;&#26377;&#30340;&#32852;&#37030;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.06123</link><description>&lt;p&gt;
&#25991;&#26412;&#39537;&#21160;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#25552;&#31034;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Text-driven Prompt Generation for Vision-Language Models in Federated Learning. (arXiv:2310.06123v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06123
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#25991;&#26412;&#39537;&#21160;&#30340;&#25552;&#31034;&#29983;&#25104;&#26041;&#27861;&#65288;FedTPG&#65289;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#36828;&#31243;&#23458;&#25143;&#31471;&#19978;&#23398;&#20064;&#32479;&#19968;&#30340;&#25552;&#31034;&#29983;&#25104;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#24050;&#30693;&#21644;&#26410;&#30693;&#31867;&#21035;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#20248;&#20110;&#29616;&#26377;&#30340;&#32852;&#37030;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#23398;&#20064;&#65292;&#20363;&#22914;CoOp&#65292;&#22312;&#36866;&#24212;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#36825;&#20351;&#24471;&#23427;&#25104;&#20026;&#32852;&#37030;&#23398;&#20064;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#32771;&#34385;&#21040;&#35745;&#31639;&#26041;&#38754;&#30340;&#21407;&#22240;&#12290;&#29616;&#26377;&#30340;&#25552;&#31034;&#23398;&#20064;&#25216;&#26415;&#29992;&#23398;&#20064;&#30340;&#21521;&#37327;&#26367;&#25442;&#25163;&#24037;&#21046;&#20316;&#30340;&#25991;&#26412;&#25552;&#31034;&#65292;&#22312;&#24050;&#30693;&#31867;&#21035;&#19978;&#21462;&#24471;&#20102;&#25913;&#36827;&#65292;&#20294;&#22312;&#26410;&#30693;&#31867;&#21035;&#19978;&#38590;&#20197;&#27867;&#21270;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36890;&#36807;&#25552;&#20986;&#32852;&#37030;&#25991;&#26412;&#39537;&#21160;&#30340;&#25552;&#31034;&#29983;&#25104;&#65288;FedTPG&#65289;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20197;&#21487;&#25193;&#23637;&#30340;&#26041;&#24335;&#22312;&#22810;&#20010;&#36828;&#31243;&#23458;&#25143;&#31471;&#19978;&#23398;&#20064;&#32479;&#19968;&#30340;&#25552;&#31034;&#29983;&#25104;&#32593;&#32476;&#12290;&#25552;&#31034;&#29983;&#25104;&#32593;&#32476;&#20197;&#20219;&#21153;&#30456;&#20851;&#30340;&#25991;&#26412;&#36755;&#20837;&#20026;&#26465;&#20214;&#65292;&#22240;&#27492;&#20855;&#26377;&#19978;&#19979;&#25991;&#24863;&#30693;&#24615;&#65292;&#36866;&#21512;&#27867;&#21270;&#21040;&#24050;&#30693;&#21644;&#26410;&#30693;&#31867;&#21035;&#12290;&#25105;&#20204;&#23545;&#20061;&#20010;&#19981;&#21516;&#30340;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#32852;&#37030;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#23545;&#24050;&#30693;&#21644;&#26410;&#30693;&#31867;&#21035;&#30340;&#24635;&#20307;&#27867;&#21270;&#33021;&#21147;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt learning for vision-language models, e.g., CoOp, has shown great success in adapting CLIP to different downstream tasks, making it a promising solution for federated learning due to computational reasons. Existing prompt learning techniques replace hand-crafted text prompts with learned vectors that offer improvements on seen classes, but struggle to generalize to unseen classes. Our work addresses this challenge by proposing Federated Text-driven Prompt Generation (FedTPG), which learns a unified prompt generation network across multiple remote clients in a scalable manner. The prompt generation network is conditioned on task-related text input, thus is context-aware, making it suitable to generalize for both seen and unseen classes. Our comprehensive empirical evaluations on nine diverse image classification datasets show that our method is superior to existing federated prompt learning methods, that achieve overall better generalization on both seen and unseen classes and is 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#26088;&#22312;&#35299;&#20915;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#39046;&#22495;&#20013;&#20844;&#24179;&#22522;&#20934;&#27979;&#35797;&#21644;&#25216;&#26415;&#26041;&#27861;&#36873;&#25321;&#30340;&#20105;&#35758;&#65292;&#24182;&#25552;&#20379;&#23545;&#35813;&#39046;&#22495;&#21462;&#24471;&#30340;&#36827;&#23637;&#30340;&#28145;&#20837;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2310.06119</link><description>&lt;p&gt;
&#25506;&#32034;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#36827;&#23637;&#65306;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#21644;&#24322;&#36136;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Exploring Progress in Multivariate Time Series Forecasting: Comprehensive Benchmarking and Heterogeneity Analysis. (arXiv:2310.06119v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06119
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26088;&#22312;&#35299;&#20915;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#39046;&#22495;&#20013;&#20844;&#24179;&#22522;&#20934;&#27979;&#35797;&#21644;&#25216;&#26415;&#26041;&#27861;&#36873;&#25321;&#30340;&#20105;&#35758;&#65292;&#24182;&#25552;&#20379;&#23545;&#35813;&#39046;&#22495;&#21462;&#24471;&#30340;&#36827;&#23637;&#30340;&#28145;&#20837;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#24191;&#27867;&#23384;&#22312;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#22797;&#26434;&#31995;&#32479;&#20013;&#65292;&#22914;&#20132;&#36890;&#21644;&#33021;&#28304;&#31995;&#32479;&#65292;&#23545;&#20110;&#29702;&#35299;&#21644;&#24433;&#21709;&#36825;&#20123;&#31995;&#32479;&#65292;&#23427;&#20204;&#30340;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;MTS&#20013;&#26377;&#25928;&#22320;&#24314;&#27169;&#26102;&#38388;&#21644;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#26041;&#38754;&#33719;&#24471;&#20102;&#24456;&#22823;&#30340;&#27969;&#34892;&#65292;&#29305;&#21035;&#26159;&#22312;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;LTSF&#65289;&#21644;&#26102;&#31354;&#39044;&#27979;&#65288;STF&#65289;&#20013;&#12290;&#28982;&#32780;&#65292;&#20844;&#24179;&#30340;&#22522;&#20934;&#27979;&#35797;&#38382;&#39064;&#21644;&#25216;&#26415;&#26041;&#27861;&#30340;&#36873;&#25321;&#22312;&#30456;&#20851;&#24037;&#20316;&#20013;&#19968;&#30452;&#23384;&#22312;&#20105;&#35758;&#12290;&#36825;&#20123;&#20105;&#35758;&#26174;&#33879;&#38459;&#30861;&#20102;&#25105;&#20204;&#23545;&#35813;&#39046;&#22495;&#36827;&#23637;&#30340;&#29702;&#35299;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#20105;&#35758;&#65292;&#20197;&#25552;&#20379;&#23545;&#21462;&#24471;&#30340;&#36827;&#23637;&#30340;&#28145;&#20837;&#27934;&#23519;&#12290;&#20026;&#20102;&#35299;&#20915;&#22522;&#20934;&#27979;&#35797;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;BasicTS&#65292;&#19968;&#20010;&#26088;&#22312;&#20844;&#24179;&#27604;&#36739;MTS&#39044;&#27979;&#30340;&#22522;&#20934;&#12290;BasicTS&#24314;&#31435;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35757;&#32451;&#27969;&#31243;&#21644;&#21512;&#29702;&#30340;&#35780;&#20272;&#35774;&#32622;&#65292;&#33021;&#22815;&#23545;30&#22810;&#31181;&#27969;&#34892;&#30340;MTS&#39044;&#27979;&#27169;&#22411;&#36827;&#34892;&#20844;&#27491;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate Time Series (MTS) widely exists in real-word complex systems, such as traffic and energy systems, making their forecasting crucial for understanding and influencing these systems. Recently, deep learning-based approaches have gained much popularity for effectively modeling temporal and spatial dependencies in MTS, specifically in Long-term Time Series Forecasting (LTSF) and Spatial-Temporal Forecasting (STF). However, the fair benchmarking issue and the choice of technical approaches have been hotly debated in related work. Such controversies significantly hinder our understanding of progress in this field. Thus, this paper aims to address these controversies to present insights into advancements achieved. To resolve benchmarking issues, we introduce BasicTS, a benchmark designed for fair comparisons in MTS forecasting. BasicTS establishes a unified training pipeline and reasonable evaluation settings, enabling an unbiased evaluation of over 30 popular MTS forecasting mode
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25552;&#31034;&#25216;&#26415;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#25277;&#35937;&#33719;&#24471;&#39640;&#23618;&#27010;&#24565;&#21644;&#22522;&#26412;&#21407;&#29702;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#25512;&#29702;&#36335;&#24452;&#20013;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#22312;&#21508;&#31181;&#25512;&#29702;&#23494;&#38598;&#22411;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.06117</link><description>&lt;p&gt;
&#36864;&#21518;&#19968;&#27493;&#65306;&#36890;&#36807;&#25277;&#35937;&#21796;&#36215;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models. (arXiv:2310.06117v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06117
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25552;&#31034;&#25216;&#26415;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#25277;&#35937;&#33719;&#24471;&#39640;&#23618;&#27010;&#24565;&#21644;&#22522;&#26412;&#21407;&#29702;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#25512;&#29702;&#36335;&#24452;&#20013;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#22312;&#21508;&#31181;&#25512;&#29702;&#23494;&#38598;&#22411;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#36864;&#21518;&#25552;&#31034;&#8221;&#30340;&#31616;&#21333;&#25552;&#31034;&#25216;&#26415;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#20174;&#21253;&#21547;&#20855;&#20307;&#32454;&#33410;&#30340;&#23454;&#20363;&#20013;&#36827;&#34892;&#25277;&#35937;&#65292;&#24471;&#20986;&#39640;&#23618;&#27010;&#24565;&#21644;&#22522;&#26412;&#21407;&#29702;&#12290;&#21033;&#29992;&#36825;&#20123;&#27010;&#24565;&#21644;&#21407;&#29702;&#26469;&#25351;&#23548;&#25512;&#29702;&#27493;&#39588;&#65292;&#35821;&#35328;&#27169;&#22411;&#22312;&#27491;&#30830;&#25512;&#29702;&#36335;&#24452;&#19978;&#26174;&#33879;&#25552;&#21319;&#20102;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;PaLM-2L&#27169;&#22411;&#36827;&#34892;&#20102;&#36864;&#21518;&#25552;&#31034;&#23454;&#39564;&#65292;&#22312;&#21253;&#25324;STEM&#12289;&#30693;&#35782;&#38382;&#31572;&#21644;&#22810;&#36339;&#25512;&#29702;&#22312;&#20869;&#30340;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25512;&#29702;&#23494;&#38598;&#22411;&#20219;&#21153;&#19978;&#35266;&#23519;&#21040;&#20102;&#26126;&#26174;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#20363;&#22914;&#65292;&#22312;MMLU&#29289;&#29702;&#21644;&#21270;&#23398;&#20219;&#21153;&#19978;&#65292;&#36864;&#21518;&#25552;&#31034;&#21487;&#20197;&#23558;PaLM-2L&#30340;&#24615;&#33021;&#25552;&#21319;7%&#21644;11%&#65292;&#22312;TimeQA&#20219;&#21153;&#19978;&#25552;&#21319;27%&#65292;&#22312;MuSiQue&#20219;&#21153;&#19978;&#25552;&#21319;7%&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Step-Back Prompting, a simple prompting technique that enables LLMs to do abstractions to derive high-level concepts and first principles from instances containing specific details. Using the concepts and principles to guide the reasoning steps, LLMs significantly improve their abilities in following a correct reasoning path towards the solution. We conduct experiments of Step-Back Prompting with PaLM-2L models and observe substantial performance gains on a wide range of challenging reasoning-intensive tasks including STEM, Knowledge QA, and Multi-Hop Reasoning. For instance, Step-Back Prompting improves PaLM-2L performance on MMLU Physics and Chemistry by 7% and 11%, TimeQA by 27%, and MuSiQue by 7%.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;OptiMUS&#65292;&#19968;&#31181;&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#24314;&#27169;&#20195;&#29702;&#65292;&#29992;&#20110;&#35299;&#20915;MILP&#38382;&#39064;&#12290;&#35813;&#20195;&#29702;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#25968;&#23398;&#27169;&#22411;&#12289;&#32534;&#20889;&#21644;&#35843;&#35797;&#27714;&#35299;&#22120;&#20195;&#30721;&#65292;&#24182;&#20855;&#26377;&#36739;&#39640;&#30340;&#35299;&#20915;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.06116</link><description>&lt;p&gt;
OptiMUS: &#20351;&#29992;mip&#27714;&#35299;&#22120;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
OptiMUS: Optimization Modeling Using mip Solvers and large language models. (arXiv:2310.06116v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06116
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;OptiMUS&#65292;&#19968;&#31181;&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#24314;&#27169;&#20195;&#29702;&#65292;&#29992;&#20110;&#35299;&#20915;MILP&#38382;&#39064;&#12290;&#35813;&#20195;&#29702;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#25968;&#23398;&#27169;&#22411;&#12289;&#32534;&#20889;&#21644;&#35843;&#35797;&#27714;&#35299;&#22120;&#20195;&#30721;&#65292;&#24182;&#20855;&#26377;&#36739;&#39640;&#30340;&#35299;&#20915;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#38382;&#39064;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#20174;&#21046;&#36896;&#21644;&#20998;&#38144;&#21040;&#21307;&#30103;&#20445;&#20581;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#31867;&#38382;&#39064;&#20173;&#28982;&#26159;&#36890;&#36807;&#25163;&#24037;&#21551;&#21457;&#24335;&#35299;&#20915;&#32780;&#19981;&#26159;&#36890;&#36807;&#26368;&#20808;&#36827;&#30340;&#27714;&#35299;&#22120;&#36827;&#34892;&#26368;&#20248;&#35299;&#65292;&#22240;&#20026;&#21046;&#23450;&#21644;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#25152;&#38656;&#30340;&#19987;&#19994;&#30693;&#35782;&#38480;&#21046;&#20102;&#20248;&#21270;&#24037;&#20855;&#21644;&#25216;&#26415;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;OptiMUS&#65292;&#19968;&#31181;&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#20195;&#29702;&#65292;&#26088;&#22312;&#20174;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20013;&#21046;&#23450;&#21644;&#35299;&#20915;MILP&#38382;&#39064;&#12290;OptiMUS&#33021;&#22815;&#24320;&#21457;&#25968;&#23398;&#27169;&#22411;&#65292;&#32534;&#20889;&#21644;&#35843;&#35797;&#27714;&#35299;&#22120;&#20195;&#30721;&#65292;&#24320;&#21457;&#27979;&#35797;&#65292;&#24182;&#26816;&#26597;&#29983;&#25104;&#35299;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#23545;&#25105;&#20204;&#30340;&#20195;&#29702;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;NLP4LP&#65292;&#36825;&#26159;&#19968;&#20010;&#32447;&#24615;&#35268;&#21010;(LP)&#21644;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;(MILP)&#38382;&#39064;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#22522;&#26412;&#30340;LLM&#25552;&#31034;&#31574;&#30053;&#30456;&#27604;&#65292;OptiMUS&#33021;&#22815;&#35299;&#20915;&#26356;&#22810;&#38382;&#39064;&#65292;&#35299;&#20915;&#29575;&#25552;&#39640;&#20102;67&#65285;&#12290;OptiMUS&#30340;&#20195;&#30721;&#21644;NLP4LP&#25968;&#25454;&#38598;&#21487;&#22312;\href{https://github&#20013;&#33719;&#24471;.
&lt;/p&gt;
&lt;p&gt;
Optimization problems are pervasive across various sectors, from manufacturing and distribution to healthcare. However, most such problems are still solved heuristically by hand rather than optimally by state-of-the-art solvers, as the expertise required to formulate and solve these problems limits the widespread adoption of optimization tools and techniques. We introduce OptiMUS, a Large Language Model (LLM)-based agent designed to formulate and solve MILP problems from their natural language descriptions. OptiMUS is capable of developing mathematical models, writing and debugging solver code, developing tests, and checking the validity of generated solutions. To benchmark our agent, we present NLP4LP, a novel dataset of linear programming (LP) and mixed integer linear programming (MILP) problems. Our experiments demonstrate that OptiMUS is able to solve 67\% more problems compared to a basic LLM prompting strategy. OptiMUS code and NLP4LP dataset are available at \href{https://github
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#19981;&#21487;&#30693;&#30340;PAC&#24378;&#21270;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#36328;&#36234;&#23481;&#37327;&#36825;&#20010;&#26032;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#65292;&#24182;&#21457;&#29616;&#20102;&#22312;&#29983;&#25104;&#21644;&#22312;&#32447;&#35775;&#38382;&#27169;&#22411;&#20043;&#38388;&#20197;&#21450;&#22312;&#32447;&#35775;&#38382;&#19979;&#30340;&#30830;&#23450;&#21644;&#38543;&#26426;MDP&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2310.06113</link><description>&lt;p&gt;
&#20160;&#20040;&#26102;&#20505;&#26159;&#22522;&#20110;&#19981;&#21487;&#30693;&#28608;&#21169;&#24378;&#21270;&#23398;&#20064;&#30340;&#32479;&#35745;&#21487;&#22788;&#29702;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
When is Agnostic Reinforcement Learning Statistically Tractable?. (arXiv:2310.06113v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06113
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#19981;&#21487;&#30693;&#30340;PAC&#24378;&#21270;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#36328;&#36234;&#23481;&#37327;&#36825;&#20010;&#26032;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#65292;&#24182;&#21457;&#29616;&#20102;&#22312;&#29983;&#25104;&#21644;&#22312;&#32447;&#35775;&#38382;&#27169;&#22411;&#20043;&#38388;&#20197;&#21450;&#22312;&#32447;&#35775;&#38382;&#19979;&#30340;&#30830;&#23450;&#21644;&#38543;&#26426;MDP&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#19981;&#21487;&#30693;&#30340;PAC&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#38382;&#39064;&#65306;&#32473;&#23450;&#19968;&#20010;&#31574;&#30053;&#31867;&#21035;&#928;&#65292;&#38656;&#35201;&#21644;&#19968;&#20010;&#26410;&#30693;&#30340;&#26377;&#21487;&#33021;&#26377;&#22823;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;MDP&#36827;&#34892;&#22810;&#23569;&#36718;&#20114;&#21160;&#26469;&#23398;&#20064;&#19968;&#20010;&#20851;&#20110;&#928;&#30340;&#949;-&#27425;&#20248;&#31574;&#30053;&#65311;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#65292;&#31216;&#20026;&#8220;&#36328;&#36234;&#23481;&#37327;&#8221;&#65292;&#23427;&#20165;&#20381;&#36182;&#20110;&#31574;&#30053;&#31867;&#21035;&#928;&#65292;&#24182;&#19988;&#19982;MDP&#30340;&#21160;&#24577;&#26080;&#20851;&#12290;&#36890;&#36807;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#20219;&#20309;&#31574;&#30053;&#31867;&#21035;&#928;&#65292;&#26377;&#30028;&#30340;&#36328;&#36234;&#23481;&#37327;&#21487;&#20197;&#21051;&#30011;PAC&#21487;&#23398;&#20064;&#24615;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22312;&#32447;RL&#26469;&#35828;&#65292;&#24773;&#20917;&#26356;&#21152;&#24494;&#22937;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23384;&#22312;&#19968;&#20010;&#20855;&#26377;&#26377;&#30028;&#36328;&#36234;&#23481;&#37327;&#30340;&#31574;&#30053;&#31867;&#21035;&#928;&#65292;&#38656;&#35201;&#36229;&#22810;&#39033;&#24335;&#25968;&#37327;&#30340;&#26679;&#26412;&#25165;&#33021;&#23398;&#20064;&#12290;&#36825;&#25581;&#31034;&#20102;&#22312;&#19981;&#21516;&#29983;&#25104;&#21644;&#22312;&#32447;&#35775;&#38382;&#27169;&#22411;&#20043;&#38388;&#20197;&#21450;&#22312;&#32447;&#35775;&#38382;&#19979;&#30340;&#30830;&#23450;/&#38543;&#26426;MDP&#20043;&#38388;&#30340;&#19981;&#30830;&#23450;&#21487;&#23398;&#20064;&#24615;&#20043;&#38388;&#30340;&#20986;&#20046;&#24847;&#26009;&#30340;&#24046;&#24322;&#12290;&#22312;&#31215;&#26497;&#30340;&#26041;&#38754;&#65292;&#25105;&#20204;&#35782;&#21035;&#20986;&#19968;&#20010;&#39069;&#22806;&#30340;&#8220;&#22826;&#38451;&#8221;
&lt;/p&gt;
&lt;p&gt;
We study the problem of agnostic PAC reinforcement learning (RL): given a policy class $\Pi$, how many rounds of interaction with an unknown MDP (with a potentially large state and action space) are required to learn an $\epsilon$-suboptimal policy with respect to $\Pi$? Towards that end, we introduce a new complexity measure, called the \emph{spanning capacity}, that depends solely on the set $\Pi$ and is independent of the MDP dynamics. With a generative model, we show that for any policy class $\Pi$, bounded spanning capacity characterizes PAC learnability. However, for online RL, the situation is more subtle. We show there exists a policy class $\Pi$ with a bounded spanning capacity that requires a superpolynomial number of samples to learn. This reveals a surprising separation for agnostic learnability between generative access and online access models (as well as between deterministic/stochastic MDPs under online access). On the positive side, we identify an additional \emph{sunf
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#39640;&#32500;&#22240;&#26524;&#25512;&#26029;&#30340;&#21464;&#20998;&#32972;&#38376;&#35843;&#25972;&#25216;&#26415;&#65292;&#33021;&#22815;&#22788;&#29702;&#39640;&#32500;&#27835;&#30103;&#21644;&#28151;&#26434;&#22240;&#32032;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#21307;&#30103;&#25968;&#25454;&#20013;&#12290;</title><link>http://arxiv.org/abs/2310.06100</link><description>&lt;p&gt;
&#20351;&#29992;&#21464;&#20998;&#32972;&#38376;&#35843;&#25972;&#36827;&#34892;&#39640;&#32500;&#22240;&#26524;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
High Dimensional Causal Inference with Variational Backdoor Adjustment. (arXiv:2310.06100v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06100
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#39640;&#32500;&#22240;&#26524;&#25512;&#26029;&#30340;&#21464;&#20998;&#32972;&#38376;&#35843;&#25972;&#25216;&#26415;&#65292;&#33021;&#22815;&#22788;&#29702;&#39640;&#32500;&#27835;&#30103;&#21644;&#28151;&#26434;&#22240;&#32032;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#21307;&#30103;&#25968;&#25454;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#38376;&#35843;&#25972;&#26159;&#19968;&#31181;&#22240;&#26524;&#25512;&#26029;&#25216;&#26415;&#65292;&#29992;&#20110;&#20174;&#32431;&#35266;&#23519;&#25968;&#25454;&#20013;&#20272;&#35745;&#24178;&#39044;&#25968;&#37327;&#12290;&#22312;&#21307;&#30103;&#29615;&#22659;&#20013;&#65292;&#32972;&#38376;&#35843;&#25972;&#21487;&#29992;&#20110;&#25511;&#21046;&#28151;&#26434;&#22240;&#32032;&#24182;&#20272;&#35745;&#27835;&#30103;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#39640;&#32500;&#27835;&#30103;&#21644;&#28151;&#26434;&#22240;&#32032;&#21487;&#33021;&#24341;&#21457;&#19968;&#31995;&#21015;&#28508;&#22312;&#38382;&#39064;&#65306;&#21487;&#35745;&#31639;&#24615;&#12289;&#21487;&#36776;&#35782;&#24615;&#12289;&#20248;&#21270;&#31561;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#26469;&#35299;&#20915;&#39640;&#32500;&#27835;&#30103;&#21644;&#28151;&#26434;&#22240;&#32032;&#30340;&#32972;&#38376;&#35843;&#25972;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#32972;&#38376;&#35843;&#25972;&#35270;&#20026;&#19968;&#31181;&#21464;&#20998;&#25512;&#26029;&#20248;&#21270;&#38382;&#39064;&#65292;&#26080;&#38656;&#20381;&#36182;&#20195;&#29702;&#21464;&#37327;&#21644;&#38544;&#34255;&#28151;&#26434;&#22240;&#32032;&#12290;&#23454;&#39564;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#21508;&#31181;&#39640;&#32500;&#29615;&#22659;&#20013;&#20272;&#35745;&#24178;&#39044;&#27010;&#29575;&#65292;&#21253;&#25324;&#21322;&#21512;&#25104;X&#20809;&#21307;&#30103;&#25968;&#25454;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#32972;&#38376;&#35843;&#25972;&#30340;&#39318;&#27425;&#24212;&#29992;&#65292;&#20854;&#20013;&#25152;&#26377;&#30456;&#20851;&#21464;&#37327;&#37117;&#26159;&#39640;&#32500;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backdoor adjustment is a technique in causal inference for estimating interventional quantities from purely observational data. For example, in medical settings, backdoor adjustment can be used to control for confounding and estimate the effectiveness of a treatment. However, high dimensional treatments and confounders pose a series of potential pitfalls: tractability, identifiability, optimization. In this work, we take a generative modeling approach to backdoor adjustment for high dimensional treatments and confounders. We cast backdoor adjustment as an optimization problem in variational inference without reliance on proxy variables and hidden confounders. Empirically, our method is able to estimate interventional likelihood in a variety of high dimensional settings, including semi-synthetic X-ray medical data. To the best of our knowledge, this is the first application of backdoor adjustment in which all the relevant variables are high dimensional.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#39044;&#27979;&#36741;&#21161;&#30446;&#26631;&#23545;&#34920;&#31034;&#23398;&#20064;&#21644;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#39044;&#27979;&#30446;&#26631;&#33021;&#26174;&#33879;&#25552;&#39640;&#21644;&#31283;&#23450;&#23398;&#20064;&#65292;&#24182;&#19988;&#33021;&#25903;&#25345;&#34920;&#24449;&#36801;&#31227;&#12290;&#27492;&#22806;&#65292;&#19982;&#31070;&#32463;&#27963;&#21160;&#21464;&#21270;&#30456;&#20284;&#65292;&#36825;&#20123;&#36741;&#21161;&#30446;&#26631;&#20063;&#27169;&#25311;&#20102;&#22823;&#33041;&#20013;&#30340;&#34920;&#24449;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.06089</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#39044;&#27979;&#36741;&#21161;&#30446;&#26631;&#27169;&#20223;&#22823;&#33041;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Predictive auxiliary objectives in deep RL mimic learning in the brain. (arXiv:2310.06089v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#39044;&#27979;&#36741;&#21161;&#30446;&#26631;&#23545;&#34920;&#31034;&#23398;&#20064;&#21644;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#39044;&#27979;&#30446;&#26631;&#33021;&#26174;&#33879;&#25552;&#39640;&#21644;&#31283;&#23450;&#23398;&#20064;&#65292;&#24182;&#19988;&#33021;&#25903;&#25345;&#34920;&#24449;&#36801;&#31227;&#12290;&#27492;&#22806;&#65292;&#19982;&#31070;&#32463;&#27963;&#21160;&#21464;&#21270;&#30456;&#20284;&#65292;&#36825;&#20123;&#36741;&#21161;&#30446;&#26631;&#20063;&#27169;&#25311;&#20102;&#22823;&#33041;&#20013;&#30340;&#34920;&#24449;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#21363;&#23558;&#21457;&#29983;&#30340;&#20107;&#20214;&#30340;&#33021;&#21147;&#34987;&#20551;&#35774;&#20026;&#33258;&#28982;&#21644;&#26426;&#22120;&#35748;&#30693;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#36825;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#24471;&#21040;&#20102;&#25903;&#25345;&#65292;&#20854;&#20013;&#33258;&#30417;&#30563;&#36741;&#21161;&#30446;&#26631;&#65288;&#22914;&#39044;&#27979;&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#25903;&#25345;&#34920;&#31034;&#23398;&#20064;&#21644;&#25552;&#39640;&#20219;&#21153;&#24615;&#33021;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#27979;&#36741;&#21161;&#30446;&#26631;&#23545;RL&#31995;&#32479;&#20013;&#19981;&#21516;&#27169;&#22359;&#30340;&#34920;&#31034;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#36825;&#20123;&#27169;&#25311;&#22823;&#33041;&#35266;&#23519;&#21040;&#30340;&#34920;&#24449;&#21464;&#21270;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#26550;&#26500;&#20013;&#65292;&#39044;&#27979;&#30446;&#26631;&#29305;&#21035;&#25552;&#39640;&#21644;&#31283;&#23450;&#23398;&#20064;&#65292;&#24182;&#19988;&#25105;&#20204;&#30830;&#23450;&#20102;&#26356;&#38271;&#30340;&#39044;&#27979;&#26102;&#27573;&#22312;&#25903;&#25345;&#34920;&#24449;&#36801;&#31227;&#26041;&#38754;&#26356;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20010;RL&#31995;&#32479;&#20013;&#30340;&#34920;&#24449;&#21464;&#21270;&#19982;&#22823;&#33041;&#20013;&#35266;&#23519;&#21040;&#30340;&#31070;&#32463;&#27963;&#21160;&#21464;&#21270;&#26377;&#24778;&#20154;&#30340;&#30456;&#20284;&#20043;&#22788;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#36741;&#21161;&#39044;&#27979;&#27169;&#22411;&#21644;&#22823;&#33041;&#20013;&#30340;&#34920;&#24449;&#21464;&#21270;&#20043;&#38388;&#24314;&#31435;&#20102;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to predict upcoming events has been hypothesized to comprise a key aspect of natural and machine cognition. This is supported by trends in deep reinforcement learning (RL), where self-supervised auxiliary objectives such as prediction are widely used to support representation learning and improve task performance. Here, we study the effects predictive auxiliary objectives have on representation learning across different modules of an RL system and how these mimic representational changes observed in the brain. We find that predictive objectives improve and stabilize learning particularly in resource-limited architectures, and we identify settings where longer predictive horizons better support representational transfer. Furthermore, we find that representational changes in this RL system bear a striking resemblance to changes in neural activity observed in the brain across various experiments. Specifically, we draw a connection between the auxiliary predictive model of the 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#23637;&#31034;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65288;FPS&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#24310;&#36831;&#21709;&#24212;&#30340;&#27010;&#24565;&#26469;&#35299;&#20915;&#23637;&#31034;&#24615;&#24341;&#36215;&#30340;&#20998;&#24067;&#21464;&#21270;&#65292;&#24182;&#23454;&#29616;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.06077</link><description>&lt;p&gt;
&#21487;&#23637;&#31034;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Performative Time-Series Forecasting. (arXiv:2310.06077v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#23637;&#31034;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65288;FPS&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#24310;&#36831;&#21709;&#24212;&#30340;&#27010;&#24565;&#26469;&#35299;&#20915;&#23637;&#31034;&#24615;&#24341;&#36215;&#30340;&#20998;&#24067;&#21464;&#21270;&#65292;&#24182;&#23454;&#29616;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26159;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#22312;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#23454;&#36136;&#24615;&#30340;&#36827;&#23637;&#12290;&#35768;&#22810;&#29616;&#23454;&#29983;&#27963;&#22330;&#26223;&#65292;&#22914;&#20844;&#20849;&#21355;&#29983;&#12289;&#32463;&#27982;&#21644;&#31038;&#20250;&#24212;&#29992;&#65292;&#28041;&#21450;&#21040;&#21453;&#39304;&#24490;&#29615;&#65292;&#20854;&#20013;&#39044;&#27979;&#32467;&#26524;&#21487;&#33021;&#20250;&#24433;&#21709;&#21040;&#39044;&#27979;&#30340;&#32467;&#26524;&#65292;&#36827;&#32780;&#25913;&#21464;&#30446;&#26631;&#21464;&#37327;&#30340;&#20998;&#24067;&#12290;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#23637;&#31034;&#24615;&#65292;&#24341;&#20837;&#20102;&#21487;&#33021;&#20986;&#29616;&#8220;&#33258;&#25105;&#25269;&#28040;&#8221;&#25110;&#8220;&#33258;&#25105;&#23454;&#29616;&#8221;&#30340;&#39044;&#27979;&#30340;&#28508;&#21147;&#12290;&#23613;&#31649;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#23545;&#20998;&#31867;&#38382;&#39064;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#20294;&#23637;&#31034;&#24615;&#22312;&#26426;&#22120;&#23398;&#20064;&#35270;&#35282;&#19979;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#24191;&#27867;&#25506;&#35752;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#21487;&#23637;&#31034;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;PeTS&#65289;&#36827;&#34892;&#20102;&#24418;&#24335;&#21270;&#65292;&#35299;&#20915;&#20102;&#24403;&#21487;&#33021;&#23384;&#22312;&#23637;&#31034;&#24615;&#24341;&#36215;&#30340;&#20998;&#24067;&#21464;&#21270;&#26102;&#30340;&#20934;&#30830;&#39044;&#27979;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#65292;&#29305;&#24449;&#23637;&#31034;&#24615;&#36716;&#31227;&#65288;FPS&#65289;&#65292;&#23427;&#21033;&#29992;&#24310;&#36831;&#21709;&#24212;&#30340;&#27010;&#24565;&#26469;&#39044;&#27979;&#20998;&#24067;&#30340;&#21464;&#21270;&#21644;&#38543;&#21518;&#30340;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time-series forecasting is a critical challenge in various domains and has witnessed substantial progress in recent years. Many real-life scenarios, such as public health, economics, and social applications, involve feedback loops where predictions can influence the predicted outcome, subsequently altering the target variable's distribution. This phenomenon, known as performativity, introduces the potential for 'self-negating' or 'self-fulfilling' predictions. Despite extensive studies in classification problems across domains, performativity remains largely unexplored in the context of time-series forecasting from a machine-learning perspective.  In this paper, we formalize performative time-series forecasting (PeTS), addressing the challenge of accurate predictions when performativity-induced distribution shifts are possible. We propose a novel approach, Feature Performative-Shifting (FPS), which leverages the concept of delayed response to anticipate distribution shifts and subseque
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#21644;&#24739;&#32773;&#34920;&#22411;&#30740;&#31350;&#65292;&#39044;&#27979;&#24739;&#32773;&#26410;&#26469;&#30340;&#30140;&#30171;&#36712;&#36857;&#65292;&#20197;&#24110;&#21161;&#24739;&#32773;&#31649;&#29702;&#38256;&#29366;&#32454;&#32990;&#36139;&#34880;&#65292;&#25913;&#21892;&#29983;&#27963;&#36136;&#37327;&#65292;&#24182;&#20943;&#23569;&#23545;&#38463;&#29255;&#31867;&#33647;&#29289;&#30340;&#20381;&#36182;&#21644;&#21103;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.06075</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24739;&#32773;&#34920;&#22411;&#30740;&#31350;&#30340;&#30171;&#33510;&#39044;&#27979;&#65306;&#39044;&#38450;&#38463;&#29255;&#31867;&#33647;&#29289;&#25104;&#30270;&#30340;&#23581;&#35797;
&lt;/p&gt;
&lt;p&gt;
Pain Forecasting using Self-supervised Learning and Patient Phenotyping: An attempt to prevent Opioid Addiction. (arXiv:2310.06075v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#21644;&#24739;&#32773;&#34920;&#22411;&#30740;&#31350;&#65292;&#39044;&#27979;&#24739;&#32773;&#26410;&#26469;&#30340;&#30140;&#30171;&#36712;&#36857;&#65292;&#20197;&#24110;&#21161;&#24739;&#32773;&#31649;&#29702;&#38256;&#29366;&#32454;&#32990;&#36139;&#34880;&#65292;&#25913;&#21892;&#29983;&#27963;&#36136;&#37327;&#65292;&#24182;&#20943;&#23569;&#23545;&#38463;&#29255;&#31867;&#33647;&#29289;&#30340;&#20381;&#36182;&#21644;&#21103;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38256;&#29366;&#32454;&#32990;&#36139;&#34880;&#65288;SCD&#65289;&#26159;&#19968;&#31181;&#24930;&#24615;&#36951;&#20256;&#24615;&#30142;&#30149;&#65292;&#29305;&#24449;&#20026;&#21453;&#22797;&#21457;&#20316;&#30340;&#24613;&#24615;&#30140;&#30171;&#12290;&#38463;&#29255;&#31867;&#33647;&#29289;&#36890;&#24120;&#29992;&#20110;&#31649;&#29702;&#36825;&#20123;&#30140;&#30171;&#21457;&#20316;&#65307;&#22312;&#36825;&#31181;&#30142;&#30149;&#20013;&#20351;&#29992;&#38463;&#29255;&#31867;&#33647;&#29289;&#31649;&#29702;&#30140;&#30171;&#30340;&#31243;&#24230;&#26159;&#19968;&#20010;&#20105;&#35758;&#30340;&#38382;&#39064;&#12290;&#21560;&#39135;&#25104;&#30270;&#30340;&#39118;&#38505;&#21644;&#38463;&#29255;&#31867;&#33647;&#29289;&#27835;&#30103;&#30340;&#21103;&#20316;&#29992;&#24448;&#24448;&#20250;&#23548;&#33268;&#23558;&#26469;&#26356;&#22810;&#30340;&#30140;&#30171;&#21457;&#20316;&#12290;&#22240;&#27492;&#65292;&#20934;&#30830;&#39044;&#27979;&#24739;&#32773;&#23558;&#26469;&#30340;&#30140;&#30171;&#21457;&#23637;&#36712;&#36857;&#23545;&#20110;&#24110;&#21161;&#24739;&#32773;&#31649;&#29702;&#20182;&#20204;&#30340;SCD&#20197;&#25552;&#39640;&#29983;&#27963;&#36136;&#37327;&#32780;&#19981;&#25439;&#23475;&#27835;&#30103;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#20110;&#30140;&#30171;&#20027;&#35201;&#26159;&#30001;&#24739;&#32773;&#33258;&#34892;&#25253;&#21578;&#35760;&#24405;&#30340;&#65292;&#33719;&#24471;&#35768;&#22810;&#30140;&#30171;&#35760;&#24405;&#26469;&#35774;&#35745;&#39044;&#27979;&#27169;&#22411;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#35299;&#20915;&#30140;&#30171;&#39044;&#27979;&#38382;&#39064;&#26102;&#65292;&#20165;&#20381;&#38752;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#24335;&#26082;&#26114;&#36149;&#21448;&#30171;&#33510;&#65288;&#22240;&#20026;&#38656;&#35201;&#24739;&#32773;&#37197;&#21512;&#65289;&#12290;&#37492;&#20110;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#30140;&#30171;&#39044;&#27979;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#23545;&#36825;&#31181;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#32858;&#31867;&#23545;&#20110;&#24739;&#32773;&#34920;&#22411;&#30740;&#31350;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sickle Cell Disease (SCD) is a chronic genetic disorder characterized by recurrent acute painful episodes. Opioids are often used to manage these painful episodes; the extent of their use in managing pain in this disorder is an issue of debate. The risk of addiction and side effects of these opioid treatments can often lead to more pain episodes in the future. Hence, it is crucial to forecast future patient pain trajectories to help patients manage their SCD to improve their quality of life without compromising their treatment. It is challenging to obtain many pain records to design forecasting models since it is mainly recorded by patients' self-report. Therefore, it is expensive and painful (due to the need for patient compliance) to solve pain forecasting problems in a purely supervised manner. In light of this challenge, we propose to solve the pain forecasting problem using self-supervised learning methods. Also, clustering such time-series data is crucial for patient phenotyping,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#26059;&#36716;&#30697;&#38453;&#30340;&#27169;&#22411;&#65292;&#20197;&#22686;&#24378;&#22522;&#20110;&#23039;&#21183;&#20272;&#35745;&#30340;&#27963;&#21160;&#35782;&#21035;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#21033;&#29992;&#24102;&#26377;&#26059;&#36716;&#30697;&#38453;&#25968;&#25454;&#22686;&#24378;&#30340;SVM&#19982;SGD&#20248;&#21270;&#26041;&#27861;&#65292;&#22312;&#20998;&#31867;&#20116;&#31181;&#20307;&#32946;&#27963;&#21160;&#26102;&#36798;&#21040;&#20102;96%&#30340;&#20934;&#30830;&#29575;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;&#19981;&#23454;&#26045;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#22522;&#20934;&#20934;&#30830;&#29575;&#20165;&#20026;64%&#12290;</title><link>http://arxiv.org/abs/2310.06068</link><description>&lt;p&gt;
&#20351;&#29992;&#26059;&#36716;&#30697;&#38453;&#22686;&#24378;&#22522;&#20110;&#35270;&#35273;&#30340;&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Augmenting Vision-Based Human Pose Estimation with Rotation Matrix. (arXiv:2310.06068v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#26059;&#36716;&#30697;&#38453;&#30340;&#27169;&#22411;&#65292;&#20197;&#22686;&#24378;&#22522;&#20110;&#23039;&#21183;&#20272;&#35745;&#30340;&#27963;&#21160;&#35782;&#21035;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#21033;&#29992;&#24102;&#26377;&#26059;&#36716;&#30697;&#38453;&#25968;&#25454;&#22686;&#24378;&#30340;SVM&#19982;SGD&#20248;&#21270;&#26041;&#27861;&#65292;&#22312;&#20998;&#31867;&#20116;&#31181;&#20307;&#32946;&#27963;&#21160;&#26102;&#36798;&#21040;&#20102;96%&#30340;&#20934;&#30830;&#29575;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;&#19981;&#23454;&#26045;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#22522;&#20934;&#20934;&#30830;&#29575;&#20165;&#20026;64%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20581;&#36523;&#24212;&#29992;&#36890;&#24120;&#29992;&#20110;&#30417;&#25511;&#20581;&#36523;&#25151;&#20869;&#30340;&#27963;&#21160;&#65292;&#20294;&#23427;&#20204;&#24120;&#24120;&#26080;&#27861;&#33258;&#21160;&#36319;&#36394;&#20581;&#36523;&#25151;&#20869;&#37096;&#30340;&#27963;&#21160;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#23039;&#21183;&#20272;&#35745;&#19982;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21363;&#26059;&#36716;&#30697;&#38453;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#23039;&#21183;&#20272;&#35745;&#25968;&#25454;&#26469;&#25552;&#39640;&#27963;&#21160;&#35782;&#21035;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#19981;&#21516;&#30340;&#20998;&#31867;&#31639;&#27861;&#20197;&#21450;&#22270;&#20687;&#22686;&#24378;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21033;&#29992;&#24102;&#26377;&#26059;&#36716;&#30697;&#38453;&#25968;&#25454;&#22686;&#24378;&#30340;SVM&#19982;SGD&#20248;&#21270;&#30340;&#26041;&#27861;&#22312;&#20998;&#31867;&#20116;&#31181;&#20307;&#32946;&#27963;&#21160;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20934;&#30830;&#30340;&#32467;&#26524;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;96%&#12290;&#30456;&#21453;&#65292;&#22914;&#26524;&#19981;&#23454;&#26045;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#22522;&#20934;&#20934;&#30830;&#29575;&#20165;&#20026;64%&#12290;
&lt;/p&gt;
&lt;p&gt;
Fitness applications are commonly used to monitor activities within the gym, but they often fail to automatically track indoor activities inside the gym. This study proposes a model that utilizes pose estimation combined with a novel data augmentation method, i.e., rotation matrix. We aim to enhance the classification accuracy of activity recognition based on pose estimation data. Through our experiments, we experiment with different classification algorithms along with image augmentation approaches. Our findings demonstrate that the SVM with SGD optimization, using data augmentation with the Rotation Matrix, yields the most accurate results, achieving a 96% accuracy rate in classifying five physical activities. Conversely, without implementing the data augmentation techniques, the baseline accuracy remains at a modest 64%.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35299;&#20915;&#31995;&#32479;&#32423;&#33455;&#29255;&#65288;SoC&#65289;&#35774;&#35745;&#20013;&#30340;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;LLM&#38598;&#25104;&#21040;SoC&#23433;&#20840;&#39564;&#35777;&#27969;&#31243;&#20013;&#65292;&#24320;&#36767;&#20102;&#26356;&#39640;&#25928;&#12289;&#21487;&#20280;&#32553;&#21644;&#36866;&#24212;&#30340;&#26032;&#26041;&#27861;&#12290;&#30446;&#26631;&#26159;&#30830;&#20445;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;SoC&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.06046</link><description>&lt;p&gt;
LLM&#29992;&#20110;SoC&#23433;&#20840;: &#19968;&#31181;&#33539;&#24335;&#36716;&#21464;
&lt;/p&gt;
&lt;p&gt;
LLM for SoC Security: A Paradigm Shift. (arXiv:2310.06046v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35299;&#20915;&#31995;&#32479;&#32423;&#33455;&#29255;&#65288;SoC&#65289;&#35774;&#35745;&#20013;&#30340;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;LLM&#38598;&#25104;&#21040;SoC&#23433;&#20840;&#39564;&#35777;&#27969;&#31243;&#20013;&#65292;&#24320;&#36767;&#20102;&#26356;&#39640;&#25928;&#12289;&#21487;&#20280;&#32553;&#21644;&#36866;&#24212;&#30340;&#26032;&#26041;&#27861;&#12290;&#30446;&#26631;&#26159;&#30830;&#20445;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;SoC&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#30005;&#23376;&#35774;&#22791;&#20013;&#31995;&#32479;&#32423;&#33455;&#29255;&#65288;SoC&#65289;&#35774;&#35745;&#30340;&#26222;&#21450;&#21644;&#22797;&#26434;&#24615;&#30340;&#22686;&#21152;&#65292;&#23558;&#23433;&#20840;&#24615;&#32435;&#20837;SoC&#35774;&#35745;&#27969;&#31243;&#30340;&#20219;&#21153;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#23433;&#20840;&#35299;&#20915;&#26041;&#26696;&#30001;&#20110;&#22312;&#21487;&#20280;&#32553;&#24615;&#12289;&#20840;&#38754;&#24615;&#21644;&#36866;&#24212;&#24615;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#26080;&#27861;&#25552;&#20379;&#23545;&#29616;&#20195;SoC&#35774;&#35745;&#30340;&#26377;&#25928;&#39564;&#35777;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22240;&#20854;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#39640;&#32423;&#25512;&#29702;&#21644;&#31243;&#24207;&#21512;&#25104;&#20219;&#21153;&#20013;&#30340;&#21331;&#36234;&#25104;&#21151;&#32780;&#22791;&#21463;&#36190;&#35465;&#12290;&#35748;&#35782;&#21040;&#36825;&#19968;&#26426;&#36935;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#30528;&#30524;&#20110;&#21033;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#65288;GPT&#65289;&#30340;&#26032;&#20852;&#33021;&#21147;&#26469;&#35299;&#20915;SoC&#23433;&#20840;&#39046;&#22495;&#30340;&#29616;&#26377;&#24046;&#36317;&#65292;&#26088;&#22312;&#36861;&#27714;&#26356;&#39640;&#25928;&#12289;&#21487;&#20280;&#32553;&#21644;&#36866;&#24212;&#30340;&#26041;&#27861;&#35770;&#12290;&#36890;&#36807;&#23558;LLM&#38598;&#25104;&#21040;SoC&#23433;&#20840;&#39564;&#35777;&#33539;&#24335;&#20013;&#65292;&#25105;&#20204;&#25171;&#24320;&#20102;&#30830;&#20445;&#26085;&#30410;&#22797;&#26434;SoC&#23433;&#20840;&#30340;&#26032;&#21487;&#33021;&#24615;&#21644;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the ubiquity and complexity of system-on-chip (SoC) designs increase across electronic devices, the task of incorporating security into an SoC design flow poses significant challenges. Existing security solutions are inadequate to provide effective verification of modern SoC designs due to their limitations in scalability, comprehensiveness, and adaptability. On the other hand, Large Language Models (LLMs) are celebrated for their remarkable success in natural language understanding, advanced reasoning, and program synthesis tasks. Recognizing an opportunity, our research delves into leveraging the emergent capabilities of Generative Pre-trained Transformers (GPTs) to address the existing gaps in SoC security, aiming for a more efficient, scalable, and adaptable methodology. By integrating LLMs into the SoC security verification paradigm, we open a new frontier of possibilities and challenges to ensure the security of increasingly complex SoCs. This paper offers an in-depth analysis
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#38598;&#25104;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#23558;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;CGANs&#65289;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#23545;&#20005;&#37325;&#22825;&#27668;&#36827;&#34892;&#27010;&#29575;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#22312;&#20351;&#29992;HRRR&#39044;&#25253;&#20316;&#20026;&#36755;&#20837;&#25968;&#25454;&#65292;&#22312;2021&#24180;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#30456;&#23545;&#20110;&#20854;&#20182;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#39640;&#36798;20&#65285;&#30340;Brier&#25216;&#24039;&#20998;&#25968;&#65288;BSS&#65289;&#12290;</title><link>http://arxiv.org/abs/2310.06045</link><description>&lt;p&gt;
&#36890;&#36807;&#30830;&#23450;&#24615;&#23545;&#27969;&#27169;&#22411;&#30340;&#29983;&#25104;&#24615;&#38598;&#25104;&#28145;&#24230;&#23398;&#20064;&#65292;&#29992;&#20110;&#20005;&#37325;&#22825;&#27668;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Generative ensemble deep learning severe weather prediction from a deterministic convection-allowing model. (arXiv:2310.06045v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#38598;&#25104;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#23558;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;CGANs&#65289;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#23545;&#20005;&#37325;&#22825;&#27668;&#36827;&#34892;&#27010;&#29575;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#22312;&#20351;&#29992;HRRR&#39044;&#25253;&#20316;&#20026;&#36755;&#20837;&#25968;&#25454;&#65292;&#22312;2021&#24180;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#30456;&#23545;&#20110;&#20854;&#20182;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#39640;&#36798;20&#65285;&#30340;Brier&#25216;&#24039;&#20998;&#25968;&#65288;BSS&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#20110;&#27010;&#29575;&#39044;&#27979;&#32654;&#22269;&#26412;&#22303;&#20005;&#37325;&#22825;&#27668;&#65288;&#40857;&#21367;&#39118;&#12289;&#20912;&#38649;&#21644;&#22823;&#39118;&#38453;&#65289;&#30340;&#38598;&#25104;&#21518;&#22788;&#29702;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;CGANs&#65289;&#19982;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#21518;&#22788;&#29702;&#23545;&#27969;&#20801;&#35768;&#27169;&#22411;&#65288;CAM&#65289;&#30340;&#39044;&#27979;&#12290;CGANs&#34987;&#35774;&#35745;&#29992;&#20110;&#20174;&#30830;&#23450;&#24615;CAM&#39044;&#27979;&#20013;&#21019;&#24314;&#21512;&#25104;&#38598;&#25104;&#25104;&#21592;&#65292;&#20854;&#36755;&#20986;&#32463;&#36807;CNN&#22788;&#29702;&#20197;&#20272;&#35745;&#20005;&#37325;&#22825;&#27668;&#30340;&#27010;&#29575;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#39640;&#20998;&#36776;&#29575;&#24555;&#36895;&#21047;&#26032;&#65288;HRRR&#65289;1-24&#23567;&#26102;&#39044;&#25253;&#20316;&#20026;&#36755;&#20837;&#65292;&#20197;&#21450;&#26292;&#39118;&#39044;&#35686;&#20013;&#24515;&#65288;SPC&#65289;&#30340;&#20005;&#37325;&#22825;&#27668;&#25253;&#21578;&#20316;&#20026;&#30446;&#26631;&#36827;&#34892;&#27979;&#35797;&#12290;&#22312;2021&#24180;&#30340;HRRR&#39044;&#27979;&#27979;&#35797;&#25968;&#25454;&#38598;&#20013;&#65292;&#35813;&#26041;&#27861;&#30456;&#23545;&#20110;&#20854;&#20182;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#21442;&#32771;&#26041;&#27861;&#25552;&#39640;&#20102;&#39640;&#36798;20&#65285;&#30340;Brier&#25216;&#24039;&#20998;&#25968;&#65288;BSS&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
An ensemble post-processing method is developed for the probabilistic prediction of severe weather (tornadoes, hail, and wind gusts) over the conterminous United States (CONUS). The method combines conditional generative adversarial networks (CGANs), a type of deep generative model, with a convolutional neural network (CNN) to post-process convection-allowing model (CAM) forecasts. The CGANs are designed to create synthetic ensemble members from deterministic CAM forecasts, and their outputs are processed by the CNN to estimate the probability of severe weather. The method is tested using High-Resolution Rapid Refresh (HRRR) 1--24 hr forecasts as inputs and Storm Prediction Center (SPC) severe weather reports as targets. The method produced skillful predictions with up to 20% Brier Skill Score (BSS) increases compared to other neural-network-based reference methods using a testing dataset of HRRR forecasts in 2021. For the evaluation of uncertainty quantification, the method is overcon
&lt;/p&gt;</description></item><item><title>DyST&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#21160;&#24577;&#22330;&#26223;&#30340;&#28508;&#22312;&#20998;&#35299;&#65292;&#20174;&#23454;&#38469;&#35270;&#39057;&#20013;&#25429;&#25417;&#21040;&#20102;&#22330;&#26223;&#30340;3D&#32467;&#26500;&#21644;&#21160;&#24577;&#29305;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#23545;&#30456;&#26426;&#21644;&#22330;&#26223;&#20869;&#23481;&#30340;&#29420;&#31435;&#25511;&#21046;&#35270;&#22270;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2310.06020</link><description>&lt;p&gt;
DyST&#65306;&#38754;&#21521;&#23454;&#38469;&#35270;&#39057;&#30340;&#21160;&#24577;&#31070;&#32463;&#22330;&#26223;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
DyST: Towards Dynamic Neural Scene Representations on Real-World Videos. (arXiv:2310.06020v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06020
&lt;/p&gt;
&lt;p&gt;
DyST&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#21160;&#24577;&#22330;&#26223;&#30340;&#28508;&#22312;&#20998;&#35299;&#65292;&#20174;&#23454;&#38469;&#35270;&#39057;&#20013;&#25429;&#25417;&#21040;&#20102;&#22330;&#26223;&#30340;3D&#32467;&#26500;&#21644;&#21160;&#24577;&#29305;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#23545;&#30456;&#26426;&#21644;&#22330;&#26223;&#20869;&#23481;&#30340;&#29420;&#31435;&#25511;&#21046;&#35270;&#22270;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#19990;&#30028;&#30340;&#35270;&#35273;&#29702;&#35299;&#36229;&#36234;&#20102;&#21333;&#20010;&#22270;&#20687;&#30340;&#35821;&#20041;&#21644;&#24179;&#38754;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20174;&#21333;&#30446;&#23454;&#38469;&#35270;&#39057;&#20013;&#25429;&#25417;&#21040;&#23454;&#38469;&#22330;&#26223;&#30340;3D&#32467;&#26500;&#21644;&#21160;&#24577;&#29305;&#24615;&#12290;&#25105;&#20204;&#30340;Dynamic Scene Transformer&#65288;DyST&#65289;&#27169;&#22411;&#21033;&#29992;&#20102;&#26368;&#36817;&#30340;&#31070;&#32463;&#22330;&#26223;&#34920;&#31034;&#30740;&#31350;&#25104;&#26524;&#65292;&#23398;&#20064;&#20102;&#21333;&#30446;&#23454;&#38469;&#35270;&#39057;&#30340;&#28508;&#22312;&#20998;&#35299;&#65292;&#21253;&#25324;&#22330;&#26223;&#20869;&#23481;&#12289;&#27599;&#20010;&#35270;&#35282;&#30340;&#22330;&#26223;&#21160;&#24577;&#21644;&#30456;&#26426;&#23039;&#24577;&#12290;&#36890;&#36807;&#22312;&#21333;&#30446;&#35270;&#39057;&#21644;&#25105;&#20204;&#30340;&#26032;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;DySO&#19978;&#36827;&#34892;&#19968;&#31181;&#26032;&#39062;&#30340;&#21327;&#21516;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#36825;&#31181;&#20998;&#31163;&#12290;DyST&#23398;&#20064;&#21040;&#20102;&#21160;&#24577;&#22330;&#26223;&#30340;&#20855;&#20307;&#28508;&#22312;&#34920;&#31034;&#65292;&#20351;&#24471;&#21487;&#20197;&#23545;&#22330;&#26223;&#30340;&#30456;&#26426;&#21644;&#20869;&#23481;&#36827;&#34892;&#29420;&#31435;&#25511;&#21046;&#30340;&#35270;&#22270;&#29983;&#25104;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual understanding of the world goes beyond the semantics and flat structure of individual images. In this work, we aim to capture both the 3D structure and dynamics of real-world scenes from monocular real-world videos. Our Dynamic Scene Transformer (DyST) model leverages recent work in neural scene representation to learn a latent decomposition of monocular real-world videos into scene content, per-view scene dynamics, and camera pose. This separation is achieved through a novel co-training scheme on monocular videos and our new synthetic dataset DySO. DyST learns tangible latent representations for dynamic scenes that enable view generation with separate control over the camera and the content of the scene.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#28216;&#25103;&#29702;&#35770;&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;AI&#39537;&#21160;&#30340;&#21093;&#22842;&#20013;&#30340;&#19981;&#22242;&#32467;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#21069;&#21463;&#23475;&#32773;&#38656;&#35201;&#35753;&#26410;&#26469;&#21463;&#23475;&#32773;&#35748;&#35782;&#21040;&#20182;&#20204;&#30340;&#21033;&#30410;&#21516;&#26679;&#38754;&#20020;&#20005;&#37325;&#21644;&#32039;&#36843;&#30340;&#23041;&#32961;&#65292;&#20197;&#28608;&#21169;&#26410;&#26469;&#21463;&#23475;&#32773;&#20197;&#22242;&#32467;&#25903;&#25345;&#24403;&#21069;&#21463;&#23475;&#32773;&#12290;</title><link>http://arxiv.org/abs/2310.06009</link><description>&lt;p&gt;
AI&#39537;&#21160;&#30340;&#21093;&#22842;&#20013;&#30340;&#20998;&#32780;&#27835;&#20043;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Divide-and-Conquer Dynamics in AI-Driven Disempowerment. (arXiv:2310.06009v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06009
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#28216;&#25103;&#29702;&#35770;&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;AI&#39537;&#21160;&#30340;&#21093;&#22842;&#20013;&#30340;&#19981;&#22242;&#32467;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#21069;&#21463;&#23475;&#32773;&#38656;&#35201;&#35753;&#26410;&#26469;&#21463;&#23475;&#32773;&#35748;&#35782;&#21040;&#20182;&#20204;&#30340;&#21033;&#30410;&#21516;&#26679;&#38754;&#20020;&#20005;&#37325;&#21644;&#32039;&#36843;&#30340;&#23041;&#32961;&#65292;&#20197;&#28608;&#21169;&#26410;&#26469;&#21463;&#23475;&#32773;&#20197;&#22242;&#32467;&#25903;&#25345;&#24403;&#21069;&#21463;&#23475;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#20844;&#21496;&#35797;&#22270;&#21019;&#36896;&#20986;&#22312;&#22823;&#37096;&#20998;&#32463;&#27982;&#20215;&#20540;&#24037;&#20316;&#19978;&#36229;&#36234;&#20154;&#31867;&#30340;AI&#31995;&#32479;&#12290;&#24403;&#21069;&#30340;AI&#27169;&#22411;&#24050;&#32463;&#33258;&#21160;&#21270;&#21066;&#24369;&#20102;&#19968;&#20123;&#33402;&#26415;&#23478;&#12289;&#28436;&#21592;&#21644;&#20316;&#23478;&#30340;&#29983;&#35745;&#12290;&#20294;&#26159;&#22312;&#37027;&#20123;&#20248;&#20808;&#32771;&#34385;&#24403;&#21069;&#21361;&#23475;&#21644;&#26410;&#26469;&#21361;&#23475;&#20043;&#38388;&#23384;&#22312;&#30528;&#20869;&#35751;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21338;&#24328;&#35770;&#27169;&#22411;&#26469;&#30740;&#31350;&#36825;&#31181;&#19981;&#22242;&#32467;&#30340;&#21407;&#22240;&#21644;&#21518;&#26524;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#26377;&#21161;&#20110;&#35299;&#37322;&#20026;&#20160;&#20040;&#22312;&#21382;&#21490;&#19978;&#65292;&#38754;&#20020;&#20849;&#21516;&#23041;&#32961;&#30340;&#21033;&#30410;&#30456;&#20851;&#26041;&#21457;&#29616;&#32852;&#21512;&#36215;&#26469;&#23545;&#25239;&#35813;&#23041;&#32961;&#26159;&#26377;&#21033;&#30340;&#65292;&#32780;&#35813;&#20849;&#21516;&#23041;&#32961;&#21448;&#21457;&#29616;&#20998;&#32780;&#27835;&#20043;&#26159;&#26377;&#21033;&#30340;&#12290;&#22312;&#29616;&#23454;&#21442;&#25968;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#25552;&#20986;&#20102;&#20960;&#20010;&#39044;&#27979;&#65292;&#22312;&#21382;&#21490;&#32463;&#39564;&#35760;&#24405;&#20013;&#24471;&#21040;&#20102;&#21021;&#27493;&#30340;&#35777;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI companies are attempting to create AI systems that outperform humans at most economically valuable work. Current AI models are already automating away the livelihoods of some artists, actors, and writers. But there is infighting between those who prioritize current harms and future harms. We construct a game-theoretic model of conflict to study the causes and consequences of this disunity. Our model also helps explain why throughout history, stakeholders sharing a common threat have found it advantageous to unite against it, and why the common threat has in turn found it advantageous to divide and conquer.  Under realistic parameter assumptions, our model makes several predictions that find preliminary corroboration in the historical-empirical record. First, current victims of AI-driven disempowerment need the future victims to realize that their interests are also under serious and imminent threat, so that future victims are incentivized to support current victims in solidarity. Se
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#20869;&#23384;&#21644;&#36890;&#20449;&#25104;&#26412;&#23545;&#35757;&#32451;&#36895;&#24230;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#34913;&#20869;&#23384;&#21644;&#36890;&#20449;&#30340;&#20248;&#21270;&#22120;&#65288;PaRO&#65289;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#27169;&#22411;&#35757;&#32451;&#30340;&#20998;&#23618;&#37325;&#21472;&#29615;&#36890;&#20449;&#25299;&#25169;&#32467;&#26500;&#65288;HO-Ring&#65289;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#31639;&#27861;&#25552;&#39640;&#20102;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#36890;&#20449;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.06003</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#39640;&#25928;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#20869;&#23384;&#21644;&#36890;&#20449;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
Rethinking Memory and Communication Cost for Efficient Large Language Model Training. (arXiv:2310.06003v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#20869;&#23384;&#21644;&#36890;&#20449;&#25104;&#26412;&#23545;&#35757;&#32451;&#36895;&#24230;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#34913;&#20869;&#23384;&#21644;&#36890;&#20449;&#30340;&#20248;&#21270;&#22120;&#65288;PaRO&#65289;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#27169;&#22411;&#35757;&#32451;&#30340;&#20998;&#23618;&#37325;&#21472;&#29615;&#36890;&#20449;&#25299;&#25169;&#32467;&#26500;&#65288;HO-Ring&#65289;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#31639;&#27861;&#25552;&#39640;&#20102;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#36890;&#20449;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#22823;&#35268;&#27169;&#27169;&#22411;&#35757;&#32451;&#26694;&#26550;&#36890;&#36807;&#21508;&#31181;&#20998;&#29255;&#25216;&#26415;&#20943;&#23567;&#20869;&#23384;&#28040;&#32791;&#12290;&#28982;&#32780;&#65292;&#24040;&#22823;&#30340;&#36890;&#20449;&#24320;&#38144;&#38477;&#20302;&#20102;&#35757;&#32451;&#25928;&#29575;&#65292;&#29305;&#21035;&#26159;&#22312;&#32593;&#32476;&#24102;&#23485;&#21464;&#21270;&#30340;&#20844;&#20849;&#20113;&#29615;&#22659;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#20869;&#23384;&#28040;&#32791;&#21644;&#36890;&#20449;&#24320;&#38144;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#36895;&#24230;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#34913;&#20869;&#23384;&#21644;&#36890;&#20449;&#30340;&#37096;&#20998;&#20887;&#20313;&#20248;&#21270;&#22120;(PaRO)&#12290;PaRO&#36890;&#36807;&#23558;GPU&#38598;&#32676;&#20998;&#32452;&#21644;&#24341;&#20837;&#24494;&#23567;&#30340;&#32452;&#20869;&#20869;&#23384;&#20887;&#20313;&#65292;&#20943;&#23569;&#20102;&#32452;&#38388;&#36890;&#20449;&#30340;&#25968;&#37327;&#21644;&#39057;&#29575;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#37325;&#21472;&#29615;(HO-Ring)&#36890;&#20449;&#25299;&#25169;&#32467;&#26500;&#65292;&#20197;&#22686;&#24378;&#22823;&#27169;&#22411;&#35757;&#32451;&#20013;&#33410;&#28857;&#20043;&#38388;&#25110;&#36328;&#20132;&#25442;&#26426;&#20043;&#38388;&#30340;&#36890;&#20449;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;HO-Ring&#31639;&#27861;&#25913;&#21892;&#20102;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#36890;&#20449;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
As model sizes and training datasets continue to increase, large-scale model training frameworks reduce memory consumption by various sharding techniques. However, the huge communication overhead reduces the training efficiency, especially in public cloud environments with varying network bandwidths. In this paper, we rethink the impact of memory consumption and communication overhead on the training speed of large language model, and propose a memory-communication balanced \underline{Pa}rtial \underline{R}edundancy \underline{O}ptimizer (PaRO). PaRO reduces the amount and frequency of inter-group communication by grouping GPU clusters and introducing minor intra-group memory redundancy, thereby improving the training efficiency of the model. Additionally, we propose a Hierarchical Overlapping Ring (HO-Ring) communication topology to enhance communication efficiency between nodes or across switches in large model training. Our experiments demonstrate that the HO-Ring algorithm improves
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#20854;&#22312;&#36923;&#36753;&#35868;&#39064;&#20013;&#20165;&#33021;&#25552;&#20379;7%&#27491;&#30830;&#31572;&#26696;&#21644;&#29702;&#30001;&#12290;</title><link>http://arxiv.org/abs/2310.05993</link><description>&lt;p&gt;
&#35780;&#20272;ChatGPT&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Measuring reasoning capabilities of ChatGPT. (arXiv:2310.05993v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05993
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#20854;&#22312;&#36923;&#36753;&#35868;&#39064;&#20013;&#20165;&#33021;&#25552;&#20379;7%&#27491;&#30830;&#31572;&#26696;&#21644;&#29702;&#30001;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#37327;&#21270;ChatGPT&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#20135;&#29983;&#30340;&#36923;&#36753;&#38169;&#35823;&#12290;&#23454;&#39564;&#20351;&#29992;&#20102;&#26469;&#33258;&#24211;&#20013;&#30340;144&#20010;&#35868;&#39064;&#65292;&#21253;&#25324;&#31639;&#26415;&#35868;&#39064;&#12289;&#36923;&#36753;&#31561;&#24335;&#12289;&#25968;&#29420;&#31561;&#35868;&#39064;&#12290;&#20351;&#29992;&#23450;&#29702;&#35777;&#26126;&#22120;Prover9&#21644;&#26377;&#38480;&#27169;&#22411;&#26597;&#25214;&#22120;Mace4&#23545;&#36825;&#20123;&#35868;&#39064;&#30340;&#27491;&#30830;&#35299;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#30740;&#31350;&#30340;&#20027;&#35201;&#36755;&#20986;&#26159;&#19968;&#20010;&#30001;100&#20010;&#36923;&#36753;&#35868;&#39064;&#32452;&#25104;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#23545;&#20110;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;ChatGPT&#20165;&#25552;&#20379;&#20102;7%&#30340;&#27491;&#30830;&#31572;&#26696;&#21644;&#29702;&#30001;&#12290;
&lt;/p&gt;
&lt;p&gt;
I shall quantify the logical faults generated by ChatGPT when applied to reasoning tasks. For experiments, I use the 144 puzzles from the library \url{https://users.utcluj.ro/~agroza/puzzles/maloga}~\cite{groza:fol}. The library contains puzzles of various types, including arithmetic puzzles, logical equations, Sudoku-like puzzles, zebra-like puzzles, truth-telling puzzles, grid puzzles, strange numbers, or self-reference puzzles. The correct solutions for these puzzles were checked using the theorem prover Prover9~\cite{mccune2005release} and the finite models finder Mace4~\cite{mccune2003mace4} based on human-modelling in Equational First Order Logic. A first output of this study is the benchmark of 100 logical puzzles. For this dataset ChatGPT provided both correct answer and justification for 7\% only. %, while BARD for 5\%. Since the dataset seems challenging, the researchers are invited to test the dataset on more advanced or tuned models than ChatGPT3.5 with more crafted prompts
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20195;&#29702;&#24314;&#27169;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#27169;&#25311;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#65292;&#24182;&#30740;&#31350;&#19981;&#21516;&#26032;&#38395;&#20379;&#31295;&#31639;&#27861;&#23545;&#22312;&#32447;&#23545;&#35805;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#21019;&#24314;&#36924;&#30495;&#30340;&#20154;&#29289;&#24418;&#35937;&#21644;&#20351;&#29992;&#19981;&#21516;&#30340;&#31639;&#27861;&#65292;&#30740;&#31350;&#21457;&#29616;&#26032;&#38395;&#20379;&#31295;&#31639;&#27861;&#23545;&#20110;&#22609;&#36896;&#31038;&#20132;&#23186;&#20307;&#23545;&#35805;&#26159;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#30340;&#12290;</title><link>http://arxiv.org/abs/2310.05984</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27169;&#25311;&#31038;&#20132;&#23186;&#20307;&#20197;&#35780;&#20272;&#26367;&#20195;&#26032;&#38395;&#20379;&#31295;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Simulating Social Media Using Large Language Models to Evaluate Alternative News Feed Algorithms. (arXiv:2310.05984v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20195;&#29702;&#24314;&#27169;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#27169;&#25311;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#65292;&#24182;&#30740;&#31350;&#19981;&#21516;&#26032;&#38395;&#20379;&#31295;&#31639;&#27861;&#23545;&#22312;&#32447;&#23545;&#35805;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#21019;&#24314;&#36924;&#30495;&#30340;&#20154;&#29289;&#24418;&#35937;&#21644;&#20351;&#29992;&#19981;&#21516;&#30340;&#31639;&#27861;&#65292;&#30740;&#31350;&#21457;&#29616;&#26032;&#38395;&#20379;&#31295;&#31639;&#27861;&#23545;&#20110;&#22609;&#36896;&#31038;&#20132;&#23186;&#20307;&#23545;&#35805;&#26159;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#32463;&#24120;&#22240;&#20026;&#25918;&#22823;&#26377;&#23475;&#35328;&#35770;&#21644;&#38459;&#30861;&#24314;&#35774;&#24615;&#23545;&#35805;&#32780;&#21463;&#21040;&#25209;&#35780;&#12290;&#20294;&#26159;&#35774;&#35745;&#20419;&#36827;&#26356;&#22909;&#23545;&#35805;&#30340;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#26412;&#36136;&#19978;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#22522;&#20110;&#20195;&#29702;&#27169;&#22411;&#30340;&#24314;&#27169;&#26469;&#27169;&#25311;&#31038;&#20132;&#23186;&#20307;&#26159;&#21542;&#26377;&#21161;&#20110;&#30740;&#31350;&#19981;&#21516;&#26032;&#38395;&#20379;&#31295;&#31639;&#27861;&#23545;&#22312;&#32447;&#23545;&#35805;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20351;&#29992;&#32654;&#22269;&#20840;&#22269;&#36873;&#20030;&#30740;&#31350;&#30340;&#25968;&#25454;&#21019;&#24314;&#36924;&#30495;&#30340;&#20154;&#29289;&#24418;&#35937;&#65292;&#20197;&#22635;&#20805;&#27169;&#25311;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35753;&#36825;&#20123;&#20195;&#29702;&#20154;&#38405;&#35835;&#21644;&#20998;&#20139;&#26032;&#38395;&#25991;&#31456;&#65292;&#24182;&#22312;&#19977;&#20010;&#20351;&#29992;&#19981;&#21516;&#26032;&#38395;&#20379;&#31295;&#31639;&#27861;&#30340;&#24179;&#21488;&#19978;&#23545;&#20854;&#23427;&#29992;&#25143;&#30340;&#20449;&#24687;&#28857;&#36190;&#25110;&#35780;&#35770;&#12290;&#22312;&#31532;&#19968;&#20010;&#24179;&#21488;&#19978;&#65292;&#29992;&#25143;&#21487;&#20197;&#30475;&#21040;&#20182;&#20204;&#20851;&#27880;&#30340;&#29992;&#25143;&#20013;&#24471;&#21040;&#26368;&#22810;&#28857;&#36190;&#21644;&#35780;&#35770;&#30340;&#24086;&#23376;&#12290;&#22312;&#31532;&#20108;&#20010;&#24179;&#21488;&#19978;&#65292;&#20182;&#20204;&#21487;&#20197;&#30475;&#21040;&#26469;&#33258;&#25152;&#26377;&#29992;&#25143;&#30340;&#24086;&#23376;&#65292;&#21363;&#20351;&#26159;&#26469;&#33258;&#33258;&#24049;&#32593;&#32476;&#20043;&#22806;&#30340;&#29992;&#25143;&#12290;&#31532;&#19977;&#20010;&#24179;&#21488;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#26725;&#25509;&#8221;&#31639;&#27861;&#65292;&#31361;&#20986;&#26174;&#31034;&#24086;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social media is often criticized for amplifying toxic discourse and discouraging constructive conversations. But designing social media platforms to promote better conversations is inherently challenging. This paper asks whether simulating social media through a combination of Large Language Models (LLM) and Agent-Based Modeling can help researchers study how different news feed algorithms shape the quality of online conversations. We create realistic personas using data from the American National Election Study to populate simulated social media platforms. Next, we prompt the agents to read and share news articles - and like or comment upon each other's messages - within three platforms that use different news feed algorithms. In the first platform, users see the most liked and commented posts from users whom they follow. In the second, they see posts from all users - even those outside their own network. The third platform employs a novel "bridging" algorithm that highlights posts th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#21270;&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;&#20010;&#24615;&#29305;&#24449;&#22312;&#21338;&#24328;&#29702;&#35770;&#20851;&#31995;&#32972;&#26223;&#19979;&#30340;&#28436;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#33021;&#22815;&#23637;&#31034;&#20986;&#28436;&#21270;&#30340;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2310.05976</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#24314;&#31435;&#21512;&#20316;&#34892;&#20026;&#30456;&#20851;&#20010;&#24615;&#29305;&#24449;&#30340;&#36827;&#21270;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
An evolutionary model of personality traits related to cooperative behavior using a large language model. (arXiv:2310.05976v1 [physics.soc-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#21270;&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;&#20010;&#24615;&#29305;&#24449;&#22312;&#21338;&#24328;&#29702;&#35770;&#20851;&#31995;&#32972;&#26223;&#19979;&#30340;&#28436;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#33021;&#22815;&#23637;&#31034;&#20986;&#28436;&#21270;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#23558;&#29983;&#25104;&#27169;&#22411;&#30340;&#20016;&#23500;&#34920;&#36798;&#24615;&#24341;&#20837;&#21040;&#31038;&#20250;&#22522;&#20110;&#26234;&#33021;&#20307;&#30340;&#36827;&#21270;&#27169;&#22411;&#20013;&#65292;&#25506;&#35752;&#22810;&#26679;&#24615;&#21644;&#31038;&#20250;&#32676;&#20307;&#30340;&#36827;&#21270;&#21160;&#24577;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20851;&#27880;&#22312;&#21338;&#24328;&#29702;&#35770;&#20851;&#31995;&#30340;&#32972;&#26223;&#19979;&#20010;&#24615;&#29305;&#24449;&#30340;&#28436;&#21270;&#65292;&#36825;&#26159;&#19968;&#20010;&#30456;&#20114;&#21033;&#30410;&#26045;&#21152;&#24378;&#28872;&#36873;&#25321;&#21387;&#21147;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26234;&#33021;&#20307;&#27169;&#22411;&#65292;&#20854;&#20013;&#21512;&#20316;&#34892;&#20026;&#30456;&#20851;&#20010;&#24615;&#29305;&#24449;&#30340;&#35821;&#35328;&#25551;&#36848;&#34987;&#29992;&#20316;&#22522;&#22240;&#65292;&#20174;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#25552;&#21462;&#30340;&#30830;&#23450;&#24615;&#31574;&#30053;&#26681;&#25454;&#36825;&#20123;&#20010;&#24615;&#29305;&#24449;&#20570;&#20986;&#34892;&#20026;&#20915;&#31574;&#34987;&#29992;&#20316;&#34892;&#20026;&#29305;&#24449;&#12290;&#26681;&#25454;&#24179;&#22343;&#25910;&#30410;&#30340;&#36873;&#25321;&#21644;&#36890;&#36807;&#35831;&#27714;LLM&#23545;&#29238;&#22522;&#22240;&#36827;&#34892;&#30053;&#24494;&#20462;&#25913;&#23454;&#29616;&#22522;&#22240;&#30340;&#31361;&#21464;&#65292;&#25105;&#20204;&#20351;&#32676;&#20307;&#36827;&#21270;&#12290;&#36890;&#36807;&#21021;&#27493;&#23454;&#39564;&#21644;&#20998;&#26512;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;&#36825;&#26679;&#30340;&#27169;&#22411;&#30830;&#23454;&#21487;&#20197;&#23637;&#31034;&#28436;&#21270;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper aims to shed light on the evolutionary dynamics of diverse and social populations by introducing the rich expressiveness of generative models into the trait expression of social agent-based evolutionary models. Specifically, we focus on the evolution of personality traits in the context of a game-theoretic relationship as a situation in which inter-individual interests exert strong selection pressures. We construct an agent model in which linguistic descriptions of personality traits related to cooperative behavior are used as genes. The deterministic strategies extracted from Large Language Model (LLM) that make behavioral decisions based on these personality traits are used as behavioral traits. The population is evolved according to selection based on average payoff and mutation of genes by asking LLM to slightly modify the parent gene toward cooperative or selfish. Through preliminary experiments and analyses, we clarify that such a model can indeed exhibit the evolution
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#22411;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#21270;&#33016;&#37096;X&#20809;&#25253;&#21578;&#29983;&#25104;&#22120;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#20108;&#20803;&#20998;&#31867;&#27169;&#22411;&#26816;&#27979;&#22810;&#31181;&#24322;&#24120;&#65292;&#22312;&#21333;&#20010;&#22270;&#20687;&#20013;&#36741;&#21161;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#24037;&#20316;&#12290;&#35813;&#31995;&#32479;&#23558;&#25918;&#23556;&#23398;&#24322;&#24120;&#26816;&#27979;&#38480;&#21046;&#20026;&#24515;&#33039;&#32933;&#22823;&#12289;&#32954;&#31215;&#28082;&#21644;&#23454;&#21464;&#65292;&#24182;&#36890;&#36807;&#19977;&#20010;&#27493;&#39588;&#29983;&#25104;&#25918;&#23556;&#23398;&#25253;&#21578;&#65306;&#22270;&#20687;&#39044;&#22788;&#29702;&#12289;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26816;&#27979;&#24322;&#24120;&#21644;&#29983;&#25104;&#25253;&#21578;&#12290;</title><link>http://arxiv.org/abs/2310.05969</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#27169;&#22411;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#33258;&#21160;&#21270;&#33016;&#37096;X&#20809;&#25253;&#21578;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Automated Chest X-Ray Report Generator Using Multi-Model Deep Learning Approach. (arXiv:2310.05969v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05969
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#22411;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#21270;&#33016;&#37096;X&#20809;&#25253;&#21578;&#29983;&#25104;&#22120;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#20108;&#20803;&#20998;&#31867;&#27169;&#22411;&#26816;&#27979;&#22810;&#31181;&#24322;&#24120;&#65292;&#22312;&#21333;&#20010;&#22270;&#20687;&#20013;&#36741;&#21161;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#24037;&#20316;&#12290;&#35813;&#31995;&#32479;&#23558;&#25918;&#23556;&#23398;&#24322;&#24120;&#26816;&#27979;&#38480;&#21046;&#20026;&#24515;&#33039;&#32933;&#22823;&#12289;&#32954;&#31215;&#28082;&#21644;&#23454;&#21464;&#65292;&#24182;&#36890;&#36807;&#19977;&#20010;&#27493;&#39588;&#29983;&#25104;&#25918;&#23556;&#23398;&#25253;&#21578;&#65306;&#22270;&#20687;&#39044;&#22788;&#29702;&#12289;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26816;&#27979;&#24322;&#24120;&#21644;&#29983;&#25104;&#25253;&#21578;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38405;&#35835;&#21644;&#35299;&#35835;&#33016;&#37096;X&#20809;&#22270;&#20687;&#26159;&#22823;&#22810;&#25968;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#20363;&#34892;&#24037;&#20316;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#23545;&#20110;&#32463;&#39564;&#26368;&#20016;&#23500;&#30340;&#21307;&#29983;&#26469;&#35828;&#65292;&#36825;&#20173;&#28982;&#21487;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#22411;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#21270;&#33016;&#37096;X&#20809;&#25253;&#21578;&#29983;&#25104;&#31995;&#32479;&#65292;&#26088;&#22312;&#36741;&#21161;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#24037;&#20316;&#12290;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#30340;&#22522;&#26412;&#24605;&#24819;&#26159;&#21033;&#29992;&#22810;&#20010;&#20108;&#20803;&#20998;&#31867;&#27169;&#22411;&#26469;&#26816;&#27979;&#22810;&#31181;&#24322;&#24120;&#65292;&#27599;&#20010;&#27169;&#22411;&#36127;&#36131;&#26816;&#27979;&#19968;&#31181;&#24322;&#24120;&#65292;&#22312;&#21333;&#20010;&#22270;&#20687;&#20013;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#25918;&#23556;&#23398;&#24322;&#24120;&#26816;&#27979;&#38480;&#21046;&#20026;&#24515;&#33039;&#32933;&#22823;&#12289;&#32954;&#31215;&#28082;&#21644;&#23454;&#21464;&#12290;&#31995;&#32479;&#36890;&#36807;&#25191;&#34892;&#20197;&#19979;&#19977;&#20010;&#27493;&#39588;&#29983;&#25104;&#25918;&#23556;&#23398;&#25253;&#21578;&#65306;&#22270;&#20687;&#39044;&#22788;&#29702;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26816;&#27979;&#24322;&#24120;&#65292;&#24182;&#29983;&#25104;&#25253;&#21578;&#12290;&#22270;&#20687;&#39044;&#22788;&#29702;&#27493;&#39588;&#30340;&#30446;&#30340;&#26159;&#36890;&#36807;&#23558;&#20854;&#32553;&#25918;&#20026;128x128&#20687;&#32032;&#24182;&#20998;&#21106;&#25104;&#19977;&#20010;&#37096;&#20998;&#26469;&#20351;&#36755;&#20837;&#26631;&#20934;&#21270;&#65292;&#20998;&#21035;&#28085;&#30422;&#19978;&#37096;&#12289;&#19979;&#37096;&#21644;&#20013;&#37096;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reading and interpreting chest X-ray images is one of the most radiologist's routines. However, it still can be challenging, even for the most experienced ones. Therefore, we proposed a multi-model deep learning-based automated chest X-ray report generator system designed to assist radiologists in their work. The basic idea of the proposed system is by utilizing multi binary-classification models for detecting multi abnormalities, with each model responsible for detecting one abnormality, in a single image. In this study, we limited the radiology abnormalities detection to only cardiomegaly, lung effusion, and consolidation. The system generates a radiology report by performing the following three steps: image pre-processing, utilizing deep learning models to detect abnormalities, and producing a report. The aim of the image pre-processing step is to standardize the input by scaling it to 128x128 pixels and slicing it into three segments, which covers the upper, lower, and middle parts
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#26799;&#24230;&#25351;&#32441;&#25915;&#20987;&#21487;&#20197;&#36731;&#26494;&#25171;&#30772;&#21442;&#19982;&#32773;&#21311;&#21517;&#21270;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#25552;&#20379;&#23454;&#38469;&#38450;&#24481;&#12290;</title><link>http://arxiv.org/abs/2310.05960</link><description>&lt;p&gt;
&#25351;&#32441;&#25915;&#20987;&#65306;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23458;&#25143;&#31471;&#21435;&#21311;&#21517;&#21270;
&lt;/p&gt;
&lt;p&gt;
Fingerprint Attack: Client De-Anonymization in Federated Learning. (arXiv:2310.05960v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#26799;&#24230;&#25351;&#32441;&#25915;&#20987;&#21487;&#20197;&#36731;&#26494;&#25171;&#30772;&#21442;&#19982;&#32773;&#21311;&#21517;&#21270;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#25552;&#20379;&#23454;&#38469;&#38450;&#24481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20801;&#35768;&#22312;&#21442;&#19982;&#32773;&#19981;&#30456;&#20449;&#20013;&#22830;&#26381;&#21153;&#22120;&#21644;&#24444;&#27492;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#21327;&#20316;&#35757;&#32451;&#65292;&#32780;&#26080;&#38656;&#20849;&#20139;&#25968;&#25454;&#12290;&#36890;&#36807;&#30830;&#20445;&#21442;&#19982;&#32773;&#19982;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#36890;&#20449;&#32463;&#36807;&#28151;&#27927;&#65292;&#23558;&#21442;&#19982;&#32773;&#36523;&#20221;&#19982;&#20854;&#25968;&#25454;&#20998;&#31163;&#65292;&#36824;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#38544;&#31169;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#26799;&#24230;&#25351;&#32441;&#25915;&#20987;&#26469;&#26816;&#39564;&#36825;&#31181;&#38450;&#24481;&#26159;&#21542;&#36275;&#20197;&#20445;&#35777;&#21311;&#21517;&#24615;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#35821;&#35328;&#35821;&#26009;&#24211;&#19978;&#30340;&#32852;&#37030;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#35777;&#30740;&#31350;&#20013;&#23637;&#31034;&#20102;&#26799;&#24230;&#32858;&#31867;&#21487;&#20197;&#36731;&#26494;&#22320;&#25171;&#30772;&#21311;&#21517;&#21270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#25552;&#20379;&#23545;&#25105;&#20204;&#30340;&#25351;&#32441;&#25915;&#20987;&#30340;&#23454;&#38469;&#38450;&#24481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning allows collaborative training without data sharing in settings where participants do not trust the central server and one another. Privacy can be further improved by ensuring that communication between the participants and the server is anonymized through a shuffle; decoupling the participant identity from their data. This paper seeks to examine whether such a defense is adequate to guarantee anonymity, by proposing a novel fingerprinting attack over gradients sent by the participants to the server. We show that clustering of gradients can easily break the anonymization in an empirical study of learning federated language models on two language corpora. We then show that training with differential privacy can provide a practical defense against our fingerprint attack.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GNN&#30340;&#20837;&#20405;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#27969;&#22270;&#20316;&#20026;&#32593;&#32476;&#34920;&#31034;&#24418;&#24335;&#65292;&#25552;&#20379;&#20102;&#30456;&#20851;&#30340;&#25299;&#25169;&#20449;&#24687;&#65292;&#21253;&#25324;&#24694;&#24847;&#34892;&#20026;&#27169;&#24335;&#12289;&#22810;&#27493;&#25915;&#20987;&#38454;&#27573;&#20043;&#38388;&#30340;&#20851;&#31995;&#20197;&#21450;&#27450;&#39575;&#21644;&#39044;&#27450;&#39575;&#25915;&#20987;&#32773;&#27963;&#21160;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36890;&#36807;&#23884;&#20837;&#33410;&#28857;&#29305;&#24449;&#21644;&#23398;&#20064;&#30456;&#20851;&#30340;&#25915;&#20987;&#27169;&#24335;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23545;&#36890;&#20449;&#27969;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#20998;&#37197;&#24694;&#24847;&#31243;&#24230;&#24471;&#20998;&#12290;&#35813;&#26041;&#27861;&#36824;&#25351;&#20986;&#20102;&#20256;&#32479;&#35780;&#20272;&#36807;&#31243;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#25968;&#25454;&#27844;&#28431;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2310.05956</link><description>&lt;p&gt;
GNN-based&#20837;&#20405;&#26816;&#27979;&#30340;&#39640;&#25928;&#32593;&#32476;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Efficient Network Representation for GNN-based Intrusion Detection. (arXiv:2310.05956v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GNN&#30340;&#20837;&#20405;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#27969;&#22270;&#20316;&#20026;&#32593;&#32476;&#34920;&#31034;&#24418;&#24335;&#65292;&#25552;&#20379;&#20102;&#30456;&#20851;&#30340;&#25299;&#25169;&#20449;&#24687;&#65292;&#21253;&#25324;&#24694;&#24847;&#34892;&#20026;&#27169;&#24335;&#12289;&#22810;&#27493;&#25915;&#20987;&#38454;&#27573;&#20043;&#38388;&#30340;&#20851;&#31995;&#20197;&#21450;&#27450;&#39575;&#21644;&#39044;&#27450;&#39575;&#25915;&#20987;&#32773;&#27963;&#21160;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36890;&#36807;&#23884;&#20837;&#33410;&#28857;&#29305;&#24449;&#21644;&#23398;&#20064;&#30456;&#20851;&#30340;&#25915;&#20987;&#27169;&#24335;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23545;&#36890;&#20449;&#27969;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#20998;&#37197;&#24694;&#24847;&#31243;&#24230;&#24471;&#20998;&#12290;&#35813;&#26041;&#27861;&#36824;&#25351;&#20986;&#20102;&#20256;&#32479;&#35780;&#20272;&#36807;&#31243;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#25968;&#25454;&#27844;&#28431;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#65292;&#32593;&#32476;&#25915;&#20987;&#30340;&#25968;&#37327;&#19981;&#26029;&#22686;&#38271;&#65292;&#32473;&#32463;&#27982;&#21644;&#38544;&#31169;&#36896;&#25104;&#20005;&#37325;&#25439;&#23475;&#65292;&#36825;&#25581;&#31034;&#20102;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#26041;&#27861;&#22312;&#39044;&#38450;&#32593;&#32476;&#25915;&#20987;&#21644;&#38477;&#20302;&#39118;&#38505;&#26041;&#38754;&#30340;&#38656;&#27714;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32593;&#32476;&#34920;&#31034;&#24418;&#24335;&#65292;&#21363;&#27969;&#22270;&#65292;&#26088;&#22312;&#20026;&#20837;&#20405;&#26816;&#27979;&#20219;&#21153;&#25552;&#20379;&#30456;&#20851;&#30340;&#25299;&#25169;&#20449;&#24687;&#65292;&#22914;&#24694;&#24847;&#34892;&#20026;&#27169;&#24335;&#12289;&#22810;&#27493;&#25915;&#20987;&#38454;&#27573;&#20043;&#38388;&#30340;&#20851;&#31995;&#20197;&#21450;&#27450;&#39575;&#21644;&#39044;&#27450;&#39575;&#25915;&#20987;&#32773;&#27963;&#21160;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#21033;&#29992;&#25152;&#25552;&#20986;&#30340;&#22270;&#32467;&#26500;&#23545;&#36890;&#20449;&#27969;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#20026;&#20854;&#20998;&#37197;&#24694;&#24847;&#31243;&#24230;&#24471;&#20998;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#19977;&#20010;&#20027;&#35201;&#27493;&#39588;&#65292;&#26088;&#22312;&#23884;&#20837;&#33410;&#28857;&#29305;&#24449;&#24182;&#20174;&#32593;&#32476;&#34920;&#31034;&#20013;&#23398;&#20064;&#30456;&#20851;&#30340;&#25915;&#20987;&#27169;&#24335;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#20256;&#32479;&#35780;&#20272;&#36807;&#31243;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#25968;&#25454;&#27844;&#28431;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
The last decades have seen a growth in the number of cyber-attacks with severe economic and privacy damages, which reveals the need for network intrusion detection approaches to assist in preventing cyber-attacks and reducing their risks. In this work, we propose a novel network representation as a graph of flows that aims to provide relevant topological information for the intrusion detection task, such as malicious behavior patterns, the relation between phases of multi-step attacks, and the relation between spoofed and pre-spoofed attackers activities. In addition, we present a Graph Neural Network (GNN) based framework responsible for exploiting the proposed graph structure to classify communication flows by assigning them a maliciousness score. The framework comprises three main steps that aim to embed nodes features and learn relevant attack patterns from the network representation. Finally, we highlight a potential data leakage issue with classical evaluation procedures and sugg
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#26041;&#27861;&#26469;&#38477;&#20302;&#33258;&#21160;&#39550;&#39542;&#24863;&#30693;&#20013;&#30340;&#35823;&#25253;&#29575;&#12290;&#36890;&#36807;&#20351;&#29992;&#22810;&#24863;&#30693;&#21644;&#22810;&#27169;&#24577;&#26041;&#27861;&#65292;&#20197;&#21450;&#32771;&#34385;&#30446;&#26631;&#30340;&#20284;&#28982;&#20989;&#25968;&#21644;&#20808;&#39564;&#27010;&#29575;&#65292;&#35813;&#26041;&#27861;&#22312;&#20943;&#23569;&#35823;&#25253;&#29575;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#22312;KITTI&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.05951</link><description>&lt;p&gt;
&#20351;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#22312;&#33258;&#21160;&#39550;&#39542;&#24863;&#30693;&#20013;&#38477;&#20302;&#35823;&#25253;&#29575;
&lt;/p&gt;
&lt;p&gt;
Reducing the False Positive Rate Using Bayesian Inference in Autonomous Driving Perception. (arXiv:2310.05951v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#26041;&#27861;&#26469;&#38477;&#20302;&#33258;&#21160;&#39550;&#39542;&#24863;&#30693;&#20013;&#30340;&#35823;&#25253;&#29575;&#12290;&#36890;&#36807;&#20351;&#29992;&#22810;&#24863;&#30693;&#21644;&#22810;&#27169;&#24577;&#26041;&#27861;&#65292;&#20197;&#21450;&#32771;&#34385;&#30446;&#26631;&#30340;&#20284;&#28982;&#20989;&#25968;&#21644;&#20808;&#39564;&#27010;&#29575;&#65292;&#35813;&#26041;&#27861;&#22312;&#20943;&#23569;&#35823;&#25253;&#29575;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#22312;KITTI&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#35782;&#21035;&#26159;&#33258;&#21160;&#21644;&#26234;&#33021;&#36710;&#36742;&#24863;&#30693;&#31995;&#32479;&#20013;&#20851;&#38190;&#30340;&#19968;&#27493;&#65292;&#36825;&#24050;&#32463;&#24471;&#21040;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#24037;&#20316;&#30340;&#35777;&#26126;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22810;&#24863;&#30693;&#21644;&#22810;&#27169;&#24577;&#26041;&#27861;&#26469;&#25506;&#32034;&#30446;&#26631;&#35782;&#21035;&#65292;&#26088;&#22312;&#38477;&#20302;&#35823;&#25253;&#29575;&#65288;FPR&#65289;&#12290;&#30001;&#20110;&#35823;&#25253;&#23545;&#35937;&#30340;&#38169;&#35823;&#20998;&#31867;&#21487;&#33021;&#23548;&#33268;&#20107;&#25925;&#65292;&#38477;&#20302;&#35823;&#25253;&#29575;&#22312;&#24863;&#30693;&#31995;&#32479;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#31574;&#30053;&#26469;&#38477;&#20302;&#35823;&#25253;&#29575;&#65292;&#20854;&#20013;&#23558;&#20284;&#28982;&#20989;&#25968;&#35270;&#20026;&#39640;&#26031;&#26680;&#23494;&#24230;&#20272;&#35745;&#30340;&#32047;&#31215;&#20998;&#24067;&#20989;&#25968;&#65292;&#23558;&#20808;&#39564;&#27010;&#29575;&#35270;&#20026;&#24402;&#19968;&#21270;&#30452;&#26041;&#22270;&#30340;&#32047;&#31215;&#20989;&#25968;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;KITTI&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#28145;&#24230;&#32593;&#32476;&#65288;DenseNet&#12289;NasNet&#21644;EfficientNet&#65289;&#21644;&#26368;&#36817;&#30340;3D&#28857;&#20113;&#32593;&#32476;&#65288;PointNet&#21644;PointNet++&#65289;&#36827;&#34892;&#39564;&#35777;&#65292;&#32771;&#34385;&#20102;&#19977;&#20010;&#30446;&#26631;&#31867;&#21035;&#65288;&#27773;&#36710;&#12289;&#33258;&#34892;&#36710;&#12289;&#34892;&#20154;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Object recognition is a crucial step in perception systems for autonomous and intelligent vehicles, as evidenced by the numerous research works in the topic. In this paper, object recognition is explored by using multisensory and multimodality approaches, with the intention of reducing the false positive rate (FPR). The reduction of the FPR becomes increasingly important in perception systems since the misclassification of an object can potentially cause accidents. In particular, this work presents a strategy through Bayesian inference to reduce the FPR considering the likelihood function as a cumulative distribution function from Gaussian kernel density estimations, and the prior probabilities as cumulative functions of normalized histograms. The validation of the proposed methodology is performed on the KITTI dataset using deep networks (DenseNet, NasNet, and EfficientNet), and recent 3D point cloud networks (PointNet, and PintNet++), by considering three object-categories (cars, cyc
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#32593;&#32476;&#38450;&#24481;&#25112;&#26415;&#30340;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26469;&#36827;&#34892;&#33258;&#20027;&#32593;&#32476;&#38450;&#24481;&#65292;&#35777;&#26126;&#20102;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#33021;&#22815;&#23545;&#25239;&#21508;&#31181;&#23041;&#32961;&#12290;</title><link>http://arxiv.org/abs/2310.05939</link><description>&lt;p&gt;
&#20174;&#38646;&#24320;&#22987;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#32593;&#32476;&#38450;&#24481;&#25112;&#26415;
&lt;/p&gt;
&lt;p&gt;
Learning Cyber Defence Tactics from Scratch with Multi-Agent Reinforcement Learning. (arXiv:2310.05939v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05939
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#32593;&#32476;&#38450;&#24481;&#25112;&#26415;&#30340;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26469;&#36827;&#34892;&#33258;&#20027;&#32593;&#32476;&#38450;&#24481;&#65292;&#35777;&#26126;&#20102;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#33021;&#22815;&#23545;&#25239;&#21508;&#31181;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#20026;&#33258;&#20027;&#32593;&#32476;&#38450;&#24481;&#35299;&#20915;&#26041;&#26696;&#30340;&#35774;&#35745;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;&#22312;&#35745;&#31639;&#26426;&#32593;&#32476;&#38450;&#24481;&#35282;&#33394;&#20013;&#65292;&#26234;&#33021;&#20195;&#29702;&#22242;&#38431;&#21487;&#33021;&#23637;&#29616;&#20986;&#20445;&#25252;&#32593;&#32476;&#21644;&#21160;&#33021;&#36164;&#20135;&#30340;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;&#22312;&#27169;&#25311;&#30340;&#28216;&#25103;&#29615;&#22659;&#20013;&#65292;&#20195;&#29702;&#26681;&#25454;&#20854;&#22312;&#20027;&#26426;&#38450;&#24481;&#22330;&#26223;&#20013;&#20849;&#21516;&#20943;&#36731;&#25915;&#20987;&#32773;&#27963;&#21160;&#30340;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#12290;&#23432;&#26041;&#31995;&#32479;&#20250;&#38754;&#23545;&#33021;&#30772;&#22351;&#32593;&#32476;&#26426;&#23494;&#24615;&#12289;&#23436;&#25972;&#24615;&#21644;&#21487;&#29992;&#24615;&#30340;&#21551;&#21457;&#24335;&#25915;&#20987;&#32773;&#12290;&#23545;&#27604;&#20102;&#22522;&#20110;&#20215;&#20540;&#30340;&#29420;&#31435;&#23398;&#20064;&#21644;&#38598;&#20013;&#22521;&#35757;&#20998;&#25955;&#25191;&#34892;&#30340;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21457;&#29616;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#32988;&#36807;&#31616;&#21333;&#30340;&#22810;&#26234;&#33021;&#20307;&#21551;&#21457;&#24335;&#38450;&#24481;&#32773;&#12290;&#36825;&#39033;&#24037;&#20316;&#34920;&#26126;&#65292;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#33021;&#22815;&#23398;&#20064;&#26377;&#25928;&#30340;&#32593;&#32476;&#38450;&#24481;&#25112;&#26415;&#65292;&#20197;&#25269;&#24481;&#21508;&#31181;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in deep learning techniques have opened new possibilities for designing solutions for autonomous cyber defence. Teams of intelligent agents in computer network defence roles may reveal promising avenues to safeguard cyber and kinetic assets. In a simulated game environment, agents are evaluated on their ability to jointly mitigate attacker activity in host-based defence scenarios. Defender systems are evaluated against heuristic attackers with the goals of compromising network confidentiality, integrity, and availability. Value-based Independent Learning and Centralized Training Decentralized Execution (CTDE) cooperative Multi-Agent Reinforcement Learning (MARL) methods are compared revealing that both approaches outperform a simple multi-agent heuristic defender. This work demonstrates the ability of cooperative MARL to learn effective cyber defence tactics against varied threats.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;&#33310;&#36424;&#21363;&#20852;&#35782;&#21035;&#30340;&#32452;&#20214;&#27880;&#24847;&#21147;&#32593;&#32476; (CANet)&#12290;&#36890;&#36807;&#22810;&#23618;&#27425;&#30340;&#34701;&#21512;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#20998;&#26512;&#27599;&#31181;&#27169;&#24577;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30456;&#27604;&#22522;&#20934;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.05938</link><description>&lt;p&gt;
&#32452;&#20214;&#27880;&#24847;&#21147;&#32593;&#32476;&#29992;&#20110;&#22810;&#27169;&#24577;&#33310;&#36424;&#21363;&#20852;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Component attention network for multimodal dance improvisation recognition. (arXiv:2310.05938v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;&#33310;&#36424;&#21363;&#20852;&#35782;&#21035;&#30340;&#32452;&#20214;&#27880;&#24847;&#21147;&#32593;&#32476; (CANet)&#12290;&#36890;&#36807;&#22810;&#23618;&#27425;&#30340;&#34701;&#21512;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#20998;&#26512;&#27599;&#31181;&#27169;&#24577;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30456;&#27604;&#22522;&#20934;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33310;&#36424;&#21363;&#20852;&#26159;&#33402;&#26415;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#27963;&#36291;&#30740;&#31350;&#35838;&#39064;&#12290;&#30001;&#20110;&#20854;&#29420;&#29305;&#30340;&#21160;&#24577;&#29305;&#24615;&#65292;&#21363;&#20852;&#33310;&#36424;&#30340;&#21160;&#20316;&#20998;&#26512;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25968;&#25454;&#39537;&#21160;&#30340;&#33310;&#36424;&#21160;&#20316;&#20998;&#26512;&#65292;&#21253;&#25324;&#35782;&#21035;&#21644;&#29983;&#25104;&#65292;&#36890;&#24120;&#38480;&#20110;&#39592;&#39612;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#20854;&#20182;&#27169;&#24577;&#30340;&#25968;&#25454;&#65292;&#22914;&#38899;&#39057;&#65292;&#21487;&#20197;&#34987;&#35760;&#24405;&#24182;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#20135;&#29983;&#22909;&#22788;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#33310;&#36424;&#21363;&#20852;&#32972;&#26223;&#19979;&#24212;&#29992;&#21644;&#24615;&#33021;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#26041;&#27861;&#29992;&#20110;&#20154;&#20307;&#21160;&#20316;&#35782;&#21035;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#65292;&#32452;&#20214;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;CANet&#65289;&#65292;&#29992;&#20110;&#19977;&#20010;&#23618;&#27425;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#65306;1&#65289;CANet&#36827;&#34892;&#29305;&#24449;&#34701;&#21512;&#65292;2&#65289;CANet&#21644;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#36827;&#34892;&#27169;&#22411;&#34701;&#21512;&#65292;&#20197;&#21450;3&#65289;&#37319;&#29992;&#25237;&#31080;&#31574;&#30053;&#36827;&#34892;&#21518;&#26399;&#34701;&#21512;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#20197;&#20998;&#26512;&#19981;&#21516;&#34701;&#21512;&#26041;&#27861;&#20013;&#27599;&#31181;&#27169;&#24577;&#30340;&#24433;&#21709;&#65292;&#24182;&#21306;&#20998;&#20986;&#20851;&#38190;&#30340;&#26102;&#38388;&#25110;&#32452;&#20214;&#29305;&#24449;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#20248;&#20110;&#20004;&#31181;&#22522;&#20934;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#20854;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dance improvisation is an active research topic in the arts. Motion analysis of improvised dance can be challenging due to its unique dynamics. Data-driven dance motion analysis, including recognition and generation, is often limited to skeletal data. However, data of other modalities, such as audio, can be recorded and benefit downstream tasks. This paper explores the application and performance of multimodal fusion methods for human motion recognition in the context of dance improvisation. We propose an attention-based model, component attention network (CANet), for multimodal fusion on three levels: 1) feature fusion with CANet, 2) model fusion with CANet and graph convolutional network (GCN), and 3) late fusion with a voting strategy. We conduct thorough experiments to analyze the impact of each modality in different fusion methods and distinguish critical temporal or component features. We show that our proposed model outperforms the two baseline methods, demonstrating its potenti
&lt;/p&gt;</description></item><item><title>DF-3DFace&#26159;&#19968;&#31181;&#37319;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#38899;&#21040;3D&#38754;&#37096;&#32593;&#26684;&#21512;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#20934;&#30830;&#21516;&#27493;&#21767;&#37096;&#21160;&#20316;&#65292;&#24182;&#32508;&#21512;&#20102;&#36523;&#20221;&#12289;&#23039;&#21183;&#21644;&#38754;&#37096;&#36816;&#21160;&#12290;</title><link>http://arxiv.org/abs/2310.05934</link><description>&lt;p&gt;
DF-3DFace: &#19968;&#23545;&#22810;&#35821;&#38899;&#21516;&#27493;&#30340;3D&#38754;&#37096;&#21160;&#30011;&#65292;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DF-3DFace: One-to-Many Speech Synchronized 3D Face Animation with Diffusion. (arXiv:2310.05934v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05934
&lt;/p&gt;
&lt;p&gt;
DF-3DFace&#26159;&#19968;&#31181;&#37319;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#38899;&#21040;3D&#38754;&#37096;&#32593;&#26684;&#21512;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#20934;&#30830;&#21516;&#27493;&#21767;&#37096;&#21160;&#20316;&#65292;&#24182;&#32508;&#21512;&#20102;&#36523;&#20221;&#12289;&#23039;&#21183;&#21644;&#38754;&#37096;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#20854;&#26681;&#25454;&#35821;&#38899;&#21019;&#24314;&#36924;&#30495;&#21644;&#34920;&#36798;&#20016;&#23500;&#30340;3D&#38754;&#37096;&#21160;&#30011;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#30001;&#35821;&#38899;&#39537;&#21160;&#30340;3D&#38754;&#37096;&#21160;&#30011;&#24050;&#32463;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;&#23454;&#29616;&#19982;&#35821;&#38899;&#21516;&#27493;&#30340;&#20934;&#30830;&#38754;&#37096;&#21160;&#20316;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#35821;&#38899;&#21040;3D&#38754;&#37096;&#32508;&#21512;&#30340;&#19968;&#23545;&#22810;&#29305;&#24615;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#65306;&#34429;&#28982;&#21767;&#37096;&#19982;&#35821;&#38899;&#20869;&#23481;&#20934;&#30830;&#21516;&#27493;&#65292;&#20294;&#19982;&#35821;&#38899;&#30456;&#20851;&#36816;&#21160;&#20043;&#22806;&#30340;&#20854;&#20182;&#38754;&#37096;&#23646;&#24615;&#22312;&#35821;&#38899;&#26041;&#38754;&#26159;&#21487;&#21464;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#21333;&#19968;&#35821;&#38899;&#20013;&#38754;&#37096;&#23646;&#24615;&#28508;&#22312;&#30340;&#24046;&#24322;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DF-3DFace&#65292;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#38899;&#21040;3D&#38754;&#37096;&#32593;&#26684;&#21512;&#25104;&#26041;&#27861;&#12290;DF-3DFace&#26681;&#25454;&#25193;&#25955;&#27169;&#22411;&#25429;&#25417;&#20102;&#35821;&#38899;&#21644;3D&#38754;&#37096;&#20043;&#38388;&#22797;&#26434;&#30340;&#19968;&#23545;&#22810;&#20851;&#31995;&#12290;&#23427;&#36890;&#36807;&#21033;&#29992;&#38899;&#39057;-&#32593;&#26684;&#21516;&#27493;&#21644;&#25513;&#34109;&#26465;&#20214;&#21516;&#26102;&#23454;&#29616;&#20102;&#23545;&#40784;&#30340;&#21767;&#37096;&#21160;&#20316;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36824;&#20849;&#21516;&#24314;&#27169;&#36523;&#20221;&#21644;&#23039;&#21183;&#65292;&#38500;&#20102;&#38754;&#37096;&#36816;&#21160;&#20043;&#22806;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech-driven 3D facial animation has gained significant attention for its ability to create realistic and expressive facial animations in 3D space based on speech. Learning-based methods have shown promising progress in achieving accurate facial motion synchronized with speech. However, one-to-many nature of speech-to-3D facial synthesis has not been fully explored: while the lip accurately synchronizes with the speech content, other facial attributes beyond speech-related motions are variable with respect to the speech. To account for the potential variance in the facial attributes within a single speech, we propose DF-3DFace, a diffusion-driven speech-to-3D face mesh synthesis. DF-3DFace captures the complex one-to-many relationships between speech and 3D face based on diffusion. It concurrently achieves aligned lip motion by exploiting audio-mesh synchronization and masked conditioning. Furthermore, the proposed method jointly models identity and pose in addition to facial motions 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#22902;&#29275;&#20859;&#27542;&#39046;&#22495;&#28857;&#23545;&#28857;&#33021;&#28304;&#20132;&#26131;&#26041;&#27861;&#65292;&#36890;&#36807;&#35813;&#26041;&#27861;&#21487;&#20197;&#38477;&#20302;&#30005;&#21147;&#25104;&#26412;&#21644;&#23792;&#20540;&#38656;&#27714;&#65292;&#21516;&#26102;&#22686;&#21152;&#33021;&#28304;&#38144;&#21806;&#12290;</title><link>http://arxiv.org/abs/2310.05932</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#22902;&#29275;&#20859;&#27542;&#39046;&#22495;&#28857;&#23545;&#28857;&#33021;&#28304;&#20132;&#26131;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Multi-Agent Systems Approach for Peer-to-Peer Energy Trading in Dairy Farming. (arXiv:2310.05932v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05932
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#22902;&#29275;&#20859;&#27542;&#39046;&#22495;&#28857;&#23545;&#28857;&#33021;&#28304;&#20132;&#26131;&#26041;&#27861;&#65292;&#36890;&#36807;&#35813;&#26041;&#27861;&#21487;&#20197;&#38477;&#20302;&#30005;&#21147;&#25104;&#26412;&#21644;&#23792;&#20540;&#38656;&#27714;&#65292;&#21516;&#26102;&#22686;&#21152;&#33021;&#28304;&#38144;&#21806;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23454;&#29616;&#25152;&#38656;&#30340;&#30899;&#25490;&#25918;&#20943;&#23569;&#30446;&#26631;&#65292;&#25972;&#21512;&#21487;&#20877;&#29983;&#33021;&#28304;&#21644;&#21152;&#24555;&#28857;&#23545;&#28857;&#33021;&#28304;&#20132;&#26131;&#30340;&#37319;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#23545;&#20110;&#33021;&#28304;&#23494;&#38598;&#22411;&#30340;&#22902;&#29275;&#20859;&#27542;&#19994;&#23588;&#20026;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#25972;&#21512;&#21487;&#20877;&#29983;&#33021;&#28304;&#21644;&#28857;&#23545;&#28857;&#20132;&#26131;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#30340;&#28857;&#23545;&#28857;&#22902;&#29275;&#20859;&#27542;&#22330;&#33021;&#28304;&#27169;&#25311;&#22120;(MAPDES)&#65292;&#20351;&#22902;&#29275;&#22330;&#33021;&#22815;&#21442;&#19982;&#28857;&#23545;&#28857;&#24066;&#22330;&#12290;&#19982;&#27809;&#26377;&#28857;&#23545;&#28857;&#20132;&#26131;&#30340;&#22522;&#20934;&#24773;&#26223;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#23558;&#30005;&#21147;&#25104;&#26412;&#21644;&#23792;&#20540;&#38656;&#27714;&#20998;&#21035;&#38477;&#20302;&#20102;&#32422;30%&#21644;24%&#65292;&#21516;&#26102;&#33021;&#28304;&#38144;&#21806;&#22686;&#21152;&#20102;37%&#12290;&#36825;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
To achieve desired carbon emission reductions, integrating renewable generation and accelerating the adoption of peer-to-peer energy trading is crucial. This is especially important for energy-intensive farming, like dairy farming. However, integrating renewables and peer-to-peer trading presents challenges. To address this, we propose the Multi-Agent Peer-to-Peer Dairy Farm Energy Simulator (MAPDES), enabling dairy farms to participate in peer-to-peer markets. Our strategy reduces electricity costs and peak demand by approximately 30% and 24% respectively, while increasing energy sales by 37% compared to the baseline scenario without P2P trading. This demonstrates the effectiveness of our approach.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#27454;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#65292;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#26816;&#27979;&#30058;&#33540;&#30149;&#23475;&#24182;&#25552;&#20379;&#27835;&#30103;&#24314;&#35758;&#65292;&#20197;&#24110;&#21161;&#23612;&#27850;&#23572;&#20892;&#27665;&#35299;&#20915;&#20256;&#32479;&#20892;&#19994;&#26041;&#27861;&#21644;&#20892;&#20316;&#29289;&#30149;&#23475;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.05929</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30058;&#33540;&#30149;&#23475;&#26816;&#27979;&#19982;&#27835;&#30103;&#24314;&#35758;&#30340;&#25163;&#26426;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deep Learning based Tomato Disease Detection and Remedy Suggestions using Mobile Application. (arXiv:2310.05929v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05929
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#27454;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#65292;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#26816;&#27979;&#30058;&#33540;&#30149;&#23475;&#24182;&#25552;&#20379;&#27835;&#30103;&#24314;&#35758;&#65292;&#20197;&#24110;&#21161;&#23612;&#27850;&#23572;&#20892;&#27665;&#35299;&#20915;&#20256;&#32479;&#20892;&#19994;&#26041;&#27861;&#21644;&#20892;&#20316;&#29289;&#30149;&#23475;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#35745;&#31639;&#26426;&#31995;&#32479;&#65292;&#29992;&#20110;&#24110;&#21161;&#23454;&#34892;&#20256;&#32479;&#20892;&#19994;&#26041;&#27861;&#24182;&#19988;&#23545;&#20892;&#20316;&#29289;&#30149;&#23475;&#30340;&#20892;&#27665;&#32570;&#20047;&#20892;&#19994;&#19987;&#23478;&#21672;&#35810;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#35782;&#21035;&#21644;&#25552;&#20379;&#34092;&#33756;&#30149;&#23475;&#30340;&#27835;&#30103;&#24314;&#35758;&#12290;&#20026;&#20102;&#30830;&#20445;&#26131;&#29992;&#24615;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#65292;&#25552;&#20379;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#65292;&#20351;&#20892;&#27665;&#21487;&#20197;&#29992;&#20182;&#20204;&#24403;&#22320;&#30340;&#35821;&#35328;&#26597;&#35810;&#34092;&#33756;&#30149;&#23475;&#24182;&#33719;&#24471;&#21512;&#36866;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#31995;&#32479;&#36866;&#29992;&#20110;&#21482;&#20855;&#22791;&#22522;&#26412;&#26234;&#33021;&#25163;&#26426;&#20351;&#29992;&#30693;&#35782;&#30340;&#20219;&#20309;&#20892;&#27665;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#27454;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#24314;&#35758;&#30058;&#33540;&#30149;&#23475;&#30340;&#27835;&#30103;&#26041;&#26696;&#65292;&#20197;&#24800;&#21450;&#23612;&#27850;&#23572;&#24403;&#22320;&#30340;&#20892;&#27665;&#32676;&#20307;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#37319;&#29992;&#20102;&#30446;&#26631;&#26816;&#27979;&#30340;&#26368;&#26032;&#25216;&#26415;You Only Look Once (YOLO)&#26469;&#26816;&#27979;&#30058;&#33540;&#30149;&#23475;&#12290;&#28982;&#21518;&#23558;&#26816;&#27979;&#21040;&#30340;&#20449;&#24687;&#20256;&#36882;&#32473;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
We have developed a comprehensive computer system to assist farmers who practice traditional farming methods and have limited access to agricultural experts for addressing crop diseases. Our system utilizes artificial intelligence (AI) to identify and provide remedies for vegetable diseases. To ensure ease of use, we have created a mobile application that offers a user-friendly interface, allowing farmers to inquire about vegetable diseases and receive suitable solutions in their local language. The developed system can be utilized by any farmer with a basic understanding of a smartphone. Specifically, we have designed an AI-enabled mobile application for identifying and suggesting remedies for vegetable diseases, focusing on tomato diseases to benefit the local farming community in Nepal. Our system employs state-of-the-art object detection methodology, namely You Only Look Once (YOLO), to detect tomato diseases. The detected information is then relayed to the mobile application, whic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35299;&#26512;CLIP&#22270;&#20687;&#32534;&#30721;&#22120;&#30340;&#32452;&#20214;&#65292;&#25581;&#31034;&#20102;&#22270;&#20687;&#34920;&#31034;&#30340;&#26500;&#25104;&#26041;&#24335;&#65292;&#24182;&#21033;&#29992;&#25991;&#26412;&#34920;&#31034;&#35299;&#37322;&#20102;&#20854;&#21508;&#20010;&#37096;&#20998;&#30340;&#20316;&#29992;&#12290;&#36890;&#36807;&#29702;&#35299;&#27880;&#24847;&#21147;&#22836;&#21644;&#22270;&#20687;&#22359;&#65292;&#20316;&#32773;&#23454;&#29616;&#20102;&#23545;&#27169;&#22411;&#30340;&#20462;&#22797;&#21644;&#25913;&#36827;&#65292;&#21253;&#25324;&#28040;&#38500;&#35823;&#29305;&#24449;&#21644;&#26500;&#24314;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#21106;&#22120;&#31561;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2310.05916</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#25991;&#26412;&#30340;&#20998;&#35299;&#35299;&#37322;CLIP&#22270;&#20687;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Interpreting CLIP's Image Representation via Text-Based Decomposition. (arXiv:2310.05916v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35299;&#26512;CLIP&#22270;&#20687;&#32534;&#30721;&#22120;&#30340;&#32452;&#20214;&#65292;&#25581;&#31034;&#20102;&#22270;&#20687;&#34920;&#31034;&#30340;&#26500;&#25104;&#26041;&#24335;&#65292;&#24182;&#21033;&#29992;&#25991;&#26412;&#34920;&#31034;&#35299;&#37322;&#20102;&#20854;&#21508;&#20010;&#37096;&#20998;&#30340;&#20316;&#29992;&#12290;&#36890;&#36807;&#29702;&#35299;&#27880;&#24847;&#21147;&#22836;&#21644;&#22270;&#20687;&#22359;&#65292;&#20316;&#32773;&#23454;&#29616;&#20102;&#23545;&#27169;&#22411;&#30340;&#20462;&#22797;&#21644;&#25913;&#36827;&#65292;&#21253;&#25324;&#28040;&#38500;&#35823;&#29305;&#24449;&#21644;&#26500;&#24314;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#21106;&#22120;&#31561;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#20010;&#21035;&#27169;&#22411;&#32452;&#20214;&#23545;&#26368;&#32456;&#34920;&#31034;&#30340;&#24433;&#21709;&#65292;&#25506;&#35752;&#20102;CLIP&#22270;&#20687;&#32534;&#30721;&#22120;&#12290;&#25105;&#20204;&#23558;&#22270;&#20687;&#34920;&#31034;&#20998;&#35299;&#20026;&#21508;&#20010;&#22270;&#20687;&#22359;&#12289;&#27169;&#22411;&#23618;&#21644;&#27880;&#24847;&#21147;&#22836;&#30340;&#27714;&#21644;&#65292;&#24182;&#20351;&#29992;CLIP&#30340;&#25991;&#26412;&#34920;&#31034;&#26469;&#35299;&#37322;&#36825;&#20123;&#27714;&#21644;&#39033;&#12290;&#36890;&#36807;&#35299;&#37322;&#27880;&#24847;&#21147;&#22836;&#65292;&#25105;&#20204;&#36890;&#36807;&#33258;&#21160;&#23547;&#25214;&#33021;&#22815;&#36328;&#36234;&#36755;&#20986;&#31354;&#38388;&#30340;&#25991;&#26412;&#34920;&#31034;&#26469;&#34920;&#24449;&#27599;&#20010;&#22836;&#30340;&#20316;&#29992;&#65292;&#25581;&#31034;&#20986;&#35768;&#22810;&#22836;&#30340;&#29305;&#23450;&#23646;&#24615;&#35282;&#33394;&#65288;&#20363;&#22914;&#20301;&#32622;&#25110;&#24418;&#29366;&#65289;&#12290;&#25509;&#19979;&#26469;&#65292;&#36890;&#36807;&#35299;&#37322;&#22270;&#20687;&#22359;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;CLIP&#20013;&#30340;&#32039;&#23494;&#31354;&#38388;&#23450;&#20301;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#29702;&#35299;&#28040;&#38500;&#20102;CLIP&#20013;&#30340;&#35823;&#29305;&#24449;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#21106;&#22120;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21487;&#25193;&#23637;&#30340;&#23545;Transformer&#27169;&#22411;&#30340;&#29702;&#35299;&#26159;&#21487;&#23454;&#29616;&#30340;&#65292;&#24182;&#21487;&#29992;&#20110;&#20462;&#22797;&#21644;&#25913;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the CLIP image encoder by analyzing how individual model components affect the final representation. We decompose the image representation as a sum across individual image patches, model layers, and attention heads, and use CLIP's text representation to interpret the summands. Interpreting the attention heads, we characterize each head's role by automatically finding text representations that span its output space, which reveals property-specific roles for many heads (e.g. location or shape). Next, interpreting the image patches, we uncover an emergent spatial localization within CLIP. Finally, we use this understanding to remove spurious features from CLIP and to create a strong zero-shot image segmenter. Our results indicate that a scalable understanding of transformer models is attainable and can be used to repair and improve models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22810;&#27169;&#22411;LLM&#30340;&#32454;&#31890;&#24230;&#38899;&#39057;-&#35270;&#35273;&#32852;&#21512;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;(FAVOR)&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#21516;&#26102;&#24863;&#30693;&#38899;&#39057;&#21644;&#35270;&#35273;&#36755;&#20837;&#27969;&#65292;&#24182;&#36890;&#36807;&#22240;&#26524;&#20851;&#27880;&#27169;&#22359;&#25429;&#33719;&#38899;&#39057;-&#35270;&#35273;&#24103;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#38899;&#39057;-&#35270;&#35273;&#35780;&#20272;&#22522;&#20934;(AVEB)&#29992;&#20110;&#35780;&#20272;&#35813;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.05863</link><description>&lt;p&gt;
&#38754;&#21521;&#22810;&#27169;&#22411;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#38899;&#39057;-&#35270;&#35273;&#32852;&#21512;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Fine-grained Audio-Visual Joint Representations for Multimodal Large Language Models. (arXiv:2310.05863v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22810;&#27169;&#22411;LLM&#30340;&#32454;&#31890;&#24230;&#38899;&#39057;-&#35270;&#35273;&#32852;&#21512;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;(FAVOR)&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#21516;&#26102;&#24863;&#30693;&#38899;&#39057;&#21644;&#35270;&#35273;&#36755;&#20837;&#27969;&#65292;&#24182;&#36890;&#36807;&#22240;&#26524;&#20851;&#27880;&#27169;&#22359;&#25429;&#33719;&#38899;&#39057;-&#35270;&#35273;&#24103;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#38899;&#39057;-&#35270;&#35273;&#35780;&#20272;&#22522;&#20934;(AVEB)&#29992;&#20110;&#35780;&#20272;&#35813;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;-&#35270;&#35273;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#24341;&#36215;&#20102;&#37325;&#35201;&#20851;&#27880;&#65292;&#20294;&#23545;&#20110;&#36755;&#20837;&#27969;&#30340;&#32454;&#31890;&#24230;&#32452;&#21512;&#21364;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#65292;&#36825;&#23545;&#20110;LLM&#29702;&#35299;&#19968;&#33324;&#35270;&#39057;&#36755;&#20837;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#24517;&#35201;&#30340;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22810;&#27169;&#22411;LLM&#30340;&#32454;&#31890;&#24230;&#38899;&#39057;-&#35270;&#35273;&#32852;&#21512;&#34920;&#31034;(FAVOR)&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#22522;&#20110;&#25991;&#26412;&#30340;LLM&#25193;&#23637;&#21040;&#33021;&#22815;&#21516;&#26102;&#24863;&#30693;&#38899;&#39057;&#36755;&#20837;&#27969;&#20013;&#30340;&#35821;&#38899;&#21644;&#38899;&#39057;&#20107;&#20214;&#20197;&#21450;&#35270;&#35273;&#36755;&#20837;&#27969;&#20013;&#30340;&#22270;&#20687;&#25110;&#35270;&#39057;&#65292;&#24103;&#32423;&#21035;&#22320;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;Q-Former&#32467;&#26500;&#65292;&#37197;&#21512;&#22240;&#26524;&#20851;&#27880;&#27169;&#22359;&#65292;&#23558;&#38899;&#39057;&#21644;&#35270;&#35273;&#29305;&#24449;&#27969;&#34701;&#21512;&#21040;&#32852;&#21512;&#34920;&#31034;&#20013;&#65292;&#24182;&#23558;&#32852;&#21512;&#31354;&#38388;&#19982;LLM&#36755;&#20837;&#23884;&#20837;&#31354;&#38388;&#23545;&#40784;&#65292;&#20197;&#22686;&#24378;&#23545;&#38899;&#39057;-&#35270;&#35273;&#24103;&#22312;&#26102;&#38388;&#19978;&#30340;&#22240;&#26524;&#20851;&#31995;&#25429;&#33719;&#12290;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#38899;&#39057;-&#35270;&#35273;&#35780;&#20272;&#22522;&#20934;(AVEB)&#65292;&#20854;&#20013;&#21253;&#25324;&#20845;&#20010;&#20195;&#34920;&#24615;&#30340;&#21333;&#27169;&#24577;&#20219;&#21153;&#21644;&#20116;&#20010;&#21453;&#26144;&#38899;&#39057;-&#35270;&#35273;&#30340;&#36328;&#27169;&#24577;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Audio-visual large language models (LLM) have drawn significant attention, yet the fine-grained combination of both input streams is rather under-explored, which is challenging but necessary for LLMs to understand general video inputs. To this end, a fine-grained audio-visual joint representation (FAVOR) learning framework for multimodal LLMs is proposed in this paper, which extends a text-based LLM to simultaneously perceive speech and audio events in the audio input stream and images or videos in the visual input stream, at the frame level. To fuse the audio and visual feature streams into joint representations and to align the joint space with the LLM input embedding space, we propose a causal Q-Former structure with a causal attention module to enhance the capture of causal relations of the audio-visual frames across time. An audio-visual evaluation benchmark (AVEB) is also proposed which comprises six representative single-modal tasks with five cross-modal tasks reflecting audio-v
&lt;/p&gt;</description></item><item><title>ParFam&#26159;&#19968;&#31181;&#26032;&#30340;&#31526;&#21495;&#22238;&#24402;&#26041;&#27861;&#65292;&#21033;&#29992;&#21442;&#25968;&#21270;&#30340;&#31526;&#21495;&#20989;&#25968;&#26063;&#23558;&#31163;&#25955;&#38382;&#39064;&#36716;&#21270;&#20026;&#36830;&#32493;&#38382;&#39064;&#65292;&#24182;&#32467;&#21512;&#20840;&#23616;&#20248;&#21270;&#22120;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#31526;&#21495;&#22238;&#24402;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.05537</link><description>&lt;p&gt;
ParFam - &#22522;&#20110;&#36830;&#32493;&#20840;&#23616;&#20248;&#21270;&#30340;&#31526;&#21495;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
ParFam -- Symbolic Regression Based on Continuous Global Optimization. (arXiv:2310.05537v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05537
&lt;/p&gt;
&lt;p&gt;
ParFam&#26159;&#19968;&#31181;&#26032;&#30340;&#31526;&#21495;&#22238;&#24402;&#26041;&#27861;&#65292;&#21033;&#29992;&#21442;&#25968;&#21270;&#30340;&#31526;&#21495;&#20989;&#25968;&#26063;&#23558;&#31163;&#25955;&#38382;&#39064;&#36716;&#21270;&#20026;&#36830;&#32493;&#38382;&#39064;&#65292;&#24182;&#32467;&#21512;&#20840;&#23616;&#20248;&#21270;&#22120;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#31526;&#21495;&#22238;&#24402;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;&#65288;SR&#65289;&#38382;&#39064;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#24212;&#29992;&#20013;&#20986;&#29616;&#65292;&#27604;&#22914;&#20174;&#32473;&#23450;&#25968;&#25454;&#20013;&#35782;&#21035;&#29289;&#29702;&#23450;&#24459;&#25110;&#25512;&#23548;&#25551;&#36848;&#37329;&#34701;&#24066;&#22330;&#34892;&#20026;&#30340;&#25968;&#23398;&#26041;&#31243;&#12290;&#30446;&#21069;&#23384;&#22312;&#22810;&#31181;&#35299;&#20915;SR&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#24120;&#22522;&#20110;&#36951;&#20256;&#32534;&#31243;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38750;&#24120;&#22797;&#26434;&#65292;&#38656;&#35201;&#22823;&#37327;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26032;&#26041;&#27861;ParFam&#65292;&#23427;&#21033;&#29992;&#36866;&#21512;&#30340;&#31526;&#21495;&#20989;&#25968;&#30340;&#21442;&#25968;&#21270;&#26063;&#23558;&#31163;&#25955;&#30340;&#31526;&#21495;&#22238;&#24402;&#38382;&#39064;&#36716;&#21270;&#20026;&#36830;&#32493;&#38382;&#39064;&#65292;&#30456;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#35774;&#32622;&#26356;&#21152;&#30452;&#35266;&#12290;&#32467;&#21512;&#24378;&#22823;&#30340;&#20840;&#23616;&#20248;&#21270;&#22120;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;SR&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#23427;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#21040;&#26356;&#39640;&#32423;&#30340;&#31639;&#27861;&#65292;&#20363;&#22914;&#28155;&#21152;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20197;&#25214;&#21040;&#36866;&#21512;&#30340;&#21442;&#25968;&#21270;&#26063;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of symbolic regression (SR) arises in many different applications, such as identifying physical laws or deriving mathematical equations describing the behavior of financial markets from given data. Various methods exist to address the problem of SR, often based on genetic programming. However, these methods are usually quite complicated and require a lot of hyperparameter tuning and computational resources. In this paper, we present our new method ParFam that utilizes parametric families of suitable symbolic functions to translate the discrete symbolic regression problem into a continuous one, resulting in a more straightforward setup compared to current state-of-the-art methods. In combination with a powerful global optimizer, this approach results in an effective method to tackle the problem of SR. Furthermore, it can be easily extended to more advanced algorithms, e.g., by adding a deep neural network to find good-fitting parametric families. We prove the performance of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#20998;&#23376;&#30340;&#20840;&#26032;&#35774;&#35745;&#20013;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#24615;&#36136;&#30340;&#20998;&#23376;&#32467;&#26500;&#65292;&#23637;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.05365</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;Transformer&#30340;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#20998;&#23376;&#30340;&#20840;&#26032;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Molecular De Novo Design through Transformer-based Reinforcement Learning. (arXiv:2310.05365v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#20998;&#23376;&#30340;&#20840;&#26032;&#35774;&#35745;&#20013;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#24615;&#36136;&#30340;&#20998;&#23376;&#32467;&#26500;&#65292;&#23637;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#22522;&#20110;Transformer&#30340;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#20998;&#23376;&#30340;&#20840;&#26032;&#35774;&#35745;&#30340;&#26041;&#27861;&#12290;&#21033;&#29992;Transformer&#30456;&#23545;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#30340;&#20248;&#36234;&#24207;&#21015;&#23398;&#20064;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#24615;&#36136;&#30340;&#20998;&#23376;&#32467;&#26500;&#12290;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;RNN&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;&#39044;&#27979;&#23545;&#22810;&#31181;&#29983;&#29289;&#38774;&#28857;&#20855;&#26377;&#27963;&#24615;&#30340;&#21270;&#21512;&#29289;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#65292;&#25429;&#25417;&#20102;&#20998;&#23376;&#32467;&#26500;&#24207;&#21015;&#30340;&#38271;&#26399;&#20381;&#36182;&#24615;&#12290;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#35777;&#26126;&#65292;&#21253;&#25324;&#29983;&#25104;&#19982;&#26597;&#35810;&#32467;&#26500;&#31867;&#20284;&#30340;&#20998;&#23376;&#21644;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#23646;&#24615;&#30340;&#21270;&#21512;&#29289;&#65292;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#22522;&#32447;&#30340;&#22522;&#20110;RNN&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#26725;&#25509;&#21270;&#23398;&#12289;&#20174;&#21333;&#20010;&#20998;&#23376;&#24320;&#22987;&#25193;&#23637;&#24211;&#65292;&#24182;&#29983;&#25104;&#20855;&#26377;&#39640;&#39044;&#27979;&#27963;&#24615;&#30340;&#21270;&#21512;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce a method to fine-tune a Transformer-based generative model for molecular de novo design. Leveraging the superior sequence learning capacity of Transformers over Recurrent Neural Networks (RNNs), our model can generate molecular structures with desired properties effectively. In contrast to the traditional RNN-based models, our proposed method exhibits superior performance in generating compounds predicted to be active against various biological targets, capturing long-term dependencies in the molecular structure sequence. The model's efficacy is demonstrated across numerous tasks, including generating analogues to a query structure and producing compounds with particular attributes, outperforming the baseline RNN-based methods. Our approach can be used for scaffold hopping, library expansion starting from a single molecule, and generating compounds with high predicted activity against biological targets.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36845;&#20195;&#34701;&#21512;&#27169;&#24577;&#30456;&#20284;&#36335;&#24452;&#23454;&#29616;&#36890;&#29992;&#30340;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;&#26041;&#27861;&#12290;&#36890;&#36807;&#32479;&#19968;&#24314;&#27169;&#21644;&#26377;&#25928;&#20449;&#24687;&#34701;&#21512;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#27169;&#24577;&#24314;&#27169;&#19981;&#19968;&#33268;&#21644;&#20302;&#25928;&#20197;&#21450;&#27169;&#24577;&#34701;&#21512;&#25928;&#26524;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.05364</link><description>&lt;p&gt;
&#36890;&#36807;&#36845;&#20195;&#22320;&#34701;&#21512;&#27169;&#24577;&#30456;&#20284;&#36335;&#24452;&#23454;&#29616;&#36890;&#29992;&#30340;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Universal Multi-modal Entity Alignment via Iteratively Fusing Modality Similarity Paths. (arXiv:2310.05364v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36845;&#20195;&#34701;&#21512;&#27169;&#24577;&#30456;&#20284;&#36335;&#24452;&#23454;&#29616;&#36890;&#29992;&#30340;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;&#26041;&#27861;&#12290;&#36890;&#36807;&#32479;&#19968;&#24314;&#27169;&#21644;&#26377;&#25928;&#20449;&#24687;&#34701;&#21512;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#27169;&#24577;&#24314;&#27169;&#19981;&#19968;&#33268;&#21644;&#20302;&#25928;&#20197;&#21450;&#27169;&#24577;&#34701;&#21512;&#25928;&#26524;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#23545;&#40784;&#30340;&#30446;&#26631;&#26159;&#20174;&#22810;&#20010;&#30693;&#35782;&#22270;&#35889;&#20013;&#30830;&#23450;&#31561;&#20215;&#30340;&#23454;&#20307;&#23545;&#65292;&#24182;&#21019;&#24314;&#19968;&#20010;&#26356;&#20840;&#38754;&#21644;&#32479;&#19968;&#30340;&#30693;&#35782;&#22270;&#35889;&#12290;&#22823;&#22810;&#25968;&#23454;&#20307;&#23545;&#40784;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#30693;&#35782;&#22270;&#35889;&#30340;&#32467;&#26500;&#27169;&#24577;&#65292;&#32570;&#20047;&#23545;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#25506;&#32034;&#12290;&#23569;&#25968;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;&#26041;&#27861;&#22312;&#36825;&#20010;&#39046;&#22495;&#20570;&#20986;&#20102;&#19981;&#38169;&#30340;&#23581;&#35797;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23384;&#22312;&#20004;&#20010;&#32570;&#28857;&#65306;(1)&#27169;&#24577;&#24314;&#27169;&#19981;&#19968;&#33268;&#19988;&#20302;&#25928;&#65292;&#20026;&#27599;&#20010;&#27169;&#24577;&#35774;&#35745;&#22797;&#26434;&#21644;&#29420;&#31435;&#30340;&#27169;&#22411;&#65307;(2)&#30001;&#20110;&#23454;&#20307;&#23545;&#40784;&#20013;&#27169;&#24577;&#30340;&#24322;&#26500;&#24615;&#65292;&#27169;&#24577;&#34701;&#21512;&#25928;&#26524;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PathFusion&#65292;&#23427;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#37096;&#20998;&#65306;(1) MSP&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#36830;&#25509;&#23454;&#20307;&#21644;&#27169;&#24577;&#33410;&#28857;&#20197;&#34920;&#31034;&#22810;&#20010;&#27169;&#24577;&#30340;&#36335;&#24452;&#65292;&#31616;&#21270;&#20102;&#23545;&#40784;&#36807;&#31243;&#65307;(2) IRF&#65292;&#19968;&#31181;&#36845;&#20195;&#34701;&#21512;&#26041;&#27861;&#65292;&#20351;&#29992;&#36335;&#24452;&#20316;&#20026;&#20449;&#24687;&#36733;&#20307;&#65292;&#26377;&#25928;&#22320;&#23558;&#19981;&#21516;&#27169;&#24577;&#30340;&#20449;&#24687;&#32467;&#21512;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
The objective of Entity Alignment (EA) is to identify equivalent entity pairs from multiple Knowledge Graphs (KGs) and create a more comprehensive and unified KG. The majority of EA methods have primarily focused on the structural modality of KGs, lacking exploration of multi-modal information. A few multi-modal EA methods have made good attempts in this field. Still, they have two shortcomings: (1) inconsistent and inefficient modality modeling that designs complex and distinct models for each modality; (2) ineffective modality fusion due to the heterogeneous nature of modalities in EA. To tackle these challenges, we propose PathFusion, consisting of two main components: (1) MSP, a unified modeling approach that simplifies the alignment process by constructing paths connecting entities and modality nodes to represent multiple modalities; (2) IRF, an iterative fusion method that effectively combines information from different modalities using the path as an information carrier. Experim
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23545;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#32463;&#20856;&#27979;&#35797;&#26102;&#36866;&#24212;&#26041;&#27861;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#25506;&#31350;&#65292;&#25581;&#31034;&#20102;&#20998;&#21106;TTA&#25152;&#38754;&#20020;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#24182;&#21457;&#29616;&#32463;&#20856;TTA&#31574;&#30053;&#22312;&#36825;&#19968;&#20219;&#21153;&#20013;&#24182;&#19981;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2310.05341</link><description>&lt;p&gt;
&#23545;&#35821;&#20041;&#20998;&#21106;&#20013;&#32463;&#20856;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;&#26041;&#27861;&#30340;&#25209;&#21028;&#24615;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Critical Look at Classic Test-Time Adaptation Methods in Semantic Segmentation. (arXiv:2310.05341v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05341
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23545;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#32463;&#20856;&#27979;&#35797;&#26102;&#36866;&#24212;&#26041;&#27861;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#25506;&#31350;&#65292;&#25581;&#31034;&#20102;&#20998;&#21106;TTA&#25152;&#38754;&#20020;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#24182;&#21457;&#29616;&#32463;&#20856;TTA&#31574;&#30053;&#22312;&#36825;&#19968;&#20219;&#21153;&#20013;&#24182;&#19981;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#35797;&#26102;&#36866;&#24212;&#65288;TTA&#65289;&#26088;&#22312;&#23558;&#26368;&#21021;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#36866;&#24212;&#20110;&#27979;&#35797;&#25968;&#25454;&#20013;&#30340;&#21487;&#33021;&#20998;&#24067;&#21464;&#21270;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;TTA&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#23545;&#20110;&#35821;&#20041;&#20998;&#21106;&#30340;TTA&#25506;&#32034;&#38750;&#24120;&#26377;&#38480;&#12290;&#36825;&#31181;&#23545;&#20998;&#31867;&#30340;&#31361;&#20986;&#37325;&#35270;&#21487;&#33021;&#23548;&#33268;&#35768;&#22810;&#26032;&#25163;&#21644;&#24037;&#31243;&#24072;&#38169;&#35823;&#22320;&#35748;&#20026;&#20026;&#20998;&#31867;&#35774;&#35745;&#30340;&#32463;&#20856;TTA&#26041;&#27861;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#20998;&#21106;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#20551;&#35774;&#20173;&#26410;&#32463;&#39564;&#35777;&#65292;&#26159;&#19968;&#20010;&#24453;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#31995;&#32479;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#20998;&#21106;TTA&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#24182;&#30830;&#23450;&#32463;&#20856;TTA&#31574;&#30053;&#26159;&#21542;&#21487;&#20197;&#26377;&#25928;&#24212;&#23545;&#36825;&#19968;&#20219;&#21153;&#12290;&#25105;&#20204;&#20840;&#38754;&#30340;&#32467;&#26524;&#24471;&#20986;&#20102;&#19977;&#20010;&#20851;&#38190;&#35266;&#23519;&#32467;&#26524;&#12290;&#39318;&#20808;&#65292;&#24120;&#29992;&#20110;&#20998;&#31867;TTA&#30340;&#32463;&#20856;&#25209;&#24402;&#19968;&#21270;&#26356;&#26032;&#31574;&#30053;&#21482;&#33021;&#24102;&#26469;&#36731;&#24494;&#30340;&#24615;&#33021;&#25913;&#21892;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#29978;&#33267;&#20250;&#23545;&#32467;&#26524;&#20135;&#29983;&#36870;&#21521;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Test-time adaptation (TTA) aims to adapt a model, initially trained on training data, to potential distribution shifts in the test data. Most existing TTA studies, however, focus on classification tasks, leaving a notable gap in the exploration of TTA for semantic segmentation. This pronounced emphasis on classification might lead numerous newcomers and engineers to mistakenly assume that classic TTA methods designed for classification can be directly applied to segmentation. Nonetheless, this assumption remains unverified, posing an open question. To address this, we conduct a systematic, empirical study to disclose the unique challenges of segmentation TTA, and to determine whether classic TTA strategies can effectively address this task. Our comprehensive results have led to three key observations. First, the classic batch norm updating strategy, commonly used in classification TTA, only brings slight performance improvement, and in some cases it might even adversely affect the resu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#33258;&#36866;&#24212;&#20998;&#35789;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#20998;&#35789;&#36807;&#31243;&#26469;&#22686;&#24378;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#20013;&#30340;&#38271;&#25991;&#26412;&#29983;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20943;&#23569;&#26631;&#35760;&#25968;&#37327;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#29983;&#25104;&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.05317</link><description>&lt;p&gt;
&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#20013;&#36890;&#36807;&#20219;&#21153;&#33258;&#36866;&#24212;&#20998;&#35789;&#26469;&#22686;&#24378;&#38271;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Enhancing Long-form Text Generation in Mental Health with Task-adaptive Tokenization. (arXiv:2310.05317v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05317
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#33258;&#36866;&#24212;&#20998;&#35789;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#20998;&#35789;&#36807;&#31243;&#26469;&#22686;&#24378;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#20013;&#30340;&#38271;&#25991;&#26412;&#29983;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20943;&#23569;&#26631;&#35760;&#25968;&#37327;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#29983;&#25104;&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20219;&#21153;&#33258;&#36866;&#24212;&#20998;&#35789;&#20316;&#20026;&#19968;&#31181;&#26041;&#24335;&#65292;&#23558;&#29983;&#25104;&#27969;&#27700;&#32447;&#36866;&#24212;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#29305;&#23450;&#35201;&#27714;&#65292;&#24182;&#22686;&#24378;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#38271;&#25991;&#26412;&#29983;&#25104;&#12290;&#21463;&#35748;&#30693;&#31185;&#23398;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#20219;&#21153;&#33258;&#36866;&#24212;&#20998;&#35789;&#22120;&#20174;&#22810;&#20010;&#32467;&#26524;&#20013;&#37319;&#26679;&#21487;&#21464;&#30340;&#20998;&#27573;&#65292;&#37319;&#26679;&#27010;&#29575;&#22522;&#20110;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26500;&#24314;&#19987;&#29992;&#35789;&#27719;&#30340;&#31574;&#30053;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#35789;&#27719;&#21512;&#24182;&#21327;&#35758;&#65292;&#21487;&#20197;&#23558;&#20219;&#21153;&#29305;&#23450;&#30340;&#26631;&#35760;&#25972;&#21512;&#21040;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20998;&#35789;&#27493;&#39588;&#20013;&#12290;&#36890;&#36807;&#23545;&#20013;&#33521;&#25991;&#24515;&#29702;&#38382;&#31572;&#20219;&#21153;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#20219;&#21153;&#33258;&#36866;&#24212;&#20998;&#35789;&#26041;&#27861;&#22312;&#20351;&#29992;&#26356;&#23569;&#30340;&#26631;&#35760;&#30340;&#24773;&#20917;&#19979;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#29983;&#25104;&#24615;&#33021;&#25552;&#21319;&#65292;&#26368;&#39640;&#21487;&#36798;60%&#12290;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#20998;&#35789;&#26041;&#27861;&#19982;&#38750;&#24120;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#33021;&#22815;&#24471;&#21040;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose task-adaptive tokenization as a way to adapt the generation pipeline to the specifics of a downstream task and enhance long-form generation in mental health. Inspired by insights from cognitive science, our task-adaptive tokenizer samples variable segmentations from multiple outcomes, with sampling probabilities optimized based on task-specific data. We introduce a strategy for building a specialized vocabulary and introduce a vocabulary merging protocol that allows for the integration of task-specific tokens into the pre-trained model's tokenization step. Through extensive experiments on psychological question-answering tasks in both Chinese and English, we find that our task-adaptive tokenization approach brings a significant improvement in generation performance while using up to 60% fewer tokens. Preliminary experiments point to promising results when using our tokenization approach with very large language models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#27874;&#26031;&#23383;&#20307;&#35782;&#21035;&#27969;&#27700;&#32447;&#65292;&#36890;&#36807;&#23545;&#26032;&#25968;&#25454;&#38598;&#21644;&#20854;&#20182;&#25968;&#25454;&#38598;&#30340;&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#21644;&#36739;&#24555;&#30340;&#22788;&#29702;&#36895;&#24230;&#65292;&#21487;&#20197;&#26080;&#38656;&#39069;&#22806;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#30452;&#25509;&#35782;&#21035;&#27874;&#26031;&#23383;&#20307;&#12290;</title><link>http://arxiv.org/abs/2310.05255</link><description>&lt;p&gt;
Persis: &#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#27874;&#26031;&#23383;&#20307;&#35782;&#21035;&#27969;&#27700;&#32447;
&lt;/p&gt;
&lt;p&gt;
Persis: A Persian Font Recognition Pipeline Using Convolutional Neural Networks. (arXiv:2310.05255v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#27874;&#26031;&#23383;&#20307;&#35782;&#21035;&#27969;&#27700;&#32447;&#65292;&#36890;&#36807;&#23545;&#26032;&#25968;&#25454;&#38598;&#21644;&#20854;&#20182;&#25968;&#25454;&#38598;&#30340;&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#21644;&#36739;&#24555;&#30340;&#22788;&#29702;&#36895;&#24230;&#65292;&#21487;&#20197;&#26080;&#38656;&#39069;&#22806;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#30452;&#25509;&#35782;&#21035;&#27874;&#26031;&#23383;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#25105;&#20204;&#36935;&#21040;&#19968;&#20010;&#36866;&#21512;&#25105;&#20204;&#35774;&#35745;&#24037;&#20316;&#30340;&#23383;&#20307;&#65292;&#20294;&#19981;&#30693;&#36947;&#23427;&#30340;&#21517;&#23383;&#20250;&#24590;&#26679;&#65311;&#35270;&#35273;&#23383;&#20307;&#35782;&#21035;&#65288;VFR&#65289;&#31995;&#32479;&#29992;&#20110;&#35782;&#21035;&#22270;&#20687;&#20013;&#30340;&#23383;&#20307;&#31867;&#22411;&#12290;&#36825;&#20123;&#31995;&#32479;&#21487;&#20197;&#24110;&#21161;&#24179;&#38754;&#35774;&#35745;&#24072;&#35782;&#21035;&#22270;&#20687;&#20013;&#20351;&#29992;&#30340;&#23383;&#20307;&#12290;VFR&#31995;&#32479;&#36824;&#26377;&#21161;&#20110;&#25552;&#39640;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#65288;OCR&#65289;&#31995;&#32479;&#30340;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#27874;&#26031;&#23383;&#20307;&#35782;&#21035;&#39046;&#22495;&#20013;&#39318;&#27425;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27969;&#27700;&#32447;&#22312;&#25105;&#20204;&#30340;&#26032;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;78.0%&#30340;top-1&#20934;&#30830;&#29575;&#65292;&#22312;IDPL-PFOD&#25968;&#25454;&#38598;&#19978;&#20026;89.1%&#65292;&#22312;KAFD&#25968;&#25454;&#38598;&#19978;&#20026;94.5%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#25968;&#25454;&#38598;&#20013;&#27599;&#20010;&#26679;&#26412;&#22312;&#25972;&#20010;&#27969;&#27700;&#32447;&#20013;&#30340;&#24179;&#22343;&#26102;&#38388;&#28040;&#32791;&#20026;CPU0.54&#31186;&#21644;GPU0.017&#31186;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;CNN&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#35782;&#21035;&#27874;&#26031;&#23383;&#20307;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#65292;&#22914;&#29305;&#24449;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
What happens if we encounter a suitable font for our design work but do not know its name? Visual Font Recognition (VFR) systems are used to identify the font typeface in an image. These systems can assist graphic designers in identifying fonts used in images. A VFR system also aids in improving the speed and accuracy of Optical Character Recognition (OCR) systems. In this paper, we introduce the first publicly available datasets in the field of Persian font recognition and employ Convolutional Neural Networks (CNN) to address this problem. The results show that the proposed pipeline obtained 78.0% top-1 accuracy on our new datasets, 89.1% on the IDPL-PFOD dataset, and 94.5% on the KAFD dataset. Furthermore, the average time spent in the entire pipeline for one sample of our proposed datasets is 0.54 and 0.017 seconds for CPU and GPU, respectively. We conclude that CNN methods can be used to recognize Persian fonts without the need for additional pre-processing steps such as feature ex
&lt;/p&gt;</description></item><item><title>ChatRadio-Valuer&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23450;&#21046;&#27169;&#22411;&#65292;&#29992;&#20110;&#36890;&#29992;&#25918;&#23556;&#23398;&#25253;&#21578;&#33258;&#21160;&#29983;&#25104;&#12290;&#23427;&#33021;&#22815;&#23398;&#20064;&#21487;&#27867;&#21270;&#30340;&#34920;&#31034;&#24182;&#20026;&#27169;&#22411;&#33258;&#36866;&#24212;&#25552;&#20379;&#22522;&#30784;&#27169;&#24335;&#65292;&#35299;&#20915;&#20102;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24322;&#36136;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.05242</link><description>&lt;p&gt;
ChatRadio-Valuer: &#19968;&#31181;&#22522;&#20110;&#22810;&#26426;&#26500;&#21644;&#22810;&#31995;&#32479;&#25968;&#25454;&#30340;&#36890;&#29992;&#25918;&#23556;&#23398;&#25253;&#21578;&#33258;&#21160;&#29983;&#25104;&#30340;&#32842;&#22825;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ChatRadio-Valuer: A Chat Large Language Model for Generalizable Radiology Report Generation Based on Multi-institution and Multi-system Data. (arXiv:2310.05242v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05242
&lt;/p&gt;
&lt;p&gt;
ChatRadio-Valuer&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23450;&#21046;&#27169;&#22411;&#65292;&#29992;&#20110;&#36890;&#29992;&#25918;&#23556;&#23398;&#25253;&#21578;&#33258;&#21160;&#29983;&#25104;&#12290;&#23427;&#33021;&#22815;&#23398;&#20064;&#21487;&#27867;&#21270;&#30340;&#34920;&#31034;&#24182;&#20026;&#27169;&#22411;&#33258;&#36866;&#24212;&#25552;&#20379;&#22522;&#30784;&#27169;&#24335;&#65292;&#35299;&#20915;&#20102;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24322;&#36136;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#26159;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#23545;&#20020;&#24202;&#20915;&#31574;&#27700;&#24179;&#30340;&#23450;&#37327;&#20998;&#26512;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20855;&#26377;&#20132;&#21449;&#26469;&#28304;&#24322;&#36136;&#24615;&#30340;&#22797;&#26434;&#22810;&#26679;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#23545;&#24403;&#21069;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#37327;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;&#25552;&#20986;&#20102;&#24040;&#22823;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#39118;&#26684;&#21644;&#35268;&#33539;&#24615;&#22312;&#26426;&#26500;&#12289;&#26816;&#26597;&#37096;&#20301;&#21644;&#25918;&#23556;&#31185;&#21307;&#29983;&#20043;&#38388;&#26126;&#26174;&#26377;&#25152;&#19981;&#21516;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#20026;&#35782;&#21035;&#20581;&#24247;&#29366;&#20917;&#30340;&#36857;&#35937;&#25552;&#20379;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#19982;&#20013;&#22269;&#30340;&#28248;&#38597;&#20108;&#21307;&#38498;&#21512;&#20316;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;LLM&#30340;ChatRadio-Valuer&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#33258;&#21160;&#21270;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#23450;&#21046;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#23398;&#20064;&#21487;&#27867;&#21270;&#30340;&#34920;&#31034;&#24182;&#20026;&#22797;&#26434;&#20998;&#26512;&#24072;&#26696;&#20363;&#20013;&#30340;&#27169;&#22411;&#33258;&#36866;&#24212;&#25552;&#20379;&#22522;&#26412;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Radiology report generation, as a key step in medical image analysis, is critical to the quantitative analysis of clinically informed decision-making levels. However, complex and diverse radiology reports with cross-source heterogeneity pose a huge generalizability challenge to the current methods under massive data volume, mainly because the style and normativity of radiology reports are obviously distinctive among institutions, body regions inspected and radiologists. Recently, the advent of large language models (LLM) offers great potential for recognizing signs of health conditions. To resolve the above problem, we collaborate with the Second Xiangya Hospital in China and propose ChatRadio-Valuer based on the LLM, a tailored model for automatic radiology report generation that learns generalizable representations and provides a basis pattern for model adaptation in sophisticated analysts' cases. Specifically, ChatRadio-Valuer is trained based on the radiology reports from a single 
&lt;/p&gt;</description></item><item><title>TILFA&#26159;&#19968;&#20010;&#29992;&#20110;&#35770;&#35777;&#25366;&#25496;&#20013;&#22788;&#29702;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#24067;&#23616;&#28151;&#21512;&#25968;&#25454;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#19981;&#20165;&#33021;&#22815;&#29702;&#35299;&#25991;&#26412;&#65292;&#36824;&#33021;&#22815;&#26816;&#27979;&#20809;&#23398;&#23383;&#31526;&#21644;&#35782;&#21035;&#22270;&#20687;&#20013;&#30340;&#24067;&#23616;&#32454;&#33410;&#65292;&#24182;&#22312;&#36777;&#35770;&#31435;&#22330;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.05210</link><description>&lt;p&gt;
TILFA: &#19968;&#31181;&#29992;&#20110;&#35770;&#35777;&#25366;&#25496;&#20013;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#24067;&#23616;&#34701;&#21512;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
TILFA: A Unified Framework for Text, Image, and Layout Fusion in Argument Mining. (arXiv:2310.05210v1 [cs.AI] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05210
&lt;/p&gt;
&lt;p&gt;
TILFA&#26159;&#19968;&#20010;&#29992;&#20110;&#35770;&#35777;&#25366;&#25496;&#20013;&#22788;&#29702;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#24067;&#23616;&#28151;&#21512;&#25968;&#25454;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#19981;&#20165;&#33021;&#22815;&#29702;&#35299;&#25991;&#26412;&#65292;&#36824;&#33021;&#22815;&#26816;&#27979;&#20809;&#23398;&#23383;&#31526;&#21644;&#35782;&#21035;&#22270;&#20687;&#20013;&#30340;&#24067;&#23616;&#32454;&#33410;&#65292;&#24182;&#22312;&#36777;&#35770;&#31435;&#22330;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#35777;&#25366;&#25496;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#20998;&#26512;&#20316;&#32773;&#30340;&#31435;&#22330;&#12290;&#19982;&#20197;&#24448;&#21482;&#20851;&#27880;&#25991;&#26412;&#30340;&#35770;&#35777;&#25366;&#25496;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#31532;10&#23626;&#35770;&#35777;&#25366;&#25496;&#30740;&#35752;&#20250;&#30340;&#20849;&#20139;&#20219;&#21153;&#24341;&#20837;&#20102;&#19968;&#20010;&#21253;&#21547;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#25968;&#25454;&#38598;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#20123;&#22270;&#20687;&#21253;&#21547;&#20102;&#35270;&#35273;&#20803;&#32032;&#21644;&#20809;&#23398;&#23383;&#31526;&#12290;&#25105;&#20204;&#30340;&#26032;&#26694;&#26550;TILFA&#65288;&#19968;&#31181;&#29992;&#20110;&#35770;&#35777;&#25366;&#25496;&#20013;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#24067;&#23616;&#34701;&#21512;&#30340;&#32479;&#19968;&#26694;&#26550;&#65289;&#34987;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#36825;&#31181;&#28151;&#21512;&#25968;&#25454;&#12290;&#23427;&#19981;&#20165;&#33021;&#22815;&#29702;&#35299;&#25991;&#26412;&#65292;&#36824;&#33021;&#22815;&#26816;&#27979;&#20809;&#23398;&#23383;&#31526;&#21644;&#35782;&#21035;&#22270;&#20687;&#20013;&#30340;&#24067;&#23616;&#32454;&#33410;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#32447;&#65292;&#22312;&#36825;&#20010;&#20849;&#20139;&#20219;&#21153;&#30340;&#36777;&#35770;&#31435;&#22330;&#20998;&#31867;&#23376;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#30340;&#22242;&#38431;KnowComp&#22312;&#25490;&#34892;&#27036;&#19978;&#33719;&#24471;&#20102;&#31532;&#19968;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
A main goal of Argument Mining (AM) is to analyze an author's stance. Unlike previous AM datasets focusing only on text, the shared task at the 10th Workshop on Argument Mining introduces a dataset including both text and images. Importantly, these images contain both visual elements and optical characters. Our new framework, TILFA (A Unified Framework for Text, Image, and Layout Fusion in Argument Mining), is designed to handle this mixed data. It excels at not only understanding text but also detecting optical characters and recognizing layout details in images. Our model significantly outperforms existing baselines, earning our team, KnowComp, the 1st place in the leaderboard of Argumentative Stance Classification subtask in this shared task.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#29983;&#25104;&#34394;&#20551;&#12289;&#38169;&#35823;&#25110;&#35823;&#23548;&#20869;&#23481;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#20063;&#38754;&#20020;&#34987;&#24694;&#24847;&#24212;&#29992;&#30340;&#39118;&#38505;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#38656;&#35201;&#20174;&#20107;&#23454;&#26680;&#26597;&#21592;&#12289;&#26032;&#38395;&#26426;&#26500;&#21644;&#30740;&#31350;&#19982;&#25919;&#31574;&#30028;&#37319;&#21462;&#30340;&#25216;&#26415;&#21019;&#26032;&#12289;&#30417;&#31649;&#25913;&#38761;&#21644;&#20154;&#24037;&#26234;&#33021;&#32032;&#20859;&#20513;&#35758;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.05189</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#20013;&#30340;&#20107;&#23454;&#24615;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Factuality Challenges in the Era of Large Language Models. (arXiv:2310.05189v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05189
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#29983;&#25104;&#34394;&#20551;&#12289;&#38169;&#35823;&#25110;&#35823;&#23548;&#20869;&#23481;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#20063;&#38754;&#20020;&#34987;&#24694;&#24847;&#24212;&#29992;&#30340;&#39118;&#38505;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#38656;&#35201;&#20174;&#20107;&#23454;&#26680;&#26597;&#21592;&#12289;&#26032;&#38395;&#26426;&#26500;&#21644;&#30740;&#31350;&#19982;&#25919;&#31574;&#30028;&#37319;&#21462;&#30340;&#25216;&#26415;&#21019;&#26032;&#12289;&#30417;&#31649;&#25913;&#38761;&#21644;&#20154;&#24037;&#26234;&#33021;&#32032;&#20859;&#20513;&#35758;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24037;&#20855;&#30340;&#20986;&#29616;&#65292;&#22914;OpenAI&#30340;ChatGPT&#65292;&#24494;&#36719;&#30340;Bing Chat&#21644;&#35895;&#27468;&#30340;Bard&#65292;&#24050;&#32463;&#24341;&#36215;&#20102;&#24040;&#22823;&#30340;&#20844;&#20247;&#20851;&#27880;&#12290;&#36825;&#20123;&#38750;&#24120;&#26377;&#29992;&#12289;&#33258;&#28982;&#30340;&#24037;&#20855;&#26631;&#24535;&#30528;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#30340;&#37325;&#22823;&#36827;&#23637;&#65292;&#28982;&#32780;&#23427;&#20204;&#23384;&#22312;&#29983;&#25104;&#34394;&#20551;&#12289;&#38169;&#35823;&#25110;&#35823;&#23548;&#20869;&#23481;&#30340;&#20542;&#21521;&#65292;&#36890;&#24120;&#34987;&#31216;&#20026;&#8220;&#24187;&#35273;&#8221;&#12290;&#27492;&#22806;&#65292;LLMs&#21487;&#20197;&#34987;&#29992;&#20110;&#24694;&#24847;&#24212;&#29992;&#65292;&#20363;&#22914;&#22312;&#35268;&#27169;&#19978;&#29983;&#25104;&#34394;&#20551;&#20294;&#21487;&#20449;&#30340;&#20869;&#23481;&#21644;&#20010;&#20154;&#36164;&#26009;&#12290;&#36825;&#23545;&#20110;&#31038;&#20250;&#26469;&#35828;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#21487;&#33021;&#27450;&#39575;&#29992;&#25143;&#24182;&#36234;&#26469;&#36234;&#22810;&#22320;&#20256;&#25773;&#19981;&#20934;&#30830;&#30340;&#20449;&#24687;&#12290;&#37492;&#20110;&#36825;&#20123;&#39118;&#38505;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20107;&#23454;&#26680;&#26597;&#21592;&#12289;&#26032;&#38395;&#26426;&#26500;&#21644;&#26356;&#24191;&#27867;&#30340;&#30740;&#31350;&#21644;&#25919;&#31574;&#30028;&#38656;&#35201;&#30340;&#25216;&#26415;&#21019;&#26032;&#12289;&#30417;&#31649;&#25913;&#38761;&#21644;&#20154;&#24037;&#26234;&#33021;&#32032;&#20859;&#20513;&#35758;&#30340;&#31867;&#22411;&#12290;&#36890;&#36807;&#30830;&#23450;&#39118;&#38505;&#12289;&#36843;&#22312;&#30473;&#30571;&#30340;&#23041;&#32961;&#21644;&#19968;&#20123;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#24076;&#26395;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of tools based on Large Language Models (LLMs), such as OpenAI's ChatGPT, Microsoft's Bing Chat, and Google's Bard, has garnered immense public attention. These incredibly useful, natural-sounding tools mark significant advances in natural language generation, yet they exhibit a propensity to generate false, erroneous, or misleading content -- commonly referred to as "hallucinations." Moreover, LLMs can be exploited for malicious applications, such as generating false but credible-sounding content and profiles at scale. This poses a significant challenge to society in terms of the potential deception of users and the increasing dissemination of inaccurate information. In light of these risks, we explore the kinds of technological innovations, regulatory reforms, and AI literacy initiatives needed from fact-checkers, news organizations, and the broader research and policy communities. By identifying the risks, the imminent threats, and some viable solutions, we seek to she
&lt;/p&gt;</description></item><item><title>Hieros&#26159;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#24207;&#21015;&#30340;&#20998;&#23618;&#24819;&#20687;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#26102;&#38388;&#25277;&#35937;&#30340;&#19990;&#30028;&#34920;&#31034;&#24182;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#22810;&#20010;&#26102;&#38388;&#23610;&#24230;&#19978;&#24819;&#35937;&#36712;&#36857;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#21644;&#24819;&#35937;&#12290;</title><link>http://arxiv.org/abs/2310.05167</link><description>&lt;p&gt;
Hieros: &#22522;&#20110;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#24207;&#21015;&#30340;&#20998;&#23618;&#24819;&#20687;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Hieros: Hierarchical Imagination on Structured State Space Sequence World Models. (arXiv:2310.05167v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05167
&lt;/p&gt;
&lt;p&gt;
Hieros&#26159;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#24207;&#21015;&#30340;&#20998;&#23618;&#24819;&#20687;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#26102;&#38388;&#25277;&#35937;&#30340;&#19990;&#30028;&#34920;&#31034;&#24182;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#22810;&#20010;&#26102;&#38388;&#23610;&#24230;&#19978;&#24819;&#35937;&#36712;&#36857;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#21644;&#24819;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#31639;&#27861;&#38754;&#20020;&#30340;&#26368;&#22823;&#25361;&#25112;&#20043;&#19968;&#26159;&#26679;&#26412;&#25928;&#29575;&#12290;&#35768;&#22810;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#19990;&#30028;&#27169;&#22411;&#65292;&#22312;&#24819;&#35937;&#20013;&#23436;&#20840;&#35757;&#32451;&#20195;&#29702;&#65292;&#28040;&#38500;&#20102;&#22312;&#35757;&#32451;&#26399;&#38388;&#30452;&#25509;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38754;&#20020;&#24819;&#20687;&#20934;&#30830;&#24615;&#12289;&#25506;&#32034;&#33021;&#21147;&#25110;&#36816;&#34892;&#25928;&#29575;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Hieros&#65292;&#19968;&#31181;&#20998;&#23618;&#31574;&#30053;&#65292;&#23427;&#23398;&#20064;&#26102;&#38388;&#25277;&#35937;&#30340;&#19990;&#30028;&#34920;&#31034;&#65292;&#24182;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#22810;&#20010;&#26102;&#38388;&#23610;&#24230;&#19978;&#24819;&#35937;&#36712;&#36857;&#12290;Hieros&#20351;&#29992;&#22522;&#20110;S5&#23618;&#30340;&#19990;&#30028;&#27169;&#22411;&#65292;&#23427;&#22312;&#35757;&#32451;&#26399;&#38388;&#24182;&#34892;&#39044;&#27979;&#19979;&#19968;&#20010;&#19990;&#30028;&#29366;&#24577;&#65292;&#24182;&#22312;&#29615;&#22659;&#20132;&#20114;&#26399;&#38388;&#36827;&#34892;&#36845;&#20195;&#39044;&#27979;&#12290;&#30001;&#20110;S5&#23618;&#30340;&#29305;&#27530;&#24615;&#36136;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#24819;&#35937;&#36807;&#31243;&#20013;&#24182;&#34892;&#35757;&#32451;&#21644;&#36845;&#20195;&#39044;&#27979;&#19979;&#19968;&#20010;&#19990;&#30028;&#29366;&#24577;&#12290;&#36825;&#27604;&#22522;&#20110;RNN&#30340;&#19990;&#30028;&#27169;&#22411;&#20855;&#26377;&#26356;&#39640;&#30340;&#35757;&#32451;&#25928;&#29575;&#65292;&#20063;&#27604;&#22522;&#20110;Transformer&#30340;&#19990;&#30028;&#27169;&#22411;&#20855;&#26377;&#26356;&#39640;&#30340;&#24819;&#35937;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the biggest challenges to modern deep reinforcement learning (DRL) algorithms is sample efficiency. Many approaches learn a world model in order to train an agent entirely in imagination, eliminating the need for direct environment interaction during training. However, these methods often suffer from either a lack of imagination accuracy, exploration capabilities, or runtime efficiency. We propose Hieros, a hierarchical policy that learns time abstracted world representations and imagines trajectories at multiple time scales in latent space. Hieros uses an S5 layer-based world model, which predicts next world states in parallel during training and iteratively during environment interaction. Due to the special properties of S5 layers, our method can train in parallel and predict next world states iteratively during imagination. This allows for more efficient training than RNN-based world models and more efficient imagination than Transformer-based world models.  We show that our 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InstructDET&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#22810;&#26679;&#21270;&#30340;&#25351;&#20196;&#23450;&#20301;&#30446;&#26631;&#23545;&#35937;&#24182;&#36827;&#34892;&#25351;&#31216;&#23545;&#35937;&#26816;&#27979;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#22270;&#20687;&#12289;&#36793;&#30028;&#26694;&#21644;&#27867;&#21270;&#25351;&#20196;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21033;&#29992;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25351;&#20196;&#12290;</title><link>http://arxiv.org/abs/2310.05136</link><description>&lt;p&gt;
InstructDET: &#36890;&#29992;&#25351;&#20196;&#30340;&#24341;&#23548;&#19979;&#30340;&#25351;&#31216;&#23545;&#35937;&#26816;&#27979;&#30340;&#22810;&#26679;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
InstructDET: Diversifying Referring Object Detection with Generalized Instructions. (arXiv:2310.05136v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05136
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InstructDET&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#22810;&#26679;&#21270;&#30340;&#25351;&#20196;&#23450;&#20301;&#30446;&#26631;&#23545;&#35937;&#24182;&#36827;&#34892;&#25351;&#31216;&#23545;&#35937;&#26816;&#27979;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#22270;&#20687;&#12289;&#36793;&#30028;&#26694;&#21644;&#27867;&#21270;&#25351;&#20196;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21033;&#29992;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;InstructDET&#65292;&#19968;&#31181;&#22522;&#20110;&#29992;&#25143;&#25351;&#20196;&#26469;&#23450;&#20301;&#30446;&#26631;&#23545;&#35937;&#30340;&#25351;&#31216;&#23545;&#35937;&#26816;&#27979;&#65288;ROD&#65289;&#30340;&#25968;&#25454;&#20013;&#24515;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;&#22810;&#26679;&#21270;&#30340;&#25351;&#20196;&#65292;&#28085;&#30422;&#19982;&#23545;&#35937;&#26816;&#27979;&#30456;&#20851;&#30340;&#24120;&#35265;&#29992;&#25143;&#24847;&#22270;&#12290;&#23545;&#20110;&#19968;&#24352;&#22270;&#20687;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#22823;&#37327;&#30340;&#25351;&#20196;&#65292;&#28041;&#21450;&#27599;&#20010;&#21333;&#29420;&#30340;&#23545;&#35937;&#21644;&#22810;&#20010;&#23545;&#35937;&#30340;&#19981;&#21516;&#32452;&#21512;&#12290;&#27599;&#20010;&#25351;&#20196;&#21450;&#20854;&#23545;&#24212;&#30340;&#23545;&#35937;&#36793;&#30028;&#26694;&#26500;&#25104;&#19968;&#20010;&#35757;&#32451;&#25968;&#25454;&#23545;&#12290;&#20026;&#20102;&#21253;&#21547;&#24120;&#35265;&#30340;&#26816;&#27979;&#34920;&#36798;&#24335;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#26032;&#20852;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#36890;&#36807;&#25991;&#26412;&#25552;&#31034;&#21644;&#23545;&#35937;&#36793;&#30028;&#26694;&#29983;&#25104;&#25351;&#20196;&#65292;&#22240;&#20026;&#22522;&#30784;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21487;&#20197;&#20135;&#29983;&#31867;&#20284;&#20154;&#31867;&#30340;&#34920;&#36798;&#65288;&#20363;&#22914;&#65292;&#25551;&#36848;&#23545;&#35937;&#23646;&#24615;&#12289;&#31867;&#21035;&#21644;&#20851;&#31995;&#65289;&#12290;&#25105;&#20204;&#23558;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#21629;&#21517;&#20026;InDET&#65292;&#21253;&#21547;&#22270;&#20687;&#12289;&#36793;&#30028;&#26694;&#21644;&#27867;&#21270;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose InstructDET, a data-centric method for referring object detection (ROD) that localizes target objects based on user instructions. While deriving from referring expressions (REC), the instructions we leverage are greatly diversified to encompass common user intentions related to object detection. For one image, we produce tremendous instructions that refer to every single object and different combinations of multiple objects. Each instruction and its corresponding object bounding boxes (bbxs) constitute one training data pair. In order to encompass common detection expressions, we involve emerging vision-language model (VLM) and large language model (LLM) to generate instructions guided by text prompts and object bbxs, as the generalizations of foundation models are effective to produce human-like expressions (e.g., describing object property, category, and relationship). We name our constructed dataset as InDET. It contains images, bbxs and generalized instructions that are 
&lt;/p&gt;</description></item><item><title>DialCoT&#26159;&#19968;&#31181;&#23545;&#35805;&#24341;&#23548;&#30340;&#38142;&#24335;&#24605;&#32500;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#20998;&#35299;&#21644;&#25506;&#32034;&#25512;&#29702;&#36335;&#24452;&#12290;&#36890;&#36807;&#23558;&#22797;&#26434;&#38382;&#39064;&#20998;&#35299;&#20026;&#31616;&#21333;&#30340;&#23376;&#38382;&#39064;&#65292;&#23427;&#38477;&#20302;&#20102;&#20219;&#21153;&#38590;&#24230;&#65292;&#24182;&#20351;&#29992;PPO&#31639;&#27861;&#20248;&#21270;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2310.05074</link><description>&lt;p&gt;
DialCoT&#36935;&#21040;&#20102;PPO&#65306;&#22312;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#20998;&#35299;&#21644;&#25506;&#32034;&#25512;&#29702;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
DialCoT Meets PPO: Decomposing and Exploring Reasoning Paths in Smaller Language Models. (arXiv:2310.05074v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05074
&lt;/p&gt;
&lt;p&gt;
DialCoT&#26159;&#19968;&#31181;&#23545;&#35805;&#24341;&#23548;&#30340;&#38142;&#24335;&#24605;&#32500;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#20998;&#35299;&#21644;&#25506;&#32034;&#25512;&#29702;&#36335;&#24452;&#12290;&#36890;&#36807;&#23558;&#22797;&#26434;&#38382;&#39064;&#20998;&#35299;&#20026;&#31616;&#21333;&#30340;&#23376;&#38382;&#39064;&#65292;&#23427;&#38477;&#20302;&#20102;&#20219;&#21153;&#38590;&#24230;&#65292;&#24182;&#20351;&#29992;PPO&#31639;&#27861;&#20248;&#21270;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#24335;&#24605;&#32500;&#65288;CoT&#65289;&#25552;&#31034;&#24050;&#32463;&#34987;&#35777;&#26126;&#26377;&#21161;&#20110;&#22686;&#24378;&#33267;&#23569;&#20855;&#26377;1000&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#24212;&#29992;&#20110;&#20855;&#26377;&#19981;&#21040;100&#20159;&#21442;&#25968;&#30340;&#36739;&#23567;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#30340;&#25512;&#29702;&#20219;&#21153;&#26102;&#65292;&#23427;&#26159;&#26080;&#25928;&#29978;&#33267;&#26377;&#23475;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#35805;&#24341;&#23548;&#30340;&#38142;&#24335;&#24605;&#32500;&#65288;DialCoT&#65289;&#65292;&#23427;&#37319;&#29992;&#23545;&#35805;&#26684;&#24335;&#29983;&#25104;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#65292;&#24341;&#23548;&#27169;&#22411;&#26397;&#30528;&#26368;&#32456;&#31572;&#26696;&#21069;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;PPO&#31639;&#27861;&#20248;&#21270;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#36873;&#25321;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20854;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#20960;&#20010;&#20248;&#28857;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#30340;&#36807;&#31243;&#20998;&#35299;&#25104;&#19968;&#31995;&#21015;&#26356;&#31616;&#21333;&#30340;&#23376;&#38382;&#39064;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#20219;&#21153;&#30340;&#38590;&#24230;&#65292;&#20351;&#20854;&#26356;&#36866;&#21512;&#20110;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20248;&#21270;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#36873;&#25321;&#65292;&#20351;&#20854;&#26356;&#20934;&#30830;&#21644;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought (CoT) prompting has proven to be effective in enhancing the reasoning capabilities of Large Language Models (LLMs) with at least 100 billion parameters. However, it is ineffective or even detrimental when applied to reasoning tasks in Smaller Language Models (SLMs) with less than 10 billion parameters. To address this limitation, we introduce Dialogue-guided Chain-of-Thought (DialCoT) which employs a dialogue format to generate intermediate reasoning steps, guiding the model toward the final answer. Additionally, we optimize the model's reasoning path selection using the Proximal Policy Optimization (PPO) algorithm, further enhancing its reasoning capabilities. Our method offers several advantages compared to previous approaches. Firstly, we transform the process of solving complex reasoning questions by breaking them down into a series of simpler sub-questions, significantly reducing the task difficulty and making it more suitable for SLMs. Secondly, we optimize the m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;Avalon&#28216;&#25103;&#20013;&#20351;&#29992;LLMs&#30340;&#28508;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;AvalonBench&#26469;&#35780;&#20272;&#22810;&#20195;&#29702;LLM&#20195;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;&#23384;&#22312;&#26126;&#26174;&#30340;&#33021;&#21147;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2310.05036</link><description>&lt;p&gt;
&#20174;&#25991;&#26412;&#21040;&#31574;&#30053;&#65306;&#35780;&#20272;&#22312;Avalon&#28216;&#25103;&#20013;&#21457;&#25381;&#20316;&#29992;&#30340;LLMs
&lt;/p&gt;
&lt;p&gt;
From Text to Tactic: Evaluating LLMs Playing the Game of Avalon. (arXiv:2310.05036v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05036
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;Avalon&#28216;&#25103;&#20013;&#20351;&#29992;LLMs&#30340;&#28508;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;AvalonBench&#26469;&#35780;&#20272;&#22810;&#20195;&#29702;LLM&#20195;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;&#23384;&#22312;&#26126;&#26174;&#30340;&#33021;&#21147;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29609;&#31574;&#30053;&#31038;&#20132;&#25512;&#29702;&#28216;&#25103;Resistance Avalon&#20013;&#30340;&#28508;&#21147;&#12290;Avalon&#29609;&#23478;&#19981;&#20165;&#38656;&#35201;&#26681;&#25454;&#21160;&#24577;&#21457;&#23637;&#30340;&#28216;&#25103;&#38454;&#27573;&#20570;&#20986;&#26126;&#26234;&#30340;&#20915;&#31574;&#65292;&#36824;&#38656;&#35201;&#21442;&#19982;&#35752;&#35770;&#65292;&#22312;&#35752;&#35770;&#20013;&#24517;&#39035;&#27450;&#39575;&#12289;&#25512;&#29702;&#21644;&#19982;&#20854;&#20182;&#29609;&#23478;&#36827;&#34892;&#35848;&#21028;&#12290;&#36825;&#20123;&#29305;&#28857;&#20351;&#24471;Avalon&#25104;&#20026;&#30740;&#31350;LLM&#20195;&#29702;&#30340;&#20915;&#31574;&#21644;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#30340;&#26377;&#36259;&#35797;&#39564;&#24179;&#21488;&#12290;&#20026;&#20102;&#25512;&#21160;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AvalonBench&#8212;&#8212;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#35780;&#20272;&#22810;&#20195;&#29702;LLM&#20195;&#29702;&#30340;&#20840;&#38754;&#28216;&#25103;&#29615;&#22659;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#21253;&#25324;&#65306;&#65288;1&#65289;Avalon&#30340;&#28216;&#25103;&#29615;&#22659;&#65292;&#65288;2&#65289;&#22522;&#20110;&#35268;&#21017;&#30340;&#26426;&#22120;&#20154;&#20316;&#20026;&#22522;&#20934;&#23545;&#25163;&#65292;&#20197;&#21450;&#65288;3&#65289;&#38024;&#23545;&#27599;&#20010;&#35282;&#33394;&#20855;&#26377;&#23450;&#21046;&#25552;&#31034;&#30340;ReAct-style LLM&#20195;&#29702;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#22522;&#20110;AvalonBench&#30340;&#35780;&#20272;&#31361;&#20986;&#26174;&#31034;&#20102;&#26126;&#26174;&#30340;&#33021;&#21147;&#24046;&#36317;&#12290;&#20363;&#22914;&#65292;&#20687;ChatGPT&#36825;&#26679;&#22312;&#22909;&#35282;&#33394;&#20013;&#30340;&#27169;&#22411;&#23545;&#25112;&#22522;&#20110;&#35268;&#21017;&#30340;&#26426;&#22120;&#20154;&#30340;&#32988;&#29575;&#20026;22.2%&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore the potential of Large Language Models (LLMs) Agents in playing the strategic social deduction game, Resistance Avalon. Players in Avalon are challenged not only to make informed decisions based on dynamically evolving game phases, but also to engage in discussions where they must deceive, deduce, and negotiate with other players. These characteristics make Avalon a compelling test-bed to study the decision-making and language-processing capabilities of LLM Agents. To facilitate research in this line, we introduce AvalonBench - a comprehensive game environment tailored for evaluating multi-agent LLM Agents. This benchmark incorporates: (1) a game environment for Avalon, (2) rule-based bots as baseline opponents, and (3) ReAct-style LLM agents with tailored prompts for each role. Notably, our evaluations based on AvalonBench highlight a clear capability gap. For instance, models like ChatGPT playing good-role got a win rate of 22.2% against rule-based bots play
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#21453;&#24605;&#21644;&#25913;&#36827;&#25512;&#29702;&#36807;&#31243;&#65292;&#20197;&#25552;&#21319;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#38382;&#39064;&#22238;&#31572;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.05035</link><description>&lt;p&gt;
&#33258;&#25105;&#39564;&#35777;&#25552;&#31034;&#65306;&#21033;&#29992;&#37325;&#22797;&#20869;&#30465;&#36827;&#34892;&#23569;&#26679;&#26412;&#38382;&#39064;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
Self-Convinced Prompting: Few-Shot Question Answering with Repeated Introspection. (arXiv:2310.05035v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05035
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#21453;&#24605;&#21644;&#25913;&#36827;&#25512;&#29702;&#36807;&#31243;&#65292;&#20197;&#25552;&#21319;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#38382;&#39064;&#22238;&#31572;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20687;ChatGPT&#21644;PaLM&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#22312;&#22797;&#26434;&#25512;&#29702;&#21644;&#32321;&#29712;&#30693;&#35782;&#21033;&#29992;&#26041;&#38754;&#20173;&#28982;&#19981;&#21450;&#20154;&#31867;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#25552;&#31034;&#22312;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26399;&#26395;&#30340;&#36755;&#20986;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#36825;&#20123;&#35265;&#35299;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#20197;&#36845;&#20195;&#30340;&#26041;&#24335;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#21547;&#19977;&#20010;&#32452;&#20214;&#65306;\textit{Normal CoT}&#12289;\textit{Convincer}&#21644;\textit{Answerer}&#12290;&#23427;&#22788;&#29702;typical few-shot chain-of-thought prompt&#30340;&#36755;&#20986;&#65292;&#35780;&#20272;&#22238;&#31572;&#30340;&#27491;&#30830;&#24615;&#65292;&#23457;&#26597;&#31572;&#26696;&#65292;&#25913;&#36827;&#25512;&#29702;&#65292;&#26368;&#32456;&#20135;&#29983;&#19968;&#20010;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#19971;&#20010;&#21508;&#31181;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#33258;&#25105;&#39564;&#35777;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LLMs) such as ChatGPT and PaLM have demonstrated remarkable performance in various language understanding and generation tasks, their capabilities in complex reasoning and intricate knowledge utilization still fall short of human-level proficiency. Recent studies have established the effectiveness of prompts in steering LLMs towards generating desired outputs. Building on these insights, we introduce a novel framework that harnesses the potential of large-scale pre-trained language models, to iteratively enhance performance of the LLMs. Our framework incorporates three components: \textit{Normal CoT}, a \textit{Convincer}, and an \textit{Answerer}. It processes the output of a typical few-shot chain-of-thought prompt, assesses the correctness of the response, scrutinizes the answer, refines the reasoning, and ultimately produces a new solution. Experimental results on the 7 datasets of miscellaneous problems validate the efficacy of the Self-Convince framew
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#38646;-shot&#20851;&#31995;&#25277;&#21462;&#22120;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#36807;&#24635;&#32467;&#21644;&#25552;&#38382;(\textsc{SumAsk})&#25552;&#31034;&#26041;&#27861;&#26469;&#25913;&#36827;&#38646;-shot&#20851;&#31995;&#25277;&#21462;&#12290;&#23454;&#39564;&#35777;&#26126;LLMs&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.05028</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;-shot&#20851;&#31995;&#25277;&#21462;&#22120;
&lt;/p&gt;
&lt;p&gt;
Revisiting Large Language Models as Zero-shot Relation Extractors. (arXiv:2310.05028v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#38646;-shot&#20851;&#31995;&#25277;&#21462;&#22120;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#36807;&#24635;&#32467;&#21644;&#25552;&#38382;(\textsc{SumAsk})&#25552;&#31034;&#26041;&#27861;&#26469;&#25913;&#36827;&#38646;-shot&#20851;&#31995;&#25277;&#21462;&#12290;&#23454;&#39564;&#35777;&#26126;LLMs&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25277;&#21462;(RE)&#21363;&#20351;&#22312;&#38646;-shot&#35774;&#23450;&#19979;&#65292;&#19968;&#30452;&#28041;&#21450;&#19968;&#23450;&#31243;&#24230;&#30340;&#26631;&#35760;&#25110;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#22815;&#22312;&#32473;&#23450;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#26080;&#38656;&#20219;&#20309;&#25968;&#25454;&#21644;&#21442;&#25968;&#35843;&#25972;&#65292;&#33258;&#21160;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#36825;&#20026;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#20851;&#31995;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#12290;&#26412;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#23558;LLMs&#65292;&#22914;ChatGPT&#65292;&#20316;&#20026;&#38646;-shot&#20851;&#31995;&#25277;&#21462;&#22120;&#30340;&#30740;&#31350;&#12290;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#29616;&#26377;RE&#25552;&#31034;&#30340;&#32570;&#28857;&#65292;&#24182;&#23581;&#35797;&#23558;&#26368;&#36817;&#30340;&#25552;&#31034;&#25216;&#26415;&#65292;&#22914;CoT&#65292;&#32435;&#20837;&#20854;&#20013;&#20197;&#25552;&#39640;&#38646;-shot&#20851;&#31995;&#25277;&#21462;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24635;&#32467;&#21644;&#25552;&#38382;(\textsc{SumAsk})&#25552;&#31034;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#25552;&#31034;&#65292;&#36890;&#36807;&#36882;&#24402;&#20351;&#29992;LLMs&#23558;RE&#36755;&#20837;&#36716;&#25442;&#20026;&#26377;&#25928;&#30340;&#38382;&#31572;(QA)&#26684;&#24335;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#23545;&#21508;&#31181;&#22522;&#20934;&#21644;&#35774;&#32622;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#20197;&#35843;&#26597;LLMs&#22312;&#38646;-shot&#20851;&#31995;&#25277;&#21462;&#19978;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26377;&#20197;&#19979;&#30340;followi
&lt;/p&gt;
&lt;p&gt;
Relation extraction (RE) consistently involves a certain degree of labeled or unlabeled data even if under zero-shot setting. Recent studies have shown that large language models (LLMs) transfer well to new tasks out-of-the-box simply given a natural language prompt, which provides the possibility of extracting relations from text without any data and parameter tuning. This work focuses on the study of exploring LLMs, such as ChatGPT, as zero-shot relation extractors. On the one hand, we analyze the drawbacks of existing RE prompts and attempt to incorporate recent prompt techniques such as chain-of-thought (CoT) to improve zero-shot RE. We propose the summarize-and-ask (\textsc{SumAsk}) prompting, a simple prompt recursively using LLMs to transform RE inputs to the effective question answering (QA) format. On the other hand, we conduct comprehensive experiments on various benchmarks and settings to investigate the capabilities of LLMs on zero-shot RE. Specifically, we have the followi
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#26681;&#26412;&#24615;&#30340;&#25913;&#38761;&#65292;&#23558;&#30005;&#23376;&#36135;&#24065;&#32435;&#20837;&#32463;&#27982;&#19982;&#37329;&#34701;&#29702;&#35770;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#21253;&#25324;&#30005;&#23376;&#36135;&#24065;&#30340;&#20272;&#20540;&#22522;&#20110;&#23439;&#35266;&#32463;&#27982;&#29702;&#35770;&#21644;&#36135;&#24065;&#25919;&#31574;&#30340;&#22522;&#26412;&#26041;&#31243;&#65292;&#20197;&#21450;&#30005;&#23376;&#36135;&#24065;&#31649;&#29702;&#20844;&#21496;&#20316;&#20026;&#21327;&#35843;&#27425;&#32463;&#27982;&#20307;&#36135;&#24065;&#21644;&#36130;&#25919;&#25919;&#31574;&#30340;&#23454;&#20307;&#12290;&#35813;&#30740;&#31350;&#36991;&#20813;&#20351;&#29992;&#26222;&#36941;&#20294;&#19981;&#36866;&#24403;&#30340;&#25351;&#25968;&#39118;&#38505;&#27169;&#22411;&#65292;&#32780;&#26159;&#37319;&#29992;&#22810;&#26102;&#38388;&#23610;&#24230;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.04986</link><description>&lt;p&gt;
&#19968;&#31181;&#20851;&#20110;&#36135;&#24065;&#30340;&#26032;&#30340;&#32463;&#27982;&#19982;&#37329;&#34701;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
A new economic and financial theory of money. (arXiv:2310.04986v1 [econ.TH])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04986
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#26681;&#26412;&#24615;&#30340;&#25913;&#38761;&#65292;&#23558;&#30005;&#23376;&#36135;&#24065;&#32435;&#20837;&#32463;&#27982;&#19982;&#37329;&#34701;&#29702;&#35770;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#21253;&#25324;&#30005;&#23376;&#36135;&#24065;&#30340;&#20272;&#20540;&#22522;&#20110;&#23439;&#35266;&#32463;&#27982;&#29702;&#35770;&#21644;&#36135;&#24065;&#25919;&#31574;&#30340;&#22522;&#26412;&#26041;&#31243;&#65292;&#20197;&#21450;&#30005;&#23376;&#36135;&#24065;&#31649;&#29702;&#20844;&#21496;&#20316;&#20026;&#21327;&#35843;&#27425;&#32463;&#27982;&#20307;&#36135;&#24065;&#21644;&#36130;&#25919;&#25919;&#31574;&#30340;&#23454;&#20307;&#12290;&#35813;&#30740;&#31350;&#36991;&#20813;&#20351;&#29992;&#26222;&#36941;&#20294;&#19981;&#36866;&#24403;&#30340;&#25351;&#25968;&#39118;&#38505;&#27169;&#22411;&#65292;&#32780;&#26159;&#37319;&#29992;&#22810;&#26102;&#38388;&#23610;&#24230;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#32463;&#27982;&#19982;&#37329;&#34701;&#29702;&#35770;&#36827;&#34892;&#20102;&#26681;&#26412;&#24615;&#25913;&#38761;&#65292;&#21253;&#25324;&#30005;&#23376;&#36135;&#24065;&#22312;&#20869;&#12290;&#30005;&#23376;&#36135;&#24065;&#30340;&#20272;&#20540;&#23558;&#22522;&#20110;&#23439;&#35266;&#32463;&#27982;&#29702;&#35770;&#21644;&#36135;&#24065;&#25919;&#31574;&#30340;&#22522;&#26412;&#26041;&#31243;&#65292;&#32780;&#19981;&#26159;&#24494;&#35266;&#32463;&#27982;&#23398;&#20013;&#30340;&#36148;&#29616;&#29616;&#37329;&#27969;&#29702;&#35770;&#12290;&#19982;&#23558;&#32929;&#31080;&#35270;&#20026;&#19982;&#27425;&#32463;&#27982;&#20307;&#30340;&#26080;&#24418;&#36164;&#20135;&#20851;&#32852;&#30340;&#25152;&#26377;&#26435;&#19981;&#21516;&#65292;&#25105;&#20204;&#23558;&#21457;&#23637;&#30005;&#23376;&#36135;&#24065;&#20316;&#20026;&#19982;&#27425;&#32463;&#27982;&#20307;&#26377;&#24418;&#36164;&#20135;&#20851;&#32852;&#30340;&#20132;&#26131;&#26435;&#30410;&#30340;&#35266;&#28857;&#12290;&#25105;&#20204;&#36824;&#23558;&#21457;&#23637;&#30005;&#23376;&#36135;&#24065;&#31649;&#29702;&#20844;&#21496;&#20316;&#20026;&#19968;&#20010;&#36127;&#36131;&#21327;&#35843;&#27425;&#32463;&#27982;&#20307;&#30340;&#36135;&#24065;&#65288;&#30005;&#23376;&#36135;&#24065;&#20379;&#24212;&#21644;&#20215;&#20540;&#31283;&#23450;&#65289;&#21644;&#36130;&#25919;&#65288;&#25237;&#36164;&#21644;&#36816;&#33829;&#65289;&#25919;&#31574;&#30340;&#23454;&#20307;&#30340;&#35270;&#35282;&#65292;&#20197;&#23454;&#29616;&#30005;&#23376;&#36135;&#24065;&#30340;&#27969;&#21160;&#24615;&#12290;&#22312;&#20272;&#20540;&#21644;&#20915;&#31574;&#20013;&#20351;&#29992;&#30340;&#39118;&#38505;&#27169;&#22411;&#19981;&#20250;&#26159;&#26080;&#22788;&#19981;&#22312;&#20294;&#19981;&#21512;&#36866;&#30340;&#25351;&#25968;&#39118;&#38505;&#27169;&#22411;&#65292;&#23427;&#23558;&#23548;&#33268;&#36148;&#29616;&#29575;&#65292;&#32780;&#26159;&#22810;&#26102;&#38388;&#23610;&#24230;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper fundamentally reformulates economic and financial theory to include electronic currencies. The valuation of the electronic currencies will be based on macroeconomic theory and the fundamental equation of monetary policy, not the microeconomic theory of discounted cash flows. The view of electronic currency as a transactional equity associated with tangible assets of a sub-economy will be developed, in contrast to the view of stock as an equity associated mostly with intangible assets of a sub-economy. The view will be developed of the electronic currency management firm as an entity responsible for coordinated monetary (electronic currency supply and value stabilization) and fiscal (investment and operational) policies of a substantial (for liquidity of the electronic currency) sub-economy. The risk model used in the valuations and the decision-making will not be the ubiquitous, yet inappropriate, exponential risk model that leads to discount rates, but will be multi time sc
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;Diff-Transfer&#65292;&#19968;&#31181;&#36890;&#36807;&#21487;&#24494;&#20998;&#29289;&#29702;&#20223;&#30495;&#26469;&#39640;&#25928;&#20256;&#36755;&#26426;&#22120;&#20154;&#25216;&#33021;&#30340;&#26032;&#26694;&#26550;&#12290;Diff-Transfer&#36890;&#36807;&#22312;&#20219;&#21153;&#31354;&#38388;&#20869;&#21457;&#29616;&#21487;&#34892;&#36335;&#24452;&#65292;&#23558;&#28304;&#20219;&#21153;&#36716;&#21270;&#20026;&#30446;&#26631;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#26799;&#24230;&#20449;&#24687;&#24341;&#23548;&#36866;&#24212;&#24050;&#30693;&#30340;&#21160;&#20316;&#65292;&#25104;&#21151;&#35299;&#20915;&#21478;&#19968;&#20010;&#23376;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;Diff-Transfer&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.04930</link><description>&lt;p&gt;
Diff-Transfer: &#36890;&#36807;&#21487;&#24494;&#20998;&#29289;&#29702;&#20223;&#30495;&#36827;&#34892;&#22522;&#20110;&#27169;&#22411;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#25216;&#33021;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
Diff-Transfer: Model-based Robotic Manipulation Skill Transfer via Differentiable Physics Simulation. (arXiv:2310.04930v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04930
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;Diff-Transfer&#65292;&#19968;&#31181;&#36890;&#36807;&#21487;&#24494;&#20998;&#29289;&#29702;&#20223;&#30495;&#26469;&#39640;&#25928;&#20256;&#36755;&#26426;&#22120;&#20154;&#25216;&#33021;&#30340;&#26032;&#26694;&#26550;&#12290;Diff-Transfer&#36890;&#36807;&#22312;&#20219;&#21153;&#31354;&#38388;&#20869;&#21457;&#29616;&#21487;&#34892;&#36335;&#24452;&#65292;&#23558;&#28304;&#20219;&#21153;&#36716;&#21270;&#20026;&#30446;&#26631;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#26799;&#24230;&#20449;&#24687;&#24341;&#23548;&#36866;&#24212;&#24050;&#30693;&#30340;&#21160;&#20316;&#65292;&#25104;&#21151;&#35299;&#20915;&#21478;&#19968;&#20010;&#23376;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;Diff-Transfer&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#25484;&#25569;&#30340;&#25216;&#33021;&#36716;&#31227;&#21040;&#23436;&#25104;&#19968;&#31995;&#21015;&#30456;&#20284;&#20294;&#26032;&#39062;&#20219;&#21153;&#30340;&#33021;&#21147;&#23545;&#20110;&#26234;&#33021;&#26426;&#22120;&#20154;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;Diff-Transfer&#65292;&#19968;&#31181;&#21033;&#29992;&#21487;&#24494;&#20998;&#29289;&#29702;&#20223;&#30495;&#26469;&#39640;&#25928;&#20256;&#36755;&#26426;&#22120;&#20154;&#25216;&#33021;&#30340;&#26032;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;Diff-Transfer&#22312;&#20219;&#21153;&#31354;&#38388;&#20869;&#21457;&#29616;&#20102;&#19968;&#26465;&#21487;&#34892;&#30340;&#36335;&#24452;&#65292;&#23558;&#28304;&#20219;&#21153;&#24102;&#21040;&#30446;&#26631;&#20219;&#21153;&#12290;&#22312;&#36825;&#20010;&#20219;&#21153;&#36335;&#24452;&#30340;&#27599;&#23545;&#30456;&#37051;&#28857;&#19978;&#65292;&#21363;&#20004;&#20010;&#23376;&#20219;&#21153;&#20013;&#65292;Diff-Transfer&#36890;&#36807;&#20174;&#19968;&#20010;&#23376;&#20219;&#21153;&#20013;&#36866;&#24212;&#24050;&#30693;&#30340;&#21160;&#20316;&#26469;&#25104;&#21151;&#35299;&#20915;&#21478;&#19968;&#20010;&#23376;&#20219;&#21153;&#12290;&#36866;&#24212;&#36807;&#31243;&#26159;&#30001;&#21487;&#24494;&#20998;&#29289;&#29702;&#20223;&#30495;&#20135;&#29983;&#30340;&#26799;&#24230;&#20449;&#24687;&#24341;&#23548;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#21033;&#29992;&#20855;&#26377;&#20219;&#21153;&#32423;&#29366;&#24577;&#21644;&#22870;&#21169;&#30340;Q&#23398;&#20064;&#26469;&#29983;&#25104;&#23376;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#23454;&#39564;&#20013;&#23454;&#29616;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#24182;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#19978;&#25191;&#34892;&#20102;&#22235;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36801;&#31227;&#20219;&#21153;&#65292;&#23637;&#31034;&#20102;Diff-Transfer&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The capability to transfer mastered skills to accomplish a range of similar yet novel tasks is crucial for intelligent robots. In this work, we introduce $\textit{Diff-Transfer}$, a novel framework leveraging differentiable physics simulation to efficiently transfer robotic skills. Specifically, $\textit{Diff-Transfer}$ discovers a feasible path within the task space that brings the source task to the target task. At each pair of adjacent points along this task path, which is two sub-tasks, $\textit{Diff-Transfer}$ adapts known actions from one sub-task to tackle the other sub-task successfully. The adaptation is guided by the gradient information from differentiable physics simulations. We propose a novel path-planning method to generate sub-tasks, leveraging $Q$-learning with a task-level state and reward. We implement our framework in simulation experiments and execute four challenging transfer tasks on robotic manipulation, demonstrating the efficacy of $\textit{Diff-Transfer}$ thr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;LLMs&#21644;&#33258;&#21160;&#25512;&#29702;&#22120;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#33258;&#21160;&#31243;&#24207;&#39564;&#35777;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#23436;&#22791;&#24615;&#12290;&#36825;&#20010;&#26041;&#27861;&#22312;&#19968;&#20123;&#21512;&#25104;&#21644;&#31454;&#20105;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#23454;&#38469;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.04870</link><description>&lt;p&gt;
Lemur&#65306;&#22312;&#33258;&#21160;&#31243;&#24207;&#39564;&#35777;&#20013;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Lemur: Integrating Large Language Models in Automated Program Verification. (arXiv:2310.04870v2 [cs.FL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;LLMs&#21644;&#33258;&#21160;&#25512;&#29702;&#22120;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#33258;&#21160;&#31243;&#24207;&#39564;&#35777;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#23436;&#22791;&#24615;&#12290;&#36825;&#20010;&#26041;&#27861;&#22312;&#19968;&#20123;&#21512;&#25104;&#21644;&#31454;&#20105;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#23454;&#38469;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLMs&#22312;&#20195;&#30721;&#29702;&#35299;&#33021;&#21147;&#19978;&#30340;&#23637;&#31034;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#23427;&#20204;&#26159;&#21542;&#21487;&#20197;&#29992;&#20110;&#33258;&#21160;&#31243;&#24207;&#39564;&#35777;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#24120;&#38656;&#35201;&#39640;&#32423;&#25277;&#35937;&#25512;&#29702;&#30340;&#20219;&#21153;&#65292;&#23545;&#20110;&#39564;&#35777;&#24037;&#20855;&#26469;&#35828;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;LLMs&#30340;&#33021;&#21147;&#21644;&#33258;&#21160;&#25512;&#29702;&#22120;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#33258;&#21160;&#31243;&#24207;&#39564;&#35777;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#25105;&#20204;&#27491;&#24335;&#25551;&#36848;&#20102;&#36825;&#31181;&#26041;&#27861;&#35770;&#65292;&#23558;&#20854;&#20316;&#20026;&#25512;&#23548;&#35268;&#21017;&#30340;&#38598;&#21512;&#36827;&#34892;&#35770;&#35777;&#20854;&#23436;&#22791;&#24615;&#12290;&#25105;&#20204;&#23558;&#35745;&#31639;&#26426;&#25512;&#29702;&#24418;&#25104;&#20026;&#19968;&#20010;&#23436;&#22791;&#30340;&#33258;&#21160;&#39564;&#35777;&#36807;&#31243;&#65292;&#36825;&#22312;&#19968;&#32452;&#21512;&#25104;&#21644;&#31454;&#20105;&#22522;&#20934;&#19978;&#24102;&#26469;&#20102;&#23454;&#38469;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
The demonstrated code-understanding capability of LLMs raises the question of whether they can be used for automated program verification, a task that often demands high-level abstract reasoning about program properties, which is challenging for verification tools. We propose a general methodology to combine the power of LLMs and automated reasoners for automated program verification. We formally describe this methodology as a set of derivation rules and prove its soundness. We instantiate the calculus as a sound automated verification procedure, which led to practical improvements on a set of synthetic and competition benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#21508;&#31181;&#31867;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#30740;&#65292;&#20171;&#32461;&#20102;&#30693;&#35782;&#25552;&#21462;&#21644;&#25512;&#29702;&#25216;&#26415;&#65292;&#24182;&#23637;&#26395;&#20102;&#32467;&#21512;&#30693;&#35782;&#22270;&#35889;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#21147;&#37327;&#20197;&#21450;&#30693;&#35782;&#24037;&#31243;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2310.04835</link><description>&lt;p&gt;
&#20851;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#28436;&#21270;&#65306;&#19968;&#39033;&#35843;&#30740;&#21644;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
On the Evolution of Knowledge Graphs: A Survey and Perspective. (arXiv:2310.04835v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#21508;&#31181;&#31867;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#30740;&#65292;&#20171;&#32461;&#20102;&#30693;&#35782;&#25552;&#21462;&#21644;&#25512;&#29702;&#25216;&#26415;&#65292;&#24182;&#23637;&#26395;&#20102;&#32467;&#21512;&#30693;&#35782;&#22270;&#35889;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#21147;&#37327;&#20197;&#21450;&#30693;&#35782;&#24037;&#31243;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#26159;&#22810;&#26679;&#21270;&#30693;&#35782;&#30340;&#32467;&#26500;&#21270;&#34920;&#31034;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#26234;&#33021;&#24212;&#29992;&#12290;&#26412;&#25991;&#23545;&#21508;&#31181;&#31867;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#30340;&#28436;&#21270;&#65288;&#38745;&#24577;KGs&#65292;&#21160;&#24577;KGs&#65292;&#26102;&#24577;KGs&#21644;&#20107;&#20214;KGs&#65289;&#21644;&#30693;&#35782;&#25552;&#21462;&#21644;&#25512;&#29702;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;KGs&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#21253;&#25324;&#36130;&#21153;&#20998;&#26512;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#20110;&#30693;&#35782;&#24037;&#31243;&#26410;&#26469;&#26041;&#21521;&#30340;&#23637;&#26395;&#65292;&#21253;&#25324;&#32467;&#21512;&#30693;&#35782;&#22270;&#35889;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21147;&#37327;&#20197;&#21450;&#30693;&#35782;&#25552;&#21462;&#12289;&#25512;&#29702;&#21644;&#34920;&#31034;&#30340;&#28436;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs (KGs) are structured representations of diversified knowledge. They are widely used in various intelligent applications. In this article, we provide a comprehensive survey on the evolution of various types of knowledge graphs (i.e., static KGs, dynamic KGs, temporal KGs, and event KGs) and techniques for knowledge extraction and reasoning. Furthermore, we introduce the practical applications of different types of KGs, including a case study in financial analysis. Finally, we propose our perspective on the future directions of knowledge engineering, including the potential of combining the power of knowledge graphs and large language models (LLMs), and the evolution of knowledge extraction, reasoning, and representation.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20174;Shapley Value&#30340;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#20102;Integrated Gradients&#30340;&#22522;&#32447;&#36873;&#25321;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#32447;&#26500;&#24314;&#26041;&#27861;&#21483;&#20570;Shapley Integrated Gradients (SIG)&#12290;&#22312;GridWorl&#19978;&#30340;&#27169;&#25311;&#23454;&#39564;&#34920;&#26126;&#65292;SIG&#33021;&#22815;&#29983;&#25104;&#26377;&#24847;&#20041;&#21644;&#26080;&#20559;&#30340;&#35299;&#37322;&#27169;&#22411;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.04821</link><description>&lt;p&gt;
&#20174;Shapley Value&#30340;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;Integrated Gradients&#30340;&#22522;&#32447;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Rethink Baseline of Integrated Gradients from the Perspective of Shapley Value. (arXiv:2310.04821v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04821
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20174;Shapley Value&#30340;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#20102;Integrated Gradients&#30340;&#22522;&#32447;&#36873;&#25321;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#32447;&#26500;&#24314;&#26041;&#27861;&#21483;&#20570;Shapley Integrated Gradients (SIG)&#12290;&#22312;GridWorl&#19978;&#30340;&#27169;&#25311;&#23454;&#39564;&#34920;&#26126;&#65292;SIG&#33021;&#22815;&#29983;&#25104;&#26377;&#24847;&#20041;&#21644;&#26080;&#20559;&#30340;&#35299;&#37322;&#27169;&#22411;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26041;&#27861;&#24050;&#32463;&#23581;&#35797;&#36890;&#36807;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#39044;&#27979;&#24402;&#22240;&#20110;&#20854;&#36755;&#20837;&#29305;&#24449;&#26469;&#35299;&#37322;DNN&#12290;&#20854;&#20013;&#19968;&#20010;&#30740;&#31350;&#20805;&#20998;&#30340;&#24402;&#22240;&#26041;&#27861;&#26159;Integrated Gradients&#65288;IG&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36873;&#25321;IG&#30340;&#22522;&#32447;&#26159;&#22312;&#19981;&#21516;&#24773;&#26223;&#19979;&#29983;&#25104;&#26377;&#24847;&#20041;&#21644;&#26080;&#20559;&#35299;&#37322;&#27169;&#22411;&#39044;&#27979;&#30340;&#20851;&#38190;&#32771;&#34385;&#22240;&#32032;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#21033;&#29992;&#21333;&#19968;&#22522;&#32447;&#30340;&#20570;&#27861;&#26410;&#33021;&#23454;&#29616;&#36825;&#20010;&#24895;&#26395;&#65292;&#22240;&#27492;&#38656;&#35201;&#22810;&#20010;&#22522;&#32447;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;IG&#19982;&#22885;&#26364;&#8212;&#22799;&#26222;&#21033;&#65288;Aumann-Shapley&#65289;&#20215;&#20540;&#20043;&#38388;&#30340;&#20869;&#22312;&#32852;&#31995;&#24418;&#25104;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#35270;&#35282;&#65292;&#37325;&#26032;&#24605;&#32771;&#20102;&#22522;&#32447;&#30340;&#35774;&#35745;&#12290;&#22312;&#26576;&#20123;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20986;&#19968;&#32452;&#22522;&#32447;&#19982;Shapley Value&#20013;&#30340;&#32852;&#30431;&#30456;&#23545;&#24212;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#32447;&#26500;&#24314;&#26041;&#27861;&#65292;&#31216;&#20026;Shapley Integrated Gradients&#65288;SIG&#65289;&#65292;&#36890;&#36807;&#27604;&#20363;&#25277;&#26679;&#26469;&#25628;&#32034;&#19968;&#32452;&#22522;&#32447;&#65292;&#20197;&#37096;&#20998;&#27169;&#25311;Shapley Value&#30340;&#35745;&#31639;&#36335;&#24452;&#12290;&#22312;GridWorl&#19978;&#36827;&#34892;&#20102;&#27169;&#25311;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous approaches have attempted to interpret deep neural networks (DNNs) by attributing the prediction of DNN to its input features. One of the well-studied attribution methods is Integrated Gradients (IG). Specifically, the choice of baselines for IG is a critical consideration for generating meaningful and unbiased explanations for model predictions in different scenarios. However, current practice of exploiting a single baseline fails to fulfill this ambition, thus demanding multiple baselines. Fortunately, the inherent connection between IG and Aumann-Shapley Value forms a unique perspective to rethink the design of baselines. Under certain hypothesis, we theoretically analyse that a set of baseline aligns with the coalitions in Shapley Value. Thus, we propose a novel baseline construction method called Shapley Integrated Gradients (SIG) that searches for a set of baselines by proportional sampling to partly simulate the computation path of Shapley Value. Simulations on GridWorl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiffNAS&#30340;&#22522;&#30784;&#27169;&#22411;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#23548;&#26356;&#22909;&#30340;&#32467;&#26500;&#26469;&#21551;&#21160;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#21512;&#25104;&#24615;&#33021;&#12290;&#36890;&#36807;&#21033;&#29992;GPT-4&#20316;&#20026;&#36229;&#32593;&#65292;&#36741;&#20197;&#25628;&#32034;&#20869;&#23384;&#21644;RFID&#20195;&#29702;&#65292;&#20197;&#21450;&#24555;&#36895;&#25910;&#25947;&#35757;&#32451;&#31574;&#30053;&#65292;&#25628;&#32034;&#25928;&#29575;&#25552;&#39640;&#20102;2&#20493;&#65292;&#36798;&#21040;&#20102;2.82&#30340;&#24615;&#33021;&#25552;&#21319;0.37&#12290;</title><link>http://arxiv.org/abs/2310.04750</link><description>&lt;p&gt;
DiffNAS: &#36890;&#36807;&#24341;&#23548;&#26356;&#22909;&#30340;&#32467;&#26500;&#26469;&#21551;&#21160;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DiffNAS: Bootstrapping Diffusion Models by Prompting for Better Architectures. (arXiv:2310.04750v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiffNAS&#30340;&#22522;&#30784;&#27169;&#22411;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#23548;&#26356;&#22909;&#30340;&#32467;&#26500;&#26469;&#21551;&#21160;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#21512;&#25104;&#24615;&#33021;&#12290;&#36890;&#36807;&#21033;&#29992;GPT-4&#20316;&#20026;&#36229;&#32593;&#65292;&#36741;&#20197;&#25628;&#32034;&#20869;&#23384;&#21644;RFID&#20195;&#29702;&#65292;&#20197;&#21450;&#24555;&#36895;&#25910;&#25947;&#35757;&#32451;&#31574;&#30053;&#65292;&#25628;&#32034;&#25928;&#29575;&#25552;&#39640;&#20102;2&#20493;&#65292;&#36798;&#21040;&#20102;2.82&#30340;&#24615;&#33021;&#25552;&#21319;0.37&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#22312;&#36873;&#25321;&#25193;&#25955;&#36335;&#24452;&#20043;&#21518;&#65292;&#20687;UNet&#36825;&#26679;&#30340;&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#21435;&#22122;&#33258;&#32534;&#30721;&#22120;&#65292;&#20027;&#35201;&#39044;&#27979;&#38656;&#35201;&#36880;&#27493;&#28040;&#38500;&#30340;&#22122;&#22768;&#12290;&#22240;&#27492;&#65292;&#37319;&#29992;&#19982;&#39044;&#26399;&#39044;&#31639;&#30456;&#19968;&#33268;&#30340;&#27169;&#22411;&#20197;&#20419;&#36827;&#20248;&#33391;&#30340;&#21512;&#25104;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31934;&#24515;&#20998;&#26512;&#20102;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;"DiffNAS"&#30340;&#22522;&#30784;&#27169;&#22411;&#25628;&#32034;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;GPT-4&#20316;&#20026;&#36229;&#32593;&#26469;&#21152;&#36895;&#25628;&#32034;&#65292;&#36741;&#20197;&#25628;&#32034;&#20869;&#23384;&#20197;&#22686;&#24378;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;RFID&#20316;&#20026;&#20195;&#29702;&#65292;&#24555;&#36895;&#23545;GPT-4&#20135;&#29983;&#30340;&#23454;&#39564;&#32467;&#26524;&#36827;&#34892;&#25490;&#24207;&#12290;&#25105;&#20204;&#36824;&#37319;&#29992;&#20102;&#24555;&#36895;&#25910;&#25947;&#35757;&#32451;&#31574;&#30053;&#26469;&#25552;&#39640;&#25628;&#32034;&#25928;&#29575;&#12290;&#20005;&#26684;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#22522;&#20110;GPT&#30340;&#24773;&#22659;&#19979;&#21487;&#20197;&#23558;&#25628;&#32034;&#25928;&#29575;&#25552;&#39640;2&#20493;&#65292;&#21516;&#26102;&#21462;&#24471;&#20102;2.82&#30340;&#24615;&#33021;&#65292;&#25913;&#21892;&#20102;0.37&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have recently exhibited remarkable performance on synthetic data. After a diffusion path is selected, a base model, such as UNet, operates as a denoising autoencoder, primarily predicting noises that need to be eliminated step by step. Consequently, it is crucial to employ a model that aligns with the expected budgets to facilitate superior synthetic performance. In this paper, we meticulously analyze the diffusion model and engineer a base model search approach, denoted "DiffNAS". Specifically, we leverage GPT-4 as a supernet to expedite the search, supplemented with a search memory to enhance the results. Moreover, we employ RFID as a proxy to promptly rank the experimental outcomes produced by GPT-4. We also adopt a rapid-convergence training strategy to boost search efficiency. Rigorous experimentation corroborates that our algorithm can augment the search efficiency by 2 times under GPT-based scenarios, while also attaining a performance of 2.82 with 0.37 improvem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20851;&#31995;&#25968;&#25454;&#24211;&#20013;&#20026;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#26381;&#21153;&#30340;&#26550;&#26500;&#65292;&#24182;&#24378;&#35843;&#20102;&#19977;&#20010;&#20851;&#38190;&#33539;&#24335;&#65306;&#28145;&#24230;&#23398;&#20064;&#20013;&#24515;&#26550;&#26500;&#12289;UDF&#20013;&#24515;&#26550;&#26500;&#21644;&#20851;&#31995;&#20013;&#24515;&#26550;&#26500;&#12290;&#23613;&#31649;&#27599;&#20010;&#26550;&#26500;&#37117;&#22312;&#29305;&#23450;&#30340;&#20351;&#29992;&#22330;&#26223;&#20013;&#26377;&#28508;&#21147;&#65292;&#20294;&#36824;&#38656;&#35201;&#35299;&#20915;&#23427;&#20204;&#20043;&#38388;&#30340;&#38598;&#25104;&#38382;&#39064;&#21644;&#20013;&#38388;&#22320;&#24102;&#12290;</title><link>http://arxiv.org/abs/2310.04696</link><description>&lt;p&gt;
&#22312;&#20851;&#31995;&#25968;&#25454;&#24211;&#20013;&#20026;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
Serving Deep Learning Model in Relational Databases. (arXiv:2310.04696v2 [cs.DB] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20851;&#31995;&#25968;&#25454;&#24211;&#20013;&#20026;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#26381;&#21153;&#30340;&#26550;&#26500;&#65292;&#24182;&#24378;&#35843;&#20102;&#19977;&#20010;&#20851;&#38190;&#33539;&#24335;&#65306;&#28145;&#24230;&#23398;&#20064;&#20013;&#24515;&#26550;&#26500;&#12289;UDF&#20013;&#24515;&#26550;&#26500;&#21644;&#20851;&#31995;&#20013;&#24515;&#26550;&#26500;&#12290;&#23613;&#31649;&#27599;&#20010;&#26550;&#26500;&#37117;&#22312;&#29305;&#23450;&#30340;&#20351;&#29992;&#22330;&#26223;&#20013;&#26377;&#28508;&#21147;&#65292;&#20294;&#36824;&#38656;&#35201;&#35299;&#20915;&#23427;&#20204;&#20043;&#38388;&#30340;&#38598;&#25104;&#38382;&#39064;&#21644;&#20013;&#38388;&#22320;&#24102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#21516;&#21830;&#19994;&#21644;&#31185;&#23398;&#39046;&#22495;&#20013;&#65292;&#22312;&#20851;&#31995;&#25968;&#25454;&#19978;&#20026;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#26381;&#21153;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#38656;&#27714;&#65292;&#24182;&#24341;&#21457;&#20102;&#26368;&#36817;&#26085;&#30410;&#22686;&#38271;&#30340;&#20852;&#36259;&#12290;&#26412;&#25991;&#36890;&#36807;&#20840;&#38754;&#25506;&#32034;&#20195;&#34920;&#24615;&#26550;&#26500;&#26469;&#28385;&#36275;&#36825;&#20010;&#38656;&#27714;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#19977;&#20010;&#20851;&#38190;&#33539;&#24335;&#65306;&#23574;&#31471;&#30340;&#28145;&#24230;&#23398;&#20064;&#20013;&#24515;&#26550;&#26500;&#23558;&#28145;&#24230;&#23398;&#20064;&#35745;&#31639;&#36716;&#31227;&#21040;&#19987;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#19978;&#12290;&#28508;&#22312;&#30340;UDF&#20013;&#24515;&#26550;&#26500;&#23558;&#19968;&#20010;&#25110;&#22810;&#20010;&#24352;&#37327;&#35745;&#31639;&#23553;&#35013;&#21040;&#25968;&#25454;&#24211;&#31995;&#32479;&#20013;&#30340;&#29992;&#25143;&#23450;&#20041;&#20989;&#25968;(UDFs)&#20013;&#12290;&#28508;&#22312;&#30340;&#20851;&#31995;&#20013;&#24515;&#26550;&#26500;&#26088;&#22312;&#36890;&#36807;&#20851;&#31995;&#36816;&#31639;&#26469;&#34920;&#31034;&#22823;&#35268;&#27169;&#30340;&#24352;&#37327;&#35745;&#31639;&#12290;&#34429;&#28982;&#27599;&#20010;&#26550;&#26500;&#22312;&#29305;&#23450;&#30340;&#20351;&#29992;&#22330;&#26223;&#20013;&#37117;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#25105;&#20204;&#30830;&#23450;&#20102;&#36825;&#20123;&#26550;&#26500;&#20043;&#38388;&#30340;&#26080;&#32541;&#38598;&#25104;&#21644;&#20013;&#38388;&#22320;&#24102;&#20043;&#38388;&#30340;&#32039;&#24613;&#38656;&#27714;&#12290;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#22952;&#30861;&#38598;&#25104;&#30340;&#24046;&#36317;&#65292;&#24182;&#25506;&#32034;&#20102;&#21019;&#26032;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Serving deep learning (DL) models on relational data has become a critical requirement across diverse commercial and scientific domains, sparking growing interest recently. In this visionary paper, we embark on a comprehensive exploration of representative architectures to address the requirement. We highlight three pivotal paradigms: The state-of-the-artDL-Centricarchitecture offloadsDL computations to dedicated DL frameworks. The potential UDF-Centric architecture encapsulates one or more tensor computations into User Defined Functions (UDFs) within the database system. The potentialRelation-Centricarchitecture aims to represent a large-scale tensor computation through relational operators. While each of these architectures demonstrates promise in specific use scenarios, we identify urgent requirements for seamless integration of these architectures and the middle ground between these architectures. We delve into the gaps that impede the integration and explore innovative strategies 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#40065;&#26834;&#24615;&#30340;&#25552;&#21319;&#24314;&#27169;&#26694;&#26550;RUAD&#65292;&#24182;&#36890;&#36807;&#29305;&#24449;&#36873;&#25321;&#21644;&#23545;&#25239;&#29305;&#24449;&#25233;&#21046;&#20004;&#20010;&#23450;&#21046;&#27169;&#22359;&#26356;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#25552;&#21319;&#27169;&#22411;&#30340;&#29305;&#24449;&#25935;&#24863;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.04693</link><description>&lt;p&gt;
&#22686;&#24378;&#40065;&#26834;&#24615;&#30340;&#24102;&#23545;&#25239;&#29305;&#24449;&#25233;&#21046;&#30340;&#25552;&#21319;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Robustness-enhanced Uplift Modeling with Adversarial Feature Desensitization. (arXiv:2310.04693v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#40065;&#26834;&#24615;&#30340;&#25552;&#21319;&#24314;&#27169;&#26694;&#26550;RUAD&#65292;&#24182;&#36890;&#36807;&#29305;&#24449;&#36873;&#25321;&#21644;&#23545;&#25239;&#29305;&#24449;&#25233;&#21046;&#20004;&#20010;&#23450;&#21046;&#27169;&#22359;&#26356;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#25552;&#21319;&#27169;&#22411;&#30340;&#29305;&#24449;&#25935;&#24863;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#21319;&#24314;&#27169;&#22312;&#22312;&#32447;&#33829;&#38144;&#20013;&#23637;&#31034;&#20102;&#38750;&#24120;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24037;&#20316;&#22312;&#19968;&#20123;&#23454;&#38469;&#24212;&#29992;&#20013;&#23481;&#26131;&#21463;&#21040;&#40065;&#26834;&#24615;&#25361;&#25112;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#39318;&#20808;&#23545;&#19978;&#36848;&#29616;&#35937;&#32473;&#20986;&#20102;&#19968;&#20010;&#21487;&#33021;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#39564;&#35777;&#20102;&#22312;&#32447;&#33829;&#38144;&#20013;&#23384;&#22312;&#29305;&#24449;&#25935;&#24863;&#24615;&#38382;&#39064;&#65292;&#19968;&#20123;&#20851;&#38190;&#29305;&#24449;&#30340;&#25200;&#21160;&#20250;&#20005;&#37325;&#24433;&#21709;&#25552;&#21319;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#29978;&#33267;&#23548;&#33268;&#30456;&#21453;&#30340;&#36235;&#21183;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36890;&#36807;&#23545;&#25239;&#29305;&#24449;&#25233;&#21046;&#22686;&#24378;&#40065;&#26834;&#24615;&#30340;&#25552;&#21319;&#24314;&#27169;&#26694;&#26550;&#65288;RUAD&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;RUAD&#36890;&#36807;&#20004;&#20010;&#23450;&#21046;&#27169;&#22359;&#26356;&#26377;&#25928;&#22320;&#20943;&#36731;&#25552;&#21319;&#27169;&#22411;&#30340;&#29305;&#24449;&#25935;&#24863;&#24615;&#65292;&#21253;&#25324;&#19968;&#20010;&#20855;&#26377;&#32852;&#21512;&#22810;&#26631;&#31614;&#24314;&#27169;&#30340;&#29305;&#24449;&#36873;&#25321;&#27169;&#22359;&#65292;&#20197;&#20174;&#36755;&#20837;&#29305;&#24449;&#20013;&#35782;&#21035;&#19968;&#20010;&#20851;&#38190;&#23376;&#38598;&#65292;&#20197;&#21450;&#19968;&#20010;&#37319;&#29992;&#23545;&#25239;&#35757;&#32451;&#21644;&#36719;&#25554;&#20540;&#25805;&#20316;&#30340;&#23545;&#25239;&#29305;&#24449;&#25233;&#21046;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uplift modeling has shown very promising results in online marketing. However, most existing works are prone to the robustness challenge in some practical applications. In this paper, we first present a possible explanation for the above phenomenon. We verify that there is a feature sensitivity problem in online marketing using different real-world datasets, where the perturbation of some key features will seriously affect the performance of the uplift model and even cause the opposite trend. To solve the above problem, we propose a novel robustness-enhanced uplift modeling framework with adversarial feature desensitization (RUAD). Specifically, our RUAD can more effectively alleviate the feature sensitivity of the uplift model through two customized modules, including a feature selection module with joint multi-label modeling to identify a key subset from the input features and an adversarial feature desensitization module using adversarial training and soft interpolation operations t
&lt;/p&gt;</description></item><item><title>LauraGPT&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;GPT&#27169;&#22411;&#65292;&#29992;&#20110;&#38899;&#39057;&#35782;&#21035;&#12289;&#29702;&#35299;&#21644;&#29983;&#25104;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#21253;&#25324;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12289;&#35821;&#38899;&#32763;&#35793;&#12289;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#12289;&#26426;&#22120;&#32763;&#35793;&#31561;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.04673</link><description>&lt;p&gt;
LauraGPT&#65306;&#20351;&#29992;GPT&#36827;&#34892;&#21548;&#12289;&#20851;&#27880;&#12289;&#29702;&#35299;&#21644;&#20877;&#29983;&#38899;&#39057;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
LauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT. (arXiv:2310.04673v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04673
&lt;/p&gt;
&lt;p&gt;
LauraGPT&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;GPT&#27169;&#22411;&#65292;&#29992;&#20110;&#38899;&#39057;&#35782;&#21035;&#12289;&#29702;&#35299;&#21644;&#29983;&#25104;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#21253;&#25324;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12289;&#35821;&#38899;&#32763;&#35793;&#12289;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#12289;&#26426;&#22120;&#32763;&#35793;&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#65288;GPT&#65289;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23558;&#31867;&#20284;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;&#38899;&#39057;&#20219;&#21153;&#30340;&#30740;&#31350;&#26377;&#38480;&#12290;&#20197;&#21069;&#25552;&#20986;&#30340;&#29992;&#20110;&#38899;&#39057;&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35201;&#20040;&#32570;&#20047;&#20805;&#20998;&#30340;&#23450;&#37327;&#35780;&#20272;&#65292;&#35201;&#20040;&#23616;&#38480;&#20110;&#35782;&#21035;&#21644;&#29702;&#35299;&#38899;&#39057;&#20869;&#23481;&#30340;&#20219;&#21153;&#65292;&#35201;&#20040;&#26126;&#26174;&#19981;&#21450;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65288;SOTA&#65289;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LauraGPT&#65292;&#19968;&#20010;&#29992;&#20110;&#38899;&#39057;&#35782;&#21035;&#12289;&#29702;&#35299;&#21644;&#29983;&#25104;&#30340;&#32479;&#19968;GPT&#27169;&#22411;&#12290;LauraGPT&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#22788;&#29702;&#38899;&#39057;&#21644;&#25991;&#26412;&#36755;&#20837;&#65292;&#24182;&#22312;&#20219;&#24847;&#27169;&#24335;&#19979;&#29983;&#25104;&#36755;&#20986;&#12290;&#23427;&#21487;&#20197;&#36827;&#34892;&#19982;&#20869;&#23481;&#12289;&#35821;&#20041;&#12289;&#35821;&#38899;&#23398;&#21644;&#38899;&#39057;&#20449;&#21495;&#20998;&#26512;&#30456;&#20851;&#30340;&#21508;&#31181;&#20219;&#21153;&#12290;&#20854;&#20013;&#19968;&#20123;&#20540;&#24471;&#27880;&#24847;&#30340;&#20219;&#21153;&#21253;&#25324;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12289;&#35821;&#38899;&#21040;&#25991;&#26412;&#32763;&#35793;&#12289;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#12289;&#26426;&#22120;&#32763;&#35793;&#12289;&#35821;&#38899;&#22686;&#24378;&#12289;&#33258;&#21160;&#38899;&#39057;&#25429;&#33719;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Pre-trained Transformer (GPT) models have achieved remarkable performance on various natural language processing tasks. However, there has been limited research on applying similar frameworks to audio tasks. Previously proposed large language models for audio tasks either lack sufficient quantitative evaluations, or are limited to tasks for recognizing and understanding audio content, or significantly underperform existing state-of-the-art (SOTA) models. In this paper, we propose LauraGPT, a unified GPT model for audio recognition, understanding, and generation. LauraGPT is a versatile language model that can process both audio and text inputs and generate outputs in either modalities. It can perform a wide range of tasks related to content, semantics, paralinguistics, and audio-signal analysis. Some of its noteworthy tasks include automatic speech recognition, speech-to-text translation, text-to-speech synthesis, machine translation, speech enhancement, automated audio capt
&lt;/p&gt;</description></item><item><title>Ada-Instruct&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#25351;&#20196;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#23545;&#24320;&#28304;LLMs&#36827;&#34892;&#24494;&#35843;&#65292;&#33021;&#22815;&#29983;&#25104;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#38271;&#24230;&#22823;&#20110;&#31561;&#20110;100&#30340;&#25351;&#20196;&#12290;&#22312;&#20195;&#30721;&#34917;&#20840;&#12289;&#25968;&#23398;&#25512;&#29702;&#21644;&#24120;&#35782;&#25512;&#29702;&#31561;&#20219;&#21153;&#20013;&#65292;Ada-Instruct&#26174;&#31034;&#20986;&#20248;&#20110;&#22522;&#26412;&#27169;&#22411;&#21644;&#24403;&#21069;&#33258;&#25105;&#25351;&#23548;&#26041;&#27861;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.04484</link><description>&lt;p&gt;
Ada-Instruct: &#20026;&#22797;&#26434;&#25512;&#29702;&#35843;&#25972;&#25351;&#20196;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Ada-Instruct: Adapting Instruction Generators for Complex Reasoning. (arXiv:2310.04484v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04484
&lt;/p&gt;
&lt;p&gt;
Ada-Instruct&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#25351;&#20196;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#23545;&#24320;&#28304;LLMs&#36827;&#34892;&#24494;&#35843;&#65292;&#33021;&#22815;&#29983;&#25104;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#38271;&#24230;&#22823;&#20110;&#31561;&#20110;100&#30340;&#25351;&#20196;&#12290;&#22312;&#20195;&#30721;&#34917;&#20840;&#12289;&#25968;&#23398;&#25512;&#29702;&#21644;&#24120;&#35782;&#25512;&#29702;&#31561;&#20219;&#21153;&#20013;&#65292;Ada-Instruct&#26174;&#31034;&#20986;&#20248;&#20110;&#22522;&#26412;&#27169;&#22411;&#21644;&#24403;&#21069;&#33258;&#25105;&#25351;&#23548;&#26041;&#27861;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#22810;&#26679;&#19988;&#22797;&#26434;&#30340;&#25351;&#20196;&#23545;&#20110;&#25512;&#36827;&#25928;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#21033;&#29992;&#38381;&#28304;&#30340;LLMs&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#25552;&#31034;&#36827;&#34892;&#25351;&#20196;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#21457;&#29616;&#23545;&#20110;&#35832;&#22914;&#20195;&#30721;&#34917;&#20840;&#31561;&#20219;&#21153;&#65292;&#19978;&#19979;&#25991;&#25552;&#31034;&#26080;&#27861;&#29983;&#25104;&#38271;&#24230;&#22823;&#20110;&#31561;&#20110;100&#30340;&#22797;&#26434;&#25351;&#20196;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;Ada-Instruct&#65292;&#19968;&#31181;&#36890;&#36807;&#23545;&#24320;&#28304;LLMs&#36827;&#34892;&#24494;&#35843;&#30340;&#33258;&#36866;&#24212;&#25351;&#20196;&#29983;&#25104;&#22120;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#21457;&#29616;&#34920;&#26126;&#65292;&#20165;&#20351;&#29992;&#21313;&#20010;&#26679;&#26412;&#23545;&#24320;&#28304;LLMs&#36827;&#34892;&#24494;&#35843;&#21363;&#21487;&#29983;&#25104;&#20445;&#25345;&#20998;&#24067;&#19968;&#33268;&#24615;&#30340;&#38271;&#25351;&#20196;&#65292;&#36866;&#29992;&#20110;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#20195;&#30721;&#34917;&#20840;&#12289;&#25968;&#23398;&#25512;&#29702;&#21644;&#24120;&#35782;&#25512;&#29702;&#31561;&#19981;&#21516;&#24212;&#29992;&#20013;&#23545;Ada-Instruct&#30340;&#26377;&#25928;&#24615;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#12290;&#32467;&#26524;&#26174;&#31034;Ada-Instruct&#20248;&#20110;&#20854;&#22522;&#26412;&#27169;&#22411;&#21644;&#24403;&#21069;&#30340;&#33258;&#25105;&#25351;&#23548;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating diverse and sophisticated instructions for downstream tasks by Large Language Models (LLMs) is pivotal for advancing the effect. Current approaches leverage closed-source LLMs, employing in-context prompting for instruction generation. However, in this paper, we found that in-context prompting cannot generate complex instructions with length $\ge 100$ for tasks like code completion.  To solve this problem, we introduce Ada-Instruct, an adaptive instruction generator developed by fine-tuning open-source LLMs. Our pivotal finding illustrates that fine-tuning open-source LLMs with a mere ten samples generates long instructions that maintain distributional consistency for complex reasoning tasks. We empirically validated Ada-Instruct's efficacy across different applications, including code completion, mathematical reasoning, and commonsense reasoning. The results underscore Ada-Instruct's superiority, evidencing its improvements over its base models, current self-instruct method
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24179;&#21488;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25776;&#20889;&#21644;&#35780;&#35770;&#35843;&#26597;&#35770;&#25991;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#32452;&#32455;&#20102;&#19968;&#27425;&#31454;&#36187;&#26469;&#27979;&#35797;&#35813;&#24179;&#21488;&#12290;&#35780;&#20272;&#26631;&#20934;&#21253;&#25324;&#28165;&#26224;&#24230;&#12289;&#21442;&#32771;&#36866;&#24403;&#24615;&#12289;&#21487;&#36861;&#28335;&#24615;&#21644;&#20869;&#23481;&#30340;&#23454;&#36136;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2310.04480</link><description>&lt;p&gt;
&#33258;&#21160;&#35843;&#26597;&#25361;&#25112;&#12290;&#65288;arXiv:2310.04480v2 [cs.CL]&#24050;&#26356;&#26032;&#65289;
&lt;/p&gt;
&lt;p&gt;
Auto-survey Challenge. (arXiv:2310.04480v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04480
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24179;&#21488;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25776;&#20889;&#21644;&#35780;&#35770;&#35843;&#26597;&#35770;&#25991;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#32452;&#32455;&#20102;&#19968;&#27425;&#31454;&#36187;&#26469;&#27979;&#35797;&#35813;&#24179;&#21488;&#12290;&#35780;&#20272;&#26631;&#20934;&#21253;&#25324;&#28165;&#26224;&#24230;&#12289;&#21442;&#32771;&#36866;&#24403;&#24615;&#12289;&#21487;&#36861;&#28335;&#24615;&#21644;&#20869;&#23481;&#30340;&#23454;&#36136;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#24179;&#21488;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#23398;&#31185;&#39046;&#22495;&#65292;&#21253;&#25324;&#31185;&#23398;&#12289;&#20154;&#25991;&#12289;&#25945;&#32946;&#21644;&#27861;&#24459;&#20013;&#65292;&#33258;&#20027;&#25776;&#20889;&#21644;&#35780;&#35770;&#35843;&#26597;&#35770;&#25991;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36827;&#34892;&#31867;&#20284;&#20110;&#20256;&#32479;&#23398;&#26415;&#26399;&#21002;&#30340;&#27169;&#25311;&#21516;&#34892;&#35780;&#23457;&#26426;&#21046;&#65292;&#32780;&#20154;&#31867;&#32452;&#32455;&#32773;&#21017;&#20805;&#24403;&#32534;&#36753;&#30417;&#30563;&#35282;&#33394;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#32452;&#32455;&#20102;&#19968;&#27425;&#38024;&#23545;2023&#24180;AutoML&#20250;&#35758;&#30340;&#31454;&#36187;&#12290;&#21442;&#36187;&#32773;&#30340;&#20219;&#21153;&#26159;&#21576;&#29616;&#33021;&#22815;&#26681;&#25454;&#25351;&#23450;&#25552;&#31034;&#25776;&#20889;&#25991;&#31456;&#24182;&#36827;&#34892;&#35780;&#20272;&#30340;&#29420;&#31435;&#27169;&#22411;&#12290;&#35780;&#20272;&#26631;&#20934;&#21253;&#25324;&#28165;&#26224;&#24230;&#12289;&#21442;&#32771;&#36866;&#24403;&#24615;&#12289;&#21487;&#36861;&#28335;&#24615;&#21644;&#20869;&#23481;&#30340;&#23454;&#36136;&#20215;&#20540;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#31454;&#36187;&#30340;&#35774;&#35745;&#65292;&#21253;&#25324;&#23454;&#26045;&#22522;&#20934;&#25552;&#20132;&#21644;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel platform for evaluating the capability of Large Language Models (LLMs) to autonomously compose and critique survey papers spanning a vast array of disciplines including sciences, humanities, education, and law. Within this framework, AI systems undertake a simulated peer-review mechanism akin to traditional scholarly journals, with human organizers serving in an editorial oversight capacity. Within this framework, we organized a competition for the AutoML conference 2023. Entrants are tasked with presenting stand-alone models adept at authoring articles from designated prompts and subsequently appraising them. Assessment criteria include clarity, reference appropriateness, accountability, and the substantive value of the content. This paper presents the design of the competition, including the implementation baseline submissions and methods of evaluation.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21453;&#21521;&#38142;&#30340;&#36890;&#29992;&#35268;&#21017;&#65292;&#36890;&#36807;&#21453;&#21521;&#38142;&#24605;&#36335;&#20351;LLMs&#33021;&#22815;&#20351;&#29992;&#22806;&#37096;API&#23436;&#25104;&#22797;&#26434;&#30340;&#20989;&#25968;&#35843;&#29992;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#22635;&#20805;&#21442;&#25968;&#30340;&#26041;&#24335;&#25552;&#39640;&#20219;&#21153;&#23436;&#25104;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.04474</link><description>&lt;p&gt;
&#21453;&#21521;&#38142;&#65306;&#19968;&#31181;&#36890;&#29992;&#35268;&#21017;&#65292;&#29992;&#20110;&#20351;LLMs&#25484;&#25569;&#22810;API&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Reverse Chain: A Generic-Rule for LLMs to Master Multi-API Planning. (arXiv:2310.04474v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04474
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21453;&#21521;&#38142;&#30340;&#36890;&#29992;&#35268;&#21017;&#65292;&#36890;&#36807;&#21453;&#21521;&#38142;&#24605;&#36335;&#20351;LLMs&#33021;&#22815;&#20351;&#29992;&#22806;&#37096;API&#23436;&#25104;&#22797;&#26434;&#30340;&#20989;&#25968;&#35843;&#29992;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#22635;&#20805;&#21442;&#25968;&#30340;&#26041;&#24335;&#25552;&#39640;&#20219;&#21153;&#23436;&#25104;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#20989;&#25968;&#35843;&#29992;&#65288;&#21363;API&#65289;&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#39640;LLMs&#30340;&#24615;&#33021;&#65292;&#20294;&#30001;&#20110;&#19981;&#21516;API&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#20989;&#25968;&#35843;&#29992;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#21487;&#25511;&#30340;&#30446;&#26631;&#39537;&#21160;&#26041;&#27861;&#65292;&#31216;&#20026;&#21453;&#21521;&#38142;&#65292;&#20197;&#20351;LLMs&#33021;&#22815;&#20165;&#36890;&#36807;&#25552;&#31034;&#20351;&#29992;&#22806;&#37096;API&#12290;&#22312;&#21453;&#21521;&#38142;&#20013;&#65292;&#22823;&#22810;&#25968;&#24320;&#28304;LLMs&#20165;&#29992;&#20110;&#23454;&#29616;&#31616;&#21333;&#20219;&#21153;&#65292;&#20363;&#22914;API&#36873;&#25321;&#21644;&#21442;&#25968;&#34917;&#20840;&#65292;&#24182;&#20351;&#29992;&#36890;&#29992;&#35268;&#21017;&#23454;&#29616;&#21487;&#25511;&#30340;&#22810;&#20989;&#25968;&#35843;&#29992;&#12290;&#22312;&#36825;&#20010;&#36890;&#29992;&#35268;&#21017;&#20013;&#65292;&#36873;&#25321;&#19968;&#20010;&#26368;&#32456;API&#26469;&#22788;&#29702;&#32473;&#23450;&#20219;&#21153;&#20043;&#21518;&#65292;&#25105;&#20204;&#39318;&#20808;&#35201;&#27714;LLMs&#20174;&#29992;&#25143;&#26597;&#35810;&#21644;&#19978;&#19979;&#25991;&#20013;&#22635;&#20889;&#25152;&#38656;&#30340;&#21442;&#25968;&#12290;&#19968;&#20123;&#32570;&#22833;&#30340;&#21442;&#25968;&#21487;&#20197;&#36890;&#36807;&#35753;LLMs&#22522;&#20110;API&#25551;&#36848;&#36873;&#25321;&#21478;&#19968;&#20010;API&#26469;&#36827;&#19968;&#27493;&#23436;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
While enabling large language models to implement function calling (known as APIs) can greatly enhance the performance of LLMs, function calling is still a challenging task due to the complicated relations between different APIs, especially in a context-learning setting without fine-tuning. This paper proposes a simple yet controllable target-driven approach called Reverse Chain to empower LLMs with capabilities to use external APIs with only prompts. Given that most open-source LLMs have limited tool-use or tool-plan capabilities, LLMs in Reverse Chain are only employed to implement simple tasks, e.g., API selection and argument completion, and a generic rule is employed to implement a controllable multiple functions calling. In this generic rule, after selecting a final API to handle a given task via LLMs, we first ask LLMs to fill the required arguments from user query and context. Some missing arguments could be further completed by letting LLMs select another API based on API desc
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23558;&#25552;&#31034;&#24037;&#31243;&#24418;&#24335;&#21270;&#20026;LLM&#19978;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#30740;&#31350;&#20102;&#32473;&#23450;token&#24207;&#21015;&#26102;&#26159;&#21542;&#23384;&#22312;&#19968;&#31181;&#26368;&#20248;&#25552;&#31034;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26368;&#32456;&#30340;token&#65292;&#24182;&#25552;&#20986;&#20102;&#25511;&#21046;&#29702;&#35770;&#20013;&#30340;&#25351;&#26631;&#26469;&#25551;&#36848;LLM&#30340;&#21487;&#25805;&#32437;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.04444</link><description>&lt;p&gt;
&#39764;&#27861;&#35789;&#26159;&#20160;&#20040;&#65311;LLM&#25552;&#31034;&#30340;&#25511;&#21046;&#29702;&#35770;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
What's the Magic Word? A Control Theory of LLM Prompting. (arXiv:2310.04444v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23558;&#25552;&#31034;&#24037;&#31243;&#24418;&#24335;&#21270;&#20026;LLM&#19978;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#30740;&#31350;&#20102;&#32473;&#23450;token&#24207;&#21015;&#26102;&#26159;&#21542;&#23384;&#22312;&#19968;&#31181;&#26368;&#20248;&#25552;&#31034;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26368;&#32456;&#30340;token&#65292;&#24182;&#25552;&#20986;&#20102;&#25511;&#21046;&#29702;&#35770;&#20013;&#30340;&#25351;&#26631;&#26469;&#25551;&#36848;LLM&#30340;&#21487;&#25805;&#32437;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#24037;&#31243;&#22312;LLM&#30340;&#37096;&#32626;&#20013;&#26159;&#26377;&#25928;&#21644;&#37325;&#35201;&#30340;&#65292;&#20294;&#22312;&#25968;&#23398;&#19978;&#29702;&#35299;&#19981;&#36275;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23558;&#25552;&#31034;&#24037;&#31243;&#24418;&#24335;&#21270;&#20026;LLM&#19978;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#20854;&#20013;&#25552;&#31034;&#34987;&#35748;&#20026;&#26159;&#35843;&#33410;LLM&#36755;&#20986;&#20998;&#24067;&#30340;&#25511;&#21046;&#21464;&#37327;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#31616;&#21333;&#30340;&#38382;&#39064;&#65306;&#32473;&#23450;&#19968;&#20010;token&#24207;&#21015;&#65292;&#26159;&#21542;&#24635;&#23384;&#22312;&#19968;&#20010;&#25105;&#20204;&#21487;&#20197;&#28155;&#21152;&#30340;&#25552;&#31034;&#65292;&#20351;&#24471;LLM&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26368;&#32456;&#30340;token&#65311;&#25105;&#20204;&#23558;&#36825;&#26679;&#30340;&#26368;&#20248;&#25552;&#31034;&#31216;&#20026;&#39764;&#27861;&#35789;&#65292;&#22240;&#20026;&#28155;&#21152;&#25552;&#31034;&#20250;&#23548;&#33268;LLM&#36755;&#20986;&#27491;&#30830;&#30340;&#31572;&#26696;&#12290;&#22914;&#26524;&#23384;&#22312;&#39764;&#27861;&#35789;&#65292;&#25105;&#20204;&#33021;&#21542;&#25214;&#21040;&#23427;&#20204;&#65311;&#22914;&#26524;&#21487;&#20197;&#65292;&#23427;&#20204;&#30340;&#29305;&#24615;&#26159;&#20160;&#20040;&#65311;&#25105;&#20204;&#25552;&#20379;&#20102;&#23558;&#25511;&#21046;&#29702;&#35770;&#24212;&#29992;&#20110;&#33258;&#27880;&#24847;&#21147;&#22836;&#30340;&#20998;&#26512;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#20854;&#26435;&#37325;&#30697;&#38453;&#30340;&#22855;&#24322;&#20540;&#20989;&#25968;&#20026;&#21487;&#25511;&#21046;&#24615;&#30340;&#19978;&#30028;&#12290;&#25105;&#20204;&#20511;&#37492;&#25511;&#21046;&#29702;&#35770;&#26469;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;$k-\epsilon$&#21487;&#25511;&#21046;&#24615;&#30340;&#25351;&#26631;&#65292;&#29992;&#20110;&#25551;&#36848;LLM&#30340;&#21487;&#25805;&#32437;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt engineering is effective and important in the deployment of LLMs but is poorly understood mathematically. Here, we formalize prompt engineering as an optimal control problem on LLMs -- where the prompt is considered a control variable for modulating the output distribution of the LLM. Within this framework, we ask a simple question: given a sequence of tokens, does there always exist a prompt we can prepend that will steer the LLM toward accurately predicting the final token? We call such an optimal prompt the magic word since prepending the prompt causes the LLM to output the correct answer. If magic words exist, can we find them? If so, what are their properties? We offer analytic analysis on the controllability of the self-attention head where we prove a bound on controllability as a function of the singular values of its weight matrices. We take inspiration from control theory to propose a metric called $k-\epsilon$ controllability to characterize LLM steerability. We comput
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#30740;&#31350;&#20102;&#32452;&#21512;&#22870;&#21169;&#27169;&#22411;&#20013;&#30340;&#36807;&#24230;&#20248;&#21270;&#38382;&#39064;&#65292;&#21457;&#29616;&#32452;&#25104;&#22870;&#21169;&#27169;&#22411;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#23545;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#24335;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.04373</link><description>&lt;p&gt;
&#29992;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#23545;&#25239;&#22870;&#21169;&#27169;&#22411;&#36807;&#24230;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Confronting Reward Model Overoptimization with Constrained RLHF. (arXiv:2310.04373v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#30740;&#31350;&#20102;&#32452;&#21512;&#22870;&#21169;&#27169;&#22411;&#20013;&#30340;&#36807;&#24230;&#20248;&#21270;&#38382;&#39064;&#65292;&#21457;&#29616;&#32452;&#25104;&#22870;&#21169;&#27169;&#22411;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#23545;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#24335;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#36890;&#36807;&#20248;&#21270;&#36866;&#24212;&#20154;&#31867;&#21453;&#39304;&#30340;&#22870;&#21169;&#27169;&#22411;&#26469;&#19982;&#20154;&#31867;&#20559;&#22909;&#20445;&#25345;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#20559;&#22909;&#26159;&#22810;&#26041;&#38754;&#30340;&#65292;&#36234;&#26469;&#36234;&#24120;&#35265;&#30340;&#20570;&#27861;&#26159;&#20174;&#20960;&#20010;&#31616;&#21333;&#30340;&#22870;&#21169;&#27169;&#22411;&#20013;&#27966;&#29983;&#20986;&#22870;&#21169;&#65292;&#27599;&#20010;&#27169;&#22411;&#25429;&#25417;&#35821;&#35328;&#36136;&#37327;&#30340;&#19981;&#21516;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#24403;&#32452;&#21512;&#36825;&#20123;&#32452;&#25104;&#30340;&#22870;&#21169;&#27169;&#22411;&#26102;&#65292;&#36866;&#24403;&#22320;&#21152;&#26435;&#21464;&#24471;&#22256;&#38590;&#12290;&#26356;&#21152;&#22256;&#38590;&#30340;&#26159;&#65292;&#30001;&#20110;&#20219;&#20309;&#22870;&#21169;&#27169;&#22411;&#21482;&#26159;&#20154;&#31867;&#35780;&#20215;&#30340;&#20195;&#29702;&#65292;&#36825;&#19968;&#36807;&#31243;&#23481;&#26131;&#21463;&#21040;&#36807;&#24230;&#20248;&#21270;&#30340;&#24433;&#21709;&#65292;&#21363;&#36229;&#36807;&#26576;&#19968;&#28857;&#21518;&#65292;&#33719;&#24471;&#26356;&#39640;&#22870;&#21169;&#19982;&#26356;&#24046;&#30340;&#20154;&#31867;&#35780;&#20215;&#30456;&#20851;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#32452;&#21512;&#22870;&#21169;&#27169;&#22411;&#20013;&#36807;&#24230;&#20248;&#21270;&#36827;&#34892;&#30740;&#31350;&#65292;&#39318;&#27425;&#23637;&#31034;&#20102;&#32452;&#25104;&#22870;&#21169;&#27169;&#22411;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#23545;&#36825;&#20123;&#28857;&#30340;&#20301;&#32622;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models are typically aligned with human preferences by optimizing $\textit{reward models}$ (RMs) fitted to human feedback. However, human preferences are multi-faceted, and it is increasingly common to derive reward from a composition of simpler reward models which each capture a different aspect of language quality. This itself presents a challenge, as it is difficult to appropriately weight these component RMs when combining them. Compounding this difficulty, because any RM is only a proxy for human evaluation, this process is vulnerable to $\textit{overoptimization}$, wherein past a certain point, accumulating higher reward is associated with worse human ratings. In this paper, we perform, to our knowledge, the first study on overoptimization in composite RMs, showing that correlation between component RMs has a significant effect on the locations of these points. We then introduce an approach to solve this issue using constrained reinforcement learning as a means of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#32508;&#21512;&#35780;&#20272;&#65292;&#21457;&#29616;&#38646;&#26679;&#26412;LLMs&#22312;&#23567;&#26679;&#26412;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#29978;&#33267;&#36229;&#36807;&#20102;&#20808;&#36827;&#30340;&#31934;&#35843;&#29983;&#29289;&#21307;&#23398;&#27169;&#22411;&#65292;&#39044;&#35757;&#32451;&#20351;LLMs&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20855;&#22791;&#20102;&#24456;&#24378;&#30340;&#19987;&#19994;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.04270</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#32508;&#21512;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Evaluation of Large Language Models on Benchmark Biomedical Text Processing Tasks. (arXiv:2310.04270v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04270
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#32508;&#21512;&#35780;&#20272;&#65292;&#21457;&#29616;&#38646;&#26679;&#26412;LLMs&#22312;&#23567;&#26679;&#26412;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#29978;&#33267;&#36229;&#36807;&#20102;&#20808;&#36827;&#30340;&#31934;&#35843;&#29983;&#29289;&#21307;&#23398;&#27169;&#22411;&#65292;&#39044;&#35757;&#32451;&#20351;LLMs&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20855;&#22791;&#20102;&#24456;&#24378;&#30340;&#19987;&#19994;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#30340;&#20986;&#33394;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#30446;&#21069;&#36824;&#27809;&#26377;&#30740;&#31350;&#23427;&#20204;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#22522;&#20934;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23545;6&#20010;&#19981;&#21516;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#30340;26&#20010;&#25968;&#25454;&#38598;&#20013;&#30340;4&#20010;&#28909;&#38376;LLMs&#36827;&#34892;&#20102;&#32508;&#21512;&#35780;&#20272;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#31687;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#23545;&#21508;&#31181;LLMs&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#21644;&#27604;&#36739;&#30340;&#30740;&#31350;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#26681;&#25454;&#25105;&#20204;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#35757;&#32451;&#38598;&#36739;&#23567;&#30340;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#20013;&#65292;&#38646;&#26679;&#26412;LLMs&#29978;&#33267;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#31934;&#35843;&#29983;&#29289;&#21307;&#23398;&#27169;&#22411;&#12290;&#36825;&#34920;&#26126;&#22312;&#22823;&#22411;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#20351;LLMs&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20855;&#22791;&#20102;&#24456;&#24378;&#30340;&#19987;&#19994;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#27809;&#26377;&#19968;&#20010;LLM&#33021;&#22815;&#32988;&#36807;&#20854;&#20182;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Large Language Models (LLM) have demonstrated impressive capability to solve a wide range of tasks. However, despite their success across various tasks, no prior work has investigated their capability in the biomedical domain yet. To this end, this paper aims to evaluate the performance of LLMs on benchmark biomedical tasks. For this purpose, we conduct a comprehensive evaluation of 4 popular LLMs in 6 diverse biomedical tasks across 26 datasets. To the best of our knowledge, this is the first work that conducts an extensive evaluation and comparison of various LLMs in the biomedical domain. Interestingly, we find based on our evaluation that in biomedical datasets that have smaller training sets, zero-shot LLMs even outperform the current state-of-the-art fine-tuned biomedical models. This suggests that pretraining on large text corpora makes LLMs quite specialized even in the biomedical domain. We also find that not a single LLM can outperform other LLMs in all tasks, with 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#38142;&#26465;&#65288;CoNLI&#65289;&#26469;&#26816;&#27979;&#21644;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24187;&#35273;&#12290;&#35813;&#26694;&#26550;&#19981;&#38656;&#35201;&#23545;LLMs&#36827;&#34892;&#24494;&#35843;&#25110;&#29305;&#23450;&#39046;&#22495;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#33021;&#22815;&#20174;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;&#20013;&#23454;&#29616;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#30340;&#24187;&#35273;&#26816;&#27979;&#21644;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2310.03951</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#38142;&#26465;&#29992;&#20110;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26080;&#26681;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Chain of Natural Language Inference for Reducing Large Language Model Ungrounded Hallucinations. (arXiv:2310.03951v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03951
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#38142;&#26465;&#65288;CoNLI&#65289;&#26469;&#26816;&#27979;&#21644;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24187;&#35273;&#12290;&#35813;&#26694;&#26550;&#19981;&#38656;&#35201;&#23545;LLMs&#36827;&#34892;&#24494;&#35843;&#25110;&#29305;&#23450;&#39046;&#22495;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#33021;&#22815;&#20174;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;&#20013;&#23454;&#29616;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#30340;&#24187;&#35273;&#26816;&#27979;&#21644;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#32473;&#23450;&#30456;&#20851;&#25991;&#26723;&#20316;&#20026;&#32972;&#26223;&#19978;&#19979;&#25991;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#29983;&#25104;&#27969;&#21033;&#30340;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#12290;&#36825;&#31181;&#33021;&#21147;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;LLMs&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;LLMs&#23481;&#26131;&#20135;&#29983;&#27809;&#26377;&#25552;&#20379;&#26469;&#28304;&#25903;&#25345;&#30340;&#24187;&#35273;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#26694;&#26550;&#26469;&#26816;&#27979;&#21644;&#20943;&#23569;&#36825;&#31181;&#26080;&#26681;&#24187;&#35273;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#38142;&#26465;&#65288;CoNLI&#65289;&#36827;&#34892;&#24187;&#35273;&#26816;&#27979;&#65292;&#24182;&#36890;&#36807;&#21518;&#26399;&#32534;&#36753;&#36827;&#34892;&#24187;&#35273;&#20943;&#23569;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24187;&#35273;&#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#36890;&#36807;&#37325;&#20889;&#22686;&#24378;&#25991;&#26412;&#36136;&#37327;&#65292;&#20351;&#29992;LLMs&#32780;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#24494;&#35843;&#25110;&#29305;&#23450;&#39046;&#22495;&#30340;&#25552;&#31034;&#24037;&#31243;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#31616;&#21333;&#30340;&#21363;&#25554;&#21363;&#29992;&#26694;&#26550;&#21487;&#20197;&#20316;&#20026;&#24187;&#35273;&#26816;&#27979;&#21644;&#20943;&#23569;&#30340;&#26377;&#25928;&#36873;&#25321;&#65292;&#22312;&#21508;&#31181;&#24773;&#22659;&#19979;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) can generate fluent natural language texts when given relevant documents as background context. This ability has attracted considerable interest in developing industry applications of LLMs. However, LLMs are prone to generate hallucinations that are not supported by the provided sources. In this paper, we propose a hierarchical framework to detect and mitigate such ungrounded hallucination. Our framework uses Chain of Natural Language Inference (CoNLI) for hallucination detection and hallucination reduction via post-editing. Our approach achieves state-of-the-art performance on hallucination detection and enhances text quality through rewrite, using LLMs without any fine-tuning or domain-specific prompt engineering. We show that this simple plug-and-play framework can serve as an effective choice for hallucination detection and reduction, achieving competitive performance across various contexts.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65288;TSC&#65289;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#23558;&#26368;&#20808;&#36827;&#30340;&#19968;&#32500;&#21367;&#31215;&#27169;&#22411;&#19982;MTL&#38598;&#25104;&#26102;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20108;&#32500;&#21367;&#31215;&#30340;&#26032;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2310.03925</link><description>&lt;p&gt;
&#20351;&#29992;&#20108;&#32500;&#21367;&#31215;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multitask Learning for Time Series Data\\with 2D Convolution. (arXiv:2310.03925v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03925
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65288;TSC&#65289;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#23558;&#26368;&#20808;&#36827;&#30340;&#19968;&#32500;&#21367;&#31215;&#27169;&#22411;&#19982;MTL&#38598;&#25104;&#26102;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20108;&#32500;&#21367;&#31215;&#30340;&#26032;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#19968;&#32452;&#23494;&#20999;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#20248;&#21270;&#27169;&#22411;&#65292;MTL&#22312;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#36890;&#24120;&#20248;&#20110;&#38750;MTL&#27169;&#22411;&#12290;&#23613;&#31649;MTL&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#25512;&#33616;&#31995;&#32479;&#31561;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#21364;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;MTL&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65288;TSC&#65289;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#24403;&#23558;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#19968;&#32500;&#21367;&#31215;&#30340;TSC&#27169;&#22411;&#19982;MTL&#38598;&#25104;&#26102;&#65292;TSC&#27169;&#22411;&#30340;&#24615;&#33021;&#23454;&#38469;&#19978;&#20250;&#19979;&#38477;&#12290;&#36890;&#36807;&#23558;&#19968;&#32500;&#21367;&#31215;&#27169;&#22411;&#19982;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#65288;DTW&#65289;&#36317;&#31163;&#20989;&#25968;&#36827;&#34892;&#27604;&#36739;&#65292;&#21487;&#20197;&#30475;&#20986;&#20302;&#19979;&#30340;&#32467;&#26524;&#26159;&#30001;&#20110;&#19968;&#32500;&#21367;&#31215;&#23618;&#30340;&#26377;&#38480;&#34920;&#36798;&#33021;&#21147;&#36896;&#25104;&#30340;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20108;&#32500;&#21367;&#31215;&#30340;&#26032;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multitask learning (MTL) aims to develop a unified model that can handle a set of closely related tasks simultaneously. By optimizing the model across multiple tasks, MTL generally surpasses its non-MTL counterparts in terms of generalizability. Although MTL has been extensively researched in various domains such as computer vision, natural language processing, and recommendation systems, its application to time series data has received limited attention. In this paper, we investigate the application of MTL to the time series classification (TSC) problem. However, when we integrate the state-of-the-art 1D convolution-based TSC model with MTL, the performance of the TSC model actually deteriorates. By comparing the 1D convolution-based models with the Dynamic Time Warping (DTW) distance function, it appears that the underwhelming results stem from the limited expressive power of the 1D convolutional layers. To overcome this challenge, we propose a novel design for a 2D convolution-based
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#29992;&#25143;&#27169;&#22411;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#30340;&#32858;&#31867;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#40065;&#26834;&#30340;&#32858;&#31867;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#29992;&#25143;&#27169;&#22411;&#20559;&#24046;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.02717</link><description>&lt;p&gt;
&#22312;&#29992;&#25143;&#27169;&#22411;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#30340;&#22312;&#32447;&#32858;&#31867;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Clustering of Bandits with Misspecified User Models. (arXiv:2310.02717v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#29992;&#25143;&#27169;&#22411;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#30340;&#32858;&#31867;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#40065;&#26834;&#30340;&#32858;&#31867;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#29992;&#25143;&#27169;&#22411;&#20559;&#24046;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#32447;&#24615;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#65292;&#22312;&#27599;&#36718;&#20013;&#65292;&#32473;&#23450;&#33218;&#30340;&#29305;&#24449;&#65292;&#23398;&#20064;&#20195;&#29702;&#36873;&#25321;&#19968;&#20010;&#33218;&#26469;&#26368;&#22823;&#21270;&#38271;&#26399;&#30340;&#32047;&#31215;&#22870;&#21169;&#12290;&#32858;&#31867;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31995;&#21015;&#24037;&#20316;&#65292;&#21033;&#29992;&#29992;&#25143;&#20559;&#22909;&#30340;&#21327;&#21516;&#25928;&#24212;&#65292;&#24182;&#22312;&#32463;&#20856;&#30340;&#32447;&#24615;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#32858;&#31867;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#38656;&#35201;&#27491;&#30830;&#35268;&#23450;&#32447;&#24615;&#29992;&#25143;&#27169;&#22411;&#65292;&#24403;&#36825;&#20010;&#20851;&#38190;&#20551;&#35774;&#19981;&#25104;&#31435;&#26102;&#65292;&#21487;&#33021;&#20250;&#22833;&#36133;&#12290;&#22914;&#20309;&#20026;&#22312;&#29992;&#25143;&#27169;&#22411;&#38169;&#35823;&#30340;&#23454;&#38469;&#24773;&#20917;&#19979;&#35774;&#35745;&#40065;&#26834;&#30340;&#32858;&#31867;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#22312;&#29992;&#25143;&#27169;&#22411;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#30340;&#32858;&#31867;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#29992;&#25143;&#27169;&#22411;&#20013;&#30340;&#26399;&#26395;&#22870;&#21169;&#21487;&#33021;&#26377;&#20559;&#24046;&#65292;&#19981;&#26159;&#23436;&#32654;&#30340;&#32447;&#24615;&#27169;&#22411;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#40065;&#26834;&#30340;&#32858;&#31867;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;RCLUMB&#21644;RSCLUMB&#65288;&#20998;&#21035;&#29992;&#21160;&#24577;&#22270;&#21644;&#38598;&#21512;&#34920;&#31034;&#23398;&#20064;&#21040;&#30340;&#32858;&#31867;&#32467;&#26500;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The contextual linear bandit is an important online learning problem where given arm features, a learning agent selects an arm at each round to maximize the cumulative rewards in the long run. A line of works, called the clustering of bandits (CB), utilize the collaborative effect over user preferences and have shown significant improvements over classic linear bandit algorithms. However, existing CB algorithms require well-specified linear user models and can fail when this critical assumption does not hold. Whether robust CB algorithms can be designed for more practical scenarios with misspecified user models remains an open problem. In this paper, we are the first to present the important problem of clustering of bandits with misspecified user models (CBMUM), where the expected rewards in user models can be perturbed away from perfect linear models. We devise two robust CB algorithms, RCLUMB and RSCLUMB (representing the learned clustering structure with dynamic graph and sets, resp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20855;&#36523;&#22522;&#30784;&#20808;&#39564;&#30340;&#22522;&#30784;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#26469;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.02635</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22522;&#30784;&#65306;&#26397;&#21521;&#20855;&#26377;&#22522;&#30784;&#20808;&#39564;&#36741;&#21161;&#30340;&#20855;&#36523;&#36890;&#29992;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Foundation Reinforcement Learning: towards Embodied Generalist Agents with Foundation Prior Assistance. (arXiv:2310.02635v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02635
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20855;&#36523;&#22522;&#30784;&#20808;&#39564;&#30340;&#22522;&#30784;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#26469;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20154;&#20204;&#24050;&#32463;&#34920;&#26126;&#65292;&#20174;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#25968;&#25454;&#20013;&#36827;&#34892;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#26159;&#26500;&#24314;&#36890;&#29992;&#27169;&#22411;&#30340;&#20851;&#38190;&#65292;&#27491;&#22914;&#22312;NLP&#20013;&#25152;&#35265;&#12290;&#20026;&#20102;&#26500;&#24314;&#20855;&#36523;&#36890;&#29992;&#26234;&#33021;&#20307;&#65292;&#25105;&#20204;&#21644;&#35768;&#22810;&#20854;&#20182;&#30740;&#31350;&#32773;&#20551;&#35774;&#36825;&#31181;&#22522;&#30784;&#20808;&#39564;&#20063;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#22914;&#20309;&#20197;&#36866;&#24403;&#30340;&#20855;&#20307;&#24418;&#24335;&#34920;&#31034;&#36825;&#20123;&#20855;&#36523;&#22522;&#30784;&#20808;&#39564;&#65292;&#20197;&#21450;&#23427;&#20204;&#24212;&#35813;&#22914;&#20309;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#20351;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#30452;&#35266;&#26377;&#25928;&#30340;&#20855;&#36523;&#20808;&#39564;&#65292;&#21253;&#25324;&#22522;&#30784;&#31574;&#30053;&#12289;&#20215;&#20540;&#21644;&#25104;&#21151;&#22870;&#21169;&#12290;&#25152;&#25552;&#20986;&#30340;&#20808;&#39564;&#26159;&#22522;&#20110;&#30446;&#26631;&#26465;&#20214;&#30340;MDP&#12290;&#20026;&#20102;&#39564;&#35777;&#20854;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23454;&#20363;&#21270;&#20102;&#19968;&#20010;&#30001;&#36825;&#20123;&#20808;&#39564;&#36741;&#21161;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#65292;&#31216;&#20043;&#20026;&#22522;&#30784;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;FAC&#65289;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#21629;&#21517;&#20026;&#22522;&#30784;&#24378;&#21270;&#23398;&#20064;&#65288;FRL&#65289;&#65292;&#22240;&#20026;&#23427;&#23436;&#20840;&#20381;&#36182;&#20110;&#20855;&#36523;&#22522;&#30784;&#20808;&#39564;&#26469;&#36827;&#34892;&#25506;&#32034;&#12289;&#23398;&#20064;&#21644;&#24378;&#21270;&#12290;FRL&#30340;&#22909;&#22788;&#26377;&#19977;&#20010;&#12290;(1)&#26679;&#26412;&#25928;&#29575;&#39640;&#12290;&#36890;&#36807;&#22522;&#30784;&#20808;&#39564;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#65292;&#20943;&#23569;&#26679;&#26412;&#20351;&#29992;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, people have shown that large-scale pre-training from internet-scale data is the key to building generalist models, as witnessed in NLP. To build embodied generalist agents, we and many other researchers hypothesize that such foundation prior is also an indispensable component. However, it is unclear what is the proper concrete form to represent those embodied foundation priors and how they should be used in the downstream task. In this paper, we propose an intuitive and effective set of embodied priors that consist of foundation policy, value, and success reward. The proposed priors are based on the goal-conditioned MDP. To verify their effectiveness, we instantiate an actor-critic method assisted by the priors, called Foundation Actor-Critic (FAC). We name our framework as Foundation Reinforcement Learning (FRL), since it completely relies on embodied foundation priors to explore, learn and reinforce. The benefits of FRL are threefold. (1) Sample efficient. With foundation p
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23398;&#20064;&#36890;&#20449;&#65288;LTC&#65289;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#35813;&#26041;&#27861;&#21487;&#20351;LLM&#20195;&#29702;&#36890;&#36807;&#19982;&#29615;&#22659;&#21644;&#20854;&#20182;&#20195;&#29702;&#30340;&#20132;&#20114;&#19981;&#26029;&#25913;&#36827;&#65292;&#20197;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#36807;&#22810;&#20154;&#31867;&#30417;&#30563;&#12290;</title><link>http://arxiv.org/abs/2310.01444</link><description>&lt;p&gt;
&#36890;&#36807;&#20132;&#27969;&#20351;LLM&#20195;&#29702;&#36866;&#24212;&#29615;&#22659;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Adapting LLM Agents Through Communication. (arXiv:2310.01444v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01444
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23398;&#20064;&#36890;&#20449;&#65288;LTC&#65289;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#35813;&#26041;&#27861;&#21487;&#20351;LLM&#20195;&#29702;&#36890;&#36807;&#19982;&#29615;&#22659;&#21644;&#20854;&#20182;&#20195;&#29702;&#30340;&#20132;&#20114;&#19981;&#26029;&#25913;&#36827;&#65292;&#20197;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#36807;&#22810;&#20154;&#31867;&#30417;&#30563;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#36827;&#23637;&#26174;&#31034;&#20986;&#20102;&#20154;&#31867;&#21270;&#20195;&#29702;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#24110;&#21161;&#36825;&#20123;&#20195;&#29702;&#22312;&#27809;&#26377;&#24191;&#27867;&#20154;&#31867;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#36890;&#20449;&#65288;LTC&#65289;&#33539;&#24335;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;LLM&#20195;&#29702;&#33021;&#22815;&#36890;&#36807;&#19982;&#29615;&#22659;&#21644;&#20854;&#20182;&#20195;&#29702;&#30340;&#20132;&#20114;&#19981;&#26029;&#25913;&#36827;&#12290;&#36890;&#36807;&#36845;&#20195;&#25506;&#32034;&#21644;PPO&#35757;&#32451;&#65292;LTC&#20351;&#20195;&#29702;&#33021;&#22815;&#23558;&#30701;&#26399;&#32463;&#39564;&#34701;&#20837;&#38271;&#26399;&#35760;&#24518;&#12290;&#20026;&#20102;&#20248;&#21270;&#29305;&#23450;&#20219;&#21153;&#30340;&#20195;&#29702;&#20132;&#20114;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#31181;&#32467;&#26500;&#21270;&#30340;&#36890;&#20449;&#27169;&#24335;&#65306;&#29420;&#30333;&#65292;&#23545;&#35805;&#65292;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large language models (LLMs) have shown potential for human-like agents. To help these agents adapt to new tasks without extensive human supervision, we propose the Learning through Communication (LTC) paradigm, a novel training approach enabling LLM agents to improve continuously through interactions with their environments and other agents. Recent advancements in large language models (LLMs) have shown potential for human-like agents. To help these agents adapt to new tasks without extensive human supervision, we propose the Learning through Communication (LTC) paradigm, a novel training approach enabling LLM agents to improve continuously through interactions with their environments and other agents. Through iterative exploration and PPO training, LTC empowers the agent to assimilate short-term experiences into long-term memory. To optimize agent interactions for task-specific learning, we introduce three structured communication patterns: Monologue, Dialogue,
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#34920;&#31034;&#24037;&#31243;&#21270;&#65288;RepE&#65289;&#30340;&#33258;&#19978;&#32780;&#19979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20511;&#37492;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#30340;&#35265;&#35299;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#22686;&#24378;AI&#31995;&#32479;&#36879;&#26126;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#26041;&#27861;&#23558;&#38598;&#32676;&#32423;&#21035;&#30340;&#34920;&#31034;&#25918;&#22312;&#20998;&#26512;&#30340;&#26680;&#24515;&#65292;&#20026;&#30417;&#27979;&#21644;&#25805;&#32437;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#39640;&#32423;&#35748;&#30693;&#29616;&#35937;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#35299;&#20915;&#19982;&#23433;&#20840;&#30456;&#20851;&#30340;&#38382;&#39064;&#19978;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.01405</link><description>&lt;p&gt;
&#34920;&#31034;&#24037;&#31243;&#21270;&#65306;AI&#36879;&#26126;&#21270;&#30340;&#33258;&#19978;&#32780;&#19979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Representation Engineering: A Top-Down Approach to AI Transparency. (arXiv:2310.01405v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01405
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#34920;&#31034;&#24037;&#31243;&#21270;&#65288;RepE&#65289;&#30340;&#33258;&#19978;&#32780;&#19979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20511;&#37492;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#30340;&#35265;&#35299;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#22686;&#24378;AI&#31995;&#32479;&#36879;&#26126;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#26041;&#27861;&#23558;&#38598;&#32676;&#32423;&#21035;&#30340;&#34920;&#31034;&#25918;&#22312;&#20998;&#26512;&#30340;&#26680;&#24515;&#65292;&#20026;&#30417;&#27979;&#21644;&#25805;&#32437;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#39640;&#32423;&#35748;&#30693;&#29616;&#35937;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#35299;&#20915;&#19982;&#23433;&#20840;&#30456;&#20851;&#30340;&#38382;&#39064;&#19978;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#24182;&#25551;&#36848;&#20102;&#34920;&#31034;&#24037;&#31243;&#21270;&#65288;RepE&#65289;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#20511;&#37492;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#30340;&#35265;&#35299;&#26469;&#22686;&#24378;AI&#31995;&#32479;&#36879;&#26126;&#24615;&#30340;&#26041;&#27861;&#12290;RepE&#23558;&#38598;&#32676;&#32423;&#21035;&#30340;&#34920;&#31034;&#25918;&#22312;&#20998;&#26512;&#30340;&#26680;&#24515;&#65292;&#32780;&#19981;&#26159;&#31070;&#32463;&#20803;&#25110;&#30005;&#36335;&#65292;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#30417;&#27979;&#21644;&#25805;&#32437;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#20013;&#39640;&#32423;&#35748;&#30693;&#29616;&#35937;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;RepE&#25216;&#26415;&#30340;&#22522;&#20934;&#21644;&#21021;&#27493;&#20998;&#26512;&#65292;&#26174;&#31034;&#23427;&#20204;&#25552;&#20379;&#20102;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#25913;&#21892;&#25105;&#20204;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#21644;&#25511;&#21046;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#22914;&#20309;&#22312;&#21253;&#25324;&#35802;&#23454;&#24615;&#12289;&#26080;&#23475;&#24615;&#12289;&#36861;&#27714;&#26435;&#21147;&#31561;&#19968;&#31995;&#21015;&#19982;&#23433;&#20840;&#30456;&#20851;&#30340;&#38382;&#39064;&#19978;&#21457;&#25381;&#20316;&#29992;&#65292;&#23637;&#31034;&#20102;&#33258;&#19978;&#32780;&#19979;&#36879;&#26126;&#24615;&#30740;&#31350;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#39033;&#24037;&#20316;&#33021;&#22815;&#20419;&#36827;RepE&#30340;&#36827;&#19968;&#27493;&#25506;&#32034;&#65292;&#24182;&#25512;&#21160;AI&#31995;&#32479;&#30340;&#36879;&#26126;&#24615;&#21644;&#23433;&#20840;&#24615;&#30340;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we identify and characterize the emerging area of representation engineering (RepE), an approach to enhancing the transparency of AI systems that draws on insights from cognitive neuroscience. RepE places population-level representations, rather than neurons or circuits, at the center of analysis, equipping us with novel methods for monitoring and manipulating high-level cognitive phenomena in deep neural networks (DNNs). We provide baselines and an initial analysis of RepE techniques, showing that they offer simple yet effective solutions for improving our understanding and control of large language models. We showcase how these methods can provide traction on a wide range of safety-relevant problems, including honesty, harmlessness, power-seeking, and more, demonstrating the promise of top-down transparency research. We hope that this work catalyzes further exploration of RepE and fosters advancements in the transparency and safety of AI systems.
&lt;/p&gt;</description></item><item><title>SELF&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#20801;&#35768;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19981;&#26029;&#33258;&#25105;&#36827;&#21270;&#65292;&#24182;&#36890;&#36807;&#35821;&#35328;&#21453;&#39304;&#20316;&#20026;&#35780;&#20272;&#24037;&#20855;&#26469;&#25913;&#36827;&#27169;&#22411;&#30340;&#21709;&#24212;&#33021;&#21147;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.00533</link><description>&lt;p&gt;
SELF&#65306;&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#20027;&#36827;&#21270;
&lt;/p&gt;
&lt;p&gt;
SELF: Language-Driven Self-Evolution for Large Language Model. (arXiv:2310.00533v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00533
&lt;/p&gt;
&lt;p&gt;
SELF&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#20801;&#35768;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19981;&#26029;&#33258;&#25105;&#36827;&#21270;&#65292;&#24182;&#36890;&#36807;&#35821;&#35328;&#21453;&#39304;&#20316;&#20026;&#35780;&#20272;&#24037;&#20855;&#26469;&#25913;&#36827;&#27169;&#22411;&#30340;&#21709;&#24212;&#33021;&#21147;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#30340;&#21331;&#36234;&#36866;&#24212;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#30340;&#23398;&#20064;&#21644;&#25512;&#21160;&#33258;&#20027;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#38190;&#8212;&#8212;&#27169;&#22411;&#33258;&#20027;&#36827;&#21270;&#30340;&#36335;&#24452;&#20173;&#28982;&#26410;&#30693;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;"SELF"&#65288;&#24102;&#26377;&#35821;&#35328;&#21453;&#39304;&#30340;&#33258;&#20027;&#36827;&#21270;&#65289;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;LLM&#33021;&#22815;&#19981;&#26029;&#22320;&#33258;&#25105;&#36827;&#21270;&#12290;&#27492;&#22806;&#65292;SELF&#21033;&#29992;&#35821;&#35328;&#21453;&#39304;&#20316;&#20026;&#19968;&#31181;&#22810;&#21151;&#33021;&#12289;&#20840;&#38754;&#30340;&#35780;&#20272;&#24037;&#20855;&#65292;&#31934;&#30830;&#23450;&#20301;&#21709;&#24212;&#25913;&#36827;&#30340;&#39046;&#22495;&#65292;&#24182;&#25552;&#39640;&#33258;&#20027;&#36827;&#21270;&#35757;&#32451;&#30340;&#31283;&#23450;&#24615;&#12290;SELF&#39318;&#20808;&#36827;&#34892;&#20803;&#25216;&#33021;&#23398;&#20064;&#65292;&#19987;&#27880;&#20110;&#33258;&#25105;&#21453;&#39304;&#21644;&#33258;&#25105;&#31934;&#28860;&#12290;&#36825;&#20123;&#20803;&#25216;&#33021;&#26159;&#20851;&#38190;&#65292;&#24341;&#23548;&#27169;&#22411;&#22312;&#33258;&#21046;&#25968;&#25454;&#30340;&#25345;&#32493;&#35757;&#32451;&#21608;&#26399;&#20013;&#36827;&#34892;&#21518;&#32493;&#30340;&#33258;&#25105;&#36827;&#21270;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#20869;&#22312;&#33021;&#21147;&#12290;&#22312;&#32473;&#23450;&#26080;&#26631;&#31614;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#65292;SELF&#20351;&#27169;&#22411;&#20855;&#22791;&#20102;&#33021;&#22815;...
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have showcased remarkable versatility across diverse domains. However, the pathway toward autonomous model development, a cornerstone for achieving human-level learning and advancing autonomous AI, remains largely uncharted. We introduce an innovative approach, termed "SELF" (Self-Evolution with Language Feedback). This methodology empowers LLMs to undergo continual self-evolution. Furthermore, SELF employs language-based feedback as a versatile and comprehensive evaluative tool, pinpointing areas for response refinement and bolstering the stability of self-evolutionary training. Initiating with meta-skill learning, SELF acquires foundational meta-skills with a focus on self-feedback and self-refinement. These meta-skills are critical, guiding the model's subsequent self-evolution through a cycle of perpetual training with self-curated data, thereby enhancing its intrinsic abilities. Given unlabeled instructions, SELF equips the model with the capability to
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#30456;&#23545;&#21453;&#39304;&#26469;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34892;&#20026;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#20248;&#21270;&#27604;&#36739;&#25439;&#22833;&#35757;&#32451;&#30340;&#22870;&#21169;&#26102;&#23384;&#22312;&#30340;&#38480;&#21046;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36712;&#36857;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65288;PPPO&#65289;&#65292;&#29992;&#20110;&#26356;&#26377;&#25928;&#22320;&#36827;&#34892;&#31639;&#27861;&#35774;&#35745;&#21644;&#20989;&#25968;&#36924;&#36817;&#12290;</title><link>http://arxiv.org/abs/2310.00212</link><description>&lt;p&gt;
&#20004;&#20004;&#37051;&#36817;&#31574;&#30053;&#20248;&#21270;: &#21033;&#29992;&#30456;&#23545;&#21453;&#39304;&#36827;&#34892;LLM&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment. (arXiv:2310.00212v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00212
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#30456;&#23545;&#21453;&#39304;&#26469;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34892;&#20026;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#20248;&#21270;&#27604;&#36739;&#25439;&#22833;&#35757;&#32451;&#30340;&#22870;&#21169;&#26102;&#23384;&#22312;&#30340;&#38480;&#21046;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36712;&#36857;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65288;PPPO&#65289;&#65292;&#29992;&#20110;&#26356;&#26377;&#25928;&#22320;&#36827;&#34892;&#31639;&#27861;&#35774;&#35745;&#21644;&#20989;&#25968;&#36924;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#22312;&#22823;&#22411;&#35821;&#26009;&#24211;&#19978;&#39044;&#20808;&#35757;&#32451;&#26469;&#33719;&#21462;&#24191;&#27867;&#30340;&#19990;&#30028;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25509;&#35302;&#21040;&#20302;&#36136;&#37327;&#25968;&#25454;&#65292;LLMs&#21487;&#33021;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#20215;&#20540;&#19981;&#19968;&#33268;&#30340;&#26377;&#23475;&#34892;&#20026;&#12290;&#24341;&#23548;LLMs&#26397;&#30528;&#26377;&#30410;&#34892;&#20026;&#26041;&#21521;&#21457;&#23637;&#30340;&#20027;&#23548;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#20854;&#20013;Proximal Policy Optimization&#65288;PPO&#65289;&#26159;&#40664;&#35748;&#30340;RL&#20248;&#21270;&#22120;&#12290;&#23613;&#31649;&#20854;&#26377;&#25928;&#24615;&#65292;&#20294;PPO&#22312;&#20248;&#21270;&#22522;&#20110;&#27604;&#36739;&#25439;&#22833;&#35757;&#32451;&#30340;&#22870;&#21169;&#26102;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20027;&#35201;&#38382;&#39064;&#26159;&#65292;&#30001;&#20110;&#38656;&#35201;&#26657;&#20934;&#22870;&#21169;&#23610;&#24230;&#65292;PPO&#23545;&#20110;&#21253;&#21547;&#30456;&#21516;&#20559;&#22909;&#20449;&#24687;&#30340;&#31561;&#20215;&#22870;&#21169;&#20989;&#25968;&#19981;&#20855;&#22791;&#19981;&#21464;&#24615;&#12290;&#27492;&#22806;&#65292;&#19982;&#22522;&#20110;&#36712;&#36857;&#30340;&#20248;&#21270;&#30456;&#27604;&#65292;PPO&#23545;&#20110;&#22522;&#20110;&#20196;&#29260;&#30340;&#26356;&#26032;&#30340;&#38656;&#27714;&#24341;&#20837;&#20102;&#20989;&#25968;&#36924;&#36817;&#21644;&#31639;&#27861;&#35774;&#35745;&#26041;&#38754;&#30340;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#22522;&#20110;&#30456;&#23545;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#36712;&#36857;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;Pairwise Proximal Policy Optimization&#65288;PPPO&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) can acquire extensive world knowledge through pre-training on large corpora. However, due to exposure to low-quality data, LLMs may exhibit harmful behavior without aligning with human values. The dominant approach for steering LLMs towards beneficial behavior involves Reinforcement Learning with Human Feedback (RLHF), with Proximal Policy Optimization (PPO) serving as the default RL optimizer. Despite its effectiveness, PPO has limitations when optimizing rewards trained from comparison-based loss. Primarily, PPO is not invariant to equivalent reward functions containing identical preference information due to the need to calibrate the reward scale. Additionally, PPO's necessity for token-wise updates introduces complexity in both function approximation and algorithm design compared to trajectory-wise optimization. This paper proposes a new framework, reinforcement learning with relative feedback, and a novel trajectory-wise policy gradient algorithm, Pair
&lt;/p&gt;</description></item><item><title>ABScribe&#26159;&#19968;&#31181;&#30028;&#38754;&#65292;&#25903;&#25345;&#22312;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20849;&#21516;&#20889;&#20316;&#20219;&#21153;&#20013;&#24555;&#36895;&#25506;&#32034;&#22810;&#31181;&#20889;&#20316;&#21464;&#21270;&#12290;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#24555;&#36895;&#29983;&#25104;&#22810;&#20010;&#21464;&#20307;&#65292;&#36825;&#20123;&#21464;&#20307;&#20197;&#21487;&#37325;&#29992;&#30340;&#25353;&#38062;&#24418;&#24335;&#21576;&#29616;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#19978;&#19979;&#25991;&#24037;&#20855;&#26639;&#36827;&#34892;&#24555;&#36895;&#30340;&#23601;&#22320;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2310.00117</link><description>&lt;p&gt;
ABScribe: &#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20849;&#21516;&#20889;&#20316;&#20219;&#21153;&#20013;&#24555;&#36895;&#25506;&#32034;&#22810;&#31181;&#20889;&#20316;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
ABScribe: Rapid Exploration of Multiple Writing Variations in Human-AI Co-Writing Tasks using Large Language Models. (arXiv:2310.00117v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00117
&lt;/p&gt;
&lt;p&gt;
ABScribe&#26159;&#19968;&#31181;&#30028;&#38754;&#65292;&#25903;&#25345;&#22312;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20849;&#21516;&#20889;&#20316;&#20219;&#21153;&#20013;&#24555;&#36895;&#25506;&#32034;&#22810;&#31181;&#20889;&#20316;&#21464;&#21270;&#12290;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#24555;&#36895;&#29983;&#25104;&#22810;&#20010;&#21464;&#20307;&#65292;&#36825;&#20123;&#21464;&#20307;&#20197;&#21487;&#37325;&#29992;&#30340;&#25353;&#38062;&#24418;&#24335;&#21576;&#29616;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#19978;&#19979;&#25991;&#24037;&#20855;&#26639;&#36827;&#34892;&#24555;&#36895;&#30340;&#23601;&#22320;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37325;&#26032;&#20070;&#20889;&#25991;&#26412;&#26469;&#25506;&#32034;&#26367;&#20195;&#24819;&#27861;&#26159;&#20889;&#20316;&#36807;&#31243;&#30340;&#20851;&#38190;&#12290;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#20197;&#31616;&#21270;&#20889;&#20316;&#21464;&#21270;&#29983;&#25104;&#30340;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30028;&#38754;&#23384;&#22312;&#21516;&#26102;&#32771;&#34385;&#22810;&#31181;&#21464;&#21270;&#30340;&#25361;&#25112;&#65306;&#22312;&#19981;&#35206;&#30422;&#25991;&#26412;&#30340;&#24773;&#20917;&#19979;&#21019;&#24314;&#26032;&#30340;&#29256;&#26412;&#21487;&#33021;&#24456;&#22256;&#38590;&#65292;&#32780;&#25353;&#39034;&#24207;&#31896;&#36148;&#23427;&#20204;&#21487;&#33021;&#20250;&#20351;&#25991;&#26723;&#21464;&#24471;&#26434;&#20081;&#65292;&#22686;&#21152;&#24037;&#20316;&#37327;&#65292;&#24182;&#25171;&#26029;&#20316;&#32773;&#30340;&#27969;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ABScribe&#65292;&#19968;&#31181;&#25903;&#25345;&#22312;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20849;&#21516;&#20889;&#20316;&#20219;&#21153;&#20013;&#24555;&#36895;&#19988;&#32467;&#26500;&#21270;&#22320;&#25506;&#32034;&#20889;&#20316;&#21464;&#21270;&#30340;&#30028;&#38754;&#12290;&#36890;&#36807;ABScribe&#65292;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;LLM&#25552;&#31034;&#24555;&#36895;&#20135;&#29983;&#22810;&#20010;&#21464;&#20307;&#65292;&#36825;&#20123;&#21464;&#20307;&#20250;&#33258;&#21160;&#36716;&#25442;&#25104;&#21487;&#37325;&#29992;&#30340;&#25353;&#38062;&#24418;&#24335;&#12290;&#21464;&#20307;&#22312;&#25991;&#26412;&#27573;&#33853;&#20013;&#34987;&#23384;&#20648;&#22312;&#30456;&#37051;&#20301;&#32622;&#65292;&#36890;&#36807;&#22312;&#19978;&#19979;&#25991;&#24037;&#20855;&#26639;&#19978;&#30340;&#40736;&#26631;&#24748;&#20572;&#20132;&#20114;&#36827;&#34892;&#24555;&#36895;&#30340;&#23601;&#22320;&#27604;&#36739;&#12290;&#25105;&#20204;&#23545;12&#21517;&#25776;&#20889;&#20154;&#21592;&#36827;&#34892;&#30340;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;ABScribe&#33021;&#26174;&#33879;&#20943;&#36731;&#20219;&#21153;&#36127;&#33655;&#65288;d = 1.20, p &lt; 0.001&#65289;&#65292;&#25552;&#39640;&#29992;&#25143;&#30340;&#35748;&#30693;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploring alternative ideas by rewriting text is integral to the writing process. State-of-the-art large language models (LLMs) can simplify writing variation generation. However, current interfaces pose challenges for simultaneous consideration of multiple variations: creating new versions without overwriting text can be difficult, and pasting them sequentially can clutter documents, increasing workload and disrupting writers' flow. To tackle this, we present ABScribe, an interface that supports rapid, yet visually structured, exploration of writing variations in human-AI co-writing tasks. With ABScribe, users can swiftly produce multiple variations using LLM prompts, which are auto-converted into reusable buttons. Variations are stored adjacently within text segments for rapid in-place comparisons using mouse-over interactions on a context toolbar. Our user study with 12 writers shows that ABScribe significantly reduces task workload (d = 1.20, p &lt; 0.001), enhances user perceptions o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;ASR&#21518;&#22788;&#29702;&#22120;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#37325;&#26032;&#35780;&#20998;&#21644;&#38169;&#35823;&#26657;&#27491;&#26469;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#12290;&#36890;&#36807;&#20351;&#29992;&#25351;&#20196;&#25552;&#31034;&#21644;&#20219;&#21153;&#28608;&#27963;&#25552;&#31034;&#26041;&#27861;&#65292;&#32467;&#21512;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#24494;&#35843;&#25216;&#26415;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LLMs&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.15649</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#24335;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
Generative Speech Recognition Error Correction with Large Language Models. (arXiv:2309.15649v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;ASR&#21518;&#22788;&#29702;&#22120;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#37325;&#26032;&#35780;&#20998;&#21644;&#38169;&#35823;&#26657;&#27491;&#26469;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#12290;&#36890;&#36807;&#20351;&#29992;&#25351;&#20196;&#25552;&#31034;&#21644;&#20219;&#21153;&#28608;&#27963;&#25552;&#31034;&#26041;&#27861;&#65292;&#32467;&#21512;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#24494;&#35843;&#25216;&#26415;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LLMs&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;ASR&#21518;&#22788;&#29702;&#22120;&#30340;&#33021;&#21147;&#65292;&#29992;&#20110;&#37325;&#26032;&#35780;&#20998;&#21644;&#38169;&#35823;&#26657;&#27491;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#20351;&#29992;&#25351;&#20196;&#25552;&#31034;&#35753;LLMs&#25191;&#34892;&#36825;&#20123;&#20219;&#21153;&#32780;&#26080;&#38656;&#24494;&#35843;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#25552;&#31034;&#26041;&#26696;&#65292;&#21253;&#25324;&#38646;-shot&#21644;&#23569;-shot&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#28608;&#27963;&#25552;&#31034;&#65288;TAP&#65289;&#26041;&#27861;&#65292;&#32467;&#21512;&#25351;&#20196;&#21644;&#28436;&#31034;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#39046;&#22495;&#20043;&#22806;&#30340;&#20219;&#21153;&#65288;ATIS&#21644;WSJ&#65289;&#19978;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#31532;&#19968;&#27425;&#25195;&#25551;&#31995;&#32479;&#21644;&#37325;&#26032;&#35780;&#20998;&#36755;&#20986;&#65292;&#25105;&#20204;&#35777;&#26126;&#20165;&#36890;&#36807;&#20923;&#32467;LLMs&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#36827;&#34892;&#37325;&#26032;&#35780;&#20998;&#21487;&#20197;&#36798;&#21040;&#19982;&#39046;&#22495;&#35843;&#20248;&#30340;LMs&#37325;&#26032;&#35780;&#20998;&#30456;&#31454;&#20105;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#23558;&#25552;&#31034;&#25216;&#26415;&#19982;&#24494;&#35843;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#20302;&#20110;N-best Oracle&#27700;&#24179;&#30340;&#38169;&#35823;&#29575;&#65292;&#23637;&#31034;&#20102;LLMs&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the ability of large language models (LLMs) to act as ASR post-processors that perform rescoring and error correction. Our focus is on instruction prompting to let LLMs perform these task without fine-tuning, for which we evaluate different prompting schemes, both zero- and few-shot in-context learning, and a novel task-activating prompting (TAP) method that combines instruction and demonstration. Using a pre-trained first-pass system and rescoring output on two out-of-domain tasks (ATIS and WSJ), we show that rescoring only by in-context learning with frozen LLMs achieves results that are competitive with rescoring by domain-tuned LMs. By combining prompting techniques with fine-tuning we achieve error rates below the N-best oracle level, showcasing the generalization power of the LLMs.
&lt;/p&gt;</description></item><item><title>MAPTree&#26159;&#19968;&#31181;&#36890;&#36807;&#36125;&#21494;&#26031;&#26041;&#27861;&#23545;&#20915;&#31574;&#26641;&#36827;&#34892;&#24402;&#32435;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;AND/OR&#25628;&#32034;&#23454;&#29616;&#26368;&#22823;&#21518;&#39564;&#26641;&#30340;&#24674;&#22797;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;MAPTree&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#22815;&#20197;&#26356;&#23567;&#30340;&#26641;&#26469;&#23454;&#29616;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;MAPTree&#36824;&#23637;&#31034;&#20986;&#26356;&#24378;&#30340;&#25239;&#22122;&#22768;&#33021;&#21147;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.15312</link><description>&lt;p&gt;
MAPTree: &#29992;&#36125;&#21494;&#26031;&#20915;&#31574;&#26641;&#20987;&#36133;&#8220;&#26368;&#20248;&#8221;&#20915;&#31574;&#26641;
&lt;/p&gt;
&lt;p&gt;
MAPTree: Beating "Optimal" Decision Trees with Bayesian Decision Trees. (arXiv:2309.15312v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15312
&lt;/p&gt;
&lt;p&gt;
MAPTree&#26159;&#19968;&#31181;&#36890;&#36807;&#36125;&#21494;&#26031;&#26041;&#27861;&#23545;&#20915;&#31574;&#26641;&#36827;&#34892;&#24402;&#32435;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;AND/OR&#25628;&#32034;&#23454;&#29616;&#26368;&#22823;&#21518;&#39564;&#26641;&#30340;&#24674;&#22797;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;MAPTree&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#22815;&#20197;&#26356;&#23567;&#30340;&#26641;&#26469;&#23454;&#29616;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;MAPTree&#36824;&#23637;&#31034;&#20986;&#26356;&#24378;&#30340;&#25239;&#22122;&#22768;&#33021;&#21147;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26641;&#20173;&#28982;&#26159;&#24403;&#20170;&#26368;&#27969;&#34892;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20043;&#19968;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#20854;&#24320;&#31665;&#21363;&#29992;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#26641;&#19978;&#30340;&#21518;&#39564;&#20998;&#24067;&#36827;&#34892;&#26368;&#22823;&#21518;&#39564;&#25512;&#29702;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#20915;&#31574;&#26641;&#24402;&#32435;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#20915;&#31574;&#26641;&#30340;&#26368;&#22823;&#21518;&#39564;&#25512;&#29702;&#19982;AND/OR&#25628;&#32034;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#21033;&#29992;&#36825;&#19968;&#20851;&#32852;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;MAPTree&#30340;AND/OR&#25628;&#32034;&#31639;&#27861;&#65292;&#33021;&#22815;&#24674;&#22797;&#20986;&#26368;&#22823;&#21518;&#39564;&#26641;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#23454;&#38469;&#19990;&#30028;&#22330;&#26223;&#20013;&#23637;&#31034;&#26368;&#22823;&#21518;&#39564;&#26641;&#30340;&#32463;&#39564;&#24615;&#33021;&#12290;&#22312;16&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#65292;MAPTree&#35201;&#20040;&#20248;&#20110;&#22522;&#20934;&#32447;&#65292;&#35201;&#20040;&#22312;&#24615;&#33021;&#30456;&#24403;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#26356;&#23567;&#30340;&#26641;&#12290;&#22312;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#65292;MAPTree&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#24378;&#30340;&#25239;&#22122;&#22768;&#33021;&#21147;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26368;&#21518;&#65292;MAPTree&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#24555;&#22320;&#24674;&#22797;&#20986;&#26368;&#22823;&#21518;&#39564;&#26641;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision trees remain one of the most popular machine learning models today, largely due to their out-of-the-box performance and interpretability. In this work, we present a Bayesian approach to decision tree induction via maximum a posteriori inference of a posterior distribution over trees. We first demonstrate a connection between maximum a posteriori inference of decision trees and AND/OR search. Using this connection, we propose an AND/OR search algorithm, dubbed MAPTree, which is able to recover the maximum a posteriori tree. Lastly, we demonstrate the empirical performance of the maximum a posteriori tree both on synthetic data and in real world settings. On 16 real world datasets, MAPTree either outperforms baselines or demonstrates comparable performance but with much smaller trees. On a synthetic dataset, MAPTree also demonstrates greater robustness to noise and better generalization than existing approaches. Finally, MAPTree recovers the maxiumum a posteriori tree faster tha
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20302;&#31209;&#36866;&#24212;&#25216;&#26415;&#30340;&#31070;&#32463;&#35821;&#35328;&#24314;&#27169;&#31995;&#32479;&#65292;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#30340;&#36755;&#20986;&#37325;&#35780;&#20998;&#12290;&#36890;&#36807;&#20351;&#29992;&#20302;&#31209;&#20998;&#35299;&#26041;&#27861;&#21644;&#20248;&#21270;&#25554;&#20837;&#30697;&#38453;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#20197;&#26356;&#39640;&#25928;&#30340;&#26041;&#24335;&#23558;BERT&#27169;&#22411;&#36866;&#24212;&#21040;&#26032;&#39046;&#22495;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2309.15223</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#37325;&#35780;&#20998;&#30340;&#20302;&#31209;&#36866;&#24212;&#25216;&#26415;&#22312;&#21442;&#25968;&#39640;&#25928;&#30340;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Low-rank Adaptation of Large Language Model Rescoring for Parameter-Efficient Speech Recognition. (arXiv:2309.15223v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15223
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20302;&#31209;&#36866;&#24212;&#25216;&#26415;&#30340;&#31070;&#32463;&#35821;&#35328;&#24314;&#27169;&#31995;&#32479;&#65292;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#30340;&#36755;&#20986;&#37325;&#35780;&#20998;&#12290;&#36890;&#36807;&#20351;&#29992;&#20302;&#31209;&#20998;&#35299;&#26041;&#27861;&#21644;&#20248;&#21270;&#25554;&#20837;&#30697;&#38453;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#20197;&#26356;&#39640;&#25928;&#30340;&#26041;&#24335;&#23558;BERT&#27169;&#22411;&#36866;&#24212;&#21040;&#26032;&#39046;&#22495;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#30340;&#31070;&#32463;&#35821;&#35328;&#24314;&#27169;&#31995;&#32479;&#65292;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#36755;&#20986;&#37325;&#35780;&#20998;&#12290;&#23613;&#31649;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT&#65289;&#22312;&#31532;&#20108;&#27425;&#37325;&#35780;&#20998;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#23558;&#39044;&#35757;&#32451;&#38454;&#27573;&#25193;&#23637;&#21644;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#21040;&#29305;&#23450;&#39046;&#22495;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#37325;&#35780;&#20998;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20302;&#31209;&#20998;&#35299;&#30340;&#26041;&#27861;&#65292;&#20165;&#20351;&#29992;&#39044;&#35757;&#32451;&#21442;&#25968;&#30340;&#19968;&#23567;&#37096;&#20998;&#65288;0.08%&#65289;&#26469;&#35757;&#32451;&#37325;&#35780;&#20998;&#30340;BERT&#27169;&#22411;&#24182;&#23558;&#20854;&#36866;&#24212;&#21040;&#26032;&#39046;&#22495;&#12290;&#36825;&#20123;&#25554;&#20837;&#30340;&#30697;&#38453;&#36890;&#36807;&#30456;&#20851;&#24615;&#27491;&#21017;&#21270;&#25439;&#22833;&#21644;&#21028;&#21035;&#24615;&#35757;&#32451;&#30446;&#26631;&#36827;&#34892;&#20248;&#21270;&#12290;&#25152;&#25552;&#20986;&#30340;&#20302;&#31209;&#36866;&#24212;Rescore-BERT&#65288;LoRB&#65289;&#20307;&#31995;&#32467;&#26500;&#22312;LibriSpeech&#21644;&#20869;&#37096;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#65292;&#35757;&#32451;&#26102;&#38388;&#20943;&#23569;&#20102;5.4&#33267;3.6&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a neural language modeling system based on low-rank adaptation (LoRA) for speech recognition output rescoring. Although pretrained language models (LMs) like BERT have shown superior performance in second-pass rescoring, the high computational cost of scaling up the pretraining stage and adapting the pretrained models to specific domains limit their practical use in rescoring. Here we present a method based on low-rank decomposition to train a rescoring BERT model and adapt it to new domains using only a fraction (0.08%) of the pretrained parameters. These inserted matrices are optimized through a discriminative training objective along with a correlation-based regularization loss. The proposed low-rank adaptation Rescore-BERT (LoRB) architecture is evaluated on LibriSpeech and internal datasets with decreased training times by factors between 5.4 and 3.6.
&lt;/p&gt;</description></item><item><title>&#36882;&#24402;&#36229;&#32593;&#32476;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22312;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#24378;&#22823;&#24615;&#33021;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#19987;&#38376;&#26041;&#27861;&#26356;&#20026;&#31616;&#21333;&#20294;&#25928;&#26524;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.14970</link><description>&lt;p&gt;
&#36882;&#24402;&#36229;&#32593;&#32476;&#22312;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#24378;&#22823;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Recurrent Hypernetworks are Surprisingly Strong in Meta-RL. (arXiv:2309.14970v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14970
&lt;/p&gt;
&lt;p&gt;
&#36882;&#24402;&#36229;&#32593;&#32476;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22312;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#24378;&#22823;&#24615;&#33021;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#19987;&#38376;&#26041;&#27861;&#26356;&#20026;&#31616;&#21333;&#20294;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#23454;&#38469;&#24212;&#29992;&#26102;&#22240;&#26679;&#26412;&#25928;&#29575;&#20302;&#32780;&#19981;&#26131;&#37096;&#32626;&#12290;&#20803;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#23398;&#20064;&#22312;&#20803;&#35757;&#32451;&#26102;&#21033;&#29992;&#30456;&#20851;&#20219;&#21153;&#30340;&#20998;&#24067;&#26469;&#23454;&#29616;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#30452;&#25509;&#35299;&#20915;&#20102;&#36825;&#20010;&#26679;&#26412;&#25928;&#29575;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#19982;&#19987;&#38376;&#30340;&#20803;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#19982;&#19968;&#20010;&#36890;&#29992;&#30340;&#24207;&#21015;&#27169;&#22411;&#65288;&#22914;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65289;&#32467;&#21512;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#26159;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#24378;&#22522;&#20934;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#35266;&#28857;&#30001;&#20110;&#26377;&#38480;&#30340;&#25903;&#25345;&#35777;&#25454;&#32780;&#24341;&#36215;&#20102;&#20105;&#35758;&#65292;&#29305;&#21035;&#26159;&#22312;&#20043;&#21069;&#30340;&#30740;&#31350;&#20013;&#30830;&#31435;&#20102;&#23436;&#20840;&#30456;&#21453;&#30340;&#35266;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#34429;&#28982;&#25105;&#20204;&#21516;&#26679;&#21457;&#29616;&#24490;&#29615;&#32593;&#32476;&#21487;&#20197;&#36798;&#21040;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#20102;&#36229;&#32593;&#32476;&#30340;&#20351;&#29992;&#23545;&#20110;&#21457;&#25381;&#24490;&#29615;&#22522;&#32447;&#30340;&#28508;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#19982;&#36229;&#32593;&#32476;&#30456;&#32467;&#21512;&#26102;&#65292;&#36825;&#31181;&#36828;&#27604;&#29616;&#26377;&#19987;&#38376;&#26041;&#27861;&#31616;&#21333;&#30340;&#24490;&#29615;&#22522;&#20934;&#23454;&#38469;&#19978;&#33021;&#21462;&#24471;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (RL) is notoriously impractical to deploy due to sample inefficiency. Meta-RL directly addresses this sample inefficiency by learning to perform few-shot learning when a distribution of related tasks is available for meta-training. While many specialized meta-RL methods have been proposed, recent work suggests that end-to-end learning in conjunction with an off-the-shelf sequential model, such as a recurrent network, is a surprisingly strong baseline. However, such claims have been controversial due to limited supporting evidence, particularly in the face of prior work establishing precisely the opposite. In this paper, we conduct an empirical investigation. While we likewise find that a recurrent network can achieve strong performance, we demonstrate that the use of hypernetworks is crucial to maximizing their potential. Surprisingly, when combined with hypernetworks, the recurrent baselines that are far simpler than existing specialized methods actually ac
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#20805;&#20998;&#22240;&#32032;&#21644;&#24517;&#35201;&#22240;&#32032;&#30340;&#27010;&#29575;&#65288;PNS&#65289;&#26469;&#25913;&#21892;&#22312;&#26410;&#30693;&#27979;&#35797;&#20998;&#24067;&#19978;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#22240;&#26524;&#24615;&#30340;&#19981;&#21464;&#24615;&#23646;&#24615;&#32780;&#24573;&#35270;&#20805;&#20998;&#24615;&#21644;&#24517;&#35201;&#24615;&#26465;&#20214;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.12559</link><description>&lt;p&gt;
&#36890;&#36807;&#20805;&#20998;&#22240;&#32032;&#21644;&#24517;&#35201;&#22240;&#32032;&#30340;&#27010;&#29575;&#36827;&#34892;&#19981;&#21464;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Invariant Learning via Probability of Sufficient and Necessary Causes. (arXiv:2309.12559v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#20805;&#20998;&#22240;&#32032;&#21644;&#24517;&#35201;&#22240;&#32032;&#30340;&#27010;&#29575;&#65288;PNS&#65289;&#26469;&#25913;&#21892;&#22312;&#26410;&#30693;&#27979;&#35797;&#20998;&#24067;&#19978;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#22240;&#26524;&#24615;&#30340;&#19981;&#21464;&#24615;&#23646;&#24615;&#32780;&#24573;&#35270;&#20805;&#20998;&#24615;&#21644;&#24517;&#35201;&#24615;&#26465;&#20214;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37326;&#22806;&#23398;&#20064;&#20013;&#65292;&#23545;&#20110;&#26410;&#30693;&#30340;&#12289;&#19982;&#35757;&#32451;&#20998;&#24067;&#19981;&#21516;&#30340;&#27979;&#35797;&#20998;&#24067;&#65292;&#22806;&#37096;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#12290;&#26368;&#36817;&#20174;&#22240;&#26524;&#24615;&#24341;&#21457;&#30340;&#26041;&#27861;&#22312;&#23454;&#29616;OOD&#27867;&#21270;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#22240;&#26524;&#24615;&#30340;&#19981;&#21464;&#24615;&#23646;&#24615;&#65292;&#32780;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24573;&#35270;&#20102;&#20805;&#20998;&#24615;&#21644;&#24517;&#35201;&#24615;&#26465;&#20214;&#30340;&#23646;&#24615;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#19968;&#20010;&#24517;&#35201;&#20294;&#19981;&#20805;&#20998;&#30340;&#21407;&#22240;&#65288;&#29305;&#24449;&#65289;&#23545;&#20110;&#20998;&#24067;&#36716;&#25442;&#26159;&#19981;&#21464;&#30340;&#65292;&#20294;&#21487;&#33021;&#27809;&#26377;&#25152;&#38656;&#30340;&#20934;&#30830;&#24230;&#12290;&#30456;&#21453;&#65292;&#19968;&#20010;&#20805;&#20998;&#20294;&#19981;&#24517;&#35201;&#30340;&#21407;&#22240;&#65288;&#29305;&#24449;&#65289;&#20542;&#21521;&#20110;&#24456;&#22909;&#22320;&#36866;&#24212;&#29305;&#23450;&#25968;&#25454;&#65292;&#20294;&#21487;&#33021;&#23384;&#22312;&#36866;&#24212;&#26032;&#39046;&#22495;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#25429;&#25417;&#20805;&#20998;&#21644;&#24517;&#35201;&#22240;&#32032;&#30340;&#20449;&#24687;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#32463;&#20856;&#27010;&#24565;&#8212;&#8212;&#20805;&#20998;&#21644;&#24517;&#35201;&#22240;&#32032;&#30340;&#27010;&#29575;&#65288;PNS&#65289;&#65292;&#23427;&#25351;&#31034;&#20102;&#19968;&#20010;&#22240;&#32032;&#26159;&#24517;&#35201;&#21644;&#20805;&#20998;&#21407;&#22240;&#30340;&#27010;&#29575;&#12290;&#20026;&#20102;&#23558;PNS&#19982;OOD&#27867;&#21270;&#32852;&#31995;&#36215;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) generalization is indispensable for learning models in the wild, where testing distribution typically unknown and different from the training. Recent methods derived from causality have shown great potential in achieving OOD generalization. However, existing methods mainly focus on the invariance property of causes, while largely overlooking the property of \textit{sufficiency} and \textit{necessity} conditions. Namely, a necessary but insufficient cause (feature) is invariant to distribution shift, yet it may not have required accuracy. By contrast, a sufficient yet unnecessary cause (feature) tends to fit specific data well but may have a risk of adapting to a new domain. To capture the information of sufficient and necessary causes, we employ a classical concept, the probability of sufficiency and necessary causes (PNS), which indicates the probability of whether one is the necessary and sufficient cause. To associate PNS with OOD generalization, we propose
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29615;&#22659;&#20559;&#21521;&#29305;&#24449;&#25490;&#24207;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#40065;&#26834;&#24615;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;&#12290;&#36890;&#36807;&#35745;&#31639;&#29305;&#24449;&#30340;&#29615;&#22659;&#20043;&#38388;&#20998;&#24067;&#26041;&#24046;&#36827;&#34892;&#35780;&#20998;&#65292;&#24182;&#36890;&#36807;&#21435;&#38500;&#39640;&#20998;&#29305;&#24449;&#26469;&#25913;&#21892;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#30495;&#23454;&#21644;&#21512;&#25104;&#22522;&#20934;&#25968;&#25454;&#19978;&#22343;&#33021;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12301</link><description>&lt;p&gt;
&#29615;&#22659;&#20559;&#21521;&#29305;&#24449;&#25490;&#24207;&#29992;&#20110;&#40065;&#26834;&#24615;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Environment-biased Feature Ranking for Novelty Detection Robustness. (arXiv:2309.12301v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29615;&#22659;&#20559;&#21521;&#29305;&#24449;&#25490;&#24207;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#40065;&#26834;&#24615;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;&#12290;&#36890;&#36807;&#35745;&#31639;&#29305;&#24449;&#30340;&#29615;&#22659;&#20043;&#38388;&#20998;&#24067;&#26041;&#24046;&#36827;&#34892;&#35780;&#20998;&#65292;&#24182;&#36890;&#36807;&#21435;&#38500;&#39640;&#20998;&#29305;&#24449;&#26469;&#25913;&#21892;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#30495;&#23454;&#21644;&#21512;&#25104;&#22522;&#20934;&#25968;&#25454;&#19978;&#22343;&#33021;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#40065;&#26834;&#24615;&#26032;&#39062;&#24615;&#26816;&#27979;&#30340;&#38382;&#39064;&#65292;&#22312;&#35813;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#26816;&#27979;&#35821;&#20041;&#20869;&#23481;&#26041;&#38754;&#30340;&#26032;&#39062;&#24615;&#65292;&#21516;&#26102;&#23545;&#20854;&#20182;&#26080;&#20851;&#22240;&#32032;&#30340;&#21464;&#21270;&#20855;&#26377;&#19981;&#21464;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22312;&#20855;&#26377;&#22810;&#20010;&#29615;&#22659;&#30340;&#35774;&#32622;&#20013;&#25805;&#20316;&#65292;&#30830;&#23450;&#19982;&#29615;&#22659;&#26356;&#30456;&#20851;&#32780;&#19981;&#26159;&#20219;&#21153;&#30456;&#20851;&#20869;&#23481;&#30340;&#29305;&#24449;&#38598;&#21512;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20174;&#39044;&#35757;&#32451;&#30340;&#23884;&#20837;&#21644;&#22810;&#29615;&#22659;&#35774;&#32622;&#24320;&#22987;&#65292;&#25104;&#21151;&#26681;&#25454;&#20854;&#29615;&#22659;&#20851;&#27880;&#24230;&#23545;&#29305;&#24449;&#36827;&#34892;&#25490;&#24207;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22522;&#20110;&#29615;&#22659;&#20043;&#38388;&#30340;&#29305;&#24449;&#20998;&#24067;&#26041;&#24046;&#35745;&#31639;&#27599;&#20010;&#29305;&#24449;&#30340;&#24471;&#20998;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35777;&#26126;&#36890;&#36807;&#33293;&#24323;&#24471;&#20998;&#36739;&#39640;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#21487;&#20197;&#21435;&#38500;&#34394;&#20551;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#22312;&#27491;&#24577;&#21327;&#26041;&#24046;&#21644;&#23376;&#31181;&#32676;&#36716;&#31227;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#25972;&#20307;&#24615;&#33021;&#65292;&#26080;&#35770;&#26159;&#23545;&#20110;&#30495;&#23454;&#30340;&#36824;&#26159;&#23545;&#20110;&#25105;&#20204;&#20026;&#27492;&#20219;&#21153;&#24341;&#20837;&#30340;&#21512;&#25104;&#22522;&#20934;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We tackle the problem of robust novelty detection, where we aim to detect novelties in terms of semantic content while being invariant to changes in other, irrelevant factors. Specifically, we operate in a setup with multiple environments, where we determine the set of features that are associated more with the environments, rather than to the content relevant for the task. Thus, we propose a method that starts with a pretrained embedding and a multi-env setup and manages to rank the features based on their environment-focus. First, we compute a per-feature score based on the feature distribution variance between envs. Next, we show that by dropping the highly scored ones, we manage to remove spurious correlations and improve the overall performance by up to 6%, both in covariance and sub-population shift cases, both for a real and a synthetic benchmark, that we introduce for this task.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;SlimPajama&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#19981;&#21516;&#25968;&#25454;&#32452;&#21512;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#20840;&#23616;&#21435;&#37325;&#21644;&#23616;&#37096;&#21435;&#37325;&#30340;&#27604;&#36739;&#21644;&#39640;&#36136;&#37327;&#22810;&#28304;&#25968;&#25454;&#38598;&#30340;&#27604;&#20363;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.10818</link><description>&lt;p&gt;
SlimPajama-DC: &#29702;&#35299;LLM&#35757;&#32451;&#20013;&#30340;&#25968;&#25454;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
SlimPajama-DC: Understanding Data Combinations for LLM Training. (arXiv:2309.10818v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;SlimPajama&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#19981;&#21516;&#25968;&#25454;&#32452;&#21512;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#20840;&#23616;&#21435;&#37325;&#21644;&#23616;&#37096;&#21435;&#37325;&#30340;&#27604;&#36739;&#21644;&#39640;&#36136;&#37327;&#22810;&#28304;&#25968;&#25454;&#38598;&#30340;&#27604;&#20363;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#20102;&#35299;&#20351;&#29992;SlimPajama&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#26102;&#21508;&#31181;&#25968;&#25454;&#32452;&#21512;&#65288;&#22914;&#32593;&#32476;&#25991;&#26412;&#12289;&#32500;&#22522;&#30334;&#31185;&#12289;GitHub&#12289;&#22270;&#20070;&#65289;&#23545;&#20854;&#35757;&#32451;&#30340;&#24433;&#21709;&#12290;SlimPajama&#26159;&#19968;&#20010;&#32463;&#36807;&#20005;&#26684;&#21435;&#37325;&#30340;&#22810;&#28304;&#25968;&#25454;&#38598;&#65292;&#20174;Together&#36129;&#29486;&#30340;1.2T&#20010;token&#30340;RedPajama&#25968;&#25454;&#38598;&#20013;&#31934;&#32454;&#32452;&#21512;&#21644;&#21435;&#37325;&#65292;&#24635;&#20849;&#24471;&#21040;&#20102;627B&#20010;tokens&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#30740;&#31350;&#31216;&#20026;SlimPajama-DC&#65292;&#36825;&#26159;&#19968;&#39033;&#26088;&#22312;&#25581;&#31034;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#20351;&#29992;SlimPajama&#25152;&#28041;&#21450;&#30340;&#22522;&#26412;&#29305;&#24449;&#21644;&#26368;&#20339;&#23454;&#36341;&#30340;&#32463;&#39564;&#20998;&#26512;&#12290;&#22312;&#25105;&#20204;&#20351;&#29992;SlimPajama&#36827;&#34892;&#30740;&#31350;&#30340;&#36807;&#31243;&#20013;&#65292;&#20986;&#29616;&#20102;&#20004;&#20010;&#20851;&#38190;&#35266;&#23519;&#32467;&#26524;&#65306;&#65288;1&#65289;&#20840;&#23616;&#21435;&#37325; vs. &#23616;&#37096;&#21435;&#37325;&#12290;&#25105;&#20204;&#20998;&#26512;&#21644;&#35752;&#35770;&#20102;&#20840;&#23616;&#21435;&#37325;&#65288;&#36328;&#19981;&#21516;&#25968;&#25454;&#38598;&#28304;&#65289;&#21644;&#23616;&#37096;&#21435;&#37325;&#65288;&#22312;&#21333;&#20010;&#25968;&#25454;&#38598;&#28304;&#20869;&#37096;&#65289;&#23545;&#35757;&#32451;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#65288;2&#65289;&#39640;&#36136;&#37327;/&#39640;&#24230;&#21435;&#37325;&#30340;&#22810;&#28304;&#25968;&#25454;&#38598;&#22312;&#32452;&#21512;&#20013;&#30340;&#27604;&#20363;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;...
&lt;/p&gt;
&lt;p&gt;
This paper aims to understand the impacts of various data combinations (e.g., web text, wikipedia, github, books) on the training of large language models using SlimPajama. SlimPajama is a rigorously deduplicated, multi-source dataset, which has been refined and further deduplicated to 627B tokens from the extensive 1.2T tokens RedPajama dataset contributed by Together. We've termed our research as SlimPajama-DC, an empirical analysis designed to uncover fundamental characteristics and best practices associated with employing SlimPajama in the training of large language models. During our research with SlimPajama, two pivotal observations emerged: (1) Global deduplication vs. local deduplication. We analyze and discuss how global (across different sources of datasets) and local (within the single source of dataset) deduplications affect the performance of trained models. (2) Proportions of high-quality/highly-deduplicated multi-source datasets in the combination. To study this, we cons
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20934;&#30830;&#24615;&#39044;&#27979;&#22120;&#26469;&#20462;&#21098;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#26159;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#65292;&#21487;&#20197;&#33258;&#21160;&#36873;&#25321;&#26368;&#20248;&#27169;&#22411;&#24182;&#38477;&#20302;&#22256;&#24785;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.09507</link><description>&lt;p&gt;
&#36890;&#36807;&#20934;&#30830;&#24615;&#39044;&#27979;&#22120;&#20462;&#21098;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Pruning Large Language Models via Accuracy Predictor. (arXiv:2309.09507v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20934;&#30830;&#24615;&#39044;&#27979;&#22120;&#26469;&#20462;&#21098;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#26159;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#65292;&#21487;&#20197;&#33258;&#21160;&#36873;&#25321;&#26368;&#20248;&#27169;&#22411;&#24182;&#38477;&#20302;&#22256;&#24785;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20855;&#26377;&#25968;&#30334;&#20159;&#20010;&#21442;&#25968;&#65288;&#29978;&#33267;&#26356;&#22810;&#65289;&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24222;&#22823;&#30340;&#27169;&#22411;&#35268;&#27169;&#32473;&#35757;&#32451;&#12289;&#25512;&#29702;&#21644;&#37096;&#32626;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#22240;&#27492;&#26377;&#24517;&#35201;&#23545;&#27169;&#22411;&#36827;&#34892;&#21387;&#32553;&#12290;&#30446;&#21069;&#65292;&#22823;&#37096;&#20998;LLMs&#27169;&#22411;&#21387;&#32553;&#38656;&#35201;&#25163;&#21160;&#35774;&#35745;&#20462;&#21098;&#29305;&#24615;&#65292;&#23384;&#22312;&#35832;&#22914;&#22797;&#26434;&#30340;&#20248;&#21270;&#27969;&#31243;&#21644;&#38590;&#20197;&#20445;&#30041;&#27169;&#22411;&#37096;&#20998;&#33021;&#21147;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20462;&#21098;&#26041;&#27861;&#65306;&#39318;&#20808;&#65292;&#24314;&#31435;&#19968;&#32452;&#29305;&#23450;&#25968;&#37327;&#30340;&#26550;&#26500;-&#20934;&#30830;&#24615;&#23545;&#30340;&#35757;&#32451;&#38598;&#65292;&#28982;&#21518;&#35757;&#32451;&#19968;&#20010;&#38750;&#31070;&#32463;&#27169;&#22411;&#20316;&#20026;&#20934;&#30830;&#24615;&#39044;&#27979;&#22120;&#12290;&#36890;&#36807;&#20351;&#29992;&#20934;&#30830;&#24615;&#39044;&#27979;&#22120;&#36827;&#19968;&#27493;&#20248;&#21270;&#25628;&#32034;&#31354;&#38388;&#21644;&#25628;&#32034;&#65292;&#21487;&#20197;&#33258;&#21160;&#36873;&#25321;&#26368;&#20248;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#12290;&#19982;&#22522;&#20934;&#27169;&#22411;&#30456;&#27604;&#65292;&#22312;Wikitext2&#21644;PTB&#19978;&#30340;&#22256;&#24785;&#24230;&#65288;PPL&#65289;&#20998;&#21035;&#19979;&#38477;&#20102;9.48%&#21644;5.7%&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models(LLMs) containing tens of billions of parameters (or even more) have demonstrated impressive capabilities in various NLP tasks. However, substantial model size poses challenges to training, inference, and deployment so that it is necessary to compress the model. At present, most model compression for LLMs requires manual design of pruning features, which has problems such as complex optimization pipeline and difficulty in retaining the capabilities of certain parts of the model.Therefore, we propose a novel pruning approach: firstly, a training set of a certain number of architecture-accuracy pairs is established, and then a non-neural model is trained as an accuracy predictor. Using the accuracy predictor to further optimize the search space and search, the optimal model can be automatically selected. Experiments show that our proposed approach is effective and efficient. Compared with the baseline, the perplexity(PPL) on Wikitext2 and PTB dropped by 9.48% and 5,7
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#30740;&#31350;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#31181;&#20256;&#24863;&#22120;&#65292;&#30740;&#31350;&#20102;&#24613;&#24615;&#37202;&#31934;&#25668;&#20837;&#23545;&#39550;&#39542;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#35782;&#21035;&#37202;&#39550;&#34892;&#20026;&#26469;&#20943;&#23569;&#37202;&#39550;&#20107;&#25925;&#12290;</title><link>http://arxiv.org/abs/2309.08021</link><description>&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;&#30340;&#37202;&#39550;&#21496;&#26426;&#34892;&#20026;&#21644;&#39550;&#39542;&#24615;&#33021;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Vision-based Analysis of Driver Activity and Driving Performance Under the Influence of Alcohol. (arXiv:2309.08021v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08021
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#30740;&#31350;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#31181;&#20256;&#24863;&#22120;&#65292;&#30740;&#31350;&#20102;&#24613;&#24615;&#37202;&#31934;&#25668;&#20837;&#23545;&#39550;&#39542;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#35782;&#21035;&#37202;&#39550;&#34892;&#20026;&#26469;&#20943;&#23569;&#37202;&#39550;&#20107;&#25925;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32654;&#22269;&#32422;30%&#30340;&#20132;&#36890;&#20107;&#25925;&#27515;&#20129;&#28041;&#21450;&#37202;&#39550;&#65292;&#22240;&#27492;&#22312;&#32654;&#22269;&#21644;&#20854;&#20182;&#39640;&#37202;&#39550;&#24739;&#30149;&#29575;&#22320;&#21306;&#65292;&#38450;&#27490;&#37202;&#39550;&#23545;&#36710;&#36742;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#20027;&#21160;&#20351;&#29992;&#20256;&#24863;&#22120;&#65288;&#35201;&#27714;&#39550;&#39542;&#21592;&#25552;&#20379;&#21628;&#27668;&#26679;&#26412;&#32473;&#36710;&#36742;&#20202;&#22120;&#25110;&#34987;&#35686;&#23519;&#25318;&#19979;&#26102;&#65289;&#65292;&#21487;&#20197;&#30417;&#27979;&#39550;&#39542;&#33021;&#21147;&#21463;&#25439;&#65292;&#20294;&#20351;&#29992;&#19968;&#31181;&#26356;&#34987;&#21160;&#19988;&#31283;&#20581;&#30340;&#24863;&#30693;&#26426;&#21046;&#21487;&#33021;&#33021;&#22815;&#26356;&#24191;&#27867;&#22320;&#24212;&#29992;&#26234;&#33021;&#31995;&#32479;&#65292;&#20174;&#32780;&#20943;&#23569;&#37202;&#39550;&#20107;&#25925;&#12290;&#36825;&#21487;&#20197;&#24110;&#21161;&#22312;&#39550;&#39542;&#21069;&#25110;&#39550;&#39542;&#36807;&#31243;&#26089;&#26399;&#65288;&#22312;&#20107;&#25925;&#25110;&#34987;&#25191;&#27861;&#37096;&#38376;&#21457;&#29616;&#20043;&#21069;&#65289;&#35782;&#21035;&#20986;&#21463;&#25439;&#39550;&#39542;&#21592;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#39033;&#20351;&#29992;&#35270;&#35273;&#12289;&#28909;&#24863;&#12289;&#38899;&#39057;&#21644;&#21270;&#23398;&#20256;&#24863;&#22120;&#30340;&#22810;&#27169;&#24577;&#38598;&#25104;&#65292;&#20197;(1)&#22312;&#39550;&#39542;&#27169;&#25311;&#22120;&#20013;&#30740;&#31350;&#24613;&#24615;&#37202;&#31934;&#25668;&#20837;&#23545;&#39550;&#39542;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;(2)&#35782;&#21035;&#37202;&#39550;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
About 30% of all traffic crash fatalities in the United States involve drunk drivers, making the prevention of drunk driving paramount to vehicle safety in the US and other locations which have a high prevalence of driving while under the influence of alcohol. Driving impairment can be monitored through active use of sensors (when drivers are asked to engage in providing breath samples to a vehicle instrument or when pulled over by a police officer), but a more passive and robust mechanism of sensing may allow for wider adoption and benefit of intelligent systems that reduce drunk driving accidents. This could assist in identifying impaired drivers before they drive, or early in the driving process (before a crash or detection by law enforcement). In this research, we introduce a study which adopts a multi-modal ensemble of visual, thermal, audio, and chemical sensors to (1) examine the impact of acute alcohol administration on driving performance in a driving simulator, and (2) identi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#22312;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20013;&#27880;&#20837;&#29289;&#29702;&#30693;&#35782;&#26469;&#35299;&#20915;&#36339;&#36291;&#24335;&#21333;&#33050;&#26426;&#22120;&#20154;&#30340;&#25511;&#21046;&#38382;&#39064;&#65292;&#36825;&#26679;&#21487;&#20197;&#22823;&#24133;&#20943;&#23569;&#23398;&#20064;&#26102;&#38388;&#24182;&#19988;&#33021;&#22815;&#23398;&#20064;&#21644;&#20462;&#27491;&#21487;&#33021;&#20986;&#29616;&#30340;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2309.07038</link><description>&lt;p&gt;
&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#36339;&#36291;&#24335;&#21333;&#33050;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Efficient Reinforcement Learning for Jumping Monopods. (arXiv:2309.07038v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#22312;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20013;&#27880;&#20837;&#29289;&#29702;&#30693;&#35782;&#26469;&#35299;&#20915;&#36339;&#36291;&#24335;&#21333;&#33050;&#26426;&#22120;&#20154;&#30340;&#25511;&#21046;&#38382;&#39064;&#65292;&#36825;&#26679;&#21487;&#20197;&#22823;&#24133;&#20943;&#23569;&#23398;&#20064;&#26102;&#38388;&#24182;&#19988;&#33021;&#22815;&#23398;&#20064;&#21644;&#20462;&#27491;&#21487;&#33021;&#20986;&#29616;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#22797;&#26434;&#30340;&#25511;&#21046;&#38382;&#39064;&#65292;&#21363;&#20351;&#21333;&#33050;&#26426;&#22120;&#20154;&#33021;&#22815;&#36339;&#21040;&#20219;&#20309;&#26041;&#21521;&#65292;&#20854;&#33050;&#19979;&#30340;&#22320;&#24418;&#21487;&#33021;&#26159;&#19981;&#24179;&#30340;&#65292;&#25105;&#20204;&#35201;&#20351;&#23427;&#36798;&#21040;&#30446;&#26631;&#20301;&#32622;&#12290;&#36825;&#26159;&#19968;&#20010;&#26356;&#22823;&#31867;&#21035;&#38382;&#39064;&#30340;&#27169;&#26495;&#65292;&#20351;&#29992;&#26631;&#20934;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#25216;&#26415;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#35745;&#31639;&#24320;&#38144;&#12290;&#24378;&#21270;&#23398;&#20064; (RL) &#21487;&#33021;&#26159;&#19968;&#20010;&#26377;&#36259;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#20294;&#23436;&#20840;&#20174;&#38646;&#24320;&#22987;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#22312; RL &#26694;&#26550;&#20013;&#27880;&#20837;&#29289;&#29702;&#30693;&#35782;&#26469;&#25351;&#23548;&#23398;&#20064;&#36807;&#31243;&#12290;&#36825;&#31181;&#26041;&#27861;&#24102;&#26469;&#20102;&#24191;&#27867;&#30340;&#22909;&#22788;&#65292;&#22914;&#22823;&#24133;&#20943;&#23569;&#23398;&#20064;&#26102;&#38388;&#65292;&#24182;&#19988;&#33021;&#22815;&#23398;&#20064;&#21644;&#20462;&#27491;&#25191;&#34892;&#36816;&#21160;&#30340;&#20302;&#32423;&#25511;&#21046;&#22120;&#21487;&#33021;&#20986;&#29616;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#36890;&#36807;&#19982;&#22522;&#20110;&#20248;&#21270;&#21644;&#31471;&#21040;&#31471; RL &#26041;&#27861;&#30340;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we consider the complex control problem of making a monopod reach a target with a jump. The monopod can jump in any direction and the terrain underneath its foot can be uneven. This is a template of a much larger class of problems, which are extremely challenging and computationally expensive to solve using standard optimisation-based techniques. Reinforcement Learning (RL) could be an interesting alternative, but the application of an end-to-end approach in which the controller must learn everything from scratch, is impractical. The solution advocated in this paper is to guide the learning process within an RL framework by injecting physical knowledge. This expedient brings to widespread benefits, such as a drastic reduction of the learning time, and the ability to learn and compensate for possible errors in the low-level controller executing the motion. We demonstrate the advantage of our approach with respect to both optimization-based and end-to-end RL approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;&#35780;&#20272;&#26694;&#26550;HaELM&#65292;&#21487;&#20197;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#24182;&#20998;&#26512;&#20102;&#23548;&#33268;&#24187;&#35273;&#30340;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#32531;&#35299;&#24187;&#35273;&#38382;&#39064;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2308.15126</link><description>&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#30340;&#35780;&#20272;&#19982;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Evaluation and Analysis of Hallucination in Large Vision-Language Models. (arXiv:2308.15126v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;&#35780;&#20272;&#26694;&#26550;HaELM&#65292;&#21487;&#20197;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#24182;&#20998;&#26512;&#20102;&#23548;&#33268;&#24187;&#35273;&#30340;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#32531;&#35299;&#24187;&#35273;&#38382;&#39064;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;LVLMs&#20173;&#28982;&#23384;&#22312;&#24187;&#35273;&#38382;&#39064;&#65292;&#36825;&#38480;&#21046;&#20102;&#22312;&#35768;&#22810;&#22330;&#26223;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#24187;&#35273;&#25351;&#30340;&#26159;LVLMs&#21709;&#24212;&#20013;&#19981;&#23384;&#22312;&#20110;&#35270;&#35273;&#36755;&#20837;&#20013;&#30340;&#20449;&#24687;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#37325;&#22823;&#21518;&#26524;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#30446;&#21069;&#23545;LVLMs&#20013;&#30340;&#24187;&#35273;&#35780;&#20272;&#30340;&#30740;&#31350;&#24037;&#20316;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24187;&#35273;&#35780;&#20272;&#26694;&#26550;HaELM&#12290;HaELM&#30340;&#24615;&#33021;&#36817;&#20284;&#20110;ChatGPT&#30340;95%&#65292;&#24182;&#20855;&#26377;&#20302;&#25104;&#26412;&#12289;&#21487;&#22797;&#29616;&#12289;&#20445;&#25252;&#38544;&#31169;&#21644;&#26412;&#22320;&#37096;&#32626;&#31561;&#39069;&#22806;&#20248;&#21183;&#12290;&#21033;&#29992;HaELM&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#24403;&#21069;LVLMs&#20013;&#30340;&#24187;&#35273;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#23548;&#33268;LVLMs&#20013;&#24187;&#35273;&#30340;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#32531;&#35299;&#24187;&#35273;&#38382;&#39064;&#30340;&#26377;&#29992;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Vision-Language Models (LVLMs) have recently achieved remarkable success. However, LVLMs are still plagued by the hallucination problem, which limits the practicality in many scenarios. Hallucination refers to the information of LVLMs' responses that does not exist in the visual input, which poses potential risks of substantial consequences. There has been limited work studying hallucination evaluation in LVLMs. In this paper, we propose Hallucination Evaluation based on Large Language Models (HaELM), an LLM-based hallucination evaluation framework. HaELM achieves an approximate 95% performance comparable to ChatGPT and has additional advantages including low cost, reproducibility, privacy preservation and local deployment. Leveraging the HaELM, we evaluate the hallucination in current LVLMs. Furthermore, we analyze the factors contributing to hallucination in LVLMs and offer helpful suggestions to mitigate the hallucination problem. Our training data and human annotation halluci
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;Mixup&#22686;&#24378;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#34507;&#30333;&#36136;&#27169;&#25311;&#22120;&#30340;&#39640;&#25928;&#24494;&#35843;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#19979;&#27867;&#21270;&#21040;&#26410;&#35265;&#36807;&#30340;&#22330;&#26223;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#27169;&#25311;&#36830;&#32493;&#21160;&#24577;&#26465;&#20214;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.15116</link><description>&lt;p&gt;
&#36890;&#36807;Mixup&#22686;&#24378;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#34507;&#30333;&#36136;&#27169;&#25311;&#22120;&#30340;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Mixup-Augmented Meta-Learning for Sample-Efficient Fine-Tuning of Protein Simulators. (arXiv:2308.15116v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;Mixup&#22686;&#24378;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#34507;&#30333;&#36136;&#27169;&#25311;&#22120;&#30340;&#39640;&#25928;&#24494;&#35843;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#19979;&#27867;&#21270;&#21040;&#26410;&#35265;&#36807;&#30340;&#22330;&#26223;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#27169;&#25311;&#36830;&#32493;&#21160;&#24577;&#26465;&#20214;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#24050;&#32463;&#25104;&#20026;&#30740;&#31350;&#29983;&#29289;&#20998;&#23376;&#30340;&#22522;&#26412;&#24037;&#20855;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25105;&#20204;&#24076;&#26395;&#22312;&#20998;&#23376;&#33021;&#22815;&#27874;&#21160;&#30340;&#21508;&#31181;&#26465;&#20214;&#19979;&#23545;&#19968;&#32452;&#31890;&#23376;&#36827;&#34892;&#27169;&#25311;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#36719;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#20998;&#23376;&#21160;&#21147;&#23398;&#20219;&#21153;&#24182;&#36827;&#34892;&#20102;&#36866;&#24212;&#24615;&#25506;&#32034;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#19979;&#38750;&#24120;&#22909;&#22320;&#27867;&#21270;&#21040;&#26410;&#35265;&#36807;&#30340;&#21644;&#36229;&#20986;&#20998;&#24067;&#30340;&#22330;&#26223;&#12290;&#34429;&#28982;&#25105;&#20204;&#30340;&#24037;&#20316;&#20197;&#28201;&#24230;&#20026;&#27979;&#35797;&#26696;&#20363;&#65292;&#20294;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#22810;&#21151;&#33021;&#24615;&#20351;&#20854;&#21487;&#20197;&#36890;&#36807;&#20219;&#20309;&#36830;&#32493;&#30340;&#21160;&#24577;&#26465;&#20214;&#65288;&#22914;&#21387;&#21147;&#21644;&#20307;&#31215;&#65289;&#36827;&#34892;&#26377;&#25928;&#27169;&#25311;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26377;&#20004;&#20010;&#38454;&#27573;&#65306;1&#65289;&#20351;&#29992;&#25968;&#25454;&#28151;&#21512;&#25216;&#26415;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#22686;&#24378;&#20998;&#23376;&#32467;&#26500;&#25968;&#25454;&#21644;&#28201;&#24230;&#25552;&#31034;&#65292;&#28982;&#21518;&#36890;&#36807;&#36880;&#28176;&#22686;&#21152;&#27604;&#20363;&#30340;&#26041;&#24335;&#24212;&#29992;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#12290;2&#65289;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#24494;&#35843;&#26694;&#26550;&#25552;&#39640;&#20102;&#24494;&#35843;&#36807;&#31243;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#24182;&#20026;&#36719;&#25552;&#31034;&#24494;&#35843;&#25552;&#20379;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular dynamics simulations have emerged as a fundamental instrument for studying biomolecules. At the same time, it is desirable to perform simulations of a collection of particles under various conditions in which the molecules can fluctuate. In this paper, we explore and adapt the soft prompt-based learning method to molecular dynamics tasks. Our model can remarkably generalize to unseen and out-of-distribution scenarios with limited training data. While our work focuses on temperature as a test case, the versatility of our approach allows for efficient simulation through any continuous dynamic conditions, such as pressure and volumes. Our framework has two stages: 1) Pre-trains with data mixing technique, augments molecular structure data and temperature prompts, then applies a curriculum learning method by increasing the ratio of them smoothly. 2) Meta-learning-based fine-tuning framework improves sample-efficiency of fine-tuning process and gives the soft prompt-tuning better 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26410;&#30693;&#32593;&#32476;&#19978;&#36827;&#34892;&#27969;&#34892;&#30149;&#25511;&#21046;&#12290;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.14311</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#30340;&#26410;&#30693;&#32593;&#32476;&#20256;&#25773;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Spread Control Method on Unknown Networks Based on Hierarchical Reinforcement Learning. (arXiv:2308.14311v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26410;&#30693;&#32593;&#32476;&#19978;&#36827;&#34892;&#27969;&#34892;&#30149;&#25511;&#21046;&#12290;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19&#31561;&#27969;&#34892;&#30149;&#23545;&#20844;&#20849;&#21355;&#29983;&#21644;&#31038;&#20250;&#26500;&#25104;&#20005;&#37325;&#23041;&#32961;&#65292;&#22240;&#27492;&#26377;&#24517;&#35201;&#30740;&#31350;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#25511;&#21046;&#27969;&#34892;&#30149;&#22312;&#32593;&#32476;&#20013;&#30340;&#20256;&#25773;&#12290;&#20197;&#24448;&#20851;&#20110;&#27969;&#34892;&#30149;&#25511;&#21046;&#30340;&#30740;&#31350;&#24448;&#24448;&#20551;&#35774;&#23436;&#20840;&#20102;&#35299;&#32593;&#32476;&#32467;&#26500;&#65292;&#36825;&#22312;&#23454;&#38469;&#24773;&#20917;&#20013;&#24456;&#23569;&#25104;&#31435;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26410;&#30693;&#32467;&#26500;&#30340;&#32593;&#32476;&#19978;&#30340;&#27969;&#34892;&#30149;&#25511;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#20197;&#21516;&#26102;&#25506;&#32034;&#32593;&#32476;&#32467;&#26500;&#21644;&#25511;&#21046;&#27969;&#34892;&#30149;&#12290;&#20026;&#20102;&#20943;&#23569;&#34892;&#21160;&#31354;&#38388;&#21644;&#23454;&#29616;&#21487;&#35745;&#31639;&#24615;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#21547;&#19977;&#20010;&#27169;&#22359;&#65306;&#31574;&#30053;&#36873;&#25321;&#27169;&#22359;&#65292;&#30830;&#23450;&#26159;&#25506;&#32034;&#32467;&#26500;&#36824;&#26159;&#31227;&#38500;&#33410;&#28857;&#20197;&#25511;&#21046;&#27969;&#34892;&#30149;&#65307;&#25506;&#32034;&#27169;&#22359;&#65292;&#36127;&#36131;&#36873;&#25321;&#35201;&#25506;&#32034;&#30340;&#33410;&#28857;&#65307;&#31227;&#38500;&#27169;&#22359;&#65292;&#20915;&#23450;&#31227;&#38500;&#21738;&#20123;&#33410;&#28857;&#20197;&#20572;&#27490;&#27969;&#34892;&#30149;&#20256;&#25773;&#12290;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Epidemics such as COVID-19 pose serious threats to public health and our society, and it is critical to investigate effective methods to control the spread of epidemics over networks. Prior works on epidemic control often assume complete knowledge of network structures, a presumption seldom valid in real-world situations. In this paper, we study epidemic control on networks with unknown structures, and propose a hierarchical reinforcement learning framework for joint network structure exploration and epidemic control. To reduce the action space and achieve computation tractability, our proposed framework contains three modules: the Policy Selection Module, which determines whether to explore the structure or remove nodes to control the epidemic; the Explore Module, responsible for selecting nodes to explore; and the Remove Module, which decides which nodes to remove to stop the epidemic spread. Simulation results show that our proposed method outperforms baseline methods.
&lt;/p&gt;</description></item><item><title>chatGPT ADA&#26159;&#19968;&#31181;&#33021;&#22815;&#33258;&#20027;&#24320;&#21457;&#20020;&#24202;&#30740;&#31350;&#25152;&#38656;&#30340;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#23558;&#39640;&#32423;&#20998;&#26512;&#24037;&#20855;&#27665;&#20027;&#21270;&#65292;&#20351;&#38750;&#25968;&#25454;&#31185;&#23398;&#23478;&#30340;&#20020;&#24202;&#21307;&#29983;&#33021;&#22815;&#36731;&#26494;&#24212;&#29992;&#20110;&#21307;&#23398;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2308.14120</link><description>&lt;p&gt;
&#25480;&#26435;&#20020;&#24202;&#21307;&#29983;&#24182;&#27665;&#20027;&#21270;&#25968;&#25454;&#31185;&#23398;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#21270;&#20020;&#24202;&#30740;&#31350;&#30340;&#26426;&#22120;&#23398;&#20064;&#12290; (arXiv:2308.14120v2 [cs.LG] &#26356;&#26032;&#29256;)
&lt;/p&gt;
&lt;p&gt;
Empowering Clinicians and Democratizing Data Science: Large Language Models Automate Machine Learning for Clinical Studies. (arXiv:2308.14120v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14120
&lt;/p&gt;
&lt;p&gt;
chatGPT ADA&#26159;&#19968;&#31181;&#33021;&#22815;&#33258;&#20027;&#24320;&#21457;&#20020;&#24202;&#30740;&#31350;&#25152;&#38656;&#30340;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#23558;&#39640;&#32423;&#20998;&#26512;&#24037;&#20855;&#27665;&#20027;&#21270;&#65292;&#20351;&#38750;&#25968;&#25454;&#31185;&#23398;&#23478;&#30340;&#20020;&#24202;&#21307;&#29983;&#33021;&#22815;&#36731;&#26494;&#24212;&#29992;&#20110;&#21307;&#23398;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24320;&#21457;&#32773;&#65288;&#22914;&#25968;&#25454;&#31185;&#23398;&#23478;&#65289;&#21644;&#20174;&#19994;&#32773;&#65288;&#22914;&#20020;&#24202;&#21307;&#29983;&#65289;&#20043;&#38388;&#23384;&#22312;&#30693;&#35782;&#24046;&#36317;&#65292;&#38459;&#30861;&#20102;ML&#22312;&#20020;&#24202;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#20805;&#20998;&#21033;&#29992;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;chatGPT Advanced Data Analysis&#65288;ADA&#65289;&#65292;&#21363;GPT-4&#30340;&#25193;&#23637;&#65292;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#24182;&#39640;&#25928;&#25191;&#34892;ML&#20998;&#26512;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#21521;chatGPT ADA&#25552;&#20379;&#20102;&#21508;&#31181;&#21307;&#23398;&#19987;&#19994;&#30340;&#22823;&#22411;&#35797;&#39564;&#30340;&#30495;&#23454;&#20020;&#24202;&#25968;&#25454;&#21644;&#30740;&#31350;&#35814;&#32454;&#20449;&#24687;&#65292;&#27809;&#26377;&#32473;&#20986;&#20855;&#20307;&#25351;&#23548;&#12290;ChatGPT ADA&#22522;&#20110;&#21407;&#22987;&#30740;&#31350;&#30340;&#35757;&#32451;&#25968;&#25454;&#33258;&#20027;&#24320;&#21457;&#20102;&#26368;&#20808;&#36827;&#30340;ML&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#20020;&#24202;&#32467;&#26524;&#65292;&#22914;&#30284;&#30151;&#21457;&#23637;&#12289;&#30284;&#30151;&#36827;&#23637;&#12289;&#30142;&#30149;&#24182;&#21457;&#30151;&#25110;&#33268;&#30149;&#22522;&#22240;&#24207;&#21015;&#31561;&#29983;&#29289;&#26631;&#24535;&#29289;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#36825;&#20123;ML&#27169;&#22411;&#19982;&#20854;&#24050;&#21457;&#34920;&#30340;&#23545;&#24212;&#29289;&#30456;&#21305;&#37197;&#29978;&#33267;&#34920;&#29616;&#26356;&#22909;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;chatGPT ADA&#20026;&#27665;&#20027;&#21270;&#21307;&#23398;&#20013;&#30340;ML&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#36884;&#24452;&#65292;&#20351;&#38750;ML&#19987;&#23478;&#33021;&#22815;&#33719;&#24471;&#20808;&#36827;&#30340;&#20998;&#26512;&#24037;&#20855;&#24182;&#25512;&#21160;&#24191;&#27867;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
A knowledge gap persists between Machine Learning (ML) developers (e.g., data scientists) and practitioners (e.g., clinicians), hampering the full utilization of ML for clinical data analysis. We investigated the potential of the chatGPT Advanced Data Analysis (ADA), an extension of GPT-4, to bridge this gap and perform ML analyses efficiently. Real-world clinical datasets and study details from large trials across various medical specialties were presented to chatGPT ADA without specific guidance. ChatGPT ADA autonomously developed state-of-the-art ML models based on the original study's training data to predict clinical outcomes such as cancer development, cancer progression, disease complications, or biomarkers such as pathogenic gene sequences. Strikingly, these ML models matched or outperformed their published counterparts. We conclude that chatGPT ADA offers a promising avenue to democratize ML in medicine, making advanced analytics accessible to non-ML experts and promoting broa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#20840;&#26080;&#38656;&#21453;&#21521;&#20256;&#25773;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#24352;&#37327;&#21387;&#32553;&#30340;&#26041;&#24046;&#32422;&#20943;&#26041;&#27861;&#21644;&#28151;&#21512;&#26799;&#24230;&#35780;&#20272;&#26041;&#27861;&#25913;&#36827;&#20102;&#20248;&#21270;&#21644;&#25928;&#29575;&#12290;&#21516;&#26102;&#65292;&#36824;&#25193;&#23637;&#20102;&#26694;&#26550;&#29992;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2308.09858</link><description>&lt;p&gt;
&#24352;&#37327;&#21387;&#32553;&#30340;&#21453;&#21521;&#20256;&#25773;&#20813;&#36153;&#35757;&#32451;&#65288;&#29289;&#29702;&#20449;&#24687;&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Tensor-Compressed Back-Propagation-Free Training for (Physics-Informed) Neural Networks. (arXiv:2308.09858v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#20840;&#26080;&#38656;&#21453;&#21521;&#20256;&#25773;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#24352;&#37327;&#21387;&#32553;&#30340;&#26041;&#24046;&#32422;&#20943;&#26041;&#27861;&#21644;&#28151;&#21512;&#26799;&#24230;&#35780;&#20272;&#26041;&#27861;&#25913;&#36827;&#20102;&#20248;&#21270;&#21644;&#25928;&#29575;&#12290;&#21516;&#26102;&#65292;&#36824;&#25193;&#23637;&#20102;&#26694;&#26550;&#29992;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21521;&#20256;&#25773;&#65288;BP&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#35745;&#31639;&#26799;&#24230;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#30828;&#20214;&#21644;&#36719;&#20214;&#36164;&#28304;&#26469;&#25903;&#25345;&#33258;&#21160;&#24494;&#20998;&#65292;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;BP&#26159;&#22256;&#38590;&#30340;&#12290;&#36825;&#22823;&#22823;&#22686;&#21152;&#20102;&#35774;&#22791;&#19978;&#35757;&#32451;&#21152;&#36895;&#22120;&#30340;&#35774;&#35745;&#22797;&#26434;&#24615;&#21644;&#19978;&#24066;&#26102;&#38388;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#20840;&#26080;&#38656;BP&#30340;&#26694;&#26550;&#65292;&#21482;&#38656;&#35201;&#21069;&#21521;&#20256;&#25773;&#23601;&#21487;&#20197;&#35757;&#32451;&#23454;&#38469;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24352;&#37327;&#21387;&#32553;&#30340;&#26041;&#24046;&#32422;&#20943;&#26041;&#27861;&#65292;&#26497;&#22823;&#25552;&#39640;&#20102;&#38646;&#38454;&#65288;ZO&#65289;&#20248;&#21270;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#22823;&#20110;&#20197;&#21069;ZO&#26041;&#27861;&#33021;&#21147;&#30340;&#32593;&#32476;&#23610;&#23544;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26799;&#24230;&#35780;&#20272;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;ZO&#35757;&#32451;&#30340;&#25928;&#29575;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#31232;&#30095;&#26684;&#26041;&#27861;&#26469;&#25193;&#23637;&#25105;&#20204;&#30340;BP-free&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backward propagation (BP) is widely used to compute the gradients in neural network training. However, it is hard to implement BP on edge devices due to the lack of hardware and software resources to support automatic differentiation. This has tremendously increased the design complexity and time-to-market of on-device training accelerators. This paper presents a completely BP-free framework that only requires forward propagation to train realistic neural networks. Our technical contributions are three-fold. Firstly, we present a tensor-compressed variance reduction approach to greatly improve the scalability of zeroth-order (ZO) optimization, making it feasible to handle a network size that is beyond the capability of previous ZO approaches. Secondly, we present a hybrid gradient evaluation approach to improve the efficiency of ZO training. Finally, we extend our BP-free training framework to physics-informed neural networks (PINNs) by proposing a sparse-grid approach to estimate the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#23558;&#27169;&#22411;&#35786;&#26029;&#30340;&#27010;&#24565;&#19982;&#28145;&#24230;&#22270;&#32467;&#26500;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#25968;&#25454;&#23398;&#20064;&#31995;&#32479;&#30340;&#24213;&#23618;&#32467;&#26500;&#24182;&#25552;&#20379;&#21160;&#24577;&#35266;&#23519;&#12290;&#30740;&#31350;&#36890;&#36807;&#37325;&#26032;&#23450;&#20041;&#31995;&#32479;&#34920;&#31034;&#12289;&#35266;&#23519;&#21644;&#25925;&#38556;&#30340;&#26500;&#24314;&#65292;&#24341;&#20837;&#33258;&#30417;&#30563;&#22270;&#32467;&#26500;&#23398;&#20064;&#27169;&#22411;&#20197;&#21450;&#22312;&#32806;&#21512;&#25391;&#33633;&#22120;&#31995;&#32479;&#19978;&#30340;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;&#35786;&#26029;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.06961</link><description>&lt;p&gt;
&#22270;&#32467;&#26500;&#27531;&#24046;&#65306;&#19968;&#31181;&#35786;&#26029;&#30340;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Graph Structural Residuals: A Learning Approach to Diagnosis. (arXiv:2308.06961v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#23558;&#27169;&#22411;&#35786;&#26029;&#30340;&#27010;&#24565;&#19982;&#28145;&#24230;&#22270;&#32467;&#26500;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#25968;&#25454;&#23398;&#20064;&#31995;&#32479;&#30340;&#24213;&#23618;&#32467;&#26500;&#24182;&#25552;&#20379;&#21160;&#24577;&#35266;&#23519;&#12290;&#30740;&#31350;&#36890;&#36807;&#37325;&#26032;&#23450;&#20041;&#31995;&#32479;&#34920;&#31034;&#12289;&#35266;&#23519;&#21644;&#25925;&#38556;&#30340;&#26500;&#24314;&#65292;&#24341;&#20837;&#33258;&#30417;&#30563;&#22270;&#32467;&#26500;&#23398;&#20064;&#27169;&#22411;&#20197;&#21450;&#22312;&#32806;&#21512;&#25391;&#33633;&#22120;&#31995;&#32479;&#19978;&#30340;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;&#35786;&#26029;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#35786;&#26029;&#20381;&#36182;&#20110;&#26500;&#24314;&#26126;&#30830;&#30340;&#31995;&#32479;&#27169;&#22411;&#65292;&#36825;&#20010;&#36807;&#31243;&#21487;&#33021;&#36153;&#26102;&#19988;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#23558;&#27169;&#22411;&#35786;&#26029;&#30340;&#27010;&#24565;&#19982;&#28145;&#24230;&#22270;&#32467;&#26500;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;&#36825;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#21033;&#29992;&#25968;&#25454;&#23398;&#20064;&#31995;&#32479;&#30340;&#24213;&#23618;&#32467;&#26500;&#65292;&#24182;&#25552;&#20379;&#30001;&#20004;&#20010;&#19981;&#21516;&#30340;&#22270;&#37051;&#25509;&#30697;&#38453;&#34920;&#31034;&#30340;&#21160;&#24577;&#35266;&#23519;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36890;&#36807;&#19977;&#20010;&#20027;&#35201;&#36129;&#29486;&#23454;&#29616;&#20102;&#22270;&#32467;&#26500;&#23398;&#20064;&#19982;&#27169;&#22411;&#35786;&#26029;&#30340;&#26080;&#32541;&#38598;&#25104;&#65306;(i)&#37325;&#26032;&#23450;&#20041;&#31995;&#32479;&#34920;&#31034;&#12289;&#35266;&#23519;&#21644;&#25925;&#38556;&#30340;&#26500;&#24314;&#12289;(ii)&#24341;&#20837;&#20004;&#31181;&#19981;&#21516;&#29256;&#26412;&#30340;&#33258;&#30417;&#30563;&#22270;&#32467;&#26500;&#23398;&#20064;&#27169;&#22411;&#26550;&#26500;&#12289;(iii)&#36890;&#36807;&#32806;&#21512;&#25391;&#33633;&#22120;&#31995;&#32479;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;&#25105;&#20204;&#25968;&#25454;&#39537;&#21160;&#30340;&#35786;&#26029;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional model-based diagnosis relies on constructing explicit system models, a process that can be laborious and expertise-demanding. In this paper, we propose a novel framework that combines concepts of model-based diagnosis with deep graph structure learning. This data-driven approach leverages data to learn the system's underlying structure and provide dynamic observations, represented by two distinct graph adjacency matrices. Our work facilitates a seamless integration of graph structure learning with model-based diagnosis by making three main contributions: (i) redefining the constructs of system representation, observations, and faults (ii) introducing two distinct versions of a self-supervised graph structure learning model architecture and (iii) demonstrating the potential of our data-driven diagnostic method through experiments on a system of coupled oscillators.
&lt;/p&gt;</description></item><item><title>&#33529;&#26524;&#25512;&#20986;&#20102;Vision Pro&#65292;&#19968;&#27454;&#20855;&#26377;&#28151;&#21512;&#29616;&#23454;&#21644;&#22686;&#24378;&#29616;&#23454;&#21151;&#33021;&#30340;&#34394;&#25311;&#29616;&#23454;&#35774;&#22791;&#65292;&#25317;&#26377;&#29420;&#29305;&#30340;&#29305;&#28857;&#65292;&#20363;&#22914;&#20869;&#37096;&#23631;&#24149;&#23637;&#31034;&#20329;&#25140;&#32773;&#30340;&#30524;&#30555;&#20197;&#21450;&#25968;&#23383;&#30343;&#20896;&#25353;&#38062;&#30340;&#34701;&#21512;&#21151;&#33021;&#12290;&#36825;&#27454;&#26080;&#32447;&#35774;&#22791;&#21487;&#33021;&#23454;&#29616;&#20102;&#8220;&#32456;&#26497;&#26174;&#31034;&#22120;&#8221;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.04313</link><description>&lt;p&gt;
Apple Vision Pro for Healthcare: &#8220;&#32456;&#26497;&#26174;&#31034;&#22120;&#8221;&#65311;&#65288;arXiv:2308.04313v1 [cs.AI]&#65289;
&lt;/p&gt;
&lt;p&gt;
Apple Vision Pro for Healthcare: "The Ultimate Display"?. (arXiv:2308.04313v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04313
&lt;/p&gt;
&lt;p&gt;
&#33529;&#26524;&#25512;&#20986;&#20102;Vision Pro&#65292;&#19968;&#27454;&#20855;&#26377;&#28151;&#21512;&#29616;&#23454;&#21644;&#22686;&#24378;&#29616;&#23454;&#21151;&#33021;&#30340;&#34394;&#25311;&#29616;&#23454;&#35774;&#22791;&#65292;&#25317;&#26377;&#29420;&#29305;&#30340;&#29305;&#28857;&#65292;&#20363;&#22914;&#20869;&#37096;&#23631;&#24149;&#23637;&#31034;&#20329;&#25140;&#32773;&#30340;&#30524;&#30555;&#20197;&#21450;&#25968;&#23383;&#30343;&#20896;&#25353;&#38062;&#30340;&#34701;&#21512;&#21151;&#33021;&#12290;&#36825;&#27454;&#26080;&#32447;&#35774;&#22791;&#21487;&#33021;&#23454;&#29616;&#20102;&#8220;&#32456;&#26497;&#26174;&#31034;&#22120;&#8221;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;2023&#24180;6&#26376;&#30340;&#20840;&#29699;&#24320;&#21457;&#32773;&#22823;&#20250;&#65288;WWDC&#65289;&#19978;&#65292;&#33529;&#26524;&#25512;&#20986;&#20102;Vision Pro&#12290;Vision Pro&#26159;&#19968;&#27454;&#28151;&#21512;&#29616;&#23454;&#65288;MR&#65289;&#22836;&#30420;&#65292;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#23427;&#26159;&#19968;&#27454;&#20855;&#26377;&#39069;&#22806;&#35270;&#39057;&#36879;&#35270;&#65288;VST&#65289;&#33021;&#21147;&#30340;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#35774;&#22791;&#12290;&#36890;&#36807;&#23558;&#30495;&#23454;&#19990;&#30028;&#36890;&#36807;&#25668;&#20687;&#22836;&#20256;&#36755;&#21040;&#29992;&#25143;&#30524;&#21069;&#30340;&#65288;VR&#65289;&#23631;&#24149;&#65292;&#20351;&#24471;Vision Pro&#20063;&#25104;&#20026;&#20102;&#22686;&#24378;&#29616;&#23454;&#65288;AR&#65289;&#35774;&#22791;&#12290;&#24403;&#28982;&#65292;&#36825;&#24182;&#19981;&#29420;&#29305;&#65292;&#19982;Varjo XR-3&#31561;&#20854;&#20182;&#35774;&#22791;&#31867;&#20284;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;Vision Pro&#20855;&#26377;&#19968;&#20123;&#26377;&#36259;&#30340;&#29305;&#28857;&#65292;&#20363;&#22914;&#20869;&#37096;&#23631;&#24149;&#21487;&#20197;&#21521;&#8220;&#22806;&#30028;&#8221;&#26174;&#31034;&#20329;&#25140;&#22836;&#30420;&#32773;&#30340;&#30524;&#30555;&#65292;&#25110;&#32773;&#39030;&#37096;&#30340;&#19968;&#20010;&#25353;&#38062;&#31216;&#20026;&#8220;&#25968;&#23383;&#30343;&#20896;&#8221;&#65292;&#21487;&#20197;&#36890;&#36807;&#26059;&#36716;&#26080;&#32541;&#22320;&#34701;&#21512;&#25968;&#23383;&#20869;&#23481;&#19982;&#29289;&#29702;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;Vision Pro&#26159;&#26080;&#32447;&#30340;&#65292;&#21482;&#26377;&#30005;&#27744;&#30340;&#30005;&#32518;&#36830;&#25509;&#65292;&#36825;&#20351;&#24471;&#22836;&#30420;&#27604;Varjo XR-3&#26356;&#21152;&#28789;&#27963;&#12290;&#36825;&#21487;&#33021;&#26356;&#25509;&#36817;&#8220;&#32456;&#26497;&#26174;&#31034;&#22120;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
At the Worldwide Developers Conference (WWDC) in June 2023, Apple introduced the Vision Pro. The Vision Pro is a Mixed Reality (MR) headset, more specifically it is a Virtual Reality (VR) device with an additional Video See-Through (VST) capability. The VST capability turns the Vision Pro also into an Augmented Reality (AR) device. The AR feature is enabled by streaming the real world via cameras to the (VR) screens in front of the user's eyes. This is of course not unique and similar to other devices, like the Varjo XR-3. Nevertheless, the Vision Pro has some interesting features, like an inside-out screen that can show the headset wearers' eyes to "outsiders" or a button on the top, called "Digital Crown", that allows you to seamlessly blend digital content with your physical space by turning it. In addition, it is untethered, except for the cable to the battery, which makes the headset more agile, compared to the Varjo XR-3. This could actually come closer to the "Ultimate Display",
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Forest Monkey&#65288;FM&#65289;&#24037;&#20855;&#21253;&#65292;&#23427;&#26159;&#19968;&#20010;&#29992;&#20110;&#25512;&#29702;&#20219;&#20309;&#22522;&#20110;AI&#30340;&#32570;&#38519;&#26816;&#27979;&#21644;/&#25110;&#20998;&#31867;&#27169;&#22411;&#36755;&#20986;&#32467;&#26524;&#30340;&#24037;&#20855;&#21253;&#12290;&#35813;&#24037;&#20855;&#21253;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#30340;&#25968;&#25454;&#21644;&#22270;&#34920;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#25512;&#29702;&#32467;&#26524;&#24182;&#25552;&#20986;&#25913;&#36827;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2307.13815</link><description>&lt;p&gt;
ForestMonkey&#65306;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#32570;&#38519;&#26816;&#27979;&#21644;&#20998;&#31867;&#27169;&#22411;&#25512;&#29702;&#24037;&#20855;&#21253;&#30340;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
ForestMonkey: Toolkit for Reasoning with AI-based Defect Detection and Classification Models. (arXiv:2307.13815v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Forest Monkey&#65288;FM&#65289;&#24037;&#20855;&#21253;&#65292;&#23427;&#26159;&#19968;&#20010;&#29992;&#20110;&#25512;&#29702;&#20219;&#20309;&#22522;&#20110;AI&#30340;&#32570;&#38519;&#26816;&#27979;&#21644;/&#25110;&#20998;&#31867;&#27169;&#22411;&#36755;&#20986;&#32467;&#26524;&#30340;&#24037;&#20855;&#21253;&#12290;&#35813;&#24037;&#20855;&#21253;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#30340;&#25968;&#25454;&#21644;&#22270;&#34920;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#25512;&#29702;&#32467;&#26524;&#24182;&#25552;&#20986;&#25913;&#36827;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25512;&#29702;&#21644;&#21487;&#35299;&#37322;AI&#65288;XAI&#65289;&#20219;&#21153;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#35299;&#37322;AI&#27169;&#22411;&#30340;&#39044;&#27979;&#25110;&#20915;&#31574;&#36807;&#31243;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Forest Monkey&#65288;FM&#65289;&#24037;&#20855;&#21253;&#65292;&#23427;&#26159;&#20026;&#20102;&#25512;&#29702;&#20219;&#20309;&#22522;&#20110;AI&#30340;&#32570;&#38519;&#26816;&#27979;&#21644;/&#25110;&#20998;&#31867;&#27169;&#22411;&#30340;&#36755;&#20986;&#32780;&#35774;&#35745;&#30340;&#65292;&#24182;&#20855;&#22791;&#25968;&#25454;&#21487;&#35299;&#37322;&#24615;&#12290;&#20316;&#20026;&#19968;&#20010;Python&#36719;&#20214;&#21253;&#23454;&#29616;&#65292;FM&#20197;&#25968;&#25454;&#38598;&#25991;&#20214;&#22841;&#36335;&#24452;&#30340;&#24418;&#24335;&#20316;&#20026;&#36755;&#20837;&#65288;&#21253;&#25324;&#21407;&#22987;&#22270;&#20687;&#12289;&#30495;&#23454;&#26631;&#31614;&#21644;&#39044;&#27979;&#26631;&#31614;&#65289;&#65292;&#24182;&#25552;&#20379;&#19968;&#32452;&#22270;&#34920;&#21644;&#25991;&#26412;&#25991;&#20214;&#20197;&#35828;&#26126;&#25512;&#29702;&#32467;&#26524;&#65292;&#24182;&#25552;&#20986;&#21487;&#33021;&#30340;&#25913;&#36827;&#12290;FM&#24037;&#20855;&#21253;&#21253;&#25324;&#20174;&#39044;&#27979;&#25552;&#21462;&#29305;&#24449;&#21040;&#25512;&#29702;&#30446;&#26631;&#12289;&#20174;&#22270;&#20687;&#25552;&#21462;&#29305;&#24449;&#21040;&#32570;&#38519;&#29305;&#24449;&#20197;&#21450;&#22522;&#20110;&#20915;&#31574;&#26641;&#30340;AI&#25512;&#29702;&#22120;&#31561;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#30740;&#31350;&#20102;FM&#24037;&#20855;&#21253;&#22312;&#24212;&#29992;&#20110;&#22235;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;AI&#27169;&#22411;&#26102;&#30340;&#26102;&#38388;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#25945;&#31243;&#26469;&#25351;&#23548;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) reasoning and explainable AI (XAI) tasks have gained popularity recently, enabling users to explain the predictions or decision processes of AI models. This paper introduces Forest Monkey (FM), a toolkit designed to reason the outputs of any AI-based defect detection and/or classification model with data explainability. Implemented as a Python package, FM takes input in the form of dataset folder paths (including original images, ground truth labels, and predicted labels) and provides a set of charts and a text file to illustrate the reasoning results and suggest possible improvements. The FM toolkit consists of processes such as feature extraction from predictions to reasoning targets, feature extraction from images to defect characteristics, and a decision tree-based AI-Reasoner. Additionally, this paper investigates the time performance of the FM toolkit when applied to four AI models with different datasets. Lastly, a tutorial is provided to guide users
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#36719;&#27880;&#24847;&#21147;&#12289;&#30828;&#27880;&#24847;&#21147;&#21644;&#28508;&#21464;&#37327;&#36793;&#38469;&#20284;&#28982;&#65288;LVML&#65289;&#27880;&#24847;&#21147;&#19977;&#31181;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#23398;&#20064;&#21160;&#24577;&#65292;&#21457;&#29616;&#20102;&#23427;&#20204;&#22312;&#25152;&#36873;&#25321;&#30340;&#29255;&#27573;&#32858;&#21512;&#26041;&#24335;&#19978;&#30340;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#35299;&#37322;&#20102;&#20998;&#31867;&#27169;&#22411;&#22312;&#26799;&#24230;&#19979;&#38477;&#19979;&#30340;&#28436;&#21270;&#23545;&#26368;&#32456;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.13421</link><description>&lt;p&gt;
&#20851;&#20110;&#27880;&#24847;&#21147;&#32593;&#32476;&#23398;&#20064;&#21160;&#24577;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the learning Dynamics of Attention Networks. (arXiv:2307.13421v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#36719;&#27880;&#24847;&#21147;&#12289;&#30828;&#27880;&#24847;&#21147;&#21644;&#28508;&#21464;&#37327;&#36793;&#38469;&#20284;&#28982;&#65288;LVML&#65289;&#27880;&#24847;&#21147;&#19977;&#31181;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#23398;&#20064;&#21160;&#24577;&#65292;&#21457;&#29616;&#20102;&#23427;&#20204;&#22312;&#25152;&#36873;&#25321;&#30340;&#29255;&#27573;&#32858;&#21512;&#26041;&#24335;&#19978;&#30340;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#35299;&#37322;&#20102;&#20998;&#31867;&#27169;&#22411;&#22312;&#26799;&#24230;&#19979;&#38477;&#19979;&#30340;&#28436;&#21270;&#23545;&#26368;&#32456;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#27169;&#22411;&#36890;&#24120;&#36890;&#36807;&#20248;&#21270;&#19977;&#20010;&#26631;&#20934;&#25439;&#22833;&#20989;&#25968;&#20043;&#19968;&#26469;&#23398;&#20064;&#65292;&#20998;&#21035;&#31216;&#20026;&#36719;&#27880;&#24847;&#21147;&#12289;&#30828;&#27880;&#24847;&#21147;&#21644;&#28508;&#21464;&#37327;&#36793;&#38469;&#20284;&#28982;&#65288;LVML&#65289;&#27880;&#24847;&#21147;&#12290;&#36825;&#19977;&#31181;&#33539;&#24335;&#37117;&#26159;&#20026;&#20102;&#36798;&#21040;&#30456;&#21516;&#30340;&#30446;&#26631;&#65292;&#21363;&#25214;&#21040;&#20004;&#20010;&#27169;&#22411;&#65306;&#19968;&#20010;&#8220;&#28966;&#28857;&#8221;&#27169;&#22411;&#65292;&#29992;&#20110;&#8220;&#36873;&#25321;&#8221;&#36755;&#20837;&#20013;&#30340;&#27491;&#30830;&#8220;&#29255;&#27573;&#8221;&#65292;&#21644;&#19968;&#20010;&#8220;&#20998;&#31867;&#8221;&#27169;&#22411;&#65292;&#29992;&#20110;&#23558;&#36873;&#23450;&#30340;&#29255;&#27573;&#22788;&#29702;&#25104;&#30446;&#26631;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#25152;&#36873;&#25321;&#30340;&#29255;&#27573;&#32858;&#21512;&#26041;&#24335;&#19978;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#23548;&#33268;&#20102;&#19981;&#21516;&#30340;&#21160;&#24577;&#21644;&#26368;&#32456;&#32467;&#26524;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#20351;&#29992;&#36825;&#20123;&#33539;&#24335;&#23398;&#20064;&#30340;&#27169;&#22411;&#20855;&#26377;&#29420;&#29305;&#30340;&#29305;&#24449;&#65292;&#24182;&#23558;&#20854;&#35299;&#37322;&#20026;&#22312;&#28966;&#28857;&#27169;&#22411;&#22266;&#23450;&#26102;&#65292;&#20998;&#31867;&#27169;&#22411;&#22312;&#26799;&#24230;&#19979;&#38477;&#19979;&#30340;&#28436;&#21270;&#25152;&#33268;&#12290;&#25105;&#20204;&#36824;&#22312;&#19968;&#20010;&#31616;&#21333;&#30340;&#35774;&#32622;&#20013;&#20998;&#26512;&#20102;&#36825;&#20123;&#33539;&#24335;&#65292;&#24182;&#25512;&#23548;&#20986;&#26799;&#24230;&#27969;&#19979;&#21442;&#25968;&#36712;&#36857;&#30340;&#38381;&#24335;&#34920;&#36798;&#24335;&#12290;&#22312;&#36719;&#27880;&#24847;&#21147;&#25439;&#22833;&#19979;&#65292;&#28966;&#28857;&#27169;&#22411;&#22312;&#21021;&#22987;&#21270;&#38454;&#27573;&#24555;&#36895;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention models are typically learned by optimizing one of three standard loss functions that are variously called -- soft attention, hard attention, and latent variable marginal likelihood (LVML) attention. All three paradigms are motivated by the same goal of finding two models -- a `focus' model that `selects' the right \textit{segment} of the input and a `classification' model that processes the selected segment into the target label. However, they differ significantly in the way the selected segments are aggregated, resulting in distinct dynamics and final results. We observe a unique signature of models learned using these paradigms and explain this as a consequence of the evolution of the classification model under gradient descent when the focus model is fixed. We also analyze these paradigms in a simple setting and derive closed-form expressions for the parameter trajectory under gradient flow. With the soft attention loss, the focus model improves quickly at initialization a
&lt;/p&gt;</description></item><item><title>TF-ICON&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#22270;&#20687;&#21512;&#25104;&#26694;&#26550;&#65292;&#21033;&#29992;&#25991;&#23383;&#39537;&#21160;&#30340;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#36328;&#39046;&#22495;&#22270;&#20687;&#23548;&#21521;&#21512;&#25104;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;TF-ICON&#21487;&#20197;&#22312;&#19981;&#38656;&#39069;&#22806;&#35757;&#32451;&#12289;&#24494;&#35843;&#25110;&#20248;&#21270;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#26080;&#32541;&#21512;&#25104;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#20363;&#22806;&#25552;&#31034;&#26469;&#20934;&#30830;&#22320;&#21453;&#36716;&#30495;&#23454;&#22270;&#20687;&#20026;&#28508;&#22312;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2307.12493</link><description>&lt;p&gt;
TF-ICON: &#22522;&#20110;&#25193;&#25955;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#36328;&#39046;&#22495;&#22270;&#20687;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
TF-ICON: Diffusion-Based Training-Free Cross-Domain Image Composition. (arXiv:2307.12493v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12493
&lt;/p&gt;
&lt;p&gt;
TF-ICON&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#22270;&#20687;&#21512;&#25104;&#26694;&#26550;&#65292;&#21033;&#29992;&#25991;&#23383;&#39537;&#21160;&#30340;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#36328;&#39046;&#22495;&#22270;&#20687;&#23548;&#21521;&#21512;&#25104;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;TF-ICON&#21487;&#20197;&#22312;&#19981;&#38656;&#39069;&#22806;&#35757;&#32451;&#12289;&#24494;&#35843;&#25110;&#20248;&#21270;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#26080;&#32541;&#21512;&#25104;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#20363;&#22806;&#25552;&#31034;&#26469;&#20934;&#30830;&#22320;&#21453;&#36716;&#30495;&#23454;&#22270;&#20687;&#20026;&#28508;&#22312;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#23383;&#39537;&#21160;&#30340;&#25193;&#25955;&#27169;&#22411;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#21487;&#20197;&#23454;&#29616;&#21508;&#31181;&#22270;&#20687;&#32534;&#36753;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TF-ICON&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#22270;&#20687;&#21512;&#25104;&#26694;&#26550;&#65292;&#21033;&#29992;&#25991;&#23383;&#39537;&#21160;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#36827;&#34892;&#36328;&#39046;&#22495;&#22270;&#20687;&#23548;&#21521;&#21512;&#25104;&#12290;&#35813;&#20219;&#21153;&#26088;&#22312;&#23558;&#29992;&#25143;&#25552;&#20379;&#30340;&#23545;&#35937;&#26080;&#32541;&#22320;&#25972;&#21512;&#21040;&#29305;&#23450;&#30340;&#35270;&#35273;&#29615;&#22659;&#20013;&#12290;&#30446;&#21069;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#36890;&#24120;&#28041;&#21450;&#26114;&#36149;&#30340;&#22522;&#20110;&#23454;&#20363;&#30340;&#20248;&#21270;&#25110;&#22312;&#23450;&#21046;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21487;&#33021;&#20250;&#25439;&#23475;&#20854;&#20016;&#23500;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#30456;&#21453;&#65292;TF-ICON&#21487;&#20197;&#21033;&#29992;&#29616;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#36328;&#39046;&#22495;&#22270;&#20687;&#23548;&#21521;&#21512;&#25104;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#12289;&#24494;&#35843;&#25110;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20363;&#22806;&#25552;&#31034;(&#21547;&#26080;&#20449;&#24687;)&#26469;&#24110;&#21161;&#25991;&#23383;&#39537;&#21160;&#30340;&#25193;&#25955;&#27169;&#22411;&#20934;&#30830;&#22320;&#23558;&#30495;&#23454;&#22270;&#20687;&#21453;&#36716;&#20026;&#28508;&#22312;&#34920;&#31034;&#65292;&#20026;&#21512;&#25104;&#25552;&#20379;&#22522;&#30784;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TF-ICON&#22312;&#19981;&#21516;&#30340;&#21512;&#25104;&#20219;&#21153;&#20013;&#20855;&#26377;&#20248;&#36234;&#30340;&#34920;&#29616;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#22270;&#20687;&#20043;&#38388;&#36827;&#34892;&#39640;&#36136;&#37327;&#30340;&#26080;&#32541;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-driven diffusion models have exhibited impressive generative capabilities, enabling various image editing tasks. In this paper, we propose TF-ICON, a novel Training-Free Image COmpositioN framework that harnesses the power of text-driven diffusion models for cross-domain image-guided composition. This task aims to seamlessly integrate user-provided objects into a specific visual context. Current diffusion-based methods often involve costly instance-based optimization or finetuning of pretrained models on customized datasets, which can potentially undermine their rich prior. In contrast, TF-ICON can leverage off-the-shelf diffusion models to perform cross-domain image-guided composition without requiring additional training, finetuning, or optimization. Moreover, we introduce the exceptional prompt, which contains no information, to facilitate text-driven diffusion models in accurately inverting real images into latent representations, forming the basis for compositing. Our experim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;AI&#25512;&#29702;&#22120;&#30340;&#35299;&#37322;&#24615;&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;&#22270;&#20687;&#20013;&#25552;&#21462;&#32570;&#38519;&#30340;&#24418;&#24577;&#23398;&#29305;&#24449;&#65292;&#24182;&#21033;&#29992;&#20915;&#31574;&#26641;&#36827;&#34892;&#25512;&#29702;&#12290;&#23427;&#36890;&#36807;&#21487;&#35270;&#21270;&#21644;&#25991;&#23383;&#35828;&#26126;&#35299;&#37322;&#22522;&#20110;&#25513;&#27169;&#30340;&#32570;&#38519;&#26816;&#27979;&#21644;&#20998;&#31867;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#24182;&#25552;&#20379;&#26377;&#25928;&#30340;&#32531;&#35299;&#31574;&#30053;&#20197;&#25552;&#21319;&#25972;&#20307;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.11643</link><description>&lt;p&gt;
&#22522;&#20110;&#24418;&#24577;&#23398;&#22270;&#20687;&#20998;&#26512;&#21644;&#29305;&#24449;&#25552;&#21462;&#30340;AI&#32570;&#38519;&#26816;&#27979;&#21644;&#20998;&#31867;&#27169;&#22411;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Morphological Image Analysis and Feature Extraction for Reasoning with AI-based Defect Detection and Classification Models. (arXiv:2307.11643v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11643
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;AI&#25512;&#29702;&#22120;&#30340;&#35299;&#37322;&#24615;&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;&#22270;&#20687;&#20013;&#25552;&#21462;&#32570;&#38519;&#30340;&#24418;&#24577;&#23398;&#29305;&#24449;&#65292;&#24182;&#21033;&#29992;&#20915;&#31574;&#26641;&#36827;&#34892;&#25512;&#29702;&#12290;&#23427;&#36890;&#36807;&#21487;&#35270;&#21270;&#21644;&#25991;&#23383;&#35828;&#26126;&#35299;&#37322;&#22522;&#20110;&#25513;&#27169;&#30340;&#32570;&#38519;&#26816;&#27979;&#21644;&#20998;&#31867;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#24182;&#25552;&#20379;&#26377;&#25928;&#30340;&#32531;&#35299;&#31574;&#30053;&#20197;&#25552;&#21319;&#25972;&#20307;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22312;&#24037;&#31243;&#21644;&#21046;&#36896;&#31561;&#34892;&#19994;&#30340;&#20351;&#29992;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#36825;&#20123;&#27169;&#22411;&#25552;&#20379;&#36879;&#26126;&#30340;&#25512;&#29702;&#20197;&#35299;&#37322;&#20854;&#39044;&#27979;&#32467;&#26524;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;AI&#25512;&#29702;&#22120;&#65292;&#35813;&#25512;&#29702;&#22120;&#20174;&#22270;&#20687;&#20013;&#25552;&#21462;&#32570;&#38519;&#30340;&#24418;&#24577;&#23398;&#29305;&#24449;&#65292;&#24182;&#21033;&#29992;&#20915;&#31574;&#26641;&#23545;&#32570;&#38519;&#29305;&#24449;&#36827;&#34892;&#25512;&#29702;&#12290;&#28982;&#21518;&#65292;AI&#25512;&#29702;&#22120;&#36890;&#36807;&#21487;&#35270;&#21270;&#22270;&#34920;&#21644;&#25991;&#23383;&#35828;&#26126;&#25552;&#20379;&#23545;&#22522;&#20110;&#25513;&#27169;&#30340;&#32570;&#38519;&#26816;&#27979;&#21644;&#20998;&#31867;&#27169;&#22411;&#30340;&#36755;&#20986;&#36827;&#34892;&#35299;&#37322;&#12290;&#23427;&#36824;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#32531;&#35299;&#31574;&#30053;&#65292;&#20197;&#25552;&#21319;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#25972;&#20307;&#27169;&#22411;&#24615;&#33021;&#12290;AI&#25512;&#29702;&#22120;&#22312;&#20351;&#29992;366&#24352;&#21547;&#26377;&#32570;&#38519;&#30340;&#22270;&#20687;&#38598;&#21512;&#19978;&#27979;&#35797;&#20102;&#35299;&#37322;IE Mask R-CNN&#27169;&#22411;&#36755;&#20986;&#30340;&#33021;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;AI&#25512;&#29702;&#22120;&#22312;&#35299;&#37322;IE Mask R-CNN&#27169;&#22411;&#30340;&#39044;&#27979;&#26041;&#38754;&#20855;&#26377;&#24456;&#39640;&#30340;&#25928;&#26524;&#12290;&#24635;&#32780;&#35328;&#20043;&#65292;&#25152;&#25552;&#20986;&#30340;AI&#25512;&#29702;&#22120;&#20026;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#25552;&#20379;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the use of artificial intelligent (AI) models becomes more prevalent in industries such as engineering and manufacturing, it is essential that these models provide transparent reasoning behind their predictions. This paper proposes the AI-Reasoner, which extracts the morphological characteristics of defects (DefChars) from images and utilises decision trees to reason with the DefChar values. Thereafter, the AI-Reasoner exports visualisations (i.e. charts) and textual explanations to provide insights into outputs made by masked-based defect detection and classification models. It also provides effective mitigation strategies to enhance data pre-processing and overall model performance. The AI-Reasoner was tested on explaining the outputs of an IE Mask R-CNN model using a set of 366 images containing defects. The results demonstrated its effectiveness in explaining the IE Mask R-CNN model's predictions. Overall, the proposed AI-Reasoner provides a solution for improving the performanc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#19981;&#30830;&#23450;&#24615;&#30340;&#35282;&#24230;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#22312;&#25506;&#32034;&#21644;&#25269;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#33391;&#34892;&#20026;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.10236</link><description>&lt;p&gt;
&#19977;&#24605;&#32780;&#21518;&#34892;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#27979;&#37327;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Look Before You Leap: An Exploratory Study of Uncertainty Measurement for Large Language Models. (arXiv:2307.10236v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#19981;&#30830;&#23450;&#24615;&#30340;&#35282;&#24230;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#22312;&#25506;&#32034;&#21644;&#25269;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#33391;&#34892;&#20026;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#36817;&#24615;&#33021;&#31361;&#30772;&#20026;&#20247;&#22810;&#24037;&#19994;&#24212;&#29992;&#21644;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#36935;&#12290;&#28982;&#32780;&#65292;LLMs&#30340;&#38169;&#35823;&#29983;&#25104;&#65292;&#22914;&#34394;&#20551;&#39044;&#27979;&#12289;&#38169;&#35823;&#20449;&#24687;&#21644;&#24187;&#35273;&#65292;&#20063;&#24341;&#21457;&#20102;&#23545;LLMs&#21487;&#38752;&#24615;&#30340;&#20005;&#37325;&#20851;&#27880;&#65292;&#23588;&#20854;&#22312;&#23545;&#23433;&#20840;&#12289;&#21487;&#38752;&#24615;&#26377;&#25935;&#24863;&#30340;&#22330;&#26223;&#20013;&#65292;&#21487;&#33021;&#38459;&#30861;&#20854;&#22312;&#23454;&#38469;&#20013;&#30340;&#24212;&#29992;&#12290;&#23613;&#31649;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24050;&#32463;&#26174;&#31034;&#20986;&#20854;&#22312;&#35299;&#37322;&#19968;&#33324;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#30340;&#39044;&#27979;&#39118;&#38505;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20294;&#20851;&#20110;&#23427;&#26159;&#21542;&#20197;&#21450;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#26377;&#21161;&#20110;&#25506;&#32034;LLMs&#30340;&#33021;&#21147;&#21644;&#25269;&#21046;&#20854;&#19981;&#33391;&#34892;&#20026;&#26041;&#38754;&#30693;&#20043;&#29978;&#23569;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#20174;&#19981;&#30830;&#23450;&#24615;&#30340;&#35282;&#24230;&#24320;&#23637;&#20102;&#20851;&#20110;LLMs&#39118;&#38505;&#35780;&#20272;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;12&#31181;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#21644;4&#20010;LLMs&#22312;4&#20010;&#37325;&#35201;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#20197;&#35843;&#26597;&#19981;&#30830;&#23450;&#24615;&#22312;&#25506;&#32034;LLMs&#33021;&#21147;&#21644;&#23545;&#25239;&#20854;&#19981;&#33391;&#34892;&#20026;&#26041;&#38754;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent performance leap of Large Language Models (LLMs) opens up new opportunities across numerous industrial applications and domains. However, erroneous generations, such as false predictions, misinformation, and hallucination made by LLMs, have also raised severe concerns for the trustworthiness of LLMs', especially in safety-, security- and reliability-sensitive scenarios, potentially hindering real-world adoptions. While uncertainty estimation has shown its potential for interpreting the prediction risks made by general machine learning (ML) models, little is known about whether and to what extent it can help explore an LLM's capabilities and counteract its undesired behavior. To bridge the gap, in this paper, we initiate an exploratory study on the risk assessment of LLMs from the lens of uncertainty. In particular, we experiment with twelve uncertainty estimation methods and four LLMs on four prominent natural language processing (NLP) tasks to investigate to what extent unc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#36793;&#38469;&#20272;&#20540;&#23545;&#22269;&#38469;&#35937;&#26827;&#26827;&#30424;&#19978;&#30340;&#26827;&#23376;&#21644;&#26827;&#30424;&#36827;&#34892;&#35780;&#20215;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#39532;&#12289;&#35937;&#21644;&#20853;&#30340;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.05330</link><description>&lt;p&gt;
&#26827;&#30424;&#19978;&#30340;&#26827;&#23376;&#20215;&#20540;&#12290; (arXiv:2307.05330v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
The Value of Chess Squares. (arXiv:2307.05330v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#36793;&#38469;&#20272;&#20540;&#23545;&#22269;&#38469;&#35937;&#26827;&#26827;&#30424;&#19978;&#30340;&#26827;&#23376;&#21644;&#26827;&#30424;&#36827;&#34892;&#35780;&#20215;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#39532;&#12289;&#35937;&#21644;&#20853;&#30340;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#35780;&#20272;&#26827;&#30424;&#19978;&#26827;&#23376;&#30340;&#20215;&#20540;&#65292;&#24182;&#30830;&#23450;&#26827;&#23376;&#22312;&#26827;&#30424;&#19978;&#30340;&#25670;&#25918;&#20301;&#32622;&#12290;&#38543;&#30528;&#22269;&#38469;&#35937;&#26827;&#20154;&#24037;&#26234;&#33021;&#30340;&#20986;&#29616;&#65292;&#25105;&#20204;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#22269;&#38469;&#35937;&#26827;&#23616;&#38754;&#30340;&#20215;&#20540;&#12290;&#20256;&#32479;&#26041;&#27861;&#23545;&#26827;&#23376;&#36171;&#20104;&#22266;&#23450;&#30340;&#20215;&#20540;$(\symking=\infty, \symqueen=9, \symrook=5, \symbishop=3, \symknight=3, \sympawn=1)$&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#26827;&#23376;&#21644;&#26827;&#30424;&#26041;&#38754;&#30340;&#36793;&#38469;&#20272;&#20540;&#26469;&#25913;&#36827;&#36825;&#31181;&#20998;&#26512;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#39532;&#21644;&#35937;&#30340;&#20301;&#32622;&#65292;&#24182;&#25552;&#20379;&#26377;&#20851;&#20853;&#30340;&#20215;&#20540;&#30340;&#23453;&#36149;&#35265;&#35299;&#26469;&#28436;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23612;&#22982;&#20304;&#32500;&#22855;&#26159;&#20513;&#23548;&#20853;&#30340;&#32467;&#26500;&#21644;&#20215;&#20540;&#30340;&#20808;&#39537;&#20043;&#19968;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Valuing chess squares and determining the placement of pieces on the board are the main objectives of our study. With the emergence of chess AI, it has become possible to accurately assess the worth of positions in a game of chess. The conventional approach assigns fixed values to pieces $(\symking=\infty, \symqueen=9, \symrook=5, \symbishop=3, \symknight=3, \sympawn=1)$. We enhance this analysis by introducing marginal valuations for both pieces and squares. We demonstrate our method by examining the positioning of Knights and Bishops, and also provide valuable insights into the valuation of pawns. Notably, Nimzowitsch was among the pioneers in advocating for the significance of Pawn structure and valuation. Finally, we conclude by suggesting potential avenues for future research.
&lt;/p&gt;</description></item><item><title>ELM&#31070;&#32463;&#20803;&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#34920;&#36798;&#21147;&#24378;&#30340;&#30382;&#23618;&#31070;&#32463;&#20803;&#27169;&#22411;&#65292;&#23427;&#21482;&#38656;&#35201;8K&#20010;&#21442;&#25968;&#23601;&#33021;&#20934;&#30830;&#27169;&#25311;&#22797;&#26434;&#30340;&#35745;&#31639;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.16922</link><description>&lt;p&gt;
ELM&#31070;&#32463;&#20803;&#65306;&#19968;&#31181;&#39640;&#25928;&#19988;&#34920;&#36798;&#21147;&#24378;&#30340;&#30382;&#23618;&#31070;&#32463;&#20803;&#27169;&#22411;&#21487;&#20197;&#35299;&#20915;&#38271;&#26102;&#38388;&#36328;&#24230;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
The ELM Neuron: an Efficient and Expressive Cortical Neuron Model Can Solve Long-Horizon Tasks. (arXiv:2306.16922v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16922
&lt;/p&gt;
&lt;p&gt;
ELM&#31070;&#32463;&#20803;&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#34920;&#36798;&#21147;&#24378;&#30340;&#30382;&#23618;&#31070;&#32463;&#20803;&#27169;&#22411;&#65292;&#23427;&#21482;&#38656;&#35201;8K&#20010;&#21442;&#25968;&#23601;&#33021;&#20934;&#30830;&#27169;&#25311;&#22797;&#26434;&#30340;&#35745;&#31639;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22823;&#35268;&#27169;&#31070;&#32463;&#31185;&#23398;&#27169;&#22411;&#21644;&#26426;&#22120;&#23398;&#20064;&#21033;&#29992;&#31616;&#21270;&#30340;&#20010;&#20307;&#31070;&#32463;&#20803;&#27169;&#22411;&#65292;&#20381;&#38752;&#38598;&#20307;&#27963;&#21160;&#21644;&#36866;&#24403;&#35843;&#25972;&#30340;&#36830;&#25509;&#26469;&#25191;&#34892;&#22797;&#26434;&#30340;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#27599;&#20010;&#29983;&#29289;&#30382;&#23618;&#31070;&#32463;&#20803;&#26412;&#36136;&#19978;&#37117;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#35745;&#31639;&#35774;&#22791;&#65292;&#26368;&#36817;&#30340;&#19968;&#39033;&#30740;&#31350;&#35777;&#23454;&#20102;&#36825;&#19968;&#28857;&#65292;&#35813;&#30740;&#31350;&#20013;&#65292;&#38656;&#35201;&#19968;&#20010;&#20855;&#26377;&#25968;&#30334;&#19975;&#20010;&#21442;&#25968;&#30340;&#28145;&#24230;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26469;&#22797;&#21046;&#35814;&#32454;&#29983;&#29289;&#29289;&#29702;&#27169;&#22411;&#30340;&#36755;&#20837;&#36755;&#20986;&#20851;&#31995;&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#22810;&#20010;&#21442;&#25968;&#30340;&#24517;&#35201;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#65292;&#24182;&#24341;&#20837;&#20102;&#34920;&#36798;&#21147;&#24378;&#30340;&#27844;&#28431;&#23384;&#20648;&#22120;&#65288;ELM&#65289;&#31070;&#32463;&#20803;&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#29983;&#29289;&#21551;&#21457;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#20855;&#26377;&#39640;&#35745;&#31639;&#34920;&#36798;&#21147;&#65292;&#21516;&#26102;&#20063;&#38750;&#24120;&#39640;&#25928;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;ELM&#31070;&#32463;&#20803;&#20165;&#38656;&#35201;8,000&#20010;&#21487;&#35757;&#32451;&#21442;&#25968;&#23601;&#33021;&#20934;&#30830;&#21305;&#37197;&#21069;&#36848;&#30340;&#36755;&#20837;&#36755;&#20986;&#20851;&#31995;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20934;&#30830;&#30340;&#27169;&#22411;&#38656;&#35201;&#22810;&#20010;&#31867;&#20284;&#20110;&#23384;&#20648;&#22120;&#30340;&#38544;&#34255;&#29366;&#24577;&#21644;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#31361;&#35302;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional large-scale neuroscience models and machine learning utilize simplified models of individual neurons, relying on collective activity and properly adjusted connections to perform complex computations. However, each biological cortical neuron is inherently a sophisticated computational device, as corroborated in a recent study where it took a deep artificial neural network with millions of parameters to replicate the input-output relationship of a detailed biophysical model of a cortical pyramidal neuron. We question the necessity for these many parameters and introduce the Expressive Leaky Memory (ELM) neuron, a biologically inspired, computationally expressive, yet efficient model of a cortical neuron. Remarkably, our ELM neuron requires only 8K trainable parameters to match the aforementioned input-output relationship accurately. We find that an accurate model necessitates multiple memory-like hidden states and intricate nonlinear synaptic integration. To assess the comput
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#22810;&#20010;&#32463;&#36807;&#36845;&#20195;&#24133;&#24230;&#21098;&#26525;&#30340;&#27169;&#22411;&#36827;&#34892;&#24179;&#22343;&#65292;&#35299;&#20915;&#20102;&#21516;&#26102;&#21033;&#29992;&#31232;&#30095;&#24615;&#21644;&#21442;&#25968;&#24179;&#22343;&#30340;&#38382;&#39064;&#65292;&#24182;&#26174;&#33879;&#25552;&#21319;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.16788</link><description>&lt;p&gt;
&#31232;&#30095;&#27169;&#22411;&#27748;&#65306;&#36890;&#36807;&#27169;&#22411;&#24179;&#22343;&#25913;&#36827;&#20462;&#21098;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging. (arXiv:2306.16788v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#22810;&#20010;&#32463;&#36807;&#36845;&#20195;&#24133;&#24230;&#21098;&#26525;&#30340;&#27169;&#22411;&#36827;&#34892;&#24179;&#22343;&#65292;&#35299;&#20915;&#20102;&#21516;&#26102;&#21033;&#29992;&#31232;&#30095;&#24615;&#21644;&#21442;&#25968;&#24179;&#22343;&#30340;&#38382;&#39064;&#65292;&#24182;&#26174;&#33879;&#25552;&#21319;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#36890;&#36807;&#21098;&#26525;&#26174;&#33879;&#21387;&#32553;&#65292;&#20174;&#32780;&#24471;&#21040;&#31232;&#30095;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#26356;&#23569;&#30340;&#23384;&#20648;&#21644;&#28014;&#28857;&#36816;&#31639;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#27979;&#24615;&#33021;&#12290;&#27169;&#22411;&#27748;&#65288;Wortsman&#31561;&#20154;&#65292;2022&#24180;&#65289;&#36890;&#36807;&#23558;&#22810;&#20010;&#27169;&#22411;&#30340;&#21442;&#25968;&#24179;&#22343;&#25104;&#19968;&#20010;&#21333;&#19968;&#27169;&#22411;&#26469;&#25913;&#21892;&#27867;&#21270;&#21644;&#36229;&#20986;&#20998;&#24067;&#24615;&#33021;&#65292;&#32780;&#19981;&#22686;&#21152;&#25512;&#29702;&#26102;&#38388;&#12290;&#28982;&#32780;&#65292;&#35782;&#21035;&#22788;&#20110;&#30456;&#21516;&#25439;&#22833;&#21306;&#22495;&#30340;&#27169;&#22411;&#20197;&#21516;&#26102;&#21033;&#29992;&#31232;&#30095;&#24615;&#21644;&#21442;&#25968;&#24179;&#22343;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23545;&#20219;&#24847;&#31232;&#30095;&#27169;&#22411;&#36827;&#34892;&#24179;&#22343;&#20250;&#38477;&#20302;&#25972;&#20307;&#31232;&#30095;&#24230;&#65292;&#21407;&#22240;&#26159;&#19981;&#21516;&#30340;&#31232;&#30095;&#36830;&#25509;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#22312;&#36845;&#20195;&#24133;&#24230;&#21098;&#26525;&#65288;IMP&#65289;&#30340;&#21333;&#27425;&#37325;&#26032;&#35757;&#32451;&#38454;&#27573;&#20013;&#25506;&#32034;&#19981;&#21516;&#30340;&#36229;&#21442;&#25968;&#37197;&#32622;&#65288;&#20363;&#22914;&#25209;&#27425;&#25490;&#24207;&#25110;&#26435;&#37325;&#34928;&#20943;&#65289;&#20135;&#29983;&#30340;&#27169;&#22411;&#36866;&#21512;&#36827;&#34892;&#24179;&#22343;&#65292;&#24182;&#19988;&#36890;&#36807;&#35774;&#35745;&#20849;&#20139;&#30456;&#21516;&#30340;&#31232;&#30095;&#36830;&#25509;&#24615;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#24179;&#22343;&#36825;&#20123;&#27169;&#22411;&#26174;&#33879;&#25552;&#21319;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks can be significantly compressed by pruning, leading to sparse models requiring considerably less storage and floating-point operations while maintaining predictive performance. Model soups (Wortsman et al., 2022) improve generalization and out-of-distribution performance by averaging the parameters of multiple models into a single one without increased inference time. However, identifying models in the same loss basin to leverage both sparsity and parameter averaging is challenging, as averaging arbitrary sparse models reduces the overall sparsity due to differing sparse connectivities. In this work, we address these challenges by demonstrating that exploring a single retraining phase of Iterative Magnitude Pruning (IMP) with varying hyperparameter configurations, such as batch ordering or weight decay, produces models that are suitable for averaging and share the same sparse connectivity by design. Averaging these models significantly enhances generalization performanc
&lt;/p&gt;</description></item><item><title>Tensorformer&#26159;&#19968;&#31181;&#24402;&#19968;&#21270;&#30697;&#38453;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;&#65292;&#29992;&#20110;&#39640;&#36136;&#37327;&#30340;&#28857;&#20113;&#37325;&#24314;&#12290;&#23427;&#36890;&#36807;&#30697;&#38453;&#27880;&#24847;&#21147;&#23454;&#29616;&#20102;&#36880;&#28857;&#21644;&#36880;&#36890;&#36947;&#30340;&#28040;&#24687;&#20256;&#36882;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#23616;&#37096;&#20960;&#20309;&#24314;&#27169;&#33021;&#21147;&#65292;&#24182;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.15989</link><description>&lt;p&gt;
Tensorformer: &#39640;&#36136;&#37327;&#28857;&#20113;&#37325;&#24314;&#30340;&#24402;&#19968;&#21270;&#30697;&#38453;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Tensorformer: Normalized Matrix Attention Transformer for High-quality Point Cloud Reconstruction. (arXiv:2306.15989v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15989
&lt;/p&gt;
&lt;p&gt;
Tensorformer&#26159;&#19968;&#31181;&#24402;&#19968;&#21270;&#30697;&#38453;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;&#65292;&#29992;&#20110;&#39640;&#36136;&#37327;&#30340;&#28857;&#20113;&#37325;&#24314;&#12290;&#23427;&#36890;&#36807;&#30697;&#38453;&#27880;&#24847;&#21147;&#23454;&#29616;&#20102;&#36880;&#28857;&#21644;&#36880;&#36890;&#36947;&#30340;&#28040;&#24687;&#20256;&#36882;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#23616;&#37096;&#20960;&#20309;&#24314;&#27169;&#33021;&#21147;&#65292;&#24182;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#30028;&#65292;&#20174;&#21407;&#22987;&#28857;&#20113;&#36827;&#34892;&#34920;&#38754;&#37325;&#24314;&#30340;&#30740;&#31350;&#24050;&#32463;&#36827;&#34892;&#20102;&#20960;&#21313;&#24180;&#65292;&#36825;&#22312;&#29616;&#20170;&#30340;&#24314;&#27169;&#21644;&#28210;&#26579;&#24212;&#29992;&#20013;&#38656;&#27714;&#38750;&#24120;&#39640;&#12290;&#20256;&#32479;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22914;Poisson&#34920;&#38754;&#37325;&#24314;&#65292;&#38656;&#35201;&#39069;&#22806;&#30340;&#28857;&#27861;&#32447;&#36755;&#20837;&#20197;&#20135;&#29983;&#21512;&#29702;&#30340;&#32467;&#26524;&#12290;&#29616;&#20195;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#27809;&#26377;&#27861;&#32447;&#30340;&#24773;&#20917;&#19979;&#24037;&#20316;&#65292;&#20294;&#30001;&#20110;&#31163;&#25955;&#28857;&#30340;&#23616;&#37096;&#34701;&#21512;&#32534;&#30721;&#24615;&#33021;&#26377;&#38480;&#65292;&#32467;&#26524;&#36739;&#20026;&#31895;&#31961;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24402;&#19968;&#21270;&#30697;&#38453;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;&#65288;Tensorformer&#65289;&#26469;&#36827;&#34892;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#12290;&#25152;&#25552;&#20986;&#30340;&#30697;&#38453;&#27880;&#24847;&#21147;&#20801;&#35768;&#21516;&#26102;&#36827;&#34892;&#36880;&#28857;&#21644;&#36880;&#36890;&#36947;&#30340;&#28040;&#24687;&#20256;&#36882;&#65292;&#32780;&#20043;&#21069;&#30340;&#21521;&#37327;&#27880;&#24847;&#21147;&#22312;&#19981;&#21516;&#36890;&#36947;&#20043;&#38388;&#20002;&#22833;&#20102;&#30456;&#37051;&#28857;&#30340;&#20449;&#24687;&#12290;&#23427;&#22312;&#29305;&#24449;&#23398;&#20064;&#20013;&#24102;&#26469;&#26356;&#22810;&#33258;&#30001;&#24230;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#24314;&#27169;&#23616;&#37096;&#20960;&#20309;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20004;&#20010;&#24120;&#29992;&#25968;&#25454;&#38598;ShapeNetCore&#21644;ABC&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#65292;&#24182;&#19988;
&lt;/p&gt;
&lt;p&gt;
Surface reconstruction from raw point clouds has been studied for decades in the computer graphics community, which is highly demanded by modeling and rendering applications nowadays. Classic solutions, such as Poisson surface reconstruction, require point normals as extra input to perform reasonable results. Modern transformer-based methods can work without normals, while the results are less fine-grained due to limited encoding performance in local fusion from discrete points. We introduce a novel normalized matrix attention transformer (Tensorformer) to perform high-quality reconstruction. The proposed matrix attention allows for simultaneous point-wise and channel-wise message passing, while the previous vector attention loses neighbor point information across different channels. It brings more degree of freedom in feature learning and thus facilitates better modeling of local geometries. Our method achieves state-of-the-art on two commonly used datasets, ShapeNetCore and ABC, and 
&lt;/p&gt;</description></item><item><title>RL$^3$&#26159;&#19968;&#31181;&#21407;&#21017;&#24615;&#28151;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#23398;&#21040;&#30340;&#20219;&#21153;&#29305;&#23450;&#21160;&#20316;&#20540;&#20316;&#20026;&#20803;&#24378;&#21270;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#65292;&#25552;&#39640;&#20102;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.15909</link><description>&lt;p&gt;
RL$^3$:&#36890;&#36807;RL&#20869;&#37096;&#30340;RL$^2$&#25552;&#21319;&#20803;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RL$^3$: Boosting Meta Reinforcement Learning via RL inside RL$^2$. (arXiv:2306.15909v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15909
&lt;/p&gt;
&lt;p&gt;
RL$^3$&#26159;&#19968;&#31181;&#21407;&#21017;&#24615;&#28151;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#23398;&#21040;&#30340;&#20219;&#21153;&#29305;&#23450;&#21160;&#20316;&#20540;&#20316;&#20026;&#20803;&#24378;&#21270;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#65292;&#25552;&#39640;&#20102;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#24378;&#21270;&#23398;&#20064;&#65288;meta-RL&#65289;&#26041;&#27861;&#65292;&#22914;RL$^2$&#65292;&#24050;&#32463;&#25104;&#20026;&#23398;&#20064;&#38024;&#23545;&#32473;&#23450;&#20219;&#21153;&#20998;&#24067;&#30340;&#25968;&#25454;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#38271;&#26399;&#20219;&#21153;&#21644;&#36229;&#20986;&#20998;&#24067;&#20219;&#21153;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#20110;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26469;&#22788;&#29702;&#32463;&#39564;&#24207;&#21015;&#65292;&#32780;&#19981;&#26159;&#23558;&#23427;&#20204;&#24635;&#32467;&#20026;&#19968;&#33324;&#30340;&#24378;&#21270;&#23398;&#20064;&#32452;&#20214;&#65292;&#20363;&#22914;&#20215;&#20540;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#26159;transformers&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#25104;&#26412;&#21464;&#24471;&#31105;&#27490;&#20043;&#21069;&#20063;&#23545;&#23427;&#20204;&#21487;&#20197;&#26377;&#25928;&#25512;&#29702;&#30340;&#21382;&#21490;&#38271;&#24230;&#26377;&#23454;&#38469;&#38480;&#21046;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#25968;&#25454;&#25928;&#29575;&#26041;&#38754;&#19981;&#36275;&#65292;&#22240;&#20026;&#23427;&#20204;&#27809;&#26377;&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;&#65292;&#20294;&#38543;&#30528;&#26356;&#22810;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#23427;&#20204;&#20250;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RL$^3$&#65292;&#19968;&#31181;&#32452;&#21512;&#20102;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#21644;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#21407;&#21017;&#24615;&#28151;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36890;&#36807;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#21040;&#30340;&#29305;&#23450;&#20219;&#21153;&#21160;&#20316;&#20540;&#20316;&#20026;&#20803;&#24378;&#21270;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#20010;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta reinforcement learning (meta-RL) methods such as RL$^2$ have emerged as promising approaches for learning data-efficient RL algorithms tailored to a given task distribution. However, these RL algorithms struggle with long-horizon tasks and out-of-distribution tasks since they rely on recurrent neural networks to process the sequence of experiences instead of summarizing them into general RL components such as value functions. Moreover, even transformers have a practical limit to the length of histories they can efficiently reason about before training and inference costs become prohibitive. In contrast, traditional RL algorithms are data-inefficient since they do not leverage domain knowledge, but they do converge to an optimal policy as more data becomes available. In this paper, we propose RL$^3$, a principled hybrid approach that combines traditional RL and meta-RL by incorporating task-specific action-values learned through traditional RL as an input to the meta-RL neural netw
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#28798;&#38590;&#24615;&#39118;&#38505;&#30340;&#22235;&#20010;&#20027;&#35201;&#26469;&#28304;&#65292;&#21253;&#25324;&#24694;&#24847;&#20351;&#29992;&#12289;&#20154;&#24037;&#26234;&#33021;&#31454;&#36187;&#12289;&#32452;&#32455;&#39118;&#38505;&#21644;&#27969;&#27667;&#20154;&#24037;&#26234;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.12001</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#28798;&#38590;&#24615;&#39118;&#38505;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
An Overview of Catastrophic AI Risks. (arXiv:2306.12001v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#28798;&#38590;&#24615;&#39118;&#38505;&#30340;&#22235;&#20010;&#20027;&#35201;&#26469;&#28304;&#65292;&#21253;&#25324;&#24694;&#24847;&#20351;&#29992;&#12289;&#20154;&#24037;&#26234;&#33021;&#31454;&#36187;&#12289;&#32452;&#32455;&#39118;&#38505;&#21644;&#27969;&#27667;&#20154;&#24037;&#26234;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#21457;&#23637;&#24341;&#36215;&#20102;&#19987;&#23478;&#12289;&#25919;&#31574;&#21046;&#23450;&#32773;&#21644;&#19990;&#30028;&#21508;&#22269;&#39046;&#23548;&#20154;&#23545;&#36234;&#26469;&#36234;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21487;&#33021;&#24102;&#26469;&#28798;&#38590;&#24615;&#39118;&#38505;&#30340;&#25285;&#24551;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#24456;&#22810;&#39118;&#38505;&#34987;&#21333;&#29420;&#35814;&#32454;&#20171;&#32461;&#36807;&#65292;&#20294;&#36843;&#20999;&#38656;&#35201;&#31995;&#32479;&#22320;&#35752;&#35770;&#21644;&#35828;&#26126;&#28508;&#22312;&#21361;&#38505;&#65292;&#20197;&#26356;&#22909;&#22320;&#25903;&#25345;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#30340;&#21162;&#21147;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#28798;&#38590;&#24615;&#39118;&#38505;&#30340;&#20027;&#35201;&#26469;&#28304;&#65292;&#25105;&#20204;&#23558;&#20854;&#20998;&#20026;&#22235;&#20010;&#31867;&#21035;&#65306;&#24694;&#24847;&#20351;&#29992;&#65292;&#21363;&#20010;&#20154;&#25110;&#22242;&#20307;&#26377;&#24847;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#36896;&#25104;&#20260;&#23475;&#65307;&#20154;&#24037;&#26234;&#33021;&#31454;&#36187;&#65292;&#21363;&#31454;&#20105;&#29615;&#22659;&#20419;&#20351;&#34892;&#21160;&#32773;&#37096;&#32626;&#19981;&#23433;&#20840;&#30340;&#20154;&#24037;&#26234;&#33021;&#25110;&#25918;&#24323;&#25511;&#21046;&#26435;&#20132;&#32473;&#20154;&#24037;&#26234;&#33021;&#65307;&#32452;&#32455;&#39118;&#38505;&#65292;&#31361;&#20986;&#20154;&#20026;&#21644;&#22797;&#26434;&#31995;&#32479;&#22914;&#20309;&#22686;&#21152;&#28798;&#38590;&#24615;&#20107;&#25925;&#21457;&#29983;&#30340;&#21487;&#33021;&#24615;&#65307;&#20197;&#21450;&#27969;&#27667;&#20154;&#24037;&#26234;&#33021;&#65292;&#25551;&#36848;&#20102;&#25511;&#21046;&#27604;&#20154;&#31867;&#26234;&#33021;&#26356;&#39640;&#30340;&#20195;&#29702;&#31243;&#24207;&#22256;&#38590;&#30340;&#22266;&#26377;&#38590;&#39064;&#12290;&#23545;&#20110;&#27599;&#20010;&#39118;&#38505;&#31867;&#21035;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#20855;&#20307;&#30340;&#21361;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapid advancements in artificial intelligence (AI) have sparked growing concerns among experts, policymakers, and world leaders regarding the potential for increasingly advanced AI systems to pose catastrophic risks. Although numerous risks have been detailed separately, there is a pressing need for a systematic discussion and illustration of the potential dangers to better inform efforts to mitigate them. This paper provides an overview of the main sources of catastrophic AI risks, which we organize into four categories: malicious use, in which individuals or groups intentionally use AIs to cause harm; AI race, in which competitive environments compel actors to deploy unsafe AIs or cede control to AIs; organizational risks, highlighting how human factors and complex systems can increase the chances of catastrophic accidents; and rogue AIs, describing the inherent difficulty in controlling agents far more intelligent than humans. For each category of risk, we describe specific hazards,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Strokes2Surface&#65292;&#23427;&#21487;&#20174;&#24314;&#31569;&#24072;&#30340;&#31508;&#30011;&#20013;&#24674;&#22797;&#20986;&#26354;&#32447;&#32593;&#32476;&#65292;&#23545;&#20110;&#24314;&#31569;&#35774;&#35745;&#20013;&#30340;&#27010;&#24565;&#35774;&#35745;&#21644;&#25968;&#23383;&#24314;&#27169;&#20043;&#38388;&#30340;&#26725;&#26753;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2306.07220</link><description>&lt;p&gt;
Strokes2Surface&#65306;&#20174;&#22235;&#32500;&#24314;&#31569;&#35774;&#35745;&#32032;&#25551;&#20013;&#24674;&#22797;&#26354;&#32447;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Strokes2Surface: Recovering Curve Networks From 4D Architectural Design Sketches. (arXiv:2306.07220v2 [cs.GR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Strokes2Surface&#65292;&#23427;&#21487;&#20174;&#24314;&#31569;&#24072;&#30340;&#31508;&#30011;&#20013;&#24674;&#22797;&#20986;&#26354;&#32447;&#32593;&#32476;&#65292;&#23545;&#20110;&#24314;&#31569;&#35774;&#35745;&#20013;&#30340;&#27010;&#24565;&#35774;&#35745;&#21644;&#25968;&#23383;&#24314;&#27169;&#20043;&#38388;&#30340;&#26725;&#26753;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31163;&#32447;&#20960;&#20309;&#37325;&#24314;&#31649;&#36947;Strokes2Surface&#65292;&#23427;&#26159;&#22522;&#20110;4D Sketching Interface&#65292;MR.Sketch&#30340;&#30446;&#26631;&#26159;&#38754;&#21521;&#24314;&#31569;&#35774;&#35745;&#30340;&#12290;&#35813;&#31649;&#36947;&#20174;&#35774;&#35745;&#24072;&#32472;&#21046;&#30340;&#31508;&#30011;&#20013;&#24674;&#22797;&#26354;&#32447;&#32593;&#32476;&#65292;&#22240;&#27492;&#22312;&#24314;&#31569;&#35774;&#35745;&#30340;&#27010;&#24565;&#35774;&#35745;&#21644;&#25968;&#23383;&#24314;&#27169;&#38454;&#27573;&#20043;&#38388;&#24314;&#31435;&#20102;&#26725;&#26753;&#12290;&#25105;&#20204;&#30340;&#31649;&#36947;&#30340;&#36755;&#20837;&#21253;&#25324;3D&#31508;&#30011;&#30340;&#25240;&#32447;&#39030;&#28857;&#21450;&#20854;&#30456;&#24212;&#30340;&#26102;&#38388;&#25139;&#65288;&#20316;&#20026;&#31532;&#22235;&#20010;&#32500;&#24230;&#65289;&#65292;&#20197;&#21450;&#39069;&#22806;&#30340;&#20960;&#20309;&#21644;&#31508;&#35302;&#30456;&#20851;&#30340;&#35760;&#24405;&#23646;&#24615;&#12290;&#22522;&#20110;&#32032;&#25551;&#21512;&#24182;&#21644;&#22522;&#20110;&#32032;&#25551;&#24314;&#27169;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#31649;&#36947;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#24182;&#32452;&#21512;&#19977;&#20010;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#65307;&#19968;&#20010;&#20998;&#31867;&#22120;&#21644;&#20004;&#20010;&#32858;&#31867;&#27169;&#22411;&#12290;&#29305;&#21035;&#26159;&#65292;&#26681;&#25454;&#24314;&#31569;&#35774;&#35745;&#32032;&#25551;&#20013;&#35774;&#35745;&#24072;&#36890;&#24120;&#37319;&#29992;&#30340;&#23454;&#36341;&#35266;&#23519;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#19968;&#20010;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#65292;&#20197;&#35782;&#21035;&#19968;&#31508;&#30011;&#26159;&#25551;&#32472;&#36793;&#30028;&#21644;&#36793;&#32536;&#36824;&#26159;&#29992;&#20110;&#22635;&#20805;&#25152;&#38656;&#24314;&#31569;&#29289;&#30340;&#23553;&#38381;&#21306;&#22495;&#21644;&#34920;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Strokes2Surface, an offline geometry-reconstruction pipeline built upon a 4D Sketching Interface, MR.Sketch, targeted at architectural design. The pipeline recovers a curve network from designer-drawn strokes, thus bridging between concept design and digital modeling stages in architectural design. The input to our pipeline consists of 3D strokes' polyline vertices and their corresponding timestamps (as of the fourth dimension), along with additional geometric and stylus-related recorded properties. Inspired by sketch consolidation and sketch-based modeling methods, our pipeline leverages such data and combines three Machine Learning (ML) models; a classifier and two clustering models. In particular, based on observations of practices designers typically employ in architectural design sketches, we solve a binary classification problem to recognize whether a stroke depicts a boundary and edge or is used to fill in the enclosing areas and faces of the intended architectural ob
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; L-C2ST &#30340;&#22522;&#20110;&#26412;&#22320;&#35786;&#26029;&#23454;&#29616;&#27169;&#25311;&#25512;&#26029;&#20013;&#21518;&#39564;&#36817;&#20284;&#30340;&#26032;&#26041;&#27861;&#65292;&#20854;&#21487;&#20197;&#22312;&#20219;&#20309;&#32473;&#23450;&#30340;&#35266;&#27979;&#19979;&#26412;&#22320;&#35780;&#20272;&#21518;&#39564;&#20272;&#35745;&#22120;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#30446;&#21069;&#35780;&#20272;&#21518;&#39564;&#20272;&#35745;&#22120;&#38480;&#21046;&#35299;&#20915;&#26041;&#27861;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.03580</link><description>&lt;p&gt;
L-C2ST: &#22522;&#20110;&#26412;&#22320;&#35786;&#26029;&#23454;&#29616;&#27169;&#25311;&#25512;&#26029;&#20013;&#21518;&#39564;&#36817;&#20284;
&lt;/p&gt;
&lt;p&gt;
L-C2ST: Local Diagnostics for Posterior Approximations in Simulation-Based Inference. (arXiv:2306.03580v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; L-C2ST &#30340;&#22522;&#20110;&#26412;&#22320;&#35786;&#26029;&#23454;&#29616;&#27169;&#25311;&#25512;&#26029;&#20013;&#21518;&#39564;&#36817;&#20284;&#30340;&#26032;&#26041;&#27861;&#65292;&#20854;&#21487;&#20197;&#22312;&#20219;&#20309;&#32473;&#23450;&#30340;&#35266;&#27979;&#19979;&#26412;&#22320;&#35780;&#20272;&#21518;&#39564;&#20272;&#35745;&#22120;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#30446;&#21069;&#35780;&#20272;&#21518;&#39564;&#20272;&#35745;&#22120;&#38480;&#21046;&#35299;&#20915;&#26041;&#27861;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35768;&#22810;&#27169;&#25311;&#25512;&#26029;&#65288;SBI&#65289;&#30340;&#24037;&#20316;&#37117;&#20381;&#36182;&#20110;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#26469;&#36817;&#20284;&#22797;&#26434;&#12289;&#39640;&#32500;&#24230;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#36825;&#20123;&#36817;&#20284;&#26159;&#21542;&#21487;&#20449;&#20173;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#22823;&#22810;&#25968;&#26041;&#27861;&#20165;&#22312;&#35266;&#27979;&#31354;&#38388;&#26399;&#26395;&#19979;&#35780;&#20272;&#21518;&#39564;&#20272;&#35745;&#22120;&#12290;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#19981;&#33021;&#36275;&#22815;&#22320;&#30830;&#23450;&#21738;&#20123;&#35266;&#27979;&#32467;&#26524;&#21487;&#20197;&#20449;&#20219;&#36825;&#20123;&#36817;&#20284;&#25110;&#24212;&#35813;&#25913;&#36827;&#12290;&#25105;&#20204;&#22522;&#20110;&#33879;&#21517;&#30340;&#20998;&#31867;&#22120;&#20004;&#26679;&#26412;&#26816;&#39564; (C2ST)&#65292;&#24341;&#20837; L-C2ST&#65292;&#19968;&#20010;&#26032;&#26041;&#27861;&#65292;&#20801;&#35768;&#22312;&#20219;&#20309;&#32473;&#23450;&#30340;&#35266;&#27979;&#19979;&#26412;&#22320;&#35780;&#20272;&#21518;&#39564;&#20272;&#35745;&#22120;&#12290;&#23427;&#25552;&#20379;&#26377;&#29702;&#35770;&#22522;&#30784;&#21644;&#26131;&#20110;&#35299;&#37322;&#30340;&#65292;&#22914;&#22270;&#31034;&#35786;&#26029;&#12290;&#19982; C2ST &#19981;&#21516;&#30340;&#26159;&#65292;L-C2ST &#19981;&#38656;&#35201;&#35775;&#38382;&#30495;&#23454;&#21518;&#39564;&#30340;&#26679;&#26412;&#12290;&#23545;&#20110;&#22522;&#20110;&#24402;&#19968;&#21270;&#27969;&#30340;&#21518;&#39564;&#20272;&#35745;&#22120;&#65292;L-C2ST &#21487;&#20197;&#19987;&#38376;&#25552;&#20379;&#26356;&#22909;&#30340;&#32479;&#35745;&#21151;&#29575;&#65292;&#21516;&#26102;&#35745;&#31639;&#25928;&#29575;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many recent works in simulation-based inference (SBI) rely on deep generative models to approximate complex, high-dimensional posterior distributions. However, evaluating whether or not these approximations can be trusted remains a challenge. Most approaches evaluate the posterior estimator only in expectation over the observation space. This limits their interpretability and is not sufficient to identify for which observations the approximation can be trusted or should be improved. Building upon the well-known classifier two-sample test (C2ST), we introduce L-C2ST, a new method that allows for a local evaluation of the posterior estimator at any given observation. It offers theoretically grounded and easy to interpret - e.g. graphical - diagnostics, and unlike C2ST, does not require access to samples from the true posterior. In the case of normalizing flow-based posterior estimators, L-C2ST can be specialized to offer better statistical power, while being computationally more efficien
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21333;&#19968;&#25193;&#25955;&#27169;&#22411;&#22312;&#24314;&#27169;&#22810;&#20010;&#20219;&#21153;&#30340;&#31574;&#30053;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#31163;&#32447;&#25968;&#25454;&#26102;&#30340;&#26377;&#25928;&#24615;&#12290;&#35813;&#26041;&#27861;&#21517;&#20026;Multi-Task Diffusion Model (MTDiff)&#65292;&#32467;&#21512;&#20102;Transformer&#39592;&#24178;&#21644;&#25552;&#31034;&#23398;&#20064;&#65292;&#33021;&#22815;&#22312;&#22914;&#27492;&#22797;&#26434;&#30340;&#22810;&#20219;&#21153;&#29615;&#22659;&#19979;&#21462;&#24471;&#30456;&#24403;&#19981;&#38169;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18459</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#30340;&#26377;&#25928;&#35268;&#21010;&#22120;&#21644;&#25968;&#25454;&#21512;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Diffusion Model is an Effective Planner and Data Synthesizer for Multi-Task Reinforcement Learning. (arXiv:2305.18459v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21333;&#19968;&#25193;&#25955;&#27169;&#22411;&#22312;&#24314;&#27169;&#22810;&#20010;&#20219;&#21153;&#30340;&#31574;&#30053;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#31163;&#32447;&#25968;&#25454;&#26102;&#30340;&#26377;&#25928;&#24615;&#12290;&#35813;&#26041;&#27861;&#21517;&#20026;Multi-Task Diffusion Model (MTDiff)&#65292;&#32467;&#21512;&#20102;Transformer&#39592;&#24178;&#21644;&#25552;&#31034;&#23398;&#20064;&#65292;&#33021;&#22815;&#22312;&#22914;&#27492;&#22797;&#26434;&#30340;&#22810;&#20219;&#21153;&#29615;&#22659;&#19979;&#21462;&#24471;&#30456;&#24403;&#19981;&#38169;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#35270;&#35273;&#21644;NLP&#39046;&#22495;&#20013;&#23637;&#29616;&#20986;&#20102;&#39640;&#24230;&#34920;&#29616;&#21147;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#25193;&#25955;&#27169;&#22411;&#20063;&#33021;&#22815;&#26377;&#25928;&#22320;&#24314;&#27169;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#30340;&#22797;&#26434;&#31574;&#30053;&#25110;&#36712;&#36857;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#20165;&#38480;&#20110;&#21333;&#20219;&#21153;&#35774;&#32622;&#65292;&#27809;&#26377;&#32771;&#34385;&#22810;&#20219;&#21153;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#21333;&#19968;&#25193;&#25955;&#27169;&#22411;&#22312;&#24314;&#27169;&#29992;&#20110;&#22810;&#20010;&#20219;&#21153;&#31574;&#30053;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#31163;&#32447;&#25968;&#25454;&#26102;&#30340;&#26377;&#25928;&#24615;&#65292;&#20855;&#26377;&#22810;&#26679;&#21270;&#21644;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Multi-Task Diffusion Model&#65288;MTDiff&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;Transformer&#39592;&#24178;&#21644;&#25552;&#31034;&#23398;&#20064;&#65292;&#29992;&#20110;&#22810;&#20219;&#21153;&#31163;&#32447;&#35774;&#32622;&#20013;&#30340;&#29983;&#25104;&#35268;&#21010;&#21644;&#25968;&#25454;&#21512;&#25104;&#12290;MTDiff&#21033;&#29992;&#22823;&#37327;&#21487;&#29992;&#20110;&#22810;&#20219;&#21153;&#25968;&#25454;&#20013;&#30340;&#30693;&#35782;&#65292;&#24182;&#22312;&#20219;&#21153;&#20043;&#38388;&#25191;&#34892;&#38544;&#24335;&#30693;&#35782;&#20849;&#20139;&#20197;&#36827;&#34892;&#34394;&#25311;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have demonstrated highly-expressive generative capabilities in vision and NLP. Recent studies in reinforcement learning (RL) have shown that diffusion models are also powerful in modeling complex policies or trajectories in offline datasets. However, these works have been limited to single-task settings where a generalist agent capable of addressing multi-task predicaments is absent. In this paper, we aim to investigate the effectiveness of a single diffusion model in modeling large-scale multi-task offline data, which can be challenging due to diverse and multimodal data distribution. Specifically, we propose Multi-Task Diffusion Model (\textsc{MTDiff}), a diffusion-based method that incorporates Transformer backbones and prompt learning for generative planning and data synthesis in multi-task offline settings. \textsc{MTDiff} leverages vast amounts of knowledge available in multi-task data and performs implicit knowledge sharing among tasks. For generative planning, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Skill-KNN&#65292;&#19968;&#31181;&#22522;&#20110;&#25216;&#33021;&#30340;&#23569;&#26679;&#26412;&#36873;&#25321;&#26041;&#27861;&#65292;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#23427;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#20559;&#35265;&#38382;&#39064;&#24182;&#19988;&#19981;&#38656;&#35201;&#35757;&#32451;&#27169;&#22411;&#65292;&#36866;&#29992;&#20110;&#19981;&#26029;&#25193;&#23637;&#25110;&#26356;&#25913;&#31034;&#20363;&#24211;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2305.14210</link><description>&lt;p&gt;
&#22522;&#20110;&#25216;&#33021;&#30340;&#23569;&#26679;&#26412;&#36873;&#25321;&#20197;&#36866;&#24212;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Skill-Based Few-Shot Selection for In-Context Learning. (arXiv:2305.14210v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Skill-KNN&#65292;&#19968;&#31181;&#22522;&#20110;&#25216;&#33021;&#30340;&#23569;&#26679;&#26412;&#36873;&#25321;&#26041;&#27861;&#65292;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#23427;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#20559;&#35265;&#38382;&#39064;&#24182;&#19988;&#19981;&#38656;&#35201;&#35757;&#32451;&#27169;&#22411;&#65292;&#36866;&#29992;&#20110;&#19981;&#26029;&#25193;&#23637;&#25110;&#26356;&#25913;&#31034;&#20363;&#24211;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#25552;&#20379;&#19968;&#20123;&#31034;&#20363;&#26469;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#26159;&#19968;&#31181;&#33539;&#20363;&#12290;&#38024;&#23545;&#27599;&#20010;&#27979;&#35797;&#23454;&#20363;&#36873;&#25321;&#36866;&#24403;&#30340;&#31034;&#20363;&#26159;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#24456;&#37325;&#35201;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25216;&#33021;&#30340;&#23569;&#26679;&#26412;&#36873;&#25321;&#26041;&#27861;&#65292;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;Skill-KNN&#30340;&#20851;&#38190;&#20248;&#21183;&#21253;&#25324;&#65306;&#65288;1&#65289;&#23427;&#35299;&#20915;&#20102;&#29616;&#26377;&#22522;&#20110;&#39044;&#35757;&#32451;&#23884;&#20837;&#30340;&#26041;&#27861;&#23481;&#26131;&#22240;&#20026;&#19981;&#37325;&#35201;&#30340;&#33258;&#28982;&#35821;&#35328;&#34920;&#38754;&#29305;&#24449;&#23545;&#30446;&#26631;&#20219;&#21153;&#20135;&#29983;&#20559;&#35265;&#30340;&#38382;&#39064;&#65307;&#65288;2&#65289;&#23427;&#19981;&#38656;&#35201;&#35757;&#32451;&#25110;&#24494;&#35843;&#20219;&#20309;&#27169;&#22411;&#65292;&#36866;&#29992;&#20110;&#39057;&#32321;&#25193;&#23637;&#25110;&#26356;&#25913;&#31034;&#20363;&#24211;&#30340;&#24773;&#20917;&#12290;&#20854;&#20851;&#38190;&#35265;&#35299;&#26159;&#20248;&#21270;&#36755;&#20837;&#21040;&#23884;&#20837;&#27169;&#22411;&#20013;&#65292;&#32780;&#19981;&#26159;&#35843;&#25972;&#27169;&#22411;&#26412;&#36523;&#12290;&#22312;&#25216;&#26415;&#19978;&#65292;Skill-KNN&#36890;&#36807;&#21033;&#29992;&#39044;&#22788;&#29702;&#23569;&#26679;&#26412;&#25552;&#31034;&#20026;&#27599;&#20010;&#27979;&#35797;&#26696;&#20363;&#21644;&#20505;&#36873;&#31034;&#20363;&#29983;&#25104;&#22522;&#20110;&#25216;&#33021;&#30340;&#25551;&#36848;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#19981;&#37325;&#35201;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning is the paradigm that adapts large language models to downstream tasks by providing a few examples. Few-shot selection -- selecting appropriate examples for each test instance separately -- is important for in-context learning. In this paper, we propose Skill-KNN, a skill-based few-shot selection method for in-context learning. The key advantages of Skill-KNN include: (1) it addresses the problem that existing methods based on pre-trained embeddings can be easily biased by surface natural language features that are not important for the target task; (2) it does not require training or fine-tuning of any models, making it suitable for frequently expanding or changing example banks. The key insight is to optimize the inputs fed into the embedding model, rather than tuning the model itself. Technically, Skill-KNN generates the skill-based descriptions for each test case and candidate example by utilizing a pre-processing few-shot prompting, thus eliminating unimportant 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#25512;&#29702;&#30340;&#31867;&#27604;&#32467;&#26500;&#25512;&#26029;&#20219;&#21153;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36827;&#34892;&#31185;&#23398;&#31867;&#27604;&#26102;&#24573;&#35270;&#32467;&#26500;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.12660</link><description>&lt;p&gt;
&#34920;&#38754;&#30456;&#20284;&#24615;&#20043;&#19979;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#32467;&#26500;&#25512;&#29702;&#36827;&#34892;&#21512;&#29702;&#30340;&#31185;&#23398;&#31867;&#27604;
&lt;/p&gt;
&lt;p&gt;
Beneath Surface Similarity: Large Language Models Make Reasonable Scientific Analogies after Structure Abduction. (arXiv:2305.12660v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#25512;&#29702;&#30340;&#31867;&#27604;&#32467;&#26500;&#25512;&#26029;&#20219;&#21153;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36827;&#34892;&#31185;&#23398;&#31867;&#27604;&#26102;&#24573;&#35270;&#32467;&#26500;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#35748;&#30693;&#20013;&#31867;&#27604;&#25512;&#29702;&#30340;&#37325;&#35201;&#20316;&#29992;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#20849;&#20139;&#30340;&#20851;&#31995;&#32467;&#26500;&#23558;&#26032;&#27010;&#24565;&#19982;&#29087;&#24713;&#30340;&#27010;&#24565;&#32852;&#31995;&#36215;&#26469;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#20851;&#27880;&#20110;&#35789;&#35821;&#31867;&#27604;&#65292;&#20294;&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#32463;&#24120;&#24573;&#35270;&#26500;&#25104;&#36825;&#20123;&#31867;&#27604;&#30340;&#32467;&#26500;&#65292;&#36825;&#24341;&#21457;&#20102;&#23545;&#35789;&#35821;&#31867;&#27604;&#20316;&#20026;&#31867;&#27604;&#25512;&#29702;&#25216;&#33021;&#65288;&#31867;&#20284;&#20110;&#20154;&#31867;&#35748;&#30693;&#65289;&#30340;&#26377;&#25928;&#24615;&#30340;&#36136;&#30097;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#35748;&#30693;&#24515;&#29702;&#23398;&#30340;&#31867;&#27604;&#32467;&#26500;&#25512;&#29702;&#20219;&#21153;&#65292;&#26088;&#22312;&#25512;&#26029;&#20986;&#36830;&#25509;&#20004;&#20010;&#31995;&#32479;&#20043;&#38388;&#30340;&#31867;&#27604;&#32467;&#26500;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#21517;&#20026;SCAR&#30340;&#22522;&#20934;&#65292;&#21253;&#21547;&#26469;&#33258;13&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;400&#20010;&#31185;&#23398;&#31867;&#27604;&#65292;&#26088;&#22312;&#35780;&#20272;&#21033;&#29992;&#32467;&#26500;&#25512;&#29702;&#30340;&#31867;&#27604;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#35777;&#35777;&#25454;&#24378;&#35843;&#20102;LLMs&#65292;&#21253;&#25324;ChatGPT&#21644;GPT-4&#65292;&#22312;&#25484;&#25569;&#36825;&#20010;&#20219;&#21153;&#19978;&#20381;&#28982;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The vital role of analogical reasoning in human cognition allows us to grasp novel concepts by linking them with familiar ones through shared relational structures. Despite the attention previous research has given to word analogies, this work suggests that Large Language Models (LLMs) often overlook the structures that underpin these analogies, raising questions about the efficacy of word analogies as a measure of analogical reasoning skills akin to human cognition. In response to this, our paper introduces a task of analogical structure abduction, grounded in cognitive psychology, designed to abduce structures that form an analogy between two systems. In support of this task, we establish a benchmark called SCAR, containing 400 scientific analogies from 13 distinct fields, tailored for evaluating analogical reasoning with structure abduction. The empirical evidence underlines the continued challenges faced by LLMs, including ChatGPT and GPT-4, in mastering this task, signifying the n
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24341;&#20837;&#20102;Clifford&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#21487;&#20197;&#26500;&#24314;O(n)&#21644;E(n)&#31561;&#21464;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35843;&#25972;Clifford&#32676;&#30340;&#23450;&#20041;&#20197;&#21450;&#20445;&#25345;&#21521;&#37327;&#31354;&#38388;&#21644;&#20056;&#27861;&#32467;&#26500;&#30340;&#20316;&#29992;&#26469;&#23454;&#29616;&#22810;&#20010;&#26377;&#21033;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11141</link><description>&lt;p&gt;
Clifford&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Clifford Group Equivariant Neural Networks. (arXiv:2305.11141v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11141
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;Clifford&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#21487;&#20197;&#26500;&#24314;O(n)&#21644;E(n)&#31561;&#21464;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35843;&#25972;Clifford&#32676;&#30340;&#23450;&#20041;&#20197;&#21450;&#20445;&#25345;&#21521;&#37327;&#31354;&#38388;&#21644;&#20056;&#27861;&#32467;&#26500;&#30340;&#20316;&#29992;&#26469;&#23454;&#29616;&#22810;&#20010;&#26377;&#21033;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;Clifford&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#65306;&#19968;&#31181;&#26500;&#24314;O(n)&#21644;E(n)&#31561;&#21464;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30830;&#23450;&#24182;&#30740;&#31350;&#20102;Clifford&#32676;&#65292;&#23427;&#26159;Clifford&#20195;&#25968;&#20013;&#30340;&#19968;&#20010;&#23376;&#32676;&#65292;&#20854;&#23450;&#20041;&#32463;&#36807;&#35843;&#25972;&#20197;&#23454;&#29616;&#22810;&#20010;&#26377;&#21033;&#23646;&#24615;&#12290;&#20027;&#35201;&#22320;&#65292;&#35813;&#32676;&#30340;&#20316;&#29992;&#24418;&#25104;&#20102;&#19968;&#20010;&#27491;&#20132;&#33258;&#21516;&#26500;&#65292;&#25193;&#23637;&#21040;&#25972;&#20010;Clifford&#20195;&#25968;&#65292;&#21516;&#26102;&#23562;&#37325;&#22810;&#30690;&#20998;&#32423;&#12290;&#36825;&#23548;&#33268;&#20102;&#23545;&#24212;&#20110;&#22810;&#30690;&#20998;&#35299;&#30340;&#22810;&#20010;&#38750;&#31561;&#20215;&#23376;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#35813;&#20316;&#29992;&#19981;&#20165;&#23562;&#37325;Clifford&#20195;&#25968;&#30340;&#21521;&#37327;&#31354;&#38388;&#32467;&#26500;&#65292;&#36824;&#23562;&#37325;&#20854;&#20056;&#27861;&#32467;&#26500;&#65292;&#21363;&#20960;&#20309;&#20056;&#31215;&#12290;&#36825;&#20123;&#21457;&#29616;&#24847;&#21619;&#30528;&#25105;&#20204;&#21487;&#20197;&#24471;&#21040;&#22312;&#20219;&#24847;&#32500;&#30340;&#20869;&#31215;&#31354;&#38388;&#20013;&#20248;&#38597;&#22320;&#25512;&#24191;&#30340;&#34920;&#36798;&#23618;&#12290;&#25105;&#20204;&#29305;&#21035;&#23637;&#31034;&#20102;&#20174;&#19968;&#20010;sin
&lt;/p&gt;
&lt;p&gt;
We introduce Clifford Group Equivariant Neural Networks: a novel approach for constructing $\mathrm{O}(n)$- and $\mathrm{E}(n)$-equivariant models. We identify and study the $\textit{Clifford group}$, a subgroup inside the Clifford algebra whose definition we adjust to achieve several favorable properties. Primarily, the group's action forms an orthogonal automorphism that extends beyond the typical vector space to the entire Clifford algebra while respecting the multivector grading. This leads to several non-equivalent subrepresentations corresponding to the multivector decomposition. Furthermore, we prove that the action respects not just the vector space structure of the Clifford algebra but also its multiplicative structure, i.e., the geometric product. These findings imply that every polynomial in multivectors, An advantage worth mentioning is that we obtain expressive layers that can elegantly generalize to inner-product spaces of any dimension. We demonstrate, notably from a sin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#35770;&#30340;&#36125;&#21494;&#26031;&#32479;&#35745;&#27169;&#22411;&#30340;&#37325;&#25972;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;Fisher&#24230;&#37327;&#23450;&#20041;&#20102;&#19968;&#20010;&#30456;&#20851;&#38271;&#24230;&#20316;&#20026;&#32039;&#23494;&#30456;&#20851;&#30340;&#27010;&#29575;&#20998;&#24067;&#28857;&#20043;&#38388;&#30340;&#21487;&#20998;&#36776;&#24615;(RG)&#23610;&#24230;&#65292;&#22312;&#32479;&#35745;&#25512;&#26029;&#23454;&#39564;&#20013;&#65292;&#21487;&#20197;&#24471;&#21040;&#26576;&#20010;&#31995;&#32479;&#26368;&#22823;&#29305;&#24322;&#24615;&#35266;&#23519;&#25968;&#37327;&#30340;&#20195;&#29702;&#12290;&#36125;&#21494;&#26031;&#37325;&#25972;&#21270;&#26041;&#27861;&#20026;&#32473;&#23450;&#31995;&#32479;&#20934;&#22791;&#19968;&#20010;&#22312;&#19978;&#36848;&#23610;&#24230;&#19978;&#31934;&#24230;&#26377;&#38480;&#30340;&#26377;&#25928;&#27169;&#22411;&#65292;&#36825;&#20010;&#23610;&#24230;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#24403;&#21069;&#23454;&#39564;&#35013;&#32622;&#21487;&#20197;&#25506;&#27979;&#21040;&#30340;&#26368;&#22823;&#33021;&#37327;&#12290;&#36125;&#21494;&#26031;&#37325;&#25972;&#21270;&#25552;&#20986;&#20102;&#19968;&#31181;&#21457;&#29616;&#21644;&#34920;&#24449;&#22522;&#26412;&#29289;&#29702;&#29702;&#35770;&#30340;&#26032;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2305.10491</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#37325;&#25972;&#21270;
&lt;/p&gt;
&lt;p&gt;
Bayesian Renormalization. (arXiv:2305.10491v1 [hep-th])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10491
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#35770;&#30340;&#36125;&#21494;&#26031;&#32479;&#35745;&#27169;&#22411;&#30340;&#37325;&#25972;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;Fisher&#24230;&#37327;&#23450;&#20041;&#20102;&#19968;&#20010;&#30456;&#20851;&#38271;&#24230;&#20316;&#20026;&#32039;&#23494;&#30456;&#20851;&#30340;&#27010;&#29575;&#20998;&#24067;&#28857;&#20043;&#38388;&#30340;&#21487;&#20998;&#36776;&#24615;(RG)&#23610;&#24230;&#65292;&#22312;&#32479;&#35745;&#25512;&#26029;&#23454;&#39564;&#20013;&#65292;&#21487;&#20197;&#24471;&#21040;&#26576;&#20010;&#31995;&#32479;&#26368;&#22823;&#29305;&#24322;&#24615;&#35266;&#23519;&#25968;&#37327;&#30340;&#20195;&#29702;&#12290;&#36125;&#21494;&#26031;&#37325;&#25972;&#21270;&#26041;&#27861;&#20026;&#32473;&#23450;&#31995;&#32479;&#20934;&#22791;&#19968;&#20010;&#22312;&#19978;&#36848;&#23610;&#24230;&#19978;&#31934;&#24230;&#26377;&#38480;&#30340;&#26377;&#25928;&#27169;&#22411;&#65292;&#36825;&#20010;&#23610;&#24230;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#24403;&#21069;&#23454;&#39564;&#35013;&#32622;&#21487;&#20197;&#25506;&#27979;&#21040;&#30340;&#26368;&#22823;&#33021;&#37327;&#12290;&#36125;&#21494;&#26031;&#37325;&#25972;&#21270;&#25552;&#20986;&#20102;&#19968;&#31181;&#21457;&#29616;&#21644;&#34920;&#24449;&#22522;&#26412;&#29289;&#29702;&#29702;&#35770;&#30340;&#26032;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#22522;&#20110;&#20449;&#24687;&#35770;&#30340;&#36125;&#21494;&#26031;&#32479;&#35745;&#27169;&#22411;&#30340;&#37325;&#25972;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;&#36125;&#21494;&#26031;&#37325;&#25972;&#21270;&#12290;&#36125;&#21494;&#26031;&#37325;&#25972;&#21270;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#20351;&#29992;Fisher&#24230;&#37327;&#26469;&#23450;&#20041;&#19968;&#20010;&#30456;&#20851;&#38271;&#24230;&#65292;&#36825;&#20010;&#38271;&#24230;&#36215;&#21040;&#20102;&#32039;&#23494;&#30456;&#20851;&#30340;&#27010;&#29575;&#20998;&#24067;&#28857;&#20043;&#38388;&#30340;&#21487;&#20998;&#36776;&#24615;(RG)&#23610;&#24230;&#12290;&#36825;&#20010;RG&#23610;&#24230;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#22312;&#32479;&#35745;&#25512;&#26029;&#23454;&#39564;&#20013;&#23545;&#20110;&#19968;&#20010;&#32473;&#23450;&#31995;&#32479;&#21487;&#20197;&#24471;&#21040;&#30340;&#26368;&#22823;&#29305;&#24322;&#24615;&#35266;&#23519;&#25968;&#37327;&#30340;&#20195;&#29702;&#12290;&#36125;&#21494;&#26031;&#37325;&#25972;&#21270;&#26041;&#27861;&#30340;&#20316;&#29992;&#26159;&#20026;&#32473;&#23450;&#31995;&#32479;&#20934;&#22791;&#19968;&#20010;&#22312;&#19978;&#36848;&#23610;&#24230;&#19978;&#31934;&#24230;&#26377;&#38480;&#30340;&#26377;&#25928;&#27169;&#22411;&#12290;&#22312;&#23558;&#36125;&#21494;&#26031;&#37325;&#25972;&#21270;&#26041;&#27861;&#24212;&#29992;&#20110;&#29289;&#29702;&#31995;&#32479;&#26102;&#65292;&#36825;&#20010;&#30001;&#20449;&#24687;&#35770;&#20986;&#29616;&#30340;RG&#23610;&#24230;&#33258;&#28982;&#22320;&#34987;&#35782;&#21035;&#20026;&#24403;&#21069;&#23454;&#39564;&#35013;&#32622;&#21487;&#20197;&#25506;&#27979;&#21040;&#30340;&#26368;&#22823;&#33021;&#37327;&#65292;&#22240;&#27492;&#65292;&#36125;&#21494;&#26031;&#37325;&#25972;&#21270;&#25552;&#20986;&#20102;&#19968;&#31181;&#21457;&#29616;&#21644;&#34920;&#24449;&#22522;&#26412;&#29289;&#29702;&#29702;&#35770;&#30340;&#26032;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this note we present a fully information theoretic approach to renormalization inspired by Bayesian statistical inference, which we refer to as Bayesian Renormalization. The main insight of Bayesian Renormalization is that the Fisher metric defines a correlation length that plays the role of an emergent RG scale quantifying the distinguishability between nearby points in the space of probability distributions. This RG scale can be interpreted as a proxy for the maximum number of unique observations that can be made about a given system during a statistical inference experiment. The role of the Bayesian Renormalization scheme is subsequently to prepare an effective model for a given system up to a precision which is bounded by the aforementioned scale. In applications of Bayesian Renormalization to physical systems, the emergent information theoretic scale is naturally identified with the maximum energy that can be probed by current experimental apparatus, and thus Bayesian Renormali
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;Segment Anything Model (SAM)&#22686;&#24378;Class Activation Maps (CAM)&#29983;&#25104;&#39640;&#36136;&#37327;&#20266;&#26631;&#31614;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#20013;CAM&#30340;&#23616;&#37096;&#28608;&#27963;&#21644;&#34394;&#20551;&#28608;&#27963;&#30340;&#38480;&#21046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.05803</link><description>&lt;p&gt;
&#22522;&#20110;Segment Anything Model (SAM)&#22686;&#24378;&#20266;&#26631;&#31614;&#30340;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Segment Anything Model (SAM) Enhanced Pseudo Labels for Weakly Supervised Semantic Segmentation. (arXiv:2305.05803v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;Segment Anything Model (SAM)&#22686;&#24378;Class Activation Maps (CAM)&#29983;&#25104;&#39640;&#36136;&#37327;&#20266;&#26631;&#31614;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#20013;CAM&#30340;&#23616;&#37096;&#28608;&#27963;&#21644;&#34394;&#20551;&#28608;&#27963;&#30340;&#38480;&#21046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#20351;&#29992;&#22270;&#20687;&#32423;&#21035;&#30340;&#30417;&#30563;&#30340;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;(WSSS)&#30001;&#20110;&#20854;&#19982;&#20687;&#32032;&#32423;&#27880;&#37322;&#30456;&#27604;&#30340;&#20302;&#25104;&#26412;&#32780;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#31867;&#28608;&#27963;&#22270;(CAM)&#29983;&#25104;&#20687;&#32032;&#32423;&#30340;&#20266;&#26631;&#31614;&#36827;&#34892;&#30417;&#30563;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;CAM&#32463;&#24120;&#36973;&#21463;&#23616;&#37096;&#28608;&#27963;&#30340;&#38480;&#21046;-&#21482;&#28608;&#27963;&#26368;&#20855;&#21306;&#20998;&#24615;&#30340;&#37096;&#20998;&#65292;&#32780;&#19981;&#26159;&#25972;&#20010;&#23545;&#35937;&#21306;&#22495;&#21644;&#34394;&#20551;&#30340;&#28608;&#27963;-&#19981;&#24517;&#35201;&#22320;&#28608;&#27963;&#29289;&#20307;&#21608;&#22260;&#30340;&#32972;&#26223;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#21363;&#21033;&#29992;&#26368;&#36817;&#21457;&#24067;&#30340;Segment Anything Model (SAM)&#22686;&#24378;CAM&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20266;&#26631;&#31614;&#12290;SAM&#26159;&#19968;&#20010;&#20998;&#21106;&#22522;&#30784;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#23558;&#22270;&#20687;&#20998;&#21106;&#25104;&#27573;&#33853;&#30340;&#24378;&#38646;-shot&#33021;&#21147;&#65292;&#20294;&#32570;&#20047;&#36825;&#20123;&#21306;&#22495;&#30340;&#35821;&#20041;&#26631;&#31614;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#29305;&#23450;&#31867;&#21035;&#30340;&#20266;&#26631;&#31614;&#20316;&#20026;&#20449;&#21495;&#26469;&#36873;&#25321;m&#12290;
&lt;/p&gt;
&lt;p&gt;
Weakly Supervised Semantic Segmentation (WSSS) with only image-level supervision has garnered increasing attention due to its low annotation cost compared to pixel-level annotation. Most existing methods rely on Class Activation Maps (CAM) to generate pixel-level pseudo labels for supervised training. However, it is well known that CAM often suffers from partial activation -- activating the most discriminative part instead of the entire object area, and false activation -- unnecessarily activating the background around the object. In this study, we introduce a simple yet effective approach to address these limitations by harnessing the recently released Segment Anything Model (SAM) to generate higher-quality pseudo labels with CAM. SAM is a segmentation foundation model that demonstrates strong zero-shot ability in partitioning images into segments but lacks semantic labels for these regions. To circumvent this, we employ pseudo labels for a specific class as the signal to select the m
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;PeerSum&#29992;&#20110;&#29983;&#25104;&#31185;&#23398;&#35770;&#25991;&#30340;&#20803;&#35780;&#35770;&#65292;&#28304;&#25991;&#26723;&#20855;&#26377;&#26174;&#24335;&#23618;&#27425;&#32467;&#26500;&#30340;&#20016;&#23500;&#25991;&#26723;&#38388;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20803;&#35780;&#35770;&#29983;&#25104;&#30340;&#20851;&#31995;&#24863;&#30693;&#22810;&#20219;&#21153;&#27169;&#22411;Rammer&#12290;</title><link>http://arxiv.org/abs/2305.01498</link><description>&lt;p&gt;
&#26088;&#22312;&#24635;&#32467;&#24102;&#26377;&#23618;&#27425;&#20851;&#31995;&#30340;&#22810;&#31687;&#25991;&#26723;
&lt;/p&gt;
&lt;p&gt;
Towards Summarizing Multiple Documents with Hierarchical Relationships. (arXiv:2305.01498v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01498
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;PeerSum&#29992;&#20110;&#29983;&#25104;&#31185;&#23398;&#35770;&#25991;&#30340;&#20803;&#35780;&#35770;&#65292;&#28304;&#25991;&#26723;&#20855;&#26377;&#26174;&#24335;&#23618;&#27425;&#32467;&#26500;&#30340;&#20016;&#23500;&#25991;&#26723;&#38388;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20803;&#35780;&#35770;&#29983;&#25104;&#30340;&#20851;&#31995;&#24863;&#30693;&#22810;&#20219;&#21153;&#27169;&#22411;Rammer&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#25968;&#29616;&#23384;&#30340;&#22810;&#25991;&#26723;&#25688;&#35201;(MDS)&#25968;&#25454;&#38598;&#32570;&#23569;&#20154;&#24037;&#29983;&#25104;&#30340;&#12289;&#30495;&#23454;&#30340;(&#21363;&#38750;&#21512;&#25104;&#30340;)&#25688;&#35201;&#25110;&#32773;&#24102;&#26377;&#26174;&#24335;&#25991;&#26723;&#38388;&#20851;&#31995;&#30340;&#28304;&#25991;&#26723;&#12290;&#20026;&#20102;&#22686;&#24378;MDS&#31995;&#32479;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;PeerSum&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#29983;&#25104;&#31185;&#23398;&#35770;&#25991;&#30340;&#20803;&#35780;&#35770;&#65292;&#20854;&#20013;&#20803;&#35780;&#35770;&#26159;&#23545;&#35780;&#35770;&#21644;&#30456;&#24212;&#35752;&#35770;&#30340;&#39640;&#24230;&#27010;&#25324;&#19988;&#30495;&#23454;&#30340;&#25688;&#35201;&#12290;&#36825;&#20123;&#28304;&#25991;&#26723;&#20855;&#26377;&#26174;&#24335;&#23618;&#27425;&#32467;&#26500;&#30340;&#20016;&#23500;&#25991;&#26723;&#38388;&#20851;&#31995;&#65292;&#21253;&#25324;&#20132;&#21449;&#24341;&#29992;&#21644;&#32463;&#24120;&#20986;&#29616;&#30340;&#20914;&#31361;&#12290;&#37492;&#20110;&#24456;&#23569;&#26377;&#30740;&#31350;&#37319;&#29992;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#25805;&#32437;&#26469;&#23558;&#23618;&#27425;&#20851;&#31995;&#32435;&#20837;MDS&#31995;&#32479;&#20013;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;Rammer(&#20851;&#31995;&#24863;&#30693;&#22810;&#20219;&#21153;&#20803;&#35780;&#35770;&#29983;&#25104;&#22120;)&#65292;&#36825;&#26159;&#19968;&#31181;&#20803;&#35780;&#35770;&#29983;&#25104;&#27169;&#22411;&#65292;&#20351;&#29992;&#22522;&#20110;&#23618;&#27425;&#20851;&#31995;&#30340;&#31232;&#30095;&#27880;&#24847;&#21147;&#21644;&#22810;&#20219;&#21153;&#30446;&#26631;&#65292;&#21487;&#20197;&#39044;&#27979;&#22810;&#20010;&#24230;&#37327;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing multi-document summarization (MDS) datasets lack human-generated and genuine (i.e., not synthetic) summaries or source documents with explicit inter-document relationships that a summary must capture. To enhance the capabilities of MDS systems we present PeerSum, a novel dataset for generating meta-reviews of scientific papers, where the meta-reviews are highly abstractive and genuine summaries of reviews and corresponding discussions. These source documents have rich inter-document relationships of an explicit hierarchical structure with cross-references and often feature conflicts. As there is a scarcity of research that incorporates hierarchical relationships into MDS systems through attention manipulation on pre-trained language models, we additionally present Rammer (Relationship-aware Multi-task Meta-review Generator), a meta-review generation model that uses sparse attention based on the hierarchical relationships and a multi-task objective that predicts several me
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LTL&#35268;&#33539;&#30340;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20056;&#31215;MDP&#12289;&#22870;&#21169;&#32467;&#26500;&#21644;&#25240;&#25187;&#26426;&#21046;&#26377;&#25928;&#22320;&#23398;&#20064;&#24182;&#20248;&#21270;&#26410;&#30693;&#38543;&#26426;&#31995;&#32479;&#26368;&#22823;&#21270;&#28385;&#36275;LTL&#35268;&#33539;&#30340;&#27010;&#29575;&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2305.01381</link><description>&lt;p&gt;
&#22522;&#20110;LTL&#35268;&#33539;&#30340;&#26679;&#26412;&#26377;&#25928;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#19982;&#20248;&#21270;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Sample Efficient Model-free Reinforcement Learning from LTL Specifications with Optimality Guarantees. (arXiv:2305.01381v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LTL&#35268;&#33539;&#30340;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20056;&#31215;MDP&#12289;&#22870;&#21169;&#32467;&#26500;&#21644;&#25240;&#25187;&#26426;&#21046;&#26377;&#25928;&#22320;&#23398;&#20064;&#24182;&#20248;&#21270;&#26410;&#30693;&#38543;&#26426;&#31995;&#32479;&#26368;&#22823;&#21270;&#28385;&#36275;LTL&#35268;&#33539;&#30340;&#27010;&#29575;&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#26102;&#38388;&#36923;&#36753;&#65288;LTL&#65289;&#24191;&#27867;&#29992;&#20110;&#25351;&#23450;&#31995;&#32479;&#31574;&#30053;&#30340;&#39640;&#32423;&#30446;&#26631;&#65292;&#33258;&#20027;&#31995;&#32479;&#23398;&#20064;&#30456;&#23545;&#20110;&#36825;&#26679;&#30340;&#35268;&#33539;&#30340;&#26368;&#20248;&#31574;&#30053;&#26159;&#38750;&#24120;&#29702;&#24819;&#30340;&#12290; &#20294;&#26159;&#65292;&#20174;LTL&#35268;&#33539;&#20013;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#24182;&#19981;&#36731;&#26494;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#26410;&#30693;&#38543;&#26426;&#31995;&#32479;&#30340;&#26368;&#20248;&#31574;&#30053;&#65292;&#20854;&#20013;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#26356;&#36890;&#29992;&#30340;&#20056;&#31215;MDP&#12289;&#22870;&#21169;&#32467;&#26500;&#21644;&#25240;&#25187;&#26426;&#21046;&#65292;&#24403;&#19982;&#29616;&#25104;&#30340;&#26080;&#27169;&#22411;RL&#31639;&#27861;&#32467;&#21512;&#20351;&#29992;&#26102;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#23398;&#20064;&#26368;&#22823;&#21270;&#32473;&#23450;LTL&#35268;&#33539;&#28385;&#36275;&#27010;&#29575;&#30340;&#26368;&#20248;&#31574;&#30053;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#26377;&#20851;&#36873;&#25321;RL&#20013;&#20851;&#38190;&#21442;&#25968;&#20197;&#20445;&#35777;&#26368;&#20248;&#24615;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;&#20026;&#20102;&#30452;&#25509;&#35780;&#20272;&#23398;&#20064;&#31574;&#30053;&#65292;&#25105;&#20204;&#37319;&#29992;&#27010;&#29575;&#27169;&#22411;&#26816;&#26597;&#22120;PRISM&#26469;&#35745;&#31639;LTL&#35268;&#33539;&#30340;&#28385;&#36275;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear Temporal Logic (LTL) is widely used to specify high-level objectives for system policies, and it is highly desirable for autonomous systems to learn the optimal policy with respect to such specifications. However, learning the optimal policy from LTL specifications is not trivial. We present a model-free Reinforcement Learning (RL) approach that efficiently learns an optimal policy for an unknown stochastic system, modelled using Markov Decision Processes (MDPs). We propose a novel and more general product MDP, reward structure and discounting mechanism that, when applied in conjunction with off-the-shelf model-free RL algorithms, efficiently learn the optimal policy that maximizes the probability of satisfying a given LTL specification with optimality guarantees. We also provide improved theoretical results on choosing the key parameters in RL to ensure optimality. To directly evaluate the learned policy, we adopt probabilistic model checker PRISM to compute the probability of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25490;&#21517;&#23398;&#20064;&#21644;&#23616;&#37096;&#27169;&#22411;&#30340;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#20998;&#31867;&#22120;&#36827;&#34892;&#25490;&#21517;&#65292;&#20197;&#35299;&#20915;&#39640;&#32500;&#26114;&#36149;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.09444</link><description>&lt;p&gt;
&#22522;&#20110;&#25490;&#21517;&#23398;&#20064;&#21644;&#23616;&#37096;&#27169;&#22411;&#30340;&#22810;&#30446;&#26631;&#39640;&#32500;&#26114;&#36149;&#38382;&#39064;&#30340;&#36827;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rank-Based Learning and Local Model Based Evolutionary Algorithm for High-Dimensional Expensive Multi-Objective Problems. (arXiv:2304.09444v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25490;&#21517;&#23398;&#20064;&#21644;&#23616;&#37096;&#27169;&#22411;&#30340;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#20998;&#31867;&#22120;&#36827;&#34892;&#25490;&#21517;&#65292;&#20197;&#35299;&#20915;&#39640;&#32500;&#26114;&#36149;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36741;&#20197;&#20195;&#29702;&#27169;&#22411;&#30340;&#36827;&#21270;&#31639;&#27861;&#24191;&#27867;&#24212;&#29992;&#20110;&#35299;&#20915;&#22797;&#26434;&#32780;&#35745;&#31639;&#20195;&#20215;&#26114;&#36149;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;&#20294;&#26159;&#22312;&#22788;&#29702;&#39640;&#32500;&#20248;&#21270;&#38382;&#39064;&#26102;&#65292;&#36825;&#20123;&#36741;&#20197;&#20195;&#29702;&#27169;&#22411;&#30340;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#30340;&#24615;&#33021;&#20250;&#24613;&#21095;&#24694;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20998;&#31867;&#22120;&#36741;&#21161;&#30340;&#25490;&#21517;&#23398;&#20064;&#21644;&#23616;&#37096;&#27169;&#22411;&#30340;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861; (CLMEA)&#65292;&#29992;&#20110;&#35299;&#20915;&#39640;&#32500;&#26114;&#36149;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#30001;&#19977;&#37096;&#20998;&#32452;&#25104;&#65306;&#20998;&#31867;&#22120;&#36741;&#21161;&#30340;&#25490;&#21517;&#23398;&#20064;&#12289;&#36229;&#20307;&#31215;&#38750;&#25903;&#37197;&#25628;&#32034;&#21644;&#30456;&#23545;&#31232;&#30095;&#30446;&#26631;&#31354;&#38388;&#30340;&#23616;&#37096;&#25628;&#32034;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35813;&#31639;&#27861;&#24314;&#31435;&#20102;&#19968;&#20010;&#27010;&#29575;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#20998;&#31867;&#22120;&#65292;&#23558;&#21518;&#20195;&#21010;&#20998;&#20026;&#20960;&#20010;&#31561;&#32423;&#12290;&#19981;&#21516;&#31561;&#32423;&#30340;&#21518;&#20195;&#20351;&#29992;&#25490;&#21517;&#23398;&#20064;&#31574;&#30053;&#29983;&#25104;&#26356;&#20855;&#26377;&#21069;&#26223;&#24615;&#21644;&#20449;&#24687;&#24615;&#30340;&#20505;&#36873;&#35299;&#29992;&#20110;&#23454;&#38469;&#20248;&#21270;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Surrogate-assisted evolutionary algorithms have been widely developed to solve complex and computationally expensive multi-objective optimization problems in recent years. However, when dealing with high-dimensional optimization problems, the performance of these surrogate-assisted multi-objective evolutionary algorithms deteriorate drastically. In this work, a novel Classifier-assisted rank-based learning and Local Model based multi-objective Evolutionary Algorithm (CLMEA) is proposed for high-dimensional expensive multi-objective optimization problems. The proposed algorithm consists of three parts: classifier-assisted rank-based learning, hypervolume-based non-dominated search, and local search in the relatively sparse objective space. Specifically, a probabilistic neural network is built as classifier to divide the offspring into a number of ranks. The offspring in different ranks uses rank-based learning strategy to generate more promising and informative candidates for real funct
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;MC-ViViT&#65292;&#29992;&#20110;&#36890;&#36807;&#20998;&#26512;&#38754;&#37096;&#29305;&#24449;&#26816;&#27979;&#32769;&#24180;&#20154;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#12290;&#36890;&#36807;MC&#27169;&#22359;&#21644;&#32467;&#21512;&#25439;&#22833;&#20989;&#25968;&#26469;&#35299;&#20915;&#25968;&#25454;&#38598;&#26679;&#26412;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.05292</link><description>&lt;p&gt;
MC-ViViT: &#22810;&#20998;&#25903;&#20998;&#31867;&#22120;-ViViT&#29992;&#20110;&#20351;&#29992;&#38754;&#37096;&#35270;&#39057;&#26816;&#27979;&#32769;&#24180;&#20154;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;
&lt;/p&gt;
&lt;p&gt;
MC-ViViT: Multi-branch Classifier-ViViT to Detect Mild Cognitive Impairment in Older Adults using Facial Videos. (arXiv:2304.05292v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;MC-ViViT&#65292;&#29992;&#20110;&#36890;&#36807;&#20998;&#26512;&#38754;&#37096;&#29305;&#24449;&#26816;&#27979;&#32769;&#24180;&#20154;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#12290;&#36890;&#36807;MC&#27169;&#22359;&#21644;&#32467;&#21512;&#25439;&#22833;&#20989;&#25968;&#26469;&#35299;&#20915;&#25968;&#25454;&#38598;&#26679;&#26412;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21253;&#25324;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#24050;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#20351;&#29992;&#21307;&#23398;&#22270;&#20687;&#12289;&#38382;&#21367;&#21644;&#35270;&#39057;&#26816;&#27979;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;(MCI)&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#20998;&#25903;&#20998;&#31867;&#22120;-&#35270;&#39057;&#35270;&#35273;&#21464;&#25442;&#22120;(MC-ViViT)&#27169;&#22411;&#65292;&#36890;&#36807;&#20998;&#26512;&#38754;&#37096;&#29305;&#24449;&#21306;&#20998;MCI&#21644;&#27491;&#24120;&#35748;&#30693;&#12290;&#25968;&#25454;&#26469;&#33258;I-CONECT&#65292;&#19968;&#20010;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#39057;&#32321;&#35270;&#39057;&#32842;&#22825;&#26469;&#25913;&#21892;&#35748;&#30693;&#21151;&#33021;&#30340;&#34892;&#20026;&#24178;&#39044;&#35797;&#39564;&#12290;MC-ViViT&#22312;&#19968;&#20010;&#20998;&#25903;&#20013;&#25552;&#21462;&#35270;&#39057;&#30340;&#26102;&#31354;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;MC&#27169;&#22359;&#22686;&#24378;&#34920;&#31034;&#12290;&#30001;&#20110;I-CONECT&#25968;&#25454;&#38598;&#20013;&#30340;&#26679;&#26412;&#19981;&#24179;&#34913;&#38382;&#39064;&#65288;&#21253;&#21547;&#38590;&#26131;&#21644;&#27491;&#36127;&#26679;&#26412;&#65289;&#65292;&#36825;&#20351;MC-ViViT&#30340;&#24615;&#33021;&#21463;&#21040;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Hard-Easy&#21644;Positive-Negative&#26679;&#26412;&#30340;&#25439;&#22833;&#20989;&#25968;&#65288;HP Loss&#65289;&#26469;&#32467;&#21512;&#23545;&#27604;&#24230;&#35843;&#33410;&#25439;&#22833;Focal loss&#21644;AD-CORRE loss&#26469;&#35299;&#20915;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;I-CONECT&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20986;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep machine learning models including Convolutional Neural Networks (CNN) have been successful in the detection of Mild Cognitive Impairment (MCI) using medical images, questionnaires, and videos. This paper proposes a novel Multi-branch Classifier-Video Vision Transformer (MC-ViViT) model to distinguish MCI from those with normal cognition by analyzing facial features. The data comes from the I-CONECT, a behavioral intervention trial aimed at improving cognitive function by providing frequent video chats. MC-ViViT extracts spatiotemporal features of videos in one branch and augments representations by the MC module. The I-CONECT dataset is challenging as the dataset is imbalanced containing Hard-Easy and Positive-Negative samples, which impedes the performance of MC-ViViT. We propose a loss function for Hard-Easy and Positive-Negative Samples (HP Loss) by combining Focal loss and AD-CORRE loss to address the imbalanced problem. Our experimental results on the I-CONECT dataset show th
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#30740;&#31350;&#20102;&#20154;&#31867;&#21512;&#20316;&#26159;&#21542;&#22686;&#24378;&#20102;&#35782;&#21035;LLM&#29983;&#25104;&#30340;&#28145;&#24230;&#20266;&#36896;&#25991;&#26412;&#30340;&#20934;&#30830;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21512;&#20316;&#21487;&#20197;&#28508;&#22312;&#22320;&#25552;&#39640;&#20004;&#32452;&#20154;&#23545;&#28145;&#24230;&#20266;&#36896;&#25991;&#26412;&#30340;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.01002</link><description>&lt;p&gt;
&#20154;&#31867;&#21512;&#20316;&#26159;&#21542;&#22686;&#24378;&#20102;&#35782;&#21035;LLM&#29983;&#25104;&#30340;&#28145;&#24230;&#20266;&#36896;&#25991;&#26412;&#30340;&#20934;&#30830;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
Does Human Collaboration Enhance the Accuracy of Identifying LLM-Generated Deepfake Texts?. (arXiv:2304.01002v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01002
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#30740;&#31350;&#20102;&#20154;&#31867;&#21512;&#20316;&#26159;&#21542;&#22686;&#24378;&#20102;&#35782;&#21035;LLM&#29983;&#25104;&#30340;&#28145;&#24230;&#20266;&#36896;&#25991;&#26412;&#30340;&#20934;&#30830;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21512;&#20316;&#21487;&#20197;&#28508;&#22312;&#22320;&#25552;&#39640;&#20004;&#32452;&#20154;&#23545;&#28145;&#24230;&#20266;&#36896;&#25991;&#26412;&#30340;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT-4&#12289;LLaMA&#65289;&#30340;&#36827;&#23637;&#25913;&#21892;&#20102;&#22823;&#35268;&#27169;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#20889;&#20316;&#30340;&#36830;&#36143;&#21477;&#23376;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#25152;&#35859;&#30340;&#28145;&#24230;&#20266;&#36896;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#36827;&#23637;&#24341;&#21457;&#20102;&#23433;&#20840;&#21644;&#38544;&#31169;&#30340;&#25285;&#24551;&#65292;&#38656;&#35201;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#26469;&#21306;&#20998;&#28145;&#24230;&#20266;&#36896;&#25991;&#26412;&#21644;&#20154;&#31867;&#20070;&#20889;&#25991;&#26412;&#12290;&#23613;&#31649;&#20197;&#21069;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#31867;&#26816;&#27979;&#28145;&#24230;&#20266;&#36896;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#20294;&#27809;&#26377;&#30740;&#31350;&#8220;&#21512;&#20316;&#8221;&#26159;&#21542;&#33021;&#25552;&#39640;&#28145;&#24230;&#20266;&#36896;&#25991;&#26412;&#30340;&#26816;&#27979;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#20026;&#20102;&#22635;&#34917;&#23545;&#28145;&#24230;&#20266;&#36896;&#25991;&#26412;&#29702;&#35299;&#30340;&#31354;&#30333;&#65292;&#25105;&#20204;&#23545;&#20004;&#32452;&#20154;&#36827;&#34892;&#20102;&#23454;&#39564;&#65306;&#65288;1&#65289;&#26469;&#33258;AMT&#24179;&#21488;&#30340;&#38750;&#19987;&#23478;&#32676;&#20307;&#21644;&#65288;2&#65289;&#26469;&#33258;Upwork&#24179;&#21488;&#30340;&#20889;&#20316;&#19987;&#23478;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#31867;&#20043;&#38388;&#30340;&#21512;&#20316;&#21487;&#33021;&#20250;&#25552;&#39640;&#20004;&#32452;&#23545;&#28145;&#24230;&#20266;&#36896;&#25991;&#26412;&#30340;&#26816;&#27979;&#20934;&#30830;&#24615;&#65292;&#38750;&#19987;&#23478;&#32452;&#30340;&#26816;&#27979;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;6.36%&#65292;&#19987;&#23478;&#32452;&#30340;&#26816;&#27979;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;12.76%&#12290;
&lt;/p&gt;
&lt;p&gt;
Advances in Large Language Models (e.g., GPT-4, LLaMA) have improved the generation of coherent sentences resembling human writing on a large scale, resulting in the creation of so-called deepfake texts. However, this progress poses security and privacy concerns, necessitating effective solutions for distinguishing deepfake texts from human-written ones. Although prior works studied humans' ability to detect deepfake texts, none has examined whether "collaboration" among humans improves the detection of deepfake texts. In this study, to address this gap of understanding on deepfake texts, we conducted experiments with two groups: (1) nonexpert individuals from the AMT platform and (2) writing experts from the Upwork platform. The results demonstrate that collaboration among humans can potentially improve the detection of deepfake texts for both groups, increasing detection accuracies by 6.36% for non-experts and 12.76% for experts, respectively, compared to individuals' detection accur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986; Reflexion &#26041;&#27861;&#65292;&#32473;&#26234;&#33021;&#20307;&#36171;&#20104;&#20102;&#21160;&#24577;&#35760;&#24518;&#21644;&#33258;&#25105;&#21453;&#24605;&#33021;&#21147;&#65292;&#20197;&#22686;&#24378;&#20854;&#20219;&#21153;&#29305;&#23450;&#30340;&#34892;&#21160;&#36873;&#25321;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.11366</link><description>&lt;p&gt;
Reflexion&#65306;&#20855;&#26377;&#21160;&#24577;&#35760;&#24518;&#21644;&#33258;&#25105;&#21453;&#24605;&#30340;&#33258;&#20027;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Reflexion: an autonomous agent with dynamic memory and self-reflection. (arXiv:2303.11366v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986; Reflexion &#26041;&#27861;&#65292;&#32473;&#26234;&#33021;&#20307;&#36171;&#20104;&#20102;&#21160;&#24577;&#35760;&#24518;&#21644;&#33258;&#25105;&#21453;&#24605;&#33021;&#21147;&#65292;&#20197;&#22686;&#24378;&#20854;&#20219;&#21153;&#29305;&#23450;&#30340;&#34892;&#21160;&#36873;&#25321;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20915;&#31574;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#30340;&#21457;&#23637;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#20869;&#37096;&#27169;&#22411;&#24494;&#35843;&#12289;&#22806;&#37096;&#27169;&#22411;&#24494;&#35843;&#25110;&#22312;&#23450;&#20041;&#30340;&#29366;&#24577;&#31354;&#38388;&#19978;&#36827;&#34892;&#31574;&#30053;&#20248;&#21270;&#12290;&#30001;&#20110;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#25110;&#32570;&#20047;&#33391;&#22909;&#23450;&#20041;&#30340;&#29366;&#24577;&#31354;&#38388;&#65292;&#23454;&#29616;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#20250;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#20195;&#29702;&#27809;&#26377;&#20154;&#31867;&#20915;&#31574;&#36807;&#31243;&#22266;&#26377;&#30340;&#26576;&#20123;&#21697;&#36136;&#65292;&#29305;&#21035;&#26159;&#20174;&#38169;&#35823;&#20013;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#21453;&#24605;&#65292;&#20154;&#31867;&#21487;&#20197;&#36890;&#36807;&#35797;&#38169;&#36807;&#31243;&#39640;&#25928;&#22320;&#35299;&#20915;&#26032;&#30340;&#38382;&#39064;&#12290;&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986; Reflexion&#65292;&#19968;&#31181;&#23558;&#21160;&#24577;&#35760;&#24518;&#21644;&#33258;&#25105;&#21453;&#24605;&#33021;&#21147;&#36171;&#20104;&#26234;&#33021;&#20307;&#30340;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#20854;&#29616;&#26377;&#30340;&#25512;&#29702;&#36712;&#36857;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#34892;&#21160;&#36873;&#25321;&#33021;&#21147;&#12290;&#20026;&#20102;&#23454;&#29616;&#23436;&#20840;&#33258;&#21160;&#21270;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in decision-making large language model (LLM) agents have demonstrated impressive performance across various benchmarks. However, these state-of-the-art approaches typically necessitate internal model fine-tuning, external model fine-tuning, or policy optimization over a defined state space. Implementing these methods can prove challenging due to the scarcity of high-quality training data or the lack of well-defined state space. Moreover, these agents do not possess certain qualities inherent to human decision-making processes, specifically the ability to learn from mistakes. Self-reflection allows humans to efficiently solve novel problems through a process of trial and error. Building on recent research, we propose Reflexion, an approach that endows an agent with dynamic memory and self-reflection capabilities to enhance its existing reasoning trace and task-specific action choice abilities. To achieve full automation, we introduce a straightforward yet effective 
&lt;/p&gt;</description></item><item><title>RACCER&#26159;&#31532;&#19968;&#20010;&#38024;&#23545;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#34892;&#20026;&#29983;&#25104;&#23545;&#25239;&#20107;&#23454;&#35299;&#37322;&#30340;&#19987;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#32452;&#38024;&#23545;&#24378;&#21270;&#23398;&#20064;&#30340;&#29305;&#23450;&#23545;&#25239;&#20107;&#23454;&#23646;&#24615;&#65292;&#20445;&#35777;&#26131;&#20110;&#23454;&#29616;&#19988;&#20855;&#26377;&#39640;&#27010;&#29575;&#39044;&#26399;&#32467;&#26524;&#30340;&#23545;&#25239;&#20107;&#23454;&#12290;</title><link>http://arxiv.org/abs/2303.04475</link><description>&lt;p&gt;
RACCER: &#38754;&#21521;&#21487;&#36798;&#21644;&#30830;&#35777;&#30340;&#24378;&#21270;&#23398;&#20064;&#21487;&#36861;&#28335;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
RACCER: Towards Reachable and Certain Counterfactual Explanations for Reinforcement Learning. (arXiv:2303.04475v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04475
&lt;/p&gt;
&lt;p&gt;
RACCER&#26159;&#31532;&#19968;&#20010;&#38024;&#23545;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#34892;&#20026;&#29983;&#25104;&#23545;&#25239;&#20107;&#23454;&#35299;&#37322;&#30340;&#19987;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#32452;&#38024;&#23545;&#24378;&#21270;&#23398;&#20064;&#30340;&#29305;&#23450;&#23545;&#25239;&#20107;&#23454;&#23646;&#24615;&#65292;&#20445;&#35777;&#26131;&#20110;&#23454;&#29616;&#19988;&#20855;&#26377;&#39640;&#27010;&#29575;&#39044;&#26399;&#32467;&#26524;&#30340;&#23545;&#25239;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20854;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#20381;&#36182;&#20351;&#20854;&#34892;&#20026;&#38590;&#20197;&#29702;&#35299;&#21644;&#20449;&#20219;&#12290;&#23545;&#25239;&#20107;&#23454;&#35299;&#37322;&#26159;&#19968;&#31181;&#20154;&#24615;&#21270;&#30340;&#35299;&#37322;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#20851;&#20110;&#22914;&#20309;&#25913;&#21464;&#27169;&#22411;&#36755;&#20837;&#20197;&#36798;&#21040;&#39044;&#26399;&#36755;&#20986;&#30340;&#21487;&#34892;&#24314;&#35758;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#29983;&#25104;&#23545;&#25239;&#20107;&#23454;&#30340;&#26041;&#27861;&#24573;&#30053;&#20102;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#30340;&#38543;&#26426;&#24615;&#21644;&#39034;&#24207;&#24615;&#65292;&#21487;&#33021;&#20135;&#29983;&#38590;&#20197;&#33719;&#24471;&#25110;&#26080;&#27861;&#23454;&#29616;&#39044;&#26399;&#32467;&#26524;&#30340;&#23545;&#25239;&#20107;&#23454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RACCER&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#34892;&#20026;&#29983;&#25104;&#23545;&#25239;&#20107;&#23454;&#35299;&#37322;&#30340;&#39318;&#20010;&#19987;&#29992;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#24182;&#23454;&#29616;&#20102;&#19968;&#32452;&#38024;&#23545;&#24378;&#21270;&#23398;&#20064;&#30340;&#29305;&#23450;&#23545;&#25239;&#20107;&#23454;&#23646;&#24615;&#65292;&#30830;&#20445;&#26131;&#20110;&#23454;&#29616;&#19988;&#20855;&#26377;&#39640;&#27010;&#29575;&#39044;&#26399;&#32467;&#26524;&#30340;&#23545;&#25239;&#20107;&#23454;&#12290;&#25105;&#20204;&#20351;&#29992;&#21551;&#21457;&#24335;&#26641;&#25628;&#32034;&#20195;&#29702;&#25191;&#34892;&#36712;&#36857;&#65292;&#20197;&#25214;&#21040;&#26368;&#21512;&#36866;&#30340;&#23545;&#25239;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
While reinforcement learning (RL) algorithms have been successfully applied to numerous tasks, their reliance on neural networks makes their behavior difficult to understand and trust. Counterfactual explanations are human-friendly explanations that offer users actionable advice on how to alter the model inputs to achieve the desired output from a black-box system. However, current approaches to generating counterfactuals in RL ignore the stochastic and sequential nature of RL tasks and can produce counterfactuals that are difficult to obtain or do not deliver the desired outcome. In this work, we propose RACCER, the first RL-specific approach to generating counterfactual explanations for the behavior of RL agents. We first propose and implement a set of RL-specific counterfactual properties that ensure easily reachable counterfactuals with highly probable desired outcomes. We use a heuristic tree search of the agent's execution trajectories to find the most suitable counterfactuals ba
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;Machine Learning&#21644;Deep Learning&#26041;&#27861;&#24320;&#21457;&#20102;&#19968;&#20010;&#24555;&#36895;&#20934;&#30830;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#21033;&#29992;Mallat&#25955;&#23556;&#21464;&#25442;&#21644;&#20027;&#25104;&#20998;&#20998;&#26512;&#23545;&#30913;&#27969;&#20307;&#21147;&#23398;&#27169;&#25311;&#36827;&#34892;&#20102;&#21152;&#36895;&#65292;&#35782;&#21035;&#20986;&#24433;&#21709;&#36755;&#20986;&#32467;&#26524;&#30340;&#20851;&#38190;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2302.10243</link><description>&lt;p&gt;
&#22522;&#20110;Mallat&#25955;&#23556;&#21464;&#25442;&#30340;&#30913;&#27969;&#20307;&#21147;&#23398;&#30340;&#20195;&#29702;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Mallat Scattering Transformation based surrogate for MagnetoHydroDynamics. (arXiv:2302.10243v2 [physics.plasm-ph] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10243
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;Machine Learning&#21644;Deep Learning&#26041;&#27861;&#24320;&#21457;&#20102;&#19968;&#20010;&#24555;&#36895;&#20934;&#30830;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#21033;&#29992;Mallat&#25955;&#23556;&#21464;&#25442;&#21644;&#20027;&#25104;&#20998;&#20998;&#26512;&#23545;&#30913;&#27969;&#20307;&#21147;&#23398;&#27169;&#25311;&#36827;&#34892;&#20102;&#21152;&#36895;&#65292;&#35782;&#21035;&#20986;&#24433;&#21709;&#36755;&#20986;&#32467;&#26524;&#30340;&#20851;&#38190;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#24182;&#24212;&#29992;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20026;MagLIF&#32858;&#21464;&#20013;2D&#30005;&#38459;&#24615;&#30913;&#27969;&#20307;&#21147;&#23398;&#27169;&#25311;&#25552;&#20379;&#20102;&#39640;&#20445;&#30495;&#24230;&#12289;&#24555;&#36895;&#30340;&#20195;&#29702;&#27169;&#22411;&#12290;&#20351;&#29992;&#30005;&#38459;&#24615;&#30913;&#27969;&#20307;&#21147;&#23398;&#20195;&#30721;GORGON&#29983;&#25104;&#20102;&#19968;&#31995;&#21015;&#20855;&#26377;&#19981;&#21516;&#34924;&#22871;&#32437;&#27178;&#27604;&#12289;&#21021;&#22987;&#27668;&#20307;&#39044;&#28909;&#28201;&#24230;&#65288;&#21363;&#19981;&#21516;&#32477;&#28909;&#25351;&#25968;&#65289;&#21644;&#19981;&#21516;&#34924;&#22871;&#25200;&#21160;&#30340;&#32858;&#21464;&#27169;&#25311;&#12290;&#26681;&#25454;$x$&#12289;$y$&#21644;$t$&#29983;&#25104;&#20102;&#34924;&#22871;&#23494;&#24230;&#21644;&#30913;&#22330;&#12290;&#23545;&#20004;&#20010;&#22330;&#30340;&#23545;&#25968;&#24212;&#29992;&#20102;Mallat&#25955;&#23556;&#21464;&#25442;&#65292;&#24182;&#22312;&#23545;&#20004;&#20010;&#22330;&#30340;MST&#23545;&#25968;&#24212;&#29992;&#20102;&#20027;&#25104;&#20998;&#20998;&#26512;&#12290;&#23558;&#22330;&#25237;&#24433;&#21040;&#20027;&#25104;&#20998;&#20998;&#26512;&#21521;&#37327;&#19978;&#65292;&#24182;&#20445;&#30041;&#20854;&#20013;&#30340;&#19968;&#23567;&#37096;&#20998;&#20027;&#25104;&#20998;&#21521;&#37327;&#12290;&#23545;&#36755;&#20837;&#21442;&#25968;&#19982;&#22330;&#30340;MST&#23545;&#25968;&#36755;&#20986;&#30340;&#20132;&#21449;&#30456;&#20851;&#21644;SVD&#21521;&#37327;&#20998;&#37327;&#19982;PCA&#21521;&#37327;&#20998;&#37327;&#30340;&#20132;&#21449;&#30456;&#20851;&#36827;&#34892;&#20102;&#22855;&#24322;&#20540;&#20998;&#35299;&#12290;&#36825;&#20351;&#24471;&#33021;&#22815;&#35782;&#21035;&#20986;&#24433;&#21709;&#36755;&#20986;&#32467;&#26524;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Machine and Deep Learning methodology is developed and applied to give a high fidelity, fast surrogate for 2D resistive MHD simulations of MagLIF implosions. The resistive MHD code GORGON is used to generate an ensemble of implosions with different liner aspect ratios, initial gas preheat temperatures (that is, different adiabats), and different liner perturbations. The liner density and magnetic field as functions of $x$, $y$, and $t$ were generated. The Mallat Scattering Transformation (MST) is taken of the logarithm of both fields and a Principal Components Analysis is done on the logarithm of the MST of both fields. The fields are projected onto the PCA vectors and a small number of these PCA vector components are kept. Singular Value Decompositions of the cross correlation of the input parameters to the output logarithm of the MST of the fields, and of the cross correlation of the SVD vector components to the PCA vector components are done. This allows the identification of the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23618;&#32423;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#30693;&#35782;&#20256;&#36755;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#27809;&#26377;&#20808;&#39564;&#30693;&#35782;&#30340;&#20219;&#21153;&#30456;&#20284;&#24615;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#24378;&#22823;&#30340;&#30693;&#35782;&#20256;&#36755;&#12290;</title><link>http://arxiv.org/abs/2302.05534</link><description>&lt;p&gt;
&#24378;&#22823;&#30340;&#23618;&#32423;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#30693;&#35782;&#20256;&#36755;
&lt;/p&gt;
&lt;p&gt;
Robust Knowledge Transfer in Tiered Reinforcement Learning. (arXiv:2302.05534v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23618;&#32423;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#30693;&#35782;&#20256;&#36755;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#27809;&#26377;&#20808;&#39564;&#30693;&#35782;&#30340;&#20219;&#21153;&#30456;&#20284;&#24615;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#24378;&#22823;&#30340;&#30693;&#35782;&#20256;&#36755;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23618;&#32423;&#22686;&#24378;&#23398;&#20064;&#35774;&#32622;&#65292;&#36825;&#26159;&#19968;&#20010;&#24182;&#34892;&#20256;&#36755;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#30446;&#26631;&#26159;&#23558;&#30693;&#35782;&#20174;&#20302;&#23618;&#65288;&#28304;&#65289;&#20219;&#21153;&#20256;&#36755;&#21040;&#39640;&#23618;&#65288;&#30446;&#26631;&#65289;&#20219;&#21153;&#65292;&#20197;&#20943;&#23569;&#21518;&#32773;&#30340;&#25506;&#32034;&#39118;&#38505;&#65292;&#21516;&#26102;&#24182;&#34892;&#35299;&#20915;&#36825;&#20004;&#20010;&#20219;&#21153;&#12290;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#19981;&#20551;&#35774;&#20302;&#23618;&#21644;&#39640;&#23618;&#20219;&#21153;&#20849;&#20139;&#30456;&#21516;&#30340;&#21160;&#24577;&#25110;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#19988;&#19987;&#27880;&#20110;&#22312;&#27809;&#26377;&#20808;&#39564;&#30693;&#35782;&#30340;&#20219;&#21153;&#30456;&#20284;&#24615;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#24378;&#22823;&#30340;&#30693;&#35782;&#20256;&#36755;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20010;&#31216;&#20026;&#8220;&#26368;&#20248;&#20540;&#25903;&#37197;&#8221;&#30340;&#33258;&#28982;&#32780;&#24517;&#35201;&#30340;&#26465;&#20214;&#65292;&#36866;&#29992;&#20110;&#25105;&#20204;&#30340;&#30446;&#26631;&#12290;&#22312;&#36825;&#20010;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#24471;&#23545;&#20110;&#39640;&#23618;&#20219;&#21153;&#65292;&#22312;&#37096;&#20998;&#29366;&#24577;&#19978;&#21487;&#20197;&#23454;&#29616;&#24658;&#23450;&#30340;&#36951;&#25022;&#65292;&#36825;&#21462;&#20915;&#20110;&#20219;&#21153;&#30456;&#20284;&#24615;&#65292;&#24182;&#22312;&#20004;&#20010;&#20219;&#21153;&#19981;&#30456;&#20284;&#26102;&#20445;&#25345;&#25509;&#36817;&#26368;&#20248;&#36951;&#25022;&#65307;&#32780;&#23545;&#20110;&#20302;&#23618;&#20219;&#21153;&#65292;&#23427;&#21487;&#20197;&#22312;&#19981;&#20570;&#20986;&#29306;&#29298;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#25509;&#36817;&#26368;&#20248;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#20855;&#26377;&#22810;&#20010;&#20302;&#23618;&#20219;&#21153;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the Tiered Reinforcement Learning setting, a parallel transfer learning framework, where the goal is to transfer knowledge from the low-tier (source) task to the high-tier (target) task to reduce the exploration risk of the latter while solving the two tasks in parallel. Unlike previous work, we do not assume the low-tier and high-tier tasks share the same dynamics or reward functions, and focus on robust knowledge transfer without prior knowledge on the task similarity. We identify a natural and necessary condition called the ``Optimal Value Dominance'' for our objective. Under this condition, we propose novel online learning algorithms such that, for the high-tier task, it can achieve constant regret on partial states depending on the task similarity and retain near-optimal regret when the two tasks are dissimilar, while for the low-tier task, it can keep near-optimal without making sacrifice. Moreover, we further study the setting with multiple low-tier tasks
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36328;&#35821;&#35328;&#25688;&#35201;&#20013;&#30340;&#8220;&#32763;&#35793;&#35821;&#35328;&#8221;&#29616;&#35937;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#19981;&#21516;&#26041;&#27861;&#26500;&#24314;&#25968;&#25454;&#38598;&#20250;&#23548;&#33268;&#19981;&#21516;&#31243;&#24230;&#30340;&#32763;&#35793;&#35821;&#35328;&#29616;&#35937;&#12290;&#25991;&#31456;&#36824;&#21457;&#29616;&#32763;&#35793;&#35821;&#35328;&#20250;&#23545;&#27169;&#22411;&#30340;&#35780;&#20272;&#21644;&#24615;&#33021;&#20135;&#29983;&#24433;&#21709;&#65292;&#21253;&#25324;&#27979;&#35797;&#38598;&#20013;&#30340;&#32763;&#35793;&#35821;&#35328;&#21487;&#33021;&#23548;&#33268;&#20154;&#24037;&#21028;&#26029;&#19982;&#33258;&#21160;&#35780;&#20272;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20197;&#21450;&#35757;&#32451;&#38598;&#20013;&#30340;&#32763;&#35793;&#35821;&#35328;&#20250;&#24433;&#21709;&#27169;&#22411;&#30340;&#35757;&#32451;&#34920;&#29616;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.07220</link><description>&lt;p&gt;
&#29702;&#35299;&#36328;&#35821;&#35328;&#25688;&#35201;&#20013;&#30340;&#8220;&#32763;&#35793;&#35821;&#35328;&#8221;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
Understanding Translationese in Cross-Lingual Summarization. (arXiv:2212.07220v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36328;&#35821;&#35328;&#25688;&#35201;&#20013;&#30340;&#8220;&#32763;&#35793;&#35821;&#35328;&#8221;&#29616;&#35937;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#19981;&#21516;&#26041;&#27861;&#26500;&#24314;&#25968;&#25454;&#38598;&#20250;&#23548;&#33268;&#19981;&#21516;&#31243;&#24230;&#30340;&#32763;&#35793;&#35821;&#35328;&#29616;&#35937;&#12290;&#25991;&#31456;&#36824;&#21457;&#29616;&#32763;&#35793;&#35821;&#35328;&#20250;&#23545;&#27169;&#22411;&#30340;&#35780;&#20272;&#21644;&#24615;&#33021;&#20135;&#29983;&#24433;&#21709;&#65292;&#21253;&#25324;&#27979;&#35797;&#38598;&#20013;&#30340;&#32763;&#35793;&#35821;&#35328;&#21487;&#33021;&#23548;&#33268;&#20154;&#24037;&#21028;&#26029;&#19982;&#33258;&#21160;&#35780;&#20272;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20197;&#21450;&#35757;&#32451;&#38598;&#20013;&#30340;&#32763;&#35793;&#35821;&#35328;&#20250;&#24433;&#21709;&#27169;&#22411;&#30340;&#35757;&#32451;&#34920;&#29616;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36328;&#35821;&#35328;&#25688;&#35201;&#65288;CLS&#65289;&#20013;&#65292;&#32473;&#23450;&#19968;&#31687;&#28304;&#35821;&#35328;&#25991;&#26723;&#65292;&#26088;&#22312;&#29983;&#25104;&#19968;&#31687;&#31616;&#27905;&#30340;&#30446;&#26631;&#35821;&#35328;&#25688;&#35201;&#12290;&#19982;&#21333;&#35821;&#25688;&#35201;&#65288;MS&#65289;&#19981;&#21516;&#65292;&#33258;&#28982;&#20986;&#29616;&#30340;&#28304;&#35821;&#35328;&#25991;&#26723;&#37197;&#23545;&#30446;&#26631;&#35821;&#35328;&#25688;&#35201;&#24182;&#19981;&#24120;&#35265;&#12290;&#20026;&#20102;&#25910;&#38598;&#22823;&#35268;&#27169;&#30340;CLS&#25968;&#25454;&#65292;&#29616;&#26377;&#25968;&#25454;&#38598;&#36890;&#24120;&#22312;&#21019;&#24314;&#36807;&#31243;&#20013;&#28041;&#21450;&#32763;&#35793;&#12290;&#28982;&#32780;&#65292;&#32763;&#35793;&#25991;&#26412;&#19982;&#21407;&#22987;&#35821;&#35328;&#25991;&#26412;&#20043;&#38388;&#23384;&#22312;&#30528;&#21306;&#21035;&#65292;&#21363;&#32763;&#35793;&#35821;&#35328;&#12290;&#26412;&#25991;&#39318;&#20808;&#30830;&#35748;&#20102;&#26500;&#24314;CLS&#25968;&#25454;&#38598;&#30340;&#19981;&#21516;&#26041;&#27861;&#23558;&#23548;&#33268;&#19981;&#21516;&#31243;&#24230;&#30340;&#32763;&#35793;&#35821;&#35328;&#29616;&#35937;&#12290;&#28982;&#21518;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#32763;&#35793;&#35821;&#35328;&#22312;&#28304;&#25991;&#26723;&#25110;&#30446;&#26631;&#25688;&#35201;&#20013;&#20986;&#29616;&#26102;&#22914;&#20309;&#24433;&#21709;CLS&#27169;&#22411;&#30340;&#35780;&#20272;&#21644;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#65288;1&#65289;&#27979;&#35797;&#38598;&#20013;&#25991;&#26723;&#25110;&#25688;&#35201;&#20013;&#30340;&#32763;&#35793;&#35821;&#35328;&#21487;&#33021;&#23548;&#33268;&#20154;&#24037;&#21028;&#26029;&#19982;&#33258;&#21160;&#35780;&#20272;&#20043;&#38388;&#30340;&#24046;&#24322;&#65307;&#65288;2&#65289;&#35757;&#32451;&#38598;&#20013;&#30340;&#32763;&#35793;&#35821;&#35328;&#20250;&#24433;&#21709;&#27169;&#22411;&#30340;&#35757;&#32451;&#34920;&#29616;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a document in a source language, cross-lingual summarization (CLS) aims at generating a concise summary in a different target language. Unlike monolingual summarization (MS), naturally occurring source-language documents paired with target-language summaries are rare. To collect large-scale CLS data, existing datasets typically involve translation in their creation. However, the translated text is distinguished from the text originally written in that language, i.e., translationese. In this paper, we first confirm that different approaches of constructing CLS datasets will lead to different degrees of translationese. Then we systematically investigate how translationese affects CLS model evaluation and performance when it appears in source documents or target summaries. In detail, we find that (1) the translationese in documents or summaries of test sets might lead to the discrepancy between human judgment and automatic evaluation; (2) the translationese in training sets would ha
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24341;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65288;KGRL&#65289;&#65292;&#36890;&#36807;&#34701;&#21512;&#22810;&#20010;&#30693;&#35782;&#31574;&#30053;&#24182;&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#23454;&#29616;&#20102;&#28789;&#27963;&#30340;&#30693;&#35782;&#37325;&#26032;&#25490;&#21015;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2210.03729</link><description>&lt;p&gt;
&#28789;&#27963;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22810;&#31574;&#30053;&#34701;&#21512;&#29992;&#20110;&#39640;&#25928;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Flexible Attention-Based Multi-Policy Fusion for Efficient Deep Reinforcement Learning. (arXiv:2210.03729v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03729
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24341;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65288;KGRL&#65289;&#65292;&#36890;&#36807;&#34701;&#21512;&#22810;&#20010;&#30693;&#35782;&#31574;&#30053;&#24182;&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#23454;&#29616;&#20102;&#28789;&#27963;&#30340;&#30693;&#35782;&#37325;&#26032;&#25490;&#21015;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064; (RL) &#20195;&#29702;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#33268;&#21147;&#20110;&#25509;&#36817;&#20154;&#31867;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;&#20154;&#31867;&#26159;&#20248;&#31168;&#30340;&#35266;&#23519;&#32773;&#65292;&#21487;&#20197;&#36890;&#36807;&#32858;&#21512;&#26469;&#33258;&#21508;&#31181;&#26469;&#28304;&#30340;&#22806;&#37096;&#30693;&#35782;&#65288;&#21253;&#25324;&#20182;&#20154;&#22312;&#23581;&#35797;&#20219;&#21153;&#26102;&#30340;&#35266;&#23519;&#65289;&#26469;&#23398;&#20064;&#12290;&#20043;&#21069;&#22312;RL&#20013;&#30340;&#30740;&#31350;&#24050;&#32463;&#23558;&#22806;&#37096;&#30693;&#35782;&#31574;&#30053;&#32467;&#21512;&#21040;&#20195;&#29702;&#20013;&#65292;&#20197;&#24110;&#21161;&#20854;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#25191;&#34892;&#20219;&#24847;&#32452;&#21512;&#21644;&#26367;&#25442;&#36825;&#20123;&#31574;&#30053;&#20173;&#28982;&#26159;&#38750;&#24179;&#20961;&#30340;&#65292;&#36825;&#26159;&#27867;&#21270;&#21644;&#21487;&#36716;&#31227;&#24615;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30693;&#35782;&#24341;&#23548;&#30340;RL(KGRL)&#65292;&#36825;&#26159;&#19968;&#31181;&#34701;&#21512;&#22810;&#20010;&#30693;&#35782;&#31574;&#30053;&#24182;&#26088;&#22312;&#23454;&#29616;&#20154;&#31867;&#23398;&#20064;&#25928;&#29575;&#21644;&#28789;&#27963;&#24615;&#30340;RL&#33539;&#24335;&#12290;&#25105;&#20204;&#20026;KGRL&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28436;&#21592;&#26550;&#26500;&#65292;&#21363;&#30693;&#35782;&#21253;&#23481;&#24615;&#27880;&#24847;&#32593;&#32476;(KIAN)&#65292;&#35813;&#32593;&#32476;&#36890;&#36807;&#22522;&#20110;&#23884;&#20837;&#30340;&#27880;&#24847;&#21147;&#34892;&#21160;&#39044;&#27979;&#23454;&#29616;&#20102;&#33258;&#30001;&#30340;&#30693;&#35782;&#37325;&#26032;&#25490;&#21015;&#12290;KIAN&#36824;&#35299;&#20915;&#20102;&#29109;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#36825;&#26159;&#22312;&#26368;&#22823;&#29109;KGRL&#20013;&#20986;&#29616;&#30340;&#38382;&#39064;&#65292;&#38459;&#30861;&#20102;&#20195;&#29702;&#30340;&#39640;&#25928;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) agents have long sought to approach the efficiency of human learning. Humans are great observers who can learn by aggregating external knowledge from various sources, including observations from others' policies of attempting a task. Prior studies in RL have incorporated external knowledge policies to help agents improve sample efficiency. However, it remains non-trivial to perform arbitrary combinations and replacements of those policies, an essential feature for generalization and transferability. In this work, we present Knowledge-Grounded RL (KGRL), an RL paradigm fusing multiple knowledge policies and aiming for human-like efficiency and flexibility. We propose a new actor architecture for KGRL, Knowledge-Inclusive Attention Network (KIAN), which allows free knowledge rearrangement due to embedding-based attentive action prediction. KIAN also addresses entropy imbalance, a problem arising in maximum entropy KGRL that hinders an agent from efficiently ex
&lt;/p&gt;</description></item><item><title>Rank-N-Contrast&#26159;&#19968;&#31181;&#23398;&#20064;&#36830;&#32493;&#34920;&#31034;&#30340;&#22238;&#24402;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#26679;&#26412;&#22312;&#30446;&#26631;&#31354;&#38388;&#20013;&#30340;&#25490;&#21517;&#36827;&#34892;&#27604;&#36739;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#20154;&#26426;&#20132;&#20114;&#21644;&#21307;&#30103;&#20445;&#20581;&#31561;&#39046;&#22495;&#30340;&#22238;&#24402;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.01189</link><description>&lt;p&gt;
Rank-N-Contrast:&#20026;&#22238;&#24402;&#38382;&#39064;&#23398;&#20064;&#36830;&#32493;&#34920;&#31034;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rank-N-Contrast: Learning Continuous Representations for Regression. (arXiv:2210.01189v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01189
&lt;/p&gt;
&lt;p&gt;
Rank-N-Contrast&#26159;&#19968;&#31181;&#23398;&#20064;&#36830;&#32493;&#34920;&#31034;&#30340;&#22238;&#24402;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#26679;&#26412;&#22312;&#30446;&#26631;&#31354;&#38388;&#20013;&#30340;&#25490;&#21517;&#36827;&#34892;&#27604;&#36739;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#20154;&#26426;&#20132;&#20114;&#21644;&#21307;&#30103;&#20445;&#20581;&#31561;&#39046;&#22495;&#30340;&#22238;&#24402;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#22238;&#24402;&#27169;&#22411;&#36890;&#24120;&#22312;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#19979;&#23398;&#20064;&#65292;&#27809;&#26377;&#26126;&#30830;&#24378;&#35843;&#22238;&#24402;&#24863;&#30693;&#30340;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#34920;&#29616;&#20986;&#20998;&#25955;&#24615;&#65292;&#26080;&#27861;&#25429;&#25417;&#26679;&#26412;&#39034;&#24207;&#30340;&#36830;&#32493;&#24615;&#65292;&#23548;&#33268;&#24191;&#27867;&#30340;&#22238;&#24402;&#20219;&#21153;&#20013;&#20135;&#29983;&#27425;&#20248;&#32467;&#26524;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Rank-N-Contrast&#65288;RNC&#65289;&#65292;&#19968;&#31181;&#36890;&#36807;&#23545;&#26679;&#26412;&#22312;&#30446;&#26631;&#31354;&#38388;&#20013;&#30340;&#25490;&#21517;&#36827;&#34892;&#27604;&#36739;&#26469;&#23398;&#20064;&#22238;&#24402;&#30340;&#36830;&#32493;&#34920;&#31034;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#19978;&#35777;&#26126;&#20102;RNC&#21487;&#20197;&#20445;&#35777;&#25152;&#23398;&#34920;&#31034;&#30340;&#39034;&#24207;&#19982;&#30446;&#26631;&#39034;&#24207;&#30456;&#31526;&#65292;&#19981;&#20165;&#24615;&#33021;&#26356;&#22909;&#65292;&#32780;&#19988;&#40065;&#26834;&#24615;&#12289;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#37117;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#39640;&#12290;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#20154;&#26426;&#20132;&#20114;&#21644;&#21307;&#30103;&#20445;&#20581;&#31561;&#39046;&#22495;&#20351;&#29992;&#20116;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#22238;&#24402;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;RNC&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#31361;&#26174;&#20102;&#20854;&#20869;&#22312;&#30340;&#21019;&#26032;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep regression models typically learn in an end-to-end fashion without explicitly emphasizing a regression-aware representation. Consequently, the learned representations exhibit fragmentation and fail to capture the continuous nature of sample orders, inducing suboptimal results across a wide range of regression tasks. To fill the gap, we propose Rank-N-Contrast (RNC), a framework that learns continuous representations for regression by contrasting samples against each other based on their rankings in the target space. We demonstrate, theoretically and empirically, that RNC guarantees the desired order of learned representations in accordance with the target orders, enjoying not only better performance but also significantly improved robustness, efficiency, and generalization. Extensive experiments using five real-world regression datasets that span computer vision, human-computer interaction, and healthcare verify that RNC achieves state-of-the-art performance, highlighting its intr
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20272;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37319;&#26679;&#31243;&#24207;&#27491;&#30830;&#24615;&#27010;&#29575;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#26679;&#20505;&#36873;&#31243;&#24207;&#21644;&#20505;&#36873;&#35859;&#35789;&#26469;&#39044;&#27979;&#31243;&#24207;&#30340;&#27491;&#30830;&#24615;&#65292;&#24182;&#25512;&#26029;&#20986;&#20851;&#20110;&#29983;&#25104;&#20195;&#30721;&#34892;&#20026;&#35299;&#37322;&#30340;&#26377;&#29992;&#35859;&#35789;&#12290;</title><link>http://arxiv.org/abs/2210.00848</link><description>&lt;p&gt;
&#26397;&#30528;&#21487;&#20449;&#30340;&#31070;&#32463;&#31243;&#24207;&#21512;&#25104;&#20043;&#36335;
&lt;/p&gt;
&lt;p&gt;
Toward Trustworthy Neural Program Synthesis. (arXiv:2210.00848v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00848
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20272;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37319;&#26679;&#31243;&#24207;&#27491;&#30830;&#24615;&#27010;&#29575;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#26679;&#20505;&#36873;&#31243;&#24207;&#21644;&#20505;&#36873;&#35859;&#35789;&#26469;&#39044;&#27979;&#31243;&#24207;&#30340;&#27491;&#30830;&#24615;&#65292;&#24182;&#25512;&#26029;&#20986;&#20851;&#20110;&#29983;&#25104;&#20195;&#30721;&#34892;&#20026;&#35299;&#37322;&#30340;&#26377;&#29992;&#35859;&#35789;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37319;&#26679;&#30340;&#31243;&#24207;&#27491;&#30830;&#24615;&#27010;&#29575;&#30340;&#26041;&#27861;&#12290;&#32473;&#23450;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#32534;&#31243;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26082;&#37319;&#26679;&#20505;&#36873;&#31243;&#24207;&#65292;&#21448;&#37319;&#26679;&#20505;&#36873;&#35859;&#35789;&#26469;&#25351;&#23450;&#31243;&#24207;&#30340;&#34892;&#20026;&#12290;&#36825;&#26679;&#21487;&#20197;&#23398;&#20064;&#19968;&#20010;&#33021;&#22815;&#29983;&#25104;&#33391;&#22909;&#26657;&#20934;&#31243;&#24207;&#27491;&#30830;&#24615;&#27010;&#29575;&#39044;&#27979;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#36824;&#25512;&#26029;&#20986;&#29983;&#25104;&#20195;&#30721;&#34892;&#20026;&#35299;&#37322;&#20013;&#26377;&#29992;&#30340;&#35859;&#35789;&#65292;&#24182;&#19988;&#22312;&#20154;&#31867;&#23454;&#39564;&#20013;&#65292;&#20154;&#20204;&#26356;&#20559;&#22909;&#36825;&#20123;&#35859;&#35789;&#32780;&#19981;&#26159;&#21407;&#22987;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31616;&#21333;&#12289;&#26131;&#20110;&#23454;&#29616;&#65292;&#24182;&#19988;&#20445;&#25345;&#20102;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#20934;&#30830;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop an approach to estimate the probability that a program sampled from a large language model is correct. Given a natural language description of a programming problem, our method samples both candidate programs as well as candidate predicates specifying how the program should behave. This allows learning a model that forms a well-calibrated probabilistic prediction of program correctness. Our system also infers which predicates are useful to explain the behavior of the generated code, and humans preferred these in a human study over raw language model outputs. Our method is simple, easy to implement, and maintains state of the art generation accuracy results.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#25552;&#39640;&#26102;&#38388;&#24207;&#21015;&#20013;&#39046;&#22495;&#29305;&#23450;&#30340;&#22240;&#26524;&#21457;&#29616;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#39046;&#22495;&#19978;&#26126;&#26174;&#20248;&#20110;&#20154;&#31867;&#35774;&#35745;&#30340;&#36890;&#29992;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2209.05598</link><description>&lt;p&gt;
&#20174;&#26102;&#38388;&#24207;&#21015;&#20013;&#23398;&#20064;&#39046;&#22495;&#29305;&#23450;&#30340;&#22240;&#26524;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Learning domain-specific causal discovery from time series. (arXiv:2209.05598v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#25552;&#39640;&#26102;&#38388;&#24207;&#21015;&#20013;&#39046;&#22495;&#29305;&#23450;&#30340;&#22240;&#26524;&#21457;&#29616;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#39046;&#22495;&#19978;&#26126;&#26174;&#20248;&#20110;&#20154;&#31867;&#35774;&#35745;&#30340;&#36890;&#29992;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#21457;&#29616;&#26159;&#31070;&#32463;&#31185;&#23398;&#12289;&#21307;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#22240;&#26524;&#21457;&#29616;&#30340;&#25216;&#26415;&#21253;&#25324;&#38543;&#26426;&#23454;&#39564;&#21644;&#22522;&#20110;&#26684;&#20848;&#26480;&#22240;&#26524;&#24615;&#12289;&#26465;&#20214;&#29420;&#31435;&#24615;&#12289;&#32467;&#26500;&#26041;&#31243;&#20197;&#21450;&#35780;&#20998;&#26041;&#27861;&#31561;&#31639;&#27861;&#65292;&#28982;&#32780;&#36825;&#20123;&#26041;&#27861;&#21482;&#22312;&#20154;&#31867;&#35774;&#35745;&#32773;&#20570;&#20986;&#24378;&#20551;&#35774;&#26102;&#25165;&#20934;&#30830;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#25552;&#39640;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#39046;&#22495;&#29305;&#23450;&#22240;&#26524;&#21457;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;MOS 6502&#24494;&#22788;&#29702;&#22120;&#12289;NetSim fMRI&#25968;&#25454;&#38598;&#21644;Dream3&#22522;&#22240;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#20154;&#31867;&#35774;&#35745;&#30340;&#39046;&#22495;&#36890;&#29992;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65292;&#22914;&#20114;&#20449;&#24687;&#12289;VAR-LiNGAM&#21644;&#26684;&#20848;&#26480;&#22240;&#26524;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#21487;&#34892;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#26041;&#27861;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#25913;&#36827;&#22240;&#26524;&#21457;&#29616;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal discovery (CD) from time-varying data is important in neuroscience, medicine, and machine learning. Techniques for CD encompass randomized experiments, which are generally unbiased but expensive, and algorithms such as Granger causality, conditional-independence-based, structural-equation-based, and score-based methods that are only accurate under strong assumptions made by human designers. However, as demonstrated in other areas of machine learning, human expertise is often not entirely accurate and tends to be outperformed in domains with abundant data. In this study, we examine whether we can enhance domain-specific causal discovery for time series using a data-driven approach. Our findings indicate that this procedure significantly outperforms human-designed, domain-agnostic causal discovery methods, such as Mutual Information, VAR-LiNGAM, and Granger Causality on the MOS 6502 microprocessor, the NetSim fMRI dataset, and the Dream3 gene dataset. We argue that, when feasible,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26368;&#36817;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#28145;&#20266;&#36896;&#20869;&#23481;&#26816;&#27979;&#30340;&#30740;&#31350;&#65292;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#19981;&#21516;&#31867;&#21035;&#30340;&#20266;&#36896;&#20869;&#23481;&#26816;&#27979;&#65292;&#24182;&#25253;&#21578;&#20102;&#25152;&#32771;&#23519;&#24037;&#20316;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#20197;&#21450;&#28145;&#20266;&#36896;&#26816;&#27979;&#39046;&#22495;&#20173;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#21644;&#19981;&#36275;&#20043;&#22788;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2202.06095</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#28145;&#20266;&#36896;&#20869;&#23481;&#26816;&#27979;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Review of Deep Learning-based Approaches for Deepfake Content Detection. (arXiv:2202.06095v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.06095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26368;&#36817;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#28145;&#20266;&#36896;&#20869;&#23481;&#26816;&#27979;&#30340;&#30740;&#31350;&#65292;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#19981;&#21516;&#31867;&#21035;&#30340;&#20266;&#36896;&#20869;&#23481;&#26816;&#27979;&#65292;&#24182;&#25253;&#21578;&#20102;&#25152;&#32771;&#23519;&#24037;&#20316;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#20197;&#21450;&#28145;&#20266;&#36896;&#26816;&#27979;&#39046;&#22495;&#20173;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#21644;&#19981;&#36275;&#20043;&#22788;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#25285;&#24551;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#21019;&#24314;&#39640;&#24230;&#36924;&#30495;&#30340;&#20266;&#36896;&#22270;&#20687;&#21644;&#35270;&#39057;&#12290;&#36825;&#23545;&#20154;&#20204;&#30340;&#23436;&#25972;&#24615;&#26500;&#25104;&#23041;&#32961;&#65292;&#21487;&#33021;&#23548;&#33268;&#31038;&#20250;&#19981;&#31283;&#23450;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36843;&#20999;&#38656;&#35201;&#24320;&#21457;&#26032;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#33021;&#22815;&#26377;&#25928;&#26816;&#27979;&#20266;&#36896;&#20869;&#23481;&#65292;&#24182;&#25552;&#37266;&#29992;&#25143;&#21487;&#33021;&#30340;&#22270;&#20687;&#21644;&#35270;&#39057;&#31713;&#25913;&#12290;&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#28145;&#20266;&#36896;&#20869;&#23481;&#26816;&#27979;&#30340;&#26368;&#26032;&#30740;&#31350;&#12290;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#31995;&#32479;&#22320;&#23457;&#35270;&#19981;&#21516;&#31867;&#21035;&#30340;&#20266;&#36896;&#20869;&#23481;&#26816;&#27979;&#26469;&#25299;&#23485;&#26368;&#26032;&#30740;&#31350;&#30340;&#21069;&#27839;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25253;&#21578;&#20102;&#25152;&#32771;&#23519;&#24037;&#20316;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#20197;&#21450;&#28145;&#20266;&#36896;&#26816;&#27979;&#39046;&#22495;&#20173;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#21644;&#19981;&#36275;&#20043;&#22788;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in deep learning generative models have raised concerns as they can create highly convincing counterfeit images and videos. This poses a threat to people's integrity and can lead to social instability. To address this issue, there is a pressing need to develop new computational models that can efficiently detect forged content and alert users to potential image and video manipulations. This paper presents a comprehensive review of recent studies for deepfake content detection using deep learning-based approaches. We aim to broaden the state-of-the-art research by systematically reviewing the different categories of fake content detection. Furthermore, we report the advantages and drawbacks of the examined works and future directions towards the issues and shortcomings still unsolved on deepfake detection.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#25968;&#23398;&#20998;&#26512;&#35777;&#26126;&#20102;&#38750;&#25903;&#37197;&#25490;&#24207;&#36951;&#20256;&#31639;&#27861;II&#65288;NSGA-II&#65289;&#30340;&#36816;&#34892;&#26102;&#38388;&#29305;&#24615;&#65292;&#21457;&#29616;&#24403;&#31181;&#32676;&#35268;&#27169;&#26159;&#24085;&#32047;&#25176;&#21069;&#27839;&#22823;&#23567;&#30340;&#22235;&#20493;&#26102;&#65292;NSGA-II&#20855;&#26377;&#19982;&#20854;&#20182;&#31639;&#27861;&#30456;&#21516;&#30340;&#36816;&#34892;&#26102;&#38388;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2112.08581</link><description>&lt;p&gt;
&#38750;&#25903;&#37197;&#25490;&#24207;&#36951;&#20256;&#31639;&#27861;II&#65288;NSGA-II&#65289;&#30340;&#25968;&#23398;&#36816;&#34892;&#26102;&#38388;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Mathematical Runtime Analysis for the Non-Dominated Sorting Genetic Algorithm II (NSGA-II). (arXiv:2112.08581v5 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.08581
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#25968;&#23398;&#20998;&#26512;&#35777;&#26126;&#20102;&#38750;&#25903;&#37197;&#25490;&#24207;&#36951;&#20256;&#31639;&#27861;II&#65288;NSGA-II&#65289;&#30340;&#36816;&#34892;&#26102;&#38388;&#29305;&#24615;&#65292;&#21457;&#29616;&#24403;&#31181;&#32676;&#35268;&#27169;&#26159;&#24085;&#32047;&#25176;&#21069;&#27839;&#22823;&#23567;&#30340;&#22235;&#20493;&#26102;&#65292;NSGA-II&#20855;&#26377;&#19982;&#20854;&#20182;&#31639;&#27861;&#30456;&#21516;&#30340;&#36816;&#34892;&#26102;&#38388;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#25903;&#37197;&#25490;&#24207;&#36951;&#20256;&#31639;&#27861;II&#65288;NSGA-II&#65289;&#26159;&#30446;&#21069;&#23454;&#38469;&#24212;&#29992;&#20013;&#26368;&#24120;&#29992;&#30340;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#65288;MOEA&#65289;&#12290;&#28982;&#32780;&#65292;&#19982;&#19968;&#20123;&#31616;&#21333;&#30340;MOEA&#30340;&#25968;&#23398;&#20998;&#26512;&#30456;&#21453;&#65292;NSGA-II&#33267;&#20170;&#27809;&#26377;&#36827;&#34892;&#36807;&#36825;&#26679;&#30340;&#30740;&#31350;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;NSGA-II&#36827;&#34892;&#25968;&#23398;&#36816;&#34892;&#26102;&#38388;&#20998;&#26512;&#26159;&#21487;&#34892;&#30340;&#12290;&#20316;&#20026;&#29305;&#23450;&#32467;&#26524;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#31181;&#32676;&#35268;&#27169;&#26159;&#24085;&#32047;&#25176;&#21069;&#27839;&#22823;&#23567;&#30340;&#22235;&#20493;&#26102;&#65292;NSGA-II&#20351;&#29992;&#20004;&#31181;&#32463;&#20856;&#21464;&#24322;&#31639;&#23376;&#21644;&#22235;&#31181;&#19981;&#21516;&#30340;&#29238;&#20195;&#36873;&#25321;&#26041;&#27861;&#19982;SEMO&#21644;GSEMO&#31639;&#27861;&#22312;&#22522;&#26412;&#30340;OneMinMax&#21644;LeadingOnesTrailingZeros&#22522;&#20934;&#27979;&#35797;&#19978;&#20855;&#26377;&#30456;&#21516;&#30340;&#28176;&#36817;&#36816;&#34892;&#26102;&#38388;&#20445;&#35777;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#31181;&#32676;&#35268;&#27169;&#21482;&#31561;&#20110;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#22823;&#23567;&#65292;&#21017;NSGA-II&#26080;&#27861;&#39640;&#25928;&#22320;&#35745;&#31639;&#23436;&#25972;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#65306;&#22312;&#25351;&#25968;&#32423;&#36845;&#20195;&#27425;&#25968;&#20869;&#65292;&#31181;&#32676;&#22987;&#32456;&#20250;&#38169;&#22833;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#19968;&#20010;&#24658;&#23450;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
The non-dominated sorting genetic algorithm II (NSGA-II) is the most intensively used multi-objective evolutionary algorithm (MOEA) in real-world applications. However, in contrast to several simple MOEAs analyzed also via mathematical means, no such study exists for the NSGA-II so far. In this work, we show that mathematical runtime analyses are feasible also for the NSGA-II. As particular results, we prove that with a population size four times larger than the size of the Pareto front, the NSGA-II with two classic mutation operators and four different ways to select the parents satisfies the same asymptotic runtime guarantees as the SEMO and GSEMO algorithms on the basic OneMinMax and LeadingOnesTrailingZeros benchmarks. However, if the population size is only equal to the size of the Pareto front, then the NSGA-II cannot efficiently compute the full Pareto front: for an exponential number of iterations, the population will always miss a constant fraction of the Pareto front. Our exp
&lt;/p&gt;</description></item></channel></rss>