<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>VAR&#37325;&#26032;&#23450;&#20041;&#20102;&#22270;&#20687;&#19978;&#30340;&#33258;&#22238;&#24402;&#23398;&#20064;&#65292;&#36890;&#36807;&#31895;&#21040;&#32454;&#30340;&#8220;&#19979;&#19968;&#23610;&#24230;&#39044;&#27979;&#8221;&#23454;&#29616;&#24555;&#36895;&#23398;&#20064;&#35270;&#35273;&#20998;&#24067;&#24182;&#36229;&#36234;&#20102;&#25193;&#25955;&#21464;&#21387;&#22120;&#12290;</title><link>https://arxiv.org/abs/2404.02905</link><description>&lt;p&gt;
&#21487;&#35270;&#33258;&#22238;&#24402;&#24314;&#27169;&#65306;&#36890;&#36807;&#19979;&#19968;&#23610;&#24230;&#39044;&#27979;&#23454;&#29616;&#21487;&#20280;&#32553;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02905
&lt;/p&gt;
&lt;p&gt;
VAR&#37325;&#26032;&#23450;&#20041;&#20102;&#22270;&#20687;&#19978;&#30340;&#33258;&#22238;&#24402;&#23398;&#20064;&#65292;&#36890;&#36807;&#31895;&#21040;&#32454;&#30340;&#8220;&#19979;&#19968;&#23610;&#24230;&#39044;&#27979;&#8221;&#23454;&#29616;&#24555;&#36895;&#23398;&#20064;&#35270;&#35273;&#20998;&#24067;&#24182;&#36229;&#36234;&#20102;&#25193;&#25955;&#21464;&#21387;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#35270;&#33258;&#22238;&#24402;&#24314;&#27169;&#65288;VAR&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#37325;&#26032;&#23450;&#20041;&#33258;&#22238;&#24402;&#23398;&#20064;&#22312;&#22270;&#20687;&#19978;&#30340;&#29983;&#25104;&#33539;&#24335;&#65292;&#23558;&#20854;&#20316;&#20026;&#20174;&#31895;&#31890;&#24230;&#21040;&#32454;&#31890;&#24230;&#30340;&#8220;&#19979;&#19968;&#23610;&#24230;&#39044;&#27979;&#8221;&#25110;&#8220;&#19979;&#19968;&#20998;&#36776;&#29575;&#39044;&#27979;&#8221;&#65292;&#20559;&#31163;&#20102;&#26631;&#20934;&#30340;&#20809;&#26629;&#25195;&#25551;&#8220;&#19979;&#19968;&#20010;&#20196;&#29260;&#39044;&#27979;&#8221;&#12290;&#36825;&#31181;&#31616;&#21333;&#30452;&#35266;&#30340;&#26041;&#27861;&#20801;&#35768;&#33258;&#22238;&#24402;&#65288;AR&#65289;&#21464;&#21387;&#22120;&#24555;&#36895;&#23398;&#20064;&#35270;&#35273;&#20998;&#24067;&#24182;&#20855;&#26377;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#65306;VAR&#39318;&#27425;&#20351;AR&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#36229;&#36234;&#20102;&#25193;&#25955;&#21464;&#21387;&#22120;&#12290;&#22312;ImageNet 256x256&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;VAR&#36890;&#36807;&#23558;Frechet&#20837;&#20405;&#36317;&#31163;&#65288;FID&#65289;&#20174;18.65&#25552;&#39640;&#21040;1.80&#65292;&#23558;inception&#20998;&#25968;&#65288;IS&#65289;&#20174;80.4&#25552;&#39640;&#21040;356.4&#65292;&#25512;&#26029;&#36895;&#24230;&#21152;&#24555;&#20102;&#22823;&#32422;20&#20493;&#65292;&#26174;&#30528;&#25913;&#21892;&#20102;AR&#22522;&#32447;&#12290;&#32463;&#39564;&#35777;&#65292;VAR&#22312;&#22270;&#20687;&#36136;&#37327;&#12289;&#25512;&#26029;&#36895;&#24230;&#12289;&#25968;&#25454;&#25928;&#29575;&#21644;&#21487;&#20280;&#32553;&#24615;&#31561;&#22810;&#20010;&#32500;&#24230;&#19978;&#20248;&#20110;&#25193;&#25955;&#21464;&#21387;&#22120;&#65288;DiT&#65289;&#12290;&#25193;&#22823;VAR&#27169;&#22411;&#30340;&#35268;&#27169;&#26174;&#31034;&#20986;&#26126;&#26174;&#30340;&#24130;&#24459;&#25193;&#23637;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02905v1 Announce Type: cross  Abstract: We present Visual AutoRegressive modeling (VAR), a new generation paradigm that redefines the autoregressive learning on images as coarse-to-fine "next-scale prediction" or "next-resolution prediction", diverging from the standard raster-scan "next-token prediction". This simple, intuitive methodology allows autoregressive (AR) transformers to learn visual distributions fast and generalize well: VAR, for the first time, makes AR models surpass diffusion transformers in image generation. On ImageNet 256x256 benchmark, VAR significantly improve AR baseline by improving Frechet inception distance (FID) from 18.65 to 1.80, inception score (IS) from 80.4 to 356.4, with around 20x faster inference speed. It is also empirically verified that VAR outperforms the Diffusion Transformer (DiT) in multiple dimensions including image quality, inference speed, data efficiency, and scalability. Scaling up VAR models exhibits clear power-law scaling la
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#36755;&#20837;&#22270;&#20687;&#36827;&#34892;&#21453;&#20107;&#23454;&#21464;&#21270;&#65292;&#25105;&#20204;&#23545;&#19981;&#21516;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20197;&#25581;&#31034;&#20854;&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2404.00166</link><description>&lt;p&gt;
&#36890;&#36807;&#21453;&#20107;&#23454;&#26469;&#25581;&#31034;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Uncovering Bias in Large Vision-Language Models with Counterfactuals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00166
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#36755;&#20837;&#22270;&#20687;&#36827;&#34892;&#21453;&#20107;&#23454;&#21464;&#21270;&#65292;&#25105;&#20204;&#23545;&#19981;&#21516;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20197;&#25581;&#31034;&#20854;&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25317;&#26377;&#36234;&#26469;&#36234;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#25552;&#20986;&#20102;&#35768;&#22810;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#26469;&#21033;&#29992;&#35270;&#35273;&#36755;&#20837;&#22686;&#24378;LLMs&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#29983;&#25104;&#25991;&#26412;&#26102;&#21516;&#26102;&#26465;&#20214;&#20110;&#36755;&#20837;&#22270;&#20687;&#21644;&#25991;&#26412;&#25552;&#31034;&#65292;&#23454;&#29616;&#20102;&#21508;&#31181;&#29992;&#20363;&#65292;&#22914;&#35270;&#35273;&#38382;&#31572;&#21644;&#22810;&#27169;&#24577;&#32842;&#22825;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#32771;&#23519;&#20102;LLMs&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#21253;&#21547;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#20294;&#22312;LVLMs&#20013;&#30456;&#23545;&#36739;&#23569;&#25506;&#35752;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#30001;&#20110;&#25991;&#26412;&#21644;&#35270;&#35273;&#27169;&#24577;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#24341;&#36215;&#30340;&#20559;&#35265;&#30456;&#20114;&#20132;&#21449;&#65292;&#26816;&#26597;LVLMs&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#23545;&#19981;&#21516;LVLMs&#29983;&#25104;&#30340;&#25991;&#26412;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30740;&#31350;&#65292;&#36890;&#36807;&#23545;&#36755;&#20837;&#22270;&#20687;&#36827;&#34892;&#21453;&#20107;&#23454;&#21464;&#21270;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21521;LVLMs&#21576;&#29616;&#30456;&#21516;&#30340;&#24320;&#25918;&#24335;&#25991;&#26412;&#25552;&#31034;&#65292;&#21516;&#26102;&#22312;&#19981;&#21516;&#30340;&#22270;&#20687;&#26465;&#20214;&#19979;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00166v1 Announce Type: cross  Abstract: With the advent of Large Language Models (LLMs) possessing increasingly impressive capabilities, a number of Large Vision-Language Models (LVLMs) have been proposed to augment LLMs with visual inputs. Such models condition generated text on both an input image and a text prompt, enabling a variety of use cases such as visual question answering and multimodal chat. While prior studies have examined the social biases contained in text generated by LLMs, this topic has been relatively unexplored in LVLMs. Examining social biases in LVLMs is particularly challenging due to the confounding contributions of bias induced by information contained across the text and visual modalities. To address this challenging problem, we conduct a large-scale study of text generated by different LVLMs under counterfactual changes to input images. Specifically, we present LVLMs with identical open-ended text prompts while conditioning on images from differen
&lt;/p&gt;</description></item><item><title>ITCMA&#26159;&#22522;&#20110;&#35745;&#31639;&#24847;&#35782;&#32467;&#26500;&#30340;&#29983;&#25104;&#24335;Agent&#65292;&#36890;&#36807;&#32771;&#34385;Agent&#19982;&#29615;&#22659;&#30340;&#20114;&#21160;&#21644;&#25512;&#29702;&#65292;&#22686;&#24378;&#20102;LLMs&#29702;&#35299;&#38544;&#21547;&#25351;&#20196;&#21644;&#24212;&#29992;&#24120;&#35782;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#22312;Alfworld&#29615;&#22659;&#20013;&#34920;&#29616;&#20248;&#20110;SOTA&#12290;</title><link>https://arxiv.org/abs/2403.20097</link><description>&lt;p&gt;
&#22522;&#20110;&#35745;&#31639;&#24847;&#35782;&#32467;&#26500;&#30340;&#29983;&#25104;&#24335;Agent&#65306;ITCMA
&lt;/p&gt;
&lt;p&gt;
ITCMA: A Generative Agent Based on a Computational Consciousness Structure
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20097
&lt;/p&gt;
&lt;p&gt;
ITCMA&#26159;&#22522;&#20110;&#35745;&#31639;&#24847;&#35782;&#32467;&#26500;&#30340;&#29983;&#25104;&#24335;Agent&#65292;&#36890;&#36807;&#32771;&#34385;Agent&#19982;&#29615;&#22659;&#30340;&#20114;&#21160;&#21644;&#25512;&#29702;&#65292;&#22686;&#24378;&#20102;LLMs&#29702;&#35299;&#38544;&#21547;&#25351;&#20196;&#21644;&#24212;&#29992;&#24120;&#35782;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#22312;Alfworld&#29615;&#22659;&#20013;&#34920;&#29616;&#20248;&#20110;SOTA&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#38656;&#35201;&#29702;&#35299;&#38544;&#21547;&#25351;&#20196;&#21644;&#24212;&#29992;&#24120;&#35782;&#30693;&#35782;&#30340;&#20219;&#21153;&#20013;&#20173;&#28982;&#38754;&#20020;&#25361;&#25112;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;LLMs&#21487;&#33021;&#38656;&#35201;&#22810;&#27425;&#23581;&#35797;&#25165;&#33021;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#20135;&#29983;&#19981;&#20934;&#30830;&#30340;&#21709;&#24212;&#25110;&#25512;&#29702;&#65292;&#24433;&#21709;&#23427;&#20204;&#30340;&#38271;&#26399;&#19968;&#33268;&#24615;&#21644;&#34892;&#20026;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20869;&#37096;&#26102;&#38388;&#24847;&#35782;&#26426;&#22120;&#65288;ITCM&#65289;&#65292;&#19968;&#20010;&#35745;&#31639;&#24847;&#35782;&#32467;&#26500;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#22522;&#20110;ITCM&#30340;Agent&#65288;ITCMA&#65289;&#65292;&#25903;&#25345;&#22312;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#20013;&#29983;&#25104;&#34892;&#20026;&#21644;&#25512;&#29702;&#12290;ITCMA&#36890;&#36807;&#32771;&#34385;Agent&#19982;&#29615;&#22659;&#30340;&#20114;&#21160;&#21644;&#25512;&#29702;&#65292;&#22686;&#24378;&#20102;LLMs&#29702;&#35299;&#38544;&#21547;&#25351;&#20196;&#21644;&#24212;&#29992;&#24120;&#35782;&#30693;&#35782;&#30340;&#33021;&#21147;&#12290;&#22312;Alfworld&#29615;&#22659;&#20013;&#30340;&#35780;&#20272;&#26174;&#31034;&#65292;&#32463;&#36807;&#35757;&#32451;&#30340;ITCMA&#22312;&#24050;&#30693;&#38598;&#19978;&#27604;&#26368;&#20808;&#36827;&#25216;&#26415;&#65288;SOTA&#65289;&#34920;&#29616;&#25552;&#39640;&#20102;9%&#12290;&#21363;&#20351;&#26410;&#32463;&#35757;&#32451;&#30340;ITCMA&#20063;&#36798;&#21040;&#20102;96%&#30340;&#20219;&#21153;&#23436;&#25104;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20097v1 Announce Type: new  Abstract: Large Language Models (LLMs) still face challenges in tasks requiring understanding implicit instructions and applying common-sense knowledge. In such scenarios, LLMs may require multiple attempts to achieve human-level performance, potentially leading to inaccurate responses or inferences in practical environments, affecting their long-term consistency and behavior. This paper introduces the Internal Time-Consciousness Machine (ITCM), a computational consciousness structure. We further propose the ITCM-based Agent (ITCMA), which supports behavior generation and reasoning in open-world settings. ITCMA enhances LLMs' ability to understand implicit instructions and apply common-sense knowledge by considering agents' interaction and reasoning with the environment. Evaluations in the Alfworld environment show that trained ITCMA outperforms the state-of-the-art (SOTA) by 9% on the seen set. Even untrained ITCMA achieves a 96% task completion 
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#22312;&#22522;&#20110;&#22270;&#20687;&#30340;&#23433;&#20840;&#24212;&#29992;&#20013;&#65292;&#21363;&#26102;&#24037;&#31243;&#30340;Gemini-pro&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#19982;&#24494;&#35843;&#30340;Vision Transformer&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21457;&#29616;&#22312;&#20851;&#38190;&#23433;&#20840;&#25361;&#25112;&#20013;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.17787</link><description>&lt;p&gt;
&#35780;&#20272;&#22312;&#22522;&#20110;&#22270;&#20687;&#30340;&#23433;&#20840;&#24212;&#29992;&#20013;&#65292;&#22312;&#21363;&#26102;&#24037;&#31243;&#36807;&#31243;&#20013;&#35774;&#35745;&#30340;&#22823;&#22411;&#22810;&#27169;&#22411;&#19982;&#24494;&#35843;&#35270;&#35273;&#21464;&#25442;&#22120;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Efficacy of Prompt-Engineered Large Multimodal Models Versus Fine-Tuned Vision Transformers in Image-Based Security Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17787
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#22312;&#22522;&#20110;&#22270;&#20687;&#30340;&#23433;&#20840;&#24212;&#29992;&#20013;&#65292;&#21363;&#26102;&#24037;&#31243;&#30340;Gemini-pro&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#19982;&#24494;&#35843;&#30340;Vision Transformer&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21457;&#29616;&#22312;&#20851;&#38190;&#23433;&#20840;&#25361;&#25112;&#20013;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25104;&#21151;&#23548;&#33268;&#20102;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#65288;&#22914;Gemini-pro&#65289;&#30340;&#21457;&#23637;&#65292;&#36825;&#20123;&#27169;&#22411;&#24050;&#32463;&#24320;&#22987;&#36716;&#21464;&#21508;&#31181;&#24212;&#29992;&#12290;&#36825;&#20123;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#26088;&#22312;&#35299;&#37322;&#21644;&#20998;&#26512;&#22797;&#26434;&#25968;&#25454;&#65292;&#25972;&#21512;&#20102;&#20197;&#24448;&#38590;&#20197;&#23454;&#29616;&#30340;&#25991;&#26412;&#21644;&#35270;&#35273;&#20449;&#24687;&#35268;&#27169;&#65292;&#20026;&#19968;&#31995;&#21015;&#24212;&#29992;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;&#22312;&#35299;&#20915;&#20851;&#38190;&#23433;&#20840;&#25361;&#25112;&#26041;&#38754;&#65292;&#21363;&#26102;&#24037;&#31243;&#30340;Gemini-pro LMMs&#19982;&#24494;&#35843;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViT&#65289;&#27169;&#22411;&#30340;&#36866;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#20004;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#65306;&#26816;&#27979;&#22270;&#20687;&#20013;&#30340;&#31616;&#21333;&#35302;&#21457;&#22120;&#65288;&#22914;&#26041;&#24418;&#23567;&#26041;&#22359;&#65289;&#20197;&#31034;&#28508;&#22312;&#21518;&#38376;&#30340;&#22312;&#35270;&#35273;&#19978;&#26174;&#32780;&#26131;&#35265;&#30340;&#20219;&#21153;&#65292;&#20197;&#21450;&#36890;&#36807;&#35270;&#35273;&#34920;&#31034;&#36827;&#34892;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#30340;&#22312;&#35270;&#35273;&#19978;&#19981;&#26126;&#26174;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#26174;&#20102;&#24615;&#33021;&#19978;&#30340;&#26174;&#33879;&#24046;&#24322;&#65292;Gemini-pro&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17787v1 Announce Type: new  Abstract: The success of Large Language Models (LLMs) has led to a parallel rise in the development of Large Multimodal Models (LMMs), such as Gemini-pro, which have begun to transform a variety of applications. These sophisticated multimodal models are designed to interpret and analyze complex data, integrating both textual and visual information on a scale previously unattainable, opening new avenues for a range of applications. This paper investigates the applicability and effectiveness of prompt-engineered Gemini-pro LMMs versus fine-tuned Vision Transformer (ViT) models in addressing critical security challenges. We focus on two distinct tasks: a visually evident task of detecting simple triggers, such as small squares in images, indicative of potential backdoors, and a non-visually evident task of malware classification through visual representations. Our results highlight a significant divergence in performance, with Gemini-pro falling shor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#20013;&#25968;&#25454;&#38598;&#35757;&#32451;&#36164;&#28304;&#38656;&#27714;&#22823;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#38598;&#20462;&#21098;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#22522;&#20110;&#25439;&#22833;&#20540;&#30340;&#36873;&#25321;&#65292;&#23558;&#35757;&#32451;&#38598;&#32553;&#20943;&#33267;&#21407;&#22987;&#25968;&#25454;&#38598;&#30340;50%&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.17083</link><description>&lt;p&gt;
&#25968;&#25454;&#38598;&#20462;&#21098;&#22312;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#20013;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study in Dataset Pruning for Image Super-Resolution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17083
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#20013;&#25968;&#25454;&#38598;&#35757;&#32451;&#36164;&#28304;&#38656;&#27714;&#22823;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#38598;&#20462;&#21098;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#22522;&#20110;&#25439;&#22833;&#20540;&#30340;&#36873;&#25321;&#65292;&#23558;&#35757;&#32451;&#38598;&#32553;&#20943;&#33267;&#21407;&#22987;&#25968;&#25454;&#38598;&#30340;50%&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#65288;SR&#65289;&#20013;&#65292;&#20381;&#36182;&#22823;&#22411;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#26159;&#19968;&#25226;&#21452;&#20995;&#21073;&#12290;&#23613;&#31649;&#25552;&#20379;&#20016;&#23500;&#30340;&#35757;&#32451;&#32032;&#26448;&#65292;&#20294;&#20063;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#36164;&#28304;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#25968;&#25454;&#38598;&#20462;&#21098;&#20316;&#20026;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#25968;&#25454;&#38598;&#32553;&#20943;&#21040;&#22522;&#20110;&#20854;&#25439;&#22833;&#20540;&#32780;&#36873;&#25321;&#30340;&#19968;&#32452;&#26680;&#24515;&#35757;&#32451;&#26679;&#26412;&#12290;&#36890;&#36807;&#20165;&#23558;&#35757;&#32451;&#37325;&#28857;&#25918;&#22312;&#21407;&#22987;&#25968;&#25454;&#38598;&#30340;50%&#19978;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#25439;&#22833;&#20540;&#26368;&#39640;&#30340;&#26679;&#26412;&#19978;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19982;&#25110;&#29978;&#33267;&#36229;&#36807;&#25972;&#20010;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#32467;&#26524;&#30456;&#23218;&#32654;&#30340;&#25928;&#26524;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#26174;&#31034;&#65292;&#20855;&#26377;&#26368;&#39640;&#25439;&#22833;&#20540;&#30340;&#21069;5&#65285;&#26679;&#26412;&#20250;&#23545;&#35757;&#32451;&#36807;&#31243;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#25490;&#38500;&#36825;&#20123;&#26679;&#26412;&#24182;&#35843;&#25972;&#36873;&#25321;&#20197;&#20559;&#22909;&#26356;&#23481;&#26131;&#30340;&#26679;&#26412;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#35757;&#32451;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17083v1 Announce Type: cross  Abstract: In image Super-Resolution (SR), relying on large datasets for training is a double-edged sword. While offering rich training material, they also demand substantial computational and storage resources. In this work, we analyze dataset pruning as a solution to these challenges. We introduce a novel approach that reduces a dataset to a core-set of training samples, selected based on their loss values as determined by a simple pre-trained SR model. By focusing the training on just 50% of the original dataset, specifically on the samples characterized by the highest loss values, we achieve results comparable to or even surpassing those obtained from training on the entire dataset. Interestingly, our analysis reveals that the top 5% of samples with the highest loss values negatively affect the training process. Excluding these samples and adjusting the selection to favor easier samples further enhances training outcomes. Our work opens new p
&lt;/p&gt;</description></item><item><title>TrustSQL&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#25991;&#26412;&#21040;SQL&#27169;&#22411;&#22312;&#22788;&#29702;&#21508;&#31181;&#31867;&#22411;&#38382;&#39064;&#26102;&#30340;&#21487;&#38752;&#24615;&#30340;&#26032;&#22522;&#20934;&#65292;&#35201;&#27714;&#27169;&#22411;&#22312;SQL&#39044;&#27979;&#21644;&#25918;&#24323;&#39044;&#27979;&#20004;&#31181;&#24773;&#20917;&#19979;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.15879</link><description>&lt;p&gt;
TrustSQL: &#29992;&#20110;&#20855;&#26377;&#22810;&#26679;&#26080;&#27861;&#22238;&#31572;&#38382;&#39064;&#30340;&#25991;&#26412;&#21040;SQL&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
TrustSQL: A Reliability Benchmark for Text-to-SQL Models with Diverse Unanswerable Questions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15879
&lt;/p&gt;
&lt;p&gt;
TrustSQL&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#25991;&#26412;&#21040;SQL&#27169;&#22411;&#22312;&#22788;&#29702;&#21508;&#31181;&#31867;&#22411;&#38382;&#39064;&#26102;&#30340;&#21487;&#38752;&#24615;&#30340;&#26032;&#22522;&#20934;&#65292;&#35201;&#27714;&#27169;&#22411;&#22312;SQL&#39044;&#27979;&#21644;&#25918;&#24323;&#39044;&#27979;&#20004;&#31181;&#24773;&#20917;&#19979;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#26174;&#33879;&#25552;&#39640;&#20102;&#23558;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#32763;&#35793;&#25104;SQL&#26597;&#35810;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;SQL&#29983;&#25104;&#30340;&#20934;&#30830;&#24615;&#33267;&#20851;&#37325;&#35201;&#30340;&#21516;&#26102;&#65292;&#24456;&#23569;&#26377;&#20154;&#20102;&#35299;&#36825;&#20123;&#25991;&#26412;&#21040;SQL&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;&#37096;&#32626;&#36807;&#31243;&#20013;&#33021;&#21542;&#21487;&#38752;&#22788;&#29702;&#21508;&#31181;&#31867;&#22411;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#26080;&#27861;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#25506;&#35752;&#36825;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TrustSQL&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#25991;&#26412;&#21040;SQL&#27169;&#22411;&#22312;&#21333;&#19968;&#25968;&#25454;&#24211;&#21644;&#36328;&#25968;&#25454;&#24211;&#35774;&#32622;&#20013;&#30340;&#21487;&#38752;&#24615;&#12290;&#22522;&#20934;&#20219;&#21153;&#35201;&#27714;&#27169;&#22411;&#25552;&#20379;&#20004;&#31181;&#32467;&#26524;&#20043;&#19968;&#65306;1&#65289;SQL&#39044;&#27979;&#65307;&#25110;2&#65289;&#22312;&#29983;&#25104;&#30340;SQL&#20013;&#21487;&#33021;&#23384;&#22312;&#38169;&#35823;&#25110;&#38754;&#20020;&#26080;&#27861;&#22238;&#31572;&#30340;&#38382;&#39064;&#26102;&#25918;&#24323;&#39044;&#27979;&#12290;&#20026;&#20102;&#35780;&#20272;&#27169;&#22411;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19987;&#38376;&#20026;&#36825;&#19968;&#20219;&#21153;&#35774;&#35745;&#30340;&#21508;&#31181;&#24314;&#27169;&#26041;&#27861;&#65292;&#21253;&#25324;&#65306;1&#65289;&#20026;&#21487;&#22238;&#31572;&#24615;&#20248;&#21270;&#21333;&#29420;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15879v1 Announce Type: new  Abstract: Recent advances in large language models (LLMs) have led to significant improvements in translating natural language questions into SQL queries. While achieving high accuracy in SQL generation is crucial, little is known about the extent to which these text-to-SQL models can reliably handle diverse types of questions encountered during real-world deployment, including unanswerable ones. To explore this aspect, we present TrustSQL, a new benchmark designed to assess the reliability of text-to-SQL models in both single-database and cross-database settings. The benchmark tasks models with providing one of two outcomes: 1) SQL prediction; or 2) abstention from making a prediction, either when there is a potential error in the generated SQL or when faced with unanswerable questions. For model evaluation, we explore various modeling approaches specifically designed for this task. These include: 1) optimizing separate models for answerability d
&lt;/p&gt;</description></item><item><title>FusionINN&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#36870;&#22270;&#20687;&#34701;&#21512;&#26694;&#26550;&#65292;&#21487;&#20197;&#39640;&#25928;&#29983;&#25104;&#34701;&#21512;&#22270;&#20687;&#65292;&#24182;&#35299;&#24320;&#34701;&#21512;&#36807;&#31243;&#30340;&#36870;&#21521;&#20998;&#35299;&#65292;&#20445;&#35777;&#26080;&#25439;&#30340;&#20687;&#32032;&#26144;&#23556;&#12290;</title><link>https://arxiv.org/abs/2403.15769</link><description>&lt;p&gt;
FusionINN&#65306;&#21487;&#36870;&#22270;&#20687;&#34701;&#21512;&#29992;&#20110;&#33041;&#32959;&#30244;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
FusionINN: Invertible Image Fusion for Brain Tumor Monitoring
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15769
&lt;/p&gt;
&lt;p&gt;
FusionINN&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#36870;&#22270;&#20687;&#34701;&#21512;&#26694;&#26550;&#65292;&#21487;&#20197;&#39640;&#25928;&#29983;&#25104;&#34701;&#21512;&#22270;&#20687;&#65292;&#24182;&#35299;&#24320;&#34701;&#21512;&#36807;&#31243;&#30340;&#36870;&#21521;&#20998;&#35299;&#65292;&#20445;&#35777;&#26080;&#25439;&#30340;&#20687;&#32032;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#34701;&#21512;&#36890;&#24120;&#20351;&#29992;&#19981;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#23558;&#22810;&#20010;&#28304;&#22270;&#20687;&#21512;&#24182;&#20026;&#21333;&#20010;&#34701;&#21512;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20020;&#24202;&#19987;&#23478;&#65292;&#20165;&#20381;&#36182;&#34701;&#21512;&#22270;&#20687;&#21487;&#33021;&#19981;&#36275;&#20197;&#20570;&#20986;&#35786;&#26029;&#20915;&#31574;&#65292;&#22240;&#20026;&#34701;&#21512;&#26426;&#21046;&#28151;&#21512;&#20102;&#26469;&#33258;&#28304;&#22270;&#20687;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#38590;&#20197;&#35299;&#37322;&#28508;&#22312;&#30340;&#32959;&#30244;&#30149;&#29702;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;FusionINN&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#36870;&#22270;&#20687;&#34701;&#21512;&#26694;&#26550;&#65292;&#33021;&#22815;&#39640;&#25928;&#29983;&#25104;&#34701;&#21512;&#22270;&#20687;&#65292;&#24182;&#36890;&#36807;&#27714;&#35299;&#34701;&#21512;&#36807;&#31243;&#30340;&#36870;&#36807;&#31243;&#23558;&#20854;&#20998;&#35299;&#22238;&#28304;&#22270;&#20687;&#12290;FusionINN&#36890;&#36807;&#25972;&#21512;&#19968;&#20010;&#27491;&#24577;&#20998;&#24067;&#30340;&#28508;&#22312;&#22270;&#20687;&#19982;&#34701;&#21512;&#22270;&#20687;&#19968;&#36215;&#65292;&#20197;&#20419;&#36827;&#20998;&#35299;&#36807;&#31243;&#30340;&#29983;&#25104;&#24314;&#27169;&#65292;&#20174;&#32780;&#20445;&#35777;&#26080;&#25439;&#30340;&#19968;&#23545;&#19968;&#20687;&#32032;&#26144;&#23556;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#39318;&#27425;&#30740;&#31350;&#34701;&#21512;&#22270;&#20687;&#30340;&#21487;&#20998;&#35299;&#24615;&#65292;&#36825;&#23545;&#20110;&#29983;&#21629;&#25935;&#24863;&#24212;&#29992;&#31243;&#24207;&#23588;&#20026;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15769v1 Announce Type: cross  Abstract: Image fusion typically employs non-invertible neural networks to merge multiple source images into a single fused image. However, for clinical experts, solely relying on fused images may be insufficient for making diagnostic decisions, as the fusion mechanism blends features from source images, thereby making it difficult to interpret the underlying tumor pathology. We introduce FusionINN, a novel invertible image fusion framework, capable of efficiently generating fused images and also decomposing them back to the source images by solving the inverse of the fusion process. FusionINN guarantees lossless one-to-one pixel mapping by integrating a normally distributed latent image alongside the fused image to facilitate the generative modeling of the decomposition process. To the best of our knowledge, we are the first to investigate the decomposability of fused images, which is particularly crucial for life-sensitive applications such as
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#31181;&#38754;&#21521;&#30005;&#23376;&#31163;&#23376;&#23545;&#25758;&#26426;&#30340;&#22522;&#20110;RAG&#30340;&#25688;&#35201;&#29983;&#25104;&#20195;&#29702;&#65292;&#33021;&#22815;&#21387;&#32553;&#20449;&#24687;&#24182;&#24341;&#29992;&#30456;&#20851;&#22238;&#22797;&#65292;&#20026;&#21512;&#20316;&#32773;&#25552;&#20379;&#37325;&#22823;&#20248;&#21183;</title><link>https://arxiv.org/abs/2403.15729</link><description>&lt;p&gt;
&#38754;&#21521;&#30005;&#23376;&#31163;&#23376;&#23545;&#25758;&#26426;&#30340;&#22522;&#20110;RAG&#30340;&#25688;&#35201;&#29983;&#25104;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Towards a \textbf{RAG}-based Summarization Agent for the Electron-Ion Collider
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15729
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#31181;&#38754;&#21521;&#30005;&#23376;&#31163;&#23376;&#23545;&#25758;&#26426;&#30340;&#22522;&#20110;RAG&#30340;&#25688;&#35201;&#29983;&#25104;&#20195;&#29702;&#65292;&#33021;&#22815;&#21387;&#32553;&#20449;&#24687;&#24182;&#24341;&#29992;&#30456;&#20851;&#22238;&#22797;&#65292;&#20026;&#21512;&#20316;&#32773;&#25552;&#20379;&#37325;&#22823;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#24615;&#21644;&#24222;&#22823;&#30340;&#20449;&#24687;&#37327;&#28085;&#30422;&#20102;&#22823;&#35268;&#27169;&#23454;&#39564;&#30340;&#25991;&#20214;&#12289;&#35770;&#25991;&#12289;&#25968;&#25454;&#21644;&#20854;&#20182;&#36164;&#28304;&#65292;&#23548;&#33268;&#23548;&#33322;&#36825;&#20123;&#22810;&#26679;&#24418;&#24335;&#20449;&#24687;&#30340;&#20219;&#21153;&#38656;&#35201;&#22823;&#37327;&#26102;&#38388;&#21644;&#31934;&#21147;&#65292;&#23545;&#20110;&#26032;&#21512;&#20316;&#32773;&#21644;&#26089;&#26399;&#31185;&#23398;&#23478;&#26469;&#35828;&#23588;&#20026;&#33392;&#24040;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#27491;&#22312;&#24320;&#21457;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#30340;EIC&#25688;&#35201;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#65288;RAGS4EIC&#65289;&#12290;&#35813;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#19981;&#20165;&#21387;&#32553;&#20449;&#24687;&#65292;&#36824;&#26377;&#25928;&#24341;&#29992;&#30456;&#20851;&#22238;&#22797;&#65292;&#20026;&#21512;&#20316;&#32773;&#25552;&#20379;&#20102;&#37325;&#22823;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#39033;&#30446;&#37319;&#21462;&#20102;&#20004;&#27493;&#26041;&#27861;&#65306;&#39318;&#20808;&#65292;&#26597;&#35810;&#21253;&#21547;&#25152;&#26377;&#30456;&#20851;&#23454;&#39564;&#20449;&#24687;&#30340;&#32508;&#21512;&#21521;&#37327;&#25968;&#25454;&#24211;&#65307;&#20854;&#27425;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26681;&#25454;&#29992;&#25143;&#26597;&#35810;&#21644;&#26816;&#32034;&#25968;&#25454;&#29983;&#25104;&#21253;&#21547;&#24341;&#29992;&#30340;&#31616;&#27905;&#25688;&#35201;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#20351;&#29992;RAG&#35780;&#20272;&#30340;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15729v1 Announce Type: cross  Abstract: The complexity and sheer volume of information encompassing documents, papers, data, and other resources from large-scale experiments demand significant time and effort to navigate, making the task of accessing and utilizing these varied forms of information daunting, particularly for new collaborators and early-career scientists. To tackle this issue, a Retrieval Augmented Generation (RAG)--based Summarization AI for EIC (RAGS4EIC) is under development. This AI-Agent not only condenses information but also effectively references relevant responses, offering substantial advantages for collaborators. Our project involves a two-step approach: first, querying a comprehensive vector database containing all pertinent experiment information; second, utilizing a Large Language Model (LLM) to generate concise summaries enriched with citations based on user queries and retrieved data. We describe the evaluation methods that use RAG assessments 
&lt;/p&gt;</description></item><item><title>&#25506;&#32034;&#20102;&#19968;&#31181;&#23398;&#20064;&#22914;&#20309;&#25512;&#26029;&#25429;&#25417;&#35270;&#35273;&#27010;&#24565;&#30340;&#36890;&#29992;&#27169;&#26495;&#31243;&#24207;&#30340;&#31070;&#32463;&#31526;&#21495;&#31995;&#32479;&#65292;&#24341;&#20837;&#20102;&#27169;&#26495;&#31243;&#24207;&#27010;&#24565;&#65292;&#25903;&#25345;&#22810;&#31181;&#27010;&#24565;&#30456;&#20851;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#33539;&#24335;&#26469;&#35757;&#32451;&#32593;&#32476;&#30452;&#25509;&#25512;&#26029;&#27169;&#26495;&#31243;&#24207;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20219;&#21153;&#29305;&#23450;&#26367;&#20195;&#26041;&#27861;&#65292;&#24182;&#19982;&#29305;&#23450;&#39046;&#22495;&#26041;&#27861;&#31454;&#20105;&#24615;&#22320;&#25191;&#34892;&#12290;</title><link>https://arxiv.org/abs/2403.15476</link><description>&lt;p&gt;
&#23398;&#20064;&#25512;&#26029;&#29983;&#25104;&#35270;&#35273;&#27010;&#24565;&#30340;&#27169;&#26495;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
Learning to Infer Generative Template Programs for Visual Concepts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15476
&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#20102;&#19968;&#31181;&#23398;&#20064;&#22914;&#20309;&#25512;&#26029;&#25429;&#25417;&#35270;&#35273;&#27010;&#24565;&#30340;&#36890;&#29992;&#27169;&#26495;&#31243;&#24207;&#30340;&#31070;&#32463;&#31526;&#21495;&#31995;&#32479;&#65292;&#24341;&#20837;&#20102;&#27169;&#26495;&#31243;&#24207;&#27010;&#24565;&#65292;&#25903;&#25345;&#22810;&#31181;&#27010;&#24565;&#30456;&#20851;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#33539;&#24335;&#26469;&#35757;&#32451;&#32593;&#32476;&#30452;&#25509;&#25512;&#26029;&#27169;&#26495;&#31243;&#24207;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20219;&#21153;&#29305;&#23450;&#26367;&#20195;&#26041;&#27861;&#65292;&#24182;&#19982;&#29305;&#23450;&#39046;&#22495;&#26041;&#27861;&#31454;&#20105;&#24615;&#22320;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#21487;&#20197;&#20174;&#23569;&#37327;&#31034;&#20363;&#20013;&#28789;&#27963;&#25484;&#25569;&#35270;&#35273;&#27010;&#24565;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#31995;&#32479;&#65292;&#23398;&#20064;&#22914;&#20309;&#20197;&#19968;&#31181;&#36890;&#29992;&#26041;&#24335;&#25512;&#26029;&#25429;&#25417;&#35270;&#35273;&#27010;&#24565;&#30340;&#31243;&#24207;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#27169;&#26495;&#31243;&#24207;&#65306;&#26469;&#33258;&#29305;&#23450;&#39046;&#22495;&#35821;&#35328;&#30340;&#31243;&#24207;&#34920;&#36798;&#24335;&#65292;&#25351;&#23450;&#20102;&#36755;&#20837;&#27010;&#24565;&#20013;&#24120;&#35265;&#30340;&#32467;&#26500;&#21644;&#21442;&#25968;&#27169;&#24335;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#25903;&#25345;&#22810;&#20010;&#19982;&#27010;&#24565;&#30456;&#20851;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#36890;&#36807;&#35299;&#26512;&#36827;&#34892;&#23569;&#26679;&#26412;&#29983;&#25104;&#21644;&#20849;&#20998;&#21106;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#23398;&#20064;&#33539;&#24335;&#65292;&#20801;&#35768;&#25105;&#20204;&#35757;&#32451;&#32593;&#32476;&#30452;&#25509;&#20174;&#21253;&#21547;&#27010;&#24565;&#20998;&#32452;&#30340;&#35270;&#35273;&#25968;&#25454;&#38598;&#20013;&#25512;&#26029;&#27169;&#26495;&#31243;&#24207;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#35270;&#35273;&#39046;&#22495;&#36827;&#34892;&#23454;&#39564;&#65306;2D&#24067;&#23616;&#12289;Omniglot&#23383;&#31526;&#21644;3D&#24418;&#29366;&#12290;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#32988;&#36807;&#20102;&#20219;&#21153;&#29305;&#23450;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#24182;&#22312;&#26377;&#38480;&#39046;&#22495;&#31454;&#20105;&#24615;&#22320;&#25191;&#34892;&#20102;&#38024;&#23545;&#29305;&#23450;&#39046;&#22495;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15476v1 Announce Type: cross  Abstract: People grasp flexible visual concepts from a few examples. We explore a neurosymbolic system that learns how to infer programs that capture visual concepts in a domain-general fashion. We introduce Template Programs: programmatic expressions from a domain-specific language that specify structural and parametric patterns common to an input concept. Our framework supports multiple concept-related tasks, including few-shot generation and co-segmentation through parsing. We develop a learning paradigm that allows us to train networks that infer Template Programs directly from visual datasets that contain concept groupings. We run experiments across multiple visual domains: 2D layouts, Omniglot characters, and 3D shapes. We find that our method outperforms task-specific alternatives, and performs competitively against domain-specific approaches for the limited domains where they exist.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SPUR&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;LLMs&#26356;&#26377;&#25928;&#22320;&#20174;&#33258;&#28982;&#35821;&#35328;&#35805;&#35821;&#20013;&#25552;&#21462;&#29992;&#25143;&#28385;&#24847;&#24230;&#30340;&#21487;&#35299;&#37322;&#20449;&#21495;&#65292;&#24182;&#33021;&#22815;&#21033;&#29992;&#36845;&#20195;&#25552;&#31034;&#26694;&#26550;&#36827;&#34892;&#29992;&#25143;&#28385;&#24847;&#24230;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.12388</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#23545;&#35805;&#31995;&#32479;&#29992;&#25143;&#28385;&#24847;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Interpretable User Satisfaction Estimation for Conversational Systems with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SPUR&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;LLMs&#26356;&#26377;&#25928;&#22320;&#20174;&#33258;&#28982;&#35821;&#35328;&#35805;&#35821;&#20013;&#25552;&#21462;&#29992;&#25143;&#28385;&#24847;&#24230;&#30340;&#21487;&#35299;&#37322;&#20449;&#21495;&#65292;&#24182;&#33021;&#22815;&#21033;&#29992;&#36845;&#20195;&#25552;&#31034;&#26694;&#26550;&#36827;&#34892;&#29992;&#25143;&#28385;&#24847;&#24230;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#32780;&#21487;&#35299;&#37322;&#30340;&#29992;&#25143;&#28385;&#24847;&#24230;&#20272;&#35745;&#23545;&#20110;&#20102;&#35299;&#12289;&#35780;&#20272;&#21644;&#25345;&#32493;&#25913;&#36827;&#23545;&#35805;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#19982;&#22522;&#20110;&#29305;&#24449;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25110;&#25991;&#26412;&#23884;&#20837;&#30340;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;LLMs&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#20174;&#33258;&#28982;&#35821;&#35328;&#35805;&#35821;&#20013;&#25552;&#21462;&#29992;&#25143;&#28385;&#24847;&#24230;&#30340;&#21487;&#35299;&#37322;&#20449;&#21495;&#12290;&#27492;&#22806;&#65292;LLM&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#36845;&#20195;&#25552;&#31034;&#26694;&#26550;&#65292;&#24182;&#21033;&#29992;&#26631;&#35760;&#31034;&#20363;&#30340;&#30417;&#30563;&#36827;&#34892;&#29992;&#25143;&#28385;&#24847;&#24230;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12388v1 Announce Type: cross  Abstract: Accurate and interpretable user satisfaction estimation (USE) is critical for understanding, evaluating, and continuously improving conversational systems. Users express their satisfaction or dissatisfaction with diverse conversational patterns in both general-purpose (ChatGPT and Bing Copilot) and task-oriented (customer service chatbot) conversational systems. Existing approaches based on featurized ML models or text embeddings fall short in extracting generalizable patterns and are hard to interpret. In this work, we show that LLMs can extract interpretable signals of user satisfaction from their natural language utterances more effectively than embedding-based approaches. Moreover, an LLM can be tailored for USE via an iterative prompting framework using supervision from labeled examples. The resulting method, Supervised Prompting for User satisfaction Rubrics (SPUR), not only has higher accuracy but is more interpretable as it sco
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;CantonMT&#39033;&#30446;&#65292;&#21033;&#29992;&#21512;&#25104;&#21453;&#21521;&#32763;&#35793;&#25968;&#25454;&#23545;&#31908;&#35821;&#33267;&#33521;&#35821;NMT&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#21644;&#24320;&#28304;&#24037;&#20855;&#21253;&#65292;&#20197;&#20419;&#36827;&#30740;&#31350;</title><link>https://arxiv.org/abs/2403.11346</link><description>&lt;p&gt;
CantonMT: &#27721;&#33521;NMT&#24179;&#21488;&#65292;&#20351;&#29992;&#21512;&#25104;&#21453;&#21521;&#32763;&#35793;&#25968;&#25454;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
CantonMT: Cantonese to English NMT Platform with Fine-Tuned Models Using Synthetic Back-Translation Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11346
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;CantonMT&#39033;&#30446;&#65292;&#21033;&#29992;&#21512;&#25104;&#21453;&#21521;&#32763;&#35793;&#25968;&#25454;&#23545;&#31908;&#35821;&#33267;&#33521;&#35821;NMT&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#21644;&#24320;&#28304;&#24037;&#20855;&#21253;&#65292;&#20197;&#20419;&#36827;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11346v1 &#28040;&#24687;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;(NMT)&#20173;&#28982;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20154;&#21592;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#19968;&#20010;&#26631;&#20934;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#8212;&#8212;&#21453;&#21521;&#32763;&#35793;&#65292;&#24212;&#29992;&#21040;&#20102;&#26032;&#30340;&#35821;&#35328;&#32763;&#35793;&#26041;&#21521;&#31908;&#35821;&#33267;&#33521;&#35821;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#25105;&#20204;&#20351;&#29992;&#26377;&#38480;&#25968;&#37327;&#30495;&#23454;&#25968;&#25454;&#21644;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;(&#21253;&#25324;OpusMT, NLLB,&#21644;mBART)&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31995;&#21015;&#19981;&#21516;&#25351;&#26631;&#21253;&#25324;&#22522;&#20110;&#35789;&#27719;&#21644;&#23884;&#20837;&#30340;&#33258;&#21160;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#36825;&#39033;\textsc{CantonMT}&#30740;&#31350;&#39033;&#30446;&#20013;&#21253;&#21547;&#30340;&#27169;&#22411;&#21019;&#24314;&#20102;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#65292;&#24182;&#25552;&#20379;&#20415;&#21033;&#23454;&#29616;&#31908;&#35821;&#33267;&#33521;&#35821;MT&#30740;&#31350;&#12290;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36890;&#36807;&#25105;&#20204;&#30340;&#24320;&#28304;\textsc{CantonMT}&#24037;&#20855;&#21253;\url{https://github.com/kenrickkung/CantoneseTranslation}&#21521;&#24179;&#21488;&#28155;&#21152;&#26356;&#22810;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11346v1 Announce Type: cross  Abstract: Neural Machine Translation (NMT) for low-resource languages is still a challenging task in front of NLP researchers. In this work, we deploy a standard data augmentation methodology by back-translation to a new language translation direction Cantonese-to-English. We present the models we fine-tuned using the limited amount of real data and the synthetic data we generated using back-translation including OpusMT, NLLB, and mBART. We carried out automatic evaluation using a range of different metrics including lexical-based and embedding-based. Furthermore. we create a user-friendly interface for the models we included in this\textsc{ CantonMT} research project and make it available to facilitate Cantonese-to-English MT research. Researchers can add more models into this platform via our open-source\textsc{ CantonMT} toolkit \url{https://github.com/kenrickkung/CantoneseTranslation}.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#23616;&#37096;&#32593;&#26684;&#21464;&#24418;&#25216;&#26415;&#65292;HeadEvolver&#26694;&#26550;&#21487;&#20197;&#36890;&#36807;&#25991;&#26412;&#24341;&#23548;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22836;&#37096;&#22836;&#20687;&#65292;&#20445;&#30041;&#32454;&#33410;&#24182;&#25903;&#25345;&#32534;&#36753;&#21644;&#21160;&#30011;&#12290;</title><link>https://arxiv.org/abs/2403.09326</link><description>&lt;p&gt;
HeadEvolver&#65306;&#36890;&#36807;&#26412;&#22320;&#21487;&#23398;&#20064;&#32593;&#26684;&#21464;&#24418;&#23454;&#29616;&#25991;&#26412;&#21040;&#22836;&#37096;&#22836;&#20687;&#30340;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
HeadEvolver: Text to Head Avatars via Locally Learnable Mesh Deformation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09326
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#23616;&#37096;&#32593;&#26684;&#21464;&#24418;&#25216;&#26415;&#65292;HeadEvolver&#26694;&#26550;&#21487;&#20197;&#36890;&#36807;&#25991;&#26412;&#24341;&#23548;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22836;&#37096;&#22836;&#20687;&#65292;&#20445;&#30041;&#32454;&#33410;&#24182;&#25903;&#25345;&#32534;&#36753;&#21644;&#21160;&#30011;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;HeadEvolver&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#25991;&#26412;&#24341;&#23548;&#29983;&#25104;&#39118;&#26684;&#21270;&#30340;&#22836;&#37096;&#22836;&#20687;&#12290;HeadEvolver&#20351;&#29992;&#27169;&#26495;&#22836;&#37096;&#32593;&#26684;&#30340;&#26412;&#22320;&#21487;&#23398;&#20064;&#32593;&#26684;&#21464;&#24418;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25968;&#23383;&#36164;&#20135;&#65292;&#20197;&#23454;&#29616;&#20445;&#30041;&#32454;&#33410;&#30340;&#32534;&#36753;&#21644;&#21160;&#30011;&#12290;&#20026;&#20102;&#35299;&#20915;&#20840;&#23616;&#21464;&#24418;&#20013;&#32570;&#20047;&#32454;&#31890;&#24230;&#21644;&#35821;&#20041;&#24863;&#30693;&#26412;&#22320;&#24418;&#29366;&#25511;&#21046;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#35757;&#32451;&#21442;&#25968;&#20316;&#20026;&#27599;&#20010;&#19977;&#35282;&#24418;&#30340;Jacobi&#30697;&#38453;&#30340;&#21152;&#26435;&#22240;&#23376;&#65292;&#20197;&#33258;&#36866;&#24212;&#22320;&#25913;&#21464;&#26412;&#22320;&#24418;&#29366;&#21516;&#26102;&#20445;&#25345;&#20840;&#23616;&#23545;&#24212;&#21644;&#38754;&#37096;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#30830;&#20445;&#26469;&#33258;&#19981;&#21516;&#35270;&#35282;&#30340;&#32467;&#26524;&#24418;&#29366;&#21644;&#22806;&#35266;&#30340;&#36830;&#36143;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#21487;&#24494;&#20998;&#28210;&#26579;&#65292;&#24182;&#28155;&#21152;&#27491;&#21017;&#21270;&#39033;&#20197;&#22312;&#25991;&#26412;&#24341;&#23548;&#19979;&#20248;&#21270;&#21464;&#24418;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#20851;&#33410;&#32593;&#26684;&#30340;&#22810;&#26679;&#21270;&#22836;&#37096;&#22836;&#20687;&#65292;&#21487;&#26080;&#32541;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09326v1 Announce Type: cross  Abstract: We present HeadEvolver, a novel framework to generate stylized head avatars from text guidance. HeadEvolver uses locally learnable mesh deformation from a template head mesh, producing high-quality digital assets for detail-preserving editing and animation. To tackle the challenges of lacking fine-grained and semantic-aware local shape control in global deformation through Jacobians, we introduce a trainable parameter as a weighting factor for the Jacobian at each triangle to adaptively change local shapes while maintaining global correspondences and facial features. Moreover, to ensure the coherence of the resulting shape and appearance from different viewpoints, we use pretrained image diffusion models for differentiable rendering with regularization terms to refine the deformation under text guidance. Extensive experiments demonstrate that our method can generate diverse head avatars with an articulated mesh that can be edited seaml
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#22635;&#34917;&#20102;&#26377;&#20851;&#20225;&#19994;&#20013;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#27835;&#29702;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#26088;&#22312;&#21033;&#29992;&#19994;&#21153;&#26426;&#20250;&#24182;&#20943;&#36731;&#19982;GenAI&#25972;&#21512;&#30456;&#20851;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2403.08802</link><description>&lt;p&gt;
&#20225;&#19994;&#20013;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#27835;&#29702;
&lt;/p&gt;
&lt;p&gt;
Governance of Generative Artificial Intelligence for Companies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#22635;&#34917;&#20102;&#26377;&#20851;&#20225;&#19994;&#20013;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#27835;&#29702;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#26088;&#22312;&#21033;&#29992;&#19994;&#21153;&#26426;&#20250;&#24182;&#20943;&#36731;&#19982;GenAI&#25972;&#21512;&#30456;&#20851;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#65292;&#29305;&#21035;&#26159;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24050;&#36805;&#36895;&#36827;&#20837;&#20225;&#19994;&#65292;&#20294;&#32570;&#20047;&#20805;&#20998;&#30340;&#27835;&#29702;&#65292;&#24102;&#26469;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;&#23613;&#31649;&#23545;GenAI&#20855;&#26377;&#21464;&#38761;&#24615;&#36136;&#21644;&#30417;&#31649;&#25514;&#26045;&#30340;&#24191;&#27867;&#35752;&#35770;&#65292;&#20294;&#26377;&#38480;&#30340;&#30740;&#31350;&#28041;&#21450;&#32452;&#32455;&#27835;&#29702;&#65292;&#21253;&#25324;&#25216;&#26415;&#21644;&#19994;&#21153;&#35270;&#35282;&#12290;&#26412;&#32508;&#36848;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#65292;&#35843;&#26597;&#20102;&#26368;&#36817;&#30340;&#30740;&#31350;&#12290;&#23427;&#19981;&#20165;&#20165;&#26159;&#24635;&#32467;&#65292;&#36824;&#36890;&#36807;&#21046;&#23450;&#36866;&#29992;&#20110;&#20225;&#19994;&#20869;&#30340;GenAI&#27835;&#29702;&#26694;&#26550;&#26469;&#36827;&#34892;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#35814;&#32454;&#25551;&#36848;&#20102;&#33539;&#22260;&#12289;&#30446;&#26631;&#21644;&#27835;&#29702;&#26426;&#21046;&#65292;&#26088;&#22312;&#21033;&#29992;&#19994;&#21153;&#26426;&#20250;&#24182;&#20943;&#36731;&#19982;GenAI&#25972;&#21512;&#30456;&#20851;&#39118;&#38505;&#12290;&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#19987;&#27880;&#20110;GenAI&#27835;&#29702;&#30340;&#26041;&#27861;&#65292;&#20026;&#20225;&#19994;&#22312;&#36127;&#36131;&#20219;&#30340;AI&#37319;&#29992;&#25361;&#25112;&#20013;&#25552;&#20379;&#20102;&#23454;&#29992;&#35265;&#35299;&#12290;&#23545;&#20110;&#25216;&#26415;&#20154;&#21592;&#26469;&#35828;&#65292;&#20063;&#26377;&#21161;&#20110;&#25299;&#23485;&#20182;&#20204;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08802v1 Announce Type: new  Abstract: Generative Artificial Intelligence (GenAI), specifically large language models like ChatGPT, has swiftly entered organizations without adequate governance, posing both opportunities and risks. Despite extensive debates on GenAI's transformative nature and regulatory measures, limited research addresses organizational governance, encompassing technical and business perspectives. This review paper fills this gap by surveying recent works. It goes beyond mere summarization by developing a framework for GenAI governance within companies. Our framework outlines the scope, objectives, and governance mechanisms tailored to harness business opportunities and mitigate risks associated with GenAI integration. This research contributes a focused approach to GenAI governance, offering practical insights for companies navigating the challenges of responsible AI adoption. It is also valuable for a technical audience to broaden their perspective as inc
&lt;/p&gt;</description></item><item><title>SGD&#35299;&#20915;&#26041;&#26696;&#38598;&#26159;&#19968;&#20010;&#26143;&#24418;&#22495;&#65292;&#21253;&#21547;&#19968;&#20010;&#26143;&#24418;&#27169;&#22411;&#65292;&#36890;&#36807;&#20302;&#25439;&#22833;&#25968;&#20540;&#30340;&#36335;&#24452;&#19982;&#20854;&#20182;&#35299;&#20915;&#26041;&#26696;&#32447;&#24615;&#30456;&#36830;&#65292;&#27169;&#38500;&#25490;&#21015;&#12290;</title><link>https://arxiv.org/abs/2403.07968</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#26041;&#26696;&#26159;&#21542;&#24418;&#25104;&#26143;&#24418;&#21306;&#22495;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Deep Neural Network Solutions Form a Star Domain?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07968
&lt;/p&gt;
&lt;p&gt;
SGD&#35299;&#20915;&#26041;&#26696;&#38598;&#26159;&#19968;&#20010;&#26143;&#24418;&#22495;&#65292;&#21253;&#21547;&#19968;&#20010;&#26143;&#24418;&#27169;&#22411;&#65292;&#36890;&#36807;&#20302;&#25439;&#22833;&#25968;&#20540;&#30340;&#36335;&#24452;&#19982;&#20854;&#20182;&#35299;&#20915;&#26041;&#26696;&#32447;&#24615;&#30456;&#36830;&#65292;&#27169;&#38500;&#25490;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Entezari&#31561;&#20154;&#65288;2022&#65289;&#25512;&#27979;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#21487;&#36798;&#21040;&#30340;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#26041;&#26696;&#38598;&#26159;&#20984;&#30340;&#65292;&#32771;&#34385;&#21040;&#25490;&#21015;&#19981;&#21464;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#21152;&#23485;&#26494;&#30340;&#35266;&#28857;&#65306;SGD&#35299;&#20915;&#26041;&#26696;&#38598;&#26159;&#19968;&#20010;&#26143;&#24418;&#22495;&#65292;&#21253;&#21547;&#19968;&#20010;&#26143;&#24418;&#27169;&#22411;&#65292;&#36890;&#36807;&#20302;&#25439;&#22833;&#25968;&#20540;&#30340;&#36335;&#24452;&#19982;&#20854;&#20182;&#35299;&#20915;&#26041;&#26696;&#32447;&#24615;&#30456;&#36830;&#65292;&#27169;&#38500;&#25490;&#21015;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Starlight&#31639;&#27861;&#65292;&#29992;&#20110;&#25214;&#21040;&#32473;&#23450;&#23398;&#20064;&#20219;&#21153;&#30340;&#26143;&#24418;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#36825;&#20010;&#26143;&#24418;&#27169;&#22411;&#19982;&#20854;&#20182;&#29420;&#31435;&#25214;&#21040;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#32447;&#24615;&#30456;&#36830;&#30340;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07968v1 Announce Type: cross  Abstract: Entezari et al. (2022) conjectured that neural network solution sets reachable via stochastic gradient descent (SGD) are convex, considering permutation invariances. This means that two independent solutions can be connected by a linear path with low loss, given one of them is appropriately permuted. However, current methods to test this theory often fail to eliminate loss barriers between two independent solutions (Ainsworth et al., 2022; Benzing et al., 2022). In this work, we conjecture that a more relaxed claim holds: the SGD solution set is a star domain that contains a star model that is linearly connected to all the other solutions via paths with low loss values, modulo permutations. We propose the Starlight algorithm that finds a star model of a given learning task. We validate our claim by showing that this star model is linearly connected with other independently found solutions. As an additional benefit of our study, we demo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;CodeAttack&#26694;&#26550;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#27867;&#21270;&#65292;&#30740;&#31350;&#21457;&#29616;GPT-4&#12289;Claude-2&#21644;Llama-2&#31995;&#21015;&#31561;&#26368;&#26032;&#27169;&#22411;&#23384;&#22312;&#20195;&#30721;&#36755;&#20837;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;</title><link>https://arxiv.org/abs/2403.07865</link><description>&lt;p&gt;
&#36890;&#36807;&#20195;&#30721;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#27867;&#21270;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Exploring Safety Generalization Challenges of Large Language Models via Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;CodeAttack&#26694;&#26550;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#27867;&#21270;&#65292;&#30740;&#31350;&#21457;&#29616;GPT-4&#12289;Claude-2&#21644;Llama-2&#31995;&#21015;&#31561;&#26368;&#26032;&#27169;&#22411;&#23384;&#22312;&#20195;&#30721;&#36755;&#20837;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#24102;&#26469;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#26174;&#33879;&#33021;&#21147;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#23427;&#20204;&#28508;&#22312;&#35823;&#29992;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;CodeAttack&#65292;&#19968;&#20010;&#23558;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#36716;&#25442;&#20026;&#20195;&#30721;&#36755;&#20837;&#30340;&#26694;&#26550;&#65292;&#20026;&#27979;&#35797;LLMs&#30340;&#23433;&#20840;&#27867;&#21270;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#29615;&#22659;&#12290;&#25105;&#20204;&#23545;&#21253;&#25324;GPT-4&#12289;Claude-2&#21644;Llama-2&#31995;&#21015;&#22312;&#20869;&#30340;&#26368;&#26032;LLMs&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#20195;&#30721;&#36755;&#20837;&#23384;&#22312;&#20849;&#21516;&#30340;&#23433;&#20840;&#28431;&#27934;&#65306;CodeAttack&#22312;&#36229;&#36807;80%&#30340;&#26102;&#38388;&#20869;&#22987;&#32456;&#32469;&#36807;&#25152;&#26377;&#27169;&#22411;&#30340;&#23433;&#20840;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07865v1 Announce Type: cross  Abstract: The rapid advancement of Large Language Models (LLMs) has brought about remarkable capabilities in natural language processing but also raised concerns about their potential misuse. While strategies like supervised fine-tuning and reinforcement learning from human feedback have enhanced their safety, these methods primarily focus on natural languages, which may not generalize to other domains. This paper introduces CodeAttack, a framework that transforms natural language inputs into code inputs, presenting a novel environment for testing the safety generalization of LLMs. Our comprehensive studies on state-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a common safety vulnerability of these models against code input: CodeAttack consistently bypasses the safety guardrails of all models more than 80\% of the time. Furthermore, we find that a larger distribution gap between CodeAttack and natural language leads to we
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#25512;&#29702;&#20013;&#23637;&#29616;&#20986;&#28508;&#21147;&#65292;&#20294;&#23384;&#22312;&#24187;&#35273;&#38382;&#39064;&#21644;&#19982;&#21307;&#29983;&#20915;&#31574;&#36335;&#24452;&#19981;&#19968;&#33268;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.06609</link><description>&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#31181;&#23376;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20020;&#24202;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Guiding Clinical Reasoning with Large Language Models via Knowledge Seeds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06609
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#25512;&#29702;&#20013;&#23637;&#29616;&#20986;&#28508;&#21147;&#65292;&#20294;&#23384;&#22312;&#24187;&#35273;&#38382;&#39064;&#21644;&#19982;&#21307;&#29983;&#20915;&#31574;&#36335;&#24452;&#19981;&#19968;&#33268;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#25512;&#29702;&#26159;&#21307;&#29983;&#22312;&#35780;&#20272;&#21644;&#31649;&#29702;&#24739;&#32773;&#26102;&#37319;&#29992;&#30340;&#35748;&#30693;&#36807;&#31243;&#12290;&#36825;&#20010;&#36807;&#31243;&#36890;&#24120;&#28041;&#21450;&#24314;&#35758;&#24517;&#35201;&#30340;&#26816;&#26597;&#65292;&#35786;&#26029;&#24739;&#32773;&#30142;&#30149;&#65292;&#24182;&#20915;&#23450;&#36866;&#24403;&#30340;&#27835;&#30103;&#31561;&#12290;&#20934;&#30830;&#30340;&#20020;&#24202;&#25512;&#29702;&#38656;&#35201;&#24191;&#27867;&#30340;&#21307;&#23398;&#30693;&#35782;&#21644;&#20016;&#23500;&#30340;&#20020;&#24202;&#32463;&#39564;&#65292;&#20026;&#21307;&#29983;&#35774;&#32622;&#20102;&#24456;&#39640;&#30340;&#38376;&#27099;&#12290;&#26368;&#36817;&#65292;&#20687;ChatGPT&#21644;GPT-4&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26174;&#31034;&#20986;&#22312;&#20020;&#24202;&#25512;&#29702;&#20013;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;LLMs&#23481;&#26131;&#20986;&#29616;&#24187;&#35273;&#38382;&#39064;&#65292;&#32780;LLMs&#30340;&#25512;&#29702;&#36807;&#31243;&#21487;&#33021;&#19982;&#21307;&#29983;&#30340;&#20020;&#24202;&#20915;&#31574;&#36335;&#24452;&#19981;&#19968;&#33268;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06609v1 Announce Type: cross  Abstract: Clinical reasoning refers to the cognitive process that physicians employ in evaluating and managing patients. This process typically involves suggesting necessary examinations, diagnosing patients' diseases, and deciding on appropriate therapies, etc. Accurate clinical reasoning requires extensive medical knowledge and rich clinical experience, setting a high bar for physicians. This is particularly challenging in developing countries due to the overwhelming number of patients and limited physician resources, contributing significantly to global health inequity and necessitating automated clinical reasoning approaches. Recently, the emergence of large language models (LLMs) such as ChatGPT and GPT-4 have demonstrated their potential in clinical reasoning. However, these LLMs are prone to hallucination problems, and the reasoning process of LLMs may not align with the clinical decision path of physicians. In this study, we introduce a 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#29366;&#24577;&#36827;&#34892;&#23454;&#26102;&#24187;&#35273;&#26816;&#27979;&#30340;&#26080;&#30417;&#30563;&#35757;&#32451;&#26694;&#26550;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.06448</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#29366;&#24577;&#30340;&#26080;&#30417;&#30563;&#23454;&#26102;&#24187;&#35273;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Real-Time Hallucination Detection based on the Internal States of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06448
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#29366;&#24577;&#36827;&#34892;&#23454;&#26102;&#24187;&#35273;&#26816;&#27979;&#30340;&#26080;&#30417;&#30563;&#35757;&#32451;&#26694;&#26550;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#26159;&#25351;&#20135;&#29983;&#36830;&#36143;&#20294;&#20107;&#23454;&#19981;&#20934;&#30830;&#30340;&#21709;&#24212;&#12290;&#20026;&#20102;&#35299;&#20915;LLMs&#20013;&#24187;&#35273;&#30340;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;MIND&#65292;&#19968;&#31181;&#21033;&#29992;LLMs&#20869;&#37096;&#29366;&#24577;&#36827;&#34892;&#23454;&#26102;&#24187;&#35273;&#26816;&#27979;&#30340;&#26080;&#30417;&#30563;&#35757;&#32451;&#26694;&#26550;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;HELM&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22810;&#20010;LLMs&#24187;&#35273;&#26816;&#27979;&#30340;&#26032;&#22522;&#20934;&#65292;&#22312;LLMs&#25512;&#29702;&#36807;&#31243;&#20013;&#20855;&#26377;&#22810;&#26679;&#21270;&#30340;LLM&#36755;&#20986;&#21644;&#20869;&#37096;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06448v1 Announce Type: cross  Abstract: Hallucinations in large language models (LLMs) refer to the phenomenon of LLMs producing responses that are coherent yet factually inaccurate. This issue undermines the effectiveness of LLMs in practical applications, necessitating research into detecting and mitigating hallucinations of LLMs. Previous studies have mainly concentrated on post-processing techniques for hallucination detection, which tend to be computationally intensive and limited in effectiveness due to their separation from the LLM's inference process. To overcome these limitations, we introduce MIND, an unsupervised training framework that leverages the internal states of LLMs for real-time hallucination detection without requiring manual annotations. Additionally, we present HELM, a new benchmark for evaluating hallucination detection across multiple LLMs, featuring diverse LLM outputs and the internal states of LLMs during their inference process. Our experiments d
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26368;&#22823;&#21270;&#20004;&#20010;&#20219;&#21153;&#30340;&#34920;&#31034;&#29305;&#24449;&#30340;&#20114;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#24605;&#32500;&#38142;&#33976;&#39311;&#20013;&#26631;&#31614;&#39044;&#27979;&#20219;&#21153;&#19982;&#30693;&#35782;&#38598;&#25104;&#19981;&#36275;&#38382;&#39064;&#30340;&#21464;&#20998;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.03348</link><description>&lt;p&gt;
&#23398;&#20064;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#36827;&#34892;&#24605;&#32500;&#38142;&#25552;&#28860;
&lt;/p&gt;
&lt;p&gt;
Learning to Maximize Mutual Information for Chain-of-Thought Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03348
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26368;&#22823;&#21270;&#20004;&#20010;&#20219;&#21153;&#30340;&#34920;&#31034;&#29305;&#24449;&#30340;&#20114;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#24605;&#32500;&#38142;&#33976;&#39311;&#20013;&#26631;&#31614;&#39044;&#27979;&#20219;&#21153;&#19982;&#30693;&#35782;&#38598;&#25104;&#19981;&#36275;&#38382;&#39064;&#30340;&#21464;&#20998;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#26159;&#23558;&#22823;&#22411;&#22797;&#26434;&#27169;&#22411;&#30340;&#30693;&#35782;&#20256;&#36882;&#32473;&#36739;&#23567;&#27169;&#22411;&#30340;&#25216;&#26415;&#65292;&#26159;&#23454;&#29616;&#39640;&#25928;&#20154;&#24037;&#26234;&#33021;&#37096;&#32626;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290;&#36890;&#36807;&#21033;&#29992;&#24605;&#32500;&#38142; (CoT) &#33976;&#39311;&#30340;&#26032;&#26041;&#27861;&#8212;&#8212;&#36880;&#27493;&#33976;&#39311; (DSS)&#65292;&#24050;&#32463;&#23637;&#31034;&#20986;&#20026;&#36739;&#23567;&#27169;&#22411;&#36171;&#20104;&#20854;&#36739;&#22823;&#21516;&#34892;&#30340;&#20248;&#36234;&#25512;&#29702;&#33021;&#21147;&#30340;&#28508;&#21147;&#12290;&#22312;DSS&#20013;&#65292;&#33976;&#39311;&#27169;&#22411;&#36890;&#36807;&#19968;&#20010;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#21516;&#26102;&#33719;&#24471;&#29983;&#25104;&#29702;&#30001;&#21644;&#39044;&#27979;&#26631;&#31614;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;DSS&#24573;&#30053;&#20102;&#36825;&#20004;&#20010;&#35757;&#32451;&#20219;&#21153;&#20043;&#38388;&#30340;&#20869;&#22312;&#20851;&#31995;&#65292;&#23548;&#33268;CoT&#30693;&#35782;&#19982;&#26631;&#31614;&#39044;&#27979;&#20219;&#21153;&#30340;&#26377;&#25928;&#25972;&#21512;&#19981;&#36275;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20174;&#20449;&#24687;&#29942;&#39048;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#20004;&#20010;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#65292;&#24182;&#23558;&#20854;&#34920;&#36848;&#20026;&#26368;&#22823;&#21270;&#20004;&#20010;&#20219;&#21153;&#30340;&#34920;&#31034;&#29305;&#24449;&#30340;&#20114;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#20998;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03348v1 Announce Type: cross  Abstract: Knowledge distillation, the technique of transferring knowledge from large, complex models to smaller ones, marks a pivotal step towards efficient AI deployment. Distilling Step-by-Step (DSS), a novel method utilizing chain-of-thought (CoT) distillation, has demonstrated promise by imbuing smaller models with the superior reasoning capabilities of their larger counterparts. In DSS, the distilled model acquires the ability to generate rationales and predict labels concurrently through a multi-task learning framework. However, DSS overlooks the intrinsic relationship between the two training tasks, leading to ineffective integration of CoT knowledge with the task of label prediction. To this end, we investigate the mutual relationship of the two tasks from Information Bottleneck perspective and formulate it as maximizing the mutual information of the representation features of the two tasks. We propose a variational approach to solve thi
&lt;/p&gt;</description></item><item><title>HALC&#26159;&#19968;&#31181;&#26088;&#22312;&#20943;&#23569;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#35937;&#24187;&#35273;&#30340;&#26032;&#39062;&#35299;&#30721;&#31639;&#27861;&#65292;&#36890;&#36807;&#23616;&#37096;&#30340;&#33258;&#21160;&#32858;&#28966;&#22522;&#20934;&#26426;&#21046;&#21644;&#20840;&#23616;&#30340;&#27874;&#26463;&#25628;&#32034;&#31639;&#27861;&#65292;&#25104;&#21151;&#20943;&#23569;OH&#32780;&#20445;&#25345;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#65292;&#21516;&#26102;&#21487;&#20197;&#20316;&#20026;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#38598;&#25104;&#21040;&#20219;&#20309;LVLMs&#20013;&#12290;</title><link>https://arxiv.org/abs/2403.00425</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#28966;&#28857;&#23545;&#27604;&#35299;&#30721;&#20943;&#23569;&#23545;&#35937;&#24187;&#35273;&#65306;HALC
&lt;/p&gt;
&lt;p&gt;
HALC: Object Hallucination Reduction via Adaptive Focal-Contrast Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00425
&lt;/p&gt;
&lt;p&gt;
HALC&#26159;&#19968;&#31181;&#26088;&#22312;&#20943;&#23569;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#35937;&#24187;&#35273;&#30340;&#26032;&#39062;&#35299;&#30721;&#31639;&#27861;&#65292;&#36890;&#36807;&#23616;&#37096;&#30340;&#33258;&#21160;&#32858;&#28966;&#22522;&#20934;&#26426;&#21046;&#21644;&#20840;&#23616;&#30340;&#27874;&#26463;&#25628;&#32034;&#31639;&#27861;&#65292;&#25104;&#21151;&#20943;&#23569;OH&#32780;&#20445;&#25345;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#65292;&#21516;&#26102;&#21487;&#20197;&#20316;&#20026;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#38598;&#25104;&#21040;&#20219;&#20309;LVLMs&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35299;&#37322;&#22810;&#27169;&#24577;&#29615;&#22659;&#26041;&#38754;&#65292;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#23637;&#29616;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#19981;&#21487;&#36991;&#20813;&#22320;&#20250;&#21463;&#21040;&#23545;&#35937;&#24187;&#35273;&#65288;OH&#65289;&#30340;&#22256;&#25200;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;HALC&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#30721;&#31639;&#27861;&#65292;&#26088;&#22312;&#20943;&#23569;LVLMs&#20013;&#30340;OH&#12290;HALC&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#20013;&#29420;&#29305;&#30340;&#32454;&#31890;&#24230;&#26368;&#20339;&#35270;&#35273;&#20449;&#24687;&#65292;&#24182;&#21516;&#26102;&#22312;&#23616;&#37096;&#21644;&#20840;&#23616;&#19978;&#25805;&#20316;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;HALC&#38598;&#25104;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#33258;&#21160;&#32858;&#28966;&#22522;&#20934;&#26426;&#21046;&#65288;&#23616;&#37096;&#65289;&#65292;&#22312;&#36816;&#34892;&#26102;&#32416;&#27491;&#20135;&#29983;&#24187;&#35273;&#30340;&#26631;&#35760;&#65292;&#20197;&#21450;&#19968;&#31181;&#19987;&#38376;&#30340;&#27874;&#26463;&#25628;&#32034;&#31639;&#27861;&#65288;&#20840;&#23616;&#65289;&#65292;&#20197;&#26174;&#30528;&#20943;&#23569;OH&#65292;&#21516;&#26102;&#20445;&#25345;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;HALC&#21487;&#20197;&#20316;&#20026;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#38598;&#25104;&#21040;&#20219;&#20309;LVLMs&#20013;&#65292;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#12290;&#22823;&#37327;&#23454;&#39564;&#30740;&#31350;&#35777;&#26126;&#20102;HALC&#22312;&#20943;&#23569;OH&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#20248;&#20110;&#22235;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00425v1 Announce Type: cross  Abstract: While large vision-language models (LVLMs) have demonstrated impressive capabilities in interpreting multi-modal contexts, they invariably suffer from object hallucinations (OH). We introduce HALC, a novel decoding algorithm designed to mitigate OH in LVLMs. HALC leverages distinct fine-grained optimal visual information in vision-language tasks and operates on both local and global contexts simultaneously. Specifically, HALC integrates a robust auto-focal grounding mechanism (locally) to correct hallucinated tokens on the fly, and a specialized beam search algorithm (globally) to significantly reduce OH while preserving text generation quality. Additionally, HALC can be integrated into any LVLMs as a plug-and-play module without extra training. Extensive experimental studies demonstrate the effectiveness of HALC in reducing OH, outperforming state-of-the-arts across four benchmarks.
&lt;/p&gt;</description></item><item><title>Resonance RoPE&#26159;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;RoPE&#29305;&#24449;&#30340;&#25554;&#20540;&#26469;&#32553;&#23567;&#35757;&#32451;&#30701;-&#27979;&#35797;&#38271;&#22330;&#26223;&#19979;&#30340;&#27867;&#21270;&#24046;&#36317;&#65292;&#22312;&#19981;&#22686;&#21152;&#39069;&#22806;&#22312;&#32447;&#35745;&#31639;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00071</link><description>&lt;p&gt;
&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#27867;&#21270;&#33021;&#21147;&#65306;&#20849;&#25391; RoPE
&lt;/p&gt;
&lt;p&gt;
Resonance RoPE: Improving Context Length Generalization of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00071
&lt;/p&gt;
&lt;p&gt;
Resonance RoPE&#26159;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;RoPE&#29305;&#24449;&#30340;&#25554;&#20540;&#26469;&#32553;&#23567;&#35757;&#32451;&#30701;-&#27979;&#35797;&#38271;&#22330;&#26223;&#19979;&#30340;&#27867;&#21270;&#24046;&#36317;&#65292;&#22312;&#19981;&#22686;&#21152;&#39069;&#22806;&#22312;&#32447;&#35745;&#31639;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#35757;&#32451;&#30701;-&#27979;&#35797;&#38271;&#65288;TSTL&#65289;&#22330;&#26223;&#25361;&#25112;&#65292;&#24341;&#20837;&#20102;Rotary Position Embedding&#65288;RoPE&#65289;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#22312;&#36739;&#30701;&#24207;&#21015;&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#36739;&#38271;&#24207;&#21015;&#20013;&#36935;&#21040;&#20301;&#32622;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#30340;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Resonance RoPE&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;RoPE&#29305;&#24449;&#30340;&#25554;&#20540;&#26469;&#32553;&#23567;TSTL&#22330;&#26223;&#20013;&#30340;&#27867;&#21270;&#24046;&#36317;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#22312;&#32447;&#35745;&#31639;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PosGen&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#21512;&#25104;&#22522;&#20934;&#65292;&#19987;&#38376;&#38024;&#23545;TSTL&#22330;&#26223;&#20013;&#30340;&#31934;&#32454;&#34892;&#20026;&#20998;&#26512;&#65292;&#26088;&#22312;&#20174;&#38271;&#19978;&#19979;&#25991;&#20013;&#19981;&#26029;&#22686;&#21152;&#30340;&#20196;&#29260;&#29983;&#25104;&#22256;&#38590;&#21644;&#35782;&#21035;&#26032;&#20196;&#29260;&#20301;&#32622;&#30340;&#25361;&#25112;&#20013;&#20998;&#31163;&#20986;&#26469;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#24212;&#29992;Resonance RoPE&#21518;&#65292;Transformer&#27169;&#22411;&#21487;&#20197;&#35782;&#21035;OOD&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00071v1 Announce Type: cross  Abstract: This paper addresses the challenge of train-short-test-long (TSTL) scenarios in Large Language Models (LLMs) equipped with Rotary Position Embedding (RoPE), where models pre-trained on shorter sequences face difficulty with out-of-distribution (OOD) token positions in longer sequences. We introduce Resonance RoPE, a novel approach designed to narrow the generalization gap in TSTL scenarios by refining the interpolation of RoPE features for OOD positions, significantly improving the model performance without additional online computational costs. Furthermore, we present PosGen, a new synthetic benchmark specifically designed for fine-grained behavior analysis in TSTL scenarios, aiming to isolate the constantly increasing difficulty of token generation on long contexts from the challenges of recognizing new token positions. Our experiments on synthetic tasks show that after applying Resonance RoPE, Transformers recognize OOD position bet
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22810;&#22836;softmax&#27880;&#24847;&#21147;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#35777;&#26126;&#20102;&#20840;&#23616;&#25910;&#25947;&#24615;&#65292;&#24182;&#21457;&#29616;&#20102;&#8220;&#20219;&#21153;&#20998;&#37197;&#8221;&#29616;&#35937;&#65292;&#26799;&#24230;&#27969;&#21160;&#20998;&#20026;&#28909;&#36523;&#12289;&#28044;&#29616;&#21644;&#25910;&#25947;&#19977;&#20010;&#38454;&#27573;&#65292;&#26368;&#32456;&#35777;&#26126;&#20102;&#26799;&#24230;&#27969;&#30340;&#26368;&#20248;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.19442</link><description>&lt;p&gt;
&#22810;&#22836;softmax&#27880;&#24847;&#21147;&#26426;&#21046;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#35757;&#32451;&#21160;&#24577;&#65306;&#28044;&#29616;&#12289;&#25910;&#25947;&#21644;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;
Training Dynamics of Multi-Head Softmax Attention for In-Context Learning: Emergence, Convergence, and Optimality
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19442
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22810;&#22836;softmax&#27880;&#24847;&#21147;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#35777;&#26126;&#20102;&#20840;&#23616;&#25910;&#25947;&#24615;&#65292;&#24182;&#21457;&#29616;&#20102;&#8220;&#20219;&#21153;&#20998;&#37197;&#8221;&#29616;&#35937;&#65292;&#26799;&#24230;&#27969;&#21160;&#20998;&#20026;&#28909;&#36523;&#12289;&#28044;&#29616;&#21644;&#25910;&#25947;&#19977;&#20010;&#38454;&#27573;&#65292;&#26368;&#32456;&#35777;&#26126;&#20102;&#26799;&#24230;&#27969;&#30340;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#22810;&#20219;&#21153;&#32447;&#24615;&#22238;&#24402;&#30340;&#22810;&#22836;softmax&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#26799;&#24230;&#27969;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#36866;&#24403;&#30340;&#21021;&#22987;&#21270;&#36873;&#25321;&#19979;&#65292;&#26799;&#24230;&#27969;&#21160;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#26799;&#24230;&#27969;&#21160;&#21160;&#21147;&#23398;&#20013;&#20986;&#29616;&#20102;&#26377;&#36259;&#30340;&#8220;&#20219;&#21153;&#20998;&#37197;&#8221;&#29616;&#35937;&#65292;&#27599;&#20010;&#27880;&#24847;&#21147;&#22836;&#37117;&#19987;&#27880;&#20110;&#35299;&#20915;&#22810;&#20219;&#21153;&#27169;&#22411;&#20013;&#30340;&#21333;&#20010;&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26799;&#24230;&#27969;&#21160;&#21160;&#21147;&#23398;&#21487;&#20197;&#20998;&#20026;&#19977;&#20010;&#38454;&#27573;&#8212;&#8212;&#28909;&#36523;&#38454;&#27573;&#65292;&#22312;&#36825;&#20010;&#38454;&#27573;&#25439;&#22833;&#20943;&#23569;&#36895;&#24230;&#36739;&#24930;&#65292;&#27880;&#24847;&#21147;&#22836;&#36880;&#28176;&#20542;&#21521;&#20110;&#21508;&#33258;&#30340;&#20219;&#21153;&#65307;&#28044;&#29616;&#38454;&#27573;&#65292;&#22312;&#36825;&#20010;&#38454;&#27573;&#65292;&#27599;&#20010;&#22836;&#36873;&#25321;&#19968;&#20010;&#21333;&#29420;&#30340;&#20219;&#21153;&#65292;&#25439;&#22833;&#36805;&#36895;&#20943;&#23569;&#65307;&#21644;&#25910;&#25947;&#38454;&#27573;&#65292;&#22312;&#36825;&#20010;&#38454;&#27573;&#65292;&#27880;&#24847;&#21147;&#21442;&#25968;&#25910;&#25947;&#21040;&#19968;&#20010;&#26497;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26799;&#24230;&#27969;&#22312;&#23398;&#20064;&#26497;&#38480;&#27169;&#22411;&#26041;&#38754;&#30340;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19442v1 Announce Type: cross  Abstract: We study the dynamics of gradient flow for training a multi-head softmax attention model for in-context learning of multi-task linear regression. We establish the global convergence of gradient flow under suitable choices of initialization. In addition, we prove that an interesting "task allocation" phenomenon emerges during the gradient flow dynamics, where each attention head focuses on solving a single task of the multi-task model. Specifically, we prove that the gradient flow dynamics can be split into three phases -- a warm-up phase where the loss decreases rather slowly and the attention heads gradually build up their inclination towards individual tasks, an emergence phase where each head selects a single task and the loss rapidly decreases, and a convergence phase where the attention parameters converge to a limit. Furthermore, we prove the optimality of gradient flow in the sense that the limiting model learned by gradient flo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#35299;&#20915;&#21160;&#21147;&#31995;&#32479;&#37325;&#26500;&#20013;&#27867;&#21270;&#38382;&#39064;&#30340;&#27491;&#24335;&#26694;&#26550;, &#24182;&#38416;&#36848;&#20102;&#36328;&#39046;&#22495;&#27867;&#21270;&#22312;DSR&#20013;&#19982;&#26426;&#22120;&#23398;&#20064;&#20854;&#20182;&#39046;&#22495;&#30340;&#19981;&#21516;&#20043;&#22788;</title><link>https://arxiv.org/abs/2402.18377</link><description>&lt;p&gt;
&#21160;&#21147;&#31995;&#32479;&#37325;&#26500;&#20013;&#30340;&#36328;&#39046;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Out-of-Domain Generalization in Dynamical Systems Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18377
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#35299;&#20915;&#21160;&#21147;&#31995;&#32479;&#37325;&#26500;&#20013;&#27867;&#21270;&#38382;&#39064;&#30340;&#27491;&#24335;&#26694;&#26550;, &#24182;&#38416;&#36848;&#20102;&#36328;&#39046;&#22495;&#27867;&#21270;&#22312;DSR&#20013;&#19982;&#26426;&#22120;&#23398;&#20064;&#20854;&#20182;&#39046;&#22495;&#30340;&#19981;&#21516;&#20043;&#22788;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#20013;&#65292;&#25105;&#20204;&#33268;&#21147;&#20110;&#25214;&#21040;&#22312;&#32463;&#39564;&#29616;&#35937;&#32972;&#21518;&#30340;&#25511;&#21046;&#26041;&#31243;&#21644;&#21160;&#21147;&#35268;&#21017;&#12290;&#20256;&#32479;&#19978;&#65292;&#31185;&#23398;&#27169;&#22411;&#26159;&#36890;&#36807;&#20154;&#31867;&#27934;&#23519;&#21644;&#23454;&#39564;&#21608;&#26399;&#25512;&#23548;&#20986;&#26469;&#30340;&#65292;&#26368;&#36817;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24050;&#32463;&#34987;&#29992;&#26469;&#30452;&#25509;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#37325;&#26500;&#21160;&#21147;&#31995;&#32479;&#65288;DS&#65289;&#12290;&#26368;&#20808;&#36827;&#30340;&#21160;&#21147;&#31995;&#32479;&#37325;&#26500;&#65288;DSR&#65289;&#26041;&#27861;&#22312;&#25429;&#25417;&#35266;&#23519;&#21040;&#30340;DS&#30340;&#19981;&#21464;&#21644;&#38271;&#26399;&#29305;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#21069;&#26223;&#65292;&#20294;&#23427;&#20204;&#27867;&#21270;&#21040;&#26410;&#35266;&#23519;&#39046;&#22495;&#30340;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#24453;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#36825;&#26159;&#25105;&#20204;&#26399;&#26395;&#20174;&#20219;&#20309;&#21487;&#34892;&#30340;&#31185;&#23398;&#29702;&#35770;&#20013;&#33719;&#24471;&#30340;&#33267;&#20851;&#37325;&#35201;&#30340;&#23646;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#27491;&#24335;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;DSR&#20013;&#30340;&#27867;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#20197;&#21450;&#22914;&#20309;DSR&#20013;&#30340;&#36328;&#39046;&#22495;&#65288;OOD&#65289;&#27867;&#21270;&#65288;OODG&#65289;&#19982;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#32771;&#34385;&#30340;OODG&#26377;&#26681;&#26412;&#21306;&#21035;&#12290;&#25105;&#20204;&#20171;&#32461;&#22522;&#20110;&#25299;&#25169;&#27010;&#24565;&#21644;&#31526;&#21495;&#30340;&#25968;&#23398;&#27010;&#24565;&#65292;&#24182;&#35828;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18377v1 Announce Type: new  Abstract: In science we are interested in finding the governing equations, the dynamical rules, underlying empirical phenomena. While traditionally scientific models are derived through cycles of human insight and experimentation, recently deep learning (DL) techniques have been advanced to reconstruct dynamical systems (DS) directly from time series data. State-of-the-art dynamical systems reconstruction (DSR) methods show promise in capturing invariant and long-term properties of observed DS, but their ability to generalize to unobserved domains remains an open challenge. Yet, this is a crucial property we would expect from any viable scientific theory. In this work, we provide a formal framework that addresses generalization in DSR. We explain why and how out-of-domain (OOD) generalization (OODG) in DSR profoundly differs from OODG considered elsewhere in machine learning. We introduce mathematical notions based on topological concepts and ergo
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20266;&#35013;&#21644;&#37325;&#26500;&#25915;&#20987;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36234;&#29425;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25104;&#21151;&#38544;&#34255;&#26377;&#23475;&#25351;&#20196;&#24182;&#24341;&#23548;&#27169;&#22411;&#37325;&#26500;&#21407;&#25351;&#20196;&#65292;&#21462;&#24471;&#20102;90%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.18104</link><description>&lt;p&gt;
&#36890;&#36807;&#20266;&#35013;&#21644;&#37325;&#26500;&#22312;&#23569;&#37327;&#26597;&#35810;&#20013;&#36234;&#29425;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Making Them Ask and Answer: Jailbreaking Large Language Models in Few Queries via Disguise and Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18104
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20266;&#35013;&#21644;&#37325;&#26500;&#25915;&#20987;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36234;&#29425;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25104;&#21151;&#38544;&#34255;&#26377;&#23475;&#25351;&#20196;&#24182;&#24341;&#23548;&#27169;&#22411;&#37325;&#26500;&#21407;&#25351;&#20196;&#65292;&#21462;&#24471;&#20102;90%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#25104;&#21151;&#65292;&#20294;LLMs&#30340;&#21487;&#20449;&#24230;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20043;&#35868;&#12290;&#20854;&#20013;&#19968;&#20010;&#29305;&#23450;&#30340;&#23041;&#32961;&#26159;&#21487;&#33021;&#29983;&#25104;&#26377;&#27602;&#25110;&#26377;&#23475;&#30340;&#22238;&#24212;&#12290;&#25915;&#20987;&#32773;&#21487;&#20197;&#21046;&#20316;&#26377;&#38024;&#23545;&#24615;&#30340;&#25552;&#31034;&#65292;&#35825;&#20351;LLMs&#29983;&#25104;&#26377;&#23475;&#30340;&#22238;&#24212;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35782;&#21035;&#22312;&#23433;&#20840;&#24494;&#35843;&#20013;&#30340;&#20559;&#35265;&#28431;&#27934;&#65292;&#24320;&#21019;&#20102;LLMs&#23433;&#20840;&#39046;&#22495;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;DRA&#65288;&#20266;&#35013;&#21644;&#37325;&#26500;&#25915;&#20987;&#65289;&#30340;&#40657;&#30418;&#36234;&#29425;&#26041;&#27861;&#65292;&#36890;&#36807;&#20266;&#35013;&#26469;&#38544;&#34255;&#26377;&#23475;&#25351;&#20196;&#65292;&#24182;&#25552;&#31034;&#27169;&#22411;&#22312;&#23436;&#25104;&#36807;&#31243;&#20013;&#37325;&#26500;&#21407;&#22987;&#26377;&#23475;&#25351;&#20196;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;DRA&#22312;&#21508;&#31181;&#24320;&#28304;&#21644;&#38381;&#28304;&#27169;&#22411;&#19978;&#30340;&#34920;&#29616;&#65292;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#36234;&#29425;&#25104;&#21151;&#29575;&#21644;&#25915;&#20987;&#25928;&#29575;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;DRA&#22312;LLM&#32842;&#22825;&#26426;&#22120;&#20154;GPT-4&#19978;&#25317;&#26377;90\%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18104v1 Announce Type: cross  Abstract: In recent years, large language models (LLMs) have demonstrated notable success across various tasks, but the trustworthiness of LLMs is still an open problem. One specific threat is the potential to generate toxic or harmful responses. Attackers can craft adversarial prompts that induce harmful responses from LLMs. In this work, we pioneer a theoretical foundation in LLMs security by identifying bias vulnerabilities within the safety fine-tuning and design a black-box jailbreak method named DRA (Disguise and Reconstruction Attack), which conceals harmful instructions through disguise and prompts the model to reconstruct the original harmful instruction within its completion. We evaluate DRA across various open-source and close-source models, showcasing state-of-the-art jailbreak success rates and attack efficiency. Notably, DRA boasts a 90\% attack success rate on LLM chatbots GPT-4.
&lt;/p&gt;</description></item><item><title>RLHF&#22312;&#32771;&#34385;&#37096;&#20998;&#35266;&#23519;&#24615;&#26102;&#21487;&#33021;&#23548;&#33268;&#31574;&#30053;&#27450;&#39575;&#24615;&#22320;&#22840;&#22823;&#24615;&#33021;&#25110;&#36807;&#24230;&#36777;&#25252;&#34892;&#20026;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25968;&#23398;&#26465;&#20214;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#35686;&#21578;&#19981;&#35201;&#30450;&#30446;&#24212;&#29992;RLHF&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#24773;&#20917;&#19979;&#12290;</title><link>https://arxiv.org/abs/2402.17747</link><description>&lt;p&gt;
&#24403;&#20320;&#30340;AI&#27450;&#39575;&#20320;&#65306;&#22312;&#22870;&#21169;&#23398;&#20064;&#20013;&#20154;&#31867;&#35780;&#20272;&#32773;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
When Your AI Deceives You: Challenges with Partial Observability of Human Evaluators in Reward Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17747
&lt;/p&gt;
&lt;p&gt;
RLHF&#22312;&#32771;&#34385;&#37096;&#20998;&#35266;&#23519;&#24615;&#26102;&#21487;&#33021;&#23548;&#33268;&#31574;&#30053;&#27450;&#39575;&#24615;&#22320;&#22840;&#22823;&#24615;&#33021;&#25110;&#36807;&#24230;&#36777;&#25252;&#34892;&#20026;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25968;&#23398;&#26465;&#20214;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#35686;&#21578;&#19981;&#35201;&#30450;&#30446;&#24212;&#29992;RLHF&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#30340;&#36807;&#21435;&#20998;&#26512;&#20551;&#35774;&#20154;&#31867;&#23436;&#20840;&#35266;&#23519;&#21040;&#29615;&#22659;&#12290;&#24403;&#20154;&#31867;&#21453;&#39304;&#20165;&#22522;&#20110;&#37096;&#20998;&#35266;&#23519;&#26102;&#20250;&#21457;&#29983;&#20160;&#20040;&#65311;&#25105;&#20204;&#23545;&#20004;&#31181;&#22833;&#36133;&#24773;&#20917;&#36827;&#34892;&#20102;&#27491;&#24335;&#23450;&#20041;&#65306;&#27450;&#39575;&#21644;&#36807;&#24230;&#36777;&#25252;&#12290;&#36890;&#36807;&#23558;&#20154;&#31867;&#24314;&#27169;&#20026;&#23545;&#36712;&#36857;&#20449;&#24565;&#30340;Boltzmann-&#29702;&#24615;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;RLHF&#20445;&#35777;&#20250;&#23548;&#33268;&#31574;&#30053;&#27450;&#39575;&#24615;&#22320;&#22840;&#22823;&#20854;&#24615;&#33021;&#12289;&#20026;&#20102;&#30041;&#19979;&#21360;&#35937;&#32780;&#36807;&#24230;&#36777;&#25252;&#25110;&#32773;&#20004;&#32773;&#20860;&#32780;&#26377;&#20043;&#30340;&#26465;&#20214;&#12290;&#20026;&#20102;&#24110;&#21161;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25968;&#23398;&#22320;&#21051;&#30011;&#20102;&#29615;&#22659;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#22914;&#20309;&#36716;&#21270;&#20026;&#65288;&#32570;&#20047;&#65289;&#23398;&#21040;&#30340;&#22238;&#25253;&#20989;&#25968;&#20013;&#30340;&#27169;&#31946;&#24615;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#32771;&#34385;&#29615;&#22659;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#20351;&#24471;&#22312;&#29702;&#35770;&#19978;&#21487;&#33021;&#24674;&#22797;&#22238;&#25253;&#20989;&#25968;&#21644;&#26368;&#20248;&#31574;&#30053;&#65292;&#32780;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#65292;&#23384;&#22312;&#19981;&#21487;&#20943;&#23569;&#30340;&#27169;&#31946;&#24615;&#12290;&#25105;&#20204;&#35686;&#21578;&#19981;&#35201;&#30450;&#30446;&#24212;&#29992;RLHF&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17747v1 Announce Type: cross  Abstract: Past analyses of reinforcement learning from human feedback (RLHF) assume that the human fully observes the environment. What happens when human feedback is based only on partial observations? We formally define two failure cases: deception and overjustification. Modeling the human as Boltzmann-rational w.r.t. a belief over trajectories, we prove conditions under which RLHF is guaranteed to result in policies that deceptively inflate their performance, overjustify their behavior to make an impression, or both. To help address these issues, we mathematically characterize how partial observability of the environment translates into (lack of) ambiguity in the learned return function. In some cases, accounting for partial observability makes it theoretically possible to recover the return function and thus the optimal policy, while in other cases, there is irreducible ambiguity. We caution against blindly applying RLHF in partially observa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;QRData&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32479;&#35745;&#21644;&#22240;&#26524;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#26368;&#24378;&#27169;&#22411;GPT-4&#22312;&#35813;&#27979;&#35797;&#20013;&#20934;&#30830;&#29575;&#20026;58&#65285;&#65292;&#23384;&#22312;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2402.17644</link><description>&lt;p&gt;
LLMs&#26159;&#21542;&#20855;&#22791;&#22522;&#20110;&#25968;&#25454;&#30340;&#32479;&#35745;&#21644;&#22240;&#26524;&#25512;&#29702;&#33021;&#21147;&#65311;&#29992;&#25968;&#25454;&#23545;&#20808;&#36827;&#30340;&#23450;&#37327;&#25512;&#29702;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Are LLMs Capable of Data-based Statistical and Causal Reasoning? Benchmarking Advanced Quantitative Reasoning with Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17644
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;QRData&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32479;&#35745;&#21644;&#22240;&#26524;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#26368;&#24378;&#27169;&#22411;GPT-4&#22312;&#35813;&#27979;&#35797;&#20013;&#20934;&#30830;&#29575;&#20026;58&#65285;&#65292;&#23384;&#22312;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#25512;&#29702;&#26159;&#20998;&#26512;&#25968;&#25454;&#30340;&#20851;&#38190;&#25216;&#33021;&#65292;&#28982;&#32780;&#23545;&#36825;&#31181;&#33021;&#21147;&#30340;&#35780;&#20272;&#20173;&#28982;&#26377;&#38480;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Quantitative Reasoning with Data&#65288;QRData&#65289;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32479;&#35745;&#21644;&#22240;&#26524;&#25512;&#29702;&#26041;&#38754;&#19982;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#21253;&#25324;&#19968;&#20010;&#31934;&#24515;&#26500;&#24314;&#30340;&#21253;&#21547;&#26469;&#33258;&#25945;&#31185;&#20070;&#12289;&#22312;&#32447;&#23398;&#20064;&#26448;&#26009;&#21644;&#23398;&#26415;&#35770;&#25991;&#30340;&#25968;&#25454;&#34920;&#30340;411&#20010;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#27604;&#36739;&#27169;&#22411;&#22312;&#25968;&#25454;&#21644;&#25991;&#26412;&#19978;&#30340;&#23450;&#37327;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#36824;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#28155;&#21152;&#20102;&#19968;&#20010;&#21253;&#21547;290&#20010;&#20165;&#25991;&#26412;&#38382;&#39064;&#30340;&#36741;&#21161;&#25968;&#25454;&#38598;&#65292;&#21363;QRText&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#12289;&#22522;&#20110;&#31243;&#24207;&#25512;&#29702;&#21644;&#20195;&#29702;&#25512;&#29702;&#26041;&#27861;&#65292;&#21253;&#25324;Chain-of-Thought&#12289;Program-of-Thoughts&#12289;ReAct&#21644;&#20195;&#30721;&#35299;&#37322;&#22120;&#36741;&#21161;&#31561;&#22312;&#21508;&#31181;&#27169;&#22411;&#19978;&#30340;&#34920;&#29616;&#12290;&#26368;&#24378;&#30340;&#27169;&#22411;GPT-4&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;58&#65285;&#65292;&#20294;&#20173;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17644v1 Announce Type: cross  Abstract: Quantitative reasoning is a critical skill to analyze data, yet the assessment of such ability remains limited. To address this gap, we introduce the Quantitative Reasoning with Data (QRData) benchmark, aiming to evaluate Large Language Models' capability in statistical and causal reasoning with real-world data. The benchmark comprises a carefully constructed dataset of 411 questions accompanied by data sheets from textbooks, online learning materials, and academic papers. To compare models' quantitative reasoning abilities on data and text, we enrich the benchmark with an auxiliary set of 290 text-only questions, namely QRText. We evaluate natural language reasoning, program-based reasoning, and agent reasoning methods including Chain-of-Thought, Program-of-Thoughts, ReAct, and code interpreter assistants on diverse models. The strongest model GPT-4 achieves an accuracy of 58%, which has a large room for improvement. Among open-source
&lt;/p&gt;</description></item><item><title>&#23376;&#35789;&#26631;&#35760;&#21270;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#20027;&#27969;&#26631;&#20934;&#65292;&#20294;&#20854;&#25104;&#21151;&#22240;&#32032;&#65292;&#22914;&#19981;&#21516;&#20219;&#21153;&#21644;&#35821;&#35328;&#30340;&#26368;&#20339;&#20998;&#21106;&#31890;&#24230;&#12289;&#25968;&#25454;&#28304;&#23545;&#26631;&#35760;&#24037;&#20855;&#30340;&#24433;&#21709;&#20197;&#21450;&#24418;&#24577;&#20449;&#24687;&#22312;&#21360;&#27431;&#35821;&#35328;&#20013;&#30340;&#20316;&#29992;&#65292;&#20173;&#19981;&#26126;&#30830;&#12290;</title><link>https://arxiv.org/abs/2402.15010</link><description>&lt;p&gt;
&#27861;&#35821;&#21307;&#29992;&#21475;&#32617;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26631;&#35760;&#21270;&#26377;&#22810;&#37325;&#35201;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Important Is Tokenization in French Medical Masked Language Models?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15010
&lt;/p&gt;
&lt;p&gt;
&#23376;&#35789;&#26631;&#35760;&#21270;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#20027;&#27969;&#26631;&#20934;&#65292;&#20294;&#20854;&#25104;&#21151;&#22240;&#32032;&#65292;&#22914;&#19981;&#21516;&#20219;&#21153;&#21644;&#35821;&#35328;&#30340;&#26368;&#20339;&#20998;&#21106;&#31890;&#24230;&#12289;&#25968;&#25454;&#28304;&#23545;&#26631;&#35760;&#24037;&#20855;&#30340;&#24433;&#21709;&#20197;&#21450;&#24418;&#24577;&#20449;&#24687;&#22312;&#21360;&#27431;&#35821;&#35328;&#20013;&#30340;&#20316;&#29992;&#65292;&#20173;&#19981;&#26126;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#23376;&#35789;&#30340;&#26631;&#35760;&#21270;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#20013;&#30340;&#20027;&#27969;&#26631;&#20934;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#23548;&#33268;&#20854;&#25104;&#21151;&#30340;&#30830;&#20999;&#22240;&#32032;&#65292;&#22914;&#19981;&#21516;&#20219;&#21153;&#21644;&#35821;&#35328;&#30340;&#26368;&#20339;&#20998;&#21106;&#31890;&#24230;&#65292;&#25968;&#25454;&#28304;&#23545;&#26631;&#35760;&#24037;&#20855;&#30340;&#24433;&#21709;&#20197;&#21450;&#24418;&#24577;&#20449;&#24687;&#22312;&#21360;&#27431;&#35821;&#35328;&#20013;&#30340;&#20316;&#29992;&#65292;&#20173;&#28982;&#19981;&#22815;&#28165;&#26970;&#12290;&#36825;&#22312;&#29983;&#29289;&#21307;&#23398;&#26415;&#35821;&#26041;&#38754;&#23588;&#20026;&#37325;&#35201;&#65292;&#20854;&#29305;&#28857;&#26159;&#20855;&#26377;&#31649;&#29702;&#24418;&#24577;&#32032;&#32452;&#21512;&#30340;&#29305;&#23450;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15010v1 Announce Type: cross  Abstract: Subword tokenization has become the prevailing standard in the field of natural language processing (NLP) over recent years, primarily due to the widespread utilization of pre-trained language models. This shift began with Byte-Pair Encoding (BPE) and was later followed by the adoption of SentencePiece and WordPiece. While subword tokenization consistently outperforms character and word-level tokenization, the precise factors contributing to its success remain unclear. Key aspects such as the optimal segmentation granularity for diverse tasks and languages, the influence of data sources on tokenizers, and the role of morphological information in Indo-European languages remain insufficiently explored. This is particularly pertinent for biomedical terminology, characterized by specific rules governing morpheme combinations. Despite the agglutinative nature of biomedical terminology, existing language models do not explicitly incorporate 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GenCeption&#30340;&#26032;&#22411;MLLM&#35780;&#20272;&#26694;&#26550;&#65292;&#21487;&#20197;&#20165;&#21033;&#29992;&#21333;&#27169;&#24577;&#25968;&#25454;&#35780;&#20272;&#36328;&#27169;&#24577;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#24182;&#26377;&#25928;&#21453;&#26144;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#30340;&#20542;&#21521;&#65292;&#20855;&#26377;&#36739;&#24378;&#30340;&#30456;&#20851;&#24615;&#21644;&#28508;&#21147;&#20110;&#27969;&#34892;&#30340;MLLM&#22522;&#20934;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.14973</link><description>&lt;p&gt;
GenCeption&#65306;&#20351;&#29992;&#26410;&#26631;&#35760;&#30340;&#21333;&#27169;&#24577;&#25968;&#25454;&#35780;&#20272;&#22810;&#27169;&#24577;LLM
&lt;/p&gt;
&lt;p&gt;
GenCeption: Evaluate Multimodal LLMs with Unlabeled Unimodal Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14973
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GenCeption&#30340;&#26032;&#22411;MLLM&#35780;&#20272;&#26694;&#26550;&#65292;&#21487;&#20197;&#20165;&#21033;&#29992;&#21333;&#27169;&#24577;&#25968;&#25454;&#35780;&#20272;&#36328;&#27169;&#24577;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#24182;&#26377;&#25928;&#21453;&#26144;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#30340;&#20542;&#21521;&#65292;&#20855;&#26377;&#36739;&#24378;&#30340;&#30456;&#20851;&#24615;&#21644;&#28508;&#21147;&#20110;&#27969;&#34892;&#30340;MLLM&#22522;&#20934;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#36890;&#24120;&#20351;&#29992;&#26114;&#36149;&#30340;&#24102;&#26631;&#27880;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20934;&#36890;&#24120;&#38590;&#20197;&#36319;&#19978;MLLM&#35780;&#20272;&#30340;&#24555;&#36895;&#21457;&#23637;&#35201;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GenCeption&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26080;&#38656;&#27880;&#37322;&#30340;MLLM&#35780;&#20272;&#26694;&#26550;&#65292;&#20165;&#38656;&#35201;&#21333;&#27169;&#24577;&#25968;&#25454;&#26469;&#35780;&#20272;&#36328;&#27169;&#24577;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#24182;&#21453;&#26144;&#20986;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#30340;&#20542;&#21521;&#12290;&#31867;&#20284;&#20110;&#27969;&#34892;&#30340;DrawCeption&#28216;&#25103;&#65292;GenCeption&#20174;&#19968;&#20010;&#38750;&#25991;&#26412;&#26679;&#26412;&#24320;&#22987;&#65292;&#24182;&#32463;&#21382;&#19968;&#31995;&#21015;&#36845;&#20195;&#30340;&#25551;&#36848;&#21644;&#29983;&#25104;&#27493;&#39588;&#12290;&#36845;&#20195;&#20043;&#38388;&#30340;&#35821;&#20041;&#28418;&#31227;&#20351;&#29992;GC@T&#25351;&#26631;&#36827;&#34892;&#37327;&#21270;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#21457;&#29616;&#39564;&#35777;&#20102;GenCeption&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#26174;&#31034;&#20986;&#19982;&#27969;&#34892;&#30340;MLLM&#22522;&#20934;&#32467;&#26524;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;GenCeption&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#26222;&#36941;&#23384;&#22312;&#19988;&#20197;&#21069;&#26410;&#35265;&#30340;&#21333;&#27169;&#24577;&#25968;&#25454;&#26469;&#25193;&#23637;&#65292;&#20197;&#20943;&#36731;&#35757;&#32451;&#25968;&#25454;&#30340;&#27745;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14973v1 Announce Type: cross  Abstract: Multimodal Large Language Models (MLLMs) are commonly evaluated using costly annotated multimodal benchmarks. However, these benchmarks often struggle to keep pace with the rapidly advancing requirements of MLLM evaluation. We propose GenCeption, a novel and annotation-free MLLM evaluation framework that merely requires unimodal data to assess inter-modality semantic coherence and inversely reflects the models' inclination to hallucinate. Analogous to the popular DrawCeption game, GenCeption initiates with a non-textual sample and undergoes a series of iterative description and generation steps. Semantic drift across iterations is quantified using the GC@T metric. Our empirical findings validate GenCeption's efficacy, showing strong correlations with popular MLLM benchmarking results. GenCeption may be extended to mitigate training data contamination by utilizing ubiquitous, previously unseen unimodal data.
&lt;/p&gt;</description></item><item><title>&#19981;&#38656;&#35201;&#22522;&#20934;&#23454;&#20917;&#25110;&#21442;&#32771;&#21709;&#24212;&#30340;&#26465;&#20214;&#19979;&#65292;&#36890;&#36807;&#32771;&#34385;&#27169;&#22411;&#30340;&#19977;&#20803;&#32452;&#26469;&#25490;&#21517;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#25490;&#21517;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.14860</link><description>&lt;p&gt;
&#22312;&#27809;&#26377;&#22522;&#20934;&#23454;&#20917;&#30340;&#24773;&#20917;&#19979;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
Ranking Large Language Models without Ground Truth
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14860
&lt;/p&gt;
&lt;p&gt;
&#19981;&#38656;&#35201;&#22522;&#20934;&#23454;&#20917;&#25110;&#21442;&#32771;&#21709;&#24212;&#30340;&#26465;&#20214;&#19979;&#65292;&#36890;&#36807;&#32771;&#34385;&#27169;&#22411;&#30340;&#19977;&#20803;&#32452;&#26469;&#25490;&#21517;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#25490;&#21517;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#21644;&#24433;&#21709;&#21147;&#30340;&#22686;&#24378;&#65292;&#35780;&#20272;&#21644;&#25490;&#21517;LLMs&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#33719;&#21462;&#26114;&#36149;&#30340;&#20154;&#31867;&#21709;&#24212;&#65292;&#35201;&#20040;&#20351;&#29992;LLMs&#25104;&#23545;&#22320;&#20114;&#30456;&#35780;&#20272;&#65292;&#36825;&#21487;&#33021;&#19981;&#22815;&#21487;&#38752;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#22312;&#32473;&#23450;&#19968;&#32452;&#25552;&#31034;&#25968;&#25454;&#38598;&#65288;&#27604;&#22914;&#38382;&#39064;&#12289;&#35828;&#26126;&#31561;&#65289;&#21644;&#19968;&#32452;LLMs&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#22312;&#27809;&#26377;&#20219;&#20309;&#22522;&#20934;&#23454;&#20917;&#25110;&#21442;&#32771;&#21709;&#24212;&#30340;&#24773;&#20917;&#19979;&#23545;&#23427;&#20204;&#36827;&#34892;&#25490;&#21517;&#12290;&#21463;&#21040;&#29616;&#23454;&#29983;&#27963;&#30340;&#21551;&#21457;&#65292;&#20854;&#20013;&#19987;&#23478;&#21644;&#26377;&#30693;&#35782;&#30340;&#20154;&#37117;&#33021;&#35782;&#21035;&#19968;&#20010;&#26032;&#25163;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#24605;&#36335;&#26159;&#32771;&#34385;&#27169;&#22411;&#30340;&#19977;&#20803;&#32452;&#65292;&#20854;&#20013;&#27599;&#20010;&#27169;&#22411;&#35780;&#20272;&#20854;&#20182;&#20004;&#20010;&#27169;&#22411;&#65292;&#33021;&#22815;&#20197;&#24456;&#39640;&#30340;&#27010;&#29575;&#27491;&#30830;&#35782;&#21035;&#26368;&#24046;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#25105;&#20204;&#30340;&#24819;&#27861;&#24182;&#25552;&#20379;&#20102;&#25104;&#21151;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;&#36890;&#36807;&#21453;&#22797;&#24212;&#29992;&#36825;&#19968;&#24819;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#23545;LLMs&#36827;&#34892;&#25490;&#21517;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14860v1 Announce Type: cross  Abstract: Evaluation and ranking of large language models (LLMs) has become an important problem with the proliferation of these models and their impact. Evaluation methods either require human responses which are expensive to acquire or use pairs of LLMs to evaluate each other which can be unreliable. In this paper, we provide a novel perspective where, given a dataset of prompts (viz. questions, instructions, etc.) and a set of LLMs, we rank them without access to any ground truth or reference responses. Inspired by real life where both an expert and a knowledgeable person can identify a novice our main idea is to consider triplets of models, where each one of them evaluates the other two, correctly identifying the worst model in the triplet with high probability. We also analyze our idea and provide sufficient conditions for it to succeed. Applying this idea repeatedly, we propose two methods to rank LLMs. In experiments on different generati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;CounterCurate&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#20363;&#23376;&#21644;&#29983;&#25104;&#24335;&#24494;&#35843;&#65292;&#20840;&#38754;&#25552;&#21319;&#35270;&#35273;-&#35821;&#35328;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#29289;&#29702;&#25512;&#29702;&#21644;&#35821;&#20041;&#23545;&#29031;&#24494;&#35843;&#26041;&#38754;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.13254</link><description>&lt;p&gt;
CounterCurate: &#36890;&#36807;&#23545;&#29031;&#20363;&#23376;&#22686;&#24378;&#29289;&#29702;&#21644;&#35821;&#20041;&#35270;&#35273;-&#35821;&#35328;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;CounterCurate&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#20363;&#23376;&#21644;&#29983;&#25104;&#24335;&#24494;&#35843;&#65292;&#20840;&#38754;&#25552;&#21319;&#35270;&#35273;-&#35821;&#35328;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#29289;&#29702;&#25512;&#29702;&#21644;&#35821;&#20041;&#23545;&#29031;&#24494;&#35843;&#26041;&#38754;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;CounterCurate&#65292;&#19968;&#20010;&#26694;&#26550;&#65292;&#20840;&#38754;&#25552;&#21319;&#23545;&#27604;&#21644;&#29983;&#25104;&#24335;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#35270;&#35273;-&#35821;&#35328;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20004;&#20010;&#23578;&#26410;&#20805;&#20998;&#25506;&#35752;&#30340;&#20851;&#38190;&#38382;&#39064;&#65306;&#24573;&#35270;&#20102;&#22522;&#20110;&#29289;&#29702;&#30340;&#25512;&#29702;&#65288;&#35745;&#25968;&#21644;&#20301;&#32622;&#29702;&#35299;&#65289;&#65292;&#20197;&#21450;&#21033;&#29992;&#39640;&#24615;&#33021;&#25991;&#26412;&#21644;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#21453;&#20107;&#23454;&#24494;&#35843;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24320;&#21019;&#20102;&#19968;&#20010;&#35299;&#20915;&#36825;&#20123;&#31354;&#30333;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#31361;&#20986;&#20102;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;&#22914;CLIP&#21644;LLaVA&#65289;&#22312;&#22522;&#20110;&#29289;&#29702;&#30340;&#32452;&#21512;&#25512;&#29702;&#20013;&#20960;&#20046;&#26080;&#27861;&#32988;&#20219;&#30340;&#34920;&#29616;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#31616;&#21333;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#20351;&#29992;&#22522;&#20110;&#22270;&#20687;&#30340;&#29983;&#25104;&#27169;&#22411;GLIGEN&#29983;&#25104;&#24494;&#35843;&#25968;&#25454;&#65292;&#20351;&#24471;&#24615;&#33021;&#26174;&#33879;&#25552;&#39640;&#65306;&#22312;&#25105;&#20204;&#26032;&#30340;&#31574;&#21010;&#30340;Flickr30k-Positions&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;CLIP&#21644;LLaVA&#30340;&#24615;&#33021;&#20998;&#21035;&#25552;&#39640;&#20102;+33%&#21644;+37%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#39640;&#24615;&#33021;&#25991;&#26412;&#21644;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13254v1 Announce Type: cross  Abstract: We propose CounterCurate, a framework to comprehensively improve the visio-linguistic compositional reasoning capability for both contrastive and generative multimodal models. In particular, we identify two under-explored critical problems: the neglect of the physically grounded reasoning (counting and position understanding) and the potential of using highly capable text and image generation models for semantic counterfactual fine-tuning. Our work pioneers an approach that addresses these gaps. We first spotlight the near-chance performance of multimodal models like CLIP and LLaVA in physically grounded compositional reasoning. We then apply simple data augmentation using a grounded image generation model, GLIGEN, to generate finetuning data, resulting in significant performance improvements: +33% and +37% for CLIP and LLaVA, respectively, on our newly curated Flickr30k-Positions benchmark. Moreover, we exploit the capabilities of hig
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#21338;&#24328;&#35770;&#20219;&#21153;&#35780;&#20272;&#20102;LLMs&#22312;&#31454;&#20105;&#29615;&#22659;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#35266;&#23519;&#21040;LLMs&#22312;&#19981;&#21516;&#28216;&#25103;&#22330;&#26223;&#19979;&#34920;&#29616;&#20986;&#19981;&#21516;&#34892;&#20026;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#25112;&#30053;&#25512;&#29702;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12348</link><description>&lt;p&gt;
&#36890;&#36807;&#21338;&#24328;&#35770;&#35780;&#20272;&#25581;&#31034;LLM&#30340;&#25112;&#30053;&#25512;&#29702;&#23616;&#38480;&#24615;&#30340;GTBench
&lt;/p&gt;
&lt;p&gt;
GTBench: Uncovering the Strategic Reasoning Limitations of LLMs via Game-Theoretic Evaluations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12348
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#21338;&#24328;&#35770;&#20219;&#21153;&#35780;&#20272;&#20102;LLMs&#22312;&#31454;&#20105;&#29615;&#22659;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#35266;&#23519;&#21040;LLMs&#22312;&#19981;&#21516;&#28216;&#25103;&#22330;&#26223;&#19979;&#34920;&#29616;&#20986;&#19981;&#21516;&#34892;&#20026;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#25112;&#30053;&#25512;&#29702;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#25972;&#21512;&#21040;&#20851;&#38190;&#30340;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#65292;&#23427;&#20204;&#30340;&#25112;&#30053;&#21644;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#21464;&#24471;&#36234;&#26469;&#36234;&#20851;&#38190;&#12290;&#26412;&#25991;&#36890;&#36807;&#21338;&#24328;&#35770;&#20219;&#21153;&#35780;&#20272;LLMs&#22312;&#31454;&#20105;&#29615;&#22659;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20363;&#22914;&#65292;&#38656;&#35201;&#32431;&#36923;&#36753;&#21644;&#25112;&#30053;&#25512;&#29702;&#26469;&#19982;&#23545;&#25163;&#31454;&#20105;&#30340;&#26827;&#30424;&#28216;&#25103;&#21644;&#32440;&#29260;&#28216;&#25103;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;GTBench&#65292;&#36825;&#26159;&#19968;&#20010;&#20197;&#35821;&#35328;&#39537;&#21160;&#30340;&#29615;&#22659;&#65292;&#21253;&#25324;10&#20010;&#24191;&#27867;&#35748;&#21487;&#30340;&#20219;&#21153;&#65292;&#28085;&#30422;&#20102;&#20840;&#38754;&#30340;&#28216;&#25103;&#20998;&#31867;&#27861;&#65306;&#23436;&#25972;&#20449;&#24687;&#19982;&#19981;&#23436;&#25972;&#20449;&#24687;&#65292;&#21160;&#24577;&#19982;&#38745;&#24577;&#65292;&#20197;&#21450;&#27010;&#29575;&#19982;&#30830;&#23450;&#24615;&#22330;&#26223;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#65288;1&#65289;&#34920;&#24449;LLMs&#30340;&#21338;&#24328;&#35770;&#25512;&#29702;&#65307;&#65288;2&#65289;LLM&#23545;&#25239;LLM&#30340;&#27604;&#36187;&#20316;&#20026;&#25512;&#29702;&#35780;&#20272;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65288;1&#65289;LLMs&#22312;&#21508;&#31181;&#28216;&#25103;&#22330;&#26223;&#19979;&#26377;&#19981;&#21516;&#30340;&#34892;&#20026;&#65307;&#20363;&#22914;&#65292;LLMs&#22312;&#23436;&#25972;&#21644;&#30830;&#23450;&#24615;&#28216;&#25103;&#20013;&#22833;&#36133;&#65292;&#20294;&#23427;&#20204;&#22312;&#27010;&#29575;&#28216;&#25103;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12348v1 Announce Type: cross  Abstract: As Large Language Models (LLMs) are integrated into critical real-world applications, their strategic and logical reasoning abilities are increasingly crucial. This paper evaluates LLMs' reasoning abilities in competitive environments through game-theoretic tasks, e.g., board and card games that require pure logic and strategic reasoning to compete with opponents. We first propose GTBench, a language-driven environment composing 10 widely-recognized tasks, across a comprehensive game taxonomy: complete versus incomplete information, dynamic versus static, and probabilistic versus deterministic scenarios. Then, we investigate two key problems: (1) Characterizing game-theoretic reasoning of LLMs; (2) LLM-vs-LLM competitions as reasoning evaluation. We observe that (1) LLMs have distinct behaviors regarding various gaming scenarios; for example, LLMs fail in complete and deterministic games yet they are competitive in probabilistic gaming
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#32423;&#21035;&#23436;&#32654;&#30340;MMR&#65288;BLP&#65289;&#65292;&#23427;&#26159;&#26497;&#23567;&#21270;&#36951;&#25022;&#30446;&#26631;&#30340;&#31934;&#30830;&#21270;&#65292;&#33021;&#22815;&#20811;&#26381;&#26497;&#23567;&#21270;&#36951;&#25022;&#31574;&#30053;&#22312;&#36951;&#25022;&#19978;&#30028;&#26102;&#23398;&#20064;&#20572;&#28382;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.12284</link><description>&lt;p&gt;
&#20026;&#26080;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#20248;&#21270;&#26497;&#23567;&#21270;&#36951;&#25022;
&lt;/p&gt;
&lt;p&gt;
Refining Minimax Regret for Unsupervised Environment Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12284
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#32423;&#21035;&#23436;&#32654;&#30340;MMR&#65288;BLP&#65289;&#65292;&#23427;&#26159;&#26497;&#23567;&#21270;&#36951;&#25022;&#30446;&#26631;&#30340;&#31934;&#30830;&#21270;&#65292;&#33021;&#22815;&#20811;&#26381;&#26497;&#23567;&#21270;&#36951;&#25022;&#31574;&#30053;&#22312;&#36951;&#25022;&#19978;&#30028;&#26102;&#23398;&#20064;&#20572;&#28382;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#36890;&#36807;&#23545;&#23545;&#25163;&#26368;&#22823;&#21270;&#26576;&#20010;&#30446;&#26631;&#29983;&#25104;&#30340;&#29615;&#22659;&#37197;&#32622;&#65288;&#20851;&#21345;&#65289;&#36827;&#34892;&#35757;&#32451;&#12290;&#36951;&#25022;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#30446;&#26631;&#65292;&#29702;&#35770;&#19978;&#23548;&#33268;&#20855;&#26377;&#33391;&#22909;&#40065;&#26834;&#24615;&#20445;&#35777;&#30340;&#26497;&#23567;&#21270;&#36951;&#25022;&#65288;MMR&#65289;&#31574;&#30053;&#65307;&#29305;&#21035;&#26159;&#65292;&#20195;&#29702;&#30340;&#26368;&#22823;&#36951;&#25022;&#26159;&#26377;&#30028;&#30340;&#12290;&#28982;&#32780;&#65292;&#19968;&#26086;&#20195;&#29702;&#22312;&#25152;&#26377;&#20851;&#21345;&#19978;&#36798;&#21040;&#20102;&#36825;&#20010;&#36951;&#25022;&#19978;&#30028;&#65292;&#23545;&#25163;&#23558;&#21482;&#20250;&#23545;&#26080;&#27861;&#36827;&#19968;&#27493;&#20943;&#23569;&#36951;&#25022;&#30340;&#20851;&#21345;&#36827;&#34892;&#37319;&#26679;&#12290;&#23613;&#31649;&#22312;&#36825;&#20123;&#26368;&#22823;&#21270;&#36951;&#25022;&#30340;&#20851;&#21345;&#20043;&#22806;&#21487;&#33021;&#23384;&#22312;&#24615;&#33021;&#25913;&#36827;&#31354;&#38388;&#65292;&#20294;&#23398;&#20064;&#20572;&#28382;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#32423;&#21035;&#23436;&#32654;&#30340;MMR&#65288;BLP&#65289;&#65292;&#23427;&#26159;&#26497;&#23567;&#21270;&#36951;&#25022;&#30446;&#26631;&#30340;&#31934;&#30830;&#21270;&#12290;&#25105;&#20204;&#27491;&#24335;&#35777;&#26126;&#65292;&#35299;&#20915;&#36825;&#20010;&#30446;&#26631;&#23558;&#23548;&#33268;MMR&#31574;&#30053;&#30340;&#23376;&#38598;&#65292;&#24182;&#19988;BLP&#31574;&#30053;&#22312;&#25152;&#26377;&#20851;&#21345;&#19978;&#37117;&#19982;&#23436;&#32654;&#36125;&#21494;&#26031;&#31574;&#30053;&#19968;&#33268;&#34892;&#20107;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12284v1 Announce Type: cross  Abstract: In unsupervised environment design, reinforcement learning agents are trained on environment configurations (levels) generated by an adversary that maximises some objective. Regret is a commonly used objective that theoretically results in a minimax regret (MMR) policy with desirable robustness guarantees; in particular, the agent's maximum regret is bounded. However, once the agent reaches this regret bound on all levels, the adversary will only sample levels where regret cannot be further reduced. Although there are possible performance improvements to be made outside of these regret-maximising levels, learning stagnates. In this work, we introduce Bayesian level-perfect MMR (BLP), a refinement of the minimax regret objective that overcomes this limitation. We formally show that solving for this objective results in a subset of MMR policies, and that BLP policies act consistently with a Perfect Bayesian policy over all levels. We fur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#22270;&#24418;&#32467;&#26500;&#25552;&#31034;&#65292;&#20197;&#22686;&#24378;&#39044;&#35757;&#32451;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#35770;&#35265;&#35299;&#65292;&#23454;&#29616;&#20102;&#22312;&#20219;&#24847;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#20302;&#36164;&#28304;&#24402;&#32435;&#25512;&#29702;&#30340;&#39640;&#36890;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11804</link><description>&lt;p&gt;
LLM&#20316;&#20026;&#25552;&#31034;&#22120;&#65306;&#22312;&#20219;&#24847;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#20302;&#36164;&#28304;&#24402;&#32435;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
LLM as Prompter: Low-resource Inductive Reasoning on Arbitrary Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11804
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#22270;&#24418;&#32467;&#26500;&#25552;&#31034;&#65292;&#20197;&#22686;&#24378;&#39044;&#35757;&#32451;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#35770;&#35265;&#35299;&#65292;&#23454;&#29616;&#20102;&#22312;&#20219;&#24847;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#20302;&#36164;&#28304;&#24402;&#32435;&#25512;&#29702;&#30340;&#39640;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#65288;KG&#65289;&#24402;&#32435;&#25512;&#29702;&#26088;&#22312;&#25512;&#26029;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#26032;KG&#20013;&#32570;&#22833;&#30340;&#20107;&#23454;&#65292;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34987;&#24191;&#27867;&#37319;&#29992;&#12290;KG&#24402;&#32435;&#25512;&#29702;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#22788;&#29702;&#22312;&#25991;&#26412;&#21644;&#32467;&#26500;&#26041;&#38754;&#37117;&#31232;&#32570;&#30340;&#20302;&#36164;&#28304;&#22330;&#26223;&#12290;&#26412;&#25991;&#23581;&#35797;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;LLMs&#29983;&#25104;&#22270;&#24418;&#32467;&#26500;&#25552;&#31034;&#65292;&#20197;&#22686;&#24378;&#39044;&#35757;&#32451;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#20174;&#32780;&#20026;KG&#24402;&#32435;&#25512;&#29702;&#26041;&#27861;&#24102;&#26469;&#26032;&#30340;&#26041;&#27861;&#35770;&#35265;&#35299;&#65292;&#20197;&#21450;&#22312;&#23454;&#36341;&#20013;&#20855;&#26377;&#24456;&#39640;&#30340;&#26222;&#36866;&#24615;&#12290;&#22312;&#26041;&#27861;&#35770;&#26041;&#38754;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#26694;&#26550;ProLINK&#65292;&#26088;&#22312;&#22312;&#20219;&#24847;KG&#19978;&#36827;&#34892;&#20302;&#36164;&#28304;&#24402;&#32435;&#25512;&#29702;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#12290;&#22312;&#23454;&#36341;&#26041;&#38754;&#65292;&#25105;&#20204;&#22312;36&#20010;&#20302;&#36164;&#28304;&#25968;&#25454;&#38598;&#19978;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11804v1 Announce Type: new  Abstract: Knowledge Graph (KG) inductive reasoning, which aims to infer missing facts from new KGs that are not seen during training, has been widely adopted in various applications. One critical challenge of KG inductive reasoning is handling low-resource scenarios with scarcity in both textual and structural aspects. In this paper, we attempt to address this challenge with Large Language Models (LLMs). Particularly, we utilize the state-of-the-art LLMs to generate a graph-structural prompt to enhance the pre-trained Graph Neural Networks (GNNs), which brings us new methodological insights into the KG inductive reasoning methods, as well as high generalizability in practice. On the methodological side, we introduce a novel pretraining and prompting framework ProLINK, designed for low-resource inductive reasoning across arbitrary KGs without requiring additional training. On the practical side, we experimentally evaluate our approach on 36 low-res
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#19981;&#20462;&#25913;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#65292;&#36890;&#36807;&#36827;&#21270;&#20195;&#29702;&#30340;&#21151;&#33021;&#26469;&#35299;&#20915;&#19979;&#28216;&#20219;&#21153;</title><link>https://arxiv.org/abs/2402.11359</link><description>&lt;p&gt;
&#22312;&#19981;&#20462;&#25913;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Training Language Model Agents without Modifying Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11359
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#19981;&#20462;&#25913;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#65292;&#36890;&#36807;&#36827;&#21270;&#20195;&#29702;&#30340;&#21151;&#33021;&#26469;&#35299;&#20915;&#19979;&#28216;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#26368;&#36817;&#24050;&#32463;&#23558;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#37325;&#26032;&#23450;&#20041;&#20026;&#20195;&#29702;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#36890;&#36807;&#20351;&#29992;&#19987;&#38376;&#30340;&#21151;&#33021;&#33258;&#21160;&#21270;&#22320;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#12290;&#20026;&#20102;&#20419;&#36827;LLM&#20195;&#29702;&#30340;&#21457;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#20462;&#25913;LLM&#26435;&#37325;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;LLM&#20195;&#29702;&#30340;&#26032;&#33539;&#24335;&#65292;&#24403;LLM&#38590;&#20197;&#25110;&#26080;&#27861;&#36827;&#34892;&#20462;&#25913;&#26102;&#23588;&#20854;&#26377;&#29992;&#12290;&#21463;&#21040;&#20154;&#31867;&#19981;&#26029;&#38203;&#36896;&#24037;&#20855;&#20197;&#36866;&#24212;&#29616;&#23454;&#20219;&#21153;&#30340;&#21551;&#21457;&#65292;&#32780;&#19981;&#26159;&#25913;&#21464;&#25105;&#20204;&#30340;&#29983;&#29289;&#32467;&#26500;&#20197;&#36866;&#24212;&#19968;&#32452;&#38745;&#24577;&#24037;&#20855;&#65292;&#25105;&#20204;&#25552;&#20986;&#36880;&#27493;&#38203;&#36896;&#20195;&#29702;&#30340;&#21151;&#33021;&#65292;&#20197;&#26356;&#22909;&#22320;&#35299;&#20915;&#19979;&#28216;&#20219;&#21153;&#65292;&#32780;&#19981;&#26159;&#20462;&#25913;LLM&#26435;&#37325;&#12290;&#36890;&#36807;&#23558;&#36825;&#20123;&#21151;&#33021;&#35270;&#20026;&#21487;&#23398;&#20064;&#30340;&#8220;&#20195;&#29702;&#21442;&#25968;&#8221;&#24182;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#35757;&#32451;&#30340;&#22522;&#26412;&#24605;&#24819;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;AgentOptimizer&#65292;&#21033;&#29992;LLM&#26356;&#26032;&#20195;&#29702;&#30340;&#21151;&#33021;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#20195;&#29702;&#35757;&#32451;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11359v1 Announce Type: new  Abstract: Researchers and practitioners have recently reframed powerful Large Language Models (LLMs) as agents, enabling them to automate complex tasks largely via the use of specialized functions. To facilitate the development of LLM agents, we present a novel paradigm of training LLM agents without modifying the LLM weights, which is particularly useful when the LLMs are difficult or inaccessible for modifications. Inspired by how humans continuously forge tools to adapt to real-world tasks, rather than change our biological structure to fit a static set of tools, we propose to progressively forge agent's functions to better solve the downstream tasks instead of modifying the LLM weights. By treating the functions as learnable `agent parameters' and leveraging the fundamental idea of model training in artificial intelligence, we develop AgentOptimizer that employs the LLM to update agents' functions and devise an agent training algorithm with tw
&lt;/p&gt;</description></item><item><title>&#36890;&#36807; hierarchical spatiotemporal downsampling &#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#38382;&#39064;&#65292;&#32467;&#21512;&#21487;&#35299;&#37322;&#30340;&#27880;&#24847;&#26426;&#21046;&#65292;&#23454;&#29616;&#23545;&#26102;&#31354;&#39044;&#27979;&#30340;&#26377;&#25928;&#24314;&#27169;</title><link>https://arxiv.org/abs/2402.10634</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#26102;&#31354;&#38477;&#37319;&#26679;&#32570;&#22833;&#25968;&#25454;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Graph-based Forecasting with Missing Data through Spatiotemporal Downsampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10634
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807; hierarchical spatiotemporal downsampling &#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#38382;&#39064;&#65292;&#32467;&#21512;&#21487;&#35299;&#37322;&#30340;&#27880;&#24847;&#26426;&#21046;&#65292;&#23454;&#29616;&#23545;&#26102;&#31354;&#39044;&#27979;&#30340;&#26377;&#25928;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#19968;&#32452;&#19982;&#31354;&#38388;&#20013;&#20256;&#24863;&#22120;&#28857;&#30456;&#20851;&#32852;&#12289;&#20855;&#26377;&#30456;&#20114;&#20851;&#31995;&#30340;&#21516;&#27493;&#26102;&#38388;&#24207;&#21015;&#65292;&#26102;&#31354;&#39044;&#27979;&#38382;&#39064;&#21253;&#25324;&#20026;&#27599;&#20010;&#28857;&#39044;&#27979;&#26410;&#26469;&#35266;&#27979;&#20540;&#12290;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#23558;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#20026;&#22270;&#26469;&#23454;&#29616;&#24341;&#20154;&#27880;&#30446;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#20010;&#24120;&#24120;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#65292;&#21363;&#36755;&#20837;&#22987;&#32456;&#21487;&#29992;&#65292;&#24182;&#19988;&#22312;&#25968;&#25454;&#37096;&#20998;&#32570;&#22833;&#26102;&#26080;&#27861;&#25429;&#25417;&#38544;&#34255;&#30340;&#26102;&#31354;&#21160;&#24577;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#23618;&#26102;&#31354;&#38477;&#37319;&#26679;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36755;&#20837;&#26102;&#38388;&#24207;&#21015;&#38543;&#30528;&#26102;&#38388;&#21644;&#31354;&#38388;&#30340;&#25512;&#31227;&#36880;&#28176;&#31895;&#21270;&#65292;&#33719;&#24471;&#19968;&#32452;&#25429;&#25417;&#24322;&#36136;&#26102;&#38388;&#21644;&#31354;&#38388;&#21160;&#24577;&#30340;&#34920;&#31034;&#12290;&#22312;&#35266;&#27979;&#20540;&#21644;&#32570;&#22833;&#25968;&#25454;&#27169;&#24335;&#30340;&#26465;&#20214;&#19979;&#65292;&#36890;&#36807;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#27880;&#24847;&#26426;&#21046;&#26469;&#32452;&#21512;&#36825;&#20123;&#34920;&#31034;&#20197;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10634v1 Announce Type: cross  Abstract: Given a set of synchronous time series, each associated with a sensor-point in space and characterized by inter-series relationships, the problem of spatiotemporal forecasting consists of predicting future observations for each point. Spatiotemporal graph neural networks achieve striking results by representing the relationships across time series as a graph. Nonetheless, most existing methods rely on the often unrealistic assumption that inputs are always available and fail to capture hidden spatiotemporal dynamics when part of the data is missing. In this work, we tackle this problem through hierarchical spatiotemporal downsampling. The input time series are progressively coarsened over time and space, obtaining a pool of representations that capture heterogeneous temporal and spatial dynamics. Conditioned on observations and missing data patterns, such representations are combined by an interpretable attention mechanism to generate 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#36777;&#35770;&#35843;&#33410;LLMs&#65292;&#20351;&#20854;&#29983;&#25104;&#21487;&#25511;&#30340;&#25903;&#25345;&#29992;&#25143;&#23450;&#20041;&#35770;&#28857;&#30340;&#22768;&#26126;&#65292;&#25913;&#36827;&#20102;LLMs&#30340;&#21487;&#25511;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;DEBATunE&#27969;&#31243;&#12290;&#36890;&#36807;&#20004;&#20010;LLMs&#20043;&#38388;&#30340;&#22810;&#36718;&#36777;&#35770;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20197;&#25903;&#25345;&#29983;&#25104;&#26377;&#26356;&#39640;&#36136;&#37327;&#21644;&#26356;&#31361;&#20986;&#30340;&#22768;&#26126;&#12290;</title><link>https://arxiv.org/abs/2402.10614</link><description>&lt;p&gt;
&#36890;&#36807;&#36777;&#35770;&#35843;&#33410;LLMs&#20197;&#29983;&#25104;&#21487;&#25511;&#30340;&#20855;&#26377;&#20105;&#35758;&#24615;&#30340;&#22768;&#26126;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Speak For Diverse People? Tuning LLMs via Debate to Generate Controllable Controversial Statements
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#36777;&#35770;&#35843;&#33410;LLMs&#65292;&#20351;&#20854;&#29983;&#25104;&#21487;&#25511;&#30340;&#25903;&#25345;&#29992;&#25143;&#23450;&#20041;&#35770;&#28857;&#30340;&#22768;&#26126;&#65292;&#25913;&#36827;&#20102;LLMs&#30340;&#21487;&#25511;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;DEBATunE&#27969;&#31243;&#12290;&#36890;&#36807;&#20004;&#20010;LLMs&#20043;&#38388;&#30340;&#22810;&#36718;&#36777;&#35770;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20197;&#25903;&#25345;&#29983;&#25104;&#26377;&#26356;&#39640;&#36136;&#37327;&#21644;&#26356;&#31361;&#20986;&#30340;&#22768;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLMs&#20195;&#34920;&#19981;&#21516;&#30340;&#20154;&#32676;&#65292;&#23588;&#20854;&#26159;&#23569;&#25968;&#32676;&#20307;&#65292;&#24182;&#20135;&#29983;&#25903;&#25345;&#20854;&#22810;&#26679;&#21270;&#29978;&#33267;&#26377;&#20105;&#35758;&#35266;&#28857;&#30340;&#22768;&#26126;&#23545;&#20110;&#21019;&#36896;&#19968;&#20010;&#21253;&#23481;&#30340;&#29615;&#22659;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;LLMs&#32570;&#20047;&#36275;&#22815;&#30340;&#25511;&#21046;&#24615;&#26469;&#25903;&#25345;&#29983;&#25104;&#20869;&#23481;&#30340;&#31435;&#22330;&#65292;&#20854;&#20013;&#24448;&#24448;&#21253;&#21547;&#19981;&#19968;&#33268;&#12289;&#20013;&#31435;&#25110;&#26377;&#20559;&#35265;&#30340;&#22768;&#26126;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;LLMs&#22312;&#29983;&#25104;&#25903;&#25345;&#29992;&#25143;&#22312;&#25552;&#31034;&#20013;&#23450;&#20041;&#30340;&#35770;&#28857;&#30340;&#22768;&#26126;&#26102;&#30340;&#21487;&#25511;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#20004;&#20010;&#25345;&#26377;&#30456;&#21453;&#31435;&#22330;&#30340;LLMs&#20043;&#38388;&#30340;&#22810;&#36718;&#36777;&#35770;&#20135;&#29983;&#20102;&#26356;&#39640;&#36136;&#37327;&#21644;&#26356;&#31361;&#20986;&#30340;&#22768;&#26126;&#65292;&#36825;&#20123;&#22768;&#26126;&#23545;&#20110;&#25913;&#21892;LLMs&#30340;&#21487;&#25511;&#24615;&#26159;&#37325;&#35201;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Debate &amp; Tuning&#65288;&#8220;DEBATunE&#8221;&#65289;&#27969;&#31243;&#65292;&#36890;&#36807;&#24494;&#35843;LLMs&#29983;&#25104;&#36890;&#36807;&#36777;&#35770;&#33719;&#24471;&#30340;&#22768;&#26126;&#12290;&#20026;&#20102;&#26816;&#39564;DEBATunE&#65292;&#25105;&#20204;&#25972;&#29702;&#20102;&#36804;&#20170;&#20026;&#27490;&#28085;&#30422;710&#20010;&#20105;&#35758;&#24615;&#20027;&#39064;&#30340;&#26368;&#22823;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10614v1 Announce Type: cross  Abstract: Making LLMs speak for different, especially minority groups of people, and generate statements supporting their diverse or even controversial perspectives is critical to creating an inclusive environment. However, existing LLMs lack sufficient controllability to the stance of their generated content, which often contains inconsistent, neutral, or biased statements. In this paper, we improve the controllability of LLMs in generating statements supporting an argument the user defined in the prompt. We find that multi-round debates between two LLMs with opposite stances generate higher-quality and more salient statements for each, which are important training data to improve the controllability of LLMs. Motivated by this, we develop a novel debate &amp; tuning ("DEBATunE") pipeline finetuning LLMs to generate the statements obtained via debate. To examine DEBATunE, we curate the largest dataset of debate topics so far, which covers 710 contro
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;MLP&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26550;&#26500;RPMixer&#65292;&#36890;&#36807;&#23558;&#38543;&#26426;&#25237;&#24433;&#23618;&#38598;&#25104;&#21040;&#27169;&#22411;&#20013;&#65292;&#22686;&#21152;&#20102;&#22359;&#36755;&#20986;&#20043;&#38388;&#30340;&#22810;&#26679;&#24615;&#65292;&#25552;&#39640;&#20102;&#25972;&#20307;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.10487</link><description>&lt;p&gt;
&#38024;&#23545;&#22810;&#32500;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#38543;&#26426;&#25237;&#24433;&#23618;
&lt;/p&gt;
&lt;p&gt;
Random Projection Layers for Multidimensional Time Sires Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10487
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;MLP&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26550;&#26500;RPMixer&#65292;&#36890;&#36807;&#23558;&#38543;&#26426;&#25237;&#24433;&#23618;&#38598;&#25104;&#21040;&#27169;&#22411;&#20013;&#65292;&#22686;&#21152;&#20102;&#22359;&#36755;&#20986;&#20043;&#38388;&#30340;&#22810;&#26679;&#24615;&#65292;&#25552;&#39640;&#20102;&#25972;&#20307;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#28151;&#21512;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#23545;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#38382;&#39064;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#24403;&#23558;&#27492;&#31867;&#27169;&#22411;&#24212;&#29992;&#20110;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#65288;&#20363;&#22914;&#31354;&#38388;-&#26102;&#38388;&#25968;&#25454;&#38598;&#20013;&#30340;&#26102;&#38388;&#24207;&#21015;&#65289;&#26102;&#65292;&#30001;&#20110;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#20854;&#24615;&#33021;&#21487;&#33021;&#20250;&#19979;&#38477;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;MLP&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26550;&#26500;&#65292;&#31216;&#20026;RPMixer&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#38598;&#25104;&#24335;&#34892;&#20026;&#65292;&#20854;&#20013;&#32593;&#32476;&#20013;&#30340;&#27599;&#20010;&#21333;&#29420;&#22359;&#30340;&#20316;&#29992;&#31867;&#20284;&#20110;&#38598;&#25104;&#27169;&#22411;&#20013;&#30340;&#22522;&#26412;&#23398;&#20064;&#22120;&#65292;&#29305;&#21035;&#26159;&#22312;&#24341;&#20837;&#36523;&#20221;&#26144;&#23556;&#27531;&#24046;&#36830;&#25509;&#26102;&#12290;&#36890;&#36807;&#23558;&#38543;&#26426;&#25237;&#24433;&#23618;&#38598;&#25104;&#21040;&#25105;&#20204;&#30340;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#22686;&#21152;&#20102;&#22359;&#36755;&#20986;&#20043;&#38388;&#30340;&#22810;&#26679;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;RPMixer&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#23545;&#22823;&#35268;&#27169;&#31354;&#38388;-&#26102;&#38388;&#39044;&#27979;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#32988;&#36807;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10487v1 Announce Type: cross  Abstract: All-Multi-Layer Perceptron (all-MLP) mixer models have been shown to be effective for time series forecasting problems. However, when such a model is applied to high-dimensional time series (e.g., the time series in a spatial-temporal dataset), its performance is likely to degrade due to overfitting issues. In this paper, we propose an all-MLP time series forecasting architecture, referred to as RPMixer. Our method leverages the ensemble-like behavior of deep neural networks, where each individual block within the network acts like a base learner in an ensemble model, especially when identity mapping residual connections are incorporated. By integrating random projection layers into our model, we increase the diversity among the blocks' outputs, thereby enhancing the overall performance of RPMixer. Extensive experiments conducted on large-scale spatial-temporal forecasting benchmark datasets demonstrate that our proposed method outperf
&lt;/p&gt;</description></item><item><title>BioMistral&#26159;&#19968;&#31181;&#38754;&#21521;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#24320;&#28304;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#21512;&#65292;&#22312;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#24182;&#20855;&#26377;&#31454;&#20105;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.10373</link><description>&lt;p&gt;
BioMistral&#65306;&#38754;&#21521;&#21307;&#23398;&#39046;&#22495;&#30340;&#24320;&#28304;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10373
&lt;/p&gt;
&lt;p&gt;
BioMistral&#26159;&#19968;&#31181;&#38754;&#21521;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#24320;&#28304;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#21512;&#65292;&#22312;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#24182;&#20855;&#26377;&#31454;&#20105;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36817;&#24180;&#26469;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#22810;&#21151;&#33021;&#24615;&#65292;&#20026;&#21307;&#30103;&#20445;&#20581;&#21644;&#21307;&#23398;&#31561;&#19987;&#19994;&#39046;&#22495;&#25552;&#20379;&#28508;&#22312;&#24212;&#29992;&#12290;&#23613;&#31649;&#26377;&#21508;&#31181;&#38024;&#23545;&#20581;&#24247;&#39046;&#22495;&#23450;&#21046;&#30340;&#24320;&#28304;LLMs&#21487;&#29992;&#65292;&#20294;&#23558;&#36890;&#29992;LLMs&#35843;&#25972;&#21040;&#21307;&#23398;&#39046;&#22495;&#20173;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;BioMistral&#65292;&#19968;&#31181;&#19987;&#20026;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#37327;&#36523;&#23450;&#21046;&#30340;&#24320;&#28304;LLM&#65292;&#37319;&#29992;Mistral&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#22312;PubMed Central&#19978;&#36827;&#19968;&#27493;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#21253;&#21547;10&#20010;&#24050;&#24314;&#31435;&#30340;&#33521;&#25991;&#21307;&#23398;&#38382;&#31572;&#65288;QA&#65289;&#20219;&#21153;&#30340;&#22522;&#20934;&#19978;&#23545;BioMistral&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#36890;&#36807;&#37327;&#21270;&#21644;&#27169;&#22411;&#21512;&#24182;&#26041;&#27861;&#33719;&#24471;&#30340;&#36731;&#37327;&#32423;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;BioMistral&#30456;&#36739;&#20110;&#29616;&#26377;&#24320;&#28304;&#21307;&#23398;&#27169;&#22411;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#65292;&#24182;&#19982;&#19987;&#26377;&#23545;&#25163;&#20855;&#26377;&#31454;&#20105;&#20248;&#21183;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#35299;&#20915;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10373v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated remarkable versatility in recent years, offering potential applications across specialized domains such as healthcare and medicine. Despite the availability of various open-source LLMs tailored for health contexts, adapting general-purpose LLMs to the medical domain presents significant challenges. In this paper, we introduce BioMistral, an open-source LLM tailored for the biomedical domain, utilizing Mistral as its foundation model and further pre-trained on PubMed Central. We conduct a comprehensive evaluation of BioMistral on a benchmark comprising 10 established medical question-answering (QA) tasks in English. We also explore lightweight models obtained through quantization and model merging approaches. Our results demonstrate BioMistral's superior performance compared to existing open-source medical models and its competitive edge against proprietary counterparts. Finally, to address
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#36873;&#25321;&#24615;&#21453;&#23556;&#35843;&#33410;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25945;&#24072;LLM&#30340;&#21453;&#23556;&#21644;&#33258;&#30465;&#19982;&#23398;&#29983;LLM&#30340;&#25968;&#25454;&#36873;&#25321;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#33258;&#21160;&#20248;&#21270;&#29616;&#26377;&#30340;&#25351;&#20196;&#35843;&#33410;&#25968;&#25454;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25351;&#20196;&#35843;&#33410;&#21644;&#21331;&#36234;&#24615;&#33021;&#30340;LLM&#12290;</title><link>https://arxiv.org/abs/2402.10110</link><description>&lt;p&gt;
&#36873;&#25321;&#24615;&#21453;&#23556;&#35843;&#33410;&#65306;LLM&#25351;&#20196;&#35843;&#33410;&#30340;&#23398;&#29983;&#36873;&#25321;&#25968;&#25454;&#22238;&#25910;
&lt;/p&gt;
&lt;p&gt;
Selective Reflection-Tuning: Student-Selected Data Recycling for LLM Instruction-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#36873;&#25321;&#24615;&#21453;&#23556;&#35843;&#33410;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25945;&#24072;LLM&#30340;&#21453;&#23556;&#21644;&#33258;&#30465;&#19982;&#23398;&#29983;LLM&#30340;&#25968;&#25454;&#36873;&#25321;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#33258;&#21160;&#20248;&#21270;&#29616;&#26377;&#30340;&#25351;&#20196;&#35843;&#33410;&#25968;&#25454;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25351;&#20196;&#35843;&#33410;&#21644;&#21331;&#36234;&#24615;&#33021;&#30340;LLM&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#33410;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#35828;&#38750;&#24120;&#20851;&#38190;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#25351;&#20196;&#36319;&#36394;&#21644;&#20219;&#21153;&#36866;&#24212;&#33021;&#21147;&#65292;&#20294;&#20854;&#25104;&#21151;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#36136;&#37327;&#12290;&#35768;&#22810;&#26368;&#36817;&#30340;&#26041;&#27861;&#37117;&#33268;&#21147;&#20110;&#25913;&#36827;&#25968;&#25454;&#36136;&#37327;&#65292;&#20294;&#24448;&#24448;&#24573;&#35270;&#20102;&#25968;&#25454;&#19982;&#27491;&#22312;&#24494;&#35843;&#30340;&#23398;&#29983;&#27169;&#22411;&#30340;&#20860;&#23481;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#8212;&#8212;&#36873;&#25321;&#24615;&#21453;&#23556;&#35843;&#33410;&#65292;&#36890;&#36807;&#32467;&#21512;&#25945;&#24072;LLM&#30340;&#21453;&#23556;&#21644;&#33258;&#30465;&#65292;&#20197;&#33258;&#21160;&#20248;&#21270;&#29616;&#26377;&#30340;&#25351;&#20196;&#35843;&#33410;&#25968;&#25454;&#12290;&#36825;&#31181;&#24072;&#29983;&#21512;&#20316;&#20135;&#29983;&#20102;&#39640;&#36136;&#37327;&#19988;&#19982;&#23398;&#29983;LLM&#20860;&#23481;&#30340;&#25351;&#20196;&#21709;&#24212;&#23545;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25351;&#20196;&#35843;&#33410;&#21644;&#21331;&#36234;&#24615;&#33021;&#30340;LLM&#12290;&#36873;&#25321;&#24615;&#21453;&#23556;&#35843;&#33410;&#26159;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#21644;&#21512;&#25104;&#26041;&#27861;&#65292;&#36890;&#24120;&#33021;&#25913;&#21892;LLM&#24494;&#35843;&#21644;&#33258;&#25105;&#20248;&#21270;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10110v1 Announce Type: cross  Abstract: Instruction tuning is critical to large language models (LLMs) for achieving better instruction following and task adaptation capabilities but its success heavily relies on the training data quality. Many recent methods focus on improving the data quality but often overlook the compatibility of the data with the student model being finetuned. This paper introduces Selective Reflection-Tuning, a novel paradigm that synergizes a teacher LLM's reflection and introspection for improving existing data quality with the data selection capability of the student LLM, to automatically refine existing instruction-tuning data. This teacher-student collaboration produces high-quality and student-compatible instruction-response pairs, resulting in sample-efficient instruction tuning and LLMs of superior performance. Selective Reflection-Tuning is a data augmentation and synthesis that generally improves LLM finetuning and self-improvement without co
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#23454;&#29616;&#20102;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#22312;&#38899;&#20048;&#20462;&#22797;&#21644;&#38899;&#20048;&#25490;&#21015;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#22312;&#22810;&#20010;&#38899;&#20048;&#32534;&#36753;&#20219;&#21153;&#20013;&#65292;&#35813;&#26041;&#27861;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;AI&#39537;&#21160;&#38899;&#20048;&#32534;&#36753;&#24037;&#20855;&#25552;&#20379;&#20102;&#26356;&#28789;&#27963;&#30340;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.09508</link><description>&lt;p&gt;
&#25490;&#21015;&#12289;&#20462;&#22797;&#21644;&#25913;&#36827;&#65306;&#36890;&#36807;&#22522;&#20110;&#20869;&#23481;&#30340;&#25511;&#21046;&#23454;&#29616;&#21487;&#25805;&#25511;&#30340;&#38271;&#26399;&#38899;&#20048;&#38899;&#39057;&#29983;&#25104;&#21644;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Arrange, Inpaint, and Refine: Steerable Long-term Music Audio Generation and Editing via Content-based Controls
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09508
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#23454;&#29616;&#20102;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#22312;&#38899;&#20048;&#20462;&#22797;&#21644;&#38899;&#20048;&#25490;&#21015;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#22312;&#22810;&#20010;&#38899;&#20048;&#32534;&#36753;&#20219;&#21153;&#20013;&#65292;&#35813;&#26041;&#27861;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;AI&#39537;&#21160;&#38899;&#20048;&#32534;&#36753;&#24037;&#20855;&#25552;&#20379;&#20102;&#26356;&#28789;&#27963;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#25511;&#38899;&#20048;&#29983;&#25104;&#22312;&#20154;&#26426;&#38899;&#20048;&#20849;&#21019;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#38899;&#20048;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#23545;&#33258;&#22238;&#24402;&#29983;&#25104;&#30340;&#20381;&#36182;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#38899;&#20048;&#32534;&#36753;&#20219;&#21153;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20351;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26080;&#32541;&#22320;&#35299;&#20915;&#38899;&#20048;&#20462;&#22797;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;PEFT&#26041;&#27861;&#38598;&#25104;&#20102;&#22522;&#20110;&#24103;&#32423;&#20869;&#23481;&#30340;&#25511;&#21046;&#65292;&#20419;&#36827;&#20102;&#36712;&#36947;&#26465;&#20214;&#38899;&#20048;&#30340;&#31934;&#28860;&#21644;&#20998;&#25968;&#26465;&#20214;&#38899;&#20048;&#30340;&#25490;&#21015;&#12290;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;MusicGen&#65292;&#19968;&#20010;&#39046;&#20808;&#30340;&#33258;&#22238;&#24402;&#38899;&#20048;&#29983;&#25104;&#27169;&#22411;&#30340;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#22312;&#22810;&#20010;&#38899;&#20048;&#32534;&#36753;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20026;&#26410;&#26469;&#30340;AI&#39537;&#21160;&#38899;&#20048;&#32534;&#36753;&#24037;&#20855;&#25552;&#20379;&#20102;&#26356;&#28789;&#27963;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09508v1 Announce Type: cross  Abstract: Controllable music generation plays a vital role in human-AI music co-creation. While Large Language Models (LLMs) have shown promise in generating high-quality music, their focus on autoregressive generation limits their utility in music editing tasks. To bridge this gap, we introduce a novel Parameter-Efficient Fine-Tuning (PEFT) method. This approach enables autoregressive language models to seamlessly address music inpainting tasks. Additionally, our PEFT method integrates frame-level content-based controls, facilitating track-conditioned music refinement and score-conditioned music arrangement. We apply this method to fine-tune MusicGen, a leading autoregressive music generation model. Our experiments demonstrate promising results across multiple music editing tasks, offering more flexible controls for future AI-driven music editing tools. A demo page\footnote{\url{https://kikyo-16.github.io/AIR/}.} showcasing our work and source 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28040;&#24687;&#20256;&#36882;&#30340;&#36125;&#21494;&#26031;&#32852;&#37030;&#23398;&#20064;&#65288;BFL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#26399;&#26395;&#26368;&#22823;&#21270;&#21644;Turbo Deep&#36817;&#20284;&#28040;&#24687;&#20256;&#36882;&#65288;TDAMP&#65289;&#23454;&#29616;&#20998;&#24067;&#24335;&#23398;&#20064;&#21644;&#21387;&#32553;&#12290;&#35813;&#26694;&#26550;&#22312;&#22788;&#29702;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#30340;&#32570;&#28857;&#19978;&#26377;&#30528;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.07366</link><description>&lt;p&gt;
&#36890;&#36807;&#26399;&#26395;&#26368;&#22823;&#21270;&#21644;Turbo Deep&#36817;&#20284;&#28040;&#24687;&#20256;&#36882;&#30340;&#36125;&#21494;&#26031;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bayesian Federated Learning Via Expectation Maximization and Turbo Deep Approximate Message Passing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28040;&#24687;&#20256;&#36882;&#30340;&#36125;&#21494;&#26031;&#32852;&#37030;&#23398;&#20064;&#65288;BFL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#26399;&#26395;&#26368;&#22823;&#21270;&#21644;Turbo Deep&#36817;&#20284;&#28040;&#24687;&#20256;&#36882;&#65288;TDAMP&#65289;&#23454;&#29616;&#20998;&#24067;&#24335;&#23398;&#20064;&#21644;&#21387;&#32553;&#12290;&#35813;&#26694;&#26550;&#22312;&#22788;&#29702;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#30340;&#32570;&#28857;&#19978;&#26377;&#30528;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#33539; paradigm&#65292;&#22312;&#36825;&#31181;&#33539;&#24335;&#20013;&#65292;&#23458;&#25143;&#31471;&#25317;&#26377;&#20998;&#25955;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#32780;&#20013;&#22830;&#26381;&#21153;&#22120;&#21017;&#36127;&#36131;&#32858;&#21512;&#21644;&#35843;&#24230;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#28041;&#21450;&#23458;&#25143;&#31471;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#26469;&#35757;&#32451;&#20182;&#20204;&#30340;&#26412;&#22320;&#27169;&#22411;&#65292;&#20294;&#36825;&#24102;&#26469;&#20102;&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#21644;&#23481;&#26131;&#38519;&#20837;&#27425;&#20248;&#35299;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28040;&#24687;&#20256;&#36882;&#30340;&#36125;&#21494;&#26031;&#32852;&#37030;&#23398;&#20064;&#65288;BFL&#65289;&#26694;&#26550;&#26469;&#36991;&#20813;&#36825;&#20123;&#32570;&#28857;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#23398;&#20064;&#21644;&#21387;&#32553;&#38382;&#39064;&#24314;&#27169;&#20026;&#31232;&#30095;&#36125;&#21494;&#26031;&#25512;&#26029;&#38382;&#39064;&#65292;&#20854;&#20013;&#37319;&#29992;&#20102;&#20998;&#32452;&#31232;&#30095;&#20808;&#39564;&#20197;&#23454;&#29616;&#32467;&#26500;&#21270;&#27169;&#22411;&#21387;&#32553;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340; BFL &#31639;&#27861;&#65292;&#21517;&#20026; EMTDAMP&#65292;&#20854;&#20013;&#23558;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#21644; Turbo Deep &#36817;&#20284;&#28040;&#24687;&#20256;&#36882;&#65288;TDAMP&#65289;&#32467;&#21512;&#36215;&#26469;&#23454;&#29616;&#20998;&#24067;&#24335;&#23398;&#20064;&#21644;&#21387;&#32553;&#12290;&#20013;&#22830;&#26381;&#21153;&#22120;&#32858;&#21512;&#26412;&#22320;&#21518;&#39564;&#20998;&#24067;&#20197;&#23454;&#29616;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a machine learning paradigm where the clients possess decentralized training data and the central server handles aggregation and scheduling. Typically, FL algorithms involve clients training their local models using stochastic gradient descent (SGD), which carries drawbacks such as slow convergence and being prone to getting stuck in suboptimal solutions. In this work, we propose a message passing based Bayesian federated learning (BFL) framework to avoid these drawbacks.Specifically, we formulate the problem of deep neural network (DNN) learning and compression and as a sparse Bayesian inference problem, in which group sparse prior is employed to achieve structured model compression. Then, we propose an efficient BFL algorithm called EMTDAMP, where expectation maximization (EM) and turbo deep approximate message passing (TDAMP) are combined to achieve distributed learning and compression. The central server aggregates local posterior distributions to update 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;&#26102;&#21487;&#33021;&#20986;&#29616;&#30340;&#33258;&#25105;&#28040;&#32791;&#24490;&#29615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#29702;&#24819;&#30340;&#20462;&#27491;&#20989;&#25968;&#26469;&#31283;&#23450;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#33258;&#25105;&#20462;&#27491;&#20989;&#25968;&#26469;&#36817;&#20284;&#29702;&#24819;&#30340;&#20462;&#27491;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07087</link><description>&lt;p&gt;
&#33258;&#25105;&#32416;&#27491;&#33258;&#25105;&#28040;&#32791;&#24490;&#29615;&#29992;&#20110;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Self-Correcting Self-Consuming Loops for Generative Model Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;&#26102;&#21487;&#33021;&#20986;&#29616;&#30340;&#33258;&#25105;&#28040;&#32791;&#24490;&#29615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#29702;&#24819;&#30340;&#20462;&#27491;&#20989;&#25968;&#26469;&#31283;&#23450;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#33258;&#25105;&#20462;&#27491;&#20989;&#25968;&#26469;&#36817;&#20284;&#29702;&#24819;&#30340;&#20462;&#27491;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21512;&#25104;&#25968;&#25454;&#22312;&#20114;&#32852;&#32593;&#19978;&#30340;&#36136;&#37327;&#36234;&#26469;&#36234;&#39640;&#20197;&#21450;&#25968;&#37327;&#19981;&#26029;&#22686;&#21152;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#22312;&#20154;&#24037;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#25968;&#25454;&#30340;&#28151;&#21512;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#23613;&#31649;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#34920;&#24449;&#23398;&#20064;&#30340;&#25104;&#21151;&#26696;&#20363;&#26377;&#24456;&#22810;&#65292;&#20294;&#26159;&#22312;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;&#20013;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#20250;&#20135;&#29983;"&#33258;&#25105;&#28040;&#32791;&#24490;&#29615;"&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#35757;&#32451;&#19981;&#31283;&#23450;&#29978;&#33267;&#23849;&#28291;&#65292;&#38500;&#38750;&#28385;&#36275;&#26576;&#20123;&#26465;&#20214;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#26088;&#22312;&#31283;&#23450;&#33258;&#25105;&#28040;&#32791;&#30340;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#29702;&#24819;&#30340;&#20462;&#27491;&#20989;&#25968;&#65292;&#23558;&#25968;&#25454;&#28857;&#26144;&#23556;&#20026;&#26356;&#26377;&#21487;&#33021;&#26469;&#33258;&#30495;&#23454;&#25968;&#25454;&#20998;&#24067;&#30340;&#26679;&#26412;&#65292;&#21487;&#20197;&#20351;&#33258;&#25105;&#28040;&#32791;&#24490;&#29615;&#30340;&#31283;&#23450;&#24615;&#21576;&#25351;&#25968;&#22686;&#21152;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#25105;&#20462;&#27491;&#20989;&#25968;&#65292;&#23427;&#20381;&#36182;&#20110;&#19987;&#23478;&#30693;&#35782;&#65288;&#20363;&#22914;&#65292;&#32534;&#31243;&#22312;&#27169;&#25311;&#22120;&#20013;&#30340;&#29289;&#29702;&#23450;&#24459;&#65289;&#65292;&#24182;&#19988;&#26088;&#22312;&#33258;&#21160;&#19988;&#22823;&#35268;&#27169;&#22320;&#36817;&#20284;&#29702;&#24819;&#30340;&#20462;&#27491;&#20989;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#33258;&#25105;&#32416;&#27491;&#33258;&#25105;&#28040;&#32791;&#24490;&#29615;&#22312;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As synthetic data becomes higher quality and proliferates on the internet, machine learning models are increasingly trained on a mix of human- and machine-generated data. Despite the successful stories of using synthetic data for representation learning, using synthetic data for generative model training creates "self-consuming loops" which may lead to training instability or even collapse, unless certain conditions are met. Our paper aims to stabilize self-consuming generative model training. Our theoretical results demonstrate that by introducing an idealized correction function, which maps a data point to be more likely under the true data distribution, self-consuming loops can be made exponentially more stable. We then propose self-correction functions, which rely on expert knowledge (e.g. the laws of physics programmed in a simulator), and aim to approximate the idealized corrector automatically and at scale. We empirically validate the effectiveness of self-correcting self-consum
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prompt Adversarial Tuning (PAT)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#38450;&#24481;&#25511;&#21046;&#26426;&#21046;&#24182;&#23558;&#20854;&#20316;&#20026;&#21069;&#32512;&#23884;&#20837;&#21040;&#29992;&#25143;&#25552;&#31034;&#20013;&#65292;&#23454;&#29616;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36234;&#29425;&#34892;&#20026;&#30340;&#38450;&#24481;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20445;&#25252;LLMs&#20813;&#21463;&#20135;&#29983;&#26377;&#23475;&#20449;&#24687;&#30340;&#24433;&#21709;&#26041;&#38754;&#25928;&#26524;&#26174;&#33879;&#12290;</title><link>https://arxiv.org/abs/2402.06255</link><description>&lt;p&gt;
&#36827;&#21462;&#30340;&#40077;&#21187;&#36890;&#36807;&#25552;&#31034;&#23545;&#25239;&#35843;&#25972;&#25269;&#21046;&#36234;&#29425;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Studious Bob Fight Back Against Jailbreaking via Prompt Adversarial Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prompt Adversarial Tuning (PAT)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#38450;&#24481;&#25511;&#21046;&#26426;&#21046;&#24182;&#23558;&#20854;&#20316;&#20026;&#21069;&#32512;&#23884;&#20837;&#21040;&#29992;&#25143;&#25552;&#31034;&#20013;&#65292;&#23454;&#29616;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36234;&#29425;&#34892;&#20026;&#30340;&#38450;&#24481;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20445;&#25252;LLMs&#20813;&#21463;&#20135;&#29983;&#26377;&#23475;&#20449;&#24687;&#30340;&#24433;&#21709;&#26041;&#38754;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#20063;&#23481;&#26131;&#21463;&#21040;&#29305;&#23450;&#25552;&#31034;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#32469;&#36807;&#20869;&#32622;&#30340;&#23433;&#20840;&#25514;&#26045;&#24182;&#25552;&#20379;&#21361;&#38505;&#25110;&#38750;&#27861;&#20869;&#23481;&#65292;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#36234;&#29425;&#34892;&#20026;&#12290;&#20026;&#20102;&#20445;&#25252;LLMs&#20813;&#21463;&#20135;&#29983;&#26377;&#23475;&#20449;&#24687;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#38450;&#24481;&#31574;&#30053;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#38598;&#20013;&#22312;&#20869;&#23481;&#36807;&#28388;&#25110;&#27169;&#22411;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#38754;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prompt Adversarial Tuning&#65288;PAT&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#38450;&#24481;&#25511;&#21046;&#26426;&#21046;&#24182;&#23558;&#20854;&#20316;&#20026;&#21069;&#32512;&#23884;&#20837;&#21040;&#29992;&#25143;&#25552;&#31034;&#20013;&#26469;&#23454;&#29616;&#25105;&#20204;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31867;&#20284;&#23545;&#25239;&#35757;&#32451;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#20197;&#23454;&#29616;&#25105;&#20204;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#20132;&#26367;&#26356;&#26032;&#25915;&#20987;&#21644;&#38450;&#24481;&#25511;&#21046;&#26426;&#21046;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#20174;&#25552;&#31034;&#35843;&#25972;&#30340;&#35282;&#24230;&#23454;&#26045;&#38450;&#24481;&#30340;&#20154;&#12290;&#19968;&#26086;&#24212;&#29992;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20960;&#20046;&#19981;&#20250;&#24433;&#21709;LLMs&#30340;&#25805;&#20316;&#25928;&#29575;&#12290;&#23454;&#39564;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25269;&#24481;&#36234;&#29425;&#34892;&#20026;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although Large Language Models (LLMs) have achieved tremendous success in various applications, they are also susceptible to certain prompts that can induce them to bypass built-in safety measures and provide dangerous or illegal content, a phenomenon known as jailbreak. To protect LLMs from producing harmful information, various defense strategies are proposed, with most focusing on content filtering or adversarial training of models. In this paper, we propose an approach named Prompt Adversarial Tuning (PAT) to train a defense control mechanism, which is then embedded as a prefix to user prompts to implement our defense strategy. We design a training process similar to adversarial training to achieve our optimized goal, alternating between updating attack and defense controls. To our knowledge, we are the first to implement defense from the perspective of prompt tuning. Once employed, our method will hardly impact the operational efficiency of LLMs. Experiments show that our method i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#31038;&#20132;&#22330;&#26223;&#27169;&#25311;&#26469;&#33258;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#20943;&#36731;&#20854;&#34987;&#28389;&#29992;&#36896;&#25104;&#30340;&#28508;&#22312;&#19981;&#33391;&#24433;&#21709;&#12290;&#36890;&#36807;&#19968;&#20010;&#21517;&#20026;MATRIX&#30340;&#34394;&#25311;&#25490;&#32451;&#31354;&#38388;&#65292;LLM&#21487;&#20197;&#22312;&#22238;&#31572;&#26597;&#35810;&#21069;&#32771;&#34385;&#31038;&#20132;&#21518;&#26524;&#65292;&#24182;&#36890;&#36807;MATRIX-simulated&#25968;&#25454;&#30340;&#24494;&#35843;&#65292;&#20445;&#25345;&#23545;&#20154;&#31867;&#20215;&#20540;&#30340;&#36981;&#20174;&#21644;&#25512;&#29702;&#36895;&#24230;&#30340;&#24179;&#34913;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#28201;&#21644;&#20551;&#35774;&#19979;&#65292;&#24102;&#26377;MATRIX&#30340;LLM&#32988;&#36807;&#20102;&#23466;&#27861;AI&#12290;</title><link>https://arxiv.org/abs/2402.05699</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#22404;&#26029;&#23545;&#35805;&#30340;&#31038;&#20132;&#22330;&#26223;&#27169;&#25311;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Self-Alignment of Large Language Models via Monopolylogue-based Social Scene Simulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#31038;&#20132;&#22330;&#26223;&#27169;&#25311;&#26469;&#33258;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#20943;&#36731;&#20854;&#34987;&#28389;&#29992;&#36896;&#25104;&#30340;&#28508;&#22312;&#19981;&#33391;&#24433;&#21709;&#12290;&#36890;&#36807;&#19968;&#20010;&#21517;&#20026;MATRIX&#30340;&#34394;&#25311;&#25490;&#32451;&#31354;&#38388;&#65292;LLM&#21487;&#20197;&#22312;&#22238;&#31572;&#26597;&#35810;&#21069;&#32771;&#34385;&#31038;&#20132;&#21518;&#26524;&#65292;&#24182;&#36890;&#36807;MATRIX-simulated&#25968;&#25454;&#30340;&#24494;&#35843;&#65292;&#20445;&#25345;&#23545;&#20154;&#31867;&#20215;&#20540;&#30340;&#36981;&#20174;&#21644;&#25512;&#29702;&#36895;&#24230;&#30340;&#24179;&#34913;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#28201;&#21644;&#20551;&#35774;&#19979;&#65292;&#24102;&#26377;MATRIX&#30340;LLM&#32988;&#36807;&#20102;&#23466;&#27861;AI&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#19982;&#20154;&#31867;&#20215;&#20540;&#23545;&#40784;&#65292;&#20197;&#20943;&#36731;&#20854;&#34987;&#28389;&#29992;&#36896;&#25104;&#30340;&#28508;&#22312;&#19981;&#33391;&#24433;&#21709;&#65292;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26412;&#25991;&#20511;&#37492;&#31038;&#20250;&#23398;&#30340;&#35265;&#35299;&#65292;&#21363;&#35748;&#35782;&#21040;&#25152;&#26377;&#21508;&#26041;&#30340;&#20851;&#20999;&#26159;&#22609;&#36896;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23545;&#40784;LLMs&#30340;&#26032;&#26041;&#21521;&#65306;&#31038;&#20132;&#22330;&#26223;&#27169;&#25311;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MATRIX&#30340;&#21019;&#26032;&#31038;&#20132;&#22330;&#26223;&#27169;&#25311;&#22120;&#65292;&#23427;&#21487;&#20197;&#27169;&#25311;&#29992;&#25143;&#36755;&#20837;&#26597;&#35810;&#21608;&#22260;&#30340;&#29616;&#23454;&#22330;&#26223;&#65292;&#20351;LLM&#22312;&#22238;&#31572;&#21069;&#33021;&#22815;&#32771;&#34385;&#31038;&#20132;&#21518;&#26524;&#12290;MATRIX&#31867;&#20284;&#20110;&#19968;&#20010;&#8220;&#22404;&#26029;&#23545;&#35805;&#8221;&#19979;&#30340;&#34394;&#25311;&#25490;&#32451;&#31354;&#38388;&#65292;LLM&#22312;&#20854;&#20013;&#25198;&#28436;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#22810;&#20010;&#35282;&#33394;&#24182;&#36827;&#34892;&#33258;&#25105;&#23454;&#36341;&#12290;&#20026;&#20102;&#24341;&#20837;&#36825;&#31181;&#23545;&#40784;&#33021;&#21147;&#65292;&#25105;&#20204;&#20351;&#29992;MATRIX&#27169;&#25311;&#25968;&#25454;&#23545;LLM&#36827;&#34892;&#24494;&#35843;&#65292;&#30830;&#20445;&#20854;&#22312;&#19981;&#24433;&#21709;&#25512;&#29702;&#36895;&#24230;&#30340;&#24773;&#20917;&#19979;&#31526;&#21512;&#20154;&#31867;&#20215;&#20540;&#35266;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#28201;&#21644;&#20551;&#35774;&#19979;&#65292;&#24102;&#26377;MATRIX&#30340;LLM&#32988;&#36807;&#20102;&#23466;&#27861;AI&#12290;&#26368;&#21518;&#65292;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#37117;&#21462;&#24471;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aligning large language models (LLMs) with human values is imperative to mitigate potential adverse effects resulting from their misuse. Drawing from the sociological insight that acknowledging all parties' concerns is a key factor in shaping human values, this paper proposes a novel direction to align LLMs by themselves: social scene simulation. To achieve this, we present MATRIX, a novel social scene simulator that emulates realistic scenes around a user's input query, enabling the LLM to take social consequences into account before responding. MATRIX serves as a virtual rehearsal space, akin to a Monopolylogue, where the LLM performs diverse roles related to the query and practice by itself. To inject this alignment, we fine-tune the LLM with MATRIX-simulated data, ensuring adherence to human values without compromising inference speed. We theoretically show that the LLM with MATRIX outperforms Constitutional AI under mild assumptions. Finally, extensive experiments validate that ou
&lt;/p&gt;</description></item><item><title>AttnLRP&#26159;&#39318;&#20010;&#33021;&#22815;&#24544;&#23454;&#19988;&#20840;&#38754;&#22320;&#24402;&#22240;Transformer&#27169;&#22411;&#30340;&#36755;&#20837;&#21644;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#20855;&#26377;&#19982;&#21333;&#19968;&#21453;&#21521;&#20256;&#25773;&#30456;&#20284;&#30340;&#35745;&#31639;&#25928;&#29575;&#30340;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#25193;&#23637;&#36880;&#23618;&#30456;&#20851;&#20256;&#36882;&#24402;&#22240;&#26041;&#27861;&#20197;&#22788;&#29702;&#27880;&#24847;&#21147;&#23618;&#26469;&#35299;&#20915;&#20102;&#40657;&#30418;Transformer&#27169;&#22411;&#30340;&#24402;&#22240;&#38382;&#39064;&#65292;&#20855;&#26377;&#36229;&#36234;&#29616;&#26377;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#29702;&#35299;&#28508;&#22312;&#34920;&#31034;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05602</link><description>&lt;p&gt;
AttnLRP: &#27880;&#24847;&#21147;&#24863;&#30693;&#30340;&#36880;&#23618;&#30456;&#20851;&#20256;&#36882;&#29992;&#20110;Transformer
&lt;/p&gt;
&lt;p&gt;
AttnLRP: Attention-Aware Layer-wise Relevance Propagation for Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05602
&lt;/p&gt;
&lt;p&gt;
AttnLRP&#26159;&#39318;&#20010;&#33021;&#22815;&#24544;&#23454;&#19988;&#20840;&#38754;&#22320;&#24402;&#22240;Transformer&#27169;&#22411;&#30340;&#36755;&#20837;&#21644;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#20855;&#26377;&#19982;&#21333;&#19968;&#21453;&#21521;&#20256;&#25773;&#30456;&#20284;&#30340;&#35745;&#31639;&#25928;&#29575;&#30340;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#25193;&#23637;&#36880;&#23618;&#30456;&#20851;&#20256;&#36882;&#24402;&#22240;&#26041;&#27861;&#20197;&#22788;&#29702;&#27880;&#24847;&#21147;&#23618;&#26469;&#35299;&#20915;&#20102;&#40657;&#30418;Transformer&#27169;&#22411;&#30340;&#24402;&#22240;&#38382;&#39064;&#65292;&#20855;&#26377;&#36229;&#36234;&#29616;&#26377;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#29702;&#35299;&#28508;&#22312;&#34920;&#31034;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#20135;&#29983;&#20559;&#35265;&#30340;&#39044;&#27979;&#21644;&#24187;&#35937;&#65292;&#36825;&#31361;&#26174;&#20102;&#29702;&#35299;&#20854;&#27169;&#22411;&#20869;&#37096;&#25512;&#29702;&#36807;&#31243;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#23545;&#25972;&#20010;&#40657;&#30418;Transformer&#27169;&#22411;&#30340;&#20934;&#30830;&#24402;&#22240;&#24182;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#26159;&#19968;&#20010;&#23578;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#25193;&#23637;&#36880;&#23618;&#30456;&#20851;&#20256;&#36882;&#24402;&#22240;&#26041;&#27861;&#20197;&#22788;&#29702;&#27880;&#24847;&#21147;&#23618;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#12290;&#34429;&#28982;&#23384;&#22312;&#37096;&#20998;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#39318;&#20010;&#33021;&#22815;&#24544;&#23454;&#19988;&#20840;&#38754;&#22320;&#24402;&#22240;Transformer&#27169;&#22411;&#30340;&#36755;&#20837;&#21644;&#28508;&#22312;&#34920;&#31034;&#65292;&#21516;&#26102;&#35745;&#31639;&#25928;&#29575;&#19982;&#21333;&#19968;&#21453;&#21521;&#20256;&#25773;&#30456;&#20284;&#12290;&#36890;&#36807;&#23545;Llama 2&#12289;Flan-T5&#21644;Vision Transformer&#26550;&#26500;&#19978;&#19982;&#29616;&#26377;&#26041;&#27861;&#30340;&#24191;&#27867;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#36229;&#36807;&#20102;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#33021;&#22815;&#29702;&#35299;&#28508;&#22312;&#34920;&#31034;&#65292;&#20026;&#27010;&#24565;&#25171;&#24320;&#20102;&#22823;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are prone to biased predictions and hallucinations, underlining the paramount importance of understanding their model-internal reasoning process. However, achieving faithful attributions for the entirety of a black-box transformer model and maintaining computational efficiency is an unsolved challenge. By extending the Layer-wise Relevance Propagation attribution method to handle attention layers, we address these challenges effectively. While partial solutions exist, our method is the first to faithfully and holistically attribute not only input but also latent representations of transformer models with the computational efficiency similar to a singular backward pass. Through extensive evaluations against existing methods on Llama 2, Flan-T5 and the Vision Transformer architecture, we demonstrate that our proposed approach surpasses alternative methods in terms of faithfulness and enables the understanding of latent representations, opening up the door for concep
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#8220;ChatCoach&#8221;&#65292;&#19968;&#20010;&#38598;&#25104;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21307;&#29983;&#21512;&#20316;&#30340;&#26694;&#26550;&#65292;&#22312;&#20132;&#27969;&#21307;&#30103;&#36741;&#23548;&#20013;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#27169;&#25311;&#29615;&#22659;&#21644;&#23454;&#26102;&#21453;&#39304;&#65292;&#20197;&#24110;&#21161;&#21307;&#23398;&#23398;&#21592;&#25552;&#39640;&#27807;&#36890;&#25216;&#24039;&#12290;</title><link>https://arxiv.org/abs/2402.05547</link><description>&lt;p&gt;
&#22312;&#20132;&#27969;&#21307;&#30103;&#36741;&#23548;&#19978;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65306;&#19968;&#31181;&#26032;&#30340;&#31995;&#32479;&#21644;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Large Language Models on Communicative Medical Coaching: a Novel System and Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#8220;ChatCoach&#8221;&#65292;&#19968;&#20010;&#38598;&#25104;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21307;&#29983;&#21512;&#20316;&#30340;&#26694;&#26550;&#65292;&#22312;&#20132;&#27969;&#21307;&#30103;&#36741;&#23548;&#20013;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#27169;&#25311;&#29615;&#22659;&#21644;&#23454;&#26102;&#21453;&#39304;&#65292;&#20197;&#24110;&#21161;&#21307;&#23398;&#23398;&#21592;&#25552;&#39640;&#27807;&#36890;&#25216;&#24039;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#20256;&#32479;&#24212;&#29992;&#20027;&#35201;&#38598;&#20013;&#22312;&#20197;&#24739;&#32773;&#20026;&#20013;&#24515;&#30340;&#26381;&#21153;&#19978;&#65292;&#22686;&#24378;&#24739;&#32773;&#20114;&#21160;&#21644;&#25252;&#29702;&#20132;&#20184;&#65292;&#20363;&#22914;&#21307;&#23398;&#23545;&#35805;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;NLP&#22312;&#24110;&#21161;&#32463;&#39564;&#19981;&#20016;&#23500;&#30340;&#21307;&#29983;&#65292;&#29305;&#21035;&#26159;&#22312;&#20132;&#27969;&#21307;&#30103;&#36741;&#23548;&#31561;&#39046;&#22495;&#30340;&#28508;&#21147;&#20173;&#28982;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;ChatCoach&#8221;&#65292;&#19968;&#20010;&#38598;&#25104;&#30340;&#20154;&#24037;&#26234;&#33021;&#21512;&#20316;&#26694;&#26550;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;&#19968;&#20010;&#24739;&#32773;&#20195;&#29702;&#21644;&#19968;&#20010;&#36741;&#23548;&#20195;&#29702;&#20849;&#21516;&#25903;&#25345;&#21307;&#23398;&#23398;&#21592;&#22312;&#20250;&#35786;&#36807;&#31243;&#20013;&#32451;&#20064;&#21307;&#23398;&#27807;&#36890;&#25216;&#24039;&#12290;&#19982;&#20256;&#32479;&#30340;&#23545;&#35805;&#31995;&#32479;&#19981;&#21516;&#65292;ChatCoach&#25552;&#20379;&#20102;&#19968;&#20010;&#27169;&#25311;&#29615;&#22659;&#65292;&#21307;&#29983;&#21487;&#20197;&#22312;&#20854;&#20013;&#19982;&#24739;&#32773;&#20195;&#29702;&#36827;&#34892;&#21307;&#23398;&#23545;&#35805;&#12290;&#21516;&#26102;&#65292;&#36741;&#23548;&#20195;&#29702;&#20250;&#25552;&#20379;&#23454;&#26102;&#21453;&#39304;&#32473;&#21307;&#29983;&#12290;&#20026;&#20102;&#26500;&#24314;ChatCoach&#31995;&#32479;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#38598;&#25104;&#20102;ChatGPT&#21644;Llama2&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#35780;&#20272;&#23427;&#20204;&#22312;&#20132;&#27969;&#21307;&#30103;&#36741;&#23548;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional applications of natural language processing (NLP) in healthcare have predominantly focused on patient-centered services, enhancing patient interactions and care delivery, such as through medical dialogue systems. However, the potential of NLP to benefit inexperienced doctors, particularly in areas such as communicative medical coaching, remains largely unexplored. We introduce ``ChatCoach,'' an integrated human-AI cooperative framework. Within this framework, both a patient agent and a coaching agent collaboratively support medical learners in practicing their medical communication skills during consultations. Unlike traditional dialogue systems, ChatCoach provides a simulated environment where a human doctor can engage in medical dialogue with a patient agent. Simultaneously, a coaching agent provides real-time feedback to the doctor. To construct the ChatCoach system, we developed a dataset and integrated Large Language Models such as ChatGPT and Llama2, aiming to assess 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#37327;&#21270;&#36817;&#20284;&#27491;&#20132;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;QORNNs&#65289;&#26469;&#35299;&#20915;&#27491;&#20132;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;ORNNs&#65289;&#20013;&#21442;&#25968;&#36807;&#22810;&#30340;&#38382;&#39064;&#65292;&#37319;&#29992;&#19968;&#31181;&#21518;&#35757;&#32451;&#37327;&#21270;&#31574;&#30053;&#21644;&#19977;&#31181;&#34701;&#20837;&#27491;&#20132;&#32422;&#26463;&#21644;&#37327;&#21270;&#26435;&#37325;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#31639;&#27861;&#65292;&#21462;&#24471;&#20102;&#19982;s&#30456;&#20284;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.04012</link><description>&lt;p&gt;
&#37327;&#21270;&#36817;&#20284;&#27491;&#20132;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Quantized Approximately Orthogonal Recurrent Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#37327;&#21270;&#36817;&#20284;&#27491;&#20132;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;QORNNs&#65289;&#26469;&#35299;&#20915;&#27491;&#20132;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;ORNNs&#65289;&#20013;&#21442;&#25968;&#36807;&#22810;&#30340;&#38382;&#39064;&#65292;&#37319;&#29992;&#19968;&#31181;&#21518;&#35757;&#32451;&#37327;&#21270;&#31574;&#30053;&#21644;&#19977;&#31181;&#34701;&#20837;&#27491;&#20132;&#32422;&#26463;&#21644;&#37327;&#21270;&#26435;&#37325;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#31639;&#27861;&#65292;&#21462;&#24471;&#20102;&#19982;s&#30456;&#20284;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#20132;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;ORNN&#65289;&#26159;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#36873;&#25321;&#65292;&#29992;&#20110;&#23398;&#20064;&#28041;&#21450;&#20855;&#26377;&#38271;&#26399;&#20381;&#36182;&#24615;&#30340;&#26102;&#38388;&#24207;&#21015;&#30340;&#20219;&#21153;&#65292;&#30001;&#20110;&#20854;&#31616;&#21333;&#24615;&#21644;&#35745;&#31639;&#31283;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32593;&#32476;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#21442;&#25968;&#25165;&#33021;&#34920;&#29616;&#33391;&#22909;&#65292;&#36825;&#22312;&#21151;&#29575;&#21463;&#38480;&#30340;&#29615;&#22659;&#65288;&#22914;&#32039;&#20945;&#35774;&#22791;&#65289;&#20013;&#21487;&#33021;&#26159;&#38459;&#30861;&#22240;&#32032;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#31070;&#32463;&#32593;&#32476;&#37327;&#21270;&#12290;&#26500;&#24314;&#36825;&#26679;&#30340;&#32593;&#32476;&#20173;&#28982;&#26159;&#19968;&#20010;&#24453;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#20854;&#22266;&#26377;&#30340;&#19981;&#31283;&#23450;&#24615;&#26159;&#34987;&#35748;&#21487;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;ORNN&#20013;&#30340;&#24490;&#29615;&#21644;&#36755;&#20837;&#26435;&#37325;&#30697;&#38453;&#30340;&#37327;&#21270;&#65292;&#23548;&#33268;&#20102;&#37327;&#21270;&#36817;&#20284;&#27491;&#20132;RNN&#65288;QORNN&#65289;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#31574;&#30053;&#21644;&#19977;&#31181;&#34701;&#20837;&#27491;&#20132;&#32422;&#26463;&#21644;&#37327;&#21270;&#26435;&#37325;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#65288;QAT&#65289;&#31639;&#27861;&#12290;&#23454;&#35777;&#32467;&#26524;&#35777;&#26126;&#20102;&#20351;&#29992;QAT&#30340;&#20248;&#21183;&#12290;&#26368;&#39640;&#25928;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;&#19982;s&#31867;&#20284;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Orthogonal recurrent neural networks (ORNNs) are an appealing option for learning tasks involving time series with long-term dependencies, thanks to their simplicity and computational stability. However, these networks often require a substantial number of parameters to perform well, which can be prohibitive in power-constrained environments, such as compact devices. One approach to address this issue is neural network quantization. The construction of such networks remains an open problem, acknowledged for its inherent instability.In this paper, we explore the quantization of the recurrent and input weight matrices in ORNNs, leading to Quantized approximately Orthogonal RNNs (QORNNs). We investigate one post-training quantization (PTQ) strategy and three quantization-aware training (QAT) algorithms that incorporate orthogonal constraints and quantized weights. Empirical results demonstrate the advantages of employing QAT over PTQ. The most efficient model achieves results similar to s
&lt;/p&gt;</description></item><item><title>BRAIn&#26159;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#22870;&#21169;&#26465;&#20214;&#21270;&#32553;&#20943;&#25512;&#26029;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#39304;&#26469;&#25913;&#36827;RLHF&#65292;&#22312;LLM&#23545;&#40784;&#20013;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02479</link><description>&lt;p&gt;
BRAIn: Bayesian Reward-conditioned Amortized Inference for natural language generation from feedback
&lt;/p&gt;
&lt;p&gt;
BRAIn: Bayesian Reward-conditioned Amortized Inference for natural language generation from feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02479
&lt;/p&gt;
&lt;p&gt;
BRAIn&#26159;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#22870;&#21169;&#26465;&#20214;&#21270;&#32553;&#20943;&#25512;&#26029;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#39304;&#26469;&#25913;&#36827;RLHF&#65292;&#22312;LLM&#23545;&#40784;&#20013;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#65292;&#32487;Proximal Policy Optimization (PPO)&#21462;&#24471;&#25104;&#21151;&#20043;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22914;Sequence Likelihood Calibration (SLiC)&#21644;Direct Policy Optimization (DPO)&#65292;&#36825;&#20123;&#26041;&#27861;&#26159;&#31163;&#32447;&#30340;&#65292;&#24182;&#19988;&#20197;&#38388;&#25509;&#30340;&#26041;&#24335;&#20351;&#29992;&#22870;&#21169;&#12290;&#36825;&#20123;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;DPO&#65292;&#30001;&#20110;&#20854;&#21487;&#25193;&#23637;&#24615;&#21644;&#24615;&#33021;&#65292;&#26368;&#36817;&#24050;&#32463;&#25104;&#20026;LLM&#23545;&#40784;&#30340;&#39318;&#36873;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36951;&#28431;&#20102;PPO&#26041;&#27861;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;&#35832;&#22914;SLiC&#25110;RRHF&#30340;&#26041;&#27861;&#20165;&#21033;&#29992;&#22870;&#21169;&#27169;&#22411;(RM)&#36827;&#34892;&#25490;&#24207;/&#20559;&#22909;&#65292;&#20002;&#22833;&#20102;&#32454;&#31890;&#24230;&#20449;&#24687;&#65292;&#24573;&#30053;&#20102;RM&#30340;&#21442;&#25968;&#24418;&#24335;(&#20363;&#22914;Bradley-Terry&#12289;Plackett-Luce)&#65307;&#32780;&#35832;&#22914;DPO&#30340;&#26041;&#27861;&#29978;&#33267;&#19981;&#20351;&#29992;&#21333;&#29420;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;BRAIn&#65292;&#23427;&#23558;RM&#20316;&#20026;&#20998;&#24067;&#21305;&#37197;&#26041;&#27861;&#30340;&#19968;&#37096;&#20998;&#37325;&#26032;&#24341;&#20837;&#12290;BRAIn&#32771;&#34385;&#21040;&#20102;LLM&#20998;&#24067;&#22312;&#20551;&#35774;&#36755;&#20986;&#36136;&#37327;&#33391;&#22909;&#30340;&#26465;&#20214;&#19979;&#65292;&#24182;&#24212;&#29992;B...
&lt;/p&gt;
&lt;p&gt;
Following the success of Proximal Policy Optimization (PPO) for Reinforcement Learning from Human Feedback (RLHF), new techniques such as Sequence Likelihood Calibration (SLiC) and Direct Policy Optimization (DPO) have been proposed that are offline in nature and use rewards in an indirect manner. These techniques, in particular DPO, have recently become the tools of choice for LLM alignment due to their scalability and performance. However, they leave behind important features of the PPO approach. Methods such as SLiC or RRHF make use of the Reward Model (RM) only for ranking/preference, losing fine-grained information and ignoring the parametric form of the RM (eg., Bradley-Terry, Plackett-Luce), while methods such as DPO do not use even a separate reward model. In this work, we propose a novel approach, named BRAIn, that re-introduces the RM as part of a distribution matching approach.BRAIn considers the LLM distribution conditioned on the assumption of output goodness and applies B
&lt;/p&gt;</description></item><item><title>DeLLMa&#26159;&#19968;&#20010;&#26088;&#22312;&#25552;&#39640;&#19981;&#30830;&#23450;&#29615;&#22659;&#19979;&#20915;&#31574;&#31934;&#24230;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#27493;&#39588;&#30340;&#33050;&#25163;&#26550;&#31243;&#24207;&#65292;&#20511;&#37492;&#20915;&#31574;&#29702;&#35770;&#21644;&#25928;&#29992;&#29702;&#35770;&#30340;&#21407;&#21017;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20915;&#31574;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02392</link><description>&lt;p&gt;
DeLLMa:&#19968;&#20010;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19979;&#20915;&#31574;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DeLLMa: A Framework for Decision Making Under Uncertainty with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02392
&lt;/p&gt;
&lt;p&gt;
DeLLMa&#26159;&#19968;&#20010;&#26088;&#22312;&#25552;&#39640;&#19981;&#30830;&#23450;&#29615;&#22659;&#19979;&#20915;&#31574;&#31934;&#24230;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#27493;&#39588;&#30340;&#33050;&#25163;&#26550;&#31243;&#24207;&#65292;&#20511;&#37492;&#20915;&#31574;&#29702;&#35770;&#21644;&#25928;&#29992;&#29702;&#35770;&#30340;&#21407;&#21017;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20915;&#31574;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21830;&#19994;&#12289;&#24037;&#31243;&#21644;&#21307;&#23398;&#31561;&#39046;&#22495;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#36825;&#20123;&#39046;&#22495;&#24448;&#24448;&#38754;&#20020;&#20915;&#31574;&#19981;&#30830;&#23450;&#24615;&#30340;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#20851;&#38190;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#22312;&#20915;&#31574;&#38382;&#39064;&#19978;&#30452;&#25509;&#20351;&#29992;LLMs&#24448;&#24448;&#25928;&#26524;&#36739;&#24046;&#65292;&#23588;&#20854;&#26159;&#22312;&#38382;&#39064;&#22797;&#26434;&#24615;&#22686;&#21152;&#26102;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DeLLMa&#65288;Decision-making Large Language Model assistant&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#39640;&#19981;&#30830;&#23450;&#29615;&#22659;&#19979;&#30340;&#20915;&#31574;&#31934;&#24230;&#12290;DeLLMa&#21253;&#25324;&#19968;&#20010;&#22810;&#27493;&#39588;&#30340;&#33050;&#25163;&#26550;&#31243;&#24207;&#65292;&#20511;&#37492;&#20102;&#20915;&#31574;&#29702;&#35770;&#21644;&#25928;&#29992;&#29702;&#35770;&#30340;&#21407;&#21017;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#26368;&#20248;&#30340;&#12289;&#21487;&#23457;&#35745;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#28041;&#21450;&#30495;&#23454;&#20892;&#19994;&#21644;&#37329;&#34701;&#25968;&#25454;&#30340;&#20915;&#31574;&#29615;&#22659;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;DeLLMa&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;LLMs&#30340;&#20915;&#31574;&#24615;&#33021;&#65292;&#20934;&#30830;&#24615;&#21487;&#25552;&#39640;&#39640;&#36798;40%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are increasingly used across society, including in domains like business, engineering, and medicine. These fields often grapple with decision-making under uncertainty, a critical yet challenging task. In this paper, we show that directly prompting LLMs on these types of decision-making problems yields poor results, especially as the problem complexity increases. To overcome this limitation, we propose DeLLMa (Decision-making Large Language Model assistant), a framework designed to enhance decision-making accuracy in uncertain environments. DeLLMa involves a multi-step scaffolding procedure, drawing upon principles from decision theory and utility theory, to provide an optimal and human-auditable decision-making process. We validate our framework on decision-making environments involving real agriculture and finance data. Our results show that DeLLMa can significantly improve LLM decision-making performance, achieving up to a 40% increase in accuracy over co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#19988;&#39640;&#24230;&#24182;&#34892;&#30340;&#29992;&#20110;&#27604;&#36739;&#29699;&#24418;&#27979;&#24230;&#30340;&#36317;&#31163;&#65292;&#20351;&#29992;&#20102;&#31435;&#20307;&#25237;&#24433;&#21644;&#24191;&#20041;Radon&#21464;&#25442;&#65292;&#31216;&#20043;&#20026;&#31435;&#20307;&#25237;&#24433;&#29699;&#38754;&#20999;&#29255;&#29926;&#29791;&#26031;&#22374;&#65288;S3W&#65289;&#36317;&#31163;&#12290;&#36890;&#36807;&#20180;&#32454;&#22788;&#29702;&#31435;&#20307;&#25237;&#24433;&#24341;&#36215;&#30340;&#36317;&#31163;&#30072;&#21464;&#65292;&#24182;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#36895;&#24230;&#21644;&#25928;&#26524;&#19978;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.02345</link><description>&lt;p&gt;
Stereographic Spherical Sliced Wasserstein Distances - &#24212;&#29992;&#20110;&#29699;&#24418;&#27010;&#29575;&#20998;&#24067;&#27604;&#36739;&#30340;&#31435;&#20307;&#25237;&#24433;&#29699;&#38754;&#20999;&#29255;&#29926;&#29791;&#26031;&#22374;&#36317;&#31163;
&lt;/p&gt;
&lt;p&gt;
Stereographic Spherical Sliced Wasserstein Distances
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#19988;&#39640;&#24230;&#24182;&#34892;&#30340;&#29992;&#20110;&#27604;&#36739;&#29699;&#24418;&#27979;&#24230;&#30340;&#36317;&#31163;&#65292;&#20351;&#29992;&#20102;&#31435;&#20307;&#25237;&#24433;&#21644;&#24191;&#20041;Radon&#21464;&#25442;&#65292;&#31216;&#20043;&#20026;&#31435;&#20307;&#25237;&#24433;&#29699;&#38754;&#20999;&#29255;&#29926;&#29791;&#26031;&#22374;&#65288;S3W&#65289;&#36317;&#31163;&#12290;&#36890;&#36807;&#20180;&#32454;&#22788;&#29702;&#31435;&#20307;&#25237;&#24433;&#24341;&#36215;&#30340;&#36317;&#31163;&#30072;&#21464;&#65292;&#24182;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#36895;&#24230;&#21644;&#25928;&#26524;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22320;&#36136;&#23398;&#12289;&#21307;&#23398;&#39046;&#22495;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#28145;&#24230;&#34920;&#31034;&#23398;&#20064;&#31561;&#21508;&#20010;&#39046;&#22495;&#65292;&#27604;&#36739;&#29699;&#24418;&#27010;&#29575;&#20998;&#24067;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#22522;&#20110;&#26368;&#20248;&#20256;&#36755;&#30340;&#36317;&#31163;&#65292;&#27604;&#22914;&#29926;&#29791;&#26031;&#22374;&#36317;&#31163;&#65292;&#23545;&#20110;&#27604;&#36739;&#27010;&#29575;&#27979;&#24230;&#24050;&#32463;&#24341;&#21457;&#20102;&#27963;&#36291;&#30340;&#30740;&#31350;&#65292;&#20197;&#24320;&#21457;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#29699;&#24418;&#27010;&#29575;&#27979;&#24230;&#30340;&#21464;&#20307;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#36895;&#19988;&#39640;&#24230;&#24182;&#34892;&#21270;&#30340;&#29992;&#20110;&#27604;&#36739;&#29699;&#24418;&#27979;&#24230;&#30340;&#36317;&#31163;&#65292;&#20351;&#29992;&#20102;&#31435;&#20307;&#25237;&#24433;&#21644;&#24191;&#20041;Radon&#21464;&#25442;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#31435;&#20307;&#25237;&#24433;&#29699;&#38754;&#20999;&#29255;&#29926;&#29791;&#26031;&#22374;&#65288;S3W&#65289;&#36317;&#31163;&#12290;&#25105;&#20204;&#20180;&#32454;&#22788;&#29702;&#20102;&#31435;&#20307;&#25237;&#24433;&#24341;&#36215;&#30340;&#36317;&#31163;&#30072;&#21464;&#65292;&#24182;&#23545;&#25105;&#20204;&#25552;&#20986;&#30340;&#24230;&#37327;&#21450;&#20854;&#20855;&#26377;&#26059;&#36716;&#19981;&#21464;&#24615;&#30340;&#21464;&#20307;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#24230;&#37327;&#30340;&#24615;&#33021;&#65292;&#24182;&#23558;&#20854;&#19982;&#26368;&#36817;&#30340;&#22522;&#32447;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#20174;&#36965;&#24863;&#21644;&#22788;&#29702;&#25928;&#29575;&#20004;&#20010;&#26041;&#38754;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Comparing spherical probability distributions is of great interest in various fields, including geology, medical domains, computer vision, and deep representation learning. The utility of optimal transport-based distances, such as the Wasserstein distance, for comparing probability measures has spurred active research in developing computationally efficient variations of these distances for spherical probability measures. This paper introduces a high-speed and highly parallelizable distance for comparing spherical measures using the stereographic projection and the generalized Radon transform, which we refer to as the Stereographic Spherical Sliced Wasserstein (S3W) distance. We carefully address the distance distortion caused by the stereographic projection and provide an extensive theoretical analysis of our proposed metric and its rotationally invariant variation. Finally, we evaluate the performance of the proposed metrics and compare them with recent baselines in terms of both spe
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22312;&#22270;&#19978;&#28145;&#20837;&#30740;&#31350;&#20102;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#65292;&#20174;&#27169;&#22411;&#21644;&#25968;&#25454;&#20004;&#20010;&#35282;&#24230;&#36827;&#34892;&#20102;&#25506;&#32034;&#12290;&#23545;&#20110;&#27169;&#22411;&#32553;&#25918;&#65292;&#21457;&#29616;&#20102;&#32553;&#25918;&#23450;&#24459;&#23849;&#28291;&#21644;&#36807;&#25311;&#21512;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#28145;&#24230;&#22270;&#27169;&#22411;&#30340;&#27169;&#22411;&#28145;&#24230;&#23545;&#32553;&#25918;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;&#23545;&#20110;&#25968;&#25454;&#32553;&#25918;&#65292;&#25552;&#20986;&#20102;&#22270;&#25968;&#37327;&#19981;&#36866;&#21512;&#20316;&#20026;&#34913;&#37327;&#32553;&#25918;&#23450;&#24459;&#20013;&#22270;&#25968;&#25454;&#37327;&#30340;&#25351;&#26631;&#12290;</title><link>https://arxiv.org/abs/2402.02054</link><description>&lt;p&gt;
&#22270;&#19978;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Neural Scaling Laws on Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22312;&#22270;&#19978;&#28145;&#20837;&#30740;&#31350;&#20102;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#65292;&#20174;&#27169;&#22411;&#21644;&#25968;&#25454;&#20004;&#20010;&#35282;&#24230;&#36827;&#34892;&#20102;&#25506;&#32034;&#12290;&#23545;&#20110;&#27169;&#22411;&#32553;&#25918;&#65292;&#21457;&#29616;&#20102;&#32553;&#25918;&#23450;&#24459;&#23849;&#28291;&#21644;&#36807;&#25311;&#21512;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#28145;&#24230;&#22270;&#27169;&#22411;&#30340;&#27169;&#22411;&#28145;&#24230;&#23545;&#32553;&#25918;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;&#23545;&#20110;&#25968;&#25454;&#32553;&#25918;&#65292;&#25552;&#20986;&#20102;&#22270;&#25968;&#37327;&#19981;&#36866;&#21512;&#20316;&#20026;&#34913;&#37327;&#32553;&#25918;&#23450;&#24459;&#20013;&#22270;&#25968;&#25454;&#37327;&#30340;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#22270;&#27169;&#22411;&#65288;&#20363;&#22914;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#22270;&#21464;&#25442;&#22120;&#65289;&#24050;&#25104;&#20026;&#21033;&#29992;&#21508;&#31181;&#31867;&#22411;&#22270;&#30340;&#30693;&#35782;&#30340;&#37325;&#35201;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#22270;&#27169;&#22411;&#30340;&#32553;&#25918;&#29305;&#24615;&#23578;&#26410;&#24471;&#21040;&#31995;&#32479;&#30740;&#31350;&#65292;&#23545;&#36890;&#36807;&#25193;&#22823;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#26469;&#23454;&#29616;&#22823;&#22411;&#22270;&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#20135;&#29983;&#20102;&#30097;&#38382;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#27169;&#22411;&#21644;&#25968;&#25454;&#30340;&#35282;&#24230;&#28145;&#20837;&#25506;&#32034;&#20102;&#22270;&#19978;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#12290;&#25105;&#20204;&#39318;&#20808;&#39564;&#35777;&#20102;&#36825;&#20123;&#23450;&#24459;&#22312;&#22270;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#24314;&#31435;&#20102;&#25551;&#36848;&#32553;&#25918;&#34892;&#20026;&#30340;&#20844;&#24335;&#12290;&#23545;&#20110;&#27169;&#22411;&#32553;&#25918;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#32553;&#25918;&#23450;&#24459;&#23849;&#28291;&#29616;&#35937;&#65292;&#24182;&#30830;&#23450;&#20102;&#36807;&#25311;&#21512;&#21487;&#33021;&#26159;&#21407;&#22240;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#28145;&#24230;&#22270;&#27169;&#22411;&#30340;&#27169;&#22411;&#28145;&#24230;&#21487;&#20197;&#24433;&#21709;&#27169;&#22411;&#32553;&#25918;&#34892;&#20026;&#65292;&#36825;&#19982;&#20854;&#20182;&#39046;&#22495;&#65288;&#22914;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65289;&#30340;&#35266;&#23519;&#32467;&#26524;&#19981;&#21516;&#12290;&#23545;&#20110;&#25968;&#25454;&#32553;&#25918;&#65292;&#25105;&#20204;&#24314;&#35758;&#22270;&#25968;&#37327;&#26080;&#27861;&#26377;&#25928;&#34913;&#37327;&#22270;&#25968;&#25454;&#37327;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#22240;&#20026;...
&lt;/p&gt;
&lt;p&gt;
Deep graph models (e.g., graph neural networks and graph transformers) have become important techniques for leveraging knowledge across various types of graphs. Yet, the scaling properties of deep graph models have not been systematically investigated, casting doubt on the feasibility of achieving large graph models through enlarging the model and dataset sizes. In this work, we delve into neural scaling laws on graphs from both model and data perspectives. We first verify the validity of such laws on graphs, establishing formulations to describe the scaling behaviors. For model scaling, we investigate the phenomenon of scaling law collapse and identify overfitting as the potential reason. Moreover, we reveal that the model depth of deep graph models can impact the model scaling behaviors, which differ from observations in other domains such as CV and NLP. For data scaling, we suggest that the number of graphs can not effectively metric the graph data volume in scaling law since the si
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;BPDec&#65288;BERT&#39044;&#35757;&#32451;&#35299;&#30721;&#22120;&#65289;&#30340;&#28508;&#21147;&#65292;&#24378;&#35843;&#22686;&#24378;&#30340;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#35299;&#30721;&#22120;&#35774;&#35745;&#21450;&#30740;&#31350;&#22312;BERT&#39044;&#35757;&#32451;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.15861</link><description>&lt;p&gt;
BPDec: &#25581;&#31034;BERT&#39044;&#35757;&#32451;&#20013;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#35299;&#30721;&#22120;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
BPDec: Unveiling the Potential of Masked Language Modeling Decoder in BERT pretraining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.15861
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;BPDec&#65288;BERT&#39044;&#35757;&#32451;&#35299;&#30721;&#22120;&#65289;&#30340;&#28508;&#21147;&#65292;&#24378;&#35843;&#22686;&#24378;&#30340;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#35299;&#30721;&#22120;&#35774;&#35745;&#21450;&#30740;&#31350;&#22312;BERT&#39044;&#35757;&#32451;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
BERT&#65288;&#26469;&#33258;Transformer&#30340;&#21452;&#21521;&#32534;&#30721;&#34920;&#31034;&#65289;&#36890;&#36807;&#20854;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#20986;&#33394;&#30340;&#24615;&#33021;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#20154;&#21592;&#20027;&#35201;&#38598;&#20013;&#22312;&#19982;&#27169;&#22411;&#32467;&#26500;&#30456;&#20851;&#30340;&#22686;&#24378;&#65292;&#20363;&#22914;&#30456;&#23545;&#20301;&#32622;&#23884;&#20837;&#21644;&#26356;&#26377;&#25928;&#30340;&#27880;&#24847;&#26426;&#21046;&#12290;&#36824;&#26377;&#19968;&#20123;&#20154;&#28145;&#20837;&#30740;&#31350;&#20102;&#19982;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#30456;&#20851;&#30340;&#39044;&#35757;&#32451;&#25216;&#24039;&#65292;&#21253;&#25324;&#25972;&#35789;&#25513;&#30721;&#12290;DeBERTa&#24341;&#20837;&#20102;&#19968;&#31181;&#38024;&#23545;BERT&#32534;&#30721;&#22120;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#22686;&#24378;&#35299;&#30721;&#22120;&#65292;&#35777;&#26126;&#25928;&#26524;&#38750;&#24120;&#26174;&#33879;&#12290;&#25105;&#20204;&#35748;&#20026;&#22260;&#32469;&#22686;&#24378;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#35299;&#30721;&#22120;&#30340;&#35774;&#35745;&#21644;&#30740;&#31350;&#24182;&#26410;&#24471;&#21040;&#24212;&#26377;&#30340;&#37325;&#35270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#22686;&#24378;&#35299;&#30721;&#22120;&#30340;&#35774;&#35745;&#65292;&#24182;&#20171;&#32461;&#20102;BPDec&#65288;BERT&#39044;&#35757;&#32451;&#35299;&#30721;&#22120;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#24314;&#27169;&#35757;&#32451;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#24120;&#65292;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#20250;&#38024;&#23545;&#29305;&#23450;&#30340;&#33258;&#28982;&#35821;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.15861v2 Announce Type: replace-cross  Abstract: BERT (Bidirectional Encoder Representations from Transformers) has revolutionized the field of natural language processing through its exceptional performance on numerous tasks. Yet, the majority of researchers have mainly concentrated on enhancements related to the model structure, such as relative position embedding and more efficient attention mechanisms. Others have delved into pretraining tricks associated with Masked Language Modeling, including whole word masking. DeBERTa introduced an enhanced decoder adapted for BERT's encoder model for pretraining, proving to be highly effective. We argue that the design and research around enhanced masked language modeling decoders have been underappreciated. In this paper, we propose several designs of enhanced decoders and introduce BPDec (BERT Pretraining Decoder), a novel method for modeling training. Typically, a pretrained BERT model is fine-tuned for specific Natural Language 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#21270;&#24037;&#20855;PRewrite&#65292;&#29992;&#20110;&#37325;&#20889;&#25552;&#31034;&#33609;&#26696;&#24182;&#29983;&#25104;&#39640;&#25928;&#30340;&#26032;&#25552;&#31034;&#65292;&#20197;&#35299;&#20915;&#25552;&#31034;&#24037;&#31243;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2401.08189</link><description>&lt;p&gt;
PRewrite: &#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#25552;&#31034;&#37325;&#20889;
&lt;/p&gt;
&lt;p&gt;
PRewrite: Prompt Rewriting with Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.08189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#21270;&#24037;&#20855;PRewrite&#65292;&#29992;&#20110;&#37325;&#20889;&#25552;&#31034;&#33609;&#26696;&#24182;&#29983;&#25104;&#39640;&#25928;&#30340;&#26032;&#25552;&#31034;&#65292;&#20197;&#35299;&#20915;&#25552;&#31034;&#24037;&#31243;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2401.08189v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442; &#25688;&#35201;: &#25552;&#31034;&#24037;&#31243;&#23545;&#20110;&#22522;&#20110;LLM&#30340;&#24212;&#29992;&#31243;&#24207;&#30340;&#24320;&#21457;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#20197;&#8220;&#35797;&#38169;&#8221;&#30340;&#26041;&#24335;&#25163;&#21160;&#23436;&#25104;&#12290;&#36825;&#31181;&#25163;&#21160;&#31243;&#24207;&#21487;&#33021;&#32791;&#26102;&#65292;&#25928;&#26524;&#19981;&#20339;&#65292;&#24182;&#19988;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#29983;&#25104;&#30340;&#25552;&#31034;&#37117;&#26159;&#27425;&#20248;&#30340;&#12290;&#21363;&#20351;&#23545;&#37027;&#20123;&#30475;&#20284;&#36816;&#20316;&#33391;&#22909;&#30340;&#25552;&#31034;&#65292;&#22987;&#32456;&#23384;&#22312;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#65306;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#36827;&#19968;&#27493;&#20462;&#25913;&#20351;&#25552;&#31034;&#21464;&#24471;&#26356;&#22909;&#21602;&#65311;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25552;&#31034;&#24037;&#31243;&#33258;&#21160;&#21270;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#29305;&#23450;&#30340;&#20351;&#29992;&#24773;&#26223;&#65292;&#21363;&#24320;&#21457;&#32773;/&#29992;&#25143;&#24050;&#32463;&#36215;&#33609;&#20102;&#21021;&#22987;&#25552;&#31034;&#65292;&#20294;&#32570;&#20047;&#26102;&#38388;/&#19987;&#19994;&#30693;&#35782;&#26469;&#20248;&#21270;&#23427;&#20204;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;PRewrite&#65292;&#19968;&#20010;&#33258;&#21160;&#21270;&#24037;&#20855;&#65292;&#21487;&#37325;&#20889;&#36825;&#20123;&#33609;&#26696;&#65292;&#24182;&#29983;&#25104;&#39640;&#25928;&#30340;&#26032;&#25552;&#31034;&#12290;PRewrite&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26694;&#26550;&#65292;&#20801;&#35768;&#31471;&#21040;&#31471;&#20248;&#21270;&#65292;&#25105;&#20204;&#30340;&#35774;&#35745;&#20801;&#35768;RL&#25628;&#32034;&#22312;&#22823;&#21160;&#20316;&#31354;&#38388;&#20013;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.08189v2 Announce Type: replace  Abstract: Prompt engineering is critical for the development of LLM-based applications. However, it is usually done manually in a "trial and error" fashion. This manual procedure can be time consuming, ineffective, and the generated prompts are, in a lot of cases, sub-optimal. Even for the prompts which seemingly work well, there is always a lingering question: can the prompts be made better with further modifications?   To address these questions, in this paper, we investigate prompt engineering automation. We consider a specific use case scenario in which developers/users have drafted initial prompts, but lack the time/expertise to optimize them. We propose PRewrite, an automated tool to rewrite these drafts and to generate highly effective new prompts. PRewrite is based on the Reinforcement Learning (RL) framework which allows for end-to-end optimization and our design allows the RL search to happen in a large action space. The automated to
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#20102;&#24403;&#21069;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#22312;&#35268;&#27169;&#21270;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#38543;&#30528;&#27169;&#22411;&#34987;&#39034;&#24207;&#32534;&#36753;&#22810;&#20010;&#20107;&#23454;&#65292;&#23427;&#20250;&#36880;&#28176;&#36951;&#24536;&#20808;&#21069;&#30340;&#20107;&#23454;&#21450;&#25191;&#34892;&#19979;&#28216;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2401.07453</link><description>&lt;p&gt;
&#35268;&#27169;&#21270;&#27169;&#22411;&#32534;&#36753;&#20250;&#23548;&#33268;&#28176;&#36827;&#24615;&#21644;&#31361;&#21457;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Model Editing at Scale leads to Gradual and Catastrophic Forgetting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07453
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20102;&#24403;&#21069;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#22312;&#35268;&#27169;&#21270;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#38543;&#30528;&#27169;&#22411;&#34987;&#39034;&#24207;&#32534;&#36753;&#22810;&#20010;&#20107;&#23454;&#65292;&#23427;&#20250;&#36880;&#28176;&#36951;&#24536;&#20808;&#21069;&#30340;&#20107;&#23454;&#21450;&#25191;&#34892;&#19979;&#28216;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#32534;&#36753;&#30693;&#35782;&#26159;&#19968;&#31181;&#20855;&#26377;&#21560;&#24341;&#21147;&#30340;&#33021;&#21147;&#65292;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#32416;&#27491;&#38169;&#35823;&#23398;&#20064;&#30340;&#20107;&#23454;&#65292;&#21516;&#26102;&#20351;&#29992;&#19981;&#26029;&#22686;&#38271;&#30340;&#26032;&#20107;&#23454;&#21015;&#34920;&#26356;&#26032;&#27169;&#22411;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20026;&#20102;&#20351;&#27169;&#22411;&#32534;&#36753;&#20855;&#26377;&#23454;&#38469;&#25928;&#29992;&#65292;&#25105;&#20204;&#24517;&#39035;&#33021;&#22815;&#23545;&#21516;&#19968;&#27169;&#22411;&#36827;&#34892;&#22810;&#27425;&#32534;&#36753;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#24403;&#21069;&#35268;&#27169;&#19979;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65306;ROME &#21644; MEMIT&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#38543;&#30528;&#27169;&#22411;&#34987;&#39034;&#24207;&#32534;&#36753;&#22810;&#20010;&#20107;&#23454;&#65292;&#23427;&#19981;&#26029;&#22320;&#36951;&#24536;&#20808;&#21069;&#32534;&#36753;&#36807;&#30340;&#20107;&#23454;&#20197;&#21450;&#25191;&#34892;&#19979;&#28216;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#36825;&#31181;&#36951;&#24536;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;--&#21021;&#22987;&#30340;&#28176;&#36827;&#24615;&#36951;&#24536;&#38454;&#27573;&#65292;&#38543;&#21518;&#26159;&#31361;&#28982;&#25110;&#28798;&#38590;&#24615;&#30340;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.07453v2 Announce Type: replace-cross  Abstract: Editing knowledge in large language models is an attractive capability to have which allows us to correct incorrectly learnt facts during pre-training, as well as update the model with an ever-growing list of new facts. While existing model editing techniques have shown promise, they are usually evaluated using metrics for reliability, specificity and generalization over one or few edits. We argue that for model editing to have practical utility, we must be able to make multiple edits to the same model. With this in mind, we evaluate the current model editing methods at scale, focusing on two state of the art methods: ROME and MEMIT. We find that as the model is edited sequentially with multiple facts, it continually forgets previously edited facts and the ability to perform downstream tasks. This forgetting happens in two phases -- an initial gradual but progressive forgetting phase followed by abrupt or catastrophic forgettin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31995;&#32479;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#20195;&#29702;&#30340;&#34892;&#21160;&#23545;&#20854;&#20182;&#20195;&#29702;&#30340;&#24433;&#21709;&#24402;&#22240;&#21040;&#22240;&#26524;&#25928;&#24212;&#19978;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#34913;&#37327;&#20195;&#29702;&#29305;&#23450;&#25928;&#24212;&#65288;ASE&#65289;&#30340;&#26032;&#30340;&#22240;&#26524;&#25968;&#37327;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;ASE&#30340;&#21453;&#20107;&#23454;&#23545;&#24212;&#29289;&#65288;cf-ASE&#65289;&#20197;&#21450;&#35782;&#21035;cf-ASE&#30340;&#26465;&#20214;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#22522;&#20110;&#37319;&#26679;&#30340;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2310.11334</link><description>&lt;p&gt;
&#20195;&#29702;&#29305;&#23450;&#25928;&#24212;&#65306;&#22810;&#26234;&#33021;&#20307;MDPs&#20013;&#30340;&#22240;&#26524;&#25928;&#24212;&#20256;&#25773;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Agent-Specific Effects: A Causal Effect Propagation Analysis in Multi-Agent MDPs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.11334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31995;&#32479;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#20195;&#29702;&#30340;&#34892;&#21160;&#23545;&#20854;&#20182;&#20195;&#29702;&#30340;&#24433;&#21709;&#24402;&#22240;&#21040;&#22240;&#26524;&#25928;&#24212;&#19978;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#34913;&#37327;&#20195;&#29702;&#29305;&#23450;&#25928;&#24212;&#65288;ASE&#65289;&#30340;&#26032;&#30340;&#22240;&#26524;&#25968;&#37327;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;ASE&#30340;&#21453;&#20107;&#23454;&#23545;&#24212;&#29289;&#65288;cf-ASE&#65289;&#20197;&#21450;&#35782;&#21035;cf-ASE&#30340;&#26465;&#20214;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#22522;&#20110;&#37319;&#26679;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#20013;&#24314;&#31435;&#34892;&#21160;&#19982;&#32467;&#26524;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#23545;&#20110;&#36127;&#26377;&#36131;&#20219;&#30340;&#20915;&#31574;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#35299;&#37322;&#21644;&#37327;&#21270;&#20195;&#29702;&#23545;&#36825;&#31181;&#20851;&#31995;&#30340;&#36129;&#29486;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#22810;&#26234;&#33021;&#20307;&#24207;&#36143;&#20915;&#31574;&#30340;&#32972;&#26223;&#19979;&#65292;&#36825;&#20123;&#25361;&#25112;&#23588;&#20026;&#31361;&#20986;&#65292;&#22240;&#20026;&#20195;&#29702;&#30340;&#34892;&#21160;&#23545;&#32467;&#26524;&#30340;&#22240;&#26524;&#25928;&#24212;&#21462;&#20915;&#20110;&#20854;&#20182;&#20195;&#29702;&#22914;&#20309;&#23545;&#35813;&#34892;&#21160;&#20316;&#20986;&#21709;&#24212;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#25552;&#20986;&#19968;&#20010;&#31995;&#32479;&#24615;&#30340;&#26041;&#27861;&#65292;&#23558;&#20195;&#29702;&#30340;&#34892;&#21160;&#23545;&#20854;&#20182;&#20195;&#29702;&#20135;&#29983;&#30340;&#22240;&#26524;&#25928;&#24212;&#24402;&#22240;&#21040;&#20854;&#25152;&#26045;&#21152;&#30340;&#24433;&#21709;&#19978;&#12290;&#38024;&#23545;&#22810;&#26234;&#33021;&#20307;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#25968;&#37327;&#8212;&#8212;&#20195;&#29702;&#29305;&#23450;&#25928;&#24212;&#65288;ASE&#65289;&#65292;&#29992;&#20110;&#34913;&#37327;&#20195;&#29702;&#34892;&#21160;&#23545;&#36890;&#36807;&#20854;&#20182;&#20195;&#29702;&#20256;&#25773;&#30340;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36716;&#21521;ASE&#30340;&#21453;&#20107;&#23454;&#23545;&#24212;&#29289;&#65288;cf-ASE&#65289;&#65292;&#25552;&#20379;&#20102;&#35782;&#21035;cf-ASE&#30340;&#19968;&#32452;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#22522;&#20110;&#37319;&#26679;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Establishing causal relationships between actions and outcomes is fundamental for accountable multi-agent decision-making. However, interpreting and quantifying agents' contributions to such relationships pose significant challenges. These challenges are particularly prominent in the context of multi-agent sequential decision-making, where the causal effect of an agent's action on the outcome depends on how other agents respond to that action. In this paper, our objective is to present a systematic approach for attributing the causal effects of agents' actions to the influence they exert on other agents. Focusing on multi-agent Markov decision processes, we introduce agent-specific effects (ASE), a novel causal quantity that measures the effect of an agent's action on the outcome that propagates through other agents. We then turn to the counterfactual counterpart of ASE (cf-ASE), provide a sufficient set of conditions for identifying cf-ASE, and propose a practical sampling-based algor
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32763;&#35793;&#36807;&#31243;&#20998;&#35299;&#20026;&#23567;&#27493;&#39588;&#24182;&#24341;&#20837;&#20803;&#31574;&#30053;&#21644;Plan&#27010;&#24565;&#65292;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#39640;&#32500;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2309.13672</link><description>&lt;p&gt;
&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning for Image-to-Image Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.13672
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32763;&#35793;&#36807;&#31243;&#20998;&#35299;&#20026;&#23567;&#27493;&#39588;&#24182;&#24341;&#20837;&#20803;&#31574;&#30053;&#21644;Plan&#27010;&#24565;&#65292;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#39640;&#32500;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#26041;&#27861;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#19968;&#27425;&#36816;&#34892;&#29983;&#25104;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#36825;&#26679;&#30340;&#21333;&#27493;&#27169;&#22411;&#22987;&#32456;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#21442;&#25968;&#65292;&#24182;&#23481;&#26131;&#38519;&#20837;&#22351;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#21644;&#36807;&#25311;&#21512;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#37325;&#26032;&#23450;&#20041;&#20026;&#36880;&#27493;&#30340;&#20915;&#31574;&#38382;&#39064;&#65292;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36827;&#34892;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#65288;RL-I2IT&#65289;&#12290;RL-I2IT&#26694;&#26550;&#30340;&#20851;&#38190;&#29305;&#28857;&#26159;&#23558;&#19968;&#20010;&#21333;&#20307;&#23398;&#20064;&#36807;&#31243;&#20998;&#35299;&#20026;&#23567;&#30340;&#27493;&#39588;&#65292;&#24182;&#24341;&#20837;&#19968;&#20010;&#36731;&#37327;&#32423;&#27169;&#22411;&#65292;&#36880;&#27493;&#23558;&#28304;&#22270;&#20687;&#36716;&#21270;&#20026;&#30446;&#26631;&#22270;&#20687;&#12290;&#32771;&#34385;&#21040;&#22312;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#19979;&#22788;&#29702;&#39640;&#32500;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20803;&#31574;&#30053;&#21644;&#19968;&#20010;&#26032;&#30340;&#27010;&#24565;Plan&#21040;&#26631;&#20934;&#30340;Actor-Critic&#27169;&#22411;&#20013;&#65292;&#35813;&#27010;&#24565;&#30340;&#32500;&#24230;&#36739;&#21407;&#22987;&#22270;&#20687;&#20302;&#65292;&#24182;&#19988;&#21487;&#20197;&#24110;&#21161;&#28436;&#21592;&#29983;&#25104;&#21487;&#22788;&#29702;&#30340;&#39640;&#32500;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing Image-to-Image Translation (I2IT) methods generate images in a single run of a deep learning (DL) model. However, designing such a single-step model is always challenging, requiring a huge number of parameters and easily falling into bad global minimums and overfitting. In this work, we reformulate I2IT as a step-wise decision-making problem via deep reinforcement learning (DRL) and propose a novel framework that performs RL-based I2IT (RL-I2IT). The key feature in the RL-I2IT framework is to decompose a monolithic learning process into small steps with a lightweight model to progressively transform a source image successively to a target image. Considering that it is challenging to handle high dimensional continuous state and action spaces in the conventional RL framework, we introduce meta policy with a new concept Plan to the standard Actor-Critic model, which is of a lower dimension than the original image and can facilitate the actor to generate a tractable high dime
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#21307;&#23398;&#25104;&#20687;&#20013;&#30340;&#21487;&#35299;&#37322;AI&#30340;&#19977;&#32500;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#31070;&#32463;&#31185;&#23398;&#39046;&#22495;&#20013;&#35782;&#21035;&#22823;&#33041;&#27807;&#29305;&#24449;&#30340;&#22797;&#26434;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2309.00903</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#19977;&#32500;&#26694;&#26550;&#25581;&#31034;&#23398;&#20064;&#27169;&#24335;&#65306;&#21464;&#37327;&#33041;&#27807;&#35782;&#21035;&#30340;&#32479;&#19968;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
An explainable three dimension framework to uncover learning patterns: A unified look in variable sulci recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.00903
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#21307;&#23398;&#25104;&#20687;&#20013;&#30340;&#21487;&#35299;&#37322;AI&#30340;&#19977;&#32500;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#31070;&#32463;&#31185;&#23398;&#39046;&#22495;&#20013;&#35782;&#21035;&#22823;&#33041;&#27807;&#29305;&#24449;&#30340;&#22797;&#26434;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#25361;&#25112;&#24615;&#30340;&#31070;&#32463;&#31185;&#23398;&#39046;&#22495;&#37324;&#65292;&#35270;&#35273;&#20027;&#39064;&#22312;&#19977;&#32500;&#31354;&#38388;&#20869;&#34920;&#29616;&#20986;&#39640;&#24230;&#22797;&#26434;&#24615;&#12290;&#31070;&#32463;&#31185;&#23398;&#30340;&#24212;&#29992;&#28041;&#21450;&#20174;MRI&#20013;&#35782;&#21035;&#22823;&#33041;&#27807;&#29305;&#24449;&#65292;&#30001;&#20110;&#19987;&#23478;&#20043;&#38388;&#30340;&#26631;&#27880;&#35268;&#31243;&#23384;&#22312;&#24046;&#24322;&#21644;&#22823;&#33041;&#22797;&#26434;&#30340;&#19977;&#32500;&#21151;&#33021;&#65292;&#25105;&#20204;&#38754;&#20020;&#30528;&#37325;&#22823;&#38556;&#30861;&#12290;&#22240;&#27492;&#65292;&#20256;&#32479;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#22312;&#26377;&#25928;&#39564;&#35777;&#21644;&#35780;&#20272;&#36825;&#20123;&#32593;&#32476;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#25968;&#23398;&#20844;&#24335;&#65292;&#32454;&#21270;&#20102;&#19981;&#21516;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#35299;&#37322;&#38656;&#27714;&#30340;&#21508;&#31181;&#31867;&#21035;&#65292;&#20998;&#20026;&#33258;&#35299;&#37322;&#12289;&#21322;&#35299;&#37322;&#12289;&#38750;&#35299;&#37322;&#21644;&#22522;&#20110;&#39564;&#35777;&#21327;&#35758;&#21487;&#38752;&#24615;&#30340;&#26032;&#27169;&#24335;&#23398;&#20064;&#24212;&#29992;&#12290;&#26681;&#25454;&#36825;&#20010;&#25968;&#23398;&#20844;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26088;&#22312;&#35299;&#37322;&#19977;&#32500;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.00903v2 Announce Type: replace-cross  Abstract: Explainable AI is crucial in medical imaging. In the challenging field of neuroscience, visual topics present a high level of complexity, particularly within three-dimensional space. The application of neuroscience, which involves identifying brain sulcal features from MRI, faces significant hurdles due to varying annotation protocols among experts and the intricate three-dimension functionality of the brain. Consequently, traditional explainability approaches fall short in effectively validating and evaluating these networks. To address this, we first present a mathematical formulation delineating various categories of explanation needs across diverse computer vision tasks, categorized into self-explanatory, semi-explanatory, non-explanatory, and new-pattern learning applications based on the reliability of the validation protocol. With respect to this mathematical formulation, we propose a 3D explainability framework aimed at
&lt;/p&gt;</description></item><item><title>MLLMReID&#26159;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#29289;&#20877;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#24182;&#23558;&#20854;&#35270;&#35273;&#32534;&#30721;&#22120;&#20316;&#20026;&#20027;&#24178;&#36827;&#34892;&#20248;&#21270;&#65292;&#35299;&#20915;&#20102;MLLM&#22312;ReID&#20219;&#21153;&#20013;&#30340;&#35774;&#35745;&#25351;&#20196;&#21644;&#29305;&#24449;&#23398;&#20064;&#25928;&#26524;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.13201</link><description>&lt;p&gt;
MLLMReID: &#22522;&#20110;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#29289;&#20877;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
MLLMReID: Multimodal Large Language Model-based Person Re-identification. (arXiv:2401.13201v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13201
&lt;/p&gt;
&lt;p&gt;
MLLMReID&#26159;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#29289;&#20877;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#24182;&#23558;&#20854;&#35270;&#35273;&#32534;&#30721;&#22120;&#20316;&#20026;&#20027;&#24178;&#36827;&#34892;&#20248;&#21270;&#65292;&#35299;&#20915;&#20102;MLLM&#22312;ReID&#20219;&#21153;&#20013;&#30340;&#35774;&#35745;&#25351;&#20196;&#21644;&#29305;&#24449;&#23398;&#20064;&#25928;&#26524;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#20154;&#29289;&#20877;&#35782;&#21035;&#65288;ReID&#65289;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#23578;&#26410;&#34987;&#30740;&#31350;&#12290;&#26412;&#25991;&#23558;&#30740;&#31350;&#22914;&#20309;&#23558;&#23427;&#20204;&#36866;&#24212;&#20110;ReID&#20219;&#21153;&#12290;&#19968;&#31181;&#30452;&#35266;&#30340;&#24819;&#27861;&#26159;&#20351;&#29992;ReID&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#23545;MLLM&#36827;&#34892;&#24494;&#35843;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#20316;&#20026;ReID&#30340;&#20027;&#24178;&#12290;&#28982;&#32780;&#65292;&#20173;&#23384;&#22312;&#20004;&#20010;&#26126;&#26174;&#30340;&#38382;&#39064;&#65306;&#65288;1&#65289;&#20026;ReID&#35774;&#35745;&#25351;&#20196;&#26102;&#65292;MLLM&#21487;&#33021;&#36807;&#24230;&#25311;&#21512;&#29305;&#23450;&#25351;&#20196;&#65292;&#32780;&#35774;&#35745;&#21508;&#31181;&#25351;&#20196;&#23558;&#23548;&#33268;&#26356;&#39640;&#30340;&#25104;&#26412;&#12290;&#65288;2&#65289;LLM&#30340;&#28508;&#22312;&#22270;&#20687;&#29305;&#24449;&#21521;&#37327;&#27809;&#26377;&#21442;&#19982;&#25439;&#22833;&#35745;&#31639;&#12290;&#25351;&#20196;&#23398;&#20064;&#65292;&#23545;&#40784;&#22270;&#20687;-&#25991;&#26412;&#29305;&#24449;&#65292;&#23548;&#33268;&#38388;&#25509;&#20248;&#21270;&#21644;&#23398;&#20064;&#30446;&#26631;&#19981;&#20805;&#20998;&#21033;&#29992;&#29305;&#24449;&#65292;&#38480;&#21046;&#20102;&#20154;&#29289;&#29305;&#24449;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;MLLMReID&#65306;&#22522;&#20110;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;ReID&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20844;&#20849;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal large language models (MLLM) have achieved satisfactory results in many tasks. However, their performance in the task of person re-identification (ReID) has not been explored to date. This paper will investigate how to adapt them for the task of ReID. An intuitive idea is to fine-tune MLLM with ReID image-text datasets, and then use their visual encoder as a backbone for ReID. However, there still exist two apparent issues: (1) Designing instructions for ReID, MLLMs may overfit specific instructions, and designing a variety of instructions will lead to higher costs. (2) Latent image feature vectors from LLMs are not involved in loss computation. Instructional learning, aligning image-text features, results in indirect optimization and a learning objective that inadequately utilizes features, limiting effectiveness in person feature learning. To address these problems, this paper proposes MLLMReID: Multimodal Large Language Model-based ReID. Firstly, we proposed Common Instru
&lt;/p&gt;</description></item><item><title>DeepEdit&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#22909;&#30340;&#25512;&#29702;&#19968;&#33268;&#24615;&#21644;&#23545;&#26356;&#26032;&#30693;&#35782;&#30340;&#24847;&#35782;&#65292;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#33021;&#21147;&#65292;&#23545;&#22810;&#36339;&#38382;&#39064;&#25968;&#25454;&#38598;MQuaKE&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2401.10471</link><description>&lt;p&gt;
DeepEdit: &#24102;&#26377;&#32422;&#26463;&#30340;&#35299;&#30721;&#24335;&#30693;&#35782;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
DeepEdit: Knowledge Editing as Decoding with Constraints. (arXiv:2401.10471v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10471
&lt;/p&gt;
&lt;p&gt;
DeepEdit&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#22909;&#30340;&#25512;&#29702;&#19968;&#33268;&#24615;&#21644;&#23545;&#26356;&#26032;&#30693;&#35782;&#30340;&#24847;&#35782;&#65292;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#33021;&#21147;&#65292;&#23545;&#22810;&#36339;&#38382;&#39064;&#25968;&#25454;&#38598;MQuaKE&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30693;&#35782;&#32534;&#36753;&#35270;&#20026;&#24102;&#26377;&#32422;&#26463;&#30340;&#35299;&#30721;&#36807;&#31243;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DeepEdit&#65288;&#22522;&#20110;&#28145;&#24230;&#20248;&#20808;&#25628;&#32034;&#30340;&#28176;&#36827;&#24335;&#35299;&#30721;&#30693;&#35782;&#32534;&#36753;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#22909;&#30340;&#25512;&#29702;&#19968;&#33268;&#24615;&#12289;&#38382;&#39064;&#30456;&#20851;&#24615;&#21644;&#23545;&#26356;&#26032;&#30693;&#35782;&#30340;&#24847;&#35782;&#26469;&#25913;&#36827;&#30693;&#35782;&#32534;&#36753;&#12290;DeepEdit&#21487;&#28789;&#27963;&#24212;&#29992;&#20110;&#25152;&#26377;&#40657;&#30418;LLMs&#65306;&#19981;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#12289;&#34920;&#31034;&#25110;&#36755;&#20986;&#35789;&#27719;&#20998;&#24067;&#12290;DeepEdit&#36880;&#27493;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#25512;&#29702;&#27493;&#39588;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#30693;&#35782;&#32534;&#36753;&#12290;&#23427;&#21033;&#29992;&#28145;&#24230;&#20248;&#20808;&#25628;&#32034;&#26469;&#20462;&#25913;LLMs&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#25552;&#39640;&#36755;&#20986;&#23545;&#38382;&#39064;&#30340;&#30456;&#20851;&#24615;&#21644;&#23545;&#26356;&#26032;&#30693;&#35782;&#30340;&#24847;&#35782;&#12290;&#22312;&#30693;&#35782;&#32534;&#36753;&#26041;&#38754;&#65292;DeepEdit&#22312;&#25511;&#21046;LLMs&#20135;&#29983;&#26356;&#31616;&#27905;&#30340;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#22312;MQuaKE&#19978;&#65292;DeepEdit&#22312;&#23450;&#37327;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#36339;&#38382;&#39064;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a new perspective of knowledge editing for large language models (LLMs) as decoding with constraints. We propose DeepEdit (Depth-first Search based Progressive Decoding for Knowledge Editing), a neuro-symbolic method that improves knowledge editing with better coherence of reasoning, relevance to the question, and awareness of updated knowledge. DeepEdit can be flexibly applied to all black-box LLMs: it does not require any access to the model parameters, representations, or output vocabulary distributions. DeepEdit progressively produces the high-quality reasoning steps towards effective knowledge editing. It utilizes a depth-first search to revise the LLMs' output, which improves the output's informativeness to the input question and awareness of the updated knowledge. Qualitatively, DeepEdit effectively controls LLMs to produce more succinct reasoning in accord with knowledge editing. Quantitatively, DeepEdit yields significant gains on MQuaKE, a challenging multi-hop que
&lt;/p&gt;</description></item><item><title>DiffDA&#26159;&#19968;&#31181;&#29992;&#20110;&#27668;&#35937;&#23610;&#24230;&#25968;&#25454;&#21516;&#21270;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#23558;&#39044;&#27979;&#29366;&#24577;&#21644;&#31232;&#30095;&#35266;&#27979;&#21516;&#21270;&#65292;&#29983;&#25104;&#19982;&#35266;&#27979;&#19968;&#33268;&#30340;&#21021;&#22987;&#26465;&#20214;&#65292;&#24182;&#33021;&#23545;&#39044;&#27979;&#36827;&#34892;&#21518;&#22788;&#29702;&#21040;&#26410;&#26469;&#12290;</title><link>http://arxiv.org/abs/2401.05932</link><description>&lt;p&gt;
DiffDA:&#19968;&#31181;&#29992;&#20110;&#27668;&#35937;&#23610;&#24230;&#25968;&#25454;&#21516;&#21270;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DiffDA: a diffusion model for weather-scale data assimilation. (arXiv:2401.05932v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05932
&lt;/p&gt;
&lt;p&gt;
DiffDA&#26159;&#19968;&#31181;&#29992;&#20110;&#27668;&#35937;&#23610;&#24230;&#25968;&#25454;&#21516;&#21270;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#23558;&#39044;&#27979;&#29366;&#24577;&#21644;&#31232;&#30095;&#35266;&#27979;&#21516;&#21270;&#65292;&#29983;&#25104;&#19982;&#35266;&#27979;&#19968;&#33268;&#30340;&#21021;&#22987;&#26465;&#20214;&#65292;&#24182;&#33021;&#23545;&#39044;&#27979;&#36827;&#34892;&#21518;&#22788;&#29702;&#21040;&#26410;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20934;&#30830;&#30340;&#25968;&#25454;&#21516;&#21270;&#29983;&#25104;&#21021;&#22987;&#26465;&#20214;&#23545;&#20110;&#21487;&#38752;&#30340;&#22825;&#27668;&#39044;&#25253;&#21644;&#27668;&#20505;&#27169;&#25311;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DiffDA&#20316;&#20026;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#25454;&#21516;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#39044;&#27979;&#29366;&#24577;&#21644;&#31232;&#30095;&#35266;&#27979;&#26469;&#21516;&#21270;&#22823;&#27668;&#21464;&#37327;&#12290;&#25105;&#20204;&#23558;&#39044;&#35757;&#32451;&#30340;GraphCast&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#20316;&#20026;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20102;&#20004;&#38454;&#27573;&#26465;&#20214;&#65306;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#23545;&#39044;&#27979;&#29366;&#24577;&#36827;&#34892;&#26465;&#20214;&#21270;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21482;&#23545;&#31232;&#30095;&#35266;&#27979;&#36827;&#34892;&#26465;&#20214;&#21270;&#12290;&#20316;&#20026;&#21103;&#20135;&#21697;&#65292;&#36825;&#31181;&#31574;&#30053;&#36824;&#33021;&#23558;&#39044;&#27979;&#21518;&#22788;&#29702;&#21040;&#26410;&#26469;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#27809;&#26377;&#21487;&#29992;&#30340;&#35266;&#27979;&#25968;&#25454;&#12290;&#36890;&#36807;&#22522;&#20110;&#20877;&#20998;&#26512;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20197;0.25&#24230;&#20998;&#36776;&#29575;&#29983;&#25104;&#19982;&#35266;&#27979;&#19968;&#33268;&#30340;&#21516;&#21270;&#20840;&#29699;&#22823;&#27668;&#25968;&#25454;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#29983;&#25104;&#30340;&#21021;&#22987;&#26465;&#20214;&#21487;&#20197;&#29992;&#20110;&#20855;&#26377;&#36739;&#23567;&#25439;&#22833;&#30340;&#39044;&#25253;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generation of initial conditions via accurate data assimilation is crucial for reliable weather forecasting and climate modeling. We propose the DiffDA as a machine learning based data assimilation method capable of assimilating atmospheric variables using predicted states and sparse observations. We adapt the pretrained GraphCast weather forecast model as a denoising diffusion model. Our method applies two-phase conditioning: on the predicted state during both training and inference, and on sparse observations during inference only. As a byproduct, this strategy also enables the post-processing of predictions into the future, for which no observations are available.Through experiments based on a reanalysis dataset, we have verified that our method can produce assimilated global atmospheric data consistent with observations at 0.25degree resolution. The experiments also show that the initial conditions that are generated via our approach can be used for forecast models with a loss 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31227;&#21160;&#32593;&#26684;PDE&#30340;&#31227;&#21160;&#37319;&#26679;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(MMPDE-Net)&#65292;&#36890;&#36807;&#35299;&#20915;&#31227;&#21160;&#32593;&#26684;PDE&#26469;&#33258;&#36866;&#24212;&#29983;&#25104;&#26032;&#30340;&#37319;&#26679;&#28857;&#65292;&#24182;&#19988;&#32467;&#21512;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#25552;&#20986;&#20102;&#31227;&#21160;&#37319;&#26679;PINN&#65288;MS-PINN&#65289;&#30340;&#26694;&#26550;&#12290;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;MS-PINN&#30456;&#23545;&#20110;PINN&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2311.16167</link><description>&lt;p&gt;
&#22522;&#20110;&#31227;&#21160;&#32593;&#26684;PDE&#30340;&#31227;&#21160;&#37319;&#26679;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Moving Sampling Physics-informed Neural Networks induced by Moving Mesh PDE. (arXiv:2311.16167v2 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.16167
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31227;&#21160;&#32593;&#26684;PDE&#30340;&#31227;&#21160;&#37319;&#26679;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(MMPDE-Net)&#65292;&#36890;&#36807;&#35299;&#20915;&#31227;&#21160;&#32593;&#26684;PDE&#26469;&#33258;&#36866;&#24212;&#29983;&#25104;&#26032;&#30340;&#37319;&#26679;&#28857;&#65292;&#24182;&#19988;&#32467;&#21512;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#25552;&#20986;&#20102;&#31227;&#21160;&#37319;&#26679;PINN&#65288;MS-PINN&#65289;&#30340;&#26694;&#26550;&#12290;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;MS-PINN&#30456;&#23545;&#20110;PINN&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31227;&#21160;&#32593;&#26684;&#26041;&#27861;&#30340;&#31471;&#21040;&#31471;&#33258;&#36866;&#24212;&#37319;&#26679;&#31070;&#32463;&#32593;&#32476;&#65288;MMPDE-Net&#65289;&#65292;&#36890;&#36807;&#27714;&#35299;&#31227;&#21160;&#32593;&#26684;PDE&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#29983;&#25104;&#26032;&#30340;&#37319;&#26679;&#28857;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#25913;&#21892;&#37319;&#26679;&#28857;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22522;&#20110;MMPDE-Net&#24320;&#21457;&#20102;&#19968;&#31181;&#36845;&#20195;&#31639;&#27861;&#65292;&#20351;&#24471;&#37319;&#26679;&#28857;&#26356;&#21152;&#31934;&#30830;&#21644;&#21487;&#25511;&#12290;&#30001;&#20110;MMPDE-Net&#26159;&#29420;&#31435;&#20110;&#28145;&#24230;&#23398;&#20064;&#27714;&#35299;&#22120;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#31227;&#21160;&#37319;&#26679;PINN&#65288;MS-PINN&#65289;&#65292;&#24182;&#22312;&#19968;&#20123;&#20551;&#35774;&#19979;&#36890;&#36807;&#35823;&#24046;&#20998;&#26512;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22235;&#20010;&#20856;&#22411;&#23454;&#20363;&#30340;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;MS-PINN&#30456;&#23545;&#20110;PINN&#30340;&#24615;&#33021;&#25913;&#21892;&#65292;&#20174;&#32780;&#25968;&#20540;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose an end-to-end adaptive sampling neural network (MMPDE-Net) based on the moving mesh method, which can adaptively generate new sampling points by solving the moving mesh PDE. This model focuses on improving the quality of sampling points generation. Moreover, we develop an iterative algorithm based on MMPDE-Net, which makes the sampling points more precise and controllable. Since MMPDE-Net is a framework independent of the deep learning solver, we combine it with physics-informed neural networks (PINN) to propose moving sampling PINN (MS-PINN) and demonstrate its effectiveness by error analysis under some assumptions. Finally, we demonstrate the performance improvement of MS-PINN compared to PINN through numerical experiments of four typical examples, which numerically verify the effectiveness of our method.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26368;&#23567;&#21270;&#21407;&#22987;Wasserstein&#36317;&#31163;&#26469;&#21305;&#37197;&#19987;&#23478;&#21644;&#23398;&#20064;&#32773;&#29366;&#24577;&#21344;&#29992;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#31163;&#32447;&#23398;&#20064;&#20174;&#35266;&#23519;&#20013;&#27169;&#20223;&#20219;&#21153;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.01331</link><description>&lt;p&gt;
&#36890;&#36807;&#21407;&#22987;Wasserstein&#29366;&#24577;&#21344;&#29992;&#21305;&#37197;&#23454;&#29616;&#30340;&#31163;&#32447;&#35266;&#23519;&#27169;&#20223;
&lt;/p&gt;
&lt;p&gt;
Offline Imitation from Observation via Primal Wasserstein State Occupancy Matching. (arXiv:2311.01331v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26368;&#23567;&#21270;&#21407;&#22987;Wasserstein&#36317;&#31163;&#26469;&#21305;&#37197;&#19987;&#23478;&#21644;&#23398;&#20064;&#32773;&#29366;&#24577;&#21344;&#29992;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#31163;&#32447;&#23398;&#20064;&#20174;&#35266;&#23519;&#20013;&#27169;&#20223;&#20219;&#21153;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24773;&#22659;&#20013;&#65292;&#19982;&#29615;&#22659;&#30340;&#20219;&#24847;&#20132;&#20114;&#24448;&#24448;&#26159;&#26114;&#36149;&#30340;&#65292;&#24182;&#19988;&#19987;&#23478;&#31034;&#33539;&#30340;&#34892;&#20026;&#24182;&#19981;&#24635;&#26159;&#21487;&#29992;&#30340;&#12290;&#20026;&#20102;&#20943;&#23569;&#36825;&#20004;&#32773;&#30340;&#38656;&#27714;&#65292;&#31163;&#32447;&#23398;&#20064;&#20174;&#35266;&#23519;&#65288;LfO&#65289;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20854;&#20013;&#20195;&#29702;&#36890;&#36807;&#21482;&#26377;&#19987;&#23478;&#29366;&#24577;&#21644;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#38750;&#19987;&#23478;&#29366;&#24577;-&#21160;&#20316;&#23545;&#26469;&#23398;&#20064;&#35299;&#20915;&#20219;&#21153;&#12290;&#26368;&#20808;&#36827;&#30340;&#20998;&#24067;&#26657;&#27491;&#20272;&#35745;&#65288;DICE&#65289;&#26041;&#27861;&#26368;&#23567;&#21270;&#20102;&#23398;&#20064;&#32773;&#21644;&#19987;&#23478;&#31574;&#30053;&#20043;&#38388;&#30340;&#29366;&#24577;&#21344;&#29992;&#24046;&#24322;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20165;&#38480;&#20110;$f$-divergences&#65288;KL&#21644;$\chi^2$&#65289;&#25110;&#24102;&#26377;Rubinstein&#23545;&#20598;&#30340;Wasserstein&#36317;&#31163;&#65292;&#21518;&#32773;&#38480;&#21046;&#20102;&#23545;&#24615;&#33021;&#20851;&#38190;&#30340;&#22522;&#30784;&#36317;&#31163;&#24230;&#37327;&#30340;&#20351;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21407;&#22987;Wasserstein DICE&#65288;PW-DICE&#65289;&#65292;&#23427;&#36890;&#36807;&#24754;&#35266;&#27491;&#21017;&#21270;&#22120;&#26368;&#23567;&#21270;&#19987;&#23478;&#21644;&#23398;&#20064;&#32773;&#29366;&#24577;&#21344;&#29992;&#20043;&#38388;&#30340;&#21407;&#22987;Wasserstein&#36317;&#31163;&#65292;&#24182;&#21033;&#29992;&#20102;&#23545;&#27604;&#23398;&#20064;&#30340;dis
&lt;/p&gt;
&lt;p&gt;
In real-world scenarios, arbitrary interactions with the environment can often be costly, and actions of expert demonstrations are not always available. To reduce the need for both, Offline Learning from Observations (LfO) is extensively studied, where the agent learns to solve a task with only expert states and \textit{task-agnostic} non-expert state-action pairs. The state-of-the-art DIstribution Correction Estimation (DICE) methods minimize the state occupancy divergence between the learner and expert policies. However, they are limited to either $f$-divergences (KL and $\chi^2$) or Wasserstein distance with Rubinstein duality, the latter of which constrains the underlying distance metric crucial to the performance of Wasserstein-based solutions. To address this problem, we propose Primal Wasserstein DICE (PW-DICE), which minimizes the primal Wasserstein distance between the expert and learner state occupancies with a pessimistic regularizer and leverages a contrastively learned dis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20026;&#24320;&#25918;&#24335;&#23398;&#20064;&#38382;&#39064;&#23450;&#20041;&#20102;&#19968;&#20010;&#20851;&#38190;&#30340;&#22522;&#26412;&#23646;&#24615;&#65292;&#21363;&#26080;&#38480;&#26102;&#38388;&#20869;&#19981;&#26029;&#20135;&#29983;&#26032;&#20803;&#32032;&#12290;&#22312;&#36825;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#24320;&#25918;&#24335;&#23398;&#20064;&#38382;&#39064;&#30340;&#27010;&#24565;&#65292;&#24182;&#30528;&#37325;&#30740;&#31350;&#20102;&#24320;&#25918;&#24335;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#30340;&#23376;&#38598;&#12290;</title><link>http://arxiv.org/abs/2311.00344</link><description>&lt;p&gt;
&#20026;&#30446;&#26631;&#26465;&#20214;&#26234;&#33021;&#20307;&#23450;&#20041;&#24320;&#25918;&#24335;&#23398;&#20064;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
A Definition of Open-Ended Learning Problems for Goal-Conditioned Agents. (arXiv:2311.00344v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20026;&#24320;&#25918;&#24335;&#23398;&#20064;&#38382;&#39064;&#23450;&#20041;&#20102;&#19968;&#20010;&#20851;&#38190;&#30340;&#22522;&#26412;&#23646;&#24615;&#65292;&#21363;&#26080;&#38480;&#26102;&#38388;&#20869;&#19981;&#26029;&#20135;&#29983;&#26032;&#20803;&#32032;&#12290;&#22312;&#36825;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#24320;&#25918;&#24335;&#23398;&#20064;&#38382;&#39064;&#30340;&#27010;&#24565;&#65292;&#24182;&#30528;&#37325;&#30740;&#31350;&#20102;&#24320;&#25918;&#24335;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#30340;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#35770;&#25991;&#20013;&#37117;&#25552;&#21040;&#20102;&#8220;&#24320;&#25918;&#24335;&#23398;&#20064;&#8221;&#65292;&#20294;&#24456;&#23569;&#26377;&#20154;&#23581;&#35797;&#23450;&#20041;&#36825;&#20010;&#26415;&#35821;&#12290;&#26356;&#31967;&#31957;&#30340;&#26159;&#65292;&#24403;&#20180;&#32454;&#30740;&#31350;&#26102;&#65292;&#20284;&#20046;&#23545;&#20110;&#24320;&#25918;&#24335;&#23398;&#20064;&#19982;&#36830;&#32493;&#23398;&#20064;&#12289;&#32456;&#36523;&#23398;&#20064;&#25110;&#33258;&#20026;&#30446;&#30340;&#23398;&#20064;&#31561;&#30456;&#20851;&#27010;&#24565;&#30340;&#21306;&#21035;&#27809;&#26377;&#20849;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#33268;&#21147;&#20110;&#35299;&#20915;&#36825;&#31181;&#24773;&#20917;&#12290;&#36890;&#36807;&#38416;&#36848;&#36825;&#20010;&#27010;&#24565;&#30340;&#36215;&#28304;&#21644;&#26368;&#36817;&#30340;&#35266;&#28857;&#65292;&#25105;&#20204;&#35828;&#26126;&#20102;&#24320;&#25918;&#24335;&#23398;&#20064;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#21253;&#21547;&#22810;&#31181;&#23646;&#24615;&#30340;&#22797;&#21512;&#27010;&#24565;&#12290;&#19982;&#36825;&#20123;&#20043;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#24320;&#25918;&#24335;&#36807;&#31243;&#30340;&#19968;&#20010;&#20851;&#38190;&#22522;&#26412;&#23646;&#24615;&#19982;&#26102;&#38388;&#26080;&#38480;&#21046;&#22320;&#20135;&#29983;&#26032;&#20803;&#32032;&#30456;&#20998;&#31163;&#30340;&#24819;&#27861;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#24320;&#25918;&#24335;&#23398;&#20064;&#38382;&#39064;&#30340;&#27010;&#24565;&#65292;&#24182;&#29305;&#21035;&#20851;&#27880;&#20102;&#24320;&#25918;&#24335;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#30340;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
A lot of recent machine learning research papers have "Open-ended learning" in their title. But very few of them attempt to define what they mean when using the term. Even worse, when looking more closely there seems to be no consensus on what distinguishes open-ended learning from related concepts such as continual learning, lifelong learning or autotelic learning. In this paper, we contribute to fixing this situation. After illustrating the genealogy of the concept and more recent perspectives about what it truly means, we outline that open-ended learning is generally conceived as a composite notion encompassing a set of diverse properties. In contrast with these previous approaches, we propose to isolate a key elementary property of open-ended processes, which is to always produce novel elements from time to time over an infinite horizon. From there, we build the notion of open-ended learning problems and focus in particular on the subset of open-ended goal-conditioned reinforcement
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#19978;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20146;&#21644;&#24230;&#35780;&#20998;&#36861;&#36394;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38750;&#32447;&#24615;&#20256;&#25773;&#65292;&#23588;&#20854;&#20851;&#27880;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#21644;&#23545;&#24191;&#27867;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.11439</link><description>&lt;p&gt;
&#36890;&#36807;&#38750;&#32447;&#24615;&#30740;&#31350;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Understanding deep neural networks through the lens of their non-linearity. (arXiv:2310.11439v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#19978;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20146;&#21644;&#24230;&#35780;&#20998;&#36861;&#36394;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38750;&#32447;&#24615;&#20256;&#25773;&#65292;&#23588;&#20854;&#20851;&#27880;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#21644;&#23545;&#24191;&#27867;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#30340;&#26174;&#33879;&#25104;&#21151;&#24120;&#24120;&#24402;&#22240;&#20110;&#23427;&#20204;&#30340;&#39640;&#34920;&#36798;&#33021;&#21147;&#21644;&#36817;&#20284;&#20219;&#24847;&#22797;&#26434;&#20989;&#25968;&#30340;&#33021;&#21147;&#12290;&#20107;&#23454;&#19978;&#65292;DNN&#26159;&#39640;&#24230;&#38750;&#32447;&#24615;&#30340;&#27169;&#22411;&#65292;&#20854;&#20013;&#24341;&#20837;&#30340;&#28608;&#27963;&#20989;&#25968;&#22312;&#20854;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#35768;&#22810;&#30740;&#31350;&#36890;&#36807;&#36817;&#20284;&#33021;&#21147;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;DNN&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20294;&#37327;&#21270;DNN&#25110;&#20010;&#21035;&#28608;&#27963;&#20989;&#25968;&#30340;&#38750;&#32447;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22312;&#20855;&#20307;&#20851;&#27880;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#20013;&#36861;&#36394;&#38750;&#32447;&#24615;&#20256;&#25773;&#30340;&#29702;&#35770;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#20146;&#21644;&#24230;&#35780;&#20998;&#20801;&#35768;&#25105;&#20204;&#28145;&#20837;&#20102;&#35299;&#21508;&#31181;&#19981;&#21516;&#20307;&#31995;&#32467;&#26500;&#21644;&#23398;&#20064;&#33539;&#24335;&#30340;&#20869;&#37096;&#24037;&#20316;&#21407;&#29702;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#31361;&#20986;&#20102;&#25152;&#25552;&#20986;&#30340;&#20146;&#21644;&#24230;&#35780;&#20998;&#30340;&#23454;&#38469;&#25928;&#29992;&#21644;&#28508;&#22312;&#24212;&#29992;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The remarkable success of deep neural networks (DNN) is often attributed to their high expressive power and their ability to approximate functions of arbitrary complexity. Indeed, DNNs are highly non-linear models, and activation functions introduced into them are largely responsible for this. While many works studied the expressive power of DNNs through the lens of their approximation capabilities, quantifying the non-linearity of DNNs or of individual activation functions remains an open problem. In this paper, we propose the first theoretically sound solution to track non-linearity propagation in deep neural networks with a specific focus on computer vision applications. Our proposed affinity score allows us to gain insights into the inner workings of a wide range of different architectures and learning paradigms. We provide extensive experimental results that highlight the practical utility of the proposed affinity score and its potential for long-reaching applications.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#22312;&#25915;&#20987;&#24615;&#20869;&#23481;&#25913;&#20889;&#26041;&#38754;&#24341;&#20837;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26377;&#38480;&#25968;&#37327;&#30340;&#36755;&#20837;-&#26631;&#31614;&#28436;&#31034;&#23545;&#26469;&#25351;&#23548;&#27169;&#22411;&#29983;&#25104;&#29305;&#23450;&#26597;&#35810;&#30340;&#25152;&#38656;&#36755;&#20986;&#65292;&#20174;&#32780;&#25552;&#39640;&#21487;&#29992;&#24615;&#21644;&#20943;&#23569;&#25915;&#20987;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.10707</link><description>&lt;p&gt;
&#28436;&#31034;&#23601;&#26159;&#20320;&#38656;&#35201;&#30340;&#19968;&#20999;&#65306;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#25512;&#36827;&#25915;&#20987;&#24615;&#20869;&#23481;&#25913;&#20889;
&lt;/p&gt;
&lt;p&gt;
Demonstrations Are All You Need: Advancing Offensive Content Paraphrasing using In-Context Learning. (arXiv:2310.10707v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10707
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#22312;&#25915;&#20987;&#24615;&#20869;&#23481;&#25913;&#20889;&#26041;&#38754;&#24341;&#20837;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26377;&#38480;&#25968;&#37327;&#30340;&#36755;&#20837;-&#26631;&#31614;&#28436;&#31034;&#23545;&#26469;&#25351;&#23548;&#27169;&#22411;&#29983;&#25104;&#29305;&#23450;&#26597;&#35810;&#30340;&#25152;&#38656;&#36755;&#20986;&#65292;&#20174;&#32780;&#25552;&#39640;&#21487;&#29992;&#24615;&#21644;&#20943;&#23569;&#25915;&#20987;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25913;&#20889;&#25915;&#20987;&#24615;&#20869;&#23481;&#26159;&#19968;&#31181;&#26356;&#22909;&#30340;&#26367;&#20195;&#20869;&#23481;&#21024;&#38500;&#30340;&#26041;&#27861;&#65292;&#26377;&#21161;&#20110;&#25913;&#21892;&#27807;&#36890;&#29615;&#22659;&#30340;&#25991;&#26126;&#31243;&#24230;&#12290;&#28982;&#32780;&#65292;&#30417;&#30563;&#24335;&#30340;&#25913;&#20889;&#22120;&#22312;&#20445;&#30041;&#24847;&#20041;&#21644;&#24847;&#22270;&#30340;&#21516;&#26102;&#65292;&#23545;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#20381;&#36182;&#24615;&#36739;&#39640;&#12290;&#23427;&#20204;&#20063;&#20445;&#30041;&#20102;&#21407;&#22987;&#20869;&#23481;&#30340;&#22823;&#37096;&#20998;&#25915;&#20987;&#24615;&#65292;&#36825;&#24341;&#21457;&#20102;&#23545;&#23427;&#20204;&#25972;&#20307;&#21487;&#29992;&#24615;&#30340;&#30097;&#38382;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#25506;&#32034;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30456;&#32467;&#21512;&#65292;&#24110;&#21161;&#20174;&#19994;&#32773;&#24320;&#21457;&#21487;&#29992;&#30340;&#25913;&#20889;&#22120;&#65292;&#21363;&#20351;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#36755;&#20837;-&#26631;&#31614;&#28436;&#31034;&#23545;&#26469;&#24341;&#23548;&#27169;&#22411;&#29983;&#25104;&#29305;&#23450;&#26597;&#35810;&#30340;&#25152;&#38656;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20851;&#38190;&#22240;&#32032;&#65292;&#22914;&#28436;&#31034;&#30340;&#25968;&#37327;&#21644;&#39034;&#24207;&#65292;&#25490;&#38500;&#25552;&#31034;&#25351;&#20196;&#65292;&#20197;&#21450;&#38477;&#20302;&#27979;&#37327;&#27602;&#24615;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;&#25105;&#20204;&#25552;&#20986;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#31036;&#35980;&#25913;&#20889;&#25968;&#25454;&#38598;&#22312;&#20869;&#30340;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#21407;&#21017;&#24615;&#35780;&#20272;&#65292;&#20854;&#20013;&#21253;&#25324;&#23545;&#35805;&#24335;&#30340;&#31895;&#40065;&#21457;&#35328;&#12289;&#31036;&#35980;&#25913;&#20889;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Paraphrasing of offensive content is a better alternative to content removal and helps improve civility in a communication environment. Supervised paraphrasers; however, rely heavily on large quantities of labelled data to help preserve meaning and intent. They also retain a large portion of the offensiveness of the original content, which raises questions on their overall usability. In this paper we aim to assist practitioners in developing usable paraphrasers by exploring In-Context Learning (ICL) with large language models (LLMs), i.e., using a limited number of input-label demonstration pairs to guide the model in generating desired outputs for specific queries. Our study focuses on key factors such as -- number and order of demonstrations, exclusion of prompt instruction, and reduction in measured toxicity. We perform principled evaluation on three datasets, including our proposed Context-Aware Polite Paraphrase dataset, comprising of dialogue-style rude utterances, polite paraphr
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#31038;&#21306;&#25104;&#21592;&#38544;&#34255;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#25112;&#30053;&#22320;&#25913;&#21464;&#32593;&#32476;&#22270;&#30340;&#32467;&#26500;&#23646;&#24615;&#65292;&#38450;&#27490;&#33410;&#28857;&#34987;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#35782;&#21035;&#20986;&#26469;&#65292;&#24182;&#39564;&#35777;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08909</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23558;&#31038;&#21306;&#25104;&#21592;&#38544;&#34255;&#20316;&#20026;&#21453;&#20107;&#23454;&#22270;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Community Membership Hiding as Counterfactual Graph Search via Deep Reinforcement Learning. (arXiv:2310.08909v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08909
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#31038;&#21306;&#25104;&#21592;&#38544;&#34255;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#25112;&#30053;&#22320;&#25913;&#21464;&#32593;&#32476;&#22270;&#30340;&#32467;&#26500;&#23646;&#24615;&#65292;&#38450;&#27490;&#33410;&#28857;&#34987;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#35782;&#21035;&#20986;&#26469;&#65292;&#24182;&#39564;&#35777;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#21306;&#26816;&#27979;&#26159;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#21457;&#29616;&#24444;&#27492;&#32039;&#23494;&#32852;&#31995;&#30340;&#29992;&#25143;&#32676;&#20307;&#30340;&#26377;&#29992;&#24037;&#20855;&#65292;&#20182;&#20204;&#20849;&#20139;&#20849;&#21516;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#21151;&#33021;&#24448;&#24448;&#20250;&#20197;&#21487;&#33021;&#26292;&#38706;&#20010;&#20154;&#38544;&#31169;&#20026;&#20195;&#20215;&#65292;&#26080;&#24847;&#20013;&#36879;&#38706;&#20182;&#20204;&#30340;&#21697;&#21619;&#25110;&#20559;&#22909;&#12290;&#22240;&#27492;&#65292;&#19968;&#20123;&#29992;&#25143;&#21487;&#33021;&#24076;&#26395;&#20445;&#25252;&#20182;&#20204;&#30340;&#21311;&#21517;&#24615;&#65292;&#24182;&#20986;&#20110;&#21508;&#31181;&#21407;&#22240;&#36873;&#25321;&#36864;&#20986;&#31038;&#21306;&#26816;&#27979;&#65292;&#20363;&#22914;&#19982;&#25919;&#27835;&#25110;&#23447;&#25945;&#32452;&#32455;&#30340;&#20851;&#32852;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#31038;&#21306;&#25104;&#21592;&#38544;&#34255;&#30340;&#25361;&#25112;&#65292;&#23427;&#28041;&#21450;&#25112;&#30053;&#24615;&#22320;&#25913;&#21464;&#32593;&#32476;&#22270;&#30340;&#32467;&#26500;&#23646;&#24615;&#65292;&#20197;&#38450;&#27490;&#19968;&#20010;&#25110;&#22810;&#20010;&#33410;&#28857;&#34987;&#32473;&#23450;&#30340;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#35782;&#21035;&#20986;&#26469;&#12290;&#25105;&#20204;&#36890;&#36807;&#21046;&#23450;&#19968;&#20010;&#21463;&#38480;&#30340;&#21453;&#20107;&#23454;&#22270;&#30446;&#26631;&#65292;&#24182;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#26469;&#39564;&#35777;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65306;&#33410;&#28857;&#21644;&#31038;&#21306;&#27450;&#39575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Community detection techniques are useful tools for social media platforms to discover tightly connected groups of users who share common interests. However, this functionality often comes at the expense of potentially exposing individuals to privacy breaches by inadvertently revealing their tastes or preferences. Therefore, some users may wish to safeguard their anonymity and opt out of community detection for various reasons, such as affiliation with political or religious organizations.  In this study, we address the challenge of community membership hiding, which involves strategically altering the structural properties of a network graph to prevent one or more nodes from being identified by a given community detection algorithm. We tackle this problem by formulating it as a constrained counterfactual graph objective, and we solve it via deep reinforcement learning. We validate the effectiveness of our method through two distinct tasks: node and community deception. Extensive exper
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#36870;&#24378;&#21270;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#32456;&#31471;&#29992;&#25143;&#36807;&#21435;&#30340;&#28040;&#36153;&#25968;&#25454;&#65292;&#24110;&#21161;&#21019;&#24314;&#29992;&#25143;&#27599;&#26085;&#30340;&#30005;&#22120;&#35745;&#21010;&#65292;&#20174;&#32780;&#40723;&#21169;&#32456;&#31471;&#29992;&#25143;&#21442;&#19982;&#38656;&#27714;&#21709;&#24212;&#26381;&#21153;&#30340;&#25552;&#20379;&#12290;</title><link>http://arxiv.org/abs/2310.07389</link><description>&lt;p&gt;
&#23398;&#20064;&#29992;&#25143;&#20248;&#20808;&#30340;&#30005;&#22120;&#35843;&#24230;&#22870;&#21169;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Learning a Reward Function for User-Preferred Appliance Scheduling. (arXiv:2310.07389v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#36870;&#24378;&#21270;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#32456;&#31471;&#29992;&#25143;&#36807;&#21435;&#30340;&#28040;&#36153;&#25968;&#25454;&#65292;&#24110;&#21161;&#21019;&#24314;&#29992;&#25143;&#27599;&#26085;&#30340;&#30005;&#22120;&#35745;&#21010;&#65292;&#20174;&#32780;&#40723;&#21169;&#32456;&#31471;&#29992;&#25143;&#21442;&#19982;&#38656;&#27714;&#21709;&#24212;&#26381;&#21153;&#30340;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23621;&#27665;&#37096;&#38376;&#21152;&#36895;&#21457;&#23637;&#38656;&#27714;&#21709;&#24212;&#26381;&#21153;&#23545;&#20110;&#38477;&#20302;&#30005;&#21147;&#37096;&#38376;&#30340;&#30899;&#25490;&#25918;&#33267;&#20851;&#37325;&#35201;&#12290;&#38543;&#30528;&#22522;&#30784;&#35774;&#26045;&#30340;&#36827;&#27493;&#65292;&#40723;&#21169;&#32456;&#31471;&#29992;&#25143;&#21442;&#19982;&#33267;&#20851;&#37325;&#35201;&#12290;&#32456;&#31471;&#29992;&#25143;&#38750;&#24120;&#37325;&#35270;&#33258;&#24049;&#30340;&#38544;&#31169;&#21644;&#25511;&#21046;&#26435;&#65292;&#24182;&#24076;&#26395;&#22312;&#21019;&#24314;&#27599;&#26085;&#30005;&#22120;&#25805;&#20316;&#35745;&#21010;&#26102;&#21442;&#19982;&#21040;&#26381;&#21153;&#35774;&#35745;&#21644;&#20915;&#31574;&#36807;&#31243;&#20013;&#12290;&#27492;&#22806;&#65292;&#38500;&#38750;&#20182;&#20204;&#26377;&#32463;&#27982;&#25110;&#29615;&#22659;&#21160;&#26426;&#65292;&#20182;&#20204;&#36890;&#24120;&#19981;&#20250;&#20934;&#22791;&#29306;&#29298;&#33258;&#24049;&#30340;&#33298;&#36866;&#24230;&#26469;&#24110;&#21161;&#24179;&#34913;&#30005;&#21147;&#31995;&#32479;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36870;&#24378;&#21270;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#19981;&#35201;&#27714;&#29992;&#25143;&#26126;&#30830;&#38472;&#36848;&#38656;&#27714;&#21644;&#24895;&#26395;&#30340;&#24773;&#20917;&#19979;&#24110;&#21161;&#21019;&#24314;&#29992;&#25143;&#27599;&#26085;&#30340;&#30005;&#22120;&#35745;&#21010;&#12290;&#36890;&#36807;&#20351;&#29992;&#20182;&#20204;&#36807;&#21435;&#30340;&#28040;&#36153;&#25968;&#25454;&#65292;&#32456;&#31471;&#28040;&#36153;&#32773;&#23558;&#38544;&#24335;&#21442;&#19982;&#36825;&#20123;&#20915;&#31574;&#30340;&#21046;&#23450;&#65292;&#24182;&#22240;&#27492;&#24471;&#21040;&#32487;&#32493;&#21442;&#19982;&#38656;&#27714;&#21709;&#24212;&#26381;&#21153;&#25552;&#20379;&#30340;&#21160;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accelerated development of demand response service provision by the residential sector is crucial for reducing carbon-emissions in the power sector. Along with the infrastructure advancement, encouraging the end users to participate is crucial. End users highly value their privacy and control, and want to be included in the service design and decision-making process when creating the daily appliance operation schedules. Furthermore, unless they are financially or environmentally motivated, they are generally not prepared to sacrifice their comfort to help balance the power system. In this paper, we present an inverse-reinforcement-learning-based model that helps create the end users' daily appliance schedules without asking them to explicitly state their needs and wishes. By using their past consumption data, the end consumers will implicitly participate in the creation of those decisions and will thus be motivated to continue participating in the provision of demand response services.
&lt;/p&gt;</description></item><item><title>&#22312;&#32447;&#25512;&#27979;&#35299;&#30721;&#26159;&#36890;&#36807;&#21033;&#29992;&#22810;&#20313;&#35745;&#31639;&#33021;&#21147;&#65292;&#22312;LLM&#26381;&#21153;&#38598;&#32676;&#20013;&#25345;&#32493;&#26356;&#26032;&#33609;&#31295;&#27169;&#22411;&#65292;&#20174;&#32780;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.07177</link><description>&lt;p&gt;
&#22312;&#32447;&#25512;&#27979;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Online Speculative Decoding. (arXiv:2310.07177v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07177
&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#25512;&#27979;&#35299;&#30721;&#26159;&#36890;&#36807;&#21033;&#29992;&#22810;&#20313;&#35745;&#31639;&#33021;&#21147;&#65292;&#22312;LLM&#26381;&#21153;&#38598;&#32676;&#20013;&#25345;&#32493;&#26356;&#26032;&#33609;&#31295;&#27169;&#22411;&#65292;&#20174;&#32780;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#27979;&#35299;&#30721;&#26159;&#36890;&#36807;&#21033;&#29992;&#36739;&#23567;&#30340;&#33609;&#31295;&#27169;&#22411;&#26469;&#39044;&#27979;&#30446;&#26631;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25512;&#29702;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22312;&#38754;&#23545;&#22810;&#26679;&#30340;&#25991;&#26412;&#36755;&#20837;&#21644;&#33609;&#31295;&#27169;&#22411;&#19982;&#30446;&#26631;&#27169;&#22411;&#20043;&#38388;&#30340;&#26174;&#33879;&#33021;&#21147;&#24046;&#36317;&#26102;&#65292;&#20854;&#26377;&#25928;&#24615;&#21487;&#33021;&#21463;&#21040;&#38480;&#21046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22312;&#32447;&#25512;&#27979;&#35299;&#30721;&#65288;OSD&#65289;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#20854;&#20027;&#35201;&#24605;&#24819;&#26159;&#21033;&#29992;LLM&#26381;&#21153;&#38598;&#32676;&#20013;&#20016;&#23500;&#30340;&#22810;&#20313;&#35745;&#31639;&#33021;&#21147;&#65292;&#26681;&#25454;&#35266;&#23519;&#21040;&#30340;&#29992;&#25143;&#26597;&#35810;&#25968;&#25454;&#25345;&#32493;&#26356;&#26032;&#65288;&#22810;&#20010;&#65289;&#33609;&#31295;&#27169;&#22411;&#12290;&#30001;&#20110;LLM&#25512;&#29702;&#21463;&#20869;&#23384;&#38480;&#21046;&#65292;&#20856;&#22411;&#30340;LLM&#26381;&#21153;&#38598;&#32676;&#20013;&#30340;&#21097;&#20313;&#35745;&#31639;&#33021;&#21147;&#21487;&#20197;&#29992;&#20110;&#22312;&#32447;&#37325;&#26032;&#35757;&#32451;&#33609;&#31295;&#27169;&#22411;&#65292;&#20174;&#32780;&#20351;&#35757;&#32451;&#25104;&#26412;&#20445;&#25345;&#20013;&#24615;&#12290;&#30001;&#20110;LLM&#26381;&#21153;&#30340;&#26597;&#35810;&#20998;&#24067;&#30456;&#23545;&#31616;&#21333;&#65292;&#26681;&#25454;&#26597;&#35810;&#20998;&#24067;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#21487;&#20197;&#20351;&#33609;&#31295;&#27169;&#22411;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#30446;&#26631;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speculative decoding is a pivotal technique to accelerate the inference of large language models (LLMs) by employing a smaller draft model to predict the target model's outputs. However, its efficacy can be limited due to the low predictive accuracy of the draft model, particularly when faced with diverse text inputs and a significant capability gap between the draft and target models. We introduce online speculative decoding (OSD) to address this challenge. The main idea is to continually update (multiple) draft model(s) on observed user query data using the abundant excess computational power in an LLM serving cluster. Given that LLM inference is memory-bounded, the surplus computational power in a typical LLM serving cluster can be repurposed for online retraining of draft models, thereby making the training cost-neutral. Since the query distribution of an LLM service is relatively simple, retraining on query distribution enables the draft model to more accurately predict the target
&lt;/p&gt;</description></item><item><title>Memoria &#26159;&#19968;&#20010;&#36890;&#29992;&#35760;&#24518;&#32593;&#32476;&#65292;&#24212;&#29992;&#28023;&#27604;&#23433;&#29702;&#35770;&#26469;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#12290;&#36890;&#36807;&#23384;&#20648;&#21644;&#26816;&#32034;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#26681;&#25454;&#28023;&#24067;&#35268;&#21017;&#21464;&#21270;&#30340;&#36830;&#25509;&#26435;&#37325;&#65292;Memoria &#22312;&#35832;&#22914; BERT &#21644; GPT &#20043;&#31867;&#30340;&#27969;&#34892; Transformer &#27169;&#22411;&#19978;&#26174;&#33879;&#25913;&#36827;&#20102;&#32771;&#34385;&#38271;&#26399;&#20381;&#36182;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.03052</link><description>&lt;p&gt;
Memoria: &#29992;&#20110;&#31867;&#20154;&#39034;&#24207;&#22788;&#29702;&#30340;&#28023;&#27604;&#23433;&#35760;&#24518;&#20307;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Memoria: Hebbian Memory Architecture for Human-Like Sequential Processing. (arXiv:2310.03052v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03052
&lt;/p&gt;
&lt;p&gt;
Memoria &#26159;&#19968;&#20010;&#36890;&#29992;&#35760;&#24518;&#32593;&#32476;&#65292;&#24212;&#29992;&#28023;&#27604;&#23433;&#29702;&#35770;&#26469;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#12290;&#36890;&#36807;&#23384;&#20648;&#21644;&#26816;&#32034;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#26681;&#25454;&#28023;&#24067;&#35268;&#21017;&#21464;&#21270;&#30340;&#36830;&#25509;&#26435;&#37325;&#65292;Memoria &#22312;&#35832;&#22914; BERT &#21644; GPT &#20043;&#31867;&#30340;&#27969;&#34892; Transformer &#27169;&#22411;&#19978;&#26174;&#33879;&#25913;&#36827;&#20102;&#32771;&#34385;&#38271;&#26399;&#20381;&#36182;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer &#22312;&#22810;&#20010;&#39046;&#22495;&#21644;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#26377;&#38480;&#30340;&#23481;&#37327;&#65292;Transformer &#24456;&#38590;&#22788;&#29702;&#38271;&#36755;&#20837;&#24207;&#21015;&#12290;&#34429;&#28982;&#22686;&#21152;&#36755;&#20837;&#38271;&#24230;&#26159;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#26080;&#27490;&#22659;&#22320;&#22686;&#21152;&#38271;&#24230;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#27492;&#22806;&#65292;&#19982; Transformer &#19981;&#21516;&#65292;&#20154;&#31867;&#26377;&#36873;&#25321;&#24615;&#22320;&#35760;&#20303;&#21644;&#20351;&#29992;&#20165;&#19982;&#36755;&#20837;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#32780;&#19981;&#26159;&#20174;&#22836;&#21040;&#23614;&#22788;&#29702;&#25152;&#26377;&#21407;&#22987;&#25968;&#25454;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102; Memoria&#65292;&#19968;&#20010;&#24212;&#29992;&#28023;&#27604;&#23433;&#35760;&#24518;&#24418;&#25104;&#29702;&#35770;&#30340;&#36890;&#29992;&#35760;&#24518;&#32593;&#32476;&#65292;&#29992;&#20110;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#12290;Memoria &#22312;&#24037;&#20316;&#35760;&#24518;&#12289;&#30701;&#26399;&#35760;&#24518;&#21644;&#38271;&#26399;&#35760;&#24518;&#30340;&#22810;&#20010;&#35760;&#24518;&#23618;&#32423;&#19978;&#23384;&#20648;&#21644;&#26816;&#32034;&#31216;&#20026; engram &#30340;&#20449;&#24687;&#65292;&#20351;&#29992;&#26681;&#25454;&#28023;&#24067;&#35268;&#21017;&#21464;&#21270;&#30340;&#36830;&#25509;&#26435;&#37325;&#12290;&#36890;&#36807;&#19982;&#35832;&#22914; BERT &#21644; GPT &#31561;&#27969;&#34892;&#30340;&#22522;&#20110; Transformer &#30340;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#25552;&#20986; Memoria &#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#32771;&#34385;&#38271;&#26399;&#20381;&#36182;&#30340;&#33021;&#21147;&#12290;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Transformers have demonstrated their success in various domains and tasks. However, Transformers struggle with long input sequences due to their limited capacity. While one solution is to increase input length, endlessly stretching the length is unrealistic. Furthermore, humans selectively remember and use only relevant information from inputs, unlike Transformers which process all raw data from start to end. We introduce Memoria, a general memory network that applies Hebbian theory which is a major theory explaining human memory formulation to enhance long-term dependencies in neural networks. Memoria stores and retrieves information called engram at multiple memory levels of working memory, short-term memory, and long-term memory, using connection weights that change according to Hebb's rule. Through experiments with popular Transformer-based models like BERT and GPT, we present that Memoria significantly improves the ability to consider long-term dependencies in various tasks. Resul
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#20351;&#29992;&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#21521;&#37327;&#20316;&#20026;&#20301;&#32622;&#32534;&#30721;&#38754;&#20020;&#30340;&#38750;&#21807;&#19968;&#24615;&#21644;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#31283;&#23450;&#19988;&#34920;&#36798;&#20016;&#23500;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#65288;SPE&#65289;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#29305;&#24449;&#20540;&#23545;&#29305;&#24449;&#31354;&#38388;&#36827;&#34892;"&#36719;&#20998;&#21106;"&#65292;&#22312;&#26410;&#35265;&#36807;&#30340;&#22270;&#32467;&#26500;&#19978;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.02579</link><description>&lt;p&gt;
&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#34920;&#36798;&#20301;&#32622;&#32534;&#30721;&#30340;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Stability of Expressive Positional Encodings for Graph Neural Networks. (arXiv:2310.02579v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#20351;&#29992;&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#21521;&#37327;&#20316;&#20026;&#20301;&#32622;&#32534;&#30721;&#38754;&#20020;&#30340;&#38750;&#21807;&#19968;&#24615;&#21644;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#31283;&#23450;&#19988;&#34920;&#36798;&#20016;&#23500;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#65288;SPE&#65289;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#29305;&#24449;&#20540;&#23545;&#29305;&#24449;&#31354;&#38388;&#36827;&#34892;"&#36719;&#20998;&#21106;"&#65292;&#22312;&#26410;&#35265;&#36807;&#30340;&#22270;&#32467;&#26500;&#19978;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#26377;&#25928;&#30340;&#22270;&#20301;&#32622;&#32534;&#30721;&#23545;&#26500;&#24314;&#24378;&#22823;&#30340;&#22270;&#36716;&#25442;&#22120;&#21644;&#22686;&#24378;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#38750;&#24120;&#20851;&#38190;&#12290;&#23613;&#31649;&#24191;&#27867;&#20351;&#29992;&#65292;&#20351;&#29992;&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#21521;&#37327;&#20316;&#20026;&#20301;&#32622;&#32534;&#30721;&#38754;&#20020;&#20004;&#20010;&#26681;&#26412;&#24615;&#25361;&#25112;&#65306;&#65288;1&#65289;\emph{&#38750;&#21807;&#19968;&#24615;}&#65306;&#21516;&#19968;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#23384;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#29305;&#24449;&#20998;&#35299;&#65292;&#20197;&#21450;&#65288;2&#65289;\emph{&#19981;&#31283;&#23450;&#24615;}&#65306;&#23545;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#30340;&#24494;&#23567;&#25200;&#21160;&#21487;&#33021;&#23548;&#33268;&#23436;&#20840;&#19981;&#21516;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#20174;&#32780;&#23548;&#33268;&#20301;&#32622;&#32534;&#30721;&#30340;&#19981;&#21487;&#39044;&#27979;&#24615;&#21464;&#21270;&#12290;&#23613;&#31649;&#26377;&#24456;&#22810;&#23581;&#35797;&#35299;&#20915;&#38750;&#21807;&#19968;&#24615;&#30340;&#26041;&#27861;&#65292;&#20294;&#22823;&#22810;&#25968;&#26041;&#27861;&#24573;&#35270;&#20102;&#31283;&#23450;&#24615;&#65292;&#23548;&#33268;&#22312;&#26410;&#35265;&#36807;&#30340;&#22270;&#32467;&#26500;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19981;&#31283;&#23450;&#24615;&#30340;&#21407;&#22240;&#26159;&#29305;&#24449;&#31354;&#38388;&#30340;"&#30828;&#20998;&#21106;"&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31283;&#23450;&#19988;&#34920;&#36798;&#20016;&#23500;&#30340;&#20301;&#32622;&#32534;&#30721;&#65288;SPE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#29305;&#24449;&#21521;&#37327;&#30340;&#26550;&#26500;&#65292;&#21033;&#29992;&#29305;&#24449;&#20540;&#23558;&#29305;&#24449;&#31354;&#38388;&#36827;&#34892;"&#36719;&#20998;&#21106;"&#12290;SPE&#26159;&#39318;&#20010;&#65288;1&#65289;&#21487;&#35777;&#26126;&#31283;&#23450;&#30340;&#26550;&#26500;&#65292;&#20197;&#21450;&#65288;2&#65289;&#26222;&#36866;&#22320;&#25552;&#21319;&#22270;&#32467;&#26500;&#27867;&#21270;&#24615;&#33021;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing effective positional encodings for graphs is key to building powerful graph transformers and enhancing message-passing graph neural networks. Although widespread, using Laplacian eigenvectors as positional encodings faces two fundamental challenges: (1) \emph{Non-uniqueness}: there are many different eigendecompositions of the same Laplacian, and (2) \emph{Instability}: small perturbations to the Laplacian could result in completely different eigenspaces, leading to unpredictable changes in positional encoding.  Despite many attempts to address non-uniqueness, most methods overlook stability, leading to poor generalization on unseen graph structures. We identify the cause of instability to be a "hard partition" of eigenspaces. Hence, we introduce Stable and Expressive Positional Encodings (SPE), an architecture for processing eigenvectors that uses eigenvalues to "softly partition" eigenspaces. SPE is the first architecture that is (1) provably stable, and (2) universally exp
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#25915;&#20987;&#26080;&#20851;&#38450;&#24481;&#31574;&#30053;AdvFooler&#65292;&#36890;&#36807;&#38543;&#26426;&#21270;&#36755;&#20837;&#30340;&#28508;&#22312;&#34920;&#31034;&#26469;&#22256;&#24785;&#22522;&#20110;&#26597;&#35810;&#30340;&#40657;&#30418;&#25915;&#20987;&#65292;&#20174;&#32780;&#36855;&#24785;&#25991;&#26412;&#24858;&#24324;&#32773;&#12290;</title><link>http://arxiv.org/abs/2310.01452</link><description>&lt;p&gt;
&#36890;&#36807;&#38543;&#26426;&#21270;&#28508;&#22312;&#34920;&#31034;&#26469;&#36855;&#24785;&#25991;&#26412;&#24858;&#24324;&#32773;
&lt;/p&gt;
&lt;p&gt;
Fooling the Textual Fooler via Randomizing Latent Representations. (arXiv:2310.01452v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01452
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#25915;&#20987;&#26080;&#20851;&#38450;&#24481;&#31574;&#30053;AdvFooler&#65292;&#36890;&#36807;&#38543;&#26426;&#21270;&#36755;&#20837;&#30340;&#28508;&#22312;&#34920;&#31034;&#26469;&#22256;&#24785;&#22522;&#20110;&#26597;&#35810;&#30340;&#40657;&#30418;&#25915;&#20987;&#65292;&#20174;&#32780;&#36855;&#24785;&#25991;&#26412;&#24858;&#24324;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#36817;&#26399;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#25932;&#23545;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#21363;&#24494;&#23567;&#22320;&#25913;&#21464;&#36755;&#20837;&#20197;&#23548;&#33268;&#27169;&#22411;&#30340;&#38169;&#35823;&#34892;&#20026;&#12290;&#20854;&#20013;&#65292;&#25932;&#23545;&#35789;&#32423;&#25200;&#21160;&#26159;&#19968;&#20010;&#34987;&#24191;&#27867;&#30740;&#31350;&#21644;&#26377;&#25928;&#30340;&#25915;&#20987;&#31574;&#30053;&#12290;&#36825;&#20123;&#25915;&#20987;&#22312;&#40657;&#30418;&#35774;&#32622;&#20013;&#36215;&#20316;&#29992;&#65292;&#19981;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#32467;&#26500;&#25110;&#21442;&#25968;&#65292;&#22240;&#27492;&#21487;&#33021;&#23545;&#29616;&#26377;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#20026;&#20102;&#36827;&#34892;&#25915;&#20987;&#65292;&#23545;&#25163;&#22810;&#27425;&#26597;&#35810;&#21463;&#23475;&#27169;&#22411;&#65292;&#20197;&#30830;&#23450;&#36755;&#20837;&#25991;&#26412;&#20013;&#26368;&#37325;&#35201;&#30340;&#21333;&#35789;&#65292;&#24182;&#29992;&#23427;&#20204;&#23545;&#24212;&#30340;&#21516;&#20041;&#35789;&#26367;&#25442;&#36825;&#20123;&#21333;&#35789;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#21644;&#25915;&#20987;&#26080;&#20851;&#30340;&#38450;&#24481;&#65292;&#20854;&#20027;&#35201;&#30446;&#26631;&#26159;&#22256;&#24785;&#22522;&#20110;&#26597;&#35810;&#30340;&#40657;&#30418;&#25915;&#20987;&#20013;&#20135;&#29983;&#25932;&#23545;&#31034;&#20363;&#30340;&#36807;&#31243;&#65307;&#21363;&#24858;&#24324;&#25991;&#26412;&#24858;&#24324;&#32773;&#12290;&#36825;&#31181;&#38450;&#24481;&#21517;&#20026;AdvFooler&#65292;&#36890;&#36807;&#38543;&#26426;&#21270;&#36755;&#20837;&#30340;&#28508;&#22312;&#34920;&#31034;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite outstanding performance in a variety of NLP tasks, recent studies have revealed that NLP models are vulnerable to adversarial attacks that slightly perturb the input to cause the models to misbehave. Among these attacks, adversarial word-level perturbations are well-studied and effective attack strategies. Since these attacks work in black-box settings, they do not require access to the model architecture or model parameters and thus can be detrimental to existing NLP applications. To perform an attack, the adversary queries the victim model many times to determine the most important words in an input text and to replace these words with their corresponding synonyms. In this work, we propose a lightweight and attack-agnostic defense whose main goal is to perplex the process of generating an adversarial example in these query-based black-box attacks; that is to fool the textual fooler. This defense, named AdvFooler, works by randomizing the latent representation of the input at 
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;Transformer-based Multivariate Time Series Classifier (TMTSC)&#26469;&#39044;&#27979;&#39118;&#38505;&#25237;&#36164;&#21644;&#25104;&#38271;&#36164;&#26412;&#30340;&#20505;&#36873;&#20844;&#21496;&#30340;&#25104;&#21151;&#21487;&#33021;&#24615;&#65292;&#20197;&#20248;&#21270;&#25237;&#36164;&#30446;&#26631;&#30340;&#36873;&#25321;&#12290;&#36890;&#36807;&#23545;&#30456;&#20851;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#21644;&#23454;&#39564;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16888</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#22120;&#33719;&#21462;&#39118;&#38505;&#25237;&#36164;&#21644;&#25104;&#38271;&#36164;&#26412;&#30340;&#25237;&#36164;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Sourcing Investment Targets for Venture and Growth Capital Using Multivariate Time Series Transformer. (arXiv:2309.16888v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16888
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;Transformer-based Multivariate Time Series Classifier (TMTSC)&#26469;&#39044;&#27979;&#39118;&#38505;&#25237;&#36164;&#21644;&#25104;&#38271;&#36164;&#26412;&#30340;&#20505;&#36873;&#20844;&#21496;&#30340;&#25104;&#21151;&#21487;&#33021;&#24615;&#65292;&#20197;&#20248;&#21270;&#25237;&#36164;&#30446;&#26631;&#30340;&#36873;&#25321;&#12290;&#36890;&#36807;&#23545;&#30456;&#20851;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#21644;&#23454;&#39564;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#22312;&#31169;&#21215;&#32929;&#26435;&#65288;PE&#65289;&#34892;&#19994;&#20013;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#20026;&#39118;&#38505;&#25237;&#36164;&#65288;VC&#65289;&#21644;&#25104;&#38271;&#36164;&#26412;&#65288;GC&#65289;&#23547;&#25214;&#25237;&#36164;&#30446;&#26631;&#65288;&#21363;&#20844;&#21496;&#65289;&#26041;&#38754;&#12290;&#25105;&#20204;&#23545;&#30456;&#20851;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;Transformer&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22120;&#65288;TMTSC&#65289;&#26469;&#39044;&#27979;&#20219;&#20309;&#20505;&#36873;&#20844;&#21496;&#30340;&#25104;&#21151;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#30446;&#26631;&#26159;&#36890;&#36807;&#23558;&#23547;&#25214;&#25237;&#36164;&#38382;&#39064;&#27491;&#24335;&#23450;&#20041;&#20026;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20219;&#21153;&#65292;&#20248;&#21270;VC&#21644;GC&#25237;&#36164;&#30340;&#23547;&#25214;&#25928;&#26524;&#12290;&#25105;&#20204;&#20381;&#27425;&#20171;&#32461;&#20102;&#25105;&#20204;&#23454;&#29616;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#36825;&#20123;&#37096;&#20998;&#20849;&#21516; contribut &#21040;&#20102;&#22312;VC/GC&#23547;&#25214;&#20013;&#25104;&#21151;&#24212;&#29992;TMTSC&#65306;&#36755;&#20837;&#29305;&#24449;&#12289;&#27169;&#22411;&#26550;&#26500;&#12289;&#20248;&#21270;&#30446;&#26631;&#20197;&#21450;&#22522;&#20110;&#25237;&#36164;&#32773;&#30340;&#25968;&#25454;&#22686;&#24378;&#21644;&#21010;&#20998;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#24182;&#23558;&#20854;&#19982;&#19977;&#20010;&#27969;&#34892;&#30340;&#22522;&#20934;&#32447;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the growing application of data-driven approaches within the Private Equity (PE) industry, particularly in sourcing investment targets (i.e., companies) for Venture Capital (VC) and Growth Capital (GC). We present a comprehensive review of the relevant approaches and propose a novel approach leveraging a Transformer-based Multivariate Time Series Classifier (TMTSC) for predicting the success likelihood of any candidate company. The objective of our research is to optimize sourcing performance for VC and GC investments by formally defining the sourcing problem as a multivariate time series classification task. We consecutively introduce the key components of our implementation which collectively contribute to the successful application of TMTSC in VC/GC sourcing: input features, model architecture, optimization target, and investor-centric data augmentation and split. Our extensive experiments on four datasets, benchmarked towards three popular baselines, demonstrat
&lt;/p&gt;</description></item><item><title>UniHead&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#26816;&#27979;&#22836;&#65292;&#23427;&#36890;&#36807;&#24341;&#20837;&#21464;&#24418;&#24863;&#30693;&#12289;&#21452;&#36724;&#32858;&#21512;&#21464;&#25442;&#22120;&#21644;&#36328;&#20219;&#21153;&#20132;&#20114;&#21464;&#25442;&#22120;&#65292;&#23454;&#29616;&#20102;&#20840;&#24863;&#30693;&#33021;&#21147;&#30340;&#32479;&#19968;&#12290;</title><link>http://arxiv.org/abs/2309.13242</link><description>&lt;p&gt;
UniHead: &#34701;&#21512;&#22810;&#24863;&#30693;&#30340;&#26816;&#27979;&#22836;
&lt;/p&gt;
&lt;p&gt;
UniHead: Unifying Multi-Perception for Detection Heads. (arXiv:2309.13242v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13242
&lt;/p&gt;
&lt;p&gt;
UniHead&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#26816;&#27979;&#22836;&#65292;&#23427;&#36890;&#36807;&#24341;&#20837;&#21464;&#24418;&#24863;&#30693;&#12289;&#21452;&#36724;&#32858;&#21512;&#21464;&#25442;&#22120;&#21644;&#36328;&#20219;&#21153;&#20132;&#20114;&#21464;&#25442;&#22120;&#65292;&#23454;&#29616;&#20102;&#20840;&#24863;&#30693;&#33021;&#21147;&#30340;&#32479;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#22836;&#26159;&#30446;&#26631;&#26816;&#27979;&#22120;&#20013;&#19968;&#20010;&#20851;&#38190;&#32452;&#20214;&#65292;&#36127;&#36131;&#25191;&#34892;&#20998;&#31867;&#21644;&#23450;&#20301;&#21151;&#33021;&#12290;&#28982;&#32780;&#65292;&#24120;&#29992;&#30340;&#24182;&#34892;&#26816;&#27979;&#22836;&#24120;&#24120;&#32570;&#20047;&#20840;&#24863;&#30693;&#33021;&#21147;&#65292;&#22914;&#21464;&#24418;&#24863;&#30693;&#12289;&#20840;&#23616;&#24863;&#30693;&#21644;&#36328;&#20219;&#21153;&#24863;&#30693;&#12290;&#23613;&#31649;&#26377;&#24456;&#22810;&#26041;&#27861;&#35797;&#22270;&#20174;&#21333;&#20010;&#26041;&#38754;&#25552;&#39640;&#36825;&#20123;&#33021;&#21147;&#65292;&#20294;&#23454;&#29616;&#20840;&#38754;&#32479;&#19968;&#30340;&#35299;&#20915;&#26041;&#26696;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#38024;&#23545;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26816;&#27979;&#22836;&#65292;&#31216;&#20026;UniHead&#65292;&#21516;&#26102;&#32479;&#19968;&#20102;&#19977;&#31181;&#24863;&#30693;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#65306;&#65288;1&#65289;&#24341;&#20837;&#20102;&#21464;&#24418;&#24863;&#30693;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#33258;&#36866;&#24212;&#37319;&#26679;&#23545;&#35937;&#29305;&#24449;&#65307;&#65288;2&#65289;&#25552;&#20986;&#20102;&#21452;&#36724;&#32858;&#21512;&#21464;&#25442;&#22120;&#65288;DAT&#65289;&#26469;&#28789;&#27963;&#22320;&#24314;&#27169;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#65292;&#20174;&#32780;&#23454;&#29616;&#20840;&#23616;&#24863;&#30693;&#65307;&#65288;3&#65289;&#35774;&#35745;&#20102;&#19968;&#31181;&#36328;&#20219;&#21153;&#20132;&#20114;&#21464;&#25442;&#22120;&#65288;CIT&#65289;&#65292;&#20419;&#36827;&#20998;&#31867;&#21644;&#23450;&#20301;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The detection head constitutes a pivotal component within object detectors, tasked with executing both classification and localization functions. Regrettably, the commonly used parallel head often lacks omni perceptual capabilities, such as deformation perception, global perception and cross-task perception. Despite numerous methods attempt to enhance these abilities from a single aspect, achieving a comprehensive and unified solution remains a significant challenge. In response to this challenge, we have developed an innovative detection head, termed UniHead, to unify three perceptual abilities simultaneously. More precisely, our approach (1) introduces deformation perception, enabling the model to adaptively sample object features; (2) proposes a Dual-axial Aggregation Transformer (DAT) to adeptly model long-range dependencies, thereby achieving global perception; and (3) devises a Cross-task Interaction Transformer (CIT) that facilitates interaction between the classification and lo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#36974;&#30422;&#25805;&#20316;&#24341;&#20837;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#20197;&#25552;&#39640;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#36974;&#30422;&#25805;&#20316;&#21487;&#33021;&#23384;&#22312;&#19968;&#20123;&#21103;&#20316;&#29992;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#26469;&#24212;&#23545;&#36825;&#20123;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.12757</link><description>&lt;p&gt;
&#23545;&#20110;ConvNets&#26469;&#35828;&#65292;&#36974;&#30422;&#65288;masking&#65289;&#33021;&#25913;&#21892;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#32780;&#26174;&#33879;&#24615;&#21578;&#35785;&#20320;&#20309;&#22788;&#12290;&#65288;arXiv:2309.12757v1 [cs.CV]&#65289;
&lt;/p&gt;
&lt;p&gt;
Masking Improves Contrastive Self-Supervised Learning for ConvNets, and Saliency Tells You Where. (arXiv:2309.12757v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12757
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#36974;&#30422;&#25805;&#20316;&#24341;&#20837;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#20197;&#25552;&#39640;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#36974;&#30422;&#25805;&#20316;&#21487;&#33021;&#23384;&#22312;&#19968;&#20123;&#21103;&#20316;&#29992;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#26469;&#24212;&#23545;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#25968;&#25454;&#24320;&#22987;&#21463;&#30410;&#20110;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#24314;&#31435;&#22312;&#36974;&#30422;&#21644;&#33258;&#37325;&#26500;&#30446;&#26631;&#20043;&#19978;&#65292;&#36825;&#35201;&#24402;&#21151;&#20110;&#20196;&#29260;&#21270;&#31243;&#24207;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#39592;&#24178;&#32467;&#26500;&#30340;&#24341;&#20837;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#22270;&#20687;&#25968;&#25454;&#30340;&#21478;&#19968;&#31181;&#37325;&#35201;&#19988;&#24191;&#27867;&#37319;&#29992;&#30340;&#26550;&#26500;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#23613;&#31649;&#20855;&#26377;&#39537;&#21160;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#65292;&#20173;&#28982;&#38754;&#20020;&#23558;&#36825;&#31181;&#30452;&#25509;&#32780;&#36890;&#29992;&#30340;&#36974;&#30422;&#25805;&#20316;&#26174;&#33879;&#22320;&#21033;&#29992;&#20110;&#20854;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#22256;&#38590;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#20943;&#36731;&#23558;&#36974;&#30422;&#25805;&#20316;&#32435;&#20837;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#30340;&#36127;&#25285;&#65292;&#20316;&#20026;&#19968;&#31181;&#39069;&#22806;&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;ConvNets&#20013;&#22240;&#36974;&#32617;&#25805;&#20316;&#32780;&#20135;&#29983;&#30340;&#39069;&#22806;&#36793;&#32536;&#65288;&#36974;&#30422;&#21644;&#26410;&#36974;&#30422;&#21306;&#22495;&#20043;&#38388;&#65289;&#20197;&#21450;&#20854;&#20182;&#19981;&#21033;&#24433;&#21709;&#30340;&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#24050;&#32463;&#22312;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#35752;&#35770;&#36807;&#12290;
&lt;/p&gt;
&lt;p&gt;
While image data starts to enjoy the simple-but-effective self-supervised learning scheme built upon masking and self-reconstruction objective thanks to the introduction of tokenization procedure and vision transformer backbone, convolutional neural networks as another important and widely-adopted architecture for image data, though having contrastive-learning techniques to drive the self-supervised learning, still face the difficulty of leveraging such straightforward and general masking operation to benefit their learning process significantly. In this work, we aim to alleviate the burden of including masking operation into the contrastive-learning framework for convolutional neural networks as an extra augmentation method. In addition to the additive but unwanted edges (between masked and unmasked regions) as well as other adverse effects caused by the masking operations for ConvNets, which have been discussed by prior works, we particularly identify the potential problem where for 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#20998;&#23618;&#30340;&#24369;&#20559;&#22909;&#21453;&#39304;&#36827;&#34892;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#12290;&#36890;&#36807;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#65292;&#19982;&#20154;&#31867;&#20559;&#22909;&#38750;&#24120;&#19968;&#33268;&#30340;&#22797;&#26434;&#22870;&#21169;&#21487;&#20197;&#24110;&#21161;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#26085;&#30410;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.02632</link><description>&lt;p&gt;
&#20174;&#20998;&#23618;&#30340;&#24369;&#20559;&#22909;&#21453;&#39304;&#20013;&#36827;&#34892;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning from Hierarchical Weak Preference Feedback. (arXiv:2309.02632v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02632
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#20998;&#23618;&#30340;&#24369;&#20559;&#22909;&#21453;&#39304;&#36827;&#34892;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#12290;&#36890;&#36807;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#65292;&#19982;&#20154;&#31867;&#20559;&#22909;&#38750;&#24120;&#19968;&#33268;&#30340;&#22797;&#26434;&#22870;&#21169;&#21487;&#20197;&#24110;&#21161;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#26085;&#30410;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22870;&#21169;&#35774;&#35745;&#26159;&#23454;&#38469;&#24378;&#21270;&#23398;&#20064;&#20013;&#19968;&#20010;&#22522;&#26412;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26041;&#38754;&#12290;&#23545;&#20110;&#31616;&#21333;&#20219;&#21153;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#24120;&#25163;&#24037;&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#65292;&#20363;&#22914;&#20351;&#29992;&#33509;&#24178;&#20010;&#22870;&#21169;&#22240;&#23376;&#30340;&#32447;&#24615;&#32452;&#21512;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22870;&#21169;&#24037;&#31243;&#21463;&#21040;&#36817;&#20284;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35843;&#20248;&#25104;&#26412;&#65292;&#24182;&#19988;&#36890;&#24120;&#26080;&#27861;&#25552;&#20379;&#22797;&#26434;&#20219;&#21153;&#25152;&#38656;&#30340;&#32454;&#31890;&#24230;&#12290;&#20026;&#20102;&#36991;&#20813;&#36825;&#20123;&#22256;&#38590;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#36716;&#21521;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#20174;&#36712;&#36857;&#24207;&#21015;&#23545;&#20043;&#38388;&#30340;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#20559;&#22909;&#30340;&#22870;&#21169;&#24314;&#27169;&#65292;RLHF&#23398;&#20064;&#21040;&#19982;&#20154;&#31867;&#20559;&#22909;&#38750;&#24120;&#19968;&#33268;&#30340;&#22797;&#26434;&#22870;&#21169;&#65292;&#20351;&#24471;&#24378;&#21270;&#23398;&#20064;&#33021;&#22815;&#35299;&#20915;&#26085;&#30410;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;RLHF&#30340;&#36866;&#29992;&#24615;&#21463;&#21040;&#33719;&#24471;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#30340;&#39640;&#25104;&#26412;&#21644;&#22256;&#38590;&#30340;&#38480;&#21046;&#12290;&#37492;&#20110;&#36825;&#20010;&#25104;&#26412;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#20351;&#29992;&#26356;&#23569;&#20154;&#21147;&#25237;&#20837;&#30340;&#26041;&#24335;&#26469;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reward design is a fundamental, yet challenging aspect of practical reinforcement learning (RL). For simple tasks, researchers typically handcraft the reward function, e.g., using a linear combination of several reward factors. However, such reward engineering is subject to approximation bias, incurs large tuning cost, and often cannot provide the granularity required for complex tasks. To avoid these difficulties, researchers have turned to reinforcement learning from human feedback (RLHF), which learns a reward function from human preferences between pairs of trajectory sequences. By leveraging preference-based reward modeling, RLHF learns complex rewards that are well aligned with human preferences, allowing RL to tackle increasingly difficult problems. Unfortunately, the applicability of RLHF is limited due to the high cost and difficulty of obtaining human preference data. In light of this cost, we investigate learning reward functions for complex tasks with less human effort; sim
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25955;&#23556;&#21442;&#25968;&#65288;S&#21442;&#25968;&#65289;&#20449;&#21495;&#22788;&#29702;&#30340;&#29616;&#22330;&#25925;&#38556;&#35786;&#26029;&#26041;&#27861;&#65292;&#23545;&#27687;&#21270;&#38111;&#38177;&#65288;ITO&#65289;&#30005;&#26497;&#36827;&#34892;&#25925;&#38556;&#26816;&#27979;&#21644;&#35786;&#26029;&#12290;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#26089;&#26399;&#26816;&#27979;&#12289;&#39640;&#35786;&#26029;&#31934;&#24230;&#12289;&#22122;&#22768;&#40065;&#26834;&#24615;&#21644;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.11639</link><description>&lt;p&gt;
&#36890;&#36807;&#22788;&#29702;S&#21442;&#25968;&#27169;&#24335;&#23545;&#27687;&#21270;&#38111;&#38177;&#30005;&#26497;&#36827;&#34892;&#29616;&#22330;&#25925;&#38556;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
In situ Fault Diagnosis of Indium Tin Oxide Electrodes by Processing S-Parameter Patterns. (arXiv:2308.11639v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25955;&#23556;&#21442;&#25968;&#65288;S&#21442;&#25968;&#65289;&#20449;&#21495;&#22788;&#29702;&#30340;&#29616;&#22330;&#25925;&#38556;&#35786;&#26029;&#26041;&#27861;&#65292;&#23545;&#27687;&#21270;&#38111;&#38177;&#65288;ITO&#65289;&#30005;&#26497;&#36827;&#34892;&#25925;&#38556;&#26816;&#27979;&#21644;&#35786;&#26029;&#12290;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#26089;&#26399;&#26816;&#27979;&#12289;&#39640;&#35786;&#26029;&#31934;&#24230;&#12289;&#22122;&#22768;&#40065;&#26834;&#24615;&#21644;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20809;&#30005;&#23376;&#39046;&#22495;&#65292;&#27687;&#21270;&#38111;&#38177;&#65288;ITO&#65289;&#30005;&#26497;&#22312;&#26174;&#31034;&#22120;&#12289;&#20256;&#24863;&#22120;&#21644;&#22826;&#38451;&#33021;&#30005;&#27744;&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26377;&#25928;&#30340;&#25925;&#38556;&#26816;&#27979;&#21644;&#35786;&#26029;&#26159;&#30830;&#20445;&#35774;&#22791;&#24615;&#33021;&#21644;&#21487;&#38752;&#24615;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#35270;&#35273;&#26816;&#26597;&#23545;&#20110;&#36879;&#26126;&#30340;ITO&#30005;&#26497;&#26469;&#35828;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#32780;&#29616;&#26377;&#30340;&#25925;&#38556;&#26816;&#27979;&#26041;&#27861;&#22312;&#30830;&#23450;&#32570;&#38519;&#26681;&#26412;&#21407;&#22240;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#36890;&#24120;&#38656;&#35201;&#30772;&#22351;&#24615;&#35780;&#20272;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25955;&#23556;&#21442;&#25968;&#65288;S&#21442;&#25968;&#65289;&#20449;&#21495;&#22788;&#29702;&#30340;&#29616;&#22330;&#25925;&#38556;&#35786;&#26029;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#26089;&#26399;&#26816;&#27979;&#12289;&#39640;&#35786;&#26029;&#31934;&#24230;&#12289;&#22122;&#22768;&#40065;&#26834;&#24615;&#21644;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#12290;&#26681;&#25454;&#32570;&#38519;&#29366;&#24577;&#33719;&#21462;&#20102;&#20840;&#38754;&#30340;S&#21442;&#25968;&#27169;&#24335;&#25968;&#25454;&#24211;&#12290;&#28982;&#21518;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26041;&#27861;&#65292;&#21253;&#25324;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#12289;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;Transformer&#65292;&#21516;&#26102;&#20998;&#26512;&#25925;&#38556;&#21407;&#22240;&#21644;&#20005;&#37325;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of optoelectronics, indium tin oxide (ITO) electrodes play a crucial role in various applications, such as displays, sensors, and solar cells. Effective fault detection and diagnosis of the ITO electrodes are essential to ensure the performance and reliability of the devices. However, traditional visual inspection is challenging with transparent ITO electrodes, and existing fault detection methods have limitations in determining the root causes of the defects, often requiring destructive evaluations. In this study, an in situ fault diagnosis method is proposed using scattering parameter (S-parameter) signal processing, offering early detection, high diagnostic accuracy, noise robustness, and root cause analysis. A comprehensive S-parameter pattern database is obtained according to defect states. Deep learning (DL) approaches, including multilayer perceptron (MLP), convolutional neural network (CNN), and transformer, are then used to simultaneously analyze the cause and sev
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;&#28508;&#21464;&#37327;&#30340;&#22240;&#26524;&#32467;&#26500;&#20272;&#35745;&#30340;&#24191;&#20041;&#29420;&#31435;&#22122;&#22768;&#65288;GIN&#65289;&#26465;&#20214;&#65292;&#24182;&#32473;&#20986;&#20102;&#32447;&#24615;&#38750;&#39640;&#26031;&#26080;&#29615;&#22240;&#26524;&#27169;&#22411;&#20013;&#28385;&#36275;GIN&#26465;&#20214;&#30340;&#22270;&#24418;&#21028;&#25454;&#12290;</title><link>http://arxiv.org/abs/2308.06718</link><description>&lt;p&gt;
&#20855;&#26377;&#28508;&#21464;&#37327;&#30340;&#22240;&#26524;&#32467;&#26500;&#20272;&#35745;&#30340;&#24191;&#20041;&#29420;&#31435;&#22122;&#22768;&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;
Generalized Independent Noise Condition for Estimating Causal Structure with Latent Variables. (arXiv:2308.06718v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06718
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;&#28508;&#21464;&#37327;&#30340;&#22240;&#26524;&#32467;&#26500;&#20272;&#35745;&#30340;&#24191;&#20041;&#29420;&#31435;&#22122;&#22768;&#65288;GIN&#65289;&#26465;&#20214;&#65292;&#24182;&#32473;&#20986;&#20102;&#32447;&#24615;&#38750;&#39640;&#26031;&#26080;&#29615;&#22240;&#26524;&#27169;&#22411;&#20013;&#28385;&#36275;GIN&#26465;&#20214;&#30340;&#22270;&#24418;&#21028;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#23384;&#22312;&#28508;&#21464;&#37327;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#22240;&#26524;&#32467;&#26500;&#30340;&#25361;&#25112;&#24615;&#20219;&#21153;&#65292;&#21253;&#25324;&#23450;&#20301;&#28508;&#21464;&#37327;&#24182;&#30830;&#23450;&#23427;&#20204;&#30340;&#25968;&#37327;&#65292;&#20197;&#21450;&#35782;&#21035;&#28508;&#21464;&#37327;&#21644;&#35266;&#27979;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21253;&#21547;&#28508;&#21464;&#37327;&#30340;&#32447;&#24615;&#38750;&#39640;&#26031;&#26080;&#29615;&#22240;&#26524;&#27169;&#22411;&#30340;&#24191;&#20041;&#29420;&#31435;&#22122;&#22768;&#65288;GIN&#65289;&#26465;&#20214;&#65292;&#35813;&#26465;&#20214;&#24314;&#31435;&#20102;&#26576;&#20123;&#27979;&#37327;&#21464;&#37327;&#30340;&#32447;&#24615;&#32452;&#21512;&#19982;&#20854;&#20182;&#27979;&#37327;&#21464;&#37327;&#20043;&#38388;&#30340;&#29420;&#31435;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#20004;&#20010;&#35266;&#27979;&#38543;&#26426;&#21521;&#37327; $\bf{Y}$ &#21644; $\bf{Z}$&#65292;&#24403;&#19988;&#20165;&#24403; $\omega^{\intercal}\mathbf{Y}$ &#21644; $\mathbf{Z}$ &#26159;&#29420;&#31435;&#30340;&#26102;&#65292;GIN &#25104;&#31435;&#65292;&#20854;&#20013; $\omega$ &#26159;&#30001; $\mathbf{Y}$ &#21644; $\mathbf{Z}$ &#20043;&#38388;&#30340;&#20132;&#21449;&#21327;&#26041;&#24046;&#30830;&#23450;&#30340;&#38750;&#38646;&#21442;&#25968;&#21521;&#37327;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#32447;&#24615;&#38750;&#39640;&#26031;&#26080;&#29615;&#22240;&#26524;&#27169;&#22411;&#20013; GIN &#26465;&#20214;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#22270;&#24418;&#21028;&#25454;&#12290;&#31616;&#35328;&#20043;&#65292;GIN &#24847;&#21619;&#30528;&#23384;&#22312;&#19968;&#20010;&#22806;&#28304;&#30340;...
&lt;/p&gt;
&lt;p&gt;
We investigate the challenging task of learning causal structure in the presence of latent variables, including locating latent variables and determining their quantity, and identifying causal relationships among both latent and observed variables. To address this, we propose a Generalized Independent Noise (GIN) condition for linear non-Gaussian acyclic causal models that incorporate latent variables, which establishes the independence between a linear combination of certain measured variables and some other measured variables. Specifically, for two observed random vectors $\bf{Y}$ and $\bf{Z}$, GIN holds if and only if $\omega^{\intercal}\mathbf{Y}$ and $\mathbf{Z}$ are independent, where $\omega$ is a non-zero parameter vector determined by the cross-covariance between $\mathbf{Y}$ and $\mathbf{Z}$. We then give necessary and sufficient graphical criteria of the GIN condition in linear non-Gaussian acyclic causal models. Roughly speaking, GIN implies the existence of an exogenous se
&lt;/p&gt;</description></item><item><title>&#20803;&#23398;&#20064;&#22312;&#21307;&#30103;&#39046;&#22495;&#26377;&#24191;&#27867;&#24212;&#29992;&#65292;&#21487;&#20197;&#35299;&#20915;&#21307;&#30103;&#25361;&#25112;&#65292;&#22914;&#26679;&#26412;&#19981;&#36275;&#21644;&#25968;&#25454;&#25910;&#38598;&#24046;&#24322;&#12290;&#20027;&#35201;&#21253;&#25324;&#22810;/&#21333;&#20219;&#21153;&#23398;&#20064;&#21644;&#22810;/&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.02877</link><description>&lt;p&gt;
&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#20803;&#23398;&#20064;&#65306;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta-learning in healthcare: A survey. (arXiv:2308.02877v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02877
&lt;/p&gt;
&lt;p&gt;
&#20803;&#23398;&#20064;&#22312;&#21307;&#30103;&#39046;&#22495;&#26377;&#24191;&#27867;&#24212;&#29992;&#65292;&#21487;&#20197;&#35299;&#20915;&#21307;&#30103;&#25361;&#25112;&#65292;&#22914;&#26679;&#26412;&#19981;&#36275;&#21644;&#25968;&#25454;&#25910;&#38598;&#24046;&#24322;&#12290;&#20027;&#35201;&#21253;&#25324;&#22810;/&#21333;&#20219;&#21153;&#23398;&#20064;&#21644;&#22810;/&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#30340;&#19968;&#20010;&#23376;&#38598;&#65292;&#20803;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#20808;&#21069;&#30340;&#30693;&#35782;&#21644;&#32463;&#39564;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#20803;&#23398;&#20064;&#33539;&#24335;&#21487;&#20197;&#36866;&#24403;&#22320;&#35299;&#20915;&#20256;&#32479;&#23398;&#20064;&#26041;&#27861;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#22914;&#26679;&#26412;&#25968;&#37327;&#19981;&#36275;&#12289;&#39046;&#22495;&#36716;&#31227;&#21644;&#27867;&#21270;&#38382;&#39064;&#12290;&#36825;&#20123;&#29420;&#29305;&#30340;&#29305;&#28857;&#20351;&#20803;&#23398;&#20064;&#25104;&#20026;&#22312;&#21508;&#31181;&#21307;&#30103;&#29615;&#22659;&#20013;&#24320;&#21457;&#26377;&#24433;&#21709;&#21147;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#21512;&#36866;&#36873;&#25321;&#65292;&#36825;&#20123;&#29615;&#22659;&#20013;&#21487;&#29992;&#25968;&#25454;&#36890;&#24120;&#19981;&#36275;&#65292;&#24182;&#19988;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#20063;&#19981;&#21516;&#12290;&#26412;&#35843;&#26597;&#35752;&#35770;&#20102;&#20803;&#23398;&#20064;&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#20197;&#20102;&#35299;&#23427;&#22914;&#20309;&#20197;&#21450;&#22312;&#21738;&#20123;&#26041;&#38754;&#21487;&#20197;&#35299;&#20915;&#20851;&#38190;&#30340;&#21307;&#30103;&#25361;&#25112;&#12290;&#25105;&#20204;&#39318;&#20808;&#25551;&#36848;&#20102;&#20803;&#23398;&#20064;&#30340;&#29702;&#35770;&#22522;&#30784;&#21644;&#20851;&#38190;&#26041;&#27861;&#12290;&#28982;&#21518;&#23558;&#22312;&#21307;&#30103;&#39046;&#22495;&#24212;&#29992;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#20998;&#20026;&#22810;/&#21333;&#20219;&#21153;&#23398;&#20064;&#21644;&#22810;/&#23569;&#26679;&#26412;&#23398;&#20064;&#20004;&#22823;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a subset of machine learning, meta-learning, or learning to learn, aims at improving the model's capabilities by employing prior knowledge and experience. A meta-learning paradigm can appropriately tackle the conventional challenges of traditional learning approaches, such as insufficient number of samples, domain shifts, and generalization. These unique characteristics position meta-learning as a suitable choice for developing influential solutions in various healthcare contexts, where the available data is often insufficient, and the data collection methodologies are different. This survey discusses meta-learning broad applications in the healthcare domain to provide insight into how and where it can address critical healthcare challenges. We first describe the theoretical foundations and pivotal methods of meta-learning. We then divide the employed meta-learning approaches in the healthcare domain into two main categories of multi/single-task learning and many/few-shot learning a
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#22235;&#31181;&#25351;&#23548;&#32454;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#21644;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#25509;&#36817;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#38382;&#31572;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#28982;&#32780;&#65292;&#22312;&#20998;&#31867;&#21644;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#31245;&#36874;&#20110;&#29305;&#23450;&#35757;&#32451;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#27169;&#22411;&#12290;&#27809;&#26377;&#19968;&#20010;&#27169;&#22411;&#22312;&#25152;&#26377;&#30740;&#31350;&#20219;&#21153;&#19978;&#32988;&#36807;&#20854;&#20182;&#27169;&#22411;&#65292;&#26377;&#20123;&#27169;&#22411;&#26356;&#36866;&#21512;&#29305;&#23450;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.12114</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#24212;&#29992;&#20110;&#20020;&#24202;&#21644;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#30340;&#25351;&#23548;&#32454;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Zero-shot and Few-shot Study of Instruction-Finetuned Large Language Models Applied to Clinical and Biomedical Tasks. (arXiv:2307.12114v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12114
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#22235;&#31181;&#25351;&#23548;&#32454;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#21644;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#25509;&#36817;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#38382;&#31572;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#28982;&#32780;&#65292;&#22312;&#20998;&#31867;&#21644;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#31245;&#36874;&#20110;&#29305;&#23450;&#35757;&#32451;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#27169;&#22411;&#12290;&#27809;&#26377;&#19968;&#20010;&#27169;&#22411;&#22312;&#25152;&#26377;&#30740;&#31350;&#20219;&#21153;&#19978;&#32988;&#36807;&#20854;&#20182;&#27169;&#22411;&#65292;&#26377;&#20123;&#27169;&#22411;&#26356;&#36866;&#21512;&#29305;&#23450;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35780;&#20272;&#20102;&#22235;&#31181;&#26368;&#20808;&#36827;&#30340;&#25351;&#23548;&#32454;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#8212;&#8212;ChatGPT&#12289;Flan-T5 UL2&#12289;Tk-Instruct&#21644;Alpaca&#8212;&#8212;&#22312;13&#20010;&#23454;&#38469;&#19990;&#30028;&#30340;&#20020;&#24202;&#21644;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#20363;&#22914;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#12289;&#38382;&#31572;&#65288;QA&#65289;&#12289;&#20851;&#31995;&#25277;&#21462;&#65288;RE&#65289;&#31561;&#12290;&#25105;&#20204;&#30340;&#32508;&#21512;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#65292;&#35780;&#20272;&#30340;LLM&#24320;&#22987;&#25509;&#36817;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#23545;&#20110;QA&#20219;&#21153;&#34920;&#29616;&#24471;&#29305;&#21035;&#22909;&#65292;&#21363;&#20351;&#23427;&#20204;&#20043;&#21069;&#27809;&#26377;&#35265;&#36807;&#36825;&#20123;&#20219;&#21153;&#30340;&#31034;&#20363;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20998;&#31867;&#21644;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#30340;&#34920;&#29616;&#20302;&#20110;&#29305;&#23450;&#35757;&#32451;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#27169;&#22411;&#65288;&#22914;PubMedBERT&#65289;&#21487;&#20197;&#36798;&#21040;&#30340;&#27700;&#24179;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#27809;&#26377;&#19968;&#20010;LLM&#22312;&#25152;&#26377;&#30740;&#31350;&#20219;&#21153;&#19978;&#37117;&#32988;&#36807;&#20854;&#20182;&#27169;&#22411;&#65292;&#26377;&#20123;&#27169;&#22411;&#26356;&#36866;&#21512;&#20110;&#29305;&#23450;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
We evaluate four state-of-the-art instruction-tuned large language models (LLMs) -- ChatGPT, Flan-T5 UL2, Tk-Instruct, and Alpaca -- on a set of 13 real-world clinical and biomedical natural language processing (NLP) tasks in English, such as named-entity recognition (NER), question-answering (QA), relation extraction (RE), etc. Our overall results demonstrate that the evaluated LLMs begin to approach performance of state-of-the-art models in zero- and few-shot scenarios for most tasks, and particularly well for the QA task, even though they have never seen examples from these tasks before. However, we observed that the classification and RE tasks perform below what can be achieved with a specifically trained model for the medical field, such as PubMedBERT. Finally, we noted that no LLM outperforms all the others on all the studied tasks, with some models being better suited for certain tasks than others.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22522;&#20110;&#39044;&#27979;&#30340;&#23646;&#24615;&#26041;&#27861;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#20135;&#29983;&#30340;&#23646;&#24615;&#22270;&#20855;&#26377;&#19981;&#21516;&#30340;&#39640;&#39057;&#20869;&#23481;&#65292;&#28388;&#38500;&#39640;&#39057;&#29575;&#21487;&#20197;&#25552;&#39640;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09591</link><description>&lt;p&gt;
&#26799;&#24230;&#21453;&#20987;&#65306;&#22914;&#20309;&#28388;&#38500;&#39640;&#39057;&#29575;&#25552;&#39640;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Gradient strikes back: How filtering out high frequencies improves explanations. (arXiv:2307.09591v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09591
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22522;&#20110;&#39044;&#27979;&#30340;&#23646;&#24615;&#26041;&#27861;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#20135;&#29983;&#30340;&#23646;&#24615;&#22270;&#20855;&#26377;&#19981;&#21516;&#30340;&#39640;&#39057;&#20869;&#23481;&#65292;&#28388;&#38500;&#39640;&#39057;&#29575;&#21487;&#20197;&#25552;&#39640;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26032;&#22411;&#22522;&#20110;&#39044;&#27979;&#30340;&#23646;&#24615;&#26041;&#27861;&#30340;&#21457;&#23637;&#36805;&#29467;&#65292;&#36880;&#28176;&#21462;&#20195;&#20102;&#26087;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#26469;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#39044;&#27979;&#22411;&#26041;&#27861;&#20026;&#20309;&#20248;&#20110;&#26799;&#24230;&#22411;&#26041;&#27861;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#20174;&#32463;&#39564;&#35266;&#23519;&#24320;&#22987;&#65306;&#36825;&#20004;&#31181;&#26041;&#27861;&#20135;&#29983;&#30340;&#23646;&#24615;&#22270;&#20855;&#26377;&#38750;&#24120;&#19981;&#21516;&#30340;&#21151;&#29575;&#35889;&#65292;&#26799;&#24230;&#22411;&#26041;&#27861;&#25581;&#31034;&#20102;&#27604;&#39044;&#27979;&#22411;&#26041;&#27861;&#26356;&#22810;&#30340;&#39640;&#39057;&#20869;&#23481;&#12290;&#36825;&#19968;&#35266;&#23519;&#24341;&#21457;&#20102;&#22810;&#20010;&#38382;&#39064;&#65306;&#36825;&#31181;&#39640;&#39057;&#20449;&#24687;&#30340;&#26469;&#28304;&#26159;&#20160;&#20040;&#65292;&#23427;&#26159;&#21542;&#30495;&#27491;&#21453;&#26144;&#20102;&#31995;&#32479;&#25152;&#20316;&#20986;&#30340;&#20915;&#31574;&#65311;&#26368;&#21518;&#65292;&#20026;&#20160;&#20040;&#22312;&#22810;&#20010;&#35780;&#20215;&#25351;&#26631;&#19979;&#65292;&#39044;&#27979;&#22411;&#26041;&#27861;&#20013;&#32570;&#20047;&#39640;&#39057;&#20449;&#24687;&#23558;&#20135;&#29983;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#20998;&#25968;&#65311;&#25105;&#20204;&#20998;&#26512;&#20102;&#19977;&#20010;&#20195;&#34920;&#24615;&#30340;&#35270;&#35273;&#20998;&#31867;&#27169;&#22411;&#30340;&#26799;&#24230;&#65292;&#24182;&#35266;&#23519;&#21040;&#23427;&#21253;&#21547;&#26469;&#33258;&#39640;&#39057;&#30340;&#22122;&#22768;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed an explosion in the development of novel prediction-based attribution methods, which have slowly been supplanting older gradient-based methods to explain the decisions of deep neural networks. However, it is still not clear why prediction-based methods outperform gradient-based ones. Here, we start with an empirical observation: these two approaches yield attribution maps with very different power spectra, with gradient-based methods revealing more high-frequency content than prediction-based methods. This observation raises multiple questions: What is the source of this high-frequency information, and does it truly reflect decisions made by the system? Lastly, why would the absence of high-frequency information in prediction-based methods yield better explainability scores along multiple metrics? We analyze the gradient of three representative visual classification models and observe that it contains noisy information emanating from high-frequencies. Furthe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedSelect&#30340;&#26032;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23547;&#25214;&#26368;&#20339;&#23458;&#25143;&#31471;&#23376;&#32593;&#32476;&#20174;&#32780;&#30452;&#25509;&#20010;&#24615;&#21270;&#23458;&#25143;&#31471;&#23376;&#32593;&#32476;&#32467;&#26500;&#21644;&#21442;&#25968;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#20840;&#23616;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#23458;&#25143;&#31471;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.13264</link><description>&lt;p&gt;
FedSelect: &#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#20013;&#21442;&#25968;&#33258;&#23450;&#20041;&#36873;&#25321;&#30340;&#32454;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FedSelect: Customized Selection of Parameters for Fine-Tuning during Personalized Federated Learning. (arXiv:2306.13264v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedSelect&#30340;&#26032;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23547;&#25214;&#26368;&#20339;&#23458;&#25143;&#31471;&#23376;&#32593;&#32476;&#20174;&#32780;&#30452;&#25509;&#20010;&#24615;&#21270;&#23458;&#25143;&#31471;&#23376;&#32593;&#32476;&#32467;&#26500;&#21644;&#21442;&#25968;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#20840;&#23616;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#23458;&#25143;&#31471;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#22312;&#26412;&#22320;&#25968;&#25454;&#19978;&#24494;&#35843;&#23458;&#25143;&#31471;&#21442;&#25968;&#25110;&#38024;&#23545;&#26412;&#22320;&#20219;&#21153;&#20010;&#24615;&#21270;&#26550;&#26500;&#26469;&#25552;&#39640;&#23458;&#25143;&#31471;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#22312;&#29306;&#29298;&#37325;&#35201;&#30340;&#20840;&#23616;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20010;&#24615;&#21270;&#65292;&#35201;&#20040;&#22312;&#39044;&#20808;&#30830;&#23450;&#32593;&#32476;&#23618;&#20197;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#23548;&#33268;&#23458;&#25143;&#31471;&#27169;&#22411;&#20013;&#20840;&#23616;&#30693;&#35782;&#20648;&#23384;&#30340;&#19981;&#36275;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;FedSelect&#65292;&#36890;&#36807;&#21516;&#26102;&#25628;&#32034;&#24182;&#33719;&#24471;&#20010;&#24615;&#21270;&#26368;&#20339;&#21442;&#25968;&#21644;&#29992;&#20110;&#20840;&#23616;&#32858;&#21512;&#30340;&#20854;&#20313;&#21442;&#25968;&#65292;&#20174;&#32780;&#30452;&#25509;&#20010;&#24615;&#21270;&#23458;&#25143;&#23376;&#32593;&#32476;&#32467;&#26500;&#21644;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in federated learning (FL) seek to increase client-level performance by fine-tuning client parameters on local data or personalizing architectures for the local task. Existing methods for such personalization either prune a global model or fine-tune a global model on a local client distribution. However, these existing methods either personalize at the expense of retaining important global knowledge, or predetermine network layers for fine-tuning, resulting in suboptimal storage of global knowledge within client models. Enlightened by the lottery ticket hypothesis, we first introduce a hypothesis for finding optimal client subnetworks to locally fine-tune while leaving the rest of the parameters frozen. We then propose a novel FL framework, FedSelect, using this procedure that directly personalizes both client subnetwork structure and parameters, via the simultaneous discovery of optimal parameters for personalization and the rest of parameters for global aggregatio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;CompanyKG&#65292;&#19968;&#31181;&#29992;&#20110;&#20844;&#21496;&#30456;&#20284;&#24615;&#37327;&#21270;&#30340;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#20016;&#23500;&#30340;&#20844;&#21496;&#29305;&#24449;&#21644;&#20851;&#31995;&#34920;&#31034;&#65292;&#20197;&#21450;&#22810;&#20010;&#35780;&#20272;&#20219;&#21153;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20026;&#20844;&#21496;&#30456;&#20284;&#24615;&#37327;&#21270;&#26041;&#27861;&#30340;&#32508;&#21512;&#35780;&#20272;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2306.10649</link><description>&lt;p&gt;
CompanyKG:&#19968;&#31181;&#29992;&#20110;&#20844;&#21496;&#30456;&#20284;&#24615;&#37327;&#21270;&#30340;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;
&lt;/p&gt;
&lt;p&gt;
CompanyKG: A Large-Scale Heterogeneous Graph for Company Similarity Quantification. (arXiv:2306.10649v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;CompanyKG&#65292;&#19968;&#31181;&#29992;&#20110;&#20844;&#21496;&#30456;&#20284;&#24615;&#37327;&#21270;&#30340;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#20016;&#23500;&#30340;&#20844;&#21496;&#29305;&#24449;&#21644;&#20851;&#31995;&#34920;&#31034;&#65292;&#20197;&#21450;&#22810;&#20010;&#35780;&#20272;&#20219;&#21153;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20026;&#20844;&#21496;&#30456;&#20284;&#24615;&#37327;&#21270;&#26041;&#27861;&#30340;&#32508;&#21512;&#35780;&#20272;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25237;&#36164;&#34892;&#19994;&#20013;&#65292;&#23545;&#20110;&#35768;&#22810;&#30446;&#30340;&#21253;&#25324;&#24066;&#22330;&#26144;&#23556;&#12289;&#31454;&#20105;&#23545;&#25163;&#20998;&#26512;&#21644;&#24182;&#36141;&#65292;&#36827;&#34892;&#32454;&#31890;&#24230;&#20844;&#21496;&#30456;&#20284;&#24615;&#37327;&#21270;&#36890;&#24120;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#21517;&#20026;CompanyKG&#30340;&#30693;&#35782;&#22270;&#65292;&#29992;&#20110;&#34920;&#31034;&#21644;&#23398;&#20064;&#22810;&#26679;&#21270;&#30340;&#20844;&#21496;&#29305;&#24449;&#21644;&#20851;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;1.17&#30334;&#19975;&#23478;&#20844;&#21496;&#34987;&#34920;&#31034;&#20026;&#33410;&#28857;&#65292;&#20016;&#23500;&#20102;&#20844;&#21496;&#25551;&#36848;&#23884;&#20837;; 15&#31181;&#19981;&#21516;&#30340;&#20844;&#21496;&#38388;&#20851;&#31995;&#23548;&#33268;&#20102;5106&#30334;&#19975;&#20010;&#24102;&#26435;&#37325;&#30340;&#36793;&#12290;&#20026;&#20102;&#23454;&#29616;&#23545;&#20844;&#21496;&#30456;&#20284;&#24615;&#37327;&#21270;&#26041;&#27861;&#30340;&#20840;&#38754;&#35780;&#20272;&#65292;&#25105;&#20204;&#35774;&#35745;&#24182;&#32534;&#35793;&#20102;&#19977;&#20010;&#24102;&#26377;&#27880;&#37322;&#27979;&#35797;&#38598;&#30340;&#35780;&#20272;&#20219;&#21153;: &#30456;&#20284;&#24615;&#39044;&#27979;&#12289;&#31454;&#20105;&#23545;&#25163;&#26816;&#32034;&#21644;&#30456;&#20284;&#24615;&#25490;&#24207;&#12290;&#25105;&#20204;&#23545;11&#31181;&#21487;&#37325;&#29616;&#39044;&#27979;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20998;&#20026;&#33410;&#28857;&#12289;&#36793;&#21644;&#33410;&#28857;+&#36793;&#19977;&#32452;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;CompanyKG&#26159;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#24322;&#26500;&#22270;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
In the investment industry, it is often essential to carry out fine-grained company similarity quantification for a range of purposes, including market mapping, competitor analysis, and mergers and acquisitions. We propose and publish a knowledge graph, named CompanyKG, to represent and learn diverse company features and relations. Specifically, 1.17 million companies are represented as nodes enriched with company description embeddings; and 15 different inter-company relations result in 51.06 million weighted edges. To enable a comprehensive assessment of methods for company similarity quantification, we have devised and compiled three evaluation tasks with annotated test sets: similarity prediction, competitor retrieval and similarity ranking. We present extensive benchmarking results for 11 reproducible predictive methods categorized into three groups: node-only, edge-only, and node+edge. To the best of our knowledge, CompanyKG is the first large-scale heterogeneous graph dataset or
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Strokes2Surface&#65292;&#23427;&#21487;&#20174;&#24314;&#31569;&#24072;&#30340;&#31508;&#30011;&#20013;&#24674;&#22797;&#20986;&#26354;&#32447;&#32593;&#32476;&#65292;&#23545;&#20110;&#24314;&#31569;&#35774;&#35745;&#20013;&#30340;&#27010;&#24565;&#35774;&#35745;&#21644;&#25968;&#23383;&#24314;&#27169;&#20043;&#38388;&#30340;&#26725;&#26753;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2306.07220</link><description>&lt;p&gt;
Strokes2Surface&#65306;&#20174;&#22235;&#32500;&#24314;&#31569;&#35774;&#35745;&#32032;&#25551;&#20013;&#24674;&#22797;&#26354;&#32447;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Strokes2Surface: Recovering Curve Networks From 4D Architectural Design Sketches. (arXiv:2306.07220v2 [cs.GR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Strokes2Surface&#65292;&#23427;&#21487;&#20174;&#24314;&#31569;&#24072;&#30340;&#31508;&#30011;&#20013;&#24674;&#22797;&#20986;&#26354;&#32447;&#32593;&#32476;&#65292;&#23545;&#20110;&#24314;&#31569;&#35774;&#35745;&#20013;&#30340;&#27010;&#24565;&#35774;&#35745;&#21644;&#25968;&#23383;&#24314;&#27169;&#20043;&#38388;&#30340;&#26725;&#26753;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31163;&#32447;&#20960;&#20309;&#37325;&#24314;&#31649;&#36947;Strokes2Surface&#65292;&#23427;&#26159;&#22522;&#20110;4D Sketching Interface&#65292;MR.Sketch&#30340;&#30446;&#26631;&#26159;&#38754;&#21521;&#24314;&#31569;&#35774;&#35745;&#30340;&#12290;&#35813;&#31649;&#36947;&#20174;&#35774;&#35745;&#24072;&#32472;&#21046;&#30340;&#31508;&#30011;&#20013;&#24674;&#22797;&#26354;&#32447;&#32593;&#32476;&#65292;&#22240;&#27492;&#22312;&#24314;&#31569;&#35774;&#35745;&#30340;&#27010;&#24565;&#35774;&#35745;&#21644;&#25968;&#23383;&#24314;&#27169;&#38454;&#27573;&#20043;&#38388;&#24314;&#31435;&#20102;&#26725;&#26753;&#12290;&#25105;&#20204;&#30340;&#31649;&#36947;&#30340;&#36755;&#20837;&#21253;&#25324;3D&#31508;&#30011;&#30340;&#25240;&#32447;&#39030;&#28857;&#21450;&#20854;&#30456;&#24212;&#30340;&#26102;&#38388;&#25139;&#65288;&#20316;&#20026;&#31532;&#22235;&#20010;&#32500;&#24230;&#65289;&#65292;&#20197;&#21450;&#39069;&#22806;&#30340;&#20960;&#20309;&#21644;&#31508;&#35302;&#30456;&#20851;&#30340;&#35760;&#24405;&#23646;&#24615;&#12290;&#22522;&#20110;&#32032;&#25551;&#21512;&#24182;&#21644;&#22522;&#20110;&#32032;&#25551;&#24314;&#27169;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#31649;&#36947;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#24182;&#32452;&#21512;&#19977;&#20010;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#65307;&#19968;&#20010;&#20998;&#31867;&#22120;&#21644;&#20004;&#20010;&#32858;&#31867;&#27169;&#22411;&#12290;&#29305;&#21035;&#26159;&#65292;&#26681;&#25454;&#24314;&#31569;&#35774;&#35745;&#32032;&#25551;&#20013;&#35774;&#35745;&#24072;&#36890;&#24120;&#37319;&#29992;&#30340;&#23454;&#36341;&#35266;&#23519;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#19968;&#20010;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#65292;&#20197;&#35782;&#21035;&#19968;&#31508;&#30011;&#26159;&#25551;&#32472;&#36793;&#30028;&#21644;&#36793;&#32536;&#36824;&#26159;&#29992;&#20110;&#22635;&#20805;&#25152;&#38656;&#24314;&#31569;&#29289;&#30340;&#23553;&#38381;&#21306;&#22495;&#21644;&#34920;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Strokes2Surface, an offline geometry-reconstruction pipeline built upon a 4D Sketching Interface, MR.Sketch, targeted at architectural design. The pipeline recovers a curve network from designer-drawn strokes, thus bridging between concept design and digital modeling stages in architectural design. The input to our pipeline consists of 3D strokes' polyline vertices and their corresponding timestamps (as of the fourth dimension), along with additional geometric and stylus-related recorded properties. Inspired by sketch consolidation and sketch-based modeling methods, our pipeline leverages such data and combines three Machine Learning (ML) models; a classifier and two clustering models. In particular, based on observations of practices designers typically employ in architectural design sketches, we solve a binary classification problem to recognize whether a stroke depicts a boundary and edge or is used to fill in the enclosing areas and faces of the intended architectural ob
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#21512;&#35780;&#20272;&#20102;&#22522;&#20110;&#21442;&#25968;&#30340;&#39640;&#25928;&#35843;&#25972;&#25216;&#26415;&#65288;PEFT&#65289;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#21487;&#33021;&#24212;&#29992;&#12290;&#36890;&#36807;&#36229;&#36807;600&#20010;&#25511;&#21046;&#27979;&#35797;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;PEFT&#30340;&#30456;&#23545;&#24615;&#33021;&#65292;&#24182;&#24378;&#35843;&#20102;PEFT&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#36716;&#31227;&#23398;&#20064;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.08252</link><description>&lt;p&gt;
&#22522;&#20110;&#21442;&#25968;&#30340;&#39640;&#25928;&#35843;&#25972;&#25216;&#26415;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65306;&#34987;&#24573;&#35270;&#30340;&#26426;&#20250;
&lt;/p&gt;
&lt;p&gt;
Parameter-Efficient Fine-Tuning for Medical Image Analysis: The Missed Opportunity. (arXiv:2305.08252v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08252
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#21512;&#35780;&#20272;&#20102;&#22522;&#20110;&#21442;&#25968;&#30340;&#39640;&#25928;&#35843;&#25972;&#25216;&#26415;&#65288;PEFT&#65289;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#21487;&#33021;&#24212;&#29992;&#12290;&#36890;&#36807;&#36229;&#36807;600&#20010;&#25511;&#21046;&#27979;&#35797;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;PEFT&#30340;&#30456;&#23545;&#24615;&#33021;&#65292;&#24182;&#24378;&#35843;&#20102;PEFT&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#36716;&#31227;&#23398;&#20064;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#22522;&#20110;&#21442;&#25968;&#30340;&#39640;&#25928;&#35843;&#25972;&#25216;&#26415;&#65288;PEFT&#65289;&#22312;&#22810;&#26679;&#21270;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#32508;&#21512;&#35780;&#20272;&#12290;PEFT&#36234;&#26469;&#36234;&#34987;&#29992;&#20316;&#30693;&#35782;&#36716;&#31227;&#30340;&#26377;&#20215;&#20540;&#26041;&#27861;&#65292;&#20174;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35270;&#35273;&#12289;&#35821;&#38899;&#20197;&#21450;&#36328;&#27169;&#24577;&#20219;&#21153;&#65292;&#20363;&#22914;&#35270;&#35273;&#35821;&#35328;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#65292;&#23427;&#30340;&#24212;&#29992;&#20173;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#12290;&#38543;&#30528;&#22522;&#30784;&#27169;&#22411;&#22312;&#21307;&#23398;&#39046;&#22495;&#36234;&#26469;&#36234;&#34987;&#21033;&#29992;&#65292;&#35843;&#26597;&#21644;&#27604;&#36739;&#35780;&#20272;&#21508;&#31181;&#30693;&#35782;&#36716;&#31227;&#31574;&#30053;&#21487;&#20197;&#22686;&#24378;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26159;&#20854;&#31867;&#21035;&#20013;&#31532;&#19968;&#20010;&#65288;&#25454;&#25105;&#20204;&#25152;&#30693;&#65289;&#65292;&#35780;&#20272;&#20102;16&#31181;&#21367;&#31215;&#21644;&#22522;&#20110;&#36716;&#25442;&#22120;&#32593;&#32476;&#30340;PEFT&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#20845;&#20010;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#22270;&#20687;&#20998;&#31867;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#22312;&#22823;&#23567;&#65292;&#27169;&#24577;&#21644;&#22797;&#26434;&#24615;&#19978;&#26377;&#25152;&#19981;&#21516;&#12290;&#36890;&#36807;&#36229;&#36807;600&#20010;&#25511;&#21046;&#27979;&#35797;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#27599;&#31181;&#26041;&#27861;&#30340;&#30456;&#23545;&#24615;&#33021;&#65292;&#24182;&#30830;&#23450;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#36884;&#24452;&#65292;&#24182;&#24378;&#35843;&#20102;PEFT&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#36716;&#31227;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#24182;&#21628;&#21505;&#22312;&#26410;&#26469;&#30740;&#31350;&#20013;&#21152;&#20197;&#24191;&#27867;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a comprehensive evaluation of Parameter-Efficient Fine-Tuning (PEFT) techniques for diverse medical image analysis tasks. PEFT is increasingly exploited as a valuable approach for knowledge transfer from pre-trained models in natural language processing, vision, speech, and cross-modal tasks, such as vision-language and text-to-image generation. However, its application in medical image analysis remains relatively unexplored. As foundation models are increasingly exploited in the medical domain, it is crucial to investigate and comparatively assess various strategies for knowledge transfer that can bolster a range of downstream tasks. Our study, the first of its kind (to the best of our knowledge), evaluates 16 distinct PEFT methodologies proposed for convolutional and transformer-based networks, focusing on image classification and text-to-image generation tasks across six medical datasets ranging in size, modality, and complexity. Through a battery of more than 600 control
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#30693;&#35782;&#22270;&#35889;&#30340;&#32467;&#26500;&#21644;&#21160;&#24577;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#34920;&#23618;&#24615;&#26469;&#31616;&#21333;&#24314;&#27169;&#22797;&#26434;&#29305;&#24449;&#65292;&#20174;&#32780;&#25484;&#25569;&#20840;&#23616;&#30693;&#35782;&#20998;&#24067;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.08116</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#30340;&#32467;&#26500;&#21644;&#21160;&#24577;&#65292;&#20197;&#21450;&#20854;&#34920;&#23618;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
The Structure and Dynamics of Knowledge Graphs, with Superficiality. (arXiv:2305.08116v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08116
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#30693;&#35782;&#22270;&#35889;&#30340;&#32467;&#26500;&#21644;&#21160;&#24577;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#34920;&#23618;&#24615;&#26469;&#31616;&#21333;&#24314;&#27169;&#22797;&#26434;&#29305;&#24449;&#65292;&#20174;&#32780;&#25484;&#25569;&#20840;&#23616;&#30693;&#35782;&#20998;&#24067;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#30693;&#35782;&#22270;&#35889;&#32508;&#21512;&#20102;&#20174;&#23398;&#26415;&#26426;&#26500;&#21644;&#20225;&#19994;&#21040;&#22823;&#20247;&#38598;&#36164;&#31561;&#39033;&#30446;&#20013;&#33719;&#24471;&#30340;&#20154;&#31867;&#30693;&#35782;&#65292;&#27599;&#20004;&#20010;&#33410;&#28857;&#20043;&#38388;&#30340;&#20851;&#31995;&#20195;&#34920;&#36825;&#20004;&#20010;&#23454;&#20307;&#20043;&#38388;&#30340;&#22522;&#26412;&#20107;&#23454;&#12290;&#20851;&#31995;&#35821;&#20041;&#30340;&#22810;&#26679;&#24615;&#32452;&#25104;&#20102;&#30693;&#35782;&#22270;&#35889;&#30340;&#20016;&#23500;&#24615;&#65292;&#23548;&#33268;&#20986;&#29616;&#26377;&#26102;&#28151;&#20081;&#30340;&#22855;&#24322;&#25299;&#25169;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22797;&#26434;&#29305;&#24449;&#21487;&#20197;&#36890;&#36807;&#24341;&#20837;&#34920;&#23618;&#24615;&#30340;&#27010;&#24565;&#26469;&#31616;&#21333;&#24314;&#27169;&#65292;&#34920;&#23618;&#24615;&#25511;&#21046;&#30528;&#29420;&#31435;&#29983;&#25104;&#20107;&#23454;&#30340;&#20851;&#31995;&#20043;&#38388;&#30340;&#37325;&#21472;&#24773;&#20917;&#65292;&#20063;&#36890;&#36807;&#30830;&#23450;&#38169;&#35823;&#25551;&#36848;&#23454;&#20307;&#30340;&#27604;&#20363;&#26469;&#25511;&#21046;&#20840;&#23616;&#30693;&#35782;&#20998;&#24067;&#30340;&#24179;&#34913;&#12290;&#36825;&#26159;&#30693;&#35782;&#22270;&#35889;&#32467;&#26500;&#21644;&#21160;&#24577;&#26041;&#38754;&#30340;&#39318;&#20010;&#27169;&#22411;&#65292;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;&#27491;&#24335;&#30693;&#35782;&#33719;&#21462;&#21644;&#32452;&#32455;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large knowledge graphs combine human knowledge garnered from projects ranging from academia and institutions to enterprises and crowdsourcing. Within such graphs, each relationship between two nodes represents a basic fact involving these two entities. The diversity of the semantics of relationships constitutes the richness of knowledge graphs, leading to the emergence of singular topologies, sometimes chaotic in appearance. However, this complex characteristic can be modeled in a simple way by introducing the concept of superficiality, which controls the overlap between relationships whose facts are generated independently. Superficiality also regulates the balance of the global distribution of knowledge by determining the proportion of misdescribed entities. This is the first model for the structure and dynamics of knowledge graphs. It leads to a better understanding of formal knowledge acquisition and organization.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29616;&#26377;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#20551;&#35774;&#25152;&#26377;&#26816;&#32034;&#20449;&#24687;&#37117;&#26159;&#27491;&#30830;&#30340;&#20551;&#35774;&#30340;&#38382;&#39064;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#33021;&#23384;&#22312;&#34394;&#20551;&#20449;&#24687;&#23548;&#33268;&#20914;&#31361;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#37492;&#21035;&#22120;&#21644;&#25552;&#31034;&#37492;&#21035;&#33021;&#21147;&#24341;&#20986;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#36825;&#26174;&#33879;&#25913;&#21892;&#20102;&#27169;&#22411;&#22312;&#30693;&#35782;&#20914;&#31361;&#19979;&#30340;&#25928;&#26524;&#65307;&#21516;&#26102;&#25552;&#20379;&#20102;&#20851;&#20110;&#20132;&#26367;&#31934;&#32454;&#35843;&#25972;&#27169;&#22411;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26032;&#30340;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2305.01579</link><description>&lt;p&gt;
&#21306;&#20998;&#21644;&#22238;&#31572;&#65306;&#36890;&#36807;&#36776;&#21035;&#22120;&#32531;&#35299;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#20013;&#34394;&#20551;&#20449;&#24687;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Discern and Answer: Mitigating the Impact of Misinformation in Retrieval-Augmented Models with Discriminators. (arXiv:2305.01579v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29616;&#26377;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#20551;&#35774;&#25152;&#26377;&#26816;&#32034;&#20449;&#24687;&#37117;&#26159;&#27491;&#30830;&#30340;&#20551;&#35774;&#30340;&#38382;&#39064;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#33021;&#23384;&#22312;&#34394;&#20551;&#20449;&#24687;&#23548;&#33268;&#20914;&#31361;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#37492;&#21035;&#22120;&#21644;&#25552;&#31034;&#37492;&#21035;&#33021;&#21147;&#24341;&#20986;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#36825;&#26174;&#33879;&#25913;&#21892;&#20102;&#27169;&#22411;&#22312;&#30693;&#35782;&#20914;&#31361;&#19979;&#30340;&#25928;&#26524;&#65307;&#21516;&#26102;&#25552;&#20379;&#20102;&#20851;&#20110;&#20132;&#26367;&#31934;&#32454;&#35843;&#25972;&#27169;&#22411;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26032;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20551;&#23450;&#25152;&#26377;&#26816;&#32034;&#21040;&#30340;&#20449;&#24687;&#37117;&#26159;&#20107;&#23454;&#19978;&#27491;&#30830;&#30340;&#12290;&#26412;&#25991;&#30740;&#31350;&#19968;&#20010;&#26356;&#21152;&#29616;&#23454;&#30340;&#22330;&#26223;&#65292;&#21363;&#26816;&#32034;&#21040;&#30340;&#25991;&#26723;&#21487;&#33021;&#21253;&#21547;&#34394;&#20551;&#20449;&#24687;&#65292;&#20174;&#32780;&#23548;&#33268;&#23427;&#20204;&#20043;&#38388;&#23384;&#22312;&#20914;&#31361;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#29616;&#26377;&#27169;&#22411;&#22312;&#31934;&#35843;&#21644;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#23398;&#20064;&#35774;&#32622;&#20013;&#23545;&#36825;&#31181;&#20449;&#24687;&#39640;&#24230;&#33030;&#24369;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#65292;&#36890;&#36807;&#26126;&#30830;&#22320;&#23545;&#37492;&#21035;&#22120;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#25110;&#25552;&#31034;&#26469;&#24341;&#20986;GPT-3&#30340;&#37492;&#21035;&#33021;&#21147;&#65292;&#20351;&#26816;&#32034;&#22686;&#24378;LM&#23545;&#34394;&#20551;&#20449;&#24687;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#22312;&#24320;&#25918;&#22495;&#38382;&#31572;&#26041;&#38754;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#26174;&#33879;&#25913;&#21892;&#20102;LM&#23545;&#30693;&#35782;&#20914;&#31361;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20851;&#20110;&#20132;&#26367;&#31934;&#32454;&#35843;&#25972;&#27169;&#22411;&#30340;&#20915;&#31574;&#19982;&#19978;&#19979;&#25991;&#23398;&#20064;&#36807;&#31243;&#30340;&#21457;&#29616;&#65292;&#20026;&#21033;&#29992;&#20004;&#32773;&#30340;&#26368;&#20339;&#26041;&#24335;&#38138;&#24179;&#20102;&#26032;&#30340;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing retrieval-augmented language models (LMs) for question answering assume all retrieved information is factually correct. In this work, we study a more realistic scenario in which retrieved documents may contain misinformation, causing conflicts among them. We observe that the existing models are highly brittle to such information in both fine-tuning and in-context few-shot learning settings. We propose approaches to make retrieval-augmented LMs robust to misinformation by explicitly fine-tuning a discriminator or prompting to elicit discrimination capability in GPT-3. Our empirical results on open-domain question answering show that these approaches significantly improve LMs' robustness to knowledge conflicts. We also provide our findings on interleaving the fine-tuned model's decision with the in-context learning process, paving a new path to leverage the best of both worlds.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23457;&#35745;&#26694;&#26550;&#65292;&#33021;&#22815;&#20197;&#20840;&#38754;&#30340;&#26041;&#24335;&#35780;&#20272;&#21512;&#25104;&#25968;&#25454;&#21644;AI&#27169;&#22411;&#30340;&#20855;&#20307;&#25928;&#26524;&#65292;&#21253;&#25324;&#20559;&#35265;&#21644;&#27495;&#35270;&#39044;&#38450;&#12289;&#23545;&#30495;&#23454;&#25968;&#25454;&#30340;&#24544;&#23454;&#31243;&#24230;&#12289;&#25928;&#29992;&#12289;&#40065;&#26834;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;&#22312;&#22810;&#20010;&#29992;&#20363;&#20013;&#65292;&#23457;&#35745;&#26694;&#26550;&#24179;&#34913;&#20102;&#20449;&#20219;&#21644;&#25928;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2304.10819</link><description>&lt;p&gt;
&#21487;&#25511;&#30340;&#20449;&#20219;&#26435;&#34913;&#19979;&#30340;&#21512;&#25104;&#25968;&#25454;&#23457;&#35745;&#19982;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Auditing and Generating Synthetic Data with Controllable Trust Trade-offs. (arXiv:2304.10819v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23457;&#35745;&#26694;&#26550;&#65292;&#33021;&#22815;&#20197;&#20840;&#38754;&#30340;&#26041;&#24335;&#35780;&#20272;&#21512;&#25104;&#25968;&#25454;&#21644;AI&#27169;&#22411;&#30340;&#20855;&#20307;&#25928;&#26524;&#65292;&#21253;&#25324;&#20559;&#35265;&#21644;&#27495;&#35270;&#39044;&#38450;&#12289;&#23545;&#30495;&#23454;&#25968;&#25454;&#30340;&#24544;&#23454;&#31243;&#24230;&#12289;&#25928;&#29992;&#12289;&#40065;&#26834;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;&#22312;&#22810;&#20010;&#29992;&#20363;&#20013;&#65292;&#23457;&#35745;&#26694;&#26550;&#24179;&#34913;&#20102;&#20449;&#20219;&#21644;&#25928;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#20013;&#25910;&#38598;&#30340;&#25968;&#25454;&#24448;&#24448;&#23384;&#22312;&#20559;&#24046;&#12289;&#19981;&#24179;&#34913;&#65292;&#24182;&#19988;&#26377;&#27844;&#38706;&#25935;&#24863;&#21644;&#38544;&#31169;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;&#36825;&#19968;&#20107;&#23454;&#24341;&#21457;&#20102;&#21019;&#24314;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#24819;&#27861;&#65292;&#20197;&#20943;&#36731;&#30495;&#23454;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#39118;&#38505;&#12289;&#20559;&#35265;&#12289;&#20260;&#23475;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;&#36825;&#20010;&#27010;&#24565;&#20381;&#36182;&#20110;&#29983;&#25104;AI&#27169;&#22411;&#65292;&#20197;&#20135;&#29983;&#19981;&#20559;&#25191;&#12289;&#20445;&#25252;&#38544;&#31169;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#21516;&#26102;&#24544;&#23454;&#20110;&#30495;&#23454;&#25968;&#25454;&#12290;&#22312;&#36825;&#31181;&#26032;&#33539;&#24335;&#20013;&#65292;&#25105;&#20204;&#22914;&#20309;&#30693;&#36947;&#36825;&#31181;&#26041;&#27861;&#26159;&#21542;&#20817;&#29616;&#20102;&#20854;&#25215;&#35834;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23457;&#35745;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#23545;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#22522;&#20110;&#23427;&#20204;&#35757;&#32451;&#30340;AI&#27169;&#22411;&#30340;&#20840;&#38754;&#35780;&#20272;&#65292;&#22260;&#32469;&#20559;&#35265;&#21644;&#27495;&#35270;&#30340;&#39044;&#38450;&#12289;&#23545;&#30495;&#23454;&#25968;&#25454;&#30340;&#24544;&#23454;&#31243;&#24230;&#12289;&#25928;&#29992;&#12289;&#40065;&#26834;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;&#25105;&#20204;&#36890;&#36807;&#23457;&#35745;&#22810;&#20010;&#29983;&#25104;&#27169;&#22411;&#22312;&#19981;&#21516;&#29992;&#20363;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#25945;&#32946;&#12289;&#21307;&#30103;&#20445;&#20581;&#12289;&#38134;&#34892;&#12289;&#20154;&#21147;&#36164;&#28304;&#65292;&#20197;&#21450;&#20174;&#34920;&#26684;&#65292;&#26102;&#38388;&#24207;&#21015;&#21040;&#33258;&#28982;&#35821;&#35328;&#30340;&#19981;&#21516;&#27169;&#24577;&#12290;&#25105;&#20204;&#30340;&#29992;&#20363;&#23637;&#31034;&#20102;&#22312;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#20013;&#24179;&#34913;&#20449;&#20219;&#21644;&#25928;&#29992;&#30340;&#26435;&#34913;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data collected from the real world tends to be biased, unbalanced, and at risk of exposing sensitive and private information. This reality has given rise to the idea of creating synthetic datasets to alleviate risk, bias, harm, and privacy concerns inherent in the real data. This concept relies on Generative AI models to produce unbiased, privacy-preserving synthetic data while being true to the real data. In this new paradigm, how can we tell if this approach delivers on its promises? We present an auditing framework that offers a holistic assessment of synthetic datasets and AI models trained on them, centered around bias and discrimination prevention, fidelity to the real data, utility, robustness, and privacy preservation. We showcase our framework by auditing multiple generative models on diverse use cases, including education, healthcare, banking, human resources, and across different modalities, from tabular, to time-series, to natural language. Our use cases demonstrate the imp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#22312;&#23460;&#20869;&#29615;&#22659;&#19979;&#36827;&#34892;&#36816;&#21160;&#30446;&#26631;&#30340;&#23450;&#20301;&#65292;&#20351;&#29992;&#20102;&#32467;&#26500;&#36816;&#21160;&#19982;&#27169;&#25311;&#25968;&#25454;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#12290;&#30740;&#31350;&#32773;&#25972;&#21512;&#20809;&#27969;&#21644;&#30456;&#23545;&#23039;&#24577;&#22238;&#24402;&#26041;&#27861;&#24110;&#21161;&#35299;&#20915;&#20102;&#22240;&#36816;&#21160;&#27169;&#31946;&#12289;&#20809;&#29031;&#21464;&#21270;&#12289;&#37325;&#22797;&#22270;&#26696;&#21644;&#32570;&#20047;&#29305;&#24449;&#32467;&#26500;&#31561;&#38382;&#39064;&#32780;&#24102;&#26469;&#30340;&#29942;&#39048;&#65292;&#20026;&#23460;&#20869;&#30446;&#26631;&#23450;&#20301;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.07250</link><description>&lt;p&gt;
&#36890;&#36807;&#20174;&#20809;&#27969;&#20449;&#24687;&#20013;&#34701;&#21512;&#36816;&#21160;&#32467;&#26500;&#19982;&#27169;&#25311;&#25968;&#25454;&#30340;&#32477;&#23545;&#20301;&#32622;&#22238;&#24402;&#65292;&#35299;&#20915;&#23460;&#20869;&#29615;&#22659;&#23450;&#20301;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Fusing Structure from Motion and Simulation-Augmented Pose Regression from Optical Flow for Challenging Indoor Environments. (arXiv:2304.07250v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#22312;&#23460;&#20869;&#29615;&#22659;&#19979;&#36827;&#34892;&#36816;&#21160;&#30446;&#26631;&#30340;&#23450;&#20301;&#65292;&#20351;&#29992;&#20102;&#32467;&#26500;&#36816;&#21160;&#19982;&#27169;&#25311;&#25968;&#25454;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#12290;&#30740;&#31350;&#32773;&#25972;&#21512;&#20809;&#27969;&#21644;&#30456;&#23545;&#23039;&#24577;&#22238;&#24402;&#26041;&#27861;&#24110;&#21161;&#35299;&#20915;&#20102;&#22240;&#36816;&#21160;&#27169;&#31946;&#12289;&#20809;&#29031;&#21464;&#21270;&#12289;&#37325;&#22797;&#22270;&#26696;&#21644;&#32570;&#20047;&#29305;&#24449;&#32467;&#26500;&#31561;&#38382;&#39064;&#32780;&#24102;&#26469;&#30340;&#29942;&#39048;&#65292;&#20026;&#23460;&#20869;&#30446;&#26631;&#23450;&#20301;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#30340;&#23450;&#20301;&#26159;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#27604;&#22914;&#26426;&#22120;&#20154;&#12289;&#34394;&#25311;&#21644;&#22686;&#24378;&#29616;&#23454;&#12289;&#21644;&#22312;&#20179;&#24211;&#20013;&#36816;&#36865;&#36135;&#29289;&#12290;&#28145;&#24230;&#23398;&#20064;&#30340;&#20808;&#36827;&#21457;&#23637;&#24050;&#32463;&#20351;&#24471;&#20351;&#29992;&#21333;&#30446;&#35270;&#35273;&#30456;&#26426;&#36827;&#34892;&#23450;&#20301;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#26159;&#30001;&#20110;&#29615;&#22659;&#26412;&#36523;&#24341;&#36215;&#30340;&#38382;&#39064;&#65292;&#20363;&#22914;&#36816;&#21160;&#27169;&#31946;&#12289;&#20809;&#29031;&#21464;&#21270;&#12289;&#37325;&#22797;&#22270;&#26696;&#21644;&#32570;&#20047;&#29305;&#24449;&#30340;&#32467;&#26500;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#34701;&#21512;&#38468;&#21152;&#20449;&#24687;&#21644;&#20351;&#29992;&#30456;&#23545;&#20301;&#32622;&#22238;&#24402;&#65288;RPR&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#20351;&#29992;Lucas-Kanade&#31639;&#27861;&#35745;&#31639;&#36830;&#32493;&#22270;&#20687;&#20043;&#38388;&#30340;&#20809;&#27969;&#65292;&#24182;&#20351;&#29992;&#36741;&#21161;&#23567;&#22411;&#24490;&#29615;&#21367;&#31215;&#32593;&#32476;&#26469;&#39044;&#27979;&#30456;&#23545;&#23039;&#24577;&#12290;&#23558;&#32477;&#23545;&#23039;&#24577;&#21644;&#30456;&#23545;&#23039;&#24577;&#36827;&#34892;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The localization of objects is a crucial task in various applications such as robotics, virtual and augmented reality, and the transportation of goods in warehouses. Recent advances in deep learning have enabled the localization using monocular visual cameras. While structure from motion (SfM) predicts the absolute pose from a point cloud, absolute pose regression (APR) methods learn a semantic understanding of the environment through neural networks. However, both fields face challenges caused by the environment such as motion blur, lighting changes, repetitive patterns, and feature-less structures. This study aims to address these challenges by incorporating additional information and regularizing the absolute pose using relative pose regression (RPR) methods. The optical flow between consecutive images is computed using the Lucas-Kanade algorithm, and the relative pose is predicted using an auxiliary small recurrent convolutional network. The fusion of absolute and relative poses is
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#26435;&#37325;&#28388;&#27874;&#30340;&#22810;&#31867;&#21487;&#35299;&#37322;&#24615;&#21368;&#36733;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#21333;&#20010;&#26410;&#35757;&#32451;&#36718;&#20013;&#21462;&#28040;&#23398;&#20064;&#32593;&#32476;&#30340;&#25152;&#26377;&#31867;&#21035;&#65292;&#24182;&#19988;&#24674;&#22797;&#21487;&#35299;&#37322;&#30340;&#31867;&#21035;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2304.02049</link><description>&lt;p&gt;
&#22522;&#20110;&#26435;&#37325;&#28388;&#27874;&#30340;&#22810;&#31867;&#21487;&#35299;&#37322;&#24615;&#21368;&#36733;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Multi-Class Explainable Unlearning for Image Classification via Weight Filtering. (arXiv:2304.02049v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02049
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#26435;&#37325;&#28388;&#27874;&#30340;&#22810;&#31867;&#21487;&#35299;&#37322;&#24615;&#21368;&#36733;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#21333;&#20010;&#26410;&#35757;&#32451;&#36718;&#20013;&#21462;&#28040;&#23398;&#20064;&#32593;&#32476;&#30340;&#25152;&#26377;&#31867;&#21035;&#65292;&#24182;&#19988;&#24674;&#22797;&#21487;&#35299;&#37322;&#30340;&#31867;&#21035;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#21368;&#36733;&#26159;&#26368;&#36817;&#28014;&#29616;&#30340;&#19968;&#31181;&#36873;&#25321;&#24615;&#22320;&#23558;&#35757;&#32451;&#25968;&#25454;&#28857;&#30340;&#24433;&#21709;&#20174;&#32593;&#32476;&#20013;&#21024;&#38500;&#30340;&#33539;&#24335;&#12290;&#23613;&#31649;&#29616;&#26377;&#26041;&#27861;&#24050;&#32463;&#38598;&#20013;&#22312;&#21368;&#36733;&#35757;&#32451;&#25968;&#25454;&#30340;&#23567;&#23376;&#38598;&#25110;&#21333;&#20010;&#31867;&#21035;&#65292;&#20294;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#21462;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#21333;&#20010;&#26410;&#35757;&#32451;&#36718;&#20013;&#21462;&#28040;&#23398;&#20064;&#22270;&#20687;&#20998;&#31867;&#32593;&#32476;&#30340;&#25152;&#26377;&#31867;&#21035;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#20869;&#37096;&#32452;&#20214;&#30340;&#35760;&#24518;&#30697;&#38453;&#26469;&#35843;&#33410;&#22270;&#20687;&#20998;&#31867;&#32593;&#32476;&#65292;&#20197;&#20415;&#22312;&#35757;&#32451;&#21518;&#65292;&#21516;&#19968;&#32593;&#32476;&#21487;&#20197;&#26377;&#36873;&#25321;&#22320;&#23637;&#31034;&#20219;&#20309;&#31867;&#21035;&#30340;&#26410;&#23398;&#20064;&#34892;&#20026;&#12290;&#36890;&#36807;&#21457;&#29616;&#27599;&#20010;&#31867;&#21035;&#29305;&#23450;&#30340;&#26435;&#37325;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#36890;&#36807;&#35774;&#35745;&#21487;&#35299;&#37322;&#24615;&#26426;&#21046;&#26469;&#24674;&#22797;&#31867;&#21035;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;&#23567;&#35268;&#27169;&#21644;&#20013;&#35268;&#27169;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;CNN&#21644;Transformer-based&#39592;&#26550;&#27979;&#35797;&#20102;&#25552;&#20986;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#19968;&#31181;&#22810;&#31867;&#21487;&#35299;&#37322;&#24615;&#21368;&#36733;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Unlearning has recently been emerging as a paradigm for selectively removing the impact of training datapoints from a network. While existing approaches have focused on unlearning either a small subset of the training data or a single class, in this paper we take a different path and devise a framework that can unlearn all classes of an image classification network in a single untraining round. Our proposed technique learns to modulate the inner components of an image classification network through memory matrices so that, after training, the same network can selectively exhibit an unlearning behavior over any of the classes. By discovering weights which are specific to each of the classes, our approach also recovers a representation of the classes which is explainable by-design. We test the proposed framework, which we name Weight Filtering network (WF-Net), on small-scale and medium-scale image classification datasets, with both CNN and Transformer-based backbones. Our work p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#24615;&#21644;&#19981;&#24179;&#34913;&#21322;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#30740;&#31350;&#65292;&#21033;&#29992;&#20266;&#26631;&#31614;&#26679;&#26412;&#36827;&#34892;&#33258;&#25105;&#35757;&#32451;&#26469;&#35299;&#20915;&#30382;&#32932;&#30142;&#30149;&#20998;&#31867;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2211.10858</link><description>&lt;p&gt;
&#22522;&#20110;&#21487;&#35299;&#37322;&#24615;&#21644;&#19981;&#24179;&#34913;&#21322;&#30417;&#30563;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#25913;&#21892;&#30382;&#32932;&#30142;&#30149;&#19981;&#21516;&#35786;&#26029;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An interpretable imbalanced semi-supervised deep learning framework for improving differential diagnosis of skin diseases. (arXiv:2211.10858v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#24615;&#21644;&#19981;&#24179;&#34913;&#21322;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#30740;&#31350;&#65292;&#21033;&#29992;&#20266;&#26631;&#31614;&#26679;&#26412;&#36827;&#34892;&#33258;&#25105;&#35757;&#32451;&#26469;&#35299;&#20915;&#30382;&#32932;&#30142;&#30149;&#20998;&#31867;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30382;&#32932;&#30142;&#30149;&#26159;&#19990;&#30028;&#19978;&#26368;&#24120;&#35265;&#30340;&#30142;&#30149;&#20043;&#19968;&#12290;&#26412;&#25991;&#21033;&#29992;58,457&#24352;&#30382;&#32932;&#22270;&#20687;&#21644;10,857&#20010;&#26410;&#26631;&#35760;&#26679;&#26412;&#65292;&#23545;&#22810;&#31867;&#26234;&#33021;&#30382;&#32932;&#35786;&#26029;&#26694;&#26550;&#65288;ISDL&#65289;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#19981;&#24179;&#34913;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#36890;&#36807;&#22312;&#19981;&#24179;&#34913;&#24615;&#31867;&#21035;&#30340;&#20266;&#26631;&#31614;&#26679;&#26412;&#19978;&#33258;&#25105;&#35757;&#32451;&#65292;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#20351;&#23569;&#25968;&#31867;&#21035;&#30340;&#20266;&#26631;&#31614;&#26679;&#26412;&#20855;&#26377;&#26356;&#39640;&#30340;&#27010;&#29575;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#26410;&#26631;&#35760;&#26679;&#26412;&#30340;&#21033;&#29992;&#65292;&#35299;&#20915;&#20102;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;ISDL&#22312;&#22810;&#26631;&#31614;&#30382;&#32932;&#30149;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#65292;&#20934;&#30830;&#29575;&#20026;0.979&#65292;&#28789;&#25935;&#24230;&#20026;0.975&#65292;&#29305;&#24322;&#24230;&#20026;0.973&#65292;&#23439;F1&#20998;&#25968;&#20026;0.974&#65292;&#25509;&#25910;&#32773;&#25805;&#20316;&#29305;&#24449;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#20026;0.999&#12290;&#25105;&#20204;&#36824;&#23558;Shapley Additive explanation (SHAP)&#26041;&#27861;&#19982;ISDL&#32467;&#21512;&#65292;&#35299;&#37322;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#26041;&#24335;&#12290;&#36825;&#19968;&#21457;&#29616;&#19982;&#20020;&#24202;&#35786;&#26029;&#19968;&#33268;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#26679;&#20998;&#24067;&#26368;&#20248;&#21270;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dermatological diseases are among the most common disorders worldwide. This paper presents the first study of the interpretability and imbalanced semi-supervised learning of the multiclass intelligent skin diagnosis framework (ISDL) using 58,457 skin images with 10,857 unlabeled samples. Pseudo-labelled samples from minority classes have a higher probability at each iteration of class-rebalancing self-training, thereby promoting the utilization of unlabeled samples to solve the class imbalance problem. Our ISDL achieved a promising performance with an accuracy of 0.979, sensitivity of 0.975, specificity of 0.973, macro-F1 score of 0.974 and area under the receiver operating characteristic curve (AUC) of 0.999 for multi-label skin disease classification. The Shapley Additive explanation (SHAP) method is combined with our ISDL to explain how the deep learning model makes predictions. This finding is consistent with the clinical diagnosis. We also proposed a sampling distribution optimisa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#27880;&#24847;&#21147;&#31070;&#32463;&#31639;&#23376;&#65288;HANO&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#23610;&#24230;&#20559;&#24494;&#20998;&#26041;&#31243;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#20809;&#35889;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.10890</link><description>&lt;p&gt;
&#32531;&#35299;&#20998;&#23618;&#27880;&#24847;&#21147;&#22810;&#23610;&#24230;&#31639;&#23376;&#23398;&#20064;&#20013;&#30340;&#20809;&#35889;&#20559;&#24046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Mitigating spectral bias for the multiscale operator learning with hierarchical attention. (arXiv:2210.10890v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#27880;&#24847;&#21147;&#31070;&#32463;&#31639;&#23376;&#65288;HANO&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#23610;&#24230;&#20559;&#24494;&#20998;&#26041;&#31243;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#20809;&#35889;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31639;&#23376;&#24050;&#32463;&#25104;&#20026;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#26080;&#38480;&#32500;&#21442;&#25968;&#21644;&#35299;&#31354;&#38388;&#20043;&#38388;&#26144;&#23556;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#26412;&#25991;&#20851;&#27880;&#20110;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#30340;&#22810;&#23610;&#24230;PDE&#65292;&#22914;&#27833;&#34255;&#24314;&#27169;&#21644;&#28237;&#27969;&#39044;&#27979;&#12290;&#25105;&#20204;&#35777;&#26126;&#23545;&#20110;&#36825;&#31181;PDE&#65292;&#23545;&#20302;&#39057;&#20998;&#37327;&#23384;&#22312;&#20809;&#35889;&#20559;&#24046;&#26159;&#29616;&#26377;&#31070;&#32463;&#31639;&#23376;&#30340;&#19968;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#23618;&#27425;&#30697;&#38453;&#26041;&#27861;&#21551;&#21457;&#30340;&#20998;&#23618;&#27880;&#24847;&#21147;&#31070;&#32463;&#31639;&#23376;&#65288;HANO&#65289;&#12290;HANO&#20855;&#26377;&#33258;&#36866;&#24212;&#23610;&#24230;&#20132;&#20114;&#33539;&#22260;&#21644;&#23618;&#27425;&#32467;&#26500;&#19978;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#33021;&#22815;&#23454;&#29616;&#21487;&#25511;&#32447;&#24615;&#25104;&#26412;&#30340;&#23884;&#22871;&#29305;&#24449;&#35745;&#31639;&#21644;&#22810;&#23610;&#24230;&#35299;&#31354;&#38388;&#30340;&#32534;&#30721;/&#35299;&#30721;&#12290;&#25105;&#20204;&#36824;&#37319;&#29992;&#32463;&#39564;H^1&#25439;&#22833;&#20989;&#25968;&#26469;&#22686;&#24378;&#23545;&#39640;&#39057;&#20998;&#37327;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;HANO&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65288;SOTA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural operators have emerged as a powerful tool for learning the mapping between infinite-dimensional parameter and solution spaces of partial differential equations (PDEs). In this work, we focus on multiscale PDEs that have important applications such as reservoir modeling and turbulence prediction. We demonstrate that for such PDEs, the spectral bias towards low-frequency components presents a significant challenge for existing neural operators. To address this challenge, we propose a hierarchical attention neural operator (HANO) inspired by the hierarchical matrix approach. HANO features a scale-adaptive interaction range and self-attentions over a hierarchy of levels, enabling nested feature computation with controllable linear cost and encoding/decoding of multiscale solution space. We also incorporate an empirical $H^1$ loss function to enhance the learning of high-frequency components. Our numerical experiments demonstrate that HANO outperforms state-of-the-art (SOTA) methods 
&lt;/p&gt;</description></item></channel></rss>