<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20174;&#20559;&#35265;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#29983;&#25104;&#23545;&#35805;&#20013;&#20844;&#24179;&#25991;&#26412;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#20844;&#24179;&#25991;&#26412;&#29983;&#25104;&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#35777;&#26126;&#20102;&#23398;&#20064;&#20844;&#24179;&#21487;&#20197;&#24402;&#32467;&#20026;&#25913;&#21892;&#20154;&#31867;&#26679;&#24335;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.04303</link><description>&lt;p&gt;
&#20174;&#20559;&#35265;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#29983;&#25104;&#23545;&#35805;&#20013;&#20844;&#24179;&#30340;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Learning to Generate Equitable Text in Dialogue from Biased Training Data. (arXiv:2307.04303v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04303
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20174;&#20559;&#35265;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#29983;&#25104;&#23545;&#35805;&#20013;&#20844;&#24179;&#25991;&#26412;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#20844;&#24179;&#25991;&#26412;&#29983;&#25104;&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#35777;&#26126;&#20102;&#23398;&#20064;&#20844;&#24179;&#21487;&#20197;&#24402;&#32467;&#20026;&#25913;&#21892;&#20154;&#31867;&#26679;&#24335;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#31995;&#32479;&#22312;&#20915;&#31574;&#36807;&#31243;&#21644;&#29983;&#25104;&#22238;&#22797;&#20013;&#27880;&#37325;&#20844;&#24179;&#21407;&#21017;&#23545;&#20110;&#29992;&#25143;&#30340;&#21442;&#19982;&#12289;&#28385;&#24847;&#24230;&#21644;&#20219;&#21153;&#23436;&#25104;&#33267;&#20851;&#37325;&#35201;&#12290;&#32570;&#20047;&#20844;&#24179;&#21644;&#21253;&#23481;&#21407;&#21017;&#21487;&#33021;&#38459;&#30861;&#20849;&#21516;&#22522;&#30784;&#30340;&#24418;&#25104;&#65292;&#20174;&#32780;&#23545;&#31995;&#32479;&#30340;&#25972;&#20307;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#23545;&#35805;&#20013;&#20844;&#24179;&#25991;&#26412;&#29983;&#25104;&#30340;&#32508;&#21512;&#30740;&#31350;&#36824;&#19981;&#20840;&#38754;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#35745;&#31639;&#23398;&#20064;&#30340;&#29702;&#35770;&#26469;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25991;&#26412;&#29983;&#25104;&#20013;&#20844;&#24179;&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#23398;&#20064;&#20154;&#31867;&#26679;&#24335;&#21644;&#23398;&#20064;&#20844;&#24179;&#20043;&#38388;&#30340;&#27491;&#24335;&#20851;&#32852;&#65306;&#25913;&#36827;&#20844;&#24179;&#30340;&#31639;&#27861;&#26368;&#32456;&#24402;&#32467;&#20026;&#25913;&#21892;&#20154;&#31867;&#26679;&#24335;&#30340;&#31639;&#27861;&#65288;&#22312;&#25193;&#20805;&#25968;&#25454;&#19978;&#65289;&#12290;&#22522;&#20110;&#36825;&#20010;&#29702;&#35299;&#65292;&#25105;&#20204;&#36824;&#21046;&#23450;&#20102;&#21512;&#29702;&#30340;&#26465;&#20214;&#65292;&#20351;&#24471;&#25991;&#26412;&#29983;&#25104;&#31639;&#27861;&#33021;&#22815;&#23398;&#20064;&#29983;&#25104;&#20844;&#24179;&#30340;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ingrained principles of fairness in a dialogue system's decision-making process and generated responses are crucial for user engagement, satisfaction, and task achievement. Absence of equitable and inclusive principles can hinder the formation of common ground, which in turn negatively impacts the overall performance of the system. For example, misusing pronouns in a user interaction may cause ambiguity about the intended subject. Yet, there is no comprehensive study of equitable text generation in dialogue. Aptly, in this work, we use theories of computational learning to study this problem. We provide formal definitions of equity in text generation, and further, prove formal connections between learning human-likeness and learning equity: algorithms for improving equity ultimately reduce to algorithms for improving human-likeness (on augmented data). With this insight, we also formulate reasonable conditions under which text generation algorithms can learn to generate equitable t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#19987;&#19994;&#38899;&#39057;&#24037;&#31243;&#24072;&#30340;&#38656;&#27714;&#65292;&#24182;&#25552;&#20986;&#20102;&#24403;&#21069;&#22312;&#38899;&#39057;&#29983;&#25104;&#26041;&#38754;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2307.04292</link><description>&lt;p&gt;
&#22522;&#20110;&#38656;&#27714;&#39537;&#21160;&#30340;&#29983;&#25104;&#38899;&#39057;AI&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Demand-Driven Perspective on Generative Audio AI. (arXiv:2307.04292v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#19987;&#19994;&#38899;&#39057;&#24037;&#31243;&#24072;&#30340;&#38656;&#27714;&#65292;&#24182;&#25552;&#20986;&#20102;&#24403;&#21069;&#22312;&#38899;&#39057;&#29983;&#25104;&#26041;&#38754;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25104;&#21151;&#24212;&#29992;AI&#30740;&#31350;&#65292;&#20102;&#35299;&#34892;&#19994;&#38656;&#27714;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#36890;&#36807;&#19982;&#19987;&#19994;&#38899;&#39057;&#24037;&#31243;&#24072;&#36827;&#34892;&#35843;&#26597;&#65292;&#30830;&#23450;&#20102;&#30740;&#31350;&#37325;&#28857;&#21644;&#23450;&#20041;&#20102;&#21508;&#31181;&#30740;&#31350;&#20219;&#21153;&#12290;&#21516;&#26102;&#22522;&#20110;&#35843;&#26597;&#32467;&#26524;&#27010;&#36848;&#20102;&#24403;&#21069;&#38899;&#39057;&#36136;&#37327;&#21644;&#21487;&#25511;&#24615;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#24378;&#35843;&#24403;&#21069;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#26159;&#23454;&#29616;&#39640;&#36136;&#37327;&#38899;&#39057;&#29983;&#25104;&#30340;&#20027;&#35201;&#29942;&#39048;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#35299;&#20915;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#35777;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
To achieve successful deployment of AI research, it is crucial to understand the demands of the industry. In this paper, we present the results of a survey conducted with professional audio engineers, in order to determine research priorities and define various research tasks. We also summarize the current challenges in audio quality and controllability based on the survey. Our analysis emphasizes that the availability of datasets is currently the main bottleneck for achieving high-quality audio generation. Finally, we suggest potential solutions for some revealed issues with empirical evidence.
&lt;/p&gt;</description></item><item><title>GG-ODE&#26159;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#36328;&#29615;&#22659;&#23398;&#20064;&#22797;&#26434;&#31995;&#32479;&#21160;&#24577;&#65292;&#21033;&#29992;&#31070;&#32463;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21442;&#25968;&#21270;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2307.04287</link><description>&lt;p&gt;
&#24191;&#20041;&#22270;ODE&#65306;&#36328;&#29615;&#22659;&#23398;&#20064;&#22797;&#26434;&#31995;&#32479;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Generalizing Graph ODE for Learning Complex System Dynamics across Environments. (arXiv:2307.04287v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04287
&lt;/p&gt;
&lt;p&gt;
GG-ODE&#26159;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#36328;&#29615;&#22659;&#23398;&#20064;&#22797;&#26434;&#31995;&#32479;&#21160;&#24577;&#65292;&#21033;&#29992;&#31070;&#32463;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21442;&#25968;&#21270;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#21160;&#24577;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#22914;&#29983;&#29289;&#23398;&#20013;&#30340;&#20998;&#23376;&#21160;&#21147;&#23398;&#12290;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#27169;&#22411;&#37117;&#26159;&#22522;&#20110;&#35266;&#27979;&#21040;&#30340;&#21382;&#21490;&#25968;&#25454;&#23398;&#20064;&#21333;&#19968;&#31995;&#32479;&#21160;&#24577;&#24182;&#39044;&#27979;&#26410;&#26469;&#36712;&#36857;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#21487;&#33021;&#35266;&#23519;&#21040;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#29983;&#25104;&#30340;&#22810;&#20010;&#31995;&#32479;&#65292;&#36825;&#20123;&#31995;&#32479;&#22312;&#28201;&#24230;&#21644;&#37325;&#21147;&#31561;&#28508;&#22312;&#22806;&#22240;&#32032;&#19978;&#23384;&#22312;&#24046;&#24322;&#12290;&#19968;&#20010;&#31616;&#21333;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#23398;&#20064;&#22810;&#20010;&#29615;&#22659;&#29305;&#23450;&#27169;&#22411;&#65292;&#20294;&#23427;&#26410;&#33021;&#21033;&#29992;&#36328;&#29615;&#22659;&#21160;&#24577;&#20013;&#30340;&#28508;&#22312;&#20849;&#24615;&#65292;&#24182;&#22312;&#27599;&#20010;&#29615;&#22659;&#25968;&#25454;&#31232;&#32570;&#25110;&#26377;&#38480;&#26102;&#25552;&#20379;&#36739;&#24046;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GG-ODE&#65288;&#24191;&#20041;&#22270;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#36328;&#29615;&#22659;&#23398;&#20064;&#36830;&#32493;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#21160;&#24577;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#29992;&#30001;&#22270;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#30340;&#31070;&#32463;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#26469;&#23398;&#20064;&#31995;&#32479;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning multi-agent system dynamics has been extensively studied for various real-world applications, such as molecular dynamics in biology. Most of the existing models are built to learn single system dynamics from observed historical data and predict the future trajectory. In practice, however, we might observe multiple systems that are generated across different environments, which differ in latent exogenous factors such as temperature and gravity. One simple solution is to learn multiple environment-specific models, but it fails to exploit the potential commonalities among the dynamics across environments and offers poor prediction results where per-environment data is sparse or limited. Here, we present GG-ODE (Generalized Graph Ordinary Differential Equations), a machine learning framework for learning continuous multi-agent system dynamics across environments. Our model learns system dynamics using neural ordinary differential equations (ODE) parameterized by Graph Neural Netwo
&lt;/p&gt;</description></item><item><title>ChatGPT&#26159;&#30001;OpenAI&#21019;&#24314;&#30340;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#24615;&#30340;&#21464;&#38761;&#12290;&#23427;&#26159;&#31532;&#19968;&#20010;&#23454;&#29616;&#20844;&#20247;&#22823;&#35268;&#27169;&#19982;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GAI&#65289;&#20114;&#21160;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#20063;&#24341;&#21457;&#20102;&#31867;&#20284;&#25216;&#26415;&#30340;&#30740;&#31350;&#20852;&#36259;&#21644;&#24212;&#29992;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2307.04251</link><description>&lt;p&gt;
ChatGPT&#22312;&#29983;&#25104;&#24335;AI&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#24212;&#29992;&#65306;&#19968;&#20221;&#31616;&#27905;&#30340;&#35843;&#26597;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
ChatGPT in the Age of Generative AI and Large Language Models: A Concise Survey. (arXiv:2307.04251v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04251
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#30001;OpenAI&#21019;&#24314;&#30340;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#24615;&#30340;&#21464;&#38761;&#12290;&#23427;&#26159;&#31532;&#19968;&#20010;&#23454;&#29616;&#20844;&#20247;&#22823;&#35268;&#27169;&#19982;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GAI&#65289;&#20114;&#21160;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#20063;&#24341;&#21457;&#20102;&#31867;&#20284;&#25216;&#26415;&#30340;&#30740;&#31350;&#20852;&#36259;&#21644;&#24212;&#29992;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#30001;OpenAI&#21019;&#24314;&#30340;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#32463;&#36807;&#31934;&#24515;&#35757;&#32451;&#24182;&#20351;&#29992;&#20102;&#22823;&#37327;&#25968;&#25454;&#12290;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#24615;&#30340;&#21464;&#38761;&#65292;&#24182;&#25512;&#21160;&#20102;LLM&#33021;&#21147;&#30340;&#36793;&#30028;&#12290;ChatGPT&#22312;&#22823;&#35268;&#27169;&#33539;&#22260;&#20869;&#23454;&#29616;&#20102;&#26222;&#36941;&#20844;&#20247;&#19982;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GAI&#65289;&#30340;&#20114;&#21160;&#65292;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#23427;&#36824;&#24341;&#21457;&#20102;&#24320;&#21457;&#31867;&#20284;&#25216;&#26415;&#21644;&#30740;&#31350;&#20854;&#24212;&#29992;&#21644;&#24433;&#21709;&#30340;&#20852;&#36259;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#23545;ChatGPT&#21450;&#20854;&#28436;&#21270;&#30340;&#24403;&#21069;&#30740;&#31350;&#26041;&#21521;&#36827;&#34892;&#31616;&#26126;&#35843;&#26597;&#12290;&#25105;&#20204;&#21516;&#26102;&#32771;&#34385;&#20102;ChatGPT&#30340;&#29627;&#29827;&#30418;&#21644;&#40657;&#30418;&#35270;&#35282;&#65292;&#21253;&#25324;&#25216;&#26415;&#30340;&#32452;&#25104;&#37096;&#20998;&#21644;&#22522;&#26412;&#35201;&#32032;&#65292;&#20197;&#21450;&#20854;&#24212;&#29992;&#12289;&#24433;&#21709;&#21644;&#24433;&#21709;&#12290;&#29627;&#29827;&#30418;&#26041;&#27861;&#30528;&#37325;&#20110;&#29702;&#35299;&#25216;&#26415;&#30340;&#20869;&#37096;&#36816;&#20316;&#65292;&#32780;&#40657;&#30418;&#26041;&#27861;&#23558;&#20854;&#35270;&#20026;&#19968;&#20010;&#22797;&#26434;&#31995;&#32479;&#65292;&#22240;&#27492;&#30740;&#31350;&#20854;&#36755;&#20837;&#65292;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is a large language model (LLM) created by OpenAI that has been carefully trained on a large amount of data. It has revolutionized the field of natural language processing (NLP) and has pushed the boundaries of LLM capabilities. ChatGPT has played a pivotal role in enabling widespread public interaction with generative artificial intelligence (GAI) on a large scale. It has also sparked research interest in developing similar technologies and investigating their applications and implications. In this paper, our primary goal is to provide a concise survey on the current lines of research on ChatGPT and its evolution. We considered both the glass box and black box views of ChatGPT, encompassing the components and foundational elements of the technology, as well as its applications, impacts, and implications. The glass box approach focuses on understanding the inner workings of the technology, and the black box approach embraces it as a complex system, and thus examines its inputs,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36827;&#34892;&#21518;&#22788;&#29702;&#30340;&#26032;&#22411;&#27969;&#27700;&#32447;&#65292;&#20197;&#25552;&#39640;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#65292;&#29305;&#21035;&#38024;&#23545;&#21360;&#21047;&#25945;&#31185;&#20070;&#21644;&#25163;&#20889;&#25991;&#23383;&#31561;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.04245</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36827;&#34892;&#21518;&#22788;&#29702;&#30340;&#25913;&#36827;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#30340;&#26032;&#22411;&#27969;&#27700;&#32447;
&lt;/p&gt;
&lt;p&gt;
A Novel Pipeline for Improving Optical Character Recognition through Post-processing Using Natural Language Processing. (arXiv:2307.04245v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04245
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36827;&#34892;&#21518;&#22788;&#29702;&#30340;&#26032;&#22411;&#27969;&#27700;&#32447;&#65292;&#20197;&#25552;&#39640;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#65292;&#29305;&#21035;&#38024;&#23545;&#21360;&#21047;&#25945;&#31185;&#20070;&#21644;&#25163;&#20889;&#25991;&#23383;&#31561;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#65288;OCR&#65289;&#25216;&#26415;&#22312;&#25968;&#23383;&#21270;&#22270;&#20070;&#21644;&#38750;&#32467;&#26500;&#21270;&#25991;&#26723;&#31561;&#26041;&#38754;&#26377;&#24191;&#27867;&#24212;&#29992;&#65292;&#22312;&#31227;&#21160;&#32479;&#35745;&#12289;&#25191;&#27861;&#12289;&#20132;&#36890;&#21644;&#23433;&#20840;&#31995;&#32479;&#31561;&#20854;&#20182;&#39046;&#22495;&#20063;&#26377;&#24212;&#29992;&#12290;&#24403;&#21069;&#30340;&#25216;&#26415;&#22312;&#35782;&#21035;&#36710;&#29260;&#12289;&#21830;&#24215;&#21517;&#31561;&#21360;&#21047;&#25991;&#23383;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#28982;&#32780;&#23545;&#20110;&#21360;&#21047;&#25945;&#31185;&#20070;&#21644;&#25163;&#20889;&#25991;&#23383;&#31561;&#24212;&#29992;&#65292;&#29616;&#26377;&#25216;&#26415;&#30340;&#20934;&#30830;&#24615;&#26377;&#38480;&#12290;&#36896;&#25104;&#36825;&#31181;&#38382;&#39064;&#30340;&#21407;&#22240;&#21487;&#33021;&#22312;&#20110;&#24418;&#29366;&#30456;&#20284;&#30340;&#23383;&#31526;&#21644;&#25163;&#20889;&#23383;&#31526;&#30340;&#21464;&#21270;&#12290;&#37492;&#20110;&#36825;&#20123;&#38382;&#39064;&#22312;&#20165;&#20351;&#29992;OCR&#25216;&#26415;&#35299;&#20915;&#26102;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#36827;&#34892;&#21518;&#22788;&#29702;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#27969;&#27700;&#32447;&#65292;&#39318;&#20808;&#23545;&#25163;&#20889;&#25110;&#21360;&#21047;&#25991;&#26412;&#36827;&#34892;OCR&#65292;&#28982;&#21518;&#20351;&#29992;NLP&#25552;&#39640;&#20854;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optical Character Recognition (OCR) technology finds applications in digitizing books and unstructured documents, along with applications in other domains such as mobility statistics, law enforcement, traffic, security systems, etc. The state-of-the-art methods work well with the OCR with printed text on license plates, shop names, etc. However, applications such as printed textbooks and handwritten texts have limited accuracy with existing techniques. The reason may be attributed to similar-looking characters and variations in handwritten characters. Since these issues are challenging to address with OCR technologies exclusively, we propose a post-processing approach using Natural Language Processing (NLP) tools. This work presents an end-to-end pipeline that first performs OCR on the handwritten or printed text and then improves its accuracy using NLP.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20302;&#21487;&#35265;&#24230;&#22330;&#26223;&#20013;&#20351;&#29992;&#32418;&#22806;&#21644;&#28909;&#25104;&#20687;&#34701;&#21512;&#30340;&#23454;&#26102;&#20154;&#20307;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#25668;&#20687;&#22836;&#30340;&#22788;&#29702;&#65292;&#33719;&#21462;&#20851;&#38190;&#20449;&#24687;&#20197;&#25552;&#39640;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.04223</link><description>&lt;p&gt;
&#20351;&#29992;&#32418;&#22806;&#21644;&#28909;&#25104;&#20687;&#34701;&#21512;&#30340;&#23454;&#26102;&#20154;&#20307;&#26816;&#27979;&#22312;&#28779;&#28798;&#22330;&#26223;&#20013;
&lt;/p&gt;
&lt;p&gt;
Real-time Human Detection in Fire Scenarios using Infrared and Thermal Imaging Fusion. (arXiv:2307.04223v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20302;&#21487;&#35265;&#24230;&#22330;&#26223;&#20013;&#20351;&#29992;&#32418;&#22806;&#21644;&#28909;&#25104;&#20687;&#34701;&#21512;&#30340;&#23454;&#26102;&#20154;&#20307;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#25668;&#20687;&#22836;&#30340;&#22788;&#29702;&#65292;&#33719;&#21462;&#20851;&#38190;&#20449;&#24687;&#20197;&#25552;&#39640;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28779;&#28798;&#34987;&#35748;&#20026;&#26159;&#23545;&#20154;&#31867;&#29983;&#21629;&#26368;&#20005;&#37325;&#30340;&#23041;&#32961;&#20043;&#19968;&#65292;&#23548;&#33268;&#39640;&#27010;&#29575;&#30340;&#20260;&#20129;&#12290;&#36825;&#20123;&#20005;&#37325;&#21518;&#26524;&#28304;&#20110;&#28779;&#28798;&#20135;&#29983;&#30340;&#27987;&#28895;&#65292;&#22823;&#22823;&#38480;&#21046;&#20102;&#36867;&#29983;&#32773;&#21644;&#25937;&#25588;&#38431;&#30340;&#35270;&#37326;&#12290;&#22312;&#22914;&#27492;&#21361;&#38505;&#30340;&#29615;&#22659;&#19979;&#65292;&#20351;&#29992;&#22522;&#20110;&#35270;&#35273;&#30340;&#20154;&#20307;&#26816;&#27979;&#31995;&#32479;&#33021;&#22815;&#25552;&#39640;&#25937;&#25588;&#26356;&#22810;&#29983;&#21629;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30001;&#28895;&#38654;&#24341;&#36215;&#30340;&#20302;&#21487;&#35265;&#24230;&#22330;&#26223;&#20013;&#65292;&#22522;&#20110;&#22810;&#25668;&#20687;&#22836;&#30340;&#32418;&#22806;&#21644;&#28909;&#25104;&#20687;&#34701;&#21512;&#31574;&#30053;&#36827;&#34892;&#20154;&#20307;&#26816;&#27979;&#12290;&#36890;&#36807;&#22788;&#29702;&#22810;&#20010;&#25668;&#20687;&#22836;&#65292;&#21487;&#20197;&#25910;&#38598;&#20851;&#38190;&#20449;&#24687;&#20197;&#29983;&#25104;&#26356;&#26377;&#29992;&#30340;&#20154;&#20307;&#26816;&#27979;&#29305;&#24449;&#12290;&#39318;&#20808;&#65292;&#20351;&#29992;&#20809;&#21152;&#28909;&#26827;&#30424;&#26657;&#20934;&#25668;&#20687;&#22836;&#12290;&#28982;&#21518;&#65292;&#23558;&#20174;&#36755;&#20837;&#22270;&#20687;&#25552;&#21462;&#30340;&#29305;&#24449;&#21512;&#24182;&#65292;&#28982;&#21518;&#36890;&#36807;&#19968;&#20010;&#36731;&#37327;&#32423;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20154;&#20307;&#26816;&#27979;&#20219;&#21153;&#12290;&#23454;&#39564;&#26159;&#22312;&#19968;&#20010;NVIDIA Jetson Nano&#35745;&#31639;&#26426;&#19978;&#36827;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fire is considered one of the most serious threats to human lives which results in a high probability of fatalities. Those severe consequences stem from the heavy smoke emitted from a fire that mostly restricts the visibility of escaping victims and rescuing squad. In such hazardous circumstances, the use of a vision-based human detection system is able to improve the ability to save more lives. To this end, a thermal and infrared imaging fusion strategy based on multiple cameras for human detection in low-visibility scenarios caused by smoke is proposed in this paper. By processing with multiple cameras, vital information can be gathered to generate more useful features for human detection. Firstly, the cameras are calibrated using a Light Heating Chessboard. Afterward, the features extracted from the input images are merged prior to being passed through a lightweight deep neural network to perform the human detection task. The experiments conducted on an NVIDIA Jetson Nano computer d
&lt;/p&gt;</description></item><item><title>LakeBench&#26159;&#19968;&#20010;&#29992;&#20110;&#25968;&#25454;&#28246;&#19978;&#25968;&#25454;&#21457;&#29616;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#28304;&#30340;&#34920;&#26684;&#65292;&#23427;&#20026;&#20225;&#19994;&#22312;&#23547;&#25214;&#30456;&#20851;&#34920;&#26684;&#30340;&#20219;&#21153;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#24615;&#33021;&#27604;&#36739;&#12290;&#30446;&#21069;&#20844;&#20849;&#39046;&#22495;&#20013;&#32570;&#20047;&#36825;&#26679;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#32780;&#24050;&#26377;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#36824;&#26377;&#24456;&#22823;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2307.04217</link><description>&lt;p&gt;
LakeBench: &#29992;&#20110;&#25968;&#25454;&#28246;&#19978;&#30340;&#25968;&#25454;&#21457;&#29616;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
LakeBench: Benchmarks for Data Discovery over Data Lakes. (arXiv:2307.04217v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04217
&lt;/p&gt;
&lt;p&gt;
LakeBench&#26159;&#19968;&#20010;&#29992;&#20110;&#25968;&#25454;&#28246;&#19978;&#25968;&#25454;&#21457;&#29616;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#28304;&#30340;&#34920;&#26684;&#65292;&#23427;&#20026;&#20225;&#19994;&#22312;&#23547;&#25214;&#30456;&#20851;&#34920;&#26684;&#30340;&#20219;&#21153;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#24615;&#33021;&#27604;&#36739;&#12290;&#30446;&#21069;&#20844;&#20849;&#39046;&#22495;&#20013;&#32570;&#20047;&#36825;&#26679;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#32780;&#24050;&#26377;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#36824;&#26377;&#24456;&#22823;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20225;&#19994;&#20013;&#65292;&#26234;&#33021;&#23548;&#33322;&#25968;&#25454;&#28246;&#30340;&#38656;&#27714;&#36234;&#26469;&#36234;&#22823;&#65292;&#29305;&#21035;&#20851;&#27880;&#25968;&#25454;&#21457;&#29616;&#12290;&#23545;&#20225;&#19994;&#26469;&#35828;&#65292;&#33021;&#22815;&#25214;&#21040;&#25968;&#25454;&#20179;&#24211;&#20013;&#30456;&#20851;&#30340;&#34920;&#26684;&#38750;&#24120;&#37325;&#35201;&#12290;&#36825;&#20123;&#34920;&#26684;&#21487;&#20197;&#26159;&#21487;&#32852;&#25509;&#30340;&#12289;&#21487;&#21512;&#24182;&#30340;&#65292;&#25110;&#32773;&#26159;&#24444;&#27492;&#30340;&#23376;&#38598;&#12290;&#30446;&#21069;&#20844;&#20849;&#39046;&#22495;&#20013;&#32570;&#20047;&#38024;&#23545;&#36825;&#20123;&#20219;&#21153;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#30456;&#20851;&#24037;&#20316;&#20027;&#35201;&#38024;&#23545;&#31169;&#26377;&#25968;&#25454;&#38598;&#12290;&#22312;LakeBench&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;CKAN&#12289;Socrata&#21644;&#27431;&#27954;&#20013;&#22830;&#38134;&#34892;&#31561;&#21508;&#31181;&#25968;&#25454;&#28304;&#30340;&#34920;&#26684;&#65292;&#24320;&#21457;&#20102;&#22810;&#20010;&#29992;&#20110;&#36825;&#20123;&#20219;&#21153;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;4&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#34920;&#26684;&#22522;&#20934;&#27169;&#22411;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#29616;&#26377;&#27169;&#22411;&#27809;&#26377;&#22312;&#25105;&#20204;&#20026;&#36825;&#20010;&#22522;&#20934;&#27979;&#35797;&#24320;&#21457;&#30340;&#25968;&#25454;&#21457;&#29616;&#20219;&#21153;&#19978;&#36827;&#34892;&#36807;&#35757;&#32451;&#65292;&#22240;&#27492;&#20854;&#24615;&#33021;&#20173;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#24314;&#31435;&#36825;&#26679;&#30340;&#22522;&#20934;&#27979;&#35797;&#21487;&#33021;&#23545;&#31038;&#21306;&#26500;&#24314;&#26631;&#20934;&#25968;&#25454;&#21457;&#29616;&#27169;&#22411;&#26159;&#26377;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Within enterprises, there is a growing need to intelligently navigate data lakes, specifically focusing on data discovery. Of particular importance to enterprises is the ability to find related tables in data repositories. These tables can be unionable, joinable, or subsets of each other. There is a dearth of benchmarks for these tasks in the public domain, with related work targeting private datasets. In LakeBench, we develop multiple benchmarks for these tasks by using the tables that are drawn from a diverse set of data sources such as government data from CKAN, Socrata, and the European Central Bank. We compare the performance of 4 publicly available tabular foundational models on these tasks. None of the existing models had been trained on the data discovery tasks that we developed for this benchmark; not surprisingly, their performance shows significant room for improvement. The results suggest that the establishment of such benchmarks may be useful to the community to build tabu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;&#33258;&#32534;&#30721;&#22120;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#33021;&#22815;&#26174;&#33879;&#21387;&#32553;&#22823;&#35268;&#27169;&#39640;&#20998;&#36776;&#29575;&#31185;&#23398;&#25968;&#25454;&#65292;&#24182;&#20445;&#25345;&#39640;&#37325;&#24314;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.04216</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#23618;&#33258;&#32534;&#30721;&#22120;&#30340;&#22823;&#35268;&#27169;&#39640;&#20998;&#36776;&#29575;&#31185;&#23398;&#25968;&#25454;&#26377;&#25439;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Autoencoder-based Lossy Compression for Large-scale High-resolution Scientific Data. (arXiv:2307.04216v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;&#33258;&#32534;&#30721;&#22120;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#33021;&#22815;&#26174;&#33879;&#21387;&#32553;&#22823;&#35268;&#27169;&#39640;&#20998;&#36776;&#29575;&#31185;&#23398;&#25968;&#25454;&#65292;&#24182;&#20445;&#25345;&#39640;&#37325;&#24314;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25439;&#21387;&#32553;&#24050;&#25104;&#20026;&#35768;&#22810;&#39046;&#22495;&#20013;&#20943;&#23567;&#25968;&#25454;&#22823;&#23567;&#30340;&#37325;&#35201;&#25216;&#26415;&#12290;&#36825;&#31181;&#21387;&#32553;&#26041;&#27861;&#23545;&#20110;&#22823;&#23567;&#22312;&#20960;&#20010;PB&#33539;&#22260;&#20869;&#30340;&#22823;&#35268;&#27169;&#31185;&#23398;&#25968;&#25454;&#23588;&#20026;&#37325;&#35201;&#12290;&#34429;&#28982;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#27169;&#22411;&#24050;&#25104;&#21151;&#22320;&#29992;&#20110;&#21387;&#32553;&#22270;&#20687;&#21644;&#35270;&#39057;&#65292;&#20294;&#36825;&#31181;&#31070;&#32463;&#32593;&#32476;&#22312;&#31185;&#23398;&#25968;&#25454;&#39046;&#22495;&#23578;&#26410;&#24191;&#20026;&#20851;&#27880;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#19981;&#20165;&#21487;&#20197;&#26174;&#33879;&#21387;&#32553;&#22823;&#35268;&#27169;&#31185;&#23398;&#25968;&#25454;&#65292;&#36824;&#21487;&#20197;&#20445;&#25345;&#39640;&#37325;&#24314;&#36136;&#37327;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#20844;&#24320;&#30340;&#31185;&#23398;&#22522;&#20934;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#24212;&#29992;&#20110;&#19968;&#31181;&#22823;&#35268;&#27169;&#39640;&#20998;&#36776;&#29575;&#30340;&#27668;&#20505;&#27169;&#25311;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;140&#30340;&#21387;&#32553;&#27604;&#65292;&#21516;&#26102;&#20445;&#25345;&#37325;&#24314;&#36136;&#37327;&#12290;&#39640;&#20998;&#36776;&#29575;&#31038;&#21306;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;(CESM) Version 1.3&#30340;&#27169;&#25311;&#25968;&#25454;&#22312;&#21387;&#32553;&#27604;&#36798;&#21040;200&#30340;&#21516;&#26102;&#36827;&#34892;&#20102;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lossy compression has become an important technique to reduce data size in many domains. This type of compression is especially valuable for large-scale scientific data, whose size ranges up to several petabytes. Although Autoencoder-based models have been successfully leveraged to compress images and videos, such neural networks have not widely gained attention in the scientific data domain. Our work presents a neural network that not only significantly compresses large-scale scientific data but also maintains high reconstruction quality. The proposed model is tested with scientific benchmark data available publicly and applied to a large-scale high-resolution climate modeling data set. Our model achieves a compression ratio of 140 on several benchmark data sets without compromising the reconstruction quality. Simulation data from the High-Resolution Community Earth System Model (CESM) Version 1.3 over 500 years are also being compressed with a compression ratio of 200 while the recon
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20225;&#19994;&#20013;&#37096;&#32626;&#20445;&#25252;&#38544;&#31169;&#30340;&#21512;&#25104;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#37325;&#28857;&#20851;&#27880;&#20010;&#20154;&#21644;&#39640;&#24230;&#25935;&#24863;&#25968;&#25454;&#25152;&#24341;&#36215;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#37492;&#21035;&#20986;&#20102;40&#22810;&#20010;&#25361;&#25112;&#24182;&#23558;&#20854;&#31995;&#32479;&#21270;&#65292;&#25552;&#20986;&#20102;&#20225;&#19994;&#21487;&#20197;&#37319;&#29992;&#30340;&#25112;&#30053;&#21644;&#31995;&#32479;&#26041;&#27861;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.04208</link><description>&lt;p&gt;
&#20851;&#20110;&#22312;&#20225;&#19994;&#20013;&#37096;&#32626;&#20445;&#25252;&#38544;&#31169;&#30340;&#21512;&#25104;&#25968;&#25454;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
On the Challenges of Deploying Privacy-Preserving Synthetic Data in the Enterprise. (arXiv:2307.04208v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20225;&#19994;&#20013;&#37096;&#32626;&#20445;&#25252;&#38544;&#31169;&#30340;&#21512;&#25104;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#37325;&#28857;&#20851;&#27880;&#20010;&#20154;&#21644;&#39640;&#24230;&#25935;&#24863;&#25968;&#25454;&#25152;&#24341;&#36215;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#37492;&#21035;&#20986;&#20102;40&#22810;&#20010;&#25361;&#25112;&#24182;&#23558;&#20854;&#31995;&#32479;&#21270;&#65292;&#25552;&#20986;&#20102;&#20225;&#19994;&#21487;&#20197;&#37319;&#29992;&#30340;&#25112;&#30053;&#21644;&#31995;&#32479;&#26041;&#27861;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#27491;&#33719;&#24471;&#31354;&#21069;&#30340;&#26222;&#21450;&#65292;&#30001;&#20110;&#20854;&#21331;&#36234;&#30340;&#33021;&#21147;&#24102;&#26469;&#20102;&#20852;&#22859;&#21644;&#24551;&#34385;&#30340;&#28151;&#21512;&#24863;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#37096;&#32626;&#21512;&#25104;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#36825;&#26159;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#20010;&#23376;&#39046;&#22495;&#12290;&#25105;&#20204;&#23558;&#37325;&#28857;&#25918;&#22312;&#20225;&#19994;&#37096;&#32626;&#19978;&#65292;&#29305;&#21035;&#20851;&#27880;&#20010;&#20154;&#21644;&#39640;&#24230;&#25935;&#24863;&#25968;&#25454;&#25152;&#24341;&#36215;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#25105;&#20204;&#37492;&#21035;&#20986;&#20102;40&#22810;&#20010;&#25361;&#25112;&#65292;&#24182;&#23558;&#23427;&#20204;&#31995;&#32479;&#21270;&#20026;&#20116;&#20010;&#20027;&#35201;&#32452;&#21035; - i&#65289;&#29983;&#25104;&#65292;ii&#65289;&#22522;&#30784;&#35774;&#26045;&#21644;&#26550;&#26500;&#65292;iii&#65289;&#27835;&#29702;&#65292;iv&#65289;&#21512;&#35268;&#21644;&#30417;&#31649;&#65292;&#21644;v&#65289;&#37319;&#32435;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20225;&#19994;&#21487;&#20197;&#37319;&#29992;&#30340;&#25112;&#30053;&#21644;&#31995;&#32479;&#26041;&#27861;&#26469;&#26377;&#25928;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#24314;&#31435;&#23545;&#23454;&#26045;&#35299;&#20915;&#26041;&#26696;&#30340;&#20449;&#20219;&#26469;&#23454;&#29616;&#20182;&#20204;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI technologies are gaining unprecedented popularity, causing a mix of excitement and apprehension through their remarkable capabilities. In this paper, we study the challenges associated with deploying synthetic data, a subfield of Generative AI. Our focus centers on enterprise deployment, with an emphasis on privacy concerns caused by the vast amount of personal and highly sensitive data. We identify 40+ challenges and systematize them into five main groups -- i) generation, ii) infrastructure &amp; architecture, iii) governance, iv) compliance &amp; regulation, and v) adoption. Additionally, we discuss a strategic and systematic approach that enterprises can employ to effectively address the challenges and achieve their goals by establishing trust in the implemented solutions.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#22312;&#24314;&#31569;&#26045;&#24037;&#39046;&#22495;&#20013;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#23454;&#29616;&#20154;&#26426;&#30452;&#35266;&#20132;&#27969;&#30340;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#26045;&#24037;&#29616;&#22330;&#22797;&#26434;&#24615;&#21644;&#20154;&#21147;&#30701;&#32570;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.04195</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#29992;&#20110;&#24314;&#31569;&#26045;&#24037;&#39046;&#22495;&#20013;&#20154;&#26426;&#20114;&#21160;&#30340;&#30452;&#35266;&#24615;&#20132;&#27969;
&lt;/p&gt;
&lt;p&gt;
Natural Language Instructions for Intuitive Human Interaction with Robotic Assistants in Field Construction Work. (arXiv:2307.04195v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04195
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#22312;&#24314;&#31569;&#26045;&#24037;&#39046;&#22495;&#20013;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#23454;&#29616;&#20154;&#26426;&#30452;&#35266;&#20132;&#27969;&#30340;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#26045;&#24037;&#29616;&#22330;&#22797;&#26434;&#24615;&#21644;&#20154;&#21147;&#30701;&#32570;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#30340;&#24341;&#20837;&#34987;&#24191;&#27867;&#35748;&#20026;&#26377;&#26395;&#35299;&#20915;&#26045;&#24037;&#34892;&#19994;&#38754;&#20020;&#30340;&#21171;&#21160;&#21147;&#30701;&#32570;&#21644;&#29983;&#20135;&#21147;&#20572;&#28382;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#22797;&#26434;&#21644;&#26080;&#32467;&#26500;&#30340;&#26045;&#24037;&#29616;&#22330;&#20351;&#29992;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#26426;&#22120;&#20154;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20154;&#26426;&#21327;&#20316;&#22312;&#26045;&#24037;&#20013;&#26174;&#31034;&#20986;&#20102;&#32467;&#21512;&#20154;&#31867;&#24037;&#20154;&#30340;&#28789;&#27963;&#24615;&#21644;&#26426;&#22120;&#20154;&#21161;&#25163;&#30340;&#29289;&#29702;&#33021;&#21147;&#20849;&#21516;&#24212;&#23545;&#26045;&#24037;&#24037;&#20316;&#20013;&#22266;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#28508;&#21147;&#12290;&#22312;&#24341;&#20837;&#20154;&#26426;&#21327;&#20316;&#26102;&#65292;&#35782;&#21035;&#22242;&#38431;&#21512;&#20316;&#21644;&#30417;&#30563;&#22312;&#23454;&#22320;&#26045;&#24037;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20026;&#20154;&#31867;&#24037;&#20154;&#21644;&#26426;&#22120;&#20154;&#21161;&#25163;&#24314;&#31435;&#19968;&#31181;&#33258;&#28982;&#30452;&#35266;&#30340;&#20132;&#27969;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#20132;&#20114;&#21487;&#20197;&#20351;&#38750;&#26426;&#22120;&#20154;&#32534;&#31243;&#19987;&#23478;&#30340;&#20154;&#31867;&#24037;&#20154;&#19982;&#26426;&#22120;&#20154;&#20043;&#38388;&#36827;&#34892;&#30452;&#35266;&#21644;&#29087;&#24713;&#30340;&#20132;&#27969;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22312;&#24314;&#31569;&#39046;&#22495;&#23545;&#36825;&#19968;&#20027;&#39064;&#30340;&#30740;&#31350;&#36824;&#24456;&#26377;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#20801;&#35768;&#23454;&#29616;&#36825;&#31181;&#30452;&#35266;&#30340;&#20132;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;
The introduction of robots is widely considered to have significant potential of alleviating the issues of worker shortage and stagnant productivity that afflict the construction industry. However, it is challenging to use fully automated robots in complex and unstructured construction sites. Human-Robot Collaboration (HRC) has shown promise of combining human workers' flexibility and robot assistants' physical abilities to jointly address the uncertainties inherent in construction work. When introducing HRC in construction, it is critical to recognize the importance of teamwork and supervision in field construction and establish a natural and intuitive communication system for the human workers and robotic assistants. Natural language-based interaction can enable intuitive and familiar communication with robots for human workers who are non-experts in robot programming. However, limited research has been conducted on this topic in construction. This paper proposes a framework to allow
&lt;/p&gt;</description></item><item><title>SAS&#35270;&#39057;&#38382;&#31572;&#36890;&#36807;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#35299;&#20915;&#20102;&#35270;&#39057;&#38382;&#31572;&#20013;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;</title><link>http://arxiv.org/abs/2307.04192</link><description>&lt;p&gt;
SAS&#35270;&#39057;&#38382;&#31572;&#65306;&#33258;&#36866;&#24212;&#37319;&#26679;&#29992;&#20110;&#39640;&#25928;&#35270;&#39057;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
SAS Video-QA: Self-Adaptive Sampling for Efficient Video Question-Answering. (arXiv:2307.04192v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04192
&lt;/p&gt;
&lt;p&gt;
SAS&#35270;&#39057;&#38382;&#31572;&#36890;&#36807;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#35299;&#20915;&#20102;&#35270;&#39057;&#38382;&#31572;&#20013;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#38382;&#31572;&#26159;&#35270;&#39057;&#29702;&#35299;&#39046;&#22495;&#30340;&#19968;&#39033;&#22522;&#30784;&#20219;&#21153;&#12290;&#23613;&#31649;&#24403;&#21069;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(VLMs)&#37197;&#22791;&#20102;&#35270;&#39057;&#21464;&#25442;&#22120;(Video Transformers)&#65292;&#23454;&#29616;&#20102;&#26102;&#38388;&#24314;&#27169;&#24182;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#32467;&#26524;&#65292;&#20294;&#20195;&#20215;&#26159;&#24040;&#22823;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#22240;&#27492;&#22312;&#23454;&#26102;&#24212;&#29992;&#22330;&#26223;&#20013;&#36807;&#20110;&#26114;&#36149;&#12290;&#19968;&#31181;&#32463;&#27982;&#30340;&#35299;&#20915;&#26041;&#27861;&#26159;&#21482;&#23545;&#35270;&#39057;&#30340;&#19968;&#23567;&#37096;&#20998;&#24103;&#36827;&#34892;&#37319;&#26679;&#65292;&#26469;&#20195;&#34920;&#35270;&#39057;&#30340;&#20027;&#35201;&#20869;&#23481;&#65292;&#24182;&#22312;&#36825;&#20123;&#37319;&#26679;&#24103;&#19978;&#35843;&#25972;&#22270;&#20687;-&#25991;&#26412;&#27169;&#22411;&#12290;&#26368;&#36817;&#30340;&#35270;&#39057;&#29702;&#35299;&#27169;&#22411;&#36890;&#24120;&#38543;&#26426;&#37319;&#26679;&#19968;&#32452;&#24103;&#25110;&#29255;&#27573;&#65292;&#32780;&#19981;&#32771;&#34385;&#23427;&#20204;&#30340;&#20869;&#37096;&#20851;&#32852;&#24615;&#21644;&#19982;&#38382;&#39064;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#26080;&#30446;&#26631;&#30340;&#37319;&#26679;&#21487;&#33021;&#20250;&#36951;&#28431;&#21487;&#20197;&#25512;&#23548;&#20986;&#27491;&#30830;&#31572;&#26696;&#30340;&#20851;&#38190;&#24103;&#65292;&#22312;&#37319;&#26679;&#31232;&#30095;&#31243;&#24230;&#22686;&#21152;&#26102;&#65292;&#24773;&#20917;&#20250;&#21464;&#24471;&#26356;&#31967;&#65292;&#32780;&#38543;&#30528;&#35270;&#39057;&#38271;&#24230;&#30340;&#22686;&#21152;&#65292;&#37319;&#26679;&#31232;&#30095;&#31243;&#24230;&#20063;&#20250;&#22686;&#21152;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#24103;&#37319;&#26679;&#31574;&#30053;&#65292;&#20998;&#21035;&#26159;
&lt;/p&gt;
&lt;p&gt;
Video question--answering is a fundamental task in the field of video understanding. Although current vision--language models (VLMs) equipped with Video Transformers have enabled temporal modeling and yielded superior results, they are at the cost of huge computational power and thus too expensive to deploy in real-time application scenarios. An economical workaround only samples a small portion of frames to represent the main content of that video and tune an image--text model on these sampled frames. Recent video understanding models usually randomly sample a set of frames or clips, regardless of internal correlations between their visual contents, nor their relevance to the problem. We argue that such kinds of aimless sampling may omit the key frames from which the correct answer can be deduced, and the situation gets worse when the sampling sparsity increases, which always happens as the video lengths increase. To mitigate this issue, we propose two frame sampling strategies, namel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28508;&#22312;&#22270;&#27880;&#24847;&#21147;&#65288;LGA&#65289;&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#22270;&#20687;&#20013;&#30340;&#20840;&#23616;&#19978;&#19979;&#25991;&#12290;LGA&#22312;&#35745;&#31639;&#19978;&#31616;&#27905;&#19988;&#31283;&#23450;&#65292;&#33021;&#22815;&#22312;&#23567;&#35268;&#27169;&#20307;&#31995;&#32467;&#26500;&#20013;&#23454;&#29616;&#25509;&#36817;&#22823;&#35268;&#27169;&#20307;&#31995;&#32467;&#26500;&#30340;&#24615;&#33021;&#65292;&#20351;&#24471;&#36731;&#37327;&#32423;&#20307;&#31995;&#32467;&#26500;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#26356;&#21152;&#23454;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.04149</link><description>&lt;p&gt;
&#22686;&#24378;&#31354;&#38388;&#19978;&#19979;&#25991;&#30340;&#28508;&#22312;&#22270;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Latent Graph Attention for Enhanced Spatial Context. (arXiv:2307.04149v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28508;&#22312;&#22270;&#27880;&#24847;&#21147;&#65288;LGA&#65289;&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#22270;&#20687;&#20013;&#30340;&#20840;&#23616;&#19978;&#19979;&#25991;&#12290;LGA&#22312;&#35745;&#31639;&#19978;&#31616;&#27905;&#19988;&#31283;&#23450;&#65292;&#33021;&#22815;&#22312;&#23567;&#35268;&#27169;&#20307;&#31995;&#32467;&#26500;&#20013;&#23454;&#29616;&#25509;&#36817;&#22823;&#35268;&#27169;&#20307;&#31995;&#32467;&#26500;&#30340;&#24615;&#33021;&#65292;&#20351;&#24471;&#36731;&#37327;&#32423;&#20307;&#31995;&#32467;&#26500;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#26356;&#21152;&#23454;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#20013;&#30340;&#20840;&#23616;&#19978;&#19979;&#25991;&#22312;&#22270;&#20687;&#21040;&#22270;&#20687;&#36716;&#25442;&#38382;&#39064;&#20013;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#21644;&#22270;&#27169;&#22411;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#25429;&#25417;&#21040;&#20102;&#20840;&#23616;&#19978;&#19979;&#25991;&#65292;&#20294;&#26159;&#36825;&#20123;&#26041;&#27861;&#35745;&#31639;&#19978;&#27604;&#36739;&#26114;&#36149;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#21482;&#33021;&#23398;&#20064;&#22270;&#20687;&#20013;&#20219;&#24847;&#20004;&#20010;&#28857;&#20043;&#38388;&#30340;&#37197;&#23545;&#35821;&#20041;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28508;&#22312;&#22270;&#27880;&#24847;&#21147;&#65288;LGA&#65289;&#30340;&#35745;&#31639;&#31616;&#27905;&#65288;&#19982;&#33410;&#28857;&#25968;&#37327;&#21576;&#32447;&#24615;&#20851;&#31995;&#65289;&#21644;&#31283;&#23450;&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#20840;&#23616;&#19978;&#19979;&#25991;&#32435;&#20837;&#29616;&#26377;&#20307;&#31995;&#32467;&#26500;&#20013;&#65292;&#29305;&#21035;&#26159;&#22686;&#24378;&#23567;&#35268;&#27169;&#20307;&#31995;&#32467;&#26500;&#30340;&#24615;&#33021;&#65292;&#20351;&#24471;&#36731;&#37327;&#32423;&#20307;&#31995;&#32467;&#26500;&#23545;&#20110;&#35745;&#31639;&#33021;&#21147;&#36739;&#20302;&#21644;&#33021;&#37327;&#38656;&#27714;&#36739;&#20302;&#30340;&#36793;&#32536;&#35774;&#22791;&#26356;&#21152;&#26377;&#29992;&#12290;LGA&#20351;&#29992;&#23616;&#37096;&#36830;&#25509;&#22270;&#32593;&#32476;&#26469;&#22312;&#31354;&#38388;&#19978;&#20256;&#25773;&#20449;&#24687;&#65292;&#20174;&#32780;&#26041;&#20415;&#22320;&#26500;&#24314;&#36828;&#36317;&#31163;&#30340;&#20004;&#20010;&#31354;&#38388;&#28857;&#20043;&#38388;&#30340;&#35821;&#20041;&#19968;&#33268;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Global contexts in images are quite valuable in image-to-image translation problems. Conventional attention-based and graph-based models capture the global context to a large extent, however, these are computationally expensive. Moreover, the existing approaches are limited to only learning the pairwise semantic relation between any two points on the image. In this paper, we present Latent Graph Attention (LGA) a computationally inexpensive (linear to the number of nodes) and stable, modular framework for incorporating the global context in the existing architectures, especially empowering small-scale architectures to give performance closer to large size architectures, thus making the light-weight architectures more useful for edge devices with lower compute power and lower energy needs. LGA propagates information spatially using a network of locally connected graphs, thereby facilitating to construct a semantically coherent relation between any two spatially distant points that also 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#24403;&#21069;&#22270;&#34920;&#20998;&#31867;&#25216;&#26415;&#30340;&#30740;&#31350;&#29616;&#29366;&#65292;&#24182;&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#12289;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#24615;&#33021;&#20998;&#26512;&#65292;&#20197;&#25552;&#39640;&#23545;&#22270;&#34920;&#30340;&#33258;&#21160;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.04147</link><description>&lt;p&gt;
&#19968;&#39033;&#20851;&#20110;&#22270;&#34920;&#20998;&#31867;&#30340;&#35843;&#26597;&#21644;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Survey and Approach to Chart Classification. (arXiv:2307.04147v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#24403;&#21069;&#22270;&#34920;&#20998;&#31867;&#25216;&#26415;&#30340;&#30740;&#31350;&#29616;&#29366;&#65292;&#24182;&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#12289;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#24615;&#33021;&#20998;&#26512;&#65292;&#20197;&#25552;&#39640;&#23545;&#22270;&#34920;&#30340;&#33258;&#21160;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#22312;&#25991;&#26723;&#20013;&#34920;&#31034;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#35270;&#35273;&#20449;&#24687;&#28304;&#65292;&#24182;&#20419;&#36827;&#20102;&#23545;&#36890;&#24120;&#20197;&#25968;&#23383;&#24418;&#24335;&#20256;&#36798;&#30340;&#20449;&#24687;&#30340;&#28145;&#20837;&#29702;&#35299;&#21644;&#35299;&#37322;&#12290;&#22312;&#31185;&#23398;&#25991;&#29486;&#20013;&#65292;&#26377;&#35768;&#22810;&#22270;&#34920;&#65292;&#27599;&#20010;&#22270;&#34920;&#37117;&#26377;&#20854;&#39118;&#26684;&#19978;&#30340;&#24046;&#24322;&#12290;&#26368;&#36817;&#65292;&#25991;&#26723;&#29702;&#35299;&#39046;&#22495;&#24320;&#22987;&#35299;&#20915;&#33258;&#21160;&#22270;&#34920;&#29702;&#35299;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#28041;&#21450;&#21040;&#22270;&#34920;&#20998;&#31867;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22270;&#34920;&#20998;&#31867;&#25216;&#26415;&#30340;&#35843;&#26597;&#65292;&#24182;&#35752;&#35770;&#20102;&#21487;&#29992;&#25968;&#25454;&#38598;&#21450;&#20854;&#25903;&#25345;&#30340;&#22270;&#34920;&#31867;&#22411;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#36129;&#29486;&#20998;&#20026;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#12289;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#30340;&#20256;&#32479;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#26368;&#36817;&#21457;&#24067;&#30340;CHARTINFO UB-UNITECH PMC&#25968;&#25454;&#38598;&#19978;&#23545;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27604;&#36739;&#24615;&#33021;&#20998;&#26512;&#65292;&#35813;&#25968;&#25454;&#38598;&#29992;&#20110;ICPR 2022&#30340;CHART-Infographics&#31454;&#36187;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;15&#20010;&#19981;&#21516;&#30340;&#22270;&#34920;&#31867;&#21035;&#65292;&#21253;&#25324;22923&#20010;&#35757;&#32451;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Charts represent an essential source of visual information in documents and facilitate a deep understanding and interpretation of information typically conveyed numerically. In the scientific literature, there are many charts, each with its stylistic differences. Recently the document understanding community has begun to address the problem of automatic chart understanding, which begins with chart classification. In this paper, we present a survey of the current state-of-the-art techniques for chart classification and discuss the available datasets and their supported chart types. We broadly classify these contributions as traditional approaches based on ML, CNN, and Transformers. Furthermore, we carry out an extensive comparative performance analysis of CNN-based and transformer-based approaches on the recently published CHARTINFO UB-UNITECH PMC dataset for the CHART-Infographics competition at ICPR 2022. The data set includes 15 different chart categories, including 22,923 training i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#31216;&#20026;&#20998;&#21106;-&#31867;&#21035;&#28608;&#27963;&#26144;&#23556;&#65288;SeCAM&#65289;&#12290;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#29616;&#26377;&#31639;&#27861;&#30340;&#20248;&#28857;&#65292;&#24182;&#20811;&#26381;&#20102;&#23427;&#20204;&#30340;&#32570;&#28857;&#65292;&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#39044;&#27979;&#36827;&#34892;&#35299;&#37322;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.04137</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#30340;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22312;&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Novel Explainable Artificial Intelligence Model in Image Classification problem. (arXiv:2307.04137v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04137
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#31216;&#20026;&#20998;&#21106;-&#31867;&#21035;&#28608;&#27963;&#26144;&#23556;&#65288;SeCAM&#65289;&#12290;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#29616;&#26377;&#31639;&#27861;&#30340;&#20248;&#28857;&#65292;&#24182;&#20811;&#26381;&#20102;&#23427;&#20204;&#30340;&#32570;&#28857;&#65292;&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#39044;&#27979;&#36827;&#34892;&#35299;&#37322;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#22312;&#35768;&#22810;&#19981;&#21516;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#65292;&#24182;&#23545;&#20154;&#31867;&#29983;&#27963;&#20135;&#29983;&#20102;&#28145;&#36828;&#32780;&#30452;&#25509;&#30340;&#24433;&#21709;&#12290;&#38543;&#20043;&#32780;&#26469;&#30340;&#26159;&#65292;&#23545;&#20110;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#30340;&#21407;&#29702;&#30340;&#38656;&#27714;&#12290;&#30001;&#20110;&#24403;&#21069;&#22823;&#22810;&#25968;&#39640;&#31934;&#24230;&#27169;&#22411;&#37117;&#26159;&#40657;&#30418;&#23376;&#65292;&#26082;&#26377;AI&#31185;&#23398;&#23478;&#20063;&#27809;&#26377;&#26368;&#32456;&#29992;&#25143;&#28145;&#20837;&#29702;&#35299;&#36825;&#20123;&#27169;&#22411;&#30340;&#20869;&#37096;&#24773;&#20917;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#35299;&#37322;AI&#27169;&#22411;&#30340;&#30446;&#30340;&#65292;&#30740;&#31350;&#20102;&#35768;&#22810;&#31639;&#27861;&#65292;&#23588;&#20854;&#26159;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#20013;&#65292;&#22914;LIME&#12289;CAM&#12289;GradCAM&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31639;&#27861;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#27604;&#22914;LIME&#30340;&#25191;&#34892;&#26102;&#38388;&#38271;&#21644;CAM&#23545;&#20855;&#20307;&#24615;&#21644;&#28165;&#26224;&#24615;&#30340;&#35299;&#37322;&#19981;&#28165;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#20998;&#21106;-&#31867;&#21035;&#28608;&#27963;&#26144;&#23556;&#65288;SeCAM&#65289;&#65292;&#23427;&#32467;&#21512;&#20102;&#19978;&#36848;&#31639;&#27861;&#30340;&#20248;&#28857;&#65292;&#21516;&#26102;&#20811;&#26381;&#20102;&#23427;&#20204;&#30340;&#32570;&#28857;&#12290;&#25105;&#20204;&#29992;&#21508;&#31181;&#25968;&#25454;&#38598;&#26469;&#27979;&#35797;&#20102;&#36825;&#20010;&#31639;&#27861;&#65292;&#24182;&#19982;&#20854;&#20182;&#31639;&#27861;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, artificial intelligence is increasingly being applied widely in many different fields and has a profound and direct impact on human life. Following this is the need to understand the principles of the model making predictions. Since most of the current high-precision models are black boxes, neither the AI scientist nor the end-user deeply understands what's going on inside these models. Therefore, many algorithms are studied for the purpose of explaining AI models, especially those in the problem of image classification in the field of computer vision such as LIME, CAM, GradCAM. However, these algorithms still have limitations such as LIME's long execution time and CAM's confusing interpretation of concreteness and clarity. Therefore, in this paper, we propose a new method called Segmentation - Class Activation Mapping (SeCAM) that combines the advantages of these algorithms above, while at the same time overcoming their disadvantages. We tested this algorithm with var
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#35270;&#39057;&#29255;&#27573;&#20013;&#25552;&#21462;&#30340;&#29289;&#20307;&#34892;&#20026;&#36827;&#34892;&#25512;&#29702;&#26469;&#35782;&#21035;&#21103;&#35789;&#31867;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25928;&#26524;&#19978;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.04132</link><description>&lt;p&gt;
&#23545;&#35270;&#39057;&#29255;&#27573;&#20013;&#29289;&#20307;&#34892;&#20026;&#30340;&#25512;&#29702;&#29992;&#20110;&#21103;&#35789;&#31867;&#22411;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Reasoning over the Behaviour of Objects in Video-Clips for Adverb-Type Recognition. (arXiv:2307.04132v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#35270;&#39057;&#29255;&#27573;&#20013;&#25552;&#21462;&#30340;&#29289;&#20307;&#34892;&#20026;&#36827;&#34892;&#25512;&#29702;&#26469;&#35782;&#21035;&#21103;&#35789;&#31867;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25928;&#26524;&#19978;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26681;&#25454;&#23545;&#25551;&#36848;&#22330;&#26223;&#24207;&#21015;&#30340;&#21103;&#35789;&#26368;&#20339;&#35782;&#21035;&#26041;&#27861;&#20026;&#22522;&#30784;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#20174;&#21407;&#22987;&#35270;&#39057;&#29255;&#27573;&#20013;&#25552;&#21462;&#30340;&#29289;&#20307;&#34892;&#20026;&#36827;&#34892;&#25512;&#29702;&#26469;&#35782;&#21035;&#29255;&#27573;&#23545;&#24212;&#30340;&#21103;&#35789;&#31867;&#22411;&#12290;&#19982;&#20043;&#21069;&#38024;&#23545;&#24120;&#35268;&#22330;&#26223;&#21103;&#35789;&#35782;&#21035;&#30340;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#35270;&#39057;&#29255;&#27573;&#30340;&#34892;&#21160;&#31867;&#22411;&#26410;&#30693;&#30340;&#26356;&#19968;&#33324;&#30340;&#38382;&#39064;&#35774;&#32622;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27969;&#31243;&#65292;&#20174;&#21407;&#22987;&#35270;&#39057;&#29255;&#27573;&#20013;&#25552;&#21462;&#20102;&#21487;&#20197;&#20154;&#31867;&#29702;&#35299;&#30340;&#29289;&#20307;&#34892;&#20026;&#20107;&#23454;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#31526;&#21495;&#21644;&#36716;&#25442;&#22120;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#23545;&#36825;&#20123;&#25552;&#21462;&#20986;&#30340;&#20107;&#23454;&#36827;&#34892;&#25805;&#20316;&#20197;&#35782;&#21035;&#21103;&#35789;&#31867;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#25903;&#25345;&#31526;&#21495;&#35270;&#39057;&#22788;&#29702;&#30340;&#24037;&#20316;&#65292;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#35270;&#39057;&#39046;&#22495;&#30456;&#20851;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, following the intuition that adverbs describing scene-sequences are best identified by reasoning over high-level concepts of object-behavior, we propose the design of a new framework that reasons over object-behaviours extracted from raw-video-clips to recognize the clip's corresponding adverb-types. Importantly, while previous works for general scene adverb-recognition assume knowledge of the clips underlying action-types, our method is directly applicable in the more general problem setting where the action-type of a video-clip is unknown. Specifically, we propose a novel pipeline that extracts human-interpretable object-behaviour-facts from raw video clips and propose novel symbolic and transformer based reasoning methods that operate over these extracted facts to identify adverb-types. Experiment results demonstrate that our proposed methods perform favourably against the previous state-of-the-art. Additionally, to support efforts in symbolic video-processing, we rele
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#30899;&#25928;&#29575;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65288;CE-NAS&#65289;&#65292;&#36890;&#36807;&#21160;&#24577;&#24179;&#34913;&#33021;&#25928;&#25277;&#26679;&#21644;&#33021;&#32791;&#35780;&#20272;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#30899;&#21644;&#25628;&#32034;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.04131</link><description>&lt;p&gt;
&#30899;&#25928;&#29575;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Carbon-Efficient Neural Architecture Search. (arXiv:2307.04131v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#30899;&#25928;&#29575;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65288;CE-NAS&#65289;&#65292;&#36890;&#36807;&#21160;&#24577;&#24179;&#34913;&#33021;&#25928;&#25277;&#26679;&#21644;&#33021;&#32791;&#35780;&#20272;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#30899;&#21644;&#25628;&#32034;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#27169;&#22411;&#35774;&#35745;&#36807;&#31243;&#20013;&#38477;&#20302;&#33021;&#28304;&#25104;&#26412;&#24182;&#25552;&#39640;&#30899;&#25928;&#29575;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#31216;&#20026;&#30899;&#25928;&#29575;NAS&#65288;CE-NAS&#65289;&#65292;&#23427;&#30001;&#20855;&#26377;&#19981;&#21516;&#33021;&#28304;&#38656;&#27714;&#30340;NAS&#35780;&#20272;&#31639;&#27861;&#12289;&#22810;&#30446;&#26631;&#20248;&#21270;&#22120;&#21644;&#21551;&#21457;&#24335;GPU&#20998;&#37197;&#31574;&#30053;&#32452;&#25104;&#12290;CE-NAS&#26681;&#25454;&#24403;&#21069;&#30340;&#30899;&#25490;&#25918;&#21160;&#24577;&#24179;&#34913;&#20102;&#33021;&#25928;&#25277;&#26679;&#21644;&#33021;&#32791;&#35780;&#20272;&#20219;&#21153;&#12290;&#20351;&#29992;&#26368;&#36817;&#30340;NAS&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#30899;&#36861;&#36394;&#25968;&#25454;&#65292;&#25105;&#20204;&#30340;&#36861;&#36394;&#39537;&#21160;&#27169;&#25311;&#34920;&#26126;CE-NAS&#22312;&#30899;&#21644;&#25628;&#32034;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#19977;&#20010;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents a novel approach to neural architecture search (NAS) that aims to reduce energy costs and increase carbon efficiency during the model design process. The proposed framework, called carbon-efficient NAS (CE-NAS), consists of NAS evaluation algorithms with different energy requirements, a multi-objective optimizer, and a heuristic GPU allocation strategy. CE-NAS dynamically balances energy-efficient sampling and energy-consuming evaluation tasks based on current carbon emissions. Using a recent NAS benchmark dataset and two carbon traces, our trace-driven simulations demonstrate that CE-NAS achieves better carbon and search efficiency than the three baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#23569;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#65292;&#20351;&#29992;&#25991;&#26412;&#23884;&#20837;&#21644;&#24230;&#37327;&#27169;&#22359;&#26469;&#25552;&#39640;&#22270;&#20687;&#20998;&#31867;&#24615;&#33021;&#21644;&#21487;&#36801;&#31227;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.04114</link><description>&lt;p&gt;
&#12298;FILM:&#22914;&#20309;&#35753;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#20174;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#21463;&#30410;?&#12299;
&lt;/p&gt;
&lt;p&gt;
FILM: How can Few-Shot Image Classification Benefit from Pre-Trained Language Models?. (arXiv:2307.04114v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04114
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#23569;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#65292;&#20351;&#29992;&#25991;&#26412;&#23884;&#20837;&#21644;&#24230;&#37327;&#27169;&#22359;&#26469;&#25552;&#39640;&#22270;&#20687;&#20998;&#31867;&#24615;&#33021;&#21644;&#21487;&#36801;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#23398;&#20064;&#26088;&#22312;&#35757;&#32451;&#33021;&#22815;&#21033;&#29992;&#23569;&#37327;&#26679;&#26412;&#25512;&#24191;&#21040;&#26032;&#31867;&#21035;&#30340;&#27169;&#22411;&#12290;&#26368;&#36817;&#65292;&#19968;&#31995;&#21015;&#24037;&#20316;&#25552;&#20986;&#21033;&#29992;&#21487;&#35775;&#38382;&#30340;&#31867;&#21035;&#21517;&#31216;&#35821;&#20041;&#20449;&#24687;&#26469;&#22686;&#24378;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#25913;&#36827;&#26631;&#20934;&#23569;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;&#20013;&#30340;&#35270;&#35273;&#21407;&#22411;&#21644;&#29305;&#24449;&#25552;&#21462;&#22120;&#31561;&#29616;&#26377;&#27169;&#22359;&#65292;&#38480;&#21046;&#20102;&#35821;&#20041;&#20449;&#24687;&#30340;&#20805;&#20998;&#21033;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#23569;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;&#12290;&#20026;&#20102;&#35299;&#20915;&#35270;&#35273;&#29305;&#24449;&#21644;&#25991;&#26412;&#23884;&#20837;&#20043;&#38388;&#30340;&#23545;&#40784;&#25361;&#25112;&#65292;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#20102;&#26694;&#26550;&#30340;&#25991;&#26412;&#20998;&#25903;&#65292;&#24182;&#24341;&#20837;&#20102;&#24230;&#37327;&#27169;&#22359;&#26469;&#25512;&#24191;&#20313;&#24358;&#30456;&#20284;&#24230;&#12290;&#20026;&#20102;&#26356;&#22909;&#30340;&#21487;&#36801;&#31227;&#24615;&#65292;&#25105;&#20204;&#35753;&#24230;&#37327;&#27169;&#22359;&#36866;&#24212;&#19981;&#21516;&#30340;&#23569;&#26679;&#26412;&#20219;&#21153;&#65292;&#24182;&#37319;&#29992;MAML&#36827;&#34892;&#21452;&#23618;&#20248;&#21270;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot learning aims to train models that can be generalized to novel classes with only a few samples. Recently, a line of works are proposed to enhance few-shot learning with accessible semantic information from class names. However, these works focus on improving existing modules such as visual prototypes and feature extractors of the standard few-shot learning framework. This limits the full potential use of semantic information. In this paper, we propose a novel few-shot learning framework that uses pre-trained language models based on contrastive learning. To address the challenge of alignment between visual features and textual embeddings obtained from text-based pre-trained language model, we carefully design the textual branch of our framework and introduce a metric module to generalize the cosine similarity. For better transferability, we let the metric module adapt to different few-shot tasks and adopt MAML to train the model via bi-level optimization. Moreover, we conduct 
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#30740;&#31350;&#36890;&#36807;&#29992;&#25143;&#23454;&#35777;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#21487;&#35299;&#37322;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#22312;&#33258;&#36866;&#24212;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#38382;&#39064;&#12290;&#36890;&#36807;&#25552;&#20379;&#21487;&#35270;&#21270;&#30340;&#27934;&#23519;&#65292;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#20851;&#38190;&#26102;&#38388;&#28857;&#30340;&#20915;&#31574;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2307.04098</link><description>&lt;p&gt;
&#19968;&#20010;&#20851;&#20110;&#21487;&#35299;&#37322;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#22312;&#33258;&#36866;&#24212;&#31995;&#32479;&#20013;&#24212;&#29992;&#30340;&#29992;&#25143;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A User Study on Explainable Online Reinforcement Learning for Adaptive Systems. (arXiv:2307.04098v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04098
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#30740;&#31350;&#36890;&#36807;&#29992;&#25143;&#23454;&#35777;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#21487;&#35299;&#37322;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#22312;&#33258;&#36866;&#24212;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#38382;&#39064;&#12290;&#36890;&#36807;&#25552;&#20379;&#21487;&#35270;&#21270;&#30340;&#27934;&#23519;&#65292;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#20851;&#38190;&#26102;&#38388;&#28857;&#30340;&#20915;&#31574;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35774;&#35745;&#26102;&#38388;&#30340;&#19981;&#30830;&#23450;&#24615;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;&#23454;&#29616;&#33258;&#36866;&#24212;&#31995;&#32479;&#12290;&#22312;&#32447;RL&#21033;&#29992;&#23454;&#38469;&#36816;&#34892;&#25968;&#25454;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#21033;&#29992;&#22312;&#36816;&#34892;&#26102;&#25165;&#33021;&#24471;&#21040;&#30340;&#21453;&#39304;&#12290;&#28982;&#32780;&#65292;&#22312;&#32447;RL&#38656;&#35201;&#23450;&#20041;&#19968;&#20010;&#26377;&#25928;&#19988;&#27491;&#30830;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#26469;&#37327;&#21270;&#23545;RL&#31639;&#27861;&#30340;&#21453;&#39304;&#24182;&#25351;&#23548;&#23398;&#20064;&#12290;&#38543;&#30528;&#28145;&#24230;RL&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#23398;&#21040;&#30340;&#30693;&#35782;&#19981;&#20877;&#20197;&#26174;&#24335;&#30340;&#26041;&#24335;&#34920;&#31034;&#65292;&#32780;&#26159;&#20197;&#31070;&#32463;&#32593;&#32476;&#30340;&#24418;&#24335;&#34920;&#31034;&#12290;&#23545;&#20110;&#20154;&#31867;&#26469;&#35828;&#65292;&#23558;&#31070;&#32463;&#32593;&#32476;&#30340;&#21442;&#25968;&#21270;&#19982;&#20855;&#20307;&#30340;RL&#20915;&#31574;&#32852;&#31995;&#36215;&#26469;&#20960;&#20046;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#22240;&#27492;&#65292;&#28145;&#24230;RL&#22312;&#26412;&#36136;&#19978;&#21464;&#25104;&#20102;&#19968;&#20010;&#40657;&#30418;&#23376;&#65292;&#20005;&#37325;&#38480;&#21046;&#20102;&#33258;&#36866;&#24212;&#31995;&#32479;&#30340;&#35843;&#35797;&#12290;&#25105;&#20204;&#20043;&#21069;&#20171;&#32461;&#20102;&#21487;&#35299;&#37322;RL&#25216;&#26415;XRL-DINE&#65292;&#23427;&#25552;&#20379;&#20102;&#23545;&#20851;&#38190;&#26102;&#38388;&#28857;&#30340;&#20915;&#31574;&#21407;&#22240;&#30340;&#21487;&#35270;&#21270;&#27934;&#23519;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#25324;54&#20010;&#36719;&#20214;&#29992;&#25143;&#30340;&#23454;&#35777;&#29992;&#25143;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Online reinforcement learning (RL) is increasingly used for realizing adaptive systems in the presence of design time uncertainty. Online RL facilitates learning from actual operational data and thereby leverages feedback only available at runtime. However, Online RL requires the definition of an effective and correct reward function, which quantifies the feedback to the RL algorithm and thereby guides learning. With Deep RL gaining interest, the learned knowledge is no longer explicitly represented, but is represented as a neural network. For a human, it becomes practically impossible to relate the parametrization of the neural network to concrete RL decisions. Deep RL thus essentially appears as a black box, which severely limits the debugging of adaptive systems. We previously introduced the explainable RL technique XRL-DINE, which provides visual insights into why certain decisions were made at important time points. Here, we introduce an empirical user study involving 54 software 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#20041;&#30693;&#35782;&#22270;&#33258;&#21160;&#21019;&#24314;&#25919;&#31574;&#36777;&#35770;&#26696;&#20363;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20105;&#35770;&#30340;&#35821;&#20041;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#38480;&#21046;&#26368;&#30701;&#36335;&#24452;&#36941;&#21382;&#65292;&#26377;&#25928;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#36777;&#35770;&#26696;&#20363;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#32654;&#22269;&#31454;&#36187;&#36777;&#35770;&#20013;&#65292;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#25913;&#36827;&#20102;&#24050;&#26377;&#25968;&#25454;&#38598;DebateSum&#65292;&#24182;&#36129;&#29486;&#20102;&#26032;&#30340;&#20363;&#23376;&#21644;&#26377;&#29992;&#30340;&#20803;&#25968;&#25454;&#12290;&#36890;&#36807;&#20351;&#29992;txtai&#35821;&#20041;&#25628;&#32034;&#21644;&#30693;&#35782;&#22270;&#24037;&#20855;&#38142;&#65292;&#21019;&#24314;&#21644;&#36129;&#29486;&#20102;9&#20010;&#35821;&#20041;&#30693;&#35782;&#22270;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#35780;&#20272;&#26041;&#27861;&#26469;&#30830;&#23450;&#21738;&#20010;&#30693;&#35782;&#22270;&#26356;&#36866;&#21512;&#25919;&#31574;&#36777;&#35770;&#26696;&#20363;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2307.04090</link><description>&lt;p&gt;
DebateKG: &#29992;&#35821;&#20041;&#30693;&#35782;&#22270;&#33258;&#21160;&#21019;&#24314;&#25919;&#31574;&#36777;&#35770;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
DebateKG: Automatic Policy Debate Case Creation with Semantic Knowledge Graphs. (arXiv:2307.04090v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#20041;&#30693;&#35782;&#22270;&#33258;&#21160;&#21019;&#24314;&#25919;&#31574;&#36777;&#35770;&#26696;&#20363;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20105;&#35770;&#30340;&#35821;&#20041;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#38480;&#21046;&#26368;&#30701;&#36335;&#24452;&#36941;&#21382;&#65292;&#26377;&#25928;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#36777;&#35770;&#26696;&#20363;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#32654;&#22269;&#31454;&#36187;&#36777;&#35770;&#20013;&#65292;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#25913;&#36827;&#20102;&#24050;&#26377;&#25968;&#25454;&#38598;DebateSum&#65292;&#24182;&#36129;&#29486;&#20102;&#26032;&#30340;&#20363;&#23376;&#21644;&#26377;&#29992;&#30340;&#20803;&#25968;&#25454;&#12290;&#36890;&#36807;&#20351;&#29992;txtai&#35821;&#20041;&#25628;&#32034;&#21644;&#30693;&#35782;&#22270;&#24037;&#20855;&#38142;&#65292;&#21019;&#24314;&#21644;&#36129;&#29486;&#20102;9&#20010;&#35821;&#20041;&#30693;&#35782;&#22270;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#35780;&#20272;&#26041;&#27861;&#26469;&#30830;&#23450;&#21738;&#20010;&#30693;&#35782;&#22270;&#26356;&#36866;&#21512;&#25919;&#31574;&#36777;&#35770;&#26696;&#20363;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30456;&#20851;&#24037;&#20316;&#34920;&#26126;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#22312;&#35299;&#20915;&#31454;&#36187;&#36777;&#35770;&#20013;&#30340;&#38382;&#39064;&#26041;&#38754;&#20855;&#26377;&#24212;&#29992;&#24615;&#12290;&#31454;&#36187;&#36777;&#35770;&#20013;&#26368;&#37325;&#35201;&#30340;&#20219;&#21153;&#20043;&#19968;&#26159;&#36777;&#25163;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#36777;&#35770;&#26696;&#20363;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#38480;&#21046;&#26368;&#30701;&#36335;&#24452;&#36941;&#21382;&#22312;&#20105;&#35770;&#30340;&#35821;&#20041;&#30693;&#35782;&#22270;&#19978;&#26500;&#24314;&#26377;&#25928;&#30340;&#36777;&#35770;&#26696;&#20363;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21517;&#20026;DebateSum&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#30740;&#31350;&#20102;&#36825;&#31181;&#28508;&#21147;&#65292;&#35813;&#25968;&#25454;&#38598;&#38024;&#23545;&#30340;&#26159;&#19968;&#31181;&#21517;&#20026;&#25919;&#31574;&#36777;&#35770;&#30340;&#32654;&#22269;&#31454;&#36187;&#36777;&#35770;&#31867;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#21521;&#25968;&#25454;&#38598;&#20013;&#24341;&#20837;53180&#20010;&#26032;&#30340;&#20363;&#23376;&#65292;&#24182;&#20026;&#27599;&#20010;&#20363;&#23376;&#25552;&#20379;&#36827;&#19968;&#27493;&#26377;&#29992;&#30340;&#20803;&#25968;&#25454;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;DebateSum&#12290;&#25105;&#20204;&#21033;&#29992;txtai&#35821;&#20041;&#25628;&#32034;&#21644;&#30693;&#35782;&#22270;&#24037;&#20855;&#38142;&#22522;&#20110;&#36825;&#20010;&#25968;&#25454;&#38598;&#20135;&#29983;&#24182;&#36129;&#29486;&#20102;9&#20010;&#35821;&#20041;&#30693;&#35782;&#22270;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#30830;&#23450;&#22312;&#25919;&#31574;&#36777;&#35770;&#26696;&#20363;&#29983;&#25104;&#30340;&#32972;&#26223;&#19979;&#21738;&#20010;&#30693;&#35782;&#22270;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work within the Argument Mining community has shown the applicability of Natural Language Processing systems for solving problems found within competitive debate. One of the most important tasks within competitive debate is for debaters to create high quality debate cases. We show that effective debate cases can be constructed using constrained shortest path traversals on Argumentative Semantic Knowledge Graphs. We study this potential in the context of a type of American Competitive Debate, called Policy Debate, which already has a large scale dataset targeting it called DebateSum. We significantly improve upon DebateSum by introducing 53180 new examples, as well as further useful metadata for every example, to the dataset. We leverage the txtai semantic search and knowledge graph toolchain to produce and contribute 9 semantic knowledge graphs built on this dataset. We create a unique method for evaluating which knowledge graphs are better in the context of producing policy deb
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65288;AMUCL&#65289;&#65292;&#29992;&#20110;&#20998;&#26512;&#30284;&#30151;&#22810;&#32452;&#23398;&#25968;&#25454;&#24182;&#35782;&#21035;&#30284;&#30151;&#26032;&#20122;&#22411;&#12290;&#36890;&#36807;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#35299;&#32806;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#65288;DMACL&#65289;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#28145;&#24230;&#25552;&#21462;&#22810;&#32452;&#23398;&#25968;&#25454;&#29305;&#24449;&#24182;&#36827;&#34892;&#20122;&#22411;&#32858;&#31867;&#12290;</title><link>http://arxiv.org/abs/2307.04075</link><description>&lt;p&gt;
&#22522;&#20110;&#30284;&#30151;&#22810;&#32452;&#23398;&#25968;&#25454;&#30340;&#30284;&#30151;&#26032;&#20122;&#22411;&#21644;&#27835;&#30103;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-Head Attention Mechanism Learning for Cancer New Subtypes and Treatment Based on Cancer Multi-Omics Data. (arXiv:2307.04075v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65288;AMUCL&#65289;&#65292;&#29992;&#20110;&#20998;&#26512;&#30284;&#30151;&#22810;&#32452;&#23398;&#25968;&#25454;&#24182;&#35782;&#21035;&#30284;&#30151;&#26032;&#20122;&#22411;&#12290;&#36890;&#36807;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#35299;&#32806;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#65288;DMACL&#65289;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#28145;&#24230;&#25552;&#21462;&#22810;&#32452;&#23398;&#25968;&#25454;&#29305;&#24449;&#24182;&#36827;&#34892;&#20122;&#22411;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#30284;&#30151;&#30340;&#39640;&#24322;&#36136;&#24615;&#21644;&#20020;&#24202;&#29305;&#24449;&#65292;&#19981;&#21516;&#30284;&#30151;&#20122;&#22411;&#20043;&#38388;&#30340;&#22810;&#32452;&#23398;&#25968;&#25454;&#21644;&#20020;&#24202;&#29305;&#24449;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#30284;&#30151;&#20122;&#22411;&#30340;&#35782;&#21035;&#21644;&#21457;&#29616;&#23545;&#20110;&#30284;&#30151;&#30340;&#35786;&#26029;&#12289;&#27835;&#30103;&#21644;&#39044;&#21518;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65288;AMUCL&#65289;&#65292;&#29992;&#20110;&#20998;&#26512;&#30284;&#30151;&#22810;&#32452;&#23398;&#25968;&#25454;&#65292;&#20174;&#32780;&#35782;&#21035;&#21644;&#34920;&#24449;&#30284;&#30151;&#20122;&#22411;&#12290;AMUCL&#26694;&#26550;&#21253;&#25324;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#29992;&#20110;&#28145;&#24230;&#25552;&#21462;&#22810;&#32452;&#23398;&#25968;&#25454;&#29305;&#24449;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#35299;&#32806;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#65288;DMACL&#65289;&#65292;&#29992;&#20110;&#23398;&#20064;&#22810;&#32452;&#23398;&#25968;&#25454;&#29305;&#24449;&#21644;&#32858;&#31867;&#65292;&#24182;&#35782;&#21035;&#26032;&#30340;&#30284;&#30151;&#20122;&#22411;&#12290;&#36825;&#31181;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#29305;&#24449;&#31354;&#38388;&#20013;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#26469;&#32858;&#31867;&#20122;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the high heterogeneity and clinical characteristics of cancer, there are significant differences in multi-omics data and clinical features among subtypes of different cancers. Therefore, the identification and discovery of cancer subtypes are crucial for the diagnosis, treatment, and prognosis of cancer. In this study, we proposed a generalization framework based on attention mechanisms for unsupervised contrastive learning (AMUCL) to analyze cancer multi-omics data for the identification and characterization of cancer subtypes. AMUCL framework includes a unsupervised multi-head attention mechanism, which deeply extracts multi-omics data features. Importantly, a decoupled contrastive learning model (DMACL) based on a multi-head attention mechanism is proposed to learn multi-omics data features and clusters and identify new cancer subtypes. This unsupervised contrastive learning method clusters subtypes by calculating the similarity between samples in the feature space and sample
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#31574;&#30053;&#24615;&#20080;&#23478;&#30340;&#24773;&#22659;&#21160;&#24577;&#23450;&#20215;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#21160;&#24577;&#23450;&#20215;&#31574;&#30053;&#65292;&#23558;&#20080;&#23478;&#30340;&#31574;&#30053;&#34892;&#20026;&#32435;&#20837;&#22312;&#32447;&#23398;&#20064;&#20013;&#65292;&#20197;&#26368;&#22823;&#21270;&#21334;&#26041;&#30340;&#32047;&#35745;&#25910;&#30410;&#12290;</title><link>http://arxiv.org/abs/2307.04055</link><description>&lt;p&gt;
&#20855;&#26377;&#31574;&#30053;&#24615;&#20080;&#23478;&#30340;&#24773;&#22659;&#21160;&#24577;&#23450;&#20215;
&lt;/p&gt;
&lt;p&gt;
Contextual Dynamic Pricing with Strategic Buyers. (arXiv:2307.04055v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#31574;&#30053;&#24615;&#20080;&#23478;&#30340;&#24773;&#22659;&#21160;&#24577;&#23450;&#20215;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#21160;&#24577;&#23450;&#20215;&#31574;&#30053;&#65292;&#23558;&#20080;&#23478;&#30340;&#31574;&#30053;&#34892;&#20026;&#32435;&#20837;&#22312;&#32447;&#23398;&#20064;&#20013;&#65292;&#20197;&#26368;&#22823;&#21270;&#21334;&#26041;&#30340;&#32047;&#35745;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#23450;&#20215;&#26159;&#20225;&#19994;&#24120;&#29992;&#30340;&#19968;&#31181;&#38024;&#23545;&#20010;&#20307;&#29305;&#24449;&#21046;&#23450;&#20215;&#26684;&#30340;&#31574;&#30053;&#12290;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;&#20080;&#23478;&#20063;&#21487;&#20197;&#36890;&#36807;&#25805;&#32437;&#29305;&#24449;&#25968;&#25454;&#26469;&#33719;&#21462;&#26356;&#20302;&#30340;&#20215;&#26684;&#65292;&#20294;&#36825;&#20063;&#20250;&#23548;&#33268;&#29305;&#23450;&#30340;&#25805;&#20316;&#25104;&#26412;&#12290;&#36825;&#31181;&#31574;&#30053;&#34892;&#20026;&#21487;&#33021;&#20250;&#38459;&#30861;&#20225;&#19994;&#26368;&#22823;&#21270;&#21033;&#28070;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#31574;&#30053;&#24615;&#20080;&#23478;&#30340;&#24773;&#22659;&#21160;&#24577;&#23450;&#20215;&#38382;&#39064;&#12290;&#21334;&#26041;&#26080;&#27861;&#35266;&#23519;&#21040;&#20080;&#23478;&#30340;&#30495;&#23454;&#29305;&#24449;&#65292;&#32780;&#21482;&#33021;&#35266;&#23519;&#21040;&#20080;&#23478;&#26681;&#25454;&#31574;&#30053;&#34892;&#20026;&#25805;&#32437;&#21518;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#21334;&#26041;&#21482;&#33021;&#35266;&#23519;&#21040;&#20080;&#23478;&#23545;&#20135;&#21697;&#30340;&#20272;&#20540;&#65292;&#32780;&#26080;&#27861;&#30452;&#25509;&#33719;&#21462;&#20855;&#20307;&#25968;&#20540;&#65292;&#21482;&#33021;&#24471;&#21040;&#19968;&#20010;&#20108;&#36827;&#21046;&#30340;&#21709;&#24212;&#65292;&#34920;&#31034;&#26159;&#21542;&#21457;&#29983;&#38144;&#21806;&#12290;&#37492;&#20110;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#21160;&#24577;&#23450;&#20215;&#31574;&#30053;&#65292;&#23558;&#20080;&#23478;&#30340;&#31574;&#30053;&#34892;&#20026;&#32435;&#20837;&#22312;&#32447;&#23398;&#20064;&#20013;&#65292;&#20197;&#26368;&#22823;&#21270;&#21334;&#26041;&#30340;&#32047;&#35745;&#25910;&#30410;&#12290;&#39318;&#20808;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;&#19981;&#32771;&#34385;&#31574;&#30053;&#24615;&#30340;&#23450;&#20215;&#31574;&#30053;&#30340;&#23384;&#22312;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized pricing, which involves tailoring prices based on individual characteristics, is commonly used by firms to implement a consumer-specific pricing policy. In this process, buyers can also strategically manipulate their feature data to obtain a lower price, incurring certain manipulation costs. Such strategic behavior can hinder firms from maximizing their profits. In this paper, we study the contextual dynamic pricing problem with strategic buyers. The seller does not observe the buyer's true feature, but a manipulated feature according to buyers' strategic behavior. In addition, the seller does not observe the buyers' valuation of the product, but only a binary response indicating whether a sale happens or not. Recognizing these challenges, we propose a strategic dynamic pricing policy that incorporates the buyers' strategic behavior into the online learning to maximize the seller's cumulative revenue. We first prove that existing non-strategic pricing policies that neglect
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20026;&#24555;&#36882;&#36816;&#33829;&#20013;&#30340;&#36127;&#36733;&#35268;&#21010;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#20840;&#38754;&#32771;&#34385;&#20102;&#36127;&#36733;&#21644;&#27969;&#31243;&#35268;&#21010;&#30340;&#25361;&#25112;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#20915;&#31574;&#25903;&#25345;&#24037;&#20855;&#12290;&#30740;&#31350;&#21457;&#29616;&#22312;&#32593;&#32476;&#20013;&#23384;&#22312;&#22823;&#37327;&#30340;&#23545;&#31216;&#24615;&#65292;&#23548;&#33268;&#20248;&#21270;&#27714;&#35299;&#22120;&#36820;&#22238;&#19981;&#21516;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#38477;&#20302;&#20102;&#35268;&#21010;&#20154;&#21592;&#23545;&#20248;&#21270;&#27714;&#35299;&#30340;&#20449;&#20219;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.04050</link><description>&lt;p&gt;
&#22522;&#20110;&#20248;&#21270;&#30340;&#23398;&#20064;&#29992;&#20110;&#21345;&#36710;&#36816;&#36755;&#26381;&#21153;&#32593;&#32476;&#20013;&#30340;&#21160;&#24577;&#36127;&#36733;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Optimization-based Learning for Dynamic Load Planning in Trucking Service Networks. (arXiv:2307.04050v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20026;&#24555;&#36882;&#36816;&#33829;&#20013;&#30340;&#36127;&#36733;&#35268;&#21010;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#20840;&#38754;&#32771;&#34385;&#20102;&#36127;&#36733;&#21644;&#27969;&#31243;&#35268;&#21010;&#30340;&#25361;&#25112;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#20915;&#31574;&#25903;&#25345;&#24037;&#20855;&#12290;&#30740;&#31350;&#21457;&#29616;&#22312;&#32593;&#32476;&#20013;&#23384;&#22312;&#22823;&#37327;&#30340;&#23545;&#31216;&#24615;&#65292;&#23548;&#33268;&#20248;&#21270;&#27714;&#35299;&#22120;&#36820;&#22238;&#19981;&#21516;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#38477;&#20302;&#20102;&#35268;&#21010;&#20154;&#21592;&#23545;&#20248;&#21270;&#27714;&#35299;&#30340;&#20449;&#20219;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36127;&#36733;&#35268;&#21010;&#38382;&#39064;&#26159;&#24555;&#36882;&#36816;&#33829;&#20013;&#26381;&#21153;&#32593;&#32476;&#35774;&#35745;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#23427;&#20915;&#23450;&#20102;&#22312;&#32456;&#31471;&#20043;&#38388;&#22914;&#20309;&#22312;&#26102;&#38388;&#19978;&#20998;&#37197;&#22810;&#23569;&#36742;&#25302;&#36710;&#65288;&#25110;&#36127;&#36733;&#65289;&#36827;&#34892;&#27966;&#36963;&#12290;&#21478;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#30830;&#23450;&#19968;&#20010;&#27969;&#31243;&#35745;&#21010;&#65292;&#23427;&#25351;&#23450;&#20102;&#22914;&#20309;&#23558;&#21253;&#35065;&#20307;&#31215;&#20998;&#37197;&#32473;&#35745;&#21010;&#30340;&#36127;&#36733;&#12290;&#26412;&#25991;&#32771;&#34385;&#21040;&#20102;&#21160;&#24577;&#36127;&#36733;&#35268;&#21010;&#38382;&#39064;&#65288;DLPP&#65289;&#65292;&#23427;&#21516;&#26102;&#32771;&#34385;&#20102;&#36127;&#36733;&#21644;&#27969;&#31243;&#35268;&#21010;&#30340;&#25361;&#25112;&#65292;&#20197;&#22312;&#25805;&#20316;&#26085;&#20043;&#21069;&#38543;&#30528;&#38656;&#27714;&#39044;&#27979;&#30340;&#21464;&#21270;&#32780;&#35843;&#25972;&#36127;&#36733;&#21644;&#27969;&#31243;&#12290;&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#20915;&#31574;&#25903;&#25345;&#24037;&#20855;&#65292;&#20026;&#32593;&#32476;&#20013;&#21508;&#20010;&#32456;&#31471;&#30340;&#35268;&#21010;&#20154;&#21592;&#25552;&#20379;&#20915;&#31574;&#25903;&#25345;&#12290;&#26412;&#25991;&#23558;DLPP&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;MIP&#65292;&#24182;&#35777;&#26126;&#23427;&#22312;&#27599;&#20010;&#21830;&#21697;&#37117;&#21487;&#20197;&#36890;&#36807;&#20027;&#36335;&#24452;&#21644;&#22791;&#29992;&#36335;&#24452;&#36827;&#34892;&#36335;&#30001;&#30340;&#32593;&#32476;&#20013;&#26377;&#22823;&#37327;&#30340;&#23545;&#31216;&#24615;&#12290;&#22240;&#27492;&#65292;&#20248;&#21270;&#27714;&#35299;&#22120;&#21487;&#33021;&#20250;&#23545;&#23494;&#20999;&#30456;&#20851;&#30340;&#38382;&#39064;&#36820;&#22238;&#26681;&#26412;&#19981;&#21516;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#35268;&#21010;&#20154;&#21592;&#24863;&#21040;&#22256;&#24785;&#65292;&#38477;&#20302;&#23545;&#20248;&#21270;&#27714;&#35299;&#30340;&#20449;&#20219;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The load planning problem is a critical challenge in service network design for parcel carriers: it decides how many trailers (or loads) to assign for dispatch over time between pairs of terminals. Another key challenge is to determine a flow plan, which specifies how parcel volumes are assigned to planned loads. This paper considers the Dynamic Load Planning Problem (DLPP) that considers both flow and load planning challenges jointly to adjust loads and flows as the demand forecast changes over time before the day of operations. The paper aims at developing a decision-support tool to inform planners making these decisions at terminals across the network. The paper formulates the DLPP as a MIP and shows that it admits a large number of symmetries in a network where each commodity can be routed through primary and alternate paths. As a result, an optimization solver may return fundamentally different solutions to closely related problems, confusing planners and reducing trust in optimiz
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35774;&#35745;&#20102;DeepFuse&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#23454;&#29616;&#29992;&#25143;&#21644;CNN&#20043;&#38388;&#30452;&#25509;&#21453;&#39304;&#24490;&#29615;&#30340;&#20132;&#20114;&#35774;&#35745;&#65292;&#36890;&#36807;&#23616;&#37096;&#35299;&#37322;&#24110;&#21161;CNN&#24037;&#31243;&#24072;&#31995;&#32479;&#22320;&#35786;&#26029;&#21644;&#20462;&#25913;CNN&#30340;&#33030;&#24369;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.04036</link><description>&lt;p&gt;
&#35774;&#35745;&#19968;&#20010;&#20154;&#31867;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#30452;&#25509;&#21453;&#39304;&#24490;&#29615;&#36890;&#36807;&#23616;&#37096;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Designing a Direct Feedback Loop between Humans and Convolutional Neural Networks through Local Explanations. (arXiv:2307.04036v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04036
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35774;&#35745;&#20102;DeepFuse&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#23454;&#29616;&#29992;&#25143;&#21644;CNN&#20043;&#38388;&#30452;&#25509;&#21453;&#39304;&#24490;&#29615;&#30340;&#20132;&#20114;&#35774;&#35745;&#65292;&#36890;&#36807;&#23616;&#37096;&#35299;&#37322;&#24110;&#21161;CNN&#24037;&#31243;&#24072;&#31995;&#32479;&#22320;&#35786;&#26029;&#21644;&#20462;&#25913;CNN&#30340;&#33030;&#24369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23616;&#37096;&#35299;&#37322;&#36890;&#36807;&#22270;&#20687;&#28909;&#22270;&#35299;&#37322;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#26159;&#22914;&#20309;&#24471;&#20986;&#20182;&#20204;&#30340;&#36755;&#20986;&#32467;&#26524;&#30340;&#12290;&#30001;&#20110;&#20854;&#30452;&#35266;&#26126;&#20102;&#30340;&#29305;&#28857;&#65292;&#35813;&#26041;&#27861;&#24050;&#25104;&#20026;&#35786;&#26029;CNN&#30340;&#26368;&#21463;&#27426;&#36814;&#30340;&#21487;&#35299;&#37322;AI&#65288;XAI&#65289;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#25105;&#20204;&#30340;&#21021;&#27493;&#30740;&#31350;&#65292;&#25105;&#20204;&#25429;&#25417;&#21040;ML&#24037;&#31243;&#24072;&#23545;&#23616;&#37096;&#35299;&#37322;&#30340;&#20215;&#20540;&#21644;&#19981;&#21487;&#25110;&#32570;&#30340;&#30475;&#27861;&#23384;&#22312;&#30683;&#30462;&#65306;&#19968;&#26041;&#38754;&#35748;&#20026;&#23616;&#37096;&#35299;&#37322;&#26159;&#26500;&#24314;CNN&#19981;&#21487;&#25110;&#32570;&#30340;&#24895;&#26223;&#65292;&#21478;&#19968;&#26041;&#38754;&#21364;&#21448;&#35748;&#20026;&#23427;&#26159;&#19968;&#20010;&#28040;&#32791;&#33021;&#37327;&#30340;&#36807;&#31243;&#65292;&#22240;&#20026;&#23427;&#22522;&#20110;&#21551;&#21457;&#24335;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#33030;&#24369;&#24615;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#35786;&#26029;&#25152;&#23398;&#21040;&#30340;&#33030;&#24369;&#24615;&#25351;&#23548;CNN&#30340;&#36807;&#31243;&#20063;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#24046;&#36317;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;DeepFuse&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#23454;&#29616;&#29992;&#25143;&#21644;CNN&#20043;&#38388;&#30452;&#25509;&#21453;&#39304;&#24490;&#29615;&#30340;&#20132;&#20114;&#35774;&#35745;&#65292;&#29992;&#20110;&#35786;&#26029;&#21644;&#20462;&#25913;CNN&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#20351;&#29992;&#23616;&#37096;&#35299;&#37322;&#24110;&#21161;CNN&#24037;&#31243;&#24072;&#31995;&#32479;&#22320;&#25628;&#32034;&#8220;&#19981;&#21512;&#29702;&#8221;&#30340;&#23616;&#37096;&#35299;&#37322;&#65292;&#24182;&#20026;&#37027;&#20123;&#34987;&#35782;&#21035;&#20026;&#19981;&#21512;&#29702;&#30340;&#35299;&#37322;&#26631;&#27880;&#26032;&#30340;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
The local explanation provides heatmaps on images to explain how Convolutional Neural Networks (CNNs) derive their output. Due to its visual straightforwardness, the method has been one of the most popular explainable AI (XAI) methods for diagnosing CNNs. Through our formative study (S1), however, we captured ML engineers' ambivalent perspective about the local explanation as a valuable and indispensable envision in building CNNs versus the process that exhausts them due to the heuristic nature of detecting vulnerability. Moreover, steering the CNNs based on the vulnerability learned from the diagnosis seemed highly challenging. To mitigate the gap, we designed DeepFuse, the first interactive design that realizes the direct feedback loop between a user and CNNs in diagnosing and revising CNN's vulnerability using local explanations. DeepFuse helps CNN engineers to systemically search "unreasonable" local explanations and annotate the new boundaries for those identified as unreasonable 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27979;&#35797;&#26102;&#39046;&#22495;&#27867;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#20351;&#29992;&#27010;&#29575;&#20266;&#26631;&#31614;&#21644;&#21464;&#20998;&#37051;&#23621;&#26631;&#31614;&#26469;&#25512;&#24191;&#28304;&#22495;&#35757;&#32451;&#30340;&#27169;&#22411;&#21040;&#30446;&#26631;&#39046;&#22495;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.04033</link><description>&lt;p&gt;
&#23398;&#20064;&#29992;&#20110;&#27979;&#35797;&#26102;&#39046;&#22495;&#27867;&#21270;&#30340;&#21464;&#20998;&#37051;&#23621;&#26631;&#31614;
&lt;/p&gt;
&lt;p&gt;
Learning Variational Neighbor Labels for Test-Time Domain Generalization. (arXiv:2307.04033v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27979;&#35797;&#26102;&#39046;&#22495;&#27867;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#20351;&#29992;&#27010;&#29575;&#20266;&#26631;&#31614;&#21644;&#21464;&#20998;&#37051;&#23621;&#26631;&#31614;&#26469;&#25512;&#24191;&#28304;&#22495;&#35757;&#32451;&#30340;&#27169;&#22411;&#21040;&#30446;&#26631;&#39046;&#22495;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#33268;&#21147;&#20110;&#39046;&#22495;&#27867;&#21270;&#65292;&#22312;&#26410;&#30693;&#30340;&#30446;&#26631;&#39046;&#22495;&#20013;&#21482;&#22312;&#28304;&#39046;&#22495;&#19978;&#36827;&#34892;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#28304;&#22495;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#30446;&#26631;&#22495;&#19978;&#36827;&#34892;&#25512;&#29702;&#65292;&#21033;&#29992;&#26080;&#26631;&#31614;&#30446;&#26631;&#25968;&#25454;&#26412;&#36523;&#30340;&#20215;&#20540;&#12290;&#25105;&#20204;&#20570;&#20986;&#20102;&#19977;&#20010;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30446;&#26631;&#26679;&#26412;&#30340;&#27010;&#29575;&#20266;&#26631;&#31614;&#65292;&#20197;&#22312;&#27979;&#35797;&#26102;&#23558;&#28304;&#39046;&#22495;&#35757;&#32451;&#30340;&#27169;&#22411;&#25512;&#24191;&#21040;&#30446;&#26631;&#39046;&#22495;&#12290;&#25105;&#20204;&#23558;&#27979;&#35797;&#26102;&#30340;&#25512;&#24191;&#24314;&#27169;&#20026;&#21464;&#20998;&#25512;&#29702;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#20266;&#26631;&#31614;&#24314;&#27169;&#20026;&#20998;&#24067;&#65292;&#32771;&#34385;&#27867;&#21270;&#36807;&#31243;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#20943;&#36731;&#20266;&#26631;&#31614;&#19981;&#20934;&#30830;&#24615;&#24102;&#26469;&#30340;&#35823;&#23548;&#20449;&#21495;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#21464;&#20998;&#37051;&#23621;&#26631;&#31614;&#65292;&#23558;&#37051;&#36817;&#30446;&#26631;&#26679;&#26412;&#30340;&#20449;&#24687;&#32435;&#20837;&#21040;&#29983;&#25104;&#26356;&#24378;&#40065;&#26834;&#20266;&#26631;&#31614;&#30340;&#36807;&#31243;&#20013;&#12290;&#31532;&#19977;&#65292;&#20026;&#20102;&#23398;&#20064;&#23558;&#26356;&#20855;&#20195;&#34920;&#24615;&#30340;&#30446;&#26631;&#20449;&#24687;&#32435;&#20837;&#21040;&#29983;&#25104;&#26356;&#20934;&#30830;&#12289;&#26356;&#24378;&#40065;&#26834;&#30340;&#21464;&#20998;&#37051;&#23621;&#26631;&#31614;&#30340;&#33021;&#21147;&#20013;&#65292;&#25105;&#20204;
&lt;/p&gt;
&lt;p&gt;
This paper strives for domain generalization, where models are trained exclusively on source domains before being deployed at unseen target domains. We follow the strict separation of source training and target testing but exploit the value of the unlabeled target data itself during inference. We make three contributions. First, we propose probabilistic pseudo-labeling of target samples to generalize the source-trained model to the target domain at test time. We formulate the generalization at test time as a variational inference problem by modeling pseudo labels as distributions to consider the uncertainty during generalization and alleviate the misleading signal of inaccurate pseudo labels. Second, we learn variational neighbor labels that incorporate the information of neighboring target samples to generate more robust pseudo labels. Third, to learn the ability to incorporate more representative target information and generate more precise and robust variational neighbor labels, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23436;&#20840;&#20449;&#24687;&#21338;&#24328;&#20013;&#29609;&#23478;&#20919;&#28448;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20197;&#29273;&#36824;&#29273;&#30340;&#27010;&#24565;&#20316;&#20026;&#35299;&#20915;&#29702;&#24615;&#36873;&#25321;&#24179;&#23616;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.04029</link><description>&lt;p&gt;
&#35770;"&#20919;&#28448;"&#21644;&#23436;&#20840;&#20449;&#24687;&#21338;&#24328;&#20013;&#30340;&#21453;&#21521;&#24402;&#32435;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On "Indifference" and Backward Induction in Games with Perfect Information. (arXiv:2307.04029v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23436;&#20840;&#20449;&#24687;&#21338;&#24328;&#20013;&#29609;&#23478;&#20919;&#28448;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20197;&#29273;&#36824;&#29273;&#30340;&#27010;&#24565;&#20316;&#20026;&#35299;&#20915;&#29702;&#24615;&#36873;&#25321;&#24179;&#23616;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28216;&#25103;&#20013;&#29609;&#23478;&#23545;&#20110;&#20004;&#20010;&#19981;&#21516;&#30340;&#32467;&#26524;&#20919;&#28448;&#26080;&#21160;&#20110;&#34935;&#65292;&#23567;&#30340;&#24494;&#25200;&#26080;&#27861;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22240;&#20026;&#23454;&#38469;&#30340;&#36873;&#25321;&#21487;&#33021;&#23545;&#20854;&#20182;&#29609;&#23478;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#65292;&#23548;&#33268;&#20182;&#20204;&#20197;&#23545;&#20919;&#28448;&#30340;&#29609;&#23478;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#30340;&#26041;&#24335;&#34892;&#21160;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#21487;&#20197;&#36890;&#36807;&#22522;&#20110;&#20854;&#20182;&#29609;&#23478;&#25928;&#29992;&#30340;&#29702;&#24615;&#27010;&#24565;&#30340;&#32454;&#21270;&#26469;&#35299;&#20915;&#29702;&#24615;&#36873;&#25321;&#20043;&#38388;&#30340;&#24179;&#23616;&#38382;&#39064;&#12290;&#20854;&#20013;&#19968;&#31181;&#32454;&#21270;&#26041;&#27861;&#26159;"&#20197;&#29273;&#36824;&#29273;"&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
Indifference of a player with respect to two distinct outcomes of a game cannot be handled by small perturbations, because the actual choice may have significant impact on other players, and cause them to act in a way that has significant impact of the indifferent player. It is argued that ties among rational choices can be resolved by refinements of the concept of rationality based on the utilities of other players. One such refinement is the concept of Tit-for-Tat.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#27169;&#20223;&#20154;&#31867;&#33402;&#26415;&#23478;&#26041;&#38754;&#30340;&#25104;&#21151;&#31243;&#24230;&#65292;&#24182;&#25552;&#20986;&#23558;&#29256;&#26435;&#36131;&#20219;&#19982;&#27169;&#22411;&#33021;&#21147;&#32852;&#31995;&#36215;&#26469;&#21487;&#33021;&#26159;&#26377;&#29992;&#30340;&#12290;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;Contrastive Language-Image Pretrained (CLIP)&#32534;&#30721;&#22120;&#23545;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#26469;&#34913;&#37327;&#27169;&#22411;&#27169;&#20223;&#29305;&#23450;&#33402;&#26415;&#23478;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.04028</link><description>&lt;p&gt;
&#35780;&#20272;&#25193;&#25955;&#27169;&#22411;&#22312;&#27169;&#20223;&#20154;&#31867;&#33402;&#26415;&#23478;&#26041;&#38754;&#30340;&#25104;&#21151;&#31243;&#24230;
&lt;/p&gt;
&lt;p&gt;
Measuring the Success of Diffusion Models at Imitating Human Artists. (arXiv:2307.04028v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04028
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#27169;&#20223;&#20154;&#31867;&#33402;&#26415;&#23478;&#26041;&#38754;&#30340;&#25104;&#21151;&#31243;&#24230;&#65292;&#24182;&#25552;&#20986;&#23558;&#29256;&#26435;&#36131;&#20219;&#19982;&#27169;&#22411;&#33021;&#21147;&#32852;&#31995;&#36215;&#26469;&#21487;&#33021;&#26159;&#26377;&#29992;&#30340;&#12290;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;Contrastive Language-Image Pretrained (CLIP)&#32534;&#30721;&#22120;&#23545;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#26469;&#34913;&#37327;&#27169;&#22411;&#27169;&#20223;&#29305;&#23450;&#33402;&#26415;&#23478;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#25193;&#25955;&#27169;&#22411;&#22312;&#20154;&#24037;&#26234;&#33021;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25104;&#26524;&#12290;&#23427;&#20204;&#30340;&#25104;&#21151;&#37096;&#20998;&#26159;&#22240;&#20026;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#20102;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#25968;&#25454;&#65292;&#20854;&#20013;&#32463;&#24120;&#21253;&#21547;&#26377;&#29256;&#26435;&#30340;&#20316;&#21697;&#12290;&#36825;&#24341;&#21457;&#20102;&#20851;&#20110;&#36825;&#20123;&#27169;&#22411;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#20174;&#20154;&#31867;&#33402;&#26415;&#23478;&#30340;&#20316;&#21697;&#20013;&#23398;&#20064;&#12289;&#27169;&#20223;&#25110;&#22797;&#21046;&#30340;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#35748;&#20026;&#65292;&#23558;&#29256;&#26435;&#36131;&#20219;&#19982;&#27169;&#22411;&#30340;&#33021;&#21147;&#32852;&#31995;&#36215;&#26469;&#65292;&#21487;&#33021;&#22312;&#29983;&#25104;&#27169;&#22411;&#30340;&#19981;&#26029;&#21457;&#23637;&#30340;&#29615;&#22659;&#20013;&#26159;&#26377;&#29992;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20851;&#20110;&#29256;&#26435;&#21644;&#29983;&#25104;&#31995;&#32479;&#30340;&#27861;&#24459;&#20998;&#26512;&#24448;&#24448;&#20391;&#37325;&#20110;&#20351;&#29992;&#21463;&#20445;&#25252;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#25968;&#25454;&#12289;&#35757;&#32451;&#21644;&#31995;&#32479;&#20043;&#38388;&#30340;&#32852;&#31995;&#24120;&#24120;&#34987;&#25513;&#30422;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#31616;&#21333;&#30340;&#22270;&#20687;&#20998;&#31867;&#25216;&#26415;&#65292;&#20197;&#34913;&#37327;&#27169;&#22411;&#27169;&#20223;&#29305;&#23450;&#33402;&#26415;&#23478;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;(CLIP)&#32534;&#30721;&#22120;&#20197;&#38646;&#26679;&#26412;&#30340;&#26041;&#24335;&#23545;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#36807;&#31243;&#39318;&#20808;&#25552;&#31034;&#27169;&#22411;&#27169;&#20223;&#29305;&#23450;&#30340;&#33402;&#26415;&#23478;&#65292;&#28982;&#21518;&#27979;&#35797;CLIP&#26159;&#21542;&#33021;&#22815;&#20934;&#30830;&#20998;&#31867;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern diffusion models have set the state-of-the-art in AI image generation. Their success is due, in part, to training on Internet-scale data which often includes copyrighted work. This prompts questions about the extent to which these models learn from, imitate, or copy the work of human artists. This work suggests that tying copyright liability to the capabilities of the model may be useful given the evolving ecosystem of generative models. Specifically, much of the legal analysis of copyright and generative systems focuses on the use of protected data for training. As a result, the connections between data, training, and the system are often obscured. In our approach, we consider simple image classification techniques to measure a model's ability to imitate specific artists. Specifically, we use Contrastive Language-Image Pretrained (CLIP) encoders to classify images in a zero-shot fashion. Our process first prompts a model to imitate a specific artist. Then, we test whether CLIP 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;GP&#24341;&#23548;&#30340;MPPI&#26041;&#27861;&#29992;&#20110;&#22312;&#22797;&#26434;&#26410;&#30693;&#26434;&#20081;&#29615;&#22659;&#20013;&#36827;&#34892;&#39640;&#25928;&#23548;&#33322;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#23616;&#37096;&#24863;&#30693;&#27169;&#22411;&#21644;&#22312;&#32447;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#26500;&#24314;&#19981;&#30830;&#23450;&#24615;&#34920;&#38754;&#65292;&#35782;&#21035;&#24182;&#25512;&#33616;&#26368;&#20248;&#23376;&#30446;&#26631;&#32473;&#23616;&#37096;&#30340;MPPI&#35268;&#21010;&#22120;&#12290;&#26368;&#32456;&#23454;&#29616;&#20102;&#28385;&#36275;&#35201;&#27714;&#30340;&#26368;&#20248;&#25511;&#21046;&#24207;&#21015;&#12290;</title><link>http://arxiv.org/abs/2307.04019</link><description>&lt;p&gt;
GP&#24341;&#23548;&#30340;MPPI&#22312;&#22797;&#26434;&#26410;&#30693;&#26434;&#20081;&#29615;&#22659;&#20013;&#30340;&#39640;&#25928;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
GP-guided MPPI for Efficient Navigation in Complex Unknown Cluttered Environments. (arXiv:2307.04019v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;GP&#24341;&#23548;&#30340;MPPI&#26041;&#27861;&#29992;&#20110;&#22312;&#22797;&#26434;&#26410;&#30693;&#26434;&#20081;&#29615;&#22659;&#20013;&#36827;&#34892;&#39640;&#25928;&#23548;&#33322;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#23616;&#37096;&#24863;&#30693;&#27169;&#22411;&#21644;&#22312;&#32447;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#26500;&#24314;&#19981;&#30830;&#23450;&#24615;&#34920;&#38754;&#65292;&#35782;&#21035;&#24182;&#25512;&#33616;&#26368;&#20248;&#23376;&#30446;&#26631;&#32473;&#23616;&#37096;&#30340;MPPI&#35268;&#21010;&#22120;&#12290;&#26368;&#32456;&#23454;&#29616;&#20102;&#28385;&#36275;&#35201;&#27714;&#30340;&#26368;&#20248;&#25511;&#21046;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20855;&#26377;&#26377;&#38480;&#24863;&#30693;&#33021;&#21147;&#30340;&#26410;&#30693;&#26434;&#20081;&#29615;&#22659;&#20013;&#36827;&#34892;&#26426;&#22120;&#20154;&#23548;&#33322;&#23545;&#26426;&#22120;&#20154;&#23398;&#26469;&#35828;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#23616;&#37096;&#36712;&#36857;&#20248;&#21270;&#26041;&#27861;&#65292;&#22914;&#27169;&#22411;&#39044;&#27979;&#36335;&#24452;&#31215;&#20998;&#65288;MPPI&#65289;&#65292;&#26159;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#36935;&#21040;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#26465;&#20214;&#25110;&#22312;&#35745;&#21010;&#33539;&#22260;&#20043;&#22806;&#23548;&#33322;&#26102;&#65292;&#38656;&#35201;&#20840;&#23616;&#24341;&#23548;&#26469;&#30830;&#20445;&#26377;&#25928;&#30340;&#23548;&#33322;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;GP-MPPI&#65292;&#19968;&#31181;&#22522;&#20110;&#22312;&#32447;&#23398;&#20064;&#30340;&#25511;&#21046;&#31574;&#30053;&#65292;&#23427;&#23558;MPPI&#19982;&#22522;&#20110;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#65288;SGP&#65289;&#30340;&#23616;&#37096;&#24863;&#30693;&#27169;&#22411;&#30456;&#32467;&#21512;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#21033;&#29992;SGP&#30340;&#23398;&#20064;&#33021;&#21147;&#26500;&#24314;&#19968;&#20010;&#26041;&#24046;&#65288;&#19981;&#30830;&#23450;&#24615;&#65289;&#34920;&#38754;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#20102;&#35299;&#21608;&#22260;&#30340;&#21487;&#23548;&#33322;&#31354;&#38388;&#65292;&#35782;&#21035;&#19968;&#32452;&#24314;&#35758;&#30340;&#23376;&#30446;&#26631;&#65292;&#24182;&#26368;&#32456;&#25512;&#33616;&#26368;&#23567;&#21270;&#39044;&#23450;&#20041;&#25104;&#26412;&#20989;&#25968;&#30340;&#26368;&#20248;&#23376;&#30446;&#26631;&#32473;&#23616;&#37096;&#30340;MPPI&#35268;&#21010;&#22120;&#12290;&#20043;&#21518;&#65292;MPPI&#35745;&#31639;&#20986;&#28385;&#36275;&#35201;&#27714;&#30340;&#26368;&#20248;&#25511;&#21046;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robotic navigation in unknown, cluttered environments with limited sensing capabilities poses significant challenges in robotics. Local trajectory optimization methods, such as Model Predictive Path Intergal (MPPI), are a promising solution to this challenge. However, global guidance is required to ensure effective navigation, especially when encountering challenging environmental conditions or navigating beyond the planning horizon. This study presents the GP-MPPI, an online learning-based control strategy that integrates MPPI with a local perception model based on Sparse Gaussian Process (SGP). The key idea is to leverage the learning capability of SGP to construct a variance (uncertainty) surface, which enables the robot to learn about the navigable space surrounding it, identify a set of suggested subgoals, and ultimately recommend the optimal subgoal that minimizes a predefined cost function to the local MPPI planner. Afterward, MPPI computes the optimal control sequence that sati
&lt;/p&gt;</description></item><item><title>&#31532;&#21313;&#20061;&#23626;&#29702;&#24615;&#21644;&#30693;&#35782;&#29702;&#35770;&#26041;&#38754;&#30340;&#20250;&#35758;&#26088;&#22312;&#27719;&#38598;&#26469;&#33258;&#22810;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#65292;&#36827;&#19968;&#27493;&#30740;&#31350;&#28041;&#21450;&#29702;&#24615;&#21644;&#30693;&#35782;&#30340;&#36328;&#23398;&#31185;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.04005</link><description>&lt;p&gt;
&#31532;&#21313;&#20061;&#23626;&#29702;&#24615;&#21644;&#30693;&#35782;&#29702;&#35770;&#26041;&#38754;&#30340;&#20250;&#35758;&#35770;&#25991;&#38598;
&lt;/p&gt;
&lt;p&gt;
Proceedings Ninetheenth conference on Theoretical Aspects of Rationality and Knowledge. (arXiv:2307.04005v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04005
&lt;/p&gt;
&lt;p&gt;
&#31532;&#21313;&#20061;&#23626;&#29702;&#24615;&#21644;&#30693;&#35782;&#29702;&#35770;&#26041;&#38754;&#30340;&#20250;&#35758;&#26088;&#22312;&#27719;&#38598;&#26469;&#33258;&#22810;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#65292;&#36827;&#19968;&#27493;&#30740;&#31350;&#28041;&#21450;&#29702;&#24615;&#21644;&#30693;&#35782;&#30340;&#36328;&#23398;&#31185;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
TARK&#20250;&#35758;(&#29702;&#24615;&#21644;&#30693;&#35782;&#29702;&#35770;&#26041;&#38754;&#30340;&#20250;&#35758;)&#26088;&#22312;&#27719;&#38598;&#26469;&#33258;&#35745;&#31639;&#26426;&#31185;&#23398;&#12289;&#20154;&#24037;&#26234;&#33021;&#12289;&#21338;&#24328;&#35770;&#12289;&#20915;&#31574;&#35770;&#12289;&#21746;&#23398;&#12289;&#36923;&#36753;&#12289;&#35821;&#35328;&#23398;&#21644;&#35748;&#30693;&#31185;&#23398;&#31561;&#21508;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#65292;&#26088;&#22312;&#36827;&#19968;&#27493;&#29702;&#35299;&#28041;&#21450;&#29702;&#24615;&#21644;&#30693;&#35782;&#30340;&#36328;&#23398;&#31185;&#38382;&#39064;&#12290;&#33258;1986&#24180;&#20197;&#26469;&#65292;&#35813;&#20250;&#35758;&#20197;&#20004;&#24180;&#20026;&#26399;&#38388;&#65292;&#22312;&#19990;&#30028;&#21508;&#22320;&#20030;&#34892;&#65292;&#20854;&#21019;&#22987;&#20154;&#26159;&#32422;&#29791;&#22827;&#183;&#21704;&#23572;&#26222;&#24681; (&#24247;&#22856;&#23572;&#22823;&#23398;)&#12290;&#24863;&#20852;&#36259;&#30340;&#20027;&#39064;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#30693;&#35782;&#12289;&#20449;&#24565;&#12289;&#24847;&#35782;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#35821;&#20041;&#27169;&#22411;&#65292;&#26377;&#38480;&#29702;&#24615;&#21644;&#36164;&#28304;&#21463;&#38480;&#25512;&#29702;&#65292;&#24120;&#35782;&#30693;&#35782;&#25512;&#29702;&#65292;&#35748;&#30693;&#36923;&#36753;&#65292;&#35748;&#30693;&#21338;&#24328;&#35770;&#65292;&#30693;&#35782;&#19982;&#34892;&#21160;&#65292;&#22312;&#30693;&#35782;&#21644;&#20854;&#20182;&#24515;&#29702;&#29366;&#24577;&#30340;&#25512;&#29702;&#24212;&#29992;&#65292;&#20449;&#24565;&#20462;&#27491;&#65292;&#35745;&#31639;&#31038;&#20250;&#36873;&#25321;&#65292;&#31639;&#27861;&#21338;&#24328;&#35770;&#20197;&#21450;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#22522;&#30784;&#12290;Informa
&lt;/p&gt;
&lt;p&gt;
The TARK conference (Theoretical Aspects of Rationality and Knowledge) is a conference that aims to bring together researchers from a wide variety of fields, including computer science, artificial intelligence, game theory, decision theory, philosophy, logic, linguistics, and cognitive science. Its goal is to further our understanding of interdisciplinary issues involving reasoning about rationality and knowledge.  Previous conferences have been held biennially around the world since 1986, on the initiative of Joe Halpern (Cornell University). Topics of interest include, but are not limited to, semantic models for knowledge, belief, awareness and uncertainty, bounded rationality and resource-bounded reasoning, commonsense epistemic reasoning, epistemic logic, epistemic game theory, knowledge and action, applications of reasoning about knowledge and other mental states, belief revision, computational social choice, algorithmic game theory, and foundations of multi-agent systems. Informa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;Sarsa&#31639;&#27861;&#35299;&#20915;&#22320;&#19979;&#20572;&#36710;&#22330;&#30340;&#38745;&#24577;&#22330;&#26223;&#29983;&#25104;&#38382;&#39064;&#65292;&#20026;&#33258;&#21160;&#39550;&#39542;&#27169;&#22411;&#35757;&#32451;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.03988</link><description>&lt;p&gt;
&#22522;&#20110;PCG&#30340;&#38745;&#24577;&#22320;&#19979;&#20572;&#36710;&#22330;&#22330;&#26223;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
PCG-based Static Underground Garage Scenario Generation. (arXiv:2307.03988v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03988
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;Sarsa&#31639;&#27861;&#35299;&#20915;&#22320;&#19979;&#20572;&#36710;&#22330;&#30340;&#38745;&#24577;&#22330;&#26223;&#29983;&#25104;&#38382;&#39064;&#65292;&#20026;&#33258;&#21160;&#39550;&#39542;&#27169;&#22411;&#35757;&#32451;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#25216;&#26415;&#26377;&#20116;&#20010;&#31561;&#32423;&#65292;&#20174;L0&#21040;L5&#12290;&#30446;&#21069;&#65292;&#21482;&#33021;&#23454;&#29616;L2&#32423;&#21035;&#65288;&#37096;&#20998;&#33258;&#21160;&#21270;&#65289;&#65292;&#36317;&#31163;&#36798;&#21040;&#26368;&#32456;&#30340;L5&#32423;&#21035;&#65288;&#23436;&#20840;&#33258;&#21160;&#21270;&#65289;&#36824;&#26377;&#24456;&#38271;&#30340;&#36335;&#35201;&#36208;&#12290;&#36328;&#36234;&#36825;&#20123;&#32423;&#21035;&#30340;&#20851;&#38190;&#22312;&#20110;&#35757;&#32451;&#33258;&#21160;&#39550;&#39542;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20165;&#20381;&#36182;&#30495;&#23454;&#19990;&#30028;&#30340;&#36947;&#36335;&#25968;&#25454;&#26469;&#35757;&#32451;&#27169;&#22411;&#36828;&#36828;&#19981;&#22815;&#65292;&#32780;&#19988;&#28040;&#32791;&#22823;&#37327;&#36164;&#28304;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#36890;&#36807;&#27169;&#25311;&#22120;&#35757;&#32451;&#33258;&#21160;&#39550;&#39542;&#27169;&#22411;&#30340;&#20363;&#23376;&#65292;&#27169;&#25311;&#30340;&#26159;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#65292;&#20294;&#36825;&#20123;&#22330;&#26223;&#38656;&#35201;&#23436;&#20840;&#25163;&#21160;&#26500;&#24314;&#12290;&#30452;&#25509;&#20174;&#36947;&#36335;&#32593;&#32476;&#26684;&#24335;&#36716;&#25442;3D&#22330;&#26223;&#20250;&#32570;&#20047;&#22823;&#37327;&#32454;&#33410;&#65292;&#26080;&#27861;&#29992;&#20316;&#35757;&#32451;&#38598;&#12290;&#22320;&#19979;&#20572;&#36710;&#22330;&#38745;&#24577;&#22330;&#26223;&#27169;&#25311;&#34987;&#35270;&#20026;&#19968;&#31181;&#31243;&#24207;&#20869;&#23481;&#29983;&#25104;&#65288;PCG&#65289;&#38382;&#39064;&#12290;&#26412;&#25991;&#23558;&#20351;&#29992;Sarsa&#31639;&#27861;&#35299;&#20915;&#22320;&#19979;&#20572;&#36710;&#22330;&#32467;&#26500;&#30340;&#31243;&#24207;&#20869;&#23481;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous driving technology has five levels, from L0 to L5. Currently, only the L2 level (partial automation) can be achieved, and there is a long way to go before reaching the final level of L5 (full automation). The key to crossing these levels lies in training the autonomous driving model. However, relying solely on real-world road data to train the model is far from enough and consumes a great deal of resources. Although there are already examples of training autonomous driving models through simulators that simulate real-world scenarios, these scenarios require complete manual construction. Directly converting 3D scenes from road network formats will lack a large amount of detail and cannot be used as training sets. Underground parking garage static scenario simulation is regarded as a procedural content generation (PCG) problem. This paper will use the Sarsa algorithm to solve procedural content generation on underground garage structures.
&lt;/p&gt;</description></item><item><title>&#33258;&#20027;2.0&#23545;&#20110;&#33258;&#20027;&#20135;&#19994;&#23454;&#29616;&#35268;&#27169;&#32463;&#27982;&#33267;&#20851;&#37325;&#35201;&#65292;&#24403;&#21069;&#30340;&#33258;&#20027;1.0&#21457;&#23637;&#33539;&#24335;&#38480;&#21046;&#20102;&#20854;&#20805;&#20998;&#20174;&#35745;&#31639;&#25104;&#26412;&#21644;&#21487;&#29992;&#25968;&#25454;&#30340;&#35268;&#27169;&#32463;&#27982;&#20013;&#21463;&#30410;&#12290;&#20026;&#20102;&#23454;&#29616;&#33258;&#20027;2.0&#65292;&#38656;&#35201;&#35299;&#20915;&#25216;&#26415;&#21644;&#32463;&#27982;&#23618;&#38754;&#19978;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.03973</link><description>&lt;p&gt;
&#33258;&#20027;2.0&#65306;&#35268;&#27169;&#32463;&#27982;&#30340;&#36861;&#27714;
&lt;/p&gt;
&lt;p&gt;
Autonomy 2.0: The Quest for Economies of Scale. (arXiv:2307.03973v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03973
&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;2.0&#23545;&#20110;&#33258;&#20027;&#20135;&#19994;&#23454;&#29616;&#35268;&#27169;&#32463;&#27982;&#33267;&#20851;&#37325;&#35201;&#65292;&#24403;&#21069;&#30340;&#33258;&#20027;1.0&#21457;&#23637;&#33539;&#24335;&#38480;&#21046;&#20102;&#20854;&#20805;&#20998;&#20174;&#35745;&#31639;&#25104;&#26412;&#21644;&#21487;&#29992;&#25968;&#25454;&#30340;&#35268;&#27169;&#32463;&#27982;&#20013;&#21463;&#30410;&#12290;&#20026;&#20102;&#23454;&#29616;&#33258;&#20027;2.0&#65292;&#38656;&#35201;&#35299;&#20915;&#25216;&#26415;&#21644;&#32463;&#27982;&#23618;&#38754;&#19978;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36807;&#21435;&#21313;&#24180;&#20013;&#26426;&#22120;&#20154;&#25216;&#26415;&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#25105;&#20204;&#29616;&#22312;&#24050;&#32463;&#36827;&#20837;&#20102;&#33258;&#20027;&#26426;&#22120;&#26102;&#20195;&#12290;&#22312;&#36825;&#20010;&#26032;&#30340;&#20449;&#24687;&#25216;&#26415;&#26102;&#20195;&#65292;&#33258;&#20027;&#26426;&#22120;&#65292;&#22914;&#26381;&#21153;&#26426;&#22120;&#20154;&#12289;&#33258;&#20027;&#26080;&#20154;&#26426;&#12289;&#36865;&#36135;&#26426;&#22120;&#20154;&#21644;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65292;&#23558;&#25552;&#20379;&#26381;&#21153;&#65292;&#32780;&#19981;&#26159;&#20154;&#31867;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#36890;&#36807;&#26816;&#39564;&#25968;&#23383;&#32463;&#27982;&#30340;&#25216;&#26415;&#25361;&#25112;&#21644;&#32463;&#27982;&#24433;&#21709;&#65292;&#25105;&#20204;&#35748;&#20026;&#21487;&#25193;&#23637;&#24615;&#22312;&#25216;&#26415;&#23618;&#38754;&#19978;&#26159;&#38750;&#24120;&#24517;&#35201;&#30340;&#65292;&#24182;&#19988;&#22312;&#32463;&#27982;&#23618;&#38754;&#19978;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#65292;&#22240;&#27492;&#26159;&#23454;&#29616;&#33258;&#20027;&#20135;&#19994;&#20805;&#20998;&#28508;&#21147;&#30340;&#20851;&#38190;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#24403;&#21069;&#30340;&#21457;&#23637;&#33539;&#24335;&#65292;&#34987;&#31216;&#20026;&#33258;&#20027;1.0&#65292;&#26159;&#19982;&#24037;&#31243;&#24072;&#25968;&#37327;&#25104;&#27604;&#20363;&#32780;&#19981;&#26159;&#19982;&#25968;&#25454;&#37327;&#25110;&#35745;&#31639;&#36164;&#28304;&#25104;&#27604;&#20363;&#21457;&#23637;&#30340;&#65292;&#22240;&#27492;&#38459;&#27490;&#20102;&#33258;&#20027;&#20135;&#19994;&#20805;&#20998;&#20174;&#35268;&#27169;&#32463;&#27982;&#20013;&#21463;&#30410;&#65292;&#23588;&#20854;&#26159;&#35745;&#31639;&#25104;&#26412;&#30340;&#25351;&#25968;&#32423;&#38477;&#20302;&#21644;&#21487;&#29992;&#25968;&#25454;&#29190;&#28856;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#23454;&#29616;&#33258;&#20027;2.0&#25152;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#21644;&#21487;&#33021;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advancement of robotics and AI technologies in the past decade, we have now entered the age of autonomous machines. In this new age of information technology, autonomous machines, such as service robots, autonomous drones, delivery robots, and autonomous vehicles, rather than humans, will provide services. In this article, through examining the technical challenges and economic impact of the digital economy, we argue that scalability is both highly necessary from a technical perspective and significantly advantageous from an economic perspective, thus is the key for the autonomy industry to achieve its full potential. Nonetheless, the current development paradigm, dubbed Autonomy 1.0, scales with the number of engineers, instead of with the amount of data or compute resources, hence preventing the autonomy industry to fully benefit from the economies of scale, especially the exponentially cheapening compute cost and the explosion of available data. We further analyze the key s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#25143;&#25552;&#20379;&#27880;&#37322;&#30340;&#32534;&#31243;&#31034;&#20363;&#31995;&#32479;&#20013;&#30340;&#22810;&#24847;&#22270;&#26816;&#27979;&#38382;&#39064;&#65292;&#36890;&#36807;&#32534;&#31243;&#31034;&#20363;&#25216;&#26415;&#33258;&#21160;&#25512;&#29702;&#20986;&#36716;&#25442;&#31243;&#24207;&#65292;&#20197;&#35299;&#20915;&#20225;&#19994;&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#25968;&#25454;&#26144;&#23556;&#21644;&#36716;&#25442;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.03966</link><description>&lt;p&gt;
&#29992;&#25143;&#25552;&#20379;&#27880;&#37322;&#30340;&#32534;&#31243;&#31034;&#20363;&#31995;&#32479;&#20013;&#30340;&#22810;&#24847;&#22270;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-Intent Detection in User Provided Annotations for Programming by Examples Systems. (arXiv:2307.03966v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03966
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#25143;&#25552;&#20379;&#27880;&#37322;&#30340;&#32534;&#31243;&#31034;&#20363;&#31995;&#32479;&#20013;&#30340;&#22810;&#24847;&#22270;&#26816;&#27979;&#38382;&#39064;&#65292;&#36890;&#36807;&#32534;&#31243;&#31034;&#20363;&#25216;&#26415;&#33258;&#21160;&#25512;&#29702;&#20986;&#36716;&#25442;&#31243;&#24207;&#65292;&#20197;&#35299;&#20915;&#20225;&#19994;&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#25968;&#25454;&#26144;&#23556;&#21644;&#36716;&#25442;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26144;&#23556;&#20225;&#19994;&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;&#25968;&#25454;&#26144;&#23556;&#20173;&#28982;&#26159;&#38598;&#25104;&#24320;&#21457;&#30340;&#22522;&#26412;&#37096;&#20998;&#65292;&#20294;&#20854;&#32791;&#26102;&#36739;&#38271;&#12290;&#36234;&#26469;&#36234;&#22810;&#30340;&#24212;&#29992;&#31243;&#24207;&#32570;&#20047;&#21629;&#21517;&#26631;&#20934;&#65292;&#23884;&#22871;&#23383;&#27573;&#32467;&#26500;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#38598;&#25104;&#24320;&#21457;&#32773;&#30340;&#22797;&#26434;&#24615;&#12290;&#19968;&#26086;&#26144;&#23556;&#23436;&#25104;&#65292;&#25968;&#25454;&#36716;&#25442;&#26159;&#29992;&#25143;&#38754;&#20020;&#30340;&#19979;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#27599;&#20010;&#24212;&#29992;&#31243;&#24207;&#37117;&#24076;&#26395;&#25968;&#25454;&#20197;&#19968;&#23450;&#30340;&#26684;&#24335;&#23384;&#22312;&#12290;&#27492;&#22806;&#65292;&#22312;&#26500;&#24314;&#38598;&#25104;&#27969;&#31243;&#26102;&#65292;&#24320;&#21457;&#20154;&#21592;&#38656;&#35201;&#20102;&#35299;&#28304;&#25968;&#25454;&#23383;&#27573;&#21644;&#30446;&#26631;&#25968;&#25454;&#23383;&#27573;&#30340;&#26684;&#24335;&#65292;&#24182;&#35774;&#35745;&#20986;&#21487;&#20197;&#23558;&#25968;&#25454;&#20174;&#28304;&#26684;&#24335;&#36716;&#25442;&#20026;&#30446;&#26631;&#26684;&#24335;&#30340;&#36716;&#25442;&#31243;&#24207;&#12290;&#33258;&#20154;&#24037;&#26234;&#33021; (AI) &#30340;&#26089;&#26399;&#20197;&#26469;&#65292;&#36890;&#36807;&#31243;&#24207;&#32508;&#21512;&#33539;&#24335;&#33258;&#21160;&#29983;&#25104;&#36716;&#25442;&#31243;&#24207;&#30340;&#38382;&#39064;&#19968;&#30452;&#21463;&#21040;&#30740;&#31350;&#12290;&#32534;&#31243;&#31034;&#20363; (PBE) &#23601;&#26159;&#36825;&#26679;&#19968;&#31181;&#25216;&#26415;&#65292;&#30446;&#26631;&#26159;&#33258;&#21160;&#25512;&#29702;&#20986;&#19968;&#20010;&#35745;&#31639;&#26426;&#31243;&#24207;&#65292;&#20197;&#23436;&#25104;&#29992;&#25143;&#25552;&#20379;&#30340;&#26684;&#24335;&#25110;&#23383;&#31526;&#20018;&#36716;&#25442;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
In mapping enterprise applications, data mapping remains a fundamental part of integration development, but its time consuming. An increasing number of applications lack naming standards, and nested field structures further add complexity for the integration developers. Once the mapping is done, data transformation is the next challenge for the users since each application expects data to be in a certain format. Also, while building integration flow, developers need to understand the format of the source and target data field and come up with transformation program that can change data from source to target format. The problem of automatic generation of a transformation program through program synthesis paradigm from some specifications has been studied since the early days of Artificial Intelligence (AI). Programming by Example (PBE) is one such kind of technique that targets automatic inferencing of a computer program to accomplish a format or string conversion task from user-provide
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#34987;&#36951;&#24536;&#26435;&#65288;RTBF&#65289;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#25552;&#20379;&#20102;&#23454;&#26045;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.03941</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#34987;&#36951;&#24536;&#26435;&#65306;&#28085;&#20041;&#12289;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Right to be Forgotten in the Era of Large Language Models: Implications, Challenges, and Solutions. (arXiv:2307.03941v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#34987;&#36951;&#24536;&#26435;&#65288;RTBF&#65289;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#25552;&#20379;&#20102;&#23454;&#26045;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34987;&#36951;&#24536;&#26435;&#65288;RTBF&#65289;&#26368;&#21021;&#26159;&#30001;&#35895;&#27468;&#35199;&#29677;&#29273;&#19982;&#22467;&#20811;&#26031;&#20869;&#22612;&#32034;&#22996;&#21592;&#20250;(Mario Costeja Gonz\'alez)&#20043;&#38388;&#30340;&#23448;&#21496;&#32467;&#26524;&#32780;&#30830;&#31435;&#30340;&#65292;&#24182;&#19988;&#21518;&#26469;&#34987;&#20316;&#20026;&#27431;&#27954;&#32852;&#30431;&#19968;&#33324;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#65288;GDPR&#65289;&#19979;&#30340;&#21024;&#38500;&#26435;&#12290;RTBF&#20801;&#35768;&#20010;&#20154;&#21521;&#32452;&#32455;&#35831;&#27714;&#21024;&#38500;&#20010;&#20154;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#25628;&#32034;&#24341;&#25806;&#65292;&#20010;&#20154;&#21487;&#20197;&#21521;&#32452;&#32455;&#21457;&#36865;&#35831;&#27714;&#65292;&#25490;&#38500;&#20182;&#20204;&#30340;&#20449;&#24687;&#22312;&#26597;&#35810;&#32467;&#26524;&#20013;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#21644;&#20854;&#22312;&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#30340;&#24212;&#29992;&#65292;LLM&#21551;&#29992;&#30340;&#36719;&#20214;&#31995;&#32479;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#20294;&#23427;&#20204;&#24182;&#27809;&#26377;&#34987;&#25490;&#38500;&#22312;RTBF&#20043;&#22806;&#12290;&#30456;&#27604;&#25628;&#32034;&#24341;&#25806;&#20351;&#29992;&#30340;&#32034;&#24341;&#26041;&#27861;&#65292;LLMs&#20197;&#19968;&#31181;&#23436;&#20840;&#19981;&#21516;&#30340;&#26041;&#24335;&#23384;&#20648;&#21644;&#22788;&#29702;&#20449;&#24687;&#65292;&#36825;&#20026;&#31526;&#21512;RTBF&#25552;&#20986;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#22914;&#20309;&#23454;&#26045;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#20197;&#31526;&#21512;RTBF&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Right to be Forgotten (RTBF) was first established as the result of the ruling of Google Spain SL, Google Inc. v AEPD, Mario Costeja Gonz\'alez, and was later included as the Right to Erasure under the General Data Protection Regulation (GDPR) of European Union to allow individuals the right to request personal data be deleted by organizations. Specifically for search engines, individuals can send requests to organizations to exclude their information from the query results. With the recent development of Large Language Models (LLMs) and their use in chatbots, LLM-enabled software systems have become popular. But they are not excluded from the RTBF. Compared with the indexing approach used by search engines, LLMs store, and process information in a completely different way. This poses new challenges for compliance with the RTBF. In this paper, we explore these challenges and provide our insights on how to implement technical solutions for the RTBF, including the use of machine unle
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#27169;&#24335;&#22797;&#26434;&#30340;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#30340;&#24402;&#32435;&#20803;&#36335;&#24452;&#23398;&#20064;&#26694;&#26550;SchemaWalk&#12290;</title><link>http://arxiv.org/abs/2307.03937</link><description>&lt;p&gt;
&#38024;&#23545;&#27169;&#24335;&#22797;&#26434;&#30340;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#30340;&#24402;&#32435;&#20803;&#36335;&#24452;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Inductive Meta-path Learning for Schema-complex Heterogeneous Information Networks. (arXiv:2307.03937v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03937
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#27169;&#24335;&#22797;&#26434;&#30340;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#30340;&#24402;&#32435;&#20803;&#36335;&#24452;&#23398;&#20064;&#26694;&#26550;SchemaWalk&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;(HINs)&#26159;&#20855;&#26377;&#22810;&#31181;&#33410;&#28857;&#31867;&#22411;&#21644;&#36793;&#31867;&#22411;&#30340;&#20449;&#24687;&#32593;&#32476;&#12290;&#20803;&#36335;&#24452;&#30340;&#27010;&#24565;&#21363;&#19968;&#31995;&#21015;&#36830;&#25509;&#20004;&#20010;&#23454;&#20307;&#30340;&#23454;&#20307;&#31867;&#22411;&#21644;&#20851;&#31995;&#31867;&#22411;&#30340;&#24207;&#21015;&#34987;&#25552;&#20986;&#20026;&#25552;&#20379;&#23545;&#19981;&#21516;HIN&#20219;&#21153;&#30340;&#20803;&#32423;&#21487;&#35299;&#37322;&#35821;&#20041;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#20256;&#32479;&#19978;&#65292;&#20803;&#36335;&#24452;&#20027;&#35201;&#29992;&#20110;&#27169;&#24335;&#31616;&#21333;&#30340;HINs&#65292;&#20363;&#22914;&#21482;&#26377;&#23569;&#37327;&#23454;&#20307;&#31867;&#22411;&#30340;&#25991;&#29486;&#32593;&#32476;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20803;&#36335;&#24452;&#36890;&#24120;&#36890;&#36807;&#39046;&#22495;&#30693;&#35782;&#26522;&#20030;&#12290;&#28982;&#32780;&#65292;&#20803;&#36335;&#24452;&#22312;&#27169;&#24335;&#22797;&#26434;&#30340;HINs(&#20363;&#22914;&#20855;&#26377;&#25968;&#30334;&#31181;&#23454;&#20307;&#21644;&#20851;&#31995;&#31867;&#22411;&#30340;&#30693;&#35782;&#24211;)&#20013;&#30340;&#24212;&#29992;&#21463;&#21040;&#20102;&#30001;&#20803;&#36335;&#24452;&#26522;&#20030;&#24341;&#36215;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#26377;&#25928;&#35780;&#20272;&#20803;&#36335;&#24452;&#38656;&#35201;&#26522;&#20030;&#30456;&#20851;&#36335;&#24452;&#23454;&#20363;&#65292;&#36825;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#20803;&#36335;&#24452;&#23398;&#20064;&#36807;&#31243;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27169;&#24335;&#22797;&#26434;&#30340;HINs&#30340;&#24402;&#32435;&#20803;&#36335;&#24452;&#23398;&#20064;&#26694;&#26550;SchemaWalk&#12290;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Information Networks (HINs) are information networks with multiple types of nodes and edges. The concept of meta-path, i.e., a sequence of entity types and relation types connecting two entities, is proposed to provide the meta-level explainable semantics for various HIN tasks. Traditionally, meta-paths are primarily used for schema-simple HINs, e.g., bibliographic networks with only a few entity types, where meta-paths are often enumerated with domain knowledge. However, the adoption of meta-paths for schema-complex HINs, such as knowledge bases (KBs) with hundreds of entity and relation types, has been limited due to the computational complexity associated with meta-path enumeration. Additionally, effectively assessing meta-paths requires enumerating relevant path instances, which adds further complexity to the meta-path learning process. To address these challenges, we propose SchemaWalk, an inductive meta-path learning framework for schema-complex HINs. We represent m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#38754;&#21521;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#25928;&#20869;&#23384;&#35745;&#31639;&#30828;&#20214;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#24320;&#25918;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20110;IMC&#30340;QNN&#30828;&#20214;&#36335;&#32447;&#22270;&#12290;</title><link>http://arxiv.org/abs/2307.03936</link><description>&lt;p&gt;
&#38754;&#21521;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#25928;&#20869;&#23384;&#35745;&#31639;&#30828;&#20214;&#65306;&#26368;&#26032;&#36827;&#23637;&#12289;&#24320;&#25918;&#25361;&#25112;&#21644;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Towards Efficient In-memory Computing Hardware for Quantized Neural Networks: State-of-the-art, Open Challenges and Perspectives. (arXiv:2307.03936v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03936
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#38754;&#21521;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#25928;&#20869;&#23384;&#35745;&#31639;&#30828;&#20214;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#24320;&#25918;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20110;IMC&#30340;QNN&#30828;&#20214;&#36335;&#32447;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20113;&#35745;&#31639;&#20013;&#22788;&#29702;&#30340;&#25968;&#25454;&#37327;&#12289;&#29289;&#32852;&#32593;&#24212;&#29992;&#30340;&#21457;&#23637;&#20197;&#21450;&#26085;&#30410;&#22686;&#38271;&#30340;&#25968;&#25454;&#38544;&#31169;&#20851;&#27880;&#65292;&#36843;&#20351;&#20174;&#22522;&#20110;&#20113;&#30340;&#22788;&#29702;&#36716;&#21521;&#22522;&#20110;&#36793;&#32536;&#30340;&#22788;&#29702;&#12290;&#36793;&#32536;&#19978;&#26377;&#38480;&#30340;&#33021;&#37327;&#21644;&#35745;&#31639;&#36164;&#28304;&#25512;&#21160;&#20102;&#20174;&#20256;&#32479;&#20911;&#183;&#35834;&#20381;&#26364;&#26550;&#26500;&#21521;&#20869;&#23384;&#35745;&#31639;&#65288;IMC&#65289;&#30340;&#36807;&#28193;&#65292;&#29305;&#21035;&#26159;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#26041;&#38754;&#12290;&#32593;&#32476;&#21387;&#32553;&#25216;&#26415;&#34987;&#24212;&#29992;&#20110;&#22312;&#26377;&#38480;&#30340;&#30828;&#20214;&#36164;&#28304;&#19978;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#12290;&#37327;&#21270;&#26159;&#26368;&#39640;&#25928;&#30340;&#32593;&#32476;&#21387;&#32553;&#25216;&#26415;&#20043;&#19968;&#65292;&#21487;&#20197;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#12289;&#24310;&#36831;&#21644;&#33021;&#37327;&#28040;&#32791;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;IMC&#30340;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#65288;QNN&#65289;&#24182;&#23558;&#22522;&#20110;&#36719;&#20214;&#30340;&#37327;&#21270;&#26041;&#27861;&#19982;IMC&#30828;&#20214;&#23454;&#29616;&#30456;&#32467;&#21512;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#24320;&#25918;&#25361;&#25112;&#65292;QNN&#35774;&#35745;&#35201;&#27714;&#65292;&#24314;&#35758;&#21644;&#23637;&#26395;&#20197;&#21450;&#22522;&#20110;IMC&#30340;QNN&#30828;&#20214;&#36335;&#32447;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The amount of data processed in the cloud, the development of Internet-of-Things (IoT) applications, and growing data privacy concerns force the transition from cloud-based to edge-based processing. Limited energy and computational resources on edge push the transition from traditional von Neumann architectures to In-memory Computing (IMC), especially for machine learning and neural network applications. Network compression techniques are applied to implement a neural network on limited hardware resources. Quantization is one of the most efficient network compression techniques allowing to reduce the memory footprint, latency, and energy consumption. This paper provides a comprehensive review of IMC-based Quantized Neural Networks (QNN) and links software-based quantization approaches to IMC hardware implementation. Moreover, open challenges, QNN design requirements, recommendations, and perspectives along with an IMC-based QNN hardware roadmap are provided.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;&#21644;&#37325;&#26500;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#19968;&#33324;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#30340;&#21487;&#35745;&#31639;&#30340;&#37325;&#26500;&#40065;&#26834;&#24615;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2307.03928</link><description>&lt;p&gt;
&#29992;&#20551;&#35774;&#26816;&#39564;&#35299;&#37322;&#24046;&#20998;&#38544;&#31169;&#26469;&#38480;&#21046;&#25968;&#25454;&#37325;&#26500;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Bounding data reconstruction attacks with the hypothesis testing interpretation of differential privacy. (arXiv:2307.03928v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;&#21644;&#37325;&#26500;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#19968;&#33324;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#30340;&#21487;&#35745;&#31639;&#30340;&#37325;&#26500;&#40065;&#26834;&#24615;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#32034;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#37325;&#26500;&#40065;&#26834;&#24615;&#65288;ReRo&#65289;&#65292;&#23427;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25968;&#25454;&#37325;&#26500;&#25915;&#20987;&#25104;&#21151;&#29575;&#30340;&#19978;&#30028;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#26426;&#21046;&#20063;&#25552;&#20379;&#20102;ReRo&#65292;&#20294;&#36804;&#20170;&#21482;&#23637;&#31034;&#20102;&#28176;&#36817;&#33945;&#29305;&#21345;&#32599;&#20272;&#35745;&#30340;&#32039;&#23494;ReRo&#19978;&#30028;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#19968;&#33324;&#30340;DP&#26426;&#21046;&#65292;&#30452;&#25509;&#21487;&#35745;&#31639;&#30340;ReRo&#19978;&#30028;&#26159;&#21487;&#21462;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20551;&#35774;&#26816;&#39564;DP&#21644;ReRo&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#24182;&#25512;&#23548;&#20986;Laplace&#21644;Gaussian&#26426;&#21046;&#21450;&#20854;&#23376;&#37319;&#26679;&#21464;&#20307;&#30340;&#38381;&#24335;&#12289;&#35299;&#26512;&#25110;&#25968;&#20540;ReRo&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore Reconstruction Robustness (ReRo), which was recently proposed as an upper bound on the success of data reconstruction attacks against machine learning models. Previous research has demonstrated that differential privacy (DP) mechanisms also provide ReRo, but so far, only asymptotic Monte Carlo estimates of a tight ReRo bound have been shown. Directly computable ReRo bounds for general DP mechanisms are thus desirable. In this work, we establish a connection between hypothesis testing DP and ReRo and derive closed-form, analytic or numerical ReRo bounds for the Laplace and Gaussian mechanisms and their subsampled variants.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#23558;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#21327;&#20316;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#21457;&#23637;&#33539;&#24335;&#30340;&#26041;&#27861;&#65292;&#24378;&#35843;&#26377;&#25928;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#38656;&#35201;&#20805;&#20998;&#21033;&#29992;&#21452;&#26041;&#30340;&#29420;&#29305;&#33021;&#21147;&#65292;&#21516;&#26102;&#20811;&#26381;&#25361;&#25112;&#21644;&#38480;&#21046;&#65292;&#25552;&#39640;&#32852;&#21512;&#34920;&#29616;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#25351;&#20986;&#29616;&#26377;&#30740;&#31350;&#24448;&#24448;&#26410;&#32771;&#34385;&#21040;&#21160;&#24577;&#12289;&#36866;&#24212;&#24615;&#21644;&#21327;&#20316;&#22242;&#38431;&#29615;&#22659;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#21151;&#33021;&#65292;&#21628;&#21505;&#21152;&#24378;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#21327;&#20316;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.03913</link><description>&lt;p&gt;
&#22312;&#21457;&#23637;&#26377;&#25928;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#21327;&#20316;&#20013;&#24212;&#29992;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#20197;&#20154;&#24037;&#26234;&#33021;-&#20154;&#31867;&#20849;&#21516;&#35748;&#30693;&#31995;&#32479;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Applying human-centered AI in developing effective human-AI teaming: A perspective of human-AI joint cognitive systems. (arXiv:2307.03913v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#23558;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#21327;&#20316;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#21457;&#23637;&#33539;&#24335;&#30340;&#26041;&#27861;&#65292;&#24378;&#35843;&#26377;&#25928;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#38656;&#35201;&#20805;&#20998;&#21033;&#29992;&#21452;&#26041;&#30340;&#29420;&#29305;&#33021;&#21147;&#65292;&#21516;&#26102;&#20811;&#26381;&#25361;&#25112;&#21644;&#38480;&#21046;&#65292;&#25552;&#39640;&#32852;&#21512;&#34920;&#29616;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#25351;&#20986;&#29616;&#26377;&#30740;&#31350;&#24448;&#24448;&#26410;&#32771;&#34385;&#21040;&#21160;&#24577;&#12289;&#36866;&#24212;&#24615;&#21644;&#21327;&#20316;&#22242;&#38431;&#29615;&#22659;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#21151;&#33021;&#65292;&#21628;&#21505;&#21152;&#24378;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#21327;&#20316;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21644;&#24212;&#29992;&#24050;&#23558;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#21327;&#20316;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#26469;&#21457;&#23637;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#21327;&#20316;&#35748;&#35782;&#21040;&#20154;&#24037;&#26234;&#33021;&#23558;&#20316;&#20026;&#19968;&#21517;&#38431;&#21451;&#32780;&#19981;&#20165;&#20165;&#26159;&#24037;&#20855;&#19982;&#20154;&#31867;&#21327;&#20316;&#12290;&#26377;&#25928;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#38656;&#35201;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#29420;&#29305;&#33021;&#21147;&#65292;&#21516;&#26102;&#20811;&#26381;&#27599;&#20010;&#25104;&#21592;&#30340;&#24050;&#30693;&#25361;&#25112;&#21644;&#38480;&#21046;&#65292;&#22686;&#24378;&#20154;&#31867;&#33021;&#21147;&#65292;&#24182;&#23558;&#32852;&#21512;&#24615;&#33021;&#25552;&#39640;&#21040;&#20219;&#20309;&#23454;&#20307;&#20043;&#19978;&#12290;2023&#24180;&#20840;&#22269;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#21644;&#25112;&#30053;&#35745;&#21010;&#26356;&#26032;&#35748;&#35782;&#21040;&#65292;&#20027;&#35201;&#20851;&#27880;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#29420;&#31435;&#24615;&#33021;&#30340;&#30740;&#31350;&#35745;&#21010;&#24448;&#24448;&#26410;&#32771;&#34385;&#21040;&#20154;&#24037;&#26234;&#33021;&#22312;&#21160;&#24577;&#12289;&#36866;&#24212;&#24615;&#21644;&#21327;&#20316;&#22242;&#38431;&#29615;&#22659;&#20013;&#24517;&#39035;&#25552;&#20379;&#30340;&#21151;&#33021;&#65292;&#24182;&#21628;&#21505;&#36827;&#19968;&#27493;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#21327;&#20316;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#26159;&#21542;&#33021;&#20316;&#20026;&#20154;&#31867;&#30340;&#38431;&#21451;&#23384;&#22312;&#20105;&#35758;&#12290;&#20027;&#35201;&#30340;&#20851;&#27880;&#28857;&#22312;&#20110;&#37319;&#29992;"&#21327;&#20316;"&#33539;&#24335;&#26159;&#21542;&#19982;&#20154;&#31867;&#30340;&#35748;&#30693;&#36807;&#31243;&#30456;&#30683;&#30462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research and application have used human-AI teaming (HAT) as a new paradigm to develop AI systems. HAT recognizes that AI will function as a teammate instead of simply a tool in collaboration with humans. Effective human-AI teams need to be capable of taking advantage of the unique abilities of both humans and AI while overcoming the known challenges and limitations of each member, augmenting human capabilities, and raising joint performance beyond that of either entity. The National AI Research and Strategic Plan 2023 update has recognized that research programs focusing primarily on the independent performance of AI systems generally fail to consider the functionality that AI must provide within the context of dynamic, adaptive, and collaborative teams and calls for further research on human-AI teaming and collaboration. However, there has been debate about whether AI can work as a teammate with humans. The primary concern is that adopting the "teaming" paradigm contradicts the human
&lt;/p&gt;</description></item><item><title>ScriptWorld&#26159;&#19968;&#20010;&#22522;&#20110;&#25991;&#26412;&#30340;&#29615;&#22659;&#65292;&#29992;&#20110;&#25945;&#25480;&#26234;&#33021;&#20307;&#20851;&#20110;&#30495;&#23454;&#19990;&#30028;&#26085;&#24120;&#20219;&#21153;&#21644;&#24120;&#35782;&#30693;&#35782;&#12290;&#23427;&#26159;&#31532;&#19968;&#20010;&#37319;&#29992;&#33050;&#26412;&#25968;&#25454;&#38598;&#35774;&#35745;&#30340;&#20114;&#21160;&#24335;&#22522;&#20110;&#25991;&#26412;&#30340;&#28216;&#25103;&#26694;&#26550;&#65292;&#22312;10&#20010;&#26085;&#24120;&#27963;&#21160;&#20013;&#25552;&#20379;&#20102;&#28216;&#25103;&#29615;&#22659;&#65292;&#24182;&#36890;&#36807;&#23545;&#29615;&#22659;&#30340;&#35814;&#32454;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#24341;&#20837;&#33050;&#26412;&#24335;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#21487;&#20197;&#26377;&#25928;&#25552;&#20379;&#24120;&#35782;&#30693;&#35782;&#24182;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.03906</link><description>&lt;p&gt;
ScriptWorld: &#29992;&#20110;&#23398;&#20064;&#36807;&#31243;&#24615;&#30693;&#35782;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
ScriptWorld: Text Based Environment For Learning Procedural Knowledge. (arXiv:2307.03906v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03906
&lt;/p&gt;
&lt;p&gt;
ScriptWorld&#26159;&#19968;&#20010;&#22522;&#20110;&#25991;&#26412;&#30340;&#29615;&#22659;&#65292;&#29992;&#20110;&#25945;&#25480;&#26234;&#33021;&#20307;&#20851;&#20110;&#30495;&#23454;&#19990;&#30028;&#26085;&#24120;&#20219;&#21153;&#21644;&#24120;&#35782;&#30693;&#35782;&#12290;&#23427;&#26159;&#31532;&#19968;&#20010;&#37319;&#29992;&#33050;&#26412;&#25968;&#25454;&#38598;&#35774;&#35745;&#30340;&#20114;&#21160;&#24335;&#22522;&#20110;&#25991;&#26412;&#30340;&#28216;&#25103;&#26694;&#26550;&#65292;&#22312;10&#20010;&#26085;&#24120;&#27963;&#21160;&#20013;&#25552;&#20379;&#20102;&#28216;&#25103;&#29615;&#22659;&#65292;&#24182;&#36890;&#36807;&#23545;&#29615;&#22659;&#30340;&#35814;&#32454;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#24341;&#20837;&#33050;&#26412;&#24335;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#21487;&#20197;&#26377;&#25928;&#25552;&#20379;&#24120;&#35782;&#30693;&#35782;&#24182;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#28216;&#25103;&#20026;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26234;&#33021;&#20307;&#24320;&#21457;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#24120;&#35782;&#30693;&#35782;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;&#29616;&#26377;&#30340;&#25991;&#26412;&#29615;&#22659;&#36890;&#24120;&#20381;&#36182;&#20110;&#34394;&#26500;&#30340;&#24773;&#26223;&#21644;&#35282;&#33394;&#26469;&#21019;&#24314;&#28216;&#25103;&#26694;&#26550;&#65292;&#19982;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#30456;&#24046;&#29978;&#36828;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ScriptWorld&#65306;&#19968;&#20010;&#22522;&#20110;&#25991;&#26412;&#30340;&#29615;&#22659;&#65292;&#29992;&#20110;&#25945;&#25480;&#26234;&#33021;&#20307;&#20851;&#20110;&#30495;&#23454;&#19990;&#30028;&#26085;&#24120;&#20219;&#21153;&#21644;&#24120;&#35782;&#30693;&#35782;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#37319;&#29992;&#33050;&#26412;&#25968;&#25454;&#38598;&#35774;&#35745;&#30340;&#20114;&#21160;&#24335;&#22522;&#20110;&#25991;&#26412;&#30340;&#28216;&#25103;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;10&#20010;&#26085;&#24120;&#27963;&#21160;&#30340;&#28216;&#25103;&#29615;&#22659;&#65292;&#24182;&#23545;&#25152;&#25552;&#20986;&#30340;&#29615;&#22659;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#22522;&#20934;&#27169;&#22411;/&#26234;&#33021;&#20307;&#26469;&#29609;ScriptWorld&#20013;&#30340;&#28216;&#25103;&#12290;&#20026;&#20102;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#31181;&#29615;&#22659;&#20013;&#30340;&#20316;&#29992;&#65292;&#25105;&#20204;&#21033;&#29992;&#20174;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#33719;&#24471;&#30340;&#29305;&#24449;&#26469;&#36741;&#21161;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;ScriptWorld&#20013;&#24341;&#20837;&#33050;&#26412;&#24335;&#30340;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#21487;&#20197;&#26377;&#25928;&#25552;&#20379;&#24120;&#35782;&#30693;&#35782;&#21644;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-based games provide a framework for developing natural language understanding and commonsense knowledge about the world in reinforcement learning based agents. Existing text-based environments often rely on fictional situations and characters to create a gaming framework and are far from real-world scenarios. In this paper, we introduce ScriptWorld: a text-based environment for teaching agents about real-world daily chores and hence imparting commonsense knowledge. To the best of our knowledge, it is the first interactive text-based gaming framework that consists of daily real-world human activities designed using scripts dataset. We provide gaming environments for 10 daily activities and perform a detailed analysis of the proposed environment. We develop RL-based baseline models/agents to play the games in Scriptworld. To understand the role of language models in such environments, we leverage features obtained from pre-trained language models in the RL agents. Our experiments sh
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#22870;&#21169;&#37325;&#26032;&#21152;&#26435;&#12289;&#37325;&#36873;&#21644;&#37325;&#26032;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#21407;&#22411;&#38646;&#20214;&#32593;&#32476;&#30340;&#20998;&#31867;&#25928;&#26524;&#65292;&#35299;&#20915;&#20102;&#23398;&#20064;&#20174;&#22270;&#20687;&#30340;&#34394;&#20551;&#25110;&#19981;&#19968;&#33268;&#30340;&#37096;&#20998;&#36827;&#34892;&#20998;&#31867;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.03887</link><description>&lt;p&gt;
&#36890;&#36807;&#22870;&#21169;&#37325;&#26032;&#21152;&#26435;&#12289;&#37325;&#36873;&#21644;&#37325;&#26032;&#35757;&#32451;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#21407;&#22411;&#38646;&#20214;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Improving Prototypical Part Networks with Reward Reweighing, Reselection, and Retraining. (arXiv:2307.03887v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#22870;&#21169;&#37325;&#26032;&#21152;&#26435;&#12289;&#37325;&#36873;&#21644;&#37325;&#26032;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#21407;&#22411;&#38646;&#20214;&#32593;&#32476;&#30340;&#20998;&#31867;&#25928;&#26524;&#65292;&#35299;&#20915;&#20102;&#23398;&#20064;&#20174;&#22270;&#20687;&#30340;&#34394;&#20551;&#25110;&#19981;&#19968;&#33268;&#30340;&#37096;&#20998;&#36827;&#34892;&#20998;&#31867;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#33268;&#21147;&#20110;&#24320;&#21457;&#28145;&#24230;&#21487;&#35299;&#37322;&#30340;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#65292;&#33021;&#22815;&#28165;&#26970;&#22320;&#23558;&#27169;&#22411;&#30340;&#36755;&#20986;&#24402;&#22240;&#20110;&#25968;&#25454;&#30340;&#29305;&#23450;&#29305;&#24449;&#12290;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#26159;&#21407;&#22411;&#38646;&#20214;&#32593;&#32476;&#65288;ProtoPNet&#65289;&#65292;&#23427;&#22522;&#20110;&#36755;&#20837;&#30340;&#26377;&#24847;&#20041;&#37096;&#20998;&#26469;&#23581;&#35797;&#20998;&#31867;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#32463;&#24120;&#23398;&#20064;&#20174;&#22270;&#20687;&#30340;&#34394;&#20551;&#25110;&#19981;&#19968;&#33268;&#30340;&#37096;&#20998;&#36827;&#34892;&#20998;&#31867;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21463;&#21040;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#30340;&#26368;&#26032;&#21457;&#23637;&#21551;&#21457;&#65292;&#36890;&#36807;&#22312;CUB-200-2011&#25968;&#25454;&#38598;&#19978;&#25910;&#38598;&#20154;&#31867;&#21407;&#22411;&#36136;&#37327;&#30340;1-5&#20998;&#32423;&#27880;&#37322;&#65292;&#26500;&#24314;&#19968;&#20010;&#23398;&#20064;&#35782;&#21035;&#38750;&#34394;&#20551;&#21407;&#22411;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#37325;&#26032;&#21152;&#26435;&#12289;&#37325;&#36873;&#21644;&#37325;&#26032;&#35757;&#32451;&#30340;&#21407;&#22411;&#38646;&#20214;&#32593;&#32476;&#65288;R3-ProtoPNet&#65289;&#65292;&#35813;&#32593;&#32476;&#22312;ProtoPNet&#35757;&#32451;&#24490;&#29615;&#20013;&#22686;&#21152;&#20102;&#19977;&#20010;&#39069;&#22806;&#30340;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, work has gone into developing deep interpretable methods for image classification that clearly attributes a model's output to specific features of the data. One such of these methods is the prototypical part network (ProtoPNet), which attempts to classify images based on meaningful parts of the input. While this method results in interpretable classifications, this method often learns to classify from spurious or inconsistent parts of the image. Hoping to remedy this, we take inspiration from the recent developments in Reinforcement Learning with Human Feedback (RLHF) to fine-tune these prototypes. By collecting human annotations of prototypes quality via a 1-5 scale on the CUB-200-2011 dataset, we construct a reward model that learns to identify non-spurious prototypes. In place of a full RL update, we propose the reweighted, reselected, and retrained prototypical part network (R3-ProtoPNet), which adds an additional three steps to the ProtoPNet training loop. The fir
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#34503;&#30340;&#25925;&#20107;&#8221;&#30340;&#28151;&#21512;&#20513;&#35758;&#28216;&#25103;&#65292;&#36890;&#36807;&#29609;&#23478;&#36873;&#25321;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#26469;&#32534;&#20889;&#25925;&#20107;&#12290;&#23454;&#39564;&#32467;&#26524;&#21457;&#29616;&#65292;&#28216;&#25103;&#26426;&#21046;&#26174;&#33879;&#24433;&#21709;&#20102;&#25925;&#20107;&#36755;&#20986;&#12289;&#29609;&#23478;&#30340;&#21019;&#36896;&#36807;&#31243;&#21644;&#35282;&#33394;&#35748;&#30693;&#12290;</title><link>http://arxiv.org/abs/2307.03877</link><description>&lt;p&gt;
&#35774;&#35745;&#28151;&#21512;&#20513;&#35758;&#35270;&#39057;&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
Designing Mixed-Initiative Video Games. (arXiv:2307.03877v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#34503;&#30340;&#25925;&#20107;&#8221;&#30340;&#28151;&#21512;&#20513;&#35758;&#28216;&#25103;&#65292;&#36890;&#36807;&#29609;&#23478;&#36873;&#25321;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#26469;&#32534;&#20889;&#25925;&#20107;&#12290;&#23454;&#39564;&#32467;&#26524;&#21457;&#29616;&#65292;&#28216;&#25103;&#26426;&#21046;&#26174;&#33879;&#24433;&#21709;&#20102;&#25925;&#20107;&#36755;&#20986;&#12289;&#29609;&#23478;&#30340;&#21019;&#36896;&#36807;&#31243;&#21644;&#35282;&#33394;&#35748;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#21457;&#23637;&#20351;&#20154;&#31867;&#33021;&#22815;&#19982;&#26426;&#22120;&#20849;&#21516;&#21019;&#20316;&#20869;&#23481;&#12290;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#30340;&#24847;&#22806;&#24615;&#21487;&#20197;&#32473;&#29992;&#25143;&#24102;&#26469;&#28789;&#24863;&#21644;&#23089;&#20048;&#12290;&#28982;&#32780;&#65292;&#20849;&#21516;&#21019;&#20316;&#20132;&#20114;&#24635;&#26159;&#38754;&#21521;&#20869;&#23481;&#21019;&#20316;&#32773;&#35774;&#35745;&#30340;&#65292;&#21487;&#35775;&#38382;&#24615;&#24046;&#12290;&#20026;&#20102;&#25506;&#32034;&#28151;&#21512;&#20513;&#35758;&#20849;&#21019;&#30340;&#28216;&#25103;&#21270;&#65292;&#24182;&#20351;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#20132;&#20114;&#23545;&#29609;&#23478;&#26469;&#35828;&#26082;&#26377;&#36259;&#21448;&#26131;&#20110;&#25509;&#35302;&#65292;&#25105;&#35774;&#35745;&#20102;&#19968;&#20010;&#21407;&#22411;&#28216;&#25103;&#8220;&#34503;&#30340;&#25925;&#20107;&#8221;&#65292;&#29609;&#23478;&#21487;&#20197;&#36890;&#36807;&#29609;&#19968;&#20010;&#31867;&#20284;&#8220;&#36138;&#21507;&#34503;&#8221;&#30340;&#28216;&#25103;&#26469;&#36873;&#25321;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#26469;&#32534;&#20889;&#19968;&#20010;&#34503;&#30340;&#25925;&#20107;&#12290;&#36890;&#36807;&#36827;&#34892;&#19968;&#39033;&#26377;&#25511;&#21046;&#32452;&#23454;&#39564;&#65292;&#25506;&#35752;&#20102;&#22312;&#35774;&#35745;&#30340;&#30028;&#38754;&#20013;&#29609;&#23478;&#21644;AI&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#21160;&#21147;&#23398;&#12290;&#36890;&#36807;&#23545;11&#21517;&#29609;&#23478;&#65288;n=11&#65289;&#36827;&#34892;&#30740;&#31350;&#65292;&#25105;&#21457;&#29616;&#24403;&#29609;&#23478;&#19982;&#20004;&#20010;&#29256;&#26412;&#19968;&#36215;&#29609;&#26102;&#65292;&#20182;&#20204;&#37319;&#29992;&#19981;&#21516;&#30340;&#31574;&#30053;&#65292;&#28216;&#25103;&#26426;&#21046;&#26174;&#33879;&#24433;&#21709;&#20102;&#36755;&#20986;&#30340;&#25925;&#20107;&#12289;&#29609;&#23478;&#30340;&#21019;&#36896;&#36807;&#31243;&#20197;&#21450;&#35282;&#33394;&#30340;&#35748;&#30693;&#65292;&#19981;&#21516;&#30340;&#29609;&#23478;&#30340;&#21019;&#36896;&#24615;&#36807;&#31243;&#21463;&#21040;&#19981;&#21516;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of Artificial Intelligence (AI) enables humans to co-create content with machines. The unexpectedness of AI-generated content can bring inspiration and entertainment to users. However, the co-creation interactions are always designed for content creators and have poor accessibility. To explore gamification of mixed-initiative co-creation and make human-AI interactions accessible and fun for players, I prototyped Snake Story, a mixed-initiative game where players can select AI-generated texts to write a story of a snake by playing a "Snake" like game. A controlled experiment was conducted to investigate the dynamics of player-AI interactions with and without the game component in the designed interface. As a result of a study with 11 players (n=11), I found that players utilized different strategies when playing with the two versions, game mechanics significantly affected the output stories, players' creative process, as well as role perceptions, and players with differe
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#24110;&#21161;&#35299;&#37322;&#21644;&#35299;&#35835;&#20379;&#24212;&#38142;&#20248;&#21270;&#32467;&#26524;&#30340;&#26041;&#27861;&#12290;&#20182;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#25509;&#21463;&#26222;&#36890;&#25991;&#26412;&#26597;&#35810;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#36755;&#20986;&#20851;&#20110;&#24213;&#23618;&#20248;&#21270;&#32467;&#26524;&#30340;&#27934;&#23519;&#12290;&#36890;&#36807;&#23450;&#37327;&#22238;&#31572;&#20551;&#35774;&#24773;&#20917;&#65292;&#35813;&#26694;&#26550;&#22312;&#19981;&#25918;&#24323;&#26368;&#20808;&#36827;&#30340;&#32452;&#21512;&#20248;&#21270;&#25216;&#26415;&#30340;&#24773;&#20917;&#19979;&#24110;&#21161;&#20225;&#19994;&#36816;&#33829;&#32773;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#20449;&#20219;&#20248;&#21270;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.03875</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#20379;&#24212;&#38142;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Supply Chain Optimization. (arXiv:2307.03875v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03875
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#24110;&#21161;&#35299;&#37322;&#21644;&#35299;&#35835;&#20379;&#24212;&#38142;&#20248;&#21270;&#32467;&#26524;&#30340;&#26041;&#27861;&#12290;&#20182;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#25509;&#21463;&#26222;&#36890;&#25991;&#26412;&#26597;&#35810;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#36755;&#20986;&#20851;&#20110;&#24213;&#23618;&#20248;&#21270;&#32467;&#26524;&#30340;&#27934;&#23519;&#12290;&#36890;&#36807;&#23450;&#37327;&#22238;&#31572;&#20551;&#35774;&#24773;&#20917;&#65292;&#35813;&#26694;&#26550;&#22312;&#19981;&#25918;&#24323;&#26368;&#20808;&#36827;&#30340;&#32452;&#21512;&#20248;&#21270;&#25216;&#26415;&#30340;&#24773;&#20917;&#19979;&#24110;&#21161;&#20225;&#19994;&#36816;&#33829;&#32773;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#20449;&#20219;&#20248;&#21270;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#20379;&#24212;&#38142;&#25805;&#20316;&#28041;&#21450;&#21508;&#31181;&#22797;&#26434;&#30340;&#20915;&#31574;&#38382;&#39064;&#12290;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#65292;&#20379;&#24212;&#38142;&#21463;&#30410;&#20110;&#35745;&#31639;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#20174;&#25163;&#21160;&#22788;&#29702;&#36807;&#28193;&#21040;&#33258;&#21160;&#21270;&#21644;&#25104;&#26412;&#25928;&#30410;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#20225;&#19994;&#36816;&#33829;&#32773;&#20173;&#28982;&#38656;&#35201;&#33457;&#36153;&#22823;&#37327;&#31934;&#21147;&#26469;&#35299;&#37322;&#21644;&#35299;&#35835;&#20248;&#21270;&#32467;&#26524;&#32473;&#30456;&#20851;&#20154;&#22763;&#12290;&#21463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26368;&#36817;&#30340;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#31181;&#39072;&#35206;&#24615;&#25216;&#26415;&#22914;&#20309;&#24110;&#21161;&#24357;&#21512;&#20379;&#24212;&#38142;&#33258;&#21160;&#21270;&#21644;&#20154;&#31867;&#29702;&#35299;&#19982;&#20449;&#20219;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;\name{}&#30340;&#26694;&#26550;&#65292;&#23427;&#25509;&#21463;&#26222;&#36890;&#25991;&#26412;&#26597;&#35810;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#36755;&#20986;&#20851;&#20110;&#24213;&#23618;&#20248;&#21270;&#32467;&#26524;&#30340;&#27934;&#23519;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24182;&#27809;&#26377;&#25918;&#24323;&#26368;&#20808;&#36827;&#30340;&#32452;&#21512;&#20248;&#21270;&#25216;&#26415;&#65292;&#32780;&#26159;&#21033;&#29992;&#23427;&#26469;&#23450;&#37327;&#22320;&#22238;&#31572;&#20551;&#35774;&#24773;&#20917;&#65288;&#20363;&#22914;&#65292;&#22914;&#26524;&#25105;&#20204;&#20351;&#29992;&#20379;&#24212;&#21830;B&#32780;&#19981;&#26159;&#20379;&#24212;&#21830;A&#65292;&#25104;&#26412;&#20250;&#22914;&#20309;&#21464;&#21270;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supply chain operations traditionally involve a variety of complex decision making problems. Over the last few decades, supply chains greatly benefited from advances in computation, which allowed the transition from manual processing to automation and cost-effective optimization. Nonetheless, business operators still need to spend substantial efforts in \emph{explaining} and interpreting the optimization outcomes to stakeholders. Motivated by the recent advances in Large Language Models (LLMs), we study how this disruptive technology can help bridge the gap between supply chain automation and human comprehension and trust thereof. We design \name{} -- a framework that accepts as input queries in plain text, and outputs insights about the underlying optimization outcomes. Our framework does not forgo the state-of-the-art combinatorial optimization technology, but rather leverages it to quantitatively answer what-if scenarios (e.g., how would the cost change if we used supplier B instead
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39046;&#22495;&#33258;&#36866;&#24212;&#27969;&#31243;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#30340;&#38134;&#26631;&#20934;&#26631;&#31614;&#23558;&#30446;&#26631;&#39046;&#22495;&#25968;&#25454;&#19982;&#28304;&#39046;&#22495;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;Ki-67&#35780;&#20998;&#30340;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#22312;&#39046;&#22495;&#22806;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.03872</link><description>&lt;p&gt;
&#20351;&#29992;&#38134;&#26631;&#20934;&#26631;&#31614;&#22312;&#25968;&#23383;&#30149;&#29702;&#23398;&#20013;&#36827;&#34892;Ki-67&#35780;&#20998;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;: &#36827;&#19968;&#27493;&#25509;&#36817;&#22823;&#35268;&#27169;&#37096;&#32626;
&lt;/p&gt;
&lt;p&gt;
Domain Adaptation using Silver Standard Labels for Ki-67 Scoring in Digital Pathology: A Step Closer to Widescale Deployment. (arXiv:2307.03872v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39046;&#22495;&#33258;&#36866;&#24212;&#27969;&#31243;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#30340;&#38134;&#26631;&#20934;&#26631;&#31614;&#23558;&#30446;&#26631;&#39046;&#22495;&#25968;&#25454;&#19982;&#28304;&#39046;&#22495;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;Ki-67&#35780;&#20998;&#30340;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#22312;&#39046;&#22495;&#22806;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#34987;&#25552;&#20986;&#26469;&#25552;&#39640;Ki-67 PI&#35780;&#20998;&#30340;&#23458;&#35266;&#24615;&#21644;&#25928;&#29575;&#12290;&#25361;&#25112;&#22312;&#20110;&#65292;&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#38750;&#24120;&#20934;&#30830;&#65292;&#20294;&#22312;&#24212;&#29992;&#20110;&#39046;&#22495;&#22806;&#25968;&#25454;&#26102;&#24615;&#33021;&#19979;&#38477;&#12290;&#36825;&#23545;&#20110;&#20020;&#24202;&#24212;&#29992;&#26469;&#35828;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#27169;&#22411;&#36890;&#24120;&#26159;&#20351;&#29992;&#20379;&#24212;&#21830;&#21487;&#29992;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#32780;&#36825;&#20123;&#25968;&#25454;&#19981;&#26469;&#33258;&#30446;&#26631;&#39046;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#39046;&#22495;&#33258;&#36866;&#24212;&#27969;&#31243;&#65292;&#21033;&#29992;&#26080;&#30417;&#30563;&#26694;&#26550;&#22312;&#30446;&#26631;&#39046;&#22495;&#29983;&#25104;&#38134;&#26631;&#20934;&#65288;&#20266;&#65289;&#26631;&#31614;&#65292;&#36825;&#20123;&#26631;&#31614;&#29992;&#20110;&#22686;&#21152;&#40644;&#37329;&#26631;&#20934;&#65288;GS&#65289;&#28304;&#39046;&#22495;&#25968;&#25454;&#12290;&#30740;&#31350;&#22312;&#20004;&#20010;&#32463;&#36807;&#39564;&#35777;&#30340;Ki-67&#35780;&#20998;&#26550;&#26500;&#65288;UV-Net&#21644;piNET&#65289;&#19978;&#27979;&#35797;&#20102;&#20116;&#20010;&#35757;&#32451;&#31574;&#30053;&#65306;(1) &#20165;SS: &#22312;&#30446;&#26631;&#38134;&#26631;&#20934;&#65288;SS&#65289;&#26631;&#31614;&#19978;&#35757;&#32451;&#65292;(2) &#20165;GS: &#22312;&#28304;GS&#26631;&#31614;&#19978;&#35757;&#32451;&#65292;(3) &#28151;&#21512;: &#22312;&#30446;&#26631;SS&#21644;&#28304;GS&#26631;&#31614;&#19978;&#35757;&#32451;&#65292;(4) GS+SS: &#22312;&#28304;GS&#26631;&#31614;&#19978;&#35757;&#32451;&#24182;&#22312;&#30446;&#26631;SS&#26631;&#31614;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#21450;&#25105;&#20204;&#25552;&#20986;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning systems have been proposed to improve the objectivity and efficiency of Ki- 67 PI scoring. The challenge is that while very accurate, deep learning techniques suffer from reduced performance when applied to out-of-domain data. This is a critical challenge for clinical translation, as models are typically trained using data available to the vendor, which is not from the target domain. To address this challenge, this study proposes a domain adaptation pipeline that employs an unsupervised framework to generate silver standard (pseudo) labels in the target domain, which is used to augment the gold standard (GS) source domain data. Five training regimes were tested on two validated Ki-67 scoring architectures (UV-Net and piNET), (1) SS Only: trained on target silver standard (SS) labels, (2) GS Only: trained on source GS labels, (3) Mixed: trained on target SS and source GS labels, (4) GS+SS: trained on source GS labels and fine-tuned on target SS labels, and our proposed met
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#19982;&#22823;&#25968;&#25454;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#26080;&#32447;&#32593;&#32476;&#35774;&#35745;&#21644;&#20248;&#21270;&#20013;&#36935;&#21040;&#30340;&#22797;&#26434;&#24615;&#38382;&#39064;&#12290;&#20854;&#20013;&#65292;&#20010;&#24615;&#21270;&#26080;&#32447;&#32593;&#32476;&#36164;&#28304;&#20998;&#37197;&#26159;AI&#30340;&#20027;&#35201;&#26410;&#26469;&#24212;&#29992;&#20043;&#19968;&#12290;</title><link>http://arxiv.org/abs/2307.03867</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#26080;&#32447;&#32593;&#32476;&#36164;&#28304;&#20998;&#37197;&#65306;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#19982;&#22823;&#25968;&#25454;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Personalized Resource Allocation in Wireless Networks: An AI-Enabled and Big Data-Driven Multi-Objective Optimization. (arXiv:2307.03867v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03867
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#19982;&#22823;&#25968;&#25454;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#26080;&#32447;&#32593;&#32476;&#35774;&#35745;&#21644;&#20248;&#21270;&#20013;&#36935;&#21040;&#30340;&#22797;&#26434;&#24615;&#38382;&#39064;&#12290;&#20854;&#20013;&#65292;&#20010;&#24615;&#21270;&#26080;&#32447;&#32593;&#32476;&#36164;&#28304;&#20998;&#37197;&#26159;AI&#30340;&#20027;&#35201;&#26410;&#26469;&#24212;&#29992;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#26080;&#32447;&#32593;&#32476;&#30340;&#35774;&#35745;&#21644;&#20248;&#21270;&#22823;&#22810;&#22522;&#20110;&#24378;&#22823;&#30340;&#25968;&#23398;&#21644;&#29702;&#35770;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;5G&#21450;&#20854;&#20043;&#21518;&#30340;&#26102;&#20195;&#20013;&#65292;&#38543;&#30528;&#26032;&#24212;&#29992;&#30340;&#20986;&#29616;&#65292;&#32593;&#32476;&#35774;&#35745;&#21644;&#20248;&#21270;&#23558;&#38754;&#20020;&#21069;&#25152;&#26410;&#26377;&#30340;&#22797;&#26434;&#24615;&#12290;&#22240;&#27492;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#22312;&#26080;&#32447;&#32593;&#32476;&#35774;&#35745;&#21644;&#20248;&#21270;&#20013;&#30340;&#24212;&#29992;&#21463;&#21040;&#20851;&#27880;&#65292;AI&#25552;&#20379;&#20102;&#35299;&#20915;&#23454;&#26102;&#26497;&#20854;&#22797;&#26434;&#38382;&#39064;&#30340;&#28789;&#27963;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;AI&#30340;&#20027;&#35201;&#26410;&#26469;&#24212;&#29992;&#20043;&#19968;&#26159;&#20026;&#20247;&#22810;&#29992;&#20363;&#23454;&#29616;&#29992;&#25143;&#32423;&#20010;&#24615;&#21270;&#12290;AI&#23558;&#25913;&#21464;&#25105;&#20204;&#19982;&#35745;&#31639;&#26426;&#20132;&#20114;&#30340;&#26041;&#24335;&#65292;&#35745;&#31639;&#26426;&#23558;&#33021;&#22815;&#20197;&#26080;&#24178;&#25200;&#30340;&#26041;&#24335;&#24863;&#30693;&#29992;&#25143;&#30340;&#25351;&#20196;&#21644;&#24773;&#32490;&#65292;&#20351;&#25972;&#20010;&#36807;&#31243;&#23545;&#29992;&#25143;&#36879;&#26126;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#19968;&#33021;&#21147;&#65292;&#24182;&#20511;&#21161;&#35745;&#31639;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#21487;&#20197;&#37325;&#26032;&#35774;&#35745;&#26080;&#32447;&#32593;&#32476;&#65292;&#23454;&#29616;&#32593;&#32476;&#26381;&#21153;&#30340;&#20010;&#24615;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The design and optimization of wireless networks have mostly been based on strong mathematical and theoretical modeling. Nonetheless, as novel applications emerge in the era of 5G and beyond, unprecedented levels of complexity will be encountered in the design and optimization of the network. As a result, the use of Artificial Intelligence (AI) is envisioned for wireless network design and optimization due to the flexibility and adaptability it offers in solving extremely complex problems in real-time. One of the main future applications of AI is enabling user-level personalization for numerous use cases. AI will revolutionize the way we interact with computers in which computers will be able to sense commands and emotions from humans in a non-intrusive manner, making the entire process transparent to users. By leveraging this capability, and accelerated by the advances in computing technologies, wireless networks can be redesigned to enable the personalization of network services to t
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#32500;&#25252;&#35268;&#21010;&#12289;&#35843;&#24230;&#31574;&#30053;&#21644;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#36830;&#32493;&#30340;&#26465;&#20214;&#30417;&#25511;&#25968;&#25454;&#26469;&#24320;&#21457;&#26234;&#33021;&#32500;&#25252;&#35268;&#21010;&#22120;&#65292;&#23454;&#29616;&#38477;&#20302;&#25104;&#26412;&#12289;&#24310;&#38271;&#36164;&#20135;&#23551;&#21629;&#21644;&#30830;&#20445;&#24037;&#20316;&#22330;&#25152;&#23433;&#20840;&#31561;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2307.03860</link><description>&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#32500;&#25252;&#35268;&#21010;&#12289;&#35843;&#24230;&#31574;&#30053;&#21644;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Reinforcement and Deep Reinforcement Learning-based Solutions for Machine Maintenance Planning, Scheduling Policies, and Optimization. (arXiv:2307.03860v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03860
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#32500;&#25252;&#35268;&#21010;&#12289;&#35843;&#24230;&#31574;&#30053;&#21644;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#36830;&#32493;&#30340;&#26465;&#20214;&#30417;&#25511;&#25968;&#25454;&#26469;&#24320;&#21457;&#26234;&#33021;&#32500;&#25252;&#35268;&#21010;&#22120;&#65292;&#23454;&#29616;&#38477;&#20302;&#25104;&#26412;&#12289;&#24310;&#38271;&#36164;&#20135;&#23551;&#21629;&#21644;&#30830;&#20445;&#24037;&#20316;&#22330;&#25152;&#23433;&#20840;&#31561;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31995;&#32479;&#21644;&#26426;&#22120;&#20250;&#32463;&#21382;&#21508;&#31181;&#25925;&#38556;&#27169;&#24335;&#65292;&#23548;&#33268;&#26426;&#22120;&#20581;&#24247;&#31243;&#24230;&#19979;&#38477;&#65292;&#22240;&#27492;&#38656;&#35201;&#32500;&#25252;&#25805;&#20316;&#26469;&#23558;&#23427;&#20204;&#24674;&#22797;&#21040;&#33021;&#22815;&#25191;&#34892;&#39044;&#26399;&#21151;&#33021;&#30340;&#29366;&#24577;&#12290;&#30001;&#20110;&#32500;&#25252;&#20219;&#21153;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#65292;&#25152;&#20197;&#32500;&#25252;&#35268;&#21010;&#23545;&#20110;&#30830;&#20445;&#29983;&#20135;&#31995;&#32479;&#21644;&#20854;&#20182;&#34892;&#19994;&#30340;&#24179;&#31283;&#36816;&#34892;&#33267;&#20851;&#37325;&#35201;&#12290;&#32500;&#25252;&#35268;&#21010;&#26159;&#19968;&#20010;&#20915;&#31574;&#38382;&#39064;&#65292;&#26088;&#22312;&#21046;&#23450;&#26368;&#20248;&#30340;&#32500;&#25252;&#31574;&#30053;&#21644;&#35745;&#21010;&#65292;&#20197;&#24110;&#21161;&#38477;&#20302;&#32500;&#25252;&#25104;&#26412;&#12289;&#24310;&#38271;&#36164;&#20135;&#23551;&#21629;&#12289;&#26368;&#22823;&#21270;&#21487;&#29992;&#24615;&#65292;&#26368;&#32456;&#30830;&#20445;&#24037;&#20316;&#22330;&#25152;&#23433;&#20840;&#12290;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#20915;&#31574;&#31639;&#27861;&#65292;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#24212;&#29992;&#20110;&#24320;&#21457;&#21160;&#24577;&#32500;&#25252;&#35745;&#21010;&#65292;&#21516;&#26102;&#21033;&#29992;&#31995;&#32479;&#21644;&#26426;&#22120;&#29366;&#24577;&#30340;&#36830;&#32493;&#20449;&#24687;&#12290;&#36890;&#36807;&#21033;&#29992;&#31995;&#32479;&#21644;&#26426;&#22120;&#30340;&#26465;&#20214;&#30417;&#25511;&#25968;&#25454;&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#24320;&#21457;&#26234;&#33021;&#30340;&#32500;&#25252;&#35268;&#21010;&#22120;&#65292;&#21487;&#20197;&#20248;&#21270;&#32500;&#25252;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Systems and machines undergo various failure modes that result in machine health degradation, so maintenance actions are required to restore them back to a state where they can perform their expected functions. Since maintenance tasks are inevitable, maintenance planning is essential to ensure the smooth operations of the production system and other industries at large. Maintenance planning is a decision-making problem that aims at developing optimum maintenance policies and plans that help reduces maintenance costs, extend asset life, maximize their availability, and ultimately ensure workplace safety. Reinforcement learning is a data-driven decision-making algorithm that has been increasingly applied to develop dynamic maintenance plans while leveraging the continuous information from condition monitoring of the system and machine states. By leveraging the condition monitoring data of systems and machines with reinforcement learning, smart maintenance planners can be developed, which
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#30340;&#28151;&#21512;&#23398;&#20064;&#33539;&#24335;&#65292;&#26088;&#22312;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32570;&#20047;&#35299;&#37322;&#24615;&#21644;&#21487;&#36801;&#31227;&#24615;&#30340;&#38382;&#39064;&#12290;&#37325;&#28857;&#20851;&#27880;&#26426;&#22120;&#20154;&#25163;&#26415;&#39046;&#22495;&#65292;&#23545;&#29616;&#26377;&#30340;&#33258;&#20027;&#25163;&#26415;&#26426;&#22120;&#20154;&#35299;&#20915;&#26041;&#26696;&#21644;&#25361;&#25112;&#36827;&#34892;&#20102;&#32508;&#36848;&#12290;</title><link>http://arxiv.org/abs/2307.03853</link><description>&lt;p&gt;
&#25945;&#25105;&#22914;&#20309;&#23398;&#20064;&#65306;&#38754;&#21521;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#30340;&#26426;&#22120;&#20154;&#25163;&#26415;&#31995;&#32479;&#30340;&#36879;&#35270;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Teach Me How to Learn: A Perspective Review towards User-centered Neuro-symbolic Learning for Robotic Surgical Systems. (arXiv:2307.03853v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03853
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#30340;&#28151;&#21512;&#23398;&#20064;&#33539;&#24335;&#65292;&#26088;&#22312;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32570;&#20047;&#35299;&#37322;&#24615;&#21644;&#21487;&#36801;&#31227;&#24615;&#30340;&#38382;&#39064;&#12290;&#37325;&#28857;&#20851;&#27880;&#26426;&#22120;&#20154;&#25163;&#26415;&#39046;&#22495;&#65292;&#23545;&#29616;&#26377;&#30340;&#33258;&#20027;&#25163;&#26415;&#26426;&#22120;&#20154;&#35299;&#20915;&#26041;&#26696;&#21644;&#25361;&#25112;&#36827;&#34892;&#20102;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36827;&#23637;&#20351;&#24471;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#24863;&#30693;&#30340;&#38750;&#31526;&#21495;&#23618;&#27425;&#19978;&#35782;&#21035;&#29289;&#20307;&#65288;&#20363;&#22914;&#36890;&#36807;&#20256;&#24863;&#22120;&#34701;&#21512;&#21644;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20027;&#35201;&#22522;&#20110;&#40657;&#30418;&#23376;&#23398;&#20064;&#27169;&#22411;&#20173;&#28982;&#32570;&#20047;&#35299;&#37322;&#24615;&#21644;&#21487;&#36801;&#31227;&#24615;&#65292;&#24182;&#19988;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#38656;&#27714;&#12290;&#19968;&#20010;&#26367;&#20195;&#35299;&#20915;&#26041;&#26696;&#26159;&#36890;&#36807;&#19987;&#23478;&#21453;&#39304;&#65288;&#21363;&#20154;&#22312;&#29615;&#36335;&#23398;&#20064;&#65289;&#22312;&#24863;&#30693;&#30340;&#38750;&#31526;&#21495;&#23618;&#27425;&#21644;&#27010;&#24565;&#30340;&#31526;&#21495;&#23618;&#27425;&#19978;&#25945;&#23548;&#26426;&#22120;&#20154;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36825;&#31181;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#28151;&#21512;&#23398;&#20064;&#33539;&#24335;&#30340;&#27010;&#24565;&#65292;&#37325;&#28857;&#20851;&#27880;&#26426;&#22120;&#20154;&#25163;&#26415;&#24773;&#22659;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#38750;&#26426;&#22120;&#20154;&#21644;&#19968;&#20123;&#36890;&#29992;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#28151;&#21512;&#23398;&#20064;&#19978;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#20851;&#27880;&#25163;&#26415;&#26426;&#22120;&#20154;&#39046;&#22495;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#30456;&#20851;&#30340;&#30740;&#31350;&#65292;&#37325;&#28857;&#20851;&#27880;&#20154;&#22312;&#29615;&#36335;&#25163;&#26415;&#26426;&#22120;&#20154;&#31995;&#32479;&#19978;&#30340;&#30740;&#31350;&#12290;&#36825;&#20010;&#35780;&#20272;&#31361;&#20986;&#20102;&#33258;&#20027;&#25163;&#26415;&#26426;&#22120;&#20154;&#30340;&#26368;&#31361;&#20986;&#35299;&#20915;&#26041;&#26696;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in machine learning models allowed robots to identify objects on a perceptual nonsymbolic level (e.g., through sensor fusion and natural language understanding). However, these primarily black-box learning models still lack interpretation and transferability and require high data and computational demand. An alternative solution is to teach a robot on both perceptual nonsymbolic and conceptual symbolic levels through hybrid neurosymbolic learning approaches with expert feedback (i.e., human-in-the-loop learning). This work proposes a concept for this user-centered hybrid learning paradigm that focuses on robotic surgical situations. While most recent research focused on hybrid learning for non-robotic and some generic robotic domains, little work focuses on surgical robotics. We survey this related research while focusing on human-in-the-loop surgical robotic systems. This evaluation highlights the most prominent solutions for autonomous surgical robots and the challeng
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21487;&#23454;&#29616;&#22238;&#24402;&#38382;&#39064;&#30340;PAC&#23398;&#20064;&#21644;&#22312;&#32447;&#23398;&#20064;&#30340;&#32479;&#35745;&#22797;&#26434;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#20110;&#21487;&#23398;&#20064;&#24615;&#30340;&#24517;&#35201;&#26465;&#20214;&#21644;&#20805;&#20998;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2307.03848</link><description>&lt;p&gt;
&#21487;&#23454;&#29616;&#22238;&#24402;&#30340;&#26368;&#20248;&#23398;&#20064;&#31639;&#27861;&#65306;PAC&#23398;&#20064;&#21644;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Optimal Learners for Realizable Regression: PAC Learning and Online Learning. (arXiv:2307.03848v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21487;&#23454;&#29616;&#22238;&#24402;&#38382;&#39064;&#30340;PAC&#23398;&#20064;&#21644;&#22312;&#32447;&#23398;&#20064;&#30340;&#32479;&#35745;&#22797;&#26434;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#20110;&#21487;&#23398;&#20064;&#24615;&#30340;&#24517;&#35201;&#26465;&#20214;&#21644;&#20805;&#20998;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#23545;&#21487;&#23454;&#29616;&#22238;&#24402;&#22312;PAC&#23398;&#20064;&#21644;&#22312;&#32447;&#23398;&#20064;&#30340;&#32479;&#35745;&#22797;&#26434;&#24230;&#36827;&#34892;&#21051;&#30011;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#26377;&#38480;&#30340;fat shattering&#32500;&#24230;&#23545;&#20110;PAC&#23398;&#20064;&#30340;&#20805;&#20998;&#24615;&#20197;&#21450;&#26377;&#38480;&#30340;scaled Natarajan&#32500;&#24230;&#23545;&#20110;&#24517;&#35201;&#24615;&#30340;&#23384;&#22312;&#65292;&#20294;&#33258;&#20174;Simon 1997&#65288;SICOMP '97&#65289;&#30340;&#24037;&#20316;&#20197;&#26469;&#65292;&#23545;&#20110;&#26356;&#23436;&#25972;&#30340;&#21051;&#30011;&#30340;&#36827;&#23637;&#29978;&#23569;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#31181;&#26368;&#23567;&#21270;&#23454;&#20363;&#26368;&#20248;&#23398;&#20064;&#31639;&#27861;&#26469;&#23545;&#21487;&#23454;&#29616;&#22238;&#24402;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26082;&#23450;&#24615;&#21448;&#23450;&#37327;&#22320;&#21051;&#30011;&#20102;&#21738;&#20123;&#31867;&#30340;&#23454;&#25968;&#39044;&#27979;&#22120;&#21487;&#20197;&#34987;&#23398;&#20064;&#30340;&#26032;&#39062;&#32500;&#24230;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20010;&#19982;&#22270;&#32500;&#24230;&#30456;&#20851;&#30340;&#32452;&#21512;&#32500;&#24230;&#65292;&#35813;&#32500;&#24230;&#21051;&#30011;&#20102;&#22312;&#21487;&#23454;&#29616;&#35774;&#32622;&#20013;&#30340;ERM&#21487;&#23398;&#20064;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#19982;DS&#32500;&#24230;&#30456;&#20851;&#30340;&#32452;&#21512;&#32500;&#24230;&#24314;&#31435;&#20102;&#23398;&#20064;&#21487;&#34892;&#24615;&#30340;&#24517;&#35201;&#26465;&#20214;&#65292;&#24182;&#29468;&#27979;&#23427;&#20063;&#21487;&#33021;&#26159;&#20805;&#20998;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we aim to characterize the statistical complexity of realizable regression both in the PAC learning setting and the online learning setting.  Previous work had established the sufficiency of finiteness of the fat shattering dimension for PAC learnability and the necessity of finiteness of the scaled Natarajan dimension, but little progress had been made towards a more complete characterization since the work of Simon 1997 (SICOMP '97). To this end, we first introduce a minimax instance optimal learner for realizable regression and propose a novel dimension that both qualitatively and quantitatively characterizes which classes of real-valued predictors are learnable. We then identify a combinatorial dimension related to the Graph dimension that characterizes ERM learnability in the realizable setting. Finally, we establish a necessary condition for learnability based on a combinatorial dimension related to the DS dimension, and conjecture that it may also be sufficient in 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RADAR&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#23398;&#20064;&#23454;&#29616;&#20102;&#40065;&#26834;&#30340;AI&#25991;&#26412;&#26816;&#27979;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;AI&#25991;&#26412;&#26816;&#27979;&#22120;&#23545;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25913;&#20889;&#19981;&#20855;&#22791;&#40065;&#26834;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.03838</link><description>&lt;p&gt;
RADAR: &#36890;&#36807;&#23545;&#25239;&#24615;&#23398;&#20064;&#23454;&#29616;&#40065;&#26834;&#30340;AI&#25991;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
RADAR: Robust AI-Text Detection via Adversarial Learning. (arXiv:2307.03838v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RADAR&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#23398;&#20064;&#23454;&#29616;&#20102;&#40065;&#26834;&#30340;AI&#25991;&#26412;&#26816;&#27979;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;AI&#25991;&#26412;&#26816;&#27979;&#22120;&#23545;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25913;&#20889;&#19981;&#20855;&#22791;&#40065;&#26834;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#20197;&#21450;ChatGPT&#31867;&#24212;&#29992;&#30340;&#26222;&#21450;&#24050;&#32463;&#27169;&#31946;&#20102;&#20154;&#31867;&#21644;&#26426;&#22120;&#20043;&#38388;&#39640;&#36136;&#37327;&#25991;&#26412;&#29983;&#25104;&#30340;&#30028;&#38480;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;&#23545;&#25105;&#20204;&#30340;&#25216;&#26415;&#21644;&#31038;&#20250;&#39044;&#26399;&#30340;&#38761;&#21629;&#24615;&#21464;&#21270;&#22806;&#65292;&#21306;&#20998;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#65288;AI&#25991;&#26412;&#65289;&#21644;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#22256;&#38590;&#20063;&#24102;&#26469;&#20102;&#26032;&#30340;&#28389;&#29992;&#21644;&#20844;&#24179;&#24615;&#25361;&#25112;&#65292;&#20363;&#22914;&#34394;&#20551;&#20869;&#23481;&#29983;&#25104;&#65292;&#25220;&#34989;&#20197;&#21450;&#23545;&#26080;&#36764;&#20316;&#32773;&#30340;&#38169;&#35823;&#25351;&#25511;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#30740;&#31350;&#34920;&#26126;&#24403;&#21069;&#30340;AI&#25991;&#26412;&#26816;&#27979;&#22120;&#23545;&#22522;&#20110;LLM&#30340;&#25913;&#20889;&#19981;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#20294;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;RADAR&#30340;&#26032;&#26694;&#26550;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#23545;&#25239;&#24615;&#23398;&#20064;&#20849;&#21516;&#35757;&#32451;&#20102;&#19968;&#20010;&#40065;&#26834;&#30340;AI&#25991;&#26412;&#26816;&#27979;&#22120;&#12290;RADAR&#22522;&#20110;&#19968;&#20010;&#25913;&#20889;&#22120;&#21644;&#19968;&#20010;&#26816;&#27979;&#22120;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#12290;&#25913;&#20889;&#22120;&#30340;&#30446;&#26631;&#26159;&#29983;&#25104;&#36924;&#30495;&#30340;&#20869;&#23481;&#20197;&#35268;&#36991;AI&#25991;&#26412;&#26816;&#27979;&#12290;RADAR&#20351;&#29992;&#26469;&#33258;&#26816;&#27979;&#22120;&#30340;&#21453;&#39304;&#26469;&#26356;&#26032;&#25913;&#20889;&#22120;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in large language models (LLMs) and the intensifying popularity of ChatGPT-like applications have blurred the boundary of high-quality text generation between humans and machines. However, in addition to the anticipated revolutionary changes to our technology and society, the difficulty of distinguishing LLM-generated texts (AI-text) from human-generated texts poses new challenges of misuse and fairness, such as fake content generation, plagiarism, and false accusation of innocent writers. While existing works show that current AI-text detectors are not robust to LLM-based paraphrasing, this paper aims to bridge this gap by proposing a new framework called RADAR, which jointly trains a Robust AI-text Detector via Adversarial leaRning. RADAR is based on adversarial training of a paraphraser and a detector. The paraphraser's goal is to generate realistic contents to evade AI-text detection. RADAR uses the feedback from the detector to update the paraphraser, and vice vers
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22522;&#20110;&#20248;&#21270;&#21644;&#22522;&#20110;&#23398;&#20064;&#26041;&#27861;&#30340;&#38646;&#26679;&#26412;&#25193;&#25955;&#20248;&#21270;&#65288;ZeDO&#65289;&#31649;&#36947;&#65292;&#29992;&#20110;&#35299;&#20915;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#20013;&#30340;&#36328;&#39046;&#22495;&#21644;&#37326;&#22806;&#25361;&#25112;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.03833</link><description>&lt;p&gt;
&#22238;&#24402;&#20248;&#21270;&#65306;&#22522;&#20110;&#25193;&#25955;&#30340;&#38646;&#26679;&#26412;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Back to Optimization: Diffusion-based Zero-Shot 3D Human Pose Estimation. (arXiv:2307.03833v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22522;&#20110;&#20248;&#21270;&#21644;&#22522;&#20110;&#23398;&#20064;&#26041;&#27861;&#30340;&#38646;&#26679;&#26412;&#25193;&#25955;&#20248;&#21270;&#65288;ZeDO&#65289;&#31649;&#36947;&#65292;&#29992;&#20110;&#35299;&#20915;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#20013;&#30340;&#36328;&#39046;&#22495;&#21644;&#37326;&#22806;&#25361;&#25112;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;&#22823;&#22810;&#25968;&#22522;&#20934;&#27979;&#35797;&#20013;&#27604;&#20256;&#32479;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#23427;&#20204;&#20027;&#23548;&#20102;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;&#37326;&#22806;&#30340;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#20173;&#28982;&#26159;&#22522;&#20110;&#23398;&#20064;&#30340;&#27169;&#22411;&#38754;&#20020;&#30340;&#26368;&#22823;&#25361;&#25112;&#65292;&#26080;&#35770;&#26159;2D-3D&#25552;&#21319;&#65292;&#22270;&#20687;&#21040;3D&#36824;&#26159;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#35757;&#32451;&#30340;&#32593;&#32476;&#38544;&#21547;&#22320;&#23398;&#20064;&#20102;&#30456;&#26426;&#20869;&#21442;&#21644;&#22522;&#20110;&#39046;&#22495;&#30340;3D&#20154;&#20307;&#23039;&#21183;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#32479;&#35745;&#24179;&#22343;&#26469;&#20272;&#35745;&#23039;&#21183;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#21487;&#20197;&#36880;&#26696;&#20363;&#20272;&#35745;&#32467;&#26524;&#65292;&#33021;&#22815;&#22312;&#37326;&#22806;&#39044;&#27979;&#26356;&#22810;&#26679;&#21270;&#21644;&#22797;&#26434;&#30340;&#20154;&#20307;&#23039;&#21183;&#12290;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#20248;&#21270;&#21644;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#25193;&#25955;&#20248;&#21270;&#65288;ZeDO&#65289;&#31649;&#36947;&#29992;&#20110;&#35299;&#20915;&#36328;&#39046;&#22495;&#21644;&#37326;&#22806;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#22810;&#20551;&#35774;ZeDO&#22312;Human3.6M&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65288;minMPJPE 51.4mm&#65289;&#65292;&#24182;&#19988;&#26080;&#38656;&#23545;&#20854;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning-based methods have dominated the 3D human pose estimation (HPE) tasks with significantly better performance in most benchmarks than traditional optimization-based methods. Nonetheless, 3D HPE in the wild is still the biggest challenge of learning-based models, whether with 2D-3D lifting, image-to-3D, or diffusion-based methods, since the trained networks implicitly learn camera intrinsic parameters and domain-based 3D human pose distributions and estimate poses by statistical average. On the other hand, the optimization-based methods estimate results case-by-case, which can predict more diverse and sophisticated human poses in the wild. By combining the advantages of optimization-based and learning-based methods, we propose the Zero-shot Diffusion-based Optimization (ZeDO) pipeline for 3D HPE to solve the problem of cross-domain and in-the-wild 3D HPE. Our multi-hypothesis ZeDO achieves state-of-the-art (SOTA) performance on Human3.6M as minMPJPE $51.4$mm without training with
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#31181;MRI&#24378;&#24230;&#26631;&#20934;&#21270;&#26041;&#27861;&#22312;&#22810;&#20013;&#24515;FLAIR MRI&#30333;&#36136;&#30149;&#21464;&#20998;&#21106;&#20013;&#30340;&#25928;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#27169;&#22411;&#65292;&#23558;&#19981;&#21516;&#26041;&#27861;&#30340;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#32452;&#21512;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#24378;&#24230;&#26631;&#20934;&#21270;&#21487;&#20197;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#26032;&#26426;&#26500;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.03827</link><description>&lt;p&gt;
&#24378;&#24230;&#26631;&#20934;&#21270;&#23545;&#22810;&#20013;&#24515;FLAIR MRI&#20013;&#28145;&#24230;&#23398;&#20064;&#30333;&#36136;&#30149;&#21464;&#20998;&#21106;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Effect of Intensity Standardization on Deep Learning for WML Segmentation in Multi-Centre FLAIR MRI. (arXiv:2307.03827v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#31181;MRI&#24378;&#24230;&#26631;&#20934;&#21270;&#26041;&#27861;&#22312;&#22810;&#20013;&#24515;FLAIR MRI&#30333;&#36136;&#30149;&#21464;&#20998;&#21106;&#20013;&#30340;&#25928;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#27169;&#22411;&#65292;&#23558;&#19981;&#21516;&#26041;&#27861;&#30340;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#32452;&#21512;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#24378;&#24230;&#26631;&#20934;&#21270;&#21487;&#20197;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#26032;&#26426;&#26500;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;MRI&#20013;&#29992;&#20110;&#30333;&#36136;&#30149;&#21464;&#20998;&#21106;&#26102;&#65292;&#24403;&#24212;&#29992;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#20043;&#22806;&#30340;&#25195;&#25551;&#20202;&#25110;&#20013;&#24515;&#30340;&#25968;&#25454;&#26102;&#65292;&#24615;&#33021;&#20250;&#38477;&#20302;&#12290;&#30001;&#20110;&#24403;&#21069;&#27169;&#22411;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#26032;&#26426;&#26500;&#30340;&#25968;&#25454;&#65292;&#36825;&#23545;&#20110;&#36716;&#21270;&#21644;&#24191;&#27867;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20960;&#31181;MRI&#24378;&#24230;&#26631;&#20934;&#21270;&#26041;&#27861;&#20316;&#20026;&#22810;&#20013;&#24515;&#28082;&#20307;&#34928;&#20943;&#21453;&#36716;&#24674;&#22797;&#65288;FLAIR&#65289;MRI&#30333;&#36136;&#30149;&#21464;&#20998;&#21106;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;FLAIR MRI&#30340;&#26041;&#27861;IAMLAB&#65292;&#20197;&#21450;&#20854;&#20182;&#24120;&#35265;&#30340;&#26631;&#20934;&#21270;&#25216;&#26415;&#65292;&#22914;White-strip&#12289;Nyul&#21644;Z-score&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#27169;&#22411;&#65292;&#23558;&#27599;&#20010;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#32452;&#21512;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#36339;&#36291;&#36830;&#25509;UNet&#65288;SC UNet&#65289;&#27169;&#22411;&#65292;&#20351;&#29992;&#26631;&#20934;&#21270;&#22270;&#20687;&#21644;&#21407;&#22987;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#22810;&#20010;&#32500;&#24230;&#19978;&#35780;&#20272;&#20102;&#20998;&#21106;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) methods for white matter lesion (WML) segmentation in MRI suffer a reduction in performance when applied on data from a scanner or centre that is out-of-distribution (OOD) from the training data. This is critical for translation and widescale adoption, since current models cannot be readily applied to data from new institutions. In this work, we evaluate several intensity standardization methods for MRI as a preprocessing step for WML segmentation in multi-centre Fluid-Attenuated Inversion Recovery (FLAIR) MRI. We evaluate a method specifically developed for FLAIR MRI called IAMLAB along with other popular normalization techniques such as White-strip, Nyul and Z-score. We proposed an Ensemble model that combines predictions from each of these models. A skip-connection UNet (SC UNet) was trained on the standardized images, as well as the original data and segmentation performance was evaluated over several dimensions. The training (in-distribution) data consists of a 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;AI&#32842;&#22825;&#31995;&#32479;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#30740;&#31350;&#20102;&#23558;&#32842;&#22825;&#31995;&#32479;&#19982;&#25628;&#32034;&#24037;&#20855;&#32467;&#21512;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;&#35813;&#30740;&#31350;&#21457;&#29616;AI&#32842;&#22825;&#31995;&#32479;&#26377;&#26395;&#25913;&#21464;&#20154;&#20204;&#30340;&#25628;&#32034;&#34892;&#20026;&#21644;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2307.03826</link><description>&lt;p&gt;
AI&#32842;&#22825;&#22914;&#20309;&#25913;&#21464;&#25628;&#32034;&#34892;&#20026;&#65311;
&lt;/p&gt;
&lt;p&gt;
How does AI chat change search behaviors?. (arXiv:2307.03826v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03826
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;AI&#32842;&#22825;&#31995;&#32479;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#30740;&#31350;&#20102;&#23558;&#32842;&#22825;&#31995;&#32479;&#19982;&#25628;&#32034;&#24037;&#20855;&#32467;&#21512;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;&#35813;&#30740;&#31350;&#21457;&#29616;AI&#32842;&#22825;&#31995;&#32479;&#26377;&#26395;&#25913;&#21464;&#20154;&#20204;&#30340;&#25628;&#32034;&#34892;&#20026;&#21644;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;AI&#24037;&#20855;&#22914;chatGPT&#26377;&#26395;&#25913;&#21464;&#20154;&#20204;&#19982;&#22312;&#32447;&#20449;&#24687;&#30340;&#20114;&#21160;&#26041;&#24335;&#12290;&#36817;&#26399;&#65292;&#24494;&#36719;&#23459;&#24067;&#20102;&#20182;&#20204;&#30340;&#8220;&#26032;Bing&#8221;&#25628;&#32034;&#31995;&#32479;&#65292;&#20854;&#20013;&#25972;&#21512;&#20102;&#26469;&#33258;OpenAI&#30340;&#32842;&#22825;&#21644;&#29983;&#25104;&#24335;AI&#25216;&#26415;&#12290;&#35895;&#27468;&#20063;&#23459;&#24067;&#20102;&#23558;&#37096;&#32626;&#31867;&#20284;&#25216;&#26415;&#30340;&#25628;&#32034;&#30028;&#38754;&#30340;&#35745;&#21010;&#12290;&#36825;&#20123;&#26032;&#25216;&#26415;&#23558;&#25913;&#21464;&#20154;&#20204;&#25628;&#32034;&#20449;&#24687;&#30340;&#26041;&#24335;&#12290;&#26412;&#30740;&#31350;&#26159;&#23545;&#20154;&#20204;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#22914;&#20309;&#20351;&#29992;&#29983;&#25104;&#24335;AI&#32842;&#22825;&#31995;&#32479;&#65288;&#20197;&#19979;&#31616;&#31216;&#20026;chat&#65289;&#20197;&#21450;&#23558;chat&#31995;&#32479;&#19982;&#29616;&#26377;&#25628;&#32034;&#24037;&#20855;&#32467;&#21512;&#21487;&#33021;&#22914;&#20309;&#24433;&#21709;&#29992;&#25143;&#30340;&#25628;&#32034;&#34892;&#20026;&#21644;&#31574;&#30053;&#30340;&#26089;&#26399;&#35843;&#26597;&#30740;&#31350;&#12290;&#25105;&#20204;&#25253;&#36947;&#20102;&#19968;&#20010;&#25506;&#32034;&#24615;&#29992;&#25143;&#30740;&#31350;&#65292;&#26377;10&#21517;&#21442;&#19982;&#32773;&#20351;&#29992;&#20102;&#19968;&#20010;&#20351;&#29992;OpenAI GPT-3.5 API&#21644;Bing Web Search v5 API&#30340;&#32508;&#21512;Chat+Search&#31995;&#32479;&#12290;&#21442;&#19982;&#32773;&#23436;&#25104;&#20102;&#19977;&#20010;&#25628;&#32034;&#20219;&#21153;&#12290;&#22312;&#36825;&#31687;&#21021;&#27493;&#32467;&#26524;&#30340;&#39044;&#21360;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25253;&#36947;&#20102;&#29992;&#25143;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#38382;&#39064;&#21644;&#20351;&#29992;chat&#31995;&#32479;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI tools such as chatGPT are poised to change the way people engage with online information. Recently, Microsoft announced their "new Bing" search system which incorporates chat and generative AI technology from OpenAI. Google has announced plans to deploy search interfaces that incorporate similar types of technology. These new technologies will transform how people can search for information. The research presented here is an early investigation into how people make use of a generative AI chat system (referred to simply as chat from here on) as part of a search process, and how the incorporation of chat systems with existing search tools may effect users search behaviors and strategies.  We report on an exploratory user study with 10 participants who used a combined Chat+Search system that utilized the OpenAI GPT-3.5 API and the Bing Web Search v5 API. Participants completed three search tasks. In this pre-print paper of preliminary results, we report on ways that users in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35814;&#32454;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23884;&#20837;&#24335;&#31995;&#32479;&#24320;&#21457;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-4&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#36328;&#39046;&#22495;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#33021;&#22815;&#29983;&#25104;&#20934;&#30830;&#30340;&#31243;&#24207;&#21644;&#39537;&#21160;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2307.03817</link><description>&lt;p&gt;
&#25506;&#32034;&#21644;&#25551;&#36848;&#29992;&#20110;&#23884;&#20837;&#24335;&#31995;&#32479;&#24320;&#21457;&#21644;&#35843;&#35797;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Exploring and Characterizing Large Language Models For Embedded System Development and Debugging. (arXiv:2307.03817v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35814;&#32454;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23884;&#20837;&#24335;&#31995;&#32479;&#24320;&#21457;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-4&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#36328;&#39046;&#22495;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#33021;&#22815;&#29983;&#25104;&#20934;&#30830;&#30340;&#31243;&#24207;&#21644;&#39537;&#21160;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#23637;&#29616;&#20986;&#20102;&#29983;&#25104;&#20195;&#30721;&#30340;&#26174;&#30528;&#33021;&#21147;&#65292;&#28982;&#32780;&#23427;&#20204;&#23545;&#20110;&#23884;&#20837;&#24335;&#31995;&#32479;&#24320;&#21457;&#25152;&#38656;&#30340;&#36328;&#39046;&#22495;&#30828;&#20214;&#21644;&#36719;&#20214;&#30693;&#35782;&#30340;&#33021;&#21147;&#23578;&#26410;&#34987;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#39046;&#20808;&#30340;LLMs&#65288;GPT-3.5&#12289;GPT-4&#12289;PaLM 2&#65289;&#20197;&#35780;&#20272;&#23427;&#20204;&#22312;&#23884;&#20837;&#24335;&#31995;&#32479;&#24320;&#21457;&#20013;&#30340;&#24615;&#33021;&#65292;&#30740;&#31350;&#20154;&#31867;&#31243;&#24207;&#21592;&#22914;&#20309;&#19982;&#36825;&#20123;&#24037;&#20855;&#20132;&#20114;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#23884;&#20837;&#24335;&#31995;&#32479;&#36719;&#20214;&#24037;&#31243;&#24037;&#20316;&#27969;&#31243;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#30828;&#20214;&#22312;&#22238;&#36335;&#35780;&#20272;&#24179;&#21488;&#65292;&#29992;&#20110;&#39564;&#35777;LLM&#29983;&#25104;&#30340;&#31243;&#24207;&#65292;&#20351;&#29992;&#20256;&#24863;&#22120;&#25191;&#34892;&#22120;&#23545;&#12290;&#25105;&#20204;&#36890;&#36807;N=450&#20010;&#23454;&#39564;&#27604;&#36739;&#20102;&#25152;&#26377;&#19977;&#20010;&#27169;&#22411;&#65292;&#20196;&#20154;&#24778;&#35766;&#22320;&#21457;&#29616;GPT-4&#29305;&#21035;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#36328;&#39046;&#22495;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20174;&#19968;&#20010;&#21333;&#19968;&#30340;&#25552;&#31034;&#20013;&#29983;&#25104;&#23436;&#20840;&#27491;&#30830;&#30340;&#31243;&#24207;&#12290;&#22312;N=50&#20010;&#35797;&#39564;&#20013;&#65292;GPT-4&#30340;I2C&#25509;&#21475;&#21151;&#33021;&#36798;&#21040;&#20102;66%&#12290;GPT-4&#36824;&#29983;&#25104;&#20102;&#23492;&#23384;&#22120;&#32423;&#39537;&#21160;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown remarkable abilities to generate code, however their ability to develop software for embedded systems, which requires cross-domain knowledge of hardware and software has not been studied. In this paper we systematically evaluate leading LLMs (GPT-3.5, GPT-4, PaLM 2) to assess their performance for embedded system development, study how human programmers interact with these tools, and develop an AI-based software engineering workflow for building embedded systems.  We develop an an end-to-end hardware-in-the-loop evaluation platform for verifying LLM generated programs using sensor actuator pairs. We compare all three models with N=450 experiments and find surprisingly that GPT-4 especially shows an exceptional level of cross-domain understanding and reasoning, in some cases generating fully correct programs from a single prompt. In N=50 trials, GPT-4 produces functional I2C interfaces 66% of the time. GPT-4 also produces register-level drivers, c
&lt;/p&gt;</description></item><item><title>URL&#22522;&#20934;&#26159;&#19968;&#20010;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#36716;&#31227;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#26041;&#24335;&#65292;&#30740;&#31350;&#21457;&#29616;&#19987;&#27880;&#20110;&#34920;&#31034;&#26412;&#36523;&#19981;&#30830;&#23450;&#24615;&#25110;&#30452;&#25509;&#20272;&#35745;&#39044;&#27979;&#39118;&#38505;&#30340;&#26041;&#27861;&#25928;&#26524;&#20248;&#20110;&#22522;&#20110;&#27010;&#29575;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.03810</link><description>&lt;p&gt;
URL&#65306;&#19968;&#31181;&#21487;&#36716;&#31227;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#34920;&#31034;&#23398;&#20064;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
URL: A Representation Learning Benchmark for Transferable Uncertainty Estimates. (arXiv:2307.03810v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03810
&lt;/p&gt;
&lt;p&gt;
URL&#22522;&#20934;&#26159;&#19968;&#20010;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#36716;&#31227;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#26041;&#24335;&#65292;&#30740;&#31350;&#21457;&#29616;&#19987;&#27880;&#20110;&#34920;&#31034;&#26412;&#36523;&#19981;&#30830;&#23450;&#24615;&#25110;&#30452;&#25509;&#20272;&#35745;&#39044;&#27979;&#39118;&#38505;&#30340;&#26041;&#27861;&#25928;&#26524;&#20248;&#20110;&#22522;&#20110;&#27010;&#29575;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#31034;&#23398;&#20064;&#26174;&#33879;&#25512;&#21160;&#20102;&#35813;&#39046;&#22495;&#21457;&#23637;&#20986;&#33021;&#22815;&#20316;&#20026;&#20174;&#38646;&#24320;&#22987;&#36801;&#31227;&#21040;&#26032;&#25968;&#25454;&#38598;&#26102;&#30340;&#26377;&#20215;&#20540;&#36215;&#28857;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#38543;&#30528;&#23545;&#21487;&#38752;&#26426;&#22120;&#23398;&#20064;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#65292;&#38656;&#35201;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#19981;&#20165;&#33021;&#25552;&#20379;&#23884;&#20837;&#21521;&#37327;&#65292;&#36824;&#33021;&#25552;&#20379;&#21487;&#36716;&#31227;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#20026;&#20102;&#24341;&#23548;&#36825;&#26679;&#30340;&#27169;&#22411;&#30340;&#24320;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;URL&#65288;Uncertainty-aware Representation Learning&#65289;&#22522;&#20934;&#12290;&#38500;&#20102;&#34920;&#31034;&#30340;&#21487;&#36716;&#31227;&#24615;&#20043;&#22806;&#65292;&#23427;&#36824;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#27979;&#37327;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#38646;&#26679;&#26412;&#21487;&#36716;&#31227;&#24615;&#12290;&#25105;&#20204;&#24212;&#29992;URL&#26469;&#35780;&#20272;11&#31181;&#22312;ImageNet&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#24182;&#36716;&#31227;&#21040;8&#20010;&#19979;&#28216;&#25968;&#25454;&#38598;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#22120;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30528;&#37325;&#20110;&#34920;&#31034;&#26412;&#36523;&#30340;&#19981;&#30830;&#23450;&#24615;&#25110;&#30452;&#25509;&#20272;&#35745;&#39044;&#27979;&#39118;&#38505;&#30340;&#26041;&#27861;&#20248;&#20110;&#22522;&#20110;&#19978;&#28216;&#31867;&#21035;&#30340;&#27010;&#29575;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#21487;&#36716;&#31227;&#30340;&#19981;&#30830;&#23450;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representation learning has significantly driven the field to develop pretrained models that can act as a valuable starting point when transferring to new datasets. With the rising demand for reliable machine learning and uncertainty quantification, there is a need for pretrained models that not only provide embeddings but also transferable uncertainty estimates. To guide the development of such models, we propose the Uncertainty-aware Representation Learning (URL) benchmark. Besides the transferability of the representations, it also measures the zero-shot transferability of the uncertainty estimate using a novel metric. We apply URL to evaluate eleven uncertainty quantifiers that are pretrained on ImageNet and transferred to eight downstream datasets. We find that approaches that focus on the uncertainty of the representation itself or estimate the prediction risk directly outperform those that are based on the probabilities of upstream classes. Yet, achieving transferable uncertaint
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#23545;&#27604;&#24335;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;&#65292;&#36890;&#36807;&#25366;&#25496;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#21487;&#20197;&#25214;&#21040;&#27450;&#39575;&#20027;&#22270;&#20687;&#65292;&#36825;&#20123;&#22270;&#20687;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#25552;&#31034;&#19979;&#33021;&#27450;&#39575;CLIP&#27169;&#22411;&#65292;&#32780;&#23545;&#20154;&#31867;&#26469;&#35828;&#26159;&#26080;&#27861;&#35748;&#20986;&#30340;&#12290;&#27450;&#39575;&#20027;&#22270;&#20687;&#22312;&#23569;&#37327;&#22270;&#20687;&#26631;&#39064;&#19978;&#30340;&#35757;&#32451;&#19978;&#21487;&#33021;&#36866;&#29992;&#20110;&#26356;&#22810;&#25968;&#37327;&#30340;&#35821;&#20041;&#30456;&#20851;&#30340;&#26631;&#39064;&#12290;&#20004;&#31181;&#21487;&#33021;&#30340;&#32531;&#35299;&#31574;&#30053;&#34987;&#35780;&#20272;&#65292;&#24182;&#21457;&#29616;&#33030;&#24369;&#24615;&#19982;&#23545;&#27604;&#24335;&#39044;&#35757;&#32451;&#20013;&#30340;&#27169;&#24577;&#24046;&#36317;&#23494;&#20999;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2307.03798</link><description>&lt;p&gt;
CLIPMasterPrints: &#20351;&#29992;&#28508;&#22312;&#21464;&#37327;&#28436;&#21270;&#27450;&#39575;&#23545;&#27604;&#24335;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
CLIPMasterPrints: Fooling Contrastive Language-Image Pre-training Using Latent Variable Evolution. (arXiv:2307.03798v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#23545;&#27604;&#24335;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;&#65292;&#36890;&#36807;&#25366;&#25496;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#21487;&#20197;&#25214;&#21040;&#27450;&#39575;&#20027;&#22270;&#20687;&#65292;&#36825;&#20123;&#22270;&#20687;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#25552;&#31034;&#19979;&#33021;&#27450;&#39575;CLIP&#27169;&#22411;&#65292;&#32780;&#23545;&#20154;&#31867;&#26469;&#35828;&#26159;&#26080;&#27861;&#35748;&#20986;&#30340;&#12290;&#27450;&#39575;&#20027;&#22270;&#20687;&#22312;&#23569;&#37327;&#22270;&#20687;&#26631;&#39064;&#19978;&#30340;&#35757;&#32451;&#19978;&#21487;&#33021;&#36866;&#29992;&#20110;&#26356;&#22810;&#25968;&#37327;&#30340;&#35821;&#20041;&#30456;&#20851;&#30340;&#26631;&#39064;&#12290;&#20004;&#31181;&#21487;&#33021;&#30340;&#32531;&#35299;&#31574;&#30053;&#34987;&#35780;&#20272;&#65292;&#24182;&#21457;&#29616;&#33030;&#24369;&#24615;&#19982;&#23545;&#27604;&#24335;&#39044;&#35757;&#32451;&#20013;&#30340;&#27169;&#24577;&#24046;&#36317;&#23494;&#20999;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20197;&#23545;&#27604;&#24335;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#20026;&#20195;&#34920;&#30340;&#21516;&#26102;&#21033;&#29992;&#35270;&#35273;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#27169;&#22411;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#22810;&#21151;&#33021;&#24615;&#65292;&#20294;&#23427;&#20204;&#23545;&#20110;&#25152;&#35859;&#30340;&#27450;&#39575;&#20027;&#22270;&#20687;&#26159;&#33030;&#24369;&#30340;&#12290;&#27450;&#39575;&#20027;&#22270;&#20687;&#33021;&#22815;&#26368;&#22823;&#21270;CLIP&#27169;&#22411;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#25552;&#31034;&#19979;&#30340;&#32622;&#20449;&#24230;&#35780;&#20998;&#65292;&#21516;&#26102;&#23545;&#20110;&#20154;&#31867;&#26469;&#35828;&#26159;&#26080;&#27861;&#35748;&#20986;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;&#28436;&#21270;&#31574;&#30053;&#25110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#22312;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#25628;&#32034;&#27450;&#39575;&#20027;&#22270;&#20687;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25366;&#25496;&#20986;&#30340;&#27450;&#39575;&#20027;&#22270;&#20687;&#30340;&#29305;&#24615;&#65292;&#24182;&#21457;&#29616;&#22312;&#23569;&#37327;&#22270;&#20687;&#26631;&#39064;&#19978;&#35757;&#32451;&#30340;&#22270;&#20687;&#21487;&#33021;&#36866;&#29992;&#20110;&#26356;&#22810;&#25968;&#37327;&#30340;&#35821;&#20041;&#30456;&#20851;&#30340;&#26631;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20004;&#31181;&#21487;&#33021;&#30340;&#32531;&#35299;&#31574;&#30053;&#65292;&#24182;&#21457;&#29616;&#23545;&#27450;&#39575;&#20027;&#20363;&#23376;&#30340;&#33030;&#24369;&#24615;&#19982;&#23545;&#27604;&#24335;&#39044;&#35757;&#32451;&#20013;&#30340;&#27169;&#24577;&#24046;&#36317;&#23494;&#20999;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models leveraging both visual and textual data such as Contrastive Language-Image Pre-training (CLIP), are increasingly gaining importance. In this work, we show that despite their versatility, such models are vulnerable to what we refer to as fooling master images. Fooling master images are capable of maximizing the confidence score of a CLIP model for a significant number of widely varying prompts, while being unrecognizable for humans. We demonstrate how fooling master images can be mined by searching the latent space of generative models by means of an evolution strategy or stochastic gradient descent. We investigate the properties of the mined fooling master images, and find that images trained on a small number of image captions potentially generalize to a much larger number of semantically related captions. Further, we evaluate two possible mitigation strategies and find that vulnerability to fooling master examples is closely related to a modality gap in contrastive pre-trained
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#27874;&#26031;&#35821;Twitter&#35805;&#35821;&#30340;&#35745;&#31639;&#20998;&#26512;&#65292;&#20272;&#31639;&#20102;&#39532;&#33678;&#183;&#38463;&#31859;&#23612;&#21435;&#19990;&#21518;&#20234;&#26391;&#31038;&#20250;&#23545;&#24615;&#21035;&#24179;&#31561;&#31435;&#22330;&#30340;&#36716;&#21464;&#12290;&#30740;&#31350;&#37319;&#29992;&#21442;&#19982;&#24335;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#28041;&#21450;&#20234;&#26391;&#22899;&#24615;&#22312;&#26631;&#27880;&#36807;&#31243;&#20013;&#30340;&#31215;&#26497;&#35282;&#33394;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;&#39532;&#33678;&#183;&#38463;&#31859;&#23612;&#30340;&#27515;&#20129;&#24341;&#21457;&#20102;&#27874;&#26031;&#35821;&#35805;&#35821;&#30340;&#26497;&#21270;&#65292;&#31215;&#26497;&#25512;&#25991;&#25968;&#37327;&#30053;&#22810;&#20110;&#36127;&#38754;&#25512;&#25991;&#25968;&#37327;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#35266;&#23519;&#21040;&#25919;&#24220;&#21644;&#25239;&#35758;&#32773;&#22312;Twitter&#19978;&#30340;&#23384;&#22312;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2307.03764</link><description>&lt;p&gt;
&#23545;&#20110;&#22899;&#24615;&#32780;&#35328;&#65292;&#29983;&#27963;&#21644;&#33258;&#30001;&#65306;&#22522;&#20110;&#21442;&#19982;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#35770;&#25991;&#30740;&#31350;&#20234;&#26391;&#24615;&#21035;&#26007;&#20105;&#20013;&#30340;&#20998;&#27700;&#23725;&#26102;&#21051;
&lt;/p&gt;
&lt;p&gt;
For Women, Life, Freedom: A Participatory AI-Based Social Web Analysis of a Watershed Moment in Iran's Gender Struggles. (arXiv:2307.03764v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#27874;&#26031;&#35821;Twitter&#35805;&#35821;&#30340;&#35745;&#31639;&#20998;&#26512;&#65292;&#20272;&#31639;&#20102;&#39532;&#33678;&#183;&#38463;&#31859;&#23612;&#21435;&#19990;&#21518;&#20234;&#26391;&#31038;&#20250;&#23545;&#24615;&#21035;&#24179;&#31561;&#31435;&#22330;&#30340;&#36716;&#21464;&#12290;&#30740;&#31350;&#37319;&#29992;&#21442;&#19982;&#24335;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#28041;&#21450;&#20234;&#26391;&#22899;&#24615;&#22312;&#26631;&#27880;&#36807;&#31243;&#20013;&#30340;&#31215;&#26497;&#35282;&#33394;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;&#39532;&#33678;&#183;&#38463;&#31859;&#23612;&#30340;&#27515;&#20129;&#24341;&#21457;&#20102;&#27874;&#26031;&#35821;&#35805;&#35821;&#30340;&#26497;&#21270;&#65292;&#31215;&#26497;&#25512;&#25991;&#25968;&#37327;&#30053;&#22810;&#20110;&#36127;&#38754;&#25512;&#25991;&#25968;&#37327;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#35266;&#23519;&#21040;&#25919;&#24220;&#21644;&#25239;&#35758;&#32773;&#22312;Twitter&#19978;&#30340;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#27874;&#26031;&#35821;Twitter&#35805;&#35821;&#30340;&#35745;&#31639;&#20998;&#26512;&#65292;&#26088;&#22312;&#20272;&#31639;&#39532;&#33678;&#183;&#38463;&#31859;&#23612;&#22312;&#35686;&#26041;&#25304;&#30041;&#26399;&#38388;&#21435;&#19990;&#21518;&#23545;&#24615;&#21035;&#24179;&#31561;&#31435;&#22330;&#30340;&#36716;&#21464;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#20027;&#21160;&#23398;&#20064;&#27969;&#31243;&#26469;&#35757;&#32451;&#19968;&#20010;&#31435;&#22330;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#22312;&#20110;&#20234;&#26391;&#22899;&#24615;&#20197;&#20027;&#21160;&#30340;&#35282;&#33394;&#21442;&#19982;&#22312;&#26500;&#24314;&#36825;&#20010;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#30340;&#26631;&#27880;&#12290;&#25105;&#20204;&#30340;&#26631;&#27880;&#32773;&#19981;&#20165;&#25552;&#20379;&#26631;&#31614;&#65292;&#36824;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#20851;&#38190;&#35789;&#20197;&#36827;&#34892;&#26356;&#26377;&#24847;&#20041;&#30340;&#35821;&#26009;&#24211;&#21019;&#24314;&#65292;&#24182;&#25552;&#20379;&#31616;&#30701;&#30340;&#31034;&#20363;&#25991;&#26723;&#20197;&#36827;&#34892;&#24341;&#23548;&#24335;&#25277;&#26679;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#32467;&#26524;&#34920;&#26126;&#65292;&#39532;&#33678;&#183;&#38463;&#31859;&#23612;&#30340;&#27515;&#20129;&#24341;&#21457;&#20102;&#26497;&#21270;&#30340;&#27874;&#26031;&#35821;&#35805;&#35821;&#65292;&#23545;&#24615;&#21035;&#24179;&#31561;&#30340;&#36127;&#38754;&#21644;&#31215;&#26497;&#25512;&#25991;&#25968;&#37327;&#37117;&#22686;&#21152;&#20102;&#12290;&#31215;&#26497;&#25512;&#25991;&#30340;&#22686;&#21152;&#30053;&#22823;&#20110;&#36127;&#38754;&#25512;&#25991;&#30340;&#22686;&#21152;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#65292;&#23601;&#36134;&#21495;&#21019;&#24314;&#26102;&#38388;&#32780;&#35328;&#65292;&#19982;&#25919;&#24220;&#23545;&#40784;&#30340;Twitter&#36134;&#21495;&#21644;&#25903;&#25345;&#25239;&#35758;&#30340;Twitter&#36134;&#21495;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a computational analysis of the Persian language Twitter discourse with the aim to estimate the shift in stance toward gender equality following the death of Mahsa Amini in police custody. We present an ensemble active learning pipeline to train a stance classifier. Our novelty lies in the involvement of Iranian women in an active role as annotators in building this AI system. Our annotators not only provide labels, but they also suggest valuable keywords for more meaningful corpus creation as well as provide short example documents for a guided sampling step. Our analyses indicate that Mahsa Amini's death triggered polarized Persian language discourse where both fractions of negative and positive tweets toward gender equality increased. The increase in positive tweets was slightly greater than the increase in negative tweets. We also observe that with respect to account creation time, between the state-aligned Twitter accounts and pro-protest Twitter accounts
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#21512;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#24182;&#25351;&#20986;&#20102;&#20854;&#35780;&#20272;&#26041;&#27861;&#23384;&#22312;&#30340;&#38382;&#39064;&#65307;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#24212;&#35813;&#20855;&#22791;&#30340;&#33021;&#21147;&#65292;&#24182;&#35752;&#35770;&#20102;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#32570;&#22833;&#20043;&#22788;&#65292;&#21363;&#30693;&#19982;&#34892;&#30340;&#32479;&#19968;&#12290;</title><link>http://arxiv.org/abs/2307.03762</link><description>&lt;p&gt;
&#36142;&#23384;&#22312;&#35745;&#31639;&#26426;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#32570;&#22833;&#20043;&#22788;
&lt;/p&gt;
&lt;p&gt;
Brain in a Vat: On Missing Pieces Towards Artificial General Intelligence in Large Language Models. (arXiv:2307.03762v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03762
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#21512;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#24182;&#25351;&#20986;&#20102;&#20854;&#35780;&#20272;&#26041;&#27861;&#23384;&#22312;&#30340;&#38382;&#39064;&#65307;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#24212;&#35813;&#20855;&#22791;&#30340;&#33021;&#21147;&#65292;&#24182;&#35752;&#35770;&#20102;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#32570;&#22833;&#20043;&#22788;&#65292;&#21363;&#30693;&#19982;&#34892;&#30340;&#32479;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35266;&#28857;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#32508;&#21512;&#35780;&#20272;&#20102;&#20351;&#29992;&#26631;&#20934;&#21270;&#27979;&#35797;&#21644;&#20197;&#33021;&#21147;&#20026;&#23548;&#21521;&#30340;&#22522;&#20934;&#27979;&#35797;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#29616;&#26377;&#35780;&#20272;&#12290;&#25105;&#20204;&#25351;&#20986;&#20102;&#24403;&#21069;&#35780;&#20272;&#26041;&#27861;&#23384;&#22312;&#30340;&#20960;&#20010;&#38382;&#39064;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#22840;&#22823;&#20102;LLMs&#30340;&#33021;&#21147;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#38416;&#36848;&#20102;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#24212;&#35813;&#20855;&#22791;&#30340;&#33021;&#21147;&#65292;&#36229;&#36234;&#20102;LLMs&#30340;&#33021;&#21147;&#33539;&#22260;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26234;&#33021;&#20195;&#29702;&#30340;&#22235;&#20010;&#29305;&#24449;&#65306;1&#65289;&#20182;&#20204;&#21487;&#20197;&#25191;&#34892;&#26080;&#38480;&#30340;&#20219;&#21153;&#65307;2&#65289;&#20182;&#20204;&#21487;&#20197;&#22312;&#29305;&#23450;&#29615;&#22659;&#20013;&#29983;&#25104;&#26032;&#30340;&#20219;&#21153;&#65307;3&#65289;&#20182;&#20204;&#22522;&#20110;&#25903;&#25745;&#20219;&#21153;&#29983;&#25104;&#30340;&#20215;&#20540;&#20307;&#31995;&#36827;&#34892;&#25805;&#20316;&#65307;4&#65289;&#20182;&#20204;&#25317;&#26377;&#21453;&#26144;&#29616;&#23454;&#30340;&#19990;&#30028;&#27169;&#22411;&#65292;&#36825;&#24433;&#21709;&#30528;&#20182;&#20204;&#19982;&#19990;&#30028;&#30340;&#20114;&#21160;&#12290;&#22312;&#27492;&#35266;&#28857;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#32570;&#22833;&#20043;&#22788;&#65292;&#21363;&#30693;&#19982;&#34892;&#30340;&#32479;&#19968;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#19982;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#29289;&#20307;&#31215;&#26497;&#20114;&#21160;&#21487;&#20197;&#20026;&#24418;&#25104;&#27010;&#24565;&#24615;&#34920;&#31034;&#25552;&#20379;&#26356;&#21487;&#38752;&#30340;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this perspective paper, we first comprehensively review existing evaluations of Large Language Models (LLMs) using both standardized tests and ability-oriented benchmarks. We pinpoint several problems with current evaluation methods that tend to overstate the capabilities of LLMs. We then articulate what artificial general intelligence should encompass beyond the capabilities of LLMs. We propose four characteristics of generally intelligent agents: 1) they can perform unlimited tasks; 2) they can generate new tasks within a context; 3) they operate based on a value system that underpins task generation; and 4) they have a world model reflecting reality, which shapes their interaction with the world. Building on this viewpoint, we highlight the missing pieces in artificial general intelligence, that is, the unity of knowing and acting. We argue that active engagement with objects in the real world delivers more robust signals for forming conceptual representations. Additionally, know
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;DyGATAD&#65292;&#29992;&#20110;&#22312;&#24322;&#26500;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#21160;&#24577;&#22270;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#35782;&#21035;&#38598;&#20307;&#24322;&#24120;&#34892;&#20026;&#65292;&#20854;&#20013;&#24322;&#24120;&#34892;&#20026;&#21487;&#33021;&#30001;&#20110;&#31995;&#32479;&#20869;&#37096;&#30456;&#20114;&#20851;&#31995;&#30340;&#21464;&#21270;&#24341;&#36215;&#12290;&#36825;&#22312;&#24037;&#19994;&#29289;&#32852;&#32593;&#30417;&#25511;&#31995;&#32479;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2307.03761</link><description>&lt;p&gt;
&#24322;&#26500;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#30340;&#21160;&#24577;&#22270;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Dynamic Graph Attention for Anomaly Detection in Heterogeneous Sensor Networks. (arXiv:2307.03761v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03761
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;DyGATAD&#65292;&#29992;&#20110;&#22312;&#24322;&#26500;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#21160;&#24577;&#22270;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#35782;&#21035;&#38598;&#20307;&#24322;&#24120;&#34892;&#20026;&#65292;&#20854;&#20013;&#24322;&#24120;&#34892;&#20026;&#21487;&#33021;&#30001;&#20110;&#31995;&#32479;&#20869;&#37096;&#30456;&#20114;&#20851;&#31995;&#30340;&#21464;&#21270;&#24341;&#36215;&#12290;&#36825;&#22312;&#24037;&#19994;&#29289;&#32852;&#32593;&#30417;&#25511;&#31995;&#32479;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#21270;&#36716;&#22411;&#30340;&#26102;&#20195;&#65292;&#30001;&#24037;&#19994;&#29289;&#32852;&#32593; (IIoT) &#30417;&#25511;&#30340;&#31995;&#32479;&#36890;&#36807;&#24322;&#26500;&#20256;&#24863;&#22120;&#32593;&#32476;&#29983;&#25104;&#22823;&#37327;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015; (MTS) &#25968;&#25454;&#12290;&#34429;&#28982;&#36825;&#20123;&#25968;&#25454;&#26377;&#21161;&#20110;&#26465;&#20214;&#30417;&#25511;&#21644;&#24322;&#24120;&#26816;&#27979;&#65292;&#20294;&#26159;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#26085;&#30410;&#22797;&#26434;&#21644;&#30456;&#20114;&#20381;&#36182;&#30340;&#20851;&#31995;&#20063;&#32473;&#24322;&#24120;&#26816;&#27979;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#23613;&#31649;&#22312;&#36825;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#20027;&#35201;&#38598;&#20013;&#22312;&#28857;&#24322;&#24120;&#21644;&#32972;&#26223;&#24322;&#24120;&#65292;&#23545;&#38598;&#20307;&#24322;&#24120;&#30340;&#20851;&#27880;&#36739;&#23569;&#12290;&#38598;&#20307;&#24322;&#24120;&#30340;&#19968;&#31181;&#24120;&#35265;&#21464;&#31181;&#26159;&#24322;&#24120;&#38598;&#20307;&#34892;&#20026;&#30001;&#31995;&#32479;&#20869;&#37096;&#30340;&#30456;&#20114;&#20851;&#31995;&#21464;&#21270;&#24341;&#36215;&#12290;&#36825;&#21487;&#33021;&#26159;&#30001;&#20110;&#24322;&#24120;&#29615;&#22659;&#26465;&#20214;&#65288;&#22914;&#36807;&#28909;&#65289;&#12289;&#30001;&#20110;&#32593;&#32476;&#25915;&#20987;&#36896;&#25104;&#30340;&#19981;&#27491;&#30830;&#25805;&#20316;&#35774;&#32622;&#25110;&#31995;&#32479;&#32423;&#25925;&#38556;&#24341;&#36215;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102; DyGATAD&#65288;&#19968;&#31181;&#21160;&#24577;&#22270;&#27880;&#24847;&#21147;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65289;&#65292;&#37319;&#29992;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#30340;&#24322;&#24120;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of digital transformation, systems monitored by the Industrial Internet of Things (IIoTs) generate large amounts of Multivariate Time Series (MTS) data through heterogeneous sensor networks. While this data facilitates condition monitoring and anomaly detection, the increasing complexity and interdependencies within the sensor network pose significant challenges for anomaly detection. Despite progress in this field, much of the focus has been on point anomalies and contextual anomalies, with lesser attention paid to collective anomalies. A less addressed but common variant of collective anomalies is when the abnormal collective behavior is caused by shifts in interrelationships within the system. This can be due to abnormal environmental conditions like overheating, improper operational settings resulting from cyber-physical attacks, or system-level faults. To address these challenges, this paper proposes DyGATAD (Dynamic Graph Attention for Anomaly Detection), a graph-based
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#36848;&#20171;&#32461;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#39044;&#27979;&#12289;&#20998;&#31867;&#12289;&#24322;&#24120;&#26816;&#27979;&#21644;&#25554;&#20540;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#26174;&#24335;&#22320;&#24314;&#27169;&#26102;&#38388;&#24207;&#21015;&#21644;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20026;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20998;&#26512;&#24102;&#26469;&#20102;&#26032;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2307.03759</link><description>&lt;p&gt;
&#19968;&#39033;&#20851;&#20110;&#26102;&#38388;&#24207;&#21015;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#32508;&#36848;&#65306;&#39044;&#27979;&#12289;&#20998;&#31867;&#12289;&#25554;&#20540;&#21644;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Survey on Graph Neural Networks for Time Series: Forecasting, Classification, Imputation, and Anomaly Detection. (arXiv:2307.03759v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03759
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#36848;&#20171;&#32461;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#39044;&#27979;&#12289;&#20998;&#31867;&#12289;&#24322;&#24120;&#26816;&#27979;&#21644;&#25554;&#20540;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#26174;&#24335;&#22320;&#24314;&#27169;&#26102;&#38388;&#24207;&#21015;&#21644;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20026;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20998;&#26512;&#24102;&#26469;&#20102;&#26032;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#26159;&#35760;&#24405;&#21160;&#24577;&#31995;&#32479;&#27979;&#37327;&#32467;&#26524;&#30340;&#20027;&#35201;&#25968;&#25454;&#31867;&#22411;&#65292;&#36890;&#36807;&#29289;&#29702;&#20256;&#24863;&#22120;&#21644;&#22312;&#32447;&#36807;&#31243;&#65288;&#34394;&#25311;&#20256;&#24863;&#22120;&#65289;&#29983;&#25104;&#22823;&#37327;&#25968;&#25454;&#12290;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#23545;&#20110;&#25581;&#31034;&#21487;&#29992;&#25968;&#25454;&#20013;&#25152;&#34164;&#21547;&#30340;&#20016;&#23500;&#20449;&#24687;&#33267;&#20851;&#37325;&#35201;&#12290;&#38543;&#30528;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#22522;&#20110;GNN&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26041;&#27861;&#20063;&#22823;&#24133;&#22686;&#21152;&#12290;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#26174;&#24335;&#22320;&#24314;&#27169;&#26102;&#38388;&#24207;&#21015;&#21644;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#32780;&#20256;&#32479;&#21644;&#20854;&#20182;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#21017;&#38754;&#20020;&#22256;&#38590;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20221;&#20840;&#38754;&#30340;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#32508;&#36848;&#65288;GNN4TS&#65289;&#65292;&#21253;&#25324;&#22235;&#20010;&#22522;&#26412;&#32500;&#24230;&#65306;&#39044;&#27979;&#12289;&#20998;&#31867;&#12289;&#24322;&#24120;&#26816;&#27979;&#21644;&#25554;&#20540;&#12290;&#25105;&#20204;&#26088;&#22312;&#25351;&#23548;&#35774;&#35745;&#24072;&#21644;&#23454;&#36341;&#32773;&#20102;&#35299;&#12289;&#26500;&#24314;&#24212;&#29992;&#21644;&#25512;&#21160;GNN4TS&#30340;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#38754;&#21521;&#20219;&#21153;&#30340;GNN4TS&#20998;&#31867;&#20307;&#31995;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Time series are the primary data type used to record dynamic system measurements and generated in great volume by both physical sensors and online processes (virtual sensors). Time series analytics is therefore crucial to unlocking the wealth of information implicit in available data. With the recent advancements in graph neural networks (GNNs), there has been a surge in GNN-based approaches for time series analysis. Approaches can explicitly model inter-temporal and inter-variable relationships, which traditional and other deep neural network-based methods struggle to do. In this survey, we provide a comprehensive review of graph neural networks for time series analysis (GNN4TS), encompassing four fundamental dimensions: Forecasting, classification, anomaly detection, and imputation. Our aim is to guide designers and practitioners to understand, build applications, and advance research of GNN4TS. At first, we provide a comprehensive task-oriented taxonomy of GNN4TS. Then, we present a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#25509;&#20837;&#20013;&#30340;&#26080;&#32447;&#30005;&#36164;&#28304;&#31454;&#20105;&#26426;&#21046;&#30340;&#32593;&#32476;&#20869;&#22312;&#26041;&#27861;&#26469;&#23454;&#29616;&#20998;&#24067;&#24335;&#29992;&#25143;&#36873;&#25321;&#36827;&#34892;&#32852;&#37030;&#23398;&#20064;&#12290;&#36890;&#36807;&#25805;&#32437;&#31454;&#20105;&#31383;&#21475;&#22823;&#23567;&#21644;&#20351;&#29992;&#35757;&#32451;&#25968;&#25454;&#20559;&#24046;&#20316;&#20026;&#20248;&#20808;&#32423;&#20381;&#25454;&#65292;&#21487;&#20197;&#24555;&#36895;&#23454;&#29616;&#39640;&#25928;&#30340;&#25910;&#25947;&#65292;&#21516;&#26102;&#20445;&#35777;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.03758</link><description>&lt;p&gt;
&#22312;&#26080;&#32447;&#32593;&#32476;&#19978;&#30340;&#32852;&#37030;&#23398;&#20064;&#65306;&#36890;&#36807;&#38543;&#26426;&#25509;&#20837;&#23454;&#29616;&#20998;&#24067;&#24335;&#29992;&#25143;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Federated Learning over a Wireless Network: Distributed User Selection through Random Access. (arXiv:2307.03758v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#25509;&#20837;&#20013;&#30340;&#26080;&#32447;&#30005;&#36164;&#28304;&#31454;&#20105;&#26426;&#21046;&#30340;&#32593;&#32476;&#20869;&#22312;&#26041;&#27861;&#26469;&#23454;&#29616;&#20998;&#24067;&#24335;&#29992;&#25143;&#36873;&#25321;&#36827;&#34892;&#32852;&#37030;&#23398;&#20064;&#12290;&#36890;&#36807;&#25805;&#32437;&#31454;&#20105;&#31383;&#21475;&#22823;&#23567;&#21644;&#20351;&#29992;&#35757;&#32451;&#25968;&#25454;&#20559;&#24046;&#20316;&#20026;&#20248;&#20808;&#32423;&#20381;&#25454;&#65292;&#21487;&#20197;&#24555;&#36895;&#23454;&#29616;&#39640;&#25928;&#30340;&#25910;&#25947;&#65292;&#21516;&#26102;&#20445;&#35777;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#36873;&#25321;&#23545;&#20110;&#38477;&#20302;&#32852;&#37030;&#23398;&#20064;&#22312;&#26080;&#32447;&#32593;&#32476;&#19978;&#30340;&#36890;&#20449;&#25104;&#26412;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#38598;&#20013;&#24335;&#29992;&#25143;&#36873;&#25321;&#20250;&#24341;&#36215;&#39069;&#22806;&#30340;&#31995;&#32479;&#22797;&#26434;&#24615;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#25509;&#20837;&#20013;&#30340;&#26080;&#32447;&#30005;&#36164;&#28304;&#31454;&#20105;&#26426;&#21046;&#30340;&#32593;&#32476;&#20869;&#22312;&#26041;&#27861;&#26469;&#36827;&#34892;&#20998;&#24067;&#24335;&#29992;&#25143;&#36873;&#25321;&#12290;&#20197;&#36733;&#27874;&#24863;&#30693;&#22810;&#36335;&#35775;&#38382;&#65288;CSMA&#65289;&#26426;&#21046;&#20026;&#38543;&#26426;&#25509;&#20837;&#30340;&#20363;&#23376;&#65292;&#25105;&#20204;&#36890;&#36807;&#25805;&#32437;&#31454;&#20105;&#31383;&#21475;&#65288;CW&#65289;&#22823;&#23567;&#65292;&#22312;&#27599;&#36718;&#35757;&#32451;&#20013;&#20248;&#20808;&#20026;&#26576;&#20123;&#29992;&#25143;&#33719;&#21462;&#26080;&#32447;&#30005;&#36164;&#28304;&#12290;&#35757;&#32451;&#25968;&#25454;&#20559;&#24046;&#34987;&#29992;&#20316;&#24102;&#26377;&#29992;&#25143;&#36873;&#25321;&#30340;&#32852;&#37030;&#23398;&#20064;&#30340;&#30446;&#26631;&#22330;&#26223;&#12290;&#20248;&#20808;&#32423;&#22522;&#20110;&#26032;&#35757;&#32451;&#30340;&#23616;&#37096;&#27169;&#22411;&#19982;&#19978;&#19968;&#36718;&#30340;&#20840;&#23616;&#27169;&#22411;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#20026;&#20102;&#36991;&#20813;&#26576;&#20123;&#29992;&#25143;&#36807;&#24230;&#36129;&#29486;&#65292;&#20351;&#29992;&#35745;&#25968;&#26426;&#21046;&#26469;&#30830;&#20445;&#20844;&#24179;&#24615;&#12290;&#20351;&#29992;&#21508;&#31181;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#27169;&#25311;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#24555;&#36895;&#23454;&#29616;&#31867;&#20284;&#20110;&#38598;&#20013;&#24335;&#35757;&#32451;&#30340;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
User selection has become crucial for decreasing the communication costs of federated learning (FL) over wireless networks. However, centralized user selection causes additional system complexity. This study proposes a network intrinsic approach of distributed user selection that leverages the radio resource competition mechanism in random access. Taking the carrier sensing multiple access (CSMA) mechanism as an example of random access, we manipulate the contention window (CW) size to prioritize certain users for obtaining radio resources in each round of training. Training data bias is used as a target scenario for FL with user selection. Prioritization is based on the distance between the newly trained local model and the global model of the previous round. To avoid excessive contribution by certain users, a counting mechanism is used to ensure fairness. Simulations with various datasets demonstrate that this method can rapidly achieve convergence similar to that of the centralized 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#25968;&#25454;&#36136;&#37327;&#20445;&#35777;&#30340;&#20132;&#20114;&#24037;&#20855;QI2&#65292;&#35813;&#24037;&#20855;&#25903;&#25345;&#23545;&#22810;&#20010;&#25968;&#25454;&#36136;&#37327;&#26041;&#38754;&#30340;&#39564;&#35777;&#21644;&#23450;&#37327;&#25968;&#25454;&#36136;&#37327;&#35201;&#27714;&#30340;&#39564;&#35777;&#12290;&#36890;&#36807;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#28436;&#31034;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#24212;&#29992;&#21644;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.03419</link><description>&lt;p&gt;
QI2 -- &#19968;&#20010;&#29992;&#20110;&#25968;&#25454;&#36136;&#37327;&#20445;&#35777;&#30340;&#20132;&#20114;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
QI2 -- an Interactive Tool for Data Quality Assurance. (arXiv:2307.03419v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#25968;&#25454;&#36136;&#37327;&#20445;&#35777;&#30340;&#20132;&#20114;&#24037;&#20855;QI2&#65292;&#35813;&#24037;&#20855;&#25903;&#25345;&#23545;&#22810;&#20010;&#25968;&#25454;&#36136;&#37327;&#26041;&#38754;&#30340;&#39564;&#35777;&#21644;&#23450;&#37327;&#25968;&#25454;&#36136;&#37327;&#35201;&#27714;&#30340;&#39564;&#35777;&#12290;&#36890;&#36807;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#28436;&#31034;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#24212;&#29992;&#21644;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25968;&#25454;&#36136;&#37327;&#30340;&#37325;&#35201;&#24615;&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#21644;&#22823;&#25968;&#25454;&#30340;&#22686;&#38271;&#24433;&#21709;&#21644;&#20998;&#24067;&#32780;&#22686;&#21152;&#12290;&#27492;&#22806;&#65292;&#27431;&#27954;&#22996;&#21592;&#20250;&#35745;&#21010;&#30340;AI&#27861;&#26696;&#20026;&#25968;&#25454;&#36136;&#37327;&#23450;&#20041;&#20102;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27861;&#24459;&#35201;&#27714;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#24066;&#22330;&#25512;&#20986;&#19982;&#23433;&#20840;&#30456;&#20851;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25903;&#25345;&#22810;&#20010;&#25968;&#25454;&#36136;&#37327;&#26041;&#38754;&#30340;&#25968;&#25454;&#36136;&#37327;&#20445;&#35777;&#36807;&#31243;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#39564;&#35777;&#23450;&#37327;&#25968;&#25454;&#36136;&#37327;&#35201;&#27714;&#12290;&#36890;&#36807;&#23567;&#20363;&#23376;&#25968;&#25454;&#38598;&#20171;&#32461;&#21644;&#35299;&#37322;&#20102;&#35813;&#27010;&#24565;&#21644;&#20248;&#21183;&#12290;&#22914;&#20309;&#24212;&#29992;&#35813;&#26041;&#27861;&#22312;&#22522;&#20110;&#25163;&#20889;&#25968;&#23383;&#30340;&#30693;&#21517;MNIST&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The importance of high data quality is increasing with the growing impact and distribution of ML systems and big data. Also the planned AI Act from the European commission defines challenging legal requirements for data quality especially for the market introduction of safety relevant ML systems. In this paper we introduce a novel approach that supports the data quality assurance process of multiple data quality aspects. This approach enables the verification of quantitative data quality requirements. The concept and benefits are introduced and explained on small example data sets. How the method is applied is demonstrated on the well known MNIST data set based an handwritten digits.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22270;&#23398;&#20064;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#23581;&#35797;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#27969;&#31243;&#65306;&#23558;LLMs&#20316;&#20026;&#22686;&#24378;&#22120;&#36890;&#36807;&#28023;&#37327;&#30693;&#35782;&#26469;&#22686;&#24378;&#33410;&#28857;&#30340;&#25991;&#26412;&#23646;&#24615;&#65292;&#24182;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#29983;&#25104;&#39044;&#27979;&#65292;&#20197;&#21450;&#30452;&#25509;&#20351;&#29992;LLMs&#20316;&#20026;&#29420;&#31435;&#30340;&#39044;&#27979;&#22120;&#12290;</title><link>http://arxiv.org/abs/2307.03393</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22270;&#23398;&#20064;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs. (arXiv:2307.03393v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22270;&#23398;&#20064;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#23581;&#35797;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#27969;&#31243;&#65306;&#23558;LLMs&#20316;&#20026;&#22686;&#24378;&#22120;&#36890;&#36807;&#28023;&#37327;&#30693;&#35782;&#26469;&#22686;&#24378;&#33410;&#28857;&#30340;&#25991;&#26412;&#23646;&#24615;&#65292;&#24182;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#29983;&#25104;&#39044;&#27979;&#65292;&#20197;&#21450;&#30452;&#25509;&#20351;&#29992;LLMs&#20316;&#20026;&#29420;&#31435;&#30340;&#39044;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23398;&#20064;&#22240;&#20854;&#24191;&#27867;&#30340;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#32780;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#12290;&#20197;&#25991;&#26412;&#33410;&#28857;&#23646;&#24615;&#20026;&#20027;&#30340;&#22270;&#23398;&#20064;&#26368;&#27969;&#34892;&#30340;&#27969;&#31243;&#20027;&#35201;&#20381;&#36182;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#24182;&#21033;&#29992;&#27973;&#23618;&#25991;&#26412;&#23884;&#20837;&#20316;&#20026;&#21021;&#22987;&#33410;&#28857;&#34920;&#31034;&#65292;&#20294;&#23384;&#22312;&#36890;&#29992;&#30693;&#35782;&#21644;&#28145;&#21051;&#35821;&#20041;&#29702;&#35299;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;&#36817;&#24180;&#26469;&#65292;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#35777;&#26126;&#20855;&#26377;&#24191;&#27867;&#30340;&#24120;&#35782;&#21644;&#24378;&#22823;&#30340;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#65292;&#24050;&#32463;&#39072;&#35206;&#20102;&#29616;&#26377;&#30340;&#22788;&#29702;&#25991;&#26412;&#25968;&#25454;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25506;&#32034;LLMs&#22312;&#22270;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#30740;&#31350;&#20004;&#31181;&#21487;&#33021;&#30340;&#27969;&#31243;&#65306;LLMs&#20316;&#20026;&#22686;&#24378;&#22120;&#21644;LLMs&#20316;&#20026;&#39044;&#27979;&#22120;&#12290;&#21069;&#32773;&#21033;&#29992;LLMs&#36890;&#36807;&#20854;&#28023;&#37327;&#30693;&#35782;&#22686;&#24378;&#33410;&#28857;&#30340;&#25991;&#26412;&#23646;&#24615;&#65292;&#28982;&#21518;&#36890;&#36807;GNNs&#29983;&#25104;&#39044;&#27979;&#12290;&#21518;&#32773;&#35797;&#22270;&#30452;&#25509;&#20351;&#29992;LLMs&#20316;&#20026;&#29420;&#31435;&#30340;&#39044;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning on Graphs has attracted immense attention due to its wide real-world applications. The most popular pipeline for learning on graphs with textual node attributes primarily relies on Graph Neural Networks (GNNs), and utilizes shallow text embedding as initial node representations, which has limitations in general knowledge and profound semantic understanding. In recent years, Large Language Models (LLMs) have been proven to possess extensive common knowledge and powerful semantic comprehension abilities that have revolutionized existing workflows to handle text data. In this paper, we aim to explore the potential of LLMs in graph machine learning, especially the node classification task, and investigate two possible pipelines: LLMs-as-Enhancers and LLMs-as-Predictors. The former leverages LLMs to enhance nodes' text attributes with their massive knowledge and then generate predictions through GNNs. The latter attempts to directly employ LLMs as standalone predictors. We conduct 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#35299;&#37322;&#20013;&#65292;&#27867;&#21270;&#20102;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#29702;&#35299;&#26799;&#24230;&#22270;&#30340;&#21487;&#35299;&#37322;&#32479;&#35745;&#25968;&#25454;&#65292;&#22914;&#26368;&#39640;&#21152;&#26435;&#36335;&#24452;&#21644;&#29109;&#12290;&#20316;&#32773;&#36890;&#36807;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#21644;&#24212;&#29992;&#20110;BERT&#30340;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.03056</link><description>&lt;p&gt;
&#27867;&#21270;&#21453;&#21521;&#20256;&#25773;&#29992;&#20110;&#22522;&#20110;&#26799;&#24230;&#30340;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Generalizing Backpropagation for Gradient-Based Interpretability. (arXiv:2307.03056v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#35299;&#37322;&#20013;&#65292;&#27867;&#21270;&#20102;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#29702;&#35299;&#26799;&#24230;&#22270;&#30340;&#21487;&#35299;&#37322;&#32479;&#35745;&#25968;&#25454;&#65292;&#22914;&#26368;&#39640;&#21152;&#26435;&#36335;&#24452;&#21644;&#29109;&#12290;&#20316;&#32773;&#36890;&#36807;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#21644;&#24212;&#29992;&#20110;BERT&#30340;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29992;&#20110;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27969;&#34892;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#20381;&#36182;&#20110;&#35745;&#31639;&#27169;&#22411;&#36755;&#20986;&#23545;&#36755;&#20837;&#30340;&#26799;&#24230;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#25351;&#31034;&#21738;&#20123;&#36755;&#20837;&#29305;&#24449;&#21487;&#33021;&#23545;&#27169;&#22411;&#30340;&#39044;&#27979;&#24456;&#37325;&#35201;&#65292;&#20294;&#23427;&#20204;&#23545;&#27169;&#22411;&#26412;&#36523;&#30340;&#20869;&#37096;&#24037;&#20316;&#20102;&#35299;&#29978;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#27169;&#22411;&#30340;&#26799;&#24230;&#35745;&#31639;&#26159;&#20351;&#29992;&#21322;&#29615;&#30340;&#26356;&#19968;&#33324;&#24418;&#24335;&#30340;&#29305;&#20363;&#12290;&#36825;&#31181;&#35266;&#23519;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#27867;&#21270;&#65292;&#20197;&#39640;&#25928;&#22320;&#35745;&#31639;&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#26799;&#24230;&#22270;&#30340;&#20854;&#20182;&#21487;&#35299;&#37322;&#32479;&#35745;&#25968;&#25454;&#65292;&#20363;&#22914;&#26368;&#39640;&#21152;&#26435;&#36335;&#24452;&#21644;&#29109;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#36825;&#20010;&#27867;&#21270;&#31639;&#27861;&#65292;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#23427;&#35745;&#31639;&#30340;&#32479;&#35745;&#25968;&#25454;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#30740;&#31350;BERT&#22312;&#20027;&#35859;&#25968;&#19968;&#33268;&#24615;&#20219;&#21153;&#65288;SVA&#65289;&#19978;&#30340;&#34892;&#20026;&#12290;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#27169;&#22411;&#32452;&#20214;&#19978;&#36890;&#36807;&#30340;&#26799;&#24230;&#27969;&#37327;&#21453;&#26144;&#20102;&#20854;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many popular feature-attribution methods for interpreting deep neural networks rely on computing the gradients of a model's output with respect to its inputs. While these methods can indicate which input features may be important for the model's prediction, they reveal little about the inner workings of the model itself. In this paper, we observe that the gradient computation of a model is a special case of a more general formulation using semirings. This observation allows us to generalize the backpropagation algorithm to efficiently compute other interpretable statistics about the gradient graph of a neural network, such as the highest-weighted path and entropy. We implement this generalized algorithm, evaluate it on synthetic datasets to better understand the statistics it computes, and apply it to study BERT's behavior on the subject-verb number agreement task (SVA). With this method, we (a) validate that the amount of gradient flow through a component of a model reflects its impor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#35270;&#35273;Transformer&#36827;&#34892;&#33402;&#26415;&#35748;&#35777;&#65292;&#36890;&#36807;&#23545;&#27604;&#23454;&#39564;&#35777;&#26126;EfficientNet&#22312;&#35813;&#20219;&#21153;&#20013;&#34920;&#29616;&#26368;&#20339;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#26426;&#36741;&#21161;&#33402;&#26415;&#21697;&#35748;&#35777;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.03039</link><description>&lt;p&gt;
&#20351;&#29992;&#35270;&#35273;Transformer&#36827;&#34892;&#33402;&#26415;&#35748;&#35777;
&lt;/p&gt;
&lt;p&gt;
Art Authentication with Vision Transformers. (arXiv:2307.03039v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03039
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#35270;&#35273;Transformer&#36827;&#34892;&#33402;&#26415;&#35748;&#35777;&#65292;&#36890;&#36807;&#23545;&#27604;&#23454;&#39564;&#35777;&#26126;EfficientNet&#22312;&#35813;&#20219;&#21153;&#20013;&#34920;&#29616;&#26368;&#20339;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#26426;&#36741;&#21161;&#33402;&#26415;&#21697;&#35748;&#35777;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;Transformer&#27169;&#22411;&#22312;&#35821;&#35328;&#39046;&#22495;&#21462;&#24471;&#25104;&#21151;&#21518;&#65292;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#35270;&#35273;&#20219;&#21153;&#12290;&#35270;&#35273;Transformer&#24050;&#22312;&#21253;&#25324;&#22270;&#20687;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#31561;&#20219;&#21153;&#20013;&#25512;&#21160;&#20102;&#26368;&#26032;&#25216;&#26415;&#36827;&#23637;&#12290;&#23613;&#31649;&#22823;&#37327;&#30740;&#31350;&#24050;&#35777;&#26126;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#33402;&#26415;&#24402;&#23646;&#21644;&#33402;&#26415;&#35748;&#35777;&#20219;&#21153;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20102;&#35270;&#35273;Transformer&#22312;&#33402;&#26415;&#35748;&#35777;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#24182;&#20174;&#32780;&#25552;&#39640;&#20102;&#35745;&#31639;&#26426;&#36741;&#21161;&#33402;&#26415;&#21697;&#35748;&#35777;&#30340;&#21487;&#38752;&#24615;&#12290;&#20351;&#29992;&#30001;Vincent van Gogh&#30495;&#36857;&#21644;&#20004;&#20010;&#23545;&#27604;&#25968;&#25454;&#38598;&#32452;&#25104;&#30340;&#31934;&#24515;&#32534;&#21046;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23558;Swin Transformer&#30340;&#33402;&#26415;&#35748;&#35777;&#24615;&#33021;&#19982;EfficientNet&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#36890;&#36807;&#20351;&#29992;&#21253;&#21547;&#27169;&#20223;&#21697;&#21644;&#31867;&#20284;van Gogh&#39118;&#26684;&#30011;&#23478;&#20316;&#21697;&#30340;&#26631;&#20934;&#23545;&#27604;&#38598;&#65292;&#25105;&#20204;&#21457;&#29616;EfficientNet&#22312;&#25972;&#20307;&#19978;&#21462;&#24471;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Transformers, initially developed for language, have been successfully applied to visual tasks. Vision Transformers have been shown to push the state-of-the-art in a wide range of tasks, including image classification, object detection, and semantic segmentation. While ample research has shown promising results in art attribution and art authentication tasks using Convolutional Neural Networks, this paper examines if the superiority of Vision Transformers extends to art authentication, improving, thus, the reliability of computer-based authentication of artworks. Using a carefully compiled dataset of authentic paintings by Vincent van Gogh and two contrast datasets, we compare the art authentication performances of Swin Transformers with those of EfficientNet. Using a standard contrast set containing imitations and proxies (works by painters with styles closely related to van Gogh), we find that EfficientNet achieves the best performance overall. With a contrast set th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31034;&#20363;&#30340;&#25511;&#21046;&#26041;&#27861;&#65288;IRDEC&#65289;&#65292;&#36890;&#36807;&#20869;&#22312;&#22870;&#21169;&#39537;&#21160;&#20197;&#21450;&#25506;&#32034;&#33719;&#24471;&#20808;&#21069;&#34892;&#20026;&#65292;&#24182;&#19982;&#31034;&#33539;&#20013;&#30340;&#20219;&#21153;&#29305;&#23450;&#34892;&#20026;&#36830;&#25509;&#65292;&#20174;&#32780;&#35299;&#20915;&#20855;&#26377;&#31232;&#30095;&#22870;&#21169;&#30340;&#20219;&#21153;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#19977;&#20010;&#23548;&#33322;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.02889</link><description>&lt;p&gt;
&#23398;&#20064;&#25506;&#32034;&#20808;&#21069;&#34892;&#20026;&#26469;&#35299;&#20915;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Learning to Solve Tasks with Exploring Prior Behaviours. (arXiv:2307.02889v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31034;&#20363;&#30340;&#25511;&#21046;&#26041;&#27861;&#65288;IRDEC&#65289;&#65292;&#36890;&#36807;&#20869;&#22312;&#22870;&#21169;&#39537;&#21160;&#20197;&#21450;&#25506;&#32034;&#33719;&#24471;&#20808;&#21069;&#34892;&#20026;&#65292;&#24182;&#19982;&#31034;&#33539;&#20013;&#30340;&#20219;&#21153;&#29305;&#23450;&#34892;&#20026;&#36830;&#25509;&#65292;&#20174;&#32780;&#35299;&#20915;&#20855;&#26377;&#31232;&#30095;&#22870;&#21169;&#30340;&#20219;&#21153;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#19977;&#20010;&#23548;&#33322;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#31034;&#33539;&#24120;&#34987;&#24191;&#27867;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#31232;&#30095;&#22870;&#21169;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#20219;&#21153;&#24448;&#24448;&#20855;&#26377;&#19982;&#31034;&#33539;&#19981;&#21516;&#30340;&#21021;&#22987;&#26465;&#20214;&#65292;&#36825;&#23601;&#38656;&#35201;&#39069;&#22806;&#30340;&#20808;&#21069;&#34892;&#20026;&#12290;&#20363;&#22914;&#65292;&#20551;&#35774;&#25105;&#20204;&#24471;&#21040;&#20102;&#8220;&#20174;&#25171;&#24320;&#25277;&#23625;&#20013;&#25343;&#21462;&#29289;&#20307;&#8221;&#30340;&#20219;&#21153;&#30340;&#31034;&#33539;&#65292;&#20294;&#22312;&#35757;&#32451;&#26102;&#25277;&#23625;&#26159;&#20851;&#38381;&#30340;&#12290;&#22914;&#26524;&#27809;&#26377;&#25484;&#25569;&#25171;&#24320;&#25277;&#23625;&#30340;&#20808;&#21069;&#34892;&#20026;&#65292;&#26426;&#22120;&#20154;&#24456;&#38590;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#22312;&#22870;&#21169;&#39537;&#21160;&#30340;&#22522;&#20110;&#31034;&#20363;&#30340;&#25511;&#21046;&#26041;&#27861;&#65288;IRDEC&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36171;&#20104;&#26234;&#33021;&#20307;&#25506;&#32034;&#21644;&#33719;&#21462;&#25152;&#38656;&#30340;&#20808;&#21069;&#34892;&#20026;&#30340;&#33021;&#21147;&#65292;&#24182;&#19982;&#31034;&#33539;&#20013;&#30340;&#20219;&#21153;&#29305;&#23450;&#34892;&#20026;&#36830;&#25509;&#65292;&#20174;&#32780;&#35299;&#20915;&#31232;&#30095;&#22870;&#21169;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#23637;&#31034;&#20808;&#21069;&#34892;&#20026;&#31034;&#33539;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19977;&#20010;&#23548;&#33322;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Demonstrations are widely used in Deep Reinforcement Learning (DRL) for facilitating solving tasks with sparse rewards. However, the tasks in real-world scenarios can often have varied initial conditions from the demonstration, which would require additional prior behaviours. For example, consider we are given the demonstration for the task of \emph{picking up an object from an open drawer}, but the drawer is closed in the training. Without acquiring the prior behaviours of opening the drawer, the robot is unlikely to solve the task. To address this, in this paper we propose an Intrinsic Rewards Driven Example-based Control \textbf{(IRDEC)}. Our method can endow agents with the ability to explore and acquire the required prior behaviours and then connect to the task-specific behaviours in the demonstration to solve sparse-reward tasks without requiring additional demonstration of the prior behaviours. The performance of our method outperforms other baselines on three navigation tasks a
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#20381;&#36182;ERM&#39044;&#35328;&#26426;&#35843;&#29992;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#21487;&#23454;&#29616;&#24773;&#20917;&#19979;&#20855;&#26377;&#26377;&#38480;&#30340;&#36951;&#25022;&#65292;&#24182;&#22312;&#19981;&#21487;&#30693;&#24773;&#20917;&#19979;&#20855;&#26377;&#20122;&#32447;&#24615;&#22686;&#38271;&#30340;&#36951;&#25022;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20379;&#20102;&#31867;&#20284;&#30340;&#32467;&#26524;&#29992;&#20110;&#38750;&#21442;&#25968;&#21338;&#24328;&#29615;&#22659;&#20013;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#21363;&#20165;&#20381;&#36182;&#26368;&#20339;&#21709;&#24212;&#39044;&#35328;&#26426;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#25910;&#25947;&#21040;&#36817;&#20284;&#26497;&#23567;-&#26497;&#22823;&#22343;&#34913;&#28857;&#12290;</title><link>http://arxiv.org/abs/2307.01689</link><description>&lt;p&gt;
&#22312;&#32447;&#23398;&#20064;&#21644;&#20351;&#29992;ERM&#39044;&#35328;&#26426;&#35299;&#20915;&#26080;&#31351;&#21338;&#24328;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Online Learning and Solving Infinite Games with an ERM Oracle. (arXiv:2307.01689v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01689
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#20381;&#36182;ERM&#39044;&#35328;&#26426;&#35843;&#29992;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#21487;&#23454;&#29616;&#24773;&#20917;&#19979;&#20855;&#26377;&#26377;&#38480;&#30340;&#36951;&#25022;&#65292;&#24182;&#22312;&#19981;&#21487;&#30693;&#24773;&#20917;&#19979;&#20855;&#26377;&#20122;&#32447;&#24615;&#22686;&#38271;&#30340;&#36951;&#25022;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20379;&#20102;&#31867;&#20284;&#30340;&#32467;&#26524;&#29992;&#20110;&#38750;&#21442;&#25968;&#21338;&#24328;&#29615;&#22659;&#20013;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#21363;&#20165;&#20381;&#36182;&#26368;&#20339;&#21709;&#24212;&#39044;&#35328;&#26426;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#25910;&#25947;&#21040;&#36817;&#20284;&#26497;&#23567;-&#26497;&#22823;&#22343;&#34913;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#22312;&#32447;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#65292;ERM&#36275;&#20197;&#36798;&#21040;&#25509;&#36817;&#26368;&#20248;&#27867;&#21270;&#35823;&#24046;&#30340;&#30446;&#26631;&#65292;&#20294;&#22312;&#22312;&#32447;&#23398;&#20064;&#29615;&#22659;&#19979;&#24182;&#38750;&#22914;&#27492;&#65292;&#36890;&#24120;&#30340;&#27010;&#24565;&#31867;&#31639;&#27861;&#20381;&#36182;&#35745;&#31639;&#25928;&#29575;&#36739;&#20302;&#30340;&#39044;&#35328;&#26426;&#65292;&#22914;&#26631;&#20934;&#26368;&#20248;&#31639;&#27861;(SOA)&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#20381;&#36182;ERM&#39044;&#35328;&#26426;&#35843;&#29992;&#30340;&#22312;&#32447;&#20108;&#20998;&#31867;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#22312;&#21487;&#23454;&#29616;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#26377;&#38480;&#30340;&#36951;&#25022;(regret)&#65292;&#22312;&#19981;&#21487;&#30693;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#20122;&#32447;&#24615;&#22686;&#38271;&#30340;&#36951;&#25022;&#12290;&#25105;&#20204;&#36890;&#36807;&#24213;&#23618;&#27010;&#24565;&#31867;&#30340;Littlestone&#21644;&#38408;&#20540;&#32500;&#24230;&#26469;&#38480;&#21046;&#36951;&#25022;&#12290;&#25105;&#20204;&#33719;&#24471;&#20102;&#31867;&#20284;&#30340;&#32467;&#26524;&#29992;&#20110;&#38750;&#21442;&#25968;&#21338;&#24328;&#65292;&#20854;&#20013;ERM&#39044;&#35328;&#26426;&#21487;&#20197;&#34987;&#29702;&#35299;&#20026;&#26368;&#20339;&#21709;&#24212;&#39044;&#35328;&#26426;&#65292;&#26681;&#25454;&#20854;&#20182;&#29609;&#23478;&#30340;&#28216;&#25103;&#21382;&#21490;&#25214;&#21040;&#19968;&#20010;&#29609;&#23478;&#30340;&#26368;&#20339;&#21709;&#24212;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20165;&#20381;&#36182;&#26368;&#20339;&#21709;&#24212;&#39044;&#35328;&#26426;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#25910;&#25947;&#21040;&#20004;&#20154;&#38646;&#21644;&#21338;&#24328;&#30340;&#36817;&#20284;&#26497;&#23567;-&#26497;&#22823;&#22343;&#34913;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
While ERM suffices to attain near-optimal generalization error in the stochastic learning setting, this is not known to be the case in the online learning setting, where algorithms for general concept classes rely on computationally inefficient oracles such as the Standard Optimal Algorithm (SOA). In this work, we propose an algorithm for online binary classification setting that relies solely on ERM oracle calls, and show that it has finite regret in the realizable setting and sublinearly growing regret in the agnostic setting. We bound the regret in terms of the Littlestone and threshold dimensions of the underlying concept class.  We obtain similar results for nonparametric games, where the ERM oracle can be interpreted as a best response oracle, finding the best response of a player to a given history of play of the other players. In this setting, we provide learning algorithms that only rely on best response oracles and converge to approximate-minimax equilibria in two-player zero
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25913;&#36827;&#27169;&#22411;&#35299;&#37322;&#30340;&#26041;&#27861;&#26469;&#32531;&#35299;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;&#65292;&#36890;&#36807;&#24341;&#23548;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#21521;&#21069;&#26223;&#38598;&#20013;&#65292;&#20174;&#32780;&#25552;&#21319;&#23545;&#20027;&#35201;&#27010;&#24565;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.01473</link><description>&lt;p&gt;
&#32531;&#35299;&#20559;&#35265;&#65306;&#36890;&#36807;&#25913;&#36827;&#27169;&#22411;&#35299;&#37322;&#26469;&#25552;&#21319;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Mitigating Bias: Enhancing Image Classification by Improving Model Explanations. (arXiv:2307.01473v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01473
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25913;&#36827;&#27169;&#22411;&#35299;&#37322;&#30340;&#26041;&#27861;&#26469;&#32531;&#35299;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;&#65292;&#36890;&#36807;&#24341;&#23548;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#21521;&#21069;&#26223;&#38598;&#20013;&#65292;&#20174;&#32780;&#25552;&#21319;&#23545;&#20027;&#35201;&#27010;&#24565;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#22797;&#26434;&#27169;&#24335;&#21644;&#27010;&#24565;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#20542;&#21521;&#20110;&#36807;&#20998;&#20381;&#36182;&#20110;&#22270;&#29255;&#32972;&#26223;&#20013;&#30340;&#31616;&#21333;&#21644;&#23481;&#26131;&#35782;&#21035;&#30340;&#29305;&#24449;&#65292;&#32780;&#19981;&#26159;&#23427;&#20204;&#26412;&#24212;&#20998;&#31867;&#30340;&#20027;&#35201;&#27010;&#24565;&#25110;&#23545;&#35937;&#12290;&#36825;&#31181;&#29616;&#35937;&#32473;&#22270;&#20687;&#20998;&#31867;&#22120;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#22240;&#20026;&#22270;&#29255;&#20013;&#30340;&#20851;&#38190;&#20803;&#32032;&#21487;&#33021;&#20250;&#34987;&#25513;&#30422;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#24182;&#25913;&#21892;&#22270;&#20687;&#20998;&#31867;&#22120;&#23545;&#20027;&#35201;&#27010;&#24565;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#22312;&#20998;&#31867;&#20219;&#21153;&#36807;&#31243;&#20013;&#21516;&#26102;&#24341;&#23548;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#21521;&#21069;&#26223;&#38598;&#20013;&#12290;&#36890;&#36807;&#24378;&#35843;&#21069;&#26223;&#65292;&#21363;&#20027;&#35201;&#24863;&#20852;&#36259;&#30340;&#23545;&#35937;&#65292;&#25105;&#20204;&#26088;&#22312;&#23558;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#20174;&#32972;&#26223;&#30340;&#20027;&#23548;&#24433;&#21709;&#19978;&#36716;&#31227;&#24320;&#26469;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26426;&#21046;&#26469;&#40723;&#21169;&#27169;&#22411;&#36275;&#22815;&#22320;&#20998;&#37197;&#27880;&#24847;&#21147;&#32473;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models have demonstrated remarkable capabilities in learning complex patterns and concepts from training data. However, recent findings indicate that these models tend to rely heavily on simple and easily discernible features present in the background of images rather than the main concepts or objects they are intended to classify. This phenomenon poses a challenge to image classifiers as the crucial elements of interest in images may be overshadowed. In this paper, we propose a novel approach to address this issue and improve the learning of main concepts by image classifiers. Our central idea revolves around concurrently guiding the model's attention toward the foreground during the classification task. By emphasizing the foreground, which encapsulates the primary objects of interest, we aim to shift the focus of the model away from the dominant influence of the background. To accomplish this, we introduce a mechanism that encourages the model to allocate sufficient att
&lt;/p&gt;</description></item><item><title>vONTSS&#26159;&#19968;&#31181;&#22522;&#20110;vMF&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#21322;&#30417;&#30563;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#23427;&#22312;&#20998;&#31867;&#20934;&#30830;&#29575;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#19988;&#25903;&#25345;&#26080;&#30417;&#30563;&#20027;&#39064;&#24314;&#27169;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;vONTSS&#27604;&#26368;&#36817;&#30340;NTM&#26356;&#24555;&#12290;</title><link>http://arxiv.org/abs/2307.01226</link><description>&lt;p&gt;
vONTSS&#65306;&#22522;&#20110;vMF&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#21322;&#30417;&#30563;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
vONTSS: vMF based semi-supervised neural topic modeling with optimal transport. (arXiv:2307.01226v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01226
&lt;/p&gt;
&lt;p&gt;
vONTSS&#26159;&#19968;&#31181;&#22522;&#20110;vMF&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#21322;&#30417;&#30563;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#23427;&#22312;&#20998;&#31867;&#20934;&#30830;&#29575;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#19988;&#25903;&#25345;&#26080;&#30417;&#30563;&#20027;&#39064;&#24314;&#27169;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;vONTSS&#27604;&#26368;&#36817;&#30340;NTM&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21463;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21551;&#21457;&#30340;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65288;NTM&#65289;&#24341;&#36215;&#20102;&#24456;&#22810;&#30740;&#31350;&#20852;&#36259;&#65292;&#28982;&#32780;&#65292;&#30001;&#20110;&#25972;&#21512;&#20154;&#31867;&#30693;&#35782;&#30340;&#25361;&#25112;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;vONTSS&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;von Mises-Fisher&#65288;vMF&#65289;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21644;&#26368;&#20248;&#20256;&#36755;&#12290;&#22312;&#21322;&#30417;&#30563;&#35774;&#32622;&#20013;&#65292;&#24403;&#25552;&#20379;&#27599;&#20010;&#20027;&#39064;&#30340;&#23569;&#37327;&#20851;&#38190;&#35789;&#26102;&#65292;vONTSS&#29983;&#25104;&#28508;&#22312;&#20027;&#39064;&#24182;&#20248;&#21270;&#20027;&#39064;-&#20851;&#38190;&#35789;&#36136;&#37327;&#21644;&#20027;&#39064;&#20998;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;vONTSS&#22312;&#20998;&#31867;&#20934;&#30830;&#29575;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#21322;&#30417;&#30563;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#12290;vONTSS&#36824;&#25903;&#25345;&#26080;&#30417;&#30563;&#20027;&#39064;&#24314;&#27169;&#12290;&#23450;&#37327;&#21644;&#23450;&#24615;&#23454;&#39564;&#35777;&#26126;&#65292;vONTSS&#22312;&#26080;&#30417;&#30563;&#35774;&#32622;&#19979;&#22312;&#22810;&#20010;&#26041;&#38754;&#20248;&#20110;&#26368;&#36817;&#30340;NTM&#65306;vONTSS&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21457;&#29616;&#39640;&#24230;&#32858;&#31867;&#21644;&#36830;&#36143;&#30340;&#20027;&#39064;&#12290;&#23427;&#20063;&#27604;&#29616;&#26377;-&#25163;&#27861;&#24555;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Neural Topic Models (NTM), inspired by variational autoencoders, have attracted a lot of research interest; however, these methods have limited applications in the real world due to the challenge of incorporating human knowledge. This work presents a semi-supervised neural topic modeling method, vONTSS, which uses von Mises-Fisher (vMF) based variational autoencoders and optimal transport. When a few keywords per topic are provided, vONTSS in the semi-supervised setting generates potential topics and optimizes topic-keyword quality and topic classification. Experiments show that vONTSS outperforms existing semi-supervised topic modeling methods in classification accuracy and diversity. vONTSS also supports unsupervised topic modeling. Quantitative and qualitative experiments show that vONTSS in the unsupervised setting outperforms recent NTMs on multiple aspects: vONTSS discovers highly clustered and coherent topics on benchmark datasets. It is also much faster than the state
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#26032;&#25512;&#33616;&#31995;&#32479;&#26041;&#27861;GenRec&#65292;&#36890;&#36807;&#30452;&#25509;&#29983;&#25104;&#30446;&#26631;&#25512;&#33616;&#39033;&#32780;&#19981;&#26159;&#35745;&#31639;&#25490;&#21517;&#20998;&#25968;&#65292;&#21033;&#29992;LLM&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#29702;&#35299;&#33021;&#21147;&#26469;&#29983;&#25104;&#30456;&#20851;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2307.00457</link><description>&lt;p&gt;
GenRec:&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#24335;&#25512;&#33616;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
GenRec: Large Language Model for Generative Recommendation. (arXiv:2307.00457v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#26032;&#25512;&#33616;&#31995;&#32479;&#26041;&#27861;GenRec&#65292;&#36890;&#36807;&#30452;&#25509;&#29983;&#25104;&#30446;&#26631;&#25512;&#33616;&#39033;&#32780;&#19981;&#26159;&#35745;&#31639;&#25490;&#21517;&#20998;&#25968;&#65292;&#21033;&#29992;LLM&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#29702;&#35299;&#33021;&#21147;&#26469;&#29983;&#25104;&#30456;&#20851;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(Large Language Model&#65292;LLM)&#24050;&#32463;&#25104;&#20026;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#22312;&#29983;&#25104;&#24335;&#25512;&#33616;&#33539;&#24335;&#19979;&#65292;&#23427;&#20204;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#28508;&#21147;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#25991;&#26412;&#25968;&#25454;&#30340;&#25512;&#33616;&#31995;&#32479;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#26469;&#36827;&#34892;&#25512;&#33616;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#33616;&#31995;&#32479;(GenRec)&#65292;&#35813;&#31995;&#32479;&#21033;&#29992;LLM&#30340;&#34920;&#36798;&#33021;&#21147;&#30452;&#25509;&#29983;&#25104;&#30446;&#26631;&#25512;&#33616;&#39033;&#65292;&#32780;&#19981;&#26159;&#20687;&#20256;&#32479;&#30340;&#21028;&#21035;&#24335;&#25512;&#33616;&#31995;&#32479;&#19968;&#26679;&#36880;&#20010;&#35745;&#31639;&#27599;&#20010;&#20505;&#36873;&#39033;&#30340;&#25490;&#21517;&#20998;&#25968;&#12290;GenRec&#21033;&#29992;LLM&#30340;&#29702;&#35299;&#33021;&#21147;&#26469;&#35299;&#37322;&#19978;&#19979;&#25991;&#12289;&#23398;&#20064;&#29992;&#25143;&#20559;&#22909;&#24182;&#29983;&#25104;&#30456;&#20851;&#25512;&#33616;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#32534;&#30721;&#30340;&#20016;&#23500;&#30693;&#35782;&#26469;&#23436;&#25104;&#25512;&#33616;&#20219;&#21153;&#12290;&#25105;&#20204;&#39318;&#20808;&#21046;&#23450;&#20102;&#19987;&#38376;&#30340;&#25552;&#31034;&#65292;&#20197;&#22686;&#24378;LLM&#29702;&#35299;&#25512;&#33616;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, large language models (LLM) have emerged as powerful tools for diverse natural language processing tasks. However, their potential for recommender systems under the generative recommendation paradigm remains relatively unexplored. This paper presents an innovative approach to recommendation systems using large language models (LLMs) based on text data. In this paper, we present a novel LLM for generative recommendation (GenRec) that utilized the expressive power of LLM to directly generate the target item to recommend, rather than calculating ranking score for each candidate item one by one as in traditional discriminative recommendation. GenRec uses LLM's understanding ability to interpret context, learn user preferences, and generate relevant recommendation. Our proposed approach leverages the vast knowledge encoded in large language models to accomplish recommendation tasks. We first we formulate specialized prompts to enhance the ability of LLM to comprehend recomm
&lt;/p&gt;</description></item><item><title>&#36793;&#32536;&#20113;&#35745;&#31639;&#19979;&#35268;&#27169;&#21270;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#21644;&#25361;&#25112;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#35752;&#35770;&#20102;&#21033;&#29992;&#36793;&#32536;&#20113;&#35745;&#31639;&#33539;&#24335;&#26500;&#24314;GenAI&#31995;&#32479;&#30340;&#21560;&#24341;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.17170</link><description>&lt;p&gt;
&#36793;&#32536;&#20113;&#35745;&#31639;&#19979;&#35268;&#27169;&#21270;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
An Overview on Generative AI at Scale with Edge-Cloud Computing. (arXiv:2306.17170v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17170
&lt;/p&gt;
&lt;p&gt;
&#36793;&#32536;&#20113;&#35745;&#31639;&#19979;&#35268;&#27169;&#21270;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#21644;&#25361;&#25112;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#35752;&#35770;&#20102;&#21033;&#29992;&#36793;&#32536;&#20113;&#35745;&#31639;&#33539;&#24335;&#26500;&#24314;GenAI&#31995;&#32479;&#30340;&#21560;&#24341;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#20010;&#29305;&#23450;&#31867;&#21035;&#65292;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#29983;&#25104;&#31867;&#20284;&#20110;&#20154;&#31867;&#21019;&#36896;&#30340;&#26032;&#20869;&#23481;&#12290;GenAI&#31995;&#32479;&#30340;&#24555;&#36895;&#21457;&#23637;&#24050;&#32463;&#22312;&#20114;&#32852;&#32593;&#19978;&#20135;&#29983;&#20102;&#22823;&#37327;&#30340;&#26032;&#25968;&#25454;&#65292;&#32473;&#24403;&#21069;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#26694;&#26550;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#30446;&#21069;&#65292;&#30001;&#20110;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;GenAI&#26381;&#21153;&#20381;&#36182;&#20110;&#20256;&#32479;&#30340;&#20113;&#35745;&#31639;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#20256;&#36755;&#21644;&#22823;&#37327;&#35831;&#27714;&#65292;&#36825;&#31181;&#26381;&#21153;&#23558;&#36935;&#21040;&#39640;&#24310;&#36831;&#30340;&#38382;&#39064;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#36890;&#36807;&#36793;&#32536;&#21644;&#20113;&#20043;&#38388;&#30340;&#21327;&#21516;&#65292;&#36793;&#32536;&#20113;&#35745;&#31639;&#21487;&#20197;&#25552;&#20379;&#36275;&#22815;&#30340;&#35745;&#31639;&#33021;&#21147;&#21644;&#20302;&#24310;&#36831;&#12290;&#22240;&#27492;&#65292;&#22312;&#36793;&#32536;&#20113;&#35745;&#31639;&#33539;&#24335;&#30340;&#25903;&#25345;&#19979;&#26500;&#24314;&#35268;&#27169;&#21270;&#30340;GenAI&#31995;&#32479;&#20855;&#26377;&#21560;&#24341;&#21147;&#12290;&#22312;&#26412;&#32508;&#36848;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#21035;&#22238;&#39038;&#20102;GenAI&#21644;&#36793;&#32536;&#20113;&#35745;&#31639;&#30340;&#26368;&#26032;&#21457;&#23637;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#31034;&#20363;&#24615;&#30340;GenAI&#24212;&#29992;&#26469;&#35752;&#35770;&#25216;&#26415;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a specific category of artificial intelligence (AI), generative artificial intelligence (GenAI) generates new content that resembles what is created by humans. The rapid development of GenAI systems has created a huge amount of new data on the Internet, posing new challenges to current computing and communication frameworks. Currently, GenAI services rely on the traditional cloud computing framework due to the need for large computation resources. However, such services will encounter high latency because of data transmission and a high volume of requests. On the other hand, edge-cloud computing can provide adequate computation power and low latency at the same time through the collaboration between edges and the cloud. Thus, it is attractive to build GenAI systems at scale by leveraging the edge-cloud computing paradigm. In this overview paper, we review recent developments in GenAI and edge-cloud computing, respectively. Then, we use two exemplary GenAI applications to discuss tec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#24037;&#20855;&#30340;&#21151;&#33021;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;&#26816;&#27979;&#24037;&#20855;&#26082;&#19981;&#20934;&#30830;&#20063;&#19981;&#21487;&#38752;&#12290;</title><link>http://arxiv.org/abs/2306.15666</link><description>&lt;p&gt;
AI-&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24037;&#20855;&#30340;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Testing of Detection Tools for AI-Generated Text. (arXiv:2306.15666v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#24037;&#20855;&#30340;&#21151;&#33021;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;&#26816;&#27979;&#24037;&#20855;&#26082;&#19981;&#20934;&#30830;&#20063;&#19981;&#21487;&#38752;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#24418;&#22120;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#24378;&#35843;&#20102;&#22312;&#23398;&#26415;&#29615;&#22659;&#20013;&#19981;&#20844;&#24179;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#24182;&#21152;&#24378;&#20102;&#23547;&#25214;&#26816;&#27979;&#27492;&#31867;&#20869;&#23481;&#35299;&#20915;&#26041;&#26696;&#30340;&#21162;&#21147;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#24037;&#20855;&#30340;&#19968;&#33324;&#21151;&#33021;&#65292;&#24182;&#26681;&#25454;&#20934;&#30830;&#24615;&#21644;&#38169;&#35823;&#31867;&#22411;&#20998;&#26512;&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#30740;&#31350;&#26088;&#22312;&#22238;&#31572;&#20197;&#19979;&#30740;&#31350;&#38382;&#39064;&#65306;&#29616;&#26377;&#30340;&#26816;&#27979;&#24037;&#20855;&#26159;&#21542;&#33021;&#21487;&#38752;&#22320;&#21306;&#20998;&#20154;&#31867;&#32534;&#20889;&#30340;&#25991;&#26412;&#21644;ChatGPT&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#20197;&#21450;&#26426;&#22120;&#32763;&#35793;&#21644;&#20869;&#23481;&#28151;&#28102;&#25216;&#26415;&#26159;&#21542;&#24433;&#21709;&#23545;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#12290;&#30740;&#31350;&#28085;&#30422;&#20102;12&#31181;&#20844;&#24320;&#21487;&#29992;&#30340;&#24037;&#20855;&#21644;&#20004;&#20010;&#24191;&#27867;&#24212;&#29992;&#20110;&#23398;&#26415;&#29615;&#22659;&#20013;&#30340;&#21830;&#19994;&#31995;&#32479;&#65288;Turnitin&#21644;PlagiarismCheck&#65289;&#12290;&#30740;&#31350;&#20154;&#21592;&#24471;&#20986;&#32467;&#35770;&#65292;&#29616;&#26377;&#30340;&#26816;&#27979;&#24037;&#20855;&#26082;&#19981;&#20934;&#30830;&#20063;&#19981;&#21487;&#38752;&#65292;&#24182;&#19988;&#23384;&#22312;&#20027;&#35201;&#30340;&#21487;&#36776;&#35782;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in generative pre-trained transformer large language models have emphasised the potential risks of unfair use of artificial intelligence (AI) generated content in an academic environment and intensified efforts in searching for solutions to detect such content. The paper examines the general functionality of detection tools for artificial intelligence generated text and evaluates them based on accuracy and error type analysis. Specifically, the study seeks to answer research questions about whether existing detection tools can reliably differentiate between human-written text and ChatGPT-generated text, and whether machine translation and content obfuscation techniques affect the detection of AIgenerated text. The research covers 12 publicly available tools and two commercial systems (Turnitin and PlagiarismCheck) that are widely used in the academic setting. The researchers conclude that the available detection tools are neither accurate nor reliable and have a main bi
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#22797;&#26434;&#24230;&#20026;$O(\log(n))$&#30340;&#20108;&#27425;&#35268;&#21010;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#20005;&#26684;&#30340;&#29702;&#35770;&#35777;&#26126;&#39564;&#35777;&#20102;&#35813;&#31639;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;&#36825;&#19968;&#37325;&#22823;&#31361;&#30772;&#20351;&#24471;&#25105;&#20204;&#20174;$O(\sqrt{n})$&#30340;&#20248;&#21270;&#31639;&#27861;&#36807;&#28193;&#21040;$O(\log(n))$&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#20854;&#22312;&#22823;&#25968;&#25454;&#21644;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.15079</link><description>&lt;p&gt;
&#20174;$O(\sqrt{n})$&#21040;$O(\log(n))$&#30340;&#20108;&#27425;&#35268;&#21010;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
From $O(\sqrt n)$ to $O(\log n)$ in Quadratic Programming. (arXiv:2306.15079v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15079
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#22797;&#26434;&#24230;&#20026;$O(\log(n))$&#30340;&#20108;&#27425;&#35268;&#21010;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#20005;&#26684;&#30340;&#29702;&#35770;&#35777;&#26126;&#39564;&#35777;&#20102;&#35813;&#31639;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;&#36825;&#19968;&#37325;&#22823;&#31361;&#30772;&#20351;&#24471;&#25105;&#20204;&#20174;$O(\sqrt{n})$&#30340;&#20248;&#21270;&#31639;&#27861;&#36807;&#28193;&#21040;$O(\log(n))$&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#20854;&#22312;&#22823;&#25968;&#25454;&#21644;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#24180;&#26469;&#65292;&#25968;&#20540;&#20248;&#21270;&#29702;&#35770;&#19968;&#30452;&#23384;&#22312;&#19968;&#20010;&#22256;&#25200;&#65292;&#21363;&#26159;&#21542;&#23384;&#22312;&#19968;&#20010;&#36845;&#20195;&#22797;&#26434;&#24230;&#20026;$O(\log(n))$&#30340;&#20248;&#21270;&#31639;&#27861;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#20840;&#26032;&#30340;&#20248;&#21270;&#31639;&#27861;&#21644;&#20005;&#26684;&#30340;&#29702;&#35770;&#35777;&#26126;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#20197;&#26377;&#30028;&#30418;&#20108;&#27425;&#35268;&#21010;&#38382;&#39064;&#65288;Box-QP&#65289;&#20026;&#36215;&#28857;&#65292;&#35768;&#22810;&#23454;&#38469;&#20248;&#21270;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#23545;&#20598;&#29702;&#35770;&#36716;&#21270;&#20026;Box-QP&#38382;&#39064;&#12290;&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#20010;&#36845;&#20195;&#22797;&#26434;&#24230;&#20026;$O(\log(n))$&#30340;QP&#31639;&#27861;&#65292;&#23588;&#20854;&#26159;&#20854;&#34920;&#29616;&#31867;&#20284;&#20110;&#8220;&#30452;&#25509;&#8221;&#26041;&#27861;&#65306;&#25152;&#38656;&#36845;&#20195;&#27425;&#25968;&#26159;&#30830;&#23450;&#24615;&#30340;&#65292;&#31934;&#30830;&#20540;&#20026;$\left\lceil\log\left(\frac{3.125n}{\epsilon}\right)/\log(1.5625)\right\rceil$&#12290;&#36825;&#19968;&#37325;&#22823;&#31361;&#30772;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#20174;$O(\sqrt{n})$&#30340;&#20248;&#21270;&#31639;&#27861;&#36807;&#28193;&#21040;$O(\log(n))$&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#20854;&#20986;&#33394;&#30340;&#21487;&#25193;&#23637;&#24615;&#22312;&#24403;&#20170;&#30340;&#22823;&#25968;&#25454;&#21644;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#23588;&#20026;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
A "dark cloud" hangs over numerical optimization theory for decades, namely, whether an optimization algorithm $O(\log(n))$ iteration complexity exists. "Yes", this paper answers, with a new optimization algorithm and strict theory proof. It starts with box-constrained quadratic programming (Box-QP), and many practical optimization problems fall into Box-QP. Smooth quadratic programming (QP) and nonsmooth Lasso can be reformulated as Box-QP via duality theory. It is the first time to present an $O(\log(n))$ iteration complexity QP algorithm, in particular, which behaves like a "direct" method: the required number of iterations is deterministic with exact value $\left\lceil\log\left(\frac{3.125n}{\epsilon}\right)/\log(1.5625)\right\rceil$. This significant breakthrough enables us to transition from the $O(\sqrt{n})$ to the $O(\log(n))$ optimization algorithm, whose amazing scalability is particularly relevant in today's era of big data and artificial intelligence.
&lt;/p&gt;</description></item><item><title>Diff-TTSG&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#29992;&#20110;&#32852;&#21512;&#23398;&#20064;&#21512;&#25104;&#35821;&#38899;&#21644;&#25163;&#21183;&#65292;&#30456;&#27604;&#20110;&#20808;&#21069;&#26368;&#26032;&#25216;&#26415;&#30340;&#38750;&#27010;&#29575;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#20154;&#31867;&#35762;&#35805;&#21644;&#36816;&#21160;&#30340;&#21464;&#21270;&#65292;&#20135;&#29983;&#26356;&#36924;&#30495;&#21644;&#22810;&#26679;&#21270;&#30340;&#38598;&#25104;&#35821;&#38899;&#21644;&#25163;&#21183;&#21512;&#25104;&#12290;</title><link>http://arxiv.org/abs/2306.09417</link><description>&lt;p&gt;
Diff-TTSG: &#21435;&#22122;&#27010;&#29575;&#38598;&#25104;&#35821;&#38899;&#21644;&#25163;&#21183;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Diff-TTSG: Denoising probabilistic integrated speech and gesture synthesis. (arXiv:2306.09417v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09417
&lt;/p&gt;
&lt;p&gt;
Diff-TTSG&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#29992;&#20110;&#32852;&#21512;&#23398;&#20064;&#21512;&#25104;&#35821;&#38899;&#21644;&#25163;&#21183;&#65292;&#30456;&#27604;&#20110;&#20808;&#21069;&#26368;&#26032;&#25216;&#26415;&#30340;&#38750;&#27010;&#29575;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#20154;&#31867;&#35762;&#35805;&#21644;&#36816;&#21160;&#30340;&#21464;&#21270;&#65292;&#20135;&#29983;&#26356;&#36924;&#30495;&#21644;&#22810;&#26679;&#21270;&#30340;&#38598;&#25104;&#35821;&#38899;&#21644;&#25163;&#21183;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26391;&#35835;&#35821;&#38899;&#21512;&#25104;&#23454;&#29616;&#39640;&#33258;&#28982;&#24230;&#35780;&#20998;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#24320;&#22987;&#20851;&#27880;&#21512;&#25104;&#33258;&#28982;&#35328;&#35821;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#38754;&#23545;&#38754;&#30340;&#33258;&#21457;&#23545;&#35805;&#26082;&#26377;&#21475;&#22836;&#30340;&#65292;&#20063;&#26377;&#38750;&#35821;&#35328;&#30340;&#65288;&#20363;&#22914;&#65292;&#20849;&#21516;&#35328;&#35821;&#25163;&#21183;&#65289;&#12290;&#26368;&#36817;&#25165;&#24320;&#22987;&#30740;&#31350;&#32852;&#21512;&#21512;&#25104;&#36825;&#20004;&#31181;&#27169;&#24577;&#22312;&#19968;&#20010;&#21333;&#19968;&#30340;&#31995;&#32479;&#20013;&#30340;&#22909;&#22788;&#12290;&#20808;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#20351;&#29992;&#38750;&#27010;&#29575;&#26041;&#27861;&#65292;&#26080;&#27861;&#25429;&#25417;&#20154;&#31867;&#35762;&#35805;&#21644;&#36816;&#21160;&#30340;&#21464;&#21270;&#65292;&#24182;&#21487;&#33021;&#20135;&#29983;&#36807;&#24230;&#24179;&#28369;&#30340;&#20266;&#24433;&#21644;&#27425;&#20248;&#30340;&#21512;&#25104;&#36136;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#31216;&#20026; Diff-TTSG&#65292;&#20849;&#21516;&#23398;&#20064;&#21512;&#25104;&#35821;&#38899;&#21644;&#25163;&#21183;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20174;&#22836;&#24320;&#22987;&#20351;&#29992;&#23567;&#22411;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#32452;&#23567;&#24515;&#30340;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#20027;&#35266;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#38598;&#25104;&#35821;&#38899;&#21644;&#25163;&#21183;&#21512;&#25104;&#31995;&#32479;&#65292;&#24182;&#29992;&#23427;&#20204;&#26469;&#39564;&#35777;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#23545;&#20110;&#21512;&#25104;&#30340;&#26679;&#20363;&#32780;&#35328;&#65292;Diff-TTSG&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#20135;&#29983;&#26356;&#36924;&#30495;&#21644;&#22810;&#26679;&#21270;&#30340;&#38598;&#25104;&#35821;&#38899;&#21644;&#25163;&#21183;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
With read-aloud speech synthesis achieving high naturalness scores, there is a growing research interest in synthesising spontaneous speech. However, human spontaneous face-to-face conversation has both spoken and non-verbal aspects (here, co-speech gestures). Only recently has research begun to explore the benefits of jointly synthesising these two modalities in a single system. The previous state of the art used non-probabilistic methods, which fail to capture the variability of human speech and motion, and risk producing oversmoothing artefacts and sub-optimal synthesis quality. We present the first diffusion-based probabilistic model, called Diff-TTSG, that jointly learns to synthesise speech and gestures together. Our method can be trained on small datasets from scratch. Furthermore, we describe a set of careful uni- and multi-modal subjective tests for evaluating integrated speech and gesture synthesis systems, and use them to validate our proposed approach. For synthesised examp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#21160;&#20316;&#39118;&#26684;&#36716;&#31227;&#30340;&#26679;&#24335;&#22810;&#26679;&#24615;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#24037;&#19994;&#26631;&#20934;&#30340;&#20154;&#20307;&#39592;&#39612;&#32467;&#26500;&#65292;&#21487;&#30452;&#25509;&#24212;&#29992;&#20110;3D&#35282;&#33394;&#39033;&#30446;&#20013;&#12290;&#35813;&#25968;&#25454;&#38598;&#30340;&#26377;&#25928;&#24615;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#24182;&#25552;&#20986;&#20102;&#21160;&#20316;&#39118;&#26684;&#36716;&#31227;&#39046;&#22495;&#30340;&#25361;&#25112;&#19982;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.08861</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#22522;&#20110;AI&#30340;&#21160;&#20316;&#32534;&#36753;&#19982;&#39118;&#26684;&#21270;&#30340;&#23454;&#29992;&#21160;&#20316;&#25429;&#25417;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Motion Capture Dataset for Practical Use of AI-based Motion Editing and Stylization. (arXiv:2306.08861v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08861
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#21160;&#20316;&#39118;&#26684;&#36716;&#31227;&#30340;&#26679;&#24335;&#22810;&#26679;&#24615;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#24037;&#19994;&#26631;&#20934;&#30340;&#20154;&#20307;&#39592;&#39612;&#32467;&#26500;&#65292;&#21487;&#30452;&#25509;&#24212;&#29992;&#20110;3D&#35282;&#33394;&#39033;&#30446;&#20013;&#12290;&#35813;&#25968;&#25454;&#38598;&#30340;&#26377;&#25928;&#24615;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#24182;&#25552;&#20986;&#20102;&#21160;&#20316;&#39118;&#26684;&#36716;&#31227;&#39046;&#22495;&#30340;&#25361;&#25112;&#19982;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26679;&#24335;&#22810;&#26679;&#24615;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#21160;&#20316;&#39118;&#26684;&#36716;&#31227;&#39046;&#22495;&#12290;&#35813;&#21160;&#20316;&#25968;&#25454;&#38598;&#20351;&#29992;&#20102;&#24037;&#19994;&#26631;&#20934;&#30340;&#20154;&#20307;&#39592;&#39612;&#32467;&#26500;&#65292;&#22240;&#27492;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#35768;&#22810;&#39033;&#30446;&#20013;&#30340;3D&#35282;&#33394;&#12290;&#25105;&#20204;&#36890;&#36807;&#21521;&#20844;&#20247;&#21644;&#24066;&#22330;&#21457;&#24067;&#36825;&#20010;&#25552;&#35758;&#30340;&#21160;&#20316;&#25968;&#25454;&#38598;&#65292;&#26469;&#34920;&#26126;&#21160;&#20316;&#39118;&#26684;&#36716;&#31227;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#40723;&#21169;&#26410;&#26469;&#22312;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#24037;&#20316;&#12290;&#25105;&#20204;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#23545;&#21160;&#20316;&#39118;&#26684;&#36716;&#31227;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#20102;&#25152;&#25552;&#35758;&#30340;&#25968;&#25454;&#38598;&#22312;&#21160;&#20316;&#39118;&#26684;&#36716;&#31227;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we proposed a new style-diverse dataset for the domain of motion style transfer. The motion dataset uses an industrial-standard human bone structure and thus is industry-ready to be plugged into 3D characters for many projects. We claim the challenges in motion style transfer and encourage future work in this domain by releasing the proposed motion dataset both to the public and the market. We conduct a comprehensive study on motion style transfer in the experiment using the state-of-the-art method, and the results show the proposed dataset's validity for the motion style transfer task.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#32508;&#36848;&#20102;&#22312;&#32467;&#26500;&#21270;&#25968;&#25454;&#39046;&#22495;&#20013;&#26368;&#36817;&#25552;&#20986;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#20171;&#32461;&#20102;&#20854;&#29702;&#35770;&#22522;&#30784;&#21644;&#24212;&#29992;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2306.04139</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#25968;&#25454;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Generative Diffusion Models for Structured Data. (arXiv:2306.04139v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#32508;&#36848;&#20102;&#22312;&#32467;&#26500;&#21270;&#25968;&#25454;&#39046;&#22495;&#20013;&#26368;&#36817;&#25552;&#20986;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#20171;&#32461;&#20102;&#20854;&#29702;&#35770;&#22522;&#30784;&#21644;&#24212;&#29992;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65288;generative diffusion models&#65289;&#22312;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#39046;&#22495;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#25104;&#26524;&#65292;&#23637;&#29616;&#20102;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#30340;&#20986;&#33394;&#34920;&#29616;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#21253;&#25324;&#34920;&#26684;&#21644;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65289;&#22312;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#30028;&#20013;&#21463;&#21040;&#30340;&#20851;&#27880;&#30456;&#23545;&#36739;&#23569;&#65292;&#23613;&#31649;&#20854;&#26080;&#22788;&#19981;&#22312;&#19988;&#24212;&#29992;&#24191;&#27867;&#12290;&#22240;&#27492;&#65292;&#19982;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#20854;&#20182;&#25968;&#25454;&#24418;&#24335;&#30456;&#27604;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#23545;&#32467;&#26500;&#21270;&#25968;&#25454;&#24314;&#27169;&#30340;&#25991;&#29486;&#21450;&#20854;&#32508;&#36848;&#20173;&#28982;&#32570;&#20047;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#32467;&#26500;&#21270;&#25968;&#25454;&#39046;&#22495;&#20013;&#26368;&#36817;&#25552;&#20986;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#20840;&#38754;&#32508;&#36848;&#12290;&#39318;&#20808;&#65292;&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#29702;&#35770;&#30340;&#31616;&#35201;&#27010;&#36848;&#65292;&#38543;&#21518;&#21448;&#35814;&#32454;&#25551;&#36848;&#20102;&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#36890;&#29992;&#20219;&#21153;&#21644;&#29305;&#23450;&#39046;&#22495;&#24212;&#29992;&#20013;&#20351;&#29992;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#22823;&#37096;&#20998;&#24320;&#21019;&#24615;&#24037;&#20316;&#30340;&#25216;&#26415;&#25551;&#36848;&#12290;&#26368;&#21518;&#65292;
&lt;/p&gt;
&lt;p&gt;
In recent years, generative diffusion models have achieved a rapid paradigm shift in deep generative models by showing groundbreaking performance across various applications. Meanwhile, structured data, encompassing tabular and time series data, has been received comparatively limited attention from the deep learning research community, despite its omnipresence and extensive applications. Thus, there is still a lack of literature and its review on structured data modelling via diffusion models, compared to other data modalities such as computer vision and natural language processing. Hence, in this paper, we present a comprehensive review of recently proposed diffusion models in the field of structured data. First, this survey provides a concise overview of the score-based diffusion model theory, subsequently proceeding to the technical descriptions of the majority of pioneering works using structured data in both data-driven general tasks and domain-specific applications. Thereafter, 
&lt;/p&gt;</description></item><item><title>&#39640;&#39118;&#38505;&#39046;&#22495;&#23545;&#20110;&#21069;&#25991;&#35299;&#37322;&#24615;&#30340;&#36861;&#27714;&#26159;&#37325;&#35201;&#30340;&#65292;&#20294;&#35813;&#27010;&#24565;&#33258;&#36523;&#21547;&#20041;&#27169;&#31946;&#19981;&#28165;&#65292;&#21462;&#20915;&#20110;&#25805;&#20316;&#29615;&#22659;&#21644;&#35266;&#23519;&#32773;&#21028;&#26029;&#12290;&#36879;&#26126;&#30340;&#39044;&#27979;&#27169;&#22411;&#21487;&#33021;&#20173;&#38656;&#21518;&#32493;&#22788;&#29702;&#25165;&#33021;&#25552;&#20379;&#21512;&#36866;&#30340;&#35299;&#37322;&#24615;&#27934;&#35265;&#65292;&#22240;&#27492;&#20256;&#32479;&#30340;&#21069;&#25991;&#35299;&#37322;&#24615;&#27010;&#24565;&#20173;&#38656;&#35201;&#36827;&#19968;&#27493;&#26126;&#30830;&#21644;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2306.02312</link><description>&lt;p&gt;
&#39640;&#39118;&#38505;&#39046;&#22495;&#23545;&#20110;&#21069;&#25991;&#35299;&#37322;&#24615;&#21560;&#24341;&#21147;&#30340;&#65288;&#19981;&#65289;&#21512;&#29702;&#21560;&#24341;&#21147;&#65306;&#36879;&#26126;&#24230;&#23545;&#20110;&#21487;&#29702;&#35299;&#24615;&#26469;&#35828;&#26159;&#24517;&#35201;&#20294;&#19981;&#36275;&#22815;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
(Un)reasonable Allure of Ante-hoc Interpretability for High-stakes Domains: Transparency Is Necessary but Insufficient for Comprehensibility. (arXiv:2306.02312v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02312
&lt;/p&gt;
&lt;p&gt;
&#39640;&#39118;&#38505;&#39046;&#22495;&#23545;&#20110;&#21069;&#25991;&#35299;&#37322;&#24615;&#30340;&#36861;&#27714;&#26159;&#37325;&#35201;&#30340;&#65292;&#20294;&#35813;&#27010;&#24565;&#33258;&#36523;&#21547;&#20041;&#27169;&#31946;&#19981;&#28165;&#65292;&#21462;&#20915;&#20110;&#25805;&#20316;&#29615;&#22659;&#21644;&#35266;&#23519;&#32773;&#21028;&#26029;&#12290;&#36879;&#26126;&#30340;&#39044;&#27979;&#27169;&#22411;&#21487;&#33021;&#20173;&#38656;&#21518;&#32493;&#22788;&#29702;&#25165;&#33021;&#25552;&#20379;&#21512;&#36866;&#30340;&#35299;&#37322;&#24615;&#27934;&#35265;&#65292;&#22240;&#27492;&#20256;&#32479;&#30340;&#21069;&#25991;&#35299;&#37322;&#24615;&#27010;&#24565;&#20173;&#38656;&#35201;&#36827;&#19968;&#27493;&#26126;&#30830;&#21644;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21069;&#25991;&#35299;&#37322;&#24615;&#24050;&#25104;&#20026;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#65288;&#22914;&#21307;&#30103;&#20445;&#20581;&#65289;&#20013;&#30340;&#36861;&#27714;&#65292;&#28982;&#32780;&#65292;&#36825;&#20010;&#27010;&#24565;&#24456;&#38590;&#25417;&#25720;&#65292;&#32570;&#20047;&#24191;&#27867;&#25509;&#21463;&#30340;&#23450;&#20041;&#65292;&#24182;&#19988;&#21462;&#20915;&#20110;&#25805;&#20316;&#29615;&#22659;&#12290;&#23427;&#21487;&#20197;&#25351;&#30340;&#26159;&#37027;&#20123;&#32467;&#26500;&#31526;&#21512;&#29305;&#23450;&#39046;&#22495;&#32422;&#26463;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#25110;&#32773;&#26159;&#26412;&#36136;&#19978;&#36879;&#26126;&#30340;&#27169;&#22411;&#12290;&#21518;&#19968;&#31181;&#35266;&#24565;&#20551;&#35774;&#35266;&#23519;&#32773;&#23545;&#27492;&#36136;&#37327;&#36827;&#34892;&#21028;&#26029;&#65292;&#32780;&#21069;&#19968;&#31181;&#35266;&#24565;&#21017;&#20551;&#35774;&#20182;&#20204;&#20855;&#26377;&#25216;&#26415;&#21644;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65288;&#20174;&#32780;&#20351;&#20854;&#20182;&#35299;&#37322;&#23545;&#35937;&#32676;&#20307;&#24863;&#21040;&#38476;&#29983;&#65289;&#12290;&#27492;&#22806;&#65292;&#21069;&#25991;&#35299;&#37322;&#24615;&#19982;&#36739;&#19981;&#29702;&#24819;&#30340;&#21518;&#25991;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#30340;&#21306;&#21035;&#27169;&#31946;&#65292;&#21518;&#32773;&#26159;&#25351;&#26500;&#24314;&#19968;&#20010;&#21333;&#29420;&#30340;&#35299;&#37322;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#32780;&#36879;&#26126;&#30340;&#39044;&#27979;&#27169;&#22411;&#20173;&#21487;&#33021;&#38656;&#35201;&#65288;&#21518;&#32493;&#65289;&#22788;&#29702;&#25165;&#33021;&#20135;&#29983;&#36866;&#24403;&#30340;&#35299;&#37322;&#24615;&#27934;&#35265;&#12290;&#22240;&#27492;&#65292;&#21069;&#25991;&#35299;&#37322;&#24615;&#26159;&#19968;&#20010;&#36229;&#36127;&#33655;&#30340;&#27010;&#24565;&#65292;&#23427;&#21253;&#21547;&#19968;&#31995;&#21015;&#38544;&#21547;&#30340;&#23646;&#24615;&#65292;&#25105;&#20204;&#23558;&#22312;&#26412;&#25991;&#20013;&#35814;&#32454;&#38416;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ante-hoc interpretability has become the holy grail of explainable artificial intelligence for high-stakes domains such as healthcare; however, this notion is elusive, lacks a widely-accepted definition and depends on the operational context. It can refer to predictive models whose structure adheres to domain-specific constraints, or ones that are inherently transparent. The latter conceptualisation assumes observers who judge this quality, whereas the former presupposes them to have technical and domain expertise (thus alienating other groups of explainees). Additionally, the distinction between ante-hoc interpretability and the less desirable post-hoc explainability, which refers to methods that construct a separate explanatory model, is vague given that transparent predictive models may still require (post-)processing to yield suitable explanatory insights. Ante-hoc interpretability is thus an overloaded concept that comprises a range of implicit properties, which we unpack in this 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;&#38750;&#27954;&#20302;&#36164;&#28304;&#22320;&#21306;&#21033;&#29992;&#22686;&#24378;&#22411;AI&#31995;&#32479;&#21644;3D&#25968;&#23383;&#23402;&#29983;&#20419;&#36827;&#20581;&#24247;&#21644;&#24184;&#31119;&#30340;&#28508;&#21147;&#65292;&#37325;&#28857;&#20851;&#27880;&#31185;&#23398;&#30693;&#35782;&#21457;&#29616;&#12289;&#25345;&#32493;&#23398;&#20064;&#12289;&#23454;&#29992;&#20114;&#25805;&#20316;&#24615;&#21644;&#20132;&#20114;&#24335;&#35299;&#37322;&#21644;&#20915;&#31574;&#36825;&#20123;&#37325;&#35201;&#30740;&#31350;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.01772</link><description>&lt;p&gt;
&#21033;&#29992;&#22686;&#24378;&#22411;AI&#31995;&#32479;&#21644;3D&#25968;&#23383;&#23402;&#29983;&#26469;&#37325;&#26032;&#26500;&#24819;&#38750;&#27954;&#20302;&#36164;&#28304;&#22320;&#21306;&#30340;&#20581;&#24247;&#21644;&#24184;&#31119;
&lt;/p&gt;
&lt;p&gt;
Re-imagining health and well-being in low resource African settings using an augmented AI system and a 3D digital twin. (arXiv:2306.01772v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;&#38750;&#27954;&#20302;&#36164;&#28304;&#22320;&#21306;&#21033;&#29992;&#22686;&#24378;&#22411;AI&#31995;&#32479;&#21644;3D&#25968;&#23383;&#23402;&#29983;&#20419;&#36827;&#20581;&#24247;&#21644;&#24184;&#31119;&#30340;&#28508;&#21147;&#65292;&#37325;&#28857;&#20851;&#27880;&#31185;&#23398;&#30693;&#35782;&#21457;&#29616;&#12289;&#25345;&#32493;&#23398;&#20064;&#12289;&#23454;&#29992;&#20114;&#25805;&#20316;&#24615;&#21644;&#20132;&#20114;&#24335;&#35299;&#37322;&#21644;&#20915;&#31574;&#36825;&#20123;&#37325;&#35201;&#30740;&#31350;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#21644;&#30740;&#31350;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#22312;&#38750;&#27954;&#20302;&#36164;&#28304;&#22269;&#23478;&#20013;&#20419;&#36827;&#20581;&#24247;&#21644;&#24184;&#31119;&#30340;&#28508;&#21147;&#21644;&#30456;&#20851;&#24615;&#12290;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21644;&#25968;&#23383;&#23402;&#29983;&#39046;&#22495;&#30340;&#26032;&#20852;&#36235;&#21183;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21021;&#22987;&#30340;&#22686;&#24378;&#22411;AI&#31995;&#32479;&#26550;&#26500;&#65292;&#20197;&#35828;&#26126;AI&#31995;&#32479;&#22914;&#20309;&#19982;3D&#25968;&#23383;&#23402;&#29983;&#21327;&#21516;&#24037;&#20316;&#12290;&#25105;&#20204;&#24378;&#35843;&#31185;&#23398;&#30693;&#35782;&#21457;&#29616;&#12289;&#25345;&#32493;&#23398;&#20064;&#12289;&#23454;&#29992;&#20114;&#25805;&#20316;&#24615;&#21644;&#20132;&#20114;&#24335;&#35299;&#37322;&#21644;&#20915;&#31574;&#26159;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21644;&#25968;&#23383;&#23402;&#29983;&#30340;&#37325;&#35201;&#30740;&#31350;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we discuss and explore the potential and relevance of recent developments in artificial intelligence (AI) and digital twins for health and well-being in low-resource African countries. Using an AI systems perspective, we review emerging trends in AI systems and digital twins and propose an initial augmented AI system architecture to illustrate how an AI system can work in conjunction with a 3D digital twin. We highlight scientific knowledge discovery, continual learning, pragmatic interoperability, and interactive explanation and decision-making as important research challenges for AI systems and digital twins.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiffusEmp&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22522;&#20110;&#26465;&#20214;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#22810;&#32423;&#25511;&#21046;&#20449;&#21495;&#26469;&#29983;&#25104;&#26356;&#21152;&#32454;&#33268;&#21644;&#20010;&#24615;&#21270;&#30340;&#20849;&#24773;&#22238;&#24212;&#12290;</title><link>http://arxiv.org/abs/2306.01657</link><description>&lt;p&gt;
DiffusEmp:&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#22810;&#32423;&#25511;&#21046;&#20849;&#24773;&#22238;&#24212;&#29983;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DiffusEmp: A Diffusion Model-Based Framework with Multi-Grained Control for Empathetic Response Generation. (arXiv:2306.01657v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiffusEmp&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22522;&#20110;&#26465;&#20214;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#22810;&#32423;&#25511;&#21046;&#20449;&#21495;&#26469;&#29983;&#25104;&#26356;&#21152;&#32454;&#33268;&#21644;&#20010;&#24615;&#21270;&#30340;&#20849;&#24773;&#22238;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20849;&#24773;&#26159;&#24320;&#25918;&#24335;&#20132;&#27969;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#22240;&#32032;&#65292;&#23427;&#33258;&#28982;&#22320;&#23637;&#31034;&#20102;&#19968;&#20010;&#20154;&#23545;&#20182;&#20154;&#30340;&#20851;&#24515;&#21644;&#29702;&#35299;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#29983;&#25104;&#20849;&#24773;&#21709;&#24212;&#30340;&#26041;&#27861;&#65292;&#20294;&#29616;&#26377;&#30340;&#20316;&#21697;&#24448;&#24448;&#23548;&#33268;&#21333;&#35843;&#30340;&#20849;&#24773;&#65292;&#21363;&#25351;&#36890;&#29992;&#21644;&#23433;&#20840;&#30340;&#34920;&#36798;&#26041;&#24335;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#26174;&#24335;&#25511;&#21046;&#26469;&#24341;&#23548;&#20849;&#24773;&#34920;&#36798;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;DiffusEmp&#26694;&#26550;&#65292;&#22522;&#20110;&#26465;&#20214;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65292;&#32479;&#19968;&#20102;&#23545;&#35805;&#19978;&#19979;&#25991;&#21644;&#23646;&#24615;&#23548;&#21521;&#25511;&#21046;&#20449;&#21495;&#30340;&#21033;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24341;&#20837;&#36890;&#20449;&#26426;&#21046;&#12289;&#24847;&#22270;&#21644;&#35821;&#20041;&#26694;&#26550;&#20316;&#20026;&#22810;&#23618;&#27425;&#20449;&#21495;&#65292;&#21487;&#20174;&#31895;&#31961;&#21040;&#32454;&#33268;&#22320;&#25511;&#21046;&#20849;&#24773;&#23454;&#29616;&#12290;&#28982;&#21518;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#29305;&#23450;&#30340;&#23631;&#34109;&#31574;&#30053;&#65292;&#20197;&#21453;&#26144;&#22810;&#23618;&#27425;&#20449;&#21495;&#19982;&#21709;&#24212;&#20196;&#29260;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#23558;&#20854;&#25972;&#21512;&#21040;&#25193;&#25955;&#27169;&#22411;&#20013;&#20197;&#24433;&#21709;&#29983;&#25104;&#36807;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;EmpatheticDialogue&#19978;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20248;&#20110;&#20854;&#20182;&#20849;&#24773;&#22238;&#24212;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Empathy is a crucial factor in open-domain conversations, which naturally shows one's caring and understanding to others. Though several methods have been proposed to generate empathetic responses, existing works often lead to monotonous empathy that refers to generic and safe expressions. In this paper, we propose to use explicit control to guide the empathy expression and design a framework DiffusEmp based on conditional diffusion language model to unify the utilization of dialogue context and attribute-oriented control signals. Specifically, communication mechanism, intent, and semantic frame are imported as multi-grained signals that control the empathy realization from coarse to fine levels. We then design a specific masking strategy to reflect the relationship between multi-grained signals and response tokens, and integrate it into the diffusion model to influence the generative process. Experimental results on a benchmark dataset EmpatheticDialogue show that our framework outper
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30417;&#30563;&#24335;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;&#65288;SACL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#31867;&#21035;&#20998;&#24067;&#32467;&#26500;&#34920;&#31034;&#65292;&#36890;&#36807;&#32852;&#21512;&#31867;&#21035;&#20998;&#24067;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#65292;&#26377;&#25928;&#21033;&#29992;&#26631;&#31614;&#32423;&#29305;&#24615;&#19968;&#33268;&#24615;&#24182;&#20445;&#30041;&#32454;&#31890;&#24230;&#30340;&#31867;&#20869;&#29305;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#20013;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.01505</link><description>&lt;p&gt;
&#22312;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#20013;&#20351;&#29992;&#30417;&#30563;&#24335;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Supervised Adversarial Contrastive Learning for Emotion Recognition in Conversations. (arXiv:2306.01505v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30417;&#30563;&#24335;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;&#65288;SACL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#31867;&#21035;&#20998;&#24067;&#32467;&#26500;&#34920;&#31034;&#65292;&#36890;&#36807;&#32852;&#21512;&#31867;&#21035;&#20998;&#24067;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#65292;&#26377;&#25928;&#21033;&#29992;&#26631;&#31614;&#32423;&#29305;&#24615;&#19968;&#33268;&#24615;&#24182;&#20445;&#30041;&#32454;&#31890;&#24230;&#30340;&#31867;&#20869;&#29305;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#20013;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#35782;&#21035;&#22312;&#23545;&#35805;&#20013;&#26159;&#25552;&#21462;&#27867;&#21270;&#21644;&#31283;&#20581;&#34920;&#31034;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30417;&#30563;&#24335;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;&#65288;SACL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#31867;&#21035;&#20998;&#24067;&#32467;&#26500;&#34920;&#31034;&#12290;&#35813;&#26694;&#26550;&#24212;&#29992;&#20110;&#23545;&#27604;&#24863;&#30693;&#23545;&#25239;&#24615;&#35757;&#32451;&#20197;&#29983;&#25104;&#26368;&#22351;&#24773;&#20917;&#30340;&#26679;&#26412;&#65292;&#24182;&#22312;&#21407;&#22987;&#21644;&#23545;&#25239;&#26679;&#26412;&#19978;&#20351;&#29992;&#32852;&#21512;&#31867;&#21035;&#20998;&#24067;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#12290;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#26631;&#31614;&#32423;&#29305;&#24615;&#19968;&#33268;&#24615;&#24182;&#20445;&#30041;&#32454;&#31890;&#24230;&#30340;&#31867;&#20869;&#29305;&#24615;&#12290;&#20026;&#20102;&#36991;&#20813;&#23545;&#19978;&#19979;&#25991;&#30456;&#20851;&#25968;&#25454;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#23545;&#25239;&#24615;&#35757;&#32451;&#31574;&#30053;&#65292;&#20174;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#26356;&#22810;&#19981;&#21516;&#30340;&#29305;&#24449;&#65292;&#24182;&#22686;&#24378;&#27169;&#22411;&#23545;&#19978;&#19979;&#25991;&#30340;&#23481;&#38169;&#24615;&#12290;&#22312;&#35813;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#24207;&#21015;&#30340;&#26041;&#27861;SACL-LSTM&#65292;&#29992;&#20110;&#23398;&#20064;&#38024;&#23545;ERC&#30340;&#26631;&#31614;&#19968;&#33268;&#21644;&#19978;&#19979;&#25991;&#31283;&#20581;&#30340;&#24773;&#24863;&#29305;&#24449;&#12290;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;SACL-LSTM&#22312;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extracting generalized and robust representations is a major challenge in emotion recognition in conversations (ERC). To address this, we propose a supervised adversarial contrastive learning (SACL) framework for learning class-spread structured representations. The framework applies contrast-aware adversarial training to generate worst-case samples and uses a joint class-spread contrastive learning objective on both original and adversarial samples. It can effectively utilize label-level feature consistency and retain fine-grained intra-class features. To avoid the negative impact of adversarial perturbations on context-dependent data, we design a contextual adversarial training strategy to learn more diverse features from context and enhance the model's context robustness. We develop a sequence-based method SACL-LSTM under this framework, to learn label-consistent and context-robust emotional features for ERC. Experiments on three datasets demonstrate that SACL-LSTM achieves state-of
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#24863;&#30693;&#24352;&#37327;&#21387;&#32553;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#23569;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#30340;&#27169;&#22411;&#22823;&#23567;&#65292;&#31639;&#26415;&#36816;&#31639;&#21644;&#26368;&#32456;&#36816;&#34892;&#26102;&#24310;&#12290;&#35813;&#26041;&#27861;&#32463;&#36807;&#36880;&#23618;&#33976;&#39311;&#21518;&#22312;&#25991;&#26412;&#34164;&#21547;&#21644;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01076</link><description>&lt;p&gt;
&#37327;&#21270;&#24863;&#30693;&#21644;&#24352;&#37327;&#21387;&#32553;&#35757;&#32451;&#65306;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;Transformer
&lt;/p&gt;
&lt;p&gt;
Quantization-Aware and Tensor-Compressed Training of Transformers for Natural Language Understanding. (arXiv:2306.01076v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01076
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#24863;&#30693;&#24352;&#37327;&#21387;&#32553;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#23569;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#30340;&#27169;&#22411;&#22823;&#23567;&#65292;&#31639;&#26415;&#36816;&#31639;&#21644;&#26368;&#32456;&#36816;&#34892;&#26102;&#24310;&#12290;&#35813;&#26041;&#27861;&#32463;&#36807;&#36880;&#23618;&#33976;&#39311;&#21518;&#22312;&#25991;&#26412;&#34164;&#21547;&#21644;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#35843;&#30340;Transformer&#27169;&#22411;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24222;&#22823;&#30340;&#27169;&#22411;&#22823;&#23567;&#38459;&#27490;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#37096;&#32626;&#39640;&#24615;&#33021;Transformer&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#24863;&#30693;&#24352;&#37327;&#21387;&#32553;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#20943;&#23569;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#30340;&#27169;&#22411;&#22823;&#23567;&#65292;&#31639;&#26415;&#36816;&#31639;&#21644;&#26368;&#32456;&#36816;&#34892;&#26102;&#24310;&#12290;&#25105;&#20204;&#23558;Transformer&#30340;&#23884;&#20837;&#21644;&#32447;&#24615;&#23618;&#21387;&#32553;&#20026;&#23567;&#22411;&#20302;&#31209;&#24352;&#37327;&#26680;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#27169;&#22411;&#21442;&#25968;&#12290;&#37319;&#29992;&#21487;&#23398;&#20064;&#27604;&#20363;&#22240;&#23376;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#26469;&#36827;&#19968;&#27493;&#33719;&#24471;&#24352;&#37327;&#21387;&#32553;&#27169;&#22411;&#30340;&#20302;&#31934;&#24230;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#31471;&#21040;&#31471;&#35757;&#32451;&#21644;&#33976;&#39311;&#35757;&#32451;&#12290;&#20026;&#20102;&#25552;&#39640;&#25910;&#25947;&#24615;&#65292;&#37319;&#29992;&#36880;&#23618;&#33976;&#39311;&#26041;&#27861;&#20174;&#39044;&#35757;&#32451;Transformer&#20013;&#25552;&#21462;&#20986;&#19968;&#20010;&#32463;&#36807;&#37327;&#21270;&#21644;&#24352;&#37327;&#21387;&#32553;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#22312;&#20004;&#20010;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#65292;&#21363;&#25991;&#26412;&#34164;&#21547;&#21644;&#24773;&#24863;&#20998;&#26512;&#20013;&#23637;&#31034;&#20102;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuned transformer models have shown superior performances in many natural language tasks. However, the large model size prohibits deploying high-performance transformer models on resource-constrained devices. This paper proposes a quantization-aware tensor-compressed training approach to reduce the model size, arithmetic operations, and ultimately runtime latency of transformer-based models. We compress the embedding and linear layers of transformers into small low-rank tensor cores, which significantly reduces model parameters. A quantization-aware training with learnable scale factors is used to further obtain low-precision representations of the tensor-compressed models. The developed approach can be used for both end-to-end training and distillation-based training. To improve the convergence, a layer-by-layer distillation is applied to distill a quantized and tensor-compressed student model from a pre-trained transformer. The performance is demonstrated in two natural language
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;&#19987;&#38376;&#21270;&#65292;&#21253;&#25324;&#21160;&#26426;&#12289;&#25361;&#25112;&#12289;&#26041;&#27861;&#35770;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#29305;&#23450;&#39046;&#22495;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#27861;&#65292;&#23545;&#29616;&#26377;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#23450;&#21046;&#25216;&#26415;&#36827;&#34892;&#20102;&#35814;&#32454;&#27604;&#36739;&#65292;&#24182;&#24191;&#27867;&#35752;&#35770;&#20102;&#36825;&#19968;&#39046;&#22495;&#20013;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.18703</link><description>&lt;p&gt;
&#36229;&#36234;&#19968;&#20010;&#27169;&#22411;&#36866;&#29992;&#20110;&#25152;&#26377;&#39046;&#22495;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;&#19987;&#38376;&#21270;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Beyond One-Model-Fits-All: A Survey of Domain Specialization for Large Language Models. (arXiv:2305.18703v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;&#19987;&#38376;&#21270;&#65292;&#21253;&#25324;&#21160;&#26426;&#12289;&#25361;&#25112;&#12289;&#26041;&#27861;&#35770;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#29305;&#23450;&#39046;&#22495;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#27861;&#65292;&#23545;&#29616;&#26377;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#23450;&#21046;&#25216;&#26415;&#36827;&#34892;&#20102;&#35814;&#32454;&#27604;&#36739;&#65292;&#24182;&#24191;&#27867;&#35752;&#35770;&#20102;&#36825;&#19968;&#39046;&#22495;&#20013;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#22823;&#22823;&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#20026;&#24191;&#27867;&#24212;&#29992;&#25552;&#20379;&#20102;&#39640;&#24230;&#23454;&#29992;&#12289;&#20219;&#21153;&#26080;&#20851;&#30340;&#22522;&#30784;&#12290;LLMs &#20316;&#20026;&#36890;&#29992;&#20219;&#21153;&#27714;&#35299;&#22120;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20419;&#20351;&#20154;&#20204;&#23558;&#20854;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#65292;&#22914;&#21307;&#30103;&#20445;&#20581;&#12289;&#37329;&#34701;&#21644;&#25945;&#32946;&#65292;&#24182;&#23558;&#20854;&#29992;&#20316;&#21161;&#25163;&#29978;&#33267;&#26367;&#20195;&#29305;&#23450;&#39046;&#22495;&#30340;&#19987;&#23478;&#21644;&#24037;&#20855;&#12290;&#20294;&#26159;&#65292;&#23558;LLMs&#30452;&#25509;&#24212;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#20013;&#30340;&#22797;&#26434;&#38382;&#39064;&#20250;&#36935;&#21040;&#35768;&#22810;&#22256;&#38590;&#65292;&#21253;&#25324;&#39046;&#22495;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#12289;&#39046;&#22495;&#30693;&#35782;&#30340;&#22797;&#26434;&#24615;&#12289;&#39046;&#22495;&#30446;&#26631;&#30340;&#29420;&#29305;&#24615;&#20197;&#21450;&#32422;&#26463;&#30340;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#31181;&#24046;&#36317;&#65292;&#26368;&#36817;&#20960;&#24180;&#36827;&#34892;&#20102;&#24613;&#21095;&#22686;&#21152;&#30340;&#30740;&#31350;&#21644;&#23454;&#36341;&#33268;&#21147;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;&#19987;&#38376;&#21270;&#65292;&#28982;&#32780;&#36825;&#26041;&#38754;&#30340;&#30740;&#31350;&#23578;&#26410;&#34987;&#31995;&#32479;&#22320;&#24635;&#32467;&#12290;&#22312;&#36825;&#31687;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#23545;LLMs&#30340;&#39046;&#22495;&#19987;&#38376;&#21270;&#36827;&#34892;&#20102;&#20840;&#38754;&#27010;&#36848;&#65292;&#21253;&#25324;&#21160;&#26426;&#12289;&#25361;&#25112;&#12289;&#26041;&#27861;&#35770;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#29305;&#23450;&#39046;&#22495;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#27861;&#65292;&#23545;&#29616;&#26377;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#23450;&#21046;&#25216;&#26415;&#36827;&#34892;&#20102;&#35814;&#32454;&#27604;&#36739;&#65292;&#24182;&#24191;&#27867;&#35752;&#35770;&#20102;&#36825;&#19968;&#39046;&#22495;&#20013;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have significantly advanced the field of natural language processing (NLP), providing a highly useful, task-agnostic foundation for a wide range of applications. The great promise of LLMs as general task solvers motivated people to extend their functionality largely beyond just a ``chatbot'', and use it as an assistant or even replacement for domain experts and tools in specific domains such as healthcare, finance, and education. However, directly applying LLMs to solve sophisticated problems in specific domains meets many hurdles, caused by the heterogeneity of domain data, the sophistication of domain knowledge, the uniqueness of domain objectives, and the diversity of the constraints (e.g., various social norms, cultural conformity, religious beliefs, and ethical standards in the domain applications). To fill such a gap, explosively-increase research, and practices have been conducted in very recent years on the domain specialization of LLMs, which, howe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#24615;&#24378;&#30340;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#27169;&#22411;CMRL&#65292;&#36890;&#36807;&#26816;&#27979;&#19982;&#21270;&#23398;&#21453;&#24212;&#22240;&#26524;&#30456;&#20851;&#30340;&#26680;&#24515;&#20122;&#32467;&#26500;&#26469;&#24212;&#23545;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.18451</link><description>&lt;p&gt;
&#20855;&#26377;&#22240;&#26524;&#20122;&#32467;&#26500;&#30340;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#27169;&#22411;&#22312;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#26102;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Shift-Robust Molecular Relational Learning with Causal Substructure. (arXiv:2305.18451v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18451
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#24615;&#24378;&#30340;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#27169;&#22411;CMRL&#65292;&#36890;&#36807;&#26816;&#27979;&#19982;&#21270;&#23398;&#21453;&#24212;&#22240;&#26524;&#30456;&#20851;&#30340;&#26680;&#24515;&#20122;&#32467;&#26500;&#26469;&#24212;&#23545;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#24341;&#36215;&#20102;&#20998;&#23376;&#31185;&#23398;&#39046;&#22495;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#20854;&#30446;&#26631;&#26159;&#39044;&#27979;&#20998;&#23376;&#23545;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#34892;&#20026;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#24615;&#24378;&#30340;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#27169;&#22411;CMRL&#65292;&#23427;&#36890;&#36807;&#26816;&#27979;&#19982;&#21270;&#23398;&#21453;&#24212;&#22240;&#26524;&#30456;&#20851;&#30340;&#26680;&#24515;&#20122;&#32467;&#26500;&#26469;&#24212;&#23545;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#20551;&#23450;&#22522;&#20110;&#20998;&#23376;&#31185;&#23398;&#39046;&#22495;&#30693;&#35782;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#24182;&#26500;&#24314;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#26469;&#25581;&#31034;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22522;&#20110;SCM&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26465;&#20214;&#24178;&#39044;&#26694;&#26550;&#65292;&#20854;&#24178;&#39044;&#26159;&#22522;&#20110;&#25104;&#23545;&#20998;&#23376;&#26465;&#20214;&#30340;&#12290;&#20351;&#29992;&#26465;&#20214;&#24178;&#39044;&#26694;&#26550;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#25104;&#21151;&#22320;&#20174;&#22240;&#26524;&#20122;&#32467;&#26500;&#20013;&#23398;&#20064;&#65292;&#24182;&#20943;&#36731;&#20102;&#19982;&#21270;&#23398;&#21453;&#24212;&#34394;&#20551;&#30456;&#20851;&#30340;&#24555;&#25463;&#20122;&#32467;&#26500;&#30340;&#28151;&#28102;&#25928;&#24212;&#12290;&#26412;&#25991;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, molecular relational learning, whose goal is to predict the interaction behavior between molecular pairs, got a surge of interest in molecular sciences due to its wide range of applications. In this work, we propose CMRL that is robust to the distributional shift in molecular relational learning by detecting the core substructure that is causally related to chemical reactions. To do so, we first assume a causal relationship based on the domain knowledge of molecular sciences and construct a structural causal model (SCM) that reveals the relationship between variables. Based on the SCM, we introduce a novel conditional intervention framework whose intervention is conditioned on the paired molecule. With the conditional intervention framework, our model successfully learns from the causal substructure and alleviates the confounding effect of shortcut substructures that are spuriously correlated to chemical reactions. Extensive experiments on various tasks with real-world and sy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22122;&#22768;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#65288;NSNN&#65289;&#21644;&#22122;&#22768;&#39537;&#21160;&#23398;&#20064;&#35268;&#21017;&#65288;NDL&#65289;&#65292;&#23637;&#31034;&#20102;&#22122;&#22768;&#21487;&#20197;&#20316;&#20026;&#35745;&#31639;&#21644;&#23398;&#20064;&#30340;&#36164;&#28304;&#65292;&#24182;&#20026;&#19968;&#33324;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;&#30740;&#31350;&#36824;&#23637;&#31034;&#20102;NSNNs&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#35821;&#38899;&#35782;&#21035;&#31561;&#23454;&#38469;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#34920;&#26126;&#23427;&#20204;&#26159;&#26410;&#26469;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#31995;&#32479;&#30340;&#28508;&#22312;&#26377;&#21147;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2305.16044</link><description>&lt;p&gt;
&#22312;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;&#23558;&#22122;&#22768;&#20316;&#20026;&#35745;&#31639;&#21644;&#23398;&#20064;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;
Exploiting Noise as a Resource for Computation and Learning in Spiking Neural Networks. (arXiv:2305.16044v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22122;&#22768;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#65288;NSNN&#65289;&#21644;&#22122;&#22768;&#39537;&#21160;&#23398;&#20064;&#35268;&#21017;&#65288;NDL&#65289;&#65292;&#23637;&#31034;&#20102;&#22122;&#22768;&#21487;&#20197;&#20316;&#20026;&#35745;&#31639;&#21644;&#23398;&#20064;&#30340;&#36164;&#28304;&#65292;&#24182;&#20026;&#19968;&#33324;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;&#30740;&#31350;&#36824;&#23637;&#31034;&#20102;NSNNs&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#35821;&#38899;&#35782;&#21035;&#31561;&#23454;&#38469;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#34920;&#26126;&#23427;&#20204;&#26159;&#26410;&#26469;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#31995;&#32479;&#30340;&#28508;&#22312;&#26377;&#21147;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#26159;&#22823;&#33041;&#38750;&#20961;&#20449;&#24687;&#22788;&#29702;&#33021;&#21147;&#30340;&#22522;&#30784;&#65292;&#24182;&#24050;&#25104;&#20026;&#31070;&#32463;&#24418;&#24577;&#26234;&#33021;&#30340;&#25903;&#26609;&#27169;&#22411;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22122;&#22768;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#65288;NSNN&#65289;&#21644;&#22122;&#22768;&#39537;&#21160;&#23398;&#20064;&#35268;&#21017;&#65288;NDL&#65289;&#65292;&#37319;&#29992;&#24102;&#26377;&#22122;&#22768;&#31070;&#32463;&#20803;&#21160;&#21147;&#23398;&#30340;&#33033;&#20914;&#31070;&#32463;&#20803;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#26174;&#31034;&#22122;&#22768;&#21487;&#20197;&#20316;&#20026;&#35745;&#31639;&#21644;&#23398;&#20064;&#30340;&#36164;&#28304;&#65292;&#24182;&#29702;&#35770;&#19978;&#20026;&#19968;&#33324;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;NDL&#20026;&#20195;&#29702;&#26799;&#24230;&#25552;&#20379;&#20102;&#28145;&#20837;&#30340;&#29983;&#29289;&#23398;&#21512;&#29702;&#24615;&#12290;&#36890;&#36807;&#23558;&#21508;&#31181;SNN&#26550;&#26500;&#21644;&#31639;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#33021;&#65292;&#24182;&#19988;&#27604;&#30830;&#23450;&#24615;SNNs&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#23637;&#31034;&#20102;NSNNs&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#35821;&#38899;&#35782;&#21035;&#31561;&#23454;&#38469;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#34920;&#26126;&#23427;&#20204;&#26159;&#26410;&#26469;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#31995;&#32479;&#30340;&#28508;&#22312;&#26377;&#21147;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Networks of spiking neurons underpin the extraordinary information-processing capabilities of the brain and have emerged as pillar models in neuromorphic intelligence. Despite extensive research on spiking neural networks (SNNs), most are established on deterministic models. Integrating noise into SNNs leads to biophysically more realistic neural dynamics and may benefit model performance. This work presents the noisy spiking neural network (NSNN) and the noise-driven learning rule (NDL) by introducing a spiking neuron model incorporating noisy neuronal dynamics. Our approach shows how noise may act as a resource for computation and learning and theoretically provides a framework for general SNNs. Moreover, NDL provides an insightful biological rationale for surrogate gradients. By incorporating various SNN architectures and algorithms, we show that our approach exhibits competitive performance and improved robustness against challenging perturbations than deterministic SNNs. Additiona
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#38024;&#23545;&#32972;&#21253;&#32422;&#26463;&#19979;&#30340;&#38750;&#21333;&#35843;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#30340;&#20004;&#31181;&#26032;&#30340;&#32447;&#24615;&#26597;&#35810;&#36924;&#36817;&#31639;&#27861;&#65292;&#20854;&#20013;$\mathsf{DLA}$&#21644;$\mathsf{RLA}$&#37117;&#26377;&#24120;&#25968;&#36817;&#20284;&#27604;&#65292;&#19988;&#26597;&#35810;&#22797;&#26434;&#24230;&#20026;$O(n \log(1/\epsilon)/\epsilon)$&#12290;</title><link>http://arxiv.org/abs/2305.10292</link><description>&lt;p&gt;
&#38024;&#23545;&#32972;&#21253;&#32422;&#26463;&#19979;&#30340;&#38750;&#21333;&#35843;&#23376;&#27169;&#26368;&#22823;&#21270;&#30340;&#32447;&#24615;&#26597;&#35810;&#36924;&#36817;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Linear Query Approximation Algorithms for Non-monotone Submodular Maximization under Knapsack Constraint. (arXiv:2305.10292v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10292
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#38024;&#23545;&#32972;&#21253;&#32422;&#26463;&#19979;&#30340;&#38750;&#21333;&#35843;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#30340;&#20004;&#31181;&#26032;&#30340;&#32447;&#24615;&#26597;&#35810;&#36924;&#36817;&#31639;&#27861;&#65292;&#20854;&#20013;$\mathsf{DLA}$&#21644;$\mathsf{RLA}$&#37117;&#26377;&#24120;&#25968;&#36817;&#20284;&#27604;&#65292;&#19988;&#26597;&#35810;&#22797;&#26434;&#24230;&#20026;$O(n \log(1/\epsilon)/\epsilon)$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#39318;&#27425;&#24341;&#20837;&#20102;&#20004;&#31181;&#20855;&#26377;&#32447;&#24615;&#26597;&#35810;&#22797;&#26434;&#24230;&#30340;&#24120;&#25968;&#36817;&#20284;&#31639;&#27861;&#26469;&#35299;&#20915;&#22320;&#38754;&#38598;&#21512;&#22823;&#23567;&#20026;$n$&#65292;&#22312;&#32972;&#21253;&#32422;&#26463;&#19979;&#30340;&#38750;&#21333;&#35843;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#20998;&#21035;&#20026;$\mathsf{DLA}$&#21644;$\mathsf{RLA}$&#12290;&#20854;&#20013;$\mathsf{DLA}$&#26159;&#25552;&#20379;6+$\epsilon$&#36817;&#20284;&#27604;&#30340;&#30830;&#23450;&#24615;&#31639;&#27861;&#65292;&#32780;$\mathsf{RLA}$&#26159;&#20855;&#26377;4+$\epsilon$&#36817;&#20284;&#27604;&#30340;&#38543;&#26426;&#31639;&#27861;&#12290;&#20004;&#31181;&#31639;&#27861;&#30340;&#26597;&#35810;&#22797;&#26434;&#24230;&#22343;&#20026;$O(n \log(1/\epsilon)/\epsilon)$&#12290;&#33719;&#21462;&#32447;&#24615;&#26597;&#35810;&#19979;&#30340;&#24120;&#25968;&#36817;&#20284;&#27604;&#30340;&#20851;&#38190;&#24605;&#24819;&#22312;&#20110;: (1)&#23558;&#22320;&#38754;&#38598;&#21512;&#20998;&#20026;&#20004;&#20010;&#21512;&#36866;&#30340;&#23376;&#38598;&#65292;&#29992;&#32447;&#24615;&#26597;&#35810;&#22312;&#36825;&#20123;&#23376;&#38598;&#20013;&#25214;&#21040;&#36817;&#20284;&#26368;&#20248;&#35299;&#65292;(2)&#23558;&#38408;&#20540;&#36138;&#24515;&#19982;&#20004;&#20010;&#19981;&#30456;&#20132;&#38598;&#21512;&#25110;&#38543;&#26426;&#36873;&#25321;&#36827;&#31243;&#30340;&#24615;&#36136;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#35299;&#30340;&#36136;&#37327;&#12290;&#38500;&#20102;&#29702;&#35770;&#20998;&#26512;&#22806;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#19977;&#20010;&#24212;&#29992;&#31243;&#24207;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#65306;&#25910;&#20837;&#26368;&#22823;&#21270;&#65292;&#22270;&#20687;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work, for the first time, introduces two constant factor approximation algorithms with linear query complexity for non-monotone submodular maximization over a ground set of size $n$ subject to a knapsack constraint, $\mathsf{DLA}$ and $\mathsf{RLA}$. $\mathsf{DLA}$ is a deterministic algorithm that provides an approximation factor of $6+\epsilon$ while $\mathsf{RLA}$ is a randomized algorithm with an approximation factor of $4+\epsilon$. Both run in $O(n \log(1/\epsilon)/\epsilon)$ query complexity. The key idea to obtain a constant approximation ratio with linear query lies in: (1) dividing the ground set into two appropriate subsets to find the near-optimal solution over these subsets with linear queries, and (2) combining a threshold greedy with properties of two disjoint sets or a random selection process to improve solution quality. In addition to the theoretical analysis, we have evaluated our proposed solutions with three applications: Revenue Maximization, Image Summarizat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#30340;&#39033;&#30446;&#32034;&#24341;&#38382;&#39064;&#36827;&#34892;&#20102;&#31995;&#32479;&#26816;&#26597;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#32034;&#24341;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#39033;&#30446;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.06569</link><description>&lt;p&gt;
&#22914;&#20309;&#20026;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#32034;&#24341;&#39033;&#30446;ID
&lt;/p&gt;
&lt;p&gt;
How to Index Item IDs for Recommendation Foundation Models. (arXiv:2305.06569v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#30340;&#39033;&#30446;&#32034;&#24341;&#38382;&#39064;&#36827;&#34892;&#20102;&#31995;&#32479;&#26816;&#26597;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#32034;&#24341;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#39033;&#30446;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#23558;&#25512;&#33616;&#20219;&#21153;&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#25512;&#33616;&#12290;&#23427;&#36890;&#36807;&#30452;&#25509;&#29983;&#25104;&#24314;&#35758;&#30340;&#39033;&#30446;&#32780;&#19981;&#26159;&#35745;&#31639;&#20256;&#32479;&#25512;&#33616;&#27169;&#22411;&#20013;&#27599;&#20010;&#20505;&#36873;&#39033;&#30446;&#30340;&#25490;&#21517;&#24471;&#20998;&#65292;&#31616;&#21270;&#20102;&#25512;&#33616;&#31649;&#36947;&#65292;&#36991;&#20813;&#20102;&#22810;&#27573;&#36807;&#28388;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#36991;&#20813;&#22312;&#20915;&#23450;&#35201;&#25512;&#33616;&#21738;&#20123;&#39033;&#30446;&#26102;&#29983;&#25104;&#36807;&#38271;&#30340;&#25991;&#26412;&#65292;&#20026;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#21019;&#24314;LLM&#20860;&#23481;&#30340;&#39033;&#30446;ID&#26159;&#24517;&#35201;&#30340;&#12290;&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#30340;&#39033;&#30446;&#32034;&#24341;&#38382;&#39064;&#65292;&#20197;P5&#20026;&#20195;&#34920;&#30340;&#20027;&#24178;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#21508;&#31181;&#32034;&#24341;&#26041;&#27861;&#22797;&#21046;&#20854;&#32467;&#26524;&#12290;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;&#20960;&#31181;&#24494;&#19981;&#36275;&#36947;&#30340;&#39033;&#30446;&#32034;&#24341;&#26041;&#27861;&#65288;&#22914;&#29420;&#31435;&#32034;&#24341;&#12289;&#26631;&#39064;&#32034;&#24341;&#21644;&#38543;&#26426;&#32034;&#24341;&#65289;&#30340;&#38382;&#39064;&#65292;&#24182;&#34920;&#26126;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32034;&#24341;&#26041;&#27861;&#65292;&#31216;&#20026;&#19978;&#19979;&#25991;&#24863;&#30693;&#32034;&#24341;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#31181;&#32034;&#24341;&#26041;&#27861;&#22312;&#39033;&#30446;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#32034;&#24341;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommendation foundation model utilizes large language models (LLM) for recommendation by converting recommendation tasks into natural language tasks. It enables generative recommendation which directly generates the item(s) to recommend rather than calculating a ranking score for each and every candidate item in traditional recommendation models, simplifying the recommendation pipeline from multi-stage filtering to single-stage filtering. To avoid generating excessively long text when deciding which item(s) to recommend, creating LLM-compatible item IDs is essential for recommendation foundation models. In this study, we systematically examine the item indexing problem for recommendation foundation models, using P5 as the representative backbone model and replicating its results with various indexing methods. To emphasize the importance of item indexing, we first discuss the issues of several trivial item indexing methods, such as independent indexing, title indexing, and random inde
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#27493;&#39588;&#26041;&#27861;&#65292;&#31616;&#21270;&#20102;&#21464;&#20998;&#36125;&#21494;&#26031;&#36817;&#20284;&#25512;&#26029;&#26041;&#27861;&#30340;&#25512;&#23548;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2304.14251</link><description>&lt;p&gt;
&#31616;&#21270;&#21464;&#20998;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#25512;&#23548;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Variational Bayes Made Easy. (arXiv:2304.14251v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14251
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#27493;&#39588;&#26041;&#27861;&#65292;&#31616;&#21270;&#20102;&#21464;&#20998;&#36125;&#21494;&#26031;&#36817;&#20284;&#25512;&#26029;&#26041;&#27861;&#30340;&#25512;&#23548;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#36125;&#21494;&#26031;&#26041;&#27861;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#36817;&#20284;&#25512;&#26029;&#26041;&#27861;&#65292;&#20294;&#20854;&#25512;&#23548;&#36807;&#31243;&#21487;&#33021;&#24456;&#32321;&#29712;&#12290;&#20026;&#20102;&#31616;&#21270;&#36825;&#20010;&#36807;&#31243;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#20010;&#19977;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26174;&#24335;&#23547;&#25214;&#20851;&#20110;&#24050;&#30693;&#20998;&#24067;&#26399;&#26395;&#30340;&#32447;&#24615;&#24615;&#65292;&#26469;&#30830;&#23450;&#21518;&#39564;&#20998;&#24067;&#24418;&#24335;&#12290;&#28982;&#21518;&#25105;&#20204;&#21487;&#20197;&#30452;&#25509;&#36890;&#36807;&#8220;&#35835;&#21462;&#8221;&#36825;&#20123;&#26399;&#26395;&#21069;&#30340;&#39033;&#65292;&#20889;&#20986;&#26356;&#26032;&#12290;&#36825;&#20010;&#26041;&#27861;&#20351;&#24471;&#25512;&#23548;&#26356;&#21152;&#31616;&#21333;&#65292;&#24555;&#36895;&#65292;&#31616;&#30701;&#21644;&#36890;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational Bayes is a popular method for approximate inference but its derivation can be cumbersome. To simplify the process, we give a 3-step recipe to identify the posterior form by explicitly looking for linearity with respect to expectations of well-known distributions. We can then directly write the update by simply ``reading-off'' the terms in front of those expectations. The recipe makes the derivation easier, faster, shorter, and more general.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;Transformer&#30340;&#26041;&#27861;&#65292;&#21363;REPORT&#65292;&#33021;&#22815;&#21516;&#26102;&#32858;&#21512;&#20851;&#31995;&#36335;&#24452;&#21644;&#19978;&#19979;&#25991;&#65292;&#25429;&#25417;&#23454;&#20307;&#20043;&#38388;&#30340;&#32852;&#31995;&#21644;&#20869;&#22312;&#29305;&#24615;&#12290;&#23427;&#23436;&#20840;&#20381;&#36182;&#20110;&#20851;&#31995;&#35821;&#20041;&#65292;&#24182;&#33021;&#33258;&#28982;&#22320;&#25512;&#24191;&#21040;&#23436;&#20840;&#24402;&#32435;&#30340;&#35774;&#32622;&#20013;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.00215</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#23618;Transformer&#30340;&#20851;&#31995;&#36335;&#24452;&#21644;&#19978;&#19979;&#25991;&#24402;&#32435;&#20851;&#31995;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Inductive Relation Prediction from Relational Paths and Context with Hierarchical Transformers. (arXiv:2304.00215v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;Transformer&#30340;&#26041;&#27861;&#65292;&#21363;REPORT&#65292;&#33021;&#22815;&#21516;&#26102;&#32858;&#21512;&#20851;&#31995;&#36335;&#24452;&#21644;&#19978;&#19979;&#25991;&#65292;&#25429;&#25417;&#23454;&#20307;&#20043;&#38388;&#30340;&#32852;&#31995;&#21644;&#20869;&#22312;&#29305;&#24615;&#12290;&#23427;&#23436;&#20840;&#20381;&#36182;&#20110;&#20851;&#31995;&#35821;&#20041;&#65292;&#24182;&#33021;&#33258;&#28982;&#22320;&#25512;&#24191;&#21040;&#23436;&#20840;&#24402;&#32435;&#30340;&#35774;&#32622;&#20013;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30693;&#35782;&#22270;&#35889;&#20013;&#36827;&#34892;&#20851;&#31995;&#39044;&#27979;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#29616;&#26377;&#30340;&#23884;&#20837;&#24335;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#36716;&#23548;&#35774;&#32622;&#65292;&#32570;&#20047;&#24402;&#32435;&#33021;&#21147;&#65292;&#26080;&#27861;&#25512;&#24191;&#21040;&#26032;&#30340;&#23454;&#20307;&#19978;&#36827;&#34892;&#25512;&#29702;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32479;&#19968;&#30340;&#20998;&#23618;Transformer&#26694;&#26550;&#65292;&#21363;REPORT&#65292;&#21516;&#26102;&#32858;&#21512;&#20851;&#31995;&#36335;&#24452;&#21644;&#19978;&#19979;&#25991;&#65292;&#25429;&#25417;&#23454;&#20307;&#20043;&#38388;&#30340;&#32852;&#31995;&#21644;&#20869;&#22312;&#29305;&#24615;&#65292;&#36825;&#31181;&#26041;&#27861;&#23436;&#20840;&#20381;&#36182;&#20110;&#20851;&#31995;&#35821;&#20041;&#65292;&#24182;&#33021;&#33258;&#28982;&#22320;&#25512;&#24191;&#21040;&#23436;&#20840;&#24402;&#32435;&#30340;&#35774;&#32622;&#20013;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;REPORT&#34920;&#29616;&#20248;&#20110;&#25152;&#26377;&#22522;&#32447;&#26041;&#27861;&#65292;&#29978;&#33267;&#22312;&#20004;&#20010;&#23436;&#20840;&#24402;&#32435;&#30340;&#25968;&#25454;&#38598;&#30340;&#20843;&#20010;&#29256;&#26412;&#23376;&#38598;&#19978;&#20063;&#26159;&#22914;&#27492;&#12290;&#27492;&#22806;&#65292;REPORT&#33021;&#22815;&#23558;&#25512;&#29702;&#25512;&#24191;&#21040;&#35757;&#32451;&#21644;&#25512;&#29702;&#20013;&#27809;&#26377;&#20844;&#20849;&#23454;&#20307;&#30340;&#26032;&#23454;&#20307;&#19978;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation prediction on knowledge graphs (KGs) is a key research topic. Dominant embedding-based methods mainly focus on the transductive setting and lack the inductive ability to generalize to new entities for inference. Existing methods for inductive reasoning mostly mine the connections between entities, i.e., relational paths, without considering the nature of head and tail entities contained in the relational context. This paper proposes a novel method that captures both connections between entities and the intrinsic nature of entities, by simultaneously aggregating RElational Paths and cOntext with a unified hieRarchical Transformer framework, namely REPORT. REPORT relies solely on relation semantics and can naturally generalize to the fully-inductive setting, where KGs for training and inference have no common entities. In the experiments, REPORT performs consistently better than all baselines on almost all the eight version subsets of two fully-inductive datasets. Moreover. REPO
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#36896;&#24615;&#38382;&#39064;&#65292;&#20998;&#26512;&#20102;&#19982;&#20043;&#30456;&#20851;&#30340;&#26426;&#22120;&#21019;&#36896;&#24615;&#30340;&#38590;&#28857;&#21644;&#26131;&#28857;&#65292;&#24182;&#37325;&#28857;&#20998;&#26512;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#21019;&#24847;&#20135;&#19994;&#20013;&#30340;&#31038;&#20250;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.00008</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#36896;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Creativity of Large Language Models. (arXiv:2304.00008v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00008
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#36896;&#24615;&#38382;&#39064;&#65292;&#20998;&#26512;&#20102;&#19982;&#20043;&#30456;&#20851;&#30340;&#26426;&#22120;&#21019;&#36896;&#24615;&#30340;&#38590;&#28857;&#21644;&#26131;&#28857;&#65292;&#24182;&#37325;&#28857;&#20998;&#26512;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#21019;&#24847;&#20135;&#19994;&#20013;&#30340;&#31038;&#20250;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#27491;&#22312;&#39072;&#35206;&#20154;&#24037;&#26234;&#33021;&#30340;&#22810;&#20010;&#39046;&#22495;&#12290;&#20854;&#20013;&#26368;&#26174;&#33879;&#30340;&#24212;&#29992;&#20043;&#19968;&#26159;&#21019;&#20316;&#65292;&#20363;&#22914;&#35799;&#27468;&#25110;&#25925;&#20107;&#65306;&#29983;&#25104;&#30340;&#36755;&#20986;&#36890;&#24120;&#20855;&#26377;&#24778;&#20154;&#30340;&#36136;&#37327;&#12290;&#20294;&#26159;&#65292;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#26159;&#65306;LLMs&#30495;&#30340;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#21019;&#36896;&#24615;&#30340;&#21527;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#21019;&#36896;&#24615;&#29702;&#35770;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;LLMs&#30340;&#21457;&#23637;&#65292;&#25506;&#35752;&#20102;&#20851;&#38190;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#21644;&#25361;&#25112;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#19982;LLMs&#30456;&#20851;&#30340;&#26426;&#22120;&#21019;&#36896;&#24615;&#26041;&#38754;&#30830;&#23450;&#20102;&#19968;&#32452;&#8220;&#26131;&#8221;&#21644;&#8220;&#38590;&#8221;&#38382;&#39064;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#21019;&#24847;&#20135;&#19994;&#20013;&#30340;&#31038;&#20250;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are revolutionizing several areas of Artificial Intelligence. One of the most remarkable applications is creative writing, e.g., poetry or storytelling: the generated outputs are often of astonishing quality. However, a natural question arise: can LLMs really be considered creative? In this article we firstly analyze the development of LLMs under the lens of creativity theories, investigating the key open questions and challenges. Then, we identify a set of "easy" and "hard" problems in machine creativity, discussing them in relation to LLMs. Finally, we analyze the societal impact of these technologies with a particular focus on the creative industries.
&lt;/p&gt;</description></item><item><title>&#27169;&#22411;&#22522;&#20110;&#36798;&#23572;&#25991;&#33258;&#28982;&#36873;&#25321;&#65292;&#32467;&#21512;&#20989;&#25968;&#36873;&#25321;&#21644;&#36816;&#31639;&#31526;&#36873;&#25321;&#20004;&#20010;&#36807;&#31243;&#65292;&#36890;&#36807;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#26500;&#24314;&#29702;&#35770;&#65292;&#21487;&#33258;&#21160;&#21457;&#29616;&#21644;&#34920;&#31034;&#33258;&#28982;&#23450;&#24459;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#27169;&#25311;&#22810;&#39046;&#22495;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#19968;&#31181;&#26032;&#26041;&#27861;&#35299;&#20915;&#25551;&#36848;&#33258;&#28982;&#23450;&#24459;&#30340;&#20005;&#26684;&#25968;&#23398;&#27169;&#22411;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.17607</link><description>&lt;p&gt;
&#21457;&#29616;&#33258;&#28982;&#23450;&#24459;&#30340;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Machine learning for discovering laws of nature. (arXiv:2303.17607v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17607
&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#22522;&#20110;&#36798;&#23572;&#25991;&#33258;&#28982;&#36873;&#25321;&#65292;&#32467;&#21512;&#20989;&#25968;&#36873;&#25321;&#21644;&#36816;&#31639;&#31526;&#36873;&#25321;&#20004;&#20010;&#36807;&#31243;&#65292;&#36890;&#36807;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#26500;&#24314;&#29702;&#35770;&#65292;&#21487;&#33258;&#21160;&#21457;&#29616;&#21644;&#34920;&#31034;&#33258;&#28982;&#23450;&#24459;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#27169;&#25311;&#22810;&#39046;&#22495;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#19968;&#31181;&#26032;&#26041;&#27861;&#35299;&#20915;&#25551;&#36848;&#33258;&#28982;&#23450;&#24459;&#30340;&#20005;&#26684;&#25968;&#23398;&#27169;&#22411;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35266;&#31890;&#23376;&#36981;&#24490;&#37327;&#23376;&#21147;&#23398;&#30340;&#21407;&#29702;&#8212;&#8212;&#37027;&#20040;&#23439;&#35266;&#21644;&#24494;&#35266;&#19990;&#30028;&#20043;&#38388;&#30340;&#26126;&#30830;&#30028;&#38480;&#22312;&#21738;&#37324;&#21602;&#65311;&#27491;&#26159;&#36825;&#20010;&#8220;&#35299;&#37322;&#38382;&#39064;&#8221;&#20419;&#20351;&#34203;&#23450;&#35860;&#25552;&#20986;&#20102;&#20182;&#33879;&#21517;&#30340;&#24605;&#24819;&#23454;&#39564;&#65288;&#19968;&#21482;&#21516;&#26102;&#27515;&#20129;&#21644;&#27963;&#30528;&#30340;&#29483;&#65289;&#65292;&#24341;&#21457;&#20102;&#20851;&#20110;&#37327;&#23376;&#27979;&#37327;&#38382;&#39064;&#30340;&#28608;&#28872;&#20105;&#35770;&#65292;&#20294;&#33267;&#20170;&#20173;&#27809;&#26377;&#20196;&#20154;&#28385;&#24847;&#30340;&#31572;&#26696;&#12290;&#36825;&#27491;&#26159;&#25551;&#36848;&#33258;&#28982;&#23450;&#24459;&#30340;&#20005;&#26684;&#25968;&#23398;&#27169;&#22411;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36798;&#23572;&#25991;&#33258;&#28982;&#36873;&#25321;&#30340;&#35745;&#31639;&#27169;&#22411;&#26469;&#25551;&#36848;&#21644;&#29702;&#35299;&#33258;&#28982;&#23450;&#24459;&#12290;&#23454;&#38469;&#19978;&#65292;&#26080;&#35770;&#26159;&#23439;&#35266;&#31890;&#23376;&#12289;&#24494;&#35266;&#30005;&#23376;&#36824;&#26159;&#23433;&#20840;&#38382;&#39064;&#65292;&#23427;&#20204;&#37117;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#23454;&#20307;&#65292;&#36825;&#20010;&#23454;&#20307;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#21464;&#21270;&#65292;&#21487;&#20197;&#29992;&#29366;&#24577;&#21644;&#20540;&#32452;&#25104;&#30340;&#25968;&#25454;&#24207;&#21015;&#26469;&#25551;&#36848;&#12290;&#35266;&#23519;&#32773;&#21487;&#20197;&#20174;&#36825;&#20010;&#25968;&#25454;&#24207;&#21015;&#20013;&#23398;&#20064;&#65292;&#26500;&#24314;&#29702;&#35770;&#65288;&#36890;&#24120;&#30001;&#20989;&#25968;&#21644;&#24494;&#20998;&#26041;&#31243;&#32452;&#25104;&#65289;&#12290;&#25105;&#20204;&#19981;&#20877;&#20351;&#29992;&#29992;&#25143;&#30340;&#32463;&#39564;&#25110;&#36923;&#36753;&#26469;&#24314;&#27169;&#65292;&#32780;&#26159;&#20351;&#29992;&#25968;&#25454;&#12290;&#35745;&#31639;&#27169;&#22411;&#30340;&#26680;&#24515;&#22522;&#20110;&#20004;&#20010;&#36807;&#31243;&#65306;&#20989;&#25968;&#36873;&#25321;&#21644;&#36816;&#31639;&#31526;&#36873;&#25321;&#12290;&#20989;&#25968;&#36873;&#25321;&#36807;&#31243;&#31867;&#20284;&#20110;&#36798;&#23572;&#25991;&#30340;&#36827;&#21270;&#65292;&#20801;&#35768;&#20855;&#26377;&#20248;&#21183;&#29305;&#24449;&#30340;&#20989;&#25968;&#29983;&#23384;&#21644;&#32321;&#27542;&#65307;&#32780;&#36816;&#31639;&#31526;&#36873;&#25321;&#36807;&#31243;&#25429;&#25417;&#20102;&#33258;&#28982;&#23450;&#24459;&#30340;&#30456;&#20114;&#20381;&#23384;&#24615;&#65292;&#21487;&#20197;&#24179;&#34913;&#33258;&#28982;&#30028;&#20013;&#19981;&#21516;&#20989;&#25968;&#30340;&#20248;&#21183;&#12290;&#35813;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#20174;&#25968;&#25454;&#20013;&#33258;&#21160;&#21457;&#29616;&#21644;&#34920;&#31034;&#33258;&#28982;&#23450;&#24459;&#65292;&#24182;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#27169;&#25311;&#37327;&#23376;&#21147;&#23398;&#12289;&#32463;&#20856;&#21147;&#23398;&#21644;&#31995;&#32479;&#29983;&#29289;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
A microscopic particle obeys the principles of quantum mechanics -- so where is the sharp boundary between the macroscopic and microscopic worlds? It was this "interpretation problem" that prompted Schr\"odinger to propose his famous thought experiment (a cat that is simultaneously both dead and alive) and sparked a great debate about the quantum measurement problem, and there is still no satisfactory answer yet. This is precisely the inadequacy of rigorous mathematical models in describing the laws of nature. We propose a computational model to describe and understand the laws of nature based on Darwin's natural selection. In fact, whether it's a macro particle, a micro electron or a security, they can all be considered as an entity, the change of this entity over time can be described by a data series composed of states and values. An observer can learn from this data series to construct theories (usually consisting of functions and differential equations). We don't model with the us
&lt;/p&gt;</description></item><item><title>BlackVIP&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#40657;&#30418;&#35270;&#35273;&#25552;&#31034;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#27809;&#26377;&#21442;&#25968;&#21487;&#35775;&#38382;&#24615;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#22320;&#36866;&#24212;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#21327;&#35843;&#22120;&#21644;SPSA-GC&#32452;&#20214;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.14773</link><description>&lt;p&gt;
BlackVIP: &#38024;&#23545;&#31283;&#20581;&#36801;&#31227;&#23398;&#20064;&#30340;&#40657;&#30418;&#35270;&#35273;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
BlackVIP: Black-Box Visual Prompting for Robust Transfer Learning. (arXiv:2303.14773v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14773
&lt;/p&gt;
&lt;p&gt;
BlackVIP&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#40657;&#30418;&#35270;&#35273;&#25552;&#31034;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#27809;&#26377;&#21442;&#25968;&#21487;&#35775;&#38382;&#24615;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#22320;&#36866;&#24212;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#21327;&#35843;&#22120;&#21644;SPSA-GC&#32452;&#20214;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;PTMs&#65289;&#30340;&#20852;&#36215;&#65292;&#23558;&#36825;&#20123;&#27169;&#22411;&#24494;&#35843;&#20026;&#20247;&#22810;&#19979;&#28216;&#20219;&#21153;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#22823;&#22411;&#27169;&#22411;&#30340;&#21442;&#25968;&#25928;&#29575;&#36716;&#31227;&#23398;&#20064;&#65288;PETL&#65289;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;PETL&#26041;&#27861;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#20381;&#36182;&#20048;&#35266;&#30340;&#20551;&#35774;&#65306;1&#65289;PTM&#30340;&#25972;&#20010;&#21442;&#25968;&#38598;&#26159;&#21487;&#29992;&#30340;&#65292;2&#65289;&#20855;&#22791;&#36275;&#22815;&#22823;&#30340;&#20869;&#23384;&#23481;&#37327;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#22312;&#22823;&#22810;&#25968;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;PTMs&#20316;&#20026;&#40657;&#30418;API&#25110;&#19987;&#26377;&#36719;&#20214;&#25552;&#20379;&#65292;&#27809;&#26377;&#26126;&#30830;&#21442;&#25968;&#21487;&#35775;&#38382;&#24615;&#12290;&#27492;&#22806;&#65292;&#28385;&#36275;&#29616;&#20195;PTMs&#30340;&#22823;&#20869;&#23384;&#35201;&#27714;&#20063;&#24456;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#40657;&#30418;&#35270;&#35273;&#25552;&#31034;&#65288;BlackVIP&#65289;&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#36866;&#24212;PTMs&#65292;&#32780;&#19981;&#38656;&#35201;&#20851;&#20110;&#27169;&#22411;&#26550;&#26500;&#21644;&#21442;&#25968;&#30340;&#30693;&#35782;&#12290;BlackVIP&#21253;&#25324;&#20004;&#20010;&#32452;&#20214;&#65306;1&#65289;&#21327;&#35843;&#22120;&#21644;2&#65289;&#24102;&#26799;&#24230;&#26657;&#27491;&#30340;&#21516;&#26102;&#25200;&#21160;&#38543;&#26426;&#36924;&#36817;&#65288;SPSA-GC&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the surge of large-scale pre-trained models (PTMs), fine-tuning these models to numerous downstream tasks becomes a crucial problem. Consequently, parameter efficient transfer learning (PETL) of large models has grasped huge attention. While recent PETL methods showcase impressive performance, they rely on optimistic assumptions: 1) the entire parameter set of a PTM is available, and 2) a sufficiently large memory capacity for the fine-tuning is equipped. However, in most real-world applications, PTMs are served as a black-box API or proprietary software without explicit parameter accessibility. Besides, it is hard to meet a large memory requirement for modern PTMs. In this work, we propose black-box visual prompting (BlackVIP), which efficiently adapts the PTMs without knowledge about model architectures and parameters. BlackVIP has two components; 1) Coordinator and 2) simultaneous perturbation stochastic approximation with gradient correction (SPSA-GC). The Coordinator designs 
&lt;/p&gt;</description></item><item><title>NeuSE&#26159;&#19968;&#31181;&#29992;&#20110;&#23545;&#35937;&#30340;&#31070;&#32463;&#32593;&#32476;&#23884;&#20837;&#31639;&#27861;&#65292;&#25903;&#25345;&#23454;&#29616;&#19982;&#38271;&#26399;&#22330;&#26223;&#21464;&#21270;&#19968;&#33268;&#30340;&#23545;&#35937;&#31354;&#38388;&#29702;&#35299;&#12290;&#23427;&#21487;&#20197;&#32534;&#30721;&#23436;&#25972;&#30340;&#24418;&#29366;&#20449;&#24687;&#65292;&#24182;&#19982;&#29289;&#29702;&#19990;&#30028;&#20013;&#30340;&#23545;&#35937;&#21516;&#26102;&#21464;&#25442;&#65292;&#33021;&#22815;&#30452;&#25509;&#25512;&#26029;&#30456;&#23545;&#24103;&#21464;&#25442;&#21644;&#30456;&#26426;&#23039;&#24577;&#32422;&#26463;&#65292;&#24182;&#32500;&#25345;&#36866;&#24212;&#21464;&#21270;&#30340;&#36731;&#37327;&#32423;&#23545;&#35937;&#22320;&#22270;&#12290;</title><link>http://arxiv.org/abs/2303.07308</link><description>&lt;p&gt;
NeuSE: Neural SE(3)-&#31561;&#21464;&#23884;&#20837;&#29992;&#20110;&#19968;&#33268;&#30340;&#23545;&#35937;&#31354;&#38388;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
NeuSE: Neural SE(3)-Equivariant Embedding for Consistent Spatial Understanding with Objects. (arXiv:2303.07308v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07308
&lt;/p&gt;
&lt;p&gt;
NeuSE&#26159;&#19968;&#31181;&#29992;&#20110;&#23545;&#35937;&#30340;&#31070;&#32463;&#32593;&#32476;&#23884;&#20837;&#31639;&#27861;&#65292;&#25903;&#25345;&#23454;&#29616;&#19982;&#38271;&#26399;&#22330;&#26223;&#21464;&#21270;&#19968;&#33268;&#30340;&#23545;&#35937;&#31354;&#38388;&#29702;&#35299;&#12290;&#23427;&#21487;&#20197;&#32534;&#30721;&#23436;&#25972;&#30340;&#24418;&#29366;&#20449;&#24687;&#65292;&#24182;&#19982;&#29289;&#29702;&#19990;&#30028;&#20013;&#30340;&#23545;&#35937;&#21516;&#26102;&#21464;&#25442;&#65292;&#33021;&#22815;&#30452;&#25509;&#25512;&#26029;&#30456;&#23545;&#24103;&#21464;&#25442;&#21644;&#30456;&#26426;&#23039;&#24577;&#32422;&#26463;&#65292;&#24182;&#32500;&#25345;&#36866;&#24212;&#21464;&#21270;&#30340;&#36731;&#37327;&#32423;&#23545;&#35937;&#22320;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;NeuSE&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#23545;&#35937;&#30340;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;SE(3)-&#31561;&#21464;&#23884;&#20837;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22914;&#20309;&#25903;&#25345;&#23545;&#35937;SLAM&#20197;&#23454;&#29616;&#19982;&#38271;&#26399;&#22330;&#26223;&#21464;&#21270;&#19968;&#33268;&#30340;&#31354;&#38388;&#29702;&#35299;&#12290;NeuSE&#26159;&#20174;&#37096;&#20998;&#23545;&#35937;&#35266;&#27979;&#20013;&#21019;&#24314;&#30340;&#19968;&#32452;&#28508;&#22312;&#23545;&#35937;&#23884;&#20837;&#65292;&#22312;&#19982;&#29289;&#29702;&#19990;&#30028;&#20013;&#30340;&#23545;&#35937;&#21516;&#26102;SE(3)-&#31561;&#21464;&#21464;&#25442;&#30340;&#21516;&#26102;&#65292;&#23427;&#20316;&#20026;&#19968;&#20010;&#32039;&#20945;&#30340;&#28857;&#20113;&#26367;&#20195;&#23436;&#25972;&#30340;&#23545;&#35937;&#27169;&#22411;&#65292;&#32534;&#30721;&#20102;&#23436;&#25972;&#30340;&#24418;&#29366;&#20449;&#24687;&#12290;&#36890;&#36807;NeuSE&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#25512;&#26029;&#30340;&#28508;&#22312;&#20195;&#30721;&#20013;&#33719;&#24471;&#30456;&#23545;&#24103;&#21464;&#25442;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#20351;&#29992;NeuSE&#36827;&#34892;&#23545;&#35937;&#24418;&#29366;&#21644;&#23039;&#24577;&#29305;&#24449;&#21270;&#30340;SLAM&#33539;&#24335;&#21487;&#20197;&#29420;&#31435;&#36816;&#34892;&#25110;&#19982;&#20856;&#22411;&#30340;SLAM&#31995;&#32479;&#32467;&#21512;&#20351;&#29992;&#12290;&#23427;&#30452;&#25509;&#25512;&#26029;&#19982;&#26222;&#36890;SLAM&#23039;&#24577;&#22270;&#20248;&#21270;&#20860;&#23481;&#30340;SE(3)&#30456;&#26426;&#23039;&#24577;&#32422;&#26463;&#65292;&#21516;&#26102;&#36824;&#32500;&#25345;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#22320;&#22270;&#65292;&#33021;&#22815;&#36866;&#24212;&#29616;&#23454;&#19990;&#30028;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21253;&#21547;&#25913;&#21464;&#30340;&#23545;&#35937;&#30340;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#24207;&#21015;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present NeuSE, a novel Neural SE(3)-Equivariant Embedding for objects, and illustrate how it supports object SLAM for consistent spatial understanding with long-term scene changes. NeuSE is a set of latent object embeddings created from partial object observations. It serves as a compact point cloud surrogate for complete object models, encoding full shape information while transforming SE(3)-equivariantly in tandem with the object in the physical world. With NeuSE, relative frame transforms can be directly derived from inferred latent codes. Our proposed SLAM paradigm, using NeuSE for object shape and pose characterization, can operate independently or in conjunction with typical SLAM systems. It directly infers SE(3) camera pose constraints that are compatible with general SLAM pose graph optimization, while also maintaining a lightweight object-centric map that adapts to real-world changes. Our approach is evaluated on synthetic and real-world sequences featuring changed objects 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32593;&#32476;&#39532;&#23572;&#21487;&#22827;&#21183;&#21338;&#24328;&#20013;&#30340;&#23616;&#37096;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#36890;&#36807;&#23616;&#37096;&#21270;&#21644;&#20989;&#25968;&#36924;&#36817;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;&#65292;&#26679;&#26412;&#22797;&#26434;&#24230;&#19981;&#20381;&#36182;&#20110;&#20195;&#29702;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.04865</link><description>&lt;p&gt;
&#32593;&#32476;&#39532;&#23572;&#21487;&#22827;&#21183;&#21338;&#24328;&#20013;&#23616;&#37096;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;
Convergence Rates for Localized Actor-Critic in Networked Markov Potential Games. (arXiv:2303.04865v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32593;&#32476;&#39532;&#23572;&#21487;&#22827;&#21183;&#21338;&#24328;&#20013;&#30340;&#23616;&#37096;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#36890;&#36807;&#23616;&#37096;&#21270;&#21644;&#20989;&#25968;&#36924;&#36817;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;&#65292;&#26679;&#26412;&#22797;&#26434;&#24230;&#19981;&#20381;&#36182;&#20110;&#20195;&#29702;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#32593;&#32476;&#39532;&#23572;&#21487;&#22827;&#21183;&#21338;&#24328;&#20013;&#24341;&#20837;&#19968;&#31867;&#19982;&#32593;&#32476;&#20013;&#30340;&#33410;&#28857;&#30456;&#20851;&#30340;&#21183;&#21338;&#24328;&#12290;&#27599;&#20010;&#20195;&#29702;&#37117;&#26377;&#33258;&#24049;&#30340;&#23616;&#37096;&#21183;&#20989;&#25968;&#65292;&#27599;&#20010;&#20195;&#29702;&#30340;&#22870;&#21169;&#21482;&#21462;&#20915;&#20110;&#37051;&#22495;&#20013;&#20195;&#29702;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23616;&#37096;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65292;&#22240;&#20026;&#27599;&#20010;&#20195;&#29702;&#21482;&#20351;&#29992;&#23616;&#37096;&#20449;&#24687;&#65292;&#19981;&#38656;&#35201;&#35775;&#38382;&#20840;&#23616;&#29366;&#24577;&#12290;&#27492;&#22806;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#20351;&#29992;&#20989;&#25968;&#36924;&#36817;&#26469;&#20811;&#26381;&#32500;&#24230;&#28798;&#38590;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#22312;&#23616;&#37096;&#21270;&#35823;&#24046;&#21644;&#20989;&#25968;&#36924;&#36817;&#35823;&#24046;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#24179;&#22343;&#32435;&#20160;&#36951;&#25022;&#24230;&#37327;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;$\tilde{\mathcal {O}}(\tilde{\epsilon}^{-4})$&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#19981;&#20381;&#36182;&#20110;&#20195;&#29702;&#25968;&#37327;&#30340;&#22810;&#26234;&#33021;&#20307;&#31454;&#20105;&#21338;&#24328;&#30340;&#26377;&#38480;&#26679;&#26412;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a class of networked Markov potential games in which agents are associated with nodes in a network. Each agent has its own local potential function, and the reward of each agent depends only on the states and actions of the agents within a neighborhood. In this context, we propose a localized actor-critic algorithm. The algorithm is scalable since each agent uses only local information and does not need access to the global state. Further, the algorithm overcomes the curse of dimensionality through the use of function approximation. Our main results provide finite-sample guarantees up to a localization error and a function approximation error. Specifically, we achieve an $\tilde{\mathcal{O}}(\tilde{\epsilon}^{-4})$ sample complexity measured by the averaged Nash regret. This is the first finite-sample bound for multi-agent competitive games that does not depend on the number of agents.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;3D&#28857;&#20113;&#20013;&#36827;&#34892;&#26080;&#38480;&#25968;&#37327;&#25903;&#25745;&#26816;&#27979;&#30340;&#24320;&#25918;&#35789;&#27719;&#25903;&#25745;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#23398;&#20064;&#25903;&#25745;&#25991;&#26412;&#21644;&#28857;&#29305;&#24449;&#26469;&#21033;&#29992;&#25903;&#25745;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#38646;-shot&#26816;&#27979;&#65292;&#33021;&#22815;&#22312;&#27809;&#26377;&#27880;&#37322;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#20197;&#21069;&#26410;&#35265;&#21040;&#30340;&#25903;&#25745;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;OpenAD&#22312;&#21508;&#31181;&#35774;&#32622;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.02401</link><description>&lt;p&gt;
&#22312;3D&#28857;&#20113;&#20013;&#30340;&#24320;&#25918;&#35789;&#27719;&#25903;&#25745;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Open-Vocabulary Affordance Detection in 3D Point Clouds. (arXiv:2303.02401v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;3D&#28857;&#20113;&#20013;&#36827;&#34892;&#26080;&#38480;&#25968;&#37327;&#25903;&#25745;&#26816;&#27979;&#30340;&#24320;&#25918;&#35789;&#27719;&#25903;&#25745;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#23398;&#20064;&#25903;&#25745;&#25991;&#26412;&#21644;&#28857;&#29305;&#24449;&#26469;&#21033;&#29992;&#25903;&#25745;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#38646;-shot&#26816;&#27979;&#65292;&#33021;&#22815;&#22312;&#27809;&#26377;&#27880;&#37322;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#20197;&#21069;&#26410;&#35265;&#21040;&#30340;&#25903;&#25745;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;OpenAD&#22312;&#21508;&#31181;&#35774;&#32622;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25903;&#25745;&#26816;&#27979;&#26159;&#19968;&#20010;&#20855;&#26377;&#24191;&#27867;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#25903;&#25745;&#26816;&#27979;&#26041;&#27861;&#23616;&#38480;&#20110;&#39044;&#23450;&#20041;&#30340;&#25903;&#25745;&#26631;&#31614;&#65292;&#21487;&#33021;&#38480;&#21046;&#20102;&#26234;&#33021;&#26426;&#22120;&#20154;&#22312;&#22797;&#26434;&#21644;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#36866;&#24212;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24320;&#25918;&#35789;&#27719;&#25903;&#25745;&#26816;&#27979;&#65288;OpenAD&#65289;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;3D&#28857;&#20113;&#20013;&#26816;&#27979;&#26080;&#38480;&#25968;&#37327;&#30340;&#25903;&#25745;&#12290;&#36890;&#36807;&#21516;&#26102;&#23398;&#20064;&#25903;&#25745;&#25991;&#26412;&#21644;&#28857;&#29305;&#24449;&#65292;OpenAD&#25104;&#21151;&#22320;&#21033;&#29992;&#20102;&#25903;&#25745;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#38646;-shot&#26816;&#27979;&#65292;&#24182;&#33021;&#22815;&#22312;&#27809;&#26377;&#20219;&#20309;&#27880;&#37322;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#20197;&#21069;&#26410;&#35265;&#21040;&#30340;&#25903;&#25745;&#12290;&#22823;&#37327;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;OpenAD&#22312;&#21508;&#31181;&#25903;&#25745;&#26816;&#27979;&#35774;&#32622;&#19978;&#26377;&#25928;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;OpenAD&#22312;&#23454;&#38469;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Affordance detection is a challenging problem with a wide variety of robotic applications. Traditional affordance detection methods are limited to a predefined set of affordance labels, hence potentially restricting the adaptability of intelligent robots in complex and dynamic environments. In this paper, we present the Open-Vocabulary Affordance Detection (OpenAD) method, which is capable of detecting an unbounded number of affordances in 3D point clouds. By simultaneously learning the affordance text and the point feature, OpenAD successfully exploits the semantic relationships between affordances. Therefore, our proposed method enables zero-shot detection and can be able to detect previously unseen affordances without a single annotation example. Intensive experimental results show that OpenAD works effectively on a wide range of affordance detection setups and outperforms other baselines by a large margin. Additionally, we demonstrate the practicality of the proposed OpenAD in real
&lt;/p&gt;</description></item><item><title>OmniForce&#26159;&#19968;&#20010;&#22522;&#20110;&#20154;&#31867;&#20013;&#24515;&#12289;&#22823;&#27169;&#22411;&#21644;&#20113;&#36793;&#21327;&#21516;&#30340;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#26088;&#22312;&#35299;&#20915;&#24320;&#25918;&#29615;&#22659;&#19979;&#30340;AutoML&#38382;&#39064;&#12290;&#24403;&#21069;&#30340;AutoML&#31995;&#32479;&#21644;&#24179;&#21488;&#20302;&#25928;&#19988;&#35745;&#31639;&#22797;&#26434;&#65292;&#32780;OmniForce&#36890;&#36807;&#20154;&#26426;&#20132;&#20114;&#25216;&#26415;&#25913;&#36827;&#20102;&#36825;&#19968;&#29616;&#29366;&#12290;</title><link>http://arxiv.org/abs/2303.00501</link><description>&lt;p&gt;
OmniForce: &#22522;&#20110;&#20154;&#31867;&#20013;&#24515;&#12289;&#22823;&#27169;&#22411;&#21644;&#20113;&#36793;&#21327;&#21516;&#30340;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
OmniForce: On Human-Centered, Large Model Empowered and Cloud-Edge Collaborative AutoML System. (arXiv:2303.00501v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00501
&lt;/p&gt;
&lt;p&gt;
OmniForce&#26159;&#19968;&#20010;&#22522;&#20110;&#20154;&#31867;&#20013;&#24515;&#12289;&#22823;&#27169;&#22411;&#21644;&#20113;&#36793;&#21327;&#21516;&#30340;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#26088;&#22312;&#35299;&#20915;&#24320;&#25918;&#29615;&#22659;&#19979;&#30340;AutoML&#38382;&#39064;&#12290;&#24403;&#21069;&#30340;AutoML&#31995;&#32479;&#21644;&#24179;&#21488;&#20302;&#25928;&#19988;&#35745;&#31639;&#22797;&#26434;&#65292;&#32780;OmniForce&#36890;&#36807;&#20154;&#26426;&#20132;&#20114;&#25216;&#26415;&#25913;&#36827;&#20102;&#36825;&#19968;&#29616;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064; (AutoML) &#26088;&#22312;&#20197;&#26368;&#23569;&#20154;&#21147;&#25237;&#20837;&#26500;&#24314;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#34429;&#28982;&#22312;AutoML&#39046;&#22495;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#20197;&#20415;&#22312;&#26500;&#24314;&#20154;&#24037;&#26234;&#33021; (AI) &#24212;&#29992;&#31243;&#24207;&#26102;&#23558;&#20154;&#31867;&#25490;&#38500;&#22312;&#22806;&#65292;&#20294;&#40092;&#26377;&#25991;&#29486;&#20851;&#27880;AutoML&#22914;&#20309;&#22312;&#24320;&#25918;&#29615;&#22659;&#22330;&#26223;&#20013;&#24037;&#20316;&#65292;&#20363;&#22914;&#35757;&#32451;&#21644;&#26356;&#26032;&#22823;&#22411;&#27169;&#22411;&#12289;&#24037;&#19994;&#20379;&#24212;&#38142;&#25110;&#24037;&#19994;&#34394;&#25311;&#19990;&#30028;&#12290;&#22312;&#36825;&#20123;&#22330;&#26223;&#20013;&#65292;&#20154;&#20204;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#32463;&#24120;&#38754;&#20020;&#24320;&#25918;&#29615;&#36335;&#38382;&#39064;&#65306;&#20182;&#20204;&#24517;&#39035;&#19981;&#26029;&#25910;&#38598;&#25968;&#25454;&#12289;&#26356;&#26032;&#25968;&#25454;&#21644;&#27169;&#22411;&#12289;&#28385;&#36275;&#24320;&#21457;&#21644;&#37096;&#32626;&#29615;&#22659;&#30340;&#35201;&#27714;&#12289;&#25903;&#25345;&#22823;&#35268;&#27169;&#35774;&#22791;&#12289;&#20462;&#25913;&#35780;&#20272;&#25351;&#26631;&#31561;&#12290;&#20351;&#29992;&#32431;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#35299;&#20915;&#24320;&#25918;&#29615;&#22659;&#38382;&#39064;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#12289;&#35745;&#31639;&#36164;&#28304;&#21644;&#26469;&#33258;&#19987;&#32844;&#25968;&#25454;&#24037;&#31243;&#24072;&#30340;&#21162;&#21147;&#65292;&#20351;&#24471;&#24403;&#21069;&#30340;AutoML&#31995;&#32479;&#21644;&#24179;&#21488;&#20302;&#25928;&#19988;&#35745;&#31639;&#22797;&#26434;&#12290;&#20154;&#26426;&#20132;&#20114;&#25216;&#26415;&#26159;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated machine learning (AutoML) seeks to build ML models with minimal human effort. While considerable research has been conducted in the area of AutoML in general, aiming to take humans out of the loop when building artificial intelligence (AI) applications, scant literature has focused on how AutoML works well in open-environment scenarios such as the process of training and updating large models, industrial supply chains or the industrial metaverse, where people often face open-loop problems during the search process: they must continuously collect data, update data and models, satisfy the requirements of the development and deployment environment, support massive devices, modify evaluation metrics, etc. Addressing the open-environment issue with pure data-driven approaches requires considerable data, computing resources, and effort from dedicated data engineers, making current AutoML systems and platforms inefficient and computationally intractable. Human-computer interaction i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FedCLIP&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23454;&#29616;CLIP&#30340;&#24555;&#36895;&#27867;&#21270;&#21644;&#20010;&#24615;&#21270;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35774;&#35745;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#36866;&#37197;&#22120;&#65292;&#20805;&#20998;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#20449;&#24687;&#65292;&#24182;&#30830;&#20445;&#27169;&#22411;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#30340;&#23458;&#25143;&#31471;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#35757;&#32451;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.13485</link><description>&lt;p&gt;
FedCLIP&#65306;&#32852;&#37030;&#23398;&#20064;&#20013;&#29992;&#20110;CLIP&#30340;&#24555;&#36895;&#27867;&#21270;&#21644;&#20010;&#24615;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FedCLIP: Fast Generalization and Personalization for CLIP in Federated Learning. (arXiv:2302.13485v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13485
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FedCLIP&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23454;&#29616;CLIP&#30340;&#24555;&#36895;&#27867;&#21270;&#21644;&#20010;&#24615;&#21270;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35774;&#35745;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#36866;&#37197;&#22120;&#65292;&#20805;&#20998;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#20449;&#24687;&#65292;&#24182;&#30830;&#20445;&#27169;&#22411;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#30340;&#23458;&#25143;&#31471;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#35757;&#32451;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#19968;&#31181;&#20445;&#25252;&#38544;&#31169;&#35745;&#31639;&#30340;&#26032;&#33539;&#24335;&#24050;&#32463;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;FL&#38754;&#20020;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#38480;&#21046;&#20102;&#20854;&#23454;&#38469;&#24615;&#33021;&#65306;&#25968;&#25454;&#20998;&#24067;&#24322;&#36136;&#24615;&#21644;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#24102;&#26469;&#30340;&#39640;&#36164;&#28304;&#25104;&#26412;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#19981;&#21516;&#23458;&#25143;&#31471;&#20013;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#20351;&#24471;&#29616;&#26377;&#30340;FL&#31639;&#27861;&#38590;&#20197;&#25910;&#25947;&#65292;&#32780;&#39640;&#36164;&#28304;&#25104;&#26412;&#65288;&#21253;&#25324;&#35745;&#31639;&#21644;&#36890;&#20449;&#25104;&#26412;&#65289;&#22686;&#21152;&#20102;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#37096;&#32626;&#38590;&#24230;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#32780;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;FedCLIP&#65292;&#29992;&#20110;&#23454;&#29616;&#32852;&#37030;&#23398;&#20064;&#20013;CLIP&#30340;&#24555;&#36895;&#27867;&#21270;&#21644;&#20010;&#24615;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#36866;&#37197;&#22120;&#26469;&#36866;&#24212;&#22823;&#22411;&#27169;&#22411;CLIP&#65292;&#20854;&#20313;&#25805;&#20316;&#20165;&#20381;&#36182;&#20110;&#36866;&#37197;&#22120;&#12290;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#21487;&#20197;&#20805;&#20998;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#20449;&#24687;&#65292;&#24182;&#30830;&#20445;&#27169;&#22411;&#22312;&#29305;&#23450;&#20219;&#21153;&#30340;&#23458;&#25143;&#31471;&#20013;&#20855;&#26377;&#33258;&#36866;&#24212;&#24615;&#12290;&#21516;&#26102;&#65292;&#23567;&#35268;&#27169;&#25805;&#20316;&#21487;&#20197;&#32531;&#35299;&#35745;&#31639;&#36127;&#25285;&#21644;&#36890;&#20449;&#21387;&#21147;&#65292;&#25552;&#39640;&#27169;&#22411;&#35757;&#32451;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has emerged as a new paradigm for privacy-preserving computation in recent years. Unfortunately, FL faces two critical challenges that hinder its actual performance: data distribution heterogeneity and high resource costs brought by large foundation models. Specifically, the non-IID data in different clients make existing FL algorithms hard to converge while the high resource costs, including computational and communication costs that increase the deployment difficulty in real-world scenarios. In this paper, we propose an effective yet simple method, named FedCLIP, to achieve fast generalization and personalization for CLIP in federated learning. Concretely, we design an attention-based adapter for the large model, CLIP, and the rest operations merely depend on adapters. Lightweight adapters can make the most use of pretrained model information and ensure models be adaptive for clients in specific tasks. Simultaneously, small-scale operations can mitigate the co
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24635;&#32467;&#20102;&#26102;&#24577;&#22270;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#29616;&#29366;&#12289;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#25552;&#20379;&#20102;&#23398;&#20064;&#35774;&#32622;&#21644;&#20219;&#21153;&#30340;&#20005;&#26684;&#35268;&#33539;&#21270;&#20197;&#21450;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#26368;&#30456;&#20851;&#30340;&#24320;&#25918;&#25361;&#25112;&#65292;&#20174;&#30740;&#31350;&#21644;&#24212;&#29992;&#35282;&#24230;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2302.01018</link><description>&lt;p&gt;
&#26102;&#24577;&#22270;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65306;&#29616;&#29366;&#12289;&#25361;&#25112;&#21644;&#26426;&#36935;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks for temporal graphs: State of the art, open challenges, and opportunities. (arXiv:2302.01018v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01018
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24635;&#32467;&#20102;&#26102;&#24577;&#22270;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#29616;&#29366;&#12289;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#25552;&#20379;&#20102;&#23398;&#20064;&#35774;&#32622;&#21644;&#20219;&#21153;&#30340;&#20005;&#26684;&#35268;&#33539;&#21270;&#20197;&#21450;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#26368;&#30456;&#20851;&#30340;&#24320;&#25918;&#25361;&#25112;&#65292;&#20174;&#30740;&#31350;&#21644;&#24212;&#29992;&#35282;&#24230;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#32463;&#25104;&#20026;&#23398;&#20064;&#65288;&#38745;&#24577;&#65289;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#20027;&#35201;&#33539;&#20363;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#30340;&#31995;&#32479;&#26159;&#21160;&#24577;&#30340;&#65292;&#22240;&#20026;&#22270;&#21644;&#33410;&#28857;/&#36793;&#23646;&#24615;&#38543;&#30528;&#26102;&#38388;&#32780;&#21464;&#21270;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;GNN&#30340;&#26102;&#24577;&#22270;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#25193;&#23637;GNN&#33021;&#21147;&#30340;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#20851;&#20110;&#26102;&#24577;GNN&#30340;&#29616;&#29366;&#20840;&#38754;&#30340;&#27010;&#36848;&#65292;&#24341;&#20837;&#20102;&#23398;&#20064;&#35774;&#32622;&#21644;&#20219;&#21153;&#30340;&#20005;&#26684;&#35268;&#33539;&#21270;&#20197;&#21450;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#20197;&#34920;&#31034;&#21644;&#22788;&#29702;&#26102;&#24577;&#26041;&#38754;&#30340;&#29616;&#26377;&#26041;&#27861;&#12290;&#25105;&#20204;&#20174;&#30740;&#31350;&#21644;&#24212;&#29992;&#35282;&#24230;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#26368;&#30456;&#20851;&#30340;&#24320;&#25918;&#25361;&#25112;&#65292;&#32467;&#26463;&#20102;&#36825;&#39033;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have become the leading paradigm for learning on (static) graph-structured data. However, many real-world systems are dynamic in nature, since the graph and node/edge attributes change over time. In recent years, GNN-based models for temporal graphs have emerged as a promising area of research to extend the capabilities of GNNs. In this work, we provide the first comprehensive overview of the current state-of-the-art of temporal GNN, introducing a rigorous formalization of learning settings and tasks and a novel taxonomy categorizing existing approaches in terms of how the temporal aspect is represented and processed. We conclude the survey with a discussion of the most relevant open challenges for the field, from both research and application perspectives.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;IM-IAD&#24037;&#19994;&#21046;&#36896;&#20013;&#30340;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#22522;&#20934;&#65292;&#36890;&#36807;&#32479;&#19968;&#30340;&#35774;&#32622;&#35780;&#20272;&#20102;16&#20010;&#31639;&#27861;&#22312;7&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#26088;&#22312;&#35299;&#20915;&#30446;&#21069;&#36825;&#19968;&#39046;&#22495;&#30740;&#31350;&#30340;&#19981;&#35268;&#33539;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.13359</link><description>&lt;p&gt;
IM-IAD&#65306;&#24037;&#19994;&#21046;&#36896;&#20013;&#30340;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
IM-IAD: Industrial Image Anomaly Detection Benchmark in Manufacturing. (arXiv:2301.13359v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13359
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;IM-IAD&#24037;&#19994;&#21046;&#36896;&#20013;&#30340;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#22522;&#20934;&#65292;&#36890;&#36807;&#32479;&#19968;&#30340;&#35774;&#32622;&#35780;&#20272;&#20102;16&#20010;&#31639;&#27861;&#22312;7&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#26088;&#22312;&#35299;&#20915;&#30446;&#21069;&#36825;&#19968;&#39046;&#22495;&#30740;&#31350;&#30340;&#19981;&#35268;&#33539;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#65288;IAD&#65289;&#26159;&#24037;&#19994;&#21046;&#36896;&#20013;&#19968;&#39033;&#26032;&#20852;&#19988;&#37325;&#35201;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#12290;&#36817;&#24180;&#26469;&#65292;&#35768;&#22810;&#20808;&#36827;&#30340;&#31639;&#27861;&#24050;&#32463;&#34987;&#21457;&#24067;&#65292;&#20294;&#23427;&#20204;&#30340;&#24615;&#33021;&#23384;&#22312;&#24456;&#22823;&#24046;&#24322;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#32570;&#20047;&#23454;&#38469;&#30340;&#24037;&#19994;&#21046;&#36896;&#35774;&#32622;&#24456;&#21487;&#33021;&#38459;&#30861;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#21457;&#23637;&#21644;&#20351;&#29992;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;IAD&#26041;&#27861;&#23578;&#26410;&#32463;&#36807;&#31995;&#32479;&#35780;&#20272;&#12290;&#22240;&#27492;&#65292;&#36825;&#20351;&#24471;&#30740;&#31350;&#20154;&#21592;&#24456;&#38590;&#20998;&#26512;&#23427;&#20204;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;&#20026;&#19981;&#21516;&#25110;&#29305;&#27530;&#24773;&#20917;&#32780;&#35774;&#35745;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#24037;&#19994;&#21046;&#36896;&#35774;&#32622;&#26469;&#35780;&#20272;&#36825;&#20123;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#20960;&#20010;&#26041;&#38754;&#65292;&#22914;&#21508;&#31181;&#30417;&#30563;&#32423;&#21035;&#65288;&#26080;&#30417;&#30563;vs&#21322;&#30417;&#30563;&#65289;&#12289;&#23569;&#26679;&#26412;&#23398;&#20064;&#12289;&#25345;&#32493;&#23398;&#20064;&#12289;&#22122;&#22768;&#26631;&#31614;&#12289;&#20869;&#23384;&#20351;&#29992;&#21644;&#25512;&#26029;&#36895;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24039;&#22937;&#22320;&#26500;&#24314;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#22522;&#20934;&#65288;IM-IAD&#65289;&#65292;&#35813;&#22522;&#20934;&#22312;&#32479;&#19968;&#30340;&#35774;&#32622;&#19979;&#21253;&#25324;&#20102;16&#20010;&#31639;&#27861;&#21644;7&#20010;&#20027;&#27969;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image anomaly detection (IAD) is an emerging and vital computer vision task in industrial manufacturing (IM). Recently many advanced algorithms have been published, but their performance deviates greatly. We realize that the lack of actual IM settings most probably hinders the development and usage of these methods in real-world applications. As far as we know, IAD methods are not evaluated systematically. As a result, this makes it difficult for researchers to analyze them because they are designed for different or special cases. To solve this problem, we first propose a uniform IM setting to assess how well these algorithms perform, which includes several aspects, i.e., various levels of supervision (unsupervised vs. semi-supervised), few-shot learning, continual learning, noisy labels, memory usage, and inference speed. Moreover, we skillfully build a comprehensive image anomaly detection benchmark (IM-IAD) that includes 16 algorithms on 7 mainstream datasets with uniform settings. 
&lt;/p&gt;</description></item><item><title>D-Adaptation&#26159;&#19968;&#31181;&#21487;&#20197;&#33258;&#21160;&#35774;&#32622;&#23398;&#20064;&#29575;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#26368;&#23567;&#21270;&#20984;&#24615;Lipschitz&#20989;&#25968;&#65292;&#29992;&#20110;&#23454;&#29616;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#65292;&#32780;&#26080;&#38656;&#36229;&#21442;&#25968;&#65292;&#20063;&#26080;&#38656;&#39069;&#22806;&#23545;&#25968;&#22240;&#23376;&#25913;&#36827;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#33258;&#21160;&#21305;&#37197;&#25163;&#21160;&#35843;&#25972;&#30340;&#23398;&#20064;&#29575;&#12290;</title><link>http://arxiv.org/abs/2301.07733</link><description>&lt;p&gt;
&#36890;&#36807;D&#36866;&#24212;&#23454;&#29616;&#23398;&#20064;&#29575;&#33258;&#30001;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning-Rate-Free Learning by D-Adaptation. (arXiv:2301.07733v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07733
&lt;/p&gt;
&lt;p&gt;
D-Adaptation&#26159;&#19968;&#31181;&#21487;&#20197;&#33258;&#21160;&#35774;&#32622;&#23398;&#20064;&#29575;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#26368;&#23567;&#21270;&#20984;&#24615;Lipschitz&#20989;&#25968;&#65292;&#29992;&#20110;&#23454;&#29616;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#65292;&#32780;&#26080;&#38656;&#36229;&#21442;&#25968;&#65292;&#20063;&#26080;&#38656;&#39069;&#22806;&#23545;&#25968;&#22240;&#23376;&#25913;&#36827;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#33258;&#21160;&#21305;&#37197;&#25163;&#21160;&#35843;&#25972;&#30340;&#23398;&#20064;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
D&#36866;&#24212;&#26159;&#19968;&#31181;&#33258;&#21160;&#35774;&#32622;&#23398;&#20064;&#29575;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#28176;&#36817;&#22320;&#23454;&#29616;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#65292;&#29992;&#20110;&#26368;&#23567;&#21270;&#20984;&#24615;Lipschitz&#20989;&#25968;&#65292;&#26080;&#38656;&#22238;&#28335;&#25110;&#32447;&#24615;&#25628;&#32034;&#65292;&#24182;&#19988;&#27599;&#27493;&#26080;&#38656;&#36827;&#34892;&#39069;&#22806;&#30340;&#20989;&#25968;&#20540;&#25110;&#26799;&#24230;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#36825;&#19968;&#31867;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#26080;&#36229;&#21442;&#25968;&#19988;&#25910;&#25947;&#36895;&#29575;&#26080;&#38656;&#39069;&#22806;&#23545;&#25968;&#22240;&#23376;&#25913;&#36827;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#38024;&#23545;SGD&#21644;Adam&#21464;&#20307;&#23637;&#31034;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#20854;&#20013;&#35813;&#26041;&#27861;&#33258;&#21160;&#21305;&#37197;&#25163;&#21160;&#35843;&#25972;&#30340;&#23398;&#20064;&#29575;&#65292;&#22312;&#21313;&#22810;&#20010;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#24212;&#29992;&#65292;&#21253;&#25324;&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#38382;&#39064;&#12290;&#24320;&#28304;&#23454;&#29616;&#22312; \url{https://github.com/facebookresearch/dadaptation}.
&lt;/p&gt;
&lt;p&gt;
D-Adaptation is an approach to automatically setting the learning rate which asymptotically achieves the optimal rate of convergence for minimizing convex Lipschitz functions, with no back-tracking or line searches, and no additional function value or gradient evaluations per step. Our approach is the first hyper-parameter free method for this class without additional multiplicative log factors in the convergence rate. We present extensive experiments for SGD and Adam variants of our method, where the method automatically matches hand-tuned learning rates across more than a dozen diverse machine learning problems, including large-scale vision and language problems.  An open-source implementation is available at \url{https://github.com/facebookresearch/dadaptation}.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#39034;&#24207;&#36125;&#21494;&#26031;&#25512;&#26029;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#23613;&#31649;&#20351;&#29992;&#20102;&#30495;&#23454;&#21518;&#39564;&#65292;&#36825;&#31181;&#26041;&#27861;&#20173;&#26080;&#27861;&#38450;&#27490;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#21516;&#26102;&#65292;&#27169;&#22411;&#35823;&#24046;&#21644;&#20219;&#21153;&#25968;&#25454;&#19981;&#24179;&#34913;&#20063;&#20250;&#23548;&#33268;&#36830;&#32493;&#23398;&#20064;&#24615;&#33021;&#30340;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2301.01828</link><description>&lt;p&gt;
&#20851;&#20110;&#36830;&#32493;&#23398;&#20064;&#30340;&#39034;&#24207;&#36125;&#21494;&#26031;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
On Sequential Bayesian Inference for Continual Learning. (arXiv:2301.01828v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01828
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#39034;&#24207;&#36125;&#21494;&#26031;&#25512;&#26029;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#23613;&#31649;&#20351;&#29992;&#20102;&#30495;&#23454;&#21518;&#39564;&#65292;&#36825;&#31181;&#26041;&#27861;&#20173;&#26080;&#27861;&#38450;&#27490;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#21516;&#26102;&#65292;&#27169;&#22411;&#35823;&#24046;&#21644;&#20219;&#21153;&#25968;&#25454;&#19981;&#24179;&#34913;&#20063;&#20250;&#23548;&#33268;&#36830;&#32493;&#23398;&#20064;&#24615;&#33021;&#30340;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39034;&#24207;&#36125;&#21494;&#26031;&#25512;&#26029;&#21487;&#29992;&#20110;&#36830;&#32493;&#23398;&#20064;&#65292;&#20197;&#38450;&#27490;&#36807;&#21435;&#20219;&#21153;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#25552;&#20379;&#20449;&#24687;&#20016;&#23500;&#30340;&#20808;&#39564;&#12290;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#39034;&#24207;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#24182;&#27979;&#35797;&#26159;&#21542;&#26377;&#35775;&#38382;&#30495;&#23454;&#21518;&#39564;&#30340;&#20445;&#35777;&#21487;&#20197;&#38450;&#27490;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#20026;&#20102;&#20570;&#21040;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#20351;&#29992;&#21704;&#23494;&#39039;&#33945;&#29305;&#21345;&#27931;&#25191;&#34892;&#39034;&#24207;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#21704;&#23494;&#39039;&#33945;&#29305;&#21345;&#27931;&#26679;&#26412;&#25311;&#21512;&#23494;&#24230;&#20272;&#35745;&#22120;&#65292;&#23558;&#21518;&#39564;&#20256;&#25773;&#20026;&#26032;&#20219;&#21153;&#30340;&#20808;&#39564;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#26080;&#27861;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#35777;&#26126;&#20102;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#25191;&#34892;&#39034;&#24207;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#22256;&#38590;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#39034;&#24207;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#36830;&#32493;&#23398;&#20064;&#30340;&#31616;&#21333;&#20998;&#26512;&#31034;&#20363;&#65292;&#24182;&#24378;&#35843;&#20102;&#27169;&#22411;&#38169;&#35823;&#35828;&#26126;&#38382;&#39064;&#65292;&#21363;&#20351;&#36827;&#34892;&#20102;&#20934;&#30830;&#30340;&#25512;&#26029;&#65292;&#20063;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#30340;&#36830;&#32493;&#23398;&#20064;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20219;&#21153;&#25968;&#25454;&#19981;&#24179;&#34913;&#21487;&#33021;&#23548;&#33268;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential Bayesian inference can be used for continual learning to prevent catastrophic forgetting of past tasks and provide an informative prior when learning new tasks. We revisit sequential Bayesian inference and test whether having access to the true posterior is guaranteed to prevent catastrophic forgetting in Bayesian neural networks. To do this we perform sequential Bayesian inference using Hamiltonian Monte Carlo. We propagate the posterior as a prior for new tasks by fitting a density estimator on Hamiltonian Monte Carlo samples. We find that this approach fails to prevent catastrophic forgetting demonstrating the difficulty in performing sequential Bayesian inference in neural networks. From there we study simple analytical examples of sequential Bayesian inference and CL and highlight the issue of model misspecification which can lead to sub-optimal continual learning performance despite exact inference. Furthermore, we discuss how task data imbalances can cause forgetting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35777;&#26126;&#40065;&#26834;&#30340;&#22522;&#20110;&#26174;&#33879;&#24615;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#35299;&#37322;&#21402;&#24230;&#21644;&#31283;&#23450;&#39030;&#37096;&#26174;&#33879;&#29305;&#24449;&#65292;&#25913;&#36827;&#20102;&#35299;&#37322;&#30340;&#25968;&#20540;&#21644;&#32479;&#35745;&#31283;&#23450;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#32593;&#32476;&#21644;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.14106</link><description>&lt;p&gt;
&#21487;&#35777;&#26126;&#40065;&#26834;&#30340;&#22522;&#20110;&#26174;&#33879;&#24615;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Provable Robust Saliency-based Explanations. (arXiv:2212.14106v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14106
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35777;&#26126;&#40065;&#26834;&#30340;&#22522;&#20110;&#26174;&#33879;&#24615;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#35299;&#37322;&#21402;&#24230;&#21644;&#31283;&#23450;&#39030;&#37096;&#26174;&#33879;&#29305;&#24449;&#65292;&#25913;&#36827;&#20102;&#35299;&#37322;&#30340;&#25968;&#20540;&#21644;&#32479;&#35745;&#31283;&#23450;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#32593;&#32476;&#21644;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#40065;&#26834;&#35299;&#37322;&#23545;&#20110;&#24314;&#31435;&#20154;&#31867;&#23545;&#27169;&#22411;&#30340;&#20449;&#20219;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#20351;&#29992;&#39030;&#37096;-k&#30340;&#20132;&#38598;&#26469;&#35780;&#20272;&#35299;&#37322;&#30340;&#40065;&#26834;&#24615;&#26159;&#24120;&#29992;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#37117;&#22522;&#20110;$\ell_p$&#33539;&#25968;&#65292;&#20174;&#32780;&#22312;&#35780;&#20272;&#21644;&#20248;&#21270;&#30446;&#26631;&#20043;&#38388;&#23384;&#22312;&#19981;&#21305;&#37197;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#35299;&#37322;&#30340;&#21402;&#24230;&#26469;&#34913;&#37327;&#39030;&#37096;-k&#26174;&#33879;&#29305;&#24449;&#25490;&#21517;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#35774;&#35745;&#20102;&#22522;&#20110;&#19968;&#31181;&#26032;&#39062;&#21487;&#34892;&#30340;&#26367;&#20195;&#30446;&#26631;&#30340;R2ET&#31639;&#27861;&#65292;&#20197;&#39640;&#25928;&#22320;&#26368;&#22823;&#21270;&#21402;&#24230;&#24182;&#31283;&#23450;&#39030;&#37096;&#26174;&#33879;&#29305;&#24449;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;R2ET&#21644;&#23545;&#25239;&#35757;&#32451;&#20043;&#38388;&#30340;&#32852;&#31995;&#65307;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#20844;&#24335;&#21644;&#27867;&#21270;&#35823;&#24046;&#30028;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#26367;&#20195;&#30446;&#26631;&#21487;&#20197;&#25913;&#36827;&#35299;&#37322;&#30340;&#25968;&#20540;&#21644;&#32479;&#35745;&#31283;&#23450;&#24615;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;&#32593;&#32476;&#26550;&#26500;&#21644;&#25968;&#25454;&#27169;&#24577;&#36827;&#34892;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;R2ET&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust explanations of machine learning models are critical to establishing human trust in the models. The top-$k$ intersection is widely used to evaluate the robustness of explanations. However, most existing attacking and defense strategies are based on $\ell_p$ norms, thus creating a mismatch between the evaluation and optimization objectives. To this end, we define explanation thickness for measuring top-$k$ salient features ranking stability, and design the \textit{R2ET} algorithm based on a novel tractable surrogate to maximize the thickness and stabilize the top salient features efficiently. Theoretically, we prove a connection between R2ET and adversarial training; using a novel multi-objective optimization formulation and a generalization error bound, we further prove that the surrogate objective can improve both the numerical and statistical stability of the explanations. Experiments with a wide spectrum of network architectures and data modalities demonstrate that R2ET attai
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;Twitter&#19978;&#19982;MEV&#30456;&#20851;&#30340;&#35805;&#39064;&#65292;&#32467;&#26524;&#34920;&#26126;&#25512;&#25991;&#35752;&#35770;&#20102;&#28145;&#21051;&#30340;&#20262;&#29702;&#38382;&#39064;&#65292;&#21253;&#25324;&#23433;&#20840;&#12289;&#20844;&#24179;&#12289;&#24773;&#24863;&#24773;&#32490;&#21644;&#23545;MEV&#35299;&#20915;&#26041;&#26696;&#30340;&#28212;&#26395;&#12290;</title><link>http://arxiv.org/abs/2212.06951</link><description>&lt;p&gt;
&#21306;&#22359;&#38142;&#19978;&#30340;AI&#20262;&#29702;: &#22522;&#20110;Twitter&#25968;&#25454;&#30340;&#21306;&#22359;&#38142;&#23433;&#20840;&#20027;&#39064;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
AI Ethics on Blockchain: Topic Analysis on Twitter Data for Blockchain Security. (arXiv:2212.06951v3 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;Twitter&#19978;&#19982;MEV&#30456;&#20851;&#30340;&#35805;&#39064;&#65292;&#32467;&#26524;&#34920;&#26126;&#25512;&#25991;&#35752;&#35770;&#20102;&#28145;&#21051;&#30340;&#20262;&#29702;&#38382;&#39064;&#65292;&#21253;&#25324;&#23433;&#20840;&#12289;&#20844;&#24179;&#12289;&#24773;&#24863;&#24773;&#32490;&#21644;&#23545;MEV&#35299;&#20915;&#26041;&#26696;&#30340;&#28212;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21306;&#22359;&#38142;&#20351;&#29992;&#20998;&#24067;&#24335;&#32593;&#32476;&#35753;&#35745;&#31639;&#26426;&#31995;&#32479;&#26356;&#23433;&#20840;&#65292;&#20294;&#24403;&#21069;&#30340;&#21306;&#22359;&#38142;&#35774;&#35745;&#22312;&#20132;&#26131;&#39034;&#24207;&#26041;&#38754;&#23384;&#22312;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#30719;&#24037;&#21487;&#20197;&#37325;&#26032;&#25490;&#24207;&#20132;&#26131;&#20197;&#29983;&#25104;&#21033;&#28070;&#65292;&#36825;&#34987;&#31216;&#20026;&#30719;&#24037;&#21487;&#25552;&#21462;&#20215;&#20540;&#65288;MEV&#65289;&#38382;&#39064;&#12290;&#29616;&#26377;&#30740;&#31350;&#35748;&#20026;MEV&#26159;&#19968;&#20010;&#20005;&#37325;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#30693;&#21517;&#30340;Flashbots&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#22823;&#22810;&#20998;&#26512;&#20102;&#21306;&#22359;&#38142;&#25968;&#25454;&#65292;&#36825;&#21487;&#33021;&#26080;&#27861;&#25429;&#25417;&#21040;MEV&#22312;&#26356;&#24191;&#27867;&#30340;AI&#31038;&#20250;&#20013;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24212;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26041;&#27861;&#20840;&#38754;&#20998;&#26512;&#20102;MEV&#25512;&#25991;&#20013;&#30340;&#35805;&#39064;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#36229;&#36807;20,000&#20010;MEV&#21644;Flashbots&#26631;&#31614;&#30340;&#25512;&#25991;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#35805;&#39064;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#20123;&#25512;&#25991;&#35752;&#35770;&#20102;&#28145;&#21051;&#30340;&#20262;&#29702;&#38382;&#39064;&#65292;&#21253;&#25324;&#23433;&#20840;&#12289;&#20844;&#24179;&#12289;&#24773;&#24863;&#24773;&#32490;&#21644;&#23545;MEV&#35299;&#20915;&#26041;&#26696;&#30340;&#28212;&#26395;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#21306;&#22359;&#38142;&#19978;MEV&#27963;&#21160;&#30340;&#20849;&#21516;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Blockchain has empowered computer systems to be more secure using a distributed network. However, the current blockchain design suffers from fairness issues in transaction ordering. Miners are able to reorder transactions to generate profits, the so-called miner extractable value (MEV). Existing research recognizes MEV as a severe security issue and proposes potential solutions, including prominent Flashbots. However, previous studies have mostly analyzed blockchain data, which might not capture the impacts of MEV in a much broader AI society. Thus, in this research, we applied natural language processing (NLP) methods to comprehensively analyze topics in tweets on MEV. We collected more than 20000 tweets with MEV and Flashbots hashtags and analyzed their topics. Our results show that the tweets discussed profound topics of ethical concern, including security, equity, emotional sentiments, and the desire for solutions to MEV. We also identify the co-movements of MEV activities on block
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#22810;&#30446;&#26631; A* &#31639;&#27861;&#65292;&#31216;&#20026; RME-MOA*&#65292;&#26088;&#22312;&#35299;&#20915;&#22810;&#30446;&#26631;&#26368;&#30701;&#36335;&#24452;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#26469;&#25552;&#39640;&#31639;&#27861;&#30340;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#36739;&#20302;&#30340;&#36816;&#34892;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2212.03712</link><description>&lt;p&gt;
&#22686;&#24378;&#30340;&#22810;&#30446;&#26631; A* &#31639;&#27861;&#19982;&#37096;&#20998;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Enhanced Multi-Objective A* with Partial Expansion. (arXiv:2212.03712v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#22810;&#30446;&#26631; A* &#31639;&#27861;&#65292;&#31216;&#20026; RME-MOA*&#65292;&#26088;&#22312;&#35299;&#20915;&#22810;&#30446;&#26631;&#26368;&#30701;&#36335;&#24452;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#26469;&#25552;&#39640;&#31639;&#27861;&#30340;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#36739;&#20302;&#30340;&#36816;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#26368;&#30701;&#36335;&#24452;&#38382;&#39064;(MO-SPP)&#36890;&#24120;&#22312;&#22270;&#19978;&#25552;&#20986;&#65292;&#23427;&#30830;&#23450;&#20102;&#19968;&#32452;&#20174;&#36215;&#22987;&#39030;&#28857;&#21040;&#30446;&#26631;&#39030;&#28857;&#30340;&#36335;&#24452;&#65292;&#21516;&#26102;&#20248;&#21270;&#22810;&#20010;&#30446;&#26631;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#19981;&#23384;&#22312;&#19968;&#26465;&#36335;&#24452;&#33021;&#22815;&#21516;&#26102;&#20248;&#21270;&#25152;&#26377;&#30446;&#26631;&#65292;&#22240;&#27492;&#35813;&#38382;&#39064;&#26088;&#22312;&#23547;&#25214;&#19968;&#32452;&#25152;&#35859;&#30340; Pareto-&#26368;&#20248;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#24320;&#21457;&#20102;&#20960;&#31181;&#22810;&#30446;&#26631; A* (MOA*) &#31639;&#27861;&#26469;&#24555;&#36895;&#35745;&#31639;&#20855;&#26377;&#36136;&#37327;&#20445;&#35777;&#30340;&#35299;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123; MOA* &#31639;&#27861;&#22312;&#20869;&#23384;&#20351;&#29992;&#26041;&#38754;&#32463;&#24120;&#23384;&#22312;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#24403;&#22270;&#30340;&#20998;&#25903;&#22240;&#23376;(&#21363;&#20219;&#24847;&#39030;&#28857;&#30340;&#37051;&#23621;&#25968;&#30446;)&#36739;&#22823;&#26102;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#20943;&#23569; MOA* &#30340;&#39640;&#20869;&#23384;&#28040;&#32791;&#65292;&#21516;&#26102;&#20960;&#20046;&#19981;&#22686;&#21152;&#36816;&#34892;&#26102;&#38388;&#12290;&#36890;&#36807;&#25512;&#24191;&#21644;&#32479;&#19968;&#20960;&#31181;&#21333;&#30446;&#26631;&#21644;&#22810;&#30446;&#26631;&#25628;&#32034;&#31639;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; Runtime and Memory Efficient MOA* (RME-MOA*) &#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#38388;&#21644;&#20869;&#23384;&#20043;&#38388;&#27714;&#24471;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Multi-Objective Shortest Path Problem (MO-SPP), typically posed on a graph, determines a set of paths from a start vertex to a destination vertex while optimizing multiple objectives. In general, there does not exist a single solution path that can simultaneously optimize all the objectives and the problem thus seeks to find a set of so-called Pareto-optimal solutions. To address this problem, several Multi-Objective A* (MOA*) algorithms were recently developed to quickly compute solutions with quality guarantees. However, these MOA* algorithms often suffer from high memory usage, especially when the branching factor (i.e. the number of neighbors of any vertex) of the graph is large. This work thus aims at reducing the high memory consumption of MOA* with little increase in the runtime. By generalizing and unifying several single- and multi-objective search algorithms, we develop the Runtime and Memory Efficient MOA* (RME-MOA*) approach, which can balance between runtime and memory
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#20855;&#26377;&#30701;&#26399;&#12289;&#24773;&#33410;&#21644;&#35821;&#20041;&#20869;&#23384;&#31995;&#32479;&#30340;&#26426;&#22120;&#20195;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#24314;&#27169;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#30701;&#26399;&#35760;&#24518;&#30340;&#31649;&#29702;&#21644;&#23384;&#20648;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#20154;&#31867;&#35760;&#24518;&#31995;&#32479;&#32467;&#26500;&#30340;&#20195;&#29702;&#27604;&#27809;&#26377;&#35813;&#32467;&#26500;&#30340;&#20195;&#29702;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2212.02098</link><description>&lt;p&gt;
&#19968;&#20010;&#20855;&#26377;&#30701;&#26399;&#12289;&#24773;&#33410;&#21644;&#35821;&#20041;&#20869;&#23384;&#31995;&#32479;&#30340;&#26426;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Machine with Short-Term, Episodic, and Semantic Memory Systems. (arXiv:2212.02098v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#20855;&#26377;&#30701;&#26399;&#12289;&#24773;&#33410;&#21644;&#35821;&#20041;&#20869;&#23384;&#31995;&#32479;&#30340;&#26426;&#22120;&#20195;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#24314;&#27169;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#30701;&#26399;&#35760;&#24518;&#30340;&#31649;&#29702;&#21644;&#23384;&#20648;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#20154;&#31867;&#35760;&#24518;&#31995;&#32479;&#32467;&#26500;&#30340;&#20195;&#29702;&#27604;&#27809;&#26377;&#35813;&#32467;&#26500;&#30340;&#20195;&#29702;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#35748;&#30693;&#31185;&#23398;&#29702;&#35770;&#20013;&#26174;&#24615;&#20154;&#31867;&#35760;&#24518;&#31995;&#32479;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#20855;&#26377;&#30701;&#26399;&#12289;&#24773;&#33410;&#21644;&#35821;&#20041;&#35760;&#24518;&#31995;&#32479;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#27599;&#20010;&#35760;&#24518;&#31995;&#32479;&#37117;&#29992;&#30693;&#35782;&#22270;&#35889;&#24314;&#27169;&#12290;&#20026;&#20102;&#35780;&#20272;&#35813;&#31995;&#32479;&#24182;&#20998;&#26512;&#35813;&#20195;&#29702;&#30340;&#34892;&#20026;&#65292;&#25105;&#20204;&#35774;&#35745;&#24182;&#21457;&#24067;&#20102;&#25105;&#20204;&#33258;&#24049;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#29615;&#22659;&#8220;&#25151;&#38388;&#8221;&#65292;&#22312;&#36825;&#20010;&#29615;&#22659;&#20013;&#65292;&#20195;&#29702;&#24517;&#39035;&#23398;&#20064;&#22914;&#20309;&#32534;&#30721;&#12289;&#23384;&#20648;&#21644;&#26816;&#32034;&#35760;&#24518;&#65292;&#36890;&#36807;&#22238;&#31572;&#38382;&#39064;&#26469;&#26368;&#22823;&#21270;&#22238;&#25253;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#22522;&#20110;&#28145;&#24230;Q&#23398;&#20064;&#30340;&#20195;&#29702;&#25104;&#21151;&#23398;&#20064;&#20102;&#30701;&#26399;&#35760;&#24518;&#26159;&#21542;&#24212;&#35813;&#34987;&#36951;&#24536;&#65292;&#36824;&#26159;&#24212;&#35813;&#23384;&#20648;&#22312;&#24773;&#33410;&#25110;&#35821;&#20041;&#35760;&#24518;&#31995;&#32479;&#20013;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20855;&#26377;&#31867;&#20154;&#35760;&#24518;&#31995;&#32479;&#30340;&#20195;&#29702;&#22312;&#29615;&#22659;&#20013;&#34920;&#29616;&#20248;&#20110;&#27809;&#26377;&#36825;&#31181;&#35760;&#24518;&#32467;&#26500;&#30340;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the cognitive science theory of the explicit human memory systems, we have modeled an agent with short-term, episodic, and semantic memory systems, each of which is modeled with a knowledge graph. To evaluate this system and analyze the behavior of this agent, we designed and released our own reinforcement learning agent environment, "the Room", where an agent has to learn how to encode, store, and retrieve memories to maximize its return by answering questions. We show that our deep Q-learning based agent successfully learns whether a short-term memory should be forgotten, or rather be stored in the episodic or semantic memory systems. Our experiments indicate that an agent with human-like memory systems can outperform an agent without this memory structure in the environment.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31070;&#32463;&#20613;&#37324;&#21494;&#28388;&#27874;&#22120;&#32452;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26082;&#22312;&#31354;&#38388;&#19978;&#21448;&#22312;&#39057;&#29575;&#19978;&#20998;&#35299;&#20449;&#21495;&#65292;&#36890;&#36807;&#20351;&#29992;&#20613;&#37324;&#21494;&#32534;&#30721;&#29305;&#23450;&#39057;&#29575;&#26469;&#23384;&#20648;&#27599;&#20010;&#32593;&#26684;&#65292;&#36825;&#19968;&#26041;&#27861;&#22312;2D&#22270;&#20687;&#25311;&#21512;&#12289;3D&#24418;&#29366;&#37325;&#24314;&#21644;&#31070;&#32463;&#36752;&#23556;&#22330;&#31561;&#22810;&#20010;&#20219;&#21153;&#19978;&#65292;&#34920;&#29616;&#20986;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#30340;&#27169;&#22411;&#32039;&#20945;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2212.01735</link><description>&lt;p&gt;
&#31070;&#32463;&#20613;&#37324;&#21494;&#28388;&#27874;&#22120;&#32452;
&lt;/p&gt;
&lt;p&gt;
Neural Fourier Filter Bank. (arXiv:2212.01735v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01735
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31070;&#32463;&#20613;&#37324;&#21494;&#28388;&#27874;&#22120;&#32452;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26082;&#22312;&#31354;&#38388;&#19978;&#21448;&#22312;&#39057;&#29575;&#19978;&#20998;&#35299;&#20449;&#21495;&#65292;&#36890;&#36807;&#20351;&#29992;&#20613;&#37324;&#21494;&#32534;&#30721;&#29305;&#23450;&#39057;&#29575;&#26469;&#23384;&#20648;&#27599;&#20010;&#32593;&#26684;&#65292;&#36825;&#19968;&#26041;&#27861;&#22312;2D&#22270;&#20687;&#25311;&#21512;&#12289;3D&#24418;&#29366;&#37325;&#24314;&#21644;&#31070;&#32463;&#36752;&#23556;&#22330;&#31561;&#22810;&#20010;&#20219;&#21153;&#19978;&#65292;&#34920;&#29616;&#20986;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#30340;&#27169;&#22411;&#32039;&#20945;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#20379;&#39640;&#25928;&#19988;&#39640;&#24230;&#35814;&#32454;&#30340;&#37325;&#26500;&#12290;&#21463;&#23567;&#27874;&#21551;&#21457;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#19968;&#20010;&#31070;&#32463;&#22330;&#65292;&#26082;&#22312;&#31354;&#38388;&#19978;&#21448;&#22312;&#39057;&#29575;&#19978;&#20998;&#35299;&#20449;&#21495;&#12290;&#25105;&#20204;&#36981;&#24490;&#26368;&#36817;&#30340;&#22522;&#20110;&#32593;&#26684;&#30340;&#31354;&#38388;&#20998;&#35299;&#33539;&#20363;&#65292;&#20294;&#19982;&#29616;&#26377;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#36890;&#36807;&#20613;&#37324;&#21494;&#29305;&#24449;&#32534;&#30721;&#40723;&#21169;&#22312;&#27599;&#20010;&#32593;&#26684;&#20013;&#23384;&#20648;&#29305;&#23450;&#30340;&#39057;&#29575;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#24102;&#27491;&#24358;&#28608;&#27963;&#20989;&#25968;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65292;&#20197;&#22312;&#36866;&#24403;&#30340;&#23618;&#27425;&#19978;&#25509;&#21463;&#36825;&#20123;&#20613;&#37324;&#21494;&#32534;&#30721;&#30340;&#29305;&#24449;&#65292;&#20197;&#20351;&#39640;&#39057;&#32452;&#20214;&#20381;&#27425;&#32047;&#31215;&#22312;&#20302;&#39057;&#32452;&#20214;&#20043;&#19978;&#65292;&#26368;&#21518;&#23558;&#23427;&#20204;&#30456;&#21152;&#20197;&#24418;&#25104;&#26368;&#32456;&#36755;&#20986;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#65288;2D&#22270;&#20687;&#25311;&#21512;&#65292;3D&#24418;&#29366;&#37325;&#24314;&#21644;&#31070;&#32463;&#36752;&#23556;&#22330;&#65289;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#21253;&#25324;&#27169;&#22411;&#32039;&#20945;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#20197;&#22312;https://github.com/ubc-vision/NFFB&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel method to provide efficient and highly detailed reconstructions. Inspired by wavelets, we learn a neural field that decompose the signal both spatially and frequency-wise. We follow the recent grid-based paradigm for spatial decomposition, but unlike existing work, encourage specific frequencies to be stored in each grid via Fourier features encodings. We then apply a multi-layer perceptron with sine activations, taking these Fourier encoded features in at appropriate layers so that higher-frequency components are accumulated on top of lower-frequency components sequentially, which we sum up to form the final output. We demonstrate that our method outperforms the state of the art regarding model compactness and convergence speed on multiple tasks: 2D image fitting, 3D shape reconstruction, and neural radiance fields. Our code is available at https://github.com/ubc-vision/NFFB.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26465;&#20214;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#22312;&#35299;&#20915;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#35813;&#26041;&#27861;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#24314;&#27169;&#20026;&#22238;&#25253;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#36991;&#20813;&#21160;&#24577;&#35268;&#21010;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#32422;&#26463;&#21644;&#25216;&#33021;&#20316;&#20026;&#26465;&#20214;&#21464;&#37327;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.15657</link><description>&lt;p&gt;
&#26465;&#20214;&#29983;&#25104;&#24314;&#27169;&#26159;&#21542;&#36275;&#20197;&#35299;&#20915;&#20915;&#31574;&#38382;&#39064;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Conditional Generative Modeling all you need for Decision-Making?. (arXiv:2211.15657v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15657
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26465;&#20214;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#22312;&#35299;&#20915;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#35813;&#26041;&#27861;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#24314;&#27169;&#20026;&#22238;&#25253;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#36991;&#20813;&#21160;&#24577;&#35268;&#21010;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#32422;&#26463;&#21644;&#25216;&#33021;&#20316;&#20026;&#26465;&#20214;&#21464;&#37327;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#26465;&#20214;&#29983;&#25104;&#24314;&#27169;&#30340;&#25913;&#36827;&#20351;&#24471;&#20165;&#20973;&#35821;&#35328;&#25551;&#36848;&#23601;&#33021;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#25506;&#35752;&#36825;&#20123;&#26041;&#27861;&#26159;&#21542;&#21487;&#20197;&#30452;&#25509;&#35299;&#20915;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#12290;&#25105;&#20204;&#20174;&#26465;&#20214;&#29983;&#25104;&#24314;&#27169;&#30340;&#35270;&#35282;&#26469;&#30475;&#24453;&#20915;&#31574;&#65292;&#32780;&#38750;&#24378;&#21270;&#23398;&#20064;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#21487;&#20197;&#20248;&#20110;&#29616;&#26377;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#31574;&#30053;&#24314;&#27169;&#20026;&#22238;&#25253;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36991;&#20813;&#21160;&#24577;&#35268;&#21010;&#30340;&#38656;&#35201;&#65292;&#36827;&#32780;&#28040;&#38500;&#20256;&#32479;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#25152;&#24102;&#26469;&#30340;&#35768;&#22810;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#32771;&#34385;&#20004;&#20010;&#20854;&#20182;&#30340;&#26465;&#20214;&#21464;&#37327;&#65306;&#32422;&#26463;&#21644;&#25216;&#33021;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#23558;&#31574;&#30053;&#24314;&#27169;&#20026;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20165;&#23545;&#21333;&#20010;&#32422;&#26463;&#25110;&#25216;&#33021;&#36827;&#34892;&#26465;&#20214;&#35774;&#32622;&#65292;&#23548;&#33268;&#27979;&#35797;&#26102;&#30340;&#34892;&#20026;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone. We investigate whether these methods can directly address the problem of sequential decision-making. We view decision-making not through the lens of reinforcement learning (RL), but rather through conditional generative modeling. To our surprise, we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks. By modeling a policy as a return-conditional diffusion model, we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL. We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables: constraints and skills. Conditioning on a single constraint or skill during training leads to behaviors at test-time that 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#40657;&#30418;&#39564;&#35777;&#26041;&#27861;&#26469;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#39550;&#39542;&#30340;&#23433;&#20840;&#24615;&#33021;&#30340;&#33258;&#25105;&#25913;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21457;&#29616;&#33258;&#21160;&#39550;&#39542;&#30340;&#22833;&#36133;&#22330;&#26223;&#24182;&#37325;&#26032;&#35757;&#32451;&#65292;&#20197;&#25913;&#21892;&#22312;&#35757;&#32451;&#20013;&#32570;&#20047;&#30340;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#30340;&#24615;&#33021;&#12290;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.16575</link><description>&lt;p&gt;
&#22522;&#20110;&#40657;&#30418;&#39564;&#35777;&#31639;&#27861;&#30340;&#33258;&#25105;&#25913;&#36827;&#24378;&#21270;&#23398;&#20064;&#39550;&#39542;&#23433;&#20840;&#24615;&#33021;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Self-Improving Safety Performance of Reinforcement Learning Based Driving with Black-Box Verification Algorithms. (arXiv:2210.16575v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16575
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#40657;&#30418;&#39564;&#35777;&#26041;&#27861;&#26469;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#39550;&#39542;&#30340;&#23433;&#20840;&#24615;&#33021;&#30340;&#33258;&#25105;&#25913;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21457;&#29616;&#33258;&#21160;&#39550;&#39542;&#30340;&#22833;&#36133;&#22330;&#26223;&#24182;&#37325;&#26032;&#35757;&#32451;&#65292;&#20197;&#25913;&#21892;&#22312;&#35757;&#32451;&#20013;&#32570;&#20047;&#30340;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#30340;&#24615;&#33021;&#12290;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#25913;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#21033;&#29992;&#40657;&#30418;&#39564;&#35777;&#26041;&#27861;&#26469;&#22686;&#24378;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#20027;&#39550;&#39542;&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#33021;&#12290;&#26368;&#36817;&#20960;&#24180;&#26469;&#65292;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#33258;&#20027;&#39550;&#39542;&#24212;&#29992;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#35757;&#32451;&#22330;&#26223;&#30340;&#22810;&#26679;&#24615;&#12290;&#22312;&#35757;&#32451;&#38454;&#27573;&#32570;&#20047;&#23433;&#20840;&#20851;&#38190;&#30340;&#22330;&#26223;&#21487;&#33021;&#23548;&#33268;&#22312;&#30495;&#23454;&#39550;&#39542;&#24212;&#29992;&#20013;&#30340;&#27867;&#21270;&#24615;&#33021;&#36739;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#40657;&#30418;&#39564;&#35777;&#26041;&#27861;&#25506;&#32034;&#35757;&#32451;&#38598;&#30340;&#24369;&#28857;&#12290;&#21457;&#29616;&#33258;&#21160;&#39550;&#39542;&#22833;&#36133;&#22330;&#26223;&#21518;&#65292;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#37325;&#26032;&#21551;&#21160;RL&#20195;&#29702;&#30340;&#35757;&#32451;&#65292;&#20197;&#25913;&#36827;&#20808;&#21069;&#19981;&#23433;&#20840;&#30340;&#22330;&#26223;&#30340;&#24615;&#33021;&#12290;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#21457;&#29616;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#65288;ACC&#65289;&#24212;&#29992;&#20013;&#34892;&#20026;&#20915;&#31574;&#30340;&#23433;&#20840;&#22833;&#36133;&#65292;&#24182;&#19988;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a self-improving artificial intelligence system to enhance the safety performance of reinforcement learning (RL)-based autonomous driving (AD) agents using black-box verification methods. RL algorithms have become popular in AD applications in recent years. However, the performance of existing RL algorithms heavily depends on the diversity of training scenarios. A lack of safety-critical scenarios during the training phase could result in poor generalization performance in real-world driving applications. We propose a novel framework in which the weaknesses of the training set are explored through black-box verification methods. After discovering AD failure scenarios, the RL agent's training is re-initiated via transfer learning to improve the performance of previously unsafe scenarios. Simulation results demonstrate that our approach efficiently discovers safety failures of action decisions in RL-based adaptive cruise control (ACC) applications and significant
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#26131;&#20110;&#23454;&#26045;&#30340;&#35821;&#38899;&#35748;&#35777;&#31995;&#32479;&#25968;&#25454;&#20013;&#27602;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#24378;&#22823;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#31216;&#20026;Guardian&#65292;&#23427;&#26159;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#37492;&#21035;&#22120;&#12290;</title><link>http://arxiv.org/abs/2209.04547</link><description>&lt;p&gt;
&#38450;&#24481;&#35821;&#38899;&#35748;&#35777;&#20013;&#30340;&#25968;&#25454;&#20013;&#27602;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Defend Data Poisoning Attacks on Voice Authentication. (arXiv:2209.04547v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.04547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#26131;&#20110;&#23454;&#26045;&#30340;&#35821;&#38899;&#35748;&#35777;&#31995;&#32479;&#25968;&#25454;&#20013;&#27602;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#24378;&#22823;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#31216;&#20026;Guardian&#65292;&#23427;&#26159;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#37492;&#21035;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#23637;&#65292;&#35828;&#35805;&#20154;&#39564;&#35777;&#24050;&#32463;&#21462;&#24471;&#20102;&#38750;&#24120;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#27491;&#22312;&#20316;&#20026;&#19968;&#31181;&#29983;&#29289;&#29305;&#24449;&#35748;&#35777;&#36873;&#39033;&#22312;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#30340;&#35768;&#22810;&#22330;&#26223;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#29305;&#21035;&#26159;&#22312;&#19981;&#26029;&#22686;&#38271;&#30340;&#32593;&#32476;&#26381;&#21153;&#24066;&#22330;&#20013;&#12290;&#19982;&#20256;&#32479;&#23494;&#30721;&#30456;&#27604;&#65292;"&#22768;&#38899;&#23494;&#30721;"&#26356;&#26041;&#20415;&#65292;&#22240;&#20026;&#23427;&#20204;&#20351;&#20154;&#20204;&#19981;&#24517;&#35760;&#24518;&#19981;&#21516;&#30340;&#23494;&#30721;&#12290;&#28982;&#32780;&#65292;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#25915;&#20987;&#27491;&#22312;&#20351;&#36825;&#20123;&#35821;&#38899;&#35748;&#35777;&#31995;&#32479;&#38754;&#20020;&#39118;&#38505;&#12290;&#22312;&#27809;&#26377;&#24378;&#22823;&#30340;&#23433;&#20840;&#20445;&#35777;&#30340;&#24773;&#20917;&#19979;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#27450;&#39575;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#26469;&#35775;&#38382;&#21512;&#27861;&#29992;&#25143;&#30340;&#32593;&#32476;&#36134;&#25143;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#26131;&#20110;&#23454;&#26045;&#30340;&#35821;&#38899;&#35748;&#35777;&#31995;&#32479;&#25968;&#25454;&#20013;&#27602;&#25915;&#20987;&#65292;&#36825;&#31181;&#25915;&#20987;&#20960;&#20046;&#38590;&#20197;&#34987;&#29616;&#26377;&#30340;&#38450;&#24481;&#26426;&#21046;&#25429;&#25417;&#21040;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#24378;&#22823;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#31216;&#20026;Guardian&#65292;&#23427;&#26159;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#37492;&#21035;&#22120;&#12290;Guardian&#37492;&#21035;&#22120;&#32467;&#21512;&#20102;&#19968;&#31995;&#21015;&#26032;&#25216;&#26415;&#65292;&#21253;&#25324;...
&lt;/p&gt;
&lt;p&gt;
With the advances in deep learning, speaker verification has achieved very high accuracy and is gaining popularity as a type of biometric authentication option in many scenes of our daily life, especially the growing market of web services. Compared to traditional passwords, "vocal passwords" are much more convenient as they relieve people from memorizing different passwords. However, new machine learning attacks are putting these voice authentication systems at risk. Without a strong security guarantee, attackers could access legitimate users' web accounts by fooling the deep neural network (DNN) based voice recognition models. In this paper, we demonstrate an easy-to-implement data poisoning attack to the voice authentication system, which can hardly be captured by existing defense mechanisms. Thus, we propose a more robust defense method, called Guardian, which is a convolutional neural network-based discriminator. The Guardian discriminator integrates a series of novel techniques i
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;DualNets&#65292;&#19968;&#20010;&#36890;&#29992;&#30340;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#24555;&#36895;&#23398;&#20064;&#21644;&#24930;&#36895;&#23398;&#20064;&#31995;&#32479;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25345;&#32493;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2209.02370</link><description>&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#65292;&#24555;&#19982;&#24930;
&lt;/p&gt;
&lt;p&gt;
Continual Learning, Fast and Slow. (arXiv:2209.02370v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.02370
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;DualNets&#65292;&#19968;&#20010;&#36890;&#29992;&#30340;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#24555;&#36895;&#23398;&#20064;&#21644;&#24930;&#36895;&#23398;&#20064;&#31995;&#32479;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25345;&#32493;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26681;&#25454;&#31070;&#32463;&#31185;&#23398;&#20013;&#30340;&#34917;&#20805;&#23398;&#20064;&#31995;&#32479;&#65288;CLS&#65289;&#29702;&#35770;&#65292;&#20154;&#31867;&#36890;&#36807;&#20004;&#20010;&#20114;&#34917;&#31995;&#32479;&#26377;&#25928;&#22320;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#65306;&#19968;&#20010;&#20197;&#28023;&#39532;&#20307;&#20026;&#20013;&#24515;&#30340;&#24555;&#36895;&#23398;&#20064;&#31995;&#32479;&#65292;&#29992;&#20110;&#24555;&#36895;&#23398;&#20064;&#20855;&#20307;&#30340;&#20010;&#20307;&#32463;&#39564;&#65307;&#19968;&#20010;&#20301;&#20110;&#26032;&#30382;&#36136;&#30340;&#24930;&#36895;&#23398;&#20064;&#31995;&#32479;&#65292;&#29992;&#20110;&#36880;&#28176;&#33719;&#21462;&#20851;&#20110;&#29615;&#22659;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#12290;&#21463;&#21040;&#36825;&#19968;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DualNets&#65288;&#21452;&#32593;&#32476;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;&#65292;&#30001;&#19968;&#20010;&#24555;&#36895;&#23398;&#20064;&#31995;&#32479;&#21644;&#19968;&#20010;&#24930;&#36895;&#23398;&#20064;&#31995;&#32479;&#32452;&#25104;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#20174;&#29305;&#23450;&#20219;&#21153;&#20013;&#36827;&#34892;&#27169;&#24335;&#20998;&#31163;&#34920;&#31034;&#30340;&#30417;&#30563;&#23398;&#20064;&#21644;&#20174;&#20219;&#21153;&#26080;&#20851;&#30340;&#19968;&#33324;&#34920;&#31034;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;DualNets&#21487;&#20197;&#26080;&#32541;&#22320;&#23558;&#36825;&#20004;&#31181;&#34920;&#31034;&#31867;&#22411;&#21512;&#24182;&#21040;&#19968;&#20010;&#25972;&#20307;&#26694;&#26550;&#20013;&#65292;&#20197;&#20419;&#36827;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26356;&#22909;&#25345;&#32493;&#23398;&#20064;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
According to the Complementary Learning Systems (CLS) theory~\cite{mcclelland1995there} in neuroscience, humans do effective \emph{continual learning} through two complementary systems: a fast learning system centered on the hippocampus for rapid learning of the specifics, individual experiences; and a slow learning system located in the neocortex for the gradual acquisition of structured knowledge about the environment. Motivated by this theory, we propose \emph{DualNets} (for Dual Networks), a general continual learning framework comprising a fast learning system for supervised learning of pattern-separated representation from specific tasks and a slow learning system for representation learning of task-agnostic general representation via Self-Supervised Learning (SSL). DualNets can seamlessly incorporate both representation types into a holistic framework to facilitate better continual learning in deep neural networks. Via extensive experiments, we demonstrate the promising results 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22810;&#23186;&#20307;&#29983;&#25104;&#24335;&#33050;&#26412;&#23398;&#20064;&#20219;&#21153;&#65292;&#26088;&#22312;&#36890;&#36807;&#36319;&#36394;&#25991;&#26412;&#21644;&#35270;&#35273;&#27169;&#24577;&#20013;&#30340;&#21382;&#21490;&#29366;&#24577;&#26469;&#29983;&#25104;&#21518;&#32493;&#27493;&#39588;&#65292;&#33021;&#23545;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#20855;&#26377;&#24402;&#32435;&#33021;&#21147;&#24182;&#20855;&#26377;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.12306</link><description>&lt;p&gt;
&#22810;&#23186;&#20307;&#29983;&#25104;&#24335;&#33050;&#26412;&#23398;&#20064;&#29992;&#20110;&#20219;&#21153;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Multimedia Generative Script Learning for Task Planning. (arXiv:2208.12306v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.12306
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22810;&#23186;&#20307;&#29983;&#25104;&#24335;&#33050;&#26412;&#23398;&#20064;&#20219;&#21153;&#65292;&#26088;&#22312;&#36890;&#36807;&#36319;&#36394;&#25991;&#26412;&#21644;&#35270;&#35273;&#27169;&#24577;&#20013;&#30340;&#21382;&#21490;&#29366;&#24577;&#26469;&#29983;&#25104;&#21518;&#32493;&#27493;&#39588;&#65292;&#33021;&#23545;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#20855;&#26377;&#24402;&#32435;&#33021;&#21147;&#24182;&#20855;&#26377;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#23548;&#21521;&#30340;&#29983;&#25104;&#24335;&#33050;&#26412;&#23398;&#20064;&#26088;&#22312;&#22522;&#20110;&#30446;&#26631;&#29983;&#25104;&#21518;&#32493;&#27493;&#39588;&#65292;&#36825;&#26159;&#24110;&#21161;&#26426;&#22120;&#20154;&#25191;&#34892;&#26085;&#24120;&#29983;&#27963;&#20013;&#20856;&#22411;&#27963;&#21160;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#26524;&#21382;&#21490;&#29366;&#24577;&#19981;&#20165;&#30001;&#32473;&#20154;&#30340;&#35821;&#35328;&#25351;&#31034;&#25429;&#33719;&#65292;&#32780;&#19988;&#36824;&#36890;&#36807;&#30456;&#20276;&#30340;&#22270;&#20687;&#25552;&#20379;&#20102;&#38468;&#21152;&#20449;&#24687;&#65292;&#37027;&#20040;&#27492;&#20219;&#21153;&#30340;&#34920;&#29616;&#21487;&#20197;&#25913;&#21892;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#21363;&#22810;&#23186;&#20307;&#29983;&#25104;&#24335;&#33050;&#26412;&#23398;&#20064;&#65292;&#20197;&#36890;&#36807;&#36319;&#36394;&#25991;&#26412;&#21644;&#35270;&#35273;&#27169;&#24577;&#20013;&#30340;&#21382;&#21490;&#29366;&#24577;&#26469;&#29983;&#25104;&#21518;&#32493;&#27493;&#39588;&#65292;&#24182;&#25552;&#20379;&#20102;&#21253;&#21547;2,338&#20010;&#20219;&#21153;&#21644;31,496&#20010;&#27493;&#39588;&#21450;&#20854;&#25551;&#36848;&#24615;&#22270;&#20687;&#30340;&#31532;&#19968;&#20010;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#29983;&#25104;&#21487;&#35270;&#29366;&#24577;&#21487;&#36319;&#36394;&#30340;&#33050;&#26412;&#65292;&#23545;&#20110;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#20855;&#26377;&#24402;&#32435;&#33021;&#21147;&#65292;&#24182;&#19988;&#20854;&#27493;&#39588;&#20855;&#26377;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#22810;&#23186;&#20307;&#36873;&#25321;&#24615;&#32534;&#30721;&#22120;&#23545;&#35270;&#35273;&#29366;&#24577;&#21464;&#21270;&#36827;&#34892;&#32534;&#30721;&#65292;&#21033;&#29992;&#26816;&#32034;&#22686;&#24378;&#35299;&#30721;&#22120;&#20256;&#36882;&#20808;&#21069;&#35266;&#23519;&#21040;&#30340;&#20219;&#21153;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#38543;&#26426;&#25277;&#26679;&#21644;&#27874;&#26463;&#25628;&#32034;&#35299;&#30721;&#29983;&#25104;&#22810;&#26679;&#30340;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
Goal-oriented generative script learning aims to generate subsequent steps based on a goal, which is an essential task to assist robots in performing stereotypical activities of daily life. We show that the performance of this task can be improved if historical states are not just captured by the linguistic instructions given to people, but are augmented with the additional information provided by accompanying images. Therefore, we propose a new task, Multimedia Generative Script Learning, to generate subsequent steps by tracking historical states in both text and vision modalities, as well as presenting the first benchmark containing 2,338 tasks and 31,496 steps with descriptive images. We aim to generate scripts that are visual-state trackable, inductive for unseen tasks, and diverse in their individual steps. We propose to encode visual state changes through a multimedia selective encoder, transferring knowledge from previously observed tasks using a retrieval-augmented decoder, and
&lt;/p&gt;</description></item><item><title>&#19981;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#21487;&#20197;&#23545;&#20219;&#21153;&#30340;&#27867;&#21270;&#35823;&#24046;&#20135;&#29983;&#38750;&#21333;&#35843;&#30340;&#24433;&#21709;&#65292;&#20351;&#29992;&#23569;&#37327;&#19981;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#26159;&#26377;&#20215;&#20540;&#30340;&#12290;</title><link>http://arxiv.org/abs/2208.10967</link><description>&lt;p&gt;
&#19981;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#20215;&#20540;
&lt;/p&gt;
&lt;p&gt;
The Value of Out-of-Distribution Data. (arXiv:2208.10967v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10967
&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#21487;&#20197;&#23545;&#20219;&#21153;&#30340;&#27867;&#21270;&#35823;&#24046;&#20135;&#29983;&#38750;&#21333;&#35843;&#30340;&#24433;&#21709;&#65292;&#20351;&#29992;&#23569;&#37327;&#19981;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#26159;&#26377;&#20215;&#20540;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26399;&#26395;&#38543;&#30528;&#31867;&#20284;&#20219;&#21153;&#26679;&#26412;&#30340;&#22686;&#21152;&#65292;&#27867;&#21270;&#35823;&#24046;&#20250;&#20943;&#23567;&#65307;&#32780;&#38543;&#30528;&#26469;&#33258;&#19981;&#21516;&#20998;&#24067;&#65288;OOD&#65289;&#20219;&#21153;&#26679;&#26412;&#30340;&#22686;&#21152;&#65292;&#27867;&#21270;&#35823;&#24046;&#20250;&#22686;&#22823;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#21453;&#30452;&#35273;&#30340;&#29616;&#35937;&#65306;&#20219;&#21153;&#30340;&#27867;&#21270;&#35823;&#24046;&#21487;&#20197;&#26159;&#26679;&#26412;&#20174;OOD&#20219;&#21153;&#20013;&#30340;&#25968;&#37327;&#30340;&#38750;&#21333;&#35843;&#20989;&#25968;&#12290;&#38543;&#30528;OOD&#26679;&#26412;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#30446;&#26631;&#20219;&#21153;&#30340;&#27867;&#21270;&#35823;&#24046;&#22312;&#36229;&#36807;&#19968;&#20010;&#38408;&#20540;&#20043;&#21069;&#20250;&#20808;&#20943;&#23567;&#21518;&#22686;&#22823;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#20351;&#29992;&#23569;&#37327;OOD&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#26159;&#26377;&#20215;&#20540;&#30340;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;Fisher&#32447;&#24615;&#21028;&#21035;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;&#22914;MNIST&#12289;CIFAR-10&#12289;CINIC-10&#12289;PACS&#21644;DomainNet&#65289;&#19978;&#30340;&#28145;&#24230;&#32593;&#32476;&#26469;&#23637;&#31034;&#21644;&#20998;&#26512;&#36825;&#19968;&#29616;&#35937;&#12290;&#22312;&#25105;&#20204;&#30693;&#36947;&#21738;&#20123;&#26679;&#26412;&#23646;&#20110;OOD&#30340;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#21033;&#29992;&#30446;&#26631;&#21644;OOD&#32463;&#39564;&#39118;&#38505;&#30340;&#36866;&#24403;&#21152;&#26435;&#30446;&#26631;&#26469;&#21033;&#29992;&#36825;&#20123;&#38750;&#21333;&#35843;&#36235;&#21183;&#12290;&#23613;&#31649;&#23454;&#38469;&#24212;&#29992;&#26377;&#38480;&#65292;&#20294;&#36825;&#34920;&#26126;&#22914;&#26524;&#25105;&#20204;&#33021;&#22815;&#26816;&#27979;&#21040;OOD&#26679;&#26412;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#26159;&#26377;&#20215;&#20540;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We expect the generalization error to improve with more samples from a similar task, and to deteriorate with more samples from an out-of-distribution (OOD) task. In this work, we show a counter-intuitive phenomenon: the generalization error of a task can be a non-monotonic function of the number of OOD samples. As the number of OOD samples increases, the generalization error on the target task improves before deteriorating beyond a threshold. In other words, there is value in training on small amounts of OOD data. We use Fisher's Linear Discriminant on synthetic datasets and deep networks on computer vision benchmarks such as MNIST, CIFAR-10, CINIC-10, PACS and DomainNet to demonstrate and analyze this phenomenon. In the idealistic setting where we know which samples are OOD, we show that these non-monotonic trends can be exploited using an appropriately weighted objective of the target and OOD empirical risk. While its practical utility is limited, this does suggest that if we can det
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#27979;&#35797;&#26041;&#27861;&#65292;&#31216;&#20026;&#22270;&#28789;&#23454;&#39564;&#65288;TE&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#31867;&#34892;&#20026;&#27169;&#25311;&#33021;&#21147;&#12290;&#35770;&#25991;&#36890;&#36807;&#27169;&#25311;&#32463;&#27982;&#23398;&#12289;&#24515;&#29702;&#35821;&#35328;&#23398;&#21644;&#31038;&#20250;&#24515;&#29702;&#23398;&#23454;&#39564;&#65292;&#25104;&#21151;&#22797;&#21046;&#20102;&#24050;&#26377;&#30740;&#31350;&#65292;&#24182;&#25581;&#31034;&#20102;&#19968;&#31181;&#8220;&#36229;&#20934;&#30830;&#24230;&#25197;&#26354;&#8221;&#30340;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2208.10264</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27169;&#25311;&#22810;&#20010;&#20154;&#24182;&#22797;&#21046;&#20154;&#31867;&#23454;&#39564;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies. (arXiv:2208.10264v5 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10264
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#27979;&#35797;&#26041;&#27861;&#65292;&#31216;&#20026;&#22270;&#28789;&#23454;&#39564;&#65288;TE&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#31867;&#34892;&#20026;&#27169;&#25311;&#33021;&#21147;&#12290;&#35770;&#25991;&#36890;&#36807;&#27169;&#25311;&#32463;&#27982;&#23398;&#12289;&#24515;&#29702;&#35821;&#35328;&#23398;&#21644;&#31038;&#20250;&#24515;&#29702;&#23398;&#23454;&#39564;&#65292;&#25104;&#21151;&#22797;&#21046;&#20102;&#24050;&#26377;&#30740;&#31350;&#65292;&#24182;&#25581;&#31034;&#20102;&#19968;&#31181;&#8220;&#36229;&#20934;&#30830;&#24230;&#25197;&#26354;&#8221;&#30340;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#22270;&#28789;&#23454;&#39564;&#65288;TE&#65289;&#30340;&#26032;&#22411;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#32473;&#23450;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT&#27169;&#22411;&#65289;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#21487;&#20197;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#30340;&#19981;&#21516;&#26041;&#38754;&#12290;TE&#36824;&#21487;&#20197;&#25581;&#31034;&#35821;&#35328;&#27169;&#22411;&#22312;&#27169;&#25311;&#29305;&#23450;&#20154;&#31867;&#34892;&#20026;&#26041;&#38754;&#30340;&#19968;&#33268;&#24615;&#25197;&#26354;&#12290;&#19982;&#22270;&#28789;&#27979;&#35797;&#19981;&#21516;&#65292;&#22270;&#28789;&#23454;&#39564;&#38656;&#35201;&#27169;&#25311;&#20195;&#34920;&#24615;&#21442;&#19982;&#20154;&#31867;&#21463;&#35797;&#32773;&#30740;&#31350;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#36827;&#34892;TEs&#65292;&#35797;&#22270;&#22797;&#21046;&#20043;&#21069;&#30740;&#31350;&#20013;&#30830;&#31435;&#30340;&#21457;&#29616;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#27169;&#25311;TEs&#30340;&#26041;&#27861;&#65292;&#24182;&#20030;&#20363;&#35828;&#26126;&#20854;&#29992;&#20110;&#27604;&#36739;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22810;&#22823;&#31243;&#24230;&#19978;&#22797;&#21046;&#32463;&#20856;&#32463;&#27982;&#23398;&#12289;&#24515;&#29702;&#35821;&#35328;&#23398;&#21644;&#31038;&#20250;&#24515;&#29702;&#23398;&#23454;&#39564;&#30340;&#33021;&#21147;&#65306;&#26368;&#32456;&#28216;&#25103;&#12289;&#22253;&#36335;&#21477;&#23376;&#12289;&#31859;&#23572;&#26684;&#25289;&#22982;&#30005;&#20987;&#23454;&#39564;&#21644;&#20247;&#20154;&#30340;&#26234;&#24935;&#12290;&#22312;&#21069;&#19977;&#20010;TEs&#20013;&#65292;&#20351;&#29992;&#26368;&#26032;&#27169;&#22411;&#25104;&#21151;&#22797;&#21046;&#20102;&#29616;&#26377;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#32780;&#26368;&#21518;&#19968;&#20010;TE&#25581;&#31034;&#20102;&#23384;&#22312;&#20110;&#26368;&#26032;&#27169;&#22411;&#20013;&#30340;&#8220;&#36229;&#20934;&#30830;&#24230;&#25197;&#26354;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new type of test, called a Turing Experiment (TE), for evaluating to what extent a given language model, such as GPT models, can simulate different aspects of human behavior. A TE can also reveal consistent distortions in a language model's simulation of a specific human behavior. Unlike the Turing Test, which involves simulating a single arbitrary individual, a TE requires simulating a representative sample of participants in human subject research. We carry out TEs that attempt to replicate well-established findings from prior studies. We design a methodology for simulating TEs and illustrate its use to compare how well different language models are able to reproduce classic economic, psycholinguistic, and social psychology experiments: Ultimatum Game, Garden Path Sentences, Milgram Shock Experiment, and Wisdom of Crowds. In the first three TEs, the existing findings were replicated using recent models, while the last TE reveals a "hyper-accuracy distortion" present in
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#30005;&#35270;&#21095;&#26412;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;Multilingual Multiparty Coref&#65288;MMC&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#35821;&#35328;&#22810;&#26041;&#23545;&#35805;&#20013;&#30340;&#20849;&#25351;&#35299;&#26512;&#38382;&#39064;&#12290;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#40644;&#37329;&#26631;&#27880;&#65292;&#22312;&#20854;&#20182;&#35821;&#35328;&#19978;&#21019;&#24314;&#20102;&#38134;&#20849;&#25351;&#35299;&#26512;&#25968;&#25454;&#65292;&#24182;&#21457;&#29616;MMC&#22312;&#22810;&#26041;&#20849;&#25351;&#35299;&#26512;&#30340;&#35206;&#30422;&#33539;&#22260;&#19978;&#27604;&#20043;&#21069;&#30340;&#25968;&#25454;&#38598;&#26356;&#24191;&#27867;&#12290;&#22312;&#38134;&#26631;&#27880;&#25968;&#25454;&#19978;&#65292;&#26080;&#35770;&#26159;&#29992;&#20110;&#25968;&#25454;&#22686;&#24378;&#36824;&#26159;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#65292;&#37117;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#26377;&#25928;&#22320;&#27169;&#25311;&#20102;&#38646;&#23556;&#36328;&#35821;&#35328;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2208.01307</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#22810;&#26041;&#23545;&#35805;&#20013;&#30340;&#20849;&#25351;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
Multilingual Coreference Resolution in Multiparty Dialogue. (arXiv:2208.01307v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.01307
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#30005;&#35270;&#21095;&#26412;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;Multilingual Multiparty Coref&#65288;MMC&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#35821;&#35328;&#22810;&#26041;&#23545;&#35805;&#20013;&#30340;&#20849;&#25351;&#35299;&#26512;&#38382;&#39064;&#12290;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#40644;&#37329;&#26631;&#27880;&#65292;&#22312;&#20854;&#20182;&#35821;&#35328;&#19978;&#21019;&#24314;&#20102;&#38134;&#20849;&#25351;&#35299;&#26512;&#25968;&#25454;&#65292;&#24182;&#21457;&#29616;MMC&#22312;&#22810;&#26041;&#20849;&#25351;&#35299;&#26512;&#30340;&#35206;&#30422;&#33539;&#22260;&#19978;&#27604;&#20043;&#21069;&#30340;&#25968;&#25454;&#38598;&#26356;&#24191;&#27867;&#12290;&#22312;&#38134;&#26631;&#27880;&#25968;&#25454;&#19978;&#65292;&#26080;&#35770;&#26159;&#29992;&#20110;&#25968;&#25454;&#22686;&#24378;&#36824;&#26159;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#65292;&#37117;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#26377;&#25928;&#22320;&#27169;&#25311;&#20102;&#38646;&#23556;&#36328;&#35821;&#35328;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22810;&#26041;&#23545;&#35805;&#23454;&#20307;&#20849;&#25351;&#35299;&#26512;&#25968;&#25454;&#38598;&#23578;&#19981;&#23436;&#21892;&#65292;&#24182;&#19988;&#36824;&#23384;&#22312;&#35768;&#22810;&#25361;&#25112;&#23578;&#24453;&#35299;&#20915;&#12290;&#25105;&#20204;&#22522;&#20110;&#30005;&#35270;&#21095;&#26412;&#21019;&#36896;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#65292;&#21517;&#20026;Multilingual Multiparty Coref&#65288;MMC&#65289;&#65292;&#29992;&#20110;&#36825;&#20010;&#20219;&#21153;&#12290;&#30001;&#20110;&#22810;&#31181;&#35821;&#35328;&#20013;&#23384;&#22312;&#36136;&#37327;&#36739;&#39640;&#30340;&#23383;&#24149;&#65292;&#25105;&#20204;&#25552;&#20986;&#37325;&#22797;&#20351;&#29992;&#26631;&#27880;&#26469;&#21019;&#24314;&#20854;&#20182;&#35821;&#35328;&#65288;&#20013;&#25991;&#21644;&#27874;&#26031;&#35821;&#65289;&#19978;&#30340;&#38134;&#20849;&#25351;&#35299;&#26512;&#25968;&#25454;&#65292;&#36890;&#36807;&#26631;&#27880;&#20256;&#36882;&#26469;&#23454;&#29616;&#12290;&#22312;&#40644;&#37329;&#26631;&#27880;&#65288;&#33521;&#25991;&#65289;&#25968;&#25454;&#19978;&#65292;&#29616;&#26377;&#27169;&#22411;&#22312;MMC&#19978;&#34920;&#29616;&#30456;&#23545;&#36739;&#24046;&#65292;&#36825;&#34920;&#26126;MMC&#22312;&#22810;&#26041;&#20849;&#25351;&#35299;&#26512;&#30340;&#35206;&#30422;&#33539;&#22260;&#19978;&#27604;&#20043;&#21069;&#30340;&#25968;&#25454;&#38598;&#26356;&#24191;&#27867;&#12290;&#22312;&#38134;&#26631;&#27880;&#25968;&#25454;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#26080;&#35770;&#26159;&#29992;&#20110;&#25968;&#25454;&#22686;&#24378;&#36824;&#26159;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#65292;&#37117;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#36825;&#26377;&#25928;&#22320;&#27169;&#25311;&#20102;&#38646;&#23556;&#36328;&#35821;&#35328;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing multiparty dialogue datasets for entity coreference resolution are nascent, and many challenges are still unaddressed. We create a large-scale dataset, Multilingual Multiparty Coref (MMC), for this task based on TV transcripts. Due to the availability of gold-quality subtitles in multiple languages, we propose reusing the annotations to create silver coreference resolution data in other languages (Chinese and Farsi) via annotation projection. On the gold (English) data, off-the-shelf models perform relatively poorly on MMC, suggesting that MMC has broader coverage of multiparty coreference than prior datasets. On the silver data, we find success both using it for data augmentation and training from scratch, which effectively simulates the zero-shot cross-lingual setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#22312;&#23545;&#25239;&#25915;&#20987;&#19979;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22312;&#22823;&#25968;&#25454;&#12289;&#36229;&#21442;&#25968;&#21270;&#26497;&#38480;&#19979;&#65292;BNN&#30340;&#21518;&#39564;&#20855;&#26377;&#26799;&#24230;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#36825;&#23545;&#20110;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#30340;&#33030;&#24369;&#24615;&#38382;&#39064;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2207.06154</link><description>&lt;p&gt;
&#23545;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#22312;&#23545;&#25239;&#25915;&#20987;&#19979;&#30340;&#40065;&#26834;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Robustness of Bayesian Neural Networks to Adversarial Attacks. (arXiv:2207.06154v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.06154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#22312;&#23545;&#25239;&#25915;&#20987;&#19979;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22312;&#22823;&#25968;&#25454;&#12289;&#36229;&#21442;&#25968;&#21270;&#26497;&#38480;&#19979;&#65292;BNN&#30340;&#21518;&#39564;&#20855;&#26377;&#26799;&#24230;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#36825;&#23545;&#20110;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#30340;&#33030;&#24369;&#24615;&#38382;&#39064;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#65292;&#23545;&#25239;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#26159;&#28145;&#24230;&#23398;&#20064;&#24191;&#27867;&#24212;&#29992;&#30340;&#20027;&#35201;&#38556;&#30861;&#20043;&#19968;&#12290;&#23613;&#31649;&#22312;&#23454;&#36341;&#21644;&#29702;&#35770;&#26041;&#38754;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#21162;&#21147;&#65292;&#20294;&#35757;&#32451;&#20986;&#23545;&#25239;&#25915;&#20987;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#22823;&#25968;&#25454;&#12289;&#36229;&#21442;&#25968;&#21270;&#26497;&#38480;&#19979;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#23545;&#25239;&#25915;&#20987;&#30340;&#20960;&#20309;&#24615;&#36136;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#36825;&#20010;&#26497;&#38480;&#19979;&#65292;&#26799;&#24230;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#26159;&#30001;&#20110;&#25968;&#25454;&#20998;&#24067;&#30340;&#36864;&#21270;&#23548;&#33268;&#30340;&#65292;&#20063;&#23601;&#26159;&#24403;&#25968;&#25454;&#20301;&#20110;&#29615;&#22659;&#31354;&#38388;&#30340;&#19968;&#20010;&#20302;&#32500;&#23376;&#27969;&#24418;&#19978;&#26102;&#12290;&#20316;&#20026;&#30452;&#25509;&#32467;&#26524;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#36825;&#20010;&#26497;&#38480;&#19979;&#65292;BNN&#30340;&#21518;&#39564;&#23545;&#26799;&#24230;&#25915;&#20987;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#20851;&#38190;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21363;&#20351;&#20174;&#21518;&#39564;&#20013;&#37319;&#26679;&#30340;&#27599;&#20010;&#31070;&#32463;&#32593;&#32476;&#23545;&#26799;&#24230;&#25915;&#20987;&#37117;&#20855;&#26377;&#33030;&#24369;&#24615;&#65292;&#25439;&#22833;&#20989;&#25968;&#23545;BNN&#21518;&#39564;&#20998;&#24067;&#30340;&#26399;&#26395;&#26799;&#24230;&#20173;&#28982;&#36235;&#20110;&#38646;&#12290;&#22312;t&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vulnerability to adversarial attacks is one of the principal hurdles to the adoption of deep learning in safety-critical applications. Despite significant efforts, both practical and theoretical, training deep learning models robust to adversarial attacks is still an open problem. In this paper, we analyse the geometry of adversarial attacks in the large-data, overparameterized limit for Bayesian Neural Networks (BNNs). We show that, in the limit, vulnerability to gradient-based attacks arises as a result of degeneracy in the data distribution, i.e., when the data lies on a lower-dimensional submanifold of the ambient space. As a direct consequence, we demonstrate that in this limit BNN posteriors are robust to gradient-based adversarial attacks. Crucially, we prove that the expected gradient of the loss with respect to the BNN posterior distribution is vanishing, even when each neural network sampled from the posterior is vulnerable to gradient-based attacks. Experimental results on t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20462;&#25913;&#20915;&#31574;&#26641;&#30340;&#26041;&#27861;&#65292;&#21363;&#29369;&#35947;&#26641;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#36827;&#34892;&#23398;&#20064;&#65292;&#25512;&#29702;&#21644;&#25552;&#20379;&#24378;&#20581;&#30340;&#26631;&#31614;&#20998;&#24067;&#65292;&#21487;&#29992;&#20110;&#20854;&#20182;&#25512;&#29702;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2206.12252</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#19979;&#23398;&#20064;&#22522;&#20110;&#35770;&#35777;&#30340;&#25512;&#29702;&#30340;&#29369;&#35947;&#26641;
&lt;/p&gt;
&lt;p&gt;
Indecision Trees: Learning Argument-Based Reasoning under Quantified Uncertainty. (arXiv:2206.12252v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.12252
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20462;&#25913;&#20915;&#31574;&#26641;&#30340;&#26041;&#27861;&#65292;&#21363;&#29369;&#35947;&#26641;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#36827;&#34892;&#23398;&#20064;&#65292;&#25512;&#29702;&#21644;&#25552;&#20379;&#24378;&#20581;&#30340;&#26631;&#31614;&#20998;&#24067;&#65292;&#21487;&#29992;&#20110;&#20854;&#20182;&#25512;&#29702;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#24448;&#24448;&#23384;&#22312;&#38382;&#39064;&#65292;&#27604;&#22914;&#19981;&#21487;&#35299;&#37322;&#30340;&#40657;&#30418;&#27169;&#22411;&#65292;&#19981;&#23436;&#32654;&#27979;&#37327;&#30340;&#30830;&#23450;&#24615;&#20551;&#35774;&#65292;&#25110;&#32773;&#21482;&#25552;&#20379;&#21333;&#19968;&#20998;&#31867;&#32780;&#19981;&#26159;&#27010;&#29575;&#20998;&#24067;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#29369;&#35947;&#26641;&#65292;&#19968;&#31181;&#23545;&#20915;&#31574;&#26641;&#36827;&#34892;&#20462;&#25913;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#36827;&#34892;&#23398;&#20064;&#65292;&#33021;&#22815;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#25512;&#29702;&#65292;&#25552;&#20379;&#19968;&#20010;&#24378;&#20581;&#30340;&#26631;&#31614;&#20998;&#24067;&#65292;&#24182;&#21487;&#20998;&#35299;&#20026;&#19968;&#32452;&#36923;&#36753;&#35770;&#35777;&#29992;&#20110;&#20854;&#20182;&#25512;&#29702;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Using Machine Learning systems in the real world can often be problematic, with inexplicable black-box models, the assumed certainty of imperfect measurements, or providing a single classification instead of a probability distribution.  This paper introduces Indecision Trees, a modification to Decision Trees which learn under uncertainty, can perform inference under uncertainty, provide a robust distribution over the possible labels, and can be disassembled into a set of logical arguments for use in other reasoning systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DORA&#30340;&#25968;&#25454;&#19981;&#21487;&#30693;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#34920;&#24449;&#31354;&#38388;&#65292;&#24182;&#21487;&#20197;&#35782;&#21035;&#19981;&#31526;&#21512;&#20154;&#31867;&#30452;&#35266;&#35748;&#30693;&#30340;&#34920;&#24449;&#12290;</title><link>http://arxiv.org/abs/2206.04530</link><description>&lt;p&gt;
DORA&#65306;&#25506;&#32034;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24322;&#24120;&#20540;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
DORA: Exploring outlier representations in Deep Neural Networks. (arXiv:2206.04530v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.04530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DORA&#30340;&#25968;&#25454;&#19981;&#21487;&#30693;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#34920;&#24449;&#31354;&#38388;&#65292;&#24182;&#21487;&#20197;&#35782;&#21035;&#19981;&#31526;&#21512;&#20154;&#31867;&#30452;&#35266;&#35748;&#30693;&#30340;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#23398;&#20064;&#22797;&#26434;&#25277;&#35937;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#23481;&#26131;&#24847;&#22806;&#22320;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#34394;&#20551;&#30340;&#29305;&#24449;&#12290;&#20026;&#20102;&#30830;&#20445;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#65292;&#26816;&#26597;&#23398;&#20064;&#34920;&#31034;&#20043;&#38388;&#30340;&#20851;&#31995;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#24847;&#22806;&#30340;&#27010;&#24565;&#24448;&#24448;&#34920;&#29616;&#20026;&#19982;&#25152;&#38656;&#30340;&#20219;&#21153;&#19981;&#31526;&#30340;&#24322;&#24120;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DORA&#65288;Data-agnOstic Representation Analysis&#65289;&#65306;&#29992;&#20110;&#20998;&#26512;DNN&#34920;&#31034;&#31354;&#38388;&#30340;&#31532;&#19968;&#20010;&#25968;&#25454;&#19981;&#21487;&#30693;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#37319;&#29992;&#20102;&#25152;&#25552;&#20986;&#30340;&#34920;&#31034;&#20043;&#38388;&#30340;&#26497;&#31471;&#28608;&#27963;&#65288;EA&#65289;&#36317;&#31163;&#24230;&#37327;&#65292;&#22312;&#19981;&#35775;&#38382;&#20219;&#20309;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21033;&#29992;&#32593;&#32476;&#20869;&#33258;&#35828;&#26126;&#33021;&#21147;&#12290;&#25105;&#20204;&#23450;&#37327;&#39564;&#35777;&#20102;&#24230;&#37327;&#30340;&#27491;&#30830;&#24615;&#21644;&#19982;&#20154;&#20026;&#23450;&#20041;&#30340;&#35821;&#20041;&#36317;&#31163;&#30340;&#19968;&#33268;&#24615;&#12290;EA&#36317;&#31163;&#19982;&#20154;&#31867;&#21028;&#26029;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#20351;&#25105;&#20204;&#33021;&#22815;&#30830;&#23450;&#34920;&#24449;&#65292;&#20854;&#22522;&#26412;&#27010;&#24565;&#34987;&#35748;&#20026;&#26159;&#19981;&#33258;&#28982;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although Deep Neural Networks (DNNs) are incredibly effective in learning complex abstractions, they are susceptible to unintentionally learning spurious artifacts from the training data. To ensure model transparency, it is crucial to examine the relationships between learned representations, as unintended concepts often manifest themselves to be anomalous to the desired task. In this work, we introduce DORA (Data-agnOstic Representation Analysis): the first data-agnostic framework for the analysis of the representation space of DNNs. Our framework employs the proposed Extreme-Activation (EA) distance measure between representations that utilizes self-explaining capabilities within the network without accessing any data. We quantitatively validate the metric's correctness and alignment with human-defined semantic distances. The coherence between the EA distance and human judgment enables us to identify representations whose underlying concepts would be considered unnatural by humans by
&lt;/p&gt;</description></item><item><title>FIGS&#26159;&#19968;&#31181;&#24555;&#36895;&#21487;&#35299;&#37322;&#30340;&#36138;&#23146;&#26641;&#27714;&#21644;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#36923;&#36753;&#35268;&#21017;&#19982;&#21152;&#27861;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#36866;&#24212;&#21152;&#24615;&#32467;&#26500;&#21516;&#26102;&#20445;&#25345;&#39640;&#24230;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;FIGS&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#22914;&#21307;&#23398;&#20013;&#23637;&#31034;&#20102;&#20854;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2201.11931</link><description>&lt;p&gt;
&#24555;&#36895;&#21487;&#35299;&#37322;&#30340;&#36138;&#23146;&#26641;&#27714;&#21644;
&lt;/p&gt;
&lt;p&gt;
Fast Interpretable Greedy-Tree Sums. (arXiv:2201.11931v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.11931
&lt;/p&gt;
&lt;p&gt;
FIGS&#26159;&#19968;&#31181;&#24555;&#36895;&#21487;&#35299;&#37322;&#30340;&#36138;&#23146;&#26641;&#27714;&#21644;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#36923;&#36753;&#35268;&#21017;&#19982;&#21152;&#27861;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#36866;&#24212;&#21152;&#24615;&#32467;&#26500;&#21516;&#26102;&#20445;&#25345;&#39640;&#24230;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;FIGS&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#22914;&#21307;&#23398;&#20013;&#23637;&#31034;&#20102;&#20854;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20294;&#36890;&#24120;&#20250;&#29306;&#29298;&#35299;&#37322;&#24615;&#65292;&#36825;&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#22914;&#21307;&#23398;&#20013;&#26159;&#19968;&#20010;&#20851;&#38190;&#32771;&#34385;&#22240;&#32032;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20174;&#19994;&#32773;&#36890;&#24120;&#20351;&#29992;&#39640;&#24230;&#21487;&#35299;&#37322;&#30340;&#20915;&#31574;&#26641;&#27169;&#22411;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#23545;&#21152;&#24615;&#32467;&#26500;&#23384;&#22312;&#24402;&#32435;&#20559;&#24046;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#31181;&#20559;&#24046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24555;&#36895;&#21487;&#35299;&#37322;&#30340;&#36138;&#23146;&#26641;&#27714;&#21644;&#65288;FIGS&#65289;&#65292;&#23427;&#23558;CART&#31639;&#27861;&#25512;&#24191;&#21040;&#21516;&#26102;&#22686;&#38271;&#21487;&#21464;&#25968;&#37327;&#30340;&#26641;&#36827;&#34892;&#27714;&#21644;&#12290;&#36890;&#36807;&#23558;&#36923;&#36753;&#35268;&#21017;&#19982;&#21152;&#27861;&#30456;&#32467;&#21512;&#65292;FIGS&#33021;&#22815;&#36866;&#24212;&#21152;&#24615;&#32467;&#26500;&#21516;&#26102;&#20445;&#25345;&#39640;&#24230;&#21487;&#35299;&#37322;&#24615;&#12290;&#23545;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;FIGS&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#20026;&#20102;&#23637;&#31034;FIGS&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#30340;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#23558;FIGS&#25913;&#36827;&#20026;&#23398;&#20064;&#20020;&#24202;&#20915;&#31574;&#24037;&#20855;&#65288;CDIs&#65289;&#65292;CDIs&#26159;&#25351;&#23548;&#20020;&#24202;&#20915;&#31574;&#30340;&#24037;&#20855;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FIGS&#30340;&#19968;&#20010;&#21464;&#31181;&#65292;&#31216;&#20026;G-FIGS&#65292;&#23427;&#32771;&#34385;&#20102;&#21152;&#24615;&#32467;&#26500;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern machine learning has achieved impressive prediction performance, but often sacrifices interpretability, a critical consideration in high-stakes domains such as medicine. In such settings, practitioners often use highly interpretable decision tree models, but these suffer from inductive bias against additive structure. To overcome this bias, we propose Fast Interpretable Greedy-Tree Sums (FIGS), which generalizes the CART algorithm to simultaneously grow a flexible number of trees in summation. By combining logical rules with addition, FIGS is able to adapt to additive structure while remaining highly interpretable. Extensive experiments on real-world datasets show that FIGS achieves state-of-the-art prediction performance. To demonstrate the usefulness of FIGS in high-stakes domains, we adapt FIGS to learn clinical decision instruments (CDIs), which are tools for guiding clinical decision-making. Specifically, we introduce a variant of FIGS known as G-FIGS that accounts for the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20445;&#23432;&#20998;&#24067;&#26368;&#22823;&#21518;&#39564;&#31574;&#30053;&#20248;&#21270; (CDMPO) &#30340;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#24067;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#20272;&#35745;Q&#20989;&#25968;&#21644;C&#20989;&#25968;&#65292;&#20197;&#21450;&#20351;&#29992;&#20445;&#23432;&#30340;&#20215;&#20540;&#20989;&#25968;&#25439;&#22833;&#26469;&#20943;&#23569;&#32422;&#26463;&#36829;&#21453;&#27425;&#25968;&#12290;</title><link>http://arxiv.org/abs/2201.07286</link><description>&lt;p&gt;
&#20855;&#26377;&#23433;&#20840;&#32422;&#26463;&#30340;&#20445;&#23432;&#20998;&#24067;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Conservative Distributional Reinforcement Learning with Safety Constraints. (arXiv:2201.07286v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.07286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20445;&#23432;&#20998;&#24067;&#26368;&#22823;&#21518;&#39564;&#31574;&#30053;&#20248;&#21270; (CDMPO) &#30340;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#24067;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#20272;&#35745;Q&#20989;&#25968;&#21644;C&#20989;&#25968;&#65292;&#20197;&#21450;&#20351;&#29992;&#20445;&#23432;&#30340;&#20215;&#20540;&#20989;&#25968;&#25439;&#22833;&#26469;&#20943;&#23569;&#32422;&#26463;&#36829;&#21453;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#25506;&#32034;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#19968;&#20010;&#21463;&#32422;&#26463;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#38382;&#39064;&#65292;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#39044;&#26399;&#30340;&#38271;&#26399;&#25104;&#26412;&#26159;&#21463;&#21040;&#32422;&#26463;&#30340;&#12290;&#20808;&#21069;&#30340;&#31163;&#31574;&#30053;&#31639;&#27861;&#36890;&#36807;&#24341;&#20837;&#25289;&#26684;&#26391;&#26085;&#26494;&#24347;&#25216;&#26415;&#65292;&#23558;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#36716;&#25442;&#20026;&#30456;&#24212;&#30340;&#26080;&#32422;&#26463;&#23545;&#20598;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#19978;&#36848;&#31639;&#27861;&#30340;&#25104;&#26412;&#20989;&#25968;&#25552;&#20379;&#20102;&#19981;&#20934;&#30830;&#30340;&#20272;&#35745;&#65292;&#24182;&#23548;&#33268;&#25289;&#26684;&#26391;&#26085;&#20056;&#23376;&#23398;&#20064;&#30340;&#19981;&#31283;&#23450;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#31216;&#20026;&#20445;&#23432;&#20998;&#24067;&#26368;&#22823;&#21518;&#39564;&#31574;&#30053;&#20248;&#21270;(CDMPO)&#12290;&#39318;&#20808;&#65292;&#20026;&#20102;&#20934;&#30830;&#21028;&#26029;&#24403;&#21069;&#24773;&#20917;&#26159;&#21542;&#28385;&#36275;&#32422;&#26463;&#26465;&#20214;&#65292;CDMPO&#37319;&#29992;&#20998;&#24067;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#20272;&#35745;Q&#20989;&#25968;&#21644;C&#20989;&#25968;&#12290;&#28982;&#21518;&#65292;CDMPO&#20351;&#29992;&#20445;&#23432;&#30340;&#20215;&#20540;&#20989;&#25968;&#25439;&#22833;&#26469;&#20943;&#23569;&#25506;&#32034;&#36807;&#31243;&#20013;&#36829;&#32422;&#27425;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#21152;&#26435;&#24179;&#22343;&#27604;&#20363;&#31215;&#20998;D&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safety exploration can be regarded as a constrained Markov decision problem where the expected long-term cost is constrained. Previous off-policy algorithms convert the constrained optimization problem into the corresponding unconstrained dual problem by introducing the Lagrangian relaxation technique. However, the cost function of the above algorithms provides inaccurate estimations and causes the instability of the Lagrange multiplier learning. In this paper, we present a novel off-policy reinforcement learning algorithm called Conservative Distributional Maximum a Posteriori Policy Optimization (CDMPO). At first, to accurately judge whether the current situation satisfies the constraints, CDMPO adapts distributional reinforcement learning method to estimate the Q-function and C-function. Then, CDMPO uses a conservative value function loss to reduce the number of violations of constraints during the exploration process. In addition, we utilize Weighted Average Proportional Integral D
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#25968;&#23398;&#20998;&#26512;&#35777;&#26126;&#20102;&#38750;&#25903;&#37197;&#25490;&#24207;&#36951;&#20256;&#31639;&#27861;II&#65288;NSGA-II&#65289;&#30340;&#36816;&#34892;&#26102;&#38388;&#29305;&#24615;&#65292;&#21457;&#29616;&#24403;&#31181;&#32676;&#35268;&#27169;&#26159;&#24085;&#32047;&#25176;&#21069;&#27839;&#22823;&#23567;&#30340;&#22235;&#20493;&#26102;&#65292;NSGA-II&#20855;&#26377;&#19982;&#20854;&#20182;&#31639;&#27861;&#30456;&#21516;&#30340;&#36816;&#34892;&#26102;&#38388;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2112.08581</link><description>&lt;p&gt;
&#38750;&#25903;&#37197;&#25490;&#24207;&#36951;&#20256;&#31639;&#27861;II&#65288;NSGA-II&#65289;&#30340;&#25968;&#23398;&#36816;&#34892;&#26102;&#38388;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Mathematical Runtime Analysis for the Non-Dominated Sorting Genetic Algorithm II (NSGA-II). (arXiv:2112.08581v5 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.08581
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#25968;&#23398;&#20998;&#26512;&#35777;&#26126;&#20102;&#38750;&#25903;&#37197;&#25490;&#24207;&#36951;&#20256;&#31639;&#27861;II&#65288;NSGA-II&#65289;&#30340;&#36816;&#34892;&#26102;&#38388;&#29305;&#24615;&#65292;&#21457;&#29616;&#24403;&#31181;&#32676;&#35268;&#27169;&#26159;&#24085;&#32047;&#25176;&#21069;&#27839;&#22823;&#23567;&#30340;&#22235;&#20493;&#26102;&#65292;NSGA-II&#20855;&#26377;&#19982;&#20854;&#20182;&#31639;&#27861;&#30456;&#21516;&#30340;&#36816;&#34892;&#26102;&#38388;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#25903;&#37197;&#25490;&#24207;&#36951;&#20256;&#31639;&#27861;II&#65288;NSGA-II&#65289;&#26159;&#30446;&#21069;&#23454;&#38469;&#24212;&#29992;&#20013;&#26368;&#24120;&#29992;&#30340;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#65288;MOEA&#65289;&#12290;&#28982;&#32780;&#65292;&#19982;&#19968;&#20123;&#31616;&#21333;&#30340;MOEA&#30340;&#25968;&#23398;&#20998;&#26512;&#30456;&#21453;&#65292;NSGA-II&#33267;&#20170;&#27809;&#26377;&#36827;&#34892;&#36807;&#36825;&#26679;&#30340;&#30740;&#31350;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;NSGA-II&#36827;&#34892;&#25968;&#23398;&#36816;&#34892;&#26102;&#38388;&#20998;&#26512;&#26159;&#21487;&#34892;&#30340;&#12290;&#20316;&#20026;&#29305;&#23450;&#32467;&#26524;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#31181;&#32676;&#35268;&#27169;&#26159;&#24085;&#32047;&#25176;&#21069;&#27839;&#22823;&#23567;&#30340;&#22235;&#20493;&#26102;&#65292;NSGA-II&#20351;&#29992;&#20004;&#31181;&#32463;&#20856;&#21464;&#24322;&#31639;&#23376;&#21644;&#22235;&#31181;&#19981;&#21516;&#30340;&#29238;&#20195;&#36873;&#25321;&#26041;&#27861;&#19982;SEMO&#21644;GSEMO&#31639;&#27861;&#22312;&#22522;&#26412;&#30340;OneMinMax&#21644;LeadingOnesTrailingZeros&#22522;&#20934;&#27979;&#35797;&#19978;&#20855;&#26377;&#30456;&#21516;&#30340;&#28176;&#36817;&#36816;&#34892;&#26102;&#38388;&#20445;&#35777;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#31181;&#32676;&#35268;&#27169;&#21482;&#31561;&#20110;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#22823;&#23567;&#65292;&#21017;NSGA-II&#26080;&#27861;&#39640;&#25928;&#22320;&#35745;&#31639;&#23436;&#25972;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#65306;&#22312;&#25351;&#25968;&#32423;&#36845;&#20195;&#27425;&#25968;&#20869;&#65292;&#31181;&#32676;&#22987;&#32456;&#20250;&#38169;&#22833;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#19968;&#20010;&#24658;&#23450;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
The non-dominated sorting genetic algorithm II (NSGA-II) is the most intensively used multi-objective evolutionary algorithm (MOEA) in real-world applications. However, in contrast to several simple MOEAs analyzed also via mathematical means, no such study exists for the NSGA-II so far. In this work, we show that mathematical runtime analyses are feasible also for the NSGA-II. As particular results, we prove that with a population size four times larger than the size of the Pareto front, the NSGA-II with two classic mutation operators and four different ways to select the parents satisfies the same asymptotic runtime guarantees as the SEMO and GSEMO algorithms on the basic OneMinMax and LeadingOnesTrailingZeros benchmarks. However, if the population size is only equal to the size of the Pareto front, then the NSGA-II cannot efficiently compute the full Pareto front: for an exponential number of iterations, the population will always miss a constant fraction of the Pareto front. Our exp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;RPANet&#65292;&#29992;&#20110;&#36890;&#36807;&#24179;&#38754;&#35270;&#24046;&#20174;&#21333;&#30446;&#22270;&#20687;&#24207;&#21015;&#20013;&#24863;&#30693;&#19977;&#32500;&#20449;&#24687;&#12290;RPANet&#20805;&#20998;&#21033;&#29992;&#20102;&#39550;&#39542;&#22330;&#26223;&#20013;&#36947;&#36335;&#24179;&#38754;&#20960;&#20309;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2111.11089</link><description>&lt;p&gt;
&#21333;&#30446;&#36947;&#36335;&#24179;&#38754;&#35270;&#24046;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Monocular Road Planar Parallax Estimation. (arXiv:2111.11089v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.11089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;RPANet&#65292;&#29992;&#20110;&#36890;&#36807;&#24179;&#38754;&#35270;&#24046;&#20174;&#21333;&#30446;&#22270;&#20687;&#24207;&#21015;&#20013;&#24863;&#30693;&#19977;&#32500;&#20449;&#24687;&#12290;RPANet&#20805;&#20998;&#21033;&#29992;&#20102;&#39550;&#39542;&#22330;&#26223;&#20013;&#36947;&#36335;&#24179;&#38754;&#20960;&#20309;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20272;&#35745;&#21487;&#39550;&#39542;&#34920;&#38754;&#21644;&#21608;&#22260;&#29615;&#22659;&#30340;&#19977;&#32500;&#32467;&#26500;&#26159;&#36741;&#21161;&#21644;&#33258;&#21160;&#39550;&#39542;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#36890;&#24120;&#36890;&#36807;&#20351;&#29992;3D&#20256;&#24863;&#22120;&#65288;&#22914;LiDAR&#65289;&#25110;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#30452;&#25509;&#39044;&#27979;&#28857;&#30340;&#28145;&#24230;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#21069;&#32773;&#26114;&#36149;&#65292;&#21518;&#32773;&#32570;&#20047;&#22330;&#26223;&#30340;&#20960;&#20309;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;RPANet&#65292;&#29992;&#20110;&#22522;&#20110;&#24179;&#38754;&#35270;&#24046;&#20174;&#21333;&#30446;&#22270;&#20687;&#24207;&#21015;&#20013;&#24863;&#30693;&#19977;&#32500;&#20449;&#24687;&#65292;&#20805;&#20998;&#21033;&#29992;&#39550;&#39542;&#22330;&#26223;&#20013;&#26080;&#22788;&#19981;&#22312;&#30340;&#36947;&#36335;&#24179;&#38754;&#20960;&#20309;&#12290;RPANet&#20197;&#36890;&#36807;&#36947;&#36335;&#24179;&#38754;&#30340;&#21333;&#24212;&#24615;&#23545;&#40784;&#30340;&#22270;&#20687;&#23545;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#36755;&#20986;&#29992;&#20110;&#19977;&#32500;&#37325;&#24314;&#30340;&#947;&#22270;&#65288;&#39640;&#24230;&#19982;&#28145;&#24230;&#30340;&#27604;&#20540;&#65289;&#12290;&#947;&#22270;&#21487;&#20197;&#26500;&#24314;&#20004;&#20010;&#30456;&#37051;&#24103;&#20043;&#38388;&#30340;&#20108;&#32500;&#36716;&#25442;&#65292;&#26263;&#31034;&#20102;&#24179;&#38754;&#35270;&#24046;&#65292;&#24182;&#21487;&#20197;&#19982;&#36335;&#38754;&#20960;&#20309;&#20449;&#24687;&#20197;&#21450;&#20854;&#20182;&#32972;&#26223;&#32422;&#26463;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating the 3D structure of the drivable surface and surrounding environment is a crucial task for assisted and autonomous driving. It is commonly solved either by using 3D sensors such as LiDAR or directly predicting the depth of points via deep learning. However, the former is expensive, and the latter lacks the use of geometry information for the scene. In this paper, instead of following existing methodologies, we propose Road Planar Parallax Attention Network (RPANet), a new deep neural network for 3D sensing from monocular image sequences based on planar parallax, which takes full advantage of the omnipresent road plane geometry in driving scenes. RPANet takes a pair of images aligned by the homography of the road plane as input and outputs a $\gamma$ map (the ratio of height to depth) for 3D reconstruction. The $\gamma$ map has the potential to construct a two-dimensional transformation between two consecutive frames. It implies planar parallax and can be combined with the ro
&lt;/p&gt;</description></item><item><title>GFlowNets&#26159;&#19968;&#31181;&#29983;&#25104;&#27969;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20027;&#21160;&#23398;&#20064;&#29615;&#22659;&#20013;&#37319;&#26679;&#22810;&#26679;&#21270;&#30340;&#20505;&#36873;&#38598;&#12290;&#23427;&#20204;&#20855;&#26377;&#20272;&#35745;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#21644;&#36793;&#38469;&#20998;&#24067;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#34920;&#31034;&#20851;&#20110;&#22797;&#21512;&#23545;&#35937;&#65288;&#22914;&#38598;&#21512;&#21644;&#22270;&#65289;&#30340;&#20998;&#24067;&#12290;&#36890;&#36807;&#21333;&#27425;&#35757;&#32451;&#30340;&#29983;&#25104;&#20256;&#36882;&#65292;GFlowNets&#20998;&#25674;&#20102;&#35745;&#31639;&#26114;&#36149;&#30340;MCMC&#26041;&#27861;&#30340;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2111.09266</link><description>&lt;p&gt;
GFlowNet&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
GFlowNet Foundations. (arXiv:2111.09266v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.09266
&lt;/p&gt;
&lt;p&gt;
GFlowNets&#26159;&#19968;&#31181;&#29983;&#25104;&#27969;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20027;&#21160;&#23398;&#20064;&#29615;&#22659;&#20013;&#37319;&#26679;&#22810;&#26679;&#21270;&#30340;&#20505;&#36873;&#38598;&#12290;&#23427;&#20204;&#20855;&#26377;&#20272;&#35745;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#21644;&#36793;&#38469;&#20998;&#24067;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#34920;&#31034;&#20851;&#20110;&#22797;&#21512;&#23545;&#35937;&#65288;&#22914;&#38598;&#21512;&#21644;&#22270;&#65289;&#30340;&#20998;&#24067;&#12290;&#36890;&#36807;&#21333;&#27425;&#35757;&#32451;&#30340;&#29983;&#25104;&#20256;&#36882;&#65292;GFlowNets&#20998;&#25674;&#20102;&#35745;&#31639;&#26114;&#36149;&#30340;MCMC&#26041;&#27861;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;GFlowNets&#65289;&#34987;&#24341;&#20837;&#20026;&#22312;&#20027;&#21160;&#23398;&#20064;&#29615;&#22659;&#20013;&#37319;&#26679;&#22810;&#26679;&#21270;&#30340;&#20505;&#36873;&#38598;&#30340;&#26041;&#27861;&#65292;&#20854;&#35757;&#32451;&#30446;&#26631;&#20351;&#20854;&#36817;&#20284;&#25353;&#29031;&#32473;&#23450;&#30340;&#22870;&#21169;&#20989;&#25968;&#36827;&#34892;&#37319;&#26679;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;GFlowNets&#30340;&#19968;&#20123;&#39069;&#22806;&#30340;&#29702;&#35770;&#24615;&#36136;&#12290;&#23427;&#20204;&#21487;&#20197;&#29992;&#20110;&#20272;&#35745;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#21644;&#30456;&#24212;&#30340;&#36793;&#38469;&#20998;&#24067;&#65292;&#20854;&#20013;&#19968;&#20123;&#21464;&#37327;&#26410;&#25351;&#23450;&#65292;&#29305;&#21035;&#26159;&#21487;&#20197;&#34920;&#31034;&#20851;&#20110;&#22797;&#21512;&#23545;&#35937;&#65288;&#22914;&#38598;&#21512;&#21644;&#22270;&#65289;&#30340;&#20998;&#24067;&#12290;GFlowNets&#36890;&#36807;&#21333;&#27425;&#35757;&#32451;&#30340;&#29983;&#25104;&#20256;&#36882;&#26469;&#20998;&#25674;&#36890;&#24120;&#30001;&#35745;&#31639;&#26114;&#36149;&#30340;MCMC&#26041;&#27861;&#23436;&#25104;&#30340;&#24037;&#20316;&#12290;&#23427;&#20204;&#36824;&#21487;&#20197;&#29992;&#20110;&#20272;&#35745;&#20998;&#21306;&#20989;&#25968;&#21644;&#33258;&#30001;&#33021;&#65292;&#32473;&#23450;&#19968;&#20010;&#23376;&#38598;&#65288;&#23376;&#22270;&#65289;&#30340;&#36229;&#38598;&#65288;&#36229;&#22270;&#65289;&#30340;&#26465;&#20214;&#27010;&#29575;&#65292;&#20197;&#21450;&#32473;&#23450;&#19968;&#20010;&#38598;&#21512;&#65288;&#22270;&#65289;&#30340;&#25152;&#26377;&#36229;&#38598;&#65288;&#36229;&#22270;&#65289;&#30340;&#36793;&#38469;&#20998;&#24067;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20123;&#21464;&#20307;&#65292;&#20351;&#24471;&#21487;&#20197;&#20272;&#35745;&#29109;&#30340;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Flow Networks (GFlowNets) have been introduced as a method to sample a diverse set of candidates in an active learning context, with a training objective that makes them approximately sample in proportion to a given reward function. In this paper, we show a number of additional theoretical properties of GFlowNets. They can be used to estimate joint probability distributions and the corresponding marginal distributions where some variables are unspecified and, of particular interest, can represent distributions over composite objects like sets and graphs. GFlowNets amortize the work typically done by computationally expensive MCMC methods in a single but trained generative pass. They could also be used to estimate partition functions and free energies, conditional probabilities of supersets (supergraphs) given a subset (subgraph), as well as marginal distributions over all supersets (supergraphs) of a given set (graph). We introduce variations enabling the estimation of entro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SCORE&#31639;&#27861;&#65292;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#38477;&#20302;&#12290;&#36890;&#36807;&#24341;&#20837;&#36864;&#28779;&#34892;&#20026;&#20811;&#38534;&#27491;&#21017;&#21270;&#22120;&#65292;SCORE&#23454;&#29616;&#20102;SoTA&#24615;&#33021;&#65292;&#24182;&#28040;&#38500;&#20102;&#27425;&#20248;&#24615;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2110.12468</link><description>&lt;p&gt;
SCORE&#65306;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#38477;&#20302;
&lt;/p&gt;
&lt;p&gt;
SCORE: Spurious COrrelation REduction for Offline Reinforcement Learning. (arXiv:2110.12468v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.12468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SCORE&#31639;&#27861;&#65292;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#38477;&#20302;&#12290;&#36890;&#36807;&#24341;&#20837;&#36864;&#28779;&#34892;&#20026;&#20811;&#38534;&#27491;&#21017;&#21270;&#22120;&#65292;SCORE&#23454;&#29616;&#20102;SoTA&#24615;&#33021;&#65292;&#24182;&#28040;&#38500;&#20102;&#27425;&#20248;&#24615;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21033;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#35299;&#20915;&#24207;&#36143;&#20915;&#31574;&#38382;&#39064;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#35770;&#25991;&#21482;&#35752;&#35770;&#20102;&#23545;&#25239;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#34892;&#20026;&#30340;&#38450;&#24481;&#65292;&#32780;&#26412;&#25991;&#30740;&#31350;&#20102;&#26356;&#24191;&#27867;&#30340;&#38382;&#39064;&#65292;&#21363;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#19982;&#20915;&#31574;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#36825;&#26159;&#23548;&#33268;&#27425;&#20248;&#24615;&#30340;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#26377;&#25928;&#19988;&#29702;&#35770;&#19978;&#21487;&#35777;&#26126;&#30340;&#31639;&#27861;&#65306;&#29992;&#20110;&#31163;&#32447;RL&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#38477;&#20302;&#65288;SCORE&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;SCORE&#22312;&#26631;&#20934;&#22522;&#20934;&#65288;D4RL&#65289;&#19978;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#20197;3.1&#20493;&#21152;&#36895;&#29575;&#23454;&#29616;&#20102;SoTA&#24615;&#33021;&#12290;&#25152;&#25552;&#31639;&#27861;&#24341;&#20837;&#20102;&#19968;&#20010;&#36864;&#28779;&#34892;&#20026;&#20811;&#38534;&#27491;&#21017;&#21270;&#22120;&#26469;&#24110;&#21161;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#36825;&#23545;&#20110;&#28040;&#38500;&#27425;&#20248;&#24615;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#21512;&#29702;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#28201;&#21644;&#30340;&#26465;&#20214;&#19979;&#20854;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#30340;&#27425;&#32447;&#24615;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) harnesses the power of massive datasets for resolving sequential decision problems. Most existing papers only discuss defending against out-of-distribution (OOD) actions while we investigate a broader issue, the spurious correlations between epistemic uncertainty and decision-making, an essential factor that causes suboptimality. In this paper, we propose Spurious COrrelation REduction (SCORE) for offline RL, a practically effective and theoretically provable algorithm. We empirically show that SCORE achieves the SoTA performance with 3.1x acceleration on various tasks in a standard benchmark (D4RL). The proposed algorithm introduces an annealing behavior cloning regularizer to help produce a high-quality estimation of uncertainty which is critical for eliminating spurious correlations from suboptimality. Theoretically, we justify the rationality of the proposed method and prove its convergence to the optimal policy with a sublinear rate under mild a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;ceteris paribus&#20026;&#22522;&#30784;&#30340;&#27169;&#24577;&#35821;&#35328;&#65292;&#29992;&#20110;&#35299;&#37322;&#20108;&#36827;&#21046;&#20998;&#31867;&#22120;&#21450;&#20854;&#23646;&#24615;&#30340;&#25512;&#29702;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20004;&#20010;&#20851;&#20110;&#35821;&#35328;&#22522;&#25968;&#30340;&#35777;&#26126;&#31995;&#32479;&#30340;&#23436;&#22791;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;&#26080;&#38480;&#21464;&#37327;&#21644;&#26377;&#38480;&#21464;&#37327;&#24773;&#20917;&#19979;&#30340;&#21487;&#28385;&#36275;&#24615;&#26816;&#26597;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#36825;&#31181;&#35821;&#35328;&#26469;&#24418;&#24335;&#21270;&#22810;&#31181;&#35299;&#37322;&#27010;&#24565;&#65292;&#21253;&#25324;&#23545;&#20107;&#23454;&#12289;&#23545;&#27604;&#21644;&#21453;&#20107;&#23454;&#35299;&#37322;&#20197;&#21450;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2105.14452</link><description>&lt;p&gt;
&#19968;&#31181;&#32479;&#19968;&#30340;&#36923;&#36753;&#26694;&#26550;&#29992;&#20110;&#20998;&#31867;&#22120;&#31995;&#32479;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
A unified logical framework for explanations in classifier systems. (arXiv:2105.14452v6 [cs.LO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.14452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;ceteris paribus&#20026;&#22522;&#30784;&#30340;&#27169;&#24577;&#35821;&#35328;&#65292;&#29992;&#20110;&#35299;&#37322;&#20108;&#36827;&#21046;&#20998;&#31867;&#22120;&#21450;&#20854;&#23646;&#24615;&#30340;&#25512;&#29702;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20004;&#20010;&#20851;&#20110;&#35821;&#35328;&#22522;&#25968;&#30340;&#35777;&#26126;&#31995;&#32479;&#30340;&#23436;&#22791;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;&#26080;&#38480;&#21464;&#37327;&#21644;&#26377;&#38480;&#21464;&#37327;&#24773;&#20917;&#19979;&#30340;&#21487;&#28385;&#36275;&#24615;&#26816;&#26597;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#36825;&#31181;&#35821;&#35328;&#26469;&#24418;&#24335;&#21270;&#22810;&#31181;&#35299;&#37322;&#27010;&#24565;&#65292;&#21253;&#25324;&#23545;&#20107;&#23454;&#12289;&#23545;&#27604;&#21644;&#21453;&#20107;&#23454;&#35299;&#37322;&#20197;&#21450;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#20013;&#65292;&#24067;&#23572;&#20989;&#25968;&#23545;&#20110;&#35299;&#37322;&#20108;&#36827;&#21046;&#20998;&#31867;&#22120;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#20256;&#32479;&#30340;&#24067;&#23572;&#20989;&#25968;&#26041;&#27861;&#37319;&#29992;&#21629;&#39064;&#36923;&#36753;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;ceteris paribus&#20026;&#22522;&#30784;&#30340;&#27169;&#24577;&#35821;&#35328;&#65292;&#25903;&#25345;&#23545;&#20108;&#36827;&#21046;&#36755;&#20837;&#20998;&#31867;&#22120;&#21450;&#20854;&#23646;&#24615;&#36827;&#34892;&#25512;&#29702;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31995;&#21015;&#20998;&#31867;&#22120;&#27169;&#22411;&#65292;&#23558;&#20854;&#20844;&#29702;&#21270;&#20026;&#20851;&#20110;&#35821;&#35328;&#22522;&#25968;&#30340;&#20004;&#20010;&#35777;&#26126;&#31995;&#32479;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#20844;&#29702;&#31995;&#32479;&#30340;&#23436;&#22791;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#26080;&#38480;&#21464;&#37327;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#27169;&#24577;&#35821;&#35328;&#30340;&#21487;&#28385;&#36275;&#24615;&#26816;&#26597;&#38382;&#39064;&#26159;NEXPTIME&#23436;&#20840;&#30340;&#65292;&#32780;&#22312;&#26377;&#38480;&#21464;&#37327;&#24773;&#20917;&#19979;&#65292;&#35813;&#38382;&#39064;&#21464;&#20026;&#22810;&#39033;&#24335;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#36824;&#22312;&#26080;&#38480;&#21464;&#37327;&#24773;&#20917;&#19979;&#30830;&#23450;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;NP&#29255;&#27573;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#35821;&#35328;&#26469;&#24418;&#24335;&#21270;&#23545;&#20107;&#23454;&#26465;&#20214;&#20197;&#21450;&#21253;&#25324;&#20174;&#23646;&#12289;&#23545;&#27604;&#21644;&#21453;&#20107;&#23454;&#35299;&#37322;&#20197;&#21450;&#20559;&#35265;&#22312;&#20869;&#30340;&#21508;&#31181;&#35299;&#37322;&#27010;&#24565;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#25193;&#23637;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed a renewed interest in Boolean function in explaining binary classifiers in the field of explainable AI (XAI). The standard approach of Boolean function is propositional logic. We present a modal language of a ceteris paribus nature which supports reasoning about binary input classifiers and their properties. We study a family of classifier models, axiomatize it as two proof systems regarding the cardinality of the language and show completeness of our axiomatics. Moreover, we prove that satisfiability checking problem for our modal language is NEXPTIME-complete in the infinite-variable case, while it becomes polynomial in the finite-variable case. We furthermore identify an interesting NP fragment of our language in the infinite-variable case. We leverage the language to formalize counterfactual conditional as well as a variety of notions of explanation including abductive, contrastive and counterfactual explanations, and biases. Finally, we present two exte
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#35270;&#35273;Transformer&#30340;&#24212;&#29992;&#12290;Transformer&#26159;&#19968;&#31181;&#22522;&#20110;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#21644;&#36739;&#23569;&#35270;&#35273;&#29305;&#23450;&#30340;&#20559;&#35265;&#12290;&#26412;&#25991;&#23545;&#19981;&#21516;&#20219;&#21153;&#19979;&#30340;&#35270;&#35273;Transformer&#27169;&#22411;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#20998;&#26512;&#65292;&#21253;&#25324;&#39592;&#24178;&#32593;&#32476;&#12289;&#39640;/&#20013;&#23618;&#35270;&#35273;&#12289;&#20302;&#23618;&#35270;&#35273;&#21644;&#35270;&#39057;&#22788;&#29702;&#12290;&#21516;&#26102;&#36824;&#20171;&#32461;&#20102;&#23558;Transformer&#24212;&#29992;&#20110;&#30495;&#23454;&#35774;&#22791;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2012.12556</link><description>&lt;p&gt;
&#20851;&#20110;&#35270;&#35273;Transformer&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Visual Transformer. (arXiv:2012.12556v6 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2012.12556
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#35270;&#35273;Transformer&#30340;&#24212;&#29992;&#12290;Transformer&#26159;&#19968;&#31181;&#22522;&#20110;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#21644;&#36739;&#23569;&#35270;&#35273;&#29305;&#23450;&#30340;&#20559;&#35265;&#12290;&#26412;&#25991;&#23545;&#19981;&#21516;&#20219;&#21153;&#19979;&#30340;&#35270;&#35273;Transformer&#27169;&#22411;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#20998;&#26512;&#65292;&#21253;&#25324;&#39592;&#24178;&#32593;&#32476;&#12289;&#39640;/&#20013;&#23618;&#35270;&#35273;&#12289;&#20302;&#23618;&#35270;&#35273;&#21644;&#35270;&#39057;&#22788;&#29702;&#12290;&#21516;&#26102;&#36824;&#20171;&#32461;&#20102;&#23558;Transformer&#24212;&#29992;&#20110;&#30495;&#23454;&#35774;&#22791;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26159;&#19968;&#31181;&#22522;&#20110;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#26368;&#21021;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#12290;&#30001;&#20110;&#20854;&#24378;&#22823;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#30740;&#31350;&#20154;&#21592;&#27491;&#22312;&#25506;&#32034;&#23558;Transformer&#24212;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#22312;&#21508;&#31181;&#35270;&#35273;&#22522;&#20934;&#20013;&#65292;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#34920;&#29616;&#25509;&#36817;&#25110;&#20248;&#20110;&#21367;&#31215;&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#31561;&#20854;&#20182;&#31867;&#22411;&#30340;&#32593;&#32476;&#12290;&#30001;&#20110;&#20854;&#39640;&#24615;&#33021;&#21644;&#36739;&#23569;&#38656;&#35201;&#35270;&#35273;&#29305;&#23450;&#30340;&#24402;&#32435;&#20559;&#35265;&#65292;Transformer&#36234;&#26469;&#36234;&#21463;&#21040;&#35745;&#31639;&#26426;&#35270;&#35273;&#31038;&#21306;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#19981;&#21516;&#20219;&#21153;&#36827;&#34892;&#20998;&#31867;&#21644;&#20998;&#26512;&#20854;&#20248;&#32570;&#28857;&#30340;&#26041;&#24335;&#26469;&#22238;&#39038;&#36825;&#20123;&#35270;&#35273;Transformer&#27169;&#22411;&#12290;&#25105;&#20204;&#25506;&#32034;&#30340;&#20027;&#35201;&#31867;&#21035;&#21253;&#25324;&#39592;&#24178;&#32593;&#32476;&#12289;&#39640;/&#20013;&#23618;&#35270;&#35273;&#12289;&#20302;&#23618;&#35270;&#35273;&#21644;&#35270;&#39057;&#22788;&#29702;&#12290;&#25105;&#20204;&#36824;&#21253;&#25324;&#20102;&#23558;Transformer&#25512;&#21521;&#30495;&#23454;&#35774;&#22791;&#24212;&#29992;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer, first applied to the field of natural language processing, is a type of deep neural network mainly based on the self-attention mechanism. Thanks to its strong representation capabilities, researchers are looking at ways to apply transformer to computer vision tasks. In a variety of visual benchmarks, transformer-based models perform similar to or better than other types of networks such as convolutional and recurrent neural networks. Given its high performance and less need for vision-specific inductive bias, transformer is receiving more and more attention from the computer vision community. In this paper, we review these vision transformer models by categorizing them in different tasks and analyzing their advantages and disadvantages. The main categories we explore include the backbone network, high/mid-level vision, low-level vision, and video processing. We also include efficient transformer methods for pushing transformer into real device-based applications. Furthermo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#36830;&#20998;&#25968;&#26469;&#23398;&#20064;&#22806;&#25512;&#30340;&#26041;&#27861;&#65292;&#20197;&#39044;&#27979;&#36229;&#23548;&#20307;&#26448;&#26009;&#30340;&#20020;&#30028;&#28201;&#24230;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#38656;&#35201;&#22312;&#25193;&#23637;&#39046;&#22495;&#20869;&#20934;&#30830;&#36817;&#20284;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#23588;&#20854;&#22312;&#35774;&#35745;&#26032;&#32467;&#26500;&#26102;&#26368;&#20026;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2012.03774</link><description>&lt;p&gt;
&#23398;&#20064;&#20351;&#29992;&#36830;&#20998;&#25968;&#36827;&#34892;&#22806;&#25512;&#65306;&#39044;&#27979;&#36229;&#23548;&#20307;&#26448;&#26009;&#30340;&#20020;&#30028;&#28201;&#24230;
&lt;/p&gt;
&lt;p&gt;
Learning to extrapolate using continued fractions: Predicting the critical temperature of superconductor materials. (arXiv:2012.03774v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2012.03774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#36830;&#20998;&#25968;&#26469;&#23398;&#20064;&#22806;&#25512;&#30340;&#26041;&#27861;&#65292;&#20197;&#39044;&#27979;&#36229;&#23548;&#20307;&#26448;&#26009;&#30340;&#20020;&#30028;&#28201;&#24230;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#38656;&#35201;&#22312;&#25193;&#23637;&#39046;&#22495;&#20869;&#20934;&#30830;&#36817;&#20284;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#23588;&#20854;&#22312;&#35774;&#35745;&#26032;&#32467;&#26500;&#26102;&#26368;&#20026;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#39046;&#22495;&#20013;&#65292;&#20351;&#29992;&#26377;&#38480;&#30340;&#23454;&#20363;$S={(\mathbf{x^{(i)}},y^{(i)})}$&#26469;&#36817;&#20284;&#26410;&#30693;&#30446;&#26631;&#20989;&#25968;$y=f(\mathbf{x})$&#65292;&#20854;&#20013;$\mathbf{x^{(i)}}\in D$&#65292;$D$&#34920;&#31034;&#24863;&#20852;&#36259;&#30340;&#39046;&#22495;&#65292;&#26159;&#19968;&#20010;&#24120;&#35265;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#23558;$S$&#31216;&#20026;&#35757;&#32451;&#38598;&#65292;&#24182;&#26088;&#22312;&#25214;&#21040;&#19968;&#20010;&#20302;&#22797;&#26434;&#24230;&#30340;&#25968;&#23398;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#36817;&#20284;&#36825;&#20010;&#30446;&#26631;&#20989;&#25968;&#23545;&#20110;&#26032;&#30340;&#23454;&#20363;$\mathbf{x}$&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#36890;&#36807;&#22312;&#19968;&#20010;&#21333;&#29420;&#30340;&#38598;&#21512;$T=\{\mathbf{x^{(j)}}\}\subset D$&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#20854;&#20013;$T\neq S$&#65292;&#32463;&#24120;&#26377;$T\cap S=\emptyset$&#65292;&#20197;&#35780;&#20272;&#20854;&#22312;&#35757;&#32451;&#38598;&#20043;&#22806;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#26576;&#20123;&#24212;&#29992;&#19981;&#20165;&#38656;&#35201;&#22312;&#21407;&#22987;&#39046;&#22495;$D$&#20869;&#20934;&#30830;&#36817;&#20284;&#65292;&#36824;&#38656;&#35201;&#22312;&#21253;&#21547;$D$&#30340;&#25193;&#23637;&#39046;&#22495;$D'$&#20869;&#20934;&#30830;&#36817;&#20284;&#12290;&#36825;&#22312;&#28041;&#21450;&#26032;&#32467;&#26500;&#35774;&#35745;&#30340;&#22330;&#26223;&#20013;&#23588;&#20026;&#37325;&#35201;&#65292;&#22240;&#20026;&#26368;&#23567;&#21270;&#36817;&#20284;&#35823;&#24046;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of Artificial Intelligence (AI) and Machine Learning (ML), the approximation of unknown target functions $y=f(\mathbf{x})$ using limited instances $S={(\mathbf{x^{(i)}},y^{(i)})}$, where $\mathbf{x^{(i)}} \in D$ and $D$ represents the domain of interest, is a common objective. We refer to $S$ as the training set and aim to identify a low-complexity mathematical model that can effectively approximate this target function for new instances $\mathbf{x}$. Consequently, the model's generalization ability is evaluated on a separate set $T=\{\mathbf{x^{(j)}}\} \subset D$, where $T \neq S$, frequently with $T \cap S = \emptyset$, to assess its performance beyond the training set. However, certain applications require accurate approximation not only within the original domain $D$ but also in an extended domain $D'$ that encompasses $D$. This becomes particularly relevant in scenarios involving the design of new structures, where minimizing errors in approximations is crucial. For e
&lt;/p&gt;</description></item></channel></rss>