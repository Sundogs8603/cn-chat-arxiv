<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21322;&#21512;&#20316;&#39550;&#39542;&#27719;&#20837;&#33258;&#21160;&#39550;&#39542;&#23558;&#20250;&#23545;&#20844;&#36335;&#25972;&#20307;&#27969;&#37327;&#20135;&#29983;&#20160;&#20040;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#20102;&#21322;&#21512;&#20316;&#30340;&#22909;&#22788;&#19981;&#25104;&#27604;&#20363;&#22320;&#24433;&#21709;&#21033;&#24049;&#21644;&#39640;&#36895;&#39550;&#39542;&#21592;&#12290;</title><link>http://arxiv.org/abs/2304.11693</link><description>&lt;p&gt;
&#30740;&#31350;&#21322;&#21512;&#20316;&#39550;&#39542;&#23545;&#20844;&#36335;&#25972;&#20307;&#27969;&#37327;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Studying the Impact of Semi-Cooperative Drivers on Overall Highway Flow. (arXiv:2304.11693v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21322;&#21512;&#20316;&#39550;&#39542;&#27719;&#20837;&#33258;&#21160;&#39550;&#39542;&#23558;&#20250;&#23545;&#20844;&#36335;&#25972;&#20307;&#27969;&#37327;&#20135;&#29983;&#20160;&#20040;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#20102;&#21322;&#21512;&#20316;&#30340;&#22909;&#22788;&#19981;&#25104;&#27604;&#20363;&#22320;&#24433;&#21709;&#21033;&#24049;&#21644;&#39640;&#36895;&#39550;&#39542;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#21512;&#20316;&#34892;&#20026;&#26159;&#20154;&#31867;&#39550;&#39542;&#21592;&#22266;&#26377;&#30340;&#29305;&#24615;&#65292;&#24212;&#35813;&#32771;&#34385;&#21040;&#33258;&#21160;&#39550;&#39542;&#12290;&#27492;&#22806;&#65292;&#26032;&#30340;&#33258;&#20027;&#35268;&#21010;&#22120;&#21487;&#20197;&#32771;&#34385;&#20154;&#31867;&#39550;&#39542;&#21592;&#30340;&#31038;&#20250;&#20215;&#20540;&#21462;&#21521;&#65288;SVO&#65289;&#20197;&#29983;&#25104;&#31526;&#21512;&#31038;&#20250;&#20934;&#21017;&#30340;&#36712;&#36857;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26032;&#22411;&#35268;&#21010;&#22120;&#23545;&#20132;&#36890;&#27969;&#37327;&#30340;&#25972;&#20307;&#24433;&#21709;&#20173;&#38656;&#20102;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38544;&#24335;&#21322;&#21512;&#20316;&#39550;&#39542;&#65292;&#20854;&#20013;&#20195;&#29702;&#20154;&#37096;&#32626;&#20102;&#19968;&#20010;&#21338;&#24328;&#35770;&#29256;&#26412;&#30340;&#36845;&#20195;&#26368;&#20339;&#21709;&#24212;&#65292;&#20551;&#23450;&#30693;&#36947;&#20854;&#20182;&#20195;&#29702;&#20154;&#30340;SVO&#12290;&#25105;&#20204;&#27169;&#25311;&#21517;&#20041;&#20132;&#36890;&#27969;&#37327;&#65292;&#24182;&#30740;&#31350;&#20102;&#36947;&#36335;&#19978;&#30340;&#21033;&#20182;&#20195;&#29702;&#20154;&#27604;&#20363;&#26159;&#21542;&#24433;&#21709;&#20010;&#20307;&#25110;&#31995;&#32479;&#32423;&#39550;&#39542;&#34920;&#29616;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#21033;&#20182;&#20195;&#29702;&#20154;&#30340;&#27604;&#20363;&#23545;&#25972;&#20307;&#20132;&#36890;&#27969;&#37327;&#24433;&#21709;&#36739;&#23567;&#65292;&#32780;&#21322;&#21512;&#20316;&#30340;&#22909;&#22788;&#19981;&#25104;&#27604;&#20363;&#22320;&#24433;&#21709;&#21033;&#24049;&#21644;&#39640;&#36895;&#39550;&#39542;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-cooperative behaviors are intrinsic properties of human drivers and should be considered for autonomous driving. In addition, new autonomous planners can consider the social value orientation (SVO) of human drivers to generate socially-compliant trajectories. Yet the overall impact on traffic flow for this new class of planners remain to be understood. In this work, we present study of implicit semi-cooperative driving where agents deploy a game-theoretic version of iterative best response assuming knowledge of the SVOs of other agents. We simulate nominal traffic flow and investigate whether the proportion of prosocial agents on the road impact individual or system-wide driving performance. Experiments show that the proportion of prosocial agents has a minor impact on overall traffic flow and that benefits of semi-cooperation disproportionally affect egoistic and high-speed drivers.
&lt;/p&gt;</description></item><item><title>CoReFace &#26159;&#19968;&#31181;&#26679;&#26412;&#23548;&#21521;&#30340;&#23545;&#27604;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#30452;&#25509;&#32422;&#26463;&#22270;&#20687;&#38388;&#20851;&#31995;&#65292;&#25552;&#39640;&#28145;&#24230;&#20154;&#33080;&#35782;&#21035;&#24615;&#33021;&#24182;&#33021;&#22788;&#29702;&#22823;&#30340;&#23039;&#24577;&#21644;&#34920;&#24773;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.11668</link><description>&lt;p&gt;
CoReFace: &#38754;&#21521;&#28145;&#24230;&#20154;&#33080;&#35782;&#21035;&#30340;&#26679;&#26412;&#23548;&#21521;&#23545;&#27604;&#27491;&#21017;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CoReFace: Sample-Guided Contrastive Regularization for Deep Face Recognition. (arXiv:2304.11668v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11668
&lt;/p&gt;
&lt;p&gt;
CoReFace &#26159;&#19968;&#31181;&#26679;&#26412;&#23548;&#21521;&#30340;&#23545;&#27604;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#30452;&#25509;&#32422;&#26463;&#22270;&#20687;&#38388;&#20851;&#31995;&#65292;&#25552;&#39640;&#28145;&#24230;&#20154;&#33080;&#35782;&#21035;&#24615;&#33021;&#24182;&#33021;&#22788;&#29702;&#22823;&#30340;&#23039;&#24577;&#21644;&#34920;&#24773;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#34920;&#36798;&#30340;&#21487;&#20998;&#24615;&#23545;&#20110;&#24320;&#25918;&#24335;&#20154;&#33080;&#35782;&#21035;&#33267;&#20851;&#37325;&#35201;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#20195;&#34920;&#36523;&#20221;&#30340;&#21487;&#23398;&#20064;&#20998;&#31867;&#23618;&#26435;&#37325;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#36807;&#31243;&#19981;&#23398;&#20064;&#36523;&#20221;&#34920;&#31034;&#65292;&#24182;&#22312;&#35757;&#32451;&#20013;&#21024;&#38500;&#20998;&#31867;&#22120;&#12290;&#36825;&#31181;&#19981;&#19968;&#33268;&#21487;&#33021;&#20250;&#22256;&#24785;&#29305;&#24449;&#32534;&#30721;&#22120;&#65292;&#38459;&#30861;&#22522;&#20110;&#36523;&#20221;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#32531;&#35299;&#20197;&#19978;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#38754;&#21521;&#20154;&#33080;&#35782;&#21035;&#30340;&#23545;&#27604;&#27491;&#21017;&#21270;&#65288;CoReFace&#65289;&#65292;&#36890;&#36807;&#23545;&#29305;&#24449;&#34920;&#31034;&#23398;&#20064;&#24212;&#29992;&#22522;&#20110;&#22270;&#20687;&#30340;&#27491;&#21017;&#21270;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#37319;&#29992;&#26679;&#26412;&#23548;&#21521;&#30340;&#23545;&#27604;&#23398;&#20064;&#26469;&#30452;&#25509;&#32422;&#26463;&#22270;&#20687;&#38388;&#20851;&#31995;&#65292;&#36825;&#19982;&#35780;&#20272;&#36807;&#31243;&#19968;&#33268;&#12290;&#20026;&#20102;&#23558;&#23545;&#27604;&#23398;&#20064;&#34701;&#20837;&#20154;&#33080;&#35782;&#21035;&#20013;&#65292;&#25105;&#20204;&#22686;&#24378;&#20102;&#23884;&#20837;&#32780;&#19981;&#26159;&#22270;&#20687;&#65292;&#36991;&#20813;&#20102;&#22270;&#20687;&#36136;&#37327;&#36864;&#21270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38024;&#23545;&#21464;&#25442;&#19981;&#21464;&#20154;&#33080;&#35782;&#21035;&#30340;&#23545;&#27604;&#25439;&#22833;&#65292;&#33021;&#22815;&#22788;&#29702;&#22823;&#30340;&#23039;&#24577;&#21644;&#34920;&#24773;&#21464;&#21270;&#12290;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25552;&#39640;&#28145;&#24230;&#20154;&#33080;&#35782;&#21035;&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The discriminability of feature representation is the key to open-set face recognition. Previous methods rely on the learnable weights of the classification layer that represent the identities. However, the evaluation process learns no identity representation and drops the classifier from training. This inconsistency could confuse the feature encoder in understanding the evaluation goal and hinder the effect of identity-based methods. To alleviate the above problem, we propose a novel approach namely Contrastive Regularization for Face recognition (CoReFace) to apply image-level regularization in feature representation learning. Specifically, we employ sample-guided contrastive learning to regularize the training with the image-image relationship directly, which is consistent with the evaluation process. To integrate contrastive learning into face recognition, we augment embeddings instead of images to avoid the image quality degradation. Then, we propose a novel contrastive loss for t
&lt;/p&gt;</description></item><item><title>IslamicPCQA&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#38750;&#32467;&#26500;&#21270;&#20449;&#24687;&#28304;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#30340;&#27874;&#26031;&#35821;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20174;9&#37096;&#20234;&#26031;&#20848;&#30334;&#31185;&#20840;&#20070;&#20013;&#25552;&#21462;&#30340;12,282&#20010;&#38382;&#39064;-&#31572;&#26696;&#23545;&#65292;&#26088;&#22312;&#26041;&#20415;&#22238;&#31572;&#28041;&#21450;&#20234;&#26031;&#20848;&#25991;&#26412;&#36164;&#28304;&#30340;&#22797;&#26434;&#27874;&#26031;&#35821;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.11664</link><description>&lt;p&gt;
IslamicPCQA&#65306;&#22522;&#20110;&#20234;&#26031;&#20848;&#25991;&#26412;&#36164;&#28304;&#30340;&#27874;&#26031;&#35821;&#22810;&#36339;&#22797;&#26434;&#38382;&#31572;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
IslamicPCQA: A Dataset for Persian Multi-hop Complex Question Answering in Islamic Text Resources. (arXiv:2304.11664v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11664
&lt;/p&gt;
&lt;p&gt;
IslamicPCQA&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#38750;&#32467;&#26500;&#21270;&#20449;&#24687;&#28304;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#30340;&#27874;&#26031;&#35821;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20174;9&#37096;&#20234;&#26031;&#20848;&#30334;&#31185;&#20840;&#20070;&#20013;&#25552;&#21462;&#30340;12,282&#20010;&#38382;&#39064;-&#31572;&#26696;&#23545;&#65292;&#26088;&#22312;&#26041;&#20415;&#22238;&#31572;&#28041;&#21450;&#20234;&#26031;&#20848;&#25991;&#26412;&#36164;&#28304;&#30340;&#22797;&#26434;&#27874;&#26031;&#35821;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#65292;&#38382;&#31572;&#31995;&#32479;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26159;&#20351;&#29992;&#21508;&#31181;&#20449;&#24687;&#28304;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#12290;&#22810;&#36339;&#38382;&#39064;&#26159;&#19968;&#31181;&#38656;&#35201;&#22810;&#27493;&#25512;&#29702;&#25165;&#33021;&#22238;&#31572;&#30340;&#22797;&#26434;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;IslamicPCQA&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#31532;&#19968;&#20221;&#22522;&#20110;&#38750;&#32467;&#26500;&#21270;&#20449;&#24687;&#28304;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#30340;&#27874;&#26031;&#35821;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20174;9&#37096;&#20234;&#26031;&#20848;&#30334;&#31185;&#20840;&#20070;&#20013;&#25552;&#21462;&#30340;12,282&#20010;&#38382;&#39064;-&#31572;&#26696;&#23545;&#12290;&#35813;&#25968;&#25454;&#38598;&#21463;HotpotQA&#33521;&#35821;&#25968;&#25454;&#38598;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#32463;&#36807;&#23450;&#21046;&#20197;&#36866;&#24212;&#27874;&#26031;&#35821;&#30340;&#22797;&#26434;&#24615;&#12290;&#22238;&#31572;&#35813;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#38656;&#35201;&#22810;&#20010;&#27573;&#33853;&#21644;&#25512;&#29702;&#36807;&#31243;&#12290;&#38382;&#39064;&#19981;&#38480;&#20110;&#20219;&#20309;&#20808;&#21069;&#30340;&#30693;&#35782;&#24211;&#25110;&#26412;&#20307;&#35770;&#65292;&#24182;&#19988;&#20026;&#20102;&#25552;&#20379;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#35813;&#25968;&#25454;&#38598;&#36824;&#21253;&#25324;&#25903;&#25345;&#20107;&#23454;&#21644;&#20851;&#38190;&#21477;&#23376;&#12290;&#20934;&#22791;&#22909;&#30340;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#20234;&#26031;&#20848;&#20027;&#39064;&#65292;&#26088;&#22312;&#26041;&#20415;&#22238;&#31572;&#28041;&#21450;&#20234;&#26031;&#20848;&#25991;&#26412;&#36164;&#28304;&#30340;&#22797;&#26434;&#27874;&#26031;&#35821;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, one of the main challenges for Question Answering Systems is to answer complex questions using various sources of information. Multi-hop questions are a type of complex questions that require multi-step reasoning to answer. In this article, the IslamicPCQA dataset is introduced. This is the first Persian dataset for answering complex questions based on non-structured information sources and consists of 12,282 question-answer pairs extracted from 9 Islamic encyclopedias. This dataset has been created inspired by the HotpotQA English dataset approach, which was customized to suit the complexities of the Persian language. Answering questions in this dataset requires more than one paragraph and reasoning. The questions are not limited to any prior knowledge base or ontology, and to provide robust reasoning ability, the dataset also includes supporting facts and key sentences. The prepared dataset covers a wide range of Islamic topics and aims to facilitate answering complex Persi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31574;&#30053;&#26469;&#36991;&#20813;&#28145;&#24230;&#24179;&#34913;&#27169;&#22411;&#23618;&#20013;&#21453;&#21521;&#20256;&#25773;&#30340;&#35745;&#31639;&#36127;&#25285;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#21152;&#36895;&#35757;&#32451;&#65292;&#21516;&#26102;&#19981;&#20250;&#24433;&#21709;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.11663</link><description>&lt;p&gt;
&#28145;&#24230;&#24179;&#34913;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Efficient Training of Deep Equilibrium Models. (arXiv:2304.11663v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11663
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31574;&#30053;&#26469;&#36991;&#20813;&#28145;&#24230;&#24179;&#34913;&#27169;&#22411;&#23618;&#20013;&#21453;&#21521;&#20256;&#25773;&#30340;&#35745;&#31639;&#36127;&#25285;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#21152;&#36895;&#35757;&#32451;&#65292;&#21516;&#26102;&#19981;&#20250;&#24433;&#21709;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24179;&#34913;&#27169;&#22411;&#65288;DEQ&#65289;&#22312;&#23398;&#20064;&#25968;&#25454;&#34920;&#31034;&#26041;&#38754;&#24050;&#32463;&#34987;&#35777;&#26126;&#38750;&#24120;&#24378;&#22823;&#12290;&#20854;&#24605;&#24819;&#26159;&#29992;&#38544;&#24335;&#30340;&#22266;&#23450;&#28857;&#26041;&#31243;&#26367;&#25442;&#20256;&#32479;&#65288;&#26174;&#24335;&#30340;&#65289;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65292;&#20174;&#32780;&#20801;&#35768;&#35299;&#32806;&#21069;&#21521;&#21644;&#21518;&#21521;&#20256;&#36882;&#12290;&#29305;&#21035;&#22320;&#65292;&#36890;&#36807;&#38544;&#24335;&#20989;&#25968;&#23450;&#29702;&#65292;DEQ&#23618;&#30340;&#35757;&#32451;&#21464;&#24471;&#38750;&#24120;&#39640;&#25928;&#12290;&#20294;&#26159;&#65292;&#36890;&#36807;DEQ&#23618;&#30340;&#21453;&#21521;&#20256;&#25773;&#20173;&#38656;&#35201;&#35299;&#20915;&#19968;&#20010;&#26114;&#36149;&#30340;&#22522;&#20110;Jacobian&#30340;&#26041;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31574;&#30053;&#26469;&#36991;&#20813;&#36825;&#31181;&#35745;&#31639;&#36127;&#25285;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;Broyden&#26041;&#27861;&#30340;Jacobian&#36817;&#20284;&#65292;&#22312;&#21069;&#21521;&#20256;&#36882;&#20043;&#21518;&#35745;&#31639;&#21453;&#21521;&#20256;&#36882;&#20013;&#30340;&#26799;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#31616;&#21333;&#22320;&#37325;&#22797;&#20351;&#29992;&#36825;&#20010;&#36817;&#20284;&#21487;&#20197;&#26174;&#33879;&#21152;&#36895;&#35757;&#32451;&#65292;&#21516;&#26102;&#19981;&#20250;&#36896;&#25104;&#24615;&#33021;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep equilibrium models (DEQs) have proven to be very powerful for learning data representations. The idea is to replace traditional (explicit) feedforward neural networks with an implicit fixed-point equation, which allows to decouple the forward and backward passes. In particular, training DEQ layers becomes very memory-efficient via the implicit function theorem. However, backpropagation through DEQ layers still requires solving an expensive Jacobian-based equation. In this paper, we introduce a simple but effective strategy to avoid this computational burden. Our method relies on the Jacobian approximation of Broyden's method after the forward pass to compute the gradients during the backward pass. Experiments show that simply re-using this approximation can significantly speed up the training while not causing any performance degradation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986; Iter-CoT &#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#36845;&#20195;&#22686;&#24378;&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;&#65292;&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#36866;&#24230;&#38590;&#24230;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#21487;&#22238;&#31572;&#30340;&#38382;&#39064;&#65292;&#24182;&#20276;&#38543;&#25512;&#29702;&#38142;&#20316;&#20026;&#31034;&#20363;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#20351;&#27169;&#22411;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#29983;&#25104;&#25512;&#29702;&#38142;&#12290;</title><link>http://arxiv.org/abs/2304.11657</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21152;&#24378;&#36845;&#20195;&#22686;&#24378;&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Enhancing Chain-of-Thoughts Prompting with Iterative Bootstrapping in Large Language Models. (arXiv:2304.11657v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986; Iter-CoT &#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#36845;&#20195;&#22686;&#24378;&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;&#65292;&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#36866;&#24230;&#38590;&#24230;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#21487;&#22238;&#31572;&#30340;&#38382;&#39064;&#65292;&#24182;&#20276;&#38543;&#25512;&#29702;&#38142;&#20316;&#20026;&#31034;&#20363;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#20351;&#27169;&#22411;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#29983;&#25104;&#25512;&#29702;&#38142;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36880;&#27493;&#24341;&#23548;&#24605;&#32500;&#38142; (CoT) &#20316;&#20026;&#31034;&#33539;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#21487;&#20197;&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#19978;&#23454;&#29616;&#39640;&#24230;&#26377;&#25928;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;LLMs &#29983;&#25104;&#30340;&#28436;&#31034;&#25512;&#29702;&#38142;&#23481;&#26131;&#20986;&#29616;&#38169;&#35823;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#38169;&#35823;&#12290;&#27492;&#22806;&#65292;&#19981;&#24688;&#24403;&#30340;&#31034;&#20363; (&#36807;&#20110;&#31616;&#21333;&#25110;&#22797;&#26434;) &#21487;&#20197;&#24433;&#21709;&#22312;&#19981;&#21516;&#38590;&#24230;&#32423;&#21035;&#19979;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Iter-CoT (&#36845;&#20195;&#24341;&#23548;&#24605;&#32500;&#38142;&#25552;&#31034;) &#30340;&#36845;&#20195;&#24341;&#23548;&#26041;&#27861;&#65292;&#29992;&#20110;&#36873;&#25321;&#23454;&#20363;&#24182;&#29983;&#25104;&#25512;&#29702;&#38142;&#12290;&#36890;&#36807;&#21033;&#29992;&#36845;&#20195;&#22686;&#24378;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;LLMs &#33258;&#20027;&#26356;&#27491;&#38169;&#35823;&#65292;&#20174;&#32780;&#20135;&#29983;&#26356;&#31934;&#30830;&#12289;&#20840;&#38754;&#30340;&#25512;&#29702;&#38142;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36873;&#25321;&#20855;&#26377;&#36866;&#24230;&#38590;&#24230;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#21487;&#22238;&#31572;&#30340;&#38382;&#39064;&#65292;&#24182;&#20276;&#38543;&#25512;&#29702;&#38142;&#20316;&#20026;&#31034;&#20363;&#65292;&#20174;&#32780;&#22686;&#24378;LLMs &#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) can achieve highly effective performance on various reasoning tasks by incorporating step-by-step chain-of-thought (CoT) prompting as demonstrations. However, the reasoning chains of demonstrations generated by LLMs are prone to errors, which can subsequently lead to incorrect reasoning during inference. Furthermore, inappropriate exemplars (overly simplistic or complex), can affect overall performance among varying levels of difficulty. We introduce Iter-CoT (Iterative bootstrapping in Chain-of-Thoughts Prompting), an iterative bootstrapping approach for selecting exemplars and generating reasoning chains. By utilizing iterative bootstrapping, our approach enables LLMs to autonomously rectify errors, resulting in more precise and comprehensive reasoning chains. Simultaneously, our approach selects challenging yet answerable questions accompanied by reasoning chains as exemplars with a moderate level of difficulty, which enhances the LLMs' generalizability 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35268;&#33539;&#35821;&#35328;&#65292;&#21517;&#20026;&#20248;&#20808;&#23450;&#24615;&#36873;&#25321;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65292;&#29992;&#20110;&#25551;&#36848;&#20855;&#26377;&#20248;&#20808;&#32423;&#20559;&#22909;&#30340;&#27010;&#29575;&#29615;&#22659;&#19979;&#30340;&#26102;&#38388;&#35268;&#21010;&#12290;&#35813;&#35821;&#35328;&#20801;&#35768;&#31616;&#27905;&#22320;&#25351;&#23450;&#19982;&#23436;&#25104;&#27599;&#20010;&#26102;&#38388;&#20219;&#21153;&#30456;&#23545;&#24212;&#30340;&#20855;&#26377;&#30456;&#24212;&#20248;&#20808;&#32423;&#30340;&#26102;&#38388;&#30446;&#26631;&#65292;&#24182;&#36890;&#36807;&#26377;&#38480;&#36712;&#36857;&#19978;&#30340;&#20248;&#20808;&#32423;&#21512;&#21462;&#21644;&#26377;&#24207;&#26512;&#21462;&#26469;&#25193;&#23637;&#26377;&#38480;&#36712;&#36857;&#19978;&#30340;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#12290;</title><link>http://arxiv.org/abs/2304.11641</link><description>&lt;p&gt;
&#20855;&#26377;&#20248;&#20808;&#32423;&#20559;&#22909;&#30340;&#27010;&#29575;&#35268;&#21010;&#21644;&#26102;&#38388;&#36923;&#36753;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Planning with Prioritized Preferences over Temporal Logic Objectives. (arXiv:2304.11641v1 [cs.FL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35268;&#33539;&#35821;&#35328;&#65292;&#21517;&#20026;&#20248;&#20808;&#23450;&#24615;&#36873;&#25321;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65292;&#29992;&#20110;&#25551;&#36848;&#20855;&#26377;&#20248;&#20808;&#32423;&#20559;&#22909;&#30340;&#27010;&#29575;&#29615;&#22659;&#19979;&#30340;&#26102;&#38388;&#35268;&#21010;&#12290;&#35813;&#35821;&#35328;&#20801;&#35768;&#31616;&#27905;&#22320;&#25351;&#23450;&#19982;&#23436;&#25104;&#27599;&#20010;&#26102;&#38388;&#20219;&#21153;&#30456;&#23545;&#24212;&#30340;&#20855;&#26377;&#30456;&#24212;&#20248;&#20808;&#32423;&#30340;&#26102;&#38388;&#30446;&#26631;&#65292;&#24182;&#36890;&#36807;&#26377;&#38480;&#36712;&#36857;&#19978;&#30340;&#20248;&#20808;&#32423;&#21512;&#21462;&#21644;&#26377;&#24207;&#26512;&#21462;&#26469;&#25193;&#23637;&#26377;&#38480;&#36712;&#36857;&#19978;&#30340;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26631;&#35760;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#20855;&#26377;&#22810;&#20010;&#26102;&#38388;&#30446;&#26631;&#30340;&#29992;&#25143;&#20248;&#20808;&#32423;&#20559;&#22909;&#19979;&#30340;&#27010;&#29575;&#29615;&#22659;&#19979;&#30340;&#26102;&#38388;&#35268;&#21010;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#23558;&#36825;&#20123;&#20559;&#22909;&#21453;&#26144;&#20026;&#30446;&#26631;&#30340;&#20248;&#20808;&#32423;&#21015;&#34920;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35268;&#33539;&#35821;&#35328;&#65292;&#31216;&#20026;&#26377;&#38480;&#36712;&#36857;&#19978;&#30340;&#20248;&#20808;&#23450;&#24615;&#36873;&#25321;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65292;&#20854;&#36890;&#36807;&#23450;&#24615;&#36873;&#25321;&#36923;&#36753;&#19978;&#30340;&#20248;&#20808;&#32423;&#21512;&#21462;&#21644;&#26377;&#24207;&#26512;&#21462;&#26469;&#25193;&#23637;&#20102;&#26377;&#38480;&#36712;&#36857;&#19978;&#30340;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#12290;&#35813;&#35821;&#35328;&#20801;&#35768;&#31616;&#27905;&#22320;&#25351;&#23450;&#19982;&#23436;&#25104;&#27599;&#20010;&#26102;&#38388;&#20219;&#21153;&#30456;&#23545;&#24212;&#30340;&#20855;&#26377;&#30456;&#24212;&#20248;&#20808;&#32423;&#30340;&#26102;&#38388;&#30446;&#26631;&#12290;&#25551;&#36848;&#31995;&#32479;&#34892;&#20026;&#30340;&#26377;&#38480;&#36712;&#36857;&#22522;&#20110;&#20854;&#23545;&#20844;&#24335;&#30340;&#19981;&#28385;&#24847;&#24230;&#24471;&#20998;&#36827;&#34892;&#25490;&#21517;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20174;&#26032;&#35821;&#35328;&#21040;&#21152;&#26435;&#30830;&#23450;&#24615;&#26377;&#38480;&#33258;&#21160;&#26426;&#30340;&#31995;&#32479;&#21270;&#36716;&#25442;&#12290;&#21033;&#29992;&#36825;&#20010;&#35745;&#31639;&#27169;&#22411;&#65292;&#25105;&#20204;&#21046;&#23450;&#24182;&#35299;&#20915;&#20102;&#19968;&#20010;&#35745;&#31639;&#26368;&#20248;&#31574;&#30053;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies temporal planning in probabilistic environments, modeled as labeled Markov decision processes (MDPs), with user preferences over multiple temporal goals. Existing works reflect such preferences as a prioritized list of goals. This paper introduces a new specification language, termed prioritized qualitative choice linear temporal logic on finite traces, which augments linear temporal logic on finite traces with prioritized conjunction and ordered disjunction from prioritized qualitative choice logic. This language allows for succinctly specifying temporal objectives with corresponding preferences accomplishing each temporal task. The finite traces that describe the system's behaviors are ranked based on their dissatisfaction scores with respect to the formula. We propose a systematic translation from the new language to a weighted deterministic finite automaton. Utilizing this computational model, we formulate and solve a problem of computing an optimal policy that m
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20803;&#21629;&#20196;&#36890;&#20449;&#30340;&#26694;&#26550;&#65292;&#21363; MCC &#26694;&#26550;&#65292;&#20197;&#26377;&#25928;&#25506;&#32034; MOBA &#28216;&#25103;&#30340;&#20154;&#26426;&#21327;&#20316;&#65292;&#24182;&#25552;&#20379;&#35299;&#37322;&#24615;&#36890;&#20449;&#26426;&#21046;&#26469;&#24110;&#21161;&#29702;&#35299;&#20154;&#26426;&#21327;&#20316;&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;</title><link>http://arxiv.org/abs/2304.11632</link><description>&lt;p&gt;
&#22312; MOBA &#28216;&#25103;&#20013;&#23454;&#29616;&#26377;&#25928;&#21644;&#21487;&#35299;&#37322;&#30340;&#20154;&#26426;&#21327;&#20316;&#65306;&#19968;&#31181;&#36890;&#20449;&#8203;&#8203;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Towards Effective and Interpretable Human-Agent Collaboration in MOBA Games: A Communication Perspective. (arXiv:2304.11632v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11632
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20803;&#21629;&#20196;&#36890;&#20449;&#30340;&#26694;&#26550;&#65292;&#21363; MCC &#26694;&#26550;&#65292;&#20197;&#26377;&#25928;&#25506;&#32034; MOBA &#28216;&#25103;&#30340;&#20154;&#26426;&#21327;&#20316;&#65292;&#24182;&#25552;&#20379;&#35299;&#37322;&#24615;&#36890;&#20449;&#26426;&#21046;&#26469;&#24110;&#21161;&#29702;&#35299;&#20154;&#26426;&#21327;&#20316;&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
MOBA &#28216;&#25103;&#65292;&#22914; Dota2 &#21644;&#29579;&#32773;&#33635;&#32768;&#65292;&#24050;&#32463;&#25104;&#20026;&#26368;&#36817;&#20851;&#20110;&#28216;&#25103;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#27979;&#35797;&#22522;&#22320;&#65292;&#36804;&#20170;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#35768;&#22810;&#22312;&#20154;&#31867;&#27700;&#24179;&#19978;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20027;&#35201;&#38598;&#20013;&#22312;&#22914;&#20309;&#19982;&#20154;&#31867;&#31454;&#20105;&#65292;&#32780;&#19981;&#26159;&#25506;&#32034;&#22914;&#20309;&#19982;&#20154;&#31867;&#21512;&#20316;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#39318;&#27425;&#23581;&#35797;&#30740;&#31350; MOBA &#28216;&#25103;&#20013;&#30340;&#20154;&#26426;&#21327;&#20316;&#12290;&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#20803;&#21629;&#20196;&#36890;&#20449;&#30340;&#26694;&#26550;&#65288;MCC&#65289;&#65292;&#20351;&#20154;&#31867;&#21644;&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#26126;&#30830;&#30340;&#36890;&#20449;&#21327;&#20316;&#65292;&#20197;&#23454;&#29616;&#22312; MOBA &#28216;&#25103;&#20013;&#26377;&#25928;&#30340;&#20154;&#26426;&#21327;&#20316;&#12290;MCC &#26694;&#26550;&#30001;&#20004;&#20010;&#20851;&#38190;&#27169;&#22359;&#32452;&#25104;&#65306;1&#65289;&#35299;&#37322;&#24615;&#36890;&#20449;&#21327;&#35758;&#65292;&#21363;&#20803;&#21629;&#20196;&#65292;&#20197;&#24357;&#21512;&#20154;&#31867;&#21644;&#20195;&#29702;&#20043;&#38388;&#30340;&#36890;&#20449;&#24046;&#36317;&#65307;2&#65289;&#20803;&#21629;&#20196;&#20540;&#20272;&#35745;&#22120;&#65292;&#21363;&#20803;&#21629;&#20196;&#36873;&#25321;&#22120;&#65292;&#36873;&#25321;&#27599;&#20010;&#20195;&#29702;&#23436;&#25104;&#26377;&#25928;&#20154;&#26426;&#21327;&#20316;&#30340;&#26377;&#20215;&#20540;&#20803;&#21629;&#20196;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340; MCC &#26694;&#26550;&#21487;&#20197;&#26377;&#25928;&#22320;&#21152;&#24555;&#20154;&#26426;&#21327;&#20316;&#65292;&#24182;&#19988; MCC &#26694;&#26550;&#25552;&#20379;&#30340;&#21487;&#35299;&#37322;&#24615;&#36890;&#20449;&#26426;&#21046;&#26377;&#21161;&#20110;&#28145;&#20837;&#20102;&#35299;&#22312; MOBA &#28216;&#25103;&#20013;&#20154;&#26426;&#21327;&#20316;&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
MOBA games, e.g., Dota2 and Honor of Kings, have been actively used as the testbed for the recent AI research on games, and various AI systems have been developed at the human level so far. However, these AI systems mainly focus on how to compete with humans, less on exploring how to collaborate with humans. To this end, this paper makes the first attempt to investigate human-agent collaboration in MOBA games. In this paper, we propose to enable humans and agents to collaborate through explicit communication by designing an efficient and interpretable Meta-Command Communication-based framework, dubbed MCC, for accomplishing effective human-agent collaboration in MOBA games. The MCC framework consists of two pivotal modules: 1) an interpretable communication protocol, i.e., the Meta-Command, to bridge the communication gap between humans and agents; 2) a meta-command value estimator, i.e., the Meta-Command Selector, to select a valuable meta-command for each agent to achieve effective h
&lt;/p&gt;</description></item><item><title>TSGCNeXt&#26159;&#19968;&#20010;&#29992;&#20110;&#22522;&#20110;&#39592;&#26550;&#30340;&#21160;&#20316;&#35782;&#21035;&#30340;&#27169;&#22411;&#65292;&#20855;&#26377;&#38271;&#26399;&#23398;&#20064;&#28508;&#21147;&#65292;&#23427;&#37319;&#29992;&#21160;&#38745;&#24577;&#22810;&#22270;&#21367;&#31215;&#26469;&#27719;&#38598;&#22810;&#20010;&#29420;&#31435;&#25299;&#25169;&#22270;&#30340;&#29305;&#24449;&#65292;&#20197;&#21450;&#26500;&#24314;&#20102;&#19968;&#20010;&#22270;&#24418;&#21367;&#31215;&#35757;&#32451;&#21152;&#36895;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2304.11631</link><description>&lt;p&gt;
TSGCNeXt&#65306;&#20855;&#22791;&#38271;&#26399;&#23398;&#20064;&#28508;&#21147;&#30340;&#39640;&#25928;&#22522;&#20110;&#39592;&#26550;&#30340;&#21160;&#20316;&#35782;&#21035;&#30340;&#21160;&#38745;&#24577;&#22810;&#22270;&#21367;&#31215;
&lt;/p&gt;
&lt;p&gt;
TSGCNeXt: Dynamic-Static Multi-Graph Convolution for Efficient Skeleton-Based Action Recognition with Long-term Learning Potential. (arXiv:2304.11631v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11631
&lt;/p&gt;
&lt;p&gt;
TSGCNeXt&#26159;&#19968;&#20010;&#29992;&#20110;&#22522;&#20110;&#39592;&#26550;&#30340;&#21160;&#20316;&#35782;&#21035;&#30340;&#27169;&#22411;&#65292;&#20855;&#26377;&#38271;&#26399;&#23398;&#20064;&#28508;&#21147;&#65292;&#23427;&#37319;&#29992;&#21160;&#38745;&#24577;&#22810;&#22270;&#21367;&#31215;&#26469;&#27719;&#38598;&#22810;&#20010;&#29420;&#31435;&#25299;&#25169;&#22270;&#30340;&#29305;&#24449;&#65292;&#20197;&#21450;&#26500;&#24314;&#20102;&#19968;&#20010;&#22270;&#24418;&#21367;&#31215;&#35757;&#32451;&#21152;&#36895;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#30340;&#21457;&#23637;&#65292;&#22522;&#20110;&#39592;&#26550;&#30340;&#21160;&#20316;&#35782;&#21035;&#22312;&#20154;&#31867;&#21160;&#20316;&#35782;&#21035;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#36235;&#21521;&#20110;&#26500;&#24314;&#20855;&#26377;&#20887;&#20313;&#35757;&#32451;&#30340;&#22797;&#26434;&#23398;&#20064;&#26426;&#21046;&#65292;&#24182;&#23384;&#22312;&#38271;&#26102;&#38388;&#24207;&#21015;&#30340;&#29942;&#39048;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Temporal-Spatio Graph ConvNeXt&#65288;TSGCNeXt&#65289;&#26469;&#25506;&#32034;&#38271;&#26102;&#38388;&#39592;&#39612;&#24207;&#21015;&#30340;&#39640;&#25928;&#23398;&#20064;&#26426;&#21046;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22270;&#24418;&#23398;&#20064;&#26426;&#21046;&#65292;&#21160;&#38745;&#20998;&#31163;&#22810;&#22270;&#21367;&#31215;&#65288;DS-SMG&#65289;&#65292;&#20197;&#27719;&#38598;&#22810;&#20010;&#29420;&#31435;&#25299;&#25169;&#22270;&#30340;&#29305;&#24449;&#24182;&#36991;&#20813;&#33410;&#28857;&#20449;&#24687;&#22312;&#21160;&#24577;&#21367;&#31215;&#26399;&#38388;&#34987;&#24573;&#30053;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22270;&#24418;&#21367;&#31215;&#35757;&#32451;&#21152;&#36895;&#26426;&#21046;&#65292;&#20197;55.08&#65285;&#30340;&#36895;&#24230;&#25552;&#39640;&#21160;&#24577;&#22270;&#24418;&#23398;&#20064;&#30340;&#21453;&#21521;&#20256;&#25773;&#35745;&#31639;&#36895;&#24230;&#12290;&#26368;&#21518;&#65292;TSGCNeXt&#36890;&#36807;&#19977;&#20010;&#26102;&#31354;&#23398;&#20064;&#27169;&#22359;&#37325;&#26032;&#26500;&#24314;&#20102;GCN&#30340;&#25972;&#20307;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#26356;&#21152;&#39640;&#25928;&#30340;&#22522;&#20110;&#39592;&#26550;&#30340;&#21160;&#20316;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Skeleton-based action recognition has achieved remarkable results in human action recognition with the development of graph convolutional networks (GCNs). However, the recent works tend to construct complex learning mechanisms with redundant training and exist a bottleneck for long time-series. To solve these problems, we propose the Temporal-Spatio Graph ConvNeXt (TSGCNeXt) to explore efficient learning mechanism of long temporal skeleton sequences. Firstly, a new graph learning mechanism with simple structure, Dynamic-Static Separate Multi-graph Convolution (DS-SMG) is proposed to aggregate features of multiple independent topological graphs and avoid the node information being ignored during dynamic convolution. Next, we construct a graph convolution training acceleration mechanism to optimize the back-propagation computing of dynamic graph learning with 55.08\% speed-up. Finally, the TSGCNeXt restructure the overall structure of GCN with three Spatio-temporal learning modules,effic
&lt;/p&gt;</description></item><item><title>&#32858;&#21512;&#21464;&#37327;&#19978;&#30340;&#22240;&#26524;&#24615;&#19981;&#30830;&#23450;&#24615;&#21487;&#33021;&#20250;&#20351;&#24471;&#21407;&#26412;&#19981;&#28151;&#28102;&#30340;&#22240;&#26524;&#20851;&#31995;&#21464;&#24471;&#28151;&#28102;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#38656;&#35201;&#25509;&#21463;&#23439;&#35266;&#22240;&#26524;&#20851;&#31995;&#36890;&#24120;&#21482;&#19982;&#24494;&#35266;&#29366;&#24577;&#30456;&#20851;&#30340;&#20107;&#23454;&#12290;</title><link>http://arxiv.org/abs/2304.11625</link><description>&lt;p&gt;
&#26377;&#24847;&#20041;&#30340;&#22240;&#26524;&#32858;&#21512;&#21644;&#24726;&#35770;&#24615;&#28151;&#28102;
&lt;/p&gt;
&lt;p&gt;
Meaningful Causal Aggregation and Paradoxical Confounding. (arXiv:2304.11625v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11625
&lt;/p&gt;
&lt;p&gt;
&#32858;&#21512;&#21464;&#37327;&#19978;&#30340;&#22240;&#26524;&#24615;&#19981;&#30830;&#23450;&#24615;&#21487;&#33021;&#20250;&#20351;&#24471;&#21407;&#26412;&#19981;&#28151;&#28102;&#30340;&#22240;&#26524;&#20851;&#31995;&#21464;&#24471;&#28151;&#28102;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#38656;&#35201;&#25509;&#21463;&#23439;&#35266;&#22240;&#26524;&#20851;&#31995;&#36890;&#24120;&#21482;&#19982;&#24494;&#35266;&#29366;&#24577;&#30456;&#20851;&#30340;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32858;&#21512;&#21464;&#37327;&#20013;&#65292;&#24178;&#39044;&#30340;&#24433;&#21709;&#36890;&#24120;&#26159;&#19981;&#30830;&#23450;&#30340;&#65292;&#22240;&#20026;&#30456;&#21516;&#30340;&#23439;&#35266;&#24178;&#39044;&#30340;&#19981;&#21516;&#24494;&#35266;&#23454;&#29616;&#21487;&#33021;&#20250;&#23548;&#33268;&#19979;&#28216;&#23439;&#35266;&#21464;&#37327;&#30340;&#19981;&#21516;&#21464;&#21270;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#23545;&#20110;&#32858;&#21512;&#21464;&#37327;&#65292;&#22240;&#26524;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#21487;&#20197;&#20351;&#24471;&#21407;&#26412;&#19981;&#28151;&#28102;&#30340;&#22240;&#26524;&#20851;&#31995;&#21464;&#24471;&#28151;&#28102;&#65292;&#24182;&#19988;&#21453;&#20043;&#20134;&#28982;&#65292;&#36825;&#19968;&#28857;&#21462;&#20915;&#20110;&#30456;&#24212;&#30340;&#24494;&#35266;&#23454;&#29616;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#21482;&#26377;&#22312;&#32858;&#21512;&#22240;&#26524;&#31995;&#32479;&#27809;&#26377;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25165;&#21487;&#20197;&#23454;&#38469;&#24212;&#29992;&#36825;&#31181;&#26041;&#27861;&#12290;&#21542;&#21017;&#65292;&#25105;&#20204;&#38656;&#35201;&#25509;&#21463;&#19968;&#28857;&#65292;&#23601;&#26159;&#23439;&#35266;&#22240;&#26524;&#20851;&#31995;&#36890;&#24120;&#21482;&#19982;&#24494;&#35266;&#29366;&#24577;&#30456;&#20851;&#12290;&#22312;&#31215;&#26497;&#26041;&#38754;&#65292;&#25105;&#20204;&#34920;&#26126;&#24403;&#23439;&#35266;&#24178;&#39044;&#30340;&#20998;&#24067;&#19982;&#35266;&#27979;&#20998;&#24067;&#20013;&#24494;&#35266;&#29366;&#24577;&#30340;&#20998;&#24067;&#30456;&#21516;&#26102;&#65292;&#22240;&#26524;&#20851;&#31995;&#21487;&#20197;&#36827;&#34892;&#32858;&#21512;&#65292;&#24182;&#35752;&#35770;&#20102;&#27492;&#35266;&#23519;&#30340;&#27010;&#25324;&#12290;
&lt;/p&gt;
&lt;p&gt;
In aggregated variables the impact of interventions is typically ill-defined because different micro-realizations of the same macro-intervention can result in different changes of downstream macro-variables. We show that this ill-definedness of causality on aggregated variables can turn unconfounded causal relations into confounded ones and vice versa, depending on the respective micro-realization. We argue that it is practically infeasible to only use aggregated causal systems when we are free from this ill-definedness. Instead, we need to accept that macro causal relations are typically defined only with reference to the micro states. On the positive side, we show that cause-effect relations can be aggregated when the macro interventions are such that the distribution of micro states is the same as in the observational distribution and also discuss generalizations of this observation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#36965;&#24863;&#22270;&#20687;&#20803;&#25968;&#25454;&#38598;SATIN&#65292;&#23427;&#30001;27&#20010;&#29616;&#26377;&#30340;&#36965;&#24863;&#25968;&#25454;&#38598;&#32452;&#25104;&#65292;&#24182;&#20351;&#29992;&#19968;&#31995;&#21015;&#35270;&#35273;-&#35821;&#35328;&#65288;VL&#65289;&#27169;&#22411;&#20840;&#38754;&#35780;&#20272;&#20102;&#23427;&#30340;&#38646;-shot&#36716;&#31227;&#20998;&#31867;&#33021;&#21147;&#12290;&#35813;&#30740;&#31350;&#21457;&#29616;SATIN&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#24378;&#22823;&#26041;&#27861;&#30340;&#20998;&#31867;&#31934;&#24230;&#20026;52.0&#65285;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#20844;&#20849;&#25490;&#34892;&#27036;&#20197;&#36319;&#36394;&#27169;&#22411;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2304.11619</link><description>&lt;p&gt;
SATIN&#65306;&#19968;&#20010;&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#23545;&#21355;&#26143;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#30340;&#22810;&#20219;&#21153;&#20803;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
SATIN: A Multi-Task Metadataset for Classifying Satellite Imagery using Vision-Language Models. (arXiv:2304.11619v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#36965;&#24863;&#22270;&#20687;&#20803;&#25968;&#25454;&#38598;SATIN&#65292;&#23427;&#30001;27&#20010;&#29616;&#26377;&#30340;&#36965;&#24863;&#25968;&#25454;&#38598;&#32452;&#25104;&#65292;&#24182;&#20351;&#29992;&#19968;&#31995;&#21015;&#35270;&#35273;-&#35821;&#35328;&#65288;VL&#65289;&#27169;&#22411;&#20840;&#38754;&#35780;&#20272;&#20102;&#23427;&#30340;&#38646;-shot&#36716;&#31227;&#20998;&#31867;&#33021;&#21147;&#12290;&#35813;&#30740;&#31350;&#21457;&#29616;SATIN&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#24378;&#22823;&#26041;&#27861;&#30340;&#20998;&#31867;&#31934;&#24230;&#20026;52.0&#65285;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#20844;&#20849;&#25490;&#34892;&#27036;&#20197;&#36319;&#36394;&#27169;&#22411;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#36965;&#24863;&#22270;&#20687;&#21487;&#20197;&#23454;&#29616;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#65292;&#20174;&#22303;&#22320;&#21033;&#29992;&#35268;&#21010;&#21040;&#26862;&#26519;&#30733;&#20240;&#30417;&#27979;&#37117;&#26377;&#21487;&#33021;&#12290;&#30001;&#20110;&#22320;&#29699;&#22320;&#29702;&#22810;&#26679;&#24615;&#30340;&#23384;&#22312;&#65292;&#23545;&#36825;&#20123;&#25968;&#25454;&#36827;&#34892;&#31283;&#20581;&#20998;&#31867;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#34429;&#28982;&#23384;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#21355;&#26143;&#21644;&#33322;&#31354;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#65292;&#20294;&#23578;&#26410;&#26377;&#19968;&#20010;&#36866;&#21512;&#28085;&#30422;&#36825;&#31181;&#22810;&#26679;&#24615;&#30340;&#22522;&#20934;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#26469;&#33258;27&#20010;&#29616;&#26377;&#36965;&#24863;&#25968;&#25454;&#38598;&#30340;&#20803;&#25968;&#25454;&#38598;SATellite ImageNet&#65288;SATIN&#65289;&#65292;&#24182;&#20840;&#38754;&#35780;&#20272;&#20102;&#19968;&#31995;&#21015;&#35270;&#35273;-&#35821;&#35328;&#65288;VL&#65289;&#27169;&#22411;&#22312;SATIN&#19978;&#30340;&#38646;-shot&#36716;&#31227;&#20998;&#31867;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;SATIN&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;-&#25105;&#20204;&#35780;&#20272;&#30340;&#26368;&#24378;&#26041;&#27861;&#30340;&#20998;&#31867;&#31934;&#24230;&#20026;52.0&#65285;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20844;&#20849;&#25490;&#34892;&#27036;&#65292;&#20197;&#25351;&#23548;&#21644;&#36319;&#36394;VL&#27169;&#22411;&#22312;&#36825;&#19968;&#37325;&#35201;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpreting remote sensing imagery enables numerous downstream applications ranging from land-use planning to deforestation monitoring. Robustly classifying this data is challenging due to the Earth's geographic diversity. While many distinct satellite and aerial image classification datasets exist, there is yet to be a benchmark curated that suitably covers this diversity. In this work, we introduce SATellite ImageNet (SATIN), a metadataset curated from 27 existing remotely sensed datasets, and comprehensively evaluate the zero-shot transfer classification capabilities of a broad range of vision-language (VL) models on SATIN. We find SATIN to be a challenging benchmark-the strongest method we evaluate achieves a classification accuracy of 52.0%. We provide a $\href{https://satinbenchmark.github.io}{\text{public leaderboard}}$ to guide and track the progress of VL models in this important domain.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#27169;&#24577;&#24863;&#30693;&#36127;&#37319;&#26679;&#26041;&#27861;(MANS)&#65292;&#36890;&#36807;&#23545;&#30693;&#35782;&#22270;&#35889;&#20013;&#23454;&#20307;&#30340;&#32467;&#26500;&#24615;&#21644;&#35270;&#35273;&#23884;&#20837;&#36827;&#34892;&#23545;&#40784;&#65292;MANS&#33021;&#22815;&#23398;&#20064;&#26356;&#26377;&#24847;&#20041;&#30340;&#23884;&#20837;&#20197;&#22312;&#22810;&#27169;&#24577;KGE&#20013;&#36798;&#21040;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#36731;&#37327;&#32423;&#21644;&#39640;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.11618</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#20013;&#30340;&#27169;&#24577;&#24863;&#30693;&#36127;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Modality-Aware Negative Sampling for Multi-modal Knowledge Graph Embedding. (arXiv:2304.11618v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#27169;&#24577;&#24863;&#30693;&#36127;&#37319;&#26679;&#26041;&#27861;(MANS)&#65292;&#36890;&#36807;&#23545;&#30693;&#35782;&#22270;&#35889;&#20013;&#23454;&#20307;&#30340;&#32467;&#26500;&#24615;&#21644;&#35270;&#35273;&#23884;&#20837;&#36827;&#34892;&#23545;&#40784;&#65292;MANS&#33021;&#22815;&#23398;&#20064;&#26356;&#26377;&#24847;&#20041;&#30340;&#23884;&#20837;&#20197;&#22312;&#22810;&#27169;&#24577;KGE&#20013;&#36798;&#21040;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#36731;&#37327;&#32423;&#21644;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#20013;&#65292;&#36127;&#37319;&#26679;&#34987;&#24191;&#27867;&#24212;&#29992;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20135;&#29983;&#36127;&#19977;&#20803;&#32452;&#20197;&#36827;&#34892;&#27491;&#36127;&#21306;&#20998;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#36127;&#37319;&#26679;&#26041;&#27861;&#22312;&#22810;&#27169;&#24577;&#20449;&#24687;&#32771;&#34385;&#26102;&#19981;&#36866;&#29992;&#20110;KGE&#27169;&#22411;&#65292;&#32780;&#19988;&#30001;&#20110;&#23427;&#20204;&#30340;&#22797;&#26434;&#35774;&#35745;&#32780;&#25928;&#29575;&#20302;&#19979;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65288;MMKGE&#65289;&#30340;&#27169;&#24577;&#24863;&#30693;&#36127;&#37319;&#26679;&#65288;MANS&#65289;&#65292;&#20197;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;MANS&#33021;&#22815;&#23545;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#23454;&#20307;&#36827;&#34892;&#32467;&#26500;&#24615;&#21644;&#35270;&#35273;&#23884;&#20837;&#30340;&#23545;&#40784;&#65292;&#23398;&#20064;&#26377;&#24847;&#20041;&#30340;&#23884;&#20837;&#20197;&#22312;&#22810;&#27169;&#24577;KGE&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#21516;&#26102;&#20445;&#25345;&#36731;&#37327;&#32423;&#21644;&#39640;&#25928;&#29575;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;MANS&#20248;&#20110;&#29616;&#26377;&#30340;NS&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#23545;MANS&#36827;&#34892;&#20102;&#36827;&#19968;&#27493;&#25506;&#32034;&#20197;&#35777;&#23454;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Negative sampling (NS) is widely used in knowledge graph embedding (KGE), which aims to generate negative triples to make a positive-negative contrast during training. However, existing NS methods are unsuitable when multi-modal information is considered in KGE models. They are also inefficient due to their complex design. In this paper, we propose Modality-Aware Negative Sampling (MANS) for multi-modal knowledge graph embedding (MMKGE) to address the mentioned problems. MANS could align structural and visual embeddings for entities in KGs and learn meaningful embeddings to perform better in multi-modal KGE while keeping lightweight and efficient. Empirical results on two benchmarks demonstrate that MANS outperforms existing NS methods. Meanwhile, we make further explorations about MANS to confirm its effectiveness.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#30340;&#26631;&#31614;&#20256;&#25773;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#22270;&#24418;&#32454;&#21270;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#21407;&#22411;&#20272;&#35745;&#19981;&#20934;&#30830;&#21644;&#26680;&#20989;&#25968;&#19979;&#20122;&#20248;&#22270;&#24418;&#26500;&#24314;&#30340;&#38382;&#39064;&#65292;&#22312;&#23569;&#26679;&#36801;&#31227;&#23398;&#20064;&#21644;&#21322;&#30417;&#30563;FSL&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2304.11598</link><description>&lt;p&gt;
&#36845;&#20195;&#22270;&#24418;&#32454;&#21270;&#30340;&#22522;&#20110;&#21407;&#22411;&#30340;&#26631;&#31614;&#20256;&#25773;&#30340;&#36801;&#31227;&#23569;&#26679;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transductive Few-shot Learning with Prototype-based Label Propagation by Iterative Graph Refinement. (arXiv:2304.11598v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#30340;&#26631;&#31614;&#20256;&#25773;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#22270;&#24418;&#32454;&#21270;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#21407;&#22411;&#20272;&#35745;&#19981;&#20934;&#30830;&#21644;&#26680;&#20989;&#25968;&#19979;&#20122;&#20248;&#22270;&#24418;&#26500;&#24314;&#30340;&#38382;&#39064;&#65292;&#22312;&#23569;&#26679;&#36801;&#31227;&#23398;&#20064;&#21644;&#21322;&#30417;&#30563;FSL&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#23398;&#20064;(FSL)&#22240;&#20854;&#36866;&#24212;&#26032;&#39046;&#22495;&#30340;&#33021;&#21147;&#32780;&#21463;&#27426;&#36814;&#12290;&#19982;&#24402;&#32435;&#24335;&#23569;&#26679;&#23398;&#20064;&#30456;&#27604;&#65292;&#20256;&#23548;&#24335;&#27169;&#22411;&#36890;&#24120;&#34920;&#29616;&#26356;&#22909;&#65292;&#22240;&#20026;&#23427;&#20204;&#21033;&#29992;&#26597;&#35810;&#38598;&#30340;&#25152;&#26377;&#26679;&#26412;&#12290;&#29616;&#26377;&#26041;&#27861;&#30340;&#20004;&#20010;&#31867;&#21035;&#65292;&#22522;&#20110;&#21407;&#22411;&#21644;&#22522;&#20110;&#22270;&#24418;&#30340;&#26041;&#27861;&#65292;&#20998;&#21035;&#20855;&#26377;&#21407;&#22411;&#20272;&#35745;&#19981;&#20934;&#30830;&#21644;&#26680;&#20989;&#25968;&#19979;&#20122;&#20248;&#22270;&#24418;&#26500;&#24314;&#30340;&#32570;&#28857;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#21407;&#22411;&#30340;&#26631;&#31614;&#20256;&#25773;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#22270;&#24418;&#26500;&#24314;&#22522;&#20110;&#21407;&#22411;&#21644;&#26679;&#26412;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#32780;&#19981;&#26159;&#26679;&#26412;&#20043;&#38388;&#12290;&#38543;&#30528;&#21407;&#22411;&#30340;&#26356;&#26032;&#65292;&#22270;&#24418;&#20063;&#20250;&#38543;&#20043;&#21464;&#21270;&#12290;&#25105;&#20204;&#36824;&#20272;&#35745;&#27599;&#20010;&#21407;&#22411;&#30340;&#26631;&#31614;&#65292;&#32780;&#19981;&#26159;&#23558;&#21407;&#22411;&#35270;&#20026;&#31867;&#20013;&#24515;&#12290;&#22312;mini-ImageNet&#65292;tiered-ImageNet&#65292;CIFAR-FS&#21644;CUB&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#23569;&#26679;&#36801;&#31227;&#23398;&#20064;&#21644;&#21322;&#30417;&#30563;FSL&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot learning (FSL) is popular due to its ability to adapt to novel classes. Compared with inductive few-shot learning, transductive models typically perform better as they leverage all samples of the query set. The two existing classes of methods, prototype-based and graph-based, have the disadvantages of inaccurate prototype estimation and sub-optimal graph construction with kernel functions, respectively. In this paper, we propose a novel prototype-based label propagation to solve these issues. Specifically, our graph construction is based on the relation between prototypes and samples rather than between samples. As prototypes are being updated, the graph changes. We also estimate the label of each prototype instead of considering a prototype be the class centre. On mini-ImageNet, tiered-ImageNet, CIFAR-FS and CUB datasets, we show the proposed method outperforms other state-of-the-art methods in transductive FSL and semi-supervised FSL when some unlabeled data accompanies the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20559;&#30456;&#20851;&#30340;&#28145;&#24230;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20351;&#29992;&#21327;&#26041;&#24046;&#30697;&#38453;&#34920;&#24449;&#30456;&#20851;&#24615;&#22312;&#23384;&#22312;&#28151;&#28102;&#25928;&#24212;&#26102;&#30340;&#35823;&#23548;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.11597</link><description>&lt;p&gt;
&#22522;&#20110;&#20559;&#30456;&#20851;&#30340;&#28145;&#24230;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Learning Partial Correlation based Deep Visual Representation for Image Classification. (arXiv:2304.11597v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20559;&#30456;&#20851;&#30340;&#28145;&#24230;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20351;&#29992;&#21327;&#26041;&#24046;&#30697;&#38453;&#34920;&#24449;&#30456;&#20851;&#24615;&#22312;&#23384;&#22312;&#28151;&#28102;&#25928;&#24212;&#26102;&#30340;&#35823;&#23548;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#35270;&#35273;&#34920;&#31034;&#24050;&#32463;&#35777;&#26126;&#20102;&#20854;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#23545;&#21367;&#31215;&#29305;&#24449;&#26144;&#23556;&#20013;&#19981;&#21516;&#36890;&#36947;&#20043;&#38388;&#30340;&#25104;&#23545;&#30456;&#20851;&#24615;&#36827;&#34892;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#23384;&#22312;&#21478;&#19968;&#20010;&#36890;&#36947;&#19982;&#24863;&#20852;&#36259;&#30340;&#20004;&#20010;&#36890;&#36947;&#30456;&#20851;&#65292;&#21017;&#25104;&#23545;&#30456;&#20851;&#24615;&#23558;&#21464;&#24471;&#35823;&#23548;&#20154;&#65292;&#23548;&#33268;&#8220;&#28151;&#28102;&#8221;&#25928;&#24212;&#12290;&#38024;&#23545;&#36825;&#31181;&#24773;&#20917;&#65292;&#24212;&#35813;&#20272;&#35745;&#8220;&#20559;&#30456;&#20851;&#8221;&#65292;&#20197;&#28040;&#38500;&#28151;&#28102;&#25928;&#24212;&#12290;&#28982;&#32780;&#65292;&#21487;&#38752;&#22320;&#20272;&#35745;&#20559;&#30456;&#20851;&#38656;&#35201;&#35299;&#20915;&#19968;&#20010;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;&#31232;&#30095;&#36870;&#21327;&#26041;&#24046;&#30697;&#38453;&#20272;&#35745;&#65288;SICE&#65289;&#12290;&#22914;&#20309;&#23558;&#27492;&#36807;&#31243;&#34701;&#20837;CNN&#20013;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;SICE&#21046;&#23450;&#20026;CNN&#30340;&#19968;&#20010;&#26032;&#32467;&#26500;&#23618;&#12290;&#20026;&#30830;&#20445;&#31471;&#21040;&#31471;&#30340;&#21487;&#35757;&#32451;&#24615;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#36845;&#20195;&#26041;&#27861;&#65292;&#22312;&#21069;&#21521;&#21644;&#21518;&#21521;&#20256;&#25773;&#27493;&#39588;&#20013;&#35299;&#20915;&#19978;&#36848;&#30697;&#38453;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#33719;&#24471;&#20102;&#22522;&#20110;&#20559;&#30456;&#20851;&#30340;&#28145;&#24230;&#35270;&#35273;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual representation based on covariance matrix has demonstrates its efficacy for image classification by characterising the pairwise correlation of different channels in convolutional feature maps. However, pairwise correlation will become misleading once there is another channel correlating with both channels of interest, resulting in the ``confounding'' effect. For this case, ``partial correlation'' which removes the confounding effect shall be estimated instead. Nevertheless, reliably estimating partial correlation requires to solve a symmetric positive definite matrix optimisation, known as sparse inverse covariance estimation (SICE). How to incorporate this process into CNN remains an open issue. In this work, we formulate SICE as a novel structured layer of CNN. To ensure end-to-end trainability, we develop an iterative method to solve the above matrix optimisation during forward and backward propagation steps. Our work obtains a partial correlation based deep visual representa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;Segment Non-Euclidean Anything&#65288;SNA&#65289;&#26041;&#27861;&#65292;&#26088;&#22312;&#25193;&#22823;&#35821;&#20041;&#20998;&#21106;&#30340;&#33539;&#22260;&#65292;&#23548;&#33268;&#26356;&#22810;&#30340;&#22522;&#30784;&#27169;&#22411;&#29992;&#20110;&#38750;&#27431;&#20960;&#37324;&#24471;&#22495;&#20013;&#30340;&#20998;&#21106;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2304.11595</link><description>&lt;p&gt;
&#38750;&#27431;&#20960;&#37324;&#24471;&#22495;&#20013;&#30340;&#20998;&#21106;&#38382;&#39064;&#65306;&#25361;&#25112;&#19982;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Segment Anything in Non-Euclidean Domains: Challenges and Opportunities. (arXiv:2304.11595v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;Segment Non-Euclidean Anything&#65288;SNA&#65289;&#26041;&#27861;&#65292;&#26088;&#22312;&#25193;&#22823;&#35821;&#20041;&#20998;&#21106;&#30340;&#33539;&#22260;&#65292;&#23548;&#33268;&#26356;&#22810;&#30340;&#22522;&#30784;&#27169;&#22411;&#29992;&#20110;&#38750;&#27431;&#20960;&#37324;&#24471;&#22495;&#20013;&#30340;&#20998;&#21106;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#19968;&#39033;&#24037;&#20316;&#31216;&#20026;Segment Anything&#65288;SA&#65289;&#22312;&#23558;&#35821;&#20041;&#20998;&#21106;&#30340;&#36793;&#30028;&#25512;&#21521;&#22522;&#30784;&#27169;&#22411;&#26102;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;SA&#30340;&#24433;&#21709;&#24341;&#21457;&#20102;&#26497;&#20026;&#27963;&#36291;&#30340;&#35752;&#35770;&#65292;&#24182;&#24341;&#39046;&#20102;&#19968;&#27874;&#26032;&#30340;&#21457;&#23637;&#28010;&#28526;&#65292;&#29992;&#20110;&#27431;&#20960;&#37324;&#24471;&#22495;&#20013;&#30340;&#21508;&#31181;&#20219;&#21153;&#65292;&#22914;&#29289;&#20307;&#26816;&#27979;&#21644;&#22270;&#20687;&#20462;&#22797;&#12290;&#23613;&#31649;SA&#25152;&#24102;&#26469;&#30340;&#36827;&#23637;&#24456;&#26377;&#21069;&#36884;&#65292;&#20294;&#35813;&#27010;&#24565;&#23578;&#26410;&#25193;&#23637;&#21040;&#38750;&#27431;&#20960;&#37324;&#24471;&#22270;&#24418;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#31216;&#20026;Segment Non-Euclidean Anything&#65288;SNA&#65289;&#30340;&#26032;&#33539;&#24335;&#65292;&#35813;&#33539;&#24335;&#26088;&#22312;&#24320;&#21457;&#33021;&#22815;&#22788;&#29702;&#38750;&#27431;&#20960;&#37324;&#24471;&#22495;&#20013;&#21508;&#31181;&#22270;&#24418;&#25968;&#25454;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#35797;&#22270;&#25193;&#22823;SA&#30340;&#33539;&#22260;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#22880;&#23450;&#22522;&#30784;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;&#19982;SA&#30456;&#20851;&#30340;&#22522;&#30784;&#27169;&#22411;&#30340;&#26368;&#26032;&#25104;&#23601;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30528;&#30524;&#20110;&#23558;SA&#27010;&#24565;&#24212;&#29992;&#20110;&#22270;&#24418;&#25968;&#25454;&#26102;&#20986;&#29616;&#30340;&#29420;&#29305;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent work known as Segment Anything (SA) has made significant strides in pushing the boundaries of semantic segmentation into the era of foundation models. The impact of SA has sparked extremely active discussions and ushered in an encouraging new wave of developing foundation models for the diverse tasks in the Euclidean domain, such as object detection and image inpainting. Despite the promising advances led by SA, the concept has yet to be extended to the non-Euclidean graph domain. In this paper, we explore a novel Segment Non-Euclidean Anything (SNA) paradigm that strives to develop foundation models that can handle the diverse range of graph data within the non-Euclidean domain, seeking to expand the scope of SA and lay the groundwork for future research in this direction. To achieve this goal, we begin by discussing the recent achievements in foundation models associated with SA. We then shed light on the unique challenges that arise when applying the SA concept to graph a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20803;&#22810;&#37325;&#22270;&#30340;&#26032;&#27010;&#24565;&#26469;&#20195;&#26367;&#20803;&#22270;&#21644;&#20803;&#36335;&#24452;&#29992;&#20110;&#22788;&#29702;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#20013;&#30340;&#20449;&#24687;&#32858;&#21512;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#31283;&#23450;&#30340;&#21487;&#24494;&#20998;&#25628;&#32034;&#26041;&#27861;&#26469;&#20248;&#21270;&#20803;&#22810;&#37325;&#22270;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;HIN&#65292;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#22797;&#26434;&#21040;&#31616;&#27905;&#30340;C2C&#20803;&#22810;&#37325;&#22270;&#26469;&#38477;&#20302;&#20887;&#20313;&#20449;&#24687;&#20256;&#25773;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.11574</link><description>&lt;p&gt;
&#20803;&#22810;&#37325;&#22270;&#25628;&#32034;&#65306;&#37325;&#26032;&#24605;&#32771;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#19978;&#30340;&#20803;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Meta-multigraph Search: Rethinking Meta-structure on Heterogeneous Information Networks. (arXiv:2304.11574v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20803;&#22810;&#37325;&#22270;&#30340;&#26032;&#27010;&#24565;&#26469;&#20195;&#26367;&#20803;&#22270;&#21644;&#20803;&#36335;&#24452;&#29992;&#20110;&#22788;&#29702;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#20013;&#30340;&#20449;&#24687;&#32858;&#21512;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#31283;&#23450;&#30340;&#21487;&#24494;&#20998;&#25628;&#32034;&#26041;&#27861;&#26469;&#20248;&#21270;&#20803;&#22810;&#37325;&#22270;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;HIN&#65292;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#22797;&#26434;&#21040;&#31616;&#27905;&#30340;C2C&#20803;&#22810;&#37325;&#22270;&#26469;&#38477;&#20302;&#20887;&#20313;&#20449;&#24687;&#20256;&#25773;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#32467;&#26500;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#23450;&#20041;&#22312;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#65288;HIN&#65289;&#20013;&#21738;&#20123;&#37051;&#23621;&#30340;&#23376;&#38598;&#26469;&#32858;&#21512;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#29616;&#26377;&#30340;&#20803;&#32467;&#26500;&#65292;&#21253;&#25324;&#20803;&#36335;&#24452;&#21644;&#20803;&#22270;&#65292;&#24182;&#35266;&#23519;&#21040;&#23427;&#20204;&#26368;&#21021;&#26159;&#25163;&#21160;&#35774;&#35745;&#30340;&#22266;&#23450;&#27169;&#24335;&#65292;&#22240;&#27492;&#19981;&#36275;&#20197;&#32534;&#30721;&#19981;&#21516;HIN&#19978;&#30340;&#21508;&#31181;&#20016;&#23500;&#35821;&#20041;&#20449;&#24687;&#12290;&#36890;&#36807;&#21453;&#24605;&#23427;&#20204;&#30340;&#38480;&#21046;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#31216;&#20026;&#20803;&#22810;&#37325;&#22270;&#30340;&#26032;&#27010;&#24565;&#65292;&#20316;&#20026;&#20803;&#22270;&#30340;&#26356;&#20855;&#34920;&#29616;&#21147;&#21644;&#28789;&#27963;&#30340;&#27867;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#21487;&#24494;&#20998;&#30340;&#25628;&#32034;&#26041;&#27861;&#65292;&#20197;&#33258;&#21160;&#20248;&#21270;&#29305;&#23450;HIN&#21644;&#20219;&#21153;&#30340;&#20803;&#22810;&#37325;&#22270;&#12290;&#30001;&#20110;&#20803;&#22810;&#37325;&#22270;&#30340;&#28789;&#27963;&#24615;&#21487;&#33021;&#20250;&#20256;&#25773;&#20887;&#20313;&#30340;&#28040;&#24687;&#65292;&#22240;&#27492;&#25105;&#20204;&#36827;&#19968;&#27493;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#22797;&#26434;&#21040;&#31616;&#27905;&#30340;C2C&#20803;&#22810;&#37325;&#22270;&#65292;&#23427;&#27839;&#30528;&#20803;&#22810;&#37325;&#22270;&#30340;&#28145;&#24230;&#20174;&#22797;&#26434;&#21040;&#31616;&#27905;&#22320;&#20256;&#25773;&#28040;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#21487;&#24494;&#20998;&#25628;&#32034;&#36890;&#24120;&#20250;&#36973;&#21463;&#19981;&#31283;&#23450;&#30340;&#25628;&#32034;&#21644;&#26174;&#30528;&#30340;g
&lt;/p&gt;
&lt;p&gt;
Meta-structures are widely used to define which subset of neighbors to aggregate information in heterogeneous information networks (HINs). In this work, we investigate existing meta-structures, including meta-path and meta-graph, and observe that they are initially designed manually with fixed patterns and hence are insufficient to encode various rich semantic information on diverse HINs. Through reflection on their limitation, we define a new concept called meta-multigraph as a more expressive and flexible generalization of meta-graph, and propose a stable differentiable search method to automatically optimize the meta-multigraph for specific HINs and tasks. As the flexibility of meta-multigraphs may propagate redundant messages, we further introduce a complex-to-concise (C2C) meta-multigraph that propagates messages from complex to concise along the depth of meta-multigraph. Moreover, we observe that the differentiable search typically suffers from unstable search and a significant g
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#21306;&#20998;&#30001;ChatGPT&#29983;&#25104;&#21644;&#20154;&#31867;&#25776;&#20889;&#30340;&#21307;&#23398;&#25991;&#26412;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#26469;&#26377;&#25928;&#26816;&#27979;&#21307;&#23398;&#39046;&#22495;&#30340;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#65292;&#20197;&#36991;&#20813;&#21487;&#33021;&#23548;&#33268;&#30340;&#20551;&#28040;&#24687;&#21644;&#23545;&#20844;&#20247;&#36896;&#25104;&#30340;&#25439;&#23475;&#12290;</title><link>http://arxiv.org/abs/2304.11567</link><description>&lt;p&gt;
&#21306;&#20998;ChatGPT&#29983;&#25104;&#21644;&#20154;&#20889;&#30340;&#21307;&#23398;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Differentiate ChatGPT-generated and Human-written Medical Texts. (arXiv:2304.11567v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11567
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#21306;&#20998;&#30001;ChatGPT&#29983;&#25104;&#21644;&#20154;&#31867;&#25776;&#20889;&#30340;&#21307;&#23398;&#25991;&#26412;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#26469;&#26377;&#25928;&#26816;&#27979;&#21307;&#23398;&#39046;&#22495;&#30340;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#65292;&#20197;&#36991;&#20813;&#21487;&#33021;&#23548;&#33268;&#30340;&#20551;&#28040;&#24687;&#21644;&#23545;&#20844;&#20247;&#36896;&#25104;&#30340;&#25439;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#35821;&#27861;&#23436;&#32654;&#12289;&#31867;&#20284;&#20154;&#31867;&#25991;&#26412;&#20869;&#23481;&#30340;&#22823;&#37327;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#20020;&#24202;&#30149;&#21382;&#21644;&#35786;&#26029;&#31561;&#21307;&#23398;&#25991;&#26412;&#38656;&#35201;&#20005;&#26684;&#30340;&#39564;&#35777;&#65292;&#30001;ChatGPT&#29983;&#25104;&#30340;&#38169;&#35823;&#21307;&#23398;&#20869;&#23481;&#21487;&#33021;&#23548;&#33268;&#20551;&#28040;&#24687;&#65292;&#20174;&#32780;&#23545;&#21307;&#30103;&#20445;&#20581;&#21644;&#20844;&#20247;&#36896;&#25104;&#37325;&#22823;&#21361;&#23475;&#12290;&#30446;&#30340;&#65306;&#26412;&#30740;&#31350;&#26159;&#20851;&#20110;&#21307;&#30103;&#39046;&#22495;&#36127;&#36131;&#21644;&#36947;&#24503;&#30340;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#30340;&#31532;&#19968;&#39033;&#30740;&#31350;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#20998;&#26512;&#30001;&#20154;&#31867;&#19987;&#23478;&#25776;&#20889;&#30340;&#21307;&#23398;&#25991;&#26412;&#21644;&#30001;ChatGPT&#29983;&#25104;&#30340;&#25991;&#26412;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#35774;&#35745;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#26469;&#26377;&#25928;&#22320;&#26816;&#27979;&#21644;&#21306;&#20998;&#30001;ChatGPT&#29983;&#25104;&#30340;&#21307;&#23398;&#25991;&#26412;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#22871;&#21253;&#21547;&#20154;&#31867;&#19987;&#23478;&#25776;&#20889;&#21644;&#30001;ChatGPT&#29983;&#25104;&#30340;&#21307;&#23398;&#25991;&#26412;&#30340;&#25968;&#25454;&#38598;&#21512;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20123;&#25991;&#26412;&#30340;&#35821;&#35328;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: Large language models such as ChatGPT are capable of generating grammatically perfect and human-like text content, and a large number of ChatGPT-generated texts have appeared on the Internet. However, medical texts such as clinical notes and diagnoses require rigorous validation, and erroneous medical content generated by ChatGPT could potentially lead to disinformation that poses significant harm to healthcare and the general public.  Objective: This research is among the first studies on responsible and ethical AIGC (Artificial Intelligence Generated Content) in medicine. We focus on analyzing the differences between medical texts written by human experts and generated by ChatGPT, and designing machine learning workflows to effectively detect and differentiate medical texts generated by ChatGPT.  Methods: We first construct a suite of datasets containing medical texts written by human experts and generated by ChatGPT. In the next step, we analyze the linguistic features o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#26580;&#24615;&#21452;&#24037;MIMO&#31995;&#32479;&#30340;&#36731;&#37327;&#32423;&#26426;&#22120;&#23398;&#20064;CLI&#25269;&#28040;&#22120;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20855;&#26377;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#39640;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#20869;&#23384;&#28040;&#32791;&#30340;&#26174;&#33879;&#38477;&#20302;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#23454;&#38469;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.11559</link><description>&lt;p&gt;
&#22522;&#20110;RF&#38142;&#36335;&#29305;&#24449;&#30340;&#26580;&#24615;&#21452;&#24037;MIMO&#31995;&#32479;&#20013;&#25968;&#23383;&#20132;&#21449;&#38142;&#25509;&#24178;&#25200;&#25269;&#28040;&#30340;&#36731;&#37327;&#32423;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Lightweight Machine Learning for Digital Cross-Link Interference Cancellation with RF Chain Characteristics in Flexible Duplex MIMO Systems. (arXiv:2304.11559v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#26580;&#24615;&#21452;&#24037;MIMO&#31995;&#32479;&#30340;&#36731;&#37327;&#32423;&#26426;&#22120;&#23398;&#20064;CLI&#25269;&#28040;&#22120;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20855;&#26377;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#39640;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#20869;&#23384;&#28040;&#32791;&#30340;&#26174;&#33879;&#38477;&#20302;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#23454;&#38469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26580;&#24615;&#21452;&#24037;&#65288;FD&#65289;&#25216;&#26415;&#65288;&#21253;&#25324;&#21160;&#24577;&#26102;&#20998;&#21452;&#24037;&#65288;D-TDD&#65289;&#21644;&#21160;&#24577;&#39057;&#20998;&#21452;&#24037;&#65288;D-FDD&#65289;&#65289;&#34987;&#35748;&#20026;&#26159;&#23454;&#29616;5G-Advanced&#25110;6G&#31227;&#21160;&#36890;&#20449;&#31995;&#32479;&#26356;&#28789;&#27963;&#30340;&#19978;&#34892;/&#19979;&#34892;&#20256;&#36755;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#23427;&#21487;&#33021;&#24341;&#20837;&#20005;&#37325;&#30340;&#20132;&#21449;&#38142;&#25509;&#24178;&#25200;&#65288;CLI&#65289;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#20943;&#36731;CLI&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#29616;&#23454;&#30340;&#22522;&#31449;&#65288;BS&#65289;&#21040;BS&#36890;&#36947;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#21547;&#23556;&#39057;&#65288;RF&#65289;&#38142;&#36335;&#29305;&#24615;&#65292;&#36825;&#20123;&#29305;&#24615;&#34920;&#29616;&#20986;&#19968;&#31181;&#30828;&#20214;&#20381;&#36182;&#24615;&#38750;&#32447;&#24615;&#23646;&#24615;&#65292;&#22240;&#27492;&#20256;&#32479;&#36890;&#36947;&#24314;&#27169;&#30340;&#31934;&#24230;&#19981;&#36275;&#20197;&#29992;&#20110;CLI&#30340;&#21462;&#28040;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36890;&#36947;&#21442;&#25968;&#20272;&#35745;&#30340;&#22810;&#39033;&#24335;CLI&#25269;&#28040;&#22120;&#21644;&#20004;&#20010;&#20351;&#29992;&#36731;&#37327;&#32423;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65288;FNN&#65289;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;CLI&#25269;&#28040;&#22120;&#12290;&#25105;&#20204;&#30340;&#27169;&#25311;&#32467;&#26524;&#21644;&#20998;&#26512;&#34920;&#26126;&#65292;&#19982;&#22810;&#39033;&#24335;&#25269;&#28040;&#22120;&#30456;&#27604;&#65292;&#22522;&#20110;ML&#30340;CLI&#25269;&#28040;&#22120;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#39640;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#20869;&#23384;&#28040;&#32791;&#30340;&#26174;&#33879;&#38477;&#20302;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22312;FD MIMO&#31995;&#32479;&#20013;&#36827;&#34892;&#23454;&#38469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The flexible duplex (FD) technique, including dynamic time-division duplex (D-TDD) and dynamic frequency-division duplex (D-FDD), is regarded as a promising solution to achieving a more flexible uplink/downlink transmission in 5G-Advanced or 6G mobile communication systems. However, it may introduce serious cross-link interference (CLI). For better mitigating the impact of CLI, we first present a more realistic base station (BS)-to-BS channel model incorporating the radio frequency (RF) chain characteristics, which exhibit a hardware-dependent nonlinear property, and hence the accuracy of conventional channel modelling is inadequate for CLI cancellation. Then, we propose a channel parameter estimation based polynomial CLI canceller and two machine learning (ML) based CLI cancellers that use the lightweight feedforward neural network (FNN). Our simulation results and analysis show that the ML based CLI cancellers achieve notable performance improvement and dramatic reduction of computat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24605;&#32500;&#38142;&#36880;&#20010;&#35299;&#20915;&#23376;&#20219;&#21153;&#30340;&#26041;&#24335;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Text-to-SQL&#25552;&#31034;&#26041;&#27861;&#30340;&#33539;&#20363;&#65292;&#36816;&#29992;&#20110;LLM&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#20854;&#25191;&#34892;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.11556</link><description>&lt;p&gt;
&#20998;&#32780;&#27835;&#20043;&#65292;&#24605;&#32500;&#38142;&#25351;&#23548;&#19979;&#30340;Text-to-SQL
&lt;/p&gt;
&lt;p&gt;
Divide and Prompt: Chain of Thought Prompting for Text-to-SQL. (arXiv:2304.11556v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11556
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24605;&#32500;&#38142;&#36880;&#20010;&#35299;&#20915;&#23376;&#20219;&#21153;&#30340;&#26041;&#24335;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Text-to-SQL&#25552;&#31034;&#26041;&#27861;&#30340;&#33539;&#20363;&#65292;&#36816;&#29992;&#20110;LLM&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#20854;&#25191;&#34892;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#24335;&#24605;&#32500;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#24050;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#12290;Text-to-SQL&#26159;&#19968;&#20010;&#23558;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#36716;&#25442;&#20026;SQL&#35821;&#21477;&#30340;&#20851;&#38190;&#35821;&#20041;&#20998;&#26512;&#20219;&#21153;&#65292;&#28041;&#21450;&#22797;&#26434;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#20351;&#29992;&#24605;&#32500;&#38142;&#25351;&#23548;&#26469;&#28608;&#27963;LLM&#22312;Text-to-SQL&#20219;&#21153;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;Text-to-SQL&#25552;&#31034;&#26041;&#27861;&#30340;&#33539;&#24335;&#65292;&#31216;&#20026;&#20998;&#32780;&#27835;&#20043;&#65292;&#36890;&#36807;&#20808;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#20219;&#21153;&#65292;&#28982;&#21518;&#36890;&#36807;&#24605;&#32500;&#38142;&#36880;&#20010;&#35299;&#20915;&#23376;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;3&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;LLM&#30340;Text-to-SQL&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#25552;&#31034;&#24341;&#23548;LLM&#29983;&#25104;&#20855;&#26377;&#26356;&#39640;&#25191;&#34892;&#20934;&#30830;&#24615;&#30340;Text-to-SQL&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain-of-thought (CoT) prompting combined with large language models (LLMs) have achieved encouraging results on complex reasoning tasks. Text-to-SQL is a critical semantic parsing task that converts natural language questions into SQL statements, involving a complex reasoning process. However, there is little work about using CoT prompting to activate LLM's reasoning capabilities on Text-to-SQL tasks. In this work, we propose a new paradigm for prompting Text-to-SQL tasks, called Divide-and-Prompt, which first divides the task into subtasks, and then approach each subtask through CoT. We present 3 prompting-based methods to enhance the Text-to-SQL ability of LLMs. Experiments show that these prompts guide LLMs to generate Text-to-SQL with higher execution accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#24212;&#29992;&#21644;&#32771;&#34385;&#20262;&#29702;&#21644;&#21746;&#23398;&#21407;&#21017;&#20197;&#30830;&#20445;&#21487;&#38752;&#30340;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#30340;&#37325;&#35201;&#24615;&#12290;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#30103;&#20013;&#24102;&#26469;&#20102;&#26356;&#22810;&#25361;&#25112;&#65292;&#24517;&#39035;&#35299;&#20915;&#20559;&#35265;&#12289;&#36879;&#26126;&#24230;&#12289;&#33258;&#20027;&#26435;&#12289;&#36131;&#20219;&#21644;&#38382;&#36131;&#21046;&#31561;&#38382;&#39064;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#35299;&#20915;&#21150;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.11530</link><description>&lt;p&gt;
&#36890;&#36807;&#20262;&#29702;&#21644;&#21746;&#23398;&#21407;&#21017;&#30830;&#20445;&#21487;&#20449;&#36182;&#30340;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Ensuring Trustworthy Medical Artificial Intelligencethrough Ethical and Philosophical Principles. (arXiv:2304.11530v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#24212;&#29992;&#21644;&#32771;&#34385;&#20262;&#29702;&#21644;&#21746;&#23398;&#21407;&#21017;&#20197;&#30830;&#20445;&#21487;&#38752;&#30340;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#30340;&#37325;&#35201;&#24615;&#12290;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#30103;&#20013;&#24102;&#26469;&#20102;&#26356;&#22810;&#25361;&#25112;&#65292;&#24517;&#39035;&#35299;&#20915;&#20559;&#35265;&#12289;&#36879;&#26126;&#24230;&#12289;&#33258;&#20027;&#26435;&#12289;&#36131;&#20219;&#21644;&#38382;&#36131;&#21046;&#31561;&#38382;&#39064;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#35299;&#20915;&#21150;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#22312;&#21307;&#30103;&#25252;&#29702;&#26041;&#38754;&#20855;&#26377;&#26497;&#22823;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#36890;&#36807;&#25552;&#39640;&#21307;&#30103;&#19987;&#23478;&#21644;&#24739;&#32773;&#30340;&#20307;&#39564;&#26469;&#24443;&#24213;&#25913;&#21464;&#20247;&#22810;&#21307;&#30103;&#25252;&#29702;&#12290;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#24037;&#20855;&#22914;&#26524;&#33021;&#22815;&#34920;&#29616;&#20986;&#33394;&#29978;&#33267;&#19982;&#20020;&#24202;&#19987;&#23478;&#30340;&#27700;&#24179;&#30456;&#24403;&#65292;&#23601;&#21487;&#20197;&#20135;&#29983;&#24040;&#22823;&#30340;&#25928;&#30410;&#12290;&#22240;&#27492;&#65292;&#21457;&#23637;&#20013;&#22269;&#23478;&#21487;&#20197;&#25552;&#20379;&#20808;&#36827;&#30340;&#21307;&#30103;&#25252;&#29702;&#26381;&#21153;&#65292;&#24182;&#35299;&#20915;&#32570;&#20047;&#19987;&#19994;&#21307;&#30103;&#20174;&#19994;&#32773;&#30340;&#38382;&#39064;&#12290;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24037;&#20855;&#21487;&#20197;&#33410;&#30465;&#26102;&#38388;&#12289;&#36164;&#28304;&#21644;&#25972;&#20307;&#27835;&#30103;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#19982;&#20154;&#31867;&#30456;&#27604;&#65292;&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#25581;&#31034;&#22823;&#37327;&#36755;&#20837;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#29978;&#33267;&#21487;&#20197;&#20026;&#21307;&#23398;&#25552;&#20379;&#26032;&#30340;&#22522;&#20110;&#35777;&#25454;&#30340;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#30103;&#25252;&#29702;&#20013;&#25972;&#21512;&#20154;&#24037;&#26234;&#33021;&#20063;&#24102;&#26469;&#20102;&#20960;&#20010;&#20262;&#29702;&#21644;&#21746;&#23398;&#19978;&#30340;&#38382;&#39064;&#65292;&#22914;&#20559;&#35265;&#12289;&#36879;&#26126;&#24230;&#12289;&#33258;&#20027;&#26435;&#12289;&#36131;&#20219;&#21644;&#38382;&#36131;&#21046;&#65292;&#36825;&#20123;&#38382;&#39064;&#24517;&#39035;&#22312;&#23558;&#36825;&#20123;&#24037;&#20855;&#25972;&#21512;&#21040;&#20020;&#24202;&#29615;&#22659;&#20043;&#21069;&#24471;&#21040;&#35299;&#20915;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#30103;&#25252;&#29702;&#20013;&#30340;&#26368;&#26032;&#24212;&#29992;&#20197;&#21450;&#32771;&#34385;&#20262;&#29702;&#21644;&#21746;&#23398;&#21407;&#21017;&#20197;&#30830;&#20445;&#21487;&#20449;&#36182;&#30340;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#19982;&#21307;&#30103;&#25252;&#29702;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#30456;&#20851;&#30340;&#21508;&#31181;&#25361;&#25112;&#65292;&#21253;&#25324;&#25968;&#25454;&#20559;&#35265;&#12289;&#36879;&#26126;&#24230;&#30340;&#38656;&#35201;&#12289;&#33258;&#20027;&#20915;&#31574;&#30340;&#38382;&#39064;&#20197;&#21450;&#38382;&#36131;&#21046;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#28508;&#22312;&#26041;&#26696;&#65292;&#21253;&#25324;&#30830;&#20445;&#36879;&#26126;&#24230;&#21644;&#38382;&#36131;&#21046;&#30340;&#26694;&#26550;&#20197;&#21450;&#25351;&#23548;&#20154;&#24037;&#26234;&#33021;&#24320;&#21457;&#32773;&#32771;&#34385;&#20262;&#29702;&#21407;&#21017;&#30340;&#25351;&#21335;&#12290;&#36890;&#36807;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#24182;&#23454;&#26045;&#20262;&#29702;&#21644;&#21746;&#23398;&#21407;&#21017;&#65292;&#25105;&#20204;&#21487;&#20197;&#30830;&#20445;&#24320;&#21457;&#20986;&#31526;&#21512;&#35786;&#25152;&#35774;&#32622;&#30340;&#21463;&#20449;&#20219;&#30340;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) methods have great potential to revolutionize numerous medical care by enhancing the experience of medical experts and patients. AI based computer-assisted diagnosis tools can have a tremendous benefit if they can outperform or perform similarly to the level of a clinical expert. As a result, advanced healthcare services can be affordable in developing nations, and the problem of a lack of expert medical practitioners can be addressed. AI based tools can save time, resources, and overall cost for patient treatment. Furthermore, in contrast to humans, AI can uncover complex relations in the data from a large set of inputs and even lead to new evidence-based knowledge in medicine. However, integrating AI in healthcare raises several ethical and philosophical concerns, such as bias, transparency, autonomy, responsibility and accountability, which must be addressed before integrating such tools into clinical settings. In this article, we emphasize recent advanc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#26102;&#21453;&#39304;&#31574;&#30053;&#65292;&#20197;&#25511;&#21046;&#27969;&#20307;&#24377;&#29699;&#19978;&#30340;&#27700;&#21160;&#21147;&#21147;&#65292;&#21487;&#22312;&#38750;&#21442;&#25968;&#25511;&#21046;&#21442;&#25968;&#31354;&#38388;&#20869;&#20316;&#20986;&#21512;&#29702;&#26377;&#25928;&#30340;&#25511;&#21046;&#20915;&#31574;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#20102;&#35299;&#27969;&#25511;&#36807;&#31243;&#30340;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2304.11526</link><description>&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#27969;&#20307;&#24377;&#29699;&#30340;&#27700;&#21160;&#21147;&#21147;&#8203;&#8203;&#8203;&#8203;&#8203;&#8203;&#8203;
&lt;/p&gt;
&lt;p&gt;
How to Control Hydrodynamic Force on Fluidic Pinball via Deep Reinforcement Learning. (arXiv:2304.11526v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11526
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#26102;&#21453;&#39304;&#31574;&#30053;&#65292;&#20197;&#25511;&#21046;&#27969;&#20307;&#24377;&#29699;&#19978;&#30340;&#27700;&#21160;&#21147;&#21147;&#65292;&#21487;&#22312;&#38750;&#21442;&#25968;&#25511;&#21046;&#21442;&#25968;&#31354;&#38388;&#20869;&#20316;&#20986;&#21512;&#29702;&#26377;&#25928;&#30340;&#25511;&#21046;&#20915;&#31574;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#20102;&#35299;&#27969;&#25511;&#36807;&#31243;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#23454;&#26102;&#21453;&#39304;&#31574;&#30053;&#65292;&#20197;&#25511;&#21046;&#27969;&#20307;&#24377;&#29699;&#19978;&#30340;&#27700;&#21160;&#21147;&#21147;&#12290;&#36890;&#36807;&#20805;&#20998;&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#21644;&#32534;&#30721;&#21382;&#21490;&#35266;&#23519;&#32467;&#26524;&#65292;&#33258;&#21160;&#23398;&#20064;&#24182;&#36845;&#20195;&#19978;&#21315;&#27425;&#65292;&#35813;&#31574;&#30053;&#21487;&#20197;&#22312;&#38750;&#21442;&#25968;&#25511;&#21046;&#21442;&#25968;&#31354;&#38388;&#20869;&#20316;&#20986;&#21512;&#29702;&#26377;&#25928;&#30340;&#25511;&#21046;&#20915;&#31574;&#65292;&#27604;&#20887;&#38271;&#30340;&#26292;&#21147;&#25628;&#32034;&#25214;&#21040;&#30340;&#26368;&#20248;&#31574;&#30053;&#36824;&#35201;&#22909;&#12290;&#38543;&#21518;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#20854;&#20013;&#19968;&#39033;&#32467;&#26524;&#36827;&#34892;&#20998;&#26512;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#20197;&#27492;&#20102;&#35299;&#27969;&#25511;&#36807;&#31243;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (DRL) for fluidic pinball, three individually rotating cylinders in the uniform flow arranged in an equilaterally triangular configuration, can learn the efficient flow control strategies due to the validity of self-learning and data-driven state estimation for complex fluid dynamic problems. In this work, we present a DRL-based real-time feedback strategy to control the hydrodynamic force on fluidic pinball, i.e., force extremum and tracking, from cylinders' rotation. By adequately designing reward functions and encoding historical observations, and after automatic learning of thousands of iterations, the DRL-based control was shown to make reasonable and valid control decisions in nonparametric control parameter space, which is comparable to and even better than the optimal policy found through lengthy brute-force searching. Subsequently, one of these results was analyzed by a machine learning model that enabled us to shed light on the basis of decision-ma
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#35843;&#21046;&#30340;&#20010;&#24615;&#21270;&#32852;&#21512;&#23398;&#20064;&#26041;&#26696;&#65292;&#21517;&#20026;FedSUMM&#65292;&#29992;&#20110;&#24322;&#26500;&#25991;&#26412;&#25688;&#35201;&#12290;&#23427;&#33021;&#22815;&#22312;&#38544;&#31169;&#20445;&#25252;&#19979;&#23454;&#29616;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.11524</link><description>&lt;p&gt;
&#22522;&#20110;&#26799;&#24230;&#35843;&#21046;&#30340;&#20010;&#24615;&#21270;&#32852;&#21512;&#23398;&#20064;&#22312;&#24322;&#26500;&#25991;&#26412;&#25688;&#35201;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Personalized Federated Learning via Gradient Modulation for Heterogeneous Text Summarization. (arXiv:2304.11524v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#35843;&#21046;&#30340;&#20010;&#24615;&#21270;&#32852;&#21512;&#23398;&#20064;&#26041;&#26696;&#65292;&#21517;&#20026;FedSUMM&#65292;&#29992;&#20110;&#24322;&#26500;&#25991;&#26412;&#25688;&#35201;&#12290;&#23427;&#33021;&#22815;&#22312;&#38544;&#31169;&#20445;&#25252;&#19979;&#23454;&#29616;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#25688;&#35201;&#23545;&#20110;&#20449;&#24687;&#32858;&#21512;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#19988;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#20294;&#26159;&#65292;&#23545;&#25968;&#25454;&#38544;&#31169;&#21644;&#23433;&#20840;&#30340;&#25285;&#24551;&#38480;&#21046;&#20102;&#25968;&#25454;&#30340;&#25910;&#38598;&#21644;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#23398;&#20064;&#25991;&#26412;&#25688;&#35201;&#26041;&#26696;&#65292;&#23427;&#20801;&#35768;&#29992;&#25143;&#22312;&#19981;&#20849;&#20139;&#21407;&#22987;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20197;&#21512;&#20316;&#23398;&#20064;&#30340;&#26041;&#24335;&#20998;&#20139;&#20840;&#23616;&#27169;&#22411;&#12290;&#20010;&#24615;&#21270;&#32852;&#21512;&#23398;&#20064;&#65288;PFL&#65289;&#22312;&#20840;&#23616;&#27169;&#22411;&#20248;&#21270;&#36807;&#31243;&#20013;&#24179;&#34913;&#20010;&#24615;&#21270;&#21644;&#27867;&#21270;&#65292;&#20197;&#25351;&#23548;&#26412;&#22320;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#22810;&#20010;&#26412;&#22320;&#25968;&#25454;&#20855;&#26377;&#19981;&#21516;&#30340;&#35821;&#20041;&#21644;&#19978;&#19979;&#25991;&#20998;&#24067;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#26412;&#22320;&#27169;&#22411;&#23398;&#20064;&#21040;&#20559;&#31163;&#30340;&#35821;&#20041;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedSUMM&#65292;&#19968;&#31181;&#21160;&#24577;&#26799;&#24230;&#36866;&#37197;&#22120;&#65292;&#20026;&#26412;&#22320;&#27169;&#22411;&#25552;&#20379;&#26356;&#36866;&#24403;&#30340;&#26412;&#22320;&#21442;&#25968;&#12290;&#21516;&#26102;&#65292;FedSUMM&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#26469;&#38450;&#27490;&#20998;&#24067;&#24335;&#35757;&#32451;&#26399;&#38388;&#30340;&#21442;&#25968;&#27844;&#38706;&#12290;&#23454;&#39564;&#35777;&#25454;&#39564;&#35777;&#20102;FedSUMM&#21487;&#20197;&#22312;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#26041;&#38754;&#23454;&#29616;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text summarization is essential for information aggregation and demands large amounts of training data. However, concerns about data privacy and security limit data collection and model training. To eliminate this concern, we propose a federated learning text summarization scheme, which allows users to share the global model in a cooperative learning manner without sharing raw data. Personalized federated learning (PFL) balances personalization and generalization in the process of optimizing the global model, to guide the training of local models. However, multiple local data have different distributions of semantics and context, which may cause the local model to learn deviated semantic and context information. In this paper, we propose FedSUMM, a dynamic gradient adapter to provide more appropriate local parameters for local model. Simultaneously, FedSUMM uses differential privacy to prevent parameter leakage during distributed training. Experimental evidence verifies FedSUMM can ach
&lt;/p&gt;</description></item><item><title>LayerNAS&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#39033;&#24335;&#22797;&#26434;&#24230;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#23558;&#25628;&#32034;&#20998;&#20026;&#22810;&#20010;&#30446;&#26631;&#65292;&#24182;&#23558;&#25628;&#32034;&#25104;&#26412;&#21644;&#22870;&#21169;&#20803;&#32032;&#20998;&#24320;&#65292;&#33021;&#22815;&#24555;&#36895;&#26377;&#25928;&#22320;&#21457;&#29616;&#20248;&#36234;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.11517</link><description>&lt;p&gt;
LayerNAS&#65306;&#22810;&#30446;&#26631;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#30340;&#22810;&#39033;&#24335;&#22797;&#26434;&#24230;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LayerNAS: Neural Architecture Search in Polynomial Complexity. (arXiv:2304.11517v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11517
&lt;/p&gt;
&lt;p&gt;
LayerNAS&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#39033;&#24335;&#22797;&#26434;&#24230;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#23558;&#25628;&#32034;&#20998;&#20026;&#22810;&#20010;&#30446;&#26631;&#65292;&#24182;&#23558;&#25628;&#32034;&#25104;&#26412;&#21644;&#22870;&#21169;&#20803;&#32032;&#20998;&#24320;&#65292;&#33021;&#22815;&#24555;&#36895;&#26377;&#25928;&#22320;&#21457;&#29616;&#20248;&#36234;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#24050;&#25104;&#20026;&#21457;&#29616;&#26377;&#25928;&#27169;&#22411;&#26550;&#26500;&#30340;&#27969;&#34892;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#30446;&#26631;&#30828;&#20214;&#32780;&#35328;&#12290;&#22240;&#27492;&#65292;&#22312;&#32422;&#26463;&#26465;&#20214;&#19979;&#25214;&#21040;&#26368;&#20339;&#26550;&#26500;&#30340;NAS&#26041;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LayerNAS&#65292;&#23558;&#22810;&#30446;&#26631;NAS&#30340;&#25361;&#25112;&#36716;&#21270;&#20026;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#26377;&#25928;&#22320;&#23558;&#25628;&#32034;&#22797;&#26434;&#24230;&#38480;&#21046;&#20026;&#22810;&#39033;&#24335;&#12290;&#23545;&#20110;&#20855;&#26377;$L$&#23618;&#30340;&#27169;&#22411;&#26550;&#26500;&#65292;&#25105;&#20204;&#20026;&#27599;&#19968;&#23618;&#25191;&#34892;&#36880;&#23618;&#25628;&#32034;&#65292;&#20174;&#19968;&#20010;&#25628;&#32034;&#36873;&#39033;&#38598;$\mathbb{S}$&#20013;&#36827;&#34892;&#36873;&#25321;&#12290;LayerNAS&#26681;&#25454;&#19968;&#20010;&#30446;&#26631;&#65292;&#20363;&#22914;&#27169;&#22411;&#22823;&#23567;&#25110;&#24310;&#36831;&#65292;&#23545;&#27169;&#22411;&#20505;&#36873;&#36827;&#34892;&#20998;&#32452;&#65292;&#24182;&#22522;&#20110;&#21478;&#19968;&#20010;&#30446;&#26631;&#25628;&#32034;&#26368;&#20339;&#27169;&#22411;&#65292;&#20174;&#32780;&#20998;&#21106;&#20102;&#25628;&#32034;&#30340;&#25104;&#26412;&#21644;&#22870;&#21169;&#20803;&#32032;&#12290;&#36825;&#31181;&#26041;&#27861;&#23558;&#25628;&#32034;&#22797;&#26434;&#24230;&#38480;&#21046;&#22312;$O(H \cdot |\mathbb{S}| \cdot L)$&#65292;&#20854;&#20013;$H$&#26159;&#22312;LayerNAS&#20013;&#35774;&#23450;&#30340;&#24120;&#25968;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;LayerNAS&#33021;&#22815;&#25345;&#32493;&#21457;&#29616;&#20248;&#36234;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Architecture Search (NAS) has become a popular method for discovering effective model architectures, especially for target hardware. As such, NAS methods that find optimal architectures under constraints are essential. In our paper, we propose LayerNAS to address the challenge of multi-objective NAS by transforming it into a combinatorial optimization problem, which effectively constrains the search complexity to be polynomial.  For a model architecture with $L$ layers, we perform layerwise-search for each layer, selecting from a set of search options $\mathbb{S}$. LayerNAS groups model candidates based on one objective, such as model size or latency, and searches for the optimal model based on another objective, thereby splitting the cost and reward elements of the search. This approach limits the search complexity to $ O(H \cdot |\mathbb{S}| \cdot L) $, where $H$ is a constant set in LayerNAS.  Our experiments show that LayerNAS is able to consistently discover superior models
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#24490;&#29615;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#26816;&#27979;&#39640;&#36895;&#20844;&#36335;&#19978;&#30340;&#31038;&#20132;&#24322;&#24120;&#39550;&#39542;&#34892;&#20026;&#65292;&#22810;&#26679;&#24615;&#30340;&#36710;&#36742;&#20132;&#20114;&#21644;&#26102;&#31354;&#21464;&#21270;&#22686;&#21152;&#20102;&#26816;&#27979;&#30340;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.11513</link><description>&lt;p&gt;
&#36890;&#36807;&#24490;&#29615;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#26816;&#27979;&#39640;&#36895;&#20844;&#36335;&#19978;&#30340;&#31038;&#20132;&#24322;&#24120;&#39550;&#39542;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Detecting Socially Abnormal Highway Driving Behaviors via Recurrent Graph Attention Networks. (arXiv:2304.11513v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11513
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#24490;&#29615;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#26816;&#27979;&#39640;&#36895;&#20844;&#36335;&#19978;&#30340;&#31038;&#20132;&#24322;&#24120;&#39550;&#39542;&#34892;&#20026;&#65292;&#22810;&#26679;&#24615;&#30340;&#36710;&#36742;&#20132;&#20114;&#21644;&#26102;&#31354;&#21464;&#21270;&#22686;&#21152;&#20102;&#26816;&#27979;&#30340;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29289;&#32852;&#32593;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#19979;&#19968;&#20195;&#20132;&#36890;&#30417;&#25511;&#22522;&#30784;&#35774;&#26045;&#36890;&#36807;&#32593;&#32476;&#36830;&#25509;&#65292;&#20197;&#24110;&#21161;&#20132;&#36890;&#25968;&#25454;&#25910;&#38598;&#21644;&#26234;&#33021;&#20132;&#36890;&#31649;&#29702;&#12290;&#20132;&#36890;&#20013;&#26368;&#37325;&#35201;&#30340;&#20219;&#21153;&#20043;&#19968;&#26159;&#24322;&#24120;&#26816;&#27979;&#65292;&#22240;&#20026;&#24322;&#24120;&#21496;&#26426;&#20250;&#38477;&#20302;&#20132;&#36890;&#25928;&#29575;&#24182;&#24341;&#36215;&#23433;&#20840;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#38598;&#20013;&#20110;&#20174;&#39640;&#36895;&#20844;&#36335;&#35270;&#39057;&#30417;&#25511;&#31995;&#32479;&#29983;&#25104;&#30340;&#36712;&#36857;&#20013;&#26816;&#27979;&#24322;&#24120;&#39550;&#39542;&#34892;&#20026;&#12290;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;&#24322;&#24120;&#39550;&#39542;&#34892;&#20026;&#26816;&#27979;&#26041;&#27861;&#20391;&#37325;&#20110;&#22788;&#29702;&#21333;&#20010;&#36710;&#36742;&#32780;&#19981;&#32771;&#34385;&#36710;&#36742;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#26412;&#30740;&#31350;&#32771;&#34385;&#26816;&#27979;&#22810;&#31181;&#19981;&#31526;&#21512;&#38468;&#36817;&#20854;&#20182;&#39550;&#39542;&#21592;&#34892;&#20026;&#30340;&#31038;&#20132;&#24322;&#24120;&#39550;&#39542;&#34892;&#20026;&#12290;&#36825;&#20010;&#20219;&#21153;&#30001;&#20110;&#36710;&#36742;&#20132;&#20114;&#30340;&#22810;&#26679;&#24615;&#21644;&#39640;&#36895;&#20844;&#36335;&#20132;&#36890;&#30340;&#26102;&#31354;&#21464;&#21270;&#32780;&#21464;&#24471;&#22797;&#26434;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;RGANs&#23545;&#19981;&#21516;&#36710;&#36742;&#20043;&#38388;&#30340;&#20132;&#20114;&#20197;&#21450;&#39640;&#36895;&#20844;&#36335;&#20132;&#36890;&#30340;&#26102;&#31354;&#21464;&#21270;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid development of Internet of Things technologies, the next generation traffic monitoring infrastructures are connected via the web, to aid traffic data collection and intelligent traffic management. One of the most important tasks in traffic is anomaly detection, since abnormal drivers can reduce traffic efficiency and cause safety issues. This work focuses on detecting abnormal driving behaviors from trajectories produced by highway video surveillance systems. Most of the current abnormal driving behavior detection methods focus on a limited category of abnormal behaviors that deal with a single vehicle without considering vehicular interactions. In this work, we consider the problem of detecting a variety of socially abnormal driving behaviors, i.e., behaviors that do not conform to the behavior of other nearby drivers. This task is complicated by the variety of vehicular interactions and the spatial-temporal varying nature of highway traffic. To solve this problem, we p
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#20132;&#36890;&#20107;&#25925;&#25253;&#21578;&#20013;&#21487;&#20197;&#33719;&#21462;&#30340;&#20449;&#24687;&#65292;&#39044;&#27979;&#24182;&#20248;&#21270;&#24212;&#24613;&#21709;&#24212;&#22242;&#38431;&#30340;&#36164;&#28304;&#37096;&#32626;&#65292;&#20174;&#32780;&#20943;&#23569;&#20107;&#25925;&#25345;&#32493;&#26102;&#38388;&#65292;&#21516;&#26102;&#25552;&#39640;&#20844;&#20247;&#23433;&#20840;&#12290;</title><link>http://arxiv.org/abs/2304.11507</link><description>&lt;p&gt;
&#29992;&#20110;&#19968;&#20307;&#21270;&#20107;&#20214;&#25345;&#32493;&#26102;&#38388;&#39044;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Machine learning framework for end-to-end implementation of Incident duration prediction. (arXiv:2304.11507v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11507
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#20132;&#36890;&#20107;&#25925;&#25253;&#21578;&#20013;&#21487;&#20197;&#33719;&#21462;&#30340;&#20449;&#24687;&#65292;&#39044;&#27979;&#24182;&#20248;&#21270;&#24212;&#24613;&#21709;&#24212;&#22242;&#38431;&#30340;&#36164;&#28304;&#37096;&#32626;&#65292;&#20174;&#32780;&#20943;&#23569;&#20107;&#25925;&#25345;&#32493;&#26102;&#38388;&#65292;&#21516;&#26102;&#25552;&#39640;&#20844;&#20247;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#32463;&#24120;&#24615;&#20107;&#20214;&#65292;&#22914;&#36710;&#36742;&#30896;&#25758;&#21644;&#38556;&#30861;&#29289;&#25152;&#24341;&#36215;&#30340;&#20132;&#36890;&#25317;&#22581;&#26159;&#20132;&#36890;&#31649;&#29702;&#20013;&#24515;(TMCs)&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#21450;&#26102;&#28165;&#38500;&#20107;&#20214;&#23545;&#20110;&#25913;&#21892;&#23433;&#20840;&#12289;&#20943;&#23569;&#24310;&#35823;&#21644;&#25490;&#25918;&#23545;&#20844;&#20247;&#20986;&#34892;&#30340;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;TMCs&#21644;&#20854;&#20182;&#24212;&#24613;&#21709;&#24212;&#22242;&#38431;&#24448;&#24448;&#38590;&#20197;&#20934;&#30830;&#22320;&#39044;&#27979;&#20107;&#20214;&#30340;&#25345;&#32493;&#26102;&#38388;(&#31561;&#21040;&#36335;&#38754;&#24471;&#21040;&#28165;&#29702;)&#65292;&#20351;&#24471;&#20915;&#31574;&#22914;&#20309;&#37096;&#32626;&#36164;&#28304;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#20998;&#26512;&#26694;&#26550;&#21644;&#31471;&#21040;&#31471;&#30340;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#26681;&#25454;&#20107;&#20214;&#25253;&#21578;&#21518;&#31435;&#21363;&#21487;&#29992;&#30340;&#20449;&#24687;&#39044;&#27979;&#20107;&#20214;&#25345;&#32493;&#26102;&#38388;&#12290;&#23545;&#20107;&#20214;&#25345;&#32493;&#26102;&#38388;&#30340;&#36136;&#37327;&#39044;&#27979;&#21487;&#20197;&#24110;&#21161;TMCs&#21644;&#20854;&#20182;&#24212;&#24613;&#21709;&#24212;&#22242;&#38431;&#37319;&#21462;&#20027;&#21160;&#25514;&#26045;&#65292;&#37096;&#32626;&#25937;&#25588;&#26381;&#21153;&#65292;&#22914;&#25302;&#36710;&#12289;&#32500;&#25252;&#22242;&#38431;&#25110;&#28608;&#27963;&#22791;&#36873;&#36335;&#32447;&#12290;&#39044;&#27979;&#20351;&#29992;&#20998;&#31867;&#21644;&#22238;&#24402;&#26426;&#22120;&#23398;&#20064;&#27169;&#22359;&#30340;&#32452;&#21512;&#12290;&#35813;&#27169;&#22411;&#22312;&#26469;&#33258;&#20315;&#32599;&#37324;&#36798;&#20132;&#36890;&#37096;&#30340;&#23454;&#38469;&#25968;&#25454;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#24456;&#26377;&#24076;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic congestion caused by non-recurring incidents such as vehicle crashes and debris is a key issue for Traffic Management Centers (TMCs). Clearing incidents in a timely manner is essential for improving safety and reducing delays and emissions for the traveling public. However, TMCs and other responders face a challenge in predicting the duration of incidents (until the roadway is clear), making decisions of what resources to deploy difficult. To address this problem, this research developed an analytical framework and end-to-end machine-learning solution for predicting incident duration based on information available as soon as an incident report is received. Quality predictions of incident duration can help TMCs and other responders take a proactive approach in deploying responder services such as tow trucks, maintenance crews or activating alternative routes. The predictions use a combination of classification and regression machine learning modules. The performance of the devel
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#31034;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24515;&#26234;&#29702;&#35770;&#65288;ToM&#65289;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#35777;&#26126;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#21487;&#20197;&#25552;&#21319;LLMs&#22312;&#22797;&#26434;&#25512;&#29702;&#29305;&#21035;&#26159;ToM&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.11490</link><description>&lt;p&gt;
&#36890;&#36807;&#25552;&#31034;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24515;&#26234;&#29702;&#35770;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Boosting Theory-of-Mind Performance in Large Language Models via Prompting. (arXiv:2304.11490v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#31034;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24515;&#26234;&#29702;&#35770;&#65288;ToM&#65289;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#35777;&#26126;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#21487;&#20197;&#25552;&#21319;LLMs&#22312;&#22797;&#26434;&#25512;&#29702;&#29305;&#21035;&#26159;ToM&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2023&#24180;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#22797;&#26434;&#25512;&#29702;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;&#24515;&#26234;&#29702;&#35770;&#65288;ToM&#65289;&#20219;&#21153;&#38656;&#35201;&#29702;&#35299;&#20195;&#29702;&#20154;&#30340;&#20449;&#24565;&#12289;&#30446;&#26631;&#21644;&#24515;&#29702;&#29366;&#24577;&#65292;&#23545;&#20110;&#28041;&#21450;&#20154;&#31867;&#30340;&#24120;&#35782;&#25512;&#29702;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#27492;&#25552;&#39640;LLM&#22312;&#36825;&#26041;&#38754;&#30340;&#34920;&#29616;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#27979;&#37327;&#20102;GPT-4&#21644;&#19977;&#20010;GPT-3.5&#21464;&#20307;&#65288;Davinci-2&#12289;Davinci-3&#12289;GPT-3.5-Turbo&#65289;&#30340;ToM&#34920;&#29616;&#65292;&#24182;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#25552;&#39640;&#23427;&#20204;&#30340;ToM&#29702;&#35299;&#21147;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#21253;&#21547;&#20004;&#27493;&#24605;&#32500;&#25512;&#29702;&#21644;&#36880;&#27493;&#24605;&#32771;&#35828;&#26126;&#30340;&#25552;&#31034;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#35757;&#32451;&#30340;LLMs&#65288;&#38500;Davinci-2&#22806;&#30340;&#25152;&#26377;&#27169;&#22411;&#65289;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#25552;&#39640;&#20102;&#23427;&#20204;&#30340;ToM&#20934;&#30830;&#24615;&#12290;GPT-4&#22312;&#38646;&#36718;&#24773;&#20917;&#19979;&#34920;&#29616;&#26368;&#20339;&#65292;&#36798;&#21040;&#20102;&#36817;80%&#30340;ToM&#20934;&#30830;&#24615;&#65292;&#20294;&#20173;&#19981;&#36275;&#27979;&#35797;&#38598;&#19978;87%&#30340;&#20154;&#31867;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#25552;&#20379;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#25552;&#31034;&#26102;&#65292;GPT-4&#21644;&#19977;&#20010;GPT-3.5&#21464;&#20307;&#30340;ToM&#20934;&#30830;&#24615;&#26174;&#33879;&#39640;&#20110;&#26080;&#25552;&#31034;&#26102;&#65292;&#20854;&#20013;&#34920;&#29616;&#26368;&#22909;&#30340;&#27169;&#22411;&#65288;GPT-3.5-Turbo&#65289;&#36798;&#21040;&#20102;92%&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#23637;&#31034;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#25552;&#21319;LLM&#22312;&#22797;&#26434;&#25512;&#29702;&#23588;&#20854;&#26159;ToM&#20219;&#21153;&#20013;&#34920;&#29616;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) excel in many tasks in 2023, but they still face challenges in complex reasoning. Theory-of-mind (ToM) tasks, which require understanding agents' beliefs, goals, and mental states, are essential for common-sense reasoning involving humans, making it crucial to enhance LLM performance in this area. This study measures the ToM performance of GPT-4 and three GPT-3.5 variants (Davinci-2, Davinci-3, GPT-3.5-Turbo), and investigates the effectiveness of in-context learning in improving their ToM comprehension. We evaluated prompts featuring two-shot chain of thought reasoning and step-by-step thinking instructions. We found that LLMs trained with Reinforcement Learning from Human Feedback (RLHF) (all models excluding Davinci-2) improved their ToM accuracy via in-context learning. GPT-4 performed best in zero-shot settings, reaching nearly 80% ToM accuracy, but still fell short of the 87% human accuracy on the test set. However, when supplied with prompts for in-c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#26377;&#25928;&#35782;&#21035;&#20102;&#24110;&#27966;&#30456;&#20851;&#31038;&#20132;&#23186;&#20307;&#19978;&#21487;&#33021;&#38656;&#35201;&#31038;&#21306;&#36164;&#28304;&#24110;&#21161;&#30340;&#20154;&#32676;&#65292;&#25299;&#23637;&#20102;&#31038;&#21306;&#25104;&#21592;&#29031;&#39038;&#30340;&#33539;&#30068;&#12290;</title><link>http://arxiv.org/abs/2304.11485</link><description>&lt;p&gt;
&#35782;&#21035;&#19982;&#24110;&#21161;&#31038;&#21306;&#25104;&#21592;&#30340;&#24110;&#27966;&#30456;&#20851;&#31038;&#20132;&#23186;&#20307;&#20132;&#27969;&#20013;&#30340;&#35789;&#27719;&#20559;&#24046;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Understanding Lexical Biases when Identifying Gang-related Social Media Communications. (arXiv:2304.11485v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11485
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#26377;&#25928;&#35782;&#21035;&#20102;&#24110;&#27966;&#30456;&#20851;&#31038;&#20132;&#23186;&#20307;&#19978;&#21487;&#33021;&#38656;&#35201;&#31038;&#21306;&#36164;&#28304;&#24110;&#21161;&#30340;&#20154;&#32676;&#65292;&#25299;&#23637;&#20102;&#31038;&#21306;&#25104;&#21592;&#29031;&#39038;&#30340;&#33539;&#30068;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#19982;&#24110;&#27966;&#27963;&#21160;&#30340;&#20154;&#20351;&#29992;&#21253;&#25324;Facebook&#21644;Twitter&#22312;&#20869;&#30340;&#20027;&#27969;&#31038;&#20132;&#23186;&#20307;&#26469;&#34920;&#36798;&#22066;&#35773;&#21644;&#23041;&#32961;&#20197;&#21450;&#21696;&#24764;&#21644;&#32426;&#24565;&#12290;&#28982;&#32780;&#65292;&#35782;&#21035;&#24110;&#27966;&#30456;&#20851;&#27963;&#21160;&#30340;&#24433;&#21709;&#20197;&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#20026;&#31038;&#21306;&#25104;&#21592;&#25552;&#20379;&#24110;&#21161;&#26159;&#20855;&#26377;&#29420;&#29305;&#25361;&#25112;&#30340;&#65292;&#36825;&#21253;&#25324;&#36947;&#24503;&#19978;&#35782;&#21035;&#21463;&#24110;&#27966;&#27963;&#21160;&#24433;&#21709;&#30340;&#20010;&#20307;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#22256;&#38590;&#21644;&#38656;&#35201;&#32771;&#34385;&#36825;&#20123;&#20010;&#20307;&#22312;&#25512;&#25991;&#20013;&#24120;&#29992;&#30340;&#38750;&#26631;&#20934;&#35821;&#35328;&#39118;&#26684;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#21487;&#33021;&#38656;&#35201;&#31038;&#21306;&#29031;&#39038;&#36164;&#28304;&#65292;&#22914;&#39038;&#38382;&#12289;&#20914;&#31361;&#35843;&#35299;&#32773;&#25110;&#23398;&#26415;/&#19987;&#19994;&#22521;&#35757;&#35745;&#21010;&#30340;&#20010;&#20307;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#20108;&#20803;&#36923;&#36753;&#20998;&#31867;&#22120;&#22312;&#35782;&#21035;&#21463;&#24110;&#27966;&#30456;&#20851;&#26292;&#21147;&#24433;&#21709;&#30340;&#20010;&#20307;&#26102;&#65292;&#22312;&#20351;&#29992;&#19982;2015&#24180;&#24052;&#23572;&#30340;&#25705;&#26292;&#21160;&#30456;&#20851;&#30340;&#24110;&#27966;&#30456;&#20851;&#25512;&#25991;&#26679;&#26412;&#26102;&#65292;&#20248;&#20110;&#22522;&#32447;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Individuals involved in gang-related activity use mainstream social media including Facebook and Twitter to express taunts and threats as well as grief and memorializing. However, identifying the impact of gang-related activity in order to serve community member needs through social media sources has a unique set of challenges. This includes the difficulty of ethically identifying training data of individuals impacted by gang activity and the need to account for a non-standard language style commonly used in the tweets from these individuals. Our study provides evidence of methods where natural language processing tools can be helpful in efficiently identifying individuals who may be in need of community care resources such as counselors, conflict mediators, or academic/professional training programs. We demonstrate that our binary logistic classifier outperforms baseline standards in identifying individuals impacted by gang-related violence using a sample of gang-related tweets associ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#26469;&#33258;&#20843;&#20010;&#35821;&#38899;&#21644;&#38899;&#39057;PTMs&#25552;&#21462;&#30340;&#23884;&#20837;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#26088;&#22312;&#25552;&#39640;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#30340;&#21457;&#23637;&#36895;&#24230;&#21644;&#25928;&#29575;&#65292;&#24182;&#20351;&#20854;&#33021;&#22815;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.11472</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#38899;&#21644;&#38899;&#39057;&#23884;&#20837;&#19982;&#24773;&#24863;&#35782;&#21035;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comparative Study of Pre-trained Speech and Audio Embeddings for Speech Emotion Recognition. (arXiv:2304.11472v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#26469;&#33258;&#20843;&#20010;&#35821;&#38899;&#21644;&#38899;&#39057;PTMs&#25552;&#21462;&#30340;&#23884;&#20837;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#26088;&#22312;&#25552;&#39640;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#30340;&#21457;&#23637;&#36895;&#24230;&#21644;&#25928;&#29575;&#65292;&#24182;&#20351;&#20854;&#33021;&#22815;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;PTMs&#65289;&#22312;&#35821;&#38899;&#21644;&#38899;&#39057;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#24040;&#22823;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;&#20174;&#36825;&#20123;&#27169;&#22411;&#20013;&#25552;&#21462;&#20986;&#30340;&#23884;&#20837;&#21487;&#20197;&#20316;&#20026;&#36755;&#20837;&#65292;&#29992;&#20110;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#26159;&#24773;&#24863;&#35782;&#21035;&#65292;&#23427;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#23545;&#39038;&#23458;&#21628;&#21483;&#30340;&#21160;&#24577;&#20998;&#26512;&#12289;&#24515;&#29702;&#20581;&#24247;&#35780;&#20272;&#21644;&#20010;&#24615;&#21270;&#35821;&#35328;&#23398;&#20064;&#31561;&#12290;PTM&#23884;&#20837;&#26377;&#21161;&#20110;&#25512;&#21160;&#24773;&#24863;&#35782;&#21035;&#30340;&#21457;&#23637;&#65292;&#20294;&#32570;&#20047;&#19968;&#20010;&#32771;&#34385;&#22810;&#20010;&#26041;&#38754;&#30340;&#32508;&#21512;&#27604;&#36739;&#65292;&#20363;&#22914;&#23884;&#20837;&#27169;&#22411;&#26550;&#26500;&#12289;&#29992;&#20110;&#39044;&#35757;&#32451;&#30340;&#25968;&#25454;&#20197;&#21450;&#39044;&#35757;&#32451;&#36807;&#31243;&#31561;&#12290;PTM&#23884;&#20837;&#30340;&#24443;&#24213;&#27604;&#36739;&#23558;&#26377;&#21161;&#20110;&#26356;&#24555;&#65292;&#26356;&#39640;&#25928;&#22320;&#24320;&#21457;&#27169;&#22411;&#65292;&#24182;&#20351;&#23427;&#20204;&#33021;&#22815;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;&#26412;&#25991;&#21033;&#29992;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#23545;&#26469;&#33258;&#20843;&#20010;&#35821;&#38899;&#21644;&#38899;&#39057;PTMs&#25552;&#21462;&#30340;&#23884;&#20837;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65288;&#21253;&#25324;wav2vec 2.0&#65292;data2vec&#65292;wavLM&#65292;UniSpeec&#65289;
&lt;/p&gt;
&lt;p&gt;
Pre-trained models (PTMs) have shown great promise in the speech and audio domain. Embeddings leveraged from these models serve as inputs for learning algorithms with applications in various downstream tasks. One such crucial task is Speech Emotion Recognition (SER) which has a wide range of applications, including dynamic analysis of customer calls, mental health assessment, and personalized language learning. PTM embeddings have helped advance SER, however, a comprehensive comparison of these PTM embeddings that consider multiple facets such as embedding model architecture, data used for pre-training, and the pre-training procedure being followed is missing. A thorough comparison of PTM embeddings will aid in the faster and more efficient development of models and enable their deployment in real-world scenarios. In this work, we exploit this research gap and perform a comparative analysis of embeddings extracted from eight speech and audio PTMs (wav2vec 2.0, data2vec, wavLM, UniSpeec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#29992;&#20110;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#30340;&#24555;&#36895;&#37319;&#26679;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#36739;&#23569;&#30340;&#27493;&#39588;&#21363;&#21487;&#20445;&#25345;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#21160;&#24577;&#35843;&#33410;&#38271;&#26102;&#38388;&#21453;&#21521;&#35823;&#24046;&#30340;&#38480;&#21046;&#21453;&#21521;&#35823;&#24046;&#35843;&#24230;&#65288;RBE&#35843;&#24230;&#65289;&#30340;&#24555;&#36895;&#37319;&#26679;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.11446</link><description>&lt;p&gt;
&#36890;&#36807;&#21453;&#21521;&#35823;&#24046;&#20998;&#26512;&#21152;&#36895;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Fast Diffusion Probabilistic Model Sampling through the lens of Backward Error Analysis. (arXiv:2304.11446v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#29992;&#20110;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#30340;&#24555;&#36895;&#37319;&#26679;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#36739;&#23569;&#30340;&#27493;&#39588;&#21363;&#21487;&#20445;&#25345;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#21160;&#24577;&#35843;&#33410;&#38271;&#26102;&#38388;&#21453;&#21521;&#35823;&#24046;&#30340;&#38480;&#21046;&#21453;&#21521;&#35823;&#24046;&#35843;&#24230;&#65288;RBE&#35843;&#24230;&#65289;&#30340;&#24555;&#36895;&#37319;&#26679;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#26159;&#19968;&#31867;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#36817;&#24180;&#26469;&#65292;DDPM&#22312;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#26679;&#26412;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;DDPM&#30340;&#19968;&#20010;&#37325;&#22823;&#38480;&#21046;&#26159;&#37319;&#26679;&#36895;&#24230;&#24930;&#12290;&#35201;&#29983;&#25104;&#19968;&#20010;&#26679;&#26412;&#65292;&#36890;&#24120;&#38656;&#35201;&#36827;&#34892;&#25968;&#30334;&#25110;&#25968;&#21315;&#27425;&#39034;&#24207;&#31070;&#32463;&#32593;&#32476;&#30340;&#20989;&#25968;&#35780;&#20272;&#27493;&#39588;&#12290;&#26412;&#25991;&#26088;&#22312;&#20026;DDPM&#24320;&#21457;&#19968;&#31181;&#24555;&#36895;&#37319;&#26679;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#36739;&#23569;&#30340;&#27493;&#39588;&#21363;&#21487;&#20445;&#25345;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;DDPM&#30340;&#25512;&#26029;&#36807;&#31243;&#36817;&#20284;&#20110;&#36830;&#32493;&#26497;&#38480;&#20013;&#27714;&#35299;&#30456;&#24212;&#30340;&#25193;&#25955;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;&#25193;&#25955;ODE&#65289;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#21453;&#21521;&#35823;&#24046;&#22914;&#20309;&#24433;&#21709;&#25193;&#25955;ODE&#21644;DDPM&#20013;&#30340;&#26679;&#26412;&#36136;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#21160;&#24577;&#35843;&#33410;&#38271;&#26102;&#38388;&#21453;&#21521;&#35823;&#24046;&#30340;\textbf{&#38480;&#21046;&#21453;&#21521;&#35823;&#24046;&#35843;&#24230;&#65288;RBE&#35843;&#24230;&#65289;}&#30340;&#24555;&#36895;&#37319;&#26679;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#22521;&#35757;&#21363;&#21487;&#21152;&#36895;DDPM&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising diffusion probabilistic models (DDPMs) are a class of powerful generative models. The past few years have witnessed the great success of DDPMs in generating high-fidelity samples. A significant limitation of the DDPMs is the slow sampling procedure. DDPMs generally need hundreds or thousands of sequential function evaluations (steps) of neural networks to generate a sample. This paper aims to develop a fast sampling method for DDPMs requiring much fewer steps while retaining high sample quality. The inference process of DDPMs approximates solving the corresponding diffusion ordinary differential equations (diffusion ODEs) in the continuous limit. This work analyzes how the backward error affects the diffusion ODEs and the sample quality in DDPMs. We propose fast sampling through the \textbf{Restricting Backward Error schedule (RBE schedule)} based on dynamically moderating the long-time backward error. Our method accelerates DDPMs without any further training. Our experiments
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#21363;&#20415;&#20351;&#29992;FedMD&#30340;&#23433;&#20840;&#26426;&#21046;&#65292;&#20173;&#23384;&#22312;&#34987;&#31934;&#24515;&#35774;&#35745;&#30340;&#24694;&#24847;&#25915;&#20987;&#21033;&#29992;&#30340;&#39118;&#38505;&#65292;&#22914;Paired-Logits&#21453;&#28436;&#25915;&#20987;&#65292;&#20250;&#23548;&#33268;&#38544;&#31169;&#25968;&#25454;&#26333;&#20809;&#12290;</title><link>http://arxiv.org/abs/2304.11436</link><description>&lt;p&gt;
&#36890;&#36807;Paired-Logits&#21453;&#28436;&#25915;&#20987;&#24674;&#22797;&#22270;&#20687;&#30340;FedMD
&lt;/p&gt;
&lt;p&gt;
Breaching FedMD: Image Recovery via Paired-Logits Inversion Attack. (arXiv:2304.11436v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#21363;&#20415;&#20351;&#29992;FedMD&#30340;&#23433;&#20840;&#26426;&#21046;&#65292;&#20173;&#23384;&#22312;&#34987;&#31934;&#24515;&#35774;&#35745;&#30340;&#24694;&#24847;&#25915;&#20987;&#21033;&#29992;&#30340;&#39118;&#38505;&#65292;&#22914;Paired-Logits&#21453;&#28436;&#25915;&#20987;&#65292;&#20250;&#23548;&#33268;&#38544;&#31169;&#25968;&#25454;&#26333;&#20809;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#19982;&#27169;&#22411;&#33976;&#39311;&#65288;FedMD&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#21327;&#20316;&#23398;&#20064;&#33539;&#24335;&#65292;&#20854;&#20013;&#20165;&#20256;&#36755;&#20844;&#20849;&#25968;&#25454;&#38598;&#30340;&#36755;&#20986;logits&#20316;&#20026;&#33976;&#39311;&#30693;&#35782;&#65292;&#32780;&#19981;&#26159;&#20256;&#36882;&#26131;&#21463;&#26799;&#24230;&#21453;&#28436;&#25915;&#20987;&#30340;&#31169;&#26377;&#27169;&#22411;&#21442;&#25968;&#65292;&#36825;&#26159;&#32852;&#37030;&#23398;&#20064;&#20013;&#24050;&#30693;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#21363;&#20351;&#20849;&#20139;&#20844;&#20849;&#25968;&#25454;&#38598;&#30340;&#36755;&#20986; logit&#27604;&#30452;&#25509;&#20849;&#20139;&#26799;&#24230;&#26356;&#23433;&#20840;&#65292;&#20173;&#23384;&#22312;&#22240;&#31934;&#24515;&#35774;&#35745;&#30340;&#24694;&#24847;&#25915;&#20987;&#23548;&#33268;&#30340;&#25968;&#25454;&#26333;&#20809;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24694;&#24847;&#26381;&#21153;&#22120;&#21487;&#20197;&#35757;&#32451;&#19968;&#20010;&#21453;&#28436;&#31070;&#32463;&#32593;&#32476;&#26469;&#21033;&#29992;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#27169;&#22411;&#20043;&#38388;&#30340;&#32622;&#20449;&#24230;&#24046;&#65292;&#38024;&#23545;FedMD&#21450;&#20854;&#21464;&#31181;&#36827;&#34892;PLI&#65288;&#37197;&#23545;logits&#21453;&#28436;&#65289;&#25915;&#20987;&#12290;&#22312;&#22810;&#20010;&#20154;&#33080;&#35782;&#21035;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#31867;&#20284;&#20110;FedMD&#30340;&#26041;&#26696;&#20013;&#65292;&#20165;&#20351;&#29992;&#20844;&#20849;&#25968;&#25454;&#38598;&#30340;&#37197;&#23545;&#26381;&#21153;&#22120;-&#23458;&#25143;&#31471;logits&#65292;&#24694;&#24847;&#26381;&#21153;&#22120;&#33021;&#22815;&#37325;&#26500;&#31169;&#26377;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning with Model Distillation (FedMD) is a nascent collaborative learning paradigm, where only output logits of public datasets are transmitted as distilled knowledge, instead of passing on private model parameters that are susceptible to gradient inversion attacks, a known privacy risk in federated learning. In this paper, we found that even though sharing output logits of public datasets is safer than directly sharing gradients, there still exists a substantial risk of data exposure caused by carefully designed malicious attacks. Our study shows that a malicious server can inject a PLI (Paired-Logits Inversion) attack against FedMD and its variants by training an inversion neural network that exploits the confidence gap between the server and client models. Experiments on multiple facial recognition datasets validate that under FedMD-like schemes, by using paired server-client logits of public datasets only, the malicious server is able to reconstruct private images on a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26465;&#20214;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#26465;&#20214;&#33258;&#22238;&#24402;&#30340;&#26041;&#24335;&#23558;&#20248;&#21270;&#21644;&#29983;&#25104;&#36807;&#31243;&#20998;&#35299;&#20026;&#26356;&#23481;&#26131;&#21644;&#21487;&#22788;&#29702;&#30340;&#27493;&#39588;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#27169;&#24335;&#65292;&#32467;&#21512;&#20132;&#21449;&#29109;&#25439;&#22833;&#21644;&#23545;&#25239;&#24615;&#25439;&#22833;&#31283;&#23450;&#35757;&#32451;&#36807;&#31243;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#39034;&#24207;&#25512;&#33616;&#26041;&#38754;&#20855;&#26377;&#36739;&#20248;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.11433</link><description>&lt;p&gt;
&#26465;&#20214;&#21435;&#22122;&#25193;&#25955;&#29992;&#20110;&#39034;&#24207;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Conditional Denoising Diffusion for Sequential Recommendation. (arXiv:2304.11433v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11433
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26465;&#20214;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#26465;&#20214;&#33258;&#22238;&#24402;&#30340;&#26041;&#24335;&#23558;&#20248;&#21270;&#21644;&#29983;&#25104;&#36807;&#31243;&#20998;&#35299;&#20026;&#26356;&#23481;&#26131;&#21644;&#21487;&#22788;&#29702;&#30340;&#27493;&#39588;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#27169;&#24335;&#65292;&#32467;&#21512;&#20132;&#21449;&#29109;&#25439;&#22833;&#21644;&#23545;&#25239;&#24615;&#25439;&#22833;&#31283;&#23450;&#35757;&#32451;&#36807;&#31243;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#39034;&#24207;&#25512;&#33616;&#26041;&#38754;&#20855;&#26377;&#36739;&#20248;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#33021;&#22815;&#23398;&#20064;&#20869;&#22312;&#30340;&#25968;&#25454;&#20998;&#24067;&#24182;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#65292;&#29983;&#25104;&#27169;&#22411;&#21463;&#21040;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20004;&#31181;&#20027;&#35201;&#30340;&#29983;&#25104;&#27169;&#22411;&#8212;&#8212;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#21644;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#22312;&#39034;&#24207;&#25512;&#33616;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#23384;&#22312;&#25361;&#25112;&#65292;GANs&#23384;&#22312;&#19981;&#31283;&#23450;&#30340;&#20248;&#21270;&#65292;&#32780;VAEs&#21017;&#23481;&#26131;&#21457;&#29983;&#21518;&#39564;&#23849;&#22604;&#21644;&#36807;&#24230;&#24179;&#28369;&#30340;&#29983;&#25104;&#12290;&#39034;&#24207;&#25512;&#33616;&#30340;&#31232;&#30095;&#21644;&#22024;&#26434;&#30340;&#29305;&#24615;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26465;&#20214;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#21253;&#25324;&#24207;&#21015;&#32534;&#30721;&#22120;&#65292;&#20132;&#21449;&#27880;&#24847;&#21435;&#22122;&#35299;&#30721;&#22120;&#21644;&#36880;&#27493;&#25193;&#25955;&#22120;&#12290;&#36825;&#31181;&#26041;&#27861;&#20197;&#26465;&#20214;&#33258;&#22238;&#24402;&#30340;&#26041;&#24335;&#23558;&#20248;&#21270;&#21644;&#29983;&#25104;&#36807;&#31243;&#20998;&#35299;&#20026;&#26356;&#23481;&#26131;&#21644;&#21487;&#22788;&#29702;&#30340;&#27493;&#39588;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#27169;&#24335;&#65292;&#32467;&#21512;&#20132;&#21449;&#29109;&#25439;&#22833;&#21644;&#23545;&#25239;&#24615;&#25439;&#22833;&#31283;&#23450;&#35757;&#32451;&#36807;&#31243;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#39034;&#24207;&#25512;&#33616;&#26041;&#38754;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#26080;&#35770;&#26159;&#22312;&#23450;&#37327;&#25351;&#26631;&#19978;&#36824;&#26159;&#22312;&#23450;&#24615;&#25351;&#26631;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models have attracted significant interest due to their ability to handle uncertainty by learning the inherent data distributions. However, two prominent generative models, namely Generative Adversarial Networks (GANs) and Variational AutoEncoders (VAEs), exhibit challenges that impede achieving optimal performance in sequential recommendation tasks. Specifically, GANs suffer from unstable optimization, while VAEs are prone to posterior collapse and over-smoothed generations. The sparse and noisy nature of sequential recommendation further exacerbates these issues. In response to these limitations, we present a conditional denoising diffusion model, which includes a sequence encoder, a cross-attentive denoising decoder, and a step-wise diffuser. This approach streamlines the optimization and generation process by dividing it into easier and tractable steps in a conditional autoregressive manner. Furthermore, we introduce a novel optimization schema that incorporates both cro
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#22810;&#35270;&#35282;&#21095;&#36879;&#26816;&#27979;&#26694;&#26550;&#65292;MVSD&#65292;&#35813;&#26694;&#26550;&#23558;&#30005;&#24433;&#30693;&#35782;&#21644;&#29992;&#25143;&#27963;&#21160;&#32435;&#20837;&#32771;&#34385;&#65292;&#24182;&#19988;&#22312;LCS&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#20248;&#20110;&#24378;&#22522;&#32447;&#30340;&#30456;&#20851;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.11411</link><description>&lt;p&gt;
&#29992;&#30005;&#24433;&#30693;&#35782;&#21644;&#29992;&#25143;&#32593;&#32476;&#26816;&#27979;&#24433;&#35780;&#20013;&#30340;&#21095;&#36879;
&lt;/p&gt;
&lt;p&gt;
Detecting Spoilers in Movie Reviews with External Movie Knowledge and User Networks. (arXiv:2304.11411v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11411
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#22810;&#35270;&#35282;&#21095;&#36879;&#26816;&#27979;&#26694;&#26550;&#65292;MVSD&#65292;&#35813;&#26694;&#26550;&#23558;&#30005;&#24433;&#30693;&#35782;&#21644;&#29992;&#25143;&#27963;&#21160;&#32435;&#20837;&#32771;&#34385;&#65292;&#24182;&#19988;&#22312;LCS&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#20248;&#20110;&#24378;&#22522;&#32447;&#30340;&#30456;&#20851;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#30005;&#24433;&#35780;&#35770;&#24179;&#21488;&#20026;&#30005;&#24433;&#20135;&#19994;&#21644;&#20844;&#20247;&#25552;&#20379;&#20247;&#21253;&#21453;&#39304;&#65292;&#20294;&#21095;&#36879;&#35780;&#35770;&#20005;&#37325;&#25439;&#23475;&#20102;&#29992;&#25143;&#20307;&#39564;&#12290;&#23613;&#31649;&#24050;&#32463;&#36827;&#34892;&#20102;&#21021;&#27493;&#30340;&#30740;&#31350;&#20197;&#33258;&#21160;&#35782;&#21035;&#21095;&#36879;&#65292;&#20294;&#20854;&#20165;&#20851;&#27880;&#20110;&#35780;&#35770;&#20869;&#23481;&#26412;&#36523;&#65292;&#32780;&#24378;&#22823;&#30340;&#21095;&#36879;&#26816;&#27979;&#38656;&#35201;&#23558;&#35780;&#35770;&#25918;&#20837;&#20851;&#20110;&#30005;&#24433;&#30340;&#20107;&#23454;&#21644;&#30693;&#35782;&#12289;&#29992;&#25143;&#22312;&#30005;&#24433;&#35780;&#35770;&#24179;&#21488;&#19978;&#30340;&#34892;&#20026;&#31561;&#19978;&#19979;&#25991;&#20013;&#12290;&#22240;&#27492;&#65292;&#22312;&#25105;&#20204;&#39318;&#20808;&#25972;&#29702;&#20102;&#19968;&#20010;&#22522;&#20110;&#32593;&#32476;&#30340;&#21095;&#36879;&#26816;&#27979;&#25968;&#25454;&#38598;LCS&#21644;&#19968;&#20010;&#20840;&#38754;&#30340;&#26368;&#26032;&#30005;&#24433;&#30693;&#35782;&#24211;UKM&#20043;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MVSD&#65292;&#19968;&#31181;&#32771;&#34385;&#21040;&#26377;&#20851;&#30005;&#24433;&#21644;&#29992;&#25143;&#27963;&#21160;&#30340;&#22806;&#37096;&#30693;&#35782;&#30340;&#26032;&#22411;&#22810;&#35270;&#35282;&#21095;&#36879;&#26816;&#27979;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MVSD&#26500;&#24314;&#19977;&#20010;&#20114;&#32852;&#30340;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#26469;&#27169;&#25311;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#26469;&#28304;&#21450;&#20854;&#22810;&#35270;&#22270;&#23646;&#24615;&#65292;&#32780;&#25105;&#20204;&#35774;&#35745;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#24322;&#26500;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;HGCN&#65289;&#26469;&#34701;&#21512;&#22810;&#35270;&#22270;&#23884;&#20837;&#24182;&#39044;&#27979;&#21095;&#36879;&#12290;&#25105;&#20204;&#22312;LCS&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102;MVSD&#22312;&#20934;&#30830;&#24615;&#21644;F1&#24471;&#20998;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#24378;&#22522;&#32447;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;&#19968;&#20010;&#27969;&#34892;&#30340;&#30005;&#24433;&#35780;&#35770;&#32593;&#31449;&#19978;&#23637;&#31034;&#20102;MVSD&#22312;&#30495;&#23454;&#21095;&#36879;&#35782;&#21035;&#20219;&#21153;&#19978;&#30340;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online movie review platforms are providing crowdsourced feedback for the film industry and the general public, while spoiler reviews greatly compromise user experience. Although preliminary research efforts were made to automatically identify spoilers, they merely focus on the review content itself, while robust spoiler detection requires putting the review into the context of facts and knowledge regarding movies, user behavior on film review platforms, and more. In light of these challenges, we first curate a large-scale network-based spoiler detection dataset LCS and a comprehensive and up-to-date movie knowledge base UKM. We then propose MVSD, a novel Multi-View Spoiler Detection framework that takes into account the external knowledge about movies and user activities on movie review platforms. Specifically, MVSD constructs three interconnecting heterogeneous information networks to model diverse data sources and their multi-view attributes, while we design and employ a novel heter
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#36755;&#20837;&#34920;&#31034;&#26041;&#27861;&#65292;&#20998;&#21035;&#26159;&#21333;&#20010;&#26080;&#32447;&#30005;&#36335;&#24452;&#29305;&#24449;&#12289;&#26080;&#32447;&#30005;&#38142;&#36335;&#29305;&#24449;&#21644;&#22522;&#20110;&#22270;&#20687;&#30340;&#34920;&#31034;&#65292;&#35774;&#35745;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;NLOS&#23450;&#20301;&#65292;&#25903;&#25345;&#26356;&#20016;&#23500;&#30340;&#39044;&#27979;&#36755;&#20986;&#21644;&#19981;&#21487;&#38752;&#39044;&#27979;&#30340;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2304.11396</link><description>&lt;p&gt;
&#26080;&#32447;NLOS&#23450;&#20301;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65306;&#36755;&#20837;&#34920;&#31034;&#19982;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
ML-based Approaches for Wireless NLOS Localization: Input Representations and Uncertainty Estimation. (arXiv:2304.11396v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#36755;&#20837;&#34920;&#31034;&#26041;&#27861;&#65292;&#20998;&#21035;&#26159;&#21333;&#20010;&#26080;&#32447;&#30005;&#36335;&#24452;&#29305;&#24449;&#12289;&#26080;&#32447;&#30005;&#38142;&#36335;&#29305;&#24449;&#21644;&#22522;&#20110;&#22270;&#20687;&#30340;&#34920;&#31034;&#65292;&#35774;&#35745;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;NLOS&#23450;&#20301;&#65292;&#25903;&#25345;&#26356;&#20016;&#23500;&#30340;&#39044;&#27979;&#36755;&#20986;&#21644;&#19981;&#21487;&#38752;&#39044;&#27979;&#30340;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#30452;&#35270;&#36335;&#24452;&#65288;NLOS&#65289;&#23450;&#20301;&#26159;&#35768;&#22810;&#26080;&#32447;&#32593;&#32476;&#24212;&#29992;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#32570;&#20047;&#21487;&#29992;&#25968;&#25454;&#38598;&#20351;&#24471;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;NLOS&#23450;&#20301;&#38382;&#39064;&#21464;&#24471;&#22256;&#38590;&#65292;&#20294;&#26159;&#36817;&#26399;&#21512;&#25104;&#25968;&#25454;&#38598;&#29983;&#25104;&#30340;&#26032;&#21457;&#23637;&#20026;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#26426;&#36935;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#36755;&#20837;&#34920;&#31034;&#26041;&#27861;&#65306;&#65288;i&#65289;&#21333;&#20010;&#26080;&#32447;&#30005;&#36335;&#24452;&#29305;&#24449;&#65292;&#65288;ii&#65289;&#26080;&#32447;&#30005;&#38142;&#36335;&#29305;&#24449;&#65288;&#22810;&#24452;&#65289;&#65292;&#21644;&#65288;iii&#65289;&#22522;&#20110;&#22270;&#20687;&#30340;&#34920;&#31034;&#12290;&#22312;&#21518;&#20004;&#31181;&#26032;&#30340;&#34920;&#31034;&#26041;&#27861;&#30340;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#65292;&#24182;&#19988;&#23613;&#31649;&#27809;&#26377;&#26174;&#33879;&#25552;&#39640;NLOS&#23450;&#20301;&#24615;&#33021;&#65292;&#20182;&#20204;&#33021;&#22815;&#25903;&#25345;&#26356;&#20016;&#23500;&#30340;&#39044;&#27979;&#36755;&#20986;&#65292;&#20174;&#32780;&#20351;&#39044;&#27979;&#32467;&#26524;&#28145;&#20837;&#20998;&#26512;&#12290;&#29305;&#21035;&#26159;&#65292;&#26356;&#20016;&#23500;&#30340;&#36755;&#20986;&#33021;&#22815;&#21487;&#38752;&#22320;&#35782;&#21035;&#20986;&#19981;&#21487;&#38752;&#30340;&#39044;&#27979;&#65292;&#24182;&#25903;&#25345;&#32473;&#23450;&#23454;&#20363;&#30340;&#21069;K&#20010;&#20505;&#36873;&#20301;&#32622;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#36824;&#27979;&#37327;&#20102;&#19981;&#21516;&#34920;&#31034;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#23637;&#31034;&#20102;&#22522;&#20110;&#22270;&#20687;&#30340;&#34920;&#31034;&#26041;&#24335;&#33021;&#22815;&#33719;&#24471;&#26356;&#22909;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
The challenging problem of non-line-of-sight (NLOS) localization is critical for many wireless networking applications. The lack of available datasets has made NLOS localization difficult to tackle with ML-driven methods, but recent developments in synthetic dataset generation have provided new opportunities for research. This paper explores three different input representations: (i) single wireless radio path features, (ii) wireless radio link features (multi-path), and (iii) image-based representations. Inspired by the two latter new representations, we design two convolutional neural networks (CNNs) and we demonstrate that, although not significantly improving the NLOS localization performance, they are able to support richer prediction outputs, thus allowing deeper analysis of the predictions. In particular, the richer outputs enable reliable identification of non-trustworthy predictions and support the prediction of the top-K candidate locations for a given instance. We also measu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;3D&#21040;BEV&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#20351;BEV&#20998;&#21106;&#27169;&#22411;&#20855;&#26377;&#26356;&#20016;&#23500;&#30340;&#32467;&#26500;&#21644;&#20960;&#20309;&#20449;&#24687;&#65292;&#20197;&#25552;&#39640;&#20998;&#21106;&#30340;&#20934;&#30830;&#24615;&#21644;&#25512;&#29702;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.11393</link><description>&lt;p&gt;
&#20174;3D&#21040;&#40479;&#30640;&#22270;&#30340;&#30693;&#35782;&#33976;&#39311;&#29992;&#20110;LiDAR&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distillation from 3D to Bird's-Eye-View for LiDAR Semantic Segmentation. (arXiv:2304.11393v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11393
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;3D&#21040;BEV&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#20351;BEV&#20998;&#21106;&#27169;&#22411;&#20855;&#26377;&#26356;&#20016;&#23500;&#30340;&#32467;&#26500;&#21644;&#20960;&#20309;&#20449;&#24687;&#65292;&#20197;&#25552;&#39640;&#20998;&#21106;&#30340;&#20934;&#30830;&#24615;&#21644;&#25512;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LiDAR&#28857;&#20113;&#20998;&#21106;&#26159;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#29702;&#35299;&#26368;&#22522;&#26412;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#27169;&#22411;&#24456;&#38590;&#21516;&#26102;&#23454;&#29616;&#39640;&#25512;&#29702;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;3D&#21040;BEV&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#23558;&#26469;&#33258;3D&#20307;&#32032;&#27169;&#22411;&#30340;&#20016;&#23500;&#30693;&#35782;&#36716;&#31227;&#21040;BEV&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20027;&#35201;&#21253;&#25324;&#20004;&#20010;&#27169;&#22359;&#65306;&#20307;&#32032;&#21040;&#26609;&#29366;&#20307;&#33976;&#39311;&#27169;&#22359;&#21644;&#26631;&#31614;&#26435;&#37325;&#33976;&#39311;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
LiDAR point cloud segmentation is one of the most fundamental tasks for autonomous driving scene understanding. However, it is difficult for existing models to achieve both high inference speed and accuracy simultaneously. For example, voxel-based methods perform well in accuracy, while Bird's-Eye-View (BEV)-based methods can achieve real-time inference. To overcome this issue, we develop an effective 3D-to-BEV knowledge distillation method that transfers rich knowledge from 3D voxel-based models to BEV-based models. Our framework mainly consists of two modules: the voxel-to-pillar distillation module and the label-weight distillation module. Voxel-to-pillar distillation distills sparse 3D features to BEV features for middle layers to make the BEV-based model aware of more structural and geometric information. Label-weight distillation helps the model pay more attention to regions with more height information. Finally, we conduct experiments on the SemanticKITTI dataset and Paris-Lille
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#27010;&#29575;&#36923;&#36753;&#25512;&#29702;&#30340;&#24207;&#21015;&#25512;&#33616;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#31070;&#32463;-&#31526;&#21495;SR&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#29305;&#24449;&#23884;&#20837;&#21644;&#36923;&#36753;&#23884;&#20837;&#20998;&#31163;&#65292;SR-PLR&#32467;&#21512;&#30456;&#20284;&#24615;&#21305;&#37197;&#21644;&#36923;&#36753;&#25512;&#29702;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#29992;&#25143;&#21475;&#21619;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#28436;&#21464;&#12290;</title><link>http://arxiv.org/abs/2304.11383</link><description>&lt;p&gt;
&#24102;&#26377;&#27010;&#29575;&#36923;&#36753;&#25512;&#29702;&#30340;&#24207;&#21015;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Sequential Recommendation with Probabilistic Logical Reasoning. (arXiv:2304.11383v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11383
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#27010;&#29575;&#36923;&#36753;&#25512;&#29702;&#30340;&#24207;&#21015;&#25512;&#33616;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#31070;&#32463;-&#31526;&#21495;SR&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#29305;&#24449;&#23884;&#20837;&#21644;&#36923;&#36753;&#23884;&#20837;&#20998;&#31163;&#65292;SR-PLR&#32467;&#21512;&#30456;&#20284;&#24615;&#21305;&#37197;&#21644;&#36923;&#36753;&#25512;&#29702;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#29992;&#25143;&#21475;&#21619;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#28436;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#21644;&#31526;&#21495;&#23398;&#20064;&#26159;&#24207;&#21015;&#25512;&#33616;&#65288;SR&#65289;&#20013;&#32463;&#24120;&#20351;&#29992;&#30340;&#20004;&#31181;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#31070;&#32463;-&#31526;&#21495;SR&#27169;&#22411;&#23637;&#31034;&#20102;&#23427;&#20204;&#20351;SR&#20855;&#22791;&#24182;&#21457;&#24863;&#30693;&#21644;&#35748;&#30693;&#33021;&#21147;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#31070;&#32463;-&#31526;&#21495;SR&#20173;&#28982;&#23384;&#22312;&#38382;&#39064;&#65292;&#22914;&#22312;&#36923;&#36753;&#25512;&#29702;&#20013;&#34920;&#31034;&#29992;&#25143;&#21644;&#29289;&#21697;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;Deep Neural Network&#65288;DNN&#65289;SR&#27169;&#22411;&#19982;&#36923;&#36753;&#25512;&#29702;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#31216;&#20026;&#24102;&#26377;&#27010;&#29575;&#36923;&#36753;&#25512;&#29702;&#30340;&#24207;&#21015;&#25512;&#33616;&#65288;&#31616;&#31216;SR-PLR&#65289;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#22312;DNN&#21644;&#27010;&#29575;&#36923;&#36753;&#32593;&#32476;&#20013;&#35299;&#24320;&#29305;&#24449;&#23884;&#20837;&#21644;&#36923;&#36753;&#23884;&#20837;&#65292;&#20801;&#35768;SR-PLR&#20174;&#30456;&#20284;&#24615;&#21305;&#37197;&#21644;&#36923;&#36753;&#25512;&#29702;&#20013;&#21463;&#30410;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#25429;&#25417;&#29992;&#25143;&#21475;&#21619;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#28436;&#21464;&#65292;SR-PLR&#20351;&#29992;&#27010;&#29575;&#26041;&#27861;&#23884;&#20837;&#29992;&#25143;&#21644;&#29289;&#21697;&#65292;&#24182;&#23545;&#29992;&#25143;&#30340;&#20132;&#20114;&#27169;&#24335;&#36827;&#34892;&#27010;&#29575;&#36923;&#36753;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning and symbolic learning are two frequently employed methods in Sequential Recommendation (SR). Recent neural-symbolic SR models demonstrate their potential to enable SR to be equipped with concurrent perception and cognition capacities. However, neural-symbolic SR remains a challenging problem due to open issues like representing users and items in logical reasoning. In this paper, we combine the Deep Neural Network (DNN) SR models with logical reasoning and propose a general framework named Sequential Recommendation with Probabilistic Logical Reasoning (short for SR-PLR). This framework allows SR-PLR to benefit from both similarity matching and logical reasoning by disentangling feature embedding and logic embedding in the DNN and probabilistic logic network. To better capture the uncertainty and evolution of user tastes, SR-PLR embeds users and items with a probabilistic method and conducts probabilistic logical reasoning on users' interaction patterns. Then the feature a
&lt;/p&gt;</description></item><item><title>SimplyMime&#26159;&#19968;&#31181;&#26088;&#22312;&#28040;&#38500;&#28040;&#36153;&#30005;&#23376;&#20135;&#21697;&#38656;&#27714;&#22810;&#20010;&#36965;&#25511;&#22120;&#30340;&#31995;&#32479;&#65292;&#37319;&#29992;&#20102;&#21160;&#24577;&#25163;&#21183;&#35782;&#21035;&#20307;&#31995;&#32467;&#26500;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#30452;&#35266;&#30340;&#25511;&#21046;&#26041;&#24335;&#65292;&#21516;&#26102;&#36824;&#20855;&#26377;&#23433;&#20840;&#24615;&#26041;&#38754;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.11377</link><description>&lt;p&gt;
SimplyMime&#65306;&#35302;&#25163;&#21487;&#21450;&#30340;&#25511;&#21046;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
SimplyMime: A Control at Our Fingertips. (arXiv:2304.11377v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11377
&lt;/p&gt;
&lt;p&gt;
SimplyMime&#26159;&#19968;&#31181;&#26088;&#22312;&#28040;&#38500;&#28040;&#36153;&#30005;&#23376;&#20135;&#21697;&#38656;&#27714;&#22810;&#20010;&#36965;&#25511;&#22120;&#30340;&#31995;&#32479;&#65292;&#37319;&#29992;&#20102;&#21160;&#24577;&#25163;&#21183;&#35782;&#21035;&#20307;&#31995;&#32467;&#26500;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#30452;&#35266;&#30340;&#25511;&#21046;&#26041;&#24335;&#65292;&#21516;&#26102;&#36824;&#20855;&#26377;&#23433;&#20840;&#24615;&#26041;&#38754;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25216;&#26415;&#30340;&#19981;&#26029;&#28436;&#36827;&#65292;&#28040;&#36153;&#30005;&#23376;&#20135;&#21697;&#65288;&#22914;&#30005;&#35270;&#12289;&#26426;&#39030;&#30418;&#12289;&#23478;&#24237;&#24433;&#38498;&#21644;&#31354;&#35843;&#65289;&#30340;&#20351;&#29992;&#22312;&#29616;&#20195;&#31038;&#20250;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#12290;&#38543;&#30528;&#27599;&#24180;&#26032;&#35774;&#22791;&#30340;&#36827;&#20837;&#65292;&#25805;&#20316;&#23427;&#20204;&#38656;&#35201;&#22810;&#20010;&#32418;&#22806;&#32447;&#36965;&#25511;&#22120;&#30340;&#32047;&#31215;&#19981;&#20165;&#28010;&#36153;&#33021;&#28304;&#21644;&#36164;&#28304;&#65292;&#32780;&#19988;&#20026;&#29992;&#25143;&#21019;&#36896;&#20102;&#32321;&#29712;&#21644;&#28151;&#20081;&#30340;&#29615;&#22659;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SimplyMime&#30340;&#26032;&#22411;&#31995;&#32479;&#65292;&#26088;&#22312;&#28040;&#38500;&#28040;&#36153;&#30005;&#23376;&#20135;&#21697;&#38656;&#35201;&#22810;&#20010;&#36965;&#25511;&#22120;&#30340;&#38656;&#27714;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#30452;&#35266;&#30340;&#25511;&#21046;&#26041;&#24335;&#65292;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35774;&#22791;&#12290;SimplyMime&#21033;&#29992;&#20102;&#19968;&#31181;&#21160;&#24577;&#25163;&#21183;&#35782;&#21035;&#20307;&#31995;&#32467;&#26500;&#65292;&#34701;&#21512;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#26426;&#20132;&#20114;&#65292;&#21019;&#24314;&#20986;&#19968;&#31181;&#20808;&#36827;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#36731;&#26494;&#22320;&#19982;&#32477;&#22823;&#22810;&#25968;&#28040;&#36153;&#30005;&#23376;&#20135;&#21697;&#36827;&#34892;&#20132;&#20114;&#12290;&#27492;&#22806;&#65292;SimplyMime&#36824;&#20855;&#26377;&#23433;&#20840;&#24615;&#26041;&#38754;&#30340;&#21151;&#33021;&#65292;&#21487;&#20197;&#39564;&#35777;&#21644;&#35748;&#35777;&#35774;&#22791;&#12290;
&lt;/p&gt;
&lt;p&gt;
The utilization of consumer electronics, such as televisions, set-top boxes, home theaters, and air conditioners, has become increasingly prevalent in modern society as technology continues to evolve. As new devices enter our homes each year, the accumulation of multiple infrared remote controls to operate them not only results in a waste of energy and resources, but also creates a cumbersome and cluttered environment for the user. This paper presents a novel system, named SimplyMime, which aims to eliminate the need for multiple remote controls for consumer electronics and provide the user with intuitive control without the need for additional devices. SimplyMime leverages a dynamic hand gesture recognition architecture, incorporating Artificial Intelligence and Human-Computer Interaction, to create a sophisticated system that enables users to interact with a vast majority of consumer electronics with ease. Additionally, SimplyMime has a security aspect where it can verify and authent
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#39033;&#30446;&#21644;&#31454;&#36187;&#30340;&#26412;&#31185;&#35838;&#31243;&#65292;&#26088;&#22312;&#20026;&#22823;&#23398;&#29983;&#25552;&#20379;&#25628;&#32034;&#26041;&#27861;&#22312;&#26827;&#30424;&#28216;&#25103;&#20013;&#30340;&#24212;&#29992;&#12290;&#23398;&#29983;&#36890;&#36807;&#26500;&#24314;AI&#20195;&#29702;&#26469;&#21442;&#21152;&#40657;&#30333;&#26827;&#38182;&#26631;&#36187;&#65292;&#26368;&#32456;&#35780;&#20272;&#20182;&#20204;&#30340;&#39033;&#30446;&#36136;&#37327;&#21644;&#27604;&#36187;&#34920;&#29616;&#12290;&#35813;&#35838;&#31243;&#20197;&#31454;&#20105;&#24335;&#23398;&#20064;&#30340;&#24418;&#24335;&#23454;&#29616;&#28216;&#25103;&#21270;&#65292;&#28608;&#21457;&#23398;&#29983;&#21442;&#19982;&#23398;&#20064;&#65292;&#26377;&#21161;&#20110;&#25171;&#19979;AI&#21644;&#31639;&#27861;&#23398;&#31185;&#30340;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2304.11376</link><description>&lt;p&gt;
&#36890;&#36807;AI&#26827;&#30424;&#28216;&#25103;&#38182;&#26631;&#36187;&#28608;&#21457;&#23398;&#29983;&#21442;&#19982;&#24230;
&lt;/p&gt;
&lt;p&gt;
Stimulating student engagement with an AI board game tournament. (arXiv:2304.11376v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11376
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#39033;&#30446;&#21644;&#31454;&#36187;&#30340;&#26412;&#31185;&#35838;&#31243;&#65292;&#26088;&#22312;&#20026;&#22823;&#23398;&#29983;&#25552;&#20379;&#25628;&#32034;&#26041;&#27861;&#22312;&#26827;&#30424;&#28216;&#25103;&#20013;&#30340;&#24212;&#29992;&#12290;&#23398;&#29983;&#36890;&#36807;&#26500;&#24314;AI&#20195;&#29702;&#26469;&#21442;&#21152;&#40657;&#30333;&#26827;&#38182;&#26631;&#36187;&#65292;&#26368;&#32456;&#35780;&#20272;&#20182;&#20204;&#30340;&#39033;&#30446;&#36136;&#37327;&#21644;&#27604;&#36187;&#34920;&#29616;&#12290;&#35813;&#35838;&#31243;&#20197;&#31454;&#20105;&#24335;&#23398;&#20064;&#30340;&#24418;&#24335;&#23454;&#29616;&#28216;&#25103;&#21270;&#65292;&#28608;&#21457;&#23398;&#29983;&#21442;&#19982;&#23398;&#20064;&#65292;&#26377;&#21161;&#20110;&#25171;&#19979;AI&#21644;&#31639;&#27861;&#23398;&#31185;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#26412;&#30340;AI&#25216;&#26415;&#26159;&#29702;&#35299;&#26356;&#39640;&#32423;&#27010;&#24565;&#30340;&#20851;&#38190;&#12290;&#25105;&#20204;&#35748;&#20026;&#22312;&#39640;&#31561;&#25945;&#32946;&#26089;&#26399;&#24341;&#20837;&#25628;&#32034;&#26041;&#27861;&#31561;AI&#25216;&#26415;&#26377;&#21161;&#20110;&#21019;&#36896;&#26356;&#28145;&#21051;&#30340;&#29702;&#35299;&#65292;&#20026;&#26356;&#39640;&#32423;&#21035;&#30340;AI&#21644;&#31639;&#27861;&#35838;&#31243;&#25171;&#19979;&#22522;&#30784;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#22522;&#20110;&#39033;&#30446;&#21644;&#31454;&#36187;&#30340;&#26412;&#31185;&#35838;&#31243;&#65292;&#20026;&#22823;&#20108;&#23398;&#29983;&#25552;&#20379;&#25628;&#32034;&#26041;&#27861;&#24212;&#29992;&#20110;&#26827;&#30424;&#28216;&#25103;&#30340;&#31616;&#20171;&#12290;&#22312;&#20004;&#20154;&#19968;&#32452;&#30340;&#24773;&#20917;&#19979;&#65292;&#23398;&#29983;&#24517;&#39035;&#20351;&#29992;&#32593;&#32476;&#32534;&#31243;&#21644;AI&#26041;&#27861;&#26469;&#26500;&#24314;AI&#20195;&#29702;&#20197;&#21442;&#21152;&#26827;&#30424;&#28216;&#25103;&#38182;&#26631;&#36187;&#65292;&#20170;&#24180;&#30340;&#28216;&#25103;&#26159;&#40657;&#30333;&#26827;&#12290;&#23398;&#29983;&#23558;&#26681;&#25454;&#39033;&#30446;&#30340;&#36136;&#37327;&#21644;&#22312;&#26368;&#32456;&#38182;&#26631;&#36187;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#24341;&#20837;&#31454;&#20105;&#24335;&#23398;&#20064;&#30340;&#28216;&#25103;&#21270;&#24418;&#24335;&#21487;&#20197;&#20026;&#23398;&#29983;&#25552;&#20379;&#26356;&#22909;&#30340;&#23398;&#20064;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Strong foundations in basic AI techniques are key to understanding more advanced concepts. We believe that introducing AI techniques, such as search methods, early in higher education helps create a deeper understanding of the concepts seen later in more advanced AI and algorithms courses. We present a project-based and competition-based bachelor course that gives second-year students an introduction to search methods applied to board games. In groups of two, students have to use network programming and AI methods to build an AI agent to compete in a board game tournament-othello was this year's game. Students are evaluated based on the quality of their projects and on their performance during the final tournament. We believe that the introduction of gamification, in the form of competition-based learning, allows for a better learning experience for the students.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32858;&#28966;&#20110;2020&#24180;&#32654;&#22269;&#24635;&#32479;&#36873;&#20030;&#65292;&#36890;&#36807;&#26500;&#24314;&#29992;&#25143;-&#25512;&#25991;&#21452;&#20998;&#22270;&#24182;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#20197;&#26816;&#27979; Twitter &#20013;&#30340;&#25919;&#27835;&#35266;&#28857;&#12290;&#36890;&#36807;&#24341;&#20837; Skip Aggregation &#26426;&#21046;&#65292;&#26377;&#25928;&#21033;&#29992;&#29992;&#25143;&#34892;&#20026;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.11367</link><description>&lt;p&gt;
&#36890;&#36807;&#21452;&#20998;&#22270;&#20998;&#26512;&#26816;&#27979; Twitter &#20013;&#30340;&#25919;&#27835;&#35266;&#28857;&#65306;&#19968;&#31181;&#22522;&#20110; Skip Aggregation &#22270;&#21367;&#31215;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting Political Opinions in Tweets through Bipartite Graph Analysis: A Skip Aggregation Graph Convolution Approach. (arXiv:2304.11367v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32858;&#28966;&#20110;2020&#24180;&#32654;&#22269;&#24635;&#32479;&#36873;&#20030;&#65292;&#36890;&#36807;&#26500;&#24314;&#29992;&#25143;-&#25512;&#25991;&#21452;&#20998;&#22270;&#24182;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#20197;&#26816;&#27979; Twitter &#20013;&#30340;&#25919;&#27835;&#35266;&#28857;&#12290;&#36890;&#36807;&#24341;&#20837; Skip Aggregation &#26426;&#21046;&#65292;&#26377;&#25928;&#21033;&#29992;&#29992;&#25143;&#34892;&#20026;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#20247;&#33286;&#35770;&#26159;&#24433;&#21709;&#25919;&#27835;&#20915;&#31574;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#22914;&#20170;&#65292;&#31038;&#20132;&#23186;&#20307;&#25104;&#20026;&#20010;&#20154;&#21442;&#19982;&#25919;&#27835;&#35752;&#35770;&#21644;&#34920;&#36798;&#25919;&#27835;&#35266;&#28857;&#30340;&#37325;&#35201;&#24179;&#21488;&#65292;&#20026;&#30740;&#31350;&#20844;&#20247;&#33286;&#35770;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#36164;&#28304;&#12290;&#26412;&#25991;&#32858;&#28966;&#20110;2020&#24180;&#32654;&#22269;&#24635;&#32479;&#36873;&#20030;&#65292;&#24182;&#20174; Twitter &#20013;&#21019;&#24314;&#20102;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#20110;&#29992;&#25143;&#30340;&#21457;&#24067;&#21644;&#36716;&#25512;&#34892;&#20026;&#30340;&#29992;&#25143;-&#25512;&#25991;&#21452;&#20998;&#22270;&#65292;&#24182;&#23558;&#20219;&#21153;&#36716;&#25442;&#20026;&#19968;&#20010;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#65292;&#20197;&#26816;&#27979;&#25512;&#25991;&#20013;&#30340;&#25919;&#27835;&#35266;&#28857;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340; Skip Aggregation &#26426;&#21046;&#65292;&#20351;&#25512;&#25991;&#33410;&#28857;&#20174;&#20108;&#38454;&#37051;&#23621;&#65288;&#20063;&#26159;&#25512;&#25991;&#33410;&#28857;&#65289;&#32858;&#21512;&#20449;&#24687;&#65292;&#26377;&#25928;&#21033;&#29992;&#29992;&#25143;&#34892;&#20026;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#26174;&#33879;&#20248;&#20110;&#20960;&#31181;&#31454;&#20105;&#24615;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
Public opinion is a crucial factor in shaping political decision-making. Nowadays, social media has become an essential platform for individuals to engage in political discussions and express their political views, presenting researchers with an invaluable resource for analyzing public opinion. In this paper, we focus on the 2020 US presidential election and create a large-scale dataset from Twitter. To detect political opinions in tweets, we build a user-tweet bipartite graph based on users' posting and retweeting behaviors and convert the task into a Graph Neural Network (GNN)-based node classification problem. Then, we introduce a novel skip aggregation mechanism that makes tweet nodes aggregate information from second-order neighbors, which are also tweet nodes due to the graph's bipartite nature, effectively leveraging user behavioral information. The experimental results show that our proposed model significantly outperforms several competitive baselines. Further analyses demonst
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30495;&#23454;&#20154;&#33080;&#33258;&#25200;&#21160;&#29983;&#25104;&#20266;&#23545;&#25239;&#24615;&#20154;&#33080;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#35757;&#32451;&#30340;&#23545;&#25239;&#24615;&#20154;&#33080;&#26816;&#27979;&#22120;&#19981;&#38656;&#25915;&#20987;&#25968;&#25454;&#21363;&#21487;&#26816;&#27979;&#26032;&#22411;&#26410;&#30693;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2304.11359</link><description>&lt;p&gt;
&#20165;&#20351;&#29992;&#30495;&#23454;&#20154;&#33080;&#33258;&#25200;&#21160;&#26816;&#27979;&#23545;&#25239;&#24615;&#20154;&#33080;
&lt;/p&gt;
&lt;p&gt;
Detecting Adversarial Faces Using Only Real Face Self-Perturbations. (arXiv:2304.11359v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11359
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30495;&#23454;&#20154;&#33080;&#33258;&#25200;&#21160;&#29983;&#25104;&#20266;&#23545;&#25239;&#24615;&#20154;&#33080;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#35757;&#32451;&#30340;&#23545;&#25239;&#24615;&#20154;&#33080;&#26816;&#27979;&#22120;&#19981;&#38656;&#25915;&#20987;&#25968;&#25454;&#21363;&#21487;&#26816;&#27979;&#26032;&#22411;&#26410;&#30693;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#25915;&#20987;&#26088;&#22312;&#36890;&#36807;&#21521;&#36755;&#20837;&#26679;&#26412;&#28155;&#21152;&#29305;&#23450;&#22122;&#22768;&#26469;&#25200;&#20081;&#30446;&#26631;&#31995;&#32479;&#30340;&#21151;&#33021;&#65292;&#24403;&#24212;&#29992;&#20110;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#26102;&#65292;&#23545;&#23433;&#20840;&#24615;&#21644;&#31283;&#20581;&#24615;&#24102;&#26469;&#28508;&#22312;&#23041;&#32961;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#38450;&#24481;&#25216;&#26415;&#22312;&#26816;&#27979;&#26576;&#20123;&#29305;&#23450;&#30340;&#23545;&#25239;&#24615;&#20154;&#33080;&#65288;adv-faces&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#20855;&#26377;&#23436;&#20840;&#19981;&#21516;&#22122;&#22768;&#27169;&#24335;&#30340;&#26032;&#25915;&#20987;&#26041;&#27861;&#23588;&#20854;&#26159;&#22522;&#20110; GAN &#30340;&#25915;&#20987;&#21017;&#32469;&#36807;&#23427;&#20204;&#24182;&#36798;&#21040;&#26356;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;&#26356;&#31967;&#31957;&#30340;&#26159;&#65292;&#29616;&#26377;&#25216;&#26415;&#38656;&#35201;&#25915;&#20987;&#25968;&#25454;&#25165;&#33021;&#23454;&#29616;&#38450;&#24481;&#65292;&#20351;&#24471;&#38450;&#24481;&#32773;&#26080;&#27861;&#38450;&#24481;&#26410;&#34987;&#21457;&#29616;&#30340;&#26032;&#20852;&#25915;&#20987;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;adv-faces&#30340;&#20869;&#22312;&#26222;&#36941;&#24615;&#65292;&#36890;&#36807;&#20351;&#29992;&#19977;&#31181;&#21551;&#21457;&#24335;&#35774;&#35745;&#30340;&#22122;&#22768;&#27169;&#24335;&#25200;&#21160;&#30495;&#23454;&#20154;&#33080;&#26469;&#29983;&#25104;&#20266;&#23545;&#25239;&#24615;&#20154;&#33080;&#12290;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#20165;&#20351;&#29992;&#30495;&#23454;&#20154;&#33080;&#21450;&#20854;&#33258;&#25200;&#21160;&#35757;&#32451;&#23545;&#25239;&#24615;&#20154;&#33080;&#26816;&#27979;&#22120;&#30340;&#30740;&#31350;&#65292;&#19981;&#21463;&#21463;&#23475;&#32773;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#24433;&#21709;&#65292;&#20063;&#19981;&#21463;&#26410;&#30693;&#25915;&#20987;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks aim to disturb the functionality of a target system by adding specific noise to the input samples, bringing potential threats to security and robustness when applied to facial recognition systems. Although existing defense techniques achieve high accuracy in detecting some specific adversarial faces (adv-faces), new attack methods especially GAN-based attacks with completely different noise patterns circumvent them and reach a higher attack success rate. Even worse, existing techniques require attack data before implementing the defense, making it impractical to defend newly emerging attacks that are unseen to defenders. In this paper, we investigate the intrinsic generality of adv-faces and propose to generate pseudo adv-faces by perturbing real faces with three heuristically designed noise patterns. We are the first to train an adv-face detector using only real faces and their self-perturbations, agnostic to victim facial recognition systems, and agnostic to unsee
&lt;/p&gt;</description></item><item><title>GEDI&#26159;&#19968;&#31181;&#23558;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#22522;&#20110;&#20284;&#28982;&#29983;&#25104;&#27169;&#22411;&#32467;&#21512;&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#12290;&#23427;&#19982;&#29616;&#26377;&#30340;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#32852;&#21512;&#35757;&#32451;&#65292;&#26080;&#38656;&#39069;&#22806;&#30417;&#30563;&#25110;&#39044;&#35757;&#32451;&#27493;&#39588;&#65292;&#33021;&#22815;&#20135;&#29983;&#26356;&#22909;&#30340;&#31526;&#21495;&#34920;&#31034;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#35777;&#26126;GEDI&#21487;&#20197;&#22312;&#32858;&#31867;&#24615;&#33021;&#19978;&#26174;&#33879;&#36229;&#36234;&#29616;&#26377;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;&#65292;&#22312;&#23567;&#25968;&#25454;&#33539;&#22260;&#20869;&#30340;&#24615;&#33021;&#20063;&#24471;&#21040;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2304.11357</link><description>&lt;p&gt;
&#36890;&#36807;&#32852;&#21512;&#29983;&#25104;&#24335;&#21644;&#21028;&#21035;&#24335;&#35757;&#32451;&#23398;&#20064;&#31526;&#21495;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Symbolic Representations Through Joint GEnerative and DIscriminative Training. (arXiv:2304.11357v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11357
&lt;/p&gt;
&lt;p&gt;
GEDI&#26159;&#19968;&#31181;&#23558;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#22522;&#20110;&#20284;&#28982;&#29983;&#25104;&#27169;&#22411;&#32467;&#21512;&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#12290;&#23427;&#19982;&#29616;&#26377;&#30340;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#32852;&#21512;&#35757;&#32451;&#65292;&#26080;&#38656;&#39069;&#22806;&#30417;&#30563;&#25110;&#39044;&#35757;&#32451;&#27493;&#39588;&#65292;&#33021;&#22815;&#20135;&#29983;&#26356;&#22909;&#30340;&#31526;&#21495;&#34920;&#31034;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#35777;&#26126;GEDI&#21487;&#20197;&#22312;&#32858;&#31867;&#24615;&#33021;&#19978;&#26174;&#33879;&#36229;&#36234;&#29616;&#26377;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;&#65292;&#22312;&#23567;&#25968;&#25454;&#33539;&#22260;&#20869;&#30340;&#24615;&#33021;&#20063;&#24471;&#21040;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;GEDI&#65292;&#23427;&#26159;&#19968;&#31181;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#23558;&#29616;&#26377;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#19982;&#22522;&#20110;&#20284;&#28982;&#30340;&#29983;&#25104;&#27169;&#22411;&#30456;&#32467;&#21512;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#29983;&#25104;&#24335;&#21644;&#21028;&#21035;&#24335;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#27604;&#29420;&#31435;&#35299;&#20915;&#26041;&#26696;&#20135;&#29983;&#20102;&#26356;&#22909;&#30340;&#31526;&#21495;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;GEDI&#21487;&#20197;&#36731;&#26494;&#38598;&#25104;&#24182;&#19982;&#29616;&#26377;&#30340;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#32852;&#21512;&#35757;&#32451;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#30417;&#30563;&#25110;&#26114;&#36149;&#30340;&#39044;&#35757;&#32451;&#27493;&#39588;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#21253;&#25324;SVHN&#12289;CIFAR10&#21644;CIFAR100&#22312;&#20869;&#30340;&#23454;&#38469;&#25968;&#25454;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;GEDI&#22312;&#32858;&#31867;&#24615;&#33021;&#26041;&#38754;&#22823;&#22823;&#20248;&#20110;&#29616;&#26377;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;&#12290;&#31526;&#21495;&#32452;&#20214;&#36827;&#19968;&#27493;&#20801;&#35768;&#23427;&#21033;&#29992;&#36923;&#36753;&#32422;&#26463;&#24418;&#24335;&#30340;&#30693;&#35782;&#65292;&#25552;&#39640;&#23567;&#25968;&#25454;&#33539;&#22260;&#20869;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce GEDI, a Bayesian framework that combines existing self-supervised learning objectives with likelihood-based generative models. This framework leverages the benefits of both GEnerative and DIscriminative approaches, resulting in improved symbolic representations over standalone solutions. Additionally, GEDI can be easily integrated and trained jointly with existing neuro-symbolic frameworks without the need for additional supervision or costly pre-training steps. We demonstrate through experiments on real-world data, including SVHN, CIFAR10, and CIFAR100, that GEDI outperforms existing self-supervised learning strategies in terms of clustering performance by a significant margin. The symbolic component further allows it to leverage knowledge in the form of logical constraints to improve performance in the small data regime.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#22914;&#20309;&#20351;&#29992;&#22823;&#22411;&#30340;&#36890;&#29992;&#20998;&#21106;&#27169;&#22411;SAM&#26469;&#25552;&#21319;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;SAM&#29983;&#25104;&#30340;&#25513;&#27169;&#12289;&#29305;&#24449;&#21644;&#31283;&#23450;&#24615;&#20998;&#25968;&#26469;&#26500;&#24314;&#21644;&#35757;&#32451;&#26356;&#22909;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#65292;&#24182;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.11332</link><description>&lt;p&gt;
&#21033;&#29992;SAM&#30340;&#36755;&#20837;&#22686;&#24378;&#25216;&#26415;: &#20197;&#20998;&#21106;&#22522;&#30784;&#27169;&#22411;&#20026;&#22522;&#30784;&#25552;&#21319;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Input Augmentation with SAM: Boosting Medical Image Segmentation with Segmentation Foundation Model. (arXiv:2304.11332v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#22914;&#20309;&#20351;&#29992;&#22823;&#22411;&#30340;&#36890;&#29992;&#20998;&#21106;&#27169;&#22411;SAM&#26469;&#25552;&#21319;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;SAM&#29983;&#25104;&#30340;&#25513;&#27169;&#12289;&#29305;&#24449;&#21644;&#31283;&#23450;&#24615;&#20998;&#25968;&#26469;&#26500;&#24314;&#21644;&#35757;&#32451;&#26356;&#22909;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#65292;&#24182;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Segment Anything Model (SAM)&#26159;&#19968;&#20010;&#26368;&#36817;&#21457;&#23637;&#30340;&#36890;&#29992;&#20998;&#21106;&#27169;&#22411;&#65292;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;. SAM&#20351;&#29992;&#20102;&#36229;&#36807;1&#20159;&#20010;&#25513;&#27169;&#30340;1100&#19975;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#20026;&#33258;&#28982;&#22330;&#26223;&#22270;&#20687;&#20013;&#30340;&#24191;&#27867;&#23545;&#35937;&#29983;&#25104;&#20998;&#21106;&#32467;&#26524;&#12290;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#36825;&#26679;&#19968;&#20010;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#26469;&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#23613;&#31649;SAM&#24182;&#27809;&#26377;&#31435;&#21363;&#20026;&#21307;&#23398;&#22270;&#20687;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#20998;&#21106;&#65292;&#20294;&#20854;&#29983;&#25104;&#30340;&#25513;&#27169;&#12289;&#29305;&#24449;&#21644;&#31283;&#23450;&#24615;&#20998;&#25968;&#23545;&#20110;&#26500;&#24314;&#21644;&#35757;&#32451;&#26356;&#22909;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#38750;&#24120;&#26377;&#29992;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;SAM&#26469;&#22686;&#24378;&#32463;&#20856;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#65288;&#22914;U-Net&#65289;&#30340;&#22270;&#20687;&#36755;&#20837;&#12290;&#23545;&#20004;&#20010;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Segment Anything Model (SAM) is a recently developed large model for general-purpose segmentation for computer vision tasks. SAM was trained using 11 million images with over 1 billion masks and can produce segmentation results for a wide range of objects in natural scene images. SAM can be viewed as a general perception model for segmentation (partitioning images into semantically meaningful regions). Thus, how to utilize such a large foundation model for medical image segmentation is an emerging research target. This paper shows that although SAM does not immediately give high-quality segmentation for medical images, its generated masks, features, and stability scores are useful for building and training better medical image segmentation models. In particular, we demonstrate how to use SAM to augment image inputs for a commonly-used medical image segmentation model (e.g., U-Net). Experiments on two datasets show the effectiveness of our proposed method.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#26469;&#35299;&#20915;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#20013;&#30340;&#26497;&#31471;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#29992;&#23454;&#38469;&#32780;&#19981;&#26159;&#27169;&#25311;&#25968;&#25454;&#26469;&#22686;&#21152;&#23569;&#25968;&#27966;&#31867;&#21035;&#25968;&#37327;&#65292;&#22312; Covid &#30456;&#20851;&#30340; Twitter &#25968;&#25454;&#19978;&#23454;&#39564;&#34920;&#26126;&#20854;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640; F1 &#20540;&#12290;</title><link>http://arxiv.org/abs/2304.11318</link><description>&lt;p&gt;
&#19968;&#31181;&#21322;&#30417;&#30563;&#30340;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Semi-Supervised Framework for Misinformation Detection. (arXiv:2304.11318v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11318
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#26469;&#35299;&#20915;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#20013;&#30340;&#26497;&#31471;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#29992;&#23454;&#38469;&#32780;&#19981;&#26159;&#27169;&#25311;&#25968;&#25454;&#26469;&#22686;&#21152;&#23569;&#25968;&#27966;&#31867;&#21035;&#25968;&#37327;&#65292;&#22312; Covid &#30456;&#20851;&#30340; Twitter &#25968;&#25454;&#19978;&#23454;&#39564;&#34920;&#26126;&#20854;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640; F1 &#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#19978;&#34394;&#20551;&#20449;&#24687;&#30340;&#20256;&#25773;&#24050;&#25104;&#20026;&#19968;&#31181;&#26222;&#36941;&#30340;&#31038;&#20250;&#38382;&#39064;&#65292;&#26159;&#35768;&#22810;&#31038;&#20250;&#19981;&#23433;&#30340;&#21407;&#22240;&#12290;&#26426;&#22120;&#23398;&#20064;&#24050;&#32463;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#24212;&#29992;&#21069;&#26223;&#65292;&#20294;&#26159;&#22312;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#26102;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#34394;&#20551;&#20449;&#24687;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#30340;&#26222;&#21450;&#24615;&#20351;&#24471;&#23427;&#20204;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#21482;&#21344;&#23569;&#25968;&#12290;&#20854;&#27425;&#65292;&#26631;&#35760;&#22823;&#37327;&#25968;&#25454;&#26469;&#35757;&#32451;&#19968;&#20010;&#26377;&#29992;&#30340;&#20998;&#31867;&#22120;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#37492;&#20110;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#22788;&#29702;&#26497;&#31471;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#65292;&#20854;&#20248;&#28857;&#22312;&#20110;&#20351;&#29992;&#23454;&#38469;&#25968;&#25454;&#32780;&#19981;&#26159;&#27169;&#25311;&#25968;&#25454;&#26469;&#22686;&#21152;&#23569;&#25968;&#27966;&#31867;&#21035;&#25968;&#37327;&#65292;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#26356;&#20248;&#12290;&#25105;&#20204;&#22312;&#20004;&#32452;&#19982; Covid &#30456;&#20851;&#30340; Twitter &#25968;&#25454;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#22312;&#26497;&#24230;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#20102;&#26174;&#30528;&#30340; F1 &#20540;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
The spread of misinformation in social media outlets has become a prevalent societal problem and is the cause of many kinds of social unrest. Curtailing its prevalence is of great importance and machine learning has shown significant promise. However, there are two main challenges when applying machine learning to this problem. First, while much too prevalent in one respect, misinformation, actually, represents only a minor proportion of all the postings seen on social media. Second, labeling the massive amount of data necessary to train a useful classifier becomes impractical. Considering these challenges, we propose a simple semi-supervised learning framework in order to deal with extreme class imbalances that has the advantage, over other approaches, of using actual rather than simulated data to inflate the minority class. We tested our framework on two sets of Covid-related Twitter data and obtained significant improvement in F1-measure on extremely imbalanced scenarios, as compare
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#31639;&#27861;&#65292;&#37319;&#29992;&#21452;&#26102;&#38388;&#23610;&#24230;&#36866;&#24212;&#26426;&#21046;&#65292;&#32467;&#21512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#39044;&#35328;&#26426;&#65292;&#23454;&#26102;&#20272;&#35745;&#19981;&#21305;&#37197;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#23545;&#20855;&#26377;&#26410;&#30693;&#32467;&#26500;&#30340;&#19981;&#21305;&#37197;&#21644;&#26377;&#30028;&#29366;&#24577;-&#21160;&#20316;&#30456;&#20851;&#19981;&#30830;&#23450;&#24615;&#30340;&#31995;&#32479;&#30340;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2304.11315</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#25903;&#25345;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22312;&#19981;&#21305;&#37197;&#19981;&#30830;&#23450;&#24615;&#32531;&#35299;&#26041;&#38754;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Unmatched uncertainty mitigation through neural network supported model predictive control. (arXiv:2304.11315v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#31639;&#27861;&#65292;&#37319;&#29992;&#21452;&#26102;&#38388;&#23610;&#24230;&#36866;&#24212;&#26426;&#21046;&#65292;&#32467;&#21512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#39044;&#35328;&#26426;&#65292;&#23454;&#26102;&#20272;&#35745;&#19981;&#21305;&#37197;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#23545;&#20855;&#26377;&#26410;&#30693;&#32467;&#26500;&#30340;&#19981;&#21305;&#37197;&#21644;&#26377;&#30028;&#29366;&#24577;-&#21160;&#20316;&#30456;&#20851;&#19981;&#30830;&#23450;&#24615;&#30340;&#31995;&#32479;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#20855;&#26377;&#26410;&#30693;&#32467;&#26500;&#30340;&#19981;&#21305;&#37197;&#21644;&#26377;&#30028;&#29366;&#24577;-&#21160;&#20316;&#30456;&#20851;&#19981;&#30830;&#23450;&#24615;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20316;&#20026;&#23398;&#20064;&#22522;&#30784;MPC&#65288;LBMPC&#65289;&#20013;&#30340;&#39044;&#35328;&#26426;&#65292;&#20197;&#20272;&#35745;&#19981;&#21305;&#37197;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#30001;&#20110;&#23454;&#26102;&#20272;&#35745;&#20854;&#31995;&#25968;&#30340;&#25216;&#26415;&#22256;&#38590;&#65292;&#36890;&#24120;&#35748;&#20026;LBMPC&#38590;&#20197;&#20351;&#29992;&#38750;&#21442;&#25968;&#39044;&#35328;&#26426;&#65292;&#20363;&#22914;DNN&#12290;&#25105;&#20204;&#37319;&#29992;&#21452;&#26102;&#38388;&#23610;&#24230;&#36866;&#24212;&#26426;&#21046;&#65292;&#22312;&#20869;&#37096;&#23618;&#20197;&#32531;&#24930;&#30340;&#26102;&#38388;&#23610;&#24230;&#20351;&#29992;&#22312;&#32447;&#25910;&#38598;&#24182;&#26377;&#36873;&#25321;&#22320;&#23384;&#20648;&#22312;&#32531;&#20914;&#21306;&#20013;&#30340;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#21516;&#26102;&#23454;&#26102;&#26356;&#26032;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#21518;&#19968;&#23618;&#30340;&#26435;&#37325;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#21943;&#27668;&#21457;&#21160;&#26426;&#21387;&#32553;&#31995;&#32479;&#27169;&#22411;&#30340;&#25968;&#20540;&#23454;&#39564;&#36827;&#34892;&#39564;&#35777;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#21487;&#20197;&#23454;&#26102;&#23454;&#29616;&#30340;&#65292;&#24182;&#20855;&#26377;&#29702;&#35770;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a deep learning based model predictive control (MPC) algorithm for systems with unmatched and bounded state-action dependent uncertainties of unknown structure. We utilize a deep neural network (DNN) as an oracle in the underlying optimization problem of learning based MPC (LBMPC) to estimate unmatched uncertainties. Generally, non-parametric oracles such as DNN are considered difficult to employ with LBMPC due to the technical difficulties associated with estimation of their coefficients in real time. We employ a dual-timescale adaptation mechanism, where the weights of the last layer of the neural network are updated in real time while the inner layers are trained on a slower timescale using the training data collected online and selectively stored in a buffer. Our results are validated through a numerical experiment on the compression system model of jet engine. These results indicate that the proposed approach is implementable in real time and carries the theore
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#21069;&#30651;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#25216;&#26415;&#26469;&#20248;&#21270;&#26465;&#20214;&#39640;&#26031;&#20998;&#24067;&#30340;&#22343;&#20540;&#20272;&#35745;&#65292;&#36890;&#36807;&#23545;&#20004;&#20010;&#20272;&#35745;&#36827;&#34892;&#22806;&#25512;&#26469;&#35745;&#31639;&#26356;&#20934;&#30830;&#30340;&#20272;&#35745;&#20540;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#23545;DNN&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#27604;&#26368;&#26032;&#26041;&#27861;&#26356;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.11312</link><description>&lt;p&gt;
&#22522;&#20110;&#21069;&#30651;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#22343;&#20540;&#20272;&#35745;&#26041;&#27861;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Lookahead Diffusion Probabilistic Models for Refining Mean Estimation. (arXiv:2304.11312v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#21069;&#30651;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#25216;&#26415;&#26469;&#20248;&#21270;&#26465;&#20214;&#39640;&#26031;&#20998;&#24067;&#30340;&#22343;&#20540;&#20272;&#35745;&#65292;&#36890;&#36807;&#23545;&#20004;&#20010;&#20272;&#35745;&#36827;&#34892;&#22806;&#25512;&#26469;&#35745;&#31639;&#26356;&#20934;&#30830;&#30340;&#20272;&#35745;&#20540;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#23545;DNN&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#27604;&#26368;&#26032;&#26041;&#27861;&#26356;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21069;&#30651;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;LA-DPMs&#65289;&#30340;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#21033;&#29992;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#22312;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#20013;&#65292;&#22312;&#36830;&#32493;&#26102;&#38388;&#27493;&#39588;&#20043;&#21518;&#36755;&#20986;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#20197;&#20248;&#21270;&#26465;&#20214;&#39640;&#26031;&#20998;&#24067;&#30340;&#22343;&#20540;&#20272;&#35745;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#23545;&#20004;&#20010;$\boldsymbol{x}$&#20272;&#35745;&#36827;&#34892;&#22806;&#25512;&#26469;&#35745;&#31639;&#26356;&#20934;&#30830;&#30340;$\boldsymbol{x}$&#20272;&#35745;&#30340;&#26041;&#27861;&#12290;&#36825;&#21487;&#20197;&#36890;&#36807;&#22312;&#29616;&#26377;DPMs&#30340;&#21518;&#21521;&#36807;&#31243;&#20013;&#24341;&#20837;&#39069;&#22806;&#30340;&#36830;&#25509;&#26469;&#36731;&#26494;&#23558;&#20854;&#38598;&#25104;&#21040;&#21518;&#21521;&#36807;&#31243;&#20013;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#23545;DNN&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#26368;&#26032;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;LA-DPMs&#26041;&#27861;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose lookahead diffusion probabilistic models (LA-DPMs) to exploit the correlation in the outputs of the deep neural networks (DNNs) over subsequent timesteps in diffusion probabilistic models (DPMs) to refine the mean estimation of the conditional Gaussian distributions in the backward process. A typical DPM first obtains an estimate of the original data sample $\boldsymbol{x}$ by feeding the most recent state $\boldsymbol{z}_i$ and index $i$ into the DNN model and then computes the mean vector of the conditional Gaussian distribution for $\boldsymbol{z}_{i-1}$. We propose to calculate a more accurate estimate for $\boldsymbol{x}$ by performing extrapolation on the two estimates of $\boldsymbol{x}$ that are obtained by feeding $(\boldsymbol{z}_{i+1},i+1)$ and $(\boldsymbol{z}_{i},i)$ into the DNN model. The extrapolation can be easily integrated into the backward process of existing DPMs by introducing an additional connection over two consecutive timesteps, and fine-tuning is n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#20256;&#25773;&#23398;&#35270;&#35282;&#20986;&#21457;&#65292;&#25552;&#20986;&#20154;&#26426;&#20132;&#20114;&#20013;&#30340;&#20116;&#31181;&#38750;&#35821;&#35328;&#20195;&#30721;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#35774;&#35745;&#27169;&#24335;&#12290;&#35813;&#30740;&#31350;&#35748;&#20026;&#65292;&#23558;&#36825;&#20123;&#38750;&#35821;&#35328;&#20195;&#30721;&#25972;&#21512;&#21040;&#26426;&#22120;&#20154;&#20013;&#21487;&#20197;&#20351;&#26426;&#22120;&#20154;&#26356;&#21152;&#8220;&#29983;&#21160;&#8221;&#21644;&#8220;&#31038;&#20250;&#21270;&#8221;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2304.11293</link><description>&lt;p&gt;
&#20154;&#26426;&#20132;&#20114;&#20013;&#30340;&#38750;&#35821;&#35328;&#26263;&#31034;&#65306;&#20256;&#25773;&#23398;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Nonverbal Cues in Human-Robot Interaction: A Communication Studies Perspective. (arXiv:2304.11293v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11293
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#20256;&#25773;&#23398;&#35270;&#35282;&#20986;&#21457;&#65292;&#25552;&#20986;&#20154;&#26426;&#20132;&#20114;&#20013;&#30340;&#20116;&#31181;&#38750;&#35821;&#35328;&#20195;&#30721;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#35774;&#35745;&#27169;&#24335;&#12290;&#35813;&#30740;&#31350;&#35748;&#20026;&#65292;&#23558;&#36825;&#20123;&#38750;&#35821;&#35328;&#20195;&#30721;&#25972;&#21512;&#21040;&#26426;&#22120;&#20154;&#20013;&#21487;&#20197;&#20351;&#26426;&#22120;&#20154;&#26356;&#21152;&#8220;&#29983;&#21160;&#8221;&#21644;&#8220;&#31038;&#20250;&#21270;&#8221;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#19982;&#20154;&#20043;&#38388;&#30340;&#20132;&#27969;&#38500;&#20102;&#35821;&#35328;&#20043;&#22806;&#65292;&#36824;&#21253;&#25324;&#24191;&#27867;&#30340;&#38750;&#35821;&#35328;&#26263;&#31034;&#12290;&#23558;&#36825;&#20123;&#26263;&#31034;&#24212;&#29992;&#20110;&#35774;&#35745;&#19982;&#20154;&#20132;&#20114;&#30340;&#26426;&#22120;&#20154;&#21644;&#20854;&#20182;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#65292;&#21487;&#33021;&#20250;&#24102;&#26469;&#26356;&#33258;&#28982;&#12289;&#26131;&#29702;&#35299;&#21644;&#26131;&#25509;&#36817;&#30340;&#20132;&#20114;&#20307;&#39564;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#23450;&#20041;&#24615;&#30340;&#38750;&#35821;&#35328;&#20195;&#30721;&#65292;&#20197;&#22320;&#22336;&#36890;&#20449;&#30740;&#31350;&#39046;&#22495;&#30340;&#20116;&#20010;&#20154;&#31867;&#24863;&#23448;&#31995;&#32479;&#65288;&#35270;&#12289;&#21548;&#12289;&#35302;&#12289;&#21957;&#12289;&#21619;&#65289;&#65292;&#21487;&#20197;&#23558;&#36825;&#20123;&#20195;&#30721;&#32763;&#35793;&#20026;&#20154;&#26426;&#20132;&#20114;&#30340;&#35774;&#35745;&#27169;&#24335;&#65292;&#20351;&#29992;&#31934;&#36873;&#30340;&#36890;&#20449;&#30740;&#31350;&#21644;&#20154;&#26426;&#20132;&#20114;&#25991;&#29486;&#12290;&#30001;&#20110;&#38750;&#35821;&#35328;&#20195;&#30721;&#26159;&#20154;&#31867;&#20132;&#27969;&#30340;&#37325;&#35201;&#26041;&#24335;&#20043;&#19968;&#65292;&#25105;&#20204;&#35748;&#20026;&#23558;&#26426;&#22120;&#20154;&#30340;&#38750;&#35821;&#35328;&#20195;&#30721;&#25972;&#21512;&#21040;&#20154;&#26426;&#20132;&#20114;&#20013;&#65292;&#23558;&#20026;&#26426;&#22120;&#20154;&#24102;&#26469;&#8220;&#29983;&#21160;&#24863;&#8221;&#25110;&#8220;&#31038;&#20250;&#20195;&#29702;&#8221;&#65292;&#21542;&#21017;&#26426;&#22120;&#20154;&#23558;&#32570;&#20047;&#36825;&#31181;&#20307;&#39564;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30740;&#31350;&#26041;&#21521;&#30340;&#24314;&#35758;&#65292;&#20197;&#20419;&#36827;&#38750;&#35821;&#35328;&#20132;&#27969;&#39046;&#22495;&#22312;&#20154;&#26426;&#20132;&#20114;&#20013;&#30340;&#21457;&#23637;&#21644;&#25913;&#36827;&#20043;&#38388;&#30340;&#27807;&#36890;&#12290;
&lt;/p&gt;
&lt;p&gt;
Communication between people is characterized by a broad range of nonverbal cues. Transferring these cues into the design of robots and other artificial agents that interact with people may foster more natural, inviting, and accessible experiences. In this position paper, we offer a series of definitive nonverbal codes for human-robot interaction (HRI) that address the five human sensory systems (visual, auditory, haptic, olfactory, gustatory) drawn from the field of communication studies. We discuss how these codes can be translated into design patterns for HRI using a curated sample of the communication studies and HRI literatures. As nonverbal codes are an essential mode in human communication, we argue that integrating robotic nonverbal codes in HRI will afford robots a feeling of "aliveness" or "social agency" that would otherwise be missing. We end with suggestions for research directions to stimulate work on nonverbal communication within the field of HRI and improve communicati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#21160;&#25552;&#21462;&#19982;&#33021;&#28304;&#30456;&#20851;&#30340;&#24212;&#29992;&#31243;&#24207;&#35780;&#35770;&#30340;&#19981;&#21516;&#25216;&#26415;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#31070;&#32463;&#32593;&#32476;&#20248;&#20110;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.11292</link><description>&lt;p&gt;
&#20851;&#20110;&#24212;&#29992;&#35780;&#35770;&#20013;&#33021;&#28304;&#30456;&#20851;&#38382;&#39064;&#30340;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
On the Identification of the Energy related Issues from the App Reviews. (arXiv:2304.11292v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#21160;&#25552;&#21462;&#19982;&#33021;&#28304;&#30456;&#20851;&#30340;&#24212;&#29992;&#31243;&#24207;&#35780;&#35770;&#30340;&#19981;&#21516;&#25216;&#26415;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#31070;&#32463;&#32593;&#32476;&#20248;&#20110;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;&#31243;&#24207;&#30340;&#33021;&#28304;&#25928;&#29575;&#38382;&#39064;&#21487;&#33021;&#20250;&#23545;&#24212;&#29992;&#31243;&#24207;&#29992;&#25143;&#36896;&#25104;&#37325;&#22823;&#38382;&#39064;&#65292;&#24182;&#22312;&#24212;&#29992;&#21830;&#24215;&#24191;&#27867;&#35752;&#35770;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30740;&#31350;&#19982;&#33021;&#28304;&#30456;&#20851;&#30340;&#24212;&#29992;&#31243;&#24207;&#35780;&#35770;&#20197;&#30830;&#23450;&#33021;&#28304;&#30456;&#20851;&#29992;&#25143;&#21453;&#39304;&#30340;&#20027;&#35201;&#21407;&#22240;&#25110;&#31867;&#21035;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#36824;&#27809;&#26377;&#30740;&#31350;&#26377;&#25928;&#22320;&#33258;&#21160;&#25552;&#21462;&#19982;&#33021;&#28304;&#30456;&#20851;&#30340;&#24212;&#29992;&#31243;&#24207;&#35780;&#35770;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#25216;&#26415;&#65292;&#20197;&#33258;&#21160;&#25552;&#21462;&#19982;&#33021;&#28304;&#30456;&#20851;&#30340;&#29992;&#25143;&#21453;&#39304;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12289;F1&#20998;&#25968;&#21644;&#36816;&#34892;&#26102;&#38388;&#65292;&#24182;&#19982;&#30456;&#20851;&#29305;&#24449;&#32452;&#21512;&#21644;&#30456;&#23545;&#36739;&#26032;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#24635;&#20849;&#27604;&#36739;&#20102;60&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#21450;&#20351;&#29992;&#20845;&#31181;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#19977;&#31181;&#21333;&#35789;&#23884;&#20837;&#27169;&#22411;&#26500;&#24314;&#30340;30&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#36890;&#36807;&#35813;&#24037;&#20855;&#65292;&#24320;&#21457;&#20154;&#21592;&#21487;&#20197;&#36941;&#21382;&#36825;&#20010;&#22823;&#35268;&#27169;&#30340;&#32467;&#26524;&#38598;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#31070;&#32463;&#32593;&#32476;&#20248;&#20110;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The energy inefficiency of the apps can be a major issue for the app users which is discussed on App Stores extensively. Previous research has shown the importance of investigating the energy related app reviews to identify the major causes or categories of energy related user feedback. However, there is no study that efficiently extracts the energy related app reviews automatically. In this paper, we empirically study different techniques for automatic extraction of the energy related user feedback. We compare the accuracy, F1-score and run time of numerous machine-learning models with relevant feature combinations and relatively modern Neural Network-based models. In total, 60 machine learning models are compared to 30 models that we build using six neural network architectures and three word embedding models. We develop a visualization tool for this study through which a developer can traverse through this large-scale result set. The results show that neural networks outperform the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#21270;&#20998;&#26512;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#27700;&#21360;&#12289;&#25351;&#32441;&#12289;&#27169;&#22411;&#35775;&#38382;&#21644;&#25915;&#20987;&#30340;&#20445;&#25252;&#25216;&#26415;&#65292;&#24182;&#26500;&#24314;&#20102;&#32508;&#21512;&#30340;&#23041;&#32961;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.11285</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36866;&#24403;&#30340;&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#26426;&#21046;&#30340;&#36776;&#21035;&#65306;&#38024;&#23545;&#27700;&#21360;&#12289;&#25351;&#32441;&#12289;&#27169;&#22411;&#35775;&#38382;&#21644;&#25915;&#20987;&#30340;&#31995;&#32479;&#21270;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Identifying Appropriate Intellectual Property Protection Mechanisms for Machine Learning Models: A Systematization of Watermarking, Fingerprinting, Model Access, and Attacks. (arXiv:2304.11285v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11285
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#21270;&#20998;&#26512;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#27700;&#21360;&#12289;&#25351;&#32441;&#12289;&#27169;&#22411;&#35775;&#38382;&#21644;&#25915;&#20987;&#30340;&#20445;&#25252;&#25216;&#26415;&#65292;&#24182;&#26500;&#24314;&#20102;&#32508;&#21512;&#30340;&#23041;&#32961;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#21830;&#19994;&#24212;&#29992;&#36234;&#26469;&#36234;&#26222;&#21450;&#65307;&#21516;&#26102;&#65292;ML&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#32780;&#19988;&#35757;&#32451;&#25104;&#26412;&#36234;&#26469;&#36234;&#39640;&#65292;&#36825;&#20351;&#24471;&#35757;&#32451;&#27169;&#22411;&#30340;&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#65288;IPP&#65289;&#25104;&#20026;&#19968;&#20010;&#32039;&#36843;&#30340;&#38382;&#39064;&#12290;&#19982;&#20854;&#20182;&#39046;&#22495;&#21487;&#20197;&#24314;&#31435;&#22312;&#23545;&#20854;IP&#36827;&#34892;&#20445;&#25252;&#30340;&#23041;&#32961;&#12289;&#25915;&#20987;&#21644;&#38450;&#24481;&#25514;&#26045;&#26377;&#30528;&#28145;&#21051;&#29702;&#35299;&#30340;&#22522;&#30784;&#19978;&#19981;&#21516;&#65292;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#30456;&#20851;&#30740;&#31350;&#20173;&#28982;&#38750;&#24120;&#38646;&#25955;&#12290;&#36825;&#20063;&#26159;&#30001;&#20110;&#32570;&#23569;&#32479;&#19968;&#30340;&#35270;&#35282;&#20197;&#21450;&#36825;&#20123;&#26041;&#38754;&#30340;&#20849;&#21516;&#20998;&#31867;&#23398;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#21270;&#20102;&#25105;&#20204;&#22312;ML&#20013;&#20851;&#20110;IPP&#30340;&#21457;&#29616;&#65292;&#21516;&#26102;&#19987;&#27880;&#20110;&#32534;&#20889;&#26102;&#24050;&#30830;&#35748;&#30340;&#23041;&#32961;&#12289;&#25915;&#20987;&#21644;&#38450;&#24481;&#25514;&#26045;&#12290;&#25105;&#20204;&#20026;ML&#20013;&#30340;IP&#24314;&#31435;&#20102;&#32508;&#21512;&#30340;&#23041;&#32961;&#27169;&#22411;&#65292;&#23558;&#25915;&#20987;&#21644;&#38450;&#24481;&#25514;&#26045;&#20998;&#31867;&#21040;&#32479;&#19968;&#21644;&#25972;&#21512;&#30340;&#20998;&#31867;&#27861;&#20013;&#65292;&#20174;&#32780;&#25645;&#36215;&#20102;&#26469;&#33258;ML&#21644;&#23433;&#20840;&#31038;&#21306;&#30340;&#30740;&#31350;&#20043;&#38388;&#30340;&#26725;&#26753;&#12290;
&lt;/p&gt;
&lt;p&gt;
The commercial use of Machine Learning (ML) is spreading; at the same time, ML models are becoming more complex and more expensive to train, which makes Intellectual Property Protection (IPP) of trained models a pressing issue. Unlike other domains that can build on a solid understanding of the threats, attacks and defenses available to protect their IP, the ML-related research in this regard is still very fragmented. This is also due to a missing unified view as well as a common taxonomy of these aspects.  In this paper, we systematize our findings on IPP in ML, while focusing on threats and attacks identified and defenses proposed at the time of writing. We develop a comprehensive threat model for IP in ML, categorizing attacks and defenses within a unified and consolidated taxonomy, thus bridging research from both the ML and security communities.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#26234;&#33021;&#20195;&#29702;&#22312;&#21453;&#35823;&#20449;&#24687;&#26041;&#38754;&#23545;&#20849;&#35782;&#24187;&#35273;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#20381;&#36182;&#65288;&#34892;&#20026;&#65289;&#20250;&#24433;&#21709;&#22522;&#20110;&#20849;&#35782;&#30340;&#35299;&#37322;&#65292;&#36825;&#23545;&#20351;&#29992;XAI&#30340;&#21453;&#35823;&#20449;&#24687;&#31995;&#32479;&#30340;&#35774;&#35745;&#25351;&#23548;&#26377;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2304.11279</link><description>&lt;p&gt;
&#22522;&#20110;&#21453;&#35823;&#20449;&#24687;&#26234;&#33021;&#20307;&#30340;&#20849;&#35782;&#35299;&#37322;&#20013;&#30340;&#20449;&#20219;&#19982;&#20381;&#36182;
&lt;/p&gt;
&lt;p&gt;
Trust and Reliance in Consensus-Based Explanations from an Anti-Misinformation Agent. (arXiv:2304.11279v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11279
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#26234;&#33021;&#20195;&#29702;&#22312;&#21453;&#35823;&#20449;&#24687;&#26041;&#38754;&#23545;&#20849;&#35782;&#24187;&#35273;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#20381;&#36182;&#65288;&#34892;&#20026;&#65289;&#20250;&#24433;&#21709;&#22522;&#20110;&#20849;&#35782;&#30340;&#35299;&#37322;&#65292;&#36825;&#23545;&#20351;&#29992;XAI&#30340;&#21453;&#35823;&#20449;&#24687;&#31995;&#32479;&#30340;&#35774;&#35745;&#25351;&#23548;&#26377;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#20849;&#35782;&#24187;&#35273;&#8221;&#25351;&#30340;&#26159;&#20154;&#20204;&#35748;&#20026;&#22810;&#20010;&#26469;&#28304;&#37117;&#36798;&#25104;&#20102;&#19968;&#33268;&#65292;&#20294;&#23454;&#38469;&#19978;&#36825;&#20123;&#26469;&#28304;&#26159;&#30456;&#21516;&#30340;&#65292;&#22240;&#27492;&#19981;&#23384;&#22312;&#8220;&#30495;&#27491;&#8221;&#30340;&#19968;&#33268;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#24110;&#21161;&#25552;&#39640;&#31038;&#20132;&#23186;&#20307;&#20803;&#35748;&#30693;&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#26234;&#33021;&#20195;&#29702;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#29616;&#35937;&#30340;&#23384;&#22312;&#12290;&#22312;Twitter&#31561;&#24179;&#21488;&#19978;&#65292;&#34394;&#20551;&#20449;&#24687;&#26159;&#19968;&#20010;&#20840;&#29699;&#24615;&#30340;&#38382;&#39064;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#19968;&#20010;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20316;&#20026;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#31995;&#32479;&#65292;&#26234;&#33021;&#20195;&#29702;&#22312;&#23545;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#30340;&#38169;&#35823;&#21028;&#26029;&#19978;&#25552;&#20379;&#35299;&#37322;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20449;&#20219;&#65288;&#24577;&#24230;&#65289;&#21644;&#20381;&#36182;&#65288;&#34892;&#20026;&#65289;&#20316;&#20026;XAI&#29992;&#25143;&#20307;&#39564;&#65288;UX&#65289;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#20197;&#21450;&#36825;&#20123;&#22240;&#32032;&#26159;&#21542;&#20250;&#24433;&#21709;&#20849;&#35782;&#24187;&#35273;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#20449;&#20219;&#27809;&#26377;&#24433;&#21709;&#65292;&#20294;&#20381;&#36182;&#23545;&#22522;&#20110;&#20849;&#35782;&#30340;&#35299;&#37322;&#26377;&#24433;&#21709;&#12290;&#35813;&#30740;&#31350;&#21487;&#25351;&#23548;&#20351;&#29992;XAI&#30340;&#21453;&#35823;&#20449;&#24687;&#31995;&#32479;&#30340;&#35774;&#35745;&#65292;&#29305;&#21035;&#26159;&#35299;&#37322;&#30340;&#29992;&#25143;&#20013;&#24515;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
The illusion of consensus occurs when people believe there is consensus across multiple sources, but the sources are the same and thus there is no "true" consensus. We explore this phenomenon in the context of an AI-based intelligent agent designed to augment metacognition on social media. Misinformation, especially on platforms like Twitter, is a global problem for which there is currently no good solution. As an explainable AI (XAI) system, the agent provides explanations for its decisions on the misinformed nature of social media content. In this late-breaking study, we explored the roles of trust (attitude) and reliance (behaviour) as key elements of XAI user experience (UX) and whether these influenced the illusion of consensus. Findings show no effect of trust, but an effect of reliance on consensus-based explanations. This work may guide the design of anti-misinformation systems that use XAI, especially the user-centred design of explanations.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;PyTorch&#30340;Fully Sharded Data Parallel&#65288;FSDP&#65289;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#21487;&#25193;&#23637;&#22823;&#22411;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#20248;&#21270;&#21508;&#31181;&#30828;&#20214;&#37197;&#32622;&#30340;&#36164;&#28304;&#21033;&#29992;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.11277</link><description>&lt;p&gt;
PyTorch FSDP&#65306;&#20840;&#38754;&#20998;&#29255;&#25968;&#25454;&#24182;&#34892;&#35268;&#27169;&#21270;&#30340;&#32463;&#39564;
&lt;/p&gt;
&lt;p&gt;
PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel. (arXiv:2304.11277v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11277
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;PyTorch&#30340;Fully Sharded Data Parallel&#65288;FSDP&#65289;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#21487;&#25193;&#23637;&#22823;&#22411;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#20248;&#21270;&#21508;&#31181;&#30828;&#20214;&#37197;&#32622;&#30340;&#36164;&#28304;&#21033;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#22823;&#22411;&#27169;&#22411;&#22312;&#24191;&#27867;&#39046;&#22495;&#20869;&#20855;&#26377;&#20248;&#24322;&#30340;&#24615;&#33021;&#28508;&#21147;&#12290;&#23613;&#31649;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30740;&#31350;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20351;&#24471;&#24320;&#21457;&#21644;&#25506;&#32034;&#22823;&#22411;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#65292;&#20294;&#36825;&#20123;&#33021;&#21147;&#20173;&#21463;&#38480;&#20110;&#23569;&#25968;&#39640;&#32423;&#29992;&#25143;&#21644;&#34892;&#19994;&#39046;&#34966;&#65292;&#23548;&#33268;&#25216;&#26415;&#19978;&#30340;&#38544;&#21547;&#22721;&#22418;&#38459;&#30861;&#24191;&#27867;&#31038;&#21306;&#35775;&#38382;&#21644;&#21033;&#29992;&#36825;&#20123;&#25216;&#26415;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;PyTorch Fully Sharded Data Parallel&#65288;FSDP&#65289;&#20316;&#20026;&#22823;&#22411;&#27169;&#22411;&#35757;&#32451;&#30340;&#20135;&#19994;&#32423;&#35299;&#20915;&#26041;&#26696;&#12290;FSDP&#24050;&#19982;&#20960;&#20010;&#20851;&#38190;PyTorch&#26680;&#24515;&#32452;&#20214;&#65288;&#21253;&#25324;&#24352;&#37327;&#23454;&#29616;&#12289;&#20998;&#21457;&#22120;&#31995;&#32479;&#21644;CUDA&#20869;&#23384;&#32531;&#23384;&#20998;&#37197;&#22120;&#65289;&#23494;&#20999;&#21327;&#20316;&#65292;&#20197;&#25552;&#20379;&#38750;&#20405;&#20837;&#24335;&#29992;&#25143;&#20307;&#39564;&#21644;&#39640;&#35757;&#32451;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;FSDP&#26412;&#22320;&#38598;&#25104;&#20102;&#19968;&#31995;&#21015;&#25216;&#26415;&#21644;&#35774;&#32622;&#65292;&#20248;&#21270;&#20102;&#21508;&#31181;&#30828;&#20214;&#37197;&#32622;&#30340;&#36164;&#28304;&#21033;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is widely acknowledged that large models have the potential to deliver superior performance across a broad range of domains. Despite the remarkable progress made in the field of machine learning systems research, which has enabled the development and exploration of large models, such abilities remain confined to a small group of advanced users and industry leaders, resulting in an implicit technical barrier for the wider community to access and leverage these technologies. In this paper, we introduce PyTorch Fully Sharded Data Parallel (FSDP) as an industry-grade solution for large model training. FSDP has been closely co-designed with several key PyTorch core components including Tensor implementation, dispatcher system, and CUDA memory caching allocator, to provide non-intrusive user experiences and high training efficiency. Additionally, FSDP natively incorporates a range of techniques and settings to optimize resource utilization across a variety of hardware configurations. The 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#19981;&#21516;&#23569;&#26679;&#26412;&#25968;&#25454;&#38598;&#12289;&#26550;&#26500;&#12289;&#39044;&#35757;&#32451;&#21021;&#22987;&#21270;&#21644;&#31283;&#20581;&#24615;&#24178;&#39044;&#30340;&#33258;&#28982;&#20998;&#24067;&#28418;&#31227;&#30340;&#31283;&#20581;&#24615;&#36827;&#34892;&#20102;&#39318;&#27425;&#28145;&#20837;&#30740;&#31350;&#65292;&#21457;&#29616;&#27809;&#26377;&#21333;&#19968;&#30340;&#36873;&#25321;&#27169;&#22411;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#31283;&#20581;&#65292;&#29616;&#26377;&#30340;&#24178;&#39044;&#25514;&#26045;&#20063;&#21487;&#33021;&#26080;&#27861;&#25552;&#39640;&#26576;&#20123;&#25968;&#25454;&#38598;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.11263</link><description>&lt;p&gt;
&#33258;&#28982;&#20998;&#24067;&#28418;&#31227;&#19979;&#20302;&#26679;&#26412;&#31283;&#20581;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Low-Shot Robustness to Natural Distribution Shifts. (arXiv:2304.11263v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#19981;&#21516;&#23569;&#26679;&#26412;&#25968;&#25454;&#38598;&#12289;&#26550;&#26500;&#12289;&#39044;&#35757;&#32451;&#21021;&#22987;&#21270;&#21644;&#31283;&#20581;&#24615;&#24178;&#39044;&#30340;&#33258;&#28982;&#20998;&#24067;&#28418;&#31227;&#30340;&#31283;&#20581;&#24615;&#36827;&#34892;&#20102;&#39318;&#27425;&#28145;&#20837;&#30740;&#31350;&#65292;&#21457;&#29616;&#27809;&#26377;&#21333;&#19968;&#30340;&#36873;&#25321;&#27169;&#22411;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#31283;&#20581;&#65292;&#29616;&#26377;&#30340;&#24178;&#39044;&#25514;&#26045;&#20063;&#21487;&#33021;&#26080;&#27861;&#25552;&#39640;&#26576;&#20123;&#25968;&#25454;&#38598;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#32467;&#21512;&#26356;&#22909;&#30340;&#24494;&#35843;&#26041;&#27861;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#24050;&#32463;&#21462;&#24471;&#20102;&#38024;&#23545;&#33258;&#28982;&#20998;&#24067;&#28418;&#31227;&#30340;&#40065;&#26834;&#24615;&#30340;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#24494;&#35843;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#65292;&#32780;&#24403;&#35757;&#32451;&#25968;&#25454;&#37327;&#19981;&#39640;&#26102;&#35266;&#23519;&#21040;&#30340;&#24773;&#20917;&#23578;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#19981;&#21516;&#23569;&#26679;&#26412;&#25968;&#25454;&#38598;&#12289;&#26550;&#26500;&#12289;&#39044;&#35757;&#32451;&#21021;&#22987;&#21270;&#21644;&#26368;&#20808;&#36827;&#30340;&#31283;&#20581;&#24615;&#24178;&#39044;&#30340;&#33258;&#28982;&#20998;&#24067;&#28418;&#31227;&#30340;&#31283;&#20581;&#24615;&#36827;&#34892;&#20102;&#39318;&#27425;&#28145;&#20837;&#30740;&#31350;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#27809;&#26377;&#21333;&#19968;&#30340;&#36873;&#25321;&#27169;&#22411;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#31283;&#20581;&#65292;&#21363;&#20351;&#22312;&#23436;&#25972;&#26679;&#26412;&#19979;&#65292;&#29616;&#26377;&#30340;&#24178;&#39044;&#25514;&#26045;&#20063;&#21487;&#33021;&#26080;&#27861;&#25552;&#39640;&#26576;&#20123;&#25968;&#25454;&#38598;&#30340;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#33021;&#22815;&#28608;&#21169;&#31038;&#21306;&#20851;&#27880;&#36825;&#20010;&#23454;&#38469;&#37325;&#35201;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robustness to natural distribution shifts has seen remarkable progress thanks to recent pre-training strategies combined with better fine-tuning methods. However, such fine-tuning assumes access to large amounts of labelled data, and the extent to which the observations hold when the amount of training data is not as high remains unknown. We address this gap by performing the first in-depth study of robustness to various natural distribution shifts in different low-shot regimes: spanning datasets, architectures, pre-trained initializations, and state-of-the-art robustness interventions. Most importantly, we find that there is no single model of choice that is often more robust than others, and existing interventions can fail to improve robustness on some datasets even if they do so in the full-shot regime. We hope that our work will motivate the community to focus on this problem of practical importance.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23884;&#20837;&#24335;&#35745;&#31639;&#20934;&#22791;&#22909;&#30340;&#28023;&#20107;&#38556;&#30861;&#29289;&#26816;&#27979;&#32593;&#32476;&#8212;&#8212;eWaSR&#65292;&#33021;&#22815;&#22312;&#20445;&#35777;&#26816;&#27979;&#36136;&#37327;&#30340;&#21069;&#25552;&#19979;&#36816;&#34892;&#36895;&#24230;&#26356;&#24555;&#65292;&#19988;&#22312;F1&#24471;&#20998;&#26041;&#38754;&#20063;&#20248;&#20110;&#20854;&#20182;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#23884;&#20837;&#24335;&#23601;&#32490;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2304.11249</link><description>&lt;p&gt;
eWaSR&#8212;&#8212;&#19968;&#31181;&#23884;&#20837;&#24335;&#35745;&#31639;&#20934;&#22791;&#22909;&#30340;&#28023;&#20107;&#38556;&#30861;&#29289;&#26816;&#27979;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
eWaSR -- an embedded-compute-ready maritime obstacle detection network. (arXiv:2304.11249v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23884;&#20837;&#24335;&#35745;&#31639;&#20934;&#22791;&#22909;&#30340;&#28023;&#20107;&#38556;&#30861;&#29289;&#26816;&#27979;&#32593;&#32476;&#8212;&#8212;eWaSR&#65292;&#33021;&#22815;&#22312;&#20445;&#35777;&#26816;&#27979;&#36136;&#37327;&#30340;&#21069;&#25552;&#19979;&#36816;&#34892;&#36895;&#24230;&#26356;&#24555;&#65292;&#19988;&#22312;F1&#24471;&#20998;&#26041;&#38754;&#20063;&#20248;&#20110;&#20854;&#20182;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#23884;&#20837;&#24335;&#23601;&#32490;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#20107;&#38556;&#30861;&#29289;&#26816;&#27979;&#23545;&#20110;&#33258;&#20027;&#27700;&#38754;&#33337;&#33334;&#65288;ASV&#65289;&#30340;&#23433;&#20840;&#23548;&#33322;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#22522;&#20110;&#22270;&#20687;&#30340;&#26816;&#27979;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#24050;&#32463;&#22823;&#22823;&#25552;&#39640;&#65292;&#20294;&#23427;&#20204;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#35201;&#27714;&#38459;&#27490;&#20102;&#22312;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#30340;&#37096;&#32626;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#24403;&#21069;&#34920;&#29616;&#26368;&#20339;&#30340;&#28023;&#20107;&#38556;&#30861;&#29289;&#26816;&#27979;&#32593;&#32476;WaSR&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#22312;&#20998;&#26512;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#26367;&#25442;&#26368;&#32791;&#35745;&#31639;&#36164;&#28304;&#30340;&#38454;&#27573;&#24182;&#25552;&#20986;&#20854;&#23884;&#20837;&#24335;&#35745;&#31639;&#20934;&#22791;&#22909;&#30340;&#21464;&#20307;eWaSR&#12290;&#29305;&#21035;&#22320;&#65292;&#26032;&#35774;&#35745;&#36981;&#24490;&#20102;&#22522;&#20110;transformer&#30340;&#36731;&#37327;&#32423;&#32593;&#32476;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;eWaSR&#22312;&#20165;&#26377;0.52&#65285;&#30340;F1&#24471;&#20998;&#24615;&#33021;&#19979;&#38477;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;WaSR&#30456;&#24403;&#30340;&#26816;&#27979;&#32467;&#26524;&#65292;&#24182;&#22312;F1&#24471;&#20998;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#23884;&#20837;&#24335;&#23601;&#32490;&#26550;&#26500;&#39640;&#36798;9.74&#65285;&#12290;&#22312;&#26631;&#20934;GPU&#19978;&#65292;eWaSR&#30340;&#36816;&#34892;&#36895;&#24230;&#27604;&#21407;&#22987;WaSR&#24555;10&#20493;&#65288;115 FPS vs 11 FPS&#65289;&#12290;&#22312;OAK-D&#23454;&#38469;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#30340;&#27979;&#35797;&#34920;&#26126;&#65292;&#23613;&#31649;WaSR&#30001;&#20110;&#20869;&#23384;&#38480;&#21046;&#32780;&#26080;&#27861;&#36816;&#34892;&#65292;&#20294;eWaSR&#20173;&#28982;&#33021;&#22815;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maritime obstacle detection is critical for safe navigation of autonomous surface vehicles (ASVs). While the accuracy of image-based detection methods has advanced substantially, their computational and memory requirements prohibit deployment on embedded devices. In this paper we analyze the currently best-performing maritime obstacle detection network WaSR. Based on the analysis we then propose replacements for the most computationally intensive stages and propose its embedded-compute-ready variant eWaSR. In particular, the new design follows the most recent advancements of transformer-based lightweight networks. eWaSR achieves comparable detection results to state-of-the-art WaSR with only 0.52% F1 score performance drop and outperforms other state-of-the-art embedded-ready architectures by over 9.74% in F1 score. On a standard GPU, eWaSR runs 10x faster than the original WaSR (115 FPS vs 11 FPS). Tests on a real embedded device OAK-D show that, while WaSR cannot run due to memory re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;-&#35821;&#35328;&#27880;&#24847;&#21147;&#31574;&#30053;(SLAP)&#65292;&#20351;&#29992;&#19977;&#32500;&#26631;&#35760;&#20316;&#20026;&#36755;&#20837;&#34920;&#31034;&#65292;&#20197;&#35757;&#32451;&#21333;&#19968;&#22810;&#20219;&#21153;&#21644;&#35821;&#35328;&#26465;&#20214;&#21270;&#30340;&#21160;&#20316;&#39044;&#27979;&#31574;&#30053;&#65292;&#33021;&#22815;&#22312;&#24341;&#20837;&#26410;&#35265;&#36807;&#30340;&#24178;&#25200;&#29289;&#21644;&#29289;&#20307;&#37197;&#32622;&#26102;&#36798;&#21040;47.5%&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.11235</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#30340;&#31354;&#38388;-&#35821;&#35328;&#27880;&#24847;&#21147;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Spatial-Language Attention Policies for Efficient Robot Learning. (arXiv:2304.11235v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;-&#35821;&#35328;&#27880;&#24847;&#21147;&#31574;&#30053;(SLAP)&#65292;&#20351;&#29992;&#19977;&#32500;&#26631;&#35760;&#20316;&#20026;&#36755;&#20837;&#34920;&#31034;&#65292;&#20197;&#35757;&#32451;&#21333;&#19968;&#22810;&#20219;&#21153;&#21644;&#35821;&#35328;&#26465;&#20214;&#21270;&#30340;&#21160;&#20316;&#39044;&#27979;&#31574;&#30053;&#65292;&#33021;&#22815;&#22312;&#24341;&#20837;&#26410;&#35265;&#36807;&#30340;&#24178;&#25200;&#29289;&#21644;&#29289;&#20307;&#37197;&#32622;&#26102;&#36798;&#21040;47.5%&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;Transformer&#24314;&#31435;&#21644;&#35757;&#32451;&#26426;&#22120;&#20154;&#20915;&#31574;&#21046;&#23450;&#30340;&#31354;&#38388;&#34920;&#31034;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20026;&#20102;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#36816;&#34892;&#65292;&#25105;&#20204;&#24517;&#39035;&#33021;&#22815;&#24555;&#36895;&#35757;&#32451;&#25110;&#24494;&#35843;&#26426;&#22120;&#20154;&#24863;&#30693;&#21644;&#21160;&#20316;&#31574;&#30053;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#24773;&#20917;&#24182;&#20855;&#26377;&#25968;&#25454;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#12290;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;-&#35821;&#35328;&#27880;&#24847;&#21147;&#31574;&#30053;&#65288;SLAP&#65289;&#12290;SLAP&#20351;&#29992;&#19977;&#32500;&#26631;&#35760;&#20316;&#20026;&#36755;&#20837;&#34920;&#31034;&#65292;&#20197;&#35757;&#32451;&#21333;&#19968;&#22810;&#20219;&#21153;&#12289;&#35821;&#35328;&#26465;&#20214;&#21270;&#30340;&#21160;&#20316;&#39044;&#27979;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#23637;&#31034;&#20102;80%&#30340;&#25104;&#21151;&#29575;&#65292;&#36328;&#36234;&#20102;8&#39033;&#20219;&#21153;&#24182;&#20165;&#20351;&#29992;&#21333;&#19968;&#27169;&#22411;&#65292;&#22312;&#24341;&#20837;&#26410;&#35265;&#36807;&#30340;&#24178;&#25200;&#29289;&#21644;&#29289;&#20307;&#37197;&#32622;&#26102;&#20173;&#20445;&#25345;&#20102;47.5%&#30340;&#25104;&#21151;&#29575;&#65292;&#21363;&#20351;&#27599;&#20010;&#20219;&#21153;&#20165;&#20351;&#29992;&#23569;&#25968;&#31034;&#20363;&#12290;&#30456;&#23545;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#65288;&#20165;&#20351;&#29992;&#26410;&#35265;&#24178;&#25200;&#29289;&#21644;&#37197;&#32622;&#30340;&#24773;&#20917;&#19979;&#65292;&#25104;&#21151;&#29575;&#20026;20%&#65289;&#65292;&#36825;&#34920;&#31034;&#20102;30%&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate how to build and train spatial representations for robot decision making with Transformers. In particular, for robots to operate in a range of environments, we must be able to quickly train or fine-tune robot sensorimotor policies that are robust to clutter, data efficient, and generalize well to different circumstances. As a solution, we propose Spatial Language Attention Policies (SLAP). SLAP uses three-dimensional tokens as the input representation to train a single multi-task, language-conditioned action prediction policy. Our method shows 80% success rate in the real world across eight tasks with a single model, and a 47.5% success rate when unseen clutter and unseen object configurations are introduced, even with only a handful of examples per task. This represents an improvement of 30% over prior work (20% given unseen distractors and configurations).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#27431;&#30431;&#12289;&#32654;&#22269;&#21644;&#33521;&#22269;&#20851;&#20110;AI&#21487;&#35299;&#37322;&#24615;&#30340;&#25919;&#31574;&#21644;&#26631;&#20934;&#25991;&#29486;&#36827;&#34892;&#20102;&#20027;&#39064;&#21644;&#24046;&#36317;&#20998;&#26512;&#65292;&#26088;&#22312;&#35299;&#20915;&#30001;&#20110;&#32570;&#20047;&#20849;&#21516;&#30340;&#30417;&#31649;&#22522;&#32447;&#21644;&#35299;&#37322;&#30340;&#24773;&#22659;&#24615;&#32780;&#38590;&#20197;&#37319;&#29992;&#30340;&#21487;&#35299;&#37322;&#24615;&#20135;&#20986;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.11218</link><description>&lt;p&gt;
AI&#25919;&#31574;&#30340;&#21487;&#35299;&#37322;&#24615;&#65306;&#27431;&#30431;&#12289;&#32654;&#22269;&#21644;&#33521;&#22269;&#36890;&#20449;&#12289;&#25253;&#21578;&#12289;&#35268;&#21017;&#21644;&#26631;&#20934;&#30340;&#25209;&#21028;&#24615;&#23457;&#26597;
&lt;/p&gt;
&lt;p&gt;
Explainability in AI Policies: A Critical Review of Communications, Reports, Regulations, and Standards in the EU, US, and UK. (arXiv:2304.11218v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#27431;&#30431;&#12289;&#32654;&#22269;&#21644;&#33521;&#22269;&#20851;&#20110;AI&#21487;&#35299;&#37322;&#24615;&#30340;&#25919;&#31574;&#21644;&#26631;&#20934;&#25991;&#29486;&#36827;&#34892;&#20102;&#20027;&#39064;&#21644;&#24046;&#36317;&#20998;&#26512;&#65292;&#26088;&#22312;&#35299;&#20915;&#30001;&#20110;&#32570;&#20047;&#20849;&#21516;&#30340;&#30417;&#31649;&#22522;&#32447;&#21644;&#35299;&#37322;&#30340;&#24773;&#22659;&#24615;&#32780;&#38590;&#20197;&#37319;&#29992;&#30340;&#21487;&#35299;&#37322;&#24615;&#20135;&#20986;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20844;&#20247;&#23545;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31995;&#32479;&#30340;&#21487;&#35299;&#37322;&#24615;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20197;&#25552;&#20379;&#20154;&#31867;&#30417;&#30563;&#30340;&#26041;&#27861;&#12290;&#36825;&#24050;&#32463;&#36716;&#21270;&#20026;&#22823;&#37327;&#30340;&#30740;&#31350;&#25104;&#26524;&#65292;&#20363;&#22914;&#26469;&#33258;&#21487;&#35299;&#37322;AI&#30340;&#25104;&#26524;&#65292;&#20197;&#22686;&#24378;&#31995;&#32479;&#30340;&#36879;&#26126;&#24230;&#21644;&#25511;&#21046;&#65292;&#36827;&#34892;&#31995;&#32479;&#35843;&#35797;&#21644;&#30417;&#25511;&#65292;&#24182;&#25552;&#39640;&#29992;&#25143;&#26381;&#21153;&#30340;&#31995;&#32479;&#27969;&#31243;&#21644;&#36755;&#20986;&#30340;&#21487;&#29702;&#35299;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#20849;&#21516;&#30340;&#30417;&#31649;&#22522;&#32447;&#21644;&#35299;&#37322;&#30340;&#24773;&#22659;&#24615;&#65292;&#36825;&#20123;&#20135;&#20986;&#22312;&#23454;&#36341;&#23618;&#38754;&#19978;&#24456;&#38590;&#37319;&#29992;&#12290;&#25919;&#24220;&#25919;&#31574;&#29616;&#22312;&#27491;&#22312;&#23581;&#35797;&#35299;&#20915;&#36825;&#19968;&#36843;&#20999;&#38656;&#27714;&#65292;&#28982;&#32780;&#65292;&#21457;&#34920;&#30340;&#36890;&#20449;&#12289;&#35268;&#31456;&#21644;&#26631;&#20934;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#37319;&#29992;&#20102;&#26377;&#26681;&#25454;&#30340;&#35266;&#28857;&#65292;&#25903;&#25345;&#30740;&#31350;&#12289;&#20135;&#19994;&#21644;&#20844;&#27665;&#21033;&#30410;&#65292;&#36825;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#27431;&#30431;&#12289;&#32654;&#22269;&#21644;&#33521;&#22269;&#30340;&#21487;&#35299;&#37322;&#24615;&#25919;&#31574;&#21644;&#26631;&#20934;&#25991;&#29486;&#36827;&#34892;&#20102;&#39318;&#27425;&#20027;&#39064;&#21644;&#24046;&#36317;&#20998;&#26512;&#12290;&#36890;&#36807;&#23545;&#25919;&#31574;&#25991;&#20214;&#36827;&#34892;&#20005;&#26684;&#30340;&#35843;&#26597;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#36825;&#20010;&#22823;&#37327;&#30340;&#25919;&#31574;&#21644;&#26631;&#20934;&#20570;&#20986;&#20102;&#19968;&#20010;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Public attention towards explainability of artificial intelligence (AI) systems has been rising in recent years to offer methodologies for human oversight. This has translated into the proliferation of research outputs, such as from Explainable AI, to enhance transparency and control for system debugging and monitoring, and intelligibility of system process and output for user services. Yet, such outputs are difficult to adopt on a practical level due to a lack of a common regulatory baseline, and the contextual nature of explanations. Governmental policies are now attempting to tackle such exigence, however it remains unclear to what extent published communications, regulations, and standards adopt an informed perspective to support research, industry, and civil interests. In this study, we perform the first thematic and gap analysis of this plethora of policies and standards on explainability in the EU, US, and UK. Through a rigorous survey of policy documents, we first contribute an
&lt;/p&gt;</description></item><item><title>ACROCPoLis&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#20010;&#20849;&#20139;&#35789;&#27719;&#65292;&#26126;&#30830;&#20102;&#19981;&#21516;&#24773;&#20917;&#21644;&#31243;&#24207;&#30340;&#20844;&#24179;&#35780;&#20272;&#30456;&#20851;&#22240;&#32032;&#21450;&#20854;&#30456;&#20114;&#20851;&#31995;&#65292;&#35753;&#25105;&#20204;&#33021;&#22815;&#27604;&#36739;&#31867;&#20284;&#30340;&#24773;&#20917;&#65292;&#30830;&#23450;&#24433;&#21709;&#19981;&#21516;&#36793;&#32536;&#21270;&#32676;&#20307;&#30340;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2304.11217</link><description>&lt;p&gt;
ACROCPoLis: &#19968;&#20010;&#29992;&#20110;&#20844;&#24179;&#24615;&#20998;&#26512;&#30340;&#25551;&#36848;&#24615;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ACROCPoLis: A Descriptive Framework for Making Sense of Fairness. (arXiv:2304.11217v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11217
&lt;/p&gt;
&lt;p&gt;
ACROCPoLis&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#20010;&#20849;&#20139;&#35789;&#27719;&#65292;&#26126;&#30830;&#20102;&#19981;&#21516;&#24773;&#20917;&#21644;&#31243;&#24207;&#30340;&#20844;&#24179;&#35780;&#20272;&#30456;&#20851;&#22240;&#32032;&#21450;&#20854;&#30456;&#20114;&#20851;&#31995;&#65292;&#35753;&#25105;&#20204;&#33021;&#22815;&#27604;&#36739;&#31867;&#20284;&#30340;&#24773;&#20917;&#65292;&#30830;&#23450;&#24433;&#21709;&#19981;&#21516;&#36793;&#32536;&#21270;&#32676;&#20307;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#24179;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#36947;&#24503;&#21644;&#36127;&#36131;&#20219;&#30340;&#24320;&#21457;&#21644;&#20351;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#26377;&#35768;&#22810;&#31639;&#27861;&#20844;&#24179;&#30340;&#26694;&#26550;&#21644;&#24418;&#24335;&#27010;&#24565;&#21487;&#29992;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#25552;&#20986;&#30340;&#20844;&#24179;&#24615;&#35299;&#20915;&#26041;&#26696;&#37117;&#22260;&#32469;&#25216;&#26415;&#32771;&#34385;&#65292;&#32780;&#19981;&#26159;&#23545;&#26368;&#21463;&#24433;&#21709;&#31038;&#21306;&#30340;&#38656;&#27714;&#21644;&#21518;&#26524;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24819;&#25226;&#37325;&#28857;&#36716;&#31227;&#21040;&#23450;&#20041;&#20043;&#22806;&#65292;&#24182;&#20801;&#35768;&#21253;&#25324;&#31038;&#20250;&#21644;&#20851;&#31995;&#26041;&#38754;&#65292;&#20197;&#34920;&#31034;AI&#31995;&#32479;&#30340;&#24433;&#21709;&#22914;&#20309;&#24433;&#21709;&#20010;&#20154;&#21644;&#31038;&#20250;&#32676;&#20307;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;ACROCPoLis&#26694;&#26550;&#26469;&#36890;&#36807;&#24314;&#27169;&#24378;&#35843;&#20844;&#24179;&#24615;&#26041;&#38754;&#65292;&#20174;&#32780;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#20010;&#20849;&#20139;&#35789;&#27719;&#65292;&#26126;&#30830;&#20102;&#19981;&#21516;&#24773;&#20917;&#21644;&#31243;&#24207;&#30340;&#20844;&#24179;&#35780;&#20272;&#30456;&#20851;&#22240;&#32032;&#21450;&#20854;&#30456;&#20114;&#20851;&#31995;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#27604;&#36739;&#31867;&#20284;&#30340;&#24773;&#20917;&#65292;&#31361;&#20986;&#24046;&#24322;&#65292;&#20197;&#21450;&#30830;&#23450;&#24433;&#21709;&#19981;&#21516;&#36793;&#32536;&#21270;&#32676;&#20307;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fairness is central to the ethical and responsible development and use of AI systems, with a large number of frameworks and formal notions of algorithmic fairness being available. However, many of the fairness solutions proposed revolve around technical considerations and not the needs of and consequences for the most impacted communities. We therefore want to take the focus away from definitions and allow for the inclusion of societal and relational aspects to represent how the effects of AI systems impact and are experienced by individuals and social groups. In this paper, we do this by means of proposing the ACROCPoLis framework to represent allocation processes with a modeling emphasis on fairness aspects. The framework provides a shared vocabulary in which the factors relevant to fairness assessments for different situations and procedures are made explicit, as well as their interrelationships. This enables us to compare analogous situations, to highlight the differences in dissim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;ChatGPT&#20316;&#20026;&#19968;&#31181;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25152;&#24102;&#26469;&#30340;&#20262;&#29702;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#37319;&#21462;&#25216;&#26415;&#65288;&#25968;&#23383;&#27700;&#21360;&#12289;&#26679;&#24335;&#21270;&#12289;&#26816;&#27979;&#22120;&#21644;&#20107;&#23454;&#26680;&#26597;&#22120;&#65289;&#21644;&#38750;&#25216;&#26415;&#25514;&#26045;&#65288;&#20351;&#29992;&#26465;&#27454;&#12289;&#36879;&#26126;&#24230;&#21644;&#29992;&#25143;&#25945;&#32946;&#65289;&#26469;&#20943;&#36731;&#20854;&#21487;&#33021;&#25104;&#20026;&#22823;&#35268;&#27169;&#27450;&#39575;&#27494;&#22120;&#21644;&#20419;&#36827;&#27450;&#35784;&#29359;&#32618;&#30340;&#21361;&#38505;&#12290;</title><link>http://arxiv.org/abs/2304.11215</link><description>&lt;p&gt;
ChatGPT&#65306;&#19981;&#21482;&#26159;&#19968;&#31181;&#22823;&#35268;&#27169;&#27450;&#39575;&#27494;&#22120;&#65292;&#22522;&#20110;&#20154;&#26412;&#20027;&#20041;&#20154;&#24037;&#26234;&#33021;&#65288;HCAI&#65289;&#35270;&#35282;&#30340;&#20262;&#29702;&#25361;&#25112;&#19982;&#24212;&#23545;
&lt;/p&gt;
&lt;p&gt;
ChatGPT: More than a Weapon of Mass Deception, Ethical challenges and responses from the Human-Centered Artificial Intelligence (HCAI) perspective. (arXiv:2304.11215v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;ChatGPT&#20316;&#20026;&#19968;&#31181;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25152;&#24102;&#26469;&#30340;&#20262;&#29702;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#37319;&#21462;&#25216;&#26415;&#65288;&#25968;&#23383;&#27700;&#21360;&#12289;&#26679;&#24335;&#21270;&#12289;&#26816;&#27979;&#22120;&#21644;&#20107;&#23454;&#26680;&#26597;&#22120;&#65289;&#21644;&#38750;&#25216;&#26415;&#25514;&#26045;&#65288;&#20351;&#29992;&#26465;&#27454;&#12289;&#36879;&#26126;&#24230;&#21644;&#29992;&#25143;&#25945;&#32946;&#65289;&#26469;&#20943;&#36731;&#20854;&#21487;&#33021;&#25104;&#20026;&#22823;&#35268;&#27169;&#27450;&#39575;&#27494;&#22120;&#21644;&#20419;&#36827;&#27450;&#35784;&#29359;&#32618;&#30340;&#21361;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;ChatGPT&#20316;&#20026;&#19968;&#31181;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25152;&#20135;&#29983;&#30340;&#20262;&#29702;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#20154;&#26412;&#20027;&#20041;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#30340;&#24212;&#23545;&#12290;&#35813;&#26694;&#26550;&#29702;&#35299;&#25216;&#26415;&#39318;&#20808;&#26159;&#19968;&#31181;&#36171;&#33021;&#12289;&#22686;&#24378;&#21644;&#25552;&#21319;&#20154;&#31867;&#26426;&#33021;&#30340;&#24037;&#20855;&#65292;&#21516;&#26102;&#23558;&#20154;&#31867;&#31119;&#31049;&#20316;&#20026;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#23436;&#20840;&#19982;&#20262;&#29702;&#23398;&#65292;&#21363;&#20154;&#31867;&#32321;&#33635;&#30340;&#31185;&#23398;&#30456;&#19968;&#33268;&#12290;&#27492;&#22806;&#65292;HCAI&#20026;&#21487;&#38752;&#12289;&#23433;&#20840;&#21644;&#20540;&#24471;&#20449;&#36182;&#30340;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#20102;&#30446;&#26631;&#12289;&#21407;&#21017;&#12289;&#31243;&#24207;&#21644;&#32467;&#26500;&#65292;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#25105;&#20204;&#30340;ChatGPT&#35780;&#20272;&#20013;&#12290;ChatGPT&#25152;&#24102;&#26469;&#30340;&#20027;&#35201;&#21361;&#38505;&#26159;&#25104;&#20026;&#19968;&#31181;&#22823;&#35268;&#27169;&#27450;&#39575;&#65288;WMD&#65289;&#27494;&#22120;&#21644;&#28041;&#21450;&#27450;&#35784;&#30340;&#29359;&#32618;&#27963;&#21160;&#30340;&#20419;&#36827;&#22240;&#32032;&#12290;&#25105;&#20204;&#22238;&#39038;&#20102;&#25216;&#26415;&#35268;&#26684;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#23427;&#30340;&#28508;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;&#28982;&#21518;&#25105;&#20204;&#24314;&#35758;&#23454;&#26045;&#25216;&#26415;&#25514;&#26045;&#65288;&#25968;&#23383;&#27700;&#21360;&#12289;&#26679;&#24335;&#21270;&#12289;&#26816;&#27979;&#22120;&#21644;&#20107;&#23454;&#26680;&#26597;&#22120;&#65289;&#21644;&#38750;&#25216;&#26415;&#25514;&#26045;&#65288;&#20351;&#29992;&#26465;&#27454;&#12289;&#36879;&#26126;&#24230;&#21644;&#29992;&#25143;&#25945;&#32946;&#65289;&#26469;&#20943;&#36731;ChatGPT&#25152;&#24102;&#26469;&#30340;&#20262;&#29702;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article explores the ethical problems arising from the use of ChatGPT as a kind of generative AI and suggests responses based on the Human-Centered Artificial Intelligence (HCAI) framework. The HCAI framework is appropriate because it understands technology above all as a tool to empower, augment, and enhance human agency while referring to human wellbeing as a grand challenge, thus perfectly aligning itself with ethics, the science of human flourishing. Further, HCAI provides objectives, principles, procedures, and structures for reliable, safe, and trustworthy AI which we apply to our ChatGPT assessments. The main danger ChatGPT presents is the propensity to be used as a weapon of mass deception (WMD) and an enabler of criminal activities involving deceit. We review technical specifications to better comprehend its potentials and limitations. We then suggest both technical (watermarking, styleme, detectors, and fact-checkers) and non-technical measures (terms of use, transparenc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#23558;ChatGPT&#29992;&#20316;&#26412;&#31185;&#35745;&#31639;&#26426;&#31185;&#23398;&#35838;&#31243;&#23398;&#20064;&#21644;&#35780;&#20272;&#24037;&#20855;&#30340;&#21069;&#26223;&#21644;&#38556;&#30861;&#65292;&#36890;&#36807;&#23545;&#25968;&#25454;&#32467;&#26500;&#21644;&#31639;&#27861;&#35838;&#31243;&#30340;&#23398;&#29983;&#36827;&#34892;&#30701;&#26399;&#32534;&#31243;&#20219;&#21153;&#30340;&#35843;&#26597;&#65292;&#35777;&#26126;&#20102;ChatGPT&#33021;&#22815;&#25552;&#21319;&#23398;&#29983;&#30340;&#21442;&#19982;&#24230;&#12289;&#21512;&#20316;&#24615;&#12289;&#21487;&#35775;&#38382;&#24615;&#21644;&#21487;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.11214</link><description>&lt;p&gt;
&#25506;&#32034;&#22312;&#26412;&#31185;&#35745;&#31639;&#26426;&#31185;&#23398;&#35838;&#31243;&#20013;&#23558;ChatGPT&#29992;&#20316;&#23398;&#20064;&#21644;&#35780;&#20272;&#24037;&#20855;&#30340;&#26426;&#20250;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Exploring the Use of ChatGPT as a Tool for Learning and Assessment in Undergraduate Computer Science Curriculum: Opportunities and Challenges. (arXiv:2304.11214v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11214
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#23558;ChatGPT&#29992;&#20316;&#26412;&#31185;&#35745;&#31639;&#26426;&#31185;&#23398;&#35838;&#31243;&#23398;&#20064;&#21644;&#35780;&#20272;&#24037;&#20855;&#30340;&#21069;&#26223;&#21644;&#38556;&#30861;&#65292;&#36890;&#36807;&#23545;&#25968;&#25454;&#32467;&#26500;&#21644;&#31639;&#27861;&#35838;&#31243;&#30340;&#23398;&#29983;&#36827;&#34892;&#30701;&#26399;&#32534;&#31243;&#20219;&#21153;&#30340;&#35843;&#26597;&#65292;&#35777;&#26126;&#20102;ChatGPT&#33021;&#22815;&#25552;&#21319;&#23398;&#29983;&#30340;&#21442;&#19982;&#24230;&#12289;&#21512;&#20316;&#24615;&#12289;&#21487;&#35775;&#38382;&#24615;&#21644;&#21487;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#25945;&#32946;&#39046;&#22495;&#65292;&#24212;&#29992;&#20154;&#24037;&#26234;&#33021;&#36827;&#34892;&#25945;&#23398;&#21644;&#23398;&#20064;&#26159;&#19968;&#20010;&#22791;&#21463;&#20851;&#27880;&#30340;&#36235;&#21183;&#12290;&#20316;&#20026;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24037;&#20855;&#65292;ChatGPT&#25552;&#20379;&#20102;&#35768;&#22810;&#20248;&#21183;&#65292;&#22914;&#22686;&#24378;&#20102;&#23398;&#29983;&#30340;&#21442;&#19982;&#24230;&#12289;&#21512;&#20316;&#24615;&#12289;&#21487;&#35775;&#38382;&#24615;&#21644;&#21487;&#29992;&#24615;&#12290;&#26412;&#25991;&#23601;&#22312;&#26412;&#31185;&#35745;&#31639;&#26426;&#31185;&#23398;&#35838;&#31243;&#20013;&#23558;ChatGPT&#29992;&#20316;&#23398;&#20064;&#21644;&#35780;&#20272;&#24037;&#20855;&#30340;&#21069;&#26223;&#21644;&#38556;&#30861;&#36827;&#34892;&#20102;&#35752;&#35770;&#65292;&#29305;&#21035;&#26159;&#22312;&#25945;&#25480;&#22522;&#30784;&#32534;&#31243;&#35838;&#31243;&#26041;&#38754;&#12290;&#26412;&#30740;&#31350;&#38024;&#23545;&#24050;&#23436;&#25104;&#25968;&#25454;&#32467;&#26500;&#21644;&#31639;&#27861;&#65288;&#19968;&#38376;&#20108;&#24180;&#32423;&#35838;&#31243;&#65289;&#35838;&#31243;&#20316;&#19994;&#30340;&#23398;&#29983;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#20004;&#32452;&#23398;&#29983;&#34987;&#35201;&#27714;&#22312;&#30701;&#26102;&#38388;&#20869;&#35299;&#20915;&#32534;&#31243;&#25361;&#25112;&#38382;&#39064;&#12290;&#23545;&#29031;&#32452;&#65288;A&#32452;&#65289;&#21482;&#26377;&#32534;&#31243;&#35838;&#31243;&#30340;&#35838;&#26412;&#21644;&#31508;&#35760;&#65292;&#24182;&#27809;&#26377;&#25552;&#20379;&#20114;&#32852;&#32593;&#35775;&#38382;&#26435;&#38480;&#12290;&#32780;B&#32452;&#30340;&#23398;&#29983;&#21487;&#20197;&#20351;&#29992;ChatGPT&#65292;&#24182;&#40723;&#21169;&#20182;&#20204;&#20351;&#29992;&#23427;&#26469;&#24110;&#21161;&#35299;&#20915;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The application of Artificial intelligence for teaching and learning in the academic sphere is a trending subject of interest in the computing education. ChatGPT, as an AI-based tool, provides various advantages, such as heightened student involvement, cooperation, accessibility and availability. This paper addresses the prospects and obstacles associated with utilizing ChatGPT as a tool for learning and assessment in undergraduate Computer Science curriculum in particular to teaching and learning fundamental programming courses. Students having completed the course work for a Data Structures and Algorithms (a sophomore level course) participated in this study. Two groups of students were given programming challenges to solve within a short period of time. The control group (group A) had access to text books and notes of programming courses, however no Internet access was provided. Group B students were given access to ChatGPT and were encouraged to use it to help solve the programming
&lt;/p&gt;</description></item><item><title>SSS3D&#26159;&#20026;&#20102;&#39640;&#25928;&#19977;&#32500;&#35821;&#20041;&#22330;&#26223;&#20998;&#21106;&#32780;&#35774;&#35745;&#30340;&#24555;&#36895;&#22810;&#30446;&#26631;&#31070;&#32463;&#20307;&#31995;&#32467;&#26500;&#25628;&#32034;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#36229;&#32423;&#32593;&#32476;RandLA-Net&#23454;&#29616;&#26435;&#37325;&#20849;&#20139;&#24182;&#26174;&#33879;&#20943;&#23569;&#25628;&#32034;&#26102;&#38388;&#65292;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#25628;&#32034;&#22312;&#26356;&#30701;&#30340;&#26102;&#38388;&#20869;&#25214;&#21040;&#26368;&#20339;&#23376;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2304.11207</link><description>&lt;p&gt;
SSS3D&#65306;&#24555;&#36895;&#30340;&#31070;&#32463;&#20307;&#31995;&#32467;&#26500;&#25628;&#32034;&#65292;&#29992;&#20110;&#39640;&#25928;&#19977;&#32500;&#35821;&#20041;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
SSS3D: Fast Neural Architecture Search For Efficient Three-Dimensional Semantic Segmentation. (arXiv:2304.11207v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11207
&lt;/p&gt;
&lt;p&gt;
SSS3D&#26159;&#20026;&#20102;&#39640;&#25928;&#19977;&#32500;&#35821;&#20041;&#22330;&#26223;&#20998;&#21106;&#32780;&#35774;&#35745;&#30340;&#24555;&#36895;&#22810;&#30446;&#26631;&#31070;&#32463;&#20307;&#31995;&#32467;&#26500;&#25628;&#32034;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#36229;&#32423;&#32593;&#32476;RandLA-Net&#23454;&#29616;&#26435;&#37325;&#20849;&#20139;&#24182;&#26174;&#33879;&#20943;&#23569;&#25628;&#32034;&#26102;&#38388;&#65292;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#25628;&#32034;&#22312;&#26356;&#30701;&#30340;&#26102;&#38388;&#20869;&#25214;&#21040;&#26368;&#20339;&#23376;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;SSS3D&#65292;&#36825;&#26159;&#19968;&#20010;&#24555;&#36895;&#30340;&#22810;&#30446;&#26631;NAS&#26694;&#26550;&#65292;&#19987;&#20026;&#26597;&#25214;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#19977;&#32500;&#35821;&#20041;&#22330;&#26223;&#20998;&#21106;&#32593;&#32476;&#32780;&#35774;&#35745;&#12290;&#23427;&#20351;&#29992;RandLA-Net&#20316;&#20026;&#36229;&#32423;&#32593;&#32476;&#65292;&#20197;&#23454;&#29616;&#26435;&#37325;&#20849;&#20139;&#65292;&#24182;&#36890;&#36807;&#21333;&#38454;&#27573;&#25628;&#32034;&#23558;&#25628;&#32034;&#26102;&#38388;&#20943;&#23569;99.67&#65285;&#12290;SSS3D&#20855;&#26377;&#22797;&#26434;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#30001;&#37319;&#26679;&#21644;&#26550;&#26500;&#21442;&#25968;&#32452;&#25104;&#65292;&#21487;&#20197;&#24418;&#25104;2.88*10^17&#31181;&#21487;&#33021;&#30340;&#32593;&#32476;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#20943;&#23569;&#25628;&#32034;&#26102;&#38388;&#65292;SSS3D&#23558;&#23436;&#25972;&#30340;&#25628;&#32034;&#31354;&#38388;&#20998;&#25104;&#20004;&#20010;&#38454;&#27573;&#65292;&#20197;&#21333;&#38454;&#27573;&#25628;&#32034;&#25152;&#38656;&#26102;&#38388;&#30340;54&#65285;&#25214;&#21040;&#26368;&#20339;&#23376;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present SSS3D, a fast multi-objective NAS framework designed to find computationally efficient 3D semantic scene segmentation networks. It uses RandLA-Net, an off-the-shelf point-based network, as a super-network to enable weight sharing and reduce search time by 99.67% for single-stage searches. SSS3D has a complex search space composed of sampling and architectural parameters that can form 2.88 * 10^17 possible networks. To further reduce search time, SSS3D splits the complete search space and introduces a two-stage search that finds optimal subnetworks in 54% of the time required by single-stage searches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#36793;&#32536;&#35745;&#31639;&#26426;&#22120;&#20154;&#25235;&#21462;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#23450;&#21046;&#30340;&#22810;&#20219;&#21153;&#28145;&#24230;&#33258;&#27880;&#24847;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21517;&#20026;&#24555;&#36895;GraspNeXt&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#24555;&#36895;GraspNeXt&#22312;&#31934;&#24230;&#21644;&#36895;&#24230;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2304.11196</link><description>&lt;p&gt;
&#24555;&#36895;GraspNeXt&#65306;&#29992;&#20110;&#36793;&#32536;&#35745;&#31639;&#26426;&#22120;&#20154;&#25235;&#21462;&#30340;&#22810;&#20219;&#21153;&#35270;&#35273;&#23398;&#20064;&#30340;&#24555;&#36895;&#33258;&#27880;&#24847;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Fast GraspNeXt: A Fast Self-Attention Neural Network Architecture for Multi-task Learning in Computer Vision Tasks for Robotic Grasping on the Edge. (arXiv:2304.11196v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#36793;&#32536;&#35745;&#31639;&#26426;&#22120;&#20154;&#25235;&#21462;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#23450;&#21046;&#30340;&#22810;&#20219;&#21153;&#28145;&#24230;&#33258;&#27880;&#24847;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21517;&#20026;&#24555;&#36895;GraspNeXt&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#24555;&#36895;GraspNeXt&#22312;&#31934;&#24230;&#21644;&#36895;&#24230;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#24050;&#32463;&#34987;&#35777;&#26126;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#39537;&#21160;&#30340;&#35270;&#35273;&#31995;&#32479;&#22312;&#26426;&#22120;&#20154;&#25235;&#21462;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#39640;&#26550;&#26500;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#21487;&#33021;&#23548;&#33268;&#22312;&#23454;&#38469;&#21046;&#36896;&#21644;&#20179;&#24211;&#29615;&#22659;&#20013;&#36890;&#24120;&#29992;&#20110;&#26426;&#22120;&#20154;&#25163;&#33218;&#30340;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#37096;&#32626;&#25928;&#26524;&#36739;&#24046;&#12290;&#22240;&#27492;&#65292;&#35774;&#35745;&#39640;&#25928;&#30340;&#12289;&#38024;&#23545;&#36793;&#32536;&#35745;&#31639;&#26426;&#22120;&#20154;&#25235;&#21462;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#23450;&#21046;&#30340;&#22810;&#20219;&#21153;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#23545;&#20110;&#22312;&#21046;&#36896;&#29615;&#22659;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#38750;&#24120;&#24517;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24555;&#36895;GraspNeXt&#65292;&#19968;&#31181;&#38024;&#23545;&#23884;&#20837;&#24335;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#24555;&#36895;&#33258;&#27880;&#24847;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#30340;&#26426;&#22120;&#20154;&#25235;&#21462;&#12290;&#36890;&#36807;&#37319;&#29992;&#19968;&#20010;&#22522;&#20110;&#29983;&#25104;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#31574;&#30053;&#21644;&#19968;&#32452;&#29992;&#20110;&#23454;&#29616;&#22810;&#20219;&#21153;&#23398;&#20064;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#39640;&#24230;&#24179;&#34913;&#30340;&#26550;&#26500;&#32422;&#26463;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#24555;&#36895;GraspNeXt&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;&#24555;&#36895;GraspNeXt&#22312;&#31934;&#24230;&#21644;&#36895;&#24230;&#26041;&#38754;&#22343;&#20248;&#20110;&#30446;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-task learning has shown considerable promise for improving the performance of deep learning-driven vision systems for the purpose of robotic grasping. However, high architectural and computational complexity can result in poor suitability for deployment on embedded devices that are typically leveraged in robotic arms for real-world manufacturing and warehouse environments. As such, the design of highly efficient multi-task deep neural network architectures tailored for computer vision tasks for robotic grasping on the edge is highly desired for widespread adoption in manufacturing environments. Motivated by this, we propose Fast GraspNeXt, a fast self-attention neural network architecture tailored for embedded multi-task learning in computer vision tasks for robotic grasping. To build Fast GraspNeXt, we leverage a generative network architecture search strategy with a set of architectural constraints customized to achieve a strong balance between multi-task learning performance a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#23558;&#35302;&#35273;&#21453;&#39304;&#38598;&#25104;&#21040;&#29289;&#29702;&#26426;&#22120;&#20154;&#20132;&#20114;&#30340;&#35270;&#39057;&#39044;&#27979;&#27169;&#22411;&#20013;&#30340;&#24433;&#21709;&#65292;&#24182;&#20171;&#32461;&#20102;&#20004;&#20010;&#26032;&#30340;&#26426;&#22120;&#20154;&#25512;&#21160;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#22522;&#20110;&#30913;&#24615;&#30340;&#35302;&#35273;&#20256;&#24863;&#22120;&#36827;&#34892;&#26080;&#30417;&#30563;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2304.11193</link><description>&lt;p&gt;
&#35270;&#35273;&#21644;&#35302;&#35273;&#24863;&#35273;&#30456;&#32467;&#21512;&#30340;&#35270;&#39057;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Combining Vision and Tactile Sensation for Video Prediction. (arXiv:2304.11193v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#23558;&#35302;&#35273;&#21453;&#39304;&#38598;&#25104;&#21040;&#29289;&#29702;&#26426;&#22120;&#20154;&#20132;&#20114;&#30340;&#35270;&#39057;&#39044;&#27979;&#27169;&#22411;&#20013;&#30340;&#24433;&#21709;&#65292;&#24182;&#20171;&#32461;&#20102;&#20004;&#20010;&#26032;&#30340;&#26426;&#22120;&#20154;&#25512;&#21160;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#22522;&#20110;&#30913;&#24615;&#30340;&#35302;&#35273;&#20256;&#24863;&#22120;&#36827;&#34892;&#26080;&#30417;&#30563;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#23558;&#35302;&#35273;&#24863;&#35273;&#28155;&#21152;&#21040;&#29289;&#29702;&#26426;&#22120;&#20154;&#20132;&#20114;&#30340;&#35270;&#39057;&#39044;&#27979;&#27169;&#22411;&#20013;&#30340;&#24433;&#21709;&#12290;&#39044;&#27979;&#26426;&#22120;&#20154;&#34892;&#20026;&#23545;&#29615;&#22659;&#30340;&#24433;&#21709;&#26159;&#26426;&#22120;&#20154;&#25216;&#26415;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#21033;&#29992;&#35270;&#35273;&#21644;&#26426;&#22120;&#20154;&#21160;&#20316;&#25968;&#25454;&#26469;&#29983;&#25104;&#19968;&#23450;&#26102;&#38388;&#27573;&#20869;&#30340;&#35270;&#39057;&#39044;&#27979;&#65292;&#28982;&#21518;&#21487;&#20197;&#29992;&#20110;&#35843;&#25972;&#26426;&#22120;&#20154;&#21160;&#20316;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#20381;&#36182;&#20110;&#35270;&#35273;&#21644;&#35302;&#35273;&#21453;&#39304;&#26469;&#21457;&#23637;&#21644;&#32500;&#25252;&#20182;&#20204;&#23545;&#29289;&#29702;&#29615;&#22659;&#30340;&#24515;&#29702;&#27169;&#22411;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#35302;&#35273;&#21453;&#39304;&#38598;&#25104;&#21040;&#29289;&#29702;&#26426;&#22120;&#20154;&#20132;&#20114;&#30340;&#35270;&#39057;&#39044;&#27979;&#27169;&#22411;&#20013;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#22810;&#27169;&#24577;&#38598;&#25104;&#26041;&#27861;&#65292;&#24182;&#27604;&#36739;&#20102;&#36825;&#20123;&#35302;&#35273;&#22686;&#24378;&#30340;&#35270;&#39057;&#39044;&#27979;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#20004;&#20010;&#26032;&#30340;&#26426;&#22120;&#20154;&#25512;&#21160;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#22522;&#20110;&#30913;&#24615;&#30340;&#35302;&#35273;&#20256;&#24863;&#22120;&#36827;&#34892;&#26080;&#30417;&#30563;&#23398;&#20064;&#12290;&#31532;&#19968;&#20010;&#25968;&#25454;&#38598;&#21253;&#21547;&#20855;&#26377;&#19981;&#21516;&#29289;&#29702;&#29305;&#24615;&#30340;&#35270;&#35273;&#19978;&#30456;&#21516;&#30340;&#23545;&#35937;&#65292;&#31532;&#20108;&#20010;&#25968;&#25454;&#38598;&#29992;&#20110;&#27979;&#35797;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore the impact of adding tactile sensation to video prediction models for physical robot interactions. Predicting the impact of robotic actions on the environment is a fundamental challenge in robotics. Current methods leverage visual and robot action data to generate video predictions over a given time period, which can then be used to adjust robot actions. However, humans rely on both visual and tactile feedback to develop and maintain a mental model of their physical surroundings. In this paper, we investigate the impact of integrating tactile feedback into video prediction models for physical robot interactions. We propose three multi-modal integration approaches and compare the performance of these tactile-enhanced video prediction models. Additionally, we introduce two new datasets of robot pushing that use a magnetic-based tactile sensor for unsupervised learning. The first dataset contains visually identical objects with different physical properties, whil
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#20219;&#21153;&#33258;&#36866;&#24212;&#20266;&#26631;&#31614;&#8221;&#30340;&#36328;&#24863;&#30693;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20266;&#26631;&#31614;&#29983;&#25104;&#26410;&#26631;&#35760;&#30340;&#26597;&#35810;&#38598;&#65292;&#20351;&#29992;&#30417;&#30563;&#35774;&#32622;&#24182;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#26597;&#35810;&#38598;&#65292;&#21487;&#20197;&#22788;&#29702;&#26356;&#22810;&#23454;&#20363;&#65292;&#20174;&#32780;&#24102;&#26469;&#26356;&#22909;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.11173</link><description>&lt;p&gt;
&#22522;&#20110;&#20219;&#21153;&#33258;&#36866;&#24212;&#20266;&#26631;&#31614;&#30340;&#36328;&#24863;&#30693;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Task-Adaptive Pseudo Labeling for Transductive Meta-Learning. (arXiv:2304.11173v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#20219;&#21153;&#33258;&#36866;&#24212;&#20266;&#26631;&#31614;&#8221;&#30340;&#36328;&#24863;&#30693;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20266;&#26631;&#31614;&#29983;&#25104;&#26410;&#26631;&#35760;&#30340;&#26597;&#35810;&#38598;&#65292;&#20351;&#29992;&#30417;&#30563;&#35774;&#32622;&#24182;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#26597;&#35810;&#38598;&#65292;&#21487;&#20197;&#22788;&#29702;&#26356;&#22810;&#23454;&#20363;&#65292;&#20174;&#32780;&#24102;&#26469;&#26356;&#22909;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#23398;&#20064;&#36890;&#36807;&#26377;&#38480;&#25968;&#37327;&#30340;&#25903;&#25345;&#38598;&#26469;&#36827;&#34892;&#36866;&#24212;&#24615;&#23398;&#20064;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#26679;&#26412;&#20559;&#24046;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36328;&#24863;&#30693;&#20803;&#23398;&#20064;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#24863;&#30693;&#24615;&#23398;&#20064;&#35270;&#35282;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#20219;&#21153;&#33258;&#36866;&#24212;&#20266;&#26631;&#31614;&#8221;&#30340;&#36328;&#24863;&#30693;&#20803;&#23398;&#20064;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#21033;&#29992;&#26631;&#35760;&#30340;&#25903;&#25345;&#38598;&#29983;&#25104;&#26410;&#26631;&#35760;&#30340;&#26597;&#35810;&#38598;&#30340;&#20266;&#26631;&#31614;&#65292;&#20511;&#27492;&#20351;&#24471;&#22312;&#36866;&#24212;&#36807;&#31243;&#20013;&#21487;&#20197;&#37319;&#29992;&#30417;&#30563;&#35774;&#32622;&#24182;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#26597;&#35810;&#38598;&#12290;&#32467;&#26524;&#65292;&#35813;&#26041;&#27861;&#27604;&#24863;&#30693;&#24615;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#26356;&#22810;&#30340;&#31034;&#20363;&#65292;&#20174;&#32780;&#21487;&#20197;&#20135;&#29983;&#26356;&#22909;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;&#38656;&#35201;&#27880;&#24847;&#30340;&#26159;&#65292;&#35813;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#39318;&#20010;&#23558;&#20219;&#21153;&#33258;&#36866;&#24212;&#24212;&#29992;&#20110;&#20266;&#26631;&#31614;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;5&#36335;1-shot few-shot&#20013;&#32988;&#36807;&#20102;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta-learning performs adaptation through a limited amount of support set, which may cause a sample bias problem. To solve this problem, transductive meta-learning is getting more and more attention, going beyond the conventional inductive learning perspective. This paper proposes so-called task-adaptive pseudo labeling for transductive meta-learning. Specifically, pseudo labels for unlabeled query sets are generated from labeled support sets through label propagation. Pseudo labels enable to adopt the supervised setting as it is and also use the unlabeled query set in the adaptation process. As a result, the proposed method is able to deal with more examples in the adaptation process than inductive ones, which can result in better classification performance of the model. Note that the proposed method is the first approach of applying task adaptation to pseudo labeling. Experiments show that the proposed method outperforms the state-of-the-art (SOTA) technique in 5-way 1-shot few-shot 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39063;&#31890;&#29699;&#35745;&#31639;&#30340;&#33258;&#36866;&#24212;&#22810;&#31890;&#24230;&#34920;&#31034;&#21644;&#35745;&#31639;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#30340;&#25928;&#29575;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.11171</link><description>&lt;p&gt;
&#39063;&#31890;&#29699;&#35745;&#31639;&#65306;&#19968;&#31181;&#39640;&#25928;&#12289;&#40065;&#26834;&#21644;&#21487;&#35299;&#37322;&#30340;&#33258;&#36866;&#24212;&#22810;&#31890;&#24230;&#34920;&#31034;&#21644;&#35745;&#31639;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Granular ball computing: an efficient, robust, and interpretable adaptive multi-granularity representation and computation method. (arXiv:2304.11171v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39063;&#31890;&#29699;&#35745;&#31639;&#30340;&#33258;&#36866;&#24212;&#22810;&#31890;&#24230;&#34920;&#31034;&#21644;&#35745;&#31639;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#30340;&#25928;&#29575;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#35748;&#30693;&#20855;&#26377;&#8220;&#20808;&#22823;&#21518;&#23567;&#8221;&#30340;&#35748;&#30693;&#26426;&#21046;&#65292;&#22240;&#27492;&#20855;&#26377;&#33258;&#36866;&#24212;&#30340;&#22810;&#31890;&#24230;&#25551;&#36848;&#33021;&#21147;&#12290;&#36825;&#23548;&#33268;&#20102;&#26377;&#25928;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#31561;&#35745;&#31639;&#29305;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#39063;&#31890;&#29699;&#35745;&#31639;&#30340;&#33258;&#36866;&#24212;&#22810;&#31890;&#24230;&#34920;&#31034;&#21644;&#35745;&#31639;&#26041;&#27861;&#12290;&#20182;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#20960;&#20010;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#35777;&#26126;&#20854;&#30456;&#23545;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human cognition has a ``large-scale first'' cognitive mechanism, therefore possesses adaptive multi-granularity description capabilities. This results in computational characteristics such as efficiency, robustness, and interpretability. Although most existing artificial intelligence learning methods have certain multi-granularity features, they do not fully align with the ``large-scale first'' cognitive mechanism. Multi-granularity granular-ball computing is an important model method developed in recent years. This method can use granular-balls of different sizes to adaptively represent and cover the sample space, and perform learning based on granular-balls. Since the number of coarse-grained "granular-ball" is smaller than the number of sample points, granular-ball computing is more efficient; the coarse-grained characteristics of granular-balls are less likely to be affected by fine-grained sample points, making them more robust; the multi-granularity structure of granular-balls ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36777;&#35777;&#35780;&#20272;&#26041;&#24335;&#65292;&#26088;&#22312;&#25551;&#32472;&#35821;&#35328;&#27169;&#22411;&#31995;&#32479;&#22833;&#36133;&#30340;&#36793;&#30028;&#24182;&#26816;&#26597;&#20854;&#19968;&#33268;&#24615;&#65292;&#20197;&#21450;&#23545;LLMs&#36890;&#35782;&#31354;&#38388;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#20102;&#21021;&#27493;&#23450;&#24615;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2304.11164</link><description>&lt;p&gt;
&#36777;&#35777;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#65306;&#23545;LLMs&#36890;&#35782;&#31354;&#38388;&#25512;&#29702;&#33021;&#21147;&#30340;&#21021;&#27493;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs. (arXiv:2304.11164v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36777;&#35777;&#35780;&#20272;&#26041;&#24335;&#65292;&#26088;&#22312;&#25551;&#32472;&#35821;&#35328;&#27169;&#22411;&#31995;&#32479;&#22833;&#36133;&#30340;&#36793;&#30028;&#24182;&#26816;&#26597;&#20854;&#19968;&#33268;&#24615;&#65292;&#20197;&#21450;&#23545;LLMs&#36890;&#35782;&#31354;&#38388;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#20102;&#21021;&#27493;&#23450;&#24615;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35821;&#35328;&#27169;&#22411;&#38750;&#24120;&#21463;&#27426;&#36814;&#65292;&#24182;&#19988;&#20854;&#33021;&#21147;&#24471;&#21040;&#20102;&#35768;&#22810;&#36190;&#35465;&#65292;&#21253;&#25324;&#36890;&#35782;&#25512;&#29702;&#12290;&#37492;&#20110;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#22312;&#20197;&#24448;&#36890;&#35782;&#25512;&#29702;&#38745;&#24577;&#22522;&#20934;&#19978;&#21462;&#24471;&#36234;&#26469;&#36234;&#22909;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#21478;&#31867;&#30340;&#36777;&#35777;&#35780;&#20272;&#26041;&#24335;&#12290;&#36825;&#31181;&#35780;&#20272;&#30340;&#30446;&#26631;&#19981;&#26159;&#33719;&#24471;&#19968;&#20010;&#24635;&#20307;&#24615;&#33021;&#20540;&#65292;&#32780;&#26159;&#25214;&#21040;&#22833;&#36133;&#30340;&#22320;&#26041;&#24182;&#25551;&#32472;&#31995;&#32479;&#30340;&#36793;&#30028;&#12290;&#19982;&#31995;&#32479;&#23545;&#35805;&#21487;&#20197;&#26816;&#26597;&#20854;&#19968;&#33268;&#24615;&#65292;&#24182;&#33719;&#24471;&#36234;&#36807;&#21333;&#19968;&#35777;&#25454;&#30340;&#36793;&#30028;&#20445;&#35777;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23601;&#31354;&#38388;&#25512;&#29702;&#65288;&#36890;&#35782;&#25512;&#29702;&#30340;&#22522;&#26412;&#26041;&#38754;&#65289;&#30340;&#29305;&#23450;&#24773;&#20917;&#36827;&#34892;&#20102;&#19968;&#20123;&#23450;&#24615;&#30740;&#31350;&#12290;&#32467;&#35770;&#21253;&#25324;&#23545;&#26410;&#26469;&#24037;&#20316;&#30340;&#19968;&#20123;&#24314;&#35758;&#65292;&#26088;&#22312;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#24182;&#23558;&#36825;&#31181;&#36777;&#35777;&#35780;&#20272;&#31995;&#32479;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models have become very popular recently and many claims have been made about their abilities, including for commonsense reasoning. Given the increasingly better results of current language models on previous static benchmarks for commonsense reasoning, we explore an alternative dialectical evaluation. The goal of this kind of evaluation is not to obtain an aggregate performance value but to find failures and map the boundaries of the system. Dialoguing with the system gives the opportunity to check for consistency and get more reassurance of these boundaries beyond anecdotal evidence. In this paper we conduct some qualitative investigations of this kind of evaluation for the particular case of spatial reasoning (which is a fundamental aspect of commonsense reasoning). We conclude with some suggestions for future work both to improve the capabilities of language models and to systematise this kind of dialectical evaluation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;Graph-ToolFormer&#26694;&#26550;&#36171;&#20104;LLMs&#22270;&#24418;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#29616;&#26377;LLMs&#22312;&#25191;&#34892;&#22270;&#24418;&#23398;&#20064;&#20219;&#21153;&#20013;&#23384;&#22312;&#30340;&#22266;&#26377;&#24369;&#28857;&#12290;</title><link>http://arxiv.org/abs/2304.11116</link><description>&lt;p&gt;
Graph-ToolFormer: &#36890;&#36807;ChatGPT&#22686;&#24378;&#30340;&#25552;&#31034;&#65292;&#36171;&#20104;LLMs&#22270;&#24418;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT. (arXiv:2304.11116v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;Graph-ToolFormer&#26694;&#26550;&#36171;&#20104;LLMs&#22270;&#24418;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#29616;&#26377;LLMs&#22312;&#25191;&#34892;&#22270;&#24418;&#23398;&#20064;&#20219;&#21153;&#20013;&#23384;&#22312;&#30340;&#22266;&#26377;&#24369;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#33021;&#22815;&#23545;&#22797;&#26434;&#22270;&#24418;&#25968;&#25454;&#36827;&#34892;&#25512;&#29702;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#24403;&#21069;&#65292;LLMs&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#23398;&#20064;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#38750;&#24120;&#20986;&#33394;&#30340;&#34920;&#29616;&#65292;&#36825;&#20123;&#25193;&#23637;&#20063;&#24050;&#34987;&#24212;&#29992;&#20110;&#30740;&#31350;&#20855;&#26377;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#35270;&#35273;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;&#22270;&#24418;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#29616;&#26377;&#30340;LLMs&#30001;&#20110;&#22312;&#25191;&#34892;&#22810;&#27493;&#36923;&#36753;&#25512;&#29702;&#12289;&#31934;&#30830;&#30340;&#25968;&#23398;&#35745;&#31639;&#20197;&#21450;&#23545;&#31354;&#38388;&#21644;&#26102;&#38388;&#22240;&#32032;&#30340;&#24863;&#30693;&#26041;&#38754;&#23384;&#22312;&#19968;&#20123;&#22266;&#26377;&#24369;&#28857;&#65292;&#22240;&#27492;&#21576;&#29616;&#20986;&#38750;&#24120;&#20005;&#37325;&#30340;&#32570;&#38519;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#23558;&#35843;&#26597;&#25506;&#32034;&#36171;&#20104;&#29616;&#26377;LLMs&#22270;&#24418;&#25512;&#29702;&#33021;&#21147;&#30340;&#21407;&#29702;&#12289;&#26041;&#27861;&#21644;&#31639;&#27861;&#65292;&#36825;&#23558;&#23545;LLMs&#21644;&#22270;&#24418;&#23398;&#20064;&#30340;&#24403;&#21069;&#30740;&#31350;&#20135;&#29983;&#24040;&#22823;&#24433;&#21709;&#12290;&#21463;&#26368;&#26032;&#30340;ChatGPT&#21644;Toolformer&#27169;&#22411;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Graph-ToolFormer&#65288;&#38754;&#21521;&#22270;&#24418;&#25512;&#29702;&#30340;Toolformer&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;ChatGPT&#22686;&#24378;&#30340;&#25552;&#31034;&#26469;&#25945;&#23548;LLMs&#33258;&#36523;&#65292;&#26088;&#22312;&#22521;&#20859;&#20182;&#20204;&#30340;&#22270;&#24418;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we aim to develop a large language model (LLM) with the reasoning ability on complex graph data. Currently, LLMs have achieved very impressive performance on various natural language learning tasks, extensions of which have also been applied to study the vision tasks with multi-modal data. However, when it comes to the graph learning tasks, existing LLMs present very serious flaws due to their several inherited weaknesses in performing {multi-step logic reasoning}, {precise mathematical calculation} and {perception about the spatial and temporal factors}.  To address such challenges, in this paper, we will investigate the principles, methodologies and algorithms to empower existing LLMs with graph reasoning ability, which will have tremendous impacts on the current research of both LLMs and graph learning. Inspired by the latest ChatGPT and Toolformer models, we propose the Graph-ToolFormer (Graph Reasoning oriented Toolformer) framework to teach LLMs themselves with pro
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FARM&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26377;&#25928;&#22788;&#29702;&#23454;&#26102;&#25968;&#25454;&#27969;&#24182;&#25552;&#20379;&#24179;&#34913;&#30340;&#30456;&#20851;&#24615;&#24230;&#37327;&#65292;&#36827;&#32780;&#30830;&#23450;&#22806;&#37096;&#25968;&#25454;&#22312;&#39044;&#27979;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.11028</link><description>&lt;p&gt;
&#39044;&#27979;&#20013;&#30340;&#22806;&#22312;&#25968;&#25454;&#65306;&#19968;&#31181;&#29992;&#20110;&#20851;&#32852;&#35780;&#20272;&#30340;FARM&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Exogenous Data in Forecasting: FARM -- An Approach for Relevance Evaluation. (arXiv:2304.11028v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11028
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FARM&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26377;&#25928;&#22788;&#29702;&#23454;&#26102;&#25968;&#25454;&#27969;&#24182;&#25552;&#20379;&#24179;&#34913;&#30340;&#30456;&#20851;&#24615;&#24230;&#37327;&#65292;&#36827;&#32780;&#30830;&#23450;&#22806;&#37096;&#25968;&#25454;&#22312;&#39044;&#27979;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22806;&#22312;&#25968;&#25454;&#34987;&#35748;&#20026;&#22312;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#38024;&#23545;&#24688;&#24403;&#30340;&#36873;&#25321;&#65292;&#20840;&#38754;&#30340;&#30456;&#20851;&#24615;&#20998;&#26512;&#26159;&#19968;&#20010;&#22522;&#26412;&#30340;&#31532;&#19968;&#27493;&#65292;&#20174;&#22806;&#22312;&#25968;&#25454;&#19982;&#21442;&#32771;&#26102;&#38388;&#24207;&#21015;&#30340;&#30456;&#20284;&#24615;&#24320;&#22987;&#12290;&#21463;&#29616;&#26377;&#26102;&#38388;&#24207;&#21015;&#30456;&#20284;&#24615;&#25351;&#26631;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FARM&#65288;&#21069;&#21521;&#35282;&#30456;&#20851;&#24230;&#37327;&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#23454;&#26102;&#25968;&#25454;&#27969;&#12290;&#25105;&#20204;&#30340;&#21069;&#21521;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#31181;&#35282;&#24230;&#29305;&#24449;&#65292;&#35813;&#29305;&#24449;&#21033;&#29992;&#21518;&#32493;&#25968;&#25454;&#28857;&#30340;&#21464;&#21270;&#27604;&#36739;&#26469;&#23545;&#40784;&#32463;&#36807;&#26102;&#38388;&#21464;&#24418;&#30340;&#24207;&#21015;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#32467;&#21512;&#20102;&#26412;&#22320;&#21644;&#20840;&#23616;&#25351;&#26631;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#24179;&#34913;&#30340;&#30456;&#20851;&#24615;&#24230;&#37327;&#12290;&#36825;&#23548;&#33268;&#23558;&#37096;&#20998;&#12289;&#20013;&#38388;&#21305;&#37197;&#20063;&#35270;&#20026;&#22806;&#22312;&#25968;&#25454;&#24207;&#21015;&#37325;&#35201;&#25351;&#26631;&#30340;&#32771;&#34385;&#22240;&#32032;&#12290;&#20316;&#20026;&#31532;&#19968;&#27493;&#39564;&#35777;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;FARM&#26041;&#27861;&#23545;&#21512;&#25104;&#20294;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#20449;&#21495;&#21644;&#30495;&#23454;&#19990;&#30028;&#26102;&#38388;&#24207;&#21015;&#35760;&#24405;&#30340;&#24212;&#29992;&#12290;&#21516;&#26102;&#23637;&#31034;&#20102;FARM&#26041;&#27861;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24230;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exogenous data is believed to play a key role for increasing forecasting accuracy. For an appropriate selection, a throughout relevance analysis is a fundamental first step, starting from the exogenous data similarity with the reference time series. Inspired by existing metrics for time series similarity, we introduce a new approach named FARM - Forward Angular Relevance Measure, able to effectively deal with real-time data streams. Our forward method relies on an angular feature that compares changes in subsequent data points to align time-warped series in an efficient way. The proposed algorithm combines local and global measures to provide a balanced relevance measure. This results in considering also partial, intermediate matches as relevant indicators for exogenous data series significance. As a first validation step, we present the application of our FARM approach to both synthetic but representative signals and real-world time series recordings. While demonstrating the improved 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;FedEx&#26694;&#26550;&#65292;&#21033;&#29992;&#26080;&#20154;&#26426;&#31561;&#31227;&#21160;&#20256;&#36755;&#22120;&#24314;&#31435;&#38388;&#25509;&#36890;&#20449;&#36890;&#36947;&#65292;&#22312;&#35299;&#20915;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#20043;&#38388;&#26080;&#27861;&#30452;&#25509;&#36890;&#20449;&#30340;&#38382;&#39064;&#26102;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#23458;&#25143;&#31471;&#20998;&#37197;&#21644;&#26080;&#20154;&#26426;&#36335;&#24452;&#35268;&#21010;&#30340;&#26041;&#27861;&#65292;&#26368;&#23567;&#21270;&#25972;&#20307;&#35757;&#32451;&#26102;&#38388;&#21644;&#36890;&#20449;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2304.10744</link><description>&lt;p&gt;
&#22522;&#20110;&#38388;&#25509;&#36890;&#20449;&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23458;&#25143;&#31471;&#20998;&#37197;&#21644;&#26080;&#20154;&#26426;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Joint Client Assignment and UAV Route Planning for Indirect-Communication Federated Learning. (arXiv:2304.10744v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10744
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;FedEx&#26694;&#26550;&#65292;&#21033;&#29992;&#26080;&#20154;&#26426;&#31561;&#31227;&#21160;&#20256;&#36755;&#22120;&#24314;&#31435;&#38388;&#25509;&#36890;&#20449;&#36890;&#36947;&#65292;&#22312;&#35299;&#20915;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#20043;&#38388;&#26080;&#27861;&#30452;&#25509;&#36890;&#20449;&#30340;&#38382;&#39064;&#26102;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#23458;&#25143;&#31471;&#20998;&#37197;&#21644;&#26080;&#20154;&#26426;&#36335;&#24452;&#35268;&#21010;&#30340;&#26041;&#27861;&#65292;&#26368;&#23567;&#21270;&#25972;&#20307;&#35757;&#32451;&#26102;&#38388;&#21644;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#21019;&#24314;&#29992;&#20110;&#24378;&#22823;&#24212;&#29992;&#31243;&#24207;&#30340;&#20849;&#20139;&#27169;&#22411;&#65292;&#21516;&#26102;&#20801;&#35768;&#25968;&#25454;&#20445;&#30041;&#22312;&#35774;&#22791;&#19978;&#65292;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#25968;&#25454;&#38544;&#31169;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#38477;&#20302;&#30340;&#24310;&#36831;&#31561;&#20248;&#28857;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#31995;&#32479;&#20013;&#65292;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#30452;&#25509;&#36890;&#20449;&#21487;&#33021;&#19981;&#21487;&#33021;&#65292;&#20363;&#22914;&#27809;&#26377;&#36866;&#24403;&#36890;&#20449;&#22522;&#30784;&#35774;&#26045;&#30340;&#36828;&#31243;&#22320;&#21306;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550; FedEx&#65288;&#36890;&#36807;&#27169;&#22411;&#20256;&#36882;&#23454;&#29616;&#32852;&#37030;&#23398;&#20064;&#65289;&#65292;&#36825;&#20010;&#26694;&#26550;&#21033;&#29992;&#31227;&#21160;&#20256;&#36755;&#22120;&#65292;&#22914;&#26080;&#20154;&#26426;&#65292;&#24314;&#31435;&#20102;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#38388;&#25509;&#36890;&#20449;&#36890;&#36947;&#12290;&#36825;&#20123;&#20256;&#36755;&#22120;&#20316;&#20026;&#20013;&#20171;&#65292;&#20801;&#35768;&#27169;&#22411;&#20449;&#24687;&#30340;&#20132;&#25442;&#12290;&#38388;&#25509;&#36890;&#20449;&#30340;&#20351;&#29992;&#20026;&#25910;&#25947;&#20998;&#26512;&#21644;&#20248;&#21270;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#20256;&#36755;&#22120;&#36816;&#21160;&#24341;&#20837;&#30340;&#24310;&#36831;&#23545;&#20840;&#23616;&#27169;&#22411;&#20998;&#21457;&#21644;&#26412;&#22320;&#27169;&#22411;&#25910;&#38598;&#37117;&#20250;&#20135;&#29983;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#23458;&#25143;&#31471;&#20998;&#37197;&#21644;&#26080;&#20154;&#26426;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#20248;&#21270;&#20102;&#31227;&#21160;&#20256;&#36755;&#22120;&#30340;&#20351;&#29992;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#24635;&#20307;&#35757;&#32451;&#26102;&#38388;&#21644;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a machine learning approach that enables the creation of shared models for powerful applications while allowing data to remain on devices. This approach provides benefits such as improved data privacy, security, and reduced latency. However, in some systems, direct communication between clients and servers may not be possible, such as remote areas without proper communication infrastructure. To overcome this challenge, a new framework called FedEx (Federated Learning via Model Express Delivery) is proposed. This framework employs mobile transporters, such as UAVs, to establish indirect communication channels between the server and clients. These transporters act as intermediaries and allow for model information exchange. The use of indirect communication presents new challenges for convergence analysis and optimization, as the delay introduced by the transporters' movement creates issues for both global model dissemination and local model collection. To addre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;Z3&#27714;&#35299;&#22120;&#23545;&#20840;&#23616;&#40065;&#26834;&#24615;&#21487;&#39564;&#35777;&#26694;&#26550;DeepGlobal&#36827;&#34892;&#26356;&#26126;&#30830;&#30340;&#23450;&#20041;&#21644;&#20248;&#21270;&#30340;&#24037;&#20316;&#65292;&#26469;&#24314;&#31435;FNN&#30340;&#24418;&#24335;&#21270;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#26356;&#26377;&#25928;&#30340;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.10558</link><description>&lt;p&gt;
&#20351;&#29992;Z3&#36827;&#34892;FNN&#20840;&#23616;&#40065;&#26834;&#24615;&#30340;&#24418;&#24335;&#21270;&#24314;&#27169;&#21644;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Using Z3 for Formal Modeling and Verification of FNN Global Robustness. (arXiv:2304.10558v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;Z3&#27714;&#35299;&#22120;&#23545;&#20840;&#23616;&#40065;&#26834;&#24615;&#21487;&#39564;&#35777;&#26694;&#26550;DeepGlobal&#36827;&#34892;&#26356;&#26126;&#30830;&#30340;&#23450;&#20041;&#21644;&#20248;&#21270;&#30340;&#24037;&#20316;&#65292;&#26469;&#24314;&#31435;FNN&#30340;&#24418;&#24335;&#21270;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#26356;&#26377;&#25928;&#30340;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65288;FNN&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#23545;&#23545;&#25239;&#26679;&#26412;&#24456;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#20960;&#31181;&#25216;&#26415;&#26469;&#39564;&#35777;FNN&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#20294;&#22823;&#22810;&#25968;&#25216;&#26415;&#37117;&#38598;&#20013;&#22312;&#38024;&#23545;&#21333;&#20010;&#25968;&#25454;&#28857;&#30340;&#23616;&#37096;&#25200;&#21160;&#37051;&#22495;&#30340;&#40065;&#26834;&#24615;&#39564;&#35777;&#19978;&#12290;&#20840;&#23616;&#40065;&#26834;&#24615;&#20998;&#26512;&#20173;&#23384;&#22312;&#36739;&#22823;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;DeepGlobal&#26159;&#19968;&#31181;&#20840;&#23616;&#40065;&#26834;&#24615;&#21487;&#39564;&#35777;&#26694;&#26550;&#65292;&#26088;&#22312;&#30830;&#23450;FNN&#30340;&#25152;&#26377;&#21487;&#33021;&#30340;&#23545;&#25239;&#21361;&#38505;&#21306;&#22495;&#65288;ADR&#65289;&#65292;&#19981;&#38480;&#20110;&#27979;&#35797;&#38598;&#20013;&#30340;&#25968;&#25454;&#26679;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DeepGlobal&#30340;&#23436;&#25972;&#35268;&#33539;&#21644;&#23454;&#29616;&#65292;&#21033;&#29992;SMT&#27714;&#35299;&#22120;Z3&#36827;&#34892;&#26356;&#26126;&#30830;&#30340;&#23450;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#39033;&#25913;&#36827;&#20197;&#36827;&#34892;&#26356;&#39640;&#25928;&#30340;&#39564;&#35777;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#23454;&#29616;&#21644;&#25913;&#36827;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23545;&#19968;&#32452;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#36827;&#34892;&#20102;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Feedforward Neural Networks (FNNs) have achieved remarkable success in various tasks, they are vulnerable to adversarial examples. Several techniques have been developed to verify the adversarial robustness of FNNs, but most of them focus on robustness verification against the local perturbation neighborhood of a single data point. There is still a large research gap in global robustness analysis. The global-robustness verifiable framework DeepGlobal has been proposed to identify \textit{all} possible Adversarial Dangerous Regions (ADRs) of FNNs, not limited to data samples in a test set. In this paper, we propose a complete specification and implementation of DeepGlobal utilizing the SMT solver Z3 for more explicit definition, and propose several improvements to DeepGlobal for more efficient verification. To evaluate the effectiveness of our implementation and improvements, we conduct extensive experiments on a set of benchmark datasets. Visualization of our experiment results s
&lt;/p&gt;</description></item><item><title>Transformer&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#65292;&#21487;&#20197;&#23398;&#20064;&#24207;&#21015;&#25110;&#25968;&#25454;&#38598;&#34920;&#31034;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26102;&#31354;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#23398;&#31934;&#30830;&#12289;&#30452;&#35266;&#12289;&#31616;&#27905;&#30340;Transformer&#26550;&#26500;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2304.10557</link><description>&lt;p&gt;
Transformer&#20171;&#32461;
&lt;/p&gt;
&lt;p&gt;
An Introduction to Transformers. (arXiv:2304.10557v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10557
&lt;/p&gt;
&lt;p&gt;
Transformer&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#65292;&#21487;&#20197;&#23398;&#20064;&#24207;&#21015;&#25110;&#25968;&#25454;&#38598;&#34920;&#31034;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26102;&#31354;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#23398;&#31934;&#30830;&#12289;&#30452;&#35266;&#12289;&#31616;&#27905;&#30340;Transformer&#26550;&#26500;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26159;&#19968;&#31181;&#21487;&#20197;&#23398;&#20064;&#24207;&#21015;&#25110;&#25968;&#25454;&#38598;&#34920;&#31034;&#30340;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#12290;Transformer&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26102;&#31354;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#34429;&#28982;&#26377;&#24456;&#22810;Transformer&#30340;&#20171;&#32461;&#65292;&#20294;&#22823;&#22810;&#25968;&#37117;&#32570;&#23569;&#23545;&#20854;&#26550;&#26500;&#30340;&#31934;&#30830;&#25968;&#23398;&#25551;&#36848;&#65292;&#20854;&#35774;&#35745;&#36873;&#25321;&#30340;&#30452;&#35273;&#20063;&#24120;&#24120;&#32570;&#22833;&#12290;&#27492;&#22806;&#65292;&#38543;&#30528;&#30740;&#31350;&#36335;&#24452;&#30340;&#26354;&#25240;&#65292;Transformer&#37096;&#20214;&#30340;&#35299;&#37322;&#21487;&#33021;&#26159;&#24322;&#36136;&#30340;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#25968;&#23398;&#31934;&#30830;&#12289;&#30452;&#35266;&#12289;&#31616;&#27905;&#30340;Transformer&#26550;&#26500;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
The transformer is a neural network component that can be used to learn useful representations of sequences or sets of datapoints. The transformer has driven recent advances in natural language processing, computer vision, and spatio-temporal modelling. There are many introductions to transformers, but most do not contain precise mathematical descriptions of the architecture and the intuitions behind the design choices are often also missing. Moreover, as research takes a winding path, the explanations for the components of the transformer can be idiosyncratic. In this note we aim for a mathematically precise, intuitive, and clean description of the transformer architecture.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#31243;&#24207;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;3D&#25193;&#25955;&#20248;&#20808;&#32423;&#21035;&#21152;&#19978;&#26032;&#39062;&#30340;&#22522;&#20110;&#23494;&#24230;&#30340;&#24471;&#20998;&#33976;&#39311;&#37319;&#26679;&#25439;&#22833;&#30340;&#26041;&#27861;&#26469;&#38450;&#27490;NeRF&#20248;&#21270;&#36807;&#31243;&#20013;&#20986;&#29616;&#22270;&#24418;&#20266;&#24433;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.10532</link><description>&lt;p&gt;
Nerfbusters&#65306;&#20174;&#38543;&#24847;&#25429;&#33719;&#30340;NeRF&#20013;&#21435;&#38500;&#24189;&#28789;&#20284;&#30340;&#22270;&#20687;&#20266;&#24433;
&lt;/p&gt;
&lt;p&gt;
Nerfbusters: Removing Ghostly Artifacts from Casually Captured NeRFs. (arXiv:2304.10532v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10532
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#31243;&#24207;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;3D&#25193;&#25955;&#20248;&#20808;&#32423;&#21035;&#21152;&#19978;&#26032;&#39062;&#30340;&#22522;&#20110;&#23494;&#24230;&#30340;&#24471;&#20998;&#33976;&#39311;&#37319;&#26679;&#25439;&#22833;&#30340;&#26041;&#27861;&#26469;&#38450;&#27490;NeRF&#20248;&#21270;&#36807;&#31243;&#20013;&#20986;&#29616;&#22270;&#24418;&#20266;&#24433;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#24847;&#25429;&#33719;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#22312;&#28210;&#26579;&#25668;&#20687;&#26426;&#36712;&#36857;&#20043;&#22806;&#26102;&#20250;&#20986;&#29616;&#28014;&#28857;&#38169;&#35823;&#25110;&#26377;&#32570;&#38519;&#30340;&#20960;&#20309;&#22270;&#24418;&#31561;&#20266;&#24433;&#12290;&#29616;&#26377;&#30340;&#35780;&#20272;&#21327;&#35758;&#36890;&#24120;&#26080;&#27861;&#25429;&#25417;&#36825;&#20123;&#25928;&#24212;&#65292;&#22240;&#20026;&#36890;&#24120;&#20165;&#22312;&#35757;&#32451;&#25235;&#21462;&#30340;&#27599;&#20010;&#31532;&#20843;&#24103;&#35780;&#20272;&#22270;&#20687;&#36136;&#37327;&#12290;&#20026;&#20102;&#25512;&#21160;&#26032;&#35270;&#35282;&#21512;&#25104;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#31243;&#24207;&#65292;&#20854;&#20013;&#35760;&#24405;&#20102;&#22330;&#26223;&#30340;&#20004;&#20010;&#25668;&#20687;&#26426;&#36712;&#36857;&#65306;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#65292;&#21478;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#12290;&#22312;&#36825;&#31181;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#23454;&#38469;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#25163;&#24037;&#21046;&#20316;&#30340;&#35268;&#21017;&#19981;&#20165;&#19981;&#33021;&#21435;&#38500;&#28014;&#28857;&#38169;&#35823;&#65292;&#32780;&#19988;&#20063;&#19981;&#33021;&#25913;&#21892;&#22330;&#26223;&#20960;&#20309;&#24418;&#29366;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;3D&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#26412;&#22320;3D&#20808;&#39564;&#21644;&#26032;&#39062;&#30340;&#22522;&#20110;&#23494;&#24230;&#30340;&#24471;&#20998;&#33976;&#39311;&#37319;&#26679;&#25439;&#22833;&#65292;&#22312;NeRF&#20248;&#21270;&#36807;&#31243;&#20013;&#38450;&#27490;&#20986;&#29616;&#20266;&#20687;&#29616;&#35937;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#22522;&#20110;&#25968;&#25454;&#30340;&#20248;&#20808;&#32423;&#21035;&#33021;&#22815;&#21435;&#38500;&#28014;&#28857;&#38169;&#35823;&#24182;&#25913;&#21892;&#38543;&#24847;&#25429;&#33719;&#30340;&#22330;&#26223;&#20960;&#20309;&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
Casually captured Neural Radiance Fields (NeRFs) suffer from artifacts such as floaters or flawed geometry when rendered outside the camera trajectory. Existing evaluation protocols often do not capture these effects, since they usually only assess image quality at every 8th frame of the training capture. To push forward progress in novel-view synthesis, we propose a new dataset and evaluation procedure, where two camera trajectories are recorded of the scene: one used for training, and the other for evaluation. In this more challenging in-the-wild setting, we find that existing hand-crafted regularizers do not remove floaters nor improve scene geometry. Thus, we propose a 3D diffusion-based method that leverages local 3D priors and a novel density-based score distillation sampling loss to discourage artifacts during NeRF optimization. We show that this data-driven prior removes floaters and improves scene geometry for casual captures.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#22312;&#31038;&#20132;&#35745;&#31639;&#20219;&#21153;&#20013;&#26159;&#21542;&#21487;&#20197;&#22797;&#21046;&#20154;&#31867;&#29983;&#25104;&#30340;&#26631;&#31614;&#27880;&#37322;&#65292;&#32467;&#26524;&#34920;&#26126;ChatGPT&#26377;&#28508;&#21147;&#22788;&#29702;&#36825;&#20123;&#25968;&#25454;&#27880;&#37322;&#20219;&#21153;&#65292;&#23613;&#31649;&#20173;&#23384;&#22312;&#35768;&#22810;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.10145</link><description>&lt;p&gt;
ChatGPT&#33021;&#21542;&#22797;&#21046;&#20154;&#31867;&#29983;&#25104;&#30340;&#26631;&#31614;&#65311;&#23545;&#31038;&#20132;&#35745;&#31639;&#20219;&#21153;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Can ChatGPT Reproduce Human-Generated Labels? A Study of Social Computing Tasks. (arXiv:2304.10145v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10145
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#22312;&#31038;&#20132;&#35745;&#31639;&#20219;&#21153;&#20013;&#26159;&#21542;&#21487;&#20197;&#22797;&#21046;&#20154;&#31867;&#29983;&#25104;&#30340;&#26631;&#31614;&#27880;&#37322;&#65292;&#32467;&#26524;&#34920;&#26126;ChatGPT&#26377;&#28508;&#21147;&#22788;&#29702;&#36825;&#20123;&#25968;&#25454;&#27880;&#37322;&#20219;&#21153;&#65292;&#23613;&#31649;&#20173;&#23384;&#22312;&#35768;&#22810;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#30340;&#21457;&#24067;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#21462;&#20195;&#20154;&#31867;&#26234;&#24935;&#30340;&#21508;&#31181;&#21487;&#33021;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#20102;&#35299;ChatGPT&#26159;&#21542;&#26377;&#28508;&#21147;&#22312;&#31038;&#20132;&#35745;&#31639;&#20219;&#21153;&#20013;&#22797;&#21046;&#20154;&#31867;&#29983;&#25104;&#30340;&#26631;&#31614;&#27880;&#37322;&#12290;&#36825;&#26679;&#30340;&#25104;&#23601;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#31038;&#20132;&#35745;&#31639;&#30740;&#31350;&#30340;&#25104;&#26412;&#21644;&#22797;&#26434;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;ChatGPT&#37325;&#26032;&#26631;&#35760;&#20102;&#20116;&#20010;&#20855;&#26377;&#37324;&#31243;&#30865;&#24847;&#20041;&#30340;&#25968;&#25454;&#38598;&#65292;&#28041;&#21450;&#31435;&#22330;&#26816;&#27979;&#65288;2&#20010;&#65289;&#12289;&#24773;&#24863;&#20998;&#26512;&#12289;&#20167;&#24680;&#35328;&#35770;&#21644;&#26426;&#22120;&#20154;&#26816;&#27979;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#26377;&#28508;&#21147;&#22788;&#29702;&#36825;&#20123;&#25968;&#25454;&#27880;&#37322;&#20219;&#21153;&#65292;&#23613;&#31649;&#20173;&#23384;&#22312;&#35768;&#22810;&#25361;&#25112;&#12290;ChatGPT&#33719;&#24471;&#20102;&#24179;&#22343;&#31934;&#24230;0.609&#12290; ChatGPT&#23545;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;&#30340;&#34920;&#29616;&#26368;&#20339;&#65292;&#27491;&#30830;&#27880;&#37322;&#20102;64.9&#65285;&#30340;&#25512;&#25991;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#26174;&#31034;&#24615;&#33021;&#22312;&#19981;&#21516;&#26631;&#31614;&#20043;&#38388;&#26377;&#24456;&#22823;&#24046;&#24322;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#39033;&#24037;&#20316;&#21487;&#20197;&#24320;&#36767;&#26032;&#30340;&#20998;&#26512;&#32447;&#36335;&#65292;&#24182;&#20316;&#20026;&#26410;&#26469;&#21033;&#29992;ChatGPT&#36827;&#34892;&#25968;&#25454;&#27880;&#37322;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
The release of ChatGPT has uncovered a range of possibilities whereby large language models (LLMs) can substitute human intelligence. In this paper, we seek to understand whether ChatGPT has the potential to reproduce human-generated label annotations in social computing tasks. Such an achievement could significantly reduce the cost and complexity of social computing research. As such, we use ChatGPT to re-label five seminal datasets covering stance detection (2x), sentiment analysis, hate speech, and bot detection. Our results highlight that ChatGPT does have the potential to handle these data annotation tasks, although a number of challenges remain. ChatGPT obtains an average precision 0.609. Performance is highest for the sentiment analysis dataset, with ChatGPT correctly annotating 64.9% of tweets. Yet, we show that performance varies substantially across individual labels. We believe this work can open up new lines of analysis and act as a basis for future research into the exploi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21452;&#35760;&#24518;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702; (2M)&#65292;&#23427;&#32467;&#21512;&#20102;&#24773;&#33410;&#35760;&#24518;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#20248;&#28857;&#26469;&#25552;&#39640;&#23398;&#20064;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.10098</link><description>&lt;p&gt;
&#21452;&#35760;&#24518;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Two-Memory Reinforcement Learning. (arXiv:2304.10098v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21452;&#35760;&#24518;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702; (2M)&#65292;&#23427;&#32467;&#21512;&#20102;&#24773;&#33410;&#35760;&#24518;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#20248;&#28857;&#26469;&#25552;&#39640;&#23398;&#20064;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21462;&#24471;&#20102;&#37325;&#35201;&#30340;&#32463;&#39564;&#24615;&#25104;&#21151;&#65292;&#20294;&#30001;&#20110;&#22870;&#21169;&#20449;&#24687;&#20256;&#25773;&#21644;&#21442;&#25968;&#31070;&#32463;&#32593;&#32476;&#26356;&#26032;&#30340;&#36895;&#24230;&#36739;&#24930;&#65292;&#23427;&#20542;&#21521;&#20110;&#23398;&#20064;&#24471;&#27604;&#36739;&#24930;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#38750;&#21442;&#25968;&#21270;&#30340;&#24773;&#33410;&#35760;&#24518;&#25552;&#20379;&#20102;&#30456;&#23545;&#36739;&#24555;&#30340;&#23398;&#20064;&#26367;&#20195;&#26041;&#26696;&#65292;&#23427;&#19981;&#38656;&#35201;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#20351;&#29992;&#26368;&#22823;&#24773;&#33410;&#22238;&#25253;&#20316;&#20026;&#29366;&#24577;-&#21160;&#20316;&#20540;&#36827;&#34892;&#34892;&#21160;&#36873;&#25321;&#12290;&#24773;&#33410;&#35760;&#24518;&#21644;&#24378;&#21270;&#23398;&#20064;&#37117;&#26377;&#21508;&#33258;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20154;&#31867;&#21487;&#20197;&#21516;&#26102;&#21033;&#29992;&#22810;&#20010;&#35760;&#24518;&#31995;&#32479;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#20174;&#20013;&#33719;&#30410;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21452;&#35760;&#24518;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65288;2M&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#24773;&#33410;&#35760;&#24518;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#20248;&#28857;&#12290; 2M &#20195;&#29702;&#21033;&#29992;&#24773;&#33410;&#35760;&#24518;&#37096;&#20998;&#30340;&#36895;&#24230;&#21644;&#24378;&#21270;&#23398;&#20064;&#37096;&#20998;&#30340;&#26368;&#20248;&#24615;&#21644;&#24191;&#27867;&#36866;&#29992;&#24615;&#30456;&#20114;&#34917;&#20805;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
While deep reinforcement learning has shown important empirical success, it tends to learn relatively slow due to slow propagation of rewards information and slow update of parametric neural networks. Non-parametric episodic memory, on the other hand, provides a faster learning alternative that does not require representation learning and uses maximum episodic return as state-action values for action selection. Episodic memory and reinforcement learning both have their own strengths and weaknesses. Notably, humans can leverage multiple memory systems concurrently during learning and benefit from all of them. In this work, we propose a method called Two-Memory reinforcement learning agent (2M) that combines episodic memory and reinforcement learning to distill both of their strengths. The 2M agent exploits the speed of the episodic memory part and the optimality and the generalization capacity of the reinforcement learning part to complement each other. Our experiments demonstrate that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#31232;&#30095;&#32852;&#21512;&#20998;&#24067;&#65292;&#35777;&#26126;&#20102;LLMs&#33021;&#22815;&#23436;&#25104;&#35821;&#35328;&#29702;&#35299;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#24605;&#36335;&#21551;&#21457;&#20197;&#21450;&#26377;&#25928;&#25351;&#20196;&#24494;&#35843;&#30340;&#26032;&#20852;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.09960</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#28508;&#22312;&#31354;&#38388;&#29702;&#35770;&#23545;&#24212;&#26032;&#20852;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
A Latent Space Theory for Emergent Abilities in Large Language Models. (arXiv:2304.09960v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#31232;&#30095;&#32852;&#21512;&#20998;&#24067;&#65292;&#35777;&#26126;&#20102;LLMs&#33021;&#22815;&#23436;&#25104;&#35821;&#35328;&#29702;&#35299;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#24605;&#36335;&#21551;&#21457;&#20197;&#21450;&#26377;&#25928;&#25351;&#20196;&#24494;&#35843;&#30340;&#26032;&#20852;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#24182;&#19981;&#26159;&#38543;&#26426;&#29983;&#25104;&#65292;&#32780;&#26159;&#20026;&#20102;&#20256;&#36882;&#20449;&#24687;&#12290;&#35821;&#35328;&#19982;&#20854;&#24213;&#23618;&#21547;&#20041;&#20043;&#38388;&#23384;&#22312;&#24378;&#28872;&#30340;&#20851;&#32852;&#65292;&#22312;&#20854;&#30456;&#20851;&#24615;&#26041;&#38754;&#26377;&#30528;&#20005;&#37325;&#20559;&#24046;&#30340;&#31232;&#30095;&#32852;&#21512;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#31232;&#30095;&#24615;&#65292;&#36825;&#20123;&#39640;&#23792;&#20540;&#24688;&#22909;&#19982;&#35821;&#35328;&#30340;&#36793;&#32536;&#20998;&#24067;&#21305;&#37197;&#12290;&#38543;&#30528;&#22823;&#25968;&#25454;&#21644;&#22823;&#27169;&#22411;&#19978;&#35757;&#32451;&#30340;LLMs&#30340;&#20986;&#29616;&#65292;&#25105;&#20204;&#29616;&#22312;&#21487;&#20197;&#31934;&#30830;&#35780;&#20272;&#35821;&#35328;&#30340;&#36793;&#32536;&#20998;&#24067;&#65292;&#36825;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#20415;&#30340;&#25506;&#32034;&#32852;&#21512;&#20998;&#24067;&#31232;&#30095;&#32467;&#26500;&#23454;&#29616;&#26377;&#25928;&#25512;&#29702;&#30340;&#26041;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#35821;&#35328;&#20998;&#31867;&#20026;&#26126;&#30830;&#19982;{\epsilon}-&#27169;&#31946;&#65292;&#24182;&#25552;&#20986;&#23450;&#37327;&#32467;&#26524;&#65292;&#20197;&#34920;&#26126;LLMs&#30340;&#26032;&#20852;&#33021;&#21147;&#65288;&#20363;&#22914;&#35821;&#35328;&#29702;&#35299;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#24605;&#36335;&#21551;&#21457;&#20197;&#21450;&#26377;&#25928;&#25351;&#20196;&#24494;&#35843;&#65289;&#37117;&#21487;&#20197;&#24402;&#22240;&#20110;&#23545;&#31232;&#30095;&#32852;&#21512;&#20998;&#24067;&#36827;&#34892;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Languages are not created randomly but rather to communicate information. There is a strong association between languages and their underlying meanings, resulting in a sparse joint distribution that is heavily peaked according to their correlations. Moreover, these peak values happen to match with the marginal distribution of languages due to the sparsity. With the advent of LLMs trained on big data and large models, we can now precisely assess the marginal distribution of languages, providing a convenient means of exploring the sparse structures in the joint distribution for effective inferences. In this paper, we categorize languages as either unambiguous or {\epsilon}-ambiguous and present quantitative results to demonstrate that the emergent abilities of LLMs, such as language understanding, in-context learning, chain-of-thought prompting, and effective instruction fine-tuning, can all be attributed to Bayesian inference on the sparse joint distribution of languages.
&lt;/p&gt;</description></item><item><title>GeneGPT&#36890;&#36807;&#23569;&#37327;NCBI API&#35843;&#29992;URL&#35831;&#27714;&#20316;&#20026;&#28436;&#31034;&#65292;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;NCBI Web API&#22238;&#31572;&#22522;&#22240;&#32452;&#38382;&#39064;&#65292;&#24182;&#22312;GeneTuring&#27979;&#35797;&#20013;&#36798;&#21040;&#20102;&#20248;&#24322;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.09667</link><description>&lt;p&gt;
GeneGPT: &#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;NCBI Web API
&lt;/p&gt;
&lt;p&gt;
GeneGPT: Teaching Large Language Models to Use NCBI Web APIs. (arXiv:2304.09667v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09667
&lt;/p&gt;
&lt;p&gt;
GeneGPT&#36890;&#36807;&#23569;&#37327;NCBI API&#35843;&#29992;URL&#35831;&#27714;&#20316;&#20026;&#28436;&#31034;&#65292;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;NCBI Web API&#22238;&#31572;&#22522;&#22240;&#32452;&#38382;&#39064;&#65292;&#24182;&#22312;GeneTuring&#27979;&#35797;&#20013;&#36798;&#21040;&#20102;&#20248;&#24322;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;GeneGPT&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20351;&#29992;&#22269;&#23478;&#29983;&#29289;&#25216;&#26415;&#20449;&#24687;&#20013;&#24515;&#65288;NCBI&#65289;&#30340;Web&#24212;&#29992;&#31243;&#24207;&#32534;&#31243;&#25509;&#21475;&#65288;API&#65289;&#65292;&#24182;&#22238;&#31572;&#22522;&#22240;&#32452;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#23569;&#37327;&#30340;NCBI API&#35843;&#29992;URL&#35831;&#27714;&#20316;&#20026;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#28436;&#31034;&#65292;&#21551;&#21457;Codex&#65288;code-davinci-002&#65289;&#35299;&#20915;GeneTuring&#27979;&#35797;&#12290;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#19968;&#26086;&#26816;&#27979;&#21040;&#35843;&#29992;&#35831;&#27714;&#65292;&#25105;&#20204;&#23601;&#20572;&#27490;&#35299;&#30721;&#24182;&#20351;&#29992;&#29983;&#25104;&#30340;URL&#36827;&#34892;API&#35843;&#29992;&#12290;&#25105;&#20204;&#28982;&#21518;&#23558;NCBI API&#36820;&#22238;&#30340;&#21407;&#22987;&#25191;&#34892;&#32467;&#26524;&#38468;&#21152;&#21040;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#65292;&#24182;&#32487;&#32493;&#29983;&#25104;&#30452;&#21040;&#25214;&#21040;&#31572;&#26696;&#25110;&#26816;&#27979;&#21040;&#21478;&#19968;&#20010;API&#35843;&#29992;&#12290;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;GeneGPT&#22312;GeneTuring&#25968;&#25454;&#38598;&#30340;&#22235;&#20010;One-shot&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#19977;&#20010;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#22312;&#20116;&#20010;Zero-shot&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#22235;&#20010;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;GeneGPT&#30340;&#23439;&#24179;&#22343;&#20998;&#25968;&#20026;0.76&#65292;&#36828;&#39640;&#20110;&#26816;&#32034;&#22686;&#24378;LLM&#65292;&#22914;New Bin&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present GeneGPT, a novel method for teaching large language models (LLMs) to use the Web Application Programming Interfaces (APIs) of the National Center for Biotechnology Information (NCBI) and answer genomics questions. Specifically, we prompt Codex (code-davinci-002) to solve the GeneTuring tests with few-shot URL requests of NCBI API calls as demonstrations for in-context learning. During inference, we stop the decoding once a call request is detected and make the API call with the generated URL. We then append the raw execution results returned by NCBI APIs to the generated texts and continue the generation until the answer is found or another API call is detected. Our preliminary results show that GeneGPT achieves state-of-the-art results on three out of four one-shot tasks and four out of five zero-shot tasks in the GeneTuring dataset. Overall, GeneGPT achieves a macro-average score of 0.76, which is much higher than retrieval-augmented LLMs such as the New Bin
&lt;/p&gt;</description></item><item><title>SemEval 2023&#20030;&#21150;&#20102;LegalEval&#20849;&#20139;&#20219;&#21153;&#65292;&#21363;&#29702;&#35299;&#27861;&#24459;&#25991;&#26412;&#65292;&#21253;&#25324; &#33258;&#21160;&#32467;&#26500;&#21270;&#21644;&#35821;&#20041;&#36830;&#36143;&#21270;&#30340;&#27861;&#24459;&#25991;&#20214;&#65288;Task-A&#65289;&#65292;&#27861;&#24459;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;Task-B&#65289;&#20197;&#21450;&#33258;&#21160;&#39044;&#27979;&#27861;&#24459;&#26696;&#20214;&#32467;&#26524;&#21644;&#25552;&#20379;&#39044;&#27979;&#35299;&#37322;&#65288;Task-C&#65289;&#12290;26&#20010;&#22242;&#38431;&#25552;&#20132;&#20102;&#31995;&#32479;&#35770;&#25991;&#24182;&#22312;&#25152;&#26377;&#23376;&#20219;&#21153;&#20013;&#20248;&#20110;&#22522;&#20934;&#32447;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2304.09548</link><description>&lt;p&gt;
SemEval 2023 &#20219;&#21153;6: LegalEval -- &#29702;&#35299;&#27861;&#24459;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
SemEval 2023 Task 6: LegalEval -- Understanding Legal Texts. (arXiv:2304.09548v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09548
&lt;/p&gt;
&lt;p&gt;
SemEval 2023&#20030;&#21150;&#20102;LegalEval&#20849;&#20139;&#20219;&#21153;&#65292;&#21363;&#29702;&#35299;&#27861;&#24459;&#25991;&#26412;&#65292;&#21253;&#25324; &#33258;&#21160;&#32467;&#26500;&#21270;&#21644;&#35821;&#20041;&#36830;&#36143;&#21270;&#30340;&#27861;&#24459;&#25991;&#20214;&#65288;Task-A&#65289;&#65292;&#27861;&#24459;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;Task-B&#65289;&#20197;&#21450;&#33258;&#21160;&#39044;&#27979;&#27861;&#24459;&#26696;&#20214;&#32467;&#26524;&#21644;&#25552;&#20379;&#39044;&#27979;&#35299;&#37322;&#65288;Task-C&#65289;&#12290;26&#20010;&#22242;&#38431;&#25552;&#20132;&#20102;&#31995;&#32479;&#35770;&#25991;&#24182;&#22312;&#25152;&#26377;&#23376;&#20219;&#21153;&#20013;&#20248;&#20110;&#22522;&#20934;&#32447;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#21475;&#20247;&#22810;&#30340;&#22269;&#23478;&#65292;&#24453;&#22788;&#29702;&#30340;&#27861;&#24459;&#26696;&#20214;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#26377;&#24517;&#35201;&#24320;&#21457;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#25216;&#26415;&#65292;&#23545;&#27861;&#24459;&#25991;&#20214;&#36827;&#34892;&#22788;&#29702;&#21644;&#33258;&#21160;&#29702;&#35299;&#12290;&#20026;&#20102;&#20419;&#36827;&#22312;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#22312; SemEval 2023 &#19978;&#32452;&#32455;&#20102;&#20849;&#20139;&#20219;&#21153; LegalEval - &#29702;&#35299;&#27861;&#24459;&#25991;&#26412;&#12290;LegalEval &#20219;&#21153;&#26377;&#19977;&#20010;&#23376;&#20219;&#21153;&#65306;Task-A&#65288;&#20462;&#36766;&#35282;&#33394;&#26631;&#35760;&#65289;&#26159;&#33258;&#21160;&#23558;&#27861;&#24459;&#25991;&#20214;&#32467;&#26500;&#21270;&#20026;&#35821;&#20041;&#36830;&#36143;&#30340;&#21333;&#20803;&#65292;Task-B&#65288;&#27861;&#24459;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65289;&#22788;&#29702;&#22312;&#27861;&#24459;&#25991;&#20214;&#20013;&#35782;&#21035;&#30456;&#20851;&#23454;&#20307;&#65292;&#32780; Task-C&#65288;&#27861;&#38498;&#21028;&#20915;&#39044;&#27979;&#19982;&#35299;&#37322;&#65289;&#25506;&#32034;&#20102;&#33258;&#21160;&#39044;&#27979;&#27861;&#24459;&#26696;&#20214;&#32467;&#26524;&#20197;&#21450;&#25552;&#20379;&#39044;&#27979;&#35299;&#37322;&#30340;&#21487;&#33021;&#24615;&#12290;&#20849;&#26377;26&#20010;&#22242;&#38431;&#65288;&#20998;&#24067;&#22312;&#20840;&#29699;&#30340;&#32422;100&#21517;&#21442;&#19982;&#32773;&#65289;&#25552;&#20132;&#20102;&#31995;&#32479;&#35770;&#25991;&#12290;&#22312;&#27599;&#20010;&#23376;&#20219;&#21153;&#20013;&#65292;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#37117;&#20248;&#20110;&#22522;&#20934;&#32447;&#65307;&#20294;&#26159;&#65292;&#20173;&#28982;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102; LegalEval &#20219;&#21153;&#30340;&#32452;&#32455;&#21644;&#32454;&#33410;&#65292;&#24182;&#27010;&#36848;&#20102;&#21442;&#19982;&#31995;&#32479;&#21450;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In populous countries, pending legal cases have been growing exponentially. There is a need for developing NLP-based techniques for processing and automatically understanding legal documents. To promote research in the area of Legal NLP we organized the shared task LegalEval - Understanding Legal Texts at SemEval 2023. LegalEval task has three sub-tasks: Task-A (Rhetorical Roles Labeling) is about automatically structuring legal documents into semantically coherent units, Task-B (Legal Named Entity Recognition) deals with identifying relevant entities in a legal document and Task-C (Court Judgement Prediction with Explanation) explores the possibility of automatically predicting the outcome of a legal case along with providing an explanation for the prediction. In total 26 teams (approx. 100 participants spread across the world) submitted systems paper. In each of the sub-tasks, the proposed systems outperformed the baselines; however, there is a lot of scope for improvement. This pape
&lt;/p&gt;</description></item><item><title>PaTeCon&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#30340;&#30693;&#35782;&#22270;&#35889;&#26102;&#38388;&#32422;&#26463;&#25366;&#25496;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#26102;&#38388;&#32422;&#26463;&#26469;&#32500;&#25252;KG&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#65292;&#24182;&#22312;&#19981;&#38656;&#35201;&#20154;&#24037;&#19987;&#23478;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#22320;&#26816;&#27979;&#28508;&#22312;&#30340;&#26102;&#38388;&#20914;&#31361;&#12290;</title><link>http://arxiv.org/abs/2304.09015</link><description>&lt;p&gt;
PaTeCon&#65306;&#22522;&#20110;&#27169;&#24335;&#30340;&#30693;&#35782;&#22270;&#35889;&#26102;&#38388;&#32422;&#26463;&#25366;&#25496;&#26041;&#27861;&#29992;&#20110;&#20914;&#31361;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
PaTeCon: A Pattern-Based Temporal Constraint Mining Method for Conflict Detection on Knowledge Graphs. (arXiv:2304.09015v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09015
&lt;/p&gt;
&lt;p&gt;
PaTeCon&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#30340;&#30693;&#35782;&#22270;&#35889;&#26102;&#38388;&#32422;&#26463;&#25366;&#25496;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#26102;&#38388;&#32422;&#26463;&#26469;&#32500;&#25252;KG&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#65292;&#24182;&#22312;&#19981;&#38656;&#35201;&#20154;&#24037;&#19987;&#23478;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#22320;&#26816;&#27979;&#28508;&#22312;&#30340;&#26102;&#38388;&#20914;&#31361;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#30740;&#31350;&#31038;&#21306;&#20013;&#65292;&#26102;&#38388;&#20107;&#23454;&#25351;&#29305;&#23450;&#26102;&#38388;&#27573;&#20869;&#21457;&#29983;&#30340;&#20107;&#20214;&#30340;&#25968;&#25454;&#12290;&#24341;&#20837;&#26102;&#38388;&#38480;&#21046;&#32473;KG&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#32500;&#25252;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20381;&#36182;&#20110;&#25163;&#21160;&#21015;&#20030;&#26102;&#38388;&#32422;&#26463;&#26469;&#26816;&#27979;&#20914;&#31361;&#65292;&#36825;&#24456;&#36153;&#21147;&#19988;&#21487;&#33021;&#23384;&#22312;&#31890;&#24230;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#30340;&#26102;&#38388;&#32422;&#26463;&#25366;&#25496;&#26041;&#27861;PaTeCon&#65292;&#23427;&#20351;&#29992;&#33258;&#21160;&#30830;&#23450;&#30340;&#22270;&#24418;&#27169;&#24335;&#21450;&#20854;&#30456;&#20851;&#32479;&#35745;&#20449;&#24687;&#20195;&#26367;&#20154;&#24037;&#19987;&#23478;&#26469;&#29983;&#25104;&#26102;&#38388;&#32422;&#26463;&#12290;&#20855;&#20307;&#22320;&#65292;PaTeCon&#26681;&#25454;&#20854;&#27979;&#37327;&#24471;&#20998;&#21160;&#24577;&#22320;&#23558;&#31867;&#38480;&#21046;&#38468;&#21152;&#21040;&#20505;&#36873;&#32422;&#26463;&#19978;&#12290;&#25105;&#20204;&#22522;&#20110;&#32500;&#22522;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;PaTeCon&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal facts, the facts for characterizing events that hold in specific time periods, are attracting rising attention in the knowledge graph (KG) research communities. In terms of quality management, the introduction of time restrictions brings new challenges to maintaining the temporal consistency of KGs and detecting potential temporal conflicts. Previous studies rely on manually enumerated temporal constraints to detect conflicts, which are labor-intensive and may have granularity issues. We start from the common pattern of temporal facts and constraints and propose a pattern-based temporal constraint mining method, PaTeCon. PaTeCon uses automatically determined graph patterns and their relevant statistical information over the given KG instead of human experts to generate time constraints. Specifically, PaTeCon dynamically attaches class restriction to candidate constraints according to their measuring scores.We evaluate PaTeCon on two large-scale datasets based on Wikidata and F
&lt;/p&gt;</description></item><item><title>CornerFormer&#26159;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#19981;&#21516;&#24314;&#27169;&#31574;&#30053;&#20110;&#21333;&#20010;&#27169;&#22411;&#20013;&#34701;&#21512;&#35282;&#28857;&#26816;&#27979;&#21644;&#36793;&#32536;&#39044;&#27979;&#26469;&#25552;&#21319;&#31934;&#32454;&#32467;&#26500;&#37325;&#24314;&#30340;&#34920;&#29616;&#65292;&#24182;&#22312;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.07072</link><description>&lt;p&gt;
CornerFormer: &#25552;&#21319;&#35282;&#28857;&#34920;&#24449;&#20197;&#36827;&#34892;&#31934;&#32454;&#32467;&#26500;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
CornerFormer: Boosting Corner Representation for Fine-Grained Structured Reconstruction. (arXiv:2304.07072v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07072
&lt;/p&gt;
&lt;p&gt;
CornerFormer&#26159;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#19981;&#21516;&#24314;&#27169;&#31574;&#30053;&#20110;&#21333;&#20010;&#27169;&#22411;&#20013;&#34701;&#21512;&#35282;&#28857;&#26816;&#27979;&#21644;&#36793;&#32536;&#39044;&#27979;&#26469;&#25552;&#21319;&#31934;&#32454;&#32467;&#26500;&#37325;&#24314;&#30340;&#34920;&#29616;&#65292;&#24182;&#22312;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#37325;&#24314;&#26159;&#19968;&#31181;&#38750;&#24179;&#20961;&#30340;&#23494;&#38598;&#39044;&#27979;&#38382;&#39064;&#65292;&#23427;&#20174;&#26629;&#26684;&#22270;&#20687;&#20013;&#25552;&#21462;&#32467;&#26500;&#20449;&#24687;&#65288;&#20363;&#22914;&#65292;&#24314;&#31569;&#35282;&#28857;&#21644;&#36793;&#32536;&#65289;&#65292;&#28982;&#21518;&#30456;&#24212;&#22320;&#37325;&#24314;&#20026;&#20108;&#32500;&#24179;&#38754;&#22270;&#12290;&#19982;&#24120;&#35265;&#30340;&#20998;&#21106;&#25110;&#26816;&#27979;&#38382;&#39064;&#30456;&#27604;&#65292;&#23427;&#26174;&#33879;&#20381;&#36182;&#20110;&#21033;&#29992;&#25972;&#20307;&#20960;&#20309;&#20449;&#24687;&#36827;&#34892;&#32467;&#26500;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#30446;&#21069;&#65292;&#22522;&#20110;transformer&#30340;&#26041;&#27861;&#37319;&#29992;&#20004;&#38454;&#27573;&#26041;&#24335;&#35299;&#20915;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22312;&#31532;&#19968;&#20010;&#27169;&#22411;&#20013;&#26816;&#27979;&#35282;&#28857;&#65292;&#24182;&#22312;&#31532;&#20108;&#20010;&#27169;&#22411;&#20013;&#20998;&#31867;&#25311;&#35758;&#36793;&#32536;&#65288;&#35282;&#23545;&#65289;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23558;&#20004;&#20010;&#38454;&#27573;&#20998;&#24320;&#25104;&#19981;&#21516;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#21482;&#20849;&#20139;&#20027;&#24178;&#32534;&#30721;&#22120;&#12290;&#19982;&#29616;&#26377;&#30340;&#24314;&#27169;&#31574;&#30053;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22686;&#24378;&#30340;&#35282;&#28857;&#34920;&#31034;&#26041;&#27861;&#65306;1&#65289;&#36890;&#36807;&#22312;&#19981;&#21516;&#30340;&#31890;&#24230;&#20013;&#20849;&#20139;&#29305;&#24449;&#65292;&#23427;&#22312;&#35282;&#28857;&#26816;&#27979;&#21644;&#36793;&#32536;&#39044;&#27979;&#20043;&#38388;&#34701;&#21512;&#30693;&#35782;&#65307;2&#65289;&#35282;&#28857;&#20505;&#36873;&#32773;&#26681;&#25454;&#20854;&#26041;&#21521;&#20316;&#20026;&#22235;&#20010;&#28909;&#22270;&#36890;&#36947;&#25552;&#20986;&#12290;&#23450;&#24615;&#21644;&#23450;&#37327;&#35780;&#20272;&#22343;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;CornerFormer&#26126;&#26174;&#20248;&#20110;&#20197;&#21069;&#30340;transformer-based&#27169;&#22411;&#65292;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structured reconstruction is a non-trivial dense prediction problem, which extracts structural information (\eg, building corners and edges) from a raster image, then reconstructs it to a 2D planar graph accordingly. Compared with common segmentation or detection problems, it significantly relays on the capability that leveraging holistic geometric information for structural reasoning. Current transformer-based approaches tackle this challenging problem in a two-stage manner, which detect corners in the first model and classify the proposed edges (corner-pairs) in the second model. However, they separate two-stage into different models and only share the backbone encoder. Unlike the existing modeling strategies, we present an enhanced corner representation method: 1) It fuses knowledge between the corner detection and edge prediction by sharing feature in different granularity; 2) Corner candidates are proposed in four heatmap channels w.r.t its direction. Both qualitative and quantita
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#36816;&#21160;&#39044;&#27979;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#32570;&#20047;&#35299;&#37322;&#24615;&#21644;&#21487;&#33021;&#36829;&#21453;&#29289;&#29702;&#32422;&#26463;&#12290;&#22240;&#27492;&#65292;&#32467;&#21512;&#24046;&#20998;&#32422;&#26463;&#36816;&#21160;&#27169;&#22411;&#33021;&#25552;&#20379;&#29289;&#29702;&#19978;&#21487;&#34892;&#30340;&#36712;&#36857;&#65292;&#30740;&#31350;&#34920;&#26126;&#20302;&#38454;&#31215;&#20998;&#22120;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#25968;&#20540;&#27714;&#35299;&#22120;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.05116</link><description>&lt;p&gt;
&#19981;&#21516;&#32422;&#26463;&#36816;&#21160;&#27169;&#22411;&#22312;&#22522;&#20110;&#22270;&#30340;&#36712;&#36857;&#39044;&#27979;&#20013;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluation of Differentially Constrained Motion Models for Graph-Based Trajectory Prediction. (arXiv:2304.05116v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05116
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#36816;&#21160;&#39044;&#27979;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#32570;&#20047;&#35299;&#37322;&#24615;&#21644;&#21487;&#33021;&#36829;&#21453;&#29289;&#29702;&#32422;&#26463;&#12290;&#22240;&#27492;&#65292;&#32467;&#21512;&#24046;&#20998;&#32422;&#26463;&#36816;&#21160;&#27169;&#22411;&#33021;&#25552;&#20379;&#29289;&#29702;&#19978;&#21487;&#34892;&#30340;&#36712;&#36857;&#65292;&#30740;&#31350;&#34920;&#26126;&#20302;&#38454;&#31215;&#20998;&#22120;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#25968;&#20540;&#27714;&#35299;&#22120;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#34920;&#29616;&#20986;&#33394;&#19988;&#21487;&#35843;&#24615;&#39640;&#65292;&#25104;&#20026;&#36816;&#21160;&#39044;&#27979;&#30340;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#39640;&#24230;&#28789;&#27963;&#24615;&#20276;&#38543;&#30340;&#26159;&#35299;&#37322;&#24615;&#32570;&#22833;&#21644;&#21487;&#33021;&#36829;&#21453;&#30340;&#29289;&#29702;&#32422;&#26463;&#12290;&#20351;&#29992;&#24046;&#20998;&#32422;&#26463;&#36816;&#21160;&#27169;&#22411;&#26469;&#25552;&#20379;&#29289;&#29702;&#19978;&#21487;&#34892;&#30340;&#36712;&#36857;&#65292;&#21487;&#20197;&#20316;&#20026;&#19982;&#36825;&#20123;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30456;&#37197;&#21512;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;&#26412;&#30740;&#31350;&#22522;&#20110;&#20808;&#21069;&#25552;&#20986;&#30340;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411; MTP-GO&#65292;&#30740;&#31350;&#20102;&#21508;&#31181;&#36816;&#21160;&#27169;&#22411;&#32467;&#21512;&#25968;&#20540;&#27714;&#35299;&#22120;&#36827;&#34892;&#39044;&#27979;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#20026;&#20102;&#33719;&#24471;&#31934;&#30830;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#31616;&#21333;&#30340;&#27169;&#22411;&#65292;&#22914;&#20302;&#38454;&#31215;&#20998;&#22120;&#27169;&#22411;&#65292;&#20248;&#20110;&#26356;&#22797;&#26434;&#30340;&#36816;&#21160;&#23398;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25968;&#20540;&#27714;&#35299;&#22120;&#21487;&#20197;&#23545;&#36816;&#21160;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given their adaptability and encouraging performance, deep-learning models are becoming standard for motion prediction in autonomous driving. However, with great flexibility comes a lack of interpretability and possible violations of physical constraints. Accompanying these data-driven methods with differentially-constrained motion models to provide physically feasible trajectories is a promising future direction. The foundation for this work is a previously introduced graph-neural-network-based model, MTP-GO. The neural network learns to compute the inputs to an underlying motion model to provide physically feasible trajectories. This research investigates the performance of various motion models in combination with numerical solvers for the prediction task. The study shows that simpler models, such as low-order integrator models, are preferred over more complex ones, e.g., kinematic models, to achieve accurate predictions. Further, the numerical solver can have a substantial impact o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#20351;&#29992;&#32454;&#35843;BERT&#27169;&#22411;&#21644;&#22810;&#25968;&#25237;&#31080;&#38598;&#25104;&#27169;&#22411;&#26469;&#26816;&#27979;&#21644;&#35299;&#37322;&#22312;&#32447;&#24615;&#21035;&#27495;&#35270;&#30340;&#26041;&#27861;&#12290;&#32763;&#36716;&#26174;&#30528;&#38477;&#20302;&#20102;&#22899;&#24615;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#32463;&#21382;&#19981;&#25104;&#27604;&#20363;&#30340;&#24615;&#21035;&#27495;&#35270;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2304.03518</link><description>&lt;p&gt;
SSS&#22312;SemEval-2023&#20219;&#21153;10&#20013;&#30340;&#35770;&#25991;&#65306;&#20351;&#29992;&#25237;&#31080;&#32454;&#35843;&#21464;&#21387;&#22120;&#21487;&#35299;&#37322;&#30340;&#26816;&#27979;&#22312;&#32447;&#24615;&#21035;&#27495;&#35270;&#12290; (arXiv&#65306;2304.03518v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
SSS at SemEval-2023 Task 10: Explainable Detection of Online Sexism using Majority Voted Fine-Tuned Transformers. (arXiv:2304.03518v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#20351;&#29992;&#32454;&#35843;BERT&#27169;&#22411;&#21644;&#22810;&#25968;&#25237;&#31080;&#38598;&#25104;&#27169;&#22411;&#26469;&#26816;&#27979;&#21644;&#35299;&#37322;&#22312;&#32447;&#24615;&#21035;&#27495;&#35270;&#30340;&#26041;&#27861;&#12290;&#32763;&#36716;&#26174;&#30528;&#38477;&#20302;&#20102;&#22899;&#24615;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#32463;&#21382;&#19981;&#25104;&#27604;&#20363;&#30340;&#24615;&#21035;&#27495;&#35270;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#22312;SemEval 2023&#20219;&#21153;10&#20013;&#25552;&#20132;&#30340;&#20316;&#21697;-&#21487;&#35299;&#37322;&#30340;&#22312;&#32447;&#24615;&#21035;&#27495;&#35270;&#26816;&#27979;&#65288;EDOS&#65289;&#65292;&#20998;&#20026;&#19977;&#20010;&#23376;&#20219;&#21153;&#12290;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#30340;&#19981;&#26029;&#22686;&#38271;&#23548;&#33268;&#22899;&#24615;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#38754;&#20020;&#19981;&#25104;&#27604;&#20363;&#30340;&#24615;&#21035;&#27495;&#35270;&#12290;&#36825;&#20351;&#24471;&#26816;&#27979;&#21644;&#35299;&#37322;&#22312;&#32447;&#24615;&#21035;&#27495;&#35270;&#20869;&#23481;&#21464;&#24471;&#27604;&#20197;&#24448;&#26356;&#21152;&#37325;&#35201;&#65292;&#20197;&#20351;&#31038;&#20132;&#23186;&#20307;&#23545;&#22899;&#24615;&#26356;&#21152;&#23433;&#20840;&#21644;&#21487;&#35775;&#38382;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#23454;&#39564;&#21644;&#24494;&#35843;&#22522;&#20110;BERT&#30340;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#22810;&#25968;&#25237;&#31080;&#38598;&#21512;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20248;&#20110;&#21333;&#20010;&#22522;&#32447;&#27169;&#22411;&#24471;&#20998;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#20219;&#21153;A&#20013;&#23454;&#29616;&#20102;&#23439;F1&#20998;&#25968;0.8392&#65292;&#22312;&#20219;&#21153;B&#20013;&#20026;0.6092&#65292;&#22312;&#20219;&#21153;C&#20013;&#20026;0.4319&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes our submission to Task 10 at SemEval 2023-Explainable Detection of Online Sexism (EDOS), divided into three subtasks. The recent rise in social media platforms has seen an increase in disproportionate levels of sexism experienced by women on social media platforms. This has made detecting and explaining online sexist content more important than ever to make social media safer and more accessible for women. Our approach consists of experimenting and finetuning BERT-based models and using a Majority Voting ensemble model that outperforms individual baseline model scores. Our system achieves a macro F1 score of 0.8392 for Task A, 0.6092 for Task B, and 0.4319 for Task C.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#21382;&#31243;&#20197;&#21450;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#65292;&#24182;&#24378;&#35843;&#27169;&#22411;&#25193;&#23637;&#23558;&#24102;&#26469;&#24615;&#33021;&#25913;&#36827;&#21644;&#29305;&#27530;&#33021;&#21147;&#30340;&#21457;&#25496;&#12290;</title><link>http://arxiv.org/abs/2303.18223</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Large Language Models. (arXiv:2303.18223v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#21382;&#31243;&#20197;&#21450;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#65292;&#24182;&#24378;&#35843;&#27169;&#22411;&#25193;&#23637;&#23558;&#24102;&#26469;&#24615;&#33021;&#25913;&#36827;&#21644;&#29305;&#27530;&#33021;&#21147;&#30340;&#21457;&#25496;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#26412;&#36136;&#19978;&#26159;&#19968;&#20010;&#30001;&#35821;&#27861;&#35268;&#21017;&#25511;&#21046;&#30340;&#22797;&#26434;&#31934;&#32454;&#30340;&#20154;&#31867;&#34920;&#36798;&#31995;&#32479;&#65292;&#23545;&#20110;&#24320;&#21457;&#29702;&#35299;&#21644;&#25484;&#25569;&#35821;&#35328;&#30340;&#33021;&#21147;&#30340;AI&#31639;&#27861;&#26469;&#35828;&#26159;&#19968;&#39033;&#37325;&#22823;&#25361;&#25112;&#12290;&#20316;&#20026;&#20027;&#35201;&#26041;&#27861;&#20043;&#19968;&#65292;&#35821;&#35328;&#24314;&#27169;&#22312;&#36807;&#21435;&#20108;&#21313;&#24180;&#37324;&#24191;&#27867;&#30740;&#31350;&#29992;&#20110;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#65292;&#20174;&#32479;&#35745;&#35821;&#35328;&#27169;&#22411;&#28436;&#21270;&#20026;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#22312;&#35299;&#20915;&#21508;&#31181;NLP&#20219;&#21153;&#26041;&#38754;&#26174;&#31034;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#30001;&#20110;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#27169;&#22411;&#32553;&#25918;&#21487;&#20197;&#23548;&#33268;&#24615;&#33021;&#25913;&#36827;&#65292;&#20182;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#22686;&#21152;&#27169;&#22411;&#35268;&#27169;&#26469;&#30740;&#31350;&#32553;&#25918;&#25928;&#24212;&#65292;&#26377;&#36259;&#30340;&#26159;&#65292;&#24403;&#21442;&#25968;&#35268;&#27169;&#36229;&#36807;&#19968;&#23450;&#27700;&#24179;&#26102;&#65292;&#36825;&#20123;&#25193;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#23454;&#29616;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#32780;&#19988;&#36824;&#26174;&#31034;&#20986;&#19968;&#20123;&#23567;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25152;&#27809;&#26377;&#30340;&#29305;&#27530;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale langu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#26426;&#20132;&#20114;&#25216;&#26415;&#20026;&#30740;&#31350;&#35770;&#25991;&#25552;&#20379;&#26234;&#33021;&#12289;&#20132;&#20114;&#24335;&#21644;&#26080;&#38556;&#30861;&#30340;&#38405;&#35835;&#30028;&#38754;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#20171;&#32461;&#20102;&#36328;&#26426;&#26500;&#21512;&#20316;&#30340;&#35821;&#20041;&#38405;&#35835;&#22120;&#39033;&#30446;&#12290;</title><link>http://arxiv.org/abs/2303.14334</link><description>&lt;p&gt;
&#35821;&#20041;&#38405;&#35835;&#22120;&#39033;&#30446;&#65306;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#20132;&#20114;&#24335;&#38405;&#35835;&#30028;&#38754;&#22686;&#24378;&#23398;&#26415;&#25991;&#26723;
&lt;/p&gt;
&lt;p&gt;
The Semantic Reader Project: Augmenting Scholarly Documents through AI-Powered Interactive Reading Interfaces. (arXiv:2303.14334v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#26426;&#20132;&#20114;&#25216;&#26415;&#20026;&#30740;&#31350;&#35770;&#25991;&#25552;&#20379;&#26234;&#33021;&#12289;&#20132;&#20114;&#24335;&#21644;&#26080;&#38556;&#30861;&#30340;&#38405;&#35835;&#30028;&#38754;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#20171;&#32461;&#20102;&#36328;&#26426;&#26500;&#21512;&#20316;&#30340;&#35821;&#20041;&#38405;&#35835;&#22120;&#39033;&#30446;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#26415;&#20986;&#29256;&#29289;&#26159;&#23398;&#32773;&#21521;&#20182;&#20154;&#20256;&#36882;&#30693;&#35782;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#35770;&#25991;&#20449;&#24687;&#23494;&#38598;&#65292;&#38543;&#30528;&#31185;&#23398;&#25991;&#29486;&#37327;&#30340;&#22686;&#38271;&#65292;&#38656;&#35201;&#26032;&#25216;&#26415;&#25903;&#25345;&#38405;&#35835;&#36807;&#31243;&#12290;&#19982;&#36890;&#36807;&#20114;&#32852;&#32593;&#25216;&#26415;&#36716;&#21464;&#30340;&#26597;&#25214;&#35770;&#25991;&#36807;&#31243;&#19981;&#21516;&#65292;&#38405;&#35835;&#30740;&#31350;&#35770;&#25991;&#30340;&#20307;&#39564;&#20960;&#21313;&#24180;&#26469;&#20960;&#20046;&#27809;&#26377;&#25913;&#21464;&#12290;&#34429;&#28982;PDF&#26684;&#24335;&#22240;&#20854;&#20415;&#25658;&#24615;&#32780;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#23427;&#26377;&#37325;&#22823;&#32570;&#28857;&#65292;&#21253;&#25324;&#65306;&#38745;&#24577;&#20869;&#23481;&#65292;&#20302;&#35270;&#35273;&#35835;&#32773;&#30340;&#21487;&#35775;&#38382;&#24615;&#24046;&#65292;&#20197;&#21450;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#38405;&#35835;&#22256;&#38590;&#12290;&#26412;&#25991;&#25506;&#35752;&#8220;&#26368;&#36817;&#30340;AI&#21644;HCI&#36827;&#23637;&#33021;&#21542;&#20026;&#36951;&#30041;&#30340;PDF&#25552;&#20379;&#26234;&#33021;&#65292;&#20132;&#20114;&#24335;&#21644;&#26080;&#38556;&#30861;&#30340;&#38405;&#35835;&#30028;&#38754;&#65311;&#8221;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#35821;&#20041;&#38405;&#35835;&#22120;&#39033;&#30446;&#65292;&#36825;&#26159;&#22810;&#20010;&#26426;&#26500;&#30340;&#21327;&#20316;&#21162;&#21147;&#65292;&#26088;&#22312;&#25506;&#32034;&#20026;&#30740;&#31350;&#35770;&#25991;&#33258;&#21160;&#21019;&#24314;&#21160;&#24577;&#38405;&#35835;&#30028;&#38754;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scholarly publications are key to the transfer of knowledge from scholars to others. However, research papers are information-dense, and as the volume of the scientific literature grows, the need for new technology to support the reading process grows. In contrast to the process of finding papers, which has been transformed by Internet technology, the experience of reading research papers has changed little in decades. The PDF format for sharing research papers is widely used due to its portability, but it has significant downsides including: static content, poor accessibility for low-vision readers, and difficulty reading on mobile devices. This paper explores the question "Can recent advances in AI and HCI power intelligent, interactive, and accessible reading interfaces -- even for legacy PDFs?" We describe the Semantic Reader Project, a collaborative effort across multiple institutions to explore automatic creation of dynamic reading interfaces for research papers. Through this pro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#39046;&#22495;&#25209;&#22788;&#29702;&#21644;&#20195;&#29702;&#26799;&#24230;&#36716;&#31227;&#30340;&#35821;&#20041;&#22810;&#35270;&#35282;&#27169;&#22411;&#65292;&#21487;&#20197;&#35299;&#20915;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#24847;&#22270;&#26816;&#27979;&#21644;&#35825;&#23548;&#26032;&#24847;&#22270;&#30340;&#38382;&#39064;&#65292;&#22312;Open Intent Induction&#20013;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2303.13099</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#30340;&#38646;&#26679;&#26412;&#24320;&#25918;&#24847;&#22270;&#24402;&#32435;&#65306;&#22810;&#39046;&#22495;&#25209;&#22788;&#29702;&#21644;&#20195;&#29702;&#26799;&#24230;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Multi-View Zero-Shot Open Intent Induction from Dialogues: Multi Domain Batch and Proxy Gradient Transfer. (arXiv:2303.13099v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#39046;&#22495;&#25209;&#22788;&#29702;&#21644;&#20195;&#29702;&#26799;&#24230;&#36716;&#31227;&#30340;&#35821;&#20041;&#22810;&#35270;&#35282;&#27169;&#22411;&#65292;&#21487;&#20197;&#35299;&#20915;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#24847;&#22270;&#26816;&#27979;&#21644;&#35825;&#23548;&#26032;&#24847;&#22270;&#30340;&#38382;&#39064;&#65292;&#22312;Open Intent Induction&#20013;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#31995;&#32479;&#20013;&#65292;&#26816;&#27979;&#21644;&#35825;&#23548;&#26032;&#30340;&#24847;&#22270;&#26159;&#23558;&#35813;&#31995;&#32479;&#24212;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#30340;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#35821;&#20041;&#22810;&#35270;&#35282;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#38590;&#39064;&#65306;&#65288;1&#65289;&#29992;&#20110;&#19968;&#33324;&#23884;&#20837;&#30340;SBERT&#65288;2&#65289;&#22810;&#39046;&#22495;&#25209;&#22788;&#29702;&#65288;MDB&#65289;&#29992;&#20110;&#23545;&#35805;&#39046;&#22495;&#30693;&#35782;&#65292;&#20197;&#21450;&#65288;3&#65289;&#29992;&#20110;&#38598;&#32676;&#19987;&#19994;&#35821;&#20041;&#30340;&#20195;&#29702;&#26799;&#24230;&#36716;&#31227;&#65288;PGT&#65289;&#12290; MDB&#19968;&#27425;&#21521;&#27169;&#22411;&#25552;&#20379;&#22810;&#31181;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#23398;&#20064;&#22810;&#39046;&#22495;&#30693;&#35782;&#26469;&#35299;&#20915;&#22810;&#39046;&#22495;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;PGT&#65292;&#23427;&#37319;&#29992;Siamese&#32593;&#32476;&#30452;&#25509;&#20351;&#29992;&#32858;&#31867;&#26041;&#27861;&#24494;&#35843;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#22914;&#20309;&#20351;&#29992;PGT&#32858;&#31867;&#23545;&#35805;&#35821;&#21477;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#32447;&#31995;&#32479;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#22810;&#35270;&#35282;&#27169;&#22411;&#19982;MDB&#21644;PGT&#26174;&#30528;&#25552;&#39640;&#20102;Open Intent Induction&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Task Oriented Dialogue (TOD) system, detecting and inducing new intents are two main challenges to apply the system in the real world. In this paper, we suggest the semantic multi-view model to resolve these two challenges: (1) SBERT for General Embedding (GE), (2) Multi Domain Batch (MDB) for dialogue domain knowledge, and (3) Proxy Gradient Transfer (PGT) for cluster-specialized semantic. MDB feeds diverse dialogue datasets to the model at once to tackle the multi-domain problem by learning the multiple domain knowledge. We introduce a novel method PGT, which employs the Siamese network to fine-tune the model with a clustering method directly.Our model can learn how to cluster dialogue utterances by using PGT. Experimental results demonstrate that our multi-view model with MDB and PGT significantly improves the Open Intent Induction performance compared to baseline systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#38544;&#24335;&#21361;&#23475;&#26816;&#27979;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#19968;&#31181;&#38754;&#21521;&#34920;&#24773;&#21253;&#24773;&#22659;&#30340;&#25299;&#25169;&#24863;&#30693;&#26368;&#20248;&#20256;&#36755;&#26694;&#26550;TOT&#65292;&#21033;&#29992;&#26368;&#20248;&#20256;&#36755;&#26680;&#26041;&#27861;&#20174;&#22810;&#20010;&#27169;&#24577;&#20013;&#25429;&#25417;&#20114;&#34917;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2303.09314</link><description>&lt;p&gt;
TOT&#65306;&#38754;&#21521;&#22810;&#27169;&#24577;&#20167;&#24680;&#26816;&#27979;&#30340;&#25299;&#25169;&#24863;&#30693;&#26368;&#20248;&#20256;&#36755;
&lt;/p&gt;
&lt;p&gt;
TOT: Topology-Aware Optimal Transport For Multimodal Hate Detection. (arXiv:2303.09314v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#38544;&#24335;&#21361;&#23475;&#26816;&#27979;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#19968;&#31181;&#38754;&#21521;&#34920;&#24773;&#21253;&#24773;&#22659;&#30340;&#25299;&#25169;&#24863;&#30693;&#26368;&#20248;&#20256;&#36755;&#26694;&#26550;TOT&#65292;&#21033;&#29992;&#26368;&#20248;&#20256;&#36755;&#26680;&#26041;&#27861;&#20174;&#22810;&#20010;&#27169;&#24577;&#20013;&#25429;&#25417;&#20114;&#34917;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#20167;&#24680;&#26816;&#27979;&#26088;&#22312;&#35782;&#21035;&#22312;&#32447;&#26377;&#23475;&#20869;&#23481;&#65288;&#22914;&#34920;&#24773;&#21253;&#31561;&#65289;&#65292;&#26159;&#26500;&#24314;&#20581;&#24247;&#30340;&#20114;&#32852;&#32593;&#29615;&#22659;&#33267;&#20851;&#37325;&#35201;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#37325;&#28857;&#20851;&#27880;&#26174;&#24335;&#20167;&#24680;&#35328;&#35770;&#30340;&#26816;&#27979;&#65292;&#32780;&#24573;&#30053;&#20102;&#38544;&#24335;&#21361;&#23475;&#30340;&#20998;&#26512;&#65292;&#36825;&#22312;&#23384;&#22312;&#30528;&#25197;&#26354;&#25110;&#32570;&#20047;&#26126;&#26174;&#25991;&#26412;&#26631;&#35760;&#21644;&#20154;&#21475;&#32479;&#35745;&#35270;&#35273;&#32447;&#32034;&#30340;&#24773;&#20917;&#19979;&#38754;&#20020;&#30528;&#29305;&#21035;&#22823;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;TOT&#65306;&#19968;&#31181;&#38754;&#21521;&#34920;&#24773;&#21253;&#24773;&#22659;&#30340;&#25299;&#25169;&#24863;&#30693;&#26368;&#20248;&#20256;&#36755;&#26694;&#26550;&#65292;&#23558;&#36328;&#27169;&#24577;&#23545;&#40784;&#38382;&#39064;&#36716;&#21270;&#20026;&#26368;&#20248;&#20256;&#36755;&#26041;&#26696;&#30340;&#27714;&#35299;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#20248;&#20256;&#36755;&#26680;&#26041;&#27861;&#20174;&#22810;&#20010;&#27169;&#24577;&#20013;&#25429;&#25417;&#20114;&#34917;&#20449;&#24687;&#12290;&#26680;&#23884;&#20837;&#25552;&#20379;&#20102;&#19968;&#31181;&#38750;&#32447;&#24615;&#36716;&#25442;&#33021;&#21147;&#65292;&#20197;&#37325;&#29616;&#36755;&#20837;&#30340;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal hate detection, which aims to identify harmful content online such as memes, is crucial for building a wholesome internet environment. Previous work has made enlightening exploration in detecting explicit hate remarks. However, most of their approaches neglect the analysis of implicit harm, which is particularly challenging as explicit text markers and demographic visual cues are often twisted or missing. The leveraged cross-modal attention mechanisms also suffer from the distributional modality gap and lack logical interpretability. To address these semantic gaps issues, we propose TOT: a topology-aware optimal transport framework to decipher the implicit harm in memes scenario, which formulates the cross-modal aligning problem as solutions for optimal transportation plans. Specifically, we leverage an optimal transport kernel method to capture complementary information from multiple modalities. The kernel embedding provides a non-linear transformation ability to reproduce 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25918;&#24323;&#20102;&#25945;&#24072;&#34920;&#29616;&#33391;&#22909;&#30340;&#20551;&#35774;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#34701;&#21512;&#20219;&#24847;&#25945;&#24072;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#36712;&#36857;&#30340;&#20215;&#20540;&#20272;&#35745;&#23454;&#29616;&#39640;&#25928;&#30340;&#25506;&#32034;&#21644;&#23433;&#20840;&#20445;&#38556;&#12290;</title><link>http://arxiv.org/abs/2303.01728</link><description>&lt;p&gt;
&#26377;&#32570;&#38519;&#22312;&#32447;&#28436;&#31034;&#30340;&#20445;&#25252;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Guarded Policy Optimization with Imperfect Online Demonstrations. (arXiv:2303.01728v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25918;&#24323;&#20102;&#25945;&#24072;&#34920;&#29616;&#33391;&#22909;&#30340;&#20551;&#35774;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#34701;&#21512;&#20219;&#24847;&#25945;&#24072;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#36712;&#36857;&#30340;&#20215;&#20540;&#20272;&#35745;&#23454;&#29616;&#39640;&#25928;&#30340;&#25506;&#32034;&#21644;&#23433;&#20840;&#20445;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#25945;&#24072;-&#23398;&#29983;&#26694;&#26550;&#8221;&#65288;TSF&#65289;&#26159;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#35774;&#32622;&#65292;&#20854;&#20013;&#25945;&#24072;&#20195;&#29702;&#36890;&#36807;&#24178;&#39044;&#21644;&#25552;&#20379;&#22312;&#32447;&#28436;&#31034;&#26469;&#30417;&#30563;&#23398;&#29983;&#20195;&#29702;&#30340;&#35757;&#32451;&#12290;&#26412;&#25991;&#25918;&#23485;&#20102;&#25945;&#24072;&#34920;&#29616;&#33391;&#22909;&#30340;&#20551;&#35774;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#33021;&#22815;&#34701;&#21512;&#20219;&#24847;&#25945;&#24072;&#31574;&#30053;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#23454;&#20363;&#21270;&#20102;&#19968;&#20010;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;,&#21363;&#25945;&#24072;-&#23398;&#29983;&#20849;&#20139;&#25511;&#21046;&#65288;TS2C&#65289;&#65292;&#23427;&#22522;&#20110;&#22522;&#20110;&#36712;&#36857;&#30340;&#20215;&#20540;&#20272;&#35745;&#26469;&#34701;&#20837;&#25945;&#24072;&#24178;&#39044;&#12290;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;TS2C&#31639;&#27861;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25506;&#32034;&#21644;&#21487;&#38752;&#30340;&#23433;&#20840;&#20445;&#38556;&#65292;&#32780;&#19981;&#20250;&#36807;&#20998;&#20381;&#36182;&#25945;&#24072;&#31574;&#30053;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Teacher-Student Framework (TSF) is a reinforcement learning setting where a teacher agent guards the training of a student agent by intervening and providing online demonstrations. Assuming optimal, the teacher policy has the perfect timing and capability to intervene in the learning process of the student agent, providing safety guarantee and exploration guidance. Nevertheless, in many real-world settings it is expensive or even impossible to obtain a well-performing teacher policy. In this work, we relax the assumption of a well-performing teacher and develop a new method that can incorporate arbitrary teacher policies with modest or inferior performance. We instantiate an Off-Policy Reinforcement Learning algorithm, termed Teacher-Student Shared Control (TS2C), which incorporates teacher intervention based on trajectory-based value estimation. Theoretical analysis validates that the proposed TS2C algorithm attains efficient exploration and substantial safety guarantee without be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;UZH_CLyp&#22312;SemEval 2023&#20219;&#21153;9&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#21253;&#21547;&#20102;&#20808;&#20351;&#29992;Head-First Fine-Tuning&#65288;HeFiT&#65289;&#26041;&#27861;&#26356;&#26032;&#22238;&#24402;&#22836;&#21442;&#25968;&#65292;&#20877;&#38477;&#20302;&#23398;&#20064;&#29575;&#26356;&#26032;&#39044;&#35757;&#32451;transformer&#30340;&#21442;&#25968;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#27809;&#26377;&#20154;&#24037;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;ChatGPT&#29983;&#25104;&#30340;&#33258;&#21160;&#23567;&#22411;&#26679;&#20363;&#26469;&#35299;&#20915;&#20302;&#36164;&#28304;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;HeFiT&#31283;&#23450;&#35757;&#32451;&#24182;&#26174;&#33879;&#25552;&#39640;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#26102;&#20063;&#33021;&#25552;&#39640;&#36328;&#35821;&#35328;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.01194</link><description>&lt;p&gt;
UZH_CLyp&#22312;SemEval-2023&#20219;&#21153;9&#20013;&#30340;&#34920;&#29616;&#65306;&#22522;&#20110;Head-First Fine-Tuning&#21644;ChatGPT&#25968;&#25454;&#29983;&#25104;&#30340;&#36328;&#35821;&#35328;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#25512;&#25991;&#20146;&#23494;&#24230;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
UZH_CLyp at SemEval-2023 Task 9: Head-First Fine-Tuning and ChatGPT Data Generation for Cross-Lingual Learning in Tweet Intimacy Prediction. (arXiv:2303.01194v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01194
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;UZH_CLyp&#22312;SemEval 2023&#20219;&#21153;9&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#21253;&#21547;&#20102;&#20808;&#20351;&#29992;Head-First Fine-Tuning&#65288;HeFiT&#65289;&#26041;&#27861;&#26356;&#26032;&#22238;&#24402;&#22836;&#21442;&#25968;&#65292;&#20877;&#38477;&#20302;&#23398;&#20064;&#29575;&#26356;&#26032;&#39044;&#35757;&#32451;transformer&#30340;&#21442;&#25968;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#27809;&#26377;&#20154;&#24037;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;ChatGPT&#29983;&#25104;&#30340;&#33258;&#21160;&#23567;&#22411;&#26679;&#20363;&#26469;&#35299;&#20915;&#20302;&#36164;&#28304;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;HeFiT&#31283;&#23450;&#35757;&#32451;&#24182;&#26174;&#33879;&#25552;&#39640;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#26102;&#20063;&#33021;&#25552;&#39640;&#36328;&#35821;&#35328;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;UZH_CLyp&#22312;SemEval 2023&#20219;&#21153;9&#8220;&#22810;&#35821;&#35328;&#25512;&#25991;&#20146;&#23494;&#24230;&#20998;&#26512;&#8221;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#22312;&#25152;&#26377;10&#31181;&#35821;&#35328;&#20013;&#22343;&#21462;&#24471;&#20102;&#31532;&#20108;&#22909;&#30340;&#32467;&#26524;&#65292;&#26681;&#25454;&#23448;&#26041;&#30340;Pearson&#30456;&#20851;&#31995;&#25968;&#22238;&#24402;&#35780;&#20272;&#25351;&#26631;&#12290;&#25105;&#20204;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#25506;&#32034;&#20102;&#20351;&#29992;Head-First Fine-Tuning&#26041;&#27861;&#65288;HeFiT&#65289;&#30340;&#30410;&#22788;&#65292;&#35813;&#26041;&#27861;&#39318;&#20808;&#20165;&#26356;&#26032;&#22238;&#24402;&#22836;&#21442;&#25968;&#65292;&#28982;&#21518;&#20877;&#20197;&#38477;&#20302;&#30340;&#23398;&#20064;&#29575;&#26356;&#26032;&#39044;&#35757;&#32451;&#30340;transformer&#32534;&#30721;&#22120;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20302;&#36164;&#28304;&#35774;&#32622;&#20013;&#20351;&#29992;&#19968;&#23567;&#32452;&#33258;&#21160;&#29983;&#25104;&#30340;&#31034;&#20363;&#65288;&#22312;&#25105;&#20204;&#30340;&#24773;&#20917;&#19979;&#65292;&#26469;&#33258;ChatGPT&#65289;&#23545;&#27809;&#26377;&#20154;&#24037;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;HeFiT&#31283;&#23450;&#20102;&#22521;&#35757;&#24182;&#19988;&#23545;&#20110;&#32570;&#20047;&#25512;&#25991;&#39046;&#22495;&#36866;&#24212;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#19968;&#33268;&#22320;&#25552;&#39640;&#20102;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#34920;&#26126;&#65292;&#24403;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#26102;&#65292;&#36328;&#35821;&#35328;&#23398;&#20064;&#30340;&#24615;&#33021;&#26174;&#30528;&#25552;&#39640;&#65292;&#35777;&#23454;&#20102;&#24403;&#21069;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#35299;&#20915;&#20302;&#36164;&#28304;&#24773;&#20917;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes the submission of UZH_CLyp for the SemEval 2023 Task 9 "Multilingual Tweet Intimacy Analysis". We achieved second-best results in all 10 languages according to the official Pearson's correlation regression evaluation measure. Our cross-lingual transfer learning approach explores the benefits of using a Head-First Fine-Tuning method (HeFiT) that first updates only the regression head parameters and then also updates the pre-trained transformer encoder parameters at a reduced learning rate. Additionally, we study the impact of using a small set of automatically generated examples (in our case, from ChatGPT) for low-resource settings where no human-labeled data is available. Our study shows that HeFiT stabilizes training and consistently improves results for pre-trained models that lack domain adaptation to tweets. Our study also shows a noticeable performance increase in cross-lingual learning when synthetic data is used, confirming the usefulness of current text gen
&lt;/p&gt;</description></item><item><title>BrainCLIP&#26159;&#19968;&#31181;&#20174;fMRI&#20013;&#33719;&#21462;&#33258;&#28982;&#22270;&#20687;&#20449;&#24687;&#30340;&#35299;&#30721;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#20102;CLIP&#36328;&#27169;&#24577;&#27867;&#21270;&#33021;&#21147;&#24182;&#22312;&#35821;&#20041;&#31354;&#38388;&#20013;&#32479;&#19968;&#35270;&#35273;&#21050;&#28608;&#20998;&#31867;&#21644;&#37325;&#26500;&#20219;&#21153;&#65292;&#21516;&#26102;&#25991;&#26412;&#30417;&#30563;&#20063;&#33021;&#25552;&#39640;&#35299;&#30721;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.12971</link><description>&lt;p&gt;
BrainCLIP&#65306;&#36890;&#36807;CLIP&#26694;&#26550;&#23454;&#29616;&#20174;fMRI&#20013;&#33719;&#21462;&#33258;&#28982;&#35270;&#35273;&#20449;&#24687;&#30340;&#36890;&#29992;&#35299;&#30721;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
BrainCLIP: Bridging Brain and Visual-Linguistic Representation via CLIP for Generic Natural Visual Stimulus Decoding from fMRI. (arXiv:2302.12971v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12971
&lt;/p&gt;
&lt;p&gt;
BrainCLIP&#26159;&#19968;&#31181;&#20174;fMRI&#20013;&#33719;&#21462;&#33258;&#28982;&#22270;&#20687;&#20449;&#24687;&#30340;&#35299;&#30721;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#20102;CLIP&#36328;&#27169;&#24577;&#27867;&#21270;&#33021;&#21147;&#24182;&#22312;&#35821;&#20041;&#31354;&#38388;&#20013;&#32479;&#19968;&#35270;&#35273;&#21050;&#28608;&#20998;&#31867;&#21644;&#37325;&#26500;&#20219;&#21153;&#65292;&#21516;&#26102;&#25991;&#26412;&#30417;&#30563;&#20063;&#33021;&#25552;&#39640;&#35299;&#30721;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;fMRI&#20449;&#21495;&#37325;&#26500;&#24863;&#30693;&#21040;&#30340;&#33258;&#28982;&#22270;&#20687;&#25110;&#35299;&#30721;&#23427;&#20204;&#30340;&#31867;&#21035;&#26159;&#20855;&#26377;&#31185;&#23398;&#24847;&#20041;&#30340;&#25361;&#25112;&#24615;&#20219;&#21153;&#12290;&#30001;&#20110;&#32570;&#20047;&#25104;&#23545;&#26679;&#26412;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#29983;&#25104;&#35821;&#20041;&#21487;&#35782;&#21035;&#30340;&#37325;&#26500;&#65292;&#24182;&#19988;&#38590;&#20197;&#25512;&#24191;&#21040;&#26032;&#39062;&#31867;&#21035;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#22823;&#33041;&#35299;&#30721;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#35821;&#20041;&#31354;&#38388;&#20013;&#32479;&#19968;&#35270;&#35273;&#21050;&#28608;&#20998;&#31867;&#21644;&#37325;&#26500;&#20219;&#21153;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#23558;&#23427;&#21629;&#21517;&#20026;BrainCLIP&#65292;&#21033;&#29992;&#20102;CLIP&#36328;&#27169;&#24577;&#27867;&#21270;&#33021;&#21147;&#65292;&#20197;&#22635;&#34917;&#22823;&#33041;&#27963;&#21160;&#65292;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#27169;&#24577;&#24046;&#36317;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;BrainCLIP&#26159;&#19968;&#31181;&#22522;&#20110;VAE&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#36890;&#36807;&#32467;&#21512;&#35270;&#35273;&#21644;&#25991;&#26412;&#30417;&#30563;&#65292;&#23558;fMRI&#27169;&#24335;&#36716;&#25442;&#20026;CLIP&#23884;&#20837;&#31354;&#38388;&#12290;&#27880;&#24847;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#24456;&#23569;&#20351;&#29992;&#22810;&#27169;&#24577;&#30417;&#30563;&#36827;&#34892;&#35270;&#35273;&#21050;&#28608;&#35299;&#30721;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25991;&#26412;&#30417;&#30563;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#35299;&#30721;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reconstructing perceived natural images or decoding their categories from fMRI signals are challenging tasks with great scientific significance. Due to the lack of paired samples, most existing methods fail to generate semantically recognizable reconstruction and are difficult to generalize to novel classes. In this work, we propose, for the first time, a task-agnostic brain decoding model by unifying the visual stimulus classification and reconstruction tasks in a semantic space. We denote it as BrainCLIP, which leverages CLIP's cross-modal generalization ability to bridge the modality gap between brain activities, images, and texts. Specifically, BrainCLIP is a VAE-based architecture that transforms fMRI patterns into the CLIP embedding space by combining visual and textual supervision. Note that previous works rarely use multi-modal supervision for visual stimulus decoding. Our experiments demonstrate that textual supervision can significantly boost the performance of decoding model
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#20998;&#31867;&#21644;&#35821;&#20041;&#20998;&#21106;&#32593;&#32476;(ICSSN)&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#36845;&#20195;&#21319;&#32423;&#20004;&#20010;&#20849;&#20139;&#29305;&#24449;&#25552;&#21462;&#22120;&#26469;&#22823;&#22823;&#22686;&#24378;&#30446;&#26631;&#32423;&#21644;&#20687;&#32032;&#32423;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.12420</link><description>&lt;p&gt;
&#39640;&#20998;&#36776;&#29575;&#36965;&#24863;&#22270;&#20687;&#22312;&#32769;&#28369;&#22369;&#25506;&#27979;&#20013;&#30340;&#36845;&#20195;&#20998;&#31867;&#21644;&#35821;&#20041;&#20998;&#21106;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
An Iterative Classification and Semantic Segmentation Network for Old Landslide Detection Using High-Resolution Remote Sensing Images. (arXiv:2302.12420v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#20998;&#31867;&#21644;&#35821;&#20041;&#20998;&#21106;&#32593;&#32476;(ICSSN)&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#36845;&#20195;&#21319;&#32423;&#20004;&#20010;&#20849;&#20139;&#29305;&#24449;&#25552;&#21462;&#22120;&#26469;&#22823;&#22823;&#22686;&#24378;&#30446;&#26631;&#32423;&#21644;&#20687;&#32032;&#32423;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26102;&#38388;&#30340;&#20316;&#29992;&#20351;&#24471;&#32769;&#28369;&#22369;&#30340;&#24418;&#24577;&#29305;&#24449;&#34987;&#37096;&#20998;&#25110;&#24378;&#28872;&#25913;&#21464;&#19988;&#19982;&#21608;&#22260;&#29615;&#22659;&#24046;&#24322;&#19981;&#22823;&#65292;&#22240;&#27492;&#32769;&#28369;&#22369;&#26816;&#27979;&#38754;&#20020;&#24040;&#22823;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#23567;&#26679;&#26412;&#38382;&#39064;&#20063;&#21046;&#32422;&#20102;&#28145;&#24230;&#23398;&#20064;&#30340;&#28145;&#20837;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#20998;&#31867;&#21644;&#35821;&#20041;&#20998;&#21106;&#32593;&#32476;(ICSSN)&#65292;&#36890;&#36807;&#36845;&#20195;&#21319;&#32423;&#20004;&#20010;&#20849;&#20139;&#29305;&#24449;&#25552;&#21462;&#22120;&#26469;&#22823;&#22823;&#22686;&#24378;&#30446;&#26631;&#32423;&#21644;&#20687;&#32032;&#32423;&#20998;&#31867;&#24615;&#33021;&#12290;&#22312;&#23545;&#35937;&#20998;&#31867;&#23376;&#32593;&#32476;&#20013;&#37319;&#29992;&#23545;&#35937;&#32423;&#23545;&#27604;&#23398;&#20064;(OCL)&#31574;&#30053;&#65292;&#37319;&#29992;&#36830;&#20307;&#32593;&#32476;&#23454;&#29616;&#20840;&#23616;&#29305;&#24449;&#25552;&#21462;&#65307;&#22312;&#35821;&#20041;&#20998;&#21106;&#23376;&#32593;&#32476;&#20013;&#35774;&#35745;&#20102;&#23376;&#23545;&#35937;&#32423;&#23545;&#27604;&#23398;&#20064;(SOCL)&#33539;&#24335;&#65292;&#20197;&#26377;&#25928;&#22320;&#20174;&#28369;&#22369;&#36793;&#30028;&#25552;&#21462;&#26174;&#33879;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#35814;&#32454;&#38416;&#36848;&#20102;&#19968;&#31181;&#36845;&#20195;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#22312;&#35821;&#20041;&#31354;&#38388;&#20013;&#34701;&#21512;&#29305;&#24449;&#65292;&#20174;&#32780;&#26082;&#25552;&#39640;&#30446;&#26631;&#32423;&#21035;&#20998;&#31867;&#24615;&#33021;&#65292;&#21448;&#25552;&#39640;&#20687;&#32032;&#32423;&#21035;&#30340;&#35821;&#20041;&#20998;&#21106;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Huge challenges exist for old landslide detection because their morphology features have been partially or strongly transformed over a long time and have little difference from their surrounding. Besides, small-sample problem also restrict in-depth learning.  In this paper, an iterative classification and semantic segmentation network (ICSSN) is developed, which can greatly enhance both object-level and pixel-level classification performance by iteratively upgrading the feature extractor shared by two network. An object-level contrastive learning (OCL) strategy is employed in the object classification sub-network featuring a siamese network to realize the global features extraction, and a sub-object-level contrastive learning (SOCL) paradigm is designed in the semantic segmentation sub-network to efficiently extract salient features from boundaries of landslides. Moreover, an iterative training strategy is elaborated to fuse features in semantic space such that both object-level and pi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#20110;&#19968;&#20123;&#20855;&#26377;&#22823;&#22495;&#21644;&#38271;&#23376;&#21477;&#30340;&#26497;&#38590;&#20363;&#23376;&#65292;&#35201;&#27714;&#36827;&#34892;&#24443;&#24213;&#25628;&#32034;&#25165;&#33021;&#35299;&#20915;&#65292;&#36825;&#24847;&#21619;&#30528;P $\neq$ NP&#12290;</title><link>http://arxiv.org/abs/2302.09512</link><description>&lt;p&gt;
SAT&#38656;&#35201;&#24443;&#24213;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
SAT Requires Exhaustive Search. (arXiv:2302.09512v4 [cs.CC] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#20110;&#19968;&#20123;&#20855;&#26377;&#22823;&#22495;&#21644;&#38271;&#23376;&#21477;&#30340;&#26497;&#38590;&#20363;&#23376;&#65292;&#35201;&#27714;&#36827;&#34892;&#24443;&#24213;&#25628;&#32034;&#25165;&#33021;&#35299;&#20915;&#65292;&#36825;&#24847;&#21619;&#30528;P $\neq$ NP&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#26500;&#36896;&#20855;&#26377;&#22823;&#22495;&#21644;&#38271;&#23376;&#21477;&#30340;CSP&#21644;SAT&#30340;&#26497;&#38590;&#20363;&#23376;&#65292;&#35777;&#26126;&#36825;&#20123;&#20363;&#23376;&#26080;&#27861;&#22312;&#19981;&#36827;&#34892;&#24443;&#24213;&#25628;&#32034;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#65292;&#36825;&#24847;&#21619;&#30528;&#19968;&#20010;&#36739;&#24369;&#30340;&#32467;&#35770;P $\neq$ NP&#12290;&#26412;&#25991;&#37319;&#29992;&#30340;&#26159;&#19968;&#31181;&#35777;&#26126;&#19981;&#21487;&#33021;&#24615;&#32467;&#26524;&#30340;&#24314;&#35774;&#24615;&#26041;&#27861;&#65292;&#19982;&#30446;&#21069;&#35745;&#31639;&#22797;&#26434;&#24615;&#29702;&#35770;&#20013;&#20351;&#29992;&#30340;&#26041;&#27861;&#38750;&#24120;&#19981;&#21516;&#65292;&#20294;&#19982;Kurt G\"{o}del&#22312;&#35777;&#26126;&#20182;&#33879;&#21517;&#30340;&#36923;&#36753;&#19981;&#21487;&#33021;&#24615;&#32467;&#26524;&#26102;&#20351;&#29992;&#30340;&#26041;&#27861;&#30456;&#20284;&#12290;&#27491;&#22914;G\"{o}del&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25968;&#23398;&#20013;&#35777;&#26126;&#24418;&#24335;&#19978;&#30340;&#19981;&#21487;&#35777;&#26126;&#24615;&#26159;&#21487;&#34892;&#30340;&#19968;&#26679;&#65292;&#26412;&#25991;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25968;&#23398;&#20013;&#35777;&#26126;&#35745;&#31639;&#19978;&#30340;&#38590;&#24230;&#19981;&#26159;&#24456;&#38590;&#30340;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23545;&#35768;&#22810;&#38382;&#39064;&#65292;&#22914;3-SAT&#65292;&#35777;&#26126;&#19979;&#30028;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#36825;&#20123;&#38382;&#39064;&#26377;&#21508;&#31181;&#26377;&#25928;&#30340;&#31574;&#30053;&#21487;&#29992;&#20110;&#36991;&#20813;&#36827;&#34892;&#24443;&#24213;&#25628;&#32034;&#12290;&#28982;&#32780;&#65292;&#22312;&#26497;&#38590;&#30340;&#20363;&#23376;&#20013;&#65292;&#24443;&#24213;&#25628;&#32034;&#21487;&#33021;&#26159;&#21807;&#19968;&#21487;&#34892;&#30340;&#36873;&#25321;&#65292;&#35777;&#26126;&#20854;&#24517;&#35201;&#24615;&#21464;&#24471;&#26356;&#21152;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, by constructing extremely hard examples of CSP (with large domains) and SAT (with long clauses), we prove that such examples cannot be solved without exhaustive search, which implies a weaker conclusion P $\neq$ NP. This constructive approach for proving impossibility results is very different (and missing) from those currently used in computational complexity theory, but is similar to that used by Kurt G\"{o}del in proving his famous logical impossibility results. Just as shown by G\"{o}del's results that proving formal unprovability is feasible in mathematics, the results of this paper show that proving computational hardness is not hard in mathematics. Specifically, proving lower bounds for many problems, such as 3-SAT, can be challenging because these problems have various effective strategies available for avoiding exhaustive search. However, in cases of extremely hard examples, exhaustive search may be the only viable option, and proving its necessity becomes more 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;CC-FedAvg&#30340;&#35745;&#31639;&#23450;&#21046;&#30340;&#32852;&#37030;&#24179;&#22343;&#31639;&#27861;&#65292;&#21487;&#35753;&#21442;&#19982;&#32773;&#26681;&#25454;&#20854;&#35745;&#31639;&#39044;&#31639;&#20915;&#23450;&#22312;&#27599;&#36718;&#20013;&#26159;&#21542;&#25191;&#34892;&#20256;&#32479;&#30340;&#26412;&#22320;&#35757;&#32451;&#25110;&#27169;&#22411;&#20272;&#31639;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CC-FedAvg&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#24182;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2212.13679</link><description>&lt;p&gt;
CC-FedAvg&#65306;&#35745;&#31639;&#23450;&#21046;&#30340;&#32852;&#37030;&#24179;&#22343;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
CC-FedAvg: Computationally Customized Federated Averaging. (arXiv:2212.13679v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.13679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;CC-FedAvg&#30340;&#35745;&#31639;&#23450;&#21046;&#30340;&#32852;&#37030;&#24179;&#22343;&#31639;&#27861;&#65292;&#21487;&#35753;&#21442;&#19982;&#32773;&#26681;&#25454;&#20854;&#35745;&#31639;&#39044;&#31639;&#20915;&#23450;&#22312;&#27599;&#36718;&#20013;&#26159;&#21542;&#25191;&#34892;&#20256;&#32479;&#30340;&#26412;&#22320;&#35757;&#32451;&#25110;&#27169;&#22411;&#20272;&#31639;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CC-FedAvg&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#24182;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#27169;&#22411;&#35757;&#32451;&#26041;&#24335;&#65292;&#36890;&#36807;&#20998;&#24067;&#22312;&#20247;&#22810;&#29289;&#32852;&#32593;&#35774;&#22791;&#19978;&#30340;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#23427;&#22312;&#26412;&#36136;&#19978;&#20551;&#35774;&#21442;&#19982;&#32773;&#30340;&#35745;&#31639;&#33021;&#21147;&#30456;&#21516;&#65292;&#20294;&#23454;&#38469;&#19978;&#65292;&#30001;&#20110;&#19981;&#21516;&#30340;&#33021;&#28304;&#39044;&#31639;&#25110;&#24182;&#34892;&#25191;&#34892;&#30340;&#20219;&#21153;&#19981;&#21516;&#65292;&#21442;&#19982;&#32773;&#35745;&#31639;&#36164;&#28304;&#23384;&#22312;&#30528;&#24046;&#24322;&#12290;&#32570;&#20047;&#35745;&#31639;&#39044;&#31639;&#30340;&#21442;&#19982;&#32773;&#24517;&#39035;&#36866;&#24403;&#35268;&#21010;&#20854;&#21463;&#38480;&#35745;&#31639;&#36164;&#28304;&#30340;&#20351;&#29992;&#65292;&#21542;&#21017;&#20182;&#20204;&#23558;&#26080;&#27861;&#23436;&#25104;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#65292;&#23548;&#33268;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#31639;&#26412;&#22320;&#27169;&#22411;&#32780;&#26080;&#38656;&#35745;&#31639;&#23494;&#38598;&#36845;&#20195;&#30340;&#31574;&#30053;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35745;&#31639;&#23450;&#21046;&#30340;&#32852;&#37030;&#24179;&#22343;&#31639;&#27861;(CC-FedAvg)&#65292;&#20801;&#35768;&#21442;&#19982;&#32773;&#26681;&#25454;&#20854;&#24403;&#21069;&#30340;&#35745;&#31639;&#39044;&#31639;&#65292;&#22312;&#27599;&#20010;&#36718;&#27425;&#20013;&#20915;&#23450;&#26159;&#25191;&#34892;&#20256;&#32479;&#30340;&#26412;&#22320;&#35757;&#32451;&#36824;&#26159;&#27169;&#22411;&#20272;&#31639;&#12290;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#32467;&#26524;&#22343;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#30340;&#32852;&#37030;&#24179;&#22343;&#31639;&#27861;&#30456;&#27604;&#65292;CC-FedAvg&#33021;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#24182;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is an emerging paradigm to train model with distributed data from numerous Internet of Things (IoT) devices. It inherently assumes a uniform capacity among participants. However, due to different conditions such as differing energy budgets or executing parallel unrelated tasks, participants have diverse computational resources in practice. Participants with insufficient computation budgets must plan for the use of restricted computational resources appropriately, otherwise they would be unable to complete the entire training procedure, resulting in model performance decline. To address the this issue, we propose a strategy for estimating local models without computationally intensive iterations. Based on it, we propose Computationally Customized Federated Averaging (CC-FedAvg), which allows participants to determine whether to perform traditional local training or model estimation in each round based on their current computational budgets. Both theoretical analy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;10,917&#31687;&#26032;&#38395;&#25991;&#31456;&#30340;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33258;&#21160;&#25353;&#20027;&#39064;&#23545;&#26032;&#38395;&#25991;&#31456;&#36827;&#34892;&#20998;&#31867;&#65292;&#23545;&#26032;&#38395;&#32467;&#26500;&#12289;&#20998;&#31867;&#21644;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#30340;&#30740;&#31350;&#20154;&#21592;&#38750;&#24120;&#26377;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2212.12061</link><description>&lt;p&gt;
MN-DS&#65306;&#26032;&#38395;&#25991;&#31456;&#23618;&#27425;&#20998;&#31867;&#30340;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MN-DS: A Multilabeled News Dataset for News Articles Hierarchical Classification. (arXiv:2212.12061v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12061
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;10,917&#31687;&#26032;&#38395;&#25991;&#31456;&#30340;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33258;&#21160;&#25353;&#20027;&#39064;&#23545;&#26032;&#38395;&#25991;&#31456;&#36827;&#34892;&#20998;&#31867;&#65292;&#23545;&#26032;&#38395;&#32467;&#26500;&#12289;&#20998;&#31867;&#21644;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#30340;&#30740;&#31350;&#20154;&#21592;&#38750;&#24120;&#26377;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;10,917&#31687;&#26032;&#38395;&#25991;&#31456;&#65292;&#28085;&#30422;&#20102;&#20174;2019&#24180;1&#26376;1&#26085;&#21040;2019&#24180;12&#26376;31&#26085;&#30340;&#23618;&#27425;&#26032;&#38395;&#20998;&#31867;&#12290;&#25105;&#20204;&#26681;&#25454;17&#20010;&#19968;&#32423;&#31867;&#21035;&#21644;109&#20010;&#20108;&#32423;&#31867;&#21035;&#30340;&#23618;&#27425;&#20998;&#31867;&#25163;&#21160;&#26631;&#35760;&#20102;&#36825;&#20123;&#25991;&#31456;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#33258;&#21160;&#25353;&#20027;&#39064;&#20998;&#31867;&#26032;&#38395;&#25991;&#31456;&#12290;&#35813;&#25968;&#25454;&#38598;&#23545;&#20110;&#20174;&#20107;&#26032;&#38395;&#32467;&#26500;&#12289;&#20998;&#31867;&#21644;&#26681;&#25454;&#21457;&#24067;&#30340;&#26032;&#38395;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#30340;&#30740;&#31350;&#20154;&#21592;&#38750;&#24120;&#26377;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article presents a dataset of 10,917 news articles with hierarchical news categories collected between January 1st 2019, and December 31st 2019. We manually labelled the articles based on a hierarchical taxonomy with 17 first-level and 109 second-level categories. This dataset can be used to train machine learning models for automatically classifying news articles by topic. This dataset can be helpful for researchers working on news structuring, classification, and predicting future events based on released news.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#25285;&#20445;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#31526;&#21512;&#33258;&#28982;&#31185;&#23398;&#24050;&#26377;&#30693;&#35782;&#24182;&#26368;&#20248;&#36924;&#36817;&#31995;&#32479;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.01346</link><description>&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;&#30340;&#33258;&#28982;&#32422;&#26463;&#25285;&#20445;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Guaranteed Conformance of Neurosymbolic Models to Natural Constraints. (arXiv:2212.01346v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01346
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#25285;&#20445;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#31526;&#21512;&#33258;&#28982;&#31185;&#23398;&#24050;&#26377;&#30693;&#35782;&#24182;&#26368;&#20248;&#36924;&#36817;&#31995;&#32479;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#25104;&#20026;&#22823;&#37096;&#20998;&#26426;&#22120;&#20154;&#21644;&#25511;&#21046;&#24212;&#29992;&#30340;&#20027;&#35201;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#20316;&#20026;&#21160;&#24577;&#31995;&#32479;&#27169;&#22411;&#12290;&#36825;&#31867;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#21448;&#21487;&#20197;&#29992;&#20110;&#35774;&#35745;&#21644;&#39564;&#35777;&#33258;&#20027;&#31995;&#32479;&#12290;&#22312;&#21307;&#30103;&#31995;&#32479;&#24314;&#27169;&#26041;&#38754;&#23588;&#20854;&#26377;&#29992;&#65292;&#22240;&#20026;&#25968;&#25454;&#33021;&#22815;&#34987;&#29992;&#20110;&#20010;&#20307;&#21270;&#27835;&#30103;&#12290;&#22312;&#23433;&#20840;&#20851;&#38190;&#30340;&#24212;&#29992;&#20013;&#65292;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#23545;&#26469;&#33258;&#33258;&#28982;&#31185;&#23398;&#30340;&#24050;&#26377;&#30693;&#35782;&#30340;&#31526;&#21512;&#24615;&#26174;&#24471;&#23588;&#20026;&#37325;&#35201;&#12290;&#36825;&#20123;&#30693;&#35782;&#36890;&#24120;&#26159;&#21487;&#29992;&#30340;&#65292;&#25110;&#32773;&#21487;&#20197;&#34987;&#27987;&#32553;&#25104;&#65288;&#21487;&#33021;&#26159;&#40657;&#30418;&#30340;&#65289;&#27169;&#22411;&#12290;&#20363;&#22914;&#65292;F1&#36187;&#36710;&#24212;&#31526;&#21512;&#29275;&#39039;&#23450;&#24459;&#65288;&#36825;&#34987;&#32534;&#30721;&#22312;&#19968;&#20010;&#21333;&#36718;&#27169;&#22411;&#20013;&#65289;&#12290;&#37492;&#20110;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#32771;&#34385;&#20197;&#19979;&#38382;&#39064;&#8212;&#8212;&#32473;&#23450;&#19968;&#20010;&#27169;&#22411;M&#21644;&#19968;&#20010;&#29366;&#24577;&#36716;&#31227;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#24076;&#26395;&#22312;&#36317;&#31163;M&#30340;&#33539;&#22260;&#20869;&#26368;&#22909;&#22320;&#36817;&#20284;&#31995;&#32479;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#26469;&#25285;&#20445;&#36825;&#31181;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#27493;&#26159;&#23558;&#25968;&#25454;&#38598;&#27987;&#32553;&#25104;&#20960;&#20010;&#20195;&#34920;&#24615;&#30340;&#26679;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have emerged as the workhorse for a large section of robotics and control applications, especially as models for dynamical systems. Such data-driven models are in turn used for designing and verifying autonomous systems. They are particularly useful in modeling medical systems where data can be leveraged to individualize treatment. In safety-critical applications, it is important that the data-driven model is conformant to established knowledge from the natural sciences. Such knowledge is often available or can often be distilled into a (possibly black-box) model. For instance, an F1 racing car should conform to Newton's laws (which are encoded within a unicycle model). In this light, we consider the following problem - given a model $M$ and a state transition dataset, we wish to best approximate the system model while being a bounded distance away from $M$. We propose a method to guarantee this conformance. Our first step is to distill the dataset into a few repre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;&#26041;&#27861;&#65292;&#37319;&#29992;&#24320;&#21457;&#21644;&#25506;&#32034;&#25910;&#36141;&#20989;&#25968;&#26469;&#24179;&#34913;&#25506;&#32034;&#21644;&#24320;&#21457;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20174;&#32780;&#25670;&#33073;&#20102;&#23616;&#37096;&#26368;&#20248;&#21644;&#38797;&#28857;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#31934;&#24230;&#21644;&#25910;&#25947;&#36895;&#24230;&#26041;&#38754;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#31232;&#30095;&#35757;&#32451;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.16667</link><description>&lt;p&gt;
&#24179;&#34913;&#25506;&#32034;&#21644;&#24320;&#21457;&#26435;&#34913;&#30340;&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Dynamic Sparse Training via Balancing the Exploration-Exploitation Trade-off. (arXiv:2211.16667v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16667
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;&#26041;&#27861;&#65292;&#37319;&#29992;&#24320;&#21457;&#21644;&#25506;&#32034;&#25910;&#36141;&#20989;&#25968;&#26469;&#24179;&#34913;&#25506;&#32034;&#21644;&#24320;&#21457;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20174;&#32780;&#25670;&#33073;&#20102;&#23616;&#37096;&#26368;&#20248;&#21644;&#38797;&#28857;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#31934;&#24230;&#21644;&#25910;&#25947;&#36895;&#24230;&#26041;&#38754;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#31232;&#30095;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36229;&#21442;&#25968;&#21270;&#24050;&#32463;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#34429;&#28982;&#26377;&#25928;&#65292;&#20294;&#22823;&#37327;&#21442;&#25968;&#38459;&#30861;&#20102;&#23427;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#30340;&#26222;&#21450;&#65292;&#24182;&#23545;&#29615;&#22659;&#20135;&#29983;&#19981;&#33391;&#24433;&#21709;&#12290;&#20351;&#29992;&#22266;&#23450;&#25968;&#37327;&#30340;&#38750;&#38646;&#26435;&#37325;&#26469;&#36827;&#34892;&#31232;&#30095;&#35757;&#32451;&#21487;&#20197;&#26174;&#30528;&#20943;&#36731;&#35757;&#32451;&#25104;&#26412;&#65292;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#65292;&#20294;&#29616;&#26377;&#30340;&#31232;&#30095;&#35757;&#32451;&#26041;&#27861;&#20027;&#35201;&#20351;&#29992;&#22522;&#20110;&#38543;&#26426;&#25110;&#36138;&#23146;&#30340;&#20943;&#23569;&#21644;&#22686;&#38271;&#31574;&#30053;&#65292;&#23548;&#33268;&#23616;&#37096;&#26368;&#23567;&#20540;&#21644;&#20302;&#31934;&#24230;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;&#35270;&#20026;&#31232;&#30095;&#36830;&#36890;&#24615;&#25628;&#32034;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#19968;&#20010;&#24320;&#21457;&#21644;&#25506;&#32034;&#25910;&#36141;&#20989;&#25968;&#26469;&#25670;&#33073;&#23616;&#37096;&#26368;&#20248;&#21644;&#38797;&#28857;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#19968;&#31181;&#25910;&#36141;&#21151;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#29702;&#35770;&#20445;&#35777;&#24182;&#38416;&#26126;&#20854;&#25910;&#25947;&#24615;&#36136;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31243;&#24207;&#33719;&#24471;&#20102;&#22810;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19978;&#19982;&#23494;&#38598;&#27169;&#22411;&#21487;&#27604;&#30340;&#31934;&#24230;&#65288;&#39640;&#36798;98&#65285;&#30340;&#31232;&#30095;&#24230;&#65289;&#65292;&#24182;&#19988;&#22312;&#31934;&#24230;&#21644;&#25910;&#25947;&#36895;&#24230;&#26041;&#38754;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#31232;&#30095;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over-parameterization of deep neural networks (DNNs) has shown high prediction accuracy for many applications. Although effective, the large number of parameters hinders its popularity on resource-limited devices and has an outsize environmental impact. Sparse training (using a fixed number of nonzero weights in each iteration) could significantly mitigate the training costs by reducing the model size. However, existing sparse training methods mainly use either random-based or greedy-based drop-and-grow strategies, resulting in local minimal and low accuracy. In this work, we consider the dynamic sparse training as a sparse connectivity search problem and design an exploitation and exploration acquisition function to escape from local optima and saddle points. We further design an acquisition function and provide the theoretical guarantees for the proposed method and clarify its convergence property. Experimental results show that sparse models (up to 98\% sparsity) obtained by our pro
&lt;/p&gt;</description></item><item><title>CEC&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#21442;&#25968;&#24773;&#26223;&#35760;&#24518;&#31639;&#27861;&#65292;&#29992;&#20110;&#36830;&#32493;&#24615;&#34892;&#21160;&#31354;&#38388;&#38382;&#39064;&#20013;&#30340;&#24207;&#21015;&#20915;&#31574;&#21046;&#23450;&#65292;&#20854;&#22312;&#20960;&#20010;&#31232;&#30095;&#22870;&#21169;&#36830;&#32493;&#25511;&#21046;&#29615;&#22659;&#20013;&#27604;&#26368;&#20808;&#36827;&#30340;RL&#21644;&#35760;&#24518;&#22686;&#24378;RL&#31639;&#27861;&#23398;&#20064;&#26356;&#24555;&#65292;&#26159;&#23398;&#20064;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#30340;&#24555;&#36895;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.15183</link><description>&lt;p&gt;
&#36830;&#32493;&#24773;&#26223;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Continuous Episodic Control. (arXiv:2211.15183v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15183
&lt;/p&gt;
&lt;p&gt;
CEC&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#21442;&#25968;&#24773;&#26223;&#35760;&#24518;&#31639;&#27861;&#65292;&#29992;&#20110;&#36830;&#32493;&#24615;&#34892;&#21160;&#31354;&#38388;&#38382;&#39064;&#20013;&#30340;&#24207;&#21015;&#20915;&#31574;&#21046;&#23450;&#65292;&#20854;&#22312;&#20960;&#20010;&#31232;&#30095;&#22870;&#21169;&#36830;&#32493;&#25511;&#21046;&#29615;&#22659;&#20013;&#27604;&#26368;&#20808;&#36827;&#30340;RL&#21644;&#35760;&#24518;&#22686;&#24378;RL&#31639;&#27861;&#23398;&#20064;&#26356;&#24555;&#65292;&#26159;&#23398;&#20064;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#30340;&#24555;&#36895;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#21442;&#25968;&#24773;&#26223;&#35760;&#24518;&#21487;&#20197;&#29992;&#20110;&#24555;&#36895;&#38145;&#23450;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#39640;&#22870;&#21169;&#30340;&#32463;&#39564;&#12290;&#19982;&#21442;&#25968;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#21442;&#25968;&#38656;&#35201;&#32531;&#24930;&#22320;&#21453;&#21521;&#20256;&#36882;&#22870;&#21169;&#20449;&#21495;&#30340;&#26041;&#27861;&#20013;&#65292;&#36825;&#20123;&#26041;&#27861;&#21482;&#38656;&#35201;&#21457;&#29616;&#19968;&#27425;&#35299;&#20915;&#26041;&#26696;&#65292;&#28982;&#21518;&#23601;&#21487;&#20197;&#21453;&#22797;&#35299;&#20915;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24773;&#26223;&#25511;&#21046;&#35299;&#20915;&#26041;&#26696;&#23384;&#20648;&#22312;&#31163;&#25955;&#34920;&#20013;&#65292;&#36825;&#31181;&#26041;&#27861;&#36804;&#20170;&#21482;&#24212;&#29992;&#20110;&#31163;&#25955;&#34892;&#21160;&#31354;&#38388;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#36830;&#32493;&#24773;&#26223;&#25511;&#21046;&#65288;CEC&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#21442;&#25968;&#24773;&#26223;&#35760;&#24518;&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#36830;&#32493;&#24615;&#34892;&#21160;&#31354;&#38388;&#38382;&#39064;&#20013;&#30340;&#24207;&#21015;&#20915;&#31574;&#21046;&#23450;&#12290;&#22312;&#20960;&#20010;&#31232;&#30095;&#22870;&#21169;&#36830;&#32493;&#25511;&#21046;&#29615;&#22659;&#20013;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#27604;&#26368;&#20808;&#36827;&#30340;&#26080;&#27169;&#22411;RL&#21644;&#35760;&#24518;&#22686;&#24378;RL&#31639;&#27861;&#23398;&#20064;&#26356;&#24555;&#65292;&#21516;&#26102;&#20445;&#25345;&#33391;&#22909;&#30340;&#38271;&#26399;&#24615;&#33021;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;CEC&#21487;&#20197;&#26159;&#23398;&#20064;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#30340;&#24555;&#36895;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Non-parametric episodic memory can be used to quickly latch onto high-rewarded experience in reinforcement learning tasks. In contrast to parametric deep reinforcement learning approaches in which reward signals need to be back-propagated slowly, these methods only need to discover the solution once, and may then repeatedly solve the task. However, episodic control solutions are stored in discrete tables, and this approach has so far only been applied to discrete action space problems. Therefore, this paper introduces Continuous Episodic Control (CEC), a novel non-parametric episodic memory algorithm for sequential decision making in problems with a continuous action space. Results on several sparse-reward continuous control environments show that our proposed method learns faster than state-of-the-art model-free RL and memory-augmented RL algorithms, while maintaining good long-run performance as well. In short, CEC can be a fast approach for learning in continuous control tasks.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#24037;&#20855;Melting Pot 2.0&#20026;&#22810;&#26234;&#33021;&#20307;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#20102;&#35780;&#20272;&#21327;&#35758;&#65292;&#22312;&#19968;&#32452;&#20856;&#22411;&#27979;&#35797;&#22330;&#26223;&#20013;&#27979;&#37327;&#23427;&#20204;&#23545;&#26032;&#39062;&#31038;&#20132;&#20249;&#20276;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.13746</link><description>&lt;p&gt;
&#29076;&#28809;2.0
&lt;/p&gt;
&lt;p&gt;
Melting Pot 2.0. (arXiv:2211.13746v4 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13746
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#24037;&#20855;Melting Pot 2.0&#20026;&#22810;&#26234;&#33021;&#20307;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#20102;&#35780;&#20272;&#21327;&#35758;&#65292;&#22312;&#19968;&#32452;&#20856;&#22411;&#27979;&#35797;&#22330;&#26223;&#20013;&#27979;&#37327;&#23427;&#20204;&#23545;&#26032;&#39062;&#31038;&#20132;&#20249;&#20276;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#25215;&#35834;&#24320;&#21457;&#27604;&#8220;&#33258;&#25105;&#20013;&#24515;&#8221;&#26041;&#27861;&#26356;&#20855;&#20154;&#31867;&#29305;&#28857;&#21644;&#26356;&#26131;&#20110;&#19982;&#20154;&#31867;&#20860;&#23481;&#30340;&#26234;&#33021;&#25216;&#26415;&#12290; Melting Pot&#26159;&#20026;&#20419;&#36827;&#22810;&#26234;&#33021;&#20307;&#20154;&#24037;&#26234;&#33021;&#24037;&#20316;&#32780;&#24320;&#21457;&#30340;&#30740;&#31350;&#24037;&#20855;&#65292;&#24182;&#25552;&#20379;&#19968;&#20010;&#35780;&#20272;&#21327;&#35758;&#65292;&#35813;&#21327;&#35758;&#22312;&#19968;&#32452;&#20856;&#22411;&#30340;&#27979;&#35797;&#22330;&#26223;&#20013;&#27979;&#37327;&#23545;&#26032;&#39062;&#31038;&#20132;&#20249;&#20276;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#27599;&#31181;&#24773;&#26223;&#23558;&#19968;&#20010;&#29289;&#29702;&#29615;&#22659;&#65288;&#8220;&#22522;&#26495;&#8221;&#65289;&#19982;&#19968;&#32452;&#21442;&#32771;&#21512;&#20316;&#32773;&#65288;&#8220;&#32972;&#26223;&#20154;&#32676;&#8221;&#65289;&#37197;&#23545;&#65292;&#20197;&#21019;&#24314;&#19968;&#20010;&#20855;&#26377;&#20010;&#20307;&#38388;&#30456;&#20114;&#20381;&#23384;&#24615;&#30340;&#31038;&#20132;&#24773;&#22659;&#12290;&#20363;&#22914;&#65292;&#19968;&#20123;&#24773;&#24418;&#21463;&#21040;&#20102;&#22522;&#20110;&#21046;&#24230;&#32463;&#27982;&#23398;&#30340;&#33258;&#28982;&#36164;&#28304;&#31649;&#29702;&#21644;&#20844;&#20849;&#29289;&#21697;&#20379;&#32473;&#22256;&#22659;&#30340;&#32771;&#34385;&#30340;&#21551;&#21457;&#65292;&#32780;&#20854;&#20182;&#24773;&#24418;&#21017;&#21463;&#21040;&#20102;&#36827;&#21270;&#29983;&#29289;&#23398;&#12289;&#21338;&#24328;&#35770;&#21644;&#20154;&#24037;&#29983;&#21629;&#31561;&#26041;&#38754;&#30340;&#32771;&#34385;&#25152;&#21551;&#21457;&#12290;Melting Pot&#26088;&#22312;&#28085;&#30422;&#19968;&#32452;&#26368;&#22823;&#22810;&#26679;&#21270;&#30340;&#24773;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-agent artificial intelligence research promises a path to develop intelligent technologies that are more human-like and more human-compatible than those produced by "solipsistic" approaches, which do not consider interactions between agents. Melting Pot is a research tool developed to facilitate work on multi-agent artificial intelligence, and provides an evaluation protocol that measures generalization to novel social partners in a set of canonical test scenarios. Each scenario pairs a physical environment (a "substrate") with a reference set of co-players (a "background population"), to create a social situation with substantial interdependence between the individuals involved. For instance, some scenarios were inspired by institutional-economics-based accounts of natural resource management and public-good-provision dilemmas. Others were inspired by considerations from evolutionary biology, game theory, and artificial life. Melting Pot aims to cover a maximally diverse set of 
&lt;/p&gt;</description></item><item><title>GammaE&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#23884;&#20837;&#27169;&#22411;&#65292;&#21033;&#29992;&#20102;Gamma&#20998;&#24067;&#30340;&#32447;&#24615;&#29305;&#24615;&#21644;&#24378;&#36793;&#30028;&#25903;&#25345;&#26469;&#25429;&#25417;&#23454;&#20307;&#21644;&#26597;&#35810;&#30340;&#26356;&#22810;&#29305;&#24449;&#65292;&#35299;&#20915;&#20102;&#30693;&#35782;&#22270;&#35889;&#20013;&#21542;&#23450;&#21644;&#32852;&#21512;&#31639;&#31526;&#30340;&#24314;&#27169;&#38382;&#39064;&#65292;&#24182;&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#22238;&#31572;&#19981;&#21516;&#31867;&#22411;&#30340;FOL&#26597;&#35810;&#12290;</title><link>http://arxiv.org/abs/2210.15578</link><description>&lt;p&gt;
GammaE: &#22522;&#20110;Gamma&#20998;&#24067;&#30340;&#30693;&#35782;&#22270;&#35889;&#36923;&#36753;&#26597;&#35810;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
GammaE: Gamma Embeddings for Logical Queries on Knowledge Graphs. (arXiv:2210.15578v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15578
&lt;/p&gt;
&lt;p&gt;
GammaE&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#23884;&#20837;&#27169;&#22411;&#65292;&#21033;&#29992;&#20102;Gamma&#20998;&#24067;&#30340;&#32447;&#24615;&#29305;&#24615;&#21644;&#24378;&#36793;&#30028;&#25903;&#25345;&#26469;&#25429;&#25417;&#23454;&#20307;&#21644;&#26597;&#35810;&#30340;&#26356;&#22810;&#29305;&#24449;&#65292;&#35299;&#20915;&#20102;&#30693;&#35782;&#22270;&#35889;&#20013;&#21542;&#23450;&#21644;&#32852;&#21512;&#31639;&#31526;&#30340;&#24314;&#27169;&#38382;&#39064;&#65292;&#24182;&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#22238;&#31572;&#19981;&#21516;&#31867;&#22411;&#30340;FOL&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#36827;&#34892;&#23884;&#20837;&#20197;&#36827;&#34892;&#22810;&#36339;&#36923;&#36753;&#25512;&#29702;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#35768;&#22810;KG&#20855;&#26377;&#24222;&#22823;&#32780;&#22797;&#26434;&#30340;&#32467;&#26500;&#12290;&#26368;&#36817;&#65292;&#35768;&#22810;&#26377;&#21069;&#36884;&#30340;&#24037;&#20316;&#23558;&#23454;&#20307;&#21644;&#26597;&#35810;&#25237;&#24433;&#21040;&#20960;&#20309;&#31354;&#38388;&#20013;&#20197;&#26377;&#25928;&#22320;&#25214;&#21040;&#31572;&#26696;&#12290;&#20294;&#26159;&#65292;&#24314;&#27169;&#21542;&#23450;&#21644;&#32852;&#21512;&#36816;&#31639;&#31526;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#21542;&#23450;&#36816;&#31639;&#31526;&#27809;&#26377;&#20005;&#26684;&#30340;&#36793;&#30028;&#65292;&#36825;&#20250;&#29983;&#25104;&#37325;&#21472;&#30340;&#23884;&#20837;&#24182;&#23548;&#33268;&#33719;&#24471;&#27169;&#31946;&#30340;&#31572;&#26696;&#12290;&#21478;&#19968;&#20010;&#38480;&#21046;&#26159;&#24182;&#38598;&#36816;&#31639;&#31526;&#26159;&#38750;&#38381;&#21512;&#30340;&#65292;&#36825;&#21066;&#24369;&#20102;&#27169;&#22411;&#22788;&#29702;&#19968;&#31995;&#21015;&#24182;&#38598;&#36816;&#31639;&#31526;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27010;&#29575;&#23884;&#20837;&#27169;&#22411;&#65292;&#21363;Gamma Embeddings&#65288;GammaE&#65289;&#65292;&#29992;&#20110;&#32534;&#30721;&#23454;&#20307;&#21644;&#26597;&#35810;&#20197;&#22238;&#31572;KG&#19978;&#19981;&#21516;&#31867;&#22411;&#30340;FOL&#26597;&#35810;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;Gamma&#20998;&#24067;&#30340;&#32447;&#24615;&#29305;&#24615;&#21644;&#24378;&#36793;&#30028;&#25903;&#25345;&#26469;&#25429;&#25417;&#23454;&#20307;&#21644;&#26597;&#35810;&#30340;&#26356;&#22810;&#29305;&#24449;&#65292;&#20174;&#32780;&#26497;&#22823;&#22320;&#20943;&#23569;&#20102;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;GammaE&#23454;&#29616;&#20102;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#22788;&#29702;&#21542;&#23450;&#21644;&#32852;&#21512;&#36816;&#31639;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;
Embedding knowledge graphs (KGs) for multi-hop logical reasoning is a challenging problem due to massive and complicated structures in many KGs. Recently, many promising works projected entities and queries into a geometric space to efficiently find answers. However, it remains challenging to model the negation and union operator. The negation operator has no strict boundaries, which generates overlapped embeddings and leads to obtaining ambiguous answers. An additional limitation is that the union operator is non-closure, which undermines the model to handle a series of union operators. To address these problems, we propose a novel probabilistic embedding model, namely Gamma Embeddings (GammaE), for encoding entities and queries to answer different types of FOL queries on KGs. We utilize the linear property and strong boundary support of the Gamma distribution to capture more features of entities and queries, which dramatically reduces model uncertainty. Furthermore, GammaE implements
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#65292;&#21253;&#25324;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;&#26426;&#22120;&#20154;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2210.14891</link><description>&lt;p&gt;
&#30772;&#30862;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Broken Neural Scaling Laws. (arXiv:2210.14891v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#65292;&#21253;&#25324;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;&#26426;&#22120;&#20154;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a smoothly broken power law functional form (referred to as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks for various architectures and a large and diverse set of tasks, including vision, language, audio, video, generative modeling, contrastive learning, robotics, uncertainty estimation/calibration, adversarial robustness, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforcement learning.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#30772;&#30862;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#65288;BNSL&#65289;&#65289;&#65292;&#23427;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65288;&#21363;&#24863;&#20852;&#36259;&#30340;&#35780;&#20272;&#25351;&#26631;&#38543;&#29992;&#20110;&#35757;&#32451;&#30340;&#35745;&#31639;&#37327;&#12289;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#12289;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#25110;&#19978;&#28216;&#24615;&#33021;&#21464;&#21270;&#32780;&#21464;&#21270;&#65289;&#23545;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#20013;&#30340;&#27599;&#20010;&#20219;&#21153;&#65292;&#21253;&#25324;&#22823;&#35268;&#27169;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#25193;&#25955;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#22810;&#27169;&#24577;&#23398;&#20064;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;AI&#23545;&#40784;&#12289;&#26426;&#22120;&#20154;&#12289;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#12289;&#25345;&#32493;&#23398;&#20064;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#33976;&#39311;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a smoothly broken power law functional form (referred to by us as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks (i.e. how the evaluation metric of interest varies as the amount of compute used for training, number of model parameters, training dataset size, or upstream performance varies) for various architectures and for each of various tasks within a large and diverse set of upstream and downstream tasks, in zero-shot, prompted, and fine-tuned settings. This set includes large-scale vision, language, audio, video, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, robotics, out-of-distribution (OOD) generalization, continual learning, uncertainty estimation / calibration, out-of-distribution detection, adversarial robustness, distillation, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforc
&lt;/p&gt;</description></item><item><title>Alt-Diff&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#23545;&#25972;&#20010;&#38597;&#21487;&#27604;&#30697;&#38453;&#36827;&#34892;&#26114;&#36149;&#35745;&#31639;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;&#24555;&#36895;&#21644;&#36882;&#24402;&#30340;&#26041;&#24335;&#24494;&#20998;&#20248;&#21270;&#38382;&#39064;&#65292;&#20174;&#32780;&#22823;&#22823;&#25552;&#39640;&#38544;&#24335;&#24494;&#20998;&#30340;&#35745;&#31639;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2210.01802</link><description>&lt;p&gt;
&#20248;&#21270;&#23618;&#30340;&#20132;&#26367;&#24494;&#20998;
&lt;/p&gt;
&lt;p&gt;
Alternating Differentiation for Optimization Layers. (arXiv:2210.01802v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01802
&lt;/p&gt;
&lt;p&gt;
Alt-Diff&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#23545;&#25972;&#20010;&#38597;&#21487;&#27604;&#30697;&#38453;&#36827;&#34892;&#26114;&#36149;&#35745;&#31639;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;&#24555;&#36895;&#21644;&#36882;&#24402;&#30340;&#26041;&#24335;&#24494;&#20998;&#20248;&#21270;&#38382;&#39064;&#65292;&#20174;&#32780;&#22823;&#22823;&#25552;&#39640;&#38544;&#24335;&#24494;&#20998;&#30340;&#35745;&#31639;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#20248;&#21270;&#38382;&#39064;&#23884;&#20837;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#20248;&#21270;&#23618;&#20197;&#32534;&#30721;&#32422;&#26463;&#21644;&#24402;&#32435;&#20808;&#39564;&#30340;&#24819;&#27861;&#22312;&#36817;&#24180;&#26469;&#24050;&#32463;&#28145;&#20837;&#20154;&#24515;&#12290;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#38598;&#20013;&#22312;&#20197;&#19968;&#31181;&#38656;&#35201;&#22312;&#38597;&#21487;&#27604;&#30697;&#38453;&#19978;&#36827;&#34892;&#26114;&#36149;&#35745;&#31639;&#30340;&#26041;&#24335;&#38544;&#24335;&#24494;&#20998;Karush-Kuhn-Tucker&#65288;KKT&#65289;&#26465;&#20214;&#19978;&#65292;&#36825;&#21487;&#33021;&#26159;&#24930;&#21644;&#20869;&#23384;&#23494;&#38598;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;&#20132;&#26367;&#24494;&#20998;&#65288;Alt-Diff&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#20197;&#19968;&#31181;&#24555;&#36895;&#19988;&#36882;&#24402;&#30340;&#26041;&#24335;&#24494;&#20998;&#20248;&#21270;&#38382;&#39064;&#65288;&#36825;&#37324;&#29305;&#21035;&#25351;&#24102;&#26377;&#22810;&#38754;&#20307;&#32422;&#26463;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#65289;&#12290;Alt-Diff&#23558;&#24494;&#20998;&#36807;&#31243;&#20998;&#35299;&#20026;&#20027;&#38382;&#39064;&#26356;&#26032;&#21644;&#23545;&#20598;&#38382;&#39064;&#26356;&#26032;&#30340;&#20132;&#26367;&#26041;&#24335;&#12290;&#22240;&#27492;&#65292;Alt-Diff&#23588;&#20854;&#33021;&#22815;&#20943;&#23567;&#38597;&#21487;&#27604;&#30697;&#38453;&#30340;&#32500;&#24230;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#20855;&#26377;&#22823;&#35268;&#27169;&#32422;&#26463;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#38544;&#24335;&#24494;&#20998;&#30340;&#35745;&#31639;&#36895;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;Alt-Diff&#33719;&#24471;&#30340;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
The idea of embedding optimization problems into deep neural networks as optimization layers to encode constraints and inductive priors has taken hold in recent years. Most existing methods focus on implicitly differentiating Karush-Kuhn-Tucker (KKT) conditions in a way that requires expensive computations on the Jacobian matrix, which can be slow and memory-intensive. In this paper, we developed a new framework, named Alternating Differentiation (Alt-Diff), that differentiates optimization problems (here, specifically in the form of convex optimization problems with polyhedral constraints) in a fast and recursive way. Alt-Diff decouples the differentiation procedure into a primal update and a dual update in an alternating way. Accordingly, Alt-Diff substantially decreases the dimensions of the Jacobian matrix especially for optimization with large-scale constraints and thus increases the computational speed of implicit differentiation. We show that the gradients obtained by Alt-Diff a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#21033;&#29992;&#22823;&#37327;&#20844;&#20849;&#31070;&#32463;&#25104;&#20687;&#25968;&#25454;&#24211;&#36827;&#34892;&#33258;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#22823;&#33041;&#35299;&#30721;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65292;&#20294;&#20854;&#30410;&#22788;&#30340;&#22823;&#23567;&#21463;&#21040;&#22810;&#20010;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2209.10099</link><description>&lt;p&gt;
&#33258;&#23398;&#20064;&#23545;&#22823;&#33041;&#35299;&#30721;&#30340;&#30410;&#22788;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the benefits of self-taught learning for brain decoding. (arXiv:2209.10099v4 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.10099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#21033;&#29992;&#22823;&#37327;&#20844;&#20849;&#31070;&#32463;&#25104;&#20687;&#25968;&#25454;&#24211;&#36827;&#34892;&#33258;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#22823;&#33041;&#35299;&#30721;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65292;&#20294;&#20854;&#30410;&#22788;&#30340;&#22823;&#23567;&#21463;&#21040;&#22810;&#20010;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#33258;&#23398;&#20064;&#26694;&#26550;&#19979;&#20351;&#29992;&#22823;&#35268;&#27169;&#20844;&#20849;&#31070;&#32463;&#25104;&#20687;&#25968;&#25454;&#24211;&#20013;&#30340;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#32479;&#35745;&#22270;&#30340;&#22909;&#22788;&#65292;&#20197;&#25913;&#21892;&#35299;&#30721;&#26032;&#20219;&#21153;&#26102;&#30340;&#34920;&#29616;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;NeuroVault&#25968;&#25454;&#24211;&#65292;&#22312;&#19968;&#20123;&#30456;&#20851;&#30340;&#32479;&#35745;&#22270;&#19978;&#35757;&#32451;&#21367;&#31215;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#23545;&#36825;&#20123;&#22270;&#36827;&#34892;&#37325;&#24314;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#24050;&#35757;&#32451;&#22909;&#30340;&#32534;&#30721;&#22120;&#26469;&#21021;&#22987;&#21270;&#19968;&#20010;&#26377;&#30417;&#30563;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#23545;&#20174;NeuroVault&#25968;&#25454;&#24211;&#22823;&#37327;&#25910;&#38598;&#30340;&#26410;&#35265;&#36807;&#30340;&#32479;&#35745;&#22270;&#30340;&#20219;&#21153;&#25110;&#35748;&#30693;&#36807;&#31243;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#21457;&#29616;&#33258;&#23398;&#20064;&#36807;&#31243;&#22987;&#32456;&#21487;&#20197;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65292;&#20294;&#20854;&#30410;&#22788;&#30340;&#22823;&#23567;&#24378;&#28872;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#27169;&#22411;&#20351;&#29992;&#30340;&#26679;&#26412;&#25968;&#37327;&#20197;&#21450;&#30446;&#26631;&#19979;&#28216;&#20219;&#21153;&#30340;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Context. We study the benefits of using a large public neuroimaging database composed of fMRI statistic maps, in a self-taught learning framework, for improving brain decoding on new tasks. First, we leverage the NeuroVault database to train, on a selection of relevant statistic maps, a convolutional autoencoder to reconstruct these maps. Then, we use this trained encoder to initialize a supervised convolutional neural network to classify tasks or cognitive processes of unseen statistic maps from large collections of the NeuroVault database. Results. We show that such a self-taught learning process always improves the performance of the classifiers but the magnitude of the benefits strongly depends on the number of samples available both for pre-training and finetuning the models and on the complexity of the targeted downstream task. Conclusion. The pre-trained model improves the classification performance and displays more generalizable features, less sensitive to individual differenc
&lt;/p&gt;</description></item><item><title>LANIT&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#30340;&#22270;&#20687;&#36716;&#25442;&#26041;&#27861;&#65292;&#21033;&#29992;&#25991;&#26412;&#20505;&#36873;&#23646;&#24615;&#26469;&#25351;&#31034;&#27599;&#20010;&#26679;&#26412;&#30340;&#22495;&#26631;&#31614;&#65292;&#23454;&#29616;&#20102;&#22810;&#37325;&#28909;&#26631;&#31614;&#65292;&#20197;&#20415;&#29992;&#25143;&#21487;&#20197;&#29992;&#35821;&#35328;&#25351;&#23450;&#24102;&#26377;&#23646;&#24615;&#38598;&#30340;&#30446;&#26631;&#22495;&#12290;&#21516;&#26102;&#65292;&#25552;&#31034;&#23398;&#20064;&#26426;&#21046;&#21487;&#20197;&#24212;&#23545;&#26368;&#21021;&#25552;&#31034;&#19981;&#20934;&#30830;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2208.14889</link><description>&lt;p&gt;
LANIT&#65306;&#22522;&#20110;&#35821;&#35328;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#22270;&#20687;&#36716;&#25442;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LANIT: Language-Driven Image-to-Image Translation for Unlabeled Data. (arXiv:2208.14889v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.14889
&lt;/p&gt;
&lt;p&gt;
LANIT&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#30340;&#22270;&#20687;&#36716;&#25442;&#26041;&#27861;&#65292;&#21033;&#29992;&#25991;&#26412;&#20505;&#36873;&#23646;&#24615;&#26469;&#25351;&#31034;&#27599;&#20010;&#26679;&#26412;&#30340;&#22495;&#26631;&#31614;&#65292;&#23454;&#29616;&#20102;&#22810;&#37325;&#28909;&#26631;&#31614;&#65292;&#20197;&#20415;&#29992;&#25143;&#21487;&#20197;&#29992;&#35821;&#35328;&#25351;&#23450;&#24102;&#26377;&#23646;&#24615;&#38598;&#30340;&#30446;&#26631;&#22495;&#12290;&#21516;&#26102;&#65292;&#25552;&#31034;&#23398;&#20064;&#26426;&#21046;&#21487;&#20197;&#24212;&#23545;&#26368;&#21021;&#25552;&#31034;&#19981;&#20934;&#30830;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22270;&#20687;&#36716;&#25442;&#25216;&#26415;&#36890;&#24120;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#23545;&#27599;&#20010;&#26679;&#26412;&#22495;&#27880;&#37322;&#30340;&#24378;&#20381;&#36182;&#24615;&#21644;/&#25110;&#26080;&#27861;&#22788;&#29702;&#27599;&#20010;&#22270;&#20687;&#30340;&#22810;&#20010;&#23646;&#24615;&#12290;&#26368;&#36817;&#30340;&#30495;&#27491;&#26080;&#30417;&#30563;&#26041;&#27861;&#37319;&#29992;&#20102;&#32858;&#31867;&#26041;&#27861;&#65292;&#20197;&#36731;&#26494;&#25552;&#20379;&#27599;&#20010;&#26679;&#26412;&#30340;&#21333;&#28909;&#22495;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26080;&#27861;&#32771;&#34385;&#21040;&#30495;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#65306;&#19968;&#20010;&#26679;&#26412;&#21487;&#33021;&#20855;&#26377;&#22810;&#20010;&#23646;&#24615;&#12290;&#27492;&#22806;&#65292;&#31751;&#30340;&#35821;&#20041;&#19982;&#20154;&#31867;&#29702;&#35299;&#19981;&#23481;&#26131;&#32806;&#21512;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#20110;&#35821;&#35328;&#30340;LANguage-driven Image-to-image Translation&#65288;LANIT&#65289;&#27169;&#22411;&#12290;&#25105;&#20204;&#21033;&#29992;&#22312;&#25968;&#25454;&#38598;&#20013;&#25552;&#20379;&#30340;&#26131;&#20110;&#33719;&#24471;&#30340;&#25991;&#26412;&#20505;&#36873;&#23646;&#24615;&#65306;&#22270;&#20687;&#21644;&#23646;&#24615;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#25351;&#31034;&#27599;&#20010;&#26679;&#26412;&#30340;&#22495;&#26631;&#31614;&#12290;&#36825;&#31181;&#24418;&#24335;&#33258;&#28982;&#22320;&#21551;&#29992;&#20102;&#22810;&#28909;&#26631;&#31614;&#65292;&#20197;&#20415;&#29992;&#25143;&#21487;&#20197;&#29992;&#35821;&#35328;&#25351;&#23450;&#24102;&#26377;&#23646;&#24615;&#38598;&#30340;&#30446;&#26631;&#22495;&#12290;&#20026;&#20102;&#24212;&#23545;&#26368;&#21021;&#25552;&#31034;&#19981;&#20934;&#30830;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#25552;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing techniques for image-to-image translation commonly have suffered from two critical problems: heavy reliance on per-sample domain annotation and/or inability of handling multiple attributes per image. Recent truly-unsupervised methods adopt clustering approaches to easily provide per-sample one-hot domain labels. However, they cannot account for the real-world setting: one sample may have multiple attributes. In addition, the semantics of the clusters are not easily coupled to the human understanding. To overcome these, we present a LANguage-driven Image-to-image Translation model, dubbed LANIT. We leverage easy-to-obtain candidate attributes given in texts for a dataset: the similarity between images and attributes indicates per-sample domain labels. This formulation naturally enables multi-hot label so that users can specify the target domain with a set of attributes in language. To account for the case that the initial prompts are inaccurate, we also present prompt learning.
&lt;/p&gt;</description></item><item><title>DSL&#26159;&#19968;&#31181;&#33021;&#22815;&#22312;&#23398;&#20064;&#20013;&#33258;&#21160;&#21457;&#29616;&#26377;&#24847;&#20041;&#31526;&#21495;&#35268;&#21017;&#30340;NeSy&#31995;&#32479;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2208.11561</link><description>&lt;p&gt;
&#28145;&#24230;&#31526;&#21495;&#23398;&#20064;&#65306;&#20174;&#24863;&#30693;&#20013;&#21457;&#29616;&#31526;&#21495;&#21644;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
Deep Symbolic Learning: Discovering Symbols and Rules from Perceptions. (arXiv:2208.11561v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.11561
&lt;/p&gt;
&lt;p&gt;
DSL&#26159;&#19968;&#31181;&#33021;&#22815;&#22312;&#23398;&#20064;&#20013;&#33258;&#21160;&#21457;&#29616;&#26377;&#24847;&#20041;&#31526;&#21495;&#35268;&#21017;&#30340;NeSy&#31995;&#32479;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#65288;NeSy&#65289;&#38598;&#25104;&#23558;&#31526;&#21495;&#25512;&#29702;&#19982;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#38656;&#35201;&#24863;&#30693;&#21644;&#25512;&#29702;&#30340;&#20219;&#21153;&#12290;&#22823;&#22810;&#25968;NeSy&#31995;&#32479;&#20381;&#36182;&#20110;&#36923;&#36753;&#30693;&#35782;&#30340;&#36830;&#32493;&#25918;&#26494;&#65292;&#24182;&#19988;&#27169;&#22411;&#31649;&#36947;&#20869;&#19981;&#20570;&#20986;&#31163;&#25955;&#20915;&#31574;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#20551;&#23450;&#32473;&#23450;&#31526;&#21495;&#35268;&#21017;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#28145;&#24230;&#31526;&#21495;&#23398;&#20064;&#65288;DSL&#65289;&#65292;&#19968;&#31181;NeSy&#31995;&#32479;&#65292;&#23427;&#21487;&#20197;&#23398;&#20064;NeSy&#20989;&#25968;&#65292;&#21363;&#23558;&#36830;&#32493;&#25968;&#25454;&#26144;&#23556;&#21040;&#31163;&#25955;&#31526;&#21495;&#30340;&#65288;&#19968;&#32452;&#65289;&#24863;&#30693;&#20989;&#25968;&#30340;&#32452;&#21512;&#21644;&#31526;&#21495;&#20989;&#25968;&#12290;DSL&#21516;&#26102;&#23398;&#20064;&#24863;&#30693;&#21644;&#31526;&#21495;&#20989;&#25968;&#65292;&#20165;&#22312;&#23427;&#20204;&#30340;&#32452;&#21512;&#65288;NeSy&#20989;&#25968;&#65289;&#19978;&#36827;&#34892;&#22521;&#35757;&#12290;DSL&#30340;&#20851;&#38190;&#21019;&#26032;&#22312;&#20110;&#65292;&#23427;&#21487;&#20197;&#22312;&#21487;&#24494;&#30340;NN&#23398;&#20064;&#31649;&#36947;&#20869;&#21019;&#24314;&#20869;&#37096;&#65288;&#21487;&#35299;&#37322;&#30340;&#65289;&#31526;&#21495;&#34920;&#31034;&#65292;&#24182;&#23558;&#23427;&#20204;&#26144;&#23556;&#21040;&#24863;&#30693;&#36755;&#20837;&#12290;&#21019;&#24314;&#30340;&#31526;&#21495;&#26159;&#33258;&#21160;&#36873;&#25321;&#30340;&#65292;&#20197;&#29983;&#25104;&#26368;&#22909;&#35299;&#37322;&#25968;&#25454;&#30340;&#31526;&#21495;&#20989;&#25968;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#20851;&#20110;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;DSL&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23398;&#20064;&#31526;&#21495;&#35268;&#21017;&#21644;&#21457;&#29616;&#26377;&#24847;&#20041;&#30340;&#31526;&#21495;&#30340;&#33021;&#21147;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neuro-Symbolic (NeSy) integration combines symbolic reasoning with Neural Networks (NNs) for tasks requiring perception and reasoning. Most NeSy systems rely on continuous relaxation of logical knowledge, and no discrete decisions are made within the model pipeline. Furthermore, these methods assume that the symbolic rules are given. In this paper, we propose Deep Symbolic Learning (DSL), a NeSy system that learns NeSy-functions, i.e., the composition of a (set of) perception functions which map continuous data to discrete symbols, and a symbolic function over the set of symbols. DSL learns simultaneously the perception and symbolic functions while being trained only on their composition (NeSy-function). The key novelty of DSL is that it can create internal (interpretable) symbolic representations and map them to perception inputs within a differentiable NN learning pipeline. The created symbols are automatically selected to generate symbolic functions that best explain the data. We pr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31526;&#21512;&#20445;&#24207;&#30340;&#39118;&#38505;&#25511;&#21046;&#26041;&#27861;&#65292;&#21487;&#20197;&#25511;&#21046;&#20219;&#20309;&#21333;&#35843;&#25439;&#22833;&#20989;&#25968;&#30340;&#26399;&#26395;&#20540;&#65292;&#31034;&#20363;&#35777;&#26126;&#20854;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20855;&#26377;&#25511;&#21046;&#35823;&#25253;&#29575;&#12289;&#22270;&#24418;&#36317;&#31163;&#21644;&#20196;&#29260;&#32423;F1&#24471;&#20998;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2208.02814</link><description>&lt;p&gt;
&#19968;&#31181;&#31526;&#21512;&#20445;&#24207;&#30340;&#39118;&#38505;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Conformal Risk Control. (arXiv:2208.02814v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.02814
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31526;&#21512;&#20445;&#24207;&#30340;&#39118;&#38505;&#25511;&#21046;&#26041;&#27861;&#65292;&#21487;&#20197;&#25511;&#21046;&#20219;&#20309;&#21333;&#35843;&#25439;&#22833;&#20989;&#25968;&#30340;&#26399;&#26395;&#20540;&#65292;&#31034;&#20363;&#35777;&#26126;&#20854;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20855;&#26377;&#25511;&#21046;&#35823;&#25253;&#29575;&#12289;&#22270;&#24418;&#36317;&#31163;&#21644;&#20196;&#29260;&#32423;F1&#24471;&#20998;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#31526;&#21512;&#24615;&#39044;&#27979;&#25512;&#24191;&#33267;&#25511;&#21046;&#20219;&#20309;&#21333;&#35843;&#25439;&#22833;&#20989;&#25968;&#30340;&#26399;&#26395;&#20540;&#12290;&#35813;&#31639;&#27861;&#23558;&#20998;&#35010;&#31526;&#21512;&#24615;&#39044;&#27979;&#21450;&#20854;&#35206;&#30422;&#20445;&#35777;&#36827;&#34892;&#20102;&#27867;&#21270;&#12290;&#31867;&#20284;&#20110;&#31526;&#21512;&#24615;&#39044;&#27979;&#65292;&#31526;&#21512;&#20445;&#24207;&#30340;&#39118;&#38505;&#25511;&#21046;&#26041;&#27861;&#22312;$\mathcal{O}(1/n)$&#22240;&#23376;&#20869;&#20445;&#25345;&#32039;&#23494;&#24615;&#12290;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#31034;&#20363;&#35777;&#26126;&#20102;&#25105;&#20204;&#31639;&#27861;&#22312;&#25511;&#21046;&#35823;&#25253;&#29575;&#12289;&#22270;&#24418;&#36317;&#31163;&#21644;&#20196;&#29260;&#32423;F1&#24471;&#20998;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We extend conformal prediction to control the expected value of any monotone loss function. The algorithm generalizes split conformal prediction together with its coverage guarantee. Like conformal prediction, the conformal risk control procedure is tight up to an $\mathcal{O}(1/n)$ factor. Worked examples from computer vision and natural language processing demonstrate the usage of our algorithm to bound the false negative rate, graph distance, and token-level F1-score.
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#22411;&#30340;&#20107;&#20214;&#32423;&#35270;&#35273;&#38382;&#31572;&#26694;&#26550;&#8212;&#8212;&#36328;&#27169;&#24577;&#22240;&#26524;&#20851;&#31995;&#25512;&#29702;&#65288;CMCIR&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#22240;&#26524;&#24178;&#39044;&#26041;&#27861;&#65292;&#21457;&#29616;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24577;&#30340;&#30495;&#27491;&#22240;&#26524;&#32467;&#26500;&#65292;&#23454;&#29616;&#24378;&#20581;&#30340;&#22240;&#26524;&#24863;&#30693;&#35270;&#35273;&#35821;&#35328;&#38382;&#31572;&#12290;</title><link>http://arxiv.org/abs/2207.12647</link><description>&lt;p&gt;
&#36328;&#27169;&#24577;&#22240;&#26524;&#20851;&#31995;&#25512;&#29702;&#22312;&#20107;&#20214;&#32423;&#35270;&#35273;&#38382;&#31572;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Cross-Modal Causal Relational Reasoning for Event-Level Visual Question Answering. (arXiv:2207.12647v5 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.12647
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#22411;&#30340;&#20107;&#20214;&#32423;&#35270;&#35273;&#38382;&#31572;&#26694;&#26550;&#8212;&#8212;&#36328;&#27169;&#24577;&#22240;&#26524;&#20851;&#31995;&#25512;&#29702;&#65288;CMCIR&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#22240;&#26524;&#24178;&#39044;&#26041;&#27861;&#65292;&#21457;&#29616;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24577;&#30340;&#30495;&#27491;&#22240;&#26524;&#32467;&#26500;&#65292;&#23454;&#29616;&#24378;&#20581;&#30340;&#22240;&#26524;&#24863;&#30693;&#35270;&#35273;&#35821;&#35328;&#38382;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#35270;&#35273;&#38382;&#31572;&#26041;&#27861;&#24448;&#24448;&#25429;&#25417;&#36328;&#27169;&#24577;&#30340;&#20266;&#30456;&#20851;&#24615;&#65292;&#32780;&#26410;&#33021;&#21457;&#29616;&#30495;&#27491;&#30340;&#22240;&#26524;&#26426;&#21046;&#65292;&#20197;&#30495;&#23454;&#22320;&#22522;&#20110;&#20027;&#23548;&#35270;&#35273;&#35777;&#25454;&#21644;&#38382;&#39064;&#24847;&#22270;&#36827;&#34892;&#25512;&#29702;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#24573;&#30053;&#20102;&#36328;&#27169;&#24577;&#20107;&#20214;&#32423;&#29702;&#35299;&#65292;&#38656;&#35201;&#32852;&#21512;&#24314;&#27169;&#20107;&#20214;&#30340;&#26102;&#38388;&#24615;&#12289;&#22240;&#26524;&#24615;&#21644;&#21160;&#24577;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#26032;&#30340;&#35282;&#24230;&#65292;&#21363;&#36328;&#27169;&#24577;&#22240;&#26524;&#20851;&#31995;&#25512;&#29702;&#65292;&#32858;&#28966;&#20110;&#20107;&#20214;&#32423;&#35270;&#35273;&#38382;&#31572;&#65292;&#24341;&#20837;&#22240;&#26524;&#24178;&#39044;&#26041;&#27861;&#26469;&#21457;&#29616;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24577;&#30340;&#30495;&#27491;&#22240;&#26524;&#32467;&#26500;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#36328;&#27169;&#24577;&#22240;&#26524;&#20851;&#31995;&#25512;&#29702;&#65288;CMCIR&#65289;&#30340;&#26032;&#22411;&#20107;&#20214;&#32423;&#35270;&#35273;&#38382;&#31572;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#24378;&#20581;&#30340;&#22240;&#26524;&#24863;&#30693;&#35270;&#35273;&#35821;&#35328;&#38382;&#31572;&#12290;&#20026;&#20102;&#21457;&#29616;&#36328;&#27169;&#24577;&#22240;&#26524;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22240;&#26524;&#24863;&#30693;&#35270;&#35273;&#35821;&#35328;&#25512;&#29702;&#65288;CVLR&#65289;&#27169;&#22359;&#65292;&#29992;&#20110;&#20849;&#21516;&#23545;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24577;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing visual question answering methods tend to capture the cross-modal spurious correlations and fail to discover the true causal mechanism that facilitates reasoning truthfully based on the dominant visual evidence and the question intention. Additionally, the existing methods usually ignore the cross-modal event-level understanding that requires to jointly model event temporality, causality, and dynamics. In this work, we focus on event-level visual question answering from a new perspective, i.e., cross-modal causal relational reasoning, by introducing causal intervention methods to discover the true causal structures for visual and linguistic modalities. Specifically, we propose a novel event-level visual question answering framework named Cross-Modal Causal RelatIonal Reasoning (CMCIR), to achieve robust causality-aware visual-linguistic question answering. To discover cross-modal causal structures, the Causality-aware Visual-Linguistic Reasoning (CVLR) module is proposed to co
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#22788;&#29702;&#20869;&#23384;&#31995;&#32479;&#19978;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#28508;&#33021;&#65292;&#24182;&#35777;&#26126;&#22522;&#20110;PIM&#30340;ML&#35757;&#32451;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#21152;&#36895;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2207.07886</link><description>&lt;p&gt;
&#22522;&#20110;&#22788;&#29702;&#20869;&#23384;&#31995;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#30340;&#23454;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
An Experimental Evaluation of Machine Learning Training on a Real Processing-in-Memory System. (arXiv:2207.07886v2 [cs.AR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.07886
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#22788;&#29702;&#20869;&#23384;&#31995;&#32479;&#19978;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#28508;&#33021;&#65292;&#24182;&#35777;&#26126;&#22522;&#20110;PIM&#30340;ML&#35757;&#32451;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#21152;&#36895;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26159;&#19968;&#31181;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#36807;&#31243;&#65292;&#30001;&#20110;&#19981;&#26029;&#35775;&#38382;&#22823;&#22411;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#36825;&#31181;&#36807;&#31243;&#36890;&#24120;&#20250;&#21463;&#21040;&#20869;&#23384;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#20197;&#22788;&#29702;&#22120;&#20026;&#20013;&#24515;&#30340;&#31995;&#32479;&#65288;&#20363;&#22914;CPU&#65292;GPU&#65289;&#22312;&#20869;&#23384;&#21333;&#20803;&#21644;&#22788;&#29702;&#21333;&#20803;&#20043;&#38388;&#30340;&#25968;&#25454;&#20256;&#36755;&#26041;&#38754;&#23384;&#22312;&#26114;&#36149;&#30340;&#29942;&#39048;&#65292;&#36825;&#20250;&#28040;&#32791;&#22823;&#37327;&#30340;&#33021;&#37327;&#21644;&#25191;&#34892;&#21608;&#26399;&#12290;&#20855;&#26377;&#22788;&#29702;&#20869;&#23384;&#65288;PIM&#65289;&#21151;&#33021;&#30340;&#20869;&#23384;&#20013;&#24515;&#35745;&#31639;&#31995;&#32479;&#21487;&#20197;&#32531;&#35299;&#36825;&#31181;&#25968;&#25454;&#31227;&#21160;&#29942;&#39048;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20102;&#35299;&#29616;&#20195;&#36890;&#29992;PIM&#26550;&#26500;&#21152;&#36895;ML&#35757;&#32451;&#30340;&#28508;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#65288;1&#65289;&#22312;&#23454;&#38469;&#36890;&#29992;PIM&#26550;&#26500;&#19978;&#23454;&#29616;&#20102;&#20960;&#31181;&#20195;&#34920;&#24615;&#30340;&#20256;&#32479;ML&#31639;&#27861;&#65288;&#21363;&#32447;&#24615;&#22238;&#24402;&#65292;&#36923;&#36753;&#22238;&#24402;&#65292;&#20915;&#31574;&#26641;&#65292;K-Means&#32858;&#31867;&#65289;&#65292;&#65288;2&#65289;&#20005;&#26684;&#35780;&#20272;&#21644;&#34920;&#24449;&#36825;&#20123;&#31639;&#27861;&#30340;&#20934;&#30830;&#24615;&#65292;&#24615;&#33021;&#21644;&#25193;&#23637;&#24615;&#65292;&#24182;&#19988;&#65288;3&#65289;&#19982;&#23427;&#20204;&#22312;CPU&#21644;GPU&#19978;&#30340;&#30456;&#24212;&#23454;&#29616;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#22312;&#23454;&#38469;&#20869;&#23384;&#20013;&#24515;&#35745;&#31639;&#24179;&#21488;&#19978;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#19982;&#30456;&#24212;&#30340;CPU&#21644;GPU&#26041;&#27861;&#30456;&#27604;&#65292;&#22522;&#20110;PIM&#30340;ML&#35757;&#32451;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#21152;&#36895;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training machine learning (ML) algorithms is a computationally intensive process, which is frequently memory-bound due to repeatedly accessing large training datasets. As a result, processor-centric systems (e.g., CPU, GPU) suffer from costly data movement between memory units and processing units, which consumes large amounts of energy and execution cycles. Memory-centric computing systems, i.e., with processing-in-memory (PIM) capabilities, can alleviate this data movement bottleneck.  Our goal is to understand the potential of modern general-purpose PIM architectures to accelerate ML training. To do so, we (1) implement several representative classic ML algorithms (namely, linear regression, logistic regression, decision tree, K-Means clustering) on a real-world general-purpose PIM architecture, (2) rigorously evaluate and characterize them in terms of accuracy, performance and scaling, and (3) compare to their counterpart implementations on CPU and GPU. Our evaluation on a real mem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#26102;&#38388;&#19979;&#30340;q-Learning&#65292;&#36890;&#36807;&#24341;&#20837;&#23567;q&#20989;&#25968;&#20316;&#20026;&#19968;&#38454;&#36817;&#20284;&#65292;&#30740;&#31350;&#20102;q-learning&#29702;&#35770;&#65292;&#24212;&#29992;&#20110;&#35774;&#35745;&#19981;&#21516;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2207.00713</link><description>&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#19979;&#30340;q-Learning
&lt;/p&gt;
&lt;p&gt;
q-Learning in Continuous Time. (arXiv:2207.00713v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.00713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#26102;&#38388;&#19979;&#30340;q-Learning&#65292;&#36890;&#36807;&#24341;&#20837;&#23567;q&#20989;&#25968;&#20316;&#20026;&#19968;&#38454;&#36817;&#20284;&#65292;&#30740;&#31350;&#20102;q-learning&#29702;&#35770;&#65292;&#24212;&#29992;&#20110;&#35774;&#35745;&#19981;&#21516;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#29109;&#27491;&#21017;&#21270;&#30340;&#25506;&#32034;&#24615;&#25193;&#25955;&#36807;&#31243;&#30340;Q-learning&#22312;&#36830;&#32493;&#26102;&#38388;&#19979;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#23567;q&#20989;&#25968;&#8221;&#20316;&#20026;&#22823;Q&#20989;&#25968;&#30340;&#19968;&#38454;&#36817;&#20284;&#65292;&#30740;&#31350;&#20102;q&#20989;&#25968;&#30340;q-learning&#29702;&#35770;&#65292;&#24182;&#24212;&#29992;&#20110;&#35774;&#35745;&#19981;&#21516;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the continuous-time counterpart of Q-learning for reinforcement learning (RL) under the entropy-regularized, exploratory diffusion process formulation introduced by Wang et al. (2020). As the conventional (big) Q-function collapses in continuous time, we consider its first-order approximation and coin the term ``(little) q-function". This function is related to the instantaneous advantage rate function as well as the Hamiltonian. We develop a ``q-learning" theory around the q-function that is independent of time discretization. Given a stochastic policy, we jointly characterize the associated q-function and value function by martingale conditions of certain stochastic processes, in both on-policy and off-policy settings. We then apply the theory to devise different actor-critic algorithms for solving underlying RL problems, depending on whether or not the density function of the Gibbs measure generated from the q-function can be computed explicitly. One of our algorithms inter
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;ACT-R/{\Phi}&#35748;&#30693;&#20307;&#31995;&#32467;&#26500;&#21644;&#29616;&#26377;&#30340;&#30693;&#35782;&#22270;&#31995;&#32479;ConceptNet&#65292;&#20174;&#35748;&#30693;&#12289;&#31038;&#20250;&#25991;&#21270;&#21644;&#29983;&#29702;&#23398;&#19977;&#20010;&#35282;&#24230;&#32771;&#34385;&#21453;&#40657;&#21644;&#31181;&#26063;&#20027;&#20041;&#23545;AI&#31995;&#32479;&#30340;&#24433;&#21709;&#65292;&#24182;&#35748;&#20026;&#22312;&#35748;&#30693;&#24314;&#27169;&#20013;&#20856;&#22411;&#30340;&#22238;&#36991;&#31038;&#20250;&#25991;&#21270;&#36807;&#31243;&#21644;&#30693;&#35782;&#32467;&#26500;&#65292;&#38544;&#21547;&#22320;&#25512;&#21160;&#20102;&#35748;&#30693;&#24314;&#27169;&#30340;&#33394;&#30450;&#26041;&#27861;&#65292;&#24182;&#38544;&#34255;&#20102;&#24635;&#26159;&#23384;&#22312;&#20110;AI&#31995;&#32479;&#20013;&#30340;&#31038;&#20250;&#25991;&#21270;&#32972;&#26223;&#12290;</title><link>http://arxiv.org/abs/2207.00644</link><description>&lt;p&gt;
&#24212;&#29992;&#35748;&#30693;&#20307;&#31995;&#32467;&#26500;&#32771;&#34385;&#21453;&#40657;&#21644;&#31181;&#26063;&#20027;&#20041;&#23545;AI&#31995;&#32479;&#35774;&#35745;&#21644;&#24320;&#21457;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Using a Cognitive Architecture to consider antiblackness in design and development of AI systems. (arXiv:2207.00644v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.00644
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;ACT-R/{\Phi}&#35748;&#30693;&#20307;&#31995;&#32467;&#26500;&#21644;&#29616;&#26377;&#30340;&#30693;&#35782;&#22270;&#31995;&#32479;ConceptNet&#65292;&#20174;&#35748;&#30693;&#12289;&#31038;&#20250;&#25991;&#21270;&#21644;&#29983;&#29702;&#23398;&#19977;&#20010;&#35282;&#24230;&#32771;&#34385;&#21453;&#40657;&#21644;&#31181;&#26063;&#20027;&#20041;&#23545;AI&#31995;&#32479;&#30340;&#24433;&#21709;&#65292;&#24182;&#35748;&#20026;&#22312;&#35748;&#30693;&#24314;&#27169;&#20013;&#20856;&#22411;&#30340;&#22238;&#36991;&#31038;&#20250;&#25991;&#21270;&#36807;&#31243;&#21644;&#30693;&#35782;&#32467;&#26500;&#65292;&#38544;&#21547;&#22320;&#25512;&#21160;&#20102;&#35748;&#30693;&#24314;&#27169;&#30340;&#33394;&#30450;&#26041;&#27861;&#65292;&#24182;&#38544;&#34255;&#20102;&#24635;&#26159;&#23384;&#22312;&#20110;AI&#31995;&#32479;&#20013;&#30340;&#31038;&#20250;&#25991;&#21270;&#32972;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22914;&#20309;&#21033;&#29992;&#35748;&#30693;&#24314;&#27169;&#26469;&#32771;&#34385;&#21453;&#40657;&#21644;&#31181;&#26063;&#20027;&#20041;&#26356;&#24191;&#27867;&#22320;&#24433;&#21709;AI&#31995;&#32479;&#30340;&#35774;&#35745;&#21644;&#24320;&#21457;&#65311;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#35752;&#35770;&#21644;&#19968;&#20010;&#31034;&#20363;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;ACT-R/{\Phi}&#35748;&#30693;&#20307;&#31995;&#32467;&#26500;&#21644;&#29616;&#26377;&#30340;&#30693;&#35782;&#22270;&#31995;&#32479;ConceptNet&#65292;&#19981;&#20165;&#20174;&#35748;&#30693;&#21644;&#31038;&#20250;&#25991;&#21270;&#30340;&#35282;&#24230;&#32771;&#34385;&#36825;&#20010;&#38382;&#39064;&#65292;&#20063;&#20174;&#29983;&#29702;&#23398;&#30340;&#35282;&#24230;&#32771;&#34385;&#12290;&#38500;&#20102;&#21033;&#29992;&#35748;&#30693;&#24314;&#27169;&#26469;&#25506;&#32034;&#21453;&#40657;&#22914;&#20309;&#22312;AI&#31995;&#32479;&#30340;&#35774;&#35745;&#21644;&#24320;&#21457;&#20013;&#34920;&#29616;&#20986;&#26469;&#65288;&#23588;&#20854;&#26159;&#20174;&#36719;&#20214;&#24037;&#31243;&#30340;&#35282;&#24230;&#65289;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#21453;&#40657;&#12289;&#20154;&#31867;&#21644;&#35745;&#31639;&#35748;&#30693;&#24314;&#27169;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#35748;&#30693;&#20307;&#31995;&#32467;&#26500;&#21644;&#35748;&#30693;&#24314;&#27169;&#20013;&#20856;&#22411;&#30340;&#22238;&#36991;&#31038;&#20250;&#25991;&#21270;&#36807;&#31243;&#21644;&#30693;&#35782;&#32467;&#26500;&#65292;&#38544;&#21547;&#22320;&#25512;&#21160;&#20102;&#35748;&#30693;&#24314;&#27169;&#30340;&#33394;&#30450;&#26041;&#27861;&#65292;&#24182;&#38544;&#34255;&#20102;&#24635;&#26159;&#23384;&#22312;&#20110;AI&#31995;&#32479;&#20013;&#30340;&#31038;&#20250;&#25991;&#21270;&#32972;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
How might we use cognitive modeling to consider the ways in which antiblackness, and racism more broadly, impact the design and development of AI systems? We provide a discussion and an example towards an answer to this question. We use the ACT-R/{\Phi} cognitive architecture and an existing knowledge graph system, ConceptNet, to consider this question not only from a cognitive and sociocultural perspective, but also from a physiological perspective. In addition to using a cognitive modeling as a means to explore how antiblackness may manifest in the design and development of AI systems (particularly from a software engineering perspective), we also introduce connections between antiblackness, the Human, and computational cognitive modeling. We argue that the typical eschewing of sociocultural processes and knowledge structures in cognitive architectures and cognitive modeling implicitly furthers a colorblind approach to cognitive modeling and hides sociocultural context that is always
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25968;&#25454;&#20462;&#21098;&#31639;&#27861;&#31361;&#30772;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#38598;&#22823;&#23567;&#19982;&#27169;&#22411;&#35823;&#24046;&#24130;&#24459;&#30340;&#23610;&#24230;&#30028;&#38480;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#26377;&#25928;&#24615;&#65292;&#21516;&#26102;&#36827;&#34892;&#20102;&#39318;&#27425;&#22823;&#35268;&#27169;&#25968;&#25454;&#20462;&#21098;&#31639;&#27861;&#22522;&#20934;&#27979;&#35797;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2206.14486</link><description>&lt;p&gt;
&#36229;&#36234;&#31070;&#32463;&#23610;&#24230;&#23450;&#24459;&#65306;&#36890;&#36807;&#25968;&#25454;&#20462;&#21098;&#25171;&#36133;&#24130;&#24459;&#23610;&#24230;
&lt;/p&gt;
&lt;p&gt;
Beyond neural scaling laws: beating power law scaling via data pruning. (arXiv:2206.14486v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.14486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25968;&#25454;&#20462;&#21098;&#31639;&#27861;&#31361;&#30772;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#38598;&#22823;&#23567;&#19982;&#27169;&#22411;&#35823;&#24046;&#24130;&#24459;&#30340;&#23610;&#24230;&#30028;&#38480;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#26377;&#25928;&#24615;&#65292;&#21516;&#26102;&#36827;&#34892;&#20102;&#39318;&#27425;&#22823;&#35268;&#27169;&#25968;&#25454;&#20462;&#21098;&#31639;&#27861;&#22522;&#20934;&#27979;&#35797;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26222;&#36941;&#23384;&#22312;&#30340;&#31070;&#32463;&#23610;&#24230;&#23450;&#24459;&#20197;&#35757;&#32451;&#38598;&#22823;&#23567;&#12289;&#27169;&#22411;&#35268;&#27169;&#25110;&#20004;&#32773;&#30340;&#24130;&#20026;&#27169;&#22411;&#35823;&#24046;&#19979;&#38477;&#30340;&#39537;&#21160;&#21147;&#65292;&#20026;&#28145;&#24230;&#23398;&#20064;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#20294;&#26159;&#65292;&#20165;&#36890;&#36807;&#23610;&#24230;&#26469;&#23454;&#29616;&#36825;&#20123;&#25913;&#36827;&#38656;&#35201;&#24040;&#22823;&#30340;&#35745;&#31639;&#21644;&#33021;&#28304;&#25104;&#26412;&#12290;&#26412;&#25991;&#30528;&#37325;&#30740;&#31350;&#25968;&#25454;&#38598;&#22823;&#23567;&#19982;&#35823;&#24046;&#27604;&#20363;&#30340;&#23610;&#24230;&#65292;&#24182;&#23637;&#31034;&#29702;&#35770;&#19978;&#25105;&#20204;&#22914;&#20309;&#31361;&#30772;&#24130;&#24459;&#23610;&#24230;&#65292;&#24182;&#22312;pruning&#31639;&#27861;&#26465;&#20214;&#19979;&#28508;&#22312;&#22320;&#29978;&#33267;&#33021;&#23558;&#20854;&#38477;&#33267;&#25351;&#25968;&#23610;&#24230;&#12290;&#25105;&#20204;&#25509;&#30528;&#22312;CIFAR-10&#12289;SVHN&#21644;ImageNet&#30340;ResNet&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#65292;&#24182;&#35266;&#23519;&#21040;&#23454;&#36341;&#20013;&#20248;&#20110;&#24130;&#24459;&#23610;&#24230;&#30340;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;&#37492;&#20110;&#23547;&#25214;&#20248;&#36136;pruning&#31639;&#27861;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#23545;ImageNet&#19978;&#30340;&#21313;&#31181;&#19981;&#21516;&#30340;&#25968;&#25454;&#20462;&#21098;&#31639;&#27861;&#36827;&#34892;&#20102;&#39318;&#27425;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Widely observed neural scaling laws, in which error falls off as a power of the training set size, model size, or both, have driven substantial performance improvements in deep learning. However, these improvements through scaling alone require considerable costs in compute and energy. Here we focus on the scaling of error with dataset size and show how in theory we can break beyond power law scaling and potentially even reduce it to exponential scaling instead if we have access to a high-quality data pruning metric that ranks the order in which training examples should be discarded to achieve any pruned dataset size. We then test this improved scaling prediction with pruned dataset size empirically, and indeed observe better than power law scaling in practice on ResNets trained on CIFAR-10, SVHN, and ImageNet. Next, given the importance of finding high-quality pruning metrics, we perform the first large-scale benchmarking study of ten different data pruning metrics on ImageNet. We fin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ACMP&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#20855;&#26377;&#21560;&#24341;&#21147;&#21644;&#25490;&#26021;&#21147;&#30340;&#30456;&#20114;&#20316;&#29992;&#31890;&#23376;&#31995;&#32479;&#36827;&#34892;&#28040;&#24687;&#20256;&#36882;&#20256;&#25773;&#65292;&#20811;&#26381;&#20102;GNN&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#23558;&#32593;&#32476;&#28145;&#24230;&#25512;&#21040;100&#23618;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#33410;&#28857;&#20998;&#31867;&#21644;&#22270;&#21305;&#37197;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.05437</link><description>&lt;p&gt;
ACMP: Allen-Cahn&#20449;&#24687;&#20256;&#36882;&#29992;&#20110;&#24102;&#26377;&#29289;&#36136;&#30456;&#21464;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
ACMP: Allen-Cahn Message Passing for Graph Neural Networks with Particle Phase Transition. (arXiv:2206.05437v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.05437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ACMP&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#20855;&#26377;&#21560;&#24341;&#21147;&#21644;&#25490;&#26021;&#21147;&#30340;&#30456;&#20114;&#20316;&#29992;&#31890;&#23376;&#31995;&#32479;&#36827;&#34892;&#28040;&#24687;&#20256;&#36882;&#20256;&#25773;&#65292;&#20811;&#26381;&#20102;GNN&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#23558;&#32593;&#32476;&#28145;&#24230;&#25512;&#21040;100&#23618;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#33410;&#28857;&#20998;&#31867;&#21644;&#22270;&#21305;&#37197;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#28040;&#24687;&#20256;&#36882;&#26159;&#22522;&#20110;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#29305;&#24449;&#25552;&#21462;&#21333;&#20803;&#65292;&#32771;&#34385;&#20174;&#19968;&#23618;&#21040;&#19979;&#19968;&#23618;&#30340;&#32593;&#32476;&#20256;&#25773;&#20013;&#30340;&#30456;&#37051;&#33410;&#28857;&#29305;&#24449;&#12290;&#25105;&#20204;&#36890;&#36807;&#20855;&#26377;&#21560;&#24341;&#21147;&#21644;&#25490;&#26021;&#21147;&#30340;&#30456;&#20114;&#20316;&#29992;&#31890;&#23376;&#31995;&#32479;&#26469;&#24314;&#27169;&#36825;&#31181;&#36807;&#31243;&#65292;&#24182;&#22312;&#30456;&#21464;&#24314;&#27169;&#20013;&#24341;&#20837;Allen-Cahn&#21147;&#12290;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#26159;&#19968;&#31181;&#21453;&#24212;&#25193;&#25955;&#36807;&#31243;&#65292;&#21487;&#20197;&#23558;&#31890;&#23376;&#20998;&#31163;&#32780;&#19981;&#20250;&#25193;&#25955;&#12290;&#36825;&#24341;&#20986;&#20102;&#19968;&#31181;Allen-Cahn&#20449;&#24687;&#20256;&#36882;(ACMP)&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#20013;&#31890;&#23376;&#31995;&#32479;&#35299;&#30340;&#25968;&#20540;&#36845;&#20195;&#26500;&#25104;&#20102;&#28040;&#24687;&#20256;&#36882;&#20256;&#25773;&#12290;ACMP&#20855;&#26377;&#31616;&#21333;&#30340;&#23454;&#29616;&#21644;&#31070;&#32463;ODE&#27714;&#35299;&#22120;&#65292;&#21487;&#20197;&#23558;&#32593;&#32476;&#28145;&#24230;&#25512;&#21040;100&#23618;&#65292;&#24182;&#20855;&#26377;&#29702;&#35770;&#19978;&#35777;&#26126;&#30340;Dirichlet&#33021;&#37327;&#20005;&#26684;&#27491;&#19979;&#30028;&#12290;&#22240;&#27492;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#28145;&#24230;&#27169;&#22411;&#30340;GNN&#65292;&#36991;&#20813;&#20102;&#24120;&#35265;&#30340;GNN&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;&#20351;&#29992;ACMP&#30340;GNN&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#23454;&#38469;&#33410;&#28857;&#20998;&#31867;&#21644;&#22270;&#21305;&#37197;&#20219;&#21153;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural message passing is a basic feature extraction unit for graph-structured data considering neighboring node features in network propagation from one layer to the next. We model such process by an interacting particle system with attractive and repulsive forces and the Allen-Cahn force arising in the modeling of phase transition. The dynamics of the system is a reaction-diffusion process which can separate particles without blowing up. This induces an Allen-Cahn message passing (ACMP) for graph neural networks where the numerical iteration for the particle system solution constitutes the message passing propagation. ACMP which has a simple implementation with a neural ODE solver can propel the network depth up to one hundred of layers with theoretically proven strictly positive lower bound of the Dirichlet energy. It thus provides a deep model of GNNs circumventing the common GNN problem of oversmoothing. GNNs with ACMP achieve state of the art performance for real-world node class
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#20998;&#31867;&#22120;&#65292;&#21033;&#29992;&#8220;Merlin-Arthur&#8221;&#21327;&#35758;&#30340;&#21551;&#21457;&#65292;&#22312;&#19981;&#20551;&#35774;&#26368;&#20248;&#26234;&#33021;&#20307;&#25110;&#29305;&#24449;&#29420;&#31435;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#30456;&#23545;&#24378;&#24230;&#21644;&#8220;&#38750;&#23545;&#31216;&#29305;&#24449;&#30456;&#20851;&#24615;&#8221;&#27010;&#24565;&#25429;&#25417;&#29305;&#24449;&#20043;&#38388;&#31934;&#30830;&#30340;&#30456;&#20851;&#24615;&#65292;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#21487;&#35299;&#37322;&#24615;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2206.00759</link><description>&lt;p&gt;
Merlin-Arthur&#20998;&#31867;&#22120;&#30340;&#24418;&#24335;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Formal Interpretability with Merlin-Arthur Classifiers. (arXiv:2206.00759v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00759
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#20998;&#31867;&#22120;&#65292;&#21033;&#29992;&#8220;Merlin-Arthur&#8221;&#21327;&#35758;&#30340;&#21551;&#21457;&#65292;&#22312;&#19981;&#20551;&#35774;&#26368;&#20248;&#26234;&#33021;&#20307;&#25110;&#29305;&#24449;&#29420;&#31435;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#30456;&#23545;&#24378;&#24230;&#21644;&#8220;&#38750;&#23545;&#31216;&#29305;&#24449;&#30456;&#20851;&#24615;&#8221;&#27010;&#24565;&#25429;&#25417;&#29305;&#24449;&#20043;&#38388;&#31934;&#30830;&#30340;&#30456;&#20851;&#24615;&#65292;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#21487;&#35299;&#37322;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31867;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#20998;&#31867;&#22120;&#65292;&#21363;&#20351;&#26159;&#20687;&#31070;&#32463;&#32593;&#32476;&#36825;&#26679;&#30340;&#22797;&#26434;&#26234;&#33021;&#20307;&#20063;&#33021;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#21487;&#35299;&#37322;&#24615;&#20445;&#35777;&#12290;&#36825;&#20123;&#20445;&#35777;&#21253;&#25324;&#23545;&#27492;&#20998;&#31867;&#22120;&#36873;&#25321;&#30340;&#29305;&#24449;&#20043;&#38388;&#20114;&#20449;&#24687;&#30340;&#19978;&#19979;&#30028;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#21463;&#20132;&#20114;&#24335;&#35777;&#26126;&#31995;&#32479;&#20013; Merlin-Arthur &#21327;&#35758;&#30340;&#21551;&#21457;&#65292;&#24182;&#20197;&#21487;&#27979;&#37327;&#30340;&#25351;&#26631;&#65288;&#22914;&#22768;&#38899;&#21644;&#23436;&#25972;&#24615;&#65289;&#34920;&#36798;&#20102;&#36825;&#20123;&#32422;&#26463;&#12290;&#19982;&#29616;&#26377;&#30340;&#20132;&#20114;&#24335;&#35774;&#32622;&#30456;&#27604;&#65292;&#25105;&#20204;&#19981;&#20381;&#36182;&#20110;&#26368;&#20248;&#26234;&#33021;&#20307;&#25110;&#29305;&#24449;&#29420;&#31435;&#20998;&#24067;&#30340;&#20551;&#35774;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#21033;&#29992;&#26234;&#33021;&#20307;&#30340;&#30456;&#23545;&#24378;&#24230;&#20197;&#21450;&#26032;&#30340;&#8220;&#38750;&#23545;&#31216;&#29305;&#24449;&#30456;&#20851;&#24615;&#8221;&#27010;&#24565;&#26469;&#25429;&#25417;&#20351;&#21487;&#35299;&#37322;&#24615;&#20445;&#35777;&#22256;&#38590;&#30340;&#31934;&#30830;&#30456;&#20851;&#24615;&#31867;&#22411;&#12290; &#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#25968;&#20540;&#23454;&#39564;&#26469;&#27979;&#35797;&#25105;&#20204;&#30340;&#32467;&#26524;&#65292;&#36825;&#20123;&#23454;&#39564;&#21487;&#39564;&#35777;&#39640;&#20114;&#20449;&#24687;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new type of multi-agent interactive classifier that provides provable interpretability guarantees even for complex agents such as neural networks. These guarantees consist of bounds on the mutual information of the features selected by this classifier. Our results are inspired by the Merlin-Arthur protocol from Interactive Proof Systems and express these bounds in terms of measurable metrics such as soundness and completeness. Compared to existing interactive setups we do not rely on optimal agents or on the assumption that features are distributed independently. Instead, we use the relative strength of the agents as well as the new concept of Asymmetric Feature Correlation which captures the precise kind of correlations that make interpretability guarantees difficult. %relates the information carried by sets of features to one of the individual features. We test our results through numerical experiments on two small-scale datasets where high mutual information can be veri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;DRL&#30340;&#23398;&#20064;&#34920;&#31034;&#24212;&#35813;&#28385;&#36275;&#19968;&#20010;&#26377;&#21033;&#30340;&#21487;&#21306;&#20998;&#34920;&#31034;&#23646;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#21017;&#21270;&#22120;PEER&#65292;&#26088;&#22312;&#36890;&#36807;&#23545;&#20869;&#37096;&#34920;&#31034;&#36827;&#34892;&#26174;&#24335;&#27491;&#21017;&#21270;&#26469;&#32500;&#25345;&#21487;&#21306;&#20998;&#34920;&#31034;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.14557</link><description>&lt;p&gt;
&#20196;&#20154;&#27822;&#20007;&#30340;&#31616;&#21333;&#27491;&#21017;&#21270;&#21487;&#20197;&#25552;&#21319;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Frustratingly Easy Regularization on Representation Can Boost Deep Reinforcement Learning. (arXiv:2205.14557v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;DRL&#30340;&#23398;&#20064;&#34920;&#31034;&#24212;&#35813;&#28385;&#36275;&#19968;&#20010;&#26377;&#21033;&#30340;&#21487;&#21306;&#20998;&#34920;&#31034;&#23646;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#21017;&#21270;&#22120;PEER&#65292;&#26088;&#22312;&#36890;&#36807;&#23545;&#20869;&#37096;&#34920;&#31034;&#36827;&#34892;&#26174;&#24335;&#27491;&#21017;&#21270;&#26469;&#32500;&#25345;&#21487;&#21306;&#20998;&#34920;&#31034;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#25215;&#35834;&#20195;&#29702;&#33021;&#22815;&#20174;&#39640;&#32500;&#20449;&#24687;&#20013;&#23398;&#20064;&#21040;&#33391;&#22909;&#30340;&#31574;&#30053;&#65292;&#32780;&#34920;&#31034;&#23398;&#20064;&#21017;&#33021;&#22815;&#28040;&#38500;&#19981;&#30456;&#20851;&#21644;&#20887;&#20313;&#30340;&#20449;&#24687;&#24182;&#20445;&#30041;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;Q&#32593;&#32476;&#21450;&#20854;&#30446;&#26631;Q&#32593;&#32476;&#30340;&#23398;&#20064;&#34920;&#31034;&#22312;&#29702;&#35770;&#19978;&#24212;&#35813;&#28385;&#36275;&#19968;&#20010;&#26377;&#21033;&#30340;&#21487;&#21306;&#20998;&#34920;&#31034;&#23646;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#20856;&#22411;&#30340;DRL&#35774;&#32622;&#20013;&#20004;&#20010;&#30456;&#37051;&#26102;&#38388;&#27493;&#38271;&#30340;&#20215;&#20540;&#20989;&#25968;&#30340;&#34920;&#31034;&#30456;&#20284;&#24230;&#23384;&#22312;&#19968;&#20010;&#19978;&#30028;&#12290;&#20294;&#26159;&#65292;&#36890;&#36807;&#35828;&#26126;&#24615;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#23398;&#20064;&#21040;&#30340;DRL&#20195;&#29702;&#21487;&#33021;&#36829;&#21453;&#36825;&#20010;&#23646;&#24615;&#65292;&#24182;&#23548;&#33268;&#27425;&#20248;&#31574;&#30053;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#34920;&#31034;&#31616;&#21333;&#27491;&#21017;&#21270;&#30340;&#31574;&#30053;&#35780;&#20272;"(PEER)&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#27491;&#21017;&#21270;&#22120;&#65292;&#26088;&#22312;&#36890;&#36807;&#23545;&#20869;&#37096;&#34920;&#31034;&#36827;&#34892;&#26174;&#24335;&#27491;&#21017;&#21270;&#26469;&#32500;&#25345;&#21487;&#21306;&#20998;&#34920;&#31034;&#23646;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25910;&#25947;&#36895;&#24230;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (DRL) gives the promise that an agent learns good policy from high-dimensional information, whereas representation learning removes irrelevant and redundant information and retains pertinent information. In this work, we demonstrate that the learned representation of the $Q$-network and its target $Q$-network should, in theory, satisfy a favorable distinguishable representation property. Specifically, there exists an upper bound on the representation similarity of the value functions of two adjacent time steps in a typical DRL setting. However, through illustrative experiments, we show that the learned DRL agent may violate this property and lead to a sub-optimal policy. Therefore, we propose a simple yet effective regularizer called Policy Evaluation with Easy Regularization on Representation (PEER), which aims to maintain the distinguishable representation property via explicit regularization on internal representations. And we provide the convergence rate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35745;&#31639;&#23398;&#29983;&#25552;&#20132;&#20316;&#19994;&#19982;&#39044;&#23450;&#20041;&#35299;&#20915;&#26041;&#26696;&#20043;&#38388;&#30340;&#32534;&#36753;&#36317;&#31163;&#65292;&#21487;&#24212;&#29992;&#20110;&#35777;&#26126;&#12289;&#32534;&#31243;&#19982;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2204.04196</link><description>&lt;p&gt;
&#35777;&#26126;&#22359;&#38382;&#39064;&#20013;&#30340;&#39640;&#25928;&#21453;&#39304;&#21644;&#37096;&#20998;&#20998;&#35780;&#20998;
&lt;/p&gt;
&lt;p&gt;
Efficient Feedback and Partial Credit Grading for Proof Blocks Problems. (arXiv:2204.04196v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.04196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35745;&#31639;&#23398;&#29983;&#25552;&#20132;&#20316;&#19994;&#19982;&#39044;&#23450;&#20041;&#35299;&#20915;&#26041;&#26696;&#20043;&#38388;&#30340;&#32534;&#36753;&#36317;&#31163;&#65292;&#21487;&#24212;&#29992;&#20110;&#35777;&#26126;&#12289;&#32534;&#31243;&#19982;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35777;&#26126;&#22359;&#26159;&#19968;&#31181;&#36719;&#20214;&#24037;&#20855;&#65292;&#20801;&#35768;&#23398;&#29983;&#36890;&#36807;&#25302;&#25918;&#32780;&#38750;&#20174;&#22836;&#20889;&#35777;&#26126;&#65292;&#32451;&#20064;&#25968;&#23398;&#35777;&#26126;&#30340;&#20889;&#20316;&#12290;&#35777;&#26126;&#22359;&#25552;&#20379;&#20102;&#20998;&#37096;&#20998;&#20998;&#21644;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#21453;&#39304;&#30340;&#33021;&#21147;&#12290;&#36825;&#26159;&#36890;&#36807;&#35745;&#31639;&#23398;&#29983;&#25552;&#20132;&#30340;&#20316;&#19994;&#19982;&#19968;&#20123;&#39044;&#23450;&#20041;&#35299;&#20915;&#26041;&#26696;&#20043;&#38388;&#30340;&#32534;&#36753;&#36317;&#31163;&#26469;&#23436;&#25104;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32534;&#36753;&#36317;&#31163;&#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#26174;&#33879;&#20248;&#20110;&#25490;&#21015;&#25972;&#20010;&#25628;&#32034;&#31354;&#38388;&#30340;&#22522;&#32447;&#31243;&#24207;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20381;&#36182;&#20110;&#23558;&#38382;&#39064;&#30340;&#32553;&#20943;&#20026;&#26368;&#23567;&#39030;&#28857;&#35206;&#30422;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#35838;&#31243;&#30340;&#25968;&#21315;&#20221;&#23398;&#29983;&#25552;&#20132;&#20013;&#23545;&#25105;&#20204;&#30340;&#31639;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#34920;&#26126;&#22522;&#32447;&#31639;&#27861;&#38590;&#20197;&#35745;&#31639;&#65292;&#24182;&#19988;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#23545;&#20110;&#23454;&#29616;&#35838;&#22530;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#26032;&#30340;&#31639;&#27861;&#20063;&#24050;&#32463;&#34987;&#29992;&#20110;&#35768;&#22810;&#20854;&#20182;&#39046;&#22495;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#35299;&#20915;&#26041;&#26696;&#31354;&#38388;&#21487;&#20197;&#24314;&#27169;&#20026;DAG&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#32534;&#31243;&#20316;&#19994;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Proof Blocks is a software tool that allows students to practice writing mathematical proofs by dragging and dropping lines instead of writing proofs from scratch. Proof Blocks offers the capability of assigning partial credit and providing solution quality feedback to students. This is done by computing the edit distance from a student's submission to some predefined set of solutions. In this work, we propose an algorithm for the edit distance problem that significantly outperforms the baseline procedure of exhaustively enumerating over the entire search space. Our algorithm relies on a reduction to the minimum vertex cover problem. We benchmark our algorithm on thousands of student submissions from multiple courses, showing that the baseline algorithm is intractable, and that our proposed algorithm is critical to enable classroom deployment. Our new algorithm has also been used for problems in many other domains where the solution space can be modeled as a DAG, including but not limi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#29992;&#20110;&#31572;&#26696;&#36873;&#25321;&#65292;&#36890;&#36807;&#26500;&#24314;&#30456;&#20851;&#35757;&#32451;&#22270;&#24182;&#38598;&#25104;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#26816;&#32034;&#22411;&#38382;&#31572;&#31995;&#32479;&#20013;&#30340;AS2&#20219;&#21153;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2203.03549</link><description>&lt;p&gt;
&#38382;&#39064;-&#22238;&#31572;&#21477;&#23376;&#22270;&#29992;&#20110;&#32852;&#21512;&#24314;&#27169;&#31572;&#26696;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Question-Answer Sentence Graph for Joint Modeling Answer Selection. (arXiv:2203.03549v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.03549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#29992;&#20110;&#31572;&#26696;&#36873;&#25321;&#65292;&#36890;&#36807;&#26500;&#24314;&#30456;&#20851;&#35757;&#32451;&#22270;&#24182;&#38598;&#25104;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#26816;&#32034;&#22411;&#38382;&#31572;&#31995;&#32479;&#20013;&#30340;AS2&#20219;&#21153;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#29992;&#20110;&#31572;&#26696;&#36873;&#25321;&#65288;AS2&#65289;&#65292;&#36825;&#26159;&#26816;&#32034;&#22411;&#38382;&#31572;&#65288;QA&#65289;&#31995;&#32479;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#22312;&#31163;&#32447;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#20026;&#27599;&#20010;&#38382;&#39064;&#26500;&#24314;&#19968;&#20010;&#23567;&#35268;&#27169;&#30340;&#30456;&#20851;&#35757;&#32451;&#22270;&#65292;&#24182;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#12290;&#22270;&#33410;&#28857;&#26159;&#38382;&#39064;&#21477;&#23376;&#21040;&#31572;&#26696;&#21477;&#23376;&#30340;&#23545;&#12290;&#25105;&#20204;&#35757;&#32451;&#24182;&#38598;&#25104;&#20102;&#29992;&#20110;&#35745;&#31639;&#38382;&#39064;-&#38382;&#39064;&#12289;&#38382;&#39064;-&#31572;&#26696;&#21644;&#31572;&#26696;-&#31572;&#26696;&#23545;&#20043;&#38388;&#24471;&#20998;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#30456;&#20851;&#24471;&#20998;&#38408;&#20540;&#26469;&#21019;&#24314;&#22270;&#36793;&#32536;&#12290;&#28982;&#21518;&#36827;&#34892;&#22312;&#32447;&#25512;&#29702;&#20197;&#35299;&#20915;&#30475;&#19981;&#35265;&#30340;&#26597;&#35810;&#30340;AS2&#20219;&#21153;&#12290;&#22312;&#20004;&#20010;&#30693;&#21517;&#30340;&#23398;&#26415;&#22522;&#20934;&#21644;&#19968;&#20010;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;SOTA QA&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research studies graph-based approaches for Answer Sentence Selection (AS2), an essential component for retrieval-based Question Answering (QA) systems. During offline learning, our model constructs a small-scale relevant training graph per question in an unsupervised manner, and integrates with Graph Neural Networks. Graph nodes are question sentence to answer sentence pairs. We train and integrate state-of-the-art (SOTA) models for computing scores between question-question, question-answer, and answer-answer pairs, and use thresholding on relevance scores for creating graph edges. Online inference is then performed to solve the AS2 task on unseen queries. Experiments on two well-known academic benchmarks and a real-world dataset show that our approach consistently outperforms SOTA QA baseline models.
&lt;/p&gt;</description></item><item><title>&#35813;&#25945;&#31243;&#20171;&#32461;&#20102;&#20998;&#25674;&#20248;&#21270;&#30340;&#22522;&#30784;&#65292;&#24182;&#24635;&#32467;&#20102;&#20854;&#22312;&#21464;&#20998;&#25512;&#26029;&#12289;&#31232;&#30095;&#32534;&#30721;&#12289;&#20803;&#23398;&#20064;&#12289;&#25511;&#21046;&#12289;&#24378;&#21270;&#23398;&#20064;&#12289;&#20984;&#20248;&#21270;&#12289;&#26368;&#20248;&#20256;&#36755;&#21644;&#28145;&#24230;&#24179;&#34913;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2202.00665</link><description>&lt;p&gt;
&#20851;&#20110;&#20998;&#25674;&#20248;&#21270;&#30340;&#25945;&#31243;
&lt;/p&gt;
&lt;p&gt;
Tutorial on amortized optimization. (arXiv:2202.00665v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.00665
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25945;&#31243;&#20171;&#32461;&#20102;&#20998;&#25674;&#20248;&#21270;&#30340;&#22522;&#30784;&#65292;&#24182;&#24635;&#32467;&#20102;&#20854;&#22312;&#21464;&#20998;&#25512;&#26029;&#12289;&#31232;&#30095;&#32534;&#30721;&#12289;&#20803;&#23398;&#20064;&#12289;&#25511;&#21046;&#12289;&#24378;&#21270;&#23398;&#20064;&#12289;&#20984;&#20248;&#21270;&#12289;&#26368;&#20248;&#20256;&#36755;&#21644;&#28145;&#24230;&#24179;&#34913;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#26159;&#19968;&#31181;&#26222;&#36941;&#30340;&#24314;&#27169;&#24037;&#20855;&#65292;&#32463;&#24120;&#22312;&#21453;&#22797;&#35299;&#20915;&#30456;&#21516;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#12290;&#20998;&#25674;&#20248;&#21270;&#26041;&#27861;&#20351;&#29992;&#23398;&#20064;&#26469;&#39044;&#27979;&#36825;&#20123;&#35774;&#32622;&#20013;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#30456;&#20284;&#38382;&#39064;&#23454;&#20363;&#20043;&#38388;&#30340;&#20849;&#20139;&#32467;&#26500;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#21464;&#20998;&#25512;&#26029;&#21644;&#24378;&#21270;&#23398;&#20064;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#33021;&#22815;&#27604;&#19981;&#20351;&#29992;&#20998;&#25674;&#30340;&#20256;&#32479;&#20248;&#21270;&#26041;&#27861;&#24555;&#20960;&#20010;&#25968;&#37327;&#32423;&#22320;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#12290;&#26412;&#27425;&#25945;&#31243;&#20171;&#32461;&#20102;&#36825;&#20123;&#36827;&#27493;&#32972;&#21518;&#30340;&#20998;&#25674;&#20248;&#21270;&#22522;&#30784;&#65292;&#24182;&#27010;&#36848;&#20102;&#23427;&#20204;&#22312;&#21464;&#20998;&#25512;&#26029;&#12289;&#31232;&#30095;&#32534;&#30721;&#12289;&#22522;&#20110;&#26799;&#24230;&#30340;&#20803;&#23398;&#20064;&#12289;&#25511;&#21046;&#12289;&#24378;&#21270;&#23398;&#20064;&#12289;&#20984;&#20248;&#21270;&#12289;&#26368;&#20248;&#20256;&#36755;&#21644;&#28145;&#24230;&#24179;&#34913;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#25945;&#31243;&#30340;&#28304;&#20195;&#30721;&#21487;&#22312;https://github.com/facebookresearch/amortized-optimization-tutorial&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimization is a ubiquitous modeling tool and is often deployed in settings which repeatedly solve similar instances of the same problem. Amortized optimization methods use learning to predict the solutions to problems in these settings, exploiting the shared structure between similar problem instances. These methods have been crucial in variational inference and reinforcement learning and are capable of solving optimization problems many orders of magnitudes times faster than traditional optimization methods that do not use amortization. This tutorial presents an introduction to the amortized optimization foundations behind these advancements and overviews their applications in variational inference, sparse coding, gradient-based meta-learning, control, reinforcement learning, convex optimization, optimal transport, and deep equilibrium networks. The source code for this tutorial is available at https://github.com/facebookresearch/amortized-optimization-tutorial.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#26041;&#27861;FedMed-GAN: &#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#21644;&#21307;&#30103;GAN&#20043;&#38388;&#30340;&#26080;&#30417;&#30563;&#33041;&#22270;&#20687;&#21512;&#25104;&#21644;&#32763;&#35793;,&#20855;&#26377;&#27169;&#24335;&#23849;&#28291;&#29616;&#35937;&#23567;&#12289;&#25968;&#25454;&#24615;&#33021;&#39640;&#31561;&#20248;&#28857;&#65292;&#24191;&#27867;&#36866;&#29992;&#20110;&#19981;&#37197;&#23545;&#21644;&#37197;&#23545;&#25968;&#25454;&#38598;&#30340;&#32852;&#37030;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2201.08953</link><description>&lt;p&gt;
FedMed-GAN: &#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#26080;&#30417;&#30563;&#36328;&#27169;&#24577;&#33041;&#22270;&#20687;&#21512;&#25104;&#19982;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
FedMed-GAN: Federated Domain Translation on Unsupervised Cross-Modality Brain Image Synthesis. (arXiv:2201.08953v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.08953
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#26041;&#27861;FedMed-GAN: &#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#21644;&#21307;&#30103;GAN&#20043;&#38388;&#30340;&#26080;&#30417;&#30563;&#33041;&#22270;&#20687;&#21512;&#25104;&#21644;&#32763;&#35793;,&#20855;&#26377;&#27169;&#24335;&#23849;&#28291;&#29616;&#35937;&#23567;&#12289;&#25968;&#25454;&#24615;&#33021;&#39640;&#31561;&#20248;&#28857;&#65292;&#24191;&#27867;&#36866;&#29992;&#20110;&#19981;&#37197;&#23545;&#21644;&#37197;&#23545;&#25968;&#25454;&#38598;&#30340;&#32852;&#37030;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22810;&#27169;&#24577;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#34987;&#35777;&#26126;&#23545;&#20110;&#30740;&#31350;&#20154;&#31867;&#35748;&#30693;&#27963;&#21160;&#21644;&#26576;&#20123;&#30142;&#30149;&#24456;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39640;&#26114;&#30340;&#26816;&#26597;&#25104;&#26412;&#12289;&#38271;&#26102;&#38388;&#30340;&#33719;&#21462;&#26102;&#38388;&#21644;&#22270;&#20687;&#25439;&#22351;&#31561;&#22810;&#31181;&#38480;&#21046;&#65292;&#38598;&#20013;&#33719;&#24471;&#23436;&#25972;&#37197;&#23545;&#30340;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#25968;&#25454;&#20998;&#25955;&#22312;&#19981;&#21516;&#30340;&#21307;&#30103;&#26426;&#26500;&#20013;&#65292;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#26080;&#27861;&#36827;&#34892;&#20013;&#22830;&#38598;&#20013;&#22521;&#35757;&#12290;&#22240;&#27492;&#65292;&#36843;&#20999;&#38656;&#35201;&#24320;&#23637;&#32852;&#37030;&#23398;&#20064;&#65292;&#20419;&#36827;&#26469;&#33258;&#19981;&#21516;&#26426;&#26500;&#30340;&#20998;&#25955;&#25968;&#25454;&#30340;&#38598;&#25104;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#26041;&#27861;-FedMed-GAN&#65292;&#29992;&#20110;&#32852;&#37030;&#39046;&#22495;&#32763;&#35793;&#21644;&#26080;&#30417;&#30563;&#33041;&#22270;&#20687;&#21512;&#25104;&#19978;&#65292;&#24357;&#34917;&#20102;&#32852;&#37030;&#23398;&#20064;&#21644;&#21307;&#30103;GAN&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;FedMed-GAN&#36890;&#36807;&#20943;&#36731;&#27169;&#24335;&#23849;&#28291;&#29616;&#35937;&#32780;&#19981;&#25439;&#22833;&#29983;&#25104;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#24191;&#27867;&#24212;&#29992;&#20110;&#32852;&#37030;&#35774;&#32622;&#19979;&#19981;&#37197;&#23545;&#12289;&#37197;&#23545;&#25968;&#25454;&#38598;&#30340;&#19981;&#21516;&#27604;&#20363;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#20020;&#24202;&#25968;&#25454;&#19978;&#37117;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#26080;&#30417;&#30563;&#36328;&#27169;&#24577;&#33041;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Utilizing multi-modal neuroimaging data has been proved to be effective to investigate human cognitive activities and certain pathologies. However, it is not practical to obtain the full set of paired neuroimaging data centrally since the collection faces several constraints, e.g., high examination cost, long acquisition time, and image corruption. In addition, these data are dispersed into different medical institutions and thus cannot be aggregated for centralized training considering the privacy issues. There is a clear need to launch a federated learning and facilitate the integration of the dispersed data from different institutions. In this paper, we propose a new benchmark for federated domain translation on unsupervised brain image synthesis (termed as FedMed-GAN) to bridge the gap between federated learning and medical GAN. FedMed-GAN mitigates the mode collapse without sacrificing the performance of generators, and is widely applied to different proportions of unpaired and pa
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#24341;&#20154;&#27880;&#30446;&#30340;&#25216;&#26415;&#65292;&#35745;&#31639;&#26426;&#31243;&#24207;&#36890;&#36807;&#23581;&#35797;&#12289;&#24471;&#21040;&#21453;&#39304;&#21644;&#20877;&#27425;&#23581;&#35797;&#26469;&#33258;&#25105;&#35299;&#20915;&#22256;&#38590;&#38382;&#39064;&#65292;&#29978;&#33267;&#22312;&#26576;&#20123;&#39046;&#22495;&#27604;&#26368;&#22909;&#30340;&#20154;&#31867;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2201.02135</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#19968;&#26412;&#25945;&#26448;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning, a textbook. (arXiv:2201.02135v5 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.02135
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#24341;&#20154;&#27880;&#30446;&#30340;&#25216;&#26415;&#65292;&#35745;&#31639;&#26426;&#31243;&#24207;&#36890;&#36807;&#23581;&#35797;&#12289;&#24471;&#21040;&#21453;&#39304;&#21644;&#20877;&#27425;&#23581;&#35797;&#26469;&#33258;&#25105;&#35299;&#20915;&#22256;&#38590;&#38382;&#39064;&#65292;&#29978;&#33267;&#22312;&#26576;&#20123;&#39046;&#22495;&#27604;&#26368;&#22909;&#30340;&#20154;&#31867;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24341;&#36215;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#22312;&#33258;&#21160;&#39550;&#39542;&#12289;&#28216;&#25103;&#29609;&#27861;&#12289;&#20998;&#23376;&#37325;&#32452;&#21644;&#26426;&#22120;&#20154;&#31561;&#21508;&#20010;&#39046;&#22495;&#37117;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#25104;&#26524;&#12290;&#22312;&#25152;&#26377;&#36825;&#20123;&#39046;&#22495;&#20013;&#65292;&#35745;&#31639;&#26426;&#31243;&#24207;&#24050;&#32463;&#23398;&#20250;&#20102;&#33258;&#25105;&#35299;&#20915;&#22256;&#38590;&#38382;&#39064;&#12290;&#23427;&#20204;&#24050;&#32463;&#23398;&#20250;&#20102;&#39134;&#34892;&#27169;&#22411;&#30452;&#21319;&#26426;&#21644;&#36827;&#34892;&#20687;&#29615;&#21644;&#32763;&#28378;&#36825;&#26679;&#30340;&#29305;&#25216;&#21160;&#20316;&#12290;&#22312;&#26576;&#20123;&#24212;&#29992;&#20013;&#65292;&#23427;&#20204;&#29978;&#33267;&#27604;&#26368;&#22909;&#30340;&#20154;&#31867;&#34920;&#29616;&#24471;&#26356;&#22909;&#65292;&#20363;&#22914; Atari&#12289;&#22260;&#26827;&#12289;&#25169;&#20811;&#21644;&#26143;&#38469;&#20105;&#38712;&#12290;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25506;&#32034;&#22797;&#26434;&#29615;&#22659;&#30340;&#26041;&#24335;&#35753;&#25105;&#20204;&#24819;&#36215;&#20102;&#23401;&#23376;&#20204;&#30340;&#23398;&#20064;&#26041;&#24335;&#65292;&#36890;&#36807;&#23581;&#35797;&#12289;&#24471;&#21040;&#21453;&#39304;&#21644;&#20877;&#27425;&#23581;&#35797;&#26469;&#20805;&#28385;&#20048;&#36259;&#22320;&#23398;&#20064;&#12290;&#35745;&#31639;&#26426;&#20284;&#20046;&#30495;&#27491;&#20855;&#22791;&#20102;&#20154;&#31867;&#23398;&#20064;&#30340;&#26041;&#38754;&#65292;&#36825;&#35302;&#21160;&#20102;&#20154;&#24037;&#26234;&#33021;&#26790;&#24819;&#30340;&#26680;&#24515;&#12290;&#30740;&#31350;&#25104;&#21151;&#24341;&#36215;&#20102;&#25945;&#32946;&#24037;&#20316;&#32773;&#30340;&#20851;&#27880;&#65292;&#23398;&#26657;&#24320;&#22987;&#24320;&#35774;&#30456;&#20851;&#35838;&#31243;&#12290;&#26412;&#20070;&#30340;&#30446;&#30340;&#23601;&#26159;&#25552;&#20379;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning has gathered much attention recently. Impressive results were achieved in activities as diverse as autonomous driving, game playing, molecular recombination, and robotics. In all these fields, computer programs have taught themselves to solve difficult problems. They have learned to fly model helicopters and perform aerobatic manoeuvers such as loops and rolls. In some applications they have even become better than the best humans, such as in Atari, Go, poker and StarCraft. The way in which deep reinforcement learning explores complex environments reminds us of how children learn, by playfully trying out things, getting feedback, and trying again. The computer seems to truly possess aspects of human learning; this goes to the heart of the dream of artificial intelligence. The successes in research have not gone unnoticed by educators, and universities have started to offer courses on the subject. The aim of this book is to provide a comprehensive overview of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#28436;&#31034;&#25351;&#23548;&#30340;&#35268;&#33539;&#25628;&#32034;&#65288;DISS&#65289;&#31639;&#27861;&#65292;&#21487;&#20197;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#23398;&#20064;&#26102;&#38388;&#20219;&#21153;&#35268;&#33539;&#12290;</title><link>http://arxiv.org/abs/2112.10807</link><description>&lt;p&gt;
&#28436;&#31034;&#25351;&#23548;&#30340;&#35268;&#33539;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Demonstration Informed Specification Search. (arXiv:2112.10807v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.10807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#28436;&#31034;&#25351;&#23548;&#30340;&#35268;&#33539;&#25628;&#32034;&#65288;DISS&#65289;&#31639;&#27861;&#65292;&#21487;&#20197;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#23398;&#20064;&#26102;&#38388;&#20219;&#21153;&#35268;&#33539;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#23398;&#20064;&#26102;&#38388;&#20219;&#21153;&#35268;&#33539;&#30340;&#38382;&#39064;&#65292;&#20363;&#22914;&#33258;&#21160;&#26426;&#21644;&#26102;&#38388;&#36923;&#36753;&#12290;&#20219;&#21153;&#35268;&#33539;&#26159;&#19968;&#31867;&#24102;&#26377;&#26126;&#30830;&#30340;&#26102;&#38388;&#21644;&#24067;&#23572;&#32452;&#21512;&#25903;&#25345;&#30340;&#31232;&#30095;&#35760;&#24518;&#22686;&#24378;&#22870;&#21169;&#12290;&#23398;&#20064;&#26102;&#38388;&#20219;&#21153;&#35268;&#33539;&#20855;&#26377;&#20197;&#19979;&#19977;&#20010;&#22256;&#38590;&#65306;&#65288;1&#65289;&#65288;&#21487;&#25968;&#30340;&#65289;&#26080;&#38480;&#25968;&#37327;&#30340;&#20219;&#21153;&#65307;&#65288;2&#65289;&#19981;&#30693;&#36947;&#32534;&#30721;&#20219;&#21153;&#25152;&#38656;&#30340;&#20869;&#23384;&#65307;&#65288;3&#65289;&#31163;&#25955;&#35299;&#31354;&#38388;&#36890;&#24120;&#38656;&#35201;&#36890;&#36807;&#65288;&#26292;&#21147;&#65289;&#26522;&#20030;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#22256;&#38590;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28436;&#31034;&#25351;&#23548;&#30340;&#35268;&#33539;&#25628;&#32034;&#65288;DISS&#65289;&#65306;&#19968;&#26063;&#21482;&#38656;&#35201;&#40657;&#30418;&#35775;&#38382;&#26368;&#22823;&#29109;&#35268;&#21010;&#22120;&#21644;&#26469;&#33258;&#26631;&#35760;&#31034;&#20363;&#30340;&#20219;&#21153;&#37319;&#26679;&#22120;&#30340;&#31639;&#27861;&#12290;&#28982;&#21518;&#65292;DISS&#36890;&#36807;&#20132;&#26367;&#29468;&#27979;&#26631;&#35760;&#31034;&#20363;&#26469;&#20351;&#25552;&#20379;&#30340;&#28436;&#31034;&#21464;&#24471;&#19981;&#37027;&#20040;&#20196;&#20154;&#24778;&#35766;&#65292;&#24182;&#37319;&#26679;&#19982;&#29468;&#27979;&#30340;&#26631;&#35760;&#31034;&#20363;&#19968;&#33268;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#20307;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers the problem of learning temporal task specifications, e.g. automata and temporal logic, from expert demonstrations. Task specifications are a class of sparse memory augmented rewards with explicit support for temporal and Boolean composition. Three features make learning temporal task specifications difficult: (1) the (countably) infinite number of tasks under consideration; (2) an a-priori ignorance of what memory is needed to encode the task; and (3) the discrete solution space - typically addressed by (brute force) enumeration. To overcome these hurdles, we propose Demonstration Informed Specification Search (DISS): a family of algorithms requiring only black box access to a maximum entropy planner and a task sampler from labeled examples. DISS then works by alternating between conjecturing labeled examples to make the provided demonstrations less surprising and sampling tasks consistent with the conjectured labeled examples. We provide a concrete implementation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#32531;&#35299;&#28145;&#24230;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#20013;&#36807;&#20110;&#33258;&#20449;&#39044;&#27979;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#23618;&#26469;&#36991;&#20813;&#20256;&#32479;&#30340;&#39044;&#27979;&#23618;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#20943;&#23569;&#20551;&#38451;&#24615;&#20013;&#30340;&#36807;&#24230;&#33258;&#20449;&#12290;</title><link>http://arxiv.org/abs/2112.01360</link><description>&lt;p&gt;
&#36335;&#29992;&#25143;&#26816;&#27979;&#30340;&#27010;&#29575;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Approach for Road-Users Detection. (arXiv:2112.01360v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.01360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#32531;&#35299;&#28145;&#24230;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#20013;&#36807;&#20110;&#33258;&#20449;&#39044;&#27979;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#23618;&#26469;&#36991;&#20813;&#20256;&#32479;&#30340;&#39044;&#27979;&#23618;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#20943;&#23569;&#20551;&#38451;&#24615;&#20013;&#30340;&#36807;&#24230;&#33258;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#24212;&#29992;&#20013;&#30340;&#30446;&#26631;&#26816;&#27979;&#24847;&#21619;&#30528;&#23545;&#35821;&#20041;&#23545;&#35937;&#30340;&#26816;&#27979;&#21644;&#36319;&#36394;&#36890;&#24120;&#26159;&#22478;&#24066;&#39550;&#39542;&#29615;&#22659;&#30340;&#29305;&#33394;&#65292;&#22914;&#34892;&#20154;&#21644;&#36710;&#36742;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30446;&#26631;&#26816;&#27979;&#20013;&#23384;&#22312;&#20551;&#38451;&#24615;&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#36890;&#24120;&#24102;&#26377;&#36807;&#20110;&#33258;&#20449;&#30340;&#24471;&#20998;&#12290;&#22312;&#33258;&#21160;&#39550;&#39542;&#21644;&#20854;&#20182;&#20851;&#38190;&#30340;&#26426;&#22120;&#20154;&#24863;&#30693;&#39046;&#22495;&#65292;&#36825;&#26159;&#38750;&#24120;&#19981;&#24076;&#26395;&#30475;&#21040;&#30340;&#65292;&#22240;&#20026;&#28041;&#21450;&#23433;&#20840;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#32531;&#35299;&#36807;&#20110;&#33258;&#20449;&#30340;&#39044;&#27979;&#38382;&#39064;&#65292;&#36890;&#36807;&#22312;&#27979;&#35797;&#20013;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#23618;&#65292;&#21521;&#28145;&#24230;&#30446;&#26631;&#26816;&#27979;&#32593;&#32476;&#20013;&#28155;&#21152;&#36825;&#31181;&#27010;&#29575;&#23618;&#12290;&#24314;&#35758;&#30340;&#26041;&#27861;&#36991;&#20813;&#20102;&#20256;&#32479;&#30340;Sigmoid&#25110;Softmax&#39044;&#27979;&#23618;&#65292;&#36825;&#20123;&#23618;&#36890;&#24120;&#20250;&#20135;&#29983;&#36807;&#20110;&#33258;&#20449;&#30340;&#39044;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#25216;&#26415;&#33021;&#22815;&#20943;&#23569;&#20551;&#38451;&#24615;&#20013;&#30340;&#36807;&#24230;&#33258;&#20449;&#65292;&#32780;&#19981;&#20250;&#38477;&#20302;&#30495;&#38451;&#24615;&#30340;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#22312;2D-KITTI&#30446;&#26631;&#26816;&#27979;&#20013;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#20351;&#29992;&#20102;YOLOV4&#21644;S&#12290;
&lt;/p&gt;
&lt;p&gt;
Object detection in autonomous driving applications implies that the detection and tracking of semantic objects are commonly native to urban driving environments, as pedestrians and vehicles. One of the major challenges in state-of-the-art deep-learning based object detection are false positives which occur with overconfident scores. This is highly undesirable in autonomous driving and other critical robotic-perception domains because of safety concerns. This paper proposes an approach to alleviate the problem of overconfident predictions by introducing a novel probabilistic layer to deep object detection networks in testing. The suggested approach avoids the traditional Sigmoid or Softmax prediction layer which often produces overconfident predictions. It is demonstrated that the proposed technique reduces overconfidence in the false positives without degrading the performance on the true positives. The approach is validated on the 2D-KITTI objection detection through the YOLOV4 and S
&lt;/p&gt;</description></item><item><title>PatchCensor&#26159;&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;Transformer&#30340;&#34917;&#19969;&#40065;&#26834;&#24615;&#35748;&#35777;&#26041;&#27861;&#65292;&#22522;&#20110;&#20840;&#38754;&#27979;&#35797;&#24182;&#32771;&#34385;&#26368;&#22351;&#30340;&#34917;&#19969;&#25915;&#20987;&#24773;&#22659;&#65292;&#33021;&#22815;&#25552;&#20379;&#21463;&#20445;&#35777;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2111.10481</link><description>&lt;p&gt;
PatchCensor&#65306;&#36890;&#36807;&#31351;&#23613;&#27979;&#35797;&#25552;&#39640;&#35270;&#35273;Transformers&#30340;&#34917;&#19969;&#40065;&#26834;&#24615;&#35748;&#35777;
&lt;/p&gt;
&lt;p&gt;
PatchCensor: Patch Robustness Certification for Transformers via Exhaustive Testing. (arXiv:2111.10481v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.10481
&lt;/p&gt;
&lt;p&gt;
PatchCensor&#26159;&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;Transformer&#30340;&#34917;&#19969;&#40065;&#26834;&#24615;&#35748;&#35777;&#26041;&#27861;&#65292;&#22522;&#20110;&#20840;&#38754;&#27979;&#35797;&#24182;&#32771;&#34385;&#26368;&#22351;&#30340;&#34917;&#19969;&#25915;&#20987;&#24773;&#22659;&#65292;&#33021;&#22815;&#25552;&#20379;&#21463;&#20445;&#35777;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20854;&#20182;&#32463;&#20856;&#31070;&#32463;&#32593;&#32476;&#19968;&#26679;&#65292;&#35270;&#35273;Transformer&#65288;ViT&#65289;&#20063;&#34987;&#35748;&#20026;&#26159;&#39640;&#24230;&#38750;&#32447;&#24615;&#30340;&#65292;&#24182;&#19988;&#23481;&#26131;&#21463;&#21040;&#33258;&#28982;&#21644;&#23545;&#25239;&#24615;&#34917;&#19969;&#25200;&#21160;&#30340;&#24178;&#25200;&#12290;&#22312;&#30495;&#23454;&#30340;&#24037;&#19994;&#29615;&#22659;&#20013;&#65292;&#36825;&#31181;&#38480;&#21046;&#21487;&#33021;&#23545;ViT&#30340;&#37096;&#32626;&#20135;&#29983;&#23041;&#32961;&#65292;&#23588;&#20854;&#26159;&#22312;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;PatchCensor&#65292;&#26088;&#22312;&#36890;&#36807;&#20840;&#38754;&#27979;&#35797;&#26469;&#35777;&#26126;ViT&#30340;&#34917;&#19969;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#35797;&#22270;&#36890;&#36807;&#32771;&#34385;&#26368;&#22351;&#30340;&#34917;&#19969;&#25915;&#20987;&#24773;&#22659;&#26469;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#12290;&#19982;&#37027;&#20123;&#21487;&#33021;&#34987;&#33258;&#36866;&#24212;&#25915;&#20987;&#30772;&#22351;&#30340;&#23545;&#25239;&#24615;&#34917;&#19969;&#30340;&#32463;&#39564;&#24615;&#38450;&#24481;&#25514;&#26045;&#19981;&#21516;&#65292;&#35748;&#35777;&#21487;&#38752;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#25552;&#20379;&#23545;&#20110;&#20219;&#24847;&#25915;&#20987;&#30340;&#21463;&#20445;&#35777;&#30340;&#20934;&#30830;&#24615;&#12290;&#20294;&#26159;&#65292;&#29616;&#26377;&#30340;&#40065;&#26834;&#24615;&#35748;&#35777;&#20027;&#35201;&#22522;&#20110;&#40065;&#26834;&#24615;&#35757;&#32451;&#65292;&#36825;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#24037;&#20316;&#65292;&#24182;&#19988;&#20250;&#22312;&#27491;&#24120;&#26679;&#26412;&#19978;&#29306;&#29298;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;PatchCensor&#26088;&#22312;&#36890;&#36807;&#26816;&#27979;&#21644;&#21076;&#38500;&#19981;&#21512;&#35268;&#21017;&#30340;&#21306;&#22495;&#26469;&#25552;&#39640;&#25972;&#20010;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Transformer (ViT) is known to be highly nonlinear like other classical neural networks and could be easily fooled by both natural and adversarial patch perturbations. This limitation could pose a threat to the deployment of ViT in the real industrial environment, especially in safety-critical scenarios. In this work, we propose PatchCensor, aiming to certify the patch robustness of ViT by applying exhaustive testing. We try to provide a provable guarantee by considering the worst patch attack scenarios. Unlike empirical defenses against adversarial patches that may be adaptively breached, certified robust approaches can provide a certified accuracy against arbitrary attacks under certain conditions. However, existing robustness certifications are mostly based on robust training, which often requires substantial training efforts and the sacrifice of model performance on normal samples. To bridge the gap, PatchCensor seeks to improve the robustness of the whole system by detecting
&lt;/p&gt;</description></item><item><title>PAIR&#31639;&#27861;&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22312;&#23494;&#38598;&#22411;&#27573;&#33853;&#26816;&#32034;&#20013;&#21516;&#26102;&#32771;&#34385;&#26597;&#35810;&#20013;&#24515;&#21644;&#27573;&#33853;&#20013;&#24515;&#30456;&#20284;&#20851;&#31995;&#65292;&#36890;&#36807;&#27491;&#24335;&#20844;&#24335;&#12289;&#30693;&#35782;&#33976;&#39311;&#21644;&#20004;&#38454;&#27573;&#35757;&#32451;&#23454;&#29616;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#20808;&#21069;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2108.06027</link><description>&lt;p&gt;
PAIR&#65306;&#21033;&#29992;&#27573;&#33853;&#20013;&#24515;&#30340;&#30456;&#20284;&#20851;&#31995;&#25913;&#36827;&#23494;&#38598;&#22411;&#27573;&#33853;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
PAIR: Leveraging Passage-Centric Similarity Relation for Improving Dense Passage Retrieval. (arXiv:2108.06027v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.06027
&lt;/p&gt;
&lt;p&gt;
PAIR&#31639;&#27861;&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22312;&#23494;&#38598;&#22411;&#27573;&#33853;&#26816;&#32034;&#20013;&#21516;&#26102;&#32771;&#34385;&#26597;&#35810;&#20013;&#24515;&#21644;&#27573;&#33853;&#20013;&#24515;&#30456;&#20284;&#20851;&#31995;&#65292;&#36890;&#36807;&#27491;&#24335;&#20844;&#24335;&#12289;&#30693;&#35782;&#33976;&#39311;&#21644;&#20004;&#38454;&#27573;&#35757;&#32451;&#23454;&#29616;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#20808;&#21069;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23494;&#38598;&#22411;&#27573;&#33853;&#26816;&#32034;&#24050;&#25104;&#20026;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#25214;&#21040;&#30456;&#20851;&#20449;&#24687;&#30340;&#19968;&#31181;&#20027;&#27969;&#26041;&#27861;&#12290;&#35768;&#22810;&#30740;&#31350;&#33268;&#21147;&#20110;&#25913;&#36827;&#24191;&#27867;&#37319;&#29992;&#30340;&#21452;&#32534;&#30721;&#22120;&#26550;&#26500;&#12290;&#20294;&#26159;&#65292;&#22823;&#22810;&#25968;&#20197;&#21069;&#30340;&#30740;&#31350;&#22312;&#23398;&#20064;&#21452;&#32534;&#30721;&#22120;&#26816;&#32034;&#22120;&#26102;&#20165;&#32771;&#34385;&#20102;&#26597;&#35810;&#20013;&#24515;&#30340;&#30456;&#20284;&#20851;&#31995;&#12290;&#20026;&#20102;&#25429;&#25417;&#26356;&#20840;&#38754;&#30340;&#30456;&#20284;&#20851;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#26597;&#35810;&#20013;&#24515;&#21644;&#27573;&#33853;&#20013;&#24515;&#30340;&#30456;&#20284;&#20851;&#31995;&#65288;&#31216;&#20026;PAIR&#65289;&#36827;&#34892;&#23494;&#38598;&#22411;&#27573;&#33853;&#26816;&#32034;&#12290;&#20026;&#20102;&#23454;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#30456;&#20284;&#20851;&#31995;&#30340;&#27491;&#24335;&#20844;&#24335;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20266;&#26631;&#35760;&#25968;&#25454;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20004;&#38454;&#27573;&#35757;&#32451;&#36807;&#31243;&#65292;&#20854;&#20013;&#21253;&#25324;&#27573;&#33853;&#20013;&#24515;&#30456;&#20284;&#20851;&#31995;&#32422;&#26463;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, dense passage retrieval has become a mainstream approach to finding relevant information in various natural language processing tasks. A number of studies have been devoted to improving the widely adopted dual-encoder architecture. However, most of the previous studies only consider query-centric similarity relation when learning the dual-encoder retriever. In order to capture more comprehensive similarity relations, we propose a novel approach that leverages both query-centric and PAssage-centric sImilarity Relations (called PAIR) for dense passage retrieval. To implement our approach, we make three major technical contributions by introducing formal formulations of the two kinds of similarity relations, generating high-quality pseudo labeled data via knowledge distillation, and designing an effective two-stage training procedure that incorporates passage-centric similarity relation constraint. Extensive experiments show that our approach significantly outperforms previous s
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#20462;&#27491;&#26041;&#27861;&#65292;&#29992;&#20110;&#32531;&#35299;&#20302;&#36136;&#37327;&#21644;&#32570;&#22833;&#25968;&#25454;&#30340;&#24433;&#21709;&#65292;&#20855;&#22791;&#31070;&#32463;&#35843;&#21046;&#29305;&#24449;&#65292;&#36890;&#36807;&#19968;&#20010;&#39069;&#22806;&#30340;&#36755;&#20837;&#30340;&#20989;&#25968;&#26367;&#25442;&#20102;&#20840;&#36830;&#25509;&#23618;&#30340;&#22266;&#23450;&#26435;&#37325;&#65292;&#20351;&#24471;&#22312;&#27979;&#35797;&#20013;&#20855;&#26377;&#35843;&#21046;&#23618;&#30340;&#27169;&#22411;&#23545;&#20110;&#25968;&#25454;&#36136;&#37327;&#30340;&#38477;&#35299;&#26356;&#21152;&#40065;&#26834;&#65292;&#21516;&#26102;&#20063;&#33021;&#22815;&#33410;&#30465;&#35757;&#32451;&#26102;&#38388;&#24182;&#19988;&#19981;&#20250;&#21463;&#21040;&#25554;&#34917;&#38169;&#35823;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2107.08574</link><description>&lt;p&gt;
&#19968;&#31181;&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#23545;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#30340;&#40065;&#26834;&#24615;&#30340;&#35843;&#21046;&#23618;
&lt;/p&gt;
&lt;p&gt;
A Modulation Layer to Increase Neural Network Robustness Against Data Quality Issues. (arXiv:2107.08574v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.08574
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#20462;&#27491;&#26041;&#27861;&#65292;&#29992;&#20110;&#32531;&#35299;&#20302;&#36136;&#37327;&#21644;&#32570;&#22833;&#25968;&#25454;&#30340;&#24433;&#21709;&#65292;&#20855;&#22791;&#31070;&#32463;&#35843;&#21046;&#29305;&#24449;&#65292;&#36890;&#36807;&#19968;&#20010;&#39069;&#22806;&#30340;&#36755;&#20837;&#30340;&#20989;&#25968;&#26367;&#25442;&#20102;&#20840;&#36830;&#25509;&#23618;&#30340;&#22266;&#23450;&#26435;&#37325;&#65292;&#20351;&#24471;&#22312;&#27979;&#35797;&#20013;&#20855;&#26377;&#35843;&#21046;&#23618;&#30340;&#27169;&#22411;&#23545;&#20110;&#25968;&#25454;&#36136;&#37327;&#30340;&#38477;&#35299;&#26356;&#21152;&#40065;&#26834;&#65292;&#21516;&#26102;&#20063;&#33021;&#22815;&#33410;&#30465;&#35757;&#32451;&#26102;&#38388;&#24182;&#19988;&#19981;&#20250;&#21463;&#21040;&#25554;&#34917;&#38169;&#35823;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#32570;&#22833;&#21644;&#36136;&#37327;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#24120;&#35265;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#39118;&#38505;&#24212;&#29992;&#39046;&#22495;&#65292;&#22914;&#21307;&#30103;&#20445;&#20581;&#12290;&#24320;&#21457;&#32773;&#36890;&#24120;&#21482;&#20351;&#29992;&#39640;&#36136;&#37327;&#25968;&#25454;&#31934;&#24515;&#31579;&#36873;&#20986;&#30340;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65307;&#28982;&#32780;&#65292;&#36825;&#20250;&#38477;&#20302;&#36825;&#20123;&#27169;&#22411;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#30340;&#25928;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#20462;&#27491;&#26041;&#27861;&#65292;&#29992;&#20110;&#32531;&#35299;&#20302;&#36136;&#37327;&#21644;&#32570;&#22833;&#25968;&#25454;&#30340;&#24433;&#21709;&#65292;&#20854;&#20013;&#21033;&#29992;&#19968;&#20010;&#39069;&#22806;&#30340;&#36755;&#20837;&#30340;&#20989;&#25968;&#26367;&#25442;&#20102;&#20840;&#36830;&#25509;&#23618;&#30340;&#22266;&#23450;&#26435;&#37325;&#12290;&#36825;&#21463;&#21551;&#21457;&#20110;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#31070;&#32463;&#35843;&#21046;&#65292;&#30382;&#36136;&#21487;&#20197;&#26681;&#25454;&#36755;&#20837;&#30340;&#21487;&#38752;&#24615;&#21644;&#20854;&#20182;&#25968;&#25454;&#30340;&#23384;&#22312;&#31243;&#24230;&#19978;&#19979;&#35843;&#33410;&#36755;&#20837;&#12290;&#22312;&#27979;&#35797;&#20013;&#65292;&#20351;&#29992;&#21487;&#38752;&#24615;&#24471;&#20998;&#20316;&#20026;&#35843;&#21046;&#20449;&#21495;&#65292;&#21457;&#29616;&#20855;&#26377;&#35843;&#21046;&#23618;&#30340;&#27169;&#22411;&#23545;&#20110;&#25968;&#25454;&#36136;&#37327;&#30340;&#38477;&#35299;&#65288;&#21253;&#25324;&#39069;&#22806;&#30340;&#32570;&#22833;&#25968;&#25454;&#65289;&#26356;&#21152;&#40065;&#26834;&#12290;&#36825;&#20123;&#27169;&#22411;&#20248;&#20110;&#25554;&#34917;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#36807;&#23436;&#20840;&#36339;&#36807;&#25554;&#34917;&#36807;&#31243;&#33410;&#30465;&#20102;&#35757;&#32451;&#26102;&#38388;&#65292;&#24182;&#19988;&#19981;&#20250;&#21463;&#21040;&#25554;&#34917;&#38169;&#35823;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data missingness and quality are common problems in machine learning, especially for high-stakes applications such as healthcare. Developers often train machine learning models on carefully curated datasets using only high quality data; however, this reduces the utility of such models in production environments. We propose a novel neural network modification to mitigate the impacts of low quality and missing data which involves replacing the fixed weights of a fully-connected layer with a function of an additional input. This is inspired from neuromodulation in biological neural networks where the cortex can up- and down-regulate inputs based on their reliability and the presence of other data. In testing, with reliability scores as a modulating signal, models with modulating layers were found to be more robust against degradation of data quality, including additional missingness. These models are superior to imputation as they save on training time by completely skipping the imputatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38405;&#35835;&#29702;&#35299;&#20013;&#20154;&#31867;&#20851;&#27880;&#21147;&#20998;&#37197;&#30340;&#35745;&#31639;&#27169;&#22411;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#25191;&#34892;&#30456;&#21516;&#30340;&#38405;&#35835;&#20219;&#21153;&#26102;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#39044;&#27979;&#27599;&#20010;&#21333;&#35789;&#30340;&#38405;&#35835;&#26102;&#38388;&#65292;&#35835;&#32773;&#22312;&#31532;&#19968;&#36941;&#38405;&#35835;&#21644;&#37325;&#26032;&#38405;&#35835;&#36807;&#31243;&#20013;&#20998;&#21035;&#20851;&#27880;&#22522;&#26412;&#25991;&#26412;&#29305;&#24449;&#21644;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#24182;&#19988;&#25991;&#26412;&#29305;&#24449;&#21644;&#38382;&#39064;&#30456;&#20851;&#24615;&#20250;&#20998;&#21035;&#35843;&#33410;&#27880;&#24847;&#21147;&#26435;&#37325;&#12290;</title><link>http://arxiv.org/abs/2107.05799</link><description>&lt;p&gt;
&#20154;&#30340;&#27880;&#24847;&#21147;&#22312;&#30446;&#26631;&#23450;&#21521;&#38405;&#35835;&#29702;&#35299;&#26102;&#20381;&#36182;&#20110;&#20219;&#21153;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Human Attention during Goal-directed Reading Comprehension Relies on Task Optimization. (arXiv:2107.05799v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.05799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38405;&#35835;&#29702;&#35299;&#20013;&#20154;&#31867;&#20851;&#27880;&#21147;&#20998;&#37197;&#30340;&#35745;&#31639;&#27169;&#22411;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#25191;&#34892;&#30456;&#21516;&#30340;&#38405;&#35835;&#20219;&#21153;&#26102;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#39044;&#27979;&#27599;&#20010;&#21333;&#35789;&#30340;&#38405;&#35835;&#26102;&#38388;&#65292;&#35835;&#32773;&#22312;&#31532;&#19968;&#36941;&#38405;&#35835;&#21644;&#37325;&#26032;&#38405;&#35835;&#36807;&#31243;&#20013;&#20998;&#21035;&#20851;&#27880;&#22522;&#26412;&#25991;&#26412;&#29305;&#24449;&#21644;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#24182;&#19988;&#25991;&#26412;&#29305;&#24449;&#21644;&#38382;&#39064;&#30456;&#20851;&#24615;&#20250;&#20998;&#21035;&#35843;&#33410;&#27880;&#24847;&#21147;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#20219;&#21153;&#20013;&#20851;&#27880;&#21147;&#20998;&#37197;&#30340;&#35745;&#31639;&#21407;&#21017;&#20173;&#28982;&#19981;&#26126;&#30830;&#12290;&#30446;&#26631;&#23450;&#21521;&#38405;&#35835;&#65292;&#21363;&#38405;&#35835;&#19968;&#31687;&#25991;&#31456;&#20197;&#22238;&#31572;&#33041;&#28023;&#20013;&#30340;&#38382;&#39064;&#65292;&#26159;&#19968;&#31181;&#24378;&#28872;&#24341;&#21457;&#27880;&#24847;&#21147;&#30340;&#24120;&#35265;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20160;&#20040;&#35745;&#31639;&#27169;&#22411;&#21487;&#20197;&#35299;&#37322;&#36825;&#31181;&#22797;&#26434;&#20219;&#21153;&#20013;&#30340;&#20851;&#27880;&#21147;&#20998;&#37197;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20013;&#65292;&#20248;&#21270;&#25191;&#34892;&#30456;&#21516;&#38405;&#35835;&#20219;&#21153;&#30340;&#20851;&#27880;&#26435;&#37325;&#21487;&#20197;&#39044;&#27979;&#27599;&#20010;&#21333;&#35789;&#19978;&#30340;&#38405;&#35835;&#26102;&#38388;&#12290;&#30524;&#21160;&#36319;&#36394;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#35835;&#32773;&#22312;&#31532;&#19968;&#36941;&#38405;&#35835;&#21644;&#37325;&#26032;&#38405;&#35835;&#36807;&#31243;&#20013;&#20998;&#21035;&#20851;&#27880;&#22522;&#26412;&#25991;&#26412;&#29305;&#24449;&#21644;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#31867;&#20284;&#22320;&#65292;&#25991;&#26412;&#29305;&#24449;&#21644;&#38382;&#39064;&#30456;&#20851;&#24615;&#22312;&#27973;&#23618;&#21644;&#28145;&#23618;DNN&#23618;&#20013;&#20998;&#21035;&#35843;&#33410;&#27880;&#24847;&#21147;&#26435;&#37325;&#12290;&#27492;&#22806;&#65292;&#24403;&#35835;&#32773;&#22312;&#33041;&#28023;&#20013;&#27809;&#26377;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#25195;&#25551;&#19968;&#31687;&#25991;&#31456;&#26102;&#65292;&#20182;&#20204;&#30340;&#38405;&#35835;&#26102;&#38388;&#21487;&#20197;&#30001;&#20026;&#21333;&#35789;&#39044;&#27979;&#20219;&#21153;&#20248;&#21270;&#30340;DNN&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#38405;&#35835;&#20013;&#20851;&#27880;&#21147;&#20998;&#37197;&#20381;&#36182;&#20110;&#20219;&#21153;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The computational principles underlying attention allocation in complex goal-directed tasks remain elusive. Goal-directed reading, i.e., reading a passage to answer a question in mind, is a common real-world task that strongly engages attention. Here, we investigate what computational models can explain attention distribution in this complex task. We show that the reading time on each word is predicted by the attention weights in transformer-based deep neural networks (DNNs) optimized to perform the same reading task. Eye-tracking further reveals that readers separately attend to basic text features and question-relevant information during first-pass reading and rereading, respectively. Similarly, text features and question relevance separately modulate attention weights in shallow and deep DNN layers. Furthermore, when readers scan a passage without a question in mind, their reading time is predicted by DNNs optimized for a word prediction task. Therefore, attention during real-world 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25163;&#37096;&#25918;&#23556;&#32447;&#26816;&#26597;&#22270;&#20687;&#30340;&#39592;&#40836;&#35780;&#20272;&#27169;&#22411;Doctor Imitator&#65292;&#36890;&#36807;&#27169;&#20223;&#21307;&#29983;&#20351;&#29992;&#35780;&#20998;&#26041;&#27861;&#36827;&#34892;&#35786;&#26029;&#36923;&#36753;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#19982;&#21307;&#29983;&#37197;&#21512;&#12290;</title><link>http://arxiv.org/abs/2102.05424</link><description>&lt;p&gt;
&#21307;&#29983;&#27169;&#20223;&#32773;&#65306;&#25163;&#37096;&#25918;&#23556;&#32447;&#26816;&#26597;&#39592;&#40836;&#35780;&#20272;&#27169;&#22411;&#36890;&#36807;&#27169;&#20223;&#21307;&#29983;&#35780;&#20998;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Doctor Imitator: Hand-Radiography-based Bone Age Assessment by Imitating Scoring Methods. (arXiv:2102.05424v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.05424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25163;&#37096;&#25918;&#23556;&#32447;&#26816;&#26597;&#22270;&#20687;&#30340;&#39592;&#40836;&#35780;&#20272;&#27169;&#22411;Doctor Imitator&#65292;&#36890;&#36807;&#27169;&#20223;&#21307;&#29983;&#20351;&#29992;&#35780;&#20998;&#26041;&#27861;&#36827;&#34892;&#35786;&#26029;&#36923;&#36753;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#19982;&#21307;&#29983;&#37197;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39592;&#40836;&#35780;&#20272;&#26159;&#20020;&#24202;&#23454;&#36341;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#30001;&#20110;&#39592;&#40836;&#35780;&#20272;&#36807;&#31243;&#32321;&#29712;&#22797;&#26434;&#12290;&#30446;&#21069;&#30340;&#33258;&#21160;&#39592;&#40836;&#35780;&#20272;&#26041;&#27861;&#24456;&#23569;&#32771;&#34385;&#35786;&#26029;&#36923;&#36753;&#65292;&#22240;&#27492;&#21487;&#33021;&#20135;&#29983;&#26576;&#20123;&#26080;&#27861;&#35299;&#37322;&#30340;&#38544;&#34255;&#29366;&#24577;&#21644;&#36755;&#20986;&#12290;&#22240;&#27492;&#65292;&#21307;&#29983;&#20204;&#21487;&#33021;&#24456;&#38590;&#19982;&#36825;&#20123;&#27169;&#22411;&#21644;&#35856;&#21512;&#20316;&#65292;&#22240;&#20026;&#24456;&#38590;&#26816;&#26597;&#27169;&#22411;&#39044;&#27979;&#30340;&#27491;&#30830;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25163;&#37096;&#25918;&#23556;&#32447;&#26816;&#26597;&#22270;&#20687;&#30340;&#39592;&#40836;&#35780;&#20272;&#22270;&#20687;&#20998;&#26512;&#26694;&#26550;&#65292;&#31216;&#20026;Doctor Imitator&#65288;DI&#65289;&#27169;&#22411;&#12290;DI&#27169;&#22411;&#30340;&#35774;&#35745;&#26159;&#20026;&#20102;&#23398;&#20064;&#21307;&#29983;&#20351;&#29992;&#35780;&#20998;&#26041;&#27861;&#65288;&#20363;&#22914;Tanner-Whitehouse&#26041;&#27861;&#65289;&#36827;&#34892;&#39592;&#40836;&#35780;&#20272;&#30340;&#35786;&#26029;&#36923;&#36753;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;DI&#27169;&#22411;&#30340;&#21367;&#31215;&#23618;&#25429;&#25417;&#25163;&#37096;&#25918;&#23556;&#32447;&#26816;&#26597;&#22270;&#20687;&#24863;&#20852;&#36259;&#21306;&#22495;&#65288;ROIs&#65289;&#30340;&#23616;&#37096;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20110;&#35299;&#21078;&#23398;&#30340;&#32452;&#21367;&#31215;&#36827;&#34892;ROI&#21306;&#22495;&#30340;&#24471;&#20998;&#39044;&#27979;&#65292;&#26368;&#21518;&#36827;&#34892;&#27714;&#21644;&#24471;&#21040;&#39592;&#40836;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bone age assessment is challenging in clinical practice due to the complicated bone age assessment process. Current automatic bone age assessment methods were designed with rare consideration of the diagnostic logistics and thus may yield certain uninterpretable hidden states and outputs. Consequently, doctors can find it hard to cooperate with such models harmoniously because it is difficult to check the correctness of the model predictions. In this work, we propose a new graph-based deep learning framework for bone age assessment with hand radiographs, called Doctor Imitator (DI). The architecture of DI is designed to learn the diagnostic logistics of doctors using the scoring methods (e.g., the Tanner-Whitehouse method) for bone age assessment. Specifically, the convolutions of DI capture the local features of the anatomical regions of interest (ROIs) on hand radiographs and predict the ROI scores by our proposed Anatomy-based Group Convolution, summing up for bone age prediction. B
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#26412;&#20307;&#22312;FOLE&#19968;&#38454;&#36923;&#36753;&#29615;&#22659;&#20013;&#30340;&#34920;&#31034;&#65292;&#29305;&#21035;&#26159;&#25552;&#20379;&#20102;ERA&#25968;&#25454;&#27169;&#22411;&#30340;&#20005;&#26684;&#25968;&#23398;&#34920;&#31034;&#65292;&#20316;&#20026;&#26412;&#20307;&#35770;&#30340;&#22522;&#30784;&#25506;&#35752;&#12290;</title><link>http://arxiv.org/abs/1512.07430</link><description>&lt;p&gt;
FOLE ERA&#65306;&#22522;&#30784;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
The ERA of FOLE: Foundation. (arXiv:1512.07430v2 [cs.DB] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1512.07430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#26412;&#20307;&#22312;FOLE&#19968;&#38454;&#36923;&#36753;&#29615;&#22659;&#20013;&#30340;&#34920;&#31034;&#65292;&#29305;&#21035;&#26159;&#25552;&#20379;&#20102;ERA&#25968;&#25454;&#27169;&#22411;&#30340;&#20005;&#26684;&#25968;&#23398;&#34920;&#31034;&#65292;&#20316;&#20026;&#26412;&#20307;&#35770;&#30340;&#22522;&#30784;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#26412;&#20307;&#22312;FOLE&#65288;Kent 2013&#65289;&#19968;&#38454;&#36923;&#36753;&#29615;&#22659;&#20013;&#30340;&#34920;&#31034;&#12290;&#26412;&#20307;&#23450;&#20041;&#20102;&#29992;&#20110;&#20026;&#35805;&#35821;&#31038;&#21306;&#24314;&#27169;&#30693;&#35782;&#36164;&#28304;&#30340;&#21407;&#35821;&#65288;Gruber 2009&#65289;&#12290;&#36825;&#20123;&#21407;&#35821;&#21253;&#25324;&#31867;&#12289;&#20851;&#31995;&#21644;&#23646;&#24615;&#65292;&#30001;&#23454;&#20307;-&#20851;&#31995;-&#23646;&#24615;&#65288;ERA&#65289;&#25968;&#25454;&#27169;&#22411;&#65288;Chen 1976&#65289;&#34920;&#31034;&#12290;&#26412;&#25991;&#26159;&#19977;&#31687;&#35770;&#25991;&#20013;&#30340;&#31532;&#19968;&#31687;&#65292;&#23427;&#22312;FOLE&#30340;&#31532;&#19968;&#38454;&#36923;&#36753;&#29615;&#22659;&#20013;&#25552;&#20379;&#20102;ERA&#25968;&#25454;&#27169;&#22411;&#30340;&#20005;&#26684;&#25968;&#23398;&#34920;&#31034;&#65292;&#29305;&#21035;&#26159;&#26412;&#20307;&#35770;&#30340;&#22522;&#30784;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper discusses the representation of ontologies in the first-order logical environment FOLE (Kent 2013). An ontology defines the primitives with which to model the knowledge resources for a community of discourse (Gruber 2009). These primitives, consisting of classes, relationships and properties, are represented by the entity-relationship-attribute ERA data model (Chen 1976). An ontology uses formal axioms to constrain the interpretation of these primitives. In short, an ontology specifies a logical theory. This paper is the first in a series of three papers that provide a rigorous mathematical representation for the ERA data model in particular, and ontologies in general, within the first-order logical environment FOLE. The first two papers show how FOLE represents the formalism and semantics of (many-sorted) first-order logic in a classification form corresponding to ideas discussed in the Information Flow Framework (IFF). In particular, this first paper provides a foundation 
&lt;/p&gt;</description></item></channel></rss>