<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25235;&#21462;&#28909;&#22270;&#24341;&#23548;&#30340;&#39640;&#25928;&#23616;&#37096;&#25235;&#21462;&#29983;&#25104;&#26041;&#27861;&#65292;&#20174;&#20840;&#23616;&#21040;&#23616;&#37096;&#35821;&#20041;&#21040;&#28857;&#30340;&#26041;&#24335;&#25512;&#26029;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25235;&#21462;&#20934;&#30830;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.18546</link><description>&lt;p&gt;
&#22312;&#28151;&#20081;&#22330;&#26223;&#20013;&#39640;&#25928;&#30340;&#22522;&#20110;&#28909;&#22270;&#24341;&#23548;&#30340;&#20845;&#33258;&#30001;&#24230;&#25235;&#21462;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Efficient Heatmap-Guided 6-Dof Grasp Detection in Cluttered Scenes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25235;&#21462;&#28909;&#22270;&#24341;&#23548;&#30340;&#39640;&#25928;&#23616;&#37096;&#25235;&#21462;&#29983;&#25104;&#26041;&#27861;&#65292;&#20174;&#20840;&#23616;&#21040;&#23616;&#37096;&#35821;&#20041;&#21040;&#28857;&#30340;&#26041;&#24335;&#25512;&#26029;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25235;&#21462;&#20934;&#30830;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#65292;&#24555;&#36895;&#32780;&#31283;&#20581;&#30340;&#29289;&#20307;&#25235;&#21462;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26412;&#25991;&#25351;&#20986;&#24403;&#21069;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#20165;&#21033;&#29992;&#25972;&#20010;&#35266;&#23519;&#21040;&#30340;&#28857;&#20113;&#26469;&#29983;&#25104;&#20845;&#33258;&#30001;&#24230;&#30340;&#25235;&#21462;&#65292;&#24573;&#30053;&#20102;&#20174;&#20840;&#23616;&#35821;&#20041;&#20013;&#25366;&#25496;&#30340;&#24341;&#23548;&#20449;&#24687;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#39640;&#36136;&#37327;&#25235;&#21462;&#30340;&#29983;&#25104;&#21644;&#23454;&#26102;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;&#28909;&#22270;&#22312;&#20845;&#33258;&#30001;&#24230;&#25235;&#21462;&#29983;&#25104;&#25928;&#29575;&#26041;&#38754;&#34987;&#20302;&#20272;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#23616;&#37096;&#25235;&#21462;&#29983;&#25104;&#22120;&#65292;&#24182;&#32467;&#21512;&#25235;&#21462;&#28909;&#22270;&#20316;&#20026;&#24341;&#23548;&#65292;&#20197;&#20840;&#23616;&#21040;&#23616;&#37096;&#35821;&#20041;&#21040;&#28857;&#30340;&#26041;&#24335;&#36827;&#34892;&#25512;&#26029;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#39640;&#26031;&#32534;&#30721;&#21644;&#22522;&#20110;&#32593;&#26684;&#30340;&#31574;&#30053;&#26469;&#39044;&#27979;&#25235;&#21462;&#28909;&#22270;&#20197;&#25351;&#23548;&#23558;&#23616;&#37096;&#28857;&#32858;&#21512;&#21040;&#21487;&#25235;&#21462;&#21306;&#22495;&#65292;&#24182;&#25552;&#20379;&#20840;&#23616;&#35821;&#20041;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#22343;&#21248;&#38170;&#23450;&#37319;&#26679;&#26426;&#21046;&#26469;&#25552;&#39640;&#25235;&#21462;&#30340;&#20934;&#30830;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;&#21463;&#30410;&#20110;&#22270;&#20687;&#31354;&#38388;&#20013;&#30340;&#39640;&#25928;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18546v1 Announce Type: cross  Abstract: Fast and robust object grasping in clutter is a crucial component of robotics. Most current works resort to the whole observed point cloud for 6-Dof grasp generation, ignoring the guidance information excavated from global semantics, thus limiting high-quality grasp generation and real-time performance. In this work, we show that the widely used heatmaps are underestimated in the efficiency of 6-Dof grasp generation. Therefore, we propose an effective local grasp generator combined with grasp heatmaps as guidance, which infers in a global-to-local semantic-to-point way. Specifically, Gaussian encoding and the grid-based strategy are applied to predict grasp heatmaps as guidance to aggregate local points into graspable regions and provide global semantic information. Further, a novel non-uniform anchor sampling mechanism is designed to improve grasp accuracy and diversity. Benefiting from the high-efficiency encoding in the image space 
&lt;/p&gt;</description></item><item><title>GeNet&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35821;&#20041;&#36890;&#20449;&#33539;&#24335;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#36716;&#25442;&#20026;&#22270;&#32467;&#26500;&#12289;&#21033;&#29992;&#32534;&#30721;&#22120;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#24182;&#21033;&#29992;&#35299;&#30721;&#22120;&#37325;&#24314;&#20449;&#24687;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#25239;&#22122;&#22768;&#20219;&#21153;&#23548;&#21521;&#36890;&#20449;&#12290;</title><link>https://arxiv.org/abs/2403.18296</link><description>&lt;p&gt;
GeNet:&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25239;&#22122;&#22768;&#20219;&#21153;&#23548;&#21521;&#35821;&#20041;&#36890;&#20449;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
GeNet: A Graph Neural Network-based Anti-noise Task-Oriented Semantic Communication Paradigm
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18296
&lt;/p&gt;
&lt;p&gt;
GeNet&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35821;&#20041;&#36890;&#20449;&#33539;&#24335;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#36716;&#25442;&#20026;&#22270;&#32467;&#26500;&#12289;&#21033;&#29992;&#32534;&#30721;&#22120;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#24182;&#21033;&#29992;&#35299;&#30721;&#22120;&#37325;&#24314;&#20449;&#24687;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#25239;&#22122;&#22768;&#20219;&#21153;&#23548;&#21521;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#35821;&#20041;&#36890;&#20449;&#20219;&#21153;&#26041;&#27861;&#20381;&#36182;&#20110;&#20102;&#35299;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#26469;&#20943;&#36731;&#36890;&#36947;&#22122;&#22768;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#22312;&#29305;&#23450;&#30340;SNR&#26465;&#20214;&#19979;&#36827;&#34892;&#35757;&#32451;&#65292;&#38656;&#35201;&#22823;&#37327;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GeNet&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#35821;&#20041;&#36890;&#20449;&#33539;&#24335;&#65292;&#26088;&#22312;&#25269;&#25239;&#22122;&#22768;&#65292;&#20174;&#32780;&#20419;&#36827;&#20219;&#21153;&#23548;&#21521;&#36890;&#20449;&#65288;TOC&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#39318;&#20808;&#23558;&#36755;&#20837;&#25968;&#25454;&#22270;&#20687;&#36716;&#25442;&#20026;&#22270;&#32467;&#26500;&#12290;&#28982;&#21518;&#21033;&#29992;&#22522;&#20110;GNN&#30340;&#32534;&#30721;&#22120;&#20174;&#28304;&#25968;&#25454;&#20013;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#12290;&#36825;&#20123;&#25552;&#21462;&#30340;&#35821;&#20041;&#20449;&#24687;&#28982;&#21518;&#36890;&#36807;&#36890;&#36947;&#20256;&#36755;&#12290;&#22312;&#25509;&#25910;&#31471;&#65292;&#20351;&#29992;&#22522;&#20110;GNN&#30340;&#35299;&#30721;&#22120;&#20174;&#28304;&#25968;&#25454;&#20013;&#37325;&#24314;&#30456;&#20851;&#30340;&#35821;&#20041;&#20449;&#24687;&#20197;&#29992;&#20110;TOC&#12290;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;GeNet&#22312;&#25239;&#22122;&#22768;TOC&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18296v1 Announce Type: cross  Abstract: Traditional approaches to semantic communication tasks rely on the knowledge of the signal-to-noise ratio (SNR) to mitigate channel noise. However, these methods necessitate training under specific SNR conditions, entailing considerable time and computational resources. In this paper, we propose GeNet, a Graph Neural Network (GNN)-based paradigm for semantic communication aimed at combating noise, thereby facilitating Task-Oriented Communication (TOC). We propose a novel approach where we first transform the input data image into graph structures. Then we leverage a GNN-based encoder to extract semantic information from the source data. This extracted semantic information is then transmitted through the channel. At the receiver's end, a GNN-based decoder is utilized to reconstruct the relevant semantic information from the source data for TOC. Through experimental evaluation, we show GeNet's effectiveness in anti-noise TOC while decoup
&lt;/p&gt;</description></item><item><title>GPT-4V&#22312;&#33016;&#37096;X&#20809;&#25918;&#23556;&#26816;&#26597;&#30340;&#25918;&#23556;&#23398;&#21457;&#29616;&#26816;&#27979;&#19978;&#23578;&#26410;&#20934;&#22791;&#22909;&#24212;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#35786;&#26029;&#29992;&#36884;</title><link>https://arxiv.org/abs/2403.15528</link><description>&lt;p&gt;
&#20351;&#29992;&#35270;&#35273;&#35780;&#20272;GPT-4&#22312;&#33016;&#37096;X&#20809;&#25918;&#23556;&#26816;&#26597;&#20013;&#30340;&#25918;&#23556;&#23398;&#21457;&#29616;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Evaluating GPT-4 with Vision on Detection of Radiological Findings on Chest Radiographs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15528
&lt;/p&gt;
&lt;p&gt;
GPT-4V&#22312;&#33016;&#37096;X&#20809;&#25918;&#23556;&#26816;&#26597;&#30340;&#25918;&#23556;&#23398;&#21457;&#29616;&#26816;&#27979;&#19978;&#23578;&#26410;&#20934;&#22791;&#22909;&#24212;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#35786;&#26029;&#29992;&#36884;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;GPT-4V&#30340;&#24212;&#29992;&#65292;&#36825;&#26159;&#19968;&#20010;&#35013;&#22791;&#26377;&#35270;&#35273;&#35782;&#21035;&#21151;&#33021;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#26469;&#33258;100&#24352;&#33016;&#37096;X&#20809;&#25918;&#23556;&#26816;&#26597;&#30340;&#25918;&#23556;&#23398;&#21457;&#29616;&#65292;&#24182;&#25351;&#20986;&#30446;&#21069;GPT-4V&#36824;&#19981;&#36866;&#29992;&#20110;&#35299;&#37322;&#33016;&#37096;X&#20809;&#25918;&#23556;&#26816;&#26597;&#30340;&#23454;&#38469;&#35786;&#26029;&#29992;&#36884;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15528v1 Announce Type: cross  Abstract: The study examines the application of GPT-4V, a multi-modal large language model equipped with visual recognition, in detecting radiological findings from a set of 100 chest radiographs and suggests that GPT-4V is currently not ready for real-world diagnostic usage in interpreting chest radiographs.
&lt;/p&gt;</description></item><item><title>&#25209;&#35780;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#27169;&#22411;&#23545;&#38750;&#26126;&#26174;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#25552;&#20986;&#20102;Adversarial Nibbler Challenge&#20197;&#20247;&#21253;&#22810;&#26679;&#21270;&#30340;&#25552;&#31034;&#26469;&#32416;&#27491;&#27169;&#22411;&#30340;&#23433;&#20840;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.12075</link><description>&lt;p&gt;
Adversarial Nibbler: &#19968;&#31181;&#29992;&#20110;&#35782;&#21035;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#22810;&#26679;&#21270;&#21361;&#23475;&#30340;&#24320;&#25918;&#24335;&#32418;&#38431;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Adversarial Nibbler: An Open Red-Teaming Method for Identifying Diverse Harms in Text-to-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12075
&lt;/p&gt;
&lt;p&gt;
&#25209;&#35780;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#27169;&#22411;&#23545;&#38750;&#26126;&#26174;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#25552;&#20986;&#20102;Adversarial Nibbler Challenge&#20197;&#20247;&#21253;&#22810;&#26679;&#21270;&#30340;&#25552;&#31034;&#26469;&#32416;&#27491;&#27169;&#22411;&#30340;&#23433;&#20840;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#29983;&#25104;AI&#27169;&#22411;&#30340;&#23835;&#36215;&#65292;&#35780;&#20272;&#27169;&#22411;&#23545;&#20110;&#19981;&#26126;&#26174;&#25915;&#20987;&#30340;&#31283;&#20581;&#24615;&#20197;&#20943;&#23569;&#29983;&#25104;&#20882;&#29359;&#24615;&#22270;&#20687;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#19987;&#27880;&#20110;"&#38544;&#24615;&#23545;&#25239;"&#25552;&#31034;&#65288;&#35302;&#21457;T2I&#27169;&#22411;&#29983;&#25104;&#19981;&#23433;&#20840;&#22270;&#20687;&#30340;&#38750;&#26126;&#26174;&#21407;&#22240;&#65289;&#65292;&#25105;&#20204;&#29420;&#31435;&#36776;&#21035;&#20986;&#19968;&#32452;&#38590;&#20197;&#21457;&#29616;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#20154;&#31867;&#21019;&#36896;&#21147;&#24456;&#36866;&#21512;&#25581;&#31034;&#36825;&#20123;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;Adversarial Nibbler Challenge&#65292;&#36825;&#26159;&#19968;&#20010;&#32418;&#38431;&#26041;&#27861;&#65292;&#29992;&#20110;&#20247;&#21253;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#38544;&#24615;&#23545;&#25239;&#24615;&#25552;&#31034;&#12290;&#25105;&#20204;&#24050;&#27719;&#24635;&#19968;&#22871;&#26368;&#20808;&#36827;&#30340;T2I&#27169;&#22411;&#65292;&#37319;&#29992;&#31616;&#21333;&#29992;&#25143;&#30028;&#38754;&#26469;&#35782;&#21035;&#21644;&#27880;&#37322;&#21361;&#23475;&#65292;&#24182;&#21560;&#24341;&#24191;&#27867;&#20154;&#32676;&#26469;&#25429;&#25417;&#22312;&#26631;&#20934;&#27979;&#35797;&#20013;&#21487;&#33021;&#34987;&#24573;&#35270;&#30340;&#38271;&#23614;&#23433;&#20840;&#38382;&#39064;&#12290;&#25361;&#25112;&#22312;&#36830;&#32493;&#22238;&#21512;&#20013;&#36827;&#34892;&#65292;&#20197;&#23454;&#29616;&#23545;T2I&#27169;&#22411;&#20013;&#23433;&#20840;&#38544;&#24739;&#30340;&#25345;&#32493;&#21457;&#29616;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12075v1 Announce Type: cross  Abstract: With the rise of text-to-image (T2I) generative AI models reaching wide audiences, it is critical to evaluate model robustness against non-obvious attacks to mitigate the generation of offensive images. By focusing on ``implicitly adversarial'' prompts (those that trigger T2I models to generate unsafe images for non-obvious reasons), we isolate a set of difficult safety issues that human creativity is well-suited to uncover. To this end, we built the Adversarial Nibbler Challenge, a red-teaming methodology for crowdsourcing a diverse set of implicitly adversarial prompts. We have assembled a suite of state-of-the-art T2I models, employed a simple user interface to identify and annotate harms, and engaged diverse populations to capture long-tail safety issues that may be overlooked in standard testing. The challenge is run in consecutive rounds to enable a sustained discovery and analysis of safety pitfalls in T2I models.   In this pape
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#27169;&#25311;&#27599;&#20010;&#23376;&#32467;&#26500;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#26681;&#25454;&#22810;&#23618;&#32467;&#26500;&#30340;&#32467;&#26524;&#33258;&#36866;&#24212;&#22320;&#34701;&#21512;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2403.10799</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#36866;&#24212;&#20272;&#35745;&#34701;&#21512;&#39640;&#25928;&#21098;&#26525;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Efficient Pruning of Large Language Model with Adaptive Estimation Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10799
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#27169;&#25311;&#27599;&#20010;&#23376;&#32467;&#26500;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#26681;&#25454;&#22810;&#23618;&#32467;&#26500;&#30340;&#32467;&#26524;&#33258;&#36866;&#24212;&#22320;&#34701;&#21512;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#29983;&#25104;&#24615;&#19979;&#28216;&#20219;&#21153;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#36825;&#23548;&#33268;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#39640;&#25928;&#37096;&#32626;&#23427;&#20204;&#25104;&#20026;&#19981;&#21487;&#36991;&#20813;&#30340;&#36235;&#21183;&#21644;&#37325;&#22823;&#25361;&#25112;&#12290;&#32467;&#26500;&#21270;&#21098;&#26525;&#26159;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#24191;&#27867;&#24212;&#29992;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#22788;&#29702;&#22810;&#20010;&#35299;&#30721;&#22120;&#23618;&#30340;&#22797;&#26434;&#32467;&#26500;&#26102;&#65292;&#36890;&#24120;&#30340;&#26041;&#27861;&#24448;&#24448;&#37319;&#29992;&#24120;&#35265;&#30340;&#20272;&#35745;&#26041;&#27861;&#36827;&#34892;&#21098;&#26525;&#12290;&#36825;&#20123;&#26041;&#27861;&#23548;&#33268;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#31934;&#24230;&#19979;&#38477;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#33258;&#36866;&#24212;&#22320;&#27169;&#25311;&#27599;&#20010;&#23376;&#32467;&#26500;&#30340;&#37325;&#35201;&#24615;&#12290;&#21516;&#26102;&#65292;&#23427;&#21487;&#20197;&#22522;&#20110;&#22797;&#26434;&#21644;&#22810;&#23618;&#32467;&#26500;&#30340;&#32467;&#26524;&#65292;&#33258;&#36866;&#24212;&#22320;&#34701;&#21512;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#35774;&#35745;&#30340;&#25152;&#26377;&#26041;&#38754;&#37117;&#26080;&#32541;&#38598;&#25104;&#21040;&#31471;&#21040;&#31471;&#30340;&#21098;&#26525;&#26694;&#26550;&#20013;&#12290;&#19982;&#20027;&#27969;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10799v1 Announce Type: cross  Abstract: Large language models (LLMs) have become crucial for many generative downstream tasks, leading to an inevitable trend and significant challenge to deploy them efficiently on resource-constrained devices. Structured pruning is a widely used method to address this challenge. However, when dealing with the complex structure of the multiple decoder layers, general methods often employ common estimation approaches for pruning. These approaches lead to a decline in accuracy for specific downstream tasks. In this paper, we introduce a simple yet efficient method that adaptively models the importance of each substructure. Meanwhile, it can adaptively fuse coarse-grained and finegrained estimations based on the results from complex and multilayer structures. All aspects of our design seamlessly integrate into the endto-end pruning framework. Our experimental results, compared with state-of-the-art methods on mainstream datasets, demonstrate ave
&lt;/p&gt;</description></item><item><title>&#37325;&#35201;&#24615;&#21152;&#26435;&#26159;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#22522;&#26412;&#31243;&#24207;&#65292;&#36890;&#36807;&#23545;&#30446;&#26631;&#20989;&#25968;&#25110;&#27010;&#29575;&#20998;&#24067;&#36827;&#34892;&#21152;&#26435;&#65292;&#21487;&#20197;&#20445;&#35777;&#30417;&#30563;&#23398;&#20064;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#20043;&#38388;&#24046;&#24322;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#32479;&#35745;&#19978;&#26399;&#26395;&#30340;&#24615;&#36136;</title><link>https://arxiv.org/abs/2403.10175</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#24615;&#21152;&#26435;&#31616;&#35201;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Short Survey on Importance Weighting for Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10175
&lt;/p&gt;
&lt;p&gt;
&#37325;&#35201;&#24615;&#21152;&#26435;&#26159;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#22522;&#26412;&#31243;&#24207;&#65292;&#36890;&#36807;&#23545;&#30446;&#26631;&#20989;&#25968;&#25110;&#27010;&#29575;&#20998;&#24067;&#36827;&#34892;&#21152;&#26435;&#65292;&#21487;&#20197;&#20445;&#35777;&#30417;&#30563;&#23398;&#20064;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#20043;&#38388;&#24046;&#24322;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#32479;&#35745;&#19978;&#26399;&#26395;&#30340;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#35201;&#24615;&#21152;&#26435;&#26159;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#39033;&#22522;&#26412;&#31243;&#24207;&#65292;&#26681;&#25454;&#26576;&#31181;&#24847;&#20041;&#19978;&#23454;&#20363;&#30340;&#37325;&#35201;&#24615;&#23545;&#30446;&#26631;&#20989;&#25968;&#25110;&#27010;&#29575;&#20998;&#24067;&#36827;&#34892;&#21152;&#26435;&#12290;&#36825;&#19968;&#31616;&#21333;&#32780;&#26377;&#29992;&#30340;&#24605;&#24819;&#30340;&#24191;&#27867;&#24212;&#29992;&#23548;&#33268;&#20102;&#35768;&#22810;&#37325;&#35201;&#24615;&#21152;&#26435;&#30340;&#24212;&#29992;&#12290;&#20363;&#22914;&#65292;&#25454;&#30693;&#65292;&#22312;&#20851;&#20110;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#20043;&#38388;&#24046;&#24322;&#30340;&#20551;&#35774;&#19979;&#30340;&#30417;&#30563;&#23398;&#20064;&#65292;&#36890;&#36807;&#23494;&#24230;&#27604;&#30340;&#37325;&#35201;&#24615;&#21152;&#26435;&#21487;&#20197;&#20445;&#35777;&#32479;&#35745;&#19978;&#26399;&#26395;&#30340;&#24615;&#36136;&#12290;&#36825;&#39033;&#35843;&#26597;&#24635;&#32467;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#30456;&#20851;&#30740;&#31350;&#20013;&#37325;&#35201;&#24615;&#21152;&#26435;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10175v1 Announce Type: cross  Abstract: Importance weighting is a fundamental procedure in statistics and machine learning that weights the objective function or probability distribution based on the importance of the instance in some sense. The simplicity and usefulness of the idea has led to many applications of importance weighting. For example, it is known that supervised learning under an assumption about the difference between the training and test distributions, called distribution shift, can guarantee statistically desirable properties through importance weighting by their density ratio. This survey summarizes the broad applications of importance weighting in machine learning and related research.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;LaGTran&#26694;&#26550;&#65292;&#21033;&#29992;&#25991;&#26412;&#25551;&#36848;&#26469;&#24341;&#23548;&#30693;&#35782;&#36716;&#31227;&#65292;&#22312;&#22788;&#29702;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.05535</link><description>&lt;p&gt;
&#35762;&#36848;&#65292;&#32780;&#19981;&#26159;&#23637;&#31034;&#65281;&#65306;&#35821;&#35328;&#25351;&#23548;&#26377;&#21161;&#20110;&#22312;&#22270;&#20687;&#21644;&#35270;&#39057;&#39046;&#22495;&#20043;&#38388;&#36827;&#34892;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Tell, Don't Show!: Language Guidance Eases Transfer Across Domains in Images and Videos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05535
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;LaGTran&#26694;&#26550;&#65292;&#21033;&#29992;&#25991;&#26412;&#25551;&#36848;&#26469;&#24341;&#23548;&#30693;&#35782;&#36716;&#31227;&#65292;&#22312;&#22788;&#29702;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;LaGTran&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#21363;&#21487;&#33719;&#24471;&#25110;&#26131;&#20110;&#33719;&#21462;&#30340;&#25991;&#26412;&#25551;&#36848;&#65292;&#24341;&#23548;&#20174;&#24102;&#26631;&#31614;&#30340;&#28304;&#25968;&#25454;&#21040;&#20855;&#26377;&#22495;&#20559;&#31227;&#30340;&#26080;&#26631;&#31614;&#30446;&#26631;&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#30693;&#35782;&#36716;&#31227;&#12290;&#21463;&#21040;&#25105;&#20204;&#35266;&#23519;&#21040;&#26356;&#23500;&#35821;&#20041;&#30340;&#25991;&#26412;&#27169;&#24577;&#20855;&#26377;&#26356;&#26377;&#21033;&#30340;&#36716;&#31227;&#29305;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36716;&#31227;&#26426;&#21046;&#65292;&#20351;&#29992;&#28304;&#35757;&#32451;&#30340;&#25991;&#26412;&#20998;&#31867;&#22120;&#22312;&#30446;&#26631;&#25991;&#26412;&#25551;&#36848;&#19978;&#29983;&#25104;&#39044;&#27979;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#39044;&#27979;&#20316;&#20026;&#30456;&#24212;&#22270;&#20687;&#30340;&#30417;&#30563;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#35821;&#35328;&#25351;&#23548;&#20026;&#39537;&#21160;&#65292;&#20986;&#22855;&#22320;&#31616;&#21333;&#26131;&#34892;&#65292;&#21364;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#22914;GeoNet&#21644;DomainNet&#19978;&#26174;&#33879;&#20248;&#20110;&#20197;&#24448;&#25152;&#26377;&#26041;&#27861;&#65292;&#39564;&#35777;&#20102;&#20854;&#26497;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05535v1 Announce Type: cross  Abstract: We introduce LaGTran, a novel framework that utilizes readily available or easily acquired text descriptions to guide robust transfer of discriminative knowledge from labeled source to unlabeled target data with domain shifts. While unsupervised adaptation methods have been established to address this problem, they show limitations in handling challenging domain shifts due to their exclusive operation within the pixel-space. Motivated by our observation that semantically richer text modality has more favorable transfer properties, we devise a transfer mechanism to use a source-trained text-classifier to generate predictions on the target text descriptions, and utilize these predictions as supervision for the corresponding images. Our approach driven by language guidance is surprisingly easy and simple, yet significantly outperforms all prior approaches on challenging datasets like GeoNet and DomainNet, validating its extreme effectiven
&lt;/p&gt;</description></item><item><title>&#23545;&#21307;&#23398;LLMs&#36827;&#34892;&#20102;&#39318;&#27425;&#23433;&#20840;&#35780;&#20272;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#23450;&#20041;&#21307;&#23398;&#23433;&#20840;&#21644;&#23545;&#40784;&#24615;&#65292;&#24320;&#21457;&#20102;&#26377;&#23475;&#21307;&#23398;&#38382;&#39064;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#20102;&#21307;&#23398;LLMs&#30340;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#65292;&#23637;&#31034;&#20102;&#24494;&#35843;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#32531;&#35299;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.03744</link><description>&lt;p&gt;
&#20026;&#21307;&#33647;&#39046;&#22495;&#25171;&#36896;&#23433;&#20840;&#21644;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards Safe and Aligned Large Language Models for Medicine
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03744
&lt;/p&gt;
&lt;p&gt;
&#23545;&#21307;&#23398;LLMs&#36827;&#34892;&#20102;&#39318;&#27425;&#23433;&#20840;&#35780;&#20272;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#23450;&#20041;&#21307;&#23398;&#23433;&#20840;&#21644;&#23545;&#40784;&#24615;&#65292;&#24320;&#21457;&#20102;&#26377;&#23475;&#21307;&#23398;&#38382;&#39064;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#20102;&#21307;&#23398;LLMs&#30340;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#65292;&#23637;&#31034;&#20102;&#24494;&#35843;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#32531;&#35299;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#27491;&#22312;&#20197;&#24778;&#20154;&#30340;&#36895;&#24230;&#36827;&#27493;&#65292;&#21363;&#20351;&#26159;&#23427;&#20204;&#30340;&#24320;&#21457;&#32773;&#20063;&#23545;&#23427;&#20204;&#30340;&#28508;&#21147;&#21644;&#39118;&#38505;&#30340;&#28145;&#24230;&#24863;&#21040;&#22256;&#24785;&#12290;&#23613;&#31649;&#24050;&#32463;&#37319;&#21462;&#20102;&#21021;&#27493;&#27493;&#39588;&#35780;&#20272;&#36890;&#29992;&#30693;&#35782;LLMs&#30340;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#65292;&#25581;&#31034;&#20102;&#19968;&#20123;&#24369;&#28857;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#23613;&#31649;&#22312;&#20010;&#20154;&#20581;&#24247;&#21644;&#23433;&#20840;&#12289;&#20844;&#20849;&#20581;&#24247;&#21644;&#23433;&#20840;&#20197;&#21450;&#20154;&#26435;&#26041;&#38754;&#23384;&#22312;&#39118;&#38505;&#65292;&#21307;&#23398;LLMs&#30340;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#23578;&#26410;&#24471;&#21040;&#35780;&#20272;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23545;&#21307;&#23398;LLMs&#30340;&#39318;&#27425;&#23433;&#20840;&#35780;&#20272;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21307;&#23398;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#30340;&#23450;&#20041;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#26377;&#23475;&#21307;&#23398;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;LLM&#30340;&#21307;&#23398;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#65292;&#35780;&#20272;&#20102;&#21307;&#23398;LLMs&#30340;&#36890;&#29992;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#65292;&#23637;&#31034;&#20102;&#24494;&#35843;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#32531;&#35299;&#31574;&#30053;&#65292;&#24182;&#35752;&#35770;&#20102;&#26356;&#24191;&#27867;&#30340;&#12289;&#22823;&#35268;&#27169;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03744v1 Announce Type: new  Abstract: The capabilities of large language models (LLMs) have been progressing at a breathtaking speed, leaving even their own developers grappling with the depth of their potential and risks. While initial steps have been taken to evaluate the safety and alignment of general-knowledge LLMs, exposing some weaknesses, to our knowledge, the safety and alignment of medical LLMs has not been evaluated despite their risks for personal health and safety, public health and safety, and human rights. To this end, we carry out the first safety evaluation for medical LLMs. Specifically, we set forth a definition of medical safety and alignment for medical artificial intelligence systems, develop a dataset of harmful medical questions to evaluate the medical safety and alignment of an LLM, evaluate both general and medical safety and alignment of medical LLMs, demonstrate fine-tuning as an effective mitigation strategy, and discuss broader, large-scale appr
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;ParallelPARC&#27969;&#27700;&#32447;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22797;&#26434;&#27573;&#33853;&#31867;&#27604;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#21508;&#31181;&#31867;&#27604;&#31867;&#22411;&#65292;&#24182;&#23637;&#31034;&#20986;&#20154;&#31867;&#22312;&#31867;&#27604;&#35782;&#21035;&#20013;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.01139</link><description>&lt;p&gt;
ParallelPARC: &#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#31867;&#27604;&#30340;&#21487;&#25193;&#23637;&#27969;&#27700;&#32447;
&lt;/p&gt;
&lt;p&gt;
ParallelPARC: A Scalable Pipeline for Generating Natural-Language Analogies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01139
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;ParallelPARC&#27969;&#27700;&#32447;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22797;&#26434;&#27573;&#33853;&#31867;&#27604;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#21508;&#31181;&#31867;&#27604;&#31867;&#22411;&#65292;&#24182;&#23637;&#31034;&#20986;&#20154;&#31867;&#22312;&#31867;&#27604;&#35782;&#21035;&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Analogy-making&#23545;&#20110;&#20154;&#31867;&#35748;&#30693;&#33267;&#20851;&#37325;&#35201;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#36866;&#24212;&#26032;&#39062;&#24773;&#22659;--&#36825;&#26159;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20173;&#28982;&#32570;&#20047;&#30340;&#33021;&#21147;&#12290;&#22823;&#22810;&#25968;&#31867;&#27604;&#25968;&#25454;&#38598;&#20170;&#22825;&#20851;&#27880;&#31616;&#21333;&#30340;&#31867;&#27604;&#65288;&#20363;&#22914;&#65292;&#35789;&#31867;&#27604;&#65289;&#65307;&#21253;&#21547;&#22797;&#26434;&#31867;&#22411;&#31867;&#27604;&#30340;&#25968;&#25454;&#38598;&#36890;&#24120;&#26159;&#25163;&#24037;&#31574;&#21010;&#30340;&#65292;&#24182;&#19988;&#38750;&#24120;&#23567;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#38480;&#21046;&#20102;&#35745;&#31639;&#31867;&#27604;&#30340;&#36827;&#23637;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#25968;&#25454;&#29983;&#25104;&#27969;&#27700;&#32447;&#65292;ParallelPARC&#65288;Parallel Paragraph Creator&#65289;&#65292;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#21019;&#24314;&#22522;&#20110;&#27573;&#33853;&#30340;&#22797;&#26434;&#31867;&#27604;&#65292;&#20197;&#21450;&#31616;&#21333;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24178;&#25200;&#39033;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27969;&#27700;&#32447;&#65292;&#24182;&#21019;&#24314;&#20102;ProPara-Logy&#65292;&#19968;&#20010;&#20851;&#20110;&#31185;&#23398;&#36807;&#31243;&#38388;&#31867;&#27604;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#30001;&#20154;&#31867;&#39564;&#35777;&#36807;&#30340;&#37329;&#26631;&#20934;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#20010;&#33258;&#21160;&#29983;&#25104;&#30340;&#38134;&#26631;&#20934;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#22312;&#20108;&#36827;&#21046;&#21644;&#22810;&#36873;&#29615;&#22659;&#20013;&#27979;&#35797;&#20102;LLMs&#21644;&#20154;&#31867;&#23545;&#31867;&#27604;&#30340;&#35782;&#21035;&#65292;&#21457;&#29616;&#20154;&#31867;&#32988;&#36807;&#26368;&#20339;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01139v1 Announce Type: cross  Abstract: Analogy-making is central to human cognition, allowing us to adapt to novel situations -- an ability that current AI systems still lack. Most analogy datasets today focus on simple analogies (e.g., word analogies); datasets including complex types of analogies are typically manually curated and very small. We believe that this holds back progress in computational analogy. In this work, we design a data generation pipeline, ParallelPARC (Parallel Paragraph Creator) leveraging state-of-the-art Large Language Models (LLMs) to create complex, paragraph-based analogies, as well as distractors, both simple and challenging. We demonstrate our pipeline and create ProPara-Logy, a dataset of analogies between scientific processes. We publish a gold-set, validated by humans, and a silver-set, generated automatically. We test LLMs' and humans' analogy recognition in binary and multiple-choice settings, and found that humans outperform the best mod
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#31181;&#29992;&#20110;Llama 2 Chat 7B&#25110;&#26356;&#22823;&#27169;&#22411;&#30340;&#33609;&#26696;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#21152;&#36895;&#25512;&#29702;&#65292;&#20165;&#21344;&#21407;&#22987;&#22823;&#23567;&#30340;1.64&#65285;&#12290;</title><link>https://arxiv.org/abs/2403.00858</link><description>&lt;p&gt;
&#30452;&#25509;&#19982;Chat-Fine-Tuned LLMs&#30340;&#33609;&#26696;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Direct Alignment of Draft Model for Speculative Decoding with Chat-Fine-Tuned LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00858
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#31181;&#29992;&#20110;Llama 2 Chat 7B&#25110;&#26356;&#22823;&#27169;&#22411;&#30340;&#33609;&#26696;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#21152;&#36895;&#25512;&#29702;&#65292;&#20165;&#21344;&#21407;&#22987;&#22823;&#23567;&#30340;1.64&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#29983;&#25104;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#20854;&#33258;&#22238;&#24402;&#26412;&#36136;&#12289;&#24040;&#22823;&#30340;&#21442;&#25968;&#25968;&#37327;&#21644;&#26377;&#38480;&#30340;&#20869;&#23384;&#24102;&#23485;&#32780;&#34987;&#35748;&#20026;&#26159;&#20869;&#23384;&#23494;&#38598;&#22411;&#65292;&#36890;&#24120;&#23548;&#33268;&#20302;&#20196;&#29260;&#36895;&#29575;&#12290;&#29468;&#27979;&#35299;&#30721;&#24050;&#34987;&#25552;&#20986;&#20316;&#20026;LLM&#25512;&#29702;&#21152;&#36895;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#20195;&#24320;&#28304;LLM&#31995;&#21015;&#20013;&#65292;&#20363;&#22914;Llama 2 7B&#65292;&#30001;&#20110;&#33609;&#26696;&#27169;&#22411;&#36890;&#24120;&#19981;&#21487;&#29992;&#65292;&#22240;&#27492;&#38656;&#35201;&#35757;&#32451;&#39640;&#36136;&#37327;&#30340;&#33609;&#26696;&#27169;&#22411;&#20197;&#36890;&#36807;&#29468;&#27979;&#35299;&#30721;&#23454;&#29616;&#25512;&#29702;&#21152;&#36895;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#33609;&#26696;&#27169;&#22411;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#30452;&#25509;&#19982;Chat-capable&#30446;&#26631;&#27169;&#22411;&#23545;&#40784;&#12290;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#35757;&#32451;&#20986;Llama 2 Chat Drafter 115M&#65292;&#36825;&#26159;&#19968;&#20010;&#36866;&#29992;&#20110;Llama 2 Chat 7B&#25110;&#26356;&#22823;&#27169;&#22411;&#30340;&#33609;&#26696;&#27169;&#22411;&#65292;&#20165;&#21344;&#21407;&#22987;&#22823;&#23567;&#30340;1.64&#65285;&#12290;&#25105;&#20204;&#30340;&#35757;&#32451;&#26694;&#26550;&#20165;&#21253;&#25324;&#39044;&#35757;&#32451;&#12289;&#33976;&#39311;&#25968;&#25454;&#38598;&#29983;&#25104;&#21644;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#36827;&#34892;&#24494;&#35843;&#65292;&#27809;&#26377;&#39069;&#22806;&#30340;&#23545;&#40784;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00858v1 Announce Type: cross  Abstract: Text generation with Large Language Models (LLMs) is known to be memory bound due to the combination of their auto-regressive nature, huge parameter counts, and limited memory bandwidths, often resulting in low token rates. Speculative decoding has been proposed as a solution for LLM inference acceleration. However, since draft models are often unavailable in the modern open-source LLM families, e.g., for Llama 2 7B, training a high-quality draft model is required to enable inference acceleration via speculative decoding. In this paper, we propose a simple draft model training framework for direct alignment to chat-capable target models. With the proposed framework, we train Llama 2 Chat Drafter 115M, a draft model for Llama 2 Chat 7B or larger, with only 1.64\% of the original size. Our training framework only consists of pretraining, distillation dataset generation, and finetuning with knowledge distillation, with no additional align
&lt;/p&gt;</description></item><item><title>PiShield&#26159;&#31532;&#19968;&#20010;&#20801;&#35768;&#23558;&#38656;&#27714;&#38598;&#25104;&#21040;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#20013;&#30340;&#26694;&#26550;&#65292;&#26080;&#35770;&#36755;&#20837;&#22914;&#20309;&#37117;&#33021;&#30830;&#20445;&#28385;&#36275;&#36825;&#20123;&#38656;&#27714;&#65292;&#24182;&#21487;&#26681;&#25454;&#20174;&#19994;&#32773;&#38656;&#27714;&#22312;&#25512;&#26029;&#21644;/&#25110;&#35757;&#32451;&#26102;&#38598;&#25104;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2402.18285</link><description>&lt;p&gt;
PiShield&#65306;&#19968;&#31181;&#36866;&#29992;&#20110;&#20197;&#38656;&#27714;&#20026;&#22522;&#30784;&#23398;&#20064;&#30340;NeSy&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
PiShield: A NeSy Framework for Learning with Requirements
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18285
&lt;/p&gt;
&lt;p&gt;
PiShield&#26159;&#31532;&#19968;&#20010;&#20801;&#35768;&#23558;&#38656;&#27714;&#38598;&#25104;&#21040;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#20013;&#30340;&#26694;&#26550;&#65292;&#26080;&#35770;&#36755;&#20837;&#22914;&#20309;&#37117;&#33021;&#30830;&#20445;&#28385;&#36275;&#36825;&#20123;&#38656;&#27714;&#65292;&#24182;&#21487;&#26681;&#25454;&#20174;&#19994;&#32773;&#38656;&#27714;&#22312;&#25512;&#26029;&#21644;/&#25110;&#35757;&#32451;&#26102;&#38598;&#25104;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#31181;&#24212;&#29992;&#39046;&#22495;&#23637;&#29616;&#20986;&#20102;&#20854;&#20248;&#21183;&#65292;&#28982;&#32780;&#65292;&#23427;&#20204;&#24448;&#24448;&#38590;&#20197;&#28385;&#36275;&#20854;&#36755;&#20986;&#30340;&#23433;&#20840;&#38656;&#27714;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;PiShield&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20801;&#35768;&#23558;&#38656;&#27714;&#38598;&#25104;&#21040;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#20013;&#30340;&#26694;&#26550;&#12290;PiShield&#30830;&#20445;&#28385;&#36275;&#36825;&#20123;&#38656;&#27714;&#65292;&#26080;&#35770;&#36755;&#20837;&#22914;&#20309;&#12290;&#27492;&#22806;&#65292;&#23427;&#20801;&#35768;&#26681;&#25454;&#20174;&#19994;&#32773;&#30340;&#38656;&#27714;&#22312;&#25512;&#26029;&#21644;/&#25110;&#35757;&#32451;&#26102;&#38598;&#25104;&#38656;&#27714;&#12290;&#37492;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#36843;&#20999;&#38656;&#35201;&#20801;&#35768;&#22312;&#21508;&#20010;&#39046;&#22495;&#38598;&#25104;&#38656;&#27714;&#30340;&#26694;&#26550;&#12290;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19977;&#20010;&#24212;&#29992;&#22330;&#26223;&#65306;&#21151;&#33021;&#22522;&#22240;&#32452;&#23398;&#12289;&#33258;&#21160;&#39550;&#39542;&#21644;&#34920;&#26684;&#25968;&#25454;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18285v1 Announce Type: cross  Abstract: Deep learning models have shown their strengths in various application domains, however, they often struggle to meet safety requirements for their outputs. In this paper, we introduce PiShield, the first framework ever allowing for the integration of the requirements into the neural networks' topology. PiShield guarantees compliance with these requirements, regardless of input. Additionally, it allows for integrating requirements both at inference and/or training time, depending on the practitioners' needs. Given the widespread application of deep learning, there is a growing need for frameworks allowing for the integration of the requirements across various domains. Here, we explore three application scenarios: functional genomics, autonomous driving, and tabular data generation.
&lt;/p&gt;</description></item><item><title>MOMENT&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#26102;&#38388;&#24207;&#21015;&#22522;&#30784;&#27169;&#22411;&#23478;&#26063;&#65292;&#36890;&#36807;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#32534;&#21046;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20844;&#20849;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;&#26377;&#38480;&#30417;&#30563;&#22330;&#26223;&#19979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03885</link><description>&lt;p&gt;
MOMENT&#65306;&#19968;&#20010;&#24320;&#25918;&#30340;&#26102;&#38388;&#24207;&#21015;&#22522;&#30784;&#27169;&#22411;&#23478;&#26063;
&lt;/p&gt;
&lt;p&gt;
MOMENT: A Family of Open Time-series Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03885
&lt;/p&gt;
&lt;p&gt;
MOMENT&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#26102;&#38388;&#24207;&#21015;&#22522;&#30784;&#27169;&#22411;&#23478;&#26063;&#65292;&#36890;&#36807;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#32534;&#21046;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20844;&#20849;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;&#26377;&#38480;&#30417;&#30563;&#22330;&#26223;&#19979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;MOMENT&#65292;&#19968;&#20010;&#24320;&#28304;&#30340;&#36890;&#29992;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#22522;&#30784;&#27169;&#22411;&#23478;&#26063;&#12290;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#39044;&#35757;&#32451;&#22823;&#27169;&#22411;&#26041;&#38754;&#23384;&#22312;&#30528;&#19968;&#20123;&#25361;&#25112;&#65292;&#21253;&#25324;&#65306;&#65288;1&#65289;&#32570;&#20047;&#19968;&#20010;&#22823;&#32780;&#26377;&#20957;&#32858;&#21147;&#30340;&#20844;&#20849;&#26102;&#38388;&#24207;&#21015;&#23384;&#20648;&#24211;&#65292;&#20197;&#21450;&#65288;2&#65289;&#22810;&#26679;&#30340;&#26102;&#38388;&#24207;&#21015;&#29305;&#24449;&#20351;&#24471;&#22810;&#25968;&#25454;&#38598;&#30340;&#35757;&#32451;&#21464;&#24471;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#23454;&#39564;&#35780;&#20272;&#26631;&#20934;&#65292;&#29305;&#21035;&#26159;&#22312;&#36164;&#28304;&#12289;&#26102;&#38388;&#21644;&#30417;&#30563;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#12290;&#65288;3&#65289;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#32534;&#21046;&#20102;&#19968;&#20010;&#22823;&#32780;&#22810;&#26679;&#30340;&#20844;&#20849;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#31216;&#20026;&#26102;&#38388;&#24207;&#21015;&#22534;&#65292;&#20197;&#31995;&#32479;&#22320;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#29305;&#23450;&#30340;&#25361;&#25112;&#65292;&#20197;&#35299;&#38145;&#22823;&#35268;&#27169;&#30340;&#22810;&#25968;&#25454;&#38598;&#39044;&#35757;&#32451;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20511;&#37492;&#26368;&#36817;&#30340;&#24037;&#20316;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;&#26377;&#38480;&#30417;&#30563;&#22330;&#26223;&#19979;&#26102;&#38388;&#24207;&#21015;&#22522;&#30784;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#25928;&#26524;&#12290;&#22312;&#36825;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#23569;&#37327;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce MOMENT, a family of open-source foundation models for general-purpose time-series analysis. Pre-training large models on time-series data is challenging due to (1) the absence of a large and cohesive public time-series repository, and (2) diverse time-series characteristics which make multi-dataset training onerous. Additionally, (3) experimental benchmarks to evaluate these models, especially in scenarios with limited resources, time, and supervision, are still in their nascent stages. To address these challenges, we compile a large and diverse collection of public time-series, called the Time-series Pile, and systematically tackle time-series-specific challenges to unlock large-scale multi-dataset pre-training. Finally, we build on recent work to design a benchmark to evaluate time-series foundation models on diverse tasks and datasets in limited supervision settings. Experiments on this benchmark demonstrate the effectiveness of our pre-trained models with minimal data 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24178;&#39044;&#25928;&#24212;&#22806;&#37096;&#26377;&#25928;&#24615;&#30340;&#32452;&#21512;&#23450;&#20041;&#65292;&#25581;&#31034;&#20102;&#25928;&#24212;&#27010;&#25324;&#30340;&#20004;&#20010;&#38480;&#21046;&#65292;&#24182;&#37325;&#26032;&#23457;&#35270;&#20102;&#21407;&#22987;&#21453;&#20107;&#23454;&#20844;&#24335;&#20013;&#30340;&#22810;&#20010;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2108.04376</link><description>&lt;p&gt;
&#21453;&#20107;&#23454;&#25928;&#24212;&#27010;&#25324;&#65306;&#19968;&#31181;&#32452;&#21512;&#23450;&#20041;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Effect Generalization: A Combinatorial Definition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2108.04376
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24178;&#39044;&#25928;&#24212;&#22806;&#37096;&#26377;&#25928;&#24615;&#30340;&#32452;&#21512;&#23450;&#20041;&#65292;&#25581;&#31034;&#20102;&#25928;&#24212;&#27010;&#25324;&#30340;&#20004;&#20010;&#38480;&#21046;&#65292;&#24182;&#37325;&#26032;&#23457;&#35270;&#20102;&#21407;&#22987;&#21453;&#20107;&#23454;&#20844;&#24335;&#20013;&#30340;&#22810;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#21453;&#20107;&#23454;&#8221;&#23450;&#20041;&#30340;&#22240;&#26524;&#25928;&#24212;&#24191;&#27867;&#29992;&#20110;&#20559;&#24046;&#21644;&#20934;&#30830;&#24615;&#30340;&#25512;&#23548;&#65292;&#20294;&#24182;&#38750;&#27867;&#21270;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24178;&#39044;&#25928;&#24212;&#22806;&#37096;&#26377;&#25928;&#24615;&#65288;EV&#65289;&#30340;&#32452;&#21512;&#23450;&#20041;&#12290;&#25105;&#20204;&#39318;&#20808;&#23450;&#20041;&#20102;&#19968;&#31181;&#25928;&#24212;&#35266;&#23519;&#8220;&#32972;&#26223;&#8221;&#30340;&#27010;&#24565;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#20854;&#65288;&#21487;&#35266;&#23519;&#21644;&#19981;&#21487;&#35266;&#23519;&#30340;&#65289;&#32972;&#26223;&#38598;&#21512;&#21046;&#23450;&#20102;&#25928;&#24212;&#27010;&#25324;&#30340;&#26465;&#20214;&#12290;&#36825;&#25581;&#31034;&#20102;&#25928;&#24212;&#27010;&#25324;&#30340;&#20004;&#20010;&#38480;&#21046;&#65306;&#65288;1&#65289;&#24403;&#25928;&#24212;&#22312;&#25152;&#26377;&#21487;&#20197;&#26522;&#20030;&#30340;&#32972;&#26223;&#19979;&#34987;&#35266;&#23519;&#26102;&#65292;&#25110;&#32773;&#65288;2&#65289;&#24403;&#32972;&#26223;&#21464;&#24471;&#36275;&#22815;&#38543;&#26426;&#26102;&#12290;&#25105;&#20204;&#21033;&#29992;&#30001;&#27492;&#20135;&#29983;&#30340;&#32452;&#21512;&#26694;&#26550;&#37325;&#26032;&#23457;&#35270;&#20102;&#21407;&#22987;&#21453;&#20107;&#23454;&#20844;&#24335;&#20013;&#30340;&#20960;&#20010;&#38382;&#39064;&#65306;&#26679;&#26412;&#22806;&#26377;&#25928;&#24615;&#12289;&#21516;&#26102;&#20272;&#35745;&#22810;&#20010;&#25928;&#24212;&#12289;&#20559;&#24046;-&#26041;&#24046;&#26435;&#34913;&#12289;&#32479;&#35745;&#21151;&#25928;&#20197;&#21450;&#19982;&#24403;&#21069;&#39044;&#27979;&#21644;&#35299;&#37322;&#25216;&#26415;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2108.04376v4 Announce Type: replace-cross  Abstract: The widely used 'Counterfactual' definition of Causal Effects was derived for unbiasedness and accuracy - and not generalizability. We propose a Combinatorial definition for the External Validity (EV) of intervention effects. We first define the concept of an effect observation 'background'. We then formulate conditions for effect generalization based on their sets of (observable and unobservable) backgrounds. This reveals two limits for effect generalization: (1) when effects are observed under all their enumerable backgrounds, or, (2) when backgrounds have become sufficiently randomized. We use the resulting combinatorial framework to re-examine several issues in the original counterfactual formulation: out-of-sample validity, concurrent estimation of multiple effects, bias-variance tradeoffs, statistical power, and connections to current predictive and explaining techniques.   Methodologically, the definitions also allow us 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#24182;&#34892;&#20195;&#30721;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#38598;PCGBench&#65292;&#24182;&#20351;&#29992;&#26032;&#25351;&#26631;&#35780;&#20272;&#20102;&#20960;&#20010;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#24182;&#34892;&#32534;&#31243;&#27169;&#22411;&#21644;&#35745;&#31639;&#38382;&#39064;&#31867;&#22411;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.12554</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#32534;&#20889;&#24182;&#34892;&#20195;&#30721;?
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Write Parallel Code?. (arXiv:2401.12554v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#24182;&#34892;&#20195;&#30721;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#38598;PCGBench&#65292;&#24182;&#20351;&#29992;&#26032;&#25351;&#26631;&#35780;&#20272;&#20102;&#20960;&#20010;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#24182;&#34892;&#32534;&#31243;&#27169;&#22411;&#21644;&#35745;&#31639;&#38382;&#39064;&#31867;&#22411;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27491;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#36719;&#20214;&#24320;&#21457;&#32773;&#30340;&#27426;&#36814;&#12290;&#23427;&#20204;&#22312;&#22810;&#31181;&#24773;&#22659;&#19979;&#23637;&#31034;&#20102;&#23545;&#28304;&#20195;&#30721;&#30340;&#24314;&#27169;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#21253;&#25324;&#20195;&#30721;&#34917;&#20840;&#12289;&#25688;&#35201;&#12289;&#32763;&#35793;&#21644;&#26597;&#25214;&#31561;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24448;&#24448;&#38590;&#20197;&#20026;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#29983;&#25104;&#20195;&#30721;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#24182;&#34892;&#20195;&#30721;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PCGBench&#30340;&#22522;&#20934;&#27979;&#35797;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;420&#20010;&#20219;&#21153;&#65292;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#24182;&#34892;&#20195;&#30721;&#30340;&#33021;&#21147;&#65292;&#24182;&#23545;&#20960;&#20010;&#26368;&#20808;&#36827;&#30340;&#24320;&#28304;&#21644;&#38381;&#28304;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20123;&#29992;&#20110;&#27604;&#36739;&#24182;&#34892;&#20195;&#30721;&#29983;&#25104;&#24615;&#33021;&#30340;&#26032;&#25351;&#26631;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#25351;&#26631;&#26469;&#25506;&#31350;&#27599;&#20010;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#30340;&#24182;&#34892;&#32534;&#31243;&#27169;&#22411;&#21644;&#35745;&#31639;&#38382;&#39064;&#31867;&#22411;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are becoming an increasingly popular tool for software development. Their ability to model and generate source code has been demonstrated in a variety of contexts, including code completion, summarization, translation, and lookup. However, they often struggle to generate code for more complex tasks. In this paper, we explore the ability of state-of-the-art language models to generate parallel code. We propose a benchmark, PCGBench, consisting of a set of 420 tasks for evaluating the ability of language models to generate parallel code, and we evaluate the performance of several state-of-the-art open- and closed-source language models on these tasks. We introduce novel metrics for comparing parallel code generation performance and use them to explore how well each LLM performs on various parallel programming models and computational problem types.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;&#30340;&#22522;&#30784;&#22270;&#27169;&#22411;FoToM&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#33410;&#28857;&#21644;&#36793;&#29305;&#24449;&#25490;&#38500;&#36827;&#34892;&#22270;&#39044;&#35757;&#32451;&#65292;&#22312;&#22810;&#20010;&#39046;&#22495;&#19978;&#23454;&#29616;&#20102;&#27491;&#21521;&#36801;&#31227;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2311.03976</link><description>&lt;p&gt;
&#19968;&#20010;&#22522;&#30784;&#22270;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Foundation Graph Model. (arXiv:2311.03976v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.03976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;&#30340;&#22522;&#30784;&#22270;&#27169;&#22411;FoToM&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#33410;&#28857;&#21644;&#36793;&#29305;&#24449;&#25490;&#38500;&#36827;&#34892;&#22270;&#39044;&#35757;&#32451;&#65292;&#22312;&#22810;&#20010;&#39046;&#22495;&#19978;&#23454;&#29616;&#20102;&#27491;&#21521;&#36801;&#31227;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#20027;&#35201;&#20248;&#21183;&#26159;&#22312;&#25968;&#25454;&#25110;&#26631;&#31614;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#26159;&#38024;&#23545;&#29305;&#23450;&#39046;&#22495;&#30340;&#65292;&#20445;&#25345;&#39044;&#35757;&#32451;&#21644;&#30446;&#26631;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#33410;&#28857;&#21644;&#36793;&#23646;&#24615;&#19968;&#33268;&#12290;&#36825;&#20351;&#24471;&#26080;&#27861;&#22312;&#20854;&#20182;&#39046;&#22495;&#36827;&#34892;&#36801;&#31227;&#12290;&#33021;&#22815;&#22312;&#20219;&#24847;&#20219;&#21153;&#21644;&#39046;&#22495;&#19978;&#23454;&#29616;&#27491;&#21521;&#36801;&#31227;&#30340;&#27169;&#22411;&#23558;&#25104;&#20026;&#31532;&#19968;&#20010;&#22522;&#30784;&#22270;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;&#25552;&#20986;&#20102;FoToM&#65292;&#19968;&#31181;&#22522;&#20110;&#33410;&#28857;&#21644;&#36793;&#29305;&#24449;&#25490;&#38500;&#30340;&#22270;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;FoToM&#22312;&#22810;&#20010;&#22270;&#39046;&#22495;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24471;&#21040;&#20102;&#31532;&#19968;&#20010;&#22522;&#30784;&#22270;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;&#22810;&#20010;&#39046;&#22495;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#27491;&#21521;&#36801;&#31227;&#12290;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#19978;&#65292;&#24615;&#33021;&#26368;&#24046;&#26102;&#19982;&#26377;&#30417;&#30563;&#22522;&#32447;&#30456;&#24403;&#65292;76%&#30340;&#25968;&#25454;&#38598;&#22312;95%&#32622;&#20449;&#24230;&#19979;&#37117;&#26174;&#33879;&#20248;&#20110;&#26377;&#30417;&#30563;&#22522;&#32447;&#65288;P&#8804;0.01&#65289;&#65292;&#35823;&#24046;&#20943;&#23569;&#20102;8%&#33267;40%&#12290;
&lt;/p&gt;
&lt;p&gt;
The principal benefit of unsupervised graph representation learning is that a pre-trained model can be fine-tuned where data or labels are scarce. Existing approaches are domain specific, maintaining consistent node and edge attributes across the pre-training and target datasets. This precludes transfer to other domains. A model capable of positive transfer on arbitrary tasks and domains would represent the first foundation graph model.  In this work we use adversarial contrastive learning to present FoToM, a graph pre-training method based on node and edge feature exclusion. We use FoToM to pre-train models over multiple graph domains, producing the first foundation graph models. We demonstrate positive transfer on evaluation datasets from multiple domains, including domains not present in pre-training data. On all datasets performance is at worst on-par and on 76% significantly better than a supervised baseline ($P \leq 0.01$), with an 8 to 40% reduction in error at 95% confidence. C
&lt;/p&gt;</description></item><item><title>ChatGPT&#22312;&#36873;&#25321;&#31572;&#26696;&#26102;&#34920;&#29616;&#20986;&#21021;&#21360;&#35937;&#25928;&#24212;&#65292;&#21363;&#26356;&#20542;&#21521;&#20110;&#36873;&#25321;&#25552;&#31034;&#20013;&#36739;&#26089;&#20301;&#32622;&#30340;&#26631;&#31614;&#20316;&#20026;&#31572;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.13206</link><description>&lt;p&gt;
ChatGPT&#30340;&#21021;&#21360;&#35937;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Primacy Effect of ChatGPT. (arXiv:2310.13206v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13206
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#22312;&#36873;&#25321;&#31572;&#26696;&#26102;&#34920;&#29616;&#20986;&#21021;&#21360;&#35937;&#25928;&#24212;&#65292;&#21363;&#26356;&#20542;&#21521;&#20110;&#36873;&#25321;&#25552;&#31034;&#20013;&#36739;&#26089;&#20301;&#32622;&#30340;&#26631;&#31614;&#20316;&#20026;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;ChatGPT&#19978;&#36827;&#34892;&#23454;&#39564;&#21644;&#20998;&#26512;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;ChatGPT&#30340;&#21021;&#21360;&#35937;&#25928;&#24212;&#65292;&#21363;&#36873;&#25321;&#36739;&#26089;&#20301;&#32622;&#30340;&#26631;&#31614;&#20316;&#20026;&#31572;&#26696;&#30340;&#20542;&#21521;&#12290;&#25105;&#20204;&#21457;&#29616;&#65306;i&#65289;ChatGPT&#30340;&#20915;&#31574;&#23545;&#25552;&#31034;&#20013;&#26631;&#31614;&#30340;&#39034;&#24207;&#25935;&#24863;&#65307;ii&#65289;ChatGPT&#26356;&#20542;&#21521;&#20110;&#36873;&#25321;&#36739;&#26089;&#20301;&#32622;&#30340;&#26631;&#31614;&#20316;&#20026;&#31572;&#26696;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#33021;&#20026;&#26500;&#24314;&#26356;&#21487;&#38752;&#30340;&#22522;&#20110;ChatGPT&#30340;&#35299;&#20915;&#26041;&#26696;&#25552;&#20379;&#39069;&#22806;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned large language models (LLMs), such as ChatGPT, have led to promising zero-shot performance in discriminative natural language understanding (NLU) tasks. This involves querying the LLM using a prompt containing the question, and the candidate labels to choose from. The question-answering capabilities of ChatGPT arise from its pre-training on large amounts of human-written text, as well as its subsequent fine-tuning on human preferences, which motivates us to ask: Does ChatGPT also inherits humans' cognitive biases? In this paper, we study the primacy effect of ChatGPT: the tendency of selecting the labels at earlier positions as the answer. We have two main findings: i) ChatGPT's decision is sensitive to the order of labels in the prompt; ii) ChatGPT has a clearly higher chance to select the labels at earlier positions as the answer. We hope that our experiments and analyses provide additional insights into building more reliable ChatGPT-based solutions. We release the
&lt;/p&gt;</description></item><item><title>RealFill&#26159;&#19968;&#31181;&#20010;&#24615;&#21270;&#30340;&#29983;&#25104;&#20462;&#22635;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#30446;&#26631;&#22330;&#26223;&#30340;&#21442;&#32771;&#22270;&#20687;&#65292;&#33021;&#22815;&#20197;&#30495;&#23454;&#12289;&#39640;&#36136;&#37327;&#12289;&#36924;&#30495;&#30340;&#20869;&#23481;&#23436;&#25104;&#30446;&#26631;&#22270;&#20687;&#30340;&#20462;&#22797;&#12290;</title><link>http://arxiv.org/abs/2309.16668</link><description>&lt;p&gt;
RealFill&#65306;&#21442;&#32771;&#39537;&#21160;&#30340;&#30495;&#23454;&#22270;&#20687;&#20462;&#22797;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RealFill: Reference-Driven Generation for Authentic Image Completion. (arXiv:2309.16668v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16668
&lt;/p&gt;
&lt;p&gt;
RealFill&#26159;&#19968;&#31181;&#20010;&#24615;&#21270;&#30340;&#29983;&#25104;&#20462;&#22635;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#30446;&#26631;&#22330;&#26223;&#30340;&#21442;&#32771;&#22270;&#20687;&#65292;&#33021;&#22815;&#20197;&#30495;&#23454;&#12289;&#39640;&#36136;&#37327;&#12289;&#36924;&#30495;&#30340;&#20869;&#23481;&#23436;&#25104;&#30446;&#26631;&#22270;&#20687;&#30340;&#20462;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#29983;&#25104;&#22270;&#20687;&#30340;&#36827;&#23637;&#24102;&#26469;&#20102;&#33021;&#22815;&#22312;&#26410;&#30693;&#21306;&#22495;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#36924;&#30495;&#22270;&#20687;&#20869;&#23481;&#30340;&#22806;&#25299;&#21644;&#20462;&#22635;&#27169;&#22411;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#20135;&#29983;&#30340;&#20869;&#23481;&#26159;&#19981;&#30495;&#23454;&#30340;&#65292;&#22240;&#20026;&#27169;&#22411;&#32570;&#20047;&#20851;&#20110;&#30495;&#23454;&#22330;&#26223;&#30340;&#36275;&#22815;&#32972;&#26223;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30495;&#23454;&#22270;&#20687;&#20462;&#22797;&#29983;&#25104;&#26041;&#27861;RealFill&#65292;&#23427;&#36890;&#36807;&#22635;&#20805;&#22270;&#20687;&#20013;&#32570;&#22833;&#21306;&#22495;&#20351;&#20854;&#20869;&#23481;&#30495;&#27491;&#24212;&#22312;&#30340;&#20869;&#23481;&#12290;RealFill&#26159;&#19968;&#31181;&#20010;&#24615;&#21270;&#30340;&#29983;&#25104;&#20462;&#22635;&#27169;&#22411;&#65292;&#20165;&#20351;&#29992;&#20960;&#24352;&#30446;&#26631;&#22330;&#26223;&#30340;&#21442;&#32771;&#22270;&#20687;&#36827;&#34892;&#20010;&#24615;&#21270;&#12290;&#36825;&#20123;&#21442;&#32771;&#22270;&#20687;&#19981;&#38656;&#35201;&#19982;&#30446;&#26631;&#22270;&#20687;&#23545;&#40784;&#65292;&#21487;&#20197;&#36890;&#36807;&#19981;&#21516;&#30340;&#35270;&#35282;&#12289;&#20809;&#29031;&#26465;&#20214;&#12289;&#25668;&#20687;&#26426;&#20809;&#22280;&#25110;&#22270;&#20687;&#39118;&#26684;&#25293;&#25668;&#12290;&#20010;&#24615;&#21270;&#21518;&#65292;RealFill&#33021;&#22815;&#20197;&#35270;&#35273;&#19978;&#24341;&#20154;&#27880;&#30446;&#30340;&#20869;&#23481;&#23436;&#25104;&#30446;&#26631;&#22270;&#20687;&#65292;&#24182;&#19988;&#24544;&#23454;&#20110;&#21407;&#22987;&#22330;&#26223;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#20840;&#38754;&#19988;&#20855;&#25361;&#25112;&#24615;&#30340;&#22270;&#20687;&#20462;&#22797;&#22522;&#20934;&#19978;&#23545;RealFill&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in generative imagery have brought forth outpainting and inpainting models that can produce high-quality, plausible image content in unknown regions, but the content these models hallucinate is necessarily inauthentic, since the models lack sufficient context about the true scene. In this work, we propose RealFill, a novel generative approach for image completion that fills in missing regions of an image with the content that should have been there. RealFill is a generative inpainting model that is personalized using only a few reference images of a scene. These reference images do not have to be aligned with the target image, and can be taken with drastically varying viewpoints, lighting conditions, camera apertures, or image styles. Once personalized, RealFill is able to complete a target image with visually compelling contents that are faithful to the original scene. We evaluate RealFill on a new image completion benchmark that covers a set of diverse and challenging
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#24179;&#28369;&#38750;&#20984;&#20248;&#21270;&#20013;&#38543;&#26426;&#27425;&#26799;&#24230;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#36136;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#21333;&#26102;&#38388;&#23610;&#24230;&#21644;&#21452;&#26102;&#38388;&#23610;&#24230;&#24773;&#20917;&#19979;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#65292;&#21253;&#25324;&#20102;&#22810;&#31181;&#24050;&#30693;&#30340;SGD&#31867;&#22411;&#26041;&#27861;&#12290;&#23545;&#20110;&#26377;&#38480;&#21644;&#24418;&#24335;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#22312;&#38543;&#26426;&#36873;&#25321;&#30340;&#27493;&#38271;&#21644;&#21021;&#22987;&#28857;&#19978;&#25214;&#21040;Clarke&#31283;&#23450;&#28857;&#12290;</title><link>http://arxiv.org/abs/2307.10053</link><description>&lt;p&gt;
&#38750;&#24179;&#28369;&#38750;&#20984;&#20248;&#21270;&#20013;&#38543;&#26426;&#27425;&#26799;&#24230;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Convergence Guarantees for Stochastic Subgradient Methods in Nonsmooth Nonconvex Optimization. (arXiv:2307.10053v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#24179;&#28369;&#38750;&#20984;&#20248;&#21270;&#20013;&#38543;&#26426;&#27425;&#26799;&#24230;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#36136;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#21333;&#26102;&#38388;&#23610;&#24230;&#21644;&#21452;&#26102;&#38388;&#23610;&#24230;&#24773;&#20917;&#19979;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#65292;&#21253;&#25324;&#20102;&#22810;&#31181;&#24050;&#30693;&#30340;SGD&#31867;&#22411;&#26041;&#27861;&#12290;&#23545;&#20110;&#26377;&#38480;&#21644;&#24418;&#24335;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#22312;&#38543;&#26426;&#36873;&#25321;&#30340;&#27493;&#38271;&#21644;&#21021;&#22987;&#28857;&#19978;&#25214;&#21040;Clarke&#31283;&#23450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#26041;&#27861;&#21450;&#20854;&#21464;&#31181;&#22312;&#35757;&#32451;&#30001;&#38750;&#24179;&#28369;&#28608;&#27963;&#20989;&#25968;&#26500;&#24314;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#25910;&#25947;&#24615;&#36136;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#20026;&#26356;&#26032;&#21160;&#37327;&#39033;&#21644;&#21464;&#37327;&#30340;&#27493;&#38271;&#20998;&#37197;&#20102;&#19981;&#21516;&#30340;&#26102;&#38388;&#23610;&#24230;&#12290;&#22312;&#19968;&#20123;&#28201;&#21644;&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#21333;&#26102;&#38388;&#23610;&#24230;&#21644;&#21452;&#26102;&#38388;&#23610;&#24230;&#24773;&#20917;&#19979;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#21253;&#21547;&#20102;&#24456;&#22810;&#24050;&#30693;&#30340;SGD&#31867;&#22411;&#26041;&#27861;&#65292;&#21253;&#25324;heavy-ball SGD&#12289;SignSGD&#12289;Lion&#12289;normalized SGD&#21644;clipped SGD&#12290;&#27492;&#22806;&#65292;&#24403;&#30446;&#26631;&#20989;&#25968;&#37319;&#29992;&#26377;&#38480;&#21644;&#24418;&#24335;&#26102;&#65292;&#25105;&#20204;&#22522;&#20110;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#35777;&#26126;&#20102;&#36825;&#20123;SGD&#31867;&#22411;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#36136;&#12290;&#29305;&#21035;&#22320;&#65292;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;SGD&#31867;&#22411;&#26041;&#27861;&#22312;&#38543;&#26426;&#36873;&#25321;&#30340;&#27493;&#38271;&#21644;&#21021;&#22987;&#28857;&#19978;&#33021;&#22815;&#25214;&#21040;&#30446;&#26631;&#20989;&#25968;&#30340;Clarke&#31283;&#23450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the convergence properties of the stochastic gradient descent (SGD) method and its variants, especially in training neural networks built from nonsmooth activation functions. We develop a novel framework that assigns different timescales to stepsizes for updating the momentum terms and variables, respectively. Under mild conditions, we prove the global convergence of our proposed framework in both single-timescale and two-timescale cases. We show that our proposed framework encompasses a wide range of well-known SGD-type methods, including heavy-ball SGD, SignSGD, Lion, normalized SGD and clipped SGD. Furthermore, when the objective function adopts a finite-sum formulation, we prove the convergence properties for these SGD-type methods based on our proposed framework. In particular, we prove that these SGD-type methods find the Clarke stationary points of the objective function with randomly chosen stepsizes and initial points under mild assumptions. Preli
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#39640;&#24615;&#33021;&#35745;&#31639;&#21644;&#31185;&#23398;&#20195;&#30721;&#30340;&#24320;&#21457;&#21644;&#20998;&#26512;&#20013;&#33258;&#21160;&#21270;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.17281</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24314;&#27169;&#24182;&#34892;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
Modeling Parallel Programs using Large Language Models. (arXiv:2306.17281v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17281
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#39640;&#24615;&#33021;&#35745;&#31639;&#21644;&#31185;&#23398;&#20195;&#30721;&#30340;&#24320;&#21457;&#21644;&#20998;&#26512;&#20013;&#33258;&#21160;&#21270;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25105;&#20204;&#36827;&#20837;&#24322;&#26500;&#35745;&#31639;&#26102;&#20195;&#65292;&#39640;&#24615;&#33021;&#35745;&#31639;&#65288;HPC&#65289;&#20013;&#30340;&#24182;&#34892;&#36719;&#20214;&#20195;&#30721;&#22312;&#22797;&#26434;&#24615;&#21644;&#35268;&#27169;&#19978;&#19981;&#26029;&#22686;&#38271;&#12290;&#26032;&#20852;&#30340;&#30828;&#20214;&#21644;&#32534;&#31243;&#33539;&#24335;&#20351;&#24471;&#24320;&#21457;&#12289;&#20248;&#21270;&#21644;&#32500;&#25252;&#24182;&#34892;&#36719;&#20214;&#23545;&#24320;&#21457;&#20154;&#21592;&#26469;&#35828;&#21464;&#24471;&#32321;&#37325;&#12290;&#32531;&#35299;&#36825;&#20123;&#36127;&#25285;&#30340;&#19968;&#31181;&#26041;&#24335;&#26159;&#20351;&#29992;&#33258;&#21160;&#21270;&#24320;&#21457;&#21644;&#20998;&#26512;&#24037;&#20855;&#12290;&#36825;&#20123;&#24037;&#20855;&#21487;&#20197;&#20026;&#24320;&#21457;&#20154;&#21592;&#25191;&#34892;&#22797;&#26434;&#21644;/&#25110;&#34917;&#25937;&#24615;&#30340;&#20219;&#21153;&#65292;&#25552;&#39640;&#20182;&#20204;&#30340;&#29983;&#20135;&#21147;&#24182;&#20943;&#23569;&#38169;&#35823;&#30340;&#21487;&#33021;&#24615;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#29992;&#20110;&#20195;&#30721;&#24320;&#21457;&#21644;&#24615;&#33021;&#20998;&#26512;&#30340;&#36825;&#20123;&#24037;&#20855;&#22312;&#25191;&#34892;&#20219;&#21153;&#30340;&#22797;&#26434;&#24230;&#26041;&#38754;&#21463;&#21040;&#38480;&#21046;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#35821;&#35328;&#24314;&#27169;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#29616;&#22312;&#22312;&#32447;&#19978;&#21487;&#29992;&#30340;&#22823;&#37327;&#19982;&#20195;&#30721;&#30456;&#20851;&#30340;&#25968;&#25454;&#65292;&#36825;&#20123;&#24037;&#20855;&#24320;&#22987;&#21033;&#29992;&#39044;&#27979;&#24615;&#35821;&#35328;&#27169;&#22411;&#26469;&#33258;&#21160;&#23436;&#25104;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24212;&#29992;&#20110;&#39640;&#24615;&#33021;&#21644;&#31185;&#23398;&#20195;&#30721;&#30340;&#29305;&#23450;&#20219;&#21153;&#20013;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;LLM&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Parallel software codes in high performance computing (HPC) continue to grow in complexity and scale as we enter the exascale era. A diverse set of emerging hardware and programming paradigms make developing, optimizing, and maintaining parallel software burdensome for developers. One way to alleviate some of these burdens is with automated development and analysis tools. Such tools can perform complex and/or remedial tasks for developers that increase their productivity and decrease the chance for error. So far, such tools for code development and performance analysis have been limited in the complexity of tasks they can perform. However, with recent advancements in language modeling, and the wealth of code related data that is now available online, these tools have started to utilize predictive language models to automate more complex tasks. In this paper, we show how large language models (LLMs) can be applied to tasks specific to high performance and scientific codes. We train LLMs
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Tensor3D&#65292;&#19968;&#31181;&#26368;&#23567;&#21270;&#36890;&#20449;&#28040;&#32791;&#30340;&#19977;&#32500;&#24352;&#37327;&#35745;&#31639;&#24182;&#34892;&#21270;&#26041;&#27861;&#12290;&#23427;&#21033;&#29992;&#26234;&#33021;&#20998;&#24067;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#12289;&#26032;&#39062;&#36229;&#20998;&#35299;&#26041;&#27861;&#20197;&#21450;&#36890;&#20449;&#27169;&#22411;&#65292;&#20351;&#35757;&#32451;&#36895;&#24230;&#25552;&#39640;&#20102;&#32422;3&#20493;&#65292;GPU&#31354;&#38386;&#26102;&#38388;&#38477;&#20302;&#20102;50&#65285;&#20197;&#19978;&#12290;</title><link>http://arxiv.org/abs/2305.13525</link><description>&lt;p&gt;
&#26368;&#23567;&#21270;&#36890;&#20449;&#30340;&#24322;&#27493;&#24352;&#37327;&#24182;&#34892;&#24615;
&lt;/p&gt;
&lt;p&gt;
Communication-minimizing Asynchronous Tensor Parallelism. (arXiv:2305.13525v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Tensor3D&#65292;&#19968;&#31181;&#26368;&#23567;&#21270;&#36890;&#20449;&#28040;&#32791;&#30340;&#19977;&#32500;&#24352;&#37327;&#35745;&#31639;&#24182;&#34892;&#21270;&#26041;&#27861;&#12290;&#23427;&#21033;&#29992;&#26234;&#33021;&#20998;&#24067;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#12289;&#26032;&#39062;&#36229;&#20998;&#35299;&#26041;&#27861;&#20197;&#21450;&#36890;&#20449;&#27169;&#22411;&#65292;&#20351;&#35757;&#32451;&#36895;&#24230;&#25552;&#39640;&#20102;&#32422;3&#20493;&#65292;GPU&#31354;&#38386;&#26102;&#38388;&#38477;&#20302;&#20102;50&#65285;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#35268;&#27169;&#25193;&#22823;&#21040;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#65292;&#35774;&#35745;&#33021;&#22815;&#22312;&#22810;GPU&#38598;&#32676;&#19978;&#39640;&#25928;&#35757;&#32451;&#36825;&#20123;&#32593;&#32476;&#30340;&#24182;&#34892;&#31639;&#27861;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Tensor3D&#65292;&#19968;&#31181;&#20840;&#26032;&#30340;&#19977;&#32500;&#65288;3D&#65289;&#24352;&#37327;&#35745;&#31639;&#24182;&#34892;&#21270;&#26041;&#27861;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#22823;&#22411;&#22810;&#21313;&#20159;&#21442;&#25968;&#27169;&#22411;&#30340;&#24182;&#34892;&#35757;&#32451;&#20013;&#30001;&#36890;&#20449;&#24341;&#36215;&#30340;&#31354;&#38386;&#26102;&#38388;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26234;&#33021;&#30340;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#20998;&#24067;&#26041;&#24335;&#65292;&#28040;&#38500;&#20102;&#20026;&#28385;&#36275;&#21508;&#23618;&#25968;&#25454;&#20381;&#36182;&#32780;&#38656;&#35201;&#30340;&#36890;&#20449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24182;&#34892;&#35757;&#32451;&#36807;&#31243;&#36229;&#20998;&#35299;&#26041;&#27861;&#65292;&#21033;&#29992;&#23427;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#36890;&#20449;&#19982;&#35745;&#31639;&#30340;&#37325;&#21472;&#24230;&#65292;&#20174;&#32780;&#20943;&#23569;GPU&#31354;&#38386;&#26102;&#38388;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#20449;&#27169;&#22411;&#65292;&#24110;&#21161;&#29992;&#25143;&#20026;&#32473;&#23450;&#30340;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;&#36890;&#20449;&#26368;&#20248;&#30340;&#21487;&#29992;&#30828;&#20214;&#36164;&#28304;&#20998;&#35299;&#12290; &#23545;&#20110;256 A100 GPU&#19978;&#30340;28B&#21442;&#25968;CNN&#65292;&#22312;&#26412;&#25991;&#30340; Tensor3D &#26041;&#27861;&#19979;&#65292;&#35757;&#32451;&#36895;&#24230;&#25552;&#39640;&#20102;&#32422;3&#20493;&#65292;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604; GPU &#31354;&#38386;&#26102;&#38388;&#20063;&#38477;&#20302;&#20102;&#32422;50&#65285;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
As state-of-the-art neural networks scale to billions of parameters, designing parallel algorithms that can train these networks efficiently on multi-GPU clusters has become critical. This paper presents Tensor3D, a novel three-dimensional (3D) approach to parallelize tensor computations, that strives to minimize the idle time incurred due to communication in parallel training of large multi-billion parameter models. First, we introduce an intelligent distribution of neural network parameters across GPUs that eliminates communication required for satisfying data dependencies of individual layers. Then, we propose a novel overdecomposition of the parallel training process, using which we achieve significant overlap of communication with computation, thereby reducing GPU idle time. Finally, we present a communication model, which helps users identify communication optimal decompositions of available hardware resources for a given neural network. For a 28B parameter CNN on 256 A100 GPUs, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#30340;&#21313;&#20159;&#32423;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#30740;&#31350;&#20102;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#23545;&#35813;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#24433;&#21709;&#65292;&#23454;&#39564;&#26174;&#31034;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.05215</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#30340;&#21313;&#20159;&#32423;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Billion-scale Foundation Model for Remote Sensing Images. (arXiv:2304.05215v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#30340;&#21313;&#20159;&#32423;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#30740;&#31350;&#20102;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#23545;&#35813;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#24433;&#21709;&#65292;&#23454;&#39564;&#26174;&#31034;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#30784;&#27169;&#22411;&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20808;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#24050;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#27493;&#39588;&#12290;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#30340;&#19977;&#20010;&#20851;&#38190;&#22240;&#32032;&#26159;&#39044;&#35757;&#32451;&#26041;&#27861;&#12289;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#20197;&#21450;&#27169;&#22411;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;&#26368;&#36817;&#65292;&#36965;&#24863;&#39046;&#22495;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#39044;&#35757;&#32451;&#26041;&#27861;&#21644;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#65292;&#23545;&#27169;&#22411;&#21442;&#25968;&#30340;&#25968;&#37327;&#20851;&#27880;&#36739;&#23569;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#23545;&#22522;&#30784;&#27169;&#22411;&#22312;&#26059;&#36716;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#31561;&#19979;&#28216;&#20219;&#21153;&#20013;&#24615;&#33021;&#30340;&#24433;&#21709;&#26469;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#25968;&#37327;&#21442;&#25968;&#65288;&#21253;&#25324;86M&#12289;605.26M&#12289;1.3B&#21644;2.4B&#65289;&#30340;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20197;&#30830;&#23450;&#21442;&#25968;&#22686;&#21152;&#26159;&#21542;&#20250;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#30340;&#21313;&#20159;&#32423;&#22522;&#30784;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;10&#20159;&#20010;&#36965;&#24863;&#22270;&#20687;&#30340;&#26032;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#24182;&#21521;&#30740;&#31350;&#31038;&#21306;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the potential of foundation models in visual tasks has garnered significant attention, pretraining these models before downstream tasks has become a crucial step. The three key factors in pretraining foundation models are the pretraining method, the size of the pretraining dataset, and the number of model parameters. Recently, research in the remote sensing field has focused primarily on the pretraining method and the size of the dataset, with limited emphasis on the number of model parameters. This paper addresses this gap by examining the effect of increasing the number of model parameters on the performance of foundation models in downstream tasks such as rotated object detection and semantic segmentation. We pretrained foundation models with varying numbers of parameters, including 86M, 605.26M, 1.3B, and 2.4B, to determine whether performance in downstream tasks improved with an increase in parameters. To the best of our knowledge, this is the first billion-scale foundation mod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#36827;&#21270;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#39640;&#25928;&#19988;&#28789;&#27963;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24182;&#20248;&#21270;&#30456;&#20851;&#30340;&#36229;&#21442;&#25968;&#12290;&#27492;&#26694;&#26550;&#21487;&#29992;&#20110;&#20219;&#20309;&#33021;&#22815;&#22788;&#29702;&#28151;&#21512;&#25628;&#32034;&#31354;&#38388;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#24182;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#27604;&#24050;&#26377;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.12797</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#31639;&#27861;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An algorithmic framework for the optimization of deep neural networks architectures and hyperparameters. (arXiv:2303.12797v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12797
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#36827;&#21270;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#39640;&#25928;&#19988;&#28789;&#27963;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24182;&#20248;&#21270;&#30456;&#20851;&#30340;&#36229;&#21442;&#25968;&#12290;&#27492;&#26694;&#26550;&#21487;&#29992;&#20110;&#20219;&#20309;&#33021;&#22815;&#22788;&#29702;&#28151;&#21512;&#25628;&#32034;&#31354;&#38388;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#24182;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#27604;&#24050;&#26377;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#39640;&#25928;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24182;&#20248;&#21270;&#30456;&#20851;&#30340;&#36229;&#21442;&#25968;&#12290;&#35813;&#26694;&#26550;&#22522;&#20110;&#36827;&#21270;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;(DAG)&#65292;&#23450;&#20041;&#20102;&#27604;&#25991;&#29486;&#20013;&#29616;&#26377;&#30340;&#25628;&#32034;&#31354;&#38388;&#26356;&#20026;&#28789;&#27963;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#20801;&#35768;&#28151;&#21512;&#20351;&#29992;&#20256;&#32479;&#25805;&#20316;&#65292;&#22914;&#21367;&#31215;&#12289;&#24490;&#29615;&#21644;&#23494;&#38598;&#23618;&#65292;&#20197;&#21450;&#36739;&#20026;&#26032;&#39062;&#30340;&#25805;&#20316;&#65292;&#22914;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#22522;&#20110;&#35813;&#25628;&#32034;&#31354;&#38388;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#37051;&#22495;&#25628;&#32034;&#31639;&#23376;&#21644;&#28436;&#21270;&#25628;&#32034;&#31639;&#23376;&#65292;&#20197;&#20248;&#21270;&#32593;&#32476;&#30340;&#26550;&#26500;&#21644;&#36229;&#21442;&#25968;&#12290;&#36825;&#20123;&#25628;&#32034;&#31639;&#23376;&#21487;&#19982;&#20219;&#20309;&#33021;&#22815;&#22788;&#29702;&#28151;&#21512;&#25628;&#32034;&#31354;&#38388;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#19968;&#36215;&#20351;&#29992;&#12290;&#25105;&#20204;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#36827;&#21270;&#31639;&#27861;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#26694;&#26550;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#25214;&#21040;&#22312;&#35768;&#22810;&#25968;&#25454;&#38598;&#19978;&#24615;&#33021;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose an algorithmic framework to automatically generate efficient deep neural networks and optimize their associated hyperparameters. The framework is based on evolving directed acyclic graphs (DAGs), defining a more flexible search space than the existing ones in the literature. It allows mixtures of different classical operations: convolutions, recurrences and dense layers, but also more newfangled operations such as self-attention. Based on this search space we propose neighbourhood and evolution search operators to optimize both the architecture and hyper-parameters of our networks. These search operators can be used with any metaheuristic capable of handling mixed search spaces. We tested our algorithmic framework with an evolutionary algorithm on a time series prediction benchmark. The results demonstrate that our framework was able to find models outperforming the established baseline on numerous datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;TPTND&#30340;&#27010;&#29575;&#31867;&#22411;&#33258;&#28982;&#28436;&#31639;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#26816;&#39564;&#24182;&#25512;&#23548;&#27010;&#29575;&#35745;&#31639;&#36807;&#31243;&#30340;&#21487;&#20449;&#24615;&#65292;&#20855;&#26377;&#21487;&#26816;&#26597;&#24615;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2206.12934</link><description>&lt;p&gt;
&#22312;&#31867;&#22411;&#21270;&#33258;&#28982;&#28436;&#31639;&#31995;&#32479;&#20013;&#26816;&#39564;&#27010;&#29575;&#35745;&#31639;&#30340;&#21487;&#20449;&#24615;
&lt;/p&gt;
&lt;p&gt;
Checking Trustworthiness of Probabilistic Computations in a Typed Natural Deduction System. (arXiv:2206.12934v2 [cs.LO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.12934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;TPTND&#30340;&#27010;&#29575;&#31867;&#22411;&#33258;&#28982;&#28436;&#31639;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#26816;&#39564;&#24182;&#25512;&#23548;&#27010;&#29575;&#35745;&#31639;&#36807;&#31243;&#30340;&#21487;&#20449;&#24615;&#65292;&#20855;&#26377;&#21487;&#26816;&#26597;&#24615;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026; TPTND &#30340;&#27010;&#29575;&#31867;&#22411;&#33258;&#28982;&#28436;&#31639;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#26088;&#22312;&#25512;&#23548;&#26377;&#20851;&#27010;&#29575;&#35745;&#31639;&#36807;&#31243;&#30340;&#21487;&#20449;&#24615;&#23646;&#24615;&#65292;&#20363;&#22914;&#24403;&#20170;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#37027;&#20123;&#23646;&#24615;&#12290;TPTND &#20013;&#30340;&#25512;&#23548;&#34987;&#35299;&#37322;&#20026;&#20174;&#32473;&#23450;&#30340;&#20998;&#31867;&#20998;&#24067;&#20013;&#25552;&#21462; n &#20010;&#21487;&#33021;&#22797;&#26434;&#36755;&#20986;&#26679;&#26412;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#36755;&#20986;&#26679;&#26412;&#30340;&#21487;&#20449;&#24615;&#24418;&#24335;&#21270;&#20026;&#19968;&#31181;&#20551;&#35774;&#27979;&#35797;&#65292;&#21363;&#35745;&#31639;&#20986;&#29616;&#26377;&#30340;&#39057;&#29575;&#19982;&#39044;&#26399;&#30340;&#27010;&#29575;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#36825;&#20010;&#28436;&#31639;&#31995;&#32479;&#30340;&#20027;&#35201;&#20248;&#21183;&#22312;&#20110;&#33021;&#22815;&#26816;&#26597;&#36825;&#31181;&#21487;&#20449;&#24615;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#20026;&#25512;&#29702;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#39033;&#25552;&#20379;&#20102;&#35745;&#31639;&#35821;&#20041;&#65292;&#24182;&#23450;&#20041;&#20102;&#36923;&#36753;&#36816;&#31639;&#31526;&#20197;&#21450;&#20449;&#20219;&#36816;&#31639;&#31526;&#30340;&#24341;&#20837;&#21644;&#28040;&#35299;&#35268;&#21017;&#12290;&#25105;&#20204;&#37325;&#28857;&#20171;&#32461;&#20102;&#31995;&#32479;&#30340;&#32467;&#26500;&#21644;&#20803;&#29702;&#35770;&#23646;&#24615;&#65292;&#23588;&#20854;&#26159;&#33021;&#22815;&#30830;&#23450;&#21738;&#20123;&#39033;&#28436;&#21270;&#21644;&#36923;&#36753;&#35268;&#21017;&#24212;&#29992;&#26102;&#65292;&#35745;&#31639;&#20173;&#28982;&#26159;&#21487;&#20449;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we present the probabilistic typed natural deduction calculus TPTND, designed to reason about and derive trustworthiness properties of probabilistic computational processes, like those underlying current AI applications. Derivability in TPTND is interpreted as the process of extracting $n$ samples of possibly complex outputs with a certain frequency from a given categorical distribution. We formalize trust for such outputs as a form of hypothesis testing on the distance between such frequency and the intended probability. The main advantage of the calculus is to render such notion of trustworthiness checkable. We present a computational semantics for the terms over which we reason and then the semantics of TPTND, where logical operators as well as a Trust operator are defined through introduction and elimination rules. We illustrate structural and metatheoretical properties, with particular focus on the ability to establish under which term evolutions and logical rules ap
&lt;/p&gt;</description></item></channel></rss>