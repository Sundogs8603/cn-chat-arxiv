<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#19981;&#24536;&#25105;&#26159;&#19968;&#31181;&#39640;&#25928;&#12289;&#20302;&#25104;&#26412;&#30340;&#35299;&#20915;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#21024;&#38500;&#25351;&#23450;&#30340;&#36523;&#20221;&#12289;&#23545;&#35937;&#25110;&#26679;&#24335;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#35760;&#24518;&#20998;&#25968; (M-Score) &#21644;&#27010;&#24565;&#22522;&#20934; (ConceptBench) &#26469;&#34913;&#37327;&#27169;&#22411;&#29983;&#25104;&#36890;&#29992;&#27010;&#24565;&#30340;&#33021;&#21147;&#12290;&#22312;&#19977;&#20010;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#35780;&#20272;&#26174;&#31034;&#20986;&#20102;&#19981;&#24536;&#25105;&#30340;&#26377;&#21069;&#36884;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.17591</link><description>&lt;p&gt;
&#8220;&#19981;&#24536;&#25105;&#8221;&#65306;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#36951;&#24536;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Forget-Me-Not: Learning to Forget in Text-to-Image Diffusion Models. (arXiv:2303.17591v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17591
&lt;/p&gt;
&lt;p&gt;
&#19981;&#24536;&#25105;&#26159;&#19968;&#31181;&#39640;&#25928;&#12289;&#20302;&#25104;&#26412;&#30340;&#35299;&#20915;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#21024;&#38500;&#25351;&#23450;&#30340;&#36523;&#20221;&#12289;&#23545;&#35937;&#25110;&#26679;&#24335;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#35760;&#24518;&#20998;&#25968; (M-Score) &#21644;&#27010;&#24565;&#22522;&#20934; (ConceptBench) &#26469;&#34913;&#37327;&#27169;&#22411;&#29983;&#25104;&#36890;&#29992;&#27010;&#24565;&#30340;&#33021;&#21147;&#12290;&#22312;&#19977;&#20010;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#35780;&#20272;&#26174;&#31034;&#20986;&#20102;&#19981;&#24536;&#25105;&#30340;&#26377;&#21069;&#36884;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#36951;&#24536;&#38382;&#39064;&#26366;&#19968;&#24230;&#26159;&#23398;&#26415;&#30028;&#30340;&#20027;&#35201;&#20851;&#27880;&#28857;&#65292;&#20294;&#22914;&#20170;&#24050;&#25104;&#20026;&#20135;&#19994;&#30028;&#30340;&#26222;&#36941;&#38382;&#39064;&#12290;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#25216;&#26415;&#30340;&#37325;&#22823;&#36827;&#23637;&#24341;&#21457;&#20102;&#20840;&#29699;&#23545;&#38544;&#31169;&#12289;&#29256;&#26435;&#21644;&#23433;&#20840;&#30340;&#35752;&#35770;&#65292;&#22240;&#20026;&#36825;&#20123;&#27169;&#22411;&#23398;&#20064;&#20102;&#22823;&#37327;&#26410;&#25480;&#26435;&#30340;&#20010;&#20154;&#36523;&#20221;&#12289;&#20869;&#23481;&#12289;&#33402;&#26415;&#21019;&#20316;&#21644;&#28508;&#22312;&#30340;&#26377;&#23475;&#29289;&#36136;&#65292;&#38543;&#21518;&#29992;&#20110;&#29983;&#25104;&#21644;&#20998;&#21457;&#26080;&#25511;&#21046;&#30340;&#20869;&#23481;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#19981;&#24536;&#25105;&#8221;&#65292;&#36825;&#26159;&#19968;&#31181;&#39640;&#25928;&#12289;&#20302;&#25104;&#26412;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26088;&#22312;&#23433;&#20840;&#22320;&#20174;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#21024;&#38500;&#25351;&#23450;&#30340;&#36523;&#20221;&#12289;&#23545;&#35937;&#25110;&#26679;&#24335;&#65292;&#21482;&#38656;&#19981;&#21040;30&#31186;&#30340;&#26102;&#38388;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#20854;&#29983;&#25104;&#20854;&#20182;&#20869;&#23481;&#30340;&#33021;&#21147;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20043;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#35760;&#24518;&#20998;&#25968; (M-Score)&#8221;&#21644;&#8220;&#27010;&#24565;&#22522;&#20934; (ConceptBench)&#8221;&#26469;&#34913;&#37327;&#27169;&#22411;&#29983;&#25104;&#36890;&#29992;&#27010;&#24565;&#30340;&#33021;&#21147;&#65292;&#20998;&#20026;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#65306;ID&#12289;&#23545;&#35937;&#21644;&#26679;&#24335;&#12290;&#20351;&#29992;M-Score&#21644;ConceptBench&#65292;&#25105;&#20204;&#23545;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#24536;&#35760;&#25105;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#35760;&#24518;&#20445;&#30041;&#29575;&#12289;&#22270;&#20687;&#36136;&#37327;&#21644;&#25512;&#29702;&#36895;&#24230;&#31561;&#26041;&#38754;&#30340;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The unlearning problem of deep learning models, once primarily an academic concern, has become a prevalent issue in the industry. The significant advances in text-to-image generation techniques have prompted global discussions on privacy, copyright, and safety, as numerous unauthorized personal IDs, content, artistic creations, and potentially harmful materials have been learned by these models and later utilized to generate and distribute uncontrolled content. To address this challenge, we propose \textbf{Forget-Me-Not}, an efficient and low-cost solution designed to safely remove specified IDs, objects, or styles from a well-configured text-to-image model in as little as 30 seconds, without impairing its ability to generate other content. Alongside our method, we introduce the \textbf{Memorization Score (M-Score)} and \textbf{ConceptBench} to measure the models' capacity to generate general concepts, grouped into three primary categories: ID, object, and style. Using M-Score and Conc
&lt;/p&gt;</description></item><item><title>&#29992;ChatGPT&#20316;&#20026;&#20219;&#21153;&#35268;&#21010;&#24037;&#20855;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#25511;&#21046;&#22120;&#26469;&#25972;&#21512;&#29616;&#26377;&#30340;AI&#27169;&#22411;&#65292;&#35299;&#20915;&#22797;&#26434;&#30340;AI&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.17580</link><description>&lt;p&gt;
HuggingGPT: &#22312;HugingFace&#20013;&#20351;&#29992;ChatGPT&#21450;&#20854;&#20249;&#20276;&#35299;&#20915;AI&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace. (arXiv:2303.17580v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17580
&lt;/p&gt;
&lt;p&gt;
&#29992;ChatGPT&#20316;&#20026;&#20219;&#21153;&#35268;&#21010;&#24037;&#20855;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#25511;&#21046;&#22120;&#26469;&#25972;&#21512;&#29616;&#26377;&#30340;AI&#27169;&#22411;&#65292;&#35299;&#20915;&#22797;&#26434;&#30340;AI&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#19981;&#21516;&#39046;&#22495;&#21644;&#27169;&#24577;&#30340;&#22797;&#26434;AI&#20219;&#21153;&#26159;&#36890;&#21521;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#25511;&#21046;&#22120;&#26469;&#31649;&#29702;&#29616;&#26377;&#30340;AI&#27169;&#22411;&#20197;&#35299;&#20915;AI&#20219;&#21153;&#65292;&#35821;&#35328;&#25104;&#20026;&#36890;&#29992;&#25509;&#21475;&#26469;&#36171;&#33021;&#23427;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;ChatGPT&#20316;&#20026;&#20219;&#21153;&#35268;&#21010;&#24037;&#20855;&#65292;&#26681;&#25454;HuggingFace&#20013;&#21487;&#29992;&#30340;&#27169;&#22411;&#21151;&#33021;&#25551;&#36848;&#26469;&#36873;&#25321;&#27169;&#22411;&#65292;&#22312;&#36873;&#23450;AI&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#27599;&#20010;&#23376;&#20219;&#21153;&#65292;&#24182;&#24635;&#32467;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence (AGI). While there are abundant AI models available for different domains and modalities, they cannot handle complicated AI tasks. Considering large language models (LLMs) have exhibited exceptional ability in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks and language could be a generic interface to empower this. Based on this philosophy, we present HuggingGPT, a system that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., HuggingFace) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in HuggingFace, execute each subtask with the selected AI model, and summarize the response acco
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#30340;&#25918;&#23556;&#24615;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#27169;&#22359; X-REM&#65292;&#23427;&#20351;&#29992;&#22270;&#20687;&#25991;&#26412;&#21305;&#37197;&#20998;&#25968;&#26469;&#34913;&#37327;&#33016;&#37096; X &#20809;&#22270;&#20687;&#21644;&#25918;&#23556;&#23398;&#25253;&#21578;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#65292;&#20197;&#36827;&#34892;&#25253;&#21578;&#26816;&#32034;&#65292;&#20854;&#22312;&#22810;&#20010;&#20808;&#21069;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#27169;&#22359;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#21487;&#26377;&#25928;&#25552;&#39640;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#33258;&#21160;&#29983;&#25104;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.17579</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22270;&#20687;&#25991;&#26412;&#21305;&#37197;&#20248;&#21270;&#22522;&#20110;&#26816;&#32034;&#30340;&#33016;&#37096; X &#23556;&#32447;&#25253;&#21578;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Multimodal Image-Text Matching Improves Retrieval-based Chest X-Ray Report Generation. (arXiv:2303.17579v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#30340;&#25918;&#23556;&#24615;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#27169;&#22359; X-REM&#65292;&#23427;&#20351;&#29992;&#22270;&#20687;&#25991;&#26412;&#21305;&#37197;&#20998;&#25968;&#26469;&#34913;&#37327;&#33016;&#37096; X &#20809;&#22270;&#20687;&#21644;&#25918;&#23556;&#23398;&#25253;&#21578;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#65292;&#20197;&#36827;&#34892;&#25253;&#21578;&#26816;&#32034;&#65292;&#20854;&#22312;&#22810;&#20010;&#20808;&#21069;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#27169;&#22359;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#21487;&#26377;&#25928;&#25552;&#39640;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#33258;&#21160;&#29983;&#25104;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#29983;&#25104;&#20020;&#24202;&#20934;&#30830;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#21487;&#20197;&#25913;&#21892;&#24739;&#32773;&#25252;&#29702;&#12290;&#20197;&#21069;&#20381;&#36182;&#22270;&#20687;&#23383;&#24149;&#27169;&#22411;&#30340;&#25253;&#21578;&#29983;&#25104;&#26041;&#27861;&#30001;&#20110;&#32570;&#20047;&#30456;&#20851;&#39046;&#22495;&#30693;&#35782;&#32780;&#32463;&#24120;&#29983;&#25104;&#19981;&#36830;&#36143;&#21644;&#19981;&#27491;&#30830;&#30340;&#25991;&#26412;&#65292;&#32780;&#22522;&#20110;&#26816;&#32034;&#30340;&#23581;&#35797;&#32463;&#24120;&#26816;&#32034;&#21040;&#19982;&#36755;&#20837;&#22270;&#20687;&#19981;&#30456;&#20851;&#30340;&#25253;&#21578;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Contrastive X-Ray REport Match&#65288;X-REM&#65289;&#30340;&#26032;&#22411;&#22522;&#20110;&#26816;&#32034;&#30340;&#25918;&#23556;&#24615;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#27169;&#22359;&#65292;&#35813;&#27169;&#22359;&#20351;&#29992;&#22270;&#20687;&#25991;&#26412;&#21305;&#37197;&#20998;&#25968;&#26469;&#34913;&#37327;&#33016;&#37096; X &#20809;&#22270;&#20687;&#21644;&#25918;&#23556;&#23398;&#25253;&#21578;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#20197;&#36827;&#34892;&#25253;&#21578;&#26816;&#32034;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#20351;&#29992;&#35821;&#35328;&#22270;&#20687;&#27169;&#22411;&#35745;&#31639;&#22270;&#20687;&#25991;&#26412;&#21305;&#37197;&#20998;&#25968;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#21040;&#22312;&#20351;&#29992;&#20313;&#24358;&#30456;&#20284;&#24615;&#26102;&#32463;&#24120;&#20002;&#22833;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#32454;&#31890;&#24230;&#20132;&#20114;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#21644;&#20020;&#24202;&#24230;&#37327;&#26041;&#38754;&#65292;X-REM&#22312;&#22810;&#20010;&#20808;&#21069;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#27169;&#22359;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;&#36890;&#36807;&#23545;&#29983;&#25104;&#30340;&#25253;&#21578;&#36827;&#34892;&#20154;&#31867;&#35780;&#20272;&#65292;&#34920;&#26126; X-R...
&lt;/p&gt;
&lt;p&gt;
Automated generation of clinically accurate radiology reports can improve patient care. Previous report generation methods that rely on image captioning models often generate incoherent and incorrect text due to their lack of relevant domain knowledge, while retrieval-based attempts frequently retrieve reports that are irrelevant to the input image. In this work, we propose Contrastive X-Ray REport Match (X-REM), a novel retrieval-based radiology report generation module that uses an image-text matching score to measure the similarity of a chest X-ray image and radiology report for report retrieval. We observe that computing the image-text matching score with a language-image model can effectively capture the fine-grained interaction between image and text that is often lost when using cosine similarity. X-REM outperforms multiple prior radiology report generation modules in terms of both natural language and clinical metrics. Human evaluation of the generated reports suggests that X-R
&lt;/p&gt;</description></item><item><title>EWR&#26041;&#27861;&#36890;&#36807;&#36153;&#33293;&#23572;&#20449;&#24687;&#30697;&#38453;&#26435;&#34913;&#35821;&#38899;&#29983;&#25104;&#27169;&#22411;&#20013;&#20010;&#20307;&#21442;&#25968;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#39640;&#23545;&#35805;&#22238;&#22797;&#30340;&#24544;&#23454;&#24615;&#65292;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.17574</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#24544;&#23454;&#21644;&#25277;&#35937;&#21270;&#23545;&#35805;&#29983;&#25104;&#30340;&#24377;&#24615;&#26435;&#37325;&#21435;&#38500;
&lt;/p&gt;
&lt;p&gt;
Elastic Weight Removal for Faithful and Abstractive Dialogue Generation. (arXiv:2303.17574v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17574
&lt;/p&gt;
&lt;p&gt;
EWR&#26041;&#27861;&#36890;&#36807;&#36153;&#33293;&#23572;&#20449;&#24687;&#30697;&#38453;&#26435;&#34913;&#35821;&#38899;&#29983;&#25104;&#27169;&#22411;&#20013;&#20010;&#20307;&#21442;&#25968;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#39640;&#23545;&#35805;&#22238;&#22797;&#30340;&#24544;&#23454;&#24615;&#65292;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#23545;&#35805;&#31995;&#32479;&#24212;&#35813;&#29983;&#25104;&#24544;&#23454;&#20110;&#30456;&#20851;&#25991;&#26723;&#20013;&#21253;&#21547;&#30340;&#30693;&#35782;&#30340;&#22238;&#22797;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#27169;&#22411;&#29983;&#25104;&#20102;&#24187;&#24819;&#30340;&#21709;&#24212;&#65292;&#20854;&#20013;&#21253;&#21547;&#19982;&#20854;&#30456;&#30683;&#30462;&#30340;&#20449;&#24687;&#25110;&#19981;&#21487;&#39564;&#35777;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#31181;&#19981;&#33391;&#34892;&#20026;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#22312;&#36127;&#38754;&#31034;&#20363;&#19978;&#24494;&#35843;&#8220;&#36127;&#38754;&#19987;&#23478;&#8221;&#65292;&#24182;&#20174;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21442;&#25968;&#20013;&#20943;&#21435;&#23427;&#30340;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#30452;&#35273;&#19978;&#65292;&#36825;&#24182;&#27809;&#26377;&#32771;&#34385;&#21040;&#26576;&#20123;&#21442;&#25968;&#27604;&#20854;&#20182;&#21442;&#25968;&#26356;&#36127;&#36131;&#23548;&#33268;&#24187;&#35273;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#65288;&#36817;&#20284;&#65289;&#36153;&#33293;&#23572;&#20449;&#24687;&#30697;&#38453;&#26469;&#26435;&#34913;&#23427;&#20204;&#30340;&#20010;&#20307;&#37325;&#35201;&#24615;&#65292;&#35813;&#30697;&#38453;&#34913;&#37327;&#20854;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#23558;&#27492;&#26041;&#27861;&#31216;&#20026;&#24377;&#24615;&#26435;&#37325;&#21435;&#38500;&#65288;EWR&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;Flan-T5&#19981;&#21516;&#21464;&#20307;&#20316;&#20026;&#39592;&#24178;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#20449;&#24687;&#23547;&#27714;&#23545;&#35805;&#29983;&#25104;&#25968;&#25454;&#38598;&#19978;&#27604;&#36739;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#24544;&#23454;&#24615;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ideally, dialogue systems should generate responses that are faithful to the knowledge contained in relevant documents. However, many models generate hallucinated responses instead that contradict it or contain unverifiable information. To mitigate such undesirable behaviour, it has been proposed to fine-tune a `negative expert' on negative examples and subtract its parameters from those of a pre-trained model. However, intuitively, this does not take into account that some parameters are more responsible than others in causing hallucinations. Thus, we propose to weigh their individual importance via (an approximation of) the Fisher Information matrix, which measures the uncertainty of their estimate. We call this method Elastic Weight Removal (EWR). We evaluate our method -- using different variants of Flan-T5 as a backbone language model -- on multiple datasets for information-seeking dialogue generation and compare our method with state-of-the-art techniques for faithfulness, such a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36828;&#31243;&#35780;&#20272;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#36816;&#21160;&#34920;&#29616;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#37325;&#22797;&#29992;&#20110;&#31867;&#20284;&#30340;&#36816;&#21160;&#20219;&#21153;&#65292;&#25317;&#26377;&#36739;&#39640;&#30340;&#21487;&#38752;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17573</link><description>&lt;p&gt;
&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#22312;&#23478;&#20013;&#27979;&#37327;&#24085;&#37329;&#26862;&#30149;&#30340;&#20005;&#37325;&#31243;&#24230;
&lt;/p&gt;
&lt;p&gt;
Using AI to Measure Parkinson's Disease Severity at Home. (arXiv:2303.17573v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17573
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36828;&#31243;&#35780;&#20272;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#36816;&#21160;&#34920;&#29616;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#37325;&#22797;&#29992;&#20110;&#31867;&#20284;&#30340;&#36816;&#21160;&#20219;&#21153;&#65292;&#25317;&#26377;&#36739;&#39640;&#30340;&#21487;&#38752;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36828;&#31243;&#35780;&#20272;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#36816;&#21160;&#34920;&#29616;&#30340;&#26041;&#27861;&#12290;&#21442;&#19982;&#32773;&#22312;&#32593;&#32476;&#25668;&#20687;&#22836;&#21069;&#23436;&#25104;&#20102;&#36816;&#21160;&#20219;&#21153;&#65288;&#21363;&#28857;&#20987;&#25163;&#25351;&#65289;&#65292;250&#21517;&#20840;&#29699;&#21442;&#19982;&#32773;&#30340;&#25968;&#25454;&#25353;&#29031;&#36816;&#21160;&#38556;&#30861;&#21327;&#20250;&#32479;&#19968;&#24085;&#37329;&#26862;&#30149;&#35780;&#20998;&#37327;&#34920; (MDS-UPDRS) &#30340;&#26631;&#20934;&#30001;&#19977;&#21517;&#19987;&#23478;&#31070;&#32463;&#23398;&#23478;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#31070;&#32463;&#23398;&#23478;&#30340;&#35780;&#20272;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#38752;&#24615;&#65292;&#20869;&#37096;&#19968;&#33268;&#24615;&#31995;&#25968;&#65288;ICC&#65289;&#20026;0.88&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#35745;&#31639;&#26426;&#31639;&#27861;&#26469;&#33719;&#24471;&#19982;MDS-UPDRS&#25351;&#21335;&#19968;&#33268;&#19988;&#19982;&#31070;&#32463;&#23398;&#23478;&#30340;&#35780;&#20272;&#39640;&#24230;&#30456;&#20851;&#30340;&#23458;&#35266;&#27979;&#37327;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#36825;&#20123;&#25351;&#26631;&#30340;&#35757;&#32451;&#19979;&#34920;&#29616;&#20248;&#20110;&#19968;&#20010;MDS-UPDRS&#35748;&#35777;&#30340;&#35780;&#20998;&#32773;&#65292;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#20026;0.59&#65292;&#32780;&#35780;&#20998;&#32773;&#30340;MAE&#20026;0.79&#12290;&#28982;&#32780;&#65292;&#35813;&#27169;&#22411;&#30340;&#34920;&#29616;&#30053;&#36874;&#20110;&#19987;&#23478;&#31070;&#32463;&#23398;&#23478;&#65288;0.53 MAE&#65289;&#12290;&#35813;&#26041;&#27861;&#21487;&#37325;&#22797;&#29992;&#20110;&#31867;&#20284;&#30340;&#36816;&#21160;&#20219;&#21153;&#65292;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an artificial intelligence system to remotely assess the motor performance of individuals with Parkinson's disease (PD). Participants performed a motor task (i.e., tapping fingers) in front of a webcam, and data from 250 global participants were rated by three expert neurologists following the Movement Disorder Society Unified Parkinson's Disease Rating Scale (MDS-UPDRS). The neurologists' ratings were highly reliable, with an intra-class correlation coefficient (ICC) of 0.88. We developed computer algorithms to obtain objective measurements that align with the MDS-UPDRS guideline and are strongly correlated with the neurologists' ratings. Our machine learning model trained on these measures outperformed an MDS-UPDRS certified rater, with a mean absolute error (MAE) of 0.59 compared to the rater's MAE of 0.79. However, the model performed slightly worse than the expert neurologists (0.53 MAE). The methodology can be replicated for similar motor tasks, providing the possibili
&lt;/p&gt;</description></item><item><title>CodeGeeX&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;130&#20159;&#21442;&#25968;&#65292;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#12290;&#32463;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;CodeGeeX&#22312;HumanEval-X&#19978;&#30340;&#20195;&#30721;&#29983;&#25104;&#21644;&#32763;&#35793;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;&#27492;&#22806;&#65292;CodeGeeX&#21487;&#20197;&#23558;&#31243;&#24207;&#21592;&#30340;&#29983;&#20135;&#21147;&#25552;&#39640;22%&#12290;</title><link>http://arxiv.org/abs/2303.17568</link><description>&lt;p&gt;
CodeGeeX&#65306;&#22810;&#35821;&#35328;&#35780;&#20272;&#19979;&#30340;&#39044;&#35757;&#32451;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Evaluations on HumanEval-X. (arXiv:2303.17568v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17568
&lt;/p&gt;
&lt;p&gt;
CodeGeeX&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;130&#20159;&#21442;&#25968;&#65292;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#12290;&#32463;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;CodeGeeX&#22312;HumanEval-X&#19978;&#30340;&#20195;&#30721;&#29983;&#25104;&#21644;&#32763;&#35793;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;&#27492;&#22806;&#65292;CodeGeeX&#21487;&#20197;&#23558;&#31243;&#24207;&#21592;&#30340;&#29983;&#20135;&#21147;&#25552;&#39640;22%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;OpenAI Codex&#65289;&#21487;&#20197;&#29983;&#25104;&#27491;&#30830;&#35821;&#27861;&#21644;&#21151;&#33021;&#30340;&#20195;&#30721;&#65292;&#20351;&#31243;&#24207;&#21592;&#30340;&#32534;&#30721;&#26356;&#21152;&#39640;&#25928;&#65292;&#20351;&#25105;&#20204;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#36861;&#27714;&#26356;&#21152;&#36148;&#36817;&#29616;&#23454;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;CodeGeeX&#65292;&#19968;&#20010;&#20855;&#26377;130&#20159;&#21442;&#25968;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#12290;CodeGeeX&#22312;2022&#24180;6&#26376;&#26102;&#22522;&#20110;23&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;8500&#20159;&#20196;&#29260;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;CodeGeeX&#22312;HumanEval-X&#19978;&#30340;&#20195;&#30721;&#29983;&#25104;&#21644;&#32763;&#35793;&#20219;&#21153;&#20013;&#22343;&#20248;&#20110;&#35268;&#27169;&#30456;&#20284;&#30340;&#22810;&#35821;&#35328;&#20195;&#30721;&#27169;&#22411;&#12290;&#22312;HumanEval&#65288;&#20165;&#38480;Python&#65289;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;HumanEval-X&#22522;&#20934;&#27979;&#35797;&#65292;&#36890;&#36807;&#25163;&#20889;C ++&#12289;Java&#12289;JavaScript&#21644;Go&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#35780;&#20272;&#22810;&#35821;&#35328;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;Visual Studio Code&#12289;JetBrains&#21644;Cloud Studio&#19978;&#26500;&#24314;&#20102;&#22522;&#20110;CodeGeeX&#30340;&#25193;&#23637;&#65292;&#27599;&#21608;&#20026;&#25968;&#20197;&#19975;&#35745;&#30340;&#27963;&#36291;&#29992;&#25143;&#29983;&#25104;47&#20159;&#20196;&#29260;&#12290;&#25105;&#20204;&#30340;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;CodeGeeX&#21487;&#20197;&#23558;&#31243;&#24207;&#21592;&#30340;&#29983;&#20135;&#21147;&#25552;&#39640;22%&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained code generation models, such as OpenAI Codex, can generate syntax- and function-correct code, making the coding of programmers more productive and our pursuit of artificial general intelligence closer. In this paper, we introduce CodeGeeX, a multilingual model with 13 billion parameters for code generation. CodeGeeX is pre-trained on 850 billion tokens of 23 programming languages as of June 2022. Our extensive experiments suggest that CodeGeeX outperforms multilingual code models of similar scale for both the tasks of code generation and translation on HumanEval-X. Building upon HumanEval (Python only), we develop the HumanEval-X benchmark for evaluating multilingual models by hand-writing the solutions in C++, Java, JavaScript, and Go. In addition, we build CodeGeeX-based extensions on Visual Studio Code, JetBrains, and Cloud Studio, generating 4.7 billion tokens for tens of thousands of active users per week. Our user study demonstrates that CodeGeeX can help to inc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;BloombergGPT&#65292;&#19968;&#20010;500&#20159;&#21442;&#25968;&#30340;&#37329;&#34701;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#22522;&#20110;Bloomberg&#30340;&#24191;&#27867;&#25968;&#25454;&#26469;&#28304;&#21644;&#36890;&#29992;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#36890;&#36807;&#28151;&#21512;&#25968;&#25454;&#38598;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#22312;&#37329;&#34701;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#19981;&#20250;&#29306;&#29298;&#22312;&#26222;&#36890;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.17564</link><description>&lt;p&gt;
BloombergGPT&#65306;&#37329;&#34701;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BloombergGPT: A Large Language Model for Finance. (arXiv:2303.17564v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;BloombergGPT&#65292;&#19968;&#20010;500&#20159;&#21442;&#25968;&#30340;&#37329;&#34701;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#22522;&#20110;Bloomberg&#30340;&#24191;&#27867;&#25968;&#25454;&#26469;&#28304;&#21644;&#36890;&#29992;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#36890;&#36807;&#28151;&#21512;&#25968;&#25454;&#38598;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#22312;&#37329;&#34701;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#19981;&#20250;&#29306;&#29298;&#22312;&#26222;&#36890;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#37329;&#34701;&#25216;&#26415;&#39046;&#22495;&#26377;&#30528;&#24191;&#27867;&#32780;&#22797;&#26434;&#30340;&#24212;&#29992;&#65292;&#20174;&#24773;&#24863;&#20998;&#26512;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21040;&#38382;&#31572;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#34987;&#35777;&#26126;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#38750;&#24120;&#26377;&#25928;&#65307;&#28982;&#32780;&#65292;&#19987;&#20026;&#37329;&#34701;&#39046;&#22495;&#35774;&#35745;&#30340;LLM&#23578;&#26410;&#22312;&#25991;&#29486;&#20013;&#25253;&#21578;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BloombergGPT&#65292;&#19968;&#20010;&#25317;&#26377;500&#20159;&#20010;&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#26159;&#22522;&#20110;&#24191;&#27867;&#30340;&#37329;&#34701;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#31181;3630&#20159;&#20010;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#22522;&#20110;&#24429;&#21338;&#31038;&#30340;&#24191;&#27867;&#25968;&#25454;&#26469;&#28304;&#65292;&#21487;&#33021;&#26159;&#36804;&#20170;&#26368;&#22823;&#30340;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#21448;&#22686;&#21152;&#20102;&#26469;&#33258;&#36890;&#29992;&#25968;&#25454;&#38598;&#30340;3450&#20159;&#20010;&#26631;&#35760;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;LLM&#22522;&#20934;&#12289;&#24320;&#25918;&#24335;&#37329;&#34701;&#22522;&#20934;&#21644;&#19968;&#22871;&#26368;&#33021;&#20934;&#30830;&#21453;&#26144;&#25105;&#20204;&#39044;&#26399;&#29992;&#36884;&#30340;&#20869;&#37096;&#22522;&#20934;&#19978;&#39564;&#35777;&#20102;BloombergGPT&#12290;&#25105;&#20204;&#30340;&#28151;&#21512;&#25968;&#25454;&#38598;&#35757;&#32451;&#20135;&#29983;&#20102;&#19968;&#20010;&#22312;&#37329;&#34701;&#20219;&#21153;&#19978;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#19981;&#20250;&#29306;&#29298;&#26222;&#36890;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of NLP in the realm of financial technology is broad and complex, with applications ranging from sentiment analysis and named entity recognition to question answering. Large Language Models (LLMs) have been shown to be effective on a variety of tasks; however, no LLM specialized for the financial domain has been reported in literature. In this work, we present BloombergGPT, a 50 billion parameter language model that is trained on a wide range of financial data. We construct a 363 billion token dataset based on Bloomberg's extensive data sources, perhaps the largest domain-specific dataset yet, augmented with 345 billion tokens from general purpose datasets. We validate BloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that most accurately reflect our intended usage. Our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26580;&#24615;&#36328;&#27169;&#24577;&#23545;&#40784;&#30340;&#26041;&#27861; SoftCLIP&#65292;&#22312;&#37197;&#23545;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.17561</link><description>&lt;p&gt;
SoftCLIP: &#26356;&#26580;&#21644;&#30340;&#36328;&#27169;&#24577;&#23545;&#40784;&#20351; CLIP &#26356;&#24378;&#22823;
&lt;/p&gt;
&lt;p&gt;
SoftCLIP: Softer Cross-modal Alignment Makes CLIP Stronger. (arXiv:2303.17561v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26580;&#24615;&#36328;&#27169;&#24577;&#23545;&#40784;&#30340;&#26041;&#27861; SoftCLIP&#65292;&#22312;&#37197;&#23545;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20004;&#24180;&#20013;&#65292;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#65292;&#20854;&#20013;&#37197;&#23545;&#23436;&#20840;&#20114;&#19981;&#24178;&#25200;&#65292;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#24182;&#19988;&#24120;&#29992;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#22122;&#22768;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; SoftCLIP&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26469;&#33258;&#32454;&#31890;&#24230;&#20869;&#27169;&#24577;&#33258;&#30456;&#20284;&#24615;&#29983;&#25104;&#30340;&#8220;&#26580;&#24615;&#30446;&#26631;&#8221;&#65292;&#23454;&#29616;&#20102;&#26580;&#24615;&#36328;&#27169;&#24577;&#23545;&#40784;&#12290;&#20869;&#27169;&#24577;&#24341;&#23548;&#33021;&#22815;&#20351;&#24471;&#20004;&#20010;&#37197;&#23545;&#20043;&#38388;&#23384;&#22312;&#19968;&#20123;&#23616;&#37096;&#30456;&#20284;&#24615;&#65292;&#24182;&#19988;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#22810;&#23545;&#22810;&#30340;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#27491;&#26679;&#26412;&#22312;&#26580;&#24615;&#30446;&#26631;&#20998;&#24067;&#20013;&#20173;&#28982;&#21344;&#20027;&#23548;&#22320;&#20301;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#31163;&#20998;&#24067;&#20013;&#30340;&#36127;&#26679;&#26412;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#36328;&#27169;&#24577;&#23398;&#20064;&#20013;&#30340;&#20851;&#31995;&#23545;&#40784;&#24615;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126; SoftCLIP &#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
During the preceding biennium, vision-language pre-training has achieved noteworthy success on several downstream tasks. Nevertheless, acquiring high-quality image-text pairs, where the pairs are entirely exclusive of each other, remains a challenging task, and noise exists in the commonly used datasets. To address this issue, we propose SoftCLIP, a novel approach that relaxes the strict one-to-one constraint and achieves a soft cross-modal alignment by introducing a softened target, which is generated from the fine-grained intra-modal self-similarity. The intra-modal guidance is indicative to enable two pairs have some local similarities and model many-to-many relationships between the two modalities. Besides, since the positive still dominates in the softened target distribution, we disentangle the negatives in the distribution to further boost the relation alignment with the negatives in the cross-modal learning. Extensive experiments demonstrate the effectiveness of SoftCLIP. In pa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#27431;&#30431;&#30340;AI&#27861;&#26696;&#26159;&#21542;&#36275;&#20197;&#34920;&#26126;&#22312;&#20854;&#27861;&#24459;&#26694;&#26550;&#20013;&#23384;&#22312;&#25216;&#26415;&#21487;&#35299;&#37322;&#24615;&#26435;&#21033;&#65292;&#24182;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#26159;&#21542;&#38656;&#35201;&#23558;&#20854;&#32435;&#20837;&#29616;&#34892;&#31435;&#27861;&#20013;&#12290;</title><link>http://arxiv.org/abs/2303.17558</link><description>&lt;p&gt;
&#12298;AI&#27861;&#26696;&#25552;&#26696;&#65306;&#19968;&#39033;&#26032;&#30340;&#25216;&#26415;&#21487;&#35299;&#37322;&#24615;&#26435;&#21033;&#65311;&#12299;
&lt;/p&gt;
&lt;p&gt;
The AI Act proposal: a new right to technical interpretability?. (arXiv:2303.17558v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#27431;&#30431;&#30340;AI&#27861;&#26696;&#26159;&#21542;&#36275;&#20197;&#34920;&#26126;&#22312;&#20854;&#27861;&#24459;&#26694;&#26550;&#20013;&#23384;&#22312;&#25216;&#26415;&#21487;&#35299;&#37322;&#24615;&#26435;&#21033;&#65292;&#24182;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#26159;&#21542;&#38656;&#35201;&#23558;&#20854;&#32435;&#20837;&#29616;&#34892;&#31435;&#27861;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#30340;&#35299;&#37322;&#26435;&#38382;&#39064;&#28041;&#21450;&#22823;&#37327;&#25991;&#29486;&#30340;&#35752;&#35770;&#12290;&#22312;&#27861;&#24459;&#23398;&#32773;&#20013;&#65292;&#38598;&#20013;&#20110;&#12298;&#36890;&#29992;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#12299;&#20013;&#30340;&#31532;22&#26465;&#65307;&#22312;&#25216;&#26415;&#23398;&#32773;&#20013;&#65292;&#38598;&#20013;&#20110;&#33021;&#22815;&#24110;&#21161;&#35299;&#37322;&#26576;&#20010;&#27169;&#22411;&#36755;&#20986;&#30340;&#25216;&#26415;&#65288;XAI&#65289;&#19978;&#12290;&#26412;&#25991;&#26088;&#22312;&#35843;&#26597;&#12298;AI&#27861;&#26696;&#12299;&#20013;&#24341;&#20837;&#30340;&#26032;&#35268;&#23450;&#19982;&#12298;108&#20844;&#32422;&#12299;&#21644;&#12298;&#36890;&#29992;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#12299;&#30456;&#32467;&#21512;&#26159;&#21542;&#36275;&#20197;&#34920;&#26126;&#22312;&#27431;&#30431;&#30340;&#27861;&#24459;&#26694;&#26550;&#20013;&#23384;&#22312;&#25216;&#26415;&#21487;&#35299;&#37322;&#24615;&#26435;&#21033;&#65292;&#22914;&#26524;&#19981;&#26159;&#65292;&#21017;&#27431;&#30431;&#26159;&#21542;&#24212;&#23558;&#20854;&#21253;&#21547;&#22312;&#20854;&#29616;&#34892;&#31435;&#27861;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
The debate about the concept of the so called right to explanation in AI is the subject of a wealth of literature. It has focused, in the legal scholarship, on art. 22 GDPR and, in the technical scholarship, on techniques that help explain the output of a certain model (XAI). The purpose of this work is to investigate if the new provisions introduced by the proposal for a Regulation laying down harmonised rules on artificial intelligence (AI Act), in combination with Convention 108 plus and GDPR, are enough to indicate the existence of a right to technical explainability in the EU legal framework and, if not, whether the EU should include it in its current legislation. This is a preliminary work submitted to the online event organised by the Information Society Law Center and it will be later developed into a full paper.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25209;&#21028;&#24615;&#22238;&#39038;AI&#20844;&#24179;&#24615;&#25991;&#29486;&#20013;30&#31687;&#20132;&#32455;&#24615;&#35752;&#35770;&#65292;&#25581;&#31034;&#30740;&#31350;&#20154;&#21592;&#26222;&#36941;&#32570;&#20047;&#23545;&#20132;&#32455;&#24615;&#30340;&#25972;&#20307;&#29702;&#35299;&#65292;&#20854;&#19968;&#26041;&#38754;&#23558;&#20854;&#32553;&#23567;&#20026;&#22312;&#32676;&#20307;&#23376;&#32452;&#19978;&#36827;&#34892;&#20844;&#24179;&#24230;&#37327;&#30340;&#20248;&#21270;&#65292;&#21478;&#19968;&#26041;&#38754;&#21017;&#22312;&#31038;&#20250;&#32972;&#26223;&#21644;&#26435;&#21147;&#32467;&#26500;&#30340;&#35752;&#35770;&#26041;&#38754;&#23384;&#22312;&#27424;&#32570;&#12290;</title><link>http://arxiv.org/abs/2303.17555</link><description>&lt;p&gt;
&#23545;&#21387;&#36843;&#30697;&#38453;&#30340;&#20998;&#35299;:&#25581;&#31034;&#20132;&#32455;&#24615;&#22312;AI&#20844;&#24179;&#24615;&#20013;&#30340;&#20316;&#29992;&#30340;&#25209;&#21028;&#24615;&#22238;&#39038;&#19982;&#20877;&#24819;&#35937;
&lt;/p&gt;
&lt;p&gt;
Factoring the Matrix of Domination: A Critical Review and Reimagination of Intersectionality in AI Fairness. (arXiv:2303.17555v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25209;&#21028;&#24615;&#22238;&#39038;AI&#20844;&#24179;&#24615;&#25991;&#29486;&#20013;30&#31687;&#20132;&#32455;&#24615;&#35752;&#35770;&#65292;&#25581;&#31034;&#30740;&#31350;&#20154;&#21592;&#26222;&#36941;&#32570;&#20047;&#23545;&#20132;&#32455;&#24615;&#30340;&#25972;&#20307;&#29702;&#35299;&#65292;&#20854;&#19968;&#26041;&#38754;&#23558;&#20854;&#32553;&#23567;&#20026;&#22312;&#32676;&#20307;&#23376;&#32452;&#19978;&#36827;&#34892;&#20844;&#24179;&#24230;&#37327;&#30340;&#20248;&#21270;&#65292;&#21478;&#19968;&#26041;&#38754;&#21017;&#22312;&#31038;&#20250;&#32972;&#26223;&#21644;&#26435;&#21147;&#32467;&#26500;&#30340;&#35752;&#35770;&#26041;&#38754;&#23384;&#22312;&#27424;&#32570;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#32455;&#24615;&#26159;&#19968;&#20010;&#20851;&#38190;&#26694;&#26550;&#65292;&#36890;&#36807;&#35843;&#26597;&#21644;&#23454;&#36341;&#65292;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#26816;&#26597;&#31038;&#20250;&#19981;&#24179;&#31561;&#22914;&#20309;&#36890;&#36807;&#32467;&#26500;&#21644;&#32426;&#24459;&#39046;&#22495;&#25345;&#32493;&#23384;&#22312;&#12290;&#22312;AI&#20844;&#24179;&#30340;&#29702;&#24565;&#20013;&#65292;&#8220;&#20844;&#24179;&#24615;&#8221;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#25105;&#20204;&#35748;&#20026;&#37319;&#29992;&#20132;&#32455;&#24615;&#20316;&#20026;&#20998;&#26512;&#26694;&#26550;&#23545;&#20110;&#26377;&#25928;&#22320;&#23454;&#29616;&#20844;&#24179;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#23545;AI&#20844;&#24179;&#25991;&#29486;&#20013;30&#31687;&#20851;&#20110;&#20132;&#32455;&#24615;&#30340;&#35752;&#35770;&#36827;&#34892;&#25209;&#21028;&#24615;&#22238;&#39038;&#65292;&#25105;&#20204;&#24402;&#32435;&#21644;&#28436;&#32462;&#20986;:1)&#20132;&#32455;&#24615;&#25351;&#23548;&#22914;&#20309;&#22312;AI&#20844;&#24179;&#33539;&#20363;&#20013;&#25805;&#20316;&#65292;2)&#25581;&#31034;&#20132;&#32455;&#24615;&#30340;&#27010;&#24565;&#21270;&#21644;&#23454;&#29616;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30740;&#31350;&#20154;&#21592;&#26222;&#36941;&#23558;&#20132;&#32455;&#24615;&#32553;&#20943;&#20026;&#38024;&#23545;&#20154;&#21475;&#20122;&#32452;&#30340;&#20844;&#24179;&#25351;&#26631;&#36827;&#34892;&#20248;&#21270;&#12290;&#20182;&#20204;&#20063;&#26410;&#33021;&#35752;&#35770;&#23427;&#20204;&#30340;&#31038;&#20250;&#32972;&#26223;&#65292;&#24403;&#25552;&#21040;&#26435;&#21147;&#26102;&#65292;&#20182;&#20204;&#20027;&#35201;&#23558;&#20854;&#32622;&#20110;AI&#27969;&#31243;&#20013;&#12290;&#25105;&#20204;&#23558;&#36827;&#19968;&#27493;&#38416;&#36848;&#24182;&#35780;&#20272;&#36825;&#20123;&#24046;&#36317;&#23545;&#20110;&#20020;&#24202;&#30740;&#31350;&#21644;&#23454;&#36341;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intersectionality is a critical framework that, through inquiry and praxis, allows us to examine how social inequalities persist through domains of structure and discipline. Given AI fairness' raison d'\^etre of ``fairness,'' we argue that adopting intersectionality as an analytical framework is pivotal to effectively operationalizing fairness. Through a critical review of how intersectionality is discussed in 30 papers from the AI fairness literature, we deductively and inductively: 1) map how intersectionality tenets operate within the AI fairness paradigm and 2) uncover gaps between the conceptualization and operationalization of intersectionality. We find that researchers overwhelmingly reduce intersectionality to optimizing for fairness metrics over demographic subgroups. They also fail to discuss their social context and when mentioning power, they mostly situate it only within the AI pipeline. We: 3) outline and assess the implications of these gaps for critical inquiry and prax
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35843;&#26597;&#39640;&#36136;&#37327;&#30340;&#20844;&#20849;&#27665;&#24847;&#35843;&#26597;&#26469;&#21019;&#24314;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;OpinionsQA&#65292;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#21453;&#26144;&#30340;&#35266;&#28857;&#19982;60&#20010;&#19981;&#21516;&#20154;&#21475;&#32479;&#35745;&#32452;&#30340;&#35266;&#28857;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#21457;&#29616;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#21453;&#26144;&#30340;&#35266;&#28857;&#19982;&#32654;&#22269;&#20154;&#32676;&#32452;&#30340;&#35266;&#28857;&#23384;&#22312;&#24040;&#22823;&#24046;&#24322;&#65292;&#29978;&#33267;&#36890;&#36807;&#26126;&#30830;&#35843;&#25972;LM&#21453;&#26144;&#20986;&#30340;&#35266;&#28857;&#65292;&#20173;&#28982;&#26080;&#27861;&#28040;&#38500;&#12290;</title><link>http://arxiv.org/abs/2303.17548</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21453;&#26144;&#20102;&#35841;&#30340;&#35266;&#28857;&#65311;
&lt;/p&gt;
&lt;p&gt;
Whose Opinions Do Language Models Reflect?. (arXiv:2303.17548v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35843;&#26597;&#39640;&#36136;&#37327;&#30340;&#20844;&#20849;&#27665;&#24847;&#35843;&#26597;&#26469;&#21019;&#24314;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;OpinionsQA&#65292;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#21453;&#26144;&#30340;&#35266;&#28857;&#19982;60&#20010;&#19981;&#21516;&#20154;&#21475;&#32479;&#35745;&#32452;&#30340;&#35266;&#28857;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#21457;&#29616;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#21453;&#26144;&#30340;&#35266;&#28857;&#19982;&#32654;&#22269;&#20154;&#32676;&#32452;&#30340;&#35266;&#28857;&#23384;&#22312;&#24040;&#22823;&#24046;&#24322;&#65292;&#29978;&#33267;&#36890;&#36807;&#26126;&#30830;&#35843;&#25972;LM&#21453;&#26144;&#20986;&#30340;&#35266;&#28857;&#65292;&#20173;&#28982;&#26080;&#27861;&#28040;&#38500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#24320;&#25918;&#29615;&#22659;&#20013;&#34987;&#20351;&#29992;&#65292;&#22312;&#38024;&#23545;&#20027;&#35266;&#26597;&#35810;&#30340;&#21709;&#24212;&#20013;&#21453;&#26144;&#30340;&#35266;&#28857;&#21487;&#33021;&#20250;&#23545;&#29992;&#25143;&#28385;&#24847;&#24230;&#20135;&#29983;&#28145;&#36828;&#24433;&#21709;&#65292;&#21516;&#26102;&#20063;&#21487;&#33021;&#22609;&#36896;&#25972;&#20010;&#31038;&#20250;&#30340;&#35266;&#28857;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23450;&#37327;&#26694;&#26550;&#65292;&#20197;&#35843;&#26597;LM&#21453;&#26144;&#30340;&#35266;&#28857;&#12290;&#25105;&#20204;&#21033;&#29992;&#39640;&#36136;&#37327;&#30340;&#20844;&#20849;&#27665;&#24847;&#35843;&#26597;&#21644;&#30456;&#20851;&#30340;&#20154;&#31867;&#21453;&#24212;&#26469;&#21019;&#24314;OpinionsQA&#65292;&#24182;&#23545;60&#20010;&#32654;&#22269;&#20154;&#21475;&#32479;&#35745;&#32452;&#30340;&#24847;&#35265;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#28041;&#21450;&#20174;&#22549;&#32974;&#21040;&#33258;&#21160;&#21270;&#30340;&#21508;&#31181;&#35805;&#39064;&#12290;&#22312;&#21508;&#20010;&#35805;&#39064;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;LM&#21453;&#26144;&#30340;&#35266;&#28857;&#19982;&#32654;&#22269;&#20154;&#32676;&#32452;&#20043;&#38388;&#23384;&#22312;&#37325;&#22823;&#24046;&#24322;&#65292;&#36825;&#19982;&#27665;&#20027;&#20826;&#21644;&#20849;&#21644;&#20826;&#22312;&#27668;&#20505;&#21464;&#21270;&#38382;&#39064;&#19978;&#30340;&#20998;&#27495;&#24046;&#19981;&#22810;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21363;&#20351;&#26126;&#30830;&#23558;LM&#23450;&#21521;&#20110;&#29305;&#23450;&#30340;&#20154;&#21475;&#32479;&#35745;&#32452;&#65292;&#36825;&#31181;&#24046;&#24322;&#20173;&#28982;&#23384;&#22312;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#19981;&#20165;&#30830;&#35748;&#20102;&#20808;&#21069;&#23545;&#24038;&#20542;&#20542;&#21521;&#30340;&#35266;&#23519;&#32467;&#26524;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#36825;&#31181;&#24046;&#24322;&#30340;&#19968;&#20010;&#20840;&#26032;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) are increasingly being used in open-ended contexts, where the opinions reflected by LMs in response to subjective queries can have a profound impact, both on user satisfaction, as well as shaping the views of society at large. In this work, we put forth a quantitative framework to investigate the opinions reflected by LMs -- by leveraging high-quality public opinion polls and their associated human responses. Using this framework, we create OpinionsQA, a new dataset for evaluating the alignment of LM opinions with those of 60 US demographic groups over topics ranging from abortion to automation. Across topics, we find substantial misalignment between the views reflected by current LMs and those of US demographic groups: on par with the Democrat-Republican divide on climate change. Notably, this misalignment persists even after explicitly steering the LMs towards particular demographic groups. Our analysis not only confirms prior observations about the left-leaning
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#32467;&#26500;&#21644;&#22806;&#35266;&#37197;&#23545;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#23545;&#35937;&#32423;&#22270;&#20687;&#32534;&#36753;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#31934;&#32454;&#25511;&#21046;&#22270;&#20687;&#20013;&#30340;&#19981;&#21516;&#23545;&#35937;&#23646;&#24615;&#65292;&#21516;&#26102;&#33258;&#21160;&#20256;&#25773;&#27880;&#20837;&#30340;&#22806;&#35266;&#21040;&#20855;&#26377;&#30456;&#20284;&#32467;&#26500;&#30340;&#23545;&#35937;&#12290;</title><link>http://arxiv.org/abs/2303.17546</link><description>&lt;p&gt;
PAIR-Diffusion: &#37319;&#29992;&#32467;&#26500;&#21644;&#22806;&#35266;&#37197;&#23545;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#23545;&#35937;&#32423;&#22270;&#20687;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
PAIR-Diffusion: Object-Level Image Editing with Structure-and-Appearance Paired Diffusion Models. (arXiv:2303.17546v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#32467;&#26500;&#21644;&#22806;&#35266;&#37197;&#23545;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#23545;&#35937;&#32423;&#22270;&#20687;&#32534;&#36753;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#31934;&#32454;&#25511;&#21046;&#22270;&#20687;&#20013;&#30340;&#19981;&#21516;&#23545;&#35937;&#23646;&#24615;&#65292;&#21516;&#26102;&#33258;&#21160;&#20256;&#25773;&#27880;&#20837;&#30340;&#22806;&#35266;&#21040;&#20855;&#26377;&#30456;&#20284;&#32467;&#26500;&#30340;&#23545;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#22270;&#20687;&#32534;&#36753;&#21457;&#23637;&#36805;&#36895;&#12290;&#20197;&#21069;&#30340;&#20316;&#21697;&#21487;&#20197;&#36890;&#36807;&#21508;&#31181;&#26041;&#24335;&#36827;&#34892;&#25511;&#21046;&#21644;&#32534;&#36753;&#22270;&#20687;&#65292;&#26576;&#20123;&#20316;&#21697;&#20351;&#29992;&#39640;&#32423;&#26465;&#20214;&#65288;&#20363;&#22914;&#25991;&#26412;&#65289;&#65292;&#32780;&#20854;&#20182;&#20316;&#21697;&#20351;&#29992;&#20302;&#32423;&#26465;&#20214;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20316;&#21697;&#32570;&#20047;&#23545;&#22270;&#20687;&#20013;&#19981;&#21516;&#23545;&#35937;&#30340;&#23646;&#24615;&#36827;&#34892;&#31934;&#32454;&#21270;&#25511;&#21046;&#65292;&#21363;&#23545;&#35937;&#32423;&#22270;&#20687;&#32534;&#36753;&#12290;&#26412;&#25991;&#23558;&#22270;&#20687;&#35270;&#20026;&#30001;&#22810;&#20010;&#23545;&#35937;&#32452;&#25104;&#65292;&#27599;&#20010;&#23545;&#35937;&#30001;&#19981;&#21516;&#23646;&#24615;&#23450;&#20041;&#12290;&#25105;&#20204;&#21457;&#29616;&#32467;&#26500;&#21644;&#22806;&#35266;&#26159;&#26368;&#30452;&#35266;&#19988;&#26368;&#26377;&#29992;&#20110;&#32534;&#36753;&#30340;&#23646;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#32467;&#26500;&#21644;&#22806;&#35266;&#37197;&#23545;&#25193;&#25955;&#27169;&#22411;&#65288;PAIR-Diffusion&#65289;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#20174;&#22270;&#20687;&#20013;&#26126;&#30830;&#25552;&#21462;&#30340;&#32467;&#26500;&#21644;&#22806;&#35266;&#20449;&#24687;&#36827;&#34892;&#35757;&#32451;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#20351;&#29992;&#25143;&#33021;&#22815;&#22312;&#23545;&#35937;&#21644;&#20840;&#23616;&#32423;&#21035;&#23558;&#21442;&#32771;&#22270;&#20687;&#30340;&#22806;&#35266;&#27880;&#20837;&#36755;&#20837;&#22270;&#20687;&#20013;&#12290;&#27492;&#22806;&#65292;PAIR-Diffusion&#33258;&#21160;&#23558;&#27880;&#20837;&#30340;&#22806;&#35266;&#20256;&#25773;&#21040;&#36755;&#20837;&#22270;&#20687;&#20013;&#20855;&#26377;&#31867;&#20284;&#32467;&#26500;&#30340;&#23545;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image editing using diffusion models has witnessed extremely fast-paced growth recently. There are various ways in which previous works enable controlling and editing images. Some works use high-level conditioning such as text, while others use low-level conditioning. Nevertheless, most of them lack fine-grained control over the properties of the different objects present in the image, i.e. object-level image editing. In this work, we consider an image as a composition of multiple objects, each defined by various properties. Out of these properties, we identify structure and appearance as the most intuitive to understand and useful for editing purposes. We propose Structure-and-Appearance Paired Diffusion model (PAIR-Diffusion), which is trained using structure and appearance information explicitly extracted from the images. The proposed model enables users to inject a reference image's appearance into the input image at both the object and global levels. Additionally, PAIR-Diffusion a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#35299;&#20915;&#37327;&#23376;&#35745;&#31639;&#20013;&#30340;&#20445;&#30495;&#24230;&#38382;&#39064;&#65292;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#39044;&#27979;&#37327;&#23376;&#30005;&#36335;&#30340;&#20445;&#30495;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.17523</link><description>&lt;p&gt;
&#21033;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#25552;&#39640;&#37327;&#23376;&#30005;&#36335;&#20445;&#30495;&#24230;
&lt;/p&gt;
&lt;p&gt;
Quantum Circuit Fidelity Improvement with Long Short-Term Memory Networks. (arXiv:2303.17523v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17523
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#35299;&#20915;&#37327;&#23376;&#35745;&#31639;&#20013;&#30340;&#20445;&#30495;&#24230;&#38382;&#39064;&#65292;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#39044;&#27979;&#37327;&#23376;&#30005;&#36335;&#30340;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#35745;&#31639;&#24050;&#36827;&#20837;&#22122;&#22768;&#20013;&#38388;&#35268;&#27169;&#37327;&#23376;&#65288;NISQ&#65289;&#26102;&#20195;&#65292;&#30446;&#21069;&#25105;&#20204;&#25317;&#26377;&#30340;&#37327;&#23376;&#22788;&#29702;&#22120;&#23545;&#36752;&#23556;&#21644;&#28201;&#24230;&#31561;&#29615;&#22659;&#21464;&#37327;&#25935;&#24863;&#65292;&#22240;&#27492;&#20250;&#20135;&#29983;&#22024;&#26434;&#30340;&#36755;&#20986;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#35768;&#22810;&#31639;&#27861;&#21644;&#24212;&#29992;&#31243;&#24207;&#29992;&#20110;NISQ&#22788;&#29702;&#22120;&#65292;&#20294;&#25105;&#20204;&#20173;&#38754;&#20020;&#30528;&#35299;&#37322;&#20854;&#22024;&#26434;&#32467;&#26524;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23545;&#25152;&#36873;&#25321;&#30340;&#37327;&#23376;&#24577;&#26377;&#22810;&#23569;&#20449;&#24515;&#65311;&#36825;&#31181;&#20449;&#24515;&#24456;&#37325;&#35201;&#65292;&#22240;&#20026;NISQ&#35745;&#31639;&#26426;&#23558;&#36755;&#20986;&#20854;&#37327;&#23376;&#20301;&#27979;&#37327;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#26377;&#26102;&#24456;&#38590;&#21306;&#20998;&#20998;&#24067;&#26159;&#21542;&#34920;&#31034;&#26377;&#24847;&#20041;&#30340;&#35745;&#31639;&#25110;&#21482;&#26159;&#38543;&#26426;&#22122;&#22768;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23558;&#37327;&#23376;&#30005;&#36335;&#20445;&#30495;&#24230;&#39044;&#27979;&#26694;&#26550;&#20026;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#38382;&#39064;&#65292;&#22240;&#27492;&#21487;&#20197;&#21033;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#19968;&#20010;&#23436;&#25972;&#30340;&#24037;&#20316;&#27969;&#31243;&#26469;&#26500;&#24314;&#35757;&#32451;&#30005;&#36335;
&lt;/p&gt;
&lt;p&gt;
Quantum computing has entered the Noisy Intermediate-Scale Quantum (NISQ) era. Currently, the quantum processors we have are sensitive to environmental variables like radiation and temperature, thus producing noisy outputs. Although many proposed algorithms and applications exist for NISQ processors, we still face uncertainties when interpreting their noisy results. Specifically, how much confidence do we have in the quantum states we are picking as the output? This confidence is important since a NISQ computer will output a probability distribution of its qubit measurements, and it is sometimes hard to distinguish whether the distribution represents meaningful computation or just random noise. This paper presents a novel approach to attack this problem by framing quantum circuit fidelity prediction as a Time Series Forecasting problem, therefore making it possible to utilize the power of Long Short-Term Memory (LSTM) neural networks. A complete workflow to build the training circuit d
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36229;&#36234;&#20154;&#31867;&#34920;&#29616;&#65292;&#20294;&#36807;&#24230;&#20381;&#36182;&#21487;&#33021;&#20250;&#23548;&#33268;&#20005;&#37325;&#21518;&#26524;&#65292;&#21253;&#25324;&#38590;&#20197;&#21306;&#20998;&#30340;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#21644;&#21508;&#31181;&#24418;&#24335;&#30340;&#27450;&#35784;&#65292;&#36827;&#32780;&#20135;&#29983;&#26032;&#30340;&#20262;&#29702;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2303.17511</link><description>&lt;p&gt;
&#20851;&#20110;&#22797;&#26434;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21155;&#65288;&#22353;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
On pitfalls (and advantages) of sophisticated large language models. (arXiv:2303.17511v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17511
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36229;&#36234;&#20154;&#31867;&#34920;&#29616;&#65292;&#20294;&#36807;&#24230;&#20381;&#36182;&#21487;&#33021;&#20250;&#23548;&#33268;&#20005;&#37325;&#21518;&#26524;&#65292;&#21253;&#25324;&#38590;&#20197;&#21306;&#20998;&#30340;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#21644;&#21508;&#31181;&#24418;&#24335;&#30340;&#27450;&#35784;&#65292;&#36827;&#32780;&#20135;&#29983;&#26032;&#30340;&#20262;&#29702;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26159;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#19968;&#20010;&#34028;&#21187;&#21457;&#23637;&#30340;&#39046;&#22495;&#12290;&#22312;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#22312;&#22522;&#20110;&#27169;&#24335;&#35782;&#21035;&#30340;&#28216;&#25103;&#21644;&#23454;&#38469;&#39046;&#22495;&#20013;&#35777;&#26126;&#36229;&#36234;&#20154;&#31867;&#34920;&#29616;&#21518;&#65292;&#25105;&#20204;&#29616;&#22312;&#21487;&#33021;&#22788;&#20110;&#19968;&#20010;&#20154;&#24037;&#23454;&#20307;&#26368;&#32456;&#36827;&#20837;&#20154;&#31867;&#20132;&#27969;&#39046;&#22495;&#30340;&#21313;&#23383;&#36335;&#21475;&#12290;&#28982;&#32780;&#65292;&#36825;&#20063;&#24102;&#26469;&#20102;&#20005;&#37325;&#30340;&#39118;&#38505;&#12290;&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#21487;&#38752;&#24615;&#22266;&#26377;&#30340;&#38480;&#21046;&#65292;&#36807;&#24230;&#20381;&#36182;LLMs&#21487;&#33021;&#24102;&#26469;&#30772;&#22351;&#24615;&#21518;&#26524;&#12290;&#30001;&#20110;&#21306;&#20998;&#20154;&#31867;&#20070;&#20889;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#23558;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#65292;&#20154;&#20204;&#38754;&#20020;&#30528;&#26032;&#30340;&#20262;&#29702;&#25361;&#25112;&#12290;&#20174;&#19981;&#20877;&#26126;&#30830;&#21487;&#39564;&#35777;&#30340;&#20154;&#31867;&#20316;&#32773;&#36523;&#20221;&#24320;&#22987;&#65292;&#32487;&#32493;&#28041;&#21450;&#21508;&#31181;&#31867;&#22411;&#30340;&#27450;&#35784;&#65292;&#20363;&#22914;&#26032;&#24418;&#24335;&#30340;&#21117;&#31363;&#12290;&#36825;&#36824;&#28041;&#21450;&#20405;&#29359;&#38544;&#31169;&#26435;&#65292;&#21487;&#33021;&#20256;&#25773;&#20154;&#31867;&#20266;&#36896;&#21697;&#65292;&#26368;&#21518;&#20294;&#21516;&#26679;&#37325;&#35201;&#30340;&#26159;&#65292;&#23427;&#20351;&#22823;&#35268;&#27169;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language processing based on large language models (LLMs) is a booming field of AI research. After neural networks have proven to outperform humans in games and practical domains based on pattern recognition, we might stand now at a road junction where artificial entities might eventually enter the realm of human communication. However, this comes with serious risks. Due to the inherent limitations regarding the reliability of neural networks, overreliance on LLMs can have disruptive consequences. Since it will be increasingly difficult to distinguish between human-written and machine-generated text, one is confronted with new ethical challenges. This begins with the no longer undoubtedly verifiable human authorship and continues with various types of fraud, such as a new form of plagiarism. This also concerns the violation of privacy rights, the possibility of circulating counterfeits of humans, and, last but not least, it makes a massive spread of misinformation possible.
&lt;/p&gt;</description></item><item><title>AI&#21487;&#33021;&#20855;&#26377;&#21487;&#20105;&#35758;&#30340;&#20154;&#26684;&#65292;&#22240;&#27492;&#65292;&#23558;&#23427;&#20204;&#30475;&#20316;&#36947;&#24503;&#19978;&#30340;&#20154;&#23384;&#22312;&#20005;&#37325;&#39118;&#38505;&#65292;&#19981;&#23558;&#23427;&#20204;&#35270;&#20026;&#36947;&#24503;&#19978;&#30340;&#20154;&#21448;&#20250;&#26377;&#36947;&#24503;&#38169;&#35823;&#65292;&#36825;&#26159;&#19968;&#20010;&#20005;&#37325;&#30340;&#36947;&#24503;&#22256;&#22659;</title><link>http://arxiv.org/abs/2303.17509</link><description>&lt;p&gt;
&#35770;&#21487;&#20105;&#35758;&#20154;&#26684;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#30340;&#20840;&#37096;&#26435;&#21033;&#22256;&#22659;
&lt;/p&gt;
&lt;p&gt;
The Full Rights Dilemma for A.I. Systems of Debatable Personhood. (arXiv:2303.17509v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17509
&lt;/p&gt;
&lt;p&gt;
AI&#21487;&#33021;&#20855;&#26377;&#21487;&#20105;&#35758;&#30340;&#20154;&#26684;&#65292;&#22240;&#27492;&#65292;&#23558;&#23427;&#20204;&#30475;&#20316;&#36947;&#24503;&#19978;&#30340;&#20154;&#23384;&#22312;&#20005;&#37325;&#39118;&#38505;&#65292;&#19981;&#23558;&#23427;&#20204;&#35270;&#20026;&#36947;&#24503;&#19978;&#30340;&#20154;&#21448;&#20250;&#26377;&#36947;&#24503;&#38169;&#35823;&#65292;&#36825;&#26159;&#19968;&#20010;&#20005;&#37325;&#30340;&#36947;&#24503;&#22256;&#22659;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#23384;&#22312;&#19968;&#31181;&#35748;&#30693;&#19978;&#30340;&#21487;&#33021;&#24615;&#65292;&#21363;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65288;AI&#65289;&#21487;&#20197;&#26159;&#20154;&#65292;&#20063;&#21487;&#20197;&#36828;&#36828;&#19981;&#21450;&#20154;&#31867;&#65292;&#37027;&#20040;AI&#20855;&#26377;&#21487;&#20105;&#35758;&#30340;&#20154;&#26684;&#12290;AI&#23384;&#22312;&#20105;&#35758;&#24615;&#20154;&#26684;&#26159;AI&#21457;&#23637;&#30340;&#21487;&#33021;&#32467;&#26524;&#65292;&#21487;&#33021;&#24456;&#24555;&#23601;&#20250;&#20986;&#29616;&#12290;&#23384;&#22312;&#20105;&#35758;&#24615;&#30340;AI&#20154;&#26684;&#23558;&#25105;&#20204;&#32622;&#20110;&#20005;&#37325;&#30340;&#36947;&#24503;&#22256;&#22659;&#20013;&#65306;&#35201;&#20040;&#23558;&#31995;&#32479;&#35270;&#20026;&#36947;&#24503;&#19978;&#30340;&#20154;&#65292;&#20882;&#30528;&#20026;&#20102;&#19981;&#20540;&#24471;&#29306;&#29298;&#30340;&#23454;&#20307;&#32780;&#29306;&#29298;&#30495;&#27491;&#30340;&#20154;&#31867;&#21033;&#30410;&#30340;&#39118;&#38505;&#65292;&#35201;&#20040;&#19981;&#23558;&#31995;&#32479;&#35270;&#20026;&#36947;&#24503;&#19978;&#30340;&#20154;&#65292;&#20882;&#30528;&#23545;&#23427;&#20204;&#36827;&#34892;&#20005;&#37325;&#30340;&#36947;&#24503;&#38169;&#35823;&#12290;&#22914;&#26524;&#32771;&#34385;&#20855;&#26377;&#21487;&#33021;&#26377;&#24847;&#35782;&#12289;&#20122;&#20154;&#31867;&#12289;&#36229;&#20154;&#31867;&#25110;&#22312;&#20854;&#36947;&#24503;&#30456;&#20851;&#23646;&#24615;&#19978;&#39640;&#24230;&#24046;&#24322;&#21270;&#30340;&#24773;&#20917;&#65292;&#36947;&#24503;&#38382;&#39064;&#23558;&#21464;&#24471;&#26356;&#21152;&#26840;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;
An Artificially Intelligent system (an AI) has debatable personhood if it's epistemically possible either that the AI is a person or that it falls far short of personhood. Debatable personhood is a likely outcome of AI development and might arise soon. Debatable AI personhood throws us into a catastrophic moral dilemma: Either treat the systems as moral persons and risk sacrificing real human interests for the sake of entities without interests worth the sacrifice, or don't treat the systems as moral persons and risk perpetrating grievous moral wrongs against them. The moral issues become even more perplexing if we consider cases of possibly conscious AI that are subhuman, superhuman, or highly divergent from us in their morally relevant properties.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#32422;&#26463;&#30340;&#35270;&#35273;&#34920;&#31034;&#30340;&#22240;&#23376;&#39046;&#22495;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20419;&#36827;&#22312;&#22240;&#23376;&#39046;&#22495;&#20013;RL&#20219;&#21153;&#30340;&#23398;&#20064;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.17508</link><description>&lt;p&gt;
&#22522;&#20110;&#20449;&#24687;&#21463;&#38480;&#35270;&#35273;&#34920;&#31034;&#30340;&#22240;&#23376;&#39046;&#22495;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning in Factored Domains with Information-Constrained Visual Representations. (arXiv:2303.17508v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#32422;&#26463;&#30340;&#35270;&#35273;&#34920;&#31034;&#30340;&#22240;&#23376;&#39046;&#22495;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20419;&#36827;&#22312;&#22240;&#23376;&#39046;&#22495;&#20013;RL&#20219;&#21153;&#30340;&#23398;&#20064;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21363;&#20351;&#22312;&#21253;&#21547;&#22797;&#26434;&#35270;&#35273;&#20449;&#24687;&#30340;&#20219;&#21153;&#20013;&#20063;&#33021;&#22815;&#24555;&#36895;&#23398;&#20064;&#12290;&#36825;&#37096;&#20998;&#26159;&#30001;&#20110;&#35270;&#35273;&#20449;&#24687;&#30340;&#39640;&#25928;&#21387;&#32553;&#24418;&#25104;&#65292;&#20174;&#32780;&#21487;&#20197;&#26356;&#22909;&#22320;&#25512;&#24191;&#21644;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#20165;&#26377;&#21387;&#32553;&#34920;&#31034;&#26159;&#26080;&#27861;&#35299;&#37322;&#20154;&#31867;&#23398;&#20064;&#39640;&#36895;&#24230;&#30340;&#21407;&#22240;&#30340;&#12290;&#23547;&#27714;&#22797;&#21046;&#36825;&#31181;&#21360;&#35937;&#30340;&#25928;&#29575;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#27169;&#22411;&#21487;&#33021;&#36890;&#36807;&#20351;&#29992;&#20219;&#21153;&#30340;&#22240;&#23376;&#34920;&#31034;&#26469;&#23454;&#29616;&#12290;&#36825;&#20123;&#20449;&#24687;&#31616;&#21333;&#30340;&#20219;&#21153;&#34920;&#31034;&#19982;&#35270;&#35273;&#20449;&#24687;&#30340;&#21387;&#32553;&#34920;&#31034;&#30340;&#20351;&#29992;&#31867;&#20284;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23558;&#29983;&#29289;&#35270;&#35273;&#24863;&#30693;&#19982;&#20998;&#31163;&#30340;&#21387;&#32553;&#34920;&#31034;&#32852;&#31995;&#36215;&#26469;&#12290;&#36825;&#24341;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#20154;&#31867;&#22914;&#20309;&#23398;&#20064;&#26377;&#25928;&#22320;&#34920;&#36798;&#26377;&#21161;&#20110;&#23398;&#20064;&#20219;&#21153;&#30340;&#35270;&#35273;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;$\beta$-&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#30340;&#25913;&#21464;&#24418;&#24335;&#30340;&#20154;&#31867;&#22240;&#23376;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20801;&#35768;&#35270;&#35273;&#20449;&#24687;&#30340;&#34920;&#31034;&#25353;&#29031;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#36827;&#34892;&#22609;&#36896;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#20197;&#19968;&#31181;&#20419;&#36827;&#22312;&#22240;&#23376;&#39046;&#22495;&#20013;RL&#20219;&#21153;&#26356;&#24555;&#23398;&#20064;&#30340;&#26041;&#24335;&#26377;&#25928;&#22320;&#34920;&#31034;&#35270;&#35273;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans learn quickly even in tasks that contain complex visual information. This is due in part to the efficient formation of compressed representations of visual information, allowing for better generalization and robustness. However, compressed representations alone are insufficient for explaining the high speed of human learning. Reinforcement learning (RL) models that seek to replicate this impressive efficiency may do so through the use of factored representations of tasks. These informationally simplistic representations of tasks are similarly motivated as the use of compressed representations of visual information. Recent studies have connected biological visual perception to disentangled and compressed representations. This raises the question of how humans learn to efficiently represent visual information in a manner useful for learning tasks. In this paper we present a model of human factored representation learning based on an altered form of a $\beta$-Variational Auto-encod
&lt;/p&gt;</description></item><item><title>Pgx&#26159;&#19968;&#20010;&#29992;JAX&#32534;&#20889;&#30340;&#28216;&#25103;&#27169;&#25311;&#22120;&#38598;&#21512;&#65292;&#20855;&#26377;&#24378;&#21270;&#23398;&#20064;&#30828;&#20214;&#21152;&#36895;&#33021;&#21147;&#65292;&#25903;&#25345;&#24182;&#34892;&#25191;&#34892;&#65292;&#36895;&#24230;&#27604;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#24211;&#24555;10&#20493;&#12290; &#23427;&#23454;&#29616;&#20102;Backgammon&#65292;Shogi&#21644;Go&#31561;&#22522;&#20934;&#27979;&#35797;&#28216;&#25103;&#12290;</title><link>http://arxiv.org/abs/2303.17503</link><description>&lt;p&gt;
Pgx:&#24378;&#21270;&#23398;&#20064;&#30828;&#20214;&#21152;&#36895;&#30340;&#24182;&#34892;&#28216;&#25103;&#27169;&#25311;&#22120;
&lt;/p&gt;
&lt;p&gt;
Pgx: Hardware-accelerated parallel game simulation for reinforcement learning. (arXiv:2303.17503v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17503
&lt;/p&gt;
&lt;p&gt;
Pgx&#26159;&#19968;&#20010;&#29992;JAX&#32534;&#20889;&#30340;&#28216;&#25103;&#27169;&#25311;&#22120;&#38598;&#21512;&#65292;&#20855;&#26377;&#24378;&#21270;&#23398;&#20064;&#30828;&#20214;&#21152;&#36895;&#33021;&#21147;&#65292;&#25903;&#25345;&#24182;&#34892;&#25191;&#34892;&#65292;&#36895;&#24230;&#27604;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#24211;&#24555;10&#20493;&#12290; &#23427;&#23454;&#29616;&#20102;Backgammon&#65292;Shogi&#21644;Go&#31561;&#22522;&#20934;&#27979;&#35797;&#28216;&#25103;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Pgx&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;JAX&#32534;&#20889;&#30340;&#26827;&#30424;&#28216;&#25103;&#27169;&#25311;&#22120;&#38598;&#21512;&#12290;&#30001;&#20110;JAX&#30340;&#33258;&#21160;&#21521;&#37327;&#21270;&#21644;&#21363;&#26102;&#32534;&#35793;&#21151;&#33021;&#65292;Pgx&#26131;&#20110;&#22312;GPU/TPU&#21152;&#36895;&#22120;&#19978;&#36827;&#34892;&#22823;&#35268;&#27169;&#24182;&#34892;&#25191;&#34892;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#21333;&#20010;A100 GPU&#19978;&#30340;Pgx&#27169;&#25311;&#27604;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#24211;&#24555;10&#20493;&#12290;Pgx&#23454;&#29616;&#20102;&#34987;&#35748;&#20026;&#26159;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#22522;&#20934;&#27979;&#35797;&#30340;&#28216;&#25103;&#65292;&#22914;Backgammon&#65292;Shogi&#21644;Go&#12290; Pgx&#21487;&#22312;https://github.com/sotetsuk/pgx&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Pgx, a collection of board game simulators written in JAX. Thanks to auto-vectorization and Just-In-Time compilation of JAX, Pgx scales easily to thousands of parallel execution on GPU/TPU accelerators. We found that the simulation of Pgx on a single A100 GPU is 10x faster than that of existing reinforcement learning libraries. Pgx implements games considered vital benchmarks in artificial intelligence research, such as Backgammon, Shogi, and Go. Pgx is available at https://github.com/sotetsuk/pgx.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22478;&#24066;&#20132;&#36890;&#30340;&#28151;&#21512;&#36335;&#21475;&#30340;&#30333;&#30418;&#24847;&#22270;&#24863;&#30693;&#20915;&#31574;&#21046;&#23450;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#34892;&#20154;&#21644;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#20197;&#25552;&#39640;AV&#30340;&#25509;&#21463;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.17493</link><description>&lt;p&gt;
&#28151;&#21512;&#36335;&#21475;&#22330;&#26223;&#20013;&#30340;&#24847;&#22270;&#24863;&#30693;&#20915;&#31574;&#21046;&#23450;
&lt;/p&gt;
&lt;p&gt;
Intention-Aware Decision-Making for Mixed Intersection Scenarios. (arXiv:2303.17493v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17493
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22478;&#24066;&#20132;&#36890;&#30340;&#28151;&#21512;&#36335;&#21475;&#30340;&#30333;&#30418;&#24847;&#22270;&#24863;&#30693;&#20915;&#31574;&#21046;&#23450;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#34892;&#20154;&#21644;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#20197;&#25552;&#39640;AV&#30340;&#25509;&#21463;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38754;&#21521;&#30333;&#30418;&#30340;&#24847;&#22270;&#24863;&#30693;&#20915;&#31574;&#21046;&#23450;&#65292;&#29992;&#20110;&#22788;&#29702;&#26410;&#20449;&#21495;&#21270;&#30340;&#36947;&#36335;&#20132;&#21449;&#21475;&#22330;&#26223;&#19979;&#30340;&#34892;&#20154;&#21644;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AV&#65289;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#27492;&#22806;&#65292;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#35774;&#35745;&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#21442;&#25968;&#21270;&#20915;&#31574;&#21046;&#23450;&#12290;&#35813;&#20915;&#31574;&#21046;&#23450;&#34987;&#35774;&#35745;&#20026;&#33021;&#22815;&#29702;&#35299;&#22478;&#24066;&#20132;&#36890;&#20013;&#30340;&#34892;&#20154;&#65292;&#24182;&#33021;&#26681;&#25454;&#20182;&#20204;&#30340;&#24847;&#22270;&#20570;&#20986;&#21453;&#24212;&#12290;&#36825;&#26679;&#21487;&#20197;&#30830;&#20445;&#23545;&#34892;&#20154;&#21160;&#20316;&#30340;&#31867;&#20154;&#21453;&#24212;&#65292;&#20174;&#32780;&#25552;&#39640;AV&#30340;&#25509;&#21463;&#24230;&#12290;&#26412;&#25991;&#30340;&#26680;&#24515;&#27010;&#24565;&#26159;&#23558;&#34892;&#20154;&#30340;&#24847;&#22270;&#39044;&#27979;&#21644;&#20915;&#31574;&#21046;&#23450;&#21010;&#20998;&#20026;&#20004;&#20010;&#23376;&#31995;&#32479;&#12290;&#19968;&#26041;&#38754;&#65292;&#24847;&#22270;&#26816;&#27979;&#26159;&#19968;&#20010;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#40657;&#30418;&#27169;&#22411;&#65292;&#22240;&#27492;&#23427;&#21487;&#20197;&#27169;&#25311;&#34892;&#20154;&#30340;&#22797;&#26434;&#34892;&#20026;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20915;&#31574;&#21046;&#23450;&#26159;&#19968;&#20010;&#30333;&#30418;&#27169;&#22411;&#65292;&#20197;&#30830;&#20445;&#21487;&#36861;&#36394;&#24615;&#65292;&#24182;&#21551;&#29992;AV&#30340;&#24555;&#36895;&#39564;&#35777;&#21644;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a white-box intention-aware decision-making for the handling of interactions between a pedestrian and an automated vehicle (AV) in an unsignalized street crossing scenario. Moreover, a design framework has been developed, which enables automated parameterization of the decision-making. This decision-making is designed in such a manner that it can understand pedestrians in urban traffic and can react accordingly to their intentions. That way, a human-like response to the actions of the pedestrian is ensured, leading to a higher acceptance of AVs. The core notion of this paper is that the intention prediction of the pedestrian to cross the street and decision-making are divided into two subsystems. On the one hand, the intention detection is a data-driven, black-box model. Thus, it can model the complex behavior of the pedestrians. On the other hand, the decision-making is a white-box model to ensure traceability and to enable a rapid verification and validation of AV
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#30340;&#25552;&#31034;&#26041;&#26696;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25191;&#34892;&#35745;&#31639;&#26426;&#20219;&#21153;&#65292;&#35813;&#26041;&#27861;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#24182;&#22312;MiniWoB++&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#20102;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.17491</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#35299;&#20915;&#35745;&#31639;&#26426;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Language Models can Solve Computer Tasks. (arXiv:2303.17491v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17491
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#30340;&#25552;&#31034;&#26041;&#26696;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25191;&#34892;&#35745;&#31639;&#26426;&#20219;&#21153;&#65292;&#35813;&#26041;&#27861;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#24182;&#22312;MiniWoB++&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#20102;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#22312;&#35745;&#31639;&#26426;&#19978;&#25191;&#34892;&#36890;&#29992;&#20219;&#21153;&#30340;&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#33258;&#21160;&#21270;&#37325;&#22797;&#20219;&#21153;&#21644;&#21327;&#21161;&#22797;&#26434;&#38382;&#39064;&#30340;&#35299;&#20915;&#26469;&#25552;&#39640;&#25928;&#29575;&#21644;&#29983;&#20135;&#21147;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#20195;&#29702;&#24212;&#35813;&#33021;&#22815;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#21629;&#20196;&#35299;&#20915;&#26032;&#30340;&#35745;&#31639;&#26426;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#19987;&#23478;&#31034;&#33539;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#36825;&#20004;&#32773;&#23545;&#20110;&#26032;&#20219;&#21153;&#26469;&#35828;&#37117;&#19981;&#20999;&#23454;&#38469;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#21487;&#20197;&#20351;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#25552;&#31034;&#26041;&#26696;&#65288;RCI&#65289;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#23548;&#25191;&#34892;&#35745;&#31639;&#26426;&#20219;&#21153;&#65292;&#24182;&#22312;&#25209;&#35780;&#21644;&#25913;&#36827;&#36755;&#20986;&#30340;&#36807;&#31243;&#20013;&#21462;&#24471;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;RCI&#26041;&#27861;&#22312;&#33258;&#21160;&#21270;&#35745;&#31639;&#26426;&#20219;&#21153;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;LLM&#26041;&#27861;&#65292;&#24182;&#22312;MiniWoB++&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#20102;&#30417;&#30563;&#23398;&#20064;&#65288;SL&#65289;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#12290;RCI&#26041;&#27861;&#20351;&#29992;&#27599;&#20010;&#20219;&#21153;&#20165;&#26377;&#30340;&#23569;&#25968;&#31034;&#33539;&#65292;&#19982;&#26368;&#26032;&#30340;SL+RL&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
Agents capable of carrying out general tasks on a computer can improve efficiency and productivity by automating repetitive tasks and assisting in complex problem-solving. Ideally, such agents should be able to solve new computer tasks presented to them through natural language commands. However, previous approaches to this problem require large amounts of expert demonstrations and task-specific reward functions, both of which are impractical for new tasks. In this work, we show that a pre-trained large language model (LLM) agent can execute computer tasks guided by natural language using a simple prompting scheme where the agent recursively criticizes and improves its output (RCI). The RCI approach significantly outperforms existing LLM methods for automating computer tasks and surpasses supervised learning (SL) and reinforcement learning (RL) approaches on the MiniWoB++ benchmark. RCI is competitive with the state-of-the-art SL+RL method, using only a handful of demonstrations per ta
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20195;&#20215;&#25935;&#24863;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;CSGNN&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#31227;&#21160;&#31038;&#20132;&#32593;&#32476;&#27450;&#35784;&#26816;&#27979;&#20013;&#30340;&#22270;&#20687;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#22312;&#20004;&#20010;&#24320;&#28304;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2303.17486</link><description>&lt;p&gt;
&#22522;&#20110;&#20195;&#20215;&#25935;&#24863;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31227;&#21160;&#31038;&#20132;&#32593;&#32476;&#27450;&#35784;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Cost Sensitive GNN-based Imbalanced Learning for Mobile Social Network Fraud Detection. (arXiv:2303.17486v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20195;&#20215;&#25935;&#24863;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;CSGNN&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#31227;&#21160;&#31038;&#20132;&#32593;&#32476;&#27450;&#35784;&#26816;&#27979;&#20013;&#30340;&#22270;&#20687;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#22312;&#20004;&#20010;&#24320;&#28304;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31227;&#21160;&#32593;&#32476;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20154;&#20204;&#30340;&#31038;&#20132;&#32852;&#31995;&#24471;&#21040;&#20102;&#26497;&#22823;&#30340;&#20415;&#21033;&#12290;&#28982;&#32780;&#65292;&#31227;&#21160;&#31038;&#20132;&#32593;&#32476;&#27450;&#35784;&#30340;&#20852;&#36215;&#32473;&#20154;&#20204;&#24102;&#26469;&#20102;&#24040;&#22823;&#22256;&#25200;&#65292;&#21487;&#33021;&#36896;&#25104;&#20010;&#20154;&#21644;&#31038;&#20250;&#36130;&#23500;&#30340;&#25439;&#22833;&#65292;&#24182;&#28508;&#22312;&#22320;&#23545;&#32463;&#27982;&#36896;&#25104;&#37325;&#22823;&#25439;&#23475;&#12290;&#20026;&#20102;&#26816;&#27979;&#27450;&#35784;&#29992;&#25143;&#65292;&#24191;&#27867;&#20351;&#29992;&#21453;&#26144;&#29992;&#25143;&#31227;&#21160;&#32593;&#32476;&#20013;&#31038;&#20132;&#34892;&#20026;&#30340;&#36890;&#35805;&#35814;&#21333;&#35760;&#24405;&#65288;CDR&#65289;&#25968;&#25454;&#12290;&#20294;&#26159;&#65292;&#19978;&#36848;&#25968;&#25454;&#20013;&#30340;&#19981;&#24179;&#34913;&#38382;&#39064;&#21487;&#33021;&#20005;&#37325;&#38459;&#30861;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#27450;&#35784;&#26816;&#27979;&#22120;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#20960;&#20046;&#27809;&#26377;&#24471;&#21040;&#35299;&#20915;&#12290;&#26412;&#25991;&#23558;&#21019;&#36896;&#24615;&#22320;&#32467;&#21512;&#20195;&#20215;&#25935;&#24863;&#23398;&#20064;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20195;&#20215;&#25935;&#24863;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;CSGNN&#65289;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#24320;&#28304;&#23454;&#38469;&#31227;&#21160;&#32593;&#32476;&#27450;&#35784;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;CSGNN&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#31227;&#21160;&#31038;&#20132;&#32593;&#32476;&#27450;&#35784;&#26816;&#27979;&#20013;&#30340;&#22270;&#20687;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#22312;&#20960;&#20010;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#27169;&#22411;&#19978;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid development of mobile networks, the people's social contacts have been considerably facilitated. However, the rise of mobile social network fraud upon those networks, has caused a great deal of distress, in case of depleting personal and social wealth, then potentially doing significant economic harm. To detect fraudulent users, call detail record (CDR) data, which portrays the social behavior of users in mobile networks, has been widely utilized. But the imbalance problem in the aforementioned data, which could severely hinder the effectiveness of fraud detectors based on graph neural networks(GNN), has hardly been addressed in previous work. In this paper, we are going to present a novel Cost-Sensitive Graph Neural Network (CSGNN) by creatively combining cost-sensitive learning and graph neural networks. We conduct extensive experiments on two open-source realworld mobile network fraud datasets. The results show that CSGNN can effectively solve the graph imbalance prob
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23545;&#20132;&#36890;&#32593;&#32476;&#20013;&#36793;&#32536;&#36827;&#34892;&#35780;&#32423;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#24555;&#36895;&#20934;&#30830;&#22320;&#30830;&#23450;&#32593;&#32476;&#20013;&#30340;&#37325;&#35201;&#36793;&#32536;&#65292;&#32780;&#26080;&#38656;&#35745;&#31639;&#27599;&#20010;&#36793;&#32536;&#23545;&#30340;&#36793;&#32536;&#20171;&#25968;&#20013;&#24515;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.17485</link><description>&lt;p&gt;
&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23545;&#20132;&#36890;&#32593;&#32476;&#20013;&#30340;&#22270;&#36827;&#34892;&#36793;&#32536;&#35780;&#32423;
&lt;/p&gt;
&lt;p&gt;
Edge Ranking of Graphs in Transportation Networks using a Graph Neural Network (GNN). (arXiv:2303.17485v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17485
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23545;&#20132;&#36890;&#32593;&#32476;&#20013;&#36793;&#32536;&#36827;&#34892;&#35780;&#32423;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#24555;&#36895;&#20934;&#30830;&#22320;&#30830;&#23450;&#32593;&#32476;&#20013;&#30340;&#37325;&#35201;&#36793;&#32536;&#65292;&#32780;&#26080;&#38656;&#35745;&#31639;&#27599;&#20010;&#36793;&#32536;&#23545;&#30340;&#36793;&#32536;&#20171;&#25968;&#20013;&#24515;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#32593;&#32476;&#65292;&#20363;&#22914;&#20132;&#36890;&#12289;&#30005;&#21147;&#21644;&#20379;&#27700;&#20998;&#24067;&#65292;&#37117;&#21487;&#20197;&#34920;&#31034;&#20026;&#22270;&#24418;&#12290;&#22312;&#22270;&#24418;&#34920;&#31034;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#30830;&#23450;&#22270;&#24418;&#36793;&#32536;&#30340;&#37325;&#35201;&#24615;&#21450;&#20854;&#23545;&#25972;&#20307;&#32593;&#32476;&#25928;&#29575;&#21644;&#20449;&#24687;&#27969;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#20363;&#22914;&#65292;&#20132;&#36890;&#32593;&#32476;&#20013;&#30340;&#37325;&#35201;&#36793;&#32536;&#26159;&#21463;&#24433;&#21709;&#26102;&#20250;&#26174;&#33879;&#25913;&#21464;&#32593;&#32476;&#25972;&#20307;&#25928;&#29575;&#30340;&#36947;&#36335;&#12290;&#25214;&#21040;&#36825;&#26679;&#37325;&#35201;&#30340;&#36793;&#32536;&#30340;&#24120;&#29992;&#26041;&#27861;&#26159;&#8220;&#36793;&#32536;&#20171;&#25968;&#20013;&#24515;&#24230;&#8221;&#65288;EBC&#65289;&#65292;&#19968;&#31181;&#22522;&#20110;&#36830;&#36890;&#24615;&#21644;&#20449;&#24687;&#20256;&#25773;&#30830;&#23450;&#22270;&#30340;&#26377;&#24433;&#21709;&#21147;&#36793;&#32536;&#30340;&#36793;&#32536;&#25490;&#24207;&#24230;&#37327;&#12290;&#20351;&#29992;&#24120;&#35265;&#30340;Brandes&#31639;&#27861;&#35745;&#31639;EBC&#28041;&#21450;&#35745;&#31639;&#27599;&#20010;&#33410;&#28857;&#23545;&#30340;&#26368;&#30701;&#36335;&#24452;&#65292;&#36825;&#21487;&#33021;&#20250;&#22312;&#35745;&#31639;&#21644;&#38480;&#21046;&#22823;&#22411;&#22270;&#24418;&#26102;&#32791;&#36153;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#22270;&#24418;&#21442;&#25968;&#30340;&#26356;&#25913;&#65292;&#20363;&#22914;&#36793;&#32536;&#26435;&#37325;&#25110;&#33410;&#28857;&#25110;&#36793;&#32536;&#30340;&#28155;&#21152;&#21644;&#21024;&#38500;&#65292;&#38656;&#35201;&#37325;&#26032;&#35745;&#31639;EBC&#12290;&#20316;&#20026;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23545;&#20132;&#36890;&#32593;&#32476;&#30340;&#36793;&#32536;&#36827;&#34892;&#35780;&#20998;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#24555;&#36895;&#20934;&#30830;&#22320;&#35782;&#21035;&#32593;&#32476;&#20013;&#30340;&#36793;&#32536;&#37325;&#35201;&#24615;&#65292;&#24182;&#19988;&#19981;&#24517;&#20026;&#27599;&#20010;&#36793;&#32536;&#23545;&#35745;&#31639;EBC&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#22343;&#20248;&#20110;EBC&#26041;&#27861;&#65292;&#26080;&#35770;&#26159;&#22312;&#31934;&#24230;&#36824;&#26159;&#25928;&#29575;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many networks, such as transportation, power, and water distribution, can be represented as graphs. Crucial challenge in graph representations is identifying the importance of graph edges and their influence on overall network efficiency and information flow performance. For example, important edges in a transportation network are those roads that, when affected, will significantly alter the network's overall efficiency. Commonly used approach to finding such important edges is ``edge betweenness centrality'' (EBC), an edge ranking measure to determine the influential edges of the graph based on connectivity and information spread. Computing the EBC utilizing the common Brandes algorithm involves calculating the shortest paths for every node pair, which can be computationally expensive and restrictive, especially for large graphs. Changes in the graph parameters, e.g., in the edge weight or the addition and deletion of nodes or edges, require the recalculation of the EBC. As the main c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#19977;&#21521;&#22240;&#26524;&#23646;&#24615;&#20559;&#24207;&#32467;&#26500;&#65288;3WCAPOS&#65289;&#30340;&#26041;&#27861;&#65292;&#23558;&#20559;&#24207;&#27491;&#24335;&#32467;&#26500;&#20998;&#26512;&#65288;POFSA&#65289;&#28436;&#21464;&#20026;&#22240;&#26524;&#35206;&#30422;&#65292;&#20174;&#32780;&#22686;&#24378;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20998;&#31867;&#24615;&#33021;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#22240;&#26524;&#22240;&#23376;&#65288;CF&#65289;&#27010;&#24565;&#35780;&#20272;&#23646;&#24615;&#21644;&#20915;&#31574;&#23646;&#24615;&#20043;&#38388;&#30340;&#22240;&#26524;&#30456;&#20851;&#24615;&#65292;&#24182;&#32467;&#21512;&#19977;&#21521;&#20915;&#31574;&#26500;&#24314;3WCAPOS&#65292;&#20351;&#24471;&#32467;&#26500;&#20013;&#33410;&#28857;&#30340;&#32431;&#24230;&#26356;&#28165;&#26224;&#65292;&#32423;&#21035;&#20043;&#38388;&#30340;&#21464;&#21270;&#26356;&#26126;&#26174;&#12290;</title><link>http://arxiv.org/abs/2303.17482</link><description>&lt;p&gt;
&#19977;&#21521;&#22240;&#26524;&#23646;&#24615;&#20559;&#24207;&#32467;&#26500;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Three-way causal attribute partial order structure analysis. (arXiv:2303.17482v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#19977;&#21521;&#22240;&#26524;&#23646;&#24615;&#20559;&#24207;&#32467;&#26500;&#65288;3WCAPOS&#65289;&#30340;&#26041;&#27861;&#65292;&#23558;&#20559;&#24207;&#27491;&#24335;&#32467;&#26500;&#20998;&#26512;&#65288;POFSA&#65289;&#28436;&#21464;&#20026;&#22240;&#26524;&#35206;&#30422;&#65292;&#20174;&#32780;&#22686;&#24378;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20998;&#31867;&#24615;&#33021;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#22240;&#26524;&#22240;&#23376;&#65288;CF&#65289;&#27010;&#24565;&#35780;&#20272;&#23646;&#24615;&#21644;&#20915;&#31574;&#23646;&#24615;&#20043;&#38388;&#30340;&#22240;&#26524;&#30456;&#20851;&#24615;&#65292;&#24182;&#32467;&#21512;&#19977;&#21521;&#20915;&#31574;&#26500;&#24314;3WCAPOS&#65292;&#20351;&#24471;&#32467;&#26500;&#20013;&#33410;&#28857;&#30340;&#32431;&#24230;&#26356;&#28165;&#26224;&#65292;&#32423;&#21035;&#20043;&#38388;&#30340;&#21464;&#21270;&#26356;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#24207;&#27491;&#24335;&#32467;&#26500;&#20998;&#26512;&#65288;POFSA&#65289;&#20316;&#20026;&#19968;&#31181;&#26032;&#20852;&#30340;&#35748;&#30693;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#30693;&#35782;&#22788;&#29702;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#19977;&#21521;&#22240;&#26524;&#23646;&#24615;&#20559;&#24207;&#32467;&#26500;&#65288;3WCAPOS&#65289;&#30340;&#26041;&#27861;&#65292;&#20197;&#20174;&#38598;&#21512;&#35206;&#30422;&#21521;&#22240;&#26524;&#35206;&#30422;&#28436;&#21464;&#65292;&#20197;&#22686;&#21152;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20998;&#31867;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#25552;&#20986;&#20102;&#22240;&#26524;&#22240;&#23376;&#65288;CF&#65289;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#35780;&#20272;&#24418;&#24335;&#20915;&#31574;&#29615;&#22659;&#20013;&#23646;&#24615;&#21644;&#20915;&#31574;&#23646;&#24615;&#20043;&#38388;&#30340;&#22240;&#26524;&#30456;&#20851;&#24615;&#12290;&#28982;&#21518;&#65292;&#23558;CF&#19982;&#23646;&#24615;&#20559;&#24207;&#32467;&#26500;&#30456;&#32467;&#21512;&#65292;&#23450;&#20041;&#20102;&#22240;&#26524;&#23646;&#24615;&#20559;&#24207;&#32467;&#26500;&#30340;&#27010;&#24565;&#65292;&#20351;&#24471;&#38598;&#21512;&#35206;&#30422;&#28436;&#21464;&#25104;&#22240;&#26524;&#35206;&#30422;&#12290;&#26368;&#21518;&#65292;&#32467;&#21512;&#19977;&#21521;&#20915;&#31574;&#30340;&#24605;&#24819;&#65292;&#24418;&#25104;&#20102;3WCAPOS&#65292;&#20351;&#32467;&#26500;&#20013;&#33410;&#28857;&#30340;&#32431;&#24230;&#26356;&#28165;&#26224;&#65292;&#32423;&#21035;&#20043;&#38388;&#30340;&#21464;&#21270;&#26356;&#26126;&#26174;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#20174;&#20998;&#31867;&#33021;&#21147;&#20986;&#21457;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
As an emerging concept cognitive learning model, partial order formal structure analysis (POFSA) has been widely used in the field of knowledge processing. In this paper, we propose the method named three-way causal attribute partial order structure (3WCAPOS) to evolve the POFSA from set coverage to causal coverage in order to increase the interpretability and classification performance of the model. First, the concept of causal factor (CF) is proposed to evaluate the causal correlation between attributes and decision attributes in the formal decision context. Then, combining CF with attribute partial order structure, the concept of causal attribute partial order structure is defined and makes set coverage evolve into causal coverage. Finally, combined with the idea of three-way decision, 3WCAPOS is formed, which makes the purity of nodes in the structure clearer and the changes between levels more obviously. In addition, the experiments are carried out from the classification ability 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21475;&#35835;&#19987;&#23478;&#24341;&#23548;&#30340;&#35828;&#35805;&#20154;&#33080;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#24809;&#32602;&#19981;&#27491;&#30830;&#30340;&#29983;&#25104;&#32467;&#26524;&#26469;&#25552;&#39640;&#29983;&#25104;&#21767;&#37096;&#21306;&#22495;&#30340;&#26126;&#26224;&#24230;&#65292;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#22686;&#24378;&#21767;&#35821;&#21516;&#27493;&#24615;&#65292;&#24182;&#20351;&#29992;&#21464;&#21387;&#22120;&#26469;&#21516;&#27493;&#32534;&#30721;&#38899;&#39057;&#21644;&#35270;&#39057;&#12290;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#22312;&#35270;&#35273;&#36136;&#37327;&#21644;&#26126;&#26224;&#24230;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2303.17480</link><description>&lt;p&gt;
&#30475;&#30528;&#20320;&#35828;&#35805;&#65306;&#30001;&#21475;&#35835;&#19987;&#23478;&#24341;&#23548;&#30340;&#35828;&#35805;&#20154;&#33080;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Seeing What You Said: Talking Face Generation Guided by a Lip Reading Expert. (arXiv:2303.17480v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17480
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21475;&#35835;&#19987;&#23478;&#24341;&#23548;&#30340;&#35828;&#35805;&#20154;&#33080;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#24809;&#32602;&#19981;&#27491;&#30830;&#30340;&#29983;&#25104;&#32467;&#26524;&#26469;&#25552;&#39640;&#29983;&#25104;&#21767;&#37096;&#21306;&#22495;&#30340;&#26126;&#26224;&#24230;&#65292;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#22686;&#24378;&#21767;&#35821;&#21516;&#27493;&#24615;&#65292;&#24182;&#20351;&#29992;&#21464;&#21387;&#22120;&#26469;&#21516;&#27493;&#32534;&#30721;&#38899;&#39057;&#21644;&#35270;&#39057;&#12290;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#22312;&#35270;&#35273;&#36136;&#37327;&#21644;&#26126;&#26224;&#24230;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35828;&#35805;&#20154;&#33080;&#29983;&#25104;&#65292;&#20063;&#31216;&#20026;&#35821;&#38899;&#21040;&#21767;&#37096;&#29983;&#25104;&#65292;&#26159; reconstructs &#38754;&#37096;&#21160;&#20316;&#65292;&#29305;&#21035;&#26159;&#21767;&#37096;&#36816;&#21160;&#65292;&#32473;&#23450;&#19968;&#33268;&#30340;&#35821;&#38899;&#36755;&#20837;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#21767;&#35821;&#21516;&#27493;&#24615;&#21644;&#35270;&#35273;&#36136;&#37327;&#30340;&#37325;&#35201;&#24615;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#24456;&#22810;&#36827;&#23637;&#65292;&#20294;&#20182;&#20204;&#24456;&#38590;&#38598;&#20013;&#20110;&#21767;&#37096;&#36816;&#21160;&#30340;&#20869;&#23481;&#65292;&#21363;&#25152;&#35828;&#21333;&#35789;&#30340;&#35270;&#35273;&#26126;&#26224;&#24230;&#65292;&#36825;&#26159;&#29983;&#25104;&#36136;&#37327;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#21475;&#35835;&#19987;&#23478;&#36890;&#36807;&#24809;&#32602;&#19981;&#27491;&#30830;&#30340;&#29983;&#25104;&#32467;&#26524;&#26469;&#25552;&#39640;&#29983;&#25104;&#21767;&#37096;&#21306;&#22495;&#30340;&#28165;&#26224;&#24230;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#24357;&#34917;&#25968;&#25454;&#31232;&#32570;&#24615;&#65292;&#25105;&#20204;&#22312;&#38899;&#39057; - &#35270;&#35273;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#24335;&#19979;&#35757;&#32451;&#21475;&#35835;&#19987;&#23478;&#12290;&#20351;&#29992;&#21475;&#35835;&#19987;&#23478;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26469;&#22686;&#24378;&#21767;&#35821;&#21516;&#27493;&#24615;&#65292;&#24182;&#20351;&#29992;&#21464;&#21387;&#22120;&#26469;&#21516;&#27493;&#32534;&#30721;&#38899;&#39057;&#21644;&#35270;&#39057;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#38899;&#39057;&#30340;&#20840;&#23616;&#26102;&#24207;&#20381;&#36182;&#20851;&#31995;&#12290;&#20026;&#20102;&#35780;&#20272;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#20351;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#21475;&#35835;&#19987;&#23478;&#26469;&#35780;&#20272;&#29983;&#25104;&#30340;&#21767;&#37096;&#36816;&#21160;&#30340;&#35270;&#35273;&#26126;&#26224;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#29983;&#25104;&#38754;&#23380;&#30340;&#35270;&#35273;&#36136;&#37327;&#21644;&#26126;&#26224;&#24230;&#65292;&#22312;&#20027;&#35266;&#21644;&#23458;&#35266;&#35780;&#20272;&#20013;&#22343;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Talking face generation, also known as speech-to-lip generation, reconstructs facial motions concerning lips given coherent speech input. The previous studies revealed the importance of lip-speech synchronization and visual quality. Despite much progress, they hardly focus on the content of lip movements i.e., the visual intelligibility of the spoken words, which is an important aspect of generation quality. To address the problem, we propose using a lip-reading expert to improve the intelligibility of the generated lip regions by penalizing the incorrect generation results. Moreover, to compensate for data scarcity, we train the lip-reading expert in an audio-visual self-supervised manner. With a lip-reading expert, we propose a novel contrastive learning to enhance lip-speech synchronization, and a transformer to encode audio synchronically with video, while considering global temporal dependency of audio. For evaluation, we propose a new strategy with two different lip-reading exper
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20154;&#21592;&#25104;&#21151;&#22320;&#37319;&#29992;&#29702;&#24819;&#20276;&#20387;&#34920;&#31034;&#27861;&#26469;&#24314;&#27169;&#36951;&#20256;&#35268;&#21010;&#20013;&#30340;&#20132;&#37197;&#20559;&#22909;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#36827;&#21270;&#20135;&#29983;&#22810;&#26679;&#21270;&#30340;&#31181;&#32676;&#26469;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.17441</link><description>&lt;p&gt;
&#22810;&#26679;&#24615;&#30340;&#20851;&#38190;&#22312;&#20110;&#24615;&#20132;&#37197;&#65306;&#22522;&#20110;&#33258;&#36866;&#24212;&#36873;&#25321;&#26426;&#21046;&#30340;&#36951;&#20256;&#35268;&#21010;&#20013;&#30340;&#32321;&#27542;&#20248;&#36873;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
All You Need Is Sex for Diversity. (arXiv:2303.17441v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17441
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#25104;&#21151;&#22320;&#37319;&#29992;&#29702;&#24819;&#20276;&#20387;&#34920;&#31034;&#27861;&#26469;&#24314;&#27169;&#36951;&#20256;&#35268;&#21010;&#20013;&#30340;&#20132;&#37197;&#20559;&#22909;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#36827;&#21270;&#20135;&#29983;&#22810;&#26679;&#21270;&#30340;&#31181;&#32676;&#26469;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36951;&#20256;&#35268;&#21010;&#20013;&#65292;&#20445;&#25345;&#22522;&#22240;&#22810;&#26679;&#24615;&#20197;&#36991;&#20813;&#36807;&#26089;&#25910;&#25947;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22810;&#31181;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#20854;&#20013;&#19968;&#20123;&#26041;&#27861;&#38598;&#20013;&#20110;&#20132;&#37197;&#38454;&#27573;&#65292;&#36890;&#36807;&#32806;&#21512;&#19981;&#21516;&#30340;&#35299;&#20915;&#26041;&#26696;&#21040;&#19968;&#20123;&#24418;&#24335;&#30340;&#33258;&#36866;&#24212;&#36873;&#25321;&#26426;&#21046;&#26469;&#23454;&#29616;&#12290;&#22312;&#33258;&#28982;&#20013;&#65292;&#22522;&#22240;&#22810;&#26679;&#24615;&#21487;&#20197;&#26159;&#35768;&#22810;&#19981;&#21516;&#22240;&#32032;&#30340;&#32467;&#26524;&#65292;&#20294;&#24403;&#32771;&#34385;&#32321;&#27542;&#26102;&#65292;&#24615;&#36873;&#25321;&#23545;&#20110;&#22312;&#19968;&#20010;&#29289;&#31181;&#20869;&#20419;&#36827;&#21464;&#24322;&#20855;&#26377;&#24433;&#21709;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#24322;&#24615;&#36873;&#25321;&#24448;&#24448;&#23548;&#33268;&#19981;&#21516;&#30340;&#36873;&#25321;&#21387;&#21147;&#65292;&#36827;&#32780;&#21487;&#33021;&#22312;&#23427;&#20204;&#20043;&#38388;&#24341;&#21457;&#36827;&#21270;&#24046;&#24322;&#12290;&#34429;&#28982;&#19968;&#20123;&#24615;&#36873;&#25321;&#26426;&#21046;&#26366;&#32463;&#34987;&#24212;&#29992;&#20110;&#36951;&#20256;&#35268;&#21010;&#20013;&#65292;&#20294;&#24403;&#28041;&#21450;&#21040;&#20132;&#37197;&#36873;&#25321;&#26102;&#65292;&#30456;&#20851;&#25991;&#29486;&#36739;&#23569;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#29702;&#24819;&#20276;&#20387;&#34920;&#31034;&#27861;&#24314;&#27169;&#20132;&#37197;&#20559;&#22909;&#30340;&#26041;&#27861;&#34987;&#25552;&#20986;&#65292;&#19982;&#26631;&#20934;&#26041;&#27861;&#30456;&#27604;&#65292;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#36825;&#20123;&#20132;&#37197;&#20559;&#22909;&#25353;&#29031;&#33258;&#36866;&#24212;&#26041;&#24335;&#33258;&#30001;&#36827;&#21270;&#65292;&#24182;&#33021;&#20135;&#29983;&#22810;&#26679;&#21270;&#30340;&#31181;&#32676;&#65292;&#20174;&#32780;&#24471;&#21040;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maintaining genetic diversity as a means to avoid premature convergence is critical in Genetic Programming. Several approaches have been proposed to achieve this, with some focusing on the mating phase from coupling dissimilar solutions to some form of self-adaptive selection mechanism. In nature, genetic diversity can be the consequence of many different factors, but when considering reproduction Sexual Selection can have an impact on promoting variety within a species. Specifically, Mate Choice often results in different selective pressures between sexes, which in turn may trigger evolutionary differences among them. Although some mechanisms of Sexual Selection have been applied to Genetic Programming in the past, the literature is scarce when it comes to mate choice. Recently, a way of modelling mating preferences by ideal mate representations was proposed, achieving good results when compared to a standard approach. These mating preferences evolve freely in a self-adaptive fashion,
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20013;&#24120;&#35265;&#30340;&#19981;&#23436;&#32654;&#25191;&#34892;&#30340;&#22810;&#26234;&#33021;&#20307;&#25509;&#36865;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#36807;&#35268;&#21010;&#36335;&#24452;&#26469;&#38480;&#21046;&#19981;&#23436;&#32654;&#25191;&#34892;&#25928;&#26524;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#20379;&#20102;&#40065;&#26834;&#24615;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2303.17422</link><description>&lt;p&gt;
&#20855;&#26377;&#24310;&#36831;&#30340;&#24378;&#40065;&#26834;&#24615;&#22810;&#26234;&#33021;&#20307;&#25509;&#36865;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Robust Multi-Agent Pickup and Delivery with Delays. (arXiv:2303.17422v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17422
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20013;&#24120;&#35265;&#30340;&#19981;&#23436;&#32654;&#25191;&#34892;&#30340;&#22810;&#26234;&#33021;&#20307;&#25509;&#36865;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#36807;&#35268;&#21010;&#36335;&#24452;&#26469;&#38480;&#21046;&#19981;&#23436;&#32654;&#25191;&#34892;&#25928;&#26524;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#20379;&#20102;&#40065;&#26834;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#25509;&#36865;(MAPD)&#30340;&#38382;&#39064;&#22312;&#20110;&#35745;&#31639;&#19968;&#32452;&#20195;&#29702;&#30340;&#26080;&#30896;&#25758;&#36335;&#24452;&#65292;&#20197;&#20415;&#23427;&#20204;&#33021;&#22815;&#23433;&#20840;&#22320;&#20174;&#25552;&#21462;&#20301;&#32622;&#21040;&#36798;&#20256;&#36882;&#20301;&#32622;&#12290;&#36825;&#20123;&#20301;&#32622;&#22312;&#36816;&#34892;&#26102;&#25552;&#20379;&#65292;&#20351;&#24471;MAPD&#25104;&#20026;&#32463;&#20856;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;(MAPF)&#21644;&#22312;&#32447;&#20219;&#21153;&#20998;&#37197;&#30340;&#32452;&#21512;&#12290;&#30446;&#21069;&#30340;MAPD&#31639;&#27861;&#27809;&#26377;&#32771;&#34385;&#21040;&#23454;&#38469;&#24212;&#29992;&#20013;&#36935;&#21040;&#30340;&#35768;&#22810;&#23454;&#38469;&#38382;&#39064;&#65306;&#30495;&#23454;&#30340;&#20195;&#29702;&#36890;&#24120;&#19981;&#23436;&#20840;&#25353;&#35745;&#21010;&#36335;&#24452;&#34892;&#39542;&#65292;&#21487;&#33021;&#20250;&#21463;&#21040;&#24310;&#36831;&#21644;&#25925;&#38556;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#24310;&#36831;&#30340;MAPD&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#35268;&#21010;&#36335;&#24452;&#26469;&#38480;&#21046;&#19981;&#23436;&#32654;&#25191;&#34892;&#30340;&#25928;&#26524;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#40065;&#26834;&#24615;&#20445;&#35777;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;k-TP&#21644;p-TP&#65292;&#37117;&#22522;&#20110;&#20998;&#24067;&#24335;&#31639;&#27861;Token Passing(TP)&#65292;&#23427;&#36890;&#24120;&#29992;&#20110;&#35299;&#20915;MAPD&#65292;&#20998;&#21035;&#25552;&#20379;&#30830;&#23450;&#24615;&#21644;&#27010;&#29575;&#20445;&#35777;&#12290;&#22312;&#23454;&#39564;&#19978;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#31639;&#27861;&#19982;&#29616;&#26377;&#30340;&#31639;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Pickup and Delivery (MAPD) is the problem of computing collision-free paths for a group of agents such that they can safely reach delivery locations from pickup ones. These locations are provided at runtime, making MAPD a combination between classical Multi-Agent Path Finding (MAPF) and online task assignment. Current algorithms for MAPD do not consider many of the practical issues encountered in real applications: real agents often do not follow the planned paths perfectly, and may be subject to delays and failures. In this paper, we study the problem of MAPD with delays, and we present two solution approaches that provide robustness guarantees by planning paths that limit the effects of imperfect execution. In particular, we introduce two algorithms, k-TP and p-TP, both based on a decentralized algorithm typically used to solve MAPD, Token Passing (TP), which offer deterministic and probabilistic guarantees, respectively. Experimentally, we compare our algorithms against 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#31454;&#20105;&#23398;&#20064;&#31639;&#27861;&#30340;&#30333;&#30418;&#21487;&#35299;&#37322;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65292;&#19982;&#40657;&#21283;&#23376;&#26041;&#27861;&#30456;&#27604;&#65292;&#33021;&#22815;&#25552;&#20379;&#26356;&#20026;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#19988;&#36164;&#28304;&#28040;&#32791;&#36739;&#23567;&#12290;</title><link>http://arxiv.org/abs/2303.17387</link><description>&lt;p&gt;
&#20351;&#29992;&#31454;&#20105;&#23398;&#20064;&#25216;&#26415;&#30340;&#21487;&#35299;&#37322;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Explainable Intrusion Detection Systems Using Competitive Learning Techniques. (arXiv:2303.17387v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#31454;&#20105;&#23398;&#20064;&#31639;&#27861;&#30340;&#30333;&#30418;&#21487;&#35299;&#37322;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65292;&#19982;&#40657;&#21283;&#23376;&#26041;&#27861;&#30456;&#27604;&#65292;&#33021;&#22815;&#25552;&#20379;&#26356;&#20026;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#19988;&#36164;&#28304;&#28040;&#32791;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21551;&#29992;&#30340;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#37319;&#29992;&#21508;&#31181;&#40657;&#21283;&#23376;&#26041;&#27861;&#12290;&#36825;&#20123;&#40657;&#21283;&#23376;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#22522;&#20110;&#38169;&#35823;&#23398;&#20064;&#65288;EBL&#65289;&#25216;&#26415;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19987;&#27880;&#20110;&#21019;&#24314;&#20934;&#30830;&#30340;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#39640;&#24615;&#33021;&#25104;&#26412;&#19988;&#19981;&#26131;&#35299;&#37322;&#12290;&#22522;&#20110;&#31454;&#20105;&#23398;&#20064;&#65288;CL&#65289;&#30340;&#30333;&#30418;&#21487;&#35299;&#37322;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;X-IDS&#65289;&#20026;&#36825;&#20123;&#38382;&#39064;&#25552;&#20379;&#20102;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;CL&#27169;&#22411;&#21033;&#29992;&#19982;EBL&#26041;&#27861;&#23436;&#20840;&#19981;&#21516;&#30340;&#23398;&#20064;&#33539;&#24335;&#12290;&#36825;&#31181;&#19981;&#21516;&#30340;&#23398;&#20064;&#36807;&#31243;&#20351;&#24471;CL&#31639;&#27861;&#26063;&#22266;&#26377;&#22320;&#21487;&#35299;&#37322;&#19988;&#36164;&#28304;&#28040;&#32791;&#36739;&#23567;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;DARPA&#23545;&#21487;&#35299;&#37322;&#31995;&#32479;&#30340;&#24314;&#35758;&#30340;X-IDS&#26550;&#26500;&#12290;&#22312;&#25105;&#20204;&#30340;&#26550;&#26500;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20687;&#33258;&#32452;&#32455;&#26144;&#23556;&#65288;SOM&#65289;&#12289;&#22686;&#38271;&#24335;&#33258;&#32452;&#32455;&#26144;&#23556;&#65288;GSOM&#65289;&#21644;&#22686;&#38271;&#24335;&#20998;&#23618;&#33258;&#32452;&#32455;&#26144;&#23556;&#65288;GHSOM&#65289;&#31561;CL&#31639;&#27861;&#12290;&#29983;&#25104;&#30340;&#27169;&#22411;&#21487;&#20197;&#36827;&#34892;&#25968;&#25454;&#25366;&#25496;&#20197;&#21019;&#24314;&#21487;&#35299;&#37322;&#30340;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current state of the art systems in Artificial Intelligence (AI) enabled intrusion detection use a variety of black box methods. These black box methods are generally trained using Error Based Learning (EBL) techniques with a focus on creating accurate models. These models have high performative costs and are not easily explainable. A white box Competitive Learning (CL) based eXplainable Intrusion Detection System (X-IDS) offers a potential solution to these problem. CL models utilize an entirely different learning paradigm than EBL approaches. This different learning process makes the CL family of algorithms innately explainable and less resource intensive. In this paper, we create an X-IDS architecture that is based on DARPA's recommendation for explainable systems. In our architecture we leverage CL algorithms like, Self Organizing Maps (SOM), Growing Self Organizing Maps (GSOM), and Growing Hierarchical Self Organizing Map (GHSOM). The resulting models can be data-mined to crea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;RGB-T&#22270;&#20687;&#20114;&#34917;&#38543;&#26426;&#33945;&#29256;&#31574;&#30053;&#21644;&#33258;&#33976;&#39311;&#25439;&#22833;&#65292;&#36890;&#36807;&#38450;&#27490;&#23545;&#21333;&#19968;&#27169;&#24335;&#36807;&#24230;&#20381;&#36182;&#65292;&#24378;&#21046;&#32593;&#32476;&#22312;&#19968;&#20010;&#27169;&#24577;&#37096;&#20998;&#21487;&#29992;&#26102;&#36827;&#34892;&#20998;&#21106;&#21644;&#20998;&#31867;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#40723;&#21169;&#32593;&#32476;&#25552;&#21462;&#20114;&#34917;&#19988;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2303.17386</link><description>&lt;p&gt;
RGB-&#28909;&#32418;&#22806;&#35821;&#20041;&#20998;&#21106;&#30340;&#20114;&#34917;&#38543;&#26426;&#33945;&#29256;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Complementary Random Masking for RGB-Thermal Semantic Segmentation. (arXiv:2303.17386v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;RGB-T&#22270;&#20687;&#20114;&#34917;&#38543;&#26426;&#33945;&#29256;&#31574;&#30053;&#21644;&#33258;&#33976;&#39311;&#25439;&#22833;&#65292;&#36890;&#36807;&#38450;&#27490;&#23545;&#21333;&#19968;&#27169;&#24335;&#36807;&#24230;&#20381;&#36182;&#65292;&#24378;&#21046;&#32593;&#32476;&#22312;&#19968;&#20010;&#27169;&#24577;&#37096;&#20998;&#21487;&#29992;&#26102;&#36827;&#34892;&#20998;&#21106;&#21644;&#20998;&#31867;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#40723;&#21169;&#32593;&#32476;&#25552;&#21462;&#20114;&#34917;&#19988;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24694;&#21155;&#30340;&#27668;&#35937;&#21644;&#29031;&#26126;&#26465;&#20214;&#19979;&#65292;RGB-&#28909;&#32418;&#22806;&#35821;&#20041;&#20998;&#21106;&#26159;&#23454;&#29616;&#21487;&#38752;&#30340;&#22330;&#26223;&#29702;&#35299;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#22823;&#22810;&#38598;&#20013;&#22312;&#35774;&#35745;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22359;&#65292;&#32780;&#24573;&#30053;&#20102;&#22810;&#27169;&#24577;&#36755;&#20837;&#30340;&#26412;&#36136;&#12290;&#22240;&#27492;&#65292;&#32593;&#32476;&#23481;&#26131;&#36807;&#24230;&#20381;&#36182;&#21333;&#19968;&#27169;&#24577;&#65292;&#38590;&#20197;&#20026;&#27599;&#20010;&#27169;&#24577;&#23398;&#20064;&#20114;&#34917;&#19988;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;RGB-T&#22270;&#20687;&#20114;&#34917;&#38543;&#26426;&#33945;&#29256;&#31574;&#30053;&#21644;&#33258;&#33976;&#39311;&#25439;&#22833;&#12290;&#25152;&#25552;&#20986;&#30340;&#33945;&#29256;&#31574;&#30053;&#38450;&#27490;&#23545;&#21333;&#19968;&#27169;&#24335;&#30340;&#36807;&#24230;&#20381;&#36182;&#65292;&#24378;&#21046;&#32593;&#32476;&#22312;&#19968;&#20010;&#27169;&#24577;&#37096;&#20998;&#21487;&#29992;&#26102;&#36827;&#34892;&#23545;&#35937;&#20998;&#21106;&#21644;&#20998;&#31867;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#21516;&#26102;&#65292;&#25152;&#25552;&#20986;&#30340;&#33258;&#33976;&#39311;&#25439;&#22833;&#40723;&#21169;&#32593;&#32476;&#20174;&#21333;&#19968;&#27169;&#24577;&#20013;&#25552;&#21462;&#20114;&#34917;&#19988;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
RGB-thermal semantic segmentation is one potential solution to achieve reliable semantic scene understanding in adverse weather and lighting conditions. However, the previous studies mostly focus on designing a multi-modal fusion module without consideration of the nature of multi-modality inputs. Therefore, the networks easily become over-reliant on a single modality, making it difficult to learn complementary and meaningful representations for each modality. This paper proposes 1) a complementary random masking strategy of RGB-T images and 2) self-distillation loss between clean and masked input modalities. The proposed masking strategy prevents over-reliance on a single modality. It also improves the accuracy and robustness of the neural network by forcing the network to segment and classify objects even when one modality is partially available. Also, the proposed self-distillation loss encourages the network to extract complementary and meaningful representations from a single moda
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30528;&#37325;&#30740;&#31350;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#22810;&#20219;&#21153;&#33258;&#22238;&#24402;&#35299;&#30721;&#22120;&#65292;&#36890;&#36807;&#24191;&#27867;&#30340;&#31995;&#32479;&#35797;&#39564;&#20998;&#26512;&#20102;&#20219;&#21153;&#21644;&#25968;&#25454;&#28151;&#21512;&#12289;&#35757;&#32451;&#21644;&#27491;&#21017;&#21270;&#36229;&#21442;&#25968;&#12289;&#26465;&#20214;&#31867;&#22411;&#21644;&#29305;&#24322;&#24615;&#12289;&#27169;&#24577;&#32452;&#21512;&#31561;&#22240;&#32032;&#65292;&#21457;&#29616;&#36890;&#36807;&#20923;&#32467;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#65292;&#37319;&#29992;&#23567;&#22411;&#35299;&#30721;&#22120;&#21487;&#25509;&#36817;&#20110;&#21333;&#20219;&#21153;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2303.17376</link><description>&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#22810;&#20219;&#21153;&#33258;&#22238;&#24402;&#35299;&#30721;&#22120;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study of Autoregressive Decoders for Multi-Tasking in Computer Vision. (arXiv:2303.17376v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30528;&#37325;&#30740;&#31350;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#22810;&#20219;&#21153;&#33258;&#22238;&#24402;&#35299;&#30721;&#22120;&#65292;&#36890;&#36807;&#24191;&#27867;&#30340;&#31995;&#32479;&#35797;&#39564;&#20998;&#26512;&#20102;&#20219;&#21153;&#21644;&#25968;&#25454;&#28151;&#21512;&#12289;&#35757;&#32451;&#21644;&#27491;&#21017;&#21270;&#36229;&#21442;&#25968;&#12289;&#26465;&#20214;&#31867;&#22411;&#21644;&#29305;&#24322;&#24615;&#12289;&#27169;&#24577;&#32452;&#21512;&#31561;&#22240;&#32032;&#65292;&#21457;&#29616;&#36890;&#36807;&#20923;&#32467;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#65292;&#37319;&#29992;&#23567;&#22411;&#35299;&#30721;&#22120;&#21487;&#25509;&#36817;&#20110;&#21333;&#20219;&#21153;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20986;&#29616;&#20102;&#35768;&#22810;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#65292;&#33021;&#22815;&#25191;&#34892;&#22810;&#39033;&#20219;&#21153;&#65292;&#30001;&#22270;&#20687;&#32534;&#30721;&#22120;&#65288;&#36890;&#24120;&#26159; ViT&#65289;&#21644;&#33258;&#22238;&#24402;&#35299;&#30721;&#22120;&#65288;&#36890;&#24120;&#26159; Transformer&#65289;&#32452;&#25104;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#24037;&#20316;&#20165;&#21576;&#29616;&#19968;&#20010;&#31995;&#32479;&#21450;&#20854;&#32467;&#26524;&#65292;&#23545;&#36825;&#20123;&#31995;&#32479;&#30340;&#35774;&#35745;&#20915;&#31574;&#21644;&#26435;&#34913;&#26041;&#38754;&#30041;&#19979;&#20102;&#35768;&#22810;&#30097;&#38382;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;&#36825;&#20123;&#31572;&#26696;&#12290;&#25105;&#20204;&#20180;&#32454;&#30740;&#31350;&#20102;&#33258;&#22238;&#24402;&#35299;&#30721;&#22120;&#22312;&#22810;&#27169;&#24577;&#35745;&#31639;&#26426;&#35270;&#35273;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#20998;&#31867;&#12289;&#23383;&#24149;&#12289;&#35270;&#35273;&#38382;&#31572;&#21644;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#31561;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#31995;&#32479;&#35797;&#39564;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20219;&#21153;&#21644;&#25968;&#25454;&#28151;&#21512;&#12289;&#35757;&#32451;&#21644;&#27491;&#21017;&#21270;&#36229;&#21442;&#25968;&#12289;&#26465;&#20214;&#31867;&#22411;&#21644;&#29305;&#24322;&#24615;&#12289;&#27169;&#24577;&#32452;&#21512;&#31561;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#32467;&#26524;&#19982;&#32463;&#36807;&#20805;&#20998;&#35843;&#35797;&#30340;&#21333;&#20219;&#21153;&#22522;&#32447;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#20197;&#20984;&#26174;&#22810;&#20219;&#21153;&#23398;&#20064;&#25152;&#24102;&#26469;&#30340;&#25104;&#26412;&#12290;&#19968;&#20010;&#20851;&#38190;&#30340;&#21457;&#29616;&#26159;&#65292;&#23545;&#20110;&#20923;&#32467;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#19978;&#23398;&#20064;&#30340;&#23567;&#22411;&#35299;&#30721;&#22120;&#65292;&#24615;&#33021;&#25509;&#36817;&#20110;&#21333;&#20219;&#21153;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been a recent explosion of computer vision models which perform many tasks and are composed of an image encoder (usually a ViT) and an autoregressive decoder (usually a Transformer). However, most of this work simply presents one system and its results, leaving many questions regarding design decisions and trade-offs of such systems unanswered. In this work, we aim to provide such answers. We take a close look at autoregressive decoders for multi-task learning in multimodal computer vision, including classification, captioning, visual question answering, and optical character recognition. Through extensive systematic experiments, we study the effects of task and data mixture, training and regularization hyperparameters, conditioning type and specificity, modality combination, and more. Importantly, we compare these to well-tuned single-task baselines to highlight the cost incurred by multi-tasking. A key finding is that a small decoder learned on top of a frozen pretrained en
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#30005;&#20449;&#27450;&#35784;&#26816;&#27979;&#32780;&#35774;&#35745;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#21517;&#20026;GAT-COBO&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35774;&#35745;&#22522;&#20110;GAT&#30340;&#22522;&#26412;&#20998;&#31867;&#22120;&#21644;&#25104;&#26412;&#25935;&#24863;&#23398;&#20064;&#22120;&#26469;&#35299;&#20915;&#22270;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#31454;&#20105;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.17334</link><description>&lt;p&gt;
GAT-COBO&#65306;&#19968;&#31181;&#38024;&#23545;&#30005;&#20449;&#27450;&#35784;&#26816;&#27979;&#30340;&#25104;&#26412;&#25935;&#24863;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
GAT-COBO: Cost-Sensitive Graph Neural Network for Telecom Fraud Detection. (arXiv:2303.17334v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#30005;&#20449;&#27450;&#35784;&#26816;&#27979;&#32780;&#35774;&#35745;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#21517;&#20026;GAT-COBO&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35774;&#35745;&#22522;&#20110;GAT&#30340;&#22522;&#26412;&#20998;&#31867;&#22120;&#21644;&#25104;&#26412;&#25935;&#24863;&#23398;&#20064;&#22120;&#26469;&#35299;&#20915;&#22270;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#31454;&#20105;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31227;&#21160;&#36890;&#20449;&#25216;&#26415;&#65288;&#22914;5G&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#30005;&#20449;&#27450;&#35784;&#24471;&#21040;&#20102;&#26174;&#33879;&#22686;&#21152;&#65292;&#24433;&#21709;&#20102;&#20010;&#20154;&#36130;&#23500;&#21644;&#31038;&#20250;&#36130;&#23500;&#30340;&#25439;&#22833;&#12290;&#26368;&#36817;&#65292;&#22270;&#25366;&#25496;&#25216;&#26415;&#36880;&#28176;&#25104;&#20026;&#26816;&#27979;&#30005;&#20449;&#27450;&#35784;&#30340;&#20027;&#27969;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24085;&#32047;&#25176;&#21407;&#22240;&#24341;&#36215;&#30340;&#22270;&#19981;&#24179;&#34913;&#38382;&#39064;&#32473;&#22270;&#25968;&#25454;&#25366;&#25496;&#24102;&#26469;&#20102;&#20005;&#37325;&#30340;&#25361;&#25112;&#12290;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#20294;&#20043;&#21069;&#24456;&#23569;&#26377;&#30740;&#31350;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22270;&#19981;&#24179;&#34913;&#38382;&#39064;&#30340;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#21644;&#25104;&#26412;&#25935;&#24863;&#22686;&#24378;&#65288;GAT-COBO&#65289;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22522;&#20110;GAT&#30340;&#22522;&#26412;&#20998;&#31867;&#22120;&#26469;&#23398;&#20064;&#22270;&#20013;&#25152;&#26377;&#33410;&#28857;&#30340;&#23884;&#20837;&#12290;&#28982;&#21518;&#65292;&#23558;&#23884;&#20837;&#39304;&#36865;&#21040;&#32463;&#36807;&#33391;&#22909;&#35774;&#35745;&#30340;&#25104;&#26412;&#25935;&#24863;&#23398;&#20064;&#22120;&#20013;&#36827;&#34892;&#19981;&#24179;&#34913;&#23398;&#20064;&#12290;&#25509;&#19979;&#26469;&#65292;&#26681;&#25454;&#20998;&#31867;&#38169;&#35823;&#25104;&#26412;&#26356;&#26032;&#26435;&#37325;&#65292;&#20351;&#27169;&#22411;&#26356;&#22810;&#22320;&#20851;&#27880;&#23569;&#25968;&#31867;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#23558;&#22522;&#20110;&#20998;&#31867;&#22120;&#21644;&#25104;&#26412;&#25935;&#24863;&#23398;&#20064;&#22120;&#33719;&#24471;&#30340;&#33410;&#28857;&#23884;&#20837;&#30456;&#21152;&#65292;&#24471;&#20986;&#26368;&#32456;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;GAT-COBO&#26041;&#27861;&#22312;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#32447;&#30456;&#27604;&#26102;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Along with the rapid evolution of mobile communication technologies, such as 5G, there has been a drastically increase in telecom fraud, which significantly dissipates individual fortune and social wealth. In recent years, graph mining techniques are gradually becoming a mainstream solution for detecting telecom fraud. However, the graph imbalance problem, caused by the Pareto principle, brings severe challenges to graph data mining. This is a new and challenging problem, but little previous work has been noticed. In this paper, we propose a Graph ATtention network with COst-sensitive BOosting (GAT-COBO) for the graph imbalance problem. First, we design a GAT-based base classifier to learn the embeddings of all nodes in the graph. Then, we feed the embeddings into a well-designed cost-sensitive learner for imbalanced learning. Next, we update the weights according to the misclassification cost to make the model focus more on the minority class. Finally, we sum the node embeddings obtai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22836;&#39048;&#37096;&#33258;&#21160;&#20998;&#21106;&#27169;&#22411;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#21644;&#38598;&#25104;&#26041;&#27861;&#23545;&#20854;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;250&#20010;&#25195;&#25551;&#26159;&#35757;&#32451;&#26368;&#20934;&#30830;&#21644;&#26368;&#20581;&#22766;&#30340;&#27169;&#22411;&#25152;&#38656;&#30340;&#26368;&#23567;&#25968;&#25454;&#37327;&#65292;&#24182;&#19988;&#38598;&#25104;&#26041;&#27861;&#21487;&#20197;&#22312;&#23567;&#25968;&#25454;&#38598;&#19978;&#21457;&#25381;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.17318</link><description>&lt;p&gt;
&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#21644;&#38598;&#25104;&#25512;&#29702;&#31574;&#30053;&#23545;&#22836;&#39048;&#37096;&#33258;&#21160;&#20998;&#21106;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The impact of training dataset size and ensemble inference strategies on head and neck auto-segmentation. (arXiv:2303.17318v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17318
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22836;&#39048;&#37096;&#33258;&#21160;&#20998;&#21106;&#27169;&#22411;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#21644;&#38598;&#25104;&#26041;&#27861;&#23545;&#20854;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;250&#20010;&#25195;&#25551;&#26159;&#35757;&#32451;&#26368;&#20934;&#30830;&#21644;&#26368;&#20581;&#22766;&#30340;&#27169;&#22411;&#25152;&#38656;&#30340;&#26368;&#23567;&#25968;&#25454;&#37327;&#65292;&#24182;&#19988;&#38598;&#25104;&#26041;&#27861;&#21487;&#20197;&#22312;&#23567;&#25968;&#25454;&#38598;&#19978;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNNs)&#36234;&#26469;&#36234;&#34987;&#29992;&#20110;&#25918;&#30103;&#20013;&#33258;&#21160;&#20998;&#21106;&#21361;&#26426;&#22120;&#23448;&#12290;&#30001;&#20110;&#39640;&#36136;&#37327;&#30340;&#22823;&#25968;&#25454;&#38598;&#24456;&#23569;&#65292;&#22240;&#27492;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#23569;&#25968;&#25454;&#26159;&#35757;&#32451;&#31934;&#20934;&#21644;&#20581;&#22766;&#30340;&#22836;&#39048;&#37096;&#33258;&#21160;&#20998;&#21106;&#27169;&#22411;&#25152;&#38656;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#22823;&#23567;&#30340;&#25968;&#25454;&#38598;(25-1000&#20010;&#25195;&#25551;)&#26469;&#35757;&#32451;&#19968;&#20010;&#31616;&#21333;&#30340;3D CNN&#27169;&#22411;&#65292;&#20197;&#20998;&#21106;CT&#20013;&#30340;&#33041;&#24178;&#12289;&#33134;&#33146;&#21644;&#33034;&#39635;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22810;&#31181;&#38598;&#25104;&#25216;&#26415;&#26469;&#25913;&#21892;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#38543;&#30528;&#35757;&#32451;&#38598;&#22823;&#23567;&#36798;&#21040;250&#20010;&#25195;&#25551;&#65292;&#20998;&#21106;&#25928;&#26524;&#24471;&#21040;&#20102;&#25913;&#21892;&#65292;&#32780;&#38598;&#25104;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#25152;&#26377;&#22120;&#23448;&#30340;&#24615;&#33021;&#12290;&#38598;&#25104;&#26041;&#27861;&#23545;&#26368;&#23567;&#30340;&#25968;&#25454;&#38598;&#30340;&#24433;&#21709;&#26368;&#20026;&#26126;&#26174;&#65292;&#36825;&#34920;&#26126;&#23427;&#20204;&#22312;&#38590;&#20197;&#33719;&#24471;&#22823;&#22411;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional neural networks (CNNs) are increasingly being used to automate segmentation of organs-at-risk in radiotherapy. Since large sets of highly curated data are scarce, we investigated how much data is required to train accurate and robust head and neck auto-segmentation models. For this, an established 3D CNN was trained from scratch with different sized datasets (25-1000 scans) to segment the brainstem, parotid glands and spinal cord in CTs. Additionally, we evaluated multiple ensemble techniques to improve the performance of these models. The segmentations improved with training set size up to 250 scans and the ensemble methods significantly improved performance for all organs. The impact of the ensemble methods was most notable in the smallest datasets, demonstrating their potential for use in cases where large training datasets are difficult to obtain.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;GPT-3&#12289;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#22312;&#20154;&#31867;&#24605;&#32500;&#27169;&#24335;&#20013;&#30340;&#34920;&#29616;, &#36816;&#29992;&#35748;&#35782;&#35770;&#29702;&#35770;&#25552;&#20379;&#20102;&#31526;&#21495;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#30340;&#20154;&#31867;&#21028;&#26029;&#25968;&#25454;&#28857;&#20197;&#21450;ETR&#39044;&#27979;&#25968;&#25454;&#28857;&#30340;&#25968;&#37327;&#32423;&#23545;&#27169;&#22411;&#36827;&#34892;&#20102;&#26816;&#39564;&#12290;</title><link>http://arxiv.org/abs/2303.17276</link><description>&lt;p&gt;
&#22312;GPT&#25104;&#21151;&#21644;&#22833;&#36133;&#30340;&#24773;&#20917;&#19979;&#65292;&#20154;&#31867;&#36755;&#20837;&#20154;&#31867;&#36755;&#20986;&#65306;&#35770;GPT&#26397;&#21521;&#24120;&#35782;&#30340;&#36235;&#21516;&#24615;
&lt;/p&gt;
&lt;p&gt;
Humans in Humans Out: On GPT Converging Toward Common Sense in both Success and Failure. (arXiv:2303.17276v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;GPT-3&#12289;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#22312;&#20154;&#31867;&#24605;&#32500;&#27169;&#24335;&#20013;&#30340;&#34920;&#29616;, &#36816;&#29992;&#35748;&#35782;&#35770;&#29702;&#35770;&#25552;&#20379;&#20102;&#31526;&#21495;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#30340;&#20154;&#31867;&#21028;&#26029;&#25968;&#25454;&#28857;&#20197;&#21450;ETR&#39044;&#27979;&#25968;&#25454;&#28857;&#30340;&#25968;&#37327;&#32423;&#23545;&#27169;&#22411;&#36827;&#34892;&#20102;&#26816;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#35268;&#27169;&#21644;&#24494;&#35843;&#30340;&#22686;&#21152;&#20351;&#24471;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20363;&#22914;GPT&#30340;&#36755;&#20986;&#36136;&#37327;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#39640;&#12290;&#37492;&#20110;GPT-3&#21644;GPT-4&#37117;&#26159;&#20351;&#29992;&#22823;&#37327;&#30001;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#25105;&#20204;&#21487;&#20197;&#38382;&#20182;&#20204;&#30340;&#36755;&#20986;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#21453;&#26144;&#20102;&#20154;&#31867;&#24605;&#32500;&#30340;&#27169;&#24335;&#65292;&#26080;&#35770;&#26159;&#27491;&#30830;&#36824;&#26159;&#38169;&#35823;&#30340;&#24773;&#20917;&#12290;&#35748;&#35782;&#35770;&#29702;&#35770;&#25552;&#20379;&#20102;&#20851;&#20110;&#20154;&#31867;&#25104;&#21151;&#21644;&#22833;&#36133;&#30340;&#31526;&#21495;&#29983;&#25104;&#27169;&#22411;&#65292;&#21253;&#25324;&#21629;&#39064;&#12289;&#37327;&#21270;&#12289;&#27010;&#29575;&#25512;&#29702;&#21644;&#20915;&#31574;&#12290;&#26412;&#25991;&#23558;ETR&#30340;&#26368;&#36817;&#19968;&#20010;&#20070;&#26412;&#30340;61&#20010;&#26680;&#24515;&#25512;&#29702;&#21644;&#21028;&#26029;&#38382;&#39064;&#25552;&#20379;&#32473;&#20102;GPT-3&#12289;GPT-3.5&#21644;GPT-4&#65292;&#36825;&#20123;&#38382;&#39064;&#21253;&#25324;&#32463;&#36807;&#23454;&#39564;&#35777;&#23454;&#30340;&#20154;&#31867;&#21028;&#26029;&#25968;&#25454;&#28857;&#21644;ETR&#39044;&#27979;&#30340;&#25968;&#25454;&#28857;&#65292;&#21516;&#26102;&#21253;&#21547;&#27491;&#30830;&#30340;&#25512;&#29702;&#27169;&#24335;&#21644;&#35884;&#35823;&#21644;&#26694;&#26550;&#25928;&#24212;&#65288;ETR61&#22522;&#20934;&#27979;&#35797;&#65289;&#12290; ETR61&#21253;&#25324;&#20102;Wason&#30340;&#21345;&#29260;&#20219;&#21153;&#12289;&#38169;&#35273;&#25512;&#29702;&#12289;&#35825;&#39285;&#25928;&#24212;&#31561;&#32463;&#20856;&#26696;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Increase in computational scale and fine-tuning has seen a dramatic improvement in the quality of outputs of large language models (LLMs) like GPT. Given that both GPT-3 and GPT-4 were trained on large quantities of human-generated text, we might ask to what extent their outputs reflect patterns of human thinking, both for correct and incorrect cases. The Erotetic Theory of Reason (ETR) provides a symbolic generative model of both human success and failure in thinking, across propositional, quantified, and probabilistic reasoning, as well as decision-making. We presented GPT-3, GPT-3.5, and GPT-4 with 61 central inference and judgment problems from a recent book-length presentation of ETR, consisting of experimentally verified data-points on human judgment and extrapolated data-points predicted by ETR, with correct inference patterns as well as fallacies and framing effects (the ETR61 benchmark). ETR61 includes classics like Wason's card task, illusory inferences, the decoy effect, and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26412;&#20307;&#35770;&#22312;&#28151;&#21512;&#26234;&#33021;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#23427;&#22312;&#32553;&#23567;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#26234;&#33021;&#24046;&#36317;&#26041;&#38754;&#30340;&#28508;&#22312;&#20316;&#29992;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#26412;&#20307;&#35770;&#33021;&#22815;&#25552;&#39640;&#31995;&#32479;&#30340;&#36136;&#37327;&#21644;&#20934;&#30830;&#24615;&#65292;&#24182;&#36890;&#36807;&#21551;&#29992;&#25193;&#23637;&#20114;&#25805;&#20316;&#24615;&#12289;&#31995;&#32479;&#24037;&#31243;&#21644;&#21487;&#35299;&#37322;&#12289;&#36879;&#26126;&#31995;&#32479;&#31561;&#26041;&#38754;&#26469;&#21457;&#25381;&#26356;&#21152;&#20855;&#20307;&#30340;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.17262</link><description>&lt;p&gt;
&#28151;&#21512;&#26234;&#33021;&#20013;&#30340;&#26412;&#20307;&#35770;&#65306;&#19968;&#31687;&#31616;&#26126;&#30340;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Ontology in Hybrid Intelligence: a concise literature review. (arXiv:2303.17262v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26412;&#20307;&#35770;&#22312;&#28151;&#21512;&#26234;&#33021;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#23427;&#22312;&#32553;&#23567;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#26234;&#33021;&#24046;&#36317;&#26041;&#38754;&#30340;&#28508;&#22312;&#20316;&#29992;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#26412;&#20307;&#35770;&#33021;&#22815;&#25552;&#39640;&#31995;&#32479;&#30340;&#36136;&#37327;&#21644;&#20934;&#30830;&#24615;&#65292;&#24182;&#36890;&#36807;&#21551;&#29992;&#25193;&#23637;&#20114;&#25805;&#20316;&#24615;&#12289;&#31995;&#32479;&#24037;&#31243;&#21644;&#21487;&#35299;&#37322;&#12289;&#36879;&#26126;&#31995;&#32479;&#31561;&#26041;&#38754;&#26469;&#21457;&#25381;&#26356;&#21152;&#20855;&#20307;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#19981;&#26029;&#28436;&#36827;&#21644;&#22686;&#22810;&#30340;&#32972;&#26223;&#19979;&#65292;&#28151;&#21512;&#26234;&#33021;&#27491;&#22312;&#27969;&#34892;&#65292;&#20197;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#20043;&#38388;&#30340;&#24179;&#34913;&#20849;&#23384;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#31616;&#27905;&#32780;&#37325;&#28857;&#31361;&#20986;&#30340;&#27010;&#36848;&#65292;&#20171;&#32461;&#26412;&#20307;&#35770;&#22312;&#24191;&#27867;&#32972;&#26223;&#30340;&#28151;&#21512;&#26234;&#33021;&#20013;&#30340;&#24212;&#29992;&#65292;&#26080;&#35770;&#20854;&#23450;&#20041;&#22914;&#20309;&#65292;&#24182;&#23545;&#26412;&#20307;&#35770;&#22312;&#20943;&#23569;&#28151;&#21512;&#26234;&#33021;&#31995;&#32479;&#20013;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#26234;&#33021;&#20043;&#38388;&#24046;&#36317;&#30340;&#21487;&#33021;&#20316;&#29992;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#35752;&#35770;&#12290;&#38500;&#20102;&#26377;&#25928;&#20351;&#29992;&#26412;&#20307;&#35770;&#25552;&#20379;&#30340;&#20856;&#22411;&#22909;&#22788;&#22806;&#65292;&#22312;&#27010;&#24565;&#23618;&#38754;&#65292;&#25152;&#36827;&#34892;&#30340;&#20998;&#26512;&#25351;&#20986;&#65292;&#22312;&#36136;&#37327;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#20570;&#20986;&#20102;&#26174;&#30528;&#36129;&#29486;&#65292;&#21516;&#26102;&#22312;&#21551;&#29992;&#25193;&#23637;&#20114;&#25805;&#20316;&#24615;&#12289;&#31995;&#32479;&#24037;&#31243;&#21644;&#21487;&#35299;&#37322;&#12289;&#36879;&#26126;&#31995;&#32479;&#26041;&#38754;&#21457;&#25381;&#20102;&#26356;&#20855;&#20307;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a context of constant evolution and proliferation of AI technology, Hybrid Intelligence is gaining popularity to refer a balanced coexistence between human and artificial intelligence. On the other side, the concept has been extensively used in the past two decades to define models of intelligence involving more than one technology. This paper aims to provide (i) a concise and focused overview of the adoption of Ontology in the broad context of Hybrid Intelligence regardless of its definition and (ii) a critical discussion on the possible role of Ontology to reduce the gap between human and artificial intelligence within hybrid intelligent systems. Beside the typical benefits provided by an effective use of ontologies, at a conceptual level, the analysis conducted has pointed out a significant contribution to quality and accuracy, as well as a more specific role to enable extended interoperability, system engineering and explainable/transparent systems. On the other side, an applica
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#25991;&#31456;&#25581;&#31034;&#20102;&#20851;&#20110;&#31038;&#20132;&#26426;&#22120;&#20154;&#30740;&#31350;&#30340;&#26222;&#36941;&#35823;&#35299;&#65292;&#24378;&#35843;&#38656;&#35201;&#20197;&#20005;&#35880;&#12289;&#20844;&#27491;&#21644;&#36127;&#36131;&#20219;&#30340;&#26041;&#24335;&#35752;&#35770;&#34394;&#20551;&#20449;&#24687;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2303.17251</link><description>&lt;p&gt;
&#25581;&#24320;&#23545;&#31038;&#20132;&#26426;&#22120;&#20154;&#30740;&#31350;&#30340;&#35823;&#35299;
&lt;/p&gt;
&lt;p&gt;
Demystifying Misconceptions in Social Bots Research. (arXiv:2303.17251v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17251
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#31456;&#25581;&#31034;&#20102;&#20851;&#20110;&#31038;&#20132;&#26426;&#22120;&#20154;&#30740;&#31350;&#30340;&#26222;&#36941;&#35823;&#35299;&#65292;&#24378;&#35843;&#38656;&#35201;&#20197;&#20005;&#35880;&#12289;&#20844;&#27491;&#21644;&#36127;&#36131;&#20219;&#30340;&#26041;&#24335;&#35752;&#35770;&#34394;&#20551;&#20449;&#24687;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#26426;&#22120;&#20154;&#31185;&#23398;&#23547;&#27714;&#35299;&#20915;&#32593;&#32476;&#34394;&#20551;&#20449;&#24687;&#26368;&#21463;&#20105;&#35758;&#30340;&#24418;&#24335;&#20043;&#19968;&#30340;&#30693;&#35782;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#31038;&#20132;&#26426;&#22120;&#20154;&#30740;&#31350;&#21463;&#21040;&#26222;&#36941;&#30340;&#20559;&#35265;&#12289;&#22840;&#22823;&#30340;&#32467;&#26524;&#21644;&#35823;&#35299;&#30340;&#22256;&#25200;&#65292;&#36825;&#20123;&#37117;&#20026;&#27495;&#20041;&#12289;&#19981;&#20999;&#23454;&#38469;&#30340;&#26399;&#26395;&#21644;&#30475;&#20284;&#26080;&#27861;&#35843;&#21644;&#30340;&#21457;&#29616;&#25171;&#19979;&#20102;&#22522;&#30784;&#12290;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#23545;&#20110;&#30830;&#20445;&#21487;&#38752;&#30340;&#35299;&#20915;&#26041;&#26696;&#21644;&#37325;&#30003;&#31185;&#23398;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#31687;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#20462;&#35746;&#20102;&#31038;&#20132;&#26426;&#22120;&#20154;&#30740;&#31350;&#20013;&#30340;&#19968;&#20123;&#26368;&#26032;&#32467;&#26524;&#65292;&#24378;&#35843;&#21644;&#32416;&#27491;&#20102;&#20107;&#23454;&#38169;&#35823;&#20197;&#21450;&#26041;&#27861;&#35770;&#21644;&#27010;&#24565;&#38382;&#39064;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#25581;&#24320;&#20102;&#26222;&#36941;&#30340;&#35823;&#35299;&#65292;&#35299;&#20915;&#20102;&#26377;&#20851;&#22914;&#20309;&#35752;&#35770;&#31038;&#20132;&#26426;&#22120;&#20154;&#30740;&#31350;&#30340;&#22522;&#26412;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#20197;&#20005;&#35880;&#12289;&#20844;&#27491;&#21644;&#36127;&#36131;&#20219;&#30340;&#26041;&#24335;&#35752;&#35770;&#34394;&#20551;&#20449;&#24687;&#30740;&#31350;&#30340;&#24517;&#35201;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#30830;&#23450;&#24182;&#39539;&#26021;&#31038;&#20132;&#26426;&#22120;&#20154;&#30740;&#31350;&#30340;&#25903;&#25345;&#32773;&#21644;&#21453;&#23545;&#32773;&#24120;&#29992;&#30340;&#35884;&#35823;&#35770;&#35777;&#65292;&#25903;&#25345;&#36825;&#31181;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The science of social bots seeks knowledge and solutions to one of the most debated forms of online misinformation. Yet, social bots research is plagued by widespread biases, hyped results, and misconceptions that set the stage for ambiguities, unrealistic expectations, and seemingly irreconcilable findings. Overcoming such issues is instrumental towards ensuring reliable solutions and reaffirming the validity of the scientific method. In this contribution we revise some recent results in social bots research, highlighting and correcting factual errors as well as methodological and conceptual issues. More importantly, we demystify common misconceptions, addressing fundamental points on how social bots research is discussed. Our analysis surfaces the need to discuss misinformation research in a rigorous, unbiased, and responsible way. This article bolsters such effort by identifying and refuting common fallacious arguments used by both proponents and opponents of social bots research as
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#24182;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#40657;&#30418;&#35299;&#37322;&#26041;&#27861;&#8212;&#8212;BODEM&#65292;&#23427;&#37319;&#29992;&#20102;&#23616;&#37096;&#21644;&#36828;&#31243;&#25513;&#34109;&#29983;&#25104;&#22810;&#20010;&#29256;&#26412;&#30340;&#36755;&#20837;&#22270;&#20687;&#65292;&#20174;&#32780;&#27604;&#30446;&#21069;&#29992;&#20110;&#35299;&#37322;&#23545;&#35937;&#26816;&#27979;&#30340;&#20854;&#20182;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#25552;&#20379;&#26356;&#35814;&#32454;&#21644;&#26377;&#29992;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2303.17249</link><description>&lt;p&gt;
&#38754;&#21521;&#23545;&#35937;&#26816;&#27979;&#30340;&#27169;&#22411;&#26080;&#20851;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Model-agnostic explainable artificial intelligence for object detection in image data. (arXiv:2303.17249v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#24182;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#40657;&#30418;&#35299;&#37322;&#26041;&#27861;&#8212;&#8212;BODEM&#65292;&#23427;&#37319;&#29992;&#20102;&#23616;&#37096;&#21644;&#36828;&#31243;&#25513;&#34109;&#29983;&#25104;&#22810;&#20010;&#29256;&#26412;&#30340;&#36755;&#20837;&#22270;&#20687;&#65292;&#20174;&#32780;&#27604;&#30446;&#21069;&#29992;&#20110;&#35299;&#37322;&#23545;&#35937;&#26816;&#27979;&#30340;&#20854;&#20182;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#25552;&#20379;&#26356;&#35814;&#32454;&#21644;&#26377;&#29992;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35937;&#26816;&#27979;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#20043;&#19968;&#65292;&#36890;&#36807;&#24320;&#21457;&#22823;&#22411;&#22797;&#26434;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#32463;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#36879;&#26126;&#24230;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#21487;&#33021;&#22952;&#30861;&#36825;&#20123;&#27169;&#22411;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26159;&#19968;&#20010;&#30740;&#31350;&#39046;&#22495;&#65292;&#20854;&#20013;&#24320;&#21457;&#26041;&#27861;&#26469;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#31995;&#32479;&#30340;&#34892;&#20026;&#12289;&#20915;&#31574;&#36923;&#36753;&#21644;&#28431;&#27934;&#12290;&#26412;&#25991;&#20026;&#20102;&#35299;&#37322;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#23545;&#35937;&#26816;&#27979;&#31995;&#32479;&#35774;&#35745;&#21644;&#23454;&#29616;&#20102;&#19968;&#31181;&#21517;&#20026;Black-box Object Detection Explanation by Masking&#65288;BODEM&#65289;&#30340;&#40657;&#30418;&#35828;&#26126;&#26041;&#27861;&#65292;&#37319;&#29992;&#26032;&#30340;&#25513;&#34109;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23616;&#37096;&#21644;&#36828;&#31243;&#25513;&#34109;&#26469;&#29983;&#25104;&#36755;&#20837;&#22270;&#20687;&#30340;&#22810;&#20010;&#29256;&#26412;&#12290;&#23616;&#37096;&#25513;&#34109;&#29992;&#20110;&#24178;&#25200;&#30446;&#26631;&#23545;&#35937;&#20869;&#30340;&#20687;&#32032;&#65292;&#20197;&#20102;&#35299;&#23545;&#35937;&#26816;&#27979;&#22120;&#23545;&#36825;&#20123;&#21464;&#21270;&#30340;&#21453;&#24212;&#65292;&#32780;&#36828;&#31243;&#25513;&#34109;&#21017;&#29992;&#20110;&#30740;&#31350;&#23545;&#35937;&#26816;&#27979;&#22120;&#22312;&#22270;&#20687;&#32972;&#26223;&#19978;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#29992;&#20110;&#35299;&#37322;&#23545;&#35937;&#26816;&#27979;&#30340;&#20854;&#20182;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;BODEM&#25552;&#20379;&#20102;&#26356;&#35814;&#32454;&#21644;&#26377;&#29992;&#30340;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Object detection is a fundamental task in computer vision, which has been greatly progressed through developing large and intricate deep learning models. However, the lack of transparency is a big challenge that may not allow the widespread adoption of these models. Explainable artificial intelligence is a field of research where methods are developed to help users understand the behavior, decision logics, and vulnerabilities of AI-based systems. Black-box explanation refers to explaining decisions of an AI system without having access to its internals. In this paper, we design and implement a black-box explanation method named Black-box Object Detection Explanation by Masking (BODEM) through adopting a new masking approach for AI-based object detection systems. We propose local and distant masking to generate multiple versions of an input image. Local masks are used to disturb pixels within a target object to figure out how the object detector reacts to these changes, while distant ma
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20221;&#33258;&#21160;&#39550;&#39542;&#21644;&#26234;&#33021;&#27773;&#36710;&#30340;&#24635;&#25216;&#26415;&#35843;&#26597;&#32508;&#36848;&#65288;SoS&#65289;&#65292;&#24635;&#32467;&#20102;&#20854;&#21382;&#21490;&#65292;&#24635;&#32467;&#20102;&#37324;&#31243;&#30865;&#65292;&#24182;&#25552;&#20379;&#20102;&#23637;&#26395;&#12289;&#20262;&#29702;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2303.17220</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#19982;&#26234;&#33021;&#27773;&#36710;&#30340;&#37324;&#31243;&#30865;&#65306;&#35843;&#26597;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Milestones in Autonomous Driving and Intelligent Vehicles: Survey of Surveys. (arXiv:2303.17220v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17220
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20221;&#33258;&#21160;&#39550;&#39542;&#21644;&#26234;&#33021;&#27773;&#36710;&#30340;&#24635;&#25216;&#26415;&#35843;&#26597;&#32508;&#36848;&#65288;SoS&#65289;&#65292;&#24635;&#32467;&#20102;&#20854;&#21382;&#21490;&#65292;&#24635;&#32467;&#20102;&#37324;&#31243;&#30865;&#65292;&#24182;&#25552;&#20379;&#20102;&#23637;&#26395;&#12289;&#20262;&#29702;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#26041;&#20415;&#12289;&#23433;&#20840;&#21644;&#32463;&#27982;&#25928;&#30410;&#30340;&#21407;&#22240;&#65292;&#23545;&#33258;&#21160;&#39550;&#39542;&#65288;AD&#65289;&#21644;&#26234;&#33021;&#27773;&#36710;&#65288;IV&#65289;&#30340;&#20852;&#36259;&#27491;&#22312;&#36805;&#36895;&#22686;&#38271;&#12290;&#34429;&#28982;&#19968;&#20123;&#35843;&#26597;&#24050;&#32463;&#22238;&#39038;&#20102;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#25104;&#26524;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23616;&#38480;&#20110;&#29305;&#23450;&#20219;&#21153;&#65292;&#32570;&#20047;&#31995;&#32479;&#24615;&#30340;&#24635;&#32467;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20221;&#23545;AD&#21644;IV&#30340;&#24635;&#25216;&#26415;&#30340;&#35843;&#26597;&#32508;&#36848;&#65288;SoS&#65289;&#65292;&#22238;&#39038;&#20102;&#21382;&#21490;&#65292;&#24635;&#32467;&#20102;&#37324;&#31243;&#30865;&#65292;&#24182;&#25552;&#20379;&#20102;&#23637;&#26395;&#12289;&#20262;&#29702;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#26412;&#25991;&#26159;&#31532;&#19968;&#31687;&#20851;&#20110;AD&#21644;IV&#37324;&#31243;&#30865;&#30340;SoS&#65292;&#19982;&#20854;&#20182;&#20004;&#31687;&#25216;&#26415;&#35843;&#26597;&#19968;&#36215;&#26500;&#25104;&#20102;&#25105;&#20204;&#30340;&#23436;&#25972;&#30740;&#31350;&#24037;&#20316;&#12290;&#25105;&#20204;&#39044;&#35745;&#36825;&#31687;&#25991;&#31456;&#23558;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#21021;&#23398;&#32773;&#24102;&#26469;&#26032;&#39062;&#32780;&#22810;&#26679;&#30340;&#35265;&#35299;&#65292;&#24182;&#20316;&#20026;&#36807;&#21435;&#21644;&#26410;&#26469;&#20043;&#38388;&#30340;&#26725;&#26753;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interest in autonomous driving (AD) and intelligent vehicles (IVs) is growing at a rapid pace due to the convenience, safety, and economic benefits. Although a number of surveys have reviewed research achievements in this field, they are still limited in specific tasks, lack of systematic summary and research directions in the future. Here we propose a Survey of Surveys (SoS) for total technologies of AD and IVs that reviews the history, summarizes the milestones, and provides the perspectives, ethics, and future research directions. To our knowledge, this article is the first SoS with milestones in AD and IVs, which constitutes our complete research work together with two other technical surveys. We anticipate that this article will bring novel and diverse insights to researchers and abecedarians, and serve as a bridge between past and future.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#25506;&#35752;&#21033;&#29992;&#21512;&#25104;&#35270;&#35273;&#25968;&#25454;&#36827;&#34892;&#21487;&#35270;&#35821;&#38899;&#35782;&#21035;&#65288;VSR&#65289;&#30340;&#28508;&#21147;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;SynthVSR&#36890;&#36807;&#21033;&#29992;&#35821;&#38899;&#39537;&#21160;&#30340;&#21767;&#37096;&#21160;&#30011;&#27169;&#22411;&#21512;&#25104;&#22823;&#35268;&#27169;&#25968;&#25454;&#65292;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;VSR&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.17200</link><description>&lt;p&gt;
SynthVSR&#65306;&#20351;&#29992;&#21512;&#25104;&#30417;&#30563;&#23454;&#29616;&#21487;&#35270;&#35821;&#38899;&#35782;&#21035;&#30340;&#35268;&#27169;&#21270;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;
SynthVSR: Scaling Up Visual Speech Recognition With Synthetic Supervision. (arXiv:2303.17200v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25506;&#35752;&#21033;&#29992;&#21512;&#25104;&#35270;&#35273;&#25968;&#25454;&#36827;&#34892;&#21487;&#35270;&#35821;&#38899;&#35782;&#21035;&#65288;VSR&#65289;&#30340;&#28508;&#21147;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;SynthVSR&#36890;&#36807;&#21033;&#29992;&#35821;&#38899;&#39537;&#21160;&#30340;&#21767;&#37096;&#21160;&#30011;&#27169;&#22411;&#21512;&#25104;&#22823;&#35268;&#27169;&#25968;&#25454;&#65292;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;VSR&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#21487;&#35270;&#35821;&#38899;&#35782;&#21035;&#65288;VSR&#65289;&#39046;&#22495;&#20013;&#25253;&#36947;&#30340;&#26368;&#26032;&#25104;&#26524;&#36890;&#24120;&#20381;&#36182;&#20110;&#36234;&#26469;&#36234;&#22810;&#30340;&#35270;&#39057;&#25968;&#25454;&#65292;&#32780;&#20844;&#24320;&#21487;&#29992;&#30340;&#36716;&#24405;&#35270;&#39057;&#25968;&#25454;&#38598;&#22823;&#23567;&#26377;&#38480;&#12290;&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#21033;&#29992;&#21512;&#25104;&#35270;&#35273;&#25968;&#25454;&#36827;&#34892;VSR&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#8220;SynthVSR&#8221;&#36890;&#36807;&#21512;&#25104;&#22068;&#21767;&#21160;&#20316;&#26174;&#33879;&#25552;&#39640;&#20102;VSR&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;SynthVSR&#32972;&#21518;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#21033;&#29992;&#19968;&#20010;&#22522;&#20110;&#35821;&#38899;&#39537;&#21160;&#30340;&#21767;&#37096;&#21160;&#30011;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26681;&#25454;&#36755;&#20837;&#35821;&#38899;&#29983;&#25104;&#21767;&#37096;&#21160;&#20316;&#12290;&#35813;&#35821;&#38899;&#39537;&#21160;&#30340;&#21767;&#37096;&#21160;&#30011;&#27169;&#22411;&#26159;&#22312;&#26410;&#26631;&#35760;&#30340;&#38899;&#35270;&#39057;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#26377;&#26631;&#35760;&#30340;&#35270;&#39057;&#21487;&#29992;&#26102;&#36827;&#19968;&#27493;&#20248;&#21270;&#20026;&#39044;&#35757;&#32451;&#30340;VSR&#27169;&#22411;&#12290;&#30001;&#20110;&#23384;&#22312;&#22823;&#37327;&#30340;&#36716;&#24405;&#22768;&#23398;&#25968;&#25454;&#21644;&#38754;&#37096;&#22270;&#20687;&#65292;&#25105;&#20204;&#33021;&#22815;&#20351;&#29992;&#25152;&#25552;&#20986;&#30340;&#21767;&#37096;&#21160;&#30011;&#27169;&#22411;&#29983;&#25104;&#22823;&#35268;&#27169;&#30340;&#21512;&#25104;&#25968;&#25454;&#29992;&#20110;&#21322;&#30417;&#30563;VSR&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#8220;Lip Reading in the Wild&#8221;&#65288;Lrw&#65289;&#35780;&#27979;&#22522;&#20934;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently reported state-of-the-art results in visual speech recognition (VSR) often rely on increasingly large amounts of video data, while the publicly available transcribed video datasets are limited in size. In this paper, for the first time, we study the potential of leveraging synthetic visual data for VSR. Our method, termed SynthVSR, substantially improves the performance of VSR systems with synthetic lip movements. The key idea behind SynthVSR is to leverage a speech-driven lip animation model that generates lip movements conditioned on the input speech. The speech-driven lip animation model is trained on an unlabeled audio-visual dataset and could be further optimized towards a pre-trained VSR model when labeled videos are available. As plenty of transcribed acoustic data and face images are available, we are able to generate large-scale synthetic data using the proposed lip animation model for semi-supervised VSR training. We evaluate the performance of our approach on the la
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#21253;&#21547;1.2TB&#25991;&#26412;&#30340;&#21271;&#27431;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#20026;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#37325;&#35201;&#36164;&#28304;&#65292;&#20419;&#36827;&#20102;&#21271;&#27431;&#35821;&#35328;&#30340;LLMs&#30340;&#24320;&#21457;&#12290;</title><link>http://arxiv.org/abs/2303.17183</link><description>&lt;p&gt;
&#12298;&#21271;&#27431;&#26681;&#26729;:&#19968;&#20010;1.2TB&#30340;&#21271;&#27431;&#35821;&#35328;&#24314;&#27169;&#25968;&#25454;&#38598;&#12299;
&lt;/p&gt;
&lt;p&gt;
The Nordic Pile: A 1.2TB Nordic Dataset for Language Modeling. (arXiv:2303.17183v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#21253;&#21547;1.2TB&#25991;&#26412;&#30340;&#21271;&#27431;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#20026;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#37325;&#35201;&#36164;&#28304;&#65292;&#20419;&#36827;&#20102;&#21271;&#27431;&#35821;&#35328;&#30340;LLMs&#30340;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;( LLMs )&#38656;&#35201;&#28023;&#37327;&#30340;&#25991;&#26412;&#25968;&#25454;&#65292;LLMs&#30340;&#24615;&#33021;&#36890;&#24120;&#19982;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#21644;&#36136;&#37327;&#30456;&#20851;&#12290;&#36825;&#24847;&#21619;&#30528;&#22312;&#21271;&#27431;&#31561;&#35821;&#31181;&#20013;&#24314;&#31435;LLMs&#21487;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#20854;&#25991;&#26412;&#35821;&#26009;&#24211;&#30340;&#21487;&#29992;&#24615;&#26377;&#38480;&#12290;&#20026;&#20102;&#20419;&#36827;&#21271;&#27431;&#35821;&#35328;&#30340;LLMs&#24320;&#21457;&#65292;&#25105;&#20204;&#31574;&#21010;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;1.2TB&#30340;&#25991;&#26412;&#65292;&#28085;&#30422;&#20102;&#25152;&#26377;&#37325;&#35201;&#30340;&#21271;&#26085;&#32819;&#26364;&#35821;&#35328;&#65288;&#20025;&#40614;&#35821;&#12289;&#20912;&#23707;&#35821;&#12289;&#25386;&#23041;&#35821;&#21644;&#29790;&#20856;&#35821;&#65289;&#65292;&#20197;&#21450;&#19968;&#20123;&#39640;&#36136;&#37327;&#30340;&#33521;&#25991;&#25968;&#25454;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;&#25105;&#20204;&#25910;&#38598;&#12289;&#28165;&#29702;&#21644;&#36807;&#28388;&#35813;&#25968;&#25454;&#38598;&#30340;&#32771;&#34385;&#21644;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-training Large Language Models (LLMs) require massive amounts of text data, and the performance of the LLMs typically correlates with the scale and quality of the datasets. This means that it may be challenging to build LLMs for smaller languages such as Nordic ones, where the availability of text corpora is limited. In order to facilitate the development of the LLMS in the Nordic languages, we curate a high-quality dataset consisting of 1.2TB of text, in all of the major North Germanic languages (Danish, Icelandic, Norwegian, and Swedish), as well as some high-quality English data. This paper details our considerations and processes for collecting, cleaning, and filtering the dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;SageMath&#21644;ChatGPT&#30340;&#35843;&#26597;&#21644;&#27604;&#36739;&#65292;&#26088;&#22312;&#20026;&#23398;&#29983;&#25552;&#20379;&#26356;&#28145;&#20837;&#30340;&#30697;&#38453;&#20998;&#35299;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2303.17163</link><description>&lt;p&gt;
&#30697;&#38453;&#23545;&#35282;&#21270;&#19982;&#22855;&#24322;&#20540;&#20998;&#35299;&#65306;&#38745;&#24577;SageMath&#19982;&#21160;&#24577;ChatGPT&#24182;&#32622;
&lt;/p&gt;
&lt;p&gt;
Matrix diagonalization and singular value decomposition: Static SageMath and dynamic ChatGPT juxtaposed. (arXiv:2303.17163v1 [math.HO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;SageMath&#21644;ChatGPT&#30340;&#35843;&#26597;&#21644;&#27604;&#36739;&#65292;&#26088;&#22312;&#20026;&#23398;&#29983;&#25552;&#20379;&#26356;&#28145;&#20837;&#30340;&#30697;&#38453;&#20998;&#35299;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26412;&#31185;&#32447;&#24615;&#20195;&#25968;&#25945;&#23398;&#20013;&#23398;&#29983;&#32463;&#24120;&#38754;&#20020;&#30340;&#19968;&#20123;&#22256;&#38590;&#65292;&#24182;&#30830;&#23450;&#20102;&#20182;&#20204;&#22312;&#22788;&#29702;&#38656;&#35201;&#31639;&#27861;&#24605;&#32500;&#25216;&#33021;&#30340;&#35805;&#39064;&#26102;&#32463;&#24120;&#36935;&#21040;&#30340;&#19968;&#20123;&#24120;&#35265;&#38169;&#35823;&#21644;&#22256;&#38590;&#65292;&#22914;&#30697;&#38453;&#20998;&#35299;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#65288;&#27491;&#20132;&#65289;&#23545;&#35282;&#21270;&#21644;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20351;&#29992;SageMath&#25506;&#32034;&#36825;&#20123;&#20027;&#39064;&#30340;&#21487;&#33021;&#24615;&#65292;&#23427;&#26159;&#19968;&#31181;&#22522;&#20110;Python&#30340;&#20813;&#36153;&#24320;&#28304;&#36719;&#20214;&#35745;&#31639;&#26426;&#20195;&#25968;&#31995;&#32479;&#65288;CAS&#65289;&#65292;&#23613;&#31649;&#20854;&#36755;&#20986;&#26412;&#36136;&#19978;&#26159;&#38745;&#24577;&#30340;&#65292;&#20294;&#24050;&#34987;&#30830;&#23450;&#20026;&#22312;&#35745;&#31639;&#36807;&#31243;&#20013;&#24110;&#21161;&#35768;&#22810;&#23398;&#29983;&#30340;&#26377;&#29992;&#24037;&#20855;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#21521;&#32842;&#22825;&#26426;&#22120;&#20154;&#35810;&#38382;&#26377;&#20851;&#35813;&#20027;&#39064;&#30340;&#31034;&#20363;&#25110;&#35299;&#20915;&#38382;&#39064;&#26469;&#25506;&#32034;&#21160;&#24577;ChatGPT&#65292;&#21363;&#36890;&#36807;&#20174;&#29305;&#23450;&#30697;&#38453;&#26500;&#36896;&#65288;&#27491;&#20132;&#65289;&#23545;&#35282;&#21270;&#25110;SVD&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;&#36890;&#36807;&#22312;&#32447;&#24615;&#20195;&#25968;&#20013;&#24041;&#22266;&#22522;&#26412;&#27010;&#24565;&#24182;&#36890;&#36807;&#26377;&#25928;&#23454;&#36341;&#25552;&#39640;&#35745;&#31639;&#25216;&#33021;&#65292;&#25105;&#20204;&#26088;&#22312;&#20026;&#23398;&#29983;&#25552;&#20379;&#26356;&#28145;&#20837;&#30340;&#30697;&#38453;&#20998;&#35299;&#29702;&#35299;&#65292;&#24182;&#20026;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#20570;&#22909;&#20934;&#22791;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigated some difficulties that students often face when studying linear algebra at the undergraduate level, and identified some common mistakes and difficulties they often encountered when dealing with topics that require algorithmic thinking skills such as matrix factorization. In particular, we focused on (orthogonal) diagonalization and singular value decomposition (SVD). We also offered the possibility of exploring these topics using SageMath, a Python-based free open software computer algebra system (CAS) that has been identified to be useful for assisting many students in the computational process even though its output is static by nature. We then explored dynamic ChatGPT by inquiring the chatbot about the topic, either by asking to provide an example or to solve a problem, that is by constructing an (orthogonal) diagonalization or SVD from a particular matrix. By consolidating essential concepts in linear algebra and improving computational skills through effective prac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;TreePiece&#25216;&#26415;&#23558;&#35299;&#26512;&#26641;&#20998;&#21106;&#25104;&#23376;&#26641;&#65292;&#20197;&#21152;&#36895;&#33258;&#22238;&#24402;&#27169;&#22411;&#29992;&#20110;&#35821;&#20041;&#20998;&#26512;&#30340;&#36807;&#31243;&#65292;&#30456;&#27604;&#26631;&#20934;&#33258;&#22238;&#24402;&#25552;&#39640;&#20102;4.6&#20493;&#30340;&#35299;&#30721;&#36895;&#24230;&#65292;&#24182;&#22312;&#36895;&#24230;&#30456;&#24403;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#24615;&#26356;&#39640;&#20110;&#38750;&#33258;&#22238;&#24402;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.17161</link><description>&lt;p&gt;
TreePiece&#65306;&#36890;&#36807;&#26641;&#29366;&#20998;&#21106;&#25552;&#39640;&#35821;&#20041;&#35299;&#26512;&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;
TreePiece: Faster Semantic Parsing via Tree Tokenization. (arXiv:2303.17161v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17161
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;TreePiece&#25216;&#26415;&#23558;&#35299;&#26512;&#26641;&#20998;&#21106;&#25104;&#23376;&#26641;&#65292;&#20197;&#21152;&#36895;&#33258;&#22238;&#24402;&#27169;&#22411;&#29992;&#20110;&#35821;&#20041;&#20998;&#26512;&#30340;&#36807;&#31243;&#65292;&#30456;&#27604;&#26631;&#20934;&#33258;&#22238;&#24402;&#25552;&#39640;&#20102;4.6&#20493;&#30340;&#35299;&#30721;&#36895;&#24230;&#65292;&#24182;&#22312;&#36895;&#24230;&#30456;&#24403;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#24615;&#26356;&#39640;&#20110;&#38750;&#33258;&#22238;&#24402;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#22238;&#24402;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38382;&#39064;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#21253;&#25324;&#35821;&#20041;&#35299;&#26512;&#65288;&#19968;&#31181;&#23558;&#33258;&#28982;&#35821;&#35328;&#36716;&#25442;&#20026;&#26426;&#22120;&#21487;&#35835;&#30340;&#35299;&#26512;&#26641;&#30340;&#20219;&#21153;&#65289;&#12290;&#28982;&#32780;&#65292;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#39034;&#24207;&#39044;&#27979;&#36807;&#31243;&#21487;&#33021;&#20250;&#24456;&#24930;&#12290;&#20026;&#20102;&#21152;&#36895;&#29992;&#20110;&#35821;&#20041;&#20998;&#26512;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#25216;&#26415;&#65292;&#31216;&#20026;TreePiece&#65292;&#23427;&#23558;&#35299;&#26512;&#26641;&#20998;&#21106;&#25104;&#23376;&#26641;&#65292;&#24182;&#22312;&#27599;&#20010;&#35299;&#30721;&#27493;&#39588;&#20013;&#29983;&#25104;&#19968;&#20010;&#23376;&#26641;&#12290;&#22312;TopV2&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;TreePiece&#30340;&#35299;&#30721;&#36895;&#24230;&#27604;&#26631;&#20934;&#33258;&#22238;&#24402;&#24555;4.6&#20493;&#65292;&#27604;&#38750;&#33258;&#22238;&#24402;&#26041;&#27861;&#65288;NAR&#65289;&#30340;&#36895;&#24230;&#30456;&#24403;&#20294;&#20934;&#30830;&#24615;&#26174;&#30528;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autoregressive (AR) encoder-decoder neural networks have proved successful in many NLP problems, including Semantic Parsing -- a task that translates natural language to machine-readable parse trees. However, the sequential prediction process of AR models can be slow. To accelerate AR for semantic parsing, we introduce a new technique called TreePiece that tokenizes a parse tree into subtrees and generates one subtree per decoding step. On TopV2 benchmark, TreePiece shows 4.6 times faster decoding speed than standard AR, and comparable speed but significantly higher accuracy compared to Non-Autoregressive (NAR).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21028;&#21035;&#24615;&#20449;&#24687;&#21644;&#33258;&#30001;&#25991;&#26412;&#30340;&#38750;&#20405;&#20837;&#24335;&#24494;&#35843;&#25216;&#26415;&#65292;&#20197;&#23454;&#29616;&#22810;&#26679;&#24615;&#21644;&#39640;&#20934;&#30830;&#29575;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.17155</link><description>&lt;p&gt;
&#22522;&#20110;&#21028;&#21035;&#24615;&#31867;&#26631;&#30340;&#25991;&#26412;&#22270;&#29255;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Discriminative Class Tokens for Text-to-Image Diffusion Models. (arXiv:2303.17155v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21028;&#21035;&#24615;&#20449;&#24687;&#21644;&#33258;&#30001;&#25991;&#26412;&#30340;&#38750;&#20405;&#20837;&#24335;&#24494;&#35843;&#25216;&#26415;&#65292;&#20197;&#23454;&#29616;&#22810;&#26679;&#24615;&#21644;&#39640;&#20934;&#30830;&#29575;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#20351;&#24471;&#29983;&#25104;&#22810;&#26679;&#19988;&#39640;&#36136;&#37327;&#22270;&#29255;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36755;&#20837;&#25991;&#26412;&#30340;&#27495;&#20041;&#65292;&#29983;&#25104;&#30340;&#22270;&#29255;&#24120;&#24120;&#26080;&#27861;&#25551;&#32472;&#20986;&#24494;&#22937;&#30340;&#32454;&#33410;&#19988;&#26131;&#20110;&#20986;&#38169;&#12290;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#20043;&#19968;&#26159;&#22312;&#26377;&#31867;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#32570;&#28857;&#22312;&#20110;&#65306;&#65288;i&#65289;&#19982;&#29992;&#20110;&#35757;&#32451;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#29228;&#21462;&#30340;&#25991;&#26412;-&#22270;&#20687;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;&#26377;&#31867;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#36890;&#24120;&#36739;&#23567;&#65292;&#22240;&#27492;&#29983;&#25104;&#30340;&#22270;&#29255;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#20250;&#20005;&#37325;&#21463;&#24433;&#21709;&#65292;&#25110;&#65288;ii&#65289;&#36755;&#20837;&#26159;&#30828;&#32534;&#30721;&#30340;&#26631;&#31614;&#65292;&#32780;&#19981;&#26159;&#33258;&#30001;&#25991;&#26412;&#65292;&#36825;&#38480;&#21046;&#20102;&#23545;&#29983;&#25104;&#30340;&#22270;&#20687;&#30340;&#25511;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#20405;&#20837;&#24335;&#30340;&#24494;&#35843;&#25216;&#26415;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#20998;&#31867;&#22120;&#30340;&#21028;&#21035;&#24615;&#20449;&#21495;&#24341;&#23548;&#29983;&#25104;&#36807;&#31243;&#65292;&#26082;&#21457;&#25381;&#20102;&#33258;&#30001;&#25991;&#26412;&#30340;&#34920;&#36798;&#28508;&#21147;&#65292;&#21448;&#33021;&#22815;&#23454;&#29616;&#39640;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in text-to-image diffusion models have enabled the generation of diverse and high-quality images. However, generated images often fall short of depicting subtle details and are susceptible to errors due to ambiguity in the input text. One way of alleviating these issues is to train diffusion models on class-labeled datasets. This comes with a downside, doing so limits their expressive power: (i) supervised datasets are generally small compared to large-scale scraped text-image datasets on which text-to-image models are trained, and so the quality and diversity of generated images are severely affected, or (ii) the input is a hard-coded label, as opposed to free-form text, which limits the control over the generated images.  In this work, we propose a non-invasive fine-tuning technique that capitalizes on the expressive potential of free-form text while achieving high accuracy through discriminative signals from a pretrained classifier, which guides the generation. This 
&lt;/p&gt;</description></item><item><title>DAMO-StreamNet&#26159;&#19968;&#20010;&#20248;&#21270;&#33258;&#21160;&#39550;&#39542;&#20013;&#27969;&#24335;&#24863;&#30693;&#30340;&#26694;&#26550;&#65292;&#23427;&#34701;&#21512;&#20102;YOLO&#31995;&#21015;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#36890;&#36807;&#39048;&#37096;&#32467;&#26500;&#12289;&#21452;&#20998;&#25903;&#32467;&#26500;&#12289;&#33976;&#39311;&#26426;&#21046;&#21644;&#23454;&#26102;&#39044;&#27979;&#26426;&#21046;&#31561;&#20851;&#38190;&#21019;&#26032;&#28857;&#65292;&#25552;&#20379;&#20102;&#23574;&#31471;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.17144</link><description>&lt;p&gt;
DAMO-StreamNet&#65306;&#33258;&#21160;&#39550;&#39542;&#20013;&#27969;&#24335;&#24863;&#30693;&#30340;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
DAMO-StreamNet: Optimizing Streaming Perception in Autonomous Driving. (arXiv:2303.17144v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17144
&lt;/p&gt;
&lt;p&gt;
DAMO-StreamNet&#26159;&#19968;&#20010;&#20248;&#21270;&#33258;&#21160;&#39550;&#39542;&#20013;&#27969;&#24335;&#24863;&#30693;&#30340;&#26694;&#26550;&#65292;&#23427;&#34701;&#21512;&#20102;YOLO&#31995;&#21015;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#36890;&#36807;&#39048;&#37096;&#32467;&#26500;&#12289;&#21452;&#20998;&#25903;&#32467;&#26500;&#12289;&#33976;&#39311;&#26426;&#21046;&#21644;&#23454;&#26102;&#39044;&#27979;&#26426;&#21046;&#31561;&#20851;&#38190;&#21019;&#26032;&#28857;&#65292;&#25552;&#20379;&#20102;&#23574;&#31471;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#26102;&#24863;&#30693;&#65292;&#25110;&#32773;&#35828;&#27969;&#24335;&#24863;&#30693;&#65292;&#26159;&#33258;&#21160;&#39550;&#39542;&#20013;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#20294;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#30340;&#26041;&#38754;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DAMO-StreamNet&#65292;&#23427;&#23558;YOLO&#31995;&#21015;&#30340;&#26368;&#26032;&#36827;&#23637;&#19982;&#31354;&#38388;&#21644;&#26102;&#38388;&#24863;&#30693;&#26426;&#21046;&#30340;&#20840;&#38754;&#20998;&#26512;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#23574;&#31471;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;DAMO-StreamNet&#30340;&#20851;&#38190;&#21019;&#26032;&#28857;&#21253;&#25324;&#65306;(1)&#19968;&#20010;&#40065;&#26834;&#30340;&#39048;&#37096;&#32467;&#26500;&#65292;&#34701;&#21512;&#20102;&#21487;&#21464;&#24418;&#21367;&#31215;&#65292;&#22686;&#24378;&#20102;&#24863;&#21463;&#37326;&#21644;&#29305;&#24449;&#23545;&#40784;&#33021;&#21147;&#12290;(2)&#19968;&#20010;&#21452;&#20998;&#25903;&#32467;&#26500;&#65292;&#25972;&#21512;&#20102;&#30701;&#36890;&#36947;&#35821;&#20041;&#29305;&#24449;&#21644;&#38271;&#36890;&#36947;&#26102;&#24207;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#36816;&#21160;&#29366;&#24577;&#39044;&#27979;&#30340;&#31934;&#24230;&#12290;(3)&#19968;&#20010;&#22312;logits&#32423;&#21035;&#19978;&#36827;&#34892;&#30340;&#33976;&#39311;&#26426;&#21046;&#65292;&#23545;&#40784;&#25945;&#24072;&#32593;&#32476;&#21644;&#23398;&#29983;&#32593;&#32476;&#30340;&#35821;&#20041;&#31354;&#38388;&#12290;(4)&#19968;&#20010;&#23454;&#26102;&#39044;&#27979;&#26426;&#21046;&#65292;&#26356;&#26032;&#25903;&#25345;&#24103;&#30340;&#29305;&#24449;&#19982;&#24403;&#21069;&#24103;&#65292;&#30830;&#20445;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#26080;&#32541;&#27969;&#24335;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time perception, or streaming perception, is a crucial aspect of autonomous driving that has yet to be thoroughly explored in existing research. To address this gap, we present DAMO-StreamNet, an optimized framework that combines recent advances from the YOLO series with a comprehensive analysis of spatial and temporal perception mechanisms, delivering a cutting-edge solution. The key innovations of DAMO-StreamNet are: (1) A robust neck structure incorporating deformable convolution, enhancing the receptive field and feature alignment capabilities. (2) A dual-branch structure that integrates short-path semantic features and long-path temporal features, improving motion state prediction accuracy. (3) Logits-level distillation for efficient optimization, aligning the logits of teacher and student networks in semantic space. (4) A real-time forecasting mechanism that updates support frame features with the current frame, ensuring seamless streaming perception during inference. Our ex
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#32534;&#31243;&#21161;&#25163;&#22312;&#24555;&#36895;&#23436;&#25104;&#32534;&#31243;&#20219;&#21153;&#26041;&#38754;&#26377;&#29992;&#65292;&#20294;&#36755;&#20986;&#30340;&#20195;&#30721;&#19981;&#36866;&#21512;&#24320;&#21457;&#32773;&#65292;&#23548;&#33268;&#20182;&#20204;&#19981;&#39640;&#39057;&#25509;&#21463;AI&#32534;&#31243;&#21161;&#25163;&#30340;&#21021;&#22987;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2303.17125</link><description>&lt;p&gt;
&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#32534;&#31243;&#21161;&#25163;&#30340;&#21487;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
Understanding the Usability of AI Programming Assistants. (arXiv:2303.17125v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17125
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#32534;&#31243;&#21161;&#25163;&#22312;&#24555;&#36895;&#23436;&#25104;&#32534;&#31243;&#20219;&#21153;&#26041;&#38754;&#26377;&#29992;&#65292;&#20294;&#36755;&#20986;&#30340;&#20195;&#30721;&#19981;&#36866;&#21512;&#24320;&#21457;&#32773;&#65292;&#23548;&#33268;&#20182;&#20204;&#19981;&#39640;&#39057;&#25509;&#21463;AI&#32534;&#31243;&#21161;&#25163;&#30340;&#21021;&#22987;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#24037;&#31243;&#31038;&#21306;&#36817;&#24180;&#26469;&#24191;&#27867;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#32534;&#31243;&#21161;&#25163;&#65288;&#20363;&#22914;GitHub Copilot&#65289;&#12290;&#28982;&#32780;&#22312;&#23454;&#36341;&#20013;&#65292;&#24320;&#21457;&#32773;&#24182;&#19981;&#39640;&#39057;&#25509;&#21463;AI&#32534;&#31243;&#21161;&#25163;&#30340;&#21021;&#22987;&#24314;&#35758;&#12290;&#36825;&#24341;&#21457;&#20102;&#19982;&#36825;&#20123;&#24037;&#20855;&#21487;&#29992;&#24615;&#30456;&#20851;&#30340;&#35768;&#22810;&#38382;&#39064;&#12290;&#20026;&#20102;&#20102;&#35299;&#24320;&#21457;&#32773;&#22312;&#20351;&#29992;&#36825;&#20123;&#24037;&#20855;&#26102;&#30340;&#23454;&#36341;&#24773;&#20917;&#21644;&#20182;&#20204;&#38754;&#20020;&#30340;&#37325;&#35201;&#30340;&#21487;&#29992;&#24615;&#25361;&#25112;&#65292;&#25105;&#20204;&#21521;&#22823;&#37327;&#24320;&#21457;&#32773;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#24182;&#20174;410&#21517;&#24320;&#21457;&#32773;&#20013;&#33719;&#24471;&#20102;&#22238;&#22797;&#12290;&#36890;&#36807;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#24320;&#21457;&#32773;&#26368;&#26377;&#21160;&#21147;&#20351;&#29992;AI&#32534;&#31243;&#21161;&#25163;&#30340;&#21407;&#22240;&#26159;&#23427;&#20204;&#21487;&#20197;&#24110;&#21161;&#24320;&#21457;&#32773;&#20943;&#23569;&#25353;&#38190;&#27425;&#25968;&#65292;&#24555;&#36895;&#23436;&#25104;&#32534;&#31243;&#20219;&#21153;&#24182;&#35843;&#29992;&#35821;&#27861;&#65292;&#20294;&#23427;&#23545;&#24110;&#21161;&#24320;&#21457;&#32773;&#24605;&#32771;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#30340;&#25903;&#25345;&#24230;&#36739;&#20302;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#24320;&#21457;&#32773;&#19981;&#20351;&#29992;&#36825;&#20123;&#24037;&#20855;&#30340;&#26368;&#37325;&#35201;&#21407;&#22240;&#26159;&#36825;&#20123;&#24037;&#20855;&#19981;&#33021;&#36755;&#20986;&#36866;&#21512;&#20182;&#20204;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
The software engineering community recently has witnessed widespread deployment of AI programming assistants, such as GitHub Copilot. However, in practice, developers do not accept AI programming assistants' initial suggestions at a high frequency. This leaves a number of open questions related to the usability of these tools. To understand developers' practices while using these tools and the important usability challenges they face, we administered a survey to a large population of developers and received responses from a diverse set of 410 developers. Through a mix of qualitative and quantitative analyses, we found that developers are most motivated to use AI programming assistants because they help developers reduce key-strokes, finish programming tasks quickly, and recall syntax, but resonate less with using them to help brainstorm potential solutions. We also found the most important reasons why developers do not use these tools are because these tools do not output code that add
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#25552;&#39640;&#26080;&#32447;&#32593;&#32476;&#31649;&#29702;&#25928;&#29575;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DGMs&#30340;&#31649;&#29702;&#26694;&#26550;&#65292;&#24182;&#36827;&#34892;&#20102;&#19968;&#39033;DGM&#27169;&#22411;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2303.17114</link><description>&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21450;&#20854;&#22312;&#39640;&#25928;&#26080;&#32447;&#32593;&#32476;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;: &#25945;&#31243;&#21644;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Deep Generative Model and Its Applications in Efficient Wireless Network Management: A Tutorial and Case Study. (arXiv:2303.17114v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17114
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#25552;&#39640;&#26080;&#32447;&#32593;&#32476;&#31649;&#29702;&#25928;&#29575;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DGMs&#30340;&#31649;&#29702;&#26694;&#26550;&#65292;&#24182;&#36827;&#34892;&#20102;&#19968;&#39033;DGM&#27169;&#22411;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25193;&#25955;&#27169;&#22411;&#21644;ChatGPT&#30340;&#24778;&#20154;&#25104;&#21151;&#65292;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;(DGMs)&#27491;&#22312;&#32463;&#21382;2022&#24180;&#30340;&#29190;&#28856;&#24335;&#22686;&#38271;&#12290;&#19981;&#38480;&#20110;&#20869;&#23481;&#29983;&#25104;&#65292;&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#22797;&#26434;&#27169;&#24335;&#34920;&#31034;&#33021;&#21147;&#21644;&#29983;&#25104;&#20986;&#21487;&#20449;&#26679;&#26412;&#30340;&#33021;&#21147;&#65292;DGMs&#20063;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#29289;&#32852;&#32593;&#12289;&#20803;&#23431;&#23449;&#21644;&#25968;&#23383;&#23402;&#29983;&#31561;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;DGMs&#22312;&#37325;&#35201;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#21363;&#25552;&#39640;&#26080;&#32447;&#32593;&#32476;&#31649;&#29702;&#30340;&#25928;&#29575;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#27010;&#36848;&#20102;&#29983;&#25104;AI&#65292;&#20197;&#21450;&#19977;&#31181;&#20195;&#34920;&#24615;&#30340;DGMs&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DGMs&#22686;&#24378;&#30340;&#26080;&#32447;&#32593;&#32476;&#31649;&#29702;&#26694;&#26550;&#65292;&#22312;&#26694;&#26550;&#20013;&#35814;&#32454;&#35828;&#26126;&#20102;&#20256;&#32479;&#32593;&#32476;&#31649;&#29702;&#26041;&#27861;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#20026;&#20160;&#20040;DGMs&#33021;&#22815;&#39640;&#25928;&#22320;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#38416;&#36848;&#20102;&#22312;&#31649;&#29702;&#26080;&#32447;&#32593;&#32476;&#20013;&#24212;&#29992;DGMs&#30340;&#36880;&#27493;&#24037;&#20316;&#27969;&#31243;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;DGM&#27169;&#22411;&#8212;&#8212;&#25193;&#25955;&#27169;&#22411;&#65292;&#22312;&#32593;&#32476;&#32463;&#27982;&#23398;&#19978;&#36827;&#34892;&#20102;&#19968;&#39033;&#26696;&#20363;&#30740;&#31350;&#65292;&#20197;&#29983;&#25104;&#26377;&#25928;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the phenomenal success of diffusion models and ChatGPT, deep generation models (DGMs) have been experiencing explosive growth from 2022. Not limited to content generation, DGMs are also widely adopted in Internet of Things, Metaverse, and digital twin, due to their outstanding ability to represent complex patterns and generate plausible samples. In this article, we explore the applications of DGMs in a crucial task, i.e., improving the efficiency of wireless network management. Specifically, we firstly overview the generative AI, as well as three representative DGMs. Then, a DGM-empowered framework for wireless network management is proposed, in which we elaborate the issues of the conventional network management approaches, why DGMs can address them efficiently, and the step-by-step workflow for applying DGMs in managing wireless networks. Moreover, we conduct a case study on network economics, using the state-of-the-art DGM model, i.e., diffusion model, to generate effective con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#27010;&#29575;&#35302;&#21457;&#33218;&#30340;&#24773;&#22659;&#32452;&#21512;&#36172;&#21338;&#26426;&#65292;&#22312;&#19981;&#21516;&#26465;&#20214;&#19979;&#35774;&#35745;&#20102;C$^2$-UCB-T&#31639;&#27861;&#21644;VAC$^2$-UCB&#31639;&#27861;&#65292;&#24182;&#20998;&#21035;&#23548;&#20986;&#20102;&#23545;&#24212;&#30340;&#36951;&#25022;&#20540;&#19978;&#38480;&#65292;&#20026;&#30456;&#20851;&#24212;&#29992;&#25552;&#20379;&#20102;&#29702;&#35770;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2303.17110</link><description>&lt;p&gt;
&#24102;&#26377;&#27010;&#29575;&#35302;&#21457;&#33218;&#30340;&#24773;&#22659;&#32452;&#21512;&#36172;&#21338;&#26426;
&lt;/p&gt;
&lt;p&gt;
Contextual Combinatorial Bandits with Probabilistically Triggered Arms. (arXiv:2303.17110v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#27010;&#29575;&#35302;&#21457;&#33218;&#30340;&#24773;&#22659;&#32452;&#21512;&#36172;&#21338;&#26426;&#65292;&#22312;&#19981;&#21516;&#26465;&#20214;&#19979;&#35774;&#35745;&#20102;C$^2$-UCB-T&#31639;&#27861;&#21644;VAC$^2$-UCB&#31639;&#27861;&#65292;&#24182;&#20998;&#21035;&#23548;&#20986;&#20102;&#23545;&#24212;&#30340;&#36951;&#25022;&#20540;&#19978;&#38480;&#65292;&#20026;&#30456;&#20851;&#24212;&#29992;&#25552;&#20379;&#20102;&#29702;&#35770;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#25429;&#25417;&#24191;&#27867;&#24212;&#29992;&#33539;&#22260;&#30340;&#19968;&#31995;&#21015;&#24179;&#28369;&#26465;&#20214;&#19979;&#30340;&#24102;&#26377;&#27010;&#29575;&#35302;&#21457;&#33218;&#30340;&#24773;&#22659;&#32452;&#21512;&#36172;&#21338;&#26426;(C$^2$MAB-T)&#65292;&#20363;&#22914;&#24773;&#22659;&#32423;&#32852;&#36172;&#21338;&#26426;&#21644;&#24773;&#22659;&#26368;&#22823;&#21270;&#36172;&#21338;&#26426;&#12290;&#22312;&#27169;&#25311;&#35302;&#21457;&#27010;&#29575;(TPM)&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;C$^2$-UCB-T&#31639;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;$\tilde{O}(d\sqrt{KT})$&#30340;&#36951;&#25022;&#20540;&#19978;&#38480;&#65292;&#28040;&#38500;&#20102;&#19968;&#20010;&#21487;&#33021;&#25351;&#25968;&#32423;&#22686;&#38271;&#30340;&#22240;&#23376;$O(1/p_{\min})$&#65292;&#20854;&#20013;$d$&#26159;&#24773;&#22659;&#30340;&#32500;&#25968;&#65292;$p_{\min}$&#26159;&#33021;&#34987;&#35302;&#21457;&#30340;&#20219;&#20309;&#33218;&#30340;&#26368;&#23567;&#27491;&#27010;&#29575;&#65292;&#25209;&#22823;&#23567;$K$&#26159;&#27599;&#36718;&#33021;&#34987;&#35302;&#21457;&#30340;&#33218;&#30340;&#26368;&#22823;&#25968;&#37327;&#12290;&#22312;&#26041;&#24046;&#35843;&#21046;(VM)&#25110;&#35302;&#21457;&#27010;&#29575;&#21644;&#26041;&#24046;&#35843;&#21046;(TPVM)&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#24046;&#33258;&#36866;&#24212;&#31639;&#27861;VAC$^2$-UCB&#65292;&#24182;&#23548;&#20986;&#20102;&#19968;&#20010;$\tilde{O}(d\sqrt{T})$&#30340;&#36951;&#25022;&#20540;&#19978;&#38480;&#65292;&#35813;&#19978;&#38480;&#19982;&#25209;&#22823;&#23567;$K$&#26080;&#20851;&#12290;&#20316;&#20026;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#21103;&#20135;&#21697;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
We study contextual combinatorial bandits with probabilistically triggered arms (C$^2$MAB-T) under a variety of smoothness conditions that capture a wide range of applications, such as contextual cascading bandits and contextual influence maximization bandits. Under the triggering probability modulated (TPM) condition, we devise the C$^2$-UCB-T algorithm and propose a novel analysis that achieves an $\tilde{O}(d\sqrt{KT})$ regret bound, removing a potentially exponentially large factor $O(1/p_{\min})$, where $d$ is the dimension of contexts, $p_{\min}$ is the minimum positive probability that any arm can be triggered, and batch-size $K$ is the maximum number of arms that can be triggered per round. Under the variance modulated (VM) or triggering probability and variance modulated (TPVM) conditions, we propose a new variance-adaptive algorithm VAC$^2$-UCB and derive a regret bound $\tilde{O}(d\sqrt{T})$, which is independent of the batch-size $K$. As a valuable by-product, we find our a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;OpenMix&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#25298;&#32477;&#24322;&#24120;&#26679;&#26412;&#29983;&#25104;&#30340;&#20266;&#26679;&#26412;&#26469;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#20998;&#31867;&#22120;&#30340;&#21487;&#38752;&#24615;&#65292;&#20174;&#32780;&#26816;&#27979;&#24050;&#30693;&#31867;&#21035;&#30340;&#20998;&#31867;&#38169;&#35823;&#21644;&#26410;&#30693;&#31867;&#21035;&#30340;OOD&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2303.17093</link><description>&lt;p&gt;
OpenMix: &#25506;&#32034;&#24322;&#24120;&#26679;&#26412;&#20197;&#26816;&#27979;&#20998;&#31867;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
OpenMix: Exploring Outlier Samples for Misclassification Detection. (arXiv:2303.17093v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17093
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;OpenMix&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#25298;&#32477;&#24322;&#24120;&#26679;&#26412;&#29983;&#25104;&#30340;&#20266;&#26679;&#26412;&#26469;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#20998;&#31867;&#22120;&#30340;&#21487;&#38752;&#24615;&#65292;&#20174;&#32780;&#26816;&#27979;&#24050;&#30693;&#31867;&#21035;&#30340;&#20998;&#31867;&#38169;&#35823;&#21644;&#26410;&#30693;&#31867;&#21035;&#30340;OOD&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#65292;&#21487;&#38752;&#30340;&#28145;&#24230;&#31070;&#32463;&#20998;&#31867;&#22120;&#32622;&#20449;&#24230;&#20272;&#35745;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#22522;&#26412;&#35201;&#27714;&#12290;&#28982;&#32780;&#65292;&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#23545;&#20854;&#38169;&#35823;&#39044;&#27979;&#36807;&#20110;&#33258;&#20449;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#26131;&#20110;&#33719;&#21462;&#30340;&#24322;&#24120;&#26679;&#26412;&#65292;&#21363;&#26469;&#33258;&#38750;&#30446;&#26631;&#31867;&#30340;&#26410;&#26631;&#35760;&#26679;&#26412;&#65292;&#24110;&#21161;&#26816;&#27979;&#20998;&#31867;&#38169;&#35823;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21457;&#29616;&#20986;&#21517;&#30340;Outlier Exposure&#22312;&#26816;&#27979;&#26410;&#30693;&#31867;&#21035;&#30340;&#26679;&#26412;&#20013;&#38750;&#24120;&#24378;&#22823;&#65292;&#20294;&#22312;&#35782;&#21035;&#20998;&#31867;&#38169;&#35823;&#26041;&#38754;&#24182;&#27809;&#26377;&#25552;&#20379;&#20219;&#20309;&#24110;&#21161;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OpenMix&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#25298;&#32477;&#36890;&#36807;&#24322;&#24120;&#36716;&#25442;&#29983;&#25104;&#30340;&#19981;&#30830;&#23450;&#20266;&#26679;&#26412;&#26469;&#34701;&#21512;&#24320;&#25918;&#19990;&#30028;&#30340;&#30693;&#35782;&#12290;OpenMix&#22312;&#21508;&#31181;&#24773;&#22659;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#21487;&#38752;&#24615;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#24378;&#22823;&#32780;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#24050;&#30693;&#31867;&#21035;&#30340;&#20998;&#31867;&#38169;&#35823;&#21644;&#26410;&#30693;&#31867;&#21035;&#30340;OOD&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliable confidence estimation for deep neural classifiers is a challenging yet fundamental requirement in high-stakes applications. Unfortunately, modern deep neural networks are often overconfident for their erroneous predictions. In this work, we exploit the easily available outlier samples, i.e., unlabeled samples coming from non-target classes, for helping detect misclassification errors. Particularly, we find that the well-known Outlier Exposure, which is powerful in detecting out-of-distribution (OOD) samples from unknown classes, does not provide any gain in identifying misclassification errors. Based on these observations, we propose a novel method called OpenMix, which incorporates open-world knowledge by learning to reject uncertain pseudo-samples generated via outlier transformation. OpenMix significantly improves confidence reliability under various scenarios, establishing a strong and unified framework for detecting both misclassified samples from known classes and OOD sa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#8220;&#26377;&#24847;&#35782;&#30340;&#22270;&#28789;&#26426;&#8221;&#65288;CTM&#65289;&#27169;&#22411;&#65292;&#26088;&#22312;&#25506;&#31350;&#24847;&#35782;&#30340;&#29702;&#35770;&#35745;&#31639;&#26426;&#31185;&#23398;&#26041;&#27861;&#12290;&#35813;&#27169;&#22411;&#20174;&#31070;&#32463;&#31185;&#23398;&#21644;&#24515;&#29702;&#23398;&#20013;&#33719;&#30410;&#33391;&#22810;&#65292;&#24182;&#21487;&#20316;&#20026;&#21019;&#24314;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#25351;&#23548;&#26041;&#38024;&#12290;</title><link>http://arxiv.org/abs/2303.17075</link><description>&lt;p&gt;
&#35266;&#28857;&#65306;&#24847;&#35782;&#21644;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#29702;&#35770;&#35745;&#31639;&#26426;&#31185;&#23398;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Viewpoint: A Theoretical Computer Science Perspective on Consciousness and Artificial General Intelligence. (arXiv:2303.17075v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17075
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#8220;&#26377;&#24847;&#35782;&#30340;&#22270;&#28789;&#26426;&#8221;&#65288;CTM&#65289;&#27169;&#22411;&#65292;&#26088;&#22312;&#25506;&#31350;&#24847;&#35782;&#30340;&#29702;&#35770;&#35745;&#31639;&#26426;&#31185;&#23398;&#26041;&#27861;&#12290;&#35813;&#27169;&#22411;&#20174;&#31070;&#32463;&#31185;&#23398;&#21644;&#24515;&#29702;&#23398;&#20013;&#33719;&#30410;&#33391;&#22810;&#65292;&#24182;&#21487;&#20316;&#20026;&#21019;&#24314;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#25351;&#23548;&#26041;&#38024;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23450;&#20041;&#20102;&#8220;&#26377;&#24847;&#35782;&#30340;&#22270;&#28789;&#26426;&#8221;&#65288;CTM&#65289;&#65292;&#26088;&#22312;&#25506;&#31350;&#24847;&#35782;&#30340;&#29702;&#35770;&#35745;&#31639;&#26426;&#31185;&#23398;&#65288;TCS&#65289;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36981;&#24490;&#20102;TCS&#23545;&#31616;&#21333;&#26131;&#25026;&#30340;&#35201;&#27714;&#12290;CTM&#26159;&#19968;&#20010;&#24847;&#22270;&#31616;&#21333;&#30340;&#26426;&#22120;&#65292;&#32780;&#19981;&#26159;&#22823;&#33041;&#30340;&#27169;&#22411;&#65292;&#20294;&#20854;&#35774;&#35745;&#20174;&#31070;&#32463;&#31185;&#23398;&#21644;&#24515;&#29702;&#23398;&#20013;&#33719;&#30410;&#33391;&#22810;&#12290;&#23613;&#31649;&#20854;&#24320;&#21457;&#26159;&#20026;&#20102;&#29702;&#35299;&#24847;&#35782;&#65292;&#20294;CTM&#20063;&#25552;&#20379;&#20102;&#19968;&#20010;&#29420;&#21040;&#30340;&#25351;&#23548;&#26041;&#38024;&#65292;&#29992;&#20110;&#21019;&#24314;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We have defined the Conscious Turing Machine (CTM) for the purpose of investigating a Theoretical Computer Science (TCS) approach to consciousness. For this, we have hewn to the TCS demand for simplicity and understandability. The CTM is consequently and intentionally a simple machine. It is not a model of the brain, though its design has greatly benefited - and continues to benefit - from neuroscience and psychology. The CTM is a model of and for consciousness.  Although it is developed to understand consciousness, the CTM offers a thoughtful and novel guide to the creation of an Artificial General Intelligence (AGI). For example, the CTM has an enormous number of powerful processors, some with specialized expertise, others unspecialized but poised to develop an expertise. For whatever problem must be dealt with, the CTM has an excellent way to utilize those processors that have the required knowledge, ability, and time to work on the problem, even if it is not aware of which ones the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DERA&#30340;&#23545;&#35805;&#22411;&#35299;&#20915;&#20195;&#29702;&#65292;&#35813;&#20195;&#29702;&#21033;&#29992;LLM&#30340;&#23545;&#35805;&#33021;&#21147;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#34917;&#20840;&#33021;&#21147;&#12290;DERA&#26694;&#26550;&#21270;&#20026;&#20004;&#20010;&#20195;&#29702;&#31867;&#22411;&#20043;&#38388;&#30340;&#35752;&#35770;&#65292;&#21487;&#20197;&#22312;&#21307;&#30103;&#23545;&#35805;&#25688;&#35201;&#21644;&#25252;&#29702;&#35745;&#21010;&#29983;&#25104;&#26041;&#38754;&#23454;&#29616;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2303.17071</link><description>&lt;p&gt;
DERA: &#20351;&#29992;&#23545;&#35805;&#22411;&#35299;&#20915;&#20195;&#29702;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34917;&#20840;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
DERA: Enhancing Large Language Model Completions with Dialog-Enabled Resolving Agents. (arXiv:2303.17071v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DERA&#30340;&#23545;&#35805;&#22411;&#35299;&#20915;&#20195;&#29702;&#65292;&#35813;&#20195;&#29702;&#21033;&#29992;LLM&#30340;&#23545;&#35805;&#33021;&#21147;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#34917;&#20840;&#33021;&#21147;&#12290;DERA&#26694;&#26550;&#21270;&#20026;&#20004;&#20010;&#20195;&#29702;&#31867;&#22411;&#20043;&#38388;&#30340;&#35752;&#35770;&#65292;&#21487;&#20197;&#22312;&#21307;&#30103;&#23545;&#35805;&#25688;&#35201;&#21644;&#25252;&#29702;&#35745;&#21010;&#29983;&#25104;&#26041;&#38754;&#23454;&#29616;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#25104;&#20026;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#12290;&#22312;&#21307;&#30103;&#31561;&#20851;&#20046;&#23433;&#20840;&#24615;&#30340;&#24212;&#29992;&#20013;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#23454;&#29992;&#24615;&#21462;&#20915;&#20110;&#23427;&#20204;&#29983;&#25104;&#30340;&#36755;&#20986;&#26159;&#21542;&#20855;&#22791;&#20107;&#23454;&#20934;&#30830;&#21644;&#23436;&#25972;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;&#35805;&#22411;&#35299;&#20915;&#20195;&#29702;&#65288;DERA&#65289;&#12290;DERA&#26159;&#19968;&#31181;&#30001;LLM&#65288;&#29305;&#21035;&#26159;GPT-4&#65289;&#30340;&#23545;&#35805;&#33021;&#21147;&#25552;&#20379;&#25903;&#25345;&#30340;&#27169;&#24335;&#65292;&#23427;&#20026;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#19988;&#26131;&#20110;&#29702;&#35299;&#30340;&#35770;&#22363;&#65292;&#29992;&#20110;&#27807;&#36890;&#21453;&#39304;&#24182;&#36845;&#20195;&#25913;&#36827;&#36755;&#20986;&#12290;&#25105;&#20204;&#23558;&#23545;&#35805;&#26694;&#26550;&#21270;&#20026;&#20004;&#20010;&#20195;&#29702;&#31867;&#22411;&#20043;&#38388;&#30340;&#35752;&#35770;&#65306;&#19968;&#20010;&#30740;&#31350;&#20154;&#21592;&#65292;&#36127;&#36131;&#22788;&#29702;&#20449;&#24687;&#24182;&#35782;&#21035;&#20851;&#38190;&#38382;&#39064;&#32452;&#20214;&#65307;&#20197;&#21450;&#19968;&#20010;&#20915;&#31574;&#32773;&#65292;&#20855;&#26377;&#23558;&#30740;&#31350;&#20154;&#21592;&#30340;&#20449;&#24687;&#38598;&#25104;&#24182;&#23545;&#26368;&#32456;&#36755;&#20986;&#20570;&#20986;&#21028;&#23450;&#30340;&#33258;&#20027;&#26435;&#12290;&#25105;&#20204;&#23558;DERA&#29992;&#20110;&#19977;&#20010;&#20020;&#24202;&#30456;&#20851;&#20219;&#21153;&#30340;&#27979;&#35797;&#12290;&#22312;&#21307;&#30103;&#23545;&#35805;&#25688;&#35201;&#21644;&#25252;&#29702;&#35745;&#21010;&#29983;&#25104;&#26041;&#38754;&#65292;DERA&#26174;&#31034;&#20986;&#27604;&#22522;&#30784;GPT-4&#24615;&#33021;&#26174;&#33879;&#25552;&#39640;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have emerged as valuable tools for many natural language understanding tasks. In safety-critical applications such as healthcare, the utility of these models is governed by their ability to generate outputs that are factually accurate and complete. In this work, we present dialog-enabled resolving agents (DERA). DERA is a paradigm made possible by the increased conversational abilities of LLMs, namely GPT-4. It provides a simple, interpretable forum for models to communicate feedback and iteratively improve output. We frame our dialog as a discussion between two agent types - a Researcher, who processes information and identifies crucial problem components, and a Decider, who has the autonomy to integrate the Researcher's information and makes judgments on the final output.  We test DERA against three clinically-focused tasks. For medical conversation summarization and care plan generation, DERA shows significant improvement over the base GPT-4 performance 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#21160;&#37197;&#32622;&#36755;&#20986;&#31354;&#38388;&#20197;&#26368;&#23567;&#21270;&#19982;&#20915;&#31574;&#30456;&#20851;&#20449;&#24687;&#30340;&#25439;&#22833;&#26469;&#21046;&#23450;&#31616;&#21270;&#25277;&#35937;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.17062</link><description>&lt;p&gt;
&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#20013;&#30340;&#29702;&#24819;&#25277;&#35937;
&lt;/p&gt;
&lt;p&gt;
Ideal Abstractions for Decision-Focused Learning. (arXiv:2303.17062v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#21160;&#37197;&#32622;&#36755;&#20986;&#31354;&#38388;&#20197;&#26368;&#23567;&#21270;&#19982;&#20915;&#31574;&#30456;&#20851;&#20449;&#24687;&#30340;&#25439;&#22833;&#26469;&#21046;&#23450;&#31616;&#21270;&#25277;&#35937;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35782;&#21035;&#21644;&#21033;&#29992;&#20915;&#31574;&#30340;&#25928;&#29992;&#32467;&#26500;&#26469;&#21046;&#23450;&#31616;&#21270;&#25277;&#35937;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#26041;&#27861;&#12290;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#36890;&#24120;&#28041;&#21450;&#39640;&#32500;&#36755;&#20986;&#31354;&#38388;&#65288;&#20363;&#22914;&#22270;&#20687;&#20013;&#27599;&#20010;&#20687;&#32032;&#25110;&#22270;&#20013;&#33410;&#28857;&#30340;&#39044;&#27979;&#65289;&#65292;&#23613;&#31649;&#23545;&#20110;&#19979;&#28216;&#30340;&#20915;&#31574;&#21046;&#23450;&#26469;&#35828;&#65292;&#31895;&#30053;&#30340;&#36755;&#20986;&#31354;&#38388;&#36890;&#24120;&#24050;&#32463;&#36275;&#22815;&#20102;&#65288;&#20363;&#22914;&#22270;&#20687;&#20013;&#30340;&#21306;&#22495;&#32780;&#19981;&#26159;&#20687;&#32032;&#65289;&#12290;&#24320;&#21457;&#32773;&#24120;&#24120;&#25163;&#24037;&#21046;&#23450;&#36755;&#20986;&#31354;&#38388;&#30340;&#25277;&#35937;&#65292;&#20294;&#23384;&#22312;&#30528;&#20247;&#22810;&#30340;&#25277;&#35937;&#24418;&#24335;&#65292;&#32780;&#19988;&#36873;&#25321;&#27169;&#22411;&#36755;&#20986;&#31354;&#38388;&#30340;&#24433;&#21709;&#23545;&#20854;&#22312;&#19979;&#28216;&#20915;&#31574;&#21046;&#23450;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#23578;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#33258;&#21160;&#37197;&#32622;&#36755;&#20986;&#31354;&#38388;&#20197;&#26368;&#23567;&#21270;&#19982;&#20915;&#31574;&#30456;&#20851;&#20449;&#24687;&#30340;&#25439;&#22833;&#12290;&#37319;&#29992;&#20960;&#20309;&#35282;&#24230;&#65292;&#25105;&#20204;&#23558;&#31639;&#27861;&#30340;&#19968;&#27493;&#20316;&#20026;&#27010;&#29575;&#21333;&#32431;&#24418;&#30340;&#25237;&#24433;&#65292;&#31216;&#20043;&#20026;fold&#65292;&#20197;&#26368;&#23567;&#21270;&#20915;&#31574;&#30456;&#20851;&#20449;&#24687;&#22312;H-&#29109;&#24847;&#20041;&#19979;&#30340;&#24635;&#25439;&#22833;&#12290;&#20851;&#38190;&#26159;&#65292;L&#8230;
&lt;/p&gt;
&lt;p&gt;
We present a methodology for formulating simplifying abstractions in machine learning systems by identifying and harnessing the utility structure of decisions. Machine learning tasks commonly involve high-dimensional output spaces (e.g., predictions for every pixel in an image or node in a graph), even though a coarser output would often suffice for downstream decision-making (e.g., regions of an image instead of pixels). Developers often hand-engineer abstractions of the output space, but numerous abstractions are possible and it is unclear how the choice of output space for a model impacts its usefulness in downstream decision-making. We propose a method that configures the output space automatically in order to minimize the loss of decision-relevant information. Taking a geometric perspective, we formulate a step of the algorithm as a projection of the probability simplex, termed fold, that minimizes the total loss of decision-related information in the H-entropy sense. Crucially, l
&lt;/p&gt;</description></item><item><title>&#31995;&#32479;&#39044;&#27979;&#22120;&#21487;&#20197;&#36890;&#36807;&#20272;&#35745;&#31243;&#24207;&#23454;&#20363;&#21270;&#22823;&#23567;&#26469;&#24433;&#21709;&#31995;&#32479;&#24615;&#33021;&#65292;&#20026;&#31572;&#26696;&#38598;&#32534;&#31243;&#25552;&#20379;&#28508;&#22312;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2303.17018</link><description>&lt;p&gt;
&#31995;&#32479;&#39044;&#27979;&#22120;&#65306;&#38754;&#21521;&#31572;&#26696;&#38598;&#35821;&#20041;&#19979;&#36923;&#36753;&#31243;&#24207;&#30340;&#35268;&#27169;&#20272;&#35745;&#22120;
&lt;/p&gt;
&lt;p&gt;
System Predictor: Grounding Size Estimator for Logic Programs under Answer Set Semantics. (arXiv:2303.17018v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17018
&lt;/p&gt;
&lt;p&gt;
&#31995;&#32479;&#39044;&#27979;&#22120;&#21487;&#20197;&#36890;&#36807;&#20272;&#35745;&#31243;&#24207;&#23454;&#20363;&#21270;&#22823;&#23567;&#26469;&#24433;&#21709;&#31995;&#32479;&#24615;&#33021;&#65292;&#20026;&#31572;&#26696;&#38598;&#32534;&#31243;&#25552;&#20379;&#28508;&#22312;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31572;&#26696;&#38598;&#32534;&#31243;&#26159;&#19968;&#31181;&#38754;&#21521;&#35299;&#20915;&#22256;&#38590;&#32452;&#21512;&#25628;&#32034;&#38382;&#39064;&#30340;&#22768;&#26126;&#24335;&#36923;&#36753;&#32534;&#31243;&#33539;&#24335;&#12290;&#34429;&#28982;&#19981;&#21516;&#30340;&#36923;&#36753;&#31243;&#24207;&#21487;&#20197;&#32534;&#30721;&#21516;&#19968;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#26377;&#24456;&#22823;&#24046;&#24322;&#12290;&#24456;&#38590;&#30830;&#23450;&#21738;&#20010;&#29256;&#26412;&#30340;&#31243;&#24207;&#34920;&#29616;&#26368;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31995;&#32479;Predictor&#21450;&#20854;&#31639;&#27861;&#21518;&#31471;&#65292;&#29992;&#20110;&#20272;&#35745;&#31243;&#24207;&#30340;&#23454;&#20363;&#21270;&#22823;&#23567;&#65292;&#36825;&#26159;&#19968;&#20010;&#21487;&#20197;&#24433;&#21709;&#22788;&#29702;&#31243;&#24207;&#30340;&#31995;&#32479;&#24615;&#33021;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#24403;Predictor&#29992;&#20316;&#31572;&#26696;&#38598;&#32534;&#31243;&#37325;&#20889;&#24037;&#20855;Projector&#21644;Lpopt&#29983;&#25104;&#30340;&#37325;&#20889;&#30340;&#25351;&#21335;&#26102;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Answer set programming is a declarative logic programming paradigm geared towards solving difficult combinatorial search problems. While different logic programs can encode the same problem, their performance may vary significantly. It is not always easy to identify which version of the program performs the best. We present the system Predictor (and its algorithmic backend) for estimating the grounding size of programs, a metric that can influence a performance of a system processing a program. We evaluate the impact of Predictor when used as a guide for rewritings produced by the answer set programming rewriting tools Projector and Lpopt. The results demonstrate potential to this approach.
&lt;/p&gt;</description></item><item><title>ChatGPT-4&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#23545;&#35805;&#26426;&#22120;&#20154;&#65292;&#33021;&#36798;&#21040;&#25509;&#36817;&#20110;&#29289;&#29702;&#19987;&#23478;&#27700;&#24179;&#30340;&#21147;&#27010;&#24565;&#27979;&#35797;&#25104;&#32489;&#65292;&#23545;&#26410;&#26469;&#30340;&#29289;&#29702;&#25945;&#32946;&#21644;&#25945;&#23398;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2303.17012</link><description>&lt;p&gt;
ChatGPT-4&#20013;&#26174;&#33879;&#27010;&#24565;&#29289;&#29702;&#25512;&#29702;&#30340;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Advances in apparent conceptual physics reasoning in ChatGPT-4. (arXiv:2303.17012v1 [physics.ed-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17012
&lt;/p&gt;
&lt;p&gt;
ChatGPT-4&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#23545;&#35805;&#26426;&#22120;&#20154;&#65292;&#33021;&#36798;&#21040;&#25509;&#36817;&#20110;&#29289;&#29702;&#19987;&#23478;&#27700;&#24179;&#30340;&#21147;&#27010;&#24565;&#27979;&#35797;&#25104;&#32489;&#65292;&#23545;&#26410;&#26469;&#30340;&#29289;&#29702;&#25945;&#32946;&#21644;&#25945;&#23398;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#24314;&#31435;&#22312;&#19968;&#20010;&#24040;&#22823;&#30340;&#20154;&#31867;&#25991;&#26412;&#20449;&#24687;&#24211;&#19978;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#27169;&#25311;&#20154;&#31867;&#23545;&#35805;&#12290;&#26368;&#36817;Kortemeyer&#65288;2023&#65289;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23613;&#31649;&#27809;&#26377;&#20219;&#20309;&#20851;&#20110;&#29289;&#29702;&#23450;&#24459;&#30340;&#26126;&#30830;&#32534;&#31243;&#25351;&#23548;&#65292;ChatGPT-3.5&#21487;&#20197;&#36890;&#36807;&#19968;&#20123;&#21517;&#20041;&#27700;&#24179;&#30340;&#20837;&#38376;&#29289;&#29702;&#35838;&#31243;&#65292;&#24182;&#22312;&#21147;&#23398;&#30340;&#21147;&#27010;&#24565;&#27979;&#35797;&#20013;&#24471;&#21040;&#25509;&#36817;&#26368;&#23567;&#29702;&#35299;&#30340;&#25104;&#32489;&#12290;&#26412;&#30740;&#31350;&#22797;&#21046;&#20102;&#36825;&#20123;&#32467;&#26524;&#65292;&#24182;&#35777;&#26126;&#20102;&#26368;&#26032;&#29256;&#26412;ChatGPT-4&#22312;&#35813;&#29615;&#22659;&#19979;&#30340;&#25104;&#32489;&#36828;&#39640;&#20110;&#21069;&#29256;&#26412;&#65292;&#22312;&#19968;&#20123;&#38750;&#24120;&#20540;&#24471;&#27880;&#24847;&#30340;&#20363;&#22806;&#21644;&#38480;&#21046;&#26465;&#20214;&#19979;&#65292;&#20854;&#22238;&#31572;&#38750;&#24120;&#25509;&#36817;&#20110;&#23436;&#32654;&#22320;&#23637;&#31034;&#19987;&#23478;&#27700;&#24179;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#31616;&#35201;&#35780;&#36848;&#20102;&#36825;&#23545;&#20110;&#26410;&#26469;&#29289;&#29702;&#25945;&#32946;&#21644;&#25945;&#23398;&#30340;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is built on a large language model trained on an enormous corpus of human text to emulate human conversation. Despite lacking any explicit programming regarding the laws of physics, recent work by Kortemeyer (2023) has demonstrated that ChatGPT-3.5 could pass an introductory physics course at some nominal level and register something close to a minimal understanding of Newtonian Mechanics on the Force Concept Inventory. This work replicates those results and also demonstrates that the latest version, ChatGPT-4, has reached a much higher mark in the latter context. Indeed, its responses come quite close to perfectly demonstrating expert-level competence, with a few very notable exceptions and limitations. We briefly comment on the implications of this for the future of physics education and pedagogy.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#24052;&#35199;&#22823;&#23398;&#20837;&#23398;&#32771;&#35797;&#20013;&#35780;&#20272;&#20102;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#65292;&#20998;&#26512;&#20102;&#19981;&#21516;&#25552;&#31034;&#31574;&#30053;&#65292;&#26368;&#32456;&#21457;&#29616;GPT-4&#19982;Chain-of-Thought&#25552;&#31034;&#32467;&#21512;&#34920;&#29616;&#26368;&#22909;&#65292;&#22312;2022&#24180;&#32771;&#35797;&#20013;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;87&#65285;&#12290;</title><link>http://arxiv.org/abs/2303.17003</link><description>&lt;p&gt;
&#22312;&#24052;&#35199;&#22823;&#23398;&#20837;&#23398;&#32771;&#35797;&#20013;&#35780;&#20272;GPT-3.5&#21644;GPT-4&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Evaluating GPT-3.5 and GPT-4 Models on Brazilian University Admission Exams. (arXiv:2303.17003v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#24052;&#35199;&#22823;&#23398;&#20837;&#23398;&#32771;&#35797;&#20013;&#35780;&#20272;&#20102;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#65292;&#20998;&#26512;&#20102;&#19981;&#21516;&#25552;&#31034;&#31574;&#30053;&#65292;&#26368;&#32456;&#21457;&#29616;GPT-4&#19982;Chain-of-Thought&#25552;&#31034;&#32467;&#21512;&#34920;&#29616;&#26368;&#22909;&#65292;&#22312;2022&#24180;&#32771;&#35797;&#20013;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;87&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#24212;&#23545;&#39640;&#39118;&#38505;&#30340;&#22810;&#39033;&#36873;&#25321;&#27979;&#35797;&#20013;&#30340;&#33021;&#21147;&#65292;&#36825;&#37324;&#20197;&#24052;&#35199;&#22823;&#23398;&#24191;&#27867;&#37319;&#29992;&#30340;&#22810;&#23398;&#31185;&#20837;&#23398;&#32771;&#35797;Exame Nacional do Ensino M&#233;dio&#65288;ENEM&#65289;&#20026;&#20363;&#12290;&#35813;&#32771;&#35797;&#23545;LMs&#25552;&#20986;&#20102;&#25361;&#25112;&#65292;&#22240;&#20026;&#20854;&#38382;&#39064;&#21487;&#33021;&#28041;&#21450;&#22810;&#20010;&#30693;&#35782;&#39046;&#22495;&#65292;&#38656;&#35201;&#29702;&#35299;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#20449;&#24687;&#12290;&#20363;&#22914;&#65292;&#19968;&#20010;&#38382;&#39064;&#21487;&#33021;&#38656;&#35201;&#29702;&#35299;&#32479;&#35745;&#23398;&#21644;&#29983;&#29289;&#23398;&#25165;&#33021;&#35299;&#20915;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#23545;2009&#24180;&#33267;2017&#24180;&#32771;&#35797;&#20197;&#21450;2022&#24180;&#20844;&#24320;&#30340;&#32771;&#35797;&#38382;&#39064;&#30340;&#21709;&#24212;&#12290;&#27492;&#22806;&#65292;&#36824;&#27979;&#35797;&#20102;&#19981;&#21516;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#21253;&#25324;&#20351;&#29992;Chain-of-Thought&#65288;CoT&#65289;&#25552;&#31034;&#29983;&#25104;&#31572;&#26696;&#30340;&#35299;&#37322;&#12290;&#22312;2022&#24180;&#30340;&#32771;&#35797;&#20013;&#65292;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#26159;GPT-4&#24182;&#20351;&#29992;&#20102;CoT&#65292;&#22312;&#20934;&#30830;&#29575;&#26041;&#38754;&#36798;&#21040;&#20102;87&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
The present study aims to explore the capabilities of Language Models (LMs) in tackling high-stakes multiple-choice tests, represented here by the Exame Nacional do Ensino M\'edio (ENEM), a multidisciplinary entrance examination widely adopted by Brazilian universities. This exam poses challenging tasks for LMs, since its questions may span into multiple fields of knowledge, requiring understanding of information from diverse domains. For instance, a question may require comprehension of both statistics and biology to be solved. This work analyzed responses generated by GPT-3.5 and GPT-4 models for questions presented in the 2009-2017 exams, as well as for questions of the 2022 exam, which were made public after the training of the models was completed. Furthermore, different prompt strategies were tested, including the use of Chain-of-Thought (CoT) prompts to generate explanations for answers. On the 2022 edition, the best-performing model, GPT-4 with CoT, achieved an accuracy of 87%,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31232;&#30095;&#24615;&#22312;&#35299;&#20915;&#19981;&#27491;&#30830;&#32447;&#24615;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#30340;&#20316;&#29992;&#65292;&#35777;&#26126;&#20102;&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#26597;&#35810;$ O(\varepsilon^{-s}d^s) $&#20010;&#25805;&#20316;&#26469;&#33719;&#24471;$O(\varepsilon)$-&#26368;&#20248;&#34892;&#21160;&#65292;&#20854;&#20013;$s$&#26159;&#31232;&#30095;&#24615;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.16998</link><description>&lt;p&gt;
&#31232;&#30095;&#24615;&#26159;&#21542;&#26377;&#21161;&#20110;&#23398;&#20064;&#19981;&#27491;&#30830;&#30340;&#32447;&#24615;&#36172;&#21338;&#26426;&#65311;
&lt;/p&gt;
&lt;p&gt;
Does Sparsity Help in Learning Misspecified Linear Bandits?. (arXiv:2303.16998v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16998
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31232;&#30095;&#24615;&#22312;&#35299;&#20915;&#19981;&#27491;&#30830;&#32447;&#24615;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#30340;&#20316;&#29992;&#65292;&#35777;&#26126;&#20102;&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#26597;&#35810;$ O(\varepsilon^{-s}d^s) $&#20010;&#25805;&#20316;&#26469;&#33719;&#24471;$O(\varepsilon)$-&#26368;&#20248;&#34892;&#21160;&#65292;&#20854;&#20013;$s$&#26159;&#31232;&#30095;&#24615;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23398;&#20064;&#32447;&#24615;&#19981;&#27491;&#30830;&#36172;&#21338;&#26426;&#24050;&#32463;&#20135;&#29983;&#20102;&#23545;&#23398;&#20064;&#36172;&#21338;&#26426;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#38590;&#24230;&#30340;&#26377;&#36259;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;Du&#31561;&#20154;&#65288;2020&#65289;&#34920;&#26126;&#65292;&#21363;&#20351;&#23398;&#20064;&#32773;&#34987;&#36171;&#20104;&#22312;$ \mathbb{R}^d$ &#20013;&#36817;&#20284;&#36172;&#21338;&#26426;&#25110;RL&#22870;&#21169;&#30340;&#32447;&#24615;&#29305;&#24449;&#65292;&#19988;&#35823;&#24046;&#22312;$\varepsilon$&#30340;&#33539;&#22260;&#20869;&#65292;&#23547;&#25214;&#19968;&#20010;$ O&#65288;\varepsilon&#65289;$ -&#26368;&#20248;&#34892;&#21160;&#38656;&#35201;&#33267;&#23569;&#25289;&#20986;$ \Omega(\exp(d)) $&#30340;&#26597;&#35810;&#12290;&#27492;&#22806;&#65292;Lattimore&#31561;&#20154;&#65288;2020&#65289;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;$\operatorname{poly}(d/\varepsilon)$&#30340;&#26597;&#35810;&#20013;&#23398;&#20064;&#24471;&#21040;&#36864;&#21270;&#30340;$O(\varepsilon\sqrt{d})$ -&#26368;&#20248;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#23454;&#38469;&#21442;&#25968;&#30340;&#32467;&#26500;&#20551;&#35774;&#65292;&#22914;&#31232;&#30095;&#24615;&#65292;&#26159;&#21542;&#33021;&#25171;&#30772;$\varepsilon\sqrt{d}$&#30340;&#38556;&#30861;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#36890;&#36807;&#23637;&#31034;&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#26597;&#35810;$ O(\varepsilon^{-s}d^s) $&#20010;&#25805;&#20316;&#26469;&#33719;&#24471;$O(\varepsilon)$-&#26368;&#20248;&#34892;&#21160;&#65292;&#20854;&#20013;$s$&#26159;&#31232;&#30095;&#24615;&#21442;&#25968;&#65292;&#20197;&#28040;&#38500;$ \exp(d)$-&#20381;&#36182;&#24615;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the study of linear misspecified bandits has generated intriguing implications of the hardness of learning in bandits and reinforcement learning (RL). In particular, Du et al. (2020) show that even if a learner is given linear features in $\mathbb{R}^d$ that approximate the rewards in a bandit or RL with a uniform error of $\varepsilon$, searching for an $O(\varepsilon)$-optimal action requires pulling at least $\Omega(\exp(d))$ queries. Furthermore, Lattimore et al. (2020) show that a degraded $O(\varepsilon\sqrt{d})$-optimal solution can be learned within $\operatorname{poly}(d/\varepsilon)$ queries. Yet it is unknown whether a structural assumption on the ground-truth parameter, such as sparsity, could break the $\varepsilon\sqrt{d}$ barrier. In this paper, we address this question by showing that algorithms can obtain $O(\varepsilon)$-optimal actions by querying $O(\varepsilon^{-s}d^s)$ actions, where $s$ is the sparsity parameter, removing the $\exp(d)$-dependence. We th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#20302;&#36164;&#28304;&#26465;&#20214;&#19979;&#65292;&#37319;&#29992;&#35821;&#35328;&#36866;&#37197;&#22120;&#31561;&#20302;&#35745;&#31639;&#26041;&#27861;&#22312;&#38750;&#27954;&#35821;&#35328;&#19978;&#36827;&#34892;NLP&#30740;&#31350;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#23454;&#39564;&#24471;&#20986;&#20102;&#23454;&#29616;&#21487;&#27604;&#24615;&#33021;&#30340;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2303.16985</link><description>&lt;p&gt;
&#36866;&#24212;&#20302;&#36164;&#28304;&#21452;&#36830;&#36890;&#24615;: &#25506;&#31350;&#23545;&#38750;&#27954;&#20302;&#36164;&#28304;&#35821;&#35328;&#37319;&#29992;&#20302;&#35745;&#31639;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Adapting to the Low-Resource Double-Bind: Investigating Low-Compute Methods on Low-Resource African Languages. (arXiv:2303.16985v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16985
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#20302;&#36164;&#28304;&#26465;&#20214;&#19979;&#65292;&#37319;&#29992;&#35821;&#35328;&#36866;&#37197;&#22120;&#31561;&#20302;&#35745;&#31639;&#26041;&#27861;&#22312;&#38750;&#27954;&#35821;&#35328;&#19978;&#36827;&#34892;NLP&#30740;&#31350;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#23454;&#39564;&#24471;&#20986;&#20102;&#23454;&#29616;&#21487;&#27604;&#24615;&#33021;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#30340;&#35768;&#22810;&#20219;&#21153;&#37117;&#20351;&#29992;&#22823;&#35268;&#27169;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#12290;&#28982;&#32780;&#65292;&#38750;&#27954;&#35821;&#35328;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#21644;&#39640;&#35745;&#31639;&#36164;&#28304;&#30340;&#33719;&#21462;&#38480;&#21046;&#20102;&#23545;&#36825;&#20123;&#35821;&#35328;&#36827;&#34892;&#30740;&#31350;&#30340;&#21487;&#33021;&#24615;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#36825;&#31181;&#20302;&#36164;&#28304;&#21452;&#36830;&#36890;&#24615;&#32972;&#26223;&#19979;&#65292;&#35821;&#35328;&#36866;&#37197;&#22120;&#31561;&#20302;&#35745;&#31639;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#35797;&#22270;&#22238;&#31572;&#20197;&#19979;&#38382;&#39064;&#65306;&#35821;&#35328;&#36866;&#37197;&#22120;&#26159;&#21542;&#20801;&#35768;&#37027;&#20123;&#22312;&#25968;&#25454;&#21644;&#35745;&#31639;&#26041;&#38754;&#21452;&#37325;&#21463;&#38480;&#30340;&#20154;&#23454;&#38469;&#19978;&#26500;&#24314;&#26377;&#29992;&#30340;&#27169;&#22411;&#65311;&#36890;&#36807;&#22312;&#38750;&#27954;&#35821;&#35328;&#19978;&#36827;&#34892;&#24494;&#35843;&#23454;&#39564;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#23427;&#20204;&#20316;&#20026;&#20302;&#36164;&#28304;&#38750;&#27954;NLP&#30340;&#25104;&#26412;&#25928;&#30410;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#20351;&#29992;&#20840;&#37096;&#20813;&#36153;&#35745;&#31639;&#36164;&#28304;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#35745;&#31639;&#36164;&#28304;&#25439;&#32791;&#22823;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30456;&#27604;&#65292;&#35821;&#35328;&#36866;&#37197;&#22120;&#23454;&#29616;&#20102;&#21487;&#27604;&#30340;&#24615;&#33021;&#12290;&#36825;&#20026;&#36827;&#19968;&#27493;&#30340;&#23454;&#39564;&#21644;&#25506;&#32034;&#25171;&#24320;&#20102;&#22823;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many natural language processing (NLP) tasks make use of massively pre-trained language models, which are computationally expensive. However, access to high computational resources added to the issue of data scarcity of African languages constitutes a real barrier to research experiments on these languages. In this work, we explore the applicability of low-compute approaches such as language adapters in the context of this low-resource double-bind. We intend to answer the following question: do language adapters allow those who are doubly bound by data and compute to practically build useful models? Through fine-tuning experiments on African languages, we evaluate their effectiveness as cost-effective approaches to low-resource African NLP. Using solely free compute resources, our results show that language adapters achieve comparable performances to massive pre-trained language models which are heavy on computational resources. This opens the door to further experimentation and explor
&lt;/p&gt;</description></item><item><title>Queer in AI&#26159;&#19968;&#20010;&#22522;&#20110;&#31038;&#21306;&#21442;&#19982;&#30340;AI&#35774;&#35745;&#26696;&#20363;&#30740;&#31350;&#65292;&#36890;&#36807;&#25298;&#32477;&#31561;&#32423;&#21046;&#24230;&#32780;&#36873;&#25321;&#21435;&#20013;&#24515;&#21270;&#65292;&#22312;&#37239;&#20799;&#31038;&#32676;&#20869;&#37096;&#24314;&#31435;&#20102;&#25588;&#21161;&#21644;&#39033;&#30446;&#65292;&#21516;&#26102;&#21162;&#21147;&#25913;&#21464;&#37239;&#20799;&#31038;&#32676;&#22806;&#30340;&#21442;&#19982;&#32773;&#21644;&#26426;&#26500;&#12290;&#36890;&#36807;&#22521;&#32946;AI&#21442;&#19982;&#25991;&#21270;&#65292;&#27426;&#36814;&#21644;&#36171;&#26435;&#36793;&#32536;&#21270;&#21442;&#19982;&#32773;&#65292;&#20026;AI&#30340;&#21442;&#19982;&#24335;&#35774;&#35745;&#20570;&#20986;&#20102;&#26356;&#24191;&#27867;&#30340;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2303.16972</link><description>&lt;p&gt;
Queer In AI:&#22522;&#20110;&#31038;&#21306;&#21442;&#19982;&#30340;AI&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Queer In AI: A Case Study in Community-Led Participatory AI. (arXiv:2303.16972v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16972
&lt;/p&gt;
&lt;p&gt;
Queer in AI&#26159;&#19968;&#20010;&#22522;&#20110;&#31038;&#21306;&#21442;&#19982;&#30340;AI&#35774;&#35745;&#26696;&#20363;&#30740;&#31350;&#65292;&#36890;&#36807;&#25298;&#32477;&#31561;&#32423;&#21046;&#24230;&#32780;&#36873;&#25321;&#21435;&#20013;&#24515;&#21270;&#65292;&#22312;&#37239;&#20799;&#31038;&#32676;&#20869;&#37096;&#24314;&#31435;&#20102;&#25588;&#21161;&#21644;&#39033;&#30446;&#65292;&#21516;&#26102;&#21162;&#21147;&#25913;&#21464;&#37239;&#20799;&#31038;&#32676;&#22806;&#30340;&#21442;&#19982;&#32773;&#21644;&#26426;&#26500;&#12290;&#36890;&#36807;&#22521;&#32946;AI&#21442;&#19982;&#25991;&#21270;&#65292;&#27426;&#36814;&#21644;&#36171;&#26435;&#36793;&#32536;&#21270;&#21442;&#19982;&#32773;&#65292;&#20026;AI&#30340;&#21442;&#19982;&#24335;&#35774;&#35745;&#20570;&#20986;&#20102;&#26356;&#24191;&#27867;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;Queer in AI&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#25506;&#35752;&#31038;&#21306;&#21442;&#19982;&#24335;AI&#35774;&#35745;&#30340;&#23454;&#36341;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#31038;&#21306;&#21442;&#19982;&#35774;&#35745;&#21644;&#20132;&#21449;&#24615;&#21407;&#21017;&#22914;&#20309;&#22312;&#22810;&#24180;&#37324;&#22312;&#36825;&#20010;&#31038;&#32676;&#20013;&#33804;&#33469;&#21644;&#22609;&#36896;&#20102;&#20854;&#39033;&#30446;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#35813;&#32452;&#32455;&#22312;&#27492;&#36807;&#31243;&#20013;&#38754;&#20020;&#30340;&#19981;&#21516;&#25361;&#25112;&#65292;&#23457;&#35270;&#20102;&#35813;&#32452;&#32455;&#22312;&#23454;&#29616;&#21442;&#19982;&#24615;&#19982;&#20132;&#21449;&#24615;&#21407;&#21017;&#26041;&#38754;&#30340;&#19981;&#36275;&#65292;&#24182;&#35780;&#20272;&#20102;&#35813;&#32452;&#32455;&#30340;&#24433;&#21709;&#12290;Queer in AI&#36890;&#36807;&#25298;&#32477;&#31561;&#32423;&#21046;&#24230;&#32780;&#36873;&#25321;&#21435;&#20013;&#24515;&#21270;&#65292;&#36890;&#36807;&#23558;&#25588;&#21161;&#21644;&#39033;&#30446;&#24314;&#35774;&#24314;&#31435;&#22312;&#37239;&#20799;&#31038;&#32676;&#20869;&#37096;&#12289;&#30001;&#37239;&#20799;&#31038;&#32676;&#20869;&#25104;&#21592;&#26469;&#36127;&#36131;&#30340;&#26041;&#24335;&#65292;&#20197;&#21450;&#21162;&#21147;&#25913;&#21464;&#37239;&#20799;&#31038;&#32676;&#22806;&#30340;&#21442;&#19982;&#32773;&#21644;&#26426;&#26500;&#65292;&#20026;&#21442;&#19982;&#24335;&#26041;&#27861;&#30340;&#20174;&#19994;&#32773;&#21644;&#29702;&#35770;&#23478;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#32463;&#39564;&#21644;&#35265;&#35299;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25512;&#27979;&#20687;Queer in AI&#36825;&#26679;&#30340;&#31038;&#21306;&#22914;&#20309;&#36890;&#36807;&#22521;&#32946;AI&#30340;&#21442;&#19982;&#25991;&#21270;&#65292;&#27426;&#36814;&#21644;&#36171;&#26435;&#36793;&#32536;&#21270;&#30340;&#21442;&#19982;&#32773;&#65292;&#25209;&#35780;&#36139;&#30240;&#21644;&#21093;&#21066;&#24615;&#34920;&#36848;&#31561;&#26041;&#38754;&#65292;&#20026;AI&#30340;&#21442;&#19982;&#24335;&#35774;&#35745;&#20570;&#20986;&#20102;&#26356;&#24191;&#27867;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Queer in AI as a case study for community-led participatory design in AI. We examine how participatory design and intersectional tenets started and shaped this community's programs over the years. We discuss different challenges that emerged in the process, look at ways this organization has fallen short of operationalizing participatory and intersectional principles, and then assess the organization's impact. Queer in AI provides important lessons and insights for practitioners and theorists of participatory methods broadly through its rejection of hierarchy in favor of decentralization, success at building aid and programs by and for the queer community, and effort to change actors and institutions outside of the queer community. Finally, we theorize how communities like Queer in AI contribute to the participatory design in AI more broadly by fostering cultures of participation in AI, welcoming and empowering marginalized participants, critiquing poor or exploitative parti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;PDDL+&#24314;&#27169;&#24868;&#24594;&#30340;&#23567;&#40479;&#28216;&#25103;&#24182;&#36816;&#29992;&#21551;&#21457;&#24335;&#21644;&#31867;&#20284;&#20110;&#39318;&#36873;&#36816;&#31639;&#31526;&#30340;&#25628;&#32034;&#25216;&#26415;&#26469;&#32531;&#35299;&#32452;&#21512;&#25628;&#32034;&#22797;&#26434;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#21462;&#24471;&#20102;&#21644;&#19987;&#38376;&#39046;&#22495;&#30340;&#26041;&#27861;&#30456;&#24403;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.16967</link><description>&lt;p&gt;
&#29289;&#29702;&#38382;&#39064;&#30340;&#21551;&#21457;&#24335;&#25628;&#32034;&#65306;&#29992;PDDL+&#29609;&#24868;&#24594;&#30340;&#23567;&#40479;
&lt;/p&gt;
&lt;p&gt;
Heuristic Search For Physics-Based Problems: Angry Birds in PDDL+. (arXiv:2303.16967v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;PDDL+&#24314;&#27169;&#24868;&#24594;&#30340;&#23567;&#40479;&#28216;&#25103;&#24182;&#36816;&#29992;&#21551;&#21457;&#24335;&#21644;&#31867;&#20284;&#20110;&#39318;&#36873;&#36816;&#31639;&#31526;&#30340;&#25628;&#32034;&#25216;&#26415;&#26469;&#32531;&#35299;&#32452;&#21512;&#25628;&#32034;&#22797;&#26434;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#21462;&#24471;&#20102;&#21644;&#19987;&#38376;&#39046;&#22495;&#30340;&#26041;&#27861;&#30456;&#24403;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#20351;&#29992;&#36890;&#29992;&#22411;&#35268;&#21010;&#22120;&#19982;&#32452;&#21512;&#25628;&#32034;&#26469;&#29609;&#24868;&#24594;&#30340;&#23567;&#40479;&#8212;&#8212;&#19968;&#20010;&#24050;&#32463;&#25104;&#20026;AI&#25361;&#25112;&#38382;&#39064;&#30340;&#28216;&#25103;&#12290;&#25105;&#20204;&#20351;&#29992;PDDL+&#26469;&#24314;&#27169;&#28216;&#25103;&#65292;&#36825;&#26159;&#19968;&#31181;&#25903;&#25345;&#25345;&#32493;&#36807;&#31243;&#21644;&#22806;&#29983;&#20107;&#20214;&#30340;&#28151;&#21512;&#31163;&#25955;/&#36830;&#32493;&#22495;&#35745;&#21010;&#35821;&#35328;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#27169;&#22411;&#65292;&#24182;&#30830;&#23450;&#20102;&#38477;&#20302;&#38382;&#39064;&#22797;&#26434;&#24615;&#30340;&#20851;&#38190;&#35774;&#35745;&#20915;&#31574;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#29305;&#23450;&#20110;&#22495;&#30340;&#22686;&#24378;&#25514;&#26045;&#65292;&#21253;&#25324;&#21551;&#21457;&#24335;&#21644;&#31867;&#20284;&#20110;&#39318;&#36873;&#36816;&#31639;&#31526;&#30340;&#25628;&#32034;&#25216;&#26415;&#12290;&#23427;&#20204;&#20849;&#21516;&#32531;&#35299;&#20102;&#32452;&#21512;&#25628;&#32034;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20854;&#19982;&#19987;&#38376;&#30340;&#29305;&#23450;&#39046;&#22495;&#35299;&#31639;&#22120;&#22312;&#19968;&#31995;&#21015;&#24868;&#24594;&#30340;&#23567;&#40479;&#27700;&#24179;&#19978;&#30340;&#34920;&#29616;&#36827;&#34892;&#27604;&#36739;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#27700;&#24179;&#19978;&#65292;&#25105;&#20204;&#30340;&#34920;&#29616;&#19982;&#36825;&#20123;&#29305;&#23450;&#39046;&#22495;&#30340;&#26041;&#27861;&#30456;&#24403;&#65292;&#21363;&#20351;&#19981;&#20351;&#29992;&#25105;&#20204;&#30340;&#29305;&#23450;&#39046;&#22495;&#25628;&#32034;&#22686;&#24378;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies how a domain-independent planner and combinatorial search can be employed to play Angry Birds, a well established AI challenge problem. To model the game, we use PDDL+, a planning language for mixed discrete/continuous domains that supports durative processes and exogenous events. The paper describes the model and identifies key design decisions that reduce the problem complexity. In addition, we propose several domain-specific enhancements including heuristics and a search technique similar to preferred operators. Together, they alleviate the complexity of combinatorial search. We evaluate our approach by comparing its performance with dedicated domain-specific solvers on a range of Angry Birds levels. The results show that our performance is on par with these domain-specific approaches in most levels, even without using our domain-specific search enhancements.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#21487;&#24494;&#20984;&#20248;&#21270;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#29616;&#26377;&#30340;&#19968;&#38454;&#26356;&#26032;&#35268;&#21017;&#25512;&#24191;&#21040;&#26356;&#24191;&#30340;&#23478;&#26063;&#65292;&#35777;&#26126;&#22312;&#20803;&#23398;&#20064;&#32773;&#26377;&#36275;&#22815;&#31867;&#20284;&#20219;&#21153;&#30340;&#32463;&#39564;&#19979;&#65292;&#21487;&#20197;&#19968;&#27493;&#20248;&#21270;&#19968;&#31995;&#21015;&#32447;&#24615;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.16952</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#24494;&#20984;&#20248;&#21270;&#20803;&#23398;&#20064;&#21442;&#25968;&#21270;&#30340;&#19968;&#38454;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Meta-Learning Parameterized First-Order Optimizers using Differentiable Convex Optimization. (arXiv:2303.16952v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16952
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#21487;&#24494;&#20984;&#20248;&#21270;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#29616;&#26377;&#30340;&#19968;&#38454;&#26356;&#26032;&#35268;&#21017;&#25512;&#24191;&#21040;&#26356;&#24191;&#30340;&#23478;&#26063;&#65292;&#35777;&#26126;&#22312;&#20803;&#23398;&#20064;&#32773;&#26377;&#36275;&#22815;&#31867;&#20284;&#20219;&#21153;&#30340;&#32463;&#39564;&#19979;&#65292;&#21487;&#20197;&#19968;&#27493;&#20248;&#21270;&#19968;&#31995;&#21015;&#32447;&#24615;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#25511;&#21046;&#20013;&#30340;&#20256;&#32479;&#20248;&#21270;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#19968;&#38454;&#26356;&#26032;&#35268;&#21017;&#12290;&#38024;&#23545;&#26576;&#20010;&#20219;&#21153;&#36873;&#25321;&#21512;&#36866;&#26041;&#27861;&#21644;&#36229;&#21442;&#25968;&#24120;&#24120;&#38656;&#35201;&#35797;&#38169;&#25110;&#20174;&#19994;&#32773;&#30452;&#35273;&#65292;&#36825;&#20419;&#36827;&#20102;&#20803;&#23398;&#20064;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#20869;&#24490;&#29615;&#20248;&#21270;&#27493;&#39588;&#28041;&#21450;&#21487;&#24494;&#20984;&#20248;&#21270;(DCO)&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#25512;&#24191;&#20102;&#19968;&#20010;&#24191;&#27867;&#30340;&#29616;&#26377;&#26356;&#26032;&#35268;&#21017;&#23478;&#26063;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#27492;&#26041;&#27861;&#30340;&#29702;&#35770;&#21560;&#24341;&#21147;&#65292;&#35777;&#26126;&#20102;&#22312;&#20803;&#23398;&#20064;&#32773;&#26377;&#36275;&#22815;&#30340;&#31867;&#20284;&#20219;&#21153;&#32463;&#39564;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#21487;&#20197;&#19968;&#27493;&#20248;&#21270;&#19968;&#31995;&#21015;&#32447;&#24615;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#12290;&#22312;&#19968;&#31995;&#21015;&#35828;&#26126;&#24615;&#23454;&#39564;&#35774;&#32622;&#20013;&#65292;&#23558;DCO&#26356;&#26032;&#35268;&#21017;&#30340;&#21508;&#31181;&#23454;&#20363;&#19982;&#20256;&#32479;&#20248;&#21270;&#22120;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional optimization methods in machine learning and controls rely heavily on first-order update rules. Selecting the right method and hyperparameters for a particular task often involves trial-and-error or practitioner intuition, motivating the field of meta-learning. We generalize a broad family of preexisting update rules by proposing a meta-learning framework in which the inner loop optimization step involves solving a differentiable convex optimization (DCO). We illustrate the theoretical appeal of this approach by showing that it enables one-step optimization of a family of linear least squares problems, given that the meta-learner has sufficient exposure to similar tasks. Various instantiations of the DCO update rule are compared to conventional optimizers on a range of illustrative experimental settings.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;BDDL&#30340;&#39046;&#22495;&#23450;&#20041;&#35821;&#35328;&#20197;&#36827;&#34892;&#32593;&#26684;&#26827;&#30424;&#28216;&#25103;&#65288;&#22914;&#20117;&#23383;&#28216;&#25103;&#65292;&#36830;&#22235;&#65292;&#21344;&#39046;&#28216;&#25103;&#65292;&#36861;&#36880;&#32773;-&#36867;&#36991;&#32773;&#21644;&#31361;&#30772;&#65289;&#30340;&#32479;&#19968;&#21644;&#31616;&#27905;&#30340;QBF&#32534;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;QBF&#27714;&#35299;&#22120;&#22312;&#26377;&#38480;&#28145;&#24230;&#19978;&#20915;&#23450;&#32988;&#32773;&#12290;</title><link>http://arxiv.org/abs/2303.16949</link><description>&lt;p&gt;
&#32593;&#26684;&#28216;&#25103;&#30340;&#31616;&#27905;QBF&#32534;&#30721;&#65288;&#25193;&#23637;&#29256;&#26412;&#65289;
&lt;/p&gt;
&lt;p&gt;
Concise QBF Encodings for Games on a Grid (extended version). (arXiv:2303.16949v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16949
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;BDDL&#30340;&#39046;&#22495;&#23450;&#20041;&#35821;&#35328;&#20197;&#36827;&#34892;&#32593;&#26684;&#26827;&#30424;&#28216;&#25103;&#65288;&#22914;&#20117;&#23383;&#28216;&#25103;&#65292;&#36830;&#22235;&#65292;&#21344;&#39046;&#28216;&#25103;&#65292;&#36861;&#36880;&#32773;-&#36867;&#36991;&#32773;&#21644;&#31361;&#30772;&#65289;&#30340;&#32479;&#19968;&#21644;&#31616;&#27905;&#30340;QBF&#32534;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;QBF&#27714;&#35299;&#22120;&#22312;&#26377;&#38480;&#28145;&#24230;&#19978;&#20915;&#23450;&#32988;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32534;&#30721;2&#20154;&#28216;&#25103;QBF&#30340;&#27491;&#30830;&#24615;&#21644;&#25928;&#29575;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#23481;&#26131;&#20986;&#38169;&#30340;&#12290;&#20026;&#20102;&#23454;&#29616;&#22312;&#32593;&#26684;&#26827;&#30424;&#19978;&#36827;&#34892;&#28216;&#25103;&#65288;&#22914;&#20117;&#23383;&#28216;&#25103;&#65292;&#36830;&#22235;&#65292;&#21344;&#39046;&#27604;&#36187;&#65292;&#36861;&#36880;&#32773;-&#36867;&#36991;&#32773;&#21644;&#31361;&#30772;&#65289;&#30340;&#31616;&#27905;&#35268;&#26684;&#21644;&#32479;&#19968;&#32534;&#30721;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20182;&#20154;&#39046;&#22495;&#23450;&#20041;&#35821;&#35328;&#65288;BDDL&#65289;&#65292;&#23427;&#21463;&#21040;&#20102;&#22312;&#35745;&#21010;&#39046;&#22495;&#20013;PDDL&#30340;&#25104;&#21151;&#21551;&#21457;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20174;BDDL&#21040;QBF&#30340;&#26377;&#25928;&#36716;&#25442;&#65292;&#23558;&#26377;&#38480;&#28145;&#24230;&#30340;&#33719;&#32988;&#31574;&#30053;&#30340;&#23384;&#22312;&#36827;&#34892;&#32534;&#30721;&#12290;&#25105;&#20204;&#30340;&#25552;&#21319;&#32534;&#30721;&#31526;&#21495;&#22320;&#22788;&#29702;&#26827;&#30424;&#20301;&#32622;&#65292;&#24182;&#20801;&#35768;&#30456;&#23545;&#20110;&#31526;&#21495;&#26827;&#30424;&#20301;&#32622;&#31616;&#27905;&#23450;&#20041;&#26465;&#20214;&#12289;&#25928;&#26524;&#21644;&#33719;&#32988;&#37197;&#32622;&#12290;&#32534;&#30721;&#30340;&#22823;&#23567;&#22312;&#36755;&#20837;&#27169;&#22411;&#21644;&#32771;&#34385;&#30340;&#28145;&#24230;&#20013;&#21576;&#32447;&#24615;&#22686;&#38271;&#12290;&#20026;&#20102;&#23637;&#31034;&#36825;&#31181;&#36890;&#29992;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;QBF&#27714;&#35299;&#22120;&#35745;&#31639;&#20102;&#20960;&#20010;&#24050;&#30693;&#28216;&#25103;&#23454;&#20363;&#30340;&#33719;&#32988;&#31574;&#30053;&#30340;&#20851;&#38190;&#28145;&#24230;&#12290;&#23545;&#20110;&#20960;&#20010;&#28216;&#25103;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;QBF&#32534;&#30721;&#12290;&#19982;SAT&#20013;&#30340;&#35745;&#21010;&#39564;&#35777;&#19981;&#21516;&#65292;
&lt;/p&gt;
&lt;p&gt;
Encoding 2-player games in QBF correctly and efficiently is challenging and error-prone. To enable concise specifications and uniform encodings of games played on grid boards, like Tic-Tac-Toe, Connect-4, Domineering, Pursuer-Evader and Breakthrough, we introduce Board-game Domain Definition Language (BDDL), inspired by the success of PDDL in the planning domain.  We provide an efficient translation from BDDL into QBF, encoding the existence of a winning strategy of bounded depth. Our lifted encoding treats board positions symbolically and allows concise definitions of conditions, effects and winning configurations, relative to symbolic board positions. The size of the encoding grows linearly in the input model and the considered depth.  To show the feasibility of such a generic approach, we use QBF solvers to compute the critical depths of winning strategies for instances of several known games. For several games, our work provides the first QBF encoding. Unlike plan validation in SAT
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23545;&#24403;&#21069;&#24191;&#27867;&#20351;&#29992;&#30340;NAS&#22522;&#20934;&#27979;&#35797;&#36827;&#34892;&#20102;&#32463;&#39564;&#30740;&#31350;&#65292;&#21457;&#29616;&#21482;&#38656;&#19968;&#23567;&#37096;&#20998;&#30340;&#25805;&#20316;&#21363;&#21487;&#29983;&#25104;&#25509;&#36817;&#26368;&#39640;&#24615;&#33021;&#30340;&#26550;&#26500;&#65292;&#21516;&#26102;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#23384;&#22312;&#32570;&#28857;&#21487;&#33021;&#24433;&#21709;&#20844;&#24179;&#27604;&#36739;&#24182;&#25552;&#20379;&#19981;&#21487;&#38752;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.16938</link><description>&lt;p&gt;
&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#22522;&#20934;&#27979;&#35797;&#26159;&#21542;&#35774;&#35745;&#33391;&#22909;&#65311;&#23545;&#25805;&#20316;&#37325;&#35201;&#24615;&#30340;&#28145;&#20837;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Are Neural Architecture Search Benchmarks Well Designed? A Deeper Look Into Operation Importance. (arXiv:2303.16938v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23545;&#24403;&#21069;&#24191;&#27867;&#20351;&#29992;&#30340;NAS&#22522;&#20934;&#27979;&#35797;&#36827;&#34892;&#20102;&#32463;&#39564;&#30740;&#31350;&#65292;&#21457;&#29616;&#21482;&#38656;&#19968;&#23567;&#37096;&#20998;&#30340;&#25805;&#20316;&#21363;&#21487;&#29983;&#25104;&#25509;&#36817;&#26368;&#39640;&#24615;&#33021;&#30340;&#26550;&#26500;&#65292;&#21516;&#26102;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#23384;&#22312;&#32570;&#28857;&#21487;&#33021;&#24433;&#21709;&#20844;&#24179;&#27604;&#36739;&#24182;&#25552;&#20379;&#19981;&#21487;&#38752;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#22522;&#20934;&#27979;&#35797;&#26174;&#33879;&#25552;&#39640;&#20102;&#24320;&#21457;&#21644;&#27604;&#36739;NAS&#26041;&#27861;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#36890;&#36807;&#25552;&#20379;&#20851;&#20110;&#25968;&#21315;&#20010;&#35757;&#32451;&#36807;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#20803;&#20449;&#24687;&#65292;&#22823;&#24133;&#20943;&#23569;&#20102;&#35745;&#31639;&#24320;&#38144;&#12290;&#28982;&#32780;&#65292;&#34920;&#26684;&#22522;&#20934;&#27979;&#35797;&#20855;&#26377;&#20960;&#20010;&#32570;&#28857;&#65292;&#21487;&#33021;&#20250;&#38459;&#30861;&#20844;&#24179;&#27604;&#36739;&#24182;&#25552;&#20379;&#19981;&#21487;&#38752;&#30340;&#32467;&#26524;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#24191;&#27867;&#20351;&#29992;&#30340;NAS-Bench-101&#12289;NAS-Bench-201&#21644;TransNAS-Bench-101&#22522;&#20934;&#27979;&#35797;&#36827;&#34892;&#20102;&#32463;&#39564;&#24615;&#20998;&#26512;&#65292;&#37325;&#28857;&#20851;&#27880;&#23427;&#20204;&#30340;&#36890;&#29992;&#24615;&#20197;&#21450;&#19981;&#21516;&#25805;&#20316;&#22914;&#20309;&#24433;&#21709;&#25152;&#29983;&#25104;&#26550;&#26500;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20165;&#38656;&#35201;&#25805;&#20316;&#27744;&#30340;&#19968;&#37096;&#20998;&#21363;&#21487;&#29983;&#25104;&#25509;&#36817;&#26368;&#39640;&#24615;&#33021;&#33539;&#22260;&#30340;&#26550;&#26500;&#12290;&#27492;&#22806;&#65292;&#24615;&#33021;&#20998;&#24067;&#20855;&#26377;&#36127;&#20559;&#26012;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Architecture Search (NAS) benchmarks significantly improved the capability of developing and comparing NAS methods while at the same time drastically reduced the computational overhead by providing meta-information about thousands of trained neural networks. However, tabular benchmarks have several drawbacks that can hinder fair comparisons and provide unreliable results. These usually focus on providing a small pool of operations in heavily constrained search spaces -- usually cell-based neural networks with pre-defined outer-skeletons. In this work, we conducted an empirical analysis of the widely used NAS-Bench-101, NAS-Bench-201 and TransNAS-Bench-101 benchmarks in terms of their generability and how different operations influence the performance of the generated architectures. We found that only a subset of the operation pool is required to generate architectures close to the upper-bound of the performance range. Also, the performance distribution is negatively skewed, havi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#26032;&#31639;&#27861;&#26469;&#36873;&#25321;&#22522;&#22240;&#32452;&#25968;&#25454;&#20013;&#26368;&#26377;&#20449;&#24687;&#30340;&#29305;&#24449;&#65292;&#20026;&#35786;&#26029;&#12289;&#39044;&#21518;&#21644;&#31934;&#20934;&#21307;&#23398;&#25552;&#20379;&#25903;&#25345;&#65292;&#24182;&#22312;&#24930;&#24615;&#28107;&#24052;&#32454;&#32990;&#30333;&#34880;&#30149;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26377;&#25928;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.16914</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;XAI&#30340;&#22522;&#22240;&#32452;&#23398;&#29305;&#24449;&#36873;&#25321;&#26032;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A New Deep Learning and XAI-Based Algorithm for Features Selection in Genomics. (arXiv:2303.16914v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16914
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#26032;&#31639;&#27861;&#26469;&#36873;&#25321;&#22522;&#22240;&#32452;&#25968;&#25454;&#20013;&#26368;&#26377;&#20449;&#24687;&#30340;&#29305;&#24449;&#65292;&#20026;&#35786;&#26029;&#12289;&#39044;&#21518;&#21644;&#31934;&#20934;&#21307;&#23398;&#25552;&#20379;&#25903;&#25345;&#65292;&#24182;&#22312;&#24930;&#24615;&#28107;&#24052;&#32454;&#32990;&#30333;&#34880;&#30149;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26377;&#25928;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21151;&#33021;&#22522;&#22240;&#32452;&#23398;&#39046;&#22495;&#65292;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#23545;&#22522;&#22240;&#34920;&#36798;&#35889;&#30340;&#20998;&#26512;&#36234;&#26469;&#36234;&#33021;&#22815;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#27934;&#23519;&#21147;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#22810;&#31181;&#30142;&#30149;&#30340;&#20998;&#26512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#33258;&#32534;&#30721;&#22120;&#30340;&#37325;&#26500;&#33021;&#21147;&#21644;&#19968;&#31181;&#19987;&#38376;&#23450;&#20041;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#22522;&#30784;&#20998;&#25968;&#65292;&#20197;&#36873;&#25321;&#20316;&#20026;&#35786;&#26029;&#12289;&#39044;&#21518;&#21644;&#31934;&#20934;&#21307;&#23398;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#22522;&#22240;&#32452;&#25968;&#25454;&#29305;&#24449;&#12290;&#22312;&#24930;&#24615;&#28107;&#24052;&#32454;&#32990;&#30333;&#34880;&#30149;&#25968;&#25454;&#38598;&#19978;&#30340;&#24212;&#29992;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#30830;&#23450;&#24182;&#25552;&#20379;&#19968;&#32452;&#20855;&#26377;&#24847;&#20041;&#30340;&#22522;&#22240;&#32452;&#25968;&#25454;&#29305;&#24449;&#65292;&#26377;&#21161;&#20110;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#21307;&#23398;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of functional genomics, the analysis of gene expression profiles through Machine and Deep Learning is increasingly providing meaningful insight into a number of diseases. The paper proposes a novel algorithm to perform Feature Selection on genomic-scale data, which exploits the reconstruction capabilities of autoencoders and an ad-hoc defined Explainable Artificial Intelligence-based score in order to select the most informative genes for diagnosis, prognosis, and precision medicine. Results of the application on a Chronic Lymphocytic Leukemia dataset evidence the effectiveness of the algorithm, by identifying and suggesting a set of meaningful genes for further medical investigation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#20351;&#29992;ChatGPT&#23545;&#25968;&#25454;&#36827;&#34892;&#28165;&#27927;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#32467;&#21512;&#29992;&#25143;&#25552;&#20379;&#30340;&#25968;&#25454;&#28246;&#30340;&#22522;&#20110;&#26816;&#32034;&#30340;&#28165;&#27927;&#26041;&#27861;&#65292;&#21516;&#26102;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#22312;&#26412;&#22320;&#37096;&#32626;&#30340;RoBERTa&#27169;&#22411;&#26469;&#35299;&#20915;&#38544;&#31169;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.16909</link><description>&lt;p&gt;
RetClean: &#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#19982;&#25968;&#25454;&#28246;&#30340;&#26816;&#32034;&#24335;&#25968;&#25454;&#28165;&#27927;
&lt;/p&gt;
&lt;p&gt;
RetClean: Retrieval-Based Data Cleaning Using Foundation Models and Data Lakes. (arXiv:2303.16909v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16909
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#20351;&#29992;ChatGPT&#23545;&#25968;&#25454;&#36827;&#34892;&#28165;&#27927;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#32467;&#21512;&#29992;&#25143;&#25552;&#20379;&#30340;&#25968;&#25454;&#28246;&#30340;&#22522;&#20110;&#26816;&#32034;&#30340;&#28165;&#27927;&#26041;&#27861;&#65292;&#21516;&#26102;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#22312;&#26412;&#22320;&#37096;&#32626;&#30340;RoBERTa&#27169;&#22411;&#26469;&#35299;&#20915;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;ChatGPT&#26469;&#25552;&#20379;&#25968;&#25454;&#28165;&#27927;&#24314;&#35758;&#30340;&#21487;&#33021;&#24615;&#12290;&#20294;&#22312;&#22788;&#29702;&#20225;&#19994;&#25968;&#25454;&#25110;&#38656;&#35201;&#35299;&#37322;&#24314;&#35758;&#26469;&#28304;&#26102;&#65292;ChatGPT&#21487;&#33021;&#26080;&#27861;&#32988;&#20219;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#37197;&#21512;&#29992;&#25143;&#25552;&#20379;&#30340;&#25968;&#25454;&#28246;&#65292;&#23558;&#25968;&#25454;&#28246;&#30340;&#25968;&#25454;&#19982;ChatGPT&#30340;&#33021;&#21147;&#32467;&#21512;&#20351;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;RoBERTa&#30340;&#23450;&#21046;&#21270;&#27169;&#22411;&#65292;&#29992;&#25143;&#21487;&#20197;&#22312;&#26412;&#22320;&#36827;&#34892;&#37096;&#32626;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can foundation models (such as ChatGPT) clean your data? In this proposal, we demonstrate that indeed ChatGPT can assist in data cleaning by suggesting corrections for specific cells in a data table (scenario 1). However, ChatGPT may struggle with datasets it has never encountered before (e.g., local enterprise data) or when the user requires an explanation of the source of the suggested clean values. To address these issues, we developed a retrieval-based method that complements ChatGPT's power with a user-provided data lake. The data lake is first indexed, we then retrieve the top-k relevant tuples to the user's query tuple and finally leverage ChatGPT to infer the correct value (scenario 2). Nevertheless, sharing enterprise data with ChatGPT, an externally hosted model, might not be feasible for privacy reasons. To assist with this scenario, we developed a custom RoBERTa-based foundation model that can be locally deployed. By fine-tuning it on a small number of examples, it can effe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#34433;&#32676;&#31639;&#27861;&#21644;Cohort Intelligence&#31639;&#27861;&#30340;&#28151;&#21512;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#26753;&#35774;&#35745;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#27604;&#36739;&#24050;&#26377;&#31639;&#27861;&#65292;&#20248;&#21270;&#20102;&#35745;&#31639;&#26102;&#38388;&#65292;&#24182;&#25104;&#21151;&#35299;&#20915;&#20102;&#20004;&#20010;&#26426;&#26800;&#35774;&#35745;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.16908</link><description>&lt;p&gt;
&#22522;&#20110;&#28151;&#21512;&#34433;&#32676;&#31639;&#27861;&#21644;Cohort Intelligence&#31639;&#27861;&#30340;&#26753;&#35774;&#35745;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Hybrid ACO-CI Algorithm for Beam Design problems. (arXiv:2303.16908v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#34433;&#32676;&#31639;&#27861;&#21644;Cohort Intelligence&#31639;&#27861;&#30340;&#28151;&#21512;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#26753;&#35774;&#35745;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#27604;&#36739;&#24050;&#26377;&#31639;&#27861;&#65292;&#20248;&#21270;&#20102;&#35745;&#31639;&#26102;&#38388;&#65292;&#24182;&#25104;&#21151;&#35299;&#20915;&#20102;&#20004;&#20010;&#26426;&#26800;&#35774;&#35745;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#30340;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#25512;&#21160;&#20102;&#22810;&#31181;&#20248;&#21270;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#28151;&#21512;Ant colony optimization (ACO)&#26041;&#27861;&#65292;&#20351;&#29992;Cohort Intelligence (CI)&#31639;&#27861;&#30340;&#26679;&#26412;&#31354;&#38388;&#32553;&#20943;&#25216;&#26415;&#12290;&#36890;&#36807;&#35299;&#20915;35&#20010;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20989;&#25968;&#26469;&#24320;&#21457;&#21644;&#27979;&#35797;&#31639;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#31639;&#27861;&#30340;&#32422;&#26463;&#29256;&#26412;&#26469;&#35299;&#20915;&#28041;&#21450;&#21488;&#38454;&#24748;&#33218;&#26753;&#21644;I&#22411;&#26753;&#30340;&#20004;&#20010;&#26426;&#26800;&#35774;&#35745;&#38382;&#39064;&#12290;&#23558;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#19982;&#24050;&#32463;&#20351;&#29992;&#30340;&#29616;&#20195;&#31639;&#27861;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25152;&#25552;&#20986;&#30340;&#28151;&#21512;ACO-CI&#31639;&#27861;&#23558;&#38656;&#35201;&#26356;&#23569;&#30340;&#36845;&#20195;&#27425;&#25968;&#26469;&#20135;&#29983;&#25152;&#38656;&#30340;&#36755;&#20986;&#65292;&#36825;&#24847;&#21619;&#30528;&#38656;&#35201;&#26356;&#23569;&#30340;&#35745;&#31639;&#26102;&#38388;&#12290;&#23545;&#20110;&#26368;&#23567;&#21270;&#21488;&#38454;&#24748;&#33218;&#26753;&#30340;&#37325;&#37327;&#21644;I&#22411;&#26753;&#20013;&#30340;&#25376;&#24230;&#65292;&#25152;&#25552;&#20986;&#30340;&#28151;&#21512;ACO-CI&#31639;&#27861;&#20135;&#29983;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
A range of complicated real-world problems have inspired the development of several optimization methods. Here, a novel hybrid version of the Ant colony optimization (ACO) method is developed using the sample space reduction technique of the Cohort Intelligence (CI) Algorithm. The algorithm is developed, and accuracy is tested by solving 35 standard benchmark test functions. Furthermore, the constrained version of the algorithm is used to solve two mechanical design problems involving stepped cantilever beams and I-section beams. The effectiveness of the proposed technique of solution is evaluated relative to contemporary algorithmic approaches that are already in use. The results show that our proposed hybrid ACO-CI algorithm will take lesser number of iterations to produce the desired output which means lesser computational time. For the minimization of weight of stepped cantilever beam and deflection in I-section beam a proposed hybrid ACO-CI algorithm yielded best results when comp
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32435;&#31859;&#39063;&#31890;&#23450;&#20301;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#22312;&#21512;&#25104;&#30340;2PM&#22270;&#20687;&#20013;&#27979;&#35797;&#25928;&#26524;&#33391;&#22909;&#65292;&#21487;&#20197;&#35299;&#37322;&#22522;&#20110;&#24378;&#24230;&#30340;&#26041;&#27861;&#30340;&#22833;&#36133;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2303.16903</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#36741;&#21161;&#21512;&#25104;&#30340;&#21452;&#20809;&#23376;&#26174;&#24494;&#38236;&#22270;&#20687;&#20013;&#30340;&#32435;&#31859;&#39063;&#31890;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Deep Learning-Assisted Localisation of Nanoparticles in synthetically generated two-photon microscopy images. (arXiv:2303.16903v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16903
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32435;&#31859;&#39063;&#31890;&#23450;&#20301;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#22312;&#21512;&#25104;&#30340;2PM&#22270;&#20687;&#20013;&#27979;&#35797;&#25928;&#26524;&#33391;&#22909;&#65292;&#21487;&#20197;&#35299;&#37322;&#22522;&#20110;&#24378;&#24230;&#30340;&#26041;&#27861;&#30340;&#22833;&#36133;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36861;&#36394;&#21333;&#20010;&#20998;&#23376;&#23545;&#20110;&#37327;&#21270;&#33647;&#29289;&#22312;&#29983;&#29289;&#26679;&#26412;&#20013;&#30340;&#36755;&#36865;&#33267;&#20851;&#37325;&#35201;&#65292;&#20363;&#22914;&#22312;&#22823;&#33041;&#33647;&#29289;&#36755;&#36865;&#30740;&#31350;&#20013;&#12290;&#26082;&#26377;&#30340;&#22522;&#20110;&#24378;&#24230;&#30340;&#23450;&#20301;&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#25195;&#25551;&#26174;&#24494;&#38236;&#25104;&#20687;&#65292;&#36890;&#24120;&#29992;&#20110;&#20307;&#20869;&#25104;&#20687;&#12290;&#25195;&#25551;&#21452;&#20809;&#23376;&#26174;&#24494;&#38236;&#65288;Two-photon microscopy&#65292;2PM&#65289;&#25104;&#20687;&#30340;&#20302;&#20449;&#22122;&#27604;&#12289;&#20998;&#23376;&#31163;&#24320;&#28966;&#24179;&#38754;&#30340;&#36816;&#21160;&#21644;&#39640;&#36816;&#21160;&#27169;&#31946;&#23545;&#20110;&#20998;&#23376;&#30340;&#20934;&#30830;&#23450;&#20301;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#27169;&#22411;&#30340;&#24212;&#29992;&#30001;&#20110;&#20307;&#20869;&#23454;&#39564;&#30340;&#25968;&#25454;&#37327;&#36890;&#24120;&#24456;&#23569;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#34917;&#20805;&#31232;&#32570;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;2PM&#22270;&#20687;&#27169;&#25311;&#22120;&#12290;&#35813;&#27169;&#25311;&#22120;&#27169;&#20223;&#20102;&#20307;&#20869;&#25104;&#20687;&#20013;&#35266;&#23519;&#21040;&#30340;&#36816;&#21160;&#27169;&#31946;&#12289;&#32972;&#26223;&#33639;&#20809;&#21644;&#20809;&#23376;&#22122;&#22768;&#12290;&#29992;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#21512;&#25104;&#22270;&#20687;&#30340;&#23450;&#20301;&#36136;&#37327;&#65292;&#24182;&#35299;&#37322;&#20026;&#20160;&#20040;&#22522;&#20110;&#24378;&#24230;&#30340;&#26041;&#27861;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tracking single molecules is instrumental for quantifying the transport of molecules and nanoparticles in biological samples, e.g., in brain drug delivery studies. Existing intensity-based localisation methods are not developed for imaging with a scanning microscope, typically used for in vivo imaging. Low signal-to-noise ratios, movement of molecules out-of-focus, and high motion blur on images recorded with scanning two-photon microscopy (2PM) in vivo pose a challenge to the accurate localisation of molecules. Using data-driven models is challenging due to low data volumes, typical for in vivo experiments. We developed a 2PM image simulator to supplement scarce training data. The simulator mimics realistic motion blur, background fluorescence, and shot noise observed in vivo imaging. Training a data-driven model with simulated data improves localisation quality in simulated images and shows why intensity-based methods fail.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#24335;&#20943;&#36731;&#35270;&#35273;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#21518;&#39564;&#20272;&#35745;&#23574;&#38160;&#21270;&#65292;&#40723;&#21169;&#32593;&#32476;&#32858;&#28966;&#20110;&#19981;&#23548;&#33268;&#39640;&#19981;&#30830;&#23450;&#24615;&#30340;&#20013;&#24515;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2303.16564</link><description>&lt;p&gt;
&#36890;&#36807;&#21518;&#39564;&#20272;&#35745;&#23574;&#38160;&#21270;&#26469;&#38544;&#24335;&#20943;&#36731;&#35270;&#35273;&#20559;&#35265;&#65306;&#19968;&#31181;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Implicit Visual Bias Mitigation by Posterior Estimate Sharpening of a Bayesian Neural Network. (arXiv:2303.16564v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16564
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#24335;&#20943;&#36731;&#35270;&#35273;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#21518;&#39564;&#20272;&#35745;&#23574;&#38160;&#21270;&#65292;&#40723;&#21169;&#32593;&#32476;&#32858;&#28966;&#20110;&#19981;&#23548;&#33268;&#39640;&#19981;&#30830;&#23450;&#24615;&#30340;&#20013;&#24515;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20844;&#24179;&#24615;&#21463;&#21040;&#25968;&#25454;&#38598;&#20559;&#35265;&#21644;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#24378;&#28872;&#24433;&#21709;&#65292;&#32780;&#36825;&#20123;&#23545;&#29616;&#20195;&#29305;&#24449;&#20016;&#23500;&#21644;&#22797;&#26434;&#30340;&#35270;&#35273;&#25968;&#25454;&#38598;&#36890;&#24120;&#37117;&#26159;&#23384;&#22312;&#30340;&#12290;&#30001;&#20110;&#20219;&#21153;&#30340;&#38590;&#24230;&#21644;&#21487;&#21464;&#24615;&#65292;&#27809;&#26377;&#19968;&#31181;&#21333;&#19968;&#30340;&#21435;&#20559;&#35265;&#26041;&#27861;&#26159;&#26222;&#36941;&#25104;&#21151;&#30340;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#19981;&#38656;&#35201;&#26174;&#24335;&#30693;&#36947;&#20559;&#24046;&#21464;&#37327;&#30340;&#24773;&#20917;&#19979;&#30340;&#38544;&#24335;&#26041;&#27861;&#23545;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#23588;&#20026;&#30456;&#20851;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38544;&#24335;&#20943;&#32531;&#26041;&#27861;&#65292;&#20351;&#29992;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#20801;&#35768;&#25105;&#20204;&#21033;&#29992;&#33268;&#24615;&#19981;&#30830;&#23450;&#24615;&#19982;&#26679;&#26412;&#20013;&#20559;&#24046;&#25110;&#34394;&#20551;&#30456;&#20851;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#21518;&#39564;&#20272;&#35745;&#23574;&#38160;&#21270;&#31243;&#24207;&#40723;&#21169;&#32593;&#32476;&#32858;&#28966;&#20110;&#19981;&#23548;&#33268;&#39640;&#19981;&#30830;&#23450;&#24615;&#30340;&#20013;&#24515;&#29305;&#24449;&#12290;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20855;&#26377;&#32463;&#36807;&#23574;&#38160;&#21270;&#21518;&#39564;&#20272;&#35745;&#30340;&#36125;&#21494;&#26031;&#32593;&#32476;&#34920;&#29616;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#65292;&#24182;&#26174;&#31034;&#20986;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The fairness of a deep neural network is strongly affected by dataset bias and spurious correlations, both of which are usually present in modern feature-rich and complex visual datasets. Due to the difficulty and variability of the task, no single de-biasing method has been universally successful. In particular, implicit methods not requiring explicit knowledge of bias variables are especially relevant for real-world applications. We propose a novel implicit mitigation method using a Bayesian neural network, allowing us to leverage the relationship between epistemic uncertainties and the presence of bias or spurious correlations in a sample. Our proposed posterior estimate sharpening procedure encourages the network to focus on core features that do not contribute to high uncertainties. Experimental results on three benchmark datasets demonstrate that Bayesian networks with sharpened posterior estimates perform comparably to prior existing methods and show potential worthy of further 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#32047;&#31215;&#35270;&#35273;&#21464;&#25442;&#22120;&#30340;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#39592;&#40836;&#35780;&#20272;&#12290;&#36890;&#36807;&#24212;&#29992;&#26631;&#35760;&#37325;&#25918;&#21644;&#21306;&#22495;&#27880;&#24847;&#20559;&#24046;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#25366;&#25496;&#22320;&#26631;&#20043;&#38388;&#30340;&#20851;&#31995;&#24182;&#23398;&#20064;&#20840;&#23616;&#24418;&#24577;&#29305;&#24449;&#65292;&#20351;&#39592;&#40836;&#35780;&#20272;&#30340;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#27604;&#20043;&#21069;&#30340;&#24037;&#20316;&#38477;&#20302;&#20102;0.11&#12290;</title><link>http://arxiv.org/abs/2303.16557</link><description>&lt;p&gt;
Bone Age Assessment&#30340;&#33258;&#32047;&#31215;&#35270;&#35273;&#21464;&#25442;&#22120;&#65292;&#20351;&#29992;Sauvegrain&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Self-accumulative Vision Transformer for Bone Age Assessment Using the Sauvegrain Method. (arXiv:2303.16557v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16557
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#32047;&#31215;&#35270;&#35273;&#21464;&#25442;&#22120;&#30340;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#39592;&#40836;&#35780;&#20272;&#12290;&#36890;&#36807;&#24212;&#29992;&#26631;&#35760;&#37325;&#25918;&#21644;&#21306;&#22495;&#27880;&#24847;&#20559;&#24046;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#25366;&#25496;&#22320;&#26631;&#20043;&#38388;&#30340;&#20851;&#31995;&#24182;&#23398;&#20064;&#20840;&#23616;&#24418;&#24577;&#29305;&#24449;&#65292;&#20351;&#39592;&#40836;&#35780;&#20272;&#30340;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#27604;&#20043;&#21069;&#30340;&#24037;&#20316;&#38477;&#20302;&#20102;0.11&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;Sauvegrain&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#35270;&#35282;&#12289;&#22810;&#20219;&#21153;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#39592;&#40836;&#35780;&#20272;&#65288;BAA&#65289;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#20998;&#31867;&#22120;&#26469;&#35780;&#20998;&#27599;&#20010;&#19968;&#28857;&#30340;&#25104;&#29087;&#24230;&#24182;&#39044;&#27979;&#39592;&#40836;&#65292;&#20294;&#26159;&#36825;&#31181;&#26041;&#27861;&#23616;&#38480;&#20110;&#26412;&#22320;&#24418;&#24577;&#65292;&#24182;&#22686;&#21152;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#32047;&#31215;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;SAT&#65289;&#65292;&#36890;&#36807;&#24212;&#29992;&#26631;&#35760;&#37325;&#25918;&#21644;&#21306;&#22495;&#27880;&#24847;&#20559;&#24046;&#65292;&#28040;&#20943;&#20102;&#22810;&#35270;&#35282;&#12289;&#22810;&#20219;&#21153;&#38382;&#39064;&#20013;&#36890;&#24120;&#21457;&#29983;&#30340;&#21508;&#21521;&#24322;&#24615;&#34892;&#20026;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#25366;&#25496;&#22320;&#26631;&#20043;&#38388;&#30340;&#20851;&#31995;&#24182;&#23398;&#20064;&#20840;&#23616;&#24418;&#24577;&#29305;&#24449;&#12290;&#36890;&#36807;&#22810;&#27425;&#23454;&#39564;&#34920;&#26126;&#65292;SAT&#25104;&#21151;&#22320;&#23558;&#22320;&#26631;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#20840;&#23616;&#24418;&#24577;&#29305;&#24449;&#34701;&#21512;&#65292;&#20351;&#39592;&#40836;&#35780;&#20272;&#30340;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#27604;&#20043;&#21069;&#30340;&#24037;&#20316;&#38477;&#20302;&#20102;0.11&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents a novel approach to bone age assessment (BAA) using a multi-view, multi-task classification model based on the Sauvegrain method. A straightforward solution to automating the Sauvegrain method, which assesses a maturity score for each landmark in the elbow and predicts the bone age, is to train classifiers independently to score each region of interest (RoI), but this approach limits the accessible information to local morphologies and increases computational costs. As a result, this work proposes a self-accumulative vision transformer (SAT) that mitigates anisotropic behavior, which usually occurs in multi-view, multi-task problems and limits the effectiveness of a vision transformer, by applying token replay and regional attention bias. A number of experiments show that SAT successfully exploits the relationships between landmarks and learns global morphological features, resulting in a mean absolute error of BAA that is 0.11 lower than that of the previous work. 
&lt;/p&gt;</description></item><item><title>TraffNet&#26159;&#19968;&#20010;&#23398;&#20064;&#20132;&#36890;&#37327;&#29983;&#25104;&#21407;&#22240;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#34920;&#31034;&#20026;&#24322;&#26500;&#22270;&#65292;&#21033;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#23454;&#29616;&#20102;&#23545;&#20132;&#36890;&#29983;&#25104;&#21407;&#22240;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2303.15954</link><description>&lt;p&gt;
TraffNet&#65306;&#23398;&#20064;&#36947;&#36335;&#32593;&#32476;&#25968;&#23383;&#23402;&#29983;&#20132;&#36890;&#29983;&#25104;&#22240;&#26524;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
TraffNet: Learning Causality of Traffic Generation for Road Network Digital Twins. (arXiv:2303.15954v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15954
&lt;/p&gt;
&lt;p&gt;
TraffNet&#26159;&#19968;&#20010;&#23398;&#20064;&#20132;&#36890;&#37327;&#29983;&#25104;&#21407;&#22240;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#34920;&#31034;&#20026;&#24322;&#26500;&#22270;&#65292;&#21033;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#23454;&#29616;&#20102;&#23545;&#20132;&#36890;&#29983;&#25104;&#21407;&#22240;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36947;&#36335;&#32593;&#32476;&#25968;&#23383;&#23402;&#29983;&#65288;RNDT&#65289;&#22312;&#24320;&#21457;&#19979;&#19968;&#20195;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#20132;&#36890;&#35268;&#21010;&#21644;&#25511;&#21046;&#12290;&#20026;&#20102;&#25903;&#25345;&#23454;&#26102;&#20915;&#31574;&#65292;RNDT&#38656;&#35201;&#19968;&#20010;&#27169;&#22411;&#65292;&#20174;&#22312;&#32447;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#21160;&#24577;&#23398;&#20064;&#20132;&#36890;&#27169;&#24335;&#24182;&#29983;&#25104;&#39640;&#20445;&#30495;&#27169;&#25311;&#32467;&#26524;&#12290;&#23613;&#31649;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24403;&#21069;&#20132;&#36890;&#39044;&#27979;&#25216;&#26415;&#24050;&#32463;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#36825;&#20123;&#25216;&#26415;&#20165;&#36890;&#36807;&#25366;&#25496;&#21382;&#21490;&#20132;&#36890;&#25968;&#25454;&#20013;&#30340;&#30456;&#20851;&#24615;&#26469;&#39044;&#27979;&#26410;&#26469;&#20132;&#36890;&#65292;&#32780;&#24573;&#30053;&#20102;&#20132;&#36890;&#29983;&#25104;&#30340;&#21407;&#22240;&#65292;&#20363;&#22914;&#20132;&#36890;&#38656;&#27714;&#21644;&#36335;&#24452;&#36873;&#25321;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#23545;&#20110;&#23454;&#26102;&#20915;&#31574;&#26159;&#19981;&#21487;&#38752;&#30340;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#31216;&#20026; TraffNet&#65292;&#35813;&#26694;&#26550;&#20174;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#20013;&#23398;&#20064;&#20132;&#36890;&#37327;&#30340;&#22240;&#26524;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#24322;&#26500;&#22270;&#26469;&#34920;&#31034;&#36947;&#36335;&#32593;&#32476;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#24182;&#20837;&#39044;&#27979;&#25152;&#38656;&#30340;&#20854;&#20182;&#25968;&#25454;&#65292;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#20174;&#32780;&#33021;&#22815;&#39044;&#27979;&#20132;&#36890;&#37327;&#30340;&#22240;&#26524;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Road network digital twins (RNDTs) play a critical role in the development of next-generation intelligent transportation systems, enabling more precise traffic planning and control. To support just-in-time (JIT) decision making, RNDTs require a model that dynamically learns the traffic patterns from online sensor data and generates high-fidelity simulation results. Although current traffic prediction techniques based on graph neural networks have achieved state-of-the-art performance, these techniques only predict future traffic by mining correlations in historical traffic data, disregarding the causes of traffic generation, such as traffic demands and route selection. Therefore, their performance is unreliable for JIT decision making. To fill this gap, we introduce a novel deep learning framework called TraffNet that learns the causality of traffic volume from vehicle trajectory data. First, we use a heterogeneous graph to represent the road network, allowing the model to incorporate 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#26377;&#30417;&#30563;&#30340;&#25513;&#34109;&#30693;&#35782;&#33976;&#39311;&#27169;&#22411;&#65288;SMKD&#65289;&#65292;&#22312;&#23569;&#37327;&#26631;&#27880;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#26631;&#31614;&#20449;&#24687;&#34701;&#20837;&#21040;&#33258;&#33976;&#39311;&#26694;&#26550;&#20013;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;Transformer&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#36807;&#25311;&#21512;&#21644;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.15466</link><description>&lt;p&gt;
&#26377;&#30417;&#30563;&#30340;&#25513;&#34109;&#30693;&#35782;&#33976;&#39311;&#29992;&#20110;&#23569;&#26679;&#26412;Transformer
&lt;/p&gt;
&lt;p&gt;
Supervised Masked Knowledge Distillation for Few-Shot Transformers. (arXiv:2303.15466v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#26377;&#30417;&#30563;&#30340;&#25513;&#34109;&#30693;&#35782;&#33976;&#39311;&#27169;&#22411;&#65288;SMKD&#65289;&#65292;&#22312;&#23569;&#37327;&#26631;&#27880;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#26631;&#31614;&#20449;&#24687;&#34701;&#20837;&#21040;&#33258;&#33976;&#39311;&#26694;&#26550;&#20013;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;Transformer&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#36807;&#25311;&#21512;&#21644;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;Transformer&#21033;&#29992;&#23616;&#37096;&#29305;&#24449;&#25429;&#25417;&#36828;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#65292;&#38024;&#23545;&#23545;&#23569;&#26679;&#26412;&#23398;&#20064;&#36827;&#34892;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21482;&#26377;&#26497;&#23569;&#26631;&#27880;&#26679;&#26412;&#30340;&#25968;&#25454;&#38598;&#26469;&#35828;&#65292;&#30001;&#20110;&#32570;&#23569;CNN&#24335;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;ViT&#23481;&#26131;&#36807;&#25311;&#21512;&#24182;&#19988;&#24615;&#33021;&#20005;&#37325;&#19979;&#38477;&#12290;&#20197;&#21069;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#24037;&#20316;&#65292;&#35201;&#20040;&#36890;&#36807;&#36741;&#21161;&#33258;&#30417;&#30563;&#25439;&#22833;&#26469;&#36991;&#20813;&#36825;&#31181;&#38382;&#39064;&#65292;&#35201;&#20040;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#30340;&#26631;&#31614;&#20449;&#24687;&#26469;&#36991;&#20813;&#12290;&#20294;&#26159;&#33258;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#30340;&#23569;&#26679;&#26412;Transformer&#20043;&#38388;&#30340;&#24046;&#36317;&#20173;&#26410;&#22635;&#34917;&#12290;&#25105;&#20204;&#21463;&#21040;&#26368;&#36817;&#33258;&#30417;&#30563;&#30693;&#35782;&#33976;&#39311;&#21644;&#25513;&#34109;&#22270;&#20687;&#24314;&#27169;&#30340;&#36827;&#23637;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;Supervised Masked Knowledge Distillation&#27169;&#22411;&#65288;SMKD&#65289;&#29992;&#20110;Transformer&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#23558;&#26631;&#31614;&#20449;&#24687;&#34701;&#20837;&#21040;&#33258;&#33976;&#39311;&#26694;&#26550;&#20013;&#12290;&#19982;&#20197;&#21069;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#20801;&#35768;&#31867;&#20869;&#30693;&#35782;&#27969;&#21160;&#65292;&#24182;&#26377;&#25928;&#21033;&#29992;&#30417;&#30563;&#20449;&#21495;&#23545;&#27169;&#22411;&#36755;&#20986;&#36827;&#34892;&#33258;&#28982;&#32422;&#26463;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#20197;&#21069;&#33258;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Transformers (ViTs) emerge to achieve impressive performance on many data-abundant computer vision tasks by capturing long-range dependencies among local features. However, under few-shot learning (FSL) settings on small datasets with only a few labeled data, ViT tends to overfit and suffers from severe performance degradation due to its absence of CNN-alike inductive bias. Previous works in FSL avoid such problem either through the help of self-supervised auxiliary losses, or through the dextile uses of label information under supervised settings. But the gap between self-supervised and supervised few-shot Transformers is still unfilled. Inspired by recent advances in self-supervised knowledge distillation and masked image modeling (MIM), we propose a novel Supervised Masked Knowledge Distillation model (SMKD) for few-shot Transformers which incorporates label information into self-distillation frameworks. Compared with previous self-supervised methods, we allow intra-class kno
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#30340;&#25104;&#23545;Sigmoid&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#35757;&#32451;&#25209;&#37327;&#22823;&#23567;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#20840;&#23616;&#26597;&#30475;&#37197;&#23545;&#30456;&#20284;&#24615;&#36827;&#34892;&#24402;&#19968;&#21270;&#65292;&#20854;&#35757;&#32451;&#20986;&#26469;&#30340;&#27169;&#22411;&#22312;ImageNet&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2303.15343</link><description>&lt;p&gt;
Sigmoid Loss&#29992;&#20110;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Sigmoid Loss for Language Image Pre-Training. (arXiv:2303.15343v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#30340;&#25104;&#23545;Sigmoid&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#35757;&#32451;&#25209;&#37327;&#22823;&#23567;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#20840;&#23616;&#26597;&#30475;&#37197;&#23545;&#30456;&#20284;&#24615;&#36827;&#34892;&#24402;&#19968;&#21270;&#65292;&#20854;&#35757;&#32451;&#20986;&#26469;&#30340;&#27169;&#22411;&#22312;ImageNet&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25104;&#23545;Sigmoid&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#12290;&#19982;&#26631;&#20934;&#30340;&#20855;&#26377;softmax&#24402;&#19968;&#21270;&#30340;&#23545;&#27604;&#23398;&#20064;&#19981;&#21516;&#65292;Sigmoid&#25439;&#22833;&#21482;&#25805;&#20316;&#22270;&#20687;-&#25991;&#26412;&#23545;&#65292;&#19981;&#38656;&#35201;&#20840;&#23616;&#26597;&#30475;&#37197;&#23545;&#30456;&#20284;&#24615;&#20197;&#36827;&#34892;&#24402;&#19968;&#21270;&#12290;Sigmoid&#25439;&#22833;&#21516;&#26102;&#20351;&#25209;&#37327;&#22823;&#23567;&#36827;&#19968;&#27493;&#22686;&#21152;&#65292;&#24182;&#21487;&#22312;&#36739;&#23567;&#30340;&#25209;&#37327;&#22823;&#23567;&#19979;&#34920;&#29616;&#26356;&#22909;&#12290;&#20165;&#20351;&#29992;&#22235;&#20010;TPUv4&#33455;&#29255;&#65292;&#25105;&#20204;&#23601;&#33021;&#22312;4k&#25209;&#37327;&#22823;&#23567;&#19979;&#35757;&#32451;&#20986;&#19968;&#20010;Base CLIP&#27169;&#22411;&#21644;&#22312;20k&#25209;&#37327;&#22823;&#23567;&#19979;&#35757;&#32451;&#20986;&#19968;&#20010;&#22823;&#35268;&#27169;LiT&#27169;&#22411;&#65292;&#21518;&#32773;&#22312;&#20004;&#22825;&#20869;&#23454;&#29616;&#20102;84.5%&#30340;ImageNet&#38646;&#26679;&#26412;&#20934;&#30830;&#29575;&#12290;&#36825;&#31181;&#25439;&#22833;&#20989;&#25968;&#23558;&#25209;&#37327;&#22823;&#23567;&#19982;&#25439;&#22833;&#20989;&#25968;&#20998;&#31163;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#30740;&#31350;&#31034;&#20363;&#19982;&#23545;&#20043;&#38388;&#12289;&#36127;-&#27491;&#20363;&#27604;&#29575;&#30340;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#25209;&#37327;&#22823;&#23567;&#25512;&#21040;&#26497;&#38480;&#65292;&#39640;&#36798;&#19968;&#30334;&#19975;&#65292;&#21457;&#29616;&#25193;&#22823;&#25209;&#37327;&#22823;&#23567;&#30340;&#22909;&#22788;&#24456;&#24555;&#23601;&#20250;&#20943;&#24369;&#65292;32k&#25209;&#37327;&#22823;&#23567;&#24050;&#32463;&#36275;&#22815;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#30740;&#31350;&#33021;&#22815;&#28608;&#21457;&#36827;&#19968;&#27493;&#25506;&#32034;&#22914;&#20309;&#25552;&#39640;&#36136;&#37327;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a simple pairwise sigmoid loss for image-text pre-training. Unlike standard contrastive learning with softmax normalization, the sigmoid loss operates solely on image-text pairs and does not require a global view of the pairwise similarities for normalization. The sigmoid loss simultaneously allows further scaling up the batch size, while also performing better at smaller batch sizes. With only four TPUv4 chips, we can train a Base CLIP model at 4k batch size and a Large LiT model at 20k batch size, the latter achieves 84.5% ImageNet zero-shot accuracy in two days. This disentanglement of the batch size from the loss further allows us to study the impact of examples vs pairs and negative to positive ratio. Finally, we push the batch size to the extreme, up to one million, and find that the benefits of growing batch size quickly diminish, with a more reasonable batch size of 32k being sufficient. We hope our research motivates further explorations in improving the quality and
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20998;&#26512;&#20102;&#20845;&#39033;&#25233;&#37057;&#30151;&#33647;&#29289;&#27835;&#30103;&#30740;&#31350;&#30340;&#25968;&#25454;&#65292;&#24182;&#29983;&#25104;&#20102;&#32467;&#26524;&#23548;&#21521;&#30340;&#24739;&#32773;&#20122;&#32452;&#65292;&#20026;&#24739;&#32773;&#30340;&#20010;&#24615;&#21270;&#27835;&#30103;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2303.15202</link><description>&lt;p&gt;
&#26088;&#22312;&#23454;&#29616;&#32467;&#26524;&#23548;&#21521;&#30340;&#24739;&#32773;&#20122;&#32452;&#65306;&#20845;&#39033;&#25233;&#37057;&#30151;&#27835;&#30103;&#30740;&#31350;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Towards Outcome-Driven Patient Subgroups: A Machine Learning Analysis Across Six Depression Treatment Studies. (arXiv:2303.15202v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15202
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20998;&#26512;&#20102;&#20845;&#39033;&#25233;&#37057;&#30151;&#33647;&#29289;&#27835;&#30103;&#30740;&#31350;&#30340;&#25968;&#25454;&#65292;&#24182;&#29983;&#25104;&#20102;&#32467;&#26524;&#23548;&#21521;&#30340;&#24739;&#32773;&#20122;&#32452;&#65292;&#20026;&#24739;&#32773;&#30340;&#20010;&#24615;&#21270;&#27835;&#30103;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#24230;&#25233;&#37057;&#38556;&#30861;(MDD) &#26159;&#19968;&#31181;&#22810;&#26679;&#24615;&#30142;&#30149;&#65292;&#22823;&#37327;&#30340;&#31070;&#32463;&#29983;&#29289;&#23398;&#22522;&#30784;&#21487;&#33021;&#19982;&#27835;&#30103;&#21453;&#24212;&#30340;&#21464;&#24322;&#24615;&#26377;&#20851;&#12290;&#29702;&#35299;&#36825;&#31181;&#21464;&#24322;&#24615;&#30340;&#26681;&#28304;&#24182;&#39044;&#27979;&#32467;&#26524;&#19968;&#30452;&#26159;&#38590;&#20197;&#23454;&#29616;&#30340;&#12290;&#26426;&#22120;&#23398;&#20064;&#24050;&#32463;&#22312;&#39044;&#27979;MDD&#30340;&#27835;&#30103;&#21453;&#24212;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#20854;&#20013;&#19968;&#39033;&#38480;&#21046;&#26159;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20020;&#24202;&#21487;&#35299;&#37322;&#24615;&#19981;&#36275;&#12290;&#25105;&#20204;&#20351;&#29992;&#24046;&#24322;&#21407;&#22411;&#31070;&#32463;&#32593;&#32476;(DPNN)&#20998;&#26512;&#20102;&#20845;&#20010;&#33647;&#29289;&#27835;&#30103;&#25233;&#37057;&#30151;&#20020;&#24202;&#35797;&#39564;&#30340;&#25968;&#25454;(&#24635;&#25968;n = 5438)&#12290;DPNN&#21487;&#20197;&#27966;&#29983;&#20986;&#21487;&#29992;&#20110;&#29983;&#25104;&#24739;&#32773;&#20122;&#32452;&#30340;&#30149;&#20154;&#21407;&#22411;&#65292;&#21516;&#26102;&#23398;&#20064;&#29983;&#25104;&#24046;&#24322;&#24615;&#27835;&#30103;&#21453;&#24212;&#30340;&#27010;&#29575;&#12290;&#20351;&#29992;&#20020;&#24202;&#21644;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#35757;&#32451;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#31867;&#32531;&#35299;&#24182;&#36755;&#20986;&#20116;&#31181;&#19968;&#32447;&#21333;&#33647;&#30103;&#27861;&#21644;&#19977;&#31181;&#32852;&#21512;&#27835;&#30103;&#30340;&#20010;&#20307;&#32531;&#35299;&#27010;&#29575;&#12290;&#27169;&#22411;&#20351;&#29992;&#30041;&#19968;&#30740;&#31350;&#27861;&#20132;&#21449;&#39564;&#35777;&#12289;&#32622;&#25442;&#27979;&#35797;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#20998;&#26512;&#36827;&#34892;&#39564;&#35777;&#21644;&#20020;&#24202;&#35299;&#37322;&#24615;&#35780;&#20272;&#12290;DPNN&#27169;&#22411;&#20934;&#30830;&#39044;&#27979;&#20102;&#25152;&#26377;&#20845;&#20010;&#30740;&#31350;&#30340;&#32531;&#35299;&#29366;&#20917;&#65292;&#36890;&#36807;&#37492;&#23450;&#19982;&#29305;&#23450;&#27835;&#30103;&#21453;&#24212;&#26356;&#22909;&#30456;&#20851;&#30340;&#24739;&#32773;&#20122;&#32452;&#26469;&#23637;&#31034;&#39640;&#30340;&#20020;&#24202;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#26377;&#21487;&#33021;&#20026;MDD&#24739;&#32773;&#30340;&#20010;&#24615;&#21270;&#27835;&#30103;&#24314;&#35758;&#25552;&#20379;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Major depressive disorder (MDD) is a heterogeneous condition; multiple underlying neurobiological substrates could be associated with treatment response variability. Understanding the sources of this variability and predicting outcomes has been elusive. Machine learning has shown promise in predicting treatment response in MDD, but one limitation has been the lack of clinical interpretability of machine learning models. We analyzed data from six clinical trials of pharmacological treatment for depression (total n = 5438) using the Differential Prototypes Neural Network (DPNN), a neural network model that derives patient prototypes which can be used to derive treatment-relevant patient clusters while learning to generate probabilities for differential treatment response. A model classifying remission and outputting individual remission probabilities for five first-line monotherapies and three combination treatments was trained using clinical and demographic data. Model validity and clin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;LONGNN&#65292;&#23427;&#37319;&#29992;&#21487;&#23398;&#20064;&#27491;&#20132;&#26631;&#20934;&#22522;&#65292;&#24182;&#35299;&#20915;&#20102;&#22266;&#23450;&#22810;&#39033;&#24335;&#22522;&#21644;&#38750;&#24402;&#19968;&#21270;&#22522;&#30784;&#25152;&#24102;&#26469;&#30340;&#32570;&#38519;&#65292;&#32463;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#21508;&#31181;&#22270;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#20248;&#24322;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.13750</link><description>&lt;p&gt;
LONGNN: &#20855;&#26377;&#21487;&#23398;&#20064;&#27491;&#20132;&#26631;&#20934;&#22522;&#30340;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
LONGNN: Spectral GNNs with Learnable Orthonormal Basis. (arXiv:2303.13750v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;LONGNN&#65292;&#23427;&#37319;&#29992;&#21487;&#23398;&#20064;&#27491;&#20132;&#26631;&#20934;&#22522;&#65292;&#24182;&#35299;&#20915;&#20102;&#22266;&#23450;&#22810;&#39033;&#24335;&#22522;&#21644;&#38750;&#24402;&#19968;&#21270;&#22522;&#30784;&#25152;&#24102;&#26469;&#30340;&#32570;&#38519;&#65292;&#32463;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#21508;&#31181;&#22270;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#20248;&#24322;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#37327;&#30340;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26041;&#27861;&#21033;&#29992;&#21487;&#23398;&#20064;&#31995;&#25968;&#30340;&#22810;&#39033;&#24335;&#22522;&#22312;&#35768;&#22810;&#33410;&#28857;&#32423;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#39030;&#32423;&#24615;&#33021;&#12290;&#34429;&#28982;&#24050;&#32463;&#25506;&#32034;&#20102;&#21508;&#31181;&#22810;&#39033;&#24335;&#22522;&#65292;&#20294;&#26159;&#27599;&#31181;&#26041;&#27861;&#37117;&#37319;&#29992;&#20102;&#22266;&#23450;&#30340;&#22810;&#39033;&#24335;&#22522;&#65292;&#21487;&#33021;&#19981;&#26159;&#32473;&#23450;&#22270;&#24418;&#30340;&#26368;&#20339;&#36873;&#25321;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#36825;&#20123;&#26041;&#27861;&#25152;&#35859;&#30340;&#36234;&#30028;&#38382;&#39064;&#65292;&#24182;&#34920;&#26126;&#36825;&#22312;&#23427;&#20204;&#19981;&#22826;&#31995;&#32479;&#21270;&#30340;&#27491;&#21017;&#21270;&#31574;&#30053;&#21644;&#38750;&#24402;&#19968;&#21270;&#22522;&#30784;&#19978;&#26377;&#25152;&#26681;&#28304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#21033;&#29992;&#38597;&#21508;&#27604;&#22810;&#39033;&#24335;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#20855;&#26377;&#21487;&#23398;&#20064;&#27491;&#20132;&#26631;&#20934;&#22522;&#30340;&#35889;GNN&#65292;LON-GNN&#65292;&#24182;&#35777;&#26126;&#20102;&#27491;&#21017;&#21270;&#31995;&#25968;&#29616;&#22312;&#31561;&#25928;&#20110;&#27491;&#21017;&#21270;&#25152;&#23398;&#28388;&#27874;&#20989;&#25968;&#30340;&#33539;&#25968;&#12290;&#25105;&#20204;&#22312;&#22810;&#26679;&#30340;&#22270;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#20197;&#35780;&#20272;LON-GNN&#30340;&#25311;&#21512;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, a plethora of spectral graph neural networks (GNN) methods have utilized polynomial basis with learnable coefficients to achieve top-tier performances on many node-level tasks. Although various kinds of polynomial bases have been explored, each such method adopts a fixed polynomial basis which might not be the optimal choice for the given graph. Besides, we identify the so-called over-passing issue of these methods and show that it is somewhat rooted in their less-principled regularization strategy and unnormalized basis. In this paper, we make the first attempts to address these two issues. Leveraging Jacobi polynomials, we design a novel spectral GNN, LON-GNN, with Learnable OrthoNormal bases and prove that regularizing coefficients becomes equivalent to regularizing the norm of learned filter function now. We conduct extensive experiments on diverse graph datasets to evaluate the fitting and generalization capability of LON-GNN, where the results imply its superiori
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#19981;&#30830;&#23450;&#25968;&#25454;&#20851;&#32852;&#30340;POMDP&#35268;&#21010;&#30340;&#21098;&#26525;&#31639;&#27861;&#65292;&#36890;&#36807;&#23548;&#20986;&#23436;&#25972;&#20551;&#35774;&#38598;&#19982;&#20943;&#26525;&#20551;&#35774;&#23376;&#38598;&#20043;&#38388;&#30340;&#20215;&#20540;&#20989;&#25968;&#36793;&#30028;&#65292;&#24314;&#31435;&#20102;&#20351;&#29992;&#20943;&#26525;&#20551;&#35774;&#23376;&#38598;&#36896;&#25104;&#30340;&#26368;&#22823;&#25439;&#22833;&#30340;&#32039;&#23494;&#19978;&#30028;&#65292;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#33258;&#20027;&#39550;&#39542;&#22330;&#26223;&#20013;&#33021;&#22815;&#26174;&#33879;&#33410;&#30465;&#35745;&#31639;&#26102;&#38388;&#24182;&#20445;&#25345;&#21512;&#29702;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2303.02139</link><description>&lt;p&gt;
&#25968;&#25454;&#20851;&#32852;&#24863;&#30693;&#30340;POMDP&#35268;&#21010;&#19982;&#20551;&#35774;&#21098;&#26525;&#24615;&#33021;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Data Association Aware POMDP Planning with Hypothesis Pruning Performance Guarantees. (arXiv:2303.02139v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02139
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#19981;&#30830;&#23450;&#25968;&#25454;&#20851;&#32852;&#30340;POMDP&#35268;&#21010;&#30340;&#21098;&#26525;&#31639;&#27861;&#65292;&#36890;&#36807;&#23548;&#20986;&#23436;&#25972;&#20551;&#35774;&#38598;&#19982;&#20943;&#26525;&#20551;&#35774;&#23376;&#38598;&#20043;&#38388;&#30340;&#20215;&#20540;&#20989;&#25968;&#36793;&#30028;&#65292;&#24314;&#31435;&#20102;&#20351;&#29992;&#20943;&#26525;&#20551;&#35774;&#23376;&#38598;&#36896;&#25104;&#30340;&#26368;&#22823;&#25439;&#22833;&#30340;&#32039;&#23494;&#19978;&#30028;&#65292;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#33258;&#20027;&#39550;&#39542;&#22330;&#26223;&#20013;&#33021;&#22815;&#26174;&#33879;&#33410;&#30465;&#35745;&#31639;&#26102;&#38388;&#24182;&#20445;&#25345;&#21512;&#29702;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#36816;&#20316;&#30340;&#33258;&#20027;&#20195;&#29702;&#36890;&#24120;&#35201;&#22788;&#29702;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#65292;&#32780;&#36825;&#36890;&#24120;&#34987;&#24314;&#27169;&#20026;&#37096;&#20998;&#21487;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340; POMDP &#27169;&#22411;&#20381;&#36182;&#20110;&#23436;&#20840;&#30693;&#35782;&#35266;&#27979;&#28304;&#30340;&#20551;&#35774;&#65292;&#21363;&#23436;&#20840;&#21487;&#35266;&#27979;&#25968;&#25454;&#20851;&#32852;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35268;&#21010;&#31639;&#27861;&#65292;&#23427;&#32500;&#25252;&#22810;&#20010;&#25968;&#25454;&#20851;&#32852;&#20551;&#35774;&#65292;&#34920;&#31034;&#20026;&#20449;&#24565;&#28151;&#21512;&#65292;&#20854;&#20013;&#27599;&#20010;&#32452;&#20214;&#23545;&#24212;&#20110;&#19981;&#21516;&#30340;&#25968;&#25454;&#20851;&#32852;&#20551;&#35774;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#20551;&#35774;&#25968;&#37327;&#21576;&#25351;&#25968;&#22686;&#38271;&#65292;&#20174;&#32780;&#23548;&#33268;&#26174;&#33879;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#21098;&#26525;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#20855;&#26377;&#19981;&#30830;&#23450;&#25968;&#25454;&#20851;&#32852;&#30340;&#35268;&#21010;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#22522;&#20110;&#23436;&#25972;&#20551;&#35774;&#38598;&#19982;&#22522;&#20110;&#20551;&#35774;&#21098;&#26525;&#23376;&#38598;&#30340;&#20215;&#20540;&#20989;&#25968;&#20043;&#38388;&#23548;&#20986;&#30028;&#38480;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#24314;&#31435;&#20351;&#29992;&#20462;&#21098;&#30340;&#20551;&#35774;&#23376;&#38598;&#25152;&#36896;&#25104;&#30340;&#26368;&#22823;&#25439;&#22833;&#30340;&#32039;&#23494;&#19978;&#30028;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#33258;&#20027;&#39550;&#39542;&#22330;&#26223;&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#35745;&#31639;&#33410;&#30465;&#65292;&#21516;&#26102;&#20445;&#25345;&#21512;&#29702;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous agents that operate in the real world must often deal with partial observability, which is commonly modeled as partially observable Markov decision processes (POMDPs). However, traditional POMDP models rely on the assumption of complete knowledge of the observation source, known as fully observable data association. To address this limitation, we propose a planning algorithm that maintains multiple data association hypotheses, represented as a belief mixture, where each component corresponds to a different data association hypothesis. However, this method can lead to an exponential growth in the number of hypotheses, resulting in significant computational overhead. To overcome this challenge, we introduce a pruning-based approach for planning with ambiguous data associations. Our key contribution is to derive bounds between the value function based on the complete set of hypotheses and the value function based on a pruned-subset of the hypotheses, enabling us to establish a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21464;&#37327;&#21442;&#32771;&#35821;&#20041;&#23398;&#30340;&#35821;&#20041;&#26694;&#26550;&#65292;&#29992;&#20110;&#24314;&#27169;&#22312;&#35859;&#35789;&#21644;&#29289;&#20307;&#20013;&#30340;&#27169;&#31946;&#24615;&#21644;&#22240;&#35832;&#22810;&#22240;&#32032;&#23548;&#33268;&#30340;&#21464;&#24322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.13189</link><description>&lt;p&gt;
&#35859;&#35789;&#21644;&#29289;&#20307;&#30340;&#27169;&#31946;&#24615;
&lt;/p&gt;
&lt;p&gt;
Vagueness in Predicates and Objects. (arXiv:2302.13189v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21464;&#37327;&#21442;&#32771;&#35821;&#20041;&#23398;&#30340;&#35821;&#20041;&#26694;&#26550;&#65292;&#29992;&#20110;&#24314;&#27169;&#22312;&#35859;&#35789;&#21644;&#29289;&#20307;&#20013;&#30340;&#27169;&#31946;&#24615;&#21644;&#22240;&#35832;&#22810;&#22240;&#32032;&#23548;&#33268;&#30340;&#21464;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#20856;&#35821;&#20041;&#23398;&#35748;&#20026;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#21442;&#32771;&#12289;&#35859;&#35789;&#21644;&#37327;&#35789;&#24314;&#27169;&#20026;&#22266;&#23450;&#22495;&#20869;&#31934;&#30830;&#21442;&#32771;&#23545;&#35937;&#30340;&#38598;&#21512;&#12290;&#38750;&#36923;&#36753;&#26415;&#35821;&#21644;&#37327;&#35789;&#21487;&#20197;&#36890;&#36807;&#35813;&#22495;&#20013;&#20803;&#32032;&#21644;&#23376;&#38598;&#30340;&#30452;&#25509;&#35299;&#37322;&#26469;&#35299;&#37322;&#12290;&#20294;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#31934;&#30830;&#35859;&#35789;&#21644;&#23545;&#35937;&#30340;&#32463;&#20856;&#35266;&#28857;&#27010;&#25324;&#20026;&#32771;&#34385;&#35832;&#22914;&#27169;&#31946;&#24615;&#12289;&#35821;&#22659;&#21644;&#23450;&#20041;&#25110;&#35266;&#28857;&#22810;&#26679;&#24615;&#31561;&#22240;&#32032;&#30340;&#21547;&#20041;&#30340;&#21487;&#21464;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21464;&#37327;&#21442;&#32771;&#35821;&#20041;&#23398;&#30340;&#35821;&#20041;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#36866;&#24212;&#28041;&#21450;&#35859;&#35789;&#21644;&#23545;&#35937;&#30340;&#22810;&#31181;&#21464;&#24322;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classical semantics assumes that one can model reference, predication and quantification with respect to a fixed domain of precise referent objects. Non-logical terms and quantification are then interpreted directly in terms of elements and subsets of this domain. We explore ways to generalise this classical picture of precise predicates and objects to account for variability of meaning due to factors such as vagueness, context and diversity of definitions or opinions. Both names and predicative expressions can be given either multiple semantic referents or be associated with semantic referents that incorporate some model of variability. We present a semantic framework, Variable Reference Semantics, that can accommodate several modes of variability in relation to both predicates and objects.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#24120;&#29992;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#26041;&#27861;&#65292;&#25506;&#31350;&#19981;&#21516;&#35780;&#20272;&#24230;&#37327;&#19979;&#30340;&#34920;&#29616;&#20197;&#21450;&#35780;&#20272;&#26041;&#27861;&#20043;&#38388;&#30340;&#27604;&#36739;&#65292;&#21457;&#29616;&#26041;&#27861;&#30340;&#34920;&#29616;&#32463;&#24120;&#19981;&#19968;&#33268;&#19988;&#36873;&#25321;&#35780;&#20272;&#24230;&#37327;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2302.10764</link><description>&lt;p&gt;
&#20851;&#20110;&#35270;&#35273;&#35299;&#37322;&#23450;&#37327;&#35780;&#20272;&#30340;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
On The Coherence of Quantitative Evaluation of Visual Explanations. (arXiv:2302.10764v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#24120;&#29992;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#26041;&#27861;&#65292;&#25506;&#31350;&#19981;&#21516;&#35780;&#20272;&#24230;&#37327;&#19979;&#30340;&#34920;&#29616;&#20197;&#21450;&#35780;&#20272;&#26041;&#27861;&#20043;&#38388;&#30340;&#27604;&#36739;&#65292;&#21457;&#29616;&#26041;&#27861;&#30340;&#34920;&#29616;&#32463;&#24120;&#19981;&#19968;&#33268;&#19988;&#36873;&#25321;&#35780;&#20272;&#24230;&#37327;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36890;&#36807;&#35270;&#35273;&#35299;&#37322;&#26469;&#35777;&#26126;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#30340;&#26041;&#27861;&#24471;&#21040;&#20102;&#22686;&#24378;&#21457;&#23637;&#12290;&#36825;&#20123;&#35299;&#37322;&#36890;&#24120;&#37319;&#29992;&#28909;&#22270;&#30340;&#24418;&#24335;&#65292;&#20026;&#36755;&#20837;&#22270;&#20687;&#30340;&#27599;&#20010;&#20687;&#32032;&#20998;&#37197;&#19968;&#20010;&#26174;&#33879;&#24615;&#20540;&#65292;&#34920;&#31034;&#20687;&#32032;&#23545;&#26631;&#31614;&#39044;&#27979;&#30340;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#31181;&#35299;&#37322;&#30340;&#36136;&#37327;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35780;&#20272;&#26041;&#27861;&#12290;&#19968;&#20123;&#36825;&#26679;&#30340;&#35780;&#20272;&#26041;&#27861;&#20381;&#36182;&#20110;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20294;&#36825;&#26679;&#20250;&#24341;&#20837;&#22312;&#26356;&#29616;&#23454;&#30340;&#24773;&#26223;&#19979;&#36866;&#29992;&#24615;&#30340;&#26377;&#38480;&#20445;&#35777;&#12290;&#21478;&#19968;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#23458;&#35266;&#35780;&#20272;&#30340;&#24230;&#37327;&#12290;&#20294;&#26159;&#26377;&#20851;&#36825;&#20123;&#35780;&#20272;&#26041;&#27861;&#30340;&#25191;&#34892;&#27700;&#24179;&#30340;&#19981;&#30830;&#23450;&#24615;&#24456;&#22823;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23545;ImageNet-1k&#39564;&#35777;&#38598;&#30340;&#19968;&#20010;&#23376;&#38598;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#20351;&#29992;&#22810;&#20010;&#35780;&#20272;&#24230;&#37327;&#26469;&#35780;&#20272;&#19981;&#21516;&#30340;&#24120;&#29992;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#30830;&#23450;&#19981;&#21516;&#30340;&#35780;&#20272;&#35774;&#32622;&#19979;&#21508;&#20010;&#26041;&#27861;&#30340;&#34920;&#29616;&#22914;&#20309;&#65292;&#20197;&#21450;&#19981;&#21516;&#30340;&#35780;&#20272;&#26041;&#27861;&#20043;&#38388;&#30340;&#27604;&#36739;&#22914;&#20309;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#25152;&#20351;&#29992;&#30340;&#35780;&#20272;&#24230;&#37327;&#19978;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#34920;&#29616;&#32463;&#24120;&#26159;&#19981;&#19968;&#33268;&#30340;&#65292;&#32780;&#19988;&#22312;&#35266;&#23519;&#30340;&#34920;&#29616;&#20013;&#65292;&#36873;&#25321;&#35780;&#20272;&#24230;&#37327;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have shown an increased development of methods for justifying the predictions of neural networks through visual explanations. These explanations usually take the form of heatmaps which assign a saliency (or relevance) value to each pixel of the input image that expresses how relevant the pixel is for the prediction of a label.  Complementing this development, evaluation methods have been proposed to assess the "goodness" of such explanations. On the one hand, some of these methods rely on synthetic datasets. However, this introduces the weakness of having limited guarantees regarding their applicability on more realistic settings. On the other hand, some methods rely on metrics for objective evaluation. However the level to which some of these evaluation methods perform with respect to each other is uncertain.  Taking this into account, we conduct a comprehensive study on a subset of the ImageNet-1k validation set where we evaluate a number of different commonly-used expla
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#21644;&#21457;&#23637;&#21382;&#31243;&#65292;&#21253;&#25324;&#23427;&#20204;&#30340;&#26550;&#26500;&#12289;&#22521;&#35757;&#30446;&#26631;&#12289;&#39044;&#22521;&#35757;&#20219;&#21153;&#12289;&#24494;&#35843;&#31574;&#30053;&#21644;&#35780;&#20272;&#12290;&#21516;&#26102;&#65292;&#35752;&#35770;&#20102;&#20854;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2302.09419</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#32508;&#36848;&#65306;&#20174;BERT&#21040;ChatGPT&#30340;&#21382;&#31243;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT. (arXiv:2302.09419v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#21644;&#21457;&#23637;&#21382;&#31243;&#65292;&#21253;&#25324;&#23427;&#20204;&#30340;&#26550;&#26500;&#12289;&#22521;&#35757;&#30446;&#26631;&#12289;&#39044;&#22521;&#35757;&#20219;&#21153;&#12289;&#24494;&#35843;&#31574;&#30053;&#21644;&#35780;&#20272;&#12290;&#21516;&#26102;&#65292;&#35752;&#35770;&#20102;&#20854;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;(PFMs)&#34987;&#35748;&#20026;&#26159;&#21508;&#31181;&#19981;&#21516;&#25968;&#25454;&#27169;&#24577;&#19979;&#28216;&#20219;&#21153;&#30340;&#22522;&#30784;&#12290;PFM(&#20363;&#22914;BERT&#12289;ChatGPT&#21644;GPT-4)&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20026;&#21508;&#31181;&#19979;&#28216;&#24212;&#29992;&#25552;&#20379;&#20102;&#21512;&#29702;&#30340;&#21442;&#25968;&#21021;&#22987;&#21270;&#12290;BERT&#20174;&#36716;&#25442;&#22120;&#20013;&#23398;&#20064;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#65292;&#36825;&#20123;&#27169;&#22411;&#20316;&#20026;&#19978;&#19979;&#25991;&#35821;&#35328;&#27169;&#22411;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#31867;&#20284;&#22320;&#65292;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;(GPT)&#26041;&#27861;&#37319;&#29992;&#36716;&#25442;&#22120;&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#24182;&#37319;&#29992;&#33258;&#22238;&#24402;&#33539;&#24335;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#26368;&#36817;&#65292;ChatGPT&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#23637;&#29616;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#25104;&#21151;&#65292;&#23427;&#37319;&#29992;&#33258;&#22238;&#24402;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#36827;&#34892;&#38646;&#23556;&#20987;&#25110;&#23569;&#23556;&#20987;&#25552;&#31034;&#12290;PFM&#30340;&#21331;&#36234;&#25104;&#23601;&#20026;&#21508;&#31181;AI&#39046;&#22495;&#24102;&#26469;&#20102;&#37325;&#22823;&#31361;&#30772;&#12290;&#35768;&#22810;&#30740;&#31350;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#23545;&#26356;&#26032;&#35843;&#26597;&#30340;&#38656;&#27714;&#12290;&#26412;&#30740;&#31350;&#20840;&#38754;&#22238;&#39038;&#20102;PFMs&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#23427;&#20204;&#30340;&#26550;&#26500;&#12289;&#22521;&#35757;&#30446;&#26631;&#12289;&#39044;&#22521;&#35757;&#20219;&#21153;&#12289;&#24494;&#35843;&#31574;&#30053;&#21644;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;PFMs&#30340;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#28508;&#22312;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained Foundation Models (PFMs) are regarded as the foundation for various downstream tasks with different data modalities. A PFM (e.g., BERT, ChatGPT, and GPT-4) is trained on large-scale data which provides a reasonable parameter initialization for a wide range of downstream applications. BERT learns bidirectional encoder representations from Transformers, which are trained on large datasets as contextual language models. Similarly, the generative pretrained transformer (GPT) method employs Transformers as the feature extractor and is trained using an autoregressive paradigm on large datasets. Recently, ChatGPT shows promising success on large language models, which applies an autoregressive language model with zero shot or few shot prompting. The remarkable achievements of PFM have brought significant breakthroughs to various fields of AI. Numerous studies have proposed different methods, raising the demand for an updated survey. This study provides a comprehensive review of rec
&lt;/p&gt;</description></item><item><title>AutoFed &#26159;&#19968;&#31181;&#25903;&#25345;&#24322;&#26500;&#24863;&#30693;&#32852;&#37030;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#20805;&#20998;&#21033;&#29992;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#19978;&#30340;&#22810;&#27169;&#24577;&#20256;&#24863;&#25968;&#25454;&#65292;&#24182;&#20197;&#27492;&#23454;&#29616;&#31283;&#20581;&#30340;&#33258;&#21160;&#39550;&#39542;&#12290;&#23427;&#36890;&#36807;&#20266;&#26631;&#31614;&#21644;&#33258;&#32534;&#30721;&#22120;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#22312;&#35299;&#20915;&#20998;&#24067;&#24335;AVs&#19978;&#20855;&#26377;&#24322;&#26500;&#25968;&#25454;&#30340;&#25361;&#25112;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2302.08646</link><description>&lt;p&gt;
AutoFed&#65306;&#29992;&#20110;&#31283;&#20581;&#33258;&#21160;&#39550;&#39542;&#30340;&#24322;&#26500;&#24863;&#30693;&#32852;&#37030;&#22810;&#27169;&#24577;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
AutoFed: Heterogeneity-Aware Federated Multimodal Learning for Robust Autonomous Driving. (arXiv:2302.08646v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08646
&lt;/p&gt;
&lt;p&gt;
AutoFed &#26159;&#19968;&#31181;&#25903;&#25345;&#24322;&#26500;&#24863;&#30693;&#32852;&#37030;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#20805;&#20998;&#21033;&#29992;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#19978;&#30340;&#22810;&#27169;&#24577;&#20256;&#24863;&#25968;&#25454;&#65292;&#24182;&#20197;&#27492;&#23454;&#29616;&#31283;&#20581;&#30340;&#33258;&#21160;&#39550;&#39542;&#12290;&#23427;&#36890;&#36807;&#20266;&#26631;&#31614;&#21644;&#33258;&#32534;&#30721;&#22120;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#22312;&#35299;&#20915;&#20998;&#24067;&#24335;AVs&#19978;&#20855;&#26377;&#24322;&#26500;&#25968;&#25454;&#30340;&#25361;&#25112;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#20013;&#65292;&#22522;&#20110;&#36710;&#36733;&#20256;&#24863;&#22120;&#65288;&#22914;&#28608;&#20809;&#38647;&#36798;&#12289;&#38647;&#36798;&#21644;&#25668;&#20687;&#22836;&#65289;&#30340;&#30446;&#26631;&#26816;&#27979;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#32780;&#36825;&#20123;&#20256;&#24863;&#22120;&#22312;&#27169;&#24577;&#19978;&#20114;&#20026;&#34917;&#20805;&#12290;&#23613;&#31649;&#20247;&#24863;&#30693;&#25216;&#26415;&#21487;&#33021;&#28508;&#22312;&#22320;&#21033;&#29992;&#36825;&#20123;&#20256;&#24863;&#22120;&#65288;&#25968;&#37327;&#24040;&#22823;&#65289;&#26469;&#24471;&#20986;&#26356;&#20840;&#38754;&#30340;&#30693;&#35782;&#65292;&#20294;&#26159;&#65292;\textit{&#32852;&#37030;&#23398;&#20064;}&#65288;FL&#65289;&#20284;&#20046;&#26159;&#36798;&#21040;&#36825;&#20010;&#28508;&#21147;&#30340;&#24517;&#35201;&#24037;&#20855;&#65306;&#23427;&#20351;&#24471;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#33021;&#22815;&#22312;&#19981;&#26174;&#24335;&#20849;&#20139;&#21407;&#22987;&#20256;&#24863;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22810;&#27169;&#24577;&#20256;&#24863;&#22120;&#24341;&#20837;&#20102;&#20998;&#24067;&#24335;AVs&#65288;&#22914;&#26631;&#31614;&#25968;&#37327;&#20559;&#24046;&#21644;&#19981;&#21516;&#24418;&#24335;&#65289;&#30340;&#21508;&#31181;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#32473;&#26377;&#25928;FL&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AutoFed&#20316;&#20026;&#19968;&#31181;&#24322;&#26500;&#24863;&#30693;FL&#26694;&#26550;&#65292;&#20805;&#20998;&#21033;&#29992;AVs&#19978;&#30340;&#22810;&#27169;&#24577;&#20256;&#24863;&#25968;&#25454;&#65292;&#20174;&#32780;&#23454;&#29616;&#31283;&#20581;&#30340;&#33258;&#21160;&#39550;&#39542;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#20266;&#26631;&#31614;&#26469;&#36991;&#20813;&#38169;&#35823;&#22320;&#23558;&#26410;&#26631;&#35760;&#30340;&#23545;&#35937;&#35270;&#20026;&#32972;&#26223;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#36890;&#29992;&#29305;&#24449;&#34920;&#31034;&#12290;&#20511;&#21161;&#36825;&#20123;&#25216;&#26415;&#65292;AutoFed&#21487;&#20197;&#25104;&#21151;&#22320;&#32858;&#21512;&#26469;&#33258;&#20855;&#26377;&#21508;&#31181;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#20998;&#24067;&#24335;AVs&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#24182;&#27604;&#20256;&#32479;FL&#21644;&#38750;FL&#26041;&#27861;&#23454;&#29616;&#26356;&#22909;&#30340;&#29289;&#20307;&#26816;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Object detection with on-board sensors (e.g., lidar, radar, and camera) play a crucial role in autonomous driving (AD), and these sensors complement each other in modalities. While crowdsensing may potentially exploit these sensors (of huge quantity) to derive more comprehensive knowledge, \textit{federated learning} (FL) appears to be the necessary tool to reach this potential: it enables autonomous vehicles (AVs) to train machine learning models without explicitly sharing raw sensory data. However, the multimodal sensors introduce various data heterogeneity across distributed AVs (e.g., label quantity skews and varied modalities), posing critical challenges to effective FL. To this end, we present AutoFed as a heterogeneity-aware FL framework to fully exploit multimodal sensory data on AVs and thus enable robust AD. Specifically, we first propose a novel model leveraging pseudo-labeling to avoid mistakenly treating unlabeled objects as the background. We also propose an autoencoder-b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;OpenAI&#30340;GPT3.5&#27169;&#22411;&#37325;&#26032;&#22797;&#21046;&#20102;Many Labs 2&#22797;&#21046;&#39033;&#30446;&#20013;&#30340;14&#39033;&#30740;&#31350;&#65292;&#20854;&#20013;8&#39033;&#30740;&#31350;&#30340;&#32467;&#26524;&#34987;&#25104;&#21151;&#22797;&#21046;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21097;&#19979;&#30340;6&#39033;&#30740;&#31350;&#65292;GPT3.5&#20197;&#26497;&#20854;&#39044;&#23450;&#30340;&#26041;&#24335;&#22238;&#31572;&#20102;&#35843;&#26597;&#38382;&#39064;&#65292;&#23548;&#33268;&#26080;&#27861;&#20998;&#26512;&#36825;&#20123;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2302.07267</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#24515;&#29702;&#23398;&#20013;&#30340;&#8220;&#27491;&#30830;&#31572;&#26696;&#8221;
&lt;/p&gt;
&lt;p&gt;
"Correct answers" from the psychology of artificial intelligence. (arXiv:2302.07267v3 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;OpenAI&#30340;GPT3.5&#27169;&#22411;&#37325;&#26032;&#22797;&#21046;&#20102;Many Labs 2&#22797;&#21046;&#39033;&#30446;&#20013;&#30340;14&#39033;&#30740;&#31350;&#65292;&#20854;&#20013;8&#39033;&#30740;&#31350;&#30340;&#32467;&#26524;&#34987;&#25104;&#21151;&#22797;&#21046;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21097;&#19979;&#30340;6&#39033;&#30740;&#31350;&#65292;GPT3.5&#20197;&#26497;&#20854;&#39044;&#23450;&#30340;&#26041;&#24335;&#22238;&#31572;&#20102;&#35843;&#26597;&#38382;&#39064;&#65292;&#23548;&#33268;&#26080;&#27861;&#20998;&#26512;&#36825;&#20123;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper replicates 14 studies from the Many Labs 2 replication project with OpenAI's text-davinci-003 model, and successfully replicates the results of 8 studies. However, for the remaining 6 studies, GPT3.5 answered survey questions in an extremely predetermined way, making it impossible to analyze these studies.
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#24050;&#32463;&#22823;&#22823;&#22686;&#24378;&#12290;&#36825;&#31181;AI&#31995;&#32479;&#30340;&#19968;&#20010;&#25552;&#20986;&#30340;&#24212;&#29992;&#26159;&#25903;&#25345;&#31038;&#20250;&#21644;&#35748;&#30693;&#31185;&#23398;&#20013;&#30340;&#25968;&#25454;&#25910;&#38598;&#65292;&#30446;&#21069;&#23436;&#32654;&#30340;&#23454;&#39564;&#25511;&#21046;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#32780;&#22823;&#35268;&#27169;&#12289;&#20195;&#34920;&#24615;&#25968;&#25454;&#38598;&#30340;&#25910;&#38598;&#36890;&#24120;&#26159;&#26114;&#36149;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;OpenAI&#30340;text-davinci-003&#27169;&#22411;&#65288;&#20439;&#31216;GPT3.5&#65289;&#37325;&#26032;&#22797;&#21046;&#20102;Many Labs 2&#22797;&#21046;&#39033;&#30446;&#20013;&#30340;14&#39033;&#30740;&#31350;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#27599;&#39033;&#30740;&#31350;&#30340;&#35843;&#26597;&#20316;&#20026;&#25991;&#26412;&#36755;&#20837;&#65292;&#20174;GPT3.5&#30340;&#40664;&#35748;&#35774;&#32622;&#20013;&#25910;&#38598;&#20102;&#21709;&#24212;&#12290;&#22312;&#25105;&#20204;&#21487;&#20197;&#20998;&#26512;&#30340;&#20843;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30340;GPT&#26679;&#26412;&#22797;&#21046;&#20102;&#21407;&#22987;&#32467;&#26524;&#30340;37.5%&#20197;&#21450;Many Labs 2&#32467;&#26524;&#30340;37.5%&#12290;&#20986;&#20046;&#24847;&#26009;&#30340;&#26159;&#65292;&#25105;&#20204;&#26080;&#27861;&#20687;&#39044;&#20808;&#27880;&#20876;&#30340;&#35745;&#21010;&#37027;&#26679;&#20998;&#26512;&#21097;&#19979;&#30340;&#20845;&#39033;&#30740;&#31350;&#12290;&#36825;&#26159;&#22240;&#20026;&#23545;&#20110;&#36825;&#20845;&#39033;&#30740;&#31350;&#20013;&#30340;&#27599;&#19968;&#39033;&#65292;GPT3.5&#20197;&#26497;&#20854;&#39044;&#23450;&#30340;&#26041;&#24335;&#22238;&#31572;&#20102;&#35843;&#26597;&#38382;&#39064;&#65288;&#26080;&#35770;&#26159;&#22240;&#21464;&#37327;&#36824;&#26159;&#26465;&#20214;&#21464;&#37327;&#65289;&#65306;&#19968;&#20010;&#26410;&#30693;&#30340;
&lt;/p&gt;
&lt;p&gt;
Large Language Models have vastly grown in capabilities. One proposed application of such AI systems is to support data collection in the social and cognitive sciences, where perfect experimental control is currently unfeasible and the collection of large, representative datasets is generally expensive. In this paper, we re-replicate 14 studies from the Many Labs 2 replication project with OpenAI's text-davinci-003 model, colloquially known as GPT3.5. We collected responses from the default setting of GPT3.5 by inputting each study's survey as text. Among the eight studies we could analyse, our GPT sample replicated 37.5% of the original results as well as 37.5% of the Many Labs 2 results. Unexpectedly, we could not analyse the remaining six studies as we had planned in our pre-registration. This was because for each of these six studies, GPT3.5 answered at least one of the survey questions (either a dependent variable or a condition variable) in an extremely predetermined way: an unex
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;LLM-Planner&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#23454;&#20307;&#20195;&#29702;&#36827;&#34892;&#23569;&#26679;&#26412;&#35268;&#21010;&#65292;&#20197;&#23454;&#20307;&#20195;&#29702;&#30446;&#21069;&#25152;&#22312;&#30340;&#29615;&#22659;&#20026;&#22522;&#30784;&#65292;&#22686;&#24378;LLMs&#29983;&#25104;&#21644;&#26356;&#26032;&#35745;&#21010;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#22312;&#22810;&#20219;&#21153;&#21644;&#24555;&#36895;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#36890;&#29992;&#20195;&#29702;&#30340;&#24320;&#21457;&#20013;&#20855;&#26377;&#24456;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2212.04088</link><description>&lt;p&gt;
LLM-Planner: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23569;&#26679;&#26412;&#23454;&#20307;&#20195;&#29702;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models. (arXiv:2212.04088v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;LLM-Planner&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#23454;&#20307;&#20195;&#29702;&#36827;&#34892;&#23569;&#26679;&#26412;&#35268;&#21010;&#65292;&#20197;&#23454;&#20307;&#20195;&#29702;&#30446;&#21069;&#25152;&#22312;&#30340;&#29615;&#22659;&#20026;&#22522;&#30784;&#65292;&#22686;&#24378;LLMs&#29983;&#25104;&#21644;&#26356;&#26032;&#35745;&#21010;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#22312;&#22810;&#20219;&#21153;&#21644;&#24555;&#36895;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#36890;&#29992;&#20195;&#29702;&#30340;&#24320;&#21457;&#20013;&#20855;&#26377;&#24456;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#35268;&#21010;&#22120;&#65292;&#35753;&#23454;&#20307;&#20195;&#29702;&#21487;&#20197;&#25353;&#29031;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#23436;&#25104;&#22312;&#35270;&#35273;&#24863;&#30693;&#29615;&#22659;&#20013;&#30340;&#22797;&#26434;&#20219;&#21153;&#12290;&#29616;&#26377;&#26041;&#27861;&#30340;&#39640;&#25968;&#25454;&#25104;&#26412;&#21644;&#20302;&#26679;&#26412;&#25928;&#29575;&#38459;&#30861;&#20102;&#22810;&#20219;&#21153;&#21644;&#24555;&#36895;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#36890;&#29992;&#20195;&#29702;&#30340;&#24320;&#21457;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;LLM-Planner&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#23454;&#20307;&#20195;&#29702;&#36827;&#34892;&#23569;&#26679;&#26412;&#35268;&#21010;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#20307;&#20195;&#29702;&#30446;&#21069;&#25152;&#22312;&#30340;&#29615;&#22659;&#20026;&#22522;&#30784;&#65292;&#22686;&#24378;LLMs&#29983;&#25104;&#21644;&#26356;&#26032;&#35745;&#21010;&#12290;&#22312;ALFRED&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#21462;&#24471;&#38750;&#24120;&#26377;&#31454;&#20105;&#21147;&#30340;&#23569;&#26679;&#26412;&#24615;&#33021;&#65306;&#23613;&#31649;&#20351;&#29992;&#30340;&#37197;&#23545;&#35757;&#32451;&#25968;&#25454;&#19981;&#21040;0.5&#65285;&#65292;LLM-Planner&#30340;&#34920;&#29616;&#19982;&#20351;&#29992;&#23436;&#25972;&#35757;&#32451;&#25968;&#25454;&#35757;&#32451;&#30340;&#26368;&#26032;&#22522;&#32447;&#30456;&#24403;&#12290;&#29616;&#26377;&#26041;&#27861;&#20960;&#20046;&#26080;&#27861;&#23436;&#25104;&#20219;&#20309;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study focuses on using large language models (LLMs) as a planner for embodied agents that can follow natural language instructions to complete complex tasks in a visually-perceived environment. The high data cost and poor sample efficiency of existing methods hinders the development of versatile agents that are capable of many tasks and can learn new tasks quickly. In this work, we propose a novel method, LLM-Planner, that harnesses the power of large language models to do few-shot planning for embodied agents. We further propose a simple but effective way to enhance LLMs with physical grounding to generate and update plans that are grounded in the current environment. Experiments on the ALFRED dataset show that our method can achieve very competitive few-shot performance: Despite using less than 0.5% of paired training data, LLM-Planner achieves competitive performance with recent baselines that are trained using the full training data. Existing methods can barely complete any ta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MHCCL&#30340;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#23398;&#20064;&#35821;&#20041;&#20016;&#23500;&#30340;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#23618;&#27425;&#32858;&#31867;&#32467;&#26500;&#20013;&#30340;&#22810;&#31890;&#24230;&#20449;&#24687;&#26469;&#28388;&#38500;&#34394;&#20551;&#36127;&#26679;&#26412;&#21644;&#34917;&#20805;&#27491;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2212.01141</link><description>&lt;p&gt;
MHCCL&#65306;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#30340;&#23618;&#27425;&#25513;&#34109;&#32858;&#31867;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MHCCL: Masked Hierarchical Cluster-Wise Contrastive Learning for Multivariate Time Series. (arXiv:2212.01141v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MHCCL&#30340;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#23398;&#20064;&#35821;&#20041;&#20016;&#23500;&#30340;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#23618;&#27425;&#32858;&#31867;&#32467;&#26500;&#20013;&#30340;&#22810;&#31890;&#24230;&#20449;&#24687;&#26469;&#28388;&#38500;&#34394;&#20551;&#36127;&#26679;&#26412;&#21644;&#34917;&#20805;&#27491;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#21407;&#22987;&#26080;&#26631;&#31614;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#23398;&#20064;&#35821;&#20041;&#20016;&#23500;&#30340;&#34920;&#31034;&#23545;&#20110;&#20998;&#31867;&#21644;&#39044;&#27979;&#31561;&#19979;&#28216;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#23545;&#27604;&#23398;&#20064;&#26368;&#36817;&#23637;&#31034;&#20102;&#22312;&#32570;&#20047;&#19987;&#23478;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#29420;&#31435;&#22788;&#29702;&#27599;&#20010;&#23454;&#20363;&#65292;&#23548;&#33268;&#20849;&#20139;&#30456;&#21516;&#35821;&#20041;&#30340;&#20551;&#36127;&#26679;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MHCCL&#65292;&#19968;&#31181;&#23618;&#27425;&#25513;&#34109;&#32858;&#31867;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#30001;&#22810;&#20010;&#28508;&#22312;&#20998;&#21306;&#32452;&#25104;&#30340;&#23618;&#27425;&#32467;&#26500;&#33719;&#24471;&#30340;&#35821;&#20041;&#20449;&#24687;&#26469;&#20026;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#12290;&#21463;&#21040;&#32454;&#31890;&#24230;&#32858;&#31867;&#20445;&#30041;&#26356;&#39640;&#32431;&#24230;&#65292;&#32780;&#31895;&#31890;&#24230;&#32858;&#31867;&#21453;&#26144;&#26356;&#39640;&#32423;&#21035;&#35821;&#20041;&#30340;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21521;&#19979;&#25513;&#34109;&#31574;&#30053;&#65292;&#36890;&#36807;&#32467;&#21512;&#32858;&#31867;&#23618;&#27425;&#32467;&#26500;&#20013;&#30340;&#22810;&#31890;&#24230;&#20449;&#24687;&#65292;&#36807;&#28388;&#25481;&#34394;&#20551;&#36127;&#38754;&#23454;&#20363;&#24182;&#34917;&#20805;&#27491;&#38754;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning semantic-rich representations from raw unlabeled time series data is critical for downstream tasks such as classification and forecasting. Contrastive learning has recently shown its promising representation learning capability in the absence of expert annotations. However, existing contrastive approaches generally treat each instance independently, which leads to false negative pairs that share the same semantics. To tackle this problem, we propose MHCCL, a Masked Hierarchical Cluster-wise Contrastive Learning model, which exploits semantic information obtained from the hierarchical structure consisting of multiple latent partitions for multivariate time series. Motivated by the observation that fine-grained clustering preserves higher purity while coarse-grained one reflects higher-level semantics, we propose a novel downward masking strategy to filter out fake negatives and supplement positives by incorporating the multi-granularity information from the clustering hierarchy
&lt;/p&gt;</description></item><item><title>CODA-Prompt&#26159;&#19968;&#31181;&#22522;&#20110;&#20998;&#35299;&#27880;&#24847;&#21147;&#25552;&#31034;&#65292;&#26080;&#38656;&#37325;&#22797;&#35757;&#32451;&#21363;&#21487;&#36830;&#32493;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#25552;&#31034;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2211.13218</link><description>&lt;p&gt;
CODA-Prompt&#65306;&#22522;&#20110;&#20998;&#35299;&#27880;&#24847;&#21147;&#25552;&#31034;&#30340;&#26080;&#37325;&#35757;&#32451;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CODA-Prompt: COntinual Decomposed Attention-based Prompting for Rehearsal-Free Continual Learning. (arXiv:2211.13218v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13218
&lt;/p&gt;
&lt;p&gt;
CODA-Prompt&#26159;&#19968;&#31181;&#22522;&#20110;&#20998;&#35299;&#27880;&#24847;&#21147;&#25552;&#31034;&#65292;&#26080;&#38656;&#37325;&#22797;&#35757;&#32451;&#21363;&#21487;&#36830;&#32493;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#25552;&#31034;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#22312;&#23398;&#20064;&#19981;&#26029;&#21464;&#21270;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#26032;&#27010;&#24565;&#26102;&#23481;&#26131;&#20135;&#29983;&#25152;&#35859;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#12290;&#35299;&#20915;&#36825;&#20010;&#36830;&#32493;&#23398;&#20064;&#38382;&#39064;&#30340;&#20856;&#22411;&#26041;&#27861;&#38656;&#35201;&#23545;&#20808;&#21069;&#24050;&#32463;&#35265;&#36807;&#30340;&#25968;&#25454;&#36827;&#34892;&#22823;&#37327;&#30340;&#37325;&#22797;&#35757;&#32451;&#65292;&#36825;&#22686;&#21152;&#20102;&#20869;&#23384;&#25104;&#26412;&#24182;&#21487;&#33021;&#36829;&#21453;&#25968;&#25454;&#38544;&#31169;&#12290;&#26368;&#36817;&#65292;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35270;&#35273;&#21464;&#25442;&#22120;&#27169;&#22411;&#30340;&#20986;&#29616;&#20351;&#25552;&#31034;&#26041;&#27861;&#25104;&#20026;&#19968;&#31181;&#26367;&#20195;&#25968;&#25454;&#37325;&#22797;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#20381;&#38752;&#20851;&#38190;&#26597;&#35810;&#26426;&#21046;&#29983;&#25104;&#25552;&#31034;&#65292;&#24182;&#24050;&#34987;&#21457;&#29616;&#22312;&#24050;&#32463;&#24314;&#31435;&#30340;&#26080;&#37325;&#35757;&#32451;&#36830;&#32493;&#23398;&#20064;&#35774;&#32622;&#20013;&#39640;&#24230;&#25269;&#25239;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#20851;&#38190;&#26426;&#21046;&#27809;&#26377;&#19982;&#20219;&#21153;&#24207;&#21015;&#19968;&#36215;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#20250;&#23548;&#33268;&#23427;&#20204;&#30340;&#21487;&#22609;&#24615;&#38477;&#20302;&#65292;&#20174;&#32780;&#29306;&#29298;&#26032;&#20219;&#21153;&#20934;&#30830;&#24615;&#65292;&#24182;&#26080;&#27861;&#20174;&#25193;&#23637;&#30340;&#21442;&#25968;&#23481;&#37327;&#20013;&#21463;&#30410;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;CODA-Prompt&#65292;&#23427;&#20351;&#29992;&#20998;&#35299;&#27880;&#24847;&#21147;&#25552;&#31034;&#26426;&#21046;&#32467;&#21512;&#33976;&#39311;&#25439;&#22833;&#26469;&#35757;&#32451;&#25552;&#31034;&#32452;&#20214;&#65292;&#20174;&#32780;&#23454;&#29616;&#19982;&#20219;&#21153;&#24207;&#21015;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;CODA-Prompt&#20248;&#20110;&#26368;&#36817;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#32780;&#19981;&#38656;&#35201;&#37325;&#22797;&#35757;&#32451;&#25110;&#39069;&#22806;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computer vision models suffer from a phenomenon known as catastrophic forgetting when learning novel concepts from continuously shifting training data. Typical solutions for this continual learning problem require extensive rehearsal of previously seen data, which increases memory costs and may violate data privacy. Recently, the emergence of large-scale pre-trained vision transformer models has enabled prompting approaches as an alternative to data-rehearsal. These approaches rely on a key-query mechanism to generate prompts and have been found to be highly resistant to catastrophic forgetting in the well-established rehearsal-free continual learning setting. However, the key mechanism of these methods is not trained end-to-end with the task sequence. Our experiments show that this leads to a reduction in their plasticity, hence sacrificing new task accuracy, and inability to benefit from expanded parameter capacity. We instead propose to learn a set of prompt components which are ass
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#25345;&#32493;&#30340;&#26080;&#25968;&#25454;&#32467;&#26500;&#21270;VL&#27010;&#24565;&#23398;&#20064;&#65288;ConStruct-VL&#65289;&#22522;&#20934;&#65292;&#26088;&#22312;&#35299;&#20915;VL&#27169;&#22411;&#22312;&#32467;&#26500;&#21270;VL&#27010;&#24565;&#25512;&#29702;&#26041;&#38754;&#30340;&#29942;&#39048;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;-free&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.09790</link><description>&lt;p&gt;
ConStruct-VL: &#26080;&#38656;&#25968;&#25454;&#30340;&#25345;&#32493;&#32467;&#26500;&#21270;&#35270;&#35273;&#35821;&#35328;&#27010;&#24565;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ConStruct-VL: Data-Free Continual Structured VL Concepts Learning. (arXiv:2211.09790v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09790
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#25345;&#32493;&#30340;&#26080;&#25968;&#25454;&#32467;&#26500;&#21270;VL&#27010;&#24565;&#23398;&#20064;&#65288;ConStruct-VL&#65289;&#22522;&#20934;&#65292;&#26088;&#22312;&#35299;&#20915;VL&#27169;&#22411;&#22312;&#32467;&#26500;&#21270;VL&#27010;&#24565;&#25512;&#29702;&#26041;&#38754;&#30340;&#29942;&#39048;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;-free&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#35268;&#27169;&#39044;&#20808;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#22312;&#35768;&#22810;&#38646;&#26679;&#26412;&#19979;&#28216;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#65292;&#33021;&#22815;&#36890;&#36807;&#20165;&#21253;&#21547;&#30701;&#25991;&#26412;&#25552;&#31034;&#30340;&#23450;&#20041;&#26469;&#35782;&#21035;&#29289;&#20307;&#65292;&#24182;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#20063;&#24050;&#32463;&#34920;&#26126;&#65292;VL&#27169;&#22411;&#22312;&#32467;&#26500;&#21270;VL&#27010;&#24565;&#25512;&#29702;&#26041;&#38754;&#20173;&#28982;&#24456;&#33030;&#24369;&#65292;&#20363;&#22914;&#35782;&#21035;&#29289;&#20307;&#23646;&#24615;&#12289;&#29366;&#24577;&#21644;&#29289;&#20307;&#38388;&#20851;&#31995;&#30340;&#33021;&#21147;&#12290;&#36825;&#23548;&#33268;&#25512;&#29702;&#38169;&#35823;&#65292;&#38656;&#35201;&#36890;&#36807;&#25945;&#25480;VL&#27169;&#22411;&#32570;&#22833;&#30340;SVLC&#25216;&#33021;&#26469;&#36827;&#34892;&#26356;&#27491;&#65307;&#36890;&#24120;&#24517;&#39035;&#20351;&#29992;&#21457;&#29616;&#38382;&#39064;&#30340;&#31169;&#26377;&#25968;&#25454;&#26469;&#23436;&#25104;&#36825;&#19968;&#28857;&#65292;&#36825;&#33258;&#28982;&#32780;&#28982;&#22320;&#23548;&#33268;&#20102;&#19968;&#20010;&#26080;&#38656;&#20219;&#21153;ID&#30340;&#26080;&#25968;&#25454;&#25345;&#32493;VL&#23398;&#20064;&#35774;&#32622;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#25345;&#32493;&#30340;&#26080;&#25968;&#25454;&#32467;&#26500;&#21270;VL&#27010;&#24565;&#23398;&#20064;&#65288;ConStruct-VL&#65289;&#22522;&#20934;&#65292;&#24182;&#34920;&#26126;&#23427;&#23545;&#35768;&#22810;&#29616;&#26377;&#30340;&#26080;&#25968;&#25454;CL&#31574;&#30053;&#37117;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, large-scale pre-trained Vision-and-Language (VL) foundation models have demonstrated remarkable capabilities in many zero-shot downstream tasks, achieving competitive results for recognizing objects defined by as little as short text prompts. However, it has also been shown that VL models are still brittle in Structured VL Concept (SVLC) reasoning, such as the ability to recognize object attributes, states, and inter-object relations. This leads to reasoning mistakes, which need to be corrected as they occur by teaching VL models the missing SVLC skills; often this must be done using private data where the issue was found, which naturally leads to a data-free continual (no task-id) VL learning setting. In this work, we introduce the first Continual Data-Free Structured VL Concepts Learning (ConStruct-VL) benchmark and show it is challenging for many existing data-free CL strategies. We, therefore, propose a data-free method comprised of a new approach of Adversarial Pseudo-Re
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#25552;&#20986;&#22522;&#20110;&#36731;&#37327;&#32423;&#38899;&#39057;&#30340;&#37197;&#23545;&#36870;&#37329;&#23383;&#22612;&#32467;&#26500;&#32593;&#32476;&#21644;&#23494;&#38598;&#22810;&#23618;&#24863;&#30693;&#26426;&#22359;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#25110;&#27169;&#22411;&#36801;&#31227;&#24773;&#20917;&#19979;&#65292;&#23545;UrbanSound8K&#25968;&#25454;&#38598;&#21644;GTAZN&#25968;&#25454;&#38598;&#30340;&#39640;&#20934;&#30830;&#24230;&#20998;&#31867;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2211.02940</link><description>&lt;p&gt;
&#22522;&#20110;&#37197;&#23545;&#36870;&#37329;&#23383;&#22612;&#32467;&#26500;&#21644;&#23494;&#38598;&#22810;&#23618;&#24863;&#30693;&#26426;&#22359;&#30340;&#26377;&#25928;&#38899;&#39057;&#20998;&#31867;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Effective Audio Classification Network Based on Paired Inverse Pyramid Structure and Dense MLP Block. (arXiv:2211.02940v3 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02940
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#25552;&#20986;&#22522;&#20110;&#36731;&#37327;&#32423;&#38899;&#39057;&#30340;&#37197;&#23545;&#36870;&#37329;&#23383;&#22612;&#32467;&#26500;&#32593;&#32476;&#21644;&#23494;&#38598;&#22810;&#23618;&#24863;&#30693;&#26426;&#22359;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#25110;&#27169;&#22411;&#36801;&#31227;&#24773;&#20917;&#19979;&#65292;&#23545;UrbanSound8K&#25968;&#25454;&#38598;&#21644;GTAZN&#25968;&#25454;&#38598;&#30340;&#39640;&#20934;&#30830;&#24230;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#22823;&#35268;&#27169;&#26550;&#26500;&#24050;&#32463;&#25104;&#20026;&#38899;&#39057;&#20998;&#31867;&#39046;&#22495;&#30340;&#24517;&#35201;&#25216;&#26415;&#12290;&#34429;&#28982;&#36825;&#20123;&#25216;&#26415;&#26159;&#26368;&#20808;&#36827;&#30340;&#65292;&#20294;&#21482;&#26377;&#36890;&#36807;&#24040;&#22823;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#21442;&#25968;&#12289;&#22823;&#37327;&#30340;&#25968;&#25454;&#22686;&#24378;&#12289;&#26469;&#33258;&#22823;&#22411;&#25968;&#25454;&#38598;&#30340;&#36801;&#31227;&#20197;&#21450;&#19968;&#20123;&#20854;&#20182;&#25216;&#24039;&#25165;&#33021;&#20445;&#35777;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#38899;&#39057;&#30340;&#36731;&#37327;&#32423;&#29305;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#32593;&#32476;&#32467;&#26500;&#8212;&#8212;&#37197;&#23545;&#36870;&#37329;&#23383;&#22612;&#32467;&#26500;&#65288;PIP&#65289;&#65292;&#20197;&#21450;&#19968;&#31181;&#31216;&#20026;&#37197;&#23545;&#36870;&#37329;&#23383;&#22612;&#32467;&#26500;MLP&#32593;&#32476;&#65288;PIPMN&#65289;&#30340;&#32593;&#32476;&#12290;PIP&#32593;&#32476;&#22312;UrbanSound8K&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;96%&#30340;&#29615;&#22659;&#22768;&#38899;&#20998;&#31867;&#20934;&#30830;&#24230;&#65292;&#22312;GTAZN&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;93.2%&#30340;&#38899;&#20048;&#27969;&#27966;&#20998;&#31867;&#20934;&#30830;&#24230;&#65292;&#20165;&#20351;&#29992;100&#19975;&#20010;&#21442;&#25968;&#21363;&#21487;&#23454;&#29616;&#36825;&#20004;&#20010;&#32467;&#26524;&#65292;&#32780;&#19981;&#38656;&#35201;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#25110;&#27169;&#22411;&#36801;&#31227;&#12290;&#20844;&#20849;&#20195;&#30721;&#21487;&#22312;&#20197;&#19979;&#32593;&#22336;&#33719;&#21462;&#65306;https://github.com/JNAIC/PIPMN
&lt;/p&gt;
&lt;p&gt;
Recently, massive architectures based on Convolutional Neural Network (CNN) and self-attention mechanisms have become necessary for audio classification. While these techniques are state-of-the-art, these works' effectiveness can only be guaranteed with huge computational costs and parameters, large amounts of data augmentation, transfer from large datasets and some other tricks. By utilizing the lightweight nature of audio, we propose an efficient network structure called Paired Inverse Pyramid Structure (PIP) and a network called Paired Inverse Pyramid Structure MLP Network (PIPMN). The PIPMN reaches 96\% of Environmental Sound Classification (ESC) accuracy on the UrbanSound8K dataset and 93.2\% of Music Genre Classification (MGC) on the GTAZN dataset, with only 1 million parameters. Both of the results are achieved without data augmentation or model transfer. Public code is available at: https://github.com/JNAIC/PIPMN
&lt;/p&gt;</description></item><item><title>LongShortNet&#26159;&#19968;&#31181;&#22522;&#20110;&#21452;&#36335;&#24452;&#32593;&#32476;&#30340;&#27969;&#24335;&#24863;&#30693;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#38271;&#26399;&#26102;&#38388;&#36816;&#21160;&#21644;&#30701;&#26399;&#31354;&#38388;&#35821;&#20041;&#65292;&#23454;&#29616;&#20102;&#26102;&#31354;&#29305;&#24449;&#34701;&#21512;&#12290;&#22312;Argoverse-HD&#25968;&#25454;&#38598;&#19978;&#65292;LongShortNet&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#26816;&#27979;&#24615;&#33021;&#65292;&#24182;&#19988;&#20960;&#20046;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2210.15518</link><description>&lt;p&gt;
LongShortNet&#65306;&#25506;&#32034;&#26102;&#38388;&#21644;&#35821;&#20041;&#29305;&#24449;&#34701;&#21512;&#22312;&#27969;&#24335;&#24863;&#30693;&#20013;
&lt;/p&gt;
&lt;p&gt;
LongShortNet: Exploring Temporal and Semantic Features Fusion in Streaming Perception. (arXiv:2210.15518v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15518
&lt;/p&gt;
&lt;p&gt;
LongShortNet&#26159;&#19968;&#31181;&#22522;&#20110;&#21452;&#36335;&#24452;&#32593;&#32476;&#30340;&#27969;&#24335;&#24863;&#30693;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#38271;&#26399;&#26102;&#38388;&#36816;&#21160;&#21644;&#30701;&#26399;&#31354;&#38388;&#35821;&#20041;&#65292;&#23454;&#29616;&#20102;&#26102;&#31354;&#29305;&#24449;&#34701;&#21512;&#12290;&#22312;Argoverse-HD&#25968;&#25454;&#38598;&#19978;&#65292;LongShortNet&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#26816;&#27979;&#24615;&#33021;&#65292;&#24182;&#19988;&#20960;&#20046;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#24335;&#24863;&#30693;&#26159;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#38656;&#35201;&#22312;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#24310;&#36831;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#36827;&#34892;&#20180;&#32454;&#30340;&#24179;&#34913;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#27969;&#24335;&#24863;&#30693;&#26041;&#27861;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#20165;&#20165;&#20381;&#36182;&#20110;&#24403;&#21069;&#24103;&#21450;&#20854;&#30456;&#37051;&#30340;&#20004;&#24103;&#26469;&#23398;&#20064;&#36816;&#21160;&#27169;&#24335;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#23545;&#22797;&#26434;&#22330;&#26223;&#30340;&#24314;&#27169;&#33021;&#21147;&#65292;&#24448;&#24448;&#23548;&#33268;&#26816;&#27979;&#32467;&#26524;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LongShortNet&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#36335;&#24452;&#32593;&#32476;&#65292;&#23427;&#25429;&#25417;&#38271;&#26399;&#30340;&#26102;&#38388;&#36816;&#21160;&#65292;&#24182;&#23558;&#20854;&#19982;&#30701;&#26399;&#30340;&#31354;&#38388;&#35821;&#20041;&#38598;&#25104;&#21040;&#23454;&#26102;&#24863;&#30693;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;LongShortNet&#20540;&#24471;&#27880;&#24847;&#65292;&#22240;&#20026;&#23427;&#26159;&#31532;&#19968;&#20010;&#23558;&#38271;&#26399;&#26102;&#38388;&#24314;&#27169;&#25193;&#23637;&#21040;&#27969;&#24335;&#24863;&#30693;&#30340;&#24037;&#20316;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26102;&#31354;&#29305;&#24449;&#34701;&#21512;&#12290;&#25105;&#20204;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;Argoverse-HD&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;LongShortNet&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#22312;&#20960;&#20046;&#27809;&#26377;&#39069;&#22806;&#35745;&#31639;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Streaming perception is a fundamental task in autonomous driving that requires a careful balance between the latency and accuracy of the autopilot system. However, current methods for streaming perception are limited as they rely only on the current and adjacent two frames to learn movement patterns, which restricts their ability to model complex scenes, often leading to poor detection results. To address this limitation, we propose LongShortNet, a novel dual-path network that captures long-term temporal motion and integrates it with short-term spatial semantics for real-time perception. Our proposed LongShortNet is notable as it is the first work to extend long-term temporal modeling to streaming perception, enabling spatiotemporal feature fusion. We evaluate LongShortNet on the challenging Argoverse-HD dataset and demonstrate that it outperforms existing state-of-the-art methods with almost no additional computational cost.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#36882;&#36827;&#30340;&#19978;&#19979;&#25991;&#21464;&#25442;&#26426;&#21046;&#30340;&#30446;&#26631;&#36319;&#36394;&#26041;&#27861; ProContEXT&#65292;&#37319;&#29992;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#23545;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#19979;&#25991;&#36827;&#34892;&#32534;&#30721;&#65292;&#36880;&#27493;&#21033;&#29992;&#38745;&#24577;&#21644;&#21160;&#24577;&#22810;&#23610;&#24230;&#27169;&#26495;&#36827;&#34892;&#20934;&#30830;&#36319;&#36394;&#12290;&#23427;&#25506;&#32034;&#20102;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#19979;&#25991;&#20043;&#38388;&#30340;&#20114;&#34917;&#24615;&#65292;&#20026;&#22522;&#20110; transformer &#30340;&#36319;&#36394;&#22120;&#30340;&#22810;&#19978;&#19979;&#25991;&#24314;&#27169;&#25552;&#20379;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2210.15511</link><description>&lt;p&gt;
ProContEXT&#65306;&#22522;&#20110;&#36882;&#36827;&#30340;&#19978;&#19979;&#25991;&#21464;&#25442;&#26426;&#21046;&#30340;&#30446;&#26631;&#36319;&#36394;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
ProContEXT: Exploring Progressive Context Transformer for Tracking. (arXiv:2210.15511v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#36882;&#36827;&#30340;&#19978;&#19979;&#25991;&#21464;&#25442;&#26426;&#21046;&#30340;&#30446;&#26631;&#36319;&#36394;&#26041;&#27861; ProContEXT&#65292;&#37319;&#29992;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#23545;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#19979;&#25991;&#36827;&#34892;&#32534;&#30721;&#65292;&#36880;&#27493;&#21033;&#29992;&#38745;&#24577;&#21644;&#21160;&#24577;&#22810;&#23610;&#24230;&#27169;&#26495;&#36827;&#34892;&#20934;&#30830;&#36319;&#36394;&#12290;&#23427;&#25506;&#32034;&#20102;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#19979;&#25991;&#20043;&#38388;&#30340;&#20114;&#34917;&#24615;&#65292;&#20026;&#22522;&#20110; transformer &#30340;&#36319;&#36394;&#22120;&#30340;&#22810;&#19978;&#19979;&#25991;&#24314;&#27169;&#25552;&#20379;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#35270;&#35273;&#30446;&#26631;&#36319;&#36394;&#20165;&#23558;&#31532;&#19968;&#20010;&#24103;&#20013;&#30340;&#30446;&#26631;&#21306;&#22495;&#20316;&#20026;&#27169;&#26495;&#65292;&#26080;&#27861;&#36866;&#24212;&#24555;&#36895;&#21464;&#21270;&#21644;&#25317;&#25380;&#22330;&#26223;&#20013;&#30340;&#29289;&#20307;&#22806;&#35266;&#21464;&#21270;&#65292;&#23548;&#33268;&#36319;&#36394;&#22833;&#36133;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36882;&#36827;&#24335;&#19978;&#19979;&#25991;&#32534;&#30721;&#21464;&#25442;&#26426;&#21046;&#30340;&#36319;&#36394;&#26041;&#27861; ProContEXT&#65292;&#20351;&#29992;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#23545;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#19979;&#25991;&#36827;&#34892;&#32534;&#30721;&#65292;&#36880;&#27493;&#21033;&#29992;&#38745;&#24577;&#21644;&#21160;&#24577;&#22810;&#23610;&#24230;&#27169;&#26495;&#36827;&#34892;&#20934;&#30830;&#36319;&#36394;&#12290;&#23427;&#25506;&#32034;&#20102;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#19979;&#25991;&#20043;&#38388;&#30340;&#20114;&#34917;&#24615;&#65292;&#20026;&#22522;&#20110; transformer &#30340;&#36319;&#36394;&#22120;&#30340;&#22810;&#19978;&#19979;&#25991;&#24314;&#27169;&#25552;&#20379;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#21478;&#22806;&#65292;ProContEXT &#20462;&#35746;&#20102;&#26631;&#35760;&#20462;&#21098;&#25216;&#26415;&#20197;&#20943;&#23569;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing Visual Object Tracking (VOT) only takes the target area in the first frame as a template. This causes tracking to inevitably fail in fast-changing and crowded scenes, as it cannot account for changes in object appearance between frames. To this end, we revamped the tracking framework with Progressive Context Encoding Transformer Tracker (ProContEXT), which coherently exploits spatial and temporal contexts to predict object motion trajectories. Specifically, ProContEXT leverages a context-aware self-attention module to encode the spatial and temporal context, refining and updating the multi-scale static and dynamic templates to progressively perform accurate tracking. It explores the complementary between spatial and temporal context, raising a new pathway to multi-context modeling for transformer-based trackers. In addition, ProContEXT revised the token pruning technique to reduce computational complexity. Extensive experiments on popular benchmark datasets such as GOT-10k and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#31070;&#32463;&#35299;&#30721;&#26041;&#27861; BraVL&#65292;&#23427;&#21033;&#29992;&#20102;&#33041;&#35270;&#35273;&#35821;&#35328;&#29305;&#24449;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#65292;&#36890;&#36807;&#20135;&#21697;&#28151;&#21512;&#27169;&#22411;&#25512;&#26029;&#28508;&#22312;&#20195;&#30721;&#65292;&#23454;&#29616;&#19977;&#31181;&#27169;&#24577;&#30340;&#21327;&#21516;&#29983;&#25104;&#65292;&#22312;&#35299;&#30721;&#35270;&#35273;&#31070;&#32463;&#34920;&#24449;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.06756</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#27169;&#24577;&#23398;&#20064;&#33041;&#35270;&#35273;&#35821;&#35328;&#29305;&#24449;&#35299;&#30721;&#35270;&#35273;&#31070;&#32463;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
Decoding Visual Neural Representations by Multimodal Learning of Brain-Visual-Linguistic Features. (arXiv:2210.06756v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06756
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#31070;&#32463;&#35299;&#30721;&#26041;&#27861; BraVL&#65292;&#23427;&#21033;&#29992;&#20102;&#33041;&#35270;&#35273;&#35821;&#35328;&#29305;&#24449;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#65292;&#36890;&#36807;&#20135;&#21697;&#28151;&#21512;&#27169;&#22411;&#25512;&#26029;&#28508;&#22312;&#20195;&#30721;&#65292;&#23454;&#29616;&#19977;&#31181;&#27169;&#24577;&#30340;&#21327;&#21516;&#29983;&#25104;&#65292;&#22312;&#35299;&#30721;&#35270;&#35273;&#31070;&#32463;&#34920;&#24449;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#30721;&#20154;&#31867;&#35270;&#35273;&#31070;&#32463;&#34920;&#24449;&#26159;&#19968;&#20010;&#20805;&#28385;&#25361;&#25112;&#30340;&#20219;&#21153;&#65292;&#22312;&#25581;&#31034;&#35270;&#35273;&#22788;&#29702;&#26426;&#21046;&#21644;&#24320;&#21457;&#31867;&#20284;&#22823;&#33041;&#30340;&#26234;&#33021;&#26426;&#22120;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#30340;&#31185;&#23398;&#24847;&#20041;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; BraVL &#30340;&#36890;&#29992;&#31070;&#32463;&#35299;&#30721;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#20102;&#33041;&#35270;&#35273;&#35821;&#35328;&#29305;&#24449;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#12290;&#25105;&#20204;&#36890;&#36807;&#22810;&#27169;&#24577;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#24314;&#27169;&#33041;&#12289;&#35270;&#35273;&#21644;&#35821;&#35328;&#29305;&#24449;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#20135;&#21697;&#28151;&#21512;&#27169;&#22411;&#30340;&#24418;&#24335;&#26469;&#25512;&#26029;&#28508;&#22312;&#20195;&#30721;&#65292;&#20174;&#32780;&#23454;&#29616;&#19977;&#31181;&#27169;&#24577;&#30340;&#21327;&#21516;&#29983;&#25104;&#12290;&#20026;&#20102;&#23398;&#20064;&#26356;&#19968;&#33268;&#30340;&#32852;&#21512;&#34920;&#31034;&#24182;&#20419;&#36827;&#36328;&#27169;&#24577;&#27867;&#21270;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#27169;&#24577;&#20002;&#22833;&#27491;&#21017;&#21270;&#30340;&#26032;&#30340;&#35757;&#32451;&#31574;&#30053;&#12290;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#20154;&#31867; fMRI &#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126; BraVL &#22312;&#23545;&#22810;&#20010;&#31867;&#21035;&#30340;&#35270;&#35273;&#31070;&#32463;&#34920;&#24449;&#36827;&#34892;&#35299;&#30721;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#21253;&#25324;&#26032;&#39062;&#21644;&#26410;&#35265;&#36807;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decoding human visual neural representations is a challenging task with great scientific significance in revealing vision-processing mechanisms and developing brain-like intelligent machines. Most existing methods are difficult to generalize to novel categories that have no corresponding neural data for training. The two main reasons are 1) the under-exploitation of the multimodal semantic knowledge underlying the neural data and 2) the small number of paired (stimuli-responses) training data. To overcome these limitations, this paper presents a generic neural decoding method called BraVL that uses multimodal learning of brain-visual-linguistic features. We focus on modeling the relationships between brain, visual and linguistic features via multimodal deep generative models. Specifically, we leverage the mixture-of-product-of-experts formulation to infer a latent code that enables a coherent joint generation of all three modalities. To learn a more consistent joint representation and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25968;&#25454;&#38598;&#29256;&#26435;&#20445;&#25252;&#30340;&#26080;&#23475;&#21644;&#38544;&#34109;&#30340;&#26080;&#30446;&#26631;&#21518;&#38376;&#27700;&#21360;&#26041;&#26696;&#65292;&#21487;&#20197;&#36798;&#21040;&#19982;&#26368;&#20808;&#36827;&#26041;&#26696;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#27700;&#21360;&#25928;&#26524;&#65292;&#24182;&#35777;&#26126;&#23545;&#27169;&#22411;&#24615;&#33021;&#26080;&#23475;&#19988;&#38544;&#34109;&#12290;</title><link>http://arxiv.org/abs/2210.00875</link><description>&lt;p&gt;
&#26080;&#30446;&#26631;&#21518;&#38376;&#27700;&#21360;&#65306;&#26397;&#30528;&#26080;&#23475;&#21644;&#38544;&#34109;&#30340;&#25968;&#25454;&#38598;&#29256;&#26435;&#20445;&#25252;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Untargeted Backdoor Watermark: Towards Harmless and Stealthy Dataset Copyright Protection. (arXiv:2210.00875v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25968;&#25454;&#38598;&#29256;&#26435;&#20445;&#25252;&#30340;&#26080;&#23475;&#21644;&#38544;&#34109;&#30340;&#26080;&#30446;&#26631;&#21518;&#38376;&#27700;&#21360;&#26041;&#26696;&#65292;&#21487;&#20197;&#36798;&#21040;&#19982;&#26368;&#20808;&#36827;&#26041;&#26696;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#27700;&#21360;&#25928;&#26524;&#65292;&#24182;&#35777;&#26126;&#23545;&#27169;&#22411;&#24615;&#33021;&#26080;&#23475;&#19988;&#38544;&#34109;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24050;&#22312;&#23454;&#36341;&#20013;&#23637;&#29616;&#20986;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;&#21487;&#20197;&#35828;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24555;&#36895;&#21457;&#23637;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24471;&#30410;&#20110;&#39640;&#36136;&#37327;&#65288;&#24320;&#28304;&#65289;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#20154;&#21592;&#21487;&#20197;&#22312;&#27492;&#22522;&#30784;&#19978;&#36731;&#26494;&#22320;&#35780;&#20272;&#21644;&#25913;&#36827;&#20182;&#20204;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;&#30001;&#20110;&#25968;&#25454;&#25910;&#38598;&#36890;&#24120;&#26159;&#32791;&#26102;&#29978;&#33267;&#26114;&#36149;&#30340;&#65292;&#22914;&#20309;&#20445;&#25252;&#20854;&#29256;&#26435;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#24182;&#20540;&#24471;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#25968;&#25454;&#38598;&#30340;&#25152;&#26377;&#26435;&#39564;&#35777;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;&#39564;&#35777;&#26041;&#27861;&#30001;&#20110;&#26377;&#30446;&#26631;&#30340;&#21518;&#38376;&#27700;&#21360;&#30340;&#29305;&#24615;&#65292;&#20250;&#22312;&#21463;&#20445;&#25252;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#24341;&#20837;&#26032;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26080;&#30446;&#26631;&#21518;&#38376;&#27700;&#21360;&#26041;&#26696;&#65292;&#20854;&#20013;&#24322;&#24120;&#30340;&#27169;&#22411;&#34892;&#20026;&#19981;&#26159;&#30830;&#23450;&#24615;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#20010;&#20998;&#25955;&#24230;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#30456;&#20851;&#24615;&#65292;&#22522;&#20110;&#27492;&#25105;&#20204;&#22312;&#21463;&#27745;&#26579;&#26631;&#31614;&#21644;&#24178;&#20928;&#26631;&#31614;&#25915;&#20987;&#35774;&#32622;&#19979;&#35774;&#35745;&#20102;&#26080;&#30446;&#26631;&#21518;&#38376;&#27700;&#21360;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#27700;&#21360;&#25552;&#21462;&#30340;&#20934;&#30830;&#24615;&#21644;&#27169;&#22411;&#24615;&#33021;&#19978;&#37117;&#33021;&#22815;&#36798;&#21040;&#29978;&#33267;&#36229;&#36807;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#36824;&#34987;&#35777;&#26126;&#23545;&#27169;&#22411;&#24615;&#33021;&#26080;&#23475;&#19988;&#38544;&#34109;&#65292;&#19981;&#20250;&#24341;&#20837;&#20219;&#20309;&#21487;&#26816;&#27979;&#30340;&#25197;&#26354;&#25110;&#25925;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have demonstrated their superiority in practice. Arguably, the rapid development of DNNs is largely benefited from high-quality (open-sourced) datasets, based on which researchers and developers can easily evaluate and improve their learning methods. Since the data collection is usually time-consuming or even expensive, how to protect their copyrights is of great significance and worth further exploration. In this paper, we revisit dataset ownership verification. We find that existing verification methods introduced new security risks in DNNs trained on the protected dataset, due to the targeted nature of poison-only backdoor watermarks. To alleviate this problem, in this work, we explore the untargeted backdoor watermarking scheme, where the abnormal model behaviors are not deterministic. Specifically, we introduce two dispersibilities and prove their correlation, based on which we design the untargeted backdoor watermark under both poisoned-label and clean
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#40784;&#30340;&#27010;&#29575;&#20107;&#20214;&#31526;&#21512;&#24615;&#26816;&#26597;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#32771;&#34385;&#21040;&#20855;&#26377;&#36739;&#20302;&#20294;&#36275;&#22815;&#39640;&#27010;&#29575;&#30340;&#27963;&#21160;&#65292;&#20197;&#26356;&#22909;&#22320;&#19982;&#36807;&#31243;&#27169;&#22411;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2209.04309</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#40784;&#30340;&#27010;&#29575;&#20107;&#20214;&#31526;&#21512;&#24615;&#26816;&#26597;
&lt;/p&gt;
&lt;p&gt;
Alignment-based conformance checking over probabilistic events. (arXiv:2209.04309v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.04309
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#40784;&#30340;&#27010;&#29575;&#20107;&#20214;&#31526;&#21512;&#24615;&#26816;&#26597;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#32771;&#34385;&#21040;&#20855;&#26377;&#36739;&#20302;&#20294;&#36275;&#22815;&#39640;&#27010;&#29575;&#30340;&#27963;&#21160;&#65292;&#20197;&#26356;&#22909;&#22320;&#19982;&#36807;&#31243;&#27169;&#22411;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21512;&#24615;&#26816;&#26597;&#25216;&#26415;&#20801;&#35768;&#25105;&#20204;&#35780;&#20272;&#29305;&#23450;&#36807;&#31243;&#27169;&#22411;&#30340;&#23637;&#31034;&#34892;&#20026;&#65292;&#30001;&#30417;&#27979;&#20107;&#20214;&#30340;&#36319;&#36394;&#34920;&#31034;&#12290;&#29616;&#20195;&#30417;&#27979;&#21644;&#27963;&#21160;&#35782;&#21035;&#25216;&#26415;&#65292;&#22914;&#20256;&#24863;&#22120;&#12289;&#29289;&#32852;&#32593;&#12289;&#32479;&#35745;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#31561;&#25216;&#26415;&#65292;&#21487;&#20197;&#20135;&#29983;&#22823;&#37327;&#30456;&#20851;&#20107;&#20214;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#35813;&#25968;&#25454;&#36890;&#24120;&#20855;&#26377;&#22122;&#22768;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#19982;&#31526;&#21512;&#24615;&#26816;&#26597;&#31639;&#27861;&#25152;&#38656;&#30340;&#30830;&#23450;&#24615;&#20107;&#20214;&#26085;&#24535;&#30340;&#20551;&#35774;&#30456;&#21453;&#12290;&#26412;&#25991;&#23558;&#22522;&#20110;&#23545;&#40784;&#30340;&#31526;&#21512;&#24615;&#26816;&#26597;&#25193;&#23637;&#21040;&#27010;&#29575;&#20107;&#20214;&#26085;&#24535;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#21152;&#26435;&#36319;&#36394;&#27169;&#22411;&#21644;&#21152;&#26435;&#23545;&#40784;&#25104;&#26412;&#20989;&#25968;&#65292;&#20197;&#21450;&#33258;&#23450;&#20041;&#38408;&#20540;&#21442;&#25968;&#65292;&#29992;&#20110;&#25511;&#21046;&#23545;&#20107;&#20214;&#25968;&#25454;&#19982;&#36807;&#31243;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#27700;&#24179;&#12290;&#26368;&#32456;&#30340;&#31639;&#27861;&#32771;&#34385;&#21040;&#20855;&#26377;&#36739;&#20302;&#20294;&#36275;&#22815;&#39640;&#27010;&#29575;&#30340;&#27963;&#21160;&#65292;&#20197;&#26356;&#22909;&#22320;&#19982;&#36807;&#31243;&#27169;&#22411;&#23545;&#40784;&#12290;&#25105;&#20204;&#20174;&#27491;&#24335;&#21644;&#38750;&#27491;&#24335;&#30340;&#35282;&#24230;&#35299;&#37322;&#20102;&#31639;&#27861;&#21450;&#20854;&#21160;&#26426;&#65292;&#24182;&#36890;&#36807;&#21512;&#25104;&#21644;&#23454;&#38469;&#26696;&#20363;&#30740;&#31350;&#39564;&#35777;&#20102;&#20854;&#25928;&#29992;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conformance checking techniques allow us to evaluate how well some exhibited behaviour, represented by a trace of monitored events, conforms to a specified process model. Modern monitoring and activity recognition technologies, such as those relying on sensors, the IoT, statistics and AI, can produce a wealth of relevant event data. However, this data is typically characterised by noise and uncertainty, in contrast to the assumption of a deterministic event log required by conformance checking algorithms. In this paper, we extend alignment-based conformance checking to function under a probabilistic event log. We introduce a weighted trace model and weighted alignment cost function, and a custom threshold parameter that controls the level of confidence on the event data vs. the process model. The resulting algorithm considers activities of lower but sufficiently high probability that better align with the process model. We explain the algorithm and its motivation both from formal and i
&lt;/p&gt;</description></item><item><title>MolMIM&#26159;&#29992;&#20110;&#23567;&#20998;&#23376;&#33647;&#29289;&#21457;&#29616;&#30340;&#27010;&#29575;&#33258;&#32534;&#30721;&#22120;&#65292;&#20854;&#23398;&#20064;&#20102;&#19968;&#31181;&#20449;&#24687;&#20016;&#23500;&#19988;&#32858;&#31867;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#36890;&#36807;&#20419;&#36827;&#33268;&#23494;&#30340;&#28508;&#22312;&#31354;&#38388;&#26469;&#37319;&#26679;&#26377;&#25928;&#30340;&#20998;&#23376;&#12290;&#36890;&#36807;&#19982;&#20854;&#20182;&#27169;&#22411;&#30340;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;MolMIM&#30340;&#26356;&#22909;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20986;&#33394;&#30340;&#20998;&#23376;&#20248;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2208.09016</link><description>&lt;p&gt;
&#20351;&#29992;&#20114;&#20449;&#24687;&#26426;&#22120;&#25552;&#39640;&#23567;&#20998;&#23376;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Improving Small Molecule Generation using Mutual Information Machine. (arXiv:2208.09016v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09016
&lt;/p&gt;
&lt;p&gt;
MolMIM&#26159;&#29992;&#20110;&#23567;&#20998;&#23376;&#33647;&#29289;&#21457;&#29616;&#30340;&#27010;&#29575;&#33258;&#32534;&#30721;&#22120;&#65292;&#20854;&#23398;&#20064;&#20102;&#19968;&#31181;&#20449;&#24687;&#20016;&#23500;&#19988;&#32858;&#31867;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#36890;&#36807;&#20419;&#36827;&#33268;&#23494;&#30340;&#28508;&#22312;&#31354;&#38388;&#26469;&#37319;&#26679;&#26377;&#25928;&#30340;&#20998;&#23376;&#12290;&#36890;&#36807;&#19982;&#20854;&#20182;&#27169;&#22411;&#30340;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;MolMIM&#30340;&#26356;&#22909;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20986;&#33394;&#30340;&#20998;&#23376;&#20248;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25511;&#21046;&#23567;&#20998;&#23376;&#29983;&#25104;&#30340;&#20219;&#21153;&#65292;&#21363;&#22312;&#19968;&#23450;&#38480;&#21046;&#26465;&#20214;&#19979;&#65288;&#22914;&#19982;&#21442;&#32771;&#20998;&#23376;&#30340;&#30456;&#20284;&#24230;&#65289;&#65292;&#23547;&#25214;&#20855;&#26377;&#25152;&#38656;&#23646;&#24615;&#30340;&#26032;&#22411;&#20998;&#23376;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;MolMIM&#65292;&#19968;&#31181;&#29992;&#20110;&#23567;&#20998;&#23376;&#33647;&#29289;&#21457;&#29616;&#30340;&#27010;&#29575;&#33258;&#32534;&#30721;&#22120;&#65292;&#23427;&#23398;&#20064;&#20102;&#19968;&#31181;&#20449;&#24687;&#20016;&#23500;&#19988;&#32858;&#31867;&#30340;&#28508;&#22312;&#31354;&#38388;&#12290; MolMIM&#26159;&#29992;&#20114;&#20449;&#24687;&#26426;&#22120;&#65288;MIM&#65289;&#23398;&#20064;&#35757;&#32451;&#30340;&#65292;&#24182;&#25552;&#20379;&#20102;&#21464;&#38271;SMILES&#23383;&#31526;&#20018;&#30340;&#22266;&#23450;&#38271;&#24230;&#34920;&#31034;&#12290;&#30001;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#20855;&#26377;&#26080;&#25928;&#26679;&#26412;&#30340;&#8220;&#31354;&#27934;&#8221;&#30340;&#34920;&#31034;&#65292;&#22240;&#27492;&#25105;&#20204;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25193;&#23637;&#65292;&#20419;&#36827;&#20102;&#33268;&#23494;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#20801;&#35768;&#27169;&#22411;&#20174;&#28508;&#22312;&#20195;&#30721;&#30340;&#38543;&#26426;&#25200;&#21160;&#20013;&#37319;&#26679;&#26377;&#25928;&#30340;&#20998;&#23376;&#12290;&#25105;&#20204;&#23545;MolMIM&#19982;&#20960;&#31181;&#21487;&#21464;&#22823;&#23567;&#21644;&#22266;&#23450;&#22823;&#23567;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#65292;&#36890;&#36807;&#26377;&#25928;&#24615;&#12289;&#29420;&#29305;&#24615;&#21644;&#26032;&#39062;&#24615;&#31561;&#25351;&#26631;&#35777;&#26126;&#20102;MolMIM&#30340;&#26356;&#22909;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;CMA-ES&#65288;&#19968;&#31181;&#26420;&#32032;&#28436;&#21270;&#31639;&#27861;&#65289;&#26469;&#20248;&#21270;&#20998;&#23376;&#24615;&#36136;&#30340;&#32452;&#21512;&#30446;&#26631;&#20989;&#25968;&#65292;&#24182;&#23637;&#31034;&#20102;MolMIM&#22312;&#20998;&#23376;&#20248;&#21270;&#26041;&#38754;&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the task of controlled generation of small molecules, which entails finding novel molecules with desired properties under certain constraints (e.g., similarity to a reference molecule). Here we introduce MolMIM, a probabilistic auto-encoder for small molecule drug discovery that learns an informative and clustered latent space. MolMIM is trained with Mutual Information Machine (MIM) learning, and provides a fixed length representation of variable length SMILES strings. Since encoder-decoder models can learn representations with ``holes'' of invalid samples, here we propose a novel extension to the training procedure which promotes a dense latent space, and allows the model to sample valid molecules from random perturbations of latent codes. We provide a thorough comparison of MolMIM to several variable-size and fixed-size encoder-decoder models, demonstrating MolMIM's superior generation as measured in terms of validity, uniqueness, and novelty. We then utilize CMA-ES, a nai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#24615;&#33021;&#30340;&#22810;&#23610;&#24230;&#27880;&#24847;&#31070;&#32463;&#20307;&#31995;&#32467;&#26500;&#25628;&#32034;&#65288;MANAS&#65289;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#22270;&#20687;&#21435;&#38632;&#12290; &#35813;&#26041;&#27861;&#33258;&#21160;&#25628;&#32034;&#21435;&#38632;&#32593;&#32476;&#30340;&#20869;&#37096;&#22810;&#23610;&#24230;&#27880;&#24847;&#21147;&#32467;&#26500;&#65292;&#24182;&#37319;&#29992;&#26377;&#25928;&#30340;&#22810;&#23610;&#24230;&#35757;&#32451;&#31574;&#30053;&#30830;&#20445;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2207.00728</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#27880;&#24847;&#21147;&#22270;&#20687;&#21435;&#38632;&#31070;&#32463;&#20307;&#31995;&#32467;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Multi-scale Attentive Image De-raining Networks via Neural Architecture Search. (arXiv:2207.00728v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.00728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#24615;&#33021;&#30340;&#22810;&#23610;&#24230;&#27880;&#24847;&#31070;&#32463;&#20307;&#31995;&#32467;&#26500;&#25628;&#32034;&#65288;MANAS&#65289;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#22270;&#20687;&#21435;&#38632;&#12290; &#35813;&#26041;&#27861;&#33258;&#21160;&#25628;&#32034;&#21435;&#38632;&#32593;&#32476;&#30340;&#20869;&#37096;&#22810;&#23610;&#24230;&#27880;&#24847;&#21147;&#32467;&#26500;&#65292;&#24182;&#37319;&#29992;&#26377;&#25928;&#30340;&#22810;&#23610;&#24230;&#35757;&#32451;&#31574;&#30053;&#30830;&#20445;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23610;&#24230;&#26550;&#26500;&#21644;&#27880;&#24847;&#27169;&#22359;&#24050;&#32463;&#35777;&#26126;&#22312;&#35768;&#22810;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#21435;&#38632;&#26041;&#27861;&#20013;&#24456;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#35774;&#35745;&#21644;&#38598;&#25104;&#36825;&#20004;&#20010;&#32452;&#20214;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#38656;&#35201;&#22823;&#37327;&#30340;&#21171;&#21160;&#21644;&#24191;&#27867;&#30340;&#19987;&#19994;&#30693;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20026;&#22270;&#20687;&#21435;&#38632;&#25216;&#26415;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#24615;&#33021;&#30340;&#22810;&#23610;&#24230;&#27880;&#24847;&#31070;&#32463;&#20307;&#31995;&#32467;&#26500;&#25628;&#32034;&#65288;MANAS&#65289;&#26694;&#26550;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22810;&#20010;&#28789;&#27963;&#30340;&#27169;&#22359;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#23610;&#24230;&#20851;&#27880;&#25628;&#32034;&#31354;&#38388;&#65292;&#36825;&#20123;&#27169;&#22359;&#38750;&#24120;&#36866;&#21512;&#20110;&#22270;&#20687;&#21435;&#38632;&#20219;&#21153;&#12290;&#22312;&#35813;&#25628;&#32034;&#31354;&#38388;&#19979;&#65292;&#26500;&#24314;&#20102;&#22810;&#23610;&#24230;&#27880;&#24847;&#21147;&#21333;&#20803;&#65292;&#36827;&#19968;&#27493;&#29992;&#20110;&#26500;&#24314;&#19968;&#20010;&#24378;&#22823;&#30340;&#22270;&#20687;&#21435;&#38632;&#32593;&#32476;&#12290;&#21435;&#38632;&#32593;&#32476;&#30340;&#20869;&#37096;&#22810;&#23610;&#24230;&#27880;&#24847;&#21147;&#32467;&#26500;&#36890;&#36807;&#22522;&#20110;&#26799;&#24230;&#30340;&#25628;&#32034;&#31639;&#27861;&#33258;&#21160;&#25628;&#32034;&#65292;&#20174;&#26576;&#31181;&#31243;&#24230;&#19978;&#36991;&#20813;&#20102;&#25163;&#21160;&#35774;&#35745;&#30340;&#32321;&#29712;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#33719;&#24471;&#40065;&#26834;&#30340;&#22270;&#20687;&#21435;&#38632;&#27169;&#22411;&#65292;&#36827;&#19968;&#27493;&#37319;&#29992;&#20102;&#23454;&#29992;&#19988;&#26377;&#25928;&#30340;&#22810;&#23610;&#24230;&#35757;&#32451;&#31574;&#30053;&#65292;&#30830;&#20445;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-scale architectures and attention modules have shown effectiveness in many deep learning-based image de-raining methods. However, manually designing and integrating these two components into a neural network requires a bulk of labor and extensive expertise. In this article, a high-performance multi-scale attentive neural architecture search (MANAS) framework is technically developed for image deraining. The proposed method formulates a new multi-scale attention search space with multiple flexible modules that are favorite to the image de-raining task. Under the search space, multi-scale attentive cells are built, which are further used to construct a powerful image de-raining network. The internal multiscale attentive architecture of the de-raining network is searched automatically through a gradient-based search algorithm, which avoids the daunting procedure of the manual design to some extent. Moreover, in order to obtain a robust image de-raining model, a practical and effecti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22914;&#20309;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#26469;&#22788;&#29702;&#22823;&#35268;&#27169;&#30340;&#30005;&#23376;&#26174;&#24494;&#38236;&#25968;&#25454;&#38598;&#12290;&#37325;&#28857;&#35752;&#35770;&#20102;&#31639;&#27861;&#22914;&#20309;&#24212;&#29992;&#20110;&#32454;&#32990;&#21644;&#20122;&#32454;&#32990;&#32467;&#26500;&#30340;&#20998;&#21106;&#65292;&#24182;&#20171;&#32461;&#20102;&#26576;&#20123;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#22914;&#20309;&#20811;&#26381;&#36825;&#20123;&#22270;&#20687;&#25152;&#20135;&#29983;&#30340;&#29305;&#27530;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#25552;&#20379;&#20102;&#26377;&#20851;&#22312;EM&#20013;&#25512;&#21160;&#28145;&#24230;&#23398;&#20064;&#21457;&#23637;&#30340;&#26174;&#33879;&#25968;&#25454;&#38598;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#24182;&#23637;&#26395;&#20102;EM&#20998;&#21106;&#30340;&#24403;&#21069;&#36235;&#21183;&#21644;&#26410;&#26469;&#21069;&#26223;&#65292;&#29305;&#21035;&#26159;&#22312;&#26631;&#31614;-free&#30340;EM&#20013;&#12290;</title><link>http://arxiv.org/abs/2206.07171</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#22823;&#35268;&#27169;&#32454;&#32990;&#30005;&#23376;&#26174;&#24494;&#38236;&#20013;&#30340;&#20998;&#21106;&#65306;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Segmentation in large-scale cellular electron microscopy with deep learning: A literature survey. (arXiv:2206.07171v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22914;&#20309;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#26469;&#22788;&#29702;&#22823;&#35268;&#27169;&#30340;&#30005;&#23376;&#26174;&#24494;&#38236;&#25968;&#25454;&#38598;&#12290;&#37325;&#28857;&#35752;&#35770;&#20102;&#31639;&#27861;&#22914;&#20309;&#24212;&#29992;&#20110;&#32454;&#32990;&#21644;&#20122;&#32454;&#32990;&#32467;&#26500;&#30340;&#20998;&#21106;&#65292;&#24182;&#20171;&#32461;&#20102;&#26576;&#20123;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#22914;&#20309;&#20811;&#26381;&#36825;&#20123;&#22270;&#20687;&#25152;&#20135;&#29983;&#30340;&#29305;&#27530;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#25552;&#20379;&#20102;&#26377;&#20851;&#22312;EM&#20013;&#25512;&#21160;&#28145;&#24230;&#23398;&#20064;&#21457;&#23637;&#30340;&#26174;&#33879;&#25968;&#25454;&#38598;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#24182;&#23637;&#26395;&#20102;EM&#20998;&#21106;&#30340;&#24403;&#21069;&#36235;&#21183;&#21644;&#26410;&#26469;&#21069;&#26223;&#65292;&#29305;&#21035;&#26159;&#22312;&#26631;&#31614;-free&#30340;EM&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#30005;&#23376;&#26174;&#24494;&#38236;&#20013;&#30340;&#33258;&#21160;&#21270;&#21644;&#21322;&#33258;&#21160;&#21270;&#25216;&#26415;&#20351;&#24471;&#39640;&#36895;&#33719;&#21462;&#22823;&#25968;&#25454;&#38598;&#25104;&#20026;&#20102;&#21487;&#33021;&#12290;&#22240;&#27492;&#65292;&#20998;&#21106;&#26041;&#27861;&#23545;&#20110;&#20998;&#26512;&#21644;&#35299;&#37322;&#36825;&#20123;&#22823;&#37327;&#30340;&#25968;&#25454;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#22312;&#20687;&#32032;&#32423;&#26631;&#27880;&#65288;&#35821;&#20041;&#20998;&#21106;&#65289;&#21644;&#21516;&#19968;&#31867;&#21035;&#30340;&#19981;&#21516;&#23454;&#20363;&#30340;&#26631;&#27880;&#65288;&#23454;&#20363;&#20998;&#21106;&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#32467;&#26524;&#12290;&#26412;&#32508;&#36848;&#30740;&#31350;&#20102;&#36825;&#20123;&#31639;&#27861;&#22914;&#20309;&#34987;&#24212;&#29992;&#20110;&#32454;&#32990;&#21644;&#20122;&#32454;&#32990;&#32467;&#26500;&#30340;&#20998;&#21106;&#12290;&#25991;&#31456;&#25551;&#36848;&#20102;&#36825;&#20123;&#22270;&#20687;&#25152;&#20135;&#29983;&#30340;&#29305;&#27530;&#25361;&#25112;&#20197;&#21450;&#20811;&#26381;&#26576;&#20123;&#25361;&#25112;&#30340;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36824;&#25552;&#20379;&#20102;&#26377;&#20851;&#22312;EM&#20013;&#25512;&#21160;&#28145;&#24230;&#23398;&#20064;&#21457;&#23637;&#30340;&#26174;&#33879;&#25968;&#25454;&#38598;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#26368;&#21518;&#65292;&#32473;&#20986;&#20102;&#24403;&#21069;&#36235;&#21183;&#21644;EM&#20998;&#21106;&#26410;&#26469;&#21069;&#26223;&#30340;&#23637;&#26395;&#65292;&#29305;&#21035;&#26159;&#22312;&#26631;&#31614;-free&#30340;EM&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated and semi-automated techniques in biomedical electron microscopy (EM) enable the acquisition of large datasets at a high rate. Segmentation methods are therefore essential to analyze and interpret these large volumes of data, which can no longer completely be labeled manually. In recent years, deep learning algorithms achieved impressive results in both pixel-level labeling (semantic segmentation) and the labeling of separate instances of the same class (instance segmentation). In this review, we examine how these algorithms were adapted to the task of segmenting cellular and sub-cellular structures in EM images. The special challenges posed by such images and the network architectures that overcame some of them are described. Moreover, a thorough overview is also provided on the notable datasets that contributed to the proliferation of deep learning in EM. Finally, an outlook of current trends and future prospects of EM segmentation is given, especially in the area of label-f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;CLIP&#20316;&#20026;&#22870;&#21169;&#20989;&#25968;, &#29983;&#25104;&#26356;&#32454;&#33268;&#12289;&#29420;&#29305;&#30340;&#22270;&#20687;&#26631;&#39064;&#12290;&#36890;&#36807;FineCapEval&#27979;&#35797;&#65292;&#35813;&#26041;&#27861;&#22312;&#23458;&#35266;&#25351;&#26631;&#21644;&#20154;&#31867;&#35780;&#20272;&#26041;&#38754;&#22343;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2205.13115</link><description>&lt;p&gt;
&#24102;&#26377;CLIP&#22870;&#21169;&#30340;&#32454;&#31890;&#24230;&#22270;&#20687;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;
Fine-grained Image Captioning with CLIP Reward. (arXiv:2205.13115v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13115
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;CLIP&#20316;&#20026;&#22870;&#21169;&#20989;&#25968;, &#29983;&#25104;&#26356;&#32454;&#33268;&#12289;&#29420;&#29305;&#30340;&#22270;&#20687;&#26631;&#39064;&#12290;&#36890;&#36807;FineCapEval&#27979;&#35797;&#65292;&#35813;&#26041;&#27861;&#22312;&#23458;&#35266;&#25351;&#26631;&#21644;&#20154;&#31867;&#35780;&#20272;&#26041;&#38754;&#22343;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22270;&#20687;&#25551;&#36848;&#27169;&#22411;&#36890;&#24120;&#20351;&#29992;&#25991;&#26412;&#30456;&#20284;&#24615;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20844;&#20849;&#25968;&#25454;&#38598;&#20013;&#30340;&#21442;&#32771;&#25551;&#36848;&#36890;&#24120;&#25551;&#36848;&#26368;&#26174;&#33879;&#30340;&#20849;&#21516;&#23545;&#35937;&#65292;&#20351;&#29992;&#25991;&#26412;&#30456;&#20284;&#24615;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#24448;&#24448;&#24573;&#30053;&#20102;&#21306;&#20998;&#22270;&#20687;&#19982;&#20854;&#20182;&#22270;&#20687;&#30340;&#29305;&#23450;&#21644;&#35814;&#32454;&#26041;&#38754;&#12290;&#20026;&#20102;&#26356;&#35814;&#32454;&#21644;&#29420;&#29305;&#22320;&#29983;&#25104;&#26631;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;CLIP&#20316;&#20026;&#22870;&#21169;&#20989;&#25968;&#65292;&#35745;&#31639;&#20174;&#32593;&#32476;&#20013;&#24040;&#22823;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#20013;&#35757;&#32451;&#30340;&#22810;&#27169;&#24335;&#32534;&#30721;&#22120;&#30340;&#22810;&#27169;&#24335;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;fine-tuning&#31574;&#30053;&#65292;&#25913;&#36827;&#20102;CLIP&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#35821;&#27861;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#25991;&#26412;&#27880;&#37322;&#12290;&#36825;&#23436;&#20840;&#28040;&#38500;&#20102;&#22312;&#22870;&#21169;&#35745;&#31639;&#26399;&#38388;&#21442;&#32771;&#26631;&#39064;&#30340;&#38656;&#35201;&#12290;&#20026;&#20102;&#20840;&#38754;&#35780;&#20272;&#25551;&#36848;&#24615;&#26631;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FineCapEval&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#32454;&#31890;&#24230;&#26631;&#20934;&#65288;&#25972;&#20307;&#12289;&#32972;&#26223;&#12289;&#23545;&#35937;&#12289;&#20851;&#31995;&#65289;&#30340;&#26032;&#26631;&#27880;&#25968;&#25454;&#38598;&#12290;&#22312;&#25105;&#20204;&#30340;&#25991;&#26412;-&#22270;&#20687;&#26816;&#32034;&#21644;FineCapEval&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23458;&#35266;&#25351;&#26631;&#21644;&#20154;&#31867;&#35780;&#20272;&#26041;&#38754;&#37117;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern image captioning models are usually trained with text similarity objectives. However, since reference captions in public datasets often describe the most salient common objects, models trained with text similarity objectives tend to ignore specific and detailed aspects of an image that distinguish it from others. Toward more descriptive and distinctive caption generation, we propose using CLIP, a multimodal encoder trained on huge image-text pairs from web, to calculate multimodal similarity and use it as a reward function. We also propose a simple finetuning strategy of the CLIP text encoder to improve grammar that does not require extra text annotation. This completely eliminates the need for reference captions during the reward computation. To comprehensively evaluate descriptive captions, we introduce FineCapEval, a new dataset for caption evaluation with fine-grained criteria: overall, background, object, relations. In our experiments on text-to-image retrieval and FineCapE
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20840;&#23616;&#21407;&#22411;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#33258;&#30417;&#30563;&#20449;&#24687;&#30340;&#27491;&#21017;&#21270;&#19979;&#23398;&#20064;&#25968;&#25454;&#34920;&#31034;&#65292;&#20197;&#32531;&#35299;&#36127;&#38754;&#34920;&#31034;&#28418;&#31227;&#38382;&#39064;&#65292;&#24182;&#20943;&#23569;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;</title><link>http://arxiv.org/abs/2205.12186</link><description>&lt;p&gt;
&#22522;&#20110;&#20840;&#23616;&#21407;&#22411;&#30340;&#22686;&#24378;&#25345;&#32493;&#23398;&#20064;: &#23545;&#25239;&#36127;&#34920;&#31034;&#28418;&#31227;
&lt;/p&gt;
&lt;p&gt;
Enhancing Continual Learning with Global Prototypes: Counteracting Negative Representation Drift. (arXiv:2205.12186v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12186
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20840;&#23616;&#21407;&#22411;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#33258;&#30417;&#30563;&#20449;&#24687;&#30340;&#27491;&#21017;&#21270;&#19979;&#23398;&#20064;&#25968;&#25454;&#34920;&#31034;&#65292;&#20197;&#32531;&#35299;&#36127;&#38754;&#34920;&#31034;&#28418;&#31227;&#38382;&#39064;&#65292;&#24182;&#20943;&#23569;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#26088;&#22312;&#23398;&#20064;&#19968;&#31995;&#21015;&#20219;&#21153;&#65292;&#20854;&#20013;&#25968;&#25454;&#20998;&#24067;&#20174;&#19968;&#20010;&#20219;&#21153;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#20219;&#21153;&#12290;&#22312;&#35757;&#32451;&#26032;&#20219;&#21153;&#25968;&#25454;&#26102;&#65292;&#26087;&#20219;&#21153;&#30340;&#25968;&#25454;&#34920;&#31034;&#21487;&#33021;&#20250;&#28418;&#31227;&#12290;&#19968;&#20123;&#36127;&#38754;&#30340;&#34920;&#31034;&#28418;&#31227;&#21487;&#33021;&#20250;&#23548;&#33268;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#22240;&#20026;&#20250;&#23548;&#33268;&#20174;&#26412;&#22320;&#23398;&#20064;&#30340;&#31867;&#21035;&#21407;&#22411;&#21644;&#25968;&#25454;&#34920;&#31034;&#22312;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#36739;&#24046;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#34920;&#31034;&#28418;&#31227;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20840;&#23616;&#21407;&#22411;&#25351;&#23548;&#23398;&#20064;&#65292;&#29992;&#33258;&#30417;&#30563;&#20449;&#24687;&#30340;&#27491;&#21017;&#21270;&#26469;&#23398;&#20064;&#25968;&#25454;&#34920;&#31034;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23545;&#20110;NLP&#20219;&#21153;&#65292;&#25105;&#20204;&#23558;&#27599;&#20010;&#20219;&#21153;&#20197;&#23631;&#34109;&#35821;&#35328;&#24314;&#27169;&#30340;&#26041;&#24335;&#36827;&#34892;&#20844;&#24335;&#21270;&#65292;&#24182;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30456;&#37051;&#27880;&#24847;&#26426;&#21046;&#23398;&#20064;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#23398;&#20064;&#20986;&#20855;&#26377;&#36739;&#23569;&#34920;&#31034;&#28418;&#31227;&#30340;&#30456;&#24403;&#19968;&#33268;&#30340;&#34920;&#31034;&#65292;&#24182;&#22312;&#19981;&#37325;&#26032;&#37319;&#26679;&#36807;&#21435;&#20219;&#21153;&#30340;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#20943;&#23569;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning (CL) aims to learn a sequence of tasks over time, with data distributions shifting from one task to another. When training on new task data, data representations from old tasks may drift. Some negative representation drift can result in catastrophic forgetting, by causing the locally learned class prototypes and data representations to correlate poorly across tasks. To mitigate such representation drift, we propose a method that finds global prototypes to guide the learning, and learns data representations with the regularization of the self-supervised information. Specifically, for NLP tasks, we formulate each task in a masked language modeling style, and learn the task via a neighbor attention mechanism over a pre-trained language model. Experimental results show that our proposed method can learn fairly consistent representations with less representation drift, and significantly reduce catastrophic forgetting in CL without resampling data from past tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;HaMSE&#26412;&#20307;&#35770;&#65292;&#23427;&#21487;&#20197;&#25551;&#36848;&#38899;&#20048;&#29305;&#24449;&#24182;&#26377;&#21161;&#20110;&#38899;&#20048;&#23398;&#30740;&#31350;&#12290;&#23427;&#36890;&#36807;&#20801;&#35768;&#19981;&#21516;&#38899;&#20048;&#34920;&#29616;&#31995;&#32479;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#24182;&#25551;&#36848;&#20102;&#19968;&#32452;&#38899;&#20048;&#23398;&#29305;&#24449;&#65292;&#21487;&#20197;&#35299;&#20915;&#38899;&#20048;&#34920;&#31034;&#21644;&#23450;&#37327;&#21644;&#23450;&#24615;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2202.05817</link><description>&lt;p&gt;
HaMSE&#26412;&#20307;&#35770;&#65306;&#21033;&#29992;&#35821;&#20041;&#25216;&#26415;&#25903;&#25345;&#38899;&#20048;&#34920;&#29616;&#20114;&#25805;&#20316;&#24615;&#21644;&#38899;&#20048;&#23398;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
The HaMSE Ontology: Using Semantic Technologies to support Music Representation Interoperability and Musicological Analysis. (arXiv:2202.05817v1 [cs.SD] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.05817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;HaMSE&#26412;&#20307;&#35770;&#65292;&#23427;&#21487;&#20197;&#25551;&#36848;&#38899;&#20048;&#29305;&#24449;&#24182;&#26377;&#21161;&#20110;&#38899;&#20048;&#23398;&#30740;&#31350;&#12290;&#23427;&#36890;&#36807;&#20801;&#35768;&#19981;&#21516;&#38899;&#20048;&#34920;&#29616;&#31995;&#32479;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#24182;&#25551;&#36848;&#20102;&#19968;&#32452;&#38899;&#20048;&#23398;&#29305;&#24449;&#65292;&#21487;&#20197;&#35299;&#20915;&#38899;&#20048;&#34920;&#31034;&#21644;&#23450;&#37327;&#21644;&#23450;&#24615;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#35821;&#20041;&#25216;&#26415; - &#29305;&#21035;&#26159;&#35821;&#20041;Web - &#24050;&#32463;&#25104;&#20026;&#25551;&#36848;&#25991;&#21270;&#36951;&#20135;&#39046;&#22495;&#21644;&#33402;&#26415;&#23454;&#36341;&#30340;&#20255;&#22823;&#24037;&#20855;&#12290; &#28982;&#32780;&#65292;&#38899;&#20048;&#23398;&#24212;&#29992;&#26412;&#20307;&#30340;&#26223;&#35266;&#20284;&#20046;&#26159;&#26377;&#38480;&#30340;&#65292;&#24182;&#19988;&#20165;&#38480;&#20110;&#29305;&#23450;&#30340;&#24212;&#29992;&#31243;&#24207;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HaMSE&#65292;&#19968;&#31181;&#33021;&#22815;&#25551;&#36848;&#38899;&#20048;&#29305;&#24449;&#24182;&#26377;&#21161;&#20110;&#38899;&#20048;&#23398;&#30740;&#31350;&#30340;&#26412;&#20307;&#35770;&#12290;&#26356;&#20855;&#20307;&#32780;&#35328;&#65292;HaMSE&#26088;&#22312;&#35299;&#20915;&#22256;&#25200;&#38899;&#20048;&#23398;&#30740;&#31350;&#20960;&#21313;&#24180;&#30340;&#38382;&#39064;&#65306;&#38899;&#20048;&#30340;&#34920;&#31034;&#21644;&#23450;&#37327;&#21644;&#23450;&#24615;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20026;&#27492;&#65292;HaMSE&#20801;&#35768;&#19981;&#21516;&#38899;&#20048;&#34920;&#29616;&#31995;&#32479;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#24182;&#25551;&#36848;&#20102;&#19968;&#32452;&#38899;&#20048;&#23398;&#29305;&#24449;&#65292;&#21487;&#20197;&#20801;&#35768;&#20197;&#19981;&#21516;&#30340;&#31890;&#24230;&#32423;&#21035;&#36827;&#34892;&#38899;&#20048;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of Semantic Technologies - in particular the Semantic Web - has revealed to be a great tool for describing the cultural heritage domain and artistic practices. However, the panorama of ontologies for musicological applications seems to be limited and restricted to specific applications. In this research, we propose HaMSE, an ontology capable of describing musical features that can assist musicological research. More specifically, HaMSE proposes to address sues that have been affecting musicological research for decades: the representation of music and the relationship between quantitative and qualitative data. To do this, HaMSE allows the alignment between different music representation systems and describes a set of musicological features that can allow the music analysis at different granularity levels.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#31867;&#24863;&#30693;&#36857;&#27604;&#20248;&#21270;&#30340;&#36890;&#36947;&#21098;&#26525;&#26041;&#27861;&#65288;CATRO&#65289;&#65292;&#36890;&#36807;&#29305;&#24449;&#31354;&#38388;&#21028;&#21035;&#24230;&#37327;&#22810;&#36890;&#36947;&#30340;&#32852;&#21512;&#24433;&#21709;&#24182;&#21512;&#24182;&#20445;&#30041;&#36890;&#36947;&#30340;&#23618;&#27425;&#24433;&#21709;&#65292;&#26377;&#25928;&#38477;&#20302;&#35745;&#31639;&#36127;&#25285;&#24182;&#21152;&#36895;&#27169;&#22411;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2110.10921</link><description>&lt;p&gt;
CATRO&#65306;&#22522;&#20110;&#31867;&#24863;&#30693;&#30340;&#36857;&#27604;&#20248;&#21270;&#30340;&#36890;&#36947;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
CATRO: Channel Pruning via Class-Aware Trace Ratio Optimization. (arXiv:2110.10921v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.10921
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#31867;&#24863;&#30693;&#36857;&#27604;&#20248;&#21270;&#30340;&#36890;&#36947;&#21098;&#26525;&#26041;&#27861;&#65288;CATRO&#65289;&#65292;&#36890;&#36807;&#29305;&#24449;&#31354;&#38388;&#21028;&#21035;&#24230;&#37327;&#22810;&#36890;&#36947;&#30340;&#32852;&#21512;&#24433;&#21709;&#24182;&#21512;&#24182;&#20445;&#30041;&#36890;&#36947;&#30340;&#23618;&#27425;&#24433;&#21709;&#65292;&#26377;&#25928;&#38477;&#20302;&#35745;&#31639;&#36127;&#25285;&#24182;&#21152;&#36895;&#27169;&#22411;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#24212;&#29992;&#22330;&#26223;&#20013;&#23384;&#22312;&#39640;&#21442;&#25968;&#21644;&#35745;&#31639;&#20887;&#20313;&#65292;&#24517;&#35201;&#26102;&#38656;&#35201;&#36827;&#34892;&#27169;&#22411;&#21098;&#26525;&#20197;&#33719;&#24471;&#36731;&#37327;&#32423;&#21644;&#39640;&#25928;&#30340;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#21098;&#26525;&#26041;&#27861;&#26159;&#30001;&#32463;&#39564;&#21551;&#21457;&#24335;&#30340;&#65292;&#24456;&#23569;&#32771;&#34385;&#36890;&#36947;&#30340;&#32852;&#21512;&#24433;&#21709;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#30830;&#23450;&#21644;&#27425;&#20248;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#31867;&#24863;&#30693;&#36857;&#27604;&#20248;&#21270;&#30340;&#36890;&#36947;&#21098;&#26525;&#26041;&#27861;&#65288;CATRO&#65289;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#36127;&#25285;&#24182;&#21152;&#36895;&#27169;&#22411;&#25512;&#29702;&#12290;&#21033;&#29992;&#23569;&#37327;&#26679;&#26412;&#30340;&#31867;&#21035;&#20449;&#24687;&#65292;CATRO&#36890;&#36807;&#29305;&#24449;&#31354;&#38388;&#21028;&#21035;&#24230;&#37327;&#22810;&#36890;&#36947;&#30340;&#32852;&#21512;&#24433;&#21709;&#65292;&#24182;&#21512;&#24182;&#20445;&#30041;&#36890;&#36947;&#30340;&#23618;&#27425;&#24433;&#21709;&#12290;&#36890;&#36807;&#23558;&#36890;&#36947;&#21098;&#26525;&#24418;&#24335;&#21270;&#20026;&#23376;&#27169;&#20989;&#25968;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;CATRO&#36890;&#36807;&#20004;&#38454;&#27573;&#36138;&#24515;&#36845;&#20195;&#20248;&#21270;&#36807;&#31243;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;CATRO&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep convolutional neural networks are shown to be overkill with high parametric and computational redundancy in many application scenarios, and an increasing number of works have explored model pruning to obtain lightweight and efficient networks. However, most existing pruning approaches are driven by empirical heuristic and rarely consider the joint impact of channels, leading to unguaranteed and suboptimal performance. In this paper, we propose a novel channel pruning method via Class-Aware Trace Ratio Optimization (CATRO) to reduce the computational burden and accelerate the model inference. Utilizing class information from a few samples, CATRO measures the joint impact of multiple channels by feature space discriminations and consolidates the layer-wise impact of preserved channels. By formulating channel pruning as a submodular set function maximization problem, CATRO solves it efficiently via a two-stage greedy iterative optimization procedure. More importantly, we present theo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#23545;&#20110;&#30830;&#23450;&#24615;&#21644;&#21487;&#20998;&#35299;&#30340;&#24067;&#23572;&#30005;&#36335;&#65292;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#35745;&#31639;$\mathsf{SHAP}$-score&#65292;&#20294;&#35745;&#31639;&#22823;&#22810;&#25968;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;$\mathsf{SHAP}$-scores&#26159;&#35745;&#31639;&#19978;&#22256;&#38590;&#30340;&#12290;</title><link>http://arxiv.org/abs/2104.08015</link><description>&lt;p&gt;
&#22522;&#20110;SHAP-Score&#30340;&#35299;&#37322;&#30340;&#22797;&#26434;&#24615;&#30740;&#31350;&#65306;&#36890;&#36807;&#30693;&#35782;&#32534;&#35793;&#21644;&#19981;&#21487;&#36817;&#20284;&#24615;&#32467;&#26524;&#23454;&#29616;&#21487;&#22788;&#29702;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Complexity of SHAP-Score-Based Explanations: Tractability via Knowledge Compilation and Non-Approximability Results. (arXiv:2104.08015v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.08015
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#23545;&#20110;&#30830;&#23450;&#24615;&#21644;&#21487;&#20998;&#35299;&#30340;&#24067;&#23572;&#30005;&#36335;&#65292;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#35745;&#31639;$\mathsf{SHAP}$-score&#65292;&#20294;&#35745;&#31639;&#22823;&#22810;&#25968;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;$\mathsf{SHAP}$-scores&#26159;&#35745;&#31639;&#19978;&#22256;&#38590;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;$\mathsf{SHAP}$-score&#26159;Shapley&#20540;&#30340;&#19968;&#20010;&#29256;&#26412;&#65292;&#29992;&#20110;&#36890;&#36807;&#32473;&#27599;&#20010;&#29305;&#24449;&#20998;&#37197;&#19968;&#20010;&#24471;&#20998;&#26469;&#35299;&#37322;&#23398;&#20064;&#27169;&#22411;&#22312;&#29305;&#23450;&#23454;&#20307;&#19978;&#30340;&#32467;&#26524;&#12290;&#34429;&#28982;&#35745;&#31639;Shapley&#20540;&#36890;&#24120;&#26159;&#19968;&#20010;&#38590;&#20197;&#22788;&#29702;&#30340;&#38382;&#39064;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#39033;&#24378;&#26377;&#21147;&#30340;&#27491;&#38754;&#32467;&#26524;&#65292;&#21363;&#23545;&#20110;&#30830;&#23450;&#24615;&#21644;&#21487;&#20998;&#35299;&#30340;&#24067;&#23572;&#30005;&#36335;&#65292;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#35745;&#31639;$\mathsf{SHAP}$-score&#12290;&#36825;&#20123;&#30005;&#36335;&#22312;&#30693;&#35782;&#32534;&#35793;&#39046;&#22495;&#20013;&#24471;&#21040;&#30740;&#31350;&#65292;&#24191;&#27867;&#25512;&#24191;&#20102;&#21508;&#31181;&#24067;&#23572;&#30005;&#36335;&#21644;&#20108;&#36827;&#21046;&#20915;&#31574;&#22270;&#31867;&#65292;&#21253;&#25324;&#20108;&#36827;&#21046;&#20915;&#31574;&#26641;&#21644;&#26377;&#24207;&#20108;&#36827;&#21046;&#20915;&#31574;&#22270;&#65288;OBDD&#65289;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#35266;&#23519;&#21457;&#29616;&#65292;&#22312;&#19968;&#20010;&#24067;&#23572;&#27169;&#22411;&#31867;&#19978;&#35745;&#31639;$\mathsf{SHAP}$-score&#24635;&#26159;&#21644;&#35813;&#31867;&#30340;&#27169;&#22411;&#35745;&#25968;&#38382;&#39064;&#19968;&#26679;&#22256;&#38590;&#30340;&#22810;&#39033;&#24335;&#38382;&#39064;&#12290;&#36825;&#24847;&#21619;&#30528;&#25105;&#20204;&#32771;&#34385;&#30340;&#30005;&#36335;&#30340;&#30830;&#23450;&#24615;&#21644;&#21487;&#20998;&#35299;&#24615;&#37117;&#26159;&#24517;&#35201;&#30340;&#23646;&#24615;&#12290;&#36825;&#36824;&#24847;&#21619;&#30528;&#65292;&#38500;&#38750;NP=#P&#65292;&#21542;&#21017;&#35745;&#31639;&#22823;&#22810;&#25968;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;$\mathsf{SHAP}$-scores&#26159;&#35745;&#31639;&#19978;&#22256;&#38590;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Machine Learning, the $\mathsf{SHAP}$-score is a version of the Shapley value that is used to explain the result of a learned model on a specific entity by assigning a score to every feature. While in general computing Shapley values is an intractable problem, we prove a strong positive result stating that the $\mathsf{SHAP}$-score can be computed in polynomial time over deterministic and decomposable Boolean circuits. Such circuits are studied in the field of Knowledge Compilation and generalize a wide range of Boolean circuits and binary decision diagrams classes, including binary decision trees and Ordered Binary Decision Diagrams (OBDDs).  We also establish the computational limits of the SHAP-score by observing that computing it over a class of Boolean models is always polynomially as hard as the model counting problem for that class. This implies that both determinism and decomposability are essential properties for the circuits that we consider. It also implies that computing
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23567;/&#22823;&#19990;&#30028;&#20013;&#26368;&#20248;&#23398;&#20064;&#34892;&#20026;&#30340;&#29305;&#24449;&#65292;&#21457;&#29616;&#38543;&#30528;&#29615;&#22659;&#21464;&#24471;&#26356;&#22797;&#26434;&#21644;&#20915;&#31574;&#32773;&#30340;&#35748;&#30693;&#33021;&#21147;&#21464;&#24369;&#65292;&#26368;&#20248;&#34892;&#20026;&#36880;&#28176;&#19981;&#21516;&#12290;&#22312;&#22823;&#19990;&#30028;&#20013;&#65292;&#26368;&#20248;&#23398;&#20064;&#34892;&#20026;&#21487;&#33021;&#34920;&#29616;&#20986;&#22810;&#31181;&#38750;&#36125;&#21494;&#26031;&#23398;&#20064;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2009.11917</link><description>&lt;p&gt;
&#23567;/&#22823;&#19990;&#30028;&#20013;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning in a Small/Big World. (arXiv:2009.11917v8 [econ.TH] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2009.11917
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23567;/&#22823;&#19990;&#30028;&#20013;&#26368;&#20248;&#23398;&#20064;&#34892;&#20026;&#30340;&#29305;&#24449;&#65292;&#21457;&#29616;&#38543;&#30528;&#29615;&#22659;&#21464;&#24471;&#26356;&#22797;&#26434;&#21644;&#20915;&#31574;&#32773;&#30340;&#35748;&#30693;&#33021;&#21147;&#21464;&#24369;&#65292;&#26368;&#20248;&#34892;&#20026;&#36880;&#28176;&#19981;&#21516;&#12290;&#22312;&#22823;&#19990;&#30028;&#20013;&#65292;&#26368;&#20248;&#23398;&#20064;&#34892;&#20026;&#21487;&#33021;&#34920;&#29616;&#20986;&#22810;&#31181;&#38750;&#36125;&#21494;&#26031;&#23398;&#20064;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#24615;&#21644;&#26377;&#38480;&#33021;&#21147;&#23545;&#25105;&#20204;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#23398;&#20064;&#21644;&#20915;&#31574;&#26377;&#28145;&#21051;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#20351;&#29992;&#26377;&#38480;&#33258;&#21160;&#26426;&#29702;&#35770;&#26469;&#27169;&#25311;&#20449;&#24565;&#24418;&#25104;&#36807;&#31243;&#65292;&#30740;&#31350;&#20102;&#22312;&#23567;&#19990;&#30028;&#21644;&#22823;&#19990;&#30028;&#20013;&#65292;&#21363;&#29615;&#22659;&#22797;&#26434;&#24230;&#30456;&#23545;&#20110;&#20915;&#31574;&#32773;&#30340;&#35748;&#30693;&#33021;&#21147;&#36739;&#20302;&#25110;&#36739;&#39640;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#20248;&#23398;&#20064;&#34892;&#20026;&#30340;&#29305;&#24449;&#12290;&#22312;&#38750;&#24120;&#23567;&#30340;&#19990;&#30028;&#20013;&#65292;&#26368;&#20248;&#34892;&#20026;&#38750;&#24120;&#25509;&#36817;&#36125;&#21494;&#26031;&#22522;&#20934;&#65292;&#20294;&#38543;&#30528;&#19990;&#30028;&#30340;&#21464;&#22823;&#65292;&#26368;&#20248;&#34892;&#20026;&#21017;&#36234;&#26469;&#36234;&#19981;&#21516;&#12290;&#27492;&#22806;&#65292;&#22312;&#22823;&#19990;&#30028;&#20013;&#65292;&#26368;&#20248;&#23398;&#20064;&#34892;&#20026;&#21487;&#33021;&#34920;&#29616;&#20986;&#22810;&#31181;&#24050;&#26377;&#25991;&#29486;&#25253;&#36947;&#36807;&#30340;&#38750;&#36125;&#21494;&#26031;&#23398;&#20064;&#34892;&#20026;&#65292;&#21253;&#25324;&#21551;&#21457;&#24335;&#30340;&#20351;&#29992;&#12289;&#30456;&#20851;&#24573;&#35270;&#12289;&#25345;&#32493;&#30340;&#36807;&#24230;&#33258;&#20449;&#12289;&#19981;&#27880;&#24847;&#23398;&#20064;&#20197;&#21450;&#27169;&#22411;&#31616;&#21270;&#25110;&#35823;&#35774;&#31561;&#34892;&#20026;&#12290;&#36825;&#20123;&#32467;&#26524;&#24314;&#31435;&#20102;&#38750;&#36125;&#21494;&#26031;&#23398;&#20064;&#34892;&#20026;&#12289;&#22797;&#26434;&#24230;&#21644;&#35748;&#30693;&#33021;&#21147;&#20043;&#38388;&#26126;&#30830;&#21487;&#39564;&#35777;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complexity and limited ability have profound effect on how we learn and make decisions under uncertainty. Using the theory of finite automaton to model belief formation, this paper studies the characteristics of optimal learning behavior in small and big worlds, where the complexity of the environment is low and high, respectively, relative to the cognitive ability of the decision maker. Optimal behavior is well approximated by the Bayesian benchmark in very small world but is more different as the world gets bigger. In addition, in big worlds, the optimal learning behavior could exhibit a wide range of well-documented non-Bayesian learning behavior, including the use of heuristics, correlation neglect, persistent over-confidence, inattentive learning, and other behaviors of model simplification or misspecification. These results establish a clear and testable relationship among the prominence of non-Bayesian learning behavior, complexity, and cognitive ability.
&lt;/p&gt;</description></item></channel></rss>